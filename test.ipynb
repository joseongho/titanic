{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>701</th>\n",
       "      <td>702</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Silverthorne, Mr. Spencer Victor</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17475</td>\n",
       "      <td>26.2875</td>\n",
       "      <td>E24</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>181</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Sage, Miss. Constance Gladys</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>CA. 2343</td>\n",
       "      <td>69.5500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>373</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Beavan, Mr. William Thomas</td>\n",
       "      <td>male</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>323951</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass                              Name     Sex  \\\n",
       "701          702         1       1  Silverthorne, Mr. Spencer Victor    male   \n",
       "180          181         0       3      Sage, Miss. Constance Gladys  female   \n",
       "372          373         0       3        Beavan, Mr. William Thomas    male   \n",
       "\n",
       "      Age  SibSp  Parch    Ticket     Fare Cabin Embarked  \n",
       "701  35.0      0      0  PC 17475  26.2875   E24        S  \n",
       "180   NaN      8      2  CA. 2343  69.5500   NaN        S  \n",
       "372  19.0      0      0    323951   8.0500   NaN        S  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF = pd.read_csv('train.csv')\n",
    "print(trainDF.info())\n",
    "trainDF.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 418 entries, 0 to 417\n",
      "Data columns (total 11 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  418 non-null    int64  \n",
      " 1   Pclass       418 non-null    int64  \n",
      " 2   Name         418 non-null    object \n",
      " 3   Sex          418 non-null    object \n",
      " 4   Age          332 non-null    float64\n",
      " 5   SibSp        418 non-null    int64  \n",
      " 6   Parch        418 non-null    int64  \n",
      " 7   Ticket       418 non-null    object \n",
      " 8   Fare         417 non-null    float64\n",
      " 9   Cabin        91 non-null     object \n",
      " 10  Embarked     418 non-null    object \n",
      "dtypes: float64(2), int64(4), object(5)\n",
      "memory usage: 36.0+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>1151</td>\n",
       "      <td>3</td>\n",
       "      <td>Midtsjo, Mr. Karl Albert</td>\n",
       "      <td>male</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>345501</td>\n",
       "      <td>7.7750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>1215</td>\n",
       "      <td>1</td>\n",
       "      <td>Rowe, Mr. Alfred G</td>\n",
       "      <td>male</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>113790</td>\n",
       "      <td>26.5500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>1038</td>\n",
       "      <td>1</td>\n",
       "      <td>Hilliard, Mr. Herbert Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17463</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>E46</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Pclass                         Name   Sex   Age  SibSp  \\\n",
       "259         1151       3     Midtsjo, Mr. Karl Albert  male  21.0      0   \n",
       "323         1215       1           Rowe, Mr. Alfred G  male  33.0      0   \n",
       "146         1038       1  Hilliard, Mr. Herbert Henry  male   NaN      0   \n",
       "\n",
       "     Parch  Ticket     Fare Cabin Embarked  \n",
       "259      0  345501   7.7750   NaN        S  \n",
       "323      0  113790  26.5500   NaN        S  \n",
       "146      0   17463  51.8625   E46        S  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDF = pd.read_csv('test.csv')\n",
    "print(testDF.info())\n",
    "testDF.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Survived</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.338481</td>\n",
       "      <td>-0.077221</td>\n",
       "      <td>-0.035322</td>\n",
       "      <td>0.081629</td>\n",
       "      <td>0.257307</td>\n",
       "      <td>0.543351</td>\n",
       "      <td>-0.543351</td>\n",
       "      <td>0.168240</td>\n",
       "      <td>0.003650</td>\n",
       "      <td>-0.155660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pclass</th>\n",
       "      <td>-0.338481</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.408106</td>\n",
       "      <td>0.060832</td>\n",
       "      <td>0.018322</td>\n",
       "      <td>-0.558629</td>\n",
       "      <td>-0.124617</td>\n",
       "      <td>0.124617</td>\n",
       "      <td>-0.269658</td>\n",
       "      <td>0.230491</td>\n",
       "      <td>0.096335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>-0.077221</td>\n",
       "      <td>-0.408106</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.243699</td>\n",
       "      <td>-0.150917</td>\n",
       "      <td>0.178740</td>\n",
       "      <td>-0.063645</td>\n",
       "      <td>0.063645</td>\n",
       "      <td>0.085777</td>\n",
       "      <td>-0.019458</td>\n",
       "      <td>-0.075972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SibSp</th>\n",
       "      <td>-0.035322</td>\n",
       "      <td>0.060832</td>\n",
       "      <td>-0.243699</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.373587</td>\n",
       "      <td>0.160238</td>\n",
       "      <td>0.109609</td>\n",
       "      <td>-0.109609</td>\n",
       "      <td>-0.048396</td>\n",
       "      <td>-0.048678</td>\n",
       "      <td>0.075198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Parch</th>\n",
       "      <td>0.081629</td>\n",
       "      <td>0.018322</td>\n",
       "      <td>-0.150917</td>\n",
       "      <td>0.373587</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.221539</td>\n",
       "      <td>0.213125</td>\n",
       "      <td>-0.213125</td>\n",
       "      <td>-0.008635</td>\n",
       "      <td>-0.100943</td>\n",
       "      <td>0.073258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fare</th>\n",
       "      <td>0.257307</td>\n",
       "      <td>-0.558629</td>\n",
       "      <td>0.178740</td>\n",
       "      <td>0.160238</td>\n",
       "      <td>0.221539</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.185523</td>\n",
       "      <td>-0.185523</td>\n",
       "      <td>0.286269</td>\n",
       "      <td>-0.130059</td>\n",
       "      <td>-0.172683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sex_female</th>\n",
       "      <td>0.543351</td>\n",
       "      <td>-0.124617</td>\n",
       "      <td>-0.063645</td>\n",
       "      <td>0.109609</td>\n",
       "      <td>0.213125</td>\n",
       "      <td>0.185523</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.066564</td>\n",
       "      <td>0.088651</td>\n",
       "      <td>-0.119504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sex_male</th>\n",
       "      <td>-0.543351</td>\n",
       "      <td>0.124617</td>\n",
       "      <td>0.063645</td>\n",
       "      <td>-0.109609</td>\n",
       "      <td>-0.213125</td>\n",
       "      <td>-0.185523</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.066564</td>\n",
       "      <td>-0.088651</td>\n",
       "      <td>0.119504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Embarked_C</th>\n",
       "      <td>0.168240</td>\n",
       "      <td>-0.269658</td>\n",
       "      <td>0.085777</td>\n",
       "      <td>-0.048396</td>\n",
       "      <td>-0.008635</td>\n",
       "      <td>0.286269</td>\n",
       "      <td>0.066564</td>\n",
       "      <td>-0.066564</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.164166</td>\n",
       "      <td>-0.775441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Embarked_Q</th>\n",
       "      <td>0.003650</td>\n",
       "      <td>0.230491</td>\n",
       "      <td>-0.019458</td>\n",
       "      <td>-0.048678</td>\n",
       "      <td>-0.100943</td>\n",
       "      <td>-0.130059</td>\n",
       "      <td>0.088651</td>\n",
       "      <td>-0.088651</td>\n",
       "      <td>-0.164166</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.489874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Embarked_S</th>\n",
       "      <td>-0.155660</td>\n",
       "      <td>0.096335</td>\n",
       "      <td>-0.075972</td>\n",
       "      <td>0.075198</td>\n",
       "      <td>0.073258</td>\n",
       "      <td>-0.172683</td>\n",
       "      <td>-0.119504</td>\n",
       "      <td>0.119504</td>\n",
       "      <td>-0.775441</td>\n",
       "      <td>-0.489874</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Survived    Pclass       Age     SibSp     Parch      Fare  \\\n",
       "Survived    1.000000 -0.338481 -0.077221 -0.035322  0.081629  0.257307   \n",
       "Pclass     -0.338481  1.000000 -0.408106  0.060832  0.018322 -0.558629   \n",
       "Age        -0.077221 -0.408106  1.000000 -0.243699 -0.150917  0.178740   \n",
       "SibSp      -0.035322  0.060832 -0.243699  1.000000  0.373587  0.160238   \n",
       "Parch       0.081629  0.018322 -0.150917  0.373587  1.000000  0.221539   \n",
       "Fare        0.257307 -0.558629  0.178740  0.160238  0.221539  1.000000   \n",
       "Sex_female  0.543351 -0.124617 -0.063645  0.109609  0.213125  0.185523   \n",
       "Sex_male   -0.543351  0.124617  0.063645 -0.109609 -0.213125 -0.185523   \n",
       "Embarked_C  0.168240 -0.269658  0.085777 -0.048396 -0.008635  0.286269   \n",
       "Embarked_Q  0.003650  0.230491 -0.019458 -0.048678 -0.100943 -0.130059   \n",
       "Embarked_S -0.155660  0.096335 -0.075972  0.075198  0.073258 -0.172683   \n",
       "\n",
       "            Sex_female  Sex_male  Embarked_C  Embarked_Q  Embarked_S  \n",
       "Survived      0.543351 -0.543351    0.168240    0.003650   -0.155660  \n",
       "Pclass       -0.124617  0.124617   -0.269658    0.230491    0.096335  \n",
       "Age          -0.063645  0.063645    0.085777   -0.019458   -0.075972  \n",
       "SibSp         0.109609 -0.109609   -0.048396   -0.048678    0.075198  \n",
       "Parch         0.213125 -0.213125   -0.008635   -0.100943    0.073258  \n",
       "Fare          0.185523 -0.185523    0.286269   -0.130059   -0.172683  \n",
       "Sex_female    1.000000 -1.000000    0.066564    0.088651   -0.119504  \n",
       "Sex_male     -1.000000  1.000000   -0.066564   -0.088651    0.119504  \n",
       "Embarked_C    0.066564 -0.066564    1.000000   -0.164166   -0.775441  \n",
       "Embarked_Q    0.088651 -0.088651   -0.164166    1.000000   -0.489874  \n",
       "Embarked_S   -0.119504  0.119504   -0.775441   -0.489874    1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatDF = pd.concat([trainDF,testDF])\n",
    "concatDF = concatDF.reset_index(drop=True)\n",
    "pd.get_dummies(concatDF,columns=['Sex','Embarked']).drop(columns=['PassengerId','Name','Ticket','Cabin']).corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "      <th>Pclass_1</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014151</td>\n",
       "      <td>0.156673</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4750</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.139136</td>\n",
       "      <td>0.782000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3250</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015469</td>\n",
       "      <td>0.697802</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.103644</td>\n",
       "      <td>0.782000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015713</td>\n",
       "      <td>0.156673</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>887.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3375</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025374</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>888.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2375</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058556</td>\n",
       "      <td>0.697802</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>889.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3000</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.045771</td>\n",
       "      <td>0.697802</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>890.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3250</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058556</td>\n",
       "      <td>0.156673</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>891.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015127</td>\n",
       "      <td>0.156673</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived     Age  SibSp     Parch      Fare      Name  \\\n",
       "0            1.0       0.0  0.2750  0.125  0.000000  0.014151  0.156673   \n",
       "1            2.0       1.0  0.4750  0.125  0.000000  0.139136  0.782000   \n",
       "2            3.0       1.0  0.3250  0.000  0.000000  0.015469  0.697802   \n",
       "3            4.0       1.0  0.4375  0.125  0.000000  0.103644  0.782000   \n",
       "4            5.0       0.0  0.4375  0.000  0.000000  0.015713  0.156673   \n",
       "..           ...       ...     ...    ...       ...       ...       ...   \n",
       "886        887.0       0.0  0.3375  0.000  0.000000  0.025374  0.000000   \n",
       "887        888.0       1.0  0.2375  0.000  0.000000  0.058556  0.697802   \n",
       "888        889.0       0.0  0.3000  0.125  0.222222  0.045771  0.697802   \n",
       "889        890.0       1.0  0.3250  0.000  0.000000  0.058556  0.156673   \n",
       "890        891.0       0.0  0.4000  0.000  0.000000  0.015127  0.156673   \n",
       "\n",
       "     Sex_female  Sex_male  Embarked_C  Embarked_Q  Embarked_S  Pclass_1  \\\n",
       "0           0.0       1.0         0.0         0.0         1.0       0.0   \n",
       "1           1.0       0.0         1.0         0.0         0.0       1.0   \n",
       "2           1.0       0.0         0.0         0.0         1.0       0.0   \n",
       "3           1.0       0.0         0.0         0.0         1.0       1.0   \n",
       "4           0.0       1.0         0.0         0.0         1.0       0.0   \n",
       "..          ...       ...         ...         ...         ...       ...   \n",
       "886         0.0       1.0         0.0         0.0         1.0       0.0   \n",
       "887         1.0       0.0         0.0         0.0         1.0       1.0   \n",
       "888         1.0       0.0         0.0         0.0         1.0       0.0   \n",
       "889         0.0       1.0         1.0         0.0         0.0       1.0   \n",
       "890         0.0       1.0         0.0         1.0         0.0       0.0   \n",
       "\n",
       "     Pclass_2  Pclass_3  \n",
       "0         0.0       1.0  \n",
       "1         0.0       0.0  \n",
       "2         0.0       1.0  \n",
       "3         0.0       0.0  \n",
       "4         0.0       1.0  \n",
       "..        ...       ...  \n",
       "886       1.0       0.0  \n",
       "887       0.0       0.0  \n",
       "888       0.0       1.0  \n",
       "889       0.0       0.0  \n",
       "890       0.0       1.0  \n",
       "\n",
       "[891 rows x 15 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featuredDF = pd.read_csv('train3.csv')\n",
    "featuredDF = pd.get_dummies(featuredDF,columns=['Sex','Embarked','Pclass'])\n",
    "featuredDF = featuredDF.drop(columns=['Ticket','Cabin'])\n",
    "featuredDF['Age'] = featuredDF['Age']/featuredDF['Age'].max()\n",
    "featuredDF['Fare'] = featuredDF['Fare']/featuredDF['Fare'].max()\n",
    "featuredDF['SibSp'] = featuredDF['SibSp']/featuredDF['SibSp'].max()\n",
    "featuredDF['Parch'] = featuredDF['Parch']/featuredDF['Parch'].max()\n",
    "featuredDF = featuredDF.astype(float)\n",
    "featuredDF[:891]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.2750, 0.1250, 0.0000, 0.0142, 0.1567, 0.0000, 1.0000, 0.0000, 0.0000,\n",
       "         1.0000, 0.0000, 0.0000, 1.0000]),\n",
       " tensor([1., 0.]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, DF:pd.DataFrame):\n",
    "        self.PassengerId= DF['PassengerId'].values\n",
    "        self.Servived = pd.get_dummies(DF['Survived']).values\n",
    "        DF = DF.drop(columns=['PassengerId','Survived'])\n",
    "        self.data = DF.astype(float).values\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.PassengerId)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        x = torch.FloatTensor(self.data[idx])\n",
    "        y = torch.FloatTensor(self.Servived[idx])\n",
    "        return x, y\n",
    "\n",
    "\n",
    "dataSet = MyDataset(DF=featuredDF[:891])\n",
    "testSet = MyDataset(DF=featuredDF[891:])\n",
    "dataSet[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainSet, valSet = torch.utils.data.random_split(dataSet,(0.8,0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0.3375, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 1.0000],\n",
       "         [0.2375, 0.0000, 0.0000,  ..., 1.0000, 0.0000, 0.0000],\n",
       "         [0.3000, 0.0000, 0.0000,  ..., 1.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.2625, 0.2500, 0.2222,  ..., 1.0000, 0.0000, 0.0000],\n",
       "         [0.4375, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 1.0000],\n",
       "         [0.2000, 0.1250, 0.3333,  ..., 0.0000, 0.0000, 1.0000]]),\n",
       " tensor([[0., 1.],\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         ...,\n",
       "         [0., 1.],\n",
       "         [1., 0.],\n",
       "         [1., 0.]])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainLoader= torch.utils.data.DataLoader(trainSet,batch_size=2048,sampler=torch.utils.data.RandomSampler(trainSet))\n",
    "valLoader= torch.utils.data.DataLoader(valSet,batch_size=2048,sampler=torch.utils.data.RandomSampler(valSet))\n",
    "testLoader = torch.utils.data.DataLoader(testSet,batch_size=2048)\n",
    "next(iter(trainLoader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n",
      "NeuralNetwork(\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=13, out_features=6, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=6, out_features=6, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=6, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = torch.nn.Sequential(\n",
    "            torch.nn.Linear(13, 6),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(6, 6),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(6, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnt: 0 - valLoss: 0.6800490617752075 - trainLoss: 0.6823776960372925\n",
      "cnt: 0 - valLoss: 0.6799888014793396 - trainLoss: 0.6823043823242188\n",
      "cnt: 0 - valLoss: 0.6799285411834717 - trainLoss: 0.6822311878204346\n",
      "cnt: 0 - valLoss: 0.6798684597015381 - trainLoss: 0.6821579933166504\n",
      "cnt: 0 - valLoss: 0.6798084378242493 - trainLoss: 0.6820847988128662\n",
      "cnt: 0 - valLoss: 0.6797484755516052 - trainLoss: 0.6820117831230164\n",
      "cnt: 0 - valLoss: 0.6796885132789612 - trainLoss: 0.6819389462471008\n",
      "cnt: 0 - valLoss: 0.6796286702156067 - trainLoss: 0.6818661093711853\n",
      "cnt: 0 - valLoss: 0.679568886756897 - trainLoss: 0.6817933320999146\n",
      "cnt: 0 - valLoss: 0.6795092225074768 - trainLoss: 0.6817207336425781\n",
      "cnt: 0 - valLoss: 0.6794496774673462 - trainLoss: 0.6816481351852417\n",
      "cnt: 0 - valLoss: 0.6793901920318604 - trainLoss: 0.6815757751464844\n",
      "cnt: 0 - valLoss: 0.6793308854103088 - trainLoss: 0.681503415107727\n",
      "cnt: 0 - valLoss: 0.6792716383934021 - trainLoss: 0.6814311742782593\n",
      "cnt: 0 - valLoss: 0.6792125105857849 - trainLoss: 0.6813589334487915\n",
      "cnt: 0 - valLoss: 0.6791534423828125 - trainLoss: 0.6812868118286133\n",
      "cnt: 0 - valLoss: 0.6790944933891296 - trainLoss: 0.6812148094177246\n",
      "cnt: 0 - valLoss: 0.6790356040000916 - trainLoss: 0.6811429858207703\n",
      "cnt: 0 - valLoss: 0.678976833820343 - trainLoss: 0.6810712218284607\n",
      "cnt: 0 - valLoss: 0.678918182849884 - trainLoss: 0.6809995174407959\n",
      "cnt: 0 - valLoss: 0.678859531879425 - trainLoss: 0.6809280514717102\n",
      "cnt: 0 - valLoss: 0.6788010597229004 - trainLoss: 0.6808565258979797\n",
      "cnt: 0 - valLoss: 0.6787424683570862 - trainLoss: 0.6807852983474731\n",
      "cnt: 0 - valLoss: 0.6786842346191406 - trainLoss: 0.6807140707969666\n",
      "cnt: 0 - valLoss: 0.6786259412765503 - trainLoss: 0.6806430220603943\n",
      "cnt: 0 - valLoss: 0.6785677671432495 - trainLoss: 0.6805721521377563\n",
      "cnt: 0 - valLoss: 0.6785097122192383 - trainLoss: 0.6805012226104736\n",
      "cnt: 0 - valLoss: 0.6784515976905823 - trainLoss: 0.6804304718971252\n",
      "cnt: 0 - valLoss: 0.6783936023712158 - trainLoss: 0.6803597807884216\n",
      "cnt: 0 - valLoss: 0.6783355474472046 - trainLoss: 0.680289089679718\n",
      "cnt: 0 - valLoss: 0.6782776117324829 - trainLoss: 0.680218517780304\n",
      "cnt: 0 - valLoss: 0.6782196760177612 - trainLoss: 0.6801480650901794\n",
      "cnt: 0 - valLoss: 0.6781617403030396 - trainLoss: 0.6800776124000549\n",
      "cnt: 0 - valLoss: 0.6781038641929626 - trainLoss: 0.68000727891922\n",
      "cnt: 0 - valLoss: 0.6780461668968201 - trainLoss: 0.6799370050430298\n",
      "cnt: 0 - valLoss: 0.677988588809967 - trainLoss: 0.6798669099807739\n",
      "cnt: 0 - valLoss: 0.677931010723114 - trainLoss: 0.6797968745231628\n",
      "cnt: 0 - valLoss: 0.6778735518455505 - trainLoss: 0.6797269582748413\n",
      "cnt: 0 - valLoss: 0.6778160929679871 - trainLoss: 0.679656982421875\n",
      "cnt: 0 - valLoss: 0.6777587532997131 - trainLoss: 0.6795870661735535\n",
      "cnt: 0 - valLoss: 0.677701473236084 - trainLoss: 0.6795172095298767\n",
      "cnt: 0 - valLoss: 0.6776442527770996 - trainLoss: 0.6794474720954895\n",
      "cnt: 0 - valLoss: 0.6775871515274048 - trainLoss: 0.6793777942657471\n",
      "cnt: 0 - valLoss: 0.6775299906730652 - trainLoss: 0.6793082356452942\n",
      "cnt: 0 - valLoss: 0.6774730086326599 - trainLoss: 0.6792387366294861\n",
      "cnt: 0 - valLoss: 0.6774159669876099 - trainLoss: 0.6791693568229675\n",
      "cnt: 0 - valLoss: 0.677358865737915 - trainLoss: 0.679099977016449\n",
      "cnt: 0 - valLoss: 0.677301824092865 - trainLoss: 0.6790306568145752\n",
      "cnt: 0 - valLoss: 0.6772447824478149 - trainLoss: 0.6789613962173462\n",
      "cnt: 0 - valLoss: 0.6771878600120544 - trainLoss: 0.6788923144340515\n",
      "cnt: 0 - valLoss: 0.6771311163902283 - trainLoss: 0.6788232922554016\n",
      "cnt: 0 - valLoss: 0.6770743131637573 - trainLoss: 0.6787543892860413\n",
      "cnt: 0 - valLoss: 0.6770175695419312 - trainLoss: 0.6786854863166809\n",
      "cnt: 0 - valLoss: 0.6769602298736572 - trainLoss: 0.6786165237426758\n",
      "cnt: 0 - valLoss: 0.6769029498100281 - trainLoss: 0.6785467267036438\n",
      "cnt: 0 - valLoss: 0.6768456101417542 - trainLoss: 0.6784771084785461\n",
      "cnt: 0 - valLoss: 0.6767883896827698 - trainLoss: 0.6784073710441589\n",
      "cnt: 0 - valLoss: 0.676731288433075 - trainLoss: 0.6783376932144165\n",
      "cnt: 0 - valLoss: 0.6766742467880249 - trainLoss: 0.6782681345939636\n",
      "cnt: 0 - valLoss: 0.6766173243522644 - trainLoss: 0.6781986951828003\n",
      "cnt: 0 - valLoss: 0.6765602827072144 - trainLoss: 0.678129255771637\n",
      "cnt: 0 - valLoss: 0.6765032410621643 - trainLoss: 0.6780597567558289\n",
      "cnt: 0 - valLoss: 0.6764463782310486 - trainLoss: 0.6779903769493103\n",
      "cnt: 0 - valLoss: 0.6763895750045776 - trainLoss: 0.6779211163520813\n",
      "cnt: 0 - valLoss: 0.676332950592041 - trainLoss: 0.6778519153594971\n",
      "cnt: 0 - valLoss: 0.6762763261795044 - trainLoss: 0.6777828335762024\n",
      "cnt: 0 - valLoss: 0.6762198209762573 - trainLoss: 0.677713930606842\n",
      "cnt: 0 - valLoss: 0.676163375377655 - trainLoss: 0.6776450276374817\n",
      "cnt: 0 - valLoss: 0.6761069893836975 - trainLoss: 0.6775761842727661\n",
      "cnt: 0 - valLoss: 0.6760504841804504 - trainLoss: 0.6775074601173401\n",
      "cnt: 0 - valLoss: 0.6759940385818481 - trainLoss: 0.6774386167526245\n",
      "cnt: 0 - valLoss: 0.6759376525878906 - trainLoss: 0.6773698925971985\n",
      "cnt: 0 - valLoss: 0.6758813261985779 - trainLoss: 0.6773011684417725\n",
      "cnt: 0 - valLoss: 0.6758249402046204 - trainLoss: 0.677232563495636\n",
      "cnt: 0 - valLoss: 0.6757686734199524 - trainLoss: 0.6771640181541443\n",
      "cnt: 0 - valLoss: 0.6757124066352844 - trainLoss: 0.6770955324172974\n",
      "cnt: 0 - valLoss: 0.6756561994552612 - trainLoss: 0.6770271062850952\n",
      "cnt: 0 - valLoss: 0.6756000518798828 - trainLoss: 0.6769587993621826\n",
      "cnt: 0 - valLoss: 0.6755439043045044 - trainLoss: 0.6768906116485596\n",
      "cnt: 0 - valLoss: 0.6754878163337708 - trainLoss: 0.6768224239349365\n",
      "cnt: 0 - valLoss: 0.6754317283630371 - trainLoss: 0.6767544150352478\n",
      "cnt: 0 - valLoss: 0.6753758192062378 - trainLoss: 0.6766864061355591\n",
      "cnt: 0 - valLoss: 0.6753199100494385 - trainLoss: 0.6766185164451599\n",
      "cnt: 0 - valLoss: 0.6752640008926392 - trainLoss: 0.6765507459640503\n",
      "cnt: 0 - valLoss: 0.6752082109451294 - trainLoss: 0.6764830946922302\n",
      "cnt: 0 - valLoss: 0.6751524806022644 - trainLoss: 0.6764155030250549\n",
      "cnt: 0 - valLoss: 0.6750967502593994 - trainLoss: 0.6763479113578796\n",
      "cnt: 0 - valLoss: 0.6750410199165344 - trainLoss: 0.6762803196907043\n",
      "cnt: 0 - valLoss: 0.6749852895736694 - trainLoss: 0.6762126088142395\n",
      "cnt: 0 - valLoss: 0.6749297380447388 - trainLoss: 0.6761449575424194\n",
      "cnt: 0 - valLoss: 0.6748743057250977 - trainLoss: 0.6760774254798889\n",
      "cnt: 0 - valLoss: 0.6748188138008118 - trainLoss: 0.6760099530220032\n",
      "cnt: 0 - valLoss: 0.6747633218765259 - trainLoss: 0.6759425401687622\n",
      "cnt: 0 - valLoss: 0.67470782995224 - trainLoss: 0.6758748888969421\n",
      "cnt: 0 - valLoss: 0.6746522188186646 - trainLoss: 0.6758072972297668\n",
      "cnt: 0 - valLoss: 0.6745966672897339 - trainLoss: 0.6757398247718811\n",
      "cnt: 0 - valLoss: 0.674541175365448 - trainLoss: 0.6756723523139954\n",
      "cnt: 0 - valLoss: 0.6744856834411621 - trainLoss: 0.6756049394607544\n",
      "cnt: 0 - valLoss: 0.674430251121521 - trainLoss: 0.6755375266075134\n",
      "cnt: 0 - valLoss: 0.6743748188018799 - trainLoss: 0.6754701733589172\n",
      "cnt: 0 - valLoss: 0.6743195056915283 - trainLoss: 0.675402820110321\n",
      "cnt: 0 - valLoss: 0.6742641925811768 - trainLoss: 0.6753354668617249\n",
      "cnt: 0 - valLoss: 0.6742090582847595 - trainLoss: 0.675268292427063\n",
      "cnt: 0 - valLoss: 0.6741539835929871 - trainLoss: 0.6752011775970459\n",
      "cnt: 0 - valLoss: 0.6740989089012146 - trainLoss: 0.6751341819763184\n",
      "cnt: 0 - valLoss: 0.6740438938140869 - trainLoss: 0.675067126750946\n",
      "cnt: 0 - valLoss: 0.673988938331604 - trainLoss: 0.675000011920929\n",
      "cnt: 0 - valLoss: 0.6739341020584106 - trainLoss: 0.6749330759048462\n",
      "cnt: 0 - valLoss: 0.6738793253898621 - trainLoss: 0.674866259098053\n",
      "cnt: 0 - valLoss: 0.6738246083259583 - trainLoss: 0.6747994422912598\n",
      "cnt: 0 - valLoss: 0.673770010471344 - trainLoss: 0.6747327446937561\n",
      "cnt: 0 - valLoss: 0.6737154126167297 - trainLoss: 0.6746661067008972\n",
      "cnt: 0 - valLoss: 0.6736608147621155 - trainLoss: 0.6745994687080383\n",
      "cnt: 0 - valLoss: 0.6736061573028564 - trainLoss: 0.6745328307151794\n",
      "cnt: 0 - valLoss: 0.673551619052887 - trainLoss: 0.6744663119316101\n",
      "cnt: 0 - valLoss: 0.6734971404075623 - trainLoss: 0.6743996739387512\n",
      "cnt: 0 - valLoss: 0.6734426617622375 - trainLoss: 0.6743331551551819\n",
      "cnt: 0 - valLoss: 0.6733883023262024 - trainLoss: 0.6742667555809021\n",
      "cnt: 0 - valLoss: 0.6733340620994568 - trainLoss: 0.6742004156112671\n",
      "cnt: 0 - valLoss: 0.6732798218727112 - trainLoss: 0.6741340756416321\n",
      "cnt: 0 - valLoss: 0.6732255816459656 - trainLoss: 0.6740677356719971\n",
      "cnt: 0 - valLoss: 0.67317134141922 - trainLoss: 0.6740014553070068\n",
      "cnt: 0 - valLoss: 0.6731172800064087 - trainLoss: 0.6739352941513062\n",
      "cnt: 0 - valLoss: 0.6730632185935974 - trainLoss: 0.673869252204895\n",
      "cnt: 0 - valLoss: 0.6730092167854309 - trainLoss: 0.6738032102584839\n",
      "cnt: 0 - valLoss: 0.6729552745819092 - trainLoss: 0.6737372875213623\n",
      "cnt: 0 - valLoss: 0.6729013919830322 - trainLoss: 0.6736714243888855\n",
      "cnt: 0 - valLoss: 0.6728476881980896 - trainLoss: 0.6736056208610535\n",
      "cnt: 0 - valLoss: 0.6727940440177917 - trainLoss: 0.6735400557518005\n",
      "cnt: 0 - valLoss: 0.6727403998374939 - trainLoss: 0.6734744906425476\n",
      "cnt: 0 - valLoss: 0.6726868748664856 - trainLoss: 0.6734089255332947\n",
      "cnt: 0 - valLoss: 0.6726334095001221 - trainLoss: 0.6733435392379761\n",
      "cnt: 0 - valLoss: 0.6725800037384033 - trainLoss: 0.6732781529426575\n",
      "cnt: 0 - valLoss: 0.6725267171859741 - trainLoss: 0.6732128858566284\n",
      "cnt: 0 - valLoss: 0.6724734902381897 - trainLoss: 0.6731476187705994\n",
      "cnt: 0 - valLoss: 0.67242032289505 - trainLoss: 0.6730825304985046\n",
      "cnt: 0 - valLoss: 0.6723672151565552 - trainLoss: 0.6730175018310547\n",
      "cnt: 0 - valLoss: 0.6723142266273499 - trainLoss: 0.6729525923728943\n",
      "cnt: 0 - valLoss: 0.6722612977027893 - trainLoss: 0.6728876829147339\n",
      "cnt: 0 - valLoss: 0.6722084283828735 - trainLoss: 0.6728228330612183\n",
      "cnt: 0 - valLoss: 0.6721555590629578 - trainLoss: 0.6727581024169922\n",
      "cnt: 0 - valLoss: 0.6721028685569763 - trainLoss: 0.6726934313774109\n",
      "cnt: 0 - valLoss: 0.6720502376556396 - trainLoss: 0.6726287603378296\n",
      "cnt: 0 - valLoss: 0.6719976663589478 - trainLoss: 0.6725642681121826\n",
      "cnt: 0 - valLoss: 0.6719450950622559 - trainLoss: 0.6724998354911804\n",
      "cnt: 0 - valLoss: 0.6718926429748535 - trainLoss: 0.6724353432655334\n",
      "cnt: 0 - valLoss: 0.6718403100967407 - trainLoss: 0.672370970249176\n",
      "cnt: 0 - valLoss: 0.6717880368232727 - trainLoss: 0.6723067760467529\n",
      "cnt: 0 - valLoss: 0.6717358231544495 - trainLoss: 0.6722426414489746\n",
      "cnt: 0 - valLoss: 0.6716837286949158 - trainLoss: 0.6721786260604858\n",
      "cnt: 0 - valLoss: 0.6716315746307373 - trainLoss: 0.6721146106719971\n",
      "cnt: 0 - valLoss: 0.6715795397758484 - trainLoss: 0.6720505952835083\n",
      "cnt: 0 - valLoss: 0.6715275049209595 - trainLoss: 0.6719865798950195\n",
      "cnt: 0 - valLoss: 0.6714755892753601 - trainLoss: 0.6719226837158203\n",
      "cnt: 0 - valLoss: 0.6714237928390503 - trainLoss: 0.6718588471412659\n",
      "cnt: 0 - valLoss: 0.6713719964027405 - trainLoss: 0.6717950701713562\n",
      "cnt: 0 - valLoss: 0.671320378780365 - trainLoss: 0.6717314720153809\n",
      "cnt: 0 - valLoss: 0.6712687611579895 - trainLoss: 0.6716678738594055\n",
      "cnt: 0 - valLoss: 0.6712172627449036 - trainLoss: 0.6716043949127197\n",
      "cnt: 0 - valLoss: 0.6711658239364624 - trainLoss: 0.6715410351753235\n",
      "cnt: 0 - valLoss: 0.6711143851280212 - trainLoss: 0.671477735042572\n",
      "cnt: 0 - valLoss: 0.6710631251335144 - trainLoss: 0.6714144349098206\n",
      "cnt: 0 - valLoss: 0.6710118651390076 - trainLoss: 0.6713512539863586\n",
      "cnt: 0 - valLoss: 0.6709607243537903 - trainLoss: 0.6712881326675415\n",
      "cnt: 0 - valLoss: 0.670909583568573 - trainLoss: 0.6712250709533691\n",
      "cnt: 0 - valLoss: 0.6708585619926453 - trainLoss: 0.6711622476577759\n",
      "cnt: 0 - valLoss: 0.6708077192306519 - trainLoss: 0.6710993647575378\n",
      "cnt: 0 - valLoss: 0.6707568168640137 - trainLoss: 0.6710366606712341\n",
      "cnt: 0 - valLoss: 0.670706033706665 - trainLoss: 0.6709740161895752\n",
      "cnt: 0 - valLoss: 0.670655369758606 - trainLoss: 0.670911431312561\n",
      "cnt: 0 - valLoss: 0.6706047654151917 - trainLoss: 0.6708489656448364\n",
      "cnt: 0 - valLoss: 0.6705541610717773 - trainLoss: 0.6707866191864014\n",
      "cnt: 0 - valLoss: 0.6705036759376526 - trainLoss: 0.6707242131233215\n",
      "cnt: 0 - valLoss: 0.6704533696174622 - trainLoss: 0.6706621050834656\n",
      "cnt: 0 - valLoss: 0.6704030632972717 - trainLoss: 0.6705999970436096\n",
      "cnt: 0 - valLoss: 0.6703528761863708 - trainLoss: 0.6705379486083984\n",
      "cnt: 0 - valLoss: 0.67030268907547 - trainLoss: 0.6704761385917664\n",
      "cnt: 0 - valLoss: 0.6702526807785034 - trainLoss: 0.6704143285751343\n",
      "cnt: 0 - valLoss: 0.6702027320861816 - trainLoss: 0.6703526973724365\n",
      "cnt: 0 - valLoss: 0.6701528429985046 - trainLoss: 0.6702911853790283\n",
      "cnt: 0 - valLoss: 0.6701030135154724 - trainLoss: 0.6702297329902649\n",
      "cnt: 0 - valLoss: 0.670053243637085 - trainLoss: 0.6701682209968567\n",
      "cnt: 0 - valLoss: 0.6700035333633423 - trainLoss: 0.6701068878173828\n",
      "cnt: 0 - valLoss: 0.6699540019035339 - trainLoss: 0.6700455546379089\n",
      "cnt: 0 - valLoss: 0.6699044108390808 - trainLoss: 0.6699844598770142\n",
      "cnt: 0 - valLoss: 0.6698549389839172 - trainLoss: 0.6699233055114746\n",
      "cnt: 0 - valLoss: 0.6698055863380432 - trainLoss: 0.6698623299598694\n",
      "cnt: 0 - valLoss: 0.6697562336921692 - trainLoss: 0.6698014140129089\n",
      "cnt: 0 - valLoss: 0.6697068214416504 - trainLoss: 0.6697405576705933\n",
      "cnt: 0 - valLoss: 0.6696575284004211 - trainLoss: 0.6696797609329224\n",
      "cnt: 0 - valLoss: 0.6696082353591919 - trainLoss: 0.6696192026138306\n",
      "cnt: 0 - valLoss: 0.669559121131897 - trainLoss: 0.669558584690094\n",
      "cnt: 0 - valLoss: 0.6695099472999573 - trainLoss: 0.6694980263710022\n",
      "cnt: 0 - valLoss: 0.6694608926773071 - trainLoss: 0.6694376468658447\n",
      "cnt: 0 - valLoss: 0.6694120168685913 - trainLoss: 0.669377326965332\n",
      "cnt: 0 - valLoss: 0.6693630814552307 - trainLoss: 0.6693170666694641\n",
      "cnt: 0 - valLoss: 0.6693142652511597 - trainLoss: 0.669256865978241\n",
      "cnt: 0 - valLoss: 0.6692655086517334 - trainLoss: 0.6691967248916626\n",
      "cnt: 0 - valLoss: 0.6692168116569519 - trainLoss: 0.669136643409729\n",
      "cnt: 0 - valLoss: 0.6691681742668152 - trainLoss: 0.669076681137085\n",
      "cnt: 0 - valLoss: 0.669119656085968 - trainLoss: 0.6690168380737305\n",
      "cnt: 0 - valLoss: 0.6690711975097656 - trainLoss: 0.668956995010376\n",
      "cnt: 0 - valLoss: 0.6690227389335632 - trainLoss: 0.6688973903656006\n",
      "cnt: 0 - valLoss: 0.6689744591712952 - trainLoss: 0.6688377261161804\n",
      "cnt: 0 - valLoss: 0.6689261794090271 - trainLoss: 0.6687782406806946\n",
      "cnt: 0 - valLoss: 0.6688780188560486 - trainLoss: 0.6687188744544983\n",
      "cnt: 0 - valLoss: 0.6688300371170044 - trainLoss: 0.668659508228302\n",
      "cnt: 0 - valLoss: 0.6687819361686707 - trainLoss: 0.6686002612113953\n",
      "cnt: 0 - valLoss: 0.6687340140342712 - trainLoss: 0.6685411930084229\n",
      "cnt: 0 - valLoss: 0.6686861515045166 - trainLoss: 0.6684820652008057\n",
      "cnt: 0 - valLoss: 0.6686384081840515 - trainLoss: 0.6684231162071228\n",
      "cnt: 0 - valLoss: 0.6685906052589417 - trainLoss: 0.6683642268180847\n",
      "cnt: 0 - valLoss: 0.6685429215431213 - trainLoss: 0.6683054566383362\n",
      "cnt: 0 - valLoss: 0.668495237827301 - trainLoss: 0.6682466268539429\n",
      "cnt: 0 - valLoss: 0.6684477925300598 - trainLoss: 0.6681879162788391\n",
      "cnt: 0 - valLoss: 0.6684003472328186 - trainLoss: 0.6681293845176697\n",
      "cnt: 0 - valLoss: 0.6683529615402222 - trainLoss: 0.6680708527565002\n",
      "cnt: 0 - valLoss: 0.6683057546615601 - trainLoss: 0.6680125594139099\n",
      "cnt: 0 - valLoss: 0.6682586669921875 - trainLoss: 0.6679542660713196\n",
      "cnt: 0 - valLoss: 0.6682115197181702 - trainLoss: 0.6678960919380188\n",
      "cnt: 0 - valLoss: 0.6681645512580872 - trainLoss: 0.6678379774093628\n",
      "cnt: 0 - valLoss: 0.6681176424026489 - trainLoss: 0.6677799820899963\n",
      "cnt: 0 - valLoss: 0.6680708527565002 - trainLoss: 0.6677220463752747\n",
      "cnt: 0 - valLoss: 0.6680240631103516 - trainLoss: 0.6676642298698425\n",
      "cnt: 0 - valLoss: 0.6679773330688477 - trainLoss: 0.6676065921783447\n",
      "cnt: 0 - valLoss: 0.667930543422699 - trainLoss: 0.6675488948822021\n",
      "cnt: 0 - valLoss: 0.6678838133811951 - trainLoss: 0.6674913763999939\n",
      "cnt: 0 - valLoss: 0.6678371429443359 - trainLoss: 0.6674339175224304\n",
      "cnt: 0 - valLoss: 0.6677905917167664 - trainLoss: 0.6673765182495117\n",
      "cnt: 0 - valLoss: 0.6677441000938416 - trainLoss: 0.6673192381858826\n",
      "cnt: 0 - valLoss: 0.6676976680755615 - trainLoss: 0.667262077331543\n",
      "cnt: 0 - valLoss: 0.6676512956619263 - trainLoss: 0.6672047972679138\n",
      "cnt: 0 - valLoss: 0.6676050424575806 - trainLoss: 0.6671478152275085\n",
      "cnt: 0 - valLoss: 0.6675589084625244 - trainLoss: 0.6670907735824585\n",
      "cnt: 0 - valLoss: 0.6675127744674683 - trainLoss: 0.6670337915420532\n",
      "cnt: 0 - valLoss: 0.6674668192863464 - trainLoss: 0.6669769287109375\n",
      "cnt: 0 - valLoss: 0.6674207448959351 - trainLoss: 0.6669201254844666\n",
      "cnt: 0 - valLoss: 0.6673747301101685 - trainLoss: 0.6668634414672852\n",
      "cnt: 0 - valLoss: 0.6673288345336914 - trainLoss: 0.6668067574501038\n",
      "cnt: 0 - valLoss: 0.6672829389572144 - trainLoss: 0.6667501330375671\n",
      "cnt: 0 - valLoss: 0.6672371625900269 - trainLoss: 0.6666936278343201\n",
      "cnt: 0 - valLoss: 0.6671913862228394 - trainLoss: 0.6666373014450073\n",
      "cnt: 0 - valLoss: 0.6671457290649414 - trainLoss: 0.6665809154510498\n",
      "cnt: 0 - valLoss: 0.6671001315116882 - trainLoss: 0.6665246486663818\n",
      "cnt: 0 - valLoss: 0.6670546531677246 - trainLoss: 0.6664685010910034\n",
      "cnt: 0 - valLoss: 0.667009174823761 - trainLoss: 0.6664124131202698\n",
      "cnt: 0 - valLoss: 0.6669638156890869 - trainLoss: 0.6663563847541809\n",
      "cnt: 0 - valLoss: 0.6669185161590576 - trainLoss: 0.6663005352020264\n",
      "cnt: 0 - valLoss: 0.6668732166290283 - trainLoss: 0.6662446856498718\n",
      "cnt: 0 - valLoss: 0.6668280363082886 - trainLoss: 0.6661888957023621\n",
      "cnt: 0 - valLoss: 0.6667829155921936 - trainLoss: 0.6661332249641418\n",
      "cnt: 0 - valLoss: 0.6667379140853882 - trainLoss: 0.6660776734352112\n",
      "cnt: 0 - valLoss: 0.6666929125785828 - trainLoss: 0.6660221815109253\n",
      "cnt: 0 - valLoss: 0.6666480302810669 - trainLoss: 0.665966808795929\n",
      "cnt: 0 - valLoss: 0.666603147983551 - trainLoss: 0.6659114360809326\n",
      "cnt: 0 - valLoss: 0.6665583252906799 - trainLoss: 0.665856122970581\n",
      "cnt: 0 - valLoss: 0.6665136814117432 - trainLoss: 0.6658009886741638\n",
      "cnt: 0 - valLoss: 0.6664690375328064 - trainLoss: 0.6657459139823914\n",
      "cnt: 0 - valLoss: 0.6664243340492249 - trainLoss: 0.6656907796859741\n",
      "cnt: 0 - valLoss: 0.6663798689842224 - trainLoss: 0.6656357049942017\n",
      "cnt: 0 - valLoss: 0.6663353443145752 - trainLoss: 0.6655807495117188\n",
      "cnt: 0 - valLoss: 0.6662908792495728 - trainLoss: 0.6655257940292358\n",
      "cnt: 0 - valLoss: 0.6662464737892151 - trainLoss: 0.6654709577560425\n",
      "cnt: 0 - valLoss: 0.666202187538147 - trainLoss: 0.6654161214828491\n",
      "cnt: 0 - valLoss: 0.6661579012870789 - trainLoss: 0.6653614640235901\n",
      "cnt: 0 - valLoss: 0.6661136746406555 - trainLoss: 0.665306806564331\n",
      "cnt: 0 - valLoss: 0.6660696268081665 - trainLoss: 0.6652523279190063\n",
      "cnt: 0 - valLoss: 0.6660256385803223 - trainLoss: 0.6651977896690369\n",
      "cnt: 0 - valLoss: 0.6659817695617676 - trainLoss: 0.6651434302330017\n",
      "cnt: 0 - valLoss: 0.6659379601478577 - trainLoss: 0.6650892496109009\n",
      "cnt: 0 - valLoss: 0.6658941507339478 - trainLoss: 0.6650351285934448\n",
      "cnt: 0 - valLoss: 0.6658504605293274 - trainLoss: 0.6649810075759888\n",
      "cnt: 0 - valLoss: 0.665806770324707 - trainLoss: 0.6649269461631775\n",
      "cnt: 0 - valLoss: 0.6657631397247314 - trainLoss: 0.6648728847503662\n",
      "cnt: 0 - valLoss: 0.6657196283340454 - trainLoss: 0.6648190021514893\n",
      "cnt: 0 - valLoss: 0.6656761765480042 - trainLoss: 0.6647651791572571\n",
      "cnt: 0 - valLoss: 0.6656327843666077 - trainLoss: 0.6647113561630249\n",
      "cnt: 0 - valLoss: 0.665589451789856 - trainLoss: 0.6646576523780823\n",
      "cnt: 0 - valLoss: 0.665546178817749 - trainLoss: 0.6646040678024292\n",
      "cnt: 0 - valLoss: 0.6655029654502869 - trainLoss: 0.6645505428314209\n",
      "cnt: 0 - valLoss: 0.6654598116874695 - trainLoss: 0.6644970774650574\n",
      "cnt: 0 - valLoss: 0.6654167771339417 - trainLoss: 0.6644436120986938\n",
      "cnt: 0 - valLoss: 0.665373682975769 - trainLoss: 0.6643903255462646\n",
      "cnt: 0 - valLoss: 0.6653307676315308 - trainLoss: 0.6643370985984802\n",
      "cnt: 0 - valLoss: 0.6652878522872925 - trainLoss: 0.6642839312553406\n",
      "cnt: 0 - valLoss: 0.665244996547699 - trainLoss: 0.6642308831214905\n",
      "cnt: 0 - valLoss: 0.665202260017395 - trainLoss: 0.6641778945922852\n",
      "cnt: 0 - valLoss: 0.6651595234870911 - trainLoss: 0.6641249656677246\n",
      "cnt: 0 - valLoss: 0.6651168465614319 - trainLoss: 0.6640719771385193\n",
      "cnt: 0 - valLoss: 0.6650742292404175 - trainLoss: 0.6640191674232483\n",
      "cnt: 0 - valLoss: 0.6650316715240479 - trainLoss: 0.6639663577079773\n",
      "cnt: 0 - valLoss: 0.6649892330169678 - trainLoss: 0.6639136075973511\n",
      "cnt: 0 - valLoss: 0.6649467349052429 - trainLoss: 0.663861095905304\n",
      "cnt: 0 - valLoss: 0.6649044156074524 - trainLoss: 0.6638084650039673\n",
      "cnt: 0 - valLoss: 0.6648620367050171 - trainLoss: 0.6637558937072754\n",
      "cnt: 0 - valLoss: 0.6648197770118713 - trainLoss: 0.6637033820152283\n",
      "cnt: 0 - valLoss: 0.6647775769233704 - trainLoss: 0.6636509895324707\n",
      "cnt: 0 - valLoss: 0.6647354960441589 - trainLoss: 0.6635985970497131\n",
      "cnt: 0 - valLoss: 0.6646934151649475 - trainLoss: 0.6635463833808899\n",
      "cnt: 0 - valLoss: 0.6646515130996704 - trainLoss: 0.6634941697120667\n",
      "cnt: 0 - valLoss: 0.6646096110343933 - trainLoss: 0.6634420156478882\n",
      "cnt: 0 - valLoss: 0.664567768573761 - trainLoss: 0.6633899807929993\n",
      "cnt: 0 - valLoss: 0.6645259261131287 - trainLoss: 0.6633379459381104\n",
      "cnt: 0 - valLoss: 0.6644842028617859 - trainLoss: 0.6632860898971558\n",
      "cnt: 0 - valLoss: 0.6644425988197327 - trainLoss: 0.6632342338562012\n",
      "cnt: 0 - valLoss: 0.6644009947776794 - trainLoss: 0.6631825566291809\n",
      "cnt: 0 - valLoss: 0.664359450340271 - trainLoss: 0.6631309390068054\n",
      "cnt: 0 - valLoss: 0.6643179655075073 - trainLoss: 0.6630793809890747\n",
      "cnt: 0 - valLoss: 0.6642764806747437 - trainLoss: 0.6630278825759888\n",
      "cnt: 0 - valLoss: 0.6642351746559143 - trainLoss: 0.6629763841629028\n",
      "cnt: 0 - valLoss: 0.6641939282417297 - trainLoss: 0.6629250049591064\n",
      "cnt: 0 - valLoss: 0.6641526222229004 - trainLoss: 0.6628737449645996\n",
      "cnt: 0 - valLoss: 0.6641114354133606 - trainLoss: 0.6628224849700928\n",
      "cnt: 0 - valLoss: 0.6640703082084656 - trainLoss: 0.6627713441848755\n",
      "cnt: 0 - valLoss: 0.6640292406082153 - trainLoss: 0.6627202033996582\n",
      "cnt: 0 - valLoss: 0.6639881730079651 - trainLoss: 0.6626691222190857\n",
      "cnt: 0 - valLoss: 0.6639471650123596 - trainLoss: 0.662618100643158\n",
      "cnt: 0 - valLoss: 0.6639062166213989 - trainLoss: 0.6625670790672302\n",
      "cnt: 0 - valLoss: 0.6638653874397278 - trainLoss: 0.662516176700592\n",
      "cnt: 0 - valLoss: 0.6638246774673462 - trainLoss: 0.6624653339385986\n",
      "cnt: 0 - valLoss: 0.6637839674949646 - trainLoss: 0.6624146103858948\n",
      "cnt: 0 - valLoss: 0.6637433767318726 - trainLoss: 0.6623639464378357\n",
      "cnt: 0 - valLoss: 0.6637027859687805 - trainLoss: 0.6623133420944214\n",
      "cnt: 0 - valLoss: 0.663662314414978 - trainLoss: 0.6622628569602966\n",
      "cnt: 0 - valLoss: 0.6636217832565308 - trainLoss: 0.6622124314308167\n",
      "cnt: 0 - valLoss: 0.6635814905166626 - trainLoss: 0.6621620059013367\n",
      "cnt: 0 - valLoss: 0.6635411381721497 - trainLoss: 0.662111759185791\n",
      "cnt: 0 - valLoss: 0.6635009050369263 - trainLoss: 0.6620616316795349\n",
      "cnt: 0 - valLoss: 0.6634607315063477 - trainLoss: 0.6620115041732788\n",
      "cnt: 0 - valLoss: 0.6634204983711243 - trainLoss: 0.6619614362716675\n",
      "cnt: 0 - valLoss: 0.6633803844451904 - trainLoss: 0.6619114875793457\n",
      "cnt: 0 - valLoss: 0.6633403301239014 - trainLoss: 0.6618614196777344\n",
      "cnt: 0 - valLoss: 0.6633002758026123 - trainLoss: 0.6618115901947021\n",
      "cnt: 0 - valLoss: 0.6632604002952576 - trainLoss: 0.6617617607116699\n",
      "cnt: 0 - valLoss: 0.6632205843925476 - trainLoss: 0.6617119312286377\n",
      "cnt: 0 - valLoss: 0.6631808280944824 - trainLoss: 0.6616621613502502\n",
      "cnt: 0 - valLoss: 0.6631410717964172 - trainLoss: 0.6616126298904419\n",
      "cnt: 0 - valLoss: 0.6631014347076416 - trainLoss: 0.661562979221344\n",
      "cnt: 0 - valLoss: 0.6630617380142212 - trainLoss: 0.6615133881568909\n",
      "cnt: 0 - valLoss: 0.6630221605300903 - trainLoss: 0.6614639163017273\n",
      "cnt: 0 - valLoss: 0.6629826426506042 - trainLoss: 0.6614145040512085\n",
      "cnt: 0 - valLoss: 0.6629431843757629 - trainLoss: 0.6613651514053345\n",
      "cnt: 0 - valLoss: 0.6629037857055664 - trainLoss: 0.6613158583641052\n",
      "cnt: 0 - valLoss: 0.6628644466400146 - trainLoss: 0.6612666249275208\n",
      "cnt: 0 - valLoss: 0.6628249883651733 - trainLoss: 0.6612173914909363\n",
      "cnt: 0 - valLoss: 0.6627857089042664 - trainLoss: 0.6611682772636414\n",
      "cnt: 0 - valLoss: 0.6627463698387146 - trainLoss: 0.6611191034317017\n",
      "cnt: 0 - valLoss: 0.6627071499824524 - trainLoss: 0.6610700488090515\n",
      "cnt: 0 - valLoss: 0.6626679301261902 - trainLoss: 0.6610211133956909\n",
      "cnt: 0 - valLoss: 0.6626287698745728 - trainLoss: 0.6609722375869751\n",
      "cnt: 0 - valLoss: 0.6625896692276001 - trainLoss: 0.6609233617782593\n",
      "cnt: 0 - valLoss: 0.6625505685806274 - trainLoss: 0.660874605178833\n",
      "cnt: 0 - valLoss: 0.6625115275382996 - trainLoss: 0.660825788974762\n",
      "cnt: 0 - valLoss: 0.6624723672866821 - trainLoss: 0.6607770323753357\n",
      "cnt: 0 - valLoss: 0.6624333262443542 - trainLoss: 0.6607282757759094\n",
      "cnt: 0 - valLoss: 0.6623942852020264 - trainLoss: 0.6606796979904175\n",
      "cnt: 0 - valLoss: 0.6623553037643433 - trainLoss: 0.6606312394142151\n",
      "cnt: 0 - valLoss: 0.6623162627220154 - trainLoss: 0.6605826616287231\n",
      "cnt: 0 - valLoss: 0.6622772216796875 - trainLoss: 0.6605340838432312\n",
      "cnt: 0 - valLoss: 0.6622382402420044 - trainLoss: 0.660485565662384\n",
      "cnt: 0 - valLoss: 0.662199079990387 - trainLoss: 0.6604371666908264\n",
      "cnt: 0 - valLoss: 0.6621599793434143 - trainLoss: 0.6603885889053345\n",
      "cnt: 0 - valLoss: 0.6621208786964417 - trainLoss: 0.6603401303291321\n",
      "cnt: 0 - valLoss: 0.6620818376541138 - trainLoss: 0.6602916717529297\n",
      "cnt: 0 - valLoss: 0.6620427966117859 - trainLoss: 0.6602432131767273\n",
      "cnt: 0 - valLoss: 0.662003755569458 - trainLoss: 0.6601945757865906\n",
      "cnt: 0 - valLoss: 0.6619646549224854 - trainLoss: 0.6601459980010986\n",
      "cnt: 0 - valLoss: 0.6619255542755127 - trainLoss: 0.6600975394248962\n",
      "cnt: 0 - valLoss: 0.6618863940238953 - trainLoss: 0.6600489616394043\n",
      "cnt: 0 - valLoss: 0.6618472337722778 - trainLoss: 0.6600005626678467\n",
      "cnt: 0 - valLoss: 0.6618081331253052 - trainLoss: 0.6599521636962891\n",
      "cnt: 0 - valLoss: 0.6617690324783325 - trainLoss: 0.6599038243293762\n",
      "cnt: 0 - valLoss: 0.6617296934127808 - trainLoss: 0.6598555445671082\n",
      "cnt: 0 - valLoss: 0.6616904139518738 - trainLoss: 0.6598072052001953\n",
      "cnt: 0 - valLoss: 0.6616513133049011 - trainLoss: 0.6597588658332825\n",
      "cnt: 0 - valLoss: 0.6616122722625732 - trainLoss: 0.6597107648849487\n",
      "cnt: 0 - valLoss: 0.6615732312202454 - trainLoss: 0.6596626043319702\n",
      "cnt: 0 - valLoss: 0.661534309387207 - trainLoss: 0.6596145033836365\n",
      "cnt: 0 - valLoss: 0.6614953279495239 - trainLoss: 0.6595664620399475\n",
      "cnt: 0 - valLoss: 0.6614564061164856 - trainLoss: 0.6595184206962585\n",
      "cnt: 0 - valLoss: 0.6614174842834473 - trainLoss: 0.6594703793525696\n",
      "cnt: 0 - valLoss: 0.6613786220550537 - trainLoss: 0.6594222784042358\n",
      "cnt: 0 - valLoss: 0.6613397002220154 - trainLoss: 0.6593741774559021\n",
      "cnt: 0 - valLoss: 0.6613008379936218 - trainLoss: 0.6593259572982788\n",
      "cnt: 0 - valLoss: 0.6612619757652283 - trainLoss: 0.6592778563499451\n",
      "cnt: 0 - valLoss: 0.6612231135368347 - trainLoss: 0.6592296957969666\n",
      "cnt: 0 - valLoss: 0.6611843109130859 - trainLoss: 0.6591815948486328\n",
      "cnt: 0 - valLoss: 0.6611455678939819 - trainLoss: 0.6591334342956543\n",
      "cnt: 0 - valLoss: 0.6611068248748779 - trainLoss: 0.6590852737426758\n",
      "cnt: 0 - valLoss: 0.6610680818557739 - trainLoss: 0.6590371131896973\n",
      "cnt: 0 - valLoss: 0.6610293388366699 - trainLoss: 0.6589890718460083\n",
      "cnt: 0 - valLoss: 0.6609905362129211 - trainLoss: 0.6589409112930298\n",
      "cnt: 0 - valLoss: 0.6609517931938171 - trainLoss: 0.6588929891586304\n",
      "cnt: 0 - valLoss: 0.6609131097793579 - trainLoss: 0.658845067024231\n",
      "cnt: 0 - valLoss: 0.6608744263648987 - trainLoss: 0.6587971448898315\n",
      "cnt: 0 - valLoss: 0.660835862159729 - trainLoss: 0.6587493419647217\n",
      "cnt: 0 - valLoss: 0.6607973575592041 - trainLoss: 0.6587015986442566\n",
      "cnt: 0 - valLoss: 0.6607588529586792 - trainLoss: 0.6586538553237915\n",
      "cnt: 0 - valLoss: 0.6607204079627991 - trainLoss: 0.658606231212616\n",
      "cnt: 0 - valLoss: 0.660681962966919 - trainLoss: 0.6585586071014404\n",
      "cnt: 0 - valLoss: 0.6606438159942627 - trainLoss: 0.6585111021995544\n",
      "cnt: 0 - valLoss: 0.6606057286262512 - trainLoss: 0.6584635972976685\n",
      "cnt: 0 - valLoss: 0.6605677008628845 - trainLoss: 0.6584160923957825\n",
      "cnt: 0 - valLoss: 0.6605297327041626 - trainLoss: 0.658368706703186\n",
      "cnt: 0 - valLoss: 0.6604918837547302 - trainLoss: 0.6583211421966553\n",
      "cnt: 0 - valLoss: 0.6604539752006531 - trainLoss: 0.6582737565040588\n",
      "cnt: 0 - valLoss: 0.6604161262512207 - trainLoss: 0.6582263112068176\n",
      "cnt: 0 - valLoss: 0.6603783369064331 - trainLoss: 0.6581789255142212\n",
      "cnt: 0 - valLoss: 0.6603406071662903 - trainLoss: 0.6581315398216248\n",
      "cnt: 0 - valLoss: 0.6603029370307922 - trainLoss: 0.6580841541290283\n",
      "cnt: 0 - valLoss: 0.6602652668952942 - trainLoss: 0.6580368876457214\n",
      "cnt: 0 - valLoss: 0.6602277159690857 - trainLoss: 0.6579897403717041\n",
      "cnt: 0 - valLoss: 0.660190224647522 - trainLoss: 0.657942533493042\n",
      "cnt: 0 - valLoss: 0.6601527333259583 - trainLoss: 0.6578954458236694\n",
      "cnt: 0 - valLoss: 0.6601152420043945 - trainLoss: 0.6578483581542969\n",
      "cnt: 0 - valLoss: 0.6600778102874756 - trainLoss: 0.6578013896942139\n",
      "cnt: 0 - valLoss: 0.6600404381752014 - trainLoss: 0.6577543020248413\n",
      "cnt: 0 - valLoss: 0.660003125667572 - trainLoss: 0.6577073931694031\n",
      "cnt: 0 - valLoss: 0.6599658727645874 - trainLoss: 0.6576604843139648\n",
      "cnt: 0 - valLoss: 0.659928560256958 - trainLoss: 0.6576136350631714\n",
      "cnt: 0 - valLoss: 0.6598912477493286 - trainLoss: 0.6575667262077332\n",
      "cnt: 0 - valLoss: 0.6598538756370544 - trainLoss: 0.6575194597244263\n",
      "cnt: 0 - valLoss: 0.6598166227340698 - trainLoss: 0.6574723124504089\n",
      "cnt: 0 - valLoss: 0.6597793698310852 - trainLoss: 0.6574252247810364\n",
      "cnt: 0 - valLoss: 0.6597421765327454 - trainLoss: 0.6573781371116638\n",
      "cnt: 0 - valLoss: 0.6597051024436951 - trainLoss: 0.6573311686515808\n",
      "cnt: 0 - valLoss: 0.65966796875 - trainLoss: 0.6572842001914978\n",
      "cnt: 0 - valLoss: 0.6596309542655945 - trainLoss: 0.6572373509407043\n",
      "cnt: 0 - valLoss: 0.6595939993858337 - trainLoss: 0.6571905016899109\n",
      "cnt: 0 - valLoss: 0.659557044506073 - trainLoss: 0.6571438908576965\n",
      "cnt: 0 - valLoss: 0.6595202684402466 - trainLoss: 0.6570972204208374\n",
      "cnt: 0 - valLoss: 0.6594835519790649 - trainLoss: 0.6570506691932678\n",
      "cnt: 0 - valLoss: 0.6594468951225281 - trainLoss: 0.657004177570343\n",
      "cnt: 0 - valLoss: 0.6594102382659912 - trainLoss: 0.656957745552063\n",
      "cnt: 0 - valLoss: 0.6593735814094543 - trainLoss: 0.656911313533783\n",
      "cnt: 0 - valLoss: 0.6593371033668518 - trainLoss: 0.6568648219108582\n",
      "cnt: 0 - valLoss: 0.6593005657196045 - trainLoss: 0.6568185091018677\n",
      "cnt: 0 - valLoss: 0.659264087677002 - trainLoss: 0.6567721962928772\n",
      "cnt: 0 - valLoss: 0.6592276096343994 - trainLoss: 0.6567260026931763\n",
      "cnt: 0 - valLoss: 0.6591911911964417 - trainLoss: 0.656679630279541\n",
      "cnt: 0 - valLoss: 0.6591548323631287 - trainLoss: 0.6566334366798401\n",
      "cnt: 0 - valLoss: 0.6591185927391052 - trainLoss: 0.6565873026847839\n",
      "cnt: 0 - valLoss: 0.659082293510437 - trainLoss: 0.6565412282943726\n",
      "cnt: 0 - valLoss: 0.6590460538864136 - trainLoss: 0.6564951539039612\n",
      "cnt: 0 - valLoss: 0.6590098738670349 - trainLoss: 0.6564491391181946\n",
      "cnt: 0 - valLoss: 0.658973753452301 - trainLoss: 0.6564032435417175\n",
      "cnt: 0 - valLoss: 0.6589376330375671 - trainLoss: 0.6563572883605957\n",
      "cnt: 0 - valLoss: 0.658901572227478 - trainLoss: 0.6563114523887634\n",
      "cnt: 0 - valLoss: 0.6588656306266785 - trainLoss: 0.6562657356262207\n",
      "cnt: 0 - valLoss: 0.6588296890258789 - trainLoss: 0.656220018863678\n",
      "cnt: 0 - valLoss: 0.6587937474250793 - trainLoss: 0.6561743021011353\n",
      "cnt: 0 - valLoss: 0.6587579250335693 - trainLoss: 0.6561287045478821\n",
      "cnt: 0 - valLoss: 0.6587221026420593 - trainLoss: 0.6560831665992737\n",
      "cnt: 0 - valLoss: 0.6586862802505493 - trainLoss: 0.6560376286506653\n",
      "cnt: 0 - valLoss: 0.6586505770683289 - trainLoss: 0.6559920907020569\n",
      "cnt: 0 - valLoss: 0.6586148738861084 - trainLoss: 0.6559465527534485\n",
      "cnt: 0 - valLoss: 0.6585791707038879 - trainLoss: 0.6559010744094849\n",
      "cnt: 0 - valLoss: 0.6585435271263123 - trainLoss: 0.6558555960655212\n",
      "cnt: 0 - valLoss: 0.6585079431533813 - trainLoss: 0.6558102369308472\n",
      "cnt: 0 - valLoss: 0.6584723591804504 - trainLoss: 0.6557649374008179\n",
      "cnt: 0 - valLoss: 0.6584368944168091 - trainLoss: 0.6557196378707886\n",
      "cnt: 0 - valLoss: 0.6584014296531677 - trainLoss: 0.6556745171546936\n",
      "cnt: 0 - valLoss: 0.6583660244941711 - trainLoss: 0.6556292772293091\n",
      "cnt: 0 - valLoss: 0.6583306789398193 - trainLoss: 0.6555841565132141\n",
      "cnt: 0 - valLoss: 0.6582953333854675 - trainLoss: 0.6555390357971191\n",
      "cnt: 0 - valLoss: 0.6582600474357605 - trainLoss: 0.655493974685669\n",
      "cnt: 0 - valLoss: 0.6582248210906982 - trainLoss: 0.6554490327835083\n",
      "cnt: 0 - valLoss: 0.658189594745636 - trainLoss: 0.6554041504859924\n",
      "cnt: 0 - valLoss: 0.6581544280052185 - trainLoss: 0.6553592681884766\n",
      "cnt: 0 - valLoss: 0.6581193804740906 - trainLoss: 0.6553144454956055\n",
      "cnt: 0 - valLoss: 0.6580842733383179 - trainLoss: 0.6552696824073792\n",
      "cnt: 0 - valLoss: 0.6580492258071899 - trainLoss: 0.6552250385284424\n",
      "cnt: 0 - valLoss: 0.6580142974853516 - trainLoss: 0.6551803350448608\n",
      "cnt: 0 - valLoss: 0.6579793691635132 - trainLoss: 0.6551358103752136\n",
      "cnt: 0 - valLoss: 0.6579444408416748 - trainLoss: 0.6550911664962769\n",
      "cnt: 0 - valLoss: 0.657909631729126 - trainLoss: 0.6550467610359192\n",
      "cnt: 0 - valLoss: 0.6578748226165771 - trainLoss: 0.6550022959709167\n",
      "cnt: 0 - valLoss: 0.6578400731086731 - trainLoss: 0.6549578905105591\n",
      "cnt: 0 - valLoss: 0.6578053832054138 - trainLoss: 0.6549135446548462\n",
      "cnt: 0 - valLoss: 0.6577706336975098 - trainLoss: 0.6548693180084229\n",
      "cnt: 0 - valLoss: 0.6577358841896057 - trainLoss: 0.6548250317573547\n",
      "cnt: 0 - valLoss: 0.6577011346817017 - trainLoss: 0.6547808647155762\n",
      "cnt: 0 - valLoss: 0.6576663851737976 - trainLoss: 0.6547367572784424\n",
      "cnt: 0 - valLoss: 0.6576317548751831 - trainLoss: 0.6546927094459534\n",
      "cnt: 0 - valLoss: 0.6575971245765686 - trainLoss: 0.6546486616134644\n",
      "cnt: 0 - valLoss: 0.6575624942779541 - trainLoss: 0.6546047329902649\n",
      "cnt: 0 - valLoss: 0.6575279831886292 - trainLoss: 0.6545608043670654\n",
      "cnt: 0 - valLoss: 0.6574934720993042 - trainLoss: 0.6545169353485107\n",
      "cnt: 0 - valLoss: 0.657459020614624 - trainLoss: 0.654473066329956\n",
      "cnt: 0 - valLoss: 0.6574245691299438 - trainLoss: 0.6544293165206909\n",
      "cnt: 0 - valLoss: 0.6573902368545532 - trainLoss: 0.6543856263160706\n",
      "cnt: 0 - valLoss: 0.6573559045791626 - trainLoss: 0.654341995716095\n",
      "cnt: 0 - valLoss: 0.6573216319084167 - trainLoss: 0.6542983651161194\n",
      "cnt: 0 - valLoss: 0.6572872996330261 - trainLoss: 0.6542548537254333\n",
      "cnt: 0 - valLoss: 0.657253086566925 - trainLoss: 0.6542112827301025\n",
      "cnt: 0 - valLoss: 0.657218873500824 - trainLoss: 0.6541677117347717\n",
      "cnt: 0 - valLoss: 0.6571847200393677 - trainLoss: 0.6541242003440857\n",
      "cnt: 0 - valLoss: 0.6571506261825562 - trainLoss: 0.6540807485580444\n",
      "cnt: 0 - valLoss: 0.6571165323257446 - trainLoss: 0.654037356376648\n",
      "cnt: 0 - valLoss: 0.6570825576782227 - trainLoss: 0.6539940237998962\n",
      "cnt: 0 - valLoss: 0.6570485234260559 - trainLoss: 0.6539508104324341\n",
      "cnt: 0 - valLoss: 0.6570146083831787 - trainLoss: 0.6539075374603271\n",
      "cnt: 0 - valLoss: 0.6569806933403015 - trainLoss: 0.6538643836975098\n",
      "cnt: 0 - valLoss: 0.6569468379020691 - trainLoss: 0.6538212895393372\n",
      "cnt: 0 - valLoss: 0.6569130420684814 - trainLoss: 0.6537781953811646\n",
      "cnt: 0 - valLoss: 0.656879186630249 - trainLoss: 0.6537351608276367\n",
      "cnt: 0 - valLoss: 0.6568455100059509 - trainLoss: 0.6536921858787537\n",
      "cnt: 0 - valLoss: 0.6568118333816528 - trainLoss: 0.6536492705345154\n",
      "cnt: 0 - valLoss: 0.6567781567573547 - trainLoss: 0.6536064147949219\n",
      "cnt: 0 - valLoss: 0.6567445397377014 - trainLoss: 0.6535635590553284\n",
      "cnt: 0 - valLoss: 0.6567109227180481 - trainLoss: 0.6535208225250244\n",
      "cnt: 0 - valLoss: 0.6566774249076843 - trainLoss: 0.6534779667854309\n",
      "cnt: 0 - valLoss: 0.6566439270973206 - trainLoss: 0.6534353494644165\n",
      "cnt: 0 - valLoss: 0.6566104888916016 - trainLoss: 0.6533927321434021\n",
      "cnt: 0 - valLoss: 0.6565770506858826 - trainLoss: 0.6533501148223877\n",
      "cnt: 0 - valLoss: 0.6565436720848083 - trainLoss: 0.6533076167106628\n",
      "cnt: 0 - valLoss: 0.6565104126930237 - trainLoss: 0.653265118598938\n",
      "cnt: 0 - valLoss: 0.6564775109291077 - trainLoss: 0.6532227396965027\n",
      "cnt: 0 - valLoss: 0.6564446091651917 - trainLoss: 0.6531805992126465\n",
      "cnt: 0 - valLoss: 0.6564118266105652 - trainLoss: 0.6531385779380798\n",
      "cnt: 0 - valLoss: 0.6563790440559387 - trainLoss: 0.653096616268158\n",
      "cnt: 0 - valLoss: 0.6563463807106018 - trainLoss: 0.6530546545982361\n",
      "cnt: 0 - valLoss: 0.6563135981559753 - trainLoss: 0.6530128121376038\n",
      "cnt: 0 - valLoss: 0.6562809348106384 - trainLoss: 0.6529710292816162\n",
      "cnt: 0 - valLoss: 0.6562483906745911 - trainLoss: 0.6529291868209839\n",
      "cnt: 0 - valLoss: 0.6562157273292542 - trainLoss: 0.6528875231742859\n",
      "cnt: 0 - valLoss: 0.6561832427978516 - trainLoss: 0.6528457999229431\n",
      "cnt: 0 - valLoss: 0.656150758266449 - trainLoss: 0.6528041958808899\n",
      "cnt: 0 - valLoss: 0.6561182737350464 - trainLoss: 0.6527625918388367\n",
      "cnt: 0 - valLoss: 0.6560858488082886 - trainLoss: 0.652721107006073\n",
      "cnt: 0 - valLoss: 0.6560534238815308 - trainLoss: 0.6526796817779541\n",
      "cnt: 0 - valLoss: 0.6560211181640625 - trainLoss: 0.6526381969451904\n",
      "cnt: 0 - valLoss: 0.6559888124465942 - trainLoss: 0.6525967717170715\n",
      "cnt: 0 - valLoss: 0.655956506729126 - trainLoss: 0.6525554656982422\n",
      "cnt: 0 - valLoss: 0.6559243202209473 - trainLoss: 0.6525141596794128\n",
      "cnt: 0 - valLoss: 0.6558921337127686 - trainLoss: 0.6524729132652283\n",
      "cnt: 0 - valLoss: 0.6558599472045898 - trainLoss: 0.6524317264556885\n",
      "cnt: 0 - valLoss: 0.6558278203010559 - trainLoss: 0.6523905396461487\n",
      "cnt: 0 - valLoss: 0.6557957530021667 - trainLoss: 0.6523494720458984\n",
      "cnt: 0 - valLoss: 0.6557636857032776 - trainLoss: 0.6523084044456482\n",
      "cnt: 0 - valLoss: 0.655731737613678 - trainLoss: 0.652267336845398\n",
      "cnt: 0 - valLoss: 0.6556997299194336 - trainLoss: 0.6522263884544373\n",
      "cnt: 0 - valLoss: 0.655667781829834 - trainLoss: 0.6521855592727661\n",
      "cnt: 0 - valLoss: 0.6556359529495239 - trainLoss: 0.6521447896957397\n",
      "cnt: 0 - valLoss: 0.6556041836738586 - trainLoss: 0.6521039605140686\n",
      "cnt: 0 - valLoss: 0.6555723547935486 - trainLoss: 0.6520633101463318\n",
      "cnt: 0 - valLoss: 0.6555405259132385 - trainLoss: 0.6520226001739502\n",
      "cnt: 0 - valLoss: 0.6555088758468628 - trainLoss: 0.6519820094108582\n",
      "cnt: 0 - valLoss: 0.6554772257804871 - trainLoss: 0.6519414782524109\n",
      "cnt: 0 - valLoss: 0.6554455757141113 - trainLoss: 0.6519009470939636\n",
      "cnt: 0 - valLoss: 0.6554139256477356 - trainLoss: 0.6518604755401611\n",
      "cnt: 0 - valLoss: 0.6553823351860046 - trainLoss: 0.6518200039863586\n",
      "cnt: 0 - valLoss: 0.6553506851196289 - trainLoss: 0.6517797112464905\n",
      "cnt: 0 - valLoss: 0.6553190350532532 - trainLoss: 0.6517392992973328\n",
      "cnt: 0 - valLoss: 0.655287504196167 - trainLoss: 0.6516990661621094\n",
      "cnt: 0 - valLoss: 0.6552559733390808 - trainLoss: 0.651658833026886\n",
      "cnt: 0 - valLoss: 0.6552245020866394 - trainLoss: 0.6516185998916626\n",
      "cnt: 0 - valLoss: 0.655193030834198 - trainLoss: 0.651578426361084\n",
      "cnt: 0 - valLoss: 0.6551616787910461 - trainLoss: 0.6515383124351501\n",
      "cnt: 0 - valLoss: 0.6551302075386047 - trainLoss: 0.6514983177185059\n",
      "cnt: 0 - valLoss: 0.6550989151000977 - trainLoss: 0.6514582633972168\n",
      "cnt: 0 - valLoss: 0.6550676226615906 - trainLoss: 0.6514182686805725\n",
      "cnt: 0 - valLoss: 0.6550363302230835 - trainLoss: 0.651378333568573\n",
      "cnt: 0 - valLoss: 0.6550050973892212 - trainLoss: 0.6513384580612183\n",
      "cnt: 0 - valLoss: 0.6549739241600037 - trainLoss: 0.6512986421585083\n",
      "cnt: 0 - valLoss: 0.6549426913261414 - trainLoss: 0.6512588262557983\n",
      "cnt: 0 - valLoss: 0.6549115777015686 - trainLoss: 0.6512190699577332\n",
      "cnt: 0 - valLoss: 0.6548804640769958 - trainLoss: 0.651179313659668\n",
      "cnt: 0 - valLoss: 0.6548493504524231 - trainLoss: 0.6511396169662476\n",
      "cnt: 0 - valLoss: 0.6548183560371399 - trainLoss: 0.6510999202728271\n",
      "cnt: 0 - valLoss: 0.6547873616218567 - trainLoss: 0.6510602831840515\n",
      "cnt: 0 - valLoss: 0.6547563672065735 - trainLoss: 0.6510207056999207\n",
      "cnt: 0 - valLoss: 0.6547254323959351 - trainLoss: 0.6509811878204346\n",
      "cnt: 0 - valLoss: 0.6546946167945862 - trainLoss: 0.6509417295455933\n",
      "cnt: 0 - valLoss: 0.6546637415885925 - trainLoss: 0.6509023904800415\n",
      "cnt: 0 - valLoss: 0.6546329259872437 - trainLoss: 0.6508630514144897\n",
      "cnt: 0 - valLoss: 0.6546021699905396 - trainLoss: 0.6508237719535828\n",
      "cnt: 0 - valLoss: 0.6545714735984802 - trainLoss: 0.6507844924926758\n",
      "cnt: 0 - valLoss: 0.6545407176017761 - trainLoss: 0.6507452726364136\n",
      "cnt: 0 - valLoss: 0.6545100808143616 - trainLoss: 0.6507061123847961\n",
      "cnt: 0 - valLoss: 0.654479444026947 - trainLoss: 0.6506670117378235\n",
      "cnt: 0 - valLoss: 0.6544488668441772 - trainLoss: 0.6506279110908508\n",
      "cnt: 0 - valLoss: 0.6544182300567627 - trainLoss: 0.650588870048523\n",
      "cnt: 0 - valLoss: 0.6543877124786377 - trainLoss: 0.6505498886108398\n",
      "cnt: 0 - valLoss: 0.6543571949005127 - trainLoss: 0.6505109667778015\n",
      "cnt: 0 - valLoss: 0.6543267965316772 - trainLoss: 0.6504720449447632\n",
      "cnt: 0 - valLoss: 0.654296338558197 - trainLoss: 0.6504331827163696\n",
      "cnt: 0 - valLoss: 0.6542659401893616 - trainLoss: 0.6503942608833313\n",
      "cnt: 0 - valLoss: 0.6542355418205261 - trainLoss: 0.6503554582595825\n",
      "cnt: 0 - valLoss: 0.6542052626609802 - trainLoss: 0.6503166556358337\n",
      "cnt: 0 - valLoss: 0.6541748642921448 - trainLoss: 0.6502779126167297\n",
      "cnt: 0 - valLoss: 0.6541446447372437 - trainLoss: 0.6502391695976257\n",
      "cnt: 0 - valLoss: 0.6541144251823425 - trainLoss: 0.650200605392456\n",
      "cnt: 0 - valLoss: 0.6540842056274414 - trainLoss: 0.6501619815826416\n",
      "cnt: 0 - valLoss: 0.6540540456771851 - trainLoss: 0.6501233577728271\n",
      "cnt: 0 - valLoss: 0.6540238261222839 - trainLoss: 0.6500848531723022\n",
      "cnt: 0 - valLoss: 0.6539937853813171 - trainLoss: 0.6500464081764221\n",
      "cnt: 0 - valLoss: 0.6539637446403503 - trainLoss: 0.650007963180542\n",
      "cnt: 0 - valLoss: 0.6539336442947388 - trainLoss: 0.6499695181846619\n",
      "cnt: 0 - valLoss: 0.6539036631584167 - trainLoss: 0.6499311923980713\n",
      "cnt: 0 - valLoss: 0.6538736820220947 - trainLoss: 0.6498928666114807\n",
      "cnt: 0 - valLoss: 0.6538437604904175 - trainLoss: 0.6498546004295349\n",
      "cnt: 0 - valLoss: 0.6538138389587402 - trainLoss: 0.6498163342475891\n",
      "cnt: 0 - valLoss: 0.6537839770317078 - trainLoss: 0.6497781276702881\n",
      "cnt: 0 - valLoss: 0.6537541747093201 - trainLoss: 0.6497399806976318\n",
      "cnt: 0 - valLoss: 0.6537243127822876 - trainLoss: 0.6497018337249756\n",
      "cnt: 0 - valLoss: 0.6536945700645447 - trainLoss: 0.6496637463569641\n",
      "cnt: 0 - valLoss: 0.6536648273468018 - trainLoss: 0.6496257781982422\n",
      "cnt: 0 - valLoss: 0.6536350846290588 - trainLoss: 0.6495878100395203\n",
      "cnt: 0 - valLoss: 0.6536054015159607 - trainLoss: 0.6495498418807983\n",
      "cnt: 0 - valLoss: 0.6535759568214417 - trainLoss: 0.6495119333267212\n",
      "cnt: 0 - valLoss: 0.6535465121269226 - trainLoss: 0.6494740843772888\n",
      "cnt: 0 - valLoss: 0.6535170674324036 - trainLoss: 0.6494362354278564\n",
      "cnt: 0 - valLoss: 0.6534876823425293 - trainLoss: 0.6493985056877136\n",
      "cnt: 0 - valLoss: 0.6534583568572998 - trainLoss: 0.649360716342926\n",
      "cnt: 0 - valLoss: 0.6534290313720703 - trainLoss: 0.6493229866027832\n",
      "cnt: 0 - valLoss: 0.6533997654914856 - trainLoss: 0.6492853760719299\n",
      "cnt: 0 - valLoss: 0.6533704400062561 - trainLoss: 0.6492477059364319\n",
      "cnt: 0 - valLoss: 0.6533412337303162 - trainLoss: 0.6492101550102234\n",
      "cnt: 0 - valLoss: 0.6533120274543762 - trainLoss: 0.6491726040840149\n",
      "cnt: 0 - valLoss: 0.653282880783081 - trainLoss: 0.6491351127624512\n",
      "cnt: 0 - valLoss: 0.6532537937164307 - trainLoss: 0.6490976810455322\n",
      "cnt: 0 - valLoss: 0.6532247066497803 - trainLoss: 0.6490602493286133\n",
      "cnt: 0 - valLoss: 0.6531956195831299 - trainLoss: 0.6490228772163391\n",
      "cnt: 0 - valLoss: 0.6531665921211243 - trainLoss: 0.6489855051040649\n",
      "cnt: 0 - valLoss: 0.6531375646591187 - trainLoss: 0.6489481925964355\n",
      "cnt: 0 - valLoss: 0.6531085968017578 - trainLoss: 0.6489109992980957\n",
      "cnt: 0 - valLoss: 0.6530796885490417 - trainLoss: 0.6488737463951111\n",
      "cnt: 0 - valLoss: 0.6530507802963257 - trainLoss: 0.6488365530967712\n",
      "cnt: 0 - valLoss: 0.6530219316482544 - trainLoss: 0.6487994194030762\n",
      "cnt: 0 - valLoss: 0.6529930233955383 - trainLoss: 0.6487623453140259\n",
      "cnt: 0 - valLoss: 0.6529642343521118 - trainLoss: 0.6487252712249756\n",
      "cnt: 0 - valLoss: 0.6529354453086853 - trainLoss: 0.6486882567405701\n",
      "cnt: 0 - valLoss: 0.6529067158699036 - trainLoss: 0.6486512422561646\n",
      "cnt: 0 - valLoss: 0.6528781652450562 - trainLoss: 0.6486142873764038\n",
      "cnt: 0 - valLoss: 0.6528496742248535 - trainLoss: 0.6485773921012878\n",
      "cnt: 0 - valLoss: 0.6528213620185852 - trainLoss: 0.6485406756401062\n",
      "cnt: 0 - valLoss: 0.6527930498123169 - trainLoss: 0.6485038995742798\n",
      "cnt: 0 - valLoss: 0.6527647376060486 - trainLoss: 0.6484671831130981\n",
      "cnt: 0 - valLoss: 0.6527364253997803 - trainLoss: 0.648430585861206\n",
      "cnt: 0 - valLoss: 0.6527082324028015 - trainLoss: 0.648393988609314\n",
      "cnt: 0 - valLoss: 0.652679979801178 - trainLoss: 0.6483573913574219\n",
      "cnt: 0 - valLoss: 0.6526517868041992 - trainLoss: 0.6483209133148193\n",
      "cnt: 0 - valLoss: 0.6526236534118652 - trainLoss: 0.648284375667572\n",
      "cnt: 0 - valLoss: 0.6525955200195312 - trainLoss: 0.6482479572296143\n",
      "cnt: 0 - valLoss: 0.6525673866271973 - trainLoss: 0.6482114791870117\n",
      "cnt: 0 - valLoss: 0.6525393128395081 - trainLoss: 0.6481751203536987\n",
      "cnt: 0 - valLoss: 0.6525113582611084 - trainLoss: 0.6481388211250305\n",
      "cnt: 0 - valLoss: 0.6524832844734192 - trainLoss: 0.6481025218963623\n",
      "cnt: 0 - valLoss: 0.6524552702903748 - trainLoss: 0.6480662226676941\n",
      "cnt: 0 - valLoss: 0.6524273157119751 - trainLoss: 0.6480300426483154\n",
      "cnt: 0 - valLoss: 0.652399480342865 - trainLoss: 0.647993803024292\n",
      "cnt: 0 - valLoss: 0.6523715853691101 - trainLoss: 0.6479576826095581\n",
      "cnt: 0 - valLoss: 0.6523436903953552 - trainLoss: 0.6479215621948242\n",
      "cnt: 0 - valLoss: 0.6523159146308899 - trainLoss: 0.6478855013847351\n",
      "cnt: 0 - valLoss: 0.6522880792617798 - trainLoss: 0.647849440574646\n",
      "cnt: 0 - valLoss: 0.6522603034973145 - trainLoss: 0.6478134393692017\n",
      "cnt: 0 - valLoss: 0.6522325873374939 - trainLoss: 0.6477774977684021\n",
      "cnt: 0 - valLoss: 0.6522048115730286 - trainLoss: 0.6477415561676025\n",
      "cnt: 0 - valLoss: 0.652177095413208 - trainLoss: 0.6477056741714478\n",
      "cnt: 0 - valLoss: 0.6521494388580322 - trainLoss: 0.647669792175293\n",
      "cnt: 0 - valLoss: 0.6521218419075012 - trainLoss: 0.6476340293884277\n",
      "cnt: 0 - valLoss: 0.6520942449569702 - trainLoss: 0.6475982666015625\n",
      "cnt: 0 - valLoss: 0.652066707611084 - trainLoss: 0.6475625038146973\n",
      "cnt: 0 - valLoss: 0.652039110660553 - trainLoss: 0.6475268006324768\n",
      "cnt: 0 - valLoss: 0.6520115733146667 - trainLoss: 0.6474910974502563\n",
      "cnt: 0 - valLoss: 0.6519840955734253 - trainLoss: 0.6474555730819702\n",
      "cnt: 0 - valLoss: 0.6519566774368286 - trainLoss: 0.6474198698997498\n",
      "cnt: 0 - valLoss: 0.6519292593002319 - trainLoss: 0.6473843455314636\n",
      "cnt: 0 - valLoss: 0.6519018411636353 - trainLoss: 0.6473488211631775\n",
      "cnt: 0 - valLoss: 0.6518744826316833 - trainLoss: 0.6473133563995361\n",
      "cnt: 0 - valLoss: 0.6518471240997314 - trainLoss: 0.6472778916358948\n",
      "cnt: 0 - valLoss: 0.6518198251724243 - trainLoss: 0.6472424864768982\n",
      "cnt: 0 - valLoss: 0.6517924666404724 - trainLoss: 0.6472071409225464\n",
      "cnt: 0 - valLoss: 0.6517652273178101 - trainLoss: 0.6471718549728394\n",
      "cnt: 0 - valLoss: 0.6517379879951477 - trainLoss: 0.6471365094184875\n",
      "cnt: 0 - valLoss: 0.6517107486724854 - trainLoss: 0.6471012234687805\n",
      "cnt: 0 - valLoss: 0.6516835689544678 - trainLoss: 0.647066056728363\n",
      "cnt: 0 - valLoss: 0.6516563892364502 - trainLoss: 0.6470308899879456\n",
      "cnt: 0 - valLoss: 0.6516292691230774 - trainLoss: 0.6469956636428833\n",
      "cnt: 0 - valLoss: 0.6516022086143494 - trainLoss: 0.6469605565071106\n",
      "cnt: 0 - valLoss: 0.6515749096870422 - trainLoss: 0.6469255089759827\n",
      "cnt: 0 - valLoss: 0.6515475511550903 - trainLoss: 0.6468905210494995\n",
      "cnt: 0 - valLoss: 0.6515201926231384 - trainLoss: 0.6468554735183716\n",
      "cnt: 0 - valLoss: 0.6514929533004761 - trainLoss: 0.6468205451965332\n",
      "cnt: 0 - valLoss: 0.651465654373169 - trainLoss: 0.6467856168746948\n",
      "cnt: 0 - valLoss: 0.6514384150505066 - trainLoss: 0.6467506885528564\n",
      "cnt: 0 - valLoss: 0.6514111757278442 - trainLoss: 0.6467158198356628\n",
      "cnt: 0 - valLoss: 0.6513839960098267 - trainLoss: 0.646681010723114\n",
      "cnt: 0 - valLoss: 0.6513568162918091 - trainLoss: 0.6466462016105652\n",
      "cnt: 0 - valLoss: 0.6513296365737915 - trainLoss: 0.6466113924980164\n",
      "cnt: 0 - valLoss: 0.6513026356697083 - trainLoss: 0.6465767025947571\n",
      "cnt: 0 - valLoss: 0.6512758135795593 - trainLoss: 0.6465420722961426\n",
      "cnt: 0 - valLoss: 0.6512489914894104 - trainLoss: 0.6465075016021729\n",
      "cnt: 0 - valLoss: 0.6512222290039062 - trainLoss: 0.6464729309082031\n",
      "cnt: 0 - valLoss: 0.6511954069137573 - trainLoss: 0.6464384198188782\n",
      "cnt: 0 - valLoss: 0.6511687636375427 - trainLoss: 0.646403968334198\n",
      "cnt: 0 - valLoss: 0.6511420607566833 - trainLoss: 0.6463696956634521\n",
      "cnt: 0 - valLoss: 0.6511154174804688 - trainLoss: 0.6463354229927063\n",
      "cnt: 0 - valLoss: 0.6510888338088989 - trainLoss: 0.6463011503219604\n",
      "cnt: 0 - valLoss: 0.6510624289512634 - trainLoss: 0.6462669372558594\n",
      "cnt: 0 - valLoss: 0.6510360836982727 - trainLoss: 0.6462327837944031\n",
      "cnt: 0 - valLoss: 0.651009738445282 - trainLoss: 0.646198570728302\n",
      "cnt: 0 - valLoss: 0.6509833931922913 - trainLoss: 0.6461644768714905\n",
      "cnt: 0 - valLoss: 0.6509571075439453 - trainLoss: 0.646130383014679\n",
      "cnt: 0 - valLoss: 0.6509308815002441 - trainLoss: 0.6460963487625122\n",
      "cnt: 0 - valLoss: 0.6509045958518982 - trainLoss: 0.6460623741149902\n",
      "cnt: 0 - valLoss: 0.650878369808197 - trainLoss: 0.6460283398628235\n",
      "cnt: 0 - valLoss: 0.6508522629737854 - trainLoss: 0.6459943652153015\n",
      "cnt: 0 - valLoss: 0.650826096534729 - trainLoss: 0.6459606289863586\n",
      "cnt: 0 - valLoss: 0.6507999300956726 - trainLoss: 0.6459267735481262\n",
      "cnt: 0 - valLoss: 0.650773823261261 - trainLoss: 0.6458929777145386\n",
      "cnt: 0 - valLoss: 0.6507477760314941 - trainLoss: 0.6458591818809509\n",
      "cnt: 0 - valLoss: 0.6507216691970825 - trainLoss: 0.6458253860473633\n",
      "cnt: 0 - valLoss: 0.6506957411766052 - trainLoss: 0.6457916498184204\n",
      "cnt: 0 - valLoss: 0.6506696939468384 - trainLoss: 0.6457579731941223\n",
      "cnt: 0 - valLoss: 0.6506437063217163 - trainLoss: 0.6457242965698242\n",
      "cnt: 0 - valLoss: 0.650617778301239 - trainLoss: 0.6456906795501709\n",
      "cnt: 0 - valLoss: 0.6505917906761169 - trainLoss: 0.6456570625305176\n",
      "cnt: 0 - valLoss: 0.6505658626556396 - trainLoss: 0.645623505115509\n",
      "cnt: 0 - valLoss: 0.6505400538444519 - trainLoss: 0.6455899477005005\n",
      "cnt: 0 - valLoss: 0.6505141258239746 - trainLoss: 0.6455564498901367\n",
      "cnt: 0 - valLoss: 0.6504883766174316 - trainLoss: 0.6455230116844177\n",
      "cnt: 0 - valLoss: 0.6504625678062439 - trainLoss: 0.6454895734786987\n",
      "cnt: 0 - valLoss: 0.6504367589950562 - trainLoss: 0.6454561948776245\n",
      "cnt: 0 - valLoss: 0.6504110097885132 - trainLoss: 0.6454228162765503\n",
      "cnt: 0 - valLoss: 0.6503854990005493 - trainLoss: 0.6453894376754761\n",
      "cnt: 0 - valLoss: 0.6503600478172302 - trainLoss: 0.6453561782836914\n",
      "cnt: 0 - valLoss: 0.6503345370292664 - trainLoss: 0.6453229188919067\n",
      "cnt: 0 - valLoss: 0.6503092050552368 - trainLoss: 0.6452897191047668\n",
      "cnt: 0 - valLoss: 0.6502839922904968 - trainLoss: 0.6452564597129822\n",
      "cnt: 0 - valLoss: 0.6502587795257568 - trainLoss: 0.6452232599258423\n",
      "cnt: 0 - valLoss: 0.6502336263656616 - trainLoss: 0.6451901197433472\n",
      "cnt: 0 - valLoss: 0.6502085328102112 - trainLoss: 0.6451570391654968\n",
      "cnt: 0 - valLoss: 0.650183379650116 - trainLoss: 0.6451238989830017\n",
      "cnt: 0 - valLoss: 0.6501582860946655 - trainLoss: 0.6450908780097961\n",
      "cnt: 0 - valLoss: 0.6501332521438599 - trainLoss: 0.6450578570365906\n",
      "cnt: 0 - valLoss: 0.6501082181930542 - trainLoss: 0.6450248956680298\n",
      "cnt: 0 - valLoss: 0.6500831842422485 - trainLoss: 0.644991934299469\n",
      "cnt: 0 - valLoss: 0.6500582098960876 - trainLoss: 0.644959032535553\n",
      "cnt: 0 - valLoss: 0.650033175945282 - trainLoss: 0.644926130771637\n",
      "cnt: 0 - valLoss: 0.6500083208084106 - trainLoss: 0.6448932886123657\n",
      "cnt: 0 - valLoss: 0.6499833464622498 - trainLoss: 0.6448603868484497\n",
      "cnt: 0 - valLoss: 0.6499584317207336 - trainLoss: 0.644827663898468\n",
      "cnt: 0 - valLoss: 0.6499335765838623 - trainLoss: 0.6447948813438416\n",
      "cnt: 0 - valLoss: 0.6499086618423462 - trainLoss: 0.6447621583938599\n",
      "cnt: 0 - valLoss: 0.6498838663101196 - trainLoss: 0.6447294354438782\n",
      "cnt: 0 - valLoss: 0.6498590707778931 - trainLoss: 0.6446967720985413\n",
      "cnt: 0 - valLoss: 0.6498342752456665 - trainLoss: 0.6446641087532043\n",
      "cnt: 0 - valLoss: 0.6498094797134399 - trainLoss: 0.6446315050125122\n",
      "cnt: 0 - valLoss: 0.6497848033905029 - trainLoss: 0.6445989608764648\n",
      "cnt: 0 - valLoss: 0.6497600674629211 - trainLoss: 0.6445663571357727\n",
      "cnt: 0 - valLoss: 0.6497354507446289 - trainLoss: 0.6445339322090149\n",
      "cnt: 0 - valLoss: 0.6497107744216919 - trainLoss: 0.6445014476776123\n",
      "cnt: 0 - valLoss: 0.6496862173080444 - trainLoss: 0.6444692015647888\n",
      "cnt: 0 - valLoss: 0.649661660194397 - trainLoss: 0.6444369554519653\n",
      "cnt: 0 - valLoss: 0.6496371030807495 - trainLoss: 0.6444047093391418\n",
      "cnt: 0 - valLoss: 0.6496126055717468 - trainLoss: 0.6443725228309631\n",
      "cnt: 0 - valLoss: 0.6495881080627441 - trainLoss: 0.644340455532074\n",
      "cnt: 0 - valLoss: 0.6495636701583862 - trainLoss: 0.6443082690238953\n",
      "cnt: 0 - valLoss: 0.6495394706726074 - trainLoss: 0.6442762017250061\n",
      "cnt: 0 - valLoss: 0.6495152711868286 - trainLoss: 0.6442441940307617\n",
      "cnt: 0 - valLoss: 0.6494910717010498 - trainLoss: 0.6442123055458069\n",
      "cnt: 0 - valLoss: 0.6494669318199158 - trainLoss: 0.6441803574562073\n",
      "cnt: 0 - valLoss: 0.6494427919387817 - trainLoss: 0.6441485285758972\n",
      "cnt: 0 - valLoss: 0.6494185924530029 - trainLoss: 0.6441166996955872\n",
      "cnt: 0 - valLoss: 0.6493945717811584 - trainLoss: 0.6440848112106323\n",
      "cnt: 0 - valLoss: 0.6493704915046692 - trainLoss: 0.6440529823303223\n",
      "cnt: 0 - valLoss: 0.6493464112281799 - trainLoss: 0.6440212726593018\n",
      "cnt: 0 - valLoss: 0.6493224501609802 - trainLoss: 0.6439895033836365\n",
      "cnt: 0 - valLoss: 0.6492984294891357 - trainLoss: 0.6439578533172607\n",
      "cnt: 0 - valLoss: 0.6492744088172913 - trainLoss: 0.6439261436462402\n",
      "cnt: 0 - valLoss: 0.6492505073547363 - trainLoss: 0.6438945531845093\n",
      "cnt: 0 - valLoss: 0.6492267847061157 - trainLoss: 0.6438629031181335\n",
      "cnt: 0 - valLoss: 0.6492030620574951 - trainLoss: 0.6438314318656921\n",
      "cnt: 0 - valLoss: 0.6491793990135193 - trainLoss: 0.6437999606132507\n",
      "cnt: 0 - valLoss: 0.6491556763648987 - trainLoss: 0.6437685489654541\n",
      "cnt: 0 - valLoss: 0.6491318345069885 - trainLoss: 0.6437370777130127\n",
      "cnt: 0 - valLoss: 0.6491079330444336 - trainLoss: 0.6437056660652161\n",
      "cnt: 0 - valLoss: 0.6490840911865234 - trainLoss: 0.643674373626709\n",
      "cnt: 0 - valLoss: 0.6490602493286133 - trainLoss: 0.6436430215835571\n",
      "cnt: 0 - valLoss: 0.6490364670753479 - trainLoss: 0.6436118483543396\n",
      "cnt: 0 - valLoss: 0.6490126252174377 - trainLoss: 0.6435806751251221\n",
      "cnt: 0 - valLoss: 0.6489889025688171 - trainLoss: 0.6435495018959045\n",
      "cnt: 0 - valLoss: 0.6489651203155518 - trainLoss: 0.6435183882713318\n",
      "cnt: 0 - valLoss: 0.6489413976669312 - trainLoss: 0.6434873342514038\n",
      "cnt: 0 - valLoss: 0.648918092250824 - trainLoss: 0.643456220626831\n",
      "cnt: 0 - valLoss: 0.648895263671875 - trainLoss: 0.6434255242347717\n",
      "cnt: 0 - valLoss: 0.6488725543022156 - trainLoss: 0.6433952450752258\n",
      "cnt: 0 - valLoss: 0.6488498449325562 - trainLoss: 0.6433649659156799\n",
      "cnt: 0 - valLoss: 0.6488271951675415 - trainLoss: 0.6433347463607788\n",
      "cnt: 0 - valLoss: 0.6488045454025269 - trainLoss: 0.6433045268058777\n",
      "cnt: 0 - valLoss: 0.648781955242157 - trainLoss: 0.6432744264602661\n",
      "cnt: 0 - valLoss: 0.6487593054771423 - trainLoss: 0.6432442665100098\n",
      "cnt: 0 - valLoss: 0.6487366557121277 - trainLoss: 0.6432141661643982\n",
      "cnt: 0 - valLoss: 0.6487140655517578 - trainLoss: 0.6431838870048523\n",
      "cnt: 0 - valLoss: 0.6486915349960327 - trainLoss: 0.643153727054596\n",
      "cnt: 0 - valLoss: 0.6486690044403076 - trainLoss: 0.6431236267089844\n",
      "cnt: 0 - valLoss: 0.6486464738845825 - trainLoss: 0.6430936455726624\n",
      "cnt: 0 - valLoss: 0.6486239433288574 - trainLoss: 0.6430636048316956\n",
      "cnt: 0 - valLoss: 0.6486014723777771 - trainLoss: 0.6430335640907288\n",
      "cnt: 0 - valLoss: 0.6485790014266968 - trainLoss: 0.6430035829544067\n",
      "cnt: 0 - valLoss: 0.6485565304756165 - trainLoss: 0.6429736614227295\n",
      "cnt: 0 - valLoss: 0.6485341191291809 - trainLoss: 0.6429436802864075\n",
      "cnt: 0 - valLoss: 0.6485117077827454 - trainLoss: 0.6429138779640198\n",
      "cnt: 0 - valLoss: 0.6484893560409546 - trainLoss: 0.6428841352462769\n",
      "cnt: 0 - valLoss: 0.6484670042991638 - trainLoss: 0.6428543329238892\n",
      "cnt: 0 - valLoss: 0.6484447121620178 - trainLoss: 0.6428245902061462\n",
      "cnt: 0 - valLoss: 0.648422360420227 - trainLoss: 0.6427948474884033\n",
      "cnt: 0 - valLoss: 0.648400068283081 - trainLoss: 0.64276522397995\n",
      "cnt: 0 - valLoss: 0.6483778357505798 - trainLoss: 0.6427355408668518\n",
      "cnt: 0 - valLoss: 0.6483555436134338 - trainLoss: 0.6427059173583984\n",
      "cnt: 0 - valLoss: 0.6483332514762878 - trainLoss: 0.6426762938499451\n",
      "cnt: 0 - valLoss: 0.6483110785484314 - trainLoss: 0.6426466107368469\n",
      "cnt: 0 - valLoss: 0.6482887864112854 - trainLoss: 0.6426169872283936\n",
      "cnt: 0 - valLoss: 0.6482666730880737 - trainLoss: 0.6425874829292297\n",
      "cnt: 0 - valLoss: 0.6482444405555725 - trainLoss: 0.6425579190254211\n",
      "cnt: 0 - valLoss: 0.6482222676277161 - trainLoss: 0.6425284147262573\n",
      "cnt: 0 - valLoss: 0.6482001543045044 - trainLoss: 0.6424988508224487\n",
      "cnt: 0 - valLoss: 0.6481780409812927 - trainLoss: 0.6424694657325745\n",
      "cnt: 0 - valLoss: 0.6481558084487915 - trainLoss: 0.6424399614334106\n",
      "cnt: 0 - valLoss: 0.6481334567070007 - trainLoss: 0.6424106955528259\n",
      "cnt: 0 - valLoss: 0.6481111645698547 - trainLoss: 0.6423813104629517\n",
      "cnt: 0 - valLoss: 0.6480888724327087 - trainLoss: 0.6423520445823669\n",
      "cnt: 0 - valLoss: 0.6480665802955627 - trainLoss: 0.6423227787017822\n",
      "cnt: 0 - valLoss: 0.6480443477630615 - trainLoss: 0.6422935128211975\n",
      "cnt: 0 - valLoss: 0.6480221748352051 - trainLoss: 0.6422643065452576\n",
      "cnt: 0 - valLoss: 0.6479999423027039 - trainLoss: 0.6422351598739624\n",
      "cnt: 0 - valLoss: 0.6479777693748474 - trainLoss: 0.6422060132026672\n",
      "cnt: 0 - valLoss: 0.6479555368423462 - trainLoss: 0.6421768665313721\n",
      "cnt: 0 - valLoss: 0.6479334235191345 - trainLoss: 0.6421477794647217\n",
      "cnt: 0 - valLoss: 0.6479113101959229 - trainLoss: 0.6421186923980713\n",
      "cnt: 0 - valLoss: 0.6478891968727112 - trainLoss: 0.6420897841453552\n",
      "cnt: 0 - valLoss: 0.6478671431541443 - trainLoss: 0.6420608162879944\n",
      "cnt: 0 - valLoss: 0.6478450894355774 - trainLoss: 0.6420319080352783\n",
      "cnt: 0 - valLoss: 0.6478230953216553 - trainLoss: 0.642003059387207\n",
      "cnt: 0 - valLoss: 0.6478011012077332 - trainLoss: 0.641974151134491\n",
      "cnt: 0 - valLoss: 0.6477790474891663 - trainLoss: 0.6419453620910645\n",
      "cnt: 0 - valLoss: 0.6477571129798889 - trainLoss: 0.6419165134429932\n",
      "cnt: 0 - valLoss: 0.6477351784706116 - trainLoss: 0.6418878436088562\n",
      "cnt: 0 - valLoss: 0.6477132439613342 - trainLoss: 0.6418591737747192\n",
      "cnt: 0 - valLoss: 0.647691011428833 - trainLoss: 0.6418305039405823\n",
      "cnt: 0 - valLoss: 0.6476688981056213 - trainLoss: 0.6418017745018005\n",
      "cnt: 0 - valLoss: 0.6476467251777649 - trainLoss: 0.6417731046676636\n",
      "cnt: 0 - valLoss: 0.6476246118545532 - trainLoss: 0.6417444348335266\n",
      "cnt: 0 - valLoss: 0.6476024389266968 - trainLoss: 0.6417157649993896\n",
      "cnt: 0 - valLoss: 0.6475803256034851 - trainLoss: 0.6416870951652527\n",
      "cnt: 0 - valLoss: 0.6475582122802734 - trainLoss: 0.6416584849357605\n",
      "cnt: 0 - valLoss: 0.6475362181663513 - trainLoss: 0.6416298747062683\n",
      "cnt: 0 - valLoss: 0.6475141048431396 - trainLoss: 0.6416013240814209\n",
      "cnt: 0 - valLoss: 0.6474920511245728 - trainLoss: 0.6415727734565735\n",
      "cnt: 0 - valLoss: 0.6474701166152954 - trainLoss: 0.6415442228317261\n",
      "cnt: 0 - valLoss: 0.6474481821060181 - trainLoss: 0.641515851020813\n",
      "cnt: 0 - valLoss: 0.647426187992096 - trainLoss: 0.6414874792098999\n",
      "cnt: 0 - valLoss: 0.6474042534828186 - trainLoss: 0.6414591073989868\n",
      "cnt: 0 - valLoss: 0.647382378578186 - trainLoss: 0.6414307951927185\n",
      "cnt: 0 - valLoss: 0.6473604440689087 - trainLoss: 0.641402542591095\n",
      "cnt: 0 - valLoss: 0.6473386287689209 - trainLoss: 0.6413742303848267\n",
      "cnt: 0 - valLoss: 0.6473167538642883 - trainLoss: 0.6413459777832031\n",
      "cnt: 0 - valLoss: 0.6472948789596558 - trainLoss: 0.6413177847862244\n",
      "cnt: 0 - valLoss: 0.6472730040550232 - trainLoss: 0.6412895917892456\n",
      "cnt: 0 - valLoss: 0.6472512483596802 - trainLoss: 0.6412613987922668\n",
      "cnt: 0 - valLoss: 0.6472293734550476 - trainLoss: 0.6412332057952881\n",
      "cnt: 0 - valLoss: 0.6472076177597046 - trainLoss: 0.6412050724029541\n",
      "cnt: 0 - valLoss: 0.6471858620643616 - trainLoss: 0.6411769986152649\n",
      "cnt: 0 - valLoss: 0.6471641063690186 - trainLoss: 0.6411488652229309\n",
      "cnt: 0 - valLoss: 0.6471424102783203 - trainLoss: 0.6411207914352417\n",
      "cnt: 0 - valLoss: 0.6471206545829773 - trainLoss: 0.6410927772521973\n",
      "cnt: 0 - valLoss: 0.6470990180969238 - trainLoss: 0.6410647630691528\n",
      "cnt: 0 - valLoss: 0.6470772624015808 - trainLoss: 0.6410367488861084\n",
      "cnt: 0 - valLoss: 0.6470556259155273 - trainLoss: 0.641008734703064\n",
      "cnt: 0 - valLoss: 0.6470340490341187 - trainLoss: 0.6409808993339539\n",
      "cnt: 0 - valLoss: 0.6470124125480652 - trainLoss: 0.640953004360199\n",
      "cnt: 0 - valLoss: 0.6469908356666565 - trainLoss: 0.6409252285957336\n",
      "cnt: 0 - valLoss: 0.646969199180603 - trainLoss: 0.6408973932266235\n",
      "cnt: 0 - valLoss: 0.6469476819038391 - trainLoss: 0.640869677066803\n",
      "cnt: 0 - valLoss: 0.6469261050224304 - trainLoss: 0.6408419013023376\n",
      "cnt: 0 - valLoss: 0.6469045877456665 - trainLoss: 0.6408141255378723\n",
      "cnt: 0 - valLoss: 0.6468830704689026 - trainLoss: 0.640786349773407\n",
      "cnt: 0 - valLoss: 0.6468615531921387 - trainLoss: 0.6407586932182312\n",
      "cnt: 0 - valLoss: 0.6468400359153748 - trainLoss: 0.6407310366630554\n",
      "cnt: 0 - valLoss: 0.6468186378479004 - trainLoss: 0.6407033205032349\n",
      "cnt: 0 - valLoss: 0.6467972993850708 - trainLoss: 0.6406757831573486\n",
      "cnt: 0 - valLoss: 0.6467761397361755 - trainLoss: 0.6406483054161072\n",
      "cnt: 0 - valLoss: 0.6467549800872803 - trainLoss: 0.6406208276748657\n",
      "cnt: 0 - valLoss: 0.646733820438385 - trainLoss: 0.6405933499336243\n",
      "cnt: 0 - valLoss: 0.6467127203941345 - trainLoss: 0.6405658721923828\n",
      "cnt: 0 - valLoss: 0.6466915011405945 - trainLoss: 0.6405384540557861\n",
      "cnt: 0 - valLoss: 0.646670401096344 - trainLoss: 0.6405110359191895\n",
      "cnt: 0 - valLoss: 0.6466493010520935 - trainLoss: 0.6404836773872375\n",
      "cnt: 0 - valLoss: 0.6466282606124878 - trainLoss: 0.6404563784599304\n",
      "cnt: 0 - valLoss: 0.6466072201728821 - trainLoss: 0.6404291391372681\n",
      "cnt: 0 - valLoss: 0.6465861797332764 - trainLoss: 0.6404018998146057\n",
      "cnt: 0 - valLoss: 0.6465651988983154 - trainLoss: 0.6403747200965881\n",
      "cnt: 0 - valLoss: 0.6465442180633545 - trainLoss: 0.6403475403785706\n",
      "cnt: 0 - valLoss: 0.6465232372283936 - trainLoss: 0.6403204202651978\n",
      "cnt: 0 - valLoss: 0.6465023159980774 - trainLoss: 0.640293300151825\n",
      "cnt: 0 - valLoss: 0.6464813351631165 - trainLoss: 0.6402661800384521\n",
      "cnt: 0 - valLoss: 0.6464603543281555 - trainLoss: 0.6402390599250793\n",
      "cnt: 0 - valLoss: 0.6464394330978394 - trainLoss: 0.6402119398117065\n",
      "cnt: 0 - valLoss: 0.6464185118675232 - trainLoss: 0.6401849389076233\n",
      "cnt: 0 - valLoss: 0.6463976502418518 - trainLoss: 0.6401578783988953\n",
      "cnt: 0 - valLoss: 0.6463767886161804 - trainLoss: 0.6401309370994568\n",
      "cnt: 0 - valLoss: 0.6463558673858643 - trainLoss: 0.6401039361953735\n",
      "cnt: 0 - valLoss: 0.6463350653648376 - trainLoss: 0.6400769948959351\n",
      "cnt: 0 - valLoss: 0.6463142037391663 - trainLoss: 0.6400501132011414\n",
      "cnt: 0 - valLoss: 0.6462934017181396 - trainLoss: 0.6400231122970581\n",
      "cnt: 0 - valLoss: 0.6462725400924683 - trainLoss: 0.6399962306022644\n",
      "cnt: 0 - valLoss: 0.6462517976760864 - trainLoss: 0.6399693489074707\n",
      "cnt: 0 - valLoss: 0.6462309956550598 - trainLoss: 0.639942467212677\n",
      "cnt: 0 - valLoss: 0.646210253238678 - trainLoss: 0.6399157047271729\n",
      "cnt: 0 - valLoss: 0.6461895108222961 - trainLoss: 0.6398888230323792\n",
      "cnt: 0 - valLoss: 0.6461687684059143 - trainLoss: 0.639862060546875\n",
      "cnt: 0 - valLoss: 0.6461480855941772 - trainLoss: 0.6398352384567261\n",
      "cnt: 0 - valLoss: 0.6461273431777954 - trainLoss: 0.6398085355758667\n",
      "cnt: 0 - valLoss: 0.6461066603660583 - trainLoss: 0.6397818326950073\n",
      "cnt: 0 - valLoss: 0.6460859179496765 - trainLoss: 0.6397550702095032\n",
      "cnt: 0 - valLoss: 0.6460652947425842 - trainLoss: 0.6397283673286438\n",
      "cnt: 0 - valLoss: 0.6460446119308472 - trainLoss: 0.6397016644477844\n",
      "cnt: 0 - valLoss: 0.6460239291191101 - trainLoss: 0.6396750807762146\n",
      "cnt: 0 - valLoss: 0.6460033655166626 - trainLoss: 0.6396484375\n",
      "cnt: 0 - valLoss: 0.6459827423095703 - trainLoss: 0.6396217942237854\n",
      "cnt: 0 - valLoss: 0.6459622383117676 - trainLoss: 0.6395952105522156\n",
      "cnt: 0 - valLoss: 0.6459417939186096 - trainLoss: 0.6395686268806458\n",
      "cnt: 0 - valLoss: 0.6459214091300964 - trainLoss: 0.6395421028137207\n",
      "cnt: 0 - valLoss: 0.6459010243415833 - trainLoss: 0.6395155787467957\n",
      "cnt: 0 - valLoss: 0.6458806991577148 - trainLoss: 0.6394890546798706\n",
      "cnt: 0 - valLoss: 0.6458603739738464 - trainLoss: 0.6394625902175903\n",
      "cnt: 0 - valLoss: 0.645840048789978 - trainLoss: 0.6394361257553101\n",
      "cnt: 0 - valLoss: 0.6458196640014648 - trainLoss: 0.6394096612930298\n",
      "cnt: 0 - valLoss: 0.6457993984222412 - trainLoss: 0.6393832564353943\n",
      "cnt: 0 - valLoss: 0.6457791328430176 - trainLoss: 0.6393568515777588\n",
      "cnt: 0 - valLoss: 0.645758867263794 - trainLoss: 0.6393304467201233\n",
      "cnt: 0 - valLoss: 0.6457385420799255 - trainLoss: 0.6393040418624878\n",
      "cnt: 0 - valLoss: 0.6457183361053467 - trainLoss: 0.6392776966094971\n",
      "cnt: 0 - valLoss: 0.6456980109214783 - trainLoss: 0.6392513513565063\n",
      "cnt: 0 - valLoss: 0.6456778049468994 - trainLoss: 0.6392250657081604\n",
      "cnt: 0 - valLoss: 0.6456576585769653 - trainLoss: 0.6391987800598145\n",
      "cnt: 0 - valLoss: 0.6456374526023865 - trainLoss: 0.6391724944114685\n",
      "cnt: 0 - valLoss: 0.6456173062324524 - trainLoss: 0.6391462683677673\n",
      "cnt: 0 - valLoss: 0.6455972194671631 - trainLoss: 0.6391201615333557\n",
      "cnt: 0 - valLoss: 0.645577073097229 - trainLoss: 0.6390940546989441\n",
      "cnt: 0 - valLoss: 0.6455569267272949 - trainLoss: 0.6390679478645325\n",
      "cnt: 0 - valLoss: 0.6455368995666504 - trainLoss: 0.6390418410301208\n",
      "cnt: 0 - valLoss: 0.6455167531967163 - trainLoss: 0.6390158534049988\n",
      "cnt: 0 - valLoss: 0.645496666431427 - trainLoss: 0.6389898061752319\n",
      "cnt: 0 - valLoss: 0.6454766392707825 - trainLoss: 0.6389637589454651\n",
      "cnt: 0 - valLoss: 0.6454566121101379 - trainLoss: 0.6389378309249878\n",
      "cnt: 0 - valLoss: 0.6454365849494934 - trainLoss: 0.6389118432998657\n",
      "cnt: 0 - valLoss: 0.6454164981842041 - trainLoss: 0.6388858556747437\n",
      "cnt: 0 - valLoss: 0.6453964710235596 - trainLoss: 0.6388599276542664\n",
      "cnt: 0 - valLoss: 0.6453765034675598 - trainLoss: 0.6388339996337891\n",
      "cnt: 0 - valLoss: 0.6453565359115601 - trainLoss: 0.6388081312179565\n",
      "cnt: 0 - valLoss: 0.6453365683555603 - trainLoss: 0.638782262802124\n",
      "cnt: 0 - valLoss: 0.6453166604042053 - trainLoss: 0.6387564539909363\n",
      "cnt: 0 - valLoss: 0.6452967524528503 - trainLoss: 0.6387307047843933\n",
      "cnt: 0 - valLoss: 0.6452768445014954 - trainLoss: 0.6387050151824951\n",
      "cnt: 0 - valLoss: 0.6452568769454956 - trainLoss: 0.6386792659759521\n",
      "cnt: 0 - valLoss: 0.6452370882034302 - trainLoss: 0.638653576374054\n",
      "cnt: 0 - valLoss: 0.6452171802520752 - trainLoss: 0.6386279463768005\n",
      "cnt: 0 - valLoss: 0.645197331905365 - trainLoss: 0.6386021971702576\n",
      "cnt: 0 - valLoss: 0.64517742395401 - trainLoss: 0.6385765075683594\n",
      "cnt: 0 - valLoss: 0.6451575756072998 - trainLoss: 0.6385507583618164\n",
      "cnt: 0 - valLoss: 0.6451377272605896 - trainLoss: 0.6385250687599182\n",
      "cnt: 0 - valLoss: 0.6451178789138794 - trainLoss: 0.63849937915802\n",
      "cnt: 0 - valLoss: 0.6450980305671692 - trainLoss: 0.6384736895561218\n",
      "cnt: 0 - valLoss: 0.645078182220459 - trainLoss: 0.6384480595588684\n",
      "cnt: 0 - valLoss: 0.6450583934783936 - trainLoss: 0.638422429561615\n",
      "cnt: 0 - valLoss: 0.6450386643409729 - trainLoss: 0.6383968591690063\n",
      "cnt: 0 - valLoss: 0.6450188159942627 - trainLoss: 0.6383712291717529\n",
      "cnt: 0 - valLoss: 0.644999086856842 - trainLoss: 0.6383456587791443\n",
      "cnt: 0 - valLoss: 0.6449792981147766 - trainLoss: 0.6383200883865356\n",
      "cnt: 0 - valLoss: 0.644959568977356 - trainLoss: 0.638294517993927\n",
      "cnt: 0 - valLoss: 0.6449397802352905 - trainLoss: 0.6382690072059631\n",
      "cnt: 0 - valLoss: 0.6449201107025146 - trainLoss: 0.6382434964179993\n",
      "cnt: 0 - valLoss: 0.644900381565094 - trainLoss: 0.6382179856300354\n",
      "cnt: 0 - valLoss: 0.6448806524276733 - trainLoss: 0.6381925940513611\n",
      "cnt: 0 - valLoss: 0.6448609232902527 - trainLoss: 0.638167142868042\n",
      "cnt: 0 - valLoss: 0.6448412537574768 - trainLoss: 0.6381416916847229\n",
      "cnt: 0 - valLoss: 0.6448215842247009 - trainLoss: 0.6381163001060486\n",
      "cnt: 0 - valLoss: 0.6448019742965698 - trainLoss: 0.6380909085273743\n",
      "cnt: 0 - valLoss: 0.6447824239730835 - trainLoss: 0.6380656957626343\n",
      "cnt: 0 - valLoss: 0.6447628736495972 - trainLoss: 0.6380405426025391\n",
      "cnt: 0 - valLoss: 0.6447433233261108 - trainLoss: 0.6380153894424438\n",
      "cnt: 0 - valLoss: 0.6447237133979797 - trainLoss: 0.6379902362823486\n",
      "cnt: 0 - valLoss: 0.6447042226791382 - trainLoss: 0.6379651427268982\n",
      "cnt: 0 - valLoss: 0.6446846723556519 - trainLoss: 0.6379400491714478\n",
      "cnt: 0 - valLoss: 0.6446651816368103 - trainLoss: 0.6379149556159973\n",
      "cnt: 0 - valLoss: 0.6446456909179688 - trainLoss: 0.6378898620605469\n",
      "cnt: 0 - valLoss: 0.6446261405944824 - trainLoss: 0.6378648281097412\n",
      "cnt: 0 - valLoss: 0.6446067094802856 - trainLoss: 0.6378397941589355\n",
      "cnt: 0 - valLoss: 0.6445873379707336 - trainLoss: 0.6378148198127747\n",
      "cnt: 0 - valLoss: 0.6445680856704712 - trainLoss: 0.6377898454666138\n",
      "cnt: 0 - valLoss: 0.6445488929748535 - trainLoss: 0.6377648115158081\n",
      "cnt: 0 - valLoss: 0.6445296406745911 - trainLoss: 0.637739896774292\n",
      "cnt: 0 - valLoss: 0.6445104479789734 - trainLoss: 0.6377149224281311\n",
      "cnt: 0 - valLoss: 0.6444911956787109 - trainLoss: 0.637690007686615\n",
      "cnt: 0 - valLoss: 0.6444720029830933 - trainLoss: 0.6376650929450989\n",
      "cnt: 0 - valLoss: 0.6444528102874756 - trainLoss: 0.6376402378082275\n",
      "cnt: 0 - valLoss: 0.6444336175918579 - trainLoss: 0.6376153826713562\n",
      "cnt: 0 - valLoss: 0.6444144248962402 - trainLoss: 0.6375904679298401\n",
      "cnt: 0 - valLoss: 0.6443952918052673 - trainLoss: 0.6375657320022583\n",
      "cnt: 0 - valLoss: 0.6443762183189392 - trainLoss: 0.6375409960746765\n",
      "cnt: 0 - valLoss: 0.6443570852279663 - trainLoss: 0.6375163793563843\n",
      "cnt: 0 - valLoss: 0.6443380117416382 - trainLoss: 0.637491762638092\n",
      "cnt: 0 - valLoss: 0.6443189382553101 - trainLoss: 0.6374672055244446\n",
      "cnt: 0 - valLoss: 0.6442998647689819 - trainLoss: 0.6374425888061523\n",
      "cnt: 0 - valLoss: 0.6442807912826538 - trainLoss: 0.6374179720878601\n",
      "cnt: 0 - valLoss: 0.6442616581916809 - trainLoss: 0.6373932957649231\n",
      "cnt: 0 - valLoss: 0.644242525100708 - trainLoss: 0.6373686790466309\n",
      "cnt: 0 - valLoss: 0.6442235112190247 - trainLoss: 0.6373440027236938\n",
      "cnt: 0 - valLoss: 0.6442043781280518 - trainLoss: 0.6373193264007568\n",
      "cnt: 0 - valLoss: 0.6441853642463684 - trainLoss: 0.6372947692871094\n",
      "cnt: 0 - valLoss: 0.6441663503646851 - trainLoss: 0.6372701525688171\n",
      "cnt: 0 - valLoss: 0.6441473960876465 - trainLoss: 0.6372456550598145\n",
      "cnt: 0 - valLoss: 0.6441283822059631 - trainLoss: 0.6372212171554565\n",
      "cnt: 0 - valLoss: 0.6441095471382141 - trainLoss: 0.6371968388557434\n",
      "cnt: 0 - valLoss: 0.6440908312797546 - trainLoss: 0.637172520160675\n",
      "cnt: 0 - valLoss: 0.6440720558166504 - trainLoss: 0.6371482014656067\n",
      "cnt: 0 - valLoss: 0.6440533399581909 - trainLoss: 0.6371238827705383\n",
      "cnt: 0 - valLoss: 0.6440346240997314 - trainLoss: 0.6370996236801147\n",
      "cnt: 0 - valLoss: 0.6440161466598511 - trainLoss: 0.6370753049850464\n",
      "cnt: 0 - valLoss: 0.6439976096153259 - trainLoss: 0.6370510458946228\n",
      "cnt: 0 - valLoss: 0.6439791917800903 - trainLoss: 0.6370267868041992\n",
      "cnt: 0 - valLoss: 0.64396071434021 - trainLoss: 0.6370025873184204\n",
      "cnt: 0 - valLoss: 0.6439422965049744 - trainLoss: 0.6369785070419312\n",
      "cnt: 0 - valLoss: 0.643923819065094 - trainLoss: 0.6369544267654419\n",
      "cnt: 0 - valLoss: 0.6439054012298584 - trainLoss: 0.6369303464889526\n",
      "cnt: 0 - valLoss: 0.6438871026039124 - trainLoss: 0.6369065046310425\n",
      "cnt: 0 - valLoss: 0.6438687443733215 - trainLoss: 0.6368826031684875\n",
      "cnt: 0 - valLoss: 0.6438503861427307 - trainLoss: 0.6368587017059326\n",
      "cnt: 0 - valLoss: 0.6438320875167847 - trainLoss: 0.6368348002433777\n",
      "cnt: 0 - valLoss: 0.6438137292861938 - trainLoss: 0.6368110179901123\n",
      "cnt: 0 - valLoss: 0.6437956690788269 - trainLoss: 0.6367872357368469\n",
      "cnt: 0 - valLoss: 0.6437774896621704 - trainLoss: 0.6367634534835815\n",
      "cnt: 0 - valLoss: 0.6437593698501587 - trainLoss: 0.6367396116256714\n",
      "cnt: 0 - valLoss: 0.6437413096427917 - trainLoss: 0.636715829372406\n",
      "cnt: 0 - valLoss: 0.6437232494354248 - trainLoss: 0.6366921067237854\n",
      "cnt: 0 - valLoss: 0.6437051296234131 - trainLoss: 0.6366685032844543\n",
      "cnt: 0 - valLoss: 0.6436870694160461 - trainLoss: 0.6366448402404785\n",
      "cnt: 0 - valLoss: 0.643669068813324 - trainLoss: 0.6366212368011475\n",
      "cnt: 0 - valLoss: 0.643651008605957 - trainLoss: 0.6365976333618164\n",
      "cnt: 0 - valLoss: 0.6436329483985901 - trainLoss: 0.6365740299224854\n",
      "cnt: 0 - valLoss: 0.6436149477958679 - trainLoss: 0.6365504264831543\n",
      "cnt: 0 - valLoss: 0.6435969471931458 - trainLoss: 0.6365269422531128\n",
      "cnt: 0 - valLoss: 0.6435789465904236 - trainLoss: 0.6365033984184265\n",
      "cnt: 0 - valLoss: 0.6435610055923462 - trainLoss: 0.6364798545837402\n",
      "cnt: 0 - valLoss: 0.643543004989624 - trainLoss: 0.6364564895629883\n",
      "cnt: 0 - valLoss: 0.6435253024101257 - trainLoss: 0.6364330053329468\n",
      "cnt: 0 - valLoss: 0.6435075402259827 - trainLoss: 0.6364096999168396\n",
      "cnt: 0 - valLoss: 0.6434898376464844 - trainLoss: 0.6363864541053772\n",
      "cnt: 0 - valLoss: 0.6434721350669861 - trainLoss: 0.63636314868927\n",
      "cnt: 0 - valLoss: 0.643454372882843 - trainLoss: 0.6363399624824524\n",
      "cnt: 0 - valLoss: 0.6434367299079895 - trainLoss: 0.6363166570663452\n",
      "cnt: 0 - valLoss: 0.643419086933136 - trainLoss: 0.6362934112548828\n",
      "cnt: 0 - valLoss: 0.6434016227722168 - trainLoss: 0.6362702250480652\n",
      "cnt: 0 - valLoss: 0.6433841586112976 - trainLoss: 0.6362470388412476\n",
      "cnt: 0 - valLoss: 0.6433666348457336 - trainLoss: 0.6362238526344299\n",
      "cnt: 0 - valLoss: 0.6433492302894592 - trainLoss: 0.6362006664276123\n",
      "cnt: 0 - valLoss: 0.64333176612854 - trainLoss: 0.6361775398254395\n",
      "cnt: 0 - valLoss: 0.6433143019676208 - trainLoss: 0.6361545324325562\n",
      "cnt: 0 - valLoss: 0.6432968974113464 - trainLoss: 0.6361315250396729\n",
      "cnt: 0 - valLoss: 0.6432794332504272 - trainLoss: 0.6361084580421448\n",
      "cnt: 0 - valLoss: 0.6432620882987976 - trainLoss: 0.6360855102539062\n",
      "cnt: 0 - valLoss: 0.643244743347168 - trainLoss: 0.6360626816749573\n",
      "cnt: 0 - valLoss: 0.6432273983955383 - trainLoss: 0.6360398530960083\n",
      "cnt: 0 - valLoss: 0.6432101130485535 - trainLoss: 0.6360170841217041\n",
      "cnt: 0 - valLoss: 0.6431927680969238 - trainLoss: 0.6359942555427551\n",
      "cnt: 0 - valLoss: 0.6431754231452942 - trainLoss: 0.6359715461730957\n",
      "cnt: 0 - valLoss: 0.6431581377983093 - trainLoss: 0.6359487771987915\n",
      "cnt: 0 - valLoss: 0.6431407928466797 - trainLoss: 0.6359259486198425\n",
      "cnt: 0 - valLoss: 0.6431235074996948 - trainLoss: 0.6359032988548279\n",
      "cnt: 0 - valLoss: 0.6431062817573547 - trainLoss: 0.6358805894851685\n",
      "cnt: 0 - valLoss: 0.6430889368057251 - trainLoss: 0.6358578205108643\n",
      "cnt: 0 - valLoss: 0.6430716514587402 - trainLoss: 0.6358351707458496\n",
      "cnt: 0 - valLoss: 0.6430546045303345 - trainLoss: 0.6358125805854797\n",
      "cnt: 0 - valLoss: 0.6430376172065735 - trainLoss: 0.6357899904251099\n",
      "cnt: 0 - valLoss: 0.6430205702781677 - trainLoss: 0.6357674598693848\n",
      "cnt: 0 - valLoss: 0.643003523349762 - trainLoss: 0.6357449889183044\n",
      "cnt: 0 - valLoss: 0.642986536026001 - trainLoss: 0.6357225775718689\n",
      "cnt: 0 - valLoss: 0.64296954870224 - trainLoss: 0.6357001662254333\n",
      "cnt: 0 - valLoss: 0.6429525017738342 - trainLoss: 0.6356776356697083\n",
      "cnt: 0 - valLoss: 0.6429355144500732 - trainLoss: 0.6356552839279175\n",
      "cnt: 0 - valLoss: 0.6429186463356018 - trainLoss: 0.6356328725814819\n",
      "cnt: 0 - valLoss: 0.6429021954536438 - trainLoss: 0.6356105804443359\n",
      "cnt: 0 - valLoss: 0.6428858041763306 - trainLoss: 0.6355884075164795\n",
      "cnt: 0 - valLoss: 0.6428693532943726 - trainLoss: 0.6355661749839783\n",
      "cnt: 0 - valLoss: 0.6428530216217041 - trainLoss: 0.6355440020561218\n",
      "cnt: 0 - valLoss: 0.6428366303443909 - trainLoss: 0.6355219483375549\n",
      "cnt: 0 - valLoss: 0.6428202986717224 - trainLoss: 0.6354998350143433\n",
      "cnt: 0 - valLoss: 0.6428039073944092 - trainLoss: 0.6354778409004211\n",
      "cnt: 0 - valLoss: 0.6427875757217407 - trainLoss: 0.6354559063911438\n",
      "cnt: 0 - valLoss: 0.642771303653717 - trainLoss: 0.6354339122772217\n",
      "cnt: 0 - valLoss: 0.6427550315856934 - trainLoss: 0.6354120373725891\n",
      "cnt: 0 - valLoss: 0.6427388191223145 - trainLoss: 0.6353902220726013\n",
      "cnt: 0 - valLoss: 0.6427226662635803 - trainLoss: 0.6353684663772583\n",
      "cnt: 0 - valLoss: 0.6427066922187805 - trainLoss: 0.6353465914726257\n",
      "cnt: 0 - valLoss: 0.6426907181739807 - trainLoss: 0.6353247165679932\n",
      "cnt: 0 - valLoss: 0.6426748037338257 - trainLoss: 0.6353029608726501\n",
      "cnt: 0 - valLoss: 0.6426588892936707 - trainLoss: 0.6352812051773071\n",
      "cnt: 0 - valLoss: 0.6426429748535156 - trainLoss: 0.6352594494819641\n",
      "cnt: 0 - valLoss: 0.6426271200180054 - trainLoss: 0.6352378726005554\n",
      "cnt: 0 - valLoss: 0.6426113247871399 - trainLoss: 0.6352163553237915\n",
      "cnt: 0 - valLoss: 0.6425955295562744 - trainLoss: 0.635194718837738\n",
      "cnt: 0 - valLoss: 0.6425798535346985 - trainLoss: 0.6351732015609741\n",
      "cnt: 0 - valLoss: 0.6425641775131226 - trainLoss: 0.6351516246795654\n",
      "cnt: 0 - valLoss: 0.6425485610961914 - trainLoss: 0.6351302862167358\n",
      "cnt: 0 - valLoss: 0.6425328850746155 - trainLoss: 0.6351088881492615\n",
      "cnt: 0 - valLoss: 0.6425171494483948 - trainLoss: 0.6350875496864319\n",
      "cnt: 0 - valLoss: 0.6425014138221741 - trainLoss: 0.6350662708282471\n",
      "cnt: 0 - valLoss: 0.6424857378005981 - trainLoss: 0.6350449323654175\n",
      "cnt: 0 - valLoss: 0.6424701809883118 - trainLoss: 0.6350237131118774\n",
      "cnt: 0 - valLoss: 0.6424546241760254 - trainLoss: 0.6350025534629822\n",
      "cnt: 0 - valLoss: 0.6424391269683838 - trainLoss: 0.6349814534187317\n",
      "cnt: 0 - valLoss: 0.6424235701560974 - trainLoss: 0.6349602341651917\n",
      "cnt: 0 - valLoss: 0.6424080729484558 - trainLoss: 0.6349391341209412\n",
      "cnt: 0 - valLoss: 0.6423925757408142 - trainLoss: 0.6349180936813354\n",
      "cnt: 0 - valLoss: 0.6423770785331726 - trainLoss: 0.6348970532417297\n",
      "cnt: 0 - valLoss: 0.6423617005348206 - trainLoss: 0.6348761916160583\n",
      "cnt: 0 - valLoss: 0.6423462629318237 - trainLoss: 0.6348554491996765\n",
      "cnt: 0 - valLoss: 0.6423308253288269 - trainLoss: 0.6348346471786499\n",
      "cnt: 0 - valLoss: 0.6423153877258301 - trainLoss: 0.6348139047622681\n",
      "cnt: 0 - valLoss: 0.642300009727478 - trainLoss: 0.6347931623458862\n",
      "cnt: 0 - valLoss: 0.6422845721244812 - trainLoss: 0.6347724795341492\n",
      "cnt: 0 - valLoss: 0.6422691941261292 - trainLoss: 0.6347517371177673\n",
      "cnt: 0 - valLoss: 0.6422537565231323 - trainLoss: 0.634731113910675\n",
      "cnt: 0 - valLoss: 0.6422383785247803 - trainLoss: 0.634710431098938\n",
      "cnt: 0 - valLoss: 0.6422230005264282 - trainLoss: 0.6346898078918457\n",
      "cnt: 0 - valLoss: 0.6422077417373657 - trainLoss: 0.634669303894043\n",
      "cnt: 0 - valLoss: 0.6421923637390137 - trainLoss: 0.6346487998962402\n",
      "cnt: 0 - valLoss: 0.6421770453453064 - trainLoss: 0.6346282958984375\n",
      "cnt: 0 - valLoss: 0.6421616673469543 - trainLoss: 0.6346078515052795\n",
      "cnt: 0 - valLoss: 0.6421463489532471 - trainLoss: 0.6345874071121216\n",
      "cnt: 0 - valLoss: 0.6421310901641846 - trainLoss: 0.6345669627189636\n",
      "cnt: 0 - valLoss: 0.6421158313751221 - trainLoss: 0.6345467567443848\n",
      "cnt: 0 - valLoss: 0.6421006321907043 - trainLoss: 0.6345265507698059\n",
      "cnt: 0 - valLoss: 0.6420853734016418 - trainLoss: 0.634506344795227\n",
      "cnt: 0 - valLoss: 0.6420701146125793 - trainLoss: 0.634486198425293\n",
      "cnt: 0 - valLoss: 0.6420548558235168 - trainLoss: 0.6344659924507141\n",
      "cnt: 0 - valLoss: 0.6420396566390991 - trainLoss: 0.6344456672668457\n",
      "cnt: 0 - valLoss: 0.6420243382453918 - trainLoss: 0.6344254612922668\n",
      "cnt: 0 - valLoss: 0.6420091986656189 - trainLoss: 0.6344053149223328\n",
      "cnt: 0 - valLoss: 0.6419939398765564 - trainLoss: 0.6343851089477539\n",
      "cnt: 0 - valLoss: 0.6419786810874939 - trainLoss: 0.6343650221824646\n",
      "cnt: 0 - valLoss: 0.6419634819030762 - trainLoss: 0.6343448162078857\n",
      "cnt: 0 - valLoss: 0.6419482231140137 - trainLoss: 0.6343246698379517\n",
      "cnt: 0 - valLoss: 0.6419330835342407 - trainLoss: 0.6343045830726624\n",
      "cnt: 0 - valLoss: 0.641917884349823 - trainLoss: 0.6342845559120178\n",
      "cnt: 0 - valLoss: 0.6419026851654053 - trainLoss: 0.6342645287513733\n",
      "cnt: 0 - valLoss: 0.6418876051902771 - trainLoss: 0.6342445015907288\n",
      "cnt: 0 - valLoss: 0.6418724060058594 - trainLoss: 0.634224534034729\n",
      "cnt: 0 - valLoss: 0.6418572068214417 - trainLoss: 0.6342046856880188\n",
      "cnt: 0 - valLoss: 0.6418420076370239 - trainLoss: 0.6341847777366638\n",
      "cnt: 0 - valLoss: 0.6418267488479614 - trainLoss: 0.6341649889945984\n",
      "cnt: 0 - valLoss: 0.6418116092681885 - trainLoss: 0.6341450810432434\n",
      "cnt: 0 - valLoss: 0.6417964100837708 - trainLoss: 0.634125292301178\n",
      "cnt: 0 - valLoss: 0.6417813897132874 - trainLoss: 0.6341055035591125\n",
      "cnt: 0 - valLoss: 0.6417664289474487 - trainLoss: 0.6340856552124023\n",
      "cnt: 0 - valLoss: 0.6417515277862549 - trainLoss: 0.6340659260749817\n",
      "cnt: 0 - valLoss: 0.6417366862297058 - trainLoss: 0.6340463161468506\n",
      "cnt: 0 - valLoss: 0.6417218446731567 - trainLoss: 0.6340267658233643\n",
      "cnt: 0 - valLoss: 0.6417069435119629 - trainLoss: 0.6340071558952332\n",
      "cnt: 0 - valLoss: 0.6416921019554138 - trainLoss: 0.6339876055717468\n",
      "cnt: 0 - valLoss: 0.6416772603988647 - trainLoss: 0.6339680552482605\n",
      "cnt: 0 - valLoss: 0.6416624188423157 - trainLoss: 0.6339485049247742\n",
      "cnt: 0 - valLoss: 0.6416475772857666 - trainLoss: 0.6339289546012878\n",
      "cnt: 0 - valLoss: 0.6416326761245728 - trainLoss: 0.6339094042778015\n",
      "cnt: 0 - valLoss: 0.6416178345680237 - trainLoss: 0.63388991355896\n",
      "cnt: 0 - valLoss: 0.6416029930114746 - trainLoss: 0.6338704228401184\n",
      "cnt: 0 - valLoss: 0.6415882706642151 - trainLoss: 0.6338510513305664\n",
      "cnt: 0 - valLoss: 0.6415734887123108 - trainLoss: 0.6338316202163696\n",
      "cnt: 0 - valLoss: 0.6415588855743408 - trainLoss: 0.6338121891021729\n",
      "cnt: 0 - valLoss: 0.6415441632270813 - trainLoss: 0.6337928771972656\n",
      "cnt: 0 - valLoss: 0.6415295600891113 - trainLoss: 0.6337735056877136\n",
      "cnt: 0 - valLoss: 0.6415148973464966 - trainLoss: 0.6337541937828064\n",
      "cnt: 0 - valLoss: 0.6415002346038818 - trainLoss: 0.6337348818778992\n",
      "cnt: 0 - valLoss: 0.6414855718612671 - trainLoss: 0.6337156295776367\n",
      "cnt: 0 - valLoss: 0.6414710283279419 - trainLoss: 0.6336963176727295\n",
      "cnt: 0 - valLoss: 0.6414563655853271 - trainLoss: 0.6336771845817566\n",
      "cnt: 0 - valLoss: 0.641441822052002 - trainLoss: 0.6336581707000732\n",
      "cnt: 0 - valLoss: 0.641427218914032 - trainLoss: 0.6336392164230347\n",
      "cnt: 0 - valLoss: 0.6414127349853516 - trainLoss: 0.6336202621459961\n",
      "cnt: 0 - valLoss: 0.6413980722427368 - trainLoss: 0.6336013078689575\n",
      "cnt: 0 - valLoss: 0.6413835883140564 - trainLoss: 0.6335824131965637\n",
      "cnt: 0 - valLoss: 0.6413689851760864 - trainLoss: 0.6335635185241699\n",
      "cnt: 0 - valLoss: 0.641354501247406 - trainLoss: 0.6335446834564209\n",
      "cnt: 0 - valLoss: 0.6413399577140808 - trainLoss: 0.6335260272026062\n",
      "cnt: 0 - valLoss: 0.6413255333900452 - trainLoss: 0.633507251739502\n",
      "cnt: 0 - valLoss: 0.64131098985672 - trainLoss: 0.6334885358810425\n",
      "cnt: 0 - valLoss: 0.6412964463233948 - trainLoss: 0.633469820022583\n",
      "cnt: 0 - valLoss: 0.6412819623947144 - trainLoss: 0.6334511041641235\n",
      "cnt: 0 - valLoss: 0.6412675976753235 - trainLoss: 0.6334324479103088\n",
      "cnt: 0 - valLoss: 0.6412533521652222 - trainLoss: 0.6334137916564941\n",
      "cnt: 0 - valLoss: 0.6412391066551208 - trainLoss: 0.633395254611969\n",
      "cnt: 0 - valLoss: 0.6412249207496643 - trainLoss: 0.6333767175674438\n",
      "cnt: 0 - valLoss: 0.641210675239563 - trainLoss: 0.6333581805229187\n",
      "cnt: 0 - valLoss: 0.6411964893341064 - trainLoss: 0.6333396434783936\n",
      "cnt: 0 - valLoss: 0.6411822438240051 - trainLoss: 0.6333211660385132\n",
      "cnt: 0 - valLoss: 0.6411680579185486 - trainLoss: 0.633302628993988\n",
      "cnt: 0 - valLoss: 0.6411536335945129 - trainLoss: 0.6332840919494629\n",
      "cnt: 0 - valLoss: 0.6411392688751221 - trainLoss: 0.6332656145095825\n",
      "cnt: 0 - valLoss: 0.6411248445510864 - trainLoss: 0.6332471966743469\n",
      "cnt: 0 - valLoss: 0.6411104798316956 - trainLoss: 0.6332287192344666\n",
      "cnt: 0 - valLoss: 0.6410961151123047 - trainLoss: 0.6332102417945862\n",
      "cnt: 0 - valLoss: 0.6410817503929138 - trainLoss: 0.6331918239593506\n",
      "cnt: 0 - valLoss: 0.641067385673523 - trainLoss: 0.633173406124115\n",
      "cnt: 0 - valLoss: 0.6410530805587769 - trainLoss: 0.6331549882888794\n",
      "cnt: 0 - valLoss: 0.6410387754440308 - trainLoss: 0.6331365704536438\n",
      "cnt: 0 - valLoss: 0.6410245299339294 - trainLoss: 0.6331180930137634\n",
      "cnt: 0 - valLoss: 0.6410102248191833 - trainLoss: 0.6330996155738831\n",
      "cnt: 0 - valLoss: 0.640995979309082 - trainLoss: 0.6330811381340027\n",
      "cnt: 0 - valLoss: 0.6409816741943359 - trainLoss: 0.6330626606941223\n",
      "cnt: 0 - valLoss: 0.6409674882888794 - trainLoss: 0.6330442428588867\n",
      "cnt: 0 - valLoss: 0.6409532427787781 - trainLoss: 0.6330258846282959\n",
      "cnt: 0 - valLoss: 0.6409391760826111 - trainLoss: 0.6330075263977051\n",
      "cnt: 0 - valLoss: 0.6409252285957336 - trainLoss: 0.6329891681671143\n",
      "cnt: 0 - valLoss: 0.6409112215042114 - trainLoss: 0.6329708099365234\n",
      "cnt: 0 - valLoss: 0.6408971548080444 - trainLoss: 0.6329525113105774\n",
      "cnt: 0 - valLoss: 0.6408830881118774 - trainLoss: 0.6329341530799866\n",
      "cnt: 0 - valLoss: 0.6408689618110657 - trainLoss: 0.6329158544540405\n",
      "cnt: 0 - valLoss: 0.6408547163009644 - trainLoss: 0.6328974962234497\n",
      "cnt: 0 - valLoss: 0.6408405303955078 - trainLoss: 0.6328791379928589\n",
      "cnt: 0 - valLoss: 0.6408262848854065 - trainLoss: 0.6328607797622681\n",
      "cnt: 0 - valLoss: 0.64081209897995 - trainLoss: 0.6328426003456116\n",
      "cnt: 0 - valLoss: 0.6407979130744934 - trainLoss: 0.6328244209289551\n",
      "cnt: 0 - valLoss: 0.6407836675643921 - trainLoss: 0.6328061819076538\n",
      "cnt: 0 - valLoss: 0.6407695412635803 - trainLoss: 0.6327878832817078\n",
      "cnt: 0 - valLoss: 0.640755295753479 - trainLoss: 0.6327696442604065\n",
      "cnt: 0 - valLoss: 0.6407411694526672 - trainLoss: 0.6327515244483948\n",
      "cnt: 0 - valLoss: 0.6407269835472107 - trainLoss: 0.6327333450317383\n",
      "cnt: 0 - valLoss: 0.6407129764556885 - trainLoss: 0.6327151656150818\n",
      "cnt: 0 - valLoss: 0.6406989693641663 - trainLoss: 0.6326972246170044\n",
      "cnt: 0 - valLoss: 0.6406850218772888 - trainLoss: 0.6326793432235718\n",
      "cnt: 0 - valLoss: 0.6406709551811218 - trainLoss: 0.6326615214347839\n",
      "cnt: 0 - valLoss: 0.6406569480895996 - trainLoss: 0.6326436996459961\n",
      "cnt: 0 - valLoss: 0.6406429409980774 - trainLoss: 0.6326258182525635\n",
      "cnt: 0 - valLoss: 0.6406289339065552 - trainLoss: 0.6326079964637756\n",
      "cnt: 0 - valLoss: 0.6406148672103882 - trainLoss: 0.6325901746749878\n",
      "cnt: 0 - valLoss: 0.6406010985374451 - trainLoss: 0.6325724720954895\n",
      "cnt: 0 - valLoss: 0.6405874490737915 - trainLoss: 0.632555365562439\n",
      "cnt: 0 - valLoss: 0.6405737996101379 - trainLoss: 0.6325386166572571\n",
      "cnt: 0 - valLoss: 0.6405602097511292 - trainLoss: 0.6325218677520752\n",
      "cnt: 0 - valLoss: 0.6405466794967651 - trainLoss: 0.6325051784515381\n",
      "cnt: 0 - valLoss: 0.6405332088470459 - trainLoss: 0.6324885487556458\n",
      "cnt: 0 - valLoss: 0.6405196189880371 - trainLoss: 0.6324718594551086\n",
      "cnt: 0 - valLoss: 0.6405060887336731 - trainLoss: 0.6324552297592163\n",
      "cnt: 0 - valLoss: 0.6404925584793091 - trainLoss: 0.632438600063324\n",
      "cnt: 0 - valLoss: 0.6404790282249451 - trainLoss: 0.6324220299720764\n",
      "cnt: 0 - valLoss: 0.640465497970581 - trainLoss: 0.6324054002761841\n",
      "cnt: 0 - valLoss: 0.6404520273208618 - trainLoss: 0.6323888301849365\n",
      "cnt: 0 - valLoss: 0.6404384970664978 - trainLoss: 0.6323722004890442\n",
      "cnt: 0 - valLoss: 0.6404250264167786 - trainLoss: 0.6323556303977966\n",
      "cnt: 0 - valLoss: 0.6404116153717041 - trainLoss: 0.6323391795158386\n",
      "cnt: 0 - valLoss: 0.6403982043266296 - trainLoss: 0.6323227286338806\n",
      "cnt: 0 - valLoss: 0.6403847932815552 - trainLoss: 0.6323063373565674\n",
      "cnt: 0 - valLoss: 0.6403713226318359 - trainLoss: 0.6322898268699646\n",
      "cnt: 0 - valLoss: 0.6403579115867615 - trainLoss: 0.6322733759880066\n",
      "cnt: 0 - valLoss: 0.6403444409370422 - trainLoss: 0.6322569251060486\n",
      "cnt: 0 - valLoss: 0.640330970287323 - trainLoss: 0.6322404742240906\n",
      "cnt: 0 - valLoss: 0.640317440032959 - trainLoss: 0.6322240829467773\n",
      "cnt: 0 - valLoss: 0.6403039693832397 - trainLoss: 0.6322076916694641\n",
      "cnt: 0 - valLoss: 0.6402904987335205 - trainLoss: 0.6321911811828613\n",
      "cnt: 0 - valLoss: 0.6402770280838013 - trainLoss: 0.6321748495101929\n",
      "cnt: 0 - valLoss: 0.6402636170387268 - trainLoss: 0.6321583986282349\n",
      "cnt: 0 - valLoss: 0.6402501463890076 - trainLoss: 0.6321420073509216\n",
      "cnt: 0 - valLoss: 0.6402366757392883 - trainLoss: 0.6321256756782532\n",
      "cnt: 0 - valLoss: 0.6402232646942139 - trainLoss: 0.6321094632148743\n",
      "cnt: 0 - valLoss: 0.6402097940444946 - trainLoss: 0.6320931911468506\n",
      "cnt: 0 - valLoss: 0.6401963829994202 - trainLoss: 0.6320769786834717\n",
      "cnt: 0 - valLoss: 0.6401829123497009 - trainLoss: 0.632060706615448\n",
      "cnt: 0 - valLoss: 0.6401695013046265 - trainLoss: 0.6320444941520691\n",
      "cnt: 0 - valLoss: 0.6401560306549072 - trainLoss: 0.6320282220840454\n",
      "cnt: 0 - valLoss: 0.6401426196098328 - trainLoss: 0.6320120096206665\n",
      "cnt: 0 - valLoss: 0.6401291489601135 - trainLoss: 0.6319957971572876\n",
      "cnt: 0 - valLoss: 0.6401157379150391 - trainLoss: 0.6319796442985535\n",
      "cnt: 0 - valLoss: 0.6401023268699646 - trainLoss: 0.6319634318351746\n",
      "cnt: 0 - valLoss: 0.6400888562202454 - trainLoss: 0.6319472789764404\n",
      "cnt: 0 - valLoss: 0.6400754451751709 - trainLoss: 0.6319310665130615\n",
      "cnt: 0 - valLoss: 0.6400620341300964 - trainLoss: 0.6319147944450378\n",
      "cnt: 0 - valLoss: 0.640048623085022 - trainLoss: 0.6318985223770142\n",
      "cnt: 0 - valLoss: 0.6400351524353027 - trainLoss: 0.6318823099136353\n",
      "cnt: 0 - valLoss: 0.6400217413902283 - trainLoss: 0.6318661570549011\n",
      "cnt: 0 - valLoss: 0.6400084495544434 - trainLoss: 0.631850004196167\n",
      "cnt: 0 - valLoss: 0.6399950385093689 - trainLoss: 0.6318337917327881\n",
      "cnt: 0 - valLoss: 0.6399816870689392 - trainLoss: 0.6318176984786987\n",
      "cnt: 0 - valLoss: 0.6399682760238647 - trainLoss: 0.6318016052246094\n",
      "cnt: 0 - valLoss: 0.6399549841880798 - trainLoss: 0.63178551197052\n",
      "cnt: 0 - valLoss: 0.6399416327476501 - trainLoss: 0.6317693591117859\n",
      "cnt: 0 - valLoss: 0.6399282217025757 - trainLoss: 0.6317532658576965\n",
      "cnt: 0 - valLoss: 0.639914870262146 - trainLoss: 0.6317371726036072\n",
      "cnt: 0 - valLoss: 0.6399015188217163 - trainLoss: 0.6317210793495178\n",
      "cnt: 0 - valLoss: 0.6398881673812866 - trainLoss: 0.6317049860954285\n",
      "cnt: 0 - valLoss: 0.6398748159408569 - trainLoss: 0.6316888928413391\n",
      "cnt: 0 - valLoss: 0.6398614645004272 - trainLoss: 0.6316728591918945\n",
      "cnt: 0 - valLoss: 0.6398481130599976 - trainLoss: 0.6316567659378052\n",
      "cnt: 0 - valLoss: 0.6398347020149231 - trainLoss: 0.631640613079071\n",
      "cnt: 0 - valLoss: 0.6398213505744934 - trainLoss: 0.6316245198249817\n",
      "cnt: 0 - valLoss: 0.6398079991340637 - trainLoss: 0.6316084265708923\n",
      "cnt: 0 - valLoss: 0.6397947072982788 - trainLoss: 0.631592333316803\n",
      "cnt: 0 - valLoss: 0.6397814154624939 - trainLoss: 0.6315762400627136\n",
      "cnt: 0 - valLoss: 0.6397681832313538 - trainLoss: 0.631560206413269\n",
      "cnt: 0 - valLoss: 0.6397548913955688 - trainLoss: 0.6315441131591797\n",
      "cnt: 0 - valLoss: 0.6397416591644287 - trainLoss: 0.6315280795097351\n",
      "cnt: 0 - valLoss: 0.6397284269332886 - trainLoss: 0.6315119862556458\n",
      "cnt: 0 - valLoss: 0.6397151350975037 - trainLoss: 0.6314959526062012\n",
      "cnt: 0 - valLoss: 0.6397019028663635 - trainLoss: 0.6314799189567566\n",
      "cnt: 0 - valLoss: 0.6396886706352234 - trainLoss: 0.631463885307312\n",
      "cnt: 0 - valLoss: 0.6396753787994385 - trainLoss: 0.6314477920532227\n",
      "cnt: 0 - valLoss: 0.6396621465682983 - trainLoss: 0.6314318180084229\n",
      "cnt: 0 - valLoss: 0.6396488547325134 - trainLoss: 0.6314157247543335\n",
      "cnt: 0 - valLoss: 0.6396356821060181 - trainLoss: 0.6313996911048889\n",
      "cnt: 0 - valLoss: 0.6396223902702332 - trainLoss: 0.6313837170600891\n",
      "cnt: 0 - valLoss: 0.639609158039093 - trainLoss: 0.6313676834106445\n",
      "cnt: 0 - valLoss: 0.6395958662033081 - trainLoss: 0.6313517093658447\n",
      "cnt: 0 - valLoss: 0.6395827531814575 - trainLoss: 0.6313357353210449\n",
      "cnt: 0 - valLoss: 0.6395694613456726 - trainLoss: 0.6313197612762451\n",
      "cnt: 0 - valLoss: 0.639556348323822 - trainLoss: 0.6313039064407349\n",
      "cnt: 0 - valLoss: 0.6395432353019714 - trainLoss: 0.6312880516052246\n",
      "cnt: 0 - valLoss: 0.6395300626754761 - trainLoss: 0.6312723159790039\n",
      "cnt: 0 - valLoss: 0.6395168900489807 - trainLoss: 0.6312565803527832\n",
      "cnt: 0 - valLoss: 0.6395037174224854 - trainLoss: 0.6312408447265625\n",
      "cnt: 0 - valLoss: 0.6394906044006348 - trainLoss: 0.631225049495697\n",
      "cnt: 0 - valLoss: 0.6394774913787842 - trainLoss: 0.6312092542648315\n",
      "cnt: 0 - valLoss: 0.6394643187522888 - trainLoss: 0.6311934590339661\n",
      "cnt: 0 - valLoss: 0.6394511461257935 - trainLoss: 0.6311777234077454\n",
      "cnt: 0 - valLoss: 0.6394379734992981 - trainLoss: 0.6311620473861694\n",
      "cnt: 0 - valLoss: 0.6394248604774475 - trainLoss: 0.631146252155304\n",
      "cnt: 0 - valLoss: 0.6394117474555969 - trainLoss: 0.631130576133728\n",
      "cnt: 0 - valLoss: 0.6393985748291016 - trainLoss: 0.6311147809028625\n",
      "cnt: 0 - valLoss: 0.639385461807251 - trainLoss: 0.6310990452766418\n",
      "cnt: 0 - valLoss: 0.6393722891807556 - trainLoss: 0.6310833096504211\n",
      "cnt: 0 - valLoss: 0.6393591165542603 - trainLoss: 0.6310676336288452\n",
      "cnt: 0 - valLoss: 0.6393460035324097 - trainLoss: 0.6310518383979797\n",
      "cnt: 0 - valLoss: 0.6393328309059143 - trainLoss: 0.6310361623764038\n",
      "cnt: 0 - valLoss: 0.639319658279419 - trainLoss: 0.6310203671455383\n",
      "cnt: 0 - valLoss: 0.6393065452575684 - trainLoss: 0.6310047507286072\n",
      "cnt: 0 - valLoss: 0.6392934322357178 - trainLoss: 0.6309889554977417\n",
      "cnt: 0 - valLoss: 0.6392803192138672 - trainLoss: 0.6309732794761658\n",
      "cnt: 0 - valLoss: 0.6392672061920166 - trainLoss: 0.6309576034545898\n",
      "cnt: 0 - valLoss: 0.639254093170166 - trainLoss: 0.6309420466423035\n",
      "cnt: 0 - valLoss: 0.6392409205436707 - trainLoss: 0.6309264302253723\n",
      "cnt: 0 - valLoss: 0.6392278075218201 - trainLoss: 0.6309108734130859\n",
      "cnt: 0 - valLoss: 0.6392145752906799 - trainLoss: 0.6308953166007996\n",
      "cnt: 0 - valLoss: 0.6392014622688293 - trainLoss: 0.6308797597885132\n",
      "cnt: 0 - valLoss: 0.6391883492469788 - trainLoss: 0.630864143371582\n",
      "cnt: 0 - valLoss: 0.6391751766204834 - trainLoss: 0.6308485865592957\n",
      "cnt: 0 - valLoss: 0.6391620635986328 - trainLoss: 0.6308330297470093\n",
      "cnt: 0 - valLoss: 0.6391489505767822 - trainLoss: 0.6308175325393677\n",
      "cnt: 0 - valLoss: 0.6391358375549316 - trainLoss: 0.6308020353317261\n",
      "cnt: 0 - valLoss: 0.639122724533081 - trainLoss: 0.6307865381240845\n",
      "cnt: 0 - valLoss: 0.6391095519065857 - trainLoss: 0.6307709217071533\n",
      "cnt: 0 - valLoss: 0.6390964388847351 - trainLoss: 0.6307554244995117\n",
      "cnt: 0 - valLoss: 0.6390833258628845 - trainLoss: 0.6307399868965149\n",
      "cnt: 0 - valLoss: 0.6390702724456787 - trainLoss: 0.6307244896888733\n",
      "cnt: 0 - valLoss: 0.6390572190284729 - trainLoss: 0.6307089924812317\n",
      "cnt: 0 - valLoss: 0.6390441060066223 - trainLoss: 0.6306934952735901\n",
      "cnt: 0 - valLoss: 0.6390309929847717 - trainLoss: 0.6306780576705933\n",
      "cnt: 0 - valLoss: 0.6390178799629211 - trainLoss: 0.6306625604629517\n",
      "cnt: 0 - valLoss: 0.6390047669410706 - trainLoss: 0.6306471228599548\n",
      "cnt: 0 - valLoss: 0.63899165391922 - trainLoss: 0.630631685256958\n",
      "cnt: 0 - valLoss: 0.6389786005020142 - trainLoss: 0.6306161880493164\n",
      "cnt: 0 - valLoss: 0.6389654874801636 - trainLoss: 0.6306007504463196\n",
      "cnt: 0 - valLoss: 0.638952374458313 - trainLoss: 0.630585253238678\n",
      "cnt: 0 - valLoss: 0.6389392614364624 - trainLoss: 0.6305698156356812\n",
      "cnt: 0 - valLoss: 0.6389261484146118 - trainLoss: 0.6305543780326843\n",
      "cnt: 0 - valLoss: 0.6389130353927612 - trainLoss: 0.6305389404296875\n",
      "cnt: 0 - valLoss: 0.6388999819755554 - trainLoss: 0.6305234432220459\n",
      "cnt: 0 - valLoss: 0.6388868689537048 - trainLoss: 0.6305080652236938\n",
      "cnt: 0 - valLoss: 0.6388737559318542 - trainLoss: 0.630492627620697\n",
      "cnt: 0 - valLoss: 0.6388606429100037 - trainLoss: 0.630477249622345\n",
      "cnt: 0 - valLoss: 0.6388475298881531 - trainLoss: 0.6304618716239929\n",
      "cnt: 0 - valLoss: 0.6388344168663025 - trainLoss: 0.6304464936256409\n",
      "cnt: 0 - valLoss: 0.6388213038444519 - trainLoss: 0.6304311752319336\n",
      "cnt: 0 - valLoss: 0.6388081908226013 - trainLoss: 0.6304157972335815\n",
      "cnt: 0 - valLoss: 0.6387950778007507 - trainLoss: 0.6304004788398743\n",
      "cnt: 0 - valLoss: 0.6387819647789001 - trainLoss: 0.630385160446167\n",
      "cnt: 0 - valLoss: 0.6387688517570496 - trainLoss: 0.6303697824478149\n",
      "cnt: 0 - valLoss: 0.638755738735199 - trainLoss: 0.6303544640541077\n",
      "cnt: 0 - valLoss: 0.6387426257133484 - trainLoss: 0.6303390860557556\n",
      "cnt: 0 - valLoss: 0.638729453086853 - trainLoss: 0.6303238272666931\n",
      "cnt: 0 - valLoss: 0.6387163996696472 - trainLoss: 0.6303084492683411\n",
      "cnt: 0 - valLoss: 0.6387032866477966 - trainLoss: 0.630293071269989\n",
      "cnt: 0 - valLoss: 0.6386901140213013 - trainLoss: 0.6302778124809265\n",
      "cnt: 0 - valLoss: 0.6386770009994507 - trainLoss: 0.6302623748779297\n",
      "cnt: 0 - valLoss: 0.6386639475822449 - trainLoss: 0.6302470564842224\n",
      "cnt: 0 - valLoss: 0.6386507749557495 - trainLoss: 0.6302317380905151\n",
      "cnt: 0 - valLoss: 0.6386377215385437 - trainLoss: 0.6302163004875183\n",
      "cnt: 0 - valLoss: 0.6386245489120483 - trainLoss: 0.630200982093811\n",
      "cnt: 0 - valLoss: 0.6386114358901978 - trainLoss: 0.6301856637001038\n",
      "cnt: 0 - valLoss: 0.6385983228683472 - trainLoss: 0.6301703453063965\n",
      "cnt: 0 - valLoss: 0.6385852098464966 - trainLoss: 0.6301549673080444\n",
      "cnt: 0 - valLoss: 0.6385720372200012 - trainLoss: 0.6301396489143372\n",
      "cnt: 0 - valLoss: 0.6385589838027954 - trainLoss: 0.6301242709159851\n",
      "cnt: 0 - valLoss: 0.6385458111763 - trainLoss: 0.6301090121269226\n",
      "cnt: 0 - valLoss: 0.6385327577590942 - trainLoss: 0.6300936341285706\n",
      "cnt: 0 - valLoss: 0.6385195851325989 - trainLoss: 0.6300783753395081\n",
      "cnt: 0 - valLoss: 0.6385064125061035 - trainLoss: 0.6300631165504456\n",
      "cnt: 0 - valLoss: 0.6384932994842529 - trainLoss: 0.6300477981567383\n",
      "cnt: 0 - valLoss: 0.6384801864624023 - trainLoss: 0.6300324201583862\n",
      "cnt: 0 - valLoss: 0.638467013835907 - trainLoss: 0.630017101764679\n",
      "cnt: 0 - valLoss: 0.6384539008140564 - trainLoss: 0.6300018429756165\n",
      "cnt: 0 - valLoss: 0.6384407877922058 - trainLoss: 0.629986584186554\n",
      "cnt: 0 - valLoss: 0.6384276747703552 - trainLoss: 0.6299712657928467\n",
      "cnt: 0 - valLoss: 0.6384145617485046 - trainLoss: 0.6299559473991394\n",
      "cnt: 0 - valLoss: 0.6384013295173645 - trainLoss: 0.6299406886100769\n",
      "cnt: 0 - valLoss: 0.6383882164955139 - trainLoss: 0.6299253702163696\n",
      "cnt: 0 - valLoss: 0.6383751034736633 - trainLoss: 0.6299101114273071\n",
      "cnt: 0 - valLoss: 0.6383619904518127 - trainLoss: 0.6298948526382446\n",
      "cnt: 0 - valLoss: 0.6383488178253174 - trainLoss: 0.6298796534538269\n",
      "cnt: 0 - valLoss: 0.638335645198822 - trainLoss: 0.6298643946647644\n",
      "cnt: 0 - valLoss: 0.6383225321769714 - trainLoss: 0.6298491954803467\n",
      "cnt: 0 - valLoss: 0.6383094191551208 - trainLoss: 0.6298339366912842\n",
      "cnt: 0 - valLoss: 0.6382963061332703 - trainLoss: 0.6298186779022217\n",
      "cnt: 0 - valLoss: 0.6382831335067749 - trainLoss: 0.629803478717804\n",
      "cnt: 0 - valLoss: 0.6382700204849243 - trainLoss: 0.6297882199287415\n",
      "cnt: 0 - valLoss: 0.6382569074630737 - trainLoss: 0.6297730207443237\n",
      "cnt: 0 - valLoss: 0.6382437944412231 - trainLoss: 0.629757821559906\n",
      "cnt: 0 - valLoss: 0.6382306814193726 - trainLoss: 0.6297426223754883\n",
      "cnt: 0 - valLoss: 0.638217568397522 - trainLoss: 0.6297273635864258\n",
      "cnt: 0 - valLoss: 0.6382043361663818 - trainLoss: 0.6297122240066528\n",
      "cnt: 0 - valLoss: 0.6381911635398865 - trainLoss: 0.6296970248222351\n",
      "cnt: 0 - valLoss: 0.6381779909133911 - trainLoss: 0.6296818852424622\n",
      "cnt: 0 - valLoss: 0.6381648182868958 - trainLoss: 0.6296666860580444\n",
      "cnt: 0 - valLoss: 0.6381516456604004 - trainLoss: 0.6296514868736267\n",
      "cnt: 0 - valLoss: 0.6381384134292603 - trainLoss: 0.6296363472938538\n",
      "cnt: 0 - valLoss: 0.6381252408027649 - trainLoss: 0.629621148109436\n",
      "cnt: 0 - valLoss: 0.6381120681762695 - trainLoss: 0.6296060085296631\n",
      "cnt: 0 - valLoss: 0.6380988955497742 - trainLoss: 0.6295908689498901\n",
      "cnt: 0 - valLoss: 0.6380857229232788 - trainLoss: 0.6295756697654724\n",
      "cnt: 0 - valLoss: 0.6380724906921387 - trainLoss: 0.6295605301856995\n",
      "cnt: 0 - valLoss: 0.6380593180656433 - trainLoss: 0.6295453906059265\n",
      "cnt: 0 - valLoss: 0.638046145439148 - trainLoss: 0.6295301914215088\n",
      "cnt: 0 - valLoss: 0.6380329132080078 - trainLoss: 0.6295151114463806\n",
      "cnt: 0 - valLoss: 0.6380196809768677 - trainLoss: 0.6294999718666077\n",
      "cnt: 0 - valLoss: 0.6380065083503723 - trainLoss: 0.6294848322868347\n",
      "cnt: 0 - valLoss: 0.6379932761192322 - trainLoss: 0.6294696927070618\n",
      "cnt: 0 - valLoss: 0.6379801034927368 - trainLoss: 0.6294545531272888\n",
      "cnt: 0 - valLoss: 0.6379668712615967 - trainLoss: 0.6294394135475159\n",
      "cnt: 0 - valLoss: 0.6379536986351013 - trainLoss: 0.6294242739677429\n",
      "cnt: 0 - valLoss: 0.6379404664039612 - trainLoss: 0.6294091939926147\n",
      "cnt: 0 - valLoss: 0.6379272937774658 - trainLoss: 0.629393994808197\n",
      "cnt: 0 - valLoss: 0.6379140615463257 - trainLoss: 0.6293789148330688\n",
      "cnt: 0 - valLoss: 0.6379008889198303 - trainLoss: 0.6293637752532959\n",
      "cnt: 0 - valLoss: 0.6378876566886902 - trainLoss: 0.629348635673523\n",
      "cnt: 0 - valLoss: 0.63787442445755 - trainLoss: 0.6293335556983948\n",
      "cnt: 0 - valLoss: 0.6378612518310547 - trainLoss: 0.6293184161186218\n",
      "cnt: 0 - valLoss: 0.6378480195999146 - trainLoss: 0.6293033361434937\n",
      "cnt: 0 - valLoss: 0.6378348469734192 - trainLoss: 0.6292881965637207\n",
      "cnt: 0 - valLoss: 0.637821614742279 - trainLoss: 0.6292730569839478\n",
      "cnt: 0 - valLoss: 0.6378083825111389 - trainLoss: 0.6292579770088196\n",
      "cnt: 0 - valLoss: 0.6377952098846436 - trainLoss: 0.6292428374290466\n",
      "cnt: 0 - valLoss: 0.6377819776535034 - trainLoss: 0.6292277574539185\n",
      "cnt: 0 - valLoss: 0.6377688050270081 - trainLoss: 0.6292126774787903\n",
      "cnt: 0 - valLoss: 0.6377555727958679 - trainLoss: 0.6291975378990173\n",
      "cnt: 0 - valLoss: 0.637742280960083 - trainLoss: 0.6291825175285339\n",
      "cnt: 0 - valLoss: 0.6377291083335876 - trainLoss: 0.6291673183441162\n",
      "cnt: 0 - valLoss: 0.6377158761024475 - trainLoss: 0.6291522979736328\n",
      "cnt: 0 - valLoss: 0.6377027034759521 - trainLoss: 0.6291370987892151\n",
      "cnt: 0 - valLoss: 0.6376894116401672 - trainLoss: 0.6291220784187317\n",
      "cnt: 0 - valLoss: 0.6376761794090271 - trainLoss: 0.6291069984436035\n",
      "cnt: 0 - valLoss: 0.6376630067825317 - trainLoss: 0.6290918588638306\n",
      "cnt: 0 - valLoss: 0.6376497745513916 - trainLoss: 0.6290768384933472\n",
      "cnt: 0 - valLoss: 0.6376366019248962 - trainLoss: 0.6290618181228638\n",
      "cnt: 0 - valLoss: 0.6376234292984009 - trainLoss: 0.629046618938446\n",
      "cnt: 0 - valLoss: 0.6376101970672607 - trainLoss: 0.6290315985679626\n",
      "cnt: 0 - valLoss: 0.6375970244407654 - trainLoss: 0.6290165185928345\n",
      "cnt: 0 - valLoss: 0.63758385181427 - trainLoss: 0.6290014386177063\n",
      "cnt: 0 - valLoss: 0.6375706195831299 - trainLoss: 0.6289863586425781\n",
      "cnt: 0 - valLoss: 0.6375574469566345 - trainLoss: 0.6289713382720947\n",
      "cnt: 0 - valLoss: 0.6375442743301392 - trainLoss: 0.6289562582969666\n",
      "cnt: 0 - valLoss: 0.637531042098999 - trainLoss: 0.6289411783218384\n",
      "cnt: 0 - valLoss: 0.6375178694725037 - trainLoss: 0.6289260983467102\n",
      "cnt: 0 - valLoss: 0.6375046372413635 - trainLoss: 0.628911018371582\n",
      "cnt: 0 - valLoss: 0.6374914646148682 - trainLoss: 0.6288959980010986\n",
      "cnt: 0 - valLoss: 0.637478232383728 - trainLoss: 0.6288809180259705\n",
      "cnt: 0 - valLoss: 0.6374650597572327 - trainLoss: 0.6288658380508423\n",
      "cnt: 0 - valLoss: 0.6374518871307373 - trainLoss: 0.6288507580757141\n",
      "cnt: 0 - valLoss: 0.6374386548995972 - trainLoss: 0.6288357377052307\n",
      "cnt: 0 - valLoss: 0.6374254822731018 - trainLoss: 0.6288207173347473\n",
      "cnt: 0 - valLoss: 0.6374122500419617 - trainLoss: 0.6288056373596191\n",
      "cnt: 0 - valLoss: 0.6373990774154663 - trainLoss: 0.6287906169891357\n",
      "cnt: 0 - valLoss: 0.6373858451843262 - trainLoss: 0.6287755370140076\n",
      "cnt: 0 - valLoss: 0.6373726725578308 - trainLoss: 0.6287605166435242\n",
      "cnt: 0 - valLoss: 0.6373594403266907 - trainLoss: 0.6287454962730408\n",
      "cnt: 0 - valLoss: 0.6373462677001953 - trainLoss: 0.6287304162979126\n",
      "cnt: 0 - valLoss: 0.6373329758644104 - trainLoss: 0.6287153363227844\n",
      "cnt: 0 - valLoss: 0.637319803237915 - trainLoss: 0.6287003755569458\n",
      "cnt: 0 - valLoss: 0.6373065710067749 - trainLoss: 0.6286853551864624\n",
      "cnt: 0 - valLoss: 0.6372933983802795 - trainLoss: 0.6286702752113342\n",
      "cnt: 0 - valLoss: 0.6372801661491394 - trainLoss: 0.6286552548408508\n",
      "cnt: 0 - valLoss: 0.6372669339179993 - trainLoss: 0.6286401748657227\n",
      "cnt: 0 - valLoss: 0.6372537612915039 - trainLoss: 0.628625214099884\n",
      "cnt: 0 - valLoss: 0.6372405290603638 - trainLoss: 0.6286101341247559\n",
      "cnt: 0 - valLoss: 0.6372273564338684 - trainLoss: 0.6285951733589172\n",
      "cnt: 0 - valLoss: 0.6372140645980835 - trainLoss: 0.6285800933837891\n",
      "cnt: 0 - valLoss: 0.6372008323669434 - trainLoss: 0.6285650730133057\n",
      "cnt: 0 - valLoss: 0.637187659740448 - trainLoss: 0.6285500526428223\n",
      "cnt: 0 - valLoss: 0.6371744275093079 - trainLoss: 0.6285350918769836\n",
      "cnt: 0 - valLoss: 0.6371611952781677 - trainLoss: 0.6285200715065002\n",
      "cnt: 0 - valLoss: 0.6371479630470276 - trainLoss: 0.6285050511360168\n",
      "cnt: 0 - valLoss: 0.6371347904205322 - trainLoss: 0.6284900307655334\n",
      "cnt: 0 - valLoss: 0.6371215581893921 - trainLoss: 0.6284749507904053\n",
      "cnt: 0 - valLoss: 0.6371082663536072 - trainLoss: 0.6284599900245667\n",
      "cnt: 0 - valLoss: 0.6370950937271118 - trainLoss: 0.6284449696540833\n",
      "cnt: 0 - valLoss: 0.6370818614959717 - trainLoss: 0.6284299492835999\n",
      "cnt: 0 - valLoss: 0.6370686292648315 - trainLoss: 0.6284149885177612\n",
      "cnt: 0 - valLoss: 0.6370554566383362 - trainLoss: 0.6283999681472778\n",
      "cnt: 0 - valLoss: 0.6370422840118408 - trainLoss: 0.6283850073814392\n",
      "cnt: 0 - valLoss: 0.6370290517807007 - trainLoss: 0.6283700466156006\n",
      "cnt: 0 - valLoss: 0.6370157599449158 - trainLoss: 0.6283550262451172\n",
      "cnt: 0 - valLoss: 0.6370025873184204 - trainLoss: 0.6283400654792786\n",
      "cnt: 0 - valLoss: 0.6369893550872803 - trainLoss: 0.6283251047134399\n",
      "cnt: 0 - valLoss: 0.6369761824607849 - trainLoss: 0.6283100843429565\n",
      "cnt: 0 - valLoss: 0.6369629502296448 - trainLoss: 0.6282951831817627\n",
      "cnt: 0 - valLoss: 0.6369497179985046 - trainLoss: 0.6282801628112793\n",
      "cnt: 0 - valLoss: 0.6369364857673645 - trainLoss: 0.6282651424407959\n",
      "cnt: 0 - valLoss: 0.6369233131408691 - trainLoss: 0.628250241279602\n",
      "cnt: 0 - valLoss: 0.6369100213050842 - trainLoss: 0.6282352209091187\n",
      "cnt: 0 - valLoss: 0.6368969082832336 - trainLoss: 0.6282203197479248\n",
      "cnt: 0 - valLoss: 0.6368836164474487 - trainLoss: 0.6282053589820862\n",
      "cnt: 0 - valLoss: 0.6368703842163086 - trainLoss: 0.6281903386116028\n",
      "cnt: 0 - valLoss: 0.6368571519851685 - trainLoss: 0.6281753778457642\n",
      "cnt: 0 - valLoss: 0.6368439197540283 - trainLoss: 0.6281604766845703\n",
      "cnt: 0 - valLoss: 0.6368306875228882 - trainLoss: 0.6281455159187317\n",
      "cnt: 0 - valLoss: 0.6368175148963928 - trainLoss: 0.6281305551528931\n",
      "cnt: 0 - valLoss: 0.6368042230606079 - trainLoss: 0.6281155347824097\n",
      "cnt: 0 - valLoss: 0.6367909908294678 - trainLoss: 0.628100574016571\n",
      "cnt: 0 - valLoss: 0.6367777585983276 - trainLoss: 0.6280856728553772\n",
      "cnt: 0 - valLoss: 0.6367645263671875 - trainLoss: 0.6280706524848938\n",
      "cnt: 0 - valLoss: 0.6367513537406921 - trainLoss: 0.6280557513237\n",
      "cnt: 0 - valLoss: 0.6367380619049072 - trainLoss: 0.6280407905578613\n",
      "cnt: 0 - valLoss: 0.6367248296737671 - trainLoss: 0.6280258893966675\n",
      "cnt: 0 - valLoss: 0.6367115378379822 - trainLoss: 0.6280109286308289\n",
      "cnt: 0 - valLoss: 0.6366983652114868 - trainLoss: 0.6279959678649902\n",
      "cnt: 0 - valLoss: 0.6366850733757019 - trainLoss: 0.6279810667037964\n",
      "cnt: 0 - valLoss: 0.6366718411445618 - trainLoss: 0.627966046333313\n",
      "cnt: 0 - valLoss: 0.6366585493087769 - trainLoss: 0.6279510855674744\n",
      "cnt: 0 - valLoss: 0.6366453766822815 - trainLoss: 0.6279361844062805\n",
      "cnt: 0 - valLoss: 0.6366320848464966 - trainLoss: 0.6279212236404419\n",
      "cnt: 0 - valLoss: 0.6366188526153564 - trainLoss: 0.627906322479248\n",
      "cnt: 0 - valLoss: 0.6366056203842163 - trainLoss: 0.6278913617134094\n",
      "cnt: 0 - valLoss: 0.6365923285484314 - trainLoss: 0.6278764605522156\n",
      "cnt: 0 - valLoss: 0.6365790963172913 - trainLoss: 0.627861499786377\n",
      "cnt: 0 - valLoss: 0.6365658044815063 - trainLoss: 0.6278465390205383\n",
      "cnt: 0 - valLoss: 0.636552631855011 - trainLoss: 0.6278316378593445\n",
      "cnt: 0 - valLoss: 0.6365393400192261 - trainLoss: 0.6278166770935059\n",
      "cnt: 0 - valLoss: 0.6365260481834412 - trainLoss: 0.627801775932312\n",
      "cnt: 0 - valLoss: 0.636512815952301 - trainLoss: 0.6277868747711182\n",
      "cnt: 0 - valLoss: 0.6364995837211609 - trainLoss: 0.6277719140052795\n",
      "cnt: 0 - valLoss: 0.636486291885376 - trainLoss: 0.6277570128440857\n",
      "cnt: 0 - valLoss: 0.6364730000495911 - trainLoss: 0.6277420520782471\n",
      "cnt: 0 - valLoss: 0.6364597082138062 - trainLoss: 0.6277271509170532\n",
      "cnt: 0 - valLoss: 0.636446475982666 - trainLoss: 0.6277122497558594\n",
      "cnt: 0 - valLoss: 0.6364331245422363 - trainLoss: 0.6276972889900208\n",
      "cnt: 0 - valLoss: 0.6364197731018066 - trainLoss: 0.6276823878288269\n",
      "cnt: 0 - valLoss: 0.6364064812660217 - trainLoss: 0.6276674866676331\n",
      "cnt: 0 - valLoss: 0.6363930702209473 - trainLoss: 0.6276525259017944\n",
      "cnt: 0 - valLoss: 0.6363797783851624 - trainLoss: 0.6276376247406006\n",
      "cnt: 0 - valLoss: 0.6363663673400879 - trainLoss: 0.6276227235794067\n",
      "cnt: 0 - valLoss: 0.636353075504303 - trainLoss: 0.6276078224182129\n",
      "cnt: 0 - valLoss: 0.6363396644592285 - trainLoss: 0.6275928616523743\n",
      "cnt: 0 - valLoss: 0.6363263130187988 - trainLoss: 0.6275780200958252\n",
      "cnt: 0 - valLoss: 0.6363129615783691 - trainLoss: 0.6275631189346313\n",
      "cnt: 0 - valLoss: 0.6362996101379395 - trainLoss: 0.6275481581687927\n",
      "cnt: 0 - valLoss: 0.6362862586975098 - trainLoss: 0.6275332570075989\n",
      "cnt: 0 - valLoss: 0.6362728476524353 - trainLoss: 0.627518355846405\n",
      "cnt: 0 - valLoss: 0.6362595558166504 - trainLoss: 0.6275034546852112\n",
      "cnt: 0 - valLoss: 0.6362461447715759 - trainLoss: 0.6274885535240173\n",
      "cnt: 0 - valLoss: 0.6362327933311462 - trainLoss: 0.6274736523628235\n",
      "cnt: 0 - valLoss: 0.6362193822860718 - trainLoss: 0.6274586915969849\n",
      "cnt: 0 - valLoss: 0.6362060308456421 - trainLoss: 0.627443790435791\n",
      "cnt: 0 - valLoss: 0.6361926794052124 - trainLoss: 0.6274288892745972\n",
      "cnt: 0 - valLoss: 0.6361792683601379 - trainLoss: 0.6274139881134033\n",
      "cnt: 0 - valLoss: 0.6361659169197083 - trainLoss: 0.6273991465568542\n",
      "cnt: 0 - valLoss: 0.6361525654792786 - trainLoss: 0.6273842453956604\n",
      "cnt: 0 - valLoss: 0.6361391544342041 - trainLoss: 0.6273693442344666\n",
      "cnt: 0 - valLoss: 0.6361258029937744 - trainLoss: 0.6273545026779175\n",
      "cnt: 0 - valLoss: 0.6361123919487 - trainLoss: 0.6273395419120789\n",
      "cnt: 0 - valLoss: 0.6360990405082703 - trainLoss: 0.627324640750885\n",
      "cnt: 0 - valLoss: 0.636085569858551 - trainLoss: 0.6273097991943359\n",
      "cnt: 0 - valLoss: 0.6360722184181213 - trainLoss: 0.6272948384284973\n",
      "cnt: 0 - valLoss: 0.6360588669776917 - trainLoss: 0.6272799372673035\n",
      "cnt: 0 - valLoss: 0.6360454559326172 - trainLoss: 0.6272650957107544\n",
      "cnt: 0 - valLoss: 0.6360320448875427 - trainLoss: 0.6272501945495605\n",
      "cnt: 0 - valLoss: 0.6360187530517578 - trainLoss: 0.6272352933883667\n",
      "cnt: 0 - valLoss: 0.6360053420066833 - trainLoss: 0.6272203922271729\n",
      "cnt: 0 - valLoss: 0.6359919309616089 - trainLoss: 0.6272055506706238\n",
      "cnt: 0 - valLoss: 0.6359785199165344 - trainLoss: 0.6271906495094299\n",
      "cnt: 0 - valLoss: 0.6359651684761047 - trainLoss: 0.6271758079528809\n",
      "cnt: 0 - valLoss: 0.6359516978263855 - trainLoss: 0.627160906791687\n",
      "cnt: 0 - valLoss: 0.6359383463859558 - trainLoss: 0.6271460056304932\n",
      "cnt: 0 - valLoss: 0.6359248757362366 - trainLoss: 0.6271311640739441\n",
      "cnt: 0 - valLoss: 0.6359114646911621 - trainLoss: 0.6271162629127502\n",
      "cnt: 0 - valLoss: 0.6358981132507324 - trainLoss: 0.6271013617515564\n",
      "cnt: 0 - valLoss: 0.6358846426010132 - trainLoss: 0.6270865201950073\n",
      "cnt: 0 - valLoss: 0.6358712911605835 - trainLoss: 0.6270716190338135\n",
      "cnt: 0 - valLoss: 0.635857880115509 - trainLoss: 0.6270567774772644\n",
      "cnt: 0 - valLoss: 0.6358444690704346 - trainLoss: 0.6270418763160706\n",
      "cnt: 0 - valLoss: 0.6358310580253601 - trainLoss: 0.6270270347595215\n",
      "cnt: 0 - valLoss: 0.6358176469802856 - trainLoss: 0.6270121932029724\n",
      "cnt: 0 - valLoss: 0.6358042359352112 - trainLoss: 0.6269972920417786\n",
      "cnt: 0 - valLoss: 0.6357907652854919 - trainLoss: 0.6269823908805847\n",
      "cnt: 0 - valLoss: 0.6357774138450623 - trainLoss: 0.6269674897193909\n",
      "cnt: 0 - valLoss: 0.635763943195343 - trainLoss: 0.6269526481628418\n",
      "cnt: 0 - valLoss: 0.6357505321502686 - trainLoss: 0.6269378066062927\n",
      "cnt: 0 - valLoss: 0.6357370615005493 - trainLoss: 0.6269229650497437\n",
      "cnt: 0 - valLoss: 0.6357236504554749 - trainLoss: 0.6269080638885498\n",
      "cnt: 0 - valLoss: 0.6357102394104004 - trainLoss: 0.6268932223320007\n",
      "cnt: 0 - valLoss: 0.6356967687606812 - trainLoss: 0.6268783211708069\n",
      "cnt: 0 - valLoss: 0.6356832981109619 - trainLoss: 0.6268634796142578\n",
      "cnt: 0 - valLoss: 0.6356698274612427 - trainLoss: 0.626848578453064\n",
      "cnt: 0 - valLoss: 0.6356563568115234 - trainLoss: 0.6268337368965149\n",
      "cnt: 0 - valLoss: 0.635642945766449 - trainLoss: 0.6268188953399658\n",
      "cnt: 0 - valLoss: 0.6356294751167297 - trainLoss: 0.6268040537834167\n",
      "cnt: 0 - valLoss: 0.6356160044670105 - trainLoss: 0.6267892122268677\n",
      "cnt: 0 - valLoss: 0.6356025338172913 - trainLoss: 0.6267743110656738\n",
      "cnt: 0 - valLoss: 0.6355891227722168 - trainLoss: 0.6267594695091248\n",
      "cnt: 0 - valLoss: 0.6355756521224976 - trainLoss: 0.6267445683479309\n",
      "cnt: 0 - valLoss: 0.6355621814727783 - trainLoss: 0.6267297863960266\n",
      "cnt: 0 - valLoss: 0.6355487108230591 - trainLoss: 0.6267148852348328\n",
      "cnt: 0 - valLoss: 0.6355352401733398 - trainLoss: 0.6267000436782837\n",
      "cnt: 0 - valLoss: 0.6355217695236206 - trainLoss: 0.6266852021217346\n",
      "cnt: 0 - valLoss: 0.6355083584785461 - trainLoss: 0.6266703009605408\n",
      "cnt: 0 - valLoss: 0.6354948878288269 - trainLoss: 0.6266554594039917\n",
      "cnt: 0 - valLoss: 0.6354814171791077 - trainLoss: 0.6266406774520874\n",
      "cnt: 0 - valLoss: 0.6354679465293884 - trainLoss: 0.6266257762908936\n",
      "cnt: 0 - valLoss: 0.6354544758796692 - trainLoss: 0.6266109943389893\n",
      "cnt: 0 - valLoss: 0.6354410648345947 - trainLoss: 0.6265960931777954\n",
      "cnt: 0 - valLoss: 0.6354275941848755 - trainLoss: 0.6265812516212463\n",
      "cnt: 0 - valLoss: 0.635414183139801 - trainLoss: 0.6265664100646973\n",
      "cnt: 0 - valLoss: 0.6354007124900818 - trainLoss: 0.6265515685081482\n",
      "cnt: 0 - valLoss: 0.6353873014450073 - trainLoss: 0.6265367269515991\n",
      "cnt: 0 - valLoss: 0.6353738307952881 - trainLoss: 0.62652188539505\n",
      "cnt: 0 - valLoss: 0.6353603601455688 - trainLoss: 0.626507043838501\n",
      "cnt: 0 - valLoss: 0.6353468894958496 - trainLoss: 0.6264922618865967\n",
      "cnt: 0 - valLoss: 0.6353334784507751 - trainLoss: 0.6264773607254028\n",
      "cnt: 0 - valLoss: 0.6353200078010559 - trainLoss: 0.6264625787734985\n",
      "cnt: 0 - valLoss: 0.6353065371513367 - trainLoss: 0.6264477372169495\n",
      "cnt: 0 - valLoss: 0.6352930665016174 - trainLoss: 0.6264328956604004\n",
      "cnt: 0 - valLoss: 0.6352795958518982 - trainLoss: 0.6264181137084961\n",
      "cnt: 0 - valLoss: 0.6352661848068237 - trainLoss: 0.6264032125473022\n",
      "cnt: 0 - valLoss: 0.6352527141571045 - trainLoss: 0.6263883709907532\n",
      "cnt: 0 - valLoss: 0.6352392435073853 - trainLoss: 0.6263735294342041\n",
      "cnt: 0 - valLoss: 0.635225772857666 - trainLoss: 0.626358687877655\n",
      "cnt: 0 - valLoss: 0.6352123022079468 - trainLoss: 0.6263439059257507\n",
      "cnt: 0 - valLoss: 0.6351988315582275 - trainLoss: 0.6263290643692017\n",
      "cnt: 0 - valLoss: 0.6351853609085083 - trainLoss: 0.6263142228126526\n",
      "cnt: 0 - valLoss: 0.6351718902587891 - trainLoss: 0.6262994408607483\n",
      "cnt: 0 - valLoss: 0.6351584196090698 - trainLoss: 0.6262845396995544\n",
      "cnt: 0 - valLoss: 0.6351449489593506 - trainLoss: 0.6262697577476501\n",
      "cnt: 0 - valLoss: 0.6351314783096313 - trainLoss: 0.6262549161911011\n",
      "cnt: 0 - valLoss: 0.6351180076599121 - trainLoss: 0.626240074634552\n",
      "cnt: 0 - valLoss: 0.6351045370101929 - trainLoss: 0.6262252926826477\n",
      "cnt: 0 - valLoss: 0.6350910067558289 - trainLoss: 0.6262104511260986\n",
      "cnt: 0 - valLoss: 0.6350775361061096 - trainLoss: 0.6261956095695496\n",
      "cnt: 0 - valLoss: 0.6350640058517456 - trainLoss: 0.6261808276176453\n",
      "cnt: 0 - valLoss: 0.6350505948066711 - trainLoss: 0.6261659860610962\n",
      "cnt: 0 - valLoss: 0.6350371241569519 - trainLoss: 0.6261512041091919\n",
      "cnt: 0 - valLoss: 0.6350235939025879 - trainLoss: 0.626136302947998\n",
      "cnt: 0 - valLoss: 0.6350100636482239 - trainLoss: 0.6261215209960938\n",
      "cnt: 0 - valLoss: 0.6349966526031494 - trainLoss: 0.6261067390441895\n",
      "cnt: 0 - valLoss: 0.6349831223487854 - trainLoss: 0.6260918974876404\n",
      "cnt: 0 - valLoss: 0.6349696516990662 - trainLoss: 0.6260770559310913\n",
      "cnt: 0 - valLoss: 0.6349561214447021 - trainLoss: 0.626062273979187\n",
      "cnt: 0 - valLoss: 0.6349426507949829 - trainLoss: 0.6260474324226379\n",
      "cnt: 0 - valLoss: 0.6349291205406189 - trainLoss: 0.6260326504707336\n",
      "cnt: 0 - valLoss: 0.6349156498908997 - trainLoss: 0.6260178089141846\n",
      "cnt: 0 - valLoss: 0.6349021196365356 - trainLoss: 0.6260029077529907\n",
      "cnt: 0 - valLoss: 0.6348885297775269 - trainLoss: 0.6259881258010864\n",
      "cnt: 0 - valLoss: 0.6348750591278076 - trainLoss: 0.6259732842445374\n",
      "cnt: 0 - valLoss: 0.6348614692687988 - trainLoss: 0.6259584426879883\n",
      "cnt: 0 - valLoss: 0.6348479986190796 - trainLoss: 0.6259435415267944\n",
      "cnt: 0 - valLoss: 0.6348344683647156 - trainLoss: 0.6259286999702454\n",
      "cnt: 0 - valLoss: 0.6348209381103516 - trainLoss: 0.6259138584136963\n",
      "cnt: 0 - valLoss: 0.6348073482513428 - trainLoss: 0.6258990168571472\n",
      "cnt: 0 - valLoss: 0.6347938179969788 - trainLoss: 0.6258841753005981\n",
      "cnt: 0 - valLoss: 0.6347802877426147 - trainLoss: 0.6258692741394043\n",
      "cnt: 0 - valLoss: 0.6347667574882507 - trainLoss: 0.6258544325828552\n",
      "cnt: 0 - valLoss: 0.6347531676292419 - trainLoss: 0.6258395910263062\n",
      "cnt: 0 - valLoss: 0.6347397565841675 - trainLoss: 0.6258248090744019\n",
      "cnt: 0 - valLoss: 0.6347261667251587 - trainLoss: 0.6258099675178528\n",
      "cnt: 0 - valLoss: 0.6347125768661499 - trainLoss: 0.6257950663566589\n",
      "cnt: 0 - valLoss: 0.6346990466117859 - trainLoss: 0.6257802844047546\n",
      "cnt: 0 - valLoss: 0.6346855163574219 - trainLoss: 0.6257653832435608\n",
      "cnt: 0 - valLoss: 0.6346719861030579 - trainLoss: 0.6257505416870117\n",
      "cnt: 0 - valLoss: 0.6346584558486938 - trainLoss: 0.6257357001304626\n",
      "cnt: 0 - valLoss: 0.6346448659896851 - trainLoss: 0.6257208585739136\n",
      "cnt: 0 - valLoss: 0.6346312761306763 - trainLoss: 0.6257060766220093\n",
      "cnt: 0 - valLoss: 0.6346177458763123 - trainLoss: 0.6256911754608154\n",
      "cnt: 0 - valLoss: 0.6346042156219482 - trainLoss: 0.6256763339042664\n",
      "cnt: 0 - valLoss: 0.6345906853675842 - trainLoss: 0.6256614923477173\n",
      "cnt: 0 - valLoss: 0.6345770359039307 - trainLoss: 0.6256466507911682\n",
      "cnt: 0 - valLoss: 0.6345635056495667 - trainLoss: 0.6256318092346191\n",
      "cnt: 0 - valLoss: 0.6345499157905579 - trainLoss: 0.6256170272827148\n",
      "cnt: 0 - valLoss: 0.6345363855361938 - trainLoss: 0.625602126121521\n",
      "cnt: 0 - valLoss: 0.6345227956771851 - trainLoss: 0.6255873441696167\n",
      "cnt: 0 - valLoss: 0.6345092058181763 - trainLoss: 0.6255724430084229\n",
      "cnt: 0 - valLoss: 0.6344956755638123 - trainLoss: 0.6255576014518738\n",
      "cnt: 0 - valLoss: 0.6344820857048035 - trainLoss: 0.6255428194999695\n",
      "cnt: 0 - valLoss: 0.6344684958457947 - trainLoss: 0.6255279779434204\n",
      "cnt: 0 - valLoss: 0.6344549655914307 - trainLoss: 0.6255130767822266\n",
      "cnt: 0 - valLoss: 0.6344413161277771 - trainLoss: 0.6254982352256775\n",
      "cnt: 0 - valLoss: 0.6344277262687683 - trainLoss: 0.6254834532737732\n",
      "cnt: 0 - valLoss: 0.6344141364097595 - trainLoss: 0.6254686117172241\n",
      "cnt: 0 - valLoss: 0.6344006061553955 - trainLoss: 0.625453770160675\n",
      "cnt: 0 - valLoss: 0.6343869566917419 - trainLoss: 0.625438928604126\n",
      "cnt: 0 - valLoss: 0.6343734264373779 - trainLoss: 0.6254240870475769\n",
      "cnt: 0 - valLoss: 0.6343598365783691 - trainLoss: 0.6254092454910278\n",
      "cnt: 0 - valLoss: 0.6343461871147156 - trainLoss: 0.6253944039344788\n",
      "cnt: 0 - valLoss: 0.6343326568603516 - trainLoss: 0.6253795623779297\n",
      "cnt: 0 - valLoss: 0.6343189477920532 - trainLoss: 0.6253647804260254\n",
      "cnt: 0 - valLoss: 0.6343053579330444 - trainLoss: 0.6253498792648315\n",
      "cnt: 0 - valLoss: 0.6342917680740356 - trainLoss: 0.6253350377082825\n",
      "cnt: 0 - valLoss: 0.6342781782150269 - trainLoss: 0.6253201961517334\n",
      "cnt: 0 - valLoss: 0.6342645287513733 - trainLoss: 0.6253054141998291\n",
      "cnt: 0 - valLoss: 0.6342509984970093 - trainLoss: 0.6252906322479248\n",
      "cnt: 0 - valLoss: 0.6342374086380005 - trainLoss: 0.6252758502960205\n",
      "cnt: 0 - valLoss: 0.6342236995697021 - trainLoss: 0.6252610683441162\n",
      "cnt: 0 - valLoss: 0.6342101097106934 - trainLoss: 0.6252462863922119\n",
      "cnt: 0 - valLoss: 0.6341965198516846 - trainLoss: 0.6252314448356628\n",
      "cnt: 0 - valLoss: 0.634182870388031 - trainLoss: 0.6252167224884033\n",
      "cnt: 0 - valLoss: 0.634169340133667 - trainLoss: 0.6252018809318542\n",
      "cnt: 0 - valLoss: 0.6341556906700134 - trainLoss: 0.6251871585845947\n",
      "cnt: 0 - valLoss: 0.6341421008110046 - trainLoss: 0.6251723170280457\n",
      "cnt: 0 - valLoss: 0.6341284513473511 - trainLoss: 0.6251575350761414\n",
      "cnt: 0 - valLoss: 0.6341148018836975 - trainLoss: 0.6251426935195923\n",
      "cnt: 0 - valLoss: 0.634101152420044 - trainLoss: 0.625127911567688\n",
      "cnt: 0 - valLoss: 0.6340875625610352 - trainLoss: 0.6251131296157837\n",
      "cnt: 0 - valLoss: 0.6340739130973816 - trainLoss: 0.6250983476638794\n",
      "cnt: 0 - valLoss: 0.634060263633728 - trainLoss: 0.6250835657119751\n",
      "cnt: 0 - valLoss: 0.6340465545654297 - trainLoss: 0.6250687837600708\n",
      "cnt: 0 - valLoss: 0.6340330243110657 - trainLoss: 0.6250540018081665\n",
      "cnt: 0 - valLoss: 0.6340193748474121 - trainLoss: 0.6250391602516174\n",
      "cnt: 0 - valLoss: 0.6340057253837585 - trainLoss: 0.6250243782997131\n",
      "cnt: 0 - valLoss: 0.633992075920105 - trainLoss: 0.6250095963478088\n",
      "cnt: 0 - valLoss: 0.6339784264564514 - trainLoss: 0.6249947547912598\n",
      "cnt: 0 - valLoss: 0.6339647769927979 - trainLoss: 0.6249800324440002\n",
      "cnt: 0 - valLoss: 0.6339511871337891 - trainLoss: 0.6249651908874512\n",
      "cnt: 0 - valLoss: 0.6339374780654907 - trainLoss: 0.6249504089355469\n",
      "cnt: 0 - valLoss: 0.6339238286018372 - trainLoss: 0.6249356269836426\n",
      "cnt: 0 - valLoss: 0.6339101195335388 - trainLoss: 0.6249208450317383\n",
      "cnt: 0 - valLoss: 0.6338964700698853 - trainLoss: 0.624906063079834\n",
      "cnt: 0 - valLoss: 0.6338828206062317 - trainLoss: 0.6248912811279297\n",
      "cnt: 0 - valLoss: 0.6338691115379333 - trainLoss: 0.6248764991760254\n",
      "cnt: 0 - valLoss: 0.6338554620742798 - trainLoss: 0.6248617172241211\n",
      "cnt: 0 - valLoss: 0.6338417530059814 - trainLoss: 0.624846875667572\n",
      "cnt: 0 - valLoss: 0.6338281631469727 - trainLoss: 0.6248321533203125\n",
      "cnt: 0 - valLoss: 0.6338144540786743 - trainLoss: 0.6248173117637634\n",
      "cnt: 0 - valLoss: 0.6338008046150208 - trainLoss: 0.6248025298118591\n",
      "cnt: 0 - valLoss: 0.6337871551513672 - trainLoss: 0.6247877478599548\n",
      "cnt: 0 - valLoss: 0.6337734460830688 - trainLoss: 0.6247729659080505\n",
      "cnt: 0 - valLoss: 0.6337597370147705 - trainLoss: 0.6247581839561462\n",
      "cnt: 0 - valLoss: 0.6337460875511169 - trainLoss: 0.6247434616088867\n",
      "cnt: 0 - valLoss: 0.6337323784828186 - trainLoss: 0.6247287392616272\n",
      "cnt: 0 - valLoss: 0.6337187886238098 - trainLoss: 0.6247138977050781\n",
      "cnt: 0 - valLoss: 0.6337050795555115 - trainLoss: 0.6246991753578186\n",
      "cnt: 0 - valLoss: 0.6336913108825684 - trainLoss: 0.6246843934059143\n",
      "cnt: 0 - valLoss: 0.6336776614189148 - trainLoss: 0.6246696710586548\n",
      "cnt: 0 - valLoss: 0.6336640119552612 - trainLoss: 0.6246548295021057\n",
      "cnt: 0 - valLoss: 0.6336503028869629 - trainLoss: 0.6246401071548462\n",
      "cnt: 0 - valLoss: 0.6336365938186646 - trainLoss: 0.6246253848075867\n",
      "cnt: 0 - valLoss: 0.6336228251457214 - trainLoss: 0.6246106028556824\n",
      "cnt: 0 - valLoss: 0.6336091756820679 - trainLoss: 0.6245957612991333\n",
      "cnt: 0 - valLoss: 0.6335955262184143 - trainLoss: 0.6245810389518738\n",
      "cnt: 0 - valLoss: 0.633581817150116 - trainLoss: 0.6245662569999695\n",
      "cnt: 0 - valLoss: 0.6335681080818176 - trainLoss: 0.62455153465271\n",
      "cnt: 0 - valLoss: 0.6335543990135193 - trainLoss: 0.6245366930961609\n",
      "cnt: 0 - valLoss: 0.6335406303405762 - trainLoss: 0.6245219707489014\n",
      "cnt: 0 - valLoss: 0.6335269808769226 - trainLoss: 0.6245071887969971\n",
      "cnt: 0 - valLoss: 0.6335132122039795 - trainLoss: 0.6244924664497375\n",
      "cnt: 0 - valLoss: 0.6334995031356812 - trainLoss: 0.6244776844978333\n",
      "cnt: 0 - valLoss: 0.6334857940673828 - trainLoss: 0.624462902545929\n",
      "cnt: 0 - valLoss: 0.6334720849990845 - trainLoss: 0.6244481801986694\n",
      "cnt: 0 - valLoss: 0.6334583163261414 - trainLoss: 0.6244333982467651\n",
      "cnt: 0 - valLoss: 0.633444607257843 - trainLoss: 0.6244186162948608\n",
      "cnt: 0 - valLoss: 0.6334308981895447 - trainLoss: 0.6244038343429565\n",
      "cnt: 0 - valLoss: 0.6334171295166016 - trainLoss: 0.624389111995697\n",
      "cnt: 0 - valLoss: 0.6334034204483032 - trainLoss: 0.6243743300437927\n",
      "cnt: 0 - valLoss: 0.6333896517753601 - trainLoss: 0.6243594884872437\n",
      "cnt: 0 - valLoss: 0.6333759427070618 - trainLoss: 0.6243447661399841\n",
      "cnt: 0 - valLoss: 0.6333621740341187 - trainLoss: 0.6243300437927246\n",
      "cnt: 0 - valLoss: 0.6333484649658203 - trainLoss: 0.6243152618408203\n",
      "cnt: 0 - valLoss: 0.6333346962928772 - trainLoss: 0.624300479888916\n",
      "cnt: 0 - valLoss: 0.6333209872245789 - trainLoss: 0.6242856979370117\n",
      "cnt: 0 - valLoss: 0.6333072185516357 - trainLoss: 0.6242709755897522\n",
      "cnt: 0 - valLoss: 0.6332934498786926 - trainLoss: 0.6242561936378479\n",
      "cnt: 0 - valLoss: 0.6332796812057495 - trainLoss: 0.6242414116859436\n",
      "cnt: 0 - valLoss: 0.6332659125328064 - trainLoss: 0.6242266893386841\n",
      "cnt: 0 - valLoss: 0.6332522034645081 - trainLoss: 0.624211847782135\n",
      "cnt: 0 - valLoss: 0.6332383751869202 - trainLoss: 0.6241970658302307\n",
      "cnt: 0 - valLoss: 0.633224606513977 - trainLoss: 0.6241823434829712\n",
      "cnt: 0 - valLoss: 0.6332108378410339 - trainLoss: 0.6241675615310669\n",
      "cnt: 0 - valLoss: 0.6331970691680908 - trainLoss: 0.6241527795791626\n",
      "cnt: 0 - valLoss: 0.6331833600997925 - trainLoss: 0.6241380572319031\n",
      "cnt: 0 - valLoss: 0.6331695318222046 - trainLoss: 0.6241232752799988\n",
      "cnt: 0 - valLoss: 0.6331557631492615 - trainLoss: 0.6241084933280945\n",
      "cnt: 0 - valLoss: 0.6331419944763184 - trainLoss: 0.6240937113761902\n",
      "cnt: 0 - valLoss: 0.6331281661987305 - trainLoss: 0.6240789294242859\n",
      "cnt: 0 - valLoss: 0.6331143975257874 - trainLoss: 0.6240642070770264\n",
      "cnt: 0 - valLoss: 0.6331006288528442 - trainLoss: 0.6240493655204773\n",
      "cnt: 0 - valLoss: 0.6330868005752563 - trainLoss: 0.6240346431732178\n",
      "cnt: 0 - valLoss: 0.6330730319023132 - trainLoss: 0.6240199208259583\n",
      "cnt: 0 - valLoss: 0.6330592632293701 - trainLoss: 0.624005138874054\n",
      "cnt: 0 - valLoss: 0.6330454349517822 - trainLoss: 0.6239903569221497\n",
      "cnt: 0 - valLoss: 0.6330316662788391 - trainLoss: 0.6239755749702454\n",
      "cnt: 0 - valLoss: 0.633017897605896 - trainLoss: 0.6239607930183411\n",
      "cnt: 0 - valLoss: 0.6330040693283081 - trainLoss: 0.6239460706710815\n",
      "cnt: 0 - valLoss: 0.6329902410507202 - trainLoss: 0.6239312291145325\n",
      "cnt: 0 - valLoss: 0.6329764723777771 - trainLoss: 0.6239164471626282\n",
      "cnt: 0 - valLoss: 0.6329625844955444 - trainLoss: 0.6239017248153687\n",
      "cnt: 0 - valLoss: 0.6329488158226013 - trainLoss: 0.6238869428634644\n",
      "cnt: 0 - valLoss: 0.6329349875450134 - trainLoss: 0.6238721609115601\n",
      "cnt: 0 - valLoss: 0.6329211592674255 - trainLoss: 0.6238573789596558\n",
      "cnt: 0 - valLoss: 0.6329073309898376 - trainLoss: 0.6238425970077515\n",
      "cnt: 0 - valLoss: 0.6328935623168945 - trainLoss: 0.6238278150558472\n",
      "cnt: 0 - valLoss: 0.6328797340393066 - trainLoss: 0.6238130331039429\n",
      "cnt: 0 - valLoss: 0.632865846157074 - trainLoss: 0.6237982511520386\n",
      "cnt: 0 - valLoss: 0.6328520178794861 - trainLoss: 0.6237834692001343\n",
      "cnt: 0 - valLoss: 0.6328381896018982 - trainLoss: 0.6237687468528748\n",
      "cnt: 0 - valLoss: 0.6328243613243103 - trainLoss: 0.6237539649009705\n",
      "cnt: 0 - valLoss: 0.6328105330467224 - trainLoss: 0.6237391233444214\n",
      "cnt: 0 - valLoss: 0.6327967047691345 - trainLoss: 0.6237244009971619\n",
      "cnt: 0 - valLoss: 0.6327828168869019 - trainLoss: 0.6237096190452576\n",
      "cnt: 0 - valLoss: 0.632768988609314 - trainLoss: 0.6236947774887085\n",
      "cnt: 0 - valLoss: 0.6327551603317261 - trainLoss: 0.623680055141449\n",
      "cnt: 0 - valLoss: 0.6327412724494934 - trainLoss: 0.6236652731895447\n",
      "cnt: 0 - valLoss: 0.6327274441719055 - trainLoss: 0.6236504912376404\n",
      "cnt: 0 - valLoss: 0.6327135562896729 - trainLoss: 0.6236357688903809\n",
      "cnt: 0 - valLoss: 0.6326996684074402 - trainLoss: 0.6236209273338318\n",
      "cnt: 0 - valLoss: 0.6326857805252075 - trainLoss: 0.6236062049865723\n",
      "cnt: 0 - valLoss: 0.6326720118522644 - trainLoss: 0.623591423034668\n",
      "cnt: 0 - valLoss: 0.6326581239700317 - trainLoss: 0.6235765814781189\n",
      "cnt: 0 - valLoss: 0.6326442360877991 - trainLoss: 0.6235618591308594\n",
      "cnt: 0 - valLoss: 0.6326303482055664 - trainLoss: 0.6235470771789551\n",
      "cnt: 0 - valLoss: 0.6326165199279785 - trainLoss: 0.6235322952270508\n",
      "cnt: 0 - valLoss: 0.6326026320457458 - trainLoss: 0.6235175132751465\n",
      "cnt: 0 - valLoss: 0.6325887441635132 - trainLoss: 0.6235027313232422\n",
      "cnt: 0 - valLoss: 0.6325748562812805 - trainLoss: 0.6234879493713379\n",
      "cnt: 0 - valLoss: 0.6325609683990479 - trainLoss: 0.6234731078147888\n",
      "cnt: 0 - valLoss: 0.6325470805168152 - trainLoss: 0.6234583854675293\n",
      "cnt: 0 - valLoss: 0.6325331926345825 - trainLoss: 0.6234435439109802\n",
      "cnt: 0 - valLoss: 0.6325193047523499 - trainLoss: 0.6234288215637207\n",
      "cnt: 0 - valLoss: 0.6325054168701172 - trainLoss: 0.6234140396118164\n",
      "cnt: 0 - valLoss: 0.6324915289878845 - trainLoss: 0.6233991980552673\n",
      "cnt: 0 - valLoss: 0.6324776411056519 - trainLoss: 0.6233844757080078\n",
      "cnt: 0 - valLoss: 0.6324636936187744 - trainLoss: 0.6233696341514587\n",
      "cnt: 0 - valLoss: 0.6324498057365417 - trainLoss: 0.6233548521995544\n",
      "cnt: 0 - valLoss: 0.6324359178543091 - trainLoss: 0.6233400702476501\n",
      "cnt: 0 - valLoss: 0.6324219703674316 - trainLoss: 0.6233252882957458\n",
      "cnt: 0 - valLoss: 0.632408082485199 - trainLoss: 0.6233105063438416\n",
      "cnt: 0 - valLoss: 0.6323941946029663 - trainLoss: 0.6232957243919373\n",
      "cnt: 0 - valLoss: 0.6323801875114441 - trainLoss: 0.623280942440033\n",
      "cnt: 0 - valLoss: 0.6323662996292114 - trainLoss: 0.6232661008834839\n",
      "cnt: 0 - valLoss: 0.6323524117469788 - trainLoss: 0.6232513189315796\n",
      "cnt: 0 - valLoss: 0.6323384642601013 - trainLoss: 0.6232365965843201\n",
      "cnt: 0 - valLoss: 0.6323245763778687 - trainLoss: 0.623221755027771\n",
      "cnt: 0 - valLoss: 0.6323106288909912 - trainLoss: 0.6232069730758667\n",
      "cnt: 0 - valLoss: 0.6322966814041138 - trainLoss: 0.6231921911239624\n",
      "cnt: 0 - valLoss: 0.6322827935218811 - trainLoss: 0.6231774091720581\n",
      "cnt: 0 - valLoss: 0.6322687268257141 - trainLoss: 0.6231626272201538\n",
      "cnt: 0 - valLoss: 0.6322548389434814 - trainLoss: 0.6231477856636047\n",
      "cnt: 0 - valLoss: 0.632240891456604 - trainLoss: 0.6231330037117004\n",
      "cnt: 0 - valLoss: 0.6322269439697266 - trainLoss: 0.6231182217597961\n",
      "cnt: 0 - valLoss: 0.6322129964828491 - trainLoss: 0.6231034398078918\n",
      "cnt: 0 - valLoss: 0.6321990489959717 - trainLoss: 0.6230885982513428\n",
      "cnt: 0 - valLoss: 0.6321851015090942 - trainLoss: 0.6230738162994385\n",
      "cnt: 0 - valLoss: 0.6321711540222168 - trainLoss: 0.6230590343475342\n",
      "cnt: 0 - valLoss: 0.6321572065353394 - trainLoss: 0.6230441927909851\n",
      "cnt: 0 - valLoss: 0.6321431994438171 - trainLoss: 0.6230294704437256\n",
      "cnt: 0 - valLoss: 0.6321292519569397 - trainLoss: 0.6230146288871765\n",
      "cnt: 0 - valLoss: 0.6321153044700623 - trainLoss: 0.6229998469352722\n",
      "cnt: 0 - valLoss: 0.63210129737854 - trainLoss: 0.6229850053787231\n",
      "cnt: 0 - valLoss: 0.6320873498916626 - trainLoss: 0.6229702830314636\n",
      "cnt: 0 - valLoss: 0.6320734024047852 - trainLoss: 0.6229553818702698\n",
      "cnt: 0 - valLoss: 0.6320593357086182 - trainLoss: 0.6229406595230103\n",
      "cnt: 0 - valLoss: 0.6320453882217407 - trainLoss: 0.6229258179664612\n",
      "cnt: 0 - valLoss: 0.6320314407348633 - trainLoss: 0.6229110360145569\n",
      "cnt: 0 - valLoss: 0.6320174336433411 - trainLoss: 0.6228961944580078\n",
      "cnt: 0 - valLoss: 0.6320034265518188 - trainLoss: 0.6228814125061035\n",
      "cnt: 0 - valLoss: 0.6319894790649414 - trainLoss: 0.6228665709495544\n",
      "cnt: 0 - valLoss: 0.6319754719734192 - trainLoss: 0.6228518486022949\n",
      "cnt: 0 - valLoss: 0.631961464881897 - trainLoss: 0.6228370070457458\n",
      "cnt: 0 - valLoss: 0.63194739818573 - trainLoss: 0.6228222250938416\n",
      "cnt: 0 - valLoss: 0.6319334506988525 - trainLoss: 0.6228074431419373\n",
      "cnt: 0 - valLoss: 0.6319194436073303 - trainLoss: 0.622792661190033\n",
      "cnt: 0 - valLoss: 0.6319054365158081 - trainLoss: 0.6227777600288391\n",
      "cnt: 0 - valLoss: 0.6318914294242859 - trainLoss: 0.6227629780769348\n",
      "cnt: 0 - valLoss: 0.6318774223327637 - trainLoss: 0.6227481365203857\n",
      "cnt: 0 - valLoss: 0.6318633556365967 - trainLoss: 0.6227334141731262\n",
      "cnt: 0 - valLoss: 0.6318493485450745 - trainLoss: 0.6227185726165771\n",
      "cnt: 0 - valLoss: 0.6318353414535522 - trainLoss: 0.6227037310600281\n",
      "cnt: 0 - valLoss: 0.6318212747573853 - trainLoss: 0.6226889491081238\n",
      "cnt: 0 - valLoss: 0.6318073272705078 - trainLoss: 0.6226740479469299\n",
      "cnt: 0 - valLoss: 0.6317932605743408 - trainLoss: 0.6226593255996704\n",
      "cnt: 0 - valLoss: 0.6317792534828186 - trainLoss: 0.6226444840431213\n",
      "cnt: 0 - valLoss: 0.6317651867866516 - trainLoss: 0.622629702091217\n",
      "cnt: 0 - valLoss: 0.6317511796951294 - trainLoss: 0.622614860534668\n",
      "cnt: 0 - valLoss: 0.6317371129989624 - trainLoss: 0.6226000189781189\n",
      "cnt: 0 - valLoss: 0.6317230463027954 - trainLoss: 0.6225852370262146\n",
      "cnt: 0 - valLoss: 0.6317090392112732 - trainLoss: 0.6225703954696655\n",
      "cnt: 0 - valLoss: 0.631695032119751 - trainLoss: 0.6225556135177612\n",
      "cnt: 0 - valLoss: 0.631680965423584 - trainLoss: 0.6225407719612122\n",
      "cnt: 0 - valLoss: 0.631666898727417 - trainLoss: 0.6225259900093079\n",
      "cnt: 0 - valLoss: 0.6316528916358948 - trainLoss: 0.6225111484527588\n",
      "cnt: 0 - valLoss: 0.6316389441490173 - trainLoss: 0.6224963665008545\n",
      "cnt: 0 - valLoss: 0.6316248774528503 - trainLoss: 0.6224815249443054\n",
      "cnt: 0 - valLoss: 0.6316109299659729 - trainLoss: 0.6224666833877563\n",
      "cnt: 0 - valLoss: 0.6315968632698059 - trainLoss: 0.6224518418312073\n",
      "cnt: 0 - valLoss: 0.6315828561782837 - trainLoss: 0.622437059879303\n",
      "cnt: 0 - valLoss: 0.6315688490867615 - trainLoss: 0.6224222183227539\n",
      "cnt: 0 - valLoss: 0.6315547823905945 - trainLoss: 0.6224073767662048\n",
      "cnt: 0 - valLoss: 0.6315407752990723 - trainLoss: 0.6223925948143005\n",
      "cnt: 0 - valLoss: 0.6315267086029053 - trainLoss: 0.6223776936531067\n",
      "cnt: 0 - valLoss: 0.6315127015113831 - trainLoss: 0.6223629117012024\n",
      "cnt: 0 - valLoss: 0.6314986348152161 - trainLoss: 0.6223480701446533\n",
      "cnt: 0 - valLoss: 0.6314846277236938 - trainLoss: 0.622333288192749\n",
      "cnt: 0 - valLoss: 0.6314705610275269 - trainLoss: 0.6223184466362\n",
      "cnt: 0 - valLoss: 0.6314565539360046 - trainLoss: 0.6223036050796509\n",
      "cnt: 0 - valLoss: 0.6314424872398376 - trainLoss: 0.622288703918457\n",
      "cnt: 0 - valLoss: 0.6314284205436707 - trainLoss: 0.6222739219665527\n",
      "cnt: 0 - valLoss: 0.6314144134521484 - trainLoss: 0.6222591400146484\n",
      "cnt: 0 - valLoss: 0.6314003467559814 - trainLoss: 0.6222442388534546\n",
      "cnt: 0 - valLoss: 0.6313862204551697 - trainLoss: 0.6222294569015503\n",
      "cnt: 0 - valLoss: 0.6313722133636475 - trainLoss: 0.6222146153450012\n",
      "cnt: 0 - valLoss: 0.6313580870628357 - trainLoss: 0.6221997737884521\n",
      "cnt: 0 - valLoss: 0.6313440799713135 - trainLoss: 0.6221849322319031\n",
      "cnt: 0 - valLoss: 0.6313299536705017 - trainLoss: 0.6221701502799988\n",
      "cnt: 0 - valLoss: 0.6313159465789795 - trainLoss: 0.6221552491188049\n",
      "cnt: 0 - valLoss: 0.6313018798828125 - trainLoss: 0.6221404075622559\n",
      "cnt: 0 - valLoss: 0.6312877535820007 - trainLoss: 0.6221255660057068\n",
      "cnt: 0 - valLoss: 0.6312737464904785 - trainLoss: 0.6221107244491577\n",
      "cnt: 0 - valLoss: 0.631259560585022 - trainLoss: 0.6220958828926086\n",
      "cnt: 0 - valLoss: 0.631245493888855 - trainLoss: 0.6220810413360596\n",
      "cnt: 0 - valLoss: 0.6312314867973328 - trainLoss: 0.6220662593841553\n",
      "cnt: 0 - valLoss: 0.631217360496521 - trainLoss: 0.6220514178276062\n",
      "cnt: 0 - valLoss: 0.6312032341957092 - trainLoss: 0.6220365166664124\n",
      "cnt: 0 - valLoss: 0.6311891674995422 - trainLoss: 0.6220216751098633\n",
      "cnt: 0 - valLoss: 0.6311749815940857 - trainLoss: 0.622006893157959\n",
      "cnt: 0 - valLoss: 0.6311609745025635 - trainLoss: 0.6219919919967651\n",
      "cnt: 0 - valLoss: 0.6311468482017517 - trainLoss: 0.6219771504402161\n",
      "cnt: 0 - valLoss: 0.6311327219009399 - trainLoss: 0.621962308883667\n",
      "cnt: 0 - valLoss: 0.631118655204773 - trainLoss: 0.6219474673271179\n",
      "cnt: 0 - valLoss: 0.6311045289039612 - trainLoss: 0.6219326853752136\n",
      "cnt: 0 - valLoss: 0.6310904026031494 - trainLoss: 0.6219177842140198\n",
      "cnt: 0 - valLoss: 0.6310762763023376 - trainLoss: 0.6219029426574707\n",
      "cnt: 0 - valLoss: 0.6310621500015259 - trainLoss: 0.6218881011009216\n",
      "cnt: 0 - valLoss: 0.6310480237007141 - trainLoss: 0.6218732595443726\n",
      "cnt: 0 - valLoss: 0.6310338973999023 - trainLoss: 0.6218584179878235\n",
      "cnt: 0 - valLoss: 0.6310198307037354 - trainLoss: 0.6218435764312744\n",
      "cnt: 0 - valLoss: 0.6310057044029236 - trainLoss: 0.6218286752700806\n",
      "cnt: 0 - valLoss: 0.6309915781021118 - trainLoss: 0.6218138933181763\n",
      "cnt: 0 - valLoss: 0.6309774518013 - trainLoss: 0.6217989921569824\n",
      "cnt: 0 - valLoss: 0.6309633255004883 - trainLoss: 0.6217841506004333\n",
      "cnt: 0 - valLoss: 0.6309491991996765 - trainLoss: 0.6217692494392395\n",
      "cnt: 0 - valLoss: 0.6309350728988647 - trainLoss: 0.6217544078826904\n",
      "cnt: 0 - valLoss: 0.6309208869934082 - trainLoss: 0.6217396259307861\n",
      "cnt: 0 - valLoss: 0.6309067606925964 - trainLoss: 0.6217247247695923\n",
      "cnt: 0 - valLoss: 0.6308926343917847 - trainLoss: 0.6217098832130432\n",
      "cnt: 0 - valLoss: 0.6308784484863281 - trainLoss: 0.6216950416564941\n",
      "cnt: 0 - valLoss: 0.6308643221855164 - trainLoss: 0.6216802000999451\n",
      "cnt: 0 - valLoss: 0.6308501362800598 - trainLoss: 0.6216652989387512\n",
      "cnt: 0 - valLoss: 0.630836009979248 - trainLoss: 0.6216504573822021\n",
      "cnt: 0 - valLoss: 0.6308218240737915 - trainLoss: 0.6216356158256531\n",
      "cnt: 0 - valLoss: 0.630807638168335 - trainLoss: 0.6216207146644592\n",
      "cnt: 0 - valLoss: 0.6307935118675232 - trainLoss: 0.6216058135032654\n",
      "cnt: 0 - valLoss: 0.6307793855667114 - trainLoss: 0.6215909719467163\n",
      "cnt: 0 - valLoss: 0.6307651400566101 - trainLoss: 0.6215761303901672\n",
      "cnt: 0 - valLoss: 0.6307510137557983 - trainLoss: 0.6215612888336182\n",
      "cnt: 0 - valLoss: 0.630736768245697 - trainLoss: 0.6215464472770691\n",
      "cnt: 0 - valLoss: 0.6307226419448853 - trainLoss: 0.6215315461158752\n",
      "cnt: 0 - valLoss: 0.6307084560394287 - trainLoss: 0.6215166449546814\n",
      "cnt: 0 - valLoss: 0.6306942701339722 - trainLoss: 0.6215018033981323\n",
      "cnt: 0 - valLoss: 0.6306800842285156 - trainLoss: 0.6214869022369385\n",
      "cnt: 0 - valLoss: 0.6306658983230591 - trainLoss: 0.6214720606803894\n",
      "cnt: 0 - valLoss: 0.6306517124176025 - trainLoss: 0.6214572191238403\n",
      "cnt: 0 - valLoss: 0.630637526512146 - trainLoss: 0.6214423179626465\n",
      "cnt: 0 - valLoss: 0.6306232810020447 - trainLoss: 0.6214274168014526\n",
      "cnt: 0 - valLoss: 0.6306090354919434 - trainLoss: 0.6214126348495483\n",
      "cnt: 0 - valLoss: 0.6305949091911316 - trainLoss: 0.6213977336883545\n",
      "cnt: 0 - valLoss: 0.6305806636810303 - trainLoss: 0.6213828325271606\n",
      "cnt: 0 - valLoss: 0.6305664777755737 - trainLoss: 0.6213679313659668\n",
      "cnt: 0 - valLoss: 0.6305522322654724 - trainLoss: 0.621353030204773\n",
      "cnt: 0 - valLoss: 0.6305381059646606 - trainLoss: 0.6213381290435791\n",
      "cnt: 0 - valLoss: 0.6305238604545593 - trainLoss: 0.62132328748703\n",
      "cnt: 0 - valLoss: 0.630509614944458 - trainLoss: 0.6213083863258362\n",
      "cnt: 0 - valLoss: 0.6304953694343567 - trainLoss: 0.6212935447692871\n",
      "cnt: 0 - valLoss: 0.6304811239242554 - trainLoss: 0.6212786436080933\n",
      "cnt: 0 - valLoss: 0.630466878414154 - trainLoss: 0.6212637424468994\n",
      "cnt: 0 - valLoss: 0.6304526925086975 - trainLoss: 0.6212488412857056\n",
      "cnt: 0 - valLoss: 0.6304384469985962 - trainLoss: 0.6212339997291565\n",
      "cnt: 0 - valLoss: 0.6304241418838501 - trainLoss: 0.6212190985679626\n",
      "cnt: 0 - valLoss: 0.6304099559783936 - trainLoss: 0.6212041974067688\n",
      "cnt: 0 - valLoss: 0.6303957104682922 - trainLoss: 0.6211893558502197\n",
      "cnt: 0 - valLoss: 0.6303814053535461 - trainLoss: 0.6211744546890259\n",
      "cnt: 0 - valLoss: 0.6303672194480896 - trainLoss: 0.621159553527832\n",
      "cnt: 0 - valLoss: 0.6303529739379883 - trainLoss: 0.6211446523666382\n",
      "cnt: 0 - valLoss: 0.630338728427887 - trainLoss: 0.6211298108100891\n",
      "cnt: 0 - valLoss: 0.6303244829177856 - trainLoss: 0.6211149096488953\n",
      "cnt: 0 - valLoss: 0.6303101181983948 - trainLoss: 0.6211000084877014\n",
      "cnt: 0 - valLoss: 0.6302959322929382 - trainLoss: 0.6210851073265076\n",
      "cnt: 0 - valLoss: 0.6302816867828369 - trainLoss: 0.6210702061653137\n",
      "cnt: 0 - valLoss: 0.6302673816680908 - trainLoss: 0.6210554242134094\n",
      "cnt: 0 - valLoss: 0.6302530765533447 - trainLoss: 0.6210404634475708\n",
      "cnt: 0 - valLoss: 0.6302388906478882 - trainLoss: 0.6210256218910217\n",
      "cnt: 0 - valLoss: 0.6302245855331421 - trainLoss: 0.6210107207298279\n",
      "cnt: 0 - valLoss: 0.6302103400230408 - trainLoss: 0.620995819568634\n",
      "cnt: 0 - valLoss: 0.6301960945129395 - trainLoss: 0.6209810376167297\n",
      "cnt: 0 - valLoss: 0.6301818490028381 - trainLoss: 0.6209661364555359\n",
      "cnt: 0 - valLoss: 0.630167543888092 - trainLoss: 0.6209512948989868\n",
      "cnt: 0 - valLoss: 0.6301532983779907 - trainLoss: 0.620936393737793\n",
      "cnt: 0 - valLoss: 0.6301390528678894 - trainLoss: 0.6209215521812439\n",
      "cnt: 0 - valLoss: 0.6301247477531433 - trainLoss: 0.6209067106246948\n",
      "cnt: 0 - valLoss: 0.630110502243042 - trainLoss: 0.6208918690681458\n",
      "cnt: 0 - valLoss: 0.6300962567329407 - trainLoss: 0.6208770275115967\n",
      "cnt: 0 - valLoss: 0.6300819516181946 - trainLoss: 0.6208621263504028\n",
      "cnt: 0 - valLoss: 0.6300676465034485 - trainLoss: 0.6208472847938538\n",
      "cnt: 0 - valLoss: 0.6300534009933472 - trainLoss: 0.6208323836326599\n",
      "cnt: 0 - valLoss: 0.6300390958786011 - trainLoss: 0.6208175420761108\n",
      "cnt: 0 - valLoss: 0.6300248503684998 - trainLoss: 0.6208027005195618\n",
      "cnt: 0 - valLoss: 0.6300105452537537 - trainLoss: 0.6207877993583679\n",
      "cnt: 0 - valLoss: 0.6299962401390076 - trainLoss: 0.6207729578018188\n",
      "cnt: 0 - valLoss: 0.6299819350242615 - trainLoss: 0.6207581162452698\n",
      "cnt: 0 - valLoss: 0.6299676895141602 - trainLoss: 0.6207432150840759\n",
      "cnt: 0 - valLoss: 0.6299533843994141 - trainLoss: 0.6207283735275269\n",
      "cnt: 0 - valLoss: 0.6299390196800232 - trainLoss: 0.620713472366333\n",
      "cnt: 0 - valLoss: 0.6299248337745667 - trainLoss: 0.6206986308097839\n",
      "cnt: 0 - valLoss: 0.6299105286598206 - trainLoss: 0.6206837296485901\n",
      "cnt: 0 - valLoss: 0.6298961639404297 - trainLoss: 0.620668888092041\n",
      "cnt: 0 - valLoss: 0.6298818588256836 - trainLoss: 0.6206539869308472\n",
      "cnt: 0 - valLoss: 0.6298675537109375 - trainLoss: 0.6206390857696533\n",
      "cnt: 0 - valLoss: 0.6298533082008362 - trainLoss: 0.6206242442131042\n",
      "cnt: 0 - valLoss: 0.6298389434814453 - trainLoss: 0.6206094026565552\n",
      "cnt: 0 - valLoss: 0.6298246383666992 - trainLoss: 0.6205945014953613\n",
      "cnt: 0 - valLoss: 0.6298102736473083 - trainLoss: 0.6205796003341675\n",
      "cnt: 0 - valLoss: 0.629796028137207 - trainLoss: 0.6205646991729736\n",
      "cnt: 0 - valLoss: 0.6297816634178162 - trainLoss: 0.6205499172210693\n",
      "cnt: 0 - valLoss: 0.6297673583030701 - trainLoss: 0.6205349564552307\n",
      "cnt: 0 - valLoss: 0.629753053188324 - trainLoss: 0.6205201148986816\n",
      "cnt: 0 - valLoss: 0.6297386884689331 - trainLoss: 0.6205052137374878\n",
      "cnt: 0 - valLoss: 0.629724383354187 - trainLoss: 0.620490312576294\n",
      "cnt: 0 - valLoss: 0.6297100186347961 - trainLoss: 0.6204754710197449\n",
      "cnt: 0 - valLoss: 0.6296956539154053 - trainLoss: 0.620460569858551\n",
      "cnt: 0 - valLoss: 0.6296813488006592 - trainLoss: 0.620445728302002\n",
      "cnt: 0 - valLoss: 0.6296669840812683 - trainLoss: 0.6204308271408081\n",
      "cnt: 0 - valLoss: 0.6296525597572327 - trainLoss: 0.6204159259796143\n",
      "cnt: 0 - valLoss: 0.6296382546424866 - trainLoss: 0.6204010248184204\n",
      "cnt: 0 - valLoss: 0.6296238899230957 - trainLoss: 0.6203861236572266\n",
      "cnt: 0 - valLoss: 0.6296095848083496 - trainLoss: 0.6203712821006775\n",
      "cnt: 0 - valLoss: 0.629595160484314 - trainLoss: 0.6203563809394836\n",
      "cnt: 0 - valLoss: 0.6295807957649231 - trainLoss: 0.6203414797782898\n",
      "cnt: 0 - valLoss: 0.629566490650177 - trainLoss: 0.6203266382217407\n",
      "cnt: 0 - valLoss: 0.6295520663261414 - trainLoss: 0.6203116774559021\n",
      "cnt: 0 - valLoss: 0.6295377016067505 - trainLoss: 0.6202967762947083\n",
      "cnt: 0 - valLoss: 0.6295233368873596 - trainLoss: 0.6202818751335144\n",
      "cnt: 0 - valLoss: 0.629508912563324 - trainLoss: 0.6202669739723206\n",
      "cnt: 0 - valLoss: 0.6294945478439331 - trainLoss: 0.6202520728111267\n",
      "cnt: 0 - valLoss: 0.6294801235198975 - trainLoss: 0.6202372312545776\n",
      "cnt: 0 - valLoss: 0.6294657588005066 - trainLoss: 0.6202223300933838\n",
      "cnt: 0 - valLoss: 0.6294513940811157 - trainLoss: 0.6202074289321899\n",
      "cnt: 0 - valLoss: 0.6294369101524353 - trainLoss: 0.6201924681663513\n",
      "cnt: 0 - valLoss: 0.6294225454330444 - trainLoss: 0.6201775670051575\n",
      "cnt: 0 - valLoss: 0.6294081807136536 - trainLoss: 0.6201626658439636\n",
      "cnt: 0 - valLoss: 0.6293937563896179 - trainLoss: 0.6201477646827698\n",
      "cnt: 0 - valLoss: 0.6293793320655823 - trainLoss: 0.6201328635215759\n",
      "cnt: 0 - valLoss: 0.6293649673461914 - trainLoss: 0.6201179623603821\n",
      "cnt: 0 - valLoss: 0.629350483417511 - trainLoss: 0.6201030015945435\n",
      "cnt: 0 - valLoss: 0.6293361186981201 - trainLoss: 0.6200881004333496\n",
      "cnt: 0 - valLoss: 0.6293216943740845 - trainLoss: 0.6200731992721558\n",
      "cnt: 0 - valLoss: 0.6293072700500488 - trainLoss: 0.6200582981109619\n",
      "cnt: 0 - valLoss: 0.6292928457260132 - trainLoss: 0.6200433373451233\n",
      "cnt: 0 - valLoss: 0.6292783617973328 - trainLoss: 0.6200284361839294\n",
      "cnt: 0 - valLoss: 0.6292639970779419 - trainLoss: 0.6200135350227356\n",
      "cnt: 0 - valLoss: 0.6292495727539062 - trainLoss: 0.6199986338615417\n",
      "cnt: 0 - valLoss: 0.6292351484298706 - trainLoss: 0.6199836730957031\n",
      "cnt: 0 - valLoss: 0.6292207837104797 - trainLoss: 0.6199687719345093\n",
      "cnt: 0 - valLoss: 0.6292062997817993 - trainLoss: 0.6199538707733154\n",
      "cnt: 0 - valLoss: 0.6291918754577637 - trainLoss: 0.6199389100074768\n",
      "cnt: 0 - valLoss: 0.629177451133728 - trainLoss: 0.619924008846283\n",
      "cnt: 0 - valLoss: 0.6291629672050476 - trainLoss: 0.6199091076850891\n",
      "cnt: 0 - valLoss: 0.629148542881012 - trainLoss: 0.6198940873146057\n",
      "cnt: 0 - valLoss: 0.6291341185569763 - trainLoss: 0.6198791861534119\n",
      "cnt: 0 - valLoss: 0.6291196346282959 - trainLoss: 0.6198643445968628\n",
      "cnt: 0 - valLoss: 0.6291052103042603 - trainLoss: 0.6198493242263794\n",
      "cnt: 0 - valLoss: 0.6290907859802246 - trainLoss: 0.6198344230651855\n",
      "cnt: 0 - valLoss: 0.629076361656189 - trainLoss: 0.6198195219039917\n",
      "cnt: 0 - valLoss: 0.6290619373321533 - trainLoss: 0.6198046207427979\n",
      "cnt: 0 - valLoss: 0.6290475130081177 - trainLoss: 0.619789719581604\n",
      "cnt: 0 - valLoss: 0.629033088684082 - trainLoss: 0.6197747588157654\n",
      "cnt: 0 - valLoss: 0.6290186047554016 - trainLoss: 0.6197598576545715\n",
      "cnt: 0 - valLoss: 0.629004180431366 - trainLoss: 0.6197449564933777\n",
      "cnt: 0 - valLoss: 0.6289897561073303 - trainLoss: 0.6197300553321838\n",
      "cnt: 0 - valLoss: 0.6289752721786499 - trainLoss: 0.6197150945663452\n",
      "cnt: 0 - valLoss: 0.6289608478546143 - trainLoss: 0.6197001934051514\n",
      "cnt: 0 - valLoss: 0.6289463639259338 - trainLoss: 0.6196852922439575\n",
      "cnt: 0 - valLoss: 0.6289319396018982 - trainLoss: 0.6196703314781189\n",
      "cnt: 0 - valLoss: 0.628917396068573 - trainLoss: 0.6196553707122803\n",
      "cnt: 0 - valLoss: 0.6289029717445374 - trainLoss: 0.6196405291557312\n",
      "cnt: 0 - valLoss: 0.6288884878158569 - trainLoss: 0.6196256279945374\n",
      "cnt: 0 - valLoss: 0.6288740038871765 - trainLoss: 0.619610607624054\n",
      "cnt: 0 - valLoss: 0.6288596391677856 - trainLoss: 0.6195957064628601\n",
      "cnt: 0 - valLoss: 0.6288450956344604 - trainLoss: 0.6195808053016663\n",
      "cnt: 0 - valLoss: 0.62883061170578 - trainLoss: 0.6195658445358276\n",
      "cnt: 0 - valLoss: 0.6288161277770996 - trainLoss: 0.6195509433746338\n",
      "cnt: 0 - valLoss: 0.6288016438484192 - trainLoss: 0.6195359230041504\n",
      "cnt: 0 - valLoss: 0.6287871599197388 - trainLoss: 0.6195210218429565\n",
      "cnt: 0 - valLoss: 0.6287726759910583 - trainLoss: 0.6195061206817627\n",
      "cnt: 0 - valLoss: 0.6287581324577332 - trainLoss: 0.6194911003112793\n",
      "cnt: 0 - valLoss: 0.6287436485290527 - trainLoss: 0.6194761991500854\n",
      "cnt: 0 - valLoss: 0.6287291646003723 - trainLoss: 0.6194612979888916\n",
      "cnt: 0 - valLoss: 0.6287146806716919 - trainLoss: 0.6194462776184082\n",
      "cnt: 0 - valLoss: 0.6287001967430115 - trainLoss: 0.6194313168525696\n",
      "cnt: 0 - valLoss: 0.6286855936050415 - trainLoss: 0.6194164156913757\n",
      "cnt: 0 - valLoss: 0.6286711096763611 - trainLoss: 0.6194014549255371\n",
      "cnt: 0 - valLoss: 0.6286565661430359 - trainLoss: 0.6193864941596985\n",
      "cnt: 0 - valLoss: 0.6286420822143555 - trainLoss: 0.6193715929985046\n",
      "cnt: 0 - valLoss: 0.6286275386810303 - trainLoss: 0.6193565726280212\n",
      "cnt: 0 - valLoss: 0.6286129951477051 - trainLoss: 0.6193416118621826\n",
      "cnt: 0 - valLoss: 0.6285984516143799 - trainLoss: 0.6193267107009888\n",
      "cnt: 0 - valLoss: 0.6285839080810547 - trainLoss: 0.6193117499351501\n",
      "cnt: 0 - valLoss: 0.6285694241523743 - trainLoss: 0.6192967891693115\n",
      "cnt: 0 - valLoss: 0.6285548806190491 - trainLoss: 0.6192817687988281\n",
      "cnt: 0 - valLoss: 0.6285402774810791 - trainLoss: 0.6192668676376343\n",
      "cnt: 0 - valLoss: 0.6285257935523987 - trainLoss: 0.6192519068717957\n",
      "cnt: 0 - valLoss: 0.6285112500190735 - trainLoss: 0.619236946105957\n",
      "cnt: 0 - valLoss: 0.6284966468811035 - trainLoss: 0.6192219257354736\n",
      "cnt: 0 - valLoss: 0.6284821033477783 - trainLoss: 0.619206964969635\n",
      "cnt: 0 - valLoss: 0.6284675002098083 - trainLoss: 0.6191920042037964\n",
      "cnt: 0 - valLoss: 0.6284530162811279 - trainLoss: 0.619176983833313\n",
      "cnt: 0 - valLoss: 0.6284383535385132 - trainLoss: 0.6191620826721191\n",
      "cnt: 0 - valLoss: 0.6284238696098328 - trainLoss: 0.6191470623016357\n",
      "cnt: 0 - valLoss: 0.6284093260765076 - trainLoss: 0.6191321015357971\n",
      "cnt: 0 - valLoss: 0.6283947229385376 - trainLoss: 0.6191171407699585\n",
      "cnt: 0 - valLoss: 0.6283800601959229 - trainLoss: 0.6191021800041199\n",
      "cnt: 0 - valLoss: 0.6283655166625977 - trainLoss: 0.6190872192382812\n",
      "cnt: 0 - valLoss: 0.6283509135246277 - trainLoss: 0.6190721988677979\n",
      "cnt: 0 - valLoss: 0.6283364295959473 - trainLoss: 0.6190572381019592\n",
      "cnt: 0 - valLoss: 0.6283217072486877 - trainLoss: 0.6190422773361206\n",
      "cnt: 0 - valLoss: 0.6283071637153625 - trainLoss: 0.6190272569656372\n",
      "cnt: 0 - valLoss: 0.6282926201820374 - trainLoss: 0.6190122365951538\n",
      "cnt: 0 - valLoss: 0.6282779574394226 - trainLoss: 0.61899733543396\n",
      "cnt: 0 - valLoss: 0.6282634139060974 - trainLoss: 0.6189822554588318\n",
      "cnt: 0 - valLoss: 0.6282487511634827 - trainLoss: 0.6189672946929932\n",
      "cnt: 0 - valLoss: 0.6282341480255127 - trainLoss: 0.6189522743225098\n",
      "cnt: 0 - valLoss: 0.6282195448875427 - trainLoss: 0.6189373135566711\n",
      "cnt: 0 - valLoss: 0.628204882144928 - trainLoss: 0.6189223527908325\n",
      "cnt: 0 - valLoss: 0.628190279006958 - trainLoss: 0.6189073324203491\n",
      "cnt: 0 - valLoss: 0.628175675868988 - trainLoss: 0.6188923120498657\n",
      "cnt: 0 - valLoss: 0.6281610727310181 - trainLoss: 0.6188772916793823\n",
      "cnt: 0 - valLoss: 0.6281464099884033 - trainLoss: 0.6188622713088989\n",
      "cnt: 0 - valLoss: 0.6281318068504333 - trainLoss: 0.6188473701477051\n",
      "cnt: 0 - valLoss: 0.6281171441078186 - trainLoss: 0.6188322901725769\n",
      "cnt: 0 - valLoss: 0.6281025409698486 - trainLoss: 0.6188173294067383\n",
      "cnt: 0 - valLoss: 0.6280878186225891 - trainLoss: 0.6188023090362549\n",
      "cnt: 0 - valLoss: 0.6280732154846191 - trainLoss: 0.6187872886657715\n",
      "cnt: 0 - valLoss: 0.6280586123466492 - trainLoss: 0.6187722682952881\n",
      "cnt: 0 - valLoss: 0.6280438899993896 - trainLoss: 0.6187573075294495\n",
      "cnt: 0 - valLoss: 0.6280292272567749 - trainLoss: 0.6187422871589661\n",
      "cnt: 0 - valLoss: 0.6280146241188049 - trainLoss: 0.6187272667884827\n",
      "cnt: 0 - valLoss: 0.6279999017715454 - trainLoss: 0.6187122464179993\n",
      "cnt: 0 - valLoss: 0.6279852986335754 - trainLoss: 0.6186972260475159\n",
      "cnt: 0 - valLoss: 0.6279706358909607 - trainLoss: 0.6186822056770325\n",
      "cnt: 0 - valLoss: 0.627955973148346 - trainLoss: 0.6186671257019043\n",
      "cnt: 0 - valLoss: 0.6279413104057312 - trainLoss: 0.6186521053314209\n",
      "cnt: 0 - valLoss: 0.6279265880584717 - trainLoss: 0.6186370849609375\n",
      "cnt: 0 - valLoss: 0.6279119253158569 - trainLoss: 0.6186220645904541\n",
      "cnt: 0 - valLoss: 0.6278972625732422 - trainLoss: 0.6186070442199707\n",
      "cnt: 0 - valLoss: 0.6278825402259827 - trainLoss: 0.6185920834541321\n",
      "cnt: 0 - valLoss: 0.6278678774833679 - trainLoss: 0.6185770034790039\n",
      "cnt: 0 - valLoss: 0.6278531551361084 - trainLoss: 0.6185619831085205\n",
      "cnt: 0 - valLoss: 0.6278384923934937 - trainLoss: 0.6185469031333923\n",
      "cnt: 0 - valLoss: 0.6278237104415894 - trainLoss: 0.6185318827629089\n",
      "cnt: 0 - valLoss: 0.6278090476989746 - trainLoss: 0.6185168623924255\n",
      "cnt: 0 - valLoss: 0.6277943253517151 - trainLoss: 0.6185018420219421\n",
      "cnt: 0 - valLoss: 0.6277796030044556 - trainLoss: 0.618486762046814\n",
      "cnt: 0 - valLoss: 0.6277649402618408 - trainLoss: 0.6184717416763306\n",
      "cnt: 0 - valLoss: 0.6277501583099365 - trainLoss: 0.6184567213058472\n",
      "cnt: 0 - valLoss: 0.6277354955673218 - trainLoss: 0.6184417009353638\n",
      "cnt: 0 - valLoss: 0.6277207732200623 - trainLoss: 0.6184266209602356\n",
      "cnt: 0 - valLoss: 0.6277060508728027 - trainLoss: 0.6184115409851074\n",
      "cnt: 0 - valLoss: 0.6276913285255432 - trainLoss: 0.618396520614624\n",
      "cnt: 0 - valLoss: 0.6276765465736389 - trainLoss: 0.6183815002441406\n",
      "cnt: 0 - valLoss: 0.6276618242263794 - trainLoss: 0.6183664202690125\n",
      "cnt: 0 - valLoss: 0.6276470422744751 - trainLoss: 0.6183513402938843\n",
      "cnt: 0 - valLoss: 0.6276323795318604 - trainLoss: 0.6183363199234009\n",
      "cnt: 0 - valLoss: 0.627617597579956 - trainLoss: 0.6183212995529175\n",
      "cnt: 0 - valLoss: 0.6276028156280518 - trainLoss: 0.6183062195777893\n",
      "cnt: 0 - valLoss: 0.6275880932807922 - trainLoss: 0.6182911992073059\n",
      "cnt: 0 - valLoss: 0.6275733113288879 - trainLoss: 0.6182761192321777\n",
      "cnt: 0 - valLoss: 0.6275585889816284 - trainLoss: 0.6182610392570496\n",
      "cnt: 0 - valLoss: 0.6275438666343689 - trainLoss: 0.6182459592819214\n",
      "cnt: 0 - valLoss: 0.6275290846824646 - trainLoss: 0.618230938911438\n",
      "cnt: 0 - valLoss: 0.6275143027305603 - trainLoss: 0.6182159185409546\n",
      "cnt: 0 - valLoss: 0.627499520778656 - trainLoss: 0.6182008981704712\n",
      "cnt: 0 - valLoss: 0.6274847984313965 - trainLoss: 0.6181858777999878\n",
      "cnt: 0 - valLoss: 0.6274700164794922 - trainLoss: 0.6181707978248596\n",
      "cnt: 0 - valLoss: 0.6274552345275879 - trainLoss: 0.6181557774543762\n",
      "cnt: 0 - valLoss: 0.6274404525756836 - trainLoss: 0.618140697479248\n",
      "cnt: 0 - valLoss: 0.6274257302284241 - trainLoss: 0.6181256175041199\n",
      "cnt: 0 - valLoss: 0.6274109482765198 - trainLoss: 0.6181106567382812\n",
      "cnt: 0 - valLoss: 0.6273961067199707 - trainLoss: 0.6180955171585083\n",
      "cnt: 0 - valLoss: 0.6273813843727112 - trainLoss: 0.6180804967880249\n",
      "cnt: 0 - valLoss: 0.6273665428161621 - trainLoss: 0.6180654764175415\n",
      "cnt: 0 - valLoss: 0.6273517608642578 - trainLoss: 0.6180503964424133\n",
      "cnt: 0 - valLoss: 0.6273369789123535 - trainLoss: 0.6180353164672852\n",
      "cnt: 0 - valLoss: 0.6273221373558044 - trainLoss: 0.618020236492157\n",
      "cnt: 0 - valLoss: 0.6273073554039001 - trainLoss: 0.6180051565170288\n",
      "cnt: 0 - valLoss: 0.6272925734519958 - trainLoss: 0.6179901361465454\n",
      "cnt: 0 - valLoss: 0.6272777318954468 - trainLoss: 0.617975115776062\n",
      "cnt: 0 - valLoss: 0.6272628307342529 - trainLoss: 0.6179599761962891\n",
      "cnt: 0 - valLoss: 0.6272480487823486 - trainLoss: 0.6179448962211609\n",
      "cnt: 0 - valLoss: 0.6272331476211548 - trainLoss: 0.6179297566413879\n",
      "cnt: 0 - valLoss: 0.6272183060646057 - trainLoss: 0.6179146766662598\n",
      "cnt: 0 - valLoss: 0.6272034645080566 - trainLoss: 0.6178995370864868\n",
      "cnt: 0 - valLoss: 0.6271886229515076 - trainLoss: 0.6178844571113586\n",
      "cnt: 0 - valLoss: 0.6271737813949585 - trainLoss: 0.6178693771362305\n",
      "cnt: 0 - valLoss: 0.6271589398384094 - trainLoss: 0.6178542375564575\n",
      "cnt: 0 - valLoss: 0.6271440386772156 - trainLoss: 0.6178390979766846\n",
      "cnt: 0 - valLoss: 0.6271291971206665 - trainLoss: 0.6178240180015564\n",
      "cnt: 0 - valLoss: 0.6271143555641174 - trainLoss: 0.6178088784217834\n",
      "cnt: 0 - valLoss: 0.6270993947982788 - trainLoss: 0.6177937388420105\n",
      "cnt: 0 - valLoss: 0.6270845532417297 - trainLoss: 0.6177785992622375\n",
      "cnt: 0 - valLoss: 0.6270697712898254 - trainLoss: 0.6177635192871094\n",
      "cnt: 0 - valLoss: 0.6270548105239868 - trainLoss: 0.6177484393119812\n",
      "cnt: 0 - valLoss: 0.6270400285720825 - trainLoss: 0.6177334189414978\n",
      "cnt: 0 - valLoss: 0.6270251274108887 - trainLoss: 0.6177182793617249\n",
      "cnt: 0 - valLoss: 0.6270102858543396 - trainLoss: 0.6177031993865967\n",
      "cnt: 0 - valLoss: 0.6269953846931458 - trainLoss: 0.6176881194114685\n",
      "cnt: 0 - valLoss: 0.6269804835319519 - trainLoss: 0.6176730394363403\n",
      "cnt: 0 - valLoss: 0.6269655823707581 - trainLoss: 0.6176579594612122\n",
      "cnt: 0 - valLoss: 0.626950740814209 - trainLoss: 0.6176428198814392\n",
      "cnt: 0 - valLoss: 0.6269357800483704 - trainLoss: 0.617627739906311\n",
      "cnt: 0 - valLoss: 0.6269209384918213 - trainLoss: 0.6176126003265381\n",
      "cnt: 0 - valLoss: 0.6269060373306274 - trainLoss: 0.6175975203514099\n",
      "cnt: 0 - valLoss: 0.6268911361694336 - trainLoss: 0.6175824999809265\n",
      "cnt: 0 - valLoss: 0.6268762350082397 - trainLoss: 0.6175673604011536\n",
      "cnt: 0 - valLoss: 0.6268613338470459 - trainLoss: 0.6175522804260254\n",
      "cnt: 0 - valLoss: 0.626846432685852 - trainLoss: 0.6175371408462524\n",
      "cnt: 0 - valLoss: 0.626831591129303 - trainLoss: 0.6175220608711243\n",
      "cnt: 0 - valLoss: 0.6268166303634644 - trainLoss: 0.6175069212913513\n",
      "cnt: 0 - valLoss: 0.6268016695976257 - trainLoss: 0.6174918413162231\n",
      "cnt: 0 - valLoss: 0.6267867684364319 - trainLoss: 0.6174767017364502\n",
      "cnt: 0 - valLoss: 0.6267718076705933 - trainLoss: 0.617461621761322\n",
      "cnt: 0 - valLoss: 0.6267569065093994 - trainLoss: 0.6174464225769043\n",
      "cnt: 0 - valLoss: 0.6267420053482056 - trainLoss: 0.6174314022064209\n",
      "cnt: 0 - valLoss: 0.6267269849777222 - trainLoss: 0.6174162030220032\n",
      "cnt: 0 - valLoss: 0.6267120838165283 - trainLoss: 0.617401123046875\n",
      "cnt: 0 - valLoss: 0.6266971826553345 - trainLoss: 0.6173860430717468\n",
      "cnt: 0 - valLoss: 0.6266822218894958 - trainLoss: 0.6173709034919739\n",
      "cnt: 0 - valLoss: 0.6266672611236572 - trainLoss: 0.6173557639122009\n",
      "cnt: 0 - valLoss: 0.6266523003578186 - trainLoss: 0.617340624332428\n",
      "cnt: 0 - valLoss: 0.62663733959198 - trainLoss: 0.617325484752655\n",
      "cnt: 0 - valLoss: 0.6266223788261414 - trainLoss: 0.6173104047775269\n",
      "cnt: 0 - valLoss: 0.6266074180603027 - trainLoss: 0.6172952651977539\n",
      "cnt: 0 - valLoss: 0.6265924572944641 - trainLoss: 0.617280125617981\n",
      "cnt: 0 - valLoss: 0.6265774965286255 - trainLoss: 0.617264986038208\n",
      "cnt: 0 - valLoss: 0.6265625357627869 - trainLoss: 0.6172498464584351\n",
      "cnt: 0 - valLoss: 0.6265475153923035 - trainLoss: 0.6172347068786621\n",
      "cnt: 0 - valLoss: 0.6265325546264648 - trainLoss: 0.6172195076942444\n",
      "cnt: 0 - valLoss: 0.6265175342559814 - trainLoss: 0.6172044277191162\n",
      "cnt: 0 - valLoss: 0.6265026330947876 - trainLoss: 0.6171892881393433\n",
      "cnt: 0 - valLoss: 0.6264876127243042 - trainLoss: 0.6171740889549255\n",
      "cnt: 0 - valLoss: 0.6264726519584656 - trainLoss: 0.6171590089797974\n",
      "cnt: 0 - valLoss: 0.6264575719833374 - trainLoss: 0.6171438694000244\n",
      "cnt: 0 - valLoss: 0.6264426112174988 - trainLoss: 0.6171286702156067\n",
      "cnt: 0 - valLoss: 0.6264275908470154 - trainLoss: 0.6171135902404785\n",
      "cnt: 0 - valLoss: 0.626412570476532 - trainLoss: 0.6170985102653503\n",
      "cnt: 0 - valLoss: 0.6263976097106934 - trainLoss: 0.6170834302902222\n",
      "cnt: 0 - valLoss: 0.62638258934021 - trainLoss: 0.6170682311058044\n",
      "cnt: 0 - valLoss: 0.6263675689697266 - trainLoss: 0.617053210735321\n",
      "cnt: 0 - valLoss: 0.6263525485992432 - trainLoss: 0.6170380115509033\n",
      "cnt: 0 - valLoss: 0.6263375878334045 - trainLoss: 0.6170229315757751\n",
      "cnt: 0 - valLoss: 0.6263225674629211 - trainLoss: 0.6170077919960022\n",
      "cnt: 0 - valLoss: 0.6263075470924377 - trainLoss: 0.6169927716255188\n",
      "cnt: 0 - valLoss: 0.6262925267219543 - trainLoss: 0.6169775724411011\n",
      "cnt: 0 - valLoss: 0.6262774467468262 - trainLoss: 0.6169624924659729\n",
      "cnt: 0 - valLoss: 0.6262624263763428 - trainLoss: 0.6169474124908447\n",
      "cnt: 0 - valLoss: 0.6262474060058594 - trainLoss: 0.616932213306427\n",
      "cnt: 0 - valLoss: 0.626232385635376 - trainLoss: 0.6169170141220093\n",
      "cnt: 0 - valLoss: 0.6262173056602478 - trainLoss: 0.6169018745422363\n",
      "cnt: 0 - valLoss: 0.6262022852897644 - trainLoss: 0.6168867349624634\n",
      "cnt: 0 - valLoss: 0.6261872053146362 - trainLoss: 0.6168715953826904\n",
      "cnt: 0 - valLoss: 0.6261721253395081 - trainLoss: 0.6168563961982727\n",
      "cnt: 0 - valLoss: 0.6261570453643799 - trainLoss: 0.6168411374092102\n",
      "cnt: 0 - valLoss: 0.6261419653892517 - trainLoss: 0.6168259978294373\n",
      "cnt: 0 - valLoss: 0.6261268258094788 - trainLoss: 0.6168107390403748\n",
      "cnt: 0 - valLoss: 0.6261117458343506 - trainLoss: 0.616795539855957\n",
      "cnt: 0 - valLoss: 0.6260967254638672 - trainLoss: 0.6167803406715393\n",
      "cnt: 0 - valLoss: 0.6260815858840942 - trainLoss: 0.6167651414871216\n",
      "cnt: 0 - valLoss: 0.6260665655136108 - trainLoss: 0.6167498826980591\n",
      "cnt: 0 - valLoss: 0.6260514259338379 - trainLoss: 0.6167346835136414\n",
      "cnt: 0 - valLoss: 0.6260362863540649 - trainLoss: 0.6167194843292236\n",
      "cnt: 0 - valLoss: 0.6260212063789368 - trainLoss: 0.6167042851448059\n",
      "cnt: 0 - valLoss: 0.6260061264038086 - trainLoss: 0.6166890263557434\n",
      "cnt: 0 - valLoss: 0.6259909868240356 - trainLoss: 0.6166737675666809\n",
      "cnt: 0 - valLoss: 0.6259759068489075 - trainLoss: 0.6166585683822632\n",
      "cnt: 0 - valLoss: 0.6259607672691345 - trainLoss: 0.6166433095932007\n",
      "cnt: 0 - valLoss: 0.6259456276893616 - trainLoss: 0.616628110408783\n",
      "cnt: 0 - valLoss: 0.6259305477142334 - trainLoss: 0.6166129112243652\n",
      "cnt: 0 - valLoss: 0.6259154081344604 - trainLoss: 0.6165976524353027\n",
      "cnt: 0 - valLoss: 0.6259002685546875 - trainLoss: 0.6165823936462402\n",
      "cnt: 0 - valLoss: 0.6258850693702698 - trainLoss: 0.6165671944618225\n",
      "cnt: 0 - valLoss: 0.6258699893951416 - trainLoss: 0.6165519952774048\n",
      "cnt: 0 - valLoss: 0.6258548498153687 - trainLoss: 0.6165366768836975\n",
      "cnt: 0 - valLoss: 0.6258397698402405 - trainLoss: 0.616521418094635\n",
      "cnt: 0 - valLoss: 0.625824511051178 - trainLoss: 0.6165061593055725\n",
      "cnt: 0 - valLoss: 0.625809371471405 - trainLoss: 0.6164909601211548\n",
      "cnt: 0 - valLoss: 0.6257942318916321 - trainLoss: 0.6164757013320923\n",
      "cnt: 0 - valLoss: 0.6257790327072144 - trainLoss: 0.6164604425430298\n",
      "cnt: 0 - valLoss: 0.6257638931274414 - trainLoss: 0.6164451837539673\n",
      "cnt: 0 - valLoss: 0.6257486939430237 - trainLoss: 0.6164299249649048\n",
      "cnt: 0 - valLoss: 0.6257335543632507 - trainLoss: 0.6164146661758423\n",
      "cnt: 0 - valLoss: 0.6257184147834778 - trainLoss: 0.6163994073867798\n",
      "cnt: 0 - valLoss: 0.6257032155990601 - trainLoss: 0.6163841485977173\n",
      "cnt: 0 - valLoss: 0.6256879568099976 - trainLoss: 0.6163688898086548\n",
      "cnt: 0 - valLoss: 0.6256728172302246 - trainLoss: 0.6163535714149475\n",
      "cnt: 0 - valLoss: 0.6256576180458069 - trainLoss: 0.6163383722305298\n",
      "cnt: 0 - valLoss: 0.6256424784660339 - trainLoss: 0.6163230538368225\n",
      "cnt: 0 - valLoss: 0.6256272196769714 - trainLoss: 0.6163077354431152\n",
      "cnt: 0 - valLoss: 0.6256120800971985 - trainLoss: 0.6162925362586975\n",
      "cnt: 0 - valLoss: 0.625596821308136 - trainLoss: 0.616277277469635\n",
      "cnt: 0 - valLoss: 0.6255815625190735 - trainLoss: 0.616261899471283\n",
      "cnt: 0 - valLoss: 0.625566303730011 - trainLoss: 0.6162466406822205\n",
      "cnt: 0 - valLoss: 0.625551164150238 - trainLoss: 0.6162313222885132\n",
      "cnt: 0 - valLoss: 0.6255359053611755 - trainLoss: 0.6162160038948059\n",
      "cnt: 0 - valLoss: 0.6255207061767578 - trainLoss: 0.6162006855010986\n",
      "cnt: 0 - valLoss: 0.6255054473876953 - trainLoss: 0.6161853075027466\n",
      "cnt: 0 - valLoss: 0.6254901885986328 - trainLoss: 0.6161700487136841\n",
      "cnt: 0 - valLoss: 0.6254749298095703 - trainLoss: 0.616154670715332\n",
      "cnt: 0 - valLoss: 0.6254596710205078 - trainLoss: 0.61613929271698\n",
      "cnt: 0 - valLoss: 0.6254444718360901 - trainLoss: 0.6161240339279175\n",
      "cnt: 0 - valLoss: 0.6254291534423828 - trainLoss: 0.6161086559295654\n",
      "cnt: 0 - valLoss: 0.6254138946533203 - trainLoss: 0.6160933375358582\n",
      "cnt: 0 - valLoss: 0.6253986954689026 - trainLoss: 0.6160780191421509\n",
      "cnt: 0 - valLoss: 0.6253834366798401 - trainLoss: 0.6160627007484436\n",
      "cnt: 0 - valLoss: 0.6253681182861328 - trainLoss: 0.6160473227500916\n",
      "cnt: 0 - valLoss: 0.6253528594970703 - trainLoss: 0.6160319447517395\n",
      "cnt: 0 - valLoss: 0.6253376007080078 - trainLoss: 0.6160166263580322\n",
      "cnt: 0 - valLoss: 0.6253223419189453 - trainLoss: 0.616001307964325\n",
      "cnt: 0 - valLoss: 0.625307023525238 - trainLoss: 0.6159859299659729\n",
      "cnt: 0 - valLoss: 0.6252917051315308 - trainLoss: 0.6159705519676208\n",
      "cnt: 0 - valLoss: 0.6252764463424683 - trainLoss: 0.6159552335739136\n",
      "cnt: 0 - valLoss: 0.6252611875534058 - trainLoss: 0.6159398555755615\n",
      "cnt: 0 - valLoss: 0.6252458691596985 - trainLoss: 0.6159245371818542\n",
      "cnt: 0 - valLoss: 0.6252305507659912 - trainLoss: 0.6159091591835022\n",
      "cnt: 0 - valLoss: 0.6252152323722839 - trainLoss: 0.6158937811851501\n",
      "cnt: 0 - valLoss: 0.6251999735832214 - trainLoss: 0.6158784031867981\n",
      "cnt: 0 - valLoss: 0.6251845955848694 - trainLoss: 0.615863025188446\n",
      "cnt: 0 - valLoss: 0.6251693367958069 - trainLoss: 0.6158477067947388\n",
      "cnt: 0 - valLoss: 0.6251540184020996 - trainLoss: 0.6158323287963867\n",
      "cnt: 0 - valLoss: 0.6251386404037476 - trainLoss: 0.6158168911933899\n",
      "cnt: 0 - valLoss: 0.6251233220100403 - trainLoss: 0.6158015727996826\n",
      "cnt: 0 - valLoss: 0.625108003616333 - trainLoss: 0.6157861948013306\n",
      "cnt: 0 - valLoss: 0.6250926852226257 - trainLoss: 0.6157708168029785\n",
      "cnt: 0 - valLoss: 0.6250774264335632 - trainLoss: 0.6157554388046265\n",
      "cnt: 0 - valLoss: 0.6250620484352112 - trainLoss: 0.6157400608062744\n",
      "cnt: 0 - valLoss: 0.6250466704368591 - trainLoss: 0.6157246828079224\n",
      "cnt: 0 - valLoss: 0.6250314116477966 - trainLoss: 0.6157093048095703\n",
      "cnt: 0 - valLoss: 0.6250160932540894 - trainLoss: 0.615693986415863\n",
      "cnt: 0 - valLoss: 0.6250007748603821 - trainLoss: 0.6156786680221558\n",
      "cnt: 0 - valLoss: 0.6249854564666748 - trainLoss: 0.6156632304191589\n",
      "cnt: 0 - valLoss: 0.6249700784683228 - trainLoss: 0.6156478524208069\n",
      "cnt: 0 - valLoss: 0.6249547600746155 - trainLoss: 0.6156325340270996\n",
      "cnt: 0 - valLoss: 0.6249393820762634 - trainLoss: 0.6156170964241028\n",
      "cnt: 0 - valLoss: 0.6249240636825562 - trainLoss: 0.6156017184257507\n",
      "cnt: 0 - valLoss: 0.6249087452888489 - trainLoss: 0.6155864000320435\n",
      "cnt: 0 - valLoss: 0.6248933672904968 - trainLoss: 0.6155709624290466\n",
      "cnt: 0 - valLoss: 0.6248780488967896 - trainLoss: 0.6155555844306946\n",
      "cnt: 0 - valLoss: 0.6248626708984375 - trainLoss: 0.6155402064323425\n",
      "cnt: 0 - valLoss: 0.6248472929000854 - trainLoss: 0.6155248284339905\n",
      "cnt: 0 - valLoss: 0.6248319149017334 - trainLoss: 0.6155093908309937\n",
      "cnt: 0 - valLoss: 0.6248165369033813 - trainLoss: 0.6154940724372864\n",
      "cnt: 0 - valLoss: 0.6248011589050293 - trainLoss: 0.6154786348342896\n",
      "cnt: 0 - valLoss: 0.6247857809066772 - trainLoss: 0.6154632568359375\n",
      "cnt: 0 - valLoss: 0.6247704029083252 - trainLoss: 0.6154478192329407\n",
      "cnt: 0 - valLoss: 0.6247550249099731 - trainLoss: 0.6154324412345886\n",
      "cnt: 0 - valLoss: 0.6247396469116211 - trainLoss: 0.6154170632362366\n",
      "cnt: 0 - valLoss: 0.624724268913269 - trainLoss: 0.6154016256332397\n",
      "cnt: 0 - valLoss: 0.6247087717056274 - trainLoss: 0.6153862476348877\n",
      "cnt: 0 - valLoss: 0.6246934533119202 - trainLoss: 0.6153708100318909\n",
      "cnt: 0 - valLoss: 0.6246780157089233 - trainLoss: 0.6153554320335388\n",
      "cnt: 0 - valLoss: 0.6246626377105713 - trainLoss: 0.615339994430542\n",
      "cnt: 0 - valLoss: 0.6246472597122192 - trainLoss: 0.6153246164321899\n",
      "cnt: 0 - valLoss: 0.6246318221092224 - trainLoss: 0.6153091788291931\n",
      "cnt: 0 - valLoss: 0.6246164441108704 - trainLoss: 0.6152937412261963\n",
      "cnt: 0 - valLoss: 0.6246009469032288 - trainLoss: 0.6152783036231995\n",
      "cnt: 0 - valLoss: 0.6245855689048767 - trainLoss: 0.6152629256248474\n",
      "cnt: 0 - valLoss: 0.6245701313018799 - trainLoss: 0.6152474880218506\n",
      "cnt: 0 - valLoss: 0.6245546936988831 - trainLoss: 0.6152320504188538\n",
      "cnt: 0 - valLoss: 0.6245392560958862 - trainLoss: 0.6152166724205017\n",
      "cnt: 0 - valLoss: 0.6245238184928894 - trainLoss: 0.6152011752128601\n",
      "cnt: 0 - valLoss: 0.6245083808898926 - trainLoss: 0.6151857376098633\n",
      "cnt: 0 - valLoss: 0.624492883682251 - trainLoss: 0.6151703596115112\n",
      "cnt: 0 - valLoss: 0.6244775056838989 - trainLoss: 0.6151549220085144\n",
      "cnt: 0 - valLoss: 0.6244620084762573 - trainLoss: 0.6151394248008728\n",
      "cnt: 0 - valLoss: 0.6244465112686157 - trainLoss: 0.615123987197876\n",
      "cnt: 0 - valLoss: 0.6244310736656189 - trainLoss: 0.6151085495948792\n",
      "cnt: 0 - valLoss: 0.6244156360626221 - trainLoss: 0.6150931119918823\n",
      "cnt: 0 - valLoss: 0.6244001388549805 - trainLoss: 0.6150776147842407\n",
      "cnt: 0 - valLoss: 0.6243847012519836 - trainLoss: 0.6150622367858887\n",
      "cnt: 0 - valLoss: 0.624369204044342 - trainLoss: 0.6150467991828918\n",
      "cnt: 0 - valLoss: 0.6243537068367004 - trainLoss: 0.6150313019752502\n",
      "cnt: 0 - valLoss: 0.6243382692337036 - trainLoss: 0.6150158047676086\n",
      "cnt: 0 - valLoss: 0.6243227124214172 - trainLoss: 0.6150004267692566\n",
      "cnt: 0 - valLoss: 0.6243072748184204 - trainLoss: 0.614984929561615\n",
      "cnt: 0 - valLoss: 0.6242917776107788 - trainLoss: 0.6149694919586182\n",
      "cnt: 0 - valLoss: 0.6242762804031372 - trainLoss: 0.6149539947509766\n",
      "cnt: 0 - valLoss: 0.6242607831954956 - trainLoss: 0.6149385571479797\n",
      "cnt: 0 - valLoss: 0.6242452263832092 - trainLoss: 0.6149230599403381\n",
      "cnt: 0 - valLoss: 0.6242297887802124 - trainLoss: 0.6149076223373413\n",
      "cnt: 0 - valLoss: 0.624214231967926 - trainLoss: 0.6148921251296997\n",
      "cnt: 0 - valLoss: 0.6241986751556396 - trainLoss: 0.6148766279220581\n",
      "cnt: 0 - valLoss: 0.624183177947998 - trainLoss: 0.6148611307144165\n",
      "cnt: 0 - valLoss: 0.6241676211357117 - trainLoss: 0.6148456931114197\n",
      "cnt: 0 - valLoss: 0.6241521239280701 - trainLoss: 0.6148301362991333\n",
      "cnt: 0 - valLoss: 0.6241365671157837 - trainLoss: 0.6148146986961365\n",
      "cnt: 0 - valLoss: 0.6241210699081421 - trainLoss: 0.6147992610931396\n",
      "cnt: 0 - valLoss: 0.6241055130958557 - trainLoss: 0.614783763885498\n",
      "cnt: 0 - valLoss: 0.6240900158882141 - trainLoss: 0.6147682666778564\n",
      "cnt: 0 - valLoss: 0.6240744590759277 - trainLoss: 0.6147527694702148\n",
      "cnt: 0 - valLoss: 0.6240589022636414 - trainLoss: 0.6147373914718628\n",
      "cnt: 0 - valLoss: 0.6240434050559998 - trainLoss: 0.6147218942642212\n",
      "cnt: 0 - valLoss: 0.6240277886390686 - trainLoss: 0.6147063970565796\n",
      "cnt: 0 - valLoss: 0.624012291431427 - trainLoss: 0.6146909594535828\n",
      "cnt: 0 - valLoss: 0.6239967346191406 - trainLoss: 0.6146754622459412\n",
      "cnt: 0 - valLoss: 0.6239811182022095 - trainLoss: 0.6146599650382996\n",
      "cnt: 0 - valLoss: 0.6239655613899231 - trainLoss: 0.614644467830658\n",
      "cnt: 0 - valLoss: 0.6239500045776367 - trainLoss: 0.6146289706230164\n",
      "cnt: 0 - valLoss: 0.6239344477653503 - trainLoss: 0.6146134734153748\n",
      "cnt: 0 - valLoss: 0.6239188313484192 - trainLoss: 0.6145979762077332\n",
      "cnt: 0 - valLoss: 0.6239032745361328 - trainLoss: 0.6145824790000916\n",
      "cnt: 0 - valLoss: 0.6238877177238464 - trainLoss: 0.6145669221878052\n",
      "cnt: 0 - valLoss: 0.6238721013069153 - trainLoss: 0.6145514249801636\n",
      "cnt: 0 - valLoss: 0.6238564848899841 - trainLoss: 0.6145358681678772\n",
      "cnt: 0 - valLoss: 0.623840868473053 - trainLoss: 0.6145203709602356\n",
      "cnt: 0 - valLoss: 0.6238253116607666 - trainLoss: 0.6145047545433044\n",
      "cnt: 0 - valLoss: 0.6238097548484802 - trainLoss: 0.6144892573356628\n",
      "cnt: 0 - valLoss: 0.6237940788269043 - trainLoss: 0.6144737005233765\n",
      "cnt: 0 - valLoss: 0.6237785220146179 - trainLoss: 0.6144581437110901\n",
      "cnt: 0 - valLoss: 0.6237629055976868 - trainLoss: 0.6144426465034485\n",
      "cnt: 0 - valLoss: 0.6237473487854004 - trainLoss: 0.6144271492958069\n",
      "cnt: 0 - valLoss: 0.6237317323684692 - trainLoss: 0.6144116520881653\n",
      "cnt: 0 - valLoss: 0.6237160563468933 - trainLoss: 0.6143960952758789\n",
      "cnt: 0 - valLoss: 0.6237004995346069 - trainLoss: 0.6143805384635925\n",
      "cnt: 0 - valLoss: 0.6236849427223206 - trainLoss: 0.6143650412559509\n",
      "cnt: 0 - valLoss: 0.6236693263053894 - trainLoss: 0.6143494844436646\n",
      "cnt: 0 - valLoss: 0.6236536502838135 - trainLoss: 0.6143339276313782\n",
      "cnt: 0 - valLoss: 0.6236380934715271 - trainLoss: 0.6143184304237366\n",
      "cnt: 0 - valLoss: 0.6236224174499512 - trainLoss: 0.6143028736114502\n",
      "cnt: 0 - valLoss: 0.62360680103302 - trainLoss: 0.6142873167991638\n",
      "cnt: 0 - valLoss: 0.6235911846160889 - trainLoss: 0.6142718195915222\n",
      "cnt: 0 - valLoss: 0.6235755681991577 - trainLoss: 0.6142562627792358\n",
      "cnt: 0 - valLoss: 0.6235598921775818 - trainLoss: 0.6142407059669495\n",
      "cnt: 0 - valLoss: 0.6235442161560059 - trainLoss: 0.6142250895500183\n",
      "cnt: 0 - valLoss: 0.6235285997390747 - trainLoss: 0.6142095327377319\n",
      "cnt: 0 - valLoss: 0.6235129833221436 - trainLoss: 0.6141939759254456\n",
      "cnt: 0 - valLoss: 0.6234972476959229 - trainLoss: 0.6141783595085144\n",
      "cnt: 0 - valLoss: 0.6234816312789917 - trainLoss: 0.6141627430915833\n",
      "cnt: 0 - valLoss: 0.6234659552574158 - trainLoss: 0.6141471266746521\n",
      "cnt: 0 - valLoss: 0.6234502792358398 - trainLoss: 0.6141315698623657\n",
      "cnt: 0 - valLoss: 0.6234346032142639 - trainLoss: 0.6141159534454346\n",
      "cnt: 0 - valLoss: 0.6234189867973328 - trainLoss: 0.6141003370285034\n",
      "cnt: 0 - valLoss: 0.6234032511711121 - trainLoss: 0.6140847206115723\n",
      "cnt: 0 - valLoss: 0.6233875155448914 - trainLoss: 0.6140691637992859\n",
      "cnt: 0 - valLoss: 0.6233718395233154 - trainLoss: 0.61405348777771\n",
      "cnt: 0 - valLoss: 0.6233561635017395 - trainLoss: 0.6140378713607788\n",
      "cnt: 0 - valLoss: 0.6233404874801636 - trainLoss: 0.6140222549438477\n",
      "cnt: 0 - valLoss: 0.6233247518539429 - trainLoss: 0.6140065789222717\n",
      "cnt: 0 - valLoss: 0.6233090162277222 - trainLoss: 0.6139909029006958\n",
      "cnt: 0 - valLoss: 0.6232933402061462 - trainLoss: 0.6139752268791199\n",
      "cnt: 0 - valLoss: 0.6232776045799255 - trainLoss: 0.613959550857544\n",
      "cnt: 0 - valLoss: 0.6232618689537048 - trainLoss: 0.6139439344406128\n",
      "cnt: 0 - valLoss: 0.6232461929321289 - trainLoss: 0.6139282584190369\n",
      "cnt: 0 - valLoss: 0.6232303977012634 - trainLoss: 0.6139125227928162\n",
      "cnt: 0 - valLoss: 0.6232146620750427 - trainLoss: 0.613896906375885\n",
      "cnt: 0 - valLoss: 0.623198926448822 - trainLoss: 0.6138812303543091\n",
      "cnt: 0 - valLoss: 0.6231831908226013 - trainLoss: 0.6138655543327332\n",
      "cnt: 0 - valLoss: 0.6231675148010254 - trainLoss: 0.6138498187065125\n",
      "cnt: 0 - valLoss: 0.6231517195701599 - trainLoss: 0.6138341426849365\n",
      "cnt: 0 - valLoss: 0.6231359243392944 - trainLoss: 0.6138184666633606\n",
      "cnt: 0 - valLoss: 0.6231201887130737 - trainLoss: 0.6138027906417847\n",
      "cnt: 0 - valLoss: 0.6231043934822083 - trainLoss: 0.613787055015564\n",
      "cnt: 0 - valLoss: 0.6230886578559875 - trainLoss: 0.6137714385986328\n",
      "cnt: 0 - valLoss: 0.6230728626251221 - trainLoss: 0.6137557625770569\n",
      "cnt: 0 - valLoss: 0.6230570673942566 - trainLoss: 0.613740086555481\n",
      "cnt: 0 - valLoss: 0.6230413317680359 - trainLoss: 0.6137243509292603\n",
      "cnt: 0 - valLoss: 0.6230255961418152 - trainLoss: 0.6137086749076843\n",
      "cnt: 0 - valLoss: 0.6230097413063049 - trainLoss: 0.6136929392814636\n",
      "cnt: 0 - valLoss: 0.6229940056800842 - trainLoss: 0.6136772036552429\n",
      "cnt: 0 - valLoss: 0.622978150844574 - trainLoss: 0.613661527633667\n",
      "cnt: 0 - valLoss: 0.6229624152183533 - trainLoss: 0.6136458516120911\n",
      "cnt: 0 - valLoss: 0.622946560382843 - trainLoss: 0.6136300563812256\n",
      "cnt: 0 - valLoss: 0.6229308247566223 - trainLoss: 0.6136143803596497\n",
      "cnt: 0 - valLoss: 0.6229150295257568 - trainLoss: 0.613598644733429\n",
      "cnt: 0 - valLoss: 0.6228992342948914 - trainLoss: 0.613582968711853\n",
      "cnt: 0 - valLoss: 0.6228833794593811 - trainLoss: 0.6135672330856323\n",
      "cnt: 0 - valLoss: 0.6228675246238708 - trainLoss: 0.6135515570640564\n",
      "cnt: 0 - valLoss: 0.6228517293930054 - trainLoss: 0.6135358214378357\n",
      "cnt: 0 - valLoss: 0.6228359341621399 - trainLoss: 0.6135201454162598\n",
      "cnt: 0 - valLoss: 0.6228200793266296 - trainLoss: 0.6135043501853943\n",
      "cnt: 0 - valLoss: 0.6228042244911194 - trainLoss: 0.6134886741638184\n",
      "cnt: 0 - valLoss: 0.6227884292602539 - trainLoss: 0.6134729385375977\n",
      "cnt: 0 - valLoss: 0.6227725744247437 - trainLoss: 0.6134571433067322\n",
      "cnt: 0 - valLoss: 0.6227567195892334 - trainLoss: 0.613441526889801\n",
      "cnt: 0 - valLoss: 0.6227408647537231 - trainLoss: 0.6134257316589355\n",
      "cnt: 0 - valLoss: 0.6227250099182129 - trainLoss: 0.6134099364280701\n",
      "cnt: 0 - valLoss: 0.6227090954780579 - trainLoss: 0.6133941411972046\n",
      "cnt: 0 - valLoss: 0.6226933598518372 - trainLoss: 0.6133784055709839\n",
      "cnt: 0 - valLoss: 0.6226774454116821 - trainLoss: 0.613362729549408\n",
      "cnt: 0 - valLoss: 0.6226615905761719 - trainLoss: 0.6133469939231873\n",
      "cnt: 0 - valLoss: 0.6226457357406616 - trainLoss: 0.6133312582969666\n",
      "cnt: 0 - valLoss: 0.6226298809051514 - trainLoss: 0.6133155226707458\n",
      "cnt: 0 - valLoss: 0.6226139664649963 - trainLoss: 0.6132997870445251\n",
      "cnt: 0 - valLoss: 0.6225981116294861 - trainLoss: 0.6132840514183044\n",
      "cnt: 0 - valLoss: 0.6225822567939758 - trainLoss: 0.6132683753967285\n",
      "cnt: 0 - valLoss: 0.6225664019584656 - trainLoss: 0.6132526397705078\n",
      "cnt: 0 - valLoss: 0.6225504875183105 - trainLoss: 0.6132369041442871\n",
      "cnt: 0 - valLoss: 0.6225346922874451 - trainLoss: 0.6132212281227112\n",
      "cnt: 0 - valLoss: 0.62251877784729 - trainLoss: 0.6132054924964905\n",
      "cnt: 0 - valLoss: 0.622502863407135 - trainLoss: 0.6131897568702698\n",
      "cnt: 0 - valLoss: 0.6224870085716248 - trainLoss: 0.6131740808486938\n",
      "cnt: 0 - valLoss: 0.6224710941314697 - trainLoss: 0.6131583452224731\n",
      "cnt: 0 - valLoss: 0.6224551796913147 - trainLoss: 0.6131426095962524\n",
      "cnt: 0 - valLoss: 0.6224393248558044 - trainLoss: 0.6131269335746765\n",
      "cnt: 0 - valLoss: 0.6224233508110046 - trainLoss: 0.613111138343811\n",
      "cnt: 0 - valLoss: 0.6224074363708496 - trainLoss: 0.6130954027175903\n",
      "cnt: 0 - valLoss: 0.6223915219306946 - trainLoss: 0.6130797266960144\n",
      "cnt: 0 - valLoss: 0.6223756074905396 - trainLoss: 0.6130639910697937\n",
      "cnt: 0 - valLoss: 0.6223596930503845 - trainLoss: 0.613048255443573\n",
      "cnt: 0 - valLoss: 0.6223438382148743 - trainLoss: 0.6130324602127075\n",
      "cnt: 0 - valLoss: 0.6223278641700745 - trainLoss: 0.6130167245864868\n",
      "cnt: 0 - valLoss: 0.6223119497299194 - trainLoss: 0.6130009889602661\n",
      "cnt: 0 - valLoss: 0.6222959756851196 - trainLoss: 0.6129852533340454\n",
      "cnt: 0 - valLoss: 0.6222800612449646 - trainLoss: 0.6129694581031799\n",
      "cnt: 0 - valLoss: 0.6222640872001648 - trainLoss: 0.6129537224769592\n",
      "cnt: 0 - valLoss: 0.6222481727600098 - trainLoss: 0.6129379272460938\n",
      "cnt: 0 - valLoss: 0.6222322583198547 - trainLoss: 0.612922191619873\n",
      "cnt: 0 - valLoss: 0.6222162842750549 - trainLoss: 0.6129064559936523\n",
      "cnt: 0 - valLoss: 0.6222003698348999 - trainLoss: 0.6128908395767212\n",
      "cnt: 0 - valLoss: 0.6221843957901001 - trainLoss: 0.6128751635551453\n",
      "cnt: 0 - valLoss: 0.6221684217453003 - trainLoss: 0.6128593683242798\n",
      "cnt: 0 - valLoss: 0.6221525073051453 - trainLoss: 0.6128436326980591\n",
      "cnt: 0 - valLoss: 0.6221364736557007 - trainLoss: 0.6128279566764832\n",
      "cnt: 0 - valLoss: 0.6221205592155457 - trainLoss: 0.6128122806549072\n",
      "cnt: 0 - valLoss: 0.6221045255661011 - trainLoss: 0.6127965450286865\n",
      "cnt: 0 - valLoss: 0.6220885515213013 - trainLoss: 0.6127808690071106\n",
      "cnt: 0 - valLoss: 0.6220726370811462 - trainLoss: 0.6127652525901794\n",
      "cnt: 0 - valLoss: 0.6220566034317017 - trainLoss: 0.612749457359314\n",
      "cnt: 0 - valLoss: 0.6220406293869019 - trainLoss: 0.6127337217330933\n",
      "cnt: 0 - valLoss: 0.622024655342102 - trainLoss: 0.6127179861068726\n",
      "cnt: 0 - valLoss: 0.6220086216926575 - trainLoss: 0.6127022504806519\n",
      "cnt: 0 - valLoss: 0.6219926476478577 - trainLoss: 0.6126864552497864\n",
      "cnt: 0 - valLoss: 0.6219766139984131 - trainLoss: 0.6126706600189209\n",
      "cnt: 0 - valLoss: 0.6219606399536133 - trainLoss: 0.6126548647880554\n",
      "cnt: 0 - valLoss: 0.6219446063041687 - trainLoss: 0.6126390695571899\n",
      "cnt: 0 - valLoss: 0.6219285130500793 - trainLoss: 0.6126232743263245\n",
      "cnt: 0 - valLoss: 0.6219125986099243 - trainLoss: 0.612607479095459\n",
      "cnt: 0 - valLoss: 0.6218964457511902 - trainLoss: 0.6125917434692383\n",
      "cnt: 0 - valLoss: 0.6218804121017456 - trainLoss: 0.6125759482383728\n",
      "cnt: 0 - valLoss: 0.6218644380569458 - trainLoss: 0.6125601530075073\n",
      "cnt: 0 - valLoss: 0.6218484044075012 - trainLoss: 0.6125442981719971\n",
      "cnt: 0 - valLoss: 0.6218323707580566 - trainLoss: 0.6125285029411316\n",
      "cnt: 0 - valLoss: 0.6218163371086121 - trainLoss: 0.6125127077102661\n",
      "cnt: 0 - valLoss: 0.6218002438545227 - trainLoss: 0.6124969124794006\n",
      "cnt: 0 - valLoss: 0.6217842102050781 - trainLoss: 0.6124811172485352\n",
      "cnt: 0 - valLoss: 0.6217681169509888 - trainLoss: 0.6124652624130249\n",
      "cnt: 0 - valLoss: 0.6217520833015442 - trainLoss: 0.6124494671821594\n",
      "cnt: 0 - valLoss: 0.6217360496520996 - trainLoss: 0.612433671951294\n",
      "cnt: 0 - valLoss: 0.6217199563980103 - trainLoss: 0.6124178767204285\n",
      "cnt: 0 - valLoss: 0.6217038631439209 - trainLoss: 0.6124020218849182\n",
      "cnt: 0 - valLoss: 0.6216877698898315 - trainLoss: 0.612386167049408\n",
      "cnt: 0 - valLoss: 0.6216716766357422 - trainLoss: 0.6123703718185425\n",
      "cnt: 0 - valLoss: 0.6216555833816528 - trainLoss: 0.612354576587677\n",
      "cnt: 0 - valLoss: 0.6216395497322083 - trainLoss: 0.612338662147522\n",
      "cnt: 0 - valLoss: 0.6216234564781189 - trainLoss: 0.6123228669166565\n",
      "cnt: 0 - valLoss: 0.6216073036193848 - trainLoss: 0.6123070120811462\n",
      "cnt: 0 - valLoss: 0.6215912699699402 - trainLoss: 0.6122912168502808\n",
      "cnt: 0 - valLoss: 0.6215751767158508 - trainLoss: 0.6122754216194153\n",
      "cnt: 0 - valLoss: 0.6215590238571167 - trainLoss: 0.6122595071792603\n",
      "cnt: 0 - valLoss: 0.6215428709983826 - trainLoss: 0.6122437119483948\n",
      "cnt: 0 - valLoss: 0.6215267777442932 - trainLoss: 0.6122278571128845\n",
      "cnt: 0 - valLoss: 0.6215106844902039 - trainLoss: 0.6122120022773743\n",
      "cnt: 0 - valLoss: 0.6214945316314697 - trainLoss: 0.6121960878372192\n",
      "cnt: 0 - valLoss: 0.6214784383773804 - trainLoss: 0.6121802926063538\n",
      "cnt: 0 - valLoss: 0.6214622855186462 - trainLoss: 0.6121644973754883\n",
      "cnt: 0 - valLoss: 0.6214461326599121 - trainLoss: 0.6121485829353333\n",
      "cnt: 0 - valLoss: 0.6214300394058228 - trainLoss: 0.612132728099823\n",
      "cnt: 0 - valLoss: 0.6214138269424438 - trainLoss: 0.6121168732643127\n",
      "cnt: 0 - valLoss: 0.6213977336883545 - trainLoss: 0.6121010184288025\n",
      "cnt: 0 - valLoss: 0.6213815808296204 - trainLoss: 0.6120851635932922\n",
      "cnt: 0 - valLoss: 0.6213654279708862 - trainLoss: 0.6120693683624268\n",
      "cnt: 0 - valLoss: 0.6213492751121521 - trainLoss: 0.6120535135269165\n",
      "cnt: 0 - valLoss: 0.621333122253418 - trainLoss: 0.6120376586914062\n",
      "cnt: 0 - valLoss: 0.6213169693946838 - trainLoss: 0.6120217442512512\n",
      "cnt: 0 - valLoss: 0.6213007569313049 - trainLoss: 0.6120059490203857\n",
      "cnt: 0 - valLoss: 0.6212846636772156 - trainLoss: 0.6119900941848755\n",
      "cnt: 0 - valLoss: 0.6212685108184814 - trainLoss: 0.6119742393493652\n",
      "cnt: 0 - valLoss: 0.6212522983551025 - trainLoss: 0.611958384513855\n",
      "cnt: 0 - valLoss: 0.6212360858917236 - trainLoss: 0.6119425296783447\n",
      "cnt: 0 - valLoss: 0.6212199330329895 - trainLoss: 0.6119266748428345\n",
      "cnt: 0 - valLoss: 0.6212038397789001 - trainLoss: 0.6119108200073242\n",
      "cnt: 0 - valLoss: 0.6211875677108765 - trainLoss: 0.6118949055671692\n",
      "cnt: 0 - valLoss: 0.6211714148521423 - trainLoss: 0.6118790507316589\n",
      "cnt: 0 - valLoss: 0.6211552023887634 - trainLoss: 0.6118631362915039\n",
      "cnt: 0 - valLoss: 0.6211389899253845 - trainLoss: 0.6118472814559937\n",
      "cnt: 0 - valLoss: 0.6211228370666504 - trainLoss: 0.6118313670158386\n",
      "cnt: 0 - valLoss: 0.6211066246032715 - trainLoss: 0.6118155717849731\n",
      "cnt: 0 - valLoss: 0.6210903525352478 - trainLoss: 0.6117996573448181\n",
      "cnt: 0 - valLoss: 0.6210741400718689 - trainLoss: 0.6117838025093079\n",
      "cnt: 0 - valLoss: 0.6210579872131348 - trainLoss: 0.6117678880691528\n",
      "cnt: 0 - valLoss: 0.6210417151451111 - trainLoss: 0.6117519736289978\n",
      "cnt: 0 - valLoss: 0.6210255026817322 - trainLoss: 0.6117361187934875\n",
      "cnt: 0 - valLoss: 0.6210092902183533 - trainLoss: 0.6117202043533325\n",
      "cnt: 0 - valLoss: 0.6209930181503296 - trainLoss: 0.6117043495178223\n",
      "cnt: 0 - valLoss: 0.6209768056869507 - trainLoss: 0.6116884350776672\n",
      "cnt: 0 - valLoss: 0.6209605932235718 - trainLoss: 0.6116725206375122\n",
      "cnt: 0 - valLoss: 0.6209443211555481 - trainLoss: 0.6116566061973572\n",
      "cnt: 0 - valLoss: 0.6209280490875244 - trainLoss: 0.6116407513618469\n",
      "cnt: 0 - valLoss: 0.6209118366241455 - trainLoss: 0.6116248369216919\n",
      "cnt: 0 - valLoss: 0.6208955645561218 - trainLoss: 0.6116089224815369\n",
      "cnt: 0 - valLoss: 0.6208792924880981 - trainLoss: 0.6115930080413818\n",
      "cnt: 0 - valLoss: 0.6208630204200745 - trainLoss: 0.611577033996582\n",
      "cnt: 0 - valLoss: 0.6208467483520508 - trainLoss: 0.611561119556427\n",
      "cnt: 0 - valLoss: 0.6208304762840271 - trainLoss: 0.611545205116272\n",
      "cnt: 0 - valLoss: 0.6208142042160034 - trainLoss: 0.6115292906761169\n",
      "cnt: 0 - valLoss: 0.6207979321479797 - trainLoss: 0.6115133762359619\n",
      "cnt: 0 - valLoss: 0.620781660079956 - trainLoss: 0.6114974617958069\n",
      "cnt: 0 - valLoss: 0.6207653880119324 - trainLoss: 0.6114815473556519\n",
      "cnt: 0 - valLoss: 0.6207490563392639 - trainLoss: 0.6114656925201416\n",
      "cnt: 0 - valLoss: 0.6207327842712402 - trainLoss: 0.6114497780799866\n",
      "cnt: 0 - valLoss: 0.6207164525985718 - trainLoss: 0.6114338636398315\n",
      "cnt: 0 - valLoss: 0.6207001805305481 - trainLoss: 0.6114179491996765\n",
      "cnt: 0 - valLoss: 0.6206839084625244 - trainLoss: 0.6114020943641663\n",
      "cnt: 0 - valLoss: 0.620667576789856 - trainLoss: 0.6113861799240112\n",
      "cnt: 0 - valLoss: 0.6206512451171875 - trainLoss: 0.6113702654838562\n",
      "cnt: 0 - valLoss: 0.6206349730491638 - trainLoss: 0.6113543510437012\n",
      "cnt: 0 - valLoss: 0.6206186413764954 - trainLoss: 0.6113384366035461\n",
      "cnt: 0 - valLoss: 0.6206023097038269 - trainLoss: 0.6113225817680359\n",
      "cnt: 0 - valLoss: 0.6205860376358032 - trainLoss: 0.6113066673278809\n",
      "cnt: 0 - valLoss: 0.6205697059631348 - trainLoss: 0.611290693283081\n",
      "cnt: 0 - valLoss: 0.6205533742904663 - trainLoss: 0.611274778842926\n",
      "cnt: 0 - valLoss: 0.6205369830131531 - trainLoss: 0.611258864402771\n",
      "cnt: 0 - valLoss: 0.6205206513404846 - trainLoss: 0.611242949962616\n",
      "cnt: 0 - valLoss: 0.6205043196678162 - trainLoss: 0.6112270355224609\n",
      "cnt: 0 - valLoss: 0.6204879879951477 - trainLoss: 0.6112111210823059\n",
      "cnt: 0 - valLoss: 0.6204716563224792 - trainLoss: 0.6111951470375061\n",
      "cnt: 0 - valLoss: 0.620455265045166 - trainLoss: 0.6111792325973511\n",
      "cnt: 0 - valLoss: 0.6204389333724976 - trainLoss: 0.6111632585525513\n",
      "cnt: 0 - valLoss: 0.6204225420951843 - trainLoss: 0.6111473441123962\n",
      "cnt: 0 - valLoss: 0.6204061508178711 - trainLoss: 0.6111314296722412\n",
      "cnt: 0 - valLoss: 0.6203897595405579 - trainLoss: 0.6111155152320862\n",
      "cnt: 0 - valLoss: 0.6203733682632446 - trainLoss: 0.6110995411872864\n",
      "cnt: 0 - valLoss: 0.6203569769859314 - trainLoss: 0.6110835671424866\n",
      "cnt: 0 - valLoss: 0.6203406453132629 - trainLoss: 0.6110676527023315\n",
      "cnt: 0 - valLoss: 0.6203243136405945 - trainLoss: 0.611051619052887\n",
      "cnt: 0 - valLoss: 0.6203078627586365 - trainLoss: 0.6110357046127319\n",
      "cnt: 0 - valLoss: 0.6202914714813232 - trainLoss: 0.6110197901725769\n",
      "cnt: 0 - valLoss: 0.62027508020401 - trainLoss: 0.6110038161277771\n",
      "cnt: 0 - valLoss: 0.6202586889266968 - trainLoss: 0.6109878420829773\n",
      "cnt: 0 - valLoss: 0.6202422380447388 - trainLoss: 0.6109718680381775\n",
      "cnt: 0 - valLoss: 0.6202258467674255 - trainLoss: 0.6109558939933777\n",
      "cnt: 0 - valLoss: 0.6202094554901123 - trainLoss: 0.6109399199485779\n",
      "cnt: 0 - valLoss: 0.6201930046081543 - trainLoss: 0.6109238862991333\n",
      "cnt: 0 - valLoss: 0.6201766133308411 - trainLoss: 0.6109079122543335\n",
      "cnt: 0 - valLoss: 0.6201601624488831 - trainLoss: 0.6108918786048889\n",
      "cnt: 0 - valLoss: 0.6201437711715698 - trainLoss: 0.6108759045600891\n",
      "cnt: 0 - valLoss: 0.6201273202896118 - trainLoss: 0.6108598709106445\n",
      "cnt: 0 - valLoss: 0.6201108694076538 - trainLoss: 0.6108438968658447\n",
      "cnt: 0 - valLoss: 0.6200944781303406 - trainLoss: 0.6108278036117554\n",
      "cnt: 0 - valLoss: 0.6200780272483826 - trainLoss: 0.6108118295669556\n",
      "cnt: 0 - valLoss: 0.6200615763664246 - trainLoss: 0.610795795917511\n",
      "cnt: 0 - valLoss: 0.6200450658798218 - trainLoss: 0.6107797622680664\n",
      "cnt: 0 - valLoss: 0.6200287342071533 - trainLoss: 0.6107637286186218\n",
      "cnt: 0 - valLoss: 0.6200121641159058 - trainLoss: 0.6107476353645325\n",
      "cnt: 0 - valLoss: 0.6199957728385925 - trainLoss: 0.6107316613197327\n",
      "cnt: 0 - valLoss: 0.6199793219566345 - trainLoss: 0.6107156276702881\n",
      "cnt: 0 - valLoss: 0.6199628710746765 - trainLoss: 0.6106995940208435\n",
      "cnt: 0 - valLoss: 0.6199463605880737 - trainLoss: 0.6106835603713989\n",
      "cnt: 0 - valLoss: 0.6199299097061157 - trainLoss: 0.6106674671173096\n",
      "cnt: 0 - valLoss: 0.6199133992195129 - trainLoss: 0.6106514930725098\n",
      "cnt: 0 - valLoss: 0.6198968887329102 - trainLoss: 0.6106353998184204\n",
      "cnt: 0 - valLoss: 0.6198803782463074 - trainLoss: 0.610619306564331\n",
      "cnt: 0 - valLoss: 0.6198638677597046 - trainLoss: 0.6106033325195312\n",
      "cnt: 0 - valLoss: 0.6198473572731018 - trainLoss: 0.6105872392654419\n",
      "cnt: 0 - valLoss: 0.619830846786499 - trainLoss: 0.6105711460113525\n",
      "cnt: 0 - valLoss: 0.6198143362998962 - trainLoss: 0.610555112361908\n",
      "cnt: 0 - valLoss: 0.6197978854179382 - trainLoss: 0.6105391383171082\n",
      "cnt: 0 - valLoss: 0.6197813153266907 - trainLoss: 0.6105231046676636\n",
      "cnt: 0 - valLoss: 0.6197648644447327 - trainLoss: 0.6105071306228638\n",
      "cnt: 0 - valLoss: 0.6197482943534851 - trainLoss: 0.6104910969734192\n",
      "cnt: 0 - valLoss: 0.6197318434715271 - trainLoss: 0.6104750633239746\n",
      "cnt: 0 - valLoss: 0.6197153329849243 - trainLoss: 0.61045902967453\n",
      "cnt: 0 - valLoss: 0.6196987628936768 - trainLoss: 0.6104430556297302\n",
      "cnt: 0 - valLoss: 0.6196823120117188 - trainLoss: 0.6104270815849304\n",
      "cnt: 0 - valLoss: 0.619665801525116 - trainLoss: 0.6104110479354858\n",
      "cnt: 0 - valLoss: 0.6196492910385132 - trainLoss: 0.610395073890686\n",
      "cnt: 0 - valLoss: 0.6196327805519104 - trainLoss: 0.6103790402412415\n",
      "cnt: 0 - valLoss: 0.6196162104606628 - trainLoss: 0.6103630065917969\n",
      "cnt: 0 - valLoss: 0.6195997595787048 - trainLoss: 0.6103469729423523\n",
      "cnt: 0 - valLoss: 0.619583249092102 - trainLoss: 0.6103309988975525\n",
      "cnt: 0 - valLoss: 0.6195666790008545 - trainLoss: 0.6103149056434631\n",
      "cnt: 0 - valLoss: 0.6195501089096069 - trainLoss: 0.6102989315986633\n",
      "cnt: 0 - valLoss: 0.6195335984230042 - trainLoss: 0.6102828979492188\n",
      "cnt: 0 - valLoss: 0.6195170283317566 - trainLoss: 0.610266923904419\n",
      "cnt: 0 - valLoss: 0.6195005178451538 - trainLoss: 0.6102508902549744\n",
      "cnt: 0 - valLoss: 0.6194839477539062 - trainLoss: 0.6102348566055298\n",
      "cnt: 0 - valLoss: 0.6194673776626587 - trainLoss: 0.6102187633514404\n",
      "cnt: 0 - valLoss: 0.6194508075714111 - trainLoss: 0.6102027893066406\n",
      "cnt: 0 - valLoss: 0.6194342970848083 - trainLoss: 0.610186755657196\n",
      "cnt: 0 - valLoss: 0.619417667388916 - trainLoss: 0.6101706624031067\n",
      "cnt: 0 - valLoss: 0.6194011569023132 - trainLoss: 0.6101546883583069\n",
      "cnt: 0 - valLoss: 0.6193845272064209 - trainLoss: 0.6101385951042175\n",
      "cnt: 0 - valLoss: 0.6193679571151733 - trainLoss: 0.610122561454773\n",
      "cnt: 0 - valLoss: 0.6193513870239258 - trainLoss: 0.6101064682006836\n",
      "cnt: 0 - valLoss: 0.6193348169326782 - trainLoss: 0.6100904941558838\n",
      "cnt: 0 - valLoss: 0.6193182468414307 - trainLoss: 0.6100744009017944\n",
      "cnt: 0 - valLoss: 0.6193016171455383 - trainLoss: 0.6100583672523499\n",
      "cnt: 0 - valLoss: 0.6192850470542908 - trainLoss: 0.6100422739982605\n",
      "cnt: 0 - valLoss: 0.6192684173583984 - trainLoss: 0.6100262403488159\n",
      "cnt: 0 - valLoss: 0.6192517876625061 - trainLoss: 0.6100101470947266\n",
      "cnt: 0 - valLoss: 0.6192351579666138 - trainLoss: 0.6099940538406372\n",
      "cnt: 0 - valLoss: 0.6192185282707214 - trainLoss: 0.6099780201911926\n",
      "cnt: 0 - valLoss: 0.6192018985748291 - trainLoss: 0.6099618673324585\n",
      "cnt: 0 - valLoss: 0.6191853284835815 - trainLoss: 0.6099458932876587\n",
      "cnt: 0 - valLoss: 0.6191686987876892 - trainLoss: 0.6099298000335693\n",
      "cnt: 0 - valLoss: 0.6191520690917969 - trainLoss: 0.60991370677948\n",
      "cnt: 0 - valLoss: 0.6191354393959045 - trainLoss: 0.6098976135253906\n",
      "cnt: 0 - valLoss: 0.6191187500953674 - trainLoss: 0.6098815202713013\n",
      "cnt: 0 - valLoss: 0.6191021203994751 - trainLoss: 0.6098654866218567\n",
      "cnt: 0 - valLoss: 0.619085431098938 - trainLoss: 0.6098493933677673\n",
      "cnt: 0 - valLoss: 0.6190686225891113 - trainLoss: 0.6098332405090332\n",
      "cnt: 0 - valLoss: 0.6190518736839294 - trainLoss: 0.6098171472549438\n",
      "cnt: 0 - valLoss: 0.6190351247787476 - trainLoss: 0.6098009347915649\n",
      "cnt: 0 - valLoss: 0.6190183162689209 - trainLoss: 0.6097847819328308\n",
      "cnt: 0 - valLoss: 0.619001567363739 - trainLoss: 0.6097686290740967\n",
      "cnt: 0 - valLoss: 0.6189846992492676 - trainLoss: 0.6097525358200073\n",
      "cnt: 0 - valLoss: 0.6189679503440857 - trainLoss: 0.6097363829612732\n",
      "cnt: 0 - valLoss: 0.6189512014389038 - trainLoss: 0.6097201704978943\n",
      "cnt: 0 - valLoss: 0.6189343333244324 - trainLoss: 0.6097040772438049\n",
      "cnt: 0 - valLoss: 0.6189175248146057 - trainLoss: 0.609687864780426\n",
      "cnt: 0 - valLoss: 0.618900716304779 - trainLoss: 0.6096717119216919\n",
      "cnt: 0 - valLoss: 0.6188839077949524 - trainLoss: 0.6096555590629578\n",
      "cnt: 0 - valLoss: 0.618867039680481 - trainLoss: 0.6096394062042236\n",
      "cnt: 0 - valLoss: 0.6188502311706543 - trainLoss: 0.6096232533454895\n",
      "cnt: 0 - valLoss: 0.6188334226608276 - trainLoss: 0.6096070408821106\n",
      "cnt: 0 - valLoss: 0.618816614151001 - trainLoss: 0.6095908880233765\n",
      "cnt: 0 - valLoss: 0.6187997460365295 - trainLoss: 0.6095747351646423\n",
      "cnt: 0 - valLoss: 0.6187828779220581 - trainLoss: 0.6095585227012634\n",
      "cnt: 0 - valLoss: 0.6187660694122314 - trainLoss: 0.6095423698425293\n",
      "cnt: 0 - valLoss: 0.61874920129776 - trainLoss: 0.6095261573791504\n",
      "cnt: 0 - valLoss: 0.6187323927879333 - trainLoss: 0.6095100045204163\n",
      "cnt: 0 - valLoss: 0.6187155246734619 - trainLoss: 0.6094937920570374\n",
      "cnt: 0 - valLoss: 0.6186986565589905 - trainLoss: 0.6094776391983032\n",
      "cnt: 0 - valLoss: 0.6186818480491638 - trainLoss: 0.6094614267349243\n",
      "cnt: 0 - valLoss: 0.6186649203300476 - trainLoss: 0.6094452142715454\n",
      "cnt: 0 - valLoss: 0.6186480522155762 - trainLoss: 0.6094290018081665\n",
      "cnt: 0 - valLoss: 0.61863112449646 - trainLoss: 0.6094128489494324\n",
      "cnt: 0 - valLoss: 0.6186142563819885 - trainLoss: 0.6093965768814087\n",
      "cnt: 0 - valLoss: 0.6185973882675171 - trainLoss: 0.6093803644180298\n",
      "cnt: 0 - valLoss: 0.6185804605484009 - trainLoss: 0.6093641519546509\n",
      "cnt: 0 - valLoss: 0.6185635924339294 - trainLoss: 0.609347939491272\n",
      "cnt: 0 - valLoss: 0.618546724319458 - trainLoss: 0.6093317866325378\n",
      "cnt: 0 - valLoss: 0.6185298562049866 - trainLoss: 0.6093155741691589\n",
      "cnt: 0 - valLoss: 0.6185129284858704 - trainLoss: 0.6092993021011353\n",
      "cnt: 0 - valLoss: 0.6184960007667542 - trainLoss: 0.6092830896377563\n",
      "cnt: 0 - valLoss: 0.6184790730476379 - trainLoss: 0.6092668771743774\n",
      "cnt: 0 - valLoss: 0.6184621453285217 - trainLoss: 0.6092506051063538\n",
      "cnt: 0 - valLoss: 0.6184452176094055 - trainLoss: 0.6092343926429749\n",
      "cnt: 0 - valLoss: 0.6184283494949341 - trainLoss: 0.609218180179596\n",
      "cnt: 0 - valLoss: 0.6184114217758179 - trainLoss: 0.6092019081115723\n",
      "cnt: 0 - valLoss: 0.6183944940567017 - trainLoss: 0.6091856956481934\n",
      "cnt: 0 - valLoss: 0.6183775067329407 - trainLoss: 0.6091694831848145\n",
      "cnt: 0 - valLoss: 0.6183606386184692 - trainLoss: 0.6091532111167908\n",
      "cnt: 0 - valLoss: 0.6183436512947083 - trainLoss: 0.6091369390487671\n",
      "cnt: 0 - valLoss: 0.6183266639709473 - trainLoss: 0.6091206669807434\n",
      "cnt: 0 - valLoss: 0.618309736251831 - trainLoss: 0.6091045141220093\n",
      "cnt: 0 - valLoss: 0.6182927489280701 - trainLoss: 0.6090881824493408\n",
      "cnt: 0 - valLoss: 0.6182758212089539 - trainLoss: 0.6090719699859619\n",
      "cnt: 0 - valLoss: 0.6182588934898376 - trainLoss: 0.6090556979179382\n",
      "cnt: 0 - valLoss: 0.6182419061660767 - trainLoss: 0.6090394258499146\n",
      "cnt: 0 - valLoss: 0.6182249188423157 - trainLoss: 0.6090231537818909\n",
      "cnt: 0 - valLoss: 0.6182079315185547 - trainLoss: 0.6090068817138672\n",
      "cnt: 0 - valLoss: 0.6181909441947937 - trainLoss: 0.6089906096458435\n",
      "cnt: 0 - valLoss: 0.6181740164756775 - trainLoss: 0.6089743971824646\n",
      "cnt: 0 - valLoss: 0.6181570291519165 - trainLoss: 0.6089580655097961\n",
      "cnt: 0 - valLoss: 0.6181400418281555 - trainLoss: 0.6089417338371277\n",
      "cnt: 0 - valLoss: 0.6181230545043945 - trainLoss: 0.6089255213737488\n",
      "cnt: 0 - valLoss: 0.6181060075759888 - trainLoss: 0.6089092493057251\n",
      "cnt: 0 - valLoss: 0.6180890798568726 - trainLoss: 0.6088929772377014\n",
      "cnt: 0 - valLoss: 0.6180720925331116 - trainLoss: 0.608876645565033\n",
      "cnt: 0 - valLoss: 0.6180550456047058 - trainLoss: 0.6088603734970093\n",
      "cnt: 0 - valLoss: 0.6180381178855896 - trainLoss: 0.6088441014289856\n",
      "cnt: 0 - valLoss: 0.6180210709571838 - trainLoss: 0.6088278293609619\n",
      "cnt: 0 - valLoss: 0.6180040240287781 - trainLoss: 0.6088114976882935\n",
      "cnt: 0 - valLoss: 0.6179869771003723 - trainLoss: 0.608795166015625\n",
      "cnt: 0 - valLoss: 0.6179699897766113 - trainLoss: 0.6087788343429565\n",
      "cnt: 0 - valLoss: 0.6179529428482056 - trainLoss: 0.6087625622749329\n",
      "cnt: 0 - valLoss: 0.6179359555244446 - trainLoss: 0.6087462306022644\n",
      "cnt: 0 - valLoss: 0.6179189682006836 - trainLoss: 0.6087299585342407\n",
      "cnt: 0 - valLoss: 0.6179018616676331 - trainLoss: 0.6087136268615723\n",
      "cnt: 0 - valLoss: 0.6178849339485168 - trainLoss: 0.6086972951889038\n",
      "cnt: 0 - valLoss: 0.6178678274154663 - trainLoss: 0.6086810231208801\n",
      "cnt: 0 - valLoss: 0.6178507804870605 - trainLoss: 0.6086646914482117\n",
      "cnt: 0 - valLoss: 0.6178337931632996 - trainLoss: 0.6086483001708984\n",
      "cnt: 0 - valLoss: 0.617816686630249 - trainLoss: 0.6086320281028748\n",
      "cnt: 0 - valLoss: 0.6177995800971985 - trainLoss: 0.6086156964302063\n",
      "cnt: 0 - valLoss: 0.6177825331687927 - trainLoss: 0.6085993647575378\n",
      "cnt: 0 - valLoss: 0.617765486240387 - trainLoss: 0.6085829734802246\n",
      "cnt: 0 - valLoss: 0.6177483797073364 - trainLoss: 0.6085666418075562\n",
      "cnt: 0 - valLoss: 0.6177313327789307 - trainLoss: 0.6085503697395325\n",
      "cnt: 0 - valLoss: 0.6177142858505249 - trainLoss: 0.6085339188575745\n",
      "cnt: 0 - valLoss: 0.6176971793174744 - trainLoss: 0.608517587184906\n",
      "cnt: 0 - valLoss: 0.6176800727844238 - trainLoss: 0.6085013151168823\n",
      "cnt: 0 - valLoss: 0.6176630258560181 - trainLoss: 0.6084849238395691\n",
      "cnt: 0 - valLoss: 0.6176459193229675 - trainLoss: 0.6084685325622559\n",
      "cnt: 0 - valLoss: 0.617628812789917 - trainLoss: 0.6084522008895874\n",
      "cnt: 0 - valLoss: 0.6176117062568665 - trainLoss: 0.608435869216919\n",
      "cnt: 0 - valLoss: 0.6175945997238159 - trainLoss: 0.6084194779396057\n",
      "cnt: 0 - valLoss: 0.6175774931907654 - trainLoss: 0.6084030866622925\n",
      "cnt: 0 - valLoss: 0.6175603866577148 - trainLoss: 0.6083868145942688\n",
      "cnt: 0 - valLoss: 0.6175432801246643 - trainLoss: 0.6083703637123108\n",
      "cnt: 0 - valLoss: 0.617526113986969 - trainLoss: 0.6083540320396423\n",
      "cnt: 0 - valLoss: 0.6175090670585632 - trainLoss: 0.6083376407623291\n",
      "cnt: 0 - valLoss: 0.6174920797348022 - trainLoss: 0.6083213090896606\n",
      "cnt: 0 - valLoss: 0.6174750328063965 - trainLoss: 0.6083048582077026\n",
      "cnt: 0 - valLoss: 0.6174579858779907 - trainLoss: 0.6082885265350342\n",
      "cnt: 0 - valLoss: 0.6174408793449402 - trainLoss: 0.6082720756530762\n",
      "cnt: 0 - valLoss: 0.6174238920211792 - trainLoss: 0.6082557439804077\n",
      "cnt: 0 - valLoss: 0.6174068450927734 - trainLoss: 0.6082392930984497\n",
      "cnt: 0 - valLoss: 0.6173897385597229 - trainLoss: 0.6082229614257812\n",
      "cnt: 0 - valLoss: 0.6173726916313171 - trainLoss: 0.6082065105438232\n",
      "cnt: 0 - valLoss: 0.6173555850982666 - trainLoss: 0.60819011926651\n",
      "cnt: 0 - valLoss: 0.6173385381698608 - trainLoss: 0.6081737279891968\n",
      "cnt: 0 - valLoss: 0.6173214316368103 - trainLoss: 0.6081572771072388\n",
      "cnt: 0 - valLoss: 0.6173044443130493 - trainLoss: 0.6081408858299255\n",
      "cnt: 0 - valLoss: 0.617287278175354 - trainLoss: 0.6081244945526123\n",
      "cnt: 0 - valLoss: 0.6172702312469482 - trainLoss: 0.6081080436706543\n",
      "cnt: 0 - valLoss: 0.6172530651092529 - trainLoss: 0.6080916523933411\n",
      "cnt: 0 - valLoss: 0.6172360777854919 - trainLoss: 0.6080752611160278\n",
      "cnt: 0 - valLoss: 0.6172189116477966 - trainLoss: 0.6080588698387146\n",
      "cnt: 0 - valLoss: 0.6172018051147461 - trainLoss: 0.6080424189567566\n",
      "cnt: 0 - valLoss: 0.6171846985816956 - trainLoss: 0.6080259680747986\n",
      "cnt: 0 - valLoss: 0.617167592048645 - trainLoss: 0.6080095171928406\n",
      "cnt: 0 - valLoss: 0.6171504855155945 - trainLoss: 0.6079931259155273\n",
      "cnt: 0 - valLoss: 0.617133378982544 - trainLoss: 0.6079767346382141\n",
      "cnt: 0 - valLoss: 0.6171162128448486 - trainLoss: 0.6079602241516113\n",
      "cnt: 0 - valLoss: 0.6170991659164429 - trainLoss: 0.6079437732696533\n",
      "cnt: 0 - valLoss: 0.6170819997787476 - trainLoss: 0.6079273819923401\n",
      "cnt: 0 - valLoss: 0.617064893245697 - trainLoss: 0.6079108715057373\n",
      "cnt: 0 - valLoss: 0.6170476675033569 - trainLoss: 0.6078944802284241\n",
      "cnt: 0 - valLoss: 0.6170305013656616 - trainLoss: 0.6078780293464661\n",
      "cnt: 0 - valLoss: 0.6170133352279663 - trainLoss: 0.6078615784645081\n",
      "cnt: 0 - valLoss: 0.6169961094856262 - trainLoss: 0.60784512758255\n",
      "cnt: 0 - valLoss: 0.6169788837432861 - trainLoss: 0.607828676700592\n",
      "cnt: 0 - valLoss: 0.6169617176055908 - trainLoss: 0.6078122854232788\n",
      "cnt: 0 - valLoss: 0.6169444918632507 - trainLoss: 0.607795774936676\n",
      "cnt: 0 - valLoss: 0.6169273257255554 - trainLoss: 0.6077793836593628\n",
      "cnt: 0 - valLoss: 0.6169100403785706 - trainLoss: 0.60776287317276\n",
      "cnt: 0 - valLoss: 0.6168928146362305 - trainLoss: 0.6077464818954468\n",
      "cnt: 0 - valLoss: 0.6168755888938904 - trainLoss: 0.607729971408844\n",
      "cnt: 0 - valLoss: 0.6168584227561951 - trainLoss: 0.607713520526886\n",
      "cnt: 0 - valLoss: 0.6168411374092102 - trainLoss: 0.607697069644928\n",
      "cnt: 0 - valLoss: 0.6168238520622253 - trainLoss: 0.6076805591583252\n",
      "cnt: 0 - valLoss: 0.6168066263198853 - trainLoss: 0.6076641082763672\n",
      "cnt: 0 - valLoss: 0.6167894005775452 - trainLoss: 0.6076476573944092\n",
      "cnt: 0 - valLoss: 0.6167721748352051 - trainLoss: 0.6076311469078064\n",
      "cnt: 0 - valLoss: 0.6167548894882202 - trainLoss: 0.6076146960258484\n",
      "cnt: 0 - valLoss: 0.6167376041412354 - trainLoss: 0.6075982451438904\n",
      "cnt: 0 - valLoss: 0.6167203783988953 - trainLoss: 0.6075816750526428\n",
      "cnt: 0 - valLoss: 0.6167030930519104 - trainLoss: 0.6075652837753296\n",
      "cnt: 0 - valLoss: 0.6166858077049255 - trainLoss: 0.607548713684082\n",
      "cnt: 0 - valLoss: 0.6166684627532959 - trainLoss: 0.6075322031974792\n",
      "cnt: 0 - valLoss: 0.6166512370109558 - trainLoss: 0.6075156927108765\n",
      "cnt: 0 - valLoss: 0.6166340112686157 - trainLoss: 0.6074991226196289\n",
      "cnt: 0 - valLoss: 0.6166166663169861 - trainLoss: 0.6074826121330261\n",
      "cnt: 0 - valLoss: 0.6165993213653564 - trainLoss: 0.6074661016464233\n",
      "cnt: 0 - valLoss: 0.6165820956230164 - trainLoss: 0.6074495315551758\n",
      "cnt: 0 - valLoss: 0.6165647506713867 - trainLoss: 0.607433021068573\n",
      "cnt: 0 - valLoss: 0.6165474653244019 - trainLoss: 0.6074165105819702\n",
      "cnt: 0 - valLoss: 0.6165301203727722 - trainLoss: 0.6073998808860779\n",
      "cnt: 0 - valLoss: 0.6165128350257874 - trainLoss: 0.6073833703994751\n",
      "cnt: 0 - valLoss: 0.6164954900741577 - trainLoss: 0.6073668003082275\n",
      "cnt: 0 - valLoss: 0.6164781451225281 - trainLoss: 0.60735023021698\n",
      "cnt: 0 - valLoss: 0.6164608597755432 - trainLoss: 0.6073337197303772\n",
      "cnt: 0 - valLoss: 0.6164435148239136 - trainLoss: 0.6073170900344849\n",
      "cnt: 0 - valLoss: 0.6164261698722839 - trainLoss: 0.6073005795478821\n",
      "cnt: 0 - valLoss: 0.6164088845252991 - trainLoss: 0.6072839498519897\n",
      "cnt: 0 - valLoss: 0.6163914799690247 - trainLoss: 0.6072673797607422\n",
      "cnt: 0 - valLoss: 0.616374135017395 - trainLoss: 0.6072508692741394\n",
      "cnt: 0 - valLoss: 0.6163567900657654 - trainLoss: 0.6072342991828918\n",
      "cnt: 0 - valLoss: 0.6163394451141357 - trainLoss: 0.6072177290916443\n",
      "cnt: 0 - valLoss: 0.6163220405578613 - trainLoss: 0.607201099395752\n",
      "cnt: 0 - valLoss: 0.6163046956062317 - trainLoss: 0.6071844696998596\n",
      "cnt: 0 - valLoss: 0.6162872910499573 - trainLoss: 0.6071679592132568\n",
      "cnt: 0 - valLoss: 0.6162699460983276 - trainLoss: 0.6071513891220093\n",
      "cnt: 0 - valLoss: 0.6162525415420532 - trainLoss: 0.6071347594261169\n",
      "cnt: 0 - valLoss: 0.6162351965904236 - trainLoss: 0.6071181297302246\n",
      "cnt: 0 - valLoss: 0.6162177920341492 - trainLoss: 0.607101559638977\n",
      "cnt: 0 - valLoss: 0.6162003874778748 - trainLoss: 0.6070849895477295\n",
      "cnt: 0 - valLoss: 0.6161830425262451 - trainLoss: 0.6070683598518372\n",
      "cnt: 0 - valLoss: 0.6161656379699707 - trainLoss: 0.6070517301559448\n",
      "cnt: 0 - valLoss: 0.6161481738090515 - trainLoss: 0.6070351600646973\n",
      "cnt: 0 - valLoss: 0.6161308288574219 - trainLoss: 0.6070185303688049\n",
      "cnt: 0 - valLoss: 0.6161133646965027 - trainLoss: 0.6070019006729126\n",
      "cnt: 0 - valLoss: 0.6160959601402283 - trainLoss: 0.6069852709770203\n",
      "cnt: 0 - valLoss: 0.6160785555839539 - trainLoss: 0.6069687008857727\n",
      "cnt: 0 - valLoss: 0.6160611510276794 - trainLoss: 0.6069520115852356\n",
      "cnt: 0 - valLoss: 0.616043746471405 - trainLoss: 0.6069353818893433\n",
      "cnt: 0 - valLoss: 0.6160262823104858 - trainLoss: 0.6069188117980957\n",
      "cnt: 0 - valLoss: 0.6160088777542114 - trainLoss: 0.6069021821022034\n",
      "cnt: 0 - valLoss: 0.6159914135932922 - trainLoss: 0.606885552406311\n",
      "cnt: 0 - valLoss: 0.615973949432373 - trainLoss: 0.6068688631057739\n",
      "cnt: 0 - valLoss: 0.6159565448760986 - trainLoss: 0.6068521738052368\n",
      "cnt: 0 - valLoss: 0.6159390807151794 - trainLoss: 0.6068356037139893\n",
      "cnt: 0 - valLoss: 0.6159216165542603 - trainLoss: 0.6068189144134521\n",
      "cnt: 0 - valLoss: 0.6159041523933411 - trainLoss: 0.6068021655082703\n",
      "cnt: 0 - valLoss: 0.6158866286277771 - trainLoss: 0.6067854762077332\n",
      "cnt: 0 - valLoss: 0.6158692240715027 - trainLoss: 0.606768786907196\n",
      "cnt: 0 - valLoss: 0.6158517003059387 - trainLoss: 0.6067520976066589\n",
      "cnt: 0 - valLoss: 0.6158342957496643 - trainLoss: 0.6067354083061218\n",
      "cnt: 0 - valLoss: 0.6158167123794556 - trainLoss: 0.6067187190055847\n",
      "cnt: 0 - valLoss: 0.6157993078231812 - trainLoss: 0.6067020297050476\n",
      "cnt: 0 - valLoss: 0.6157817840576172 - trainLoss: 0.6066852807998657\n",
      "cnt: 0 - valLoss: 0.615764319896698 - trainLoss: 0.6066685914993286\n",
      "cnt: 0 - valLoss: 0.6157468557357788 - trainLoss: 0.6066519021987915\n",
      "cnt: 0 - valLoss: 0.6157292723655701 - trainLoss: 0.6066351532936096\n",
      "cnt: 0 - valLoss: 0.6157117486000061 - trainLoss: 0.6066184639930725\n",
      "cnt: 0 - valLoss: 0.6156942844390869 - trainLoss: 0.6066017746925354\n",
      "cnt: 0 - valLoss: 0.6156767010688782 - trainLoss: 0.6065850257873535\n",
      "cnt: 0 - valLoss: 0.615659236907959 - trainLoss: 0.6065683364868164\n",
      "cnt: 0 - valLoss: 0.6156416535377502 - trainLoss: 0.6065515279769897\n",
      "cnt: 0 - valLoss: 0.615624189376831 - trainLoss: 0.6065348386764526\n",
      "cnt: 0 - valLoss: 0.6156066060066223 - trainLoss: 0.6065180897712708\n",
      "cnt: 0 - valLoss: 0.6155890822410583 - trainLoss: 0.6065014004707336\n",
      "cnt: 0 - valLoss: 0.6155714988708496 - trainLoss: 0.606484591960907\n",
      "cnt: 0 - valLoss: 0.6155539751052856 - trainLoss: 0.6064679026603699\n",
      "cnt: 0 - valLoss: 0.6155364513397217 - trainLoss: 0.606451153755188\n",
      "cnt: 0 - valLoss: 0.6155188679695129 - trainLoss: 0.6064344048500061\n",
      "cnt: 0 - valLoss: 0.615501344203949 - trainLoss: 0.6064176559448242\n",
      "cnt: 0 - valLoss: 0.6154837608337402 - trainLoss: 0.6064009070396423\n",
      "cnt: 0 - valLoss: 0.6154661774635315 - trainLoss: 0.6063841581344604\n",
      "cnt: 0 - valLoss: 0.6154487133026123 - trainLoss: 0.6063674092292786\n",
      "cnt: 0 - valLoss: 0.615431010723114 - trainLoss: 0.6063506603240967\n",
      "cnt: 0 - valLoss: 0.61541348695755 - trainLoss: 0.60633385181427\n",
      "cnt: 0 - valLoss: 0.6153959035873413 - trainLoss: 0.6063171029090881\n",
      "cnt: 0 - valLoss: 0.6153783202171326 - trainLoss: 0.606300413608551\n",
      "cnt: 0 - valLoss: 0.6153607368469238 - trainLoss: 0.6062836050987244\n",
      "cnt: 0 - valLoss: 0.6153430938720703 - trainLoss: 0.6062667965888977\n",
      "cnt: 0 - valLoss: 0.6153255105018616 - trainLoss: 0.606249988079071\n",
      "cnt: 0 - valLoss: 0.6153079271316528 - trainLoss: 0.6062332391738892\n",
      "cnt: 0 - valLoss: 0.6152903437614441 - trainLoss: 0.6062164902687073\n",
      "cnt: 0 - valLoss: 0.6152727007865906 - trainLoss: 0.6061996221542358\n",
      "cnt: 0 - valLoss: 0.6152550578117371 - trainLoss: 0.606182873249054\n",
      "cnt: 0 - valLoss: 0.6152374744415283 - trainLoss: 0.6061660647392273\n",
      "cnt: 0 - valLoss: 0.6152198314666748 - trainLoss: 0.6061492562294006\n",
      "cnt: 0 - valLoss: 0.6152021884918213 - trainLoss: 0.606132447719574\n",
      "cnt: 0 - valLoss: 0.6151845455169678 - trainLoss: 0.6061155796051025\n",
      "cnt: 0 - valLoss: 0.6151669025421143 - trainLoss: 0.6060987710952759\n",
      "cnt: 0 - valLoss: 0.6151492595672607 - trainLoss: 0.6060819029808044\n",
      "cnt: 0 - valLoss: 0.6151316165924072 - trainLoss: 0.606065034866333\n",
      "cnt: 0 - valLoss: 0.6151139736175537 - trainLoss: 0.6060482263565063\n",
      "cnt: 0 - valLoss: 0.6150963306427002 - trainLoss: 0.6060313582420349\n",
      "cnt: 0 - valLoss: 0.6150786280632019 - trainLoss: 0.6060144901275635\n",
      "cnt: 0 - valLoss: 0.6150609850883484 - trainLoss: 0.605997622013092\n",
      "cnt: 0 - valLoss: 0.6150432825088501 - trainLoss: 0.6059808135032654\n",
      "cnt: 0 - valLoss: 0.6150256395339966 - trainLoss: 0.605963945388794\n",
      "cnt: 0 - valLoss: 0.6150079965591431 - trainLoss: 0.6059470772743225\n",
      "cnt: 0 - valLoss: 0.6149902939796448 - trainLoss: 0.6059302091598511\n",
      "cnt: 0 - valLoss: 0.6149725914001465 - trainLoss: 0.6059133410453796\n",
      "cnt: 0 - valLoss: 0.6149548888206482 - trainLoss: 0.6058964133262634\n",
      "cnt: 0 - valLoss: 0.6149371862411499 - trainLoss: 0.605879545211792\n",
      "cnt: 0 - valLoss: 0.6149194836616516 - trainLoss: 0.6058626770973206\n",
      "cnt: 0 - valLoss: 0.6149018406867981 - trainLoss: 0.6058458089828491\n",
      "cnt: 0 - valLoss: 0.6148841381072998 - trainLoss: 0.6058289408683777\n",
      "cnt: 0 - valLoss: 0.6148663759231567 - trainLoss: 0.6058120131492615\n",
      "cnt: 0 - valLoss: 0.6148486733436584 - trainLoss: 0.60579514503479\n",
      "cnt: 0 - valLoss: 0.6148309111595154 - trainLoss: 0.6057782173156738\n",
      "cnt: 0 - valLoss: 0.6148132085800171 - trainLoss: 0.6057612299919128\n",
      "cnt: 0 - valLoss: 0.614795446395874 - trainLoss: 0.6057443618774414\n",
      "cnt: 0 - valLoss: 0.6147777438163757 - trainLoss: 0.6057273745536804\n",
      "cnt: 0 - valLoss: 0.6147599816322327 - trainLoss: 0.6057103872299194\n",
      "cnt: 0 - valLoss: 0.6147422194480896 - trainLoss: 0.605693519115448\n",
      "cnt: 0 - valLoss: 0.6147245168685913 - trainLoss: 0.605676531791687\n",
      "cnt: 0 - valLoss: 0.6147067546844482 - trainLoss: 0.605659544467926\n",
      "cnt: 0 - valLoss: 0.6146889925003052 - trainLoss: 0.6056426167488098\n",
      "cnt: 0 - valLoss: 0.6146711707115173 - trainLoss: 0.605625569820404\n",
      "cnt: 0 - valLoss: 0.6146534085273743 - trainLoss: 0.6056086421012878\n",
      "cnt: 0 - valLoss: 0.614635705947876 - trainLoss: 0.6055915951728821\n",
      "cnt: 0 - valLoss: 0.6146178841590881 - trainLoss: 0.6055746078491211\n",
      "cnt: 0 - valLoss: 0.6146001219749451 - trainLoss: 0.6055576205253601\n",
      "cnt: 0 - valLoss: 0.6145823001861572 - trainLoss: 0.6055405735969543\n",
      "cnt: 0 - valLoss: 0.6145645380020142 - trainLoss: 0.6055235862731934\n",
      "cnt: 0 - valLoss: 0.6145467162132263 - trainLoss: 0.6055064797401428\n",
      "cnt: 0 - valLoss: 0.6145288944244385 - trainLoss: 0.6054894924163818\n",
      "cnt: 0 - valLoss: 0.6145110130310059 - trainLoss: 0.6054725050926208\n",
      "cnt: 0 - valLoss: 0.6144930124282837 - trainLoss: 0.6054554581642151\n",
      "cnt: 0 - valLoss: 0.6144751906394958 - trainLoss: 0.6054384708404541\n",
      "cnt: 0 - valLoss: 0.6144573092460632 - trainLoss: 0.6054214239120483\n",
      "cnt: 0 - valLoss: 0.6144393682479858 - trainLoss: 0.6054043769836426\n",
      "cnt: 0 - valLoss: 0.6144214868545532 - trainLoss: 0.6053873300552368\n",
      "cnt: 0 - valLoss: 0.6144035458564758 - trainLoss: 0.605370283126831\n",
      "cnt: 0 - valLoss: 0.6143856048583984 - trainLoss: 0.6053532958030701\n",
      "cnt: 0 - valLoss: 0.6143677234649658 - trainLoss: 0.6053361296653748\n",
      "cnt: 0 - valLoss: 0.6143497824668884 - trainLoss: 0.6053191423416138\n",
      "cnt: 0 - valLoss: 0.6143319010734558 - trainLoss: 0.605302095413208\n",
      "cnt: 0 - valLoss: 0.6143139600753784 - trainLoss: 0.6052849888801575\n",
      "cnt: 0 - valLoss: 0.614296019077301 - trainLoss: 0.6052679419517517\n",
      "cnt: 0 - valLoss: 0.6142780184745789 - trainLoss: 0.6052507758140564\n",
      "cnt: 0 - valLoss: 0.6142600774765015 - trainLoss: 0.6052337884902954\n",
      "cnt: 0 - valLoss: 0.6142420768737793 - trainLoss: 0.6052166819572449\n",
      "cnt: 0 - valLoss: 0.6142240166664124 - trainLoss: 0.6051996350288391\n",
      "cnt: 0 - valLoss: 0.6142059564590454 - trainLoss: 0.6051824688911438\n",
      "cnt: 0 - valLoss: 0.6141879558563232 - trainLoss: 0.6051654815673828\n",
      "cnt: 0 - valLoss: 0.6141698956489563 - trainLoss: 0.6051483154296875\n",
      "cnt: 0 - valLoss: 0.6141518354415894 - trainLoss: 0.6051312685012817\n",
      "cnt: 0 - valLoss: 0.6141337752342224 - trainLoss: 0.6051141619682312\n",
      "cnt: 0 - valLoss: 0.6141156554222107 - trainLoss: 0.6050969958305359\n",
      "cnt: 0 - valLoss: 0.6140976548194885 - trainLoss: 0.6050798892974854\n",
      "cnt: 0 - valLoss: 0.6140795946121216 - trainLoss: 0.6050627827644348\n",
      "cnt: 0 - valLoss: 0.6140615344047546 - trainLoss: 0.6050456166267395\n",
      "cnt: 0 - valLoss: 0.6140434145927429 - trainLoss: 0.605028510093689\n",
      "cnt: 0 - valLoss: 0.614025354385376 - trainLoss: 0.6050113439559937\n",
      "cnt: 0 - valLoss: 0.6140072345733643 - trainLoss: 0.6049942374229431\n",
      "cnt: 0 - valLoss: 0.6139891743659973 - trainLoss: 0.6049771308898926\n",
      "cnt: 0 - valLoss: 0.6139710545539856 - trainLoss: 0.6049599051475525\n",
      "cnt: 0 - valLoss: 0.6139529943466187 - trainLoss: 0.604942798614502\n",
      "cnt: 0 - valLoss: 0.6139348745346069 - trainLoss: 0.6049256920814514\n",
      "cnt: 0 - valLoss: 0.6139167547225952 - trainLoss: 0.6049084663391113\n",
      "cnt: 0 - valLoss: 0.6138986349105835 - trainLoss: 0.6048913598060608\n",
      "cnt: 0 - valLoss: 0.6138805150985718 - trainLoss: 0.6048741936683655\n",
      "cnt: 0 - valLoss: 0.6138623952865601 - trainLoss: 0.6048570275306702\n",
      "cnt: 0 - valLoss: 0.6138443350791931 - trainLoss: 0.6048399209976196\n",
      "cnt: 0 - valLoss: 0.6138262152671814 - trainLoss: 0.6048226952552795\n",
      "cnt: 0 - valLoss: 0.6138080358505249 - trainLoss: 0.6048055291175842\n",
      "cnt: 0 - valLoss: 0.6137899160385132 - trainLoss: 0.6047884225845337\n",
      "cnt: 0 - valLoss: 0.6137717366218567 - trainLoss: 0.6047712564468384\n",
      "cnt: 0 - valLoss: 0.6137536764144897 - trainLoss: 0.6047541499137878\n",
      "cnt: 0 - valLoss: 0.6137354969978333 - trainLoss: 0.6047369837760925\n",
      "cnt: 0 - valLoss: 0.6137174367904663 - trainLoss: 0.6047198176383972\n",
      "cnt: 0 - valLoss: 0.6136992573738098 - trainLoss: 0.6047027111053467\n",
      "cnt: 0 - valLoss: 0.6136810779571533 - trainLoss: 0.6046854853630066\n",
      "cnt: 0 - valLoss: 0.6136629581451416 - trainLoss: 0.6046683192253113\n",
      "cnt: 0 - valLoss: 0.6136447787284851 - trainLoss: 0.604651153087616\n",
      "cnt: 0 - valLoss: 0.6136266589164734 - trainLoss: 0.6046339869499207\n",
      "cnt: 0 - valLoss: 0.6136084794998169 - trainLoss: 0.6046168208122253\n",
      "cnt: 0 - valLoss: 0.6135903000831604 - trainLoss: 0.60459965467453\n",
      "cnt: 0 - valLoss: 0.6135721206665039 - trainLoss: 0.6045824885368347\n",
      "cnt: 0 - valLoss: 0.6135538220405579 - trainLoss: 0.6045653223991394\n",
      "cnt: 0 - valLoss: 0.6135355234146118 - trainLoss: 0.6045480966567993\n",
      "cnt: 0 - valLoss: 0.6135172843933105 - trainLoss: 0.6045309901237488\n",
      "cnt: 0 - valLoss: 0.6134990453720093 - trainLoss: 0.6045137643814087\n",
      "cnt: 0 - valLoss: 0.6134807467460632 - trainLoss: 0.6044965982437134\n",
      "cnt: 0 - valLoss: 0.613462507724762 - trainLoss: 0.6044794321060181\n",
      "cnt: 0 - valLoss: 0.6134442687034607 - trainLoss: 0.6044622659683228\n",
      "cnt: 0 - valLoss: 0.6134259700775146 - trainLoss: 0.6044450998306274\n",
      "cnt: 0 - valLoss: 0.6134076714515686 - trainLoss: 0.6044278740882874\n",
      "cnt: 0 - valLoss: 0.6133894324302673 - trainLoss: 0.604410707950592\n",
      "cnt: 0 - valLoss: 0.6133711934089661 - trainLoss: 0.6043936014175415\n",
      "cnt: 0 - valLoss: 0.6133528351783752 - trainLoss: 0.6043764352798462\n",
      "cnt: 0 - valLoss: 0.613334596157074 - trainLoss: 0.6043591499328613\n",
      "cnt: 0 - valLoss: 0.6133162975311279 - trainLoss: 0.6043420433998108\n",
      "cnt: 0 - valLoss: 0.6132979393005371 - trainLoss: 0.6043248176574707\n",
      "cnt: 0 - valLoss: 0.6132797002792358 - trainLoss: 0.6043076515197754\n",
      "cnt: 0 - valLoss: 0.613261342048645 - trainLoss: 0.6042904257774353\n",
      "cnt: 0 - valLoss: 0.613243043422699 - trainLoss: 0.6042732000350952\n",
      "cnt: 0 - valLoss: 0.6132247447967529 - trainLoss: 0.6042560338973999\n",
      "cnt: 0 - valLoss: 0.6132064461708069 - trainLoss: 0.6042388081550598\n",
      "cnt: 0 - valLoss: 0.6131880879402161 - trainLoss: 0.6042216420173645\n",
      "cnt: 0 - valLoss: 0.61316978931427 - trainLoss: 0.6042044162750244\n",
      "cnt: 0 - valLoss: 0.6131513714790344 - trainLoss: 0.6041871905326843\n",
      "cnt: 0 - valLoss: 0.6131330728530884 - trainLoss: 0.6041699051856995\n",
      "cnt: 0 - valLoss: 0.6131147146224976 - trainLoss: 0.6041526794433594\n",
      "cnt: 0 - valLoss: 0.6130963563919067 - trainLoss: 0.6041353940963745\n",
      "cnt: 0 - valLoss: 0.6130779981613159 - trainLoss: 0.6041181683540344\n",
      "cnt: 0 - valLoss: 0.6130596399307251 - trainLoss: 0.6041008830070496\n",
      "cnt: 0 - valLoss: 0.6130412220954895 - trainLoss: 0.6040835976600647\n",
      "cnt: 0 - valLoss: 0.6130229234695435 - trainLoss: 0.6040663123130798\n",
      "cnt: 0 - valLoss: 0.6130045056343079 - trainLoss: 0.6040490865707397\n",
      "cnt: 0 - valLoss: 0.6129860877990723 - trainLoss: 0.6040318012237549\n",
      "cnt: 0 - valLoss: 0.6129677295684814 - trainLoss: 0.60401451587677\n",
      "cnt: 0 - valLoss: 0.6129493713378906 - trainLoss: 0.6039972305297852\n",
      "cnt: 0 - valLoss: 0.6129310131072998 - trainLoss: 0.6039798259735107\n",
      "cnt: 0 - valLoss: 0.6129125356674194 - trainLoss: 0.6039626598358154\n",
      "cnt: 0 - valLoss: 0.6128942370414734 - trainLoss: 0.6039453744888306\n",
      "cnt: 0 - valLoss: 0.612875759601593 - trainLoss: 0.6039280295372009\n",
      "cnt: 0 - valLoss: 0.6128573417663574 - trainLoss: 0.6039107441902161\n",
      "cnt: 0 - valLoss: 0.6128389239311218 - trainLoss: 0.6038933992385864\n",
      "cnt: 0 - valLoss: 0.6128203868865967 - trainLoss: 0.6038761138916016\n",
      "cnt: 0 - valLoss: 0.6128019094467163 - trainLoss: 0.6038588285446167\n",
      "cnt: 0 - valLoss: 0.6127834320068359 - trainLoss: 0.6038415431976318\n",
      "cnt: 0 - valLoss: 0.6127648949623108 - trainLoss: 0.6038241982460022\n",
      "cnt: 0 - valLoss: 0.6127464175224304 - trainLoss: 0.6038068532943726\n",
      "cnt: 0 - valLoss: 0.6127278804779053 - trainLoss: 0.6037895679473877\n",
      "cnt: 0 - valLoss: 0.6127094030380249 - trainLoss: 0.6037722826004028\n",
      "cnt: 0 - valLoss: 0.612690806388855 - trainLoss: 0.6037548780441284\n",
      "cnt: 0 - valLoss: 0.6126722693443298 - trainLoss: 0.6037375926971436\n",
      "cnt: 0 - valLoss: 0.6126537322998047 - trainLoss: 0.6037202477455139\n",
      "cnt: 0 - valLoss: 0.6126351952552795 - trainLoss: 0.6037029027938843\n",
      "cnt: 0 - valLoss: 0.6126166582107544 - trainLoss: 0.6036856174468994\n",
      "cnt: 0 - valLoss: 0.6125980615615845 - trainLoss: 0.6036682724952698\n",
      "cnt: 0 - valLoss: 0.6125795245170593 - trainLoss: 0.6036508679389954\n",
      "cnt: 0 - valLoss: 0.6125609278678894 - trainLoss: 0.6036335825920105\n",
      "cnt: 0 - valLoss: 0.6125423908233643 - trainLoss: 0.6036162376403809\n",
      "cnt: 0 - valLoss: 0.6125237941741943 - trainLoss: 0.6035988330841064\n",
      "cnt: 0 - valLoss: 0.6125052571296692 - trainLoss: 0.6035814881324768\n",
      "cnt: 0 - valLoss: 0.612486720085144 - trainLoss: 0.6035641431808472\n",
      "cnt: 0 - valLoss: 0.6124681830406189 - trainLoss: 0.6035467982292175\n",
      "cnt: 0 - valLoss: 0.612449586391449 - trainLoss: 0.6035293936729431\n",
      "cnt: 0 - valLoss: 0.6124311089515686 - trainLoss: 0.6035120487213135\n",
      "cnt: 0 - valLoss: 0.6124125123023987 - trainLoss: 0.6034946441650391\n",
      "cnt: 0 - valLoss: 0.6123940348625183 - trainLoss: 0.6034772992134094\n",
      "cnt: 0 - valLoss: 0.6123754382133484 - trainLoss: 0.6034599542617798\n",
      "cnt: 0 - valLoss: 0.612356960773468 - trainLoss: 0.6034424901008606\n",
      "cnt: 0 - valLoss: 0.6123383641242981 - trainLoss: 0.603425145149231\n",
      "cnt: 0 - valLoss: 0.6123197078704834 - trainLoss: 0.6034077405929565\n",
      "cnt: 0 - valLoss: 0.6123011112213135 - trainLoss: 0.6033902764320374\n",
      "cnt: 0 - valLoss: 0.6122825741767883 - trainLoss: 0.6033728122711182\n",
      "cnt: 0 - valLoss: 0.6122639775276184 - trainLoss: 0.603355348110199\n",
      "cnt: 0 - valLoss: 0.6122453808784485 - trainLoss: 0.603337824344635\n",
      "cnt: 0 - valLoss: 0.6122267246246338 - trainLoss: 0.6033203601837158\n",
      "cnt: 0 - valLoss: 0.6122081279754639 - trainLoss: 0.6033028364181519\n",
      "cnt: 0 - valLoss: 0.612189531326294 - trainLoss: 0.6032853722572327\n",
      "cnt: 0 - valLoss: 0.6121708750724792 - trainLoss: 0.6032679080963135\n",
      "cnt: 0 - valLoss: 0.6121522784233093 - trainLoss: 0.6032504439353943\n",
      "cnt: 0 - valLoss: 0.6121336817741394 - trainLoss: 0.6032329201698303\n",
      "cnt: 0 - valLoss: 0.6121150255203247 - trainLoss: 0.6032154560089111\n",
      "cnt: 0 - valLoss: 0.61209636926651 - trainLoss: 0.6031979322433472\n",
      "cnt: 0 - valLoss: 0.6120777130126953 - trainLoss: 0.6031804084777832\n",
      "cnt: 0 - valLoss: 0.6120591163635254 - trainLoss: 0.6031628251075745\n",
      "cnt: 0 - valLoss: 0.6120404601097107 - trainLoss: 0.6031453013420105\n",
      "cnt: 0 - valLoss: 0.6120217442512512 - trainLoss: 0.6031277775764465\n",
      "cnt: 0 - valLoss: 0.6120030879974365 - trainLoss: 0.6031101942062378\n",
      "cnt: 0 - valLoss: 0.6119844317436218 - trainLoss: 0.6030926704406738\n",
      "cnt: 0 - valLoss: 0.6119657158851624 - trainLoss: 0.6030751466751099\n",
      "cnt: 0 - valLoss: 0.6119470596313477 - trainLoss: 0.6030575633049011\n",
      "cnt: 0 - valLoss: 0.611928403377533 - trainLoss: 0.6030400395393372\n",
      "cnt: 0 - valLoss: 0.6119096875190735 - trainLoss: 0.6030224561691284\n",
      "cnt: 0 - valLoss: 0.611890971660614 - trainLoss: 0.6030049324035645\n",
      "cnt: 0 - valLoss: 0.6118723154067993 - trainLoss: 0.6029873490333557\n",
      "cnt: 0 - valLoss: 0.6118535995483398 - trainLoss: 0.6029698252677917\n",
      "cnt: 0 - valLoss: 0.6118349432945251 - trainLoss: 0.602952241897583\n",
      "cnt: 0 - valLoss: 0.6118162274360657 - trainLoss: 0.6029346585273743\n",
      "cnt: 0 - valLoss: 0.611797571182251 - trainLoss: 0.6029171347618103\n",
      "cnt: 0 - valLoss: 0.6117787957191467 - trainLoss: 0.6028995513916016\n",
      "cnt: 0 - valLoss: 0.6117600798606873 - trainLoss: 0.6028819680213928\n",
      "cnt: 0 - valLoss: 0.6117413640022278 - trainLoss: 0.6028644442558289\n",
      "cnt: 0 - valLoss: 0.6117226481437683 - trainLoss: 0.6028468608856201\n",
      "cnt: 0 - valLoss: 0.6117039322853088 - trainLoss: 0.6028292775154114\n",
      "cnt: 0 - valLoss: 0.6116851568222046 - trainLoss: 0.6028116941452026\n",
      "cnt: 0 - valLoss: 0.6116665005683899 - trainLoss: 0.6027941703796387\n",
      "cnt: 0 - valLoss: 0.6116477251052856 - trainLoss: 0.6027765274047852\n",
      "cnt: 0 - valLoss: 0.6116290092468262 - trainLoss: 0.6027590036392212\n",
      "cnt: 0 - valLoss: 0.6116102337837219 - trainLoss: 0.6027413010597229\n",
      "cnt: 0 - valLoss: 0.6115913987159729 - trainLoss: 0.6027236580848694\n",
      "cnt: 0 - valLoss: 0.6115726828575134 - trainLoss: 0.6027060151100159\n",
      "cnt: 0 - valLoss: 0.6115538477897644 - trainLoss: 0.602688193321228\n",
      "cnt: 0 - valLoss: 0.6115350127220154 - trainLoss: 0.6026703715324402\n",
      "cnt: 0 - valLoss: 0.6115161776542664 - trainLoss: 0.6026525497436523\n",
      "cnt: 0 - valLoss: 0.6114973425865173 - trainLoss: 0.6026346683502197\n",
      "cnt: 0 - valLoss: 0.6114785671234131 - trainLoss: 0.6026169657707214\n",
      "cnt: 0 - valLoss: 0.6114597320556641 - trainLoss: 0.6025991439819336\n",
      "cnt: 0 - valLoss: 0.6114408373832703 - trainLoss: 0.6025813221931458\n",
      "cnt: 0 - valLoss: 0.611422061920166 - trainLoss: 0.6025636196136475\n",
      "cnt: 0 - valLoss: 0.6114031672477722 - trainLoss: 0.6025457978248596\n",
      "cnt: 0 - valLoss: 0.6113842725753784 - trainLoss: 0.6025280356407166\n",
      "cnt: 0 - valLoss: 0.6113654375076294 - trainLoss: 0.6025102734565735\n",
      "cnt: 0 - valLoss: 0.6113464832305908 - trainLoss: 0.6024923920631409\n",
      "cnt: 0 - valLoss: 0.6113275289535522 - trainLoss: 0.602474570274353\n",
      "cnt: 0 - valLoss: 0.6113088130950928 - trainLoss: 0.6024566888809204\n",
      "cnt: 0 - valLoss: 0.611289918422699 - trainLoss: 0.6024389266967773\n",
      "cnt: 0 - valLoss: 0.6112711429595947 - trainLoss: 0.6024210453033447\n",
      "cnt: 0 - valLoss: 0.6112523674964905 - trainLoss: 0.6024032235145569\n",
      "cnt: 0 - valLoss: 0.611233651638031 - trainLoss: 0.6023853421211243\n",
      "cnt: 0 - valLoss: 0.6112149953842163 - trainLoss: 0.6023674011230469\n",
      "cnt: 0 - valLoss: 0.6111962795257568 - trainLoss: 0.6023496389389038\n",
      "cnt: 0 - valLoss: 0.6111775040626526 - trainLoss: 0.6023317575454712\n",
      "cnt: 0 - valLoss: 0.6111588478088379 - trainLoss: 0.6023139357566833\n",
      "cnt: 0 - valLoss: 0.6111401319503784 - trainLoss: 0.6022961139678955\n",
      "cnt: 0 - valLoss: 0.611121416091919 - trainLoss: 0.6022782921791077\n",
      "cnt: 0 - valLoss: 0.6111027002334595 - trainLoss: 0.6022604703903198\n",
      "cnt: 0 - valLoss: 0.6110839247703552 - trainLoss: 0.602242648601532\n",
      "cnt: 0 - valLoss: 0.611065149307251 - trainLoss: 0.6022248268127441\n",
      "cnt: 0 - valLoss: 0.6110464334487915 - trainLoss: 0.6022070050239563\n",
      "cnt: 0 - valLoss: 0.6110278367996216 - trainLoss: 0.6021891832351685\n",
      "cnt: 0 - valLoss: 0.6110091209411621 - trainLoss: 0.6021713614463806\n",
      "cnt: 0 - valLoss: 0.6109905242919922 - trainLoss: 0.602153480052948\n",
      "cnt: 0 - valLoss: 0.6109718680381775 - trainLoss: 0.6021357178688049\n",
      "cnt: 0 - valLoss: 0.6109532117843628 - trainLoss: 0.6021178960800171\n",
      "cnt: 0 - valLoss: 0.6109344959259033 - trainLoss: 0.6021000742912292\n",
      "cnt: 0 - valLoss: 0.6109157800674438 - trainLoss: 0.6020823121070862\n",
      "cnt: 0 - valLoss: 0.6108971834182739 - trainLoss: 0.6020643711090088\n",
      "cnt: 0 - valLoss: 0.6108785271644592 - trainLoss: 0.6020464897155762\n",
      "cnt: 0 - valLoss: 0.6108599901199341 - trainLoss: 0.6020287871360779\n",
      "cnt: 0 - valLoss: 0.6108413934707642 - trainLoss: 0.6020110845565796\n",
      "cnt: 0 - valLoss: 0.610822856426239 - trainLoss: 0.6019933223724365\n",
      "cnt: 0 - valLoss: 0.6108042597770691 - trainLoss: 0.6019756197929382\n",
      "cnt: 0 - valLoss: 0.6107856631278992 - trainLoss: 0.6019579172134399\n",
      "cnt: 0 - valLoss: 0.610767126083374 - trainLoss: 0.6019401550292969\n",
      "cnt: 0 - valLoss: 0.6107485294342041 - trainLoss: 0.6019223928451538\n",
      "cnt: 0 - valLoss: 0.6107299327850342 - trainLoss: 0.6019046902656555\n",
      "cnt: 0 - valLoss: 0.6107112765312195 - trainLoss: 0.6018869876861572\n",
      "cnt: 0 - valLoss: 0.6106927394866943 - trainLoss: 0.6018692255020142\n",
      "cnt: 0 - valLoss: 0.6106741428375244 - trainLoss: 0.6018515825271606\n",
      "cnt: 0 - valLoss: 0.6106555461883545 - trainLoss: 0.6018338799476624\n",
      "cnt: 0 - valLoss: 0.6106369495391846 - trainLoss: 0.6018161773681641\n",
      "cnt: 0 - valLoss: 0.6106183528900146 - trainLoss: 0.6017985343933105\n",
      "cnt: 0 - valLoss: 0.6105997562408447 - trainLoss: 0.6017808318138123\n",
      "cnt: 0 - valLoss: 0.61058109998703 - trainLoss: 0.601763129234314\n",
      "cnt: 0 - valLoss: 0.6105624437332153 - trainLoss: 0.6017454266548157\n",
      "cnt: 0 - valLoss: 0.6105438470840454 - trainLoss: 0.6017277240753174\n",
      "cnt: 0 - valLoss: 0.6105251908302307 - trainLoss: 0.6017100214958191\n",
      "cnt: 0 - valLoss: 0.610506534576416 - trainLoss: 0.601692259311676\n",
      "cnt: 0 - valLoss: 0.6104879379272461 - trainLoss: 0.6016746163368225\n",
      "cnt: 0 - valLoss: 0.6104692816734314 - trainLoss: 0.6016568541526794\n",
      "cnt: 0 - valLoss: 0.6104507446289062 - trainLoss: 0.6016392111778259\n",
      "cnt: 0 - valLoss: 0.6104320883750916 - trainLoss: 0.6016214489936829\n",
      "cnt: 0 - valLoss: 0.6104134321212769 - trainLoss: 0.6016037464141846\n",
      "cnt: 0 - valLoss: 0.6103948354721069 - trainLoss: 0.6015860438346863\n",
      "cnt: 0 - valLoss: 0.610376238822937 - trainLoss: 0.6015682816505432\n",
      "cnt: 0 - valLoss: 0.6103575825691223 - trainLoss: 0.6015505790710449\n",
      "cnt: 0 - valLoss: 0.6103389859199524 - trainLoss: 0.6015328168869019\n",
      "cnt: 0 - valLoss: 0.6103203296661377 - trainLoss: 0.6015150547027588\n",
      "cnt: 0 - valLoss: 0.610301673412323 - trainLoss: 0.6014973521232605\n",
      "cnt: 0 - valLoss: 0.6102830767631531 - trainLoss: 0.6014796495437622\n",
      "cnt: 0 - valLoss: 0.6102644205093384 - trainLoss: 0.6014618873596191\n",
      "cnt: 0 - valLoss: 0.6102457642555237 - trainLoss: 0.6014441251754761\n",
      "cnt: 0 - valLoss: 0.610227108001709 - trainLoss: 0.601426362991333\n",
      "cnt: 0 - valLoss: 0.6102083921432495 - trainLoss: 0.6014086008071899\n",
      "cnt: 0 - valLoss: 0.6101897954940796 - trainLoss: 0.6013908386230469\n",
      "cnt: 0 - valLoss: 0.6101711392402649 - trainLoss: 0.6013730764389038\n",
      "cnt: 0 - valLoss: 0.6101524233818054 - trainLoss: 0.6013553738594055\n",
      "cnt: 0 - valLoss: 0.6101337671279907 - trainLoss: 0.6013375520706177\n",
      "cnt: 0 - valLoss: 0.6101150512695312 - trainLoss: 0.6013197898864746\n",
      "cnt: 0 - valLoss: 0.6100963354110718 - trainLoss: 0.6013020277023315\n",
      "cnt: 0 - valLoss: 0.6100776791572571 - trainLoss: 0.6012842655181885\n",
      "cnt: 0 - valLoss: 0.6100589632987976 - trainLoss: 0.6012665033340454\n",
      "cnt: 0 - valLoss: 0.6100403070449829 - trainLoss: 0.6012486815452576\n",
      "cnt: 0 - valLoss: 0.6100215911865234 - trainLoss: 0.6012309789657593\n",
      "cnt: 0 - valLoss: 0.610002875328064 - trainLoss: 0.6012131571769714\n",
      "cnt: 0 - valLoss: 0.6099841594696045 - trainLoss: 0.6011953353881836\n",
      "cnt: 0 - valLoss: 0.609965443611145 - trainLoss: 0.6011775135993958\n",
      "cnt: 0 - valLoss: 0.6099467277526855 - trainLoss: 0.6011597514152527\n",
      "cnt: 0 - valLoss: 0.6099280118942261 - trainLoss: 0.6011419892311096\n",
      "cnt: 0 - valLoss: 0.6099092960357666 - trainLoss: 0.601124107837677\n",
      "cnt: 0 - valLoss: 0.6098905801773071 - trainLoss: 0.6011064052581787\n",
      "cnt: 0 - valLoss: 0.6098718643188477 - trainLoss: 0.6010885834693909\n",
      "cnt: 0 - valLoss: 0.6098530888557434 - trainLoss: 0.6010708212852478\n",
      "cnt: 0 - valLoss: 0.6098343133926392 - trainLoss: 0.6010528802871704\n",
      "cnt: 0 - valLoss: 0.6098155975341797 - trainLoss: 0.6010351181030273\n",
      "cnt: 0 - valLoss: 0.609796941280365 - trainLoss: 0.6010172963142395\n",
      "cnt: 0 - valLoss: 0.6097782254219055 - trainLoss: 0.6009994745254517\n",
      "cnt: 0 - valLoss: 0.609759509563446 - trainLoss: 0.6009816527366638\n",
      "cnt: 0 - valLoss: 0.6097407937049866 - trainLoss: 0.600963830947876\n",
      "cnt: 0 - valLoss: 0.6097220778465271 - trainLoss: 0.6009460091590881\n",
      "cnt: 0 - valLoss: 0.6097033023834229 - trainLoss: 0.6009280681610107\n",
      "cnt: 0 - valLoss: 0.6096845865249634 - trainLoss: 0.6009103059768677\n",
      "cnt: 0 - valLoss: 0.6096658110618591 - trainLoss: 0.6008924245834351\n",
      "cnt: 0 - valLoss: 0.6096470355987549 - trainLoss: 0.6008744835853577\n",
      "cnt: 0 - valLoss: 0.6096283197402954 - trainLoss: 0.6008567214012146\n",
      "cnt: 0 - valLoss: 0.6096096038818359 - trainLoss: 0.6008387804031372\n",
      "cnt: 0 - valLoss: 0.6095907688140869 - trainLoss: 0.6008209586143494\n",
      "cnt: 0 - valLoss: 0.6095720529556274 - trainLoss: 0.6008030772209167\n",
      "cnt: 0 - valLoss: 0.6095532774925232 - trainLoss: 0.6007851958274841\n",
      "cnt: 0 - valLoss: 0.6095344424247742 - trainLoss: 0.6007673144340515\n",
      "cnt: 0 - valLoss: 0.6095156669616699 - trainLoss: 0.6007494330406189\n",
      "cnt: 0 - valLoss: 0.6094968914985657 - trainLoss: 0.6007315516471863\n",
      "cnt: 0 - valLoss: 0.6094781160354614 - trainLoss: 0.6007136702537537\n",
      "cnt: 0 - valLoss: 0.6094593405723572 - trainLoss: 0.600695788860321\n",
      "cnt: 0 - valLoss: 0.6094405651092529 - trainLoss: 0.6006778478622437\n",
      "cnt: 0 - valLoss: 0.6094217896461487 - trainLoss: 0.600659966468811\n",
      "cnt: 0 - valLoss: 0.6094030141830444 - trainLoss: 0.6006420850753784\n",
      "cnt: 0 - valLoss: 0.6093841791152954 - trainLoss: 0.6006242036819458\n",
      "cnt: 0 - valLoss: 0.6093654036521912 - trainLoss: 0.6006063222885132\n",
      "cnt: 0 - valLoss: 0.6093465089797974 - trainLoss: 0.6005883812904358\n",
      "cnt: 0 - valLoss: 0.6093277335166931 - trainLoss: 0.6005704402923584\n",
      "cnt: 0 - valLoss: 0.6093088984489441 - trainLoss: 0.6005525588989258\n",
      "cnt: 0 - valLoss: 0.6092900633811951 - trainLoss: 0.6005346179008484\n",
      "cnt: 0 - valLoss: 0.6092711687088013 - trainLoss: 0.6005167365074158\n",
      "cnt: 0 - valLoss: 0.609252393245697 - trainLoss: 0.6004987955093384\n",
      "cnt: 0 - valLoss: 0.609233558177948 - trainLoss: 0.6004807949066162\n",
      "cnt: 0 - valLoss: 0.609214723110199 - trainLoss: 0.6004629135131836\n",
      "cnt: 0 - valLoss: 0.60919588804245 - trainLoss: 0.6004449725151062\n",
      "cnt: 0 - valLoss: 0.6091770529747009 - trainLoss: 0.6004270911216736\n",
      "cnt: 0 - valLoss: 0.6091580986976624 - trainLoss: 0.6004090905189514\n",
      "cnt: 0 - valLoss: 0.6091392636299133 - trainLoss: 0.6003912091255188\n",
      "cnt: 0 - valLoss: 0.6091204285621643 - trainLoss: 0.6003732085227966\n",
      "cnt: 0 - valLoss: 0.6091015338897705 - trainLoss: 0.6003552079200745\n",
      "cnt: 0 - valLoss: 0.6090826988220215 - trainLoss: 0.6003373265266418\n",
      "cnt: 0 - valLoss: 0.6090638041496277 - trainLoss: 0.6003193855285645\n",
      "cnt: 0 - valLoss: 0.6090449690818787 - trainLoss: 0.6003013849258423\n",
      "cnt: 0 - valLoss: 0.6090260744094849 - trainLoss: 0.6002835035324097\n",
      "cnt: 0 - valLoss: 0.6090071797370911 - trainLoss: 0.6002655029296875\n",
      "cnt: 0 - valLoss: 0.6089882850646973 - trainLoss: 0.6002475619316101\n",
      "cnt: 0 - valLoss: 0.6089693307876587 - trainLoss: 0.6002295613288879\n",
      "cnt: 0 - valLoss: 0.6089504361152649 - trainLoss: 0.6002116203308105\n",
      "cnt: 0 - valLoss: 0.6089316010475159 - trainLoss: 0.6001936197280884\n",
      "cnt: 0 - valLoss: 0.6089127063751221 - trainLoss: 0.6001756191253662\n",
      "cnt: 0 - valLoss: 0.6088937520980835 - trainLoss: 0.6001576781272888\n",
      "cnt: 0 - valLoss: 0.6088747978210449 - trainLoss: 0.6001396775245667\n",
      "cnt: 0 - valLoss: 0.6088559031486511 - trainLoss: 0.6001217365264893\n",
      "cnt: 0 - valLoss: 0.6088370084762573 - trainLoss: 0.6001037359237671\n",
      "cnt: 0 - valLoss: 0.6088180541992188 - trainLoss: 0.6000857353210449\n",
      "cnt: 0 - valLoss: 0.608799159526825 - trainLoss: 0.6000677943229675\n",
      "cnt: 0 - valLoss: 0.6087802052497864 - trainLoss: 0.6000497341156006\n",
      "cnt: 0 - valLoss: 0.6087613105773926 - trainLoss: 0.6000317335128784\n",
      "cnt: 0 - valLoss: 0.608742356300354 - trainLoss: 0.600013792514801\n",
      "cnt: 0 - valLoss: 0.6087234020233154 - trainLoss: 0.5999957919120789\n",
      "cnt: 0 - valLoss: 0.6087045073509216 - trainLoss: 0.5999777913093567\n",
      "cnt: 0 - valLoss: 0.6086855530738831 - trainLoss: 0.5999598503112793\n",
      "cnt: 0 - valLoss: 0.6086665987968445 - trainLoss: 0.5999417901039124\n",
      "cnt: 0 - valLoss: 0.6086477041244507 - trainLoss: 0.5999237895011902\n",
      "cnt: 0 - valLoss: 0.6086287498474121 - trainLoss: 0.5999058485031128\n",
      "cnt: 0 - valLoss: 0.6086097955703735 - trainLoss: 0.5998877286911011\n",
      "cnt: 0 - valLoss: 0.6085907816886902 - trainLoss: 0.5998697876930237\n",
      "cnt: 0 - valLoss: 0.6085718870162964 - trainLoss: 0.5998517870903015\n",
      "cnt: 0 - valLoss: 0.6085529327392578 - trainLoss: 0.5998337268829346\n",
      "cnt: 0 - valLoss: 0.6085339784622192 - trainLoss: 0.5998157262802124\n",
      "cnt: 0 - valLoss: 0.6085149645805359 - trainLoss: 0.5997977256774902\n",
      "cnt: 0 - valLoss: 0.6084960103034973 - trainLoss: 0.5997796654701233\n",
      "cnt: 0 - valLoss: 0.608476996421814 - trainLoss: 0.5997616052627563\n",
      "cnt: 0 - valLoss: 0.6084580421447754 - trainLoss: 0.5997436046600342\n",
      "cnt: 0 - valLoss: 0.608439028263092 - trainLoss: 0.599725604057312\n",
      "cnt: 0 - valLoss: 0.6084200143814087 - trainLoss: 0.5997075438499451\n",
      "cnt: 0 - valLoss: 0.6084011197090149 - trainLoss: 0.5996894836425781\n",
      "cnt: 0 - valLoss: 0.6083821058273315 - trainLoss: 0.5996714234352112\n",
      "cnt: 0 - valLoss: 0.6083630919456482 - trainLoss: 0.5996533632278442\n",
      "cnt: 0 - valLoss: 0.6083440184593201 - trainLoss: 0.5996353626251221\n",
      "cnt: 0 - valLoss: 0.6083250641822815 - trainLoss: 0.5996173024177551\n",
      "cnt: 0 - valLoss: 0.6083060503005981 - trainLoss: 0.5995992422103882\n",
      "cnt: 0 - valLoss: 0.60828697681427 - trainLoss: 0.5995811820030212\n",
      "cnt: 0 - valLoss: 0.6082679629325867 - trainLoss: 0.5995631217956543\n",
      "cnt: 0 - valLoss: 0.6082489490509033 - trainLoss: 0.5995450615882874\n",
      "cnt: 0 - valLoss: 0.6082298755645752 - trainLoss: 0.5995270013809204\n",
      "cnt: 0 - valLoss: 0.6082109212875366 - trainLoss: 0.5995088815689087\n",
      "cnt: 0 - valLoss: 0.6081918478012085 - trainLoss: 0.5994908809661865\n",
      "cnt: 0 - valLoss: 0.6081727743148804 - trainLoss: 0.5994727611541748\n",
      "cnt: 0 - valLoss: 0.608153760433197 - trainLoss: 0.5994547009468079\n",
      "cnt: 0 - valLoss: 0.6081346869468689 - trainLoss: 0.5994365215301514\n",
      "cnt: 0 - valLoss: 0.6081157326698303 - trainLoss: 0.5994184613227844\n",
      "cnt: 0 - valLoss: 0.608096718788147 - trainLoss: 0.5994004011154175\n",
      "cnt: 0 - valLoss: 0.6080777049064636 - trainLoss: 0.5993823409080505\n",
      "cnt: 0 - valLoss: 0.6080586910247803 - trainLoss: 0.5993642210960388\n",
      "cnt: 0 - valLoss: 0.6080396771430969 - trainLoss: 0.5993461608886719\n",
      "cnt: 0 - valLoss: 0.6080206632614136 - trainLoss: 0.5993279814720154\n",
      "cnt: 0 - valLoss: 0.6080016493797302 - trainLoss: 0.5993099212646484\n",
      "cnt: 0 - valLoss: 0.6079826354980469 - trainLoss: 0.5992918014526367\n",
      "cnt: 0 - valLoss: 0.6079636216163635 - trainLoss: 0.5992737412452698\n",
      "cnt: 0 - valLoss: 0.6079446077346802 - trainLoss: 0.5992555618286133\n",
      "cnt: 0 - valLoss: 0.6079255938529968 - trainLoss: 0.5992374420166016\n",
      "cnt: 0 - valLoss: 0.6079064607620239 - trainLoss: 0.5992193818092346\n",
      "cnt: 0 - valLoss: 0.6078875064849854 - trainLoss: 0.5992012619972229\n",
      "cnt: 0 - valLoss: 0.6078684329986572 - trainLoss: 0.5991831421852112\n",
      "cnt: 0 - valLoss: 0.6078493595123291 - trainLoss: 0.5991650223731995\n",
      "cnt: 0 - valLoss: 0.607830286026001 - trainLoss: 0.599146842956543\n",
      "cnt: 0 - valLoss: 0.6078112125396729 - trainLoss: 0.5991287231445312\n",
      "cnt: 0 - valLoss: 0.6077921986579895 - trainLoss: 0.5991106033325195\n",
      "cnt: 0 - valLoss: 0.6077731251716614 - trainLoss: 0.599092423915863\n",
      "cnt: 0 - valLoss: 0.6077540516853333 - trainLoss: 0.5990742444992065\n",
      "cnt: 0 - valLoss: 0.6077350378036499 - trainLoss: 0.5990561842918396\n",
      "cnt: 0 - valLoss: 0.6077158451080322 - trainLoss: 0.5990380048751831\n",
      "cnt: 0 - valLoss: 0.6076967120170593 - trainLoss: 0.5990198850631714\n",
      "cnt: 0 - valLoss: 0.6076775193214417 - trainLoss: 0.5990017652511597\n",
      "cnt: 0 - valLoss: 0.6076583862304688 - trainLoss: 0.5989835858345032\n",
      "cnt: 0 - valLoss: 0.6076392531394958 - trainLoss: 0.5989654660224915\n",
      "cnt: 0 - valLoss: 0.6076200604438782 - trainLoss: 0.5989473462104797\n",
      "cnt: 0 - valLoss: 0.6076009273529053 - trainLoss: 0.5989291667938232\n",
      "cnt: 0 - valLoss: 0.6075817346572876 - trainLoss: 0.5989110469818115\n",
      "cnt: 0 - valLoss: 0.6075625419616699 - trainLoss: 0.598892867565155\n",
      "cnt: 0 - valLoss: 0.607543408870697 - trainLoss: 0.5988747477531433\n",
      "cnt: 0 - valLoss: 0.6075242161750793 - trainLoss: 0.5988565683364868\n",
      "cnt: 0 - valLoss: 0.6075049638748169 - trainLoss: 0.5988383889198303\n",
      "cnt: 0 - valLoss: 0.6074857115745544 - trainLoss: 0.5988202691078186\n",
      "cnt: 0 - valLoss: 0.6074665188789368 - trainLoss: 0.5988020896911621\n",
      "cnt: 0 - valLoss: 0.6074472665786743 - trainLoss: 0.5987839102745056\n",
      "cnt: 0 - valLoss: 0.6074280738830566 - trainLoss: 0.5987657308578491\n",
      "cnt: 0 - valLoss: 0.6074087619781494 - trainLoss: 0.5987474918365479\n",
      "cnt: 0 - valLoss: 0.6073895692825317 - trainLoss: 0.5987293720245361\n",
      "cnt: 0 - valLoss: 0.6073703765869141 - trainLoss: 0.5987111330032349\n",
      "cnt: 0 - valLoss: 0.6073510646820068 - trainLoss: 0.5986930131912231\n",
      "cnt: 0 - valLoss: 0.6073318123817444 - trainLoss: 0.5986748337745667\n",
      "cnt: 0 - valLoss: 0.6073125600814819 - trainLoss: 0.5986567139625549\n",
      "cnt: 0 - valLoss: 0.6072933077812195 - trainLoss: 0.5986385345458984\n",
      "cnt: 0 - valLoss: 0.6072741150856018 - trainLoss: 0.5986203551292419\n",
      "cnt: 0 - valLoss: 0.6072548031806946 - trainLoss: 0.5986022353172302\n",
      "cnt: 0 - valLoss: 0.6072355508804321 - trainLoss: 0.5985840559005737\n",
      "cnt: 0 - valLoss: 0.6072162985801697 - trainLoss: 0.5985658764839172\n",
      "cnt: 0 - valLoss: 0.6071969270706177 - trainLoss: 0.598547637462616\n",
      "cnt: 0 - valLoss: 0.6071775555610657 - trainLoss: 0.5985293984413147\n",
      "cnt: 0 - valLoss: 0.6071581244468689 - trainLoss: 0.5985110998153687\n",
      "cnt: 0 - valLoss: 0.6071388125419617 - trainLoss: 0.5984928607940674\n",
      "cnt: 0 - valLoss: 0.6071193814277649 - trainLoss: 0.5984745025634766\n",
      "cnt: 0 - valLoss: 0.6071000695228577 - trainLoss: 0.5984562039375305\n",
      "cnt: 0 - valLoss: 0.6070806384086609 - trainLoss: 0.5984379053115845\n",
      "cnt: 0 - valLoss: 0.6070612072944641 - trainLoss: 0.5984195470809937\n",
      "cnt: 0 - valLoss: 0.6070417761802673 - trainLoss: 0.5984012484550476\n",
      "cnt: 0 - valLoss: 0.6070224046707153 - trainLoss: 0.5983829498291016\n",
      "cnt: 0 - valLoss: 0.6070030331611633 - trainLoss: 0.5983646512031555\n",
      "cnt: 0 - valLoss: 0.6069836020469666 - trainLoss: 0.5983462929725647\n",
      "cnt: 0 - valLoss: 0.6069641709327698 - trainLoss: 0.5983279943466187\n",
      "cnt: 0 - valLoss: 0.6069447994232178 - trainLoss: 0.5983096361160278\n",
      "cnt: 0 - valLoss: 0.6069254279136658 - trainLoss: 0.5982913374900818\n",
      "cnt: 0 - valLoss: 0.6069060564041138 - trainLoss: 0.5982730388641357\n",
      "cnt: 0 - valLoss: 0.6068867444992065 - trainLoss: 0.5982546806335449\n",
      "cnt: 0 - valLoss: 0.6068673133850098 - trainLoss: 0.5982364416122437\n",
      "cnt: 0 - valLoss: 0.6068480014801025 - trainLoss: 0.5982180833816528\n",
      "cnt: 0 - valLoss: 0.6068285703659058 - trainLoss: 0.598199725151062\n",
      "cnt: 0 - valLoss: 0.606809139251709 - trainLoss: 0.598181426525116\n",
      "cnt: 0 - valLoss: 0.606789767742157 - trainLoss: 0.5981631278991699\n",
      "cnt: 0 - valLoss: 0.606770396232605 - trainLoss: 0.5981447696685791\n",
      "cnt: 0 - valLoss: 0.606751024723053 - trainLoss: 0.5981264710426331\n",
      "cnt: 0 - valLoss: 0.606731653213501 - trainLoss: 0.5981081128120422\n",
      "cnt: 0 - valLoss: 0.6067122220993042 - trainLoss: 0.5980898141860962\n",
      "cnt: 0 - valLoss: 0.6066927909851074 - trainLoss: 0.5980713963508606\n",
      "cnt: 0 - valLoss: 0.6066733598709106 - trainLoss: 0.5980530381202698\n",
      "cnt: 0 - valLoss: 0.6066539883613586 - trainLoss: 0.5980347394943237\n",
      "cnt: 0 - valLoss: 0.6066344976425171 - trainLoss: 0.5980163216590881\n",
      "cnt: 0 - valLoss: 0.6066151261329651 - trainLoss: 0.5979980230331421\n",
      "cnt: 0 - valLoss: 0.6065957546234131 - trainLoss: 0.5979796648025513\n",
      "cnt: 0 - valLoss: 0.6065763235092163 - trainLoss: 0.5979612469673157\n",
      "cnt: 0 - valLoss: 0.6065569519996643 - trainLoss: 0.5979429483413696\n",
      "cnt: 0 - valLoss: 0.6065375804901123 - trainLoss: 0.597924530506134\n",
      "cnt: 0 - valLoss: 0.606518030166626 - trainLoss: 0.5979061722755432\n",
      "cnt: 0 - valLoss: 0.606498658657074 - trainLoss: 0.5978877544403076\n",
      "cnt: 0 - valLoss: 0.6064792275428772 - trainLoss: 0.5978693962097168\n",
      "cnt: 0 - valLoss: 0.6064597964286804 - trainLoss: 0.5978509783744812\n",
      "cnt: 0 - valLoss: 0.6064403653144836 - trainLoss: 0.5978326201438904\n",
      "cnt: 0 - valLoss: 0.6064209342002869 - trainLoss: 0.5978142023086548\n",
      "cnt: 0 - valLoss: 0.6064015030860901 - trainLoss: 0.597795844078064\n",
      "cnt: 0 - valLoss: 0.6063820719718933 - trainLoss: 0.5977774858474731\n",
      "cnt: 0 - valLoss: 0.6063626408576965 - trainLoss: 0.5977590680122375\n",
      "cnt: 0 - valLoss: 0.6063432097434998 - trainLoss: 0.5977405905723572\n",
      "cnt: 0 - valLoss: 0.6063239574432373 - trainLoss: 0.5977222323417664\n",
      "cnt: 0 - valLoss: 0.6063047051429749 - trainLoss: 0.5977038741111755\n",
      "cnt: 0 - valLoss: 0.6062854528427124 - trainLoss: 0.5976856350898743\n",
      "cnt: 0 - valLoss: 0.6062662601470947 - trainLoss: 0.5976672768592834\n",
      "cnt: 0 - valLoss: 0.6062470078468323 - trainLoss: 0.5976489782333374\n",
      "cnt: 0 - valLoss: 0.6062278151512146 - trainLoss: 0.5976306796073914\n",
      "cnt: 0 - valLoss: 0.6062085628509521 - trainLoss: 0.5976123809814453\n",
      "cnt: 0 - valLoss: 0.6061893701553345 - trainLoss: 0.5975940823554993\n",
      "cnt: 0 - valLoss: 0.606170117855072 - trainLoss: 0.5975757837295532\n",
      "cnt: 0 - valLoss: 0.6061508655548096 - trainLoss: 0.5975574851036072\n",
      "cnt: 0 - valLoss: 0.6061316728591919 - trainLoss: 0.5975391268730164\n",
      "cnt: 0 - valLoss: 0.6061124205589294 - trainLoss: 0.5975208282470703\n",
      "cnt: 0 - valLoss: 0.606093168258667 - trainLoss: 0.5975025296211243\n",
      "cnt: 0 - valLoss: 0.6060738563537598 - trainLoss: 0.5974841713905334\n",
      "cnt: 0 - valLoss: 0.6060546040534973 - trainLoss: 0.5974658131599426\n",
      "cnt: 0 - valLoss: 0.6060353517532349 - trainLoss: 0.5974475741386414\n",
      "cnt: 0 - valLoss: 0.6060160994529724 - trainLoss: 0.5974291563034058\n",
      "cnt: 0 - valLoss: 0.60599684715271 - trainLoss: 0.5974107980728149\n",
      "cnt: 0 - valLoss: 0.6059775352478027 - trainLoss: 0.5973924994468689\n",
      "cnt: 0 - valLoss: 0.6059582233428955 - trainLoss: 0.5973742008209229\n",
      "cnt: 0 - valLoss: 0.6059389710426331 - trainLoss: 0.597355842590332\n",
      "cnt: 0 - valLoss: 0.6059196591377258 - trainLoss: 0.5973374843597412\n",
      "cnt: 0 - valLoss: 0.6059003472328186 - trainLoss: 0.5973191261291504\n",
      "cnt: 0 - valLoss: 0.6058810949325562 - trainLoss: 0.5973007678985596\n",
      "cnt: 0 - valLoss: 0.6058617830276489 - trainLoss: 0.5972824096679688\n",
      "cnt: 0 - valLoss: 0.6058424711227417 - trainLoss: 0.5972641110420227\n",
      "cnt: 0 - valLoss: 0.6058232188224792 - trainLoss: 0.5972457528114319\n",
      "cnt: 0 - valLoss: 0.605803906917572 - trainLoss: 0.5972274541854858\n",
      "cnt: 0 - valLoss: 0.60578453540802 - trainLoss: 0.5972090363502502\n",
      "cnt: 0 - valLoss: 0.605765163898468 - trainLoss: 0.5971907377243042\n",
      "cnt: 0 - valLoss: 0.6057459115982056 - trainLoss: 0.5971723794937134\n",
      "cnt: 0 - valLoss: 0.6057265400886536 - trainLoss: 0.5971540212631226\n",
      "cnt: 0 - valLoss: 0.6057071685791016 - trainLoss: 0.5971357226371765\n",
      "cnt: 0 - valLoss: 0.6056877970695496 - trainLoss: 0.5971173048019409\n",
      "cnt: 0 - valLoss: 0.6056684255599976 - trainLoss: 0.5970989465713501\n",
      "cnt: 0 - valLoss: 0.6056491136550903 - trainLoss: 0.5970805883407593\n",
      "cnt: 0 - valLoss: 0.6056296825408936 - trainLoss: 0.5970621705055237\n",
      "cnt: 0 - valLoss: 0.6056103110313416 - trainLoss: 0.5970438718795776\n",
      "cnt: 0 - valLoss: 0.6055909395217896 - trainLoss: 0.597025454044342\n",
      "cnt: 0 - valLoss: 0.6055715680122375 - trainLoss: 0.5970070958137512\n",
      "cnt: 0 - valLoss: 0.6055521368980408 - trainLoss: 0.5969887375831604\n",
      "cnt: 0 - valLoss: 0.6055327653884888 - trainLoss: 0.5969703197479248\n",
      "cnt: 0 - valLoss: 0.6055133938789368 - trainLoss: 0.5969519019126892\n",
      "cnt: 0 - valLoss: 0.60549396276474 - trainLoss: 0.5969334840774536\n",
      "cnt: 0 - valLoss: 0.605474591255188 - trainLoss: 0.5969151258468628\n",
      "cnt: 0 - valLoss: 0.6054551005363464 - trainLoss: 0.5968967080116272\n",
      "cnt: 0 - valLoss: 0.6054357290267944 - trainLoss: 0.5968783497810364\n",
      "cnt: 0 - valLoss: 0.6054162383079529 - trainLoss: 0.5968599915504456\n",
      "cnt: 0 - valLoss: 0.6053968071937561 - trainLoss: 0.59684157371521\n",
      "cnt: 0 - valLoss: 0.6053772568702698 - trainLoss: 0.5968232154846191\n",
      "cnt: 0 - valLoss: 0.605357825756073 - trainLoss: 0.5968048572540283\n",
      "cnt: 0 - valLoss: 0.6053383350372314 - trainLoss: 0.5967864394187927\n",
      "cnt: 0 - valLoss: 0.6053188443183899 - trainLoss: 0.5967681407928467\n",
      "cnt: 0 - valLoss: 0.6052994132041931 - trainLoss: 0.5967497229576111\n",
      "cnt: 0 - valLoss: 0.6052799224853516 - trainLoss: 0.5967313051223755\n",
      "cnt: 0 - valLoss: 0.6052603721618652 - trainLoss: 0.5967128872871399\n",
      "cnt: 0 - valLoss: 0.6052408814430237 - trainLoss: 0.5966944694519043\n",
      "cnt: 0 - valLoss: 0.6052214503288269 - trainLoss: 0.5966761112213135\n",
      "cnt: 0 - valLoss: 0.6052019000053406 - trainLoss: 0.5966576933860779\n",
      "cnt: 0 - valLoss: 0.6051823496818542 - trainLoss: 0.5966393351554871\n",
      "cnt: 0 - valLoss: 0.6051628589630127 - trainLoss: 0.5966208577156067\n",
      "cnt: 0 - valLoss: 0.6051433682441711 - trainLoss: 0.5966024994850159\n",
      "cnt: 0 - valLoss: 0.6051238179206848 - trainLoss: 0.5965840816497803\n",
      "cnt: 0 - valLoss: 0.6051042675971985 - trainLoss: 0.5965656638145447\n",
      "cnt: 0 - valLoss: 0.6050847172737122 - trainLoss: 0.5965472459793091\n",
      "cnt: 0 - valLoss: 0.6050651669502258 - trainLoss: 0.5965287089347839\n",
      "cnt: 0 - valLoss: 0.6050456762313843 - trainLoss: 0.5965102314949036\n",
      "cnt: 0 - valLoss: 0.605026125907898 - trainLoss: 0.5964917540550232\n",
      "cnt: 0 - valLoss: 0.6050065159797668 - trainLoss: 0.5964732766151428\n",
      "cnt: 0 - valLoss: 0.6049869656562805 - trainLoss: 0.5964547991752625\n",
      "cnt: 0 - valLoss: 0.6049674153327942 - trainLoss: 0.5964363217353821\n",
      "cnt: 0 - valLoss: 0.6049478650093079 - trainLoss: 0.5964178442955017\n",
      "cnt: 0 - valLoss: 0.6049282550811768 - trainLoss: 0.5963994264602661\n",
      "cnt: 0 - valLoss: 0.6049087047576904 - trainLoss: 0.5963809490203857\n",
      "cnt: 0 - valLoss: 0.6048891544342041 - trainLoss: 0.5963624119758606\n",
      "cnt: 0 - valLoss: 0.604869544506073 - trainLoss: 0.596343994140625\n",
      "cnt: 0 - valLoss: 0.6048499941825867 - trainLoss: 0.5963255763053894\n",
      "cnt: 0 - valLoss: 0.6048303842544556 - trainLoss: 0.5963070392608643\n",
      "cnt: 0 - valLoss: 0.6048108339309692 - trainLoss: 0.5962885618209839\n",
      "cnt: 0 - valLoss: 0.6047912240028381 - trainLoss: 0.5962700843811035\n",
      "cnt: 0 - valLoss: 0.604771614074707 - trainLoss: 0.5962516069412231\n",
      "cnt: 0 - valLoss: 0.6047520041465759 - trainLoss: 0.596233069896698\n",
      "cnt: 0 - valLoss: 0.6047323942184448 - trainLoss: 0.5962145924568176\n",
      "cnt: 0 - valLoss: 0.6047127842903137 - trainLoss: 0.5961961150169373\n",
      "cnt: 0 - valLoss: 0.6046931743621826 - trainLoss: 0.5961776375770569\n",
      "cnt: 0 - valLoss: 0.6046735644340515 - trainLoss: 0.5961592197418213\n",
      "cnt: 0 - valLoss: 0.6046539545059204 - trainLoss: 0.5961407423019409\n",
      "cnt: 0 - valLoss: 0.6046343445777893 - trainLoss: 0.5961222648620605\n",
      "cnt: 0 - valLoss: 0.6046146750450134 - trainLoss: 0.5961037278175354\n",
      "cnt: 0 - valLoss: 0.6045950651168823 - trainLoss: 0.5960853695869446\n",
      "cnt: 0 - valLoss: 0.6045753955841064 - trainLoss: 0.5960668325424194\n",
      "cnt: 0 - valLoss: 0.6045557856559753 - trainLoss: 0.5960483551025391\n",
      "cnt: 0 - valLoss: 0.6045361757278442 - trainLoss: 0.5960298776626587\n",
      "cnt: 0 - valLoss: 0.6045165061950684 - trainLoss: 0.5960113406181335\n",
      "cnt: 0 - valLoss: 0.6044968366622925 - trainLoss: 0.5959928631782532\n",
      "cnt: 0 - valLoss: 0.6044772267341614 - trainLoss: 0.5959743857383728\n",
      "cnt: 0 - valLoss: 0.6044576168060303 - trainLoss: 0.5959559082984924\n",
      "cnt: 0 - valLoss: 0.6044378876686096 - trainLoss: 0.5959373116493225\n",
      "cnt: 0 - valLoss: 0.6044182777404785 - trainLoss: 0.5959188342094421\n",
      "cnt: 0 - valLoss: 0.6043985486030579 - trainLoss: 0.5959003567695618\n",
      "cnt: 0 - valLoss: 0.604378879070282 - trainLoss: 0.5958818197250366\n",
      "cnt: 0 - valLoss: 0.6043592095375061 - trainLoss: 0.5958633422851562\n",
      "cnt: 0 - valLoss: 0.6043395400047302 - trainLoss: 0.5958448648452759\n",
      "cnt: 0 - valLoss: 0.6043198108673096 - trainLoss: 0.5958263278007507\n",
      "cnt: 0 - valLoss: 0.6043001413345337 - trainLoss: 0.5958077907562256\n",
      "cnt: 0 - valLoss: 0.6042804718017578 - trainLoss: 0.5957892537117004\n",
      "cnt: 0 - valLoss: 0.6042608022689819 - trainLoss: 0.5957707166671753\n",
      "cnt: 0 - valLoss: 0.6042410135269165 - trainLoss: 0.5957522392272949\n",
      "cnt: 0 - valLoss: 0.6042212843894958 - trainLoss: 0.5957337021827698\n",
      "cnt: 0 - valLoss: 0.6042014956474304 - trainLoss: 0.5957151055335999\n",
      "cnt: 0 - valLoss: 0.6041817665100098 - trainLoss: 0.5956966280937195\n",
      "cnt: 0 - valLoss: 0.6041619777679443 - trainLoss: 0.5956780314445496\n",
      "cnt: 0 - valLoss: 0.6041422486305237 - trainLoss: 0.5956594944000244\n",
      "cnt: 0 - valLoss: 0.6041224002838135 - trainLoss: 0.5956409573554993\n",
      "cnt: 0 - valLoss: 0.6041026711463928 - trainLoss: 0.5956224203109741\n",
      "cnt: 0 - valLoss: 0.6040828824043274 - trainLoss: 0.595603883266449\n",
      "cnt: 0 - valLoss: 0.604063093662262 - trainLoss: 0.5955853462219238\n",
      "cnt: 0 - valLoss: 0.6040433049201965 - trainLoss: 0.5955667495727539\n",
      "cnt: 0 - valLoss: 0.6040234565734863 - trainLoss: 0.5955482125282288\n",
      "cnt: 0 - valLoss: 0.6040036678314209 - trainLoss: 0.5955296158790588\n",
      "cnt: 0 - valLoss: 0.6039839386940002 - trainLoss: 0.5955110192298889\n",
      "cnt: 0 - valLoss: 0.60396409034729 - trainLoss: 0.5954924821853638\n",
      "cnt: 0 - valLoss: 0.6039443016052246 - trainLoss: 0.5954738855361938\n",
      "cnt: 0 - valLoss: 0.6039245128631592 - trainLoss: 0.5954553484916687\n",
      "cnt: 0 - valLoss: 0.603904664516449 - trainLoss: 0.5954367518424988\n",
      "cnt: 0 - valLoss: 0.6038848161697388 - trainLoss: 0.5954181551933289\n",
      "cnt: 0 - valLoss: 0.6038649678230286 - trainLoss: 0.5953996181488037\n",
      "cnt: 0 - valLoss: 0.6038451194763184 - trainLoss: 0.595380961894989\n",
      "cnt: 0 - valLoss: 0.6038253307342529 - trainLoss: 0.5953624248504639\n",
      "cnt: 0 - valLoss: 0.6038054823875427 - trainLoss: 0.595343828201294\n",
      "cnt: 0 - valLoss: 0.6037856340408325 - trainLoss: 0.5953251719474792\n",
      "cnt: 0 - valLoss: 0.6037657856941223 - trainLoss: 0.5953066349029541\n",
      "cnt: 0 - valLoss: 0.6037459373474121 - trainLoss: 0.5952880382537842\n",
      "cnt: 0 - valLoss: 0.6037260890007019 - trainLoss: 0.5952694416046143\n",
      "cnt: 0 - valLoss: 0.6037062406539917 - trainLoss: 0.5952509045600891\n",
      "cnt: 0 - valLoss: 0.6036863923072815 - trainLoss: 0.5952323079109192\n",
      "cnt: 0 - valLoss: 0.6036665439605713 - trainLoss: 0.5952137112617493\n",
      "cnt: 0 - valLoss: 0.6036466956138611 - trainLoss: 0.5951951146125793\n",
      "cnt: 0 - valLoss: 0.6036267876625061 - trainLoss: 0.5951765179634094\n",
      "cnt: 0 - valLoss: 0.6036069393157959 - trainLoss: 0.5951579213142395\n",
      "cnt: 0 - valLoss: 0.6035870313644409 - trainLoss: 0.5951393842697144\n",
      "cnt: 0 - valLoss: 0.6035671234130859 - trainLoss: 0.5951207876205444\n",
      "cnt: 0 - valLoss: 0.6035472750663757 - trainLoss: 0.5951021909713745\n",
      "cnt: 0 - valLoss: 0.6035273671150208 - trainLoss: 0.5950836539268494\n",
      "cnt: 0 - valLoss: 0.6035074591636658 - trainLoss: 0.5950650572776794\n",
      "cnt: 0 - valLoss: 0.6034876108169556 - trainLoss: 0.5950465202331543\n",
      "cnt: 0 - valLoss: 0.6034676432609558 - trainLoss: 0.5950279831886292\n",
      "cnt: 0 - valLoss: 0.6034477353096008 - trainLoss: 0.5950093865394592\n",
      "cnt: 0 - valLoss: 0.6034278273582458 - trainLoss: 0.5949907898902893\n",
      "cnt: 0 - valLoss: 0.6034079790115356 - trainLoss: 0.5949721932411194\n",
      "cnt: 0 - valLoss: 0.6033880114555359 - trainLoss: 0.5949535965919495\n",
      "cnt: 0 - valLoss: 0.6033681035041809 - trainLoss: 0.5949351191520691\n",
      "cnt: 0 - valLoss: 0.6033481359481812 - trainLoss: 0.5949164032936096\n",
      "cnt: 0 - valLoss: 0.6033281683921814 - trainLoss: 0.5948979258537292\n",
      "cnt: 0 - valLoss: 0.6033082604408264 - trainLoss: 0.5948792695999146\n",
      "cnt: 0 - valLoss: 0.6032882928848267 - trainLoss: 0.5948607325553894\n",
      "cnt: 0 - valLoss: 0.6032683253288269 - trainLoss: 0.5948421359062195\n",
      "cnt: 0 - valLoss: 0.6032483577728271 - trainLoss: 0.5948234796524048\n",
      "cnt: 0 - valLoss: 0.6032284498214722 - trainLoss: 0.5948048233985901\n",
      "cnt: 0 - valLoss: 0.6032084822654724 - trainLoss: 0.5947862863540649\n",
      "cnt: 0 - valLoss: 0.6031885743141174 - trainLoss: 0.5947676301002502\n",
      "cnt: 0 - valLoss: 0.6031686067581177 - trainLoss: 0.5947490334510803\n",
      "cnt: 0 - valLoss: 0.6031486392021179 - trainLoss: 0.5947304368019104\n",
      "cnt: 0 - valLoss: 0.6031286716461182 - trainLoss: 0.5947118401527405\n",
      "cnt: 0 - valLoss: 0.6031087636947632 - trainLoss: 0.5946931838989258\n",
      "cnt: 0 - valLoss: 0.6030887961387634 - trainLoss: 0.5946745872497559\n",
      "cnt: 0 - valLoss: 0.6030688285827637 - trainLoss: 0.5946559309959412\n",
      "cnt: 0 - valLoss: 0.6030488610267639 - trainLoss: 0.5946373343467712\n",
      "cnt: 0 - valLoss: 0.6030288934707642 - trainLoss: 0.5946186780929565\n",
      "cnt: 0 - valLoss: 0.6030089259147644 - trainLoss: 0.5946000814437866\n",
      "cnt: 0 - valLoss: 0.6029889583587646 - trainLoss: 0.5945814847946167\n",
      "cnt: 0 - valLoss: 0.6029689908027649 - trainLoss: 0.594562828540802\n",
      "cnt: 0 - valLoss: 0.6029489636421204 - trainLoss: 0.5945441722869873\n",
      "cnt: 0 - valLoss: 0.6029289960861206 - trainLoss: 0.5945255160331726\n",
      "cnt: 0 - valLoss: 0.6029090285301208 - trainLoss: 0.5945069193840027\n",
      "cnt: 0 - valLoss: 0.6028890013694763 - trainLoss: 0.594488263130188\n",
      "cnt: 0 - valLoss: 0.6028690338134766 - trainLoss: 0.5944696068763733\n",
      "cnt: 0 - valLoss: 0.6028490662574768 - trainLoss: 0.5944508910179138\n",
      "cnt: 0 - valLoss: 0.6028290390968323 - trainLoss: 0.5944322347640991\n",
      "cnt: 0 - valLoss: 0.6028090119361877 - trainLoss: 0.5944135785102844\n",
      "cnt: 0 - valLoss: 0.6027889847755432 - trainLoss: 0.5943949222564697\n",
      "cnt: 0 - valLoss: 0.6027689576148987 - trainLoss: 0.594376266002655\n",
      "cnt: 0 - valLoss: 0.6027489304542542 - trainLoss: 0.5943576097488403\n",
      "cnt: 0 - valLoss: 0.6027289628982544 - trainLoss: 0.5943389534950256\n",
      "cnt: 0 - valLoss: 0.6027089953422546 - trainLoss: 0.5943202972412109\n",
      "cnt: 0 - valLoss: 0.6026889085769653 - trainLoss: 0.5943016409873962\n",
      "cnt: 0 - valLoss: 0.602668821811676 - trainLoss: 0.5942829251289368\n",
      "cnt: 0 - valLoss: 0.6026487946510315 - trainLoss: 0.5942642092704773\n",
      "cnt: 0 - valLoss: 0.602628767490387 - trainLoss: 0.5942454934120178\n",
      "cnt: 0 - valLoss: 0.6026086807250977 - trainLoss: 0.5942268371582031\n",
      "cnt: 0 - valLoss: 0.6025886535644531 - trainLoss: 0.5942081212997437\n",
      "cnt: 0 - valLoss: 0.6025686264038086 - trainLoss: 0.594189465045929\n",
      "cnt: 0 - valLoss: 0.6025485396385193 - trainLoss: 0.5941707491874695\n",
      "cnt: 0 - valLoss: 0.60252845287323 - trainLoss: 0.5941520929336548\n",
      "cnt: 0 - valLoss: 0.6025084257125854 - trainLoss: 0.5941333174705505\n",
      "cnt: 0 - valLoss: 0.6024883389472961 - trainLoss: 0.5941146016120911\n",
      "cnt: 0 - valLoss: 0.6024682521820068 - trainLoss: 0.5940959453582764\n",
      "cnt: 0 - valLoss: 0.6024482250213623 - trainLoss: 0.5940772891044617\n",
      "cnt: 0 - valLoss: 0.602428138256073 - trainLoss: 0.5940585136413574\n",
      "cnt: 0 - valLoss: 0.6024080514907837 - trainLoss: 0.594039797782898\n",
      "cnt: 0 - valLoss: 0.6023879051208496 - trainLoss: 0.5940210819244385\n",
      "cnt: 0 - valLoss: 0.6023678183555603 - trainLoss: 0.594002366065979\n",
      "cnt: 0 - valLoss: 0.602347731590271 - trainLoss: 0.5939835906028748\n",
      "cnt: 0 - valLoss: 0.6023276448249817 - trainLoss: 0.5939649343490601\n",
      "cnt: 0 - valLoss: 0.6023075580596924 - trainLoss: 0.5939462184906006\n",
      "cnt: 0 - valLoss: 0.6022874116897583 - trainLoss: 0.5939274430274963\n",
      "cnt: 0 - valLoss: 0.6022672653198242 - trainLoss: 0.5939087271690369\n",
      "cnt: 0 - valLoss: 0.6022471189498901 - trainLoss: 0.5938898921012878\n",
      "cnt: 0 - valLoss: 0.6022270321846008 - trainLoss: 0.5938711166381836\n",
      "cnt: 0 - valLoss: 0.6022069454193115 - trainLoss: 0.5938524007797241\n",
      "cnt: 0 - valLoss: 0.6021867990493774 - trainLoss: 0.5938336253166199\n",
      "cnt: 0 - valLoss: 0.6021666526794434 - trainLoss: 0.5938149094581604\n",
      "cnt: 0 - valLoss: 0.6021465063095093 - trainLoss: 0.5937961339950562\n",
      "cnt: 0 - valLoss: 0.6021263003349304 - trainLoss: 0.5937773585319519\n",
      "cnt: 0 - valLoss: 0.6021062731742859 - trainLoss: 0.5937585830688477\n",
      "cnt: 0 - valLoss: 0.6020860075950623 - trainLoss: 0.5937398076057434\n",
      "cnt: 0 - valLoss: 0.6020658612251282 - trainLoss: 0.5937209725379944\n",
      "cnt: 0 - valLoss: 0.6020457148551941 - trainLoss: 0.5937022566795349\n",
      "cnt: 0 - valLoss: 0.60202556848526 - trainLoss: 0.5936834216117859\n",
      "cnt: 0 - valLoss: 0.6020053625106812 - trainLoss: 0.5936646461486816\n",
      "cnt: 0 - valLoss: 0.6019852161407471 - trainLoss: 0.5936458706855774\n",
      "cnt: 0 - valLoss: 0.601965069770813 - trainLoss: 0.5936270952224731\n",
      "cnt: 0 - valLoss: 0.6019449234008789 - trainLoss: 0.5936082601547241\n",
      "cnt: 0 - valLoss: 0.6019246578216553 - trainLoss: 0.5935894846916199\n",
      "cnt: 0 - valLoss: 0.6019045114517212 - trainLoss: 0.5935706496238708\n",
      "cnt: 0 - valLoss: 0.6018843054771423 - trainLoss: 0.5935519337654114\n",
      "cnt: 0 - valLoss: 0.6018641591072083 - trainLoss: 0.5935330986976624\n",
      "cnt: 0 - valLoss: 0.6018439531326294 - trainLoss: 0.5935142636299133\n",
      "cnt: 0 - valLoss: 0.6018237471580505 - trainLoss: 0.5934954881668091\n",
      "cnt: 0 - valLoss: 0.6018035411834717 - trainLoss: 0.5934766530990601\n",
      "cnt: 0 - valLoss: 0.6017832159996033 - trainLoss: 0.593457818031311\n",
      "cnt: 0 - valLoss: 0.6017630696296692 - trainLoss: 0.593438982963562\n",
      "cnt: 0 - valLoss: 0.6017429232597351 - trainLoss: 0.593420147895813\n",
      "cnt: 0 - valLoss: 0.6017226576805115 - trainLoss: 0.593401312828064\n",
      "cnt: 0 - valLoss: 0.6017024517059326 - trainLoss: 0.5933824777603149\n",
      "cnt: 0 - valLoss: 0.6016821265220642 - trainLoss: 0.5933636426925659\n",
      "cnt: 0 - valLoss: 0.6016619205474854 - trainLoss: 0.5933448672294617\n",
      "cnt: 0 - valLoss: 0.6016417145729065 - trainLoss: 0.5933260321617126\n",
      "cnt: 0 - valLoss: 0.6016214489936829 - trainLoss: 0.5933071970939636\n",
      "cnt: 0 - valLoss: 0.6016011834144592 - trainLoss: 0.5932883024215698\n",
      "cnt: 0 - valLoss: 0.6015809178352356 - trainLoss: 0.5932695269584656\n",
      "cnt: 0 - valLoss: 0.6015607118606567 - trainLoss: 0.5932506322860718\n",
      "cnt: 0 - valLoss: 0.6015404462814331 - trainLoss: 0.593231737613678\n",
      "cnt: 0 - valLoss: 0.6015201807022095 - trainLoss: 0.593212902545929\n",
      "cnt: 0 - valLoss: 0.6014999151229858 - trainLoss: 0.5931940674781799\n",
      "cnt: 0 - valLoss: 0.601479709148407 - trainLoss: 0.5931751728057861\n",
      "cnt: 0 - valLoss: 0.6014593839645386 - trainLoss: 0.5931563377380371\n",
      "cnt: 0 - valLoss: 0.6014390587806702 - trainLoss: 0.5931374430656433\n",
      "cnt: 0 - valLoss: 0.6014188528060913 - trainLoss: 0.5931186079978943\n",
      "cnt: 0 - valLoss: 0.6013985276222229 - trainLoss: 0.5930997133255005\n",
      "cnt: 0 - valLoss: 0.6013782620429993 - trainLoss: 0.5930808186531067\n",
      "cnt: 0 - valLoss: 0.6013579368591309 - trainLoss: 0.5930619835853577\n",
      "cnt: 0 - valLoss: 0.6013376712799072 - trainLoss: 0.5930430293083191\n",
      "cnt: 0 - valLoss: 0.6013173460960388 - trainLoss: 0.5930241942405701\n",
      "cnt: 0 - valLoss: 0.6012970805168152 - trainLoss: 0.5930052995681763\n",
      "cnt: 0 - valLoss: 0.6012767553329468 - trainLoss: 0.5929864048957825\n",
      "cnt: 0 - valLoss: 0.6012564301490784 - trainLoss: 0.5929675102233887\n",
      "cnt: 0 - valLoss: 0.60123610496521 - trainLoss: 0.5929485559463501\n",
      "cnt: 0 - valLoss: 0.6012157797813416 - trainLoss: 0.5929297208786011\n",
      "cnt: 0 - valLoss: 0.6011954545974731 - trainLoss: 0.5929108262062073\n",
      "cnt: 0 - valLoss: 0.6011751294136047 - trainLoss: 0.5928919315338135\n",
      "cnt: 0 - valLoss: 0.6011548638343811 - trainLoss: 0.5928729772567749\n",
      "cnt: 0 - valLoss: 0.6011344194412231 - trainLoss: 0.5928540825843811\n",
      "cnt: 0 - valLoss: 0.6011140942573547 - trainLoss: 0.5928351283073425\n",
      "cnt: 0 - valLoss: 0.6010937690734863 - trainLoss: 0.5928162336349487\n",
      "cnt: 0 - valLoss: 0.6010734438896179 - trainLoss: 0.5927973389625549\n",
      "cnt: 0 - valLoss: 0.6010530591011047 - trainLoss: 0.5927783846855164\n",
      "cnt: 0 - valLoss: 0.6010327339172363 - trainLoss: 0.5927594900131226\n",
      "cnt: 0 - valLoss: 0.6010123491287231 - trainLoss: 0.5927405953407288\n",
      "cnt: 0 - valLoss: 0.6009920239448547 - trainLoss: 0.5927216410636902\n",
      "cnt: 0 - valLoss: 0.6009716391563416 - trainLoss: 0.5927026867866516\n",
      "cnt: 0 - valLoss: 0.6009513139724731 - trainLoss: 0.592683732509613\n",
      "cnt: 0 - valLoss: 0.6009308695793152 - trainLoss: 0.5926647782325745\n",
      "cnt: 0 - valLoss: 0.600910484790802 - trainLoss: 0.5926458835601807\n",
      "cnt: 0 - valLoss: 0.6008901596069336 - trainLoss: 0.5926269292831421\n",
      "cnt: 0 - valLoss: 0.6008697748184204 - trainLoss: 0.5926079750061035\n",
      "cnt: 0 - valLoss: 0.6008493900299072 - trainLoss: 0.5925890207290649\n",
      "cnt: 0 - valLoss: 0.600829005241394 - trainLoss: 0.5925700664520264\n",
      "cnt: 0 - valLoss: 0.6008085608482361 - trainLoss: 0.5925511121749878\n",
      "cnt: 0 - valLoss: 0.6007881760597229 - trainLoss: 0.5925320982933044\n",
      "cnt: 0 - valLoss: 0.6007677316665649 - trainLoss: 0.5925132036209106\n",
      "cnt: 0 - valLoss: 0.6007473468780518 - trainLoss: 0.5924942493438721\n",
      "cnt: 0 - valLoss: 0.6007269024848938 - trainLoss: 0.5924752950668335\n",
      "cnt: 0 - valLoss: 0.6007065773010254 - trainLoss: 0.5924562811851501\n",
      "cnt: 0 - valLoss: 0.6006861329078674 - trainLoss: 0.5924373865127563\n",
      "cnt: 0 - valLoss: 0.6006656885147095 - trainLoss: 0.592418372631073\n",
      "cnt: 0 - valLoss: 0.6006452441215515 - trainLoss: 0.5923993587493896\n",
      "cnt: 0 - valLoss: 0.6006247997283936 - trainLoss: 0.5923804044723511\n",
      "cnt: 0 - valLoss: 0.6006043553352356 - trainLoss: 0.5923613905906677\n",
      "cnt: 0 - valLoss: 0.6005839705467224 - trainLoss: 0.5923424363136292\n",
      "cnt: 0 - valLoss: 0.6005635261535645 - trainLoss: 0.5923234224319458\n",
      "cnt: 0 - valLoss: 0.6005430817604065 - trainLoss: 0.5923044085502625\n",
      "cnt: 0 - valLoss: 0.6005225777626038 - trainLoss: 0.5922853946685791\n",
      "cnt: 0 - valLoss: 0.600502073764801 - trainLoss: 0.5922663807868958\n",
      "cnt: 0 - valLoss: 0.6004816293716431 - trainLoss: 0.5922474265098572\n",
      "cnt: 0 - valLoss: 0.6004611253738403 - trainLoss: 0.5922284126281738\n",
      "cnt: 0 - valLoss: 0.6004406809806824 - trainLoss: 0.5922093987464905\n",
      "cnt: 0 - valLoss: 0.6004201769828796 - trainLoss: 0.5921903848648071\n",
      "cnt: 0 - valLoss: 0.6003996729850769 - trainLoss: 0.5921713709831238\n",
      "cnt: 0 - valLoss: 0.6003791689872742 - trainLoss: 0.5921523571014404\n",
      "cnt: 0 - valLoss: 0.6003586649894714 - trainLoss: 0.5921333432197571\n",
      "cnt: 0 - valLoss: 0.6003381609916687 - trainLoss: 0.592114269733429\n",
      "cnt: 0 - valLoss: 0.6003177165985107 - trainLoss: 0.5920953154563904\n",
      "cnt: 0 - valLoss: 0.600297212600708 - trainLoss: 0.592076301574707\n",
      "cnt: 0 - valLoss: 0.6002766489982605 - trainLoss: 0.5920572876930237\n",
      "cnt: 0 - valLoss: 0.6002561450004578 - trainLoss: 0.5920382142066956\n",
      "cnt: 0 - valLoss: 0.600235641002655 - trainLoss: 0.5920192003250122\n",
      "cnt: 0 - valLoss: 0.6002150774002075 - trainLoss: 0.5920001864433289\n",
      "cnt: 0 - valLoss: 0.6001946330070496 - trainLoss: 0.5919811725616455\n",
      "cnt: 0 - valLoss: 0.600174069404602 - trainLoss: 0.5919621586799622\n",
      "cnt: 0 - valLoss: 0.6001535058021545 - trainLoss: 0.591943085193634\n",
      "cnt: 0 - valLoss: 0.600132942199707 - trainLoss: 0.5919240713119507\n",
      "cnt: 0 - valLoss: 0.6001123785972595 - trainLoss: 0.5919050574302673\n",
      "cnt: 0 - valLoss: 0.6000918745994568 - trainLoss: 0.5918859839439392\n",
      "cnt: 0 - valLoss: 0.600071370601654 - trainLoss: 0.5918669700622559\n",
      "cnt: 0 - valLoss: 0.6000508666038513 - trainLoss: 0.5918478965759277\n",
      "cnt: 0 - valLoss: 0.6000303626060486 - trainLoss: 0.5918288826942444\n",
      "cnt: 0 - valLoss: 0.6000098586082458 - trainLoss: 0.5918098092079163\n",
      "cnt: 0 - valLoss: 0.5999892950057983 - trainLoss: 0.5917907953262329\n",
      "cnt: 0 - valLoss: 0.5999687910079956 - trainLoss: 0.5917717218399048\n",
      "cnt: 0 - valLoss: 0.5999482274055481 - trainLoss: 0.5917527675628662\n",
      "cnt: 0 - valLoss: 0.5999277234077454 - trainLoss: 0.5917336344718933\n",
      "cnt: 0 - valLoss: 0.5999071598052979 - trainLoss: 0.59171462059021\n",
      "cnt: 0 - valLoss: 0.5998865962028503 - trainLoss: 0.5916955471038818\n",
      "cnt: 0 - valLoss: 0.5998660326004028 - trainLoss: 0.5916764736175537\n",
      "cnt: 0 - valLoss: 0.5998455286026001 - trainLoss: 0.5916574001312256\n",
      "cnt: 0 - valLoss: 0.5998249650001526 - trainLoss: 0.5916383266448975\n",
      "cnt: 0 - valLoss: 0.5998043417930603 - trainLoss: 0.5916193127632141\n",
      "cnt: 0 - valLoss: 0.5997838377952576 - trainLoss: 0.5916001796722412\n",
      "cnt: 0 - valLoss: 0.5997632741928101 - trainLoss: 0.5915811657905579\n",
      "cnt: 0 - valLoss: 0.5997426509857178 - trainLoss: 0.5915620923042297\n",
      "cnt: 0 - valLoss: 0.5997220873832703 - trainLoss: 0.5915429592132568\n",
      "cnt: 0 - valLoss: 0.5997015237808228 - trainLoss: 0.5915239453315735\n",
      "cnt: 0 - valLoss: 0.5996809005737305 - trainLoss: 0.5915048718452454\n",
      "cnt: 0 - valLoss: 0.5996603965759277 - trainLoss: 0.5914857387542725\n",
      "cnt: 0 - valLoss: 0.5996397137641907 - trainLoss: 0.5914666652679443\n",
      "cnt: 0 - valLoss: 0.5996191501617432 - trainLoss: 0.5914475917816162\n",
      "cnt: 0 - valLoss: 0.5995985269546509 - trainLoss: 0.5914284586906433\n",
      "cnt: 0 - valLoss: 0.5995778441429138 - trainLoss: 0.5914093852043152\n",
      "cnt: 0 - valLoss: 0.5995572805404663 - trainLoss: 0.5913902521133423\n",
      "cnt: 0 - valLoss: 0.5995365977287292 - trainLoss: 0.5913711786270142\n",
      "cnt: 0 - valLoss: 0.5995159149169922 - trainLoss: 0.5913520455360413\n",
      "cnt: 0 - valLoss: 0.5994951725006104 - trainLoss: 0.5913329124450684\n",
      "cnt: 0 - valLoss: 0.5994745492935181 - trainLoss: 0.5913137793540955\n",
      "cnt: 0 - valLoss: 0.5994538068771362 - trainLoss: 0.5912946462631226\n",
      "cnt: 0 - valLoss: 0.5994331240653992 - trainLoss: 0.5912754535675049\n",
      "cnt: 0 - valLoss: 0.5994124412536621 - trainLoss: 0.5912563800811768\n",
      "cnt: 0 - valLoss: 0.5993916988372803 - trainLoss: 0.5912371873855591\n",
      "cnt: 0 - valLoss: 0.599371075630188 - trainLoss: 0.5912180542945862\n",
      "cnt: 0 - valLoss: 0.5993503332138062 - trainLoss: 0.5911989212036133\n",
      "cnt: 0 - valLoss: 0.5993296504020691 - trainLoss: 0.5911797285079956\n",
      "cnt: 0 - valLoss: 0.5993090271949768 - trainLoss: 0.5911605358123779\n",
      "cnt: 0 - valLoss: 0.5992883443832397 - trainLoss: 0.591141402721405\n",
      "cnt: 0 - valLoss: 0.5992676615715027 - trainLoss: 0.5911222100257874\n",
      "cnt: 0 - valLoss: 0.5992469191551208 - trainLoss: 0.5911030173301697\n",
      "cnt: 0 - valLoss: 0.599226176738739 - trainLoss: 0.591083824634552\n",
      "cnt: 0 - valLoss: 0.5992055535316467 - trainLoss: 0.5910646915435791\n",
      "cnt: 0 - valLoss: 0.5991848111152649 - trainLoss: 0.5910454988479614\n",
      "cnt: 0 - valLoss: 0.5991640686988831 - trainLoss: 0.5910263061523438\n",
      "cnt: 0 - valLoss: 0.5991433262825012 - trainLoss: 0.5910071134567261\n",
      "cnt: 0 - valLoss: 0.5991225838661194 - trainLoss: 0.5909879207611084\n",
      "cnt: 0 - valLoss: 0.5991018414497375 - trainLoss: 0.5909687280654907\n",
      "cnt: 0 - valLoss: 0.5990811586380005 - trainLoss: 0.5909494757652283\n",
      "cnt: 0 - valLoss: 0.5990604162216187 - trainLoss: 0.5909302830696106\n",
      "cnt: 0 - valLoss: 0.599039614200592 - trainLoss: 0.5909110307693481\n",
      "cnt: 0 - valLoss: 0.599018931388855 - trainLoss: 0.5908918976783752\n",
      "cnt: 0 - valLoss: 0.5989981889724731 - trainLoss: 0.5908727049827576\n",
      "cnt: 0 - valLoss: 0.5989773869514465 - trainLoss: 0.5908534526824951\n",
      "cnt: 0 - valLoss: 0.5989565849304199 - trainLoss: 0.5908342599868774\n",
      "cnt: 0 - valLoss: 0.5989359021186829 - trainLoss: 0.5908150672912598\n",
      "cnt: 0 - valLoss: 0.5989151000976562 - trainLoss: 0.5907958149909973\n",
      "cnt: 0 - valLoss: 0.5988942980766296 - trainLoss: 0.5907765626907349\n",
      "cnt: 0 - valLoss: 0.5988735556602478 - trainLoss: 0.5907573699951172\n",
      "cnt: 0 - valLoss: 0.5988527536392212 - trainLoss: 0.5907381176948547\n",
      "cnt: 0 - valLoss: 0.5988320112228394 - trainLoss: 0.5907188653945923\n",
      "cnt: 0 - valLoss: 0.5988112092018127 - trainLoss: 0.5906996726989746\n",
      "cnt: 0 - valLoss: 0.5987904667854309 - trainLoss: 0.5906804203987122\n",
      "cnt: 0 - valLoss: 0.5987696051597595 - trainLoss: 0.5906611680984497\n",
      "cnt: 0 - valLoss: 0.5987488627433777 - trainLoss: 0.5906419157981873\n",
      "cnt: 0 - valLoss: 0.5987280607223511 - trainLoss: 0.5906226634979248\n",
      "cnt: 0 - valLoss: 0.5987071990966797 - trainLoss: 0.5906034111976624\n",
      "cnt: 0 - valLoss: 0.5986863970756531 - trainLoss: 0.5905841588973999\n",
      "cnt: 0 - valLoss: 0.5986655950546265 - trainLoss: 0.5905649065971375\n",
      "cnt: 0 - valLoss: 0.5986447930335999 - trainLoss: 0.590545654296875\n",
      "cnt: 0 - valLoss: 0.598624050617218 - trainLoss: 0.5905264019966125\n",
      "cnt: 0 - valLoss: 0.5986031293869019 - trainLoss: 0.5905071496963501\n",
      "cnt: 0 - valLoss: 0.5985823273658752 - trainLoss: 0.5904878973960876\n",
      "cnt: 0 - valLoss: 0.5985615253448486 - trainLoss: 0.5904685854911804\n",
      "cnt: 0 - valLoss: 0.5985406637191772 - trainLoss: 0.590449333190918\n",
      "cnt: 0 - valLoss: 0.5985198020935059 - trainLoss: 0.5904300212860107\n",
      "cnt: 0 - valLoss: 0.5984989404678345 - trainLoss: 0.5904107689857483\n",
      "cnt: 0 - valLoss: 0.5984781384468079 - trainLoss: 0.5903914570808411\n",
      "cnt: 0 - valLoss: 0.5984572768211365 - trainLoss: 0.5903722047805786\n",
      "cnt: 0 - valLoss: 0.5984364748001099 - trainLoss: 0.5903528928756714\n",
      "cnt: 0 - valLoss: 0.5984156131744385 - trainLoss: 0.5903336405754089\n",
      "cnt: 0 - valLoss: 0.5983946919441223 - trainLoss: 0.5903143286705017\n",
      "cnt: 0 - valLoss: 0.5983738303184509 - trainLoss: 0.5902950763702393\n",
      "cnt: 0 - valLoss: 0.5983529090881348 - trainLoss: 0.590275764465332\n",
      "cnt: 0 - valLoss: 0.5983321666717529 - trainLoss: 0.5902564525604248\n",
      "cnt: 0 - valLoss: 0.5983112454414368 - trainLoss: 0.5902370810508728\n",
      "cnt: 0 - valLoss: 0.5982903242111206 - trainLoss: 0.5902178287506104\n",
      "cnt: 0 - valLoss: 0.5982694625854492 - trainLoss: 0.5901985168457031\n",
      "cnt: 0 - valLoss: 0.5982485413551331 - trainLoss: 0.5901791453361511\n",
      "cnt: 0 - valLoss: 0.5982277393341064 - trainLoss: 0.5901598930358887\n",
      "cnt: 0 - valLoss: 0.5982068777084351 - trainLoss: 0.5901405215263367\n",
      "cnt: 0 - valLoss: 0.5981859564781189 - trainLoss: 0.5901212096214294\n",
      "cnt: 0 - valLoss: 0.5981650352478027 - trainLoss: 0.590101957321167\n",
      "cnt: 0 - valLoss: 0.5981441736221313 - trainLoss: 0.590082585811615\n",
      "cnt: 0 - valLoss: 0.5981232523918152 - trainLoss: 0.590063214302063\n",
      "cnt: 0 - valLoss: 0.5981024503707886 - trainLoss: 0.5900439620018005\n",
      "cnt: 0 - valLoss: 0.5980815291404724 - trainLoss: 0.5900246500968933\n",
      "cnt: 0 - valLoss: 0.598060667514801 - trainLoss: 0.5900052785873413\n",
      "cnt: 0 - valLoss: 0.5980398058891296 - trainLoss: 0.5899859666824341\n",
      "cnt: 0 - valLoss: 0.5980188846588135 - trainLoss: 0.5899666547775269\n",
      "cnt: 0 - valLoss: 0.5979980230331421 - trainLoss: 0.5899472832679749\n",
      "cnt: 0 - valLoss: 0.5979771018028259 - trainLoss: 0.5899279713630676\n",
      "cnt: 0 - valLoss: 0.5979562401771545 - trainLoss: 0.5899086594581604\n",
      "cnt: 0 - valLoss: 0.5979352593421936 - trainLoss: 0.5898892879486084\n",
      "cnt: 0 - valLoss: 0.5979143977165222 - trainLoss: 0.5898699164390564\n",
      "cnt: 0 - valLoss: 0.597893476486206 - trainLoss: 0.589850664138794\n",
      "cnt: 0 - valLoss: 0.5978726148605347 - trainLoss: 0.5898312926292419\n",
      "cnt: 0 - valLoss: 0.5978516340255737 - trainLoss: 0.5898119211196899\n",
      "cnt: 0 - valLoss: 0.5978307723999023 - trainLoss: 0.5897925496101379\n",
      "cnt: 0 - valLoss: 0.5978098511695862 - trainLoss: 0.5897732377052307\n",
      "cnt: 0 - valLoss: 0.59778892993927 - trainLoss: 0.5897539258003235\n",
      "cnt: 0 - valLoss: 0.5977680087089539 - trainLoss: 0.5897346138954163\n",
      "cnt: 0 - valLoss: 0.5977470874786377 - trainLoss: 0.5897152423858643\n",
      "cnt: 0 - valLoss: 0.5977261066436768 - trainLoss: 0.5896958708763123\n",
      "cnt: 0 - valLoss: 0.5977051854133606 - trainLoss: 0.5896764397621155\n",
      "cnt: 0 - valLoss: 0.5976842641830444 - trainLoss: 0.5896571278572083\n",
      "cnt: 0 - valLoss: 0.5976632833480835 - trainLoss: 0.5896376967430115\n",
      "cnt: 0 - valLoss: 0.5976423621177673 - trainLoss: 0.5896183252334595\n",
      "cnt: 0 - valLoss: 0.5976213812828064 - trainLoss: 0.5895989537239075\n",
      "cnt: 0 - valLoss: 0.5976004600524902 - trainLoss: 0.5895795822143555\n",
      "cnt: 0 - valLoss: 0.5975794792175293 - trainLoss: 0.5895601511001587\n",
      "cnt: 0 - valLoss: 0.5975584983825684 - trainLoss: 0.5895407795906067\n",
      "cnt: 0 - valLoss: 0.5975375175476074 - trainLoss: 0.5895213484764099\n",
      "cnt: 0 - valLoss: 0.5975165963172913 - trainLoss: 0.5895020365715027\n",
      "cnt: 0 - valLoss: 0.5974956154823303 - trainLoss: 0.5894826054573059\n",
      "cnt: 0 - valLoss: 0.5974746346473694 - trainLoss: 0.5894631743431091\n",
      "cnt: 0 - valLoss: 0.5974536538124084 - trainLoss: 0.5894437432289124\n",
      "cnt: 0 - valLoss: 0.5974326729774475 - trainLoss: 0.5894243121147156\n",
      "cnt: 0 - valLoss: 0.5974116921424866 - trainLoss: 0.5894048810005188\n",
      "cnt: 0 - valLoss: 0.5973906517028809 - trainLoss: 0.589385449886322\n",
      "cnt: 0 - valLoss: 0.5973697304725647 - trainLoss: 0.5893660187721252\n",
      "cnt: 0 - valLoss: 0.597348690032959 - trainLoss: 0.5893465876579285\n",
      "cnt: 0 - valLoss: 0.5973276495933533 - trainLoss: 0.5893271565437317\n",
      "cnt: 0 - valLoss: 0.5973066687583923 - trainLoss: 0.5893077254295349\n",
      "cnt: 0 - valLoss: 0.5972856879234314 - trainLoss: 0.5892882943153381\n",
      "cnt: 0 - valLoss: 0.5972646474838257 - trainLoss: 0.5892688632011414\n",
      "cnt: 0 - valLoss: 0.5972436666488647 - trainLoss: 0.5892494320869446\n",
      "cnt: 0 - valLoss: 0.597222626209259 - trainLoss: 0.5892300009727478\n",
      "cnt: 0 - valLoss: 0.5972015857696533 - trainLoss: 0.5892105102539062\n",
      "cnt: 0 - valLoss: 0.5971805453300476 - trainLoss: 0.5891910791397095\n",
      "cnt: 0 - valLoss: 0.5971595644950867 - trainLoss: 0.5891715884208679\n",
      "cnt: 0 - valLoss: 0.597138524055481 - trainLoss: 0.5891521573066711\n",
      "cnt: 0 - valLoss: 0.5971174836158752 - trainLoss: 0.5891326665878296\n",
      "cnt: 0 - valLoss: 0.5970965027809143 - trainLoss: 0.5891132354736328\n",
      "cnt: 0 - valLoss: 0.5970754027366638 - trainLoss: 0.5890937447547913\n",
      "cnt: 0 - valLoss: 0.5970543026924133 - trainLoss: 0.5890742540359497\n",
      "cnt: 0 - valLoss: 0.5970333218574524 - trainLoss: 0.5890547633171082\n",
      "cnt: 0 - valLoss: 0.5970122218132019 - trainLoss: 0.5890353322029114\n",
      "cnt: 0 - valLoss: 0.5969911813735962 - trainLoss: 0.5890158414840698\n",
      "cnt: 0 - valLoss: 0.5969700217247009 - trainLoss: 0.5889963507652283\n",
      "cnt: 0 - valLoss: 0.59694904088974 - trainLoss: 0.5889768600463867\n",
      "cnt: 0 - valLoss: 0.5969279408454895 - trainLoss: 0.5889573693275452\n",
      "cnt: 0 - valLoss: 0.596906840801239 - trainLoss: 0.5889378786087036\n",
      "cnt: 0 - valLoss: 0.5968857407569885 - trainLoss: 0.5889183878898621\n",
      "cnt: 0 - valLoss: 0.5968647003173828 - trainLoss: 0.5888989567756653\n",
      "cnt: 0 - valLoss: 0.5968436598777771 - trainLoss: 0.588879406452179\n",
      "cnt: 0 - valLoss: 0.5968225598335266 - trainLoss: 0.5888599157333374\n",
      "cnt: 0 - valLoss: 0.5968014001846313 - trainLoss: 0.5888403654098511\n",
      "cnt: 0 - valLoss: 0.5967803001403809 - trainLoss: 0.5888208746910095\n",
      "cnt: 0 - valLoss: 0.5967592000961304 - trainLoss: 0.588801383972168\n",
      "cnt: 0 - valLoss: 0.5967381000518799 - trainLoss: 0.5887818932533264\n",
      "cnt: 0 - valLoss: 0.5967170000076294 - trainLoss: 0.5887623429298401\n",
      "cnt: 0 - valLoss: 0.5966958999633789 - trainLoss: 0.5887427926063538\n",
      "cnt: 0 - valLoss: 0.5966747403144836 - trainLoss: 0.5887233018875122\n",
      "cnt: 0 - valLoss: 0.5966536402702332 - trainLoss: 0.5887037515640259\n",
      "cnt: 0 - valLoss: 0.5966324806213379 - trainLoss: 0.5886842608451843\n",
      "cnt: 0 - valLoss: 0.5966113805770874 - trainLoss: 0.588664710521698\n",
      "cnt: 0 - valLoss: 0.5965902209281921 - trainLoss: 0.5886451601982117\n",
      "cnt: 0 - valLoss: 0.5965691208839417 - trainLoss: 0.5886256694793701\n",
      "cnt: 0 - valLoss: 0.5965479016304016 - trainLoss: 0.5886061191558838\n",
      "cnt: 0 - valLoss: 0.5965268015861511 - trainLoss: 0.5885865688323975\n",
      "cnt: 0 - valLoss: 0.5965056419372559 - trainLoss: 0.5885670781135559\n",
      "cnt: 0 - valLoss: 0.5964844822883606 - trainLoss: 0.5885474681854248\n",
      "cnt: 0 - valLoss: 0.5964633822441101 - trainLoss: 0.5885279178619385\n",
      "cnt: 0 - valLoss: 0.5964421629905701 - trainLoss: 0.5885083675384521\n",
      "cnt: 0 - valLoss: 0.5964210033416748 - trainLoss: 0.5884888172149658\n",
      "cnt: 0 - valLoss: 0.5963998436927795 - trainLoss: 0.5884692668914795\n",
      "cnt: 0 - valLoss: 0.5963786244392395 - trainLoss: 0.5884497165679932\n",
      "cnt: 0 - valLoss: 0.596357524394989 - trainLoss: 0.5884301662445068\n",
      "cnt: 0 - valLoss: 0.596336305141449 - trainLoss: 0.5884105563163757\n",
      "cnt: 0 - valLoss: 0.5963150858879089 - trainLoss: 0.5883910655975342\n",
      "cnt: 0 - valLoss: 0.5962939262390137 - trainLoss: 0.5883714556694031\n",
      "cnt: 0 - valLoss: 0.5962727665901184 - trainLoss: 0.588351845741272\n",
      "cnt: 0 - valLoss: 0.5962516069412231 - trainLoss: 0.5883322954177856\n",
      "cnt: 0 - valLoss: 0.5962303876876831 - trainLoss: 0.5883128046989441\n",
      "cnt: 0 - valLoss: 0.5962091684341431 - trainLoss: 0.588293194770813\n",
      "cnt: 0 - valLoss: 0.596187949180603 - trainLoss: 0.5882736444473267\n",
      "cnt: 0 - valLoss: 0.5961667895317078 - trainLoss: 0.5882540345191956\n",
      "cnt: 0 - valLoss: 0.5961455702781677 - trainLoss: 0.5882344841957092\n",
      "cnt: 0 - valLoss: 0.5961243510246277 - trainLoss: 0.5882149338722229\n",
      "cnt: 0 - valLoss: 0.5961031317710876 - trainLoss: 0.5881953239440918\n",
      "cnt: 0 - valLoss: 0.5960819125175476 - trainLoss: 0.5881757736206055\n",
      "cnt: 0 - valLoss: 0.5960606932640076 - trainLoss: 0.5881561636924744\n",
      "cnt: 0 - valLoss: 0.5960394740104675 - trainLoss: 0.5881365537643433\n",
      "cnt: 0 - valLoss: 0.5960182547569275 - trainLoss: 0.5881170034408569\n",
      "cnt: 0 - valLoss: 0.5959969758987427 - trainLoss: 0.5880973935127258\n",
      "cnt: 0 - valLoss: 0.5959758162498474 - trainLoss: 0.5880777835845947\n",
      "cnt: 0 - valLoss: 0.5959545373916626 - trainLoss: 0.5880582332611084\n",
      "cnt: 0 - valLoss: 0.5959332585334778 - trainLoss: 0.5880386233329773\n",
      "cnt: 0 - valLoss: 0.5959120392799377 - trainLoss: 0.5880190134048462\n",
      "cnt: 0 - valLoss: 0.5958907604217529 - trainLoss: 0.5879994034767151\n",
      "cnt: 0 - valLoss: 0.5958695411682129 - trainLoss: 0.5879798531532288\n",
      "cnt: 0 - valLoss: 0.5958482623100281 - trainLoss: 0.5879602432250977\n",
      "cnt: 0 - valLoss: 0.5958269834518433 - trainLoss: 0.5879405736923218\n",
      "cnt: 0 - valLoss: 0.5958057641983032 - trainLoss: 0.5879209637641907\n",
      "cnt: 0 - valLoss: 0.5957844853401184 - trainLoss: 0.5879013538360596\n",
      "cnt: 0 - valLoss: 0.5957632660865784 - trainLoss: 0.5878817439079285\n",
      "cnt: 0 - valLoss: 0.5957420468330383 - trainLoss: 0.5878621339797974\n",
      "cnt: 0 - valLoss: 0.5957207679748535 - trainLoss: 0.5878425240516663\n",
      "cnt: 0 - valLoss: 0.5956994891166687 - trainLoss: 0.5878228545188904\n",
      "cnt: 0 - valLoss: 0.5956781506538391 - trainLoss: 0.5878032445907593\n",
      "cnt: 0 - valLoss: 0.5956569910049438 - trainLoss: 0.5877835154533386\n",
      "cnt: 0 - valLoss: 0.5956356525421143 - trainLoss: 0.5877639055252075\n",
      "cnt: 0 - valLoss: 0.5956143736839294 - trainLoss: 0.5877442955970764\n",
      "cnt: 0 - valLoss: 0.5955931544303894 - trainLoss: 0.5877245664596558\n",
      "cnt: 0 - valLoss: 0.5955718159675598 - trainLoss: 0.5877049565315247\n",
      "cnt: 0 - valLoss: 0.595550537109375 - trainLoss: 0.5876852869987488\n",
      "cnt: 0 - valLoss: 0.5955291986465454 - trainLoss: 0.5876656174659729\n",
      "cnt: 0 - valLoss: 0.5955079197883606 - trainLoss: 0.587645947933197\n",
      "cnt: 0 - valLoss: 0.5954866409301758 - trainLoss: 0.5876262784004211\n",
      "cnt: 0 - valLoss: 0.5954653024673462 - trainLoss: 0.5876066088676453\n",
      "cnt: 0 - valLoss: 0.5954440832138062 - trainLoss: 0.5875869393348694\n",
      "cnt: 0 - valLoss: 0.5954227447509766 - trainLoss: 0.5875672698020935\n",
      "cnt: 0 - valLoss: 0.595401406288147 - trainLoss: 0.5875475406646729\n",
      "cnt: 0 - valLoss: 0.5953801274299622 - trainLoss: 0.5875278115272522\n",
      "cnt: 0 - valLoss: 0.5953587889671326 - trainLoss: 0.5875081419944763\n",
      "cnt: 0 - valLoss: 0.5953373908996582 - trainLoss: 0.5874885320663452\n",
      "cnt: 0 - valLoss: 0.5953159928321838 - trainLoss: 0.5874687433242798\n",
      "cnt: 0 - valLoss: 0.595294713973999 - trainLoss: 0.5874490737915039\n",
      "cnt: 0 - valLoss: 0.5952733755111694 - trainLoss: 0.587429404258728\n",
      "cnt: 0 - valLoss: 0.5952519774436951 - trainLoss: 0.5874096751213074\n",
      "cnt: 0 - valLoss: 0.5952306389808655 - trainLoss: 0.5873899459838867\n",
      "cnt: 0 - valLoss: 0.5952092409133911 - trainLoss: 0.5873702168464661\n",
      "cnt: 0 - valLoss: 0.5951879024505615 - trainLoss: 0.5873504877090454\n",
      "cnt: 0 - valLoss: 0.5951665043830872 - trainLoss: 0.5873307585716248\n",
      "cnt: 0 - valLoss: 0.5951451063156128 - trainLoss: 0.5873110890388489\n",
      "cnt: 0 - valLoss: 0.595123827457428 - trainLoss: 0.5872913002967834\n",
      "cnt: 0 - valLoss: 0.5951023697853088 - trainLoss: 0.5872716307640076\n",
      "cnt: 0 - valLoss: 0.5950809717178345 - trainLoss: 0.5872518420219421\n",
      "cnt: 0 - valLoss: 0.5950595736503601 - trainLoss: 0.5872321724891663\n",
      "cnt: 0 - valLoss: 0.5950381755828857 - trainLoss: 0.5872123837471008\n",
      "cnt: 0 - valLoss: 0.5950168371200562 - trainLoss: 0.587192714214325\n",
      "cnt: 0 - valLoss: 0.594995379447937 - trainLoss: 0.5871729254722595\n",
      "cnt: 0 - valLoss: 0.5949739813804626 - trainLoss: 0.5871531367301941\n",
      "cnt: 0 - valLoss: 0.5949525237083435 - trainLoss: 0.5871334075927734\n",
      "cnt: 0 - valLoss: 0.5949311256408691 - trainLoss: 0.5871136784553528\n",
      "cnt: 0 - valLoss: 0.5949096083641052 - trainLoss: 0.5870940089225769\n",
      "cnt: 0 - valLoss: 0.5948881506919861 - trainLoss: 0.5870742201805115\n",
      "cnt: 0 - valLoss: 0.5948666930198669 - trainLoss: 0.587054431438446\n",
      "cnt: 0 - valLoss: 0.594845175743103 - trainLoss: 0.5870347023010254\n",
      "cnt: 0 - valLoss: 0.5948237180709839 - trainLoss: 0.58701491355896\n",
      "cnt: 0 - valLoss: 0.5948023200035095 - trainLoss: 0.5869951248168945\n",
      "cnt: 0 - valLoss: 0.5947808027267456 - trainLoss: 0.5869752764701843\n",
      "cnt: 0 - valLoss: 0.5947593450546265 - trainLoss: 0.5869554877281189\n",
      "cnt: 0 - valLoss: 0.5947378873825073 - trainLoss: 0.5869356989860535\n",
      "cnt: 0 - valLoss: 0.5947163701057434 - trainLoss: 0.5869159698486328\n",
      "cnt: 0 - valLoss: 0.5946948528289795 - trainLoss: 0.5868961811065674\n",
      "cnt: 0 - valLoss: 0.5946733951568604 - trainLoss: 0.586876392364502\n",
      "cnt: 0 - valLoss: 0.5946518778800964 - trainLoss: 0.5868566036224365\n",
      "cnt: 0 - valLoss: 0.5946303606033325 - trainLoss: 0.5868367552757263\n",
      "cnt: 0 - valLoss: 0.5946088433265686 - trainLoss: 0.5868169665336609\n",
      "cnt: 0 - valLoss: 0.5945873260498047 - trainLoss: 0.5867971181869507\n",
      "cnt: 0 - valLoss: 0.5945658087730408 - trainLoss: 0.5867773294448853\n",
      "cnt: 0 - valLoss: 0.5945442914962769 - trainLoss: 0.5867576003074646\n",
      "cnt: 0 - valLoss: 0.5945227742195129 - trainLoss: 0.5867376923561096\n",
      "cnt: 0 - valLoss: 0.594501256942749 - trainLoss: 0.5867179036140442\n",
      "cnt: 0 - valLoss: 0.5944797396659851 - trainLoss: 0.5866981148719788\n",
      "cnt: 0 - valLoss: 0.5944582223892212 - trainLoss: 0.5866782665252686\n",
      "cnt: 0 - valLoss: 0.5944367051124573 - trainLoss: 0.5866584777832031\n",
      "cnt: 0 - valLoss: 0.5944151282310486 - trainLoss: 0.5866386294364929\n",
      "cnt: 0 - valLoss: 0.5943936109542847 - trainLoss: 0.5866188406944275\n",
      "cnt: 0 - valLoss: 0.594372034072876 - trainLoss: 0.5865989327430725\n",
      "cnt: 0 - valLoss: 0.5943505167961121 - trainLoss: 0.5865790843963623\n",
      "cnt: 0 - valLoss: 0.5943289995193481 - trainLoss: 0.5865591764450073\n",
      "cnt: 0 - valLoss: 0.5943073034286499 - trainLoss: 0.5865393877029419\n",
      "cnt: 0 - valLoss: 0.594285786151886 - trainLoss: 0.5865195393562317\n",
      "cnt: 0 - valLoss: 0.5942642092704773 - trainLoss: 0.5864996314048767\n",
      "cnt: 0 - valLoss: 0.5942426323890686 - trainLoss: 0.5864797830581665\n",
      "cnt: 0 - valLoss: 0.5942211151123047 - trainLoss: 0.5864599347114563\n",
      "cnt: 0 - valLoss: 0.5941994786262512 - trainLoss: 0.5864400863647461\n",
      "cnt: 0 - valLoss: 0.5941779613494873 - trainLoss: 0.5864202380180359\n",
      "cnt: 0 - valLoss: 0.5941563248634338 - trainLoss: 0.5864003300666809\n",
      "cnt: 0 - valLoss: 0.5941347479820251 - trainLoss: 0.5863804817199707\n",
      "cnt: 0 - valLoss: 0.5941131114959717 - trainLoss: 0.5863605737686157\n",
      "cnt: 0 - valLoss: 0.5940915942192078 - trainLoss: 0.5863407254219055\n",
      "cnt: 0 - valLoss: 0.5940698981285095 - trainLoss: 0.5863208174705505\n",
      "cnt: 0 - valLoss: 0.5940483212471008 - trainLoss: 0.5863009691238403\n",
      "cnt: 0 - valLoss: 0.5940267443656921 - trainLoss: 0.5862811803817749\n",
      "cnt: 0 - valLoss: 0.5940051674842834 - trainLoss: 0.5862612724304199\n",
      "cnt: 0 - valLoss: 0.59398353099823 - trainLoss: 0.5862414836883545\n",
      "cnt: 0 - valLoss: 0.5939618945121765 - trainLoss: 0.5862216353416443\n",
      "cnt: 0 - valLoss: 0.5939403176307678 - trainLoss: 0.5862017869949341\n",
      "cnt: 0 - valLoss: 0.5939186811447144 - trainLoss: 0.5861819982528687\n",
      "cnt: 0 - valLoss: 0.5938971042633057 - trainLoss: 0.5861621499061584\n",
      "cnt: 0 - valLoss: 0.5938754677772522 - trainLoss: 0.5861422419548035\n",
      "cnt: 0 - valLoss: 0.5938538312911987 - trainLoss: 0.586122453212738\n",
      "cnt: 0 - valLoss: 0.5938321948051453 - trainLoss: 0.5861026048660278\n",
      "cnt: 0 - valLoss: 0.593810498714447 - trainLoss: 0.5860827565193176\n",
      "cnt: 0 - valLoss: 0.5937889218330383 - trainLoss: 0.5860629081726074\n",
      "cnt: 0 - valLoss: 0.5937672853469849 - trainLoss: 0.5860430598258972\n",
      "cnt: 0 - valLoss: 0.5937456488609314 - trainLoss: 0.586023211479187\n",
      "cnt: 0 - valLoss: 0.5937240123748779 - trainLoss: 0.5860033631324768\n",
      "cnt: 0 - valLoss: 0.5937023162841797 - trainLoss: 0.5859834551811218\n",
      "cnt: 0 - valLoss: 0.5936806797981262 - trainLoss: 0.5859636664390564\n",
      "cnt: 0 - valLoss: 0.5936590433120728 - trainLoss: 0.5859437584877014\n",
      "cnt: 0 - valLoss: 0.5936374068260193 - trainLoss: 0.5859239101409912\n",
      "cnt: 0 - valLoss: 0.593615710735321 - trainLoss: 0.5859040021896362\n",
      "cnt: 0 - valLoss: 0.5935940146446228 - trainLoss: 0.585884153842926\n",
      "cnt: 0 - valLoss: 0.5935723781585693 - trainLoss: 0.5858643054962158\n",
      "cnt: 0 - valLoss: 0.5935506820678711 - trainLoss: 0.5858444571495056\n",
      "cnt: 0 - valLoss: 0.5935289859771729 - trainLoss: 0.5858244895935059\n",
      "cnt: 0 - valLoss: 0.5935073494911194 - trainLoss: 0.5858046412467957\n",
      "cnt: 0 - valLoss: 0.5934856534004211 - trainLoss: 0.5857847929000854\n",
      "cnt: 0 - valLoss: 0.5934640169143677 - trainLoss: 0.5857649445533752\n",
      "cnt: 0 - valLoss: 0.5934423208236694 - trainLoss: 0.5857449769973755\n",
      "cnt: 0 - valLoss: 0.5934206247329712 - trainLoss: 0.5857251286506653\n",
      "cnt: 0 - valLoss: 0.5933989882469177 - trainLoss: 0.5857052803039551\n",
      "cnt: 0 - valLoss: 0.5933772921562195 - trainLoss: 0.5856853723526001\n",
      "cnt: 0 - valLoss: 0.5933555960655212 - trainLoss: 0.5856654644012451\n",
      "cnt: 0 - valLoss: 0.593333899974823 - trainLoss: 0.5856456160545349\n",
      "cnt: 0 - valLoss: 0.59331214427948 - trainLoss: 0.5856257081031799\n",
      "cnt: 0 - valLoss: 0.5932904481887817 - trainLoss: 0.585605800151825\n",
      "cnt: 0 - valLoss: 0.5932688117027283 - trainLoss: 0.58558589220047\n",
      "cnt: 0 - valLoss: 0.5932470560073853 - trainLoss: 0.585565984249115\n",
      "cnt: 0 - valLoss: 0.5932253003120422 - trainLoss: 0.5855460166931152\n",
      "cnt: 0 - valLoss: 0.5932036638259888 - trainLoss: 0.585526168346405\n",
      "cnt: 0 - valLoss: 0.5931819081306458 - trainLoss: 0.58550626039505\n",
      "cnt: 0 - valLoss: 0.5931601524353027 - trainLoss: 0.5854863524436951\n",
      "cnt: 0 - valLoss: 0.5931384563446045 - trainLoss: 0.5854664444923401\n",
      "cnt: 0 - valLoss: 0.5931167006492615 - trainLoss: 0.5854465365409851\n",
      "cnt: 0 - valLoss: 0.5930950045585632 - trainLoss: 0.5854265689849854\n",
      "cnt: 0 - valLoss: 0.5930732488632202 - trainLoss: 0.5854066610336304\n",
      "cnt: 0 - valLoss: 0.593051552772522 - trainLoss: 0.5853867530822754\n",
      "cnt: 0 - valLoss: 0.5930297374725342 - trainLoss: 0.5853667855262756\n",
      "cnt: 0 - valLoss: 0.5930079817771912 - trainLoss: 0.5853469371795654\n",
      "cnt: 0 - valLoss: 0.5929862856864929 - trainLoss: 0.5853269100189209\n",
      "cnt: 0 - valLoss: 0.5929645299911499 - trainLoss: 0.5853070020675659\n",
      "cnt: 0 - valLoss: 0.5929427146911621 - trainLoss: 0.5852870941162109\n",
      "cnt: 0 - valLoss: 0.5929210186004639 - trainLoss: 0.5852671265602112\n",
      "cnt: 0 - valLoss: 0.5928992033004761 - trainLoss: 0.5852472186088562\n",
      "cnt: 0 - valLoss: 0.5928774476051331 - trainLoss: 0.5852272510528564\n",
      "cnt: 0 - valLoss: 0.5928557515144348 - trainLoss: 0.5852072834968567\n",
      "cnt: 0 - valLoss: 0.592833936214447 - trainLoss: 0.5851873755455017\n",
      "cnt: 0 - valLoss: 0.5928121209144592 - trainLoss: 0.5851675271987915\n",
      "cnt: 0 - valLoss: 0.5927903056144714 - trainLoss: 0.5851475596427917\n",
      "cnt: 0 - valLoss: 0.5927685499191284 - trainLoss: 0.585127592086792\n",
      "cnt: 0 - valLoss: 0.5927467942237854 - trainLoss: 0.5851076245307922\n",
      "cnt: 0 - valLoss: 0.5927249789237976 - trainLoss: 0.5850876569747925\n",
      "cnt: 0 - valLoss: 0.5927032232284546 - trainLoss: 0.5850677490234375\n",
      "cnt: 0 - valLoss: 0.592681348323822 - trainLoss: 0.5850477814674377\n",
      "cnt: 0 - valLoss: 0.592659592628479 - trainLoss: 0.585027813911438\n",
      "cnt: 0 - valLoss: 0.592637836933136 - trainLoss: 0.5850078463554382\n",
      "cnt: 0 - valLoss: 0.5926160216331482 - trainLoss: 0.5849878787994385\n",
      "cnt: 0 - valLoss: 0.5925941467285156 - trainLoss: 0.584967851638794\n",
      "cnt: 0 - valLoss: 0.5925723314285278 - trainLoss: 0.584947943687439\n",
      "cnt: 0 - valLoss: 0.5925505757331848 - trainLoss: 0.5849279165267944\n",
      "cnt: 0 - valLoss: 0.5925287008285522 - trainLoss: 0.5849080085754395\n",
      "cnt: 0 - valLoss: 0.5925069451332092 - trainLoss: 0.5848879814147949\n",
      "cnt: 0 - valLoss: 0.5924850702285767 - trainLoss: 0.5848680138587952\n",
      "cnt: 0 - valLoss: 0.5924631953239441 - trainLoss: 0.5848480463027954\n",
      "cnt: 0 - valLoss: 0.5924414396286011 - trainLoss: 0.5848280191421509\n",
      "cnt: 0 - valLoss: 0.5924195647239685 - trainLoss: 0.5848080515861511\n",
      "cnt: 0 - valLoss: 0.5923977494239807 - trainLoss: 0.5847880840301514\n",
      "cnt: 0 - valLoss: 0.5923759341239929 - trainLoss: 0.5847680568695068\n",
      "cnt: 0 - valLoss: 0.5923541784286499 - trainLoss: 0.5847480893135071\n",
      "cnt: 0 - valLoss: 0.5923323035240173 - trainLoss: 0.5847281217575073\n",
      "cnt: 0 - valLoss: 0.5923104286193848 - trainLoss: 0.5847080945968628\n",
      "cnt: 0 - valLoss: 0.592288613319397 - trainLoss: 0.5846880078315735\n",
      "cnt: 0 - valLoss: 0.5922667980194092 - trainLoss: 0.584667980670929\n",
      "cnt: 0 - valLoss: 0.5922449231147766 - trainLoss: 0.584648072719574\n",
      "cnt: 0 - valLoss: 0.5922231078147888 - trainLoss: 0.5846281051635742\n",
      "cnt: 0 - valLoss: 0.592201292514801 - trainLoss: 0.5846081376075745\n",
      "cnt: 0 - valLoss: 0.5921794772148132 - trainLoss: 0.5845881104469299\n",
      "cnt: 0 - valLoss: 0.5921576023101807 - trainLoss: 0.584568202495575\n",
      "cnt: 0 - valLoss: 0.5921357870101929 - trainLoss: 0.5845482349395752\n",
      "cnt: 0 - valLoss: 0.5921139121055603 - trainLoss: 0.5845282077789307\n",
      "cnt: 0 - valLoss: 0.5920920372009277 - trainLoss: 0.5845082402229309\n",
      "cnt: 0 - valLoss: 0.5920702219009399 - trainLoss: 0.5844882130622864\n",
      "cnt: 0 - valLoss: 0.5920484066009521 - trainLoss: 0.5844682455062866\n",
      "cnt: 0 - valLoss: 0.5920264720916748 - trainLoss: 0.5844482779502869\n",
      "cnt: 0 - valLoss: 0.5920045971870422 - trainLoss: 0.5844283103942871\n",
      "cnt: 0 - valLoss: 0.5919827222824097 - trainLoss: 0.5844082832336426\n",
      "cnt: 0 - valLoss: 0.5919609069824219 - trainLoss: 0.584388256072998\n",
      "cnt: 0 - valLoss: 0.5919390320777893 - trainLoss: 0.5843682885169983\n",
      "cnt: 0 - valLoss: 0.5919171571731567 - trainLoss: 0.5843483209609985\n",
      "cnt: 0 - valLoss: 0.5918952822685242 - trainLoss: 0.584328293800354\n",
      "cnt: 0 - valLoss: 0.5918733477592468 - trainLoss: 0.5843083262443542\n",
      "cnt: 0 - valLoss: 0.5918514728546143 - trainLoss: 0.5842882394790649\n",
      "cnt: 0 - valLoss: 0.5918295979499817 - trainLoss: 0.5842682123184204\n",
      "cnt: 0 - valLoss: 0.5918077230453491 - trainLoss: 0.5842481851577759\n",
      "cnt: 0 - valLoss: 0.5917857885360718 - trainLoss: 0.5842282176017761\n",
      "cnt: 0 - valLoss: 0.5917638540267944 - trainLoss: 0.5842081904411316\n",
      "cnt: 0 - valLoss: 0.5917420387268066 - trainLoss: 0.5841882228851318\n",
      "cnt: 0 - valLoss: 0.5917200446128845 - trainLoss: 0.5841681361198425\n",
      "cnt: 0 - valLoss: 0.591698169708252 - trainLoss: 0.584148108959198\n",
      "cnt: 0 - valLoss: 0.5916762351989746 - trainLoss: 0.5841280817985535\n",
      "cnt: 0 - valLoss: 0.5916543006896973 - trainLoss: 0.5841080546379089\n",
      "cnt: 0 - valLoss: 0.5916324257850647 - trainLoss: 0.5840880274772644\n",
      "cnt: 0 - valLoss: 0.5916104912757874 - trainLoss: 0.5840680003166199\n",
      "cnt: 0 - valLoss: 0.59158855676651 - trainLoss: 0.5840479731559753\n",
      "cnt: 0 - valLoss: 0.5915666222572327 - trainLoss: 0.5840279459953308\n",
      "cnt: 0 - valLoss: 0.5915446877479553 - trainLoss: 0.5840079188346863\n",
      "cnt: 0 - valLoss: 0.591522753238678 - trainLoss: 0.5839877724647522\n",
      "cnt: 0 - valLoss: 0.5915007591247559 - trainLoss: 0.5839677453041077\n",
      "cnt: 0 - valLoss: 0.5914788246154785 - trainLoss: 0.5839477181434631\n",
      "cnt: 0 - valLoss: 0.5914568901062012 - trainLoss: 0.5839276909828186\n",
      "cnt: 0 - valLoss: 0.5914349555969238 - trainLoss: 0.5839076042175293\n",
      "cnt: 0 - valLoss: 0.5914130210876465 - trainLoss: 0.58388751745224\n",
      "cnt: 0 - valLoss: 0.5913910865783691 - trainLoss: 0.5838674902915955\n",
      "cnt: 0 - valLoss: 0.591369092464447 - trainLoss: 0.5838474035263062\n",
      "cnt: 0 - valLoss: 0.5913470983505249 - trainLoss: 0.5838273167610168\n",
      "cnt: 0 - valLoss: 0.5913251638412476 - trainLoss: 0.5838072896003723\n",
      "cnt: 0 - valLoss: 0.5913031697273254 - trainLoss: 0.583787202835083\n",
      "cnt: 0 - valLoss: 0.5912812352180481 - trainLoss: 0.5837671756744385\n",
      "cnt: 0 - valLoss: 0.591259241104126 - trainLoss: 0.5837470293045044\n",
      "cnt: 0 - valLoss: 0.5912371873855591 - trainLoss: 0.5837269425392151\n",
      "cnt: 0 - valLoss: 0.5912152528762817 - trainLoss: 0.5837069153785706\n",
      "cnt: 0 - valLoss: 0.5911931991577148 - trainLoss: 0.5836868286132812\n",
      "cnt: 0 - valLoss: 0.5911712646484375 - trainLoss: 0.5836666822433472\n",
      "cnt: 0 - valLoss: 0.5911492109298706 - trainLoss: 0.5836465954780579\n",
      "cnt: 0 - valLoss: 0.5911272168159485 - trainLoss: 0.5836265683174133\n",
      "cnt: 0 - valLoss: 0.5911052823066711 - trainLoss: 0.583606481552124\n",
      "cnt: 0 - valLoss: 0.591083288192749 - trainLoss: 0.5835863947868347\n",
      "cnt: 0 - valLoss: 0.5910613536834717 - trainLoss: 0.583566427230835\n",
      "cnt: 0 - valLoss: 0.5910393595695496 - trainLoss: 0.5835464000701904\n",
      "cnt: 0 - valLoss: 0.5910173654556274 - trainLoss: 0.5835263729095459\n",
      "cnt: 0 - valLoss: 0.5909953713417053 - trainLoss: 0.5835063457489014\n",
      "cnt: 0 - valLoss: 0.590973436832428 - trainLoss: 0.5834863185882568\n",
      "cnt: 0 - valLoss: 0.5909514427185059 - trainLoss: 0.5834663510322571\n",
      "cnt: 0 - valLoss: 0.5909294486045837 - trainLoss: 0.5834462642669678\n",
      "cnt: 0 - valLoss: 0.5909074544906616 - trainLoss: 0.5834262371063232\n",
      "cnt: 0 - valLoss: 0.5908854603767395 - trainLoss: 0.5834062099456787\n",
      "cnt: 0 - valLoss: 0.5908634066581726 - trainLoss: 0.5833861827850342\n",
      "cnt: 0 - valLoss: 0.5908414721488953 - trainLoss: 0.5833661556243896\n",
      "cnt: 0 - valLoss: 0.5908194780349731 - trainLoss: 0.5833460688591003\n",
      "cnt: 0 - valLoss: 0.590797483921051 - trainLoss: 0.5833261013031006\n",
      "cnt: 0 - valLoss: 0.5907754898071289 - trainLoss: 0.583306074142456\n",
      "cnt: 0 - valLoss: 0.5907535552978516 - trainLoss: 0.5832860469818115\n",
      "cnt: 0 - valLoss: 0.5907315611839294 - trainLoss: 0.5832659602165222\n",
      "cnt: 0 - valLoss: 0.5907095074653625 - trainLoss: 0.5832459330558777\n",
      "cnt: 0 - valLoss: 0.5906875133514404 - trainLoss: 0.5832259058952332\n",
      "cnt: 0 - valLoss: 0.5906655192375183 - trainLoss: 0.5832058787345886\n",
      "cnt: 0 - valLoss: 0.5906434655189514 - trainLoss: 0.5831857919692993\n",
      "cnt: 0 - valLoss: 0.5906214118003845 - trainLoss: 0.5831657648086548\n",
      "cnt: 0 - valLoss: 0.5905994176864624 - trainLoss: 0.5831457376480103\n",
      "cnt: 0 - valLoss: 0.5905773639678955 - trainLoss: 0.583125650882721\n",
      "cnt: 0 - valLoss: 0.5905553698539734 - trainLoss: 0.5831056237220764\n",
      "cnt: 0 - valLoss: 0.5905333757400513 - trainLoss: 0.5830855965614319\n",
      "cnt: 0 - valLoss: 0.5905112624168396 - trainLoss: 0.5830655097961426\n",
      "cnt: 0 - valLoss: 0.5904893279075623 - trainLoss: 0.583045482635498\n",
      "cnt: 0 - valLoss: 0.5904672741889954 - trainLoss: 0.5830253958702087\n",
      "cnt: 0 - valLoss: 0.5904451608657837 - trainLoss: 0.5830053091049194\n",
      "cnt: 0 - valLoss: 0.5904231667518616 - trainLoss: 0.5829852819442749\n",
      "cnt: 0 - valLoss: 0.5904010534286499 - trainLoss: 0.5829651951789856\n",
      "cnt: 0 - valLoss: 0.5903789401054382 - trainLoss: 0.5829451084136963\n",
      "cnt: 0 - valLoss: 0.5903568267822266 - trainLoss: 0.5829250812530518\n",
      "cnt: 0 - valLoss: 0.5903347134590149 - trainLoss: 0.5829049348831177\n",
      "cnt: 0 - valLoss: 0.5903126001358032 - trainLoss: 0.5828849077224731\n",
      "cnt: 0 - valLoss: 0.5902904272079468 - trainLoss: 0.5828648805618286\n",
      "cnt: 0 - valLoss: 0.5902682542800903 - trainLoss: 0.5828447341918945\n",
      "cnt: 0 - valLoss: 0.5902460813522339 - trainLoss: 0.58282470703125\n",
      "cnt: 0 - valLoss: 0.5902239084243774 - trainLoss: 0.5828045606613159\n",
      "cnt: 0 - valLoss: 0.5902017951011658 - trainLoss: 0.5827845335006714\n",
      "cnt: 0 - valLoss: 0.5901795625686646 - trainLoss: 0.5827645063400269\n",
      "cnt: 0 - valLoss: 0.5901573896408081 - trainLoss: 0.5827444791793823\n",
      "cnt: 0 - valLoss: 0.5901352167129517 - trainLoss: 0.582724392414093\n",
      "cnt: 0 - valLoss: 0.59011310338974 - trainLoss: 0.5827042460441589\n",
      "cnt: 0 - valLoss: 0.5900908708572388 - trainLoss: 0.5826842188835144\n",
      "cnt: 0 - valLoss: 0.5900686979293823 - trainLoss: 0.5826641321182251\n",
      "cnt: 0 - valLoss: 0.5900464653968811 - trainLoss: 0.5826441049575806\n",
      "cnt: 0 - valLoss: 0.5900243520736694 - trainLoss: 0.5826239585876465\n",
      "cnt: 0 - valLoss: 0.5900021195411682 - trainLoss: 0.5826038718223572\n",
      "cnt: 0 - valLoss: 0.5899799466133118 - trainLoss: 0.5825838446617126\n",
      "cnt: 0 - valLoss: 0.5899577140808105 - trainLoss: 0.5825637578964233\n",
      "cnt: 0 - valLoss: 0.5899354815483093 - trainLoss: 0.582543671131134\n",
      "cnt: 0 - valLoss: 0.5899133086204529 - trainLoss: 0.5825235843658447\n",
      "cnt: 0 - valLoss: 0.5898911356925964 - trainLoss: 0.5825034379959106\n",
      "cnt: 0 - valLoss: 0.5898688435554504 - trainLoss: 0.5824832916259766\n",
      "cnt: 0 - valLoss: 0.589846670627594 - trainLoss: 0.5824632048606873\n",
      "cnt: 0 - valLoss: 0.5898244380950928 - trainLoss: 0.5824430584907532\n",
      "cnt: 0 - valLoss: 0.5898021459579468 - trainLoss: 0.5824230313301086\n",
      "cnt: 0 - valLoss: 0.5897799730300903 - trainLoss: 0.5824028253555298\n",
      "cnt: 0 - valLoss: 0.5897576808929443 - trainLoss: 0.5823827385902405\n",
      "cnt: 0 - valLoss: 0.5897355079650879 - trainLoss: 0.5823625922203064\n",
      "cnt: 0 - valLoss: 0.5897132754325867 - trainLoss: 0.5823425054550171\n",
      "cnt: 0 - valLoss: 0.5896909832954407 - trainLoss: 0.582322359085083\n",
      "cnt: 0 - valLoss: 0.5896687507629395 - trainLoss: 0.5823022723197937\n",
      "cnt: 0 - valLoss: 0.5896465182304382 - trainLoss: 0.5822820663452148\n",
      "cnt: 0 - valLoss: 0.5896242260932922 - trainLoss: 0.5822619795799255\n",
      "cnt: 0 - valLoss: 0.589601993560791 - trainLoss: 0.5822418332099915\n",
      "cnt: 0 - valLoss: 0.589579701423645 - trainLoss: 0.5822217464447021\n",
      "cnt: 0 - valLoss: 0.589557409286499 - trainLoss: 0.5822015404701233\n",
      "cnt: 0 - valLoss: 0.5895352363586426 - trainLoss: 0.5821813941001892\n",
      "cnt: 0 - valLoss: 0.5895129442214966 - trainLoss: 0.5821613073348999\n",
      "cnt: 0 - valLoss: 0.5894906520843506 - trainLoss: 0.582141101360321\n",
      "cnt: 0 - valLoss: 0.5894683599472046 - trainLoss: 0.582120954990387\n",
      "cnt: 0 - valLoss: 0.5894460678100586 - trainLoss: 0.5821007490158081\n",
      "cnt: 0 - valLoss: 0.5894237756729126 - trainLoss: 0.582080602645874\n",
      "cnt: 0 - valLoss: 0.5894014835357666 - trainLoss: 0.5820604562759399\n",
      "cnt: 0 - valLoss: 0.5893791913986206 - trainLoss: 0.5820402503013611\n",
      "cnt: 0 - valLoss: 0.5893568992614746 - trainLoss: 0.5820200443267822\n",
      "cnt: 0 - valLoss: 0.5893346667289734 - trainLoss: 0.5819998383522034\n",
      "cnt: 0 - valLoss: 0.5893123745918274 - trainLoss: 0.5819796323776245\n",
      "cnt: 0 - valLoss: 0.5892900824546814 - trainLoss: 0.5819594264030457\n",
      "cnt: 0 - valLoss: 0.5892677903175354 - trainLoss: 0.5819392204284668\n",
      "cnt: 0 - valLoss: 0.5892453789710999 - trainLoss: 0.5819190144538879\n",
      "cnt: 0 - valLoss: 0.5892230868339539 - trainLoss: 0.5818988084793091\n",
      "cnt: 0 - valLoss: 0.5892007946968079 - trainLoss: 0.5818786025047302\n",
      "cnt: 0 - valLoss: 0.5891785025596619 - trainLoss: 0.5818583965301514\n",
      "cnt: 0 - valLoss: 0.5891562104225159 - trainLoss: 0.5818381309509277\n",
      "cnt: 0 - valLoss: 0.5891338586807251 - trainLoss: 0.5818179249763489\n",
      "cnt: 0 - valLoss: 0.5891115069389343 - trainLoss: 0.58179771900177\n",
      "cnt: 0 - valLoss: 0.5890892148017883 - trainLoss: 0.5817775130271912\n",
      "cnt: 0 - valLoss: 0.5890668630599976 - trainLoss: 0.5817572474479675\n",
      "cnt: 0 - valLoss: 0.5890445709228516 - trainLoss: 0.5817370414733887\n",
      "cnt: 0 - valLoss: 0.589022159576416 - trainLoss: 0.581716775894165\n",
      "cnt: 0 - valLoss: 0.58899986743927 - trainLoss: 0.5816965699195862\n",
      "cnt: 0 - valLoss: 0.5889775156974792 - trainLoss: 0.5816763639450073\n",
      "cnt: 0 - valLoss: 0.5889551639556885 - trainLoss: 0.5816560387611389\n",
      "cnt: 0 - valLoss: 0.5889328718185425 - trainLoss: 0.5816358327865601\n",
      "cnt: 0 - valLoss: 0.5889104604721069 - trainLoss: 0.5816155672073364\n",
      "cnt: 0 - valLoss: 0.5888881087303162 - trainLoss: 0.5815953612327576\n",
      "cnt: 0 - valLoss: 0.5888657569885254 - trainLoss: 0.5815750956535339\n",
      "cnt: 0 - valLoss: 0.5888434648513794 - trainLoss: 0.5815548896789551\n",
      "cnt: 0 - valLoss: 0.5888210535049438 - trainLoss: 0.5815345644950867\n",
      "cnt: 0 - valLoss: 0.5887985825538635 - trainLoss: 0.581514298915863\n",
      "cnt: 0 - valLoss: 0.5887763500213623 - trainLoss: 0.5814940929412842\n",
      "cnt: 0 - valLoss: 0.588753879070282 - trainLoss: 0.5814738273620605\n",
      "cnt: 0 - valLoss: 0.5887314677238464 - trainLoss: 0.5814535021781921\n",
      "cnt: 0 - valLoss: 0.5887091755867004 - trainLoss: 0.5814332962036133\n",
      "cnt: 0 - valLoss: 0.5886868238449097 - trainLoss: 0.5814130306243896\n",
      "cnt: 0 - valLoss: 0.5886645317077637 - trainLoss: 0.5813927054405212\n",
      "cnt: 0 - valLoss: 0.5886422395706177 - trainLoss: 0.5813724398612976\n",
      "cnt: 0 - valLoss: 0.5886199474334717 - trainLoss: 0.581352174282074\n",
      "cnt: 0 - valLoss: 0.5885976552963257 - trainLoss: 0.5813318490982056\n",
      "cnt: 0 - valLoss: 0.5885753631591797 - trainLoss: 0.5813116431236267\n",
      "cnt: 0 - valLoss: 0.5885530710220337 - trainLoss: 0.5812913179397583\n",
      "cnt: 0 - valLoss: 0.5885307788848877 - trainLoss: 0.5812710523605347\n",
      "cnt: 0 - valLoss: 0.5885084271430969 - trainLoss: 0.5812507271766663\n",
      "cnt: 0 - valLoss: 0.5884861350059509 - trainLoss: 0.5812304019927979\n",
      "cnt: 0 - valLoss: 0.5884638428688049 - trainLoss: 0.581210196018219\n",
      "cnt: 0 - valLoss: 0.5884415507316589 - trainLoss: 0.5811898708343506\n",
      "cnt: 0 - valLoss: 0.5884191989898682 - trainLoss: 0.5811695456504822\n",
      "cnt: 0 - valLoss: 0.5883968472480774 - trainLoss: 0.5811492800712585\n",
      "cnt: 0 - valLoss: 0.5883745551109314 - trainLoss: 0.5811290144920349\n",
      "cnt: 0 - valLoss: 0.5883522629737854 - trainLoss: 0.5811086297035217\n",
      "cnt: 0 - valLoss: 0.5883298516273499 - trainLoss: 0.5810883641242981\n",
      "cnt: 0 - valLoss: 0.5883075594902039 - trainLoss: 0.5810680389404297\n",
      "cnt: 0 - valLoss: 0.5882852077484131 - trainLoss: 0.5810477137565613\n",
      "cnt: 0 - valLoss: 0.5882628560066223 - trainLoss: 0.5810274481773376\n",
      "cnt: 0 - valLoss: 0.5882405638694763 - trainLoss: 0.5810070633888245\n",
      "cnt: 0 - valLoss: 0.5882181525230408 - trainLoss: 0.580986738204956\n",
      "cnt: 0 - valLoss: 0.58819580078125 - trainLoss: 0.5809664130210876\n",
      "cnt: 0 - valLoss: 0.5881734490394592 - trainLoss: 0.580946147441864\n",
      "cnt: 0 - valLoss: 0.5881510972976685 - trainLoss: 0.5809257626533508\n",
      "cnt: 0 - valLoss: 0.5881287455558777 - trainLoss: 0.5809054970741272\n",
      "cnt: 0 - valLoss: 0.5881063938140869 - trainLoss: 0.580885112285614\n",
      "cnt: 0 - valLoss: 0.5880839824676514 - trainLoss: 0.5808647871017456\n",
      "cnt: 0 - valLoss: 0.5880616307258606 - trainLoss: 0.5808444619178772\n",
      "cnt: 0 - valLoss: 0.5880392789840698 - trainLoss: 0.5808241367340088\n",
      "cnt: 0 - valLoss: 0.5880168676376343 - trainLoss: 0.5808037519454956\n",
      "cnt: 0 - valLoss: 0.5879945158958435 - trainLoss: 0.5807834267616272\n",
      "cnt: 0 - valLoss: 0.587972104549408 - trainLoss: 0.580763041973114\n",
      "cnt: 0 - valLoss: 0.5879496932029724 - trainLoss: 0.5807427167892456\n",
      "cnt: 0 - valLoss: 0.5879273414611816 - trainLoss: 0.5807223320007324\n",
      "cnt: 0 - valLoss: 0.5879049301147461 - trainLoss: 0.580702006816864\n",
      "cnt: 0 - valLoss: 0.5878825783729553 - trainLoss: 0.5806816220283508\n",
      "cnt: 0 - valLoss: 0.587860107421875 - trainLoss: 0.5806612372398376\n",
      "cnt: 0 - valLoss: 0.5878377556800842 - trainLoss: 0.5806409120559692\n",
      "cnt: 0 - valLoss: 0.5878152847290039 - trainLoss: 0.580620527267456\n",
      "cnt: 0 - valLoss: 0.5877928733825684 - trainLoss: 0.5806002020835876\n",
      "cnt: 0 - valLoss: 0.5877704620361328 - trainLoss: 0.5805798172950745\n",
      "cnt: 0 - valLoss: 0.5877480506896973 - trainLoss: 0.5805594325065613\n",
      "cnt: 0 - valLoss: 0.5877256393432617 - trainLoss: 0.5805391073226929\n",
      "cnt: 0 - valLoss: 0.5877032279968262 - trainLoss: 0.5805186629295349\n",
      "cnt: 0 - valLoss: 0.5876808166503906 - trainLoss: 0.5804983377456665\n",
      "cnt: 0 - valLoss: 0.5876584053039551 - trainLoss: 0.5804778933525085\n",
      "cnt: 0 - valLoss: 0.5876359939575195 - trainLoss: 0.5804575085639954\n",
      "cnt: 0 - valLoss: 0.5876134634017944 - trainLoss: 0.5804371237754822\n",
      "cnt: 0 - valLoss: 0.5875911116600037 - trainLoss: 0.5804167985916138\n",
      "cnt: 0 - valLoss: 0.5875686407089233 - trainLoss: 0.5803963541984558\n",
      "cnt: 0 - valLoss: 0.587546169757843 - trainLoss: 0.5803760290145874\n",
      "cnt: 0 - valLoss: 0.5875236988067627 - trainLoss: 0.5803555846214294\n",
      "cnt: 0 - valLoss: 0.5875012874603271 - trainLoss: 0.5803351998329163\n",
      "cnt: 0 - valLoss: 0.5874788165092468 - trainLoss: 0.5803148150444031\n",
      "cnt: 0 - valLoss: 0.5874564051628113 - trainLoss: 0.5802943706512451\n",
      "cnt: 0 - valLoss: 0.5874338746070862 - trainLoss: 0.5802739858627319\n",
      "cnt: 0 - valLoss: 0.5874114632606506 - trainLoss: 0.580253541469574\n",
      "cnt: 0 - valLoss: 0.5873889923095703 - trainLoss: 0.5802331566810608\n",
      "cnt: 0 - valLoss: 0.58736652135849 - trainLoss: 0.5802127718925476\n",
      "cnt: 0 - valLoss: 0.5873441100120544 - trainLoss: 0.5801923871040344\n",
      "cnt: 0 - valLoss: 0.5873215794563293 - trainLoss: 0.5801718831062317\n",
      "cnt: 0 - valLoss: 0.5872991681098938 - trainLoss: 0.5801514983177185\n",
      "cnt: 0 - valLoss: 0.5872767567634583 - trainLoss: 0.5801311135292053\n",
      "cnt: 0 - valLoss: 0.5872542858123779 - trainLoss: 0.5801106691360474\n",
      "cnt: 0 - valLoss: 0.5872318148612976 - trainLoss: 0.5800902247428894\n",
      "cnt: 0 - valLoss: 0.5872092843055725 - trainLoss: 0.5800697803497314\n",
      "cnt: 0 - valLoss: 0.5871868133544922 - trainLoss: 0.5800493359565735\n",
      "cnt: 0 - valLoss: 0.5871644020080566 - trainLoss: 0.5800288915634155\n",
      "cnt: 0 - valLoss: 0.5871419310569763 - trainLoss: 0.5800084471702576\n",
      "cnt: 0 - valLoss: 0.5871193408966064 - trainLoss: 0.5799880027770996\n",
      "cnt: 0 - valLoss: 0.5870969295501709 - trainLoss: 0.5799676179885864\n",
      "cnt: 0 - valLoss: 0.5870744585990906 - trainLoss: 0.5799471139907837\n",
      "cnt: 0 - valLoss: 0.5870519280433655 - trainLoss: 0.5799266695976257\n",
      "cnt: 0 - valLoss: 0.5870294570922852 - trainLoss: 0.5799062252044678\n",
      "cnt: 0 - valLoss: 0.5870069265365601 - trainLoss: 0.5798857808113098\n",
      "cnt: 0 - valLoss: 0.5869843363761902 - trainLoss: 0.5798653364181519\n",
      "cnt: 0 - valLoss: 0.5869618654251099 - trainLoss: 0.5798448324203491\n",
      "cnt: 0 - valLoss: 0.5869393348693848 - trainLoss: 0.5798244476318359\n",
      "cnt: 0 - valLoss: 0.5869168043136597 - trainLoss: 0.5798039436340332\n",
      "cnt: 0 - valLoss: 0.5868942737579346 - trainLoss: 0.5797834992408752\n",
      "cnt: 0 - valLoss: 0.5868717432022095 - trainLoss: 0.5797630548477173\n",
      "cnt: 0 - valLoss: 0.5868491530418396 - trainLoss: 0.5797425508499146\n",
      "cnt: 0 - valLoss: 0.5868266224861145 - trainLoss: 0.5797221064567566\n",
      "cnt: 0 - valLoss: 0.5868040323257446 - trainLoss: 0.5797016620635986\n",
      "cnt: 0 - valLoss: 0.5867814421653748 - trainLoss: 0.5796811580657959\n",
      "cnt: 0 - valLoss: 0.5867589712142944 - trainLoss: 0.5796606540679932\n",
      "cnt: 0 - valLoss: 0.5867363810539246 - trainLoss: 0.5796401500701904\n",
      "cnt: 0 - valLoss: 0.5867137908935547 - trainLoss: 0.5796197056770325\n",
      "cnt: 0 - valLoss: 0.5866912603378296 - trainLoss: 0.5795992612838745\n",
      "cnt: 0 - valLoss: 0.5866686701774597 - trainLoss: 0.5795787572860718\n",
      "cnt: 0 - valLoss: 0.5866460800170898 - trainLoss: 0.579558253288269\n",
      "cnt: 0 - valLoss: 0.5866235494613647 - trainLoss: 0.5795378088951111\n",
      "cnt: 0 - valLoss: 0.5866008996963501 - trainLoss: 0.5795173645019531\n",
      "cnt: 0 - valLoss: 0.586578369140625 - trainLoss: 0.5794968008995056\n",
      "cnt: 0 - valLoss: 0.5865557789802551 - trainLoss: 0.5794763565063477\n",
      "cnt: 0 - valLoss: 0.5865331292152405 - trainLoss: 0.5794558525085449\n",
      "cnt: 0 - valLoss: 0.5865105390548706 - trainLoss: 0.5794353485107422\n",
      "cnt: 0 - valLoss: 0.5864879488945007 - trainLoss: 0.5794148445129395\n",
      "cnt: 0 - valLoss: 0.5864653587341309 - trainLoss: 0.5793943405151367\n",
      "cnt: 0 - valLoss: 0.586442768573761 - trainLoss: 0.5793737769126892\n",
      "cnt: 0 - valLoss: 0.5864201188087463 - trainLoss: 0.5793532729148865\n",
      "cnt: 0 - valLoss: 0.5863975882530212 - trainLoss: 0.5793327689170837\n",
      "cnt: 0 - valLoss: 0.5863749384880066 - trainLoss: 0.5793122053146362\n",
      "cnt: 0 - valLoss: 0.5863524079322815 - trainLoss: 0.5792917609214783\n",
      "cnt: 0 - valLoss: 0.5863297581672668 - trainLoss: 0.5792713165283203\n",
      "cnt: 0 - valLoss: 0.586307168006897 - trainLoss: 0.5792507529258728\n",
      "cnt: 0 - valLoss: 0.5862845182418823 - trainLoss: 0.5792302489280701\n",
      "cnt: 0 - valLoss: 0.5862618684768677 - trainLoss: 0.5792097449302673\n",
      "cnt: 0 - valLoss: 0.586239218711853 - trainLoss: 0.5791892409324646\n",
      "cnt: 0 - valLoss: 0.5862166881561279 - trainLoss: 0.5791687369346619\n",
      "cnt: 0 - valLoss: 0.5861940979957581 - trainLoss: 0.5791482329368591\n",
      "cnt: 0 - valLoss: 0.5861713886260986 - trainLoss: 0.5791276693344116\n",
      "cnt: 0 - valLoss: 0.586148738861084 - trainLoss: 0.5791072249412537\n",
      "cnt: 0 - valLoss: 0.5861260890960693 - trainLoss: 0.5790866017341614\n",
      "cnt: 0 - valLoss: 0.5861034393310547 - trainLoss: 0.5790660977363586\n",
      "cnt: 0 - valLoss: 0.5860808491706848 - trainLoss: 0.5790455937385559\n",
      "cnt: 0 - valLoss: 0.5860581398010254 - trainLoss: 0.5790250897407532\n",
      "cnt: 0 - valLoss: 0.5860354900360107 - trainLoss: 0.5790045261383057\n",
      "cnt: 0 - valLoss: 0.5860128402709961 - trainLoss: 0.5789839625358582\n",
      "cnt: 0 - valLoss: 0.5859901905059814 - trainLoss: 0.5789634585380554\n",
      "cnt: 0 - valLoss: 0.585967481136322 - trainLoss: 0.5789429545402527\n",
      "cnt: 0 - valLoss: 0.5859448909759521 - trainLoss: 0.5789223909378052\n",
      "cnt: 0 - valLoss: 0.5859222412109375 - trainLoss: 0.5789018273353577\n",
      "cnt: 0 - valLoss: 0.5858995914459229 - trainLoss: 0.5788812637329102\n",
      "cnt: 0 - valLoss: 0.5858768820762634 - trainLoss: 0.5788607597351074\n",
      "cnt: 0 - valLoss: 0.585854172706604 - trainLoss: 0.5788401961326599\n",
      "cnt: 0 - valLoss: 0.5858315229415894 - trainLoss: 0.5788196325302124\n",
      "cnt: 0 - valLoss: 0.5858088135719299 - trainLoss: 0.5787990689277649\n",
      "cnt: 0 - valLoss: 0.5857861638069153 - trainLoss: 0.5787785053253174\n",
      "cnt: 0 - valLoss: 0.5857634544372559 - trainLoss: 0.5787579417228699\n",
      "cnt: 0 - valLoss: 0.5857407450675964 - trainLoss: 0.5787373781204224\n",
      "cnt: 0 - valLoss: 0.585718035697937 - trainLoss: 0.5787168741226196\n",
      "cnt: 0 - valLoss: 0.5856953859329224 - trainLoss: 0.5786962509155273\n",
      "cnt: 0 - valLoss: 0.5856726765632629 - trainLoss: 0.5786756873130798\n",
      "cnt: 0 - valLoss: 0.5856499671936035 - trainLoss: 0.5786551237106323\n",
      "cnt: 0 - valLoss: 0.5856272578239441 - trainLoss: 0.5786345601081848\n",
      "cnt: 0 - valLoss: 0.5856045484542847 - trainLoss: 0.5786139369010925\n",
      "cnt: 0 - valLoss: 0.5855818390846252 - trainLoss: 0.5785934329032898\n",
      "cnt: 0 - valLoss: 0.5855591297149658 - trainLoss: 0.5785728096961975\n",
      "cnt: 0 - valLoss: 0.5855364203453064 - trainLoss: 0.57855224609375\n",
      "cnt: 0 - valLoss: 0.585513710975647 - trainLoss: 0.5785316824913025\n",
      "cnt: 0 - valLoss: 0.5854909420013428 - trainLoss: 0.5785110592842102\n",
      "cnt: 0 - valLoss: 0.5854682326316833 - trainLoss: 0.5784904956817627\n",
      "cnt: 0 - valLoss: 0.5854454636573792 - trainLoss: 0.5784698724746704\n",
      "cnt: 0 - valLoss: 0.5854227542877197 - trainLoss: 0.5784493088722229\n",
      "cnt: 0 - valLoss: 0.5853999853134155 - trainLoss: 0.5784287452697754\n",
      "cnt: 0 - valLoss: 0.5853772759437561 - trainLoss: 0.5784080624580383\n",
      "cnt: 0 - valLoss: 0.5853545665740967 - trainLoss: 0.5783874988555908\n",
      "cnt: 0 - valLoss: 0.5853317975997925 - trainLoss: 0.5783669352531433\n",
      "cnt: 0 - valLoss: 0.5853090882301331 - trainLoss: 0.578346312046051\n",
      "cnt: 0 - valLoss: 0.5852862596511841 - trainLoss: 0.5783256888389587\n",
      "cnt: 0 - valLoss: 0.5852635502815247 - trainLoss: 0.5783051252365112\n",
      "cnt: 0 - valLoss: 0.5852407217025757 - trainLoss: 0.578284502029419\n",
      "cnt: 0 - valLoss: 0.5852180123329163 - trainLoss: 0.5782638788223267\n",
      "cnt: 0 - valLoss: 0.5851953029632568 - trainLoss: 0.5782431960105896\n",
      "cnt: 0 - valLoss: 0.5851724743843079 - trainLoss: 0.5782226324081421\n",
      "cnt: 0 - valLoss: 0.5851497650146484 - trainLoss: 0.5782020688056946\n",
      "cnt: 0 - valLoss: 0.5851268768310547 - trainLoss: 0.5781813859939575\n",
      "cnt: 0 - valLoss: 0.5851041674613953 - trainLoss: 0.5781607031822205\n",
      "cnt: 0 - valLoss: 0.5850813984870911 - trainLoss: 0.578140139579773\n",
      "cnt: 0 - valLoss: 0.5850586295127869 - trainLoss: 0.5781195163726807\n",
      "cnt: 0 - valLoss: 0.5850358009338379 - trainLoss: 0.5780988931655884\n",
      "cnt: 0 - valLoss: 0.5850130319595337 - trainLoss: 0.5780782699584961\n",
      "cnt: 0 - valLoss: 0.5849902629852295 - trainLoss: 0.578057587146759\n",
      "cnt: 0 - valLoss: 0.5849674344062805 - trainLoss: 0.578036904335022\n",
      "cnt: 0 - valLoss: 0.5849446654319763 - trainLoss: 0.5780162811279297\n",
      "cnt: 0 - valLoss: 0.5849218368530273 - trainLoss: 0.5779956579208374\n",
      "cnt: 0 - valLoss: 0.5848990678787231 - trainLoss: 0.5779750347137451\n",
      "cnt: 0 - valLoss: 0.5848762392997742 - trainLoss: 0.5779544115066528\n",
      "cnt: 0 - valLoss: 0.5848534107208252 - trainLoss: 0.5779337286949158\n",
      "cnt: 0 - valLoss: 0.5848305821418762 - trainLoss: 0.5779130458831787\n",
      "cnt: 0 - valLoss: 0.5848077535629272 - trainLoss: 0.5778924226760864\n",
      "cnt: 0 - valLoss: 0.5847849249839783 - trainLoss: 0.5778717398643494\n",
      "cnt: 0 - valLoss: 0.5847621560096741 - trainLoss: 0.5778510570526123\n",
      "cnt: 0 - valLoss: 0.5847393274307251 - trainLoss: 0.57783043384552\n",
      "cnt: 0 - valLoss: 0.5847164988517761 - trainLoss: 0.5778098106384277\n",
      "cnt: 0 - valLoss: 0.5846936702728271 - trainLoss: 0.5777890682220459\n",
      "cnt: 0 - valLoss: 0.5846708416938782 - trainLoss: 0.5777684450149536\n",
      "cnt: 0 - valLoss: 0.5846480131149292 - trainLoss: 0.5777477622032166\n",
      "cnt: 0 - valLoss: 0.5846251845359802 - trainLoss: 0.5777271389961243\n",
      "cnt: 0 - valLoss: 0.5846022963523865 - trainLoss: 0.5777063965797424\n",
      "cnt: 0 - valLoss: 0.5845794677734375 - trainLoss: 0.5776856541633606\n",
      "cnt: 0 - valLoss: 0.5845565795898438 - trainLoss: 0.5776650309562683\n",
      "cnt: 0 - valLoss: 0.5845337510108948 - trainLoss: 0.5776443481445312\n",
      "cnt: 0 - valLoss: 0.584510862827301 - trainLoss: 0.5776236653327942\n",
      "cnt: 0 - valLoss: 0.5844879746437073 - trainLoss: 0.5776029825210571\n",
      "cnt: 0 - valLoss: 0.5844652056694031 - trainLoss: 0.5775822997093201\n",
      "cnt: 0 - valLoss: 0.5844423174858093 - trainLoss: 0.577561616897583\n",
      "cnt: 0 - valLoss: 0.5844194293022156 - trainLoss: 0.577540934085846\n",
      "cnt: 0 - valLoss: 0.5843966007232666 - trainLoss: 0.5775201916694641\n",
      "cnt: 0 - valLoss: 0.5843737125396729 - trainLoss: 0.577499508857727\n",
      "cnt: 0 - valLoss: 0.5843508839607239 - trainLoss: 0.57747882604599\n",
      "cnt: 0 - valLoss: 0.5843279361724854 - trainLoss: 0.5774581432342529\n",
      "cnt: 0 - valLoss: 0.5843050479888916 - trainLoss: 0.5774374604225159\n",
      "cnt: 0 - valLoss: 0.5842821598052979 - trainLoss: 0.5774166584014893\n",
      "cnt: 0 - valLoss: 0.5842592716217041 - trainLoss: 0.5773959755897522\n",
      "cnt: 0 - valLoss: 0.5842363834381104 - trainLoss: 0.5773752927780151\n",
      "cnt: 0 - valLoss: 0.5842134952545166 - trainLoss: 0.5773545503616333\n",
      "cnt: 0 - valLoss: 0.5841906070709229 - trainLoss: 0.5773338675498962\n",
      "cnt: 0 - valLoss: 0.5841677188873291 - trainLoss: 0.5773131251335144\n",
      "cnt: 0 - valLoss: 0.5841448307037354 - trainLoss: 0.5772924423217773\n",
      "cnt: 0 - valLoss: 0.5841219425201416 - trainLoss: 0.5772716403007507\n",
      "cnt: 0 - valLoss: 0.5840989947319031 - trainLoss: 0.5772510170936584\n",
      "cnt: 0 - valLoss: 0.5840761065483093 - trainLoss: 0.5772302150726318\n",
      "cnt: 0 - valLoss: 0.5840532183647156 - trainLoss: 0.57720947265625\n",
      "cnt: 0 - valLoss: 0.584030270576477 - trainLoss: 0.5771887302398682\n",
      "cnt: 0 - valLoss: 0.5840073227882385 - trainLoss: 0.5771680474281311\n",
      "cnt: 0 - valLoss: 0.5839844346046448 - trainLoss: 0.5771473050117493\n",
      "cnt: 0 - valLoss: 0.5839614868164062 - trainLoss: 0.5771265625953674\n",
      "cnt: 0 - valLoss: 0.5839385390281677 - trainLoss: 0.5771058201789856\n",
      "cnt: 0 - valLoss: 0.583915650844574 - trainLoss: 0.5770850777626038\n",
      "cnt: 0 - valLoss: 0.5838927030563354 - trainLoss: 0.5770643949508667\n",
      "cnt: 0 - valLoss: 0.5838696956634521 - trainLoss: 0.5770435929298401\n",
      "cnt: 0 - valLoss: 0.5838468074798584 - trainLoss: 0.5770228505134583\n",
      "cnt: 0 - valLoss: 0.5838239192962646 - trainLoss: 0.5770021080970764\n",
      "cnt: 0 - valLoss: 0.5838009119033813 - trainLoss: 0.5769813656806946\n",
      "cnt: 0 - valLoss: 0.5837779641151428 - trainLoss: 0.576960563659668\n",
      "cnt: 0 - valLoss: 0.5837550163269043 - trainLoss: 0.5769398212432861\n",
      "cnt: 0 - valLoss: 0.5837321281433105 - trainLoss: 0.5769190788269043\n",
      "cnt: 0 - valLoss: 0.583709180355072 - trainLoss: 0.5768982768058777\n",
      "cnt: 0 - valLoss: 0.5836862921714783 - trainLoss: 0.5768775939941406\n",
      "cnt: 0 - valLoss: 0.5836634039878845 - trainLoss: 0.576856791973114\n",
      "cnt: 0 - valLoss: 0.5836405158042908 - trainLoss: 0.5768360495567322\n",
      "cnt: 0 - valLoss: 0.5836175680160522 - trainLoss: 0.5768153071403503\n",
      "cnt: 0 - valLoss: 0.5835946798324585 - trainLoss: 0.5767945647239685\n",
      "cnt: 0 - valLoss: 0.58357173204422 - trainLoss: 0.5767737627029419\n",
      "cnt: 0 - valLoss: 0.5835487842559814 - trainLoss: 0.5767530202865601\n",
      "cnt: 0 - valLoss: 0.5835258960723877 - trainLoss: 0.5767322778701782\n",
      "cnt: 0 - valLoss: 0.5835029482841492 - trainLoss: 0.5767114758491516\n",
      "cnt: 0 - valLoss: 0.5834800004959106 - trainLoss: 0.5766907334327698\n",
      "cnt: 0 - valLoss: 0.5834570527076721 - trainLoss: 0.5766699314117432\n",
      "cnt: 0 - valLoss: 0.5834341049194336 - trainLoss: 0.5766491293907166\n",
      "cnt: 0 - valLoss: 0.5834111571311951 - trainLoss: 0.5766283869743347\n",
      "cnt: 0 - valLoss: 0.5833882093429565 - trainLoss: 0.5766075849533081\n",
      "cnt: 0 - valLoss: 0.583365261554718 - trainLoss: 0.5765868425369263\n",
      "cnt: 0 - valLoss: 0.5833423137664795 - trainLoss: 0.5765661001205444\n",
      "cnt: 0 - valLoss: 0.5833193063735962 - trainLoss: 0.5765452980995178\n",
      "cnt: 0 - valLoss: 0.5832964181900024 - trainLoss: 0.5765244960784912\n",
      "cnt: 0 - valLoss: 0.5832734704017639 - trainLoss: 0.5765036940574646\n",
      "cnt: 0 - valLoss: 0.5832505226135254 - trainLoss: 0.576482892036438\n",
      "cnt: 0 - valLoss: 0.5832275152206421 - trainLoss: 0.5764621496200562\n",
      "cnt: 0 - valLoss: 0.5832045674324036 - trainLoss: 0.5764412879943848\n",
      "cnt: 0 - valLoss: 0.5831815600395203 - trainLoss: 0.5764205455780029\n",
      "cnt: 0 - valLoss: 0.5831586718559265 - trainLoss: 0.5763997435569763\n",
      "cnt: 0 - valLoss: 0.5831356048583984 - trainLoss: 0.5763789415359497\n",
      "cnt: 0 - valLoss: 0.5831126570701599 - trainLoss: 0.5763580799102783\n",
      "cnt: 0 - valLoss: 0.5830896496772766 - trainLoss: 0.5763373374938965\n",
      "cnt: 0 - valLoss: 0.5830666422843933 - trainLoss: 0.5763165354728699\n",
      "cnt: 0 - valLoss: 0.5830437541007996 - trainLoss: 0.5762956738471985\n",
      "cnt: 0 - valLoss: 0.5830206871032715 - trainLoss: 0.5762748718261719\n",
      "cnt: 0 - valLoss: 0.5829976797103882 - trainLoss: 0.5762540698051453\n",
      "cnt: 0 - valLoss: 0.5829747319221497 - trainLoss: 0.5762332677841187\n",
      "cnt: 0 - valLoss: 0.5829517245292664 - trainLoss: 0.5762124061584473\n",
      "cnt: 0 - valLoss: 0.5829287171363831 - trainLoss: 0.5761915445327759\n",
      "cnt: 0 - valLoss: 0.5829057097434998 - trainLoss: 0.5761707425117493\n",
      "cnt: 0 - valLoss: 0.5828827619552612 - trainLoss: 0.5761499404907227\n",
      "cnt: 0 - valLoss: 0.5828597545623779 - trainLoss: 0.576129138469696\n",
      "cnt: 0 - valLoss: 0.5828368067741394 - trainLoss: 0.5761083364486694\n",
      "cnt: 0 - valLoss: 0.5828137993812561 - trainLoss: 0.576087474822998\n",
      "cnt: 0 - valLoss: 0.582790732383728 - trainLoss: 0.5760666728019714\n",
      "cnt: 0 - valLoss: 0.5827677845954895 - trainLoss: 0.5760458707809448\n",
      "cnt: 0 - valLoss: 0.582744836807251 - trainLoss: 0.576025128364563\n",
      "cnt: 0 - valLoss: 0.5827218294143677 - trainLoss: 0.5760043263435364\n",
      "cnt: 0 - valLoss: 0.5826987624168396 - trainLoss: 0.5759835243225098\n",
      "cnt: 0 - valLoss: 0.5826758146286011 - trainLoss: 0.5759627223014832\n",
      "cnt: 0 - valLoss: 0.5826528072357178 - trainLoss: 0.5759419202804565\n",
      "cnt: 0 - valLoss: 0.5826297998428345 - trainLoss: 0.5759211182594299\n",
      "cnt: 0 - valLoss: 0.5826067924499512 - trainLoss: 0.5759003162384033\n",
      "cnt: 0 - valLoss: 0.5825837850570679 - trainLoss: 0.5758795142173767\n",
      "cnt: 0 - valLoss: 0.5825607776641846 - trainLoss: 0.5758587121963501\n",
      "cnt: 0 - valLoss: 0.5825377702713013 - trainLoss: 0.5758378505706787\n",
      "cnt: 0 - valLoss: 0.5825147032737732 - trainLoss: 0.5758170485496521\n",
      "cnt: 0 - valLoss: 0.5824916958808899 - trainLoss: 0.5757962465286255\n",
      "cnt: 0 - valLoss: 0.5824686884880066 - trainLoss: 0.5757754445075989\n",
      "cnt: 0 - valLoss: 0.5824456810951233 - trainLoss: 0.5757546424865723\n",
      "cnt: 0 - valLoss: 0.58242267370224 - trainLoss: 0.5757337808609009\n",
      "cnt: 0 - valLoss: 0.5823995471000671 - trainLoss: 0.5757129788398743\n",
      "cnt: 0 - valLoss: 0.5823765397071838 - trainLoss: 0.5756921768188477\n",
      "cnt: 0 - valLoss: 0.5823535323143005 - trainLoss: 0.5756713151931763\n",
      "cnt: 0 - valLoss: 0.5823304653167725 - trainLoss: 0.5756504535675049\n",
      "cnt: 0 - valLoss: 0.5823073387145996 - trainLoss: 0.5756296515464783\n",
      "cnt: 0 - valLoss: 0.5822843313217163 - trainLoss: 0.5756088495254517\n",
      "cnt: 0 - valLoss: 0.5822612643241882 - trainLoss: 0.5755879878997803\n",
      "cnt: 0 - valLoss: 0.5822382569313049 - trainLoss: 0.5755670666694641\n",
      "cnt: 0 - valLoss: 0.5822151899337769 - trainLoss: 0.5755462646484375\n",
      "cnt: 0 - valLoss: 0.5821921825408936 - trainLoss: 0.5755254626274109\n",
      "cnt: 0 - valLoss: 0.5821691155433655 - trainLoss: 0.5755046010017395\n",
      "cnt: 0 - valLoss: 0.5821460485458374 - trainLoss: 0.5754837989807129\n",
      "cnt: 0 - valLoss: 0.5821228623390198 - trainLoss: 0.5754629373550415\n",
      "cnt: 0 - valLoss: 0.5820998549461365 - trainLoss: 0.5754420757293701\n",
      "cnt: 0 - valLoss: 0.5820767879486084 - trainLoss: 0.5754212141036987\n",
      "cnt: 0 - valLoss: 0.5820537209510803 - trainLoss: 0.5754003524780273\n",
      "cnt: 0 - valLoss: 0.5820306539535522 - trainLoss: 0.575379490852356\n",
      "cnt: 0 - valLoss: 0.5820075869560242 - trainLoss: 0.5753586292266846\n",
      "cnt: 0 - valLoss: 0.5819844603538513 - trainLoss: 0.5753377676010132\n",
      "cnt: 0 - valLoss: 0.5819613933563232 - trainLoss: 0.5753169059753418\n",
      "cnt: 0 - valLoss: 0.5819382667541504 - trainLoss: 0.5752960443496704\n",
      "cnt: 0 - valLoss: 0.5819151997566223 - trainLoss: 0.575275182723999\n",
      "cnt: 0 - valLoss: 0.5818921327590942 - trainLoss: 0.5752543210983276\n",
      "cnt: 0 - valLoss: 0.5818690061569214 - trainLoss: 0.5752334594726562\n",
      "cnt: 0 - valLoss: 0.5818459391593933 - trainLoss: 0.5752125382423401\n",
      "cnt: 0 - valLoss: 0.5818227529525757 - trainLoss: 0.5751917362213135\n",
      "cnt: 0 - valLoss: 0.5817996859550476 - trainLoss: 0.5751708149909973\n",
      "cnt: 0 - valLoss: 0.5817765593528748 - trainLoss: 0.5751499533653259\n",
      "cnt: 0 - valLoss: 0.5817534923553467 - trainLoss: 0.5751290321350098\n",
      "cnt: 0 - valLoss: 0.5817303657531738 - trainLoss: 0.5751081705093384\n",
      "cnt: 0 - valLoss: 0.581707239151001 - trainLoss: 0.575087308883667\n",
      "cnt: 0 - valLoss: 0.5816841721534729 - trainLoss: 0.5750663876533508\n",
      "cnt: 0 - valLoss: 0.5816609859466553 - trainLoss: 0.5750455260276794\n",
      "cnt: 0 - valLoss: 0.5816379189491272 - trainLoss: 0.5750246047973633\n",
      "cnt: 0 - valLoss: 0.5816147327423096 - trainLoss: 0.5750037431716919\n",
      "cnt: 0 - valLoss: 0.5815916061401367 - trainLoss: 0.5749828219413757\n",
      "cnt: 0 - valLoss: 0.5815684795379639 - trainLoss: 0.5749619603157043\n",
      "cnt: 0 - valLoss: 0.581545352935791 - trainLoss: 0.574941098690033\n",
      "cnt: 0 - valLoss: 0.5815222263336182 - trainLoss: 0.5749201774597168\n",
      "cnt: 0 - valLoss: 0.5814990997314453 - trainLoss: 0.5748993158340454\n",
      "cnt: 0 - valLoss: 0.5814759135246277 - trainLoss: 0.5748783946037292\n",
      "cnt: 0 - valLoss: 0.5814527273178101 - trainLoss: 0.5748575329780579\n",
      "cnt: 0 - valLoss: 0.5814295411109924 - trainLoss: 0.5748365521430969\n",
      "cnt: 0 - valLoss: 0.5814064145088196 - trainLoss: 0.5748156905174255\n",
      "cnt: 0 - valLoss: 0.581383228302002 - trainLoss: 0.5747947692871094\n",
      "cnt: 0 - valLoss: 0.5813601016998291 - trainLoss: 0.574773907661438\n",
      "cnt: 0 - valLoss: 0.5813369154930115 - trainLoss: 0.5747529864311218\n",
      "cnt: 0 - valLoss: 0.5813137292861938 - trainLoss: 0.5747320652008057\n",
      "cnt: 0 - valLoss: 0.581290602684021 - trainLoss: 0.5747111439704895\n",
      "cnt: 0 - valLoss: 0.5812673568725586 - trainLoss: 0.5746902823448181\n",
      "cnt: 0 - valLoss: 0.5812441110610962 - trainLoss: 0.5746693015098572\n",
      "cnt: 0 - valLoss: 0.5812209248542786 - trainLoss: 0.574648380279541\n",
      "cnt: 0 - valLoss: 0.5811977386474609 - trainLoss: 0.5746274590492249\n",
      "cnt: 0 - valLoss: 0.5811745524406433 - trainLoss: 0.5746065378189087\n",
      "cnt: 0 - valLoss: 0.5811513066291809 - trainLoss: 0.5745856165885925\n",
      "cnt: 0 - valLoss: 0.5811280608177185 - trainLoss: 0.5745646953582764\n",
      "cnt: 0 - valLoss: 0.5811048150062561 - trainLoss: 0.5745437741279602\n",
      "cnt: 0 - valLoss: 0.5810816287994385 - trainLoss: 0.5745227932929993\n",
      "cnt: 0 - valLoss: 0.5810583829879761 - trainLoss: 0.5745019316673279\n",
      "cnt: 0 - valLoss: 0.5810350775718689 - trainLoss: 0.5744809508323669\n",
      "cnt: 0 - valLoss: 0.5810118317604065 - trainLoss: 0.574459969997406\n",
      "cnt: 0 - valLoss: 0.5809885859489441 - trainLoss: 0.5744389891624451\n",
      "cnt: 0 - valLoss: 0.5809653401374817 - trainLoss: 0.5744180083274841\n",
      "cnt: 0 - valLoss: 0.5809420347213745 - trainLoss: 0.574397087097168\n",
      "cnt: 0 - valLoss: 0.5809187889099121 - trainLoss: 0.5743761658668518\n",
      "cnt: 0 - valLoss: 0.5808956027030945 - trainLoss: 0.5743551850318909\n",
      "cnt: 0 - valLoss: 0.5808722972869873 - trainLoss: 0.5743342041969299\n",
      "cnt: 0 - valLoss: 0.5808490514755249 - trainLoss: 0.5743132829666138\n",
      "cnt: 0 - valLoss: 0.5808257460594177 - trainLoss: 0.5742923021316528\n",
      "cnt: 0 - valLoss: 0.5808025002479553 - trainLoss: 0.5742713212966919\n",
      "cnt: 0 - valLoss: 0.5807791948318481 - trainLoss: 0.574250340461731\n",
      "cnt: 0 - valLoss: 0.580755889415741 - trainLoss: 0.57422935962677\n",
      "cnt: 0 - valLoss: 0.5807326436042786 - trainLoss: 0.5742083787918091\n",
      "cnt: 0 - valLoss: 0.5807093381881714 - trainLoss: 0.5741873979568481\n",
      "cnt: 0 - valLoss: 0.580686092376709 - trainLoss: 0.5741664171218872\n",
      "cnt: 0 - valLoss: 0.5806627869606018 - trainLoss: 0.574145495891571\n",
      "cnt: 0 - valLoss: 0.5806394815444946 - trainLoss: 0.5741245150566101\n",
      "cnt: 0 - valLoss: 0.5806162357330322 - trainLoss: 0.5741034746170044\n",
      "cnt: 0 - valLoss: 0.5805928707122803 - trainLoss: 0.5740824937820435\n",
      "cnt: 0 - valLoss: 0.5805695652961731 - trainLoss: 0.5740615129470825\n",
      "cnt: 0 - valLoss: 0.5805462598800659 - trainLoss: 0.5740405321121216\n",
      "cnt: 0 - valLoss: 0.5805229544639587 - trainLoss: 0.5740195512771606\n",
      "cnt: 0 - valLoss: 0.5804996490478516 - trainLoss: 0.5739985108375549\n",
      "cnt: 0 - valLoss: 0.5804762840270996 - trainLoss: 0.573977530002594\n",
      "cnt: 0 - valLoss: 0.5804530382156372 - trainLoss: 0.5739565491676331\n",
      "cnt: 0 - valLoss: 0.5804296731948853 - trainLoss: 0.5739355087280273\n",
      "cnt: 0 - valLoss: 0.5804063677787781 - trainLoss: 0.5739145278930664\n",
      "cnt: 0 - valLoss: 0.5803830623626709 - trainLoss: 0.5738934874534607\n",
      "cnt: 0 - valLoss: 0.580359697341919 - trainLoss: 0.5738725066184998\n",
      "cnt: 0 - valLoss: 0.5803363919258118 - trainLoss: 0.5738515257835388\n",
      "cnt: 0 - valLoss: 0.5803130269050598 - trainLoss: 0.5738304853439331\n",
      "cnt: 0 - valLoss: 0.5802896618843079 - trainLoss: 0.5738095045089722\n",
      "cnt: 0 - valLoss: 0.5802663564682007 - trainLoss: 0.5737885236740112\n",
      "cnt: 0 - valLoss: 0.5802430510520935 - trainLoss: 0.5737674236297607\n",
      "cnt: 0 - valLoss: 0.5802198052406311 - trainLoss: 0.5737465023994446\n",
      "cnt: 0 - valLoss: 0.5801964402198792 - trainLoss: 0.5737254023551941\n",
      "cnt: 0 - valLoss: 0.580173134803772 - trainLoss: 0.5737044215202332\n",
      "cnt: 0 - valLoss: 0.5801498293876648 - trainLoss: 0.5736833810806274\n",
      "cnt: 0 - valLoss: 0.5801265239715576 - trainLoss: 0.5736624002456665\n",
      "cnt: 0 - valLoss: 0.5801031589508057 - trainLoss: 0.5736413598060608\n",
      "cnt: 0 - valLoss: 0.5800798535346985 - trainLoss: 0.5736203789710999\n",
      "cnt: 0 - valLoss: 0.5800565481185913 - trainLoss: 0.5735993385314941\n",
      "cnt: 0 - valLoss: 0.5800331830978394 - trainLoss: 0.5735782980918884\n",
      "cnt: 0 - valLoss: 0.5800098180770874 - trainLoss: 0.5735572576522827\n",
      "cnt: 0 - valLoss: 0.579986572265625 - trainLoss: 0.573536217212677\n",
      "cnt: 0 - valLoss: 0.5799631476402283 - trainLoss: 0.5735152363777161\n",
      "cnt: 0 - valLoss: 0.5799398422241211 - trainLoss: 0.5734941363334656\n",
      "cnt: 0 - valLoss: 0.5799164772033691 - trainLoss: 0.5734730958938599\n",
      "cnt: 0 - valLoss: 0.5798931121826172 - trainLoss: 0.5734521150588989\n",
      "cnt: 0 - valLoss: 0.5798697471618652 - trainLoss: 0.5734310150146484\n",
      "cnt: 0 - valLoss: 0.5798464417457581 - trainLoss: 0.5734100341796875\n",
      "cnt: 0 - valLoss: 0.5798230767250061 - trainLoss: 0.573388934135437\n",
      "cnt: 0 - valLoss: 0.5797997117042542 - trainLoss: 0.5733678936958313\n",
      "cnt: 0 - valLoss: 0.5797763466835022 - trainLoss: 0.5733468532562256\n",
      "cnt: 0 - valLoss: 0.5797529816627502 - trainLoss: 0.5733258128166199\n",
      "cnt: 0 - valLoss: 0.5797296762466431 - trainLoss: 0.5733047723770142\n",
      "cnt: 0 - valLoss: 0.5797062516212463 - trainLoss: 0.5732837319374084\n",
      "cnt: 0 - valLoss: 0.5796828269958496 - trainLoss: 0.573262631893158\n",
      "cnt: 0 - valLoss: 0.5796595215797424 - trainLoss: 0.573241651058197\n",
      "cnt: 0 - valLoss: 0.5796361565589905 - trainLoss: 0.5732205510139465\n",
      "cnt: 0 - valLoss: 0.5796127319335938 - trainLoss: 0.573199450969696\n",
      "cnt: 0 - valLoss: 0.5795893669128418 - trainLoss: 0.5731784105300903\n",
      "cnt: 0 - valLoss: 0.5795660018920898 - trainLoss: 0.5731573700904846\n",
      "cnt: 0 - valLoss: 0.5795426368713379 - trainLoss: 0.5731362700462341\n",
      "cnt: 0 - valLoss: 0.5795192122459412 - trainLoss: 0.5731152296066284\n",
      "cnt: 0 - valLoss: 0.5794957876205444 - trainLoss: 0.5730941891670227\n",
      "cnt: 0 - valLoss: 0.5794724225997925 - trainLoss: 0.5730730891227722\n",
      "cnt: 0 - valLoss: 0.5794489979743958 - trainLoss: 0.5730519890785217\n",
      "cnt: 0 - valLoss: 0.579425573348999 - trainLoss: 0.5730308890342712\n",
      "cnt: 0 - valLoss: 0.5794022083282471 - trainLoss: 0.5730098485946655\n",
      "cnt: 0 - valLoss: 0.5793787837028503 - trainLoss: 0.5729888081550598\n",
      "cnt: 0 - valLoss: 0.5793554186820984 - trainLoss: 0.5729676485061646\n",
      "cnt: 0 - valLoss: 0.5793319940567017 - trainLoss: 0.5729466080665588\n",
      "cnt: 0 - valLoss: 0.5793085098266602 - trainLoss: 0.5729255676269531\n",
      "cnt: 0 - valLoss: 0.5792850852012634 - trainLoss: 0.5729044079780579\n",
      "cnt: 0 - valLoss: 0.5792617201805115 - trainLoss: 0.5728833079338074\n",
      "cnt: 0 - valLoss: 0.5792382955551147 - trainLoss: 0.5728622078895569\n",
      "cnt: 0 - valLoss: 0.5792148113250732 - trainLoss: 0.5728411078453064\n",
      "cnt: 0 - valLoss: 0.5791913866996765 - trainLoss: 0.5728200078010559\n",
      "cnt: 0 - valLoss: 0.5791680216789246 - trainLoss: 0.5727989077568054\n",
      "cnt: 0 - valLoss: 0.5791445374488831 - trainLoss: 0.5727778077125549\n",
      "cnt: 0 - valLoss: 0.5791211128234863 - trainLoss: 0.5727567076683044\n",
      "cnt: 0 - valLoss: 0.5790976881980896 - trainLoss: 0.572735607624054\n",
      "cnt: 0 - valLoss: 0.5790742039680481 - trainLoss: 0.5727145671844482\n",
      "cnt: 0 - valLoss: 0.5790507793426514 - trainLoss: 0.5726934671401978\n",
      "cnt: 0 - valLoss: 0.5790273547172546 - trainLoss: 0.5726723074913025\n",
      "cnt: 0 - valLoss: 0.5790038108825684 - trainLoss: 0.572651207447052\n",
      "cnt: 0 - valLoss: 0.5789803862571716 - trainLoss: 0.5726301670074463\n",
      "cnt: 0 - valLoss: 0.5789569616317749 - trainLoss: 0.572609007358551\n",
      "cnt: 0 - valLoss: 0.5789334774017334 - trainLoss: 0.5725879073143005\n",
      "cnt: 0 - valLoss: 0.5789099931716919 - trainLoss: 0.5725667476654053\n",
      "cnt: 0 - valLoss: 0.5788865089416504 - trainLoss: 0.5725457072257996\n",
      "cnt: 0 - valLoss: 0.5788630247116089 - trainLoss: 0.5725245475769043\n",
      "cnt: 0 - valLoss: 0.5788396000862122 - trainLoss: 0.5725034475326538\n",
      "cnt: 0 - valLoss: 0.5788161158561707 - trainLoss: 0.5724823474884033\n",
      "cnt: 0 - valLoss: 0.5787926316261292 - trainLoss: 0.5724612474441528\n",
      "cnt: 0 - valLoss: 0.5787691473960876 - trainLoss: 0.5724401473999023\n",
      "cnt: 0 - valLoss: 0.5787456631660461 - trainLoss: 0.5724189281463623\n",
      "cnt: 0 - valLoss: 0.5787221789360046 - trainLoss: 0.572397768497467\n",
      "cnt: 0 - valLoss: 0.5786986947059631 - trainLoss: 0.5723766684532166\n",
      "cnt: 0 - valLoss: 0.5786751508712769 - trainLoss: 0.5723555088043213\n",
      "cnt: 0 - valLoss: 0.5786516666412354 - trainLoss: 0.572334349155426\n",
      "cnt: 0 - valLoss: 0.5786281824111938 - trainLoss: 0.5723131895065308\n",
      "cnt: 0 - valLoss: 0.5786046385765076 - trainLoss: 0.5722919702529907\n",
      "cnt: 0 - valLoss: 0.5785811543464661 - trainLoss: 0.5722708702087402\n",
      "cnt: 0 - valLoss: 0.5785576701164246 - trainLoss: 0.5722496509552002\n",
      "cnt: 0 - valLoss: 0.5785341858863831 - trainLoss: 0.5722284913063049\n",
      "cnt: 0 - valLoss: 0.5785107016563416 - trainLoss: 0.5722073316574097\n",
      "cnt: 0 - valLoss: 0.5784871578216553 - trainLoss: 0.5721861720085144\n",
      "cnt: 0 - valLoss: 0.5784636735916138 - trainLoss: 0.5721649527549744\n",
      "cnt: 0 - valLoss: 0.5784401893615723 - trainLoss: 0.5721437931060791\n",
      "cnt: 0 - valLoss: 0.5784167051315308 - trainLoss: 0.5721225738525391\n",
      "cnt: 0 - valLoss: 0.5783932209014893 - trainLoss: 0.5721014142036438\n",
      "cnt: 0 - valLoss: 0.5783697366714478 - trainLoss: 0.5720801949501038\n",
      "cnt: 0 - valLoss: 0.5783462524414062 - trainLoss: 0.5720590353012085\n",
      "cnt: 0 - valLoss: 0.57832270860672 - trainLoss: 0.5720378160476685\n",
      "cnt: 0 - valLoss: 0.5782991647720337 - trainLoss: 0.5720165967941284\n",
      "cnt: 0 - valLoss: 0.5782756805419922 - trainLoss: 0.5719954371452332\n",
      "cnt: 0 - valLoss: 0.5782521963119507 - trainLoss: 0.5719742178916931\n",
      "cnt: 0 - valLoss: 0.5782287120819092 - trainLoss: 0.5719530582427979\n",
      "cnt: 0 - valLoss: 0.5782051682472229 - trainLoss: 0.5719318389892578\n",
      "cnt: 0 - valLoss: 0.5781816840171814 - trainLoss: 0.5719106197357178\n",
      "cnt: 0 - valLoss: 0.5781581401824951 - trainLoss: 0.571889340877533\n",
      "cnt: 0 - valLoss: 0.5781347155570984 - trainLoss: 0.5718681812286377\n",
      "cnt: 0 - valLoss: 0.5781111121177673 - trainLoss: 0.5718469619750977\n",
      "cnt: 0 - valLoss: 0.5780876874923706 - trainLoss: 0.5718257427215576\n",
      "cnt: 0 - valLoss: 0.5780641436576843 - trainLoss: 0.5718044638633728\n",
      "cnt: 0 - valLoss: 0.5780406594276428 - trainLoss: 0.5717833042144775\n",
      "cnt: 0 - valLoss: 0.5780171155929565 - trainLoss: 0.5717620849609375\n",
      "cnt: 0 - valLoss: 0.5779935717582703 - trainLoss: 0.5717408061027527\n",
      "cnt: 0 - valLoss: 0.5779700875282288 - trainLoss: 0.5717195868492126\n",
      "cnt: 0 - valLoss: 0.5779466032981873 - trainLoss: 0.5716983079910278\n",
      "cnt: 0 - valLoss: 0.577923059463501 - trainLoss: 0.5716770887374878\n",
      "cnt: 0 - valLoss: 0.5778995752334595 - trainLoss: 0.5716558694839478\n",
      "cnt: 0 - valLoss: 0.5778760313987732 - trainLoss: 0.5716346502304077\n",
      "cnt: 0 - valLoss: 0.5778524875640869 - trainLoss: 0.5716134309768677\n",
      "cnt: 0 - valLoss: 0.5778290033340454 - trainLoss: 0.5715922117233276\n",
      "cnt: 0 - valLoss: 0.5778053998947144 - trainLoss: 0.5715709328651428\n",
      "cnt: 0 - valLoss: 0.5777819156646729 - trainLoss: 0.5715497136116028\n",
      "cnt: 0 - valLoss: 0.5777583718299866 - trainLoss: 0.571528434753418\n",
      "cnt: 0 - valLoss: 0.5777348279953003 - trainLoss: 0.5715071558952332\n",
      "cnt: 0 - valLoss: 0.577711284160614 - trainLoss: 0.5714859366416931\n",
      "cnt: 0 - valLoss: 0.5776877403259277 - trainLoss: 0.5714646577835083\n",
      "cnt: 0 - valLoss: 0.5776642560958862 - trainLoss: 0.5714433789253235\n",
      "cnt: 0 - valLoss: 0.5776406526565552 - trainLoss: 0.5714221596717834\n",
      "cnt: 0 - valLoss: 0.5776171088218689 - trainLoss: 0.5714008808135986\n",
      "cnt: 0 - valLoss: 0.5775935053825378 - trainLoss: 0.5713796019554138\n",
      "cnt: 0 - valLoss: 0.5775700211524963 - trainLoss: 0.5713583827018738\n",
      "cnt: 0 - valLoss: 0.5775464773178101 - trainLoss: 0.571337103843689\n",
      "cnt: 0 - valLoss: 0.577522873878479 - trainLoss: 0.5713158845901489\n",
      "cnt: 0 - valLoss: 0.5774993300437927 - trainLoss: 0.5712945461273193\n",
      "cnt: 0 - valLoss: 0.5774757266044617 - trainLoss: 0.5712733268737793\n",
      "cnt: 0 - valLoss: 0.5774521827697754 - trainLoss: 0.5712520480155945\n",
      "cnt: 0 - valLoss: 0.5774285793304443 - trainLoss: 0.5712307691574097\n",
      "cnt: 0 - valLoss: 0.5774050354957581 - trainLoss: 0.5712094902992249\n",
      "cnt: 0 - valLoss: 0.577381432056427 - trainLoss: 0.57118821144104\n",
      "cnt: 0 - valLoss: 0.5773578882217407 - trainLoss: 0.5711668729782104\n",
      "cnt: 0 - valLoss: 0.5773342847824097 - trainLoss: 0.5711455941200256\n",
      "cnt: 0 - valLoss: 0.5773107409477234 - trainLoss: 0.571124255657196\n",
      "cnt: 0 - valLoss: 0.5772871375083923 - trainLoss: 0.5711029767990112\n",
      "cnt: 0 - valLoss: 0.577263593673706 - trainLoss: 0.5710816383361816\n",
      "cnt: 0 - valLoss: 0.5772400498390198 - trainLoss: 0.5710603594779968\n",
      "cnt: 0 - valLoss: 0.577216386795044 - trainLoss: 0.5710389614105225\n",
      "cnt: 0 - valLoss: 0.5771929025650024 - trainLoss: 0.5710176825523376\n",
      "cnt: 0 - valLoss: 0.5771692991256714 - trainLoss: 0.5709962844848633\n",
      "cnt: 0 - valLoss: 0.5771457552909851 - trainLoss: 0.5709750056266785\n",
      "cnt: 0 - valLoss: 0.577122151851654 - trainLoss: 0.5709536671638489\n",
      "cnt: 0 - valLoss: 0.577098548412323 - trainLoss: 0.5709323287010193\n",
      "cnt: 0 - valLoss: 0.5770749449729919 - trainLoss: 0.5709109902381897\n",
      "cnt: 0 - valLoss: 0.5770514011383057 - trainLoss: 0.5708896517753601\n",
      "cnt: 0 - valLoss: 0.5770277976989746 - trainLoss: 0.5708683133125305\n",
      "cnt: 0 - valLoss: 0.5770041942596436 - trainLoss: 0.5708469748497009\n",
      "cnt: 0 - valLoss: 0.5769805908203125 - trainLoss: 0.5708255767822266\n",
      "cnt: 0 - valLoss: 0.5769569873809814 - trainLoss: 0.5708042979240417\n",
      "cnt: 0 - valLoss: 0.5769333839416504 - trainLoss: 0.5707829594612122\n",
      "cnt: 0 - valLoss: 0.5769097805023193 - trainLoss: 0.5707616209983826\n",
      "cnt: 0 - valLoss: 0.5768862366676331 - trainLoss: 0.570740282535553\n",
      "cnt: 0 - valLoss: 0.576862633228302 - trainLoss: 0.5707189440727234\n",
      "cnt: 0 - valLoss: 0.5768389701843262 - trainLoss: 0.570697546005249\n",
      "cnt: 0 - valLoss: 0.5768153667449951 - trainLoss: 0.5706762075424194\n",
      "cnt: 0 - valLoss: 0.5767917633056641 - trainLoss: 0.5706548094749451\n",
      "cnt: 0 - valLoss: 0.5767681002616882 - trainLoss: 0.5706335306167603\n",
      "cnt: 0 - valLoss: 0.5767444968223572 - trainLoss: 0.5706121325492859\n",
      "cnt: 0 - valLoss: 0.5767208933830261 - trainLoss: 0.5705907940864563\n",
      "cnt: 0 - valLoss: 0.5766972899436951 - trainLoss: 0.5705694556236267\n",
      "cnt: 0 - valLoss: 0.5766736268997192 - trainLoss: 0.5705480575561523\n",
      "cnt: 0 - valLoss: 0.5766500234603882 - trainLoss: 0.570526659488678\n",
      "cnt: 0 - valLoss: 0.5766263604164124 - trainLoss: 0.5705053210258484\n",
      "cnt: 0 - valLoss: 0.5766026973724365 - trainLoss: 0.5704839825630188\n",
      "cnt: 0 - valLoss: 0.5765790939331055 - trainLoss: 0.5704625844955444\n",
      "cnt: 0 - valLoss: 0.5765554904937744 - trainLoss: 0.5704411864280701\n",
      "cnt: 0 - valLoss: 0.5765318274497986 - trainLoss: 0.5704198479652405\n",
      "cnt: 0 - valLoss: 0.5765081644058228 - trainLoss: 0.5703984498977661\n",
      "cnt: 0 - valLoss: 0.5764845013618469 - trainLoss: 0.5703771114349365\n",
      "cnt: 0 - valLoss: 0.5764609575271606 - trainLoss: 0.5703557133674622\n",
      "cnt: 0 - valLoss: 0.5764372944831848 - trainLoss: 0.5703343152999878\n",
      "cnt: 0 - valLoss: 0.5764135718345642 - trainLoss: 0.5703129172325134\n",
      "cnt: 0 - valLoss: 0.5763899087905884 - trainLoss: 0.5702915787696838\n",
      "cnt: 0 - valLoss: 0.5763662457466125 - trainLoss: 0.5702701807022095\n",
      "cnt: 0 - valLoss: 0.5763425827026367 - trainLoss: 0.5702487826347351\n",
      "cnt: 0 - valLoss: 0.5763189196586609 - trainLoss: 0.5702273845672607\n",
      "cnt: 0 - valLoss: 0.5762952566146851 - trainLoss: 0.5702059864997864\n",
      "cnt: 0 - valLoss: 0.5762715935707092 - trainLoss: 0.5701845288276672\n",
      "cnt: 0 - valLoss: 0.5762478709220886 - trainLoss: 0.5701631903648376\n",
      "cnt: 0 - valLoss: 0.5762242078781128 - trainLoss: 0.5701417326927185\n",
      "cnt: 0 - valLoss: 0.5762004852294922 - trainLoss: 0.5701203346252441\n",
      "cnt: 0 - valLoss: 0.5761768221855164 - trainLoss: 0.5700989365577698\n",
      "cnt: 0 - valLoss: 0.5761530995368958 - trainLoss: 0.5700774788856506\n",
      "cnt: 0 - valLoss: 0.5761293768882751 - trainLoss: 0.5700560212135315\n",
      "cnt: 0 - valLoss: 0.5761057138442993 - trainLoss: 0.5700346231460571\n",
      "cnt: 0 - valLoss: 0.5760820508003235 - trainLoss: 0.570013165473938\n",
      "cnt: 0 - valLoss: 0.5760582685470581 - trainLoss: 0.5699917674064636\n",
      "cnt: 0 - valLoss: 0.5760346055030823 - trainLoss: 0.5699703693389893\n",
      "cnt: 0 - valLoss: 0.5760108828544617 - trainLoss: 0.5699489712715149\n",
      "cnt: 0 - valLoss: 0.5759871602058411 - trainLoss: 0.569927453994751\n",
      "cnt: 0 - valLoss: 0.5759634971618652 - trainLoss: 0.5699060559272766\n",
      "cnt: 0 - valLoss: 0.5759397149085999 - trainLoss: 0.5698845982551575\n",
      "cnt: 0 - valLoss: 0.5759159922599792 - trainLoss: 0.5698631405830383\n",
      "cnt: 0 - valLoss: 0.5758922696113586 - trainLoss: 0.569841742515564\n",
      "cnt: 0 - valLoss: 0.5758686065673828 - trainLoss: 0.5698202848434448\n",
      "cnt: 0 - valLoss: 0.5758448243141174 - trainLoss: 0.5697988867759705\n",
      "cnt: 0 - valLoss: 0.5758211016654968 - trainLoss: 0.5697773694992065\n",
      "cnt: 0 - valLoss: 0.5757973194122314 - trainLoss: 0.5697559714317322\n",
      "cnt: 0 - valLoss: 0.5757735967636108 - trainLoss: 0.569734513759613\n",
      "cnt: 0 - valLoss: 0.5757498741149902 - trainLoss: 0.5697130560874939\n",
      "cnt: 0 - valLoss: 0.5757260918617249 - trainLoss: 0.5696915984153748\n",
      "cnt: 0 - valLoss: 0.575702428817749 - trainLoss: 0.5696701407432556\n",
      "cnt: 0 - valLoss: 0.5756786465644836 - trainLoss: 0.5696487426757812\n",
      "cnt: 0 - valLoss: 0.575654923915863 - trainLoss: 0.5696272253990173\n",
      "cnt: 0 - valLoss: 0.5756311416625977 - trainLoss: 0.569605827331543\n",
      "cnt: 0 - valLoss: 0.575607419013977 - trainLoss: 0.5695843696594238\n",
      "cnt: 0 - valLoss: 0.5755836367607117 - trainLoss: 0.5695628523826599\n",
      "cnt: 0 - valLoss: 0.5755599141120911 - trainLoss: 0.5695414543151855\n",
      "cnt: 0 - valLoss: 0.5755361914634705 - trainLoss: 0.5695199966430664\n",
      "cnt: 0 - valLoss: 0.5755124092102051 - trainLoss: 0.5694985389709473\n",
      "cnt: 0 - valLoss: 0.5754886865615845 - trainLoss: 0.5694770216941833\n",
      "cnt: 0 - valLoss: 0.5754648447036743 - trainLoss: 0.5694555044174194\n",
      "cnt: 0 - valLoss: 0.5754410624504089 - trainLoss: 0.5694341063499451\n",
      "cnt: 0 - valLoss: 0.5754173398017883 - trainLoss: 0.5694126486778259\n",
      "cnt: 0 - valLoss: 0.5753936171531677 - trainLoss: 0.569391131401062\n",
      "cnt: 0 - valLoss: 0.5753697752952576 - trainLoss: 0.5693696737289429\n",
      "cnt: 0 - valLoss: 0.5753459930419922 - trainLoss: 0.569348156452179\n",
      "cnt: 0 - valLoss: 0.5753222107887268 - trainLoss: 0.5693266987800598\n",
      "cnt: 0 - valLoss: 0.5752984285354614 - trainLoss: 0.5693051815032959\n",
      "cnt: 0 - valLoss: 0.5752745866775513 - trainLoss: 0.5692837238311768\n",
      "cnt: 0 - valLoss: 0.5752508640289307 - trainLoss: 0.5692622065544128\n",
      "cnt: 0 - valLoss: 0.5752271413803101 - trainLoss: 0.5692407488822937\n",
      "cnt: 0 - valLoss: 0.5752032995223999 - trainLoss: 0.5692192316055298\n",
      "cnt: 0 - valLoss: 0.5751795172691345 - trainLoss: 0.5691977739334106\n",
      "cnt: 0 - valLoss: 0.5751557350158691 - trainLoss: 0.5691763162612915\n",
      "cnt: 0 - valLoss: 0.575131893157959 - trainLoss: 0.5691547989845276\n",
      "cnt: 0 - valLoss: 0.5751081109046936 - trainLoss: 0.5691333413124084\n",
      "cnt: 0 - valLoss: 0.5750843286514282 - trainLoss: 0.5691118240356445\n",
      "cnt: 0 - valLoss: 0.5750604867935181 - trainLoss: 0.5690903663635254\n",
      "cnt: 0 - valLoss: 0.5750367045402527 - trainLoss: 0.5690688490867615\n",
      "cnt: 0 - valLoss: 0.5750129222869873 - trainLoss: 0.5690473318099976\n",
      "cnt: 0 - valLoss: 0.5749890804290771 - trainLoss: 0.5690258145332336\n",
      "cnt: 0 - valLoss: 0.5749652981758118 - trainLoss: 0.5690042972564697\n",
      "cnt: 0 - valLoss: 0.5749413967132568 - trainLoss: 0.5689828395843506\n",
      "cnt: 0 - valLoss: 0.5749176144599915 - trainLoss: 0.5689612627029419\n",
      "cnt: 0 - valLoss: 0.5748937726020813 - trainLoss: 0.568939745426178\n",
      "cnt: 0 - valLoss: 0.5748699307441711 - trainLoss: 0.5689182877540588\n",
      "cnt: 0 - valLoss: 0.5748461484909058 - trainLoss: 0.5688967704772949\n",
      "cnt: 0 - valLoss: 0.5748223066329956 - trainLoss: 0.5688751935958862\n",
      "cnt: 0 - valLoss: 0.5747984647750854 - trainLoss: 0.5688537359237671\n",
      "cnt: 0 - valLoss: 0.5747746825218201 - trainLoss: 0.5688322186470032\n",
      "cnt: 0 - valLoss: 0.5747507810592651 - trainLoss: 0.5688107013702393\n",
      "cnt: 0 - valLoss: 0.574726939201355 - trainLoss: 0.5687891244888306\n",
      "cnt: 0 - valLoss: 0.5747030973434448 - trainLoss: 0.5687676072120667\n",
      "cnt: 0 - valLoss: 0.5746792554855347 - trainLoss: 0.5687461495399475\n",
      "cnt: 0 - valLoss: 0.5746554136276245 - trainLoss: 0.5687246322631836\n",
      "cnt: 0 - valLoss: 0.5746315717697144 - trainLoss: 0.5687030553817749\n",
      "cnt: 0 - valLoss: 0.574607789516449 - trainLoss: 0.568681538105011\n",
      "cnt: 0 - valLoss: 0.574583888053894 - trainLoss: 0.5686600208282471\n",
      "cnt: 0 - valLoss: 0.5745599865913391 - trainLoss: 0.5686385035514832\n",
      "cnt: 0 - valLoss: 0.5745362043380737 - trainLoss: 0.5686169266700745\n",
      "cnt: 0 - valLoss: 0.5745123624801636 - trainLoss: 0.5685954093933105\n",
      "cnt: 0 - valLoss: 0.5744884610176086 - trainLoss: 0.5685738921165466\n",
      "cnt: 0 - valLoss: 0.5744646191596985 - trainLoss: 0.5685523152351379\n",
      "cnt: 0 - valLoss: 0.5744407176971436 - trainLoss: 0.568530797958374\n",
      "cnt: 0 - valLoss: 0.5744168758392334 - trainLoss: 0.5685092210769653\n",
      "cnt: 0 - valLoss: 0.5743929743766785 - trainLoss: 0.5684877038002014\n",
      "cnt: 0 - valLoss: 0.5743691325187683 - trainLoss: 0.5684661865234375\n",
      "cnt: 0 - valLoss: 0.5743452310562134 - trainLoss: 0.568444550037384\n",
      "cnt: 0 - valLoss: 0.5743213891983032 - trainLoss: 0.5684230923652649\n",
      "cnt: 0 - valLoss: 0.5742974281311035 - trainLoss: 0.5684015154838562\n",
      "cnt: 0 - valLoss: 0.5742735862731934 - trainLoss: 0.5683799386024475\n",
      "cnt: 0 - valLoss: 0.5742496848106384 - trainLoss: 0.5683583617210388\n",
      "cnt: 0 - valLoss: 0.5742258429527283 - trainLoss: 0.5683368444442749\n",
      "cnt: 0 - valLoss: 0.5742019414901733 - trainLoss: 0.5683152675628662\n",
      "cnt: 0 - valLoss: 0.5741780400276184 - trainLoss: 0.5682936906814575\n",
      "cnt: 0 - valLoss: 0.5741541385650635 - trainLoss: 0.5682721734046936\n",
      "cnt: 0 - valLoss: 0.5741302371025085 - trainLoss: 0.5682505965232849\n",
      "cnt: 0 - valLoss: 0.5741063952445984 - trainLoss: 0.5682290196418762\n",
      "cnt: 0 - valLoss: 0.5740824341773987 - trainLoss: 0.5682075023651123\n",
      "cnt: 0 - valLoss: 0.5740585923194885 - trainLoss: 0.5681859254837036\n",
      "cnt: 0 - valLoss: 0.5740346908569336 - trainLoss: 0.5681643486022949\n",
      "cnt: 0 - valLoss: 0.5740107297897339 - trainLoss: 0.5681427717208862\n",
      "cnt: 0 - valLoss: 0.5739867687225342 - trainLoss: 0.5681211948394775\n",
      "cnt: 0 - valLoss: 0.573962926864624 - trainLoss: 0.5680996179580688\n",
      "cnt: 0 - valLoss: 0.5739390254020691 - trainLoss: 0.5680781006813049\n",
      "cnt: 0 - valLoss: 0.5739150643348694 - trainLoss: 0.5680564641952515\n",
      "cnt: 0 - valLoss: 0.5738911628723145 - trainLoss: 0.5680349469184875\n",
      "cnt: 0 - valLoss: 0.5738672614097595 - trainLoss: 0.5680133104324341\n",
      "cnt: 0 - valLoss: 0.5738433003425598 - trainLoss: 0.5679917335510254\n",
      "cnt: 0 - valLoss: 0.5738193392753601 - trainLoss: 0.5679701566696167\n",
      "cnt: 0 - valLoss: 0.5737954378128052 - trainLoss: 0.5679484605789185\n",
      "cnt: 0 - valLoss: 0.5737715363502502 - trainLoss: 0.5679268836975098\n",
      "cnt: 0 - valLoss: 0.5737475752830505 - trainLoss: 0.5679053068161011\n",
      "cnt: 0 - valLoss: 0.5737236738204956 - trainLoss: 0.5678836703300476\n",
      "cnt: 0 - valLoss: 0.5736997127532959 - trainLoss: 0.5678620338439941\n",
      "cnt: 0 - valLoss: 0.5736757516860962 - trainLoss: 0.5678405165672302\n",
      "cnt: 0 - valLoss: 0.5736518502235413 - trainLoss: 0.5678189396858215\n",
      "cnt: 0 - valLoss: 0.5736278891563416 - trainLoss: 0.5677973628044128\n",
      "cnt: 0 - valLoss: 0.5736039876937866 - trainLoss: 0.5677758455276489\n",
      "cnt: 0 - valLoss: 0.5735800266265869 - trainLoss: 0.5677542686462402\n",
      "cnt: 0 - valLoss: 0.5735560655593872 - trainLoss: 0.5677326917648315\n",
      "cnt: 0 - valLoss: 0.5735321044921875 - trainLoss: 0.5677111148834229\n",
      "cnt: 0 - valLoss: 0.5735082030296326 - trainLoss: 0.5676895380020142\n",
      "cnt: 0 - valLoss: 0.5734841823577881 - trainLoss: 0.5676678419113159\n",
      "cnt: 0 - valLoss: 0.5734602808952332 - trainLoss: 0.567646324634552\n",
      "cnt: 0 - valLoss: 0.5734362602233887 - trainLoss: 0.5676246285438538\n",
      "cnt: 0 - valLoss: 0.5734122395515442 - trainLoss: 0.5676029920578003\n",
      "cnt: 0 - valLoss: 0.5733883380889893 - trainLoss: 0.5675814151763916\n",
      "cnt: 0 - valLoss: 0.5733643174171448 - trainLoss: 0.5675597786903381\n",
      "cnt: 0 - valLoss: 0.5733404159545898 - trainLoss: 0.5675380825996399\n",
      "cnt: 0 - valLoss: 0.5733163952827454 - trainLoss: 0.5675165057182312\n",
      "cnt: 0 - valLoss: 0.5732924342155457 - trainLoss: 0.5674948692321777\n",
      "cnt: 0 - valLoss: 0.5732684135437012 - trainLoss: 0.5674732327461243\n",
      "cnt: 0 - valLoss: 0.5732445120811462 - trainLoss: 0.5674516558647156\n",
      "cnt: 0 - valLoss: 0.5732204914093018 - trainLoss: 0.5674299597740173\n",
      "cnt: 0 - valLoss: 0.5731964707374573 - trainLoss: 0.5674083828926086\n",
      "cnt: 0 - valLoss: 0.5731725692749023 - trainLoss: 0.5673866868019104\n",
      "cnt: 0 - valLoss: 0.5731485486030579 - trainLoss: 0.5673651099205017\n",
      "cnt: 0 - valLoss: 0.5731245875358582 - trainLoss: 0.5673434138298035\n",
      "cnt: 0 - valLoss: 0.5731005668640137 - trainLoss: 0.56732177734375\n",
      "cnt: 0 - valLoss: 0.5730765461921692 - trainLoss: 0.5673001408576965\n",
      "cnt: 0 - valLoss: 0.5730525851249695 - trainLoss: 0.5672784447669983\n",
      "cnt: 0 - valLoss: 0.573028564453125 - trainLoss: 0.5672568082809448\n",
      "cnt: 0 - valLoss: 0.5730045437812805 - trainLoss: 0.5672351717948914\n",
      "cnt: 0 - valLoss: 0.5729805827140808 - trainLoss: 0.5672135353088379\n",
      "cnt: 0 - valLoss: 0.5729565620422363 - trainLoss: 0.5671918988227844\n",
      "cnt: 0 - valLoss: 0.5729324817657471 - trainLoss: 0.567170262336731\n",
      "cnt: 0 - valLoss: 0.5729084610939026 - trainLoss: 0.5671485662460327\n",
      "cnt: 0 - valLoss: 0.5728845000267029 - trainLoss: 0.5671269297599792\n",
      "cnt: 0 - valLoss: 0.5728604793548584 - trainLoss: 0.5671052932739258\n",
      "cnt: 0 - valLoss: 0.5728364586830139 - trainLoss: 0.5670835971832275\n",
      "cnt: 0 - valLoss: 0.5728124380111694 - trainLoss: 0.5670619010925293\n",
      "cnt: 0 - valLoss: 0.572788417339325 - trainLoss: 0.5670402646064758\n",
      "cnt: 0 - valLoss: 0.5727643370628357 - trainLoss: 0.5670185685157776\n",
      "cnt: 0 - valLoss: 0.572740375995636 - trainLoss: 0.5669969320297241\n",
      "cnt: 0 - valLoss: 0.5727163553237915 - trainLoss: 0.5669752955436707\n",
      "cnt: 0 - valLoss: 0.572692334651947 - trainLoss: 0.5669535994529724\n",
      "cnt: 0 - valLoss: 0.5726682543754578 - trainLoss: 0.566931962966919\n",
      "cnt: 0 - valLoss: 0.5726442337036133 - trainLoss: 0.5669102668762207\n",
      "cnt: 0 - valLoss: 0.5726202130317688 - trainLoss: 0.5668886303901672\n",
      "cnt: 0 - valLoss: 0.5725961327552795 - trainLoss: 0.566866934299469\n",
      "cnt: 0 - valLoss: 0.5725721120834351 - trainLoss: 0.5668452382087708\n",
      "cnt: 0 - valLoss: 0.5725480318069458 - trainLoss: 0.5668235421180725\n",
      "cnt: 0 - valLoss: 0.5725240111351013 - trainLoss: 0.5668018460273743\n",
      "cnt: 0 - valLoss: 0.5724999904632568 - trainLoss: 0.5667802095413208\n",
      "cnt: 0 - valLoss: 0.5724759697914124 - trainLoss: 0.5667585134506226\n",
      "cnt: 0 - valLoss: 0.5724518299102783 - trainLoss: 0.5667368173599243\n",
      "cnt: 0 - valLoss: 0.5724278092384338 - trainLoss: 0.5667151212692261\n",
      "cnt: 0 - valLoss: 0.5724037885665894 - trainLoss: 0.5666934251785278\n",
      "cnt: 0 - valLoss: 0.5723797082901001 - trainLoss: 0.5666717290878296\n",
      "cnt: 0 - valLoss: 0.5723556280136108 - trainLoss: 0.5666500329971313\n",
      "cnt: 0 - valLoss: 0.5723315477371216 - trainLoss: 0.5666283965110779\n",
      "cnt: 0 - valLoss: 0.5723074674606323 - trainLoss: 0.5666067004203796\n",
      "cnt: 0 - valLoss: 0.5722834467887878 - trainLoss: 0.5665850043296814\n",
      "cnt: 0 - valLoss: 0.5722593665122986 - trainLoss: 0.5665633082389832\n",
      "cnt: 0 - valLoss: 0.5722353458404541 - trainLoss: 0.5665416121482849\n",
      "cnt: 0 - valLoss: 0.5722112059593201 - trainLoss: 0.5665199160575867\n",
      "cnt: 0 - valLoss: 0.5721871256828308 - trainLoss: 0.5664981603622437\n",
      "cnt: 0 - valLoss: 0.5721630454063416 - trainLoss: 0.5664765238761902\n",
      "cnt: 0 - valLoss: 0.5721389651298523 - trainLoss: 0.5664547681808472\n",
      "cnt: 0 - valLoss: 0.5721149444580078 - trainLoss: 0.5664330720901489\n",
      "cnt: 0 - valLoss: 0.5720908641815186 - trainLoss: 0.5664113759994507\n",
      "cnt: 0 - valLoss: 0.5720667243003845 - trainLoss: 0.5663896799087524\n",
      "cnt: 0 - valLoss: 0.5720426440238953 - trainLoss: 0.5663679838180542\n",
      "cnt: 0 - valLoss: 0.5720185041427612 - trainLoss: 0.566346287727356\n",
      "cnt: 0 - valLoss: 0.5719944834709167 - trainLoss: 0.5663245320320129\n",
      "cnt: 0 - valLoss: 0.5719704031944275 - trainLoss: 0.5663028359413147\n",
      "cnt: 0 - valLoss: 0.5719462633132935 - trainLoss: 0.5662811398506165\n",
      "cnt: 0 - valLoss: 0.5719221234321594 - trainLoss: 0.5662594437599182\n",
      "cnt: 0 - valLoss: 0.5718980431556702 - trainLoss: 0.5662376880645752\n",
      "cnt: 0 - valLoss: 0.5718739628791809 - trainLoss: 0.5662159323692322\n",
      "cnt: 0 - valLoss: 0.5718498229980469 - trainLoss: 0.5661942362785339\n",
      "cnt: 0 - valLoss: 0.5718257427215576 - trainLoss: 0.5661725401878357\n",
      "cnt: 0 - valLoss: 0.5718016028404236 - trainLoss: 0.5661507844924927\n",
      "cnt: 0 - valLoss: 0.5717775225639343 - trainLoss: 0.5661290884017944\n",
      "cnt: 0 - valLoss: 0.5717534422874451 - trainLoss: 0.5661073327064514\n",
      "cnt: 0 - valLoss: 0.571729302406311 - trainLoss: 0.5660856366157532\n",
      "cnt: 0 - valLoss: 0.571705162525177 - trainLoss: 0.5660639405250549\n",
      "cnt: 0 - valLoss: 0.5716810822486877 - trainLoss: 0.5660421848297119\n",
      "cnt: 0 - valLoss: 0.5716568827629089 - trainLoss: 0.5660204291343689\n",
      "cnt: 0 - valLoss: 0.5716328024864197 - trainLoss: 0.5659986734390259\n",
      "cnt: 0 - valLoss: 0.5716086626052856 - trainLoss: 0.5659769773483276\n",
      "cnt: 0 - valLoss: 0.5715845227241516 - trainLoss: 0.5659552216529846\n",
      "cnt: 0 - valLoss: 0.5715603828430176 - trainLoss: 0.5659334659576416\n",
      "cnt: 0 - valLoss: 0.5715363025665283 - trainLoss: 0.5659117698669434\n",
      "cnt: 0 - valLoss: 0.5715121626853943 - trainLoss: 0.5658899545669556\n",
      "cnt: 0 - valLoss: 0.5714879631996155 - trainLoss: 0.5658683180809021\n",
      "cnt: 0 - valLoss: 0.5714638233184814 - trainLoss: 0.5658465027809143\n",
      "cnt: 0 - valLoss: 0.5714397430419922 - trainLoss: 0.5658248066902161\n",
      "cnt: 0 - valLoss: 0.5714156031608582 - trainLoss: 0.5658031105995178\n",
      "cnt: 0 - valLoss: 0.5713915228843689 - trainLoss: 0.5657814145088196\n",
      "cnt: 0 - valLoss: 0.5713673830032349 - trainLoss: 0.5657596588134766\n",
      "cnt: 0 - valLoss: 0.571343183517456 - trainLoss: 0.5657379627227783\n",
      "cnt: 0 - valLoss: 0.571319043636322 - trainLoss: 0.5657162666320801\n",
      "cnt: 0 - valLoss: 0.5712949633598328 - trainLoss: 0.5656945705413818\n",
      "cnt: 0 - valLoss: 0.5712708234786987 - trainLoss: 0.5656728744506836\n",
      "cnt: 0 - valLoss: 0.5712466239929199 - trainLoss: 0.5656511187553406\n",
      "cnt: 0 - valLoss: 0.5712225437164307 - trainLoss: 0.5656294226646423\n",
      "cnt: 0 - valLoss: 0.5711983442306519 - trainLoss: 0.5656077265739441\n",
      "cnt: 0 - valLoss: 0.5711742043495178 - trainLoss: 0.5655859708786011\n",
      "cnt: 0 - valLoss: 0.571150004863739 - trainLoss: 0.5655642747879028\n",
      "cnt: 0 - valLoss: 0.571125864982605 - trainLoss: 0.565542459487915\n",
      "cnt: 0 - valLoss: 0.5711016654968262 - trainLoss: 0.565520703792572\n",
      "cnt: 0 - valLoss: 0.5710775256156921 - trainLoss: 0.565498948097229\n",
      "cnt: 0 - valLoss: 0.5710533261299133 - trainLoss: 0.5654771327972412\n",
      "cnt: 0 - valLoss: 0.5710291266441345 - trainLoss: 0.5654553771018982\n",
      "cnt: 0 - valLoss: 0.5710049867630005 - trainLoss: 0.5654335618019104\n",
      "cnt: 0 - valLoss: 0.5709807872772217 - trainLoss: 0.5654118061065674\n",
      "cnt: 0 - valLoss: 0.5709565281867981 - trainLoss: 0.5653899908065796\n",
      "cnt: 0 - valLoss: 0.5709323883056641 - trainLoss: 0.5653682351112366\n",
      "cnt: 0 - valLoss: 0.5709081888198853 - trainLoss: 0.5653464198112488\n",
      "cnt: 0 - valLoss: 0.5708839893341064 - trainLoss: 0.5653246641159058\n",
      "cnt: 0 - valLoss: 0.5708597302436829 - trainLoss: 0.565302848815918\n",
      "cnt: 0 - valLoss: 0.5708355903625488 - trainLoss: 0.565281093120575\n",
      "cnt: 0 - valLoss: 0.57081139087677 - trainLoss: 0.5652592778205872\n",
      "cnt: 0 - valLoss: 0.5707871317863464 - trainLoss: 0.5652375221252441\n",
      "cnt: 0 - valLoss: 0.5707629919052124 - trainLoss: 0.5652157068252563\n",
      "cnt: 0 - valLoss: 0.5707387328147888 - trainLoss: 0.5651938319206238\n",
      "cnt: 0 - valLoss: 0.57071453332901 - trainLoss: 0.5651721358299255\n",
      "cnt: 0 - valLoss: 0.5706903338432312 - trainLoss: 0.565150260925293\n",
      "cnt: 0 - valLoss: 0.5706661343574524 - trainLoss: 0.5651284456253052\n",
      "cnt: 0 - valLoss: 0.5706419348716736 - trainLoss: 0.5651066899299622\n",
      "cnt: 0 - valLoss: 0.57061767578125 - trainLoss: 0.5650849342346191\n",
      "cnt: 0 - valLoss: 0.5705934762954712 - trainLoss: 0.5650632381439209\n",
      "cnt: 0 - valLoss: 0.5705692768096924 - trainLoss: 0.5650413632392883\n",
      "cnt: 0 - valLoss: 0.5705450773239136 - trainLoss: 0.5650196671485901\n",
      "cnt: 0 - valLoss: 0.5705208778381348 - trainLoss: 0.5649979114532471\n",
      "cnt: 0 - valLoss: 0.5704966187477112 - trainLoss: 0.5649760961532593\n",
      "cnt: 0 - valLoss: 0.5704723596572876 - trainLoss: 0.564954400062561\n",
      "cnt: 0 - valLoss: 0.5704481601715088 - trainLoss: 0.564932644367218\n",
      "cnt: 0 - valLoss: 0.57042396068573 - trainLoss: 0.5649108290672302\n",
      "cnt: 0 - valLoss: 0.5703997015953064 - trainLoss: 0.5648890733718872\n",
      "cnt: 0 - valLoss: 0.5703755021095276 - trainLoss: 0.5648672580718994\n",
      "cnt: 0 - valLoss: 0.570351243019104 - trainLoss: 0.5648455023765564\n",
      "cnt: 0 - valLoss: 0.5703269839286804 - trainLoss: 0.5648236870765686\n",
      "cnt: 0 - valLoss: 0.5703027248382568 - trainLoss: 0.5648019313812256\n",
      "cnt: 0 - valLoss: 0.570278525352478 - trainLoss: 0.5647801160812378\n",
      "cnt: 0 - valLoss: 0.5702542066574097 - trainLoss: 0.5647583603858948\n",
      "cnt: 0 - valLoss: 0.5702300071716309 - trainLoss: 0.564736545085907\n",
      "cnt: 0 - valLoss: 0.5702057480812073 - trainLoss: 0.5647147297859192\n",
      "cnt: 0 - valLoss: 0.5701814293861389 - trainLoss: 0.5646929740905762\n",
      "cnt: 0 - valLoss: 0.5701571702957153 - trainLoss: 0.5646712183952332\n",
      "cnt: 0 - valLoss: 0.5701329708099365 - trainLoss: 0.5646494030952454\n",
      "cnt: 0 - valLoss: 0.5701086521148682 - trainLoss: 0.5646275877952576\n",
      "cnt: 0 - valLoss: 0.5700844526290894 - trainLoss: 0.5646057724952698\n",
      "cnt: 0 - valLoss: 0.570060133934021 - trainLoss: 0.5645840167999268\n",
      "cnt: 0 - valLoss: 0.5700358748435974 - trainLoss: 0.564562201499939\n",
      "cnt: 0 - valLoss: 0.5700116157531738 - trainLoss: 0.5645403861999512\n",
      "cnt: 0 - valLoss: 0.5699873566627502 - trainLoss: 0.5645185708999634\n",
      "cnt: 0 - valLoss: 0.5699629783630371 - trainLoss: 0.5644967555999756\n",
      "cnt: 0 - valLoss: 0.5699387192726135 - trainLoss: 0.5644749402999878\n",
      "cnt: 0 - valLoss: 0.5699144601821899 - trainLoss: 0.564453125\n",
      "cnt: 0 - valLoss: 0.5698902010917664 - trainLoss: 0.5644313097000122\n",
      "cnt: 0 - valLoss: 0.569865882396698 - trainLoss: 0.5644094944000244\n",
      "cnt: 0 - valLoss: 0.5698416233062744 - trainLoss: 0.5643876791000366\n",
      "cnt: 0 - valLoss: 0.5698173642158508 - trainLoss: 0.5643658638000488\n",
      "cnt: 0 - valLoss: 0.5697929859161377 - trainLoss: 0.564344048500061\n",
      "cnt: 0 - valLoss: 0.5697687268257141 - trainLoss: 0.5643222332000732\n",
      "cnt: 0 - valLoss: 0.5697444677352905 - trainLoss: 0.5643004179000854\n",
      "cnt: 0 - valLoss: 0.5697201490402222 - trainLoss: 0.5642786026000977\n",
      "cnt: 0 - valLoss: 0.5696958899497986 - trainLoss: 0.5642567276954651\n",
      "cnt: 0 - valLoss: 0.5696715712547302 - trainLoss: 0.5642348527908325\n",
      "cnt: 0 - valLoss: 0.5696472525596619 - trainLoss: 0.5642130374908447\n",
      "cnt: 0 - valLoss: 0.5696229338645935 - trainLoss: 0.5641911625862122\n",
      "cnt: 0 - valLoss: 0.5695986151695251 - trainLoss: 0.5641692876815796\n",
      "cnt: 0 - valLoss: 0.5695742964744568 - trainLoss: 0.564147412776947\n",
      "cnt: 0 - valLoss: 0.5695499777793884 - trainLoss: 0.5641255378723145\n",
      "cnt: 0 - valLoss: 0.5695256590843201 - trainLoss: 0.5641036629676819\n",
      "cnt: 0 - valLoss: 0.5695013403892517 - trainLoss: 0.5640817880630493\n",
      "cnt: 0 - valLoss: 0.5694770216941833 - trainLoss: 0.5640599131584167\n",
      "cnt: 0 - valLoss: 0.569452702999115 - trainLoss: 0.5640380382537842\n",
      "cnt: 0 - valLoss: 0.5694283246994019 - trainLoss: 0.5640161037445068\n",
      "cnt: 0 - valLoss: 0.5694040656089783 - trainLoss: 0.5639942288398743\n",
      "cnt: 0 - valLoss: 0.5693797469139099 - trainLoss: 0.5639723539352417\n",
      "cnt: 0 - valLoss: 0.5693553686141968 - trainLoss: 0.5639504790306091\n",
      "cnt: 0 - valLoss: 0.5693310499191284 - trainLoss: 0.5639286041259766\n",
      "cnt: 0 - valLoss: 0.5693067312240601 - trainLoss: 0.563906729221344\n",
      "cnt: 0 - valLoss: 0.5692823529243469 - trainLoss: 0.5638848543167114\n",
      "cnt: 0 - valLoss: 0.5692580342292786 - trainLoss: 0.5638629794120789\n",
      "cnt: 0 - valLoss: 0.5692336559295654 - trainLoss: 0.5638410449028015\n",
      "cnt: 0 - valLoss: 0.5692093372344971 - trainLoss: 0.563819169998169\n",
      "cnt: 0 - valLoss: 0.5691849589347839 - trainLoss: 0.5637972950935364\n",
      "cnt: 0 - valLoss: 0.5691606402397156 - trainLoss: 0.563775360584259\n",
      "cnt: 0 - valLoss: 0.5691363215446472 - trainLoss: 0.5637534856796265\n",
      "cnt: 0 - valLoss: 0.5691119432449341 - trainLoss: 0.5637315511703491\n",
      "cnt: 0 - valLoss: 0.5690876245498657 - trainLoss: 0.5637096762657166\n",
      "cnt: 0 - valLoss: 0.5690632462501526 - trainLoss: 0.5636878609657288\n",
      "cnt: 0 - valLoss: 0.5690389275550842 - trainLoss: 0.5636659264564514\n",
      "cnt: 0 - valLoss: 0.5690145492553711 - trainLoss: 0.5636439919471741\n",
      "cnt: 0 - valLoss: 0.5689902305603027 - trainLoss: 0.5636221170425415\n",
      "cnt: 0 - valLoss: 0.5689658522605896 - trainLoss: 0.5636001825332642\n",
      "cnt: 0 - valLoss: 0.5689414143562317 - trainLoss: 0.5635783076286316\n",
      "cnt: 0 - valLoss: 0.5689170956611633 - trainLoss: 0.5635563731193542\n",
      "cnt: 0 - valLoss: 0.5688927173614502 - trainLoss: 0.5635344386100769\n",
      "cnt: 0 - valLoss: 0.5688683390617371 - trainLoss: 0.5635125637054443\n",
      "cnt: 0 - valLoss: 0.5688439607620239 - trainLoss: 0.563490629196167\n",
      "cnt: 0 - valLoss: 0.5688195824623108 - trainLoss: 0.5634687542915344\n",
      "cnt: 0 - valLoss: 0.5687952041625977 - trainLoss: 0.5634467601776123\n",
      "cnt: 0 - valLoss: 0.5687708258628845 - trainLoss: 0.5634248852729797\n",
      "cnt: 0 - valLoss: 0.5687463879585266 - trainLoss: 0.5634030103683472\n",
      "cnt: 0 - valLoss: 0.5687220692634583 - trainLoss: 0.5633810758590698\n",
      "cnt: 0 - valLoss: 0.5686976909637451 - trainLoss: 0.5633591413497925\n",
      "cnt: 0 - valLoss: 0.568673312664032 - trainLoss: 0.5633372068405151\n",
      "cnt: 0 - valLoss: 0.5686489343643188 - trainLoss: 0.5633152723312378\n",
      "cnt: 0 - valLoss: 0.5686244964599609 - trainLoss: 0.5632933974266052\n",
      "cnt: 0 - valLoss: 0.5686001181602478 - trainLoss: 0.5632714629173279\n",
      "cnt: 0 - valLoss: 0.5685756802558899 - trainLoss: 0.5632495284080505\n",
      "cnt: 0 - valLoss: 0.568551242351532 - trainLoss: 0.563227653503418\n",
      "cnt: 0 - valLoss: 0.5685267448425293 - trainLoss: 0.5632057189941406\n",
      "cnt: 0 - valLoss: 0.5685022473335266 - trainLoss: 0.5631837844848633\n",
      "cnt: 0 - valLoss: 0.5684778690338135 - trainLoss: 0.5631618499755859\n",
      "cnt: 0 - valLoss: 0.5684534311294556 - trainLoss: 0.5631398558616638\n",
      "cnt: 0 - valLoss: 0.5684288740158081 - trainLoss: 0.5631179809570312\n",
      "cnt: 0 - valLoss: 0.5684044361114502 - trainLoss: 0.5630960464477539\n",
      "cnt: 0 - valLoss: 0.5683799386024475 - trainLoss: 0.5630741119384766\n",
      "cnt: 0 - valLoss: 0.5683555006980896 - trainLoss: 0.5630521774291992\n",
      "cnt: 0 - valLoss: 0.5683310627937317 - trainLoss: 0.5630302429199219\n",
      "cnt: 0 - valLoss: 0.5683065056800842 - trainLoss: 0.5630083084106445\n",
      "cnt: 0 - valLoss: 0.5682820677757263 - trainLoss: 0.5629863739013672\n",
      "cnt: 0 - valLoss: 0.5682576298713684 - trainLoss: 0.5629644393920898\n",
      "cnt: 0 - valLoss: 0.5682331323623657 - trainLoss: 0.5629425048828125\n",
      "cnt: 0 - valLoss: 0.5682085752487183 - trainLoss: 0.5629205703735352\n",
      "cnt: 0 - valLoss: 0.5681841373443604 - trainLoss: 0.5628986358642578\n",
      "cnt: 0 - valLoss: 0.5681596398353577 - trainLoss: 0.5628766417503357\n",
      "cnt: 0 - valLoss: 0.5681352019309998 - trainLoss: 0.5628547668457031\n",
      "cnt: 0 - valLoss: 0.5681107640266418 - trainLoss: 0.5628328323364258\n",
      "cnt: 0 - valLoss: 0.5680863261222839 - trainLoss: 0.5628108978271484\n",
      "cnt: 0 - valLoss: 0.568061888217926 - trainLoss: 0.5627889037132263\n",
      "cnt: 0 - valLoss: 0.5680373907089233 - trainLoss: 0.5627670288085938\n",
      "cnt: 0 - valLoss: 0.5680129528045654 - trainLoss: 0.5627450942993164\n",
      "cnt: 0 - valLoss: 0.5679885149002075 - trainLoss: 0.5627231597900391\n",
      "cnt: 0 - valLoss: 0.5679640173912048 - trainLoss: 0.5627012252807617\n",
      "cnt: 0 - valLoss: 0.5679395794868469 - trainLoss: 0.5626792907714844\n",
      "cnt: 0 - valLoss: 0.5679150819778442 - trainLoss: 0.562657356262207\n",
      "cnt: 0 - valLoss: 0.5678906440734863 - trainLoss: 0.5626353621482849\n",
      "cnt: 0 - valLoss: 0.5678661465644836 - trainLoss: 0.5626134276390076\n",
      "cnt: 0 - valLoss: 0.5678417086601257 - trainLoss: 0.5625914931297302\n",
      "cnt: 0 - valLoss: 0.5678172707557678 - trainLoss: 0.5625695586204529\n",
      "cnt: 0 - valLoss: 0.5677928328514099 - trainLoss: 0.5625476241111755\n",
      "cnt: 0 - valLoss: 0.5677683353424072 - trainLoss: 0.5625256896018982\n",
      "cnt: 0 - valLoss: 0.5677437782287598 - trainLoss: 0.5625036954879761\n",
      "cnt: 0 - valLoss: 0.5677193403244019 - trainLoss: 0.5624817609786987\n",
      "cnt: 0 - valLoss: 0.5676948428153992 - trainLoss: 0.5624598264694214\n",
      "cnt: 0 - valLoss: 0.5676703453063965 - trainLoss: 0.562437891960144\n",
      "cnt: 0 - valLoss: 0.5676459670066833 - trainLoss: 0.5624159574508667\n",
      "cnt: 0 - valLoss: 0.5676214694976807 - trainLoss: 0.5623939633369446\n",
      "cnt: 0 - valLoss: 0.567596971988678 - trainLoss: 0.5623720288276672\n",
      "cnt: 0 - valLoss: 0.5675723552703857 - trainLoss: 0.5623500943183899\n",
      "cnt: 0 - valLoss: 0.5675478577613831 - trainLoss: 0.5623281002044678\n",
      "cnt: 0 - valLoss: 0.5675232410430908 - trainLoss: 0.5623061656951904\n",
      "cnt: 0 - valLoss: 0.5674987435340881 - trainLoss: 0.5622841715812683\n",
      "cnt: 0 - valLoss: 0.5674741864204407 - trainLoss: 0.5622622966766357\n",
      "cnt: 0 - valLoss: 0.5674495697021484 - trainLoss: 0.5622403025627136\n",
      "cnt: 0 - valLoss: 0.5674250721931458 - trainLoss: 0.5622183084487915\n",
      "cnt: 0 - valLoss: 0.5674004554748535 - trainLoss: 0.5621963739395142\n",
      "cnt: 0 - valLoss: 0.5673759579658508 - trainLoss: 0.5621744394302368\n",
      "cnt: 0 - valLoss: 0.5673513412475586 - trainLoss: 0.5621524453163147\n",
      "cnt: 0 - valLoss: 0.5673267841339111 - trainLoss: 0.5621304512023926\n",
      "cnt: 0 - valLoss: 0.5673022270202637 - trainLoss: 0.5621085166931152\n",
      "cnt: 0 - valLoss: 0.5672776699066162 - trainLoss: 0.5620865225791931\n",
      "cnt: 0 - valLoss: 0.5672531127929688 - trainLoss: 0.5620645880699158\n",
      "cnt: 0 - valLoss: 0.5672284960746765 - trainLoss: 0.5620425939559937\n",
      "cnt: 0 - valLoss: 0.567203938961029 - trainLoss: 0.5620206594467163\n",
      "cnt: 0 - valLoss: 0.5671793818473816 - trainLoss: 0.561998724937439\n",
      "cnt: 0 - valLoss: 0.5671547651290894 - trainLoss: 0.5619767308235168\n",
      "cnt: 0 - valLoss: 0.5671302080154419 - trainLoss: 0.5619547367095947\n",
      "cnt: 0 - valLoss: 0.5671057105064392 - trainLoss: 0.5619328022003174\n",
      "cnt: 0 - valLoss: 0.5670811533927917 - trainLoss: 0.56191086769104\n",
      "cnt: 0 - valLoss: 0.5670565366744995 - trainLoss: 0.5618889331817627\n",
      "cnt: 0 - valLoss: 0.5670320391654968 - trainLoss: 0.5618669390678406\n",
      "cnt: 0 - valLoss: 0.5670074224472046 - trainLoss: 0.561845064163208\n",
      "cnt: 0 - valLoss: 0.5669828653335571 - trainLoss: 0.5618230700492859\n",
      "cnt: 0 - valLoss: 0.5669583082199097 - trainLoss: 0.5618011355400085\n",
      "cnt: 0 - valLoss: 0.5669337511062622 - trainLoss: 0.5617792010307312\n",
      "cnt: 0 - valLoss: 0.5669091939926147 - trainLoss: 0.5617572665214539\n",
      "cnt: 0 - valLoss: 0.5668845772743225 - trainLoss: 0.5617353320121765\n",
      "cnt: 0 - valLoss: 0.5668599605560303 - trainLoss: 0.5617133378982544\n",
      "cnt: 0 - valLoss: 0.5668354034423828 - trainLoss: 0.561691403388977\n",
      "cnt: 0 - valLoss: 0.5668108463287354 - trainLoss: 0.5616694092750549\n",
      "cnt: 0 - valLoss: 0.5667862892150879 - trainLoss: 0.5616474747657776\n",
      "cnt: 0 - valLoss: 0.5667616724967957 - trainLoss: 0.5616255402565002\n",
      "cnt: 0 - valLoss: 0.5667371153831482 - trainLoss: 0.5616035461425781\n",
      "cnt: 0 - valLoss: 0.5667125582695007 - trainLoss: 0.5615816712379456\n",
      "cnt: 0 - valLoss: 0.5666879415512085 - trainLoss: 0.5615596771240234\n",
      "cnt: 0 - valLoss: 0.5666633248329163 - trainLoss: 0.5615376830101013\n",
      "cnt: 0 - valLoss: 0.566638708114624 - trainLoss: 0.561515748500824\n",
      "cnt: 0 - valLoss: 0.5666141510009766 - trainLoss: 0.5614937543869019\n",
      "cnt: 0 - valLoss: 0.5665895938873291 - trainLoss: 0.5614718198776245\n",
      "cnt: 0 - valLoss: 0.5665650367736816 - trainLoss: 0.5614498853683472\n",
      "cnt: 0 - valLoss: 0.5665403604507446 - trainLoss: 0.561427891254425\n",
      "cnt: 0 - valLoss: 0.5665157437324524 - trainLoss: 0.5614059567451477\n",
      "cnt: 0 - valLoss: 0.5664911866188049 - trainLoss: 0.5613839626312256\n",
      "cnt: 0 - valLoss: 0.5664665699005127 - trainLoss: 0.5613619685173035\n",
      "cnt: 0 - valLoss: 0.5664420127868652 - trainLoss: 0.5613400936126709\n",
      "cnt: 0 - valLoss: 0.566417396068573 - trainLoss: 0.5613180994987488\n",
      "cnt: 0 - valLoss: 0.5663927793502808 - trainLoss: 0.5612961053848267\n",
      "cnt: 0 - valLoss: 0.5663681626319885 - trainLoss: 0.5612741708755493\n",
      "cnt: 0 - valLoss: 0.5663434863090515 - trainLoss: 0.5612521767616272\n",
      "cnt: 0 - valLoss: 0.566318929195404 - trainLoss: 0.5612301826477051\n",
      "cnt: 0 - valLoss: 0.5662943124771118 - trainLoss: 0.5612082481384277\n",
      "cnt: 0 - valLoss: 0.5662696957588196 - trainLoss: 0.5611862540245056\n",
      "cnt: 0 - valLoss: 0.5662450790405273 - trainLoss: 0.5611642599105835\n",
      "cnt: 0 - valLoss: 0.5662204623222351 - trainLoss: 0.5611422657966614\n",
      "cnt: 0 - valLoss: 0.5661959052085876 - trainLoss: 0.561120331287384\n",
      "cnt: 0 - valLoss: 0.5661712288856506 - trainLoss: 0.5610983371734619\n",
      "cnt: 0 - valLoss: 0.5661466121673584 - trainLoss: 0.5610764026641846\n",
      "cnt: 0 - valLoss: 0.5661219954490662 - trainLoss: 0.5610543489456177\n",
      "cnt: 0 - valLoss: 0.5660973787307739 - trainLoss: 0.5610324144363403\n",
      "cnt: 0 - valLoss: 0.5660727620124817 - trainLoss: 0.5610104203224182\n",
      "cnt: 0 - valLoss: 0.5660480856895447 - trainLoss: 0.5609883666038513\n",
      "cnt: 0 - valLoss: 0.5660234689712524 - trainLoss: 0.560966432094574\n",
      "cnt: 0 - valLoss: 0.5659987926483154 - trainLoss: 0.5609443187713623\n",
      "cnt: 0 - valLoss: 0.5659741163253784 - trainLoss: 0.5609223246574402\n",
      "cnt: 0 - valLoss: 0.5659494996070862 - trainLoss: 0.5609002709388733\n",
      "cnt: 0 - valLoss: 0.5659248232841492 - trainLoss: 0.5608781576156616\n",
      "cnt: 0 - valLoss: 0.5659001469612122 - trainLoss: 0.5608561635017395\n",
      "cnt: 0 - valLoss: 0.5658754706382751 - trainLoss: 0.5608340501785278\n",
      "cnt: 0 - valLoss: 0.5658508539199829 - trainLoss: 0.5608119964599609\n",
      "cnt: 0 - valLoss: 0.5658261775970459 - trainLoss: 0.5607898831367493\n",
      "cnt: 0 - valLoss: 0.5658014416694641 - trainLoss: 0.5607678890228271\n",
      "cnt: 0 - valLoss: 0.5657768249511719 - trainLoss: 0.5607458353042603\n",
      "cnt: 0 - valLoss: 0.5657522082328796 - trainLoss: 0.5607237219810486\n",
      "cnt: 0 - valLoss: 0.5657274723052979 - trainLoss: 0.5607016682624817\n",
      "cnt: 0 - valLoss: 0.5657027959823608 - trainLoss: 0.5606796145439148\n",
      "cnt: 0 - valLoss: 0.5656781196594238 - trainLoss: 0.5606575012207031\n",
      "cnt: 0 - valLoss: 0.5656534433364868 - trainLoss: 0.5606354475021362\n",
      "cnt: 0 - valLoss: 0.5656287670135498 - trainLoss: 0.5606133937835693\n",
      "cnt: 0 - valLoss: 0.5656040906906128 - trainLoss: 0.5605912804603577\n",
      "cnt: 0 - valLoss: 0.5655794143676758 - trainLoss: 0.5605692267417908\n",
      "cnt: 0 - valLoss: 0.5655547380447388 - trainLoss: 0.5605471134185791\n",
      "cnt: 0 - valLoss: 0.5655300617218018 - trainLoss: 0.5605250597000122\n",
      "cnt: 0 - valLoss: 0.5655053853988647 - trainLoss: 0.5605029463768005\n",
      "cnt: 0 - valLoss: 0.565480649471283 - trainLoss: 0.5604808926582336\n",
      "cnt: 0 - valLoss: 0.565455973148346 - trainLoss: 0.5604588985443115\n",
      "cnt: 0 - valLoss: 0.5654312968254089 - trainLoss: 0.5604367256164551\n",
      "cnt: 0 - valLoss: 0.5654065608978271 - trainLoss: 0.560414731502533\n",
      "cnt: 0 - valLoss: 0.5653818845748901 - trainLoss: 0.5603926181793213\n",
      "cnt: 0 - valLoss: 0.5653572082519531 - trainLoss: 0.5603705048561096\n",
      "cnt: 0 - valLoss: 0.5653324723243713 - trainLoss: 0.5603484511375427\n",
      "cnt: 0 - valLoss: 0.5653077960014343 - trainLoss: 0.560326337814331\n",
      "cnt: 0 - valLoss: 0.5652831196784973 - trainLoss: 0.5603042840957642\n",
      "cnt: 0 - valLoss: 0.5652583837509155 - trainLoss: 0.5602821707725525\n",
      "cnt: 0 - valLoss: 0.5652337074279785 - trainLoss: 0.5602600574493408\n",
      "cnt: 0 - valLoss: 0.5652089715003967 - trainLoss: 0.5602380037307739\n",
      "cnt: 0 - valLoss: 0.5651842355728149 - trainLoss: 0.5602158904075623\n",
      "cnt: 0 - valLoss: 0.5651595592498779 - trainLoss: 0.5601937770843506\n",
      "cnt: 0 - valLoss: 0.5651348233222961 - trainLoss: 0.5601717233657837\n",
      "cnt: 0 - valLoss: 0.5651101469993591 - trainLoss: 0.560149610042572\n",
      "cnt: 0 - valLoss: 0.5650854110717773 - trainLoss: 0.5601274967193604\n",
      "cnt: 0 - valLoss: 0.5650607347488403 - trainLoss: 0.5601054430007935\n",
      "cnt: 0 - valLoss: 0.5650359392166138 - trainLoss: 0.5600833296775818\n",
      "cnt: 0 - valLoss: 0.5650112628936768 - trainLoss: 0.5600612759590149\n",
      "cnt: 0 - valLoss: 0.5649865865707397 - trainLoss: 0.5600391626358032\n",
      "cnt: 0 - valLoss: 0.5649617910385132 - trainLoss: 0.5600170493125916\n",
      "cnt: 0 - valLoss: 0.5649371147155762 - trainLoss: 0.5599948763847351\n",
      "cnt: 0 - valLoss: 0.5649123191833496 - trainLoss: 0.5599728226661682\n",
      "cnt: 0 - valLoss: 0.5648876428604126 - trainLoss: 0.5599507093429565\n",
      "cnt: 0 - valLoss: 0.5648629665374756 - trainLoss: 0.5599286556243896\n",
      "cnt: 0 - valLoss: 0.564838171005249 - trainLoss: 0.559906542301178\n",
      "cnt: 0 - valLoss: 0.5648134350776672 - trainLoss: 0.5598844289779663\n",
      "cnt: 0 - valLoss: 0.5647887587547302 - trainLoss: 0.5598623156547546\n",
      "cnt: 0 - valLoss: 0.5647639632225037 - trainLoss: 0.5598401427268982\n",
      "cnt: 0 - valLoss: 0.5647392272949219 - trainLoss: 0.5598180294036865\n",
      "cnt: 0 - valLoss: 0.5647144913673401 - trainLoss: 0.5597958564758301\n",
      "cnt: 0 - valLoss: 0.5646898150444031 - trainLoss: 0.5597736835479736\n",
      "cnt: 0 - valLoss: 0.5646650195121765 - trainLoss: 0.5597515106201172\n",
      "cnt: 0 - valLoss: 0.5646402835845947 - trainLoss: 0.5597294569015503\n",
      "cnt: 0 - valLoss: 0.5646154880523682 - trainLoss: 0.5597072839736938\n",
      "cnt: 0 - valLoss: 0.5645907521247864 - trainLoss: 0.5596851110458374\n",
      "cnt: 0 - valLoss: 0.5645660161972046 - trainLoss: 0.5596629977226257\n",
      "cnt: 0 - valLoss: 0.564541220664978 - trainLoss: 0.5596408247947693\n",
      "cnt: 0 - valLoss: 0.5645164251327515 - trainLoss: 0.5596186518669128\n",
      "cnt: 0 - valLoss: 0.5644916892051697 - trainLoss: 0.5595964789390564\n",
      "cnt: 0 - valLoss: 0.5644669532775879 - trainLoss: 0.5595743656158447\n",
      "cnt: 0 - valLoss: 0.5644421577453613 - trainLoss: 0.5595521926879883\n",
      "cnt: 0 - valLoss: 0.5644173622131348 - trainLoss: 0.5595300197601318\n",
      "cnt: 0 - valLoss: 0.564392626285553 - trainLoss: 0.5595079064369202\n",
      "cnt: 0 - valLoss: 0.5643678307533264 - trainLoss: 0.559485673904419\n",
      "cnt: 0 - valLoss: 0.5643430352210999 - trainLoss: 0.5594635605812073\n",
      "cnt: 0 - valLoss: 0.5643182992935181 - trainLoss: 0.5594413876533508\n",
      "cnt: 0 - valLoss: 0.5642935633659363 - trainLoss: 0.5594192147254944\n",
      "cnt: 0 - valLoss: 0.5642687082290649 - trainLoss: 0.5593970417976379\n",
      "cnt: 0 - valLoss: 0.5642440319061279 - trainLoss: 0.5593749284744263\n",
      "cnt: 0 - valLoss: 0.5642192959785461 - trainLoss: 0.5593528151512146\n",
      "cnt: 0 - valLoss: 0.5641945600509644 - trainLoss: 0.5593306422233582\n",
      "cnt: 0 - valLoss: 0.5641698241233826 - trainLoss: 0.5593084692955017\n",
      "cnt: 0 - valLoss: 0.5641451478004456 - trainLoss: 0.5592862963676453\n",
      "cnt: 0 - valLoss: 0.564120352268219 - trainLoss: 0.5592641234397888\n",
      "cnt: 0 - valLoss: 0.5640956163406372 - trainLoss: 0.5592420101165771\n",
      "cnt: 0 - valLoss: 0.5640708804130554 - trainLoss: 0.5592198371887207\n",
      "cnt: 0 - valLoss: 0.5640462040901184 - trainLoss: 0.559197723865509\n",
      "cnt: 0 - valLoss: 0.5640214085578918 - trainLoss: 0.5591755509376526\n",
      "cnt: 0 - valLoss: 0.5639966726303101 - trainLoss: 0.5591533184051514\n",
      "cnt: 0 - valLoss: 0.5639718770980835 - trainLoss: 0.5591312050819397\n",
      "cnt: 0 - valLoss: 0.5639470815658569 - trainLoss: 0.5591090321540833\n",
      "cnt: 0 - valLoss: 0.5639223456382751 - trainLoss: 0.559086799621582\n",
      "cnt: 0 - valLoss: 0.5638975501060486 - trainLoss: 0.5590646266937256\n",
      "cnt: 0 - valLoss: 0.5638728141784668 - trainLoss: 0.5590424537658691\n",
      "cnt: 0 - valLoss: 0.5638480186462402 - trainLoss: 0.5590202212333679\n",
      "cnt: 0 - valLoss: 0.5638232231140137 - trainLoss: 0.5589980483055115\n",
      "cnt: 0 - valLoss: 0.5637984275817871 - trainLoss: 0.5589758157730103\n",
      "cnt: 0 - valLoss: 0.5637736916542053 - trainLoss: 0.5589536428451538\n",
      "cnt: 0 - valLoss: 0.5637488961219788 - trainLoss: 0.5589315295219421\n",
      "cnt: 0 - valLoss: 0.563724160194397 - trainLoss: 0.5589093565940857\n",
      "cnt: 0 - valLoss: 0.5636994242668152 - trainLoss: 0.5588871836662292\n",
      "cnt: 0 - valLoss: 0.5636746287345886 - trainLoss: 0.5588650703430176\n",
      "cnt: 0 - valLoss: 0.5636498332023621 - trainLoss: 0.5588429570198059\n",
      "cnt: 0 - valLoss: 0.5636250972747803 - trainLoss: 0.5588207840919495\n",
      "cnt: 0 - valLoss: 0.5636003613471985 - trainLoss: 0.558798611164093\n",
      "cnt: 0 - valLoss: 0.5635755658149719 - trainLoss: 0.5587764978408813\n",
      "cnt: 0 - valLoss: 0.5635507702827454 - trainLoss: 0.5587543249130249\n",
      "cnt: 0 - valLoss: 0.5635260343551636 - trainLoss: 0.5587321519851685\n",
      "cnt: 0 - valLoss: 0.563501238822937 - trainLoss: 0.558709979057312\n",
      "cnt: 0 - valLoss: 0.5634764432907104 - trainLoss: 0.5586879253387451\n",
      "cnt: 0 - valLoss: 0.5634516477584839 - trainLoss: 0.5586657524108887\n",
      "cnt: 0 - valLoss: 0.5634269118309021 - trainLoss: 0.5586435794830322\n",
      "cnt: 0 - valLoss: 0.5634021162986755 - trainLoss: 0.5586214661598206\n",
      "cnt: 0 - valLoss: 0.5633773803710938 - trainLoss: 0.5585992932319641\n",
      "cnt: 0 - valLoss: 0.5633525848388672 - trainLoss: 0.5585771203041077\n",
      "cnt: 0 - valLoss: 0.5633277893066406 - trainLoss: 0.5585549473762512\n",
      "cnt: 0 - valLoss: 0.5633029937744141 - trainLoss: 0.5585328936576843\n",
      "cnt: 0 - valLoss: 0.5632782578468323 - trainLoss: 0.5585107207298279\n",
      "cnt: 0 - valLoss: 0.5632534027099609 - trainLoss: 0.5584886074066162\n",
      "cnt: 0 - valLoss: 0.5632286667823792 - trainLoss: 0.5584664344787598\n",
      "cnt: 0 - valLoss: 0.5632038712501526 - trainLoss: 0.5584442615509033\n",
      "cnt: 0 - valLoss: 0.563179075717926 - trainLoss: 0.5584222078323364\n",
      "cnt: 0 - valLoss: 0.5631543397903442 - trainLoss: 0.55840003490448\n",
      "cnt: 0 - valLoss: 0.5631296634674072 - trainLoss: 0.5583779215812683\n",
      "cnt: 0 - valLoss: 0.5631049871444702 - trainLoss: 0.5583558678627014\n",
      "cnt: 0 - valLoss: 0.5630802512168884 - trainLoss: 0.5583337545394897\n",
      "cnt: 0 - valLoss: 0.5630556344985962 - trainLoss: 0.5583116412162781\n",
      "cnt: 0 - valLoss: 0.5630309581756592 - trainLoss: 0.5582895278930664\n",
      "cnt: 0 - valLoss: 0.5630062818527222 - trainLoss: 0.5582674145698547\n",
      "cnt: 0 - valLoss: 0.5629816055297852 - trainLoss: 0.5582453012466431\n",
      "cnt: 0 - valLoss: 0.5629569292068481 - trainLoss: 0.5582231879234314\n",
      "cnt: 0 - valLoss: 0.5629321932792664 - trainLoss: 0.5582010746002197\n",
      "cnt: 0 - valLoss: 0.5629075169563293 - trainLoss: 0.5581790208816528\n",
      "cnt: 0 - valLoss: 0.5628827810287476 - trainLoss: 0.5581568479537964\n",
      "cnt: 0 - valLoss: 0.5628581047058105 - trainLoss: 0.5581347346305847\n",
      "cnt: 0 - valLoss: 0.5628334879875183 - trainLoss: 0.558112621307373\n",
      "cnt: 0 - valLoss: 0.5628087520599365 - trainLoss: 0.5580905079841614\n",
      "cnt: 0 - valLoss: 0.5627840161323547 - trainLoss: 0.5580683350563049\n",
      "cnt: 0 - valLoss: 0.5627593398094177 - trainLoss: 0.5580462217330933\n",
      "cnt: 0 - valLoss: 0.5627346038818359 - trainLoss: 0.5580240488052368\n",
      "cnt: 0 - valLoss: 0.5627099275588989 - trainLoss: 0.5580019354820251\n",
      "cnt: 0 - valLoss: 0.5626852512359619 - trainLoss: 0.5579797029495239\n",
      "cnt: 0 - valLoss: 0.5626605749130249 - trainLoss: 0.557957649230957\n",
      "cnt: 0 - valLoss: 0.5626358985900879 - trainLoss: 0.5579354763031006\n",
      "cnt: 0 - valLoss: 0.5626111626625061 - trainLoss: 0.5579133033752441\n",
      "cnt: 0 - valLoss: 0.5625864267349243 - trainLoss: 0.5578911900520325\n",
      "cnt: 0 - valLoss: 0.5625617504119873 - trainLoss: 0.557869017124176\n",
      "cnt: 0 - valLoss: 0.5625370144844055 - trainLoss: 0.5578468441963196\n",
      "cnt: 0 - valLoss: 0.5625122785568237 - trainLoss: 0.5578247904777527\n",
      "cnt: 0 - valLoss: 0.5624875426292419 - trainLoss: 0.557802677154541\n",
      "cnt: 0 - valLoss: 0.5624629855155945 - trainLoss: 0.5577805638313293\n",
      "cnt: 0 - valLoss: 0.5624381899833679 - trainLoss: 0.5577584505081177\n",
      "cnt: 0 - valLoss: 0.5624135136604309 - trainLoss: 0.557736337184906\n",
      "cnt: 0 - valLoss: 0.5623887777328491 - trainLoss: 0.5577142834663391\n",
      "cnt: 0 - valLoss: 0.5623641014099121 - trainLoss: 0.5576921105384827\n",
      "cnt: 0 - valLoss: 0.5623394250869751 - trainLoss: 0.5576700568199158\n",
      "cnt: 0 - valLoss: 0.5623146891593933 - trainLoss: 0.5576479434967041\n",
      "cnt: 0 - valLoss: 0.5622899532318115 - trainLoss: 0.5576258301734924\n",
      "cnt: 0 - valLoss: 0.5622652769088745 - trainLoss: 0.5576037764549255\n",
      "cnt: 0 - valLoss: 0.5622405409812927 - trainLoss: 0.5575816035270691\n",
      "cnt: 0 - valLoss: 0.5622158050537109 - trainLoss: 0.5575594902038574\n",
      "cnt: 0 - valLoss: 0.5621911287307739 - trainLoss: 0.5575373768806458\n",
      "cnt: 0 - valLoss: 0.5621663928031921 - trainLoss: 0.5575152635574341\n",
      "cnt: 0 - valLoss: 0.5621416568756104 - trainLoss: 0.5574931502342224\n",
      "cnt: 0 - valLoss: 0.5621169209480286 - trainLoss: 0.5574710369110107\n",
      "cnt: 0 - valLoss: 0.5620921850204468 - trainLoss: 0.5574488639831543\n",
      "cnt: 0 - valLoss: 0.5620675086975098 - trainLoss: 0.5574268102645874\n",
      "cnt: 0 - valLoss: 0.562042772769928 - trainLoss: 0.5574046969413757\n",
      "cnt: 0 - valLoss: 0.5620180368423462 - trainLoss: 0.5573825836181641\n",
      "cnt: 0 - valLoss: 0.5619933605194092 - trainLoss: 0.5573604702949524\n",
      "cnt: 0 - valLoss: 0.5619686841964722 - trainLoss: 0.5573383569717407\n",
      "cnt: 0 - valLoss: 0.5619440078735352 - trainLoss: 0.5573161840438843\n",
      "cnt: 0 - valLoss: 0.5619193315505981 - trainLoss: 0.5572940111160278\n",
      "cnt: 0 - valLoss: 0.5618945956230164 - trainLoss: 0.5572718381881714\n",
      "cnt: 0 - valLoss: 0.5618698596954346 - trainLoss: 0.5572496652603149\n",
      "cnt: 0 - valLoss: 0.5618451833724976 - trainLoss: 0.5572274923324585\n",
      "cnt: 0 - valLoss: 0.5618204474449158 - trainLoss: 0.557205319404602\n",
      "cnt: 0 - valLoss: 0.5617957711219788 - trainLoss: 0.5571830868721008\n",
      "cnt: 0 - valLoss: 0.5617710947990417 - trainLoss: 0.5571609139442444\n",
      "cnt: 0 - valLoss: 0.56174635887146 - trainLoss: 0.5571387410163879\n",
      "cnt: 0 - valLoss: 0.561721682548523 - trainLoss: 0.5571165680885315\n",
      "cnt: 0 - valLoss: 0.5616969466209412 - trainLoss: 0.557094395160675\n",
      "cnt: 0 - valLoss: 0.5616722702980042 - trainLoss: 0.5570722222328186\n",
      "cnt: 0 - valLoss: 0.5616475939750671 - trainLoss: 0.5570500493049622\n",
      "cnt: 0 - valLoss: 0.5616228580474854 - trainLoss: 0.5570278763771057\n",
      "cnt: 0 - valLoss: 0.5615981221199036 - trainLoss: 0.5570056438446045\n",
      "cnt: 0 - valLoss: 0.5615733861923218 - trainLoss: 0.556983470916748\n",
      "cnt: 0 - valLoss: 0.5615487098693848 - trainLoss: 0.5569612979888916\n",
      "cnt: 0 - valLoss: 0.561523973941803 - trainLoss: 0.5569390654563904\n",
      "cnt: 0 - valLoss: 0.5614992380142212 - trainLoss: 0.5569168925285339\n",
      "cnt: 0 - valLoss: 0.5614745020866394 - trainLoss: 0.5568947196006775\n",
      "cnt: 0 - valLoss: 0.5614498257637024 - trainLoss: 0.556872546672821\n",
      "cnt: 0 - valLoss: 0.5614250898361206 - trainLoss: 0.5568503737449646\n",
      "cnt: 0 - valLoss: 0.5614003539085388 - trainLoss: 0.5568281412124634\n",
      "cnt: 0 - valLoss: 0.5613756775856018 - trainLoss: 0.5568059682846069\n",
      "cnt: 0 - valLoss: 0.5613508820533752 - trainLoss: 0.5567837357521057\n",
      "cnt: 0 - valLoss: 0.5613262057304382 - trainLoss: 0.5567615628242493\n",
      "cnt: 0 - valLoss: 0.5613014698028564 - trainLoss: 0.5567393898963928\n",
      "cnt: 0 - valLoss: 0.5612767338752747 - trainLoss: 0.5567172169685364\n",
      "cnt: 0 - valLoss: 0.5612519979476929 - trainLoss: 0.5566949844360352\n",
      "cnt: 0 - valLoss: 0.5612272620201111 - trainLoss: 0.5566727519035339\n",
      "cnt: 0 - valLoss: 0.5612024664878845 - trainLoss: 0.5566505193710327\n",
      "cnt: 0 - valLoss: 0.5611777305603027 - trainLoss: 0.5566282272338867\n",
      "cnt: 0 - valLoss: 0.5611530542373657 - trainLoss: 0.5566060543060303\n",
      "cnt: 0 - valLoss: 0.5611282587051392 - trainLoss: 0.5565837621688843\n",
      "cnt: 0 - valLoss: 0.5611034631729126 - trainLoss: 0.5565615296363831\n",
      "cnt: 0 - valLoss: 0.5610787272453308 - trainLoss: 0.5565392971038818\n",
      "cnt: 0 - valLoss: 0.561053991317749 - trainLoss: 0.5565171241760254\n",
      "cnt: 0 - valLoss: 0.5610292553901672 - trainLoss: 0.5564948320388794\n",
      "cnt: 0 - valLoss: 0.5610045194625854 - trainLoss: 0.5564725995063782\n",
      "cnt: 0 - valLoss: 0.5609797835350037 - trainLoss: 0.556450366973877\n",
      "cnt: 0 - valLoss: 0.5609549880027771 - trainLoss: 0.5564281344413757\n",
      "cnt: 0 - valLoss: 0.5609302520751953 - trainLoss: 0.5564059019088745\n",
      "cnt: 0 - valLoss: 0.5609055161476135 - trainLoss: 0.5563836693763733\n",
      "cnt: 0 - valLoss: 0.560880720615387 - trainLoss: 0.5563613772392273\n",
      "cnt: 0 - valLoss: 0.5608559846878052 - trainLoss: 0.5563391447067261\n",
      "cnt: 0 - valLoss: 0.5608312487602234 - trainLoss: 0.5563168525695801\n",
      "cnt: 0 - valLoss: 0.5608064532279968 - trainLoss: 0.5562946796417236\n",
      "cnt: 0 - valLoss: 0.5607816576957703 - trainLoss: 0.5562725067138672\n",
      "cnt: 0 - valLoss: 0.5607569813728333 - trainLoss: 0.5562503337860107\n",
      "cnt: 0 - valLoss: 0.5607321858406067 - trainLoss: 0.5562281608581543\n",
      "cnt: 0 - valLoss: 0.5607074499130249 - trainLoss: 0.5562059283256531\n",
      "cnt: 0 - valLoss: 0.5606827139854431 - trainLoss: 0.5561837553977966\n",
      "cnt: 0 - valLoss: 0.5606580376625061 - trainLoss: 0.5561615824699402\n",
      "cnt: 0 - valLoss: 0.5606333017349243 - trainLoss: 0.5561394095420837\n",
      "cnt: 0 - valLoss: 0.5606086254119873 - trainLoss: 0.5561172366142273\n",
      "cnt: 0 - valLoss: 0.5605838894844055 - trainLoss: 0.5560950636863708\n",
      "cnt: 0 - valLoss: 0.5605591535568237 - trainLoss: 0.5560728907585144\n",
      "cnt: 0 - valLoss: 0.5605344176292419 - trainLoss: 0.556050717830658\n",
      "cnt: 0 - valLoss: 0.5605097413063049 - trainLoss: 0.5560285449028015\n",
      "cnt: 0 - valLoss: 0.5604850649833679 - trainLoss: 0.5560063719749451\n",
      "cnt: 0 - valLoss: 0.5604603886604309 - trainLoss: 0.5559841990470886\n",
      "cnt: 0 - valLoss: 0.5604357123374939 - trainLoss: 0.5559620261192322\n",
      "cnt: 0 - valLoss: 0.5604109168052673 - trainLoss: 0.5559398531913757\n",
      "cnt: 0 - valLoss: 0.5603862404823303 - trainLoss: 0.5559176802635193\n",
      "cnt: 0 - valLoss: 0.5603614449501038 - trainLoss: 0.5558955073356628\n",
      "cnt: 0 - valLoss: 0.5603367686271667 - trainLoss: 0.5558733940124512\n",
      "cnt: 0 - valLoss: 0.5603120923042297 - trainLoss: 0.5558512210845947\n",
      "cnt: 0 - valLoss: 0.560287356376648 - trainLoss: 0.5558290481567383\n",
      "cnt: 0 - valLoss: 0.5602626204490662 - trainLoss: 0.5558068156242371\n",
      "cnt: 0 - valLoss: 0.5602378249168396 - trainLoss: 0.5557847023010254\n",
      "cnt: 0 - valLoss: 0.5602131485939026 - trainLoss: 0.555762529373169\n",
      "cnt: 0 - valLoss: 0.5601884722709656 - trainLoss: 0.5557402968406677\n",
      "cnt: 0 - valLoss: 0.5601637363433838 - trainLoss: 0.555718183517456\n",
      "cnt: 0 - valLoss: 0.560139000415802 - trainLoss: 0.5556960105895996\n",
      "cnt: 0 - valLoss: 0.5601142644882202 - trainLoss: 0.5556738376617432\n",
      "cnt: 0 - valLoss: 0.5600895285606384 - trainLoss: 0.5556515455245972\n",
      "cnt: 0 - valLoss: 0.5600647926330566 - trainLoss: 0.5556293725967407\n",
      "cnt: 0 - valLoss: 0.5600400567054749 - trainLoss: 0.5556071996688843\n",
      "cnt: 0 - valLoss: 0.5600153207778931 - trainLoss: 0.5555849671363831\n",
      "cnt: 0 - valLoss: 0.5599905252456665 - trainLoss: 0.5555627942085266\n",
      "cnt: 0 - valLoss: 0.5599657297134399 - trainLoss: 0.5555405616760254\n",
      "cnt: 0 - valLoss: 0.5599410533905029 - trainLoss: 0.555518388748169\n",
      "cnt: 0 - valLoss: 0.5599163174629211 - trainLoss: 0.555496096611023\n",
      "cnt: 0 - valLoss: 0.5598915815353394 - trainLoss: 0.5554739236831665\n",
      "cnt: 0 - valLoss: 0.5598668456077576 - trainLoss: 0.5554517507553101\n",
      "cnt: 0 - valLoss: 0.5598421096801758 - trainLoss: 0.5554295778274536\n",
      "cnt: 0 - valLoss: 0.5598173141479492 - trainLoss: 0.5554074048995972\n",
      "cnt: 0 - valLoss: 0.5597926378250122 - trainLoss: 0.5553852319717407\n",
      "cnt: 0 - valLoss: 0.5597679018974304 - trainLoss: 0.555363118648529\n",
      "cnt: 0 - valLoss: 0.5597432851791382 - trainLoss: 0.5553409457206726\n",
      "cnt: 0 - valLoss: 0.5597187280654907 - trainLoss: 0.5553188323974609\n",
      "cnt: 0 - valLoss: 0.5596940517425537 - trainLoss: 0.5552965998649597\n",
      "cnt: 0 - valLoss: 0.5596694350242615 - trainLoss: 0.555274486541748\n",
      "cnt: 0 - valLoss: 0.5596448183059692 - trainLoss: 0.5552523732185364\n",
      "cnt: 0 - valLoss: 0.5596202611923218 - trainLoss: 0.5552301406860352\n",
      "cnt: 0 - valLoss: 0.5595955848693848 - trainLoss: 0.5552080273628235\n",
      "cnt: 0 - valLoss: 0.5595710277557373 - trainLoss: 0.555185854434967\n",
      "cnt: 0 - valLoss: 0.5595463514328003 - trainLoss: 0.5551636815071106\n",
      "cnt: 0 - valLoss: 0.5595217347145081 - trainLoss: 0.5551415681838989\n",
      "cnt: 0 - valLoss: 0.5594971179962158 - trainLoss: 0.5551193356513977\n",
      "cnt: 0 - valLoss: 0.5594724416732788 - trainLoss: 0.555097222328186\n",
      "cnt: 0 - valLoss: 0.5594478249549866 - trainLoss: 0.5550750494003296\n",
      "cnt: 0 - valLoss: 0.5594232082366943 - trainLoss: 0.5550528764724731\n",
      "cnt: 0 - valLoss: 0.5593985915184021 - trainLoss: 0.5550307035446167\n",
      "cnt: 0 - valLoss: 0.5593739748001099 - trainLoss: 0.5550085306167603\n",
      "cnt: 0 - valLoss: 0.5593492984771729 - trainLoss: 0.5549863576889038\n",
      "cnt: 0 - valLoss: 0.5593247413635254 - trainLoss: 0.5549641847610474\n",
      "cnt: 0 - valLoss: 0.5593000054359436 - trainLoss: 0.5549420118331909\n",
      "cnt: 0 - valLoss: 0.5592753887176514 - trainLoss: 0.5549198389053345\n",
      "cnt: 0 - valLoss: 0.5592507123947144 - trainLoss: 0.554897665977478\n",
      "cnt: 0 - valLoss: 0.5592260956764221 - trainLoss: 0.5548754930496216\n",
      "cnt: 0 - valLoss: 0.5592014193534851 - trainLoss: 0.5548533201217651\n",
      "cnt: 0 - valLoss: 0.5591768622398376 - trainLoss: 0.5548310875892639\n",
      "cnt: 0 - valLoss: 0.5591521859169006 - trainLoss: 0.5548089146614075\n",
      "cnt: 0 - valLoss: 0.5591275691986084 - trainLoss: 0.5547868013381958\n",
      "cnt: 0 - valLoss: 0.5591028928756714 - trainLoss: 0.5547646284103394\n",
      "cnt: 0 - valLoss: 0.5590782761573792 - trainLoss: 0.5547424554824829\n",
      "cnt: 0 - valLoss: 0.5590535998344421 - trainLoss: 0.5547202825546265\n",
      "cnt: 0 - valLoss: 0.5590289831161499 - trainLoss: 0.55469810962677\n",
      "cnt: 0 - valLoss: 0.5590043067932129 - trainLoss: 0.5546759366989136\n",
      "cnt: 0 - valLoss: 0.5589796900749207 - trainLoss: 0.5546537637710571\n",
      "cnt: 0 - valLoss: 0.5589550733566284 - trainLoss: 0.5546315908432007\n",
      "cnt: 0 - valLoss: 0.5589304566383362 - trainLoss: 0.5546094179153442\n",
      "cnt: 0 - valLoss: 0.5589057207107544 - trainLoss: 0.5545872449874878\n",
      "cnt: 0 - valLoss: 0.5588811039924622 - trainLoss: 0.5545650720596313\n",
      "cnt: 0 - valLoss: 0.5588564276695251 - trainLoss: 0.5545428991317749\n",
      "cnt: 0 - valLoss: 0.5588318109512329 - trainLoss: 0.5545207262039185\n",
      "cnt: 0 - valLoss: 0.5588071346282959 - trainLoss: 0.554498553276062\n",
      "cnt: 0 - valLoss: 0.5587824583053589 - trainLoss: 0.5544763803482056\n",
      "cnt: 0 - valLoss: 0.5587579011917114 - trainLoss: 0.5544542074203491\n",
      "cnt: 0 - valLoss: 0.5587332248687744 - trainLoss: 0.5544320344924927\n",
      "cnt: 0 - valLoss: 0.5587085485458374 - trainLoss: 0.5544098615646362\n",
      "cnt: 0 - valLoss: 0.5586838722229004 - trainLoss: 0.5543876886367798\n",
      "cnt: 0 - valLoss: 0.5586592555046082 - trainLoss: 0.5543655753135681\n",
      "cnt: 0 - valLoss: 0.5586345791816711 - trainLoss: 0.5543434023857117\n",
      "cnt: 0 - valLoss: 0.5586099028587341 - trainLoss: 0.5543212294578552\n",
      "cnt: 0 - valLoss: 0.5585852861404419 - trainLoss: 0.5542990565299988\n",
      "cnt: 0 - valLoss: 0.5585606098175049 - trainLoss: 0.5542768836021423\n",
      "cnt: 0 - valLoss: 0.5585359334945679 - trainLoss: 0.5542547106742859\n",
      "cnt: 0 - valLoss: 0.5585112571716309 - trainLoss: 0.5542325377464294\n",
      "cnt: 0 - valLoss: 0.5584866404533386 - trainLoss: 0.554210364818573\n",
      "cnt: 0 - valLoss: 0.5584619045257568 - trainLoss: 0.5541881918907166\n",
      "cnt: 0 - valLoss: 0.5584372878074646 - trainLoss: 0.5541660189628601\n",
      "cnt: 0 - valLoss: 0.5584126114845276 - trainLoss: 0.5541438460350037\n",
      "cnt: 0 - valLoss: 0.5583879351615906 - trainLoss: 0.5541216731071472\n",
      "cnt: 0 - valLoss: 0.5583632588386536 - trainLoss: 0.5540995001792908\n",
      "cnt: 0 - valLoss: 0.5583385825157166 - trainLoss: 0.5540773272514343\n",
      "cnt: 0 - valLoss: 0.5583139061927795 - trainLoss: 0.5540551543235779\n",
      "cnt: 0 - valLoss: 0.5582892894744873 - trainLoss: 0.5540329813957214\n",
      "cnt: 0 - valLoss: 0.5582645535469055 - trainLoss: 0.554010808467865\n",
      "cnt: 0 - valLoss: 0.5582399368286133 - trainLoss: 0.5539886355400085\n",
      "cnt: 0 - valLoss: 0.5582152605056763 - trainLoss: 0.5539664626121521\n",
      "cnt: 0 - valLoss: 0.5581905245780945 - trainLoss: 0.5539442896842957\n",
      "cnt: 0 - valLoss: 0.5581659078598022 - trainLoss: 0.5539221167564392\n",
      "cnt: 0 - valLoss: 0.5581412315368652 - trainLoss: 0.5538999438285828\n",
      "cnt: 0 - valLoss: 0.5581165552139282 - trainLoss: 0.5538777709007263\n",
      "cnt: 0 - valLoss: 0.5580918192863464 - trainLoss: 0.5538555979728699\n",
      "cnt: 0 - valLoss: 0.5580671429634094 - trainLoss: 0.5538333654403687\n",
      "cnt: 0 - valLoss: 0.5580425262451172 - trainLoss: 0.5538111925125122\n",
      "cnt: 0 - valLoss: 0.5580177903175354 - trainLoss: 0.5537890791893005\n",
      "cnt: 0 - valLoss: 0.5579931139945984 - trainLoss: 0.5537669062614441\n",
      "cnt: 0 - valLoss: 0.5579684376716614 - trainLoss: 0.5537447333335876\n",
      "cnt: 0 - valLoss: 0.5579437613487244 - trainLoss: 0.5537225604057312\n",
      "cnt: 0 - valLoss: 0.5579190850257874 - trainLoss: 0.5537003874778748\n",
      "cnt: 0 - valLoss: 0.5578944087028503 - trainLoss: 0.5536782145500183\n",
      "cnt: 0 - valLoss: 0.5578697323799133 - trainLoss: 0.5536560416221619\n",
      "cnt: 0 - valLoss: 0.5578450560569763 - trainLoss: 0.5536338686943054\n",
      "cnt: 0 - valLoss: 0.5578203797340393 - trainLoss: 0.5536116361618042\n",
      "cnt: 0 - valLoss: 0.5577956438064575 - trainLoss: 0.5535894632339478\n",
      "cnt: 0 - valLoss: 0.5577709674835205 - trainLoss: 0.5535672903060913\n",
      "cnt: 0 - valLoss: 0.5577462911605835 - trainLoss: 0.5535451173782349\n",
      "cnt: 0 - valLoss: 0.5577216148376465 - trainLoss: 0.5535229444503784\n",
      "cnt: 0 - valLoss: 0.5576969385147095 - trainLoss: 0.553500771522522\n",
      "cnt: 0 - valLoss: 0.5576722025871277 - trainLoss: 0.5534785985946655\n",
      "cnt: 0 - valLoss: 0.5576475262641907 - trainLoss: 0.5534564256668091\n",
      "cnt: 0 - valLoss: 0.5576228499412537 - trainLoss: 0.5534342527389526\n",
      "cnt: 0 - valLoss: 0.5575981736183167 - trainLoss: 0.5534120798110962\n",
      "cnt: 0 - valLoss: 0.5575734376907349 - trainLoss: 0.5533899068832397\n",
      "cnt: 0 - valLoss: 0.5575487613677979 - trainLoss: 0.5533677339553833\n",
      "cnt: 0 - valLoss: 0.5575240850448608 - trainLoss: 0.5533455610275269\n",
      "cnt: 0 - valLoss: 0.5574992895126343 - trainLoss: 0.5533233880996704\n",
      "cnt: 0 - valLoss: 0.5574747323989868 - trainLoss: 0.5533011555671692\n",
      "cnt: 0 - valLoss: 0.5574499368667603 - trainLoss: 0.5532789826393127\n",
      "cnt: 0 - valLoss: 0.5574252605438232 - trainLoss: 0.5532568097114563\n",
      "cnt: 0 - valLoss: 0.5574005842208862 - trainLoss: 0.5532346367835999\n",
      "cnt: 0 - valLoss: 0.5573758482933044 - trainLoss: 0.5532124042510986\n",
      "cnt: 0 - valLoss: 0.5573511719703674 - trainLoss: 0.5531902313232422\n",
      "cnt: 0 - valLoss: 0.5573264360427856 - trainLoss: 0.5531680583953857\n",
      "cnt: 0 - valLoss: 0.5573017001152039 - trainLoss: 0.5531458854675293\n",
      "cnt: 0 - valLoss: 0.5572770237922668 - trainLoss: 0.5531237125396729\n",
      "cnt: 0 - valLoss: 0.5572522878646851 - trainLoss: 0.5531014800071716\n",
      "cnt: 0 - valLoss: 0.557227611541748 - trainLoss: 0.5530793070793152\n",
      "cnt: 0 - valLoss: 0.5572028756141663 - trainLoss: 0.553057074546814\n",
      "cnt: 0 - valLoss: 0.5571781992912292 - trainLoss: 0.5530349016189575\n",
      "cnt: 0 - valLoss: 0.5571535229682922 - trainLoss: 0.5530127286911011\n",
      "cnt: 0 - valLoss: 0.5571287870407104 - trainLoss: 0.5529906153678894\n",
      "cnt: 0 - valLoss: 0.5571041107177734 - trainLoss: 0.552968442440033\n",
      "cnt: 0 - valLoss: 0.5570793747901917 - trainLoss: 0.5529463291168213\n",
      "cnt: 0 - valLoss: 0.5570546984672546 - trainLoss: 0.5529241561889648\n",
      "cnt: 0 - valLoss: 0.5570299625396729 - trainLoss: 0.5529019832611084\n",
      "cnt: 0 - valLoss: 0.5570052862167358 - trainLoss: 0.552879810333252\n",
      "cnt: 0 - valLoss: 0.5569806098937988 - trainLoss: 0.5528576374053955\n",
      "cnt: 0 - valLoss: 0.5569559335708618 - trainLoss: 0.5528354644775391\n",
      "cnt: 0 - valLoss: 0.55693119764328 - trainLoss: 0.5528132319450378\n",
      "cnt: 0 - valLoss: 0.556906521320343 - trainLoss: 0.5527911186218262\n",
      "cnt: 0 - valLoss: 0.5568817853927612 - trainLoss: 0.552768886089325\n",
      "cnt: 0 - valLoss: 0.5568570494651794 - trainLoss: 0.5527467131614685\n",
      "cnt: 0 - valLoss: 0.5568323135375977 - trainLoss: 0.5527245402336121\n",
      "cnt: 0 - valLoss: 0.5568076372146606 - trainLoss: 0.5527023077011108\n",
      "cnt: 0 - valLoss: 0.5567829012870789 - trainLoss: 0.5526800751686096\n",
      "cnt: 0 - valLoss: 0.5567581653594971 - trainLoss: 0.5526579022407532\n",
      "cnt: 0 - valLoss: 0.5567334890365601 - trainLoss: 0.5526357889175415\n",
      "cnt: 0 - valLoss: 0.5567087531089783 - trainLoss: 0.5526135563850403\n",
      "cnt: 0 - valLoss: 0.5566840171813965 - trainLoss: 0.5525913238525391\n",
      "cnt: 0 - valLoss: 0.5566593408584595 - trainLoss: 0.5525691509246826\n",
      "cnt: 0 - valLoss: 0.5566346049308777 - trainLoss: 0.5525469779968262\n",
      "cnt: 0 - valLoss: 0.5566098690032959 - trainLoss: 0.5525248050689697\n",
      "cnt: 0 - valLoss: 0.5565851330757141 - trainLoss: 0.5525025725364685\n",
      "cnt: 0 - valLoss: 0.5565604567527771 - trainLoss: 0.5524803996086121\n",
      "cnt: 0 - valLoss: 0.5565357208251953 - trainLoss: 0.5524582266807556\n",
      "cnt: 0 - valLoss: 0.5565109848976135 - trainLoss: 0.5524359941482544\n",
      "cnt: 0 - valLoss: 0.5564863085746765 - trainLoss: 0.5524137616157532\n",
      "cnt: 0 - valLoss: 0.5564615726470947 - trainLoss: 0.5523916482925415\n",
      "cnt: 0 - valLoss: 0.5564367771148682 - trainLoss: 0.5523694157600403\n",
      "cnt: 0 - valLoss: 0.5564121007919312 - trainLoss: 0.5523471832275391\n",
      "cnt: 0 - valLoss: 0.5563873648643494 - trainLoss: 0.5523250102996826\n",
      "cnt: 0 - valLoss: 0.5563626289367676 - trainLoss: 0.5523027777671814\n",
      "cnt: 0 - valLoss: 0.5563378930091858 - trainLoss: 0.552280604839325\n",
      "cnt: 0 - valLoss: 0.556313157081604 - trainLoss: 0.5522584319114685\n",
      "cnt: 0 - valLoss: 0.556288480758667 - trainLoss: 0.5522361993789673\n",
      "cnt: 0 - valLoss: 0.5562636852264404 - trainLoss: 0.5522140264511108\n",
      "cnt: 0 - valLoss: 0.5562389492988586 - trainLoss: 0.5521917939186096\n",
      "cnt: 0 - valLoss: 0.5562142133712769 - trainLoss: 0.5521696209907532\n",
      "cnt: 0 - valLoss: 0.5561894774436951 - trainLoss: 0.5521474480628967\n",
      "cnt: 0 - valLoss: 0.5561648011207581 - trainLoss: 0.5521252155303955\n",
      "cnt: 0 - valLoss: 0.5561400651931763 - trainLoss: 0.5521029829978943\n",
      "cnt: 0 - valLoss: 0.5561153292655945 - trainLoss: 0.5520808100700378\n",
      "cnt: 0 - valLoss: 0.5560905337333679 - trainLoss: 0.5520586371421814\n",
      "cnt: 0 - valLoss: 0.5560658574104309 - trainLoss: 0.552036464214325\n",
      "cnt: 0 - valLoss: 0.5560410618782043 - trainLoss: 0.5520142912864685\n",
      "cnt: 0 - valLoss: 0.5560163855552673 - trainLoss: 0.5519920587539673\n",
      "cnt: 0 - valLoss: 0.5559915900230408 - trainLoss: 0.5519698262214661\n",
      "cnt: 0 - valLoss: 0.5559669137001038 - trainLoss: 0.5519476532936096\n",
      "cnt: 0 - valLoss: 0.555942177772522 - trainLoss: 0.5519254803657532\n",
      "cnt: 0 - valLoss: 0.5559174418449402 - trainLoss: 0.5519033074378967\n",
      "cnt: 0 - valLoss: 0.5558927059173584 - trainLoss: 0.5518811345100403\n",
      "cnt: 0 - valLoss: 0.5558679103851318 - trainLoss: 0.5518589019775391\n",
      "cnt: 0 - valLoss: 0.5558432340621948 - trainLoss: 0.5518367290496826\n",
      "cnt: 0 - valLoss: 0.5558184385299683 - trainLoss: 0.5518144965171814\n",
      "cnt: 0 - valLoss: 0.5557937622070312 - trainLoss: 0.5517923831939697\n",
      "cnt: 0 - valLoss: 0.5557689666748047 - trainLoss: 0.5517702102661133\n",
      "cnt: 0 - valLoss: 0.5557442903518677 - trainLoss: 0.5517479777336121\n",
      "cnt: 0 - valLoss: 0.5557195544242859 - trainLoss: 0.5517258048057556\n",
      "cnt: 0 - valLoss: 0.5556948184967041 - trainLoss: 0.5517036318778992\n",
      "cnt: 0 - valLoss: 0.5556700229644775 - trainLoss: 0.5516813397407532\n",
      "cnt: 0 - valLoss: 0.5556452870368958 - trainLoss: 0.5516592264175415\n",
      "cnt: 0 - valLoss: 0.5556204915046692 - trainLoss: 0.5516370534896851\n",
      "cnt: 0 - valLoss: 0.5555958151817322 - trainLoss: 0.5516148805618286\n",
      "cnt: 0 - valLoss: 0.5555711388587952 - trainLoss: 0.5515927076339722\n",
      "cnt: 0 - valLoss: 0.5555464625358582 - trainLoss: 0.551570475101471\n",
      "cnt: 0 - valLoss: 0.5555217862129211 - trainLoss: 0.5515483021736145\n",
      "cnt: 0 - valLoss: 0.5554970502853394 - trainLoss: 0.5515260696411133\n",
      "cnt: 0 - valLoss: 0.5554723739624023 - trainLoss: 0.5515038967132568\n",
      "cnt: 0 - valLoss: 0.5554476976394653 - trainLoss: 0.5514817237854004\n",
      "cnt: 0 - valLoss: 0.5554230213165283 - trainLoss: 0.551459550857544\n",
      "cnt: 0 - valLoss: 0.5553983449935913 - trainLoss: 0.5514373779296875\n",
      "cnt: 0 - valLoss: 0.5553736090660095 - trainLoss: 0.551415205001831\n",
      "cnt: 0 - valLoss: 0.5553488731384277 - trainLoss: 0.5513930320739746\n",
      "cnt: 0 - valLoss: 0.5553241968154907 - trainLoss: 0.5513708591461182\n",
      "cnt: 0 - valLoss: 0.5552994608879089 - trainLoss: 0.5513486266136169\n",
      "cnt: 0 - valLoss: 0.5552747845649719 - trainLoss: 0.5513264536857605\n",
      "cnt: 0 - valLoss: 0.5552501082420349 - trainLoss: 0.551304280757904\n",
      "cnt: 0 - valLoss: 0.5552254319190979 - trainLoss: 0.5512820482254028\n",
      "cnt: 0 - valLoss: 0.5552006363868713 - trainLoss: 0.5512598752975464\n",
      "cnt: 0 - valLoss: 0.5551759600639343 - trainLoss: 0.5512377023696899\n",
      "cnt: 0 - valLoss: 0.5551512837409973 - trainLoss: 0.5512155294418335\n",
      "cnt: 0 - valLoss: 0.5551265478134155 - trainLoss: 0.551193356513977\n",
      "cnt: 0 - valLoss: 0.5551018118858337 - trainLoss: 0.5511711835861206\n",
      "cnt: 0 - valLoss: 0.555077075958252 - trainLoss: 0.5511489510536194\n",
      "cnt: 0 - valLoss: 0.5550523400306702 - trainLoss: 0.5511267781257629\n",
      "cnt: 0 - valLoss: 0.5550276041030884 - trainLoss: 0.5511045455932617\n",
      "cnt: 0 - valLoss: 0.5550028681755066 - trainLoss: 0.55108243227005\n",
      "cnt: 0 - valLoss: 0.5549781322479248 - trainLoss: 0.5510602593421936\n",
      "cnt: 0 - valLoss: 0.5549534559249878 - trainLoss: 0.5510380864143372\n",
      "cnt: 0 - valLoss: 0.554928719997406 - trainLoss: 0.5510159134864807\n",
      "cnt: 0 - valLoss: 0.5549039244651794 - trainLoss: 0.5509936809539795\n",
      "cnt: 0 - valLoss: 0.5548792481422424 - trainLoss: 0.550971508026123\n",
      "cnt: 0 - valLoss: 0.5548545122146606 - trainLoss: 0.5509492754936218\n",
      "cnt: 0 - valLoss: 0.5548297762870789 - trainLoss: 0.5509271025657654\n",
      "cnt: 0 - valLoss: 0.5548050403594971 - trainLoss: 0.5509049296379089\n",
      "cnt: 0 - valLoss: 0.5547803044319153 - trainLoss: 0.5508827567100525\n",
      "cnt: 0 - valLoss: 0.5547555088996887 - trainLoss: 0.550860583782196\n",
      "cnt: 0 - valLoss: 0.5547308325767517 - trainLoss: 0.5508383512496948\n",
      "cnt: 0 - valLoss: 0.5547060370445251 - trainLoss: 0.5508161783218384\n",
      "cnt: 0 - valLoss: 0.5546813011169434 - trainLoss: 0.5507940053939819\n",
      "cnt: 0 - valLoss: 0.5546566247940063 - trainLoss: 0.5507718920707703\n",
      "cnt: 0 - valLoss: 0.5546318888664246 - trainLoss: 0.550749659538269\n",
      "cnt: 0 - valLoss: 0.5546071529388428 - trainLoss: 0.5507274866104126\n",
      "cnt: 0 - valLoss: 0.554582417011261 - trainLoss: 0.5507053136825562\n",
      "cnt: 0 - valLoss: 0.5545576214790344 - trainLoss: 0.5506830811500549\n",
      "cnt: 0 - valLoss: 0.5545329451560974 - trainLoss: 0.5506609082221985\n",
      "cnt: 0 - valLoss: 0.5545081496238708 - trainLoss: 0.550638735294342\n",
      "cnt: 0 - valLoss: 0.5544834136962891 - trainLoss: 0.5506165623664856\n",
      "cnt: 0 - valLoss: 0.5544586777687073 - trainLoss: 0.5505943894386292\n",
      "cnt: 0 - valLoss: 0.5544339418411255 - trainLoss: 0.5505722165107727\n",
      "cnt: 0 - valLoss: 0.5544092059135437 - trainLoss: 0.5505500435829163\n",
      "cnt: 0 - valLoss: 0.5543844699859619 - trainLoss: 0.5505278706550598\n",
      "cnt: 0 - valLoss: 0.5543596744537354 - trainLoss: 0.5505056977272034\n",
      "cnt: 0 - valLoss: 0.5543349385261536 - trainLoss: 0.5504835247993469\n",
      "cnt: 0 - valLoss: 0.5543102025985718 - trainLoss: 0.5504612922668457\n",
      "cnt: 0 - valLoss: 0.5542855262756348 - trainLoss: 0.5504391193389893\n",
      "cnt: 0 - valLoss: 0.5542607307434082 - trainLoss: 0.5504170060157776\n",
      "cnt: 0 - valLoss: 0.5542359948158264 - trainLoss: 0.5503947734832764\n",
      "cnt: 0 - valLoss: 0.5542111992835999 - trainLoss: 0.5503726005554199\n",
      "cnt: 0 - valLoss: 0.5541865229606628 - trainLoss: 0.5503504276275635\n",
      "cnt: 0 - valLoss: 0.554161787033081 - trainLoss: 0.550328254699707\n",
      "cnt: 0 - valLoss: 0.5541369915008545 - trainLoss: 0.5503060817718506\n",
      "cnt: 0 - valLoss: 0.5541123151779175 - trainLoss: 0.5502839088439941\n",
      "cnt: 0 - valLoss: 0.5540875196456909 - trainLoss: 0.5502617359161377\n",
      "cnt: 0 - valLoss: 0.5540627241134644 - trainLoss: 0.5502395629882812\n",
      "cnt: 0 - valLoss: 0.5540379881858826 - trainLoss: 0.5502173900604248\n",
      "cnt: 0 - valLoss: 0.5540133118629456 - trainLoss: 0.5501952171325684\n",
      "cnt: 0 - valLoss: 0.553988516330719 - trainLoss: 0.5501729846000671\n",
      "cnt: 0 - valLoss: 0.5539637207984924 - trainLoss: 0.5501508116722107\n",
      "cnt: 0 - valLoss: 0.5539390444755554 - trainLoss: 0.5501286387443542\n",
      "cnt: 0 - valLoss: 0.5539142489433289 - trainLoss: 0.5501065254211426\n",
      "cnt: 0 - valLoss: 0.5538895130157471 - trainLoss: 0.5500842928886414\n",
      "cnt: 0 - valLoss: 0.5538648366928101 - trainLoss: 0.5500621199607849\n",
      "cnt: 0 - valLoss: 0.5538400411605835 - trainLoss: 0.5500399470329285\n",
      "cnt: 0 - valLoss: 0.5538153052330017 - trainLoss: 0.550017774105072\n",
      "cnt: 0 - valLoss: 0.5537905097007751 - trainLoss: 0.5499956011772156\n",
      "cnt: 0 - valLoss: 0.5537657737731934 - trainLoss: 0.5499734282493591\n",
      "cnt: 0 - valLoss: 0.5537410378456116 - trainLoss: 0.5499512553215027\n",
      "cnt: 0 - valLoss: 0.553716242313385 - trainLoss: 0.5499290823936462\n",
      "cnt: 0 - valLoss: 0.5536915063858032 - trainLoss: 0.5499069094657898\n",
      "cnt: 0 - valLoss: 0.5536667704582214 - trainLoss: 0.5498847365379333\n",
      "cnt: 0 - valLoss: 0.5536420345306396 - trainLoss: 0.5498625636100769\n",
      "cnt: 0 - valLoss: 0.5536172986030579 - trainLoss: 0.5498402714729309\n",
      "cnt: 0 - valLoss: 0.5535925030708313 - trainLoss: 0.5498180985450745\n",
      "cnt: 0 - valLoss: 0.5535677671432495 - trainLoss: 0.5497959852218628\n",
      "cnt: 0 - valLoss: 0.5535430312156677 - trainLoss: 0.5497738122940063\n",
      "cnt: 0 - valLoss: 0.5535182356834412 - trainLoss: 0.5497516393661499\n",
      "cnt: 0 - valLoss: 0.5534934401512146 - trainLoss: 0.5497294664382935\n",
      "cnt: 0 - valLoss: 0.5534687638282776 - trainLoss: 0.549707293510437\n",
      "cnt: 0 - valLoss: 0.553443968296051 - trainLoss: 0.5496851205825806\n",
      "cnt: 0 - valLoss: 0.5534192323684692 - trainLoss: 0.5496629476547241\n",
      "cnt: 0 - valLoss: 0.5533944368362427 - trainLoss: 0.5496407747268677\n",
      "cnt: 0 - valLoss: 0.5533697009086609 - trainLoss: 0.5496186017990112\n",
      "cnt: 0 - valLoss: 0.5533449649810791 - trainLoss: 0.5495964288711548\n",
      "cnt: 0 - valLoss: 0.5533202290534973 - trainLoss: 0.5495742559432983\n",
      "cnt: 0 - valLoss: 0.5532954335212708 - trainLoss: 0.5495520830154419\n",
      "cnt: 0 - valLoss: 0.553270697593689 - trainLoss: 0.5495299100875854\n",
      "cnt: 0 - valLoss: 0.5532459616661072 - trainLoss: 0.549507737159729\n",
      "cnt: 0 - valLoss: 0.5532212257385254 - trainLoss: 0.5494855642318726\n",
      "cnt: 0 - valLoss: 0.5531964302062988 - trainLoss: 0.5494633913040161\n",
      "cnt: 0 - valLoss: 0.5531716346740723 - trainLoss: 0.5494412183761597\n",
      "cnt: 0 - valLoss: 0.5531468987464905 - trainLoss: 0.5494190454483032\n",
      "cnt: 0 - valLoss: 0.5531221628189087 - trainLoss: 0.5493968725204468\n",
      "cnt: 0 - valLoss: 0.5530974268913269 - trainLoss: 0.5493746995925903\n",
      "cnt: 0 - valLoss: 0.5530725717544556 - trainLoss: 0.5493525862693787\n",
      "cnt: 0 - valLoss: 0.5530478954315186 - trainLoss: 0.5493304133415222\n",
      "cnt: 0 - valLoss: 0.553023099899292 - trainLoss: 0.5493083000183105\n",
      "cnt: 0 - valLoss: 0.5529983043670654 - trainLoss: 0.5492861270904541\n",
      "cnt: 0 - valLoss: 0.5529735684394836 - trainLoss: 0.5492639541625977\n",
      "cnt: 0 - valLoss: 0.5529488325119019 - trainLoss: 0.549241840839386\n",
      "cnt: 0 - valLoss: 0.5529240965843201 - trainLoss: 0.5492196679115295\n",
      "cnt: 0 - valLoss: 0.5528993606567383 - trainLoss: 0.5491974949836731\n",
      "cnt: 0 - valLoss: 0.5528745055198669 - trainLoss: 0.5491753816604614\n",
      "cnt: 0 - valLoss: 0.5528498291969299 - trainLoss: 0.5491532683372498\n",
      "cnt: 0 - valLoss: 0.5528250932693481 - trainLoss: 0.5491310954093933\n",
      "cnt: 0 - valLoss: 0.5528002977371216 - trainLoss: 0.5491089224815369\n",
      "cnt: 0 - valLoss: 0.5527755618095398 - trainLoss: 0.5490868091583252\n",
      "cnt: 0 - valLoss: 0.5527507662773132 - trainLoss: 0.5490646362304688\n",
      "cnt: 0 - valLoss: 0.5527260303497314 - trainLoss: 0.5490424633026123\n",
      "cnt: 0 - valLoss: 0.5527012944221497 - trainLoss: 0.5490203499794006\n",
      "cnt: 0 - valLoss: 0.5526764988899231 - trainLoss: 0.548998236656189\n",
      "cnt: 0 - valLoss: 0.5526517629623413 - trainLoss: 0.5489760637283325\n",
      "cnt: 0 - valLoss: 0.5526269674301147 - trainLoss: 0.5489538908004761\n",
      "cnt: 0 - valLoss: 0.5526021718978882 - trainLoss: 0.5489317774772644\n",
      "cnt: 0 - valLoss: 0.5525773763656616 - trainLoss: 0.548909604549408\n",
      "cnt: 0 - valLoss: 0.5525526404380798 - trainLoss: 0.5488874316215515\n",
      "cnt: 0 - valLoss: 0.5525277853012085 - trainLoss: 0.5488653182983398\n",
      "cnt: 0 - valLoss: 0.5525030493736267 - trainLoss: 0.5488432049751282\n",
      "cnt: 0 - valLoss: 0.5524782538414001 - trainLoss: 0.5488210320472717\n",
      "cnt: 0 - valLoss: 0.5524534583091736 - trainLoss: 0.5487988591194153\n",
      "cnt: 0 - valLoss: 0.552428662776947 - trainLoss: 0.5487767457962036\n",
      "cnt: 0 - valLoss: 0.5524039268493652 - trainLoss: 0.5487545728683472\n",
      "cnt: 0 - valLoss: 0.5523791313171387 - trainLoss: 0.5487323999404907\n",
      "cnt: 0 - valLoss: 0.5523543357849121 - trainLoss: 0.5487103462219238\n",
      "cnt: 0 - valLoss: 0.5523295998573303 - trainLoss: 0.5486881732940674\n",
      "cnt: 0 - valLoss: 0.5523048043251038 - trainLoss: 0.5486660003662109\n",
      "cnt: 0 - valLoss: 0.552280068397522 - trainLoss: 0.5486438870429993\n",
      "cnt: 0 - valLoss: 0.5522552728652954 - trainLoss: 0.5486217141151428\n",
      "cnt: 0 - valLoss: 0.5522304773330688 - trainLoss: 0.5485995411872864\n",
      "cnt: 0 - valLoss: 0.5522056221961975 - trainLoss: 0.5485774278640747\n",
      "cnt: 0 - valLoss: 0.5521808862686157 - trainLoss: 0.548555314540863\n",
      "cnt: 0 - valLoss: 0.5521560907363892 - trainLoss: 0.5485331416130066\n",
      "cnt: 0 - valLoss: 0.5521312952041626 - trainLoss: 0.5485110282897949\n",
      "cnt: 0 - valLoss: 0.5521065592765808 - trainLoss: 0.5484888553619385\n",
      "cnt: 0 - valLoss: 0.5520817637443542 - trainLoss: 0.5484667420387268\n",
      "cnt: 0 - valLoss: 0.5520569682121277 - trainLoss: 0.5484445095062256\n",
      "cnt: 0 - valLoss: 0.5520322322845459 - trainLoss: 0.5484224557876587\n",
      "cnt: 0 - valLoss: 0.5520073771476746 - trainLoss: 0.5484002828598022\n",
      "cnt: 0 - valLoss: 0.5519826412200928 - trainLoss: 0.5483781695365906\n",
      "cnt: 0 - valLoss: 0.5519578456878662 - trainLoss: 0.5483559966087341\n",
      "cnt: 0 - valLoss: 0.5519330501556396 - trainLoss: 0.5483338236808777\n",
      "cnt: 0 - valLoss: 0.5519083142280579 - trainLoss: 0.548311710357666\n",
      "cnt: 0 - valLoss: 0.5518834590911865 - trainLoss: 0.5482895970344543\n",
      "cnt: 0 - valLoss: 0.5518587231636047 - trainLoss: 0.5482674837112427\n",
      "cnt: 0 - valLoss: 0.5518338680267334 - trainLoss: 0.5482453107833862\n",
      "cnt: 0 - valLoss: 0.5518091320991516 - trainLoss: 0.5482231378555298\n",
      "cnt: 0 - valLoss: 0.5517843961715698 - trainLoss: 0.5482010841369629\n",
      "cnt: 0 - valLoss: 0.5517595410346985 - trainLoss: 0.5481789112091064\n",
      "cnt: 0 - valLoss: 0.5517348051071167 - trainLoss: 0.5481567978858948\n",
      "cnt: 0 - valLoss: 0.5517100095748901 - trainLoss: 0.5481346249580383\n",
      "cnt: 0 - valLoss: 0.5516852140426636 - trainLoss: 0.5481124520301819\n",
      "cnt: 0 - valLoss: 0.5516603589057922 - trainLoss: 0.5480903387069702\n",
      "cnt: 0 - valLoss: 0.5516356825828552 - trainLoss: 0.5480682253837585\n",
      "cnt: 0 - valLoss: 0.5516108274459839 - trainLoss: 0.5480461120605469\n",
      "cnt: 0 - valLoss: 0.5515860915184021 - trainLoss: 0.5480239391326904\n",
      "cnt: 0 - valLoss: 0.5515612959861755 - trainLoss: 0.5480018258094788\n",
      "cnt: 0 - valLoss: 0.551536500453949 - trainLoss: 0.5479797124862671\n",
      "cnt: 0 - valLoss: 0.5515117049217224 - trainLoss: 0.5479575991630554\n",
      "cnt: 0 - valLoss: 0.5514869093894958 - trainLoss: 0.547935426235199\n",
      "cnt: 0 - valLoss: 0.5514621734619141 - trainLoss: 0.5479132533073425\n",
      "cnt: 0 - valLoss: 0.5514373779296875 - trainLoss: 0.5478911399841309\n",
      "cnt: 0 - valLoss: 0.5514125823974609 - trainLoss: 0.5478690266609192\n",
      "cnt: 0 - valLoss: 0.5513877272605896 - trainLoss: 0.5478469133377075\n",
      "cnt: 0 - valLoss: 0.5513629913330078 - trainLoss: 0.5478247404098511\n",
      "cnt: 0 - valLoss: 0.5513381958007812 - trainLoss: 0.5478026270866394\n",
      "cnt: 0 - valLoss: 0.5513134002685547 - trainLoss: 0.547780454158783\n",
      "cnt: 0 - valLoss: 0.5512886643409729 - trainLoss: 0.5477583408355713\n",
      "cnt: 0 - valLoss: 0.5512638092041016 - trainLoss: 0.5477362275123596\n",
      "cnt: 0 - valLoss: 0.551239013671875 - trainLoss: 0.5477140545845032\n",
      "cnt: 0 - valLoss: 0.5512142777442932 - trainLoss: 0.5476920008659363\n",
      "cnt: 0 - valLoss: 0.5511894822120667 - trainLoss: 0.5476698279380798\n",
      "cnt: 0 - valLoss: 0.5511646866798401 - trainLoss: 0.5476476550102234\n",
      "cnt: 0 - valLoss: 0.5511398911476135 - trainLoss: 0.5476255416870117\n",
      "cnt: 0 - valLoss: 0.551115095615387 - trainLoss: 0.5476034283638\n",
      "cnt: 0 - valLoss: 0.5510903000831604 - trainLoss: 0.5475813150405884\n",
      "cnt: 0 - valLoss: 0.5510655641555786 - trainLoss: 0.5475592017173767\n",
      "cnt: 0 - valLoss: 0.551040768623352 - trainLoss: 0.5475370287895203\n",
      "cnt: 0 - valLoss: 0.5510159730911255 - trainLoss: 0.5475149750709534\n",
      "cnt: 0 - valLoss: 0.5509911775588989 - trainLoss: 0.5474928021430969\n",
      "cnt: 0 - valLoss: 0.5509664416313171 - trainLoss: 0.5474706888198853\n",
      "cnt: 0 - valLoss: 0.550941526889801 - trainLoss: 0.5474485158920288\n",
      "cnt: 0 - valLoss: 0.5509167909622192 - trainLoss: 0.5474264025688171\n",
      "cnt: 0 - valLoss: 0.5508919954299927 - trainLoss: 0.5474042892456055\n",
      "cnt: 0 - valLoss: 0.5508671998977661 - trainLoss: 0.5473821759223938\n",
      "cnt: 0 - valLoss: 0.5508424043655396 - trainLoss: 0.5473600029945374\n",
      "cnt: 0 - valLoss: 0.550817608833313 - trainLoss: 0.5473378896713257\n",
      "cnt: 0 - valLoss: 0.5507928133010864 - trainLoss: 0.547315776348114\n",
      "cnt: 0 - valLoss: 0.5507680773735046 - trainLoss: 0.5472936034202576\n",
      "cnt: 0 - valLoss: 0.5507432818412781 - trainLoss: 0.5472714900970459\n",
      "cnt: 0 - valLoss: 0.5507184863090515 - trainLoss: 0.5472493171691895\n",
      "cnt: 0 - valLoss: 0.550693690776825 - trainLoss: 0.5472272634506226\n",
      "cnt: 0 - valLoss: 0.5506688952445984 - trainLoss: 0.5472050905227661\n",
      "cnt: 0 - valLoss: 0.5506440997123718 - trainLoss: 0.5471829771995544\n",
      "cnt: 0 - valLoss: 0.5506193041801453 - trainLoss: 0.5471608638763428\n",
      "cnt: 0 - valLoss: 0.5505945086479187 - trainLoss: 0.5471387505531311\n",
      "cnt: 0 - valLoss: 0.5505697131156921 - trainLoss: 0.5471166372299194\n",
      "cnt: 0 - valLoss: 0.5505449175834656 - trainLoss: 0.5470945239067078\n",
      "cnt: 0 - valLoss: 0.550520122051239 - trainLoss: 0.5470724105834961\n",
      "cnt: 0 - valLoss: 0.5504953265190125 - trainLoss: 0.5470502972602844\n",
      "cnt: 0 - valLoss: 0.5504705905914307 - trainLoss: 0.5470281839370728\n",
      "cnt: 0 - valLoss: 0.5504457950592041 - trainLoss: 0.5470060706138611\n",
      "cnt: 0 - valLoss: 0.5504209995269775 - trainLoss: 0.5469839572906494\n",
      "cnt: 0 - valLoss: 0.550396203994751 - trainLoss: 0.5469619035720825\n",
      "cnt: 0 - valLoss: 0.5503714680671692 - trainLoss: 0.5469397902488708\n",
      "cnt: 0 - valLoss: 0.5503466129302979 - trainLoss: 0.546917736530304\n",
      "cnt: 0 - valLoss: 0.5503218770027161 - trainLoss: 0.5468955636024475\n",
      "cnt: 0 - valLoss: 0.5502970814704895 - trainLoss: 0.5468735098838806\n",
      "cnt: 0 - valLoss: 0.5502722859382629 - trainLoss: 0.546851396560669\n",
      "cnt: 0 - valLoss: 0.5502475500106812 - trainLoss: 0.546829342842102\n",
      "cnt: 0 - valLoss: 0.5502227544784546 - trainLoss: 0.5468072295188904\n",
      "cnt: 0 - valLoss: 0.550197958946228 - trainLoss: 0.5467851161956787\n",
      "cnt: 0 - valLoss: 0.5501731634140015 - trainLoss: 0.546763002872467\n",
      "cnt: 0 - valLoss: 0.5501483678817749 - trainLoss: 0.5467409491539001\n",
      "cnt: 0 - valLoss: 0.5501236319541931 - trainLoss: 0.5467188358306885\n",
      "cnt: 0 - valLoss: 0.5500988364219666 - trainLoss: 0.5466967821121216\n",
      "cnt: 0 - valLoss: 0.55007404088974 - trainLoss: 0.5466746687889099\n",
      "cnt: 0 - valLoss: 0.5500491857528687 - trainLoss: 0.546652615070343\n",
      "cnt: 0 - valLoss: 0.5500245094299316 - trainLoss: 0.5466304421424866\n",
      "cnt: 0 - valLoss: 0.5499997138977051 - trainLoss: 0.5466083884239197\n",
      "cnt: 0 - valLoss: 0.5499749183654785 - trainLoss: 0.546586275100708\n",
      "cnt: 0 - valLoss: 0.549950122833252 - trainLoss: 0.5465642809867859\n",
      "cnt: 0 - valLoss: 0.5499253273010254 - trainLoss: 0.5465421080589294\n",
      "cnt: 0 - valLoss: 0.5499005317687988 - trainLoss: 0.5465199947357178\n",
      "cnt: 0 - valLoss: 0.5498757362365723 - trainLoss: 0.5464979410171509\n",
      "cnt: 0 - valLoss: 0.5498510003089905 - trainLoss: 0.5464758276939392\n",
      "cnt: 0 - valLoss: 0.5498262047767639 - trainLoss: 0.5464537739753723\n",
      "cnt: 0 - valLoss: 0.5498014092445374 - trainLoss: 0.5464316606521606\n",
      "cnt: 0 - valLoss: 0.5497766733169556 - trainLoss: 0.546409547328949\n",
      "cnt: 0 - valLoss: 0.549751877784729 - trainLoss: 0.5463874936103821\n",
      "cnt: 0 - valLoss: 0.5497270822525024 - trainLoss: 0.5463653802871704\n",
      "cnt: 0 - valLoss: 0.5497022867202759 - trainLoss: 0.5463433265686035\n",
      "cnt: 0 - valLoss: 0.5496774911880493 - trainLoss: 0.5463212132453918\n",
      "cnt: 0 - valLoss: 0.5496526956558228 - trainLoss: 0.5462992191314697\n",
      "cnt: 0 - valLoss: 0.5496279001235962 - trainLoss: 0.5462770462036133\n",
      "cnt: 0 - valLoss: 0.5496031641960144 - trainLoss: 0.5462549924850464\n",
      "cnt: 0 - valLoss: 0.5495783686637878 - trainLoss: 0.5462329387664795\n",
      "cnt: 0 - valLoss: 0.549553632736206 - trainLoss: 0.5462108254432678\n",
      "cnt: 0 - valLoss: 0.5495288372039795 - trainLoss: 0.5461888313293457\n",
      "cnt: 0 - valLoss: 0.5495040416717529 - trainLoss: 0.5461666584014893\n",
      "cnt: 0 - valLoss: 0.5494793057441711 - trainLoss: 0.5461446046829224\n",
      "cnt: 0 - valLoss: 0.5494545102119446 - trainLoss: 0.5461224913597107\n",
      "cnt: 0 - valLoss: 0.549429714679718 - trainLoss: 0.5461004376411438\n",
      "cnt: 0 - valLoss: 0.5494049787521362 - trainLoss: 0.5460783243179321\n",
      "cnt: 0 - valLoss: 0.5493801832199097 - trainLoss: 0.5460562705993652\n",
      "cnt: 0 - valLoss: 0.5493554472923279 - trainLoss: 0.5460341572761536\n",
      "cnt: 0 - valLoss: 0.5493306517601013 - trainLoss: 0.5460121035575867\n",
      "cnt: 0 - valLoss: 0.5493058562278748 - trainLoss: 0.5459900498390198\n",
      "cnt: 0 - valLoss: 0.549281120300293 - trainLoss: 0.5459679365158081\n",
      "cnt: 0 - valLoss: 0.5492563843727112 - trainLoss: 0.5459458827972412\n",
      "cnt: 0 - valLoss: 0.5492315888404846 - trainLoss: 0.5459237694740295\n",
      "cnt: 0 - valLoss: 0.5492068529129028 - trainLoss: 0.5459017157554626\n",
      "cnt: 0 - valLoss: 0.5491820573806763 - trainLoss: 0.5458796620368958\n",
      "cnt: 0 - valLoss: 0.5491573214530945 - trainLoss: 0.5458575487136841\n",
      "cnt: 0 - valLoss: 0.5491325259208679 - trainLoss: 0.5458354949951172\n",
      "cnt: 0 - valLoss: 0.5491077899932861 - trainLoss: 0.5458134412765503\n",
      "cnt: 0 - valLoss: 0.5490829944610596 - trainLoss: 0.5457913279533386\n",
      "cnt: 0 - valLoss: 0.549058198928833 - trainLoss: 0.5457692742347717\n",
      "cnt: 0 - valLoss: 0.5490334630012512 - trainLoss: 0.5457471609115601\n",
      "cnt: 0 - valLoss: 0.5490087270736694 - trainLoss: 0.5457251071929932\n",
      "cnt: 0 - valLoss: 0.5489839315414429 - trainLoss: 0.545703113079071\n",
      "cnt: 0 - valLoss: 0.5489591360092163 - trainLoss: 0.5456809997558594\n",
      "cnt: 0 - valLoss: 0.5489344000816345 - trainLoss: 0.5456589460372925\n",
      "cnt: 0 - valLoss: 0.5489096641540527 - trainLoss: 0.5456368327140808\n",
      "cnt: 0 - valLoss: 0.5488848686218262 - trainLoss: 0.5456147193908691\n",
      "cnt: 0 - valLoss: 0.5488600730895996 - trainLoss: 0.545592725276947\n",
      "cnt: 0 - valLoss: 0.548835277557373 - trainLoss: 0.5455705523490906\n",
      "cnt: 0 - valLoss: 0.5488105416297913 - trainLoss: 0.5455485582351685\n",
      "cnt: 0 - valLoss: 0.5487858057022095 - trainLoss: 0.5455264449119568\n",
      "cnt: 0 - valLoss: 0.5487610101699829 - trainLoss: 0.5455043911933899\n",
      "cnt: 0 - valLoss: 0.5487362146377563 - trainLoss: 0.545482337474823\n",
      "cnt: 0 - valLoss: 0.5487114787101746 - trainLoss: 0.5454602837562561\n",
      "cnt: 0 - valLoss: 0.548686683177948 - trainLoss: 0.5454382300376892\n",
      "cnt: 0 - valLoss: 0.5486618876457214 - trainLoss: 0.5454161763191223\n",
      "cnt: 0 - valLoss: 0.5486371517181396 - trainLoss: 0.5453940629959106\n",
      "cnt: 0 - valLoss: 0.5486124157905579 - trainLoss: 0.545371949672699\n",
      "cnt: 0 - valLoss: 0.5485875606536865 - trainLoss: 0.5453499555587769\n",
      "cnt: 0 - valLoss: 0.5485628843307495 - trainLoss: 0.54532790184021\n",
      "cnt: 0 - valLoss: 0.5485381484031677 - trainLoss: 0.5453057885169983\n",
      "cnt: 0 - valLoss: 0.5485133528709412 - trainLoss: 0.5452837347984314\n",
      "cnt: 0 - valLoss: 0.5484885573387146 - trainLoss: 0.5452616810798645\n",
      "cnt: 0 - valLoss: 0.5484638214111328 - trainLoss: 0.5452396273612976\n",
      "cnt: 0 - valLoss: 0.5484390258789062 - trainLoss: 0.5452175736427307\n",
      "cnt: 0 - valLoss: 0.5484142303466797 - trainLoss: 0.545195460319519\n",
      "cnt: 0 - valLoss: 0.5483894944190979 - trainLoss: 0.5451734066009521\n",
      "cnt: 0 - valLoss: 0.5483646988868713 - trainLoss: 0.5451513528823853\n",
      "cnt: 0 - valLoss: 0.5483399629592896 - trainLoss: 0.5451292991638184\n",
      "cnt: 0 - valLoss: 0.5483152270317078 - trainLoss: 0.5451072454452515\n",
      "cnt: 0 - valLoss: 0.548290491104126 - trainLoss: 0.5450851917266846\n",
      "cnt: 0 - valLoss: 0.5482656955718994 - trainLoss: 0.5450631380081177\n",
      "cnt: 0 - valLoss: 0.5482409000396729 - trainLoss: 0.5450410842895508\n",
      "cnt: 0 - valLoss: 0.5482161641120911 - trainLoss: 0.5450189709663391\n",
      "cnt: 0 - valLoss: 0.5481914281845093 - trainLoss: 0.544996976852417\n",
      "cnt: 0 - valLoss: 0.5481666326522827 - trainLoss: 0.5449749231338501\n",
      "cnt: 0 - valLoss: 0.5481418371200562 - trainLoss: 0.5449528098106384\n",
      "cnt: 0 - valLoss: 0.5481171011924744 - trainLoss: 0.5449307560920715\n",
      "cnt: 0 - valLoss: 0.5480923056602478 - trainLoss: 0.5449087619781494\n",
      "cnt: 0 - valLoss: 0.548067569732666 - trainLoss: 0.5448867082595825\n",
      "cnt: 0 - valLoss: 0.5480428338050842 - trainLoss: 0.5448645949363708\n",
      "cnt: 0 - valLoss: 0.5480180382728577 - trainLoss: 0.544842541217804\n",
      "cnt: 0 - valLoss: 0.5479933023452759 - trainLoss: 0.5448205471038818\n",
      "cnt: 0 - valLoss: 0.5479685068130493 - trainLoss: 0.5447984933853149\n",
      "cnt: 0 - valLoss: 0.5479437708854675 - trainLoss: 0.544776439666748\n",
      "cnt: 0 - valLoss: 0.547918975353241 - trainLoss: 0.5447543263435364\n",
      "cnt: 0 - valLoss: 0.5478942394256592 - trainLoss: 0.5447323322296143\n",
      "cnt: 0 - valLoss: 0.5478695034980774 - trainLoss: 0.5447102785110474\n",
      "cnt: 0 - valLoss: 0.5478447079658508 - trainLoss: 0.5446881651878357\n",
      "cnt: 0 - valLoss: 0.5478199124336243 - trainLoss: 0.5446661114692688\n",
      "cnt: 0 - valLoss: 0.5477951765060425 - trainLoss: 0.5446441173553467\n",
      "cnt: 0 - valLoss: 0.5477703809738159 - trainLoss: 0.5446220636367798\n",
      "cnt: 0 - valLoss: 0.5477457046508789 - trainLoss: 0.5446000099182129\n",
      "cnt: 0 - valLoss: 0.5477209091186523 - trainLoss: 0.5445780158042908\n",
      "cnt: 0 - valLoss: 0.5476961731910706 - trainLoss: 0.5445559024810791\n",
      "cnt: 0 - valLoss: 0.5476714372634888 - trainLoss: 0.544533908367157\n",
      "cnt: 0 - valLoss: 0.547646701335907 - trainLoss: 0.5445119142532349\n",
      "cnt: 0 - valLoss: 0.5476219654083252 - trainLoss: 0.544489860534668\n",
      "cnt: 0 - valLoss: 0.5475972294807434 - trainLoss: 0.5444678068161011\n",
      "cnt: 0 - valLoss: 0.5475724339485168 - trainLoss: 0.544445812702179\n",
      "cnt: 0 - valLoss: 0.5475477576255798 - trainLoss: 0.5444236993789673\n",
      "cnt: 0 - valLoss: 0.5475229620933533 - trainLoss: 0.5444017648696899\n",
      "cnt: 0 - valLoss: 0.5474982857704163 - trainLoss: 0.544379711151123\n",
      "cnt: 0 - valLoss: 0.5474736094474792 - trainLoss: 0.5443576574325562\n",
      "cnt: 0 - valLoss: 0.5474489331245422 - trainLoss: 0.544335663318634\n",
      "cnt: 0 - valLoss: 0.5474242568016052 - trainLoss: 0.5443136692047119\n",
      "cnt: 0 - valLoss: 0.5473995208740234 - trainLoss: 0.544291615486145\n",
      "cnt: 0 - valLoss: 0.5473748445510864 - trainLoss: 0.5442696213722229\n",
      "cnt: 0 - valLoss: 0.5473501682281494 - trainLoss: 0.544247567653656\n",
      "cnt: 0 - valLoss: 0.5473254919052124 - trainLoss: 0.5442255735397339\n",
      "cnt: 0 - valLoss: 0.5473008155822754 - trainLoss: 0.5442035794258118\n",
      "cnt: 0 - valLoss: 0.5472760796546936 - trainLoss: 0.5441815853118896\n",
      "cnt: 0 - valLoss: 0.5472514033317566 - trainLoss: 0.5441595315933228\n",
      "cnt: 0 - valLoss: 0.5472267270088196 - trainLoss: 0.5441374778747559\n",
      "cnt: 0 - valLoss: 0.5472020506858826 - trainLoss: 0.5441155433654785\n",
      "cnt: 0 - valLoss: 0.5471773743629456 - trainLoss: 0.5440934896469116\n",
      "cnt: 0 - valLoss: 0.5471526384353638 - trainLoss: 0.5440714955329895\n",
      "cnt: 0 - valLoss: 0.5471279621124268 - trainLoss: 0.5440495014190674\n",
      "cnt: 0 - valLoss: 0.5471032857894897 - trainLoss: 0.5440275073051453\n",
      "cnt: 0 - valLoss: 0.5470786094665527 - trainLoss: 0.5440055131912231\n",
      "cnt: 0 - valLoss: 0.547053873538971 - trainLoss: 0.543983519077301\n",
      "cnt: 0 - valLoss: 0.5470291972160339 - trainLoss: 0.5439614653587341\n",
      "cnt: 0 - valLoss: 0.5470045208930969 - trainLoss: 0.543939471244812\n",
      "cnt: 0 - valLoss: 0.5469798445701599 - trainLoss: 0.5439174771308899\n",
      "cnt: 0 - valLoss: 0.5469551086425781 - trainLoss: 0.5438954830169678\n",
      "cnt: 0 - valLoss: 0.5469304919242859 - trainLoss: 0.5438734889030457\n",
      "cnt: 0 - valLoss: 0.5469057559967041 - trainLoss: 0.5438514947891235\n",
      "cnt: 0 - valLoss: 0.5468810796737671 - trainLoss: 0.5438295006752014\n",
      "cnt: 0 - valLoss: 0.5468564033508301 - trainLoss: 0.5438075065612793\n",
      "cnt: 0 - valLoss: 0.5468317270278931 - trainLoss: 0.5437855124473572\n",
      "cnt: 0 - valLoss: 0.546807050704956 - trainLoss: 0.5437635183334351\n",
      "cnt: 0 - valLoss: 0.546782374382019 - trainLoss: 0.5437414646148682\n",
      "cnt: 0 - valLoss: 0.5467576384544373 - trainLoss: 0.543719470500946\n",
      "cnt: 0 - valLoss: 0.546733021736145 - trainLoss: 0.5436974763870239\n",
      "cnt: 0 - valLoss: 0.5467082858085632 - trainLoss: 0.5436755418777466\n",
      "cnt: 0 - valLoss: 0.5466835498809814 - trainLoss: 0.5436535477638245\n",
      "cnt: 0 - valLoss: 0.5466589331626892 - trainLoss: 0.5436315536499023\n",
      "cnt: 0 - valLoss: 0.546634316444397 - trainLoss: 0.5436095595359802\n",
      "cnt: 0 - valLoss: 0.5466095805168152 - trainLoss: 0.5435875058174133\n",
      "cnt: 0 - valLoss: 0.5465848445892334 - trainLoss: 0.543565571308136\n",
      "cnt: 0 - valLoss: 0.5465602278709412 - trainLoss: 0.5435435175895691\n",
      "cnt: 0 - valLoss: 0.5465355515480042 - trainLoss: 0.5435215830802917\n",
      "cnt: 0 - valLoss: 0.5465108752250671 - trainLoss: 0.5434996485710144\n",
      "cnt: 0 - valLoss: 0.5464861392974854 - trainLoss: 0.5434775948524475\n",
      "cnt: 0 - valLoss: 0.5464614629745483 - trainLoss: 0.5434556007385254\n",
      "cnt: 0 - valLoss: 0.5464367866516113 - trainLoss: 0.543433666229248\n",
      "cnt: 0 - valLoss: 0.5464121103286743 - trainLoss: 0.5434116721153259\n",
      "cnt: 0 - valLoss: 0.5463874936103821 - trainLoss: 0.5433896780014038\n",
      "cnt: 0 - valLoss: 0.5463627576828003 - trainLoss: 0.5433676838874817\n",
      "cnt: 0 - valLoss: 0.5463380813598633 - trainLoss: 0.5433456897735596\n",
      "cnt: 0 - valLoss: 0.5463134050369263 - trainLoss: 0.5433237552642822\n",
      "cnt: 0 - valLoss: 0.5462887287139893 - trainLoss: 0.5433017611503601\n",
      "cnt: 0 - valLoss: 0.5462640523910522 - trainLoss: 0.543279767036438\n",
      "cnt: 0 - valLoss: 0.5462393760681152 - trainLoss: 0.5432577729225159\n",
      "cnt: 0 - valLoss: 0.5462146997451782 - trainLoss: 0.5432357788085938\n",
      "cnt: 0 - valLoss: 0.5461900234222412 - trainLoss: 0.5432138442993164\n",
      "cnt: 0 - valLoss: 0.546165406703949 - trainLoss: 0.5431918501853943\n",
      "cnt: 0 - valLoss: 0.5461406707763672 - trainLoss: 0.5431698560714722\n",
      "cnt: 0 - valLoss: 0.5461159944534302 - trainLoss: 0.54314786195755\n",
      "cnt: 0 - valLoss: 0.5460913181304932 - trainLoss: 0.5431259274482727\n",
      "cnt: 0 - valLoss: 0.5460666418075562 - trainLoss: 0.5431039333343506\n",
      "cnt: 0 - valLoss: 0.5460419654846191 - trainLoss: 0.5430819392204285\n",
      "cnt: 0 - valLoss: 0.5460172295570374 - trainLoss: 0.5430599451065063\n",
      "cnt: 0 - valLoss: 0.5459926128387451 - trainLoss: 0.543038010597229\n",
      "cnt: 0 - valLoss: 0.5459679365158081 - trainLoss: 0.5430160164833069\n",
      "cnt: 0 - valLoss: 0.5459432601928711 - trainLoss: 0.5429940223693848\n",
      "cnt: 0 - valLoss: 0.5459185838699341 - trainLoss: 0.5429720878601074\n",
      "cnt: 0 - valLoss: 0.5458939075469971 - trainLoss: 0.5429500937461853\n",
      "cnt: 0 - valLoss: 0.5458692312240601 - trainLoss: 0.542928159236908\n",
      "cnt: 0 - valLoss: 0.5458446145057678 - trainLoss: 0.5429061651229858\n",
      "cnt: 0 - valLoss: 0.5458199381828308 - trainLoss: 0.5428842306137085\n",
      "cnt: 0 - valLoss: 0.545795202255249 - trainLoss: 0.5428622364997864\n",
      "cnt: 0 - valLoss: 0.545770525932312 - trainLoss: 0.5428402423858643\n",
      "cnt: 0 - valLoss: 0.545745849609375 - trainLoss: 0.5428182482719421\n",
      "cnt: 0 - valLoss: 0.5457212328910828 - trainLoss: 0.5427963733673096\n",
      "cnt: 0 - valLoss: 0.5456965565681458 - trainLoss: 0.5427743196487427\n",
      "cnt: 0 - valLoss: 0.5456718802452087 - trainLoss: 0.5427523851394653\n",
      "cnt: 0 - valLoss: 0.5456472039222717 - trainLoss: 0.542730450630188\n",
      "cnt: 0 - valLoss: 0.5456225275993347 - trainLoss: 0.5427084565162659\n",
      "cnt: 0 - valLoss: 0.5455978512763977 - trainLoss: 0.5426865220069885\n",
      "cnt: 0 - valLoss: 0.5455731749534607 - trainLoss: 0.5426645874977112\n",
      "cnt: 0 - valLoss: 0.5455485582351685 - trainLoss: 0.5426425337791443\n",
      "cnt: 0 - valLoss: 0.5455238223075867 - trainLoss: 0.5426205992698669\n",
      "cnt: 0 - valLoss: 0.5454992055892944 - trainLoss: 0.5425986647605896\n",
      "cnt: 0 - valLoss: 0.5454745292663574 - trainLoss: 0.5425766706466675\n",
      "cnt: 0 - valLoss: 0.5454498529434204 - trainLoss: 0.5425547957420349\n",
      "cnt: 0 - valLoss: 0.5454252362251282 - trainLoss: 0.5425328016281128\n",
      "cnt: 0 - valLoss: 0.5454005599021912 - trainLoss: 0.5425108671188354\n",
      "cnt: 0 - valLoss: 0.5453758835792542 - trainLoss: 0.5424888730049133\n",
      "cnt: 0 - valLoss: 0.5453512072563171 - trainLoss: 0.542466938495636\n",
      "cnt: 0 - valLoss: 0.5453265905380249 - trainLoss: 0.5424450039863586\n",
      "cnt: 0 - valLoss: 0.5453019142150879 - trainLoss: 0.5424230098724365\n",
      "cnt: 0 - valLoss: 0.5452771782875061 - trainLoss: 0.542401134967804\n",
      "cnt: 0 - valLoss: 0.5452526211738586 - trainLoss: 0.5423791408538818\n",
      "cnt: 0 - valLoss: 0.5452278852462769 - trainLoss: 0.5423572063446045\n",
      "cnt: 0 - valLoss: 0.5452032685279846 - trainLoss: 0.5423352718353271\n",
      "cnt: 0 - valLoss: 0.5451785922050476 - trainLoss: 0.542313277721405\n",
      "cnt: 0 - valLoss: 0.5451539158821106 - trainLoss: 0.5422913432121277\n",
      "cnt: 0 - valLoss: 0.5451292395591736 - trainLoss: 0.5422693490982056\n",
      "cnt: 0 - valLoss: 0.5451046228408813 - trainLoss: 0.542247474193573\n",
      "cnt: 0 - valLoss: 0.5450799465179443 - trainLoss: 0.5422254800796509\n",
      "cnt: 0 - valLoss: 0.5450552701950073 - trainLoss: 0.5422035455703735\n",
      "cnt: 0 - valLoss: 0.5450305938720703 - trainLoss: 0.5421816110610962\n",
      "cnt: 0 - valLoss: 0.5450059771537781 - trainLoss: 0.5421596765518188\n",
      "cnt: 0 - valLoss: 0.5449813008308411 - trainLoss: 0.5421377420425415\n",
      "cnt: 0 - valLoss: 0.5449566841125488 - trainLoss: 0.5421157479286194\n",
      "cnt: 0 - valLoss: 0.5449320077896118 - trainLoss: 0.542093813419342\n",
      "cnt: 0 - valLoss: 0.5449073314666748 - trainLoss: 0.5420718789100647\n",
      "cnt: 0 - valLoss: 0.5448827147483826 - trainLoss: 0.5420499444007874\n",
      "cnt: 0 - valLoss: 0.5448580384254456 - trainLoss: 0.5420280694961548\n",
      "cnt: 0 - valLoss: 0.5448333621025085 - trainLoss: 0.5420060753822327\n",
      "cnt: 0 - valLoss: 0.5448087453842163 - trainLoss: 0.5419841408729553\n",
      "cnt: 0 - valLoss: 0.5447840690612793 - trainLoss: 0.541962206363678\n",
      "cnt: 0 - valLoss: 0.5447594523429871 - trainLoss: 0.5419402718544006\n",
      "cnt: 0 - valLoss: 0.54473477602005 - trainLoss: 0.5419183373451233\n",
      "cnt: 0 - valLoss: 0.5447101593017578 - trainLoss: 0.5418964624404907\n",
      "cnt: 0 - valLoss: 0.5446854829788208 - trainLoss: 0.5418744683265686\n",
      "cnt: 0 - valLoss: 0.5446608662605286 - trainLoss: 0.541852593421936\n",
      "cnt: 0 - valLoss: 0.5446361303329468 - trainLoss: 0.5418306589126587\n",
      "cnt: 0 - valLoss: 0.5446115732192993 - trainLoss: 0.5418087244033813\n",
      "cnt: 0 - valLoss: 0.5445868968963623 - trainLoss: 0.541786789894104\n",
      "cnt: 0 - valLoss: 0.5445622801780701 - trainLoss: 0.5417647957801819\n",
      "cnt: 0 - valLoss: 0.5445376038551331 - trainLoss: 0.5417429208755493\n",
      "cnt: 0 - valLoss: 0.544512927532196 - trainLoss: 0.541720986366272\n",
      "cnt: 0 - valLoss: 0.5444883108139038 - trainLoss: 0.5416990518569946\n",
      "cnt: 0 - valLoss: 0.5444636344909668 - trainLoss: 0.5416771769523621\n",
      "cnt: 0 - valLoss: 0.5444390177726746 - trainLoss: 0.5416552424430847\n",
      "cnt: 0 - valLoss: 0.5444143414497375 - trainLoss: 0.5416333079338074\n",
      "cnt: 0 - valLoss: 0.5443897247314453 - trainLoss: 0.54161137342453\n",
      "cnt: 0 - valLoss: 0.5443649888038635 - trainLoss: 0.5415894389152527\n",
      "cnt: 0 - valLoss: 0.5443404316902161 - trainLoss: 0.5415675044059753\n",
      "cnt: 0 - valLoss: 0.544315755367279 - trainLoss: 0.5415456295013428\n",
      "cnt: 0 - valLoss: 0.544291079044342 - trainLoss: 0.5415236949920654\n",
      "cnt: 0 - valLoss: 0.5442664623260498 - trainLoss: 0.5415017008781433\n",
      "cnt: 0 - valLoss: 0.5442418456077576 - trainLoss: 0.541479766368866\n",
      "cnt: 0 - valLoss: 0.5442171692848206 - trainLoss: 0.5414578914642334\n",
      "cnt: 0 - valLoss: 0.5441925525665283 - trainLoss: 0.541435956954956\n",
      "cnt: 0 - valLoss: 0.5441678762435913 - trainLoss: 0.5414140224456787\n",
      "cnt: 0 - valLoss: 0.5441431999206543 - trainLoss: 0.5413920879364014\n",
      "cnt: 0 - valLoss: 0.5441185832023621 - trainLoss: 0.5413702130317688\n",
      "cnt: 0 - valLoss: 0.5440939664840698 - trainLoss: 0.5413482785224915\n",
      "cnt: 0 - valLoss: 0.5440692901611328 - trainLoss: 0.5413263440132141\n",
      "cnt: 0 - valLoss: 0.5440446734428406 - trainLoss: 0.5413044691085815\n",
      "cnt: 0 - valLoss: 0.5440199971199036 - trainLoss: 0.5412825345993042\n",
      "cnt: 0 - valLoss: 0.5439953207969666 - trainLoss: 0.5412606000900269\n",
      "cnt: 0 - valLoss: 0.5439707040786743 - trainLoss: 0.5412387251853943\n",
      "cnt: 0 - valLoss: 0.5439460873603821 - trainLoss: 0.5412167906761169\n",
      "cnt: 0 - valLoss: 0.5439214110374451 - trainLoss: 0.5411948561668396\n",
      "cnt: 0 - valLoss: 0.5438967943191528 - trainLoss: 0.541172981262207\n",
      "cnt: 0 - valLoss: 0.5438721179962158 - trainLoss: 0.5411510467529297\n",
      "cnt: 0 - valLoss: 0.5438475012779236 - trainLoss: 0.5411291122436523\n",
      "cnt: 0 - valLoss: 0.5438228845596313 - trainLoss: 0.5411072373390198\n",
      "cnt: 0 - valLoss: 0.5437982082366943 - trainLoss: 0.5410853028297424\n",
      "cnt: 0 - valLoss: 0.5437735915184021 - trainLoss: 0.5410633683204651\n",
      "cnt: 0 - valLoss: 0.5437489748001099 - trainLoss: 0.5410414934158325\n",
      "cnt: 0 - valLoss: 0.5437243580818176 - trainLoss: 0.5410195589065552\n",
      "cnt: 0 - valLoss: 0.5436997413635254 - trainLoss: 0.5409976243972778\n",
      "cnt: 0 - valLoss: 0.5436750054359436 - trainLoss: 0.5409757494926453\n",
      "cnt: 0 - valLoss: 0.5436504483222961 - trainLoss: 0.5409538745880127\n",
      "cnt: 0 - valLoss: 0.5436257123947144 - trainLoss: 0.5409318804740906\n",
      "cnt: 0 - valLoss: 0.5436011552810669 - trainLoss: 0.5409100651741028\n",
      "cnt: 0 - valLoss: 0.5435765385627747 - trainLoss: 0.5408881902694702\n",
      "cnt: 0 - valLoss: 0.5435518622398376 - trainLoss: 0.5408662557601929\n",
      "cnt: 0 - valLoss: 0.5435272455215454 - trainLoss: 0.5408443212509155\n",
      "cnt: 0 - valLoss: 0.5435026288032532 - trainLoss: 0.540822446346283\n",
      "cnt: 0 - valLoss: 0.5434780120849609 - trainLoss: 0.5408005714416504\n",
      "cnt: 0 - valLoss: 0.5434534549713135 - trainLoss: 0.5407786965370178\n",
      "cnt: 0 - valLoss: 0.5434288382530212 - trainLoss: 0.5407568216323853\n",
      "cnt: 0 - valLoss: 0.5434041619300842 - trainLoss: 0.5407349467277527\n",
      "cnt: 0 - valLoss: 0.543379545211792 - trainLoss: 0.5407130122184753\n",
      "cnt: 0 - valLoss: 0.5433549880981445 - trainLoss: 0.5406911969184875\n",
      "cnt: 0 - valLoss: 0.5433303117752075 - trainLoss: 0.5406692028045654\n",
      "cnt: 0 - valLoss: 0.5433057546615601 - trainLoss: 0.5406473875045776\n",
      "cnt: 0 - valLoss: 0.5432811379432678 - trainLoss: 0.5406255125999451\n",
      "cnt: 0 - valLoss: 0.5432565212249756 - trainLoss: 0.5406036376953125\n",
      "cnt: 0 - valLoss: 0.5432319045066833 - trainLoss: 0.5405817627906799\n",
      "cnt: 0 - valLoss: 0.5432073473930359 - trainLoss: 0.5405598878860474\n",
      "cnt: 0 - valLoss: 0.5431827306747437 - trainLoss: 0.5405380129814148\n",
      "cnt: 0 - valLoss: 0.5431581139564514 - trainLoss: 0.5405161380767822\n",
      "cnt: 0 - valLoss: 0.5431334972381592 - trainLoss: 0.5404943227767944\n",
      "cnt: 0 - valLoss: 0.5431088805198669 - trainLoss: 0.5404723882675171\n",
      "cnt: 0 - valLoss: 0.5430842638015747 - trainLoss: 0.5404505729675293\n",
      "cnt: 0 - valLoss: 0.5430596470832825 - trainLoss: 0.5404286980628967\n",
      "cnt: 0 - valLoss: 0.543035089969635 - trainLoss: 0.5404067635536194\n",
      "cnt: 0 - valLoss: 0.5430104732513428 - trainLoss: 0.5403848886489868\n",
      "cnt: 0 - valLoss: 0.5429859161376953 - trainLoss: 0.540363073348999\n",
      "cnt: 0 - valLoss: 0.5429612994194031 - trainLoss: 0.5403411388397217\n",
      "cnt: 0 - valLoss: 0.5429366827011108 - trainLoss: 0.5403193235397339\n",
      "cnt: 0 - valLoss: 0.5429120659828186 - trainLoss: 0.5402974486351013\n",
      "cnt: 0 - valLoss: 0.5428875088691711 - trainLoss: 0.5402755737304688\n",
      "cnt: 0 - valLoss: 0.5428628921508789 - trainLoss: 0.540253758430481\n",
      "cnt: 0 - valLoss: 0.5428382754325867 - trainLoss: 0.5402319431304932\n",
      "cnt: 0 - valLoss: 0.5428136587142944 - trainLoss: 0.5402100682258606\n",
      "cnt: 0 - valLoss: 0.5427891612052917 - trainLoss: 0.540188193321228\n",
      "cnt: 0 - valLoss: 0.5427644848823547 - trainLoss: 0.5401663780212402\n",
      "cnt: 0 - valLoss: 0.5427399277687073 - trainLoss: 0.5401445031166077\n",
      "cnt: 0 - valLoss: 0.542715311050415 - trainLoss: 0.5401226878166199\n",
      "cnt: 0 - valLoss: 0.5426907539367676 - trainLoss: 0.5401008725166321\n",
      "cnt: 0 - valLoss: 0.5426661372184753 - trainLoss: 0.5400790572166443\n",
      "cnt: 0 - valLoss: 0.5426415205001831 - trainLoss: 0.5400571823120117\n",
      "cnt: 0 - valLoss: 0.5426169633865356 - trainLoss: 0.5400353670120239\n",
      "cnt: 0 - valLoss: 0.5425924062728882 - trainLoss: 0.5400134921073914\n",
      "cnt: 0 - valLoss: 0.542567789554596 - trainLoss: 0.5399916768074036\n",
      "cnt: 0 - valLoss: 0.5425432324409485 - trainLoss: 0.5399698615074158\n",
      "cnt: 0 - valLoss: 0.5425186157226562 - trainLoss: 0.539948046207428\n",
      "cnt: 0 - valLoss: 0.542493999004364 - trainLoss: 0.5399261713027954\n",
      "cnt: 0 - valLoss: 0.5424694418907166 - trainLoss: 0.5399044156074524\n",
      "cnt: 0 - valLoss: 0.5424448847770691 - trainLoss: 0.5398825407028198\n",
      "cnt: 0 - valLoss: 0.5424202680587769 - trainLoss: 0.539860725402832\n",
      "cnt: 0 - valLoss: 0.5423957109451294 - trainLoss: 0.5398389101028442\n",
      "cnt: 0 - valLoss: 0.5423711538314819 - trainLoss: 0.5398170948028564\n",
      "cnt: 0 - valLoss: 0.5423465967178345 - trainLoss: 0.5397952795028687\n",
      "cnt: 0 - valLoss: 0.542322039604187 - trainLoss: 0.5397734642028809\n",
      "cnt: 0 - valLoss: 0.5422974228858948 - trainLoss: 0.5397517085075378\n",
      "cnt: 0 - valLoss: 0.5422728657722473 - trainLoss: 0.53972989320755\n",
      "cnt: 0 - valLoss: 0.5422483086585999 - trainLoss: 0.539708137512207\n",
      "cnt: 0 - valLoss: 0.5422237515449524 - trainLoss: 0.5396862626075745\n",
      "cnt: 0 - valLoss: 0.5421991348266602 - trainLoss: 0.5396645069122314\n",
      "cnt: 0 - valLoss: 0.5421745777130127 - trainLoss: 0.5396426916122437\n",
      "cnt: 0 - valLoss: 0.5421500205993652 - trainLoss: 0.5396209955215454\n",
      "cnt: 0 - valLoss: 0.5421254634857178 - trainLoss: 0.5395991206169128\n",
      "cnt: 0 - valLoss: 0.5421009063720703 - trainLoss: 0.5395774245262146\n",
      "cnt: 0 - valLoss: 0.5420763492584229 - trainLoss: 0.5395556092262268\n",
      "cnt: 0 - valLoss: 0.5420517921447754 - trainLoss: 0.5395338535308838\n",
      "cnt: 0 - valLoss: 0.5420272350311279 - trainLoss: 0.539512038230896\n",
      "cnt: 0 - valLoss: 0.5420026183128357 - trainLoss: 0.539490282535553\n",
      "cnt: 0 - valLoss: 0.5419780611991882 - trainLoss: 0.53946852684021\n",
      "cnt: 0 - valLoss: 0.5419535636901855 - trainLoss: 0.5394467115402222\n",
      "cnt: 0 - valLoss: 0.5419290065765381 - trainLoss: 0.5394249558448792\n",
      "cnt: 0 - valLoss: 0.5419045090675354 - trainLoss: 0.5394032001495361\n",
      "cnt: 0 - valLoss: 0.5418799519538879 - trainLoss: 0.5393814444541931\n",
      "cnt: 0 - valLoss: 0.5418553948402405 - trainLoss: 0.5393596291542053\n",
      "cnt: 0 - valLoss: 0.541830837726593 - trainLoss: 0.5393378138542175\n",
      "cnt: 0 - valLoss: 0.5418063402175903 - trainLoss: 0.5393161177635193\n",
      "cnt: 0 - valLoss: 0.5417817831039429 - trainLoss: 0.5392943024635315\n",
      "cnt: 0 - valLoss: 0.5417572259902954 - trainLoss: 0.5392725467681885\n",
      "cnt: 0 - valLoss: 0.5417327284812927 - trainLoss: 0.5392507910728455\n",
      "cnt: 0 - valLoss: 0.5417081713676453 - trainLoss: 0.5392290353775024\n",
      "cnt: 0 - valLoss: 0.5416836738586426 - trainLoss: 0.5392072796821594\n",
      "cnt: 0 - valLoss: 0.5416591763496399 - trainLoss: 0.5391855239868164\n",
      "cnt: 0 - valLoss: 0.5416346192359924 - trainLoss: 0.5391637086868286\n",
      "cnt: 0 - valLoss: 0.5416100025177002 - trainLoss: 0.5391419529914856\n",
      "cnt: 0 - valLoss: 0.5415855050086975 - trainLoss: 0.5391201972961426\n",
      "cnt: 0 - valLoss: 0.5415610074996948 - trainLoss: 0.5390984416007996\n",
      "cnt: 0 - valLoss: 0.5415365099906921 - trainLoss: 0.5390766859054565\n",
      "cnt: 0 - valLoss: 0.5415118932723999 - trainLoss: 0.5390549898147583\n",
      "cnt: 0 - valLoss: 0.5414873957633972 - trainLoss: 0.5390331149101257\n",
      "cnt: 0 - valLoss: 0.5414629578590393 - trainLoss: 0.5390114188194275\n",
      "cnt: 0 - valLoss: 0.5414384007453918 - trainLoss: 0.5389897227287292\n",
      "cnt: 0 - valLoss: 0.5414139032363892 - trainLoss: 0.5389679670333862\n",
      "cnt: 0 - valLoss: 0.5413894057273865 - trainLoss: 0.5389461517333984\n",
      "cnt: 0 - valLoss: 0.541364848613739 - trainLoss: 0.5389243960380554\n",
      "cnt: 0 - valLoss: 0.5413403511047363 - trainLoss: 0.5389026999473572\n",
      "cnt: 0 - valLoss: 0.5413158535957336 - trainLoss: 0.5388809442520142\n",
      "cnt: 0 - valLoss: 0.5412912964820862 - trainLoss: 0.5388591885566711\n",
      "cnt: 0 - valLoss: 0.5412667989730835 - trainLoss: 0.5388374328613281\n",
      "cnt: 0 - valLoss: 0.5412423014640808 - trainLoss: 0.5388156771659851\n",
      "cnt: 0 - valLoss: 0.5412177443504333 - trainLoss: 0.5387939810752869\n",
      "cnt: 0 - valLoss: 0.5411932468414307 - trainLoss: 0.5387722253799438\n",
      "cnt: 0 - valLoss: 0.541168749332428 - trainLoss: 0.5387505292892456\n",
      "cnt: 0 - valLoss: 0.5411441922187805 - trainLoss: 0.5387287139892578\n",
      "cnt: 0 - valLoss: 0.5411196947097778 - trainLoss: 0.5387070178985596\n",
      "cnt: 0 - valLoss: 0.5410951972007751 - trainLoss: 0.5386852622032166\n",
      "cnt: 0 - valLoss: 0.5410706400871277 - trainLoss: 0.5386635661125183\n",
      "cnt: 0 - valLoss: 0.5410462021827698 - trainLoss: 0.5386418104171753\n",
      "cnt: 0 - valLoss: 0.5410217046737671 - trainLoss: 0.538620114326477\n",
      "cnt: 0 - valLoss: 0.5409971475601196 - trainLoss: 0.538598358631134\n",
      "cnt: 0 - valLoss: 0.5409726500511169 - trainLoss: 0.538576602935791\n",
      "cnt: 0 - valLoss: 0.5409481525421143 - trainLoss: 0.538554847240448\n",
      "cnt: 0 - valLoss: 0.5409236550331116 - trainLoss: 0.5385331511497498\n",
      "cnt: 0 - valLoss: 0.5408990979194641 - trainLoss: 0.5385114550590515\n",
      "cnt: 0 - valLoss: 0.5408746004104614 - trainLoss: 0.5384896993637085\n",
      "cnt: 0 - valLoss: 0.5408501625061035 - trainLoss: 0.5384680032730103\n",
      "cnt: 0 - valLoss: 0.5408256649971008 - trainLoss: 0.5384462475776672\n",
      "cnt: 0 - valLoss: 0.5408011674880981 - trainLoss: 0.538424551486969\n",
      "cnt: 0 - valLoss: 0.5407766103744507 - trainLoss: 0.5384028553962708\n",
      "cnt: 0 - valLoss: 0.540752112865448 - trainLoss: 0.5383810997009277\n",
      "cnt: 0 - valLoss: 0.5407276749610901 - trainLoss: 0.5383593440055847\n",
      "cnt: 0 - valLoss: 0.5407031774520874 - trainLoss: 0.5383375883102417\n",
      "cnt: 0 - valLoss: 0.5406786203384399 - trainLoss: 0.5383158922195435\n",
      "cnt: 0 - valLoss: 0.5406541228294373 - trainLoss: 0.5382941961288452\n",
      "cnt: 0 - valLoss: 0.5406296253204346 - trainLoss: 0.5382724404335022\n",
      "cnt: 0 - valLoss: 0.5406051874160767 - trainLoss: 0.538250744342804\n",
      "cnt: 0 - valLoss: 0.540580689907074 - trainLoss: 0.5382290482521057\n",
      "cnt: 0 - valLoss: 0.5405561327934265 - trainLoss: 0.5382073521614075\n",
      "cnt: 0 - valLoss: 0.5405316948890686 - trainLoss: 0.5381855964660645\n",
      "cnt: 0 - valLoss: 0.5405071973800659 - trainLoss: 0.5381639003753662\n",
      "cnt: 0 - valLoss: 0.5404826998710632 - trainLoss: 0.538142204284668\n",
      "cnt: 0 - valLoss: 0.5404582619667053 - trainLoss: 0.5381205081939697\n",
      "cnt: 0 - valLoss: 0.5404337048530579 - trainLoss: 0.5380987524986267\n",
      "cnt: 0 - valLoss: 0.5404092669487 - trainLoss: 0.5380770564079285\n",
      "cnt: 0 - valLoss: 0.5403847694396973 - trainLoss: 0.5380553603172302\n",
      "cnt: 0 - valLoss: 0.5403602719306946 - trainLoss: 0.538033664226532\n",
      "cnt: 0 - valLoss: 0.5403358340263367 - trainLoss: 0.5380119681358337\n",
      "cnt: 0 - valLoss: 0.540311336517334 - trainLoss: 0.5379902720451355\n",
      "cnt: 0 - valLoss: 0.5402868986129761 - trainLoss: 0.5379685759544373\n",
      "cnt: 0 - valLoss: 0.5402624011039734 - trainLoss: 0.537946879863739\n",
      "cnt: 0 - valLoss: 0.5402378439903259 - trainLoss: 0.5379251837730408\n",
      "cnt: 0 - valLoss: 0.540213406085968 - trainLoss: 0.5379034876823425\n",
      "cnt: 0 - valLoss: 0.5401889681816101 - trainLoss: 0.5378817915916443\n",
      "cnt: 0 - valLoss: 0.5401644706726074 - trainLoss: 0.537860095500946\n",
      "cnt: 0 - valLoss: 0.5401400327682495 - trainLoss: 0.5378384590148926\n",
      "cnt: 0 - valLoss: 0.5401155352592468 - trainLoss: 0.5378167033195496\n",
      "cnt: 0 - valLoss: 0.5400910973548889 - trainLoss: 0.5377950668334961\n",
      "cnt: 0 - valLoss: 0.5400665998458862 - trainLoss: 0.5377733707427979\n",
      "cnt: 0 - valLoss: 0.5400421619415283 - trainLoss: 0.5377516746520996\n",
      "cnt: 0 - valLoss: 0.5400176644325256 - trainLoss: 0.5377299785614014\n",
      "cnt: 0 - valLoss: 0.5399932265281677 - trainLoss: 0.5377083420753479\n",
      "cnt: 0 - valLoss: 0.539968729019165 - trainLoss: 0.5376865863800049\n",
      "cnt: 0 - valLoss: 0.5399442315101624 - trainLoss: 0.5376649498939514\n",
      "cnt: 0 - valLoss: 0.5399197936058044 - trainLoss: 0.5376432538032532\n",
      "cnt: 0 - valLoss: 0.5398952960968018 - trainLoss: 0.5376216173171997\n",
      "cnt: 0 - valLoss: 0.5398708581924438 - trainLoss: 0.5375998616218567\n",
      "cnt: 0 - valLoss: 0.5398464202880859 - trainLoss: 0.5375782251358032\n",
      "cnt: 0 - valLoss: 0.5398219227790833 - trainLoss: 0.5375565886497498\n",
      "cnt: 0 - valLoss: 0.5397974848747253 - trainLoss: 0.5375348925590515\n",
      "cnt: 0 - valLoss: 0.5397730469703674 - trainLoss: 0.537513256072998\n",
      "cnt: 0 - valLoss: 0.5397485494613647 - trainLoss: 0.5374915599822998\n",
      "cnt: 0 - valLoss: 0.5397241115570068 - trainLoss: 0.5374698638916016\n",
      "cnt: 0 - valLoss: 0.5396996736526489 - trainLoss: 0.5374481678009033\n",
      "cnt: 0 - valLoss: 0.539675235748291 - trainLoss: 0.5374265313148499\n",
      "cnt: 0 - valLoss: 0.5396507382392883 - trainLoss: 0.5374048948287964\n",
      "cnt: 0 - valLoss: 0.5396263003349304 - trainLoss: 0.5373831987380981\n",
      "cnt: 0 - valLoss: 0.5396018028259277 - trainLoss: 0.5373615622520447\n",
      "cnt: 0 - valLoss: 0.5395773649215698 - trainLoss: 0.5373398661613464\n",
      "cnt: 0 - valLoss: 0.5395529270172119 - trainLoss: 0.537318229675293\n",
      "cnt: 0 - valLoss: 0.539528489112854 - trainLoss: 0.5372965931892395\n",
      "cnt: 0 - valLoss: 0.5395040512084961 - trainLoss: 0.537274956703186\n",
      "cnt: 0 - valLoss: 0.5394796133041382 - trainLoss: 0.5372532606124878\n",
      "cnt: 0 - valLoss: 0.5394551157951355 - trainLoss: 0.5372315645217896\n",
      "cnt: 0 - valLoss: 0.5394306778907776 - trainLoss: 0.5372099876403809\n",
      "cnt: 0 - valLoss: 0.5394062399864197 - trainLoss: 0.5371882915496826\n",
      "cnt: 0 - valLoss: 0.5393818020820618 - trainLoss: 0.5371666550636292\n",
      "cnt: 0 - valLoss: 0.5393573641777039 - trainLoss: 0.5371449589729309\n",
      "cnt: 0 - valLoss: 0.539332926273346 - trainLoss: 0.5371233224868774\n",
      "cnt: 0 - valLoss: 0.539308488368988 - trainLoss: 0.537101686000824\n",
      "cnt: 0 - valLoss: 0.5392839908599854 - trainLoss: 0.5370800495147705\n",
      "cnt: 0 - valLoss: 0.5392595529556274 - trainLoss: 0.537058413028717\n",
      "cnt: 0 - valLoss: 0.5392351746559143 - trainLoss: 0.5370367169380188\n",
      "cnt: 0 - valLoss: 0.5392107367515564 - trainLoss: 0.5370151400566101\n",
      "cnt: 0 - valLoss: 0.5391862988471985 - trainLoss: 0.5369934439659119\n",
      "cnt: 0 - valLoss: 0.5391618609428406 - trainLoss: 0.5369718074798584\n",
      "cnt: 0 - valLoss: 0.5391374230384827 - trainLoss: 0.5369501709938049\n",
      "cnt: 0 - valLoss: 0.5391129851341248 - trainLoss: 0.5369285345077515\n",
      "cnt: 0 - valLoss: 0.5390886068344116 - trainLoss: 0.536906898021698\n",
      "cnt: 0 - valLoss: 0.5390641093254089 - trainLoss: 0.5368852615356445\n",
      "cnt: 0 - valLoss: 0.5390397310256958 - trainLoss: 0.5368636250495911\n",
      "cnt: 0 - valLoss: 0.5390152931213379 - trainLoss: 0.5368420481681824\n",
      "cnt: 0 - valLoss: 0.5389909148216248 - trainLoss: 0.5368204116821289\n",
      "cnt: 0 - valLoss: 0.5389664769172668 - trainLoss: 0.5367988348007202\n",
      "cnt: 0 - valLoss: 0.5389420390129089 - trainLoss: 0.536777138710022\n",
      "cnt: 0 - valLoss: 0.538917601108551 - trainLoss: 0.5367555618286133\n",
      "cnt: 0 - valLoss: 0.5388932228088379 - trainLoss: 0.5367339253425598\n",
      "cnt: 0 - valLoss: 0.53886878490448 - trainLoss: 0.5367122888565063\n",
      "cnt: 0 - valLoss: 0.5388443470001221 - trainLoss: 0.5366907119750977\n",
      "cnt: 0 - valLoss: 0.5388199687004089 - trainLoss: 0.5366690158843994\n",
      "cnt: 0 - valLoss: 0.538795530796051 - trainLoss: 0.5366474390029907\n",
      "cnt: 0 - valLoss: 0.5387711524963379 - trainLoss: 0.5366258025169373\n",
      "cnt: 0 - valLoss: 0.53874671459198 - trainLoss: 0.5366042256355286\n",
      "cnt: 0 - valLoss: 0.5387222766876221 - trainLoss: 0.5365825891494751\n",
      "cnt: 0 - valLoss: 0.5386978983879089 - trainLoss: 0.5365609526634216\n",
      "cnt: 0 - valLoss: 0.538673460483551 - trainLoss: 0.5365393757820129\n",
      "cnt: 0 - valLoss: 0.5386490821838379 - trainLoss: 0.5365177989006042\n",
      "cnt: 0 - valLoss: 0.5386247038841248 - trainLoss: 0.5364962220191956\n",
      "cnt: 0 - valLoss: 0.5386002659797668 - trainLoss: 0.5364745259284973\n",
      "cnt: 0 - valLoss: 0.5385758876800537 - trainLoss: 0.5364529490470886\n",
      "cnt: 0 - valLoss: 0.5385515093803406 - trainLoss: 0.5364313721656799\n",
      "cnt: 0 - valLoss: 0.5385270714759827 - trainLoss: 0.5364097952842712\n",
      "cnt: 0 - valLoss: 0.5385026931762695 - trainLoss: 0.5363881587982178\n",
      "cnt: 0 - valLoss: 0.5384783148765564 - trainLoss: 0.5363665819168091\n",
      "cnt: 0 - valLoss: 0.5384538769721985 - trainLoss: 0.5363449454307556\n",
      "cnt: 0 - valLoss: 0.5384294986724854 - trainLoss: 0.5363233685493469\n",
      "cnt: 0 - valLoss: 0.5384050607681274 - trainLoss: 0.5363017916679382\n",
      "cnt: 0 - valLoss: 0.5383806824684143 - trainLoss: 0.5362801551818848\n",
      "cnt: 0 - valLoss: 0.5383563041687012 - trainLoss: 0.5362585783004761\n",
      "cnt: 0 - valLoss: 0.5383318662643433 - trainLoss: 0.5362370014190674\n",
      "cnt: 0 - valLoss: 0.5383075475692749 - trainLoss: 0.5362154245376587\n",
      "cnt: 0 - valLoss: 0.538283109664917 - trainLoss: 0.53619384765625\n",
      "cnt: 0 - valLoss: 0.5382587313652039 - trainLoss: 0.5361722111701965\n",
      "cnt: 0 - valLoss: 0.5382343530654907 - trainLoss: 0.5361506938934326\n",
      "cnt: 0 - valLoss: 0.5382099151611328 - trainLoss: 0.5361291170120239\n",
      "cnt: 0 - valLoss: 0.5381855964660645 - trainLoss: 0.5361075401306152\n",
      "cnt: 0 - valLoss: 0.5381612181663513 - trainLoss: 0.5360859036445618\n",
      "cnt: 0 - valLoss: 0.5381368398666382 - trainLoss: 0.5360643863677979\n",
      "cnt: 0 - valLoss: 0.5381124019622803 - trainLoss: 0.5360427498817444\n",
      "cnt: 0 - valLoss: 0.5380880236625671 - trainLoss: 0.5360211730003357\n",
      "cnt: 0 - valLoss: 0.5380637049674988 - trainLoss: 0.535999596118927\n",
      "cnt: 0 - valLoss: 0.5380393266677856 - trainLoss: 0.5359780192375183\n",
      "cnt: 0 - valLoss: 0.5380148887634277 - trainLoss: 0.5359564423561096\n",
      "cnt: 0 - valLoss: 0.5379905104637146 - trainLoss: 0.5359349250793457\n",
      "cnt: 0 - valLoss: 0.5379661917686462 - trainLoss: 0.535913348197937\n",
      "cnt: 0 - valLoss: 0.5379418134689331 - trainLoss: 0.5358917713165283\n",
      "cnt: 0 - valLoss: 0.53791743516922 - trainLoss: 0.5358701944351196\n",
      "cnt: 0 - valLoss: 0.5378930568695068 - trainLoss: 0.5358486771583557\n",
      "cnt: 0 - valLoss: 0.5378687381744385 - trainLoss: 0.535827100276947\n",
      "cnt: 0 - valLoss: 0.5378443598747253 - trainLoss: 0.5358055233955383\n",
      "cnt: 0 - valLoss: 0.5378199815750122 - trainLoss: 0.5357840061187744\n",
      "cnt: 0 - valLoss: 0.5377956628799438 - trainLoss: 0.5357624292373657\n",
      "cnt: 0 - valLoss: 0.5377712249755859 - trainLoss: 0.535740852355957\n",
      "cnt: 0 - valLoss: 0.5377469062805176 - trainLoss: 0.5357193350791931\n",
      "cnt: 0 - valLoss: 0.5377225279808044 - trainLoss: 0.5356977581977844\n",
      "cnt: 0 - valLoss: 0.5376982092857361 - trainLoss: 0.5356761813163757\n",
      "cnt: 0 - valLoss: 0.5376738905906677 - trainLoss: 0.535654604434967\n",
      "cnt: 0 - valLoss: 0.5376494526863098 - trainLoss: 0.5356330871582031\n",
      "cnt: 0 - valLoss: 0.5376250743865967 - trainLoss: 0.5356115698814392\n",
      "cnt: 0 - valLoss: 0.5376007556915283 - trainLoss: 0.5355899930000305\n",
      "cnt: 0 - valLoss: 0.5375763773918152 - trainLoss: 0.5355684757232666\n",
      "cnt: 0 - valLoss: 0.5375520586967468 - trainLoss: 0.5355469584465027\n",
      "cnt: 0 - valLoss: 0.5375276803970337 - trainLoss: 0.535525381565094\n",
      "cnt: 0 - valLoss: 0.5375033617019653 - trainLoss: 0.5355038642883301\n",
      "cnt: 0 - valLoss: 0.537479043006897 - trainLoss: 0.5354822874069214\n",
      "cnt: 0 - valLoss: 0.5374546647071838 - trainLoss: 0.5354607701301575\n",
      "cnt: 0 - valLoss: 0.5374303460121155 - trainLoss: 0.5354392528533936\n",
      "cnt: 0 - valLoss: 0.5374059677124023 - trainLoss: 0.5354176759719849\n",
      "cnt: 0 - valLoss: 0.537381649017334 - trainLoss: 0.535396158695221\n",
      "cnt: 0 - valLoss: 0.5373572707176208 - trainLoss: 0.535374641418457\n",
      "cnt: 0 - valLoss: 0.5373329520225525 - trainLoss: 0.5353531241416931\n",
      "cnt: 0 - valLoss: 0.5373085737228394 - trainLoss: 0.5353315472602844\n",
      "cnt: 0 - valLoss: 0.537284255027771 - trainLoss: 0.5353100299835205\n",
      "cnt: 0 - valLoss: 0.5372598767280579 - trainLoss: 0.5352885127067566\n",
      "cnt: 0 - valLoss: 0.5372355580329895 - trainLoss: 0.5352669954299927\n",
      "cnt: 0 - valLoss: 0.5372112393379211 - trainLoss: 0.5352454781532288\n",
      "cnt: 0 - valLoss: 0.537186861038208 - trainLoss: 0.5352239608764648\n",
      "cnt: 0 - valLoss: 0.5371625423431396 - trainLoss: 0.5352024435997009\n",
      "cnt: 0 - valLoss: 0.5371382236480713 - trainLoss: 0.535180926322937\n",
      "cnt: 0 - valLoss: 0.5371139049530029 - trainLoss: 0.5351593494415283\n",
      "cnt: 0 - valLoss: 0.5370895266532898 - trainLoss: 0.5351378917694092\n",
      "cnt: 0 - valLoss: 0.5370652079582214 - trainLoss: 0.5351163744926453\n",
      "cnt: 0 - valLoss: 0.5370409488677979 - trainLoss: 0.5350948572158813\n",
      "cnt: 0 - valLoss: 0.5370165705680847 - trainLoss: 0.5350733399391174\n",
      "cnt: 0 - valLoss: 0.5369922518730164 - trainLoss: 0.5350518226623535\n",
      "cnt: 0 - valLoss: 0.5369679927825928 - trainLoss: 0.5350303053855896\n",
      "cnt: 0 - valLoss: 0.5369436740875244 - trainLoss: 0.5350088477134705\n",
      "cnt: 0 - valLoss: 0.536919355392456 - trainLoss: 0.5349873304367065\n",
      "cnt: 0 - valLoss: 0.5368950366973877 - trainLoss: 0.5349658131599426\n",
      "cnt: 0 - valLoss: 0.5368707776069641 - trainLoss: 0.5349442958831787\n",
      "cnt: 0 - valLoss: 0.5368464589118958 - trainLoss: 0.5349228382110596\n",
      "cnt: 0 - valLoss: 0.5368221998214722 - trainLoss: 0.5349013209342957\n",
      "cnt: 0 - valLoss: 0.5367978811264038 - trainLoss: 0.5348798632621765\n",
      "cnt: 0 - valLoss: 0.5367736220359802 - trainLoss: 0.5348583459854126\n",
      "cnt: 0 - valLoss: 0.5367493629455566 - trainLoss: 0.5348368287086487\n",
      "cnt: 0 - valLoss: 0.5367250442504883 - trainLoss: 0.5348153114318848\n",
      "cnt: 0 - valLoss: 0.5367007255554199 - trainLoss: 0.5347939133644104\n",
      "cnt: 0 - valLoss: 0.5366764068603516 - trainLoss: 0.5347723960876465\n",
      "cnt: 0 - valLoss: 0.536652147769928 - trainLoss: 0.5347508788108826\n",
      "cnt: 0 - valLoss: 0.5366278886795044 - trainLoss: 0.5347293615341187\n",
      "cnt: 0 - valLoss: 0.5366036295890808 - trainLoss: 0.5347079634666443\n",
      "cnt: 0 - valLoss: 0.5365793108940125 - trainLoss: 0.5346864461898804\n",
      "cnt: 0 - valLoss: 0.5365549921989441 - trainLoss: 0.5346649885177612\n",
      "cnt: 0 - valLoss: 0.5365307331085205 - trainLoss: 0.5346434712409973\n",
      "cnt: 0 - valLoss: 0.5365064740180969 - trainLoss: 0.5346220135688782\n",
      "cnt: 0 - valLoss: 0.5364822149276733 - trainLoss: 0.534600555896759\n",
      "cnt: 0 - valLoss: 0.536457896232605 - trainLoss: 0.5345790982246399\n",
      "cnt: 0 - valLoss: 0.5364336371421814 - trainLoss: 0.5345576405525208\n",
      "cnt: 0 - valLoss: 0.5364093780517578 - trainLoss: 0.5345362424850464\n",
      "cnt: 0 - valLoss: 0.5363851189613342 - trainLoss: 0.5345147252082825\n",
      "cnt: 0 - valLoss: 0.5363608598709106 - trainLoss: 0.5344932675361633\n",
      "cnt: 0 - valLoss: 0.5363366007804871 - trainLoss: 0.534471869468689\n",
      "cnt: 0 - valLoss: 0.5363123416900635 - trainLoss: 0.5344504714012146\n",
      "cnt: 0 - valLoss: 0.5362880825996399 - trainLoss: 0.5344290137290955\n",
      "cnt: 0 - valLoss: 0.5362638235092163 - trainLoss: 0.5344075560569763\n",
      "cnt: 0 - valLoss: 0.5362395644187927 - trainLoss: 0.5343860983848572\n",
      "cnt: 0 - valLoss: 0.5362153053283691 - trainLoss: 0.5343647003173828\n",
      "cnt: 0 - valLoss: 0.5361910462379456 - trainLoss: 0.5343432426452637\n",
      "cnt: 0 - valLoss: 0.5361668467521667 - trainLoss: 0.5343218445777893\n",
      "cnt: 0 - valLoss: 0.5361425876617432 - trainLoss: 0.5343003869056702\n",
      "cnt: 0 - valLoss: 0.5361183881759644 - trainLoss: 0.5342789888381958\n",
      "cnt: 0 - valLoss: 0.536094069480896 - trainLoss: 0.5342575311660767\n",
      "cnt: 0 - valLoss: 0.5360698699951172 - trainLoss: 0.5342360734939575\n",
      "cnt: 0 - valLoss: 0.5360456109046936 - trainLoss: 0.5342146754264832\n",
      "cnt: 0 - valLoss: 0.53602135181427 - trainLoss: 0.5341932773590088\n",
      "cnt: 0 - valLoss: 0.5359970927238464 - trainLoss: 0.5341718792915344\n",
      "cnt: 0 - valLoss: 0.5359728932380676 - trainLoss: 0.5341504216194153\n",
      "cnt: 0 - valLoss: 0.535948634147644 - trainLoss: 0.5341290235519409\n",
      "cnt: 0 - valLoss: 0.5359244346618652 - trainLoss: 0.5341076254844666\n",
      "cnt: 0 - valLoss: 0.5359001755714417 - trainLoss: 0.5340862274169922\n",
      "cnt: 0 - valLoss: 0.5358759164810181 - trainLoss: 0.5340648293495178\n",
      "cnt: 0 - valLoss: 0.5358517169952393 - trainLoss: 0.5340434312820435\n",
      "cnt: 0 - valLoss: 0.5358274579048157 - trainLoss: 0.5340220332145691\n",
      "cnt: 0 - valLoss: 0.5358032584190369 - trainLoss: 0.5340006351470947\n",
      "cnt: 0 - valLoss: 0.5357790589332581 - trainLoss: 0.5339791774749756\n",
      "cnt: 0 - valLoss: 0.5357547998428345 - trainLoss: 0.5339577794075012\n",
      "cnt: 0 - valLoss: 0.5357306003570557 - trainLoss: 0.5339363813400269\n",
      "cnt: 0 - valLoss: 0.5357063412666321 - trainLoss: 0.5339149832725525\n",
      "cnt: 0 - valLoss: 0.5356821417808533 - trainLoss: 0.5338935852050781\n",
      "cnt: 0 - valLoss: 0.5356578826904297 - trainLoss: 0.5338721871376038\n",
      "cnt: 0 - valLoss: 0.5356336236000061 - trainLoss: 0.5338507294654846\n",
      "cnt: 0 - valLoss: 0.5356094837188721 - trainLoss: 0.5338293313980103\n",
      "cnt: 0 - valLoss: 0.5355852246284485 - trainLoss: 0.5338079333305359\n",
      "cnt: 0 - valLoss: 0.5355610251426697 - trainLoss: 0.5337865948677063\n",
      "cnt: 0 - valLoss: 0.5355367660522461 - trainLoss: 0.5337651968002319\n",
      "cnt: 0 - valLoss: 0.5355125665664673 - trainLoss: 0.5337437987327576\n",
      "cnt: 0 - valLoss: 0.5354883670806885 - trainLoss: 0.5337224006652832\n",
      "cnt: 0 - valLoss: 0.5354641675949097 - trainLoss: 0.5337010025978088\n",
      "cnt: 0 - valLoss: 0.5354399681091309 - trainLoss: 0.5336796641349792\n",
      "cnt: 0 - valLoss: 0.5354157090187073 - trainLoss: 0.5336582660675049\n",
      "cnt: 0 - valLoss: 0.5353915095329285 - trainLoss: 0.5336369276046753\n",
      "cnt: 0 - valLoss: 0.5353673696517944 - trainLoss: 0.5336155891418457\n",
      "cnt: 0 - valLoss: 0.5353431105613708 - trainLoss: 0.5335941314697266\n",
      "cnt: 0 - valLoss: 0.5353189706802368 - trainLoss: 0.533572793006897\n",
      "cnt: 0 - valLoss: 0.535294771194458 - trainLoss: 0.5335513949394226\n",
      "cnt: 0 - valLoss: 0.5352705717086792 - trainLoss: 0.533530056476593\n",
      "cnt: 0 - valLoss: 0.5352463722229004 - trainLoss: 0.5335086584091187\n",
      "cnt: 0 - valLoss: 0.5352221727371216 - trainLoss: 0.5334873199462891\n",
      "cnt: 0 - valLoss: 0.5351979732513428 - trainLoss: 0.5334659218788147\n",
      "cnt: 0 - valLoss: 0.535173773765564 - trainLoss: 0.5334445834159851\n",
      "cnt: 0 - valLoss: 0.5351496338844299 - trainLoss: 0.5334231853485107\n",
      "cnt: 0 - valLoss: 0.5351253747940063 - trainLoss: 0.5334018468856812\n",
      "cnt: 0 - valLoss: 0.5351012349128723 - trainLoss: 0.5333805084228516\n",
      "cnt: 0 - valLoss: 0.5350770354270935 - trainLoss: 0.5333591103553772\n",
      "cnt: 0 - valLoss: 0.5350528955459595 - trainLoss: 0.5333377718925476\n",
      "cnt: 0 - valLoss: 0.5350286960601807 - trainLoss: 0.533316433429718\n",
      "cnt: 0 - valLoss: 0.5350044369697571 - trainLoss: 0.5332950353622437\n",
      "cnt: 0 - valLoss: 0.5349802374839783 - trainLoss: 0.5332737565040588\n",
      "cnt: 0 - valLoss: 0.5349560976028442 - trainLoss: 0.5332523584365845\n",
      "cnt: 0 - valLoss: 0.5349318981170654 - trainLoss: 0.5332310199737549\n",
      "cnt: 0 - valLoss: 0.5349076986312866 - trainLoss: 0.5332096815109253\n",
      "cnt: 0 - valLoss: 0.5348834991455078 - trainLoss: 0.5331883430480957\n",
      "cnt: 0 - valLoss: 0.534859299659729 - trainLoss: 0.5331670045852661\n",
      "cnt: 0 - valLoss: 0.5348351001739502 - trainLoss: 0.5331456661224365\n",
      "cnt: 0 - valLoss: 0.5348110198974609 - trainLoss: 0.5331242680549622\n",
      "cnt: 0 - valLoss: 0.5347868204116821 - trainLoss: 0.5331029891967773\n",
      "cnt: 0 - valLoss: 0.5347626209259033 - trainLoss: 0.5330816507339478\n",
      "cnt: 0 - valLoss: 0.5347384214401245 - trainLoss: 0.5330603122711182\n",
      "cnt: 0 - valLoss: 0.5347142815589905 - trainLoss: 0.5330389142036438\n",
      "cnt: 0 - valLoss: 0.5346901416778564 - trainLoss: 0.533017635345459\n",
      "cnt: 0 - valLoss: 0.5346660017967224 - trainLoss: 0.5329962968826294\n",
      "cnt: 0 - valLoss: 0.5346418619155884 - trainLoss: 0.5329750180244446\n",
      "cnt: 0 - valLoss: 0.5346176624298096 - trainLoss: 0.5329537391662598\n",
      "cnt: 0 - valLoss: 0.5345935821533203 - trainLoss: 0.5329323410987854\n",
      "cnt: 0 - valLoss: 0.534569501876831 - trainLoss: 0.5329111218452454\n",
      "cnt: 0 - valLoss: 0.534545361995697 - trainLoss: 0.5328897833824158\n",
      "cnt: 0 - valLoss: 0.534521222114563 - trainLoss: 0.532868504524231\n",
      "cnt: 0 - valLoss: 0.534497082233429 - trainLoss: 0.5328471660614014\n",
      "cnt: 0 - valLoss: 0.5344729423522949 - trainLoss: 0.5328258872032166\n",
      "cnt: 0 - valLoss: 0.5344488620758057 - trainLoss: 0.5328046083450317\n",
      "cnt: 0 - valLoss: 0.5344247221946716 - trainLoss: 0.5327832698822021\n",
      "cnt: 0 - valLoss: 0.5344006419181824 - trainLoss: 0.5327619910240173\n",
      "cnt: 0 - valLoss: 0.5343765020370483 - trainLoss: 0.5327407121658325\n",
      "cnt: 0 - valLoss: 0.5343524217605591 - trainLoss: 0.5327193737030029\n",
      "cnt: 0 - valLoss: 0.5343283414840698 - trainLoss: 0.5326981544494629\n",
      "cnt: 0 - valLoss: 0.5343042612075806 - trainLoss: 0.5326768159866333\n",
      "cnt: 0 - valLoss: 0.5342801809310913 - trainLoss: 0.5326555371284485\n",
      "cnt: 0 - valLoss: 0.5342560410499573 - trainLoss: 0.5326343178749084\n",
      "cnt: 0 - valLoss: 0.534231960773468 - trainLoss: 0.5326130390167236\n",
      "cnt: 0 - valLoss: 0.5342079997062683 - trainLoss: 0.5325917601585388\n",
      "cnt: 0 - valLoss: 0.5341838598251343 - trainLoss: 0.5325705409049988\n",
      "cnt: 0 - valLoss: 0.5341598391532898 - trainLoss: 0.5325492024421692\n",
      "cnt: 0 - valLoss: 0.5341358184814453 - trainLoss: 0.5325279831886292\n",
      "cnt: 0 - valLoss: 0.5341117978096008 - trainLoss: 0.5325067043304443\n",
      "cnt: 0 - valLoss: 0.5340877175331116 - trainLoss: 0.5324854254722595\n",
      "cnt: 0 - valLoss: 0.5340636968612671 - trainLoss: 0.5324641466140747\n",
      "cnt: 0 - valLoss: 0.5340396761894226 - trainLoss: 0.5324429273605347\n",
      "cnt: 0 - valLoss: 0.5340156555175781 - trainLoss: 0.5324216485023499\n",
      "cnt: 0 - valLoss: 0.5339916348457336 - trainLoss: 0.5324004292488098\n",
      "cnt: 0 - valLoss: 0.5339676737785339 - trainLoss: 0.532379150390625\n",
      "cnt: 0 - valLoss: 0.5339435935020447 - trainLoss: 0.532357931137085\n",
      "cnt: 0 - valLoss: 0.533919632434845 - trainLoss: 0.5323366522789001\n",
      "cnt: 0 - valLoss: 0.5338956117630005 - trainLoss: 0.5323154330253601\n",
      "cnt: 0 - valLoss: 0.5338716506958008 - trainLoss: 0.5322942137718201\n",
      "cnt: 0 - valLoss: 0.5338476300239563 - trainLoss: 0.5322729349136353\n",
      "cnt: 0 - valLoss: 0.5338236093521118 - trainLoss: 0.5322517156600952\n",
      "cnt: 0 - valLoss: 0.5337996482849121 - trainLoss: 0.5322304368019104\n",
      "cnt: 0 - valLoss: 0.5337756276130676 - trainLoss: 0.5322092175483704\n",
      "cnt: 0 - valLoss: 0.5337516665458679 - trainLoss: 0.5321879982948303\n",
      "cnt: 0 - valLoss: 0.5337276458740234 - trainLoss: 0.5321667790412903\n",
      "cnt: 0 - valLoss: 0.5337036848068237 - trainLoss: 0.5321455597877502\n",
      "cnt: 0 - valLoss: 0.533679723739624 - trainLoss: 0.5321242809295654\n",
      "cnt: 0 - valLoss: 0.5336557030677795 - trainLoss: 0.5321031212806702\n",
      "cnt: 0 - valLoss: 0.5336317420005798 - trainLoss: 0.5320819020271301\n",
      "cnt: 0 - valLoss: 0.5336077213287354 - trainLoss: 0.5320606827735901\n",
      "cnt: 0 - valLoss: 0.5335837602615356 - trainLoss: 0.5320394039154053\n",
      "cnt: 0 - valLoss: 0.5335597991943359 - trainLoss: 0.5320181846618652\n",
      "cnt: 0 - valLoss: 0.5335358381271362 - trainLoss: 0.5319969654083252\n",
      "cnt: 0 - valLoss: 0.5335118770599365 - trainLoss: 0.5319757461547852\n",
      "cnt: 0 - valLoss: 0.5334879159927368 - trainLoss: 0.5319545865058899\n",
      "cnt: 0 - valLoss: 0.5334638953208923 - trainLoss: 0.5319333672523499\n",
      "cnt: 0 - valLoss: 0.5334399938583374 - trainLoss: 0.5319121479988098\n",
      "cnt: 0 - valLoss: 0.5334159731864929 - trainLoss: 0.5318909287452698\n",
      "cnt: 0 - valLoss: 0.5333919525146484 - trainLoss: 0.5318697690963745\n",
      "cnt: 0 - valLoss: 0.5333680510520935 - trainLoss: 0.5318485498428345\n",
      "cnt: 0 - valLoss: 0.533344030380249 - trainLoss: 0.5318273305892944\n",
      "cnt: 0 - valLoss: 0.5333200693130493 - trainLoss: 0.5318061113357544\n",
      "cnt: 0 - valLoss: 0.5332961678504944 - trainLoss: 0.5317849516868591\n",
      "cnt: 0 - valLoss: 0.5332722067832947 - trainLoss: 0.5317637324333191\n",
      "cnt: 0 - valLoss: 0.533248245716095 - trainLoss: 0.5317425727844238\n",
      "cnt: 0 - valLoss: 0.5332242846488953 - trainLoss: 0.5317213535308838\n",
      "cnt: 0 - valLoss: 0.5332003831863403 - trainLoss: 0.5317001938819885\n",
      "cnt: 0 - valLoss: 0.5331764817237854 - trainLoss: 0.5316789746284485\n",
      "cnt: 0 - valLoss: 0.5331525206565857 - trainLoss: 0.5316578149795532\n",
      "cnt: 0 - valLoss: 0.5331286191940308 - trainLoss: 0.531636655330658\n",
      "cnt: 0 - valLoss: 0.5331047773361206 - trainLoss: 0.5316154360771179\n",
      "cnt: 0 - valLoss: 0.5330808162689209 - trainLoss: 0.5315942764282227\n",
      "cnt: 0 - valLoss: 0.5330569744110107 - trainLoss: 0.5315730571746826\n",
      "cnt: 0 - valLoss: 0.5330330729484558 - trainLoss: 0.5315518975257874\n",
      "cnt: 0 - valLoss: 0.5330091714859009 - trainLoss: 0.5315307378768921\n",
      "cnt: 0 - valLoss: 0.5329853296279907 - trainLoss: 0.5315095782279968\n",
      "cnt: 0 - valLoss: 0.532961368560791 - trainLoss: 0.5314884185791016\n",
      "cnt: 0 - valLoss: 0.5329375267028809 - trainLoss: 0.5314671993255615\n",
      "cnt: 0 - valLoss: 0.5329135656356812 - trainLoss: 0.5314460396766663\n",
      "cnt: 0 - valLoss: 0.532889723777771 - trainLoss: 0.531424880027771\n",
      "cnt: 0 - valLoss: 0.5328658223152161 - trainLoss: 0.5314037203788757\n",
      "cnt: 0 - valLoss: 0.5328419804573059 - trainLoss: 0.5313826203346252\n",
      "cnt: 0 - valLoss: 0.532818078994751 - trainLoss: 0.53136146068573\n",
      "cnt: 0 - valLoss: 0.5327942371368408 - trainLoss: 0.5313402414321899\n",
      "cnt: 0 - valLoss: 0.5327703356742859 - trainLoss: 0.5313191413879395\n",
      "cnt: 0 - valLoss: 0.5327464938163757 - trainLoss: 0.5312979221343994\n",
      "cnt: 0 - valLoss: 0.532722532749176 - trainLoss: 0.5312768220901489\n",
      "cnt: 0 - valLoss: 0.5326987504959106 - trainLoss: 0.5312556624412537\n",
      "cnt: 0 - valLoss: 0.5326749086380005 - trainLoss: 0.5312345027923584\n",
      "cnt: 0 - valLoss: 0.5326510667800903 - trainLoss: 0.5312133431434631\n",
      "cnt: 0 - valLoss: 0.5326272249221802 - trainLoss: 0.5311922430992126\n",
      "cnt: 0 - valLoss: 0.53260338306427 - trainLoss: 0.5311710834503174\n",
      "cnt: 0 - valLoss: 0.5325796008110046 - trainLoss: 0.5311499834060669\n",
      "cnt: 0 - valLoss: 0.5325557589530945 - trainLoss: 0.5311288237571716\n",
      "cnt: 0 - valLoss: 0.5325319766998291 - trainLoss: 0.5311077237129211\n",
      "cnt: 0 - valLoss: 0.532508134841919 - trainLoss: 0.5310865640640259\n",
      "cnt: 0 - valLoss: 0.5324843525886536 - trainLoss: 0.5310654640197754\n",
      "cnt: 0 - valLoss: 0.5324605107307434 - trainLoss: 0.5310443043708801\n",
      "cnt: 0 - valLoss: 0.5324366688728333 - trainLoss: 0.5310232043266296\n",
      "cnt: 0 - valLoss: 0.5324129462242126 - trainLoss: 0.5310019850730896\n",
      "cnt: 0 - valLoss: 0.5323891043663025 - trainLoss: 0.5309809446334839\n",
      "cnt: 0 - valLoss: 0.5323652625083923 - trainLoss: 0.5309597849845886\n",
      "cnt: 0 - valLoss: 0.5323414206504822 - trainLoss: 0.5309386849403381\n",
      "cnt: 0 - valLoss: 0.532317578792572 - trainLoss: 0.5309175848960876\n",
      "cnt: 0 - valLoss: 0.5322938561439514 - trainLoss: 0.5308964848518372\n",
      "cnt: 0 - valLoss: 0.5322700142860413 - trainLoss: 0.5308753848075867\n",
      "cnt: 0 - valLoss: 0.5322462320327759 - trainLoss: 0.5308542847633362\n",
      "cnt: 0 - valLoss: 0.5322224497795105 - trainLoss: 0.5308331251144409\n",
      "cnt: 0 - valLoss: 0.5321986675262451 - trainLoss: 0.5308120250701904\n",
      "cnt: 0 - valLoss: 0.5321748852729797 - trainLoss: 0.5307909250259399\n",
      "cnt: 0 - valLoss: 0.5321510434150696 - trainLoss: 0.5307698249816895\n",
      "cnt: 0 - valLoss: 0.532127320766449 - trainLoss: 0.530748724937439\n",
      "cnt: 0 - valLoss: 0.5321034789085388 - trainLoss: 0.5307276248931885\n",
      "cnt: 0 - valLoss: 0.5320796370506287 - trainLoss: 0.5307065844535828\n",
      "cnt: 0 - valLoss: 0.5320559144020081 - trainLoss: 0.5306854248046875\n",
      "cnt: 0 - valLoss: 0.5320321321487427 - trainLoss: 0.530664324760437\n",
      "cnt: 0 - valLoss: 0.5320082902908325 - trainLoss: 0.5306432843208313\n",
      "cnt: 0 - valLoss: 0.5319845676422119 - trainLoss: 0.5306221842765808\n",
      "cnt: 0 - valLoss: 0.5319607257843018 - trainLoss: 0.5306011438369751\n",
      "cnt: 0 - valLoss: 0.5319370031356812 - trainLoss: 0.5305799841880798\n",
      "cnt: 0 - valLoss: 0.5319132208824158 - trainLoss: 0.5305589437484741\n",
      "cnt: 0 - valLoss: 0.5318894386291504 - trainLoss: 0.5305378437042236\n",
      "cnt: 0 - valLoss: 0.531865656375885 - trainLoss: 0.5305167436599731\n",
      "cnt: 0 - valLoss: 0.5318419337272644 - trainLoss: 0.5304957032203674\n",
      "cnt: 0 - valLoss: 0.531818151473999 - trainLoss: 0.5304746031761169\n",
      "cnt: 0 - valLoss: 0.5317944288253784 - trainLoss: 0.5304535627365112\n",
      "cnt: 0 - valLoss: 0.5317707061767578 - trainLoss: 0.5304324626922607\n",
      "cnt: 0 - valLoss: 0.5317469835281372 - trainLoss: 0.530411422252655\n",
      "cnt: 0 - valLoss: 0.5317232608795166 - trainLoss: 0.5303903818130493\n",
      "cnt: 0 - valLoss: 0.531699538230896 - trainLoss: 0.5303693413734436\n",
      "cnt: 0 - valLoss: 0.5316758155822754 - trainLoss: 0.5303482413291931\n",
      "cnt: 0 - valLoss: 0.53165203332901 - trainLoss: 0.5303272008895874\n",
      "cnt: 0 - valLoss: 0.5316283702850342 - trainLoss: 0.5303061604499817\n",
      "cnt: 0 - valLoss: 0.5316047072410583 - trainLoss: 0.5302850604057312\n",
      "cnt: 0 - valLoss: 0.5315809845924377 - trainLoss: 0.5302640199661255\n",
      "cnt: 0 - valLoss: 0.5315572619438171 - trainLoss: 0.5302429795265198\n",
      "cnt: 0 - valLoss: 0.5315335988998413 - trainLoss: 0.5302218794822693\n",
      "cnt: 0 - valLoss: 0.5315098166465759 - trainLoss: 0.5302008390426636\n",
      "cnt: 0 - valLoss: 0.5314861536026001 - trainLoss: 0.5301797986030579\n",
      "cnt: 0 - valLoss: 0.5314624309539795 - trainLoss: 0.5301587581634521\n",
      "cnt: 0 - valLoss: 0.5314387679100037 - trainLoss: 0.5301377773284912\n",
      "cnt: 0 - valLoss: 0.5314151048660278 - trainLoss: 0.5301166772842407\n",
      "cnt: 0 - valLoss: 0.531391441822052 - trainLoss: 0.5300956964492798\n",
      "cnt: 0 - valLoss: 0.5313677787780762 - trainLoss: 0.5300745964050293\n",
      "cnt: 0 - valLoss: 0.5313441157341003 - trainLoss: 0.5300536155700684\n",
      "cnt: 0 - valLoss: 0.5313204526901245 - trainLoss: 0.5300325751304626\n",
      "cnt: 0 - valLoss: 0.5312968492507935 - trainLoss: 0.5300115346908569\n",
      "cnt: 0 - valLoss: 0.5312731862068176 - trainLoss: 0.5299904942512512\n",
      "cnt: 0 - valLoss: 0.5312495827674866 - trainLoss: 0.5299694538116455\n",
      "cnt: 0 - valLoss: 0.5312259197235107 - trainLoss: 0.5299484729766846\n",
      "cnt: 0 - valLoss: 0.5312022566795349 - trainLoss: 0.5299274325370789\n",
      "cnt: 0 - valLoss: 0.5311786532402039 - trainLoss: 0.5299063920974731\n",
      "cnt: 0 - valLoss: 0.531154990196228 - trainLoss: 0.5298854112625122\n",
      "cnt: 0 - valLoss: 0.531131386756897 - trainLoss: 0.5298643708229065\n",
      "cnt: 0 - valLoss: 0.5311077833175659 - trainLoss: 0.5298433899879456\n",
      "cnt: 0 - valLoss: 0.5310842394828796 - trainLoss: 0.5298224091529846\n",
      "cnt: 0 - valLoss: 0.5310606360435486 - trainLoss: 0.5298013687133789\n",
      "cnt: 0 - valLoss: 0.5310369729995728 - trainLoss: 0.529780387878418\n",
      "cnt: 0 - valLoss: 0.5310134291648865 - trainLoss: 0.5297593474388123\n",
      "cnt: 0 - valLoss: 0.5309898257255554 - trainLoss: 0.5297383069992065\n",
      "cnt: 0 - valLoss: 0.5309662222862244 - trainLoss: 0.5297173857688904\n",
      "cnt: 0 - valLoss: 0.5309426188468933 - trainLoss: 0.5296964049339294\n",
      "cnt: 0 - valLoss: 0.530919075012207 - trainLoss: 0.529675304889679\n",
      "cnt: 0 - valLoss: 0.530895471572876 - trainLoss: 0.5296543836593628\n",
      "cnt: 0 - valLoss: 0.5308718681335449 - trainLoss: 0.5296334028244019\n",
      "cnt: 0 - valLoss: 0.5308483242988586 - trainLoss: 0.5296124219894409\n",
      "cnt: 0 - valLoss: 0.5308246612548828 - trainLoss: 0.5295913219451904\n",
      "cnt: 0 - valLoss: 0.5308010578155518 - trainLoss: 0.5295704007148743\n",
      "cnt: 0 - valLoss: 0.5307775139808655 - trainLoss: 0.5295494198799133\n",
      "cnt: 0 - valLoss: 0.5307539701461792 - trainLoss: 0.5295284390449524\n",
      "cnt: 0 - valLoss: 0.5307303667068481 - trainLoss: 0.5295074582099915\n",
      "cnt: 0 - valLoss: 0.5307068824768066 - trainLoss: 0.5294864773750305\n",
      "cnt: 0 - valLoss: 0.5306832194328308 - trainLoss: 0.5294654965400696\n",
      "cnt: 0 - valLoss: 0.5306596755981445 - trainLoss: 0.5294445157051086\n",
      "cnt: 0 - valLoss: 0.5306361317634583 - trainLoss: 0.5294235944747925\n",
      "cnt: 0 - valLoss: 0.530612587928772 - trainLoss: 0.5294026136398315\n",
      "cnt: 0 - valLoss: 0.5305889844894409 - trainLoss: 0.5293816328048706\n",
      "cnt: 0 - valLoss: 0.5305654406547546 - trainLoss: 0.5293606519699097\n",
      "cnt: 0 - valLoss: 0.5305418968200684 - trainLoss: 0.5293397307395935\n",
      "cnt: 0 - valLoss: 0.5305182933807373 - trainLoss: 0.5293186902999878\n",
      "cnt: 0 - valLoss: 0.530494749546051 - trainLoss: 0.5292977690696716\n",
      "cnt: 0 - valLoss: 0.5304712653160095 - trainLoss: 0.5292767882347107\n",
      "cnt: 0 - valLoss: 0.5304476618766785 - trainLoss: 0.5292558670043945\n",
      "cnt: 0 - valLoss: 0.5304241180419922 - trainLoss: 0.5292348861694336\n",
      "cnt: 0 - valLoss: 0.5304005742073059 - trainLoss: 0.5292140245437622\n",
      "cnt: 0 - valLoss: 0.5303770899772644 - trainLoss: 0.5291929841041565\n",
      "cnt: 0 - valLoss: 0.5303534865379333 - trainLoss: 0.5291720628738403\n",
      "cnt: 0 - valLoss: 0.5303299427032471 - trainLoss: 0.529151201248169\n",
      "cnt: 0 - valLoss: 0.5303063988685608 - trainLoss: 0.5291301608085632\n",
      "cnt: 0 - valLoss: 0.5302829146385193 - trainLoss: 0.5291092395782471\n",
      "cnt: 0 - valLoss: 0.530259370803833 - trainLoss: 0.5290883183479309\n",
      "cnt: 0 - valLoss: 0.5302358269691467 - trainLoss: 0.5290673971176147\n",
      "cnt: 0 - valLoss: 0.5302123427391052 - trainLoss: 0.5290464162826538\n",
      "cnt: 0 - valLoss: 0.530188798904419 - trainLoss: 0.5290254950523376\n",
      "cnt: 0 - valLoss: 0.5301652550697327 - trainLoss: 0.5290046334266663\n",
      "cnt: 0 - valLoss: 0.5301417112350464 - trainLoss: 0.5289837121963501\n",
      "cnt: 0 - valLoss: 0.5301181674003601 - trainLoss: 0.5289627909660339\n",
      "cnt: 0 - valLoss: 0.5300946831703186 - trainLoss: 0.5289418697357178\n",
      "cnt: 0 - valLoss: 0.5300711989402771 - trainLoss: 0.5289210081100464\n",
      "cnt: 0 - valLoss: 0.5300476551055908 - trainLoss: 0.5289000868797302\n",
      "cnt: 0 - valLoss: 0.5300241708755493 - trainLoss: 0.5288792252540588\n",
      "cnt: 0 - valLoss: 0.5300006866455078 - trainLoss: 0.5288583040237427\n",
      "cnt: 0 - valLoss: 0.5299772024154663 - trainLoss: 0.5288374423980713\n",
      "cnt: 0 - valLoss: 0.5299537181854248 - trainLoss: 0.5288165807723999\n",
      "cnt: 0 - valLoss: 0.5299301743507385 - trainLoss: 0.5287956595420837\n",
      "cnt: 0 - valLoss: 0.5299066305160522 - trainLoss: 0.5287747979164124\n",
      "cnt: 0 - valLoss: 0.5298831462860107 - trainLoss: 0.5287538766860962\n",
      "cnt: 0 - valLoss: 0.5298596620559692 - trainLoss: 0.5287330150604248\n",
      "cnt: 0 - valLoss: 0.5298361778259277 - trainLoss: 0.5287120938301086\n",
      "cnt: 0 - valLoss: 0.5298126935958862 - trainLoss: 0.5286912322044373\n",
      "cnt: 0 - valLoss: 0.5297892093658447 - trainLoss: 0.5286703705787659\n",
      "cnt: 0 - valLoss: 0.5297657251358032 - trainLoss: 0.5286495089530945\n",
      "cnt: 0 - valLoss: 0.5297421813011169 - trainLoss: 0.5286286473274231\n",
      "cnt: 0 - valLoss: 0.5297186970710754 - trainLoss: 0.5286077857017517\n",
      "cnt: 0 - valLoss: 0.5296952128410339 - trainLoss: 0.5285868644714355\n",
      "cnt: 0 - valLoss: 0.5296717286109924 - trainLoss: 0.5285660624504089\n",
      "cnt: 0 - valLoss: 0.5296482443809509 - trainLoss: 0.5285451412200928\n",
      "cnt: 0 - valLoss: 0.5296247601509094 - trainLoss: 0.5285242795944214\n",
      "cnt: 0 - valLoss: 0.5296012759208679 - trainLoss: 0.5285034775733948\n",
      "cnt: 0 - valLoss: 0.5295777916908264 - trainLoss: 0.5284826159477234\n",
      "cnt: 0 - valLoss: 0.5295543074607849 - trainLoss: 0.528461754322052\n",
      "cnt: 0 - valLoss: 0.5295308828353882 - trainLoss: 0.5284408926963806\n",
      "cnt: 0 - valLoss: 0.5295074582099915 - trainLoss: 0.5284200310707092\n",
      "cnt: 0 - valLoss: 0.5294840335845947 - trainLoss: 0.5283992290496826\n",
      "cnt: 0 - valLoss: 0.5294605493545532 - trainLoss: 0.5283783674240112\n",
      "cnt: 0 - valLoss: 0.5294371843338013 - trainLoss: 0.5283575057983398\n",
      "cnt: 0 - valLoss: 0.5294137001037598 - trainLoss: 0.5283366441726685\n",
      "cnt: 0 - valLoss: 0.5293903350830078 - trainLoss: 0.5283158421516418\n",
      "cnt: 0 - valLoss: 0.5293669104576111 - trainLoss: 0.5282950401306152\n",
      "cnt: 0 - valLoss: 0.5293434858322144 - trainLoss: 0.5282741785049438\n",
      "cnt: 0 - valLoss: 0.5293201208114624 - trainLoss: 0.5282533764839172\n",
      "cnt: 0 - valLoss: 0.5292966365814209 - trainLoss: 0.5282325148582458\n",
      "cnt: 0 - valLoss: 0.5292732119560242 - trainLoss: 0.5282116532325745\n",
      "cnt: 0 - valLoss: 0.5292497873306274 - trainLoss: 0.5281908512115479\n",
      "cnt: 0 - valLoss: 0.5292264223098755 - trainLoss: 0.5281700491905212\n",
      "cnt: 0 - valLoss: 0.5292030572891235 - trainLoss: 0.5281492471694946\n",
      "cnt: 0 - valLoss: 0.5291796326637268 - trainLoss: 0.528128445148468\n",
      "cnt: 0 - valLoss: 0.5291562080383301 - trainLoss: 0.5281076431274414\n",
      "cnt: 0 - valLoss: 0.5291327834129333 - trainLoss: 0.5280868411064148\n",
      "cnt: 0 - valLoss: 0.5291094183921814 - trainLoss: 0.5280659794807434\n",
      "cnt: 0 - valLoss: 0.5290859937667847 - trainLoss: 0.5280451774597168\n",
      "cnt: 0 - valLoss: 0.5290625691413879 - trainLoss: 0.5280243158340454\n",
      "cnt: 0 - valLoss: 0.529039204120636 - trainLoss: 0.5280035734176636\n",
      "cnt: 0 - valLoss: 0.529015839099884 - trainLoss: 0.5279827117919922\n",
      "cnt: 0 - valLoss: 0.5289924144744873 - trainLoss: 0.5279619693756104\n",
      "cnt: 0 - valLoss: 0.5289690494537354 - trainLoss: 0.5279411673545837\n",
      "cnt: 0 - valLoss: 0.5289456844329834 - trainLoss: 0.5279203653335571\n",
      "cnt: 0 - valLoss: 0.5289223194122314 - trainLoss: 0.5278995633125305\n",
      "cnt: 0 - valLoss: 0.5288988947868347 - trainLoss: 0.5278787612915039\n",
      "cnt: 0 - valLoss: 0.5288755297660828 - trainLoss: 0.5278580188751221\n",
      "cnt: 0 - valLoss: 0.5288521647453308 - trainLoss: 0.5278372168540955\n",
      "cnt: 0 - valLoss: 0.5288287997245789 - trainLoss: 0.5278164744377136\n",
      "cnt: 0 - valLoss: 0.5288054347038269 - trainLoss: 0.527795672416687\n",
      "cnt: 0 - valLoss: 0.5287820100784302 - trainLoss: 0.5277748703956604\n",
      "cnt: 0 - valLoss: 0.5287586450576782 - trainLoss: 0.5277540683746338\n",
      "cnt: 0 - valLoss: 0.5287352800369263 - trainLoss: 0.5277332663536072\n",
      "cnt: 0 - valLoss: 0.5287119150161743 - trainLoss: 0.5277125239372253\n",
      "cnt: 0 - valLoss: 0.5286885499954224 - trainLoss: 0.5276917815208435\n",
      "cnt: 0 - valLoss: 0.5286651849746704 - trainLoss: 0.5276710391044617\n",
      "cnt: 0 - valLoss: 0.5286418795585632 - trainLoss: 0.5276502370834351\n",
      "cnt: 0 - valLoss: 0.5286184549331665 - trainLoss: 0.5276294946670532\n",
      "cnt: 0 - valLoss: 0.5285950899124146 - trainLoss: 0.5276087522506714\n",
      "cnt: 0 - valLoss: 0.5285717248916626 - trainLoss: 0.5275879502296448\n",
      "cnt: 0 - valLoss: 0.5285483598709106 - trainLoss: 0.5275671482086182\n",
      "cnt: 0 - valLoss: 0.5285249948501587 - trainLoss: 0.5275464653968811\n",
      "cnt: 0 - valLoss: 0.5285016894340515 - trainLoss: 0.5275256633758545\n",
      "cnt: 0 - valLoss: 0.5284783244132996 - trainLoss: 0.5275049209594727\n",
      "cnt: 0 - valLoss: 0.5284549593925476 - trainLoss: 0.5274841785430908\n",
      "cnt: 0 - valLoss: 0.5284316539764404 - trainLoss: 0.5274634957313538\n",
      "cnt: 0 - valLoss: 0.5284082889556885 - trainLoss: 0.5274426937103271\n",
      "cnt: 0 - valLoss: 0.5283849835395813 - trainLoss: 0.5274220108985901\n",
      "cnt: 0 - valLoss: 0.5283616185188293 - trainLoss: 0.5274012088775635\n",
      "cnt: 0 - valLoss: 0.5283383131027222 - trainLoss: 0.5273804664611816\n",
      "cnt: 0 - valLoss: 0.5283148884773254 - trainLoss: 0.5273597836494446\n",
      "cnt: 0 - valLoss: 0.528291642665863 - trainLoss: 0.5273390412330627\n",
      "cnt: 0 - valLoss: 0.5282682776451111 - trainLoss: 0.5273182988166809\n",
      "cnt: 0 - valLoss: 0.5282449722290039 - trainLoss: 0.5272975564002991\n",
      "cnt: 0 - valLoss: 0.5282216668128967 - trainLoss: 0.527276873588562\n",
      "cnt: 0 - valLoss: 0.5281983017921448 - trainLoss: 0.5272560715675354\n",
      "cnt: 0 - valLoss: 0.5281749963760376 - trainLoss: 0.5272354483604431\n",
      "cnt: 0 - valLoss: 0.5281516909599304 - trainLoss: 0.5272146463394165\n",
      "cnt: 0 - valLoss: 0.5281283855438232 - trainLoss: 0.5271939635276794\n",
      "cnt: 0 - valLoss: 0.5281050801277161 - trainLoss: 0.5271732807159424\n",
      "cnt: 0 - valLoss: 0.5280817151069641 - trainLoss: 0.5271525382995605\n",
      "cnt: 0 - valLoss: 0.5280584096908569 - trainLoss: 0.5271318554878235\n",
      "cnt: 0 - valLoss: 0.5280351042747498 - trainLoss: 0.5271111130714417\n",
      "cnt: 0 - valLoss: 0.5280118584632874 - trainLoss: 0.5270904302597046\n",
      "cnt: 0 - valLoss: 0.5279884934425354 - trainLoss: 0.5270696878433228\n",
      "cnt: 0 - valLoss: 0.527965247631073 - trainLoss: 0.5270490050315857\n",
      "cnt: 0 - valLoss: 0.5279419422149658 - trainLoss: 0.5270283222198486\n",
      "cnt: 0 - valLoss: 0.5279186367988586 - trainLoss: 0.5270076394081116\n",
      "cnt: 0 - valLoss: 0.5278953909873962 - trainLoss: 0.5269868969917297\n",
      "cnt: 0 - valLoss: 0.5278720855712891 - trainLoss: 0.5269662141799927\n",
      "cnt: 0 - valLoss: 0.5278487801551819 - trainLoss: 0.5269455313682556\n",
      "cnt: 0 - valLoss: 0.5278254747390747 - trainLoss: 0.5269248485565186\n",
      "cnt: 0 - valLoss: 0.5278022289276123 - trainLoss: 0.5269041657447815\n",
      "cnt: 0 - valLoss: 0.5277789831161499 - trainLoss: 0.5268834829330444\n",
      "cnt: 0 - valLoss: 0.5277556777000427 - trainLoss: 0.5268628597259521\n",
      "cnt: 0 - valLoss: 0.5277324318885803 - trainLoss: 0.5268421173095703\n",
      "cnt: 0 - valLoss: 0.5277091264724731 - trainLoss: 0.526821494102478\n",
      "cnt: 0 - valLoss: 0.5276858806610107 - trainLoss: 0.5268007516860962\n",
      "cnt: 0 - valLoss: 0.5276625752449036 - trainLoss: 0.5267801284790039\n",
      "cnt: 0 - valLoss: 0.5276393294334412 - trainLoss: 0.5267594456672668\n",
      "cnt: 0 - valLoss: 0.5276160836219788 - trainLoss: 0.5267387628555298\n",
      "cnt: 0 - valLoss: 0.5275927782058716 - trainLoss: 0.5267181396484375\n",
      "cnt: 0 - valLoss: 0.5275695323944092 - trainLoss: 0.5266974568367004\n",
      "cnt: 0 - valLoss: 0.5275462865829468 - trainLoss: 0.5266768336296082\n",
      "cnt: 0 - valLoss: 0.5275229811668396 - trainLoss: 0.5266561508178711\n",
      "cnt: 0 - valLoss: 0.527499794960022 - trainLoss: 0.5266355276107788\n",
      "cnt: 0 - valLoss: 0.5274765491485596 - trainLoss: 0.5266148447990417\n",
      "cnt: 0 - valLoss: 0.5274532437324524 - trainLoss: 0.5265942215919495\n",
      "cnt: 0 - valLoss: 0.5274300575256348 - trainLoss: 0.5265735983848572\n",
      "cnt: 0 - valLoss: 0.5274068117141724 - trainLoss: 0.5265529155731201\n",
      "cnt: 0 - valLoss: 0.5273835062980652 - trainLoss: 0.5265322923660278\n",
      "cnt: 0 - valLoss: 0.5273603200912476 - trainLoss: 0.5265116691589355\n",
      "cnt: 0 - valLoss: 0.5273370742797852 - trainLoss: 0.5264909863471985\n",
      "cnt: 0 - valLoss: 0.5273138284683228 - trainLoss: 0.5264703631401062\n",
      "cnt: 0 - valLoss: 0.5272905826568604 - trainLoss: 0.5264496803283691\n",
      "cnt: 0 - valLoss: 0.5272673964500427 - trainLoss: 0.5264290571212769\n",
      "cnt: 0 - valLoss: 0.5272441506385803 - trainLoss: 0.5264084935188293\n",
      "cnt: 0 - valLoss: 0.5272209048271179 - trainLoss: 0.5263878107070923\n",
      "cnt: 0 - valLoss: 0.5271977186203003 - trainLoss: 0.5263671875\n",
      "cnt: 0 - valLoss: 0.5271744728088379 - trainLoss: 0.5263466238975525\n",
      "cnt: 0 - valLoss: 0.5271512866020203 - trainLoss: 0.5263260006904602\n",
      "cnt: 0 - valLoss: 0.5271280407905579 - trainLoss: 0.5263053774833679\n",
      "cnt: 0 - valLoss: 0.5271048545837402 - trainLoss: 0.5262847542762756\n",
      "cnt: 0 - valLoss: 0.5270816087722778 - trainLoss: 0.5262641906738281\n",
      "cnt: 0 - valLoss: 0.5270584225654602 - trainLoss: 0.5262435674667358\n",
      "cnt: 0 - valLoss: 0.5270351767539978 - trainLoss: 0.5262229442596436\n",
      "cnt: 0 - valLoss: 0.5270119905471802 - trainLoss: 0.5262023210525513\n",
      "cnt: 0 - valLoss: 0.5269887447357178 - trainLoss: 0.5261817574501038\n",
      "cnt: 0 - valLoss: 0.5269656181335449 - trainLoss: 0.5261611342430115\n",
      "cnt: 0 - valLoss: 0.5269423723220825 - trainLoss: 0.526140570640564\n",
      "cnt: 0 - valLoss: 0.5269191861152649 - trainLoss: 0.5261199474334717\n",
      "cnt: 0 - valLoss: 0.526896059513092 - trainLoss: 0.5260993242263794\n",
      "cnt: 0 - valLoss: 0.5268728733062744 - trainLoss: 0.5260787606239319\n",
      "cnt: 0 - valLoss: 0.5268496870994568 - trainLoss: 0.5260581374168396\n",
      "cnt: 0 - valLoss: 0.5268264412879944 - trainLoss: 0.5260376334190369\n",
      "cnt: 0 - valLoss: 0.5268033146858215 - trainLoss: 0.5260170102119446\n",
      "cnt: 0 - valLoss: 0.5267801284790039 - trainLoss: 0.5259964466094971\n",
      "cnt: 0 - valLoss: 0.5267569422721863 - trainLoss: 0.5259758234024048\n",
      "cnt: 0 - valLoss: 0.5267337560653687 - trainLoss: 0.5259552597999573\n",
      "cnt: 0 - valLoss: 0.526710569858551 - trainLoss: 0.5259346961975098\n",
      "cnt: 0 - valLoss: 0.5266873836517334 - trainLoss: 0.5259140729904175\n",
      "cnt: 0 - valLoss: 0.5266642570495605 - trainLoss: 0.5258935689926147\n",
      "cnt: 0 - valLoss: 0.5266411304473877 - trainLoss: 0.5258730053901672\n",
      "cnt: 0 - valLoss: 0.5266179442405701 - trainLoss: 0.5258524417877197\n",
      "cnt: 0 - valLoss: 0.5265947580337524 - trainLoss: 0.5258318781852722\n",
      "cnt: 0 - valLoss: 0.5265716314315796 - trainLoss: 0.5258112549781799\n",
      "cnt: 0 - valLoss: 0.526548445224762 - trainLoss: 0.5257907509803772\n",
      "cnt: 0 - valLoss: 0.5265252590179443 - trainLoss: 0.5257701873779297\n",
      "cnt: 0 - valLoss: 0.5265021324157715 - trainLoss: 0.5257496237754822\n",
      "cnt: 0 - valLoss: 0.5264789462089539 - trainLoss: 0.5257291197776794\n",
      "cnt: 0 - valLoss: 0.526455819606781 - trainLoss: 0.5257085561752319\n",
      "cnt: 0 - valLoss: 0.5264326930046082 - trainLoss: 0.5256879925727844\n",
      "cnt: 0 - valLoss: 0.5264095067977905 - trainLoss: 0.5256674885749817\n",
      "cnt: 0 - valLoss: 0.5263863801956177 - trainLoss: 0.5256469249725342\n",
      "cnt: 0 - valLoss: 0.5263632535934448 - trainLoss: 0.5256264209747314\n",
      "cnt: 0 - valLoss: 0.526340126991272 - trainLoss: 0.5256058573722839\n",
      "cnt: 0 - valLoss: 0.5263170599937439 - trainLoss: 0.5255852937698364\n",
      "cnt: 0 - valLoss: 0.5262938141822815 - trainLoss: 0.5255647897720337\n",
      "cnt: 0 - valLoss: 0.5262707471847534 - trainLoss: 0.525544285774231\n",
      "cnt: 0 - valLoss: 0.5262476205825806 - trainLoss: 0.5255237221717834\n",
      "cnt: 0 - valLoss: 0.5262245535850525 - trainLoss: 0.5255032181739807\n",
      "cnt: 0 - valLoss: 0.5262014269828796 - trainLoss: 0.525482714176178\n",
      "cnt: 0 - valLoss: 0.526178240776062 - trainLoss: 0.5254622101783752\n",
      "cnt: 0 - valLoss: 0.5261551737785339 - trainLoss: 0.5254417061805725\n",
      "cnt: 0 - valLoss: 0.5261320471763611 - trainLoss: 0.525421142578125\n",
      "cnt: 0 - valLoss: 0.5261089205741882 - trainLoss: 0.5254006385803223\n",
      "cnt: 0 - valLoss: 0.5260857939720154 - trainLoss: 0.5253801345825195\n",
      "cnt: 0 - valLoss: 0.5260627269744873 - trainLoss: 0.5253596901893616\n",
      "cnt: 0 - valLoss: 0.5260396003723145 - trainLoss: 0.5253391265869141\n",
      "cnt: 0 - valLoss: 0.5260165333747864 - trainLoss: 0.5253186225891113\n",
      "cnt: 0 - valLoss: 0.5259934067726135 - trainLoss: 0.5252981781959534\n",
      "cnt: 0 - valLoss: 0.5259702801704407 - trainLoss: 0.5252776145935059\n",
      "cnt: 0 - valLoss: 0.5259472131729126 - trainLoss: 0.5252571702003479\n",
      "cnt: 0 - valLoss: 0.5259241461753845 - trainLoss: 0.5252366065979004\n",
      "cnt: 0 - valLoss: 0.5259010791778564 - trainLoss: 0.5252161622047424\n",
      "cnt: 0 - valLoss: 0.5258779525756836 - trainLoss: 0.5251956582069397\n",
      "cnt: 0 - valLoss: 0.5258548855781555 - trainLoss: 0.5251752138137817\n",
      "cnt: 0 - valLoss: 0.5258318185806274 - trainLoss: 0.5251547694206238\n",
      "cnt: 0 - valLoss: 0.5258087515830994 - trainLoss: 0.525134265422821\n",
      "cnt: 0 - valLoss: 0.5257856249809265 - trainLoss: 0.5251137614250183\n",
      "cnt: 0 - valLoss: 0.5257625579833984 - trainLoss: 0.5250933170318604\n",
      "cnt: 0 - valLoss: 0.5257394909858704 - trainLoss: 0.5250727534294128\n",
      "cnt: 0 - valLoss: 0.5257164835929871 - trainLoss: 0.5250523686408997\n",
      "cnt: 0 - valLoss: 0.5256933569908142 - trainLoss: 0.5250318646430969\n",
      "cnt: 0 - valLoss: 0.5256703495979309 - trainLoss: 0.525011420249939\n",
      "cnt: 0 - valLoss: 0.5256472229957581 - trainLoss: 0.5249909162521362\n",
      "cnt: 0 - valLoss: 0.52562415599823 - trainLoss: 0.5249704718589783\n",
      "cnt: 0 - valLoss: 0.5256010890007019 - trainLoss: 0.5249500274658203\n",
      "cnt: 0 - valLoss: 0.5255780816078186 - trainLoss: 0.5249295830726624\n",
      "cnt: 0 - valLoss: 0.5255549550056458 - trainLoss: 0.5249090790748596\n",
      "cnt: 0 - valLoss: 0.5255318880081177 - trainLoss: 0.5248886346817017\n",
      "cnt: 0 - valLoss: 0.5255088806152344 - trainLoss: 0.5248681902885437\n",
      "cnt: 0 - valLoss: 0.5254858136177063 - trainLoss: 0.5248477458953857\n",
      "cnt: 0 - valLoss: 0.525462806224823 - trainLoss: 0.524827241897583\n",
      "cnt: 0 - valLoss: 0.5254396796226501 - trainLoss: 0.524806797504425\n",
      "cnt: 0 - valLoss: 0.5254166722297668 - trainLoss: 0.5247864127159119\n",
      "cnt: 0 - valLoss: 0.5253936052322388 - trainLoss: 0.5247659087181091\n",
      "cnt: 0 - valLoss: 0.5253705978393555 - trainLoss: 0.5247454047203064\n",
      "cnt: 0 - valLoss: 0.5253475308418274 - trainLoss: 0.5247250199317932\n",
      "cnt: 0 - valLoss: 0.5253244638442993 - trainLoss: 0.5247045755386353\n",
      "cnt: 0 - valLoss: 0.525301456451416 - trainLoss: 0.5246841311454773\n",
      "cnt: 0 - valLoss: 0.5252783894538879 - trainLoss: 0.5246636867523193\n",
      "cnt: 0 - valLoss: 0.5252554416656494 - trainLoss: 0.5246433019638062\n",
      "cnt: 0 - valLoss: 0.5252323746681213 - trainLoss: 0.5246227979660034\n",
      "cnt: 0 - valLoss: 0.525209367275238 - trainLoss: 0.5246024131774902\n",
      "cnt: 0 - valLoss: 0.5251862406730652 - trainLoss: 0.5245819687843323\n",
      "cnt: 0 - valLoss: 0.5251632928848267 - trainLoss: 0.5245615839958191\n",
      "cnt: 0 - valLoss: 0.5251402258872986 - trainLoss: 0.5245411396026611\n",
      "cnt: 0 - valLoss: 0.5251172780990601 - trainLoss: 0.5245206952095032\n",
      "cnt: 0 - valLoss: 0.525094211101532 - trainLoss: 0.52450031042099\n",
      "cnt: 0 - valLoss: 0.5250712037086487 - trainLoss: 0.524479866027832\n",
      "cnt: 0 - valLoss: 0.5250481963157654 - trainLoss: 0.5244595408439636\n",
      "cnt: 0 - valLoss: 0.5250251889228821 - trainLoss: 0.5244390964508057\n",
      "cnt: 0 - valLoss: 0.5250022411346436 - trainLoss: 0.5244187116622925\n",
      "cnt: 0 - valLoss: 0.5249791741371155 - trainLoss: 0.5243983268737793\n",
      "cnt: 0 - valLoss: 0.5249561667442322 - trainLoss: 0.5243779420852661\n",
      "cnt: 0 - valLoss: 0.5249331593513489 - trainLoss: 0.5243575572967529\n",
      "cnt: 0 - valLoss: 0.5249102115631104 - trainLoss: 0.5243371725082397\n",
      "cnt: 0 - valLoss: 0.5248871445655823 - trainLoss: 0.5243167877197266\n",
      "cnt: 0 - valLoss: 0.5248641967773438 - trainLoss: 0.5242964625358582\n",
      "cnt: 0 - valLoss: 0.5248411893844604 - trainLoss: 0.5242760181427002\n",
      "cnt: 0 - valLoss: 0.5248181819915771 - trainLoss: 0.524255633354187\n",
      "cnt: 0 - valLoss: 0.5247951745986938 - trainLoss: 0.5242353081703186\n",
      "cnt: 0 - valLoss: 0.5247722268104553 - trainLoss: 0.5242149233818054\n",
      "cnt: 0 - valLoss: 0.524749219417572 - trainLoss: 0.524194598197937\n",
      "cnt: 0 - valLoss: 0.5247262716293335 - trainLoss: 0.5241742134094238\n",
      "cnt: 0 - valLoss: 0.5247032642364502 - trainLoss: 0.5241538882255554\n",
      "cnt: 0 - valLoss: 0.5246803164482117 - trainLoss: 0.5241334438323975\n",
      "cnt: 0 - valLoss: 0.5246573686599731 - trainLoss: 0.524113118648529\n",
      "cnt: 0 - valLoss: 0.5246343612670898 - trainLoss: 0.5240927934646606\n",
      "cnt: 0 - valLoss: 0.5246114134788513 - trainLoss: 0.5240724086761475\n",
      "cnt: 0 - valLoss: 0.5245884656906128 - trainLoss: 0.524052083492279\n",
      "cnt: 0 - valLoss: 0.5245654582977295 - trainLoss: 0.5240317583084106\n",
      "cnt: 0 - valLoss: 0.5245424509048462 - trainLoss: 0.5240113735198975\n",
      "cnt: 0 - valLoss: 0.5245195031166077 - trainLoss: 0.5239909887313843\n",
      "cnt: 0 - valLoss: 0.5244966149330139 - trainLoss: 0.5239707231521606\n",
      "cnt: 0 - valLoss: 0.5244736671447754 - trainLoss: 0.5239503383636475\n",
      "cnt: 0 - valLoss: 0.5244507193565369 - trainLoss: 0.5239300727844238\n",
      "cnt: 0 - valLoss: 0.5244277119636536 - trainLoss: 0.5239096879959106\n",
      "cnt: 0 - valLoss: 0.5244048237800598 - trainLoss: 0.523889422416687\n",
      "cnt: 0 - valLoss: 0.5243818163871765 - trainLoss: 0.5238690376281738\n",
      "cnt: 0 - valLoss: 0.5243589282035828 - trainLoss: 0.5238487124443054\n",
      "cnt: 0 - valLoss: 0.5243359804153442 - trainLoss: 0.523828387260437\n",
      "cnt: 0 - valLoss: 0.5243129730224609 - trainLoss: 0.5238080620765686\n",
      "cnt: 0 - valLoss: 0.5242900848388672 - trainLoss: 0.523787796497345\n",
      "cnt: 0 - valLoss: 0.5242671966552734 - trainLoss: 0.5237674713134766\n",
      "cnt: 0 - valLoss: 0.5242442488670349 - trainLoss: 0.5237471461296082\n",
      "cnt: 0 - valLoss: 0.5242213606834412 - trainLoss: 0.5237268209457397\n",
      "cnt: 0 - valLoss: 0.5241983532905579 - trainLoss: 0.5237065553665161\n",
      "cnt: 0 - valLoss: 0.5241754651069641 - trainLoss: 0.5236862897872925\n",
      "cnt: 0 - valLoss: 0.5241525769233704 - trainLoss: 0.5236659049987793\n",
      "cnt: 0 - valLoss: 0.5241296291351318 - trainLoss: 0.5236456394195557\n",
      "cnt: 0 - valLoss: 0.5241067409515381 - trainLoss: 0.5236253142356873\n",
      "cnt: 0 - valLoss: 0.5240838527679443 - trainLoss: 0.5236050486564636\n",
      "cnt: 0 - valLoss: 0.5240609645843506 - trainLoss: 0.5235847234725952\n",
      "cnt: 0 - valLoss: 0.5240380167961121 - trainLoss: 0.5235644578933716\n",
      "cnt: 0 - valLoss: 0.5240151286125183 - trainLoss: 0.5235442519187927\n",
      "cnt: 0 - valLoss: 0.5239922404289246 - trainLoss: 0.5235239863395691\n",
      "cnt: 0 - valLoss: 0.5239693522453308 - trainLoss: 0.5235036611557007\n",
      "cnt: 0 - valLoss: 0.5239464640617371 - trainLoss: 0.523483395576477\n",
      "cnt: 0 - valLoss: 0.5239235758781433 - trainLoss: 0.5234630703926086\n",
      "cnt: 0 - valLoss: 0.5239006876945496 - trainLoss: 0.523442804813385\n",
      "cnt: 0 - valLoss: 0.5238777995109558 - trainLoss: 0.5234225392341614\n",
      "cnt: 0 - valLoss: 0.5238549113273621 - trainLoss: 0.5234022736549377\n",
      "cnt: 0 - valLoss: 0.5238320231437683 - trainLoss: 0.5233820676803589\n",
      "cnt: 0 - valLoss: 0.5238091945648193 - trainLoss: 0.5233618021011353\n",
      "cnt: 0 - valLoss: 0.5237863063812256 - trainLoss: 0.5233415365219116\n",
      "cnt: 0 - valLoss: 0.5237634181976318 - trainLoss: 0.523321270942688\n",
      "cnt: 0 - valLoss: 0.5237405896186829 - trainLoss: 0.5233010649681091\n",
      "cnt: 0 - valLoss: 0.5237177014350891 - trainLoss: 0.5232807397842407\n",
      "cnt: 0 - valLoss: 0.5236948132514954 - trainLoss: 0.5232605338096619\n",
      "cnt: 0 - valLoss: 0.5236719846725464 - trainLoss: 0.5232402682304382\n",
      "cnt: 0 - valLoss: 0.5236491560935974 - trainLoss: 0.5232200026512146\n",
      "cnt: 0 - valLoss: 0.5236262679100037 - trainLoss: 0.5231997966766357\n",
      "cnt: 0 - valLoss: 0.5236034393310547 - trainLoss: 0.5231795907020569\n",
      "cnt: 0 - valLoss: 0.5235805511474609 - trainLoss: 0.5231593251228333\n",
      "cnt: 0 - valLoss: 0.5235576629638672 - trainLoss: 0.5231390595436096\n",
      "cnt: 0 - valLoss: 0.523534893989563 - trainLoss: 0.5231189131736755\n",
      "cnt: 0 - valLoss: 0.523512065410614 - trainLoss: 0.5230986475944519\n",
      "cnt: 0 - valLoss: 0.523489236831665 - trainLoss: 0.5230783820152283\n",
      "cnt: 0 - valLoss: 0.5234663486480713 - trainLoss: 0.5230581760406494\n",
      "cnt: 0 - valLoss: 0.5234435200691223 - trainLoss: 0.5230379700660706\n",
      "cnt: 0 - valLoss: 0.5234206914901733 - trainLoss: 0.5230177640914917\n",
      "cnt: 0 - valLoss: 0.5233978629112244 - trainLoss: 0.5229975581169128\n",
      "cnt: 0 - valLoss: 0.5233750343322754 - trainLoss: 0.5229774117469788\n",
      "cnt: 0 - valLoss: 0.5233522653579712 - trainLoss: 0.5229572057723999\n",
      "cnt: 0 - valLoss: 0.5233293771743774 - trainLoss: 0.5229369401931763\n",
      "cnt: 0 - valLoss: 0.5233065485954285 - trainLoss: 0.5229167938232422\n",
      "cnt: 0 - valLoss: 0.5232837796211243 - trainLoss: 0.5228965282440186\n",
      "cnt: 0 - valLoss: 0.5232608914375305 - trainLoss: 0.5228763818740845\n",
      "cnt: 0 - valLoss: 0.5232381820678711 - trainLoss: 0.5228561758995056\n",
      "cnt: 0 - valLoss: 0.5232152938842773 - trainLoss: 0.5228359699249268\n",
      "cnt: 0 - valLoss: 0.5231925249099731 - trainLoss: 0.5228157639503479\n",
      "cnt: 0 - valLoss: 0.5231696963310242 - trainLoss: 0.522795557975769\n",
      "cnt: 0 - valLoss: 0.5231468677520752 - trainLoss: 0.522775411605835\n",
      "cnt: 0 - valLoss: 0.5231241583824158 - trainLoss: 0.5227552056312561\n",
      "cnt: 0 - valLoss: 0.523101270198822 - trainLoss: 0.522735059261322\n",
      "cnt: 0 - valLoss: 0.5230785608291626 - trainLoss: 0.5227148532867432\n",
      "cnt: 0 - valLoss: 0.5230557322502136 - trainLoss: 0.5226947069168091\n",
      "cnt: 0 - valLoss: 0.5230329632759094 - trainLoss: 0.5226745009422302\n",
      "cnt: 0 - valLoss: 0.5230101943016052 - trainLoss: 0.5226543545722961\n",
      "cnt: 0 - valLoss: 0.522987425327301 - trainLoss: 0.5226342082023621\n",
      "cnt: 0 - valLoss: 0.5229646563529968 - trainLoss: 0.522614061832428\n",
      "cnt: 0 - valLoss: 0.5229418277740479 - trainLoss: 0.5225938558578491\n",
      "cnt: 0 - valLoss: 0.5229190587997437 - trainLoss: 0.5225737690925598\n",
      "cnt: 0 - valLoss: 0.5228963494300842 - trainLoss: 0.522553563117981\n",
      "cnt: 0 - valLoss: 0.52287358045578 - trainLoss: 0.5225334167480469\n",
      "cnt: 0 - valLoss: 0.5228508114814758 - trainLoss: 0.5225132703781128\n",
      "cnt: 0 - valLoss: 0.5228279829025269 - trainLoss: 0.5224931240081787\n",
      "cnt: 0 - valLoss: 0.5228052735328674 - trainLoss: 0.5224729776382446\n",
      "cnt: 0 - valLoss: 0.5227825045585632 - trainLoss: 0.5224528908729553\n",
      "cnt: 0 - valLoss: 0.5227597951889038 - trainLoss: 0.5224327445030212\n",
      "cnt: 0 - valLoss: 0.5227370262145996 - trainLoss: 0.5224125981330872\n",
      "cnt: 0 - valLoss: 0.5227142572402954 - trainLoss: 0.5223924517631531\n",
      "cnt: 0 - valLoss: 0.522691547870636 - trainLoss: 0.522372305393219\n",
      "cnt: 0 - valLoss: 0.5226687788963318 - trainLoss: 0.5223521590232849\n",
      "cnt: 0 - valLoss: 0.5226460695266724 - trainLoss: 0.5223320722579956\n",
      "cnt: 0 - valLoss: 0.5226233005523682 - trainLoss: 0.5223119258880615\n",
      "cnt: 0 - valLoss: 0.522600531578064 - trainLoss: 0.5222918391227722\n",
      "cnt: 0 - valLoss: 0.5225778818130493 - trainLoss: 0.5222716927528381\n",
      "cnt: 0 - valLoss: 0.5225551724433899 - trainLoss: 0.522251546382904\n",
      "cnt: 0 - valLoss: 0.5225324034690857 - trainLoss: 0.5222314596176147\n",
      "cnt: 0 - valLoss: 0.5225096940994263 - trainLoss: 0.5222113132476807\n",
      "cnt: 0 - valLoss: 0.5224869251251221 - trainLoss: 0.5221912860870361\n",
      "cnt: 0 - valLoss: 0.5224642157554626 - trainLoss: 0.5221711993217468\n",
      "cnt: 0 - valLoss: 0.5224415063858032 - trainLoss: 0.5221510529518127\n",
      "cnt: 0 - valLoss: 0.5224188566207886 - trainLoss: 0.5221309661865234\n",
      "cnt: 0 - valLoss: 0.5223960876464844 - trainLoss: 0.5221108794212341\n",
      "cnt: 0 - valLoss: 0.5223734378814697 - trainLoss: 0.5220907926559448\n",
      "cnt: 0 - valLoss: 0.5223506689071655 - trainLoss: 0.5220706462860107\n",
      "cnt: 0 - valLoss: 0.5223279595375061 - trainLoss: 0.5220506191253662\n",
      "cnt: 0 - valLoss: 0.5223053693771362 - trainLoss: 0.5220305323600769\n",
      "cnt: 0 - valLoss: 0.5222825407981873 - trainLoss: 0.5220105051994324\n",
      "cnt: 0 - valLoss: 0.5222598910331726 - trainLoss: 0.5219903588294983\n",
      "cnt: 0 - valLoss: 0.522237241268158 - trainLoss: 0.5219703316688538\n",
      "cnt: 0 - valLoss: 0.5222145915031433 - trainLoss: 0.5219502449035645\n",
      "cnt: 0 - valLoss: 0.5221918821334839 - trainLoss: 0.5219302177429199\n",
      "cnt: 0 - valLoss: 0.5221691727638245 - trainLoss: 0.5219100713729858\n",
      "cnt: 0 - valLoss: 0.5221465229988098 - trainLoss: 0.5218900442123413\n",
      "cnt: 0 - valLoss: 0.5221238136291504 - trainLoss: 0.5218700170516968\n",
      "cnt: 0 - valLoss: 0.5221011638641357 - trainLoss: 0.5218499302864075\n",
      "cnt: 0 - valLoss: 0.5220785140991211 - trainLoss: 0.5218298435211182\n",
      "cnt: 0 - valLoss: 0.5220558047294617 - trainLoss: 0.5218098163604736\n",
      "cnt: 0 - valLoss: 0.522033154964447 - trainLoss: 0.5217897295951843\n",
      "cnt: 0 - valLoss: 0.5220105051994324 - trainLoss: 0.5217697024345398\n",
      "cnt: 0 - valLoss: 0.5219878554344177 - trainLoss: 0.5217496752738953\n",
      "cnt: 0 - valLoss: 0.5219652056694031 - trainLoss: 0.5217296481132507\n",
      "cnt: 0 - valLoss: 0.5219425559043884 - trainLoss: 0.5217096209526062\n",
      "cnt: 0 - valLoss: 0.5219199061393738 - trainLoss: 0.5216895937919617\n",
      "cnt: 0 - valLoss: 0.5218971967697144 - trainLoss: 0.5216695070266724\n",
      "cnt: 0 - valLoss: 0.5218746066093445 - trainLoss: 0.5216494798660278\n",
      "cnt: 0 - valLoss: 0.5218519568443298 - trainLoss: 0.5216294527053833\n",
      "cnt: 0 - valLoss: 0.5218293070793152 - trainLoss: 0.5216094255447388\n",
      "cnt: 0 - valLoss: 0.5218067169189453 - trainLoss: 0.5215893983840942\n",
      "cnt: 0 - valLoss: 0.5217840075492859 - trainLoss: 0.5215694308280945\n",
      "cnt: 0 - valLoss: 0.521761417388916 - trainLoss: 0.52154940366745\n",
      "cnt: 0 - valLoss: 0.5217388272285461 - trainLoss: 0.5215293765068054\n",
      "cnt: 0 - valLoss: 0.5217161774635315 - trainLoss: 0.5215093493461609\n",
      "cnt: 0 - valLoss: 0.5216935873031616 - trainLoss: 0.5214893221855164\n",
      "cnt: 0 - valLoss: 0.521670937538147 - trainLoss: 0.5214694142341614\n",
      "cnt: 0 - valLoss: 0.5216483473777771 - trainLoss: 0.5214493870735168\n",
      "cnt: 0 - valLoss: 0.5216257572174072 - trainLoss: 0.5214293599128723\n",
      "cnt: 0 - valLoss: 0.5216031074523926 - trainLoss: 0.5214093923568726\n",
      "cnt: 0 - valLoss: 0.5215804576873779 - trainLoss: 0.5213894248008728\n",
      "cnt: 0 - valLoss: 0.5215579271316528 - trainLoss: 0.5213693976402283\n",
      "cnt: 0 - valLoss: 0.5215352773666382 - trainLoss: 0.5213494896888733\n",
      "cnt: 0 - valLoss: 0.5215127468109131 - trainLoss: 0.5213294625282288\n",
      "cnt: 0 - valLoss: 0.5214900970458984 - trainLoss: 0.5213093757629395\n",
      "cnt: 0 - valLoss: 0.5214675068855286 - trainLoss: 0.5212894082069397\n",
      "cnt: 0 - valLoss: 0.5214449167251587 - trainLoss: 0.5212694406509399\n",
      "cnt: 0 - valLoss: 0.5214223265647888 - trainLoss: 0.5212494730949402\n",
      "cnt: 0 - valLoss: 0.521399736404419 - trainLoss: 0.5212295055389404\n",
      "cnt: 0 - valLoss: 0.5213771462440491 - trainLoss: 0.5212094783782959\n",
      "cnt: 0 - valLoss: 0.5213545560836792 - trainLoss: 0.5211895704269409\n",
      "cnt: 0 - valLoss: 0.5213319659233093 - trainLoss: 0.5211696028709412\n",
      "cnt: 0 - valLoss: 0.5213094353675842 - trainLoss: 0.5211496949195862\n",
      "cnt: 0 - valLoss: 0.5212867856025696 - trainLoss: 0.5211297273635864\n",
      "cnt: 0 - valLoss: 0.5212642550468445 - trainLoss: 0.5211097598075867\n",
      "cnt: 0 - valLoss: 0.5212416648864746 - trainLoss: 0.5210898518562317\n",
      "cnt: 0 - valLoss: 0.5212190747261047 - trainLoss: 0.5210698843002319\n",
      "cnt: 0 - valLoss: 0.5211965441703796 - trainLoss: 0.521049976348877\n",
      "cnt: 0 - valLoss: 0.5211739540100098 - trainLoss: 0.5210300087928772\n",
      "cnt: 0 - valLoss: 0.5211513638496399 - trainLoss: 0.521010160446167\n",
      "cnt: 0 - valLoss: 0.52112877368927 - trainLoss: 0.520990252494812\n",
      "cnt: 0 - valLoss: 0.5211062431335449 - trainLoss: 0.520970344543457\n",
      "cnt: 0 - valLoss: 0.521083652973175 - trainLoss: 0.5209503769874573\n",
      "cnt: 0 - valLoss: 0.5210611820220947 - trainLoss: 0.5209304690361023\n",
      "cnt: 0 - valLoss: 0.5210385918617249 - trainLoss: 0.5209105610847473\n",
      "cnt: 0 - valLoss: 0.521016001701355 - trainLoss: 0.5208906531333923\n",
      "cnt: 0 - valLoss: 0.5209934711456299 - trainLoss: 0.5208707451820374\n",
      "cnt: 0 - valLoss: 0.52097088098526 - trainLoss: 0.5208508968353271\n",
      "cnt: 0 - valLoss: 0.5209483504295349 - trainLoss: 0.5208309888839722\n",
      "cnt: 0 - valLoss: 0.5209258198738098 - trainLoss: 0.5208110809326172\n",
      "cnt: 0 - valLoss: 0.5209032297134399 - trainLoss: 0.5207911729812622\n",
      "cnt: 0 - valLoss: 0.5208808183670044 - trainLoss: 0.520771324634552\n",
      "cnt: 0 - valLoss: 0.5208582282066345 - trainLoss: 0.520751416683197\n",
      "cnt: 0 - valLoss: 0.5208356976509094 - trainLoss: 0.5207315683364868\n",
      "cnt: 0 - valLoss: 0.5208131670951843 - trainLoss: 0.5207116603851318\n",
      "cnt: 0 - valLoss: 0.5207906365394592 - trainLoss: 0.5206918120384216\n",
      "cnt: 0 - valLoss: 0.5207681059837341 - trainLoss: 0.5206719040870667\n",
      "cnt: 0 - valLoss: 0.5207456350326538 - trainLoss: 0.5206521153450012\n",
      "cnt: 0 - valLoss: 0.5207231640815735 - trainLoss: 0.5206322073936462\n",
      "cnt: 0 - valLoss: 0.5207006335258484 - trainLoss: 0.5206124186515808\n",
      "cnt: 0 - valLoss: 0.5206781029701233 - trainLoss: 0.5205925703048706\n",
      "cnt: 0 - valLoss: 0.520655632019043 - trainLoss: 0.5205727219581604\n",
      "cnt: 0 - valLoss: 0.5206331610679626 - trainLoss: 0.5205528140068054\n",
      "cnt: 0 - valLoss: 0.5206106901168823 - trainLoss: 0.5205330848693848\n",
      "cnt: 0 - valLoss: 0.5205881595611572 - trainLoss: 0.5205131769180298\n",
      "cnt: 0 - valLoss: 0.5205656886100769 - trainLoss: 0.5204933881759644\n",
      "cnt: 0 - valLoss: 0.5205432176589966 - trainLoss: 0.5204735994338989\n",
      "cnt: 0 - valLoss: 0.5205207467079163 - trainLoss: 0.5204538106918335\n",
      "cnt: 0 - valLoss: 0.5204982161521912 - trainLoss: 0.5204339623451233\n",
      "cnt: 0 - valLoss: 0.5204757452011108 - trainLoss: 0.5204140543937683\n",
      "cnt: 0 - valLoss: 0.5204532742500305 - trainLoss: 0.5203942656517029\n",
      "cnt: 0 - valLoss: 0.5204307436943054 - trainLoss: 0.5203744173049927\n",
      "cnt: 0 - valLoss: 0.5204082727432251 - trainLoss: 0.5203546285629272\n",
      "cnt: 0 - valLoss: 0.5203858017921448 - trainLoss: 0.520334780216217\n",
      "cnt: 0 - valLoss: 0.5203633904457092 - trainLoss: 0.5203149318695068\n",
      "cnt: 0 - valLoss: 0.5203409194946289 - trainLoss: 0.5202952027320862\n",
      "cnt: 0 - valLoss: 0.5203184485435486 - trainLoss: 0.5202754139900208\n",
      "cnt: 0 - valLoss: 0.5202959775924683 - trainLoss: 0.5202555656433105\n",
      "cnt: 0 - valLoss: 0.5202735066413879 - trainLoss: 0.5202357769012451\n",
      "cnt: 0 - valLoss: 0.5202510952949524 - trainLoss: 0.5202159285545349\n",
      "cnt: 0 - valLoss: 0.5202286243438721 - trainLoss: 0.5201961994171143\n",
      "cnt: 0 - valLoss: 0.5202062129974365 - trainLoss: 0.5201764106750488\n",
      "cnt: 0 - valLoss: 0.520183801651001 - trainLoss: 0.5201565623283386\n",
      "cnt: 0 - valLoss: 0.5201613306999207 - trainLoss: 0.520136833190918\n",
      "cnt: 0 - valLoss: 0.5201388597488403 - trainLoss: 0.5201170444488525\n",
      "cnt: 0 - valLoss: 0.5201164484024048 - trainLoss: 0.5200972557067871\n",
      "cnt: 0 - valLoss: 0.5200940370559692 - trainLoss: 0.5200774669647217\n",
      "cnt: 0 - valLoss: 0.5200715661048889 - trainLoss: 0.520057737827301\n",
      "cnt: 0 - valLoss: 0.5200491547584534 - trainLoss: 0.5200379490852356\n",
      "cnt: 0 - valLoss: 0.5200267434120178 - trainLoss: 0.5200181603431702\n",
      "cnt: 0 - valLoss: 0.5200043320655823 - trainLoss: 0.5199983716011047\n",
      "cnt: 0 - valLoss: 0.5199819207191467 - trainLoss: 0.5199786424636841\n",
      "cnt: 0 - valLoss: 0.5199594497680664 - trainLoss: 0.5199589133262634\n",
      "cnt: 0 - valLoss: 0.5199370384216309 - trainLoss: 0.5199391841888428\n",
      "cnt: 0 - valLoss: 0.5199146270751953 - trainLoss: 0.5199193954467773\n",
      "cnt: 0 - valLoss: 0.5198922157287598 - trainLoss: 0.5198996663093567\n",
      "cnt: 0 - valLoss: 0.5198697447776794 - trainLoss: 0.519879937171936\n",
      "cnt: 0 - valLoss: 0.5198473930358887 - trainLoss: 0.5198602080345154\n",
      "cnt: 0 - valLoss: 0.5198250412940979 - trainLoss: 0.5198405385017395\n",
      "cnt: 0 - valLoss: 0.5198025703430176 - trainLoss: 0.5198208093643188\n",
      "cnt: 0 - valLoss: 0.5197802186012268 - trainLoss: 0.5198010802268982\n",
      "cnt: 0 - valLoss: 0.5197578072547913 - trainLoss: 0.5197814106941223\n",
      "cnt: 0 - valLoss: 0.5197354555130005 - trainLoss: 0.5197616219520569\n",
      "cnt: 0 - valLoss: 0.5197129845619202 - trainLoss: 0.5197418928146362\n",
      "cnt: 0 - valLoss: 0.5196906328201294 - trainLoss: 0.5197221636772156\n",
      "cnt: 0 - valLoss: 0.5196682214736938 - trainLoss: 0.5197024941444397\n",
      "cnt: 0 - valLoss: 0.5196459293365479 - trainLoss: 0.5196828246116638\n",
      "cnt: 0 - valLoss: 0.5196235179901123 - trainLoss: 0.5196630954742432\n",
      "cnt: 0 - valLoss: 0.5196011066436768 - trainLoss: 0.5196433663368225\n",
      "cnt: 0 - valLoss: 0.519578754901886 - trainLoss: 0.5196236968040466\n",
      "cnt: 0 - valLoss: 0.5195563435554504 - trainLoss: 0.5196040272712708\n",
      "cnt: 0 - valLoss: 0.5195340514183044 - trainLoss: 0.5195842981338501\n",
      "cnt: 0 - valLoss: 0.5195116996765137 - trainLoss: 0.5195646286010742\n",
      "cnt: 0 - valLoss: 0.5194893479347229 - trainLoss: 0.5195449590682983\n",
      "cnt: 0 - valLoss: 0.5194669365882874 - trainLoss: 0.5195252895355225\n",
      "cnt: 0 - valLoss: 0.5194445848464966 - trainLoss: 0.5195056796073914\n",
      "cnt: 0 - valLoss: 0.5194222331047058 - trainLoss: 0.5194859504699707\n",
      "cnt: 0 - valLoss: 0.5193999409675598 - trainLoss: 0.5194662809371948\n",
      "cnt: 0 - valLoss: 0.5193775296211243 - trainLoss: 0.519446611404419\n",
      "cnt: 0 - valLoss: 0.5193552374839783 - trainLoss: 0.5194268822669983\n",
      "cnt: 0 - valLoss: 0.5193328857421875 - trainLoss: 0.5194072723388672\n",
      "cnt: 0 - valLoss: 0.5193105340003967 - trainLoss: 0.5193875432014465\n",
      "cnt: 0 - valLoss: 0.5192882418632507 - trainLoss: 0.5193679332733154\n",
      "cnt: 0 - valLoss: 0.5192659497261047 - trainLoss: 0.5193482637405396\n",
      "cnt: 0 - valLoss: 0.5192435383796692 - trainLoss: 0.5193286538124084\n",
      "cnt: 0 - valLoss: 0.5192212462425232 - trainLoss: 0.5193090438842773\n",
      "cnt: 0 - valLoss: 0.519199013710022 - trainLoss: 0.5192893147468567\n",
      "cnt: 0 - valLoss: 0.5191766619682312 - trainLoss: 0.5192697048187256\n",
      "cnt: 0 - valLoss: 0.5191543102264404 - trainLoss: 0.5192500948905945\n",
      "cnt: 0 - valLoss: 0.5191320180892944 - trainLoss: 0.5192304253578186\n",
      "cnt: 0 - valLoss: 0.5191097259521484 - trainLoss: 0.5192108154296875\n",
      "cnt: 0 - valLoss: 0.5190874338150024 - trainLoss: 0.5191911458969116\n",
      "cnt: 0 - valLoss: 0.5190651416778564 - trainLoss: 0.5191715359687805\n",
      "cnt: 0 - valLoss: 0.5190428495407104 - trainLoss: 0.5191519260406494\n",
      "cnt: 0 - valLoss: 0.5190205574035645 - trainLoss: 0.5191323161125183\n",
      "cnt: 0 - valLoss: 0.5189982056617737 - trainLoss: 0.5191127061843872\n",
      "cnt: 0 - valLoss: 0.5189759135246277 - trainLoss: 0.5190930366516113\n",
      "cnt: 0 - valLoss: 0.5189536213874817 - trainLoss: 0.5190734267234802\n",
      "cnt: 0 - valLoss: 0.5189313888549805 - trainLoss: 0.5190538167953491\n",
      "cnt: 0 - valLoss: 0.5189091563224792 - trainLoss: 0.5190342664718628\n",
      "cnt: 0 - valLoss: 0.5188868641853333 - trainLoss: 0.5190146565437317\n",
      "cnt: 0 - valLoss: 0.5188645720481873 - trainLoss: 0.5189950466156006\n",
      "cnt: 0 - valLoss: 0.518842339515686 - trainLoss: 0.5189754366874695\n",
      "cnt: 0 - valLoss: 0.51882004737854 - trainLoss: 0.5189558267593384\n",
      "cnt: 0 - valLoss: 0.5187978148460388 - trainLoss: 0.5189362168312073\n",
      "cnt: 0 - valLoss: 0.5187755823135376 - trainLoss: 0.518916666507721\n",
      "cnt: 0 - valLoss: 0.5187532901763916 - trainLoss: 0.5188971161842346\n",
      "cnt: 0 - valLoss: 0.5187309980392456 - trainLoss: 0.5188775062561035\n",
      "cnt: 0 - valLoss: 0.5187087655067444 - trainLoss: 0.5188579559326172\n",
      "cnt: 0 - valLoss: 0.5186865329742432 - trainLoss: 0.5188383460044861\n",
      "cnt: 0 - valLoss: 0.5186643004417419 - trainLoss: 0.5188187956809998\n",
      "cnt: 0 - valLoss: 0.5186420679092407 - trainLoss: 0.5187991857528687\n",
      "cnt: 0 - valLoss: 0.5186198353767395 - trainLoss: 0.5187796354293823\n",
      "cnt: 0 - valLoss: 0.5185975432395935 - trainLoss: 0.518760085105896\n",
      "cnt: 0 - valLoss: 0.5185753107070923 - trainLoss: 0.5187405347824097\n",
      "cnt: 0 - valLoss: 0.5185530781745911 - trainLoss: 0.5187209248542786\n",
      "cnt: 0 - valLoss: 0.5185309052467346 - trainLoss: 0.518701434135437\n",
      "cnt: 0 - valLoss: 0.5185086727142334 - trainLoss: 0.5186818242073059\n",
      "cnt: 0 - valLoss: 0.5184864401817322 - trainLoss: 0.5186622738838196\n",
      "cnt: 0 - valLoss: 0.5184642672538757 - trainLoss: 0.518642783164978\n",
      "cnt: 0 - valLoss: 0.5184419751167297 - trainLoss: 0.5186232328414917\n",
      "cnt: 0 - valLoss: 0.5184198021888733 - trainLoss: 0.5186036229133606\n",
      "cnt: 0 - valLoss: 0.5183975696563721 - trainLoss: 0.5185840725898743\n",
      "cnt: 0 - valLoss: 0.5183753371238708 - trainLoss: 0.5185645818710327\n",
      "cnt: 0 - valLoss: 0.5183532238006592 - trainLoss: 0.5185450315475464\n",
      "cnt: 0 - valLoss: 0.518330991268158 - trainLoss: 0.5185255408287048\n",
      "cnt: 0 - valLoss: 0.5183088183403015 - trainLoss: 0.5185060501098633\n",
      "cnt: 0 - valLoss: 0.5182866454124451 - trainLoss: 0.5184864401817322\n",
      "cnt: 0 - valLoss: 0.5182644128799438 - trainLoss: 0.5184669494628906\n",
      "cnt: 0 - valLoss: 0.5182422399520874 - trainLoss: 0.5184473991394043\n",
      "cnt: 0 - valLoss: 0.5182201266288757 - trainLoss: 0.5184279084205627\n",
      "cnt: 0 - valLoss: 0.5181978940963745 - trainLoss: 0.5184084177017212\n",
      "cnt: 0 - valLoss: 0.5181757211685181 - trainLoss: 0.5183889269828796\n",
      "cnt: 0 - valLoss: 0.5181534886360168 - trainLoss: 0.5183693766593933\n",
      "cnt: 0 - valLoss: 0.5181313753128052 - trainLoss: 0.5183499455451965\n",
      "cnt: 0 - valLoss: 0.5181092023849487 - trainLoss: 0.5183303952217102\n",
      "cnt: 0 - valLoss: 0.5180869698524475 - trainLoss: 0.5183109045028687\n",
      "cnt: 0 - valLoss: 0.5180648565292358 - trainLoss: 0.5182914137840271\n",
      "cnt: 0 - valLoss: 0.5180427432060242 - trainLoss: 0.5182719230651855\n",
      "cnt: 0 - valLoss: 0.5180205702781677 - trainLoss: 0.518252432346344\n",
      "cnt: 0 - valLoss: 0.5179983973503113 - trainLoss: 0.5182329416275024\n",
      "cnt: 0 - valLoss: 0.5179761648178101 - trainLoss: 0.5182134509086609\n",
      "cnt: 0 - valLoss: 0.5179541110992432 - trainLoss: 0.5181939601898193\n",
      "cnt: 0 - valLoss: 0.5179319381713867 - trainLoss: 0.5181745290756226\n",
      "cnt: 0 - valLoss: 0.5179097652435303 - trainLoss: 0.5181549787521362\n",
      "cnt: 0 - valLoss: 0.5178876519203186 - trainLoss: 0.5181355476379395\n",
      "cnt: 0 - valLoss: 0.5178655385971069 - trainLoss: 0.5181160569190979\n",
      "cnt: 0 - valLoss: 0.5178434252738953 - trainLoss: 0.5180966854095459\n",
      "cnt: 0 - valLoss: 0.5178212523460388 - trainLoss: 0.5180771350860596\n",
      "cnt: 0 - valLoss: 0.5177991390228271 - trainLoss: 0.5180577635765076\n",
      "cnt: 0 - valLoss: 0.5177770853042603 - trainLoss: 0.5180382132530212\n",
      "cnt: 0 - valLoss: 0.5177549123764038 - trainLoss: 0.5180187821388245\n",
      "cnt: 0 - valLoss: 0.5177327990531921 - trainLoss: 0.5179993510246277\n",
      "cnt: 0 - valLoss: 0.5177107453346252 - trainLoss: 0.5179799199104309\n",
      "cnt: 0 - valLoss: 0.5176885724067688 - trainLoss: 0.5179604887962341\n",
      "cnt: 0 - valLoss: 0.5176665186882019 - trainLoss: 0.5179410576820374\n",
      "cnt: 0 - valLoss: 0.5176444053649902 - trainLoss: 0.5179216265678406\n",
      "cnt: 0 - valLoss: 0.5176222920417786 - trainLoss: 0.5179021954536438\n",
      "cnt: 0 - valLoss: 0.5176001787185669 - trainLoss: 0.517882764339447\n",
      "cnt: 0 - valLoss: 0.5175780653953552 - trainLoss: 0.5178633332252502\n",
      "cnt: 0 - valLoss: 0.5175560116767883 - trainLoss: 0.5178439021110535\n",
      "cnt: 0 - valLoss: 0.5175338983535767 - trainLoss: 0.5178244709968567\n",
      "cnt: 0 - valLoss: 0.5175118446350098 - trainLoss: 0.5178050398826599\n",
      "cnt: 0 - valLoss: 0.5174897313117981 - trainLoss: 0.5177856087684631\n",
      "cnt: 0 - valLoss: 0.5174676179885864 - trainLoss: 0.5177661776542664\n",
      "cnt: 0 - valLoss: 0.5174456238746643 - trainLoss: 0.5177468061447144\n",
      "cnt: 0 - valLoss: 0.5174235105514526 - trainLoss: 0.5177273750305176\n",
      "cnt: 0 - valLoss: 0.5174014568328857 - trainLoss: 0.5177080035209656\n",
      "cnt: 0 - valLoss: 0.5173793435096741 - trainLoss: 0.5176885724067688\n",
      "cnt: 0 - valLoss: 0.5173574090003967 - trainLoss: 0.5176692008972168\n",
      "cnt: 0 - valLoss: 0.5173352956771851 - trainLoss: 0.5176498293876648\n",
      "cnt: 0 - valLoss: 0.5173132419586182 - trainLoss: 0.5176304578781128\n",
      "cnt: 0 - valLoss: 0.5172911882400513 - trainLoss: 0.5176110863685608\n",
      "cnt: 0 - valLoss: 0.5172691345214844 - trainLoss: 0.517591655254364\n",
      "cnt: 0 - valLoss: 0.5172471404075623 - trainLoss: 0.517572283744812\n",
      "cnt: 0 - valLoss: 0.5172250270843506 - trainLoss: 0.5175528526306152\n",
      "cnt: 0 - valLoss: 0.5172030329704285 - trainLoss: 0.517533540725708\n",
      "cnt: 0 - valLoss: 0.5171809792518616 - trainLoss: 0.517514169216156\n",
      "cnt: 0 - valLoss: 0.5171589255332947 - trainLoss: 0.517494797706604\n",
      "cnt: 0 - valLoss: 0.5171369314193726 - trainLoss: 0.517475426197052\n",
      "cnt: 0 - valLoss: 0.5171149373054504 - trainLoss: 0.5174560546875\n",
      "cnt: 0 - valLoss: 0.5170928239822388 - trainLoss: 0.517436683177948\n",
      "cnt: 0 - valLoss: 0.5170708298683167 - trainLoss: 0.517417311668396\n",
      "cnt: 0 - valLoss: 0.5170488953590393 - trainLoss: 0.517397940158844\n",
      "cnt: 0 - valLoss: 0.5170268416404724 - trainLoss: 0.517378568649292\n",
      "cnt: 0 - valLoss: 0.5170048475265503 - trainLoss: 0.5173592567443848\n",
      "cnt: 0 - valLoss: 0.5169827938079834 - trainLoss: 0.5173398852348328\n",
      "cnt: 0 - valLoss: 0.516960859298706 - trainLoss: 0.5173205733299255\n",
      "cnt: 0 - valLoss: 0.5169388055801392 - trainLoss: 0.5173012018203735\n",
      "cnt: 0 - valLoss: 0.5169168710708618 - trainLoss: 0.5172818303108215\n",
      "cnt: 0 - valLoss: 0.5168948173522949 - trainLoss: 0.5172625184059143\n",
      "cnt: 0 - valLoss: 0.5168728828430176 - trainLoss: 0.5172431468963623\n",
      "cnt: 0 - valLoss: 0.5168508887290955 - trainLoss: 0.5172238349914551\n",
      "cnt: 0 - valLoss: 0.5168289542198181 - trainLoss: 0.5172044634819031\n",
      "cnt: 0 - valLoss: 0.5168069005012512 - trainLoss: 0.5171852111816406\n",
      "cnt: 0 - valLoss: 0.5167850255966187 - trainLoss: 0.5171658396720886\n",
      "cnt: 0 - valLoss: 0.5167630910873413 - trainLoss: 0.5171465277671814\n",
      "cnt: 0 - valLoss: 0.5167410373687744 - trainLoss: 0.5171272158622742\n",
      "cnt: 0 - valLoss: 0.5167191028594971 - trainLoss: 0.5171079039573669\n",
      "cnt: 0 - valLoss: 0.516697108745575 - trainLoss: 0.5170885324478149\n",
      "cnt: 0 - valLoss: 0.5166751742362976 - trainLoss: 0.5170692801475525\n",
      "cnt: 0 - valLoss: 0.5166531801223755 - trainLoss: 0.5170499682426453\n",
      "cnt: 0 - valLoss: 0.5166312456130981 - trainLoss: 0.5170305967330933\n",
      "cnt: 0 - valLoss: 0.516609251499176 - trainLoss: 0.5170113444328308\n",
      "cnt: 0 - valLoss: 0.5165873765945435 - trainLoss: 0.5169920325279236\n",
      "cnt: 0 - valLoss: 0.5165653824806213 - trainLoss: 0.5169727206230164\n",
      "cnt: 0 - valLoss: 0.5165435075759888 - trainLoss: 0.5169534683227539\n",
      "cnt: 0 - valLoss: 0.5165215134620667 - trainLoss: 0.5169341564178467\n",
      "cnt: 0 - valLoss: 0.5164996385574341 - trainLoss: 0.5169149041175842\n",
      "cnt: 0 - valLoss: 0.5164777040481567 - trainLoss: 0.5168956518173218\n",
      "cnt: 0 - valLoss: 0.5164557695388794 - trainLoss: 0.5168763399124146\n",
      "cnt: 0 - valLoss: 0.516433835029602 - trainLoss: 0.5168570876121521\n",
      "cnt: 0 - valLoss: 0.5164119601249695 - trainLoss: 0.5168378353118896\n",
      "cnt: 0 - valLoss: 0.5163900256156921 - trainLoss: 0.5168185830116272\n",
      "cnt: 0 - valLoss: 0.5163681507110596 - trainLoss: 0.5167993307113647\n",
      "cnt: 0 - valLoss: 0.5163462162017822 - trainLoss: 0.5167801380157471\n",
      "cnt: 0 - valLoss: 0.5163243412971497 - trainLoss: 0.5167608261108398\n",
      "cnt: 0 - valLoss: 0.5163024067878723 - trainLoss: 0.5167416334152222\n",
      "cnt: 0 - valLoss: 0.5162805318832397 - trainLoss: 0.5167223215103149\n",
      "cnt: 0 - valLoss: 0.5162585973739624 - trainLoss: 0.5167031288146973\n",
      "cnt: 0 - valLoss: 0.5162367224693298 - trainLoss: 0.5166839361190796\n",
      "cnt: 0 - valLoss: 0.5162148475646973 - trainLoss: 0.5166646242141724\n",
      "cnt: 0 - valLoss: 0.5161930322647095 - trainLoss: 0.5166454315185547\n",
      "cnt: 0 - valLoss: 0.5161710977554321 - trainLoss: 0.516626238822937\n",
      "cnt: 0 - valLoss: 0.5161492228507996 - trainLoss: 0.5166069269180298\n",
      "cnt: 0 - valLoss: 0.516127347946167 - trainLoss: 0.5165877938270569\n",
      "cnt: 0 - valLoss: 0.5161054730415344 - trainLoss: 0.5165685415267944\n",
      "cnt: 0 - valLoss: 0.5160835981369019 - trainLoss: 0.5165493488311768\n",
      "cnt: 0 - valLoss: 0.5160617828369141 - trainLoss: 0.5165301561355591\n",
      "cnt: 0 - valLoss: 0.5160399079322815 - trainLoss: 0.5165109038352966\n",
      "cnt: 0 - valLoss: 0.5160180330276489 - trainLoss: 0.516491711139679\n",
      "cnt: 0 - valLoss: 0.5159961581230164 - trainLoss: 0.5164725184440613\n",
      "cnt: 0 - valLoss: 0.5159743428230286 - trainLoss: 0.5164533257484436\n",
      "cnt: 0 - valLoss: 0.515952467918396 - trainLoss: 0.5164341926574707\n",
      "cnt: 0 - valLoss: 0.5159306526184082 - trainLoss: 0.5164149403572083\n",
      "cnt: 0 - valLoss: 0.5159088373184204 - trainLoss: 0.5163957476615906\n",
      "cnt: 0 - valLoss: 0.5158869624137878 - trainLoss: 0.5163766145706177\n",
      "cnt: 0 - valLoss: 0.5158651471138 - trainLoss: 0.5163573622703552\n",
      "cnt: 0 - valLoss: 0.5158433318138123 - trainLoss: 0.5163382291793823\n",
      "cnt: 0 - valLoss: 0.5158215165138245 - trainLoss: 0.5163190364837646\n",
      "cnt: 0 - valLoss: 0.5157997012138367 - trainLoss: 0.5162999033927917\n",
      "cnt: 0 - valLoss: 0.5157778859138489 - trainLoss: 0.5162807106971741\n",
      "cnt: 0 - valLoss: 0.5157560110092163 - trainLoss: 0.5162615180015564\n",
      "cnt: 0 - valLoss: 0.5157343149185181 - trainLoss: 0.5162423849105835\n",
      "cnt: 0 - valLoss: 0.5157124400138855 - trainLoss: 0.5162231922149658\n",
      "cnt: 0 - valLoss: 0.5156906247138977 - trainLoss: 0.5162039995193481\n",
      "cnt: 0 - valLoss: 0.5156688690185547 - trainLoss: 0.5161848664283752\n",
      "cnt: 0 - valLoss: 0.5156470537185669 - trainLoss: 0.5161657333374023\n",
      "cnt: 0 - valLoss: 0.5156252384185791 - trainLoss: 0.5161466002464294\n",
      "cnt: 0 - valLoss: 0.5156034827232361 - trainLoss: 0.5161274671554565\n",
      "cnt: 0 - valLoss: 0.5155817270278931 - trainLoss: 0.5161082744598389\n",
      "cnt: 0 - valLoss: 0.5155599117279053 - trainLoss: 0.516089141368866\n",
      "cnt: 0 - valLoss: 0.5155380964279175 - trainLoss: 0.5160700678825378\n",
      "cnt: 0 - valLoss: 0.5155164003372192 - trainLoss: 0.5160509347915649\n",
      "cnt: 0 - valLoss: 0.5154945850372314 - trainLoss: 0.516031801700592\n",
      "cnt: 0 - valLoss: 0.5154728293418884 - trainLoss: 0.5160126686096191\n",
      "cnt: 0 - valLoss: 0.5154510736465454 - trainLoss: 0.5159935355186462\n",
      "cnt: 0 - valLoss: 0.5154293179512024 - trainLoss: 0.5159744620323181\n",
      "cnt: 0 - valLoss: 0.5154075622558594 - trainLoss: 0.5159552693367004\n",
      "cnt: 0 - valLoss: 0.5153858065605164 - trainLoss: 0.5159361958503723\n",
      "cnt: 0 - valLoss: 0.5153641104698181 - trainLoss: 0.5159170627593994\n",
      "cnt: 0 - valLoss: 0.5153423547744751 - trainLoss: 0.5158979892730713\n",
      "cnt: 0 - valLoss: 0.5153206586837769 - trainLoss: 0.5158788561820984\n",
      "cnt: 0 - valLoss: 0.5152990221977234 - trainLoss: 0.5158597826957703\n",
      "cnt: 0 - valLoss: 0.5152773261070251 - trainLoss: 0.5158407092094421\n",
      "cnt: 0 - valLoss: 0.5152555108070374 - trainLoss: 0.515821635723114\n",
      "cnt: 0 - valLoss: 0.5152338743209839 - trainLoss: 0.5158025622367859\n",
      "cnt: 0 - valLoss: 0.5152121782302856 - trainLoss: 0.515783429145813\n",
      "cnt: 0 - valLoss: 0.5151904821395874 - trainLoss: 0.5157643556594849\n",
      "cnt: 0 - valLoss: 0.5151688456535339 - trainLoss: 0.5157453417778015\n",
      "cnt: 0 - valLoss: 0.5151471495628357 - trainLoss: 0.5157262682914734\n",
      "cnt: 0 - valLoss: 0.5151255130767822 - trainLoss: 0.5157071948051453\n",
      "cnt: 0 - valLoss: 0.515103816986084 - trainLoss: 0.5156881809234619\n",
      "cnt: 0 - valLoss: 0.5150821208953857 - trainLoss: 0.5156691074371338\n",
      "cnt: 0 - valLoss: 0.5150604844093323 - trainLoss: 0.5156500339508057\n",
      "cnt: 0 - valLoss: 0.5150388479232788 - trainLoss: 0.5156310200691223\n",
      "cnt: 0 - valLoss: 0.5150171518325806 - trainLoss: 0.515612006187439\n",
      "cnt: 0 - valLoss: 0.5149955153465271 - trainLoss: 0.5155928730964661\n",
      "cnt: 0 - valLoss: 0.5149738192558289 - trainLoss: 0.5155739188194275\n",
      "cnt: 0 - valLoss: 0.5149523019790649 - trainLoss: 0.5155547857284546\n",
      "cnt: 0 - valLoss: 0.5149306654930115 - trainLoss: 0.5155357718467712\n",
      "cnt: 0 - valLoss: 0.514909029006958 - trainLoss: 0.5155168175697327\n",
      "cnt: 0 - valLoss: 0.5148874521255493 - trainLoss: 0.5154976844787598\n",
      "cnt: 0 - valLoss: 0.5148658752441406 - trainLoss: 0.5154787302017212\n",
      "cnt: 0 - valLoss: 0.5148442387580872 - trainLoss: 0.5154597163200378\n",
      "cnt: 0 - valLoss: 0.5148226618766785 - trainLoss: 0.5154406428337097\n",
      "cnt: 0 - valLoss: 0.5148010849952698 - trainLoss: 0.5154216885566711\n",
      "cnt: 0 - valLoss: 0.5147795081138611 - trainLoss: 0.5154026746749878\n",
      "cnt: 0 - valLoss: 0.5147578716278076 - trainLoss: 0.5153836607933044\n",
      "cnt: 0 - valLoss: 0.5147362351417542 - trainLoss: 0.5153646469116211\n",
      "cnt: 0 - valLoss: 0.5147145986557007 - trainLoss: 0.5153456926345825\n",
      "cnt: 0 - valLoss: 0.514693021774292 - trainLoss: 0.5153266191482544\n",
      "cnt: 0 - valLoss: 0.5146713852882385 - trainLoss: 0.5153076648712158\n",
      "cnt: 0 - valLoss: 0.5146498084068298 - trainLoss: 0.5152886509895325\n",
      "cnt: 0 - valLoss: 0.5146281719207764 - trainLoss: 0.5152696371078491\n",
      "cnt: 0 - valLoss: 0.5146065950393677 - trainLoss: 0.5152506828308105\n",
      "cnt: 0 - valLoss: 0.514585018157959 - trainLoss: 0.5152316689491272\n",
      "cnt: 0 - valLoss: 0.5145633816719055 - trainLoss: 0.5152127146720886\n",
      "cnt: 0 - valLoss: 0.5145418047904968 - trainLoss: 0.5151937007904053\n",
      "cnt: 0 - valLoss: 0.5145202279090881 - trainLoss: 0.5151747465133667\n",
      "cnt: 0 - valLoss: 0.5144986510276794 - trainLoss: 0.5151557922363281\n",
      "cnt: 0 - valLoss: 0.5144770741462708 - trainLoss: 0.5151367783546448\n",
      "cnt: 0 - valLoss: 0.5144554972648621 - trainLoss: 0.515117883682251\n",
      "cnt: 0 - valLoss: 0.5144339799880981 - trainLoss: 0.5150989294052124\n",
      "cnt: 0 - valLoss: 0.5144123435020447 - trainLoss: 0.5150799751281738\n",
      "cnt: 0 - valLoss: 0.514390766620636 - trainLoss: 0.5150610208511353\n",
      "cnt: 0 - valLoss: 0.5143693089485168 - trainLoss: 0.5150420665740967\n",
      "cnt: 0 - valLoss: 0.5143477320671082 - trainLoss: 0.5150231122970581\n",
      "cnt: 0 - valLoss: 0.5143260955810547 - trainLoss: 0.5150042176246643\n",
      "cnt: 0 - valLoss: 0.5143046379089355 - trainLoss: 0.5149852633476257\n",
      "cnt: 0 - valLoss: 0.5142830610275269 - trainLoss: 0.5149663090705872\n",
      "cnt: 0 - valLoss: 0.5142614841461182 - trainLoss: 0.5149473547935486\n",
      "cnt: 0 - valLoss: 0.514240026473999 - trainLoss: 0.5149284601211548\n",
      "cnt: 0 - valLoss: 0.5142184495925903 - trainLoss: 0.5149095058441162\n",
      "cnt: 0 - valLoss: 0.5141968727111816 - trainLoss: 0.5148906111717224\n",
      "cnt: 0 - valLoss: 0.5141754150390625 - trainLoss: 0.5148717164993286\n",
      "cnt: 0 - valLoss: 0.5141538381576538 - trainLoss: 0.5148527026176453\n",
      "cnt: 0 - valLoss: 0.5141323208808899 - trainLoss: 0.5148338675498962\n",
      "cnt: 0 - valLoss: 0.514110803604126 - trainLoss: 0.5148149132728577\n",
      "cnt: 0 - valLoss: 0.5140892863273621 - trainLoss: 0.5147960186004639\n",
      "cnt: 0 - valLoss: 0.5140677690505981 - trainLoss: 0.5147771239280701\n",
      "cnt: 0 - valLoss: 0.514046311378479 - trainLoss: 0.5147582292556763\n",
      "cnt: 0 - valLoss: 0.5140247941017151 - trainLoss: 0.5147393345832825\n",
      "cnt: 0 - valLoss: 0.5140032172203064 - trainLoss: 0.5147204399108887\n",
      "cnt: 0 - valLoss: 0.5139817595481873 - trainLoss: 0.5147014856338501\n",
      "cnt: 0 - valLoss: 0.5139602422714233 - trainLoss: 0.5146826505661011\n",
      "cnt: 0 - valLoss: 0.5139387845993042 - trainLoss: 0.5146637558937073\n",
      "cnt: 0 - valLoss: 0.5139172673225403 - trainLoss: 0.5146448612213135\n",
      "cnt: 0 - valLoss: 0.5138958692550659 - trainLoss: 0.5146260261535645\n",
      "cnt: 0 - valLoss: 0.5138742923736572 - trainLoss: 0.5146071314811707\n",
      "cnt: 0 - valLoss: 0.5138528347015381 - trainLoss: 0.5145882368087769\n",
      "cnt: 0 - valLoss: 0.513831377029419 - trainLoss: 0.5145694017410278\n",
      "cnt: 0 - valLoss: 0.5138099193572998 - trainLoss: 0.514550507068634\n",
      "cnt: 0 - valLoss: 0.5137884020805359 - trainLoss: 0.514531672000885\n",
      "cnt: 0 - valLoss: 0.5137669444084167 - trainLoss: 0.514512836933136\n",
      "cnt: 0 - valLoss: 0.5137454867362976 - trainLoss: 0.5144939422607422\n",
      "cnt: 0 - valLoss: 0.5137239694595337 - trainLoss: 0.5144751071929932\n",
      "cnt: 0 - valLoss: 0.5137025713920593 - trainLoss: 0.5144562721252441\n",
      "cnt: 0 - valLoss: 0.5136811137199402 - trainLoss: 0.5144374370574951\n",
      "cnt: 0 - valLoss: 0.5136597156524658 - trainLoss: 0.5144186019897461\n",
      "cnt: 0 - valLoss: 0.5136382579803467 - trainLoss: 0.5143997669219971\n",
      "cnt: 0 - valLoss: 0.5136168003082275 - trainLoss: 0.514380931854248\n",
      "cnt: 0 - valLoss: 0.5135954022407532 - trainLoss: 0.5143620371818542\n",
      "cnt: 0 - valLoss: 0.513573944568634 - trainLoss: 0.51434326171875\n",
      "cnt: 0 - valLoss: 0.5135524868965149 - trainLoss: 0.5143243670463562\n",
      "cnt: 0 - valLoss: 0.5135310888290405 - trainLoss: 0.514305591583252\n",
      "cnt: 0 - valLoss: 0.5135096907615662 - trainLoss: 0.5142867565155029\n",
      "cnt: 0 - valLoss: 0.513488233089447 - trainLoss: 0.5142679810523987\n",
      "cnt: 0 - valLoss: 0.5134668350219727 - trainLoss: 0.5142491459846497\n",
      "cnt: 0 - valLoss: 0.5134454369544983 - trainLoss: 0.5142303705215454\n",
      "cnt: 0 - valLoss: 0.5134239792823792 - trainLoss: 0.5142115354537964\n",
      "cnt: 0 - valLoss: 0.5134025812149048 - trainLoss: 0.5141927003860474\n",
      "cnt: 0 - valLoss: 0.5133812427520752 - trainLoss: 0.5141739249229431\n",
      "cnt: 0 - valLoss: 0.513359785079956 - trainLoss: 0.5141551494598389\n",
      "cnt: 0 - valLoss: 0.5133383870124817 - trainLoss: 0.5141363739967346\n",
      "cnt: 0 - valLoss: 0.5133170485496521 - trainLoss: 0.5141175389289856\n",
      "cnt: 0 - valLoss: 0.5132956504821777 - trainLoss: 0.5140987634658813\n",
      "cnt: 0 - valLoss: 0.5132743120193481 - trainLoss: 0.5140799880027771\n",
      "cnt: 0 - valLoss: 0.513252854347229 - trainLoss: 0.5140612125396729\n",
      "cnt: 0 - valLoss: 0.5132314562797546 - trainLoss: 0.5140424370765686\n",
      "cnt: 0 - valLoss: 0.5132101774215698 - trainLoss: 0.5140236616134644\n",
      "cnt: 0 - valLoss: 0.5131887197494507 - trainLoss: 0.5140048861503601\n",
      "cnt: 0 - valLoss: 0.5131673812866211 - trainLoss: 0.5139861106872559\n",
      "cnt: 0 - valLoss: 0.5131459832191467 - trainLoss: 0.5139673948287964\n",
      "cnt: 0 - valLoss: 0.5131247043609619 - trainLoss: 0.5139486193656921\n",
      "cnt: 0 - valLoss: 0.5131033062934875 - trainLoss: 0.5139298439025879\n",
      "cnt: 0 - valLoss: 0.5130819082260132 - trainLoss: 0.5139110684394836\n",
      "cnt: 0 - valLoss: 0.5130605697631836 - trainLoss: 0.513892412185669\n",
      "cnt: 0 - valLoss: 0.5130391716957092 - trainLoss: 0.5138735771179199\n",
      "cnt: 0 - valLoss: 0.5130178928375244 - trainLoss: 0.5138549208641052\n",
      "cnt: 0 - valLoss: 0.5129965543746948 - trainLoss: 0.5138360857963562\n",
      "cnt: 0 - valLoss: 0.5129752159118652 - trainLoss: 0.5138174295425415\n",
      "cnt: 0 - valLoss: 0.5129538774490356 - trainLoss: 0.5137985944747925\n",
      "cnt: 0 - valLoss: 0.512932538986206 - trainLoss: 0.513779878616333\n",
      "cnt: 0 - valLoss: 0.5129112005233765 - trainLoss: 0.5137611627578735\n",
      "cnt: 0 - valLoss: 0.5128899216651917 - trainLoss: 0.5137424468994141\n",
      "cnt: 0 - valLoss: 0.5128685832023621 - trainLoss: 0.5137237310409546\n",
      "cnt: 0 - valLoss: 0.5128472447395325 - trainLoss: 0.5137050151824951\n",
      "cnt: 0 - valLoss: 0.5128259658813477 - trainLoss: 0.5136863589286804\n",
      "cnt: 0 - valLoss: 0.5128046274185181 - trainLoss: 0.5136675834655762\n",
      "cnt: 0 - valLoss: 0.5127833485603333 - trainLoss: 0.5136489272117615\n",
      "cnt: 0 - valLoss: 0.5127620697021484 - trainLoss: 0.513630211353302\n",
      "cnt: 0 - valLoss: 0.5127407312393188 - trainLoss: 0.5136114954948425\n",
      "cnt: 0 - valLoss: 0.512719452381134 - trainLoss: 0.5135928392410278\n",
      "cnt: 0 - valLoss: 0.5126981735229492 - trainLoss: 0.5135741233825684\n",
      "cnt: 0 - valLoss: 0.5126768946647644 - trainLoss: 0.5135554075241089\n",
      "cnt: 0 - valLoss: 0.5126555562019348 - trainLoss: 0.5135366916656494\n",
      "cnt: 0 - valLoss: 0.5126343369483948 - trainLoss: 0.5135180354118347\n",
      "cnt: 0 - valLoss: 0.51261305809021 - trainLoss: 0.5134993195533752\n",
      "cnt: 0 - valLoss: 0.5125917792320251 - trainLoss: 0.5134806632995605\n",
      "cnt: 0 - valLoss: 0.5125705003738403 - trainLoss: 0.5134620070457458\n",
      "cnt: 0 - valLoss: 0.5125492215156555 - trainLoss: 0.5134432911872864\n",
      "cnt: 0 - valLoss: 0.5125280618667603 - trainLoss: 0.5134246945381165\n",
      "cnt: 0 - valLoss: 0.5125067234039307 - trainLoss: 0.513405978679657\n",
      "cnt: 0 - valLoss: 0.5124854445457458 - trainLoss: 0.5133873820304871\n",
      "cnt: 0 - valLoss: 0.5124642252922058 - trainLoss: 0.5133686661720276\n",
      "cnt: 0 - valLoss: 0.512442946434021 - trainLoss: 0.5133500099182129\n",
      "cnt: 0 - valLoss: 0.5124216675758362 - trainLoss: 0.513331413269043\n",
      "cnt: 0 - valLoss: 0.5124005079269409 - trainLoss: 0.5133127570152283\n",
      "cnt: 0 - valLoss: 0.5123792290687561 - trainLoss: 0.5132941007614136\n",
      "cnt: 0 - valLoss: 0.5123580098152161 - trainLoss: 0.5132754445075989\n",
      "cnt: 0 - valLoss: 0.512336790561676 - trainLoss: 0.513256847858429\n",
      "cnt: 0 - valLoss: 0.512315571308136 - trainLoss: 0.5132381916046143\n",
      "cnt: 0 - valLoss: 0.5122942924499512 - trainLoss: 0.5132195353507996\n",
      "cnt: 0 - valLoss: 0.5122731328010559 - trainLoss: 0.5132009983062744\n",
      "cnt: 0 - valLoss: 0.5122519135475159 - trainLoss: 0.5131822824478149\n",
      "cnt: 0 - valLoss: 0.5122306942939758 - trainLoss: 0.513163685798645\n",
      "cnt: 0 - valLoss: 0.512209415435791 - trainLoss: 0.5131450891494751\n",
      "cnt: 0 - valLoss: 0.512188196182251 - trainLoss: 0.5131264925003052\n",
      "cnt: 0 - valLoss: 0.5121669769287109 - trainLoss: 0.5131078362464905\n",
      "cnt: 0 - valLoss: 0.5121458172798157 - trainLoss: 0.5130892395973206\n",
      "cnt: 0 - valLoss: 0.5121245980262756 - trainLoss: 0.5130707025527954\n",
      "cnt: 0 - valLoss: 0.5121033787727356 - trainLoss: 0.5130521059036255\n",
      "cnt: 0 - valLoss: 0.5120822191238403 - trainLoss: 0.5130335092544556\n",
      "cnt: 0 - valLoss: 0.5120609998703003 - trainLoss: 0.5130149722099304\n",
      "cnt: 0 - valLoss: 0.5120397806167603 - trainLoss: 0.5129963159561157\n",
      "cnt: 0 - valLoss: 0.512018620967865 - trainLoss: 0.5129777789115906\n",
      "cnt: 0 - valLoss: 0.5119974613189697 - trainLoss: 0.5129591822624207\n",
      "cnt: 0 - valLoss: 0.5119762420654297 - trainLoss: 0.5129405856132507\n",
      "cnt: 0 - valLoss: 0.5119550824165344 - trainLoss: 0.5129220485687256\n",
      "cnt: 0 - valLoss: 0.5119339227676392 - trainLoss: 0.5129035115242004\n",
      "cnt: 0 - valLoss: 0.5119127631187439 - trainLoss: 0.5128849148750305\n",
      "cnt: 0 - valLoss: 0.5118915438652039 - trainLoss: 0.5128663182258606\n",
      "cnt: 0 - valLoss: 0.5118703842163086 - trainLoss: 0.5128477811813354\n",
      "cnt: 0 - valLoss: 0.5118492841720581 - trainLoss: 0.5128292441368103\n",
      "cnt: 0 - valLoss: 0.5118280649185181 - trainLoss: 0.5128107070922852\n",
      "cnt: 0 - valLoss: 0.5118069648742676 - trainLoss: 0.5127921104431152\n",
      "cnt: 0 - valLoss: 0.5117858052253723 - trainLoss: 0.5127736330032349\n",
      "cnt: 0 - valLoss: 0.5117645859718323 - trainLoss: 0.5127551555633545\n",
      "cnt: 0 - valLoss: 0.511743426322937 - trainLoss: 0.5127365589141846\n",
      "cnt: 0 - valLoss: 0.5117222666740417 - trainLoss: 0.5127180814743042\n",
      "cnt: 0 - valLoss: 0.5117011070251465 - trainLoss: 0.512699544429779\n",
      "cnt: 0 - valLoss: 0.5116799473762512 - trainLoss: 0.5126810073852539\n",
      "cnt: 0 - valLoss: 0.5116588473320007 - trainLoss: 0.5126625299453735\n",
      "cnt: 0 - valLoss: 0.5116377472877502 - trainLoss: 0.5126439929008484\n",
      "cnt: 0 - valLoss: 0.511616587638855 - trainLoss: 0.512625515460968\n",
      "cnt: 0 - valLoss: 0.5115954875946045 - trainLoss: 0.5126069784164429\n",
      "cnt: 0 - valLoss: 0.5115743279457092 - trainLoss: 0.5125885009765625\n",
      "cnt: 0 - valLoss: 0.511553168296814 - trainLoss: 0.5125699639320374\n",
      "cnt: 0 - valLoss: 0.5115321278572083 - trainLoss: 0.512551486492157\n",
      "cnt: 0 - valLoss: 0.511510968208313 - trainLoss: 0.5125330090522766\n",
      "cnt: 0 - valLoss: 0.5114898681640625 - trainLoss: 0.5125145316123962\n",
      "cnt: 0 - valLoss: 0.5114687085151672 - trainLoss: 0.5124960541725159\n",
      "cnt: 0 - valLoss: 0.5114476680755615 - trainLoss: 0.5124775171279907\n",
      "cnt: 0 - valLoss: 0.511426568031311 - trainLoss: 0.5124589800834656\n",
      "cnt: 0 - valLoss: 0.5114054083824158 - trainLoss: 0.5124405026435852\n",
      "cnt: 0 - valLoss: 0.5113843679428101 - trainLoss: 0.5124219655990601\n",
      "cnt: 0 - valLoss: 0.5113632678985596 - trainLoss: 0.5124034881591797\n",
      "cnt: 0 - valLoss: 0.5113421678543091 - trainLoss: 0.5123850107192993\n",
      "cnt: 0 - valLoss: 0.5113210678100586 - trainLoss: 0.512366533279419\n",
      "cnt: 0 - valLoss: 0.5112999677658081 - trainLoss: 0.5123479962348938\n",
      "cnt: 0 - valLoss: 0.5112789273262024 - trainLoss: 0.5123295783996582\n",
      "cnt: 0 - valLoss: 0.5112578272819519 - trainLoss: 0.5123111009597778\n",
      "cnt: 0 - valLoss: 0.5112367868423462 - trainLoss: 0.5122926235198975\n",
      "cnt: 0 - valLoss: 0.5112156867980957 - trainLoss: 0.5122741460800171\n",
      "cnt: 0 - valLoss: 0.51119464635849 - trainLoss: 0.5122556686401367\n",
      "cnt: 0 - valLoss: 0.5111735463142395 - trainLoss: 0.5122371912002563\n",
      "cnt: 0 - valLoss: 0.5111525654792786 - trainLoss: 0.5122187733650208\n",
      "cnt: 0 - valLoss: 0.5111315250396729 - trainLoss: 0.5122002959251404\n",
      "cnt: 0 - valLoss: 0.5111104846000671 - trainLoss: 0.5121818780899048\n",
      "cnt: 0 - valLoss: 0.5110894441604614 - trainLoss: 0.5121634006500244\n",
      "cnt: 0 - valLoss: 0.5110684037208557 - trainLoss: 0.5121449828147888\n",
      "cnt: 0 - valLoss: 0.51104736328125 - trainLoss: 0.5121265053749084\n",
      "cnt: 0 - valLoss: 0.5110263824462891 - trainLoss: 0.5121080875396729\n",
      "cnt: 0 - valLoss: 0.5110053420066833 - trainLoss: 0.5120896697044373\n",
      "cnt: 0 - valLoss: 0.5109843015670776 - trainLoss: 0.5120711922645569\n",
      "cnt: 0 - valLoss: 0.5109632611274719 - trainLoss: 0.5120527744293213\n",
      "cnt: 0 - valLoss: 0.510942280292511 - trainLoss: 0.5120343565940857\n",
      "cnt: 0 - valLoss: 0.5109212398529053 - trainLoss: 0.5120159387588501\n",
      "cnt: 0 - valLoss: 0.5109001994132996 - trainLoss: 0.5119975209236145\n",
      "cnt: 0 - valLoss: 0.5108792781829834 - trainLoss: 0.5119791626930237\n",
      "cnt: 0 - valLoss: 0.5108582377433777 - trainLoss: 0.5119606852531433\n",
      "cnt: 0 - valLoss: 0.5108372569084167 - trainLoss: 0.5119423270225525\n",
      "cnt: 0 - valLoss: 0.5108162760734558 - trainLoss: 0.5119238495826721\n",
      "cnt: 0 - valLoss: 0.5107952356338501 - trainLoss: 0.5119054913520813\n",
      "cnt: 0 - valLoss: 0.5107743144035339 - trainLoss: 0.5118870735168457\n",
      "cnt: 0 - valLoss: 0.510753333568573 - trainLoss: 0.5118686556816101\n",
      "cnt: 0 - valLoss: 0.5107323527336121 - trainLoss: 0.5118502974510193\n",
      "cnt: 0 - valLoss: 0.5107114315032959 - trainLoss: 0.5118319392204285\n",
      "cnt: 0 - valLoss: 0.5106903910636902 - trainLoss: 0.5118135809898376\n",
      "cnt: 0 - valLoss: 0.5106694102287292 - trainLoss: 0.511795163154602\n",
      "cnt: 0 - valLoss: 0.5106485486030579 - trainLoss: 0.5117768049240112\n",
      "cnt: 0 - valLoss: 0.5106275081634521 - trainLoss: 0.5117583870887756\n",
      "cnt: 0 - valLoss: 0.510606586933136 - trainLoss: 0.5117400884628296\n",
      "cnt: 0 - valLoss: 0.510585606098175 - trainLoss: 0.511721670627594\n",
      "cnt: 0 - valLoss: 0.5105646848678589 - trainLoss: 0.5117033123970032\n",
      "cnt: 0 - valLoss: 0.5105437636375427 - trainLoss: 0.5116849541664124\n",
      "cnt: 0 - valLoss: 0.5105227828025818 - trainLoss: 0.5116665959358215\n",
      "cnt: 0 - valLoss: 0.5105018615722656 - trainLoss: 0.5116482377052307\n",
      "cnt: 0 - valLoss: 0.5104809403419495 - trainLoss: 0.5116298794746399\n",
      "cnt: 0 - valLoss: 0.5104600191116333 - trainLoss: 0.5116115212440491\n",
      "cnt: 0 - valLoss: 0.5104390978813171 - trainLoss: 0.5115931630134583\n",
      "cnt: 0 - valLoss: 0.510418176651001 - trainLoss: 0.5115748643875122\n",
      "cnt: 0 - valLoss: 0.5103972554206848 - trainLoss: 0.5115565061569214\n",
      "cnt: 0 - valLoss: 0.5103763341903687 - trainLoss: 0.5115382075309753\n",
      "cnt: 0 - valLoss: 0.5103554725646973 - trainLoss: 0.5115199089050293\n",
      "cnt: 0 - valLoss: 0.5103345513343811 - trainLoss: 0.5115015506744385\n",
      "cnt: 0 - valLoss: 0.5103136301040649 - trainLoss: 0.5114831924438477\n",
      "cnt: 0 - valLoss: 0.5102927684783936 - trainLoss: 0.5114648938179016\n",
      "cnt: 0 - valLoss: 0.5102719068527222 - trainLoss: 0.5114465355873108\n",
      "cnt: 0 - valLoss: 0.510250985622406 - trainLoss: 0.5114282965660095\n",
      "cnt: 0 - valLoss: 0.5102301239967346 - trainLoss: 0.5114099979400635\n",
      "cnt: 0 - valLoss: 0.5102092623710632 - trainLoss: 0.5113916993141174\n",
      "cnt: 0 - valLoss: 0.5101882815361023 - trainLoss: 0.5113734006881714\n",
      "cnt: 0 - valLoss: 0.5101674199104309 - trainLoss: 0.5113551020622253\n",
      "cnt: 0 - valLoss: 0.5101464986801147 - trainLoss: 0.5113368034362793\n",
      "cnt: 0 - valLoss: 0.5101256370544434 - trainLoss: 0.511318564414978\n",
      "cnt: 0 - valLoss: 0.5101048350334167 - trainLoss: 0.5113002061843872\n",
      "cnt: 0 - valLoss: 0.5100839138031006 - trainLoss: 0.5112820267677307\n",
      "cnt: 0 - valLoss: 0.510063111782074 - trainLoss: 0.5112637281417847\n",
      "cnt: 0 - valLoss: 0.5100421905517578 - trainLoss: 0.5112454295158386\n",
      "cnt: 0 - valLoss: 0.5100213289260864 - trainLoss: 0.5112271904945374\n",
      "cnt: 0 - valLoss: 0.5100005269050598 - trainLoss: 0.5112088918685913\n",
      "cnt: 0 - valLoss: 0.5099796056747437 - trainLoss: 0.51119065284729\n",
      "cnt: 0 - valLoss: 0.5099588632583618 - trainLoss: 0.5111724138259888\n",
      "cnt: 0 - valLoss: 0.5099380016326904 - trainLoss: 0.5111542344093323\n",
      "cnt: 0 - valLoss: 0.5099171996116638 - trainLoss: 0.511135995388031\n",
      "cnt: 0 - valLoss: 0.5098963379859924 - trainLoss: 0.5111178159713745\n",
      "cnt: 0 - valLoss: 0.5098755359649658 - trainLoss: 0.511099636554718\n",
      "cnt: 0 - valLoss: 0.509854793548584 - trainLoss: 0.5110814571380615\n",
      "cnt: 0 - valLoss: 0.5098339319229126 - trainLoss: 0.5110632181167603\n",
      "cnt: 0 - valLoss: 0.509813129901886 - trainLoss: 0.5110450387001038\n",
      "cnt: 0 - valLoss: 0.5097923874855042 - trainLoss: 0.5110268592834473\n",
      "cnt: 0 - valLoss: 0.5097716450691223 - trainLoss: 0.5110086798667908\n",
      "cnt: 0 - valLoss: 0.5097507834434509 - trainLoss: 0.5109905004501343\n",
      "cnt: 0 - valLoss: 0.5097299814224243 - trainLoss: 0.5109723210334778\n",
      "cnt: 0 - valLoss: 0.5097092390060425 - trainLoss: 0.5109542012214661\n",
      "cnt: 0 - valLoss: 0.5096884965896606 - trainLoss: 0.5109360218048096\n",
      "cnt: 0 - valLoss: 0.509667694568634 - trainLoss: 0.5109178423881531\n",
      "cnt: 0 - valLoss: 0.5096468925476074 - trainLoss: 0.5108997225761414\n",
      "cnt: 0 - valLoss: 0.5096261501312256 - trainLoss: 0.5108815431594849\n",
      "cnt: 0 - valLoss: 0.5096054077148438 - trainLoss: 0.5108634233474731\n",
      "cnt: 0 - valLoss: 0.5095846652984619 - trainLoss: 0.5108452439308167\n",
      "cnt: 0 - valLoss: 0.5095639228820801 - trainLoss: 0.5108271241188049\n",
      "cnt: 0 - valLoss: 0.5095431208610535 - trainLoss: 0.5108089447021484\n",
      "cnt: 0 - valLoss: 0.5095223784446716 - trainLoss: 0.5107908248901367\n",
      "cnt: 0 - valLoss: 0.5095016360282898 - trainLoss: 0.5107726454734802\n",
      "cnt: 0 - valLoss: 0.509480893611908 - trainLoss: 0.5107545256614685\n",
      "cnt: 0 - valLoss: 0.5094602108001709 - trainLoss: 0.510736346244812\n",
      "cnt: 0 - valLoss: 0.5094394087791443 - trainLoss: 0.5107182860374451\n",
      "cnt: 0 - valLoss: 0.5094187259674072 - trainLoss: 0.5107001662254333\n",
      "cnt: 0 - valLoss: 0.5093979835510254 - trainLoss: 0.5106819868087769\n",
      "cnt: 0 - valLoss: 0.5093772411346436 - trainLoss: 0.5106639266014099\n",
      "cnt: 0 - valLoss: 0.5093565583229065 - trainLoss: 0.5106458067893982\n",
      "cnt: 0 - valLoss: 0.5093358159065247 - trainLoss: 0.5106276869773865\n",
      "cnt: 0 - valLoss: 0.5093151330947876 - trainLoss: 0.5106095671653748\n",
      "cnt: 0 - valLoss: 0.5092945098876953 - trainLoss: 0.5105915069580078\n",
      "cnt: 0 - valLoss: 0.5092737674713135 - trainLoss: 0.5105733871459961\n",
      "cnt: 0 - valLoss: 0.5092530846595764 - trainLoss: 0.5105553269386292\n",
      "cnt: 0 - valLoss: 0.5092324018478394 - trainLoss: 0.5105372071266174\n",
      "cnt: 0 - valLoss: 0.5092117786407471 - trainLoss: 0.5105190873146057\n",
      "cnt: 0 - valLoss: 0.50919109582901 - trainLoss: 0.5105010271072388\n",
      "cnt: 0 - valLoss: 0.5091704726219177 - trainLoss: 0.5104829668998718\n",
      "cnt: 0 - valLoss: 0.5091497898101807 - trainLoss: 0.5104648470878601\n",
      "cnt: 0 - valLoss: 0.5091291666030884 - trainLoss: 0.5104467868804932\n",
      "cnt: 0 - valLoss: 0.5091085433959961 - trainLoss: 0.5104287266731262\n",
      "cnt: 0 - valLoss: 0.5090879201889038 - trainLoss: 0.5104106664657593\n",
      "cnt: 0 - valLoss: 0.5090672373771667 - trainLoss: 0.5103926658630371\n",
      "cnt: 0 - valLoss: 0.5090466141700745 - trainLoss: 0.5103746056556702\n",
      "cnt: 0 - valLoss: 0.5090259909629822 - trainLoss: 0.5103564858436584\n",
      "cnt: 0 - valLoss: 0.5090053677558899 - trainLoss: 0.5103384852409363\n",
      "cnt: 0 - valLoss: 0.5089847445487976 - trainLoss: 0.5103204250335693\n",
      "cnt: 0 - valLoss: 0.5089641213417053 - trainLoss: 0.5103023648262024\n",
      "cnt: 0 - valLoss: 0.508943498134613 - trainLoss: 0.5102843642234802\n",
      "cnt: 0 - valLoss: 0.5089228749275208 - trainLoss: 0.5102663040161133\n",
      "cnt: 0 - valLoss: 0.5089023113250732 - trainLoss: 0.5102482438087463\n",
      "cnt: 0 - valLoss: 0.508881688117981 - trainLoss: 0.5102302432060242\n",
      "cnt: 0 - valLoss: 0.5088610649108887 - trainLoss: 0.510212242603302\n",
      "cnt: 0 - valLoss: 0.5088405013084412 - trainLoss: 0.5101941823959351\n",
      "cnt: 0 - valLoss: 0.5088198781013489 - trainLoss: 0.5101761817932129\n",
      "cnt: 0 - valLoss: 0.5087993144989014 - trainLoss: 0.5101581811904907\n",
      "cnt: 0 - valLoss: 0.5087786912918091 - trainLoss: 0.5101401209831238\n",
      "cnt: 0 - valLoss: 0.5087581872940063 - trainLoss: 0.5101221203804016\n",
      "cnt: 0 - valLoss: 0.5087375640869141 - trainLoss: 0.5101041197776794\n",
      "cnt: 0 - valLoss: 0.5087170004844666 - trainLoss: 0.5100861191749573\n",
      "cnt: 0 - valLoss: 0.508696436882019 - trainLoss: 0.5100681185722351\n",
      "cnt: 0 - valLoss: 0.5086759328842163 - trainLoss: 0.5100501179695129\n",
      "cnt: 0 - valLoss: 0.508655309677124 - trainLoss: 0.5100321173667908\n",
      "cnt: 0 - valLoss: 0.5086347460746765 - trainLoss: 0.5100141167640686\n",
      "cnt: 0 - valLoss: 0.508614182472229 - trainLoss: 0.5099961757659912\n",
      "cnt: 0 - valLoss: 0.5085936784744263 - trainLoss: 0.509978175163269\n",
      "cnt: 0 - valLoss: 0.5085731148719788 - trainLoss: 0.5099601745605469\n",
      "cnt: 0 - valLoss: 0.5085525512695312 - trainLoss: 0.5099422335624695\n",
      "cnt: 0 - valLoss: 0.5085320472717285 - trainLoss: 0.5099242329597473\n",
      "cnt: 0 - valLoss: 0.508511483669281 - trainLoss: 0.5099062919616699\n",
      "cnt: 0 - valLoss: 0.5084909796714783 - trainLoss: 0.5098883509635925\n",
      "cnt: 0 - valLoss: 0.5084704756736755 - trainLoss: 0.5098704099655151\n",
      "cnt: 0 - valLoss: 0.508449912071228 - trainLoss: 0.509852409362793\n",
      "cnt: 0 - valLoss: 0.5084294676780701 - trainLoss: 0.5098344683647156\n",
      "cnt: 0 - valLoss: 0.5084089040756226 - trainLoss: 0.5098165273666382\n",
      "cnt: 0 - valLoss: 0.5083884000778198 - trainLoss: 0.5097985863685608\n",
      "cnt: 0 - valLoss: 0.5083678960800171 - trainLoss: 0.5097805857658386\n",
      "cnt: 0 - valLoss: 0.5083473920822144 - trainLoss: 0.5097626447677612\n",
      "cnt: 0 - valLoss: 0.5083269476890564 - trainLoss: 0.5097447633743286\n",
      "cnt: 0 - valLoss: 0.5083063840866089 - trainLoss: 0.5097268223762512\n",
      "cnt: 0 - valLoss: 0.5082859396934509 - trainLoss: 0.5097088813781738\n",
      "cnt: 0 - valLoss: 0.5082654356956482 - trainLoss: 0.5096909999847412\n",
      "cnt: 0 - valLoss: 0.5082449316978455 - trainLoss: 0.5096730589866638\n",
      "cnt: 0 - valLoss: 0.5082244873046875 - trainLoss: 0.5096551179885864\n",
      "cnt: 0 - valLoss: 0.5082039833068848 - trainLoss: 0.5096372365951538\n",
      "cnt: 0 - valLoss: 0.5081835389137268 - trainLoss: 0.5096192359924316\n",
      "cnt: 0 - valLoss: 0.5081630945205688 - trainLoss: 0.509601354598999\n",
      "cnt: 0 - valLoss: 0.5081425309181213 - trainLoss: 0.5095834136009216\n",
      "cnt: 0 - valLoss: 0.5081220865249634 - trainLoss: 0.5095654726028442\n",
      "cnt: 0 - valLoss: 0.5081016421318054 - trainLoss: 0.5095475912094116\n",
      "cnt: 0 - valLoss: 0.5080811381340027 - trainLoss: 0.5095296502113342\n",
      "cnt: 0 - valLoss: 0.5080606937408447 - trainLoss: 0.5095117092132568\n",
      "cnt: 0 - valLoss: 0.5080402493476868 - trainLoss: 0.5094938278198242\n",
      "cnt: 0 - valLoss: 0.5080198049545288 - trainLoss: 0.5094759464263916\n",
      "cnt: 0 - valLoss: 0.5079993605613708 - trainLoss: 0.5094580054283142\n",
      "cnt: 0 - valLoss: 0.5079789161682129 - trainLoss: 0.5094401836395264\n",
      "cnt: 0 - valLoss: 0.5079584121704102 - trainLoss: 0.509422242641449\n",
      "cnt: 0 - valLoss: 0.507938027381897 - trainLoss: 0.5094043612480164\n",
      "cnt: 0 - valLoss: 0.507917582988739 - trainLoss: 0.5093864798545837\n",
      "cnt: 0 - valLoss: 0.507897138595581 - trainLoss: 0.5093685984611511\n",
      "cnt: 0 - valLoss: 0.5078766942024231 - trainLoss: 0.5093506574630737\n",
      "cnt: 0 - valLoss: 0.5078563094139099 - trainLoss: 0.5093328356742859\n",
      "cnt: 0 - valLoss: 0.5078359246253967 - trainLoss: 0.5093148946762085\n",
      "cnt: 0 - valLoss: 0.5078154802322388 - trainLoss: 0.5092970728874207\n",
      "cnt: 0 - valLoss: 0.5077950358390808 - trainLoss: 0.509279191493988\n",
      "cnt: 0 - valLoss: 0.5077746510505676 - trainLoss: 0.5092613101005554\n",
      "cnt: 0 - valLoss: 0.5077542662620544 - trainLoss: 0.5092434883117676\n",
      "cnt: 0 - valLoss: 0.5077338218688965 - trainLoss: 0.509225606918335\n",
      "cnt: 0 - valLoss: 0.5077134370803833 - trainLoss: 0.5092077255249023\n",
      "cnt: 0 - valLoss: 0.5076931118965149 - trainLoss: 0.5091899633407593\n",
      "cnt: 0 - valLoss: 0.5076726675033569 - trainLoss: 0.5091720223426819\n",
      "cnt: 0 - valLoss: 0.5076522827148438 - trainLoss: 0.5091542601585388\n",
      "cnt: 0 - valLoss: 0.5076318979263306 - trainLoss: 0.5091363787651062\n",
      "cnt: 0 - valLoss: 0.5076115131378174 - trainLoss: 0.5091185569763184\n",
      "cnt: 0 - valLoss: 0.507591187953949 - trainLoss: 0.5091006755828857\n",
      "cnt: 0 - valLoss: 0.507570743560791 - trainLoss: 0.5090829133987427\n",
      "cnt: 0 - valLoss: 0.5075504183769226 - trainLoss: 0.5090650320053101\n",
      "cnt: 0 - valLoss: 0.5075300335884094 - trainLoss: 0.509047269821167\n",
      "cnt: 0 - valLoss: 0.507509708404541 - trainLoss: 0.5090294480323792\n",
      "cnt: 0 - valLoss: 0.5074893832206726 - trainLoss: 0.5090116262435913\n",
      "cnt: 0 - valLoss: 0.5074689984321594 - trainLoss: 0.5089938044548035\n",
      "cnt: 0 - valLoss: 0.507448673248291 - trainLoss: 0.5089760422706604\n",
      "cnt: 0 - valLoss: 0.5074283480644226 - trainLoss: 0.5089582204818726\n",
      "cnt: 0 - valLoss: 0.5074079632759094 - trainLoss: 0.5089403986930847\n",
      "cnt: 0 - valLoss: 0.507387638092041 - trainLoss: 0.5089225769042969\n",
      "cnt: 0 - valLoss: 0.5073673129081726 - trainLoss: 0.5089048147201538\n",
      "cnt: 0 - valLoss: 0.507347047328949 - trainLoss: 0.5088870525360107\n",
      "cnt: 0 - valLoss: 0.5073266625404358 - trainLoss: 0.5088692903518677\n",
      "cnt: 0 - valLoss: 0.5073063373565674 - trainLoss: 0.5088514685630798\n",
      "cnt: 0 - valLoss: 0.5072860717773438 - trainLoss: 0.5088337063789368\n",
      "cnt: 0 - valLoss: 0.5072656869888306 - trainLoss: 0.5088158845901489\n",
      "cnt: 0 - valLoss: 0.5072454214096069 - trainLoss: 0.5087981224060059\n",
      "cnt: 0 - valLoss: 0.5072250962257385 - trainLoss: 0.5087803602218628\n",
      "cnt: 0 - valLoss: 0.5072048306465149 - trainLoss: 0.5087625980377197\n",
      "cnt: 0 - valLoss: 0.5071845054626465 - trainLoss: 0.5087448954582214\n",
      "cnt: 0 - valLoss: 0.5071642398834229 - trainLoss: 0.5087271332740784\n",
      "cnt: 0 - valLoss: 0.5071439743041992 - trainLoss: 0.5087093710899353\n",
      "cnt: 0 - valLoss: 0.5071236491203308 - trainLoss: 0.5086916089057922\n",
      "cnt: 0 - valLoss: 0.5071033835411072 - trainLoss: 0.5086738467216492\n",
      "cnt: 0 - valLoss: 0.5070831179618835 - trainLoss: 0.5086561441421509\n",
      "cnt: 0 - valLoss: 0.5070628523826599 - trainLoss: 0.5086383819580078\n",
      "cnt: 0 - valLoss: 0.507042646408081 - trainLoss: 0.5086206197738647\n",
      "cnt: 0 - valLoss: 0.5070223212242126 - trainLoss: 0.5086029171943665\n",
      "cnt: 0 - valLoss: 0.507002055644989 - trainLoss: 0.5085852146148682\n",
      "cnt: 0 - valLoss: 0.5069818496704102 - trainLoss: 0.5085674524307251\n",
      "cnt: 0 - valLoss: 0.5069615840911865 - trainLoss: 0.508549690246582\n",
      "cnt: 0 - valLoss: 0.5069413781166077 - trainLoss: 0.5085319876670837\n",
      "cnt: 0 - valLoss: 0.5069212317466736 - trainLoss: 0.5085143446922302\n",
      "cnt: 0 - valLoss: 0.5069010257720947 - trainLoss: 0.5084965825080872\n",
      "cnt: 0 - valLoss: 0.5068807601928711 - trainLoss: 0.5084789395332336\n",
      "cnt: 0 - valLoss: 0.5068605542182922 - trainLoss: 0.5084611773490906\n",
      "cnt: 0 - valLoss: 0.5068404078483582 - trainLoss: 0.5084434747695923\n",
      "cnt: 0 - valLoss: 0.5068202614784241 - trainLoss: 0.5084258317947388\n",
      "cnt: 0 - valLoss: 0.50680011510849 - trainLoss: 0.5084080696105957\n",
      "cnt: 0 - valLoss: 0.5067799687385559 - trainLoss: 0.5083904266357422\n",
      "cnt: 0 - valLoss: 0.5067598819732666 - trainLoss: 0.5083727836608887\n",
      "cnt: 0 - valLoss: 0.5067396759986877 - trainLoss: 0.5083550810813904\n",
      "cnt: 0 - valLoss: 0.5067195892333984 - trainLoss: 0.5083373188972473\n",
      "cnt: 0 - valLoss: 0.5066994428634644 - trainLoss: 0.5083196759223938\n",
      "cnt: 0 - valLoss: 0.5066792964935303 - trainLoss: 0.5083020329475403\n",
      "cnt: 0 - valLoss: 0.506659209728241 - trainLoss: 0.508284330368042\n",
      "cnt: 0 - valLoss: 0.5066390633583069 - trainLoss: 0.5082666873931885\n",
      "cnt: 0 - valLoss: 0.5066189765930176 - trainLoss: 0.508249044418335\n",
      "cnt: 0 - valLoss: 0.5065988898277283 - trainLoss: 0.5082313418388367\n",
      "cnt: 0 - valLoss: 0.5065787434577942 - trainLoss: 0.5082136988639832\n",
      "cnt: 0 - valLoss: 0.5065585970878601 - trainLoss: 0.5081961154937744\n",
      "cnt: 0 - valLoss: 0.5065385699272156 - trainLoss: 0.5081784725189209\n",
      "cnt: 0 - valLoss: 0.5065184235572815 - trainLoss: 0.5081607699394226\n",
      "cnt: 0 - valLoss: 0.5064983367919922 - trainLoss: 0.5081431269645691\n",
      "cnt: 0 - valLoss: 0.5064783096313477 - trainLoss: 0.5081254839897156\n",
      "cnt: 0 - valLoss: 0.5064582228660583 - trainLoss: 0.5081079006195068\n",
      "cnt: 0 - valLoss: 0.5064380764961243 - trainLoss: 0.5080902576446533\n",
      "cnt: 0 - valLoss: 0.5064180493354797 - trainLoss: 0.5080726742744446\n",
      "cnt: 0 - valLoss: 0.5063979029655457 - trainLoss: 0.5080550312995911\n",
      "cnt: 0 - valLoss: 0.5063778162002563 - trainLoss: 0.5080373883247375\n",
      "cnt: 0 - valLoss: 0.5063577890396118 - trainLoss: 0.5080198049545288\n",
      "cnt: 0 - valLoss: 0.5063376426696777 - trainLoss: 0.5080022215843201\n",
      "cnt: 0 - valLoss: 0.5063175559043884 - trainLoss: 0.5079845786094666\n",
      "cnt: 0 - valLoss: 0.5062975287437439 - trainLoss: 0.5079669952392578\n",
      "cnt: 0 - valLoss: 0.5062774419784546 - trainLoss: 0.5079494714736938\n",
      "cnt: 0 - valLoss: 0.5062574148178101 - trainLoss: 0.5079318284988403\n",
      "cnt: 0 - valLoss: 0.5062373280525208 - trainLoss: 0.5079143047332764\n",
      "cnt: 0 - valLoss: 0.5062172412872314 - trainLoss: 0.5078966617584229\n",
      "cnt: 0 - valLoss: 0.5061972141265869 - trainLoss: 0.5078790783882141\n",
      "cnt: 0 - valLoss: 0.5061771273612976 - trainLoss: 0.5078615546226501\n",
      "cnt: 0 - valLoss: 0.5061571002006531 - trainLoss: 0.5078440308570862\n",
      "cnt: 0 - valLoss: 0.5061370730400085 - trainLoss: 0.5078264474868774\n",
      "cnt: 0 - valLoss: 0.506117045879364 - trainLoss: 0.5078089237213135\n",
      "cnt: 0 - valLoss: 0.5060970187187195 - trainLoss: 0.5077913403511047\n",
      "cnt: 0 - valLoss: 0.5060769319534302 - trainLoss: 0.507773756980896\n",
      "cnt: 0 - valLoss: 0.5060569047927856 - trainLoss: 0.507756233215332\n",
      "cnt: 0 - valLoss: 0.5060368180274963 - trainLoss: 0.5077386498451233\n",
      "cnt: 0 - valLoss: 0.5060168504714966 - trainLoss: 0.5077211260795593\n",
      "cnt: 0 - valLoss: 0.505996823310852 - trainLoss: 0.5077036023139954\n",
      "cnt: 0 - valLoss: 0.5059767961502075 - trainLoss: 0.5076860785484314\n",
      "cnt: 0 - valLoss: 0.5059568285942078 - trainLoss: 0.5076686143875122\n",
      "cnt: 0 - valLoss: 0.5059368014335632 - trainLoss: 0.5076510310173035\n",
      "cnt: 0 - valLoss: 0.5059168338775635 - trainLoss: 0.5076335072517395\n",
      "cnt: 0 - valLoss: 0.505896806716919 - trainLoss: 0.5076159834861755\n",
      "cnt: 0 - valLoss: 0.5058768391609192 - trainLoss: 0.5075984597206116\n",
      "cnt: 0 - valLoss: 0.5058568120002747 - trainLoss: 0.5075809359550476\n",
      "cnt: 0 - valLoss: 0.5058367848396301 - trainLoss: 0.5075634717941284\n",
      "cnt: 0 - valLoss: 0.5058168172836304 - trainLoss: 0.5075459480285645\n",
      "cnt: 0 - valLoss: 0.5057969093322754 - trainLoss: 0.5075284242630005\n",
      "cnt: 0 - valLoss: 0.5057768821716309 - trainLoss: 0.5075109601020813\n",
      "cnt: 0 - valLoss: 0.5057569146156311 - trainLoss: 0.5074934363365173\n",
      "cnt: 0 - valLoss: 0.5057369470596313 - trainLoss: 0.5074759721755981\n",
      "cnt: 0 - valLoss: 0.5057169795036316 - trainLoss: 0.507458508014679\n",
      "cnt: 0 - valLoss: 0.5056970119476318 - trainLoss: 0.507440984249115\n",
      "cnt: 0 - valLoss: 0.5056770443916321 - trainLoss: 0.5074235200881958\n",
      "cnt: 0 - valLoss: 0.5056571364402771 - trainLoss: 0.5074060559272766\n",
      "cnt: 0 - valLoss: 0.5056371688842773 - trainLoss: 0.5073885917663574\n",
      "cnt: 0 - valLoss: 0.5056172013282776 - trainLoss: 0.5073711276054382\n",
      "cnt: 0 - valLoss: 0.5055972933769226 - trainLoss: 0.507353663444519\n",
      "cnt: 0 - valLoss: 0.5055773258209229 - trainLoss: 0.5073361992835999\n",
      "cnt: 0 - valLoss: 0.5055574774742126 - trainLoss: 0.5073187351226807\n",
      "cnt: 0 - valLoss: 0.5055375099182129 - trainLoss: 0.5073013305664062\n",
      "cnt: 0 - valLoss: 0.5055176019668579 - trainLoss: 0.5072838664054871\n",
      "cnt: 0 - valLoss: 0.5054976344108582 - trainLoss: 0.5072664618492126\n",
      "cnt: 0 - valLoss: 0.5054777264595032 - trainLoss: 0.5072489976882935\n",
      "cnt: 0 - valLoss: 0.5054577589035034 - trainLoss: 0.5072315335273743\n",
      "cnt: 0 - valLoss: 0.5054378509521484 - trainLoss: 0.5072140693664551\n",
      "cnt: 0 - valLoss: 0.5054179430007935 - trainLoss: 0.5071966648101807\n",
      "cnt: 0 - valLoss: 0.5053980350494385 - trainLoss: 0.5071792602539062\n",
      "cnt: 0 - valLoss: 0.5053781270980835 - trainLoss: 0.5071618556976318\n",
      "cnt: 0 - valLoss: 0.5053582191467285 - trainLoss: 0.5071443915367126\n",
      "cnt: 0 - valLoss: 0.5053383708000183 - trainLoss: 0.5071269869804382\n",
      "cnt: 0 - valLoss: 0.5053184032440186 - trainLoss: 0.5071095824241638\n",
      "cnt: 0 - valLoss: 0.5052985548973083 - trainLoss: 0.5070921778678894\n",
      "cnt: 0 - valLoss: 0.5052786469459534 - trainLoss: 0.5070748329162598\n",
      "cnt: 0 - valLoss: 0.5052587389945984 - trainLoss: 0.5070573687553406\n",
      "cnt: 0 - valLoss: 0.5052388906478882 - trainLoss: 0.5070400238037109\n",
      "cnt: 0 - valLoss: 0.505219042301178 - trainLoss: 0.5070226192474365\n",
      "cnt: 0 - valLoss: 0.505199134349823 - trainLoss: 0.5070052742958069\n",
      "cnt: 0 - valLoss: 0.5051793456077576 - trainLoss: 0.5069878697395325\n",
      "cnt: 0 - valLoss: 0.5051594376564026 - trainLoss: 0.5069705247879028\n",
      "cnt: 0 - valLoss: 0.5051395893096924 - trainLoss: 0.5069531798362732\n",
      "cnt: 0 - valLoss: 0.5051197409629822 - trainLoss: 0.5069358348846436\n",
      "cnt: 0 - valLoss: 0.505099892616272 - trainLoss: 0.5069184303283691\n",
      "cnt: 0 - valLoss: 0.5050800442695618 - trainLoss: 0.5069011449813843\n",
      "cnt: 0 - valLoss: 0.5050602555274963 - trainLoss: 0.5068838000297546\n",
      "cnt: 0 - valLoss: 0.5050404071807861 - trainLoss: 0.5068663954734802\n",
      "cnt: 0 - valLoss: 0.5050205588340759 - trainLoss: 0.5068491101264954\n",
      "cnt: 0 - valLoss: 0.5050007700920105 - trainLoss: 0.5068317651748657\n",
      "cnt: 0 - valLoss: 0.5049809813499451 - trainLoss: 0.5068144202232361\n",
      "cnt: 0 - valLoss: 0.5049610733985901 - trainLoss: 0.5067971348762512\n",
      "cnt: 0 - valLoss: 0.5049412846565247 - trainLoss: 0.5067797899246216\n",
      "cnt: 0 - valLoss: 0.5049213767051697 - trainLoss: 0.5067625045776367\n",
      "cnt: 0 - valLoss: 0.5049015283584595 - trainLoss: 0.5067452192306519\n",
      "cnt: 0 - valLoss: 0.5048816800117493 - trainLoss: 0.5067278146743774\n",
      "cnt: 0 - valLoss: 0.5048618912696838 - trainLoss: 0.5067104697227478\n",
      "cnt: 0 - valLoss: 0.5048419833183289 - trainLoss: 0.5066932439804077\n",
      "cnt: 0 - valLoss: 0.5048222541809082 - trainLoss: 0.5066759586334229\n",
      "cnt: 0 - valLoss: 0.504802405834198 - trainLoss: 0.5066586136817932\n",
      "cnt: 0 - valLoss: 0.5047826170921326 - trainLoss: 0.5066413879394531\n",
      "cnt: 0 - valLoss: 0.5047627687454224 - trainLoss: 0.5066240429878235\n",
      "cnt: 0 - valLoss: 0.5047429800033569 - trainLoss: 0.5066067576408386\n",
      "cnt: 0 - valLoss: 0.5047231316566467 - trainLoss: 0.5065894722938538\n",
      "cnt: 0 - valLoss: 0.5047034025192261 - trainLoss: 0.5065721869468689\n",
      "cnt: 0 - valLoss: 0.5046836137771606 - trainLoss: 0.5065549612045288\n",
      "cnt: 0 - valLoss: 0.5046637654304504 - trainLoss: 0.506537675857544\n",
      "cnt: 0 - valLoss: 0.504643976688385 - trainLoss: 0.5065203905105591\n",
      "cnt: 0 - valLoss: 0.5046241879463196 - trainLoss: 0.5065031051635742\n",
      "cnt: 0 - valLoss: 0.5046043992042542 - trainLoss: 0.5064858794212341\n",
      "cnt: 0 - valLoss: 0.504584550857544 - trainLoss: 0.506468653678894\n",
      "cnt: 0 - valLoss: 0.5045648813247681 - trainLoss: 0.506451427936554\n",
      "cnt: 0 - valLoss: 0.5045450329780579 - trainLoss: 0.5064341425895691\n",
      "cnt: 0 - valLoss: 0.504525363445282 - trainLoss: 0.506416916847229\n",
      "cnt: 0 - valLoss: 0.5045055150985718 - trainLoss: 0.5063996315002441\n",
      "cnt: 0 - valLoss: 0.5044858455657959 - trainLoss: 0.506382405757904\n",
      "cnt: 0 - valLoss: 0.5044659972190857 - trainLoss: 0.506365180015564\n",
      "cnt: 0 - valLoss: 0.5044463276863098 - trainLoss: 0.5063479542732239\n",
      "cnt: 0 - valLoss: 0.5044265985488892 - trainLoss: 0.5063307285308838\n",
      "cnt: 0 - valLoss: 0.5044068098068237 - trainLoss: 0.5063135027885437\n",
      "cnt: 0 - valLoss: 0.5043870806694031 - trainLoss: 0.5062962770462036\n",
      "cnt: 0 - valLoss: 0.5043673515319824 - trainLoss: 0.5062791109085083\n",
      "cnt: 0 - valLoss: 0.5043476223945618 - trainLoss: 0.5062618255615234\n",
      "cnt: 0 - valLoss: 0.5043278336524963 - trainLoss: 0.5062446594238281\n",
      "cnt: 0 - valLoss: 0.5043081641197205 - trainLoss: 0.5062274932861328\n",
      "cnt: 0 - valLoss: 0.5042884349822998 - trainLoss: 0.5062102675437927\n",
      "cnt: 0 - valLoss: 0.5042687654495239 - trainLoss: 0.5061930418014526\n",
      "cnt: 0 - valLoss: 0.5042490363121033 - trainLoss: 0.5061758756637573\n",
      "cnt: 0 - valLoss: 0.5042293667793274 - trainLoss: 0.5061586499214172\n",
      "cnt: 0 - valLoss: 0.5042096376419067 - trainLoss: 0.5061414837837219\n",
      "cnt: 0 - valLoss: 0.5041899681091309 - trainLoss: 0.5061243176460266\n",
      "cnt: 0 - valLoss: 0.504170298576355 - trainLoss: 0.5061071515083313\n",
      "cnt: 0 - valLoss: 0.5041505694389343 - trainLoss: 0.506089985370636\n",
      "cnt: 0 - valLoss: 0.5041308999061584 - trainLoss: 0.5060727596282959\n",
      "cnt: 0 - valLoss: 0.5041112303733826 - trainLoss: 0.5060555934906006\n",
      "cnt: 0 - valLoss: 0.5040915608406067 - trainLoss: 0.5060384273529053\n",
      "cnt: 0 - valLoss: 0.5040718913078308 - trainLoss: 0.5060213208198547\n",
      "cnt: 0 - valLoss: 0.5040522217750549 - trainLoss: 0.5060041546821594\n",
      "cnt: 0 - valLoss: 0.5040324926376343 - trainLoss: 0.5059869885444641\n",
      "cnt: 0 - valLoss: 0.5040128827095032 - trainLoss: 0.5059698224067688\n",
      "cnt: 0 - valLoss: 0.5039932727813721 - trainLoss: 0.5059526562690735\n",
      "cnt: 0 - valLoss: 0.5039736032485962 - trainLoss: 0.505935549736023\n",
      "cnt: 0 - valLoss: 0.5039539933204651 - trainLoss: 0.5059184432029724\n",
      "cnt: 0 - valLoss: 0.5039343237876892 - trainLoss: 0.5059012770652771\n",
      "cnt: 0 - valLoss: 0.5039146542549133 - trainLoss: 0.5058841705322266\n",
      "cnt: 0 - valLoss: 0.5038950443267822 - trainLoss: 0.505867063999176\n",
      "cnt: 0 - valLoss: 0.5038754343986511 - trainLoss: 0.5058499574661255\n",
      "cnt: 0 - valLoss: 0.5038557648658752 - trainLoss: 0.5058327913284302\n",
      "cnt: 0 - valLoss: 0.5038362145423889 - trainLoss: 0.5058156847953796\n",
      "cnt: 0 - valLoss: 0.5038166046142578 - trainLoss: 0.5057985782623291\n",
      "cnt: 0 - valLoss: 0.5037969350814819 - trainLoss: 0.5057814717292786\n",
      "cnt: 0 - valLoss: 0.5037773251533508 - trainLoss: 0.505764365196228\n",
      "cnt: 0 - valLoss: 0.5037577152252197 - trainLoss: 0.5057472586631775\n",
      "cnt: 0 - valLoss: 0.5037381649017334 - trainLoss: 0.5057302117347717\n",
      "cnt: 0 - valLoss: 0.5037184953689575 - trainLoss: 0.5057131052017212\n",
      "cnt: 0 - valLoss: 0.5036989450454712 - trainLoss: 0.5056959986686707\n",
      "cnt: 0 - valLoss: 0.5036793947219849 - trainLoss: 0.5056789517402649\n",
      "cnt: 0 - valLoss: 0.5036597847938538 - trainLoss: 0.5056617856025696\n",
      "cnt: 0 - valLoss: 0.5036402344703674 - trainLoss: 0.5056447386741638\n",
      "cnt: 0 - valLoss: 0.5036206245422363 - trainLoss: 0.5056276917457581\n",
      "cnt: 0 - valLoss: 0.5036011338233948 - trainLoss: 0.5056105852127075\n",
      "cnt: 0 - valLoss: 0.5035815238952637 - trainLoss: 0.5055935978889465\n",
      "cnt: 0 - valLoss: 0.5035619139671326 - trainLoss: 0.5055764317512512\n",
      "cnt: 0 - valLoss: 0.503542423248291 - trainLoss: 0.5055594444274902\n",
      "cnt: 0 - valLoss: 0.5035228729248047 - trainLoss: 0.5055423378944397\n",
      "cnt: 0 - valLoss: 0.5035033226013184 - trainLoss: 0.5055252909660339\n",
      "cnt: 0 - valLoss: 0.5034837126731873 - trainLoss: 0.5055082440376282\n",
      "cnt: 0 - valLoss: 0.5034642219543457 - trainLoss: 0.5054912567138672\n",
      "cnt: 0 - valLoss: 0.5034447312355042 - trainLoss: 0.5054741501808167\n",
      "cnt: 0 - valLoss: 0.5034251809120178 - trainLoss: 0.5054571628570557\n",
      "cnt: 0 - valLoss: 0.5034056305885315 - trainLoss: 0.5054401159286499\n",
      "cnt: 0 - valLoss: 0.5033861398696899 - trainLoss: 0.5054230690002441\n",
      "cnt: 0 - valLoss: 0.5033665895462036 - trainLoss: 0.5054060816764832\n",
      "cnt: 0 - valLoss: 0.5033470988273621 - trainLoss: 0.5053890943527222\n",
      "cnt: 0 - valLoss: 0.5033276081085205 - trainLoss: 0.5053720474243164\n",
      "cnt: 0 - valLoss: 0.5033080577850342 - trainLoss: 0.5053550004959106\n",
      "cnt: 0 - valLoss: 0.5032885670661926 - trainLoss: 0.5053380131721497\n",
      "cnt: 0 - valLoss: 0.5032690763473511 - trainLoss: 0.5053209662437439\n",
      "cnt: 0 - valLoss: 0.5032495856285095 - trainLoss: 0.5053039789199829\n",
      "cnt: 0 - valLoss: 0.503230094909668 - trainLoss: 0.5052869915962219\n",
      "cnt: 0 - valLoss: 0.5032106041908264 - trainLoss: 0.5052700042724609\n",
      "cnt: 0 - valLoss: 0.5031911730766296 - trainLoss: 0.5052529573440552\n",
      "cnt: 0 - valLoss: 0.5031716227531433 - trainLoss: 0.505236029624939\n",
      "cnt: 0 - valLoss: 0.5031521916389465 - trainLoss: 0.505219042301178\n",
      "cnt: 0 - valLoss: 0.5031327605247498 - trainLoss: 0.505202054977417\n",
      "cnt: 0 - valLoss: 0.5031132698059082 - trainLoss: 0.505185067653656\n",
      "cnt: 0 - valLoss: 0.5030937790870667 - trainLoss: 0.5051681399345398\n",
      "cnt: 0 - valLoss: 0.5030742883682251 - trainLoss: 0.505151093006134\n",
      "cnt: 0 - valLoss: 0.5030549168586731 - trainLoss: 0.5051341652870178\n",
      "cnt: 0 - valLoss: 0.5030354261398315 - trainLoss: 0.5051171779632568\n",
      "cnt: 0 - valLoss: 0.5030159950256348 - trainLoss: 0.5051002502441406\n",
      "cnt: 0 - valLoss: 0.502996563911438 - trainLoss: 0.5050833225250244\n",
      "cnt: 0 - valLoss: 0.5029770731925964 - trainLoss: 0.5050663352012634\n",
      "cnt: 0 - valLoss: 0.5029577016830444 - trainLoss: 0.505049467086792\n",
      "cnt: 0 - valLoss: 0.5029382705688477 - trainLoss: 0.505032479763031\n",
      "cnt: 0 - valLoss: 0.5029188394546509 - trainLoss: 0.50501549243927\n",
      "cnt: 0 - valLoss: 0.5028994083404541 - trainLoss: 0.5049985647201538\n",
      "cnt: 0 - valLoss: 0.5028800368309021 - trainLoss: 0.5049816370010376\n",
      "cnt: 0 - valLoss: 0.5028606057167053 - trainLoss: 0.5049647092819214\n",
      "cnt: 0 - valLoss: 0.5028412342071533 - trainLoss: 0.5049477815628052\n",
      "cnt: 0 - valLoss: 0.5028218030929565 - trainLoss: 0.504930853843689\n",
      "cnt: 0 - valLoss: 0.5028024315834045 - trainLoss: 0.5049139857292175\n",
      "cnt: 0 - valLoss: 0.5027830004692078 - trainLoss: 0.5048971176147461\n",
      "cnt: 0 - valLoss: 0.5027636289596558 - trainLoss: 0.5048801302909851\n",
      "cnt: 0 - valLoss: 0.5027442574501038 - trainLoss: 0.5048632025718689\n",
      "cnt: 0 - valLoss: 0.502724826335907 - trainLoss: 0.5048463344573975\n",
      "cnt: 0 - valLoss: 0.502705454826355 - trainLoss: 0.504829466342926\n",
      "cnt: 0 - valLoss: 0.502686083316803 - trainLoss: 0.5048125386238098\n",
      "cnt: 0 - valLoss: 0.5026667714118958 - trainLoss: 0.5047956705093384\n",
      "cnt: 0 - valLoss: 0.5026473999023438 - trainLoss: 0.5047787427902222\n",
      "cnt: 0 - valLoss: 0.5026280283927917 - trainLoss: 0.5047618746757507\n",
      "cnt: 0 - valLoss: 0.5026086568832397 - trainLoss: 0.5047450065612793\n",
      "cnt: 0 - valLoss: 0.5025893449783325 - trainLoss: 0.5047281384468079\n",
      "cnt: 0 - valLoss: 0.5025699734687805 - trainLoss: 0.5047112703323364\n",
      "cnt: 0 - valLoss: 0.5025506019592285 - trainLoss: 0.504694402217865\n",
      "cnt: 0 - valLoss: 0.5025313496589661 - trainLoss: 0.5046774744987488\n",
      "cnt: 0 - valLoss: 0.5025119781494141 - trainLoss: 0.5046606659889221\n",
      "cnt: 0 - valLoss: 0.5024926662445068 - trainLoss: 0.5046437978744507\n",
      "cnt: 0 - valLoss: 0.5024732947349548 - trainLoss: 0.5046269297599792\n",
      "cnt: 0 - valLoss: 0.5024539828300476 - trainLoss: 0.5046100616455078\n",
      "cnt: 0 - valLoss: 0.5024346709251404 - trainLoss: 0.5045931935310364\n",
      "cnt: 0 - valLoss: 0.5024153590202332 - trainLoss: 0.5045764446258545\n",
      "cnt: 0 - valLoss: 0.5023960471153259 - trainLoss: 0.5045595765113831\n",
      "cnt: 0 - valLoss: 0.5023767948150635 - trainLoss: 0.5045427680015564\n",
      "cnt: 0 - valLoss: 0.5023574829101562 - trainLoss: 0.5045259594917297\n",
      "cnt: 0 - valLoss: 0.502338171005249 - trainLoss: 0.5045090913772583\n",
      "cnt: 0 - valLoss: 0.5023188591003418 - trainLoss: 0.5044922232627869\n",
      "cnt: 0 - valLoss: 0.5022995471954346 - trainLoss: 0.5044754147529602\n",
      "cnt: 0 - valLoss: 0.5022803544998169 - trainLoss: 0.5044586658477783\n",
      "cnt: 0 - valLoss: 0.5022610425949097 - trainLoss: 0.5044417977333069\n",
      "cnt: 0 - valLoss: 0.5022417306900024 - trainLoss: 0.5044249892234802\n",
      "cnt: 0 - valLoss: 0.50222247838974 - trainLoss: 0.5044082403182983\n",
      "cnt: 0 - valLoss: 0.5022032260894775 - trainLoss: 0.5043914318084717\n",
      "cnt: 0 - valLoss: 0.5021839737892151 - trainLoss: 0.504374623298645\n",
      "cnt: 0 - valLoss: 0.5021646618843079 - trainLoss: 0.5043578743934631\n",
      "cnt: 0 - valLoss: 0.5021454691886902 - trainLoss: 0.5043410062789917\n",
      "cnt: 0 - valLoss: 0.5021262168884277 - trainLoss: 0.5043242573738098\n",
      "cnt: 0 - valLoss: 0.5021070241928101 - trainLoss: 0.5043074488639832\n",
      "cnt: 0 - valLoss: 0.5020877122879028 - trainLoss: 0.5042906999588013\n",
      "cnt: 0 - valLoss: 0.5020685195922852 - trainLoss: 0.5042738914489746\n",
      "cnt: 0 - valLoss: 0.5020492672920227 - trainLoss: 0.5042571425437927\n",
      "cnt: 0 - valLoss: 0.5020300149917603 - trainLoss: 0.5042403936386108\n",
      "cnt: 0 - valLoss: 0.5020108222961426 - trainLoss: 0.5042235851287842\n",
      "cnt: 0 - valLoss: 0.5019916296005249 - trainLoss: 0.5042068958282471\n",
      "cnt: 0 - valLoss: 0.5019723176956177 - trainLoss: 0.5041900873184204\n",
      "cnt: 0 - valLoss: 0.5019531846046448 - trainLoss: 0.5041733980178833\n",
      "cnt: 0 - valLoss: 0.5019339323043823 - trainLoss: 0.5041565299034119\n",
      "cnt: 0 - valLoss: 0.5019147396087646 - trainLoss: 0.5041398406028748\n",
      "cnt: 0 - valLoss: 0.501895546913147 - trainLoss: 0.5041231513023376\n",
      "cnt: 0 - valLoss: 0.5018764138221741 - trainLoss: 0.5041064023971558\n",
      "cnt: 0 - valLoss: 0.5018571615219116 - trainLoss: 0.5040895938873291\n",
      "cnt: 0 - valLoss: 0.501837968826294 - trainLoss: 0.504072904586792\n",
      "cnt: 0 - valLoss: 0.501818835735321 - trainLoss: 0.5040562152862549\n",
      "cnt: 0 - valLoss: 0.5017996430397034 - trainLoss: 0.504039466381073\n",
      "cnt: 0 - valLoss: 0.5017804503440857 - trainLoss: 0.5040227770805359\n",
      "cnt: 0 - valLoss: 0.5017613172531128 - trainLoss: 0.5040060877799988\n",
      "cnt: 0 - valLoss: 0.5017421841621399 - trainLoss: 0.5039893984794617\n",
      "cnt: 0 - valLoss: 0.501723051071167 - trainLoss: 0.5039726495742798\n",
      "cnt: 0 - valLoss: 0.5017038583755493 - trainLoss: 0.5039559602737427\n",
      "cnt: 0 - valLoss: 0.5016847252845764 - trainLoss: 0.5039392709732056\n",
      "cnt: 0 - valLoss: 0.5016655921936035 - trainLoss: 0.5039225816726685\n",
      "cnt: 0 - valLoss: 0.5016465187072754 - trainLoss: 0.5039058923721313\n",
      "cnt: 0 - valLoss: 0.5016273260116577 - trainLoss: 0.5038892030715942\n",
      "cnt: 0 - valLoss: 0.5016081929206848 - trainLoss: 0.5038725733757019\n",
      "cnt: 0 - valLoss: 0.5015891194343567 - trainLoss: 0.5038559436798096\n",
      "cnt: 0 - valLoss: 0.5015699863433838 - trainLoss: 0.5038391947746277\n",
      "cnt: 0 - valLoss: 0.5015508532524109 - trainLoss: 0.5038226246833801\n",
      "cnt: 0 - valLoss: 0.5015317797660828 - trainLoss: 0.503805935382843\n",
      "cnt: 0 - valLoss: 0.5015127062797546 - trainLoss: 0.5037892460823059\n",
      "cnt: 0 - valLoss: 0.5014935731887817 - trainLoss: 0.5037726163864136\n",
      "cnt: 0 - valLoss: 0.5014744400978088 - trainLoss: 0.5037559866905212\n",
      "cnt: 0 - valLoss: 0.5014553666114807 - trainLoss: 0.5037392973899841\n",
      "cnt: 0 - valLoss: 0.5014362931251526 - trainLoss: 0.5037227272987366\n",
      "cnt: 0 - valLoss: 0.5014171600341797 - trainLoss: 0.5037060379981995\n",
      "cnt: 0 - valLoss: 0.5013981461524963 - trainLoss: 0.5036894083023071\n",
      "cnt: 0 - valLoss: 0.5013790726661682 - trainLoss: 0.5036727786064148\n",
      "cnt: 0 - valLoss: 0.5013599991798401 - trainLoss: 0.5036561489105225\n",
      "cnt: 0 - valLoss: 0.501340925693512 - trainLoss: 0.5036395192146301\n",
      "cnt: 0 - valLoss: 0.5013218522071838 - trainLoss: 0.5036228895187378\n",
      "cnt: 0 - valLoss: 0.5013028383255005 - trainLoss: 0.5036063194274902\n",
      "cnt: 0 - valLoss: 0.5012838244438171 - trainLoss: 0.5035897493362427\n",
      "cnt: 0 - valLoss: 0.501264750957489 - trainLoss: 0.5035730600357056\n",
      "cnt: 0 - valLoss: 0.5012457370758057 - trainLoss: 0.503556489944458\n",
      "cnt: 0 - valLoss: 0.5012266039848328 - trainLoss: 0.5035398602485657\n",
      "cnt: 0 - valLoss: 0.5012075901031494 - trainLoss: 0.5035232901573181\n",
      "cnt: 0 - valLoss: 0.5011885762214661 - trainLoss: 0.5035067200660706\n",
      "cnt: 0 - valLoss: 0.5011695027351379 - trainLoss: 0.503490149974823\n",
      "cnt: 0 - valLoss: 0.5011506080627441 - trainLoss: 0.5034735798835754\n",
      "cnt: 0 - valLoss: 0.501131534576416 - trainLoss: 0.5034569501876831\n",
      "cnt: 0 - valLoss: 0.5011125206947327 - trainLoss: 0.5034403800964355\n",
      "cnt: 0 - valLoss: 0.5010935068130493 - trainLoss: 0.503423810005188\n",
      "cnt: 0 - valLoss: 0.501074492931366 - trainLoss: 0.5034072399139404\n",
      "cnt: 0 - valLoss: 0.5010555386543274 - trainLoss: 0.5033906698226929\n",
      "cnt: 0 - valLoss: 0.501036524772644 - trainLoss: 0.5033741593360901\n",
      "cnt: 0 - valLoss: 0.5010175108909607 - trainLoss: 0.5033575892448425\n",
      "cnt: 0 - valLoss: 0.5009986162185669 - trainLoss: 0.503341019153595\n",
      "cnt: 0 - valLoss: 0.5009795427322388 - trainLoss: 0.5033245086669922\n",
      "cnt: 0 - valLoss: 0.5009605884552002 - trainLoss: 0.5033079385757446\n",
      "cnt: 0 - valLoss: 0.5009415745735168 - trainLoss: 0.5032913684844971\n",
      "cnt: 0 - valLoss: 0.5009226202964783 - trainLoss: 0.5032748579978943\n",
      "cnt: 0 - valLoss: 0.5009037256240845 - trainLoss: 0.5032583475112915\n",
      "cnt: 0 - valLoss: 0.5008847117424011 - trainLoss: 0.503241777420044\n",
      "cnt: 0 - valLoss: 0.5008658170700073 - trainLoss: 0.5032252669334412\n",
      "cnt: 0 - valLoss: 0.5008468627929688 - trainLoss: 0.5032087564468384\n",
      "cnt: 0 - valLoss: 0.5008278489112854 - trainLoss: 0.5031922459602356\n",
      "cnt: 0 - valLoss: 0.5008089542388916 - trainLoss: 0.5031757354736328\n",
      "cnt: 0 - valLoss: 0.500789999961853 - trainLoss: 0.5031591653823853\n",
      "cnt: 0 - valLoss: 0.5007710456848145 - trainLoss: 0.5031427145004272\n",
      "cnt: 0 - valLoss: 0.5007521510124207 - trainLoss: 0.5031262040138245\n",
      "cnt: 0 - valLoss: 0.5007331967353821 - trainLoss: 0.5031096935272217\n",
      "cnt: 0 - valLoss: 0.5007142424583435 - trainLoss: 0.5030931830406189\n",
      "cnt: 0 - valLoss: 0.5006953477859497 - trainLoss: 0.5030767321586609\n",
      "cnt: 0 - valLoss: 0.5006764531135559 - trainLoss: 0.5030602216720581\n",
      "cnt: 0 - valLoss: 0.5006575584411621 - trainLoss: 0.5030437111854553\n",
      "cnt: 0 - valLoss: 0.5006386041641235 - trainLoss: 0.5030272603034973\n",
      "cnt: 0 - valLoss: 0.5006197094917297 - trainLoss: 0.5030107498168945\n",
      "cnt: 0 - valLoss: 0.5006008148193359 - trainLoss: 0.5029942989349365\n",
      "cnt: 0 - valLoss: 0.5005819797515869 - trainLoss: 0.5029778480529785\n",
      "cnt: 0 - valLoss: 0.5005630850791931 - trainLoss: 0.5029613375663757\n",
      "cnt: 0 - valLoss: 0.5005441904067993 - trainLoss: 0.5029448866844177\n",
      "cnt: 0 - valLoss: 0.5005253553390503 - trainLoss: 0.5029284358024597\n",
      "cnt: 0 - valLoss: 0.5005064606666565 - trainLoss: 0.5029119849205017\n",
      "cnt: 0 - valLoss: 0.5004876852035522 - trainLoss: 0.5028955340385437\n",
      "cnt: 0 - valLoss: 0.5004688501358032 - trainLoss: 0.5028790831565857\n",
      "cnt: 0 - valLoss: 0.5004499554634094 - trainLoss: 0.5028626322746277\n",
      "cnt: 0 - valLoss: 0.5004311203956604 - trainLoss: 0.5028462409973145\n",
      "cnt: 0 - valLoss: 0.5004122853279114 - trainLoss: 0.5028297901153564\n",
      "cnt: 0 - valLoss: 0.5003934502601624 - trainLoss: 0.5028133392333984\n",
      "cnt: 0 - valLoss: 0.5003746747970581 - trainLoss: 0.5027968883514404\n",
      "cnt: 0 - valLoss: 0.5003558397293091 - trainLoss: 0.5027804970741272\n",
      "cnt: 0 - valLoss: 0.5003370046615601 - trainLoss: 0.502764105796814\n",
      "cnt: 0 - valLoss: 0.5003182291984558 - trainLoss: 0.5027477145195007\n",
      "cnt: 0 - valLoss: 0.5002994537353516 - trainLoss: 0.5027312636375427\n",
      "cnt: 0 - valLoss: 0.5002806782722473 - trainLoss: 0.5027148723602295\n",
      "cnt: 0 - valLoss: 0.5002617835998535 - trainLoss: 0.5026984810829163\n",
      "cnt: 0 - valLoss: 0.5002430081367493 - trainLoss: 0.5026820302009583\n",
      "cnt: 0 - valLoss: 0.500224232673645 - trainLoss: 0.502665638923645\n",
      "cnt: 0 - valLoss: 0.5002054572105408 - trainLoss: 0.5026492476463318\n",
      "cnt: 0 - valLoss: 0.5001866817474365 - trainLoss: 0.5026327967643738\n",
      "cnt: 0 - valLoss: 0.5001679062843323 - trainLoss: 0.5026164650917053\n",
      "cnt: 0 - valLoss: 0.500149130821228 - trainLoss: 0.5026000142097473\n",
      "cnt: 0 - valLoss: 0.5001304149627686 - trainLoss: 0.5025836825370789\n",
      "cnt: 0 - valLoss: 0.5001116394996643 - trainLoss: 0.5025673508644104\n",
      "cnt: 0 - valLoss: 0.5000928640365601 - trainLoss: 0.5025509595870972\n",
      "cnt: 0 - valLoss: 0.5000741481781006 - trainLoss: 0.5025345683097839\n",
      "cnt: 0 - valLoss: 0.5000553727149963 - trainLoss: 0.5025181770324707\n",
      "cnt: 0 - valLoss: 0.5000366568565369 - trainLoss: 0.5025018453598022\n",
      "cnt: 0 - valLoss: 0.5000178813934326 - trainLoss: 0.5024855136871338\n",
      "cnt: 0 - valLoss: 0.49999913573265076 - trainLoss: 0.5024691224098206\n",
      "cnt: 0 - valLoss: 0.4999804198741913 - trainLoss: 0.5024527907371521\n",
      "cnt: 0 - valLoss: 0.4999616742134094 - trainLoss: 0.5024364590644836\n",
      "cnt: 0 - valLoss: 0.49994295835494995 - trainLoss: 0.5024200677871704\n",
      "cnt: 0 - valLoss: 0.49992427229881287 - trainLoss: 0.502403736114502\n",
      "cnt: 0 - valLoss: 0.499905526638031 - trainLoss: 0.5023874044418335\n",
      "cnt: 0 - valLoss: 0.4998868405818939 - trainLoss: 0.5023711323738098\n",
      "cnt: 0 - valLoss: 0.49986815452575684 - trainLoss: 0.5023548007011414\n",
      "cnt: 0 - valLoss: 0.49984946846961975 - trainLoss: 0.5023384094238281\n",
      "cnt: 0 - valLoss: 0.49983078241348267 - trainLoss: 0.5023221373558044\n",
      "cnt: 0 - valLoss: 0.4998120963573456 - trainLoss: 0.502305805683136\n",
      "cnt: 0 - valLoss: 0.4997934103012085 - trainLoss: 0.5022894740104675\n",
      "cnt: 0 - valLoss: 0.4997747242450714 - trainLoss: 0.5022732019424438\n",
      "cnt: 0 - valLoss: 0.4997560381889343 - trainLoss: 0.5022569298744202\n",
      "cnt: 0 - valLoss: 0.49973735213279724 - trainLoss: 0.5022405982017517\n",
      "cnt: 0 - valLoss: 0.49971866607666016 - trainLoss: 0.502224326133728\n",
      "cnt: 0 - valLoss: 0.49970000982284546 - trainLoss: 0.5022079944610596\n",
      "cnt: 0 - valLoss: 0.4996813237667084 - trainLoss: 0.5021917223930359\n",
      "cnt: 0 - valLoss: 0.49966272711753845 - trainLoss: 0.5021754503250122\n",
      "cnt: 0 - valLoss: 0.49964404106140137 - trainLoss: 0.5021591186523438\n",
      "cnt: 0 - valLoss: 0.49962538480758667 - trainLoss: 0.5021429061889648\n",
      "cnt: 0 - valLoss: 0.499606728553772 - trainLoss: 0.5021265745162964\n",
      "cnt: 0 - valLoss: 0.49958810210227966 - trainLoss: 0.5021103620529175\n",
      "cnt: 0 - valLoss: 0.49956950545310974 - trainLoss: 0.5020940899848938\n",
      "cnt: 0 - valLoss: 0.49955084919929504 - trainLoss: 0.5020778775215149\n",
      "cnt: 0 - valLoss: 0.4995322525501251 - trainLoss: 0.5020615458488464\n",
      "cnt: 0 - valLoss: 0.4995135962963104 - trainLoss: 0.5020453333854675\n",
      "cnt: 0 - valLoss: 0.4994949996471405 - trainLoss: 0.5020290613174438\n",
      "cnt: 0 - valLoss: 0.4994764029979706 - trainLoss: 0.5020127892494202\n",
      "cnt: 0 - valLoss: 0.49945780634880066 - trainLoss: 0.5019965767860413\n",
      "cnt: 0 - valLoss: 0.49943920969963074 - trainLoss: 0.5019803643226624\n",
      "cnt: 0 - valLoss: 0.4994205832481384 - trainLoss: 0.5019641518592834\n",
      "cnt: 0 - valLoss: 0.4994019865989685 - trainLoss: 0.5019479393959045\n",
      "cnt: 0 - valLoss: 0.49938341975212097 - trainLoss: 0.5019317269325256\n",
      "cnt: 0 - valLoss: 0.4993648827075958 - trainLoss: 0.501915454864502\n",
      "cnt: 0 - valLoss: 0.4993463158607483 - trainLoss: 0.5018993020057678\n",
      "cnt: 0 - valLoss: 0.49932771921157837 - trainLoss: 0.5018830895423889\n",
      "cnt: 0 - valLoss: 0.49930915236473083 - trainLoss: 0.5018669366836548\n",
      "cnt: 0 - valLoss: 0.4992905855178833 - trainLoss: 0.5018507242202759\n",
      "cnt: 0 - valLoss: 0.49927204847335815 - trainLoss: 0.5018345713615417\n",
      "cnt: 0 - valLoss: 0.49925342202186584 - trainLoss: 0.5018183588981628\n",
      "cnt: 0 - valLoss: 0.4992348253726959 - trainLoss: 0.5018022656440735\n",
      "cnt: 0 - valLoss: 0.49921631813049316 - trainLoss: 0.5017861127853394\n",
      "cnt: 0 - valLoss: 0.49919772148132324 - trainLoss: 0.5017699599266052\n",
      "cnt: 0 - valLoss: 0.4991791546344757 - trainLoss: 0.5017537474632263\n",
      "cnt: 0 - valLoss: 0.4991605877876282 - trainLoss: 0.501737654209137\n",
      "cnt: 0 - valLoss: 0.4991420805454254 - trainLoss: 0.5017215013504028\n",
      "cnt: 0 - valLoss: 0.4991235136985779 - trainLoss: 0.5017054080963135\n",
      "cnt: 0 - valLoss: 0.49910494685173035 - trainLoss: 0.5016892552375793\n",
      "cnt: 0 - valLoss: 0.4990864396095276 - trainLoss: 0.5016731023788452\n",
      "cnt: 0 - valLoss: 0.49906793236732483 - trainLoss: 0.5016570091247559\n",
      "cnt: 0 - valLoss: 0.49904942512512207 - trainLoss: 0.5016409158706665\n",
      "cnt: 0 - valLoss: 0.4990308880805969 - trainLoss: 0.5016248226165771\n",
      "cnt: 0 - valLoss: 0.49901238083839417 - trainLoss: 0.501608669757843\n",
      "cnt: 0 - valLoss: 0.4989938735961914 - trainLoss: 0.5015925765037537\n",
      "cnt: 0 - valLoss: 0.49897533655166626 - trainLoss: 0.5015764832496643\n",
      "cnt: 0 - valLoss: 0.4989568293094635 - trainLoss: 0.5015603303909302\n",
      "cnt: 0 - valLoss: 0.49893835186958313 - trainLoss: 0.5015442371368408\n",
      "cnt: 0 - valLoss: 0.49891987442970276 - trainLoss: 0.5015282034873962\n",
      "cnt: 0 - valLoss: 0.4989013671875 - trainLoss: 0.5015120506286621\n",
      "cnt: 0 - valLoss: 0.49888285994529724 - trainLoss: 0.5014959573745728\n",
      "cnt: 0 - valLoss: 0.49886441230773926 - trainLoss: 0.5014799237251282\n",
      "cnt: 0 - valLoss: 0.4988459050655365 - trainLoss: 0.5014638304710388\n",
      "cnt: 0 - valLoss: 0.49882742762565613 - trainLoss: 0.5014477372169495\n",
      "cnt: 0 - valLoss: 0.49880895018577576 - trainLoss: 0.5014316439628601\n",
      "cnt: 0 - valLoss: 0.49879053235054016 - trainLoss: 0.5014156103134155\n",
      "cnt: 0 - valLoss: 0.4987720549106598 - trainLoss: 0.501399576663971\n",
      "cnt: 0 - valLoss: 0.4987535774707794 - trainLoss: 0.5013834834098816\n",
      "cnt: 0 - valLoss: 0.4987351596355438 - trainLoss: 0.501367449760437\n",
      "cnt: 0 - valLoss: 0.49871671199798584 - trainLoss: 0.5013514161109924\n",
      "cnt: 0 - valLoss: 0.49869823455810547 - trainLoss: 0.5013353228569031\n",
      "cnt: 0 - valLoss: 0.4986798167228699 - trainLoss: 0.5013192892074585\n",
      "cnt: 0 - valLoss: 0.4986613988876343 - trainLoss: 0.5013033151626587\n",
      "cnt: 0 - valLoss: 0.49864301085472107 - trainLoss: 0.5012872219085693\n",
      "cnt: 0 - valLoss: 0.4986245632171631 - trainLoss: 0.5012711882591248\n",
      "cnt: 0 - valLoss: 0.4986061453819275 - trainLoss: 0.5012551546096802\n",
      "cnt: 0 - valLoss: 0.4985876977443695 - trainLoss: 0.5012391209602356\n",
      "cnt: 0 - valLoss: 0.4985692799091339 - trainLoss: 0.5012232065200806\n",
      "cnt: 0 - valLoss: 0.4985508918762207 - trainLoss: 0.5012071132659912\n",
      "cnt: 0 - valLoss: 0.4985325038433075 - trainLoss: 0.5011911392211914\n",
      "cnt: 0 - valLoss: 0.4985140562057495 - trainLoss: 0.5011751055717468\n",
      "cnt: 0 - valLoss: 0.4984956681728363 - trainLoss: 0.5011590719223022\n",
      "cnt: 0 - valLoss: 0.4984772801399231 - trainLoss: 0.5011430978775024\n",
      "cnt: 0 - valLoss: 0.4984588921070099 - trainLoss: 0.5011270642280579\n",
      "cnt: 0 - valLoss: 0.4984405040740967 - trainLoss: 0.5011110901832581\n",
      "cnt: 0 - valLoss: 0.49842217564582825 - trainLoss: 0.5010951161384583\n",
      "cnt: 0 - valLoss: 0.4984038174152374 - trainLoss: 0.5010791420936584\n",
      "cnt: 0 - valLoss: 0.49838539958000183 - trainLoss: 0.5010631084442139\n",
      "cnt: 0 - valLoss: 0.498367041349411 - trainLoss: 0.5010471940040588\n",
      "cnt: 0 - valLoss: 0.4983487129211426 - trainLoss: 0.5010311603546143\n",
      "cnt: 0 - valLoss: 0.49833035469055176 - trainLoss: 0.5010152459144592\n",
      "cnt: 0 - valLoss: 0.4983120262622833 - trainLoss: 0.5009992718696594\n",
      "cnt: 0 - valLoss: 0.4982936680316925 - trainLoss: 0.5009832978248596\n",
      "cnt: 0 - valLoss: 0.4982753396034241 - trainLoss: 0.5009673237800598\n",
      "cnt: 0 - valLoss: 0.49825698137283325 - trainLoss: 0.5009514093399048\n",
      "cnt: 0 - valLoss: 0.4982386529445648 - trainLoss: 0.500935435295105\n",
      "cnt: 0 - valLoss: 0.498220294713974 - trainLoss: 0.5009194612503052\n",
      "cnt: 0 - valLoss: 0.4982019364833832 - trainLoss: 0.5009035468101501\n",
      "cnt: 0 - valLoss: 0.4981836974620819 - trainLoss: 0.5008875727653503\n",
      "cnt: 0 - valLoss: 0.4981653392314911 - trainLoss: 0.5008716583251953\n",
      "cnt: 0 - valLoss: 0.49814704060554504 - trainLoss: 0.5008556842803955\n",
      "cnt: 0 - valLoss: 0.498128741979599 - trainLoss: 0.5008398294448853\n",
      "cnt: 0 - valLoss: 0.49811044335365295 - trainLoss: 0.5008239150047302\n",
      "cnt: 0 - valLoss: 0.4980921447277069 - trainLoss: 0.5008079409599304\n",
      "cnt: 0 - valLoss: 0.49807387590408325 - trainLoss: 0.5007920265197754\n",
      "cnt: 0 - valLoss: 0.4980555474758148 - trainLoss: 0.5007761120796204\n",
      "cnt: 0 - valLoss: 0.49803727865219116 - trainLoss: 0.5007601976394653\n",
      "cnt: 0 - valLoss: 0.4980190098285675 - trainLoss: 0.5007443428039551\n",
      "cnt: 0 - valLoss: 0.49800071120262146 - trainLoss: 0.5007283687591553\n",
      "cnt: 0 - valLoss: 0.4979824125766754 - trainLoss: 0.5007124543190002\n",
      "cnt: 0 - valLoss: 0.49796414375305176 - trainLoss: 0.50069659948349\n",
      "cnt: 0 - valLoss: 0.4979459047317505 - trainLoss: 0.5006807446479797\n",
      "cnt: 0 - valLoss: 0.49792757630348206 - trainLoss: 0.5006648898124695\n",
      "cnt: 0 - valLoss: 0.497909277677536 - trainLoss: 0.5006489753723145\n",
      "cnt: 0 - valLoss: 0.49789103865623474 - trainLoss: 0.5006331205368042\n",
      "cnt: 0 - valLoss: 0.4978727698326111 - trainLoss: 0.500617265701294\n",
      "cnt: 0 - valLoss: 0.4978545606136322 - trainLoss: 0.5006013512611389\n",
      "cnt: 0 - valLoss: 0.49783626198768616 - trainLoss: 0.5005855560302734\n",
      "cnt: 0 - valLoss: 0.4978180229663849 - trainLoss: 0.5005696415901184\n",
      "cnt: 0 - valLoss: 0.497799813747406 - trainLoss: 0.5005537867546082\n",
      "cnt: 0 - valLoss: 0.49778157472610474 - trainLoss: 0.5005379319190979\n",
      "cnt: 0 - valLoss: 0.4977633059024811 - trainLoss: 0.5005220770835876\n",
      "cnt: 0 - valLoss: 0.4977451264858246 - trainLoss: 0.5005062222480774\n",
      "cnt: 0 - valLoss: 0.4977268576622009 - trainLoss: 0.5004903674125671\n",
      "cnt: 0 - valLoss: 0.49770861864089966 - trainLoss: 0.5004745125770569\n",
      "cnt: 0 - valLoss: 0.49769043922424316 - trainLoss: 0.5004587769508362\n",
      "cnt: 0 - valLoss: 0.4976722300052643 - trainLoss: 0.5004428625106812\n",
      "cnt: 0 - valLoss: 0.497653990983963 - trainLoss: 0.5004270672798157\n",
      "cnt: 0 - valLoss: 0.4976358413696289 - trainLoss: 0.5004112124443054\n",
      "cnt: 0 - valLoss: 0.49761757254600525 - trainLoss: 0.5003954172134399\n",
      "cnt: 0 - valLoss: 0.49759939312934875 - trainLoss: 0.5003796219825745\n",
      "cnt: 0 - valLoss: 0.49758121371269226 - trainLoss: 0.500363826751709\n",
      "cnt: 0 - valLoss: 0.49756306409835815 - trainLoss: 0.5003479719161987\n",
      "cnt: 0 - valLoss: 0.4975448250770569 - trainLoss: 0.5003321766853333\n",
      "cnt: 0 - valLoss: 0.49752670526504517 - trainLoss: 0.5003163814544678\n",
      "cnt: 0 - valLoss: 0.49750852584838867 - trainLoss: 0.5003005862236023\n",
      "cnt: 0 - valLoss: 0.4974903464317322 - trainLoss: 0.5002847909927368\n",
      "cnt: 0 - valLoss: 0.4974721372127533 - trainLoss: 0.5002689957618713\n",
      "cnt: 0 - valLoss: 0.4974540174007416 - trainLoss: 0.5002532005310059\n",
      "cnt: 0 - valLoss: 0.4974358379840851 - trainLoss: 0.5002374053001404\n",
      "cnt: 0 - valLoss: 0.49741771817207336 - trainLoss: 0.5002216696739197\n",
      "cnt: 0 - valLoss: 0.49739956855773926 - trainLoss: 0.5002058744430542\n",
      "cnt: 0 - valLoss: 0.49738138914108276 - trainLoss: 0.5001901388168335\n",
      "cnt: 0 - valLoss: 0.49736326932907104 - trainLoss: 0.5001744031906128\n",
      "cnt: 0 - valLoss: 0.4973451495170593 - trainLoss: 0.5001586079597473\n",
      "cnt: 0 - valLoss: 0.4973269999027252 - trainLoss: 0.5001428127288818\n",
      "cnt: 0 - valLoss: 0.4973088800907135 - trainLoss: 0.5001271367073059\n",
      "cnt: 0 - valLoss: 0.49729079008102417 - trainLoss: 0.5001113414764404\n",
      "cnt: 0 - valLoss: 0.4972726106643677 - trainLoss: 0.5000956058502197\n",
      "cnt: 0 - valLoss: 0.49725452065467834 - trainLoss: 0.500079870223999\n",
      "cnt: 0 - valLoss: 0.497236430644989 - trainLoss: 0.5000641345977783\n",
      "cnt: 0 - valLoss: 0.4972183108329773 - trainLoss: 0.5000483393669128\n",
      "cnt: 0 - valLoss: 0.49720022082328796 - trainLoss: 0.5000326037406921\n",
      "cnt: 0 - valLoss: 0.4971821904182434 - trainLoss: 0.5000169277191162\n",
      "cnt: 0 - valLoss: 0.4971641004085541 - trainLoss: 0.5000011920928955\n",
      "cnt: 0 - valLoss: 0.49714601039886475 - trainLoss: 0.4999854862689972\n",
      "cnt: 0 - valLoss: 0.497127890586853 - trainLoss: 0.4999697804450989\n",
      "cnt: 0 - valLoss: 0.4971098005771637 - trainLoss: 0.4999540448188782\n",
      "cnt: 0 - valLoss: 0.49709174036979675 - trainLoss: 0.49993836879730225\n",
      "cnt: 0 - valLoss: 0.4970736503601074 - trainLoss: 0.49992266297340393\n",
      "cnt: 0 - valLoss: 0.4970555603504181 - trainLoss: 0.4999069571495056\n",
      "cnt: 0 - valLoss: 0.49703752994537354 - trainLoss: 0.4998912513256073\n",
      "cnt: 0 - valLoss: 0.4970194697380066 - trainLoss: 0.49987557530403137\n",
      "cnt: 0 - valLoss: 0.49700143933296204 - trainLoss: 0.49985986948013306\n",
      "cnt: 0 - valLoss: 0.49698343873023987 - trainLoss: 0.49984416365623474\n",
      "cnt: 0 - valLoss: 0.4969653785228729 - trainLoss: 0.4998284876346588\n",
      "cnt: 0 - valLoss: 0.49694734811782837 - trainLoss: 0.49981287121772766\n",
      "cnt: 0 - valLoss: 0.4969292879104614 - trainLoss: 0.49979716539382935\n",
      "cnt: 0 - valLoss: 0.49691128730773926 - trainLoss: 0.49978145956993103\n",
      "cnt: 0 - valLoss: 0.4968932569026947 - trainLoss: 0.49976587295532227\n",
      "cnt: 0 - valLoss: 0.49687525629997253 - trainLoss: 0.49975016713142395\n",
      "cnt: 0 - valLoss: 0.4968571960926056 - trainLoss: 0.499734491109848\n",
      "cnt: 0 - valLoss: 0.4968391954898834 - trainLoss: 0.49971887469291687\n",
      "cnt: 0 - valLoss: 0.49682119488716125 - trainLoss: 0.49970319867134094\n",
      "cnt: 0 - valLoss: 0.4968031942844391 - trainLoss: 0.4996875822544098\n",
      "cnt: 0 - valLoss: 0.4967851936817169 - trainLoss: 0.4996718764305115\n",
      "cnt: 0 - valLoss: 0.49676722288131714 - trainLoss: 0.4996562898159027\n",
      "cnt: 0 - valLoss: 0.49674922227859497 - trainLoss: 0.49964064359664917\n",
      "cnt: 0 - valLoss: 0.4967312216758728 - trainLoss: 0.499625027179718\n",
      "cnt: 0 - valLoss: 0.4967132806777954 - trainLoss: 0.4996093809604645\n",
      "cnt: 0 - valLoss: 0.49669528007507324 - trainLoss: 0.4995937645435333\n",
      "cnt: 0 - valLoss: 0.49667736887931824 - trainLoss: 0.4995781481266022\n",
      "cnt: 0 - valLoss: 0.49665936827659607 - trainLoss: 0.499562531709671\n",
      "cnt: 0 - valLoss: 0.4966413378715515 - trainLoss: 0.49954691529273987\n",
      "cnt: 0 - valLoss: 0.4966234266757965 - trainLoss: 0.4995313286781311\n",
      "cnt: 0 - valLoss: 0.4966055154800415 - trainLoss: 0.49951574206352234\n",
      "cnt: 0 - valLoss: 0.49658751487731934 - trainLoss: 0.4995001256465912\n",
      "cnt: 0 - valLoss: 0.49656960368156433 - trainLoss: 0.49948450922966003\n",
      "cnt: 0 - valLoss: 0.49655163288116455 - trainLoss: 0.49946892261505127\n",
      "cnt: 0 - valLoss: 0.49653369188308716 - trainLoss: 0.4994533360004425\n",
      "cnt: 0 - valLoss: 0.49651578068733215 - trainLoss: 0.49943774938583374\n",
      "cnt: 0 - valLoss: 0.49649783968925476 - trainLoss: 0.49942219257354736\n",
      "cnt: 0 - valLoss: 0.49647992849349976 - trainLoss: 0.4994066059589386\n",
      "cnt: 0 - valLoss: 0.49646201729774475 - trainLoss: 0.4993910491466522\n",
      "cnt: 0 - valLoss: 0.49644410610198975 - trainLoss: 0.49937549233436584\n",
      "cnt: 0 - valLoss: 0.49642619490623474 - trainLoss: 0.4993599057197571\n",
      "cnt: 0 - valLoss: 0.4964083135128021 - trainLoss: 0.4993443191051483\n",
      "cnt: 0 - valLoss: 0.49639034271240234 - trainLoss: 0.4993287920951843\n",
      "cnt: 0 - valLoss: 0.4963725209236145 - trainLoss: 0.49931323528289795\n",
      "cnt: 0 - valLoss: 0.4963546097278595 - trainLoss: 0.49929770827293396\n",
      "cnt: 0 - valLoss: 0.4963367283344269 - trainLoss: 0.4992821216583252\n",
      "cnt: 0 - valLoss: 0.49631884694099426 - trainLoss: 0.4992665946483612\n",
      "cnt: 0 - valLoss: 0.49630093574523926 - trainLoss: 0.4992510676383972\n",
      "cnt: 0 - valLoss: 0.4962831139564514 - trainLoss: 0.49923551082611084\n",
      "cnt: 0 - valLoss: 0.4962652325630188 - trainLoss: 0.49921998381614685\n",
      "cnt: 0 - valLoss: 0.49624738097190857 - trainLoss: 0.49920448660850525\n",
      "cnt: 0 - valLoss: 0.49622949957847595 - trainLoss: 0.49918895959854126\n",
      "cnt: 0 - valLoss: 0.49621161818504333 - trainLoss: 0.4991734027862549\n",
      "cnt: 0 - valLoss: 0.4961937963962555 - trainLoss: 0.4991578757762909\n",
      "cnt: 0 - valLoss: 0.49617594480514526 - trainLoss: 0.49914243817329407\n",
      "cnt: 0 - valLoss: 0.4961581230163574 - trainLoss: 0.49912694096565247\n",
      "cnt: 0 - valLoss: 0.4961403012275696 - trainLoss: 0.49911144375801086\n",
      "cnt: 0 - valLoss: 0.49612247943878174 - trainLoss: 0.49909594655036926\n",
      "cnt: 0 - valLoss: 0.4961046874523163 - trainLoss: 0.49908044934272766\n",
      "cnt: 0 - valLoss: 0.49608689546585083 - trainLoss: 0.49906495213508606\n",
      "cnt: 0 - valLoss: 0.496069073677063 - trainLoss: 0.49904945492744446\n",
      "cnt: 0 - valLoss: 0.49605122208595276 - trainLoss: 0.49903395771980286\n",
      "cnt: 0 - valLoss: 0.4960334002971649 - trainLoss: 0.49901852011680603\n",
      "cnt: 0 - valLoss: 0.49601566791534424 - trainLoss: 0.4990030527114868\n",
      "cnt: 0 - valLoss: 0.495997816324234 - trainLoss: 0.4989875555038452\n",
      "cnt: 0 - valLoss: 0.49598002433776855 - trainLoss: 0.4989721179008484\n",
      "cnt: 0 - valLoss: 0.4959622621536255 - trainLoss: 0.49895668029785156\n",
      "cnt: 0 - valLoss: 0.49594447016716003 - trainLoss: 0.49894121289253235\n",
      "cnt: 0 - valLoss: 0.49592670798301697 - trainLoss: 0.4989257752895355\n",
      "cnt: 0 - valLoss: 0.4959089159965515 - trainLoss: 0.4989103078842163\n",
      "cnt: 0 - valLoss: 0.49589118361473083 - trainLoss: 0.4988948702812195\n",
      "cnt: 0 - valLoss: 0.49587345123291016 - trainLoss: 0.49887940287590027\n",
      "cnt: 0 - valLoss: 0.4958556890487671 - trainLoss: 0.49886399507522583\n",
      "cnt: 0 - valLoss: 0.49583789706230164 - trainLoss: 0.498848557472229\n",
      "cnt: 0 - valLoss: 0.49582016468048096 - trainLoss: 0.49883314967155457\n",
      "cnt: 0 - valLoss: 0.4958024322986603 - trainLoss: 0.4988177418708801\n",
      "cnt: 0 - valLoss: 0.4957846701145172 - trainLoss: 0.4988023340702057\n",
      "cnt: 0 - valLoss: 0.49576693773269653 - trainLoss: 0.49878692626953125\n",
      "cnt: 0 - valLoss: 0.49574923515319824 - trainLoss: 0.4987715184688568\n",
      "cnt: 0 - valLoss: 0.4957314729690552 - trainLoss: 0.4987560212612152\n",
      "cnt: 0 - valLoss: 0.4957137405872345 - trainLoss: 0.49874070286750793\n",
      "cnt: 0 - valLoss: 0.4956960678100586 - trainLoss: 0.4987252950668335\n",
      "cnt: 0 - valLoss: 0.4956783354282379 - trainLoss: 0.49870988726615906\n",
      "cnt: 0 - valLoss: 0.495660662651062 - trainLoss: 0.49869444966316223\n",
      "cnt: 0 - valLoss: 0.49564293026924133 - trainLoss: 0.49867910146713257\n",
      "cnt: 0 - valLoss: 0.49562522768974304 - trainLoss: 0.49866369366645813\n",
      "cnt: 0 - valLoss: 0.49560752511024475 - trainLoss: 0.4986483156681061\n",
      "cnt: 0 - valLoss: 0.49558982253074646 - trainLoss: 0.49863290786743164\n",
      "cnt: 0 - valLoss: 0.49557211995124817 - trainLoss: 0.49861758947372437\n",
      "cnt: 0 - valLoss: 0.49555447697639465 - trainLoss: 0.4986021816730499\n",
      "cnt: 0 - valLoss: 0.49553680419921875 - trainLoss: 0.49858686327934265\n",
      "cnt: 0 - valLoss: 0.49551913142204285 - trainLoss: 0.4985714554786682\n",
      "cnt: 0 - valLoss: 0.49550142884254456 - trainLoss: 0.49855613708496094\n",
      "cnt: 0 - valLoss: 0.4954838156700134 - trainLoss: 0.4985407590866089\n",
      "cnt: 0 - valLoss: 0.49546611309051514 - trainLoss: 0.4985254108905792\n",
      "cnt: 0 - valLoss: 0.495448499917984 - trainLoss: 0.49851006269454956\n",
      "cnt: 0 - valLoss: 0.4954308569431305 - trainLoss: 0.4984947144985199\n",
      "cnt: 0 - valLoss: 0.4954131841659546 - trainLoss: 0.49847933650016785\n",
      "cnt: 0 - valLoss: 0.49539557099342346 - trainLoss: 0.49846401810646057\n",
      "cnt: 0 - valLoss: 0.49537792801856995 - trainLoss: 0.4984486997127533\n",
      "cnt: 0 - valLoss: 0.49536028504371643 - trainLoss: 0.498433381319046\n",
      "cnt: 0 - valLoss: 0.4953426420688629 - trainLoss: 0.49841806292533875\n",
      "cnt: 0 - valLoss: 0.4953250288963318 - trainLoss: 0.4984027147293091\n",
      "cnt: 0 - valLoss: 0.49530741572380066 - trainLoss: 0.4983874559402466\n",
      "cnt: 0 - valLoss: 0.49528980255126953 - trainLoss: 0.4983721375465393\n",
      "cnt: 0 - valLoss: 0.4952721893787384 - trainLoss: 0.4983568489551544\n",
      "cnt: 0 - valLoss: 0.4952545464038849 - trainLoss: 0.49834153056144714\n",
      "cnt: 0 - valLoss: 0.49523693323135376 - trainLoss: 0.49832621216773987\n",
      "cnt: 0 - valLoss: 0.4952193796634674 - trainLoss: 0.498310923576355\n",
      "cnt: 0 - valLoss: 0.49520179629325867 - trainLoss: 0.4982956051826477\n",
      "cnt: 0 - valLoss: 0.49518418312072754 - trainLoss: 0.4982803165912628\n",
      "cnt: 0 - valLoss: 0.4951665997505188 - trainLoss: 0.4982650876045227\n",
      "cnt: 0 - valLoss: 0.49514898657798767 - trainLoss: 0.49824976921081543\n",
      "cnt: 0 - valLoss: 0.4951314628124237 - trainLoss: 0.49823448061943054\n",
      "cnt: 0 - valLoss: 0.49511387944221497 - trainLoss: 0.49821919202804565\n",
      "cnt: 0 - valLoss: 0.4950963258743286 - trainLoss: 0.49820396304130554\n",
      "cnt: 0 - valLoss: 0.49507877230644226 - trainLoss: 0.49818867444992065\n",
      "cnt: 0 - valLoss: 0.4950612187385559 - trainLoss: 0.49817338585853577\n",
      "cnt: 0 - valLoss: 0.49504363536834717 - trainLoss: 0.49815815687179565\n",
      "cnt: 0 - valLoss: 0.4950261116027832 - trainLoss: 0.49814286828041077\n",
      "cnt: 0 - valLoss: 0.49500858783721924 - trainLoss: 0.49812763929367065\n",
      "cnt: 0 - valLoss: 0.4949910044670105 - trainLoss: 0.49811241030693054\n",
      "cnt: 0 - valLoss: 0.49497348070144653 - trainLoss: 0.49809712171554565\n",
      "cnt: 0 - valLoss: 0.4949559271335602 - trainLoss: 0.49808189272880554\n",
      "cnt: 0 - valLoss: 0.4949384033679962 - trainLoss: 0.49806666374206543\n",
      "cnt: 0 - valLoss: 0.49492087960243225 - trainLoss: 0.49805140495300293\n",
      "cnt: 0 - valLoss: 0.4949033856391907 - trainLoss: 0.4980362057685852\n",
      "cnt: 0 - valLoss: 0.4948858916759491 - trainLoss: 0.4980209767818451\n",
      "cnt: 0 - valLoss: 0.49486833810806274 - trainLoss: 0.49800577759742737\n",
      "cnt: 0 - valLoss: 0.49485087394714355 - trainLoss: 0.49799054861068726\n",
      "cnt: 0 - valLoss: 0.4948333501815796 - trainLoss: 0.49797531962394714\n",
      "cnt: 0 - valLoss: 0.494815856218338 - trainLoss: 0.4979601502418518\n",
      "cnt: 0 - valLoss: 0.4947983920574188 - trainLoss: 0.4979449212551117\n",
      "cnt: 0 - valLoss: 0.49478089809417725 - trainLoss: 0.49792972207069397\n",
      "cnt: 0 - valLoss: 0.49476340413093567 - trainLoss: 0.497914582490921\n",
      "cnt: 0 - valLoss: 0.49474596977233887 - trainLoss: 0.49789929389953613\n",
      "cnt: 0 - valLoss: 0.4947284758090973 - trainLoss: 0.4978841543197632\n",
      "cnt: 0 - valLoss: 0.4947110414505005 - trainLoss: 0.49786895513534546\n",
      "cnt: 0 - valLoss: 0.4946935474872589 - trainLoss: 0.4978537857532501\n",
      "cnt: 0 - valLoss: 0.4946760833263397 - trainLoss: 0.4978386163711548\n",
      "cnt: 0 - valLoss: 0.4946586489677429 - trainLoss: 0.49782344698905945\n",
      "cnt: 0 - valLoss: 0.4946412146091461 - trainLoss: 0.4978083074092865\n",
      "cnt: 0 - valLoss: 0.49462375044822693 - trainLoss: 0.4977931082248688\n",
      "cnt: 0 - valLoss: 0.4946063160896301 - trainLoss: 0.49777793884277344\n",
      "cnt: 0 - valLoss: 0.4945888817310333 - trainLoss: 0.4977627992630005\n",
      "cnt: 0 - valLoss: 0.4945714771747589 - trainLoss: 0.49774760007858276\n",
      "cnt: 0 - valLoss: 0.4945540130138397 - trainLoss: 0.4977324604988098\n",
      "cnt: 0 - valLoss: 0.4945365786552429 - trainLoss: 0.4977172911167145\n",
      "cnt: 0 - valLoss: 0.4945191740989685 - trainLoss: 0.49770215153694153\n",
      "cnt: 0 - valLoss: 0.4945017695426941 - trainLoss: 0.49768707156181335\n",
      "cnt: 0 - valLoss: 0.4944843649864197 - trainLoss: 0.4976719319820404\n",
      "cnt: 0 - valLoss: 0.4944669306278229 - trainLoss: 0.49765679240226746\n",
      "cnt: 0 - valLoss: 0.49444952607154846 - trainLoss: 0.4976416230201721\n",
      "cnt: 0 - valLoss: 0.4944321811199188 - trainLoss: 0.49762648344039917\n",
      "cnt: 0 - valLoss: 0.4944147765636444 - trainLoss: 0.497611403465271\n",
      "cnt: 0 - valLoss: 0.49439737200737 - trainLoss: 0.49759626388549805\n",
      "cnt: 0 - valLoss: 0.49437999725341797 - trainLoss: 0.4975811839103699\n",
      "cnt: 0 - valLoss: 0.49436259269714355 - trainLoss: 0.4975660443305969\n",
      "cnt: 0 - valLoss: 0.4943452477455139 - trainLoss: 0.49755093455314636\n",
      "cnt: 0 - valLoss: 0.4943279027938843 - trainLoss: 0.4975358545780182\n",
      "cnt: 0 - valLoss: 0.49431052803993225 - trainLoss: 0.4975207448005676\n",
      "cnt: 0 - valLoss: 0.49429312348365784 - trainLoss: 0.49750569462776184\n",
      "cnt: 0 - valLoss: 0.4942758083343506 - trainLoss: 0.4974905252456665\n",
      "cnt: 0 - valLoss: 0.49425846338272095 - trainLoss: 0.4974754750728607\n",
      "cnt: 0 - valLoss: 0.4942410886287689 - trainLoss: 0.49746039509773254\n",
      "cnt: 0 - valLoss: 0.49422377347946167 - trainLoss: 0.497445285320282\n",
      "cnt: 0 - valLoss: 0.4942064583301544 - trainLoss: 0.4974302649497986\n",
      "cnt: 0 - valLoss: 0.49418917298316956 - trainLoss: 0.4974152147769928\n",
      "cnt: 0 - valLoss: 0.4941718280315399 - trainLoss: 0.49740010499954224\n",
      "cnt: 0 - valLoss: 0.49415451288223267 - trainLoss: 0.49738502502441406\n",
      "cnt: 0 - valLoss: 0.4941371977329254 - trainLoss: 0.49737000465393066\n",
      "cnt: 0 - valLoss: 0.4941199719905853 - trainLoss: 0.4973549544811249\n",
      "cnt: 0 - valLoss: 0.4941025972366333 - trainLoss: 0.4973398745059967\n",
      "cnt: 0 - valLoss: 0.49408528208732605 - trainLoss: 0.4973248243331909\n",
      "cnt: 0 - valLoss: 0.4940680265426636 - trainLoss: 0.49730977416038513\n",
      "cnt: 0 - valLoss: 0.4940507113933563 - trainLoss: 0.49729475378990173\n",
      "cnt: 0 - valLoss: 0.4940333962440491 - trainLoss: 0.49727973341941833\n",
      "cnt: 0 - valLoss: 0.4940161108970642 - trainLoss: 0.49726468324661255\n",
      "cnt: 0 - valLoss: 0.49399882555007935 - trainLoss: 0.49724966287612915\n",
      "cnt: 0 - valLoss: 0.49398159980773926 - trainLoss: 0.49723467230796814\n",
      "cnt: 0 - valLoss: 0.493964284658432 - trainLoss: 0.49721968173980713\n",
      "cnt: 0 - valLoss: 0.49394702911376953 - trainLoss: 0.49720463156700134\n",
      "cnt: 0 - valLoss: 0.49392974376678467 - trainLoss: 0.49718964099884033\n",
      "cnt: 0 - valLoss: 0.4939125180244446 - trainLoss: 0.4971746504306793\n",
      "cnt: 0 - valLoss: 0.49389520287513733 - trainLoss: 0.4971596300601959\n",
      "cnt: 0 - valLoss: 0.49387797713279724 - trainLoss: 0.4971446692943573\n",
      "cnt: 0 - valLoss: 0.4938606917858124 - trainLoss: 0.4971296489238739\n",
      "cnt: 0 - valLoss: 0.4938434660434723 - trainLoss: 0.4971146583557129\n",
      "cnt: 0 - valLoss: 0.4938262701034546 - trainLoss: 0.49709972739219666\n",
      "cnt: 0 - valLoss: 0.49380895495414734 - trainLoss: 0.49708473682403564\n",
      "cnt: 0 - valLoss: 0.49379175901412964 - trainLoss: 0.49706974625587463\n",
      "cnt: 0 - valLoss: 0.49377450346946716 - trainLoss: 0.497054785490036\n",
      "cnt: 0 - valLoss: 0.4937572777271271 - trainLoss: 0.4970398247241974\n",
      "cnt: 0 - valLoss: 0.4937400817871094 - trainLoss: 0.49702486395835876\n",
      "cnt: 0 - valLoss: 0.4937228560447693 - trainLoss: 0.49700987339019775\n",
      "cnt: 0 - valLoss: 0.4937056601047516 - trainLoss: 0.4969949424266815\n",
      "cnt: 0 - valLoss: 0.4936884641647339 - trainLoss: 0.4969800114631653\n",
      "cnt: 0 - valLoss: 0.4936712384223938 - trainLoss: 0.4969650208950043\n",
      "cnt: 0 - valLoss: 0.4936540722846985 - trainLoss: 0.4969501197338104\n",
      "cnt: 0 - valLoss: 0.493636816740036 - trainLoss: 0.4969351589679718\n",
      "cnt: 0 - valLoss: 0.4936196804046631 - trainLoss: 0.49692025780677795\n",
      "cnt: 0 - valLoss: 0.49360254406929016 - trainLoss: 0.4969053268432617\n",
      "cnt: 0 - valLoss: 0.49358534812927246 - trainLoss: 0.4968903660774231\n",
      "cnt: 0 - valLoss: 0.49356815218925476 - trainLoss: 0.49687546491622925\n",
      "cnt: 0 - valLoss: 0.49355098605155945 - trainLoss: 0.496860533952713\n",
      "cnt: 0 - valLoss: 0.49353381991386414 - trainLoss: 0.49684563279151917\n",
      "cnt: 0 - valLoss: 0.4935166835784912 - trainLoss: 0.49683070182800293\n",
      "cnt: 0 - valLoss: 0.4934994876384735 - trainLoss: 0.4968158006668091\n",
      "cnt: 0 - valLoss: 0.4934823513031006 - trainLoss: 0.4968009293079376\n",
      "cnt: 0 - valLoss: 0.49346521496772766 - trainLoss: 0.496785968542099\n",
      "cnt: 0 - valLoss: 0.49344807863235474 - trainLoss: 0.49677109718322754\n",
      "cnt: 0 - valLoss: 0.4934309124946594 - trainLoss: 0.4967561960220337\n",
      "cnt: 0 - valLoss: 0.4934138059616089 - trainLoss: 0.49674132466316223\n",
      "cnt: 0 - valLoss: 0.49339666962623596 - trainLoss: 0.49672645330429077\n",
      "cnt: 0 - valLoss: 0.4933795630931854 - trainLoss: 0.4967115521430969\n",
      "cnt: 0 - valLoss: 0.4933624267578125 - trainLoss: 0.4966966509819031\n",
      "cnt: 0 - valLoss: 0.49334532022476196 - trainLoss: 0.496681809425354\n",
      "cnt: 0 - valLoss: 0.49332818388938904 - trainLoss: 0.49666693806648254\n",
      "cnt: 0 - valLoss: 0.4933110773563385 - trainLoss: 0.4966520667076111\n",
      "cnt: 0 - valLoss: 0.49329403042793274 - trainLoss: 0.4966371953487396\n",
      "cnt: 0 - valLoss: 0.4932768642902374 - trainLoss: 0.49662235379219055\n",
      "cnt: 0 - valLoss: 0.49325981736183167 - trainLoss: 0.4966074526309967\n",
      "cnt: 0 - valLoss: 0.49324271082878113 - trainLoss: 0.49659264087677\n",
      "cnt: 0 - valLoss: 0.4932256042957306 - trainLoss: 0.49657782912254333\n",
      "cnt: 0 - valLoss: 0.49320855736732483 - trainLoss: 0.49656298756599426\n",
      "cnt: 0 - valLoss: 0.4931914806365967 - trainLoss: 0.4965481460094452\n",
      "cnt: 0 - valLoss: 0.4931744337081909 - trainLoss: 0.4965333342552185\n",
      "cnt: 0 - valLoss: 0.4931573271751404 - trainLoss: 0.49651846289634705\n",
      "cnt: 0 - valLoss: 0.4931402802467346 - trainLoss: 0.496503621339798\n",
      "cnt: 0 - valLoss: 0.49312326312065125 - trainLoss: 0.4964888095855713\n",
      "cnt: 0 - valLoss: 0.4931061565876007 - trainLoss: 0.4964739978313446\n",
      "cnt: 0 - valLoss: 0.49308910965919495 - trainLoss: 0.4964591860771179\n",
      "cnt: 0 - valLoss: 0.4930720925331116 - trainLoss: 0.49644434452056885\n",
      "cnt: 0 - valLoss: 0.4930550158023834 - trainLoss: 0.49642956256866455\n",
      "cnt: 0 - valLoss: 0.49303799867630005 - trainLoss: 0.4964147210121155\n",
      "cnt: 0 - valLoss: 0.4930209815502167 - trainLoss: 0.4963999390602112\n",
      "cnt: 0 - valLoss: 0.4930039346218109 - trainLoss: 0.4963851571083069\n",
      "cnt: 0 - valLoss: 0.49298691749572754 - trainLoss: 0.4963703453540802\n",
      "cnt: 0 - valLoss: 0.49296990036964417 - trainLoss: 0.4963556230068207\n",
      "cnt: 0 - valLoss: 0.4929528534412384 - trainLoss: 0.496340811252594\n",
      "cnt: 0 - valLoss: 0.4929358661174774 - trainLoss: 0.4963259994983673\n",
      "cnt: 0 - valLoss: 0.49291881918907166 - trainLoss: 0.4963112771511078\n",
      "cnt: 0 - valLoss: 0.4929018020629883 - trainLoss: 0.4962964951992035\n",
      "cnt: 0 - valLoss: 0.4928847849369049 - trainLoss: 0.4962817430496216\n",
      "cnt: 0 - valLoss: 0.4928677976131439 - trainLoss: 0.49626702070236206\n",
      "cnt: 0 - valLoss: 0.49285078048706055 - trainLoss: 0.4962522089481354\n",
      "cnt: 0 - valLoss: 0.49283382296562195 - trainLoss: 0.49623748660087585\n",
      "cnt: 0 - valLoss: 0.4928168058395386 - trainLoss: 0.49622270464897156\n",
      "cnt: 0 - valLoss: 0.4927998185157776 - trainLoss: 0.49620798230171204\n",
      "cnt: 0 - valLoss: 0.492782860994339 - trainLoss: 0.4961932599544525\n",
      "cnt: 0 - valLoss: 0.4927658438682556 - trainLoss: 0.4961785078048706\n",
      "cnt: 0 - valLoss: 0.4927489161491394 - trainLoss: 0.49616381525993347\n",
      "cnt: 0 - valLoss: 0.4927319288253784 - trainLoss: 0.49614906311035156\n",
      "cnt: 0 - valLoss: 0.4927149713039398 - trainLoss: 0.49613434076309204\n",
      "cnt: 0 - valLoss: 0.49269798398017883 - trainLoss: 0.49611958861351013\n",
      "cnt: 0 - valLoss: 0.49268099665641785 - trainLoss: 0.4961049258708954\n",
      "cnt: 0 - valLoss: 0.49266403913497925 - trainLoss: 0.49609020352363586\n",
      "cnt: 0 - valLoss: 0.4926471412181854 - trainLoss: 0.49607548117637634\n",
      "cnt: 0 - valLoss: 0.4926301836967468 - trainLoss: 0.4960607886314392\n",
      "cnt: 0 - valLoss: 0.492613285779953 - trainLoss: 0.4960460662841797\n",
      "cnt: 0 - valLoss: 0.4925963282585144 - trainLoss: 0.49603137373924255\n",
      "cnt: 0 - valLoss: 0.4925794303417206 - trainLoss: 0.4960167109966278\n",
      "cnt: 0 - valLoss: 0.49256250262260437 - trainLoss: 0.4960020184516907\n",
      "cnt: 0 - valLoss: 0.49254557490348816 - trainLoss: 0.4959873557090759\n",
      "cnt: 0 - valLoss: 0.49252864718437195 - trainLoss: 0.4959726631641388\n",
      "cnt: 0 - valLoss: 0.4925117790699005 - trainLoss: 0.49595797061920166\n",
      "cnt: 0 - valLoss: 0.4924948215484619 - trainLoss: 0.4959433078765869\n",
      "cnt: 0 - valLoss: 0.4924779236316681 - trainLoss: 0.4959286153316498\n",
      "cnt: 0 - valLoss: 0.49246105551719666 - trainLoss: 0.4959139823913574\n",
      "cnt: 0 - valLoss: 0.49244415760040283 - trainLoss: 0.4958992898464203\n",
      "cnt: 0 - valLoss: 0.492427259683609 - trainLoss: 0.49588465690612793\n",
      "cnt: 0 - valLoss: 0.4924103915691376 - trainLoss: 0.4958699941635132\n",
      "cnt: 0 - valLoss: 0.49239352345466614 - trainLoss: 0.49585533142089844\n",
      "cnt: 0 - valLoss: 0.4923766553401947 - trainLoss: 0.4958406984806061\n",
      "cnt: 0 - valLoss: 0.49235981702804565 - trainLoss: 0.4958260655403137\n",
      "cnt: 0 - valLoss: 0.49234291911125183 - trainLoss: 0.49581143260002136\n",
      "cnt: 0 - valLoss: 0.49232611060142517 - trainLoss: 0.495796799659729\n",
      "cnt: 0 - valLoss: 0.49230918288230896 - trainLoss: 0.49578216671943665\n",
      "cnt: 0 - valLoss: 0.4922923445701599 - trainLoss: 0.4957675039768219\n",
      "cnt: 0 - valLoss: 0.49227553606033325 - trainLoss: 0.4957529306411743\n",
      "cnt: 0 - valLoss: 0.49225863814353943 - trainLoss: 0.49573832750320435\n",
      "cnt: 0 - valLoss: 0.49224185943603516 - trainLoss: 0.495723694562912\n",
      "cnt: 0 - valLoss: 0.4922249913215637 - trainLoss: 0.495709091424942\n",
      "cnt: 0 - valLoss: 0.4922081530094147 - trainLoss: 0.49569448828697205\n",
      "cnt: 0 - valLoss: 0.492191344499588 - trainLoss: 0.4956798553466797\n",
      "cnt: 0 - valLoss: 0.49217450618743896 - trainLoss: 0.4956653118133545\n",
      "cnt: 0 - valLoss: 0.4921576976776123 - trainLoss: 0.49565067887306213\n",
      "cnt: 0 - valLoss: 0.49214085936546326 - trainLoss: 0.49563607573509216\n",
      "cnt: 0 - valLoss: 0.4921240508556366 - trainLoss: 0.4956214725971222\n",
      "cnt: 0 - valLoss: 0.4921072721481323 - trainLoss: 0.495606929063797\n",
      "cnt: 0 - valLoss: 0.49209046363830566 - trainLoss: 0.4955923855304718\n",
      "cnt: 0 - valLoss: 0.49207374453544617 - trainLoss: 0.49557772278785706\n",
      "cnt: 0 - valLoss: 0.4920569062232971 - trainLoss: 0.49556317925453186\n",
      "cnt: 0 - valLoss: 0.49204012751579285 - trainLoss: 0.49554863572120667\n",
      "cnt: 0 - valLoss: 0.4920233488082886 - trainLoss: 0.4955340325832367\n",
      "cnt: 0 - valLoss: 0.4920065701007843 - trainLoss: 0.4955194890499115\n",
      "cnt: 0 - valLoss: 0.4919898211956024 - trainLoss: 0.4955049157142639\n",
      "cnt: 0 - valLoss: 0.49197307229042053 - trainLoss: 0.4954903721809387\n",
      "cnt: 0 - valLoss: 0.49195629358291626 - trainLoss: 0.4954758286476135\n",
      "cnt: 0 - valLoss: 0.491939514875412 - trainLoss: 0.4954613149166107\n",
      "cnt: 0 - valLoss: 0.4919227957725525 - trainLoss: 0.49544671177864075\n",
      "cnt: 0 - valLoss: 0.4919060170650482 - trainLoss: 0.49543219804763794\n",
      "cnt: 0 - valLoss: 0.49188926815986633 - trainLoss: 0.49541765451431274\n",
      "cnt: 0 - valLoss: 0.49187254905700684 - trainLoss: 0.49540314078330994\n",
      "cnt: 0 - valLoss: 0.49185580015182495 - trainLoss: 0.49538859724998474\n",
      "cnt: 0 - valLoss: 0.49183911085128784 - trainLoss: 0.49537408351898193\n",
      "cnt: 0 - valLoss: 0.49182233214378357 - trainLoss: 0.4953596293926239\n",
      "cnt: 0 - valLoss: 0.4918056130409241 - trainLoss: 0.4953450560569763\n",
      "cnt: 0 - valLoss: 0.49178892374038696 - trainLoss: 0.4953305423259735\n",
      "cnt: 0 - valLoss: 0.4917721748352051 - trainLoss: 0.4953160583972931\n",
      "cnt: 0 - valLoss: 0.49175548553466797 - trainLoss: 0.49530157446861267\n",
      "cnt: 0 - valLoss: 0.49173882603645325 - trainLoss: 0.4952870309352875\n",
      "cnt: 0 - valLoss: 0.49172207713127136 - trainLoss: 0.49527254700660706\n",
      "cnt: 0 - valLoss: 0.49170538783073425 - trainLoss: 0.49525803327560425\n",
      "cnt: 0 - valLoss: 0.49168866872787476 - trainLoss: 0.49524354934692383\n",
      "cnt: 0 - valLoss: 0.49167200922966003 - trainLoss: 0.4952290654182434\n",
      "cnt: 0 - valLoss: 0.4916553199291229 - trainLoss: 0.4952146112918854\n",
      "cnt: 0 - valLoss: 0.4916386902332306 - trainLoss: 0.49520012736320496\n",
      "cnt: 0 - valLoss: 0.4916219711303711 - trainLoss: 0.4951856732368469\n",
      "cnt: 0 - valLoss: 0.49160531163215637 - trainLoss: 0.4951711595058441\n",
      "cnt: 0 - valLoss: 0.49158868193626404 - trainLoss: 0.4951567053794861\n",
      "cnt: 0 - valLoss: 0.49157199263572693 - trainLoss: 0.49514228105545044\n",
      "cnt: 0 - valLoss: 0.4915553331375122 - trainLoss: 0.49512779712677\n",
      "cnt: 0 - valLoss: 0.4915387034416199 - trainLoss: 0.495113343000412\n",
      "cnt: 0 - valLoss: 0.49152207374572754 - trainLoss: 0.49509885907173157\n",
      "cnt: 0 - valLoss: 0.4915054440498352 - trainLoss: 0.49508440494537354\n",
      "cnt: 0 - valLoss: 0.4914887547492981 - trainLoss: 0.4950699210166931\n",
      "cnt: 0 - valLoss: 0.49147212505340576 - trainLoss: 0.49505555629730225\n",
      "cnt: 0 - valLoss: 0.4914554953575134 - trainLoss: 0.4950410723686218\n",
      "cnt: 0 - valLoss: 0.49143892526626587 - trainLoss: 0.4950266480445862\n",
      "cnt: 0 - valLoss: 0.49142229557037354 - trainLoss: 0.49501222372055054\n",
      "cnt: 0 - valLoss: 0.4914056658744812 - trainLoss: 0.4949977993965149\n",
      "cnt: 0 - valLoss: 0.49138906598091125 - trainLoss: 0.49498337507247925\n",
      "cnt: 0 - valLoss: 0.4913724958896637 - trainLoss: 0.4949689507484436\n",
      "cnt: 0 - valLoss: 0.49135586619377136 - trainLoss: 0.49495452642440796\n",
      "cnt: 0 - valLoss: 0.4913392663002014 - trainLoss: 0.4949401617050171\n",
      "cnt: 0 - valLoss: 0.49132266640663147 - trainLoss: 0.49492573738098145\n",
      "cnt: 0 - valLoss: 0.4913060963153839 - trainLoss: 0.4949113428592682\n",
      "cnt: 0 - valLoss: 0.49128949642181396 - trainLoss: 0.49489691853523254\n",
      "cnt: 0 - valLoss: 0.4912729263305664 - trainLoss: 0.4948825538158417\n",
      "cnt: 0 - valLoss: 0.49125635623931885 - trainLoss: 0.4948681592941284\n",
      "cnt: 0 - valLoss: 0.4912397861480713 - trainLoss: 0.49485379457473755\n",
      "cnt: 0 - valLoss: 0.4912232458591461 - trainLoss: 0.4948394000530243\n",
      "cnt: 0 - valLoss: 0.49120670557022095 - trainLoss: 0.4948250353336334\n",
      "cnt: 0 - valLoss: 0.491190105676651 - trainLoss: 0.49481067061424255\n",
      "cnt: 0 - valLoss: 0.49117356538772583 - trainLoss: 0.4947962462902069\n",
      "cnt: 0 - valLoss: 0.49115702509880066 - trainLoss: 0.4947819709777832\n",
      "cnt: 0 - valLoss: 0.4911404848098755 - trainLoss: 0.49476757645606995\n",
      "cnt: 0 - valLoss: 0.49112391471862793 - trainLoss: 0.4947532117366791\n",
      "cnt: 0 - valLoss: 0.49110737442970276 - trainLoss: 0.4947388172149658\n",
      "cnt: 0 - valLoss: 0.4910908341407776 - trainLoss: 0.49472448229789734\n",
      "cnt: 0 - valLoss: 0.4910743832588196 - trainLoss: 0.49471014738082886\n",
      "cnt: 0 - valLoss: 0.4910578429698944 - trainLoss: 0.4946958124637604\n",
      "cnt: 0 - valLoss: 0.4910413324832916 - trainLoss: 0.4946814775466919\n",
      "cnt: 0 - valLoss: 0.49102479219436646 - trainLoss: 0.4946671426296234\n",
      "cnt: 0 - valLoss: 0.49100828170776367 - trainLoss: 0.49465277791023254\n",
      "cnt: 0 - valLoss: 0.49099183082580566 - trainLoss: 0.49463850259780884\n",
      "cnt: 0 - valLoss: 0.4909752905368805 - trainLoss: 0.4946241080760956\n",
      "cnt: 0 - valLoss: 0.4909587800502777 - trainLoss: 0.4946098029613495\n",
      "cnt: 0 - valLoss: 0.4909423291683197 - trainLoss: 0.494595468044281\n",
      "cnt: 0 - valLoss: 0.4909258186817169 - trainLoss: 0.4945811927318573\n",
      "cnt: 0 - valLoss: 0.49090930819511414 - trainLoss: 0.4945668578147888\n",
      "cnt: 0 - valLoss: 0.49089285731315613 - trainLoss: 0.4945525527000427\n",
      "cnt: 0 - valLoss: 0.4908764064311981 - trainLoss: 0.49453824758529663\n",
      "cnt: 0 - valLoss: 0.4908599555492401 - trainLoss: 0.49452394247055054\n",
      "cnt: 0 - valLoss: 0.49084344506263733 - trainLoss: 0.49450966715812683\n",
      "cnt: 0 - valLoss: 0.4908269941806793 - trainLoss: 0.4944954216480255\n",
      "cnt: 0 - valLoss: 0.4908105134963989 - trainLoss: 0.49448105692863464\n",
      "cnt: 0 - valLoss: 0.4907940924167633 - trainLoss: 0.4944668710231781\n",
      "cnt: 0 - valLoss: 0.4907777011394501 - trainLoss: 0.494452565908432\n",
      "cnt: 0 - valLoss: 0.4907612204551697 - trainLoss: 0.4944382607936859\n",
      "cnt: 0 - valLoss: 0.49074476957321167 - trainLoss: 0.4944239854812622\n",
      "cnt: 0 - valLoss: 0.49072834849357605 - trainLoss: 0.4944097101688385\n",
      "cnt: 0 - valLoss: 0.49071192741394043 - trainLoss: 0.4943954646587372\n",
      "cnt: 0 - valLoss: 0.4906955361366272 - trainLoss: 0.49438121914863586\n",
      "cnt: 0 - valLoss: 0.4906791150569916 - trainLoss: 0.49436691403388977\n",
      "cnt: 0 - valLoss: 0.49066269397735596 - trainLoss: 0.49435266852378845\n",
      "cnt: 0 - valLoss: 0.4906463325023651 - trainLoss: 0.4943384528160095\n",
      "cnt: 0 - valLoss: 0.4906299114227295 - trainLoss: 0.4943241477012634\n",
      "cnt: 0 - valLoss: 0.49061349034309387 - trainLoss: 0.4943099617958069\n",
      "cnt: 0 - valLoss: 0.49059706926345825 - trainLoss: 0.4942956864833832\n",
      "cnt: 0 - valLoss: 0.4905807077884674 - trainLoss: 0.49428147077560425\n",
      "cnt: 0 - valLoss: 0.4905642867088318 - trainLoss: 0.49426722526550293\n",
      "cnt: 0 - valLoss: 0.49054795503616333 - trainLoss: 0.494253009557724\n",
      "cnt: 0 - valLoss: 0.4905315637588501 - trainLoss: 0.4942387640476227\n",
      "cnt: 0 - valLoss: 0.49051520228385925 - trainLoss: 0.4942246079444885\n",
      "cnt: 0 - valLoss: 0.4904988408088684 - trainLoss: 0.4942103624343872\n",
      "cnt: 0 - valLoss: 0.4904824495315552 - trainLoss: 0.4941961467266083\n",
      "cnt: 0 - valLoss: 0.49046608805656433 - trainLoss: 0.49418196082115173\n",
      "cnt: 0 - valLoss: 0.4904497265815735 - trainLoss: 0.4941677749156952\n",
      "cnt: 0 - valLoss: 0.4904334247112274 - trainLoss: 0.49415355920791626\n",
      "cnt: 0 - valLoss: 0.4904170632362366 - trainLoss: 0.49413934350013733\n",
      "cnt: 0 - valLoss: 0.4904007315635681 - trainLoss: 0.49412521719932556\n",
      "cnt: 0 - valLoss: 0.49038439989089966 - trainLoss: 0.49411100149154663\n",
      "cnt: 0 - valLoss: 0.4903680086135864 - trainLoss: 0.4940968453884125\n",
      "cnt: 0 - valLoss: 0.49035173654556274 - trainLoss: 0.49408265948295593\n",
      "cnt: 0 - valLoss: 0.4903353452682495 - trainLoss: 0.4940684735774994\n",
      "cnt: 0 - valLoss: 0.49031907320022583 - trainLoss: 0.49405428767204285\n",
      "cnt: 0 - valLoss: 0.49030277132987976 - trainLoss: 0.4940401613712311\n",
      "cnt: 0 - valLoss: 0.4902864396572113 - trainLoss: 0.4940260052680969\n",
      "cnt: 0 - valLoss: 0.49027010798454285 - trainLoss: 0.49401184916496277\n",
      "cnt: 0 - valLoss: 0.49025386571884155 - trainLoss: 0.493997722864151\n",
      "cnt: 0 - valLoss: 0.4902375042438507 - trainLoss: 0.49398353695869446\n",
      "cnt: 0 - valLoss: 0.49022120237350464 - trainLoss: 0.4939693212509155\n",
      "cnt: 0 - valLoss: 0.49020490050315857 - trainLoss: 0.49395519495010376\n",
      "cnt: 0 - valLoss: 0.4901885986328125 - trainLoss: 0.4939410388469696\n",
      "cnt: 0 - valLoss: 0.49017226696014404 - trainLoss: 0.4939268231391907\n",
      "cnt: 0 - valLoss: 0.49015599489212036 - trainLoss: 0.4939126968383789\n",
      "cnt: 0 - valLoss: 0.4901396930217743 - trainLoss: 0.49389854073524475\n",
      "cnt: 0 - valLoss: 0.4901233911514282 - trainLoss: 0.493884414434433\n",
      "cnt: 0 - valLoss: 0.49010708928108215 - trainLoss: 0.49387019872665405\n",
      "cnt: 0 - valLoss: 0.49009081721305847 - trainLoss: 0.4938560724258423\n",
      "cnt: 0 - valLoss: 0.4900745451450348 - trainLoss: 0.4938419461250305\n",
      "cnt: 0 - valLoss: 0.4900583028793335 - trainLoss: 0.49382781982421875\n",
      "cnt: 0 - valLoss: 0.4900420010089874 - trainLoss: 0.493813693523407\n",
      "cnt: 0 - valLoss: 0.49002572894096375 - trainLoss: 0.4937995672225952\n",
      "cnt: 0 - valLoss: 0.49000948667526245 - trainLoss: 0.49378547072410583\n",
      "cnt: 0 - valLoss: 0.48999324440956116 - trainLoss: 0.49377134442329407\n",
      "cnt: 0 - valLoss: 0.48997700214385986 - trainLoss: 0.4937572181224823\n",
      "cnt: 0 - valLoss: 0.48996075987815857 - trainLoss: 0.49374303221702576\n",
      "cnt: 0 - valLoss: 0.4899444580078125 - trainLoss: 0.49372896552085876\n",
      "cnt: 0 - valLoss: 0.4899282455444336 - trainLoss: 0.493714839220047\n",
      "cnt: 0 - valLoss: 0.4899120628833771 - trainLoss: 0.49370071291923523\n",
      "cnt: 0 - valLoss: 0.489895761013031 - trainLoss: 0.4936866760253906\n",
      "cnt: 0 - valLoss: 0.4898795783519745 - trainLoss: 0.49367254972457886\n",
      "cnt: 0 - valLoss: 0.4898633658885956 - trainLoss: 0.4936584532260895\n",
      "cnt: 0 - valLoss: 0.4898471236228943 - trainLoss: 0.4936443865299225\n",
      "cnt: 0 - valLoss: 0.4898309111595154 - trainLoss: 0.4936302602291107\n",
      "cnt: 0 - valLoss: 0.48981475830078125 - trainLoss: 0.4936162233352661\n",
      "cnt: 0 - valLoss: 0.48979851603507996 - trainLoss: 0.49360212683677673\n",
      "cnt: 0 - valLoss: 0.48978230357170105 - trainLoss: 0.49358806014060974\n",
      "cnt: 0 - valLoss: 0.48976612091064453 - trainLoss: 0.49357402324676514\n",
      "cnt: 0 - valLoss: 0.4897499084472656 - trainLoss: 0.49355989694595337\n",
      "cnt: 0 - valLoss: 0.4897337555885315 - trainLoss: 0.49354586005210876\n",
      "cnt: 0 - valLoss: 0.4897175133228302 - trainLoss: 0.49353182315826416\n",
      "cnt: 0 - valLoss: 0.48970136046409607 - trainLoss: 0.49351778626441956\n",
      "cnt: 0 - valLoss: 0.48968514800071716 - trainLoss: 0.49350374937057495\n",
      "cnt: 0 - valLoss: 0.48966899514198303 - trainLoss: 0.49348968267440796\n",
      "cnt: 0 - valLoss: 0.4896528124809265 - trainLoss: 0.49347564578056335\n",
      "cnt: 0 - valLoss: 0.48963662981987 - trainLoss: 0.49346160888671875\n",
      "cnt: 0 - valLoss: 0.48962047696113586 - trainLoss: 0.49344757199287415\n",
      "cnt: 0 - valLoss: 0.48960429430007935 - trainLoss: 0.4934335947036743\n",
      "cnt: 0 - valLoss: 0.4895881116390228 - trainLoss: 0.4934195578098297\n",
      "cnt: 0 - valLoss: 0.4895718991756439 - trainLoss: 0.49340546131134033\n",
      "cnt: 0 - valLoss: 0.4895557761192322 - trainLoss: 0.4933914244174957\n",
      "cnt: 0 - valLoss: 0.48953959345817566 - trainLoss: 0.49337735772132874\n",
      "cnt: 0 - valLoss: 0.48952344059944153 - trainLoss: 0.4933633506298065\n",
      "cnt: 0 - valLoss: 0.4895072281360626 - trainLoss: 0.49334925413131714\n",
      "cnt: 0 - valLoss: 0.4894910752773285 - trainLoss: 0.49333521723747253\n",
      "cnt: 0 - valLoss: 0.489474892616272 - trainLoss: 0.49332118034362793\n",
      "cnt: 0 - valLoss: 0.48945873975753784 - trainLoss: 0.4933071434497833\n",
      "cnt: 0 - valLoss: 0.4894425868988037 - trainLoss: 0.4932931065559387\n",
      "cnt: 0 - valLoss: 0.4894264340400696 - trainLoss: 0.4932790696620941\n",
      "cnt: 0 - valLoss: 0.48941031098365784 - trainLoss: 0.4932650625705719\n",
      "cnt: 0 - valLoss: 0.4893941879272461 - trainLoss: 0.4932510256767273\n",
      "cnt: 0 - valLoss: 0.48937803506851196 - trainLoss: 0.4932369887828827\n",
      "cnt: 0 - valLoss: 0.4893619120121002 - trainLoss: 0.49322301149368286\n",
      "cnt: 0 - valLoss: 0.48934581875801086 - trainLoss: 0.49320900440216064\n",
      "cnt: 0 - valLoss: 0.4893296957015991 - trainLoss: 0.49319496750831604\n",
      "cnt: 0 - valLoss: 0.4893135726451874 - trainLoss: 0.4931810200214386\n",
      "cnt: 0 - valLoss: 0.4892975091934204 - trainLoss: 0.4931670129299164\n",
      "cnt: 0 - valLoss: 0.48928138613700867 - trainLoss: 0.49315306544303894\n",
      "cnt: 0 - valLoss: 0.4892653226852417 - trainLoss: 0.4931390583515167\n",
      "cnt: 0 - valLoss: 0.48924922943115234 - trainLoss: 0.4931251108646393\n",
      "cnt: 0 - valLoss: 0.48923319578170776 - trainLoss: 0.49311116337776184\n",
      "cnt: 0 - valLoss: 0.4892171025276184 - trainLoss: 0.4930972158908844\n",
      "cnt: 0 - valLoss: 0.48920100927352905 - trainLoss: 0.49308323860168457\n",
      "cnt: 0 - valLoss: 0.48918503522872925 - trainLoss: 0.49306929111480713\n",
      "cnt: 0 - valLoss: 0.4891689121723175 - trainLoss: 0.4930553436279297\n",
      "cnt: 0 - valLoss: 0.4891528785228729 - trainLoss: 0.49304139614105225\n",
      "cnt: 0 - valLoss: 0.48913681507110596 - trainLoss: 0.4930274784564972\n",
      "cnt: 0 - valLoss: 0.48912084102630615 - trainLoss: 0.49301356077194214\n",
      "cnt: 0 - valLoss: 0.4891047477722168 - trainLoss: 0.4929996132850647\n",
      "cnt: 0 - valLoss: 0.4890887141227722 - trainLoss: 0.49298569560050964\n",
      "cnt: 0 - valLoss: 0.4890727400779724 - trainLoss: 0.492971807718277\n",
      "cnt: 0 - valLoss: 0.489056795835495 - trainLoss: 0.4929578900337219\n",
      "cnt: 0 - valLoss: 0.4890407621860504 - trainLoss: 0.49294397234916687\n",
      "cnt: 0 - valLoss: 0.489024817943573 - trainLoss: 0.4929300546646118\n",
      "cnt: 0 - valLoss: 0.4890088140964508 - trainLoss: 0.49291616678237915\n",
      "cnt: 0 - valLoss: 0.488992840051651 - trainLoss: 0.4929022490978241\n",
      "cnt: 0 - valLoss: 0.4889768958091736 - trainLoss: 0.4928883910179138\n",
      "cnt: 0 - valLoss: 0.48896095156669617 - trainLoss: 0.49287447333335876\n",
      "cnt: 0 - valLoss: 0.48894497752189636 - trainLoss: 0.4928605854511261\n",
      "cnt: 0 - valLoss: 0.48892906308174133 - trainLoss: 0.49284663796424866\n",
      "cnt: 0 - valLoss: 0.48891308903694153 - trainLoss: 0.49283280968666077\n",
      "cnt: 0 - valLoss: 0.48889708518981934 - trainLoss: 0.4928189516067505\n",
      "cnt: 0 - valLoss: 0.4888811409473419 - trainLoss: 0.4928050935268402\n",
      "cnt: 0 - valLoss: 0.4888652563095093 - trainLoss: 0.49279117584228516\n",
      "cnt: 0 - valLoss: 0.4888492822647095 - trainLoss: 0.4927773177623749\n",
      "cnt: 0 - valLoss: 0.48883339762687683 - trainLoss: 0.492763489484787\n",
      "cnt: 0 - valLoss: 0.4888174533843994 - trainLoss: 0.4927496016025543\n",
      "cnt: 0 - valLoss: 0.4888015389442444 - trainLoss: 0.49273571372032166\n",
      "cnt: 0 - valLoss: 0.48878565430641174 - trainLoss: 0.49272188544273376\n",
      "cnt: 0 - valLoss: 0.4887697398662567 - trainLoss: 0.4927080571651459\n",
      "cnt: 0 - valLoss: 0.4887538552284241 - trainLoss: 0.4926941990852356\n",
      "cnt: 0 - valLoss: 0.48873791098594666 - trainLoss: 0.4926803708076477\n",
      "cnt: 0 - valLoss: 0.4887219965457916 - trainLoss: 0.4926665425300598\n",
      "cnt: 0 - valLoss: 0.4887060821056366 - trainLoss: 0.49265265464782715\n",
      "cnt: 0 - valLoss: 0.48869022727012634 - trainLoss: 0.49263888597488403\n",
      "cnt: 0 - valLoss: 0.4886743426322937 - trainLoss: 0.49262502789497375\n",
      "cnt: 0 - valLoss: 0.48865848779678345 - trainLoss: 0.49261122941970825\n",
      "cnt: 0 - valLoss: 0.4886426031589508 - trainLoss: 0.492597371339798\n",
      "cnt: 0 - valLoss: 0.48862671852111816 - trainLoss: 0.49258357286453247\n",
      "cnt: 0 - valLoss: 0.4886108636856079 - trainLoss: 0.49256980419158936\n",
      "cnt: 0 - valLoss: 0.48859503865242004 - trainLoss: 0.49255603551864624\n",
      "cnt: 0 - valLoss: 0.4885791838169098 - trainLoss: 0.4925421476364136\n",
      "cnt: 0 - valLoss: 0.48856332898139954 - trainLoss: 0.49252837896347046\n",
      "cnt: 0 - valLoss: 0.4885474741458893 - trainLoss: 0.49251455068588257\n",
      "cnt: 0 - valLoss: 0.48853161931037903 - trainLoss: 0.49250081181526184\n",
      "cnt: 0 - valLoss: 0.48851579427719116 - trainLoss: 0.49248698353767395\n",
      "cnt: 0 - valLoss: 0.4884999394416809 - trainLoss: 0.49247321486473083\n",
      "cnt: 0 - valLoss: 0.48848411440849304 - trainLoss: 0.4924594759941101\n",
      "cnt: 0 - valLoss: 0.4884682893753052 - trainLoss: 0.4924456477165222\n",
      "cnt: 0 - valLoss: 0.4884524345397949 - trainLoss: 0.4924319088459015\n",
      "cnt: 0 - valLoss: 0.48843666911125183 - trainLoss: 0.492418110370636\n",
      "cnt: 0 - valLoss: 0.48842084407806396 - trainLoss: 0.49240434169769287\n",
      "cnt: 0 - valLoss: 0.4884050488471985 - trainLoss: 0.49239060282707214\n",
      "cnt: 0 - valLoss: 0.488389253616333 - trainLoss: 0.4923768639564514\n",
      "cnt: 0 - valLoss: 0.48837345838546753 - trainLoss: 0.4923630654811859\n",
      "cnt: 0 - valLoss: 0.48835763335227966 - trainLoss: 0.4923492968082428\n",
      "cnt: 0 - valLoss: 0.4883418679237366 - trainLoss: 0.49233558773994446\n",
      "cnt: 0 - valLoss: 0.4883260726928711 - trainLoss: 0.49232181906700134\n",
      "cnt: 0 - valLoss: 0.488310307264328 - trainLoss: 0.492308109998703\n",
      "cnt: 0 - valLoss: 0.4882945418357849 - trainLoss: 0.4922943413257599\n",
      "cnt: 0 - valLoss: 0.4882787764072418 - trainLoss: 0.49228063225746155\n",
      "cnt: 0 - valLoss: 0.48826298117637634 - trainLoss: 0.4922668933868408\n",
      "cnt: 0 - valLoss: 0.48824721574783325 - trainLoss: 0.49225321412086487\n",
      "cnt: 0 - valLoss: 0.48823148012161255 - trainLoss: 0.49223947525024414\n",
      "cnt: 0 - valLoss: 0.48821571469306946 - trainLoss: 0.4922257661819458\n",
      "cnt: 0 - valLoss: 0.48819997906684875 - trainLoss: 0.49221205711364746\n",
      "cnt: 0 - valLoss: 0.48818421363830566 - trainLoss: 0.49219828844070435\n",
      "cnt: 0 - valLoss: 0.48816847801208496 - trainLoss: 0.492184579372406\n",
      "cnt: 0 - valLoss: 0.48815274238586426 - trainLoss: 0.49217090010643005\n",
      "cnt: 0 - valLoss: 0.48813706636428833 - trainLoss: 0.4921571910381317\n",
      "cnt: 0 - valLoss: 0.48812130093574524 - trainLoss: 0.49214351177215576\n",
      "cnt: 0 - valLoss: 0.48810550570487976 - trainLoss: 0.4921298027038574\n",
      "cnt: 0 - valLoss: 0.48808982968330383 - trainLoss: 0.49211615324020386\n",
      "cnt: 0 - valLoss: 0.48807409405708313 - trainLoss: 0.4921025037765503\n",
      "cnt: 0 - valLoss: 0.4880583584308624 - trainLoss: 0.49208879470825195\n",
      "cnt: 0 - valLoss: 0.4880426824092865 - trainLoss: 0.4920751452445984\n",
      "cnt: 0 - valLoss: 0.48802700638771057 - trainLoss: 0.4920614957809448\n",
      "cnt: 0 - valLoss: 0.48801127076148987 - trainLoss: 0.4920477867126465\n",
      "cnt: 0 - valLoss: 0.48799553513526917 - trainLoss: 0.49203410744667053\n",
      "cnt: 0 - valLoss: 0.48797985911369324 - trainLoss: 0.49202048778533936\n",
      "cnt: 0 - valLoss: 0.4879641532897949 - trainLoss: 0.492006778717041\n",
      "cnt: 0 - valLoss: 0.487948477268219 - trainLoss: 0.4919931888580322\n",
      "cnt: 0 - valLoss: 0.4879327714443207 - trainLoss: 0.49197956919670105\n",
      "cnt: 0 - valLoss: 0.48791709542274475 - trainLoss: 0.4919659197330475\n",
      "cnt: 0 - valLoss: 0.4879014194011688 - trainLoss: 0.4919522702693939\n",
      "cnt: 0 - valLoss: 0.4878857731819153 - trainLoss: 0.49193865060806274\n",
      "cnt: 0 - valLoss: 0.48787006735801697 - trainLoss: 0.49192503094673157\n",
      "cnt: 0 - valLoss: 0.4878544211387634 - trainLoss: 0.4919114410877228\n",
      "cnt: 0 - valLoss: 0.4878387451171875 - trainLoss: 0.4918977618217468\n",
      "cnt: 0 - valLoss: 0.48782309889793396 - trainLoss: 0.49188417196273804\n",
      "cnt: 0 - valLoss: 0.4878074526786804 - trainLoss: 0.49187055230140686\n",
      "cnt: 0 - valLoss: 0.4877918064594269 - trainLoss: 0.4918569326400757\n",
      "cnt: 0 - valLoss: 0.48777616024017334 - trainLoss: 0.4918433427810669\n",
      "cnt: 0 - valLoss: 0.4877605140209198 - trainLoss: 0.4918297231197357\n",
      "cnt: 0 - valLoss: 0.48774486780166626 - trainLoss: 0.4918161630630493\n",
      "cnt: 0 - valLoss: 0.4877292811870575 - trainLoss: 0.49180254340171814\n",
      "cnt: 0 - valLoss: 0.48771366477012634 - trainLoss: 0.49178898334503174\n",
      "cnt: 0 - valLoss: 0.4876979887485504 - trainLoss: 0.49177536368370056\n",
      "cnt: 0 - valLoss: 0.48768237233161926 - trainLoss: 0.49176180362701416\n",
      "cnt: 0 - valLoss: 0.4876667857170105 - trainLoss: 0.491748183965683\n",
      "cnt: 0 - valLoss: 0.48765116930007935 - trainLoss: 0.49173465371131897\n",
      "cnt: 0 - valLoss: 0.4876355826854706 - trainLoss: 0.4917210340499878\n",
      "cnt: 0 - valLoss: 0.48761996626853943 - trainLoss: 0.4917074739933014\n",
      "cnt: 0 - valLoss: 0.48760440945625305 - trainLoss: 0.491693913936615\n",
      "cnt: 0 - valLoss: 0.4875888228416443 - trainLoss: 0.491680383682251\n",
      "cnt: 0 - valLoss: 0.4875732660293579 - trainLoss: 0.4916668236255646\n",
      "cnt: 0 - valLoss: 0.48755764961242676 - trainLoss: 0.4916532635688782\n",
      "cnt: 0 - valLoss: 0.487542062997818 - trainLoss: 0.49163973331451416\n",
      "cnt: 0 - valLoss: 0.487526535987854 - trainLoss: 0.49162620306015015\n",
      "cnt: 0 - valLoss: 0.48751094937324524 - trainLoss: 0.49161264300346375\n",
      "cnt: 0 - valLoss: 0.48749539256095886 - trainLoss: 0.49159905314445496\n",
      "cnt: 0 - valLoss: 0.4874798059463501 - trainLoss: 0.49158552289009094\n",
      "cnt: 0 - valLoss: 0.4874643087387085 - trainLoss: 0.4915720224380493\n",
      "cnt: 0 - valLoss: 0.48744869232177734 - trainLoss: 0.4915584921836853\n",
      "cnt: 0 - valLoss: 0.48743319511413574 - trainLoss: 0.4915449619293213\n",
      "cnt: 0 - valLoss: 0.48741763830184937 - trainLoss: 0.4915314316749573\n",
      "cnt: 0 - valLoss: 0.4874021112918854 - trainLoss: 0.49151790142059326\n",
      "cnt: 0 - valLoss: 0.487386554479599 - trainLoss: 0.491504430770874\n",
      "cnt: 0 - valLoss: 0.487371027469635 - trainLoss: 0.4914909601211548\n",
      "cnt: 0 - valLoss: 0.4873555302619934 - trainLoss: 0.49147742986679077\n",
      "cnt: 0 - valLoss: 0.4873400032520294 - trainLoss: 0.49146389961242676\n",
      "cnt: 0 - valLoss: 0.48732447624206543 - trainLoss: 0.49145039916038513\n",
      "cnt: 0 - valLoss: 0.48730897903442383 - trainLoss: 0.4914369285106659\n",
      "cnt: 0 - valLoss: 0.48729345202445984 - trainLoss: 0.4914233982563019\n",
      "cnt: 0 - valLoss: 0.4872779846191406 - trainLoss: 0.49140992760658264\n",
      "cnt: 0 - valLoss: 0.48726245760917664 - trainLoss: 0.491396427154541\n",
      "cnt: 0 - valLoss: 0.4872469902038574 - trainLoss: 0.4913829565048218\n",
      "cnt: 0 - valLoss: 0.4872315227985382 - trainLoss: 0.4913695156574249\n",
      "cnt: 0 - valLoss: 0.487216055393219 - trainLoss: 0.4913560450077057\n",
      "cnt: 0 - valLoss: 0.487200528383255 - trainLoss: 0.49134260416030884\n",
      "cnt: 0 - valLoss: 0.4871850907802582 - trainLoss: 0.4913291037082672\n",
      "cnt: 0 - valLoss: 0.48716962337493896 - trainLoss: 0.491315633058548\n",
      "cnt: 0 - valLoss: 0.48715415596961975 - trainLoss: 0.4913021922111511\n",
      "cnt: 0 - valLoss: 0.4871387183666229 - trainLoss: 0.4912887513637543\n",
      "cnt: 0 - valLoss: 0.4871232211589813 - trainLoss: 0.49127528071403503\n",
      "cnt: 0 - valLoss: 0.4871078133583069 - trainLoss: 0.4912618398666382\n",
      "cnt: 0 - valLoss: 0.48709234595298767 - trainLoss: 0.4912484288215637\n",
      "cnt: 0 - valLoss: 0.48707690834999084 - trainLoss: 0.49123498797416687\n",
      "cnt: 0 - valLoss: 0.4870615005493164 - trainLoss: 0.49122151732444763\n",
      "cnt: 0 - valLoss: 0.4870460629463196 - trainLoss: 0.49120810627937317\n",
      "cnt: 0 - valLoss: 0.4870305359363556 - trainLoss: 0.4911946654319763\n",
      "cnt: 0 - valLoss: 0.48701512813568115 - trainLoss: 0.49118128418922424\n",
      "cnt: 0 - valLoss: 0.48699960112571716 - trainLoss: 0.4911678731441498\n",
      "cnt: 0 - valLoss: 0.48698416352272034 - trainLoss: 0.49115443229675293\n",
      "cnt: 0 - valLoss: 0.4869687259197235 - trainLoss: 0.49114105105400085\n",
      "cnt: 0 - valLoss: 0.4869532585144043 - trainLoss: 0.4911276400089264\n",
      "cnt: 0 - valLoss: 0.48693782091140747 - trainLoss: 0.49111419916152954\n",
      "cnt: 0 - valLoss: 0.48692241311073303 - trainLoss: 0.49110084772109985\n",
      "cnt: 0 - valLoss: 0.48690691590309143 - trainLoss: 0.4910874664783478\n",
      "cnt: 0 - valLoss: 0.4868915379047394 - trainLoss: 0.4910740554332733\n",
      "cnt: 0 - valLoss: 0.48687607049942017 - trainLoss: 0.49106070399284363\n",
      "cnt: 0 - valLoss: 0.4868607223033905 - trainLoss: 0.4910472631454468\n",
      "cnt: 0 - valLoss: 0.4868452548980713 - trainLoss: 0.4910339117050171\n",
      "cnt: 0 - valLoss: 0.48682981729507446 - trainLoss: 0.4910205602645874\n",
      "cnt: 0 - valLoss: 0.4868144392967224 - trainLoss: 0.4910072088241577\n",
      "cnt: 0 - valLoss: 0.4867990016937256 - trainLoss: 0.490993857383728\n",
      "cnt: 0 - valLoss: 0.48678362369537354 - trainLoss: 0.49098044633865356\n",
      "cnt: 0 - valLoss: 0.4867682158946991 - trainLoss: 0.4909670948982239\n",
      "cnt: 0 - valLoss: 0.48675283789634705 - trainLoss: 0.4909537434577942\n",
      "cnt: 0 - valLoss: 0.486737459897995 - trainLoss: 0.4909403324127197\n",
      "cnt: 0 - valLoss: 0.48672202229499817 - trainLoss: 0.4909270703792572\n",
      "cnt: 0 - valLoss: 0.4867067337036133 - trainLoss: 0.4909137189388275\n",
      "cnt: 0 - valLoss: 0.48669132590293884 - trainLoss: 0.4909003674983978\n",
      "cnt: 0 - valLoss: 0.4866759479045868 - trainLoss: 0.49088701605796814\n",
      "cnt: 0 - valLoss: 0.48666056990623474 - trainLoss: 0.4908737540245056\n",
      "cnt: 0 - valLoss: 0.4866452217102051 - trainLoss: 0.4908604025840759\n",
      "cnt: 0 - valLoss: 0.4866298735141754 - trainLoss: 0.49084705114364624\n",
      "cnt: 0 - valLoss: 0.48661452531814575 - trainLoss: 0.49083372950553894\n",
      "cnt: 0 - valLoss: 0.4865991771221161 - trainLoss: 0.49082040786743164\n",
      "cnt: 0 - valLoss: 0.4865838289260864 - trainLoss: 0.4908071458339691\n",
      "cnt: 0 - valLoss: 0.48656848073005676 - trainLoss: 0.49079379439353943\n",
      "cnt: 0 - valLoss: 0.4865531623363495 - trainLoss: 0.49078047275543213\n",
      "cnt: 0 - valLoss: 0.4865378439426422 - trainLoss: 0.4907671809196472\n",
      "cnt: 0 - valLoss: 0.48652249574661255 - trainLoss: 0.4907538890838623\n",
      "cnt: 0 - valLoss: 0.48650720715522766 - trainLoss: 0.4907406270503998\n",
      "cnt: 0 - valLoss: 0.486491858959198 - trainLoss: 0.49072733521461487\n",
      "cnt: 0 - valLoss: 0.4864766001701355 - trainLoss: 0.49071401357650757\n",
      "cnt: 0 - valLoss: 0.48646125197410583 - trainLoss: 0.49070075154304504\n",
      "cnt: 0 - valLoss: 0.48644596338272095 - trainLoss: 0.49068745970726013\n",
      "cnt: 0 - valLoss: 0.48643064498901367 - trainLoss: 0.4906741678714752\n",
      "cnt: 0 - valLoss: 0.48641538619995117 - trainLoss: 0.4906609356403351\n",
      "cnt: 0 - valLoss: 0.48640012741088867 - trainLoss: 0.4906476140022278\n",
      "cnt: 0 - valLoss: 0.486384779214859 - trainLoss: 0.49063441157341003\n",
      "cnt: 0 - valLoss: 0.4863695204257965 - trainLoss: 0.4906211197376251\n",
      "cnt: 0 - valLoss: 0.486354261636734 - trainLoss: 0.4906078577041626\n",
      "cnt: 0 - valLoss: 0.4863390028476715 - trainLoss: 0.4905945956707001\n",
      "cnt: 0 - valLoss: 0.4863237142562866 - trainLoss: 0.49058136343955994\n",
      "cnt: 0 - valLoss: 0.4863084554672241 - trainLoss: 0.4905681312084198\n",
      "cnt: 0 - valLoss: 0.4862931966781616 - trainLoss: 0.4905548691749573\n",
      "cnt: 0 - valLoss: 0.4862779378890991 - trainLoss: 0.49054160714149475\n",
      "cnt: 0 - valLoss: 0.486262708902359 - trainLoss: 0.4905284345149994\n",
      "cnt: 0 - valLoss: 0.48624753952026367 - trainLoss: 0.4905151426792145\n",
      "cnt: 0 - valLoss: 0.48623228073120117 - trainLoss: 0.49050194025039673\n",
      "cnt: 0 - valLoss: 0.48621711134910583 - trainLoss: 0.4904887080192566\n",
      "cnt: 0 - valLoss: 0.4862019419670105 - trainLoss: 0.49047550559043884\n",
      "cnt: 0 - valLoss: 0.48618677258491516 - trainLoss: 0.4904622435569763\n",
      "cnt: 0 - valLoss: 0.4861716032028198 - trainLoss: 0.49044907093048096\n",
      "cnt: 0 - valLoss: 0.4861564338207245 - trainLoss: 0.4904358386993408\n",
      "cnt: 0 - valLoss: 0.48614123463630676 - trainLoss: 0.4904226064682007\n",
      "cnt: 0 - valLoss: 0.4861261248588562 - trainLoss: 0.4904094338417053\n",
      "cnt: 0 - valLoss: 0.48611095547676086 - trainLoss: 0.49039626121520996\n",
      "cnt: 0 - valLoss: 0.4860957860946655 - trainLoss: 0.4903830587863922\n",
      "cnt: 0 - valLoss: 0.4860805571079254 - trainLoss: 0.49036985635757446\n",
      "cnt: 0 - valLoss: 0.48606541752815247 - trainLoss: 0.4903566539287567\n",
      "cnt: 0 - valLoss: 0.4860503077507019 - trainLoss: 0.49034351110458374\n",
      "cnt: 0 - valLoss: 0.4860351085662842 - trainLoss: 0.4903303384780884\n",
      "cnt: 0 - valLoss: 0.486020028591156 - trainLoss: 0.4903171956539154\n",
      "cnt: 0 - valLoss: 0.48600485920906067 - trainLoss: 0.49030405282974243\n",
      "cnt: 0 - valLoss: 0.4859897196292877 - trainLoss: 0.49029096961021423\n",
      "cnt: 0 - valLoss: 0.48597460985183716 - trainLoss: 0.4902777671813965\n",
      "cnt: 0 - valLoss: 0.4859594702720642 - trainLoss: 0.4902646243572235\n",
      "cnt: 0 - valLoss: 0.48594433069229126 - trainLoss: 0.49025148153305054\n",
      "cnt: 0 - valLoss: 0.4859292507171631 - trainLoss: 0.49023839831352234\n",
      "cnt: 0 - valLoss: 0.48591408133506775 - trainLoss: 0.490225225687027\n",
      "cnt: 0 - valLoss: 0.4858990013599396 - trainLoss: 0.4902121126651764\n",
      "cnt: 0 - valLoss: 0.485883891582489 - trainLoss: 0.4901990294456482\n",
      "cnt: 0 - valLoss: 0.48586881160736084 - trainLoss: 0.4901858866214752\n",
      "cnt: 0 - valLoss: 0.48585373163223267 - trainLoss: 0.49017274379730225\n",
      "cnt: 0 - valLoss: 0.4858386516571045 - trainLoss: 0.49015969038009644\n",
      "cnt: 0 - valLoss: 0.48582354187965393 - trainLoss: 0.49014657735824585\n",
      "cnt: 0 - valLoss: 0.48580846190452576 - trainLoss: 0.4901334345340729\n",
      "cnt: 0 - valLoss: 0.4857933819293976 - trainLoss: 0.4901203513145447\n",
      "cnt: 0 - valLoss: 0.4857783019542694 - trainLoss: 0.4901072680950165\n",
      "cnt: 0 - valLoss: 0.48576319217681885 - trainLoss: 0.4900941550731659\n",
      "cnt: 0 - valLoss: 0.48574817180633545 - trainLoss: 0.4900811016559601\n",
      "cnt: 0 - valLoss: 0.4857330620288849 - trainLoss: 0.4900680184364319\n",
      "cnt: 0 - valLoss: 0.4857180416584015 - trainLoss: 0.4900549650192261\n",
      "cnt: 0 - valLoss: 0.4857029914855957 - trainLoss: 0.4900418519973755\n",
      "cnt: 0 - valLoss: 0.4856879413127899 - trainLoss: 0.4900287687778473\n",
      "cnt: 0 - valLoss: 0.48567289113998413 - trainLoss: 0.4900157153606415\n",
      "cnt: 0 - valLoss: 0.48565784096717834 - trainLoss: 0.4900026321411133\n",
      "cnt: 0 - valLoss: 0.48564285039901733 - trainLoss: 0.48998963832855225\n",
      "cnt: 0 - valLoss: 0.4856278598308563 - trainLoss: 0.48997652530670166\n",
      "cnt: 0 - valLoss: 0.48561275005340576 - trainLoss: 0.48996347188949585\n",
      "cnt: 0 - valLoss: 0.48559775948524475 - trainLoss: 0.48995041847229004\n",
      "cnt: 0 - valLoss: 0.48558276891708374 - trainLoss: 0.48993736505508423\n",
      "cnt: 0 - valLoss: 0.48556771874427795 - trainLoss: 0.4899243116378784\n",
      "cnt: 0 - valLoss: 0.48555275797843933 - trainLoss: 0.4899113178253174\n",
      "cnt: 0 - valLoss: 0.4855377674102783 - trainLoss: 0.48989829421043396\n",
      "cnt: 0 - valLoss: 0.4855227470397949 - trainLoss: 0.48988527059555054\n",
      "cnt: 0 - valLoss: 0.4855077564716339 - trainLoss: 0.4898722767829895\n",
      "cnt: 0 - valLoss: 0.4854927957057953 - trainLoss: 0.4898592531681061\n",
      "cnt: 0 - valLoss: 0.48547783493995667 - trainLoss: 0.48984625935554504\n",
      "cnt: 0 - valLoss: 0.48546287417411804 - trainLoss: 0.4898332953453064\n",
      "cnt: 0 - valLoss: 0.4854479134082794 - trainLoss: 0.48982030153274536\n",
      "cnt: 0 - valLoss: 0.4854329228401184 - trainLoss: 0.4898073077201843\n",
      "cnt: 0 - valLoss: 0.4854179918766022 - trainLoss: 0.4897943437099457\n",
      "cnt: 0 - valLoss: 0.48540303111076355 - trainLoss: 0.48978134989738464\n",
      "cnt: 0 - valLoss: 0.4853880703449249 - trainLoss: 0.489768385887146\n",
      "cnt: 0 - valLoss: 0.4853731691837311 - trainLoss: 0.48975539207458496\n",
      "cnt: 0 - valLoss: 0.48535823822021484 - trainLoss: 0.4897423982620239\n",
      "cnt: 0 - valLoss: 0.485343337059021 - trainLoss: 0.48972949385643005\n",
      "cnt: 0 - valLoss: 0.4853283762931824 - trainLoss: 0.4897165596485138\n",
      "cnt: 0 - valLoss: 0.4853135049343109 - trainLoss: 0.48970359563827515\n",
      "cnt: 0 - valLoss: 0.4852985441684723 - trainLoss: 0.4896906316280365\n",
      "cnt: 0 - valLoss: 0.48528361320495605 - trainLoss: 0.48967766761779785\n",
      "cnt: 0 - valLoss: 0.4852687120437622 - trainLoss: 0.4896646738052368\n",
      "cnt: 0 - valLoss: 0.48525384068489075 - trainLoss: 0.48965176939964294\n",
      "cnt: 0 - valLoss: 0.4852389097213745 - trainLoss: 0.4896388351917267\n",
      "cnt: 0 - valLoss: 0.48522403836250305 - trainLoss: 0.4896259009838104\n",
      "cnt: 0 - valLoss: 0.4852091372013092 - trainLoss: 0.48961296677589417\n",
      "cnt: 0 - valLoss: 0.48519426584243774 - trainLoss: 0.4896000623703003\n",
      "cnt: 0 - valLoss: 0.4851793944835663 - trainLoss: 0.48958712816238403\n",
      "cnt: 0 - valLoss: 0.4851645231246948 - trainLoss: 0.4895741939544678\n",
      "cnt: 0 - valLoss: 0.4851495921611786 - trainLoss: 0.4895613193511963\n",
      "cnt: 0 - valLoss: 0.4851347804069519 - trainLoss: 0.48954838514328003\n",
      "cnt: 0 - valLoss: 0.48511990904808044 - trainLoss: 0.48953551054000854\n",
      "cnt: 0 - valLoss: 0.4851050078868866 - trainLoss: 0.4895225763320923\n",
      "cnt: 0 - valLoss: 0.48509013652801514 - trainLoss: 0.4895096719264984\n",
      "cnt: 0 - valLoss: 0.48507535457611084 - trainLoss: 0.48949673771858215\n",
      "cnt: 0 - valLoss: 0.4850604832172394 - trainLoss: 0.4894838333129883\n",
      "cnt: 0 - valLoss: 0.4850456714630127 - trainLoss: 0.4894709587097168\n",
      "cnt: 0 - valLoss: 0.48503077030181885 - trainLoss: 0.4894581139087677\n",
      "cnt: 0 - valLoss: 0.48501595854759216 - trainLoss: 0.48944520950317383\n",
      "cnt: 0 - valLoss: 0.4850011169910431 - trainLoss: 0.48943236470222473\n",
      "cnt: 0 - valLoss: 0.4849863052368164 - trainLoss: 0.48941943049430847\n",
      "cnt: 0 - valLoss: 0.4849714934825897 - trainLoss: 0.4894065856933594\n",
      "cnt: 0 - valLoss: 0.48495668172836304 - trainLoss: 0.4893937408924103\n",
      "cnt: 0 - valLoss: 0.48494189977645874 - trainLoss: 0.4893808364868164\n",
      "cnt: 0 - valLoss: 0.48492708802223206 - trainLoss: 0.4893679916858673\n",
      "cnt: 0 - valLoss: 0.48491227626800537 - trainLoss: 0.4893551468849182\n",
      "cnt: 0 - valLoss: 0.4848974943161011 - trainLoss: 0.4893423020839691\n",
      "cnt: 0 - valLoss: 0.484882652759552 - trainLoss: 0.48932945728302\n",
      "cnt: 0 - valLoss: 0.4848679006099701 - trainLoss: 0.4893166124820709\n",
      "cnt: 0 - valLoss: 0.4848531186580658 - trainLoss: 0.4893037676811218\n",
      "cnt: 0 - valLoss: 0.4848383367061615 - trainLoss: 0.4892909526824951\n",
      "cnt: 0 - valLoss: 0.4848235547542572 - trainLoss: 0.489278107881546\n",
      "cnt: 0 - valLoss: 0.4848087728023529 - trainLoss: 0.4892652630805969\n",
      "cnt: 0 - valLoss: 0.484794020652771 - trainLoss: 0.4892524182796478\n",
      "cnt: 0 - valLoss: 0.4847792387008667 - trainLoss: 0.48923957347869873\n",
      "cnt: 0 - valLoss: 0.4847644865512848 - trainLoss: 0.4892267882823944\n",
      "cnt: 0 - valLoss: 0.4847497045993805 - trainLoss: 0.4892139732837677\n",
      "cnt: 0 - valLoss: 0.4847349524497986 - trainLoss: 0.4892011284828186\n",
      "cnt: 0 - valLoss: 0.4847202003002167 - trainLoss: 0.4891883134841919\n",
      "cnt: 0 - valLoss: 0.48470550775527954 - trainLoss: 0.48917555809020996\n",
      "cnt: 0 - valLoss: 0.48469072580337524 - trainLoss: 0.48916274309158325\n",
      "cnt: 0 - valLoss: 0.4846760034561157 - trainLoss: 0.48914989829063416\n",
      "cnt: 0 - valLoss: 0.4846613109111786 - trainLoss: 0.48913711309432983\n",
      "cnt: 0 - valLoss: 0.4846465587615967 - trainLoss: 0.4891243577003479\n",
      "cnt: 0 - valLoss: 0.48463183641433716 - trainLoss: 0.4891115725040436\n",
      "cnt: 0 - valLoss: 0.48461711406707764 - trainLoss: 0.48909875750541687\n",
      "cnt: 0 - valLoss: 0.4846024215221405 - trainLoss: 0.48908600211143494\n",
      "cnt: 0 - valLoss: 0.48458772897720337 - trainLoss: 0.4890732169151306\n",
      "cnt: 0 - valLoss: 0.48457300662994385 - trainLoss: 0.4890604019165039\n",
      "cnt: 0 - valLoss: 0.4845582842826843 - trainLoss: 0.489047646522522\n",
      "cnt: 0 - valLoss: 0.4845436215400696 - trainLoss: 0.48903489112854004\n",
      "cnt: 0 - valLoss: 0.48452892899513245 - trainLoss: 0.4890221357345581\n",
      "cnt: 0 - valLoss: 0.4845142662525177 - trainLoss: 0.48900938034057617\n",
      "cnt: 0 - valLoss: 0.48449960350990295 - trainLoss: 0.48899662494659424\n",
      "cnt: 0 - valLoss: 0.4844849407672882 - trainLoss: 0.4889838695526123\n",
      "cnt: 0 - valLoss: 0.48447033762931824 - trainLoss: 0.48897117376327515\n",
      "cnt: 0 - valLoss: 0.4844556748867035 - trainLoss: 0.4889584481716156\n",
      "cnt: 0 - valLoss: 0.48444101214408875 - trainLoss: 0.48894575238227844\n",
      "cnt: 0 - valLoss: 0.484426349401474 - trainLoss: 0.4889330267906189\n",
      "cnt: 0 - valLoss: 0.48441168665885925 - trainLoss: 0.48892033100128174\n",
      "cnt: 0 - valLoss: 0.4843970835208893 - trainLoss: 0.4889076054096222\n",
      "cnt: 0 - valLoss: 0.4843824505805969 - trainLoss: 0.4888949394226074\n",
      "cnt: 0 - valLoss: 0.48436784744262695 - trainLoss: 0.4888821840286255\n",
      "cnt: 0 - valLoss: 0.4843531847000122 - trainLoss: 0.4888695180416107\n",
      "cnt: 0 - valLoss: 0.48433858156204224 - trainLoss: 0.48885685205459595\n",
      "cnt: 0 - valLoss: 0.4843239486217499 - trainLoss: 0.4888441264629364\n",
      "cnt: 0 - valLoss: 0.4843093454837799 - trainLoss: 0.48883146047592163\n",
      "cnt: 0 - valLoss: 0.4842947721481323 - trainLoss: 0.4888187646865845\n",
      "cnt: 0 - valLoss: 0.48428013920783997 - trainLoss: 0.4888060986995697\n",
      "cnt: 0 - valLoss: 0.48426553606987 - trainLoss: 0.48879337310791016\n",
      "cnt: 0 - valLoss: 0.4842509627342224 - trainLoss: 0.4887807369232178\n",
      "cnt: 0 - valLoss: 0.48423638939857483 - trainLoss: 0.488768070936203\n",
      "cnt: 0 - valLoss: 0.48422181606292725 - trainLoss: 0.48875537514686584\n",
      "cnt: 0 - valLoss: 0.48420724272727966 - trainLoss: 0.48874273896217346\n",
      "cnt: 0 - valLoss: 0.4841926693916321 - trainLoss: 0.4887300729751587\n",
      "cnt: 0 - valLoss: 0.4841780960559845 - trainLoss: 0.4887174069881439\n",
      "cnt: 0 - valLoss: 0.4841635227203369 - trainLoss: 0.48870477080345154\n",
      "cnt: 0 - valLoss: 0.4841489791870117 - trainLoss: 0.48869210481643677\n",
      "cnt: 0 - valLoss: 0.48413440585136414 - trainLoss: 0.4886794686317444\n",
      "cnt: 0 - valLoss: 0.48411983251571655 - trainLoss: 0.4886668026447296\n",
      "cnt: 0 - valLoss: 0.48410525918006897 - trainLoss: 0.48865416646003723\n",
      "cnt: 0 - valLoss: 0.4840906858444214 - trainLoss: 0.48864150047302246\n",
      "cnt: 0 - valLoss: 0.4840761125087738 - trainLoss: 0.4886288046836853\n",
      "cnt: 0 - valLoss: 0.484061598777771 - trainLoss: 0.4886162281036377\n",
      "cnt: 0 - valLoss: 0.4840470254421234 - trainLoss: 0.48860353231430054\n",
      "cnt: 0 - valLoss: 0.4840324819087982 - trainLoss: 0.48859086632728577\n",
      "cnt: 0 - valLoss: 0.4840179681777954 - trainLoss: 0.488578200340271\n",
      "cnt: 0 - valLoss: 0.4840033948421478 - trainLoss: 0.4885655343532562\n",
      "cnt: 0 - valLoss: 0.48398885130882263 - trainLoss: 0.4885529577732086\n",
      "cnt: 0 - valLoss: 0.4839743375778198 - trainLoss: 0.48854029178619385\n",
      "cnt: 0 - valLoss: 0.48395976424217224 - trainLoss: 0.48852765560150146\n",
      "cnt: 0 - valLoss: 0.4839452803134918 - trainLoss: 0.4885150194168091\n",
      "cnt: 0 - valLoss: 0.4839307367801666 - trainLoss: 0.4885023534297943\n",
      "cnt: 0 - valLoss: 0.48391619324684143 - trainLoss: 0.4884897768497467\n",
      "cnt: 0 - valLoss: 0.4839016795158386 - trainLoss: 0.48847711086273193\n",
      "cnt: 0 - valLoss: 0.4838871955871582 - trainLoss: 0.4884645342826843\n",
      "cnt: 0 - valLoss: 0.483872652053833 - trainLoss: 0.48845189809799194\n",
      "cnt: 0 - valLoss: 0.4838581681251526 - trainLoss: 0.48843932151794434\n",
      "cnt: 0 - valLoss: 0.48384368419647217 - trainLoss: 0.48842668533325195\n",
      "cnt: 0 - valLoss: 0.48382920026779175 - trainLoss: 0.48841410875320435\n",
      "cnt: 0 - valLoss: 0.48381471633911133 - trainLoss: 0.48840147256851196\n",
      "cnt: 0 - valLoss: 0.4838002324104309 - trainLoss: 0.48838886618614197\n",
      "cnt: 0 - valLoss: 0.4837857484817505 - trainLoss: 0.488376259803772\n",
      "cnt: 0 - valLoss: 0.4837712347507477 - trainLoss: 0.48836368322372437\n",
      "cnt: 0 - valLoss: 0.48375675082206726 - trainLoss: 0.48835110664367676\n",
      "cnt: 0 - valLoss: 0.483742356300354 - trainLoss: 0.48833853006362915\n",
      "cnt: 0 - valLoss: 0.4837278723716736 - trainLoss: 0.48832592368125916\n",
      "cnt: 0 - valLoss: 0.48371341824531555 - trainLoss: 0.4883134067058563\n",
      "cnt: 0 - valLoss: 0.4836989939212799 - trainLoss: 0.48830077052116394\n",
      "cnt: 0 - valLoss: 0.4836845397949219 - trainLoss: 0.4882882237434387\n",
      "cnt: 0 - valLoss: 0.48367005586624146 - trainLoss: 0.4882756769657135\n",
      "cnt: 0 - valLoss: 0.4836556017398834 - trainLoss: 0.4882631003856659\n",
      "cnt: 0 - valLoss: 0.48364120721817017 - trainLoss: 0.4882505238056183\n",
      "cnt: 0 - valLoss: 0.48362675309181213 - trainLoss: 0.48823797702789307\n",
      "cnt: 0 - valLoss: 0.4836123287677765 - trainLoss: 0.48822543025016785\n",
      "cnt: 0 - valLoss: 0.48359790444374084 - trainLoss: 0.48821285367012024\n",
      "cnt: 0 - valLoss: 0.4835835099220276 - trainLoss: 0.4882003664970398\n",
      "cnt: 0 - valLoss: 0.48356911540031433 - trainLoss: 0.4881878197193146\n",
      "cnt: 0 - valLoss: 0.4835547208786011 - trainLoss: 0.48817527294158936\n",
      "cnt: 0 - valLoss: 0.48354029655456543 - trainLoss: 0.4881627857685089\n",
      "cnt: 0 - valLoss: 0.483525812625885 - trainLoss: 0.4881501793861389\n",
      "cnt: 0 - valLoss: 0.48351141810417175 - trainLoss: 0.48813769221305847\n",
      "cnt: 0 - valLoss: 0.48349693417549133 - trainLoss: 0.488125205039978\n",
      "cnt: 0 - valLoss: 0.4834825396537781 - trainLoss: 0.4881126880645752\n",
      "cnt: 0 - valLoss: 0.4834681451320648 - trainLoss: 0.48810020089149475\n",
      "cnt: 0 - valLoss: 0.483453631401062 - trainLoss: 0.48808765411376953\n",
      "cnt: 0 - valLoss: 0.48343920707702637 - trainLoss: 0.4880751669406891\n",
      "cnt: 0 - valLoss: 0.48342475295066833 - trainLoss: 0.48806264996528625\n",
      "cnt: 0 - valLoss: 0.4834102690219879 - trainLoss: 0.4880501925945282\n",
      "cnt: 0 - valLoss: 0.48339587450027466 - trainLoss: 0.48803770542144775\n",
      "cnt: 0 - valLoss: 0.48338139057159424 - trainLoss: 0.4880252182483673\n",
      "cnt: 0 - valLoss: 0.4833669662475586 - trainLoss: 0.4880127012729645\n",
      "cnt: 0 - valLoss: 0.48335254192352295 - trainLoss: 0.4880002439022064\n",
      "cnt: 0 - valLoss: 0.4833380877971649 - trainLoss: 0.48798781633377075\n",
      "cnt: 0 - valLoss: 0.48332369327545166 - trainLoss: 0.4879752993583679\n",
      "cnt: 0 - valLoss: 0.48330923914909363 - trainLoss: 0.48796290159225464\n",
      "cnt: 0 - valLoss: 0.48329484462738037 - trainLoss: 0.4879503846168518\n",
      "cnt: 0 - valLoss: 0.4832804501056671 - trainLoss: 0.48793792724609375\n",
      "cnt: 0 - valLoss: 0.48326602578163147 - trainLoss: 0.4879254698753357\n",
      "cnt: 0 - valLoss: 0.4832516312599182 - trainLoss: 0.4879130721092224\n",
      "cnt: 0 - valLoss: 0.48323726654052734 - trainLoss: 0.48790058493614197\n",
      "cnt: 0 - valLoss: 0.48322293162345886 - trainLoss: 0.4878881573677063\n",
      "cnt: 0 - valLoss: 0.483208566904068 - trainLoss: 0.48787567019462585\n",
      "cnt: 0 - valLoss: 0.48319417238235474 - trainLoss: 0.4878632426261902\n",
      "cnt: 0 - valLoss: 0.48317983746528625 - trainLoss: 0.48785075545310974\n",
      "cnt: 0 - valLoss: 0.4831654727458954 - trainLoss: 0.4878383278846741\n",
      "cnt: 0 - valLoss: 0.4831511676311493 - trainLoss: 0.487825870513916\n",
      "cnt: 0 - valLoss: 0.4831368029117584 - trainLoss: 0.48781341314315796\n",
      "cnt: 0 - valLoss: 0.48312249779701233 - trainLoss: 0.4878010153770447\n",
      "cnt: 0 - valLoss: 0.48310813307762146 - trainLoss: 0.4877886176109314\n",
      "cnt: 0 - valLoss: 0.48309382796287537 - trainLoss: 0.4877761900424957\n",
      "cnt: 0 - valLoss: 0.4830794930458069 - trainLoss: 0.48776373267173767\n",
      "cnt: 0 - valLoss: 0.4830651879310608 - trainLoss: 0.4877513647079468\n",
      "cnt: 0 - valLoss: 0.4830508232116699 - trainLoss: 0.4877389073371887\n",
      "cnt: 0 - valLoss: 0.483036607503891 - trainLoss: 0.4877265393733978\n",
      "cnt: 0 - valLoss: 0.4830222725868225 - trainLoss: 0.48771414160728455\n",
      "cnt: 0 - valLoss: 0.4830079674720764 - trainLoss: 0.4877017140388489\n",
      "cnt: 0 - valLoss: 0.4829936623573303 - trainLoss: 0.4876893162727356\n",
      "cnt: 0 - valLoss: 0.4829793870449066 - trainLoss: 0.4876768887042999\n",
      "cnt: 0 - valLoss: 0.4829651117324829 - trainLoss: 0.48766449093818665\n",
      "cnt: 0 - valLoss: 0.4829508364200592 - trainLoss: 0.48765212297439575\n",
      "cnt: 0 - valLoss: 0.4829365611076355 - trainLoss: 0.48763972520828247\n",
      "cnt: 0 - valLoss: 0.4829222559928894 - trainLoss: 0.4876273572444916\n",
      "cnt: 0 - valLoss: 0.4829080104827881 - trainLoss: 0.4876149892807007\n",
      "cnt: 0 - valLoss: 0.482893705368042 - trainLoss: 0.4876025319099426\n",
      "cnt: 0 - valLoss: 0.4828794598579407 - trainLoss: 0.4875902533531189\n",
      "cnt: 0 - valLoss: 0.48286524415016174 - trainLoss: 0.4875778555870056\n",
      "cnt: 0 - valLoss: 0.48285096883773804 - trainLoss: 0.4875654876232147\n",
      "cnt: 0 - valLoss: 0.4828367233276367 - trainLoss: 0.48755311965942383\n",
      "cnt: 0 - valLoss: 0.4828225076198578 - trainLoss: 0.4875407814979553\n",
      "cnt: 0 - valLoss: 0.4828082323074341 - trainLoss: 0.48752841353416443\n",
      "cnt: 0 - valLoss: 0.48279398679733276 - trainLoss: 0.48751604557037354\n",
      "cnt: 0 - valLoss: 0.4827798008918762 - trainLoss: 0.48750370740890503\n",
      "cnt: 0 - valLoss: 0.4827655851840973 - trainLoss: 0.4874913990497589\n",
      "cnt: 0 - valLoss: 0.48275133967399597 - trainLoss: 0.487479031085968\n",
      "cnt: 0 - valLoss: 0.48273712396621704 - trainLoss: 0.4874666929244995\n",
      "cnt: 0 - valLoss: 0.4827229380607605 - trainLoss: 0.487454354763031\n",
      "cnt: 0 - valLoss: 0.4827086925506592 - trainLoss: 0.4874420464038849\n",
      "cnt: 0 - valLoss: 0.4826945662498474 - trainLoss: 0.4874297082424164\n",
      "cnt: 0 - valLoss: 0.4826802909374237 - trainLoss: 0.4874173402786255\n",
      "cnt: 0 - valLoss: 0.48266613483428955 - trainLoss: 0.48740506172180176\n",
      "cnt: 0 - valLoss: 0.4826519191265106 - trainLoss: 0.48739272356033325\n",
      "cnt: 0 - valLoss: 0.4826377332210541 - trainLoss: 0.48738041520118713\n",
      "cnt: 0 - valLoss: 0.4826235771179199 - trainLoss: 0.487368106842041\n",
      "cnt: 0 - valLoss: 0.4826093912124634 - trainLoss: 0.4873557984828949\n",
      "cnt: 0 - valLoss: 0.48259520530700684 - trainLoss: 0.48734354972839355\n",
      "cnt: 0 - valLoss: 0.4825810492038727 - trainLoss: 0.48733121156692505\n",
      "cnt: 0 - valLoss: 0.4825669229030609 - trainLoss: 0.48731890320777893\n",
      "cnt: 0 - valLoss: 0.48255273699760437 - trainLoss: 0.4873066544532776\n",
      "cnt: 0 - valLoss: 0.4825385510921478 - trainLoss: 0.4872943162918091\n",
      "cnt: 0 - valLoss: 0.48252439498901367 - trainLoss: 0.4872820973396301\n",
      "cnt: 0 - valLoss: 0.4825102686882019 - trainLoss: 0.4872697591781616\n",
      "cnt: 0 - valLoss: 0.48249614238739014 - trainLoss: 0.4872574806213379\n",
      "cnt: 0 - valLoss: 0.48248201608657837 - trainLoss: 0.48724520206451416\n",
      "cnt: 0 - valLoss: 0.4824678599834442 - trainLoss: 0.4872329533100128\n",
      "cnt: 0 - valLoss: 0.48245376348495483 - trainLoss: 0.4872207045555115\n",
      "cnt: 0 - valLoss: 0.4824396073818207 - trainLoss: 0.48720839619636536\n",
      "cnt: 0 - valLoss: 0.4824254810810089 - trainLoss: 0.4871961772441864\n",
      "cnt: 0 - valLoss: 0.4824114143848419 - trainLoss: 0.48718389868736267\n",
      "cnt: 0 - valLoss: 0.48239725828170776 - trainLoss: 0.4871717095375061\n",
      "cnt: 0 - valLoss: 0.4823831617832184 - trainLoss: 0.4871594309806824\n",
      "cnt: 0 - valLoss: 0.482369065284729 - trainLoss: 0.48714712262153625\n",
      "cnt: 0 - valLoss: 0.482354998588562 - trainLoss: 0.4871349334716797\n",
      "cnt: 0 - valLoss: 0.48234090209007263 - trainLoss: 0.48712268471717834\n",
      "cnt: 0 - valLoss: 0.48232677578926086 - trainLoss: 0.4871104657649994\n",
      "cnt: 0 - valLoss: 0.48231270909309387 - trainLoss: 0.48709821701049805\n",
      "cnt: 0 - valLoss: 0.4822986423969269 - trainLoss: 0.4870859682559967\n",
      "cnt: 0 - valLoss: 0.4822845458984375 - trainLoss: 0.48707377910614014\n",
      "cnt: 0 - valLoss: 0.4822704792022705 - trainLoss: 0.4870615601539612\n",
      "cnt: 0 - valLoss: 0.4822564423084259 - trainLoss: 0.4870493710041046\n",
      "cnt: 0 - valLoss: 0.4822423756122589 - trainLoss: 0.48703718185424805\n",
      "cnt: 0 - valLoss: 0.4822283089160919 - trainLoss: 0.4870249927043915\n",
      "cnt: 0 - valLoss: 0.4822142422199249 - trainLoss: 0.4870127737522125\n",
      "cnt: 0 - valLoss: 0.4822002053260803 - trainLoss: 0.48700058460235596\n",
      "cnt: 0 - valLoss: 0.4821861684322357 - trainLoss: 0.4869883954524994\n",
      "cnt: 0 - valLoss: 0.4821721613407135 - trainLoss: 0.4869762361049652\n",
      "cnt: 0 - valLoss: 0.4821580946445465 - trainLoss: 0.48696401715278625\n",
      "cnt: 0 - valLoss: 0.4821441173553467 - trainLoss: 0.48695188760757446\n",
      "cnt: 0 - valLoss: 0.48213011026382446 - trainLoss: 0.4869396686553955\n",
      "cnt: 0 - valLoss: 0.48211610317230225 - trainLoss: 0.48692750930786133\n",
      "cnt: 0 - valLoss: 0.4821021258831024 - trainLoss: 0.48691532015800476\n",
      "cnt: 0 - valLoss: 0.4820881187915802 - trainLoss: 0.48690322041511536\n",
      "cnt: 0 - valLoss: 0.48207414150238037 - trainLoss: 0.4868910014629364\n",
      "cnt: 0 - valLoss: 0.48206013441085815 - trainLoss: 0.486878901720047\n",
      "cnt: 0 - valLoss: 0.4820461571216583 - trainLoss: 0.48686671257019043\n",
      "cnt: 0 - valLoss: 0.4820321798324585 - trainLoss: 0.48685455322265625\n",
      "cnt: 0 - valLoss: 0.48201820254325867 - trainLoss: 0.48684242367744446\n",
      "cnt: 0 - valLoss: 0.4820042848587036 - trainLoss: 0.4868302643299103\n",
      "cnt: 0 - valLoss: 0.481990247964859 - trainLoss: 0.4868181049823761\n",
      "cnt: 0 - valLoss: 0.48197633028030396 - trainLoss: 0.4868060052394867\n",
      "cnt: 0 - valLoss: 0.4819623827934265 - trainLoss: 0.4867939054965973\n",
      "cnt: 0 - valLoss: 0.4819484353065491 - trainLoss: 0.4867817461490631\n",
      "cnt: 0 - valLoss: 0.48193445801734924 - trainLoss: 0.4867696166038513\n",
      "cnt: 0 - valLoss: 0.4819205105304718 - trainLoss: 0.4867575168609619\n",
      "cnt: 0 - valLoss: 0.48190659284591675 - trainLoss: 0.48674535751342773\n",
      "cnt: 0 - valLoss: 0.4818926751613617 - trainLoss: 0.4867332875728607\n",
      "cnt: 0 - valLoss: 0.48187872767448425 - trainLoss: 0.48672112822532654\n",
      "cnt: 0 - valLoss: 0.4818648397922516 - trainLoss: 0.4867090582847595\n",
      "cnt: 0 - valLoss: 0.4818509519100189 - trainLoss: 0.4866969883441925\n",
      "cnt: 0 - valLoss: 0.48183703422546387 - trainLoss: 0.4866849184036255\n",
      "cnt: 0 - valLoss: 0.4818231761455536 - trainLoss: 0.48667284846305847\n",
      "cnt: 0 - valLoss: 0.48180925846099854 - trainLoss: 0.48666077852249146\n",
      "cnt: 0 - valLoss: 0.48179540038108826 - trainLoss: 0.48664870858192444\n",
      "cnt: 0 - valLoss: 0.4817815124988556 - trainLoss: 0.4866366386413574\n",
      "cnt: 0 - valLoss: 0.4817676842212677 - trainLoss: 0.4866245985031128\n",
      "cnt: 0 - valLoss: 0.48175379633903503 - trainLoss: 0.48661258816719055\n",
      "cnt: 0 - valLoss: 0.48173993825912476 - trainLoss: 0.48660051822662354\n",
      "cnt: 0 - valLoss: 0.4817260503768921 - trainLoss: 0.4865885376930237\n",
      "cnt: 0 - valLoss: 0.4817122519016266 - trainLoss: 0.48657646775245667\n",
      "cnt: 0 - valLoss: 0.4816983640193939 - trainLoss: 0.4865644872188568\n",
      "cnt: 0 - valLoss: 0.4816845655441284 - trainLoss: 0.4865524172782898\n",
      "cnt: 0 - valLoss: 0.4816707372665405 - trainLoss: 0.48654040694236755\n",
      "cnt: 0 - valLoss: 0.48165687918663025 - trainLoss: 0.48652833700180054\n",
      "cnt: 0 - valLoss: 0.48164308071136475 - trainLoss: 0.4865163564682007\n",
      "cnt: 0 - valLoss: 0.48162928223609924 - trainLoss: 0.48650437593460083\n",
      "cnt: 0 - valLoss: 0.48161548376083374 - trainLoss: 0.4864923059940338\n",
      "cnt: 0 - valLoss: 0.4816015958786011 - trainLoss: 0.48648032546043396\n",
      "cnt: 0 - valLoss: 0.48158782720565796 - trainLoss: 0.4864683151245117\n",
      "cnt: 0 - valLoss: 0.4815739691257477 - trainLoss: 0.4864562749862671\n",
      "cnt: 0 - valLoss: 0.4815601706504822 - trainLoss: 0.48644429445266724\n",
      "cnt: 0 - valLoss: 0.48154643177986145 - trainLoss: 0.4864323139190674\n",
      "cnt: 0 - valLoss: 0.48153257369995117 - trainLoss: 0.4864203631877899\n",
      "cnt: 0 - valLoss: 0.48151883482933044 - trainLoss: 0.4864083230495453\n",
      "cnt: 0 - valLoss: 0.48150506615638733 - trainLoss: 0.48639634251594543\n",
      "cnt: 0 - valLoss: 0.4814912676811218 - trainLoss: 0.48638445138931274\n",
      "cnt: 0 - valLoss: 0.4814775586128235 - trainLoss: 0.4863724708557129\n",
      "cnt: 0 - valLoss: 0.48146378993988037 - trainLoss: 0.4863605797290802\n",
      "cnt: 0 - valLoss: 0.48145008087158203 - trainLoss: 0.48634856939315796\n",
      "cnt: 0 - valLoss: 0.4814363121986389 - trainLoss: 0.48633667826652527\n",
      "cnt: 0 - valLoss: 0.4814226031303406 - trainLoss: 0.4863247871398926\n",
      "cnt: 0 - valLoss: 0.48140889406204224 - trainLoss: 0.4863128364086151\n",
      "cnt: 0 - valLoss: 0.4813951551914215 - trainLoss: 0.48630091547966003\n",
      "cnt: 0 - valLoss: 0.4813814163208008 - trainLoss: 0.48628899455070496\n",
      "cnt: 0 - valLoss: 0.48136767745018005 - trainLoss: 0.48627710342407227\n",
      "cnt: 0 - valLoss: 0.4813540279865265 - trainLoss: 0.4862651228904724\n",
      "cnt: 0 - valLoss: 0.4813402593135834 - trainLoss: 0.4862532615661621\n",
      "cnt: 0 - valLoss: 0.4813265800476074 - trainLoss: 0.48624131083488464\n",
      "cnt: 0 - valLoss: 0.4813128709793091 - trainLoss: 0.48622941970825195\n",
      "cnt: 0 - valLoss: 0.48129919171333313 - trainLoss: 0.48621752858161926\n",
      "cnt: 0 - valLoss: 0.4812854528427124 - trainLoss: 0.4862056374549866\n",
      "cnt: 0 - valLoss: 0.48127180337905884 - trainLoss: 0.48619377613067627\n",
      "cnt: 0 - valLoss: 0.4812580943107605 - trainLoss: 0.4861818850040436\n",
      "cnt: 0 - valLoss: 0.48124441504478455 - trainLoss: 0.4861699640750885\n",
      "cnt: 0 - valLoss: 0.4812307357788086 - trainLoss: 0.4861581027507782\n",
      "cnt: 0 - valLoss: 0.48121708631515503 - trainLoss: 0.4861462116241455\n",
      "cnt: 0 - valLoss: 0.4812034070491791 - trainLoss: 0.4861343502998352\n",
      "cnt: 0 - valLoss: 0.4811897575855255 - trainLoss: 0.4861225187778473\n",
      "cnt: 0 - valLoss: 0.48117610812187195 - trainLoss: 0.4861105978488922\n",
      "cnt: 0 - valLoss: 0.481162428855896 - trainLoss: 0.4860987365245819\n",
      "cnt: 0 - valLoss: 0.48114874958992004 - trainLoss: 0.486086905002594\n",
      "cnt: 0 - valLoss: 0.48113512992858887 - trainLoss: 0.4860750436782837\n",
      "cnt: 0 - valLoss: 0.4811214804649353 - trainLoss: 0.4860631823539734\n",
      "cnt: 0 - valLoss: 0.48110783100128174 - trainLoss: 0.4860512912273407\n",
      "cnt: 0 - valLoss: 0.4810941815376282 - trainLoss: 0.4860394597053528\n",
      "cnt: 0 - valLoss: 0.481080561876297 - trainLoss: 0.48602762818336487\n",
      "cnt: 0 - valLoss: 0.4810669422149658 - trainLoss: 0.48601576685905457\n",
      "cnt: 0 - valLoss: 0.48105329275131226 - trainLoss: 0.48600393533706665\n",
      "cnt: 0 - valLoss: 0.48103970289230347 - trainLoss: 0.4859921336174011\n",
      "cnt: 0 - valLoss: 0.4810261130332947 - trainLoss: 0.4859803318977356\n",
      "cnt: 0 - valLoss: 0.4810124933719635 - trainLoss: 0.4859685003757477\n",
      "cnt: 0 - valLoss: 0.4809988737106323 - trainLoss: 0.48595669865608215\n",
      "cnt: 0 - valLoss: 0.4809853136539459 - trainLoss: 0.48594480752944946\n",
      "cnt: 0 - valLoss: 0.48097169399261475 - trainLoss: 0.48593297600746155\n",
      "cnt: 0 - valLoss: 0.48095810413360596 - trainLoss: 0.4859212040901184\n",
      "cnt: 0 - valLoss: 0.4809444844722748 - trainLoss: 0.4859094023704529\n",
      "cnt: 0 - valLoss: 0.480930894613266 - trainLoss: 0.48589757084846497\n",
      "cnt: 0 - valLoss: 0.4809173047542572 - trainLoss: 0.48588576912879944\n",
      "cnt: 0 - valLoss: 0.4809037148952484 - trainLoss: 0.4858739972114563\n",
      "cnt: 0 - valLoss: 0.480890154838562 - trainLoss: 0.48586219549179077\n",
      "cnt: 0 - valLoss: 0.480876624584198 - trainLoss: 0.48585042357444763\n",
      "cnt: 0 - valLoss: 0.4808630347251892 - trainLoss: 0.4858385920524597\n",
      "cnt: 0 - valLoss: 0.4808495044708252 - trainLoss: 0.4858268201351166\n",
      "cnt: 0 - valLoss: 0.4808359146118164 - trainLoss: 0.48581501841545105\n",
      "cnt: 0 - valLoss: 0.48082235455513 - trainLoss: 0.4858032464981079\n",
      "cnt: 0 - valLoss: 0.480808824300766 - trainLoss: 0.48579147458076477\n",
      "cnt: 0 - valLoss: 0.4807952642440796 - trainLoss: 0.4857797622680664\n",
      "cnt: 0 - valLoss: 0.48078176379203796 - trainLoss: 0.4857679307460785\n",
      "cnt: 0 - valLoss: 0.4807681739330292 - trainLoss: 0.48575615882873535\n",
      "cnt: 0 - valLoss: 0.48075467348098755 - trainLoss: 0.485744446516037\n",
      "cnt: 0 - valLoss: 0.4807411730289459 - trainLoss: 0.48573267459869385\n",
      "cnt: 0 - valLoss: 0.4807276427745819 - trainLoss: 0.4857209324836731\n",
      "cnt: 0 - valLoss: 0.4807140827178955 - trainLoss: 0.48570922017097473\n",
      "cnt: 0 - valLoss: 0.4807005822658539 - trainLoss: 0.4856974482536316\n",
      "cnt: 0 - valLoss: 0.48068708181381226 - trainLoss: 0.48568567633628845\n",
      "cnt: 0 - valLoss: 0.48067355155944824 - trainLoss: 0.4856739342212677\n",
      "cnt: 0 - valLoss: 0.480660080909729 - trainLoss: 0.48566222190856934\n",
      "cnt: 0 - valLoss: 0.480646550655365 - trainLoss: 0.4856504797935486\n",
      "cnt: 0 - valLoss: 0.48063308000564575 - trainLoss: 0.4856387674808502\n",
      "cnt: 0 - valLoss: 0.4806196093559265 - trainLoss: 0.48562702536582947\n",
      "cnt: 0 - valLoss: 0.48060616850852966 - trainLoss: 0.4856153130531311\n",
      "cnt: 0 - valLoss: 0.4805926978588104 - trainLoss: 0.48560357093811035\n",
      "cnt: 0 - valLoss: 0.4805792570114136 - trainLoss: 0.4855918884277344\n",
      "cnt: 0 - valLoss: 0.48056578636169434 - trainLoss: 0.485580176115036\n",
      "cnt: 0 - valLoss: 0.4805523753166199 - trainLoss: 0.48556849360466003\n",
      "cnt: 0 - valLoss: 0.4805389642715454 - trainLoss: 0.4855567514896393\n",
      "cnt: 0 - valLoss: 0.48052549362182617 - trainLoss: 0.4855450391769409\n",
      "cnt: 0 - valLoss: 0.4805120527744293 - trainLoss: 0.48553332686424255\n",
      "cnt: 0 - valLoss: 0.48049861192703247 - trainLoss: 0.4855216443538666\n",
      "cnt: 0 - valLoss: 0.480485200881958 - trainLoss: 0.4855099320411682\n",
      "cnt: 0 - valLoss: 0.48047176003456116 - trainLoss: 0.4854982793331146\n",
      "cnt: 0 - valLoss: 0.4804583787918091 - trainLoss: 0.48548659682273865\n",
      "cnt: 0 - valLoss: 0.48044490814208984 - trainLoss: 0.48547491431236267\n",
      "cnt: 0 - valLoss: 0.48043155670166016 - trainLoss: 0.4854632616043091\n",
      "cnt: 0 - valLoss: 0.4804181158542633 - trainLoss: 0.4854515492916107\n",
      "cnt: 0 - valLoss: 0.48040470480918884 - trainLoss: 0.48543986678123474\n",
      "cnt: 0 - valLoss: 0.48039135336875916 - trainLoss: 0.48542821407318115\n",
      "cnt: 0 - valLoss: 0.4803779721260071 - trainLoss: 0.4854165315628052\n",
      "cnt: 0 - valLoss: 0.48036450147628784 - trainLoss: 0.4854048788547516\n",
      "cnt: 0 - valLoss: 0.48035115003585815 - trainLoss: 0.4853932559490204\n",
      "cnt: 0 - valLoss: 0.4803377687931061 - trainLoss: 0.4853815734386444\n",
      "cnt: 0 - valLoss: 0.480324387550354 - trainLoss: 0.4853699207305908\n",
      "cnt: 0 - valLoss: 0.48031100630760193 - trainLoss: 0.48535826802253723\n",
      "cnt: 0 - valLoss: 0.48029765486717224 - trainLoss: 0.4853466749191284\n",
      "cnt: 0 - valLoss: 0.48028430342674255 - trainLoss: 0.4853350520133972\n",
      "cnt: 0 - valLoss: 0.48027095198631287 - trainLoss: 0.48532339930534363\n",
      "cnt: 0 - valLoss: 0.4802575707435608 - trainLoss: 0.48531171679496765\n",
      "cnt: 0 - valLoss: 0.4802442491054535 - trainLoss: 0.48530012369155884\n",
      "cnt: 0 - valLoss: 0.4802308678627014 - trainLoss: 0.48528847098350525\n",
      "cnt: 0 - valLoss: 0.4802175462245941 - trainLoss: 0.48527681827545166\n",
      "cnt: 0 - valLoss: 0.48020419478416443 - trainLoss: 0.48526522517204285\n",
      "cnt: 0 - valLoss: 0.48019087314605713 - trainLoss: 0.48525363206863403\n",
      "cnt: 0 - valLoss: 0.48017755150794983 - trainLoss: 0.48524197936058044\n",
      "cnt: 0 - valLoss: 0.48016417026519775 - trainLoss: 0.48523038625717163\n",
      "cnt: 0 - valLoss: 0.48015087842941284 - trainLoss: 0.4852187931537628\n",
      "cnt: 0 - valLoss: 0.48013755679130554 - trainLoss: 0.485207200050354\n",
      "cnt: 0 - valLoss: 0.48012423515319824 - trainLoss: 0.4851956069469452\n",
      "cnt: 0 - valLoss: 0.48011094331741333 - trainLoss: 0.4851839542388916\n",
      "cnt: 0 - valLoss: 0.48009762167930603 - trainLoss: 0.4851723909378052\n",
      "cnt: 0 - valLoss: 0.4800843596458435 - trainLoss: 0.48516082763671875\n",
      "cnt: 0 - valLoss: 0.4800710380077362 - trainLoss: 0.48514920473098755\n",
      "cnt: 0 - valLoss: 0.4800577461719513 - trainLoss: 0.4851376414299011\n",
      "cnt: 0 - valLoss: 0.4800444543361664 - trainLoss: 0.4851260781288147\n",
      "cnt: 0 - valLoss: 0.4800311326980591 - trainLoss: 0.4851144850254059\n",
      "cnt: 0 - valLoss: 0.48001784086227417 - trainLoss: 0.48510292172431946\n",
      "cnt: 0 - valLoss: 0.48000454902648926 - trainLoss: 0.48509135842323303\n",
      "cnt: 0 - valLoss: 0.4799913465976715 - trainLoss: 0.4850797653198242\n",
      "cnt: 0 - valLoss: 0.4799780547618866 - trainLoss: 0.4850682020187378\n",
      "cnt: 0 - valLoss: 0.47996482253074646 - trainLoss: 0.48505666851997375\n",
      "cnt: 0 - valLoss: 0.47995153069496155 - trainLoss: 0.48504510521888733\n",
      "cnt: 0 - valLoss: 0.4799382984638214 - trainLoss: 0.4850335419178009\n",
      "cnt: 0 - valLoss: 0.4799250066280365 - trainLoss: 0.4850219786167145\n",
      "cnt: 0 - valLoss: 0.47991177439689636 - trainLoss: 0.48501044511795044\n",
      "cnt: 0 - valLoss: 0.47989851236343384 - trainLoss: 0.4849989116191864\n",
      "cnt: 0 - valLoss: 0.4798852801322937 - trainLoss: 0.48498740792274475\n",
      "cnt: 0 - valLoss: 0.4798720180988312 - trainLoss: 0.4849758446216583\n",
      "cnt: 0 - valLoss: 0.47985875606536865 - trainLoss: 0.4849643409252167\n",
      "cnt: 0 - valLoss: 0.47984549403190613 - trainLoss: 0.48495280742645264\n",
      "cnt: 0 - valLoss: 0.479832261800766 - trainLoss: 0.4849412739276886\n",
      "cnt: 0 - valLoss: 0.47981905937194824 - trainLoss: 0.48492974042892456\n",
      "cnt: 0 - valLoss: 0.4798057973384857 - trainLoss: 0.4849182367324829\n",
      "cnt: 0 - valLoss: 0.4797925651073456 - trainLoss: 0.48490673303604126\n",
      "cnt: 0 - valLoss: 0.47977936267852783 - trainLoss: 0.484895259141922\n",
      "cnt: 0 - valLoss: 0.4797660708427429 - trainLoss: 0.48488372564315796\n",
      "cnt: 0 - valLoss: 0.47975292801856995 - trainLoss: 0.4848721921443939\n",
      "cnt: 0 - valLoss: 0.4797396659851074 - trainLoss: 0.48486071825027466\n",
      "cnt: 0 - valLoss: 0.4797264635562897 - trainLoss: 0.4848492443561554\n",
      "cnt: 0 - valLoss: 0.4797132611274719 - trainLoss: 0.48483777046203613\n",
      "cnt: 0 - valLoss: 0.4797000586986542 - trainLoss: 0.4848262667655945\n",
      "cnt: 0 - valLoss: 0.4796868562698364 - trainLoss: 0.4848147928714752\n",
      "cnt: 0 - valLoss: 0.4796736538410187 - trainLoss: 0.48480331897735596\n",
      "cnt: 0 - valLoss: 0.4796605110168457 - trainLoss: 0.4847918152809143\n",
      "cnt: 0 - valLoss: 0.47964730858802795 - trainLoss: 0.48478034138679504\n",
      "cnt: 0 - valLoss: 0.4796341061592102 - trainLoss: 0.4847688674926758\n",
      "cnt: 0 - valLoss: 0.47962093353271484 - trainLoss: 0.4847573935985565\n",
      "cnt: 0 - valLoss: 0.47960779070854187 - trainLoss: 0.48474591970443726\n",
      "cnt: 0 - valLoss: 0.4795945882797241 - trainLoss: 0.48473453521728516\n",
      "cnt: 0 - valLoss: 0.47958141565322876 - trainLoss: 0.4847230315208435\n",
      "cnt: 0 - valLoss: 0.4795683026313782 - trainLoss: 0.48471158742904663\n",
      "cnt: 0 - valLoss: 0.4795551002025604 - trainLoss: 0.48470011353492737\n",
      "cnt: 0 - valLoss: 0.47954198718070984 - trainLoss: 0.4846886992454529\n",
      "cnt: 0 - valLoss: 0.4795287847518921 - trainLoss: 0.484677255153656\n",
      "cnt: 0 - valLoss: 0.4795156717300415 - trainLoss: 0.48466578125953674\n",
      "cnt: 0 - valLoss: 0.4795025587081909 - trainLoss: 0.48465436697006226\n",
      "cnt: 0 - valLoss: 0.47948941588401794 - trainLoss: 0.48464295268058777\n",
      "cnt: 0 - valLoss: 0.4794762432575226 - trainLoss: 0.4846314787864685\n",
      "cnt: 0 - valLoss: 0.479463130235672 - trainLoss: 0.4846200942993164\n",
      "cnt: 0 - valLoss: 0.4794500172138214 - trainLoss: 0.48460865020751953\n",
      "cnt: 0 - valLoss: 0.4794369041919708 - trainLoss: 0.48459723591804504\n",
      "cnt: 0 - valLoss: 0.47942379117012024 - trainLoss: 0.48458585143089294\n",
      "cnt: 0 - valLoss: 0.47941067814826965 - trainLoss: 0.48457440733909607\n",
      "cnt: 0 - valLoss: 0.4793975353240967 - trainLoss: 0.4845629930496216\n",
      "cnt: 0 - valLoss: 0.47938448190689087 - trainLoss: 0.4845516085624695\n",
      "cnt: 0 - valLoss: 0.4793713092803955 - trainLoss: 0.4845402240753174\n",
      "cnt: 0 - valLoss: 0.4793582856655121 - trainLoss: 0.4845287799835205\n",
      "cnt: 0 - valLoss: 0.4793451726436615 - trainLoss: 0.4845173954963684\n",
      "cnt: 0 - valLoss: 0.4793320894241333 - trainLoss: 0.4845059812068939\n",
      "cnt: 0 - valLoss: 0.4793189764022827 - trainLoss: 0.4844945967197418\n",
      "cnt: 0 - valLoss: 0.4793059229850769 - trainLoss: 0.4844832122325897\n",
      "cnt: 0 - valLoss: 0.4792928695678711 - trainLoss: 0.48447185754776\n",
      "cnt: 0 - valLoss: 0.4792798161506653 - trainLoss: 0.4844604432582855\n",
      "cnt: 0 - valLoss: 0.4792667329311371 - trainLoss: 0.4844490885734558\n",
      "cnt: 0 - valLoss: 0.4792536497116089 - trainLoss: 0.4844377040863037\n",
      "cnt: 0 - valLoss: 0.4792405962944031 - trainLoss: 0.484426349401474\n",
      "cnt: 0 - valLoss: 0.47922757267951965 - trainLoss: 0.4844149649143219\n",
      "cnt: 0 - valLoss: 0.47921448945999146 - trainLoss: 0.4844035804271698\n",
      "cnt: 0 - valLoss: 0.47920146584510803 - trainLoss: 0.4843922555446625\n",
      "cnt: 0 - valLoss: 0.47918838262557983 - trainLoss: 0.48438090085983276\n",
      "cnt: 0 - valLoss: 0.4791753590106964 - trainLoss: 0.48436951637268066\n",
      "cnt: 0 - valLoss: 0.479162335395813 - trainLoss: 0.4843582212924957\n",
      "cnt: 0 - valLoss: 0.4791492819786072 - trainLoss: 0.48434683680534363\n",
      "cnt: 0 - valLoss: 0.47913625836372375 - trainLoss: 0.4843355417251587\n",
      "cnt: 0 - valLoss: 0.47912323474884033 - trainLoss: 0.4843241572380066\n",
      "cnt: 0 - valLoss: 0.4791102409362793 - trainLoss: 0.48431286215782166\n",
      "cnt: 0 - valLoss: 0.47909724712371826 - trainLoss: 0.48430150747299194\n",
      "cnt: 0 - valLoss: 0.47908422350883484 - trainLoss: 0.48429015278816223\n",
      "cnt: 0 - valLoss: 0.4790711998939514 - trainLoss: 0.4842788279056549\n",
      "cnt: 0 - valLoss: 0.4790582060813904 - trainLoss: 0.48426753282546997\n",
      "cnt: 0 - valLoss: 0.47904521226882935 - trainLoss: 0.48425617814064026\n",
      "cnt: 0 - valLoss: 0.4790321886539459 - trainLoss: 0.4842448830604553\n",
      "cnt: 0 - valLoss: 0.47901925444602966 - trainLoss: 0.4842335879802704\n",
      "cnt: 0 - valLoss: 0.47900623083114624 - trainLoss: 0.48422229290008545\n",
      "cnt: 0 - valLoss: 0.4789932668209076 - trainLoss: 0.4842109978199005\n",
      "cnt: 0 - valLoss: 0.47898030281066895 - trainLoss: 0.4841996431350708\n",
      "cnt: 0 - valLoss: 0.4789673089981079 - trainLoss: 0.48418837785720825\n",
      "cnt: 0 - valLoss: 0.47895434498786926 - trainLoss: 0.4841770529747009\n",
      "cnt: 0 - valLoss: 0.478941410779953 - trainLoss: 0.4841657876968384\n",
      "cnt: 0 - valLoss: 0.47892841696739197 - trainLoss: 0.48415449261665344\n",
      "cnt: 0 - valLoss: 0.4789154529571533 - trainLoss: 0.4841431975364685\n",
      "cnt: 0 - valLoss: 0.4789024889469147 - trainLoss: 0.48413193225860596\n",
      "cnt: 0 - valLoss: 0.4788895547389984 - trainLoss: 0.484120637178421\n",
      "cnt: 0 - valLoss: 0.4788765609264374 - trainLoss: 0.48410937190055847\n",
      "cnt: 0 - valLoss: 0.4788636565208435 - trainLoss: 0.48409807682037354\n",
      "cnt: 0 - valLoss: 0.47885072231292725 - trainLoss: 0.484086811542511\n",
      "cnt: 0 - valLoss: 0.47883784770965576 - trainLoss: 0.48407554626464844\n",
      "cnt: 0 - valLoss: 0.47882482409477234 - trainLoss: 0.48406434059143066\n",
      "cnt: 0 - valLoss: 0.4788118898868561 - trainLoss: 0.4840530455112457\n",
      "cnt: 0 - valLoss: 0.4787989556789398 - trainLoss: 0.4840417504310608\n",
      "cnt: 0 - valLoss: 0.4787859618663788 - trainLoss: 0.484030544757843\n",
      "cnt: 0 - valLoss: 0.47877299785614014 - trainLoss: 0.48401930928230286\n",
      "cnt: 0 - valLoss: 0.4787600636482239 - trainLoss: 0.4840080738067627\n",
      "cnt: 0 - valLoss: 0.4787471294403076 - trainLoss: 0.48399680852890015\n",
      "cnt: 0 - valLoss: 0.47873422503471375 - trainLoss: 0.4839856028556824\n",
      "cnt: 0 - valLoss: 0.4787213206291199 - trainLoss: 0.4839743673801422\n",
      "cnt: 0 - valLoss: 0.478708416223526 - trainLoss: 0.48396316170692444\n",
      "cnt: 0 - valLoss: 0.47869548201560974 - trainLoss: 0.48395195603370667\n",
      "cnt: 0 - valLoss: 0.47868257761001587 - trainLoss: 0.4839406907558441\n",
      "cnt: 0 - valLoss: 0.478669673204422 - trainLoss: 0.48392948508262634\n",
      "cnt: 0 - valLoss: 0.4786567986011505 - trainLoss: 0.48391830921173096\n",
      "cnt: 0 - valLoss: 0.47864392399787903 - trainLoss: 0.4839071035385132\n",
      "cnt: 0 - valLoss: 0.4786309599876404 - trainLoss: 0.483895868062973\n",
      "cnt: 0 - valLoss: 0.4786180555820465 - trainLoss: 0.48388466238975525\n",
      "cnt: 0 - valLoss: 0.4786052107810974 - trainLoss: 0.4838734567165375\n",
      "cnt: 0 - valLoss: 0.4785923361778259 - trainLoss: 0.4838622510433197\n",
      "cnt: 0 - valLoss: 0.47857949137687683 - trainLoss: 0.4838510751724243\n",
      "cnt: 0 - valLoss: 0.47856661677360535 - trainLoss: 0.48383983969688416\n",
      "cnt: 0 - valLoss: 0.47855374217033386 - trainLoss: 0.48382869362831116\n",
      "cnt: 0 - valLoss: 0.4785408675670624 - trainLoss: 0.48381751775741577\n",
      "cnt: 0 - valLoss: 0.4785280227661133 - trainLoss: 0.4838063418865204\n",
      "cnt: 0 - valLoss: 0.4785151481628418 - trainLoss: 0.483795166015625\n",
      "cnt: 0 - valLoss: 0.4785023331642151 - trainLoss: 0.4837839901447296\n",
      "cnt: 0 - valLoss: 0.478489488363266 - trainLoss: 0.48377278447151184\n",
      "cnt: 0 - valLoss: 0.4784766137599945 - trainLoss: 0.48376163840293884\n",
      "cnt: 0 - valLoss: 0.4784637689590454 - trainLoss: 0.48375043272972107\n",
      "cnt: 0 - valLoss: 0.4784509837627411 - trainLoss: 0.48373931646347046\n",
      "cnt: 0 - valLoss: 0.47843822836875916 - trainLoss: 0.48372817039489746\n",
      "cnt: 0 - valLoss: 0.47842544317245483 - trainLoss: 0.48371702432632446\n",
      "cnt: 0 - valLoss: 0.4784127473831177 - trainLoss: 0.48370593786239624\n",
      "cnt: 0 - valLoss: 0.47839999198913574 - trainLoss: 0.48369479179382324\n",
      "cnt: 0 - valLoss: 0.4783872663974762 - trainLoss: 0.48368367552757263\n",
      "cnt: 0 - valLoss: 0.4783744812011719 - trainLoss: 0.4836725890636444\n",
      "cnt: 0 - valLoss: 0.47836175560951233 - trainLoss: 0.4836614429950714\n",
      "cnt: 0 - valLoss: 0.47834905982017517 - trainLoss: 0.4836503565311432\n",
      "cnt: 0 - valLoss: 0.4783363342285156 - trainLoss: 0.4836392402648926\n",
      "cnt: 0 - valLoss: 0.4783236086368561 - trainLoss: 0.48362815380096436\n",
      "cnt: 0 - valLoss: 0.47831082344055176 - trainLoss: 0.48361706733703613\n",
      "cnt: 0 - valLoss: 0.478298157453537 - trainLoss: 0.4836059510707855\n",
      "cnt: 0 - valLoss: 0.47828546166419983 - trainLoss: 0.4835948646068573\n",
      "cnt: 0 - valLoss: 0.47827276587486267 - trainLoss: 0.4835837781429291\n",
      "cnt: 0 - valLoss: 0.4782600402832031 - trainLoss: 0.48357272148132324\n",
      "cnt: 0 - valLoss: 0.47824734449386597 - trainLoss: 0.48356160521507263\n",
      "cnt: 0 - valLoss: 0.4782346785068512 - trainLoss: 0.4835505485534668\n",
      "cnt: 0 - valLoss: 0.47822192311286926 - trainLoss: 0.4835394620895386\n",
      "cnt: 0 - valLoss: 0.4782092869281769 - trainLoss: 0.48352837562561035\n",
      "cnt: 0 - valLoss: 0.4781965911388397 - trainLoss: 0.4835173487663269\n",
      "cnt: 0 - valLoss: 0.47818392515182495 - trainLoss: 0.48350629210472107\n",
      "cnt: 0 - valLoss: 0.4781712293624878 - trainLoss: 0.48349517583847046\n",
      "cnt: 0 - valLoss: 0.4781585931777954 - trainLoss: 0.4834841191768646\n",
      "cnt: 0 - valLoss: 0.47814589738845825 - trainLoss: 0.4834730923175812\n",
      "cnt: 0 - valLoss: 0.47813326120376587 - trainLoss: 0.48346203565597534\n",
      "cnt: 0 - valLoss: 0.4781206250190735 - trainLoss: 0.4834510087966919\n",
      "cnt: 0 - valLoss: 0.4781079590320587 - trainLoss: 0.48343995213508606\n",
      "cnt: 0 - valLoss: 0.47809529304504395 - trainLoss: 0.4834289252758026\n",
      "cnt: 0 - valLoss: 0.4780826270580292 - trainLoss: 0.4834178686141968\n",
      "cnt: 0 - valLoss: 0.4780699908733368 - trainLoss: 0.48340681195259094\n",
      "cnt: 0 - valLoss: 0.4780573546886444 - trainLoss: 0.4833958148956299\n",
      "cnt: 0 - valLoss: 0.47804465889930725 - trainLoss: 0.4833848178386688\n",
      "cnt: 0 - valLoss: 0.47803205251693726 - trainLoss: 0.4833737909793854\n",
      "cnt: 0 - valLoss: 0.4780194163322449 - trainLoss: 0.4833628237247467\n",
      "cnt: 0 - valLoss: 0.4780067801475525 - trainLoss: 0.48335179686546326\n",
      "cnt: 0 - valLoss: 0.47799408435821533 - trainLoss: 0.4833407402038574\n",
      "cnt: 0 - valLoss: 0.47798147797584534 - trainLoss: 0.48332977294921875\n",
      "cnt: 0 - valLoss: 0.47796887159347534 - trainLoss: 0.4833187460899353\n",
      "cnt: 0 - valLoss: 0.47795623540878296 - trainLoss: 0.48330777883529663\n",
      "cnt: 0 - valLoss: 0.4779435992240906 - trainLoss: 0.48329678177833557\n",
      "cnt: 0 - valLoss: 0.4779309928417206 - trainLoss: 0.4832857847213745\n",
      "cnt: 0 - valLoss: 0.4779183864593506 - trainLoss: 0.48327481746673584\n",
      "cnt: 0 - valLoss: 0.4779057800769806 - trainLoss: 0.4832637906074524\n",
      "cnt: 0 - valLoss: 0.4778931736946106 - trainLoss: 0.4832528233528137\n",
      "cnt: 0 - valLoss: 0.477880597114563 - trainLoss: 0.48324185609817505\n",
      "cnt: 0 - valLoss: 0.4778680205345154 - trainLoss: 0.48323091864585876\n",
      "cnt: 0 - valLoss: 0.477855384349823 - trainLoss: 0.4832199215888977\n",
      "cnt: 0 - valLoss: 0.47784286737442017 - trainLoss: 0.48320895433425903\n",
      "cnt: 0 - valLoss: 0.47783026099205017 - trainLoss: 0.48319804668426514\n",
      "cnt: 0 - valLoss: 0.4778176546096802 - trainLoss: 0.4831870496273041\n",
      "cnt: 0 - valLoss: 0.47780513763427734 - trainLoss: 0.4831760823726654\n",
      "cnt: 0 - valLoss: 0.47779256105422974 - trainLoss: 0.48316511511802673\n",
      "cnt: 0 - valLoss: 0.47777998447418213 - trainLoss: 0.48315420746803284\n",
      "cnt: 0 - valLoss: 0.4777674376964569 - trainLoss: 0.48314324021339417\n",
      "cnt: 0 - valLoss: 0.4777548611164093 - trainLoss: 0.4831323027610779\n",
      "cnt: 0 - valLoss: 0.4777422845363617 - trainLoss: 0.4831213355064392\n",
      "cnt: 0 - valLoss: 0.47772976756095886 - trainLoss: 0.4831104278564453\n",
      "cnt: 0 - valLoss: 0.47771716117858887 - trainLoss: 0.4830995202064514\n",
      "cnt: 0 - valLoss: 0.47770458459854126 - trainLoss: 0.48308855295181274\n",
      "cnt: 0 - valLoss: 0.4776920676231384 - trainLoss: 0.48307767510414124\n",
      "cnt: 0 - valLoss: 0.4776794910430908 - trainLoss: 0.48306673765182495\n",
      "cnt: 0 - valLoss: 0.477666974067688 - trainLoss: 0.48305585980415344\n",
      "cnt: 0 - valLoss: 0.4776543974876404 - trainLoss: 0.48304489254951477\n",
      "cnt: 0 - valLoss: 0.47764185070991516 - trainLoss: 0.48303401470184326\n",
      "cnt: 0 - valLoss: 0.47762927412986755 - trainLoss: 0.48302310705184937\n",
      "cnt: 0 - valLoss: 0.47761672735214233 - trainLoss: 0.48301219940185547\n",
      "cnt: 0 - valLoss: 0.47760412096977234 - trainLoss: 0.48300132155418396\n",
      "cnt: 0 - valLoss: 0.4775916635990143 - trainLoss: 0.48299041390419006\n",
      "cnt: 0 - valLoss: 0.47757914662361145 - trainLoss: 0.48297956585884094\n",
      "cnt: 0 - valLoss: 0.47756657004356384 - trainLoss: 0.48296862840652466\n",
      "cnt: 0 - valLoss: 0.4775540232658386 - trainLoss: 0.48295778036117554\n",
      "cnt: 0 - valLoss: 0.4775415062904358 - trainLoss: 0.4829469323158264\n",
      "cnt: 0 - valLoss: 0.47752901911735535 - trainLoss: 0.48293599486351013\n",
      "cnt: 0 - valLoss: 0.4775164723396301 - trainLoss: 0.4829251766204834\n",
      "cnt: 0 - valLoss: 0.4775039553642273 - trainLoss: 0.4829142987728119\n",
      "cnt: 0 - valLoss: 0.47749143838882446 - trainLoss: 0.48290345072746277\n",
      "cnt: 0 - valLoss: 0.4774789810180664 - trainLoss: 0.48289257287979126\n",
      "cnt: 0 - valLoss: 0.4774664640426636 - trainLoss: 0.48288172483444214\n",
      "cnt: 0 - valLoss: 0.47745394706726074 - trainLoss: 0.482870876789093\n",
      "cnt: 0 - valLoss: 0.4774414598941803 - trainLoss: 0.4828599989414215\n",
      "cnt: 0 - valLoss: 0.47742900252342224 - trainLoss: 0.4828491508960724\n",
      "cnt: 0 - valLoss: 0.4774164855480194 - trainLoss: 0.48283833265304565\n",
      "cnt: 0 - valLoss: 0.47740399837493896 - trainLoss: 0.4828275144100189\n",
      "cnt: 0 - valLoss: 0.4773915410041809 - trainLoss: 0.4828166663646698\n",
      "cnt: 0 - valLoss: 0.47737905383110046 - trainLoss: 0.4828058183193207\n",
      "cnt: 0 - valLoss: 0.4773665964603424 - trainLoss: 0.48279494047164917\n",
      "cnt: 0 - valLoss: 0.47735416889190674 - trainLoss: 0.4827841520309448\n",
      "cnt: 0 - valLoss: 0.4773416817188263 - trainLoss: 0.4827733337879181\n",
      "cnt: 0 - valLoss: 0.47732922434806824 - trainLoss: 0.48276248574256897\n",
      "cnt: 0 - valLoss: 0.4773167669773102 - trainLoss: 0.4827516973018646\n",
      "cnt: 0 - valLoss: 0.4773043394088745 - trainLoss: 0.4827409088611603\n",
      "cnt: 0 - valLoss: 0.47729191184043884 - trainLoss: 0.48273006081581116\n",
      "cnt: 0 - valLoss: 0.4772794544696808 - trainLoss: 0.4827192723751068\n",
      "cnt: 0 - valLoss: 0.4772670269012451 - trainLoss: 0.48270848393440247\n",
      "cnt: 0 - valLoss: 0.47725459933280945 - trainLoss: 0.48269763588905334\n",
      "cnt: 0 - valLoss: 0.4772421717643738 - trainLoss: 0.4826869070529938\n",
      "cnt: 0 - valLoss: 0.4772297143936157 - trainLoss: 0.48267608880996704\n",
      "cnt: 0 - valLoss: 0.47721734642982483 - trainLoss: 0.4826652407646179\n",
      "cnt: 0 - valLoss: 0.47720491886138916 - trainLoss: 0.48265451192855835\n",
      "cnt: 0 - valLoss: 0.4771925210952759 - trainLoss: 0.482643723487854\n",
      "cnt: 0 - valLoss: 0.4771801233291626 - trainLoss: 0.4826328754425049\n",
      "cnt: 0 - valLoss: 0.47716769576072693 - trainLoss: 0.4826221764087677\n",
      "cnt: 0 - valLoss: 0.47715526819229126 - trainLoss: 0.48261138796806335\n",
      "cnt: 0 - valLoss: 0.47714293003082275 - trainLoss: 0.4826006293296814\n",
      "cnt: 0 - valLoss: 0.4771305024623871 - trainLoss: 0.48258984088897705\n",
      "cnt: 0 - valLoss: 0.4771181643009186 - trainLoss: 0.4825790822505951\n",
      "cnt: 0 - valLoss: 0.4771057367324829 - trainLoss: 0.48256829380989075\n",
      "cnt: 0 - valLoss: 0.4770933985710144 - trainLoss: 0.4825575649738312\n",
      "cnt: 0 - valLoss: 0.4770810008049011 - trainLoss: 0.48254677653312683\n",
      "cnt: 0 - valLoss: 0.47706860303878784 - trainLoss: 0.48253607749938965\n",
      "cnt: 0 - valLoss: 0.47705626487731934 - trainLoss: 0.4825252890586853\n",
      "cnt: 0 - valLoss: 0.47704392671585083 - trainLoss: 0.48251453042030334\n",
      "cnt: 0 - valLoss: 0.4770315885543823 - trainLoss: 0.4825038015842438\n",
      "cnt: 0 - valLoss: 0.4770192801952362 - trainLoss: 0.4824931025505066\n",
      "cnt: 0 - valLoss: 0.4770069718360901 - trainLoss: 0.4824824035167694\n",
      "cnt: 0 - valLoss: 0.47699466347694397 - trainLoss: 0.48247161507606506\n",
      "cnt: 0 - valLoss: 0.47698232531547546 - trainLoss: 0.4824609160423279\n",
      "cnt: 0 - valLoss: 0.47697001695632935 - trainLoss: 0.4824502170085907\n",
      "cnt: 0 - valLoss: 0.4769577383995056 - trainLoss: 0.4824395179748535\n",
      "cnt: 0 - valLoss: 0.4769454300403595 - trainLoss: 0.48242875933647156\n",
      "cnt: 0 - valLoss: 0.47693318128585815 - trainLoss: 0.48241811990737915\n",
      "cnt: 0 - valLoss: 0.47692087292671204 - trainLoss: 0.48240742087364197\n",
      "cnt: 0 - valLoss: 0.4769085645675659 - trainLoss: 0.4823967218399048\n",
      "cnt: 0 - valLoss: 0.4768963158130646 - trainLoss: 0.4823859930038452\n",
      "cnt: 0 - valLoss: 0.47688403725624084 - trainLoss: 0.48237529397010803\n",
      "cnt: 0 - valLoss: 0.4768717885017395 - trainLoss: 0.48236459493637085\n",
      "cnt: 0 - valLoss: 0.4768594801425934 - trainLoss: 0.48235389590263367\n",
      "cnt: 0 - valLoss: 0.47684720158576965 - trainLoss: 0.4823431968688965\n",
      "cnt: 0 - valLoss: 0.4768349826335907 - trainLoss: 0.4823325574398041\n",
      "cnt: 0 - valLoss: 0.47682276368141174 - trainLoss: 0.4823218882083893\n",
      "cnt: 0 - valLoss: 0.4768105447292328 - trainLoss: 0.4823111593723297\n",
      "cnt: 0 - valLoss: 0.47679832577705383 - trainLoss: 0.4823004901409149\n",
      "cnt: 0 - valLoss: 0.4767860770225525 - trainLoss: 0.48228979110717773\n",
      "cnt: 0 - valLoss: 0.47677385807037354 - trainLoss: 0.48227912187576294\n",
      "cnt: 0 - valLoss: 0.4767616391181946 - trainLoss: 0.48226848244667053\n",
      "cnt: 0 - valLoss: 0.476749449968338 - trainLoss: 0.48225778341293335\n",
      "cnt: 0 - valLoss: 0.47673726081848145 - trainLoss: 0.48224714398384094\n",
      "cnt: 0 - valLoss: 0.4767250418663025 - trainLoss: 0.48223650455474854\n",
      "cnt: 0 - valLoss: 0.4767128527164459 - trainLoss: 0.48222583532333374\n",
      "cnt: 0 - valLoss: 0.47670063376426697 - trainLoss: 0.48221519589424133\n",
      "cnt: 0 - valLoss: 0.4766884744167328 - trainLoss: 0.48220452666282654\n",
      "cnt: 0 - valLoss: 0.47667625546455383 - trainLoss: 0.48219382762908936\n",
      "cnt: 0 - valLoss: 0.47666409611701965 - trainLoss: 0.48218321800231934\n",
      "cnt: 0 - valLoss: 0.4766519069671631 - trainLoss: 0.4821726083755493\n",
      "cnt: 0 - valLoss: 0.47663968801498413 - trainLoss: 0.4821619391441345\n",
      "cnt: 0 - valLoss: 0.47662755846977234 - trainLoss: 0.4821513295173645\n",
      "cnt: 0 - valLoss: 0.47661539912223816 - trainLoss: 0.4821406602859497\n",
      "cnt: 0 - valLoss: 0.4766032099723816 - trainLoss: 0.4821300506591797\n",
      "cnt: 0 - valLoss: 0.4765910506248474 - trainLoss: 0.48211944103240967\n",
      "cnt: 0 - valLoss: 0.47657886147499084 - trainLoss: 0.4821087718009949\n",
      "cnt: 0 - valLoss: 0.47656673192977905 - trainLoss: 0.48209816217422485\n",
      "cnt: 0 - valLoss: 0.47655460238456726 - trainLoss: 0.4820875823497772\n",
      "cnt: 0 - valLoss: 0.47654247283935547 - trainLoss: 0.4820769429206848\n",
      "cnt: 0 - valLoss: 0.4765303432941437 - trainLoss: 0.4820663332939148\n",
      "cnt: 0 - valLoss: 0.4765181839466095 - trainLoss: 0.48205575346946716\n",
      "cnt: 0 - valLoss: 0.4765060544013977 - trainLoss: 0.48204514384269714\n",
      "cnt: 0 - valLoss: 0.4764939248561859 - trainLoss: 0.4820345342159271\n",
      "cnt: 0 - valLoss: 0.4764818251132965 - trainLoss: 0.4820238947868347\n",
      "cnt: 0 - valLoss: 0.4764696955680847 - trainLoss: 0.4820133447647095\n",
      "cnt: 0 - valLoss: 0.4764575660228729 - trainLoss: 0.48200270533561707\n",
      "cnt: 0 - valLoss: 0.4764454960823059 - trainLoss: 0.4819921553134918\n",
      "cnt: 0 - valLoss: 0.4764333665370941 - trainLoss: 0.4819815754890442\n",
      "cnt: 0 - valLoss: 0.4764212369918823 - trainLoss: 0.48197099566459656\n",
      "cnt: 0 - valLoss: 0.4764091372489929 - trainLoss: 0.48196038603782654\n",
      "cnt: 0 - valLoss: 0.4763970673084259 - trainLoss: 0.4819498658180237\n",
      "cnt: 0 - valLoss: 0.4763849675655365 - trainLoss: 0.48193925619125366\n",
      "cnt: 0 - valLoss: 0.4763728678226471 - trainLoss: 0.4819287061691284\n",
      "cnt: 0 - valLoss: 0.47636088728904724 - trainLoss: 0.4819180965423584\n",
      "cnt: 0 - valLoss: 0.47634878754615784 - trainLoss: 0.48190751671791077\n",
      "cnt: 0 - valLoss: 0.4763367474079132 - trainLoss: 0.4818970263004303\n",
      "cnt: 0 - valLoss: 0.4763247072696686 - trainLoss: 0.4818864166736603\n",
      "cnt: 0 - valLoss: 0.4763126075267792 - trainLoss: 0.4818758964538574\n",
      "cnt: 0 - valLoss: 0.47630056738853455 - trainLoss: 0.48186537623405457\n",
      "cnt: 0 - valLoss: 0.4762885570526123 - trainLoss: 0.48185479640960693\n",
      "cnt: 0 - valLoss: 0.4762765169143677 - trainLoss: 0.4818442165851593\n",
      "cnt: 0 - valLoss: 0.47626444697380066 - trainLoss: 0.48183369636535645\n",
      "cnt: 0 - valLoss: 0.4762524366378784 - trainLoss: 0.4818231761455536\n",
      "cnt: 0 - valLoss: 0.4762404263019562 - trainLoss: 0.48181262612342834\n",
      "cnt: 0 - valLoss: 0.47622838616371155 - trainLoss: 0.4818021059036255\n",
      "cnt: 0 - valLoss: 0.4762164056301117 - trainLoss: 0.48179158568382263\n",
      "cnt: 0 - valLoss: 0.4762043356895447 - trainLoss: 0.4817810356616974\n",
      "cnt: 0 - valLoss: 0.4761923551559448 - trainLoss: 0.48177051544189453\n",
      "cnt: 0 - valLoss: 0.4761803448200226 - trainLoss: 0.4817599952220917\n",
      "cnt: 0 - valLoss: 0.47616833448410034 - trainLoss: 0.4817495048046112\n",
      "cnt: 0 - valLoss: 0.4761563539505005 - trainLoss: 0.48173901438713074\n",
      "cnt: 0 - valLoss: 0.47614437341690063 - trainLoss: 0.4817284941673279\n",
      "cnt: 0 - valLoss: 0.4761323928833008 - trainLoss: 0.4817180037498474\n",
      "cnt: 0 - valLoss: 0.4761204123497009 - trainLoss: 0.48170748353004456\n",
      "cnt: 0 - valLoss: 0.4761084318161011 - trainLoss: 0.4816969931125641\n",
      "cnt: 0 - valLoss: 0.47609642148017883 - trainLoss: 0.48168644309043884\n",
      "cnt: 0 - valLoss: 0.47608450055122375 - trainLoss: 0.48167598247528076\n",
      "cnt: 0 - valLoss: 0.4760724902153015 - trainLoss: 0.4816654920578003\n",
      "cnt: 0 - valLoss: 0.4760605990886688 - trainLoss: 0.4816550314426422\n",
      "cnt: 0 - valLoss: 0.4760485887527466 - trainLoss: 0.48164451122283936\n",
      "cnt: 0 - valLoss: 0.4760366678237915 - trainLoss: 0.48163408041000366\n",
      "cnt: 0 - valLoss: 0.47602471709251404 - trainLoss: 0.4816235899925232\n",
      "cnt: 0 - valLoss: 0.4760127663612366 - trainLoss: 0.4816130995750427\n",
      "cnt: 0 - valLoss: 0.4760007858276367 - trainLoss: 0.48160263895988464\n",
      "cnt: 0 - valLoss: 0.47598883509635925 - trainLoss: 0.48159217834472656\n",
      "cnt: 0 - valLoss: 0.4759769141674042 - trainLoss: 0.4815816879272461\n",
      "cnt: 0 - valLoss: 0.4759649932384491 - trainLoss: 0.481571227312088\n",
      "cnt: 0 - valLoss: 0.4759531021118164 - trainLoss: 0.48156073689460754\n",
      "cnt: 0 - valLoss: 0.47594118118286133 - trainLoss: 0.48155030608177185\n",
      "cnt: 0 - valLoss: 0.47592926025390625 - trainLoss: 0.4815398156642914\n",
      "cnt: 0 - valLoss: 0.4759173095226288 - trainLoss: 0.4815294146537781\n",
      "cnt: 0 - valLoss: 0.4759054183959961 - trainLoss: 0.4815189838409424\n",
      "cnt: 0 - valLoss: 0.4758935272693634 - trainLoss: 0.4815085828304291\n",
      "cnt: 0 - valLoss: 0.4758816063404083 - trainLoss: 0.4814980924129486\n",
      "cnt: 0 - valLoss: 0.47586968541145325 - trainLoss: 0.4814876317977905\n",
      "cnt: 0 - valLoss: 0.47585782408714294 - trainLoss: 0.48147720098495483\n",
      "cnt: 0 - valLoss: 0.47584593296051025 - trainLoss: 0.48146679997444153\n",
      "cnt: 0 - valLoss: 0.4758340120315552 - trainLoss: 0.4814563989639282\n",
      "cnt: 0 - valLoss: 0.4758221507072449 - trainLoss: 0.48144596815109253\n",
      "cnt: 0 - valLoss: 0.47581028938293457 - trainLoss: 0.48143550753593445\n",
      "cnt: 0 - valLoss: 0.47579845786094666 - trainLoss: 0.48142510652542114\n",
      "cnt: 0 - valLoss: 0.47578656673431396 - trainLoss: 0.48141470551490784\n",
      "cnt: 0 - valLoss: 0.47577470541000366 - trainLoss: 0.48140430450439453\n",
      "cnt: 0 - valLoss: 0.47576287388801575 - trainLoss: 0.481393963098526\n",
      "cnt: 0 - valLoss: 0.47575101256370544 - trainLoss: 0.4813835024833679\n",
      "cnt: 0 - valLoss: 0.47573912143707275 - trainLoss: 0.4813731610774994\n",
      "cnt: 0 - valLoss: 0.47572728991508484 - trainLoss: 0.4813627600669861\n",
      "cnt: 0 - valLoss: 0.4757154583930969 - trainLoss: 0.4813523590564728\n",
      "cnt: 0 - valLoss: 0.475703626871109 - trainLoss: 0.48134198784828186\n",
      "cnt: 0 - valLoss: 0.4756917655467987 - trainLoss: 0.48133161664009094\n",
      "cnt: 0 - valLoss: 0.4756799638271332 - trainLoss: 0.48132118582725525\n",
      "cnt: 0 - valLoss: 0.47566813230514526 - trainLoss: 0.4813108444213867\n",
      "cnt: 0 - valLoss: 0.47565633058547974 - trainLoss: 0.4813004732131958\n",
      "cnt: 0 - valLoss: 0.4756444990634918 - trainLoss: 0.4812901020050049\n",
      "cnt: 0 - valLoss: 0.4756326973438263 - trainLoss: 0.48127976059913635\n",
      "cnt: 0 - valLoss: 0.4756208658218384 - trainLoss: 0.48126938939094543\n",
      "cnt: 0 - valLoss: 0.47560909390449524 - trainLoss: 0.4812590479850769\n",
      "cnt: 0 - valLoss: 0.4755972623825073 - trainLoss: 0.48124873638153076\n",
      "cnt: 0 - valLoss: 0.4755854606628418 - trainLoss: 0.48123833537101746\n",
      "cnt: 0 - valLoss: 0.4755736291408539 - trainLoss: 0.48122796416282654\n",
      "cnt: 0 - valLoss: 0.47556188702583313 - trainLoss: 0.4812175929546356\n",
      "cnt: 0 - valLoss: 0.47555011510849 - trainLoss: 0.48120734095573425\n",
      "cnt: 0 - valLoss: 0.47553831338882446 - trainLoss: 0.48119696974754333\n",
      "cnt: 0 - valLoss: 0.4755265414714813 - trainLoss: 0.4811866283416748\n",
      "cnt: 0 - valLoss: 0.4755147397518158 - trainLoss: 0.48117631673812866\n",
      "cnt: 0 - valLoss: 0.47550299763679504 - trainLoss: 0.48116594552993774\n",
      "cnt: 0 - valLoss: 0.4754912257194519 - trainLoss: 0.4811556339263916\n",
      "cnt: 0 - valLoss: 0.47547948360443115 - trainLoss: 0.4811452627182007\n",
      "cnt: 0 - valLoss: 0.4754676818847656 - trainLoss: 0.48113495111465454\n",
      "cnt: 0 - valLoss: 0.4754559397697449 - trainLoss: 0.4811246395111084\n",
      "cnt: 0 - valLoss: 0.4754441976547241 - trainLoss: 0.48111435770988464\n",
      "cnt: 0 - valLoss: 0.4754323959350586 - trainLoss: 0.4811040163040161\n",
      "cnt: 0 - valLoss: 0.4754207134246826 - trainLoss: 0.48109373450279236\n",
      "cnt: 0 - valLoss: 0.4754089117050171 - trainLoss: 0.4810834228992462\n",
      "cnt: 0 - valLoss: 0.47539713978767395 - trainLoss: 0.48107314109802246\n",
      "cnt: 0 - valLoss: 0.4753854274749756 - trainLoss: 0.4810628294944763\n",
      "cnt: 0 - valLoss: 0.47537368535995483 - trainLoss: 0.48105257749557495\n",
      "cnt: 0 - valLoss: 0.4753619432449341 - trainLoss: 0.4810422956943512\n",
      "cnt: 0 - valLoss: 0.47535014152526855 - trainLoss: 0.48103201389312744\n",
      "cnt: 0 - valLoss: 0.4753383994102478 - trainLoss: 0.4810217618942261\n",
      "cnt: 0 - valLoss: 0.4753267168998718 - trainLoss: 0.4810114800930023\n",
      "cnt: 0 - valLoss: 0.4753149747848511 - trainLoss: 0.48100119829177856\n",
      "cnt: 0 - valLoss: 0.4753032326698303 - trainLoss: 0.4809909164905548\n",
      "cnt: 0 - valLoss: 0.47529149055480957 - trainLoss: 0.48098066449165344\n",
      "cnt: 0 - valLoss: 0.4752797782421112 - trainLoss: 0.4809703826904297\n",
      "cnt: 0 - valLoss: 0.47526806592941284 - trainLoss: 0.4809601604938507\n",
      "cnt: 0 - valLoss: 0.4752563238143921 - trainLoss: 0.48094993829727173\n",
      "cnt: 0 - valLoss: 0.4752446711063385 - trainLoss: 0.480939656496048\n",
      "cnt: 0 - valLoss: 0.47523292899131775 - trainLoss: 0.480929434299469\n",
      "cnt: 0 - valLoss: 0.47522127628326416 - trainLoss: 0.4809191823005676\n",
      "cnt: 0 - valLoss: 0.4752095639705658 - trainLoss: 0.48090893030166626\n",
      "cnt: 0 - valLoss: 0.4751978814601898 - trainLoss: 0.48089873790740967\n",
      "cnt: 0 - valLoss: 0.47518616914749146 - trainLoss: 0.4808884561061859\n",
      "cnt: 0 - valLoss: 0.4751744568347931 - trainLoss: 0.4808782637119293\n",
      "cnt: 0 - valLoss: 0.4751628637313843 - trainLoss: 0.4808681011199951\n",
      "cnt: 0 - valLoss: 0.4751511514186859 - trainLoss: 0.48085781931877136\n",
      "cnt: 0 - valLoss: 0.4751395285129547 - trainLoss: 0.48084762692451477\n",
      "cnt: 0 - valLoss: 0.47512781620025635 - trainLoss: 0.4808374345302582\n",
      "cnt: 0 - valLoss: 0.47511616349220276 - trainLoss: 0.4808271825313568\n",
      "cnt: 0 - valLoss: 0.47510451078414917 - trainLoss: 0.4808170199394226\n",
      "cnt: 0 - valLoss: 0.4750928580760956 - trainLoss: 0.48080679774284363\n",
      "cnt: 0 - valLoss: 0.4750812351703644 - trainLoss: 0.48079660534858704\n",
      "cnt: 0 - valLoss: 0.4750695824623108 - trainLoss: 0.48078641295433044\n",
      "cnt: 0 - valLoss: 0.4750578701496124 - trainLoss: 0.48077625036239624\n",
      "cnt: 0 - valLoss: 0.4750462770462036 - trainLoss: 0.48076605796813965\n",
      "cnt: 0 - valLoss: 0.47503459453582764 - trainLoss: 0.4807558059692383\n",
      "cnt: 0 - valLoss: 0.47502294182777405 - trainLoss: 0.48074567317962646\n",
      "cnt: 0 - valLoss: 0.47501131892204285 - trainLoss: 0.4807354807853699\n",
      "cnt: 0 - valLoss: 0.47499966621398926 - trainLoss: 0.4807252883911133\n",
      "cnt: 0 - valLoss: 0.47498801350593567 - trainLoss: 0.48071515560150146\n",
      "cnt: 0 - valLoss: 0.47497639060020447 - trainLoss: 0.48070502281188965\n",
      "cnt: 0 - valLoss: 0.47496479749679565 - trainLoss: 0.48069483041763306\n",
      "cnt: 0 - valLoss: 0.4749531149864197 - trainLoss: 0.48068463802337646\n",
      "cnt: 0 - valLoss: 0.47494152188301086 - trainLoss: 0.4806744456291199\n",
      "cnt: 0 - valLoss: 0.47492989897727966 - trainLoss: 0.48066434264183044\n",
      "cnt: 0 - valLoss: 0.4749182462692261 - trainLoss: 0.48065418004989624\n",
      "cnt: 0 - valLoss: 0.47490665316581726 - trainLoss: 0.48064401745796204\n",
      "cnt: 0 - valLoss: 0.47489506006240845 - trainLoss: 0.4806338846683502\n",
      "cnt: 0 - valLoss: 0.47488343715667725 - trainLoss: 0.48062369227409363\n",
      "cnt: 0 - valLoss: 0.4748718738555908 - trainLoss: 0.4806135892868042\n",
      "cnt: 0 - valLoss: 0.47486019134521484 - trainLoss: 0.4806034564971924\n",
      "cnt: 0 - valLoss: 0.4748486280441284 - trainLoss: 0.4805932939052582\n",
      "cnt: 0 - valLoss: 0.474837064743042 - trainLoss: 0.48058319091796875\n",
      "cnt: 0 - valLoss: 0.4748254716396332 - trainLoss: 0.4805730879306793\n",
      "cnt: 0 - valLoss: 0.47481390833854675 - trainLoss: 0.4805629551410675\n",
      "cnt: 0 - valLoss: 0.47480228543281555 - trainLoss: 0.4805528223514557\n",
      "cnt: 0 - valLoss: 0.4747907221317291 - trainLoss: 0.48054271936416626\n",
      "cnt: 0 - valLoss: 0.4747791588306427 - trainLoss: 0.48053261637687683\n",
      "cnt: 0 - valLoss: 0.4747675657272339 - trainLoss: 0.4805224537849426\n",
      "cnt: 0 - valLoss: 0.47475600242614746 - trainLoss: 0.4805123507976532\n",
      "cnt: 0 - valLoss: 0.47474443912506104 - trainLoss: 0.4805022180080414\n",
      "cnt: 0 - valLoss: 0.4747328460216522 - trainLoss: 0.48049211502075195\n",
      "cnt: 0 - valLoss: 0.47472134232521057 - trainLoss: 0.4804820716381073\n",
      "cnt: 0 - valLoss: 0.47470974922180176 - trainLoss: 0.48047196865081787\n",
      "cnt: 0 - valLoss: 0.47469818592071533 - trainLoss: 0.48046186566352844\n",
      "cnt: 0 - valLoss: 0.4746866226196289 - trainLoss: 0.4804517924785614\n",
      "cnt: 0 - valLoss: 0.47467508912086487 - trainLoss: 0.480441689491272\n",
      "cnt: 0 - valLoss: 0.47466346621513367 - trainLoss: 0.48043158650398254\n",
      "cnt: 0 - valLoss: 0.47465193271636963 - trainLoss: 0.4804215431213379\n",
      "cnt: 0 - valLoss: 0.4746403992176056 - trainLoss: 0.48041146993637085\n",
      "cnt: 0 - valLoss: 0.47462883591651917 - trainLoss: 0.4804014265537262\n",
      "cnt: 0 - valLoss: 0.4746173620223999 - trainLoss: 0.48039132356643677\n",
      "cnt: 0 - valLoss: 0.47460582852363586 - trainLoss: 0.4803812503814697\n",
      "cnt: 0 - valLoss: 0.4745942950248718 - trainLoss: 0.48037123680114746\n",
      "cnt: 0 - valLoss: 0.4745827913284302 - trainLoss: 0.48036113381385803\n",
      "cnt: 0 - valLoss: 0.47457125782966614 - trainLoss: 0.48035112023353577\n",
      "cnt: 0 - valLoss: 0.4745597541332245 - trainLoss: 0.48034101724624634\n",
      "cnt: 0 - valLoss: 0.47454822063446045 - trainLoss: 0.4803310036659241\n",
      "cnt: 0 - valLoss: 0.4745366871356964 - trainLoss: 0.4803209602832794\n",
      "cnt: 0 - valLoss: 0.47452518343925476 - trainLoss: 0.4803108870983124\n",
      "cnt: 0 - valLoss: 0.4745137095451355 - trainLoss: 0.4803008735179901\n",
      "cnt: 0 - valLoss: 0.47450217604637146 - trainLoss: 0.4802907705307007\n",
      "cnt: 0 - valLoss: 0.4744907021522522 - trainLoss: 0.4802808165550232\n",
      "cnt: 0 - valLoss: 0.4744792580604553 - trainLoss: 0.48027074337005615\n",
      "cnt: 0 - valLoss: 0.47446775436401367 - trainLoss: 0.4802607297897339\n",
      "cnt: 0 - valLoss: 0.4744562804698944 - trainLoss: 0.4802507162094116\n",
      "cnt: 0 - valLoss: 0.47444477677345276 - trainLoss: 0.48024067282676697\n",
      "cnt: 0 - valLoss: 0.4744333028793335 - trainLoss: 0.4802306890487671\n",
      "cnt: 0 - valLoss: 0.47442179918289185 - trainLoss: 0.4802206754684448\n",
      "cnt: 0 - valLoss: 0.4744103252887726 - trainLoss: 0.48021066188812256\n",
      "cnt: 0 - valLoss: 0.4743988811969757 - trainLoss: 0.4802006483078003\n",
      "cnt: 0 - valLoss: 0.47438743710517883 - trainLoss: 0.480190634727478\n",
      "cnt: 0 - valLoss: 0.47437599301338196 - trainLoss: 0.48018062114715576\n",
      "cnt: 0 - valLoss: 0.4743645489215851 - trainLoss: 0.4801706075668335\n",
      "cnt: 0 - valLoss: 0.4743531048297882 - trainLoss: 0.4801606237888336\n",
      "cnt: 0 - valLoss: 0.47434166073799133 - trainLoss: 0.48015064001083374\n",
      "cnt: 0 - valLoss: 0.47433018684387207 - trainLoss: 0.48014068603515625\n",
      "cnt: 0 - valLoss: 0.4743187725543976 - trainLoss: 0.4801306128501892\n",
      "cnt: 0 - valLoss: 0.4743073582649231 - trainLoss: 0.4801206886768341\n",
      "cnt: 0 - valLoss: 0.4742959141731262 - trainLoss: 0.48011067509651184\n",
      "cnt: 0 - valLoss: 0.47428447008132935 - trainLoss: 0.48010075092315674\n",
      "cnt: 0 - valLoss: 0.47427308559417725 - trainLoss: 0.4800907373428345\n",
      "cnt: 0 - valLoss: 0.47426167130470276 - trainLoss: 0.480080783367157\n",
      "cnt: 0 - valLoss: 0.47425028681755066 - trainLoss: 0.4800707697868347\n",
      "cnt: 0 - valLoss: 0.4742388427257538 - trainLoss: 0.4800608456134796\n",
      "cnt: 0 - valLoss: 0.4742274284362793 - trainLoss: 0.48005083203315735\n",
      "cnt: 0 - valLoss: 0.4742160737514496 - trainLoss: 0.48004090785980225\n",
      "cnt: 0 - valLoss: 0.4742046594619751 - trainLoss: 0.48003098368644714\n",
      "cnt: 0 - valLoss: 0.474193274974823 - trainLoss: 0.48002099990844727\n",
      "cnt: 0 - valLoss: 0.4741818606853485 - trainLoss: 0.4800110161304474\n",
      "cnt: 0 - valLoss: 0.4741704761981964 - trainLoss: 0.4800010919570923\n",
      "cnt: 0 - valLoss: 0.4741591215133667 - trainLoss: 0.4799911379814148\n",
      "cnt: 0 - valLoss: 0.474147766828537 - trainLoss: 0.4799811840057373\n",
      "cnt: 0 - valLoss: 0.4741363525390625 - trainLoss: 0.4799712300300598\n",
      "cnt: 0 - valLoss: 0.4741249978542328 - trainLoss: 0.4799613058567047\n",
      "cnt: 0 - valLoss: 0.4741136431694031 - trainLoss: 0.4799513518810272\n",
      "cnt: 0 - valLoss: 0.47410228848457336 - trainLoss: 0.4799414873123169\n",
      "cnt: 0 - valLoss: 0.47409093379974365 - trainLoss: 0.4799315333366394\n",
      "cnt: 0 - valLoss: 0.47407954931259155 - trainLoss: 0.4799216091632843\n",
      "cnt: 0 - valLoss: 0.47406819462776184 - trainLoss: 0.4799117147922516\n",
      "cnt: 0 - valLoss: 0.47405683994293213 - trainLoss: 0.4799017608165741\n",
      "cnt: 0 - valLoss: 0.4740455150604248 - trainLoss: 0.4798918664455414\n",
      "cnt: 0 - valLoss: 0.4740341603755951 - trainLoss: 0.4798819422721863\n",
      "cnt: 0 - valLoss: 0.4740228056907654 - trainLoss: 0.4798720180988312\n",
      "cnt: 0 - valLoss: 0.47401154041290283 - trainLoss: 0.47986212372779846\n",
      "cnt: 0 - valLoss: 0.4740002155303955 - trainLoss: 0.47985222935676575\n",
      "cnt: 0 - valLoss: 0.47398892045021057 - trainLoss: 0.47984230518341064\n",
      "cnt: 0 - valLoss: 0.47397759556770325 - trainLoss: 0.4798324704170227\n",
      "cnt: 0 - valLoss: 0.4739663302898407 - trainLoss: 0.47982257604599\n",
      "cnt: 0 - valLoss: 0.47395503520965576 - trainLoss: 0.4798126816749573\n",
      "cnt: 0 - valLoss: 0.4739437401294708 - trainLoss: 0.47980284690856934\n",
      "cnt: 0 - valLoss: 0.4739324152469635 - trainLoss: 0.4797929525375366\n",
      "cnt: 0 - valLoss: 0.47392114996910095 - trainLoss: 0.4797830581665039\n",
      "cnt: 0 - valLoss: 0.4739098846912384 - trainLoss: 0.47977322340011597\n",
      "cnt: 0 - valLoss: 0.47389858961105347 - trainLoss: 0.479763388633728\n",
      "cnt: 0 - valLoss: 0.47388729453086853 - trainLoss: 0.4797535240650177\n",
      "cnt: 0 - valLoss: 0.4738759994506836 - trainLoss: 0.4797435998916626\n",
      "cnt: 0 - valLoss: 0.4738647937774658 - trainLoss: 0.47973379492759705\n",
      "cnt: 0 - valLoss: 0.4738534986972809 - trainLoss: 0.4797239303588867\n",
      "cnt: 0 - valLoss: 0.47384223341941833 - trainLoss: 0.4797140657901764\n",
      "cnt: 0 - valLoss: 0.4738309979438782 - trainLoss: 0.47970423102378845\n",
      "cnt: 0 - valLoss: 0.473819762468338 - trainLoss: 0.4796944260597229\n",
      "cnt: 0 - valLoss: 0.47380849719047546 - trainLoss: 0.47968459129333496\n",
      "cnt: 0 - valLoss: 0.4737972319126129 - trainLoss: 0.47967472672462463\n",
      "cnt: 0 - valLoss: 0.47378599643707275 - trainLoss: 0.4796648919582367\n",
      "cnt: 0 - valLoss: 0.4737747609615326 - trainLoss: 0.47965508699417114\n",
      "cnt: 0 - valLoss: 0.4737635552883148 - trainLoss: 0.4796452522277832\n",
      "cnt: 0 - valLoss: 0.47375231981277466 - trainLoss: 0.4796353876590729\n",
      "cnt: 0 - valLoss: 0.4737410843372345 - trainLoss: 0.4796256124973297\n",
      "cnt: 0 - valLoss: 0.47372984886169434 - trainLoss: 0.47961580753326416\n",
      "cnt: 0 - valLoss: 0.47371867299079895 - trainLoss: 0.47960594296455383\n",
      "cnt: 0 - valLoss: 0.473707377910614 - trainLoss: 0.4795961380004883\n",
      "cnt: 0 - valLoss: 0.4736962616443634 - trainLoss: 0.4795863628387451\n",
      "cnt: 0 - valLoss: 0.47368502616882324 - trainLoss: 0.4795764982700348\n",
      "cnt: 0 - valLoss: 0.4736737906932831 - trainLoss: 0.479566752910614\n",
      "cnt: 0 - valLoss: 0.4736626148223877 - trainLoss: 0.47955694794654846\n",
      "cnt: 0 - valLoss: 0.4736514091491699 - trainLoss: 0.4795471429824829\n",
      "cnt: 0 - valLoss: 0.47364023327827454 - trainLoss: 0.47953733801841736\n",
      "cnt: 0 - valLoss: 0.47362902760505676 - trainLoss: 0.4795275330543518\n",
      "cnt: 0 - valLoss: 0.4736178517341614 - trainLoss: 0.47951772809028625\n",
      "cnt: 0 - valLoss: 0.4736066162586212 - trainLoss: 0.4795079529285431\n",
      "cnt: 0 - valLoss: 0.473595529794693 - trainLoss: 0.4794981777667999\n",
      "cnt: 0 - valLoss: 0.47358429431915283 - trainLoss: 0.47948843240737915\n",
      "cnt: 0 - valLoss: 0.47357314825057983 - trainLoss: 0.4794786274433136\n",
      "cnt: 0 - valLoss: 0.47356200218200684 - trainLoss: 0.47946885228157043\n",
      "cnt: 0 - valLoss: 0.47355082631111145 - trainLoss: 0.47945910692214966\n",
      "cnt: 0 - valLoss: 0.47353968024253845 - trainLoss: 0.4794493317604065\n",
      "cnt: 0 - valLoss: 0.4735284745693207 - trainLoss: 0.4794395864009857\n",
      "cnt: 0 - valLoss: 0.47351738810539246 - trainLoss: 0.47942978143692017\n",
      "cnt: 0 - valLoss: 0.47350621223449707 - trainLoss: 0.4794200360774994\n",
      "cnt: 0 - valLoss: 0.4734950661659241 - trainLoss: 0.479410320520401\n",
      "cnt: 0 - valLoss: 0.4734839200973511 - trainLoss: 0.4794006049633026\n",
      "cnt: 0 - valLoss: 0.4734727740287781 - trainLoss: 0.47939079999923706\n",
      "cnt: 0 - valLoss: 0.47346165776252747 - trainLoss: 0.47938108444213867\n",
      "cnt: 0 - valLoss: 0.47345057129859924 - trainLoss: 0.4793713092803955\n",
      "cnt: 0 - valLoss: 0.47343942523002625 - trainLoss: 0.47936156392097473\n",
      "cnt: 0 - valLoss: 0.47342830896377563 - trainLoss: 0.47935184836387634\n",
      "cnt: 0 - valLoss: 0.47341716289520264 - trainLoss: 0.47934213280677795\n",
      "cnt: 0 - valLoss: 0.4734060764312744 - trainLoss: 0.4793323576450348\n",
      "cnt: 0 - valLoss: 0.4733949601650238 - trainLoss: 0.479322612285614\n",
      "cnt: 0 - valLoss: 0.4733838737010956 - trainLoss: 0.479312926530838\n",
      "cnt: 0 - valLoss: 0.4733727276325226 - trainLoss: 0.47930318117141724\n",
      "cnt: 0 - valLoss: 0.47336167097091675 - trainLoss: 0.47929349541664124\n",
      "cnt: 0 - valLoss: 0.47335055470466614 - trainLoss: 0.47928375005722046\n",
      "cnt: 0 - valLoss: 0.4733394682407379 - trainLoss: 0.47927403450012207\n",
      "cnt: 0 - valLoss: 0.4733284115791321 - trainLoss: 0.4792643189430237\n",
      "cnt: 0 - valLoss: 0.47331735491752625 - trainLoss: 0.4792546331882477\n",
      "cnt: 0 - valLoss: 0.47330623865127563 - trainLoss: 0.4792449176311493\n",
      "cnt: 0 - valLoss: 0.4732951521873474 - trainLoss: 0.4792352020740509\n",
      "cnt: 0 - valLoss: 0.4732840955257416 - trainLoss: 0.4792255461215973\n",
      "cnt: 0 - valLoss: 0.47327297925949097 - trainLoss: 0.4792158603668213\n",
      "cnt: 0 - valLoss: 0.4732619822025299 - trainLoss: 0.4792061150074005\n",
      "cnt: 0 - valLoss: 0.4732509255409241 - trainLoss: 0.4791964590549469\n",
      "cnt: 0 - valLoss: 0.47323986887931824 - trainLoss: 0.4791867434978485\n",
      "cnt: 0 - valLoss: 0.4732288122177124 - trainLoss: 0.4791770875453949\n",
      "cnt: 0 - valLoss: 0.4732177257537842 - trainLoss: 0.4791674017906189\n",
      "cnt: 0 - valLoss: 0.47320666909217834 - trainLoss: 0.4791576862335205\n",
      "cnt: 0 - valLoss: 0.4731956124305725 - trainLoss: 0.4791480302810669\n",
      "cnt: 0 - valLoss: 0.47318461537361145 - trainLoss: 0.4791383445262909\n",
      "cnt: 0 - valLoss: 0.473173588514328 - trainLoss: 0.47912871837615967\n",
      "cnt: 0 - valLoss: 0.47316256165504456 - trainLoss: 0.47911903262138367\n",
      "cnt: 0 - valLoss: 0.4731515049934387 - trainLoss: 0.4791093170642853\n",
      "cnt: 0 - valLoss: 0.47314050793647766 - trainLoss: 0.47909969091415405\n",
      "cnt: 0 - valLoss: 0.4731294810771942 - trainLoss: 0.47909003496170044\n",
      "cnt: 0 - valLoss: 0.47311848402023315 - trainLoss: 0.47908034920692444\n",
      "cnt: 0 - valLoss: 0.4731074571609497 - trainLoss: 0.47907066345214844\n",
      "cnt: 0 - valLoss: 0.47309643030166626 - trainLoss: 0.479061096906662\n",
      "cnt: 0 - valLoss: 0.4730854630470276 - trainLoss: 0.479051411151886\n",
      "cnt: 0 - valLoss: 0.47307446599006653 - trainLoss: 0.47904178500175476\n",
      "cnt: 0 - valLoss: 0.4730634391307831 - trainLoss: 0.47903215885162354\n",
      "cnt: 0 - valLoss: 0.4730524718761444 - trainLoss: 0.47902247309684753\n",
      "cnt: 0 - valLoss: 0.47304150462150574 - trainLoss: 0.4790129065513611\n",
      "cnt: 0 - valLoss: 0.4730305075645447 - trainLoss: 0.4790032207965851\n",
      "cnt: 0 - valLoss: 0.4730195105075836 - trainLoss: 0.47899359464645386\n",
      "cnt: 0 - valLoss: 0.47300854325294495 - trainLoss: 0.478983998298645\n",
      "cnt: 0 - valLoss: 0.4729975461959839 - trainLoss: 0.4789743423461914\n",
      "cnt: 0 - valLoss: 0.4729865789413452 - trainLoss: 0.47896474599838257\n",
      "cnt: 0 - valLoss: 0.47297561168670654 - trainLoss: 0.47895511984825134\n",
      "cnt: 0 - valLoss: 0.47296467423439026 - trainLoss: 0.4789454936981201\n",
      "cnt: 0 - valLoss: 0.4729537069797516 - trainLoss: 0.4789358973503113\n",
      "cnt: 0 - valLoss: 0.4729427695274353 - trainLoss: 0.47892627120018005\n",
      "cnt: 0 - valLoss: 0.47293180227279663 - trainLoss: 0.4789166748523712\n",
      "cnt: 0 - valLoss: 0.47292086482048035 - trainLoss: 0.4789070785045624\n",
      "cnt: 0 - valLoss: 0.4729098975658417 - trainLoss: 0.4788975119590759\n",
      "cnt: 0 - valLoss: 0.47289901971817017 - trainLoss: 0.4788879156112671\n",
      "cnt: 0 - valLoss: 0.4728880524635315 - trainLoss: 0.47887831926345825\n",
      "cnt: 0 - valLoss: 0.4728771150112152 - trainLoss: 0.478868693113327\n",
      "cnt: 0 - valLoss: 0.4728661775588989 - trainLoss: 0.47885915637016296\n",
      "cnt: 0 - valLoss: 0.47285521030426025 - trainLoss: 0.47884953022003174\n",
      "cnt: 0 - valLoss: 0.47284433245658875 - trainLoss: 0.4788399338722229\n",
      "cnt: 0 - valLoss: 0.47283339500427246 - trainLoss: 0.47883039712905884\n",
      "cnt: 0 - valLoss: 0.47282251715660095 - trainLoss: 0.47882080078125\n",
      "cnt: 0 - valLoss: 0.47281157970428467 - trainLoss: 0.47881120443344116\n",
      "cnt: 0 - valLoss: 0.47280070185661316 - trainLoss: 0.4788016378879547\n",
      "cnt: 0 - valLoss: 0.4727897644042969 - trainLoss: 0.47879207134246826\n",
      "cnt: 0 - valLoss: 0.47277888655662537 - trainLoss: 0.47878244519233704\n",
      "cnt: 0 - valLoss: 0.47276797890663147 - trainLoss: 0.47877293825149536\n",
      "cnt: 0 - valLoss: 0.4727570712566376 - trainLoss: 0.4787634015083313\n",
      "cnt: 0 - valLoss: 0.4727461636066437 - trainLoss: 0.47875380516052246\n",
      "cnt: 0 - valLoss: 0.47273531556129456 - trainLoss: 0.4787442684173584\n",
      "cnt: 0 - valLoss: 0.4727244973182678 - trainLoss: 0.47873467206954956\n",
      "cnt: 0 - valLoss: 0.4727135896682739 - trainLoss: 0.4787251949310303\n",
      "cnt: 0 - valLoss: 0.4727027118206024 - trainLoss: 0.4787156283855438\n",
      "cnt: 0 - valLoss: 0.4726918339729309 - trainLoss: 0.47870609164237976\n",
      "cnt: 0 - valLoss: 0.472680926322937 - trainLoss: 0.4786965548992157\n",
      "cnt: 0 - valLoss: 0.4726701080799103 - trainLoss: 0.47868698835372925\n",
      "cnt: 0 - valLoss: 0.4726592004299164 - trainLoss: 0.47867751121520996\n",
      "cnt: 0 - valLoss: 0.47264835238456726 - trainLoss: 0.4786679446697235\n",
      "cnt: 0 - valLoss: 0.4726375639438629 - trainLoss: 0.47865840792655945\n",
      "cnt: 0 - valLoss: 0.4726266860961914 - trainLoss: 0.4786489009857178\n",
      "cnt: 0 - valLoss: 0.4726158380508423 - trainLoss: 0.4786393642425537\n",
      "cnt: 0 - valLoss: 0.47260499000549316 - trainLoss: 0.47862982749938965\n",
      "cnt: 0 - valLoss: 0.47259417176246643 - trainLoss: 0.47862035036087036\n",
      "cnt: 0 - valLoss: 0.4725833535194397 - trainLoss: 0.4786108732223511\n",
      "cnt: 0 - valLoss: 0.4725725054740906 - trainLoss: 0.4786013066768646\n",
      "cnt: 0 - valLoss: 0.47256168723106384 - trainLoss: 0.47859179973602295\n",
      "cnt: 0 - valLoss: 0.4725508391857147 - trainLoss: 0.47858232259750366\n",
      "cnt: 0 - valLoss: 0.4725400507450104 - trainLoss: 0.478572815656662\n",
      "cnt: 0 - valLoss: 0.47252920269966125 - trainLoss: 0.4785633087158203\n",
      "cnt: 0 - valLoss: 0.4725184142589569 - trainLoss: 0.47855380177497864\n",
      "cnt: 0 - valLoss: 0.4725075960159302 - trainLoss: 0.47854429483413696\n",
      "cnt: 0 - valLoss: 0.47249680757522583 - trainLoss: 0.4785347878932953\n",
      "cnt: 0 - valLoss: 0.4724859893321991 - trainLoss: 0.4785253405570984\n",
      "cnt: 0 - valLoss: 0.47247520089149475 - trainLoss: 0.4785158336162567\n",
      "cnt: 0 - valLoss: 0.4724644124507904 - trainLoss: 0.4785063862800598\n",
      "cnt: 0 - valLoss: 0.47245365381240845 - trainLoss: 0.47849687933921814\n",
      "cnt: 0 - valLoss: 0.4724428355693817 - trainLoss: 0.47848743200302124\n",
      "cnt: 0 - valLoss: 0.47243204712867737 - trainLoss: 0.47847792506217957\n",
      "cnt: 0 - valLoss: 0.4724212884902954 - trainLoss: 0.4784684479236603\n",
      "cnt: 0 - valLoss: 0.4724104702472687 - trainLoss: 0.4784590005874634\n",
      "cnt: 0 - valLoss: 0.4723997116088867 - trainLoss: 0.4784495532512665\n",
      "cnt: 0 - valLoss: 0.47238895297050476 - trainLoss: 0.4784400761127472\n",
      "cnt: 0 - valLoss: 0.4723781645298004 - trainLoss: 0.4784306287765503\n",
      "cnt: 0 - valLoss: 0.47236743569374084 - trainLoss: 0.478421151638031\n",
      "cnt: 0 - valLoss: 0.4723566770553589 - trainLoss: 0.4784117043018341\n",
      "cnt: 0 - valLoss: 0.47234591841697693 - trainLoss: 0.4784022569656372\n",
      "cnt: 0 - valLoss: 0.47233521938323975 - trainLoss: 0.4783928692340851\n",
      "cnt: 0 - valLoss: 0.472324401140213 - trainLoss: 0.4783833622932434\n",
      "cnt: 0 - valLoss: 0.47231370210647583 - trainLoss: 0.4783739447593689\n",
      "cnt: 0 - valLoss: 0.47230297327041626 - trainLoss: 0.478364497423172\n",
      "cnt: 0 - valLoss: 0.4722922742366791 - trainLoss: 0.4783550202846527\n",
      "cnt: 0 - valLoss: 0.4722815454006195 - trainLoss: 0.4783456027507782\n",
      "cnt: 0 - valLoss: 0.47227075695991516 - trainLoss: 0.4783361554145813\n",
      "cnt: 0 - valLoss: 0.4722600281238556 - trainLoss: 0.47832679748535156\n",
      "cnt: 0 - valLoss: 0.4722493290901184 - trainLoss: 0.4783173203468323\n",
      "cnt: 0 - valLoss: 0.47223860025405884 - trainLoss: 0.47830790281295776\n",
      "cnt: 0 - valLoss: 0.47222790122032166 - trainLoss: 0.47829848527908325\n",
      "cnt: 0 - valLoss: 0.4722172021865845 - trainLoss: 0.47828906774520874\n",
      "cnt: 0 - valLoss: 0.4722065031528473 - trainLoss: 0.47827965021133423\n",
      "cnt: 0 - valLoss: 0.4721958339214325 - trainLoss: 0.4782702922821045\n",
      "cnt: 0 - valLoss: 0.4721851050853729 - trainLoss: 0.4782608151435852\n",
      "cnt: 0 - valLoss: 0.47217443585395813 - trainLoss: 0.47825145721435547\n",
      "cnt: 0 - valLoss: 0.47216373682022095 - trainLoss: 0.47824203968048096\n",
      "cnt: 0 - valLoss: 0.4721530079841614 - trainLoss: 0.47823262214660645\n",
      "cnt: 0 - valLoss: 0.4721423387527466 - trainLoss: 0.4782232344150543\n",
      "cnt: 0 - valLoss: 0.4721316993236542 - trainLoss: 0.4782138168811798\n",
      "cnt: 0 - valLoss: 0.472121000289917 - trainLoss: 0.4782044589519501\n",
      "cnt: 0 - valLoss: 0.4721103310585022 - trainLoss: 0.47819504141807556\n",
      "cnt: 0 - valLoss: 0.4720996618270874 - trainLoss: 0.4781857132911682\n",
      "cnt: 0 - valLoss: 0.472089022397995 - trainLoss: 0.4781762957572937\n",
      "cnt: 0 - valLoss: 0.4720783531665802 - trainLoss: 0.4781669080257416\n",
      "cnt: 0 - valLoss: 0.4720677137374878 - trainLoss: 0.47815755009651184\n",
      "cnt: 0 - valLoss: 0.472057044506073 - trainLoss: 0.47814813256263733\n",
      "cnt: 0 - valLoss: 0.4720464050769806 - trainLoss: 0.47813880443573\n",
      "cnt: 0 - valLoss: 0.47203579545021057 - trainLoss: 0.47812941670417786\n",
      "cnt: 0 - valLoss: 0.47202515602111816 - trainLoss: 0.4781200587749481\n",
      "cnt: 0 - valLoss: 0.47201451659202576 - trainLoss: 0.478110671043396\n",
      "cnt: 0 - valLoss: 0.47200390696525574 - trainLoss: 0.47810131311416626\n",
      "cnt: 0 - valLoss: 0.4719932973384857 - trainLoss: 0.4780919849872589\n",
      "cnt: 0 - valLoss: 0.4719826877117157 - trainLoss: 0.4780825972557068\n",
      "cnt: 0 - valLoss: 0.4719720780849457 - trainLoss: 0.47807323932647705\n",
      "cnt: 0 - valLoss: 0.47196146845817566 - trainLoss: 0.4780639410018921\n",
      "cnt: 0 - valLoss: 0.471950888633728 - trainLoss: 0.47805461287498474\n",
      "cnt: 0 - valLoss: 0.471940279006958 - trainLoss: 0.4780452251434326\n",
      "cnt: 0 - valLoss: 0.471929669380188 - trainLoss: 0.47803589701652527\n",
      "cnt: 0 - valLoss: 0.47191911935806274 - trainLoss: 0.4780265688896179\n",
      "cnt: 0 - valLoss: 0.47190847992897034 - trainLoss: 0.47801724076271057\n",
      "cnt: 0 - valLoss: 0.4718979001045227 - trainLoss: 0.4780079126358032\n",
      "cnt: 0 - valLoss: 0.4718873202800751 - trainLoss: 0.4779985249042511\n",
      "cnt: 0 - valLoss: 0.47187677025794983 - trainLoss: 0.4779892563819885\n",
      "cnt: 0 - valLoss: 0.4718661904335022 - trainLoss: 0.4779799282550812\n",
      "cnt: 0 - valLoss: 0.4718555808067322 - trainLoss: 0.47797060012817383\n",
      "cnt: 0 - valLoss: 0.4718450605869293 - trainLoss: 0.4779612720012665\n",
      "cnt: 0 - valLoss: 0.4718345105648041 - trainLoss: 0.4779519736766815\n",
      "cnt: 0 - valLoss: 0.47182393074035645 - trainLoss: 0.47794267535209656\n",
      "cnt: 0 - valLoss: 0.4718133211135864 - trainLoss: 0.4779333472251892\n",
      "cnt: 0 - valLoss: 0.47180283069610596 - trainLoss: 0.47792401909828186\n",
      "cnt: 0 - valLoss: 0.4717922806739807 - trainLoss: 0.4779147207736969\n",
      "cnt: 0 - valLoss: 0.4717817008495331 - trainLoss: 0.4779054522514343\n",
      "cnt: 0 - valLoss: 0.47177115082740784 - trainLoss: 0.477896124124527\n",
      "cnt: 0 - valLoss: 0.47176066040992737 - trainLoss: 0.477886825799942\n",
      "cnt: 0 - valLoss: 0.4717501401901245 - trainLoss: 0.47787752747535706\n",
      "cnt: 0 - valLoss: 0.4717395603656769 - trainLoss: 0.4778682589530945\n",
      "cnt: 0 - valLoss: 0.4717290699481964 - trainLoss: 0.4778589606285095\n",
      "cnt: 0 - valLoss: 0.47171851992607117 - trainLoss: 0.4778496325016022\n",
      "cnt: 0 - valLoss: 0.4717080295085907 - trainLoss: 0.477840393781662\n",
      "cnt: 0 - valLoss: 0.47169750928878784 - trainLoss: 0.4778311550617218\n",
      "cnt: 0 - valLoss: 0.4716869592666626 - trainLoss: 0.47782185673713684\n",
      "cnt: 0 - valLoss: 0.47167646884918213 - trainLoss: 0.4778125584125519\n",
      "cnt: 0 - valLoss: 0.47166597843170166 - trainLoss: 0.4778033196926117\n",
      "cnt: 0 - valLoss: 0.4716554582118988 - trainLoss: 0.47779402136802673\n",
      "cnt: 0 - valLoss: 0.47164496779441833 - trainLoss: 0.47778475284576416\n",
      "cnt: 0 - valLoss: 0.47163450717926025 - trainLoss: 0.4777754843235016\n",
      "cnt: 0 - valLoss: 0.4716240167617798 - trainLoss: 0.4777662456035614\n",
      "cnt: 0 - valLoss: 0.47161349654197693 - trainLoss: 0.4777570068836212\n",
      "cnt: 0 - valLoss: 0.47160300612449646 - trainLoss: 0.47774770855903625\n",
      "cnt: 0 - valLoss: 0.471592515707016 - trainLoss: 0.4777384400367737\n",
      "cnt: 0 - valLoss: 0.4715820550918579 - trainLoss: 0.4777292013168335\n",
      "cnt: 0 - valLoss: 0.47157153487205505 - trainLoss: 0.4777199327945709\n",
      "cnt: 0 - valLoss: 0.4715610444545746 - trainLoss: 0.4777106046676636\n",
      "cnt: 0 - valLoss: 0.4715505540370941 - trainLoss: 0.477701336145401\n",
      "cnt: 0 - valLoss: 0.47154003381729126 - trainLoss: 0.47769203782081604\n",
      "cnt: 0 - valLoss: 0.4715295732021332 - trainLoss: 0.47768279910087585\n",
      "cnt: 0 - valLoss: 0.4715190827846527 - trainLoss: 0.4776735305786133\n",
      "cnt: 0 - valLoss: 0.4715086817741394 - trainLoss: 0.4776642918586731\n",
      "cnt: 0 - valLoss: 0.47149816155433655 - trainLoss: 0.4776550233364105\n",
      "cnt: 0 - valLoss: 0.47148770093917847 - trainLoss: 0.47764575481414795\n",
      "cnt: 0 - valLoss: 0.4714772403240204 - trainLoss: 0.47763651609420776\n",
      "cnt: 0 - valLoss: 0.4714668393135071 - trainLoss: 0.4776272475719452\n",
      "cnt: 0 - valLoss: 0.4714563488960266 - trainLoss: 0.4776179790496826\n",
      "cnt: 0 - valLoss: 0.4714459180831909 - trainLoss: 0.47760874032974243\n",
      "cnt: 0 - valLoss: 0.47143545746803284 - trainLoss: 0.47759950160980225\n",
      "cnt: 0 - valLoss: 0.47142499685287476 - trainLoss: 0.47759026288986206\n",
      "cnt: 0 - valLoss: 0.47141459584236145 - trainLoss: 0.4775809943675995\n",
      "cnt: 0 - valLoss: 0.47140413522720337 - trainLoss: 0.4775718152523041\n",
      "cnt: 0 - valLoss: 0.4713937044143677 - trainLoss: 0.4775626063346863\n",
      "cnt: 0 - valLoss: 0.4713832437992096 - trainLoss: 0.4775533676147461\n",
      "cnt: 0 - valLoss: 0.4713728725910187 - trainLoss: 0.4775440990924835\n",
      "cnt: 0 - valLoss: 0.4713624119758606 - trainLoss: 0.47753486037254333\n",
      "cnt: 0 - valLoss: 0.4713520109653473 - trainLoss: 0.4775257110595703\n",
      "cnt: 0 - valLoss: 0.4713415801525116 - trainLoss: 0.4775164723396301\n",
      "cnt: 0 - valLoss: 0.4713311493396759 - trainLoss: 0.47750720381736755\n",
      "cnt: 0 - valLoss: 0.4713207185268402 - trainLoss: 0.47749805450439453\n",
      "cnt: 0 - valLoss: 0.4713103175163269 - trainLoss: 0.47748884558677673\n",
      "cnt: 0 - valLoss: 0.4712998569011688 - trainLoss: 0.4774796664714813\n",
      "cnt: 0 - valLoss: 0.4712894856929779 - trainLoss: 0.47747039794921875\n",
      "cnt: 0 - valLoss: 0.4712790548801422 - trainLoss: 0.4774612486362457\n",
      "cnt: 0 - valLoss: 0.4712686836719513 - trainLoss: 0.47745203971862793\n",
      "cnt: 0 - valLoss: 0.471258282661438 - trainLoss: 0.47744283080101013\n",
      "cnt: 0 - valLoss: 0.4712478220462799 - trainLoss: 0.4774336814880371\n",
      "cnt: 0 - valLoss: 0.471237450838089 - trainLoss: 0.4774245321750641\n",
      "cnt: 0 - valLoss: 0.4712270200252533 - trainLoss: 0.4774153232574463\n",
      "cnt: 0 - valLoss: 0.47121661901474 - trainLoss: 0.4774061143398285\n",
      "cnt: 0 - valLoss: 0.4712061882019043 - trainLoss: 0.47739696502685547\n",
      "cnt: 0 - valLoss: 0.471195787191391 - trainLoss: 0.47738781571388245\n",
      "cnt: 0 - valLoss: 0.4711854159832001 - trainLoss: 0.47737863659858704\n",
      "cnt: 0 - valLoss: 0.4711749851703644 - trainLoss: 0.477369487285614\n",
      "cnt: 0 - valLoss: 0.4711645841598511 - trainLoss: 0.4773603081703186\n",
      "cnt: 0 - valLoss: 0.47115421295166016 - trainLoss: 0.4773511290550232\n",
      "cnt: 0 - valLoss: 0.47114384174346924 - trainLoss: 0.47734200954437256\n",
      "cnt: 0 - valLoss: 0.4711334705352783 - trainLoss: 0.47733283042907715\n",
      "cnt: 0 - valLoss: 0.4711230993270874 - trainLoss: 0.4773236811161041\n",
      "cnt: 0 - valLoss: 0.4711126983165741 - trainLoss: 0.4773145616054535\n",
      "cnt: 0 - valLoss: 0.4711023271083832 - trainLoss: 0.47730541229248047\n",
      "cnt: 0 - valLoss: 0.47109195590019226 - trainLoss: 0.47729629278182983\n",
      "cnt: 0 - valLoss: 0.47108158469200134 - trainLoss: 0.4772871434688568\n",
      "cnt: 0 - valLoss: 0.4710712134838104 - trainLoss: 0.4772780239582062\n",
      "cnt: 0 - valLoss: 0.4710609018802643 - trainLoss: 0.4772688150405884\n",
      "cnt: 0 - valLoss: 0.47105053067207336 - trainLoss: 0.47725972533226013\n",
      "cnt: 0 - valLoss: 0.47104018926620483 - trainLoss: 0.4772505760192871\n",
      "cnt: 0 - valLoss: 0.4710298180580139 - trainLoss: 0.47724151611328125\n",
      "cnt: 0 - valLoss: 0.4710194766521454 - trainLoss: 0.47723233699798584\n",
      "cnt: 0 - valLoss: 0.47100916504859924 - trainLoss: 0.47722327709198\n",
      "cnt: 0 - valLoss: 0.4709988236427307 - trainLoss: 0.47721415758132935\n",
      "cnt: 0 - valLoss: 0.47098851203918457 - trainLoss: 0.4772050380706787\n",
      "cnt: 0 - valLoss: 0.4709782004356384 - trainLoss: 0.4771959185600281\n",
      "cnt: 0 - valLoss: 0.4709678888320923 - trainLoss: 0.47718679904937744\n",
      "cnt: 0 - valLoss: 0.47095754742622375 - trainLoss: 0.4771776795387268\n",
      "cnt: 0 - valLoss: 0.470947265625 - trainLoss: 0.47716861963272095\n",
      "cnt: 0 - valLoss: 0.47093692421913147 - trainLoss: 0.47715944051742554\n",
      "cnt: 0 - valLoss: 0.4709266126155853 - trainLoss: 0.4771503806114197\n",
      "cnt: 0 - valLoss: 0.4709163308143616 - trainLoss: 0.47714126110076904\n",
      "cnt: 0 - valLoss: 0.47090601921081543 - trainLoss: 0.47713223099708557\n",
      "cnt: 0 - valLoss: 0.4708957374095917 - trainLoss: 0.47712311148643494\n",
      "cnt: 0 - valLoss: 0.47088542580604553 - trainLoss: 0.4771140515804291\n",
      "cnt: 0 - valLoss: 0.4708751440048218 - trainLoss: 0.47710493206977844\n",
      "cnt: 0 - valLoss: 0.47086483240127563 - trainLoss: 0.47709590196609497\n",
      "cnt: 0 - valLoss: 0.4708545506000519 - trainLoss: 0.47708678245544434\n",
      "cnt: 0 - valLoss: 0.4708442986011505 - trainLoss: 0.47707775235176086\n",
      "cnt: 0 - valLoss: 0.47083401679992676 - trainLoss: 0.47706863284111023\n",
      "cnt: 0 - valLoss: 0.470823734998703 - trainLoss: 0.477059543132782\n",
      "cnt: 0 - valLoss: 0.47081348299980164 - trainLoss: 0.4770504832267761\n",
      "cnt: 0 - valLoss: 0.4708032011985779 - trainLoss: 0.47704145312309265\n",
      "cnt: 0 - valLoss: 0.4707929491996765 - trainLoss: 0.4770323634147644\n",
      "cnt: 0 - valLoss: 0.47078272700309753 - trainLoss: 0.47702330350875854\n",
      "cnt: 0 - valLoss: 0.47077247500419617 - trainLoss: 0.4770142734050751\n",
      "cnt: 0 - valLoss: 0.4707622230052948 - trainLoss: 0.4770052433013916\n",
      "cnt: 0 - valLoss: 0.4707520008087158 - trainLoss: 0.47699615359306335\n",
      "cnt: 0 - valLoss: 0.47074174880981445 - trainLoss: 0.4769870936870575\n",
      "cnt: 0 - valLoss: 0.4707314968109131 - trainLoss: 0.4769780933856964\n",
      "cnt: 0 - valLoss: 0.4707212448120117 - trainLoss: 0.47696903347969055\n",
      "cnt: 0 - valLoss: 0.47071102261543274 - trainLoss: 0.4769600033760071\n",
      "cnt: 0 - valLoss: 0.47070080041885376 - trainLoss: 0.47695091366767883\n",
      "cnt: 0 - valLoss: 0.4706905782222748 - trainLoss: 0.47694188356399536\n",
      "cnt: 0 - valLoss: 0.4706803560256958 - trainLoss: 0.4769328534603119\n",
      "cnt: 0 - valLoss: 0.4706701338291168 - trainLoss: 0.4769238829612732\n",
      "cnt: 0 - valLoss: 0.47065991163253784 - trainLoss: 0.47691479325294495\n",
      "cnt: 0 - valLoss: 0.47064974904060364 - trainLoss: 0.4769057631492615\n",
      "cnt: 0 - valLoss: 0.47063952684402466 - trainLoss: 0.4768967926502228\n",
      "cnt: 0 - valLoss: 0.4706293046474457 - trainLoss: 0.4768877923488617\n",
      "cnt: 0 - valLoss: 0.47061917185783386 - trainLoss: 0.4768787622451782\n",
      "cnt: 0 - valLoss: 0.4706089198589325 - trainLoss: 0.47686973214149475\n",
      "cnt: 0 - valLoss: 0.4705987274646759 - trainLoss: 0.4768607020378113\n",
      "cnt: 0 - valLoss: 0.4705885648727417 - trainLoss: 0.4768517017364502\n",
      "cnt: 0 - valLoss: 0.4705784022808075 - trainLoss: 0.4768426716327667\n",
      "cnt: 0 - valLoss: 0.4705682098865509 - trainLoss: 0.476833701133728\n",
      "cnt: 0 - valLoss: 0.4705580472946167 - trainLoss: 0.47682470083236694\n",
      "cnt: 0 - valLoss: 0.4705478250980377 - trainLoss: 0.47681570053100586\n",
      "cnt: 0 - valLoss: 0.4705376923084259 - trainLoss: 0.47680673003196716\n",
      "cnt: 0 - valLoss: 0.4705275297164917 - trainLoss: 0.4767977297306061\n",
      "cnt: 0 - valLoss: 0.4705173671245575 - trainLoss: 0.4767887592315674\n",
      "cnt: 0 - valLoss: 0.4705072045326233 - trainLoss: 0.4767797589302063\n",
      "cnt: 0 - valLoss: 0.4704970419406891 - trainLoss: 0.4767707586288452\n",
      "cnt: 0 - valLoss: 0.47048693895339966 - trainLoss: 0.4767617881298065\n",
      "cnt: 0 - valLoss: 0.4704767167568207 - trainLoss: 0.47675278782844543\n",
      "cnt: 0 - valLoss: 0.47046661376953125 - trainLoss: 0.47674381732940674\n",
      "cnt: 0 - valLoss: 0.47045645117759705 - trainLoss: 0.47673487663269043\n",
      "cnt: 0 - valLoss: 0.47044637799263 - trainLoss: 0.47672587633132935\n",
      "cnt: 0 - valLoss: 0.4704362154006958 - trainLoss: 0.47671687602996826\n",
      "cnt: 0 - valLoss: 0.4704260528087616 - trainLoss: 0.47670793533325195\n",
      "cnt: 0 - valLoss: 0.47041594982147217 - trainLoss: 0.47669899463653564\n",
      "cnt: 0 - valLoss: 0.47040584683418274 - trainLoss: 0.47668999433517456\n",
      "cnt: 0 - valLoss: 0.4703957140445709 - trainLoss: 0.47668105363845825\n",
      "cnt: 0 - valLoss: 0.4703856110572815 - trainLoss: 0.47667211294174194\n",
      "cnt: 0 - valLoss: 0.4703754782676697 - trainLoss: 0.47666317224502563\n",
      "cnt: 0 - valLoss: 0.47036537528038025 - trainLoss: 0.47665417194366455\n",
      "cnt: 0 - valLoss: 0.4703553020954132 - trainLoss: 0.47664523124694824\n",
      "cnt: 0 - valLoss: 0.4703451991081238 - trainLoss: 0.47663629055023193\n",
      "cnt: 0 - valLoss: 0.47033512592315674 - trainLoss: 0.476627379655838\n",
      "cnt: 0 - valLoss: 0.4703249931335449 - trainLoss: 0.4766184389591217\n",
      "cnt: 0 - valLoss: 0.4703148901462555 - trainLoss: 0.4766094982624054\n",
      "cnt: 0 - valLoss: 0.47030481696128845 - trainLoss: 0.4766005575656891\n",
      "cnt: 0 - valLoss: 0.4702947437763214 - trainLoss: 0.4765916168689728\n",
      "cnt: 0 - valLoss: 0.4702846109867096 - trainLoss: 0.47658270597457886\n",
      "cnt: 0 - valLoss: 0.47027459740638733 - trainLoss: 0.47657376527786255\n",
      "cnt: 0 - valLoss: 0.4702645242214203 - trainLoss: 0.47656482458114624\n",
      "cnt: 0 - valLoss: 0.47025445103645325 - trainLoss: 0.4765559136867523\n",
      "cnt: 0 - valLoss: 0.4702444076538086 - trainLoss: 0.4765470027923584\n",
      "cnt: 0 - valLoss: 0.47023430466651917 - trainLoss: 0.4765380918979645\n",
      "cnt: 0 - valLoss: 0.4702242612838745 - trainLoss: 0.47652921080589294\n",
      "cnt: 0 - valLoss: 0.47021418809890747 - trainLoss: 0.476520299911499\n",
      "cnt: 0 - valLoss: 0.4702041745185852 - trainLoss: 0.4765113890171051\n",
      "cnt: 0 - valLoss: 0.47019410133361816 - trainLoss: 0.4765024781227112\n",
      "cnt: 0 - valLoss: 0.4701840579509735 - trainLoss: 0.4764935374259949\n",
      "cnt: 0 - valLoss: 0.47017404437065125 - trainLoss: 0.4764846861362457\n",
      "cnt: 0 - valLoss: 0.4701640009880066 - trainLoss: 0.4764757752418518\n",
      "cnt: 0 - valLoss: 0.4701539874076843 - trainLoss: 0.4764668941497803\n",
      "cnt: 0 - valLoss: 0.4701439440250397 - trainLoss: 0.47645798325538635\n",
      "cnt: 0 - valLoss: 0.4701339304447174 - trainLoss: 0.47644907236099243\n",
      "cnt: 0 - valLoss: 0.4701237976551056 - trainLoss: 0.4764401614665985\n",
      "cnt: 0 - valLoss: 0.4701137840747833 - trainLoss: 0.47643131017684937\n",
      "cnt: 0 - valLoss: 0.47010374069213867 - trainLoss: 0.4764224588871002\n",
      "cnt: 0 - valLoss: 0.47009366750717163 - trainLoss: 0.4764136075973511\n",
      "cnt: 0 - valLoss: 0.4700835943222046 - trainLoss: 0.47640472650527954\n",
      "cnt: 0 - valLoss: 0.4700735807418823 - trainLoss: 0.4763958156108856\n",
      "cnt: 0 - valLoss: 0.47006353735923767 - trainLoss: 0.47638702392578125\n",
      "cnt: 0 - valLoss: 0.4700535237789154 - trainLoss: 0.4763781428337097\n",
      "cnt: 0 - valLoss: 0.47004348039627075 - trainLoss: 0.47636929154396057\n",
      "cnt: 0 - valLoss: 0.4700334072113037 - trainLoss: 0.47636038064956665\n",
      "cnt: 0 - valLoss: 0.47002342343330383 - trainLoss: 0.4763515591621399\n",
      "cnt: 0 - valLoss: 0.4700133502483368 - trainLoss: 0.47634270787239075\n",
      "cnt: 0 - valLoss: 0.4700033664703369 - trainLoss: 0.4763338565826416\n",
      "cnt: 0 - valLoss: 0.46999332308769226 - trainLoss: 0.47632503509521484\n",
      "cnt: 0 - valLoss: 0.46998330950737 - trainLoss: 0.4763161838054657\n",
      "cnt: 0 - valLoss: 0.4699733257293701 - trainLoss: 0.47630736231803894\n",
      "cnt: 0 - valLoss: 0.46996334195137024 - trainLoss: 0.4762985110282898\n",
      "cnt: 0 - valLoss: 0.4699532985687256 - trainLoss: 0.47628968954086304\n",
      "cnt: 0 - valLoss: 0.4699432849884033 - trainLoss: 0.4762808680534363\n",
      "cnt: 0 - valLoss: 0.46993327140808105 - trainLoss: 0.47627201676368713\n",
      "cnt: 0 - valLoss: 0.4699232876300812 - trainLoss: 0.4762631952762604\n",
      "cnt: 0 - valLoss: 0.4699133634567261 - trainLoss: 0.47625431418418884\n",
      "cnt: 0 - valLoss: 0.4699033200740814 - trainLoss: 0.47624555230140686\n",
      "cnt: 0 - valLoss: 0.46989336609840393 - trainLoss: 0.4762367010116577\n",
      "cnt: 0 - valLoss: 0.46988338232040405 - trainLoss: 0.47622790932655334\n",
      "cnt: 0 - valLoss: 0.4698733985424042 - trainLoss: 0.4762190878391266\n",
      "cnt: 0 - valLoss: 0.4698634743690491 - trainLoss: 0.4762103259563446\n",
      "cnt: 0 - valLoss: 0.4698535203933716 - trainLoss: 0.47620147466659546\n",
      "cnt: 0 - valLoss: 0.4698435366153717 - trainLoss: 0.4761926829814911\n",
      "cnt: 0 - valLoss: 0.4698335528373718 - trainLoss: 0.4761838912963867\n",
      "cnt: 0 - valLoss: 0.46982359886169434 - trainLoss: 0.4761751592159271\n",
      "cnt: 0 - valLoss: 0.4698137044906616 - trainLoss: 0.47616636753082275\n",
      "cnt: 0 - valLoss: 0.46980372071266174 - trainLoss: 0.47615760564804077\n",
      "cnt: 0 - valLoss: 0.46979382634162903 - trainLoss: 0.4761488139629364\n",
      "cnt: 0 - valLoss: 0.46978387236595154 - trainLoss: 0.4761400520801544\n",
      "cnt: 0 - valLoss: 0.4697739779949188 - trainLoss: 0.47613126039505005\n",
      "cnt: 0 - valLoss: 0.46976402401924133 - trainLoss: 0.47612249851226807\n",
      "cnt: 0 - valLoss: 0.4697541296482086 - trainLoss: 0.47611376643180847\n",
      "cnt: 0 - valLoss: 0.4697442352771759 - trainLoss: 0.4761050343513489\n",
      "cnt: 0 - valLoss: 0.4697342813014984 - trainLoss: 0.4760963022708893\n",
      "cnt: 0 - valLoss: 0.4697243273258209 - trainLoss: 0.4760875701904297\n",
      "cnt: 0 - valLoss: 0.4697144627571106 - trainLoss: 0.4760788083076477\n",
      "cnt: 0 - valLoss: 0.4697045683860779 - trainLoss: 0.4760700762271881\n",
      "cnt: 0 - valLoss: 0.46969467401504517 - trainLoss: 0.47606128454208374\n",
      "cnt: 0 - valLoss: 0.46968480944633484 - trainLoss: 0.4760526418685913\n",
      "cnt: 0 - valLoss: 0.46967488527297974 - trainLoss: 0.4760438799858093\n",
      "cnt: 0 - valLoss: 0.4696650207042694 - trainLoss: 0.47603508830070496\n",
      "cnt: 0 - valLoss: 0.4696551561355591 - trainLoss: 0.47602641582489014\n",
      "cnt: 0 - valLoss: 0.46964526176452637 - trainLoss: 0.47601768374443054\n",
      "cnt: 0 - valLoss: 0.46963536739349365 - trainLoss: 0.47600898146629333\n",
      "cnt: 0 - valLoss: 0.4696255028247833 - trainLoss: 0.47600024938583374\n",
      "cnt: 0 - valLoss: 0.469615638256073 - trainLoss: 0.47599151730537415\n",
      "cnt: 0 - valLoss: 0.4696057438850403 - trainLoss: 0.47598278522491455\n",
      "cnt: 0 - valLoss: 0.46959587931632996 - trainLoss: 0.47597405314445496\n",
      "cnt: 0 - valLoss: 0.46958598494529724 - trainLoss: 0.47596538066864014\n",
      "cnt: 0 - valLoss: 0.4695761203765869 - trainLoss: 0.47595658898353577\n",
      "cnt: 0 - valLoss: 0.4695662558078766 - trainLoss: 0.47594791650772095\n",
      "cnt: 0 - valLoss: 0.46955639123916626 - trainLoss: 0.47593921422958374\n",
      "cnt: 0 - valLoss: 0.4695465564727783 - trainLoss: 0.47593048214912415\n",
      "cnt: 0 - valLoss: 0.469536691904068 - trainLoss: 0.47592175006866455\n",
      "cnt: 0 - valLoss: 0.46952685713768005 - trainLoss: 0.47591307759284973\n",
      "cnt: 0 - valLoss: 0.46951696276664734 - trainLoss: 0.4759043753147125\n",
      "cnt: 0 - valLoss: 0.4695071876049042 - trainLoss: 0.47589564323425293\n",
      "cnt: 0 - valLoss: 0.46949729323387146 - trainLoss: 0.4758869707584381\n",
      "cnt: 0 - valLoss: 0.4694874882698059 - trainLoss: 0.4758782684803009\n",
      "cnt: 0 - valLoss: 0.4694776237010956 - trainLoss: 0.47586962580680847\n",
      "cnt: 0 - valLoss: 0.4694678485393524 - trainLoss: 0.4758608937263489\n",
      "cnt: 0 - valLoss: 0.46945804357528687 - trainLoss: 0.47585222125053406\n",
      "cnt: 0 - valLoss: 0.4694482088088989 - trainLoss: 0.47584351897239685\n",
      "cnt: 0 - valLoss: 0.469438374042511 - trainLoss: 0.47583481669425964\n",
      "cnt: 0 - valLoss: 0.4694286286830902 - trainLoss: 0.4758261740207672\n",
      "cnt: 0 - valLoss: 0.46941882371902466 - trainLoss: 0.4758175015449524\n",
      "cnt: 0 - valLoss: 0.4694089889526367 - trainLoss: 0.47580885887145996\n",
      "cnt: 0 - valLoss: 0.46939927339553833 - trainLoss: 0.4758002460002899\n",
      "cnt: 0 - valLoss: 0.4693894684314728 - trainLoss: 0.4757915437221527\n",
      "cnt: 0 - valLoss: 0.4693796932697296 - trainLoss: 0.47578296065330505\n",
      "cnt: 0 - valLoss: 0.46936991810798645 - trainLoss: 0.4757743179798126\n",
      "cnt: 0 - valLoss: 0.4693601727485657 - trainLoss: 0.4757656753063202\n",
      "cnt: 0 - valLoss: 0.4693503975868225 - trainLoss: 0.47575706243515015\n",
      "cnt: 0 - valLoss: 0.46934062242507935 - trainLoss: 0.4757484197616577\n",
      "cnt: 0 - valLoss: 0.4693308174610138 - trainLoss: 0.4757397770881653\n",
      "cnt: 0 - valLoss: 0.46932104229927063 - trainLoss: 0.47573116421699524\n",
      "cnt: 0 - valLoss: 0.46931126713752747 - trainLoss: 0.4757225513458252\n",
      "cnt: 0 - valLoss: 0.4693015515804291 - trainLoss: 0.47571390867233276\n",
      "cnt: 0 - valLoss: 0.4692917764186859 - trainLoss: 0.4757053256034851\n",
      "cnt: 0 - valLoss: 0.4692820906639099 - trainLoss: 0.47569671273231506\n",
      "cnt: 0 - valLoss: 0.46927231550216675 - trainLoss: 0.47568807005882263\n",
      "cnt: 0 - valLoss: 0.46926262974739075 - trainLoss: 0.47567951679229736\n",
      "cnt: 0 - valLoss: 0.46925288438796997 - trainLoss: 0.4756709039211273\n",
      "cnt: 0 - valLoss: 0.4692431688308716 - trainLoss: 0.4756622910499573\n",
      "cnt: 0 - valLoss: 0.4692333936691284 - trainLoss: 0.475653737783432\n",
      "cnt: 0 - valLoss: 0.4692237079143524 - trainLoss: 0.4756450951099396\n",
      "cnt: 0 - valLoss: 0.4692140221595764 - trainLoss: 0.4756365418434143\n",
      "cnt: 0 - valLoss: 0.4692043364048004 - trainLoss: 0.47562792897224426\n",
      "cnt: 0 - valLoss: 0.46919459104537964 - trainLoss: 0.4756193161010742\n",
      "cnt: 0 - valLoss: 0.46918487548828125 - trainLoss: 0.47561076283454895\n",
      "cnt: 0 - valLoss: 0.46917515993118286 - trainLoss: 0.4756022095680237\n",
      "cnt: 0 - valLoss: 0.4691654145717621 - trainLoss: 0.4755936563014984\n",
      "cnt: 0 - valLoss: 0.4691556990146637 - trainLoss: 0.47558504343032837\n",
      "cnt: 0 - valLoss: 0.4691459536552429 - trainLoss: 0.4755765199661255\n",
      "cnt: 0 - valLoss: 0.46913617849349976 - trainLoss: 0.4755679666996002\n",
      "cnt: 0 - valLoss: 0.46912649273872375 - trainLoss: 0.47555941343307495\n",
      "cnt: 0 - valLoss: 0.46911677718162537 - trainLoss: 0.4755508303642273\n",
      "cnt: 0 - valLoss: 0.46910709142684937 - trainLoss: 0.475542277097702\n",
      "cnt: 0 - valLoss: 0.4690973460674286 - trainLoss: 0.47553372383117676\n",
      "cnt: 0 - valLoss: 0.4690876603126526 - trainLoss: 0.4755251705646515\n",
      "cnt: 0 - valLoss: 0.4690779745578766 - trainLoss: 0.4755166471004486\n",
      "cnt: 0 - valLoss: 0.4690681993961334 - trainLoss: 0.4755081236362457\n",
      "cnt: 0 - valLoss: 0.4690585136413574 - trainLoss: 0.47549957036972046\n",
      "cnt: 0 - valLoss: 0.4690488874912262 - trainLoss: 0.4754910171031952\n",
      "cnt: 0 - valLoss: 0.4690392315387726 - trainLoss: 0.4754824936389923\n",
      "cnt: 0 - valLoss: 0.4690295457839966 - trainLoss: 0.47547397017478943\n",
      "cnt: 0 - valLoss: 0.4690198004245758 - trainLoss: 0.47546541690826416\n",
      "cnt: 0 - valLoss: 0.4690101742744446 - trainLoss: 0.4754568934440613\n",
      "cnt: 0 - valLoss: 0.46900051832199097 - trainLoss: 0.4754483699798584\n",
      "cnt: 0 - valLoss: 0.46899089217185974 - trainLoss: 0.47543981671333313\n",
      "cnt: 0 - valLoss: 0.46898120641708374 - trainLoss: 0.47543132305145264\n",
      "cnt: 0 - valLoss: 0.4689715504646301 - trainLoss: 0.47542282938957214\n",
      "cnt: 0 - valLoss: 0.4689618945121765 - trainLoss: 0.47541430592536926\n",
      "cnt: 0 - valLoss: 0.4689522683620453 - trainLoss: 0.4754057824611664\n",
      "cnt: 0 - valLoss: 0.4689426124095917 - trainLoss: 0.4753972589969635\n",
      "cnt: 0 - valLoss: 0.46893298625946045 - trainLoss: 0.4753887951374054\n",
      "cnt: 0 - valLoss: 0.46892333030700684 - trainLoss: 0.4753802716732025\n",
      "cnt: 0 - valLoss: 0.4689136743545532 - trainLoss: 0.47537174820899963\n",
      "cnt: 0 - valLoss: 0.46890413761138916 - trainLoss: 0.47536328434944153\n",
      "cnt: 0 - valLoss: 0.46889445185661316 - trainLoss: 0.47535476088523865\n",
      "cnt: 0 - valLoss: 0.4688848555088043 - trainLoss: 0.47534623742103577\n",
      "cnt: 0 - valLoss: 0.4688752293586731 - trainLoss: 0.47533777356147766\n",
      "cnt: 0 - valLoss: 0.46886560320854187 - trainLoss: 0.4753292500972748\n",
      "cnt: 0 - valLoss: 0.46885600686073303 - trainLoss: 0.47532081604003906\n",
      "cnt: 0 - valLoss: 0.4688464105129242 - trainLoss: 0.4753122925758362\n",
      "cnt: 0 - valLoss: 0.4688367545604706 - trainLoss: 0.4753038287162781\n",
      "cnt: 0 - valLoss: 0.46882718801498413 - trainLoss: 0.47529536485671997\n",
      "cnt: 0 - valLoss: 0.4688175916671753 - trainLoss: 0.4752868711948395\n",
      "cnt: 0 - valLoss: 0.46880799531936646 - trainLoss: 0.475278377532959\n",
      "cnt: 0 - valLoss: 0.4687983989715576 - trainLoss: 0.47526994347572327\n",
      "cnt: 0 - valLoss: 0.4687888026237488 - trainLoss: 0.4752614200115204\n",
      "cnt: 0 - valLoss: 0.46877923607826233 - trainLoss: 0.47525298595428467\n",
      "cnt: 0 - valLoss: 0.4687696695327759 - trainLoss: 0.47524455189704895\n",
      "cnt: 0 - valLoss: 0.46876007318496704 - trainLoss: 0.47523608803749084\n",
      "cnt: 0 - valLoss: 0.4687505066394806 - trainLoss: 0.47522759437561035\n",
      "cnt: 0 - valLoss: 0.46874091029167175 - trainLoss: 0.47521916031837463\n",
      "cnt: 0 - valLoss: 0.4687314033508301 - trainLoss: 0.47521069645881653\n",
      "cnt: 0 - valLoss: 0.46872180700302124 - trainLoss: 0.4752022624015808\n",
      "cnt: 0 - valLoss: 0.4687122404575348 - trainLoss: 0.4751937985420227\n",
      "cnt: 0 - valLoss: 0.46870267391204834 - trainLoss: 0.475185364484787\n",
      "cnt: 0 - valLoss: 0.46869316697120667 - trainLoss: 0.47517693042755127\n",
      "cnt: 0 - valLoss: 0.4686836004257202 - trainLoss: 0.47516849637031555\n",
      "cnt: 0 - valLoss: 0.46867406368255615 - trainLoss: 0.47516003251075745\n",
      "cnt: 0 - valLoss: 0.4686645269393921 - trainLoss: 0.47515159845352173\n",
      "cnt: 0 - valLoss: 0.468654990196228 - trainLoss: 0.4751431941986084\n",
      "cnt: 0 - valLoss: 0.4686454236507416 - trainLoss: 0.4751347303390503\n",
      "cnt: 0 - valLoss: 0.4686359465122223 - trainLoss: 0.4751262962818146\n",
      "cnt: 0 - valLoss: 0.46862637996673584 - trainLoss: 0.47511786222457886\n",
      "cnt: 0 - valLoss: 0.46861687302589417 - trainLoss: 0.4751094579696655\n",
      "cnt: 0 - valLoss: 0.4686073660850525 - trainLoss: 0.4751009941101074\n",
      "cnt: 0 - valLoss: 0.46859779953956604 - trainLoss: 0.4750926196575165\n",
      "cnt: 0 - valLoss: 0.46858832240104675 - trainLoss: 0.47508418560028076\n",
      "cnt: 0 - valLoss: 0.4685788154602051 - trainLoss: 0.47507578134536743\n",
      "cnt: 0 - valLoss: 0.4685693085193634 - trainLoss: 0.4750673472881317\n",
      "cnt: 0 - valLoss: 0.46855977177619934 - trainLoss: 0.47505897283554077\n",
      "cnt: 0 - valLoss: 0.46855026483535767 - trainLoss: 0.47505053877830505\n",
      "cnt: 0 - valLoss: 0.468540757894516 - trainLoss: 0.47504210472106934\n",
      "cnt: 0 - valLoss: 0.4685313105583191 - trainLoss: 0.475033700466156\n",
      "cnt: 0 - valLoss: 0.4685218036174774 - trainLoss: 0.47502532601356506\n",
      "cnt: 0 - valLoss: 0.46851229667663574 - trainLoss: 0.47501692175865173\n",
      "cnt: 0 - valLoss: 0.46850284934043884 - trainLoss: 0.4750085473060608\n",
      "cnt: 0 - valLoss: 0.46849334239959717 - trainLoss: 0.47500014305114746\n",
      "cnt: 0 - valLoss: 0.46848392486572266 - trainLoss: 0.4749917685985565\n",
      "cnt: 0 - valLoss: 0.46847444772720337 - trainLoss: 0.4749833643436432\n",
      "cnt: 0 - valLoss: 0.4684649705886841 - trainLoss: 0.47497498989105225\n",
      "cnt: 0 - valLoss: 0.4684554636478424 - trainLoss: 0.4749665856361389\n",
      "cnt: 0 - valLoss: 0.4684460163116455 - trainLoss: 0.474958211183548\n",
      "cnt: 0 - valLoss: 0.46843650937080383 - trainLoss: 0.47494980692863464\n",
      "cnt: 0 - valLoss: 0.4684270918369293 - trainLoss: 0.4749414324760437\n",
      "cnt: 0 - valLoss: 0.4684176445007324 - trainLoss: 0.47493308782577515\n",
      "cnt: 0 - valLoss: 0.4684082269668579 - trainLoss: 0.4749247431755066\n",
      "cnt: 0 - valLoss: 0.468398779630661 - trainLoss: 0.47491633892059326\n",
      "cnt: 0 - valLoss: 0.4683893322944641 - trainLoss: 0.4749079942703247\n",
      "cnt: 0 - valLoss: 0.4683798551559448 - trainLoss: 0.47489961981773376\n",
      "cnt: 0 - valLoss: 0.4683704674243927 - trainLoss: 0.4748912751674652\n",
      "cnt: 0 - valLoss: 0.4683609902858734 - trainLoss: 0.4748828709125519\n",
      "cnt: 0 - valLoss: 0.4683515429496765 - trainLoss: 0.4748745858669281\n",
      "cnt: 0 - valLoss: 0.4683421850204468 - trainLoss: 0.47486618161201477\n",
      "cnt: 0 - valLoss: 0.4683327078819275 - trainLoss: 0.4748578667640686\n",
      "cnt: 0 - valLoss: 0.46832332015037537 - trainLoss: 0.47484949231147766\n",
      "cnt: 0 - valLoss: 0.46831393241882324 - trainLoss: 0.4748411774635315\n",
      "cnt: 0 - valLoss: 0.46830448508262634 - trainLoss: 0.47483283281326294\n",
      "cnt: 0 - valLoss: 0.4682950973510742 - trainLoss: 0.4748244881629944\n",
      "cnt: 0 - valLoss: 0.4682856798171997 - trainLoss: 0.47481611371040344\n",
      "cnt: 0 - valLoss: 0.4682762920856476 - trainLoss: 0.4748077988624573\n",
      "cnt: 0 - valLoss: 0.46826687455177307 - trainLoss: 0.4747994542121887\n",
      "cnt: 0 - valLoss: 0.46825748682022095 - trainLoss: 0.47479110956192017\n",
      "cnt: 0 - valLoss: 0.4682480990886688 - trainLoss: 0.474782794713974\n",
      "cnt: 0 - valLoss: 0.4682387113571167 - trainLoss: 0.47477445006370544\n",
      "cnt: 0 - valLoss: 0.4682293236255646 - trainLoss: 0.47476616501808167\n",
      "cnt: 0 - valLoss: 0.46821993589401245 - trainLoss: 0.4747578203678131\n",
      "cnt: 0 - valLoss: 0.4682105481624603 - trainLoss: 0.47474950551986694\n",
      "cnt: 0 - valLoss: 0.4682011008262634 - trainLoss: 0.4747411906719208\n",
      "cnt: 0 - valLoss: 0.4681917726993561 - trainLoss: 0.474732905626297\n",
      "cnt: 0 - valLoss: 0.4681823253631592 - trainLoss: 0.47472459077835083\n",
      "cnt: 0 - valLoss: 0.46817299723625183 - trainLoss: 0.47471630573272705\n",
      "cnt: 0 - valLoss: 0.4681636095046997 - trainLoss: 0.4747079908847809\n",
      "cnt: 0 - valLoss: 0.4681542217731476 - trainLoss: 0.4746996760368347\n",
      "cnt: 0 - valLoss: 0.46814486384391785 - trainLoss: 0.47469139099121094\n",
      "cnt: 0 - valLoss: 0.4681354761123657 - trainLoss: 0.47468307614326477\n",
      "cnt: 0 - valLoss: 0.468126118183136 - trainLoss: 0.4746748208999634\n",
      "cnt: 0 - valLoss: 0.4681166708469391 - trainLoss: 0.4746665060520172\n",
      "cnt: 0 - valLoss: 0.46810734272003174 - trainLoss: 0.4746582508087158\n",
      "cnt: 0 - valLoss: 0.46809789538383484 - trainLoss: 0.47464996576309204\n",
      "cnt: 0 - valLoss: 0.4680885672569275 - trainLoss: 0.47464174032211304\n",
      "cnt: 0 - valLoss: 0.4680791199207306 - trainLoss: 0.47463342547416687\n",
      "cnt: 0 - valLoss: 0.46806973218917847 - trainLoss: 0.4746251702308655\n",
      "cnt: 0 - valLoss: 0.46806034445762634 - trainLoss: 0.4746169149875641\n",
      "cnt: 0 - valLoss: 0.468051016330719 - trainLoss: 0.4746086001396179\n",
      "cnt: 0 - valLoss: 0.46804162859916687 - trainLoss: 0.4746003746986389\n",
      "cnt: 0 - valLoss: 0.46803224086761475 - trainLoss: 0.47459208965301514\n",
      "cnt: 0 - valLoss: 0.468022882938385 - trainLoss: 0.47458386421203613\n",
      "cnt: 0 - valLoss: 0.4680134952068329 - trainLoss: 0.47457554936408997\n",
      "cnt: 0 - valLoss: 0.46800410747528076 - trainLoss: 0.47456735372543335\n",
      "cnt: 0 - valLoss: 0.4679947793483734 - trainLoss: 0.47455906867980957\n",
      "cnt: 0 - valLoss: 0.4679854214191437 - trainLoss: 0.4745508134365082\n",
      "cnt: 0 - valLoss: 0.46797609329223633 - trainLoss: 0.47454261779785156\n",
      "cnt: 0 - valLoss: 0.4679667353630066 - trainLoss: 0.4745343327522278\n",
      "cnt: 0 - valLoss: 0.46795740723609924 - trainLoss: 0.4745260775089264\n",
      "cnt: 0 - valLoss: 0.4679480195045471 - trainLoss: 0.4745178520679474\n",
      "cnt: 0 - valLoss: 0.46793872117996216 - trainLoss: 0.47450965642929077\n",
      "cnt: 0 - valLoss: 0.4679293632507324 - trainLoss: 0.47450143098831177\n",
      "cnt: 0 - valLoss: 0.4679200351238251 - trainLoss: 0.4744931757450104\n",
      "cnt: 0 - valLoss: 0.46791067719459534 - trainLoss: 0.474484920501709\n",
      "cnt: 0 - valLoss: 0.4679013788700104 - trainLoss: 0.47447669506073\n",
      "cnt: 0 - valLoss: 0.4678920805454254 - trainLoss: 0.47446852922439575\n",
      "cnt: 0 - valLoss: 0.4678827226161957 - trainLoss: 0.474460244178772\n",
      "cnt: 0 - valLoss: 0.46787339448928833 - trainLoss: 0.47445204854011536\n",
      "cnt: 0 - valLoss: 0.46786409616470337 - trainLoss: 0.47444382309913635\n",
      "cnt: 0 - valLoss: 0.4678547978401184 - trainLoss: 0.4744356572628021\n",
      "cnt: 0 - valLoss: 0.46784552931785583 - trainLoss: 0.4744274318218231\n",
      "cnt: 0 - valLoss: 0.4678362309932709 - trainLoss: 0.4744192063808441\n",
      "cnt: 0 - valLoss: 0.4678269326686859 - trainLoss: 0.4744109809398651\n",
      "cnt: 0 - valLoss: 0.46781763434410095 - trainLoss: 0.4744028151035309\n",
      "cnt: 0 - valLoss: 0.467808336019516 - trainLoss: 0.4743945896625519\n",
      "cnt: 0 - valLoss: 0.4677990674972534 - trainLoss: 0.4743863642215729\n",
      "cnt: 0 - valLoss: 0.46778976917266846 - trainLoss: 0.47437819838523865\n",
      "cnt: 0 - valLoss: 0.4677804708480835 - trainLoss: 0.47436997294425964\n",
      "cnt: 0 - valLoss: 0.4677712023258209 - trainLoss: 0.4743618071079254\n",
      "cnt: 0 - valLoss: 0.46776196360588074 - trainLoss: 0.4743535816669464\n",
      "cnt: 0 - valLoss: 0.4677526354789734 - trainLoss: 0.47434544563293457\n",
      "cnt: 0 - valLoss: 0.4677433967590332 - trainLoss: 0.47433724999427795\n",
      "cnt: 0 - valLoss: 0.4677341878414154 - trainLoss: 0.47432905435562134\n",
      "cnt: 0 - valLoss: 0.46772488951683044 - trainLoss: 0.4743208885192871\n",
      "cnt: 0 - valLoss: 0.46771565079689026 - trainLoss: 0.4743127226829529\n",
      "cnt: 0 - valLoss: 0.4677063524723053 - trainLoss: 0.47430452704429626\n",
      "cnt: 0 - valLoss: 0.4676971435546875 - trainLoss: 0.47429633140563965\n",
      "cnt: 0 - valLoss: 0.4676878750324249 - trainLoss: 0.4742881953716278\n",
      "cnt: 0 - valLoss: 0.46767863631248474 - trainLoss: 0.47428005933761597\n",
      "cnt: 0 - valLoss: 0.46766936779022217 - trainLoss: 0.47427183389663696\n",
      "cnt: 0 - valLoss: 0.46766015887260437 - trainLoss: 0.4742636978626251\n",
      "cnt: 0 - valLoss: 0.4676509499549866 - trainLoss: 0.4742555618286133\n",
      "cnt: 0 - valLoss: 0.4676417112350464 - trainLoss: 0.47424736618995667\n",
      "cnt: 0 - valLoss: 0.4676325023174286 - trainLoss: 0.4742392301559448\n",
      "cnt: 0 - valLoss: 0.4676232635974884 - trainLoss: 0.474231094121933\n",
      "cnt: 0 - valLoss: 0.4676140248775482 - trainLoss: 0.47422292828559875\n",
      "cnt: 0 - valLoss: 0.4676048755645752 - trainLoss: 0.4742147922515869\n",
      "cnt: 0 - valLoss: 0.467595636844635 - trainLoss: 0.4742066562175751\n",
      "cnt: 0 - valLoss: 0.467586487531662 - trainLoss: 0.47419849038124084\n",
      "cnt: 0 - valLoss: 0.46757733821868896 - trainLoss: 0.474190354347229\n",
      "cnt: 0 - valLoss: 0.46756812930107117 - trainLoss: 0.47418221831321716\n",
      "cnt: 0 - valLoss: 0.46755895018577576 - trainLoss: 0.4741740822792053\n",
      "cnt: 0 - valLoss: 0.4675498604774475 - trainLoss: 0.4741659462451935\n",
      "cnt: 0 - valLoss: 0.4675407111644745 - trainLoss: 0.47415781021118164\n",
      "cnt: 0 - valLoss: 0.46753156185150146 - trainLoss: 0.4741496741771698\n",
      "cnt: 0 - valLoss: 0.46752235293388367 - trainLoss: 0.47414153814315796\n",
      "cnt: 0 - valLoss: 0.4675132632255554 - trainLoss: 0.4741334617137909\n",
      "cnt: 0 - valLoss: 0.46750408411026 - trainLoss: 0.47412538528442383\n",
      "cnt: 0 - valLoss: 0.467494934797287 - trainLoss: 0.4741171896457672\n",
      "cnt: 0 - valLoss: 0.46748578548431396 - trainLoss: 0.47410911321640015\n",
      "cnt: 0 - valLoss: 0.46747666597366333 - trainLoss: 0.4741010069847107\n",
      "cnt: 0 - valLoss: 0.4674675762653351 - trainLoss: 0.47409290075302124\n",
      "cnt: 0 - valLoss: 0.46745845675468445 - trainLoss: 0.4740847945213318\n",
      "cnt: 0 - valLoss: 0.4674493074417114 - trainLoss: 0.4740767180919647\n",
      "cnt: 0 - valLoss: 0.4674402177333832 - trainLoss: 0.47406861186027527\n",
      "cnt: 0 - valLoss: 0.4674311578273773 - trainLoss: 0.4740605354309082\n",
      "cnt: 0 - valLoss: 0.4674219787120819 - trainLoss: 0.47405242919921875\n",
      "cnt: 0 - valLoss: 0.46741288900375366 - trainLoss: 0.4740443825721741\n",
      "cnt: 0 - valLoss: 0.4674037992954254 - trainLoss: 0.47403624653816223\n",
      "cnt: 0 - valLoss: 0.46739470958709717 - trainLoss: 0.47402817010879517\n",
      "cnt: 0 - valLoss: 0.4673856198787689 - trainLoss: 0.4740201234817505\n",
      "cnt: 0 - valLoss: 0.4673765301704407 - trainLoss: 0.4740120470523834\n",
      "cnt: 0 - valLoss: 0.4673674404621124 - trainLoss: 0.47400400042533875\n",
      "cnt: 0 - valLoss: 0.4673583507537842 - trainLoss: 0.4739958941936493\n",
      "cnt: 0 - valLoss: 0.4673492908477783 - trainLoss: 0.4739878177642822\n",
      "cnt: 0 - valLoss: 0.4673402011394501 - trainLoss: 0.4739797115325928\n",
      "cnt: 0 - valLoss: 0.4673311114311218 - trainLoss: 0.4739716649055481\n",
      "cnt: 0 - valLoss: 0.4673220217227936 - trainLoss: 0.4739636182785034\n",
      "cnt: 0 - valLoss: 0.4673130214214325 - trainLoss: 0.47395557165145874\n",
      "cnt: 0 - valLoss: 0.46730393171310425 - trainLoss: 0.4739474356174469\n",
      "cnt: 0 - valLoss: 0.467294842004776 - trainLoss: 0.473939448595047\n",
      "cnt: 0 - valLoss: 0.4672858417034149 - trainLoss: 0.4739314019680023\n",
      "cnt: 0 - valLoss: 0.46727675199508667 - trainLoss: 0.47392329573631287\n",
      "cnt: 0 - valLoss: 0.4672677218914032 - trainLoss: 0.4739152193069458\n",
      "cnt: 0 - valLoss: 0.46725866198539734 - trainLoss: 0.4739071726799011\n",
      "cnt: 0 - valLoss: 0.46724963188171387 - trainLoss: 0.47389915585517883\n",
      "cnt: 0 - valLoss: 0.467240571975708 - trainLoss: 0.47389110922813416\n",
      "cnt: 0 - valLoss: 0.46723154187202454 - trainLoss: 0.47388312220573425\n",
      "cnt: 0 - valLoss: 0.4672224819660187 - trainLoss: 0.4738750159740448\n",
      "cnt: 0 - valLoss: 0.4672134816646576 - trainLoss: 0.4738670289516449\n",
      "cnt: 0 - valLoss: 0.4672044515609741 - trainLoss: 0.4738589823246002\n",
      "cnt: 0 - valLoss: 0.46719539165496826 - trainLoss: 0.47385093569755554\n",
      "cnt: 0 - valLoss: 0.4671863913536072 - trainLoss: 0.47384291887283325\n",
      "cnt: 0 - valLoss: 0.4671773910522461 - trainLoss: 0.4738348722457886\n",
      "cnt: 0 - valLoss: 0.4671683609485626 - trainLoss: 0.4738268256187439\n",
      "cnt: 0 - valLoss: 0.46715930104255676 - trainLoss: 0.473818838596344\n",
      "cnt: 0 - valLoss: 0.46715036034584045 - trainLoss: 0.4738108217716217\n",
      "cnt: 0 - valLoss: 0.46714136004447937 - trainLoss: 0.4738028347492218\n",
      "cnt: 0 - valLoss: 0.4671323001384735 - trainLoss: 0.4737947881221771\n",
      "cnt: 0 - valLoss: 0.4671233594417572 - trainLoss: 0.4737868010997772\n",
      "cnt: 0 - valLoss: 0.4671143591403961 - trainLoss: 0.47377875447273254\n",
      "cnt: 0 - valLoss: 0.4671053886413574 - trainLoss: 0.47377076745033264\n",
      "cnt: 0 - valLoss: 0.46709638833999634 - trainLoss: 0.47376275062561035\n",
      "cnt: 0 - valLoss: 0.46708738803863525 - trainLoss: 0.47375476360321045\n",
      "cnt: 0 - valLoss: 0.46707838773727417 - trainLoss: 0.47374674677848816\n",
      "cnt: 0 - valLoss: 0.4670693874359131 - trainLoss: 0.47373878955841064\n",
      "cnt: 0 - valLoss: 0.4670604467391968 - trainLoss: 0.47373074293136597\n",
      "cnt: 0 - valLoss: 0.4670514762401581 - trainLoss: 0.47372278571128845\n",
      "cnt: 0 - valLoss: 0.4670425057411194 - trainLoss: 0.47371476888656616\n",
      "cnt: 0 - valLoss: 0.4670335650444031 - trainLoss: 0.47370678186416626\n",
      "cnt: 0 - valLoss: 0.4670245945453644 - trainLoss: 0.47369882464408875\n",
      "cnt: 0 - valLoss: 0.46701568365097046 - trainLoss: 0.47369083762168884\n",
      "cnt: 0 - valLoss: 0.46700671315193176 - trainLoss: 0.47368279099464417\n",
      "cnt: 0 - valLoss: 0.46699777245521545 - trainLoss: 0.4736748933792114\n",
      "cnt: 0 - valLoss: 0.46698880195617676 - trainLoss: 0.47366687655448914\n",
      "cnt: 0 - valLoss: 0.46697989106178284 - trainLoss: 0.4736589193344116\n",
      "cnt: 0 - valLoss: 0.46697095036506653 - trainLoss: 0.4736509621143341\n",
      "cnt: 0 - valLoss: 0.46696197986602783 - trainLoss: 0.4736430048942566\n",
      "cnt: 0 - valLoss: 0.4669530987739563 - trainLoss: 0.4736350476741791\n",
      "cnt: 0 - valLoss: 0.46694415807724 - trainLoss: 0.47362709045410156\n",
      "cnt: 0 - valLoss: 0.46693524718284607 - trainLoss: 0.47361910343170166\n",
      "cnt: 0 - valLoss: 0.46692630648612976 - trainLoss: 0.47361114621162415\n",
      "cnt: 0 - valLoss: 0.46691736578941345 - trainLoss: 0.47360318899154663\n",
      "cnt: 0 - valLoss: 0.4669084846973419 - trainLoss: 0.4735952317714691\n",
      "cnt: 0 - valLoss: 0.466899573802948 - trainLoss: 0.4735872745513916\n",
      "cnt: 0 - valLoss: 0.4668906629085541 - trainLoss: 0.4735793471336365\n",
      "cnt: 0 - valLoss: 0.46688178181648254 - trainLoss: 0.47357138991355896\n",
      "cnt: 0 - valLoss: 0.4668728709220886 - trainLoss: 0.47356346249580383\n",
      "cnt: 0 - valLoss: 0.4668639898300171 - trainLoss: 0.4735555052757263\n",
      "cnt: 0 - valLoss: 0.46685507893562317 - trainLoss: 0.4735476076602936\n",
      "cnt: 0 - valLoss: 0.46684619784355164 - trainLoss: 0.4735396206378937\n",
      "cnt: 0 - valLoss: 0.4668372571468353 - trainLoss: 0.47353172302246094\n",
      "cnt: 0 - valLoss: 0.46682843565940857 - trainLoss: 0.4735237956047058\n",
      "cnt: 0 - valLoss: 0.46681955456733704 - trainLoss: 0.4735158383846283\n",
      "cnt: 0 - valLoss: 0.4668106734752655 - trainLoss: 0.47350791096687317\n",
      "cnt: 0 - valLoss: 0.4668017625808716 - trainLoss: 0.47350001335144043\n",
      "cnt: 0 - valLoss: 0.4667929410934448 - trainLoss: 0.4734920859336853\n",
      "cnt: 0 - valLoss: 0.4667840301990509 - trainLoss: 0.4734841287136078\n",
      "cnt: 0 - valLoss: 0.46677517890930176 - trainLoss: 0.47347623109817505\n",
      "cnt: 0 - valLoss: 0.4667663276195526 - trainLoss: 0.4734683334827423\n",
      "cnt: 0 - valLoss: 0.4667574465274811 - trainLoss: 0.4734604060649872\n",
      "cnt: 0 - valLoss: 0.4667486250400543 - trainLoss: 0.47345253825187683\n",
      "cnt: 0 - valLoss: 0.4667397439479828 - trainLoss: 0.4734446108341217\n",
      "cnt: 0 - valLoss: 0.46673086285591125 - trainLoss: 0.4734366834163666\n",
      "cnt: 0 - valLoss: 0.4667220413684845 - trainLoss: 0.47342872619628906\n",
      "cnt: 0 - valLoss: 0.46671321988105774 - trainLoss: 0.4734208583831787\n",
      "cnt: 0 - valLoss: 0.4667043685913086 - trainLoss: 0.4734129309654236\n",
      "cnt: 0 - valLoss: 0.46669554710388184 - trainLoss: 0.47340506315231323\n",
      "cnt: 0 - valLoss: 0.4666867256164551 - trainLoss: 0.4733971953392029\n",
      "cnt: 0 - valLoss: 0.4666779041290283 - trainLoss: 0.47338932752609253\n",
      "cnt: 0 - valLoss: 0.4666690528392792 - trainLoss: 0.4733814299106598\n",
      "cnt: 0 - valLoss: 0.4666602313518524 - trainLoss: 0.47337353229522705\n",
      "cnt: 0 - valLoss: 0.46665140986442566 - trainLoss: 0.4733656346797943\n",
      "cnt: 0 - valLoss: 0.4666425585746765 - trainLoss: 0.47335776686668396\n",
      "cnt: 0 - valLoss: 0.46663379669189453 - trainLoss: 0.47334983944892883\n",
      "cnt: 0 - valLoss: 0.4666249454021454 - trainLoss: 0.4733419418334961\n",
      "cnt: 0 - valLoss: 0.4666161835193634 - trainLoss: 0.47333410382270813\n",
      "cnt: 0 - valLoss: 0.46660733222961426 - trainLoss: 0.4733262360095978\n",
      "cnt: 0 - valLoss: 0.4665985405445099 - trainLoss: 0.47331833839416504\n",
      "cnt: 0 - valLoss: 0.4665897786617279 - trainLoss: 0.4733104705810547\n",
      "cnt: 0 - valLoss: 0.46658092737197876 - trainLoss: 0.4733026325702667\n",
      "cnt: 0 - valLoss: 0.46657219529151917 - trainLoss: 0.47329476475715637\n",
      "cnt: 0 - valLoss: 0.4665634036064148 - trainLoss: 0.473286896944046\n",
      "cnt: 0 - valLoss: 0.46655458211898804 - trainLoss: 0.4732789993286133\n",
      "cnt: 0 - valLoss: 0.46654582023620605 - trainLoss: 0.4732711911201477\n",
      "cnt: 0 - valLoss: 0.4665370583534241 - trainLoss: 0.47326329350471497\n",
      "cnt: 0 - valLoss: 0.4665282666683197 - trainLoss: 0.4732554852962494\n",
      "cnt: 0 - valLoss: 0.46651947498321533 - trainLoss: 0.47324758768081665\n",
      "cnt: 0 - valLoss: 0.4665106534957886 - trainLoss: 0.4732397198677063\n",
      "cnt: 0 - valLoss: 0.46650195121765137 - trainLoss: 0.47323188185691833\n",
      "cnt: 0 - valLoss: 0.4664931893348694 - trainLoss: 0.47322404384613037\n",
      "cnt: 0 - valLoss: 0.4664844274520874 - trainLoss: 0.4732162356376648\n",
      "cnt: 0 - valLoss: 0.4664756655693054 - trainLoss: 0.47320839762687683\n",
      "cnt: 0 - valLoss: 0.4664669334888458 - trainLoss: 0.4732005298137665\n",
      "cnt: 0 - valLoss: 0.46645814180374146 - trainLoss: 0.4731927216053009\n",
      "cnt: 0 - valLoss: 0.4664493799209595 - trainLoss: 0.47318485379219055\n",
      "cnt: 0 - valLoss: 0.46644070744514465 - trainLoss: 0.47317707538604736\n",
      "cnt: 0 - valLoss: 0.46643194556236267 - trainLoss: 0.4731691777706146\n",
      "cnt: 0 - valLoss: 0.4664232134819031 - trainLoss: 0.47316139936447144\n",
      "cnt: 0 - valLoss: 0.4664144515991211 - trainLoss: 0.4731535315513611\n",
      "cnt: 0 - valLoss: 0.4664057195186615 - trainLoss: 0.4731457233428955\n",
      "cnt: 0 - valLoss: 0.4663969874382019 - trainLoss: 0.47313785552978516\n",
      "cnt: 0 - valLoss: 0.4663882255554199 - trainLoss: 0.47313007712364197\n",
      "cnt: 0 - valLoss: 0.4663795232772827 - trainLoss: 0.4731222689151764\n",
      "cnt: 0 - valLoss: 0.4663708209991455 - trainLoss: 0.4731144905090332\n",
      "cnt: 0 - valLoss: 0.4663620889186859 - trainLoss: 0.47310665249824524\n",
      "cnt: 0 - valLoss: 0.4663534164428711 - trainLoss: 0.47309887409210205\n",
      "cnt: 0 - valLoss: 0.4663447141647339 - trainLoss: 0.4730910360813141\n",
      "cnt: 0 - valLoss: 0.4663360118865967 - trainLoss: 0.4730832278728485\n",
      "cnt: 0 - valLoss: 0.4663272798061371 - trainLoss: 0.4730754494667053\n",
      "cnt: 0 - valLoss: 0.46631863713264465 - trainLoss: 0.47306767106056213\n",
      "cnt: 0 - valLoss: 0.46630993485450745 - trainLoss: 0.47305983304977417\n",
      "cnt: 0 - valLoss: 0.46630123257637024 - trainLoss: 0.4730520248413086\n",
      "cnt: 0 - valLoss: 0.4662925601005554 - trainLoss: 0.473044216632843\n",
      "cnt: 0 - valLoss: 0.466283917427063 - trainLoss: 0.47303640842437744\n",
      "cnt: 0 - valLoss: 0.46627524495124817 - trainLoss: 0.47302865982055664\n",
      "cnt: 0 - valLoss: 0.46626654267311096 - trainLoss: 0.47302088141441345\n",
      "cnt: 0 - valLoss: 0.46625784039497375 - trainLoss: 0.4730130732059479\n",
      "cnt: 0 - valLoss: 0.4662491977214813 - trainLoss: 0.4730052649974823\n",
      "cnt: 0 - valLoss: 0.4662405252456665 - trainLoss: 0.4729975163936615\n",
      "cnt: 0 - valLoss: 0.4662318825721741 - trainLoss: 0.4729897677898407\n",
      "cnt: 0 - valLoss: 0.46622321009635925 - trainLoss: 0.4729819595813751\n",
      "cnt: 0 - valLoss: 0.4662145972251892 - trainLoss: 0.47297418117523193\n",
      "cnt: 0 - valLoss: 0.466205894947052 - trainLoss: 0.47296643257141113\n",
      "cnt: 0 - valLoss: 0.46619728207588196 - trainLoss: 0.47295868396759033\n",
      "cnt: 0 - valLoss: 0.46618860960006714 - trainLoss: 0.47295090556144714\n",
      "cnt: 0 - valLoss: 0.4661799669265747 - trainLoss: 0.47294309735298157\n",
      "cnt: 0 - valLoss: 0.46617135405540466 - trainLoss: 0.4729353189468384\n",
      "cnt: 0 - valLoss: 0.46616268157958984 - trainLoss: 0.47292760014533997\n",
      "cnt: 0 - valLoss: 0.4661540687084198 - trainLoss: 0.47291985154151917\n",
      "cnt: 0 - valLoss: 0.46614545583724976 - trainLoss: 0.472912073135376\n",
      "cnt: 0 - valLoss: 0.4661368131637573 - trainLoss: 0.4729043245315552\n",
      "cnt: 0 - valLoss: 0.46612823009490967 - trainLoss: 0.4728965759277344\n",
      "cnt: 0 - valLoss: 0.46611958742141724 - trainLoss: 0.4728888273239136\n",
      "cnt: 0 - valLoss: 0.4661109447479248 - trainLoss: 0.4728810787200928\n",
      "cnt: 0 - valLoss: 0.46610233187675476 - trainLoss: 0.472873330116272\n",
      "cnt: 0 - valLoss: 0.4660937786102295 - trainLoss: 0.47286558151245117\n",
      "cnt: 0 - valLoss: 0.4660851061344147 - trainLoss: 0.47285783290863037\n",
      "cnt: 0 - valLoss: 0.4660765528678894 - trainLoss: 0.47285008430480957\n",
      "cnt: 0 - valLoss: 0.466067910194397 - trainLoss: 0.47284236550331116\n",
      "cnt: 0 - valLoss: 0.46605929732322693 - trainLoss: 0.47283464670181274\n",
      "cnt: 0 - valLoss: 0.46605074405670166 - trainLoss: 0.4728269577026367\n",
      "cnt: 0 - valLoss: 0.466042160987854 - trainLoss: 0.4728192090988159\n",
      "cnt: 0 - valLoss: 0.46603354811668396 - trainLoss: 0.4728114604949951\n",
      "cnt: 0 - valLoss: 0.4660249650478363 - trainLoss: 0.4728037416934967\n",
      "cnt: 0 - valLoss: 0.46601641178131104 - trainLoss: 0.4727960526943207\n",
      "cnt: 0 - valLoss: 0.4660078287124634 - trainLoss: 0.4727883040904999\n",
      "cnt: 0 - valLoss: 0.4659993052482605 - trainLoss: 0.47278064489364624\n",
      "cnt: 0 - valLoss: 0.46599072217941284 - trainLoss: 0.47277283668518066\n",
      "cnt: 0 - valLoss: 0.4659821689128876 - trainLoss: 0.47276514768600464\n",
      "cnt: 0 - valLoss: 0.4659735858440399 - trainLoss: 0.4727574288845062\n",
      "cnt: 0 - valLoss: 0.46596500277519226 - trainLoss: 0.4727497398853302\n",
      "cnt: 0 - valLoss: 0.4659564793109894 - trainLoss: 0.4727420210838318\n",
      "cnt: 0 - valLoss: 0.4659479558467865 - trainLoss: 0.47273433208465576\n",
      "cnt: 0 - valLoss: 0.4659394323825836 - trainLoss: 0.47272661328315735\n",
      "cnt: 0 - valLoss: 0.46593090891838074 - trainLoss: 0.4727189242839813\n",
      "cnt: 0 - valLoss: 0.4659223258495331 - trainLoss: 0.4727112054824829\n",
      "cnt: 0 - valLoss: 0.4659138321876526 - trainLoss: 0.4727035164833069\n",
      "cnt: 0 - valLoss: 0.4659052789211273 - trainLoss: 0.47269579768180847\n",
      "cnt: 0 - valLoss: 0.4658967852592468 - trainLoss: 0.47268810868263245\n",
      "cnt: 0 - valLoss: 0.46588826179504395 - trainLoss: 0.4726804792881012\n",
      "cnt: 0 - valLoss: 0.46587976813316345 - trainLoss: 0.4726727604866028\n",
      "cnt: 0 - valLoss: 0.46587124466896057 - trainLoss: 0.47266510128974915\n",
      "cnt: 0 - valLoss: 0.4658627510070801 - trainLoss: 0.47265735268592834\n",
      "cnt: 0 - valLoss: 0.4658542573451996 - trainLoss: 0.4726497530937195\n",
      "cnt: 0 - valLoss: 0.4658457636833191 - trainLoss: 0.47264203429222107\n",
      "cnt: 0 - valLoss: 0.465837299823761 - trainLoss: 0.47263437509536743\n",
      "cnt: 0 - valLoss: 0.4658288061618805 - trainLoss: 0.4726267158985138\n",
      "cnt: 0 - valLoss: 0.4658202826976776 - trainLoss: 0.47261902689933777\n",
      "cnt: 0 - valLoss: 0.4658118188381195 - trainLoss: 0.47261133790016174\n",
      "cnt: 0 - valLoss: 0.465803325176239 - trainLoss: 0.4726036787033081\n",
      "cnt: 0 - valLoss: 0.4657948613166809 - trainLoss: 0.47259607911109924\n",
      "cnt: 0 - valLoss: 0.4657863676548004 - trainLoss: 0.47258836030960083\n",
      "cnt: 0 - valLoss: 0.4657779335975647 - trainLoss: 0.47258076071739197\n",
      "cnt: 0 - valLoss: 0.4657694697380066 - trainLoss: 0.47257307171821594\n",
      "cnt: 0 - valLoss: 0.4657610058784485 - trainLoss: 0.4725654125213623\n",
      "cnt: 0 - valLoss: 0.465752512216568 - trainLoss: 0.47255775332450867\n",
      "cnt: 0 - valLoss: 0.4657440185546875 - trainLoss: 0.47255009412765503\n",
      "cnt: 0 - valLoss: 0.46573561429977417 - trainLoss: 0.47254249453544617\n",
      "cnt: 0 - valLoss: 0.4657271206378937 - trainLoss: 0.4725348651409149\n",
      "cnt: 0 - valLoss: 0.46571868658065796 - trainLoss: 0.4725272059440613\n",
      "cnt: 0 - valLoss: 0.46571025252342224 - trainLoss: 0.47251954674720764\n",
      "cnt: 0 - valLoss: 0.46570178866386414 - trainLoss: 0.4725119173526764\n",
      "cnt: 0 - valLoss: 0.4656933546066284 - trainLoss: 0.47250431776046753\n",
      "cnt: 0 - valLoss: 0.4656849205493927 - trainLoss: 0.4724966883659363\n",
      "cnt: 0 - valLoss: 0.4656764566898346 - trainLoss: 0.47248902916908264\n",
      "cnt: 0 - valLoss: 0.4656680226325989 - trainLoss: 0.472481369972229\n",
      "cnt: 0 - valLoss: 0.46565958857536316 - trainLoss: 0.47247374057769775\n",
      "cnt: 0 - valLoss: 0.46565112471580505 - trainLoss: 0.4724661111831665\n",
      "cnt: 0 - valLoss: 0.46564269065856934 - trainLoss: 0.47245851159095764\n",
      "cnt: 0 - valLoss: 0.465634286403656 - trainLoss: 0.4724508821964264\n",
      "cnt: 0 - valLoss: 0.4656258821487427 - trainLoss: 0.47244322299957275\n",
      "cnt: 0 - valLoss: 0.46561744809150696 - trainLoss: 0.4724355936050415\n",
      "cnt: 0 - valLoss: 0.46560904383659363 - trainLoss: 0.47242802381515503\n",
      "cnt: 0 - valLoss: 0.4656006097793579 - trainLoss: 0.47242042422294617\n",
      "cnt: 0 - valLoss: 0.4655922055244446 - trainLoss: 0.4724127948284149\n",
      "cnt: 0 - valLoss: 0.46558380126953125 - trainLoss: 0.4724051356315613\n",
      "cnt: 0 - valLoss: 0.46557536721229553 - trainLoss: 0.4723975658416748\n",
      "cnt: 0 - valLoss: 0.4655669629573822 - trainLoss: 0.47238993644714355\n",
      "cnt: 0 - valLoss: 0.46555858850479126 - trainLoss: 0.4723823070526123\n",
      "cnt: 0 - valLoss: 0.46555018424987793 - trainLoss: 0.47237473726272583\n",
      "cnt: 0 - valLoss: 0.465541809797287 - trainLoss: 0.47236713767051697\n",
      "cnt: 0 - valLoss: 0.46553340554237366 - trainLoss: 0.4723595380783081\n",
      "cnt: 0 - valLoss: 0.4655250310897827 - trainLoss: 0.47235193848609924\n",
      "cnt: 0 - valLoss: 0.4655166268348694 - trainLoss: 0.47234436869621277\n",
      "cnt: 0 - valLoss: 0.46550828218460083 - trainLoss: 0.4723367691040039\n",
      "cnt: 0 - valLoss: 0.4654998779296875 - trainLoss: 0.47232916951179504\n",
      "cnt: 0 - valLoss: 0.46549153327941895 - trainLoss: 0.4723215699195862\n",
      "cnt: 0 - valLoss: 0.465483158826828 - trainLoss: 0.4723140001296997\n",
      "cnt: 0 - valLoss: 0.46547481417655945 - trainLoss: 0.47230643033981323\n",
      "cnt: 0 - valLoss: 0.4654664397239685 - trainLoss: 0.47229886054992676\n",
      "cnt: 0 - valLoss: 0.46545809507369995 - trainLoss: 0.4722912907600403\n",
      "cnt: 0 - valLoss: 0.465449720621109 - trainLoss: 0.4722836911678314\n",
      "cnt: 0 - valLoss: 0.46544137597084045 - trainLoss: 0.47227612137794495\n",
      "cnt: 0 - valLoss: 0.4654330015182495 - trainLoss: 0.47226858139038086\n",
      "cnt: 0 - valLoss: 0.46542468667030334 - trainLoss: 0.47226107120513916\n",
      "cnt: 0 - valLoss: 0.4654163718223572 - trainLoss: 0.4722535312175751\n",
      "cnt: 0 - valLoss: 0.4654080271720886 - trainLoss: 0.472245991230011\n",
      "cnt: 0 - valLoss: 0.46539971232414246 - trainLoss: 0.4722385108470917\n",
      "cnt: 0 - valLoss: 0.4653913974761963 - trainLoss: 0.4722309708595276\n",
      "cnt: 0 - valLoss: 0.4653830826282501 - trainLoss: 0.4722234308719635\n",
      "cnt: 0 - valLoss: 0.4653747081756592 - trainLoss: 0.4722159206867218\n",
      "cnt: 0 - valLoss: 0.4653664529323578 - trainLoss: 0.4722084105014801\n",
      "cnt: 0 - valLoss: 0.46535807847976685 - trainLoss: 0.472200870513916\n",
      "cnt: 0 - valLoss: 0.46534982323646545 - trainLoss: 0.4721933901309967\n",
      "cnt: 0 - valLoss: 0.4653415083885193 - trainLoss: 0.4721859097480774\n",
      "cnt: 0 - valLoss: 0.4653332233428955 - trainLoss: 0.4721783995628357\n",
      "cnt: 0 - valLoss: 0.46532487869262695 - trainLoss: 0.472170889377594\n",
      "cnt: 0 - valLoss: 0.4653165936470032 - trainLoss: 0.4721633493900299\n",
      "cnt: 0 - valLoss: 0.465308278799057 - trainLoss: 0.472155898809433\n",
      "cnt: 0 - valLoss: 0.4653000235557556 - trainLoss: 0.4721483588218689\n",
      "cnt: 0 - valLoss: 0.46529173851013184 - trainLoss: 0.4721408784389496\n",
      "cnt: 0 - valLoss: 0.46528342366218567 - trainLoss: 0.4721333980560303\n",
      "cnt: 0 - valLoss: 0.4652751684188843 - trainLoss: 0.4721258878707886\n",
      "cnt: 0 - valLoss: 0.4652668833732605 - trainLoss: 0.47211840748786926\n",
      "cnt: 0 - valLoss: 0.4652585983276367 - trainLoss: 0.47211095690727234\n",
      "cnt: 0 - valLoss: 0.46525028347969055 - trainLoss: 0.47210341691970825\n",
      "cnt: 0 - valLoss: 0.46524202823638916 - trainLoss: 0.47209593653678894\n",
      "cnt: 0 - valLoss: 0.46523380279541016 - trainLoss: 0.472088485956192\n",
      "cnt: 0 - valLoss: 0.4652255177497864 - trainLoss: 0.47208094596862793\n",
      "cnt: 0 - valLoss: 0.4652172327041626 - trainLoss: 0.4720735549926758\n",
      "cnt: 0 - valLoss: 0.4652089774608612 - trainLoss: 0.4720660150051117\n",
      "cnt: 0 - valLoss: 0.4652007520198822 - trainLoss: 0.47205856442451477\n",
      "cnt: 0 - valLoss: 0.4651924669742584 - trainLoss: 0.47205111384391785\n",
      "cnt: 0 - valLoss: 0.46518421173095703 - trainLoss: 0.47204357385635376\n",
      "cnt: 0 - valLoss: 0.465175986289978 - trainLoss: 0.4720361530780792\n",
      "cnt: 0 - valLoss: 0.46516773104667664 - trainLoss: 0.4720287024974823\n",
      "cnt: 0 - valLoss: 0.46515950560569763 - trainLoss: 0.4720212519168854\n",
      "cnt: 0 - valLoss: 0.46515128016471863 - trainLoss: 0.47201380133628845\n",
      "cnt: 0 - valLoss: 0.46514302492141724 - trainLoss: 0.47200635075569153\n",
      "cnt: 0 - valLoss: 0.46513476967811584 - trainLoss: 0.4719989001750946\n",
      "cnt: 0 - valLoss: 0.46512654423713684 - trainLoss: 0.4719914197921753\n",
      "cnt: 0 - valLoss: 0.46511831879615784 - trainLoss: 0.47198396921157837\n",
      "cnt: 0 - valLoss: 0.46511009335517883 - trainLoss: 0.47197651863098145\n",
      "cnt: 0 - valLoss: 0.4651018977165222 - trainLoss: 0.4719690978527069\n",
      "cnt: 0 - valLoss: 0.465093731880188 - trainLoss: 0.4719616770744324\n",
      "cnt: 0 - valLoss: 0.4650854766368866 - trainLoss: 0.47195422649383545\n",
      "cnt: 0 - valLoss: 0.4650772511959076 - trainLoss: 0.4719467759132385\n",
      "cnt: 0 - valLoss: 0.46506908535957336 - trainLoss: 0.4719393253326416\n",
      "cnt: 0 - valLoss: 0.46506088972091675 - trainLoss: 0.47193190455436707\n",
      "cnt: 0 - valLoss: 0.46505266427993774 - trainLoss: 0.4719245135784149\n",
      "cnt: 0 - valLoss: 0.46504446864128113 - trainLoss: 0.471917062997818\n",
      "cnt: 0 - valLoss: 0.4650362730026245 - trainLoss: 0.47190961241722107\n",
      "cnt: 0 - valLoss: 0.4650281071662903 - trainLoss: 0.4719022214412689\n",
      "cnt: 0 - valLoss: 0.46501991152763367 - trainLoss: 0.4718948006629944\n",
      "cnt: 0 - valLoss: 0.46501171588897705 - trainLoss: 0.47188735008239746\n",
      "cnt: 0 - valLoss: 0.4650035500526428 - trainLoss: 0.4718799591064453\n",
      "cnt: 0 - valLoss: 0.4649953842163086 - trainLoss: 0.4718725383281708\n",
      "cnt: 0 - valLoss: 0.46498721837997437 - trainLoss: 0.47186511754989624\n",
      "cnt: 0 - valLoss: 0.46497902274131775 - trainLoss: 0.4718577563762665\n",
      "cnt: 0 - valLoss: 0.4649708867073059 - trainLoss: 0.47185030579566956\n",
      "cnt: 0 - valLoss: 0.4649627208709717 - trainLoss: 0.4718429446220398\n",
      "cnt: 0 - valLoss: 0.4649544954299927 - trainLoss: 0.47183549404144287\n",
      "cnt: 0 - valLoss: 0.46494635939598083 - trainLoss: 0.47182807326316833\n",
      "cnt: 0 - valLoss: 0.464938223361969 - trainLoss: 0.4718206822872162\n",
      "cnt: 0 - valLoss: 0.46493008732795715 - trainLoss: 0.4718133211135864\n",
      "cnt: 0 - valLoss: 0.46492189168930054 - trainLoss: 0.4718058705329895\n",
      "cnt: 0 - valLoss: 0.4649137556552887 - trainLoss: 0.47179850935935974\n",
      "cnt: 0 - valLoss: 0.46490564942359924 - trainLoss: 0.47179114818573\n",
      "cnt: 0 - valLoss: 0.4648975133895874 - trainLoss: 0.47178372740745544\n",
      "cnt: 0 - valLoss: 0.4648893475532532 - trainLoss: 0.4717763662338257\n",
      "cnt: 0 - valLoss: 0.46488121151924133 - trainLoss: 0.47176897525787354\n",
      "cnt: 0 - valLoss: 0.4648730754852295 - trainLoss: 0.4717616140842438\n",
      "cnt: 0 - valLoss: 0.4648649990558624 - trainLoss: 0.471754252910614\n",
      "cnt: 0 - valLoss: 0.4648568630218506 - trainLoss: 0.47174689173698425\n",
      "cnt: 0 - valLoss: 0.46484872698783875 - trainLoss: 0.4717395305633545\n",
      "cnt: 0 - valLoss: 0.4648406207561493 - trainLoss: 0.47173210978507996\n",
      "cnt: 0 - valLoss: 0.46483251452445984 - trainLoss: 0.4717247486114502\n",
      "cnt: 0 - valLoss: 0.464824378490448 - trainLoss: 0.4717174470424652\n",
      "cnt: 0 - valLoss: 0.4648163318634033 - trainLoss: 0.4717100262641907\n",
      "cnt: 0 - valLoss: 0.46480822563171387 - trainLoss: 0.4717026650905609\n",
      "cnt: 0 - valLoss: 0.4648001194000244 - trainLoss: 0.47169530391693115\n",
      "cnt: 0 - valLoss: 0.46479201316833496 - trainLoss: 0.4716879427433014\n",
      "cnt: 0 - valLoss: 0.4647839069366455 - trainLoss: 0.47168058156967163\n",
      "cnt: 0 - valLoss: 0.46477580070495605 - trainLoss: 0.47167322039604187\n",
      "cnt: 0 - valLoss: 0.4647677540779114 - trainLoss: 0.4716659188270569\n",
      "cnt: 0 - valLoss: 0.4647596478462219 - trainLoss: 0.4716585576534271\n",
      "cnt: 0 - valLoss: 0.46475154161453247 - trainLoss: 0.47165119647979736\n",
      "cnt: 0 - valLoss: 0.4647434651851654 - trainLoss: 0.47164386510849\n",
      "cnt: 0 - valLoss: 0.46473538875579834 - trainLoss: 0.4716365337371826\n",
      "cnt: 0 - valLoss: 0.46472734212875366 - trainLoss: 0.47162917256355286\n",
      "cnt: 0 - valLoss: 0.4647192656993866 - trainLoss: 0.4716218113899231\n",
      "cnt: 0 - valLoss: 0.4647112190723419 - trainLoss: 0.4716145098209381\n",
      "cnt: 0 - valLoss: 0.46470311284065247 - trainLoss: 0.47160717844963074\n",
      "cnt: 0 - valLoss: 0.4646950960159302 - trainLoss: 0.47159987688064575\n",
      "cnt: 0 - valLoss: 0.4646870493888855 - trainLoss: 0.4715925455093384\n",
      "cnt: 0 - valLoss: 0.4646790027618408 - trainLoss: 0.4715851843357086\n",
      "cnt: 0 - valLoss: 0.46467092633247375 - trainLoss: 0.471577912569046\n",
      "cnt: 0 - valLoss: 0.46466290950775146 - trainLoss: 0.47157055139541626\n",
      "cnt: 0 - valLoss: 0.4646548926830292 - trainLoss: 0.4715632200241089\n",
      "cnt: 0 - valLoss: 0.4646468460559845 - trainLoss: 0.4715559184551239\n",
      "cnt: 0 - valLoss: 0.4646387994289398 - trainLoss: 0.4715486168861389\n",
      "cnt: 0 - valLoss: 0.46463072299957275 - trainLoss: 0.47154125571250916\n",
      "cnt: 0 - valLoss: 0.4646226763725281 - trainLoss: 0.47153398394584656\n",
      "cnt: 0 - valLoss: 0.4646146893501282 - trainLoss: 0.47152671217918396\n",
      "cnt: 0 - valLoss: 0.4646066427230835 - trainLoss: 0.4715193808078766\n",
      "cnt: 0 - valLoss: 0.4645986557006836 - trainLoss: 0.4715120792388916\n",
      "cnt: 0 - valLoss: 0.4645906090736389 - trainLoss: 0.471504807472229\n",
      "cnt: 0 - valLoss: 0.464582622051239 - trainLoss: 0.4714975357055664\n",
      "cnt: 0 - valLoss: 0.4645746350288391 - trainLoss: 0.47149017453193665\n",
      "cnt: 0 - valLoss: 0.4645666182041168 - trainLoss: 0.47148287296295166\n",
      "cnt: 0 - valLoss: 0.46455860137939453 - trainLoss: 0.47147560119628906\n",
      "cnt: 0 - valLoss: 0.46455058455467224 - trainLoss: 0.47146832942962646\n",
      "cnt: 0 - valLoss: 0.46454259753227234 - trainLoss: 0.47146105766296387\n",
      "cnt: 0 - valLoss: 0.46453458070755005 - trainLoss: 0.47145378589630127\n",
      "cnt: 0 - valLoss: 0.46452656388282776 - trainLoss: 0.47144651412963867\n",
      "cnt: 0 - valLoss: 0.46451860666275024 - trainLoss: 0.4714391529560089\n",
      "cnt: 0 - valLoss: 0.46451058983802795 - trainLoss: 0.4714319109916687\n",
      "cnt: 0 - valLoss: 0.46450257301330566 - trainLoss: 0.47142457962036133\n",
      "cnt: 0 - valLoss: 0.46449458599090576 - trainLoss: 0.47141730785369873\n",
      "cnt: 0 - valLoss: 0.46448662877082825 - trainLoss: 0.4714100658893585\n",
      "cnt: 0 - valLoss: 0.4644787013530731 - trainLoss: 0.4714027941226959\n",
      "cnt: 0 - valLoss: 0.46447068452835083 - trainLoss: 0.47139549255371094\n",
      "cnt: 0 - valLoss: 0.4644626975059509 - trainLoss: 0.4713882505893707\n",
      "cnt: 0 - valLoss: 0.4644547402858734 - trainLoss: 0.47138097882270813\n",
      "cnt: 0 - valLoss: 0.4644467532634735 - trainLoss: 0.4713737368583679\n",
      "cnt: 0 - valLoss: 0.4644388258457184 - trainLoss: 0.4713664650917053\n",
      "cnt: 0 - valLoss: 0.4644308090209961 - trainLoss: 0.4713592231273651\n",
      "cnt: 0 - valLoss: 0.46442288160324097 - trainLoss: 0.4713519215583801\n",
      "cnt: 0 - valLoss: 0.46441492438316345 - trainLoss: 0.4713446795940399\n",
      "cnt: 0 - valLoss: 0.4644069969654083 - trainLoss: 0.4713374376296997\n",
      "cnt: 0 - valLoss: 0.4643990099430084 - trainLoss: 0.4713301658630371\n",
      "cnt: 0 - valLoss: 0.4643910825252533 - trainLoss: 0.4713228940963745\n",
      "cnt: 0 - valLoss: 0.46438315510749817 - trainLoss: 0.4713156521320343\n",
      "cnt: 0 - valLoss: 0.46437519788742065 - trainLoss: 0.4713084399700165\n",
      "cnt: 0 - valLoss: 0.4643672704696655 - trainLoss: 0.47130119800567627\n",
      "cnt: 0 - valLoss: 0.464359313249588 - trainLoss: 0.47129395604133606\n",
      "cnt: 0 - valLoss: 0.4643513858318329 - trainLoss: 0.47128671407699585\n",
      "cnt: 0 - valLoss: 0.46434345841407776 - trainLoss: 0.471279501914978\n",
      "cnt: 0 - valLoss: 0.46433553099632263 - trainLoss: 0.4712722599506378\n",
      "cnt: 0 - valLoss: 0.4643276035785675 - trainLoss: 0.4712650179862976\n",
      "cnt: 0 - valLoss: 0.4643196761608124 - trainLoss: 0.4712578356266022\n",
      "cnt: 0 - valLoss: 0.46431174874305725 - trainLoss: 0.4712505638599396\n",
      "cnt: 0 - valLoss: 0.4643038511276245 - trainLoss: 0.47124338150024414\n",
      "cnt: 0 - valLoss: 0.464295893907547 - trainLoss: 0.47123607993125916\n",
      "cnt: 0 - valLoss: 0.46428796648979187 - trainLoss: 0.47122880816459656\n",
      "cnt: 0 - valLoss: 0.46428003907203674 - trainLoss: 0.47122159600257874\n",
      "cnt: 0 - valLoss: 0.46427205204963684 - trainLoss: 0.4712143540382385\n",
      "cnt: 0 - valLoss: 0.4642641842365265 - trainLoss: 0.4712070822715759\n",
      "cnt: 0 - valLoss: 0.46425625681877136 - trainLoss: 0.4711998403072357\n",
      "cnt: 0 - valLoss: 0.46424829959869385 - trainLoss: 0.4711925983428955\n",
      "cnt: 0 - valLoss: 0.4642403721809387 - trainLoss: 0.4711853265762329\n",
      "cnt: 0 - valLoss: 0.4642324447631836 - trainLoss: 0.4711780846118927\n",
      "cnt: 0 - valLoss: 0.46422451734542847 - trainLoss: 0.4711708724498749\n",
      "cnt: 0 - valLoss: 0.4642166495323181 - trainLoss: 0.47116363048553467\n",
      "cnt: 0 - valLoss: 0.464208722114563 - trainLoss: 0.47115638852119446\n",
      "cnt: 0 - valLoss: 0.46420079469680786 - trainLoss: 0.47114914655685425\n",
      "cnt: 0 - valLoss: 0.4641928970813751 - trainLoss: 0.4711419343948364\n",
      "cnt: 0 - valLoss: 0.4641849994659424 - trainLoss: 0.4711347222328186\n",
      "cnt: 0 - valLoss: 0.4641771614551544 - trainLoss: 0.471127450466156\n",
      "cnt: 0 - valLoss: 0.4641692042350769 - trainLoss: 0.47112026810646057\n",
      "cnt: 0 - valLoss: 0.46416130661964417 - trainLoss: 0.47111308574676514\n",
      "cnt: 0 - valLoss: 0.4641534090042114 - trainLoss: 0.4711058437824249\n",
      "cnt: 0 - valLoss: 0.4641455411911011 - trainLoss: 0.4710986018180847\n",
      "cnt: 0 - valLoss: 0.46413764357566833 - trainLoss: 0.4710914194583893\n",
      "cnt: 0 - valLoss: 0.4641297459602356 - trainLoss: 0.47108423709869385\n",
      "cnt: 0 - valLoss: 0.46412190794944763 - trainLoss: 0.47107699513435364\n",
      "cnt: 0 - valLoss: 0.46411406993865967 - trainLoss: 0.4710697531700134\n",
      "cnt: 0 - valLoss: 0.46410614252090454 - trainLoss: 0.471062570810318\n",
      "cnt: 0 - valLoss: 0.4640983045101166 - trainLoss: 0.47105538845062256\n",
      "cnt: 0 - valLoss: 0.4640904664993286 - trainLoss: 0.4710482060909271\n",
      "cnt: 0 - valLoss: 0.46408259868621826 - trainLoss: 0.4710409641265869\n",
      "cnt: 0 - valLoss: 0.4640747308731079 - trainLoss: 0.4710337817668915\n",
      "cnt: 0 - valLoss: 0.46406683325767517 - trainLoss: 0.47102656960487366\n",
      "cnt: 0 - valLoss: 0.4640590250492096 - trainLoss: 0.4710194170475006\n",
      "cnt: 0 - valLoss: 0.46405115723609924 - trainLoss: 0.4710122346878052\n",
      "cnt: 0 - valLoss: 0.4640432596206665 - trainLoss: 0.47100505232810974\n",
      "cnt: 0 - valLoss: 0.4640354514122009 - trainLoss: 0.4709978699684143\n",
      "cnt: 0 - valLoss: 0.4640275835990906 - trainLoss: 0.4709906578063965\n",
      "cnt: 0 - valLoss: 0.46401968598365784 - trainLoss: 0.47098347544670105\n",
      "cnt: 0 - valLoss: 0.46401187777519226 - trainLoss: 0.4709762930870056\n",
      "cnt: 0 - valLoss: 0.4640040397644043 - trainLoss: 0.47096917033195496\n",
      "cnt: 0 - valLoss: 0.46399620175361633 - trainLoss: 0.4709619879722595\n",
      "cnt: 0 - valLoss: 0.46398839354515076 - trainLoss: 0.4709548056125641\n",
      "cnt: 0 - valLoss: 0.4639805257320404 - trainLoss: 0.47094765305519104\n",
      "cnt: 0 - valLoss: 0.46397271752357483 - trainLoss: 0.470940500497818\n",
      "cnt: 0 - valLoss: 0.46396490931510925 - trainLoss: 0.47093331813812256\n",
      "cnt: 0 - valLoss: 0.4639571011066437 - trainLoss: 0.4709261357784271\n",
      "cnt: 0 - valLoss: 0.4639492630958557 - trainLoss: 0.47091901302337646\n",
      "cnt: 0 - valLoss: 0.46394145488739014 - trainLoss: 0.47091183066368103\n",
      "cnt: 0 - valLoss: 0.4639336168766022 - trainLoss: 0.47090470790863037\n",
      "cnt: 0 - valLoss: 0.4639258086681366 - trainLoss: 0.4708975851535797\n",
      "cnt: 0 - valLoss: 0.4639180302619934 - trainLoss: 0.4708903729915619\n",
      "cnt: 0 - valLoss: 0.46391022205352783 - trainLoss: 0.47088322043418884\n",
      "cnt: 0 - valLoss: 0.46390241384506226 - trainLoss: 0.4708760976791382\n",
      "cnt: 0 - valLoss: 0.4638946056365967 - trainLoss: 0.47086894512176514\n",
      "cnt: 0 - valLoss: 0.4638867676258087 - trainLoss: 0.4708618223667145\n",
      "cnt: 0 - valLoss: 0.4638790190219879 - trainLoss: 0.4708546996116638\n",
      "cnt: 0 - valLoss: 0.46387118101119995 - trainLoss: 0.47084757685661316\n",
      "cnt: 0 - valLoss: 0.4638633728027344 - trainLoss: 0.4708404242992401\n",
      "cnt: 0 - valLoss: 0.4638555645942688 - trainLoss: 0.47083327174186707\n",
      "cnt: 0 - valLoss: 0.4638477563858032 - trainLoss: 0.4708261787891388\n",
      "cnt: 0 - valLoss: 0.46383991837501526 - trainLoss: 0.47081902623176575\n",
      "cnt: 0 - valLoss: 0.46383213996887207 - trainLoss: 0.4708119332790375\n",
      "cnt: 0 - valLoss: 0.4638243615627289 - trainLoss: 0.4708048105239868\n",
      "cnt: 0 - valLoss: 0.4638165533542633 - trainLoss: 0.47079771757125854\n",
      "cnt: 0 - valLoss: 0.4638087749481201 - trainLoss: 0.4707906246185303\n",
      "cnt: 0 - valLoss: 0.46380099654197693 - trainLoss: 0.4707834720611572\n",
      "cnt: 0 - valLoss: 0.46379315853118896 - trainLoss: 0.47077640891075134\n",
      "cnt: 0 - valLoss: 0.46378540992736816 - trainLoss: 0.4707692563533783\n",
      "cnt: 0 - valLoss: 0.4637776017189026 - trainLoss: 0.47076213359832764\n",
      "cnt: 0 - valLoss: 0.4637698531150818 - trainLoss: 0.47075504064559937\n",
      "cnt: 0 - valLoss: 0.463762104511261 - trainLoss: 0.4707479476928711\n",
      "cnt: 0 - valLoss: 0.4637542963027954 - trainLoss: 0.47074082493782043\n",
      "cnt: 0 - valLoss: 0.4637465178966522 - trainLoss: 0.47073376178741455\n",
      "cnt: 0 - valLoss: 0.4637387692928314 - trainLoss: 0.4707266688346863\n",
      "cnt: 0 - valLoss: 0.4637310206890106 - trainLoss: 0.4707195460796356\n",
      "cnt: 0 - valLoss: 0.4637232720851898 - trainLoss: 0.4707125127315521\n",
      "cnt: 0 - valLoss: 0.46371549367904663 - trainLoss: 0.47070538997650146\n",
      "cnt: 0 - valLoss: 0.46370774507522583 - trainLoss: 0.4706982970237732\n",
      "cnt: 0 - valLoss: 0.4637000262737274 - trainLoss: 0.4706912338733673\n",
      "cnt: 0 - valLoss: 0.463692307472229 - trainLoss: 0.47068411111831665\n",
      "cnt: 0 - valLoss: 0.4636845588684082 - trainLoss: 0.47067704796791077\n",
      "cnt: 0 - valLoss: 0.4636768102645874 - trainLoss: 0.4706699848175049\n",
      "cnt: 0 - valLoss: 0.463669091463089 - trainLoss: 0.4706628918647766\n",
      "cnt: 0 - valLoss: 0.4636613726615906 - trainLoss: 0.4706558585166931\n",
      "cnt: 0 - valLoss: 0.46365365386009216 - trainLoss: 0.47064876556396484\n",
      "cnt: 0 - valLoss: 0.46364590525627136 - trainLoss: 0.47064170241355896\n",
      "cnt: 0 - valLoss: 0.46363818645477295 - trainLoss: 0.4706346392631531\n",
      "cnt: 0 - valLoss: 0.46363046765327454 - trainLoss: 0.4706275761127472\n",
      "cnt: 0 - valLoss: 0.4636227488517761 - trainLoss: 0.4706205129623413\n",
      "cnt: 0 - valLoss: 0.4636150300502777 - trainLoss: 0.4706134498119354\n",
      "cnt: 0 - valLoss: 0.4636073708534241 - trainLoss: 0.47060638666152954\n",
      "cnt: 0 - valLoss: 0.4635995924472809 - trainLoss: 0.47059932351112366\n",
      "cnt: 0 - valLoss: 0.46359193325042725 - trainLoss: 0.4705922603607178\n",
      "cnt: 0 - valLoss: 0.46358421444892883 - trainLoss: 0.47058525681495667\n",
      "cnt: 0 - valLoss: 0.4635765552520752 - trainLoss: 0.47057822346687317\n",
      "cnt: 0 - valLoss: 0.4635688364505768 - trainLoss: 0.4705711305141449\n",
      "cnt: 0 - valLoss: 0.46356114745140076 - trainLoss: 0.4705640971660614\n",
      "cnt: 0 - valLoss: 0.4635534882545471 - trainLoss: 0.4705570340156555\n",
      "cnt: 0 - valLoss: 0.4635457992553711 - trainLoss: 0.4705500602722168\n",
      "cnt: 0 - valLoss: 0.46353816986083984 - trainLoss: 0.4705429971218109\n",
      "cnt: 0 - valLoss: 0.46353045105934143 - trainLoss: 0.47053593397140503\n",
      "cnt: 0 - valLoss: 0.4635228216648102 - trainLoss: 0.4705289304256439\n",
      "cnt: 0 - valLoss: 0.46351513266563416 - trainLoss: 0.4705219566822052\n",
      "cnt: 0 - valLoss: 0.4635075032711029 - trainLoss: 0.4705148935317993\n",
      "cnt: 0 - valLoss: 0.46349987387657166 - trainLoss: 0.4705078601837158\n",
      "cnt: 0 - valLoss: 0.46349218487739563 - trainLoss: 0.4705008566379547\n",
      "cnt: 0 - valLoss: 0.463484525680542 - trainLoss: 0.4704938232898712\n",
      "cnt: 0 - valLoss: 0.46347689628601074 - trainLoss: 0.4704868495464325\n",
      "cnt: 0 - valLoss: 0.4634692668914795 - trainLoss: 0.4704797863960266\n",
      "cnt: 0 - valLoss: 0.46346163749694824 - trainLoss: 0.4704727530479431\n",
      "cnt: 0 - valLoss: 0.463454008102417 - trainLoss: 0.47046583890914917\n",
      "cnt: 0 - valLoss: 0.46344637870788574 - trainLoss: 0.4704588055610657\n",
      "cnt: 0 - valLoss: 0.4634386897087097 - trainLoss: 0.47045180201530457\n",
      "cnt: 0 - valLoss: 0.46343111991882324 - trainLoss: 0.47044476866722107\n",
      "cnt: 0 - valLoss: 0.463423490524292 - trainLoss: 0.47043776512145996\n",
      "cnt: 0 - valLoss: 0.46341586112976074 - trainLoss: 0.47043079137802124\n",
      "cnt: 0 - valLoss: 0.4634082615375519 - trainLoss: 0.47042375802993774\n",
      "cnt: 0 - valLoss: 0.46340063214302063 - trainLoss: 0.470416784286499\n",
      "cnt: 0 - valLoss: 0.46339306235313416 - trainLoss: 0.4704098403453827\n",
      "cnt: 0 - valLoss: 0.4633854329586029 - trainLoss: 0.4704028367996216\n",
      "cnt: 0 - valLoss: 0.46337783336639404 - trainLoss: 0.4703958034515381\n",
      "cnt: 0 - valLoss: 0.4633702337741852 - trainLoss: 0.47038888931274414\n",
      "cnt: 0 - valLoss: 0.4633626639842987 - trainLoss: 0.47038188576698303\n",
      "cnt: 0 - valLoss: 0.46335506439208984 - trainLoss: 0.4703749120235443\n",
      "cnt: 0 - valLoss: 0.46334749460220337 - trainLoss: 0.4703679084777832\n",
      "cnt: 0 - valLoss: 0.4633398950099945 - trainLoss: 0.47036096453666687\n",
      "cnt: 0 - valLoss: 0.46333226561546326 - trainLoss: 0.47035399079322815\n",
      "cnt: 0 - valLoss: 0.46332472562789917 - trainLoss: 0.47034701704978943\n",
      "cnt: 0 - valLoss: 0.4633171856403351 - trainLoss: 0.4703400731086731\n",
      "cnt: 0 - valLoss: 0.463309645652771 - trainLoss: 0.470333069562912\n",
      "cnt: 0 - valLoss: 0.46330201625823975 - trainLoss: 0.47032612562179565\n",
      "cnt: 0 - valLoss: 0.46329447627067566 - trainLoss: 0.47031915187835693\n",
      "cnt: 0 - valLoss: 0.4632869362831116 - trainLoss: 0.4703121781349182\n",
      "cnt: 0 - valLoss: 0.4632793664932251 - trainLoss: 0.4703052341938019\n",
      "cnt: 0 - valLoss: 0.4632717967033386 - trainLoss: 0.47029826045036316\n",
      "cnt: 0 - valLoss: 0.4632642865180969 - trainLoss: 0.4702913165092468\n",
      "cnt: 0 - valLoss: 0.46325671672821045 - trainLoss: 0.4702844023704529\n",
      "cnt: 0 - valLoss: 0.46324917674064636 - trainLoss: 0.47027745842933655\n",
      "cnt: 0 - valLoss: 0.4632416069507599 - trainLoss: 0.4702704846858978\n",
      "cnt: 0 - valLoss: 0.4632341265678406 - trainLoss: 0.4702635407447815\n",
      "cnt: 0 - valLoss: 0.4632265865802765 - trainLoss: 0.47025662660598755\n",
      "cnt: 0 - valLoss: 0.4632190465927124 - trainLoss: 0.4702496826648712\n",
      "cnt: 0 - valLoss: 0.4632114768028259 - trainLoss: 0.4702427387237549\n",
      "cnt: 0 - valLoss: 0.4632039964199066 - trainLoss: 0.47023576498031616\n",
      "cnt: 0 - valLoss: 0.46319645643234253 - trainLoss: 0.4702288806438446\n",
      "cnt: 0 - valLoss: 0.46318894624710083 - trainLoss: 0.47022193670272827\n",
      "cnt: 0 - valLoss: 0.46318140625953674 - trainLoss: 0.4702150225639343\n",
      "cnt: 0 - valLoss: 0.4631739556789398 - trainLoss: 0.470208078622818\n",
      "cnt: 0 - valLoss: 0.46316638588905334 - trainLoss: 0.47020113468170166\n",
      "cnt: 0 - valLoss: 0.46315890550613403 - trainLoss: 0.4701942205429077\n",
      "cnt: 0 - valLoss: 0.46315139532089233 - trainLoss: 0.4701872766017914\n",
      "cnt: 0 - valLoss: 0.46314388513565063 - trainLoss: 0.47018033266067505\n",
      "cnt: 0 - valLoss: 0.4631364047527313 - trainLoss: 0.4701734483242035\n",
      "cnt: 0 - valLoss: 0.4631288945674896 - trainLoss: 0.47016653418540955\n",
      "cnt: 0 - valLoss: 0.4631213843822479 - trainLoss: 0.4701595902442932\n",
      "cnt: 0 - valLoss: 0.4631139039993286 - trainLoss: 0.47015270590782166\n",
      "cnt: 0 - valLoss: 0.4631063938140869 - trainLoss: 0.4701457619667053\n",
      "cnt: 0 - valLoss: 0.4630988836288452 - trainLoss: 0.470138818025589\n",
      "cnt: 0 - valLoss: 0.4630914032459259 - trainLoss: 0.47013193368911743\n",
      "cnt: 0 - valLoss: 0.4630839228630066 - trainLoss: 0.4701250493526459\n",
      "cnt: 0 - valLoss: 0.4630763828754425 - trainLoss: 0.4701181948184967\n",
      "cnt: 0 - valLoss: 0.4630688726902008 - trainLoss: 0.47011125087738037\n",
      "cnt: 0 - valLoss: 0.4630613923072815 - trainLoss: 0.4701043367385864\n",
      "cnt: 0 - valLoss: 0.46305394172668457 - trainLoss: 0.47009748220443726\n",
      "cnt: 0 - valLoss: 0.46304646134376526 - trainLoss: 0.4700905978679657\n",
      "cnt: 0 - valLoss: 0.46303898096084595 - trainLoss: 0.47008371353149414\n",
      "cnt: 0 - valLoss: 0.4630315601825714 - trainLoss: 0.4700767695903778\n",
      "cnt: 0 - valLoss: 0.4630240499973297 - trainLoss: 0.47006991505622864\n",
      "cnt: 0 - valLoss: 0.463016539812088 - trainLoss: 0.4700630307197571\n",
      "cnt: 0 - valLoss: 0.4630090892314911 - trainLoss: 0.4700561463832855\n",
      "cnt: 0 - valLoss: 0.46300163865089417 - trainLoss: 0.4700492322444916\n",
      "cnt: 0 - valLoss: 0.46299418807029724 - trainLoss: 0.47004234790802\n",
      "cnt: 0 - valLoss: 0.46298670768737793 - trainLoss: 0.47003549337387085\n",
      "cnt: 0 - valLoss: 0.462979257106781 - trainLoss: 0.4700286090373993\n",
      "cnt: 0 - valLoss: 0.4629718065261841 - trainLoss: 0.47002172470092773\n",
      "cnt: 0 - valLoss: 0.46296435594558716 - trainLoss: 0.47001487016677856\n",
      "cnt: 0 - valLoss: 0.462956964969635 - trainLoss: 0.4700080156326294\n",
      "cnt: 0 - valLoss: 0.4629495143890381 - trainLoss: 0.4700011909008026\n",
      "cnt: 0 - valLoss: 0.46294206380844116 - trainLoss: 0.4699942469596863\n",
      "cnt: 0 - valLoss: 0.46293461322784424 - trainLoss: 0.4699873924255371\n",
      "cnt: 0 - valLoss: 0.4629272222518921 - trainLoss: 0.46998053789138794\n",
      "cnt: 0 - valLoss: 0.46291977167129517 - trainLoss: 0.4699736535549164\n",
      "cnt: 0 - valLoss: 0.46291232109069824 - trainLoss: 0.469966858625412\n",
      "cnt: 0 - valLoss: 0.4629049003124237 - trainLoss: 0.4699600040912628\n",
      "cnt: 0 - valLoss: 0.46289747953414917 - trainLoss: 0.4699530601501465\n",
      "cnt: 0 - valLoss: 0.46289002895355225 - trainLoss: 0.4699462354183197\n",
      "cnt: 0 - valLoss: 0.4628826379776001 - trainLoss: 0.4699394106864929\n",
      "cnt: 0 - valLoss: 0.46287524700164795 - trainLoss: 0.46993255615234375\n",
      "cnt: 0 - valLoss: 0.4628678262233734 - trainLoss: 0.46992576122283936\n",
      "cnt: 0 - valLoss: 0.4628604054450989 - trainLoss: 0.4699188768863678\n",
      "cnt: 0 - valLoss: 0.46285298466682434 - trainLoss: 0.469912052154541\n",
      "cnt: 0 - valLoss: 0.4628456234931946 - trainLoss: 0.46990516781806946\n",
      "cnt: 0 - valLoss: 0.46283820271492004 - trainLoss: 0.46989837288856506\n",
      "cnt: 0 - valLoss: 0.4628307819366455 - trainLoss: 0.4698915183544159\n",
      "cnt: 0 - valLoss: 0.46282342076301575 - trainLoss: 0.4698846638202667\n",
      "cnt: 0 - valLoss: 0.4628159999847412 - trainLoss: 0.46987780928611755\n",
      "cnt: 0 - valLoss: 0.4628085792064667 - trainLoss: 0.46987098455429077\n",
      "cnt: 0 - valLoss: 0.4628012180328369 - trainLoss: 0.4698641300201416\n",
      "cnt: 0 - valLoss: 0.46279385685920715 - trainLoss: 0.4698573350906372\n",
      "cnt: 0 - valLoss: 0.462786465883255 - trainLoss: 0.4698505103588104\n",
      "cnt: 0 - valLoss: 0.46277910470962524 - trainLoss: 0.46984371542930603\n",
      "cnt: 0 - valLoss: 0.4627717137336731 - trainLoss: 0.46983686089515686\n",
      "cnt: 0 - valLoss: 0.46276435256004333 - trainLoss: 0.46983006596565247\n",
      "cnt: 0 - valLoss: 0.46275702118873596 - trainLoss: 0.4698232114315033\n",
      "cnt: 0 - valLoss: 0.4627496600151062 - trainLoss: 0.4698164463043213\n",
      "cnt: 0 - valLoss: 0.46274226903915405 - trainLoss: 0.4698095917701721\n",
      "cnt: 0 - valLoss: 0.4627349078655243 - trainLoss: 0.4698027968406677\n",
      "cnt: 0 - valLoss: 0.4627275764942169 - trainLoss: 0.46979597210884094\n",
      "cnt: 0 - valLoss: 0.46272021532058716 - trainLoss: 0.46978917717933655\n",
      "cnt: 0 - valLoss: 0.462712824344635 - trainLoss: 0.46978238224983215\n",
      "cnt: 0 - valLoss: 0.4627055525779724 - trainLoss: 0.469775527715683\n",
      "cnt: 0 - valLoss: 0.46269816160202026 - trainLoss: 0.469768762588501\n",
      "cnt: 0 - valLoss: 0.46269088983535767 - trainLoss: 0.4697619676589966\n",
      "cnt: 0 - valLoss: 0.4626835286617279 - trainLoss: 0.4697552025318146\n",
      "cnt: 0 - valLoss: 0.46267619729042053 - trainLoss: 0.4697483777999878\n",
      "cnt: 0 - valLoss: 0.46266886591911316 - trainLoss: 0.4697415828704834\n",
      "cnt: 0 - valLoss: 0.4626615345478058 - trainLoss: 0.469734787940979\n",
      "cnt: 0 - valLoss: 0.4626542031764984 - trainLoss: 0.469728022813797\n",
      "cnt: 0 - valLoss: 0.46264687180519104 - trainLoss: 0.469721257686615\n",
      "cnt: 0 - valLoss: 0.46263957023620605 - trainLoss: 0.4697144627571106\n",
      "cnt: 0 - valLoss: 0.46263226866722107 - trainLoss: 0.4697076380252838\n",
      "cnt: 0 - valLoss: 0.4626249372959137 - trainLoss: 0.4697008728981018\n",
      "cnt: 0 - valLoss: 0.4626176953315735 - trainLoss: 0.4696941077709198\n",
      "cnt: 0 - valLoss: 0.4626103639602661 - trainLoss: 0.4696873128414154\n",
      "cnt: 0 - valLoss: 0.46260303258895874 - trainLoss: 0.4696806073188782\n",
      "cnt: 0 - valLoss: 0.46259573101997375 - trainLoss: 0.4696737825870514\n",
      "cnt: 0 - valLoss: 0.46258845925331116 - trainLoss: 0.46966707706451416\n",
      "cnt: 0 - valLoss: 0.46258115768432617 - trainLoss: 0.4696602523326874\n",
      "cnt: 0 - valLoss: 0.46257391571998596 - trainLoss: 0.46965354681015015\n",
      "cnt: 0 - valLoss: 0.4625665843486786 - trainLoss: 0.46964675188064575\n",
      "cnt: 0 - valLoss: 0.462559312582016 - trainLoss: 0.46963998675346375\n",
      "cnt: 0 - valLoss: 0.462552011013031 - trainLoss: 0.4696332514286041\n",
      "cnt: 0 - valLoss: 0.4625447392463684 - trainLoss: 0.46962639689445496\n",
      "cnt: 0 - valLoss: 0.4625375270843506 - trainLoss: 0.4696197211742401\n",
      "cnt: 0 - valLoss: 0.462530255317688 - trainLoss: 0.4696129560470581\n",
      "cnt: 0 - valLoss: 0.462522953748703 - trainLoss: 0.4696061909198761\n",
      "cnt: 0 - valLoss: 0.4625156819820404 - trainLoss: 0.4695994555950165\n",
      "cnt: 0 - valLoss: 0.4625084400177002 - trainLoss: 0.46959272027015686\n",
      "cnt: 0 - valLoss: 0.4625011682510376 - trainLoss: 0.46958592534065247\n",
      "cnt: 0 - valLoss: 0.462493896484375 - trainLoss: 0.4695792496204376\n",
      "cnt: 0 - valLoss: 0.4624866843223572 - trainLoss: 0.4695724844932556\n",
      "cnt: 0 - valLoss: 0.46247944235801697 - trainLoss: 0.469565749168396\n",
      "cnt: 0 - valLoss: 0.46247220039367676 - trainLoss: 0.469558984041214\n",
      "cnt: 0 - valLoss: 0.4624650180339813 - trainLoss: 0.46955227851867676\n",
      "cnt: 0 - valLoss: 0.46245771646499634 - trainLoss: 0.46954554319381714\n",
      "cnt: 0 - valLoss: 0.4624505341053009 - trainLoss: 0.4695388376712799\n",
      "cnt: 0 - valLoss: 0.4624432623386383 - trainLoss: 0.4695320725440979\n",
      "cnt: 0 - valLoss: 0.4624360203742981 - trainLoss: 0.4695253372192383\n",
      "cnt: 0 - valLoss: 0.46242883801460266 - trainLoss: 0.46951863169670105\n",
      "cnt: 0 - valLoss: 0.46242162585258484 - trainLoss: 0.46951186656951904\n",
      "cnt: 0 - valLoss: 0.4624144434928894 - trainLoss: 0.4695051908493042\n",
      "cnt: 0 - valLoss: 0.46240749955177307 - trainLoss: 0.46949851512908936\n",
      "cnt: 0 - valLoss: 0.46240028738975525 - trainLoss: 0.4694918394088745\n",
      "cnt: 0 - valLoss: 0.4623933732509613 - trainLoss: 0.4694851040840149\n",
      "cnt: 0 - valLoss: 0.46238619089126587 - trainLoss: 0.4694784879684448\n",
      "cnt: 0 - valLoss: 0.4623792767524719 - trainLoss: 0.4694717526435852\n",
      "cnt: 0 - valLoss: 0.4623720943927765 - trainLoss: 0.46946513652801514\n",
      "cnt: 0 - valLoss: 0.46236518025398254 - trainLoss: 0.4694584012031555\n",
      "cnt: 0 - valLoss: 0.4623579680919647 - trainLoss: 0.46945178508758545\n",
      "cnt: 0 - valLoss: 0.46235090494155884 - trainLoss: 0.4694451093673706\n",
      "cnt: 0 - valLoss: 0.4623439610004425 - trainLoss: 0.46943849325180054\n",
      "cnt: 0 - valLoss: 0.46233683824539185 - trainLoss: 0.4694318175315857\n",
      "cnt: 0 - valLoss: 0.4623298943042755 - trainLoss: 0.4694252014160156\n",
      "cnt: 0 - valLoss: 0.46232283115386963 - trainLoss: 0.469418466091156\n",
      "cnt: 0 - valLoss: 0.4623158872127533 - trainLoss: 0.4694119393825531\n",
      "cnt: 0 - valLoss: 0.4623088240623474 - trainLoss: 0.46940526366233826\n",
      "cnt: 0 - valLoss: 0.4623018801212311 - trainLoss: 0.4693986177444458\n",
      "cnt: 0 - valLoss: 0.4622948169708252 - trainLoss: 0.46939197182655334\n",
      "cnt: 0 - valLoss: 0.46228790283203125 - trainLoss: 0.4693853557109833\n",
      "cnt: 0 - valLoss: 0.4622807800769806 - trainLoss: 0.4693787097930908\n",
      "cnt: 0 - valLoss: 0.4622739255428314 - trainLoss: 0.46937209367752075\n",
      "cnt: 0 - valLoss: 0.46226680278778076 - trainLoss: 0.4693654775619507\n",
      "cnt: 0 - valLoss: 0.4622599184513092 - trainLoss: 0.4693588614463806\n",
      "cnt: 0 - valLoss: 0.4622528553009033 - trainLoss: 0.46935221552848816\n",
      "cnt: 0 - valLoss: 0.4622459411621094 - trainLoss: 0.4693455994129181\n",
      "cnt: 0 - valLoss: 0.4622388184070587 - trainLoss: 0.469338983297348\n",
      "cnt: 0 - valLoss: 0.46223196387290955 - trainLoss: 0.46933236718177795\n",
      "cnt: 0 - valLoss: 0.4622248411178589 - trainLoss: 0.4693257808685303\n",
      "cnt: 0 - valLoss: 0.4622179865837097 - trainLoss: 0.4693191349506378\n",
      "cnt: 0 - valLoss: 0.46221089363098145 - trainLoss: 0.46931248903274536\n",
      "cnt: 0 - valLoss: 0.46220406889915466 - trainLoss: 0.4693059027194977\n",
      "cnt: 0 - valLoss: 0.462196946144104 - trainLoss: 0.46929931640625\n",
      "cnt: 0 - valLoss: 0.4621901214122772 - trainLoss: 0.46929264068603516\n",
      "cnt: 0 - valLoss: 0.46218302845954895 - trainLoss: 0.46928608417510986\n",
      "cnt: 0 - valLoss: 0.4621759355068207 - trainLoss: 0.4692794680595398\n",
      "cnt: 0 - valLoss: 0.4621690809726715 - trainLoss: 0.4692728519439697\n",
      "cnt: 0 - valLoss: 0.462162047624588 - trainLoss: 0.46926626563072205\n",
      "cnt: 0 - valLoss: 0.46215519309043884 - trainLoss: 0.46925967931747437\n",
      "cnt: 0 - valLoss: 0.46214812994003296 - trainLoss: 0.4692530930042267\n",
      "cnt: 0 - valLoss: 0.4621412754058838 - trainLoss: 0.4692465364933014\n",
      "cnt: 0 - valLoss: 0.4621342420578003 - trainLoss: 0.4692399203777313\n",
      "cnt: 0 - valLoss: 0.4621273875236511 - trainLoss: 0.46923330426216125\n",
      "cnt: 0 - valLoss: 0.46212032437324524 - trainLoss: 0.4692267179489136\n",
      "cnt: 0 - valLoss: 0.46211346983909607 - trainLoss: 0.4692201316356659\n",
      "cnt: 0 - valLoss: 0.4621064364910126 - trainLoss: 0.4692135453224182\n",
      "cnt: 0 - valLoss: 0.4620995819568634 - trainLoss: 0.46920695900917053\n",
      "cnt: 0 - valLoss: 0.4620925486087799 - trainLoss: 0.46920037269592285\n",
      "cnt: 0 - valLoss: 0.4620857238769531 - trainLoss: 0.4691937565803528\n",
      "cnt: 0 - valLoss: 0.46207866072654724 - trainLoss: 0.4691872298717499\n",
      "cnt: 0 - valLoss: 0.46207186579704285 - trainLoss: 0.4691806137561798\n",
      "cnt: 0 - valLoss: 0.46206480264663696 - trainLoss: 0.4691740870475769\n",
      "cnt: 0 - valLoss: 0.4620579779148102 - trainLoss: 0.4691675007343292\n",
      "cnt: 0 - valLoss: 0.4620509743690491 - trainLoss: 0.46916088461875916\n",
      "cnt: 0 - valLoss: 0.4620441794395447 - trainLoss: 0.46915435791015625\n",
      "cnt: 0 - valLoss: 0.4620371162891388 - trainLoss: 0.46914780139923096\n",
      "cnt: 0 - valLoss: 0.462030291557312 - trainLoss: 0.4691412150859833\n",
      "cnt: 0 - valLoss: 0.4620232880115509 - trainLoss: 0.469134658575058\n",
      "cnt: 0 - valLoss: 0.4620164930820465 - trainLoss: 0.4691280722618103\n",
      "cnt: 0 - valLoss: 0.46200940012931824 - trainLoss: 0.4691215753555298\n",
      "cnt: 0 - valLoss: 0.4620026648044586 - trainLoss: 0.46911492943763733\n",
      "cnt: 0 - valLoss: 0.4619956314563751 - trainLoss: 0.4691084325313568\n",
      "cnt: 0 - valLoss: 0.461988627910614 - trainLoss: 0.46910181641578674\n",
      "cnt: 0 - valLoss: 0.46198177337646484 - trainLoss: 0.4690953195095062\n",
      "cnt: 0 - valLoss: 0.4619748294353485 - trainLoss: 0.46908873319625854\n",
      "cnt: 0 - valLoss: 0.4619680643081665 - trainLoss: 0.469082236289978\n",
      "cnt: 0 - valLoss: 0.461961030960083 - trainLoss: 0.46907562017440796\n",
      "cnt: 0 - valLoss: 0.461954265832901 - trainLoss: 0.46906912326812744\n",
      "cnt: 0 - valLoss: 0.4619472324848175 - trainLoss: 0.46906253695487976\n",
      "cnt: 0 - valLoss: 0.4619404673576355 - trainLoss: 0.46905604004859924\n",
      "cnt: 0 - valLoss: 0.4619334936141968 - trainLoss: 0.46904945373535156\n",
      "cnt: 0 - valLoss: 0.4619266986846924 - trainLoss: 0.46904292702674866\n",
      "cnt: 0 - valLoss: 0.46191972494125366 - trainLoss: 0.46903640031814575\n",
      "cnt: 0 - valLoss: 0.46191295981407166 - trainLoss: 0.46902984380722046\n",
      "cnt: 0 - valLoss: 0.46190592646598816 - trainLoss: 0.46902331709861755\n",
      "cnt: 0 - valLoss: 0.46189919114112854 - trainLoss: 0.46901682019233704\n",
      "cnt: 0 - valLoss: 0.4618922173976898 - trainLoss: 0.46901023387908936\n",
      "cnt: 0 - valLoss: 0.4618854224681854 - trainLoss: 0.46900373697280884\n",
      "cnt: 0 - valLoss: 0.4618784785270691 - trainLoss: 0.46899718046188354\n",
      "cnt: 0 - valLoss: 0.4618717133998871 - trainLoss: 0.46899062395095825\n",
      "cnt: 0 - valLoss: 0.46186473965644836 - trainLoss: 0.4689841568470001\n",
      "cnt: 0 - valLoss: 0.46185800433158875 - trainLoss: 0.4689776301383972\n",
      "cnt: 0 - valLoss: 0.46185097098350525 - trainLoss: 0.4689711034297943\n",
      "cnt: 0 - valLoss: 0.4618442952632904 - trainLoss: 0.4689645767211914\n",
      "cnt: 0 - valLoss: 0.4618372917175293 - trainLoss: 0.4689581096172333\n",
      "cnt: 0 - valLoss: 0.4618305563926697 - trainLoss: 0.46895158290863037\n",
      "cnt: 0 - valLoss: 0.46182355284690857 - trainLoss: 0.46894508600234985\n",
      "cnt: 0 - valLoss: 0.4618167579174042 - trainLoss: 0.46893852949142456\n",
      "cnt: 0 - valLoss: 0.4618097245693207 - trainLoss: 0.4689320921897888\n",
      "cnt: 0 - valLoss: 0.46180301904678345 - trainLoss: 0.4689255654811859\n",
      "cnt: 0 - valLoss: 0.4617959260940552 - trainLoss: 0.4689190685749054\n",
      "cnt: 0 - valLoss: 0.46178925037384033 - trainLoss: 0.4689125716686249\n",
      "cnt: 0 - valLoss: 0.46178221702575684 - trainLoss: 0.46890613436698914\n",
      "cnt: 0 - valLoss: 0.4617752432823181 - trainLoss: 0.46889954805374146\n",
      "cnt: 0 - valLoss: 0.46176841855049133 - trainLoss: 0.4688931405544281\n",
      "cnt: 0 - valLoss: 0.4617615044116974 - trainLoss: 0.4688866138458252\n",
      "cnt: 0 - valLoss: 0.461754709482193 - trainLoss: 0.46888020634651184\n",
      "cnt: 0 - valLoss: 0.46174776554107666 - trainLoss: 0.46887364983558655\n",
      "cnt: 0 - valLoss: 0.46174100041389465 - trainLoss: 0.4688671827316284\n",
      "cnt: 0 - valLoss: 0.4617340564727783 - trainLoss: 0.4688607156276703\n",
      "cnt: 0 - valLoss: 0.46172723174095154 - trainLoss: 0.46885427832603455\n",
      "cnt: 0 - valLoss: 0.4617202877998352 - trainLoss: 0.46884775161743164\n",
      "cnt: 0 - valLoss: 0.4617134630680084 - trainLoss: 0.4688412845134735\n",
      "cnt: 0 - valLoss: 0.46170657873153687 - trainLoss: 0.46883484721183777\n",
      "cnt: 0 - valLoss: 0.4616997539997101 - trainLoss: 0.46882832050323486\n",
      "cnt: 0 - valLoss: 0.46169281005859375 - trainLoss: 0.46882182359695435\n",
      "cnt: 0 - valLoss: 0.46168607473373413 - trainLoss: 0.4688153862953186\n",
      "cnt: 0 - valLoss: 0.4616791307926178 - trainLoss: 0.46880894899368286\n",
      "cnt: 0 - valLoss: 0.4616723954677582 - trainLoss: 0.4688025116920471\n",
      "cnt: 0 - valLoss: 0.46166545152664185 - trainLoss: 0.4687960147857666\n",
      "cnt: 0 - valLoss: 0.46165868639945984 - trainLoss: 0.46878954768180847\n",
      "cnt: 0 - valLoss: 0.4616518020629883 - trainLoss: 0.4687831401824951\n",
      "cnt: 0 - valLoss: 0.46164506673812866 - trainLoss: 0.468776673078537\n",
      "cnt: 0 - valLoss: 0.46163812279701233 - trainLoss: 0.46877020597457886\n",
      "cnt: 0 - valLoss: 0.4616313874721527 - trainLoss: 0.4687637388706207\n",
      "cnt: 0 - valLoss: 0.4616244435310364 - trainLoss: 0.4687573313713074\n",
      "cnt: 0 - valLoss: 0.46161770820617676 - trainLoss: 0.46875086426734924\n",
      "cnt: 0 - valLoss: 0.4616107642650604 - trainLoss: 0.4687443971633911\n",
      "cnt: 0 - valLoss: 0.4616040885448456 - trainLoss: 0.468737930059433\n",
      "cnt: 0 - valLoss: 0.46159714460372925 - trainLoss: 0.46873146295547485\n",
      "cnt: 0 - valLoss: 0.4615904688835144 - trainLoss: 0.4687250852584839\n",
      "cnt: 0 - valLoss: 0.46158352494239807 - trainLoss: 0.46871861815452576\n",
      "cnt: 0 - valLoss: 0.46157681941986084 - trainLoss: 0.4687121510505676\n",
      "cnt: 0 - valLoss: 0.4615698754787445 - trainLoss: 0.4687057435512543\n",
      "cnt: 0 - valLoss: 0.46156319975852966 - trainLoss: 0.4686993360519409\n",
      "cnt: 0 - valLoss: 0.46155625581741333 - trainLoss: 0.4686928689479828\n",
      "cnt: 0 - valLoss: 0.4615495800971985 - trainLoss: 0.4686864912509918\n",
      "cnt: 0 - valLoss: 0.46154266595840454 - trainLoss: 0.4686800241470337\n",
      "cnt: 0 - valLoss: 0.4615359902381897 - trainLoss: 0.46867355704307556\n",
      "cnt: 0 - valLoss: 0.46152907609939575 - trainLoss: 0.4686671495437622\n",
      "cnt: 0 - valLoss: 0.4615224003791809 - trainLoss: 0.46866071224212646\n",
      "cnt: 0 - valLoss: 0.4615154564380646 - trainLoss: 0.4686543047428131\n",
      "cnt: 0 - valLoss: 0.4615088105201721 - trainLoss: 0.46864786744117737\n",
      "cnt: 0 - valLoss: 0.4615018963813782 - trainLoss: 0.468641459941864\n",
      "cnt: 0 - valLoss: 0.46149513125419617 - trainLoss: 0.46863502264022827\n",
      "cnt: 0 - valLoss: 0.461488276720047 - trainLoss: 0.4686286151409149\n",
      "cnt: 0 - valLoss: 0.46148160099983215 - trainLoss: 0.4686221480369568\n",
      "cnt: 0 - valLoss: 0.4614746570587158 - trainLoss: 0.46861574053764343\n",
      "cnt: 0 - valLoss: 0.46146801114082336 - trainLoss: 0.4686092734336853\n",
      "cnt: 0 - valLoss: 0.46146106719970703 - trainLoss: 0.46860289573669434\n",
      "cnt: 0 - valLoss: 0.4614544212818146 - trainLoss: 0.4685964286327362\n",
      "cnt: 0 - valLoss: 0.46144747734069824 - trainLoss: 0.46859002113342285\n",
      "cnt: 0 - valLoss: 0.4614408314228058 - trainLoss: 0.4685835838317871\n",
      "cnt: 0 - valLoss: 0.46143391728401184 - trainLoss: 0.46857717633247375\n",
      "cnt: 0 - valLoss: 0.461427241563797 - trainLoss: 0.468570739030838\n",
      "cnt: 0 - valLoss: 0.46142032742500305 - trainLoss: 0.46856433153152466\n",
      "cnt: 0 - valLoss: 0.4614136815071106 - trainLoss: 0.4685578942298889\n",
      "cnt: 0 - valLoss: 0.46140679717063904 - trainLoss: 0.46855148673057556\n",
      "cnt: 0 - valLoss: 0.4614001512527466 - trainLoss: 0.46854501962661743\n",
      "cnt: 0 - valLoss: 0.46139323711395264 - trainLoss: 0.46853864192962646\n",
      "cnt: 0 - valLoss: 0.4613865911960602 - trainLoss: 0.4685322344303131\n",
      "cnt: 0 - valLoss: 0.4613797068595886 - trainLoss: 0.46852579712867737\n",
      "cnt: 0 - valLoss: 0.46137306094169617 - trainLoss: 0.468519389629364\n",
      "cnt: 0 - valLoss: 0.461366206407547 - trainLoss: 0.46851301193237305\n",
      "cnt: 0 - valLoss: 0.46135956048965454 - trainLoss: 0.4685065746307373\n",
      "cnt: 0 - valLoss: 0.4613526463508606 - trainLoss: 0.46850019693374634\n",
      "cnt: 0 - valLoss: 0.4613460600376129 - trainLoss: 0.468493789434433\n",
      "cnt: 0 - valLoss: 0.46133914589881897 - trainLoss: 0.46848738193511963\n",
      "cnt: 0 - valLoss: 0.4613325595855713 - trainLoss: 0.4684809446334839\n",
      "cnt: 0 - valLoss: 0.46132564544677734 - trainLoss: 0.4684746265411377\n",
      "cnt: 0 - valLoss: 0.46131905913352966 - trainLoss: 0.46846818923950195\n",
      "cnt: 0 - valLoss: 0.4613121449947357 - trainLoss: 0.4684617817401886\n",
      "cnt: 0 - valLoss: 0.4613055884838104 - trainLoss: 0.46845534443855286\n",
      "cnt: 0 - valLoss: 0.46129873394966125 - trainLoss: 0.46844902634620667\n",
      "cnt: 0 - valLoss: 0.4612920880317688 - trainLoss: 0.4684425890445709\n",
      "cnt: 0 - valLoss: 0.46128523349761963 - trainLoss: 0.46843627095222473\n",
      "cnt: 0 - valLoss: 0.46127864718437195 - trainLoss: 0.468429833650589\n",
      "cnt: 0 - valLoss: 0.46127182245254517 - trainLoss: 0.4684235155582428\n",
      "cnt: 0 - valLoss: 0.4612652063369751 - trainLoss: 0.46841707825660706\n",
      "cnt: 0 - valLoss: 0.46125832200050354 - trainLoss: 0.46841076016426086\n",
      "cnt: 0 - valLoss: 0.46125176548957825 - trainLoss: 0.46840429306030273\n",
      "cnt: 0 - valLoss: 0.4612449109554291 - trainLoss: 0.46839800477027893\n",
      "cnt: 0 - valLoss: 0.4612382650375366 - trainLoss: 0.4683915674686432\n",
      "cnt: 0 - valLoss: 0.4612313508987427 - trainLoss: 0.468385249376297\n",
      "cnt: 0 - valLoss: 0.461224764585495 - trainLoss: 0.46837881207466125\n",
      "cnt: 0 - valLoss: 0.46121782064437866 - trainLoss: 0.46837252378463745\n",
      "cnt: 0 - valLoss: 0.4612112045288086 - trainLoss: 0.4683661162853241\n",
      "cnt: 0 - valLoss: 0.4612043499946594 - trainLoss: 0.4683597683906555\n",
      "cnt: 0 - valLoss: 0.46119776368141174 - trainLoss: 0.46835339069366455\n",
      "cnt: 0 - valLoss: 0.4611908197402954 - trainLoss: 0.46834707260131836\n",
      "cnt: 0 - valLoss: 0.4611842632293701 - trainLoss: 0.4683407247066498\n",
      "cnt: 0 - valLoss: 0.46117734909057617 - trainLoss: 0.4683344066143036\n",
      "cnt: 0 - valLoss: 0.4611707627773285 - trainLoss: 0.46832796931266785\n",
      "cnt: 0 - valLoss: 0.4611639082431793 - trainLoss: 0.46832168102264404\n",
      "cnt: 0 - valLoss: 0.46115729212760925 - trainLoss: 0.46831536293029785\n",
      "cnt: 0 - valLoss: 0.4611504375934601 - trainLoss: 0.4683090150356293\n",
      "cnt: 0 - valLoss: 0.4611438512802124 - trainLoss: 0.4683026373386383\n",
      "cnt: 0 - valLoss: 0.46113696694374084 - trainLoss: 0.4682962894439697\n",
      "cnt: 0 - valLoss: 0.46113038063049316 - trainLoss: 0.46828997135162354\n",
      "cnt: 0 - valLoss: 0.461123526096344 - trainLoss: 0.46828362345695496\n",
      "cnt: 0 - valLoss: 0.4611169099807739 - trainLoss: 0.46827730536460876\n",
      "cnt: 0 - valLoss: 0.46111011505126953 - trainLoss: 0.4682709574699402\n",
      "cnt: 0 - valLoss: 0.46110349893569946 - trainLoss: 0.468264639377594\n",
      "cnt: 0 - valLoss: 0.4610966742038727 - trainLoss: 0.4682582914829254\n",
      "cnt: 0 - valLoss: 0.4610901176929474 - trainLoss: 0.4682519733905792\n",
      "cnt: 0 - valLoss: 0.4610832631587982 - trainLoss: 0.46824562549591064\n",
      "cnt: 0 - valLoss: 0.4610767066478729 - trainLoss: 0.46823933720588684\n",
      "cnt: 0 - valLoss: 0.46106985211372375 - trainLoss: 0.46823298931121826\n",
      "cnt: 0 - valLoss: 0.46106329560279846 - trainLoss: 0.4682266116142273\n",
      "cnt: 0 - valLoss: 0.4610564708709717 - trainLoss: 0.46822038292884827\n",
      "cnt: 0 - valLoss: 0.461049884557724 - trainLoss: 0.4682140350341797\n",
      "cnt: 0 - valLoss: 0.461043119430542 - trainLoss: 0.4682077467441559\n",
      "cnt: 0 - valLoss: 0.4610365629196167 - trainLoss: 0.4682013988494873\n",
      "cnt: 0 - valLoss: 0.4610297381877899 - trainLoss: 0.4681950807571411\n",
      "cnt: 0 - valLoss: 0.46102315187454224 - trainLoss: 0.4681887924671173\n",
      "cnt: 0 - valLoss: 0.46101635694503784 - trainLoss: 0.46818244457244873\n",
      "cnt: 0 - valLoss: 0.46100977063179016 - trainLoss: 0.4681761562824249\n",
      "cnt: 0 - valLoss: 0.46100300550460815 - trainLoss: 0.4681698679924011\n",
      "cnt: 0 - valLoss: 0.46099644899368286 - trainLoss: 0.46816352009773254\n",
      "cnt: 0 - valLoss: 0.46098971366882324 - trainLoss: 0.46815720200538635\n",
      "cnt: 0 - valLoss: 0.4609830975532532 - trainLoss: 0.46815094351768494\n",
      "cnt: 0 - valLoss: 0.46097633242607117 - trainLoss: 0.46814465522766113\n",
      "cnt: 0 - valLoss: 0.46096980571746826 - trainLoss: 0.46813830733299255\n",
      "cnt: 0 - valLoss: 0.46096304059028625 - trainLoss: 0.46813201904296875\n",
      "cnt: 0 - valLoss: 0.4609564542770386 - trainLoss: 0.4681257903575897\n",
      "cnt: 0 - valLoss: 0.46094971895217896 - trainLoss: 0.46811944246292114\n",
      "cnt: 0 - valLoss: 0.4609431028366089 - trainLoss: 0.46811315417289734\n",
      "cnt: 0 - valLoss: 0.46093639731407166 - trainLoss: 0.46810686588287354\n",
      "cnt: 0 - valLoss: 0.46092984080314636 - trainLoss: 0.46810057759284973\n",
      "cnt: 0 - valLoss: 0.46092310547828674 - trainLoss: 0.4680942893028259\n",
      "cnt: 0 - valLoss: 0.46091654896736145 - trainLoss: 0.4680880308151245\n",
      "cnt: 0 - valLoss: 0.46090981364250183 - trainLoss: 0.46808168292045593\n",
      "cnt: 0 - valLoss: 0.46090325713157654 - trainLoss: 0.4680754542350769\n",
      "cnt: 0 - valLoss: 0.4608965218067169 - trainLoss: 0.4680691063404083\n",
      "cnt: 0 - valLoss: 0.460889995098114 - trainLoss: 0.4680628478527069\n",
      "cnt: 0 - valLoss: 0.460883229970932 - trainLoss: 0.4680566191673279\n",
      "cnt: 0 - valLoss: 0.4608767330646515 - trainLoss: 0.4680503308773041\n",
      "cnt: 0 - valLoss: 0.46086999773979187 - trainLoss: 0.4680440425872803\n",
      "cnt: 0 - valLoss: 0.46086347103118896 - trainLoss: 0.46803778409957886\n",
      "cnt: 0 - valLoss: 0.46085673570632935 - trainLoss: 0.46803149580955505\n",
      "cnt: 0 - valLoss: 0.46085023880004883 - trainLoss: 0.46802523732185364\n",
      "cnt: 0 - valLoss: 0.4608435332775116 - trainLoss: 0.46801894903182983\n",
      "cnt: 0 - valLoss: 0.4608369767665863 - trainLoss: 0.4680126905441284\n",
      "cnt: 0 - valLoss: 0.46083030104637146 - trainLoss: 0.4680064618587494\n",
      "cnt: 0 - valLoss: 0.46082374453544617 - trainLoss: 0.4680001437664032\n",
      "cnt: 0 - valLoss: 0.4608170688152313 - trainLoss: 0.46799391508102417\n",
      "cnt: 0 - valLoss: 0.4608105719089508 - trainLoss: 0.46798762679100037\n",
      "cnt: 0 - valLoss: 0.4608038663864136 - trainLoss: 0.46798139810562134\n",
      "cnt: 0 - valLoss: 0.46079733967781067 - trainLoss: 0.46797510981559753\n",
      "cnt: 0 - valLoss: 0.4607906639575958 - trainLoss: 0.4679688811302185\n",
      "cnt: 0 - valLoss: 0.4607841372489929 - trainLoss: 0.4679626226425171\n",
      "cnt: 0 - valLoss: 0.46077749133110046 - trainLoss: 0.4679563641548157\n",
      "cnt: 0 - valLoss: 0.46077099442481995 - trainLoss: 0.46795016527175903\n",
      "cnt: 0 - valLoss: 0.4607643485069275 - trainLoss: 0.46794387698173523\n",
      "cnt: 0 - valLoss: 0.4607578217983246 - trainLoss: 0.4679376184940338\n",
      "cnt: 0 - valLoss: 0.46075114607810974 - trainLoss: 0.4679313898086548\n",
      "cnt: 0 - valLoss: 0.4607446789741516 - trainLoss: 0.46792516112327576\n",
      "cnt: 0 - valLoss: 0.4607379734516144 - trainLoss: 0.46791887283325195\n",
      "cnt: 0 - valLoss: 0.46073150634765625 - trainLoss: 0.4679126739501953\n",
      "cnt: 0 - valLoss: 0.4607248306274414 - trainLoss: 0.4679064154624939\n",
      "cnt: 0 - valLoss: 0.4607183337211609 - trainLoss: 0.46790021657943726\n",
      "cnt: 0 - valLoss: 0.4607117176055908 - trainLoss: 0.46789395809173584\n",
      "cnt: 0 - valLoss: 0.4607051908969879 - trainLoss: 0.4678877294063568\n",
      "cnt: 0 - valLoss: 0.46069857478141785 - trainLoss: 0.4678814709186554\n",
      "cnt: 0 - valLoss: 0.46069207787513733 - trainLoss: 0.467875212430954\n",
      "cnt: 0 - valLoss: 0.4606854319572449 - trainLoss: 0.46786901354789734\n",
      "cnt: 0 - valLoss: 0.46067899465560913 - trainLoss: 0.4678628146648407\n",
      "cnt: 0 - valLoss: 0.4606723487377167 - trainLoss: 0.4678565561771393\n",
      "cnt: 0 - valLoss: 0.46066585183143616 - trainLoss: 0.46785035729408264\n",
      "cnt: 0 - valLoss: 0.4606592357158661 - trainLoss: 0.467844158411026\n",
      "cnt: 0 - valLoss: 0.46065276861190796 - trainLoss: 0.4678378999233246\n",
      "cnt: 0 - valLoss: 0.4606461226940155 - trainLoss: 0.46783164143562317\n",
      "cnt: 0 - valLoss: 0.4606396555900574 - trainLoss: 0.4678254723548889\n",
      "cnt: 0 - valLoss: 0.4606330990791321 - trainLoss: 0.4678192436695099\n",
      "cnt: 0 - valLoss: 0.46062663197517395 - trainLoss: 0.46781307458877563\n",
      "cnt: 0 - valLoss: 0.4606199860572815 - trainLoss: 0.4678068161010742\n",
      "cnt: 0 - valLoss: 0.46061354875564575 - trainLoss: 0.4678006172180176\n",
      "cnt: 0 - valLoss: 0.46060696244239807 - trainLoss: 0.46779435873031616\n",
      "cnt: 0 - valLoss: 0.46060046553611755 - trainLoss: 0.4677881896495819\n",
      "cnt: 0 - valLoss: 0.4605938792228699 - trainLoss: 0.4677819609642029\n",
      "cnt: 0 - valLoss: 0.46058741211891174 - trainLoss: 0.46777576208114624\n",
      "cnt: 0 - valLoss: 0.46058085560798645 - trainLoss: 0.467769593000412\n",
      "cnt: 0 - valLoss: 0.4605743885040283 - trainLoss: 0.46776336431503296\n",
      "cnt: 0 - valLoss: 0.46056777238845825 - trainLoss: 0.46775713562965393\n",
      "cnt: 0 - valLoss: 0.4605613350868225 - trainLoss: 0.46775099635124207\n",
      "cnt: 0 - valLoss: 0.4605547785758972 - trainLoss: 0.46774476766586304\n",
      "cnt: 0 - valLoss: 0.4605483114719391 - trainLoss: 0.4677385985851288\n",
      "cnt: 0 - valLoss: 0.46054181456565857 - trainLoss: 0.46773234009742737\n",
      "cnt: 0 - valLoss: 0.4605353772640228 - trainLoss: 0.4677261710166931\n",
      "cnt: 0 - valLoss: 0.46052902936935425 - trainLoss: 0.46772003173828125\n",
      "cnt: 0 - valLoss: 0.4605223536491394 - trainLoss: 0.4677138030529022\n",
      "cnt: 0 - valLoss: 0.4605160057544708 - trainLoss: 0.4677076041698456\n",
      "cnt: 0 - valLoss: 0.46050938963890076 - trainLoss: 0.46770140528678894\n",
      "cnt: 0 - valLoss: 0.4605030417442322 - trainLoss: 0.4676952362060547\n",
      "cnt: 0 - valLoss: 0.4604964554309845 - trainLoss: 0.46768906712532043\n",
      "cnt: 0 - valLoss: 0.4604901075363159 - trainLoss: 0.4676828682422638\n",
      "cnt: 0 - valLoss: 0.46048352122306824 - trainLoss: 0.46767669916152954\n",
      "cnt: 0 - valLoss: 0.46047717332839966 - trainLoss: 0.4676705002784729\n",
      "cnt: 0 - valLoss: 0.4604705274105072 - trainLoss: 0.4676643908023834\n",
      "cnt: 0 - valLoss: 0.4604641795158386 - trainLoss: 0.467658132314682\n",
      "cnt: 0 - valLoss: 0.46045753359794617 - trainLoss: 0.46765196323394775\n",
      "cnt: 0 - valLoss: 0.4604511857032776 - trainLoss: 0.4676457941532135\n",
      "cnt: 0 - valLoss: 0.4604445993900299 - trainLoss: 0.46763962507247925\n",
      "cnt: 0 - valLoss: 0.46043825149536133 - trainLoss: 0.4676334857940674\n",
      "cnt: 0 - valLoss: 0.46043160557746887 - trainLoss: 0.46762731671333313\n",
      "cnt: 0 - valLoss: 0.4604252576828003 - trainLoss: 0.4676211476325989\n",
      "cnt: 0 - valLoss: 0.460418701171875 - trainLoss: 0.4676149785518646\n",
      "cnt: 0 - valLoss: 0.46041226387023926 - trainLoss: 0.46760880947113037\n",
      "cnt: 0 - valLoss: 0.46040576696395874 - trainLoss: 0.4676026999950409\n",
      "cnt: 0 - valLoss: 0.46039941906929016 - trainLoss: 0.46759650111198425\n",
      "cnt: 0 - valLoss: 0.4603927731513977 - trainLoss: 0.4675903618335724\n",
      "cnt: 0 - valLoss: 0.4603864550590515 - trainLoss: 0.4675842225551605\n",
      "cnt: 0 - valLoss: 0.46037986874580383 - trainLoss: 0.46757808327674866\n",
      "cnt: 0 - valLoss: 0.46037352085113525 - trainLoss: 0.4675719141960144\n",
      "cnt: 0 - valLoss: 0.46036696434020996 - trainLoss: 0.46756577491760254\n",
      "cnt: 0 - valLoss: 0.4603606164455414 - trainLoss: 0.4675596058368683\n",
      "cnt: 0 - valLoss: 0.4603540599346161 - trainLoss: 0.4675535261631012\n",
      "cnt: 0 - valLoss: 0.4603477120399475 - trainLoss: 0.46754735708236694\n",
      "cnt: 0 - valLoss: 0.4603411555290222 - trainLoss: 0.4675411880016327\n",
      "cnt: 0 - valLoss: 0.46033480763435364 - trainLoss: 0.4675350785255432\n",
      "cnt: 0 - valLoss: 0.4603283107280731 - trainLoss: 0.46752890944480896\n",
      "cnt: 0 - valLoss: 0.46032199263572693 - trainLoss: 0.4675227999687195\n",
      "cnt: 0 - valLoss: 0.46031540632247925 - trainLoss: 0.46751663088798523\n",
      "cnt: 0 - valLoss: 0.46030911803245544 - trainLoss: 0.46751055121421814\n",
      "cnt: 0 - valLoss: 0.46030253171920776 - trainLoss: 0.4675043821334839\n",
      "cnt: 0 - valLoss: 0.46029624342918396 - trainLoss: 0.46749821305274963\n",
      "cnt: 0 - valLoss: 0.46028968691825867 - trainLoss: 0.46749213337898254\n",
      "cnt: 0 - valLoss: 0.46028339862823486 - trainLoss: 0.4674859941005707\n",
      "cnt: 0 - valLoss: 0.46027684211730957 - trainLoss: 0.4674798548221588\n",
      "cnt: 0 - valLoss: 0.4602705240249634 - trainLoss: 0.46747374534606934\n",
      "cnt: 0 - valLoss: 0.46026402711868286 - trainLoss: 0.46746760606765747\n",
      "cnt: 0 - valLoss: 0.46025776863098145 - trainLoss: 0.467461496591568\n",
      "cnt: 0 - valLoss: 0.46025124192237854 - trainLoss: 0.46745532751083374\n",
      "cnt: 0 - valLoss: 0.46024495363235474 - trainLoss: 0.46744924783706665\n",
      "cnt: 0 - valLoss: 0.46023836731910706 - trainLoss: 0.4674431085586548\n",
      "cnt: 0 - valLoss: 0.46023207902908325 - trainLoss: 0.4674370288848877\n",
      "cnt: 0 - valLoss: 0.46022555232048035 - trainLoss: 0.4674309492111206\n",
      "cnt: 0 - valLoss: 0.46021923422813416 - trainLoss: 0.46742480993270874\n",
      "cnt: 0 - valLoss: 0.46021270751953125 - trainLoss: 0.46741870045661926\n",
      "cnt: 0 - valLoss: 0.46020635962486267 - trainLoss: 0.4674126207828522\n",
      "cnt: 0 - valLoss: 0.46019986271858215 - trainLoss: 0.4674064517021179\n",
      "cnt: 0 - valLoss: 0.46019354462623596 - trainLoss: 0.4674004316329956\n",
      "cnt: 0 - valLoss: 0.46018707752227783 - trainLoss: 0.46739429235458374\n",
      "cnt: 0 - valLoss: 0.46018072962760925 - trainLoss: 0.46738821268081665\n",
      "cnt: 0 - valLoss: 0.46017420291900635 - trainLoss: 0.4673821032047272\n",
      "cnt: 0 - valLoss: 0.46016788482666016 - trainLoss: 0.4673760235309601\n",
      "cnt: 0 - valLoss: 0.46016138792037964 - trainLoss: 0.4673699140548706\n",
      "cnt: 0 - valLoss: 0.46015509963035583 - trainLoss: 0.4673638343811035\n",
      "cnt: 0 - valLoss: 0.4601488411426544 - trainLoss: 0.46735769510269165\n",
      "cnt: 0 - valLoss: 0.4601423144340515 - trainLoss: 0.46735167503356934\n",
      "cnt: 0 - valLoss: 0.4601360261440277 - trainLoss: 0.46734553575515747\n",
      "cnt: 0 - valLoss: 0.4601295590400696 - trainLoss: 0.46733948588371277\n",
      "cnt: 0 - valLoss: 0.46012330055236816 - trainLoss: 0.4673333466053009\n",
      "cnt: 0 - valLoss: 0.46011677384376526 - trainLoss: 0.4673273265361786\n",
      "cnt: 0 - valLoss: 0.46011051535606384 - trainLoss: 0.4673212468624115\n",
      "cnt: 0 - valLoss: 0.4601040184497833 - trainLoss: 0.46731510758399963\n",
      "cnt: 0 - valLoss: 0.46009770035743713 - trainLoss: 0.4673090875148773\n",
      "cnt: 0 - valLoss: 0.4600912928581238 - trainLoss: 0.46730294823646545\n",
      "cnt: 0 - valLoss: 0.4600849747657776 - trainLoss: 0.46729689836502075\n",
      "cnt: 0 - valLoss: 0.46007850766181946 - trainLoss: 0.46729084849357605\n",
      "cnt: 0 - valLoss: 0.46007224917411804 - trainLoss: 0.46728476881980896\n",
      "cnt: 0 - valLoss: 0.4600657820701599 - trainLoss: 0.46727868914604187\n",
      "cnt: 0 - valLoss: 0.4600595235824585 - trainLoss: 0.46727266907691956\n",
      "cnt: 0 - valLoss: 0.46005308628082275 - trainLoss: 0.46726658940315247\n",
      "cnt: 0 - valLoss: 0.4600467383861542 - trainLoss: 0.4672605097293854\n",
      "cnt: 0 - valLoss: 0.4600403606891632 - trainLoss: 0.4672544300556183\n",
      "cnt: 0 - valLoss: 0.4600340723991394 - trainLoss: 0.4672483801841736\n",
      "cnt: 0 - valLoss: 0.46002787351608276 - trainLoss: 0.4672423005104065\n",
      "cnt: 0 - valLoss: 0.46002137660980225 - trainLoss: 0.4672362804412842\n",
      "cnt: 0 - valLoss: 0.4600151479244232 - trainLoss: 0.4672302007675171\n",
      "cnt: 0 - valLoss: 0.4600086808204651 - trainLoss: 0.46722412109375\n",
      "cnt: 0 - valLoss: 0.46000248193740845 - trainLoss: 0.4672180712223053\n",
      "cnt: 0 - valLoss: 0.45999598503112793 - trainLoss: 0.467212051153183\n",
      "cnt: 0 - valLoss: 0.4599897861480713 - trainLoss: 0.4672059714794159\n",
      "cnt: 0 - valLoss: 0.4599834084510803 - trainLoss: 0.4671999514102936\n",
      "cnt: 0 - valLoss: 0.4599771499633789 - trainLoss: 0.4671939015388489\n",
      "cnt: 0 - valLoss: 0.45997071266174316 - trainLoss: 0.4671878218650818\n",
      "cnt: 0 - valLoss: 0.45996445417404175 - trainLoss: 0.4671818017959595\n",
      "cnt: 0 - valLoss: 0.4599580764770508 - trainLoss: 0.46717575192451477\n",
      "cnt: 0 - valLoss: 0.4599517583847046 - trainLoss: 0.46716973185539246\n",
      "cnt: 0 - valLoss: 0.459945410490036 - trainLoss: 0.46716368198394775\n",
      "cnt: 0 - valLoss: 0.4599391520023346 - trainLoss: 0.46715760231018066\n",
      "cnt: 0 - valLoss: 0.45993277430534363 - trainLoss: 0.46715158224105835\n",
      "cnt: 0 - valLoss: 0.4599265158176422 - trainLoss: 0.46714553236961365\n",
      "cnt: 0 - valLoss: 0.45992013812065125 - trainLoss: 0.46713945269584656\n",
      "cnt: 0 - valLoss: 0.4599139094352722 - trainLoss: 0.46713346242904663\n",
      "cnt: 0 - valLoss: 0.4599077105522156 - trainLoss: 0.4671274423599243\n",
      "cnt: 0 - valLoss: 0.4599013030529022 - trainLoss: 0.4671213924884796\n",
      "cnt: 0 - valLoss: 0.45989513397216797 - trainLoss: 0.4671154022216797\n",
      "cnt: 0 - valLoss: 0.4598887264728546 - trainLoss: 0.4671093821525574\n",
      "cnt: 0 - valLoss: 0.45988255739212036 - trainLoss: 0.46710333228111267\n",
      "cnt: 0 - valLoss: 0.4598761200904846 - trainLoss: 0.46709734201431274\n",
      "cnt: 0 - valLoss: 0.45986995100975037 - trainLoss: 0.46709126234054565\n",
      "cnt: 0 - valLoss: 0.459863543510437 - trainLoss: 0.4670852720737457\n",
      "cnt: 0 - valLoss: 0.45985737442970276 - trainLoss: 0.46707919239997864\n",
      "cnt: 0 - valLoss: 0.4598509967327118 - trainLoss: 0.4670732021331787\n",
      "cnt: 0 - valLoss: 0.45984476804733276 - trainLoss: 0.4670672118663788\n",
      "cnt: 0 - valLoss: 0.4598384499549866 - trainLoss: 0.46706122159957886\n",
      "cnt: 0 - valLoss: 0.45983219146728516 - trainLoss: 0.46705517172813416\n",
      "cnt: 0 - valLoss: 0.4598258435726166 - trainLoss: 0.46704915165901184\n",
      "cnt: 0 - valLoss: 0.45981958508491516 - trainLoss: 0.4670431613922119\n",
      "cnt: 0 - valLoss: 0.4598132073879242 - trainLoss: 0.467037171125412\n",
      "cnt: 0 - valLoss: 0.45980697870254517 - trainLoss: 0.46703118085861206\n",
      "cnt: 0 - valLoss: 0.4598006010055542 - trainLoss: 0.46702510118484497\n",
      "cnt: 0 - valLoss: 0.45979437232017517 - trainLoss: 0.46701914072036743\n",
      "cnt: 0 - valLoss: 0.4597882032394409 - trainLoss: 0.4670131206512451\n",
      "cnt: 0 - valLoss: 0.45978182554244995 - trainLoss: 0.4670071303844452\n",
      "cnt: 0 - valLoss: 0.4597756564617157 - trainLoss: 0.4670011103153229\n",
      "cnt: 0 - valLoss: 0.45976921916007996 - trainLoss: 0.46699512004852295\n",
      "cnt: 0 - valLoss: 0.4597630798816681 - trainLoss: 0.466989129781723\n",
      "cnt: 0 - valLoss: 0.45975667238235474 - trainLoss: 0.4669831395149231\n",
      "cnt: 0 - valLoss: 0.4597504734992981 - trainLoss: 0.4669771194458008\n",
      "cnt: 0 - valLoss: 0.45974406599998474 - trainLoss: 0.46697112917900085\n",
      "cnt: 0 - valLoss: 0.4597379267215729 - trainLoss: 0.4669651389122009\n",
      "cnt: 0 - valLoss: 0.4597315490245819 - trainLoss: 0.4669591784477234\n",
      "cnt: 0 - valLoss: 0.4597253203392029 - trainLoss: 0.46695318818092346\n",
      "cnt: 0 - valLoss: 0.4597189724445343 - trainLoss: 0.46694719791412354\n",
      "cnt: 0 - valLoss: 0.4597128629684448 - trainLoss: 0.4669412076473236\n",
      "cnt: 0 - valLoss: 0.45970651507377625 - trainLoss: 0.4669352173805237\n",
      "cnt: 0 - valLoss: 0.4597002863883972 - trainLoss: 0.46692922711372375\n",
      "cnt: 0 - valLoss: 0.45969393849372864 - trainLoss: 0.46692323684692383\n",
      "cnt: 0 - valLoss: 0.459687739610672 - trainLoss: 0.4669172763824463\n",
      "cnt: 0 - valLoss: 0.4596814811229706 - trainLoss: 0.46691128611564636\n",
      "cnt: 0 - valLoss: 0.45967525243759155 - trainLoss: 0.4669053256511688\n",
      "cnt: 0 - valLoss: 0.4596691429615021 - trainLoss: 0.4668993353843689\n",
      "cnt: 0 - valLoss: 0.4596627354621887 - trainLoss: 0.46689340472221375\n",
      "cnt: 0 - valLoss: 0.45965665578842163 - trainLoss: 0.46688735485076904\n",
      "cnt: 0 - valLoss: 0.45965027809143066 - trainLoss: 0.4668814539909363\n",
      "cnt: 0 - valLoss: 0.4596441090106964 - trainLoss: 0.46687546372413635\n",
      "cnt: 0 - valLoss: 0.45963776111602783 - trainLoss: 0.4668694734573364\n",
      "cnt: 0 - valLoss: 0.45963168144226074 - trainLoss: 0.4668635129928589\n",
      "cnt: 0 - valLoss: 0.45962533354759216 - trainLoss: 0.46685758233070374\n",
      "cnt: 0 - valLoss: 0.4596191644668579 - trainLoss: 0.4668516218662262\n",
      "cnt: 0 - valLoss: 0.4596128463745117 - trainLoss: 0.46684566140174866\n",
      "cnt: 0 - valLoss: 0.45960673689842224 - trainLoss: 0.46683967113494873\n",
      "cnt: 0 - valLoss: 0.45960038900375366 - trainLoss: 0.4668337404727936\n",
      "cnt: 0 - valLoss: 0.4595943093299866 - trainLoss: 0.46682778000831604\n",
      "cnt: 0 - valLoss: 0.459587961435318 - trainLoss: 0.4668217897415161\n",
      "cnt: 0 - valLoss: 0.45958182215690613 - trainLoss: 0.46681588888168335\n",
      "cnt: 0 - valLoss: 0.4595755338668823 - trainLoss: 0.4668098986148834\n",
      "cnt: 0 - valLoss: 0.45956939458847046 - trainLoss: 0.4668039381504059\n",
      "cnt: 0 - valLoss: 0.45956310629844666 - trainLoss: 0.46679797768592834\n",
      "cnt: 0 - valLoss: 0.4595569372177124 - trainLoss: 0.4667920768260956\n",
      "cnt: 0 - valLoss: 0.4595507085323334 - trainLoss: 0.46678608655929565\n",
      "cnt: 0 - valLoss: 0.4595445692539215 - trainLoss: 0.4667801856994629\n",
      "cnt: 0 - valLoss: 0.4595384895801544 - trainLoss: 0.46677419543266296\n",
      "cnt: 0 - valLoss: 0.45953214168548584 - trainLoss: 0.4667682945728302\n",
      "cnt: 0 - valLoss: 0.45952609181404114 - trainLoss: 0.4667623043060303\n",
      "cnt: 0 - valLoss: 0.45951977372169495 - trainLoss: 0.4667564332485199\n",
      "cnt: 0 - valLoss: 0.45951372385025024 - trainLoss: 0.46675047278404236\n",
      "cnt: 0 - valLoss: 0.45950737595558167 - trainLoss: 0.46674448251724243\n",
      "cnt: 0 - valLoss: 0.4595012962818146 - trainLoss: 0.46673858165740967\n",
      "cnt: 0 - valLoss: 0.45949503779411316 - trainLoss: 0.4667326807975769\n",
      "cnt: 0 - valLoss: 0.45948895812034607 - trainLoss: 0.46672672033309937\n",
      "cnt: 0 - valLoss: 0.4594826400279999 - trainLoss: 0.4667208194732666\n",
      "cnt: 0 - valLoss: 0.4594765901565552 - trainLoss: 0.46671485900878906\n",
      "cnt: 0 - valLoss: 0.45947030186653137 - trainLoss: 0.4667089283466339\n",
      "cnt: 0 - valLoss: 0.4594642221927643 - trainLoss: 0.46670299768447876\n",
      "cnt: 0 - valLoss: 0.45945799350738525 - trainLoss: 0.4666970670223236\n",
      "cnt: 0 - valLoss: 0.4594518542289734 - trainLoss: 0.46669116616249084\n",
      "cnt: 0 - valLoss: 0.45944565534591675 - trainLoss: 0.4666852056980133\n",
      "cnt: 0 - valLoss: 0.45943957567214966 - trainLoss: 0.46667933464050293\n",
      "cnt: 0 - valLoss: 0.45943334698677063 - trainLoss: 0.466673344373703\n",
      "cnt: 0 - valLoss: 0.45942723751068115 - trainLoss: 0.4666675329208374\n",
      "cnt: 0 - valLoss: 0.4594212770462036 - trainLoss: 0.4666615426540375\n",
      "cnt: 0 - valLoss: 0.4594150185585022 - trainLoss: 0.4666556715965271\n",
      "cnt: 0 - valLoss: 0.4594089686870575 - trainLoss: 0.46664971113204956\n",
      "cnt: 0 - valLoss: 0.4594027101993561 - trainLoss: 0.4666438698768616\n",
      "cnt: 0 - valLoss: 0.45939671993255615 - trainLoss: 0.46663790941238403\n",
      "cnt: 0 - valLoss: 0.45939046144485474 - trainLoss: 0.46663203835487366\n",
      "cnt: 0 - valLoss: 0.4593844413757324 - trainLoss: 0.4666261076927185\n",
      "cnt: 0 - valLoss: 0.459378182888031 - trainLoss: 0.46662023663520813\n",
      "cnt: 0 - valLoss: 0.4593721926212311 - trainLoss: 0.4666142761707306\n",
      "cnt: 0 - valLoss: 0.45936593413352966 - trainLoss: 0.4666083753108978\n",
      "cnt: 0 - valLoss: 0.45935994386672974 - trainLoss: 0.46660247445106506\n",
      "cnt: 0 - valLoss: 0.4593537151813507 - trainLoss: 0.4665966033935547\n",
      "cnt: 0 - valLoss: 0.459347665309906 - trainLoss: 0.4665907323360443\n",
      "cnt: 0 - valLoss: 0.45934149622917175 - trainLoss: 0.46658480167388916\n",
      "cnt: 0 - valLoss: 0.45933541655540466 - trainLoss: 0.4665789306163788\n",
      "cnt: 0 - valLoss: 0.4593292772769928 - trainLoss: 0.466573029756546\n",
      "cnt: 0 - valLoss: 0.4593232274055481 - trainLoss: 0.46656712889671326\n",
      "cnt: 0 - valLoss: 0.45931705832481384 - trainLoss: 0.4665611982345581\n",
      "cnt: 0 - valLoss: 0.4593110680580139 - trainLoss: 0.4665553867816925\n",
      "cnt: 0 - valLoss: 0.45930489897727966 - trainLoss: 0.46654942631721497\n",
      "cnt: 0 - valLoss: 0.45929884910583496 - trainLoss: 0.466543585062027\n",
      "cnt: 0 - valLoss: 0.4592928886413574 - trainLoss: 0.4665376543998718\n",
      "cnt: 0 - valLoss: 0.4592866897583008 - trainLoss: 0.46653181314468384\n",
      "cnt: 0 - valLoss: 0.45928072929382324 - trainLoss: 0.46652594208717346\n",
      "cnt: 0 - valLoss: 0.4592744708061218 - trainLoss: 0.4665200412273407\n",
      "cnt: 0 - valLoss: 0.4592685103416443 - trainLoss: 0.46651414036750793\n",
      "cnt: 0 - valLoss: 0.45926234126091003 - trainLoss: 0.46650829911231995\n",
      "cnt: 0 - valLoss: 0.4592563807964325 - trainLoss: 0.4665023982524872\n",
      "cnt: 0 - valLoss: 0.45925015211105347 - trainLoss: 0.4664965271949768\n",
      "cnt: 0 - valLoss: 0.4592441916465759 - trainLoss: 0.46649062633514404\n",
      "cnt: 0 - valLoss: 0.4592380225658417 - trainLoss: 0.46648475527763367\n",
      "cnt: 0 - valLoss: 0.45923206210136414 - trainLoss: 0.4664789140224457\n",
      "cnt: 0 - valLoss: 0.4592258632183075 - trainLoss: 0.4664730429649353\n",
      "cnt: 0 - valLoss: 0.45921993255615234 - trainLoss: 0.4664672315120697\n",
      "cnt: 0 - valLoss: 0.4592137634754181 - trainLoss: 0.46646130084991455\n",
      "cnt: 0 - valLoss: 0.45920777320861816 - trainLoss: 0.46645545959472656\n",
      "cnt: 0 - valLoss: 0.4592016339302063 - trainLoss: 0.4664495289325714\n",
      "cnt: 0 - valLoss: 0.45919567346572876 - trainLoss: 0.4664437472820282\n",
      "cnt: 0 - valLoss: 0.4591895043849945 - trainLoss: 0.46643781661987305\n",
      "cnt: 0 - valLoss: 0.4591835141181946 - trainLoss: 0.46643203496932983\n",
      "cnt: 0 - valLoss: 0.4591774344444275 - trainLoss: 0.46642613410949707\n",
      "cnt: 0 - valLoss: 0.4591714143753052 - trainLoss: 0.4664202928543091\n",
      "cnt: 0 - valLoss: 0.4591656029224396 - trainLoss: 0.4664143919944763\n",
      "cnt: 0 - valLoss: 0.45915937423706055 - trainLoss: 0.4664085805416107\n",
      "cnt: 0 - valLoss: 0.459153413772583 - trainLoss: 0.46640267968177795\n",
      "cnt: 0 - valLoss: 0.4591473340988159 - trainLoss: 0.46639683842658997\n",
      "cnt: 0 - valLoss: 0.45914143323898315 - trainLoss: 0.466390997171402\n",
      "cnt: 0 - valLoss: 0.4591352343559265 - trainLoss: 0.466385155916214\n",
      "cnt: 0 - valLoss: 0.45912930369377136 - trainLoss: 0.4663793444633484\n",
      "cnt: 0 - valLoss: 0.4591231644153595 - trainLoss: 0.466373473405838\n",
      "cnt: 0 - valLoss: 0.45911723375320435 - trainLoss: 0.4663676619529724\n",
      "cnt: 0 - valLoss: 0.45911112427711487 - trainLoss: 0.46636179089546204\n",
      "cnt: 0 - valLoss: 0.45910516381263733 - trainLoss: 0.46635591983795166\n",
      "cnt: 0 - valLoss: 0.45909908413887024 - trainLoss: 0.46635010838508606\n",
      "cnt: 0 - valLoss: 0.4590931534767151 - trainLoss: 0.46634429693222046\n",
      "cnt: 0 - valLoss: 0.459087073802948 - trainLoss: 0.4663384258747101\n",
      "cnt: 0 - valLoss: 0.45908111333847046 - trainLoss: 0.4663326144218445\n",
      "cnt: 0 - valLoss: 0.45907503366470337 - trainLoss: 0.4663267433643341\n",
      "cnt: 0 - valLoss: 0.45906907320022583 - trainLoss: 0.4663209617137909\n",
      "cnt: 0 - valLoss: 0.45906299352645874 - trainLoss: 0.4663150906562805\n",
      "cnt: 0 - valLoss: 0.4590570628643036 - trainLoss: 0.4663092792034149\n",
      "cnt: 0 - valLoss: 0.4590511620044708 - trainLoss: 0.4663034677505493\n",
      "cnt: 0 - valLoss: 0.45904505252838135 - trainLoss: 0.4662976861000061\n",
      "cnt: 0 - valLoss: 0.45903918147087097 - trainLoss: 0.4662918150424957\n",
      "cnt: 0 - valLoss: 0.4590330719947815 - trainLoss: 0.4662860035896301\n",
      "cnt: 0 - valLoss: 0.4590272009372711 - trainLoss: 0.46628016233444214\n",
      "cnt: 0 - valLoss: 0.45902106165885925 - trainLoss: 0.4662743806838989\n",
      "cnt: 0 - valLoss: 0.45901525020599365 - trainLoss: 0.46626853942871094\n",
      "cnt: 0 - valLoss: 0.4590091109275818 - trainLoss: 0.4662627577781677\n",
      "cnt: 0 - valLoss: 0.4590032398700714 - trainLoss: 0.4662569761276245\n",
      "cnt: 0 - valLoss: 0.45899710059165955 - trainLoss: 0.46625110507011414\n",
      "cnt: 0 - valLoss: 0.45899128913879395 - trainLoss: 0.4662453234195709\n",
      "cnt: 0 - valLoss: 0.4589851498603821 - trainLoss: 0.4662395119667053\n",
      "cnt: 0 - valLoss: 0.4589792788028717 - trainLoss: 0.4662337005138397\n",
      "cnt: 0 - valLoss: 0.458973228931427 - trainLoss: 0.46622785925865173\n",
      "cnt: 0 - valLoss: 0.45896732807159424 - trainLoss: 0.4662220776081085\n",
      "cnt: 0 - valLoss: 0.4589613080024719 - trainLoss: 0.4662163257598877\n",
      "cnt: 0 - valLoss: 0.45895540714263916 - trainLoss: 0.4662104845046997\n",
      "cnt: 0 - valLoss: 0.45894932746887207 - trainLoss: 0.4662046730518341\n",
      "cnt: 0 - valLoss: 0.4589434564113617 - trainLoss: 0.4661988914012909\n",
      "cnt: 0 - valLoss: 0.4589374363422394 - trainLoss: 0.4661931097507477\n",
      "cnt: 0 - valLoss: 0.4589315354824066 - trainLoss: 0.4661872982978821\n",
      "cnt: 0 - valLoss: 0.4589257538318634 - trainLoss: 0.4661814868450165\n",
      "cnt: 0 - valLoss: 0.45891955494880676 - trainLoss: 0.46617573499679565\n",
      "cnt: 0 - valLoss: 0.45891377329826355 - trainLoss: 0.46616992354393005\n",
      "cnt: 0 - valLoss: 0.45890772342681885 - trainLoss: 0.46616408228874207\n",
      "cnt: 0 - valLoss: 0.45890185236930847 - trainLoss: 0.46615836024284363\n",
      "cnt: 0 - valLoss: 0.45889583230018616 - trainLoss: 0.4661525785923004\n",
      "cnt: 0 - valLoss: 0.45888999104499817 - trainLoss: 0.4661467671394348\n",
      "cnt: 0 - valLoss: 0.4588839113712311 - trainLoss: 0.4661409854888916\n",
      "cnt: 0 - valLoss: 0.4588780701160431 - trainLoss: 0.466135174036026\n",
      "cnt: 0 - valLoss: 0.45887207984924316 - trainLoss: 0.4661294221878052\n",
      "cnt: 0 - valLoss: 0.4588662087917328 - trainLoss: 0.46612364053726196\n",
      "cnt: 0 - valLoss: 0.45886021852493286 - trainLoss: 0.46611785888671875\n",
      "cnt: 0 - valLoss: 0.4588543176651001 - trainLoss: 0.46611207723617554\n",
      "cnt: 0 - valLoss: 0.45884832739830017 - trainLoss: 0.4661063253879547\n",
      "cnt: 0 - valLoss: 0.45884251594543457 - trainLoss: 0.4661005735397339\n",
      "cnt: 0 - valLoss: 0.45883646607398987 - trainLoss: 0.4660947620868683\n",
      "cnt: 0 - valLoss: 0.4588305950164795 - trainLoss: 0.4660889804363251\n",
      "cnt: 0 - valLoss: 0.4588245749473572 - trainLoss: 0.46608319878578186\n",
      "cnt: 0 - valLoss: 0.4588187634944916 - trainLoss: 0.4660774767398834\n",
      "cnt: 0 - valLoss: 0.45881274342536926 - trainLoss: 0.46607163548469543\n",
      "cnt: 0 - valLoss: 0.45880693197250366 - trainLoss: 0.466065913438797\n",
      "cnt: 0 - valLoss: 0.45880118012428284 - trainLoss: 0.46606016159057617\n",
      "cnt: 0 - valLoss: 0.45879513025283813 - trainLoss: 0.46605437994003296\n",
      "cnt: 0 - valLoss: 0.4587893486022949 - trainLoss: 0.46604862809181213\n",
      "cnt: 0 - valLoss: 0.4587832987308502 - trainLoss: 0.4660428762435913\n",
      "cnt: 0 - valLoss: 0.4587775766849518 - trainLoss: 0.4660370647907257\n",
      "cnt: 0 - valLoss: 0.4587715268135071 - trainLoss: 0.46603137254714966\n",
      "cnt: 0 - valLoss: 0.45876574516296387 - trainLoss: 0.46602559089660645\n",
      "cnt: 0 - valLoss: 0.45875972509384155 - trainLoss: 0.4660198390483856\n",
      "cnt: 0 - valLoss: 0.45875394344329834 - trainLoss: 0.4660141170024872\n",
      "cnt: 0 - valLoss: 0.4587479531764984 - trainLoss: 0.46600833535194397\n",
      "cnt: 0 - valLoss: 0.4587421119213104 - trainLoss: 0.46600255370140076\n",
      "cnt: 0 - valLoss: 0.4587361216545105 - trainLoss: 0.4659968316555023\n",
      "cnt: 0 - valLoss: 0.4587303400039673 - trainLoss: 0.4659910798072815\n",
      "cnt: 0 - valLoss: 0.45872437953948975 - trainLoss: 0.46598535776138306\n",
      "cnt: 0 - valLoss: 0.45871859788894653 - trainLoss: 0.46597960591316223\n",
      "cnt: 0 - valLoss: 0.458712637424469 - trainLoss: 0.4659738838672638\n",
      "cnt: 0 - valLoss: 0.4587068557739258 - trainLoss: 0.4659680724143982\n",
      "cnt: 0 - valLoss: 0.45870089530944824 - trainLoss: 0.46596238017082214\n",
      "cnt: 0 - valLoss: 0.45869505405426025 - trainLoss: 0.4659566283226013\n",
      "cnt: 0 - valLoss: 0.4586890637874603 - trainLoss: 0.4659508466720581\n",
      "cnt: 0 - valLoss: 0.4586832821369171 - trainLoss: 0.46594521403312683\n",
      "cnt: 0 - valLoss: 0.4586775302886963 - trainLoss: 0.4659394323825836\n",
      "cnt: 0 - valLoss: 0.4586714804172516 - trainLoss: 0.46593374013900757\n",
      "cnt: 0 - valLoss: 0.45866572856903076 - trainLoss: 0.46592792868614197\n",
      "cnt: 0 - valLoss: 0.45865970849990845 - trainLoss: 0.4659222662448883\n",
      "cnt: 0 - valLoss: 0.4586539566516876 - trainLoss: 0.46591654419898987\n",
      "cnt: 0 - valLoss: 0.4586479067802429 - trainLoss: 0.46591079235076904\n",
      "cnt: 0 - valLoss: 0.45864221453666687 - trainLoss: 0.4659050703048706\n",
      "cnt: 0 - valLoss: 0.45863619446754456 - trainLoss: 0.4658993184566498\n",
      "cnt: 0 - valLoss: 0.45863041281700134 - trainLoss: 0.46589359641075134\n",
      "cnt: 0 - valLoss: 0.4586244523525238 - trainLoss: 0.4658879339694977\n",
      "cnt: 0 - valLoss: 0.4586186707019806 - trainLoss: 0.46588221192359924\n",
      "cnt: 0 - valLoss: 0.45861271023750305 - trainLoss: 0.4658764600753784\n",
      "cnt: 0 - valLoss: 0.45860692858695984 - trainLoss: 0.46587076783180237\n",
      "cnt: 0 - valLoss: 0.4586010277271271 - trainLoss: 0.4658650755882263\n",
      "cnt: 0 - valLoss: 0.45859524607658386 - trainLoss: 0.4658593535423279\n",
      "cnt: 0 - valLoss: 0.4585893154144287 - trainLoss: 0.46585360169410706\n",
      "cnt: 0 - valLoss: 0.4585835337638855 - trainLoss: 0.4658479690551758\n",
      "cnt: 0 - valLoss: 0.45857760310173035 - trainLoss: 0.46584221720695496\n",
      "cnt: 0 - valLoss: 0.4585718810558319 - trainLoss: 0.4658365845680237\n",
      "cnt: 0 - valLoss: 0.45856595039367676 - trainLoss: 0.46583083271980286\n",
      "cnt: 0 - valLoss: 0.4585602283477783 - trainLoss: 0.4658252000808716\n",
      "cnt: 0 - valLoss: 0.45855429768562317 - trainLoss: 0.46581950783729553\n",
      "cnt: 0 - valLoss: 0.45854854583740234 - trainLoss: 0.46581390500068665\n",
      "cnt: 0 - valLoss: 0.4585428535938263 - trainLoss: 0.4658081531524658\n",
      "cnt: 0 - valLoss: 0.45853695273399353 - trainLoss: 0.46580252051353455\n",
      "cnt: 0 - valLoss: 0.45853129029273987 - trainLoss: 0.4657968282699585\n",
      "cnt: 0 - valLoss: 0.45852532982826233 - trainLoss: 0.46579113602638245\n",
      "cnt: 0 - valLoss: 0.45851966738700867 - trainLoss: 0.4657854735851288\n",
      "cnt: 0 - valLoss: 0.45851370692253113 - trainLoss: 0.4657798111438751\n",
      "cnt: 0 - valLoss: 0.4585079550743103 - trainLoss: 0.46577417850494385\n",
      "cnt: 0 - valLoss: 0.45850205421447754 - trainLoss: 0.4657685160636902\n",
      "cnt: 0 - valLoss: 0.4584962725639343 - trainLoss: 0.4657628834247589\n",
      "cnt: 0 - valLoss: 0.45849040150642395 - trainLoss: 0.4657571315765381\n",
      "cnt: 0 - valLoss: 0.4584846496582031 - trainLoss: 0.4657515287399292\n",
      "cnt: 0 - valLoss: 0.45847877860069275 - trainLoss: 0.46574583649635315\n",
      "cnt: 0 - valLoss: 0.4584730267524719 - trainLoss: 0.46574023365974426\n",
      "cnt: 0 - valLoss: 0.45846715569496155 - trainLoss: 0.4657345414161682\n",
      "cnt: 0 - valLoss: 0.4584614038467407 - trainLoss: 0.4657289683818817\n",
      "cnt: 0 - valLoss: 0.45845577120780945 - trainLoss: 0.4657232463359833\n",
      "cnt: 0 - valLoss: 0.4584498107433319 - trainLoss: 0.4657176434993744\n",
      "cnt: 0 - valLoss: 0.45844414830207825 - trainLoss: 0.46571195125579834\n",
      "cnt: 0 - valLoss: 0.4584382176399231 - trainLoss: 0.46570631861686707\n",
      "cnt: 0 - valLoss: 0.4584325850009918 - trainLoss: 0.4657006859779358\n",
      "cnt: 0 - valLoss: 0.45842671394348145 - trainLoss: 0.46569502353668213\n",
      "cnt: 0 - valLoss: 0.458420991897583 - trainLoss: 0.46568942070007324\n",
      "cnt: 0 - valLoss: 0.4584151804447174 - trainLoss: 0.46568378806114197\n",
      "cnt: 0 - valLoss: 0.45840945839881897 - trainLoss: 0.4656781256198883\n",
      "cnt: 0 - valLoss: 0.45840364694595337 - trainLoss: 0.46567246317863464\n",
      "cnt: 0 - valLoss: 0.4583979845046997 - trainLoss: 0.46566686034202576\n",
      "cnt: 0 - valLoss: 0.45839211344718933 - trainLoss: 0.4656612277030945\n",
      "cnt: 0 - valLoss: 0.4583864212036133 - trainLoss: 0.4656556248664856\n",
      "cnt: 0 - valLoss: 0.4583805799484253 - trainLoss: 0.46564996242523193\n",
      "cnt: 0 - valLoss: 0.45837491750717163 - trainLoss: 0.46564429998397827\n",
      "cnt: 0 - valLoss: 0.45836931467056274 - trainLoss: 0.4656386971473694\n",
      "cnt: 0 - valLoss: 0.4583633840084076 - trainLoss: 0.4656330943107605\n",
      "cnt: 0 - valLoss: 0.4583578109741211 - trainLoss: 0.4656274616718292\n",
      "cnt: 0 - valLoss: 0.45835191011428833 - trainLoss: 0.46562182903289795\n",
      "cnt: 0 - valLoss: 0.45834627747535706 - trainLoss: 0.46561622619628906\n",
      "cnt: 0 - valLoss: 0.4583404064178467 - trainLoss: 0.4656105935573578\n",
      "cnt: 0 - valLoss: 0.4583348035812378 - trainLoss: 0.4656049907207489\n",
      "cnt: 0 - valLoss: 0.4583289623260498 - trainLoss: 0.4655993580818176\n",
      "cnt: 0 - valLoss: 0.4583233594894409 - trainLoss: 0.46559372544288635\n",
      "cnt: 0 - valLoss: 0.45831748843193054 - trainLoss: 0.46558815240859985\n",
      "cnt: 0 - valLoss: 0.45831188559532166 - trainLoss: 0.4655825197696686\n",
      "cnt: 0 - valLoss: 0.45830604434013367 - trainLoss: 0.4655769467353821\n",
      "cnt: 0 - valLoss: 0.45830038189888 - trainLoss: 0.4655712842941284\n",
      "cnt: 0 - valLoss: 0.4582945704460144 - trainLoss: 0.46556565165519714\n",
      "cnt: 0 - valLoss: 0.45828893780708313 - trainLoss: 0.46556007862091064\n",
      "cnt: 0 - valLoss: 0.45828333497047424 - trainLoss: 0.465554416179657\n",
      "cnt: 0 - valLoss: 0.45827749371528625 - trainLoss: 0.46554887294769287\n",
      "cnt: 0 - valLoss: 0.45827189087867737 - trainLoss: 0.4655432105064392\n",
      "cnt: 0 - valLoss: 0.45826610922813416 - trainLoss: 0.4655376374721527\n",
      "cnt: 0 - valLoss: 0.4582604467868805 - trainLoss: 0.4655320346355438\n",
      "cnt: 0 - valLoss: 0.4582546055316925 - trainLoss: 0.46552643179893494\n",
      "cnt: 0 - valLoss: 0.458249032497406 - trainLoss: 0.46552085876464844\n",
      "cnt: 0 - valLoss: 0.4582432210445404 - trainLoss: 0.46551525592803955\n",
      "cnt: 0 - valLoss: 0.4582376480102539 - trainLoss: 0.46550965309143066\n",
      "cnt: 0 - valLoss: 0.4582318365573883 - trainLoss: 0.4655040204524994\n",
      "cnt: 0 - valLoss: 0.45822620391845703 - trainLoss: 0.4654984772205353\n",
      "cnt: 0 - valLoss: 0.45822039246559143 - trainLoss: 0.4654928743839264\n",
      "cnt: 0 - valLoss: 0.4582148492336273 - trainLoss: 0.4654872715473175\n",
      "cnt: 0 - valLoss: 0.45820900797843933 - trainLoss: 0.4654816687107086\n",
      "cnt: 0 - valLoss: 0.45820343494415283 - trainLoss: 0.4654760956764221\n",
      "cnt: 0 - valLoss: 0.45819786190986633 - trainLoss: 0.46547046303749084\n",
      "cnt: 0 - valLoss: 0.45819202065467834 - trainLoss: 0.4654649794101715\n",
      "cnt: 0 - valLoss: 0.45818644762039185 - trainLoss: 0.46545931696891785\n",
      "cnt: 0 - valLoss: 0.45818066596984863 - trainLoss: 0.46545377373695374\n",
      "cnt: 0 - valLoss: 0.45817509293556213 - trainLoss: 0.46544820070266724\n",
      "cnt: 0 - valLoss: 0.4581693112850189 - trainLoss: 0.46544259786605835\n",
      "cnt: 0 - valLoss: 0.4581637382507324 - trainLoss: 0.46543702483177185\n",
      "cnt: 0 - valLoss: 0.4581579267978668 - trainLoss: 0.46543142199516296\n",
      "cnt: 0 - valLoss: 0.4581523537635803 - trainLoss: 0.46542584896087646\n",
      "cnt: 0 - valLoss: 0.4581466019153595 - trainLoss: 0.46542027592658997\n",
      "cnt: 0 - valLoss: 0.458141028881073 - trainLoss: 0.46541473269462585\n",
      "cnt: 0 - valLoss: 0.4581352770328522 - trainLoss: 0.46540912985801697\n",
      "cnt: 0 - valLoss: 0.4581296741962433 - trainLoss: 0.46540358662605286\n",
      "cnt: 0 - valLoss: 0.45812398195266724 - trainLoss: 0.46539798378944397\n",
      "cnt: 0 - valLoss: 0.45811834931373596 - trainLoss: 0.46539247035980225\n",
      "cnt: 0 - valLoss: 0.45811283588409424 - trainLoss: 0.46538686752319336\n",
      "cnt: 0 - valLoss: 0.45810702443122864 - trainLoss: 0.46538129448890686\n",
      "cnt: 0 - valLoss: 0.4581015110015869 - trainLoss: 0.465375691652298\n",
      "cnt: 0 - valLoss: 0.4580957293510437 - trainLoss: 0.46537017822265625\n",
      "cnt: 0 - valLoss: 0.4580901861190796 - trainLoss: 0.46536457538604736\n",
      "cnt: 0 - valLoss: 0.45808446407318115 - trainLoss: 0.465359091758728\n",
      "cnt: 0 - valLoss: 0.45807889103889465 - trainLoss: 0.46535348892211914\n",
      "cnt: 0 - valLoss: 0.45807313919067383 - trainLoss: 0.46534794569015503\n",
      "cnt: 0 - valLoss: 0.4580675959587097 - trainLoss: 0.4653424024581909\n",
      "cnt: 0 - valLoss: 0.45806190371513367 - trainLoss: 0.4653368294239044\n",
      "cnt: 0 - valLoss: 0.45805642008781433 - trainLoss: 0.4653312563896179\n",
      "cnt: 0 - valLoss: 0.45805075764656067 - trainLoss: 0.4653256833553314\n",
      "cnt: 0 - valLoss: 0.4580453038215637 - trainLoss: 0.4653201401233673\n",
      "cnt: 0 - valLoss: 0.45803967118263245 - trainLoss: 0.4653145372867584\n",
      "cnt: 0 - valLoss: 0.4580341577529907 - trainLoss: 0.4653089642524719\n",
      "cnt: 0 - valLoss: 0.4580286741256714 - trainLoss: 0.4653033912181854\n",
      "cnt: 0 - valLoss: 0.4580230414867401 - trainLoss: 0.4652978777885437\n",
      "cnt: 0 - valLoss: 0.45801758766174316 - trainLoss: 0.4652923047542572\n",
      "cnt: 0 - valLoss: 0.4580118954181671 - trainLoss: 0.4652867615222931\n",
      "cnt: 0 - valLoss: 0.45800644159317017 - trainLoss: 0.4652811884880066\n",
      "cnt: 0 - valLoss: 0.4580007791519165 - trainLoss: 0.4652756154537201\n",
      "cnt: 0 - valLoss: 0.45799532532691956 - trainLoss: 0.4652700424194336\n",
      "cnt: 0 - valLoss: 0.4579896926879883 - trainLoss: 0.4652644991874695\n",
      "cnt: 0 - valLoss: 0.45798423886299133 - trainLoss: 0.46525895595550537\n",
      "cnt: 0 - valLoss: 0.45797857642173767 - trainLoss: 0.46525341272354126\n",
      "cnt: 0 - valLoss: 0.4579731225967407 - trainLoss: 0.46524786949157715\n",
      "cnt: 0 - valLoss: 0.45796748995780945 - trainLoss: 0.46524232625961304\n",
      "cnt: 0 - valLoss: 0.4579620361328125 - trainLoss: 0.46523675322532654\n",
      "cnt: 0 - valLoss: 0.457956463098526 - trainLoss: 0.46523118019104004\n",
      "cnt: 0 - valLoss: 0.45795097947120667 - trainLoss: 0.4652257263660431\n",
      "cnt: 0 - valLoss: 0.4579455852508545 - trainLoss: 0.4652201533317566\n",
      "cnt: 0 - valLoss: 0.45793986320495605 - trainLoss: 0.4652145802974701\n",
      "cnt: 0 - valLoss: 0.4579344689846039 - trainLoss: 0.46520906686782837\n",
      "cnt: 0 - valLoss: 0.45792877674102783 - trainLoss: 0.46520355343818665\n",
      "cnt: 0 - valLoss: 0.45792338252067566 - trainLoss: 0.46519798040390015\n",
      "cnt: 0 - valLoss: 0.4579177498817444 - trainLoss: 0.46519240736961365\n",
      "cnt: 0 - valLoss: 0.4579123258590698 - trainLoss: 0.4651869237422943\n",
      "cnt: 0 - valLoss: 0.45790669322013855 - trainLoss: 0.4651814103126526\n",
      "cnt: 0 - valLoss: 0.4579012989997864 - trainLoss: 0.4651758372783661\n",
      "cnt: 0 - valLoss: 0.4578956961631775 - trainLoss: 0.46517032384872437\n",
      "cnt: 0 - valLoss: 0.45789024233818054 - trainLoss: 0.46516481041908264\n",
      "cnt: 0 - valLoss: 0.45788463950157166 - trainLoss: 0.4651592969894409\n",
      "cnt: 0 - valLoss: 0.4578791856765747 - trainLoss: 0.4651537537574768\n",
      "cnt: 0 - valLoss: 0.45787379145622253 - trainLoss: 0.4651482105255127\n",
      "cnt: 0 - valLoss: 0.4578680694103241 - trainLoss: 0.46514272689819336\n",
      "cnt: 0 - valLoss: 0.4578626751899719 - trainLoss: 0.46513718366622925\n",
      "cnt: 0 - valLoss: 0.45785701274871826 - trainLoss: 0.46513164043426514\n",
      "cnt: 0 - valLoss: 0.4578516185283661 - trainLoss: 0.4651261568069458\n",
      "cnt: 0 - valLoss: 0.4578460156917572 - trainLoss: 0.4651206433773041\n",
      "cnt: 0 - valLoss: 0.45784062147140503 - trainLoss: 0.46511510014533997\n",
      "cnt: 0 - valLoss: 0.45783495903015137 - trainLoss: 0.465109646320343\n",
      "cnt: 0 - valLoss: 0.4578295052051544 - trainLoss: 0.4651040732860565\n",
      "cnt: 0 - valLoss: 0.4578239321708679 - trainLoss: 0.4650985896587372\n",
      "cnt: 0 - valLoss: 0.45781847834587097 - trainLoss: 0.46509310603141785\n",
      "cnt: 0 - valLoss: 0.4578129053115845 - trainLoss: 0.46508756279945374\n",
      "cnt: 0 - valLoss: 0.4578074812889099 - trainLoss: 0.4650820791721344\n",
      "cnt: 0 - valLoss: 0.45780184864997864 - trainLoss: 0.46507659554481506\n",
      "cnt: 0 - valLoss: 0.4577964246273041 - trainLoss: 0.46507108211517334\n",
      "cnt: 0 - valLoss: 0.4577910900115967 - trainLoss: 0.46506553888320923\n",
      "cnt: 0 - valLoss: 0.4577854573726654 - trainLoss: 0.4650600552558899\n",
      "cnt: 0 - valLoss: 0.45778003334999084 - trainLoss: 0.46505454182624817\n",
      "cnt: 0 - valLoss: 0.45777446031570435 - trainLoss: 0.4650491178035736\n",
      "cnt: 0 - valLoss: 0.4577690362930298 - trainLoss: 0.4650435447692871\n",
      "cnt: 0 - valLoss: 0.4577634930610657 - trainLoss: 0.4650380611419678\n",
      "cnt: 0 - valLoss: 0.4577580690383911 - trainLoss: 0.46503257751464844\n",
      "cnt: 0 - valLoss: 0.45775243639945984 - trainLoss: 0.4650270640850067\n",
      "cnt: 0 - valLoss: 0.45774704217910767 - trainLoss: 0.4650215804576874\n",
      "cnt: 0 - valLoss: 0.45774146914482117 - trainLoss: 0.46501612663269043\n",
      "cnt: 0 - valLoss: 0.4577361047267914 - trainLoss: 0.4650106430053711\n",
      "cnt: 0 - valLoss: 0.4577305018901825 - trainLoss: 0.465005099773407\n",
      "cnt: 0 - valLoss: 0.4577251374721527 - trainLoss: 0.46499964594841003\n",
      "cnt: 0 - valLoss: 0.457719624042511 - trainLoss: 0.4649941623210907\n",
      "cnt: 0 - valLoss: 0.4577142596244812 - trainLoss: 0.46498870849609375\n",
      "cnt: 0 - valLoss: 0.4577089846134186 - trainLoss: 0.4649832546710968\n",
      "cnt: 0 - valLoss: 0.4577034115791321 - trainLoss: 0.46497786045074463\n",
      "cnt: 0 - valLoss: 0.45769810676574707 - trainLoss: 0.4649723172187805\n",
      "cnt: 0 - valLoss: 0.45769256353378296 - trainLoss: 0.46496692299842834\n",
      "cnt: 0 - valLoss: 0.45768728852272034 - trainLoss: 0.4649614691734314\n",
      "cnt: 0 - valLoss: 0.45768171548843384 - trainLoss: 0.4649560749530792\n",
      "cnt: 0 - valLoss: 0.45767635107040405 - trainLoss: 0.4649505913257599\n",
      "cnt: 0 - valLoss: 0.45767083764076233 - trainLoss: 0.46494513750076294\n",
      "cnt: 0 - valLoss: 0.45766547322273254 - trainLoss: 0.4649397134780884\n",
      "cnt: 0 - valLoss: 0.4576599895954132 - trainLoss: 0.4649343192577362\n",
      "cnt: 0 - valLoss: 0.45765459537506104 - trainLoss: 0.46492883563041687\n",
      "cnt: 0 - valLoss: 0.4576490521430969 - trainLoss: 0.4649234414100647\n",
      "cnt: 0 - valLoss: 0.4576437473297119 - trainLoss: 0.46491801738739014\n",
      "cnt: 0 - valLoss: 0.4576385021209717 - trainLoss: 0.4649125635623932\n",
      "cnt: 0 - valLoss: 0.4576329290866852 - trainLoss: 0.464907169342041\n",
      "cnt: 0 - valLoss: 0.45762768387794495 - trainLoss: 0.46490171551704407\n",
      "cnt: 0 - valLoss: 0.45762214064598083 - trainLoss: 0.4648963212966919\n",
      "cnt: 0 - valLoss: 0.45761677622795105 - trainLoss: 0.46489086747169495\n",
      "cnt: 0 - valLoss: 0.4576112627983093 - trainLoss: 0.4648854434490204\n",
      "cnt: 0 - valLoss: 0.45760592818260193 - trainLoss: 0.4648800492286682\n",
      "cnt: 0 - valLoss: 0.4576004147529602 - trainLoss: 0.4648745656013489\n",
      "cnt: 0 - valLoss: 0.4575951397418976 - trainLoss: 0.4648691713809967\n",
      "cnt: 0 - valLoss: 0.45758965611457825 - trainLoss: 0.46486377716064453\n",
      "cnt: 0 - valLoss: 0.45758435130119324 - trainLoss: 0.46485835313796997\n",
      "cnt: 0 - valLoss: 0.4575788378715515 - trainLoss: 0.464852899312973\n",
      "cnt: 0 - valLoss: 0.45757344365119934 - trainLoss: 0.46484750509262085\n",
      "cnt: 0 - valLoss: 0.4575680196285248 - trainLoss: 0.4648420810699463\n",
      "cnt: 0 - valLoss: 0.457562655210495 - trainLoss: 0.4648367166519165\n",
      "cnt: 0 - valLoss: 0.45755741000175476 - trainLoss: 0.46483126282691956\n",
      "cnt: 0 - valLoss: 0.45755189657211304 - trainLoss: 0.4648258686065674\n",
      "cnt: 0 - valLoss: 0.4575466215610504 - trainLoss: 0.4648204743862152\n",
      "cnt: 0 - valLoss: 0.4575411379337311 - trainLoss: 0.46481502056121826\n",
      "cnt: 0 - valLoss: 0.45753583312034607 - trainLoss: 0.4648096263408661\n",
      "cnt: 0 - valLoss: 0.45753031969070435 - trainLoss: 0.4648042321205139\n",
      "cnt: 0 - valLoss: 0.45752501487731934 - trainLoss: 0.46479880809783936\n",
      "cnt: 0 - valLoss: 0.4575195610523224 - trainLoss: 0.4647934138774872\n",
      "cnt: 0 - valLoss: 0.45751431584358215 - trainLoss: 0.464788019657135\n",
      "cnt: 0 - valLoss: 0.45750880241394043 - trainLoss: 0.46478262543678284\n",
      "cnt: 0 - valLoss: 0.4575034976005554 - trainLoss: 0.46477723121643066\n",
      "cnt: 0 - valLoss: 0.45749804377555847 - trainLoss: 0.4647718071937561\n",
      "cnt: 0 - valLoss: 0.4574927091598511 - trainLoss: 0.4647664427757263\n",
      "cnt: 0 - valLoss: 0.45748746395111084 - trainLoss: 0.46476104855537415\n",
      "cnt: 0 - valLoss: 0.4574819803237915 - trainLoss: 0.464755654335022\n",
      "cnt: 0 - valLoss: 0.4574767053127289 - trainLoss: 0.4647502601146698\n",
      "cnt: 0 - valLoss: 0.45747122168540955 - trainLoss: 0.4647448658943176\n",
      "cnt: 0 - valLoss: 0.4574660062789917 - trainLoss: 0.46473947167396545\n",
      "cnt: 0 - valLoss: 0.45746052265167236 - trainLoss: 0.4647340774536133\n",
      "cnt: 0 - valLoss: 0.45745524764060974 - trainLoss: 0.4647287130355835\n",
      "cnt: 0 - valLoss: 0.4574497938156128 - trainLoss: 0.4647233188152313\n",
      "cnt: 0 - valLoss: 0.4574444890022278 - trainLoss: 0.46471795439720154\n",
      "cnt: 0 - valLoss: 0.4574390649795532 - trainLoss: 0.46471256017684937\n",
      "cnt: 0 - valLoss: 0.457433819770813 - trainLoss: 0.46470722556114197\n",
      "cnt: 0 - valLoss: 0.45742833614349365 - trainLoss: 0.4647018313407898\n",
      "cnt: 0 - valLoss: 0.45742306113243103 - trainLoss: 0.4646964371204376\n",
      "cnt: 0 - valLoss: 0.45741787552833557 - trainLoss: 0.46469104290008545\n",
      "cnt: 0 - valLoss: 0.45741236209869385 - trainLoss: 0.46468567848205566\n",
      "cnt: 0 - valLoss: 0.457407146692276 - trainLoss: 0.4646802842617035\n",
      "cnt: 0 - valLoss: 0.45740166306495667 - trainLoss: 0.4646749496459961\n",
      "cnt: 0 - valLoss: 0.45739641785621643 - trainLoss: 0.4646695554256439\n",
      "cnt: 0 - valLoss: 0.4573909342288971 - trainLoss: 0.46466416120529175\n",
      "cnt: 0 - valLoss: 0.45738571882247925 - trainLoss: 0.46465879678726196\n",
      "cnt: 0 - valLoss: 0.4573802649974823 - trainLoss: 0.46465346217155457\n",
      "cnt: 0 - valLoss: 0.45737504959106445 - trainLoss: 0.4646480083465576\n",
      "cnt: 0 - valLoss: 0.4573695957660675 - trainLoss: 0.4646427035331726\n",
      "cnt: 0 - valLoss: 0.45736438035964966 - trainLoss: 0.46463730931282043\n",
      "cnt: 0 - valLoss: 0.4573589265346527 - trainLoss: 0.46463197469711304\n",
      "cnt: 0 - valLoss: 0.4573536515235901 - trainLoss: 0.46462661027908325\n",
      "cnt: 0 - valLoss: 0.4573485255241394 - trainLoss: 0.4646212160587311\n",
      "cnt: 0 - valLoss: 0.4573430120944977 - trainLoss: 0.4646158814430237\n",
      "cnt: 0 - valLoss: 0.45733779668807983 - trainLoss: 0.4646105170249939\n",
      "cnt: 0 - valLoss: 0.45733240246772766 - trainLoss: 0.4646051824092865\n",
      "cnt: 0 - valLoss: 0.4573271572589874 - trainLoss: 0.4645997881889343\n",
      "cnt: 0 - valLoss: 0.45732173323631287 - trainLoss: 0.4645944833755493\n",
      "cnt: 0 - valLoss: 0.4573165476322174 - trainLoss: 0.46458908915519714\n",
      "cnt: 0 - valLoss: 0.45731109380722046 - trainLoss: 0.46458378434181213\n",
      "cnt: 0 - valLoss: 0.457305908203125 - trainLoss: 0.46457839012145996\n",
      "cnt: 0 - valLoss: 0.45730042457580566 - trainLoss: 0.46457308530807495\n",
      "cnt: 0 - valLoss: 0.4572952389717102 - trainLoss: 0.4645676910877228\n",
      "cnt: 0 - valLoss: 0.4572898745536804 - trainLoss: 0.46456238627433777\n",
      "cnt: 0 - valLoss: 0.4572845697402954 - trainLoss: 0.464557021856308\n",
      "cnt: 0 - valLoss: 0.45727941393852234 - trainLoss: 0.4645516574382782\n",
      "cnt: 0 - valLoss: 0.4572739899158478 - trainLoss: 0.4645463526248932\n",
      "cnt: 0 - valLoss: 0.4572688341140747 - trainLoss: 0.46454092860221863\n",
      "cnt: 0 - valLoss: 0.45726338028907776 - trainLoss: 0.464535653591156\n",
      "cnt: 0 - valLoss: 0.4572581946849823 - trainLoss: 0.46453025937080383\n",
      "cnt: 0 - valLoss: 0.45725277066230774 - trainLoss: 0.4645249545574188\n",
      "cnt: 0 - valLoss: 0.4572475850582123 - trainLoss: 0.4645196199417114\n",
      "cnt: 0 - valLoss: 0.4572422206401825 - trainLoss: 0.46451425552368164\n",
      "cnt: 0 - valLoss: 0.45723697543144226 - trainLoss: 0.46450895071029663\n",
      "cnt: 0 - valLoss: 0.45723164081573486 - trainLoss: 0.46450355648994446\n",
      "cnt: 0 - valLoss: 0.4572264552116394 - trainLoss: 0.46449825167655945\n",
      "cnt: 0 - valLoss: 0.45722103118896484 - trainLoss: 0.46449294686317444\n",
      "cnt: 0 - valLoss: 0.457215815782547 - trainLoss: 0.46448764204978943\n",
      "cnt: 0 - valLoss: 0.4572106599807739 - trainLoss: 0.46448224782943726\n",
      "cnt: 0 - valLoss: 0.45720526576042175 - trainLoss: 0.46447697281837463\n",
      "cnt: 0 - valLoss: 0.45720013976097107 - trainLoss: 0.46447157859802246\n",
      "cnt: 0 - valLoss: 0.4571947157382965 - trainLoss: 0.4644663333892822\n",
      "cnt: 0 - valLoss: 0.4571895897388458 - trainLoss: 0.46446096897125244\n",
      "cnt: 0 - valLoss: 0.4571841359138489 - trainLoss: 0.46445566415786743\n",
      "cnt: 0 - valLoss: 0.4571789503097534 - trainLoss: 0.46445032954216003\n",
      "cnt: 0 - valLoss: 0.4571736454963684 - trainLoss: 0.46444496512413025\n",
      "cnt: 0 - valLoss: 0.4571684002876282 - trainLoss: 0.46443966031074524\n",
      "cnt: 0 - valLoss: 0.45716309547424316 - trainLoss: 0.46443435549736023\n",
      "cnt: 0 - valLoss: 0.4571579098701477 - trainLoss: 0.4644290804862976\n",
      "cnt: 0 - valLoss: 0.4571525454521179 - trainLoss: 0.4644237458705902\n",
      "cnt: 0 - valLoss: 0.45714735984802246 - trainLoss: 0.4644184410572052\n",
      "cnt: 0 - valLoss: 0.4571422338485718 - trainLoss: 0.4644130766391754\n",
      "cnt: 0 - valLoss: 0.457136869430542 - trainLoss: 0.4644078016281128\n",
      "cnt: 0 - valLoss: 0.45713168382644653 - trainLoss: 0.4644024670124054\n",
      "cnt: 0 - valLoss: 0.45712631940841675 - trainLoss: 0.4643971621990204\n",
      "cnt: 0 - valLoss: 0.4571211636066437 - trainLoss: 0.4643918573856354\n",
      "cnt: 0 - valLoss: 0.4571158289909363 - trainLoss: 0.46438655257225037\n",
      "cnt: 0 - valLoss: 0.4571106433868408 - trainLoss: 0.46438124775886536\n",
      "cnt: 0 - valLoss: 0.4571053087711334 - trainLoss: 0.46437591314315796\n",
      "cnt: 0 - valLoss: 0.45710012316703796 - trainLoss: 0.46437060832977295\n",
      "cnt: 0 - valLoss: 0.4570947587490082 - trainLoss: 0.46436530351638794\n",
      "cnt: 0 - valLoss: 0.4570895731449127 - trainLoss: 0.46435999870300293\n",
      "cnt: 0 - valLoss: 0.4570842683315277 - trainLoss: 0.4643546938896179\n",
      "cnt: 0 - valLoss: 0.45707908272743225 - trainLoss: 0.4643494188785553\n",
      "cnt: 0 - valLoss: 0.45707377791404724 - trainLoss: 0.4643441140651703\n",
      "cnt: 0 - valLoss: 0.457068532705307 - trainLoss: 0.46433886885643005\n",
      "cnt: 0 - valLoss: 0.4570634067058563 - trainLoss: 0.46433356404304504\n",
      "cnt: 0 - valLoss: 0.45705804228782654 - trainLoss: 0.4643282890319824\n",
      "cnt: 0 - valLoss: 0.45705288648605347 - trainLoss: 0.4643229842185974\n",
      "cnt: 0 - valLoss: 0.45704755187034607 - trainLoss: 0.4643177092075348\n",
      "cnt: 0 - valLoss: 0.457042396068573 - trainLoss: 0.4643124043941498\n",
      "cnt: 0 - valLoss: 0.4570370316505432 - trainLoss: 0.46430715918540955\n",
      "cnt: 0 - valLoss: 0.45703184604644775 - trainLoss: 0.4643018841743469\n",
      "cnt: 0 - valLoss: 0.45702654123306274 - trainLoss: 0.4642965793609619\n",
      "cnt: 0 - valLoss: 0.4570213556289673 - trainLoss: 0.4642912745475769\n",
      "cnt: 0 - valLoss: 0.4570160210132599 - trainLoss: 0.4642859697341919\n",
      "cnt: 0 - valLoss: 0.45701083540916443 - trainLoss: 0.46428072452545166\n",
      "cnt: 0 - valLoss: 0.4570055902004242 - trainLoss: 0.46427541971206665\n",
      "cnt: 0 - valLoss: 0.45700037479400635 - trainLoss: 0.4642701745033264\n",
      "cnt: 0 - valLoss: 0.4569953382015228 - trainLoss: 0.46426481008529663\n",
      "cnt: 0 - valLoss: 0.45698994398117065 - trainLoss: 0.4642595946788788\n",
      "cnt: 0 - valLoss: 0.45698481798171997 - trainLoss: 0.46425431966781616\n",
      "cnt: 0 - valLoss: 0.45697951316833496 - trainLoss: 0.4642490744590759\n",
      "cnt: 0 - valLoss: 0.4569743871688843 - trainLoss: 0.4642437696456909\n",
      "cnt: 0 - valLoss: 0.4569690525531769 - trainLoss: 0.4642385244369507\n",
      "cnt: 0 - valLoss: 0.4569639563560486 - trainLoss: 0.46423327922821045\n",
      "cnt: 0 - valLoss: 0.4569586515426636 - trainLoss: 0.4642280042171478\n",
      "cnt: 0 - valLoss: 0.4569534957408905 - trainLoss: 0.4642227292060852\n",
      "cnt: 0 - valLoss: 0.4569482207298279 - trainLoss: 0.4642174243927002\n",
      "cnt: 0 - valLoss: 0.4569430947303772 - trainLoss: 0.46421217918395996\n",
      "cnt: 0 - valLoss: 0.4569378197193146 - trainLoss: 0.4642069339752197\n",
      "cnt: 0 - valLoss: 0.4569326639175415 - trainLoss: 0.4642016887664795\n",
      "cnt: 0 - valLoss: 0.4569275975227356 - trainLoss: 0.46419641375541687\n",
      "cnt: 0 - valLoss: 0.4569222927093506 - trainLoss: 0.46419113874435425\n",
      "cnt: 0 - valLoss: 0.4569171667098999 - trainLoss: 0.464185893535614\n",
      "cnt: 0 - valLoss: 0.4569118916988373 - trainLoss: 0.46418067812919617\n",
      "cnt: 0 - valLoss: 0.4569067656993866 - trainLoss: 0.46417537331581116\n",
      "cnt: 0 - valLoss: 0.4569014608860016 - trainLoss: 0.46417009830474854\n",
      "cnt: 0 - valLoss: 0.4568963646888733 - trainLoss: 0.4641648828983307\n",
      "cnt: 0 - valLoss: 0.45689108967781067 - trainLoss: 0.46415960788726807\n",
      "cnt: 0 - valLoss: 0.4568859934806824 - trainLoss: 0.4641543924808502\n",
      "cnt: 0 - valLoss: 0.45688071846961975 - trainLoss: 0.4641491174697876\n",
      "cnt: 0 - valLoss: 0.45687562227249146 - trainLoss: 0.46414387226104736\n",
      "cnt: 0 - valLoss: 0.45687055587768555 - trainLoss: 0.46413862705230713\n",
      "cnt: 0 - valLoss: 0.45686525106430054 - trainLoss: 0.4641334116458893\n",
      "cnt: 0 - valLoss: 0.45686018466949463 - trainLoss: 0.4641281068325043\n",
      "cnt: 0 - valLoss: 0.456854909658432 - trainLoss: 0.4641228914260864\n",
      "cnt: 0 - valLoss: 0.4568498134613037 - trainLoss: 0.4641176164150238\n",
      "cnt: 0 - valLoss: 0.4568445384502411 - trainLoss: 0.46411240100860596\n",
      "cnt: 0 - valLoss: 0.4568394720554352 - trainLoss: 0.46410712599754333\n",
      "cnt: 0 - valLoss: 0.45683416724205017 - trainLoss: 0.4641019105911255\n",
      "cnt: 0 - valLoss: 0.45682910084724426 - trainLoss: 0.46409669518470764\n",
      "cnt: 0 - valLoss: 0.4568238854408264 - trainLoss: 0.464091420173645\n",
      "cnt: 0 - valLoss: 0.4568187892436981 - trainLoss: 0.4640862047672272\n",
      "cnt: 0 - valLoss: 0.4568135142326355 - trainLoss: 0.4640809893608093\n",
      "cnt: 0 - valLoss: 0.4568084478378296 - trainLoss: 0.4640757739543915\n",
      "cnt: 0 - valLoss: 0.45680347084999084 - trainLoss: 0.46407049894332886\n",
      "cnt: 0 - valLoss: 0.45679816603660583 - trainLoss: 0.464065283536911\n",
      "cnt: 0 - valLoss: 0.4567930996417999 - trainLoss: 0.4640600383281708\n",
      "cnt: 0 - valLoss: 0.4567878246307373 - trainLoss: 0.46405482292175293\n",
      "cnt: 0 - valLoss: 0.45678281784057617 - trainLoss: 0.4640496075153351\n",
      "cnt: 0 - valLoss: 0.45677754282951355 - trainLoss: 0.46404439210891724\n",
      "cnt: 0 - valLoss: 0.45677250623703003 - trainLoss: 0.4640391767024994\n",
      "cnt: 0 - valLoss: 0.4567672610282898 - trainLoss: 0.46403393149375916\n",
      "cnt: 0 - valLoss: 0.4567621648311615 - trainLoss: 0.4640287160873413\n",
      "cnt: 0 - valLoss: 0.4567570090293884 - trainLoss: 0.4640234708786011\n",
      "cnt: 0 - valLoss: 0.45675191283226013 - trainLoss: 0.4640182852745056\n",
      "cnt: 0 - valLoss: 0.456746906042099 - trainLoss: 0.46401306986808777\n",
      "cnt: 0 - valLoss: 0.4567416310310364 - trainLoss: 0.46400782465934753\n",
      "cnt: 0 - valLoss: 0.45673662424087524 - trainLoss: 0.4640026092529297\n",
      "cnt: 0 - valLoss: 0.456731379032135 - trainLoss: 0.46399742364883423\n",
      "cnt: 0 - valLoss: 0.4567263424396515 - trainLoss: 0.463992178440094\n",
      "cnt: 0 - valLoss: 0.45672109723091125 - trainLoss: 0.46398693323135376\n",
      "cnt: 0 - valLoss: 0.45671603083610535 - trainLoss: 0.4639817178249359\n",
      "cnt: 0 - valLoss: 0.4567108452320099 - trainLoss: 0.46397653222084045\n",
      "cnt: 0 - valLoss: 0.45670583844184875 - trainLoss: 0.4639713764190674\n",
      "cnt: 0 - valLoss: 0.4567006230354309 - trainLoss: 0.46396613121032715\n",
      "cnt: 0 - valLoss: 0.456695556640625 - trainLoss: 0.4639609158039093\n",
      "cnt: 0 - valLoss: 0.45669037103652954 - trainLoss: 0.46395570039749146\n",
      "cnt: 0 - valLoss: 0.45668530464172363 - trainLoss: 0.463950514793396\n",
      "cnt: 0 - valLoss: 0.4566803276538849 - trainLoss: 0.46394526958465576\n",
      "cnt: 0 - valLoss: 0.45667508244514465 - trainLoss: 0.4639401137828827\n",
      "cnt: 0 - valLoss: 0.4566701054573059 - trainLoss: 0.46393489837646484\n",
      "cnt: 0 - valLoss: 0.4566648602485657 - trainLoss: 0.4639297127723694\n",
      "cnt: 0 - valLoss: 0.45665988326072693 - trainLoss: 0.4639245271682739\n",
      "cnt: 0 - valLoss: 0.4566545784473419 - trainLoss: 0.4639193117618561\n",
      "cnt: 0 - valLoss: 0.4566496014595032 - trainLoss: 0.46391409635543823\n",
      "cnt: 0 - valLoss: 0.4566443860530853 - trainLoss: 0.4639088809490204\n",
      "cnt: 0 - valLoss: 0.4566394090652466 - trainLoss: 0.4639037549495697\n",
      "cnt: 0 - valLoss: 0.4566342234611511 - trainLoss: 0.46389850974082947\n",
      "cnt: 0 - valLoss: 0.45662921667099 - trainLoss: 0.4638932943344116\n",
      "cnt: 0 - valLoss: 0.45662423968315125 - trainLoss: 0.46388810873031616\n",
      "cnt: 0 - valLoss: 0.456618994474411 - trainLoss: 0.4638829529285431\n",
      "cnt: 0 - valLoss: 0.4566139876842499 - trainLoss: 0.46387776732444763\n",
      "cnt: 0 - valLoss: 0.4566088020801544 - trainLoss: 0.4638725817203522\n",
      "cnt: 0 - valLoss: 0.4566038250923157 - trainLoss: 0.4638673663139343\n",
      "cnt: 0 - valLoss: 0.4565986096858978 - trainLoss: 0.46386221051216125\n",
      "cnt: 0 - valLoss: 0.4565936326980591 - trainLoss: 0.4638569951057434\n",
      "cnt: 0 - valLoss: 0.45658838748931885 - trainLoss: 0.4638518691062927\n",
      "cnt: 0 - valLoss: 0.4565834403038025 - trainLoss: 0.4638466536998749\n",
      "cnt: 0 - valLoss: 0.4565782845020294 - trainLoss: 0.4638414680957794\n",
      "cnt: 0 - valLoss: 0.4565732181072235 - trainLoss: 0.46383628249168396\n",
      "cnt: 0 - valLoss: 0.4565680921077728 - trainLoss: 0.4638311564922333\n",
      "cnt: 0 - valLoss: 0.4565630853176117 - trainLoss: 0.46382594108581543\n",
      "cnt: 0 - valLoss: 0.45655813813209534 - trainLoss: 0.46382075548171997\n",
      "cnt: 0 - valLoss: 0.4565529227256775 - trainLoss: 0.4638155698776245\n",
      "cnt: 0 - valLoss: 0.45654794573783875 - trainLoss: 0.46381038427352905\n",
      "cnt: 0 - valLoss: 0.4565427601337433 - trainLoss: 0.46380525827407837\n",
      "cnt: 0 - valLoss: 0.45653778314590454 - trainLoss: 0.4638000726699829\n",
      "cnt: 0 - valLoss: 0.4565325975418091 - trainLoss: 0.46379488706588745\n",
      "cnt: 0 - valLoss: 0.4565276801586151 - trainLoss: 0.4637897312641144\n",
      "cnt: 0 - valLoss: 0.45652249455451965 - trainLoss: 0.4637845754623413\n",
      "cnt: 0 - valLoss: 0.4565175175666809 - trainLoss: 0.46377936005592346\n",
      "cnt: 0 - valLoss: 0.45651236176490784 - trainLoss: 0.4637742340564728\n",
      "cnt: 0 - valLoss: 0.4565073847770691 - trainLoss: 0.4637690782546997\n",
      "cnt: 0 - valLoss: 0.4565024673938751 - trainLoss: 0.46376386284828186\n",
      "cnt: 0 - valLoss: 0.4564972221851349 - trainLoss: 0.46375876665115356\n",
      "cnt: 0 - valLoss: 0.4564923048019409 - trainLoss: 0.4637535810470581\n",
      "cnt: 0 - valLoss: 0.45648714900016785 - trainLoss: 0.4637484550476074\n",
      "cnt: 0 - valLoss: 0.4564822018146515 - trainLoss: 0.4637432396411896\n",
      "cnt: 0 - valLoss: 0.4564770460128784 - trainLoss: 0.4637380838394165\n",
      "cnt: 0 - valLoss: 0.4564720690250397 - trainLoss: 0.4637329578399658\n",
      "cnt: 0 - valLoss: 0.4564668834209442 - trainLoss: 0.463727742433548\n",
      "cnt: 0 - valLoss: 0.45646196603775024 - trainLoss: 0.4637226462364197\n",
      "cnt: 0 - valLoss: 0.4564568102359772 - trainLoss: 0.463717520236969\n",
      "cnt: 0 - valLoss: 0.4564518332481384 - trainLoss: 0.4637123644351959\n",
      "cnt: 0 - valLoss: 0.4564467966556549 - trainLoss: 0.46370720863342285\n",
      "cnt: 0 - valLoss: 0.45644181966781616 - trainLoss: 0.4637020528316498\n",
      "cnt: 0 - valLoss: 0.4564369320869446 - trainLoss: 0.4636968672275543\n",
      "cnt: 0 - valLoss: 0.4564317464828491 - trainLoss: 0.46369174122810364\n",
      "cnt: 0 - valLoss: 0.45642679929733276 - trainLoss: 0.46368661522865295\n",
      "cnt: 0 - valLoss: 0.4564216732978821 - trainLoss: 0.46368151903152466\n",
      "cnt: 0 - valLoss: 0.45641669631004333 - trainLoss: 0.4636763334274292\n",
      "cnt: 0 - valLoss: 0.4564116299152374 - trainLoss: 0.4636712074279785\n",
      "cnt: 0 - valLoss: 0.45640668272972107 - trainLoss: 0.4636661112308502\n",
      "cnt: 0 - valLoss: 0.456401526927948 - trainLoss: 0.46366092562675476\n",
      "cnt: 0 - valLoss: 0.4563966393470764 - trainLoss: 0.46365582942962646\n",
      "cnt: 0 - valLoss: 0.45639148354530334 - trainLoss: 0.4636506140232086\n",
      "cnt: 0 - valLoss: 0.4563865661621094 - trainLoss: 0.4636455476284027\n",
      "cnt: 0 - valLoss: 0.4563816785812378 - trainLoss: 0.46364039182662964\n",
      "cnt: 0 - valLoss: 0.4563765227794647 - trainLoss: 0.46363529562950134\n",
      "cnt: 0 - valLoss: 0.4563716650009155 - trainLoss: 0.4636301100254059\n",
      "cnt: 0 - valLoss: 0.45636653900146484 - trainLoss: 0.4636250138282776\n",
      "cnt: 0 - valLoss: 0.4563615918159485 - trainLoss: 0.4636198878288269\n",
      "cnt: 0 - valLoss: 0.4563565254211426 - trainLoss: 0.4636147916316986\n",
      "cnt: 0 - valLoss: 0.456351637840271 - trainLoss: 0.46360960602760315\n",
      "cnt: 0 - valLoss: 0.4563464820384979 - trainLoss: 0.46360450983047485\n",
      "cnt: 0 - valLoss: 0.45634156465530396 - trainLoss: 0.46359941363334656\n",
      "cnt: 0 - valLoss: 0.4563365578651428 - trainLoss: 0.4635942876338959\n",
      "cnt: 0 - valLoss: 0.4563315808773041 - trainLoss: 0.4635891616344452\n",
      "cnt: 0 - valLoss: 0.4563267230987549 - trainLoss: 0.4635840058326721\n",
      "cnt: 0 - valLoss: 0.4563215374946594 - trainLoss: 0.4635789096355438\n",
      "cnt: 0 - valLoss: 0.4563167095184326 - trainLoss: 0.4635738134384155\n",
      "cnt: 0 - valLoss: 0.4563116133213043 - trainLoss: 0.46356868743896484\n",
      "cnt: 0 - valLoss: 0.45630672574043274 - trainLoss: 0.46356356143951416\n",
      "cnt: 0 - valLoss: 0.45630156993865967 - trainLoss: 0.46355849504470825\n",
      "cnt: 0 - valLoss: 0.4562966823577881 - trainLoss: 0.46355336904525757\n",
      "cnt: 0 - valLoss: 0.4562916159629822 - trainLoss: 0.4635482430458069\n",
      "cnt: 0 - valLoss: 0.4562867283821106 - trainLoss: 0.4635431468486786\n",
      "cnt: 0 - valLoss: 0.4562816917896271 - trainLoss: 0.4635379910469055\n",
      "cnt: 0 - valLoss: 0.4562767446041107 - trainLoss: 0.4635328948497772\n",
      "cnt: 0 - valLoss: 0.4562717378139496 - trainLoss: 0.46352776885032654\n",
      "cnt: 0 - valLoss: 0.45626676082611084 - trainLoss: 0.463522732257843\n",
      "cnt: 0 - valLoss: 0.4562619626522064 - trainLoss: 0.46351757645606995\n",
      "cnt: 0 - valLoss: 0.4562568664550781 - trainLoss: 0.4635125398635864\n",
      "cnt: 0 - valLoss: 0.45625200867652893 - trainLoss: 0.46350738406181335\n",
      "cnt: 0 - valLoss: 0.45624691247940063 - trainLoss: 0.46350231766700745\n",
      "cnt: 0 - valLoss: 0.45624208450317383 - trainLoss: 0.4634971618652344\n",
      "cnt: 0 - valLoss: 0.4562370181083679 - trainLoss: 0.4634920656681061\n",
      "cnt: 0 - valLoss: 0.45623213052749634 - trainLoss: 0.4634869694709778\n",
      "cnt: 0 - valLoss: 0.45622706413269043 - trainLoss: 0.4634818732738495\n",
      "cnt: 0 - valLoss: 0.4562222361564636 - trainLoss: 0.4634768068790436\n",
      "cnt: 0 - valLoss: 0.4562171697616577 - trainLoss: 0.4634717106819153\n",
      "cnt: 0 - valLoss: 0.45621225237846375 - trainLoss: 0.4634666442871094\n",
      "cnt: 0 - valLoss: 0.45620739459991455 - trainLoss: 0.4634615182876587\n",
      "cnt: 0 - valLoss: 0.45620226860046387 - trainLoss: 0.4634564220905304\n",
      "cnt: 0 - valLoss: 0.45619747042655945 - trainLoss: 0.4634513258934021\n",
      "cnt: 0 - valLoss: 0.45619237422943115 - trainLoss: 0.4634462893009186\n",
      "cnt: 0 - valLoss: 0.45618751645088196 - trainLoss: 0.4634411334991455\n",
      "cnt: 0 - valLoss: 0.45618245005607605 - trainLoss: 0.463436096906662\n",
      "cnt: 0 - valLoss: 0.45617762207984924 - trainLoss: 0.4634310007095337\n",
      "cnt: 0 - valLoss: 0.45617246627807617 - trainLoss: 0.4634259045124054\n",
      "cnt: 0 - valLoss: 0.45616766810417175 - trainLoss: 0.4634208381175995\n",
      "cnt: 0 - valLoss: 0.45616260170936584 - trainLoss: 0.4634157121181488\n",
      "cnt: 0 - valLoss: 0.45615771412849426 - trainLoss: 0.4634106755256653\n",
      "cnt: 0 - valLoss: 0.45615291595458984 - trainLoss: 0.463405579328537\n",
      "cnt: 0 - valLoss: 0.45614781975746155 - trainLoss: 0.46340057253837585\n",
      "cnt: 0 - valLoss: 0.45614302158355713 - trainLoss: 0.4633954167366028\n",
      "cnt: 0 - valLoss: 0.45613792538642883 - trainLoss: 0.46339038014411926\n",
      "cnt: 0 - valLoss: 0.4561331272125244 - trainLoss: 0.46338531374931335\n",
      "cnt: 0 - valLoss: 0.4561280310153961 - trainLoss: 0.46338024735450745\n",
      "cnt: 0 - valLoss: 0.4561232328414917 - trainLoss: 0.46337515115737915\n",
      "cnt: 0 - valLoss: 0.4561181664466858 - trainLoss: 0.46337005496025085\n",
      "cnt: 0 - valLoss: 0.4561133086681366 - trainLoss: 0.4633650481700897\n",
      "cnt: 0 - valLoss: 0.45610830187797546 - trainLoss: 0.4633599519729614\n",
      "cnt: 0 - valLoss: 0.45610347390174866 - trainLoss: 0.4633548855781555\n",
      "cnt: 0 - valLoss: 0.45609840750694275 - trainLoss: 0.4633497893810272\n",
      "cnt: 0 - valLoss: 0.45609360933303833 - trainLoss: 0.4633447527885437\n",
      "cnt: 0 - valLoss: 0.4560888111591339 - trainLoss: 0.4633396863937378\n",
      "cnt: 0 - valLoss: 0.456083744764328 - trainLoss: 0.4633346498012543\n",
      "cnt: 0 - valLoss: 0.4560789167881012 - trainLoss: 0.4633295238018036\n",
      "cnt: 0 - valLoss: 0.4560738801956177 - trainLoss: 0.46332451701164246\n",
      "cnt: 0 - valLoss: 0.45606905221939087 - trainLoss: 0.46331942081451416\n",
      "cnt: 0 - valLoss: 0.45606404542922974 - trainLoss: 0.46331435441970825\n",
      "cnt: 0 - valLoss: 0.45605918765068054 - trainLoss: 0.46330931782722473\n",
      "cnt: 0 - valLoss: 0.4560541808605194 - trainLoss: 0.4633042514324188\n",
      "cnt: 0 - valLoss: 0.456049382686615 - trainLoss: 0.4632992148399353\n",
      "cnt: 0 - valLoss: 0.45604434609413147 - trainLoss: 0.4632941484451294\n",
      "cnt: 0 - valLoss: 0.45603951811790466 - trainLoss: 0.4632891118526459\n",
      "cnt: 0 - valLoss: 0.4560345411300659 - trainLoss: 0.46328404545783997\n",
      "cnt: 0 - valLoss: 0.4560297131538391 - trainLoss: 0.46327900886535645\n",
      "cnt: 0 - valLoss: 0.4560249447822571 - trainLoss: 0.46327391266822815\n",
      "cnt: 0 - valLoss: 0.45601987838745117 - trainLoss: 0.463268905878067\n",
      "cnt: 0 - valLoss: 0.45601508021354675 - trainLoss: 0.4632638096809387\n",
      "cnt: 0 - valLoss: 0.4560100734233856 - trainLoss: 0.4632588028907776\n",
      "cnt: 0 - valLoss: 0.4560053050518036 - trainLoss: 0.46325376629829407\n",
      "cnt: 0 - valLoss: 0.45600029826164246 - trainLoss: 0.46324869990348816\n",
      "cnt: 0 - valLoss: 0.45599550008773804 - trainLoss: 0.46324366331100464\n",
      "cnt: 0 - valLoss: 0.4559904932975769 - trainLoss: 0.46323859691619873\n",
      "cnt: 0 - valLoss: 0.4559856653213501 - trainLoss: 0.4632335901260376\n",
      "cnt: 0 - valLoss: 0.45598071813583374 - trainLoss: 0.46322858333587646\n",
      "cnt: 0 - valLoss: 0.4559759199619293 - trainLoss: 0.46322354674339294\n",
      "cnt: 0 - valLoss: 0.4559709429740906 - trainLoss: 0.46321845054626465\n",
      "cnt: 0 - valLoss: 0.45596614480018616 - trainLoss: 0.4632134437561035\n",
      "cnt: 0 - valLoss: 0.45596134662628174 - trainLoss: 0.4632083773612976\n",
      "cnt: 0 - valLoss: 0.4559563100337982 - trainLoss: 0.4632033407688141\n",
      "cnt: 0 - valLoss: 0.4559515714645386 - trainLoss: 0.46319833397865295\n",
      "cnt: 0 - valLoss: 0.45594659447669983 - trainLoss: 0.4631933271884918\n",
      "cnt: 0 - valLoss: 0.4559417963027954 - trainLoss: 0.4631882607936859\n",
      "cnt: 0 - valLoss: 0.45593681931495667 - trainLoss: 0.4631832242012024\n",
      "cnt: 0 - valLoss: 0.45593202114105225 - trainLoss: 0.46317821741104126\n",
      "cnt: 0 - valLoss: 0.4559270739555359 - trainLoss: 0.4631732106208801\n",
      "cnt: 0 - valLoss: 0.45592233538627625 - trainLoss: 0.4631681442260742\n",
      "cnt: 0 - valLoss: 0.4559173285961151 - trainLoss: 0.4631631076335907\n",
      "cnt: 0 - valLoss: 0.4559125602245331 - trainLoss: 0.46315813064575195\n",
      "cnt: 0 - valLoss: 0.45590758323669434 - trainLoss: 0.46315309405326843\n",
      "cnt: 0 - valLoss: 0.4559028148651123 - trainLoss: 0.4631480276584625\n",
      "cnt: 0 - valLoss: 0.45589789748191833 - trainLoss: 0.4631430208683014\n",
      "cnt: 0 - valLoss: 0.4558930993080139 - trainLoss: 0.46313801407814026\n",
      "cnt: 0 - valLoss: 0.45588839054107666 - trainLoss: 0.46313297748565674\n",
      "cnt: 0 - valLoss: 0.4558833837509155 - trainLoss: 0.4631279706954956\n",
      "cnt: 0 - valLoss: 0.4558786153793335 - trainLoss: 0.4631229639053345\n",
      "cnt: 0 - valLoss: 0.45587366819381714 - trainLoss: 0.4631179869174957\n",
      "cnt: 0 - valLoss: 0.4558689296245575 - trainLoss: 0.4631129503250122\n",
      "cnt: 0 - valLoss: 0.45586395263671875 - trainLoss: 0.4631078839302063\n",
      "cnt: 0 - valLoss: 0.4558591842651367 - trainLoss: 0.46310290694236755\n",
      "cnt: 0 - valLoss: 0.45585426688194275 - trainLoss: 0.46309787034988403\n",
      "cnt: 0 - valLoss: 0.4558494985103607 - trainLoss: 0.4630928933620453\n",
      "cnt: 0 - valLoss: 0.45584458112716675 - trainLoss: 0.46308785676956177\n",
      "cnt: 0 - valLoss: 0.4558398127555847 - trainLoss: 0.463082879781723\n",
      "cnt: 0 - valLoss: 0.45583489537239075 - trainLoss: 0.4630778431892395\n",
      "cnt: 0 - valLoss: 0.45583009719848633 - trainLoss: 0.46307286620140076\n",
      "cnt: 0 - valLoss: 0.45582544803619385 - trainLoss: 0.46306782960891724\n",
      "cnt: 0 - valLoss: 0.4558205008506775 - trainLoss: 0.4630628526210785\n",
      "cnt: 0 - valLoss: 0.45581573247909546 - trainLoss: 0.46305781602859497\n",
      "cnt: 0 - valLoss: 0.4558108150959015 - trainLoss: 0.4630528390407562\n",
      "cnt: 0 - valLoss: 0.45580607652664185 - trainLoss: 0.4630478024482727\n",
      "cnt: 0 - valLoss: 0.4558010697364807 - trainLoss: 0.46304282546043396\n",
      "cnt: 0 - valLoss: 0.45579639077186584 - trainLoss: 0.4630378186702728\n",
      "cnt: 0 - valLoss: 0.4557914435863495 - trainLoss: 0.4630328118801117\n",
      "cnt: 0 - valLoss: 0.45578670501708984 - trainLoss: 0.46302783489227295\n",
      "cnt: 0 - valLoss: 0.45578181743621826 - trainLoss: 0.46302279829978943\n",
      "cnt: 0 - valLoss: 0.455777108669281 - trainLoss: 0.4630178213119507\n",
      "cnt: 0 - valLoss: 0.45577213168144226 - trainLoss: 0.46301281452178955\n",
      "cnt: 0 - valLoss: 0.455767422914505 - trainLoss: 0.4630078971385956\n",
      "cnt: 0 - valLoss: 0.4557625353336334 - trainLoss: 0.46300286054611206\n",
      "cnt: 0 - valLoss: 0.45575782656669617 - trainLoss: 0.4629978537559509\n",
      "cnt: 0 - valLoss: 0.4557531177997589 - trainLoss: 0.4629928767681122\n",
      "cnt: 0 - valLoss: 0.45574817061424255 - trainLoss: 0.46298786997795105\n",
      "cnt: 0 - valLoss: 0.4557435214519501 - trainLoss: 0.4629828929901123\n",
      "cnt: 0 - valLoss: 0.4557385742664337 - trainLoss: 0.46297788619995117\n",
      "cnt: 0 - valLoss: 0.4557338356971741 - trainLoss: 0.4629729390144348\n",
      "cnt: 0 - valLoss: 0.4557289779186249 - trainLoss: 0.4629679322242737\n",
      "cnt: 0 - valLoss: 0.45572420954704285 - trainLoss: 0.46296295523643494\n",
      "cnt: 0 - valLoss: 0.45571935176849365 - trainLoss: 0.4629579484462738\n",
      "cnt: 0 - valLoss: 0.455714613199234 - trainLoss: 0.46295300126075745\n",
      "cnt: 0 - valLoss: 0.4557097256183624 - trainLoss: 0.4629480242729187\n",
      "cnt: 0 - valLoss: 0.45570501685142517 - trainLoss: 0.46294304728507996\n",
      "cnt: 0 - valLoss: 0.4557001292705536 - trainLoss: 0.4629380404949188\n",
      "cnt: 0 - valLoss: 0.45569542050361633 - trainLoss: 0.46293309330940247\n",
      "cnt: 0 - valLoss: 0.45569074153900146 - trainLoss: 0.46292808651924133\n",
      "cnt: 0 - valLoss: 0.4556858241558075 - trainLoss: 0.46292316913604736\n",
      "cnt: 0 - valLoss: 0.4556812047958374 - trainLoss: 0.46291813254356384\n",
      "cnt: 0 - valLoss: 0.4556763172149658 - trainLoss: 0.4629132151603699\n",
      "cnt: 0 - valLoss: 0.45567163825035095 - trainLoss: 0.46290817856788635\n",
      "cnt: 0 - valLoss: 0.4556666910648346 - trainLoss: 0.4629032611846924\n",
      "cnt: 0 - valLoss: 0.4556620717048645 - trainLoss: 0.46289825439453125\n",
      "cnt: 0 - valLoss: 0.4556571841239929 - trainLoss: 0.4628933072090149\n",
      "cnt: 0 - valLoss: 0.45565247535705566 - trainLoss: 0.46288830041885376\n",
      "cnt: 0 - valLoss: 0.4556475877761841 - trainLoss: 0.4628833532333374\n",
      "cnt: 0 - valLoss: 0.455642968416214 - trainLoss: 0.46287837624549866\n",
      "cnt: 0 - valLoss: 0.4556380808353424 - trainLoss: 0.4628734588623047\n",
      "cnt: 0 - valLoss: 0.45563334226608276 - trainLoss: 0.46286848187446594\n",
      "cnt: 0 - valLoss: 0.45562851428985596 - trainLoss: 0.4628634750843048\n",
      "cnt: 0 - valLoss: 0.4556238055229187 - trainLoss: 0.4628586173057556\n",
      "cnt: 0 - valLoss: 0.45561912655830383 - trainLoss: 0.4628536105155945\n",
      "cnt: 0 - valLoss: 0.45561420917510986 - trainLoss: 0.46284863352775574\n",
      "cnt: 0 - valLoss: 0.455609530210495 - trainLoss: 0.46284371614456177\n",
      "cnt: 0 - valLoss: 0.4556046426296234 - trainLoss: 0.462838739156723\n",
      "cnt: 0 - valLoss: 0.45559996366500854 - trainLoss: 0.46283379197120667\n",
      "cnt: 0 - valLoss: 0.4555950462818146 - trainLoss: 0.4628288149833679\n",
      "cnt: 0 - valLoss: 0.4555903971195221 - trainLoss: 0.46282389760017395\n",
      "cnt: 0 - valLoss: 0.4555855095386505 - trainLoss: 0.4628189206123352\n",
      "cnt: 0 - valLoss: 0.45558080077171326 - trainLoss: 0.46281400322914124\n",
      "cnt: 0 - valLoss: 0.45557597279548645 - trainLoss: 0.4628090560436249\n",
      "cnt: 0 - valLoss: 0.4555712938308716 - trainLoss: 0.46280407905578613\n",
      "cnt: 0 - valLoss: 0.45556640625 - trainLoss: 0.46279916167259216\n",
      "cnt: 0 - valLoss: 0.45556169748306274 - trainLoss: 0.4627942442893982\n",
      "cnt: 0 - valLoss: 0.45555704832077026 - trainLoss: 0.46278926730155945\n",
      "cnt: 0 - valLoss: 0.45555219054222107 - trainLoss: 0.4627843499183655\n",
      "cnt: 0 - valLoss: 0.4555475413799286 - trainLoss: 0.4627794325351715\n",
      "cnt: 0 - valLoss: 0.455542653799057 - trainLoss: 0.46277445554733276\n",
      "cnt: 0 - valLoss: 0.45553797483444214 - trainLoss: 0.4627695083618164\n",
      "cnt: 0 - valLoss: 0.45553311705589294 - trainLoss: 0.46276456117630005\n",
      "cnt: 0 - valLoss: 0.4555284380912781 - trainLoss: 0.4627596437931061\n",
      "cnt: 0 - valLoss: 0.4555235803127289 - trainLoss: 0.4627546966075897\n",
      "cnt: 0 - valLoss: 0.455518901348114 - trainLoss: 0.46274977922439575\n",
      "cnt: 0 - valLoss: 0.4555141031742096 - trainLoss: 0.4627448320388794\n",
      "cnt: 0 - valLoss: 0.45550939440727234 - trainLoss: 0.46273988485336304\n",
      "cnt: 0 - valLoss: 0.4555045962333679 - trainLoss: 0.46273499727249146\n",
      "cnt: 0 - valLoss: 0.45549988746643066 - trainLoss: 0.4627300202846527\n",
      "cnt: 0 - valLoss: 0.45549532771110535 - trainLoss: 0.4627251625061035\n",
      "cnt: 0 - valLoss: 0.45549046993255615 - trainLoss: 0.46272018551826477\n",
      "cnt: 0 - valLoss: 0.45548582077026367 - trainLoss: 0.4627152383327484\n",
      "cnt: 0 - valLoss: 0.4554809331893921 - trainLoss: 0.46271035075187683\n",
      "cnt: 0 - valLoss: 0.455476313829422 - trainLoss: 0.46270543336868286\n",
      "cnt: 0 - valLoss: 0.4554714560508728 - trainLoss: 0.4627004861831665\n",
      "cnt: 0 - valLoss: 0.4554668366909027 - trainLoss: 0.46269556879997253\n",
      "cnt: 0 - valLoss: 0.4554620087146759 - trainLoss: 0.46269065141677856\n",
      "cnt: 0 - valLoss: 0.4554573595523834 - trainLoss: 0.4626857340335846\n",
      "cnt: 0 - valLoss: 0.4554525315761566 - trainLoss: 0.4626808166503906\n",
      "cnt: 0 - valLoss: 0.45544785261154175 - trainLoss: 0.46267589926719666\n",
      "cnt: 0 - valLoss: 0.45544305443763733 - trainLoss: 0.4626709520816803\n",
      "cnt: 0 - valLoss: 0.45543843507766724 - trainLoss: 0.4626660943031311\n",
      "cnt: 0 - valLoss: 0.45543360710144043 - trainLoss: 0.46266111731529236\n",
      "cnt: 0 - valLoss: 0.45542898774147034 - trainLoss: 0.4626562297344208\n",
      "cnt: 0 - valLoss: 0.45542436838150024 - trainLoss: 0.4626512825489044\n",
      "cnt: 0 - valLoss: 0.45541951060295105 - trainLoss: 0.46264639496803284\n",
      "cnt: 0 - valLoss: 0.45541492104530334 - trainLoss: 0.46264147758483887\n",
      "cnt: 0 - valLoss: 0.45541009306907654 - trainLoss: 0.4626365900039673\n",
      "cnt: 0 - valLoss: 0.45540547370910645 - trainLoss: 0.4626316726207733\n",
      "cnt: 0 - valLoss: 0.45540064573287964 - trainLoss: 0.46262675523757935\n",
      "cnt: 0 - valLoss: 0.4553960859775543 - trainLoss: 0.4626218378543854\n",
      "cnt: 0 - valLoss: 0.4553912281990051 - trainLoss: 0.4626169502735138\n",
      "cnt: 0 - valLoss: 0.45538660883903503 - trainLoss: 0.4626120626926422\n",
      "cnt: 0 - valLoss: 0.4553818106651306 - trainLoss: 0.46260711550712585\n",
      "cnt: 0 - valLoss: 0.45537716150283813 - trainLoss: 0.4626022279262543\n",
      "cnt: 0 - valLoss: 0.4553724229335785 - trainLoss: 0.4625973403453827\n",
      "cnt: 0 - valLoss: 0.455367773771286 - trainLoss: 0.4625924229621887\n",
      "cnt: 0 - valLoss: 0.4553632438182831 - trainLoss: 0.46258750557899475\n",
      "cnt: 0 - valLoss: 0.4553583562374115 - trainLoss: 0.46258267760276794\n",
      "cnt: 0 - valLoss: 0.45535382628440857 - trainLoss: 0.4625777006149292\n",
      "cnt: 0 - valLoss: 0.45534899830818176 - trainLoss: 0.4625728726387024\n",
      "cnt: 0 - valLoss: 0.45534437894821167 - trainLoss: 0.4625679552555084\n",
      "cnt: 0 - valLoss: 0.45533958077430725 - trainLoss: 0.46256300806999207\n",
      "cnt: 0 - valLoss: 0.45533496141433716 - trainLoss: 0.46255815029144287\n",
      "cnt: 0 - valLoss: 0.45533016324043274 - trainLoss: 0.4625532627105713\n",
      "cnt: 0 - valLoss: 0.45532551407814026 - trainLoss: 0.4625483453273773\n",
      "cnt: 0 - valLoss: 0.45532071590423584 - trainLoss: 0.4625435173511505\n",
      "cnt: 0 - valLoss: 0.45531609654426575 - trainLoss: 0.46253862977027893\n",
      "cnt: 0 - valLoss: 0.4553113281726837 - trainLoss: 0.4625336527824402\n",
      "cnt: 0 - valLoss: 0.45530664920806885 - trainLoss: 0.4625288248062134\n",
      "cnt: 0 - valLoss: 0.4553021192550659 - trainLoss: 0.4625239372253418\n",
      "cnt: 0 - valLoss: 0.4552972912788391 - trainLoss: 0.4625190496444702\n",
      "cnt: 0 - valLoss: 0.4552927017211914 - trainLoss: 0.46251413226127625\n",
      "cnt: 0 - valLoss: 0.455287903547287 - trainLoss: 0.46250930428504944\n",
      "cnt: 0 - valLoss: 0.4552833139896393 - trainLoss: 0.46250438690185547\n",
      "cnt: 0 - valLoss: 0.45527851581573486 - trainLoss: 0.46249955892562866\n",
      "cnt: 0 - valLoss: 0.45527392625808716 - trainLoss: 0.4624946117401123\n",
      "cnt: 0 - valLoss: 0.45526912808418274 - trainLoss: 0.4624897837638855\n",
      "cnt: 0 - valLoss: 0.45526447892189026 - trainLoss: 0.4624848961830139\n",
      "cnt: 0 - valLoss: 0.455259770154953 - trainLoss: 0.4624800682067871\n",
      "cnt: 0 - valLoss: 0.4552551507949829 - trainLoss: 0.46247512102127075\n",
      "cnt: 0 - valLoss: 0.4552503824234009 - trainLoss: 0.46247023344039917\n",
      "cnt: 0 - valLoss: 0.4552457630634308 - trainLoss: 0.46246543526649475\n",
      "cnt: 0 - valLoss: 0.45524120330810547 - trainLoss: 0.462460458278656\n",
      "cnt: 0 - valLoss: 0.45523637533187866 - trainLoss: 0.4624556601047516\n",
      "cnt: 0 - valLoss: 0.45523181557655334 - trainLoss: 0.4624508023262024\n",
      "cnt: 0 - valLoss: 0.4552271068096161 - trainLoss: 0.4624459147453308\n",
      "cnt: 0 - valLoss: 0.455222487449646 - trainLoss: 0.462441086769104\n",
      "cnt: 0 - valLoss: 0.4552176892757416 - trainLoss: 0.46243616938591003\n",
      "cnt: 0 - valLoss: 0.45521312952041626 - trainLoss: 0.4624313712120056\n",
      "cnt: 0 - valLoss: 0.4552083909511566 - trainLoss: 0.46242648363113403\n",
      "cnt: 0 - valLoss: 0.4552038013935089 - trainLoss: 0.46242159605026245\n",
      "cnt: 0 - valLoss: 0.4551990330219269 - trainLoss: 0.46241673827171326\n",
      "cnt: 0 - valLoss: 0.4551944434642792 - trainLoss: 0.46241188049316406\n",
      "cnt: 0 - valLoss: 0.45518970489501953 - trainLoss: 0.4624069631099701\n",
      "cnt: 0 - valLoss: 0.4551851451396942 - trainLoss: 0.4624021649360657\n",
      "cnt: 0 - valLoss: 0.4551806151866913 - trainLoss: 0.4623973071575165\n",
      "cnt: 0 - valLoss: 0.45517581701278687 - trainLoss: 0.4623924195766449\n",
      "cnt: 0 - valLoss: 0.45517125725746155 - trainLoss: 0.4623875319957733\n",
      "cnt: 0 - valLoss: 0.4551665186882019 - trainLoss: 0.4623827338218689\n",
      "cnt: 0 - valLoss: 0.4551619589328766 - trainLoss: 0.4623778760433197\n",
      "cnt: 0 - valLoss: 0.45515719056129456 - trainLoss: 0.4623730778694153\n",
      "cnt: 0 - valLoss: 0.4551526606082916 - trainLoss: 0.4623681902885437\n",
      "cnt: 0 - valLoss: 0.4551478922367096 - trainLoss: 0.4623633027076721\n",
      "cnt: 0 - valLoss: 0.45514336228370667 - trainLoss: 0.46235841512680054\n",
      "cnt: 0 - valLoss: 0.45513859391212463 - trainLoss: 0.4623536467552185\n",
      "cnt: 0 - valLoss: 0.4551340937614441 - trainLoss: 0.4623487591743469\n",
      "cnt: 0 - valLoss: 0.4551292955875397 - trainLoss: 0.4623439311981201\n",
      "cnt: 0 - valLoss: 0.45512476563453674 - trainLoss: 0.4623390734195709\n",
      "cnt: 0 - valLoss: 0.4551200866699219 - trainLoss: 0.46233421564102173\n",
      "cnt: 0 - valLoss: 0.45511549711227417 - trainLoss: 0.4623294174671173\n",
      "cnt: 0 - valLoss: 0.45511096715927124 - trainLoss: 0.4623245298862457\n",
      "cnt: 0 - valLoss: 0.4551062285900116 - trainLoss: 0.4623197019100189\n",
      "cnt: 0 - valLoss: 0.45510169863700867 - trainLoss: 0.4623148441314697\n",
      "cnt: 0 - valLoss: 0.4550969898700714 - trainLoss: 0.4623100161552429\n",
      "cnt: 0 - valLoss: 0.4550924301147461 - trainLoss: 0.4623051881790161\n",
      "cnt: 0 - valLoss: 0.45508772134780884 - trainLoss: 0.4623003900051117\n",
      "cnt: 0 - valLoss: 0.4550832211971283 - trainLoss: 0.4622955024242401\n",
      "cnt: 0 - valLoss: 0.45507848262786865 - trainLoss: 0.4622907042503357\n",
      "cnt: 0 - valLoss: 0.45507392287254333 - trainLoss: 0.4622858762741089\n",
      "cnt: 0 - valLoss: 0.4550692141056061 - trainLoss: 0.4622809886932373\n",
      "cnt: 0 - valLoss: 0.45506465435028076 - trainLoss: 0.4622761607170105\n",
      "cnt: 0 - valLoss: 0.45506003499031067 - trainLoss: 0.4622713625431061\n",
      "cnt: 0 - valLoss: 0.45505544543266296 - trainLoss: 0.4622665047645569\n",
      "cnt: 0 - valLoss: 0.4550510048866272 - trainLoss: 0.4622616469860077\n",
      "cnt: 0 - valLoss: 0.45504623651504517 - trainLoss: 0.46225690841674805\n",
      "cnt: 0 - valLoss: 0.4550417363643646 - trainLoss: 0.46225202083587646\n",
      "cnt: 0 - valLoss: 0.45503702759742737 - trainLoss: 0.46224722266197205\n",
      "cnt: 0 - valLoss: 0.4550325274467468 - trainLoss: 0.46224236488342285\n",
      "cnt: 0 - valLoss: 0.45502781867980957 - trainLoss: 0.46223756670951843\n",
      "cnt: 0 - valLoss: 0.4550233483314514 - trainLoss: 0.462232768535614\n",
      "cnt: 0 - valLoss: 0.4550185799598694 - trainLoss: 0.46222788095474243\n",
      "cnt: 0 - valLoss: 0.4550141394138336 - trainLoss: 0.462223082780838\n",
      "cnt: 0 - valLoss: 0.45500943064689636 - trainLoss: 0.4622182846069336\n",
      "cnt: 0 - valLoss: 0.45500487089157104 - trainLoss: 0.4622134268283844\n",
      "cnt: 0 - valLoss: 0.4550001919269562 - trainLoss: 0.46220862865448\n",
      "cnt: 0 - valLoss: 0.45499569177627563 - trainLoss: 0.46220383048057556\n",
      "cnt: 0 - valLoss: 0.4549911916255951 - trainLoss: 0.462198942899704\n",
      "cnt: 0 - valLoss: 0.4549865424633026 - trainLoss: 0.46219414472579956\n",
      "cnt: 0 - valLoss: 0.45498204231262207 - trainLoss: 0.46218931674957275\n",
      "cnt: 0 - valLoss: 0.4549773633480072 - trainLoss: 0.4621845781803131\n",
      "cnt: 0 - valLoss: 0.45497286319732666 - trainLoss: 0.46217969059944153\n",
      "cnt: 0 - valLoss: 0.4549681842327118 - trainLoss: 0.4621749520301819\n",
      "cnt: 0 - valLoss: 0.45496368408203125 - trainLoss: 0.4621700942516327\n",
      "cnt: 0 - valLoss: 0.4549590051174164 - trainLoss: 0.46216529607772827\n",
      "cnt: 0 - valLoss: 0.45495450496673584 - trainLoss: 0.46216052770614624\n",
      "cnt: 0 - valLoss: 0.45494988560676575 - trainLoss: 0.46215566992759705\n",
      "cnt: 0 - valLoss: 0.4549453854560852 - trainLoss: 0.4621508717536926\n",
      "cnt: 0 - valLoss: 0.45494070649147034 - trainLoss: 0.4621460437774658\n",
      "cnt: 0 - valLoss: 0.4549362063407898 - trainLoss: 0.4621412456035614\n",
      "cnt: 0 - valLoss: 0.4549317955970764 - trainLoss: 0.462136447429657\n",
      "cnt: 0 - valLoss: 0.45492708683013916 - trainLoss: 0.46213167905807495\n",
      "cnt: 0 - valLoss: 0.454922616481781 - trainLoss: 0.46212682127952576\n",
      "cnt: 0 - valLoss: 0.45491790771484375 - trainLoss: 0.4621220827102661\n",
      "cnt: 0 - valLoss: 0.45491349697113037 - trainLoss: 0.4621172547340393\n",
      "cnt: 0 - valLoss: 0.4549088180065155 - trainLoss: 0.4621124565601349\n",
      "cnt: 0 - valLoss: 0.45490437746047974 - trainLoss: 0.46210765838623047\n",
      "cnt: 0 - valLoss: 0.45489972829818726 - trainLoss: 0.46210283041000366\n",
      "cnt: 0 - valLoss: 0.4548952281475067 - trainLoss: 0.46209806203842163\n",
      "cnt: 0 - valLoss: 0.4548906087875366 - trainLoss: 0.4620932340621948\n",
      "cnt: 0 - valLoss: 0.45488613843917847 - trainLoss: 0.4620884656906128\n",
      "cnt: 0 - valLoss: 0.454881489276886 - trainLoss: 0.462083637714386\n",
      "cnt: 0 - valLoss: 0.45487701892852783 - trainLoss: 0.46207889914512634\n",
      "cnt: 0 - valLoss: 0.45487260818481445 - trainLoss: 0.46207404136657715\n",
      "cnt: 0 - valLoss: 0.4548679292201996 - trainLoss: 0.4620693027973175\n",
      "cnt: 0 - valLoss: 0.45486345887184143 - trainLoss: 0.4620644748210907\n",
      "cnt: 0 - valLoss: 0.45485883951187134 - trainLoss: 0.46205976605415344\n",
      "cnt: 0 - valLoss: 0.45485439896583557 - trainLoss: 0.46205490827560425\n",
      "cnt: 0 - valLoss: 0.4548497498035431 - trainLoss: 0.4620501697063446\n",
      "cnt: 0 - valLoss: 0.4548453092575073 - trainLoss: 0.4620453715324402\n",
      "cnt: 0 - valLoss: 0.45484066009521484 - trainLoss: 0.46204057335853577\n",
      "cnt: 0 - valLoss: 0.4548362195491791 - trainLoss: 0.46203580498695374\n",
      "cnt: 0 - valLoss: 0.4548315703868866 - trainLoss: 0.4620310068130493\n",
      "cnt: 0 - valLoss: 0.4548271596431732 - trainLoss: 0.4620262086391449\n",
      "cnt: 0 - valLoss: 0.4548225402832031 - trainLoss: 0.46202144026756287\n",
      "cnt: 0 - valLoss: 0.45481806993484497 - trainLoss: 0.4620167016983032\n",
      "cnt: 0 - valLoss: 0.45481371879577637 - trainLoss: 0.4620119035243988\n",
      "cnt: 0 - valLoss: 0.4548090398311615 - trainLoss: 0.4620071053504944\n",
      "cnt: 0 - valLoss: 0.4548046290874481 - trainLoss: 0.46200230717658997\n",
      "cnt: 0 - valLoss: 0.45479995012283325 - trainLoss: 0.46199753880500793\n",
      "cnt: 0 - valLoss: 0.45479556918144226 - trainLoss: 0.4619928300380707\n",
      "cnt: 0 - valLoss: 0.45479094982147217 - trainLoss: 0.46198803186416626\n",
      "cnt: 0 - valLoss: 0.454786479473114 - trainLoss: 0.46198326349258423\n",
      "cnt: 0 - valLoss: 0.4547819197177887 - trainLoss: 0.4619784653186798\n",
      "cnt: 0 - valLoss: 0.45477747917175293 - trainLoss: 0.46197372674942017\n",
      "cnt: 0 - valLoss: 0.45477285981178284 - trainLoss: 0.46196892857551575\n",
      "cnt: 0 - valLoss: 0.45476847887039185 - trainLoss: 0.4619641900062561\n",
      "cnt: 0 - valLoss: 0.45476385951042175 - trainLoss: 0.4619593322277069\n",
      "cnt: 0 - valLoss: 0.454759418964386 - trainLoss: 0.46195465326309204\n",
      "cnt: 0 - valLoss: 0.454755038022995 - trainLoss: 0.4619498550891876\n",
      "cnt: 0 - valLoss: 0.4547503590583801 - trainLoss: 0.4619450867176056\n",
      "cnt: 0 - valLoss: 0.4547460377216339 - trainLoss: 0.46194034814834595\n",
      "cnt: 0 - valLoss: 0.45474138855934143 - trainLoss: 0.4619355797767639\n",
      "cnt: 0 - valLoss: 0.4547370374202728 - trainLoss: 0.4619308412075043\n",
      "cnt: 0 - valLoss: 0.45473238825798035 - trainLoss: 0.46192604303359985\n",
      "cnt: 0 - valLoss: 0.45472803711891174 - trainLoss: 0.4619213342666626\n",
      "cnt: 0 - valLoss: 0.45472338795661926 - trainLoss: 0.4619165360927582\n",
      "cnt: 0 - valLoss: 0.45471903681755066 - trainLoss: 0.46191173791885376\n",
      "cnt: 0 - valLoss: 0.4547143876552582 - trainLoss: 0.4619070291519165\n",
      "cnt: 0 - valLoss: 0.4547100365161896 - trainLoss: 0.4619022607803345\n",
      "cnt: 0 - valLoss: 0.45470544695854187 - trainLoss: 0.46189746260643005\n",
      "cnt: 0 - valLoss: 0.4547010660171509 - trainLoss: 0.4618927538394928\n",
      "cnt: 0 - valLoss: 0.4546966850757599 - trainLoss: 0.4618879556655884\n",
      "cnt: 0 - valLoss: 0.45469212532043457 - trainLoss: 0.4618832468986511\n",
      "cnt: 0 - valLoss: 0.4546876847743988 - trainLoss: 0.4618785083293915\n",
      "cnt: 0 - valLoss: 0.4546831250190735 - trainLoss: 0.46187373995780945\n",
      "cnt: 0 - valLoss: 0.4546787440776825 - trainLoss: 0.4618690013885498\n",
      "cnt: 0 - valLoss: 0.4546741545200348 - trainLoss: 0.4618642330169678\n",
      "cnt: 0 - valLoss: 0.4546697437763214 - trainLoss: 0.4618595242500305\n",
      "cnt: 0 - valLoss: 0.4546651542186737 - trainLoss: 0.4618547260761261\n",
      "cnt: 0 - valLoss: 0.4546608328819275 - trainLoss: 0.46184998750686646\n",
      "cnt: 0 - valLoss: 0.4546562731266022 - trainLoss: 0.4618452489376068\n",
      "cnt: 0 - valLoss: 0.4546518623828888 - trainLoss: 0.46184054017066956\n",
      "cnt: 0 - valLoss: 0.4546472728252411 - trainLoss: 0.46183574199676514\n",
      "cnt: 0 - valLoss: 0.4546428918838501 - trainLoss: 0.4618310332298279\n",
      "cnt: 0 - valLoss: 0.4546385705471039 - trainLoss: 0.46182626485824585\n",
      "cnt: 0 - valLoss: 0.4546339809894562 - trainLoss: 0.4618215560913086\n",
      "cnt: 0 - valLoss: 0.45462965965270996 - trainLoss: 0.46181681752204895\n",
      "cnt: 0 - valLoss: 0.45462507009506226 - trainLoss: 0.4618121087551117\n",
      "cnt: 0 - valLoss: 0.45462071895599365 - trainLoss: 0.4618073105812073\n",
      "cnt: 0 - valLoss: 0.45461615920066833 - trainLoss: 0.46180260181427\n",
      "cnt: 0 - valLoss: 0.45461177825927734 - trainLoss: 0.4617978632450104\n",
      "cnt: 0 - valLoss: 0.4546072483062744 - trainLoss: 0.46179309487342834\n",
      "cnt: 0 - valLoss: 0.4546028673648834 - trainLoss: 0.4617883861064911\n",
      "cnt: 0 - valLoss: 0.4545983374118805 - trainLoss: 0.46178367733955383\n",
      "cnt: 0 - valLoss: 0.4545939564704895 - trainLoss: 0.4617789089679718\n",
      "cnt: 0 - valLoss: 0.4545894265174866 - trainLoss: 0.46177420020103455\n",
      "cnt: 0 - valLoss: 0.4545850455760956 - trainLoss: 0.4617694914340973\n",
      "cnt: 0 - valLoss: 0.45458075404167175 - trainLoss: 0.46176469326019287\n",
      "cnt: 0 - valLoss: 0.45457616448402405 - trainLoss: 0.4617599844932556\n",
      "cnt: 0 - valLoss: 0.45457184314727783 - trainLoss: 0.46175527572631836\n",
      "cnt: 0 - valLoss: 0.4545673131942749 - trainLoss: 0.4617505669593811\n",
      "cnt: 0 - valLoss: 0.4545629620552063 - trainLoss: 0.46174585819244385\n",
      "cnt: 0 - valLoss: 0.45455843210220337 - trainLoss: 0.4617410898208618\n",
      "cnt: 0 - valLoss: 0.45455411076545715 - trainLoss: 0.46173638105392456\n",
      "cnt: 0 - valLoss: 0.45454955101013184 - trainLoss: 0.4617316424846649\n",
      "cnt: 0 - valLoss: 0.45454517006874084 - trainLoss: 0.46172693371772766\n",
      "cnt: 0 - valLoss: 0.4545406699180603 - trainLoss: 0.461722195148468\n",
      "cnt: 0 - valLoss: 0.4545363485813141 - trainLoss: 0.46171751618385315\n",
      "cnt: 0 - valLoss: 0.45453184843063354 - trainLoss: 0.4617127776145935\n",
      "cnt: 0 - valLoss: 0.45452743768692017 - trainLoss: 0.46170809864997864\n",
      "cnt: 0 - valLoss: 0.45452314615249634 - trainLoss: 0.461703360080719\n",
      "cnt: 0 - valLoss: 0.4545186161994934 - trainLoss: 0.46169865131378174\n",
      "cnt: 0 - valLoss: 0.4545143246650696 - trainLoss: 0.4616938829421997\n",
      "cnt: 0 - valLoss: 0.45450976490974426 - trainLoss: 0.46168917417526245\n",
      "cnt: 0 - valLoss: 0.4545055031776428 - trainLoss: 0.4616844654083252\n",
      "cnt: 0 - valLoss: 0.4545009136199951 - trainLoss: 0.46167975664138794\n",
      "cnt: 0 - valLoss: 0.4544966220855713 - trainLoss: 0.4616750478744507\n",
      "cnt: 0 - valLoss: 0.45449212193489075 - trainLoss: 0.4616703391075134\n",
      "cnt: 0 - valLoss: 0.45448780059814453 - trainLoss: 0.46166563034057617\n",
      "cnt: 0 - valLoss: 0.454483300447464 - trainLoss: 0.4616609215736389\n",
      "cnt: 0 - valLoss: 0.4544789493083954 - trainLoss: 0.46165621280670166\n",
      "cnt: 0 - valLoss: 0.45447444915771484 - trainLoss: 0.4616515040397644\n",
      "cnt: 0 - valLoss: 0.45447012782096863 - trainLoss: 0.4616468548774719\n",
      "cnt: 0 - valLoss: 0.4544658958911896 - trainLoss: 0.4616420865058899\n",
      "cnt: 0 - valLoss: 0.45446133613586426 - trainLoss: 0.4616374373435974\n",
      "cnt: 0 - valLoss: 0.45445704460144043 - trainLoss: 0.46163269877433777\n",
      "cnt: 0 - valLoss: 0.4544525444507599 - trainLoss: 0.4616280198097229\n",
      "cnt: 0 - valLoss: 0.45444822311401367 - trainLoss: 0.46162331104278564\n",
      "cnt: 0 - valLoss: 0.4544437825679779 - trainLoss: 0.4616186320781708\n",
      "cnt: 0 - valLoss: 0.4544394910335541 - trainLoss: 0.4616139829158783\n",
      "cnt: 0 - valLoss: 0.45443499088287354 - trainLoss: 0.46160921454429626\n",
      "cnt: 0 - valLoss: 0.4544306695461273 - trainLoss: 0.4616045355796814\n",
      "cnt: 0 - valLoss: 0.45442619919776917 - trainLoss: 0.46159985661506653\n",
      "cnt: 0 - valLoss: 0.45442190766334534 - trainLoss: 0.4615951478481293\n",
      "cnt: 0 - valLoss: 0.45441746711730957 - trainLoss: 0.4615904688835144\n",
      "cnt: 0 - valLoss: 0.45441317558288574 - trainLoss: 0.46158576011657715\n",
      "cnt: 0 - valLoss: 0.4544088840484619 - trainLoss: 0.4615810513496399\n",
      "cnt: 0 - valLoss: 0.45440438389778137 - trainLoss: 0.4615764319896698\n",
      "cnt: 0 - valLoss: 0.4544001519680023 - trainLoss: 0.46157172322273254\n",
      "cnt: 0 - valLoss: 0.4543956518173218 - trainLoss: 0.4615670144557953\n",
      "cnt: 0 - valLoss: 0.45439136028289795 - trainLoss: 0.46156230568885803\n",
      "cnt: 0 - valLoss: 0.4543868899345398 - trainLoss: 0.46155765652656555\n",
      "cnt: 0 - valLoss: 0.4543825685977936 - trainLoss: 0.4615529775619507\n",
      "cnt: 0 - valLoss: 0.4543780982494354 - trainLoss: 0.4615482687950134\n",
      "cnt: 0 - valLoss: 0.45437386631965637 - trainLoss: 0.46154361963272095\n",
      "cnt: 0 - valLoss: 0.4543693959712982 - trainLoss: 0.4615389108657837\n",
      "cnt: 0 - valLoss: 0.4543651342391968 - trainLoss: 0.4615342319011688\n",
      "cnt: 0 - valLoss: 0.45436087250709534 - trainLoss: 0.46152958273887634\n",
      "cnt: 0 - valLoss: 0.45435643196105957 - trainLoss: 0.4615249037742615\n",
      "cnt: 0 - valLoss: 0.45435217022895813 - trainLoss: 0.4615201950073242\n",
      "cnt: 0 - valLoss: 0.4543476700782776 - trainLoss: 0.4615155756473541\n",
      "cnt: 0 - valLoss: 0.45434343814849854 - trainLoss: 0.46151086688041687\n",
      "cnt: 0 - valLoss: 0.4543389678001404 - trainLoss: 0.4615062177181244\n",
      "cnt: 0 - valLoss: 0.45433473587036133 - trainLoss: 0.4615015387535095\n",
      "cnt: 0 - valLoss: 0.4543302357196808 - trainLoss: 0.46149682998657227\n",
      "cnt: 0 - valLoss: 0.45432597398757935 - trainLoss: 0.4614922106266022\n",
      "cnt: 0 - valLoss: 0.45432156324386597 - trainLoss: 0.4614875018596649\n",
      "cnt: 0 - valLoss: 0.4543173313140869 - trainLoss: 0.46148282289505005\n",
      "cnt: 0 - valLoss: 0.45431286096572876 - trainLoss: 0.46147817373275757\n",
      "cnt: 0 - valLoss: 0.4543085992336273 - trainLoss: 0.4614734947681427\n",
      "cnt: 0 - valLoss: 0.45430439710617065 - trainLoss: 0.4614688456058502\n",
      "cnt: 0 - valLoss: 0.4542999267578125 - trainLoss: 0.4614642262458801\n",
      "cnt: 0 - valLoss: 0.45429572463035583 - trainLoss: 0.46145951747894287\n",
      "cnt: 0 - valLoss: 0.4542912542819977 - trainLoss: 0.4614548981189728\n",
      "cnt: 0 - valLoss: 0.45428702235221863 - trainLoss: 0.46145012974739075\n",
      "cnt: 0 - valLoss: 0.45428258180618286 - trainLoss: 0.46144556999206543\n",
      "cnt: 0 - valLoss: 0.4542783200740814 - trainLoss: 0.4614408612251282\n",
      "cnt: 0 - valLoss: 0.45427390933036804 - trainLoss: 0.4614362120628357\n",
      "cnt: 0 - valLoss: 0.454269677400589 - trainLoss: 0.4614315629005432\n",
      "cnt: 0 - valLoss: 0.4542652368545532 - trainLoss: 0.46142688393592834\n",
      "cnt: 0 - valLoss: 0.45426100492477417 - trainLoss: 0.46142223477363586\n",
      "cnt: 0 - valLoss: 0.4542565643787384 - trainLoss: 0.461417555809021\n",
      "cnt: 0 - valLoss: 0.45425236225128174 - trainLoss: 0.4614129364490509\n",
      "cnt: 0 - valLoss: 0.4542481601238251 - trainLoss: 0.46140822768211365\n",
      "cnt: 0 - valLoss: 0.4542436897754669 - trainLoss: 0.46140360832214355\n",
      "cnt: 0 - valLoss: 0.45423948764801025 - trainLoss: 0.4613989591598511\n",
      "cnt: 0 - valLoss: 0.4542350471019745 - trainLoss: 0.4613942801952362\n",
      "cnt: 0 - valLoss: 0.4542308449745178 - trainLoss: 0.4613896608352661\n",
      "cnt: 0 - valLoss: 0.45422637462615967 - trainLoss: 0.46138498187065125\n",
      "cnt: 0 - valLoss: 0.454222172498703 - trainLoss: 0.46138036251068115\n",
      "cnt: 0 - valLoss: 0.454217791557312 - trainLoss: 0.46137571334838867\n",
      "cnt: 0 - valLoss: 0.45421352982521057 - trainLoss: 0.4613710343837738\n",
      "cnt: 0 - valLoss: 0.4542091488838196 - trainLoss: 0.4613664150238037\n",
      "cnt: 0 - valLoss: 0.4542049467563629 - trainLoss: 0.46136173605918884\n",
      "cnt: 0 - valLoss: 0.45420053601264954 - trainLoss: 0.46135708689689636\n",
      "cnt: 0 - valLoss: 0.4541962742805481 - trainLoss: 0.46135249733924866\n",
      "cnt: 0 - valLoss: 0.4541921317577362 - trainLoss: 0.4613477885723114\n",
      "cnt: 0 - valLoss: 0.4541877210140228 - trainLoss: 0.4613432288169861\n",
      "cnt: 0 - valLoss: 0.45418351888656616 - trainLoss: 0.4613385498523712\n",
      "cnt: 0 - valLoss: 0.4541791081428528 - trainLoss: 0.4613339304924011\n",
      "cnt: 0 - valLoss: 0.45417484641075134 - trainLoss: 0.46132925152778625\n",
      "cnt: 0 - valLoss: 0.45417049527168274 - trainLoss: 0.4613246023654938\n",
      "cnt: 0 - valLoss: 0.4541662931442261 - trainLoss: 0.4613199830055237\n",
      "cnt: 0 - valLoss: 0.4541618824005127 - trainLoss: 0.4613153636455536\n",
      "cnt: 0 - valLoss: 0.45415768027305603 - trainLoss: 0.4613107442855835\n",
      "cnt: 0 - valLoss: 0.45415329933166504 - trainLoss: 0.46130606532096863\n",
      "cnt: 0 - valLoss: 0.4541490972042084 - trainLoss: 0.46130144596099854\n",
      "cnt: 0 - valLoss: 0.4541449248790741 - trainLoss: 0.46129676699638367\n",
      "cnt: 0 - valLoss: 0.4541405141353607 - trainLoss: 0.46129220724105835\n",
      "cnt: 0 - valLoss: 0.45413637161254883 - trainLoss: 0.4612875282764435\n",
      "cnt: 0 - valLoss: 0.4541319012641907 - trainLoss: 0.4612829089164734\n",
      "cnt: 0 - valLoss: 0.45412778854370117 - trainLoss: 0.4612782895565033\n",
      "cnt: 0 - valLoss: 0.4541233479976654 - trainLoss: 0.4612736701965332\n",
      "cnt: 0 - valLoss: 0.45411917567253113 - trainLoss: 0.4612690508365631\n",
      "cnt: 0 - valLoss: 0.45411479473114014 - trainLoss: 0.46126437187194824\n",
      "cnt: 0 - valLoss: 0.45411065220832825 - trainLoss: 0.46125978231430054\n",
      "cnt: 0 - valLoss: 0.45410624146461487 - trainLoss: 0.46125513315200806\n",
      "cnt: 0 - valLoss: 0.4541020393371582 - trainLoss: 0.46125054359436035\n",
      "cnt: 0 - valLoss: 0.454097718000412 - trainLoss: 0.46124589443206787\n",
      "cnt: 0 - valLoss: 0.4540935158729553 - trainLoss: 0.46124130487442017\n",
      "cnt: 0 - valLoss: 0.4540894031524658 - trainLoss: 0.4612366259098053\n",
      "cnt: 0 - valLoss: 0.45408496260643005 - trainLoss: 0.46123206615448\n",
      "cnt: 0 - valLoss: 0.45408082008361816 - trainLoss: 0.4612273871898651\n",
      "cnt: 0 - valLoss: 0.4540764391422272 - trainLoss: 0.4612228274345398\n",
      "cnt: 0 - valLoss: 0.4540722370147705 - trainLoss: 0.4612182080745697\n",
      "cnt: 0 - valLoss: 0.4540679156780243 - trainLoss: 0.4612135589122772\n",
      "cnt: 0 - valLoss: 0.4540637731552124 - trainLoss: 0.46120893955230713\n",
      "cnt: 0 - valLoss: 0.4540594220161438 - trainLoss: 0.46120432019233704\n",
      "cnt: 0 - valLoss: 0.4540552794933319 - trainLoss: 0.46119970083236694\n",
      "cnt: 0 - valLoss: 0.4540508985519409 - trainLoss: 0.46119508147239685\n",
      "cnt: 0 - valLoss: 0.45404675602912903 - trainLoss: 0.46119046211242676\n",
      "cnt: 0 - valLoss: 0.45404258370399475 - trainLoss: 0.46118584275245667\n",
      "cnt: 0 - valLoss: 0.45403826236724854 - trainLoss: 0.46118131279945374\n",
      "cnt: 0 - valLoss: 0.45403414964675903 - trainLoss: 0.46117663383483887\n",
      "cnt: 0 - valLoss: 0.45402973890304565 - trainLoss: 0.46117204427719116\n",
      "cnt: 0 - valLoss: 0.45402562618255615 - trainLoss: 0.46116742491722107\n",
      "cnt: 0 - valLoss: 0.45402124524116516 - trainLoss: 0.461162805557251\n",
      "cnt: 0 - valLoss: 0.45401713252067566 - trainLoss: 0.4611581861972809\n",
      "cnt: 0 - valLoss: 0.45401275157928467 - trainLoss: 0.4611535966396332\n",
      "cnt: 0 - valLoss: 0.4540086090564728 - trainLoss: 0.46114903688430786\n",
      "cnt: 0 - valLoss: 0.4540042579174042 - trainLoss: 0.46114441752433777\n",
      "cnt: 0 - valLoss: 0.4540001451969147 - trainLoss: 0.4611397683620453\n",
      "cnt: 0 - valLoss: 0.45399582386016846 - trainLoss: 0.46113520860671997\n",
      "cnt: 0 - valLoss: 0.4539916515350342 - trainLoss: 0.46113061904907227\n",
      "cnt: 0 - valLoss: 0.4539875388145447 - trainLoss: 0.4611259996891022\n",
      "cnt: 0 - valLoss: 0.4539831876754761 - trainLoss: 0.46112141013145447\n",
      "cnt: 0 - valLoss: 0.4539790749549866 - trainLoss: 0.4611167907714844\n",
      "cnt: 0 - valLoss: 0.45397472381591797 - trainLoss: 0.46111223101615906\n",
      "cnt: 0 - valLoss: 0.4539705812931061 - trainLoss: 0.4611075818538666\n",
      "cnt: 0 - valLoss: 0.45396628975868225 - trainLoss: 0.46110302209854126\n",
      "cnt: 0 - valLoss: 0.45396214723587036 - trainLoss: 0.46109843254089355\n",
      "cnt: 0 - valLoss: 0.45395785570144653 - trainLoss: 0.46109381318092346\n",
      "cnt: 0 - valLoss: 0.45395368337631226 - trainLoss: 0.46108928322792053\n",
      "cnt: 0 - valLoss: 0.45394936203956604 - trainLoss: 0.46108466386795044\n",
      "cnt: 0 - valLoss: 0.45394524931907654 - trainLoss: 0.46108001470565796\n",
      "cnt: 0 - valLoss: 0.4539409577846527 - trainLoss: 0.46107539534568787\n",
      "cnt: 0 - valLoss: 0.4539368450641632 - trainLoss: 0.46107086539268494\n",
      "cnt: 0 - valLoss: 0.4539327323436737 - trainLoss: 0.46106624603271484\n",
      "cnt: 0 - valLoss: 0.4539284110069275 - trainLoss: 0.4610617160797119\n",
      "cnt: 0 - valLoss: 0.4539243280887604 - trainLoss: 0.4610570967197418\n",
      "cnt: 0 - valLoss: 0.45392000675201416 - trainLoss: 0.4610525071620941\n",
      "cnt: 0 - valLoss: 0.45391589403152466 - trainLoss: 0.4610479176044464\n",
      "cnt: 0 - valLoss: 0.45391154289245605 - trainLoss: 0.4610433578491211\n",
      "cnt: 0 - valLoss: 0.45390743017196655 - trainLoss: 0.4610387682914734\n",
      "cnt: 0 - valLoss: 0.45390310883522034 - trainLoss: 0.4610341489315033\n",
      "cnt: 0 - valLoss: 0.45389899611473083 - trainLoss: 0.46102961897850037\n",
      "cnt: 0 - valLoss: 0.4538947641849518 - trainLoss: 0.4610249996185303\n",
      "cnt: 0 - valLoss: 0.4538905918598175 - trainLoss: 0.46102043986320496\n",
      "cnt: 0 - valLoss: 0.4538865387439728 - trainLoss: 0.46101582050323486\n",
      "cnt: 0 - valLoss: 0.45388224720954895 - trainLoss: 0.46101129055023193\n",
      "cnt: 0 - valLoss: 0.45387816429138184 - trainLoss: 0.46100667119026184\n",
      "cnt: 0 - valLoss: 0.45387378334999084 - trainLoss: 0.4610021412372589\n",
      "cnt: 0 - valLoss: 0.4538697600364685 - trainLoss: 0.4609975516796112\n",
      "cnt: 0 - valLoss: 0.4538654386997223 - trainLoss: 0.4609929621219635\n",
      "cnt: 0 - valLoss: 0.4538613557815552 - trainLoss: 0.46098843216896057\n",
      "cnt: 0 - valLoss: 0.45385706424713135 - trainLoss: 0.4609838128089905\n",
      "cnt: 0 - valLoss: 0.45385295152664185 - trainLoss: 0.46097928285598755\n",
      "cnt: 0 - valLoss: 0.4538486897945404 - trainLoss: 0.46097466349601746\n",
      "cnt: 0 - valLoss: 0.4538445770740509 - trainLoss: 0.4609701633453369\n",
      "cnt: 0 - valLoss: 0.4538405239582062 - trainLoss: 0.4609655439853668\n",
      "cnt: 0 - valLoss: 0.45383626222610474 - trainLoss: 0.4609609842300415\n",
      "cnt: 0 - valLoss: 0.4538321793079376 - trainLoss: 0.4609564244747162\n",
      "cnt: 0 - valLoss: 0.4538278579711914 - trainLoss: 0.46095189452171326\n",
      "cnt: 0 - valLoss: 0.4538238048553467 - trainLoss: 0.46094730496406555\n",
      "cnt: 0 - valLoss: 0.45381948351860046 - trainLoss: 0.46094271540641785\n",
      "cnt: 0 - valLoss: 0.45381543040275574 - trainLoss: 0.46093815565109253\n",
      "cnt: 0 - valLoss: 0.4538111388683319 - trainLoss: 0.4609335660934448\n",
      "cnt: 0 - valLoss: 0.4538070559501648 - trainLoss: 0.4609290361404419\n",
      "cnt: 0 - valLoss: 0.45380282402038574 - trainLoss: 0.4609244763851166\n",
      "cnt: 0 - valLoss: 0.45379871129989624 - trainLoss: 0.46091991662979126\n",
      "cnt: 0 - valLoss: 0.4537944495677948 - trainLoss: 0.46091532707214355\n",
      "cnt: 0 - valLoss: 0.4537903964519501 - trainLoss: 0.4609107971191406\n",
      "cnt: 0 - valLoss: 0.45378634333610535 - trainLoss: 0.4609062671661377\n",
      "cnt: 0 - valLoss: 0.45378202199935913 - trainLoss: 0.46090167760849\n",
      "cnt: 0 - valLoss: 0.4537779986858368 - trainLoss: 0.4608970880508423\n",
      "cnt: 0 - valLoss: 0.45377370715141296 - trainLoss: 0.46089255809783936\n",
      "cnt: 0 - valLoss: 0.4537696838378906 - trainLoss: 0.4608880281448364\n",
      "cnt: 0 - valLoss: 0.4537654519081116 - trainLoss: 0.4608834385871887\n",
      "cnt: 0 - valLoss: 0.45376139879226685 - trainLoss: 0.4608789086341858\n",
      "cnt: 0 - valLoss: 0.4537571370601654 - trainLoss: 0.46087437868118286\n",
      "cnt: 0 - valLoss: 0.4537530243396759 - trainLoss: 0.46086978912353516\n",
      "cnt: 0 - valLoss: 0.45374882221221924 - trainLoss: 0.4608652591705322\n",
      "cnt: 0 - valLoss: 0.45374470949172974 - trainLoss: 0.4608606994152069\n",
      "cnt: 0 - valLoss: 0.453740656375885 - trainLoss: 0.4608560800552368\n",
      "cnt: 0 - valLoss: 0.45373642444610596 - trainLoss: 0.46085163950920105\n",
      "cnt: 0 - valLoss: 0.4537324011325836 - trainLoss: 0.46084702014923096\n",
      "cnt: 0 - valLoss: 0.4537281394004822 - trainLoss: 0.4608425199985504\n",
      "cnt: 0 - valLoss: 0.45372411608695984 - trainLoss: 0.4608379900455475\n",
      "cnt: 0 - valLoss: 0.453719824552536 - trainLoss: 0.4608334004878998\n",
      "cnt: 0 - valLoss: 0.45371580123901367 - trainLoss: 0.46082887053489685\n",
      "cnt: 0 - valLoss: 0.45371150970458984 - trainLoss: 0.4608243405818939\n",
      "cnt: 0 - valLoss: 0.4537074863910675 - trainLoss: 0.4608197808265686\n",
      "cnt: 0 - valLoss: 0.45370328426361084 - trainLoss: 0.4608152508735657\n",
      "cnt: 0 - valLoss: 0.4536992311477661 - trainLoss: 0.46081072092056274\n",
      "cnt: 0 - valLoss: 0.45369502902030945 - trainLoss: 0.4608061909675598\n",
      "cnt: 0 - valLoss: 0.45369094610214233 - trainLoss: 0.4608016610145569\n",
      "cnt: 0 - valLoss: 0.45368692278862 - trainLoss: 0.46079710125923157\n",
      "cnt: 0 - valLoss: 0.45368266105651855 - trainLoss: 0.46079257130622864\n",
      "cnt: 0 - valLoss: 0.453678697347641 - trainLoss: 0.4607880413532257\n",
      "cnt: 0 - valLoss: 0.45367443561553955 - trainLoss: 0.4607835114002228\n",
      "cnt: 0 - valLoss: 0.4536703824996948 - trainLoss: 0.46077895164489746\n",
      "cnt: 0 - valLoss: 0.45366618037223816 - trainLoss: 0.46077442169189453\n",
      "cnt: 0 - valLoss: 0.4536621570587158 - trainLoss: 0.4607698917388916\n",
      "cnt: 0 - valLoss: 0.45365795493125916 - trainLoss: 0.46076536178588867\n",
      "cnt: 0 - valLoss: 0.45365390181541443 - trainLoss: 0.46076080203056335\n",
      "cnt: 0 - valLoss: 0.4536496698856354 - trainLoss: 0.4607562720775604\n",
      "cnt: 0 - valLoss: 0.45364564657211304 - trainLoss: 0.4607517719268799\n",
      "cnt: 0 - valLoss: 0.4536416530609131 - trainLoss: 0.46074721217155457\n",
      "cnt: 0 - valLoss: 0.45363742113113403 - trainLoss: 0.460742712020874\n",
      "cnt: 0 - valLoss: 0.4536334276199341 - trainLoss: 0.4607382118701935\n",
      "cnt: 0 - valLoss: 0.45362919569015503 - trainLoss: 0.46073368191719055\n",
      "cnt: 0 - valLoss: 0.4536252021789551 - trainLoss: 0.4607291519641876\n",
      "cnt: 0 - valLoss: 0.45362094044685364 - trainLoss: 0.4607245922088623\n",
      "cnt: 0 - valLoss: 0.4536169767379761 - trainLoss: 0.46072012186050415\n",
      "cnt: 0 - valLoss: 0.4536127746105194 - trainLoss: 0.46071556210517883\n",
      "cnt: 0 - valLoss: 0.45360875129699707 - trainLoss: 0.4607110917568207\n",
      "cnt: 0 - valLoss: 0.4536045491695404 - trainLoss: 0.46070653200149536\n",
      "cnt: 0 - valLoss: 0.4536004960536957 - trainLoss: 0.46070200204849243\n",
      "cnt: 0 - valLoss: 0.4535965621471405 - trainLoss: 0.4606974720954895\n",
      "cnt: 0 - valLoss: 0.4535922706127167 - trainLoss: 0.46069297194480896\n",
      "cnt: 0 - valLoss: 0.4535883367061615 - trainLoss: 0.4606884717941284\n",
      "cnt: 0 - valLoss: 0.45358410477638245 - trainLoss: 0.4606839418411255\n",
      "cnt: 0 - valLoss: 0.4535801112651825 - trainLoss: 0.46067941188812256\n",
      "cnt: 0 - valLoss: 0.45357590913772583 - trainLoss: 0.46067488193511963\n",
      "cnt: 0 - valLoss: 0.4535719156265259 - trainLoss: 0.4606704115867615\n",
      "cnt: 0 - valLoss: 0.453567773103714 - trainLoss: 0.46066588163375854\n",
      "cnt: 0 - valLoss: 0.4535636901855469 - trainLoss: 0.4606613516807556\n",
      "cnt: 0 - valLoss: 0.4535595178604126 - trainLoss: 0.4606567919254303\n",
      "cnt: 0 - valLoss: 0.45355555415153503 - trainLoss: 0.46065235137939453\n",
      "cnt: 0 - valLoss: 0.45355159044265747 - trainLoss: 0.4606478214263916\n",
      "cnt: 0 - valLoss: 0.4535473585128784 - trainLoss: 0.46064332127571106\n",
      "cnt: 0 - valLoss: 0.45354342460632324 - trainLoss: 0.4606388211250305\n",
      "cnt: 0 - valLoss: 0.4535391628742218 - trainLoss: 0.4606342911720276\n",
      "cnt: 0 - valLoss: 0.4535352289676666 - trainLoss: 0.46062979102134705\n",
      "cnt: 0 - valLoss: 0.45353108644485474 - trainLoss: 0.4606252908706665\n",
      "cnt: 0 - valLoss: 0.45352703332901 - trainLoss: 0.46062082052230835\n",
      "cnt: 0 - valLoss: 0.4535228908061981 - trainLoss: 0.46061626076698303\n",
      "cnt: 0 - valLoss: 0.45351889729499817 - trainLoss: 0.4606117904186249\n",
      "cnt: 0 - valLoss: 0.4535146951675415 - trainLoss: 0.46060729026794434\n",
      "cnt: 0 - valLoss: 0.45351067185401917 - trainLoss: 0.4606027901172638\n",
      "cnt: 0 - valLoss: 0.45350655913352966 - trainLoss: 0.46059826016426086\n",
      "cnt: 0 - valLoss: 0.4535026252269745 - trainLoss: 0.4605937898159027\n",
      "cnt: 0 - valLoss: 0.45349863171577454 - trainLoss: 0.4605892598628998\n",
      "cnt: 0 - valLoss: 0.45349445939064026 - trainLoss: 0.460584819316864\n",
      "cnt: 0 - valLoss: 0.4534904956817627 - trainLoss: 0.4605802893638611\n",
      "cnt: 0 - valLoss: 0.45348629355430603 - trainLoss: 0.46057578921318054\n",
      "cnt: 0 - valLoss: 0.4534823000431061 - trainLoss: 0.4605712890625\n",
      "cnt: 0 - valLoss: 0.4534781873226166 - trainLoss: 0.46056684851646423\n",
      "cnt: 0 - valLoss: 0.453474223613739 - trainLoss: 0.4605622887611389\n",
      "cnt: 0 - valLoss: 0.45347005128860474 - trainLoss: 0.46055781841278076\n",
      "cnt: 0 - valLoss: 0.4534660577774048 - trainLoss: 0.4605533182621002\n",
      "cnt: 0 - valLoss: 0.45346197485923767 - trainLoss: 0.4605487883090973\n",
      "cnt: 0 - valLoss: 0.45345795154571533 - trainLoss: 0.46054431796073914\n",
      "cnt: 0 - valLoss: 0.45345401763916016 - trainLoss: 0.4605398178100586\n",
      "cnt: 0 - valLoss: 0.4534498453140259 - trainLoss: 0.4605353772640228\n",
      "cnt: 0 - valLoss: 0.4534459114074707 - trainLoss: 0.4605308771133423\n",
      "cnt: 0 - valLoss: 0.4534417390823364 - trainLoss: 0.46052634716033936\n",
      "cnt: 0 - valLoss: 0.45343777537345886 - trainLoss: 0.4605219066143036\n",
      "cnt: 0 - valLoss: 0.4534336030483246 - trainLoss: 0.46051740646362305\n",
      "cnt: 0 - valLoss: 0.4534296691417694 - trainLoss: 0.4605129063129425\n",
      "cnt: 0 - valLoss: 0.4534255564212799 - trainLoss: 0.46050846576690674\n",
      "cnt: 0 - valLoss: 0.45342156291007996 - trainLoss: 0.4605039656162262\n",
      "cnt: 0 - valLoss: 0.45341745018959045 - trainLoss: 0.46049946546554565\n",
      "cnt: 0 - valLoss: 0.4534134566783905 - trainLoss: 0.4604949653148651\n",
      "cnt: 0 - valLoss: 0.4534095227718353 - trainLoss: 0.46049046516418457\n",
      "cnt: 0 - valLoss: 0.4534054100513458 - trainLoss: 0.4604860246181488\n",
      "cnt: 0 - valLoss: 0.45340144634246826 - trainLoss: 0.46048152446746826\n",
      "cnt: 0 - valLoss: 0.45339730381965637 - trainLoss: 0.4604770839214325\n",
      "cnt: 0 - valLoss: 0.4533933997154236 - trainLoss: 0.46047258377075195\n",
      "cnt: 0 - valLoss: 0.4533892571926117 - trainLoss: 0.4604681432247162\n",
      "cnt: 0 - valLoss: 0.45338529348373413 - trainLoss: 0.46046364307403564\n",
      "cnt: 0 - valLoss: 0.45338118076324463 - trainLoss: 0.4604591429233551\n",
      "cnt: 0 - valLoss: 0.45337724685668945 - trainLoss: 0.46045470237731934\n",
      "cnt: 0 - valLoss: 0.4533730745315552 - trainLoss: 0.4604502022266388\n",
      "cnt: 0 - valLoss: 0.4533691108226776 - trainLoss: 0.4604457914829254\n",
      "cnt: 0 - valLoss: 0.4533652067184448 - trainLoss: 0.4604412615299225\n",
      "cnt: 0 - valLoss: 0.45336100459098816 - trainLoss: 0.4604368209838867\n",
      "cnt: 0 - valLoss: 0.4533570408821106 - trainLoss: 0.46043235063552856\n",
      "cnt: 0 - valLoss: 0.4533529281616211 - trainLoss: 0.460427850484848\n",
      "cnt: 0 - valLoss: 0.45334896445274353 - trainLoss: 0.46042340993881226\n",
      "cnt: 0 - valLoss: 0.45334485173225403 - trainLoss: 0.4604189693927765\n",
      "cnt: 0 - valLoss: 0.4533408582210541 - trainLoss: 0.46041446924209595\n",
      "cnt: 0 - valLoss: 0.4533367455005646 - trainLoss: 0.4604099988937378\n",
      "cnt: 0 - valLoss: 0.453332781791687 - trainLoss: 0.460405558347702\n",
      "cnt: 0 - valLoss: 0.4533286392688751 - trainLoss: 0.46040111780166626\n",
      "cnt: 0 - valLoss: 0.45332464575767517 - trainLoss: 0.4603966474533081\n",
      "cnt: 0 - valLoss: 0.4533207416534424 - trainLoss: 0.46039217710494995\n",
      "cnt: 0 - valLoss: 0.4533165991306305 - trainLoss: 0.4603877067565918\n",
      "cnt: 0 - valLoss: 0.45331263542175293 - trainLoss: 0.46038326621055603\n",
      "cnt: 0 - valLoss: 0.4533085227012634 - trainLoss: 0.46037885546684265\n",
      "cnt: 0 - valLoss: 0.45330458879470825 - trainLoss: 0.4603743553161621\n",
      "cnt: 0 - valLoss: 0.45330047607421875 - trainLoss: 0.46036991477012634\n",
      "cnt: 0 - valLoss: 0.4532965421676636 - trainLoss: 0.4603654444217682\n",
      "cnt: 0 - valLoss: 0.4532924294471741 - trainLoss: 0.4603610038757324\n",
      "cnt: 0 - valLoss: 0.4532884657382965 - trainLoss: 0.46035656332969666\n",
      "cnt: 0 - valLoss: 0.453284353017807 - trainLoss: 0.4603521525859833\n",
      "cnt: 0 - valLoss: 0.45328041911125183 - trainLoss: 0.46034765243530273\n",
      "cnt: 0 - valLoss: 0.45327651500701904 - trainLoss: 0.46034321188926697\n",
      "cnt: 0 - valLoss: 0.45327237248420715 - trainLoss: 0.4603388011455536\n",
      "cnt: 0 - valLoss: 0.45326846837997437 - trainLoss: 0.46033430099487305\n",
      "cnt: 0 - valLoss: 0.4532642960548401 - trainLoss: 0.4603298306465149\n",
      "cnt: 0 - valLoss: 0.4532603919506073 - trainLoss: 0.4603254497051239\n",
      "cnt: 0 - valLoss: 0.4532562792301178 - trainLoss: 0.46032097935676575\n",
      "cnt: 0 - valLoss: 0.453252375125885 - trainLoss: 0.46031653881073\n",
      "cnt: 0 - valLoss: 0.4532482624053955 - trainLoss: 0.4603120684623718\n",
      "cnt: 0 - valLoss: 0.45324429869651794 - trainLoss: 0.46030762791633606\n",
      "cnt: 0 - valLoss: 0.45324021577835083 - trainLoss: 0.4603031873703003\n",
      "cnt: 0 - valLoss: 0.45323628187179565 - trainLoss: 0.4602987766265869\n",
      "cnt: 0 - valLoss: 0.45323240756988525 - trainLoss: 0.46029427647590637\n",
      "cnt: 0 - valLoss: 0.45322829484939575 - trainLoss: 0.460289865732193\n",
      "cnt: 0 - valLoss: 0.45322439074516296 - trainLoss: 0.4602854251861572\n",
      "cnt: 0 - valLoss: 0.45322027802467346 - trainLoss: 0.46028101444244385\n",
      "cnt: 0 - valLoss: 0.4532163739204407 - trainLoss: 0.4602765440940857\n",
      "cnt: 0 - valLoss: 0.45321226119995117 - trainLoss: 0.4602721035480499\n",
      "cnt: 0 - valLoss: 0.4532083570957184 - trainLoss: 0.46026766300201416\n",
      "cnt: 0 - valLoss: 0.45320430397987366 - trainLoss: 0.460263192653656\n",
      "cnt: 0 - valLoss: 0.4532003402709961 - trainLoss: 0.4602587819099426\n",
      "cnt: 0 - valLoss: 0.45319631695747375 - trainLoss: 0.46025437116622925\n",
      "cnt: 0 - valLoss: 0.4531923830509186 - trainLoss: 0.4602499306201935\n",
      "cnt: 0 - valLoss: 0.4531885087490082 - trainLoss: 0.4602455198764801\n",
      "cnt: 0 - valLoss: 0.4531843662261963 - trainLoss: 0.4602411091327667\n",
      "cnt: 0 - valLoss: 0.4531804919242859 - trainLoss: 0.46023666858673096\n",
      "cnt: 0 - valLoss: 0.4531763792037964 - trainLoss: 0.4602322578430176\n",
      "cnt: 0 - valLoss: 0.453172504901886 - trainLoss: 0.46022775769233704\n",
      "cnt: 0 - valLoss: 0.45316842198371887 - trainLoss: 0.46022334694862366\n",
      "cnt: 0 - valLoss: 0.4531645178794861 - trainLoss: 0.4602189362049103\n",
      "cnt: 0 - valLoss: 0.4531604051589966 - trainLoss: 0.4602144956588745\n",
      "cnt: 0 - valLoss: 0.45315656065940857 - trainLoss: 0.4602101147174835\n",
      "cnt: 0 - valLoss: 0.4531524181365967 - trainLoss: 0.46020567417144775\n",
      "cnt: 0 - valLoss: 0.45314857363700867 - trainLoss: 0.460201233625412\n",
      "cnt: 0 - valLoss: 0.45314472913742065 - trainLoss: 0.4601968228816986\n",
      "cnt: 0 - valLoss: 0.45314061641693115 - trainLoss: 0.46019241213798523\n",
      "cnt: 0 - valLoss: 0.45313674211502075 - trainLoss: 0.46018797159194946\n",
      "cnt: 0 - valLoss: 0.45313265919685364 - trainLoss: 0.4601835608482361\n",
      "cnt: 0 - valLoss: 0.45312878489494324 - trainLoss: 0.4601791501045227\n",
      "cnt: 0 - valLoss: 0.4531247317790985 - trainLoss: 0.46017467975616455\n",
      "cnt: 0 - valLoss: 0.4531208276748657 - trainLoss: 0.46017032861709595\n",
      "cnt: 0 - valLoss: 0.453116774559021 - trainLoss: 0.46016591787338257\n",
      "cnt: 0 - valLoss: 0.4531129002571106 - trainLoss: 0.4601614773273468\n",
      "cnt: 0 - valLoss: 0.45310884714126587 - trainLoss: 0.4601570665836334\n",
      "cnt: 0 - valLoss: 0.45310497283935547 - trainLoss: 0.46015265583992004\n",
      "cnt: 0 - valLoss: 0.45310112833976746 - trainLoss: 0.46014824509620667\n",
      "cnt: 0 - valLoss: 0.45309701561927795 - trainLoss: 0.4601438343524933\n",
      "cnt: 0 - valLoss: 0.45309320092201233 - trainLoss: 0.4601394236087799\n",
      "cnt: 0 - valLoss: 0.4530891180038452 - trainLoss: 0.46013501286506653\n",
      "cnt: 0 - valLoss: 0.4530852735042572 - trainLoss: 0.46013060212135315\n",
      "cnt: 0 - valLoss: 0.4530811905860901 - trainLoss: 0.4601261615753174\n",
      "cnt: 0 - valLoss: 0.4530773460865021 - trainLoss: 0.4601217806339264\n",
      "cnt: 0 - valLoss: 0.45307326316833496 - trainLoss: 0.4601173400878906\n",
      "cnt: 0 - valLoss: 0.45306941866874695 - trainLoss: 0.46011295914649963\n",
      "cnt: 0 - valLoss: 0.45306533575057983 - trainLoss: 0.46010851860046387\n",
      "cnt: 0 - valLoss: 0.4530614912509918 - trainLoss: 0.4601041078567505\n",
      "cnt: 0 - valLoss: 0.4530576169490814 - trainLoss: 0.4600996971130371\n",
      "cnt: 0 - valLoss: 0.4530535042285919 - trainLoss: 0.46009528636932373\n",
      "cnt: 0 - valLoss: 0.4530496597290039 - trainLoss: 0.46009084582328796\n",
      "cnt: 0 - valLoss: 0.4530455768108368 - trainLoss: 0.4600864350795746\n",
      "cnt: 0 - valLoss: 0.453041672706604 - trainLoss: 0.4600819945335388\n",
      "cnt: 0 - valLoss: 0.45303764939308167 - trainLoss: 0.46007758378982544\n",
      "cnt: 0 - valLoss: 0.4530337452888489 - trainLoss: 0.46007317304611206\n",
      "cnt: 0 - valLoss: 0.45302966237068176 - trainLoss: 0.4600687026977539\n",
      "cnt: 0 - valLoss: 0.45302581787109375 - trainLoss: 0.46006426215171814\n",
      "cnt: 0 - valLoss: 0.45302170515060425 - trainLoss: 0.46005988121032715\n",
      "cnt: 0 - valLoss: 0.45301783084869385 - trainLoss: 0.4600554406642914\n",
      "cnt: 0 - valLoss: 0.45301398634910583 - trainLoss: 0.4600510001182556\n",
      "cnt: 0 - valLoss: 0.45300987362861633 - trainLoss: 0.4600466191768646\n",
      "cnt: 0 - valLoss: 0.4530060291290283 - trainLoss: 0.46004217863082886\n",
      "cnt: 0 - valLoss: 0.4530019462108612 - trainLoss: 0.4600377678871155\n",
      "cnt: 0 - valLoss: 0.4529980719089508 - trainLoss: 0.4600333571434021\n",
      "cnt: 0 - valLoss: 0.4529940187931061 - trainLoss: 0.4600289463996887\n",
      "cnt: 0 - valLoss: 0.4529901444911957 - trainLoss: 0.46002453565597534\n",
      "cnt: 0 - valLoss: 0.45298609137535095 - trainLoss: 0.4600200951099396\n",
      "cnt: 0 - valLoss: 0.45298218727111816 - trainLoss: 0.4600157141685486\n",
      "cnt: 0 - valLoss: 0.4529781639575958 - trainLoss: 0.4600112736225128\n",
      "cnt: 0 - valLoss: 0.4529742896556854 - trainLoss: 0.4600068926811218\n",
      "cnt: 0 - valLoss: 0.4529704451560974 - trainLoss: 0.46000245213508606\n",
      "cnt: 0 - valLoss: 0.4529664218425751 - trainLoss: 0.4599980413913727\n",
      "cnt: 0 - valLoss: 0.4529625475406647 - trainLoss: 0.4599936306476593\n",
      "cnt: 0 - valLoss: 0.45295849442481995 - trainLoss: 0.4599892199039459\n",
      "cnt: 0 - valLoss: 0.4529546797275543 - trainLoss: 0.45998480916023254\n",
      "cnt: 0 - valLoss: 0.4529506266117096 - trainLoss: 0.4599803686141968\n",
      "cnt: 0 - valLoss: 0.4529467821121216 - trainLoss: 0.4599760174751282\n",
      "cnt: 0 - valLoss: 0.45294269919395447 - trainLoss: 0.4599716365337372\n",
      "cnt: 0 - valLoss: 0.45293888449668884 - trainLoss: 0.4599671959877014\n",
      "cnt: 0 - valLoss: 0.4529348611831665 - trainLoss: 0.45996278524398804\n",
      "cnt: 0 - valLoss: 0.4529310166835785 - trainLoss: 0.45995837450027466\n",
      "cnt: 0 - valLoss: 0.4529271423816681 - trainLoss: 0.4599539041519165\n",
      "cnt: 0 - valLoss: 0.45292308926582336 - trainLoss: 0.4599495828151703\n",
      "cnt: 0 - valLoss: 0.45291927456855774 - trainLoss: 0.4599451720714569\n",
      "cnt: 0 - valLoss: 0.452915221452713 - trainLoss: 0.4599407911300659\n",
      "cnt: 0 - valLoss: 0.4529114067554474 - trainLoss: 0.45993638038635254\n",
      "cnt: 0 - valLoss: 0.45290738344192505 - trainLoss: 0.45993196964263916\n",
      "cnt: 0 - valLoss: 0.4529035687446594 - trainLoss: 0.4599275588989258\n",
      "cnt: 0 - valLoss: 0.4528995454311371 - trainLoss: 0.4599231779575348\n",
      "cnt: 0 - valLoss: 0.4528957009315491 - trainLoss: 0.459918737411499\n",
      "cnt: 0 - valLoss: 0.45289164781570435 - trainLoss: 0.45991435647010803\n",
      "cnt: 0 - valLoss: 0.4528878331184387 - trainLoss: 0.45990997552871704\n",
      "cnt: 0 - valLoss: 0.4528840184211731 - trainLoss: 0.45990556478500366\n",
      "cnt: 0 - valLoss: 0.45287999510765076 - trainLoss: 0.45990118384361267\n",
      "cnt: 0 - valLoss: 0.45287618041038513 - trainLoss: 0.4598967730998993\n",
      "cnt: 0 - valLoss: 0.4528721570968628 - trainLoss: 0.4598923623561859\n",
      "cnt: 0 - valLoss: 0.45286834239959717 - trainLoss: 0.4598880112171173\n",
      "cnt: 0 - valLoss: 0.45286431908607483 - trainLoss: 0.45988360047340393\n",
      "cnt: 0 - valLoss: 0.4528605341911316 - trainLoss: 0.45987918972969055\n",
      "cnt: 0 - valLoss: 0.45285648107528687 - trainLoss: 0.45987480878829956\n",
      "cnt: 0 - valLoss: 0.45285266637802124 - trainLoss: 0.45987045764923096\n",
      "cnt: 0 - valLoss: 0.4528487026691437 - trainLoss: 0.4598660469055176\n",
      "cnt: 0 - valLoss: 0.4528447985649109 - trainLoss: 0.4598616659641266\n",
      "cnt: 0 - valLoss: 0.45284104347229004 - trainLoss: 0.4598572850227356\n",
      "cnt: 0 - valLoss: 0.4528370499610901 - trainLoss: 0.4598528742790222\n",
      "cnt: 0 - valLoss: 0.45283323526382446 - trainLoss: 0.45984846353530884\n",
      "cnt: 0 - valLoss: 0.4528292417526245 - trainLoss: 0.45984408259391785\n",
      "cnt: 0 - valLoss: 0.4528253972530365 - trainLoss: 0.45983973145484924\n",
      "cnt: 0 - valLoss: 0.45282140374183655 - trainLoss: 0.45983535051345825\n",
      "cnt: 0 - valLoss: 0.4528176486492157 - trainLoss: 0.4598309397697449\n",
      "cnt: 0 - valLoss: 0.45281365513801575 - trainLoss: 0.4598265290260315\n",
      "cnt: 0 - valLoss: 0.45280978083610535 - trainLoss: 0.4598221778869629\n",
      "cnt: 0 - valLoss: 0.45280584692955017 - trainLoss: 0.4598177969455719\n",
      "cnt: 0 - valLoss: 0.45280203223228455 - trainLoss: 0.4598134458065033\n",
      "cnt: 0 - valLoss: 0.4527983069419861 - trainLoss: 0.4598090350627899\n",
      "cnt: 0 - valLoss: 0.45279428362846375 - trainLoss: 0.4598046541213989\n",
      "cnt: 0 - valLoss: 0.4527904689311981 - trainLoss: 0.4598003327846527\n",
      "cnt: 0 - valLoss: 0.45278647541999817 - trainLoss: 0.4597959518432617\n",
      "cnt: 0 - valLoss: 0.4527827203273773 - trainLoss: 0.45979154109954834\n",
      "cnt: 0 - valLoss: 0.4527786672115326 - trainLoss: 0.45978716015815735\n",
      "cnt: 0 - valLoss: 0.45277491211891174 - trainLoss: 0.45978280901908875\n",
      "cnt: 0 - valLoss: 0.4527709484100342 - trainLoss: 0.45977842807769775\n",
      "cnt: 0 - valLoss: 0.45276719331741333 - trainLoss: 0.45977407693862915\n",
      "cnt: 0 - valLoss: 0.45276325941085815 - trainLoss: 0.45976969599723816\n",
      "cnt: 0 - valLoss: 0.4527594745159149 - trainLoss: 0.4597654342651367\n",
      "cnt: 0 - valLoss: 0.45275574922561646 - trainLoss: 0.4597610533237457\n",
      "cnt: 0 - valLoss: 0.4527518153190613 - trainLoss: 0.4597567021846771\n",
      "cnt: 0 - valLoss: 0.45274803042411804 - trainLoss: 0.45975232124328613\n",
      "cnt: 0 - valLoss: 0.45274412631988525 - trainLoss: 0.45974797010421753\n",
      "cnt: 0 - valLoss: 0.4527403712272644 - trainLoss: 0.4597436189651489\n",
      "cnt: 0 - valLoss: 0.45273640751838684 - trainLoss: 0.4597392976284027\n",
      "cnt: 0 - valLoss: 0.4527326822280884 - trainLoss: 0.4597349464893341\n",
      "cnt: 0 - valLoss: 0.4527287483215332 - trainLoss: 0.4597305655479431\n",
      "cnt: 0 - valLoss: 0.45272502303123474 - trainLoss: 0.4597263038158417\n",
      "cnt: 0 - valLoss: 0.4527210593223572 - trainLoss: 0.4597219228744507\n",
      "cnt: 0 - valLoss: 0.45271730422973633 - trainLoss: 0.45971766114234924\n",
      "cnt: 0 - valLoss: 0.45271366834640503 - trainLoss: 0.45971328020095825\n",
      "cnt: 0 - valLoss: 0.4527096748352051 - trainLoss: 0.45970892906188965\n",
      "cnt: 0 - valLoss: 0.452705979347229 - trainLoss: 0.45970460772514343\n",
      "cnt: 0 - valLoss: 0.45270198583602905 - trainLoss: 0.4597002863883972\n",
      "cnt: 0 - valLoss: 0.4526982605457306 - trainLoss: 0.4596959054470062\n",
      "cnt: 0 - valLoss: 0.4526943266391754 - trainLoss: 0.4596916437149048\n",
      "cnt: 0 - valLoss: 0.45269063115119934 - trainLoss: 0.4596872627735138\n",
      "cnt: 0 - valLoss: 0.45268669724464417 - trainLoss: 0.4596829414367676\n",
      "cnt: 0 - valLoss: 0.4526829719543457 - trainLoss: 0.45967862010002136\n",
      "cnt: 0 - valLoss: 0.4526790678501129 - trainLoss: 0.45967423915863037\n",
      "cnt: 0 - valLoss: 0.45267534255981445 - trainLoss: 0.45966997742652893\n",
      "cnt: 0 - valLoss: 0.452671617269516 - trainLoss: 0.45966559648513794\n",
      "cnt: 0 - valLoss: 0.4526677131652832 - trainLoss: 0.4596613347530365\n",
      "cnt: 0 - valLoss: 0.45266398787498474 - trainLoss: 0.4596569538116455\n",
      "cnt: 0 - valLoss: 0.4526600241661072 - trainLoss: 0.45965269207954407\n",
      "cnt: 0 - valLoss: 0.4526563584804535 - trainLoss: 0.4596483111381531\n",
      "cnt: 0 - valLoss: 0.4526523947715759 - trainLoss: 0.45964398980140686\n",
      "cnt: 0 - valLoss: 0.45264872908592224 - trainLoss: 0.45963966846466064\n",
      "cnt: 0 - valLoss: 0.45264482498168945 - trainLoss: 0.45963534712791443\n",
      "cnt: 0 - valLoss: 0.452641099691391 - trainLoss: 0.4596310257911682\n",
      "cnt: 0 - valLoss: 0.4526372253894806 - trainLoss: 0.4596266448497772\n",
      "cnt: 0 - valLoss: 0.45263347029685974 - trainLoss: 0.4596223831176758\n",
      "cnt: 0 - valLoss: 0.45262980461120605 - trainLoss: 0.45961809158325195\n",
      "cnt: 0 - valLoss: 0.4526258707046509 - trainLoss: 0.45961374044418335\n",
      "cnt: 0 - valLoss: 0.4526221454143524 - trainLoss: 0.4596094489097595\n",
      "cnt: 0 - valLoss: 0.4526183009147644 - trainLoss: 0.4596051275730133\n",
      "cnt: 0 - valLoss: 0.45261460542678833 - trainLoss: 0.4596008062362671\n",
      "cnt: 0 - valLoss: 0.45261067152023315 - trainLoss: 0.45959651470184326\n",
      "cnt: 0 - valLoss: 0.4526069760322571 - trainLoss: 0.45959216356277466\n",
      "cnt: 0 - valLoss: 0.4526030719280243 - trainLoss: 0.45958787202835083\n",
      "cnt: 0 - valLoss: 0.45259934663772583 - trainLoss: 0.4595835506916046\n",
      "cnt: 0 - valLoss: 0.45259547233581543 - trainLoss: 0.4595792293548584\n",
      "cnt: 0 - valLoss: 0.45259174704551697 - trainLoss: 0.4595749080181122\n",
      "cnt: 0 - valLoss: 0.45258811116218567 - trainLoss: 0.45957058668136597\n",
      "cnt: 0 - valLoss: 0.4525841772556305 - trainLoss: 0.45956629514694214\n",
      "cnt: 0 - valLoss: 0.4525804817676544 - trainLoss: 0.45956194400787354\n",
      "cnt: 0 - valLoss: 0.4525766372680664 - trainLoss: 0.4595576524734497\n",
      "cnt: 0 - valLoss: 0.45257294178009033 - trainLoss: 0.45955339074134827\n",
      "cnt: 0 - valLoss: 0.45256900787353516 - trainLoss: 0.45954906940460205\n",
      "cnt: 0 - valLoss: 0.45256540179252625 - trainLoss: 0.4595447778701782\n",
      "cnt: 0 - valLoss: 0.45256146788597107 - trainLoss: 0.45954039692878723\n",
      "cnt: 0 - valLoss: 0.452557772397995 - trainLoss: 0.4595361351966858\n",
      "cnt: 0 - valLoss: 0.4525538980960846 - trainLoss: 0.4595318138599396\n",
      "cnt: 0 - valLoss: 0.4525502324104309 - trainLoss: 0.45952752232551575\n",
      "cnt: 0 - valLoss: 0.4525465667247772 - trainLoss: 0.4595232605934143\n",
      "cnt: 0 - valLoss: 0.4525426924228668 - trainLoss: 0.4595189392566681\n",
      "cnt: 0 - valLoss: 0.45253899693489075 - trainLoss: 0.4595145881175995\n",
      "cnt: 0 - valLoss: 0.45253512263298035 - trainLoss: 0.45951032638549805\n",
      "cnt: 0 - valLoss: 0.45253145694732666 - trainLoss: 0.4595060348510742\n",
      "cnt: 0 - valLoss: 0.45252755284309387 - trainLoss: 0.459501713514328\n",
      "cnt: 0 - valLoss: 0.4525238573551178 - trainLoss: 0.4594973921775818\n",
      "cnt: 0 - valLoss: 0.4525200426578522 - trainLoss: 0.45949310064315796\n",
      "cnt: 0 - valLoss: 0.4525163769721985 - trainLoss: 0.4594888389110565\n",
      "cnt: 0 - valLoss: 0.4525125026702881 - trainLoss: 0.4594845175743103\n",
      "cnt: 0 - valLoss: 0.452508807182312 - trainLoss: 0.45948028564453125\n",
      "cnt: 0 - valLoss: 0.4525051414966583 - trainLoss: 0.45947590470314026\n",
      "cnt: 0 - valLoss: 0.4525012671947479 - trainLoss: 0.4594716429710388\n",
      "cnt: 0 - valLoss: 0.4524976313114166 - trainLoss: 0.459467351436615\n",
      "cnt: 0 - valLoss: 0.45249372720718384 - trainLoss: 0.4594630300998688\n",
      "cnt: 0 - valLoss: 0.45249009132385254 - trainLoss: 0.45945873856544495\n",
      "cnt: 0 - valLoss: 0.45248621702194214 - trainLoss: 0.45945441722869873\n",
      "cnt: 0 - valLoss: 0.45248258113861084 - trainLoss: 0.4594501554965973\n",
      "cnt: 0 - valLoss: 0.4524787366390228 - trainLoss: 0.45944586396217346\n",
      "cnt: 0 - valLoss: 0.45247504115104675 - trainLoss: 0.45944157242774963\n",
      "cnt: 0 - valLoss: 0.45247143507003784 - trainLoss: 0.4594373106956482\n",
      "cnt: 0 - valLoss: 0.45246753096580505 - trainLoss: 0.45943301916122437\n",
      "cnt: 0 - valLoss: 0.45246389508247375 - trainLoss: 0.45942872762680054\n",
      "cnt: 0 - valLoss: 0.45246005058288574 - trainLoss: 0.4594244658946991\n",
      "cnt: 0 - valLoss: 0.45245644450187683 - trainLoss: 0.45942017436027527\n",
      "cnt: 0 - valLoss: 0.45245257019996643 - trainLoss: 0.45941585302352905\n",
      "cnt: 0 - valLoss: 0.45244890451431274 - trainLoss: 0.4594115912914276\n",
      "cnt: 0 - valLoss: 0.45244503021240234 - trainLoss: 0.4594072997570038\n",
      "cnt: 0 - valLoss: 0.45244139432907104 - trainLoss: 0.45940306782722473\n",
      "cnt: 0 - valLoss: 0.4524375796318054 - trainLoss: 0.4593987464904785\n",
      "cnt: 0 - valLoss: 0.45243388414382935 - trainLoss: 0.45939451456069946\n",
      "cnt: 0 - valLoss: 0.4524303078651428 - trainLoss: 0.45939013361930847\n",
      "cnt: 0 - valLoss: 0.4524264335632324 - trainLoss: 0.4593859314918518\n",
      "cnt: 0 - valLoss: 0.4524228274822235 - trainLoss: 0.4593816101551056\n",
      "cnt: 0 - valLoss: 0.4524189829826355 - trainLoss: 0.45937734842300415\n",
      "cnt: 0 - valLoss: 0.4524153769016266 - trainLoss: 0.4593730568885803\n",
      "cnt: 0 - valLoss: 0.4524114727973938 - trainLoss: 0.45936882495880127\n",
      "cnt: 0 - valLoss: 0.4524078369140625 - trainLoss: 0.45936453342437744\n",
      "cnt: 0 - valLoss: 0.45240405201911926 - trainLoss: 0.4593602120876312\n",
      "cnt: 0 - valLoss: 0.4524003863334656 - trainLoss: 0.4593559801578522\n",
      "cnt: 0 - valLoss: 0.45239657163619995 - trainLoss: 0.45935168862342834\n",
      "cnt: 0 - valLoss: 0.45239293575286865 - trainLoss: 0.4593474268913269\n",
      "cnt: 0 - valLoss: 0.45238932967185974 - trainLoss: 0.4593431353569031\n",
      "cnt: 0 - valLoss: 0.45238545536994934 - trainLoss: 0.45933884382247925\n",
      "cnt: 0 - valLoss: 0.45238181948661804 - trainLoss: 0.4593345820903778\n",
      "cnt: 0 - valLoss: 0.4523780047893524 - trainLoss: 0.45933035016059875\n",
      "cnt: 0 - valLoss: 0.4523743987083435 - trainLoss: 0.45932602882385254\n",
      "cnt: 0 - valLoss: 0.4523705840110779 - trainLoss: 0.4593218266963959\n",
      "cnt: 0 - valLoss: 0.4523669481277466 - trainLoss: 0.45931753516197205\n",
      "cnt: 0 - valLoss: 0.45236313343048096 - trainLoss: 0.4593132734298706\n",
      "cnt: 0 - valLoss: 0.45235949754714966 - trainLoss: 0.4593089818954468\n",
      "cnt: 0 - valLoss: 0.4523557126522064 - trainLoss: 0.45930469036102295\n",
      "cnt: 0 - valLoss: 0.4523520767688751 - trainLoss: 0.45930051803588867\n",
      "cnt: 0 - valLoss: 0.4523484706878662 - trainLoss: 0.45929622650146484\n",
      "cnt: 0 - valLoss: 0.4523446559906006 - trainLoss: 0.459291934967041\n",
      "cnt: 0 - valLoss: 0.4523410201072693 - trainLoss: 0.4592876732349396\n",
      "cnt: 0 - valLoss: 0.45233720541000366 - trainLoss: 0.4592834413051605\n",
      "cnt: 0 - valLoss: 0.45233359932899475 - trainLoss: 0.4592791497707367\n",
      "cnt: 0 - valLoss: 0.4523298144340515 - trainLoss: 0.4592748284339905\n",
      "cnt: 0 - valLoss: 0.4523261785507202 - trainLoss: 0.4592706263065338\n",
      "cnt: 0 - valLoss: 0.4523223638534546 - trainLoss: 0.4592663645744324\n",
      "cnt: 0 - valLoss: 0.4523187577724457 - trainLoss: 0.45926210284233093\n",
      "cnt: 0 - valLoss: 0.45231518149375916 - trainLoss: 0.4592578411102295\n",
      "cnt: 0 - valLoss: 0.4523113965988159 - trainLoss: 0.45925360918045044\n",
      "cnt: 0 - valLoss: 0.4523077607154846 - trainLoss: 0.4592493176460266\n",
      "cnt: 0 - valLoss: 0.452303946018219 - trainLoss: 0.4592450261116028\n",
      "cnt: 0 - valLoss: 0.45230036973953247 - trainLoss: 0.4592408537864685\n",
      "cnt: 0 - valLoss: 0.45229655504226685 - trainLoss: 0.4592365622520447\n",
      "cnt: 0 - valLoss: 0.4522929787635803 - trainLoss: 0.4592323303222656\n",
      "cnt: 0 - valLoss: 0.4522891938686371 - trainLoss: 0.4592280387878418\n",
      "cnt: 0 - valLoss: 0.4522855579853058 - trainLoss: 0.45922380685806274\n",
      "cnt: 0 - valLoss: 0.4522818326950073 - trainLoss: 0.4592195153236389\n",
      "cnt: 0 - valLoss: 0.45227813720703125 - trainLoss: 0.45921528339385986\n",
      "cnt: 0 - valLoss: 0.4522746205329895 - trainLoss: 0.4592110514640808\n",
      "cnt: 0 - valLoss: 0.4522708058357239 - trainLoss: 0.45920681953430176\n",
      "cnt: 0 - valLoss: 0.45226725935935974 - trainLoss: 0.4592025876045227\n",
      "cnt: 0 - valLoss: 0.4522634446620941 - trainLoss: 0.4591982960700989\n",
      "cnt: 0 - valLoss: 0.4522598385810852 - trainLoss: 0.4591940641403198\n",
      "cnt: 0 - valLoss: 0.45225605368614197 - trainLoss: 0.45918983221054077\n",
      "cnt: 0 - valLoss: 0.45225247740745544 - trainLoss: 0.4591856002807617\n",
      "cnt: 0 - valLoss: 0.4522487223148346 - trainLoss: 0.4591813385486603\n",
      "cnt: 0 - valLoss: 0.4522450864315033 - trainLoss: 0.45917707681655884\n",
      "cnt: 0 - valLoss: 0.45224133133888245 - trainLoss: 0.459172785282135\n",
      "cnt: 0 - valLoss: 0.45223772525787354 - trainLoss: 0.45916858315467834\n",
      "cnt: 0 - valLoss: 0.452234148979187 - trainLoss: 0.4591643214225769\n",
      "cnt: 0 - valLoss: 0.4522303342819214 - trainLoss: 0.45916011929512024\n",
      "cnt: 0 - valLoss: 0.45222678780555725 - trainLoss: 0.4591558277606964\n",
      "cnt: 0 - valLoss: 0.452223002910614 - trainLoss: 0.45915165543556213\n",
      "cnt: 0 - valLoss: 0.45221951603889465 - trainLoss: 0.4591473639011383\n",
      "cnt: 0 - valLoss: 0.45221570134162903 - trainLoss: 0.45914313197135925\n",
      "cnt: 0 - valLoss: 0.4522121250629425 - trainLoss: 0.4591389000415802\n",
      "cnt: 0 - valLoss: 0.45220836997032166 - trainLoss: 0.45913469791412354\n",
      "cnt: 0 - valLoss: 0.45220476388931274 - trainLoss: 0.4591304063796997\n",
      "cnt: 0 - valLoss: 0.4522009789943695 - trainLoss: 0.45912617444992065\n",
      "cnt: 0 - valLoss: 0.45219743251800537 - trainLoss: 0.4591219425201416\n",
      "cnt: 0 - valLoss: 0.45219385623931885 - trainLoss: 0.45911774039268494\n",
      "cnt: 0 - valLoss: 0.452190101146698 - trainLoss: 0.4591135084629059\n",
      "cnt: 0 - valLoss: 0.45218655467033386 - trainLoss: 0.45910927653312683\n",
      "cnt: 0 - valLoss: 0.4521827697753906 - trainLoss: 0.459104984998703\n",
      "cnt: 0 - valLoss: 0.4521792232990265 - trainLoss: 0.45910078287124634\n",
      "cnt: 0 - valLoss: 0.452175498008728 - trainLoss: 0.45909661054611206\n",
      "cnt: 0 - valLoss: 0.4521718919277191 - trainLoss: 0.45909231901168823\n",
      "cnt: 0 - valLoss: 0.45216816663742065 - trainLoss: 0.4590880274772644\n",
      "cnt: 0 - valLoss: 0.45216459035873413 - trainLoss: 0.4590838551521301\n",
      "cnt: 0 - valLoss: 0.45216104388237 - trainLoss: 0.4590796232223511\n",
      "cnt: 0 - valLoss: 0.45215725898742676 - trainLoss: 0.4590754508972168\n",
      "cnt: 0 - valLoss: 0.452153742313385 - trainLoss: 0.45907118916511536\n",
      "cnt: 0 - valLoss: 0.45214998722076416 - trainLoss: 0.4590669870376587\n",
      "cnt: 0 - valLoss: 0.4521464705467224 - trainLoss: 0.45906275510787964\n",
      "cnt: 0 - valLoss: 0.4521426558494568 - trainLoss: 0.4590584933757782\n",
      "cnt: 0 - valLoss: 0.45213907957077026 - trainLoss: 0.45905426144599915\n",
      "cnt: 0 - valLoss: 0.4521353542804718 - trainLoss: 0.45905008912086487\n",
      "cnt: 0 - valLoss: 0.45213180780410767 - trainLoss: 0.4590458273887634\n",
      "cnt: 0 - valLoss: 0.4521280825138092 - trainLoss: 0.459041565656662\n",
      "cnt: 0 - valLoss: 0.4521245062351227 - trainLoss: 0.4590373635292053\n",
      "cnt: 0 - valLoss: 0.4521210193634033 - trainLoss: 0.45903313159942627\n",
      "cnt: 0 - valLoss: 0.4521172344684601 - trainLoss: 0.4590289890766144\n",
      "cnt: 0 - valLoss: 0.45211371779441833 - trainLoss: 0.45902469754219055\n",
      "cnt: 0 - valLoss: 0.4521099925041199 - trainLoss: 0.4590204954147339\n",
      "cnt: 0 - valLoss: 0.45210644602775574 - trainLoss: 0.4590162932872772\n",
      "cnt: 0 - valLoss: 0.4521026909351349 - trainLoss: 0.45901206135749817\n",
      "cnt: 0 - valLoss: 0.45209914445877075 - trainLoss: 0.4590078294277191\n",
      "cnt: 0 - valLoss: 0.4520954489707947 - trainLoss: 0.45900362730026245\n",
      "cnt: 0 - valLoss: 0.45209187269210815 - trainLoss: 0.4589994549751282\n",
      "cnt: 0 - valLoss: 0.45208844542503357 - trainLoss: 0.45899516344070435\n",
      "cnt: 0 - valLoss: 0.45208466053009033 - trainLoss: 0.45899102091789246\n",
      "cnt: 0 - valLoss: 0.4520811140537262 - trainLoss: 0.458986759185791\n",
      "cnt: 0 - valLoss: 0.45207735896110535 - trainLoss: 0.45898258686065674\n",
      "cnt: 0 - valLoss: 0.452073872089386 - trainLoss: 0.4589782953262329\n",
      "cnt: 0 - valLoss: 0.45207011699676514 - trainLoss: 0.458974152803421\n",
      "cnt: 0 - valLoss: 0.452066570520401 - trainLoss: 0.45896992087364197\n",
      "cnt: 0 - valLoss: 0.4520628750324249 - trainLoss: 0.4589657187461853\n",
      "cnt: 0 - valLoss: 0.4520593583583832 - trainLoss: 0.45896151661872864\n",
      "cnt: 0 - valLoss: 0.4520556330680847 - trainLoss: 0.4589572846889496\n",
      "cnt: 0 - valLoss: 0.4520520865917206 - trainLoss: 0.4589531421661377\n",
      "cnt: 0 - valLoss: 0.4520485997200012 - trainLoss: 0.45894888043403625\n",
      "cnt: 0 - valLoss: 0.45204487442970276 - trainLoss: 0.458944708108902\n",
      "cnt: 0 - valLoss: 0.45204126834869385 - trainLoss: 0.45894044637680054\n",
      "cnt: 0 - valLoss: 0.4520375430583954 - trainLoss: 0.45893627405166626\n",
      "cnt: 0 - valLoss: 0.45203396677970886 - trainLoss: 0.45893198251724243\n",
      "cnt: 0 - valLoss: 0.452030211687088 - trainLoss: 0.45892778038978577\n",
      "cnt: 0 - valLoss: 0.4520266354084015 - trainLoss: 0.4589235186576843\n",
      "cnt: 0 - valLoss: 0.4520229697227478 - trainLoss: 0.45891931653022766\n",
      "cnt: 0 - valLoss: 0.4520195424556732 - trainLoss: 0.458915114402771\n",
      "cnt: 0 - valLoss: 0.4520159661769867 - trainLoss: 0.4589109420776367\n",
      "cnt: 0 - valLoss: 0.45201247930526733 - trainLoss: 0.4589068293571472\n",
      "cnt: 0 - valLoss: 0.45200905203819275 - trainLoss: 0.45890262722969055\n",
      "cnt: 0 - valLoss: 0.45200544595718384 - trainLoss: 0.4588984251022339\n",
      "cnt: 0 - valLoss: 0.45200201869010925 - trainLoss: 0.4588942229747772\n",
      "cnt: 0 - valLoss: 0.45199841260910034 - trainLoss: 0.45889005064964294\n",
      "cnt: 0 - valLoss: 0.45199498534202576 - trainLoss: 0.45888593792915344\n",
      "cnt: 0 - valLoss: 0.45199134945869446 - trainLoss: 0.4588817059993744\n",
      "cnt: 0 - valLoss: 0.4519878625869751 - trainLoss: 0.4588775336742401\n",
      "cnt: 0 - valLoss: 0.45198431611061096 - trainLoss: 0.45887336134910583\n",
      "cnt: 0 - valLoss: 0.4519808888435364 - trainLoss: 0.45886915922164917\n",
      "cnt: 0 - valLoss: 0.4519774913787842 - trainLoss: 0.4588649868965149\n",
      "cnt: 0 - valLoss: 0.45197391510009766 - trainLoss: 0.458860844373703\n",
      "cnt: 0 - valLoss: 0.4519704282283783 - trainLoss: 0.45885664224624634\n",
      "cnt: 0 - valLoss: 0.451966792345047 - trainLoss: 0.45885246992111206\n",
      "cnt: 0 - valLoss: 0.4519633948802948 - trainLoss: 0.4588482677936554\n",
      "cnt: 0 - valLoss: 0.4519597887992859 - trainLoss: 0.4588441550731659\n",
      "cnt: 0 - valLoss: 0.4519563615322113 - trainLoss: 0.45883995294570923\n",
      "cnt: 0 - valLoss: 0.4519527852535248 - trainLoss: 0.45883581042289734\n",
      "cnt: 0 - valLoss: 0.4519493579864502 - trainLoss: 0.45883166790008545\n",
      "cnt: 0 - valLoss: 0.45194578170776367 - trainLoss: 0.4588274657726288\n",
      "cnt: 0 - valLoss: 0.4519423544406891 - trainLoss: 0.4588232636451721\n",
      "cnt: 0 - valLoss: 0.4519389867782593 - trainLoss: 0.4588191509246826\n",
      "cnt: 0 - valLoss: 0.4519353210926056 - trainLoss: 0.45881497859954834\n",
      "cnt: 0 - valLoss: 0.451931893825531 - trainLoss: 0.4588107764720917\n",
      "cnt: 0 - valLoss: 0.4519282877445221 - trainLoss: 0.4588066339492798\n",
      "cnt: 0 - valLoss: 0.4519248604774475 - trainLoss: 0.4588024616241455\n",
      "cnt: 0 - valLoss: 0.451921284198761 - trainLoss: 0.458798348903656\n",
      "cnt: 0 - valLoss: 0.4519178867340088 - trainLoss: 0.45879417657852173\n",
      "cnt: 0 - valLoss: 0.4519142806529999 - trainLoss: 0.45878997445106506\n",
      "cnt: 0 - valLoss: 0.4519108533859253 - trainLoss: 0.4587858319282532\n",
      "cnt: 0 - valLoss: 0.4519074559211731 - trainLoss: 0.4587816596031189\n",
      "cnt: 0 - valLoss: 0.45190390944480896 - trainLoss: 0.458777517080307\n",
      "cnt: 0 - valLoss: 0.4519004821777344 - trainLoss: 0.45877331495285034\n",
      "cnt: 0 - valLoss: 0.4518968462944031 - trainLoss: 0.45876917243003845\n",
      "cnt: 0 - valLoss: 0.45189350843429565 - trainLoss: 0.45876502990722656\n",
      "cnt: 0 - valLoss: 0.45188990235328674 - trainLoss: 0.4587608575820923\n",
      "cnt: 0 - valLoss: 0.45188647508621216 - trainLoss: 0.4587567150592804\n",
      "cnt: 0 - valLoss: 0.45188289880752563 - trainLoss: 0.4587525725364685\n",
      "cnt: 0 - valLoss: 0.4518795311450958 - trainLoss: 0.45874840021133423\n",
      "cnt: 0 - valLoss: 0.4518761932849884 - trainLoss: 0.45874425768852234\n",
      "cnt: 0 - valLoss: 0.4518725275993347 - trainLoss: 0.45874014496803284\n",
      "cnt: 0 - valLoss: 0.4518691599369049 - trainLoss: 0.45873594284057617\n",
      "cnt: 0 - valLoss: 0.451865553855896 - trainLoss: 0.4587317705154419\n",
      "cnt: 0 - valLoss: 0.4518622159957886 - trainLoss: 0.4587276577949524\n",
      "cnt: 0 - valLoss: 0.45185860991477966 - trainLoss: 0.4587234854698181\n",
      "cnt: 0 - valLoss: 0.4518551826477051 - trainLoss: 0.4587193429470062\n",
      "cnt: 0 - valLoss: 0.45185160636901855 - trainLoss: 0.45871520042419434\n",
      "cnt: 0 - valLoss: 0.45184823870658875 - trainLoss: 0.45871105790138245\n",
      "cnt: 0 - valLoss: 0.451844722032547 - trainLoss: 0.45870688557624817\n",
      "cnt: 0 - valLoss: 0.4518412947654724 - trainLoss: 0.45870277285575867\n",
      "cnt: 0 - valLoss: 0.451837956905365 - trainLoss: 0.4586986303329468\n",
      "cnt: 0 - valLoss: 0.4518343508243561 - trainLoss: 0.4586944878101349\n",
      "cnt: 0 - valLoss: 0.4518309533596039 - trainLoss: 0.4586903154850006\n",
      "cnt: 0 - valLoss: 0.45182734727859497 - trainLoss: 0.4586861729621887\n",
      "cnt: 0 - valLoss: 0.45182400941848755 - trainLoss: 0.45868203043937683\n",
      "cnt: 0 - valLoss: 0.4518204629421234 - trainLoss: 0.45867791771888733\n",
      "cnt: 0 - valLoss: 0.45181703567504883 - trainLoss: 0.45867371559143066\n",
      "cnt: 0 - valLoss: 0.4518134295940399 - trainLoss: 0.45866960287094116\n",
      "cnt: 0 - valLoss: 0.4518101215362549 - trainLoss: 0.4586654603481293\n",
      "cnt: 0 - valLoss: 0.4518067538738251 - trainLoss: 0.4586613178253174\n",
      "cnt: 0 - valLoss: 0.45180317759513855 - trainLoss: 0.4586572051048279\n",
      "cnt: 0 - valLoss: 0.45179980993270874 - trainLoss: 0.4586530029773712\n",
      "cnt: 0 - valLoss: 0.4517962336540222 - trainLoss: 0.4586488902568817\n",
      "cnt: 0 - valLoss: 0.4517928659915924 - trainLoss: 0.4586447775363922\n",
      "cnt: 0 - valLoss: 0.45178934931755066 - trainLoss: 0.45864060521125793\n",
      "cnt: 0 - valLoss: 0.4517859220504761 - trainLoss: 0.4586365222930908\n",
      "cnt: 0 - valLoss: 0.45178234577178955 - trainLoss: 0.45863237977027893\n",
      "cnt: 0 - valLoss: 0.45177900791168213 - trainLoss: 0.45862823724746704\n",
      "cnt: 0 - valLoss: 0.4517756998538971 - trainLoss: 0.45862406492233276\n",
      "cnt: 0 - valLoss: 0.45177212357521057 - trainLoss: 0.45862001180648804\n",
      "cnt: 0 - valLoss: 0.45176875591278076 - trainLoss: 0.45861580967903137\n",
      "cnt: 0 - valLoss: 0.45176517963409424 - trainLoss: 0.45861169695854187\n",
      "cnt: 0 - valLoss: 0.45176181197166443 - trainLoss: 0.45860755443573\n",
      "cnt: 0 - valLoss: 0.4517582952976227 - trainLoss: 0.4586034417152405\n",
      "cnt: 0 - valLoss: 0.4517548680305481 - trainLoss: 0.4585992991924286\n",
      "cnt: 0 - valLoss: 0.45175135135650635 - trainLoss: 0.4585951268672943\n",
      "cnt: 0 - valLoss: 0.45174798369407654 - trainLoss: 0.4585910141468048\n",
      "cnt: 0 - valLoss: 0.4517446458339691 - trainLoss: 0.4585869312286377\n",
      "cnt: 0 - valLoss: 0.4517410695552826 - trainLoss: 0.4585827589035034\n",
      "cnt: 0 - valLoss: 0.45173773169517517 - trainLoss: 0.45857861638069153\n",
      "cnt: 0 - valLoss: 0.45173412561416626 - trainLoss: 0.4585745632648468\n",
      "cnt: 0 - valLoss: 0.45173078775405884 - trainLoss: 0.4585703909397125\n",
      "cnt: 0 - valLoss: 0.4517272710800171 - trainLoss: 0.458566278219223\n",
      "cnt: 0 - valLoss: 0.45172393321990967 - trainLoss: 0.4585621654987335\n",
      "cnt: 0 - valLoss: 0.4517204165458679 - trainLoss: 0.45855802297592163\n",
      "cnt: 0 - valLoss: 0.45171698927879333 - trainLoss: 0.45855391025543213\n",
      "cnt: 0 - valLoss: 0.4517136812210083 - trainLoss: 0.45854976773262024\n",
      "cnt: 0 - valLoss: 0.45171013474464417 - trainLoss: 0.4585457146167755\n",
      "cnt: 0 - valLoss: 0.45170679688453674 - trainLoss: 0.45854151248931885\n",
      "cnt: 0 - valLoss: 0.4517032206058502 - trainLoss: 0.45853739976882935\n",
      "cnt: 0 - valLoss: 0.4516999423503876 - trainLoss: 0.45853328704833984\n",
      "cnt: 0 - valLoss: 0.45169636607170105 - trainLoss: 0.45852917432785034\n",
      "cnt: 0 - valLoss: 0.45169302821159363 - trainLoss: 0.45852506160736084\n",
      "cnt: 0 - valLoss: 0.4516895115375519 - trainLoss: 0.45852091908454895\n",
      "cnt: 0 - valLoss: 0.45168614387512207 - trainLoss: 0.45851680636405945\n",
      "cnt: 0 - valLoss: 0.45168283581733704 - trainLoss: 0.45851269364356995\n",
      "cnt: 0 - valLoss: 0.4516792595386505 - trainLoss: 0.4585086405277252\n",
      "cnt: 0 - valLoss: 0.4516759216785431 - trainLoss: 0.45850446820259094\n",
      "cnt: 0 - valLoss: 0.4516724646091461 - trainLoss: 0.45850038528442383\n",
      "cnt: 0 - valLoss: 0.4516691267490387 - trainLoss: 0.4584963023662567\n",
      "cnt: 0 - valLoss: 0.45166561007499695 - trainLoss: 0.4584921598434448\n",
      "cnt: 0 - valLoss: 0.45166221261024475 - trainLoss: 0.4584880471229553\n",
      "cnt: 0 - valLoss: 0.451658695936203 - trainLoss: 0.4584839344024658\n",
      "cnt: 0 - valLoss: 0.4516553580760956 - trainLoss: 0.4584798216819763\n",
      "cnt: 0 - valLoss: 0.451651930809021 - trainLoss: 0.4584757089614868\n",
      "cnt: 0 - valLoss: 0.4516485333442688 - trainLoss: 0.4584716558456421\n",
      "cnt: 0 - valLoss: 0.45164528489112854 - trainLoss: 0.4584675133228302\n",
      "cnt: 0 - valLoss: 0.4516417384147644 - trainLoss: 0.4584634006023407\n",
      "cnt: 0 - valLoss: 0.45163843035697937 - trainLoss: 0.4584593176841736\n",
      "cnt: 0 - valLoss: 0.45163485407829285 - trainLoss: 0.4584551751613617\n",
      "cnt: 0 - valLoss: 0.4516316056251526 - trainLoss: 0.45845112204551697\n",
      "cnt: 0 - valLoss: 0.45162808895111084 - trainLoss: 0.4584469497203827\n",
      "cnt: 0 - valLoss: 0.4516247808933258 - trainLoss: 0.45844289660453796\n",
      "cnt: 0 - valLoss: 0.45162129402160645 - trainLoss: 0.45843881368637085\n",
      "cnt: 0 - valLoss: 0.4516179859638214 - trainLoss: 0.45843473076820374\n",
      "cnt: 0 - valLoss: 0.451614648103714 - trainLoss: 0.45843061804771423\n",
      "cnt: 0 - valLoss: 0.45161116123199463 - trainLoss: 0.45842650532722473\n",
      "cnt: 0 - valLoss: 0.4516078531742096 - trainLoss: 0.45842239260673523\n",
      "cnt: 0 - valLoss: 0.4516043961048126 - trainLoss: 0.4584182798862457\n",
      "cnt: 0 - valLoss: 0.4516009986400604 - trainLoss: 0.458414226770401\n",
      "cnt: 0 - valLoss: 0.45159757137298584 - trainLoss: 0.4584101140499115\n",
      "cnt: 0 - valLoss: 0.4515942335128784 - trainLoss: 0.4584060311317444\n",
      "cnt: 0 - valLoss: 0.45159080624580383 - trainLoss: 0.4584019184112549\n",
      "cnt: 0 - valLoss: 0.4515874683856964 - trainLoss: 0.45839783549308777\n",
      "cnt: 0 - valLoss: 0.45158421993255615 - trainLoss: 0.45839372277259827\n",
      "cnt: 0 - valLoss: 0.45158064365386963 - trainLoss: 0.45838963985443115\n",
      "cnt: 0 - valLoss: 0.4515773355960846 - trainLoss: 0.45838552713394165\n",
      "cnt: 0 - valLoss: 0.4515738785266876 - trainLoss: 0.4583814740180969\n",
      "cnt: 0 - valLoss: 0.4515705704689026 - trainLoss: 0.4583773612976074\n",
      "cnt: 0 - valLoss: 0.4515671133995056 - trainLoss: 0.4583732485771179\n",
      "cnt: 0 - valLoss: 0.4515637457370758 - trainLoss: 0.4583692252635956\n",
      "cnt: 0 - valLoss: 0.4515605568885803 - trainLoss: 0.4583651125431061\n",
      "cnt: 0 - valLoss: 0.4515569508075714 - trainLoss: 0.4583609998226166\n",
      "cnt: 0 - valLoss: 0.45155370235443115 - trainLoss: 0.45835691690444946\n",
      "cnt: 0 - valLoss: 0.4515501856803894 - trainLoss: 0.4583528935909271\n",
      "cnt: 0 - valLoss: 0.45154696702957153 - trainLoss: 0.4583487808704376\n",
      "cnt: 0 - valLoss: 0.4515434503555298 - trainLoss: 0.4583446681499481\n",
      "cnt: 0 - valLoss: 0.45154014229774475 - trainLoss: 0.458340585231781\n",
      "cnt: 0 - valLoss: 0.45153671503067017 - trainLoss: 0.4583364725112915\n",
      "cnt: 0 - valLoss: 0.45153340697288513 - trainLoss: 0.4583324193954468\n",
      "cnt: 0 - valLoss: 0.4515300989151001 - trainLoss: 0.4583283066749573\n",
      "cnt: 0 - valLoss: 0.4515266418457031 - trainLoss: 0.45832428336143494\n",
      "cnt: 0 - valLoss: 0.4515233337879181 - trainLoss: 0.45832017064094543\n",
      "cnt: 0 - valLoss: 0.4515199065208435 - trainLoss: 0.4583161175251007\n",
      "cnt: 0 - valLoss: 0.45151659846305847 - trainLoss: 0.4583120048046112\n",
      "cnt: 0 - valLoss: 0.4515131413936615 - trainLoss: 0.4583079516887665\n",
      "cnt: 0 - valLoss: 0.45150989294052124 - trainLoss: 0.458303838968277\n",
      "cnt: 0 - valLoss: 0.4515064060688019 - trainLoss: 0.45829981565475464\n",
      "cnt: 0 - valLoss: 0.45150309801101685 - trainLoss: 0.45829570293426514\n",
      "cnt: 0 - valLoss: 0.4514998495578766 - trainLoss: 0.45829159021377563\n",
      "cnt: 0 - valLoss: 0.4514963924884796 - trainLoss: 0.4582875669002533\n",
      "cnt: 0 - valLoss: 0.4514930844306946 - trainLoss: 0.4582834541797638\n",
      "cnt: 0 - valLoss: 0.45148965716362 - trainLoss: 0.45827940106391907\n",
      "cnt: 0 - valLoss: 0.4514863193035126 - trainLoss: 0.45827531814575195\n",
      "cnt: 0 - valLoss: 0.451482892036438 - trainLoss: 0.45827120542526245\n",
      "cnt: 0 - valLoss: 0.45147958397865295 - trainLoss: 0.4582671821117401\n",
      "cnt: 0 - valLoss: 0.45147618651390076 - trainLoss: 0.4582630693912506\n",
      "cnt: 0 - valLoss: 0.45147284865379333 - trainLoss: 0.45825904607772827\n",
      "cnt: 0 - valLoss: 0.4514696002006531 - trainLoss: 0.45825496315956116\n",
      "cnt: 0 - valLoss: 0.4514661729335785 - trainLoss: 0.4582509398460388\n",
      "cnt: 0 - valLoss: 0.45146289467811584 - trainLoss: 0.4582468271255493\n",
      "cnt: 0 - valLoss: 0.4514593780040741 - trainLoss: 0.4582427144050598\n",
      "cnt: 0 - valLoss: 0.4514561891555786 - trainLoss: 0.4582386910915375\n",
      "cnt: 0 - valLoss: 0.45145270228385925 - trainLoss: 0.45823460817337036\n",
      "cnt: 0 - valLoss: 0.451449453830719 - trainLoss: 0.45823055505752563\n",
      "cnt: 0 - valLoss: 0.4514460265636444 - trainLoss: 0.4582264721393585\n",
      "cnt: 0 - valLoss: 0.45144274830818176 - trainLoss: 0.4582224190235138\n",
      "cnt: 0 - valLoss: 0.4514394998550415 - trainLoss: 0.45821836590766907\n",
      "cnt: 0 - valLoss: 0.45143604278564453 - trainLoss: 0.45821428298950195\n",
      "cnt: 0 - valLoss: 0.4514327645301819 - trainLoss: 0.4582102596759796\n",
      "cnt: 0 - valLoss: 0.4514293372631073 - trainLoss: 0.4582061469554901\n",
      "cnt: 0 - valLoss: 0.45142611861228943 - trainLoss: 0.4582020938396454\n",
      "cnt: 0 - valLoss: 0.45142266154289246 - trainLoss: 0.45819810032844543\n",
      "cnt: 0 - valLoss: 0.4514193534851074 - trainLoss: 0.45819398760795593\n",
      "cnt: 0 - valLoss: 0.45141592621803284 - trainLoss: 0.4581899046897888\n",
      "cnt: 0 - valLoss: 0.4514126777648926 - trainLoss: 0.45818591117858887\n",
      "cnt: 0 - valLoss: 0.4514094293117523 - trainLoss: 0.45818179845809937\n",
      "cnt: 0 - valLoss: 0.45140597224235535 - trainLoss: 0.45817774534225464\n",
      "cnt: 0 - valLoss: 0.4514027237892151 - trainLoss: 0.4581737220287323\n",
      "cnt: 0 - valLoss: 0.4513992965221405 - trainLoss: 0.4581696391105652\n",
      "cnt: 0 - valLoss: 0.45139604806900024 - trainLoss: 0.45816558599472046\n",
      "cnt: 0 - valLoss: 0.45139262080192566 - trainLoss: 0.4581615626811981\n",
      "cnt: 0 - valLoss: 0.4513893127441406 - trainLoss: 0.4581574499607086\n",
      "cnt: 0 - valLoss: 0.4513859152793884 - trainLoss: 0.4581534266471863\n",
      "cnt: 0 - valLoss: 0.45138266682624817 - trainLoss: 0.45814940333366394\n",
      "cnt: 0 - valLoss: 0.4513794481754303 - trainLoss: 0.45814529061317444\n",
      "cnt: 0 - valLoss: 0.4513759911060333 - trainLoss: 0.4581412672996521\n",
      "cnt: 0 - valLoss: 0.4513727128505707 - trainLoss: 0.4581371545791626\n",
      "cnt: 0 - valLoss: 0.45136934518814087 - trainLoss: 0.45813313126564026\n",
      "cnt: 0 - valLoss: 0.451366126537323 - trainLoss: 0.4581291079521179\n",
      "cnt: 0 - valLoss: 0.4513626992702484 - trainLoss: 0.4581250548362732\n",
      "cnt: 0 - valLoss: 0.4513593912124634 - trainLoss: 0.4581209719181061\n",
      "cnt: 0 - valLoss: 0.4513561725616455 - trainLoss: 0.45811694860458374\n",
      "cnt: 0 - valLoss: 0.45135265588760376 - trainLoss: 0.4581129252910614\n",
      "cnt: 0 - valLoss: 0.4513494372367859 - trainLoss: 0.4581088125705719\n",
      "cnt: 0 - valLoss: 0.4513459801673889 - trainLoss: 0.45810478925704956\n",
      "cnt: 0 - valLoss: 0.45134276151657104 - trainLoss: 0.4581007659435272\n",
      "cnt: 0 - valLoss: 0.4513393044471741 - trainLoss: 0.4580967426300049\n",
      "cnt: 0 - valLoss: 0.4513360261917114 - trainLoss: 0.45809268951416016\n",
      "cnt: 0 - valLoss: 0.4513326585292816 - trainLoss: 0.4580886662006378\n",
      "cnt: 0 - valLoss: 0.4513293504714966 - trainLoss: 0.4580846130847931\n",
      "cnt: 0 - valLoss: 0.4513261318206787 - trainLoss: 0.45808058977127075\n",
      "cnt: 0 - valLoss: 0.45132267475128174 - trainLoss: 0.4580765664577484\n",
      "cnt: 0 - valLoss: 0.4513193964958191 - trainLoss: 0.4580724835395813\n",
      "cnt: 0 - valLoss: 0.4513159692287445 - trainLoss: 0.45806846022605896\n",
      "cnt: 0 - valLoss: 0.45131272077560425 - trainLoss: 0.45806440711021423\n",
      "cnt: 0 - valLoss: 0.45130929350852966 - trainLoss: 0.4580603837966919\n",
      "cnt: 0 - valLoss: 0.4513060450553894 - trainLoss: 0.45805636048316956\n",
      "cnt: 0 - valLoss: 0.4513026177883148 - trainLoss: 0.4580523371696472\n",
      "cnt: 0 - valLoss: 0.4512993395328522 - trainLoss: 0.4580483138561249\n",
      "cnt: 0 - valLoss: 0.4512961208820343 - trainLoss: 0.45804423093795776\n",
      "cnt: 0 - valLoss: 0.45129266381263733 - trainLoss: 0.45804017782211304\n",
      "cnt: 0 - valLoss: 0.45128944516181946 - trainLoss: 0.4580361545085907\n",
      "cnt: 0 - valLoss: 0.4512860178947449 - trainLoss: 0.45803213119506836\n",
      "cnt: 0 - valLoss: 0.4512827694416046 - trainLoss: 0.458028107881546\n",
      "cnt: 0 - valLoss: 0.45127934217453003 - trainLoss: 0.4580240845680237\n",
      "cnt: 0 - valLoss: 0.45127612352371216 - trainLoss: 0.45802006125450134\n",
      "cnt: 0 - valLoss: 0.4512728452682495 - trainLoss: 0.45801597833633423\n",
      "cnt: 0 - valLoss: 0.4512694180011749 - trainLoss: 0.45801201462745667\n",
      "cnt: 0 - valLoss: 0.45126619935035706 - trainLoss: 0.4580079913139343\n",
      "cnt: 0 - valLoss: 0.45126277208328247 - trainLoss: 0.458003968000412\n",
      "cnt: 0 - valLoss: 0.451259583234787 - trainLoss: 0.45799994468688965\n",
      "cnt: 0 - valLoss: 0.4512561559677124 - trainLoss: 0.45799586176872253\n",
      "cnt: 0 - valLoss: 0.45125287771224976 - trainLoss: 0.4579918384552002\n",
      "cnt: 0 - valLoss: 0.45124945044517517 - trainLoss: 0.45798781514167786\n",
      "cnt: 0 - valLoss: 0.4512462019920349 - trainLoss: 0.4579837918281555\n",
      "cnt: 0 - valLoss: 0.45124301314353943 - trainLoss: 0.45797982811927795\n",
      "cnt: 0 - valLoss: 0.45123958587646484 - trainLoss: 0.45797574520111084\n",
      "cnt: 0 - valLoss: 0.45123642683029175 - trainLoss: 0.4579717218875885\n",
      "cnt: 0 - valLoss: 0.45123291015625 - trainLoss: 0.45796769857406616\n",
      "cnt: 0 - valLoss: 0.4512297511100769 - trainLoss: 0.4579636752605438\n",
      "cnt: 0 - valLoss: 0.4512263238430023 - trainLoss: 0.45795971155166626\n",
      "cnt: 0 - valLoss: 0.45122310519218445 - trainLoss: 0.45795562863349915\n",
      "cnt: 0 - valLoss: 0.45121970772743225 - trainLoss: 0.4579516053199768\n",
      "cnt: 0 - valLoss: 0.451216459274292 - trainLoss: 0.45794764161109924\n",
      "cnt: 0 - valLoss: 0.4512132704257965 - trainLoss: 0.45794355869293213\n",
      "cnt: 0 - valLoss: 0.45120981335639954 - trainLoss: 0.45793959498405457\n",
      "cnt: 0 - valLoss: 0.45120662450790405 - trainLoss: 0.4579355716705322\n",
      "cnt: 0 - valLoss: 0.45120319724082947 - trainLoss: 0.4579315483570099\n",
      "cnt: 0 - valLoss: 0.4511999785900116 - trainLoss: 0.45792752504348755\n",
      "cnt: 0 - valLoss: 0.4511966407299042 - trainLoss: 0.4579235017299652\n",
      "cnt: 0 - valLoss: 0.4511933922767639 - trainLoss: 0.45791947841644287\n",
      "cnt: 0 - valLoss: 0.4511902332305908 - trainLoss: 0.45791545510292053\n",
      "cnt: 0 - valLoss: 0.45118677616119385 - trainLoss: 0.4579114615917206\n",
      "cnt: 0 - valLoss: 0.45118358731269836 - trainLoss: 0.457907497882843\n",
      "cnt: 0 - valLoss: 0.4511801600456238 - trainLoss: 0.4579034745693207\n",
      "cnt: 0 - valLoss: 0.4511769711971283 - trainLoss: 0.45789945125579834\n",
      "cnt: 0 - valLoss: 0.4511736035346985 - trainLoss: 0.457895427942276\n",
      "cnt: 0 - valLoss: 0.45117032527923584 - trainLoss: 0.45789140462875366\n",
      "cnt: 0 - valLoss: 0.4511669874191284 - trainLoss: 0.4578873813152313\n",
      "cnt: 0 - valLoss: 0.45116373896598816 - trainLoss: 0.45788341760635376\n",
      "cnt: 0 - valLoss: 0.4511605501174927 - trainLoss: 0.45787936449050903\n",
      "cnt: 0 - valLoss: 0.45115718245506287 - trainLoss: 0.4578753709793091\n",
      "cnt: 0 - valLoss: 0.451153963804245 - trainLoss: 0.45787134766578674\n",
      "cnt: 0 - valLoss: 0.4511505663394928 - trainLoss: 0.4578673839569092\n",
      "cnt: 0 - valLoss: 0.4511473476886749 - trainLoss: 0.45786336064338684\n",
      "cnt: 0 - valLoss: 0.4511439800262451 - trainLoss: 0.4578593373298645\n",
      "cnt: 0 - valLoss: 0.45114076137542725 - trainLoss: 0.45785534381866455\n",
      "cnt: 0 - valLoss: 0.4511374235153198 - trainLoss: 0.4578513205051422\n",
      "cnt: 0 - valLoss: 0.45113420486450195 - trainLoss: 0.45784735679626465\n",
      "cnt: 0 - valLoss: 0.45113101601600647 - trainLoss: 0.4578433334827423\n",
      "cnt: 0 - valLoss: 0.45112764835357666 - trainLoss: 0.45783933997154236\n",
      "cnt: 0 - valLoss: 0.4511244297027588 - trainLoss: 0.45783531665802\n",
      "cnt: 0 - valLoss: 0.4511210322380066 - trainLoss: 0.45783135294914246\n",
      "cnt: 0 - valLoss: 0.4511178731918335 - trainLoss: 0.4578273594379425\n",
      "cnt: 0 - valLoss: 0.4511144757270813 - trainLoss: 0.45782333612442017\n",
      "cnt: 0 - valLoss: 0.4511112570762634 - trainLoss: 0.4578193128108978\n",
      "cnt: 0 - valLoss: 0.45110809803009033 - trainLoss: 0.45781537890434265\n",
      "cnt: 0 - valLoss: 0.4511047601699829 - trainLoss: 0.4578113555908203\n",
      "cnt: 0 - valLoss: 0.4511015713214874 - trainLoss: 0.457807332277298\n",
      "cnt: 0 - valLoss: 0.45109814405441284 - trainLoss: 0.457803338766098\n",
      "cnt: 0 - valLoss: 0.45109498500823975 - trainLoss: 0.45779937505722046\n",
      "cnt: 0 - valLoss: 0.4510916471481323 - trainLoss: 0.4577953517436981\n",
      "cnt: 0 - valLoss: 0.45108842849731445 - trainLoss: 0.45779135823249817\n",
      "cnt: 0 - valLoss: 0.45108509063720703 - trainLoss: 0.4577873945236206\n",
      "cnt: 0 - valLoss: 0.45108190178871155 - trainLoss: 0.45778340101242065\n",
      "cnt: 0 - valLoss: 0.45107874274253845 - trainLoss: 0.4577793776988983\n",
      "cnt: 0 - valLoss: 0.45107534527778625 - trainLoss: 0.45777541399002075\n",
      "cnt: 0 - valLoss: 0.45107218623161316 - trainLoss: 0.4577713906764984\n",
      "cnt: 0 - valLoss: 0.45106878876686096 - trainLoss: 0.45776739716529846\n",
      "cnt: 0 - valLoss: 0.4510655701160431 - trainLoss: 0.4577634036540985\n",
      "cnt: 0 - valLoss: 0.45106229186058044 - trainLoss: 0.45775938034057617\n",
      "cnt: 0 - valLoss: 0.4510590732097626 - trainLoss: 0.457755446434021\n",
      "cnt: 0 - valLoss: 0.4510558843612671 - trainLoss: 0.45775142312049866\n",
      "cnt: 0 - valLoss: 0.45105254650115967 - trainLoss: 0.4577474594116211\n",
      "cnt: 0 - valLoss: 0.4510493874549866 - trainLoss: 0.45774343609809875\n",
      "cnt: 0 - valLoss: 0.45104601979255676 - trainLoss: 0.4577395021915436\n",
      "cnt: 0 - valLoss: 0.45104286074638367 - trainLoss: 0.45773550868034363\n",
      "cnt: 0 - valLoss: 0.45103946328163147 - trainLoss: 0.4577314853668213\n",
      "cnt: 0 - valLoss: 0.45103633403778076 - trainLoss: 0.45772749185562134\n",
      "cnt: 0 - valLoss: 0.45103296637535095 - trainLoss: 0.4577235281467438\n",
      "cnt: 0 - valLoss: 0.45102983713150024 - trainLoss: 0.4577195942401886\n",
      "cnt: 0 - valLoss: 0.45102664828300476 - trainLoss: 0.45771557092666626\n",
      "cnt: 0 - valLoss: 0.45102331042289734 - trainLoss: 0.4577116370201111\n",
      "cnt: 0 - valLoss: 0.45102015137672424 - trainLoss: 0.45770764350891113\n",
      "cnt: 0 - valLoss: 0.4510168135166168 - trainLoss: 0.4577036499977112\n",
      "cnt: 0 - valLoss: 0.45101362466812134 - trainLoss: 0.4576996862888336\n",
      "cnt: 0 - valLoss: 0.4510102868080139 - trainLoss: 0.4576956629753113\n",
      "cnt: 0 - valLoss: 0.4510071277618408 - trainLoss: 0.45769166946411133\n",
      "cnt: 0 - valLoss: 0.4510040283203125 - trainLoss: 0.45768770575523376\n",
      "cnt: 0 - valLoss: 0.4510006010532379 - trainLoss: 0.4576837122440338\n",
      "cnt: 0 - valLoss: 0.4509974718093872 - trainLoss: 0.45767977833747864\n",
      "cnt: 0 - valLoss: 0.4509941339492798 - trainLoss: 0.4576758146286011\n",
      "cnt: 0 - valLoss: 0.4509910047054291 - trainLoss: 0.4576718211174011\n",
      "cnt: 0 - valLoss: 0.45098766684532166 - trainLoss: 0.45766782760620117\n",
      "cnt: 0 - valLoss: 0.4509844481945038 - trainLoss: 0.4576638638973236\n",
      "cnt: 0 - valLoss: 0.45098114013671875 - trainLoss: 0.45765992999076843\n",
      "cnt: 0 - valLoss: 0.45097798109054565 - trainLoss: 0.4576559066772461\n",
      "cnt: 0 - valLoss: 0.45097485184669495 - trainLoss: 0.45765191316604614\n",
      "cnt: 0 - valLoss: 0.4509715437889099 - trainLoss: 0.4576479494571686\n",
      "cnt: 0 - valLoss: 0.45096835494041443 - trainLoss: 0.457643985748291\n",
      "cnt: 0 - valLoss: 0.450965017080307 - trainLoss: 0.45764002203941345\n",
      "cnt: 0 - valLoss: 0.4509618580341339 - trainLoss: 0.4576360285282135\n",
      "cnt: 0 - valLoss: 0.4509585201740265 - trainLoss: 0.4576320946216583\n",
      "cnt: 0 - valLoss: 0.4509553909301758 - trainLoss: 0.45762813091278076\n",
      "cnt: 0 - valLoss: 0.45095229148864746 - trainLoss: 0.4576241374015808\n",
      "cnt: 0 - valLoss: 0.45094889402389526 - trainLoss: 0.45762020349502563\n",
      "cnt: 0 - valLoss: 0.45094582438468933 - trainLoss: 0.4576161801815033\n",
      "cnt: 0 - valLoss: 0.45094242691993713 - trainLoss: 0.4576122462749481\n",
      "cnt: 0 - valLoss: 0.4509392976760864 - trainLoss: 0.45760831236839294\n",
      "cnt: 0 - valLoss: 0.45093604922294617 - trainLoss: 0.4576042890548706\n",
      "cnt: 0 - valLoss: 0.4509328305721283 - trainLoss: 0.45760035514831543\n",
      "cnt: 0 - valLoss: 0.45092979073524475 - trainLoss: 0.45759639143943787\n",
      "cnt: 0 - valLoss: 0.45092645287513733 - trainLoss: 0.4575923681259155\n",
      "cnt: 0 - valLoss: 0.45092326402664185 - trainLoss: 0.45758843421936035\n",
      "cnt: 0 - valLoss: 0.4509199857711792 - trainLoss: 0.4575845003128052\n",
      "cnt: 0 - valLoss: 0.4509167969226837 - trainLoss: 0.45758056640625\n",
      "cnt: 0 - valLoss: 0.45091351866722107 - trainLoss: 0.45757657289505005\n",
      "cnt: 0 - valLoss: 0.45091038942337036 - trainLoss: 0.4575726091861725\n",
      "cnt: 0 - valLoss: 0.4509070813655853 - trainLoss: 0.4575686752796173\n",
      "cnt: 0 - valLoss: 0.45090389251708984 - trainLoss: 0.45756465196609497\n",
      "cnt: 0 - valLoss: 0.4509008228778839 - trainLoss: 0.4575607180595398\n",
      "cnt: 0 - valLoss: 0.4508974850177765 - trainLoss: 0.45755675435066223\n",
      "cnt: 0 - valLoss: 0.45089438557624817 - trainLoss: 0.45755282044410706\n",
      "cnt: 0 - valLoss: 0.45089104771614075 - trainLoss: 0.4575488567352295\n",
      "cnt: 0 - valLoss: 0.45088791847229004 - trainLoss: 0.45754486322402954\n",
      "cnt: 0 - valLoss: 0.450884610414505 - trainLoss: 0.45754092931747437\n",
      "cnt: 0 - valLoss: 0.4508814513683319 - trainLoss: 0.4575369954109192\n",
      "cnt: 0 - valLoss: 0.45087841153144836 - trainLoss: 0.45753297209739685\n",
      "cnt: 0 - valLoss: 0.45087504386901855 - trainLoss: 0.45752906799316406\n",
      "cnt: 0 - valLoss: 0.45087191462516785 - trainLoss: 0.4575251042842865\n",
      "cnt: 0 - valLoss: 0.4508686661720276 - trainLoss: 0.4575211703777313\n",
      "cnt: 0 - valLoss: 0.4508655369281769 - trainLoss: 0.45751717686653137\n",
      "cnt: 0 - valLoss: 0.45086222887039185 - trainLoss: 0.4575132727622986\n",
      "cnt: 0 - valLoss: 0.4508591592311859 - trainLoss: 0.4575093388557434\n",
      "cnt: 0 - valLoss: 0.4508558213710785 - trainLoss: 0.45750531554222107\n",
      "cnt: 0 - valLoss: 0.4508526921272278 - trainLoss: 0.4575013816356659\n",
      "cnt: 0 - valLoss: 0.45084965229034424 - trainLoss: 0.4574974477291107\n",
      "cnt: 0 - valLoss: 0.45084628462791443 - trainLoss: 0.45749351382255554\n",
      "cnt: 0 - valLoss: 0.4508431553840637 - trainLoss: 0.4574895203113556\n",
      "cnt: 0 - valLoss: 0.45083990693092346 - trainLoss: 0.4574855864048004\n",
      "cnt: 0 - valLoss: 0.45083677768707275 - trainLoss: 0.4574816823005676\n",
      "cnt: 0 - valLoss: 0.4508335292339325 - trainLoss: 0.4574776589870453\n",
      "cnt: 0 - valLoss: 0.450830340385437 - trainLoss: 0.4574737250804901\n",
      "cnt: 0 - valLoss: 0.4508272707462311 - trainLoss: 0.45746979117393494\n",
      "cnt: 0 - valLoss: 0.45082396268844604 - trainLoss: 0.45746588706970215\n",
      "cnt: 0 - valLoss: 0.4508209228515625 - trainLoss: 0.4574619233608246\n",
      "cnt: 0 - valLoss: 0.4508175849914551 - trainLoss: 0.4574579894542694\n",
      "cnt: 0 - valLoss: 0.45081448554992676 - trainLoss: 0.45745405554771423\n",
      "cnt: 0 - valLoss: 0.4508112370967865 - trainLoss: 0.45745012164115906\n",
      "cnt: 0 - valLoss: 0.4508080780506134 - trainLoss: 0.4574461579322815\n",
      "cnt: 0 - valLoss: 0.45080506801605225 - trainLoss: 0.45744219422340393\n",
      "cnt: 0 - valLoss: 0.4508017301559448 - trainLoss: 0.45743829011917114\n",
      "cnt: 0 - valLoss: 0.4507986307144165 - trainLoss: 0.4574343264102936\n",
      "cnt: 0 - valLoss: 0.45079538226127625 - trainLoss: 0.4574304223060608\n",
      "cnt: 0 - valLoss: 0.45079219341278076 - trainLoss: 0.4574264883995056\n",
      "cnt: 0 - valLoss: 0.4507890045642853 - trainLoss: 0.45742255449295044\n",
      "cnt: 0 - valLoss: 0.45078590512275696 - trainLoss: 0.4574185609817505\n",
      "cnt: 0 - valLoss: 0.4507825970649719 - trainLoss: 0.4574146270751953\n",
      "cnt: 0 - valLoss: 0.450779527425766 - trainLoss: 0.4574107229709625\n",
      "cnt: 0 - valLoss: 0.45077642798423767 - trainLoss: 0.4574066996574402\n",
      "cnt: 0 - valLoss: 0.4507731795310974 - trainLoss: 0.4574028551578522\n",
      "cnt: 0 - valLoss: 0.4507700502872467 - trainLoss: 0.457398921251297\n",
      "cnt: 0 - valLoss: 0.45076680183410645 - trainLoss: 0.4573949873447418\n",
      "cnt: 0 - valLoss: 0.45076367259025574 - trainLoss: 0.45739105343818665\n",
      "cnt: 0 - valLoss: 0.45076045393943787 - trainLoss: 0.4573870897293091\n",
      "cnt: 0 - valLoss: 0.45075735449790955 - trainLoss: 0.4573831558227539\n",
      "cnt: 0 - valLoss: 0.45075422525405884 - trainLoss: 0.45737922191619873\n",
      "cnt: 0 - valLoss: 0.4507509768009186 - trainLoss: 0.45737528800964355\n",
      "cnt: 0 - valLoss: 0.45074793696403503 - trainLoss: 0.4573713541030884\n",
      "cnt: 0 - valLoss: 0.4507445991039276 - trainLoss: 0.4573674201965332\n",
      "cnt: 0 - valLoss: 0.4507415294647217 - trainLoss: 0.4573635160923004\n",
      "cnt: 0 - valLoss: 0.4507382810115814 - trainLoss: 0.45735958218574524\n",
      "cnt: 0 - valLoss: 0.4507351517677307 - trainLoss: 0.45735567808151245\n",
      "cnt: 0 - valLoss: 0.45073211193084717 - trainLoss: 0.4573517143726349\n",
      "cnt: 0 - valLoss: 0.45072877407073975 - trainLoss: 0.4573478102684021\n",
      "cnt: 0 - valLoss: 0.4507256746292114 - trainLoss: 0.4573438763618469\n",
      "cnt: 0 - valLoss: 0.4507223963737488 - trainLoss: 0.45733994245529175\n",
      "cnt: 0 - valLoss: 0.45071926712989807 - trainLoss: 0.45733603835105896\n",
      "cnt: 0 - valLoss: 0.4507159888744354 - trainLoss: 0.45733213424682617\n",
      "cnt: 0 - valLoss: 0.4507129192352295 - trainLoss: 0.457328200340271\n",
      "cnt: 0 - valLoss: 0.45070964097976685 - trainLoss: 0.4573242664337158\n",
      "cnt: 0 - valLoss: 0.45070648193359375 - trainLoss: 0.45732033252716064\n",
      "cnt: 0 - valLoss: 0.4507034420967102 - trainLoss: 0.45731639862060547\n",
      "cnt: 0 - valLoss: 0.45070013403892517 - trainLoss: 0.45731255412101746\n",
      "cnt: 0 - valLoss: 0.4506969749927521 - trainLoss: 0.4573085606098175\n",
      "cnt: 0 - valLoss: 0.45069369673728943 - trainLoss: 0.45730462670326233\n",
      "cnt: 0 - valLoss: 0.4506905674934387 - trainLoss: 0.45730072259902954\n",
      "cnt: 0 - valLoss: 0.45068737864494324 - trainLoss: 0.457296758890152\n",
      "cnt: 0 - valLoss: 0.4506842792034149 - trainLoss: 0.4572928547859192\n",
      "cnt: 0 - valLoss: 0.4506811797618866 - trainLoss: 0.457288920879364\n",
      "cnt: 0 - valLoss: 0.4506778419017792 - trainLoss: 0.45728498697280884\n",
      "cnt: 0 - valLoss: 0.4506748914718628 - trainLoss: 0.45728105306625366\n",
      "cnt: 0 - valLoss: 0.45067155361175537 - trainLoss: 0.4572770893573761\n",
      "cnt: 0 - valLoss: 0.45066845417022705 - trainLoss: 0.4572731554508209\n",
      "cnt: 0 - valLoss: 0.4506651759147644 - trainLoss: 0.45726922154426575\n",
      "cnt: 0 - valLoss: 0.4506620764732361 - trainLoss: 0.45726531744003296\n",
      "cnt: 0 - valLoss: 0.4506590664386749 - trainLoss: 0.4572613537311554\n",
      "cnt: 0 - valLoss: 0.4506557285785675 - trainLoss: 0.4572574496269226\n",
      "cnt: 0 - valLoss: 0.4506526589393616 - trainLoss: 0.4572535753250122\n",
      "cnt: 0 - valLoss: 0.45064935088157654 - trainLoss: 0.45724961161613464\n",
      "cnt: 0 - valLoss: 0.4506463408470154 - trainLoss: 0.45724567770957947\n",
      "cnt: 0 - valLoss: 0.4506430923938751 - trainLoss: 0.4572417438030243\n",
      "cnt: 0 - valLoss: 0.4506399929523468 - trainLoss: 0.4572378098964691\n",
      "cnt: 0 - valLoss: 0.45063692331314087 - trainLoss: 0.45723387598991394\n",
      "cnt: 0 - valLoss: 0.45063361525535583 - trainLoss: 0.4572300314903259\n",
      "cnt: 0 - valLoss: 0.45063066482543945 - trainLoss: 0.457226037979126\n",
      "cnt: 0 - valLoss: 0.4506273567676544 - trainLoss: 0.4572221338748932\n",
      "cnt: 0 - valLoss: 0.4506242871284485 - trainLoss: 0.457218199968338\n",
      "cnt: 0 - valLoss: 0.45062097907066345 - trainLoss: 0.45721426606178284\n",
      "cnt: 0 - valLoss: 0.45061802864074707 - trainLoss: 0.45721036195755005\n",
      "cnt: 0 - valLoss: 0.4506147503852844 - trainLoss: 0.45720648765563965\n",
      "cnt: 0 - valLoss: 0.45061159133911133 - trainLoss: 0.4572025537490845\n",
      "cnt: 0 - valLoss: 0.45060858130455017 - trainLoss: 0.4571985900402069\n",
      "cnt: 0 - valLoss: 0.4506054222583771 - trainLoss: 0.4571947157382965\n",
      "cnt: 0 - valLoss: 0.45060229301452637 - trainLoss: 0.45719078183174133\n",
      "cnt: 0 - valLoss: 0.4505990147590637 - trainLoss: 0.45718687772750854\n",
      "cnt: 0 - valLoss: 0.450595885515213 - trainLoss: 0.45718294382095337\n",
      "cnt: 0 - valLoss: 0.45059269666671753 - trainLoss: 0.4571790397167206\n",
      "cnt: 0 - valLoss: 0.4505896270275116 - trainLoss: 0.4571751356124878\n",
      "cnt: 0 - valLoss: 0.4505866467952728 - trainLoss: 0.4571712017059326\n",
      "cnt: 0 - valLoss: 0.4505833387374878 - trainLoss: 0.45716726779937744\n",
      "cnt: 0 - valLoss: 0.45058029890060425 - trainLoss: 0.45716339349746704\n",
      "cnt: 0 - valLoss: 0.45057713985443115 - trainLoss: 0.45715948939323425\n",
      "cnt: 0 - valLoss: 0.45057404041290283 - trainLoss: 0.4571555554866791\n",
      "cnt: 0 - valLoss: 0.4505707919597626 - trainLoss: 0.4571516215801239\n",
      "cnt: 0 - valLoss: 0.45056766271591187 - trainLoss: 0.4571477472782135\n",
      "cnt: 0 - valLoss: 0.4505646824836731 - trainLoss: 0.4571438133716583\n",
      "cnt: 0 - valLoss: 0.45056143403053284 - trainLoss: 0.45713987946510315\n",
      "cnt: 0 - valLoss: 0.4505583643913269 - trainLoss: 0.45713597536087036\n",
      "cnt: 0 - valLoss: 0.45055508613586426 - trainLoss: 0.45713210105895996\n",
      "cnt: 0 - valLoss: 0.4505520462989807 - trainLoss: 0.4571281671524048\n",
      "cnt: 0 - valLoss: 0.45054891705513 - trainLoss: 0.4571242928504944\n",
      "cnt: 0 - valLoss: 0.4505458474159241 - trainLoss: 0.4571203589439392\n",
      "cnt: 0 - valLoss: 0.4505428075790405 - trainLoss: 0.45711642503738403\n",
      "cnt: 0 - valLoss: 0.4505394995212555 - trainLoss: 0.457112580537796\n",
      "cnt: 0 - valLoss: 0.4505365788936615 - trainLoss: 0.45710864663124084\n",
      "cnt: 0 - valLoss: 0.45053333044052124 - trainLoss: 0.45710474252700806\n",
      "cnt: 0 - valLoss: 0.4505302309989929 - trainLoss: 0.45710083842277527\n",
      "cnt: 0 - valLoss: 0.45052698254585266 - trainLoss: 0.4570969045162201\n",
      "cnt: 0 - valLoss: 0.45052406191825867 - trainLoss: 0.4570930004119873\n",
      "cnt: 0 - valLoss: 0.4505208134651184 - trainLoss: 0.4570891261100769\n",
      "cnt: 0 - valLoss: 0.4505177140235901 - trainLoss: 0.45708519220352173\n",
      "cnt: 0 - valLoss: 0.45051467418670654 - trainLoss: 0.45708131790161133\n",
      "cnt: 0 - valLoss: 0.4505114257335663 - trainLoss: 0.45707738399505615\n",
      "cnt: 0 - valLoss: 0.4505084753036499 - trainLoss: 0.45707350969314575\n",
      "cnt: 0 - valLoss: 0.45050525665283203 - trainLoss: 0.45706960558891296\n",
      "cnt: 0 - valLoss: 0.4505021572113037 - trainLoss: 0.4570657014846802\n",
      "cnt: 0 - valLoss: 0.45049893856048584 - trainLoss: 0.4570617973804474\n",
      "cnt: 0 - valLoss: 0.45049598813056946 - trainLoss: 0.457057923078537\n",
      "cnt: 0 - valLoss: 0.4504929482936859 - trainLoss: 0.4570540189743042\n",
      "cnt: 0 - valLoss: 0.45048969984054565 - trainLoss: 0.457050085067749\n",
      "cnt: 0 - valLoss: 0.4504866302013397 - trainLoss: 0.457046240568161\n",
      "cnt: 0 - valLoss: 0.450483500957489 - trainLoss: 0.45704230666160583\n",
      "cnt: 0 - valLoss: 0.45048052072525024 - trainLoss: 0.45703843235969543\n",
      "cnt: 0 - valLoss: 0.4504772424697876 - trainLoss: 0.45703449845314026\n",
      "cnt: 0 - valLoss: 0.45047417283058167 - trainLoss: 0.45703065395355225\n",
      "cnt: 0 - valLoss: 0.4504711627960205 - trainLoss: 0.45702672004699707\n",
      "cnt: 0 - valLoss: 0.4504680037498474 - trainLoss: 0.45702287554740906\n",
      "cnt: 0 - valLoss: 0.45046499371528625 - trainLoss: 0.4570189118385315\n",
      "cnt: 0 - valLoss: 0.450461745262146 - trainLoss: 0.4570150673389435\n",
      "cnt: 0 - valLoss: 0.45045870542526245 - trainLoss: 0.4570111334323883\n",
      "cnt: 0 - valLoss: 0.45045560598373413 - trainLoss: 0.4570072889328003\n",
      "cnt: 0 - valLoss: 0.4504525661468506 - trainLoss: 0.4570033848285675\n",
      "cnt: 0 - valLoss: 0.45044955611228943 - trainLoss: 0.4569994807243347\n",
      "cnt: 0 - valLoss: 0.45044630765914917 - trainLoss: 0.4569956362247467\n",
      "cnt: 0 - valLoss: 0.4504433870315552 - trainLoss: 0.45699170231819153\n",
      "cnt: 0 - valLoss: 0.4504401683807373 - trainLoss: 0.4569878578186035\n",
      "cnt: 0 - valLoss: 0.45043712854385376 - trainLoss: 0.45698392391204834\n",
      "cnt: 0 - valLoss: 0.4504339098930359 - trainLoss: 0.45698004961013794\n",
      "cnt: 0 - valLoss: 0.4504309594631195 - trainLoss: 0.45697617530822754\n",
      "cnt: 0 - valLoss: 0.4504278004169464 - trainLoss: 0.45697227120399475\n",
      "cnt: 0 - valLoss: 0.4504247009754181 - trainLoss: 0.45696842670440674\n",
      "cnt: 0 - valLoss: 0.4504217207431793 - trainLoss: 0.45696449279785156\n",
      "cnt: 0 - valLoss: 0.4504184424877167 - trainLoss: 0.45696061849594116\n",
      "cnt: 0 - valLoss: 0.45041558146476746 - trainLoss: 0.456956684589386\n",
      "cnt: 0 - valLoss: 0.4504123628139496 - trainLoss: 0.456952840089798\n",
      "cnt: 0 - valLoss: 0.45040929317474365 - trainLoss: 0.45694899559020996\n",
      "cnt: 0 - valLoss: 0.45040610432624817 - trainLoss: 0.4569450616836548\n",
      "cnt: 0 - valLoss: 0.4504031836986542 - trainLoss: 0.4569411873817444\n",
      "cnt: 0 - valLoss: 0.45040014386177063 - trainLoss: 0.456937313079834\n",
      "cnt: 0 - valLoss: 0.45039698481559753 - trainLoss: 0.4569334387779236\n",
      "cnt: 0 - valLoss: 0.4503939747810364 - trainLoss: 0.4569295644760132\n",
      "cnt: 0 - valLoss: 0.45039084553718567 - trainLoss: 0.4569256603717804\n",
      "cnt: 0 - valLoss: 0.4503878355026245 - trainLoss: 0.4569217562675476\n",
      "cnt: 0 - valLoss: 0.45038464665412903 - trainLoss: 0.4569179117679596\n",
      "cnt: 0 - valLoss: 0.45038163661956787 - trainLoss: 0.4569140672683716\n",
      "cnt: 0 - valLoss: 0.4503787159919739 - trainLoss: 0.4569101631641388\n",
      "cnt: 0 - valLoss: 0.450375497341156 - trainLoss: 0.456906259059906\n",
      "cnt: 0 - valLoss: 0.45037251710891724 - trainLoss: 0.456902414560318\n",
      "cnt: 0 - valLoss: 0.4503692388534546 - trainLoss: 0.4568985104560852\n",
      "cnt: 0 - valLoss: 0.4503662884235382 - trainLoss: 0.4568946361541748\n",
      "cnt: 0 - valLoss: 0.4503631889820099 - trainLoss: 0.4568907618522644\n",
      "cnt: 0 - valLoss: 0.4503602087497711 - trainLoss: 0.4568869173526764\n",
      "cnt: 0 - valLoss: 0.4503571689128876 - trainLoss: 0.4568830728530884\n",
      "cnt: 0 - valLoss: 0.4503539800643921 - trainLoss: 0.4568791687488556\n",
      "cnt: 0 - valLoss: 0.45035111904144287 - trainLoss: 0.4568752944469452\n",
      "cnt: 0 - valLoss: 0.450347900390625 - trainLoss: 0.4568714201450348\n",
      "cnt: 0 - valLoss: 0.45034489035606384 - trainLoss: 0.4568675756454468\n",
      "cnt: 0 - valLoss: 0.45034167170524597 - trainLoss: 0.4568636417388916\n",
      "cnt: 0 - valLoss: 0.45033881068229675 - trainLoss: 0.456859827041626\n",
      "cnt: 0 - valLoss: 0.4503358006477356 - trainLoss: 0.4568559229373932\n",
      "cnt: 0 - valLoss: 0.4503326117992401 - trainLoss: 0.4568520784378052\n",
      "cnt: 0 - valLoss: 0.45032963156700134 - trainLoss: 0.45684814453125\n",
      "cnt: 0 - valLoss: 0.4503265619277954 - trainLoss: 0.456844300031662\n",
      "cnt: 0 - valLoss: 0.45032352209091187 - trainLoss: 0.4568404257297516\n",
      "cnt: 0 - valLoss: 0.4503203332424164 - trainLoss: 0.4568365812301636\n",
      "cnt: 0 - valLoss: 0.4503173530101776 - trainLoss: 0.45683273673057556\n",
      "cnt: 0 - valLoss: 0.450314462184906 - trainLoss: 0.4568288028240204\n",
      "cnt: 0 - valLoss: 0.4503113031387329 - trainLoss: 0.45682498812675476\n",
      "cnt: 0 - valLoss: 0.45030829310417175 - trainLoss: 0.45682114362716675\n",
      "cnt: 0 - valLoss: 0.45030513405799866 - trainLoss: 0.45681723952293396\n",
      "cnt: 0 - valLoss: 0.45030221343040466 - trainLoss: 0.45681339502334595\n",
      "cnt: 0 - valLoss: 0.4502990245819092 - trainLoss: 0.45680955052375793\n",
      "cnt: 0 - valLoss: 0.4502960443496704 - trainLoss: 0.45680567622184753\n",
      "cnt: 0 - valLoss: 0.4502929449081421 - trainLoss: 0.45680180191993713\n",
      "cnt: 0 - valLoss: 0.4502900242805481 - trainLoss: 0.45679792761802673\n",
      "cnt: 0 - valLoss: 0.4502870440483093 - trainLoss: 0.4567940831184387\n",
      "cnt: 0 - valLoss: 0.45028385519981384 - trainLoss: 0.45679017901420593\n",
      "cnt: 0 - valLoss: 0.4502808749675751 - trainLoss: 0.4567863345146179\n",
      "cnt: 0 - valLoss: 0.4502776861190796 - trainLoss: 0.4567824900150299\n",
      "cnt: 0 - valLoss: 0.45027482509613037 - trainLoss: 0.4567786753177643\n",
      "cnt: 0 - valLoss: 0.4502716362476349 - trainLoss: 0.4567747712135315\n",
      "cnt: 0 - valLoss: 0.4502686858177185 - trainLoss: 0.4567708969116211\n",
      "cnt: 0 - valLoss: 0.45026570558547974 - trainLoss: 0.4567670226097107\n",
      "cnt: 0 - valLoss: 0.4502626359462738 - trainLoss: 0.4567631781101227\n",
      "cnt: 0 - valLoss: 0.4502596855163574 - trainLoss: 0.45675936341285706\n",
      "cnt: 0 - valLoss: 0.4502565264701843 - trainLoss: 0.45675551891326904\n",
      "cnt: 0 - valLoss: 0.45025351643562317 - trainLoss: 0.45675167441368103\n",
      "cnt: 0 - valLoss: 0.4502504765987396 - trainLoss: 0.456747829914093\n",
      "cnt: 0 - valLoss: 0.45024752616882324 - trainLoss: 0.45674392580986023\n",
      "cnt: 0 - valLoss: 0.4502445161342621 - trainLoss: 0.4567400813102722\n",
      "cnt: 0 - valLoss: 0.450241357088089 - trainLoss: 0.4567362666130066\n",
      "cnt: 0 - valLoss: 0.4502384662628174 - trainLoss: 0.4567323625087738\n",
      "cnt: 0 - valLoss: 0.4502353370189667 - trainLoss: 0.4567285180091858\n",
      "cnt: 0 - valLoss: 0.4502323567867279 - trainLoss: 0.4567246735095978\n",
      "cnt: 0 - valLoss: 0.4502292573451996 - trainLoss: 0.456720769405365\n",
      "cnt: 0 - valLoss: 0.4502263367176056 - trainLoss: 0.45671701431274414\n",
      "cnt: 0 - valLoss: 0.4502234160900116 - trainLoss: 0.45671314001083374\n",
      "cnt: 0 - valLoss: 0.4502202272415161 - trainLoss: 0.4567092955112457\n",
      "cnt: 0 - valLoss: 0.45021724700927734 - trainLoss: 0.4567054510116577\n",
      "cnt: 0 - valLoss: 0.4502142667770386 - trainLoss: 0.4567016065120697\n",
      "cnt: 0 - valLoss: 0.4502112567424774 - trainLoss: 0.4566977322101593\n",
      "cnt: 0 - valLoss: 0.4502080976963043 - trainLoss: 0.4566938877105713\n",
      "cnt: 0 - valLoss: 0.4502051770687103 - trainLoss: 0.4566900432109833\n",
      "cnt: 0 - valLoss: 0.4502023458480835 - trainLoss: 0.4566861689090729\n",
      "cnt: 0 - valLoss: 0.450199156999588 - trainLoss: 0.45668238401412964\n",
      "cnt: 0 - valLoss: 0.45019617676734924 - trainLoss: 0.45667847990989685\n",
      "cnt: 0 - valLoss: 0.4501930773258209 - trainLoss: 0.45667463541030884\n",
      "cnt: 0 - valLoss: 0.4501902163028717 - trainLoss: 0.4566708207130432\n",
      "cnt: 0 - valLoss: 0.450187087059021 - trainLoss: 0.4566669762134552\n",
      "cnt: 0 - valLoss: 0.4501841366291046 - trainLoss: 0.4566631615161896\n",
      "cnt: 0 - valLoss: 0.45018115639686584 - trainLoss: 0.4566592574119568\n",
      "cnt: 0 - valLoss: 0.4501780867576599 - trainLoss: 0.45665544271469116\n",
      "cnt: 0 - valLoss: 0.4501751661300659 - trainLoss: 0.45665159821510315\n",
      "cnt: 0 - valLoss: 0.4501720070838928 - trainLoss: 0.4566477835178375\n",
      "cnt: 0 - valLoss: 0.45016902685165405 - trainLoss: 0.4566439092159271\n",
      "cnt: 0 - valLoss: 0.4501660466194153 - trainLoss: 0.4566400349140167\n",
      "cnt: 0 - valLoss: 0.4501630663871765 - trainLoss: 0.4566362798213959\n",
      "cnt: 0 - valLoss: 0.4501601755619049 - trainLoss: 0.45663243532180786\n",
      "cnt: 0 - valLoss: 0.4501569867134094 - trainLoss: 0.45662856101989746\n",
      "cnt: 0 - valLoss: 0.4501541554927826 - trainLoss: 0.45662471652030945\n",
      "cnt: 0 - valLoss: 0.4501510262489319 - trainLoss: 0.45662087202072144\n",
      "cnt: 0 - valLoss: 0.4501480758190155 - trainLoss: 0.4566170871257782\n",
      "cnt: 0 - valLoss: 0.45014500617980957 - trainLoss: 0.4566132128238678\n",
      "cnt: 0 - valLoss: 0.45014211535453796 - trainLoss: 0.4566093981266022\n",
      "cnt: 0 - valLoss: 0.45013925433158875 - trainLoss: 0.45660555362701416\n",
      "cnt: 0 - valLoss: 0.45013606548309326 - trainLoss: 0.45660167932510376\n",
      "cnt: 0 - valLoss: 0.4501331150531769 - trainLoss: 0.4565978944301605\n",
      "cnt: 0 - valLoss: 0.4501301050186157 - trainLoss: 0.4565940797328949\n",
      "cnt: 0 - valLoss: 0.45012715458869934 - trainLoss: 0.4565902352333069\n",
      "cnt: 0 - valLoss: 0.4501240849494934 - trainLoss: 0.4565863609313965\n",
      "cnt: 0 - valLoss: 0.4501211643218994 - trainLoss: 0.45658254623413086\n",
      "cnt: 0 - valLoss: 0.4501183331012726 - trainLoss: 0.45657867193222046\n",
      "cnt: 0 - valLoss: 0.4501151740550995 - trainLoss: 0.4565748870372772\n",
      "cnt: 0 - valLoss: 0.4501122534275055 - trainLoss: 0.4565710425376892\n",
      "cnt: 0 - valLoss: 0.4501091539859772 - trainLoss: 0.4565671980381012\n",
      "cnt: 0 - valLoss: 0.45010629296302795 - trainLoss: 0.45656341314315796\n",
      "cnt: 0 - valLoss: 0.45010319352149963 - trainLoss: 0.45655956864356995\n",
      "cnt: 0 - valLoss: 0.45010024309158325 - trainLoss: 0.45655572414398193\n",
      "cnt: 0 - valLoss: 0.45009738206863403 - trainLoss: 0.4565519094467163\n",
      "cnt: 0 - valLoss: 0.4500943422317505 - trainLoss: 0.4565480947494507\n",
      "cnt: 0 - valLoss: 0.4500914216041565 - trainLoss: 0.45654425024986267\n",
      "cnt: 0 - valLoss: 0.450088232755661 - trainLoss: 0.45654040575027466\n",
      "cnt: 0 - valLoss: 0.4500853717327118 - trainLoss: 0.45653659105300903\n",
      "cnt: 0 - valLoss: 0.45008236169815063 - trainLoss: 0.4565327763557434\n",
      "cnt: 0 - valLoss: 0.45007941126823425 - trainLoss: 0.4565289318561554\n",
      "cnt: 0 - valLoss: 0.45007655024528503 - trainLoss: 0.45652517676353455\n",
      "cnt: 0 - valLoss: 0.4500734508037567 - trainLoss: 0.45652130246162415\n",
      "cnt: 0 - valLoss: 0.45007067918777466 - trainLoss: 0.45651745796203613\n",
      "cnt: 0 - valLoss: 0.45006757974624634 - trainLoss: 0.4565137028694153\n",
      "cnt: 0 - valLoss: 0.45006465911865234 - trainLoss: 0.4565098285675049\n",
      "cnt: 0 - valLoss: 0.4500615894794464 - trainLoss: 0.45650598406791687\n",
      "cnt: 0 - valLoss: 0.45005878806114197 - trainLoss: 0.456502228975296\n",
      "cnt: 0 - valLoss: 0.4500557482242584 - trainLoss: 0.4564983546733856\n",
      "cnt: 0 - valLoss: 0.45005282759666443 - trainLoss: 0.4564945697784424\n",
      "cnt: 0 - valLoss: 0.4500499665737152 - trainLoss: 0.45649075508117676\n",
      "cnt: 0 - valLoss: 0.45004695653915405 - trainLoss: 0.45648694038391113\n",
      "cnt: 0 - valLoss: 0.45004409551620483 - trainLoss: 0.4564831256866455\n",
      "cnt: 0 - valLoss: 0.4500409662723541 - trainLoss: 0.4564792811870575\n",
      "cnt: 0 - valLoss: 0.4500381052494049 - trainLoss: 0.45647552609443665\n",
      "cnt: 0 - valLoss: 0.45003509521484375 - trainLoss: 0.45647165179252625\n",
      "cnt: 0 - valLoss: 0.45003217458724976 - trainLoss: 0.4564678370952606\n",
      "cnt: 0 - valLoss: 0.45002931356430054 - trainLoss: 0.4564640522003174\n",
      "cnt: 0 - valLoss: 0.450026273727417 - trainLoss: 0.45646023750305176\n",
      "cnt: 0 - valLoss: 0.45002347230911255 - trainLoss: 0.45645642280578613\n",
      "cnt: 0 - valLoss: 0.450020432472229 - trainLoss: 0.4564525783061981\n",
      "cnt: 0 - valLoss: 0.450017511844635 - trainLoss: 0.4564487636089325\n",
      "cnt: 0 - valLoss: 0.45001456141471863 - trainLoss: 0.45644494891166687\n",
      "cnt: 0 - valLoss: 0.45001161098480225 - trainLoss: 0.45644113421440125\n",
      "cnt: 0 - valLoss: 0.4500087797641754 - trainLoss: 0.4564373195171356\n",
      "cnt: 0 - valLoss: 0.4500056803226471 - trainLoss: 0.45643356442451477\n",
      "cnt: 0 - valLoss: 0.4500029385089874 - trainLoss: 0.45642971992492676\n",
      "cnt: 0 - valLoss: 0.4499998688697815 - trainLoss: 0.45642587542533875\n",
      "cnt: 0 - valLoss: 0.4499969482421875 - trainLoss: 0.4564221501350403\n",
      "cnt: 0 - valLoss: 0.44999390840530396 - trainLoss: 0.4564182460308075\n",
      "cnt: 0 - valLoss: 0.4499911069869995 - trainLoss: 0.45641449093818665\n",
      "cnt: 0 - valLoss: 0.4499882459640503 - trainLoss: 0.456410676240921\n",
      "cnt: 0 - valLoss: 0.44998520612716675 - trainLoss: 0.4564068615436554\n",
      "cnt: 0 - valLoss: 0.44998228549957275 - trainLoss: 0.45640304684638977\n",
      "cnt: 0 - valLoss: 0.44997936487197876 - trainLoss: 0.45639923214912415\n",
      "cnt: 0 - valLoss: 0.44997647404670715 - trainLoss: 0.4563954770565033\n",
      "cnt: 0 - valLoss: 0.449973464012146 - trainLoss: 0.4563916325569153\n",
      "cnt: 0 - valLoss: 0.449970543384552 - trainLoss: 0.45638784766197205\n",
      "cnt: 0 - valLoss: 0.44996780157089233 - trainLoss: 0.45638400316238403\n",
      "cnt: 0 - valLoss: 0.449964702129364 - trainLoss: 0.4563802480697632\n",
      "cnt: 0 - valLoss: 0.44996190071105957 - trainLoss: 0.45637643337249756\n",
      "cnt: 0 - valLoss: 0.44995880126953125 - trainLoss: 0.45637261867523193\n",
      "cnt: 0 - valLoss: 0.4499560594558716 - trainLoss: 0.4563688039779663\n",
      "cnt: 0 - valLoss: 0.44995296001434326 - trainLoss: 0.4563649892807007\n",
      "cnt: 0 - valLoss: 0.44995012879371643 - trainLoss: 0.45636123418807983\n",
      "cnt: 0 - valLoss: 0.449947327375412 - trainLoss: 0.4563573896884918\n",
      "cnt: 0 - valLoss: 0.44994431734085083 - trainLoss: 0.4563536047935486\n",
      "cnt: 0 - valLoss: 0.4499414563179016 - trainLoss: 0.45634984970092773\n",
      "cnt: 0 - valLoss: 0.4499383866786957 - trainLoss: 0.4563460052013397\n",
      "cnt: 0 - valLoss: 0.44993552565574646 - trainLoss: 0.4563422203063965\n",
      "cnt: 0 - valLoss: 0.44993260502815247 - trainLoss: 0.45633840560913086\n",
      "cnt: 0 - valLoss: 0.44992968440055847 - trainLoss: 0.45633465051651\n",
      "cnt: 0 - valLoss: 0.44992688298225403 - trainLoss: 0.4563308358192444\n",
      "cnt: 0 - valLoss: 0.44992390275001526 - trainLoss: 0.45632702112197876\n",
      "cnt: 0 - valLoss: 0.4499210715293884 - trainLoss: 0.4563232362270355\n",
      "cnt: 0 - valLoss: 0.4499180316925049 - trainLoss: 0.4563194513320923\n",
      "cnt: 0 - valLoss: 0.44991517066955566 - trainLoss: 0.4563156068325043\n",
      "cnt: 0 - valLoss: 0.44991225004196167 - trainLoss: 0.4563118815422058\n",
      "cnt: 0 - valLoss: 0.4499093294143677 - trainLoss: 0.4563080668449402\n",
      "cnt: 0 - valLoss: 0.44990649819374084 - trainLoss: 0.45630425214767456\n",
      "cnt: 0 - valLoss: 0.4499035179615021 - trainLoss: 0.4563004970550537\n",
      "cnt: 0 - valLoss: 0.4499007761478424 - trainLoss: 0.4562966823577881\n",
      "cnt: 0 - valLoss: 0.44989773631095886 - trainLoss: 0.45629292726516724\n",
      "cnt: 0 - valLoss: 0.44989484548568726 - trainLoss: 0.4562890827655792\n",
      "cnt: 0 - valLoss: 0.4498918652534485 - trainLoss: 0.456285297870636\n",
      "cnt: 0 - valLoss: 0.44988909363746643 - trainLoss: 0.45628154277801514\n",
      "cnt: 0 - valLoss: 0.449886292219162 - trainLoss: 0.4562777280807495\n",
      "cnt: 0 - valLoss: 0.44988319277763367 - trainLoss: 0.4562739133834839\n",
      "cnt: 0 - valLoss: 0.44988036155700684 - trainLoss: 0.45627015829086304\n",
      "cnt: 0 - valLoss: 0.44987744092941284 - trainLoss: 0.4562663733959198\n",
      "cnt: 0 - valLoss: 0.4498745799064636 - trainLoss: 0.45626258850097656\n",
      "cnt: 0 - valLoss: 0.44987162947654724 - trainLoss: 0.45625877380371094\n",
      "cnt: 0 - valLoss: 0.4498688280582428 - trainLoss: 0.4562549889087677\n",
      "cnt: 0 - valLoss: 0.44986605644226074 - trainLoss: 0.45625123381614685\n",
      "cnt: 0 - valLoss: 0.4498630166053772 - trainLoss: 0.4562474191188812\n",
      "cnt: 0 - valLoss: 0.4498601257801056 - trainLoss: 0.4562436044216156\n",
      "cnt: 0 - valLoss: 0.4498571753501892 - trainLoss: 0.45623984932899475\n",
      "cnt: 0 - valLoss: 0.44985431432724 - trainLoss: 0.4562360346317291\n",
      "cnt: 0 - valLoss: 0.4498513340950012 - trainLoss: 0.4562322199344635\n",
      "cnt: 0 - valLoss: 0.449848473072052 - trainLoss: 0.4562284052371979\n",
      "cnt: 0 - valLoss: 0.44984573125839233 - trainLoss: 0.45622462034225464\n",
      "cnt: 0 - valLoss: 0.4498427212238312 - trainLoss: 0.4562208354473114\n",
      "cnt: 0 - valLoss: 0.4498399496078491 - trainLoss: 0.4562169909477234\n",
      "cnt: 0 - valLoss: 0.4498369097709656 - trainLoss: 0.45621317625045776\n",
      "cnt: 0 - valLoss: 0.44983410835266113 - trainLoss: 0.45620936155319214\n",
      "cnt: 0 - valLoss: 0.4498310685157776 - trainLoss: 0.4562055766582489\n",
      "cnt: 0 - valLoss: 0.4498283863067627 - trainLoss: 0.4562017619609833\n",
      "cnt: 0 - valLoss: 0.4498254954814911 - trainLoss: 0.45619797706604004\n",
      "cnt: 0 - valLoss: 0.4498225450515747 - trainLoss: 0.4561941921710968\n",
      "cnt: 0 - valLoss: 0.4498196840286255 - trainLoss: 0.4561903774738312\n",
      "cnt: 0 - valLoss: 0.4498167634010315 - trainLoss: 0.45618656277656555\n",
      "cnt: 0 - valLoss: 0.44981393218040466 - trainLoss: 0.4561827480792999\n",
      "cnt: 0 - valLoss: 0.44981127977371216 - trainLoss: 0.4561789333820343\n",
      "cnt: 0 - valLoss: 0.44980818033218384 - trainLoss: 0.45617520809173584\n",
      "cnt: 0 - valLoss: 0.44980543851852417 - trainLoss: 0.4561713635921478\n",
      "cnt: 0 - valLoss: 0.4498023986816406 - trainLoss: 0.456167608499527\n",
      "cnt: 0 - valLoss: 0.44979962706565857 - trainLoss: 0.45616379380226135\n",
      "cnt: 0 - valLoss: 0.4497966170310974 - trainLoss: 0.4561599791049957\n",
      "cnt: 0 - valLoss: 0.44979387521743774 - trainLoss: 0.4561562240123749\n",
      "cnt: 0 - valLoss: 0.4497910439968109 - trainLoss: 0.45615240931510925\n",
      "cnt: 0 - valLoss: 0.44978803396224976 - trainLoss: 0.45614859461784363\n",
      "cnt: 0 - valLoss: 0.44978538155555725 - trainLoss: 0.456144779920578\n",
      "cnt: 0 - valLoss: 0.44978228211402893 - trainLoss: 0.4561409652233124\n",
      "cnt: 0 - valLoss: 0.44977954030036926 - trainLoss: 0.45613721013069153\n",
      "cnt: 0 - valLoss: 0.4497765600681305 - trainLoss: 0.4561333954334259\n",
      "cnt: 0 - valLoss: 0.4497738182544708 - trainLoss: 0.4561295807361603\n",
      "cnt: 0 - valLoss: 0.449770987033844 - trainLoss: 0.45612582564353943\n",
      "cnt: 0 - valLoss: 0.44976797699928284 - trainLoss: 0.4561220109462738\n",
      "cnt: 0 - valLoss: 0.44976523518562317 - trainLoss: 0.4561181962490082\n",
      "cnt: 0 - valLoss: 0.449762225151062 - trainLoss: 0.45611444115638733\n",
      "cnt: 0 - valLoss: 0.44975948333740234 - trainLoss: 0.4561106860637665\n",
      "cnt: 0 - valLoss: 0.4497565031051636 - trainLoss: 0.4561068117618561\n",
      "cnt: 0 - valLoss: 0.4497537910938263 - trainLoss: 0.45610305666923523\n",
      "cnt: 0 - valLoss: 0.44975095987319946 - trainLoss: 0.4560993015766144\n",
      "cnt: 0 - valLoss: 0.4497480094432831 - trainLoss: 0.45609548687934875\n",
      "cnt: 0 - valLoss: 0.44974517822265625 - trainLoss: 0.45609167218208313\n",
      "cnt: 0 - valLoss: 0.44974225759506226 - trainLoss: 0.4560878872871399\n",
      "cnt: 0 - valLoss: 0.4497394263744354 - trainLoss: 0.45608410239219666\n",
      "cnt: 0 - valLoss: 0.4497365951538086 - trainLoss: 0.4560803174972534\n",
      "cnt: 0 - valLoss: 0.4497337341308594 - trainLoss: 0.4560765027999878\n",
      "cnt: 0 - valLoss: 0.4497310221195221 - trainLoss: 0.45607274770736694\n",
      "cnt: 0 - valLoss: 0.44972798228263855 - trainLoss: 0.4560689926147461\n",
      "cnt: 0 - valLoss: 0.44972530007362366 - trainLoss: 0.45606520771980286\n",
      "cnt: 0 - valLoss: 0.4497222900390625 - trainLoss: 0.45606139302253723\n",
      "cnt: 0 - valLoss: 0.4497195780277252 - trainLoss: 0.4560576379299164\n",
      "cnt: 0 - valLoss: 0.4497165381908417 - trainLoss: 0.45605382323265076\n",
      "cnt: 0 - valLoss: 0.44971388578414917 - trainLoss: 0.4560500681400299\n",
      "cnt: 0 - valLoss: 0.44971105456352234 - trainLoss: 0.45604631304740906\n",
      "cnt: 0 - valLoss: 0.44970810413360596 - trainLoss: 0.45604249835014343\n",
      "cnt: 0 - valLoss: 0.4497053027153015 - trainLoss: 0.4560386836528778\n",
      "cnt: 0 - valLoss: 0.44970250129699707 - trainLoss: 0.45603489875793457\n",
      "cnt: 0 - valLoss: 0.4496995806694031 - trainLoss: 0.4560311436653137\n",
      "cnt: 0 - valLoss: 0.44969674944877625 - trainLoss: 0.4560273289680481\n",
      "cnt: 0 - valLoss: 0.4496939182281494 - trainLoss: 0.45602357387542725\n",
      "cnt: 0 - valLoss: 0.44969120621681213 - trainLoss: 0.4560197591781616\n",
      "cnt: 0 - valLoss: 0.4496881663799286 - trainLoss: 0.45601600408554077\n",
      "cnt: 0 - valLoss: 0.44968554377555847 - trainLoss: 0.45601221919059753\n",
      "cnt: 0 - valLoss: 0.4496825039386749 - trainLoss: 0.4560084640979767\n",
      "cnt: 0 - valLoss: 0.44967982172966003 - trainLoss: 0.45600470900535583\n",
      "cnt: 0 - valLoss: 0.4496768116950989 - trainLoss: 0.4560008943080902\n",
      "cnt: 0 - valLoss: 0.4496740996837616 - trainLoss: 0.45599716901779175\n",
      "cnt: 0 - valLoss: 0.44967135787010193 - trainLoss: 0.4559933543205261\n",
      "cnt: 0 - valLoss: 0.4496684670448303 - trainLoss: 0.4559895396232605\n",
      "cnt: 0 - valLoss: 0.4496656358242035 - trainLoss: 0.45598578453063965\n",
      "cnt: 0 - valLoss: 0.44966280460357666 - trainLoss: 0.4559819996356964\n",
      "cnt: 0 - valLoss: 0.44965997338294983 - trainLoss: 0.45597824454307556\n",
      "cnt: 0 - valLoss: 0.44965705275535583 - trainLoss: 0.4559744894504547\n",
      "cnt: 0 - valLoss: 0.4496542811393738 - trainLoss: 0.4559707045555115\n",
      "cnt: 0 - valLoss: 0.4496515691280365 - trainLoss: 0.45596688985824585\n",
      "cnt: 0 - valLoss: 0.44964855909347534 - trainLoss: 0.4559631645679474\n",
      "cnt: 0 - valLoss: 0.44964590668678284 - trainLoss: 0.45595940947532654\n",
      "cnt: 0 - valLoss: 0.44964295625686646 - trainLoss: 0.4559556543827057\n",
      "cnt: 0 - valLoss: 0.4496402442455292 - trainLoss: 0.4559517800807953\n",
      "cnt: 0 - valLoss: 0.4496372938156128 - trainLoss: 0.4559480547904968\n",
      "cnt: 0 - valLoss: 0.4496345818042755 - trainLoss: 0.455944299697876\n",
      "cnt: 0 - valLoss: 0.44963178038597107 - trainLoss: 0.45594048500061035\n",
      "cnt: 0 - valLoss: 0.44962894916534424 - trainLoss: 0.4559367597103119\n",
      "cnt: 0 - valLoss: 0.4496261477470398 - trainLoss: 0.45593300461769104\n",
      "cnt: 0 - valLoss: 0.4496232867240906 - trainLoss: 0.4559291899204254\n",
      "cnt: 0 - valLoss: 0.44962045550346375 - trainLoss: 0.45592543482780457\n",
      "cnt: 0 - valLoss: 0.4496176242828369 - trainLoss: 0.45592164993286133\n",
      "cnt: 0 - valLoss: 0.44961482286453247 - trainLoss: 0.4559178948402405\n",
      "cnt: 0 - valLoss: 0.4496121406555176 - trainLoss: 0.45591413974761963\n",
      "cnt: 0 - valLoss: 0.4496091306209564 - trainLoss: 0.4559103548526764\n",
      "cnt: 0 - valLoss: 0.4496065080165863 - trainLoss: 0.45590659976005554\n",
      "cnt: 0 - valLoss: 0.44960352778434753 - trainLoss: 0.4559028148651123\n",
      "cnt: 0 - valLoss: 0.44960081577301025 - trainLoss: 0.45589905977249146\n",
      "cnt: 0 - valLoss: 0.44959786534309387 - trainLoss: 0.4558953046798706\n",
      "cnt: 0 - valLoss: 0.4495951533317566 - trainLoss: 0.45589157938957214\n",
      "cnt: 0 - valLoss: 0.4495924115180969 - trainLoss: 0.4558877646923065\n",
      "cnt: 0 - valLoss: 0.4495895802974701 - trainLoss: 0.45588406920433044\n",
      "cnt: 0 - valLoss: 0.44958680868148804 - trainLoss: 0.4558802843093872\n",
      "cnt: 0 - valLoss: 0.44958391785621643 - trainLoss: 0.4558764696121216\n",
      "cnt: 0 - valLoss: 0.4495811462402344 - trainLoss: 0.4558727443218231\n",
      "cnt: 0 - valLoss: 0.44957849383354187 - trainLoss: 0.4558689594268799\n",
      "cnt: 0 - valLoss: 0.4495755434036255 - trainLoss: 0.4558652341365814\n",
      "cnt: 0 - valLoss: 0.4495728313922882 - trainLoss: 0.45586147904396057\n",
      "cnt: 0 - valLoss: 0.4495699107646942 - trainLoss: 0.4558577239513397\n",
      "cnt: 0 - valLoss: 0.4495672583580017 - trainLoss: 0.4558539390563965\n",
      "cnt: 0 - valLoss: 0.4495643079280853 - trainLoss: 0.455850213766098\n",
      "cnt: 0 - valLoss: 0.44956156611442566 - trainLoss: 0.4558464586734772\n",
      "cnt: 0 - valLoss: 0.4495588541030884 - trainLoss: 0.45584264397621155\n",
      "cnt: 0 - valLoss: 0.44955602288246155 - trainLoss: 0.4558389186859131\n",
      "cnt: 0 - valLoss: 0.4495532512664795 - trainLoss: 0.45583516359329224\n",
      "cnt: 0 - valLoss: 0.44955042004585266 - trainLoss: 0.455831378698349\n",
      "cnt: 0 - valLoss: 0.4495476186275482 - trainLoss: 0.45582765340805054\n",
      "cnt: 0 - valLoss: 0.44954484701156616 - trainLoss: 0.4558238983154297\n",
      "cnt: 0 - valLoss: 0.44954201579093933 - trainLoss: 0.45582014322280884\n",
      "cnt: 0 - valLoss: 0.4495393633842468 - trainLoss: 0.4558163583278656\n",
      "cnt: 0 - valLoss: 0.44953635334968567 - trainLoss: 0.45581260323524475\n",
      "cnt: 0 - valLoss: 0.44953370094299316 - trainLoss: 0.4558088779449463\n",
      "cnt: 0 - valLoss: 0.449530690908432 - trainLoss: 0.4558051526546478\n",
      "cnt: 0 - valLoss: 0.4495280086994171 - trainLoss: 0.455801397562027\n",
      "cnt: 0 - valLoss: 0.44952502846717834 - trainLoss: 0.45579761266708374\n",
      "cnt: 0 - valLoss: 0.44952237606048584 - trainLoss: 0.4557938575744629\n",
      "cnt: 0 - valLoss: 0.44951963424682617 - trainLoss: 0.45579013228416443\n",
      "cnt: 0 - valLoss: 0.4495167136192322 - trainLoss: 0.45578640699386597\n",
      "cnt: 0 - valLoss: 0.4495139420032501 - trainLoss: 0.45578259229660034\n",
      "cnt: 0 - valLoss: 0.4495111107826233 - trainLoss: 0.4557788670063019\n",
      "cnt: 0 - valLoss: 0.44950830936431885 - trainLoss: 0.45577511191368103\n",
      "cnt: 0 - valLoss: 0.44950544834136963 - trainLoss: 0.4557713270187378\n",
      "cnt: 0 - valLoss: 0.4495026767253876 - trainLoss: 0.4557676613330841\n",
      "cnt: 0 - valLoss: 0.4494999945163727 - trainLoss: 0.45576390624046326\n",
      "cnt: 0 - valLoss: 0.4494970440864563 - trainLoss: 0.4557601809501648\n",
      "cnt: 0 - valLoss: 0.4494943618774414 - trainLoss: 0.45575639605522156\n",
      "cnt: 0 - valLoss: 0.4494914412498474 - trainLoss: 0.4557526409626007\n",
      "cnt: 0 - valLoss: 0.4494887888431549 - trainLoss: 0.45574891567230225\n",
      "cnt: 0 - valLoss: 0.44948577880859375 - trainLoss: 0.4557451605796814\n",
      "cnt: 0 - valLoss: 0.44948312640190125 - trainLoss: 0.45574137568473816\n",
      "cnt: 0 - valLoss: 0.4494803845882416 - trainLoss: 0.4557377099990845\n",
      "cnt: 0 - valLoss: 0.44947755336761475 - trainLoss: 0.45573392510414124\n",
      "cnt: 0 - valLoss: 0.4494748115539551 - trainLoss: 0.4557301700115204\n",
      "cnt: 0 - valLoss: 0.4494718909263611 - trainLoss: 0.45572638511657715\n",
      "cnt: 0 - valLoss: 0.4494691491127014 - trainLoss: 0.45572271943092346\n",
      "cnt: 0 - valLoss: 0.4494663178920746 - trainLoss: 0.4557189643383026\n",
      "cnt: 0 - valLoss: 0.44946354627609253 - trainLoss: 0.45571523904800415\n",
      "cnt: 0 - valLoss: 0.44946086406707764 - trainLoss: 0.4557114541530609\n",
      "cnt: 0 - valLoss: 0.44945791363716125 - trainLoss: 0.45570772886276245\n",
      "cnt: 0 - valLoss: 0.44945526123046875 - trainLoss: 0.455704003572464\n",
      "cnt: 0 - valLoss: 0.44945234060287476 - trainLoss: 0.4557002782821655\n",
      "cnt: 0 - valLoss: 0.44944968819618225 - trainLoss: 0.4556965231895447\n",
      "cnt: 0 - valLoss: 0.44944676756858826 - trainLoss: 0.45569276809692383\n",
      "cnt: 0 - valLoss: 0.44944408535957336 - trainLoss: 0.45568907260894775\n",
      "cnt: 0 - valLoss: 0.4494413435459137 - trainLoss: 0.4556853175163269\n",
      "cnt: 0 - valLoss: 0.4494384527206421 - trainLoss: 0.45568153262138367\n",
      "cnt: 0 - valLoss: 0.4494357109069824 - trainLoss: 0.45567786693573\n",
      "cnt: 0 - valLoss: 0.4494328796863556 - trainLoss: 0.45567408204078674\n",
      "cnt: 0 - valLoss: 0.4494301378726959 - trainLoss: 0.4556703567504883\n",
      "cnt: 0 - valLoss: 0.4494274854660034 - trainLoss: 0.45566660165786743\n",
      "cnt: 0 - valLoss: 0.44942453503608704 - trainLoss: 0.45566287636756897\n",
      "cnt: 0 - valLoss: 0.4494219124317169 - trainLoss: 0.4556591212749481\n",
      "cnt: 0 - valLoss: 0.4494189918041229 - trainLoss: 0.45565542578697205\n",
      "cnt: 0 - valLoss: 0.44941624999046326 - trainLoss: 0.4556516706943512\n",
      "cnt: 0 - valLoss: 0.44941338896751404 - trainLoss: 0.45564788579940796\n",
      "cnt: 0 - valLoss: 0.4494107663631439 - trainLoss: 0.4556442201137543\n",
      "cnt: 0 - valLoss: 0.44940802454948425 - trainLoss: 0.4556404650211334\n",
      "cnt: 0 - valLoss: 0.4494051933288574 - trainLoss: 0.45563676953315735\n",
      "cnt: 0 - valLoss: 0.44940245151519775 - trainLoss: 0.4556330144405365\n",
      "cnt: 0 - valLoss: 0.4493996202945709 - trainLoss: 0.4556293189525604\n",
      "cnt: 0 - valLoss: 0.44939687848091125 - trainLoss: 0.4556255638599396\n",
      "cnt: 0 - valLoss: 0.4493941068649292 - trainLoss: 0.4556218385696411\n",
      "cnt: 0 - valLoss: 0.44939130544662476 - trainLoss: 0.45561811327934265\n",
      "cnt: 0 - valLoss: 0.44938868284225464 - trainLoss: 0.4556143879890442\n",
      "cnt: 0 - valLoss: 0.44938573241233826 - trainLoss: 0.4556106626987457\n",
      "cnt: 0 - valLoss: 0.4493831694126129 - trainLoss: 0.45560693740844727\n",
      "cnt: 0 - valLoss: 0.44938021898269653 - trainLoss: 0.4556032121181488\n",
      "cnt: 0 - valLoss: 0.44937756657600403 - trainLoss: 0.45559945702552795\n",
      "cnt: 0 - valLoss: 0.4493746757507324 - trainLoss: 0.4555957317352295\n",
      "cnt: 0 - valLoss: 0.4493720233440399 - trainLoss: 0.4555920362472534\n",
      "cnt: 0 - valLoss: 0.44936928153038025 - trainLoss: 0.45558828115463257\n",
      "cnt: 0 - valLoss: 0.4493664503097534 - trainLoss: 0.4555845558643341\n",
      "cnt: 0 - valLoss: 0.44936370849609375 - trainLoss: 0.45558086037635803\n",
      "cnt: 0 - valLoss: 0.4493609666824341 - trainLoss: 0.4555771052837372\n",
      "cnt: 0 - valLoss: 0.44935816526412964 - trainLoss: 0.4555734395980835\n",
      "cnt: 0 - valLoss: 0.4493553638458252 - trainLoss: 0.45556965470314026\n",
      "cnt: 0 - valLoss: 0.4493526220321655 - trainLoss: 0.4555659890174866\n",
      "cnt: 0 - valLoss: 0.4493500888347626 - trainLoss: 0.45556220412254333\n",
      "cnt: 0 - valLoss: 0.4493470788002014 - trainLoss: 0.45555853843688965\n",
      "cnt: 0 - valLoss: 0.4493445158004761 - trainLoss: 0.4555547535419464\n",
      "cnt: 0 - valLoss: 0.4493415951728821 - trainLoss: 0.45555102825164795\n",
      "cnt: 0 - valLoss: 0.4493389427661896 - trainLoss: 0.4555473029613495\n",
      "cnt: 0 - valLoss: 0.4493362307548523 - trainLoss: 0.4555436074733734\n",
      "cnt: 0 - valLoss: 0.44933345913887024 - trainLoss: 0.45553991198539734\n",
      "cnt: 0 - valLoss: 0.44933074712753296 - trainLoss: 0.4555361866950989\n",
      "cnt: 0 - valLoss: 0.44932791590690613 - trainLoss: 0.4555324614048004\n",
      "cnt: 0 - valLoss: 0.44932517409324646 - trainLoss: 0.45552873611450195\n",
      "cnt: 0 - valLoss: 0.4493224322795868 - trainLoss: 0.45552507042884827\n",
      "cnt: 0 - valLoss: 0.44931966066360474 - trainLoss: 0.4555213451385498\n",
      "cnt: 0 - valLoss: 0.4493171274662018 - trainLoss: 0.45551756024360657\n",
      "cnt: 0 - valLoss: 0.449314147233963 - trainLoss: 0.4555138945579529\n",
      "cnt: 0 - valLoss: 0.44931158423423767 - trainLoss: 0.4555101692676544\n",
      "cnt: 0 - valLoss: 0.4493086636066437 - trainLoss: 0.45550647377967834\n",
      "cnt: 0 - valLoss: 0.44930601119995117 - trainLoss: 0.4555027484893799\n",
      "cnt: 0 - valLoss: 0.44930315017700195 - trainLoss: 0.4554990231990814\n",
      "cnt: 0 - valLoss: 0.4493005871772766 - trainLoss: 0.45549535751342773\n",
      "cnt: 0 - valLoss: 0.44929787516593933 - trainLoss: 0.4554916024208069\n",
      "cnt: 0 - valLoss: 0.4492950141429901 - trainLoss: 0.4554879069328308\n",
      "cnt: 0 - valLoss: 0.4492923319339752 - trainLoss: 0.45548415184020996\n",
      "cnt: 0 - valLoss: 0.44928956031799316 - trainLoss: 0.4554804563522339\n",
      "cnt: 0 - valLoss: 0.4492868185043335 - trainLoss: 0.4554767906665802\n",
      "cnt: 0 - valLoss: 0.44928401708602905 - trainLoss: 0.45547306537628174\n",
      "cnt: 0 - valLoss: 0.44928133487701416 - trainLoss: 0.4554693400859833\n",
      "cnt: 0 - valLoss: 0.44927871227264404 - trainLoss: 0.4554656445980072\n",
      "cnt: 0 - valLoss: 0.4492758512496948 - trainLoss: 0.45546188950538635\n",
      "cnt: 0 - valLoss: 0.4492732584476471 - trainLoss: 0.4554581940174103\n",
      "cnt: 0 - valLoss: 0.4492703676223755 - trainLoss: 0.4554544687271118\n",
      "cnt: 0 - valLoss: 0.4492676854133606 - trainLoss: 0.45545074343681335\n",
      "cnt: 0 - valLoss: 0.44926488399505615 - trainLoss: 0.4554470181465149\n",
      "cnt: 0 - valLoss: 0.4492622911930084 - trainLoss: 0.4554433524608612\n",
      "cnt: 0 - valLoss: 0.44925954937934875 - trainLoss: 0.45543962717056274\n",
      "cnt: 0 - valLoss: 0.4492567181587219 - trainLoss: 0.45543596148490906\n",
      "cnt: 0 - valLoss: 0.4492540657520294 - trainLoss: 0.455432265996933\n",
      "cnt: 0 - valLoss: 0.44925129413604736 - trainLoss: 0.45542851090431213\n",
      "cnt: 0 - valLoss: 0.4492485821247101 - trainLoss: 0.45542481541633606\n",
      "cnt: 0 - valLoss: 0.44924601912498474 - trainLoss: 0.4554211497306824\n",
      "cnt: 0 - valLoss: 0.44924312829971313 - trainLoss: 0.4554174244403839\n",
      "cnt: 0 - valLoss: 0.4492405951023102 - trainLoss: 0.45541372895240784\n",
      "cnt: 0 - valLoss: 0.4492377042770386 - trainLoss: 0.4554100036621094\n",
      "cnt: 0 - valLoss: 0.44923511147499084 - trainLoss: 0.4554063379764557\n",
      "cnt: 0 - valLoss: 0.44923222064971924 - trainLoss: 0.4554026126861572\n",
      "cnt: 0 - valLoss: 0.4492296874523163 - trainLoss: 0.45539891719818115\n",
      "cnt: 0 - valLoss: 0.449226975440979 - trainLoss: 0.4553951919078827\n",
      "cnt: 0 - valLoss: 0.44922420382499695 - trainLoss: 0.455391526222229\n",
      "cnt: 0 - valLoss: 0.44922149181365967 - trainLoss: 0.45538774132728577\n",
      "cnt: 0 - valLoss: 0.44921875 - trainLoss: 0.4553840756416321\n",
      "cnt: 0 - valLoss: 0.4492160379886627 - trainLoss: 0.455380380153656\n",
      "cnt: 0 - valLoss: 0.44921326637268066 - trainLoss: 0.4553767144680023\n",
      "cnt: 0 - valLoss: 0.44921061396598816 - trainLoss: 0.45537301898002625\n",
      "cnt: 0 - valLoss: 0.44920802116394043 - trainLoss: 0.4553692638874054\n",
      "cnt: 0 - valLoss: 0.4492051601409912 - trainLoss: 0.4553655683994293\n",
      "cnt: 0 - valLoss: 0.4492025375366211 - trainLoss: 0.45536190271377563\n",
      "cnt: 0 - valLoss: 0.44919970631599426 - trainLoss: 0.45535820722579956\n",
      "cnt: 0 - valLoss: 0.4491971433162689 - trainLoss: 0.4553545415401459\n",
      "cnt: 0 - valLoss: 0.4491942226886749 - trainLoss: 0.45535075664520264\n",
      "cnt: 0 - valLoss: 0.4491916298866272 - trainLoss: 0.45534712076187134\n",
      "cnt: 0 - valLoss: 0.4491889178752899 - trainLoss: 0.4553433954715729\n",
      "cnt: 0 - valLoss: 0.4491860866546631 - trainLoss: 0.4553397297859192\n",
      "cnt: 0 - valLoss: 0.4491833746433258 - trainLoss: 0.4553360342979431\n",
      "cnt: 0 - valLoss: 0.44918057322502136 - trainLoss: 0.45533236861228943\n",
      "cnt: 0 - valLoss: 0.4491778314113617 - trainLoss: 0.45532864332199097\n",
      "cnt: 0 - valLoss: 0.44917529821395874 - trainLoss: 0.4553249478340149\n",
      "cnt: 0 - valLoss: 0.44917231798171997 - trainLoss: 0.4553212821483612\n",
      "cnt: 0 - valLoss: 0.44916972517967224 - trainLoss: 0.45531755685806274\n",
      "cnt: 0 - valLoss: 0.44916680455207825 - trainLoss: 0.45531389117240906\n",
      "cnt: 0 - valLoss: 0.4491642415523529 - trainLoss: 0.455310195684433\n",
      "cnt: 0 - valLoss: 0.4491613209247589 - trainLoss: 0.4553065001964569\n",
      "cnt: 0 - valLoss: 0.4491586685180664 - trainLoss: 0.4553028345108032\n",
      "cnt: 0 - valLoss: 0.4491559863090515 - trainLoss: 0.45529913902282715\n",
      "cnt: 0 - valLoss: 0.44915321469306946 - trainLoss: 0.45529547333717346\n",
      "cnt: 0 - valLoss: 0.4491505026817322 - trainLoss: 0.4552917778491974\n",
      "cnt: 0 - valLoss: 0.44914767146110535 - trainLoss: 0.4552880525588989\n",
      "cnt: 0 - valLoss: 0.44914498925209045 - trainLoss: 0.45528438687324524\n",
      "cnt: 0 - valLoss: 0.4491422474384308 - trainLoss: 0.45528069138526917\n",
      "cnt: 0 - valLoss: 0.4491395056247711 - trainLoss: 0.4552770256996155\n",
      "cnt: 0 - valLoss: 0.449136883020401 - trainLoss: 0.455273300409317\n",
      "cnt: 0 - valLoss: 0.4491340219974518 - trainLoss: 0.45526960492134094\n",
      "cnt: 0 - valLoss: 0.44913148880004883 - trainLoss: 0.45526593923568726\n",
      "cnt: 0 - valLoss: 0.44912856817245483 - trainLoss: 0.4552622437477112\n",
      "cnt: 0 - valLoss: 0.44912591576576233 - trainLoss: 0.4552585780620575\n",
      "cnt: 0 - valLoss: 0.4491230845451355 - trainLoss: 0.4552548825740814\n",
      "cnt: 0 - valLoss: 0.44912052154541016 - trainLoss: 0.4552512466907501\n",
      "cnt: 0 - valLoss: 0.4491178095340729 - trainLoss: 0.45524752140045166\n",
      "cnt: 0 - valLoss: 0.4491150379180908 - trainLoss: 0.45524388551712036\n",
      "cnt: 0 - valLoss: 0.44911232590675354 - trainLoss: 0.4552401602268219\n",
      "cnt: 0 - valLoss: 0.44910958409309387 - trainLoss: 0.4552365243434906\n",
      "cnt: 0 - valLoss: 0.4491068720817566 - trainLoss: 0.4552328586578369\n",
      "cnt: 0 - valLoss: 0.44910427927970886 - trainLoss: 0.45522913336753845\n",
      "cnt: 0 - valLoss: 0.44910138845443726 - trainLoss: 0.4552254378795624\n",
      "cnt: 0 - valLoss: 0.4490988254547119 - trainLoss: 0.4552217721939087\n",
      "cnt: 0 - valLoss: 0.4490959644317627 - trainLoss: 0.4552181363105774\n",
      "cnt: 0 - valLoss: 0.44909337162971497 - trainLoss: 0.4552144408226013\n",
      "cnt: 0 - valLoss: 0.44909051060676575 - trainLoss: 0.45521071553230286\n",
      "cnt: 0 - valLoss: 0.44908788800239563 - trainLoss: 0.45520707964897156\n",
      "cnt: 0 - valLoss: 0.44908520579338074 - trainLoss: 0.45520344376564026\n",
      "cnt: 0 - valLoss: 0.44908246397972107 - trainLoss: 0.4551997780799866\n",
      "cnt: 0 - valLoss: 0.4490797519683838 - trainLoss: 0.4551960527896881\n",
      "cnt: 0 - valLoss: 0.4490770101547241 - trainLoss: 0.4551924169063568\n",
      "cnt: 0 - valLoss: 0.44907429814338684 - trainLoss: 0.45518872141838074\n",
      "cnt: 0 - valLoss: 0.4490715563297272 - trainLoss: 0.45518505573272705\n",
      "cnt: 0 - valLoss: 0.4490688741207123 - trainLoss: 0.455181360244751\n",
      "cnt: 0 - valLoss: 0.44906625151634216 - trainLoss: 0.4551776945590973\n",
      "cnt: 0 - valLoss: 0.44906342029571533 - trainLoss: 0.4551739990711212\n",
      "cnt: 0 - valLoss: 0.4490608870983124 - trainLoss: 0.45517033338546753\n",
      "cnt: 0 - valLoss: 0.44905802607536316 - trainLoss: 0.45516663789749146\n",
      "cnt: 0 - valLoss: 0.44905540347099304 - trainLoss: 0.45516300201416016\n",
      "cnt: 0 - valLoss: 0.4490525424480438 - trainLoss: 0.4551593065261841\n",
      "cnt: 0 - valLoss: 0.44905000925064087 - trainLoss: 0.4551556706428528\n",
      "cnt: 0 - valLoss: 0.4490472972393036 - trainLoss: 0.4551520049571991\n",
      "cnt: 0 - valLoss: 0.4490445554256439 - trainLoss: 0.455148309469223\n",
      "cnt: 0 - valLoss: 0.4490419030189514 - trainLoss: 0.45514464378356934\n",
      "cnt: 0 - valLoss: 0.44903916120529175 - trainLoss: 0.45514094829559326\n",
      "cnt: 0 - valLoss: 0.44903650879859924 - trainLoss: 0.45513731241226196\n",
      "cnt: 0 - valLoss: 0.4490339159965515 - trainLoss: 0.4551336467266083\n",
      "cnt: 0 - valLoss: 0.4490310549736023 - trainLoss: 0.455130010843277\n",
      "cnt: 0 - valLoss: 0.44902852177619934 - trainLoss: 0.4551263153553009\n",
      "cnt: 0 - valLoss: 0.4490256607532501 - trainLoss: 0.4551226496696472\n",
      "cnt: 0 - valLoss: 0.44902303814888 - trainLoss: 0.45511895418167114\n",
      "cnt: 0 - valLoss: 0.44902026653289795 - trainLoss: 0.45511528849601746\n",
      "cnt: 0 - valLoss: 0.449017733335495 - trainLoss: 0.45511168241500854\n",
      "cnt: 0 - valLoss: 0.4490149915218353 - trainLoss: 0.4551079571247101\n",
      "cnt: 0 - valLoss: 0.44901227951049805 - trainLoss: 0.4551043212413788\n",
      "cnt: 0 - valLoss: 0.44900962710380554 - trainLoss: 0.4551006257534027\n",
      "cnt: 0 - valLoss: 0.44900691509246826 - trainLoss: 0.455096960067749\n",
      "cnt: 0 - valLoss: 0.4490041732788086 - trainLoss: 0.4550933241844177\n",
      "cnt: 0 - valLoss: 0.4490014910697937 - trainLoss: 0.4550896883010864\n",
      "cnt: 0 - valLoss: 0.4489987790584564 - trainLoss: 0.45508599281311035\n",
      "cnt: 0 - valLoss: 0.44899624586105347 - trainLoss: 0.45508232712745667\n",
      "cnt: 0 - valLoss: 0.44899341464042664 - trainLoss: 0.45507869124412537\n",
      "cnt: 0 - valLoss: 0.44899091124534607 - trainLoss: 0.45507505536079407\n",
      "cnt: 0 - valLoss: 0.44898808002471924 - trainLoss: 0.455071359872818\n",
      "cnt: 0 - valLoss: 0.4489854872226715 - trainLoss: 0.4550676941871643\n",
      "cnt: 0 - valLoss: 0.448982834815979 - trainLoss: 0.4550640285015106\n",
      "cnt: 0 - valLoss: 0.44898009300231934 - trainLoss: 0.45506036281585693\n",
      "cnt: 0 - valLoss: 0.4489774703979492 - trainLoss: 0.45505672693252563\n",
      "cnt: 0 - valLoss: 0.44897469878196716 - trainLoss: 0.45505303144454956\n",
      "cnt: 0 - valLoss: 0.44897204637527466 - trainLoss: 0.45504939556121826\n",
      "cnt: 0 - valLoss: 0.44896939396858215 - trainLoss: 0.4550457298755646\n",
      "cnt: 0 - valLoss: 0.4489666819572449 - trainLoss: 0.4550420939922333\n",
      "cnt: 0 - valLoss: 0.44896411895751953 - trainLoss: 0.4550383985042572\n",
      "cnt: 0 - valLoss: 0.4489613175392151 - trainLoss: 0.4550347626209259\n",
      "cnt: 0 - valLoss: 0.44895878434181213 - trainLoss: 0.4550311267375946\n",
      "cnt: 0 - valLoss: 0.4489559531211853 - trainLoss: 0.4550274610519409\n",
      "cnt: 0 - valLoss: 0.44895339012145996 - trainLoss: 0.45502376556396484\n",
      "cnt: 0 - valLoss: 0.44895055890083313 - trainLoss: 0.45502012968063354\n",
      "cnt: 0 - valLoss: 0.4489480257034302 - trainLoss: 0.45501649379730225\n",
      "cnt: 0 - valLoss: 0.44894543290138245 - trainLoss: 0.45501285791397095\n",
      "cnt: 0 - valLoss: 0.44894272089004517 - trainLoss: 0.4550091624259949\n",
      "cnt: 0 - valLoss: 0.44894006848335266 - trainLoss: 0.4550055265426636\n",
      "cnt: 0 - valLoss: 0.448937326669693 - trainLoss: 0.4550018608570099\n",
      "cnt: 0 - valLoss: 0.4489346742630005 - trainLoss: 0.454998254776001\n",
      "cnt: 0 - valLoss: 0.4489319920539856 - trainLoss: 0.4549945890903473\n",
      "cnt: 0 - valLoss: 0.4489293396472931 - trainLoss: 0.4549909234046936\n",
      "cnt: 0 - valLoss: 0.44892677664756775 - trainLoss: 0.45498722791671753\n",
      "cnt: 0 - valLoss: 0.4489240348339081 - trainLoss: 0.4549836218357086\n",
      "cnt: 0 - valLoss: 0.4489215016365051 - trainLoss: 0.45497992634773254\n",
      "cnt: 0 - valLoss: 0.4489186704158783 - trainLoss: 0.45497629046440125\n",
      "cnt: 0 - valLoss: 0.44891610741615295 - trainLoss: 0.45497265458106995\n",
      "cnt: 0 - valLoss: 0.44891348481178284 - trainLoss: 0.45496898889541626\n",
      "cnt: 0 - valLoss: 0.44891083240509033 - trainLoss: 0.45496538281440735\n",
      "cnt: 0 - valLoss: 0.4489081799983978 - trainLoss: 0.45496171712875366\n",
      "cnt: 0 - valLoss: 0.44890546798706055 - trainLoss: 0.45495808124542236\n",
      "cnt: 0 - valLoss: 0.44890284538269043 - trainLoss: 0.4549544155597687\n",
      "cnt: 0 - valLoss: 0.44890016317367554 - trainLoss: 0.4549507796764374\n",
      "cnt: 0 - valLoss: 0.44889745116233826 - trainLoss: 0.4549471437931061\n",
      "cnt: 0 - valLoss: 0.4488949775695801 - trainLoss: 0.4549434781074524\n",
      "cnt: 0 - valLoss: 0.4488922357559204 - trainLoss: 0.4549398422241211\n",
      "cnt: 0 - valLoss: 0.4488896429538727 - trainLoss: 0.4549362063407898\n",
      "cnt: 0 - valLoss: 0.44888684153556824 - trainLoss: 0.4549325108528137\n",
      "cnt: 0 - valLoss: 0.44888433814048767 - trainLoss: 0.4549289047718048\n",
      "cnt: 0 - valLoss: 0.44888150691986084 - trainLoss: 0.4549252390861511\n",
      "cnt: 0 - valLoss: 0.44887903332710266 - trainLoss: 0.4549216330051422\n",
      "cnt: 0 - valLoss: 0.44887641072273254 - trainLoss: 0.4549179673194885\n",
      "cnt: 0 - valLoss: 0.4488736689090729 - trainLoss: 0.4549143314361572\n",
      "cnt: 0 - valLoss: 0.44887104630470276 - trainLoss: 0.4549106955528259\n",
      "cnt: 0 - valLoss: 0.44886839389801025 - trainLoss: 0.45490705966949463\n",
      "cnt: 0 - valLoss: 0.44886574149131775 - trainLoss: 0.45490339398384094\n",
      "cnt: 0 - valLoss: 0.44886326789855957 - trainLoss: 0.45489972829818726\n",
      "cnt: 0 - valLoss: 0.44886043667793274 - trainLoss: 0.45489612221717834\n",
      "cnt: 0 - valLoss: 0.44885799288749695 - trainLoss: 0.45489248633384705\n",
      "cnt: 0 - valLoss: 0.4488551914691925 - trainLoss: 0.45488885045051575\n",
      "cnt: 0 - valLoss: 0.44885262846946716 - trainLoss: 0.45488521456718445\n",
      "cnt: 0 - valLoss: 0.4488498270511627 - trainLoss: 0.45488157868385315\n",
      "cnt: 0 - valLoss: 0.44884732365608215 - trainLoss: 0.45487794280052185\n",
      "cnt: 0 - valLoss: 0.44884470105171204 - trainLoss: 0.45487430691719055\n",
      "cnt: 0 - valLoss: 0.44884198904037476 - trainLoss: 0.45487070083618164\n",
      "cnt: 0 - valLoss: 0.44883933663368225 - trainLoss: 0.45486703515052795\n",
      "cnt: 0 - valLoss: 0.44883668422698975 - trainLoss: 0.45486339926719666\n",
      "cnt: 0 - valLoss: 0.44883403182029724 - trainLoss: 0.45485979318618774\n",
      "cnt: 0 - valLoss: 0.44883131980895996 - trainLoss: 0.45485612750053406\n",
      "cnt: 0 - valLoss: 0.44882866740226746 - trainLoss: 0.45485246181488037\n",
      "cnt: 0 - valLoss: 0.44882622361183167 - trainLoss: 0.4548488259315491\n",
      "cnt: 0 - valLoss: 0.44882339239120483 - trainLoss: 0.45484524965286255\n",
      "cnt: 0 - valLoss: 0.4488208293914795 - trainLoss: 0.45484161376953125\n",
      "cnt: 0 - valLoss: 0.44881802797317505 - trainLoss: 0.4548379182815552\n",
      "cnt: 0 - valLoss: 0.4488154947757721 - trainLoss: 0.45483431220054626\n",
      "cnt: 0 - valLoss: 0.44881290197372437 - trainLoss: 0.45483067631721497\n",
      "cnt: 0 - valLoss: 0.4488101601600647 - trainLoss: 0.45482704043388367\n",
      "cnt: 0 - valLoss: 0.4488075375556946 - trainLoss: 0.45482346415519714\n",
      "cnt: 0 - valLoss: 0.4488048851490021 - trainLoss: 0.45481982827186584\n",
      "cnt: 0 - valLoss: 0.44880226254463196 - trainLoss: 0.45481619238853455\n",
      "cnt: 0 - valLoss: 0.4487995505332947 - trainLoss: 0.45481249690055847\n",
      "cnt: 0 - valLoss: 0.4487968981266022 - trainLoss: 0.45480889081954956\n",
      "cnt: 0 - valLoss: 0.4487944543361664 - trainLoss: 0.45480525493621826\n",
      "cnt: 0 - valLoss: 0.44879159331321716 - trainLoss: 0.45480167865753174\n",
      "cnt: 0 - valLoss: 0.4487890899181366 - trainLoss: 0.45479804277420044\n",
      "cnt: 0 - valLoss: 0.44878628849983215 - trainLoss: 0.45479440689086914\n",
      "cnt: 0 - valLoss: 0.4487837851047516 - trainLoss: 0.45479074120521545\n",
      "cnt: 0 - valLoss: 0.44878101348876953 - trainLoss: 0.45478710532188416\n",
      "cnt: 0 - valLoss: 0.4487784802913666 - trainLoss: 0.45478352904319763\n",
      "cnt: 0 - valLoss: 0.44877588748931885 - trainLoss: 0.45477989315986633\n",
      "cnt: 0 - valLoss: 0.44877323508262634 - trainLoss: 0.4547762870788574\n",
      "cnt: 0 - valLoss: 0.44877058267593384 - trainLoss: 0.4547726511955261\n",
      "cnt: 0 - valLoss: 0.44876790046691895 - trainLoss: 0.4547690153121948\n",
      "cnt: 0 - valLoss: 0.44876527786254883 - trainLoss: 0.4547654092311859\n",
      "cnt: 0 - valLoss: 0.4487626552581787 - trainLoss: 0.4547617435455322\n",
      "cnt: 0 - valLoss: 0.4487600326538086 - trainLoss: 0.4547581374645233\n",
      "cnt: 0 - valLoss: 0.4487575590610504 - trainLoss: 0.454754501581192\n",
      "cnt: 0 - valLoss: 0.44875475764274597 - trainLoss: 0.4547508955001831\n",
      "cnt: 0 - valLoss: 0.4487523138523102 - trainLoss: 0.4547472596168518\n",
      "cnt: 0 - valLoss: 0.44874951243400574 - trainLoss: 0.4547436833381653\n",
      "cnt: 0 - valLoss: 0.4487469792366028 - trainLoss: 0.45474007725715637\n",
      "cnt: 0 - valLoss: 0.44874435663223267 - trainLoss: 0.4547363817691803\n",
      "cnt: 0 - valLoss: 0.44874173402786255 - trainLoss: 0.4547328054904938\n",
      "cnt: 0 - valLoss: 0.4487391412258148 - trainLoss: 0.45472919940948486\n",
      "cnt: 0 - valLoss: 0.44873642921447754 - trainLoss: 0.45472556352615356\n",
      "cnt: 0 - valLoss: 0.4487338066101074 - trainLoss: 0.45472192764282227\n",
      "cnt: 0 - valLoss: 0.4487312436103821 - trainLoss: 0.45471832156181335\n",
      "cnt: 0 - valLoss: 0.4487285912036896 - trainLoss: 0.45471474528312683\n",
      "cnt: 0 - valLoss: 0.448726087808609 - trainLoss: 0.45471104979515076\n",
      "cnt: 0 - valLoss: 0.44872331619262695 - trainLoss: 0.45470747351646423\n",
      "cnt: 0 - valLoss: 0.4487208127975464 - trainLoss: 0.4547038674354553\n",
      "cnt: 0 - valLoss: 0.44871804118156433 - trainLoss: 0.45470017194747925\n",
      "cnt: 0 - valLoss: 0.44871559739112854 - trainLoss: 0.4546966254711151\n",
      "cnt: 0 - valLoss: 0.4487127959728241 - trainLoss: 0.4546929895877838\n",
      "cnt: 0 - valLoss: 0.4487103223800659 - trainLoss: 0.4546894133090973\n",
      "cnt: 0 - valLoss: 0.4487077593803406 - trainLoss: 0.4546857476234436\n",
      "cnt: 0 - valLoss: 0.4487050473690033 - trainLoss: 0.4546821117401123\n",
      "cnt: 0 - valLoss: 0.44870248436927795 - trainLoss: 0.4546785354614258\n",
      "cnt: 0 - valLoss: 0.4486997723579407 - trainLoss: 0.4546748995780945\n",
      "cnt: 0 - valLoss: 0.44869720935821533 - trainLoss: 0.4546712636947632\n",
      "cnt: 0 - valLoss: 0.4486945867538452 - trainLoss: 0.45466768741607666\n",
      "cnt: 0 - valLoss: 0.4486919641494751 - trainLoss: 0.45466405153274536\n",
      "cnt: 0 - valLoss: 0.4486894905567169 - trainLoss: 0.45466047525405884\n",
      "cnt: 0 - valLoss: 0.44868674874305725 - trainLoss: 0.45465683937072754\n",
      "cnt: 0 - valLoss: 0.44868433475494385 - trainLoss: 0.45465323328971863\n",
      "cnt: 0 - valLoss: 0.4486815333366394 - trainLoss: 0.4546496868133545\n",
      "cnt: 0 - valLoss: 0.44867902994155884 - trainLoss: 0.4546459913253784\n",
      "cnt: 0 - valLoss: 0.44867628812789917 - trainLoss: 0.4546424150466919\n",
      "cnt: 0 - valLoss: 0.448673814535141 - trainLoss: 0.454638808965683\n",
      "cnt: 0 - valLoss: 0.44867128133773804 - trainLoss: 0.4546352028846741\n",
      "cnt: 0 - valLoss: 0.44866856932640076 - trainLoss: 0.45463162660598755\n",
      "cnt: 0 - valLoss: 0.4486660063266754 - trainLoss: 0.45462799072265625\n",
      "cnt: 0 - valLoss: 0.4486634433269501 - trainLoss: 0.45462438464164734\n",
      "cnt: 0 - valLoss: 0.44866085052490234 - trainLoss: 0.4546207785606384\n",
      "cnt: 0 - valLoss: 0.44865819811820984 - trainLoss: 0.45461714267730713\n",
      "cnt: 0 - valLoss: 0.4486555755138397 - trainLoss: 0.4546135663986206\n",
      "cnt: 0 - valLoss: 0.44865313172340393 - trainLoss: 0.4546099603176117\n",
      "cnt: 0 - valLoss: 0.44865038990974426 - trainLoss: 0.4546063542366028\n",
      "cnt: 0 - valLoss: 0.4486479163169861 - trainLoss: 0.45460277795791626\n",
      "cnt: 0 - valLoss: 0.4486451745033264 - trainLoss: 0.45459914207458496\n",
      "cnt: 0 - valLoss: 0.4486427307128906 - trainLoss: 0.45459550619125366\n",
      "cnt: 0 - valLoss: 0.44863998889923096 - trainLoss: 0.45459192991256714\n",
      "cnt: 0 - valLoss: 0.448637455701828 - trainLoss: 0.45458829402923584\n",
      "cnt: 0 - valLoss: 0.44863492250442505 - trainLoss: 0.4545847177505493\n",
      "cnt: 0 - valLoss: 0.4486323297023773 - trainLoss: 0.4545811116695404\n",
      "cnt: 0 - valLoss: 0.4486297369003296 - trainLoss: 0.4545775055885315\n",
      "cnt: 0 - valLoss: 0.4486271142959595 - trainLoss: 0.45457392930984497\n",
      "cnt: 0 - valLoss: 0.44862452149391174 - trainLoss: 0.45457032322883606\n",
      "cnt: 0 - valLoss: 0.448621928691864 - trainLoss: 0.45456668734550476\n",
      "cnt: 0 - valLoss: 0.44861936569213867 - trainLoss: 0.4545631408691406\n",
      "cnt: 0 - valLoss: 0.4486168920993805 - trainLoss: 0.4545595049858093\n",
      "cnt: 0 - valLoss: 0.4486141502857208 - trainLoss: 0.4545558989048004\n",
      "cnt: 0 - valLoss: 0.4486117362976074 - trainLoss: 0.4545522630214691\n",
      "cnt: 0 - valLoss: 0.44860899448394775 - trainLoss: 0.454548716545105\n",
      "cnt: 0 - valLoss: 0.4486064612865448 - trainLoss: 0.45454511046409607\n",
      "cnt: 0 - valLoss: 0.4486037790775299 - trainLoss: 0.45454147458076477\n",
      "cnt: 0 - valLoss: 0.4486013352870941 - trainLoss: 0.45453792810440063\n",
      "cnt: 0 - valLoss: 0.44859880208969116 - trainLoss: 0.45453429222106934\n",
      "cnt: 0 - valLoss: 0.44859617948532104 - trainLoss: 0.4545306861400604\n",
      "cnt: 0 - valLoss: 0.4485936164855957 - trainLoss: 0.4545271098613739\n",
      "cnt: 0 - valLoss: 0.44859105348587036 - trainLoss: 0.454523503780365\n",
      "cnt: 0 - valLoss: 0.44858843088150024 - trainLoss: 0.45451995730400085\n",
      "cnt: 0 - valLoss: 0.4485858678817749 - trainLoss: 0.45451632142066956\n",
      "cnt: 0 - valLoss: 0.4485832750797272 - trainLoss: 0.45451271533966064\n",
      "cnt: 0 - valLoss: 0.44858089089393616 - trainLoss: 0.45450910925865173\n",
      "cnt: 0 - valLoss: 0.4485781490802765 - trainLoss: 0.4545055627822876\n",
      "cnt: 0 - valLoss: 0.4485757052898407 - trainLoss: 0.4545019268989563\n",
      "cnt: 0 - valLoss: 0.44857296347618103 - trainLoss: 0.4544983208179474\n",
      "cnt: 0 - valLoss: 0.44857051968574524 - trainLoss: 0.45449477434158325\n",
      "cnt: 0 - valLoss: 0.44856780767440796 - trainLoss: 0.45449113845825195\n",
      "cnt: 0 - valLoss: 0.44856536388397217 - trainLoss: 0.4544875919818878\n",
      "cnt: 0 - valLoss: 0.448562890291214 - trainLoss: 0.4544839859008789\n",
      "cnt: 0 - valLoss: 0.4485602080821991 - trainLoss: 0.45448043942451477\n",
      "cnt: 0 - valLoss: 0.44855764508247375 - trainLoss: 0.45447680354118347\n",
      "cnt: 0 - valLoss: 0.4485550820827484 - trainLoss: 0.45447319746017456\n",
      "cnt: 0 - valLoss: 0.44855251908302307 - trainLoss: 0.4544696509838104\n",
      "cnt: 0 - valLoss: 0.44854989647865295 - trainLoss: 0.4544660151004791\n",
      "cnt: 0 - valLoss: 0.44854736328125 - trainLoss: 0.454462468624115\n",
      "cnt: 0 - valLoss: 0.4485449492931366 - trainLoss: 0.4544588327407837\n",
      "cnt: 0 - valLoss: 0.44854220747947693 - trainLoss: 0.45445528626441956\n",
      "cnt: 0 - valLoss: 0.4485398232936859 - trainLoss: 0.45445168018341064\n",
      "cnt: 0 - valLoss: 0.44853708148002625 - trainLoss: 0.45444807410240173\n",
      "cnt: 0 - valLoss: 0.44853463768959045 - trainLoss: 0.4544444978237152\n",
      "cnt: 0 - valLoss: 0.4485319256782532 - trainLoss: 0.4544408917427063\n",
      "cnt: 0 - valLoss: 0.44852954149246216 - trainLoss: 0.4544372856616974\n",
      "cnt: 0 - valLoss: 0.44852694869041443 - trainLoss: 0.45443373918533325\n",
      "cnt: 0 - valLoss: 0.4485243856906891 - trainLoss: 0.45443016290664673\n",
      "cnt: 0 - valLoss: 0.44852182269096375 - trainLoss: 0.4544265568256378\n",
      "cnt: 0 - valLoss: 0.4485192596912384 - trainLoss: 0.4544229507446289\n",
      "cnt: 0 - valLoss: 0.4485166668891907 - trainLoss: 0.45441940426826477\n",
      "cnt: 0 - valLoss: 0.4485141336917877 - trainLoss: 0.45441579818725586\n",
      "cnt: 0 - valLoss: 0.4485115706920624 - trainLoss: 0.45441222190856934\n",
      "cnt: 0 - valLoss: 0.44850918650627136 - trainLoss: 0.4544086158275604\n",
      "cnt: 0 - valLoss: 0.4485064744949341 - trainLoss: 0.4544050693511963\n",
      "cnt: 0 - valLoss: 0.4485040605068207 - trainLoss: 0.45440152287483215\n",
      "cnt: 0 - valLoss: 0.4485013782978058 - trainLoss: 0.45439791679382324\n",
      "cnt: 0 - valLoss: 0.44849893450737 - trainLoss: 0.4543943405151367\n",
      "cnt: 0 - valLoss: 0.4484962821006775 - trainLoss: 0.4543907344341278\n",
      "cnt: 0 - valLoss: 0.4484938681125641 - trainLoss: 0.45438718795776367\n",
      "cnt: 0 - valLoss: 0.44849133491516113 - trainLoss: 0.45438358187675476\n",
      "cnt: 0 - valLoss: 0.4484887421131134 - trainLoss: 0.4543800354003906\n",
      "cnt: 0 - valLoss: 0.44848623871803284 - trainLoss: 0.4543764293193817\n",
      "cnt: 0 - valLoss: 0.44848358631134033 - trainLoss: 0.4543728530406952\n",
      "cnt: 0 - valLoss: 0.44848111271858215 - trainLoss: 0.45436927676200867\n",
      "cnt: 0 - valLoss: 0.4484785199165344 - trainLoss: 0.45436570048332214\n",
      "cnt: 0 - valLoss: 0.44847604632377625 - trainLoss: 0.4543621242046356\n",
      "cnt: 0 - valLoss: 0.44847363233566284 - trainLoss: 0.4543585479259491\n",
      "cnt: 0 - valLoss: 0.4484708905220032 - trainLoss: 0.4543549418449402\n",
      "cnt: 0 - valLoss: 0.44846853613853455 - trainLoss: 0.45435139536857605\n",
      "cnt: 0 - valLoss: 0.44846585392951965 - trainLoss: 0.45434778928756714\n",
      "cnt: 0 - valLoss: 0.4484633505344391 - trainLoss: 0.454344242811203\n",
      "cnt: 0 - valLoss: 0.44846072793006897 - trainLoss: 0.4543406665325165\n",
      "cnt: 0 - valLoss: 0.4484582841396332 - trainLoss: 0.45433706045150757\n",
      "cnt: 0 - valLoss: 0.448455810546875 - trainLoss: 0.45433351397514343\n",
      "cnt: 0 - valLoss: 0.4484531581401825 - trainLoss: 0.4543299376964569\n",
      "cnt: 0 - valLoss: 0.44845065474510193 - trainLoss: 0.4543263614177704\n",
      "cnt: 0 - valLoss: 0.4484480917453766 - trainLoss: 0.4543227553367615\n",
      "cnt: 0 - valLoss: 0.44844555854797363 - trainLoss: 0.45431920886039734\n",
      "cnt: 0 - valLoss: 0.4484430253505707 - trainLoss: 0.4543156027793884\n",
      "cnt: 0 - valLoss: 0.4484405219554901 - trainLoss: 0.4543120563030243\n",
      "cnt: 0 - valLoss: 0.4484381079673767 - trainLoss: 0.45430848002433777\n",
      "cnt: 0 - valLoss: 0.44843539595603943 - trainLoss: 0.454304963350296\n",
      "cnt: 0 - valLoss: 0.4484330117702484 - trainLoss: 0.4543013274669647\n",
      "cnt: 0 - valLoss: 0.4484303593635559 - trainLoss: 0.4542977511882782\n",
      "cnt: 0 - valLoss: 0.4484279453754425 - trainLoss: 0.4542941749095917\n",
      "cnt: 0 - valLoss: 0.44842529296875 - trainLoss: 0.4542906582355499\n",
      "cnt: 0 - valLoss: 0.44842293858528137 - trainLoss: 0.454287052154541\n",
      "cnt: 0 - valLoss: 0.4484204053878784 - trainLoss: 0.4542834758758545\n",
      "cnt: 0 - valLoss: 0.4484178423881531 - trainLoss: 0.45427995920181274\n",
      "cnt: 0 - valLoss: 0.4484153389930725 - trainLoss: 0.45427635312080383\n",
      "cnt: 0 - valLoss: 0.44841280579566956 - trainLoss: 0.4542728066444397\n",
      "cnt: 0 - valLoss: 0.448410302400589 - trainLoss: 0.4542692303657532\n",
      "cnt: 0 - valLoss: 0.44840776920318604 - trainLoss: 0.45426565408706665\n",
      "cnt: 0 - valLoss: 0.4484052360057831 - trainLoss: 0.4542620778083801\n",
      "cnt: 0 - valLoss: 0.44840288162231445 - trainLoss: 0.4542585611343384\n",
      "cnt: 0 - valLoss: 0.4484001696109772 - trainLoss: 0.45425495505332947\n",
      "cnt: 0 - valLoss: 0.4483978748321533 - trainLoss: 0.45425137877464294\n",
      "cnt: 0 - valLoss: 0.44839513301849365 - trainLoss: 0.4542478621006012\n",
      "cnt: 0 - valLoss: 0.44839271903038025 - trainLoss: 0.4542442560195923\n",
      "cnt: 0 - valLoss: 0.44839006662368774 - trainLoss: 0.45424067974090576\n",
      "cnt: 0 - valLoss: 0.4483877122402191 - trainLoss: 0.45423710346221924\n",
      "cnt: 0 - valLoss: 0.44838520884513855 - trainLoss: 0.4542335569858551\n",
      "cnt: 0 - valLoss: 0.4483826756477356 - trainLoss: 0.45423004031181335\n",
      "cnt: 0 - valLoss: 0.44838017225265503 - trainLoss: 0.45422646403312683\n",
      "cnt: 0 - valLoss: 0.4483776390552521 - trainLoss: 0.4542228579521179\n",
      "cnt: 0 - valLoss: 0.4483751058578491 - trainLoss: 0.4542193114757538\n",
      "cnt: 0 - valLoss: 0.4483725428581238 - trainLoss: 0.45421576499938965\n",
      "cnt: 0 - valLoss: 0.448370099067688 - trainLoss: 0.4542122185230255\n",
      "cnt: 0 - valLoss: 0.448367714881897 - trainLoss: 0.4542086124420166\n",
      "cnt: 0 - valLoss: 0.4483650326728821 - trainLoss: 0.45420506596565247\n",
      "cnt: 0 - valLoss: 0.4483626186847687 - trainLoss: 0.45420148968696594\n",
      "cnt: 0 - valLoss: 0.44835996627807617 - trainLoss: 0.4541979432106018\n",
      "cnt: 0 - valLoss: 0.44835761189460754 - trainLoss: 0.4541943669319153\n",
      "cnt: 0 - valLoss: 0.4483549892902374 - trainLoss: 0.45419085025787354\n",
      "cnt: 0 - valLoss: 0.4483526051044464 - trainLoss: 0.4541872441768646\n",
      "cnt: 0 - valLoss: 0.44835010170936584 - trainLoss: 0.4541836977005005\n",
      "cnt: 0 - valLoss: 0.4483475983142853 - trainLoss: 0.45418015122413635\n",
      "cnt: 0 - valLoss: 0.4483450651168823 - trainLoss: 0.4541766047477722\n",
      "cnt: 0 - valLoss: 0.44834256172180176 - trainLoss: 0.4541730284690857\n",
      "cnt: 0 - valLoss: 0.4483400285243988 - trainLoss: 0.45416948199272156\n",
      "cnt: 0 - valLoss: 0.4483375549316406 - trainLoss: 0.4541659355163574\n",
      "cnt: 0 - valLoss: 0.44833502173423767 - trainLoss: 0.4541623890399933\n",
      "cnt: 0 - valLoss: 0.44833269715309143 - trainLoss: 0.4541587829589844\n",
      "cnt: 0 - valLoss: 0.4483300447463989 - trainLoss: 0.4541552662849426\n",
      "cnt: 0 - valLoss: 0.4483276903629303 - trainLoss: 0.4541517198085785\n",
      "cnt: 0 - valLoss: 0.4483250379562378 - trainLoss: 0.45414817333221436\n",
      "cnt: 0 - valLoss: 0.44832268357276917 - trainLoss: 0.4541446268558502\n",
      "cnt: 0 - valLoss: 0.44832003116607666 - trainLoss: 0.4541410803794861\n",
      "cnt: 0 - valLoss: 0.44831761717796326 - trainLoss: 0.45413750410079956\n",
      "cnt: 0 - valLoss: 0.4483151435852051 - trainLoss: 0.4541339576244354\n",
      "cnt: 0 - valLoss: 0.44831258058547974 - trainLoss: 0.4541304111480713\n",
      "cnt: 0 - valLoss: 0.44831013679504395 - trainLoss: 0.45412686467170715\n",
      "cnt: 0 - valLoss: 0.448307603597641 - trainLoss: 0.4541233479976654\n",
      "cnt: 0 - valLoss: 0.44830507040023804 - trainLoss: 0.4541197419166565\n",
      "cnt: 0 - valLoss: 0.4483025372028351 - trainLoss: 0.45411619544029236\n",
      "cnt: 0 - valLoss: 0.44830000400543213 - trainLoss: 0.4541126787662506\n",
      "cnt: 0 - valLoss: 0.4482976794242859 - trainLoss: 0.4541091024875641\n",
      "cnt: 0 - valLoss: 0.448294997215271 - trainLoss: 0.45410558581352234\n",
      "cnt: 0 - valLoss: 0.4482925534248352 - trainLoss: 0.4541020393371582\n",
      "cnt: 0 - valLoss: 0.4482899308204651 - trainLoss: 0.45409849286079407\n",
      "cnt: 0 - valLoss: 0.4482875466346741 - trainLoss: 0.4540949761867523\n",
      "cnt: 0 - valLoss: 0.4482848644256592 - trainLoss: 0.4540914297103882\n",
      "cnt: 0 - valLoss: 0.44828251004219055 - trainLoss: 0.45408785343170166\n",
      "cnt: 0 - valLoss: 0.44828006625175476 - trainLoss: 0.4540843069553375\n",
      "cnt: 0 - valLoss: 0.44827747344970703 - trainLoss: 0.4540807604789734\n",
      "cnt: 0 - valLoss: 0.44827502965927124 - trainLoss: 0.45407721400260925\n",
      "cnt: 0 - valLoss: 0.4482724368572235 - trainLoss: 0.4540736973285675\n",
      "cnt: 0 - valLoss: 0.44826996326446533 - trainLoss: 0.45407015085220337\n",
      "cnt: 0 - valLoss: 0.44826748967170715 - trainLoss: 0.45406660437583923\n",
      "cnt: 0 - valLoss: 0.4482649564743042 - trainLoss: 0.4540630877017975\n",
      "cnt: 0 - valLoss: 0.44826260209083557 - trainLoss: 0.45405954122543335\n",
      "cnt: 0 - valLoss: 0.4482599198818207 - trainLoss: 0.4540559649467468\n",
      "cnt: 0 - valLoss: 0.44825753569602966 - trainLoss: 0.4540524184703827\n",
      "cnt: 0 - valLoss: 0.44825488328933716 - trainLoss: 0.45404887199401855\n",
      "cnt: 0 - valLoss: 0.44825252890586853 - trainLoss: 0.4540453553199768\n",
      "cnt: 0 - valLoss: 0.4482499063014984 - trainLoss: 0.45404180884361267\n",
      "cnt: 0 - valLoss: 0.448247492313385 - trainLoss: 0.4540382921695709\n",
      "cnt: 0 - valLoss: 0.448245108127594 - trainLoss: 0.4540347456932068\n",
      "cnt: 0 - valLoss: 0.44824254512786865 - trainLoss: 0.45403119921684265\n",
      "cnt: 0 - valLoss: 0.4482400417327881 - trainLoss: 0.4540276825428009\n",
      "cnt: 0 - valLoss: 0.44823750853538513 - trainLoss: 0.45402413606643677\n",
      "cnt: 0 - valLoss: 0.44823503494262695 - trainLoss: 0.45402058959007263\n",
      "cnt: 0 - valLoss: 0.4482327699661255 - trainLoss: 0.4540170729160309\n",
      "cnt: 0 - valLoss: 0.4482300579547882 - trainLoss: 0.45401355624198914\n",
      "cnt: 0 - valLoss: 0.44822773337364197 - trainLoss: 0.4540099799633026\n",
      "cnt: 0 - valLoss: 0.4482250511646271 - trainLoss: 0.45400649309158325\n",
      "cnt: 0 - valLoss: 0.44822269678115845 - trainLoss: 0.45400291681289673\n",
      "cnt: 0 - valLoss: 0.44822007417678833 - trainLoss: 0.45399942994117737\n",
      "cnt: 0 - valLoss: 0.4482177197933197 - trainLoss: 0.45399588346481323\n",
      "cnt: 0 - valLoss: 0.4482152760028839 - trainLoss: 0.4539923369884491\n",
      "cnt: 0 - valLoss: 0.44821271300315857 - trainLoss: 0.45398879051208496\n",
      "cnt: 0 - valLoss: 0.44821029901504517 - trainLoss: 0.45398521423339844\n",
      "cnt: 0 - valLoss: 0.4482077658176422 - trainLoss: 0.4539817273616791\n",
      "cnt: 0 - valLoss: 0.44820523262023926 - trainLoss: 0.45397815108299255\n",
      "cnt: 0 - valLoss: 0.4482027590274811 - trainLoss: 0.4539746642112732\n",
      "cnt: 0 - valLoss: 0.4482002556324005 - trainLoss: 0.45397111773490906\n",
      "cnt: 0 - valLoss: 0.44819796085357666 - trainLoss: 0.4539676010608673\n",
      "cnt: 0 - valLoss: 0.44819530844688416 - trainLoss: 0.45396408438682556\n",
      "cnt: 0 - valLoss: 0.44819289445877075 - trainLoss: 0.4539605677127838\n",
      "cnt: 0 - valLoss: 0.44819024205207825 - trainLoss: 0.45395705103874207\n",
      "cnt: 0 - valLoss: 0.44818779826164246 - trainLoss: 0.45395350456237793\n",
      "cnt: 0 - valLoss: 0.44818517565727234 - trainLoss: 0.4539499878883362\n",
      "cnt: 0 - valLoss: 0.4481827914714813 - trainLoss: 0.4539465010166168\n",
      "cnt: 0 - valLoss: 0.44818028807640076 - trainLoss: 0.4539429247379303\n",
      "cnt: 0 - valLoss: 0.4481777548789978 - trainLoss: 0.4539394676685333\n",
      "cnt: 0 - valLoss: 0.4481752812862396 - trainLoss: 0.4539359211921692\n",
      "cnt: 0 - valLoss: 0.44817274808883667 - trainLoss: 0.45393240451812744\n",
      "cnt: 0 - valLoss: 0.4481702148914337 - trainLoss: 0.4539288878440857\n",
      "cnt: 0 - valLoss: 0.44816768169403076 - trainLoss: 0.45392540097236633\n",
      "cnt: 0 - valLoss: 0.4481651782989502 - trainLoss: 0.4539218246936798\n",
      "cnt: 0 - valLoss: 0.4481627941131592 - trainLoss: 0.45391836762428284\n",
      "cnt: 0 - valLoss: 0.4481601417064667 - trainLoss: 0.4539148509502411\n",
      "cnt: 0 - valLoss: 0.44815778732299805 - trainLoss: 0.45391127467155457\n",
      "cnt: 0 - valLoss: 0.44815513491630554 - trainLoss: 0.4539077877998352\n",
      "cnt: 0 - valLoss: 0.44815272092819214 - trainLoss: 0.45390430092811584\n",
      "cnt: 0 - valLoss: 0.44815006852149963 - trainLoss: 0.4539007246494293\n",
      "cnt: 0 - valLoss: 0.448147714138031 - trainLoss: 0.45389726758003235\n",
      "cnt: 0 - valLoss: 0.4481452405452728 - trainLoss: 0.4538937211036682\n",
      "cnt: 0 - valLoss: 0.4481426477432251 - trainLoss: 0.45389020442962646\n",
      "cnt: 0 - valLoss: 0.4481401741504669 - trainLoss: 0.4538866877555847\n",
      "cnt: 0 - valLoss: 0.44813770055770874 - trainLoss: 0.45388320088386536\n",
      "cnt: 0 - valLoss: 0.4481351971626282 - trainLoss: 0.4538796842098236\n",
      "cnt: 0 - valLoss: 0.44813263416290283 - trainLoss: 0.45387616753578186\n",
      "cnt: 0 - valLoss: 0.44813016057014465 - trainLoss: 0.4538726508617401\n",
      "cnt: 0 - valLoss: 0.4481278359889984 - trainLoss: 0.45386916399002075\n",
      "cnt: 0 - valLoss: 0.44812512397766113 - trainLoss: 0.453865647315979\n",
      "cnt: 0 - valLoss: 0.4481227695941925 - trainLoss: 0.45386213064193726\n",
      "cnt: 0 - valLoss: 0.4481201171875 - trainLoss: 0.4538586139678955\n",
      "cnt: 0 - valLoss: 0.4481177031993866 - trainLoss: 0.45385506749153137\n",
      "cnt: 0 - valLoss: 0.44811514019966125 - trainLoss: 0.4538516104221344\n",
      "cnt: 0 - valLoss: 0.44811275601387024 - trainLoss: 0.45384809374809265\n",
      "cnt: 0 - valLoss: 0.44811031222343445 - trainLoss: 0.4538445770740509\n",
      "cnt: 0 - valLoss: 0.4481077790260315 - trainLoss: 0.45384106040000916\n",
      "cnt: 0 - valLoss: 0.4481053352355957 - trainLoss: 0.4538375735282898\n",
      "cnt: 0 - valLoss: 0.44810277223587036 - trainLoss: 0.45383405685424805\n",
      "cnt: 0 - valLoss: 0.4481002688407898 - trainLoss: 0.4538305401802063\n",
      "cnt: 0 - valLoss: 0.4480977952480316 - trainLoss: 0.45382702350616455\n",
      "cnt: 0 - valLoss: 0.44809529185295105 - trainLoss: 0.4538235366344452\n",
      "cnt: 0 - valLoss: 0.4480929374694824 - trainLoss: 0.45382001996040344\n",
      "cnt: 0 - valLoss: 0.4480902850627899 - trainLoss: 0.4538165330886841\n",
      "cnt: 0 - valLoss: 0.4480879604816437 - trainLoss: 0.4538130462169647\n",
      "cnt: 0 - valLoss: 0.44808530807495117 - trainLoss: 0.453809529542923\n",
      "cnt: 0 - valLoss: 0.44808292388916016 - trainLoss: 0.4538060128688812\n",
      "cnt: 0 - valLoss: 0.44808030128479004 - trainLoss: 0.4538024961948395\n",
      "cnt: 0 - valLoss: 0.4480779469013214 - trainLoss: 0.4537990391254425\n",
      "cnt: 0 - valLoss: 0.4480755031108856 - trainLoss: 0.45379552245140076\n",
      "cnt: 0 - valLoss: 0.4480728805065155 - trainLoss: 0.4537920355796814\n",
      "cnt: 0 - valLoss: 0.4480704963207245 - trainLoss: 0.45378851890563965\n",
      "cnt: 0 - valLoss: 0.4480679929256439 - trainLoss: 0.4537850022315979\n",
      "cnt: 0 - valLoss: 0.44806551933288574 - trainLoss: 0.45378148555755615\n",
      "cnt: 0 - valLoss: 0.448062926530838 - trainLoss: 0.4537780284881592\n",
      "cnt: 0 - valLoss: 0.44806045293807983 - trainLoss: 0.45377451181411743\n",
      "cnt: 0 - valLoss: 0.4480581283569336 - trainLoss: 0.4537709653377533\n",
      "cnt: 0 - valLoss: 0.4480554759502411 - trainLoss: 0.4537675082683563\n",
      "cnt: 0 - valLoss: 0.4480530917644501 - trainLoss: 0.4537639319896698\n",
      "cnt: 0 - valLoss: 0.44805049896240234 - trainLoss: 0.4537605047225952\n",
      "cnt: 0 - valLoss: 0.4480481445789337 - trainLoss: 0.45375701785087585\n",
      "cnt: 0 - valLoss: 0.4480454921722412 - trainLoss: 0.4537535011768341\n",
      "cnt: 0 - valLoss: 0.4480431377887726 - trainLoss: 0.45374998450279236\n",
      "cnt: 0 - valLoss: 0.4480406939983368 - trainLoss: 0.4537464678287506\n",
      "cnt: 0 - valLoss: 0.44803813099861145 - trainLoss: 0.45374298095703125\n",
      "cnt: 0 - valLoss: 0.44803568720817566 - trainLoss: 0.4537394642829895\n",
      "cnt: 0 - valLoss: 0.44803309440612793 - trainLoss: 0.45373600721359253\n",
      "cnt: 0 - valLoss: 0.4480307102203369 - trainLoss: 0.45373252034187317\n",
      "cnt: 0 - valLoss: 0.44802820682525635 - trainLoss: 0.4537290036678314\n",
      "cnt: 0 - valLoss: 0.44802573323249817 - trainLoss: 0.45372554659843445\n",
      "cnt: 0 - valLoss: 0.44802340865135193 - trainLoss: 0.4537220001220703\n",
      "cnt: 0 - valLoss: 0.4480208158493042 - trainLoss: 0.4537185728549957\n",
      "cnt: 0 - valLoss: 0.44801849126815796 - trainLoss: 0.453715056180954\n",
      "cnt: 0 - valLoss: 0.44801583886146545 - trainLoss: 0.45371153950691223\n",
      "cnt: 0 - valLoss: 0.4480134844779968 - trainLoss: 0.45370805263519287\n",
      "cnt: 0 - valLoss: 0.4480110704898834 - trainLoss: 0.4537045657634735\n",
      "cnt: 0 - valLoss: 0.44800859689712524 - trainLoss: 0.45370110869407654\n",
      "cnt: 0 - valLoss: 0.44800615310668945 - trainLoss: 0.4536975920200348\n",
      "cnt: 0 - valLoss: 0.4480036199092865 - trainLoss: 0.45369410514831543\n",
      "cnt: 0 - valLoss: 0.4480011761188507 - trainLoss: 0.45369061827659607\n",
      "cnt: 0 - valLoss: 0.44799864292144775 - trainLoss: 0.4536871612071991\n",
      "cnt: 0 - valLoss: 0.4479961693286896 - trainLoss: 0.45368364453315735\n",
      "cnt: 0 - valLoss: 0.44799384474754333 - trainLoss: 0.4536801278591156\n",
      "cnt: 0 - valLoss: 0.4479912221431732 - trainLoss: 0.45367667078971863\n",
      "cnt: 0 - valLoss: 0.44798895716667175 - trainLoss: 0.4536731541156769\n",
      "cnt: 0 - valLoss: 0.44798630475997925 - trainLoss: 0.4536696970462799\n",
      "cnt: 0 - valLoss: 0.4479839503765106 - trainLoss: 0.45366618037223816\n",
      "cnt: 0 - valLoss: 0.4479813873767853 - trainLoss: 0.4536626636981964\n",
      "cnt: 0 - valLoss: 0.44797906279563904 - trainLoss: 0.4536592364311218\n",
      "cnt: 0 - valLoss: 0.44797661900520325 - trainLoss: 0.45365574955940247\n",
      "cnt: 0 - valLoss: 0.4479740560054779 - trainLoss: 0.4536522328853607\n",
      "cnt: 0 - valLoss: 0.4479716420173645 - trainLoss: 0.45364871621131897\n",
      "cnt: 0 - valLoss: 0.4479691684246063 - trainLoss: 0.4536452889442444\n",
      "cnt: 0 - valLoss: 0.44796672463417053 - trainLoss: 0.45364177227020264\n",
      "cnt: 0 - valLoss: 0.44796428084373474 - trainLoss: 0.4536382853984833\n",
      "cnt: 0 - valLoss: 0.4479617774486542 - trainLoss: 0.4536348581314087\n",
      "cnt: 0 - valLoss: 0.4479594826698303 - trainLoss: 0.45363134145736694\n",
      "cnt: 0 - valLoss: 0.4479568600654602 - trainLoss: 0.4536278247833252\n",
      "cnt: 0 - valLoss: 0.44795453548431396 - trainLoss: 0.45362433791160583\n",
      "cnt: 0 - valLoss: 0.44795191287994385 - trainLoss: 0.4536208510398865\n",
      "cnt: 0 - valLoss: 0.44794961810112 - trainLoss: 0.4536173939704895\n",
      "cnt: 0 - valLoss: 0.44794705510139465 - trainLoss: 0.45361393690109253\n",
      "cnt: 0 - valLoss: 0.44794467091560364 - trainLoss: 0.4536104202270508\n",
      "cnt: 0 - valLoss: 0.447942316532135 - trainLoss: 0.4536069333553314\n",
      "cnt: 0 - valLoss: 0.44793984293937683 - trainLoss: 0.45360347628593445\n",
      "cnt: 0 - valLoss: 0.44793739914894104 - trainLoss: 0.4536000192165375\n",
      "cnt: 0 - valLoss: 0.44793492555618286 - trainLoss: 0.4535965025424957\n",
      "cnt: 0 - valLoss: 0.44793251156806946 - trainLoss: 0.45359304547309875\n",
      "cnt: 0 - valLoss: 0.44793006777763367 - trainLoss: 0.4535895586013794\n",
      "cnt: 0 - valLoss: 0.4479275941848755 - trainLoss: 0.45358607172966003\n",
      "cnt: 0 - valLoss: 0.44792526960372925 - trainLoss: 0.4535825550556183\n",
      "cnt: 0 - valLoss: 0.4479227066040039 - trainLoss: 0.4535790681838989\n",
      "cnt: 0 - valLoss: 0.44792041182518005 - trainLoss: 0.45357561111450195\n",
      "cnt: 0 - valLoss: 0.4479178190231323 - trainLoss: 0.453572154045105\n",
      "cnt: 0 - valLoss: 0.4479154944419861 - trainLoss: 0.4535686671733856\n",
      "cnt: 0 - valLoss: 0.44791293144226074 - trainLoss: 0.45356518030166626\n",
      "cnt: 0 - valLoss: 0.4479106068611145 - trainLoss: 0.4535617530345917\n",
      "cnt: 0 - valLoss: 0.4479082226753235 - trainLoss: 0.4535582363605499\n",
      "cnt: 0 - valLoss: 0.4479057192802429 - trainLoss: 0.45355477929115295\n",
      "cnt: 0 - valLoss: 0.4479033350944519 - trainLoss: 0.4535512924194336\n",
      "cnt: 0 - valLoss: 0.44790083169937134 - trainLoss: 0.45354780554771423\n",
      "cnt: 0 - valLoss: 0.4478984475135803 - trainLoss: 0.4535443186759949\n",
      "cnt: 0 - valLoss: 0.44789594411849976 - trainLoss: 0.4535408616065979\n",
      "cnt: 0 - valLoss: 0.44789355993270874 - trainLoss: 0.45353734493255615\n",
      "cnt: 0 - valLoss: 0.44789132475852966 - trainLoss: 0.45353391766548157\n",
      "cnt: 0 - valLoss: 0.44788867235183716 - trainLoss: 0.4535304307937622\n",
      "cnt: 0 - valLoss: 0.4478863477706909 - trainLoss: 0.45352694392204285\n",
      "cnt: 0 - valLoss: 0.4478837847709656 - trainLoss: 0.4535234868526459\n",
      "cnt: 0 - valLoss: 0.4478815495967865 - trainLoss: 0.4535200297832489\n",
      "cnt: 0 - valLoss: 0.4478789269924164 - trainLoss: 0.45351654291152954\n",
      "cnt: 0 - valLoss: 0.44787657260894775 - trainLoss: 0.4535130262374878\n",
      "cnt: 0 - valLoss: 0.4478742182254791 - trainLoss: 0.45350953936576843\n",
      "cnt: 0 - valLoss: 0.44787177443504333 - trainLoss: 0.45350611209869385\n",
      "cnt: 0 - valLoss: 0.4478693902492523 - trainLoss: 0.4535026252269745\n",
      "cnt: 0 - valLoss: 0.44786688685417175 - trainLoss: 0.4534991681575775\n",
      "cnt: 0 - valLoss: 0.44786450266838074 - trainLoss: 0.45349571108818054\n",
      "cnt: 0 - valLoss: 0.44786208868026733 - trainLoss: 0.4534922242164612\n",
      "cnt: 0 - valLoss: 0.44785964488983154 - trainLoss: 0.4534887671470642\n",
      "cnt: 0 - valLoss: 0.4478573799133301 - trainLoss: 0.45348531007766724\n",
      "cnt: 0 - valLoss: 0.44785481691360474 - trainLoss: 0.45348188281059265\n",
      "cnt: 0 - valLoss: 0.4478525221347809 - trainLoss: 0.4534783661365509\n",
      "cnt: 0 - valLoss: 0.44784995913505554 - trainLoss: 0.45347490906715393\n",
      "cnt: 0 - valLoss: 0.4478476941585541 - trainLoss: 0.45347142219543457\n",
      "cnt: 0 - valLoss: 0.4478452801704407 - trainLoss: 0.4534679651260376\n",
      "cnt: 0 - valLoss: 0.4478428363800049 - trainLoss: 0.4534645080566406\n",
      "cnt: 0 - valLoss: 0.44784045219421387 - trainLoss: 0.45346102118492126\n",
      "cnt: 0 - valLoss: 0.4478379189968109 - trainLoss: 0.4534575641155243\n",
      "cnt: 0 - valLoss: 0.4478355944156647 - trainLoss: 0.4534541070461273\n",
      "cnt: 0 - valLoss: 0.4478331506252289 - trainLoss: 0.45345062017440796\n",
      "cnt: 0 - valLoss: 0.44783076643943787 - trainLoss: 0.45344722270965576\n",
      "cnt: 0 - valLoss: 0.447828471660614 - trainLoss: 0.453443706035614\n",
      "cnt: 0 - valLoss: 0.44782590866088867 - trainLoss: 0.45344027876853943\n",
      "cnt: 0 - valLoss: 0.4478236734867096 - trainLoss: 0.4534367620944977\n",
      "cnt: 0 - valLoss: 0.44782108068466187 - trainLoss: 0.4534333348274231\n",
      "cnt: 0 - valLoss: 0.447818785905838 - trainLoss: 0.4534298777580261\n",
      "cnt: 0 - valLoss: 0.44781628251075745 - trainLoss: 0.45342642068862915\n",
      "cnt: 0 - valLoss: 0.4478139877319336 - trainLoss: 0.4534229338169098\n",
      "cnt: 0 - valLoss: 0.44781163334846497 - trainLoss: 0.4534194767475128\n",
      "cnt: 0 - valLoss: 0.447809100151062 - trainLoss: 0.45341604948043823\n",
      "cnt: 0 - valLoss: 0.44780680537223816 - trainLoss: 0.4534125328063965\n",
      "cnt: 0 - valLoss: 0.44780436158180237 - trainLoss: 0.4534090757369995\n",
      "cnt: 0 - valLoss: 0.44780194759368896 - trainLoss: 0.4534056484699249\n",
      "cnt: 0 - valLoss: 0.4477995038032532 - trainLoss: 0.45340219140052795\n",
      "cnt: 0 - valLoss: 0.44779711961746216 - trainLoss: 0.4533986747264862\n",
      "cnt: 0 - valLoss: 0.4477948844432831 - trainLoss: 0.4533952474594116\n",
      "cnt: 0 - valLoss: 0.44779232144355774 - trainLoss: 0.45339182019233704\n",
      "cnt: 0 - valLoss: 0.4477900266647339 - trainLoss: 0.4533883035182953\n",
      "cnt: 0 - valLoss: 0.4477875232696533 - trainLoss: 0.4533848464488983\n",
      "cnt: 0 - valLoss: 0.4477851986885071 - trainLoss: 0.45338138937950134\n",
      "cnt: 0 - valLoss: 0.4477826952934265 - trainLoss: 0.45337796211242676\n",
      "cnt: 0 - valLoss: 0.44778040051460266 - trainLoss: 0.4533745050430298\n",
      "cnt: 0 - valLoss: 0.44777804613113403 - trainLoss: 0.4533710181713104\n",
      "cnt: 0 - valLoss: 0.44777560234069824 - trainLoss: 0.45336759090423584\n",
      "cnt: 0 - valLoss: 0.4477732479572296 - trainLoss: 0.4533641040325165\n",
      "cnt: 0 - valLoss: 0.4477708041667938 - trainLoss: 0.4533606767654419\n",
      "cnt: 0 - valLoss: 0.4477684497833252 - trainLoss: 0.45335718989372253\n",
      "cnt: 0 - valLoss: 0.4477660655975342 - trainLoss: 0.45335373282432556\n",
      "cnt: 0 - valLoss: 0.4477636218070984 - trainLoss: 0.453350305557251\n",
      "cnt: 0 - valLoss: 0.4477613568305969 - trainLoss: 0.453346848487854\n",
      "cnt: 0 - valLoss: 0.44775888323783875 - trainLoss: 0.45334336161613464\n",
      "cnt: 0 - valLoss: 0.44775664806365967 - trainLoss: 0.45333996415138245\n",
      "cnt: 0 - valLoss: 0.4477541744709015 - trainLoss: 0.45333653688430786\n",
      "cnt: 0 - valLoss: 0.4477519392967224 - trainLoss: 0.4533330202102661\n",
      "cnt: 0 - valLoss: 0.4477494955062866 - trainLoss: 0.45332959294319153\n",
      "cnt: 0 - valLoss: 0.4477473199367523 - trainLoss: 0.45332613587379456\n",
      "cnt: 0 - valLoss: 0.4477449953556061 - trainLoss: 0.45332270860671997\n",
      "cnt: 0 - valLoss: 0.4477425515651703 - trainLoss: 0.4533192217350006\n",
      "cnt: 0 - valLoss: 0.4477402865886688 - trainLoss: 0.45331576466560364\n",
      "cnt: 0 - valLoss: 0.4477379322052002 - trainLoss: 0.45331233739852905\n",
      "cnt: 0 - valLoss: 0.4477355480194092 - trainLoss: 0.4533088803291321\n",
      "cnt: 0 - valLoss: 0.4477333724498749 - trainLoss: 0.4533054232597351\n",
      "cnt: 0 - valLoss: 0.4477308988571167 - trainLoss: 0.4533019959926605\n",
      "cnt: 0 - valLoss: 0.44772869348526 - trainLoss: 0.45329850912094116\n",
      "cnt: 0 - valLoss: 0.44772621989250183 - trainLoss: 0.4532950818538666\n",
      "cnt: 0 - valLoss: 0.44772398471832275 - trainLoss: 0.4532915949821472\n",
      "cnt: 0 - valLoss: 0.4477215111255646 - trainLoss: 0.45328816771507263\n",
      "cnt: 0 - valLoss: 0.4477193057537079 - trainLoss: 0.45328474044799805\n",
      "cnt: 0 - valLoss: 0.4477170407772064 - trainLoss: 0.45328131318092346\n",
      "cnt: 0 - valLoss: 0.44771459698677063 - trainLoss: 0.4532778859138489\n",
      "cnt: 0 - valLoss: 0.44771233201026917 - trainLoss: 0.45327436923980713\n",
      "cnt: 0 - valLoss: 0.44770991802215576 - trainLoss: 0.45327094197273254\n",
      "cnt: 0 - valLoss: 0.4477076232433319 - trainLoss: 0.45326751470565796\n",
      "cnt: 0 - valLoss: 0.4477052092552185 - trainLoss: 0.453264057636261\n",
      "cnt: 0 - valLoss: 0.44770294427871704 - trainLoss: 0.453260600566864\n",
      "cnt: 0 - valLoss: 0.4477007985115051 - trainLoss: 0.45325714349746704\n",
      "cnt: 0 - valLoss: 0.4476982653141022 - trainLoss: 0.45325374603271484\n",
      "cnt: 0 - valLoss: 0.44769608974456787 - trainLoss: 0.4532502293586731\n",
      "cnt: 0 - valLoss: 0.4476935863494873 - trainLoss: 0.4532468020915985\n",
      "cnt: 0 - valLoss: 0.4476914703845978 - trainLoss: 0.4532433748245239\n",
      "cnt: 0 - valLoss: 0.447689026594162 - trainLoss: 0.45323997735977173\n",
      "cnt: 0 - valLoss: 0.4476867914199829 - trainLoss: 0.45323655009269714\n",
      "cnt: 0 - valLoss: 0.44768446683883667 - trainLoss: 0.4532330632209778\n",
      "cnt: 0 - valLoss: 0.44768214225769043 - trainLoss: 0.4532296359539032\n",
      "cnt: 0 - valLoss: 0.4476798474788666 - trainLoss: 0.4532261788845062\n",
      "cnt: 0 - valLoss: 0.44767749309539795 - trainLoss: 0.45322275161743164\n",
      "cnt: 0 - valLoss: 0.4476751983165741 - trainLoss: 0.45321932435035706\n",
      "cnt: 0 - valLoss: 0.4476728141307831 - trainLoss: 0.4532158672809601\n",
      "cnt: 0 - valLoss: 0.4476705491542816 - trainLoss: 0.4532124102115631\n",
      "cnt: 0 - valLoss: 0.4476683437824249 - trainLoss: 0.4532089829444885\n",
      "cnt: 0 - valLoss: 0.44766589999198914 - trainLoss: 0.45320555567741394\n",
      "cnt: 0 - valLoss: 0.44766372442245483 - trainLoss: 0.45320209860801697\n",
      "cnt: 0 - valLoss: 0.44766125082969666 - trainLoss: 0.4531986117362976\n",
      "cnt: 0 - valLoss: 0.4476590156555176 - trainLoss: 0.4531952440738678\n",
      "cnt: 0 - valLoss: 0.4476565718650818 - trainLoss: 0.45319175720214844\n",
      "cnt: 0 - valLoss: 0.4476543366909027 - trainLoss: 0.45318835973739624\n",
      "cnt: 0 - valLoss: 0.44765201210975647 - trainLoss: 0.45318493247032166\n",
      "cnt: 0 - valLoss: 0.44764965772628784 - trainLoss: 0.45318150520324707\n",
      "cnt: 0 - valLoss: 0.447647362947464 - trainLoss: 0.4531780183315277\n",
      "cnt: 0 - valLoss: 0.44764500856399536 - trainLoss: 0.4531746208667755\n",
      "cnt: 0 - valLoss: 0.4476426839828491 - trainLoss: 0.45317113399505615\n",
      "cnt: 0 - valLoss: 0.4476405084133148 - trainLoss: 0.45316770672798157\n",
      "cnt: 0 - valLoss: 0.4476379156112671 - trainLoss: 0.453164279460907\n",
      "cnt: 0 - valLoss: 0.4476357698440552 - trainLoss: 0.4531608819961548\n",
      "cnt: 0 - valLoss: 0.447633296251297 - trainLoss: 0.4531574547290802\n",
      "cnt: 0 - valLoss: 0.4476310610771179 - trainLoss: 0.4531540274620056\n",
      "cnt: 0 - valLoss: 0.44762861728668213 - trainLoss: 0.45315054059028625\n",
      "cnt: 0 - valLoss: 0.4476264417171478 - trainLoss: 0.45314714312553406\n",
      "cnt: 0 - valLoss: 0.4476241171360016 - trainLoss: 0.45314374566078186\n",
      "cnt: 0 - valLoss: 0.44762173295021057 - trainLoss: 0.4531403183937073\n",
      "cnt: 0 - valLoss: 0.4476194381713867 - trainLoss: 0.4531368613243103\n",
      "cnt: 0 - valLoss: 0.4476170837879181 - trainLoss: 0.45313337445259094\n",
      "cnt: 0 - valLoss: 0.44761478900909424 - trainLoss: 0.45312997698783875\n",
      "cnt: 0 - valLoss: 0.44761237502098083 - trainLoss: 0.45312654972076416\n",
      "cnt: 0 - valLoss: 0.44761011004447937 - trainLoss: 0.4531231224536896\n",
      "cnt: 0 - valLoss: 0.44760796427726746 - trainLoss: 0.453119695186615\n",
      "cnt: 0 - valLoss: 0.4476054906845093 - trainLoss: 0.4531162679195404\n",
      "cnt: 0 - valLoss: 0.4476032555103302 - trainLoss: 0.4531128406524658\n",
      "cnt: 0 - valLoss: 0.4476008117198944 - trainLoss: 0.45310941338539124\n",
      "cnt: 0 - valLoss: 0.4475986659526825 - trainLoss: 0.45310598611831665\n",
      "cnt: 0 - valLoss: 0.44759616255760193 - trainLoss: 0.45310255885124207\n",
      "cnt: 0 - valLoss: 0.44759395718574524 - trainLoss: 0.45309916138648987\n",
      "cnt: 0 - valLoss: 0.44759172201156616 - trainLoss: 0.4530957341194153\n",
      "cnt: 0 - valLoss: 0.44758936762809753 - trainLoss: 0.4530923068523407\n",
      "cnt: 0 - valLoss: 0.44758710265159607 - trainLoss: 0.4530888795852661\n",
      "cnt: 0 - valLoss: 0.44758468866348267 - trainLoss: 0.45308545231819153\n",
      "cnt: 0 - valLoss: 0.4475824236869812 - trainLoss: 0.45308202505111694\n",
      "cnt: 0 - valLoss: 0.4475800693035126 - trainLoss: 0.45307859778404236\n",
      "cnt: 0 - valLoss: 0.4475777745246887 - trainLoss: 0.4530751705169678\n",
      "cnt: 0 - valLoss: 0.4475755989551544 - trainLoss: 0.4530717432498932\n",
      "cnt: 0 - valLoss: 0.44757312536239624 - trainLoss: 0.4530683159828186\n",
      "cnt: 0 - valLoss: 0.4475709795951843 - trainLoss: 0.4530649185180664\n",
      "cnt: 0 - valLoss: 0.44756847620010376 - trainLoss: 0.4530614912509918\n",
      "cnt: 0 - valLoss: 0.44756627082824707 - trainLoss: 0.45305806398391724\n",
      "cnt: 0 - valLoss: 0.447564035654068 - trainLoss: 0.45305463671684265\n",
      "cnt: 0 - valLoss: 0.44756171107292175 - trainLoss: 0.45305120944976807\n",
      "cnt: 0 - valLoss: 0.4475593864917755 - trainLoss: 0.4530477821826935\n",
      "cnt: 0 - valLoss: 0.4475570321083069 - trainLoss: 0.4530443549156189\n",
      "cnt: 0 - valLoss: 0.44755473732948303 - trainLoss: 0.4530409276485443\n",
      "cnt: 0 - valLoss: 0.4475524425506592 - trainLoss: 0.4530375003814697\n",
      "cnt: 0 - valLoss: 0.4475501477718353 - trainLoss: 0.45303410291671753\n",
      "cnt: 0 - valLoss: 0.44754794239997864 - trainLoss: 0.45303067564964294\n",
      "cnt: 0 - valLoss: 0.44754549860954285 - trainLoss: 0.45302727818489075\n",
      "cnt: 0 - valLoss: 0.4475433826446533 - trainLoss: 0.4530238211154938\n",
      "cnt: 0 - valLoss: 0.44754087924957275 - trainLoss: 0.4530203938484192\n",
      "cnt: 0 - valLoss: 0.44753873348236084 - trainLoss: 0.4530170261859894\n",
      "cnt: 0 - valLoss: 0.44753628969192505 - trainLoss: 0.4530135989189148\n",
      "cnt: 0 - valLoss: 0.44753414392471313 - trainLoss: 0.4530101716518402\n",
      "cnt: 0 - valLoss: 0.44753187894821167 - trainLoss: 0.4530067443847656\n",
      "cnt: 0 - valLoss: 0.44752949476242065 - trainLoss: 0.4530033469200134\n",
      "cnt: 0 - valLoss: 0.4475272595882416 - trainLoss: 0.45299991965293884\n",
      "cnt: 0 - valLoss: 0.44752493500709534 - trainLoss: 0.45299652218818665\n",
      "cnt: 0 - valLoss: 0.4475226104259491 - trainLoss: 0.45299309492111206\n",
      "cnt: 0 - valLoss: 0.44752025604248047 - trainLoss: 0.4529896676540375\n",
      "cnt: 0 - valLoss: 0.447517991065979 - trainLoss: 0.4529862403869629\n",
      "cnt: 0 - valLoss: 0.44751590490341187 - trainLoss: 0.4529828131198883\n",
      "cnt: 0 - valLoss: 0.4475134015083313 - trainLoss: 0.4529794454574585\n",
      "cnt: 0 - valLoss: 0.447511225938797 - trainLoss: 0.4529760181903839\n",
      "cnt: 0 - valLoss: 0.4475088119506836 - trainLoss: 0.4529725909233093\n",
      "cnt: 0 - valLoss: 0.4475066661834717 - trainLoss: 0.45296919345855713\n",
      "cnt: 0 - valLoss: 0.44750428199768066 - trainLoss: 0.45296576619148254\n",
      "cnt: 0 - valLoss: 0.4475020468235016 - trainLoss: 0.45296239852905273\n",
      "cnt: 0 - valLoss: 0.4474998116493225 - trainLoss: 0.45295894145965576\n",
      "cnt: 0 - valLoss: 0.44749754667282104 - trainLoss: 0.45295554399490356\n",
      "cnt: 0 - valLoss: 0.4474952816963196 - trainLoss: 0.452952116727829\n",
      "cnt: 0 - valLoss: 0.44749292731285095 - trainLoss: 0.45294874906539917\n",
      "cnt: 0 - valLoss: 0.4474906623363495 - trainLoss: 0.4529453217983246\n",
      "cnt: 0 - valLoss: 0.44748860597610474 - trainLoss: 0.45294189453125\n",
      "cnt: 0 - valLoss: 0.44748610258102417 - trainLoss: 0.4529385268688202\n",
      "cnt: 0 - valLoss: 0.44748395681381226 - trainLoss: 0.45293503999710083\n",
      "cnt: 0 - valLoss: 0.4474814832210541 - trainLoss: 0.452931672334671\n",
      "cnt: 0 - valLoss: 0.44747936725616455 - trainLoss: 0.45292824506759644\n",
      "cnt: 0 - valLoss: 0.44747692346572876 - trainLoss: 0.45292484760284424\n",
      "cnt: 0 - valLoss: 0.44747477769851685 - trainLoss: 0.45292145013809204\n",
      "cnt: 0 - valLoss: 0.44747257232666016 - trainLoss: 0.45291802287101746\n",
      "cnt: 0 - valLoss: 0.4474702477455139 - trainLoss: 0.45291459560394287\n",
      "cnt: 0 - valLoss: 0.44746798276901245 - trainLoss: 0.4529111981391907\n",
      "cnt: 0 - valLoss: 0.4474656581878662 - trainLoss: 0.4529077708721161\n",
      "cnt: 0 - valLoss: 0.44746342301368713 - trainLoss: 0.4529044032096863\n",
      "cnt: 0 - valLoss: 0.4474610686302185 - trainLoss: 0.4529009759426117\n",
      "cnt: 0 - valLoss: 0.44745880365371704 - trainLoss: 0.4528976082801819\n",
      "cnt: 0 - valLoss: 0.4474567174911499 - trainLoss: 0.4528941810131073\n",
      "cnt: 0 - valLoss: 0.4474542737007141 - trainLoss: 0.4528907835483551\n",
      "cnt: 0 - valLoss: 0.4474521577358246 - trainLoss: 0.4528873562812805\n",
      "cnt: 0 - valLoss: 0.4474497139453888 - trainLoss: 0.45288392901420593\n",
      "cnt: 0 - valLoss: 0.4474475681781769 - trainLoss: 0.4528805911540985\n",
      "cnt: 0 - valLoss: 0.44744518399238586 - trainLoss: 0.4528771638870239\n",
      "cnt: 0 - valLoss: 0.4474429786205292 - trainLoss: 0.45287376642227173\n",
      "cnt: 0 - valLoss: 0.4474407434463501 - trainLoss: 0.45287033915519714\n",
      "cnt: 0 - valLoss: 0.44743844866752625 - trainLoss: 0.45286694169044495\n",
      "cnt: 0 - valLoss: 0.44743621349334717 - trainLoss: 0.45286357402801514\n",
      "cnt: 0 - valLoss: 0.44743385910987854 - trainLoss: 0.45286014676094055\n",
      "cnt: 0 - valLoss: 0.44743162393569946 - trainLoss: 0.45285671949386597\n",
      "cnt: 0 - valLoss: 0.447429358959198 - trainLoss: 0.45285335183143616\n",
      "cnt: 0 - valLoss: 0.4474271237850189 - trainLoss: 0.45284995436668396\n",
      "cnt: 0 - valLoss: 0.447424978017807 - trainLoss: 0.4528465270996094\n",
      "cnt: 0 - valLoss: 0.447422593832016 - trainLoss: 0.45284315943717957\n",
      "cnt: 0 - valLoss: 0.44742047786712646 - trainLoss: 0.452839732170105\n",
      "cnt: 0 - valLoss: 0.4474180340766907 - trainLoss: 0.4528363347053528\n",
      "cnt: 0 - valLoss: 0.44741588830947876 - trainLoss: 0.452832967042923\n",
      "cnt: 0 - valLoss: 0.44741368293762207 - trainLoss: 0.4528295397758484\n",
      "cnt: 0 - valLoss: 0.4474113881587982 - trainLoss: 0.4528261721134186\n",
      "cnt: 0 - valLoss: 0.44740918278694153 - trainLoss: 0.4528227746486664\n",
      "cnt: 0 - valLoss: 0.4474068582057953 - trainLoss: 0.4528193473815918\n",
      "cnt: 0 - valLoss: 0.4474045932292938 - trainLoss: 0.4528159201145172\n",
      "cnt: 0 - valLoss: 0.44740235805511475 - trainLoss: 0.4528125524520874\n",
      "cnt: 0 - valLoss: 0.4474000930786133 - trainLoss: 0.4528091549873352\n",
      "cnt: 0 - valLoss: 0.44739797711372375 - trainLoss: 0.45280569791793823\n",
      "cnt: 0 - valLoss: 0.44739559292793274 - trainLoss: 0.4528023600578308\n",
      "cnt: 0 - valLoss: 0.4473934769630432 - trainLoss: 0.4527989327907562\n",
      "cnt: 0 - valLoss: 0.4473910331726074 - trainLoss: 0.4527955651283264\n",
      "cnt: 0 - valLoss: 0.4473888874053955 - trainLoss: 0.4527921676635742\n",
      "cnt: 0 - valLoss: 0.4473865032196045 - trainLoss: 0.45278874039649963\n",
      "cnt: 0 - valLoss: 0.44738444685935974 - trainLoss: 0.4527854025363922\n",
      "cnt: 0 - valLoss: 0.4473821520805359 - trainLoss: 0.4527819752693176\n",
      "cnt: 0 - valLoss: 0.44737985730171204 - trainLoss: 0.4527786076068878\n",
      "cnt: 0 - valLoss: 0.44737765192985535 - trainLoss: 0.4527752101421356\n",
      "cnt: 0 - valLoss: 0.4473753571510315 - trainLoss: 0.4527718126773834\n",
      "cnt: 0 - valLoss: 0.4473731219768524 - trainLoss: 0.4527684152126312\n",
      "cnt: 0 - valLoss: 0.44737085700035095 - trainLoss: 0.45276498794555664\n",
      "cnt: 0 - valLoss: 0.4473686218261719 - trainLoss: 0.45276162028312683\n",
      "cnt: 0 - valLoss: 0.4473665654659271 - trainLoss: 0.45275822281837463\n",
      "cnt: 0 - valLoss: 0.44736412167549133 - trainLoss: 0.4527548551559448\n",
      "cnt: 0 - valLoss: 0.4473619759082794 - trainLoss: 0.45275142788887024\n",
      "cnt: 0 - valLoss: 0.4473595917224884 - trainLoss: 0.45274806022644043\n",
      "cnt: 0 - valLoss: 0.4473574757575989 - trainLoss: 0.4527446925640106\n",
      "cnt: 0 - valLoss: 0.4473553001880646 - trainLoss: 0.4527412950992584\n",
      "cnt: 0 - valLoss: 0.44735297560691833 - trainLoss: 0.45273786783218384\n",
      "cnt: 0 - valLoss: 0.44735074043273926 - trainLoss: 0.45273447036743164\n",
      "cnt: 0 - valLoss: 0.4473484754562378 - trainLoss: 0.4527311325073242\n",
      "cnt: 0 - valLoss: 0.4473462402820587 - trainLoss: 0.45272770524024963\n",
      "cnt: 0 - valLoss: 0.44734400510787964 - trainLoss: 0.45272427797317505\n",
      "cnt: 0 - valLoss: 0.4473417401313782 - trainLoss: 0.4527209401130676\n",
      "cnt: 0 - valLoss: 0.4473396837711334 - trainLoss: 0.45271754264831543\n",
      "cnt: 0 - valLoss: 0.4473372995853424 - trainLoss: 0.4527141749858856\n",
      "cnt: 0 - valLoss: 0.44733524322509766 - trainLoss: 0.4527107775211334\n",
      "cnt: 0 - valLoss: 0.44733282923698425 - trainLoss: 0.452707439661026\n",
      "cnt: 0 - valLoss: 0.44733068346977234 - trainLoss: 0.4527040123939514\n",
      "cnt: 0 - valLoss: 0.4473282992839813 - trainLoss: 0.4527006149291992\n",
      "cnt: 0 - valLoss: 0.4473262429237366 - trainLoss: 0.4526972472667694\n",
      "cnt: 0 - valLoss: 0.4473240077495575 - trainLoss: 0.4526938498020172\n",
      "cnt: 0 - valLoss: 0.44732168316841125 - trainLoss: 0.4526904821395874\n",
      "cnt: 0 - valLoss: 0.44731953740119934 - trainLoss: 0.4526870846748352\n",
      "cnt: 0 - valLoss: 0.44731730222702026 - trainLoss: 0.4526837170124054\n",
      "cnt: 0 - valLoss: 0.4473150372505188 - trainLoss: 0.4526802897453308\n",
      "cnt: 0 - valLoss: 0.44731295108795166 - trainLoss: 0.452676922082901\n",
      "cnt: 0 - valLoss: 0.44731053709983826 - trainLoss: 0.4526735246181488\n",
      "cnt: 0 - valLoss: 0.4473084807395935 - trainLoss: 0.452670156955719\n",
      "cnt: 0 - valLoss: 0.4473060965538025 - trainLoss: 0.4526667594909668\n",
      "cnt: 0 - valLoss: 0.4473039507865906 - trainLoss: 0.452663391828537\n",
      "cnt: 0 - valLoss: 0.44730162620544434 - trainLoss: 0.4526599943637848\n",
      "cnt: 0 - valLoss: 0.4472995698451996 - trainLoss: 0.452656626701355\n",
      "cnt: 0 - valLoss: 0.4472973346710205 - trainLoss: 0.4526532292366028\n",
      "cnt: 0 - valLoss: 0.44729503989219666 - trainLoss: 0.452649861574173\n",
      "cnt: 0 - valLoss: 0.44729289412498474 - trainLoss: 0.4526464641094208\n",
      "cnt: 0 - valLoss: 0.4472905993461609 - trainLoss: 0.45264312624931335\n",
      "cnt: 0 - valLoss: 0.4472884237766266 - trainLoss: 0.45263972878456116\n",
      "cnt: 0 - valLoss: 0.4472861886024475 - trainLoss: 0.45263633131980896\n",
      "cnt: 0 - valLoss: 0.4472839832305908 - trainLoss: 0.45263296365737915\n",
      "cnt: 0 - valLoss: 0.4472818672657013 - trainLoss: 0.45262956619262695\n",
      "cnt: 0 - valLoss: 0.44727951288223267 - trainLoss: 0.45262622833251953\n",
      "cnt: 0 - valLoss: 0.4472774267196655 - trainLoss: 0.4526228606700897\n",
      "cnt: 0 - valLoss: 0.4472750425338745 - trainLoss: 0.45261943340301514\n",
      "cnt: 0 - valLoss: 0.44727298617362976 - trainLoss: 0.4526160955429077\n",
      "cnt: 0 - valLoss: 0.44727060198783875 - trainLoss: 0.4526126980781555\n",
      "cnt: 0 - valLoss: 0.447268545627594 - trainLoss: 0.4526093304157257\n",
      "cnt: 0 - valLoss: 0.4472663104534149 - trainLoss: 0.4526059031486511\n",
      "cnt: 0 - valLoss: 0.44726407527923584 - trainLoss: 0.4526025652885437\n",
      "cnt: 0 - valLoss: 0.44726189970970154 - trainLoss: 0.4525992274284363\n",
      "cnt: 0 - valLoss: 0.4472596347332001 - trainLoss: 0.4525958001613617\n",
      "cnt: 0 - valLoss: 0.447257399559021 - trainLoss: 0.4525924623012543\n",
      "cnt: 0 - valLoss: 0.44725537300109863 - trainLoss: 0.4525890350341797\n",
      "cnt: 0 - valLoss: 0.44725301861763 - trainLoss: 0.4525856673717499\n",
      "cnt: 0 - valLoss: 0.44725096225738525 - trainLoss: 0.45258232951164246\n",
      "cnt: 0 - valLoss: 0.44724857807159424 - trainLoss: 0.45257893204689026\n",
      "cnt: 0 - valLoss: 0.4472465217113495 - trainLoss: 0.45257559418678284\n",
      "cnt: 0 - valLoss: 0.44724413752555847 - trainLoss: 0.45257216691970825\n",
      "cnt: 0 - valLoss: 0.44724205136299133 - trainLoss: 0.45256882905960083\n",
      "cnt: 0 - valLoss: 0.4472399055957794 - trainLoss: 0.452565461397171\n",
      "cnt: 0 - valLoss: 0.44723767042160034 - trainLoss: 0.4525620639324188\n",
      "cnt: 0 - valLoss: 0.44723546504974365 - trainLoss: 0.4525587260723114\n",
      "cnt: 0 - valLoss: 0.4472331702709198 - trainLoss: 0.4525552988052368\n",
      "cnt: 0 - valLoss: 0.4472309947013855 - trainLoss: 0.4525519609451294\n",
      "cnt: 0 - valLoss: 0.4472288191318512 - trainLoss: 0.4525485932826996\n",
      "cnt: 0 - valLoss: 0.4472265839576721 - trainLoss: 0.4525451958179474\n",
      "cnt: 0 - valLoss: 0.4472244679927826 - trainLoss: 0.45254185795783997\n",
      "cnt: 0 - valLoss: 0.4472220838069916 - trainLoss: 0.45253852009773254\n",
      "cnt: 0 - valLoss: 0.44721996784210205 - trainLoss: 0.45253509283065796\n",
      "cnt: 0 - valLoss: 0.44721758365631104 - trainLoss: 0.45253175497055054\n",
      "cnt: 0 - valLoss: 0.4472154676914215 - trainLoss: 0.4525284171104431\n",
      "cnt: 0 - valLoss: 0.4472132623195648 - trainLoss: 0.4525250494480133\n",
      "cnt: 0 - valLoss: 0.44721102714538574 - trainLoss: 0.45252174139022827\n",
      "cnt: 0 - valLoss: 0.44720879197120667 - trainLoss: 0.45251837372779846\n",
      "cnt: 0 - valLoss: 0.4472064673900604 - trainLoss: 0.45251497626304626\n",
      "cnt: 0 - valLoss: 0.4472042918205261 - trainLoss: 0.45251163840293884\n",
      "cnt: 0 - valLoss: 0.44720202684402466 - trainLoss: 0.4525083005428314\n",
      "cnt: 0 - valLoss: 0.44719982147216797 - trainLoss: 0.452504962682724\n",
      "cnt: 0 - valLoss: 0.44719773530960083 - trainLoss: 0.4525016248226166\n",
      "cnt: 0 - valLoss: 0.4471953213214874 - trainLoss: 0.45249828696250916\n",
      "cnt: 0 - valLoss: 0.4471932649612427 - trainLoss: 0.45249485969543457\n",
      "cnt: 0 - valLoss: 0.44719088077545166 - trainLoss: 0.4524915814399719\n",
      "cnt: 0 - valLoss: 0.4471888244152069 - trainLoss: 0.4524881839752197\n",
      "cnt: 0 - valLoss: 0.4471864104270935 - trainLoss: 0.4524848461151123\n",
      "cnt: 0 - valLoss: 0.4471842646598816 - trainLoss: 0.4524815082550049\n",
      "cnt: 0 - valLoss: 0.4471821188926697 - trainLoss: 0.45247817039489746\n",
      "cnt: 0 - valLoss: 0.4471798539161682 - trainLoss: 0.45247483253479004\n",
      "cnt: 0 - valLoss: 0.44717761874198914 - trainLoss: 0.4524714946746826\n",
      "cnt: 0 - valLoss: 0.44717535376548767 - trainLoss: 0.45246806740760803\n",
      "cnt: 0 - valLoss: 0.4471731185913086 - trainLoss: 0.4524647891521454\n",
      "cnt: 0 - valLoss: 0.44717106223106384 - trainLoss: 0.4524613916873932\n",
      "cnt: 0 - valLoss: 0.4471686780452728 - trainLoss: 0.45245805382728577\n",
      "cnt: 0 - valLoss: 0.4471666216850281 - trainLoss: 0.45245471596717834\n",
      "cnt: 0 - valLoss: 0.4471642076969147 - trainLoss: 0.45245134830474854\n",
      "cnt: 0 - valLoss: 0.4471621513366699 - trainLoss: 0.4524480402469635\n",
      "cnt: 0 - valLoss: 0.4471597671508789 - trainLoss: 0.4524447023868561\n",
      "cnt: 0 - valLoss: 0.44715771079063416 - trainLoss: 0.45244136452674866\n",
      "cnt: 0 - valLoss: 0.4471554756164551 - trainLoss: 0.45243802666664124\n",
      "cnt: 0 - valLoss: 0.447153240442276 - trainLoss: 0.4524346888065338\n",
      "cnt: 0 - valLoss: 0.4471510052680969 - trainLoss: 0.452431321144104\n",
      "cnt: 0 - valLoss: 0.44714877009391785 - trainLoss: 0.4524279832839966\n",
      "cnt: 0 - valLoss: 0.44714656472206116 - trainLoss: 0.45242464542388916\n",
      "cnt: 0 - valLoss: 0.44714435935020447 - trainLoss: 0.45242124795913696\n",
      "cnt: 0 - valLoss: 0.447142094373703 - trainLoss: 0.45241791009902954\n",
      "cnt: 0 - valLoss: 0.44714003801345825 - trainLoss: 0.4524145722389221\n",
      "cnt: 0 - valLoss: 0.44713765382766724 - trainLoss: 0.4524112939834595\n",
      "cnt: 0 - valLoss: 0.4471355676651001 - trainLoss: 0.45240795612335205\n",
      "cnt: 0 - valLoss: 0.4471331834793091 - trainLoss: 0.452404648065567\n",
      "cnt: 0 - valLoss: 0.44713103771209717 - trainLoss: 0.4524012804031372\n",
      "cnt: 0 - valLoss: 0.44712889194488525 - trainLoss: 0.452397882938385\n",
      "cnt: 0 - valLoss: 0.4471266567707062 - trainLoss: 0.45239460468292236\n",
      "cnt: 0 - valLoss: 0.4471244513988495 - trainLoss: 0.45239129662513733\n",
      "cnt: 0 - valLoss: 0.44712215662002563 - trainLoss: 0.4523879587650299\n",
      "cnt: 0 - valLoss: 0.44711995124816895 - trainLoss: 0.4523846209049225\n",
      "cnt: 0 - valLoss: 0.4471178948879242 - trainLoss: 0.45238128304481506\n",
      "cnt: 0 - valLoss: 0.4471155107021332 - trainLoss: 0.45237794518470764\n",
      "cnt: 0 - valLoss: 0.4471134543418884 - trainLoss: 0.4523746073246002\n",
      "cnt: 0 - valLoss: 0.4471111595630646 - trainLoss: 0.4523712694644928\n",
      "cnt: 0 - valLoss: 0.4471091032028198 - trainLoss: 0.4523679316043854\n",
      "cnt: 0 - valLoss: 0.4471069574356079 - trainLoss: 0.45236459374427795\n",
      "cnt: 0 - valLoss: 0.44710472226142883 - trainLoss: 0.45236125588417053\n",
      "cnt: 0 - valLoss: 0.44710254669189453 - trainLoss: 0.4523579180240631\n",
      "cnt: 0 - valLoss: 0.44710031151771545 - trainLoss: 0.4523545801639557\n",
      "cnt: 0 - valLoss: 0.44709816575050354 - trainLoss: 0.45235127210617065\n",
      "cnt: 0 - valLoss: 0.4470961391925812 - trainLoss: 0.45234793424606323\n",
      "cnt: 0 - valLoss: 0.4470938444137573 - trainLoss: 0.4523446261882782\n",
      "cnt: 0 - valLoss: 0.4470917284488678 - trainLoss: 0.452341228723526\n",
      "cnt: 0 - valLoss: 0.4470895826816559 - trainLoss: 0.45233795046806335\n",
      "cnt: 0 - valLoss: 0.4470874071121216 - trainLoss: 0.45233458280563354\n",
      "cnt: 0 - valLoss: 0.44708526134490967 - trainLoss: 0.4523312747478485\n",
      "cnt: 0 - valLoss: 0.44708311557769775 - trainLoss: 0.4523279368877411\n",
      "cnt: 0 - valLoss: 0.4470808804035187 - trainLoss: 0.45232465863227844\n",
      "cnt: 0 - valLoss: 0.4470788240432739 - trainLoss: 0.45232126116752625\n",
      "cnt: 0 - valLoss: 0.4470765292644501 - trainLoss: 0.4523179829120636\n",
      "cnt: 0 - valLoss: 0.4470745325088501 - trainLoss: 0.4523145854473114\n",
      "cnt: 0 - valLoss: 0.44707217812538147 - trainLoss: 0.45231133699417114\n",
      "cnt: 0 - valLoss: 0.4470701217651367 - trainLoss: 0.4523079991340637\n",
      "cnt: 0 - valLoss: 0.4470680356025696 - trainLoss: 0.4523046612739563\n",
      "cnt: 0 - valLoss: 0.4470658004283905 - trainLoss: 0.4523013234138489\n",
      "cnt: 0 - valLoss: 0.4470636546611786 - trainLoss: 0.45229798555374146\n",
      "cnt: 0 - valLoss: 0.4470614194869995 - trainLoss: 0.45229464769363403\n",
      "cnt: 0 - valLoss: 0.4470592737197876 - trainLoss: 0.452291339635849\n",
      "cnt: 0 - valLoss: 0.44705730676651 - trainLoss: 0.4522880017757416\n",
      "cnt: 0 - valLoss: 0.44705501198768616 - trainLoss: 0.45228472352027893\n",
      "cnt: 0 - valLoss: 0.4470529556274414 - trainLoss: 0.4522813856601715\n",
      "cnt: 0 - valLoss: 0.44705063104629517 - trainLoss: 0.4522780478000641\n",
      "cnt: 0 - valLoss: 0.4470486640930176 - trainLoss: 0.45227473974227905\n",
      "cnt: 0 - valLoss: 0.44704651832580566 - trainLoss: 0.45227140188217163\n",
      "cnt: 0 - valLoss: 0.4470442533493042 - trainLoss: 0.452268123626709\n",
      "cnt: 0 - valLoss: 0.4470421373844147 - trainLoss: 0.45226478576660156\n",
      "cnt: 0 - valLoss: 0.44703999161720276 - trainLoss: 0.45226144790649414\n",
      "cnt: 0 - valLoss: 0.44703781604766846 - trainLoss: 0.4522581100463867\n",
      "cnt: 0 - valLoss: 0.4470357894897461 - trainLoss: 0.4522547423839569\n",
      "cnt: 0 - valLoss: 0.44703349471092224 - trainLoss: 0.45225149393081665\n",
      "cnt: 0 - valLoss: 0.4470314681529999 - trainLoss: 0.45224815607070923\n",
      "cnt: 0 - valLoss: 0.44702914357185364 - trainLoss: 0.4522448182106018\n",
      "cnt: 0 - valLoss: 0.4470270872116089 - trainLoss: 0.45224153995513916\n",
      "cnt: 0 - valLoss: 0.44702500104904175 - trainLoss: 0.45223820209503174\n",
      "cnt: 0 - valLoss: 0.44702282547950745 - trainLoss: 0.4522348642349243\n",
      "cnt: 0 - valLoss: 0.44702067971229553 - trainLoss: 0.4522315561771393\n",
      "cnt: 0 - valLoss: 0.44701847434043884 - trainLoss: 0.45222821831703186\n",
      "cnt: 0 - valLoss: 0.44701629877090454 - trainLoss: 0.45222488045692444\n",
      "cnt: 0 - valLoss: 0.44701436161994934 - trainLoss: 0.4522216320037842\n",
      "cnt: 0 - valLoss: 0.4470120072364807 - trainLoss: 0.45221829414367676\n",
      "cnt: 0 - valLoss: 0.44700995087623596 - trainLoss: 0.45221495628356934\n",
      "cnt: 0 - valLoss: 0.4470076858997345 - trainLoss: 0.4522116184234619\n",
      "cnt: 0 - valLoss: 0.44700565934181213 - trainLoss: 0.4522083103656769\n",
      "cnt: 0 - valLoss: 0.447003573179245 - trainLoss: 0.45220497250556946\n",
      "cnt: 0 - valLoss: 0.4470013380050659 - trainLoss: 0.4522016942501068\n",
      "cnt: 0 - valLoss: 0.4469992518424988 - trainLoss: 0.4521983563899994\n",
      "cnt: 0 - valLoss: 0.4469970464706421 - trainLoss: 0.45219504833221436\n",
      "cnt: 0 - valLoss: 0.4469949007034302 - trainLoss: 0.4521917402744293\n",
      "cnt: 0 - valLoss: 0.4469929337501526 - trainLoss: 0.4521884024143219\n",
      "cnt: 0 - valLoss: 0.44699060916900635 - trainLoss: 0.4521850645542145\n",
      "cnt: 0 - valLoss: 0.44698861241340637 - trainLoss: 0.45218178629875183\n",
      "cnt: 0 - valLoss: 0.4469863474369049 - trainLoss: 0.4521784782409668\n",
      "cnt: 0 - valLoss: 0.4469843804836273 - trainLoss: 0.4521751403808594\n",
      "cnt: 0 - valLoss: 0.4469822347164154 - trainLoss: 0.45217180252075195\n",
      "cnt: 0 - valLoss: 0.44697999954223633 - trainLoss: 0.4521685242652893\n",
      "cnt: 0 - valLoss: 0.4469779431819916 - trainLoss: 0.4521652162075043\n",
      "cnt: 0 - valLoss: 0.44697579741477966 - trainLoss: 0.45216187834739685\n",
      "cnt: 0 - valLoss: 0.44697365164756775 - trainLoss: 0.4521586298942566\n",
      "cnt: 0 - valLoss: 0.44697168469429016 - trainLoss: 0.45215529203414917\n",
      "cnt: 0 - valLoss: 0.4469693601131439 - trainLoss: 0.45215198397636414\n",
      "cnt: 0 - valLoss: 0.44696730375289917 - trainLoss: 0.4521487057209015\n",
      "cnt: 0 - valLoss: 0.4469650685787201 - trainLoss: 0.45214536786079407\n",
      "cnt: 0 - valLoss: 0.4469631016254425 - trainLoss: 0.45214203000068665\n",
      "cnt: 0 - valLoss: 0.4469609558582306 - trainLoss: 0.452138751745224\n",
      "cnt: 0 - valLoss: 0.4469587802886963 - trainLoss: 0.4521354138851166\n",
      "cnt: 0 - valLoss: 0.44695666432380676 - trainLoss: 0.45213207602500916\n",
      "cnt: 0 - valLoss: 0.44695454835891724 - trainLoss: 0.4521288275718689\n",
      "cnt: 0 - valLoss: 0.4469524025917053 - trainLoss: 0.4521254897117615\n",
      "cnt: 0 - valLoss: 0.44695040583610535 - trainLoss: 0.45212215185165405\n",
      "cnt: 0 - valLoss: 0.4469481110572815 - trainLoss: 0.4521189033985138\n",
      "cnt: 0 - valLoss: 0.4469461739063263 - trainLoss: 0.45211556553840637\n",
      "cnt: 0 - valLoss: 0.44694384932518005 - trainLoss: 0.45211225748062134\n",
      "cnt: 0 - valLoss: 0.4469418525695801 - trainLoss: 0.4521089792251587\n",
      "cnt: 0 - valLoss: 0.44693973660469055 - trainLoss: 0.45210564136505127\n",
      "cnt: 0 - valLoss: 0.44693756103515625 - trainLoss: 0.45210230350494385\n",
      "cnt: 0 - valLoss: 0.4469354748725891 - trainLoss: 0.4520990550518036\n",
      "cnt: 0 - valLoss: 0.4469332695007324 - trainLoss: 0.45209571719169617\n",
      "cnt: 0 - valLoss: 0.4469311833381653 - trainLoss: 0.45209237933158875\n",
      "cnt: 0 - valLoss: 0.4469292461872101 - trainLoss: 0.4520891010761261\n",
      "cnt: 0 - valLoss: 0.44692692160606384 - trainLoss: 0.45208582282066345\n",
      "cnt: 0 - valLoss: 0.44692492485046387 - trainLoss: 0.4520825147628784\n",
      "cnt: 0 - valLoss: 0.4469226598739624 - trainLoss: 0.452079176902771\n",
      "cnt: 0 - valLoss: 0.4469206631183624 - trainLoss: 0.45207592844963074\n",
      "cnt: 0 - valLoss: 0.4469185769557953 - trainLoss: 0.4520725905895233\n",
      "cnt: 0 - valLoss: 0.446916401386261 - trainLoss: 0.4520692527294159\n",
      "cnt: 0 - valLoss: 0.44691428542137146 - trainLoss: 0.45206594467163086\n",
      "cnt: 0 - valLoss: 0.4469121992588043 - trainLoss: 0.4520626664161682\n",
      "cnt: 0 - valLoss: 0.4469100832939148 - trainLoss: 0.4520593583583832\n",
      "cnt: 0 - valLoss: 0.4469080865383148 - trainLoss: 0.45205602049827576\n",
      "cnt: 0 - valLoss: 0.44690579175949097 - trainLoss: 0.4520527422428131\n",
      "cnt: 0 - valLoss: 0.4469038248062134 - trainLoss: 0.45204949378967285\n",
      "cnt: 0 - valLoss: 0.4469015896320343 - trainLoss: 0.45204615592956543\n",
      "cnt: 0 - valLoss: 0.4468996226787567 - trainLoss: 0.4520427882671356\n",
      "cnt: 0 - valLoss: 0.4468975365161896 - trainLoss: 0.45203953981399536\n",
      "cnt: 0 - valLoss: 0.44689539074897766 - trainLoss: 0.4520362615585327\n",
      "cnt: 0 - valLoss: 0.44689327478408813 - trainLoss: 0.4520329535007477\n",
      "cnt: 0 - valLoss: 0.446891188621521 - trainLoss: 0.45202961564064026\n",
      "cnt: 0 - valLoss: 0.4468890428543091 - trainLoss: 0.4520263671875\n",
      "cnt: 0 - valLoss: 0.4468870759010315 - trainLoss: 0.4520230293273926\n",
      "cnt: 0 - valLoss: 0.4468848407268524 - trainLoss: 0.45201972126960754\n",
      "cnt: 0 - valLoss: 0.44688287377357483 - trainLoss: 0.4520164430141449\n",
      "cnt: 0 - valLoss: 0.44688060879707336 - trainLoss: 0.45201313495635986\n",
      "cnt: 0 - valLoss: 0.44687867164611816 - trainLoss: 0.4520098567008972\n",
      "cnt: 0 - valLoss: 0.44687655568122864 - trainLoss: 0.4520065486431122\n",
      "cnt: 0 - valLoss: 0.44687438011169434 - trainLoss: 0.45200324058532715\n",
      "cnt: 0 - valLoss: 0.4468722939491272 - trainLoss: 0.4519999027252197\n",
      "cnt: 0 - valLoss: 0.446870356798172 - trainLoss: 0.45199665427207947\n",
      "cnt: 0 - valLoss: 0.4468681216239929 - trainLoss: 0.4519933760166168\n",
      "cnt: 0 - valLoss: 0.44686612486839294 - trainLoss: 0.4519900679588318\n",
      "cnt: 0 - valLoss: 0.4468638300895691 - trainLoss: 0.45198678970336914\n",
      "cnt: 0 - valLoss: 0.4468618631362915 - trainLoss: 0.4519834816455841\n",
      "cnt: 0 - valLoss: 0.44685980677604675 - trainLoss: 0.4519801735877991\n",
      "cnt: 0 - valLoss: 0.44685766100883484 - trainLoss: 0.4519768953323364\n",
      "cnt: 0 - valLoss: 0.4468556046485901 - trainLoss: 0.451973557472229\n",
      "cnt: 0 - valLoss: 0.4468534588813782 - trainLoss: 0.45197030901908875\n",
      "cnt: 0 - valLoss: 0.44685137271881104 - trainLoss: 0.4519670009613037\n",
      "cnt: 0 - valLoss: 0.44684934616088867 - trainLoss: 0.4519636929035187\n",
      "cnt: 0 - valLoss: 0.4468471109867096 - trainLoss: 0.45196035504341125\n",
      "cnt: 0 - valLoss: 0.4468452036380768 - trainLoss: 0.451957106590271\n",
      "cnt: 0 - valLoss: 0.44684290885925293 - trainLoss: 0.4519537687301636\n",
      "cnt: 0 - valLoss: 0.44684094190597534 - trainLoss: 0.4519505202770233\n",
      "cnt: 0 - valLoss: 0.4468388855457306 - trainLoss: 0.4519471824169159\n",
      "cnt: 0 - valLoss: 0.4468367397785187 - trainLoss: 0.45194393396377563\n",
      "cnt: 0 - valLoss: 0.4468346834182739 - trainLoss: 0.4519405961036682\n",
      "cnt: 0 - valLoss: 0.4468325972557068 - trainLoss: 0.45193734765052795\n",
      "cnt: 0 - valLoss: 0.44683048129081726 - trainLoss: 0.4519340395927429\n",
      "cnt: 0 - valLoss: 0.4468285143375397 - trainLoss: 0.4519307613372803\n",
      "cnt: 0 - valLoss: 0.4468262493610382 - trainLoss: 0.45192742347717285\n",
      "cnt: 0 - valLoss: 0.446824312210083 - trainLoss: 0.4519241452217102\n",
      "cnt: 0 - valLoss: 0.44682207703590393 - trainLoss: 0.45192086696624756\n",
      "cnt: 0 - valLoss: 0.44682011008262634 - trainLoss: 0.4519175589084625\n",
      "cnt: 0 - valLoss: 0.4468180537223816 - trainLoss: 0.45191431045532227\n",
      "cnt: 0 - valLoss: 0.44681596755981445 - trainLoss: 0.45191097259521484\n",
      "cnt: 0 - valLoss: 0.4468138515949249 - trainLoss: 0.4519077241420746\n",
      "cnt: 0 - valLoss: 0.446811705827713 - trainLoss: 0.45190441608428955\n",
      "cnt: 0 - valLoss: 0.44680964946746826 - trainLoss: 0.4519011378288269\n",
      "cnt: 0 - valLoss: 0.44680777192115784 - trainLoss: 0.45189788937568665\n",
      "cnt: 0 - valLoss: 0.44680550694465637 - trainLoss: 0.4518945813179016\n",
      "cnt: 0 - valLoss: 0.4468035399913788 - trainLoss: 0.4518912732601166\n",
      "cnt: 0 - valLoss: 0.4468013346195221 - trainLoss: 0.45188799500465393\n",
      "cnt: 0 - valLoss: 0.4467993676662445 - trainLoss: 0.4518846869468689\n",
      "cnt: 0 - valLoss: 0.44679731130599976 - trainLoss: 0.45188143849372864\n",
      "cnt: 0 - valLoss: 0.4467952251434326 - trainLoss: 0.4518781006336212\n",
      "cnt: 0 - valLoss: 0.4467931091785431 - trainLoss: 0.45187485218048096\n",
      "cnt: 0 - valLoss: 0.4467909634113312 - trainLoss: 0.4518715441226959\n",
      "cnt: 0 - valLoss: 0.4467889070510864 - trainLoss: 0.4518682658672333\n",
      "cnt: 0 - valLoss: 0.446787029504776 - trainLoss: 0.451865017414093\n",
      "cnt: 0 - valLoss: 0.44678476452827454 - trainLoss: 0.451861709356308\n",
      "cnt: 0 - valLoss: 0.44678282737731934 - trainLoss: 0.45185840129852295\n",
      "cnt: 0 - valLoss: 0.44678059220314026 - trainLoss: 0.4518551528453827\n",
      "cnt: 0 - valLoss: 0.44677862524986267 - trainLoss: 0.45185187458992004\n",
      "cnt: 0 - valLoss: 0.4467765688896179 - trainLoss: 0.451848566532135\n",
      "cnt: 0 - valLoss: 0.44677451252937317 - trainLoss: 0.45184531807899475\n",
      "cnt: 0 - valLoss: 0.44677242636680603 - trainLoss: 0.45184198021888733\n",
      "cnt: 0 - valLoss: 0.4467702805995941 - trainLoss: 0.45183873176574707\n",
      "cnt: 0 - valLoss: 0.44676822423934937 - trainLoss: 0.45183542370796204\n",
      "cnt: 0 - valLoss: 0.44676628708839417 - trainLoss: 0.4518321454524994\n",
      "cnt: 0 - valLoss: 0.4467640519142151 - trainLoss: 0.45182886719703674\n",
      "cnt: 0 - valLoss: 0.44676217436790466 - trainLoss: 0.4518255889415741\n",
      "cnt: 0 - valLoss: 0.4467599093914032 - trainLoss: 0.45182228088378906\n",
      "cnt: 0 - valLoss: 0.446757972240448 - trainLoss: 0.4518190324306488\n",
      "cnt: 0 - valLoss: 0.44675591588020325 - trainLoss: 0.4518156945705414\n",
      "cnt: 0 - valLoss: 0.4467537999153137 - trainLoss: 0.4518124461174011\n",
      "cnt: 0 - valLoss: 0.4467517137527466 - trainLoss: 0.45180919766426086\n",
      "cnt: 0 - valLoss: 0.44674983620643616 - trainLoss: 0.45180585980415344\n",
      "cnt: 0 - valLoss: 0.4467476010322571 - trainLoss: 0.4518026113510132\n",
      "cnt: 0 - valLoss: 0.4467456638813019 - trainLoss: 0.45179933309555054\n",
      "cnt: 0 - valLoss: 0.4467434287071228 - trainLoss: 0.4517960548400879\n",
      "cnt: 0 - valLoss: 0.4467415511608124 - trainLoss: 0.45179274678230286\n",
      "cnt: 0 - valLoss: 0.4467395544052124 - trainLoss: 0.4517894983291626\n",
      "cnt: 0 - valLoss: 0.4467374086380005 - trainLoss: 0.45178624987602234\n",
      "cnt: 0 - valLoss: 0.44673535227775574 - trainLoss: 0.4517829418182373\n",
      "cnt: 0 - valLoss: 0.446733295917511 - trainLoss: 0.45177966356277466\n",
      "cnt: 0 - valLoss: 0.44673120975494385 - trainLoss: 0.4517764151096344\n",
      "cnt: 0 - valLoss: 0.4467293322086334 - trainLoss: 0.45177310705184937\n",
      "cnt: 0 - valLoss: 0.44672709703445435 - trainLoss: 0.4517698585987091\n",
      "cnt: 0 - valLoss: 0.44672513008117676 - trainLoss: 0.4517665505409241\n",
      "cnt: 0 - valLoss: 0.4467228949069977 - trainLoss: 0.4517633020877838\n",
      "cnt: 0 - valLoss: 0.44672101736068726 - trainLoss: 0.45176002383232117\n",
      "cnt: 0 - valLoss: 0.4467190206050873 - trainLoss: 0.45175671577453613\n",
      "cnt: 0 - valLoss: 0.44671687483787537 - trainLoss: 0.4517534673213959\n",
      "cnt: 0 - valLoss: 0.446714848279953 - trainLoss: 0.4517502188682556\n",
      "cnt: 0 - valLoss: 0.44671279191970825 - trainLoss: 0.4517469108104706\n",
      "cnt: 0 - valLoss: 0.4467107355594635 - trainLoss: 0.45174363255500793\n",
      "cnt: 0 - valLoss: 0.4467088580131531 - trainLoss: 0.4517403542995453\n",
      "cnt: 0 - valLoss: 0.4467065930366516 - trainLoss: 0.45173710584640503\n",
      "cnt: 0 - valLoss: 0.4467046558856964 - trainLoss: 0.4517338275909424\n",
      "cnt: 0 - valLoss: 0.4467024505138397 - trainLoss: 0.45173051953315735\n",
      "cnt: 0 - valLoss: 0.4467005431652069 - trainLoss: 0.4517273008823395\n",
      "cnt: 0 - valLoss: 0.44669851660728455 - trainLoss: 0.45172402262687683\n",
      "cnt: 0 - valLoss: 0.4466964602470398 - trainLoss: 0.4517207741737366\n",
      "cnt: 0 - valLoss: 0.44669440388679504 - trainLoss: 0.45171746611595154\n",
      "cnt: 0 - valLoss: 0.4466923177242279 - trainLoss: 0.4517141580581665\n",
      "cnt: 0 - valLoss: 0.44669026136398315 - trainLoss: 0.45171090960502625\n",
      "cnt: 0 - valLoss: 0.44668835401535034 - trainLoss: 0.4517075717449188\n",
      "cnt: 0 - valLoss: 0.44668614864349365 - trainLoss: 0.45170432329177856\n",
      "cnt: 0 - valLoss: 0.4466841518878937 - trainLoss: 0.45170098543167114\n",
      "cnt: 0 - valLoss: 0.44668200612068176 - trainLoss: 0.4516977071762085\n",
      "cnt: 0 - valLoss: 0.4466800391674042 - trainLoss: 0.45169439911842346\n",
      "cnt: 0 - valLoss: 0.4466780722141266 - trainLoss: 0.4516910910606384\n",
      "cnt: 0 - valLoss: 0.4466759264469147 - trainLoss: 0.4516878128051758\n",
      "cnt: 0 - valLoss: 0.4466739594936371 - trainLoss: 0.45168450474739075\n",
      "cnt: 0 - valLoss: 0.44667181372642517 - trainLoss: 0.4516812264919281\n",
      "cnt: 0 - valLoss: 0.4466698467731476 - trainLoss: 0.45167791843414307\n",
      "cnt: 0 - valLoss: 0.44666787981987 - trainLoss: 0.45167461037635803\n",
      "cnt: 0 - valLoss: 0.4466656744480133 - trainLoss: 0.4516713321208954\n",
      "cnt: 0 - valLoss: 0.4466637074947357 - trainLoss: 0.45166799426078796\n",
      "cnt: 0 - valLoss: 0.4466616213321686 - trainLoss: 0.45166468620300293\n",
      "cnt: 0 - valLoss: 0.446659654378891 - trainLoss: 0.4516614079475403\n",
      "cnt: 0 - valLoss: 0.44665762782096863 - trainLoss: 0.45165809988975525\n",
      "cnt: 0 - valLoss: 0.4466554522514343 - trainLoss: 0.451654851436615\n",
      "cnt: 0 - valLoss: 0.44665348529815674 - trainLoss: 0.45165151357650757\n",
      "cnt: 0 - valLoss: 0.44665148854255676 - trainLoss: 0.4516482651233673\n",
      "cnt: 0 - valLoss: 0.44664937257766724 - trainLoss: 0.4516449272632599\n",
      "cnt: 0 - valLoss: 0.44664737582206726 - trainLoss: 0.45164167881011963\n",
      "cnt: 0 - valLoss: 0.44664517045021057 - trainLoss: 0.4516383707523346\n",
      "cnt: 0 - valLoss: 0.4466431736946106 - trainLoss: 0.45163506269454956\n",
      "cnt: 0 - valLoss: 0.446641206741333 - trainLoss: 0.4516317844390869\n",
      "cnt: 0 - valLoss: 0.4466390609741211 - trainLoss: 0.4516284763813019\n",
      "cnt: 0 - valLoss: 0.4466370940208435 - trainLoss: 0.45162519812583923\n",
      "cnt: 0 - valLoss: 0.4466349482536316 - trainLoss: 0.4516218900680542\n",
      "cnt: 0 - valLoss: 0.4466329514980316 - trainLoss: 0.45161858201026917\n",
      "cnt: 0 - valLoss: 0.44663098454475403 - trainLoss: 0.4516153037548065\n",
      "cnt: 0 - valLoss: 0.44662877917289734 - trainLoss: 0.45161205530166626\n",
      "cnt: 0 - valLoss: 0.44662681221961975 - trainLoss: 0.45160871744155884\n",
      "cnt: 0 - valLoss: 0.44662466645240784 - trainLoss: 0.4516054689884186\n",
      "cnt: 0 - valLoss: 0.44662269949913025 - trainLoss: 0.45160216093063354\n",
      "cnt: 0 - valLoss: 0.44662073254585266 - trainLoss: 0.4515988230705261\n",
      "cnt: 0 - valLoss: 0.44661858677864075 - trainLoss: 0.45159557461738586\n",
      "cnt: 0 - valLoss: 0.4466165602207184 - trainLoss: 0.45159226655960083\n",
      "cnt: 0 - valLoss: 0.44661441445350647 - trainLoss: 0.4515889883041382\n",
      "cnt: 0 - valLoss: 0.4466124475002289 - trainLoss: 0.45158568024635315\n",
      "cnt: 0 - valLoss: 0.4466104805469513 - trainLoss: 0.4515824317932129\n",
      "cnt: 0 - valLoss: 0.446608304977417 - trainLoss: 0.45157909393310547\n",
      "cnt: 0 - valLoss: 0.4466063678264618 - trainLoss: 0.4515758454799652\n",
      "cnt: 0 - valLoss: 0.4466042220592499 - trainLoss: 0.4515725076198578\n",
      "cnt: 0 - valLoss: 0.4466022253036499 - trainLoss: 0.45156925916671753\n",
      "cnt: 0 - valLoss: 0.4466002285480499 - trainLoss: 0.4515659511089325\n",
      "cnt: 0 - valLoss: 0.446598082780838 - trainLoss: 0.45156264305114746\n",
      "cnt: 0 - valLoss: 0.4465961158275604 - trainLoss: 0.4515593945980072\n",
      "cnt: 0 - valLoss: 0.4465939700603485 - trainLoss: 0.4515560567378998\n",
      "cnt: 0 - valLoss: 0.4465920031070709 - trainLoss: 0.4515528082847595\n",
      "cnt: 0 - valLoss: 0.4465900659561157 - trainLoss: 0.4515494704246521\n",
      "cnt: 0 - valLoss: 0.44658786058425903 - trainLoss: 0.45154622197151184\n",
      "cnt: 0 - valLoss: 0.44658592343330383 - trainLoss: 0.4515429735183716\n",
      "cnt: 0 - valLoss: 0.4465837776660919 - trainLoss: 0.45153963565826416\n",
      "cnt: 0 - valLoss: 0.44658181071281433 - trainLoss: 0.4515363872051239\n",
      "cnt: 0 - valLoss: 0.4465799331665039 - trainLoss: 0.45153307914733887\n",
      "cnt: 0 - valLoss: 0.446577787399292 - trainLoss: 0.45152977108955383\n",
      "cnt: 0 - valLoss: 0.44657576084136963 - trainLoss: 0.4515265226364136\n",
      "cnt: 0 - valLoss: 0.4465736746788025 - trainLoss: 0.4515232443809509\n",
      "cnt: 0 - valLoss: 0.4465717375278473 - trainLoss: 0.4515199363231659\n",
      "cnt: 0 - valLoss: 0.4465697109699249 - trainLoss: 0.45151668787002563\n",
      "cnt: 0 - valLoss: 0.4465676248073578 - trainLoss: 0.4515134394168854\n",
      "cnt: 0 - valLoss: 0.4465656578540802 - trainLoss: 0.45151010155677795\n",
      "cnt: 0 - valLoss: 0.4465635418891907 - trainLoss: 0.4515068531036377\n",
      "cnt: 0 - valLoss: 0.44656163454055786 - trainLoss: 0.4515035152435303\n",
      "cnt: 0 - valLoss: 0.4465596079826355 - trainLoss: 0.4515002369880676\n",
      "cnt: 0 - valLoss: 0.44655749201774597 - trainLoss: 0.45149698853492737\n",
      "cnt: 0 - valLoss: 0.4465555250644684 - trainLoss: 0.45149365067481995\n",
      "cnt: 0 - valLoss: 0.44655343890190125 - trainLoss: 0.4514903128147125\n",
      "cnt: 0 - valLoss: 0.44655147194862366 - trainLoss: 0.4514870345592499\n",
      "cnt: 0 - valLoss: 0.44654953479766846 - trainLoss: 0.45148372650146484\n",
      "cnt: 0 - valLoss: 0.44654738903045654 - trainLoss: 0.4514803886413574\n",
      "cnt: 0 - valLoss: 0.44654545187950134 - trainLoss: 0.45147705078125\n",
      "cnt: 0 - valLoss: 0.44654330611228943 - trainLoss: 0.45147380232810974\n",
      "cnt: 0 - valLoss: 0.44654133915901184 - trainLoss: 0.4514704942703247\n",
      "cnt: 0 - valLoss: 0.44653943181037903 - trainLoss: 0.4514671564102173\n",
      "cnt: 0 - valLoss: 0.4465372860431671 - trainLoss: 0.45146381855010986\n",
      "cnt: 0 - valLoss: 0.44653528928756714 - trainLoss: 0.4514605402946472\n",
      "cnt: 0 - valLoss: 0.4465332329273224 - trainLoss: 0.4514572024345398\n",
      "cnt: 0 - valLoss: 0.4465312659740448 - trainLoss: 0.45145389437675476\n",
      "cnt: 0 - valLoss: 0.446529358625412 - trainLoss: 0.4514505863189697\n",
      "cnt: 0 - valLoss: 0.44652724266052246 - trainLoss: 0.4514473080635071\n",
      "cnt: 0 - valLoss: 0.44652530550956726 - trainLoss: 0.45144400000572205\n",
      "cnt: 0 - valLoss: 0.44652339816093445 - trainLoss: 0.4514406621456146\n",
      "cnt: 0 - valLoss: 0.44652119278907776 - trainLoss: 0.4514373242855072\n",
      "cnt: 0 - valLoss: 0.44651931524276733 - trainLoss: 0.45143407583236694\n",
      "cnt: 0 - valLoss: 0.4465172290802002 - trainLoss: 0.4514307379722595\n",
      "cnt: 0 - valLoss: 0.4465152323246002 - trainLoss: 0.4514274299144745\n",
      "cnt: 0 - valLoss: 0.4465133249759674 - trainLoss: 0.45142415165901184\n",
      "cnt: 0 - valLoss: 0.4465112090110779 - trainLoss: 0.4514208137989044\n",
      "cnt: 0 - valLoss: 0.4465092420578003 - trainLoss: 0.4514175057411194\n",
      "cnt: 0 - valLoss: 0.44650718569755554 - trainLoss: 0.45141422748565674\n",
      "cnt: 0 - valLoss: 0.44650524854660034 - trainLoss: 0.4514108896255493\n",
      "cnt: 0 - valLoss: 0.44650334119796753 - trainLoss: 0.4514075815677643\n",
      "cnt: 0 - valLoss: 0.446501225233078 - trainLoss: 0.45140427350997925\n",
      "cnt: 0 - valLoss: 0.4464992582798004 - trainLoss: 0.4514009952545166\n",
      "cnt: 0 - valLoss: 0.44649720191955566 - trainLoss: 0.45139768719673157\n",
      "cnt: 0 - valLoss: 0.44649526476860046 - trainLoss: 0.4513944089412689\n",
      "cnt: 0 - valLoss: 0.44649332761764526 - trainLoss: 0.4513911008834839\n",
      "cnt: 0 - valLoss: 0.4464912414550781 - trainLoss: 0.45138776302337646\n",
      "cnt: 0 - valLoss: 0.4464893043041229 - trainLoss: 0.4513844847679138\n",
      "cnt: 0 - valLoss: 0.4464872479438782 - trainLoss: 0.4513811767101288\n",
      "cnt: 0 - valLoss: 0.4464852809906006 - trainLoss: 0.45137786865234375\n",
      "cnt: 0 - valLoss: 0.4464833438396454 - trainLoss: 0.4513745903968811\n",
      "cnt: 0 - valLoss: 0.4464813470840454 - trainLoss: 0.4513712525367737\n",
      "cnt: 0 - valLoss: 0.4464793801307678 - trainLoss: 0.45136794447898865\n",
      "cnt: 0 - valLoss: 0.4464774429798126 - trainLoss: 0.451364666223526\n",
      "cnt: 0 - valLoss: 0.4464753270149231 - trainLoss: 0.45136135816574097\n",
      "cnt: 0 - valLoss: 0.44647344946861267 - trainLoss: 0.45135805010795593\n",
      "cnt: 0 - valLoss: 0.44647130370140076 - trainLoss: 0.4513547718524933\n",
      "cnt: 0 - valLoss: 0.44646942615509033 - trainLoss: 0.45135146379470825\n",
      "cnt: 0 - valLoss: 0.4464675486087799 - trainLoss: 0.4513481855392456\n",
      "cnt: 0 - valLoss: 0.446465402841568 - trainLoss: 0.45134487748146057\n",
      "cnt: 0 - valLoss: 0.4464634954929352 - trainLoss: 0.45134156942367554\n",
      "cnt: 0 - valLoss: 0.44646143913269043 - trainLoss: 0.4513382315635681\n",
      "cnt: 0 - valLoss: 0.44645947217941284 - trainLoss: 0.45133498311042786\n",
      "cnt: 0 - valLoss: 0.4464575946331024 - trainLoss: 0.4513317048549652\n",
      "cnt: 0 - valLoss: 0.4464554488658905 - trainLoss: 0.4513283967971802\n",
      "cnt: 0 - valLoss: 0.4464535713195801 - trainLoss: 0.45132511854171753\n",
      "cnt: 0 - valLoss: 0.4464515149593353 - trainLoss: 0.4513217806816101\n",
      "cnt: 0 - valLoss: 0.44644954800605774 - trainLoss: 0.45131850242614746\n",
      "cnt: 0 - valLoss: 0.4464477002620697 - trainLoss: 0.45131516456604004\n",
      "cnt: 0 - valLoss: 0.4464455544948578 - trainLoss: 0.4513118863105774\n",
      "cnt: 0 - valLoss: 0.4464436173439026 - trainLoss: 0.45130857825279236\n",
      "cnt: 0 - valLoss: 0.44644173979759216 - trainLoss: 0.45130524039268494\n",
      "cnt: 0 - valLoss: 0.44643959403038025 - trainLoss: 0.4513019919395447\n",
      "cnt: 0 - valLoss: 0.44643762707710266 - trainLoss: 0.45129865407943726\n",
      "cnt: 0 - valLoss: 0.4464355707168579 - trainLoss: 0.451295405626297\n",
      "cnt: 0 - valLoss: 0.4464336335659027 - trainLoss: 0.45129209756851196\n",
      "cnt: 0 - valLoss: 0.4464316666126251 - trainLoss: 0.45128875970840454\n",
      "cnt: 0 - valLoss: 0.44642961025238037 - trainLoss: 0.4512855112552643\n",
      "cnt: 0 - valLoss: 0.446427583694458 - trainLoss: 0.45128220319747925\n",
      "cnt: 0 - valLoss: 0.44642552733421326 - trainLoss: 0.4512788653373718\n",
      "cnt: 0 - valLoss: 0.44642356038093567 - trainLoss: 0.45127561688423157\n",
      "cnt: 0 - valLoss: 0.44642168283462524 - trainLoss: 0.45127227902412415\n",
      "cnt: 0 - valLoss: 0.4464195668697357 - trainLoss: 0.4512690305709839\n",
      "cnt: 0 - valLoss: 0.44641759991645813 - trainLoss: 0.45126569271087646\n",
      "cnt: 0 - valLoss: 0.44641560316085815 - trainLoss: 0.4512624442577362\n",
      "cnt: 0 - valLoss: 0.4464135766029358 - trainLoss: 0.45125913619995117\n",
      "cnt: 0 - valLoss: 0.4464116394519806 - trainLoss: 0.4512558579444885\n",
      "cnt: 0 - valLoss: 0.44640955328941345 - trainLoss: 0.45125260949134827\n",
      "cnt: 0 - valLoss: 0.4464077055454254 - trainLoss: 0.45124930143356323\n",
      "cnt: 0 - valLoss: 0.4464055597782135 - trainLoss: 0.4512459933757782\n",
      "cnt: 0 - valLoss: 0.4464035928249359 - trainLoss: 0.45124274492263794\n",
      "cnt: 0 - valLoss: 0.4464017152786255 - trainLoss: 0.4512394070625305\n",
      "cnt: 0 - valLoss: 0.4463995695114136 - trainLoss: 0.45123615860939026\n",
      "cnt: 0 - valLoss: 0.4463976323604584 - trainLoss: 0.45123282074928284\n",
      "cnt: 0 - valLoss: 0.44639578461647034 - trainLoss: 0.4512295424938202\n",
      "cnt: 0 - valLoss: 0.4463936388492584 - trainLoss: 0.45122626423835754\n",
      "cnt: 0 - valLoss: 0.446391761302948 - trainLoss: 0.4512229859828949\n",
      "cnt: 0 - valLoss: 0.4463896155357361 - trainLoss: 0.45121967792510986\n",
      "cnt: 0 - valLoss: 0.4463876783847809 - trainLoss: 0.4512163996696472\n",
      "cnt: 0 - valLoss: 0.44638580083847046 - trainLoss: 0.4512130618095398\n",
      "cnt: 0 - valLoss: 0.44638365507125854 - trainLoss: 0.45120978355407715\n",
      "cnt: 0 - valLoss: 0.44638171792030334 - trainLoss: 0.4512065351009369\n",
      "cnt: 0 - valLoss: 0.4463796317577362 - trainLoss: 0.45120319724082947\n",
      "cnt: 0 - valLoss: 0.446377694606781 - trainLoss: 0.4511999487876892\n",
      "cnt: 0 - valLoss: 0.4463757574558258 - trainLoss: 0.45119667053222656\n",
      "cnt: 0 - valLoss: 0.44637370109558105 - trainLoss: 0.45119336247444153\n",
      "cnt: 0 - valLoss: 0.44637179374694824 - trainLoss: 0.45119011402130127\n",
      "cnt: 0 - valLoss: 0.4463696777820587 - trainLoss: 0.45118677616119385\n",
      "cnt: 0 - valLoss: 0.4463677704334259 - trainLoss: 0.4511835277080536\n",
      "cnt: 0 - valLoss: 0.4463658630847931 - trainLoss: 0.45118024945259094\n",
      "cnt: 0 - valLoss: 0.44636377692222595 - trainLoss: 0.4511769115924835\n",
      "cnt: 0 - valLoss: 0.4463619291782379 - trainLoss: 0.45117366313934326\n",
      "cnt: 0 - valLoss: 0.4463598132133484 - trainLoss: 0.45117032527923584\n",
      "cnt: 0 - valLoss: 0.4463579058647156 - trainLoss: 0.4511670768260956\n",
      "cnt: 0 - valLoss: 0.446355938911438 - trainLoss: 0.45116379857063293\n",
      "cnt: 0 - valLoss: 0.44635388255119324 - trainLoss: 0.4511605203151703\n",
      "cnt: 0 - valLoss: 0.44635194540023804 - trainLoss: 0.45115724205970764\n",
      "cnt: 0 - valLoss: 0.44635000824928284 - trainLoss: 0.4511539041996002\n",
      "cnt: 0 - valLoss: 0.4463479518890381 - trainLoss: 0.45115065574645996\n",
      "cnt: 0 - valLoss: 0.44634607434272766 - trainLoss: 0.4511474072933197\n",
      "cnt: 0 - valLoss: 0.4463440179824829 - trainLoss: 0.45114409923553467\n",
      "cnt: 0 - valLoss: 0.4463420510292053 - trainLoss: 0.45114079117774963\n",
      "cnt: 0 - valLoss: 0.4463401436805725 - trainLoss: 0.4511375427246094\n",
      "cnt: 0 - valLoss: 0.44633805751800537 - trainLoss: 0.45113426446914673\n",
      "cnt: 0 - valLoss: 0.44633620977401733 - trainLoss: 0.4511309564113617\n",
      "cnt: 0 - valLoss: 0.4463341236114502 - trainLoss: 0.45112770795822144\n",
      "cnt: 0 - valLoss: 0.446332186460495 - trainLoss: 0.4511243999004364\n",
      "cnt: 0 - valLoss: 0.4463302493095398 - trainLoss: 0.45112112164497375\n",
      "cnt: 0 - valLoss: 0.4463282525539398 - trainLoss: 0.4511178731918335\n",
      "cnt: 0 - valLoss: 0.446326345205307 - trainLoss: 0.45111456513404846\n",
      "cnt: 0 - valLoss: 0.44632428884506226 - trainLoss: 0.4511112570762634\n",
      "cnt: 0 - valLoss: 0.44632232189178467 - trainLoss: 0.45110800862312317\n",
      "cnt: 0 - valLoss: 0.44632044434547424 - trainLoss: 0.4511047303676605\n",
      "cnt: 0 - valLoss: 0.4463183879852295 - trainLoss: 0.4511014223098755\n",
      "cnt: 0 - valLoss: 0.4463164806365967 - trainLoss: 0.45109817385673523\n",
      "cnt: 0 - valLoss: 0.44631442427635193 - trainLoss: 0.4510948359966278\n",
      "cnt: 0 - valLoss: 0.4463125467300415 - trainLoss: 0.45109161734580994\n",
      "cnt: 0 - valLoss: 0.4463106095790863 - trainLoss: 0.4510883390903473\n",
      "cnt: 0 - valLoss: 0.44630852341651917 - trainLoss: 0.45108506083488464\n",
      "cnt: 0 - valLoss: 0.44630658626556396 - trainLoss: 0.4510817229747772\n",
      "cnt: 0 - valLoss: 0.4463047981262207 - trainLoss: 0.45107847452163696\n",
      "cnt: 0 - valLoss: 0.4463026821613312 - trainLoss: 0.4510752260684967\n",
      "cnt: 0 - valLoss: 0.44630080461502075 - trainLoss: 0.45107197761535645\n",
      "cnt: 0 - valLoss: 0.446298748254776 - trainLoss: 0.451068639755249\n",
      "cnt: 0 - valLoss: 0.4462968111038208 - trainLoss: 0.45106539130210876\n",
      "cnt: 0 - valLoss: 0.446294903755188 - trainLoss: 0.4510621428489685\n",
      "cnt: 0 - valLoss: 0.4462928771972656 - trainLoss: 0.45105883479118347\n",
      "cnt: 0 - valLoss: 0.4462909996509552 - trainLoss: 0.45105552673339844\n",
      "cnt: 0 - valLoss: 0.4462888836860657 - trainLoss: 0.4510522782802582\n",
      "cnt: 0 - valLoss: 0.44628700613975525 - trainLoss: 0.45104900002479553\n",
      "cnt: 0 - valLoss: 0.4462851285934448 - trainLoss: 0.4510456919670105\n",
      "cnt: 0 - valLoss: 0.4462830722332001 - trainLoss: 0.45104244351387024\n",
      "cnt: 0 - valLoss: 0.4462810754776001 - trainLoss: 0.45103919506073\n",
      "cnt: 0 - valLoss: 0.4462791085243225 - trainLoss: 0.45103585720062256\n",
      "cnt: 0 - valLoss: 0.4462772309780121 - trainLoss: 0.4510326087474823\n",
      "cnt: 0 - valLoss: 0.4462752938270569 - trainLoss: 0.45102936029434204\n",
      "cnt: 0 - valLoss: 0.44627323746681213 - trainLoss: 0.4510260820388794\n",
      "cnt: 0 - valLoss: 0.4462713897228241 - trainLoss: 0.45102280378341675\n",
      "cnt: 0 - valLoss: 0.44626930356025696 - trainLoss: 0.4510195553302765\n",
      "cnt: 0 - valLoss: 0.44626742601394653 - trainLoss: 0.45101624727249146\n",
      "cnt: 0 - valLoss: 0.4462655782699585 - trainLoss: 0.4510129988193512\n",
      "cnt: 0 - valLoss: 0.44626352190971375 - trainLoss: 0.45100969076156616\n",
      "cnt: 0 - valLoss: 0.4462616443634033 - trainLoss: 0.4510065019130707\n",
      "cnt: 0 - valLoss: 0.4462597370147705 - trainLoss: 0.45100316405296326\n",
      "cnt: 0 - valLoss: 0.44625765085220337 - trainLoss: 0.4509998857975006\n",
      "cnt: 0 - valLoss: 0.44625577330589294 - trainLoss: 0.45099660754203796\n",
      "cnt: 0 - valLoss: 0.4462537467479706 - trainLoss: 0.4509933888912201\n",
      "cnt: 0 - valLoss: 0.44625189900398254 - trainLoss: 0.45099011063575745\n",
      "cnt: 0 - valLoss: 0.4462500214576721 - trainLoss: 0.4509868323802948\n",
      "cnt: 0 - valLoss: 0.44624796509742737 - trainLoss: 0.45098355412483215\n",
      "cnt: 0 - valLoss: 0.44624602794647217 - trainLoss: 0.4509803056716919\n",
      "cnt: 0 - valLoss: 0.4462440609931946 - trainLoss: 0.45097702741622925\n",
      "cnt: 0 - valLoss: 0.44624218344688416 - trainLoss: 0.450973778963089\n",
      "cnt: 0 - valLoss: 0.4462403357028961 - trainLoss: 0.45097050070762634\n",
      "cnt: 0 - valLoss: 0.4462382197380066 - trainLoss: 0.4509672522544861\n",
      "cnt: 0 - valLoss: 0.44623634219169617 - trainLoss: 0.45096394419670105\n",
      "cnt: 0 - valLoss: 0.4462343156337738 - trainLoss: 0.4509606957435608\n",
      "cnt: 0 - valLoss: 0.4462324380874634 - trainLoss: 0.45095744729042053\n",
      "cnt: 0 - valLoss: 0.44623056054115295 - trainLoss: 0.4509541988372803\n",
      "cnt: 0 - valLoss: 0.4462285339832306 - trainLoss: 0.45095086097717285\n",
      "cnt: 0 - valLoss: 0.44622665643692017 - trainLoss: 0.4509475827217102\n",
      "cnt: 0 - valLoss: 0.4462246000766754 - trainLoss: 0.45094433426856995\n",
      "cnt: 0 - valLoss: 0.4462227523326874 - trainLoss: 0.4509410858154297\n",
      "cnt: 0 - valLoss: 0.4462208151817322 - trainLoss: 0.45093783736228943\n",
      "cnt: 0 - valLoss: 0.4462188184261322 - trainLoss: 0.45093458890914917\n",
      "cnt: 0 - valLoss: 0.4462169408798218 - trainLoss: 0.45093128085136414\n",
      "cnt: 0 - valLoss: 0.44621509313583374 - trainLoss: 0.4509280025959015\n",
      "cnt: 0 - valLoss: 0.446213036775589 - trainLoss: 0.4509247839450836\n",
      "cnt: 0 - valLoss: 0.44621118903160095 - trainLoss: 0.4509214758872986\n",
      "cnt: 0 - valLoss: 0.4462091326713562 - trainLoss: 0.4509182274341583\n",
      "cnt: 0 - valLoss: 0.44620728492736816 - trainLoss: 0.4509149491786957\n",
      "cnt: 0 - valLoss: 0.44620540738105774 - trainLoss: 0.4509117305278778\n",
      "cnt: 0 - valLoss: 0.446203351020813 - trainLoss: 0.4509084224700928\n",
      "cnt: 0 - valLoss: 0.4462014436721802 - trainLoss: 0.4509051442146301\n",
      "cnt: 0 - valLoss: 0.4461994469165802 - trainLoss: 0.45090192556381226\n",
      "cnt: 0 - valLoss: 0.4461975395679474 - trainLoss: 0.4508986175060272\n",
      "cnt: 0 - valLoss: 0.44619572162628174 - trainLoss: 0.45089536905288696\n",
      "cnt: 0 - valLoss: 0.446193665266037 - trainLoss: 0.4508921205997467\n",
      "cnt: 0 - valLoss: 0.44619181752204895 - trainLoss: 0.45088887214660645\n",
      "cnt: 0 - valLoss: 0.4461897909641266 - trainLoss: 0.4508855640888214\n",
      "cnt: 0 - valLoss: 0.44618794322013855 - trainLoss: 0.4508823752403259\n",
      "cnt: 0 - valLoss: 0.4461860954761505 - trainLoss: 0.4508790671825409\n",
      "cnt: 0 - valLoss: 0.44618403911590576 - trainLoss: 0.45087581872940063\n",
      "cnt: 0 - valLoss: 0.4461822211742401 - trainLoss: 0.4508725702762604\n",
      "cnt: 0 - valLoss: 0.44618019461631775 - trainLoss: 0.45086926221847534\n",
      "cnt: 0 - valLoss: 0.4461783170700073 - trainLoss: 0.45086604356765747\n",
      "cnt: 0 - valLoss: 0.4461764991283417 - trainLoss: 0.4508627653121948\n",
      "cnt: 0 - valLoss: 0.4461744427680969 - trainLoss: 0.45085954666137695\n",
      "cnt: 0 - valLoss: 0.44617265462875366 - trainLoss: 0.4508562684059143\n",
      "cnt: 0 - valLoss: 0.44617077708244324 - trainLoss: 0.45085299015045166\n",
      "cnt: 0 - valLoss: 0.4461687505245209 - trainLoss: 0.4508497416973114\n",
      "cnt: 0 - valLoss: 0.44616690278053284 - trainLoss: 0.45084649324417114\n",
      "cnt: 0 - valLoss: 0.44616490602493286 - trainLoss: 0.4508432447910309\n",
      "cnt: 0 - valLoss: 0.4461630582809448 - trainLoss: 0.4508399963378906\n",
      "cnt: 0 - valLoss: 0.4461612105369568 - trainLoss: 0.4508366882801056\n",
      "cnt: 0 - valLoss: 0.4461591839790344 - trainLoss: 0.4508334994316101\n",
      "cnt: 0 - valLoss: 0.4461573362350464 - trainLoss: 0.45083022117614746\n",
      "cnt: 0 - valLoss: 0.4461553394794464 - trainLoss: 0.4508269727230072\n",
      "cnt: 0 - valLoss: 0.446153461933136 - trainLoss: 0.4508236348628998\n",
      "cnt: 0 - valLoss: 0.44615164399147034 - trainLoss: 0.4508204162120819\n",
      "cnt: 0 - valLoss: 0.44614967703819275 - trainLoss: 0.45081713795661926\n",
      "cnt: 0 - valLoss: 0.44614773988723755 - trainLoss: 0.450813889503479\n",
      "cnt: 0 - valLoss: 0.44614577293395996 - trainLoss: 0.45081064105033875\n",
      "cnt: 0 - valLoss: 0.4461439251899719 - trainLoss: 0.4508074223995209\n",
      "cnt: 0 - valLoss: 0.4461420774459839 - trainLoss: 0.45080411434173584\n",
      "cnt: 0 - valLoss: 0.4461400806903839 - trainLoss: 0.4508008658885956\n",
      "cnt: 0 - valLoss: 0.4461382031440735 - trainLoss: 0.4507976174354553\n",
      "cnt: 0 - valLoss: 0.4461362063884735 - trainLoss: 0.45079436898231506\n",
      "cnt: 0 - valLoss: 0.44613438844680786 - trainLoss: 0.4507911205291748\n",
      "cnt: 0 - valLoss: 0.4461325407028198 - trainLoss: 0.45078787207603455\n",
      "cnt: 0 - valLoss: 0.44613051414489746 - trainLoss: 0.4507845938205719\n",
      "cnt: 0 - valLoss: 0.4461286664009094 - trainLoss: 0.45078134536743164\n",
      "cnt: 0 - valLoss: 0.44612669944763184 - trainLoss: 0.4507780969142914\n",
      "cnt: 0 - valLoss: 0.4461248219013214 - trainLoss: 0.4507748484611511\n",
      "cnt: 0 - valLoss: 0.4461229741573334 - trainLoss: 0.45077160000801086\n",
      "cnt: 0 - valLoss: 0.4461210072040558 - trainLoss: 0.45076829195022583\n",
      "cnt: 0 - valLoss: 0.44611915946006775 - trainLoss: 0.45076507329940796\n",
      "cnt: 0 - valLoss: 0.4461173117160797 - trainLoss: 0.4507618248462677\n",
      "cnt: 0 - valLoss: 0.44611531496047974 - trainLoss: 0.45075857639312744\n",
      "cnt: 0 - valLoss: 0.4461134970188141 - trainLoss: 0.4507553279399872\n",
      "cnt: 0 - valLoss: 0.4461115300655365 - trainLoss: 0.4507520794868469\n",
      "cnt: 0 - valLoss: 0.4461096525192261 - trainLoss: 0.45074883103370667\n",
      "cnt: 0 - valLoss: 0.4461078345775604 - trainLoss: 0.4507455825805664\n",
      "cnt: 0 - valLoss: 0.44610586762428284 - trainLoss: 0.45074236392974854\n",
      "cnt: 0 - valLoss: 0.4461040794849396 - trainLoss: 0.4507390558719635\n",
      "cnt: 0 - valLoss: 0.4461020529270172 - trainLoss: 0.45073580741882324\n",
      "cnt: 0 - valLoss: 0.4461002051830292 - trainLoss: 0.450732558965683\n",
      "cnt: 0 - valLoss: 0.4460984170436859 - trainLoss: 0.4507293403148651\n",
      "cnt: 0 - valLoss: 0.4460964500904083 - trainLoss: 0.45072606205940247\n",
      "cnt: 0 - valLoss: 0.4460946023464203 - trainLoss: 0.4507228136062622\n",
      "cnt: 0 - valLoss: 0.4460925757884979 - trainLoss: 0.4507196247577667\n",
      "cnt: 0 - valLoss: 0.4460907280445099 - trainLoss: 0.45071637630462646\n",
      "cnt: 0 - valLoss: 0.4460889399051666 - trainLoss: 0.4507130980491638\n",
      "cnt: 0 - valLoss: 0.44608697295188904 - trainLoss: 0.45070987939834595\n",
      "cnt: 0 - valLoss: 0.446085125207901 - trainLoss: 0.4507066309452057\n",
      "cnt: 0 - valLoss: 0.4460831582546234 - trainLoss: 0.45070338249206543\n",
      "cnt: 0 - valLoss: 0.44608134031295776 - trainLoss: 0.4507001042366028\n",
      "cnt: 0 - valLoss: 0.4460795223712921 - trainLoss: 0.4506968557834625\n",
      "cnt: 0 - valLoss: 0.44607752561569214 - trainLoss: 0.45069360733032227\n",
      "cnt: 0 - valLoss: 0.4460757076740265 - trainLoss: 0.450690358877182\n",
      "cnt: 0 - valLoss: 0.44607388973236084 - trainLoss: 0.45068711042404175\n",
      "cnt: 0 - valLoss: 0.44607192277908325 - trainLoss: 0.4506838619709015\n",
      "cnt: 0 - valLoss: 0.4460700750350952 - trainLoss: 0.4506806433200836\n",
      "cnt: 0 - valLoss: 0.4460681080818176 - trainLoss: 0.4506773352622986\n",
      "cnt: 0 - valLoss: 0.4460662305355072 - trainLoss: 0.4506741762161255\n",
      "cnt: 0 - valLoss: 0.44606438279151917 - trainLoss: 0.45067092776298523\n",
      "cnt: 0 - valLoss: 0.4460623264312744 - trainLoss: 0.45066770911216736\n",
      "cnt: 0 - valLoss: 0.446060448884964 - trainLoss: 0.4506644606590271\n",
      "cnt: 0 - valLoss: 0.446058452129364 - trainLoss: 0.45066124200820923\n",
      "cnt: 0 - valLoss: 0.4460565745830536 - trainLoss: 0.45065799355506897\n",
      "cnt: 0 - valLoss: 0.44605469703674316 - trainLoss: 0.4506547451019287\n",
      "cnt: 0 - valLoss: 0.4460526406764984 - trainLoss: 0.4506515860557556\n",
      "cnt: 0 - valLoss: 0.4460507929325104 - trainLoss: 0.45064833760261536\n",
      "cnt: 0 - valLoss: 0.44604891538619995 - trainLoss: 0.4506451189517975\n",
      "cnt: 0 - valLoss: 0.4460468888282776 - trainLoss: 0.4506419003009796\n",
      "cnt: 0 - valLoss: 0.44604501128196716 - trainLoss: 0.45063865184783936\n",
      "cnt: 0 - valLoss: 0.4460430443286896 - trainLoss: 0.4506354033946991\n",
      "cnt: 0 - valLoss: 0.44604113698005676 - trainLoss: 0.450632244348526\n",
      "cnt: 0 - valLoss: 0.4460393190383911 - trainLoss: 0.45062896609306335\n",
      "cnt: 0 - valLoss: 0.44603726267814636 - trainLoss: 0.45062577724456787\n",
      "cnt: 0 - valLoss: 0.4460354149341583 - trainLoss: 0.45062255859375\n",
      "cnt: 0 - valLoss: 0.4460335969924927 - trainLoss: 0.45061933994293213\n",
      "cnt: 0 - valLoss: 0.4460315406322479 - trainLoss: 0.45061615109443665\n",
      "cnt: 0 - valLoss: 0.4460296630859375 - trainLoss: 0.450612872838974\n",
      "cnt: 0 - valLoss: 0.44602763652801514 - trainLoss: 0.4506096839904785\n",
      "cnt: 0 - valLoss: 0.4460257589817047 - trainLoss: 0.45060646533966064\n",
      "cnt: 0 - valLoss: 0.4460239112377167 - trainLoss: 0.4506032168865204\n",
      "cnt: 0 - valLoss: 0.4460218548774719 - trainLoss: 0.4506000578403473\n",
      "cnt: 0 - valLoss: 0.4460199773311615 - trainLoss: 0.45059677958488464\n",
      "cnt: 0 - valLoss: 0.44601812958717346 - trainLoss: 0.4505935311317444\n",
      "cnt: 0 - valLoss: 0.4460160732269287 - trainLoss: 0.4505903720855713\n",
      "cnt: 0 - valLoss: 0.4460141956806183 - trainLoss: 0.4505871534347534\n",
      "cnt: 0 - valLoss: 0.4460121691226959 - trainLoss: 0.45058396458625793\n",
      "cnt: 0 - valLoss: 0.4460102915763855 - trainLoss: 0.4505806863307953\n",
      "cnt: 0 - valLoss: 0.4460084140300751 - trainLoss: 0.4505775272846222\n",
      "cnt: 0 - valLoss: 0.4460063874721527 - trainLoss: 0.4505743086338043\n",
      "cnt: 0 - valLoss: 0.4460045397281647 - trainLoss: 0.45057111978530884\n",
      "cnt: 0 - valLoss: 0.4460024833679199 - trainLoss: 0.4505678713321686\n",
      "cnt: 0 - valLoss: 0.4460006356239319 - trainLoss: 0.4505646526813507\n",
      "cnt: 0 - valLoss: 0.4459987282752991 - trainLoss: 0.45056143403053284\n",
      "cnt: 0 - valLoss: 0.4459967315196991 - trainLoss: 0.45055827498435974\n",
      "cnt: 0 - valLoss: 0.44599485397338867 - trainLoss: 0.4505550265312195\n",
      "cnt: 0 - valLoss: 0.44599297642707825 - trainLoss: 0.4505517780780792\n",
      "cnt: 0 - valLoss: 0.4459909498691559 - trainLoss: 0.45054858922958374\n",
      "cnt: 0 - valLoss: 0.44598910212516785 - trainLoss: 0.4505453407764435\n",
      "cnt: 0 - valLoss: 0.4459870457649231 - trainLoss: 0.4505421221256256\n",
      "cnt: 0 - valLoss: 0.44598516821861267 - trainLoss: 0.4505389332771301\n",
      "cnt: 0 - valLoss: 0.44598329067230225 - trainLoss: 0.45053574442863464\n",
      "cnt: 0 - valLoss: 0.44598132371902466 - trainLoss: 0.4505324959754944\n",
      "cnt: 0 - valLoss: 0.44597941637039185 - trainLoss: 0.4505293071269989\n",
      "cnt: 0 - valLoss: 0.4459775984287262 - trainLoss: 0.45052608847618103\n",
      "cnt: 0 - valLoss: 0.44597557187080383 - trainLoss: 0.45052286982536316\n",
      "cnt: 0 - valLoss: 0.4459736943244934 - trainLoss: 0.4505196511745453\n",
      "cnt: 0 - valLoss: 0.4459717273712158 - trainLoss: 0.4505164623260498\n",
      "cnt: 0 - valLoss: 0.445969820022583 - trainLoss: 0.45051324367523193\n",
      "cnt: 0 - valLoss: 0.44596803188323975 - trainLoss: 0.4505099952220917\n",
      "cnt: 0 - valLoss: 0.445965975522995 - trainLoss: 0.4505067765712738\n",
      "cnt: 0 - valLoss: 0.44596409797668457 - trainLoss: 0.45050355792045593\n",
      "cnt: 0 - valLoss: 0.44596225023269653 - trainLoss: 0.45050039887428284\n",
      "cnt: 0 - valLoss: 0.44596022367477417 - trainLoss: 0.4504971504211426\n",
      "cnt: 0 - valLoss: 0.44595837593078613 - trainLoss: 0.4504939913749695\n",
      "cnt: 0 - valLoss: 0.44595637917518616 - trainLoss: 0.4504907429218292\n",
      "cnt: 0 - valLoss: 0.4459545314311981 - trainLoss: 0.45048755407333374\n",
      "cnt: 0 - valLoss: 0.4459526836872101 - trainLoss: 0.45048436522483826\n",
      "cnt: 0 - valLoss: 0.4459507167339325 - trainLoss: 0.4504811465740204\n",
      "cnt: 0 - valLoss: 0.44594883918762207 - trainLoss: 0.4504779279232025\n",
      "cnt: 0 - valLoss: 0.4459470212459564 - trainLoss: 0.45047470927238464\n",
      "cnt: 0 - valLoss: 0.44594496488571167 - trainLoss: 0.45047155022621155\n",
      "cnt: 0 - valLoss: 0.44594311714172363 - trainLoss: 0.4504683017730713\n",
      "cnt: 0 - valLoss: 0.44594115018844604 - trainLoss: 0.4504651427268982\n",
      "cnt: 0 - valLoss: 0.445939302444458 - trainLoss: 0.45046189427375793\n",
      "cnt: 0 - valLoss: 0.44593745470046997 - trainLoss: 0.45045867562294006\n",
      "cnt: 0 - valLoss: 0.44593545794487 - trainLoss: 0.45045551657676697\n",
      "cnt: 0 - valLoss: 0.44593358039855957 - trainLoss: 0.4504522979259491\n",
      "cnt: 0 - valLoss: 0.4459315836429596 - trainLoss: 0.45044904947280884\n",
      "cnt: 0 - valLoss: 0.44592970609664917 - trainLoss: 0.45044586062431335\n",
      "cnt: 0 - valLoss: 0.44592782855033875 - trainLoss: 0.45044267177581787\n",
      "cnt: 0 - valLoss: 0.445925772190094 - trainLoss: 0.450439453125\n",
      "cnt: 0 - valLoss: 0.44592389464378357 - trainLoss: 0.4504362940788269\n",
      "cnt: 0 - valLoss: 0.44592198729515076 - trainLoss: 0.4504331052303314\n",
      "cnt: 0 - valLoss: 0.4459199905395508 - trainLoss: 0.45042991638183594\n",
      "cnt: 0 - valLoss: 0.4459180533885956 - trainLoss: 0.45042669773101807\n",
      "cnt: 0 - valLoss: 0.4459162652492523 - trainLoss: 0.45042353868484497\n",
      "cnt: 0 - valLoss: 0.4459141790866852 - trainLoss: 0.4504203498363495\n",
      "cnt: 0 - valLoss: 0.44591230154037476 - trainLoss: 0.4504171907901764\n",
      "cnt: 0 - valLoss: 0.44591024518013 - trainLoss: 0.4504139721393585\n",
      "cnt: 0 - valLoss: 0.4459083676338196 - trainLoss: 0.45041078329086304\n",
      "cnt: 0 - valLoss: 0.44590649008750916 - trainLoss: 0.45040759444236755\n",
      "cnt: 0 - valLoss: 0.4459044933319092 - trainLoss: 0.45040443539619446\n",
      "cnt: 0 - valLoss: 0.44590261578559875 - trainLoss: 0.4504012167453766\n",
      "cnt: 0 - valLoss: 0.44590073823928833 - trainLoss: 0.4503980278968811\n",
      "cnt: 0 - valLoss: 0.4458986818790436 - trainLoss: 0.4503948390483856\n",
      "cnt: 0 - valLoss: 0.44589686393737793 - trainLoss: 0.4503916800022125\n",
      "cnt: 0 - valLoss: 0.4458949863910675 - trainLoss: 0.45038849115371704\n",
      "cnt: 0 - valLoss: 0.44589298963546753 - trainLoss: 0.45038530230522156\n",
      "cnt: 0 - valLoss: 0.44589105248451233 - trainLoss: 0.4503821134567261\n",
      "cnt: 0 - valLoss: 0.44588908553123474 - trainLoss: 0.4503789246082306\n",
      "cnt: 0 - valLoss: 0.44588717818260193 - trainLoss: 0.4503757655620575\n",
      "cnt: 0 - valLoss: 0.4458853602409363 - trainLoss: 0.4503726065158844\n",
      "cnt: 0 - valLoss: 0.4458833336830139 - trainLoss: 0.45036938786506653\n",
      "cnt: 0 - valLoss: 0.4458814859390259 - trainLoss: 0.45036622881889343\n",
      "cnt: 0 - valLoss: 0.44587963819503784 - trainLoss: 0.45036301016807556\n",
      "cnt: 0 - valLoss: 0.44587764143943787 - trainLoss: 0.45035985112190247\n",
      "cnt: 0 - valLoss: 0.44587570428848267 - trainLoss: 0.450356662273407\n",
      "cnt: 0 - valLoss: 0.4458739161491394 - trainLoss: 0.4503534734249115\n",
      "cnt: 0 - valLoss: 0.44587185978889465 - trainLoss: 0.4503503441810608\n",
      "cnt: 0 - valLoss: 0.44586995244026184 - trainLoss: 0.45034709572792053\n",
      "cnt: 0 - valLoss: 0.44586798548698425 - trainLoss: 0.45034390687942505\n",
      "cnt: 0 - valLoss: 0.4458661377429962 - trainLoss: 0.45034074783325195\n",
      "cnt: 0 - valLoss: 0.4458642899990082 - trainLoss: 0.45033758878707886\n",
      "cnt: 0 - valLoss: 0.4458622336387634 - trainLoss: 0.450334370136261\n",
      "cnt: 0 - valLoss: 0.4458604156970978 - trainLoss: 0.4503311514854431\n",
      "cnt: 0 - valLoss: 0.44585856795310974 - trainLoss: 0.4503280520439148\n",
      "cnt: 0 - valLoss: 0.4458565413951874 - trainLoss: 0.4503248333930969\n",
      "cnt: 0 - valLoss: 0.44585466384887695 - trainLoss: 0.45032167434692383\n",
      "cnt: 0 - valLoss: 0.4458528161048889 - trainLoss: 0.45031848549842834\n",
      "cnt: 0 - valLoss: 0.44585075974464417 - trainLoss: 0.45031532645225525\n",
      "cnt: 0 - valLoss: 0.4458489716053009 - trainLoss: 0.450312077999115\n",
      "cnt: 0 - valLoss: 0.44584694504737854 - trainLoss: 0.4503089189529419\n",
      "cnt: 0 - valLoss: 0.4458450973033905 - trainLoss: 0.4503057897090912\n",
      "cnt: 0 - valLoss: 0.4458432197570801 - trainLoss: 0.4503025710582733\n",
      "cnt: 0 - valLoss: 0.4458412528038025 - trainLoss: 0.4502994120121002\n",
      "cnt: 0 - valLoss: 0.4458393454551697 - trainLoss: 0.4502962529659271\n",
      "cnt: 0 - valLoss: 0.44583752751350403 - trainLoss: 0.45029306411743164\n",
      "cnt: 0 - valLoss: 0.44583550095558167 - trainLoss: 0.45028987526893616\n",
      "cnt: 0 - valLoss: 0.4458337128162384 - trainLoss: 0.4502866864204407\n",
      "cnt: 0 - valLoss: 0.4458318054676056 - trainLoss: 0.4502835273742676\n",
      "cnt: 0 - valLoss: 0.445829838514328 - trainLoss: 0.4502803683280945\n",
      "cnt: 0 - valLoss: 0.4458279609680176 - trainLoss: 0.4502771496772766\n",
      "cnt: 0 - valLoss: 0.44582599401474 - trainLoss: 0.4502739906311035\n",
      "cnt: 0 - valLoss: 0.4458240866661072 - trainLoss: 0.45027080178260803\n",
      "cnt: 0 - valLoss: 0.4458222985267639 - trainLoss: 0.45026764273643494\n",
      "cnt: 0 - valLoss: 0.44582027196884155 - trainLoss: 0.45026442408561707\n",
      "cnt: 0 - valLoss: 0.4458184540271759 - trainLoss: 0.45026132464408875\n",
      "cnt: 0 - valLoss: 0.44581660628318787 - trainLoss: 0.45025813579559326\n",
      "cnt: 0 - valLoss: 0.4458145797252655 - trainLoss: 0.45025497674942017\n",
      "cnt: 0 - valLoss: 0.44581273198127747 - trainLoss: 0.4502517580986023\n",
      "cnt: 0 - valLoss: 0.4458109736442566 - trainLoss: 0.4502485692501068\n",
      "cnt: 0 - valLoss: 0.4458089768886566 - trainLoss: 0.4502454400062561\n",
      "cnt: 0 - valLoss: 0.4458071291446686 - trainLoss: 0.45024222135543823\n",
      "cnt: 0 - valLoss: 0.4458051025867462 - trainLoss: 0.45023906230926514\n",
      "cnt: 0 - valLoss: 0.4458032548427582 - trainLoss: 0.45023590326309204\n",
      "cnt: 0 - valLoss: 0.44580143690109253 - trainLoss: 0.45023271441459656\n",
      "cnt: 0 - valLoss: 0.44579944014549255 - trainLoss: 0.45022955536842346\n",
      "cnt: 0 - valLoss: 0.44579756259918213 - trainLoss: 0.45022639632225037\n",
      "cnt: 0 - valLoss: 0.44579577445983887 - trainLoss: 0.4502232074737549\n",
      "cnt: 0 - valLoss: 0.4457937479019165 - trainLoss: 0.4502200484275818\n",
      "cnt: 0 - valLoss: 0.44579195976257324 - trainLoss: 0.4502168893814087\n",
      "cnt: 0 - valLoss: 0.44578999280929565 - trainLoss: 0.4502137303352356\n",
      "cnt: 0 - valLoss: 0.44578808546066284 - trainLoss: 0.4502105414867401\n",
      "cnt: 0 - valLoss: 0.4457862973213196 - trainLoss: 0.45020735263824463\n",
      "cnt: 0 - valLoss: 0.445784330368042 - trainLoss: 0.45020413398742676\n",
      "cnt: 0 - valLoss: 0.4457824230194092 - trainLoss: 0.45020100474357605\n",
      "cnt: 0 - valLoss: 0.4457806646823883 - trainLoss: 0.45019784569740295\n",
      "cnt: 0 - valLoss: 0.44577866792678833 - trainLoss: 0.45019465684890747\n",
      "cnt: 0 - valLoss: 0.4457768499851227 - trainLoss: 0.4501914978027344\n",
      "cnt: 0 - valLoss: 0.44577503204345703 - trainLoss: 0.4501883387565613\n",
      "cnt: 0 - valLoss: 0.44577300548553467 - trainLoss: 0.4501851797103882\n",
      "cnt: 0 - valLoss: 0.4457712173461914 - trainLoss: 0.4501819610595703\n",
      "cnt: 0 - valLoss: 0.44576922059059143 - trainLoss: 0.4501788020133972\n",
      "cnt: 0 - valLoss: 0.4457674026489258 - trainLoss: 0.45017561316490173\n",
      "cnt: 0 - valLoss: 0.44576558470726013 - trainLoss: 0.45017245411872864\n",
      "cnt: 0 - valLoss: 0.4457636773586273 - trainLoss: 0.45016929507255554\n",
      "cnt: 0 - valLoss: 0.44576185941696167 - trainLoss: 0.45016616582870483\n",
      "cnt: 0 - valLoss: 0.4457600712776184 - trainLoss: 0.45016294717788696\n",
      "cnt: 0 - valLoss: 0.4457581043243408 - trainLoss: 0.45015978813171387\n",
      "cnt: 0 - valLoss: 0.44575634598731995 - trainLoss: 0.45015662908554077\n",
      "cnt: 0 - valLoss: 0.4457545280456543 - trainLoss: 0.45015349984169006\n",
      "cnt: 0 - valLoss: 0.4457525908946991 - trainLoss: 0.4501502811908722\n",
      "cnt: 0 - valLoss: 0.44575080275535583 - trainLoss: 0.4501471221446991\n",
      "cnt: 0 - valLoss: 0.44574886560440063 - trainLoss: 0.4501439929008484\n",
      "cnt: 0 - valLoss: 0.44574710726737976 - trainLoss: 0.4501407742500305\n",
      "cnt: 0 - valLoss: 0.4457453191280365 - trainLoss: 0.4501376152038574\n",
      "cnt: 0 - valLoss: 0.4457433521747589 - trainLoss: 0.4501344859600067\n",
      "cnt: 0 - valLoss: 0.44574153423309326 - trainLoss: 0.45013126730918884\n",
      "cnt: 0 - valLoss: 0.4457397758960724 - trainLoss: 0.45012810826301575\n",
      "cnt: 0 - valLoss: 0.4457378685474396 - trainLoss: 0.45012494921684265\n",
      "cnt: 0 - valLoss: 0.4457360506057739 - trainLoss: 0.45012181997299194\n",
      "cnt: 0 - valLoss: 0.44573429226875305 - trainLoss: 0.4501186013221741\n",
      "cnt: 0 - valLoss: 0.44573238492012024 - trainLoss: 0.45011550188064575\n",
      "cnt: 0 - valLoss: 0.4457305669784546 - trainLoss: 0.45011231303215027\n",
      "cnt: 0 - valLoss: 0.44572868943214417 - trainLoss: 0.4501091539859772\n",
      "cnt: 0 - valLoss: 0.4457269012928009 - trainLoss: 0.4501059949398041\n",
      "cnt: 0 - valLoss: 0.44572508335113525 - trainLoss: 0.4501028060913086\n",
      "cnt: 0 - valLoss: 0.44572314620018005 - trainLoss: 0.4500997066497803\n",
      "cnt: 0 - valLoss: 0.4457213878631592 - trainLoss: 0.4500964879989624\n",
      "cnt: 0 - valLoss: 0.4457195997238159 - trainLoss: 0.4500933587551117\n",
      "cnt: 0 - valLoss: 0.4457177221775055 - trainLoss: 0.4500901997089386\n",
      "cnt: 0 - valLoss: 0.44571590423583984 - trainLoss: 0.4500870108604431\n",
      "cnt: 0 - valLoss: 0.44571420550346375 - trainLoss: 0.45008385181427\n",
      "cnt: 0 - valLoss: 0.44571223855018616 - trainLoss: 0.4500806927680969\n",
      "cnt: 0 - valLoss: 0.4457104206085205 - trainLoss: 0.45007753372192383\n",
      "cnt: 0 - valLoss: 0.4457085430622101 - trainLoss: 0.45007434487342834\n",
      "cnt: 0 - valLoss: 0.4457067549228668 - trainLoss: 0.45007118582725525\n",
      "cnt: 0 - valLoss: 0.44570499658584595 - trainLoss: 0.45006802678108215\n",
      "cnt: 0 - valLoss: 0.44570305943489075 - trainLoss: 0.45006489753723145\n",
      "cnt: 0 - valLoss: 0.4457012414932251 - trainLoss: 0.45006173849105835\n",
      "cnt: 0 - valLoss: 0.445699542760849 - trainLoss: 0.45005854964256287\n",
      "cnt: 0 - valLoss: 0.4456975758075714 - trainLoss: 0.45005539059638977\n",
      "cnt: 0 - valLoss: 0.44569581747055054 - trainLoss: 0.4500522315502167\n",
      "cnt: 0 - valLoss: 0.44569388031959534 - trainLoss: 0.4500490725040436\n",
      "cnt: 0 - valLoss: 0.44569218158721924 - trainLoss: 0.45004594326019287\n",
      "cnt: 0 - valLoss: 0.4456903636455536 - trainLoss: 0.4500427544116974\n",
      "cnt: 0 - valLoss: 0.4456884264945984 - trainLoss: 0.4500395953655243\n",
      "cnt: 0 - valLoss: 0.4456866979598999 - trainLoss: 0.4500364363193512\n",
      "cnt: 0 - valLoss: 0.4456849694252014 - trainLoss: 0.4500332772731781\n",
      "cnt: 0 - valLoss: 0.4456830322742462 - trainLoss: 0.4500301480293274\n",
      "cnt: 0 - valLoss: 0.44568127393722534 - trainLoss: 0.4500270187854767\n",
      "cnt: 0 - valLoss: 0.44567951560020447 - trainLoss: 0.4500238001346588\n",
      "cnt: 0 - valLoss: 0.44567763805389404 - trainLoss: 0.4500206410884857\n",
      "cnt: 0 - valLoss: 0.44567593932151794 - trainLoss: 0.450017511844635\n",
      "cnt: 0 - valLoss: 0.44567403197288513 - trainLoss: 0.4500143527984619\n",
      "cnt: 0 - valLoss: 0.44567227363586426 - trainLoss: 0.4500111937522888\n",
      "cnt: 0 - valLoss: 0.4456705152988434 - trainLoss: 0.45000800490379333\n",
      "cnt: 0 - valLoss: 0.44566863775253296 - trainLoss: 0.45000484585762024\n",
      "cnt: 0 - valLoss: 0.4456669092178345 - trainLoss: 0.45000168681144714\n",
      "cnt: 0 - valLoss: 0.445665180683136 - trainLoss: 0.44999855756759644\n",
      "cnt: 0 - valLoss: 0.4456632733345032 - trainLoss: 0.44999539852142334\n",
      "cnt: 0 - valLoss: 0.4456615746021271 - trainLoss: 0.44999226927757263\n",
      "cnt: 0 - valLoss: 0.4456598162651062 - trainLoss: 0.44998908042907715\n",
      "cnt: 0 - valLoss: 0.44565796852111816 - trainLoss: 0.44998592138290405\n",
      "cnt: 0 - valLoss: 0.4456561803817749 - trainLoss: 0.44998276233673096\n",
      "cnt: 0 - valLoss: 0.44565433263778687 - trainLoss: 0.44997963309288025\n",
      "cnt: 0 - valLoss: 0.4456526041030884 - trainLoss: 0.44997650384902954\n",
      "cnt: 0 - valLoss: 0.4456509053707123 - trainLoss: 0.44997334480285645\n",
      "cnt: 0 - valLoss: 0.4456489682197571 - trainLoss: 0.44997018575668335\n",
      "cnt: 0 - valLoss: 0.4456472396850586 - trainLoss: 0.44996699690818787\n",
      "cnt: 0 - valLoss: 0.4456455409526825 - trainLoss: 0.44996389746665955\n",
      "cnt: 0 - valLoss: 0.44564366340637207 - trainLoss: 0.44996070861816406\n",
      "cnt: 0 - valLoss: 0.4456419050693512 - trainLoss: 0.44995757937431335\n",
      "cnt: 0 - valLoss: 0.4456399977207184 - trainLoss: 0.44995442032814026\n",
      "cnt: 0 - valLoss: 0.4456383287906647 - trainLoss: 0.44995126128196716\n",
      "cnt: 0 - valLoss: 0.4456365704536438 - trainLoss: 0.44994810223579407\n",
      "cnt: 0 - valLoss: 0.445634663105011 - trainLoss: 0.4499449133872986\n",
      "cnt: 0 - valLoss: 0.4456329345703125 - trainLoss: 0.4499417841434479\n",
      "cnt: 0 - valLoss: 0.4456312656402588 - trainLoss: 0.4499386250972748\n",
      "cnt: 0 - valLoss: 0.44562938809394836 - trainLoss: 0.4499354958534241\n",
      "cnt: 0 - valLoss: 0.4456276297569275 - trainLoss: 0.449932336807251\n",
      "cnt: 0 - valLoss: 0.4456259608268738 - trainLoss: 0.4499291777610779\n",
      "cnt: 0 - valLoss: 0.4456240236759186 - trainLoss: 0.4499260187149048\n",
      "cnt: 0 - valLoss: 0.4456222951412201 - trainLoss: 0.4499228894710541\n",
      "cnt: 0 - valLoss: 0.44562041759490967 - trainLoss: 0.44991976022720337\n",
      "cnt: 0 - valLoss: 0.44561871886253357 - trainLoss: 0.4499166011810303\n",
      "cnt: 0 - valLoss: 0.4456169903278351 - trainLoss: 0.4499134123325348\n",
      "cnt: 0 - valLoss: 0.44561511278152466 - trainLoss: 0.4499102830886841\n",
      "cnt: 0 - valLoss: 0.44561338424682617 - trainLoss: 0.44990718364715576\n",
      "cnt: 0 - valLoss: 0.4456116855144501 - trainLoss: 0.4499039947986603\n",
      "cnt: 0 - valLoss: 0.44560983777046204 - trainLoss: 0.4499008357524872\n",
      "cnt: 0 - valLoss: 0.44560810923576355 - trainLoss: 0.4498977065086365\n",
      "cnt: 0 - valLoss: 0.44560641050338745 - trainLoss: 0.44989457726478577\n",
      "cnt: 0 - valLoss: 0.44560450315475464 - trainLoss: 0.44989141821861267\n",
      "cnt: 0 - valLoss: 0.4456028342247009 - trainLoss: 0.4498882591724396\n",
      "cnt: 0 - valLoss: 0.4456009566783905 - trainLoss: 0.4498851001262665\n",
      "cnt: 0 - valLoss: 0.4455992579460144 - trainLoss: 0.44988200068473816\n",
      "cnt: 0 - valLoss: 0.4455975294113159 - trainLoss: 0.44987884163856506\n",
      "cnt: 0 - valLoss: 0.4455956816673279 - trainLoss: 0.44987568259239197\n",
      "cnt: 0 - valLoss: 0.4455939829349518 - trainLoss: 0.44987255334854126\n",
      "cnt: 0 - valLoss: 0.4455922544002533 - trainLoss: 0.4498693645000458\n",
      "cnt: 0 - valLoss: 0.44559040665626526 - trainLoss: 0.44986626505851746\n",
      "cnt: 0 - valLoss: 0.44558870792388916 - trainLoss: 0.449863076210022\n",
      "cnt: 0 - valLoss: 0.44558680057525635 - trainLoss: 0.44985994696617126\n",
      "cnt: 0 - valLoss: 0.4455849826335907 - trainLoss: 0.44985684752464294\n",
      "cnt: 0 - valLoss: 0.4455832242965698 - trainLoss: 0.44985365867614746\n",
      "cnt: 0 - valLoss: 0.44558125734329224 - trainLoss: 0.44985049962997437\n",
      "cnt: 0 - valLoss: 0.4455794394016266 - trainLoss: 0.44984737038612366\n",
      "cnt: 0 - valLoss: 0.4455776810646057 - trainLoss: 0.44984421133995056\n",
      "cnt: 0 - valLoss: 0.4455757439136505 - trainLoss: 0.44984108209609985\n",
      "cnt: 0 - valLoss: 0.44557395577430725 - trainLoss: 0.44983792304992676\n",
      "cnt: 0 - valLoss: 0.445572167634964 - trainLoss: 0.44983479380607605\n",
      "cnt: 0 - valLoss: 0.445570170879364 - trainLoss: 0.44983166456222534\n",
      "cnt: 0 - valLoss: 0.4455684423446655 - trainLoss: 0.44982850551605225\n",
      "cnt: 0 - valLoss: 0.4455665051937103 - trainLoss: 0.44982534646987915\n",
      "cnt: 0 - valLoss: 0.4455646872520447 - trainLoss: 0.44982221722602844\n",
      "cnt: 0 - valLoss: 0.4455629289150238 - trainLoss: 0.44981908798217773\n",
      "cnt: 0 - valLoss: 0.445561021566391 - trainLoss: 0.44981592893600464\n",
      "cnt: 0 - valLoss: 0.44555920362472534 - trainLoss: 0.44981279969215393\n",
      "cnt: 0 - valLoss: 0.4455574154853821 - trainLoss: 0.4498096704483032\n",
      "cnt: 0 - valLoss: 0.4455554783344269 - trainLoss: 0.4498065114021301\n",
      "cnt: 0 - valLoss: 0.445553719997406 - trainLoss: 0.4498033821582794\n",
      "cnt: 0 - valLoss: 0.44555193185806274 - trainLoss: 0.4498002529144287\n",
      "cnt: 0 - valLoss: 0.44554999470710754 - trainLoss: 0.4497970938682556\n",
      "cnt: 0 - valLoss: 0.44554823637008667 - trainLoss: 0.4497939646244049\n",
      "cnt: 0 - valLoss: 0.44554629921913147 - trainLoss: 0.4497908055782318\n",
      "cnt: 0 - valLoss: 0.4455445110797882 - trainLoss: 0.4497876763343811\n",
      "cnt: 0 - valLoss: 0.44554275274276733 - trainLoss: 0.4497845470905304\n",
      "cnt: 0 - valLoss: 0.44554078578948975 - trainLoss: 0.4497814178466797\n",
      "cnt: 0 - valLoss: 0.44553902745246887 - trainLoss: 0.4497782588005066\n",
      "cnt: 0 - valLoss: 0.445537269115448 - trainLoss: 0.4497750997543335\n",
      "cnt: 0 - valLoss: 0.4455353319644928 - trainLoss: 0.4497720003128052\n",
      "cnt: 0 - valLoss: 0.44553354382514954 - trainLoss: 0.4497688412666321\n",
      "cnt: 0 - valLoss: 0.44553181529045105 - trainLoss: 0.449765682220459\n",
      "cnt: 0 - valLoss: 0.44552984833717346 - trainLoss: 0.44976258277893066\n",
      "cnt: 0 - valLoss: 0.4455280601978302 - trainLoss: 0.44975942373275757\n",
      "cnt: 0 - valLoss: 0.445526123046875 - trainLoss: 0.4497562646865845\n",
      "cnt: 0 - valLoss: 0.4455243945121765 - trainLoss: 0.44975316524505615\n",
      "cnt: 0 - valLoss: 0.445522665977478 - trainLoss: 0.44975000619888306\n",
      "cnt: 0 - valLoss: 0.44552066922187805 - trainLoss: 0.44974687695503235\n",
      "cnt: 0 - valLoss: 0.4455189108848572 - trainLoss: 0.44974374771118164\n",
      "cnt: 0 - valLoss: 0.4455171525478363 - trainLoss: 0.44974058866500854\n",
      "cnt: 0 - valLoss: 0.4455152451992035 - trainLoss: 0.44973745942115784\n",
      "cnt: 0 - valLoss: 0.44551342725753784 - trainLoss: 0.44973433017730713\n",
      "cnt: 0 - valLoss: 0.4455115497112274 - trainLoss: 0.44973117113113403\n",
      "cnt: 0 - valLoss: 0.44550982117652893 - trainLoss: 0.4497281014919281\n",
      "cnt: 0 - valLoss: 0.44550803303718567 - trainLoss: 0.4497249126434326\n",
      "cnt: 0 - valLoss: 0.4455060660839081 - trainLoss: 0.4497217535972595\n",
      "cnt: 0 - valLoss: 0.4455042779445648 - trainLoss: 0.4497186839580536\n",
      "cnt: 0 - valLoss: 0.44550246000289917 - trainLoss: 0.4497154951095581\n",
      "cnt: 0 - valLoss: 0.4455004930496216 - trainLoss: 0.4497124254703522\n",
      "cnt: 0 - valLoss: 0.4454987049102783 - trainLoss: 0.44970929622650146\n",
      "cnt: 0 - valLoss: 0.44549688696861267 - trainLoss: 0.44970616698265076\n",
      "cnt: 0 - valLoss: 0.4454949200153351 - trainLoss: 0.44970303773880005\n",
      "cnt: 0 - valLoss: 0.4454931318759918 - trainLoss: 0.44969993829727173\n",
      "cnt: 0 - valLoss: 0.4454912841320038 - trainLoss: 0.4496968388557434\n",
      "cnt: 0 - valLoss: 0.4454893469810486 - trainLoss: 0.4496936798095703\n",
      "cnt: 0 - valLoss: 0.44548746943473816 - trainLoss: 0.4496906101703644\n",
      "cnt: 0 - valLoss: 0.4454857110977173 - trainLoss: 0.4496874213218689\n",
      "cnt: 0 - valLoss: 0.4454836845397949 - trainLoss: 0.44968438148498535\n",
      "cnt: 0 - valLoss: 0.44548192620277405 - trainLoss: 0.44968125224113464\n",
      "cnt: 0 - valLoss: 0.44547995924949646 - trainLoss: 0.44967809319496155\n",
      "cnt: 0 - valLoss: 0.4454781115055084 - trainLoss: 0.4496750235557556\n",
      "cnt: 0 - valLoss: 0.44547635316848755 - trainLoss: 0.4496718943119049\n",
      "cnt: 0 - valLoss: 0.44547438621520996 - trainLoss: 0.4496687650680542\n",
      "cnt: 0 - valLoss: 0.4454725384712219 - trainLoss: 0.44966569542884827\n",
      "cnt: 0 - valLoss: 0.44547078013420105 - trainLoss: 0.44966256618499756\n",
      "cnt: 0 - valLoss: 0.4454687833786011 - trainLoss: 0.44965943694114685\n",
      "cnt: 0 - valLoss: 0.4454670250415802 - trainLoss: 0.4496563673019409\n",
      "cnt: 0 - valLoss: 0.44546517729759216 - trainLoss: 0.44965317845344543\n",
      "cnt: 0 - valLoss: 0.44546329975128174 - trainLoss: 0.4496501088142395\n",
      "cnt: 0 - valLoss: 0.4454614520072937 - trainLoss: 0.4496469795703888\n",
      "cnt: 0 - valLoss: 0.44545963406562805 - trainLoss: 0.4496438503265381\n",
      "cnt: 0 - valLoss: 0.44545772671699524 - trainLoss: 0.44964078068733215\n",
      "cnt: 0 - valLoss: 0.4454559087753296 - trainLoss: 0.44963768124580383\n",
      "cnt: 0 - valLoss: 0.44545412063598633 - trainLoss: 0.44963452219963074\n",
      "cnt: 0 - valLoss: 0.44545215368270874 - trainLoss: 0.4496314525604248\n",
      "cnt: 0 - valLoss: 0.4454503357410431 - trainLoss: 0.4496283531188965\n",
      "cnt: 0 - valLoss: 0.44544845819473267 - trainLoss: 0.4496251940727234\n",
      "cnt: 0 - valLoss: 0.4454466700553894 - trainLoss: 0.4496220648288727\n",
      "cnt: 0 - valLoss: 0.4454447627067566 - trainLoss: 0.44961899518966675\n",
      "cnt: 0 - valLoss: 0.44544288516044617 - trainLoss: 0.44961586594581604\n",
      "cnt: 0 - valLoss: 0.4454410970211029 - trainLoss: 0.4496127963066101\n",
      "cnt: 0 - valLoss: 0.44543933868408203 - trainLoss: 0.4496096074581146\n",
      "cnt: 0 - valLoss: 0.44543734192848206 - trainLoss: 0.4496065378189087\n",
      "cnt: 0 - valLoss: 0.4454355537891388 - trainLoss: 0.44960343837738037\n",
      "cnt: 0 - valLoss: 0.44543376564979553 - trainLoss: 0.44960033893585205\n",
      "cnt: 0 - valLoss: 0.44543182849884033 - trainLoss: 0.44959720969200134\n",
      "cnt: 0 - valLoss: 0.44543004035949707 - trainLoss: 0.449594110250473\n",
      "cnt: 0 - valLoss: 0.4454282224178314 - trainLoss: 0.4495909512042999\n",
      "cnt: 0 - valLoss: 0.445426344871521 - trainLoss: 0.449587881565094\n",
      "cnt: 0 - valLoss: 0.44542452692985535 - trainLoss: 0.4495847821235657\n",
      "cnt: 0 - valLoss: 0.4454227387905121 - trainLoss: 0.4495816230773926\n",
      "cnt: 0 - valLoss: 0.4454208016395569 - trainLoss: 0.44957855343818665\n",
      "cnt: 0 - valLoss: 0.4454190135002136 - trainLoss: 0.4495754539966583\n",
      "cnt: 0 - valLoss: 0.44541725516319275 - trainLoss: 0.44957229495048523\n",
      "cnt: 0 - valLoss: 0.44541528820991516 - trainLoss: 0.4495691955089569\n",
      "cnt: 0 - valLoss: 0.4454135596752167 - trainLoss: 0.449566125869751\n",
      "cnt: 0 - valLoss: 0.4454115927219391 - trainLoss: 0.44956299662590027\n",
      "cnt: 0 - valLoss: 0.4454098045825958 - trainLoss: 0.44955992698669434\n",
      "cnt: 0 - valLoss: 0.44540804624557495 - trainLoss: 0.44955679774284363\n",
      "cnt: 0 - valLoss: 0.44540610909461975 - trainLoss: 0.4495537281036377\n",
      "cnt: 0 - valLoss: 0.4454043209552765 - trainLoss: 0.449550598859787\n",
      "cnt: 0 - valLoss: 0.4454025626182556 - trainLoss: 0.4495474696159363\n",
      "cnt: 0 - valLoss: 0.445400595664978 - trainLoss: 0.44954437017440796\n",
      "cnt: 0 - valLoss: 0.44539883732795715 - trainLoss: 0.44954127073287964\n",
      "cnt: 0 - valLoss: 0.4453970193862915 - trainLoss: 0.44953814148902893\n",
      "cnt: 0 - valLoss: 0.4453950822353363 - trainLoss: 0.4495350420475006\n",
      "cnt: 0 - valLoss: 0.44539329409599304 - trainLoss: 0.4495319724082947\n",
      "cnt: 0 - valLoss: 0.44539153575897217 - trainLoss: 0.44952884316444397\n",
      "cnt: 0 - valLoss: 0.4453895688056946 - trainLoss: 0.44952577352523804\n",
      "cnt: 0 - valLoss: 0.44538769125938416 - trainLoss: 0.44952264428138733\n",
      "cnt: 0 - valLoss: 0.4453858733177185 - trainLoss: 0.4495195746421814\n",
      "cnt: 0 - valLoss: 0.4453839063644409 - trainLoss: 0.4495164752006531\n",
      "cnt: 0 - valLoss: 0.44538211822509766 - trainLoss: 0.44951340556144714\n",
      "cnt: 0 - valLoss: 0.4453802704811096 - trainLoss: 0.4495103061199188\n",
      "cnt: 0 - valLoss: 0.44537830352783203 - trainLoss: 0.4495072364807129\n",
      "cnt: 0 - valLoss: 0.445376455783844 - trainLoss: 0.44950413703918457\n",
      "cnt: 0 - valLoss: 0.4453746974468231 - trainLoss: 0.44950103759765625\n",
      "cnt: 0 - valLoss: 0.44537267088890076 - trainLoss: 0.4494979977607727\n",
      "cnt: 0 - valLoss: 0.4453708827495575 - trainLoss: 0.4494948983192444\n",
      "cnt: 0 - valLoss: 0.44536906480789185 - trainLoss: 0.4494917690753937\n",
      "cnt: 0 - valLoss: 0.44536709785461426 - trainLoss: 0.44948872923851013\n",
      "cnt: 0 - valLoss: 0.44536522030830383 - trainLoss: 0.4494856297969818\n",
      "cnt: 0 - valLoss: 0.44536343216896057 - trainLoss: 0.4494825005531311\n",
      "cnt: 0 - valLoss: 0.44536149501800537 - trainLoss: 0.44947946071624756\n",
      "cnt: 0 - valLoss: 0.44535964727401733 - trainLoss: 0.44947636127471924\n",
      "cnt: 0 - valLoss: 0.4453577995300293 - trainLoss: 0.4494732916355133\n",
      "cnt: 0 - valLoss: 0.4453558623790741 - trainLoss: 0.4494701623916626\n",
      "cnt: 0 - valLoss: 0.44535404443740845 - trainLoss: 0.44946709275245667\n",
      "cnt: 0 - valLoss: 0.4453522861003876 - trainLoss: 0.44946399331092834\n",
      "cnt: 0 - valLoss: 0.4453502595424652 - trainLoss: 0.4494609236717224\n",
      "cnt: 0 - valLoss: 0.44534847140312195 - trainLoss: 0.4494578540325165\n",
      "cnt: 0 - valLoss: 0.4453466236591339 - trainLoss: 0.44945475459098816\n",
      "cnt: 0 - valLoss: 0.4453446865081787 - trainLoss: 0.4494516849517822\n",
      "cnt: 0 - valLoss: 0.4453428387641907 - trainLoss: 0.4494485855102539\n",
      "cnt: 0 - valLoss: 0.4453410804271698 - trainLoss: 0.449445515871048\n",
      "cnt: 0 - valLoss: 0.4453390836715698 - trainLoss: 0.44944244623184204\n",
      "cnt: 0 - valLoss: 0.4453372657299042 - trainLoss: 0.4494393467903137\n",
      "cnt: 0 - valLoss: 0.4453354775905609 - trainLoss: 0.4494362771511078\n",
      "cnt: 0 - valLoss: 0.4453335404396057 - trainLoss: 0.44943317770957947\n",
      "cnt: 0 - valLoss: 0.4453316926956177 - trainLoss: 0.44943010807037354\n",
      "cnt: 0 - valLoss: 0.4453299045562744 - trainLoss: 0.4494270086288452\n",
      "cnt: 0 - valLoss: 0.4453279376029968 - trainLoss: 0.4494239389896393\n",
      "cnt: 0 - valLoss: 0.44532617926597595 - trainLoss: 0.44942086935043335\n",
      "cnt: 0 - valLoss: 0.4453243315219879 - trainLoss: 0.44941776990890503\n",
      "cnt: 0 - valLoss: 0.4453224539756775 - trainLoss: 0.4494147002696991\n",
      "cnt: 0 - valLoss: 0.4453205466270447 - trainLoss: 0.4494116008281708\n",
      "cnt: 0 - valLoss: 0.4453188478946686 - trainLoss: 0.44940853118896484\n",
      "cnt: 0 - valLoss: 0.445316880941391 - trainLoss: 0.4494054615497589\n",
      "cnt: 0 - valLoss: 0.44531503319740295 - trainLoss: 0.4494023621082306\n",
      "cnt: 0 - valLoss: 0.4453132748603821 - trainLoss: 0.44939929246902466\n",
      "cnt: 0 - valLoss: 0.4453113377094269 - trainLoss: 0.44939619302749634\n",
      "cnt: 0 - valLoss: 0.445309579372406 - trainLoss: 0.4493931233882904\n",
      "cnt: 0 - valLoss: 0.44530758261680603 - trainLoss: 0.4493900537490845\n",
      "cnt: 0 - valLoss: 0.44530582427978516 - trainLoss: 0.44938695430755615\n",
      "cnt: 0 - valLoss: 0.4453040063381195 - trainLoss: 0.4493838846683502\n",
      "cnt: 0 - valLoss: 0.4453020989894867 - trainLoss: 0.4493807852268219\n",
      "cnt: 0 - valLoss: 0.44530025124549866 - trainLoss: 0.44937771558761597\n",
      "cnt: 0 - valLoss: 0.445298433303833 - trainLoss: 0.44937464594841003\n",
      "cnt: 0 - valLoss: 0.4452965557575226 - trainLoss: 0.4493715465068817\n",
      "cnt: 0 - valLoss: 0.4452947676181793 - trainLoss: 0.4493684768676758\n",
      "cnt: 0 - valLoss: 0.44529294967651367 - trainLoss: 0.44936537742614746\n",
      "cnt: 0 - valLoss: 0.44529101252555847 - trainLoss: 0.4493623375892639\n",
      "cnt: 0 - valLoss: 0.4452892541885376 - trainLoss: 0.449359267950058\n",
      "cnt: 0 - valLoss: 0.44528746604919434 - trainLoss: 0.44935619831085205\n",
      "cnt: 0 - valLoss: 0.44528549909591675 - trainLoss: 0.4493531584739685\n",
      "cnt: 0 - valLoss: 0.4452837407588959 - trainLoss: 0.4493500590324402\n",
      "cnt: 0 - valLoss: 0.44528189301490784 - trainLoss: 0.44934698939323425\n",
      "cnt: 0 - valLoss: 0.4452800154685974 - trainLoss: 0.44934388995170593\n",
      "cnt: 0 - valLoss: 0.44527819752693176 - trainLoss: 0.4493408203125\n",
      "cnt: 0 - valLoss: 0.44527649879455566 - trainLoss: 0.44933775067329407\n",
      "cnt: 0 - valLoss: 0.4452744722366333 - trainLoss: 0.44933465123176575\n",
      "cnt: 0 - valLoss: 0.4452727138996124 - trainLoss: 0.4493315815925598\n",
      "cnt: 0 - valLoss: 0.44527092576026917 - trainLoss: 0.4493284821510315\n",
      "cnt: 0 - valLoss: 0.44526904821395874 - trainLoss: 0.44932541251182556\n",
      "cnt: 0 - valLoss: 0.4452672302722931 - trainLoss: 0.449322372674942\n",
      "cnt: 0 - valLoss: 0.44526544213294983 - trainLoss: 0.4493193030357361\n",
      "cnt: 0 - valLoss: 0.4452635645866394 - trainLoss: 0.44931620359420776\n",
      "cnt: 0 - valLoss: 0.44526174664497375 - trainLoss: 0.44931307435035706\n",
      "cnt: 0 - valLoss: 0.4452599883079529 - trainLoss: 0.4493100345134735\n",
      "cnt: 0 - valLoss: 0.4452580213546753 - trainLoss: 0.44930699467658997\n",
      "cnt: 0 - valLoss: 0.44525623321533203 - trainLoss: 0.44930392503738403\n",
      "cnt: 0 - valLoss: 0.44525447487831116 - trainLoss: 0.4493008553981781\n",
      "cnt: 0 - valLoss: 0.44525256752967834 - trainLoss: 0.4492977559566498\n",
      "cnt: 0 - valLoss: 0.4452507793903351 - trainLoss: 0.44929468631744385\n",
      "cnt: 0 - valLoss: 0.4452489912509918 - trainLoss: 0.4492915868759155\n",
      "cnt: 0 - valLoss: 0.445247083902359 - trainLoss: 0.4492885172367096\n",
      "cnt: 0 - valLoss: 0.44524532556533813 - trainLoss: 0.44928544759750366\n",
      "cnt: 0 - valLoss: 0.4452435374259949 - trainLoss: 0.44928234815597534\n",
      "cnt: 0 - valLoss: 0.4452416002750397 - trainLoss: 0.4492793083190918\n",
      "cnt: 0 - valLoss: 0.4452399015426636 - trainLoss: 0.44927626848220825\n",
      "cnt: 0 - valLoss: 0.4452380836009979 - trainLoss: 0.4492731988430023\n",
      "cnt: 0 - valLoss: 0.4452362060546875 - trainLoss: 0.449270099401474\n",
      "cnt: 0 - valLoss: 0.44523441791534424 - trainLoss: 0.44926702976226807\n",
      "cnt: 0 - valLoss: 0.44523268938064575 - trainLoss: 0.4492639899253845\n",
      "cnt: 0 - valLoss: 0.4452308118343353 - trainLoss: 0.4492608606815338\n",
      "cnt: 0 - valLoss: 0.4452289640903473 - trainLoss: 0.44925782084465027\n",
      "cnt: 0 - valLoss: 0.4452272057533264 - trainLoss: 0.4492547810077667\n",
      "cnt: 0 - valLoss: 0.445225328207016 - trainLoss: 0.4492517113685608\n",
      "cnt: 0 - valLoss: 0.4452235996723175 - trainLoss: 0.44924861192703247\n",
      "cnt: 0 - valLoss: 0.44522181153297424 - trainLoss: 0.44924554228782654\n",
      "cnt: 0 - valLoss: 0.44521990418434143 - trainLoss: 0.4492424726486206\n",
      "cnt: 0 - valLoss: 0.44521811604499817 - trainLoss: 0.44923943281173706\n",
      "cnt: 0 - valLoss: 0.44521641731262207 - trainLoss: 0.4492363929748535\n",
      "cnt: 0 - valLoss: 0.44521450996398926 - trainLoss: 0.4492332935333252\n",
      "cnt: 0 - valLoss: 0.445212721824646 - trainLoss: 0.44923022389411926\n",
      "cnt: 0 - valLoss: 0.44521090388298035 - trainLoss: 0.44922712445259094\n",
      "cnt: 0 - valLoss: 0.4452090263366699 - trainLoss: 0.4492241144180298\n",
      "cnt: 0 - valLoss: 0.4452073276042938 - trainLoss: 0.4492209851741791\n",
      "cnt: 0 - valLoss: 0.44520556926727295 - trainLoss: 0.44921794533729553\n",
      "cnt: 0 - valLoss: 0.44520363211631775 - trainLoss: 0.449214905500412\n",
      "cnt: 0 - valLoss: 0.44520190358161926 - trainLoss: 0.44921180605888367\n",
      "cnt: 0 - valLoss: 0.445200115442276 - trainLoss: 0.44920873641967773\n",
      "cnt: 0 - valLoss: 0.4451982378959656 - trainLoss: 0.4492056667804718\n",
      "cnt: 0 - valLoss: 0.4451964199542999 - trainLoss: 0.4492025673389435\n",
      "cnt: 0 - valLoss: 0.44519472122192383 - trainLoss: 0.4491995871067047\n",
      "cnt: 0 - valLoss: 0.4451928436756134 - trainLoss: 0.4491964876651764\n",
      "cnt: 0 - valLoss: 0.44519099593162537 - trainLoss: 0.44919341802597046\n",
      "cnt: 0 - valLoss: 0.4451892375946045 - trainLoss: 0.44919031858444214\n",
      "cnt: 0 - valLoss: 0.44518738985061646 - trainLoss: 0.4491872489452362\n",
      "cnt: 0 - valLoss: 0.4451856315135956 - trainLoss: 0.44918423891067505\n",
      "cnt: 0 - valLoss: 0.4451838731765747 - trainLoss: 0.44918107986450195\n",
      "cnt: 0 - valLoss: 0.4451819360256195 - trainLoss: 0.4491780400276184\n",
      "cnt: 0 - valLoss: 0.4451802372932434 - trainLoss: 0.44917500019073486\n",
      "cnt: 0 - valLoss: 0.4451785087585449 - trainLoss: 0.44917193055152893\n",
      "cnt: 0 - valLoss: 0.44517654180526733 - trainLoss: 0.4491688907146454\n",
      "cnt: 0 - valLoss: 0.44517481327056885 - trainLoss: 0.44916579127311707\n",
      "cnt: 0 - valLoss: 0.44517308473587036 - trainLoss: 0.4491627514362335\n",
      "cnt: 0 - valLoss: 0.4451712369918823 - trainLoss: 0.4491596817970276\n",
      "cnt: 0 - valLoss: 0.4451694190502167 - trainLoss: 0.44915661215782166\n",
      "cnt: 0 - valLoss: 0.4451677203178406 - trainLoss: 0.44915351271629333\n",
      "cnt: 0 - valLoss: 0.44516581296920776 - trainLoss: 0.4491504728794098\n",
      "cnt: 0 - valLoss: 0.44516411423683167 - trainLoss: 0.44914743304252625\n",
      "cnt: 0 - valLoss: 0.4451623558998108 - trainLoss: 0.4491443634033203\n",
      "cnt: 0 - valLoss: 0.44516050815582275 - trainLoss: 0.44914132356643677\n",
      "cnt: 0 - valLoss: 0.4451587200164795 - trainLoss: 0.44913825392723083\n",
      "cnt: 0 - valLoss: 0.445156991481781 - trainLoss: 0.4491352140903473\n",
      "cnt: 0 - valLoss: 0.4451551139354706 - trainLoss: 0.44913211464881897\n",
      "cnt: 0 - valLoss: 0.4451533555984497 - trainLoss: 0.44912904500961304\n",
      "cnt: 0 - valLoss: 0.44515159726142883 - trainLoss: 0.4491260349750519\n",
      "cnt: 0 - valLoss: 0.4451497197151184 - trainLoss: 0.44912296533584595\n",
      "cnt: 0 - valLoss: 0.4451479911804199 - trainLoss: 0.4491198658943176\n",
      "cnt: 0 - valLoss: 0.4451462924480438 - trainLoss: 0.4491168260574341\n",
      "cnt: 0 - valLoss: 0.445144385099411 - trainLoss: 0.44911375641822815\n",
      "cnt: 0 - valLoss: 0.44514262676239014 - trainLoss: 0.4491107165813446\n",
      "cnt: 0 - valLoss: 0.44514092803001404 - trainLoss: 0.44910764694213867\n",
      "cnt: 0 - valLoss: 0.4451390504837036 - trainLoss: 0.44910454750061035\n",
      "cnt: 0 - valLoss: 0.4451373219490051 - trainLoss: 0.4491015076637268\n",
      "cnt: 0 - valLoss: 0.44513556361198425 - trainLoss: 0.44909846782684326\n",
      "cnt: 0 - valLoss: 0.4451337158679962 - trainLoss: 0.44909539818763733\n",
      "cnt: 0 - valLoss: 0.44513195753097534 - trainLoss: 0.4490923285484314\n",
      "cnt: 0 - valLoss: 0.44513025879859924 - trainLoss: 0.44908928871154785\n",
      "cnt: 0 - valLoss: 0.44512835144996643 - trainLoss: 0.4490862488746643\n",
      "cnt: 0 - valLoss: 0.44512665271759033 - trainLoss: 0.449083149433136\n",
      "cnt: 0 - valLoss: 0.44512492418289185 - trainLoss: 0.4490801692008972\n",
      "cnt: 0 - valLoss: 0.4451230466365814 - trainLoss: 0.4490770697593689\n",
      "cnt: 0 - valLoss: 0.44512125849723816 - trainLoss: 0.44907400012016296\n",
      "cnt: 0 - valLoss: 0.4451195299625397 - trainLoss: 0.4490709602832794\n",
      "cnt: 0 - valLoss: 0.44511768221855164 - trainLoss: 0.4490678608417511\n",
      "cnt: 0 - valLoss: 0.44511598348617554 - trainLoss: 0.44906479120254517\n",
      "cnt: 0 - valLoss: 0.44511422514915466 - trainLoss: 0.4490617513656616\n",
      "cnt: 0 - valLoss: 0.4451123774051666 - trainLoss: 0.4490587115287781\n",
      "cnt: 0 - valLoss: 0.44511061906814575 - trainLoss: 0.44905564188957214\n",
      "cnt: 0 - valLoss: 0.4451088309288025 - trainLoss: 0.4490526020526886\n",
      "cnt: 0 - valLoss: 0.44510701298713684 - trainLoss: 0.4490495026111603\n",
      "cnt: 0 - valLoss: 0.4451054036617279 - trainLoss: 0.44904646277427673\n",
      "cnt: 0 - valLoss: 0.4451034665107727 - trainLoss: 0.4490433931350708\n",
      "cnt: 0 - valLoss: 0.445101797580719 - trainLoss: 0.44904035329818726\n",
      "cnt: 0 - valLoss: 0.4451000392436981 - trainLoss: 0.4490372836589813\n",
      "cnt: 0 - valLoss: 0.4450981616973877 - trainLoss: 0.44903427362442017\n",
      "cnt: 0 - valLoss: 0.4450964033603668 - trainLoss: 0.44903120398521423\n",
      "cnt: 0 - valLoss: 0.4450947046279907 - trainLoss: 0.4490281641483307\n",
      "cnt: 0 - valLoss: 0.4450928866863251 - trainLoss: 0.44902506470680237\n",
      "cnt: 0 - valLoss: 0.4450911283493042 - trainLoss: 0.44902199506759644\n",
      "cnt: 0 - valLoss: 0.4450893700122833 - trainLoss: 0.4490189552307129\n",
      "cnt: 0 - valLoss: 0.4450875520706177 - trainLoss: 0.44901594519615173\n",
      "cnt: 0 - valLoss: 0.4450858533382416 - trainLoss: 0.4490128755569458\n",
      "cnt: 0 - valLoss: 0.4450840950012207 - trainLoss: 0.44900980591773987\n",
      "cnt: 0 - valLoss: 0.44508224725723267 - trainLoss: 0.4490067958831787\n",
      "cnt: 0 - valLoss: 0.44508057832717896 - trainLoss: 0.4490037262439728\n",
      "cnt: 0 - valLoss: 0.44507884979248047 - trainLoss: 0.4490007162094116\n",
      "cnt: 0 - valLoss: 0.44507697224617004 - trainLoss: 0.4489976465702057\n",
      "cnt: 0 - valLoss: 0.44507530331611633 - trainLoss: 0.44899454712867737\n",
      "cnt: 0 - valLoss: 0.44507357478141785 - trainLoss: 0.4489915072917938\n",
      "cnt: 0 - valLoss: 0.4450717568397522 - trainLoss: 0.4489884674549103\n",
      "cnt: 0 - valLoss: 0.4450699985027313 - trainLoss: 0.44898539781570435\n",
      "cnt: 0 - valLoss: 0.4450683295726776 - trainLoss: 0.4489823877811432\n",
      "cnt: 0 - valLoss: 0.4450664818286896 - trainLoss: 0.44897931814193726\n",
      "cnt: 0 - valLoss: 0.4450647532939911 - trainLoss: 0.4489762485027313\n",
      "cnt: 0 - valLoss: 0.4450629949569702 - trainLoss: 0.44897323846817017\n",
      "cnt: 0 - valLoss: 0.4450611472129822 - trainLoss: 0.44897016882896423\n",
      "cnt: 0 - valLoss: 0.44505947828292847 - trainLoss: 0.4489671289920807\n",
      "cnt: 0 - valLoss: 0.44505777955055237 - trainLoss: 0.44896408915519714\n",
      "cnt: 0 - valLoss: 0.44505593180656433 - trainLoss: 0.4489609897136688\n",
      "cnt: 0 - valLoss: 0.44505420327186584 - trainLoss: 0.4489579498767853\n",
      "cnt: 0 - valLoss: 0.44505250453948975 - trainLoss: 0.44895491003990173\n",
      "cnt: 0 - valLoss: 0.4450506567955017 - trainLoss: 0.4489518404006958\n",
      "cnt: 0 - valLoss: 0.445048987865448 - trainLoss: 0.44894883036613464\n",
      "cnt: 0 - valLoss: 0.44504719972610474 - trainLoss: 0.4489457607269287\n",
      "cnt: 0 - valLoss: 0.4450453817844391 - trainLoss: 0.4489426910877228\n",
      "cnt: 0 - valLoss: 0.445043683052063 - trainLoss: 0.4489396810531616\n",
      "cnt: 0 - valLoss: 0.4450420141220093 - trainLoss: 0.4489366114139557\n",
      "cnt: 0 - valLoss: 0.44504010677337646 - trainLoss: 0.44893357157707214\n",
      "cnt: 0 - valLoss: 0.44503840804100037 - trainLoss: 0.4489305317401886\n",
      "cnt: 0 - valLoss: 0.44503673911094666 - trainLoss: 0.44892749190330505\n",
      "cnt: 0 - valLoss: 0.4450348913669586 - trainLoss: 0.4489244520664215\n",
      "cnt: 0 - valLoss: 0.44503316283226013 - trainLoss: 0.44892141222953796\n",
      "cnt: 0 - valLoss: 0.4450314939022064 - trainLoss: 0.44891831278800964\n",
      "cnt: 0 - valLoss: 0.445029616355896 - trainLoss: 0.4489152729511261\n",
      "cnt: 0 - valLoss: 0.4450279474258423 - trainLoss: 0.44891223311424255\n",
      "cnt: 0 - valLoss: 0.4450262188911438 - trainLoss: 0.4489091634750366\n",
      "cnt: 0 - valLoss: 0.44502437114715576 - trainLoss: 0.4489061236381531\n",
      "cnt: 0 - valLoss: 0.44502267241477966 - trainLoss: 0.4489031434059143\n",
      "cnt: 0 - valLoss: 0.44502100348472595 - trainLoss: 0.448900043964386\n",
      "cnt: 0 - valLoss: 0.4450191557407379 - trainLoss: 0.44889697432518005\n",
      "cnt: 0 - valLoss: 0.44501742720603943 - trainLoss: 0.4488939642906189\n",
      "cnt: 0 - valLoss: 0.44501572847366333 - trainLoss: 0.44889089465141296\n",
      "cnt: 0 - valLoss: 0.4450139105319977 - trainLoss: 0.4488878548145294\n",
      "cnt: 0 - valLoss: 0.4450122117996216 - trainLoss: 0.4488848149776459\n",
      "cnt: 0 - valLoss: 0.4450104832649231 - trainLoss: 0.44888177514076233\n",
      "cnt: 0 - valLoss: 0.44500869512557983 - trainLoss: 0.4488787353038788\n",
      "cnt: 0 - valLoss: 0.44500696659088135 - trainLoss: 0.44887569546699524\n",
      "cnt: 0 - valLoss: 0.4450053572654724 - trainLoss: 0.4488726556301117\n",
      "cnt: 0 - valLoss: 0.445003479719162 - trainLoss: 0.44886958599090576\n",
      "cnt: 0 - valLoss: 0.4450017809867859 - trainLoss: 0.4488665759563446\n",
      "cnt: 0 - valLoss: 0.4450001120567322 - trainLoss: 0.44886350631713867\n",
      "cnt: 0 - valLoss: 0.4449983239173889 - trainLoss: 0.4488604962825775\n",
      "cnt: 0 - valLoss: 0.44499659538269043 - trainLoss: 0.4488574266433716\n",
      "cnt: 0 - valLoss: 0.4449949264526367 - trainLoss: 0.44885438680648804\n",
      "cnt: 0 - valLoss: 0.44499310851097107 - trainLoss: 0.4488513469696045\n",
      "cnt: 0 - valLoss: 0.44499140977859497 - trainLoss: 0.44884830713272095\n",
      "cnt: 0 - valLoss: 0.44498974084854126 - trainLoss: 0.4488452672958374\n",
      "cnt: 0 - valLoss: 0.444987952709198 - trainLoss: 0.44884222745895386\n",
      "cnt: 0 - valLoss: 0.4449862241744995 - trainLoss: 0.4488391876220703\n",
      "cnt: 0 - valLoss: 0.4449845552444458 - trainLoss: 0.44883614778518677\n",
      "cnt: 0 - valLoss: 0.44498270750045776 - trainLoss: 0.4488331079483032\n",
      "cnt: 0 - valLoss: 0.44498100876808167 - trainLoss: 0.4488300681114197\n",
      "cnt: 0 - valLoss: 0.44497936964035034 - trainLoss: 0.44882699847221375\n",
      "cnt: 0 - valLoss: 0.4449775218963623 - trainLoss: 0.4488239586353302\n",
      "cnt: 0 - valLoss: 0.4449758231639862 - trainLoss: 0.44882094860076904\n",
      "cnt: 0 - valLoss: 0.4449741542339325 - trainLoss: 0.4488178789615631\n",
      "cnt: 0 - valLoss: 0.44497233629226685 - trainLoss: 0.44881486892700195\n",
      "cnt: 0 - valLoss: 0.44497063755989075 - trainLoss: 0.4488118290901184\n",
      "cnt: 0 - valLoss: 0.44496896862983704 - trainLoss: 0.44880878925323486\n",
      "cnt: 0 - valLoss: 0.4449671506881714 - trainLoss: 0.4488057494163513\n",
      "cnt: 0 - valLoss: 0.4449654817581177 - trainLoss: 0.4488027095794678\n",
      "cnt: 0 - valLoss: 0.4449637830257416 - trainLoss: 0.44879963994026184\n",
      "cnt: 0 - valLoss: 0.4449619650840759 - trainLoss: 0.4487966001033783\n",
      "cnt: 0 - valLoss: 0.4449602961540222 - trainLoss: 0.44879356026649475\n",
      "cnt: 0 - valLoss: 0.4449586272239685 - trainLoss: 0.4487905204296112\n",
      "cnt: 0 - valLoss: 0.44495677947998047 - trainLoss: 0.44878748059272766\n",
      "cnt: 0 - valLoss: 0.44495517015457153 - trainLoss: 0.4487844705581665\n",
      "cnt: 0 - valLoss: 0.44495344161987305 - trainLoss: 0.44878140091896057\n",
      "cnt: 0 - valLoss: 0.4449516534805298 - trainLoss: 0.4487784206867218\n",
      "cnt: 0 - valLoss: 0.4449499845504761 - trainLoss: 0.4487753212451935\n",
      "cnt: 0 - valLoss: 0.4449482560157776 - trainLoss: 0.4487723410129547\n",
      "cnt: 0 - valLoss: 0.4449464678764343 - trainLoss: 0.44876930117607117\n",
      "cnt: 0 - valLoss: 0.4449447989463806 - trainLoss: 0.44876620173454285\n",
      "cnt: 0 - valLoss: 0.4449431300163269 - trainLoss: 0.4487631618976593\n",
      "cnt: 0 - valLoss: 0.44494131207466125 - trainLoss: 0.44876018166542053\n",
      "cnt: 0 - valLoss: 0.44493961334228516 - trainLoss: 0.448757141828537\n",
      "cnt: 0 - valLoss: 0.44493794441223145 - trainLoss: 0.44875404238700867\n",
      "cnt: 0 - valLoss: 0.4449361264705658 - trainLoss: 0.4487510025501251\n",
      "cnt: 0 - valLoss: 0.4449344277381897 - trainLoss: 0.44874802231788635\n",
      "cnt: 0 - valLoss: 0.44493260979652405 - trainLoss: 0.4487449824810028\n",
      "cnt: 0 - valLoss: 0.4449310004711151 - trainLoss: 0.44874194264411926\n",
      "cnt: 0 - valLoss: 0.444929301738739 - trainLoss: 0.4487389028072357\n",
      "cnt: 0 - valLoss: 0.44492748379707336 - trainLoss: 0.4487358629703522\n",
      "cnt: 0 - valLoss: 0.44492584466934204 - trainLoss: 0.44873282313346863\n",
      "cnt: 0 - valLoss: 0.44492417573928833 - trainLoss: 0.4487297832965851\n",
      "cnt: 0 - valLoss: 0.44492241740226746 - trainLoss: 0.44872674345970154\n",
      "cnt: 0 - valLoss: 0.44492068886756897 - trainLoss: 0.448723703622818\n",
      "cnt: 0 - valLoss: 0.44491907954216003 - trainLoss: 0.4487207233905792\n",
      "cnt: 0 - valLoss: 0.444917231798172 - trainLoss: 0.4487176835536957\n",
      "cnt: 0 - valLoss: 0.4449155926704407 - trainLoss: 0.44871464371681213\n",
      "cnt: 0 - valLoss: 0.44491398334503174 - trainLoss: 0.4487116038799286\n",
      "cnt: 0 - valLoss: 0.4449121654033661 - trainLoss: 0.44870856404304504\n",
      "cnt: 0 - valLoss: 0.44491046667099 - trainLoss: 0.4487055540084839\n",
      "cnt: 0 - valLoss: 0.44490882754325867 - trainLoss: 0.44870248436927795\n",
      "cnt: 0 - valLoss: 0.4449070394039154 - trainLoss: 0.4486994743347168\n",
      "cnt: 0 - valLoss: 0.4449053108692169 - trainLoss: 0.44869643449783325\n",
      "cnt: 0 - valLoss: 0.4449036717414856 - trainLoss: 0.4486933946609497\n",
      "cnt: 0 - valLoss: 0.44490188360214233 - trainLoss: 0.4486903250217438\n",
      "cnt: 0 - valLoss: 0.444900244474411 - trainLoss: 0.4486873149871826\n",
      "cnt: 0 - valLoss: 0.4448985457420349 - trainLoss: 0.44868433475494385\n",
      "cnt: 0 - valLoss: 0.4448968172073364 - trainLoss: 0.4486812353134155\n",
      "cnt: 0 - valLoss: 0.4448951184749603 - trainLoss: 0.44867825508117676\n",
      "cnt: 0 - valLoss: 0.4448935389518738 - trainLoss: 0.4486752152442932\n",
      "cnt: 0 - valLoss: 0.44489172101020813 - trainLoss: 0.44867217540740967\n",
      "cnt: 0 - valLoss: 0.4448900520801544 - trainLoss: 0.4486691355705261\n",
      "cnt: 0 - valLoss: 0.4448883831501007 - trainLoss: 0.44866612553596497\n",
      "cnt: 0 - valLoss: 0.44488662481307983 - trainLoss: 0.44866305589675903\n",
      "cnt: 0 - valLoss: 0.4448849558830261 - trainLoss: 0.4486600160598755\n",
      "cnt: 0 - valLoss: 0.4448832869529724 - trainLoss: 0.4486570358276367\n",
      "cnt: 0 - valLoss: 0.44488146901130676 - trainLoss: 0.44865402579307556\n",
      "cnt: 0 - valLoss: 0.4448798596858978 - trainLoss: 0.448650985956192\n",
      "cnt: 0 - valLoss: 0.4448782205581665 - trainLoss: 0.44864794611930847\n",
      "cnt: 0 - valLoss: 0.4448763430118561 - trainLoss: 0.4486449062824249\n",
      "cnt: 0 - valLoss: 0.44487476348876953 - trainLoss: 0.4486418664455414\n",
      "cnt: 0 - valLoss: 0.4448730945587158 - trainLoss: 0.4486388862133026\n",
      "cnt: 0 - valLoss: 0.44487127661705017 - trainLoss: 0.4486357867717743\n",
      "cnt: 0 - valLoss: 0.44486960768699646 - trainLoss: 0.4486328065395355\n",
      "cnt: 0 - valLoss: 0.44486793875694275 - trainLoss: 0.448629766702652\n",
      "cnt: 0 - valLoss: 0.4448661804199219 - trainLoss: 0.44862672686576843\n",
      "cnt: 0 - valLoss: 0.44486451148986816 - trainLoss: 0.4486236870288849\n",
      "cnt: 0 - valLoss: 0.44486287236213684 - trainLoss: 0.44862064719200134\n",
      "cnt: 0 - valLoss: 0.44486117362976074 - trainLoss: 0.4486176371574402\n",
      "cnt: 0 - valLoss: 0.44485950469970703 - trainLoss: 0.44861459732055664\n",
      "cnt: 0 - valLoss: 0.4448578953742981 - trainLoss: 0.4486115574836731\n",
      "cnt: 0 - valLoss: 0.44485610723495483 - trainLoss: 0.44860851764678955\n",
      "cnt: 0 - valLoss: 0.4448544681072235 - trainLoss: 0.4486055374145508\n",
      "cnt: 0 - valLoss: 0.4448527991771698 - trainLoss: 0.44860249757766724\n",
      "cnt: 0 - valLoss: 0.4448511302471161 - trainLoss: 0.4485994875431061\n",
      "cnt: 0 - valLoss: 0.44484952092170715 - trainLoss: 0.44859641790390015\n",
      "cnt: 0 - valLoss: 0.44484788179397583 - trainLoss: 0.448593407869339\n",
      "cnt: 0 - valLoss: 0.4448460638523102 - trainLoss: 0.4485904276371002\n",
      "cnt: 0 - valLoss: 0.44484448432922363 - trainLoss: 0.4485873878002167\n",
      "cnt: 0 - valLoss: 0.4448429048061371 - trainLoss: 0.44858434796333313\n",
      "cnt: 0 - valLoss: 0.4448411166667938 - trainLoss: 0.448581337928772\n",
      "cnt: 0 - valLoss: 0.4448394775390625 - trainLoss: 0.44857826828956604\n",
      "cnt: 0 - valLoss: 0.44483792781829834 - trainLoss: 0.4485752582550049\n",
      "cnt: 0 - valLoss: 0.44483616948127747 - trainLoss: 0.4485722780227661\n",
      "cnt: 0 - valLoss: 0.44483456015586853 - trainLoss: 0.44856923818588257\n",
      "cnt: 0 - valLoss: 0.4448329508304596 - trainLoss: 0.448566198348999\n",
      "cnt: 0 - valLoss: 0.44483116269111633 - trainLoss: 0.4485631585121155\n",
      "cnt: 0 - valLoss: 0.4448295831680298 - trainLoss: 0.4485602080821991\n",
      "cnt: 0 - valLoss: 0.44482800364494324 - trainLoss: 0.4485571086406708\n",
      "cnt: 0 - valLoss: 0.4448261857032776 - trainLoss: 0.448554128408432\n",
      "cnt: 0 - valLoss: 0.4448246359825134 - trainLoss: 0.44855108857154846\n",
      "cnt: 0 - valLoss: 0.4448229670524597 - trainLoss: 0.4485480487346649\n",
      "cnt: 0 - valLoss: 0.44482120871543884 - trainLoss: 0.44854503870010376\n",
      "cnt: 0 - valLoss: 0.4448196291923523 - trainLoss: 0.448542058467865\n",
      "cnt: 0 - valLoss: 0.44481804966926575 - trainLoss: 0.44853898882865906\n",
      "cnt: 0 - valLoss: 0.4448162019252777 - trainLoss: 0.4485359787940979\n",
      "cnt: 0 - valLoss: 0.44481462240219116 - trainLoss: 0.44853296875953674\n",
      "cnt: 0 - valLoss: 0.4448130428791046 - trainLoss: 0.4485299289226532\n",
      "cnt: 0 - valLoss: 0.44481131434440613 - trainLoss: 0.44852691888809204\n",
      "cnt: 0 - valLoss: 0.4448096752166748 - trainLoss: 0.4485239088535309\n",
      "cnt: 0 - valLoss: 0.44480809569358826 - trainLoss: 0.44852083921432495\n",
      "cnt: 0 - valLoss: 0.444806307554245 - trainLoss: 0.4485178589820862\n",
      "cnt: 0 - valLoss: 0.44480472803115845 - trainLoss: 0.44851481914520264\n",
      "cnt: 0 - valLoss: 0.4448031187057495 - trainLoss: 0.44851183891296387\n",
      "cnt: 0 - valLoss: 0.44480133056640625 - trainLoss: 0.4485087990760803\n",
      "cnt: 0 - valLoss: 0.4447997808456421 - trainLoss: 0.4485057592391968\n",
      "cnt: 0 - valLoss: 0.44479817152023315 - trainLoss: 0.44850271940231323\n",
      "cnt: 0 - valLoss: 0.4447964131832123 - trainLoss: 0.44849976897239685\n",
      "cnt: 0 - valLoss: 0.44479480385780334 - trainLoss: 0.4484966993331909\n",
      "cnt: 0 - valLoss: 0.4447932243347168 - trainLoss: 0.44849368929862976\n",
      "cnt: 0 - valLoss: 0.44479143619537354 - trainLoss: 0.4484906792640686\n",
      "cnt: 0 - valLoss: 0.444789856672287 - trainLoss: 0.44848760962486267\n",
      "cnt: 0 - valLoss: 0.44478821754455566 - trainLoss: 0.4484846293926239\n",
      "cnt: 0 - valLoss: 0.4447864890098572 - trainLoss: 0.44848161935806274\n",
      "cnt: 0 - valLoss: 0.44478487968444824 - trainLoss: 0.4484786093235016\n",
      "cnt: 0 - valLoss: 0.4447833001613617 - trainLoss: 0.44847553968429565\n",
      "cnt: 0 - valLoss: 0.4447815418243408 - trainLoss: 0.4484725594520569\n",
      "cnt: 0 - valLoss: 0.4447799026966095 - trainLoss: 0.4484695494174957\n",
      "cnt: 0 - valLoss: 0.44477835297584534 - trainLoss: 0.4484665095806122\n",
      "cnt: 0 - valLoss: 0.4447765648365021 - trainLoss: 0.44846346974372864\n",
      "cnt: 0 - valLoss: 0.4447749853134155 - trainLoss: 0.44846048951148987\n",
      "cnt: 0 - valLoss: 0.4447733759880066 - trainLoss: 0.4484574794769287\n",
      "cnt: 0 - valLoss: 0.4447716772556305 - trainLoss: 0.44845443964004517\n",
      "cnt: 0 - valLoss: 0.44477003812789917 - trainLoss: 0.4484514594078064\n",
      "cnt: 0 - valLoss: 0.4447684586048126 - trainLoss: 0.44844841957092285\n",
      "cnt: 0 - valLoss: 0.444766640663147 - trainLoss: 0.4484454095363617\n",
      "cnt: 0 - valLoss: 0.4447650611400604 - trainLoss: 0.44844239950180054\n",
      "cnt: 0 - valLoss: 0.44476354122161865 - trainLoss: 0.4484393298625946\n",
      "cnt: 0 - valLoss: 0.44476181268692017 - trainLoss: 0.44843634963035583\n",
      "cnt: 0 - valLoss: 0.44476017355918884 - trainLoss: 0.4484333395957947\n",
      "cnt: 0 - valLoss: 0.44475865364074707 - trainLoss: 0.44843029975891113\n",
      "cnt: 0 - valLoss: 0.4447568953037262 - trainLoss: 0.4484272599220276\n",
      "cnt: 0 - valLoss: 0.4447551965713501 - trainLoss: 0.4484242796897888\n",
      "cnt: 0 - valLoss: 0.4447536766529083 - trainLoss: 0.44842126965522766\n",
      "cnt: 0 - valLoss: 0.44475194811820984 - trainLoss: 0.4484182596206665\n",
      "cnt: 0 - valLoss: 0.4447503685951233 - trainLoss: 0.44841521978378296\n",
      "cnt: 0 - valLoss: 0.44474878907203674 - trainLoss: 0.4484122395515442\n",
      "cnt: 0 - valLoss: 0.44474706053733826 - trainLoss: 0.44840919971466064\n",
      "cnt: 0 - valLoss: 0.4447454512119293 - trainLoss: 0.4484061896800995\n",
      "cnt: 0 - valLoss: 0.44474390149116516 - trainLoss: 0.4484032094478607\n",
      "cnt: 0 - valLoss: 0.4447421431541443 - trainLoss: 0.4484001696109772\n",
      "cnt: 0 - valLoss: 0.44474056363105774 - trainLoss: 0.44839712977409363\n",
      "cnt: 0 - valLoss: 0.4447389245033264 - trainLoss: 0.44839411973953247\n",
      "cnt: 0 - valLoss: 0.44473719596862793 - trainLoss: 0.4483911395072937\n",
      "cnt: 0 - valLoss: 0.44473567605018616 - trainLoss: 0.44838809967041016\n",
      "cnt: 0 - valLoss: 0.44473403692245483 - trainLoss: 0.4483851492404938\n",
      "cnt: 0 - valLoss: 0.44473230838775635 - trainLoss: 0.44838210940361023\n",
      "cnt: 0 - valLoss: 0.4447307884693146 - trainLoss: 0.4483790695667267\n",
      "cnt: 0 - valLoss: 0.44472914934158325 - trainLoss: 0.4483760595321655\n",
      "cnt: 0 - valLoss: 0.44472745060920715 - trainLoss: 0.44837307929992676\n",
      "cnt: 0 - valLoss: 0.444725900888443 - trainLoss: 0.4483700692653656\n",
      "cnt: 0 - valLoss: 0.44472426176071167 - trainLoss: 0.44836702942848206\n",
      "cnt: 0 - valLoss: 0.44472259283065796 - trainLoss: 0.4483639895915985\n",
      "cnt: 0 - valLoss: 0.44472092390060425 - trainLoss: 0.44836100935935974\n",
      "cnt: 0 - valLoss: 0.4447193741798401 - trainLoss: 0.4483579993247986\n",
      "cnt: 0 - valLoss: 0.4447176158428192 - trainLoss: 0.4483550190925598\n",
      "cnt: 0 - valLoss: 0.44471606612205505 - trainLoss: 0.44835197925567627\n",
      "cnt: 0 - valLoss: 0.4447144865989685 - trainLoss: 0.4483489394187927\n",
      "cnt: 0 - valLoss: 0.4447127878665924 - trainLoss: 0.44834598898887634\n",
      "cnt: 0 - valLoss: 0.4447111487388611 - trainLoss: 0.4483429491519928\n",
      "cnt: 0 - valLoss: 0.4447095990180969 - trainLoss: 0.44833993911743164\n",
      "cnt: 0 - valLoss: 0.4447079002857208 - trainLoss: 0.4483368992805481\n",
      "cnt: 0 - valLoss: 0.4447063207626343 - trainLoss: 0.4483339488506317\n",
      "cnt: 0 - valLoss: 0.44470471143722534 - trainLoss: 0.4483308792114258\n",
      "cnt: 0 - valLoss: 0.44470295310020447 - trainLoss: 0.4483279287815094\n",
      "cnt: 0 - valLoss: 0.4447013735771179 - trainLoss: 0.44832488894462585\n",
      "cnt: 0 - valLoss: 0.44469982385635376 - trainLoss: 0.4483218789100647\n",
      "cnt: 0 - valLoss: 0.44469812512397766 - trainLoss: 0.4483188986778259\n",
      "cnt: 0 - valLoss: 0.4446965456008911 - trainLoss: 0.44831588864326477\n",
      "cnt: 0 - valLoss: 0.4446949064731598 - trainLoss: 0.4483128488063812\n",
      "cnt: 0 - valLoss: 0.4446932375431061 - trainLoss: 0.4483098089694977\n",
      "cnt: 0 - valLoss: 0.44469165802001953 - trainLoss: 0.4483068287372589\n",
      "cnt: 0 - valLoss: 0.4446900486946106 - trainLoss: 0.44830381870269775\n",
      "cnt: 0 - valLoss: 0.4446883499622345 - trainLoss: 0.4483007788658142\n",
      "cnt: 0 - valLoss: 0.44468680024147034 - trainLoss: 0.4482978284358978\n",
      "cnt: 0 - valLoss: 0.444685161113739 - trainLoss: 0.4482947885990143\n",
      "cnt: 0 - valLoss: 0.4446834623813629 - trainLoss: 0.44829174876213074\n",
      "cnt: 0 - valLoss: 0.4446818232536316 - trainLoss: 0.44828876852989197\n",
      "cnt: 0 - valLoss: 0.4446803033351898 - trainLoss: 0.4482857584953308\n",
      "cnt: 0 - valLoss: 0.44467857480049133 - trainLoss: 0.44828277826309204\n",
      "cnt: 0 - valLoss: 0.4446770250797272 - trainLoss: 0.4482797682285309\n",
      "cnt: 0 - valLoss: 0.444675475358963 - trainLoss: 0.4482767581939697\n",
      "cnt: 0 - valLoss: 0.44467368721961975 - trainLoss: 0.44827377796173096\n",
      "cnt: 0 - valLoss: 0.44467219710350037 - trainLoss: 0.4482707381248474\n",
      "cnt: 0 - valLoss: 0.4446706473827362 - trainLoss: 0.44826772809028625\n",
      "cnt: 0 - valLoss: 0.4446689784526825 - trainLoss: 0.4482647478580475\n",
      "cnt: 0 - valLoss: 0.44466733932495117 - trainLoss: 0.44826170802116394\n",
      "cnt: 0 - valLoss: 0.444665789604187 - trainLoss: 0.4482586979866028\n",
      "cnt: 0 - valLoss: 0.4446640908718109 - trainLoss: 0.448255717754364\n",
      "cnt: 0 - valLoss: 0.44466254115104675 - trainLoss: 0.44825270771980286\n",
      "cnt: 0 - valLoss: 0.4446609616279602 - trainLoss: 0.4482497274875641\n",
      "cnt: 0 - valLoss: 0.4446592926979065 - trainLoss: 0.44824671745300293\n",
      "cnt: 0 - valLoss: 0.44465771317481995 - trainLoss: 0.4482436776161194\n",
      "cnt: 0 - valLoss: 0.444656103849411 - trainLoss: 0.4482406973838806\n",
      "cnt: 0 - valLoss: 0.4446544349193573 - trainLoss: 0.44823765754699707\n",
      "cnt: 0 - valLoss: 0.44465285539627075 - trainLoss: 0.4482347071170807\n",
      "cnt: 0 - valLoss: 0.44465136528015137 - trainLoss: 0.44823166728019714\n",
      "cnt: 0 - valLoss: 0.4446496069431305 - trainLoss: 0.448228657245636\n",
      "cnt: 0 - valLoss: 0.44464805722236633 - trainLoss: 0.4482256770133972\n",
      "cnt: 0 - valLoss: 0.4446464776992798 - trainLoss: 0.44822266697883606\n",
      "cnt: 0 - valLoss: 0.4446448087692261 - trainLoss: 0.4482196867465973\n",
      "cnt: 0 - valLoss: 0.44464316964149475 - trainLoss: 0.44821667671203613\n",
      "cnt: 0 - valLoss: 0.44464167952537537 - trainLoss: 0.44821369647979736\n",
      "cnt: 0 - valLoss: 0.44464001059532166 - trainLoss: 0.4482106864452362\n",
      "cnt: 0 - valLoss: 0.44463834166526794 - trainLoss: 0.44820767641067505\n",
      "cnt: 0 - valLoss: 0.44463682174682617 - trainLoss: 0.4482046961784363\n",
      "cnt: 0 - valLoss: 0.4446351230144501 - trainLoss: 0.4482016861438751\n",
      "cnt: 0 - valLoss: 0.4446335732936859 - trainLoss: 0.4481986165046692\n",
      "cnt: 0 - valLoss: 0.44463199377059937 - trainLoss: 0.4481956660747528\n",
      "cnt: 0 - valLoss: 0.44463032484054565 - trainLoss: 0.4481927156448364\n",
      "cnt: 0 - valLoss: 0.44462868571281433 - trainLoss: 0.4481896162033081\n",
      "cnt: 0 - valLoss: 0.44462719559669495 - trainLoss: 0.4481866657733917\n",
      "cnt: 0 - valLoss: 0.44462546706199646 - trainLoss: 0.4481836259365082\n",
      "cnt: 0 - valLoss: 0.4446238875389099 - trainLoss: 0.4481806457042694\n",
      "cnt: 0 - valLoss: 0.4446223974227905 - trainLoss: 0.44817763566970825\n",
      "cnt: 0 - valLoss: 0.44462063908576965 - trainLoss: 0.44817468523979187\n",
      "cnt: 0 - valLoss: 0.4446190893650055 - trainLoss: 0.4481716454029083\n",
      "cnt: 0 - valLoss: 0.44461753964424133 - trainLoss: 0.44816866517066956\n",
      "cnt: 0 - valLoss: 0.44461581110954285 - trainLoss: 0.4481656551361084\n",
      "cnt: 0 - valLoss: 0.4446142911911011 - trainLoss: 0.44816267490386963\n",
      "cnt: 0 - valLoss: 0.4446127414703369 - trainLoss: 0.44815966486930847\n",
      "cnt: 0 - valLoss: 0.44461095333099365 - trainLoss: 0.4481566846370697\n",
      "cnt: 0 - valLoss: 0.44460931420326233 - trainLoss: 0.44815367460250854\n",
      "cnt: 0 - valLoss: 0.44460776448249817 - trainLoss: 0.4481506645679474\n",
      "cnt: 0 - valLoss: 0.44460591673851013 - trainLoss: 0.448147714138031\n",
      "cnt: 0 - valLoss: 0.4446043372154236 - trainLoss: 0.44814473390579224\n",
      "cnt: 0 - valLoss: 0.44460275769233704 - trainLoss: 0.4481417238712311\n",
      "cnt: 0 - valLoss: 0.44460099935531616 - trainLoss: 0.4481387734413147\n",
      "cnt: 0 - valLoss: 0.44459936022758484 - trainLoss: 0.4481357932090759\n",
      "cnt: 0 - valLoss: 0.4445977807044983 - trainLoss: 0.44813278317451477\n",
      "cnt: 0 - valLoss: 0.4445960223674774 - trainLoss: 0.448129802942276\n",
      "cnt: 0 - valLoss: 0.44459444284439087 - trainLoss: 0.44812682271003723\n",
      "cnt: 0 - valLoss: 0.44459280371665955 - trainLoss: 0.44812384247779846\n",
      "cnt: 0 - valLoss: 0.44459113478660583 - trainLoss: 0.4481208920478821\n",
      "cnt: 0 - valLoss: 0.4445894956588745 - trainLoss: 0.4481178820133209\n",
      "cnt: 0 - valLoss: 0.4445878863334656 - trainLoss: 0.44811490178108215\n",
      "cnt: 0 - valLoss: 0.44458630681037903 - trainLoss: 0.44811195135116577\n",
      "cnt: 0 - valLoss: 0.44458457827568054 - trainLoss: 0.4481089413166046\n",
      "cnt: 0 - valLoss: 0.444582998752594 - trainLoss: 0.44810599088668823\n",
      "cnt: 0 - valLoss: 0.44458135962486267 - trainLoss: 0.44810301065444946\n",
      "cnt: 0 - valLoss: 0.4445796608924866 - trainLoss: 0.4481000006198883\n",
      "cnt: 0 - valLoss: 0.44457802176475525 - trainLoss: 0.4480970501899719\n",
      "cnt: 0 - valLoss: 0.4445764720439911 - trainLoss: 0.44809409976005554\n",
      "cnt: 0 - valLoss: 0.444574773311615 - trainLoss: 0.4480910897254944\n",
      "cnt: 0 - valLoss: 0.4445730447769165 - trainLoss: 0.4480881094932556\n",
      "cnt: 0 - valLoss: 0.44457149505615234 - trainLoss: 0.44808509945869446\n",
      "cnt: 0 - valLoss: 0.44456979632377625 - trainLoss: 0.4480821192264557\n",
      "cnt: 0 - valLoss: 0.4445682168006897 - trainLoss: 0.4480791985988617\n",
      "cnt: 0 - valLoss: 0.44456660747528076 - trainLoss: 0.44807618856430054\n",
      "cnt: 0 - valLoss: 0.44456490874290466 - trainLoss: 0.44807320833206177\n",
      "cnt: 0 - valLoss: 0.44456326961517334 - trainLoss: 0.4480702579021454\n",
      "cnt: 0 - valLoss: 0.4445616900920868 - trainLoss: 0.44806724786758423\n",
      "cnt: 0 - valLoss: 0.4445599317550659 - trainLoss: 0.44806426763534546\n",
      "cnt: 0 - valLoss: 0.44455835223197937 - trainLoss: 0.4480613172054291\n",
      "cnt: 0 - valLoss: 0.4445567727088928 - trainLoss: 0.4480583071708679\n",
      "cnt: 0 - valLoss: 0.4445551335811615 - trainLoss: 0.44805532693862915\n",
      "cnt: 0 - valLoss: 0.4445533752441406 - trainLoss: 0.44805237650871277\n",
      "cnt: 0 - valLoss: 0.4445517957210541 - trainLoss: 0.448049396276474\n",
      "cnt: 0 - valLoss: 0.4445502460002899 - trainLoss: 0.44804641604423523\n",
      "cnt: 0 - valLoss: 0.44454848766326904 - trainLoss: 0.4480434060096741\n",
      "cnt: 0 - valLoss: 0.4445469081401825 - trainLoss: 0.4480404257774353\n",
      "cnt: 0 - valLoss: 0.44454532861709595 - trainLoss: 0.4480375051498413\n",
      "cnt: 0 - valLoss: 0.44454360008239746 - trainLoss: 0.44803452491760254\n",
      "cnt: 0 - valLoss: 0.4445420205593109 - trainLoss: 0.4480315148830414\n",
      "cnt: 0 - valLoss: 0.4445403814315796 - trainLoss: 0.4480285346508026\n",
      "cnt: 0 - valLoss: 0.4445386826992035 - trainLoss: 0.44802555441856384\n",
      "cnt: 0 - valLoss: 0.44453707337379456 - trainLoss: 0.44802260398864746\n",
      "cnt: 0 - valLoss: 0.444535493850708 - trainLoss: 0.4480196237564087\n",
      "cnt: 0 - valLoss: 0.44453370571136475 - trainLoss: 0.44801661372184753\n",
      "cnt: 0 - valLoss: 0.4445321261882782 - trainLoss: 0.4480137228965759\n",
      "cnt: 0 - valLoss: 0.4445304572582245 - trainLoss: 0.44801071286201477\n",
      "cnt: 0 - valLoss: 0.4445289075374603 - trainLoss: 0.448007732629776\n",
      "cnt: 0 - valLoss: 0.44452714920043945 - trainLoss: 0.44800475239753723\n",
      "cnt: 0 - valLoss: 0.44452548027038574 - trainLoss: 0.44800177216529846\n",
      "cnt: 0 - valLoss: 0.4445239305496216 - trainLoss: 0.4479988217353821\n",
      "cnt: 0 - valLoss: 0.4445221424102783 - trainLoss: 0.4479958117008209\n",
      "cnt: 0 - valLoss: 0.4445205330848694 - trainLoss: 0.44799283146858215\n",
      "cnt: 0 - valLoss: 0.44451892375946045 - trainLoss: 0.44798988103866577\n",
      "cnt: 0 - valLoss: 0.44451719522476196 - trainLoss: 0.4479869306087494\n",
      "cnt: 0 - valLoss: 0.444515585899353 - trainLoss: 0.44798392057418823\n",
      "cnt: 0 - valLoss: 0.4445139765739441 - trainLoss: 0.44798097014427185\n",
      "cnt: 0 - valLoss: 0.4445122182369232 - trainLoss: 0.44797801971435547\n",
      "cnt: 0 - valLoss: 0.4445106089115143 - trainLoss: 0.4479750096797943\n",
      "cnt: 0 - valLoss: 0.44450899958610535 - trainLoss: 0.44797205924987793\n",
      "cnt: 0 - valLoss: 0.4445072114467621 - trainLoss: 0.44796910881996155\n",
      "cnt: 0 - valLoss: 0.44450557231903076 - trainLoss: 0.4479661285877228\n",
      "cnt: 0 - valLoss: 0.44450390338897705 - trainLoss: 0.4479631185531616\n",
      "cnt: 0 - valLoss: 0.4445021152496338 - trainLoss: 0.44796016812324524\n",
      "cnt: 0 - valLoss: 0.4445004463195801 - trainLoss: 0.44795721769332886\n",
      "cnt: 0 - valLoss: 0.44449880719184875 - trainLoss: 0.4479542672634125\n",
      "cnt: 0 - valLoss: 0.4444971978664398 - trainLoss: 0.4479513168334961\n",
      "cnt: 0 - valLoss: 0.44449537992477417 - trainLoss: 0.44794830679893494\n",
      "cnt: 0 - valLoss: 0.44449374079704285 - trainLoss: 0.44794538617134094\n",
      "cnt: 0 - valLoss: 0.44449207186698914 - trainLoss: 0.4479424059391022\n",
      "cnt: 0 - valLoss: 0.44449031352996826 - trainLoss: 0.447939395904541\n",
      "cnt: 0 - valLoss: 0.4444887042045593 - trainLoss: 0.4479365050792694\n",
      "cnt: 0 - valLoss: 0.444487065076828 - trainLoss: 0.44793349504470825\n",
      "cnt: 0 - valLoss: 0.44448527693748474 - trainLoss: 0.4479305148124695\n",
      "cnt: 0 - valLoss: 0.4444836378097534 - trainLoss: 0.4479275643825531\n",
      "cnt: 0 - valLoss: 0.44448205828666687 - trainLoss: 0.4479246139526367\n",
      "cnt: 0 - valLoss: 0.44448021054267883 - trainLoss: 0.44792163372039795\n",
      "cnt: 0 - valLoss: 0.4444785714149475 - trainLoss: 0.44791868329048157\n",
      "cnt: 0 - valLoss: 0.4444769620895386 - trainLoss: 0.4479157030582428\n",
      "cnt: 0 - valLoss: 0.4444752335548401 - trainLoss: 0.4479127526283264\n",
      "cnt: 0 - valLoss: 0.44447362422943115 - trainLoss: 0.44790980219841003\n",
      "cnt: 0 - valLoss: 0.44447198510169983 - trainLoss: 0.4479067921638489\n",
      "cnt: 0 - valLoss: 0.4444701671600342 - trainLoss: 0.4479038715362549\n",
      "cnt: 0 - valLoss: 0.44446855783462524 - trainLoss: 0.4479008913040161\n",
      "cnt: 0 - valLoss: 0.4444669187068939 - trainLoss: 0.44789794087409973\n",
      "cnt: 0 - valLoss: 0.4444652795791626 - trainLoss: 0.44789499044418335\n",
      "cnt: 0 - valLoss: 0.4444635510444641 - trainLoss: 0.44789204001426697\n",
      "cnt: 0 - valLoss: 0.444461852312088 - trainLoss: 0.4478890597820282\n",
      "cnt: 0 - valLoss: 0.4444602429866791 - trainLoss: 0.44788607954978943\n",
      "cnt: 0 - valLoss: 0.4444584846496582 - trainLoss: 0.44788312911987305\n",
      "cnt: 0 - valLoss: 0.44445690512657166 - trainLoss: 0.44788017868995667\n",
      "cnt: 0 - valLoss: 0.44445523619651794 - trainLoss: 0.4478771686553955\n",
      "cnt: 0 - valLoss: 0.44445347785949707 - trainLoss: 0.4478742778301239\n",
      "cnt: 0 - valLoss: 0.44445183873176575 - trainLoss: 0.44787126779556274\n",
      "cnt: 0 - valLoss: 0.4444501996040344 - trainLoss: 0.44786831736564636\n",
      "cnt: 0 - valLoss: 0.44444844126701355 - trainLoss: 0.44786536693573\n",
      "cnt: 0 - valLoss: 0.444446861743927 - trainLoss: 0.4478623569011688\n",
      "cnt: 0 - valLoss: 0.4444452226161957 - trainLoss: 0.4478594660758972\n",
      "cnt: 0 - valLoss: 0.44444340467453003 - trainLoss: 0.44785645604133606\n",
      "cnt: 0 - valLoss: 0.4444417655467987 - trainLoss: 0.4478535056114197\n",
      "cnt: 0 - valLoss: 0.444440096616745 - trainLoss: 0.4478505551815033\n",
      "cnt: 0 - valLoss: 0.4444384276866913 - trainLoss: 0.4478476047515869\n",
      "cnt: 0 - valLoss: 0.44443660974502563 - trainLoss: 0.44784465432167053\n",
      "cnt: 0 - valLoss: 0.4444349408149719 - trainLoss: 0.44784170389175415\n",
      "cnt: 0 - valLoss: 0.4444332420825958 - trainLoss: 0.44783878326416016\n",
      "cnt: 0 - valLoss: 0.4444314241409302 - trainLoss: 0.4478358328342438\n",
      "cnt: 0 - valLoss: 0.44442981481552124 - trainLoss: 0.4478328824043274\n",
      "cnt: 0 - valLoss: 0.44442808628082275 - trainLoss: 0.447829931974411\n",
      "cnt: 0 - valLoss: 0.4444263279438019 - trainLoss: 0.44782698154449463\n",
      "cnt: 0 - valLoss: 0.44442465901374817 - trainLoss: 0.44782400131225586\n",
      "cnt: 0 - valLoss: 0.44442296028137207 - trainLoss: 0.4478210508823395\n",
      "cnt: 0 - valLoss: 0.44442132115364075 - trainLoss: 0.4478181004524231\n",
      "cnt: 0 - valLoss: 0.4444195032119751 - trainLoss: 0.4478152096271515\n",
      "cnt: 0 - valLoss: 0.444417804479599 - trainLoss: 0.4478122591972351\n",
      "cnt: 0 - valLoss: 0.4444161653518677 - trainLoss: 0.44780927896499634\n",
      "cnt: 0 - valLoss: 0.4444143772125244 - trainLoss: 0.44780638813972473\n",
      "cnt: 0 - valLoss: 0.4444127082824707 - trainLoss: 0.4478033781051636\n",
      "cnt: 0 - valLoss: 0.4444110691547394 - trainLoss: 0.44780048727989197\n",
      "cnt: 0 - valLoss: 0.4444092810153961 - trainLoss: 0.4477975070476532\n",
      "cnt: 0 - valLoss: 0.4444076120853424 - trainLoss: 0.4477945566177368\n",
      "cnt: 0 - valLoss: 0.4444059729576111 - trainLoss: 0.44779160618782043\n",
      "cnt: 0 - valLoss: 0.4444043040275574 - trainLoss: 0.44778865575790405\n",
      "cnt: 0 - valLoss: 0.44440245628356934 - trainLoss: 0.44778576493263245\n",
      "cnt: 0 - valLoss: 0.444400817155838 - trainLoss: 0.4477827548980713\n",
      "cnt: 0 - valLoss: 0.4443992078304291 - trainLoss: 0.4477798342704773\n",
      "cnt: 0 - valLoss: 0.4443973898887634 - trainLoss: 0.4477768838405609\n",
      "cnt: 0 - valLoss: 0.4443957805633545 - trainLoss: 0.44777393341064453\n",
      "cnt: 0 - valLoss: 0.44439414143562317 - trainLoss: 0.44777101278305054\n",
      "cnt: 0 - valLoss: 0.44439244270324707 - trainLoss: 0.44776806235313416\n",
      "cnt: 0 - valLoss: 0.4443906843662262 - trainLoss: 0.4477651119232178\n",
      "cnt: 0 - valLoss: 0.4443889856338501 - trainLoss: 0.44776222109794617\n",
      "cnt: 0 - valLoss: 0.44438737630844116 - trainLoss: 0.4477592706680298\n",
      "cnt: 0 - valLoss: 0.4443855583667755 - trainLoss: 0.447756290435791\n",
      "cnt: 0 - valLoss: 0.4443839490413666 - trainLoss: 0.4477533996105194\n",
      "cnt: 0 - valLoss: 0.44438230991363525 - trainLoss: 0.44775038957595825\n",
      "cnt: 0 - valLoss: 0.4443804621696472 - trainLoss: 0.44774749875068665\n",
      "cnt: 0 - valLoss: 0.4443788528442383 - trainLoss: 0.44774457812309265\n",
      "cnt: 0 - valLoss: 0.44437718391418457 - trainLoss: 0.4477415680885315\n",
      "cnt: 0 - valLoss: 0.44437554478645325 - trainLoss: 0.4477386176586151\n",
      "cnt: 0 - valLoss: 0.4443737268447876 - trainLoss: 0.4477357268333435\n",
      "cnt: 0 - valLoss: 0.44437211751937866 - trainLoss: 0.4477327764034271\n",
      "cnt: 0 - valLoss: 0.44437044858932495 - trainLoss: 0.44772979617118835\n",
      "cnt: 0 - valLoss: 0.4443686902523041 - trainLoss: 0.44772690534591675\n",
      "cnt: 0 - valLoss: 0.44436702132225037 - trainLoss: 0.44772395491600037\n",
      "cnt: 0 - valLoss: 0.44436535239219666 - trainLoss: 0.447721004486084\n",
      "cnt: 0 - valLoss: 0.4443635940551758 - trainLoss: 0.4477180242538452\n",
      "cnt: 0 - valLoss: 0.44436192512512207 - trainLoss: 0.4477151334285736\n",
      "cnt: 0 - valLoss: 0.44436028599739075 - trainLoss: 0.4477121829986572\n",
      "cnt: 0 - valLoss: 0.4443586468696594 - trainLoss: 0.44770923256874084\n",
      "cnt: 0 - valLoss: 0.44435688853263855 - trainLoss: 0.44770631194114685\n",
      "cnt: 0 - valLoss: 0.44435521960258484 - trainLoss: 0.44770339131355286\n",
      "cnt: 0 - valLoss: 0.4443536102771759 - trainLoss: 0.4477004110813141\n",
      "cnt: 0 - valLoss: 0.44435185194015503 - trainLoss: 0.4476974606513977\n",
      "cnt: 0 - valLoss: 0.44435015320777893 - trainLoss: 0.4476945400238037\n",
      "cnt: 0 - valLoss: 0.44434854388237 - trainLoss: 0.44769158959388733\n",
      "cnt: 0 - valLoss: 0.4443467855453491 - trainLoss: 0.44768863916397095\n",
      "cnt: 0 - valLoss: 0.4443451464176178 - trainLoss: 0.44768571853637695\n",
      "cnt: 0 - valLoss: 0.44434353709220886 - trainLoss: 0.44768279790878296\n",
      "cnt: 0 - valLoss: 0.44434189796447754 - trainLoss: 0.4476798474788666\n",
      "cnt: 0 - valLoss: 0.44434013962745667 - trainLoss: 0.4476768970489502\n",
      "cnt: 0 - valLoss: 0.44433850049972534 - trainLoss: 0.4476739466190338\n",
      "cnt: 0 - valLoss: 0.4443368911743164 - trainLoss: 0.4476710557937622\n",
      "cnt: 0 - valLoss: 0.44433513283729553 - trainLoss: 0.4476681351661682\n",
      "cnt: 0 - valLoss: 0.4443334937095642 - trainLoss: 0.44766518473625183\n",
      "cnt: 0 - valLoss: 0.4443318545818329 - trainLoss: 0.44766223430633545\n",
      "cnt: 0 - valLoss: 0.444330096244812 - trainLoss: 0.44765931367874146\n",
      "cnt: 0 - valLoss: 0.4443284869194031 - trainLoss: 0.4476563632488251\n",
      "cnt: 0 - valLoss: 0.44432684779167175 - trainLoss: 0.4476534128189087\n",
      "cnt: 0 - valLoss: 0.44432520866394043 - trainLoss: 0.4476504921913147\n",
      "cnt: 0 - valLoss: 0.44432345032691956 - trainLoss: 0.4476475417613983\n",
      "cnt: 0 - valLoss: 0.4443218410015106 - trainLoss: 0.4476446211338043\n",
      "cnt: 0 - valLoss: 0.4443202614784241 - trainLoss: 0.44764167070388794\n",
      "cnt: 0 - valLoss: 0.4443184435367584 - trainLoss: 0.44763877987861633\n",
      "cnt: 0 - valLoss: 0.4443168342113495 - trainLoss: 0.44763582944869995\n",
      "cnt: 0 - valLoss: 0.4443152844905853 - trainLoss: 0.4476328492164612\n",
      "cnt: 0 - valLoss: 0.4443134665489197 - trainLoss: 0.4476299583911896\n",
      "cnt: 0 - valLoss: 0.44431185722351074 - trainLoss: 0.4476270079612732\n",
      "cnt: 0 - valLoss: 0.4443102180957794 - trainLoss: 0.4476240873336792\n",
      "cnt: 0 - valLoss: 0.4443085789680481 - trainLoss: 0.4476211369037628\n",
      "cnt: 0 - valLoss: 0.4443068206310272 - trainLoss: 0.4476182162761688\n",
      "cnt: 0 - valLoss: 0.4443052411079407 - trainLoss: 0.44761526584625244\n",
      "cnt: 0 - valLoss: 0.44430357217788696 - trainLoss: 0.44761231541633606\n",
      "cnt: 0 - valLoss: 0.4443018138408661 - trainLoss: 0.44760939478874207\n",
      "cnt: 0 - valLoss: 0.44430023431777954 - trainLoss: 0.44760650396347046\n",
      "cnt: 0 - valLoss: 0.4442985951900482 - trainLoss: 0.44760358333587646\n",
      "cnt: 0 - valLoss: 0.4442969858646393 - trainLoss: 0.4476006031036377\n",
      "cnt: 0 - valLoss: 0.4442952275276184 - trainLoss: 0.4475976824760437\n",
      "cnt: 0 - valLoss: 0.4442936182022095 - trainLoss: 0.4475947320461273\n",
      "cnt: 0 - valLoss: 0.44429200887680054 - trainLoss: 0.44759178161621094\n",
      "cnt: 0 - valLoss: 0.44429025053977966 - trainLoss: 0.44758886098861694\n",
      "cnt: 0 - valLoss: 0.4442887008190155 - trainLoss: 0.44758594036102295\n",
      "cnt: 0 - valLoss: 0.4442870020866394 - trainLoss: 0.44758298993110657\n",
      "cnt: 0 - valLoss: 0.4442852735519409 - trainLoss: 0.4475800395011902\n",
      "cnt: 0 - valLoss: 0.44428369402885437 - trainLoss: 0.4475771486759186\n",
      "cnt: 0 - valLoss: 0.44428205490112305 - trainLoss: 0.4475742280483246\n",
      "cnt: 0 - valLoss: 0.4442804455757141 - trainLoss: 0.4475712180137634\n",
      "cnt: 0 - valLoss: 0.44427868723869324 - trainLoss: 0.4475683271884918\n",
      "cnt: 0 - valLoss: 0.4442770481109619 - trainLoss: 0.4475654065608978\n",
      "cnt: 0 - valLoss: 0.44427549839019775 - trainLoss: 0.44756248593330383\n",
      "cnt: 0 - valLoss: 0.44427379965782166 - trainLoss: 0.44755953550338745\n",
      "cnt: 0 - valLoss: 0.44427213072776794 - trainLoss: 0.44755658507347107\n",
      "cnt: 0 - valLoss: 0.444270521402359 - trainLoss: 0.44755369424819946\n",
      "cnt: 0 - valLoss: 0.4442688226699829 - trainLoss: 0.44755077362060547\n",
      "cnt: 0 - valLoss: 0.4442671835422516 - trainLoss: 0.4475478529930115\n",
      "cnt: 0 - valLoss: 0.44426560401916504 - trainLoss: 0.4475448727607727\n",
      "cnt: 0 - valLoss: 0.4442640244960785 - trainLoss: 0.4475419819355011\n",
      "cnt: 0 - valLoss: 0.44426220655441284 - trainLoss: 0.4475390315055847\n",
      "cnt: 0 - valLoss: 0.44426068663597107 - trainLoss: 0.4475361406803131\n",
      "cnt: 0 - valLoss: 0.44425907731056213 - trainLoss: 0.44753319025039673\n",
      "cnt: 0 - valLoss: 0.44425731897354126 - trainLoss: 0.44753026962280273\n",
      "cnt: 0 - valLoss: 0.4442557096481323 - trainLoss: 0.44752731919288635\n",
      "cnt: 0 - valLoss: 0.44425415992736816 - trainLoss: 0.44752439856529236\n",
      "cnt: 0 - valLoss: 0.4442524313926697 - trainLoss: 0.44752150774002075\n",
      "cnt: 0 - valLoss: 0.44425082206726074 - trainLoss: 0.44751858711242676\n",
      "cnt: 0 - valLoss: 0.4442492425441742 - trainLoss: 0.4475156366825104\n",
      "cnt: 0 - valLoss: 0.44424763321876526 - trainLoss: 0.4475127160549164\n",
      "cnt: 0 - valLoss: 0.4442458748817444 - trainLoss: 0.447509765625\n",
      "cnt: 0 - valLoss: 0.44424429535865784 - trainLoss: 0.447506844997406\n",
      "cnt: 0 - valLoss: 0.4442426562309265 - trainLoss: 0.4475038945674896\n",
      "cnt: 0 - valLoss: 0.4442409574985504 - trainLoss: 0.447501003742218\n",
      "cnt: 0 - valLoss: 0.44423937797546387 - trainLoss: 0.44749802350997925\n",
      "cnt: 0 - valLoss: 0.4442377984523773 - trainLoss: 0.44749513268470764\n",
      "cnt: 0 - valLoss: 0.44423604011535645 - trainLoss: 0.44749221205711365\n",
      "cnt: 0 - valLoss: 0.4442344307899475 - trainLoss: 0.44748926162719727\n",
      "cnt: 0 - valLoss: 0.4442328214645386 - trainLoss: 0.44748634099960327\n",
      "cnt: 0 - valLoss: 0.44423118233680725 - trainLoss: 0.4474833905696869\n",
      "cnt: 0 - valLoss: 0.44422948360443115 - trainLoss: 0.4474804997444153\n",
      "cnt: 0 - valLoss: 0.44422784447669983 - trainLoss: 0.4474775791168213\n",
      "cnt: 0 - valLoss: 0.4442262053489685 - trainLoss: 0.4474746584892273\n",
      "cnt: 0 - valLoss: 0.4442245066165924 - trainLoss: 0.4474717676639557\n",
      "cnt: 0 - valLoss: 0.4442228674888611 - trainLoss: 0.4474688172340393\n",
      "cnt: 0 - valLoss: 0.44422122836112976 - trainLoss: 0.44746583700180054\n",
      "cnt: 0 - valLoss: 0.4442194700241089 - trainLoss: 0.44746294617652893\n",
      "cnt: 0 - valLoss: 0.44421789050102234 - trainLoss: 0.44746002554893494\n",
      "cnt: 0 - valLoss: 0.4442162811756134 - trainLoss: 0.44745707511901855\n",
      "cnt: 0 - valLoss: 0.44421467185020447 - trainLoss: 0.44745415449142456\n",
      "cnt: 0 - valLoss: 0.4442129135131836 - trainLoss: 0.44745126366615295\n",
      "cnt: 0 - valLoss: 0.44421130418777466 - trainLoss: 0.4474483132362366\n",
      "cnt: 0 - valLoss: 0.4442097544670105 - trainLoss: 0.4474453926086426\n",
      "cnt: 0 - valLoss: 0.4442079961299896 - trainLoss: 0.4474424719810486\n",
      "cnt: 0 - valLoss: 0.4442063271999359 - trainLoss: 0.447439581155777\n",
      "cnt: 0 - valLoss: 0.44420474767684937 - trainLoss: 0.4474365711212158\n",
      "cnt: 0 - valLoss: 0.44420307874679565 - trainLoss: 0.4474336504936218\n",
      "cnt: 0 - valLoss: 0.44420143961906433 - trainLoss: 0.4474307894706726\n",
      "cnt: 0 - valLoss: 0.444199800491333 - trainLoss: 0.4474278390407562\n",
      "cnt: 0 - valLoss: 0.4441981017589569 - trainLoss: 0.44742491841316223\n",
      "cnt: 0 - valLoss: 0.4441964626312256 - trainLoss: 0.4474220275878906\n",
      "cnt: 0 - valLoss: 0.4441949129104614 - trainLoss: 0.44741907715797424\n",
      "cnt: 0 - valLoss: 0.4441933333873749 - trainLoss: 0.44741615653038025\n",
      "cnt: 0 - valLoss: 0.444191575050354 - trainLoss: 0.44741320610046387\n",
      "cnt: 0 - valLoss: 0.44418999552726746 - trainLoss: 0.4474102854728699\n",
      "cnt: 0 - valLoss: 0.4441884160041809 - trainLoss: 0.44740739464759827\n",
      "cnt: 0 - valLoss: 0.4441866874694824 - trainLoss: 0.4474044740200043\n",
      "cnt: 0 - valLoss: 0.44418513774871826 - trainLoss: 0.4474015533924103\n",
      "cnt: 0 - valLoss: 0.4441835284233093 - trainLoss: 0.44739866256713867\n",
      "cnt: 0 - valLoss: 0.44418179988861084 - trainLoss: 0.4473957121372223\n",
      "cnt: 0 - valLoss: 0.4441801905632019 - trainLoss: 0.4473927915096283\n",
      "cnt: 0 - valLoss: 0.44417864084243774 - trainLoss: 0.4473899304866791\n",
      "cnt: 0 - valLoss: 0.44417691230773926 - trainLoss: 0.4473869204521179\n",
      "cnt: 0 - valLoss: 0.4441753029823303 - trainLoss: 0.4473840594291687\n",
      "cnt: 0 - valLoss: 0.4441736936569214 - trainLoss: 0.4473811388015747\n",
      "cnt: 0 - valLoss: 0.44417211413383484 - trainLoss: 0.4473781883716583\n",
      "cnt: 0 - valLoss: 0.44417041540145874 - trainLoss: 0.4473752975463867\n",
      "cnt: 0 - valLoss: 0.4441688060760498 - trainLoss: 0.44737231731414795\n",
      "cnt: 0 - valLoss: 0.44416722655296326 - trainLoss: 0.44736942648887634\n",
      "cnt: 0 - valLoss: 0.44416552782058716 - trainLoss: 0.44736650586128235\n",
      "cnt: 0 - valLoss: 0.44416388869285583 - trainLoss: 0.44736364483833313\n",
      "cnt: 0 - valLoss: 0.4441623389720917 - trainLoss: 0.44736069440841675\n",
      "cnt: 0 - valLoss: 0.4441606402397156 - trainLoss: 0.44735783338546753\n",
      "cnt: 0 - valLoss: 0.44415906071662903 - trainLoss: 0.44735485315322876\n",
      "cnt: 0 - valLoss: 0.4441574513912201 - trainLoss: 0.4473519027233124\n",
      "cnt: 0 - valLoss: 0.444155752658844 - trainLoss: 0.44734901189804077\n",
      "cnt: 0 - valLoss: 0.44415420293807983 - trainLoss: 0.4473460912704468\n",
      "cnt: 0 - valLoss: 0.4441525638103485 - trainLoss: 0.4473431706428528\n",
      "cnt: 0 - valLoss: 0.44415098428726196 - trainLoss: 0.4473402798175812\n",
      "cnt: 0 - valLoss: 0.44414928555488586 - trainLoss: 0.4473373591899872\n",
      "cnt: 0 - valLoss: 0.44414767622947693 - trainLoss: 0.4473344087600708\n",
      "cnt: 0 - valLoss: 0.4441460967063904 - trainLoss: 0.4473314881324768\n",
      "cnt: 0 - valLoss: 0.4441443979740143 - trainLoss: 0.4473285377025604\n",
      "cnt: 0 - valLoss: 0.44414278864860535 - trainLoss: 0.4473256766796112\n",
      "cnt: 0 - valLoss: 0.4441412687301636 - trainLoss: 0.4473227262496948\n",
      "cnt: 0 - valLoss: 0.4441395103931427 - trainLoss: 0.4473198652267456\n",
      "cnt: 0 - valLoss: 0.44413793087005615 - trainLoss: 0.4473169147968292\n",
      "cnt: 0 - valLoss: 0.444136381149292 - trainLoss: 0.44731399416923523\n",
      "cnt: 0 - valLoss: 0.4441346526145935 - trainLoss: 0.44731107354164124\n",
      "cnt: 0 - valLoss: 0.44413310289382935 - trainLoss: 0.44730818271636963\n",
      "cnt: 0 - valLoss: 0.4441314935684204 - trainLoss: 0.44730526208877563\n",
      "cnt: 0 - valLoss: 0.44412994384765625 - trainLoss: 0.44730234146118164\n",
      "cnt: 0 - valLoss: 0.44412827491760254 - trainLoss: 0.44729939103126526\n",
      "cnt: 0 - valLoss: 0.444126695394516 - trainLoss: 0.44729653000831604\n",
      "cnt: 0 - valLoss: 0.44412508606910706 - trainLoss: 0.44729360938072205\n",
      "cnt: 0 - valLoss: 0.44412341713905334 - trainLoss: 0.44729071855545044\n",
      "cnt: 0 - valLoss: 0.4441218972206116 - trainLoss: 0.44728776812553406\n",
      "cnt: 0 - valLoss: 0.44412025809288025 - trainLoss: 0.44728487730026245\n",
      "cnt: 0 - valLoss: 0.44411858916282654 - trainLoss: 0.44728192687034607\n",
      "cnt: 0 - valLoss: 0.44411700963974 - trainLoss: 0.44727906584739685\n",
      "cnt: 0 - valLoss: 0.44411545991897583 - trainLoss: 0.44727611541748047\n",
      "cnt: 0 - valLoss: 0.44411373138427734 - trainLoss: 0.4472731947898865\n",
      "cnt: 0 - valLoss: 0.44411221146583557 - trainLoss: 0.44727030396461487\n",
      "cnt: 0 - valLoss: 0.44411060214042664 - trainLoss: 0.4472673237323761\n",
      "cnt: 0 - valLoss: 0.4441090226173401 - trainLoss: 0.4472644627094269\n",
      "cnt: 0 - valLoss: 0.44410741329193115 - trainLoss: 0.4472615718841553\n",
      "cnt: 0 - valLoss: 0.4441058039665222 - trainLoss: 0.4472586512565613\n",
      "cnt: 0 - valLoss: 0.44410422444343567 - trainLoss: 0.4472557306289673\n",
      "cnt: 0 - valLoss: 0.44410255551338196 - trainLoss: 0.4472528398036957\n",
      "cnt: 0 - valLoss: 0.4441009759902954 - trainLoss: 0.4472499191761017\n",
      "cnt: 0 - valLoss: 0.44409942626953125 - trainLoss: 0.4472469985485077\n",
      "cnt: 0 - valLoss: 0.44409769773483276 - trainLoss: 0.4472441077232361\n",
      "cnt: 0 - valLoss: 0.444096177816391 - trainLoss: 0.4472411870956421\n",
      "cnt: 0 - valLoss: 0.44409462809562683 - trainLoss: 0.4472382664680481\n",
      "cnt: 0 - valLoss: 0.44409289956092834 - trainLoss: 0.4472353756427765\n",
      "cnt: 0 - valLoss: 0.4440913796424866 - trainLoss: 0.4472324252128601\n",
      "cnt: 0 - valLoss: 0.4440898299217224 - trainLoss: 0.4472295045852661\n",
      "cnt: 0 - valLoss: 0.4440881013870239 - trainLoss: 0.4472266435623169\n",
      "cnt: 0 - valLoss: 0.44408658146858215 - trainLoss: 0.4472237527370453\n",
      "cnt: 0 - valLoss: 0.4440849721431732 - trainLoss: 0.4472208023071289\n",
      "cnt: 0 - valLoss: 0.44408348202705383 - trainLoss: 0.4472179114818573\n",
      "cnt: 0 - valLoss: 0.44408175349235535 - trainLoss: 0.4472149908542633\n",
      "cnt: 0 - valLoss: 0.4440801739692688 - trainLoss: 0.4472120404243469\n",
      "cnt: 0 - valLoss: 0.44407862424850464 - trainLoss: 0.4472091794013977\n",
      "cnt: 0 - valLoss: 0.4440769553184509 - trainLoss: 0.4472062587738037\n",
      "cnt: 0 - valLoss: 0.444075345993042 - trainLoss: 0.44720330834388733\n",
      "cnt: 0 - valLoss: 0.4440738260746002 - trainLoss: 0.44720038771629333\n",
      "cnt: 0 - valLoss: 0.4440721571445465 - trainLoss: 0.4471975266933441\n",
      "cnt: 0 - valLoss: 0.44407057762145996 - trainLoss: 0.4471946060657501\n",
      "cnt: 0 - valLoss: 0.4440690279006958 - trainLoss: 0.4471917152404785\n",
      "cnt: 0 - valLoss: 0.4440673589706421 - trainLoss: 0.44718876481056213\n",
      "cnt: 0 - valLoss: 0.44406580924987793 - trainLoss: 0.4471858739852905\n",
      "cnt: 0 - valLoss: 0.4440642297267914 - trainLoss: 0.4471829831600189\n",
      "cnt: 0 - valLoss: 0.4440626800060272 - trainLoss: 0.44718003273010254\n",
      "cnt: 0 - valLoss: 0.4440609812736511 - trainLoss: 0.44717714190483093\n",
      "cnt: 0 - valLoss: 0.44405949115753174 - trainLoss: 0.4471742510795593\n",
      "cnt: 0 - valLoss: 0.4440578818321228 - trainLoss: 0.44717130064964294\n",
      "cnt: 0 - valLoss: 0.4440562129020691 - trainLoss: 0.44716840982437134\n",
      "cnt: 0 - valLoss: 0.4440546929836273 - trainLoss: 0.44716551899909973\n",
      "cnt: 0 - valLoss: 0.44405314326286316 - trainLoss: 0.44716259837150574\n",
      "cnt: 0 - valLoss: 0.4440514147281647 - trainLoss: 0.44715967774391174\n",
      "cnt: 0 - valLoss: 0.4440498650074005 - trainLoss: 0.4471568167209625\n",
      "cnt: 0 - valLoss: 0.44404837489128113 - trainLoss: 0.4471539258956909\n",
      "cnt: 0 - valLoss: 0.4440467059612274 - trainLoss: 0.4471510052680969\n",
      "cnt: 0 - valLoss: 0.44404512643814087 - trainLoss: 0.44714808464050293\n",
      "cnt: 0 - valLoss: 0.4440436065196991 - trainLoss: 0.4471451938152313\n",
      "cnt: 0 - valLoss: 0.4440420866012573 - trainLoss: 0.44714227318763733\n",
      "cnt: 0 - valLoss: 0.44404035806655884 - trainLoss: 0.44713935256004333\n",
      "cnt: 0 - valLoss: 0.4440388083457947 - trainLoss: 0.44713646173477173\n",
      "cnt: 0 - valLoss: 0.4440372884273529 - trainLoss: 0.44713354110717773\n",
      "cnt: 0 - valLoss: 0.4440356492996216 - trainLoss: 0.44713062047958374\n",
      "cnt: 0 - valLoss: 0.4440340995788574 - trainLoss: 0.44712772965431213\n",
      "cnt: 0 - valLoss: 0.44403260946273804 - trainLoss: 0.4471248388290405\n",
      "cnt: 0 - valLoss: 0.4440309405326843 - trainLoss: 0.44712188839912415\n",
      "cnt: 0 - valLoss: 0.44402939081192017 - trainLoss: 0.44711899757385254\n",
      "cnt: 0 - valLoss: 0.444027841091156 - trainLoss: 0.44711610674858093\n",
      "cnt: 0 - valLoss: 0.4440261423587799 - trainLoss: 0.4471132159233093\n",
      "cnt: 0 - valLoss: 0.4440246820449829 - trainLoss: 0.4471103549003601\n",
      "cnt: 0 - valLoss: 0.44402313232421875 - trainLoss: 0.4471074342727661\n",
      "cnt: 0 - valLoss: 0.44402140378952026 - trainLoss: 0.4471045136451721\n",
      "cnt: 0 - valLoss: 0.4440199136734009 - trainLoss: 0.4471016228199005\n",
      "cnt: 0 - valLoss: 0.4440183639526367 - trainLoss: 0.4470987021923065\n",
      "cnt: 0 - valLoss: 0.44401687383651733 - trainLoss: 0.4470957815647125\n",
      "cnt: 0 - valLoss: 0.44401514530181885 - trainLoss: 0.4470928907394409\n",
      "cnt: 0 - valLoss: 0.4440136253833771 - trainLoss: 0.4470899999141693\n",
      "cnt: 0 - valLoss: 0.4440120756626129 - trainLoss: 0.44708704948425293\n",
      "cnt: 0 - valLoss: 0.4440104365348816 - trainLoss: 0.4470841586589813\n",
      "cnt: 0 - valLoss: 0.4440089166164398 - trainLoss: 0.4470812678337097\n",
      "cnt: 0 - valLoss: 0.44400736689567566 - trainLoss: 0.44707831740379333\n",
      "cnt: 0 - valLoss: 0.44400572776794434 - trainLoss: 0.4470754861831665\n",
      "cnt: 0 - valLoss: 0.4440041780471802 - trainLoss: 0.4470725953578949\n",
      "cnt: 0 - valLoss: 0.4440026581287384 - trainLoss: 0.4470696747303009\n",
      "cnt: 0 - valLoss: 0.4440009295940399 - trainLoss: 0.4470667541027069\n",
      "cnt: 0 - valLoss: 0.44399943947792053 - trainLoss: 0.4470638632774353\n",
      "cnt: 0 - valLoss: 0.44399791955947876 - trainLoss: 0.4470609426498413\n",
      "cnt: 0 - valLoss: 0.443996399641037 - trainLoss: 0.4470580816268921\n",
      "cnt: 0 - valLoss: 0.44399476051330566 - trainLoss: 0.4470551609992981\n",
      "cnt: 0 - valLoss: 0.4439932107925415 - trainLoss: 0.4470522999763489\n",
      "cnt: 0 - valLoss: 0.44399166107177734 - trainLoss: 0.4470493793487549\n",
      "cnt: 0 - valLoss: 0.4439900517463684 - trainLoss: 0.4470464289188385\n",
      "cnt: 0 - valLoss: 0.44398850202560425 - trainLoss: 0.4470435678958893\n",
      "cnt: 0 - valLoss: 0.44398701190948486 - trainLoss: 0.4470406472682953\n",
      "cnt: 0 - valLoss: 0.44398534297943115 - trainLoss: 0.4470377564430237\n",
      "cnt: 0 - valLoss: 0.4439838230609894 - trainLoss: 0.4470348358154297\n",
      "cnt: 0 - valLoss: 0.4439822733402252 - trainLoss: 0.4470319151878357\n",
      "cnt: 0 - valLoss: 0.4439806640148163 - trainLoss: 0.4470290243625641\n",
      "cnt: 0 - valLoss: 0.44397908449172974 - trainLoss: 0.44702616333961487\n",
      "cnt: 0 - valLoss: 0.44397756457328796 - trainLoss: 0.44702327251434326\n",
      "cnt: 0 - valLoss: 0.4439760744571686 - trainLoss: 0.4470203220844269\n",
      "cnt: 0 - valLoss: 0.44397443532943726 - trainLoss: 0.44701746106147766\n",
      "cnt: 0 - valLoss: 0.4439728856086731 - trainLoss: 0.44701454043388367\n",
      "cnt: 0 - valLoss: 0.4439713954925537 - trainLoss: 0.44701164960861206\n",
      "cnt: 0 - valLoss: 0.4439697563648224 - trainLoss: 0.44700872898101807\n",
      "cnt: 0 - valLoss: 0.4439682364463806 - trainLoss: 0.44700586795806885\n",
      "cnt: 0 - valLoss: 0.44396671652793884 - trainLoss: 0.44700294733047485\n",
      "cnt: 0 - valLoss: 0.4439651072025299 - trainLoss: 0.44700002670288086\n",
      "cnt: 0 - valLoss: 0.44396355748176575 - trainLoss: 0.44699716567993164\n",
      "cnt: 0 - valLoss: 0.44396206736564636 - trainLoss: 0.44699427485466003\n",
      "cnt: 0 - valLoss: 0.44396042823791504 - trainLoss: 0.44699135422706604\n",
      "cnt: 0 - valLoss: 0.44395893812179565 - trainLoss: 0.44698843359947205\n",
      "cnt: 0 - valLoss: 0.4439573287963867 - trainLoss: 0.4469855725765228\n",
      "cnt: 0 - valLoss: 0.4439557194709778 - trainLoss: 0.44698262214660645\n",
      "cnt: 0 - valLoss: 0.443954199552536 - trainLoss: 0.4469797909259796\n",
      "cnt: 0 - valLoss: 0.4439527094364166 - trainLoss: 0.4469768702983856\n",
      "cnt: 0 - valLoss: 0.44395121932029724 - trainLoss: 0.446973979473114\n",
      "cnt: 0 - valLoss: 0.4439495801925659 - trainLoss: 0.44697105884552\n",
      "cnt: 0 - valLoss: 0.44394803047180176 - trainLoss: 0.4469681978225708\n",
      "cnt: 0 - valLoss: 0.4439464807510376 - trainLoss: 0.4469652473926544\n",
      "cnt: 0 - valLoss: 0.44394490122795105 - trainLoss: 0.4469624161720276\n",
      "cnt: 0 - valLoss: 0.4439433813095093 - trainLoss: 0.4469594955444336\n",
      "cnt: 0 - valLoss: 0.4439418613910675 - trainLoss: 0.446956604719162\n",
      "cnt: 0 - valLoss: 0.44394025206565857 - trainLoss: 0.446953684091568\n",
      "cnt: 0 - valLoss: 0.4439387023448944 - trainLoss: 0.4469508230686188\n",
      "cnt: 0 - valLoss: 0.44393718242645264 - trainLoss: 0.4469478726387024\n",
      "cnt: 0 - valLoss: 0.4439356029033661 - trainLoss: 0.4469449520111084\n",
      "cnt: 0 - valLoss: 0.44393402338027954 - trainLoss: 0.44694212079048157\n",
      "cnt: 0 - valLoss: 0.44393253326416016 - trainLoss: 0.44693922996520996\n",
      "cnt: 0 - valLoss: 0.4439310133457184 - trainLoss: 0.44693633913993835\n",
      "cnt: 0 - valLoss: 0.44392940402030945 - trainLoss: 0.44693347811698914\n",
      "cnt: 0 - valLoss: 0.4439278841018677 - trainLoss: 0.44693058729171753\n",
      "cnt: 0 - valLoss: 0.4439263939857483 - trainLoss: 0.44692766666412354\n",
      "cnt: 0 - valLoss: 0.4439247250556946 - trainLoss: 0.44692474603652954\n",
      "cnt: 0 - valLoss: 0.4439232647418976 - trainLoss: 0.4469218850135803\n",
      "cnt: 0 - valLoss: 0.4439217746257782 - trainLoss: 0.44691896438598633\n",
      "cnt: 0 - valLoss: 0.4439201354980469 - trainLoss: 0.4469161033630371\n",
      "cnt: 0 - valLoss: 0.4439185857772827 - trainLoss: 0.4469131827354431\n",
      "cnt: 0 - valLoss: 0.44391709566116333 - trainLoss: 0.4469102919101715\n",
      "cnt: 0 - valLoss: 0.443915456533432 - trainLoss: 0.4469074308872223\n",
      "cnt: 0 - valLoss: 0.4439139664173126 - trainLoss: 0.4469045102596283\n",
      "cnt: 0 - valLoss: 0.44391247630119324 - trainLoss: 0.4469016492366791\n",
      "cnt: 0 - valLoss: 0.44391101598739624 - trainLoss: 0.4468987286090851\n",
      "cnt: 0 - valLoss: 0.44390934705734253 - trainLoss: 0.4468958079814911\n",
      "cnt: 0 - valLoss: 0.44390782713890076 - trainLoss: 0.4468929171562195\n",
      "cnt: 0 - valLoss: 0.44390633702278137 - trainLoss: 0.44689008593559265\n",
      "cnt: 0 - valLoss: 0.44390469789505005 - trainLoss: 0.44688716530799866\n",
      "cnt: 0 - valLoss: 0.44390320777893066 - trainLoss: 0.44688427448272705\n",
      "cnt: 0 - valLoss: 0.4439016580581665 - trainLoss: 0.44688135385513306\n",
      "cnt: 0 - valLoss: 0.4439000189304352 - trainLoss: 0.44687843322753906\n",
      "cnt: 0 - valLoss: 0.4438984990119934 - trainLoss: 0.44687557220458984\n",
      "cnt: 0 - valLoss: 0.4438970386981964 - trainLoss: 0.4468727111816406\n",
      "cnt: 0 - valLoss: 0.4438953697681427 - trainLoss: 0.44686979055404663\n",
      "cnt: 0 - valLoss: 0.4438938498497009 - trainLoss: 0.4468669295310974\n",
      "cnt: 0 - valLoss: 0.44389235973358154 - trainLoss: 0.4468640685081482\n",
      "cnt: 0 - valLoss: 0.44389086961746216 - trainLoss: 0.4468611180782318\n",
      "cnt: 0 - valLoss: 0.44388923048973083 - trainLoss: 0.4468582272529602\n",
      "cnt: 0 - valLoss: 0.44388774037361145 - trainLoss: 0.446855366230011\n",
      "cnt: 0 - valLoss: 0.4438861906528473 - trainLoss: 0.44685250520706177\n",
      "cnt: 0 - valLoss: 0.44388455152511597 - trainLoss: 0.4468495845794678\n",
      "cnt: 0 - valLoss: 0.4438830614089966 - trainLoss: 0.44684669375419617\n",
      "cnt: 0 - valLoss: 0.4438815116882324 - trainLoss: 0.44684386253356934\n",
      "cnt: 0 - valLoss: 0.4438799023628235 - trainLoss: 0.44684094190597534\n",
      "cnt: 0 - valLoss: 0.4438784420490265 - trainLoss: 0.44683805108070374\n",
      "cnt: 0 - valLoss: 0.4438769221305847 - trainLoss: 0.44683513045310974\n",
      "cnt: 0 - valLoss: 0.44387534260749817 - trainLoss: 0.4468322992324829\n",
      "cnt: 0 - valLoss: 0.4438737630844116 - trainLoss: 0.4468293786048889\n",
      "cnt: 0 - valLoss: 0.44387227296829224 - trainLoss: 0.4468264877796173\n",
      "cnt: 0 - valLoss: 0.4438707232475281 - trainLoss: 0.4468235671520233\n",
      "cnt: 0 - valLoss: 0.4438691735267639 - trainLoss: 0.4468207359313965\n",
      "cnt: 0 - valLoss: 0.44386762380599976 - trainLoss: 0.4468178451061249\n",
      "cnt: 0 - valLoss: 0.44386616349220276 - trainLoss: 0.4468149244785309\n",
      "cnt: 0 - valLoss: 0.44386449456214905 - trainLoss: 0.44681209325790405\n",
      "cnt: 0 - valLoss: 0.44386300444602966 - trainLoss: 0.44680920243263245\n",
      "cnt: 0 - valLoss: 0.44386154413223267 - trainLoss: 0.44680628180503845\n",
      "cnt: 0 - valLoss: 0.44385987520217896 - trainLoss: 0.44680342078208923\n",
      "cnt: 0 - valLoss: 0.44385838508605957 - trainLoss: 0.4468005299568176\n",
      "cnt: 0 - valLoss: 0.4438568651676178 - trainLoss: 0.446797639131546\n",
      "cnt: 0 - valLoss: 0.44385528564453125 - trainLoss: 0.4467947781085968\n",
      "cnt: 0 - valLoss: 0.44385379552841187 - trainLoss: 0.4467918872833252\n",
      "cnt: 0 - valLoss: 0.4438522458076477 - trainLoss: 0.4467889964580536\n",
      "cnt: 0 - valLoss: 0.4438506066799164 - trainLoss: 0.446786105632782\n",
      "cnt: 0 - valLoss: 0.44384920597076416 - trainLoss: 0.4467832148075104\n",
      "cnt: 0 - valLoss: 0.44384765625 - trainLoss: 0.44678035378456116\n",
      "cnt: 0 - valLoss: 0.443846195936203 - trainLoss: 0.44677746295928955\n",
      "cnt: 0 - valLoss: 0.4438445270061493 - trainLoss: 0.44677460193634033\n",
      "cnt: 0 - valLoss: 0.4438430368900299 - trainLoss: 0.44677168130874634\n",
      "cnt: 0 - valLoss: 0.4438415765762329 - trainLoss: 0.44676879048347473\n",
      "cnt: 0 - valLoss: 0.4438399076461792 - trainLoss: 0.4467659294605255\n",
      "cnt: 0 - valLoss: 0.4438383877277374 - trainLoss: 0.4467630386352539\n",
      "cnt: 0 - valLoss: 0.44383692741394043 - trainLoss: 0.4467601478099823\n",
      "cnt: 0 - valLoss: 0.4438353180885315 - trainLoss: 0.4467572569847107\n",
      "cnt: 0 - valLoss: 0.4438338577747345 - trainLoss: 0.4467543959617615\n",
      "cnt: 0 - valLoss: 0.44383230805397034 - trainLoss: 0.44675153493881226\n",
      "cnt: 0 - valLoss: 0.443830668926239 - trainLoss: 0.44674861431121826\n",
      "cnt: 0 - valLoss: 0.4438292384147644 - trainLoss: 0.44674575328826904\n",
      "cnt: 0 - valLoss: 0.44382768869400024 - trainLoss: 0.44674283266067505\n",
      "cnt: 0 - valLoss: 0.44382622838020325 - trainLoss: 0.44673997163772583\n",
      "cnt: 0 - valLoss: 0.4438246488571167 - trainLoss: 0.4467371106147766\n",
      "cnt: 0 - valLoss: 0.44382309913635254 - trainLoss: 0.4467341899871826\n",
      "cnt: 0 - valLoss: 0.44382160902023315 - trainLoss: 0.4467313885688782\n",
      "cnt: 0 - valLoss: 0.4438199996948242 - trainLoss: 0.4467284679412842\n",
      "cnt: 0 - valLoss: 0.44381850957870483 - trainLoss: 0.44672560691833496\n",
      "cnt: 0 - valLoss: 0.44381704926490784 - trainLoss: 0.44672268629074097\n",
      "cnt: 0 - valLoss: 0.4438153803348541 - trainLoss: 0.44671982526779175\n",
      "cnt: 0 - valLoss: 0.44381389021873474 - trainLoss: 0.44671690464019775\n",
      "cnt: 0 - valLoss: 0.44381242990493774 - trainLoss: 0.44671404361724854\n",
      "cnt: 0 - valLoss: 0.4438107907772064 - trainLoss: 0.4467111825942993\n",
      "cnt: 0 - valLoss: 0.44380930066108704 - trainLoss: 0.4467082917690277\n",
      "cnt: 0 - valLoss: 0.4438078701496124 - trainLoss: 0.4467054307460785\n",
      "cnt: 0 - valLoss: 0.44380635023117065 - trainLoss: 0.4467025399208069\n",
      "cnt: 0 - valLoss: 0.4438048005104065 - trainLoss: 0.4466996490955353\n",
      "cnt: 0 - valLoss: 0.44380325078964233 - trainLoss: 0.44669678807258606\n",
      "cnt: 0 - valLoss: 0.44380179047584534 - trainLoss: 0.44669386744499207\n",
      "cnt: 0 - valLoss: 0.4438002109527588 - trainLoss: 0.44669100642204285\n",
      "cnt: 0 - valLoss: 0.44379866123199463 - trainLoss: 0.44668814539909363\n",
      "cnt: 0 - valLoss: 0.44379720091819763 - trainLoss: 0.44668522477149963\n",
      "cnt: 0 - valLoss: 0.4437956213951111 - trainLoss: 0.4466824233531952\n",
      "cnt: 0 - valLoss: 0.4437941312789917 - trainLoss: 0.4466795027256012\n",
      "cnt: 0 - valLoss: 0.4437927007675171 - trainLoss: 0.446676641702652\n",
      "cnt: 0 - valLoss: 0.4437910318374634 - trainLoss: 0.44667375087738037\n",
      "cnt: 0 - valLoss: 0.443789541721344 - trainLoss: 0.44667086005210876\n",
      "cnt: 0 - valLoss: 0.4437881112098694 - trainLoss: 0.44666799902915955\n",
      "cnt: 0 - valLoss: 0.4437865912914276 - trainLoss: 0.44666510820388794\n",
      "cnt: 0 - valLoss: 0.4437849819660187 - trainLoss: 0.44666221737861633\n",
      "cnt: 0 - valLoss: 0.4437835216522217 - trainLoss: 0.4466593861579895\n",
      "cnt: 0 - valLoss: 0.4437820315361023 - trainLoss: 0.4466564655303955\n",
      "cnt: 0 - valLoss: 0.44378045201301575 - trainLoss: 0.4466536045074463\n",
      "cnt: 0 - valLoss: 0.44377899169921875 - trainLoss: 0.4466506838798523\n",
      "cnt: 0 - valLoss: 0.44377750158309937 - trainLoss: 0.4466478228569031\n",
      "cnt: 0 - valLoss: 0.44377589225769043 - trainLoss: 0.44664499163627625\n",
      "cnt: 0 - valLoss: 0.44377437233924866 - trainLoss: 0.44664210081100464\n",
      "cnt: 0 - valLoss: 0.44377291202545166 - trainLoss: 0.44663918018341064\n",
      "cnt: 0 - valLoss: 0.44377127289772034 - trainLoss: 0.4466363489627838\n",
      "cnt: 0 - valLoss: 0.4437697231769562 - trainLoss: 0.4466334581375122\n",
      "cnt: 0 - valLoss: 0.4437682032585144 - trainLoss: 0.4466306269168854\n",
      "cnt: 0 - valLoss: 0.44376665353775024 - trainLoss: 0.4466277062892914\n",
      "cnt: 0 - valLoss: 0.4437650144100189 - trainLoss: 0.44662487506866455\n",
      "cnt: 0 - valLoss: 0.4437634348869324 - trainLoss: 0.44662201404571533\n",
      "cnt: 0 - valLoss: 0.443761944770813 - trainLoss: 0.4466191530227661\n",
      "cnt: 0 - valLoss: 0.44376030564308167 - trainLoss: 0.4466162323951721\n",
      "cnt: 0 - valLoss: 0.4437588155269623 - trainLoss: 0.4466134011745453\n",
      "cnt: 0 - valLoss: 0.44375720620155334 - trainLoss: 0.44661054015159607\n",
      "cnt: 0 - valLoss: 0.4437555968761444 - trainLoss: 0.44660767912864685\n",
      "cnt: 0 - valLoss: 0.44375407695770264 - trainLoss: 0.44660481810569763\n",
      "cnt: 0 - valLoss: 0.44375255703926086 - trainLoss: 0.4466019570827484\n",
      "cnt: 0 - valLoss: 0.44375094771385193 - trainLoss: 0.4465990662574768\n",
      "cnt: 0 - valLoss: 0.44374942779541016 - trainLoss: 0.4465962052345276\n",
      "cnt: 0 - valLoss: 0.443747878074646 - trainLoss: 0.44659337401390076\n",
      "cnt: 0 - valLoss: 0.4437463879585266 - trainLoss: 0.44659048318862915\n",
      "cnt: 0 - valLoss: 0.4437447786331177 - trainLoss: 0.4465876519680023\n",
      "cnt: 0 - valLoss: 0.44374316930770874 - trainLoss: 0.4465847909450531\n",
      "cnt: 0 - valLoss: 0.44374170899391174 - trainLoss: 0.4465819001197815\n",
      "cnt: 0 - valLoss: 0.4437400698661804 - trainLoss: 0.4465790390968323\n",
      "cnt: 0 - valLoss: 0.44373857975006104 - trainLoss: 0.44657617807388306\n",
      "cnt: 0 - valLoss: 0.4437370002269745 - trainLoss: 0.44657331705093384\n",
      "cnt: 0 - valLoss: 0.44373536109924316 - trainLoss: 0.44657042622566223\n",
      "cnt: 0 - valLoss: 0.44373390078544617 - trainLoss: 0.446567565202713\n",
      "cnt: 0 - valLoss: 0.443732351064682 - trainLoss: 0.4465647041797638\n",
      "cnt: 0 - valLoss: 0.4437308609485626 - trainLoss: 0.44656187295913696\n",
      "cnt: 0 - valLoss: 0.4437291920185089 - trainLoss: 0.44655901193618774\n",
      "cnt: 0 - valLoss: 0.44372767210006714 - trainLoss: 0.4465561509132385\n",
      "cnt: 0 - valLoss: 0.44372618198394775 - trainLoss: 0.4465532898902893\n",
      "cnt: 0 - valLoss: 0.44372454285621643 - trainLoss: 0.4465503990650177\n",
      "cnt: 0 - valLoss: 0.44372302293777466 - trainLoss: 0.44654759764671326\n",
      "cnt: 0 - valLoss: 0.44372156262397766 - trainLoss: 0.44654467701911926\n",
      "cnt: 0 - valLoss: 0.44371989369392395 - trainLoss: 0.44654181599617004\n",
      "cnt: 0 - valLoss: 0.4437183737754822 - trainLoss: 0.44653892517089844\n",
      "cnt: 0 - valLoss: 0.4437168538570404 - trainLoss: 0.4465360641479492\n",
      "cnt: 0 - valLoss: 0.4437152147293091 - trainLoss: 0.446533203125\n",
      "cnt: 0 - valLoss: 0.4437137246131897 - trainLoss: 0.44653037190437317\n",
      "cnt: 0 - valLoss: 0.4437122046947479 - trainLoss: 0.44652751088142395\n",
      "cnt: 0 - valLoss: 0.44371068477630615 - trainLoss: 0.44652464985847473\n",
      "cnt: 0 - valLoss: 0.44370904564857483 - trainLoss: 0.4465217888355255\n",
      "cnt: 0 - valLoss: 0.44370755553245544 - trainLoss: 0.4465188980102539\n",
      "cnt: 0 - valLoss: 0.44370603561401367 - trainLoss: 0.4465160369873047\n",
      "cnt: 0 - valLoss: 0.44370442628860474 - trainLoss: 0.44651320576667786\n",
      "cnt: 0 - valLoss: 0.4437028765678406 - trainLoss: 0.44651034474372864\n",
      "cnt: 0 - valLoss: 0.4437014162540436 - trainLoss: 0.4465074837207794\n",
      "cnt: 0 - valLoss: 0.44369974732398987 - trainLoss: 0.4465046226978302\n",
      "cnt: 0 - valLoss: 0.4436982274055481 - trainLoss: 0.4465017318725586\n",
      "cnt: 0 - valLoss: 0.4436967074871063 - trainLoss: 0.4464988708496094\n",
      "cnt: 0 - valLoss: 0.44369515776634216 - trainLoss: 0.44649600982666016\n",
      "cnt: 0 - valLoss: 0.4436935782432556 - trainLoss: 0.44649314880371094\n",
      "cnt: 0 - valLoss: 0.4436921179294586 - trainLoss: 0.4464903175830841\n",
      "cnt: 0 - valLoss: 0.44369056820869446 - trainLoss: 0.4464874565601349\n",
      "cnt: 0 - valLoss: 0.44368892908096313 - trainLoss: 0.4464845657348633\n",
      "cnt: 0 - valLoss: 0.44368743896484375 - trainLoss: 0.44648176431655884\n",
      "cnt: 0 - valLoss: 0.44368597865104675 - trainLoss: 0.44647884368896484\n",
      "cnt: 0 - valLoss: 0.44368430972099304 - trainLoss: 0.446476012468338\n",
      "cnt: 0 - valLoss: 0.44368281960487366 - trainLoss: 0.4464731514453888\n",
      "cnt: 0 - valLoss: 0.44368135929107666 - trainLoss: 0.4464702904224396\n",
      "cnt: 0 - valLoss: 0.44367972016334534 - trainLoss: 0.44646742939949036\n",
      "cnt: 0 - valLoss: 0.44367825984954834 - trainLoss: 0.44646453857421875\n",
      "cnt: 0 - valLoss: 0.44367679953575134 - trainLoss: 0.4464617371559143\n",
      "cnt: 0 - valLoss: 0.4436751902103424 - trainLoss: 0.4464588165283203\n",
      "cnt: 0 - valLoss: 0.44367367029190063 - trainLoss: 0.4464559853076935\n",
      "cnt: 0 - valLoss: 0.44367220997810364 - trainLoss: 0.44645312428474426\n",
      "cnt: 0 - valLoss: 0.44367071986198425 - trainLoss: 0.44645026326179504\n",
      "cnt: 0 - valLoss: 0.44366908073425293 - trainLoss: 0.4464474320411682\n",
      "cnt: 0 - valLoss: 0.4436676502227783 - trainLoss: 0.4464446008205414\n",
      "cnt: 0 - valLoss: 0.4436661899089813 - trainLoss: 0.4464416801929474\n",
      "cnt: 0 - valLoss: 0.4436645209789276 - trainLoss: 0.44643887877464294\n",
      "cnt: 0 - valLoss: 0.4436630606651306 - trainLoss: 0.44643598794937134\n",
      "cnt: 0 - valLoss: 0.4436616003513336 - trainLoss: 0.4464331269264221\n",
      "cnt: 0 - valLoss: 0.44366002082824707 - trainLoss: 0.4464302957057953\n",
      "cnt: 0 - valLoss: 0.4436584711074829 - trainLoss: 0.4464274048805237\n",
      "cnt: 0 - valLoss: 0.4436570107936859 - trainLoss: 0.44642457365989685\n",
      "cnt: 0 - valLoss: 0.44365543127059937 - trainLoss: 0.44642171263694763\n",
      "cnt: 0 - valLoss: 0.4436539113521576 - trainLoss: 0.4464188516139984\n",
      "cnt: 0 - valLoss: 0.4436524510383606 - trainLoss: 0.4464160203933716\n",
      "cnt: 0 - valLoss: 0.4436509609222412 - trainLoss: 0.44641318917274475\n",
      "cnt: 0 - valLoss: 0.44364938139915466 - trainLoss: 0.44641032814979553\n",
      "cnt: 0 - valLoss: 0.4436478912830353 - trainLoss: 0.44640740752220154\n",
      "cnt: 0 - valLoss: 0.44364646077156067 - trainLoss: 0.4464045763015747\n",
      "cnt: 0 - valLoss: 0.44364482164382935 - trainLoss: 0.4464017152786255\n",
      "cnt: 0 - valLoss: 0.44364339113235474 - trainLoss: 0.44639888405799866\n",
      "cnt: 0 - valLoss: 0.4436418414115906 - trainLoss: 0.44639602303504944\n",
      "cnt: 0 - valLoss: 0.44364020228385925 - trainLoss: 0.4463931620121002\n",
      "cnt: 0 - valLoss: 0.44363874197006226 - trainLoss: 0.4463903307914734\n",
      "cnt: 0 - valLoss: 0.44363728165626526 - trainLoss: 0.4463874101638794\n",
      "cnt: 0 - valLoss: 0.4436356723308563 - trainLoss: 0.44638460874557495\n",
      "cnt: 0 - valLoss: 0.4436342120170593 - trainLoss: 0.4463817775249481\n",
      "cnt: 0 - valLoss: 0.44363275170326233 - trainLoss: 0.4463788568973541\n",
      "cnt: 0 - valLoss: 0.44363126158714294 - trainLoss: 0.4463760256767273\n",
      "cnt: 0 - valLoss: 0.443629652261734 - trainLoss: 0.44637322425842285\n",
      "cnt: 0 - valLoss: 0.443628191947937 - trainLoss: 0.44637033343315125\n",
      "cnt: 0 - valLoss: 0.4436267614364624 - trainLoss: 0.446367472410202\n",
      "cnt: 0 - valLoss: 0.4436251223087311 - trainLoss: 0.4463646411895752\n",
      "cnt: 0 - valLoss: 0.4436236619949341 - trainLoss: 0.4463617503643036\n",
      "cnt: 0 - valLoss: 0.4436222016811371 - trainLoss: 0.44635891914367676\n",
      "cnt: 0 - valLoss: 0.44362056255340576 - trainLoss: 0.44635605812072754\n",
      "cnt: 0 - valLoss: 0.44361913204193115 - trainLoss: 0.44635316729545593\n",
      "cnt: 0 - valLoss: 0.4436176121234894 - trainLoss: 0.4463503658771515\n",
      "cnt: 0 - valLoss: 0.44361600279808044 - trainLoss: 0.44634753465652466\n",
      "cnt: 0 - valLoss: 0.44361457228660583 - trainLoss: 0.44634461402893066\n",
      "cnt: 0 - valLoss: 0.44361311197280884 - trainLoss: 0.44634178280830383\n",
      "cnt: 0 - valLoss: 0.44361165165901184 - trainLoss: 0.4463389217853546\n",
      "cnt: 0 - valLoss: 0.44360998272895813 - trainLoss: 0.4463360607624054\n",
      "cnt: 0 - valLoss: 0.44360852241516113 - trainLoss: 0.44633322954177856\n",
      "cnt: 0 - valLoss: 0.44360706210136414 - trainLoss: 0.44633036851882935\n",
      "cnt: 0 - valLoss: 0.4436054527759552 - trainLoss: 0.4463275372982025\n",
      "cnt: 0 - valLoss: 0.4436039924621582 - trainLoss: 0.4463246762752533\n",
      "cnt: 0 - valLoss: 0.4436025321483612 - trainLoss: 0.4463217556476593\n",
      "cnt: 0 - valLoss: 0.44360095262527466 - trainLoss: 0.44631892442703247\n",
      "cnt: 0 - valLoss: 0.4435994327068329 - trainLoss: 0.446316123008728\n",
      "cnt: 0 - valLoss: 0.44359803199768066 - trainLoss: 0.4463132321834564\n",
      "cnt: 0 - valLoss: 0.44359639286994934 - trainLoss: 0.4463103711605072\n",
      "cnt: 0 - valLoss: 0.44359493255615234 - trainLoss: 0.44630753993988037\n",
      "cnt: 0 - valLoss: 0.44359344244003296 - trainLoss: 0.4463047385215759\n",
      "cnt: 0 - valLoss: 0.4435919523239136 - trainLoss: 0.4463018476963043\n",
      "cnt: 0 - valLoss: 0.44359028339385986 - trainLoss: 0.4462990462779999\n",
      "cnt: 0 - valLoss: 0.44358885288238525 - trainLoss: 0.44629615545272827\n",
      "cnt: 0 - valLoss: 0.44358736276626587 - trainLoss: 0.44629335403442383\n",
      "cnt: 0 - valLoss: 0.4435858130455017 - trainLoss: 0.446290522813797\n",
      "cnt: 0 - valLoss: 0.4435843229293823 - trainLoss: 0.44628769159317017\n",
      "cnt: 0 - valLoss: 0.4435828924179077 - trainLoss: 0.44628477096557617\n",
      "cnt: 0 - valLoss: 0.4435812830924988 - trainLoss: 0.44628196954727173\n",
      "cnt: 0 - valLoss: 0.44357985258102417 - trainLoss: 0.4462791383266449\n",
      "cnt: 0 - valLoss: 0.4435783922672272 - trainLoss: 0.44627630710601807\n",
      "cnt: 0 - valLoss: 0.4435769319534302 - trainLoss: 0.44627344608306885\n",
      "cnt: 0 - valLoss: 0.4435754120349884 - trainLoss: 0.44627058506011963\n",
      "cnt: 0 - valLoss: 0.44357389211654663 - trainLoss: 0.4462677538394928\n",
      "cnt: 0 - valLoss: 0.44357243180274963 - trainLoss: 0.44626492261886597\n",
      "cnt: 0 - valLoss: 0.4435708522796631 - trainLoss: 0.44626209139823914\n",
      "cnt: 0 - valLoss: 0.4435693025588989 - trainLoss: 0.4462592601776123\n",
      "cnt: 0 - valLoss: 0.4435679018497467 - trainLoss: 0.44625645875930786\n",
      "cnt: 0 - valLoss: 0.443566232919693 - trainLoss: 0.44625362753868103\n",
      "cnt: 0 - valLoss: 0.443564772605896 - trainLoss: 0.4462507963180542\n",
      "cnt: 0 - valLoss: 0.443563312292099 - trainLoss: 0.44624796509742737\n",
      "cnt: 0 - valLoss: 0.4435616731643677 - trainLoss: 0.44624513387680054\n",
      "cnt: 0 - valLoss: 0.4435601532459259 - trainLoss: 0.4462423324584961\n",
      "cnt: 0 - valLoss: 0.4435586929321289 - trainLoss: 0.4462394416332245\n",
      "cnt: 0 - valLoss: 0.4435572326183319 - trainLoss: 0.44623667001724243\n",
      "cnt: 0 - valLoss: 0.4435555934906006 - trainLoss: 0.4462338089942932\n",
      "cnt: 0 - valLoss: 0.4435541331768036 - trainLoss: 0.4462309777736664\n",
      "cnt: 0 - valLoss: 0.4435526430606842 - trainLoss: 0.44622814655303955\n",
      "cnt: 0 - valLoss: 0.4435510039329529 - trainLoss: 0.4462253749370575\n",
      "cnt: 0 - valLoss: 0.4435495138168335 - trainLoss: 0.4462225139141083\n",
      "cnt: 0 - valLoss: 0.4435480535030365 - trainLoss: 0.44621968269348145\n",
      "cnt: 0 - valLoss: 0.4435464143753052 - trainLoss: 0.4462168514728546\n",
      "cnt: 0 - valLoss: 0.4435449540615082 - trainLoss: 0.44621407985687256\n",
      "cnt: 0 - valLoss: 0.4435434341430664 - trainLoss: 0.44621121883392334\n",
      "cnt: 0 - valLoss: 0.44354188442230225 - trainLoss: 0.4462083876132965\n",
      "cnt: 0 - valLoss: 0.44354039430618286 - trainLoss: 0.4462055563926697\n",
      "cnt: 0 - valLoss: 0.44353896379470825 - trainLoss: 0.44620272517204285\n",
      "cnt: 0 - valLoss: 0.4435374140739441 - trainLoss: 0.446199893951416\n",
      "cnt: 0 - valLoss: 0.44353580474853516 - trainLoss: 0.44619712233543396\n",
      "cnt: 0 - valLoss: 0.44353434443473816 - trainLoss: 0.44619429111480713\n",
      "cnt: 0 - valLoss: 0.44353288412094116 - trainLoss: 0.4461914300918579\n",
      "cnt: 0 - valLoss: 0.44353124499320984 - trainLoss: 0.4461885988712311\n",
      "cnt: 0 - valLoss: 0.44352975487709045 - trainLoss: 0.44618576765060425\n",
      "cnt: 0 - valLoss: 0.44352832436561584 - trainLoss: 0.4461829662322998\n",
      "cnt: 0 - valLoss: 0.4435267150402069 - trainLoss: 0.446180135011673\n",
      "cnt: 0 - valLoss: 0.4435252547264099 - trainLoss: 0.4461773633956909\n",
      "cnt: 0 - valLoss: 0.4435237646102905 - trainLoss: 0.4461745321750641\n",
      "cnt: 0 - valLoss: 0.4435221552848816 - trainLoss: 0.44617170095443726\n",
      "cnt: 0 - valLoss: 0.4435206353664398 - trainLoss: 0.44616883993148804\n",
      "cnt: 0 - valLoss: 0.4435191750526428 - trainLoss: 0.4461660087108612\n",
      "cnt: 0 - valLoss: 0.4435177147388458 - trainLoss: 0.4461631774902344\n",
      "cnt: 0 - valLoss: 0.4435161352157593 - trainLoss: 0.4461604058742523\n",
      "cnt: 0 - valLoss: 0.4435146152973175 - trainLoss: 0.4461575448513031\n",
      "cnt: 0 - valLoss: 0.4435131549835205 - trainLoss: 0.44615471363067627\n",
      "cnt: 0 - valLoss: 0.44351157546043396 - trainLoss: 0.44615188241004944\n",
      "cnt: 0 - valLoss: 0.4435100853443146 - trainLoss: 0.4461491107940674\n",
      "cnt: 0 - valLoss: 0.4435085654258728 - trainLoss: 0.44614627957344055\n",
      "cnt: 0 - valLoss: 0.44350698590278625 - trainLoss: 0.4461434483528137\n",
      "cnt: 0 - valLoss: 0.44350552558898926 - trainLoss: 0.4461406171321869\n",
      "cnt: 0 - valLoss: 0.44350412487983704 - trainLoss: 0.44613781571388245\n",
      "cnt: 0 - valLoss: 0.4435024559497833 - trainLoss: 0.4461349844932556\n",
      "cnt: 0 - valLoss: 0.44350099563598633 - trainLoss: 0.4461321532726288\n",
      "cnt: 0 - valLoss: 0.4434995651245117 - trainLoss: 0.44612932205200195\n",
      "cnt: 0 - valLoss: 0.44349807500839233 - trainLoss: 0.4461264908313751\n",
      "cnt: 0 - valLoss: 0.4434965252876282 - trainLoss: 0.4461236894130707\n",
      "cnt: 0 - valLoss: 0.4434950351715088 - trainLoss: 0.44612085819244385\n",
      "cnt: 0 - valLoss: 0.4434936046600342 - trainLoss: 0.446118026971817\n",
      "cnt: 0 - valLoss: 0.44349199533462524 - trainLoss: 0.4461151957511902\n",
      "cnt: 0 - valLoss: 0.44349053502082825 - trainLoss: 0.44611236453056335\n",
      "cnt: 0 - valLoss: 0.44348907470703125 - trainLoss: 0.4461095631122589\n",
      "cnt: 0 - valLoss: 0.4434875249862671 - trainLoss: 0.4461067318916321\n",
      "cnt: 0 - valLoss: 0.4434860348701477 - trainLoss: 0.44610390067100525\n",
      "cnt: 0 - valLoss: 0.4434846043586731 - trainLoss: 0.4461010694503784\n",
      "cnt: 0 - valLoss: 0.44348299503326416 - trainLoss: 0.4460982382297516\n",
      "cnt: 0 - valLoss: 0.44348156452178955 - trainLoss: 0.4460955262184143\n",
      "cnt: 0 - valLoss: 0.44348007440567017 - trainLoss: 0.4460926949977875\n",
      "cnt: 0 - valLoss: 0.44347861409187317 - trainLoss: 0.44608983397483826\n",
      "cnt: 0 - valLoss: 0.4434770345687866 - trainLoss: 0.4460870325565338\n",
      "cnt: 0 - valLoss: 0.443475604057312 - trainLoss: 0.446084201335907\n",
      "cnt: 0 - valLoss: 0.44347408413887024 - trainLoss: 0.44608139991760254\n",
      "cnt: 0 - valLoss: 0.4434725344181061 - trainLoss: 0.4460785686969757\n",
      "cnt: 0 - valLoss: 0.4434710741043091 - trainLoss: 0.4460757374763489\n",
      "cnt: 0 - valLoss: 0.4434696137905121 - trainLoss: 0.44607290625572205\n",
      "cnt: 0 - valLoss: 0.44346797466278076 - trainLoss: 0.4460700750350952\n",
      "cnt: 0 - valLoss: 0.44346654415130615 - trainLoss: 0.44606727361679077\n",
      "cnt: 0 - valLoss: 0.44346511363983154 - trainLoss: 0.44606447219848633\n",
      "cnt: 0 - valLoss: 0.4434637129306793 - trainLoss: 0.4460616111755371\n",
      "cnt: 0 - valLoss: 0.443462073802948 - trainLoss: 0.44605883955955505\n",
      "cnt: 0 - valLoss: 0.443460613489151 - trainLoss: 0.4460560083389282\n",
      "cnt: 0 - valLoss: 0.443459153175354 - trainLoss: 0.4460531771183014\n",
      "cnt: 0 - valLoss: 0.44345760345458984 - trainLoss: 0.44605040550231934\n",
      "cnt: 0 - valLoss: 0.44345614314079285 - trainLoss: 0.4460475742816925\n",
      "cnt: 0 - valLoss: 0.44345471262931824 - trainLoss: 0.4460447430610657\n",
      "cnt: 0 - valLoss: 0.4434531033039093 - trainLoss: 0.44604191184043884\n",
      "cnt: 0 - valLoss: 0.4434516131877899 - trainLoss: 0.4460391104221344\n",
      "cnt: 0 - valLoss: 0.4434502422809601 - trainLoss: 0.44603627920150757\n",
      "cnt: 0 - valLoss: 0.44344857335090637 - trainLoss: 0.44603344798088074\n",
      "cnt: 0 - valLoss: 0.44344714283943176 - trainLoss: 0.4460306167602539\n",
      "cnt: 0 - valLoss: 0.44344571232795715 - trainLoss: 0.44602784514427185\n",
      "cnt: 0 - valLoss: 0.44344428181648254 - trainLoss: 0.44602498412132263\n",
      "cnt: 0 - valLoss: 0.44344261288642883 - trainLoss: 0.4460221827030182\n",
      "cnt: 0 - valLoss: 0.4434412121772766 - trainLoss: 0.44601941108703613\n",
      "cnt: 0 - valLoss: 0.443439781665802 - trainLoss: 0.4460165798664093\n",
      "cnt: 0 - valLoss: 0.44343823194503784 - trainLoss: 0.44601374864578247\n",
      "cnt: 0 - valLoss: 0.44343671202659607 - trainLoss: 0.446010947227478\n",
      "cnt: 0 - valLoss: 0.44343528151512146 - trainLoss: 0.4460081160068512\n",
      "cnt: 0 - valLoss: 0.4434337019920349 - trainLoss: 0.44600534439086914\n",
      "cnt: 0 - valLoss: 0.4434322416782379 - trainLoss: 0.4460025131702423\n",
      "cnt: 0 - valLoss: 0.4434308111667633 - trainLoss: 0.4459996521472931\n",
      "cnt: 0 - valLoss: 0.44342923164367676 - trainLoss: 0.44599685072898865\n",
      "cnt: 0 - valLoss: 0.44342777132987976 - trainLoss: 0.4459940195083618\n",
      "cnt: 0 - valLoss: 0.44342631101608276 - trainLoss: 0.4459912180900574\n",
      "cnt: 0 - valLoss: 0.44342491030693054 - trainLoss: 0.44598841667175293\n",
      "cnt: 0 - valLoss: 0.443423330783844 - trainLoss: 0.4459855854511261\n",
      "cnt: 0 - valLoss: 0.443421870470047 - trainLoss: 0.44598278403282166\n",
      "cnt: 0 - valLoss: 0.4434204399585724 - trainLoss: 0.4459799528121948\n",
      "cnt: 0 - valLoss: 0.4434189200401306 - trainLoss: 0.445977121591568\n",
      "cnt: 0 - valLoss: 0.44341740012168884 - trainLoss: 0.44597434997558594\n",
      "cnt: 0 - valLoss: 0.4434159994125366 - trainLoss: 0.4459715187549591\n",
      "cnt: 0 - valLoss: 0.44341444969177246 - trainLoss: 0.44596874713897705\n",
      "cnt: 0 - valLoss: 0.44341304898262024 - trainLoss: 0.4459659159183502\n",
      "cnt: 0 - valLoss: 0.44341152906417847 - trainLoss: 0.4459630846977234\n",
      "cnt: 0 - valLoss: 0.4434100091457367 - trainLoss: 0.44596025347709656\n",
      "cnt: 0 - valLoss: 0.4434085786342621 - trainLoss: 0.4459574222564697\n",
      "cnt: 0 - valLoss: 0.44340717792510986 - trainLoss: 0.44595465064048767\n",
      "cnt: 0 - valLoss: 0.4434056580066681 - trainLoss: 0.44595181941986084\n",
      "cnt: 0 - valLoss: 0.44340410828590393 - trainLoss: 0.4459490478038788\n",
      "cnt: 0 - valLoss: 0.4434027373790741 - trainLoss: 0.44594621658325195\n",
      "cnt: 0 - valLoss: 0.4434012174606323 - trainLoss: 0.4459433853626251\n",
      "cnt: 0 - valLoss: 0.443399578332901 - trainLoss: 0.4459405839443207\n",
      "cnt: 0 - valLoss: 0.443398118019104 - trainLoss: 0.44593775272369385\n",
      "cnt: 0 - valLoss: 0.4433966279029846 - trainLoss: 0.4459349513053894\n",
      "cnt: 0 - valLoss: 0.44339504837989807 - trainLoss: 0.44593217968940735\n",
      "cnt: 0 - valLoss: 0.4433935284614563 - trainLoss: 0.4459293484687805\n",
      "cnt: 0 - valLoss: 0.4433920383453369 - trainLoss: 0.4459265470504761\n",
      "cnt: 0 - valLoss: 0.44339045882225037 - trainLoss: 0.44592371582984924\n",
      "cnt: 0 - valLoss: 0.4433889389038086 - trainLoss: 0.4459209144115448\n",
      "cnt: 0 - valLoss: 0.4433874785900116 - trainLoss: 0.44591811299324036\n",
      "cnt: 0 - valLoss: 0.443386048078537 - trainLoss: 0.4459153115749359\n",
      "cnt: 0 - valLoss: 0.4433843493461609 - trainLoss: 0.4459124803543091\n",
      "cnt: 0 - valLoss: 0.4433829188346863 - trainLoss: 0.44590967893600464\n",
      "cnt: 0 - valLoss: 0.4433814585208893 - trainLoss: 0.4459068775177002\n",
      "cnt: 0 - valLoss: 0.44337984919548035 - trainLoss: 0.44590410590171814\n",
      "cnt: 0 - valLoss: 0.44337841868400574 - trainLoss: 0.4459012448787689\n",
      "cnt: 0 - valLoss: 0.4433768689632416 - trainLoss: 0.4458984136581421\n",
      "cnt: 0 - valLoss: 0.44337528944015503 - trainLoss: 0.44589564204216003\n",
      "cnt: 0 - valLoss: 0.44337376952171326 - trainLoss: 0.4458928406238556\n",
      "cnt: 0 - valLoss: 0.44337236881256104 - trainLoss: 0.44589006900787354\n",
      "cnt: 0 - valLoss: 0.4433707892894745 - trainLoss: 0.4458872377872467\n",
      "cnt: 0 - valLoss: 0.4433692693710327 - trainLoss: 0.4458844065666199\n",
      "cnt: 0 - valLoss: 0.4433678090572357 - trainLoss: 0.44588160514831543\n",
      "cnt: 0 - valLoss: 0.4433663487434387 - trainLoss: 0.445878803730011\n",
      "cnt: 0 - valLoss: 0.4433647692203522 - trainLoss: 0.44587603211402893\n",
      "cnt: 0 - valLoss: 0.4433633089065552 - trainLoss: 0.4458732008934021\n",
      "cnt: 0 - valLoss: 0.4433618485927582 - trainLoss: 0.44587036967277527\n",
      "cnt: 0 - valLoss: 0.44336026906967163 - trainLoss: 0.4458675980567932\n",
      "cnt: 0 - valLoss: 0.44335880875587463 - trainLoss: 0.445864737033844\n",
      "cnt: 0 - valLoss: 0.44335728883743286 - trainLoss: 0.4458619952201843\n",
      "cnt: 0 - valLoss: 0.4433556795120239 - trainLoss: 0.4458591639995575\n",
      "cnt: 0 - valLoss: 0.44335421919822693 - trainLoss: 0.44585633277893066\n",
      "cnt: 0 - valLoss: 0.4433527886867523 - trainLoss: 0.44585350155830383\n",
      "cnt: 0 - valLoss: 0.4433513581752777 - trainLoss: 0.4458507299423218\n",
      "cnt: 0 - valLoss: 0.4433497190475464 - trainLoss: 0.4458479583263397\n",
      "cnt: 0 - valLoss: 0.4433482885360718 - trainLoss: 0.4458451271057129\n",
      "cnt: 0 - valLoss: 0.44334676861763 - trainLoss: 0.44584229588508606\n",
      "cnt: 0 - valLoss: 0.44334521889686584 - trainLoss: 0.445839524269104\n",
      "cnt: 0 - valLoss: 0.44334375858306885 - trainLoss: 0.4458366930484772\n",
      "cnt: 0 - valLoss: 0.44334226846694946 - trainLoss: 0.4458339214324951\n",
      "cnt: 0 - valLoss: 0.4433406889438629 - trainLoss: 0.4458310902118683\n",
      "cnt: 0 - valLoss: 0.4433392286300659 - trainLoss: 0.44582825899124146\n",
      "cnt: 0 - valLoss: 0.4433378279209137 - trainLoss: 0.4458254873752594\n",
      "cnt: 0 - valLoss: 0.44333621859550476 - trainLoss: 0.44582265615463257\n",
      "cnt: 0 - valLoss: 0.443334698677063 - trainLoss: 0.4458198845386505\n",
      "cnt: 0 - valLoss: 0.4433332681655884 - trainLoss: 0.4458170533180237\n",
      "cnt: 0 - valLoss: 0.44333186745643616 - trainLoss: 0.44581422209739685\n",
      "cnt: 0 - valLoss: 0.4433302581310272 - trainLoss: 0.4458114802837372\n",
      "cnt: 0 - valLoss: 0.4433288276195526 - trainLoss: 0.44580864906311035\n",
      "cnt: 0 - valLoss: 0.4433273673057556 - trainLoss: 0.4458058178424835\n",
      "cnt: 0 - valLoss: 0.4433257281780243 - trainLoss: 0.4458030164241791\n",
      "cnt: 0 - valLoss: 0.44332432746887207 - trainLoss: 0.44580021500587463\n",
      "cnt: 0 - valLoss: 0.44332289695739746 - trainLoss: 0.4457974135875702\n",
      "cnt: 0 - valLoss: 0.4433213174343109 - trainLoss: 0.44579461216926575\n",
      "cnt: 0 - valLoss: 0.44331982731819153 - trainLoss: 0.4457918405532837\n",
      "cnt: 0 - valLoss: 0.4433183968067169 - trainLoss: 0.44578900933265686\n",
      "cnt: 0 - valLoss: 0.4433167576789856 - trainLoss: 0.44578617811203003\n",
      "cnt: 0 - valLoss: 0.4433153569698334 - trainLoss: 0.445783406496048\n",
      "cnt: 0 - valLoss: 0.44331395626068115 - trainLoss: 0.44578057527542114\n",
      "cnt: 0 - valLoss: 0.44331249594688416 - trainLoss: 0.4457778334617615\n",
      "cnt: 0 - valLoss: 0.44331094622612 - trainLoss: 0.44577500224113464\n",
      "cnt: 0 - valLoss: 0.4433094263076782 - trainLoss: 0.4457722008228302\n",
      "cnt: 0 - valLoss: 0.443308025598526 - trainLoss: 0.44576945900917053\n",
      "cnt: 0 - valLoss: 0.44330647587776184 - trainLoss: 0.4457665681838989\n",
      "cnt: 0 - valLoss: 0.44330501556396484 - trainLoss: 0.44576379656791687\n",
      "cnt: 0 - valLoss: 0.44330355525016785 - trainLoss: 0.4457610249519348\n",
      "cnt: 0 - valLoss: 0.4433019459247589 - trainLoss: 0.44575822353363037\n",
      "cnt: 0 - valLoss: 0.4433005154132843 - trainLoss: 0.4457554221153259\n",
      "cnt: 0 - valLoss: 0.4432991147041321 - trainLoss: 0.4457525908946991\n",
      "cnt: 0 - valLoss: 0.44329753518104553 - trainLoss: 0.44574984908103943\n",
      "cnt: 0 - valLoss: 0.4432961046695709 - trainLoss: 0.4457470178604126\n",
      "cnt: 0 - valLoss: 0.4432946443557739 - trainLoss: 0.44574418663978577\n",
      "cnt: 0 - valLoss: 0.4432932436466217 - trainLoss: 0.4457413852214813\n",
      "cnt: 0 - valLoss: 0.44329166412353516 - trainLoss: 0.44573864340782166\n",
      "cnt: 0 - valLoss: 0.44329020380973816 - trainLoss: 0.4457358121871948\n",
      "cnt: 0 - valLoss: 0.44328877329826355 - trainLoss: 0.445732980966568\n",
      "cnt: 0 - valLoss: 0.443287193775177 - trainLoss: 0.44573020935058594\n",
      "cnt: 0 - valLoss: 0.44328573346138 - trainLoss: 0.4457274079322815\n",
      "cnt: 0 - valLoss: 0.44328436255455017 - trainLoss: 0.44572460651397705\n",
      "cnt: 0 - valLoss: 0.4432827830314636 - trainLoss: 0.4457217752933502\n",
      "cnt: 0 - valLoss: 0.443281352519989 - trainLoss: 0.4457189440727234\n",
      "cnt: 0 - valLoss: 0.4432799518108368 - trainLoss: 0.4457162022590637\n",
      "cnt: 0 - valLoss: 0.4432782828807831 - trainLoss: 0.4457133710384369\n",
      "cnt: 0 - valLoss: 0.44327685236930847 - trainLoss: 0.44571059942245483\n",
      "cnt: 0 - valLoss: 0.44327545166015625 - trainLoss: 0.445707768201828\n",
      "cnt: 0 - valLoss: 0.44327405095100403 - trainLoss: 0.44570499658584595\n",
      "cnt: 0 - valLoss: 0.44327250123023987 - trainLoss: 0.4457021653652191\n",
      "cnt: 0 - valLoss: 0.4432709813117981 - trainLoss: 0.44569939374923706\n",
      "cnt: 0 - valLoss: 0.4432695806026459 - trainLoss: 0.4456965923309326\n",
      "cnt: 0 - valLoss: 0.4432680010795593 - trainLoss: 0.4456937909126282\n",
      "cnt: 0 - valLoss: 0.4432666301727295 - trainLoss: 0.44569098949432373\n",
      "cnt: 0 - valLoss: 0.4432651698589325 - trainLoss: 0.4456882178783417\n",
      "cnt: 0 - valLoss: 0.44326353073120117 - trainLoss: 0.44568538665771484\n",
      "cnt: 0 - valLoss: 0.44326213002204895 - trainLoss: 0.4456826150417328\n",
      "cnt: 0 - valLoss: 0.44326069951057434 - trainLoss: 0.44567978382110596\n",
      "cnt: 0 - valLoss: 0.4432591497898102 - trainLoss: 0.4456770122051239\n",
      "cnt: 0 - valLoss: 0.44325774908065796 - trainLoss: 0.44567421078681946\n",
      "cnt: 0 - valLoss: 0.44325628876686096 - trainLoss: 0.4456713795661926\n",
      "cnt: 0 - valLoss: 0.44325488805770874 - trainLoss: 0.44566860795021057\n",
      "cnt: 0 - valLoss: 0.4432532489299774 - trainLoss: 0.44566577672958374\n",
      "cnt: 0 - valLoss: 0.4432518482208252 - trainLoss: 0.4456630051136017\n",
      "cnt: 0 - valLoss: 0.4432504177093506 - trainLoss: 0.44566023349761963\n",
      "cnt: 0 - valLoss: 0.44324883818626404 - trainLoss: 0.4456574022769928\n",
      "cnt: 0 - valLoss: 0.44324737787246704 - trainLoss: 0.44565466046333313\n",
      "cnt: 0 - valLoss: 0.44324591755867004 - trainLoss: 0.4456518292427063\n",
      "cnt: 0 - valLoss: 0.4432445168495178 - trainLoss: 0.44564899802207947\n",
      "cnt: 0 - valLoss: 0.44324296712875366 - trainLoss: 0.4456462562084198\n",
      "cnt: 0 - valLoss: 0.44324150681495667 - trainLoss: 0.44564348459243774\n",
      "cnt: 0 - valLoss: 0.44324004650115967 - trainLoss: 0.4456406533718109\n",
      "cnt: 0 - valLoss: 0.4432384967803955 - trainLoss: 0.44563788175582886\n",
      "cnt: 0 - valLoss: 0.44323694705963135 - trainLoss: 0.4456351101398468\n",
      "cnt: 0 - valLoss: 0.4432355761528015 - trainLoss: 0.44563230872154236\n",
      "cnt: 0 - valLoss: 0.4432341456413269 - trainLoss: 0.4456294775009155\n",
      "cnt: 0 - valLoss: 0.44323259592056274 - trainLoss: 0.44562673568725586\n",
      "cnt: 0 - valLoss: 0.4432312250137329 - trainLoss: 0.4456239342689514\n",
      "cnt: 0 - valLoss: 0.44322967529296875 - trainLoss: 0.445621132850647\n",
      "cnt: 0 - valLoss: 0.4432283043861389 - trainLoss: 0.4456183612346649\n",
      "cnt: 0 - valLoss: 0.44322675466537476 - trainLoss: 0.4456155300140381\n",
      "cnt: 0 - valLoss: 0.44322529435157776 - trainLoss: 0.44561275839805603\n",
      "cnt: 0 - valLoss: 0.4432239234447479 - trainLoss: 0.4456099569797516\n",
      "cnt: 0 - valLoss: 0.443222314119339 - trainLoss: 0.44560715556144714\n",
      "cnt: 0 - valLoss: 0.4432208836078644 - trainLoss: 0.4456044137477875\n",
      "cnt: 0 - valLoss: 0.44321948289871216 - trainLoss: 0.44560161232948303\n",
      "cnt: 0 - valLoss: 0.44321805238723755 - trainLoss: 0.445598840713501\n",
      "cnt: 0 - valLoss: 0.4432165026664734 - trainLoss: 0.44559600949287415\n",
      "cnt: 0 - valLoss: 0.443215012550354 - trainLoss: 0.4455932676792145\n",
      "cnt: 0 - valLoss: 0.4432136118412018 - trainLoss: 0.44559046626091003\n",
      "cnt: 0 - valLoss: 0.4432121813297272 - trainLoss: 0.4455876350402832\n",
      "cnt: 0 - valLoss: 0.443210631608963 - trainLoss: 0.44558483362197876\n",
      "cnt: 0 - valLoss: 0.4432092607021332 - trainLoss: 0.4455820620059967\n",
      "cnt: 0 - valLoss: 0.4432077407836914 - trainLoss: 0.44557929039001465\n",
      "cnt: 0 - valLoss: 0.44320622086524963 - trainLoss: 0.4455764889717102\n",
      "cnt: 0 - valLoss: 0.4432048201560974 - trainLoss: 0.44557368755340576\n",
      "cnt: 0 - valLoss: 0.4432033896446228 - trainLoss: 0.4455709457397461\n",
      "cnt: 0 - valLoss: 0.4432019889354706 - trainLoss: 0.44556811451911926\n",
      "cnt: 0 - valLoss: 0.44320040941238403 - trainLoss: 0.4455653429031372\n",
      "cnt: 0 - valLoss: 0.4431989789009094 - trainLoss: 0.44556254148483276\n",
      "cnt: 0 - valLoss: 0.4431975185871124 - trainLoss: 0.4455597698688507\n",
      "cnt: 0 - valLoss: 0.44319599866867065 - trainLoss: 0.4455569386482239\n",
      "cnt: 0 - valLoss: 0.4431946277618408 - trainLoss: 0.4455541670322418\n",
      "cnt: 0 - valLoss: 0.4431931674480438 - trainLoss: 0.44555142521858215\n",
      "cnt: 0 - valLoss: 0.4431917369365692 - trainLoss: 0.4455485939979553\n",
      "cnt: 0 - valLoss: 0.44319018721580505 - trainLoss: 0.44554582238197327\n",
      "cnt: 0 - valLoss: 0.44318875670433044 - trainLoss: 0.4455430209636688\n",
      "cnt: 0 - valLoss: 0.44318732619285583 - trainLoss: 0.4455402195453644\n",
      "cnt: 0 - valLoss: 0.4431859254837036 - trainLoss: 0.44553741812705994\n",
      "cnt: 0 - valLoss: 0.4431843161582947 - trainLoss: 0.4455346465110779\n",
      "cnt: 0 - valLoss: 0.44318291544914246 - trainLoss: 0.44553181529045105\n",
      "cnt: 0 - valLoss: 0.44318148493766785 - trainLoss: 0.4455290734767914\n",
      "cnt: 0 - valLoss: 0.4431799650192261 - trainLoss: 0.4455263018608093\n",
      "cnt: 0 - valLoss: 0.44317853450775146 - trainLoss: 0.4455234706401825\n",
      "cnt: 0 - valLoss: 0.44317713379859924 - trainLoss: 0.44552069902420044\n",
      "cnt: 0 - valLoss: 0.44317570328712463 - trainLoss: 0.445517897605896\n",
      "cnt: 0 - valLoss: 0.4431741237640381 - trainLoss: 0.44551512598991394\n",
      "cnt: 0 - valLoss: 0.44317275285720825 - trainLoss: 0.4455123543739319\n",
      "cnt: 0 - valLoss: 0.44317129254341125 - trainLoss: 0.44550955295562744\n",
      "cnt: 0 - valLoss: 0.4431697428226471 - trainLoss: 0.4455067217350006\n",
      "cnt: 0 - valLoss: 0.4431684613227844 - trainLoss: 0.44550397992134094\n",
      "cnt: 0 - valLoss: 0.4431670904159546 - trainLoss: 0.4455011785030365\n",
      "cnt: 0 - valLoss: 0.4431658089160919 - trainLoss: 0.44549834728240967\n",
      "cnt: 0 - valLoss: 0.4431643486022949 - trainLoss: 0.44549551606178284\n",
      "cnt: 0 - valLoss: 0.44316306710243225 - trainLoss: 0.445492684841156\n",
      "cnt: 0 - valLoss: 0.4431617856025696 - trainLoss: 0.44548991322517395\n",
      "cnt: 0 - valLoss: 0.44316041469573975 - trainLoss: 0.4454871416091919\n",
      "cnt: 0 - valLoss: 0.44315898418426514 - trainLoss: 0.44548431038856506\n",
      "cnt: 0 - valLoss: 0.44315770268440247 - trainLoss: 0.44548147916793823\n",
      "cnt: 0 - valLoss: 0.4431564211845398 - trainLoss: 0.4454786479473114\n",
      "cnt: 0 - valLoss: 0.4431550204753876 - trainLoss: 0.44547590613365173\n",
      "cnt: 0 - valLoss: 0.44315364956855774 - trainLoss: 0.4454731047153473\n",
      "cnt: 0 - valLoss: 0.4431523382663727 - trainLoss: 0.44547027349472046\n",
      "cnt: 0 - valLoss: 0.44315105676651 - trainLoss: 0.445467472076416\n",
      "cnt: 0 - valLoss: 0.4431496560573578 - trainLoss: 0.4454646706581116\n",
      "cnt: 0 - valLoss: 0.4431483745574951 - trainLoss: 0.44546183943748474\n",
      "cnt: 0 - valLoss: 0.44314703345298767 - trainLoss: 0.4454590380191803\n",
      "cnt: 0 - valLoss: 0.4431455731391907 - trainLoss: 0.44545626640319824\n",
      "cnt: 0 - valLoss: 0.443144291639328 - trainLoss: 0.445453405380249\n",
      "cnt: 0 - valLoss: 0.44314301013946533 - trainLoss: 0.44545066356658936\n",
      "cnt: 0 - valLoss: 0.44314172863960266 - trainLoss: 0.4454478323459625\n",
      "cnt: 0 - valLoss: 0.44314026832580566 - trainLoss: 0.4454450011253357\n",
      "cnt: 0 - valLoss: 0.4431389272212982 - trainLoss: 0.44544222950935364\n",
      "cnt: 0 - valLoss: 0.44313761591911316 - trainLoss: 0.4454394578933716\n",
      "cnt: 0 - valLoss: 0.4431363344192505 - trainLoss: 0.44543662667274475\n",
      "cnt: 0 - valLoss: 0.4431349039077759 - trainLoss: 0.4454337954521179\n",
      "cnt: 0 - valLoss: 0.4431335926055908 - trainLoss: 0.44543102383613586\n",
      "cnt: 0 - valLoss: 0.44313228130340576 - trainLoss: 0.4454282224178314\n",
      "cnt: 0 - valLoss: 0.44313088059425354 - trainLoss: 0.445425420999527\n",
      "cnt: 0 - valLoss: 0.4431295096874237 - trainLoss: 0.44542261958122253\n",
      "cnt: 0 - valLoss: 0.44312816858291626 - trainLoss: 0.4454198181629181\n",
      "cnt: 0 - valLoss: 0.44312694668769836 - trainLoss: 0.44541701674461365\n",
      "cnt: 0 - valLoss: 0.44312551617622375 - trainLoss: 0.4454141855239868\n",
      "cnt: 0 - valLoss: 0.4431242346763611 - trainLoss: 0.4454113841056824\n",
      "cnt: 0 - valLoss: 0.44312286376953125 - trainLoss: 0.44540858268737793\n",
      "cnt: 0 - valLoss: 0.4431215822696686 - trainLoss: 0.4454058110713959\n",
      "cnt: 0 - valLoss: 0.4431201219558716 - trainLoss: 0.44540297985076904\n",
      "cnt: 0 - valLoss: 0.4431188404560089 - trainLoss: 0.445400208234787\n",
      "cnt: 0 - valLoss: 0.44311755895614624 - trainLoss: 0.44539737701416016\n",
      "cnt: 0 - valLoss: 0.44311609864234924 - trainLoss: 0.4453946053981781\n",
      "cnt: 0 - valLoss: 0.4431148171424866 - trainLoss: 0.44539177417755127\n",
      "cnt: 0 - valLoss: 0.44311344623565674 - trainLoss: 0.4453889727592468\n",
      "cnt: 0 - valLoss: 0.4431121349334717 - trainLoss: 0.44538620114326477\n",
      "cnt: 0 - valLoss: 0.44311076402664185 - trainLoss: 0.44538336992263794\n",
      "cnt: 0 - valLoss: 0.443109393119812 - trainLoss: 0.4453805685043335\n",
      "cnt: 0 - valLoss: 0.44310811161994934 - trainLoss: 0.44537776708602905\n",
      "cnt: 0 - valLoss: 0.4431067109107971 - trainLoss: 0.4453749358654022\n",
      "cnt: 0 - valLoss: 0.4431053698062897 - trainLoss: 0.44537216424942017\n",
      "cnt: 0 - valLoss: 0.443104088306427 - trainLoss: 0.44536933302879333\n",
      "cnt: 0 - valLoss: 0.44310280680656433 - trainLoss: 0.44536659121513367\n",
      "cnt: 0 - valLoss: 0.44310134649276733 - trainLoss: 0.4453637897968292\n",
      "cnt: 0 - valLoss: 0.44310006499290466 - trainLoss: 0.4453609585762024\n",
      "cnt: 0 - valLoss: 0.44309869408607483 - trainLoss: 0.44535815715789795\n",
      "cnt: 0 - valLoss: 0.44309741258621216 - trainLoss: 0.4453553855419159\n",
      "cnt: 0 - valLoss: 0.44309598207473755 - trainLoss: 0.44535255432128906\n",
      "cnt: 0 - valLoss: 0.44309476017951965 - trainLoss: 0.44534972310066223\n",
      "cnt: 0 - valLoss: 0.4430934190750122 - trainLoss: 0.4453469216823578\n",
      "cnt: 0 - valLoss: 0.4430919587612152 - trainLoss: 0.44534412026405334\n",
      "cnt: 0 - valLoss: 0.44309067726135254 - trainLoss: 0.4453413486480713\n",
      "cnt: 0 - valLoss: 0.4430893361568451 - trainLoss: 0.44533851742744446\n",
      "cnt: 0 - valLoss: 0.4430880546569824 - trainLoss: 0.4453357458114624\n",
      "cnt: 0 - valLoss: 0.4430866539478302 - trainLoss: 0.44533294439315796\n",
      "cnt: 0 - valLoss: 0.44308531284332275 - trainLoss: 0.4453301429748535\n",
      "cnt: 0 - valLoss: 0.4430840313434601 - trainLoss: 0.4453273117542267\n",
      "cnt: 0 - valLoss: 0.4430827498435974 - trainLoss: 0.445324569940567\n",
      "cnt: 0 - valLoss: 0.44308120012283325 - trainLoss: 0.4453217387199402\n",
      "cnt: 0 - valLoss: 0.44307994842529297 - trainLoss: 0.44531890749931335\n",
      "cnt: 0 - valLoss: 0.4430786669254303 - trainLoss: 0.4453161358833313\n",
      "cnt: 0 - valLoss: 0.4430772662162781 - trainLoss: 0.44531336426734924\n",
      "cnt: 0 - valLoss: 0.4430759847164154 - trainLoss: 0.4453105330467224\n",
      "cnt: 0 - valLoss: 0.44307461380958557 - trainLoss: 0.4453077018260956\n",
      "cnt: 0 - valLoss: 0.4430733025074005 - trainLoss: 0.4453049302101135\n",
      "cnt: 0 - valLoss: 0.4430719017982483 - trainLoss: 0.4453021287918091\n",
      "cnt: 0 - valLoss: 0.443070650100708 - trainLoss: 0.44529932737350464\n",
      "cnt: 0 - valLoss: 0.44306933879852295 - trainLoss: 0.4452965259552002\n",
      "cnt: 0 - valLoss: 0.4430679976940155 - trainLoss: 0.44529375433921814\n",
      "cnt: 0 - valLoss: 0.44306662678718567 - trainLoss: 0.4452909231185913\n",
      "cnt: 0 - valLoss: 0.4430652856826782 - trainLoss: 0.4452880918979645\n",
      "cnt: 0 - valLoss: 0.4430639147758484 - trainLoss: 0.4452853202819824\n",
      "cnt: 0 - valLoss: 0.44306251406669617 - trainLoss: 0.44528254866600037\n",
      "cnt: 0 - valLoss: 0.4430612325668335 - trainLoss: 0.44527971744537354\n",
      "cnt: 0 - valLoss: 0.4430599510669708 - trainLoss: 0.4452769458293915\n",
      "cnt: 0 - valLoss: 0.44305866956710815 - trainLoss: 0.44527414441108704\n",
      "cnt: 0 - valLoss: 0.44305720925331116 - trainLoss: 0.4452713131904602\n",
      "cnt: 0 - valLoss: 0.4430559277534485 - trainLoss: 0.44526851177215576\n",
      "cnt: 0 - valLoss: 0.44305458664894104 - trainLoss: 0.4452657699584961\n",
      "cnt: 0 - valLoss: 0.44305333495140076 - trainLoss: 0.44526293873786926\n",
      "cnt: 0 - valLoss: 0.44305187463760376 - trainLoss: 0.44526010751724243\n",
      "cnt: 0 - valLoss: 0.4430505931377411 - trainLoss: 0.4452573359012604\n",
      "cnt: 0 - valLoss: 0.4430493116378784 - trainLoss: 0.44525453448295593\n",
      "cnt: 0 - valLoss: 0.4430478513240814 - trainLoss: 0.4452517628669739\n",
      "cnt: 0 - valLoss: 0.44304654002189636 - trainLoss: 0.4452489912509918\n",
      "cnt: 0 - valLoss: 0.4430452287197113 - trainLoss: 0.445246160030365\n",
      "cnt: 0 - valLoss: 0.4430440068244934 - trainLoss: 0.44524338841438293\n",
      "cnt: 0 - valLoss: 0.4430425465106964 - trainLoss: 0.4452405869960785\n",
      "cnt: 0 - valLoss: 0.44304129481315613 - trainLoss: 0.44523775577545166\n",
      "cnt: 0 - valLoss: 0.44303998351097107 - trainLoss: 0.4452349543571472\n",
      "cnt: 0 - valLoss: 0.443038672208786 - trainLoss: 0.44523221254348755\n",
      "cnt: 0 - valLoss: 0.4430372714996338 - trainLoss: 0.4452293813228607\n",
      "cnt: 0 - valLoss: 0.44303593039512634 - trainLoss: 0.44522660970687866\n",
      "cnt: 0 - valLoss: 0.44303464889526367 - trainLoss: 0.4452238082885742\n",
      "cnt: 0 - valLoss: 0.44303324818611145 - trainLoss: 0.4452209770679474\n",
      "cnt: 0 - valLoss: 0.44303199648857117 - trainLoss: 0.4452182650566101\n",
      "cnt: 0 - valLoss: 0.4430307149887085 - trainLoss: 0.4452154338359833\n",
      "cnt: 0 - valLoss: 0.44302934408187866 - trainLoss: 0.44521260261535645\n",
      "cnt: 0 - valLoss: 0.44302797317504883 - trainLoss: 0.4452098608016968\n",
      "cnt: 0 - valLoss: 0.443026602268219 - trainLoss: 0.44520702958106995\n",
      "cnt: 0 - valLoss: 0.4430253207683563 - trainLoss: 0.4452042579650879\n",
      "cnt: 0 - valLoss: 0.44302403926849365 - trainLoss: 0.44520142674446106\n",
      "cnt: 0 - valLoss: 0.4430224895477295 - trainLoss: 0.4451986849308014\n",
      "cnt: 0 - valLoss: 0.44302114844322205 - trainLoss: 0.44519591331481934\n",
      "cnt: 0 - valLoss: 0.4430198669433594 - trainLoss: 0.4451931118965149\n",
      "cnt: 0 - valLoss: 0.44301837682724 - trainLoss: 0.44519031047821045\n",
      "cnt: 0 - valLoss: 0.44301700592041016 - trainLoss: 0.445187509059906\n",
      "cnt: 0 - valLoss: 0.4430156350135803 - trainLoss: 0.44518476724624634\n",
      "cnt: 0 - valLoss: 0.44301432371139526 - trainLoss: 0.4451819658279419\n",
      "cnt: 0 - valLoss: 0.4430128335952759 - trainLoss: 0.44517916440963745\n",
      "cnt: 0 - valLoss: 0.44301146268844604 - trainLoss: 0.4451763927936554\n",
      "cnt: 0 - valLoss: 0.443010151386261 - trainLoss: 0.4451736509799957\n",
      "cnt: 0 - valLoss: 0.44300878047943115 - trainLoss: 0.44517087936401367\n",
      "cnt: 0 - valLoss: 0.443007230758667 - trainLoss: 0.44516807794570923\n",
      "cnt: 0 - valLoss: 0.44300585985183716 - trainLoss: 0.4451653063297272\n",
      "cnt: 0 - valLoss: 0.4430045187473297 - trainLoss: 0.4451625645160675\n",
      "cnt: 0 - valLoss: 0.44300320744514465 - trainLoss: 0.4451597332954407\n",
      "cnt: 0 - valLoss: 0.44300174713134766 - trainLoss: 0.445156991481781\n",
      "cnt: 0 - valLoss: 0.44300031661987305 - trainLoss: 0.44515421986579895\n",
      "cnt: 0 - valLoss: 0.442999005317688 - trainLoss: 0.4451514780521393\n",
      "cnt: 0 - valLoss: 0.44299745559692383 - trainLoss: 0.44514864683151245\n",
      "cnt: 0 - valLoss: 0.442996084690094 - trainLoss: 0.4451458752155304\n",
      "cnt: 0 - valLoss: 0.442994624376297 - trainLoss: 0.44514310359954834\n",
      "cnt: 0 - valLoss: 0.44299325346946716 - trainLoss: 0.44514036178588867\n",
      "cnt: 0 - valLoss: 0.442991703748703 - trainLoss: 0.445137619972229\n",
      "cnt: 0 - valLoss: 0.44299033284187317 - trainLoss: 0.44513481855392456\n",
      "cnt: 0 - valLoss: 0.44298896193504333 - trainLoss: 0.4451320469379425\n",
      "cnt: 0 - valLoss: 0.44298750162124634 - trainLoss: 0.44512930512428284\n",
      "cnt: 0 - valLoss: 0.44298601150512695 - trainLoss: 0.445126473903656\n",
      "cnt: 0 - valLoss: 0.44298458099365234 - trainLoss: 0.44512373208999634\n",
      "cnt: 0 - valLoss: 0.4429831802845001 - trainLoss: 0.44512102007865906\n",
      "cnt: 0 - valLoss: 0.44298169016838074 - trainLoss: 0.4451182186603546\n",
      "cnt: 0 - valLoss: 0.4429803192615509 - trainLoss: 0.44511544704437256\n",
      "cnt: 0 - valLoss: 0.4429788589477539 - trainLoss: 0.4451127052307129\n",
      "cnt: 0 - valLoss: 0.4429774284362793 - trainLoss: 0.44510993361473083\n",
      "cnt: 0 - valLoss: 0.44297587871551514 - trainLoss: 0.4451071321964264\n",
      "cnt: 0 - valLoss: 0.4429744482040405 - trainLoss: 0.4451043903827667\n",
      "cnt: 0 - valLoss: 0.44297298789024353 - trainLoss: 0.44510164856910706\n",
      "cnt: 0 - valLoss: 0.4429715573787689 - trainLoss: 0.4450989067554474\n",
      "cnt: 0 - valLoss: 0.4429699778556824 - trainLoss: 0.4450961947441101\n",
      "cnt: 0 - valLoss: 0.44296854734420776 - trainLoss: 0.4450933635234833\n",
      "cnt: 0 - valLoss: 0.44296711683273315 - trainLoss: 0.4450906217098236\n",
      "cnt: 0 - valLoss: 0.44296565651893616 - trainLoss: 0.44508787989616394\n",
      "cnt: 0 - valLoss: 0.442964106798172 - trainLoss: 0.4450851082801819\n",
      "cnt: 0 - valLoss: 0.4429626762866974 - trainLoss: 0.4450823664665222\n",
      "cnt: 0 - valLoss: 0.44296127557754517 - trainLoss: 0.44507962465286255\n",
      "cnt: 0 - valLoss: 0.44295987486839294 - trainLoss: 0.4450768232345581\n",
      "cnt: 0 - valLoss: 0.4429583251476288 - trainLoss: 0.44507408142089844\n",
      "cnt: 0 - valLoss: 0.4429568946361542 - trainLoss: 0.44507136940956116\n",
      "cnt: 0 - valLoss: 0.44295549392700195 - trainLoss: 0.4450685679912567\n",
      "cnt: 0 - valLoss: 0.44295406341552734 - trainLoss: 0.44506582617759705\n",
      "cnt: 0 - valLoss: 0.44295254349708557 - trainLoss: 0.445063054561615\n",
      "cnt: 0 - valLoss: 0.44295111298561096 - trainLoss: 0.4450603127479553\n",
      "cnt: 0 - valLoss: 0.44294971227645874 - trainLoss: 0.44505757093429565\n",
      "cnt: 0 - valLoss: 0.44294828176498413 - trainLoss: 0.445054829120636\n",
      "cnt: 0 - valLoss: 0.44294673204421997 - trainLoss: 0.4450520873069763\n",
      "cnt: 0 - valLoss: 0.44294533133506775 - trainLoss: 0.44504931569099426\n",
      "cnt: 0 - valLoss: 0.4429439306259155 - trainLoss: 0.4450465738773346\n",
      "cnt: 0 - valLoss: 0.4429425001144409 - trainLoss: 0.44504380226135254\n",
      "cnt: 0 - valLoss: 0.44294095039367676 - trainLoss: 0.44504106044769287\n",
      "cnt: 0 - valLoss: 0.44293954968452454 - trainLoss: 0.4450383186340332\n",
      "cnt: 0 - valLoss: 0.4429381787776947 - trainLoss: 0.44503557682037354\n",
      "cnt: 0 - valLoss: 0.44293680787086487 - trainLoss: 0.4450328052043915\n",
      "cnt: 0 - valLoss: 0.4429352581501007 - trainLoss: 0.4450300633907318\n",
      "cnt: 0 - valLoss: 0.4429338872432709 - trainLoss: 0.44502726197242737\n",
      "cnt: 0 - valLoss: 0.44293248653411865 - trainLoss: 0.4450245201587677\n",
      "cnt: 0 - valLoss: 0.44293108582496643 - trainLoss: 0.44502177834510803\n",
      "cnt: 0 - valLoss: 0.44292956590652466 - trainLoss: 0.44501906633377075\n",
      "cnt: 0 - valLoss: 0.4429281949996948 - trainLoss: 0.4450163245201111\n",
      "cnt: 0 - valLoss: 0.442926824092865 - trainLoss: 0.4450135827064514\n",
      "cnt: 0 - valLoss: 0.44292545318603516 - trainLoss: 0.445010781288147\n",
      "cnt: 0 - valLoss: 0.442923903465271 - trainLoss: 0.4450080394744873\n",
      "cnt: 0 - valLoss: 0.4429224729537964 - trainLoss: 0.44500526785850525\n",
      "cnt: 0 - valLoss: 0.44292110204696655 - trainLoss: 0.4450025260448456\n",
      "cnt: 0 - valLoss: 0.4429197907447815 - trainLoss: 0.4449997842311859\n",
      "cnt: 0 - valLoss: 0.44291824102401733 - trainLoss: 0.44499704241752625\n",
      "cnt: 0 - valLoss: 0.4429168701171875 - trainLoss: 0.4449943006038666\n",
      "cnt: 0 - valLoss: 0.44291549921035767 - trainLoss: 0.4449915587902069\n",
      "cnt: 0 - valLoss: 0.44291412830352783 - trainLoss: 0.44498878717422485\n",
      "cnt: 0 - valLoss: 0.44291257858276367 - trainLoss: 0.4449860155582428\n",
      "cnt: 0 - valLoss: 0.44291120767593384 - trainLoss: 0.4449833035469055\n",
      "cnt: 0 - valLoss: 0.442909836769104 - trainLoss: 0.44498053193092346\n",
      "cnt: 0 - valLoss: 0.4429084360599518 - trainLoss: 0.4449777901172638\n",
      "cnt: 0 - valLoss: 0.4429069757461548 - trainLoss: 0.4449750483036041\n",
      "cnt: 0 - valLoss: 0.4429055452346802 - trainLoss: 0.44497230648994446\n",
      "cnt: 0 - valLoss: 0.44290417432785034 - trainLoss: 0.4449695348739624\n",
      "cnt: 0 - valLoss: 0.44290271401405334 - trainLoss: 0.44496679306030273\n",
      "cnt: 0 - valLoss: 0.44290122389793396 - trainLoss: 0.44496405124664307\n",
      "cnt: 0 - valLoss: 0.44289982318878174 - trainLoss: 0.4449613690376282\n",
      "cnt: 0 - valLoss: 0.4428984522819519 - trainLoss: 0.44495853781700134\n",
      "cnt: 0 - valLoss: 0.4428970217704773 - trainLoss: 0.4449557960033417\n",
      "cnt: 0 - valLoss: 0.4428955316543579 - trainLoss: 0.444953054189682\n",
      "cnt: 0 - valLoss: 0.4428941011428833 - trainLoss: 0.44495031237602234\n",
      "cnt: 0 - valLoss: 0.4428927004337311 - trainLoss: 0.44494757056236267\n",
      "cnt: 0 - valLoss: 0.44289132952690125 - trainLoss: 0.444944828748703\n",
      "cnt: 0 - valLoss: 0.4428897798061371 - trainLoss: 0.44494208693504333\n",
      "cnt: 0 - valLoss: 0.44288840889930725 - trainLoss: 0.4449393153190613\n",
      "cnt: 0 - valLoss: 0.44288700819015503 - trainLoss: 0.4449365437030792\n",
      "cnt: 0 - valLoss: 0.4428855776786804 - trainLoss: 0.44493383169174194\n",
      "cnt: 0 - valLoss: 0.4428841173648834 - trainLoss: 0.4449310600757599\n",
      "cnt: 0 - valLoss: 0.4428826570510864 - trainLoss: 0.4449283480644226\n",
      "cnt: 0 - valLoss: 0.4428812563419342 - trainLoss: 0.44492557644844055\n",
      "cnt: 0 - valLoss: 0.4428797662258148 - trainLoss: 0.4449228346347809\n",
      "cnt: 0 - valLoss: 0.4428783357143402 - trainLoss: 0.4449200928211212\n",
      "cnt: 0 - valLoss: 0.44287699460983276 - trainLoss: 0.44491735100746155\n",
      "cnt: 0 - valLoss: 0.44287559390068054 - trainLoss: 0.4449146091938019\n",
      "cnt: 0 - valLoss: 0.44287410378456116 - trainLoss: 0.4449118673801422\n",
      "cnt: 0 - valLoss: 0.4428727328777313 - trainLoss: 0.44490912556648254\n",
      "cnt: 0 - valLoss: 0.4428713619709015 - trainLoss: 0.4449063837528229\n",
      "cnt: 0 - valLoss: 0.44286999106407166 - trainLoss: 0.4449036717414856\n",
      "cnt: 0 - valLoss: 0.4428684711456299 - trainLoss: 0.4449009299278259\n",
      "cnt: 0 - valLoss: 0.44286707043647766 - trainLoss: 0.44489818811416626\n",
      "cnt: 0 - valLoss: 0.4428656995296478 - trainLoss: 0.4448954463005066\n",
      "cnt: 0 - valLoss: 0.442864328622818 - trainLoss: 0.4448927044868469\n",
      "cnt: 0 - valLoss: 0.44286277890205383 - trainLoss: 0.44488996267318726\n",
      "cnt: 0 - valLoss: 0.442861407995224 - trainLoss: 0.4448871910572052\n",
      "cnt: 0 - valLoss: 0.4428600072860718 - trainLoss: 0.4448844790458679\n",
      "cnt: 0 - valLoss: 0.44285857677459717 - trainLoss: 0.44488173723220825\n",
      "cnt: 0 - valLoss: 0.44285711646080017 - trainLoss: 0.4448789954185486\n",
      "cnt: 0 - valLoss: 0.4428556561470032 - trainLoss: 0.44487622380256653\n",
      "cnt: 0 - valLoss: 0.44285428524017334 - trainLoss: 0.44487351179122925\n",
      "cnt: 0 - valLoss: 0.4428528845310211 - trainLoss: 0.44487079977989197\n",
      "cnt: 0 - valLoss: 0.4428515136241913 - trainLoss: 0.4448680579662323\n",
      "cnt: 0 - valLoss: 0.4428499937057495 - trainLoss: 0.44486531615257263\n",
      "cnt: 0 - valLoss: 0.4428485631942749 - trainLoss: 0.44486257433891296\n",
      "cnt: 0 - valLoss: 0.4428471624851227 - trainLoss: 0.4448598325252533\n",
      "cnt: 0 - valLoss: 0.44284579157829285 - trainLoss: 0.44485709071159363\n",
      "cnt: 0 - valLoss: 0.44284430146217346 - trainLoss: 0.4448543190956116\n",
      "cnt: 0 - valLoss: 0.44284290075302124 - trainLoss: 0.4448516070842743\n",
      "cnt: 0 - valLoss: 0.44284144043922424 - trainLoss: 0.4448488652706146\n",
      "cnt: 0 - valLoss: 0.44283998012542725 - trainLoss: 0.44484612345695496\n",
      "cnt: 0 - valLoss: 0.4428384304046631 - trainLoss: 0.4448433816432953\n",
      "cnt: 0 - valLoss: 0.44283705949783325 - trainLoss: 0.4448406398296356\n",
      "cnt: 0 - valLoss: 0.44283559918403625 - trainLoss: 0.44483792781829834\n",
      "cnt: 0 - valLoss: 0.44283419847488403 - trainLoss: 0.44483518600463867\n",
      "cnt: 0 - valLoss: 0.4428327679634094 - trainLoss: 0.444832444190979\n",
      "cnt: 0 - valLoss: 0.44283121824264526 - trainLoss: 0.4448297321796417\n",
      "cnt: 0 - valLoss: 0.44282978773117065 - trainLoss: 0.44482699036598206\n",
      "cnt: 0 - valLoss: 0.44282835721969604 - trainLoss: 0.4448242485523224\n",
      "cnt: 0 - valLoss: 0.4428269565105438 - trainLoss: 0.4448215663433075\n",
      "cnt: 0 - valLoss: 0.44282540678977966 - trainLoss: 0.4448188245296478\n",
      "cnt: 0 - valLoss: 0.44282394647598267 - trainLoss: 0.44481608271598816\n",
      "cnt: 0 - valLoss: 0.44282257556915283 - trainLoss: 0.4448133409023285\n",
      "cnt: 0 - valLoss: 0.44282111525535583 - trainLoss: 0.4448105990886688\n",
      "cnt: 0 - valLoss: 0.4428195655345917 - trainLoss: 0.44480785727500916\n",
      "cnt: 0 - valLoss: 0.4428181052207947 - trainLoss: 0.4448051154613495\n",
      "cnt: 0 - valLoss: 0.4428166449069977 - trainLoss: 0.4448024332523346\n",
      "cnt: 0 - valLoss: 0.44281521439552307 - trainLoss: 0.4447997510433197\n",
      "cnt: 0 - valLoss: 0.4428137540817261 - trainLoss: 0.44479700922966003\n",
      "cnt: 0 - valLoss: 0.4428122341632843 - trainLoss: 0.44479426741600037\n",
      "cnt: 0 - valLoss: 0.4428108334541321 - trainLoss: 0.4447915256023407\n",
      "cnt: 0 - valLoss: 0.4428093731403351 - trainLoss: 0.44478878378868103\n",
      "cnt: 0 - valLoss: 0.44280797243118286 - trainLoss: 0.44478604197502136\n",
      "cnt: 0 - valLoss: 0.44280654191970825 - trainLoss: 0.4447833299636841\n",
      "cnt: 0 - valLoss: 0.4428049921989441 - trainLoss: 0.4447806179523468\n",
      "cnt: 0 - valLoss: 0.44280362129211426 - trainLoss: 0.4447779059410095\n",
      "cnt: 0 - valLoss: 0.44280216097831726 - trainLoss: 0.44477516412734985\n",
      "cnt: 0 - valLoss: 0.44280076026916504 - trainLoss: 0.4447724223136902\n",
      "cnt: 0 - valLoss: 0.44279932975769043 - trainLoss: 0.4447696805000305\n",
      "cnt: 0 - valLoss: 0.44279780983924866 - trainLoss: 0.4447669982910156\n",
      "cnt: 0 - valLoss: 0.4427964389324188 - trainLoss: 0.44476428627967834\n",
      "cnt: 0 - valLoss: 0.4427949786186218 - trainLoss: 0.4447615146636963\n",
      "cnt: 0 - valLoss: 0.442793607711792 - trainLoss: 0.4447588324546814\n",
      "cnt: 0 - valLoss: 0.442792147397995 - trainLoss: 0.44475609064102173\n",
      "cnt: 0 - valLoss: 0.44279059767723083 - trainLoss: 0.44475337862968445\n",
      "cnt: 0 - valLoss: 0.442789226770401 - trainLoss: 0.4447506368160248\n",
      "cnt: 0 - valLoss: 0.44278785586357117 - trainLoss: 0.4447479546070099\n",
      "cnt: 0 - valLoss: 0.44278639554977417 - trainLoss: 0.44474515318870544\n",
      "cnt: 0 - valLoss: 0.44278502464294434 - trainLoss: 0.44474247097969055\n",
      "cnt: 0 - valLoss: 0.442783385515213 - trainLoss: 0.44473975896835327\n",
      "cnt: 0 - valLoss: 0.4427820146083832 - trainLoss: 0.4447369873523712\n",
      "cnt: 0 - valLoss: 0.44278058409690857 - trainLoss: 0.4447343051433563\n",
      "cnt: 0 - valLoss: 0.44277918338775635 - trainLoss: 0.44473159313201904\n",
      "cnt: 0 - valLoss: 0.44277775287628174 - trainLoss: 0.4447288513183594\n",
      "cnt: 0 - valLoss: 0.44277629256248474 - trainLoss: 0.4447261095046997\n",
      "cnt: 0 - valLoss: 0.44277483224868774 - trainLoss: 0.44472336769104004\n",
      "cnt: 0 - valLoss: 0.44277337193489075 - trainLoss: 0.44472068548202515\n",
      "cnt: 0 - valLoss: 0.4427720010280609 - trainLoss: 0.4447179436683655\n",
      "cnt: 0 - valLoss: 0.4427705407142639 - trainLoss: 0.4447152018547058\n",
      "cnt: 0 - valLoss: 0.44276905059814453 - trainLoss: 0.4447125196456909\n",
      "cnt: 0 - valLoss: 0.4427676200866699 - trainLoss: 0.44470977783203125\n",
      "cnt: 0 - valLoss: 0.4427661597728729 - trainLoss: 0.4447070360183716\n",
      "cnt: 0 - valLoss: 0.4427647888660431 - trainLoss: 0.4447042942047119\n",
      "cnt: 0 - valLoss: 0.44276338815689087 - trainLoss: 0.44470158219337463\n",
      "cnt: 0 - valLoss: 0.4427618086338043 - trainLoss: 0.44469889998435974\n",
      "cnt: 0 - valLoss: 0.4427604377269745 - trainLoss: 0.4446961581707001\n",
      "cnt: 0 - valLoss: 0.4427590072154999 - trainLoss: 0.4446934163570404\n",
      "cnt: 0 - valLoss: 0.44275760650634766 - trainLoss: 0.44469067454338074\n",
      "cnt: 0 - valLoss: 0.44275617599487305 - trainLoss: 0.44468799233436584\n",
      "cnt: 0 - valLoss: 0.44275468587875366 - trainLoss: 0.44468528032302856\n",
      "cnt: 0 - valLoss: 0.44275322556495667 - trainLoss: 0.4446825385093689\n",
      "cnt: 0 - valLoss: 0.44275179505348206 - trainLoss: 0.44467979669570923\n",
      "cnt: 0 - valLoss: 0.4427504241466522 - trainLoss: 0.44467711448669434\n",
      "cnt: 0 - valLoss: 0.4427490234375 - trainLoss: 0.44467437267303467\n",
      "cnt: 0 - valLoss: 0.44274747371673584 - trainLoss: 0.4446716606616974\n",
      "cnt: 0 - valLoss: 0.442746102809906 - trainLoss: 0.4446689486503601\n",
      "cnt: 0 - valLoss: 0.442744642496109 - trainLoss: 0.44466620683670044\n",
      "cnt: 0 - valLoss: 0.4427432715892792 - trainLoss: 0.44466349482536316\n",
      "cnt: 0 - valLoss: 0.44274190068244934 - trainLoss: 0.4446607530117035\n",
      "cnt: 0 - valLoss: 0.4427403509616852 - trainLoss: 0.4446580708026886\n",
      "cnt: 0 - valLoss: 0.44273895025253296 - trainLoss: 0.44465532898902893\n",
      "cnt: 0 - valLoss: 0.4427375793457031 - trainLoss: 0.44465258717536926\n",
      "cnt: 0 - valLoss: 0.4427361786365509 - trainLoss: 0.444649875164032\n",
      "cnt: 0 - valLoss: 0.44273462891578674 - trainLoss: 0.4446471631526947\n",
      "cnt: 0 - valLoss: 0.4427332580089569 - trainLoss: 0.4446444511413574\n",
      "cnt: 0 - valLoss: 0.4427318871021271 - trainLoss: 0.44464170932769775\n",
      "cnt: 0 - valLoss: 0.4427304267883301 - trainLoss: 0.4446389675140381\n",
      "cnt: 0 - valLoss: 0.44272905588150024 - trainLoss: 0.4446363151073456\n",
      "cnt: 0 - valLoss: 0.44272753596305847 - trainLoss: 0.4446335732936859\n",
      "cnt: 0 - valLoss: 0.44272613525390625 - trainLoss: 0.44463083148002625\n",
      "cnt: 0 - valLoss: 0.4427247643470764 - trainLoss: 0.4446280896663666\n",
      "cnt: 0 - valLoss: 0.4427233040332794 - trainLoss: 0.4446254074573517\n",
      "cnt: 0 - valLoss: 0.442721962928772 - trainLoss: 0.4446226954460144\n",
      "cnt: 0 - valLoss: 0.4427204430103302 - trainLoss: 0.4446199834346771\n",
      "cnt: 0 - valLoss: 0.4427190124988556 - trainLoss: 0.44461724162101746\n",
      "cnt: 0 - valLoss: 0.44271764159202576 - trainLoss: 0.4446144998073578\n",
      "cnt: 0 - valLoss: 0.44271624088287354 - trainLoss: 0.4446117877960205\n",
      "cnt: 0 - valLoss: 0.44271472096443176 - trainLoss: 0.4446091055870056\n",
      "cnt: 0 - valLoss: 0.44271320104599 - trainLoss: 0.44460639357566833\n",
      "cnt: 0 - valLoss: 0.44271183013916016 - trainLoss: 0.44460371136665344\n",
      "cnt: 0 - valLoss: 0.44271036982536316 - trainLoss: 0.4446009695529938\n",
      "cnt: 0 - valLoss: 0.44270890951156616 - trainLoss: 0.4445982277393341\n",
      "cnt: 0 - valLoss: 0.44270753860473633 - trainLoss: 0.4445955753326416\n",
      "cnt: 0 - valLoss: 0.4427059590816498 - trainLoss: 0.44459283351898193\n",
      "cnt: 0 - valLoss: 0.44270452857017517 - trainLoss: 0.44459015130996704\n",
      "cnt: 0 - valLoss: 0.44270312786102295 - trainLoss: 0.44458743929862976\n",
      "cnt: 0 - valLoss: 0.44270169734954834 - trainLoss: 0.4445846974849701\n",
      "cnt: 0 - valLoss: 0.44270023703575134 - trainLoss: 0.4445820152759552\n",
      "cnt: 0 - valLoss: 0.44269871711730957 - trainLoss: 0.4445793032646179\n",
      "cnt: 0 - valLoss: 0.44269728660583496 - trainLoss: 0.44457656145095825\n",
      "cnt: 0 - valLoss: 0.44269588589668274 - trainLoss: 0.44457387924194336\n",
      "cnt: 0 - valLoss: 0.44269445538520813 - trainLoss: 0.4445711374282837\n",
      "cnt: 0 - valLoss: 0.44269299507141113 - trainLoss: 0.4445684254169464\n",
      "cnt: 0 - valLoss: 0.442691445350647 - trainLoss: 0.4445657730102539\n",
      "cnt: 0 - valLoss: 0.44269004464149475 - trainLoss: 0.44456303119659424\n",
      "cnt: 0 - valLoss: 0.44268858432769775 - trainLoss: 0.44456028938293457\n",
      "cnt: 0 - valLoss: 0.44268718361854553 - trainLoss: 0.4445576071739197\n",
      "cnt: 0 - valLoss: 0.4426857531070709 - trainLoss: 0.4445549249649048\n",
      "cnt: 0 - valLoss: 0.44268426299095154 - trainLoss: 0.4445521831512451\n",
      "cnt: 0 - valLoss: 0.44268283247947693 - trainLoss: 0.4445495307445526\n",
      "cnt: 0 - valLoss: 0.4426814019680023 - trainLoss: 0.44454678893089294\n",
      "cnt: 0 - valLoss: 0.4426800012588501 - trainLoss: 0.4445440471172333\n",
      "cnt: 0 - valLoss: 0.4426784813404083 - trainLoss: 0.4445413053035736\n",
      "cnt: 0 - valLoss: 0.44267702102661133 - trainLoss: 0.4445386528968811\n",
      "cnt: 0 - valLoss: 0.4426756501197815 - trainLoss: 0.4445359408855438\n",
      "cnt: 0 - valLoss: 0.44267427921295166 - trainLoss: 0.44453325867652893\n",
      "cnt: 0 - valLoss: 0.44267281889915466 - trainLoss: 0.44453051686286926\n",
      "cnt: 0 - valLoss: 0.4426712691783905 - trainLoss: 0.444527804851532\n",
      "cnt: 0 - valLoss: 0.44266989827156067 - trainLoss: 0.4445251226425171\n",
      "cnt: 0 - valLoss: 0.44266843795776367 - trainLoss: 0.4445223808288574\n",
      "cnt: 0 - valLoss: 0.44266706705093384 - trainLoss: 0.4445197284221649\n",
      "cnt: 0 - valLoss: 0.442665696144104 - trainLoss: 0.44451698660850525\n",
      "cnt: 0 - valLoss: 0.44266414642333984 - trainLoss: 0.4445142447948456\n",
      "cnt: 0 - valLoss: 0.44266277551651 - trainLoss: 0.4445115923881531\n",
      "cnt: 0 - valLoss: 0.442661315202713 - trainLoss: 0.4445089101791382\n",
      "cnt: 0 - valLoss: 0.4426599442958832 - trainLoss: 0.4445061683654785\n",
      "cnt: 0 - valLoss: 0.44265854358673096 - trainLoss: 0.44450345635414124\n",
      "cnt: 0 - valLoss: 0.44265708327293396 - trainLoss: 0.44450077414512634\n",
      "cnt: 0 - valLoss: 0.4426555931568146 - trainLoss: 0.4444980323314667\n",
      "cnt: 0 - valLoss: 0.44265425205230713 - trainLoss: 0.4444953203201294\n",
      "cnt: 0 - valLoss: 0.4426528811454773 - trainLoss: 0.4444926381111145\n",
      "cnt: 0 - valLoss: 0.4426514804363251 - trainLoss: 0.4444899260997772\n",
      "cnt: 0 - valLoss: 0.4426499605178833 - trainLoss: 0.44448718428611755\n",
      "cnt: 0 - valLoss: 0.44264858961105347 - trainLoss: 0.44448453187942505\n",
      "cnt: 0 - valLoss: 0.44264718890190125 - trainLoss: 0.44448181986808777\n",
      "cnt: 0 - valLoss: 0.4426458179950714 - trainLoss: 0.4444791078567505\n",
      "cnt: 0 - valLoss: 0.4426443874835968 - trainLoss: 0.4444764256477356\n",
      "cnt: 0 - valLoss: 0.4426428973674774 - trainLoss: 0.4444736838340759\n",
      "cnt: 0 - valLoss: 0.4426415264606476 - trainLoss: 0.4444710314273834\n",
      "cnt: 0 - valLoss: 0.44264015555381775 - trainLoss: 0.44446828961372375\n",
      "cnt: 0 - valLoss: 0.44263869524002075 - trainLoss: 0.4444655776023865\n",
      "cnt: 0 - valLoss: 0.44263723492622375 - trainLoss: 0.4444628953933716\n",
      "cnt: 0 - valLoss: 0.4426358640193939 - trainLoss: 0.4444601833820343\n",
      "cnt: 0 - valLoss: 0.4426344633102417 - trainLoss: 0.4444575011730194\n",
      "cnt: 0 - valLoss: 0.4426330327987671 - trainLoss: 0.44445475935935974\n",
      "cnt: 0 - valLoss: 0.44263166189193726 - trainLoss: 0.44445204734802246\n",
      "cnt: 0 - valLoss: 0.44263017177581787 - trainLoss: 0.44444936513900757\n",
      "cnt: 0 - valLoss: 0.4426288306713104 - trainLoss: 0.4444466233253479\n",
      "cnt: 0 - valLoss: 0.4426274299621582 - trainLoss: 0.4444439709186554\n",
      "cnt: 0 - valLoss: 0.44262608885765076 - trainLoss: 0.4444412291049957\n",
      "cnt: 0 - valLoss: 0.4426247179508209 - trainLoss: 0.4444385766983032\n",
      "cnt: 0 - valLoss: 0.44262316823005676 - trainLoss: 0.44443583488464355\n",
      "cnt: 0 - valLoss: 0.44262179732322693 - trainLoss: 0.44443315267562866\n",
      "cnt: 0 - valLoss: 0.4426204264163971 - trainLoss: 0.4444304406642914\n",
      "cnt: 0 - valLoss: 0.4426190257072449 - trainLoss: 0.4444276988506317\n",
      "cnt: 0 - valLoss: 0.44261765480041504 - trainLoss: 0.4444250464439392\n",
      "cnt: 0 - valLoss: 0.44261619448661804 - trainLoss: 0.44442230463027954\n",
      "cnt: 0 - valLoss: 0.4426147937774658 - trainLoss: 0.44441962242126465\n",
      "cnt: 0 - valLoss: 0.4426134526729584 - trainLoss: 0.44441691040992737\n",
      "cnt: 0 - valLoss: 0.44261208176612854 - trainLoss: 0.4444142282009125\n",
      "cnt: 0 - valLoss: 0.4426107108592987 - trainLoss: 0.4444115161895752\n",
      "cnt: 0 - valLoss: 0.4426092207431793 - trainLoss: 0.4444087743759155\n",
      "cnt: 0 - valLoss: 0.4426078498363495 - trainLoss: 0.444406121969223\n",
      "cnt: 0 - valLoss: 0.44260647892951965 - trainLoss: 0.44440338015556335\n",
      "cnt: 0 - valLoss: 0.4426051080226898 - trainLoss: 0.44440072774887085\n",
      "cnt: 0 - valLoss: 0.4426036477088928 - trainLoss: 0.44439804553985596\n",
      "cnt: 0 - valLoss: 0.442602276802063 - trainLoss: 0.4443953037261963\n",
      "cnt: 0 - valLoss: 0.44260090589523315 - trainLoss: 0.4443926215171814\n",
      "cnt: 0 - valLoss: 0.4425995349884033 - trainLoss: 0.4443899095058441\n",
      "cnt: 0 - valLoss: 0.4425981044769287 - trainLoss: 0.4443872272968292\n",
      "cnt: 0 - valLoss: 0.4425966441631317 - trainLoss: 0.44438448548316956\n",
      "cnt: 0 - valLoss: 0.4425952434539795 - trainLoss: 0.44438183307647705\n",
      "cnt: 0 - valLoss: 0.4425938129425049 - trainLoss: 0.4443790912628174\n",
      "cnt: 0 - valLoss: 0.44259244203567505 - trainLoss: 0.4443764388561249\n",
      "cnt: 0 - valLoss: 0.4425910711288452 - trainLoss: 0.4443737268447876\n",
      "cnt: 0 - valLoss: 0.44258975982666016 - trainLoss: 0.4443710446357727\n",
      "cnt: 0 - valLoss: 0.4425882399082184 - trainLoss: 0.44436830282211304\n",
      "cnt: 0 - valLoss: 0.44258683919906616 - trainLoss: 0.44436565041542053\n",
      "cnt: 0 - valLoss: 0.4425854980945587 - trainLoss: 0.44436293840408325\n",
      "cnt: 0 - valLoss: 0.4425840973854065 - trainLoss: 0.44436025619506836\n",
      "cnt: 0 - valLoss: 0.44258272647857666 - trainLoss: 0.4443575143814087\n",
      "cnt: 0 - valLoss: 0.4425813555717468 - trainLoss: 0.4443548619747162\n",
      "cnt: 0 - valLoss: 0.44257989525794983 - trainLoss: 0.4443521201610565\n",
      "cnt: 0 - valLoss: 0.44257843494415283 - trainLoss: 0.44434940814971924\n",
      "cnt: 0 - valLoss: 0.4425770938396454 - trainLoss: 0.44434672594070435\n",
      "cnt: 0 - valLoss: 0.44257572293281555 - trainLoss: 0.44434407353401184\n",
      "cnt: 0 - valLoss: 0.44257432222366333 - trainLoss: 0.4443413317203522\n",
      "cnt: 0 - valLoss: 0.44257286190986633 - trainLoss: 0.4443386197090149\n",
      "cnt: 0 - valLoss: 0.4425714910030365 - trainLoss: 0.4443359375\n",
      "cnt: 0 - valLoss: 0.4425700902938843 - trainLoss: 0.4443332850933075\n",
      "cnt: 0 - valLoss: 0.44256874918937683 - trainLoss: 0.4443305432796478\n",
      "cnt: 0 - valLoss: 0.442567378282547 - trainLoss: 0.4443278908729553\n",
      "cnt: 0 - valLoss: 0.44256600737571716 - trainLoss: 0.44432514905929565\n",
      "cnt: 0 - valLoss: 0.4425645172595978 - trainLoss: 0.44432246685028076\n",
      "cnt: 0 - valLoss: 0.44256317615509033 - trainLoss: 0.4443197548389435\n",
      "cnt: 0 - valLoss: 0.44256171584129333 - trainLoss: 0.4443170726299286\n",
      "cnt: 0 - valLoss: 0.4425603449344635 - trainLoss: 0.4443143606185913\n",
      "cnt: 0 - valLoss: 0.44255897402763367 - trainLoss: 0.4443116784095764\n",
      "cnt: 0 - valLoss: 0.44255751371383667 - trainLoss: 0.44430896639823914\n",
      "cnt: 0 - valLoss: 0.44255614280700684 - trainLoss: 0.44430622458457947\n",
      "cnt: 0 - valLoss: 0.442554771900177 - trainLoss: 0.44430357217788696\n",
      "cnt: 0 - valLoss: 0.44255340099334717 - trainLoss: 0.44430088996887207\n",
      "cnt: 0 - valLoss: 0.44255203008651733 - trainLoss: 0.4442981779575348\n",
      "cnt: 0 - valLoss: 0.4425506591796875 - trainLoss: 0.4442954361438751\n",
      "cnt: 0 - valLoss: 0.4425491988658905 - trainLoss: 0.4442927837371826\n",
      "cnt: 0 - valLoss: 0.44254782795906067 - trainLoss: 0.44429004192352295\n",
      "cnt: 0 - valLoss: 0.44254645705223083 - trainLoss: 0.44428738951683044\n",
      "cnt: 0 - valLoss: 0.4425451159477234 - trainLoss: 0.44428470730781555\n",
      "cnt: 0 - valLoss: 0.44254380464553833 - trainLoss: 0.44428199529647827\n",
      "cnt: 0 - valLoss: 0.4425424337387085 - trainLoss: 0.4442793130874634\n",
      "cnt: 0 - valLoss: 0.4425409734249115 - trainLoss: 0.4442766010761261\n",
      "cnt: 0 - valLoss: 0.44253960251808167 - trainLoss: 0.4442739188671112\n",
      "cnt: 0 - valLoss: 0.44253823161125183 - trainLoss: 0.4442712068557739\n",
      "cnt: 0 - valLoss: 0.442536860704422 - trainLoss: 0.44426852464675903\n",
      "cnt: 0 - valLoss: 0.4425355792045593 - trainLoss: 0.44426584243774414\n",
      "cnt: 0 - valLoss: 0.44253402948379517 - trainLoss: 0.44426316022872925\n",
      "cnt: 0 - valLoss: 0.44253265857696533 - trainLoss: 0.44426044821739197\n",
      "cnt: 0 - valLoss: 0.4425313174724579 - trainLoss: 0.44425779581069946\n",
      "cnt: 0 - valLoss: 0.4425300061702728 - trainLoss: 0.4442550539970398\n",
      "cnt: 0 - valLoss: 0.4425286650657654 - trainLoss: 0.4442524015903473\n",
      "cnt: 0 - valLoss: 0.44252729415893555 - trainLoss: 0.4442496597766876\n",
      "cnt: 0 - valLoss: 0.44252583384513855 - trainLoss: 0.4442470073699951\n",
      "cnt: 0 - valLoss: 0.4425245225429535 - trainLoss: 0.44424429535865784\n",
      "cnt: 0 - valLoss: 0.44252315163612366 - trainLoss: 0.44424161314964294\n",
      "cnt: 0 - valLoss: 0.4425217807292938 - trainLoss: 0.44423890113830566\n",
      "cnt: 0 - valLoss: 0.44252049922943115 - trainLoss: 0.44423621892929077\n",
      "cnt: 0 - valLoss: 0.4425189793109894 - trainLoss: 0.44423356652259827\n",
      "cnt: 0 - valLoss: 0.4425176680088043 - trainLoss: 0.4442308247089386\n",
      "cnt: 0 - valLoss: 0.4425162971019745 - trainLoss: 0.4442281723022461\n",
      "cnt: 0 - valLoss: 0.44251492619514465 - trainLoss: 0.4442254304885864\n",
      "cnt: 0 - valLoss: 0.4425135552883148 - trainLoss: 0.4442227780818939\n",
      "cnt: 0 - valLoss: 0.4425121545791626 - trainLoss: 0.44422003626823425\n",
      "cnt: 0 - valLoss: 0.4425106644630432 - trainLoss: 0.44421738386154175\n",
      "cnt: 0 - valLoss: 0.442509263753891 - trainLoss: 0.44421467185020447\n",
      "cnt: 0 - valLoss: 0.4425078332424164 - trainLoss: 0.4442119896411896\n",
      "cnt: 0 - valLoss: 0.4425065219402313 - trainLoss: 0.44420933723449707\n",
      "cnt: 0 - valLoss: 0.4425050616264343 - trainLoss: 0.4442066252231598\n",
      "cnt: 0 - valLoss: 0.4425036311149597 - trainLoss: 0.4442039430141449\n",
      "cnt: 0 - valLoss: 0.44250214099884033 - trainLoss: 0.4442012906074524\n",
      "cnt: 0 - valLoss: 0.4425007700920105 - trainLoss: 0.4441986382007599\n",
      "cnt: 0 - valLoss: 0.4424993097782135 - trainLoss: 0.4441958963871002\n",
      "cnt: 0 - valLoss: 0.44249793887138367 - trainLoss: 0.44419318437576294\n",
      "cnt: 0 - valLoss: 0.44249656796455383 - trainLoss: 0.44419053196907043\n",
      "cnt: 0 - valLoss: 0.4424951672554016 - trainLoss: 0.44418784976005554\n",
      "cnt: 0 - valLoss: 0.442493736743927 - trainLoss: 0.44418519735336304\n",
      "cnt: 0 - valLoss: 0.44249227643013 - trainLoss: 0.44418245553970337\n",
      "cnt: 0 - valLoss: 0.442490816116333 - trainLoss: 0.44417980313301086\n",
      "cnt: 0 - valLoss: 0.4424894452095032 - trainLoss: 0.4441770911216736\n",
      "cnt: 0 - valLoss: 0.44248804450035095 - trainLoss: 0.4441744089126587\n",
      "cnt: 0 - valLoss: 0.4424866735935211 - trainLoss: 0.4441717565059662\n",
      "cnt: 0 - valLoss: 0.4424853026866913 - trainLoss: 0.4441690444946289\n",
      "cnt: 0 - valLoss: 0.4424837529659271 - trainLoss: 0.444166362285614\n",
      "cnt: 0 - valLoss: 0.4424824118614197 - trainLoss: 0.4441637098789215\n",
      "cnt: 0 - valLoss: 0.4424809515476227 - trainLoss: 0.44416099786758423\n",
      "cnt: 0 - valLoss: 0.44247958064079285 - trainLoss: 0.44415831565856934\n",
      "cnt: 0 - valLoss: 0.442478209733963 - trainLoss: 0.44415560364723206\n",
      "cnt: 0 - valLoss: 0.4424768388271332 - trainLoss: 0.44415292143821716\n",
      "cnt: 0 - valLoss: 0.44247543811798096 - trainLoss: 0.44415026903152466\n",
      "cnt: 0 - valLoss: 0.4424739480018616 - trainLoss: 0.4441475570201874\n",
      "cnt: 0 - valLoss: 0.44247257709503174 - trainLoss: 0.4441448748111725\n",
      "cnt: 0 - valLoss: 0.44247111678123474 - trainLoss: 0.44414222240448\n",
      "cnt: 0 - valLoss: 0.4424697458744049 - trainLoss: 0.4441395103931427\n",
      "cnt: 0 - valLoss: 0.44246840476989746 - trainLoss: 0.4441368281841278\n",
      "cnt: 0 - valLoss: 0.4424670338630676 - trainLoss: 0.4441341757774353\n",
      "cnt: 0 - valLoss: 0.44246554374694824 - trainLoss: 0.444131463766098\n",
      "cnt: 0 - valLoss: 0.4424641728401184 - trainLoss: 0.44412878155708313\n",
      "cnt: 0 - valLoss: 0.4424628019332886 - trainLoss: 0.4441261291503906\n",
      "cnt: 0 - valLoss: 0.44246143102645874 - trainLoss: 0.4441234767436981\n",
      "cnt: 0 - valLoss: 0.44246000051498413 - trainLoss: 0.44412073493003845\n",
      "cnt: 0 - valLoss: 0.4424586892127991 - trainLoss: 0.44411808252334595\n",
      "cnt: 0 - valLoss: 0.44245731830596924 - trainLoss: 0.44411543011665344\n",
      "cnt: 0 - valLoss: 0.4424557685852051 - trainLoss: 0.44411271810531616\n",
      "cnt: 0 - valLoss: 0.44245442748069763 - trainLoss: 0.44411003589630127\n",
      "cnt: 0 - valLoss: 0.4424531161785126 - trainLoss: 0.44410738348960876\n",
      "cnt: 0 - valLoss: 0.44245174527168274 - trainLoss: 0.4441046714782715\n",
      "cnt: 0 - valLoss: 0.4424503743648529 - trainLoss: 0.4441019892692566\n",
      "cnt: 0 - valLoss: 0.4424489438533783 - trainLoss: 0.4440993368625641\n",
      "cnt: 0 - valLoss: 0.4424475431442261 - trainLoss: 0.4440966248512268\n",
      "cnt: 0 - valLoss: 0.44244617223739624 - trainLoss: 0.4440939426422119\n",
      "cnt: 0 - valLoss: 0.4424448013305664 - trainLoss: 0.4440912902355194\n",
      "cnt: 0 - valLoss: 0.4424434304237366 - trainLoss: 0.4440886378288269\n",
      "cnt: 0 - valLoss: 0.4424420893192291 - trainLoss: 0.44408589601516724\n",
      "cnt: 0 - valLoss: 0.4424407482147217 - trainLoss: 0.44408324360847473\n",
      "cnt: 0 - valLoss: 0.44243937730789185 - trainLoss: 0.4440805912017822\n",
      "cnt: 0 - valLoss: 0.44243788719177246 - trainLoss: 0.44407784938812256\n",
      "cnt: 0 - valLoss: 0.4424365162849426 - trainLoss: 0.44407519698143005\n",
      "cnt: 0 - valLoss: 0.4424351751804352 - trainLoss: 0.4440724849700928\n",
      "cnt: 0 - valLoss: 0.44243374466896057 - trainLoss: 0.44406983256340027\n",
      "cnt: 0 - valLoss: 0.44243237376213074 - trainLoss: 0.4440671503543854\n",
      "cnt: 0 - valLoss: 0.4424310028553009 - trainLoss: 0.44406449794769287\n",
      "cnt: 0 - valLoss: 0.44242945313453674 - trainLoss: 0.4440617859363556\n",
      "cnt: 0 - valLoss: 0.4424280822277069 - trainLoss: 0.4440591037273407\n",
      "cnt: 0 - valLoss: 0.4424267113208771 - trainLoss: 0.4440564513206482\n",
      "cnt: 0 - valLoss: 0.4424252510070801 - trainLoss: 0.4440537989139557\n",
      "cnt: 0 - valLoss: 0.44242388010025024 - trainLoss: 0.4440510869026184\n",
      "cnt: 0 - valLoss: 0.44242244958877563 - trainLoss: 0.4440484344959259\n",
      "cnt: 0 - valLoss: 0.44242095947265625 - trainLoss: 0.444045752286911\n",
      "cnt: 0 - valLoss: 0.4424195885658264 - trainLoss: 0.4440430998802185\n",
      "cnt: 0 - valLoss: 0.4424181282520294 - trainLoss: 0.444040447473526\n",
      "cnt: 0 - valLoss: 0.4424167573451996 - trainLoss: 0.4440377354621887\n",
      "cnt: 0 - valLoss: 0.4424152970314026 - trainLoss: 0.44403505325317383\n",
      "cnt: 0 - valLoss: 0.4424138367176056 - trainLoss: 0.4440324008464813\n",
      "cnt: 0 - valLoss: 0.44241246581077576 - trainLoss: 0.4440297484397888\n",
      "cnt: 0 - valLoss: 0.4424109160900116 - trainLoss: 0.4440270662307739\n",
      "cnt: 0 - valLoss: 0.4424095153808594 - trainLoss: 0.44402438402175903\n",
      "cnt: 0 - valLoss: 0.44240811467170715 - trainLoss: 0.44402173161506653\n",
      "cnt: 0 - valLoss: 0.4424067437648773 - trainLoss: 0.44401901960372925\n",
      "cnt: 0 - valLoss: 0.4424052834510803 - trainLoss: 0.44401636719703674\n",
      "cnt: 0 - valLoss: 0.4424039125442505 - trainLoss: 0.44401371479034424\n",
      "cnt: 0 - valLoss: 0.44240236282348633 - trainLoss: 0.44401106238365173\n",
      "cnt: 0 - valLoss: 0.4424009919166565 - trainLoss: 0.44400838017463684\n",
      "cnt: 0 - valLoss: 0.44239962100982666 - trainLoss: 0.44400572776794434\n",
      "cnt: 0 - valLoss: 0.44239816069602966 - trainLoss: 0.44400301575660706\n",
      "cnt: 0 - valLoss: 0.44239678978919983 - trainLoss: 0.44400036334991455\n",
      "cnt: 0 - valLoss: 0.4423953592777252 - trainLoss: 0.44399771094322205\n",
      "cnt: 0 - valLoss: 0.44239383935928345 - trainLoss: 0.44399505853652954\n",
      "cnt: 0 - valLoss: 0.4423924684524536 - trainLoss: 0.44399237632751465\n",
      "cnt: 0 - valLoss: 0.442391037940979 - trainLoss: 0.44398966431617737\n",
      "cnt: 0 - valLoss: 0.44238966703414917 - trainLoss: 0.44398701190948486\n",
      "cnt: 0 - valLoss: 0.44238826632499695 - trainLoss: 0.44398435950279236\n",
      "cnt: 0 - valLoss: 0.44238683581352234 - trainLoss: 0.44398170709609985\n",
      "cnt: 0 - valLoss: 0.4423854649066925 - trainLoss: 0.44397902488708496\n",
      "cnt: 0 - valLoss: 0.4423839747905731 - trainLoss: 0.44397637248039246\n",
      "cnt: 0 - valLoss: 0.4423825442790985 - trainLoss: 0.44397372007369995\n",
      "cnt: 0 - valLoss: 0.4423811733722687 - trainLoss: 0.44397100806236267\n",
      "cnt: 0 - valLoss: 0.44237977266311646 - trainLoss: 0.44396835565567017\n",
      "cnt: 0 - valLoss: 0.44237828254699707 - trainLoss: 0.44396570324897766\n",
      "cnt: 0 - valLoss: 0.4423769414424896 - trainLoss: 0.44396305084228516\n",
      "cnt: 0 - valLoss: 0.44237545132637024 - trainLoss: 0.4439603388309479\n",
      "cnt: 0 - valLoss: 0.4423740804195404 - trainLoss: 0.443957656621933\n",
      "cnt: 0 - valLoss: 0.44237270951271057 - trainLoss: 0.4439550042152405\n",
      "cnt: 0 - valLoss: 0.44237133860588074 - trainLoss: 0.443952351808548\n",
      "cnt: 0 - valLoss: 0.4423699676990509 - trainLoss: 0.44394969940185547\n",
      "cnt: 0 - valLoss: 0.44236859679222107 - trainLoss: 0.44394704699516296\n",
      "cnt: 0 - valLoss: 0.4423670470714569 - trainLoss: 0.4439443051815033\n",
      "cnt: 0 - valLoss: 0.4423656761646271 - trainLoss: 0.4439416825771332\n",
      "cnt: 0 - valLoss: 0.44236430525779724 - trainLoss: 0.4439390301704407\n",
      "cnt: 0 - valLoss: 0.44236284494400024 - trainLoss: 0.4439363479614258\n",
      "cnt: 0 - valLoss: 0.44236141443252563 - trainLoss: 0.4439336955547333\n",
      "cnt: 0 - valLoss: 0.4423601031303406 - trainLoss: 0.44393104314804077\n",
      "cnt: 0 - valLoss: 0.44235873222351074 - trainLoss: 0.44392839074134827\n",
      "cnt: 0 - valLoss: 0.44235721230506897 - trainLoss: 0.443925678730011\n",
      "cnt: 0 - valLoss: 0.44235581159591675 - trainLoss: 0.4439229965209961\n",
      "cnt: 0 - valLoss: 0.4423544406890869 - trainLoss: 0.443920373916626\n",
      "cnt: 0 - valLoss: 0.44235309958457947 - trainLoss: 0.4439176917076111\n",
      "cnt: 0 - valLoss: 0.44235163927078247 - trainLoss: 0.4439150393009186\n",
      "cnt: 0 - valLoss: 0.44235023856163025 - trainLoss: 0.4439123868942261\n",
      "cnt: 0 - valLoss: 0.44234877824783325 - trainLoss: 0.4439097046852112\n",
      "cnt: 0 - valLoss: 0.4423474073410034 - trainLoss: 0.4439070224761963\n",
      "cnt: 0 - valLoss: 0.44234606623649597 - trainLoss: 0.4439043700695038\n",
      "cnt: 0 - valLoss: 0.44234466552734375 - trainLoss: 0.4439016580581665\n",
      "cnt: 0 - valLoss: 0.4423432946205139 - trainLoss: 0.443899005651474\n",
      "cnt: 0 - valLoss: 0.4423419237136841 - trainLoss: 0.4438963532447815\n",
      "cnt: 0 - valLoss: 0.4423403739929199 - trainLoss: 0.443893700838089\n",
      "cnt: 0 - valLoss: 0.4423390030860901 - trainLoss: 0.4438910484313965\n",
      "cnt: 0 - valLoss: 0.4423375427722931 - trainLoss: 0.443888396024704\n",
      "cnt: 0 - valLoss: 0.44233623147010803 - trainLoss: 0.4438857436180115\n",
      "cnt: 0 - valLoss: 0.4423348009586334 - trainLoss: 0.44388309121131897\n",
      "cnt: 0 - valLoss: 0.4423334300518036 - trainLoss: 0.44388043880462646\n",
      "cnt: 0 - valLoss: 0.44233188033103943 - trainLoss: 0.44387778639793396\n",
      "cnt: 0 - valLoss: 0.4423305094242096 - trainLoss: 0.44387510418891907\n",
      "cnt: 0 - valLoss: 0.4423291087150574 - trainLoss: 0.4438723921775818\n",
      "cnt: 0 - valLoss: 0.44232773780822754 - trainLoss: 0.4438697397708893\n",
      "cnt: 0 - valLoss: 0.4423263669013977 - trainLoss: 0.44386711716651917\n",
      "cnt: 0 - valLoss: 0.4423249065876007 - trainLoss: 0.4438644349575043\n",
      "cnt: 0 - valLoss: 0.4423235356807709 - trainLoss: 0.44386178255081177\n",
      "cnt: 0 - valLoss: 0.4423220455646515 - trainLoss: 0.44385913014411926\n",
      "cnt: 0 - valLoss: 0.44232067465782166 - trainLoss: 0.44385647773742676\n",
      "cnt: 0 - valLoss: 0.4423193037509918 - trainLoss: 0.44385382533073425\n",
      "cnt: 0 - valLoss: 0.4423178732395172 - trainLoss: 0.44385117292404175\n",
      "cnt: 0 - valLoss: 0.4423165023326874 - trainLoss: 0.44384846091270447\n",
      "cnt: 0 - valLoss: 0.44231513142585754 - trainLoss: 0.44384580850601196\n",
      "cnt: 0 - valLoss: 0.4423135817050934 - trainLoss: 0.44384312629699707\n",
      "cnt: 0 - valLoss: 0.44231218099594116 - trainLoss: 0.44384050369262695\n",
      "cnt: 0 - valLoss: 0.44231081008911133 - trainLoss: 0.44383785128593445\n",
      "cnt: 0 - valLoss: 0.4423094391822815 - trainLoss: 0.44383516907691956\n",
      "cnt: 0 - valLoss: 0.44230809807777405 - trainLoss: 0.44383251667022705\n",
      "cnt: 0 - valLoss: 0.4423066973686218 - trainLoss: 0.44382986426353455\n",
      "cnt: 0 - valLoss: 0.44230523705482483 - trainLoss: 0.44382721185684204\n",
      "cnt: 0 - valLoss: 0.442303866147995 - trainLoss: 0.4438245892524719\n",
      "cnt: 0 - valLoss: 0.442302405834198 - trainLoss: 0.44382187724113464\n",
      "cnt: 0 - valLoss: 0.44230103492736816 - trainLoss: 0.44381922483444214\n",
      "cnt: 0 - valLoss: 0.44229966402053833 - trainLoss: 0.44381654262542725\n",
      "cnt: 0 - valLoss: 0.4422983229160309 - trainLoss: 0.44381392002105713\n",
      "cnt: 0 - valLoss: 0.4422968327999115 - trainLoss: 0.4438112676143646\n",
      "cnt: 0 - valLoss: 0.44229546189308167 - trainLoss: 0.4438086152076721\n",
      "cnt: 0 - valLoss: 0.44229403138160706 - trainLoss: 0.4438059628009796\n",
      "cnt: 0 - valLoss: 0.442292720079422 - trainLoss: 0.4438033103942871\n",
      "cnt: 0 - valLoss: 0.442291259765625 - trainLoss: 0.4438006281852722\n",
      "cnt: 0 - valLoss: 0.44228988885879517 - trainLoss: 0.4437979757785797\n",
      "cnt: 0 - valLoss: 0.44228842854499817 - trainLoss: 0.4437953233718872\n",
      "cnt: 0 - valLoss: 0.44228705763816833 - trainLoss: 0.4437926709651947\n",
      "cnt: 0 - valLoss: 0.4422857165336609 - trainLoss: 0.4437899887561798\n",
      "cnt: 0 - valLoss: 0.44228431582450867 - trainLoss: 0.4437873065471649\n",
      "cnt: 0 - valLoss: 0.4422829747200012 - trainLoss: 0.4437846541404724\n",
      "cnt: 0 - valLoss: 0.4422816038131714 - trainLoss: 0.4437820017337799\n",
      "cnt: 0 - valLoss: 0.44228020310401917 - trainLoss: 0.4437793493270874\n",
      "cnt: 0 - valLoss: 0.44227874279022217 - trainLoss: 0.4437766969203949\n",
      "cnt: 0 - valLoss: 0.44227737188339233 - trainLoss: 0.4437740445137024\n",
      "cnt: 0 - valLoss: 0.4422760307788849 - trainLoss: 0.4437713921070099\n",
      "cnt: 0 - valLoss: 0.44227471947669983 - trainLoss: 0.4437687397003174\n",
      "cnt: 0 - valLoss: 0.4422732889652252 - trainLoss: 0.4437660872936249\n",
      "cnt: 0 - valLoss: 0.44227197766304016 - trainLoss: 0.4437634348869324\n",
      "cnt: 0 - valLoss: 0.44227051734924316 - trainLoss: 0.44376078248023987\n",
      "cnt: 0 - valLoss: 0.44226908683776855 - trainLoss: 0.44375813007354736\n",
      "cnt: 0 - valLoss: 0.4422677159309387 - trainLoss: 0.44375544786453247\n",
      "cnt: 0 - valLoss: 0.44226640462875366 - trainLoss: 0.44375279545783997\n",
      "cnt: 0 - valLoss: 0.44226503372192383 - trainLoss: 0.44375014305114746\n",
      "cnt: 0 - valLoss: 0.4422636926174164 - trainLoss: 0.44374749064445496\n",
      "cnt: 0 - valLoss: 0.442262202501297 - trainLoss: 0.44374483823776245\n",
      "cnt: 0 - valLoss: 0.44226083159446716 - trainLoss: 0.44374218583106995\n",
      "cnt: 0 - valLoss: 0.44225946068763733 - trainLoss: 0.44373953342437744\n",
      "cnt: 0 - valLoss: 0.4422581195831299 - trainLoss: 0.44373688101768494\n",
      "cnt: 0 - valLoss: 0.4422568082809448 - trainLoss: 0.44373422861099243\n",
      "cnt: 0 - valLoss: 0.4422553777694702 - trainLoss: 0.4437315762042999\n",
      "cnt: 0 - valLoss: 0.442253977060318 - trainLoss: 0.4437289237976074\n",
      "cnt: 0 - valLoss: 0.44225260615348816 - trainLoss: 0.4437262713909149\n",
      "cnt: 0 - valLoss: 0.4422512352466583 - trainLoss: 0.4437236189842224\n",
      "cnt: 0 - valLoss: 0.4422498941421509 - trainLoss: 0.4437209665775299\n",
      "cnt: 0 - valLoss: 0.44224849343299866 - trainLoss: 0.4437182545661926\n",
      "cnt: 0 - valLoss: 0.4422471523284912 - trainLoss: 0.44371557235717773\n",
      "cnt: 0 - valLoss: 0.442245751619339 - trainLoss: 0.4437130093574524\n",
      "cnt: 0 - valLoss: 0.44224438071250916 - trainLoss: 0.4437102973461151\n",
      "cnt: 0 - valLoss: 0.4422430098056793 - trainLoss: 0.4437076151371002\n",
      "cnt: 0 - valLoss: 0.4422416687011719 - trainLoss: 0.4437049925327301\n",
      "cnt: 0 - valLoss: 0.44224026799201965 - trainLoss: 0.4437023401260376\n",
      "cnt: 0 - valLoss: 0.4422388970851898 - trainLoss: 0.44369974732398987\n",
      "cnt: 0 - valLoss: 0.4422374665737152 - trainLoss: 0.44369709491729736\n",
      "cnt: 0 - valLoss: 0.4422360956668854 - trainLoss: 0.4436943829059601\n",
      "cnt: 0 - valLoss: 0.44223475456237793 - trainLoss: 0.4436917304992676\n",
      "cnt: 0 - valLoss: 0.4422333836555481 - trainLoss: 0.4436890184879303\n",
      "cnt: 0 - valLoss: 0.44223207235336304 - trainLoss: 0.44368642568588257\n",
      "cnt: 0 - valLoss: 0.4422307312488556 - trainLoss: 0.4436837136745453\n",
      "cnt: 0 - valLoss: 0.4422292411327362 - trainLoss: 0.4436810612678528\n",
      "cnt: 0 - valLoss: 0.44222787022590637 - trainLoss: 0.44367846846580505\n",
      "cnt: 0 - valLoss: 0.4422265291213989 - trainLoss: 0.44367581605911255\n",
      "cnt: 0 - valLoss: 0.44222521781921387 - trainLoss: 0.44367310404777527\n",
      "cnt: 0 - valLoss: 0.4422238767147064 - trainLoss: 0.44367045164108276\n",
      "cnt: 0 - valLoss: 0.4422225058078766 - trainLoss: 0.44366779923439026\n",
      "cnt: 0 - valLoss: 0.4422210454940796 - trainLoss: 0.44366520643234253\n",
      "cnt: 0 - valLoss: 0.44221967458724976 - trainLoss: 0.44366252422332764\n",
      "cnt: 0 - valLoss: 0.4422183930873871 - trainLoss: 0.44365984201431274\n",
      "cnt: 0 - valLoss: 0.44221702218055725 - trainLoss: 0.4436572194099426\n",
      "cnt: 0 - valLoss: 0.4422156512737274 - trainLoss: 0.4436545670032501\n",
      "cnt: 0 - valLoss: 0.44221431016921997 - trainLoss: 0.4436519145965576\n",
      "cnt: 0 - valLoss: 0.44221293926239014 - trainLoss: 0.4436493217945099\n",
      "cnt: 0 - valLoss: 0.44221144914627075 - trainLoss: 0.4436466693878174\n",
      "cnt: 0 - valLoss: 0.4422101080417633 - trainLoss: 0.4436440169811249\n",
      "cnt: 0 - valLoss: 0.44220882654190063 - trainLoss: 0.4436413645744324\n",
      "cnt: 0 - valLoss: 0.44220760464668274 - trainLoss: 0.44363871216773987\n",
      "cnt: 0 - valLoss: 0.4422062635421753 - trainLoss: 0.44363605976104736\n",
      "cnt: 0 - valLoss: 0.44220492243766785 - trainLoss: 0.44363337755203247\n",
      "cnt: 0 - valLoss: 0.44220370054244995 - trainLoss: 0.44363072514533997\n",
      "cnt: 0 - valLoss: 0.4422023296356201 - trainLoss: 0.44362807273864746\n",
      "cnt: 0 - valLoss: 0.44220098853111267 - trainLoss: 0.44362547993659973\n",
      "cnt: 0 - valLoss: 0.44219970703125 - trainLoss: 0.4436228275299072\n",
      "cnt: 0 - valLoss: 0.44219842553138733 - trainLoss: 0.4436201751232147\n",
      "cnt: 0 - valLoss: 0.4421970844268799 - trainLoss: 0.4436175227165222\n",
      "cnt: 0 - valLoss: 0.4421958029270172 - trainLoss: 0.4436148703098297\n",
      "cnt: 0 - valLoss: 0.44219452142715454 - trainLoss: 0.4436122477054596\n",
      "cnt: 0 - valLoss: 0.44219326972961426 - trainLoss: 0.4436095654964447\n",
      "cnt: 0 - valLoss: 0.4421919882297516 - trainLoss: 0.4436069428920746\n",
      "cnt: 0 - valLoss: 0.4421905279159546 - trainLoss: 0.4436042904853821\n",
      "cnt: 0 - valLoss: 0.4421892464160919 - trainLoss: 0.4436016380786896\n",
      "cnt: 0 - valLoss: 0.44218799471855164 - trainLoss: 0.44359898567199707\n",
      "cnt: 0 - valLoss: 0.4421866834163666 - trainLoss: 0.44359633326530457\n",
      "cnt: 0 - valLoss: 0.4421854019165039 - trainLoss: 0.44359368085861206\n",
      "cnt: 0 - valLoss: 0.44218409061431885 - trainLoss: 0.44359102845191956\n",
      "cnt: 0 - valLoss: 0.4421828091144562 - trainLoss: 0.44358840584754944\n",
      "cnt: 0 - valLoss: 0.4421814978122711 - trainLoss: 0.44358572363853455\n",
      "cnt: 0 - valLoss: 0.4421801269054413 - trainLoss: 0.44358310103416443\n",
      "cnt: 0 - valLoss: 0.442178875207901 - trainLoss: 0.44358038902282715\n",
      "cnt: 0 - valLoss: 0.44217759370803833 - trainLoss: 0.4435777962207794\n",
      "cnt: 0 - valLoss: 0.44217631220817566 - trainLoss: 0.4435751438140869\n",
      "cnt: 0 - valLoss: 0.4421749413013458 - trainLoss: 0.4435725212097168\n",
      "cnt: 0 - valLoss: 0.44217368960380554 - trainLoss: 0.4435698688030243\n",
      "cnt: 0 - valLoss: 0.44217243790626526 - trainLoss: 0.4435672163963318\n",
      "cnt: 0 - valLoss: 0.4421710669994354 - trainLoss: 0.4435645639896393\n",
      "cnt: 0 - valLoss: 0.44216975569725037 - trainLoss: 0.4435619115829468\n",
      "cnt: 0 - valLoss: 0.4421685039997101 - trainLoss: 0.4435592591762543\n",
      "cnt: 0 - valLoss: 0.44216716289520264 - trainLoss: 0.44355660676956177\n",
      "cnt: 0 - valLoss: 0.44216594099998474 - trainLoss: 0.44355395436286926\n",
      "cnt: 0 - valLoss: 0.44216465950012207 - trainLoss: 0.44355130195617676\n",
      "cnt: 0 - valLoss: 0.442163348197937 - trainLoss: 0.44354867935180664\n",
      "cnt: 0 - valLoss: 0.4421619176864624 - trainLoss: 0.44354602694511414\n",
      "cnt: 0 - valLoss: 0.44216060638427734 - trainLoss: 0.4435434341430664\n",
      "cnt: 0 - valLoss: 0.4421592354774475 - trainLoss: 0.4435407817363739\n",
      "cnt: 0 - valLoss: 0.4421578347682953 - trainLoss: 0.4435381591320038\n",
      "cnt: 0 - valLoss: 0.44215649366378784 - trainLoss: 0.4435355067253113\n",
      "cnt: 0 - valLoss: 0.442155122756958 - trainLoss: 0.4435328543186188\n",
      "cnt: 0 - valLoss: 0.4421537518501282 - trainLoss: 0.44353023171424866\n",
      "cnt: 0 - valLoss: 0.44215238094329834 - trainLoss: 0.44352757930755615\n",
      "cnt: 0 - valLoss: 0.44215092062950134 - trainLoss: 0.44352492690086365\n",
      "cnt: 0 - valLoss: 0.4421495497226715 - trainLoss: 0.44352227449417114\n",
      "cnt: 0 - valLoss: 0.44214823842048645 - trainLoss: 0.4435196816921234\n",
      "cnt: 0 - valLoss: 0.44214680790901184 - trainLoss: 0.4435170590877533\n",
      "cnt: 0 - valLoss: 0.44214552640914917 - trainLoss: 0.4435144066810608\n",
      "cnt: 0 - valLoss: 0.44214415550231934 - trainLoss: 0.4435117542743683\n",
      "cnt: 0 - valLoss: 0.4421428442001343 - trainLoss: 0.44350913166999817\n",
      "cnt: 0 - valLoss: 0.4421413242816925 - trainLoss: 0.44350653886795044\n",
      "cnt: 0 - valLoss: 0.44213995337486267 - trainLoss: 0.44350388646125793\n",
      "cnt: 0 - valLoss: 0.4421386420726776 - trainLoss: 0.44350123405456543\n",
      "cnt: 0 - valLoss: 0.44213730096817017 - trainLoss: 0.44349855184555054\n",
      "cnt: 0 - valLoss: 0.44213590025901794 - trainLoss: 0.4434959888458252\n",
      "cnt: 0 - valLoss: 0.4421345591545105 - trainLoss: 0.4434933364391327\n",
      "cnt: 0 - valLoss: 0.44213318824768066 - trainLoss: 0.4434906840324402\n",
      "cnt: 0 - valLoss: 0.442131906747818 - trainLoss: 0.4434880316257477\n",
      "cnt: 0 - valLoss: 0.442130446434021 - trainLoss: 0.4434853792190552\n",
      "cnt: 0 - valLoss: 0.44212907552719116 - trainLoss: 0.44348281621932983\n",
      "cnt: 0 - valLoss: 0.4421277344226837 - trainLoss: 0.44348016381263733\n",
      "cnt: 0 - valLoss: 0.44212642312049866 - trainLoss: 0.4434775114059448\n",
      "cnt: 0 - valLoss: 0.4421250522136688 - trainLoss: 0.4434748888015747\n",
      "cnt: 0 - valLoss: 0.4421237111091614 - trainLoss: 0.443472295999527\n",
      "cnt: 0 - valLoss: 0.44212237000465393 - trainLoss: 0.4434696435928345\n",
      "cnt: 0 - valLoss: 0.4421209394931793 - trainLoss: 0.44346702098846436\n",
      "cnt: 0 - valLoss: 0.44211962819099426 - trainLoss: 0.44346433877944946\n",
      "cnt: 0 - valLoss: 0.44211822748184204 - trainLoss: 0.44346174597740173\n",
      "cnt: 0 - valLoss: 0.4421168863773346 - trainLoss: 0.44345909357070923\n",
      "cnt: 0 - valLoss: 0.44211557507514954 - trainLoss: 0.4434564411640167\n",
      "cnt: 0 - valLoss: 0.4421142339706421 - trainLoss: 0.4434537887573242\n",
      "cnt: 0 - valLoss: 0.4421129524707794 - trainLoss: 0.4434511959552765\n",
      "cnt: 0 - valLoss: 0.4421115815639496 - trainLoss: 0.44344857335090637\n",
      "cnt: 0 - valLoss: 0.44211018085479736 - trainLoss: 0.44344592094421387\n",
      "cnt: 0 - valLoss: 0.44210880994796753 - trainLoss: 0.44344329833984375\n",
      "cnt: 0 - valLoss: 0.4421074688434601 - trainLoss: 0.443440705537796\n",
      "cnt: 0 - valLoss: 0.4421061873435974 - trainLoss: 0.4434380531311035\n",
      "cnt: 0 - valLoss: 0.4421048164367676 - trainLoss: 0.443435400724411\n",
      "cnt: 0 - valLoss: 0.44210347533226013 - trainLoss: 0.4434327781200409\n",
      "cnt: 0 - valLoss: 0.4421021640300751 - trainLoss: 0.4434301555156708\n",
      "cnt: 0 - valLoss: 0.4421007037162781 - trainLoss: 0.44342750310897827\n",
      "cnt: 0 - valLoss: 0.44209933280944824 - trainLoss: 0.44342491030693054\n",
      "cnt: 0 - valLoss: 0.44209805130958557 - trainLoss: 0.4434222877025604\n",
      "cnt: 0 - valLoss: 0.44209668040275574 - trainLoss: 0.4434196352958679\n",
      "cnt: 0 - valLoss: 0.4420953691005707 - trainLoss: 0.4434169828891754\n",
      "cnt: 0 - valLoss: 0.4420940577983856 - trainLoss: 0.4434143602848053\n",
      "cnt: 0 - valLoss: 0.4420927166938782 - trainLoss: 0.44341176748275757\n",
      "cnt: 0 - valLoss: 0.44209134578704834 - trainLoss: 0.44340911507606506\n",
      "cnt: 0 - valLoss: 0.4420899748802185 - trainLoss: 0.44340646266937256\n",
      "cnt: 0 - valLoss: 0.44208869338035583 - trainLoss: 0.44340381026268005\n",
      "cnt: 0 - valLoss: 0.4420872926712036 - trainLoss: 0.4434012472629547\n",
      "cnt: 0 - valLoss: 0.44208595156669617 - trainLoss: 0.4433985948562622\n",
      "cnt: 0 - valLoss: 0.4420846700668335 - trainLoss: 0.4433959424495697\n",
      "cnt: 0 - valLoss: 0.4420832395553589 - trainLoss: 0.4433932602405548\n",
      "cnt: 0 - valLoss: 0.4420819580554962 - trainLoss: 0.44339069724082947\n",
      "cnt: 0 - valLoss: 0.442080557346344 - trainLoss: 0.44338804483413696\n",
      "cnt: 0 - valLoss: 0.44207921624183655 - trainLoss: 0.44338539242744446\n",
      "cnt: 0 - valLoss: 0.4420779049396515 - trainLoss: 0.44338279962539673\n",
      "cnt: 0 - valLoss: 0.44207653403282166 - trainLoss: 0.4433801770210266\n",
      "cnt: 0 - valLoss: 0.4420752227306366 - trainLoss: 0.4433775246143341\n",
      "cnt: 0 - valLoss: 0.44207391142845154 - trainLoss: 0.443374902009964\n",
      "cnt: 0 - valLoss: 0.4420725703239441 - trainLoss: 0.44337230920791626\n",
      "cnt: 0 - valLoss: 0.4420712888240814 - trainLoss: 0.44336965680122375\n",
      "cnt: 0 - valLoss: 0.4420698285102844 - trainLoss: 0.44336700439453125\n",
      "cnt: 0 - valLoss: 0.44206854701042175 - trainLoss: 0.44336438179016113\n",
      "cnt: 0 - valLoss: 0.4420672357082367 - trainLoss: 0.443361759185791\n",
      "cnt: 0 - valLoss: 0.4420658349990845 - trainLoss: 0.4433591067790985\n",
      "cnt: 0 - valLoss: 0.4420645236968994 - trainLoss: 0.443356454372406\n",
      "cnt: 0 - valLoss: 0.44206324219703674 - trainLoss: 0.4433538615703583\n",
      "cnt: 0 - valLoss: 0.4420619606971741 - trainLoss: 0.44335123896598816\n",
      "cnt: 0 - valLoss: 0.4420605003833771 - trainLoss: 0.44334858655929565\n",
      "cnt: 0 - valLoss: 0.44205912947654724 - trainLoss: 0.4433460235595703\n",
      "cnt: 0 - valLoss: 0.44205784797668457 - trainLoss: 0.4433433711528778\n",
      "cnt: 0 - valLoss: 0.4420565366744995 - trainLoss: 0.4433407187461853\n",
      "cnt: 0 - valLoss: 0.44205525517463684 - trainLoss: 0.4433380663394928\n",
      "cnt: 0 - valLoss: 0.4420539438724518 - trainLoss: 0.44333550333976746\n",
      "cnt: 0 - valLoss: 0.4420526623725891 - trainLoss: 0.44333285093307495\n",
      "cnt: 0 - valLoss: 0.4420512318611145 - trainLoss: 0.44333019852638245\n",
      "cnt: 0 - valLoss: 0.44204995036125183 - trainLoss: 0.44332757592201233\n",
      "cnt: 0 - valLoss: 0.442048579454422 - trainLoss: 0.4433249533176422\n",
      "cnt: 0 - valLoss: 0.4420472979545593 - trainLoss: 0.4433223009109497\n",
      "cnt: 0 - valLoss: 0.44204601645469666 - trainLoss: 0.443319708108902\n",
      "cnt: 0 - valLoss: 0.4420446753501892 - trainLoss: 0.44331708550453186\n",
      "cnt: 0 - valLoss: 0.44204339385032654 - trainLoss: 0.44331443309783936\n",
      "cnt: 0 - valLoss: 0.4420420527458191 - trainLoss: 0.443311870098114\n",
      "cnt: 0 - valLoss: 0.44204068183898926 - trainLoss: 0.4433092176914215\n",
      "cnt: 0 - valLoss: 0.4420393109321594 - trainLoss: 0.4433065950870514\n",
      "cnt: 0 - valLoss: 0.44203808903694153 - trainLoss: 0.4433039426803589\n",
      "cnt: 0 - valLoss: 0.44203677773475647 - trainLoss: 0.44330134987831116\n",
      "cnt: 0 - valLoss: 0.4420354664325714 - trainLoss: 0.44329866766929626\n",
      "cnt: 0 - valLoss: 0.44203418493270874 - trainLoss: 0.4432961046695709\n",
      "cnt: 0 - valLoss: 0.4420328438282013 - trainLoss: 0.4432934522628784\n",
      "cnt: 0 - valLoss: 0.4420314431190491 - trainLoss: 0.4432907998561859\n",
      "cnt: 0 - valLoss: 0.442030131816864 - trainLoss: 0.4432882070541382\n",
      "cnt: 0 - valLoss: 0.44202885031700134 - trainLoss: 0.44328558444976807\n",
      "cnt: 0 - valLoss: 0.4420275390148163 - trainLoss: 0.44328293204307556\n",
      "cnt: 0 - valLoss: 0.4420262575149536 - trainLoss: 0.44328027963638306\n",
      "cnt: 0 - valLoss: 0.4420248866081238 - trainLoss: 0.4432777166366577\n",
      "cnt: 0 - valLoss: 0.4420235753059387 - trainLoss: 0.4432750642299652\n",
      "cnt: 0 - valLoss: 0.44202229380607605 - trainLoss: 0.4432724118232727\n",
      "cnt: 0 - valLoss: 0.44202089309692383 - trainLoss: 0.4432697892189026\n",
      "cnt: 0 - valLoss: 0.44201961159706116 - trainLoss: 0.44326716661453247\n",
      "cnt: 0 - valLoss: 0.44201818108558655 - trainLoss: 0.44326451420783997\n",
      "cnt: 0 - valLoss: 0.4420168697834015 - trainLoss: 0.4432619512081146\n",
      "cnt: 0 - valLoss: 0.44201552867889404 - trainLoss: 0.4432592988014221\n",
      "cnt: 0 - valLoss: 0.4420141875743866 - trainLoss: 0.4432567358016968\n",
      "cnt: 0 - valLoss: 0.4420129060745239 - trainLoss: 0.4432540833950043\n",
      "cnt: 0 - valLoss: 0.44201138615608215 - trainLoss: 0.44325146079063416\n",
      "cnt: 0 - valLoss: 0.4420100748538971 - trainLoss: 0.4432488679885864\n",
      "cnt: 0 - valLoss: 0.44200870394706726 - trainLoss: 0.4432462155818939\n",
      "cnt: 0 - valLoss: 0.4420073330402374 - trainLoss: 0.4432435929775238\n",
      "cnt: 0 - valLoss: 0.4420059621334076 - trainLoss: 0.4432410001754761\n",
      "cnt: 0 - valLoss: 0.44200459122657776 - trainLoss: 0.4432383179664612\n",
      "cnt: 0 - valLoss: 0.4420032203197479 - trainLoss: 0.44323575496673584\n",
      "cnt: 0 - valLoss: 0.44200173020362854 - trainLoss: 0.44323310256004333\n",
      "cnt: 0 - valLoss: 0.4420003592967987 - trainLoss: 0.443230539560318\n",
      "cnt: 0 - valLoss: 0.44199898838996887 - trainLoss: 0.4432278871536255\n",
      "cnt: 0 - valLoss: 0.44199761748313904 - trainLoss: 0.44322532415390015\n",
      "cnt: 0 - valLoss: 0.4419962465763092 - trainLoss: 0.44322267174720764\n",
      "cnt: 0 - valLoss: 0.4419948160648346 - trainLoss: 0.44322001934051514\n",
      "cnt: 0 - valLoss: 0.44199350476264954 - trainLoss: 0.4432174563407898\n",
      "cnt: 0 - valLoss: 0.4419921338558197 - trainLoss: 0.4432148039340973\n",
      "cnt: 0 - valLoss: 0.44199058413505554 - trainLoss: 0.4432121813297272\n",
      "cnt: 0 - valLoss: 0.4419892430305481 - trainLoss: 0.44320955872535706\n",
      "cnt: 0 - valLoss: 0.44198787212371826 - trainLoss: 0.4432069659233093\n",
      "cnt: 0 - valLoss: 0.4419865012168884 - trainLoss: 0.4432043433189392\n",
      "cnt: 0 - valLoss: 0.44198518991470337 - trainLoss: 0.4432017505168915\n",
      "cnt: 0 - valLoss: 0.44198381900787354 - trainLoss: 0.44319912791252136\n",
      "cnt: 0 - valLoss: 0.4419824779033661 - trainLoss: 0.44319647550582886\n",
      "cnt: 0 - valLoss: 0.44198092818260193 - trainLoss: 0.4431939125061035\n",
      "cnt: 0 - valLoss: 0.44197961688041687 - trainLoss: 0.4431912899017334\n",
      "cnt: 0 - valLoss: 0.44197824597358704 - trainLoss: 0.4431886672973633\n",
      "cnt: 0 - valLoss: 0.4419768154621124 - trainLoss: 0.44318607449531555\n",
      "cnt: 0 - valLoss: 0.4419754147529602 - trainLoss: 0.44318345189094543\n",
      "cnt: 0 - valLoss: 0.4419739842414856 - trainLoss: 0.4431808888912201\n",
      "cnt: 0 - valLoss: 0.441972553730011 - trainLoss: 0.4431782364845276\n",
      "cnt: 0 - valLoss: 0.441971093416214 - trainLoss: 0.44317567348480225\n",
      "cnt: 0 - valLoss: 0.4419696629047394 - trainLoss: 0.44317305088043213\n",
      "cnt: 0 - valLoss: 0.44196826219558716 - trainLoss: 0.4431704580783844\n",
      "cnt: 0 - valLoss: 0.4419668912887573 - trainLoss: 0.44316789507865906\n",
      "cnt: 0 - valLoss: 0.4419654309749603 - trainLoss: 0.44316521286964417\n",
      "cnt: 0 - valLoss: 0.4419640600681305 - trainLoss: 0.4431626498699188\n",
      "cnt: 0 - valLoss: 0.4419626295566559 - trainLoss: 0.4431600570678711\n",
      "cnt: 0 - valLoss: 0.4419611394405365 - trainLoss: 0.443157434463501\n",
      "cnt: 0 - valLoss: 0.4419597387313843 - trainLoss: 0.44315487146377563\n",
      "cnt: 0 - valLoss: 0.44195830821990967 - trainLoss: 0.44315221905708313\n",
      "cnt: 0 - valLoss: 0.44195693731307983 - trainLoss: 0.4431496560573578\n",
      "cnt: 0 - valLoss: 0.44195547699928284 - trainLoss: 0.44314703345298767\n",
      "cnt: 0 - valLoss: 0.441954106092453 - trainLoss: 0.44314441084861755\n",
      "cnt: 0 - valLoss: 0.44195273518562317 - trainLoss: 0.4431418180465698\n",
      "cnt: 0 - valLoss: 0.44195127487182617 - trainLoss: 0.4431391954421997\n",
      "cnt: 0 - valLoss: 0.4419497847557068 - trainLoss: 0.44313663244247437\n",
      "cnt: 0 - valLoss: 0.44194844365119934 - trainLoss: 0.44313400983810425\n",
      "cnt: 0 - valLoss: 0.44194698333740234 - trainLoss: 0.4431314170360565\n",
      "cnt: 0 - valLoss: 0.4419456124305725 - trainLoss: 0.4431287944316864\n",
      "cnt: 0 - valLoss: 0.4419442415237427 - trainLoss: 0.44312623143196106\n",
      "cnt: 0 - valLoss: 0.44194287061691284 - trainLoss: 0.44312360882759094\n",
      "cnt: 0 - valLoss: 0.441941499710083 - trainLoss: 0.44312095642089844\n",
      "cnt: 0 - valLoss: 0.4419400095939636 - trainLoss: 0.4431183934211731\n",
      "cnt: 0 - valLoss: 0.441938579082489 - trainLoss: 0.44311583042144775\n",
      "cnt: 0 - valLoss: 0.4419372081756592 - trainLoss: 0.44311317801475525\n",
      "cnt: 0 - valLoss: 0.44193583726882935 - trainLoss: 0.4431106150150299\n",
      "cnt: 0 - valLoss: 0.44193440675735474 - trainLoss: 0.4431079924106598\n",
      "cnt: 0 - valLoss: 0.4419330358505249 - trainLoss: 0.44310539960861206\n",
      "cnt: 0 - valLoss: 0.44193169474601746 - trainLoss: 0.4431028366088867\n",
      "cnt: 0 - valLoss: 0.44193020462989807 - trainLoss: 0.4431001543998718\n",
      "cnt: 0 - valLoss: 0.44192883372306824 - trainLoss: 0.4430975914001465\n",
      "cnt: 0 - valLoss: 0.4419274628162384 - trainLoss: 0.44309499859809875\n",
      "cnt: 0 - valLoss: 0.44192609190940857 - trainLoss: 0.4430924355983734\n",
      "cnt: 0 - valLoss: 0.44192469120025635 - trainLoss: 0.4430898129940033\n",
      "cnt: 0 - valLoss: 0.44192326068878174 - trainLoss: 0.4430871903896332\n",
      "cnt: 0 - valLoss: 0.4419219195842743 - trainLoss: 0.44308459758758545\n",
      "cnt: 0 - valLoss: 0.4419204294681549 - trainLoss: 0.4430820345878601\n",
      "cnt: 0 - valLoss: 0.4419190585613251 - trainLoss: 0.44307941198349\n",
      "cnt: 0 - valLoss: 0.44191768765449524 - trainLoss: 0.4430767893791199\n",
      "cnt: 0 - valLoss: 0.44191625714302063 - trainLoss: 0.44307422637939453\n",
      "cnt: 0 - valLoss: 0.44191479682922363 - trainLoss: 0.4430716335773468\n",
      "cnt: 0 - valLoss: 0.4419133961200714 - trainLoss: 0.4430690109729767\n",
      "cnt: 0 - valLoss: 0.4419120252132416 - trainLoss: 0.44306644797325134\n",
      "cnt: 0 - valLoss: 0.44191044569015503 - trainLoss: 0.44306379556655884\n",
      "cnt: 0 - valLoss: 0.4419090151786804 - trainLoss: 0.4430612325668335\n",
      "cnt: 0 - valLoss: 0.4419076442718506 - trainLoss: 0.4430586099624634\n",
      "cnt: 0 - valLoss: 0.44190624356269836 - trainLoss: 0.44305604696273804\n",
      "cnt: 0 - valLoss: 0.44190481305122375 - trainLoss: 0.4430534243583679\n",
      "cnt: 0 - valLoss: 0.4419034421443939 - trainLoss: 0.4430508613586426\n",
      "cnt: 0 - valLoss: 0.4419019818305969 - trainLoss: 0.44304823875427246\n",
      "cnt: 0 - valLoss: 0.44190043210983276 - trainLoss: 0.44304564595222473\n",
      "cnt: 0 - valLoss: 0.44189906120300293 - trainLoss: 0.4430430233478546\n",
      "cnt: 0 - valLoss: 0.4418976306915283 - trainLoss: 0.4430404603481293\n",
      "cnt: 0 - valLoss: 0.4418962597846985 - trainLoss: 0.44303783774375916\n",
      "cnt: 0 - valLoss: 0.44189488887786865 - trainLoss: 0.4430352747440338\n",
      "cnt: 0 - valLoss: 0.44189345836639404 - trainLoss: 0.4430326819419861\n",
      "cnt: 0 - valLoss: 0.4418920576572418 - trainLoss: 0.44303005933761597\n",
      "cnt: 0 - valLoss: 0.44189053773880005 - trainLoss: 0.4430274963378906\n",
      "cnt: 0 - valLoss: 0.4418891668319702 - trainLoss: 0.4430248737335205\n",
      "cnt: 0 - valLoss: 0.441887766122818 - trainLoss: 0.4430222809314728\n",
      "cnt: 0 - valLoss: 0.44188639521598816 - trainLoss: 0.44301968812942505\n",
      "cnt: 0 - valLoss: 0.44188496470451355 - trainLoss: 0.4430170953273773\n",
      "cnt: 0 - valLoss: 0.4418836534023285 - trainLoss: 0.443014532327652\n",
      "cnt: 0 - valLoss: 0.4418821930885315 - trainLoss: 0.44301190972328186\n",
      "cnt: 0 - valLoss: 0.4418806731700897 - trainLoss: 0.4430093467235565\n",
      "cnt: 0 - valLoss: 0.4418793022632599 - trainLoss: 0.4430067241191864\n",
      "cnt: 0 - valLoss: 0.44187790155410767 - trainLoss: 0.44300413131713867\n",
      "cnt: 0 - valLoss: 0.44187653064727783 - trainLoss: 0.44300153851509094\n",
      "cnt: 0 - valLoss: 0.441875159740448 - trainLoss: 0.4429989457130432\n",
      "cnt: 0 - valLoss: 0.441873699426651 - trainLoss: 0.4429963231086731\n",
      "cnt: 0 - valLoss: 0.44187235832214355 - trainLoss: 0.44299376010894775\n",
      "cnt: 0 - valLoss: 0.44187086820602417 - trainLoss: 0.4429911971092224\n",
      "cnt: 0 - valLoss: 0.44186949729919434 - trainLoss: 0.4429885745048523\n",
      "cnt: 0 - valLoss: 0.4418681263923645 - trainLoss: 0.44298598170280457\n",
      "cnt: 0 - valLoss: 0.44186675548553467 - trainLoss: 0.4429834187030792\n",
      "cnt: 0 - valLoss: 0.44186535477638245 - trainLoss: 0.4429807960987091\n",
      "cnt: 0 - valLoss: 0.441864013671875 - trainLoss: 0.442978173494339\n",
      "cnt: 0 - valLoss: 0.441862553358078 - trainLoss: 0.44297561049461365\n",
      "cnt: 0 - valLoss: 0.441861093044281 - trainLoss: 0.4429730474948883\n",
      "cnt: 0 - valLoss: 0.44185972213745117 - trainLoss: 0.4429704248905182\n",
      "cnt: 0 - valLoss: 0.44185835123062134 - trainLoss: 0.44296783208847046\n",
      "cnt: 0 - valLoss: 0.4418569505214691 - trainLoss: 0.4429652690887451\n",
      "cnt: 0 - valLoss: 0.44185560941696167 - trainLoss: 0.4429626762866974\n",
      "cnt: 0 - valLoss: 0.4418541491031647 - trainLoss: 0.4429600238800049\n",
      "cnt: 0 - valLoss: 0.44185277819633484 - trainLoss: 0.44295746088027954\n",
      "cnt: 0 - valLoss: 0.44185131788253784 - trainLoss: 0.4429548978805542\n",
      "cnt: 0 - valLoss: 0.441849946975708 - trainLoss: 0.4429522752761841\n",
      "cnt: 0 - valLoss: 0.44184863567352295 - trainLoss: 0.44294968247413635\n",
      "cnt: 0 - valLoss: 0.44184717535972595 - trainLoss: 0.442947119474411\n",
      "cnt: 0 - valLoss: 0.4418458342552185 - trainLoss: 0.4429445266723633\n",
      "cnt: 0 - valLoss: 0.44184446334838867 - trainLoss: 0.44294193387031555\n",
      "cnt: 0 - valLoss: 0.44184309244155884 - trainLoss: 0.44293931126594543\n",
      "cnt: 0 - valLoss: 0.44184163212776184 - trainLoss: 0.4429367482662201\n",
      "cnt: 0 - valLoss: 0.441840261220932 - trainLoss: 0.44293415546417236\n",
      "cnt: 0 - valLoss: 0.4418388605117798 - trainLoss: 0.44293153285980225\n",
      "cnt: 0 - valLoss: 0.44183748960494995 - trainLoss: 0.4429289698600769\n",
      "cnt: 0 - valLoss: 0.4418361485004425 - trainLoss: 0.4429263472557068\n",
      "cnt: 0 - valLoss: 0.4418346881866455 - trainLoss: 0.44292378425598145\n",
      "cnt: 0 - valLoss: 0.4418333172798157 - trainLoss: 0.44292116165161133\n",
      "cnt: 0 - valLoss: 0.4418318569660187 - trainLoss: 0.442918598651886\n",
      "cnt: 0 - valLoss: 0.4418305456638336 - trainLoss: 0.44291603565216064\n",
      "cnt: 0 - valLoss: 0.4418291747570038 - trainLoss: 0.4429134130477905\n",
      "cnt: 0 - valLoss: 0.44182783365249634 - trainLoss: 0.4429108202457428\n",
      "cnt: 0 - valLoss: 0.4418264627456665 - trainLoss: 0.44290825724601746\n",
      "cnt: 0 - valLoss: 0.44182518124580383 - trainLoss: 0.4429056644439697\n",
      "cnt: 0 - valLoss: 0.4418237805366516 - trainLoss: 0.4429031014442444\n",
      "cnt: 0 - valLoss: 0.441822350025177 - trainLoss: 0.4429004490375519\n",
      "cnt: 0 - valLoss: 0.44182097911834717 - trainLoss: 0.44289788603782654\n",
      "cnt: 0 - valLoss: 0.44181957840919495 - trainLoss: 0.4428952634334564\n",
      "cnt: 0 - valLoss: 0.4418182373046875 - trainLoss: 0.4428927004337311\n",
      "cnt: 0 - valLoss: 0.44181689620018005 - trainLoss: 0.44289010763168335\n",
      "cnt: 0 - valLoss: 0.44181549549102783 - trainLoss: 0.4428875148296356\n",
      "cnt: 0 - valLoss: 0.441814124584198 - trainLoss: 0.4428849518299103\n",
      "cnt: 0 - valLoss: 0.4418126940727234 - trainLoss: 0.44288238883018494\n",
      "cnt: 0 - valLoss: 0.44181138277053833 - trainLoss: 0.4428797960281372\n",
      "cnt: 0 - valLoss: 0.4418100118637085 - trainLoss: 0.4428771734237671\n",
      "cnt: 0 - valLoss: 0.44180864095687866 - trainLoss: 0.44287461042404175\n",
      "cnt: 0 - valLoss: 0.441807359457016 - trainLoss: 0.4428720474243164\n",
      "cnt: 0 - valLoss: 0.44180601835250854 - trainLoss: 0.4428694248199463\n",
      "cnt: 0 - valLoss: 0.4418046176433563 - trainLoss: 0.44286680221557617\n",
      "cnt: 0 - valLoss: 0.4418031573295593 - trainLoss: 0.44286423921585083\n",
      "cnt: 0 - valLoss: 0.44180187582969666 - trainLoss: 0.4428616464138031\n",
      "cnt: 0 - valLoss: 0.44180041551589966 - trainLoss: 0.44285911321640015\n",
      "cnt: 0 - valLoss: 0.441799134016037 - trainLoss: 0.44285649061203003\n",
      "cnt: 0 - valLoss: 0.44179776310920715 - trainLoss: 0.4428538978099823\n",
      "cnt: 0 - valLoss: 0.4417964518070221 - trainLoss: 0.44285130500793457\n",
      "cnt: 0 - valLoss: 0.44179514050483704 - trainLoss: 0.44284871220588684\n",
      "cnt: 0 - valLoss: 0.44179362058639526 - trainLoss: 0.4428461492061615\n",
      "cnt: 0 - valLoss: 0.4417923092842102 - trainLoss: 0.44284358620643616\n",
      "cnt: 0 - valLoss: 0.44179096817970276 - trainLoss: 0.44284096360206604\n",
      "cnt: 0 - valLoss: 0.4417895972728729 - trainLoss: 0.4428384006023407\n",
      "cnt: 0 - valLoss: 0.4417882263660431 - trainLoss: 0.4428357779979706\n",
      "cnt: 0 - valLoss: 0.44178691506385803 - trainLoss: 0.44283321499824524\n",
      "cnt: 0 - valLoss: 0.4417855739593506 - trainLoss: 0.4428305923938751\n",
      "cnt: 0 - valLoss: 0.44178420305252075 - trainLoss: 0.4428280293941498\n",
      "cnt: 0 - valLoss: 0.4417828321456909 - trainLoss: 0.44282543659210205\n",
      "cnt: 0 - valLoss: 0.4417814612388611 - trainLoss: 0.4428228437900543\n",
      "cnt: 0 - valLoss: 0.44178012013435364 - trainLoss: 0.442820280790329\n",
      "cnt: 0 - valLoss: 0.4417788088321686 - trainLoss: 0.44281768798828125\n",
      "cnt: 0 - valLoss: 0.44177746772766113 - trainLoss: 0.4428150951862335\n",
      "cnt: 0 - valLoss: 0.4417761564254761 - trainLoss: 0.4428125023841858\n",
      "cnt: 0 - valLoss: 0.4417746067047119 - trainLoss: 0.4428098797798157\n",
      "cnt: 0 - valLoss: 0.44177332520484924 - trainLoss: 0.44280731678009033\n",
      "cnt: 0 - valLoss: 0.4417719841003418 - trainLoss: 0.442804753780365\n",
      "cnt: 0 - valLoss: 0.44177067279815674 - trainLoss: 0.4428021311759949\n",
      "cnt: 0 - valLoss: 0.4417693316936493 - trainLoss: 0.44279956817626953\n",
      "cnt: 0 - valLoss: 0.44176799058914185 - trainLoss: 0.4427970051765442\n",
      "cnt: 0 - valLoss: 0.4417666792869568 - trainLoss: 0.4427943825721741\n",
      "cnt: 0 - valLoss: 0.4417652487754822 - trainLoss: 0.44279178977012634\n",
      "cnt: 0 - valLoss: 0.44176387786865234 - trainLoss: 0.442789226770401\n",
      "cnt: 0 - valLoss: 0.4417625665664673 - trainLoss: 0.44278669357299805\n",
      "cnt: 0 - valLoss: 0.44176122546195984 - trainLoss: 0.44278404116630554\n",
      "cnt: 0 - valLoss: 0.44175994396209717 - trainLoss: 0.4427814781665802\n",
      "cnt: 0 - valLoss: 0.4417586028575897 - trainLoss: 0.44277888536453247\n",
      "cnt: 0 - valLoss: 0.44175729155540466 - trainLoss: 0.44277632236480713\n",
      "cnt: 0 - valLoss: 0.44175586104393005 - trainLoss: 0.4427737295627594\n",
      "cnt: 0 - valLoss: 0.441754549741745 - trainLoss: 0.4427711069583893\n",
      "cnt: 0 - valLoss: 0.44175317883491516 - trainLoss: 0.44276854395866394\n",
      "cnt: 0 - valLoss: 0.4417518973350525 - trainLoss: 0.4427659809589386\n",
      "cnt: 0 - valLoss: 0.4417506158351898 - trainLoss: 0.4427633583545685\n",
      "cnt: 0 - valLoss: 0.4417491555213928 - trainLoss: 0.4427608251571655\n",
      "cnt: 0 - valLoss: 0.44174784421920776 - trainLoss: 0.442758172750473\n",
      "cnt: 0 - valLoss: 0.4417465329170227 - trainLoss: 0.4427556097507477\n",
      "cnt: 0 - valLoss: 0.4417450726032257 - trainLoss: 0.44275304675102234\n",
      "cnt: 0 - valLoss: 0.44174373149871826 - trainLoss: 0.442750483751297\n",
      "cnt: 0 - valLoss: 0.4417424499988556 - trainLoss: 0.44274792075157166\n",
      "cnt: 0 - valLoss: 0.44174107909202576 - trainLoss: 0.44274529814720154\n",
      "cnt: 0 - valLoss: 0.4417397975921631 - trainLoss: 0.4427427351474762\n",
      "cnt: 0 - valLoss: 0.44173839688301086 - trainLoss: 0.4427401125431061\n",
      "cnt: 0 - valLoss: 0.4417370557785034 - trainLoss: 0.44273754954338074\n",
      "cnt: 0 - valLoss: 0.4417356848716736 - trainLoss: 0.4427349865436554\n",
      "cnt: 0 - valLoss: 0.44173434376716614 - trainLoss: 0.44273242354393005\n",
      "cnt: 0 - valLoss: 0.4417329430580139 - trainLoss: 0.44272980093955994\n",
      "cnt: 0 - valLoss: 0.4417315721511841 - trainLoss: 0.442727267742157\n",
      "cnt: 0 - valLoss: 0.44173023104667664 - trainLoss: 0.44272467494010925\n",
      "cnt: 0 - valLoss: 0.4417289197444916 - trainLoss: 0.44272205233573914\n",
      "cnt: 0 - valLoss: 0.44172754883766174 - trainLoss: 0.4427194893360138\n",
      "cnt: 0 - valLoss: 0.4417261779308319 - trainLoss: 0.44271692633628845\n",
      "cnt: 0 - valLoss: 0.44172489643096924 - trainLoss: 0.44271430373191833\n",
      "cnt: 0 - valLoss: 0.4417235255241394 - trainLoss: 0.44271180033683777\n",
      "cnt: 0 - valLoss: 0.44172215461730957 - trainLoss: 0.44270917773246765\n",
      "cnt: 0 - valLoss: 0.4417208433151245 - trainLoss: 0.4427066147327423\n",
      "cnt: 0 - valLoss: 0.44171953201293945 - trainLoss: 0.4427039921283722\n",
      "cnt: 0 - valLoss: 0.441718190908432 - trainLoss: 0.44270145893096924\n",
      "cnt: 0 - valLoss: 0.4417167007923126 - trainLoss: 0.4426988661289215\n",
      "cnt: 0 - valLoss: 0.4417153596878052 - trainLoss: 0.44269630312919617\n",
      "cnt: 0 - valLoss: 0.4417140781879425 - trainLoss: 0.44269371032714844\n",
      "cnt: 0 - valLoss: 0.4417126774787903 - trainLoss: 0.4426911473274231\n",
      "cnt: 0 - valLoss: 0.44171133637428284 - trainLoss: 0.4426884949207306\n",
      "cnt: 0 - valLoss: 0.44171005487442017 - trainLoss: 0.44268593192100525\n",
      "cnt: 0 - valLoss: 0.44170868396759033 - trainLoss: 0.4426833689212799\n",
      "cnt: 0 - valLoss: 0.44170740246772766 - trainLoss: 0.44268080592155457\n",
      "cnt: 0 - valLoss: 0.4417060315608978 - trainLoss: 0.4426782429218292\n",
      "cnt: 0 - valLoss: 0.44170475006103516 - trainLoss: 0.4426756203174591\n",
      "cnt: 0 - valLoss: 0.4417033791542053 - trainLoss: 0.44267305731773376\n",
      "cnt: 0 - valLoss: 0.44170206785202026 - trainLoss: 0.44267043471336365\n",
      "cnt: 0 - valLoss: 0.44170063734054565 - trainLoss: 0.4426679015159607\n",
      "cnt: 0 - valLoss: 0.4416992664337158 - trainLoss: 0.44266530871391296\n",
      "cnt: 0 - valLoss: 0.44169795513153076 - trainLoss: 0.4426627457141876\n",
      "cnt: 0 - valLoss: 0.4416966736316681 - trainLoss: 0.4426601529121399\n",
      "cnt: 0 - valLoss: 0.44169527292251587 - trainLoss: 0.44265758991241455\n",
      "cnt: 0 - valLoss: 0.4416939318180084 - trainLoss: 0.4426550269126892\n",
      "cnt: 0 - valLoss: 0.44169265031814575 - trainLoss: 0.4426524341106415\n",
      "cnt: 0 - valLoss: 0.4416913688182831 - trainLoss: 0.44264984130859375\n",
      "cnt: 0 - valLoss: 0.44169002771377563 - trainLoss: 0.442647248506546\n",
      "cnt: 0 - valLoss: 0.4416887164115906 - trainLoss: 0.4426446855068207\n",
      "cnt: 0 - valLoss: 0.44168737530708313 - trainLoss: 0.44264209270477295\n",
      "cnt: 0 - valLoss: 0.44168591499328613 - trainLoss: 0.4426395297050476\n",
      "cnt: 0 - valLoss: 0.44168463349342346 - trainLoss: 0.44263696670532227\n",
      "cnt: 0 - valLoss: 0.4416833221912384 - trainLoss: 0.4426344037055969\n",
      "cnt: 0 - valLoss: 0.44168198108673096 - trainLoss: 0.4426318407058716\n",
      "cnt: 0 - valLoss: 0.4416806995868683 - trainLoss: 0.44262927770614624\n",
      "cnt: 0 - valLoss: 0.44167935848236084 - trainLoss: 0.4426266551017761\n",
      "cnt: 0 - valLoss: 0.441677987575531 - trainLoss: 0.4426240921020508\n",
      "cnt: 0 - valLoss: 0.44167670607566833 - trainLoss: 0.44262146949768066\n",
      "cnt: 0 - valLoss: 0.44167542457580566 - trainLoss: 0.4426189064979553\n",
      "cnt: 0 - valLoss: 0.4416740834712982 - trainLoss: 0.44261634349823\n",
      "cnt: 0 - valLoss: 0.44167280197143555 - trainLoss: 0.44261378049850464\n",
      "cnt: 0 - valLoss: 0.44167134165763855 - trainLoss: 0.4426111876964569\n",
      "cnt: 0 - valLoss: 0.4416700601577759 - trainLoss: 0.4426085948944092\n",
      "cnt: 0 - valLoss: 0.4416687488555908 - trainLoss: 0.44260603189468384\n",
      "cnt: 0 - valLoss: 0.44166743755340576 - trainLoss: 0.4426034688949585\n",
      "cnt: 0 - valLoss: 0.4416661560535431 - trainLoss: 0.44260087609291077\n",
      "cnt: 0 - valLoss: 0.44166478514671326 - trainLoss: 0.44259828329086304\n",
      "cnt: 0 - valLoss: 0.4416635036468506 - trainLoss: 0.4425957202911377\n",
      "cnt: 0 - valLoss: 0.4416622221469879 - trainLoss: 0.44259312748908997\n",
      "cnt: 0 - valLoss: 0.4416608512401581 - trainLoss: 0.4425905644893646\n",
      "cnt: 0 - valLoss: 0.4416595697402954 - trainLoss: 0.4425880014896393\n",
      "cnt: 0 - valLoss: 0.44165828824043274 - trainLoss: 0.44258543848991394\n",
      "cnt: 0 - valLoss: 0.4416569471359253 - trainLoss: 0.4425828158855438\n",
      "cnt: 0 - valLoss: 0.44165557622909546 - trainLoss: 0.4425802528858185\n",
      "cnt: 0 - valLoss: 0.441654235124588 - trainLoss: 0.44257768988609314\n",
      "cnt: 0 - valLoss: 0.44165292382240295 - trainLoss: 0.4425751268863678\n",
      "cnt: 0 - valLoss: 0.4416515827178955 - trainLoss: 0.44257256388664246\n",
      "cnt: 0 - valLoss: 0.44165030121803284 - trainLoss: 0.44256994128227234\n",
      "cnt: 0 - valLoss: 0.44164901971817017 - trainLoss: 0.4425674080848694\n",
      "cnt: 0 - valLoss: 0.4416476786136627 - trainLoss: 0.44256481528282166\n",
      "cnt: 0 - valLoss: 0.44164639711380005 - trainLoss: 0.4425622224807739\n",
      "cnt: 0 - valLoss: 0.44164517521858215 - trainLoss: 0.4425596594810486\n",
      "cnt: 0 - valLoss: 0.44164374470710754 - trainLoss: 0.44255709648132324\n",
      "cnt: 0 - valLoss: 0.4416424632072449 - trainLoss: 0.4425545334815979\n",
      "cnt: 0 - valLoss: 0.44164103269577026 - trainLoss: 0.4425519108772278\n",
      "cnt: 0 - valLoss: 0.44163981080055237 - trainLoss: 0.4425494074821472\n",
      "cnt: 0 - valLoss: 0.4416385293006897 - trainLoss: 0.4425468146800995\n",
      "cnt: 0 - valLoss: 0.44163718819618225 - trainLoss: 0.44254425168037415\n",
      "cnt: 0 - valLoss: 0.44163599610328674 - trainLoss: 0.44254177808761597\n",
      "cnt: 0 - valLoss: 0.44163453578948975 - trainLoss: 0.4425392150878906\n",
      "cnt: 0 - valLoss: 0.44163334369659424 - trainLoss: 0.4425366520881653\n",
      "cnt: 0 - valLoss: 0.4416320323944092 - trainLoss: 0.4425341486930847\n",
      "cnt: 0 - valLoss: 0.4416307508945465 - trainLoss: 0.4425315856933594\n",
      "cnt: 0 - valLoss: 0.44162946939468384 - trainLoss: 0.44252902269363403\n",
      "cnt: 0 - valLoss: 0.44162818789482117 - trainLoss: 0.4425264596939087\n",
      "cnt: 0 - valLoss: 0.4416268467903137 - trainLoss: 0.44252392649650574\n",
      "cnt: 0 - valLoss: 0.44162556529045105 - trainLoss: 0.4425213932991028\n",
      "cnt: 0 - valLoss: 0.4416242837905884 - trainLoss: 0.44251883029937744\n",
      "cnt: 0 - valLoss: 0.4416230320930481 - trainLoss: 0.4425162672996521\n",
      "cnt: 0 - valLoss: 0.44162166118621826 - trainLoss: 0.44251373410224915\n",
      "cnt: 0 - valLoss: 0.44162052869796753 - trainLoss: 0.4425112307071686\n",
      "cnt: 0 - valLoss: 0.4416189193725586 - trainLoss: 0.44250863790512085\n",
      "cnt: 0 - valLoss: 0.4416176974773407 - trainLoss: 0.4425061345100403\n",
      "cnt: 0 - valLoss: 0.4416164457798004 - trainLoss: 0.44250360131263733\n",
      "cnt: 0 - valLoss: 0.44161519408226013 - trainLoss: 0.4425009787082672\n",
      "cnt: 0 - valLoss: 0.44161397218704224 - trainLoss: 0.44249847531318665\n",
      "cnt: 0 - valLoss: 0.4416126310825348 - trainLoss: 0.4424959421157837\n",
      "cnt: 0 - valLoss: 0.4416113793849945 - trainLoss: 0.44249340891838074\n",
      "cnt: 0 - valLoss: 0.44161009788513184 - trainLoss: 0.4424908459186554\n",
      "cnt: 0 - valLoss: 0.44160884618759155 - trainLoss: 0.44248831272125244\n",
      "cnt: 0 - valLoss: 0.4416075348854065 - trainLoss: 0.4424857795238495\n",
      "cnt: 0 - valLoss: 0.4416062533855438 - trainLoss: 0.44248321652412415\n",
      "cnt: 0 - valLoss: 0.44160500168800354 - trainLoss: 0.4424807131290436\n",
      "cnt: 0 - valLoss: 0.44160374999046326 - trainLoss: 0.4424781799316406\n",
      "cnt: 0 - valLoss: 0.4416024684906006 - trainLoss: 0.4424756169319153\n",
      "cnt: 0 - valLoss: 0.4416012167930603 - trainLoss: 0.44247302412986755\n",
      "cnt: 0 - valLoss: 0.44159990549087524 - trainLoss: 0.4424705505371094\n",
      "cnt: 0 - valLoss: 0.44159871339797974 - trainLoss: 0.44246798753738403\n",
      "cnt: 0 - valLoss: 0.44159725308418274 - trainLoss: 0.4424654245376587\n",
      "cnt: 0 - valLoss: 0.44159603118896484 - trainLoss: 0.44246289134025574\n",
      "cnt: 0 - valLoss: 0.4415947496891022 - trainLoss: 0.4424603581428528\n",
      "cnt: 0 - valLoss: 0.4415935277938843 - trainLoss: 0.44245779514312744\n",
      "cnt: 0 - valLoss: 0.4415922164916992 - trainLoss: 0.4424552321434021\n",
      "cnt: 0 - valLoss: 0.44159093499183655 - trainLoss: 0.44245269894599915\n",
      "cnt: 0 - valLoss: 0.4415896236896515 - trainLoss: 0.4424501955509186\n",
      "cnt: 0 - valLoss: 0.44158846139907837 - trainLoss: 0.44244760274887085\n",
      "cnt: 0 - valLoss: 0.4415871202945709 - trainLoss: 0.44244512915611267\n",
      "cnt: 0 - valLoss: 0.4415859282016754 - trainLoss: 0.44244256615638733\n",
      "cnt: 0 - valLoss: 0.44158464670181274 - trainLoss: 0.4424400329589844\n",
      "cnt: 0 - valLoss: 0.44158339500427246 - trainLoss: 0.44243746995925903\n",
      "cnt: 0 - valLoss: 0.4415820837020874 - trainLoss: 0.4424349069595337\n",
      "cnt: 0 - valLoss: 0.4415808618068695 - trainLoss: 0.44243237376213074\n",
      "cnt: 0 - valLoss: 0.44157958030700684 - trainLoss: 0.4424298405647278\n",
      "cnt: 0 - valLoss: 0.44157838821411133 - trainLoss: 0.44242727756500244\n",
      "cnt: 0 - valLoss: 0.4415770471096039 - trainLoss: 0.4424247443675995\n",
      "cnt: 0 - valLoss: 0.44157567620277405 - trainLoss: 0.4424222409725189\n",
      "cnt: 0 - valLoss: 0.4415743947029114 - trainLoss: 0.4424196779727936\n",
      "cnt: 0 - valLoss: 0.44157320261001587 - trainLoss: 0.4424171447753906\n",
      "cnt: 0 - valLoss: 0.4415718615055084 - trainLoss: 0.4424145817756653\n",
      "cnt: 0 - valLoss: 0.4415707588195801 - trainLoss: 0.44241204857826233\n",
      "cnt: 0 - valLoss: 0.4415694773197174 - trainLoss: 0.4424095153808594\n",
      "cnt: 0 - valLoss: 0.44156819581985474 - trainLoss: 0.4424069821834564\n",
      "cnt: 0 - valLoss: 0.4415668845176697 - trainLoss: 0.4424044191837311\n",
      "cnt: 0 - valLoss: 0.44156569242477417 - trainLoss: 0.4424019157886505\n",
      "cnt: 0 - valLoss: 0.4415644109249115 - trainLoss: 0.4423993229866028\n",
      "cnt: 0 - valLoss: 0.441563218832016 - trainLoss: 0.4423968195915222\n",
      "cnt: 0 - valLoss: 0.4415619373321533 - trainLoss: 0.44239428639411926\n",
      "cnt: 0 - valLoss: 0.44156068563461304 - trainLoss: 0.4423917233943939\n",
      "cnt: 0 - valLoss: 0.44155940413475037 - trainLoss: 0.44238921999931335\n",
      "cnt: 0 - valLoss: 0.44155818223953247 - trainLoss: 0.442386656999588\n",
      "cnt: 0 - valLoss: 0.4415569603443146 - trainLoss: 0.44238415360450745\n",
      "cnt: 0 - valLoss: 0.4415557384490967 - trainLoss: 0.4423816204071045\n",
      "cnt: 0 - valLoss: 0.44155433773994446 - trainLoss: 0.44237905740737915\n",
      "cnt: 0 - valLoss: 0.4415530860424042 - trainLoss: 0.4423765242099762\n",
      "cnt: 0 - valLoss: 0.4415518045425415 - trainLoss: 0.44237399101257324\n",
      "cnt: 0 - valLoss: 0.441550612449646 - trainLoss: 0.4423714280128479\n",
      "cnt: 0 - valLoss: 0.4415493309497833 - trainLoss: 0.44236889481544495\n",
      "cnt: 0 - valLoss: 0.44154810905456543 - trainLoss: 0.4423663914203644\n",
      "cnt: 0 - valLoss: 0.44154682755470276 - trainLoss: 0.44236379861831665\n",
      "cnt: 0 - valLoss: 0.44154563546180725 - trainLoss: 0.44236132502555847\n",
      "cnt: 0 - valLoss: 0.4415443539619446 - trainLoss: 0.44235876202583313\n",
      "cnt: 0 - valLoss: 0.4415431618690491 - trainLoss: 0.4423562288284302\n",
      "cnt: 0 - valLoss: 0.4415418207645416 - trainLoss: 0.4423536956310272\n",
      "cnt: 0 - valLoss: 0.4415406584739685 - trainLoss: 0.4423511326313019\n",
      "cnt: 0 - valLoss: 0.44153937697410583 - trainLoss: 0.44234856963157654\n",
      "cnt: 0 - valLoss: 0.4415381848812103 - trainLoss: 0.44234609603881836\n",
      "cnt: 0 - valLoss: 0.4415368139743805 - trainLoss: 0.442343533039093\n",
      "cnt: 0 - valLoss: 0.4415355622768402 - trainLoss: 0.44234099984169006\n",
      "cnt: 0 - valLoss: 0.44153422117233276 - trainLoss: 0.4423384666442871\n",
      "cnt: 0 - valLoss: 0.4415329098701477 - trainLoss: 0.44233590364456177\n",
      "cnt: 0 - valLoss: 0.44153156876564026 - trainLoss: 0.4423333704471588\n",
      "cnt: 0 - valLoss: 0.4415302872657776 - trainLoss: 0.44233083724975586\n",
      "cnt: 0 - valLoss: 0.4415290057659149 - trainLoss: 0.4423283636569977\n",
      "cnt: 0 - valLoss: 0.44152775406837463 - trainLoss: 0.44232580065727234\n",
      "cnt: 0 - valLoss: 0.4415264427661896 - trainLoss: 0.4423232674598694\n",
      "cnt: 0 - valLoss: 0.4415251612663269 - trainLoss: 0.4423207640647888\n",
      "cnt: 0 - valLoss: 0.4415239095687866 - trainLoss: 0.44231823086738586\n",
      "cnt: 0 - valLoss: 0.44152262806892395 - trainLoss: 0.4423156976699829\n",
      "cnt: 0 - valLoss: 0.4415212869644165 - trainLoss: 0.44231313467025757\n",
      "cnt: 0 - valLoss: 0.441520094871521 - trainLoss: 0.4423106610774994\n",
      "cnt: 0 - valLoss: 0.44151875376701355 - trainLoss: 0.44230806827545166\n",
      "cnt: 0 - valLoss: 0.44151753187179565 - trainLoss: 0.4423055946826935\n",
      "cnt: 0 - valLoss: 0.4415162205696106 - trainLoss: 0.44230303168296814\n",
      "cnt: 0 - valLoss: 0.4415149390697479 - trainLoss: 0.4423004984855652\n",
      "cnt: 0 - valLoss: 0.44151365756988525 - trainLoss: 0.44229796528816223\n",
      "cnt: 0 - valLoss: 0.4415123462677002 - trainLoss: 0.44229546189308167\n",
      "cnt: 0 - valLoss: 0.44151100516319275 - trainLoss: 0.4422929286956787\n",
      "cnt: 0 - valLoss: 0.44150975346565247 - trainLoss: 0.44229036569595337\n",
      "cnt: 0 - valLoss: 0.4415084719657898 - trainLoss: 0.4422878324985504\n",
      "cnt: 0 - valLoss: 0.4415072202682495 - trainLoss: 0.44228529930114746\n",
      "cnt: 0 - valLoss: 0.44150590896606445 - trainLoss: 0.4422827959060669\n",
      "cnt: 0 - valLoss: 0.44150471687316895 - trainLoss: 0.44228026270866394\n",
      "cnt: 0 - valLoss: 0.4415033757686615 - trainLoss: 0.44227778911590576\n",
      "cnt: 0 - valLoss: 0.441502183675766 - trainLoss: 0.44227519631385803\n",
      "cnt: 0 - valLoss: 0.44150084257125854 - trainLoss: 0.44227269291877747\n",
      "cnt: 0 - valLoss: 0.44149959087371826 - trainLoss: 0.4422701597213745\n",
      "cnt: 0 - valLoss: 0.4414983093738556 - trainLoss: 0.44226762652397156\n",
      "cnt: 0 - valLoss: 0.4414971172809601 - trainLoss: 0.4422650933265686\n",
      "cnt: 0 - valLoss: 0.44149577617645264 - trainLoss: 0.44226258993148804\n",
      "cnt: 0 - valLoss: 0.44149455428123474 - trainLoss: 0.4422600567340851\n",
      "cnt: 0 - valLoss: 0.44149327278137207 - trainLoss: 0.44225752353668213\n",
      "cnt: 0 - valLoss: 0.44149190187454224 - trainLoss: 0.44225502014160156\n",
      "cnt: 0 - valLoss: 0.4414905905723572 - trainLoss: 0.44225242733955383\n",
      "cnt: 0 - valLoss: 0.4414893090724945 - trainLoss: 0.44224995374679565\n",
      "cnt: 0 - valLoss: 0.441488116979599 - trainLoss: 0.4422473907470703\n",
      "cnt: 0 - valLoss: 0.44148677587509155 - trainLoss: 0.44224491715431213\n",
      "cnt: 0 - valLoss: 0.44148558378219604 - trainLoss: 0.4422423243522644\n",
      "cnt: 0 - valLoss: 0.4414843022823334 - trainLoss: 0.4422398507595062\n",
      "cnt: 0 - valLoss: 0.4414830505847931 - trainLoss: 0.4422372877597809\n",
      "cnt: 0 - valLoss: 0.4414817690849304 - trainLoss: 0.4422348141670227\n",
      "cnt: 0 - valLoss: 0.4414805769920349 - trainLoss: 0.44223225116729736\n",
      "cnt: 0 - valLoss: 0.44147929549217224 - trainLoss: 0.4422297477722168\n",
      "cnt: 0 - valLoss: 0.44147801399230957 - trainLoss: 0.44222718477249146\n",
      "cnt: 0 - valLoss: 0.4414767622947693 - trainLoss: 0.4422247111797333\n",
      "cnt: 0 - valLoss: 0.4414754807949066 - trainLoss: 0.44222214818000793\n",
      "cnt: 0 - valLoss: 0.44147419929504395 - trainLoss: 0.442219614982605\n",
      "cnt: 0 - valLoss: 0.44147297739982605 - trainLoss: 0.442217081785202\n",
      "cnt: 0 - valLoss: 0.441471666097641 - trainLoss: 0.4422145485877991\n",
      "cnt: 0 - valLoss: 0.4414703845977783 - trainLoss: 0.4422120451927185\n",
      "cnt: 0 - valLoss: 0.44146907329559326 - trainLoss: 0.44220951199531555\n",
      "cnt: 0 - valLoss: 0.44146788120269775 - trainLoss: 0.4422069787979126\n",
      "cnt: 0 - valLoss: 0.4414665997028351 - trainLoss: 0.44220441579818726\n",
      "cnt: 0 - valLoss: 0.4414653480052948 - trainLoss: 0.4422019422054291\n",
      "cnt: 0 - valLoss: 0.4414641261100769 - trainLoss: 0.4421994090080261\n",
      "cnt: 0 - valLoss: 0.44146281480789185 - trainLoss: 0.44219687581062317\n",
      "cnt: 0 - valLoss: 0.44146159291267395 - trainLoss: 0.4421943128108978\n",
      "cnt: 0 - valLoss: 0.44146034121513367 - trainLoss: 0.44219183921813965\n",
      "cnt: 0 - valLoss: 0.441459059715271 - trainLoss: 0.4421892762184143\n",
      "cnt: 0 - valLoss: 0.4414578676223755 - trainLoss: 0.44218674302101135\n",
      "cnt: 0 - valLoss: 0.44145655632019043 - trainLoss: 0.4421842098236084\n",
      "cnt: 0 - valLoss: 0.4414553642272949 - trainLoss: 0.4421817362308502\n",
      "cnt: 0 - valLoss: 0.44145408272743225 - trainLoss: 0.44217920303344727\n",
      "cnt: 0 - valLoss: 0.44145292043685913 - trainLoss: 0.4421766698360443\n",
      "cnt: 0 - valLoss: 0.4414515197277069 - trainLoss: 0.44217416644096375\n",
      "cnt: 0 - valLoss: 0.441450297832489 - trainLoss: 0.4421716332435608\n",
      "cnt: 0 - valLoss: 0.4414490759372711 - trainLoss: 0.44216907024383545\n",
      "cnt: 0 - valLoss: 0.44144782423973083 - trainLoss: 0.4421665668487549\n",
      "cnt: 0 - valLoss: 0.44144654273986816 - trainLoss: 0.44216400384902954\n",
      "cnt: 0 - valLoss: 0.4414452612400055 - trainLoss: 0.44216153025627136\n",
      "cnt: 0 - valLoss: 0.4414440393447876 - trainLoss: 0.442158967256546\n",
      "cnt: 0 - valLoss: 0.4414428174495697 - trainLoss: 0.44215646386146545\n",
      "cnt: 0 - valLoss: 0.4414416551589966 - trainLoss: 0.4421539604663849\n",
      "cnt: 0 - valLoss: 0.4414403736591339 - trainLoss: 0.44215142726898193\n",
      "cnt: 0 - valLoss: 0.44143912196159363 - trainLoss: 0.4421488642692566\n",
      "cnt: 0 - valLoss: 0.44143784046173096 - trainLoss: 0.4421463906764984\n",
      "cnt: 0 - valLoss: 0.44143667817115784 - trainLoss: 0.4421437978744507\n",
      "cnt: 0 - valLoss: 0.44143539667129517 - trainLoss: 0.4421413242816925\n",
      "cnt: 0 - valLoss: 0.4414341449737549 - trainLoss: 0.44213876128196716\n",
      "cnt: 0 - valLoss: 0.441432923078537 - trainLoss: 0.442136287689209\n",
      "cnt: 0 - valLoss: 0.44143158197402954 - trainLoss: 0.44213369488716125\n",
      "cnt: 0 - valLoss: 0.44143030047416687 - trainLoss: 0.4421312212944031\n",
      "cnt: 0 - valLoss: 0.44142913818359375 - trainLoss: 0.4421286880970001\n",
      "cnt: 0 - valLoss: 0.4414278566837311 - trainLoss: 0.44212618470191956\n",
      "cnt: 0 - valLoss: 0.44142666459083557 - trainLoss: 0.4421236515045166\n",
      "cnt: 0 - valLoss: 0.4414253532886505 - trainLoss: 0.44212111830711365\n",
      "cnt: 0 - valLoss: 0.441424161195755 - trainLoss: 0.4421185851097107\n",
      "cnt: 0 - valLoss: 0.4414229094982147 - trainLoss: 0.4421160817146301\n",
      "cnt: 0 - valLoss: 0.441421777009964 - trainLoss: 0.4421135485172272\n",
      "cnt: 0 - valLoss: 0.44142046570777893 - trainLoss: 0.4421110153198242\n",
      "cnt: 0 - valLoss: 0.44141924381256104 - trainLoss: 0.44210848212242126\n",
      "cnt: 0 - valLoss: 0.44141799211502075 - trainLoss: 0.4421059787273407\n",
      "cnt: 0 - valLoss: 0.4414167106151581 - trainLoss: 0.44210344552993774\n",
      "cnt: 0 - valLoss: 0.44141554832458496 - trainLoss: 0.4421009123325348\n",
      "cnt: 0 - valLoss: 0.4414142966270447 - trainLoss: 0.4420984387397766\n",
      "cnt: 0 - valLoss: 0.44141310453414917 - trainLoss: 0.44209587574005127\n",
      "cnt: 0 - valLoss: 0.44141173362731934 - trainLoss: 0.4420933425426483\n",
      "cnt: 0 - valLoss: 0.44141054153442383 - trainLoss: 0.44209080934524536\n",
      "cnt: 0 - valLoss: 0.44140926003456116 - trainLoss: 0.4420883357524872\n",
      "cnt: 0 - valLoss: 0.4414081275463104 - trainLoss: 0.44208580255508423\n",
      "cnt: 0 - valLoss: 0.44140684604644775 - trainLoss: 0.44208329916000366\n",
      "cnt: 0 - valLoss: 0.44140565395355225 - trainLoss: 0.4420807659626007\n",
      "cnt: 0 - valLoss: 0.4414043724536896 - trainLoss: 0.44207823276519775\n",
      "cnt: 0 - valLoss: 0.4414031505584717 - trainLoss: 0.4420756995677948\n",
      "cnt: 0 - valLoss: 0.4414019286632538 - trainLoss: 0.44207319617271423\n",
      "cnt: 0 - valLoss: 0.44140076637268066 - trainLoss: 0.44207069277763367\n",
      "cnt: 0 - valLoss: 0.441399484872818 - trainLoss: 0.4420681297779083\n",
      "cnt: 0 - valLoss: 0.4413983225822449 - trainLoss: 0.44206559658050537\n",
      "cnt: 0 - valLoss: 0.4413970708847046 - trainLoss: 0.4420630931854248\n",
      "cnt: 0 - valLoss: 0.4413958787918091 - trainLoss: 0.44206055998802185\n",
      "cnt: 0 - valLoss: 0.4413946270942688 - trainLoss: 0.4420580267906189\n",
      "cnt: 0 - valLoss: 0.4413934051990509 - trainLoss: 0.4420555531978607\n",
      "cnt: 0 - valLoss: 0.44139209389686584 - trainLoss: 0.4420529901981354\n",
      "cnt: 0 - valLoss: 0.44139090180397034 - trainLoss: 0.4420504868030548\n",
      "cnt: 0 - valLoss: 0.44138970971107483 - trainLoss: 0.44204798340797424\n",
      "cnt: 0 - valLoss: 0.44138842821121216 - trainLoss: 0.4420454502105713\n",
      "cnt: 0 - valLoss: 0.44138726592063904 - trainLoss: 0.44204291701316833\n",
      "cnt: 0 - valLoss: 0.44138601422309875 - trainLoss: 0.44204041361808777\n",
      "cnt: 0 - valLoss: 0.44138482213020325 - trainLoss: 0.4420379102230072\n",
      "cnt: 0 - valLoss: 0.44138363003730774 - trainLoss: 0.44203537702560425\n",
      "cnt: 0 - valLoss: 0.44138240814208984 - trainLoss: 0.4420328140258789\n",
      "cnt: 0 - valLoss: 0.44138118624687195 - trainLoss: 0.44203031063079834\n",
      "cnt: 0 - valLoss: 0.44137996435165405 - trainLoss: 0.4420277774333954\n",
      "cnt: 0 - valLoss: 0.44137874245643616 - trainLoss: 0.4420253038406372\n",
      "cnt: 0 - valLoss: 0.44137752056121826 - trainLoss: 0.44202277064323425\n",
      "cnt: 0 - valLoss: 0.44137632846832275 - trainLoss: 0.4420202374458313\n",
      "cnt: 0 - valLoss: 0.44137513637542725 - trainLoss: 0.44201770424842834\n",
      "cnt: 0 - valLoss: 0.4413738548755646 - trainLoss: 0.4420152008533478\n",
      "cnt: 0 - valLoss: 0.4413725733757019 - trainLoss: 0.4420126676559448\n",
      "cnt: 0 - valLoss: 0.4413714110851288 - trainLoss: 0.44201013445854187\n",
      "cnt: 0 - valLoss: 0.4413701593875885 - trainLoss: 0.4420076608657837\n",
      "cnt: 0 - valLoss: 0.4413689970970154 - trainLoss: 0.44200512766838074\n",
      "cnt: 0 - valLoss: 0.4413677453994751 - trainLoss: 0.4420025944709778\n",
      "cnt: 0 - valLoss: 0.4413665533065796 - trainLoss: 0.4420000910758972\n",
      "cnt: 0 - valLoss: 0.4413653314113617 - trainLoss: 0.44199758768081665\n",
      "cnt: 0 - valLoss: 0.4413641691207886 - trainLoss: 0.4419950246810913\n",
      "cnt: 0 - valLoss: 0.4413629174232483 - trainLoss: 0.44199249148368835\n",
      "cnt: 0 - valLoss: 0.441361665725708 - trainLoss: 0.4419899880886078\n",
      "cnt: 0 - valLoss: 0.4413605034351349 - trainLoss: 0.4419874846935272\n",
      "cnt: 0 - valLoss: 0.441359281539917 - trainLoss: 0.4419849216938019\n",
      "cnt: 0 - valLoss: 0.4413580596446991 - trainLoss: 0.4419824481010437\n",
      "cnt: 0 - valLoss: 0.4413568377494812 - trainLoss: 0.44197991490364075\n",
      "cnt: 0 - valLoss: 0.4413555860519409 - trainLoss: 0.4419773817062378\n",
      "cnt: 0 - valLoss: 0.44135433435440063 - trainLoss: 0.4419748783111572\n",
      "cnt: 0 - valLoss: 0.4413531422615051 - trainLoss: 0.4419723451137543\n",
      "cnt: 0 - valLoss: 0.4413519501686096 - trainLoss: 0.4419698119163513\n",
      "cnt: 0 - valLoss: 0.4413507878780365 - trainLoss: 0.44196733832359314\n",
      "cnt: 0 - valLoss: 0.44134950637817383 - trainLoss: 0.4419648051261902\n",
      "cnt: 0 - valLoss: 0.4413483440876007 - trainLoss: 0.44196227192878723\n",
      "cnt: 0 - valLoss: 0.4413470923900604 - trainLoss: 0.44195976853370667\n",
      "cnt: 0 - valLoss: 0.4413459300994873 - trainLoss: 0.4419572651386261\n",
      "cnt: 0 - valLoss: 0.4413447380065918 - trainLoss: 0.44195470213890076\n",
      "cnt: 0 - valLoss: 0.4413435757160187 - trainLoss: 0.4419522285461426\n",
      "cnt: 0 - valLoss: 0.44134238362312317 - trainLoss: 0.44194966554641724\n",
      "cnt: 0 - valLoss: 0.4413412809371948 - trainLoss: 0.44194719195365906\n",
      "cnt: 0 - valLoss: 0.44134002923965454 - trainLoss: 0.4419446885585785\n",
      "cnt: 0 - valLoss: 0.4413388669490814 - trainLoss: 0.44194212555885315\n",
      "cnt: 0 - valLoss: 0.4413377046585083 - trainLoss: 0.44193965196609497\n",
      "cnt: 0 - valLoss: 0.44133639335632324 - trainLoss: 0.44193708896636963\n",
      "cnt: 0 - valLoss: 0.4413352608680725 - trainLoss: 0.44193458557128906\n",
      "cnt: 0 - valLoss: 0.441334068775177 - trainLoss: 0.4419320821762085\n",
      "cnt: 0 - valLoss: 0.44133296608924866 - trainLoss: 0.44192954897880554\n",
      "cnt: 0 - valLoss: 0.44133174419403076 - trainLoss: 0.4419270157814026\n",
      "cnt: 0 - valLoss: 0.4413306415081024 - trainLoss: 0.44192448258399963\n",
      "cnt: 0 - valLoss: 0.44132938981056213 - trainLoss: 0.44192200899124146\n",
      "cnt: 0 - valLoss: 0.4413283169269562 - trainLoss: 0.4419194757938385\n",
      "cnt: 0 - valLoss: 0.4413270950317383 - trainLoss: 0.44191697239875793\n",
      "cnt: 0 - valLoss: 0.44132596254348755 - trainLoss: 0.441914439201355\n",
      "cnt: 0 - valLoss: 0.44132480025291443 - trainLoss: 0.441911906003952\n",
      "cnt: 0 - valLoss: 0.4413236081600189 - trainLoss: 0.44190943241119385\n",
      "cnt: 0 - valLoss: 0.4413225054740906 - trainLoss: 0.4419068992137909\n",
      "cnt: 0 - valLoss: 0.4413212835788727 - trainLoss: 0.44190436601638794\n",
      "cnt: 0 - valLoss: 0.44132018089294434 - trainLoss: 0.4419018626213074\n",
      "cnt: 0 - valLoss: 0.4413188099861145 - trainLoss: 0.4418993294239044\n",
      "cnt: 0 - valLoss: 0.4413177967071533 - trainLoss: 0.44189685583114624\n",
      "cnt: 0 - valLoss: 0.4413165748119354 - trainLoss: 0.4418942630290985\n",
      "cnt: 0 - valLoss: 0.4413153827190399 - trainLoss: 0.44189178943634033\n",
      "cnt: 0 - valLoss: 0.4413142204284668 - trainLoss: 0.4418892562389374\n",
      "cnt: 0 - valLoss: 0.4413130581378937 - trainLoss: 0.4418867528438568\n",
      "cnt: 0 - valLoss: 0.44131186604499817 - trainLoss: 0.44188424944877625\n",
      "cnt: 0 - valLoss: 0.4413107633590698 - trainLoss: 0.4418817460536957\n",
      "cnt: 0 - valLoss: 0.44130954146385193 - trainLoss: 0.4418792128562927\n",
      "cnt: 0 - valLoss: 0.44130852818489075 - trainLoss: 0.44187667965888977\n",
      "cnt: 0 - valLoss: 0.44130727648735046 - trainLoss: 0.4418742060661316\n",
      "cnt: 0 - valLoss: 0.44130611419677734 - trainLoss: 0.44187167286872864\n",
      "cnt: 0 - valLoss: 0.4413049817085266 - trainLoss: 0.4418691396713257\n",
      "cnt: 0 - valLoss: 0.44130390882492065 - trainLoss: 0.4418666660785675\n",
      "cnt: 0 - valLoss: 0.44130265712738037 - trainLoss: 0.44186410307884216\n",
      "cnt: 0 - valLoss: 0.4413014054298401 - trainLoss: 0.4418615698814392\n",
      "cnt: 0 - valLoss: 0.4413002133369446 - trainLoss: 0.44185909628868103\n",
      "cnt: 0 - valLoss: 0.44129908084869385 - trainLoss: 0.44185662269592285\n",
      "cnt: 0 - valLoss: 0.44129788875579834 - trainLoss: 0.4418540298938751\n",
      "cnt: 0 - valLoss: 0.4412968158721924 - trainLoss: 0.44185152649879456\n",
      "cnt: 0 - valLoss: 0.4412955939769745 - trainLoss: 0.441849023103714\n",
      "cnt: 0 - valLoss: 0.44129446148872375 - trainLoss: 0.4418465197086334\n",
      "cnt: 0 - valLoss: 0.44129329919815063 - trainLoss: 0.44184401631355286\n",
      "cnt: 0 - valLoss: 0.4412921965122223 - trainLoss: 0.4418414533138275\n",
      "cnt: 0 - valLoss: 0.44129103422164917 - trainLoss: 0.44183897972106934\n",
      "cnt: 0 - valLoss: 0.44128984212875366 - trainLoss: 0.4418364465236664\n",
      "cnt: 0 - valLoss: 0.44128870964050293 - trainLoss: 0.4418339133262634\n",
      "cnt: 0 - valLoss: 0.4412875175476074 - trainLoss: 0.44183143973350525\n",
      "cnt: 0 - valLoss: 0.4412864148616791 - trainLoss: 0.4418289065361023\n",
      "cnt: 0 - valLoss: 0.44128522276878357 - trainLoss: 0.44182640314102173\n",
      "cnt: 0 - valLoss: 0.4412840008735657 - trainLoss: 0.441823810338974\n",
      "cnt: 0 - valLoss: 0.44128280878067017 - trainLoss: 0.4418213367462158\n",
      "cnt: 0 - valLoss: 0.4412817060947418 - trainLoss: 0.44181886315345764\n",
      "cnt: 0 - valLoss: 0.4412804841995239 - trainLoss: 0.4418163001537323\n",
      "cnt: 0 - valLoss: 0.4412793815135956 - trainLoss: 0.44181379675865173\n",
      "cnt: 0 - valLoss: 0.44127821922302246 - trainLoss: 0.44181132316589355\n",
      "cnt: 0 - valLoss: 0.44127705693244934 - trainLoss: 0.4418087601661682\n",
      "cnt: 0 - valLoss: 0.4412758946418762 - trainLoss: 0.44180628657341003\n",
      "cnt: 0 - valLoss: 0.4412747919559479 - trainLoss: 0.4418037533760071\n",
      "cnt: 0 - valLoss: 0.44127359986305237 - trainLoss: 0.4418012201786041\n",
      "cnt: 0 - valLoss: 0.4412725269794464 - trainLoss: 0.44179874658584595\n",
      "cnt: 0 - valLoss: 0.4412713348865509 - trainLoss: 0.441796213388443\n",
      "cnt: 0 - valLoss: 0.44127020239830017 - trainLoss: 0.44179368019104004\n",
      "cnt: 0 - valLoss: 0.44126901030540466 - trainLoss: 0.44179120659828186\n",
      "cnt: 0 - valLoss: 0.44126787781715393 - trainLoss: 0.4417886435985565\n",
      "cnt: 0 - valLoss: 0.44126665592193604 - trainLoss: 0.44178617000579834\n",
      "cnt: 0 - valLoss: 0.4412654936313629 - trainLoss: 0.4417836368083954\n",
      "cnt: 0 - valLoss: 0.4412643015384674 - trainLoss: 0.44178110361099243\n",
      "cnt: 0 - valLoss: 0.44126325845718384 - trainLoss: 0.44177863001823425\n",
      "cnt: 0 - valLoss: 0.44126206636428833 - trainLoss: 0.4417760968208313\n",
      "cnt: 0 - valLoss: 0.4412609338760376 - trainLoss: 0.44177356362342834\n",
      "cnt: 0 - valLoss: 0.44125980138778687 - trainLoss: 0.44177109003067017\n",
      "cnt: 0 - valLoss: 0.4412587285041809 - trainLoss: 0.4417685568332672\n",
      "cnt: 0 - valLoss: 0.441257506608963 - trainLoss: 0.44176605343818665\n",
      "cnt: 0 - valLoss: 0.44125640392303467 - trainLoss: 0.4417635500431061\n",
      "cnt: 0 - valLoss: 0.44125524163246155 - trainLoss: 0.4417610466480255\n",
      "cnt: 0 - valLoss: 0.4412541389465332 - trainLoss: 0.44175851345062256\n",
      "cnt: 0 - valLoss: 0.4412529766559601 - trainLoss: 0.4417560398578644\n",
      "cnt: 0 - valLoss: 0.44125187397003174 - trainLoss: 0.4417535066604614\n",
      "cnt: 0 - valLoss: 0.441250741481781 - trainLoss: 0.44175097346305847\n",
      "cnt: 0 - valLoss: 0.44124963879585266 - trainLoss: 0.4417484998703003\n",
      "cnt: 0 - valLoss: 0.44124835729599 - trainLoss: 0.44174593687057495\n",
      "cnt: 0 - valLoss: 0.44124728441238403 - trainLoss: 0.4417434334754944\n",
      "cnt: 0 - valLoss: 0.44124606251716614 - trainLoss: 0.4417409300804138\n",
      "cnt: 0 - valLoss: 0.4412449896335602 - trainLoss: 0.44173842668533325\n",
      "cnt: 0 - valLoss: 0.44124385714530945 - trainLoss: 0.4417358934879303\n",
      "cnt: 0 - valLoss: 0.44124269485473633 - trainLoss: 0.44173339009284973\n",
      "cnt: 0 - valLoss: 0.4412415623664856 - trainLoss: 0.4417308568954468\n",
      "cnt: 0 - valLoss: 0.44124045968055725 - trainLoss: 0.441728413105011\n",
      "cnt: 0 - valLoss: 0.44123929738998413 - trainLoss: 0.44172587990760803\n",
      "cnt: 0 - valLoss: 0.4412382245063782 - trainLoss: 0.4417233169078827\n",
      "cnt: 0 - valLoss: 0.44123703241348267 - trainLoss: 0.4417208433151245\n",
      "cnt: 0 - valLoss: 0.4412359595298767 - trainLoss: 0.44171836972236633\n",
      "cnt: 0 - valLoss: 0.4412347972393036 - trainLoss: 0.4417158365249634\n",
      "cnt: 0 - valLoss: 0.44123369455337524 - trainLoss: 0.4417133033275604\n",
      "cnt: 0 - valLoss: 0.4412325322628021 - trainLoss: 0.44171082973480225\n",
      "cnt: 0 - valLoss: 0.44123128056526184 - trainLoss: 0.4417082667350769\n",
      "cnt: 0 - valLoss: 0.44123008847236633 - trainLoss: 0.44170576333999634\n",
      "cnt: 0 - valLoss: 0.44122904539108276 - trainLoss: 0.44170328974723816\n",
      "cnt: 0 - valLoss: 0.44122785329818726 - trainLoss: 0.4417007565498352\n",
      "cnt: 0 - valLoss: 0.4412267804145813 - trainLoss: 0.44169825315475464\n",
      "cnt: 0 - valLoss: 0.4412256181240082 - trainLoss: 0.4416957497596741\n",
      "cnt: 0 - valLoss: 0.44122451543807983 - trainLoss: 0.4416932463645935\n",
      "cnt: 0 - valLoss: 0.4412233233451843 - trainLoss: 0.44169071316719055\n",
      "cnt: 0 - valLoss: 0.44122225046157837 - trainLoss: 0.4416882395744324\n",
      "cnt: 0 - valLoss: 0.44122108817100525 - trainLoss: 0.4416857063770294\n",
      "cnt: 0 - valLoss: 0.4412199854850769 - trainLoss: 0.44168323278427124\n",
      "cnt: 0 - valLoss: 0.4412188231945038 - trainLoss: 0.4416806995868683\n",
      "cnt: 0 - valLoss: 0.4412177503108978 - trainLoss: 0.44167816638946533\n",
      "cnt: 0 - valLoss: 0.44121649861335754 - trainLoss: 0.44167569279670715\n",
      "cnt: 0 - valLoss: 0.4412153363227844 - trainLoss: 0.4416731595993042\n",
      "cnt: 0 - valLoss: 0.44121402502059937 - trainLoss: 0.44167062640190125\n",
      "cnt: 0 - valLoss: 0.44121289253234863 - trainLoss: 0.44166815280914307\n",
      "cnt: 0 - valLoss: 0.44121167063713074 - trainLoss: 0.4416656196117401\n",
      "cnt: 0 - valLoss: 0.44121044874191284 - trainLoss: 0.44166314601898193\n",
      "cnt: 0 - valLoss: 0.4412092864513397 - trainLoss: 0.441660612821579\n",
      "cnt: 0 - valLoss: 0.441208153963089 - trainLoss: 0.4416581392288208\n",
      "cnt: 0 - valLoss: 0.4412069022655487 - trainLoss: 0.44165560603141785\n",
      "cnt: 0 - valLoss: 0.44120582938194275 - trainLoss: 0.44165313243865967\n",
      "cnt: 0 - valLoss: 0.4412045478820801 - trainLoss: 0.4416505992412567\n",
      "cnt: 0 - valLoss: 0.44120341539382935 - trainLoss: 0.44164812564849854\n",
      "cnt: 0 - valLoss: 0.44120219349861145 - trainLoss: 0.4416455924510956\n",
      "cnt: 0 - valLoss: 0.4412010610103607 - trainLoss: 0.4416431188583374\n",
      "cnt: 0 - valLoss: 0.4411998391151428 - trainLoss: 0.4416406452655792\n",
      "cnt: 0 - valLoss: 0.4411987066268921 - trainLoss: 0.44163811206817627\n",
      "cnt: 0 - valLoss: 0.4411974847316742 - trainLoss: 0.4416355788707733\n",
      "cnt: 0 - valLoss: 0.4411962330341339 - trainLoss: 0.44163310527801514\n",
      "cnt: 0 - valLoss: 0.4411950409412384 - trainLoss: 0.44163060188293457\n",
      "cnt: 0 - valLoss: 0.44119390845298767 - trainLoss: 0.441628098487854\n",
      "cnt: 0 - valLoss: 0.4411926567554474 - trainLoss: 0.44162559509277344\n",
      "cnt: 0 - valLoss: 0.44119155406951904 - trainLoss: 0.44162309169769287\n",
      "cnt: 0 - valLoss: 0.44119030237197876 - trainLoss: 0.4416205883026123\n",
      "cnt: 0 - valLoss: 0.441189169883728 - trainLoss: 0.44161808490753174\n",
      "cnt: 0 - valLoss: 0.44118794798851013 - trainLoss: 0.4416155517101288\n",
      "cnt: 0 - valLoss: 0.441186785697937 - trainLoss: 0.4416130781173706\n",
      "cnt: 0 - valLoss: 0.4411855936050415 - trainLoss: 0.44161057472229004\n",
      "cnt: 0 - valLoss: 0.44118449091911316 - trainLoss: 0.4416080713272095\n",
      "cnt: 0 - valLoss: 0.44118326902389526 - trainLoss: 0.4416055381298065\n",
      "cnt: 0 - valLoss: 0.44118213653564453 - trainLoss: 0.44160306453704834\n",
      "cnt: 0 - valLoss: 0.44118091464042664 - trainLoss: 0.4416005313396454\n",
      "cnt: 0 - valLoss: 0.4411797821521759 - trainLoss: 0.4415980577468872\n",
      "cnt: 0 - valLoss: 0.44117847084999084 - trainLoss: 0.44159552454948425\n",
      "cnt: 0 - valLoss: 0.4411773681640625 - trainLoss: 0.4415930509567261\n",
      "cnt: 0 - valLoss: 0.441176176071167 - trainLoss: 0.4415905475616455\n",
      "cnt: 0 - valLoss: 0.44117501378059387 - trainLoss: 0.44158801436424255\n",
      "cnt: 0 - valLoss: 0.4411737620830536 - trainLoss: 0.4415855407714844\n",
      "cnt: 0 - valLoss: 0.44117265939712524 - trainLoss: 0.4415830671787262\n",
      "cnt: 0 - valLoss: 0.4411714971065521 - trainLoss: 0.44158053398132324\n",
      "cnt: 0 - valLoss: 0.4411703050136566 - trainLoss: 0.44157806038856506\n",
      "cnt: 0 - valLoss: 0.4411691427230835 - trainLoss: 0.4415755867958069\n",
      "cnt: 0 - valLoss: 0.4411679804325104 - trainLoss: 0.44157305359840393\n",
      "cnt: 0 - valLoss: 0.44116681814193726 - trainLoss: 0.441570520401001\n",
      "cnt: 0 - valLoss: 0.44116562604904175 - trainLoss: 0.4415680468082428\n",
      "cnt: 0 - valLoss: 0.4411645233631134 - trainLoss: 0.44156554341316223\n",
      "cnt: 0 - valLoss: 0.4411633014678955 - trainLoss: 0.44156304001808167\n",
      "cnt: 0 - valLoss: 0.44116219878196716 - trainLoss: 0.4415605366230011\n",
      "cnt: 0 - valLoss: 0.4411608576774597 - trainLoss: 0.44155803322792053\n",
      "cnt: 0 - valLoss: 0.4411596953868866 - trainLoss: 0.44155552983283997\n",
      "cnt: 0 - valLoss: 0.44115856289863586 - trainLoss: 0.4415530264377594\n",
      "cnt: 0 - valLoss: 0.44115740060806274 - trainLoss: 0.44155052304267883\n",
      "cnt: 0 - valLoss: 0.44115620851516724 - trainLoss: 0.44154801964759827\n",
      "cnt: 0 - valLoss: 0.44115498661994934 - trainLoss: 0.4415455162525177\n",
      "cnt: 0 - valLoss: 0.441153883934021 - trainLoss: 0.44154301285743713\n",
      "cnt: 0 - valLoss: 0.4411527216434479 - trainLoss: 0.44154050946235657\n",
      "cnt: 0 - valLoss: 0.44115155935287476 - trainLoss: 0.441538006067276\n",
      "cnt: 0 - valLoss: 0.44115036725997925 - trainLoss: 0.44153550267219543\n",
      "cnt: 0 - valLoss: 0.4411492645740509 - trainLoss: 0.44153302907943726\n",
      "cnt: 0 - valLoss: 0.4411481022834778 - trainLoss: 0.4415305554866791\n",
      "cnt: 0 - valLoss: 0.44114696979522705 - trainLoss: 0.4415280520915985\n",
      "cnt: 0 - valLoss: 0.44114574790000916 - trainLoss: 0.44152548909187317\n",
      "cnt: 0 - valLoss: 0.4411446750164032 - trainLoss: 0.441523015499115\n",
      "cnt: 0 - valLoss: 0.44114333391189575 - trainLoss: 0.4415205419063568\n",
      "cnt: 0 - valLoss: 0.44114232063293457 - trainLoss: 0.44151800870895386\n",
      "cnt: 0 - valLoss: 0.4411410689353943 - trainLoss: 0.4415154755115509\n",
      "cnt: 0 - valLoss: 0.44113999605178833 - trainLoss: 0.4415130317211151\n",
      "cnt: 0 - valLoss: 0.4411388039588928 - trainLoss: 0.44151052832603455\n",
      "cnt: 0 - valLoss: 0.4411376714706421 - trainLoss: 0.441508024930954\n",
      "cnt: 0 - valLoss: 0.4411364793777466 - trainLoss: 0.4415055215358734\n",
      "cnt: 0 - valLoss: 0.44113537669181824 - trainLoss: 0.4415030777454376\n",
      "cnt: 0 - valLoss: 0.44113415479660034 - trainLoss: 0.4415004849433899\n",
      "cnt: 0 - valLoss: 0.4411330819129944 - trainLoss: 0.4414980113506317\n",
      "cnt: 0 - valLoss: 0.4411318600177765 - trainLoss: 0.44149553775787354\n",
      "cnt: 0 - valLoss: 0.44113078713417053 - trainLoss: 0.4414930045604706\n",
      "cnt: 0 - valLoss: 0.44112956523895264 - trainLoss: 0.4414905607700348\n",
      "cnt: 0 - valLoss: 0.4411284923553467 - trainLoss: 0.44148799777030945\n",
      "cnt: 0 - valLoss: 0.4411271810531616 - trainLoss: 0.44148552417755127\n",
      "cnt: 0 - valLoss: 0.44112610816955566 - trainLoss: 0.4414830505847931\n",
      "cnt: 0 - valLoss: 0.44112491607666016 - trainLoss: 0.4414805471897125\n",
      "cnt: 0 - valLoss: 0.4411237835884094 - trainLoss: 0.44147801399230957\n",
      "cnt: 0 - valLoss: 0.4411225914955139 - trainLoss: 0.441475510597229\n",
      "cnt: 0 - valLoss: 0.44112151861190796 - trainLoss: 0.4414730668067932\n",
      "cnt: 0 - valLoss: 0.4411202669143677 - trainLoss: 0.44147053360939026\n",
      "cnt: 0 - valLoss: 0.44111916422843933 - trainLoss: 0.4414680600166321\n",
      "cnt: 0 - valLoss: 0.441118061542511 - trainLoss: 0.44146549701690674\n",
      "cnt: 0 - valLoss: 0.44111689925193787 - trainLoss: 0.44146299362182617\n",
      "cnt: 0 - valLoss: 0.44111576676368713 - trainLoss: 0.441460520029068\n",
      "cnt: 0 - valLoss: 0.441114604473114 - trainLoss: 0.4414580464363098\n",
      "cnt: 0 - valLoss: 0.4411134719848633 - trainLoss: 0.44145554304122925\n",
      "cnt: 0 - valLoss: 0.44111230969429016 - trainLoss: 0.4414530396461487\n",
      "cnt: 0 - valLoss: 0.44111117720603943 - trainLoss: 0.4414505064487457\n",
      "cnt: 0 - valLoss: 0.44110995531082153 - trainLoss: 0.44144803285598755\n",
      "cnt: 0 - valLoss: 0.4411087930202484 - trainLoss: 0.441445529460907\n",
      "cnt: 0 - valLoss: 0.4411076307296753 - trainLoss: 0.4414430558681488\n",
      "cnt: 0 - valLoss: 0.4411064684391022 - trainLoss: 0.4414405822753906\n",
      "cnt: 0 - valLoss: 0.44110536575317383 - trainLoss: 0.44143804907798767\n",
      "cnt: 0 - valLoss: 0.4411041736602783 - trainLoss: 0.4414355158805847\n",
      "cnt: 0 - valLoss: 0.4411030411720276 - trainLoss: 0.44143304228782654\n",
      "cnt: 0 - valLoss: 0.44110187888145447 - trainLoss: 0.4414305090904236\n",
      "cnt: 0 - valLoss: 0.4411008059978485 - trainLoss: 0.4414280652999878\n",
      "cnt: 0 - valLoss: 0.441099613904953 - trainLoss: 0.44142550230026245\n",
      "cnt: 0 - valLoss: 0.44109851121902466 - trainLoss: 0.44142305850982666\n",
      "cnt: 0 - valLoss: 0.44109734892845154 - trainLoss: 0.4414205551147461\n",
      "cnt: 0 - valLoss: 0.4410962760448456 - trainLoss: 0.44141802191734314\n",
      "cnt: 0 - valLoss: 0.4410950839519501 - trainLoss: 0.44141557812690735\n",
      "cnt: 0 - valLoss: 0.44109395146369934 - trainLoss: 0.4414130449295044\n",
      "cnt: 0 - valLoss: 0.44109272956848145 - trainLoss: 0.4414105713367462\n",
      "cnt: 0 - valLoss: 0.4410916566848755 - trainLoss: 0.44140803813934326\n",
      "cnt: 0 - valLoss: 0.4410904347896576 - trainLoss: 0.4414055645465851\n",
      "cnt: 0 - valLoss: 0.44108936190605164 - trainLoss: 0.4414030909538269\n",
      "cnt: 0 - valLoss: 0.44108816981315613 - trainLoss: 0.44140055775642395\n",
      "cnt: 0 - valLoss: 0.44108709692955017 - trainLoss: 0.44139808416366577\n",
      "cnt: 0 - valLoss: 0.44108590483665466 - trainLoss: 0.4413955509662628\n",
      "cnt: 0 - valLoss: 0.44108477234840393 - trainLoss: 0.44139307737350464\n",
      "cnt: 0 - valLoss: 0.4410836696624756 - trainLoss: 0.44139063358306885\n",
      "cnt: 0 - valLoss: 0.44108259677886963 - trainLoss: 0.4413881003856659\n",
      "cnt: 0 - valLoss: 0.4410814344882965 - trainLoss: 0.4413856267929077\n",
      "cnt: 0 - valLoss: 0.44108033180236816 - trainLoss: 0.44138309359550476\n",
      "cnt: 0 - valLoss: 0.44107916951179504 - trainLoss: 0.4413806200027466\n",
      "cnt: 0 - valLoss: 0.4410780966281891 - trainLoss: 0.44137808680534363\n",
      "cnt: 0 - valLoss: 0.4410768151283264 - trainLoss: 0.44137561321258545\n",
      "cnt: 0 - valLoss: 0.44107571244239807 - trainLoss: 0.4413731098175049\n",
      "cnt: 0 - valLoss: 0.44107457995414734 - trainLoss: 0.4413706362247467\n",
      "cnt: 0 - valLoss: 0.4410734176635742 - trainLoss: 0.4413681626319885\n",
      "cnt: 0 - valLoss: 0.4410723149776459 - trainLoss: 0.44136565923690796\n",
      "cnt: 0 - valLoss: 0.4410712420940399 - trainLoss: 0.4413631558418274\n",
      "cnt: 0 - valLoss: 0.4410700798034668 - trainLoss: 0.44136062264442444\n",
      "cnt: 0 - valLoss: 0.4410689175128937 - trainLoss: 0.44135814905166626\n",
      "cnt: 0 - valLoss: 0.4410678446292877 - trainLoss: 0.4413556456565857\n",
      "cnt: 0 - valLoss: 0.441066712141037 - trainLoss: 0.4413531720638275\n",
      "cnt: 0 - valLoss: 0.44106557965278625 - trainLoss: 0.44135069847106934\n",
      "cnt: 0 - valLoss: 0.4410644769668579 - trainLoss: 0.441348135471344\n",
      "cnt: 0 - valLoss: 0.4410633444786072 - trainLoss: 0.4413456916809082\n",
      "cnt: 0 - valLoss: 0.44106224179267883 - trainLoss: 0.44134315848350525\n",
      "cnt: 0 - valLoss: 0.4410610795021057 - trainLoss: 0.44134068489074707\n",
      "cnt: 0 - valLoss: 0.44105982780456543 - trainLoss: 0.4413381814956665\n",
      "cnt: 0 - valLoss: 0.4410587251186371 - trainLoss: 0.4413357079029083\n",
      "cnt: 0 - valLoss: 0.44105762243270874 - trainLoss: 0.44133323431015015\n",
      "cnt: 0 - valLoss: 0.441056489944458 - trainLoss: 0.44133076071739197\n",
      "cnt: 0 - valLoss: 0.44105538725852966 - trainLoss: 0.441328227519989\n",
      "cnt: 0 - valLoss: 0.44105419516563416 - trainLoss: 0.44132569432258606\n",
      "cnt: 0 - valLoss: 0.4410531520843506 - trainLoss: 0.4413232207298279\n",
      "cnt: 0 - valLoss: 0.44105198979377747 - trainLoss: 0.4413207471370697\n",
      "cnt: 0 - valLoss: 0.4410509169101715 - trainLoss: 0.44131824374198914\n",
      "cnt: 0 - valLoss: 0.4410496950149536 - trainLoss: 0.44131577014923096\n",
      "cnt: 0 - valLoss: 0.4410485327243805 - trainLoss: 0.441313236951828\n",
      "cnt: 0 - valLoss: 0.4410473108291626 - trainLoss: 0.4413107633590698\n",
      "cnt: 0 - valLoss: 0.4410461187362671 - trainLoss: 0.44130823016166687\n",
      "cnt: 0 - valLoss: 0.4410448968410492 - trainLoss: 0.4413057565689087\n",
      "cnt: 0 - valLoss: 0.4410437345504761 - trainLoss: 0.4413032829761505\n",
      "cnt: 0 - valLoss: 0.44104239344596863 - trainLoss: 0.44130077958106995\n",
      "cnt: 0 - valLoss: 0.4410412311553955 - trainLoss: 0.44129830598831177\n",
      "cnt: 0 - valLoss: 0.44103994965553284 - trainLoss: 0.4412957727909088\n",
      "cnt: 0 - valLoss: 0.4410388469696045 - trainLoss: 0.44129329919815063\n",
      "cnt: 0 - valLoss: 0.4410376250743866 - trainLoss: 0.4412907660007477\n",
      "cnt: 0 - valLoss: 0.4410364329814911 - trainLoss: 0.4412882924079895\n",
      "cnt: 0 - valLoss: 0.4410352408885956 - trainLoss: 0.4412858188152313\n",
      "cnt: 0 - valLoss: 0.4410340487957001 - trainLoss: 0.44128331542015076\n",
      "cnt: 0 - valLoss: 0.4410328269004822 - trainLoss: 0.4412808120250702\n",
      "cnt: 0 - valLoss: 0.44103163480758667 - trainLoss: 0.4412783682346344\n",
      "cnt: 0 - valLoss: 0.44103044271469116 - trainLoss: 0.44127583503723145\n",
      "cnt: 0 - valLoss: 0.44102925062179565 - trainLoss: 0.4412733018398285\n",
      "cnt: 0 - valLoss: 0.44102802872657776 - trainLoss: 0.4412708282470703\n",
      "cnt: 0 - valLoss: 0.44102686643600464 - trainLoss: 0.44126835465431213\n",
      "cnt: 0 - valLoss: 0.44102558493614197 - trainLoss: 0.44126585125923157\n",
      "cnt: 0 - valLoss: 0.4410243630409241 - trainLoss: 0.4412633776664734\n",
      "cnt: 0 - valLoss: 0.4410231411457062 - trainLoss: 0.44126081466674805\n",
      "cnt: 0 - valLoss: 0.44102200865745544 - trainLoss: 0.44125837087631226\n",
      "cnt: 0 - valLoss: 0.44102078676223755 - trainLoss: 0.4412558376789093\n",
      "cnt: 0 - valLoss: 0.44101962447166443 - trainLoss: 0.4412533640861511\n",
      "cnt: 0 - valLoss: 0.44101840257644653 - trainLoss: 0.44125089049339294\n",
      "cnt: 0 - valLoss: 0.4410172402858734 - trainLoss: 0.4412483870983124\n",
      "cnt: 0 - valLoss: 0.44101598858833313 - trainLoss: 0.4412458837032318\n",
      "cnt: 0 - valLoss: 0.44101482629776 - trainLoss: 0.441243439912796\n",
      "cnt: 0 - valLoss: 0.4410136342048645 - trainLoss: 0.44124090671539307\n",
      "cnt: 0 - valLoss: 0.4410124719142914 - trainLoss: 0.4412384629249573\n",
      "cnt: 0 - valLoss: 0.4410112798213959 - trainLoss: 0.44123589992523193\n",
      "cnt: 0 - valLoss: 0.44101011753082275 - trainLoss: 0.44123342633247375\n",
      "cnt: 0 - valLoss: 0.44100886583328247 - trainLoss: 0.4412309229373932\n",
      "cnt: 0 - valLoss: 0.4410075843334198 - trainLoss: 0.4412285089492798\n",
      "cnt: 0 - valLoss: 0.4410064220428467 - trainLoss: 0.44122597575187683\n",
      "cnt: 0 - valLoss: 0.44100528955459595 - trainLoss: 0.44122347235679626\n",
      "cnt: 0 - valLoss: 0.44100403785705566 - trainLoss: 0.4412209987640381\n",
      "cnt: 0 - valLoss: 0.4410029351711273 - trainLoss: 0.44121846556663513\n",
      "cnt: 0 - valLoss: 0.44100168347358704 - trainLoss: 0.44121605157852173\n",
      "cnt: 0 - valLoss: 0.4410005509853363 - trainLoss: 0.4412135183811188\n",
      "cnt: 0 - valLoss: 0.4409993290901184 - trainLoss: 0.4412109851837158\n",
      "cnt: 0 - valLoss: 0.4409981966018677 - trainLoss: 0.44120854139328003\n",
      "cnt: 0 - valLoss: 0.4409969747066498 - trainLoss: 0.44120603799819946\n",
      "cnt: 0 - valLoss: 0.44099584221839905 - trainLoss: 0.4412035346031189\n",
      "cnt: 0 - valLoss: 0.44099462032318115 - trainLoss: 0.4412010610103607\n",
      "cnt: 0 - valLoss: 0.44099345803260803 - trainLoss: 0.44119858741760254\n",
      "cnt: 0 - valLoss: 0.4409922957420349 - trainLoss: 0.441196084022522\n",
      "cnt: 0 - valLoss: 0.44099101424217224 - trainLoss: 0.4411936104297638\n",
      "cnt: 0 - valLoss: 0.44098979234695435 - trainLoss: 0.4411911368370056\n",
      "cnt: 0 - valLoss: 0.440988689661026 - trainLoss: 0.44118863344192505\n",
      "cnt: 0 - valLoss: 0.4409874379634857 - trainLoss: 0.4411861300468445\n",
      "cnt: 0 - valLoss: 0.4409862756729126 - trainLoss: 0.44118359684944153\n",
      "cnt: 0 - valLoss: 0.4409850835800171 - trainLoss: 0.44118112325668335\n",
      "cnt: 0 - valLoss: 0.44098392128944397 - trainLoss: 0.4411786198616028\n",
      "cnt: 0 - valLoss: 0.44098278880119324 - trainLoss: 0.4411761462688446\n",
      "cnt: 0 - valLoss: 0.44098156690597534 - trainLoss: 0.4411736726760864\n",
      "cnt: 0 - valLoss: 0.44098037481307983 - trainLoss: 0.44117116928100586\n",
      "cnt: 0 - valLoss: 0.4409792125225067 - trainLoss: 0.4411686956882477\n",
      "cnt: 0 - valLoss: 0.4409780204296112 - trainLoss: 0.4411662220954895\n",
      "cnt: 0 - valLoss: 0.4409768581390381 - trainLoss: 0.4411637485027313\n",
      "cnt: 0 - valLoss: 0.4409756660461426 - trainLoss: 0.44116121530532837\n",
      "cnt: 0 - valLoss: 0.44097453355789185 - trainLoss: 0.4411587417125702\n",
      "cnt: 0 - valLoss: 0.440973162651062 - trainLoss: 0.4411562383174896\n",
      "cnt: 0 - valLoss: 0.44097205996513367 - trainLoss: 0.44115376472473145\n",
      "cnt: 0 - valLoss: 0.44097086787223816 - trainLoss: 0.44115129113197327\n",
      "cnt: 0 - valLoss: 0.4409697353839874 - trainLoss: 0.4411487877368927\n",
      "cnt: 0 - valLoss: 0.44096851348876953 - trainLoss: 0.44114628434181213\n",
      "cnt: 0 - valLoss: 0.4409673511981964 - trainLoss: 0.44114378094673157\n",
      "cnt: 0 - valLoss: 0.4409661591053009 - trainLoss: 0.4411413073539734\n",
      "cnt: 0 - valLoss: 0.44096502661705017 - trainLoss: 0.4411388337612152\n",
      "cnt: 0 - valLoss: 0.44096383452415466 - trainLoss: 0.44113630056381226\n",
      "cnt: 0 - valLoss: 0.44096267223358154 - trainLoss: 0.4411338269710541\n",
      "cnt: 0 - valLoss: 0.44096142053604126 - trainLoss: 0.4411313235759735\n",
      "cnt: 0 - valLoss: 0.4409603178501129 - trainLoss: 0.44112884998321533\n",
      "cnt: 0 - valLoss: 0.4409591257572174 - trainLoss: 0.4411263167858124\n",
      "cnt: 0 - valLoss: 0.4409579634666443 - trainLoss: 0.4411238729953766\n",
      "cnt: 0 - valLoss: 0.44095662236213684 - trainLoss: 0.441121369600296\n",
      "cnt: 0 - valLoss: 0.4409555196762085 - trainLoss: 0.44111886620521545\n",
      "cnt: 0 - valLoss: 0.440954327583313 - trainLoss: 0.4411163926124573\n",
      "cnt: 0 - valLoss: 0.44095319509506226 - trainLoss: 0.4411139190196991\n",
      "cnt: 0 - valLoss: 0.44095200300216675 - trainLoss: 0.4411114454269409\n",
      "cnt: 0 - valLoss: 0.44095084071159363 - trainLoss: 0.44110891222953796\n",
      "cnt: 0 - valLoss: 0.4409496486186981 - trainLoss: 0.4411064684391022\n",
      "cnt: 0 - valLoss: 0.440948486328125 - trainLoss: 0.441103994846344\n",
      "cnt: 0 - valLoss: 0.4409472942352295 - trainLoss: 0.44110146164894104\n",
      "cnt: 0 - valLoss: 0.4409460723400116 - trainLoss: 0.4410989284515381\n",
      "cnt: 0 - valLoss: 0.44094496965408325 - trainLoss: 0.4410964846611023\n",
      "cnt: 0 - valLoss: 0.44094377756118774 - trainLoss: 0.44109398126602173\n",
      "cnt: 0 - valLoss: 0.4409426152706146 - trainLoss: 0.44109147787094116\n",
      "cnt: 0 - valLoss: 0.4409414529800415 - trainLoss: 0.441089004278183\n",
      "cnt: 0 - valLoss: 0.440940260887146 - trainLoss: 0.4410865306854248\n",
      "cnt: 0 - valLoss: 0.4409389793872833 - trainLoss: 0.44108402729034424\n",
      "cnt: 0 - valLoss: 0.44093775749206543 - trainLoss: 0.44108155369758606\n",
      "cnt: 0 - valLoss: 0.4409366548061371 - trainLoss: 0.4410790801048279\n",
      "cnt: 0 - valLoss: 0.4409354627132416 - trainLoss: 0.4410766065120697\n",
      "cnt: 0 - valLoss: 0.44093433022499084 - trainLoss: 0.44107407331466675\n",
      "cnt: 0 - valLoss: 0.44093310832977295 - trainLoss: 0.4410715699195862\n",
      "cnt: 0 - valLoss: 0.4409319758415222 - trainLoss: 0.4410690665245056\n",
      "cnt: 0 - valLoss: 0.4409307837486267 - trainLoss: 0.4410666227340698\n",
      "cnt: 0 - valLoss: 0.4409296214580536 - trainLoss: 0.44106414914131165\n",
      "cnt: 0 - valLoss: 0.44092845916748047 - trainLoss: 0.4410616159439087\n",
      "cnt: 0 - valLoss: 0.44092732667922974 - trainLoss: 0.4410591423511505\n",
      "cnt: 0 - valLoss: 0.44092610478401184 - trainLoss: 0.44105663895606995\n",
      "cnt: 0 - valLoss: 0.4409250020980835 - trainLoss: 0.44105416536331177\n",
      "cnt: 0 - valLoss: 0.440923810005188 - trainLoss: 0.4410516917705536\n",
      "cnt: 0 - valLoss: 0.4409225881099701 - trainLoss: 0.441049188375473\n",
      "cnt: 0 - valLoss: 0.4409213960170746 - trainLoss: 0.44104671478271484\n",
      "cnt: 0 - valLoss: 0.44092023372650146 - trainLoss: 0.44104424118995667\n",
      "cnt: 0 - valLoss: 0.44091904163360596 - trainLoss: 0.4410417377948761\n",
      "cnt: 0 - valLoss: 0.4409179389476776 - trainLoss: 0.4410392642021179\n",
      "cnt: 0 - valLoss: 0.4409167766571045 - trainLoss: 0.44103673100471497\n",
      "cnt: 0 - valLoss: 0.44091561436653137 - trainLoss: 0.44103431701660156\n",
      "cnt: 0 - valLoss: 0.44091445207595825 - trainLoss: 0.4410317838191986\n",
      "cnt: 0 - valLoss: 0.44091328978538513 - trainLoss: 0.44102931022644043\n",
      "cnt: 0 - valLoss: 0.4409120976924896 - trainLoss: 0.44102686643600464\n",
      "cnt: 0 - valLoss: 0.4409109950065613 - trainLoss: 0.4410243630409241\n",
      "cnt: 0 - valLoss: 0.44090983271598816 - trainLoss: 0.4410218596458435\n",
      "cnt: 0 - valLoss: 0.44090867042541504 - trainLoss: 0.44101932644844055\n",
      "cnt: 0 - valLoss: 0.4409075081348419 - trainLoss: 0.4410168528556824\n",
      "cnt: 0 - valLoss: 0.4409063458442688 - trainLoss: 0.4410143494606018\n",
      "cnt: 0 - valLoss: 0.4409051239490509 - trainLoss: 0.441011905670166\n",
      "cnt: 0 - valLoss: 0.440903902053833 - trainLoss: 0.44100940227508545\n",
      "cnt: 0 - valLoss: 0.44090279936790466 - trainLoss: 0.4410068988800049\n",
      "cnt: 0 - valLoss: 0.44090160727500916 - trainLoss: 0.4410044252872467\n",
      "cnt: 0 - valLoss: 0.4409004747867584 - trainLoss: 0.4410019814968109\n",
      "cnt: 0 - valLoss: 0.4408993124961853 - trainLoss: 0.44099950790405273\n",
      "cnt: 0 - valLoss: 0.44089818000793457 - trainLoss: 0.4409969747066498\n",
      "cnt: 0 - valLoss: 0.4408969581127167 - trainLoss: 0.4409945011138916\n",
      "cnt: 0 - valLoss: 0.44089585542678833 - trainLoss: 0.4409920275211334\n",
      "cnt: 0 - valLoss: 0.4408946633338928 - trainLoss: 0.44098952412605286\n",
      "cnt: 0 - valLoss: 0.4408935308456421 - trainLoss: 0.4409870505332947\n",
      "cnt: 0 - valLoss: 0.4408923387527466 - trainLoss: 0.4409845769405365\n",
      "cnt: 0 - valLoss: 0.4408912658691406 - trainLoss: 0.44098207354545593\n",
      "cnt: 0 - valLoss: 0.44089001417160034 - trainLoss: 0.44097959995269775\n",
      "cnt: 0 - valLoss: 0.4408889412879944 - trainLoss: 0.4409771263599396\n",
      "cnt: 0 - valLoss: 0.4408876299858093 - trainLoss: 0.4409746527671814\n",
      "cnt: 0 - valLoss: 0.4408864974975586 - trainLoss: 0.44097214937210083\n",
      "cnt: 0 - valLoss: 0.4408853054046631 - trainLoss: 0.44096967577934265\n",
      "cnt: 0 - valLoss: 0.44088423252105713 - trainLoss: 0.4409672021865845\n",
      "cnt: 0 - valLoss: 0.4408830404281616 - trainLoss: 0.4409646987915039\n",
      "cnt: 0 - valLoss: 0.4408819079399109 - trainLoss: 0.4409622251987457\n",
      "cnt: 0 - valLoss: 0.4408807158470154 - trainLoss: 0.44095975160598755\n",
      "cnt: 0 - valLoss: 0.44087961316108704 - trainLoss: 0.440957248210907\n",
      "cnt: 0 - valLoss: 0.4408784508705139 - trainLoss: 0.4409547746181488\n",
      "cnt: 0 - valLoss: 0.4408772587776184 - trainLoss: 0.4409523010253906\n",
      "cnt: 0 - valLoss: 0.4408761262893677 - trainLoss: 0.44094982743263245\n",
      "cnt: 0 - valLoss: 0.44087496399879456 - trainLoss: 0.4409473240375519\n",
      "cnt: 0 - valLoss: 0.4408738315105438 - trainLoss: 0.4409447908401489\n",
      "cnt: 0 - valLoss: 0.4408726692199707 - trainLoss: 0.44094231724739075\n",
      "cnt: 0 - valLoss: 0.4408715069293976 - trainLoss: 0.44093984365463257\n",
      "cnt: 0 - valLoss: 0.4408702254295349 - trainLoss: 0.4409373700618744\n",
      "cnt: 0 - valLoss: 0.4408690631389618 - trainLoss: 0.4409349262714386\n",
      "cnt: 0 - valLoss: 0.44086799025535583 - trainLoss: 0.44093239307403564\n",
      "cnt: 0 - valLoss: 0.4408667981624603 - trainLoss: 0.44092994928359985\n",
      "cnt: 0 - valLoss: 0.4408656656742096 - trainLoss: 0.4409273862838745\n",
      "cnt: 0 - valLoss: 0.4408644735813141 - trainLoss: 0.44092491269111633\n",
      "cnt: 0 - valLoss: 0.44086337089538574 - trainLoss: 0.44092240929603577\n",
      "cnt: 0 - valLoss: 0.4408622086048126 - trainLoss: 0.4409199357032776\n",
      "cnt: 0 - valLoss: 0.4408610463142395 - trainLoss: 0.4409174621105194\n",
      "cnt: 0 - valLoss: 0.4408598840236664 - trainLoss: 0.44091495871543884\n",
      "cnt: 0 - valLoss: 0.44085872173309326 - trainLoss: 0.44091254472732544\n",
      "cnt: 0 - valLoss: 0.4408576190471649 - trainLoss: 0.4409100413322449\n",
      "cnt: 0 - valLoss: 0.4408564567565918 - trainLoss: 0.4409075677394867\n",
      "cnt: 0 - valLoss: 0.4408552944660187 - trainLoss: 0.4409050941467285\n",
      "cnt: 0 - valLoss: 0.4408540725708008 - trainLoss: 0.44090256094932556\n",
      "cnt: 0 - valLoss: 0.44085291028022766 - trainLoss: 0.4409000873565674\n",
      "cnt: 0 - valLoss: 0.44085177779197693 - trainLoss: 0.4408975839614868\n",
      "cnt: 0 - valLoss: 0.4408505856990814 - trainLoss: 0.44089511036872864\n",
      "cnt: 0 - valLoss: 0.4408494830131531 - trainLoss: 0.44089263677597046\n",
      "cnt: 0 - valLoss: 0.4408482611179352 - trainLoss: 0.4408901333808899\n",
      "cnt: 0 - valLoss: 0.44084715843200684 - trainLoss: 0.4408876597881317\n",
      "cnt: 0 - valLoss: 0.4408459961414337 - trainLoss: 0.44088518619537354\n",
      "cnt: 0 - valLoss: 0.4408448338508606 - trainLoss: 0.44088271260261536\n",
      "cnt: 0 - valLoss: 0.44084373116493225 - trainLoss: 0.4408802092075348\n",
      "cnt: 0 - valLoss: 0.44084256887435913 - trainLoss: 0.4408777356147766\n",
      "cnt: 0 - valLoss: 0.440841406583786 - trainLoss: 0.44087526202201843\n",
      "cnt: 0 - valLoss: 0.44084030389785767 - trainLoss: 0.44087275862693787\n",
      "cnt: 0 - valLoss: 0.44083914160728455 - trainLoss: 0.4408702850341797\n",
      "cnt: 0 - valLoss: 0.4408379793167114 - trainLoss: 0.4408678114414215\n",
      "cnt: 0 - valLoss: 0.44083669781684875 - trainLoss: 0.44086530804634094\n",
      "cnt: 0 - valLoss: 0.4408356249332428 - trainLoss: 0.44086283445358276\n",
      "cnt: 0 - valLoss: 0.4408344626426697 - trainLoss: 0.4408603608608246\n",
      "cnt: 0 - valLoss: 0.44083356857299805 - trainLoss: 0.4408578872680664\n",
      "cnt: 0 - valLoss: 0.44083258509635925 - trainLoss: 0.44085538387298584\n",
      "cnt: 0 - valLoss: 0.4408317804336548 - trainLoss: 0.4408528804779053\n",
      "cnt: 0 - valLoss: 0.4408308267593384 - trainLoss: 0.4408503472805023\n",
      "cnt: 0 - valLoss: 0.44082996249198914 - trainLoss: 0.44084787368774414\n",
      "cnt: 0 - valLoss: 0.4408290386199951 - trainLoss: 0.4408453404903412\n",
      "cnt: 0 - valLoss: 0.4408281445503235 - trainLoss: 0.44084280729293823\n",
      "cnt: 0 - valLoss: 0.4408271908760071 - trainLoss: 0.44084033370018005\n",
      "cnt: 0 - valLoss: 0.4408262372016907 - trainLoss: 0.4408378005027771\n",
      "cnt: 0 - valLoss: 0.4408252239227295 - trainLoss: 0.4408353269100189\n",
      "cnt: 0 - valLoss: 0.44082432985305786 - trainLoss: 0.44083282351493835\n",
      "cnt: 0 - valLoss: 0.4408233165740967 - trainLoss: 0.4408303201198578\n",
      "cnt: 0 - valLoss: 0.4408223032951355 - trainLoss: 0.4408278167247772\n",
      "cnt: 0 - valLoss: 0.4408213198184967 - trainLoss: 0.44082531332969666\n",
      "cnt: 0 - valLoss: 0.4408203661441803 - trainLoss: 0.44082286953926086\n",
      "cnt: 0 - valLoss: 0.4408193826675415 - trainLoss: 0.4408203363418579\n",
      "cnt: 0 - valLoss: 0.4408184289932251 - trainLoss: 0.44081780314445496\n",
      "cnt: 0 - valLoss: 0.4408174455165863 - trainLoss: 0.4408153295516968\n",
      "cnt: 0 - valLoss: 0.4408164620399475 - trainLoss: 0.4408128559589386\n",
      "cnt: 0 - valLoss: 0.4408154785633087 - trainLoss: 0.44081035256385803\n",
      "cnt: 0 - valLoss: 0.44081446528434753 - trainLoss: 0.44080784916877747\n",
      "cnt: 0 - valLoss: 0.44081342220306396 - trainLoss: 0.4408053159713745\n",
      "cnt: 0 - valLoss: 0.44081249833106995 - trainLoss: 0.44080284237861633\n",
      "cnt: 0 - valLoss: 0.440811425447464 - trainLoss: 0.44080033898353577\n",
      "cnt: 0 - valLoss: 0.4408104717731476 - trainLoss: 0.4407978653907776\n",
      "cnt: 0 - valLoss: 0.4408093988895416 - trainLoss: 0.4407953917980194\n",
      "cnt: 0 - valLoss: 0.44080841541290283 - trainLoss: 0.44079288840293884\n",
      "cnt: 0 - valLoss: 0.44080737233161926 - trainLoss: 0.44079041481018066\n",
      "cnt: 0 - valLoss: 0.4408062696456909 - trainLoss: 0.4407878816127777\n",
      "cnt: 0 - valLoss: 0.4408052861690521 - trainLoss: 0.44078540802001953\n",
      "cnt: 0 - valLoss: 0.44080430269241333 - trainLoss: 0.44078293442726135\n",
      "cnt: 0 - valLoss: 0.44080325961112976 - trainLoss: 0.4407804012298584\n",
      "cnt: 0 - valLoss: 0.4408022463321686 - trainLoss: 0.4407779276371002\n",
      "cnt: 0 - valLoss: 0.440801203250885 - trainLoss: 0.44077545404434204\n",
      "cnt: 0 - valLoss: 0.440800279378891 - trainLoss: 0.4407729208469391\n",
      "cnt: 0 - valLoss: 0.44079920649528503 - trainLoss: 0.4407704770565033\n",
      "cnt: 0 - valLoss: 0.44079822301864624 - trainLoss: 0.44076794385910034\n",
      "cnt: 0 - valLoss: 0.44079726934432983 - trainLoss: 0.44076547026634216\n",
      "cnt: 0 - valLoss: 0.44079625606536865 - trainLoss: 0.440762996673584\n",
      "cnt: 0 - valLoss: 0.4407952129840851 - trainLoss: 0.44076046347618103\n",
      "cnt: 0 - valLoss: 0.4407942295074463 - trainLoss: 0.44075798988342285\n",
      "cnt: 0 - valLoss: 0.44079315662384033 - trainLoss: 0.4407554864883423\n",
      "cnt: 0 - valLoss: 0.4407922029495239 - trainLoss: 0.4407530128955841\n",
      "cnt: 0 - valLoss: 0.4407910406589508 - trainLoss: 0.4407505393028259\n",
      "cnt: 0 - valLoss: 0.440790057182312 - trainLoss: 0.44074803590774536\n",
      "cnt: 0 - valLoss: 0.4407890737056732 - trainLoss: 0.4407455623149872\n",
      "cnt: 0 - valLoss: 0.4407880902290344 - trainLoss: 0.44074302911758423\n",
      "cnt: 0 - valLoss: 0.4407871663570404 - trainLoss: 0.44074058532714844\n",
      "cnt: 0 - valLoss: 0.4407860338687897 - trainLoss: 0.44073808193206787\n",
      "cnt: 0 - valLoss: 0.44078510999679565 - trainLoss: 0.4407355785369873\n",
      "cnt: 0 - valLoss: 0.4407840669155121 - trainLoss: 0.44073307514190674\n",
      "cnt: 0 - valLoss: 0.4407831132411957 - trainLoss: 0.44073063135147095\n",
      "cnt: 0 - valLoss: 0.4407820403575897 - trainLoss: 0.440728098154068\n",
      "cnt: 0 - valLoss: 0.4407810568809509 - trainLoss: 0.4407256245613098\n",
      "cnt: 0 - valLoss: 0.44078007340431213 - trainLoss: 0.44072312116622925\n",
      "cnt: 0 - valLoss: 0.44077908992767334 - trainLoss: 0.44072064757347107\n",
      "cnt: 0 - valLoss: 0.44077807664871216 - trainLoss: 0.4407181739807129\n",
      "cnt: 0 - valLoss: 0.44077712297439575 - trainLoss: 0.4407157003879547\n",
      "cnt: 0 - valLoss: 0.44077610969543457 - trainLoss: 0.44071319699287415\n",
      "cnt: 0 - valLoss: 0.44077497720718384 - trainLoss: 0.4407106935977936\n",
      "cnt: 0 - valLoss: 0.44077396392822266 - trainLoss: 0.440708190202713\n",
      "cnt: 0 - valLoss: 0.44077301025390625 - trainLoss: 0.44070571660995483\n",
      "cnt: 0 - valLoss: 0.4407719671726227 - trainLoss: 0.44070324301719666\n",
      "cnt: 0 - valLoss: 0.4407709538936615 - trainLoss: 0.4407007098197937\n",
      "cnt: 0 - valLoss: 0.4407700002193451 - trainLoss: 0.4406982362270355\n",
      "cnt: 0 - valLoss: 0.4407689869403839 - trainLoss: 0.44069573283195496\n",
      "cnt: 0 - valLoss: 0.4407680034637451 - trainLoss: 0.4406932592391968\n",
      "cnt: 0 - valLoss: 0.4407670199871063 - trainLoss: 0.4406907856464386\n",
      "cnt: 0 - valLoss: 0.44076603651046753 - trainLoss: 0.44068828225135803\n",
      "cnt: 0 - valLoss: 0.4407649636268616 - trainLoss: 0.44068580865859985\n",
      "cnt: 0 - valLoss: 0.44076400995254517 - trainLoss: 0.4406833350658417\n",
      "cnt: 0 - valLoss: 0.4407629668712616 - trainLoss: 0.4406808018684387\n",
      "cnt: 0 - valLoss: 0.4407620429992676 - trainLoss: 0.44067832827568054\n",
      "cnt: 0 - valLoss: 0.440760999917984 - trainLoss: 0.44067585468292236\n",
      "cnt: 0 - valLoss: 0.4407600164413452 - trainLoss: 0.4406733214855194\n",
      "cnt: 0 - valLoss: 0.4407588541507721 - trainLoss: 0.44067081809043884\n",
      "cnt: 0 - valLoss: 0.4407579302787781 - trainLoss: 0.44066840410232544\n",
      "cnt: 0 - valLoss: 0.4407568871974945 - trainLoss: 0.4406658709049225\n",
      "cnt: 0 - valLoss: 0.4407559037208557 - trainLoss: 0.4406634271144867\n",
      "cnt: 0 - valLoss: 0.4407549202442169 - trainLoss: 0.44066089391708374\n",
      "cnt: 0 - valLoss: 0.44075390696525574 - trainLoss: 0.44065842032432556\n",
      "cnt: 0 - valLoss: 0.44075295329093933 - trainLoss: 0.4406559467315674\n",
      "cnt: 0 - valLoss: 0.44075191020965576 - trainLoss: 0.4406534433364868\n",
      "cnt: 0 - valLoss: 0.44075092673301697 - trainLoss: 0.44065096974372864\n",
      "cnt: 0 - valLoss: 0.4407499134540558 - trainLoss: 0.44064849615097046\n",
      "cnt: 0 - valLoss: 0.440748929977417 - trainLoss: 0.4406459629535675\n",
      "cnt: 0 - valLoss: 0.4407478868961334 - trainLoss: 0.4406434893608093\n",
      "cnt: 0 - valLoss: 0.44074690341949463 - trainLoss: 0.44064104557037354\n",
      "cnt: 0 - valLoss: 0.440746009349823 - trainLoss: 0.4406384825706482\n",
      "cnt: 0 - valLoss: 0.4407450258731842 - trainLoss: 0.4406359791755676\n",
      "cnt: 0 - valLoss: 0.4407441318035126 - trainLoss: 0.44063350558280945\n",
      "cnt: 0 - valLoss: 0.44074320793151855 - trainLoss: 0.44063103199005127\n",
      "cnt: 0 - valLoss: 0.44074222445487976 - trainLoss: 0.4406284689903259\n",
      "cnt: 0 - valLoss: 0.44074130058288574 - trainLoss: 0.44062596559524536\n",
      "cnt: 0 - valLoss: 0.44074034690856934 - trainLoss: 0.4406234920024872\n",
      "cnt: 0 - valLoss: 0.4407394826412201 - trainLoss: 0.440621018409729\n",
      "cnt: 0 - valLoss: 0.4407385587692261 - trainLoss: 0.44061851501464844\n",
      "cnt: 0 - valLoss: 0.4407375156879425 - trainLoss: 0.4406159520149231\n",
      "cnt: 0 - valLoss: 0.44073665142059326 - trainLoss: 0.4406134784221649\n",
      "cnt: 0 - valLoss: 0.44073572754859924 - trainLoss: 0.44061100482940674\n",
      "cnt: 0 - valLoss: 0.44073486328125 - trainLoss: 0.44060850143432617\n",
      "cnt: 0 - valLoss: 0.44073379039764404 - trainLoss: 0.440606027841568\n",
      "cnt: 0 - valLoss: 0.4407329261302948 - trainLoss: 0.4406035542488098\n",
      "cnt: 0 - valLoss: 0.4407320022583008 - trainLoss: 0.4406009912490845\n",
      "cnt: 0 - valLoss: 0.44073107838630676 - trainLoss: 0.4405984878540039\n",
      "cnt: 0 - valLoss: 0.4407300651073456 - trainLoss: 0.4405960142612457\n",
      "cnt: 0 - valLoss: 0.44072920083999634 - trainLoss: 0.44059354066848755\n",
      "cnt: 0 - valLoss: 0.4407282769680023 - trainLoss: 0.4405910074710846\n",
      "cnt: 0 - valLoss: 0.4407274127006531 - trainLoss: 0.44058847427368164\n",
      "cnt: 0 - valLoss: 0.4407263696193695 - trainLoss: 0.44058600068092346\n",
      "cnt: 0 - valLoss: 0.44072550535202026 - trainLoss: 0.4405835270881653\n",
      "cnt: 0 - valLoss: 0.44072452187538147 - trainLoss: 0.4405810236930847\n",
      "cnt: 0 - valLoss: 0.4407236874103546 - trainLoss: 0.44057852029800415\n",
      "cnt: 0 - valLoss: 0.44072264432907104 - trainLoss: 0.4405759871006012\n",
      "cnt: 0 - valLoss: 0.4407217502593994 - trainLoss: 0.440573513507843\n",
      "cnt: 0 - valLoss: 0.4407208561897278 - trainLoss: 0.44057101011276245\n",
      "cnt: 0 - valLoss: 0.4407199025154114 - trainLoss: 0.4405685067176819\n",
      "cnt: 0 - valLoss: 0.4407189190387726 - trainLoss: 0.4405660033226013\n",
      "cnt: 0 - valLoss: 0.4407179653644562 - trainLoss: 0.44056349992752075\n",
      "cnt: 0 - valLoss: 0.44071707129478455 - trainLoss: 0.4405609965324402\n",
      "cnt: 0 - valLoss: 0.4407161772251129 - trainLoss: 0.44055846333503723\n",
      "cnt: 0 - valLoss: 0.44071513414382935 - trainLoss: 0.44055598974227905\n",
      "cnt: 0 - valLoss: 0.4407142102718353 - trainLoss: 0.4405534565448761\n",
      "cnt: 0 - valLoss: 0.4407133460044861 - trainLoss: 0.4405509829521179\n",
      "cnt: 0 - valLoss: 0.4407123923301697 - trainLoss: 0.44054850935935974\n",
      "cnt: 0 - valLoss: 0.44071146845817566 - trainLoss: 0.44054603576660156\n",
      "cnt: 0 - valLoss: 0.44071048498153687 - trainLoss: 0.4405435025691986\n",
      "cnt: 0 - valLoss: 0.44070956110954285 - trainLoss: 0.44054102897644043\n",
      "cnt: 0 - valLoss: 0.4407086670398712 - trainLoss: 0.4405384957790375\n",
      "cnt: 0 - valLoss: 0.4407076835632324 - trainLoss: 0.4405359923839569\n",
      "cnt: 0 - valLoss: 0.4407067596912384 - trainLoss: 0.44053351879119873\n",
      "cnt: 0 - valLoss: 0.44070589542388916 - trainLoss: 0.4405309855937958\n",
      "cnt: 0 - valLoss: 0.44070494174957275 - trainLoss: 0.4405285120010376\n",
      "cnt: 0 - valLoss: 0.4407039284706116 - trainLoss: 0.44052597880363464\n",
      "cnt: 0 - valLoss: 0.44070300459861755 - trainLoss: 0.44052350521087646\n",
      "cnt: 0 - valLoss: 0.4407021105289459 - trainLoss: 0.4405209720134735\n",
      "cnt: 0 - valLoss: 0.4407011866569519 - trainLoss: 0.44051849842071533\n",
      "cnt: 0 - valLoss: 0.44070014357566833 - trainLoss: 0.44051605463027954\n",
      "cnt: 0 - valLoss: 0.4406992495059967 - trainLoss: 0.4405134916305542\n",
      "cnt: 0 - valLoss: 0.4406983554363251 - trainLoss: 0.440511018037796\n",
      "cnt: 0 - valLoss: 0.44069743156433105 - trainLoss: 0.44050851464271545\n",
      "cnt: 0 - valLoss: 0.4406964182853699 - trainLoss: 0.4405060112476349\n",
      "cnt: 0 - valLoss: 0.44069552421569824 - trainLoss: 0.4405035078525543\n",
      "cnt: 0 - valLoss: 0.44069457054138184 - trainLoss: 0.44050100445747375\n",
      "cnt: 0 - valLoss: 0.4406936764717102 - trainLoss: 0.4404985010623932\n",
      "cnt: 0 - valLoss: 0.44069263339042664 - trainLoss: 0.440496027469635\n",
      "cnt: 0 - valLoss: 0.440691739320755 - trainLoss: 0.44049355387687683\n",
      "cnt: 0 - valLoss: 0.4406908452510834 - trainLoss: 0.4404909908771515\n",
      "cnt: 0 - valLoss: 0.44068995118141174 - trainLoss: 0.4404885470867157\n",
      "cnt: 0 - valLoss: 0.4406889081001282 - trainLoss: 0.44048601388931274\n",
      "cnt: 0 - valLoss: 0.44068801403045654 - trainLoss: 0.44048354029655457\n",
      "cnt: 0 - valLoss: 0.44068703055381775 - trainLoss: 0.440481036901474\n",
      "cnt: 0 - valLoss: 0.4406861662864685 - trainLoss: 0.44047853350639343\n",
      "cnt: 0 - valLoss: 0.44068509340286255 - trainLoss: 0.4404760003089905\n",
      "cnt: 0 - valLoss: 0.4406841993331909 - trainLoss: 0.4404735267162323\n",
      "cnt: 0 - valLoss: 0.4406833052635193 - trainLoss: 0.44047102332115173\n",
      "cnt: 0 - valLoss: 0.44068241119384766 - trainLoss: 0.44046851992607117\n",
      "cnt: 0 - valLoss: 0.4406813681125641 - trainLoss: 0.4404660761356354\n",
      "cnt: 0 - valLoss: 0.44068050384521484 - trainLoss: 0.4404635429382324\n",
      "cnt: 0 - valLoss: 0.44067952036857605 - trainLoss: 0.44046100974082947\n",
      "cnt: 0 - valLoss: 0.4406786262989044 - trainLoss: 0.4404585361480713\n",
      "cnt: 0 - valLoss: 0.44067761301994324 - trainLoss: 0.4404560625553131\n",
      "cnt: 0 - valLoss: 0.4406766891479492 - trainLoss: 0.44045355916023254\n",
      "cnt: 0 - valLoss: 0.4406757950782776 - trainLoss: 0.440451055765152\n",
      "cnt: 0 - valLoss: 0.44067490100860596 - trainLoss: 0.440448522567749\n",
      "cnt: 0 - valLoss: 0.440673828125 - trainLoss: 0.44044604897499084\n",
      "cnt: 0 - valLoss: 0.44067293405532837 - trainLoss: 0.4404435455799103\n",
      "cnt: 0 - valLoss: 0.44067198038101196 - trainLoss: 0.4404410719871521\n",
      "cnt: 0 - valLoss: 0.44067108631134033 - trainLoss: 0.44043853878974915\n",
      "cnt: 0 - valLoss: 0.44067004323005676 - trainLoss: 0.44043606519699097\n",
      "cnt: 0 - valLoss: 0.44066914916038513 - trainLoss: 0.440433531999588\n",
      "cnt: 0 - valLoss: 0.4406682550907135 - trainLoss: 0.44043105840682983\n",
      "cnt: 0 - valLoss: 0.44066736102104187 - trainLoss: 0.44042858481407166\n",
      "cnt: 0 - valLoss: 0.4406662881374359 - trainLoss: 0.4404260516166687\n",
      "cnt: 0 - valLoss: 0.4406653940677643 - trainLoss: 0.4404235780239105\n",
      "cnt: 0 - valLoss: 0.4406644403934479 - trainLoss: 0.44042107462882996\n",
      "cnt: 0 - valLoss: 0.44066354632377625 - trainLoss: 0.4404185712337494\n",
      "cnt: 0 - valLoss: 0.4406625032424927 - trainLoss: 0.4404160678386688\n",
      "cnt: 0 - valLoss: 0.4406616687774658 - trainLoss: 0.44041359424591064\n",
      "cnt: 0 - valLoss: 0.4406607151031494 - trainLoss: 0.4404110610485077\n",
      "cnt: 0 - valLoss: 0.440659761428833 - trainLoss: 0.4404085874557495\n",
      "cnt: 0 - valLoss: 0.4406587481498718 - trainLoss: 0.44040611386299133\n",
      "cnt: 0 - valLoss: 0.4406577944755554 - trainLoss: 0.44040361046791077\n",
      "cnt: 0 - valLoss: 0.4406568109989166 - trainLoss: 0.4404011070728302\n",
      "cnt: 0 - valLoss: 0.4406558573246002 - trainLoss: 0.44039860367774963\n",
      "cnt: 0 - valLoss: 0.44065478444099426 - trainLoss: 0.44039613008499146\n",
      "cnt: 0 - valLoss: 0.44065386056900024 - trainLoss: 0.4403936564922333\n",
      "cnt: 0 - valLoss: 0.44065290689468384 - trainLoss: 0.4403911530971527\n",
      "cnt: 0 - valLoss: 0.4406519830226898 - trainLoss: 0.44038867950439453\n",
      "cnt: 0 - valLoss: 0.4406510293483734 - trainLoss: 0.44038620591163635\n",
      "cnt: 0 - valLoss: 0.44064998626708984 - trainLoss: 0.4403837323188782\n",
      "cnt: 0 - valLoss: 0.44064900279045105 - trainLoss: 0.4403811991214752\n",
      "cnt: 0 - valLoss: 0.4406481087207794 - trainLoss: 0.44037872552871704\n",
      "cnt: 0 - valLoss: 0.440647155046463 - trainLoss: 0.44037628173828125\n",
      "cnt: 0 - valLoss: 0.44064605236053467 - trainLoss: 0.4403737485408783\n",
      "cnt: 0 - valLoss: 0.44064509868621826 - trainLoss: 0.4403712749481201\n",
      "cnt: 0 - valLoss: 0.44064420461654663 - trainLoss: 0.44036877155303955\n",
      "cnt: 0 - valLoss: 0.44064319133758545 - trainLoss: 0.440366268157959\n",
      "cnt: 0 - valLoss: 0.4406421184539795 - trainLoss: 0.4403638243675232\n",
      "cnt: 0 - valLoss: 0.4406410753726959 - trainLoss: 0.4403613209724426\n",
      "cnt: 0 - valLoss: 0.4406401515007019 - trainLoss: 0.4403589069843292\n",
      "cnt: 0 - valLoss: 0.44063910841941833 - trainLoss: 0.4403563141822815\n",
      "cnt: 0 - valLoss: 0.44063809514045715 - trainLoss: 0.4403539299964905\n",
      "cnt: 0 - valLoss: 0.4406370222568512 - trainLoss: 0.4403514564037323\n",
      "cnt: 0 - valLoss: 0.4406360387802124 - trainLoss: 0.44034895300865173\n",
      "cnt: 0 - valLoss: 0.44063499569892883 - trainLoss: 0.44034647941589355\n",
      "cnt: 0 - valLoss: 0.44063395261764526 - trainLoss: 0.4403440058231354\n",
      "cnt: 0 - valLoss: 0.4406329393386841 - trainLoss: 0.4403415024280548\n",
      "cnt: 0 - valLoss: 0.4406319856643677 - trainLoss: 0.44033902883529663\n",
      "cnt: 0 - valLoss: 0.4406309723854065 - trainLoss: 0.44033655524253845\n",
      "cnt: 0 - valLoss: 0.4406299293041229 - trainLoss: 0.4403340816497803\n",
      "cnt: 0 - valLoss: 0.44062891602516174 - trainLoss: 0.4403315782546997\n",
      "cnt: 0 - valLoss: 0.4406278729438782 - trainLoss: 0.44032910466194153\n",
      "cnt: 0 - valLoss: 0.44062694907188416 - trainLoss: 0.44032663106918335\n",
      "cnt: 0 - valLoss: 0.4406258165836334 - trainLoss: 0.4403241276741028\n",
      "cnt: 0 - valLoss: 0.44062483310699463 - trainLoss: 0.4403216540813446\n",
      "cnt: 0 - valLoss: 0.44062384963035583 - trainLoss: 0.4403192102909088\n",
      "cnt: 0 - valLoss: 0.44062283635139465 - trainLoss: 0.44031673669815063\n",
      "cnt: 0 - valLoss: 0.4406217932701111 - trainLoss: 0.4403142035007477\n",
      "cnt: 0 - valLoss: 0.4406207203865051 - trainLoss: 0.44031181931495667\n",
      "cnt: 0 - valLoss: 0.44061973690986633 - trainLoss: 0.4403092861175537\n",
      "cnt: 0 - valLoss: 0.44061869382858276 - trainLoss: 0.44030681252479553\n",
      "cnt: 0 - valLoss: 0.44061771035194397 - trainLoss: 0.44030430912971497\n",
      "cnt: 0 - valLoss: 0.4406166076660156 - trainLoss: 0.44030189514160156\n",
      "cnt: 0 - valLoss: 0.44061562418937683 - trainLoss: 0.440299391746521\n",
      "cnt: 0 - valLoss: 0.44061458110809326 - trainLoss: 0.4402969181537628\n",
      "cnt: 0 - valLoss: 0.44061365723609924 - trainLoss: 0.44029444456100464\n",
      "cnt: 0 - valLoss: 0.44061243534088135 - trainLoss: 0.4402919411659241\n",
      "cnt: 0 - valLoss: 0.44061151146888733 - trainLoss: 0.4402894675731659\n",
      "cnt: 0 - valLoss: 0.44061046838760376 - trainLoss: 0.4402869939804077\n",
      "cnt: 0 - valLoss: 0.44060948491096497 - trainLoss: 0.44028449058532715\n",
      "cnt: 0 - valLoss: 0.440608412027359 - trainLoss: 0.44028201699256897\n",
      "cnt: 0 - valLoss: 0.4406073987483978 - trainLoss: 0.4402795433998108\n",
      "cnt: 0 - valLoss: 0.44060632586479187 - trainLoss: 0.440277099609375\n",
      "cnt: 0 - valLoss: 0.4406053423881531 - trainLoss: 0.4402746260166168\n",
      "cnt: 0 - valLoss: 0.44060438871383667 - trainLoss: 0.44027209281921387\n",
      "cnt: 0 - valLoss: 0.4406033754348755 - trainLoss: 0.4402696490287781\n",
      "cnt: 0 - valLoss: 0.44060224294662476 - trainLoss: 0.4402671754360199\n",
      "cnt: 0 - valLoss: 0.4406012296676636 - trainLoss: 0.4402647018432617\n",
      "cnt: 0 - valLoss: 0.44060027599334717 - trainLoss: 0.4402622580528259\n",
      "cnt: 0 - valLoss: 0.440599262714386 - trainLoss: 0.44025975465774536\n",
      "cnt: 0 - valLoss: 0.44059813022613525 - trainLoss: 0.4402572810649872\n",
      "cnt: 0 - valLoss: 0.4405971169471741 - trainLoss: 0.440254807472229\n",
      "cnt: 0 - valLoss: 0.44059616327285767 - trainLoss: 0.44025230407714844\n",
      "cnt: 0 - valLoss: 0.4405950903892517 - trainLoss: 0.44024983048439026\n",
      "cnt: 0 - valLoss: 0.4405941367149353 - trainLoss: 0.4402473568916321\n",
      "cnt: 0 - valLoss: 0.44059303402900696 - trainLoss: 0.4402449131011963\n",
      "cnt: 0 - valLoss: 0.4405919909477234 - trainLoss: 0.44024237990379333\n",
      "cnt: 0 - valLoss: 0.4405910074710846 - trainLoss: 0.44023993611335754\n",
      "cnt: 0 - valLoss: 0.4405899941921234 - trainLoss: 0.44023752212524414\n",
      "cnt: 0 - valLoss: 0.440589040517807 - trainLoss: 0.4402350187301636\n",
      "cnt: 0 - valLoss: 0.44058793783187866 - trainLoss: 0.4402325451374054\n",
      "cnt: 0 - valLoss: 0.44058695435523987 - trainLoss: 0.4402300715446472\n",
      "cnt: 0 - valLoss: 0.4405859112739563 - trainLoss: 0.44022756814956665\n",
      "cnt: 0 - valLoss: 0.4405849874019623 - trainLoss: 0.44022509455680847\n",
      "cnt: 0 - valLoss: 0.44058382511138916 - trainLoss: 0.4402226209640503\n",
      "cnt: 0 - valLoss: 0.44058287143707275 - trainLoss: 0.4402201771736145\n",
      "cnt: 0 - valLoss: 0.4405818581581116 - trainLoss: 0.4402177035808563\n",
      "cnt: 0 - valLoss: 0.44058090448379517 - trainLoss: 0.44021520018577576\n",
      "cnt: 0 - valLoss: 0.4405798614025116 - trainLoss: 0.4402127265930176\n",
      "cnt: 0 - valLoss: 0.440578818321228 - trainLoss: 0.4402102530002594\n",
      "cnt: 0 - valLoss: 0.44057774543762207 - trainLoss: 0.44020774960517883\n",
      "cnt: 0 - valLoss: 0.44057679176330566 - trainLoss: 0.44020527601242065\n",
      "cnt: 0 - valLoss: 0.44057583808898926 - trainLoss: 0.44020283222198486\n",
      "cnt: 0 - valLoss: 0.4405748248100281 - trainLoss: 0.4402003586292267\n",
      "cnt: 0 - valLoss: 0.44057369232177734 - trainLoss: 0.4401978850364685\n",
      "cnt: 0 - valLoss: 0.44057267904281616 - trainLoss: 0.44019538164138794\n",
      "cnt: 0 - valLoss: 0.44057172536849976 - trainLoss: 0.44019293785095215\n",
      "cnt: 0 - valLoss: 0.4405707120895386 - trainLoss: 0.4401904344558716\n",
      "cnt: 0 - valLoss: 0.44056975841522217 - trainLoss: 0.4401879906654358\n",
      "cnt: 0 - valLoss: 0.4405686557292938 - trainLoss: 0.4401855170726776\n",
      "cnt: 0 - valLoss: 0.44056767225265503 - trainLoss: 0.4401830732822418\n",
      "cnt: 0 - valLoss: 0.44056662917137146 - trainLoss: 0.44018056988716125\n",
      "cnt: 0 - valLoss: 0.44056564569473267 - trainLoss: 0.4401780962944031\n",
      "cnt: 0 - valLoss: 0.4405646026134491 - trainLoss: 0.4401756227016449\n",
      "cnt: 0 - valLoss: 0.4405635893344879 - trainLoss: 0.4401731491088867\n",
      "cnt: 0 - valLoss: 0.4405626356601715 - trainLoss: 0.4401707053184509\n",
      "cnt: 0 - valLoss: 0.4405616223812103 - trainLoss: 0.440168172121048\n",
      "cnt: 0 - valLoss: 0.44056057929992676 - trainLoss: 0.4401656985282898\n",
      "cnt: 0 - valLoss: 0.4405595064163208 - trainLoss: 0.4401632845401764\n",
      "cnt: 0 - valLoss: 0.4405585825443268 - trainLoss: 0.44016075134277344\n",
      "cnt: 0 - valLoss: 0.4405575096607208 - trainLoss: 0.44015833735466003\n",
      "cnt: 0 - valLoss: 0.4405565559864044 - trainLoss: 0.44015583395957947\n",
      "cnt: 0 - valLoss: 0.44055554270744324 - trainLoss: 0.4401533603668213\n",
      "cnt: 0 - valLoss: 0.4405544698238373 - trainLoss: 0.4401508867740631\n",
      "cnt: 0 - valLoss: 0.4405534863471985 - trainLoss: 0.44014838337898254\n",
      "cnt: 0 - valLoss: 0.4405525326728821 - trainLoss: 0.44014590978622437\n",
      "cnt: 0 - valLoss: 0.4405514597892761 - trainLoss: 0.44014352560043335\n",
      "cnt: 0 - valLoss: 0.44055047631263733 - trainLoss: 0.4401409924030304\n",
      "cnt: 0 - valLoss: 0.4405493438243866 - trainLoss: 0.4401385486125946\n",
      "cnt: 0 - valLoss: 0.4405484199523926 - trainLoss: 0.4401360750198364\n",
      "cnt: 0 - valLoss: 0.440547376871109 - trainLoss: 0.44013357162475586\n",
      "cnt: 0 - valLoss: 0.440546452999115 - trainLoss: 0.4401310980319977\n",
      "cnt: 0 - valLoss: 0.44054532051086426 - trainLoss: 0.4401286244392395\n",
      "cnt: 0 - valLoss: 0.44054439663887024 - trainLoss: 0.4401261806488037\n",
      "cnt: 0 - valLoss: 0.44054335355758667 - trainLoss: 0.44012364745140076\n",
      "cnt: 0 - valLoss: 0.44054239988327026 - trainLoss: 0.44012120366096497\n",
      "cnt: 0 - valLoss: 0.44054147601127625 - trainLoss: 0.4401187300682068\n",
      "cnt: 0 - valLoss: 0.4405403435230255 - trainLoss: 0.4401162564754486\n",
      "cnt: 0 - valLoss: 0.4405393600463867 - trainLoss: 0.44011378288269043\n",
      "cnt: 0 - valLoss: 0.4405383765697479 - trainLoss: 0.44011133909225464\n",
      "cnt: 0 - valLoss: 0.44053739309310913 - trainLoss: 0.4401088356971741\n",
      "cnt: 0 - valLoss: 0.4405363202095032 - trainLoss: 0.4401063621044159\n",
      "cnt: 0 - valLoss: 0.44053539633750916 - trainLoss: 0.4401038885116577\n",
      "cnt: 0 - valLoss: 0.4405343532562256 - trainLoss: 0.44010138511657715\n",
      "cnt: 0 - valLoss: 0.4405333995819092 - trainLoss: 0.44009891152381897\n",
      "cnt: 0 - valLoss: 0.440532386302948 - trainLoss: 0.4400964677333832\n",
      "cnt: 0 - valLoss: 0.44053134322166443 - trainLoss: 0.4400940239429474\n",
      "cnt: 0 - valLoss: 0.44053035974502563 - trainLoss: 0.4400915503501892\n",
      "cnt: 0 - valLoss: 0.44052937626838684 - trainLoss: 0.44008907675743103\n",
      "cnt: 0 - valLoss: 0.4405284523963928 - trainLoss: 0.44008660316467285\n",
      "cnt: 0 - valLoss: 0.44052746891975403 - trainLoss: 0.4400840997695923\n",
      "cnt: 0 - valLoss: 0.4405263364315033 - trainLoss: 0.4400816261768341\n",
      "cnt: 0 - valLoss: 0.4405253529548645 - trainLoss: 0.4400791525840759\n",
      "cnt: 0 - valLoss: 0.4405243992805481 - trainLoss: 0.44007670879364014\n",
      "cnt: 0 - valLoss: 0.4405234754085541 - trainLoss: 0.44007420539855957\n",
      "cnt: 0 - valLoss: 0.44052231311798096 - trainLoss: 0.44007179141044617\n",
      "cnt: 0 - valLoss: 0.4405214190483093 - trainLoss: 0.4400692880153656\n",
      "cnt: 0 - valLoss: 0.44052043557167053 - trainLoss: 0.4400668144226074\n",
      "cnt: 0 - valLoss: 0.4405194818973541 - trainLoss: 0.44006434082984924\n",
      "cnt: 0 - valLoss: 0.44051849842071533 - trainLoss: 0.4400618374347687\n",
      "cnt: 0 - valLoss: 0.4405174255371094 - trainLoss: 0.4400594234466553\n",
      "cnt: 0 - valLoss: 0.4405164420604706 - trainLoss: 0.4400569200515747\n",
      "cnt: 0 - valLoss: 0.4405154585838318 - trainLoss: 0.4400544762611389\n",
      "cnt: 0 - valLoss: 0.440514475107193 - trainLoss: 0.44005200266838074\n",
      "cnt: 0 - valLoss: 0.4405135214328766 - trainLoss: 0.44004952907562256\n",
      "cnt: 0 - valLoss: 0.44051241874694824 - trainLoss: 0.4400470554828644\n",
      "cnt: 0 - valLoss: 0.44051146507263184 - trainLoss: 0.4400445520877838\n",
      "cnt: 0 - valLoss: 0.44051048159599304 - trainLoss: 0.44004207849502563\n",
      "cnt: 0 - valLoss: 0.44050952792167664 - trainLoss: 0.44003963470458984\n",
      "cnt: 0 - valLoss: 0.4405084252357483 - trainLoss: 0.44003719091415405\n",
      "cnt: 0 - valLoss: 0.4405074715614319 - trainLoss: 0.4400346577167511\n",
      "cnt: 0 - valLoss: 0.44050654768943787 - trainLoss: 0.4400322437286377\n",
      "cnt: 0 - valLoss: 0.4405055642127991 - trainLoss: 0.44002974033355713\n",
      "cnt: 0 - valLoss: 0.44050461053848267 - trainLoss: 0.44002726674079895\n",
      "cnt: 0 - valLoss: 0.4405035078525543 - trainLoss: 0.44002479314804077\n",
      "cnt: 0 - valLoss: 0.4405025243759155 - trainLoss: 0.440022349357605\n",
      "cnt: 0 - valLoss: 0.4405016005039215 - trainLoss: 0.4400198757648468\n",
      "cnt: 0 - valLoss: 0.4405006170272827 - trainLoss: 0.440017431974411\n",
      "cnt: 0 - valLoss: 0.4404997229576111 - trainLoss: 0.44001492857933044\n",
      "cnt: 0 - valLoss: 0.44049862027168274 - trainLoss: 0.44001251459121704\n",
      "cnt: 0 - valLoss: 0.44049766659736633 - trainLoss: 0.4400099813938141\n",
      "cnt: 0 - valLoss: 0.44049662351608276 - trainLoss: 0.4400075376033783\n",
      "cnt: 0 - valLoss: 0.44049569964408875 - trainLoss: 0.4400050640106201\n",
      "cnt: 0 - valLoss: 0.44049474596977234 - trainLoss: 0.4400026202201843\n",
      "cnt: 0 - valLoss: 0.44049373269081116 - trainLoss: 0.44000011682510376\n",
      "cnt: 0 - valLoss: 0.4404926896095276 - trainLoss: 0.43999770283699036\n",
      "cnt: 0 - valLoss: 0.44049176573753357 - trainLoss: 0.4399951696395874\n",
      "cnt: 0 - valLoss: 0.44049072265625 - trainLoss: 0.4399926960468292\n",
      "cnt: 0 - valLoss: 0.44048967957496643 - trainLoss: 0.43999025225639343\n",
      "cnt: 0 - valLoss: 0.4404887557029724 - trainLoss: 0.43998774886131287\n",
      "cnt: 0 - valLoss: 0.440487802028656 - trainLoss: 0.43998533487319946\n",
      "cnt: 0 - valLoss: 0.4404867887496948 - trainLoss: 0.4399828314781189\n",
      "cnt: 0 - valLoss: 0.4404858350753784 - trainLoss: 0.4399803578853607\n",
      "cnt: 0 - valLoss: 0.4404847323894501 - trainLoss: 0.4399779140949249\n",
      "cnt: 0 - valLoss: 0.44048377871513367 - trainLoss: 0.43997544050216675\n",
      "cnt: 0 - valLoss: 0.44048288464546204 - trainLoss: 0.4399729073047638\n",
      "cnt: 0 - valLoss: 0.44048190116882324 - trainLoss: 0.4399705231189728\n",
      "cnt: 0 - valLoss: 0.44048094749450684 - trainLoss: 0.4399680197238922\n",
      "cnt: 0 - valLoss: 0.4404798448085785 - trainLoss: 0.43996554613113403\n",
      "cnt: 0 - valLoss: 0.4404788911342621 - trainLoss: 0.43996307253837585\n",
      "cnt: 0 - valLoss: 0.4404779374599457 - trainLoss: 0.43996062874794006\n",
      "cnt: 0 - valLoss: 0.44047701358795166 - trainLoss: 0.4399581551551819\n",
      "cnt: 0 - valLoss: 0.4404758810997009 - trainLoss: 0.4399556517601013\n",
      "cnt: 0 - valLoss: 0.4404749572277069 - trainLoss: 0.4399532079696655\n",
      "cnt: 0 - valLoss: 0.4404739737510681 - trainLoss: 0.43995073437690735\n",
      "cnt: 0 - valLoss: 0.4404730200767517 - trainLoss: 0.43994826078414917\n",
      "cnt: 0 - valLoss: 0.4404720366001129 - trainLoss: 0.4399458169937134\n",
      "cnt: 0 - valLoss: 0.44047099351882935 - trainLoss: 0.4399433434009552\n",
      "cnt: 0 - valLoss: 0.44047001004219055 - trainLoss: 0.43994084000587463\n",
      "cnt: 0 - valLoss: 0.44046908617019653 - trainLoss: 0.43993839621543884\n",
      "cnt: 0 - valLoss: 0.44046804308891296 - trainLoss: 0.43993592262268066\n",
      "cnt: 0 - valLoss: 0.44046714901924133 - trainLoss: 0.4399334490299225\n",
      "cnt: 0 - valLoss: 0.4404660761356354 - trainLoss: 0.4399309754371643\n",
      "cnt: 0 - valLoss: 0.44046518206596375 - trainLoss: 0.4399285316467285\n",
      "cnt: 0 - valLoss: 0.4404641389846802 - trainLoss: 0.43992602825164795\n",
      "cnt: 0 - valLoss: 0.44046321511268616 - trainLoss: 0.43992361426353455\n",
      "cnt: 0 - valLoss: 0.4404621720314026 - trainLoss: 0.439921110868454\n",
      "cnt: 0 - valLoss: 0.4404611885547638 - trainLoss: 0.4399186372756958\n",
      "cnt: 0 - valLoss: 0.440460205078125 - trainLoss: 0.4399161636829376\n",
      "cnt: 0 - valLoss: 0.4404592514038086 - trainLoss: 0.43991366028785706\n",
      "cnt: 0 - valLoss: 0.4404583275318146 - trainLoss: 0.4399111866950989\n",
      "cnt: 0 - valLoss: 0.4404572546482086 - trainLoss: 0.4399087429046631\n",
      "cnt: 0 - valLoss: 0.44045621156692505 - trainLoss: 0.4399062991142273\n",
      "cnt: 0 - valLoss: 0.4404553174972534 - trainLoss: 0.4399038255214691\n",
      "cnt: 0 - valLoss: 0.440454363822937 - trainLoss: 0.43990135192871094\n",
      "cnt: 0 - valLoss: 0.440453439950943 - trainLoss: 0.43989884853363037\n",
      "cnt: 0 - valLoss: 0.44045230746269226 - trainLoss: 0.43989643454551697\n",
      "cnt: 0 - valLoss: 0.44045138359069824 - trainLoss: 0.4398939907550812\n",
      "cnt: 0 - valLoss: 0.4404503405094147 - trainLoss: 0.4398914873600006\n",
      "cnt: 0 - valLoss: 0.44044944643974304 - trainLoss: 0.43988901376724243\n",
      "cnt: 0 - valLoss: 0.44044849276542664 - trainLoss: 0.43988654017448425\n",
      "cnt: 0 - valLoss: 0.4404474198818207 - trainLoss: 0.4398840665817261\n",
      "cnt: 0 - valLoss: 0.44044649600982666 - trainLoss: 0.4398816227912903\n",
      "cnt: 0 - valLoss: 0.4404454529285431 - trainLoss: 0.4398791790008545\n",
      "cnt: 0 - valLoss: 0.4404445290565491 - trainLoss: 0.4398766756057739\n",
      "cnt: 0 - valLoss: 0.44044339656829834 - trainLoss: 0.43987420201301575\n",
      "cnt: 0 - valLoss: 0.4404424726963043 - trainLoss: 0.43987172842025757\n",
      "cnt: 0 - valLoss: 0.44044142961502075 - trainLoss: 0.43986931443214417\n",
      "cnt: 0 - valLoss: 0.44044047594070435 - trainLoss: 0.4398668110370636\n",
      "cnt: 0 - valLoss: 0.4404394030570984 - trainLoss: 0.4398643672466278\n",
      "cnt: 0 - valLoss: 0.4404383599758148 - trainLoss: 0.43986189365386963\n",
      "cnt: 0 - valLoss: 0.44043731689453125 - trainLoss: 0.43985939025878906\n",
      "cnt: 0 - valLoss: 0.44043633341789246 - trainLoss: 0.43985694646835327\n",
      "cnt: 0 - valLoss: 0.44043534994125366 - trainLoss: 0.43985453248023987\n",
      "cnt: 0 - valLoss: 0.44043439626693726 - trainLoss: 0.4398520290851593\n",
      "cnt: 0 - valLoss: 0.44043323397636414 - trainLoss: 0.4398495852947235\n",
      "cnt: 0 - valLoss: 0.44043225049972534 - trainLoss: 0.43984711170196533\n",
      "cnt: 0 - valLoss: 0.44043123722076416 - trainLoss: 0.43984466791152954\n",
      "cnt: 0 - valLoss: 0.44043028354644775 - trainLoss: 0.43984219431877136\n",
      "cnt: 0 - valLoss: 0.44042930006980896 - trainLoss: 0.4398397207260132\n",
      "cnt: 0 - valLoss: 0.440428227186203 - trainLoss: 0.4398373067378998\n",
      "cnt: 0 - valLoss: 0.44042715430259705 - trainLoss: 0.4398348331451416\n",
      "cnt: 0 - valLoss: 0.44042617082595825 - trainLoss: 0.4398323595523834\n",
      "cnt: 0 - valLoss: 0.44042515754699707 - trainLoss: 0.43982985615730286\n",
      "cnt: 0 - valLoss: 0.4404240846633911 - trainLoss: 0.43982747197151184\n",
      "cnt: 0 - valLoss: 0.4404231011867523 - trainLoss: 0.4398249387741089\n",
      "cnt: 0 - valLoss: 0.4404221475124359 - trainLoss: 0.4398224949836731\n",
      "cnt: 0 - valLoss: 0.44042110443115234 - trainLoss: 0.4398200213909149\n",
      "cnt: 0 - valLoss: 0.4404201805591583 - trainLoss: 0.4398175776004791\n",
      "cnt: 0 - valLoss: 0.4404190182685852 - trainLoss: 0.43981513381004333\n",
      "cnt: 0 - valLoss: 0.4404180645942688 - trainLoss: 0.43981266021728516\n",
      "cnt: 0 - valLoss: 0.44041708111763 - trainLoss: 0.43981021642684937\n",
      "cnt: 0 - valLoss: 0.4404160678386688 - trainLoss: 0.4398077726364136\n",
      "cnt: 0 - valLoss: 0.4404151141643524 - trainLoss: 0.4398052990436554\n",
      "cnt: 0 - valLoss: 0.4404140114784241 - trainLoss: 0.4398028254508972\n",
      "cnt: 0 - valLoss: 0.44041305780410767 - trainLoss: 0.4398003816604614\n",
      "cnt: 0 - valLoss: 0.4404120147228241 - trainLoss: 0.43979787826538086\n",
      "cnt: 0 - valLoss: 0.4404110312461853 - trainLoss: 0.43979546427726746\n",
      "cnt: 0 - valLoss: 0.4404101073741913 - trainLoss: 0.4397929608821869\n",
      "cnt: 0 - valLoss: 0.4404090344905853 - trainLoss: 0.4397905766963959\n",
      "cnt: 0 - valLoss: 0.44040799140930176 - trainLoss: 0.4397881031036377\n",
      "cnt: 0 - valLoss: 0.44040706753730774 - trainLoss: 0.43978559970855713\n",
      "cnt: 0 - valLoss: 0.44040602445602417 - trainLoss: 0.43978312611579895\n",
      "cnt: 0 - valLoss: 0.4404049515724182 - trainLoss: 0.43978074193000793\n",
      "cnt: 0 - valLoss: 0.4404039680957794 - trainLoss: 0.43977823853492737\n",
      "cnt: 0 - valLoss: 0.440403014421463 - trainLoss: 0.4397757649421692\n",
      "cnt: 0 - valLoss: 0.44040191173553467 - trainLoss: 0.4397733211517334\n",
      "cnt: 0 - valLoss: 0.44040095806121826 - trainLoss: 0.4397708475589752\n",
      "cnt: 0 - valLoss: 0.44039976596832275 - trainLoss: 0.4397684335708618\n",
      "cnt: 0 - valLoss: 0.44039881229400635 - trainLoss: 0.4397660195827484\n",
      "cnt: 0 - valLoss: 0.4403977692127228 - trainLoss: 0.43976351618766785\n",
      "cnt: 0 - valLoss: 0.4403967261314392 - trainLoss: 0.43976107239723206\n",
      "cnt: 0 - valLoss: 0.4403957426548004 - trainLoss: 0.43975862860679626\n",
      "cnt: 0 - valLoss: 0.4403945803642273 - trainLoss: 0.4397561550140381\n",
      "cnt: 0 - valLoss: 0.4403935968875885 - trainLoss: 0.4397537112236023\n",
      "cnt: 0 - valLoss: 0.44039252400398254 - trainLoss: 0.4397512674331665\n",
      "cnt: 0 - valLoss: 0.44039154052734375 - trainLoss: 0.4397487938404083\n",
      "cnt: 0 - valLoss: 0.4403904378414154 - trainLoss: 0.43974635004997253\n",
      "cnt: 0 - valLoss: 0.44038939476013184 - trainLoss: 0.43974390625953674\n",
      "cnt: 0 - valLoss: 0.44038838148117065 - trainLoss: 0.43974143266677856\n",
      "cnt: 0 - valLoss: 0.4403873383998871 - trainLoss: 0.4397389888763428\n",
      "cnt: 0 - valLoss: 0.4403863251209259 - trainLoss: 0.439736545085907\n",
      "cnt: 0 - valLoss: 0.44038525223731995 - trainLoss: 0.4397341310977936\n",
      "cnt: 0 - valLoss: 0.440384179353714 - trainLoss: 0.4397316873073578\n",
      "cnt: 0 - valLoss: 0.4403831958770752 - trainLoss: 0.4397291839122772\n",
      "cnt: 0 - valLoss: 0.440382182598114 - trainLoss: 0.4397267997264862\n",
      "cnt: 0 - valLoss: 0.44038107991218567 - trainLoss: 0.439724326133728\n",
      "cnt: 0 - valLoss: 0.44038012623786926 - trainLoss: 0.43972182273864746\n",
      "cnt: 0 - valLoss: 0.4403790235519409 - trainLoss: 0.43971940875053406\n",
      "cnt: 0 - valLoss: 0.44037801027297974 - trainLoss: 0.43971699476242065\n",
      "cnt: 0 - valLoss: 0.44037699699401855 - trainLoss: 0.4397145211696625\n",
      "cnt: 0 - valLoss: 0.44037601351737976 - trainLoss: 0.4397120773792267\n",
      "cnt: 0 - valLoss: 0.4403749406337738 - trainLoss: 0.4397096037864685\n",
      "cnt: 0 - valLoss: 0.44037386775016785 - trainLoss: 0.4397071897983551\n",
      "cnt: 0 - valLoss: 0.44037288427352905 - trainLoss: 0.4397047162055969\n",
      "cnt: 0 - valLoss: 0.44037187099456787 - trainLoss: 0.43970227241516113\n",
      "cnt: 0 - valLoss: 0.4403708875179291 - trainLoss: 0.43969982862472534\n",
      "cnt: 0 - valLoss: 0.4403698444366455 - trainLoss: 0.43969735503196716\n",
      "cnt: 0 - valLoss: 0.44036874175071716 - trainLoss: 0.43969494104385376\n",
      "cnt: 0 - valLoss: 0.44036778807640076 - trainLoss: 0.4396924674510956\n",
      "cnt: 0 - valLoss: 0.4403666853904724 - trainLoss: 0.43969008326530457\n",
      "cnt: 0 - valLoss: 0.4403657019138336 - trainLoss: 0.439687579870224\n",
      "cnt: 0 - valLoss: 0.4403647184371948 - trainLoss: 0.4396851658821106\n",
      "cnt: 0 - valLoss: 0.44036367535591125 - trainLoss: 0.4396827220916748\n",
      "cnt: 0 - valLoss: 0.4403625726699829 - trainLoss: 0.439680278301239\n",
      "cnt: 0 - valLoss: 0.4403616189956665 - trainLoss: 0.4396778345108032\n",
      "cnt: 0 - valLoss: 0.4403606057167053 - trainLoss: 0.43967536091804504\n",
      "cnt: 0 - valLoss: 0.44035956263542175 - trainLoss: 0.43967294692993164\n",
      "cnt: 0 - valLoss: 0.44035860896110535 - trainLoss: 0.43967047333717346\n",
      "cnt: 0 - valLoss: 0.4403574764728546 - trainLoss: 0.4396679997444153\n",
      "cnt: 0 - valLoss: 0.4403564929962158 - trainLoss: 0.4396655559539795\n",
      "cnt: 0 - valLoss: 0.44035542011260986 - trainLoss: 0.4396631121635437\n",
      "cnt: 0 - valLoss: 0.44035446643829346 - trainLoss: 0.4396606385707855\n",
      "cnt: 0 - valLoss: 0.4403534233570099 - trainLoss: 0.4396582245826721\n",
      "cnt: 0 - valLoss: 0.44035229086875916 - trainLoss: 0.43965575098991394\n",
      "cnt: 0 - valLoss: 0.44035133719444275 - trainLoss: 0.43965333700180054\n",
      "cnt: 0 - valLoss: 0.44035035371780396 - trainLoss: 0.43965086340904236\n",
      "cnt: 0 - valLoss: 0.440349280834198 - trainLoss: 0.43964847922325134\n",
      "cnt: 0 - valLoss: 0.4403483271598816 - trainLoss: 0.4396459758281708\n",
      "cnt: 0 - valLoss: 0.4403473436832428 - trainLoss: 0.4396435618400574\n",
      "cnt: 0 - valLoss: 0.4403461813926697 - trainLoss: 0.4396411180496216\n",
      "cnt: 0 - valLoss: 0.44034525752067566 - trainLoss: 0.4396386742591858\n",
      "cnt: 0 - valLoss: 0.44034427404403687 - trainLoss: 0.4396362006664276\n",
      "cnt: 0 - valLoss: 0.4403432011604309 - trainLoss: 0.4396337568759918\n",
      "cnt: 0 - valLoss: 0.4403422474861145 - trainLoss: 0.43963131308555603\n",
      "cnt: 0 - valLoss: 0.44034114480018616 - trainLoss: 0.43962886929512024\n",
      "cnt: 0 - valLoss: 0.4403401017189026 - trainLoss: 0.43962642550468445\n",
      "cnt: 0 - valLoss: 0.4403391182422638 - trainLoss: 0.43962398171424866\n",
      "cnt: 0 - valLoss: 0.4403381645679474 - trainLoss: 0.4396215081214905\n",
      "cnt: 0 - valLoss: 0.4403371512889862 - trainLoss: 0.43961912393569946\n",
      "cnt: 0 - valLoss: 0.44033607840538025 - trainLoss: 0.4396166205406189\n",
      "cnt: 0 - valLoss: 0.4403350353240967 - trainLoss: 0.4396142363548279\n",
      "cnt: 0 - valLoss: 0.4403340518474579 - trainLoss: 0.4396117627620697\n",
      "cnt: 0 - valLoss: 0.4403330683708191 - trainLoss: 0.4396093189716339\n",
      "cnt: 0 - valLoss: 0.4403320550918579 - trainLoss: 0.4396068751811981\n",
      "cnt: 0 - valLoss: 0.4403310716152191 - trainLoss: 0.43960437178611755\n",
      "cnt: 0 - valLoss: 0.44032999873161316 - trainLoss: 0.43960198760032654\n",
      "cnt: 0 - valLoss: 0.4403289258480072 - trainLoss: 0.43959954380989075\n",
      "cnt: 0 - valLoss: 0.4403279423713684 - trainLoss: 0.43959710001945496\n",
      "cnt: 0 - valLoss: 0.440326988697052 - trainLoss: 0.4395946264266968\n",
      "cnt: 0 - valLoss: 0.4403259754180908 - trainLoss: 0.43959224224090576\n",
      "cnt: 0 - valLoss: 0.44032493233680725 - trainLoss: 0.4395897388458252\n",
      "cnt: 0 - valLoss: 0.44032391905784607 - trainLoss: 0.4395872950553894\n",
      "cnt: 0 - valLoss: 0.4403228759765625 - trainLoss: 0.439584881067276\n",
      "cnt: 0 - valLoss: 0.4403218924999237 - trainLoss: 0.4395824372768402\n",
      "cnt: 0 - valLoss: 0.4403209388256073 - trainLoss: 0.4395799934864044\n",
      "cnt: 0 - valLoss: 0.4403197765350342 - trainLoss: 0.43957754969596863\n",
      "cnt: 0 - valLoss: 0.44031885266304016 - trainLoss: 0.43957507610321045\n",
      "cnt: 0 - valLoss: 0.44031789898872375 - trainLoss: 0.43957263231277466\n",
      "cnt: 0 - valLoss: 0.4403167963027954 - trainLoss: 0.43957018852233887\n",
      "cnt: 0 - valLoss: 0.4403159022331238 - trainLoss: 0.4395677447319031\n",
      "cnt: 0 - valLoss: 0.4403148889541626 - trainLoss: 0.4395652711391449\n",
      "cnt: 0 - valLoss: 0.44031378626823425 - trainLoss: 0.4395628571510315\n",
      "cnt: 0 - valLoss: 0.44031280279159546 - trainLoss: 0.4395603835582733\n",
      "cnt: 0 - valLoss: 0.4403117895126343 - trainLoss: 0.4395579397678375\n",
      "cnt: 0 - valLoss: 0.4403107762336731 - trainLoss: 0.43955549597740173\n",
      "cnt: 0 - valLoss: 0.4403098523616791 - trainLoss: 0.43955302238464355\n",
      "cnt: 0 - valLoss: 0.44030869007110596 - trainLoss: 0.43955063819885254\n",
      "cnt: 0 - valLoss: 0.44030773639678955 - trainLoss: 0.43954819440841675\n",
      "cnt: 0 - valLoss: 0.44030675292015076 - trainLoss: 0.43954575061798096\n",
      "cnt: 0 - valLoss: 0.4403057396411896 - trainLoss: 0.4395432770252228\n",
      "cnt: 0 - valLoss: 0.44030478596687317 - trainLoss: 0.4395408630371094\n",
      "cnt: 0 - valLoss: 0.4403037130832672 - trainLoss: 0.4395383894443512\n",
      "cnt: 0 - valLoss: 0.44030269980430603 - trainLoss: 0.4395359754562378\n",
      "cnt: 0 - valLoss: 0.4403017461299896 - trainLoss: 0.4395335018634796\n",
      "cnt: 0 - valLoss: 0.44030076265335083 - trainLoss: 0.4395310580730438\n",
      "cnt: 0 - valLoss: 0.44029971957206726 - trainLoss: 0.43952861428260803\n",
      "cnt: 0 - valLoss: 0.44029873609542847 - trainLoss: 0.43952620029449463\n",
      "cnt: 0 - valLoss: 0.4402977228164673 - trainLoss: 0.43952375650405884\n",
      "cnt: 0 - valLoss: 0.44029664993286133 - trainLoss: 0.43952131271362305\n",
      "cnt: 0 - valLoss: 0.4402956962585449 - trainLoss: 0.43951883912086487\n",
      "cnt: 0 - valLoss: 0.44029471278190613 - trainLoss: 0.43951642513275146\n",
      "cnt: 0 - valLoss: 0.44029372930526733 - trainLoss: 0.4395139813423157\n",
      "cnt: 0 - valLoss: 0.440292626619339 - trainLoss: 0.4395115077495575\n",
      "cnt: 0 - valLoss: 0.44029170274734497 - trainLoss: 0.4395091235637665\n",
      "cnt: 0 - valLoss: 0.440290629863739 - trainLoss: 0.4395066201686859\n",
      "cnt: 0 - valLoss: 0.4402896463871002 - trainLoss: 0.4395042359828949\n",
      "cnt: 0 - valLoss: 0.4402886629104614 - trainLoss: 0.4395017623901367\n",
      "cnt: 0 - valLoss: 0.44028767943382263 - trainLoss: 0.4394993185997009\n",
      "cnt: 0 - valLoss: 0.4402866065502167 - trainLoss: 0.43949687480926514\n",
      "cnt: 0 - valLoss: 0.4402855336666107 - trainLoss: 0.43949443101882935\n",
      "cnt: 0 - valLoss: 0.4402846097946167 - trainLoss: 0.43949198722839355\n",
      "cnt: 0 - valLoss: 0.4402836263179779 - trainLoss: 0.43948954343795776\n",
      "cnt: 0 - valLoss: 0.44028255343437195 - trainLoss: 0.439487099647522\n",
      "cnt: 0 - valLoss: 0.44028156995773315 - trainLoss: 0.4394846558570862\n",
      "cnt: 0 - valLoss: 0.4402805268764496 - trainLoss: 0.4394822418689728\n",
      "cnt: 0 - valLoss: 0.44027942419052124 - trainLoss: 0.439479798078537\n",
      "cnt: 0 - valLoss: 0.44027847051620483 - trainLoss: 0.4394773542881012\n",
      "cnt: 0 - valLoss: 0.4402775168418884 - trainLoss: 0.439474880695343\n",
      "cnt: 0 - valLoss: 0.44027644395828247 - trainLoss: 0.4394724667072296\n",
      "cnt: 0 - valLoss: 0.4402754604816437 - trainLoss: 0.43946999311447144\n",
      "cnt: 0 - valLoss: 0.44027453660964966 - trainLoss: 0.43946757912635803\n",
      "cnt: 0 - valLoss: 0.44027331471443176 - trainLoss: 0.43946510553359985\n",
      "cnt: 0 - valLoss: 0.44027239084243774 - trainLoss: 0.43946272134780884\n",
      "cnt: 0 - valLoss: 0.44027143716812134 - trainLoss: 0.43946021795272827\n",
      "cnt: 0 - valLoss: 0.4402703642845154 - trainLoss: 0.43945783376693726\n",
      "cnt: 0 - valLoss: 0.44026944041252136 - trainLoss: 0.4394553601741791\n",
      "cnt: 0 - valLoss: 0.4402684271335602 - trainLoss: 0.4394529163837433\n",
      "cnt: 0 - valLoss: 0.44026732444763184 - trainLoss: 0.4394504725933075\n",
      "cnt: 0 - valLoss: 0.44026637077331543 - trainLoss: 0.4394480884075165\n",
      "cnt: 0 - valLoss: 0.44026538729667664 - trainLoss: 0.4394455850124359\n",
      "cnt: 0 - valLoss: 0.4402643144130707 - trainLoss: 0.4394431412220001\n",
      "cnt: 0 - valLoss: 0.44026339054107666 - trainLoss: 0.4394407272338867\n",
      "cnt: 0 - valLoss: 0.44026243686676025 - trainLoss: 0.4394383132457733\n",
      "cnt: 0 - valLoss: 0.4402613639831543 - trainLoss: 0.4394358694553375\n",
      "cnt: 0 - valLoss: 0.44026029109954834 - trainLoss: 0.43943339586257935\n",
      "cnt: 0 - valLoss: 0.4402593672275543 - trainLoss: 0.43943095207214355\n",
      "cnt: 0 - valLoss: 0.44025832414627075 - trainLoss: 0.43942856788635254\n",
      "cnt: 0 - valLoss: 0.44025734066963196 - trainLoss: 0.439426064491272\n",
      "cnt: 0 - valLoss: 0.44025635719299316 - trainLoss: 0.4394236207008362\n",
      "cnt: 0 - valLoss: 0.4402553141117096 - trainLoss: 0.4394212067127228\n",
      "cnt: 0 - valLoss: 0.44025424122810364 - trainLoss: 0.439418762922287\n",
      "cnt: 0 - valLoss: 0.4402533173561096 - trainLoss: 0.4394163191318512\n",
      "cnt: 0 - valLoss: 0.44025227427482605 - trainLoss: 0.4394138753414154\n",
      "cnt: 0 - valLoss: 0.44025129079818726 - trainLoss: 0.4394114315509796\n",
      "cnt: 0 - valLoss: 0.44025036692619324 - trainLoss: 0.4394090473651886\n",
      "cnt: 0 - valLoss: 0.44024932384490967 - trainLoss: 0.4394066035747528\n",
      "cnt: 0 - valLoss: 0.4402482211589813 - trainLoss: 0.439404159784317\n",
      "cnt: 0 - valLoss: 0.4402472972869873 - trainLoss: 0.43940168619155884\n",
      "cnt: 0 - valLoss: 0.44024622440338135 - trainLoss: 0.43939927220344543\n",
      "cnt: 0 - valLoss: 0.44024530053138733 - trainLoss: 0.43939679861068726\n",
      "cnt: 0 - valLoss: 0.44024431705474854 - trainLoss: 0.43939441442489624\n",
      "cnt: 0 - valLoss: 0.44024327397346497 - trainLoss: 0.4393919110298157\n",
      "cnt: 0 - valLoss: 0.44024235010147095 - trainLoss: 0.4393894672393799\n",
      "cnt: 0 - valLoss: 0.440241277217865 - trainLoss: 0.43938708305358887\n",
      "cnt: 0 - valLoss: 0.44024020433425903 - trainLoss: 0.4393846392631531\n",
      "cnt: 0 - valLoss: 0.44023922085762024 - trainLoss: 0.4393821656703949\n",
      "cnt: 0 - valLoss: 0.44023826718330383 - trainLoss: 0.4393797516822815\n",
      "cnt: 0 - valLoss: 0.4402371644973755 - trainLoss: 0.4393773376941681\n",
      "cnt: 0 - valLoss: 0.4402362108230591 - trainLoss: 0.4393748939037323\n",
      "cnt: 0 - valLoss: 0.4402352273464203 - trainLoss: 0.43937239050865173\n",
      "cnt: 0 - valLoss: 0.4402341842651367 - trainLoss: 0.4393700063228607\n",
      "cnt: 0 - valLoss: 0.44023311138153076 - trainLoss: 0.4393675625324249\n",
      "cnt: 0 - valLoss: 0.44023212790489197 - trainLoss: 0.43936511874198914\n",
      "cnt: 0 - valLoss: 0.440231055021286 - trainLoss: 0.43936267495155334\n",
      "cnt: 0 - valLoss: 0.440230131149292 - trainLoss: 0.43936023116111755\n",
      "cnt: 0 - valLoss: 0.4402290880680084 - trainLoss: 0.43935781717300415\n",
      "cnt: 0 - valLoss: 0.44022807478904724 - trainLoss: 0.43935537338256836\n",
      "cnt: 0 - valLoss: 0.44022712111473083 - trainLoss: 0.43935292959213257\n",
      "cnt: 0 - valLoss: 0.44022613763809204 - trainLoss: 0.4393504858016968\n",
      "cnt: 0 - valLoss: 0.4402249753475189 - trainLoss: 0.439348042011261\n",
      "cnt: 0 - valLoss: 0.4402239918708801 - trainLoss: 0.4393455982208252\n",
      "cnt: 0 - valLoss: 0.4402230381965637 - trainLoss: 0.4393431544303894\n",
      "cnt: 0 - valLoss: 0.44022202491760254 - trainLoss: 0.439340740442276\n",
      "cnt: 0 - valLoss: 0.44022104144096375 - trainLoss: 0.4393383264541626\n",
      "cnt: 0 - valLoss: 0.44022005796432495 - trainLoss: 0.4393358528614044\n",
      "cnt: 0 - valLoss: 0.440218985080719 - trainLoss: 0.439333438873291\n",
      "cnt: 0 - valLoss: 0.4402180314064026 - trainLoss: 0.43933096528053284\n",
      "cnt: 0 - valLoss: 0.44021695852279663 - trainLoss: 0.4393285810947418\n",
      "cnt: 0 - valLoss: 0.4402158856391907 - trainLoss: 0.43932607769966125\n",
      "cnt: 0 - valLoss: 0.44021496176719666 - trainLoss: 0.43932369351387024\n",
      "cnt: 0 - valLoss: 0.4402139186859131 - trainLoss: 0.43932124972343445\n",
      "cnt: 0 - valLoss: 0.4402129352092743 - trainLoss: 0.43931880593299866\n",
      "cnt: 0 - valLoss: 0.4402119517326355 - trainLoss: 0.43931636214256287\n",
      "cnt: 0 - valLoss: 0.4402109682559967 - trainLoss: 0.43931394815444946\n",
      "cnt: 0 - valLoss: 0.44020992517471313 - trainLoss: 0.43931150436401367\n",
      "cnt: 0 - valLoss: 0.4402088522911072 - trainLoss: 0.4393090605735779\n",
      "cnt: 0 - valLoss: 0.44020792841911316 - trainLoss: 0.4393066167831421\n",
      "cnt: 0 - valLoss: 0.4402067959308624 - trainLoss: 0.4393041729927063\n",
      "cnt: 0 - valLoss: 0.4402058720588684 - trainLoss: 0.4393017888069153\n",
      "cnt: 0 - valLoss: 0.440204918384552 - trainLoss: 0.4392992854118347\n",
      "cnt: 0 - valLoss: 0.44020384550094604 - trainLoss: 0.4392968416213989\n",
      "cnt: 0 - valLoss: 0.44020289182662964 - trainLoss: 0.4392944574356079\n",
      "cnt: 0 - valLoss: 0.44020190834999084 - trainLoss: 0.4392920136451721\n",
      "cnt: 0 - valLoss: 0.44020089507102966 - trainLoss: 0.43928956985473633\n",
      "cnt: 0 - valLoss: 0.4401998221874237 - trainLoss: 0.43928712606430054\n",
      "cnt: 0 - valLoss: 0.4401988387107849 - trainLoss: 0.43928465247154236\n",
      "cnt: 0 - valLoss: 0.44019779562950134 - trainLoss: 0.43928226828575134\n",
      "cnt: 0 - valLoss: 0.44019681215286255 - trainLoss: 0.4392797648906708\n",
      "cnt: 0 - valLoss: 0.44019588828086853 - trainLoss: 0.43927738070487976\n",
      "cnt: 0 - valLoss: 0.4401948153972626 - trainLoss: 0.43927493691444397\n",
      "cnt: 0 - valLoss: 0.44019386172294617 - trainLoss: 0.4392724931240082\n",
      "cnt: 0 - valLoss: 0.44019290804862976 - trainLoss: 0.4392700493335724\n",
      "cnt: 0 - valLoss: 0.4401918053627014 - trainLoss: 0.43926766514778137\n",
      "cnt: 0 - valLoss: 0.4401908218860626 - trainLoss: 0.4392651915550232\n",
      "cnt: 0 - valLoss: 0.4401898682117462 - trainLoss: 0.4392627477645874\n",
      "cnt: 0 - valLoss: 0.44018879532814026 - trainLoss: 0.439260333776474\n",
      "cnt: 0 - valLoss: 0.44018787145614624 - trainLoss: 0.4392579197883606\n",
      "cnt: 0 - valLoss: 0.44018691778182983 - trainLoss: 0.4392554759979248\n",
      "cnt: 0 - valLoss: 0.44018590450286865 - trainLoss: 0.439253032207489\n",
      "cnt: 0 - valLoss: 0.44018495082855225 - trainLoss: 0.4392505884170532\n",
      "cnt: 0 - valLoss: 0.4401838779449463 - trainLoss: 0.4392482042312622\n",
      "cnt: 0 - valLoss: 0.44018280506134033 - trainLoss: 0.4392457604408264\n",
      "cnt: 0 - valLoss: 0.4401818513870239 - trainLoss: 0.43924325704574585\n",
      "cnt: 0 - valLoss: 0.4401809275150299 - trainLoss: 0.43924087285995483\n",
      "cnt: 0 - valLoss: 0.44017988443374634 - trainLoss: 0.43923842906951904\n",
      "cnt: 0 - valLoss: 0.4401789605617523 - trainLoss: 0.43923598527908325\n",
      "cnt: 0 - valLoss: 0.44017794728279114 - trainLoss: 0.43923354148864746\n",
      "cnt: 0 - valLoss: 0.44017693400382996 - trainLoss: 0.43923115730285645\n",
      "cnt: 0 - valLoss: 0.440175861120224 - trainLoss: 0.43922868371009827\n",
      "cnt: 0 - valLoss: 0.4401749074459076 - trainLoss: 0.43922626972198486\n",
      "cnt: 0 - valLoss: 0.4401738941669464 - trainLoss: 0.4392238259315491\n",
      "cnt: 0 - valLoss: 0.44017294049263 - trainLoss: 0.4392213821411133\n",
      "cnt: 0 - valLoss: 0.4401719272136688 - trainLoss: 0.43921899795532227\n",
      "cnt: 0 - valLoss: 0.4401709735393524 - trainLoss: 0.4392165243625641\n",
      "cnt: 0 - valLoss: 0.44016993045806885 - trainLoss: 0.4392141103744507\n",
      "cnt: 0 - valLoss: 0.44016900658607483 - trainLoss: 0.4392116367816925\n",
      "cnt: 0 - valLoss: 0.44016793370246887 - trainLoss: 0.4392091929912567\n",
      "cnt: 0 - valLoss: 0.4401669502258301 - trainLoss: 0.4392067492008209\n",
      "cnt: 0 - valLoss: 0.4401659071445465 - trainLoss: 0.4392043650150299\n",
      "cnt: 0 - valLoss: 0.4401649832725525 - trainLoss: 0.4392019212245941\n",
      "cnt: 0 - valLoss: 0.4401639401912689 - trainLoss: 0.4391994774341583\n",
      "cnt: 0 - valLoss: 0.4401630461215973 - trainLoss: 0.4391970932483673\n",
      "cnt: 0 - valLoss: 0.4401620626449585 - trainLoss: 0.43919458985328674\n",
      "cnt: 0 - valLoss: 0.4401610195636749 - trainLoss: 0.4391922056674957\n",
      "cnt: 0 - valLoss: 0.4401600956916809 - trainLoss: 0.43918973207473755\n",
      "cnt: 0 - valLoss: 0.44015902280807495 - trainLoss: 0.43918731808662415\n",
      "cnt: 0 - valLoss: 0.440157949924469 - trainLoss: 0.43918487429618835\n",
      "cnt: 0 - valLoss: 0.4401569962501526 - trainLoss: 0.43918246030807495\n",
      "cnt: 0 - valLoss: 0.44015607237815857 - trainLoss: 0.43918004631996155\n",
      "cnt: 0 - valLoss: 0.440155029296875 - trainLoss: 0.43917757272720337\n",
      "cnt: 0 - valLoss: 0.4401540756225586 - trainLoss: 0.43917515873908997\n",
      "cnt: 0 - valLoss: 0.4401531517505646 - trainLoss: 0.43917274475097656\n",
      "cnt: 0 - valLoss: 0.440152108669281 - trainLoss: 0.43917030096054077\n",
      "cnt: 0 - valLoss: 0.44015100598335266 - trainLoss: 0.439167857170105\n",
      "cnt: 0 - valLoss: 0.44015005230903625 - trainLoss: 0.4391654133796692\n",
      "cnt: 0 - valLoss: 0.44014906883239746 - trainLoss: 0.4391630291938782\n",
      "cnt: 0 - valLoss: 0.44014808535575867 - trainLoss: 0.4391605257987976\n",
      "cnt: 0 - valLoss: 0.44014713168144226 - trainLoss: 0.4391581416130066\n",
      "cnt: 0 - valLoss: 0.4401461184024811 - trainLoss: 0.4391556680202484\n",
      "cnt: 0 - valLoss: 0.4401451647281647 - trainLoss: 0.439153254032135\n",
      "cnt: 0 - valLoss: 0.4401441812515259 - trainLoss: 0.43915078043937683\n",
      "cnt: 0 - valLoss: 0.4401431679725647 - trainLoss: 0.4391483664512634\n",
      "cnt: 0 - valLoss: 0.44014203548431396 - trainLoss: 0.43914592266082764\n",
      "cnt: 0 - valLoss: 0.44014111161231995 - trainLoss: 0.43914350867271423\n",
      "cnt: 0 - valLoss: 0.44014012813568115 - trainLoss: 0.43914109468460083\n",
      "cnt: 0 - valLoss: 0.4401390552520752 - trainLoss: 0.43913865089416504\n",
      "cnt: 0 - valLoss: 0.440138041973114 - trainLoss: 0.43913620710372925\n",
      "cnt: 0 - valLoss: 0.44013702869415283 - trainLoss: 0.43913382291793823\n",
      "cnt: 0 - valLoss: 0.4401359558105469 - trainLoss: 0.43913137912750244\n",
      "cnt: 0 - valLoss: 0.4401348829269409 - trainLoss: 0.43912893533706665\n",
      "cnt: 0 - valLoss: 0.4401339292526245 - trainLoss: 0.43912655115127563\n",
      "cnt: 0 - valLoss: 0.44013282656669617 - trainLoss: 0.43912410736083984\n",
      "cnt: 0 - valLoss: 0.44013187289237976 - trainLoss: 0.43912166357040405\n",
      "cnt: 0 - valLoss: 0.4401308596134186 - trainLoss: 0.43911921977996826\n",
      "cnt: 0 - valLoss: 0.4401297867298126 - trainLoss: 0.43911677598953247\n",
      "cnt: 0 - valLoss: 0.44012880325317383 - trainLoss: 0.43911439180374146\n",
      "cnt: 0 - valLoss: 0.4401278495788574 - trainLoss: 0.43911200761795044\n",
      "cnt: 0 - valLoss: 0.4401266574859619 - trainLoss: 0.4391095042228699\n",
      "cnt: 0 - valLoss: 0.4401256740093231 - trainLoss: 0.4391070604324341\n",
      "cnt: 0 - valLoss: 0.4401246905326843 - trainLoss: 0.43910467624664307\n",
      "cnt: 0 - valLoss: 0.44012364745140076 - trainLoss: 0.4391022324562073\n",
      "cnt: 0 - valLoss: 0.44012266397476196 - trainLoss: 0.43909984827041626\n",
      "cnt: 0 - valLoss: 0.44012168049812317 - trainLoss: 0.43909740447998047\n",
      "cnt: 0 - valLoss: 0.4401206076145172 - trainLoss: 0.4390949606895447\n",
      "cnt: 0 - valLoss: 0.4401196241378784 - trainLoss: 0.43909257650375366\n",
      "cnt: 0 - valLoss: 0.4401185214519501 - trainLoss: 0.4390900731086731\n",
      "cnt: 0 - valLoss: 0.4401175379753113 - trainLoss: 0.4390876889228821\n",
      "cnt: 0 - valLoss: 0.4401164650917053 - trainLoss: 0.4390852451324463\n",
      "cnt: 0 - valLoss: 0.44011548161506653 - trainLoss: 0.4390828013420105\n",
      "cnt: 0 - valLoss: 0.44011446833610535 - trainLoss: 0.4390804171562195\n",
      "cnt: 0 - valLoss: 0.44011345505714417 - trainLoss: 0.4390779733657837\n",
      "cnt: 0 - valLoss: 0.44011247158050537 - trainLoss: 0.4390755295753479\n",
      "cnt: 0 - valLoss: 0.4401114881038666 - trainLoss: 0.4390730857849121\n",
      "cnt: 0 - valLoss: 0.440110445022583 - trainLoss: 0.4390707015991211\n",
      "cnt: 0 - valLoss: 0.44010934233665466 - trainLoss: 0.4390682578086853\n",
      "cnt: 0 - valLoss: 0.44010838866233826 - trainLoss: 0.4390658438205719\n",
      "cnt: 0 - valLoss: 0.4401073753833771 - trainLoss: 0.4390633702278137\n",
      "cnt: 0 - valLoss: 0.4401063323020935 - trainLoss: 0.4390609860420227\n",
      "cnt: 0 - valLoss: 0.4401054084300995 - trainLoss: 0.4390585422515869\n",
      "cnt: 0 - valLoss: 0.44010433554649353 - trainLoss: 0.4390561878681183\n",
      "cnt: 0 - valLoss: 0.4401033818721771 - trainLoss: 0.43905365467071533\n",
      "cnt: 0 - valLoss: 0.44010239839553833 - trainLoss: 0.4390512704849243\n",
      "cnt: 0 - valLoss: 0.4401012361049652 - trainLoss: 0.4390488266944885\n",
      "cnt: 0 - valLoss: 0.4401002526283264 - trainLoss: 0.4390464127063751\n",
      "cnt: 0 - valLoss: 0.44009920954704285 - trainLoss: 0.4390440285205841\n",
      "cnt: 0 - valLoss: 0.44009828567504883 - trainLoss: 0.4390415549278259\n",
      "cnt: 0 - valLoss: 0.44009721279144287 - trainLoss: 0.4390391409397125\n",
      "cnt: 0 - valLoss: 0.44009631872177124 - trainLoss: 0.43903666734695435\n",
      "cnt: 0 - valLoss: 0.44009527564048767 - trainLoss: 0.43903425335884094\n",
      "cnt: 0 - valLoss: 0.4400942623615265 - trainLoss: 0.4390318691730499\n",
      "cnt: 0 - valLoss: 0.44009318947792053 - trainLoss: 0.43902942538261414\n",
      "cnt: 0 - valLoss: 0.44009220600128174 - trainLoss: 0.43902698159217834\n",
      "cnt: 0 - valLoss: 0.44009116291999817 - trainLoss: 0.43902459740638733\n",
      "cnt: 0 - valLoss: 0.4400901794433594 - trainLoss: 0.43902215361595154\n",
      "cnt: 0 - valLoss: 0.4400891959667206 - trainLoss: 0.43901970982551575\n",
      "cnt: 0 - valLoss: 0.440088152885437 - trainLoss: 0.43901726603507996\n",
      "cnt: 0 - valLoss: 0.4400871694087982 - trainLoss: 0.43901488184928894\n",
      "cnt: 0 - valLoss: 0.4400861859321594 - trainLoss: 0.43901243805885315\n",
      "cnt: 0 - valLoss: 0.4400850832462311 - trainLoss: 0.43901005387306213\n",
      "cnt: 0 - valLoss: 0.4400840401649475 - trainLoss: 0.43900763988494873\n",
      "cnt: 0 - valLoss: 0.4400830566883087 - trainLoss: 0.43900516629219055\n",
      "cnt: 0 - valLoss: 0.4400821626186371 - trainLoss: 0.43900272250175476\n",
      "cnt: 0 - valLoss: 0.44008108973503113 - trainLoss: 0.43900036811828613\n",
      "cnt: 0 - valLoss: 0.4400801360607147 - trainLoss: 0.43899789452552795\n",
      "cnt: 0 - valLoss: 0.4400791525840759 - trainLoss: 0.43899548053741455\n",
      "cnt: 0 - valLoss: 0.44007813930511475 - trainLoss: 0.43899303674697876\n",
      "cnt: 0 - valLoss: 0.440077006816864 - trainLoss: 0.43899062275886536\n",
      "cnt: 0 - valLoss: 0.44007608294487 - trainLoss: 0.43898820877075195\n",
      "cnt: 0 - valLoss: 0.4400750398635864 - trainLoss: 0.43898576498031616\n",
      "cnt: 0 - valLoss: 0.4400741159915924 - trainLoss: 0.43898332118988037\n",
      "cnt: 0 - valLoss: 0.4400731325149536 - trainLoss: 0.43898093700408936\n",
      "cnt: 0 - valLoss: 0.44007205963134766 - trainLoss: 0.43897849321365356\n",
      "cnt: 0 - valLoss: 0.44007107615470886 - trainLoss: 0.4389760494232178\n",
      "cnt: 0 - valLoss: 0.44007012248039246 - trainLoss: 0.43897366523742676\n",
      "cnt: 0 - valLoss: 0.4400690495967865 - trainLoss: 0.43897122144699097\n",
      "cnt: 0 - valLoss: 0.44006794691085815 - trainLoss: 0.4389687776565552\n",
      "cnt: 0 - valLoss: 0.44006699323654175 - trainLoss: 0.4389663338661194\n",
      "cnt: 0 - valLoss: 0.44006603956222534 - trainLoss: 0.43896394968032837\n",
      "cnt: 0 - valLoss: 0.44006502628326416 - trainLoss: 0.4389615058898926\n",
      "cnt: 0 - valLoss: 0.44006404280662537 - trainLoss: 0.43895912170410156\n",
      "cnt: 0 - valLoss: 0.4400630593299866 - trainLoss: 0.43895667791366577\n",
      "cnt: 0 - valLoss: 0.440062016248703 - trainLoss: 0.43895426392555237\n",
      "cnt: 0 - valLoss: 0.4400610625743866 - trainLoss: 0.4389518201351166\n",
      "cnt: 0 - valLoss: 0.440060019493103 - trainLoss: 0.4389493465423584\n",
      "cnt: 0 - valLoss: 0.44005894660949707 - trainLoss: 0.4389469623565674\n",
      "cnt: 0 - valLoss: 0.44005799293518066 - trainLoss: 0.438944548368454\n",
      "cnt: 0 - valLoss: 0.44005700945854187 - trainLoss: 0.43894216418266296\n",
      "cnt: 0 - valLoss: 0.4400560259819031 - trainLoss: 0.4389396905899048\n",
      "cnt: 0 - valLoss: 0.4400549829006195 - trainLoss: 0.4389372766017914\n",
      "cnt: 0 - valLoss: 0.4400540590286255 - trainLoss: 0.4389348328113556\n",
      "cnt: 0 - valLoss: 0.4400530755519867 - trainLoss: 0.4389323890209198\n",
      "cnt: 0 - valLoss: 0.4400519132614136 - trainLoss: 0.4389300048351288\n",
      "cnt: 0 - valLoss: 0.44005095958709717 - trainLoss: 0.43892762064933777\n",
      "cnt: 0 - valLoss: 0.4400499761104584 - trainLoss: 0.438925176858902\n",
      "cnt: 0 - valLoss: 0.4400489330291748 - trainLoss: 0.4389226734638214\n",
      "cnt: 0 - valLoss: 0.440047949552536 - trainLoss: 0.4389203190803528\n",
      "cnt: 0 - valLoss: 0.440047025680542 - trainLoss: 0.4389178454875946\n",
      "cnt: 0 - valLoss: 0.4400459825992584 - trainLoss: 0.4389154613018036\n",
      "cnt: 0 - valLoss: 0.4400450587272644 - trainLoss: 0.4389130473136902\n",
      "cnt: 0 - valLoss: 0.44004392623901367 - trainLoss: 0.4389106035232544\n",
      "cnt: 0 - valLoss: 0.44004300236701965 - trainLoss: 0.4389081597328186\n",
      "cnt: 0 - valLoss: 0.4400419294834137 - trainLoss: 0.4389057755470276\n",
      "cnt: 0 - valLoss: 0.4400409460067749 - trainLoss: 0.4389033317565918\n",
      "cnt: 0 - valLoss: 0.4400399923324585 - trainLoss: 0.4389009475708008\n",
      "cnt: 0 - valLoss: 0.44003891944885254 - trainLoss: 0.438898503780365\n",
      "cnt: 0 - valLoss: 0.44003793597221375 - trainLoss: 0.4388960301876068\n",
      "cnt: 0 - valLoss: 0.44003698229789734 - trainLoss: 0.4388936161994934\n",
      "cnt: 0 - valLoss: 0.4400359094142914 - trainLoss: 0.4388912320137024\n",
      "cnt: 0 - valLoss: 0.4400349259376526 - trainLoss: 0.4388887286186218\n",
      "cnt: 0 - valLoss: 0.4400339126586914 - trainLoss: 0.4388863444328308\n",
      "cnt: 0 - valLoss: 0.44003283977508545 - trainLoss: 0.4388839602470398\n",
      "cnt: 0 - valLoss: 0.44003185629844666 - trainLoss: 0.4388815462589264\n",
      "cnt: 0 - valLoss: 0.44003087282180786 - trainLoss: 0.4388790726661682\n",
      "cnt: 0 - valLoss: 0.4400298595428467 - trainLoss: 0.4388766586780548\n",
      "cnt: 0 - valLoss: 0.4400288462638855 - trainLoss: 0.4388742446899414\n",
      "cnt: 0 - valLoss: 0.4400278925895691 - trainLoss: 0.4388718903064728\n",
      "cnt: 0 - valLoss: 0.4400269091129303 - trainLoss: 0.4388693869113922\n",
      "cnt: 0 - valLoss: 0.4400258958339691 - trainLoss: 0.4388670027256012\n",
      "cnt: 0 - valLoss: 0.4400249123573303 - trainLoss: 0.4388645589351654\n",
      "cnt: 0 - valLoss: 0.4400239586830139 - trainLoss: 0.4388621747493744\n",
      "cnt: 0 - valLoss: 0.4400227665901184 - trainLoss: 0.4388597309589386\n",
      "cnt: 0 - valLoss: 0.440021812915802 - trainLoss: 0.4388573169708252\n",
      "cnt: 0 - valLoss: 0.4400208294391632 - trainLoss: 0.4388548731803894\n",
      "cnt: 0 - valLoss: 0.4400198459625244 - trainLoss: 0.4388524889945984\n",
      "cnt: 0 - valLoss: 0.44001880288124084 - trainLoss: 0.4388500452041626\n",
      "cnt: 0 - valLoss: 0.44001781940460205 - trainLoss: 0.4388476610183716\n",
      "cnt: 0 - valLoss: 0.44001683592796326 - trainLoss: 0.4388452172279358\n",
      "cnt: 0 - valLoss: 0.4400158226490021 - trainLoss: 0.4388427734375\n",
      "cnt: 0 - valLoss: 0.44001486897468567 - trainLoss: 0.438840389251709\n",
      "cnt: 0 - valLoss: 0.44001391530036926 - trainLoss: 0.4388379454612732\n",
      "cnt: 0 - valLoss: 0.4400128424167633 - trainLoss: 0.4388355016708374\n",
      "cnt: 0 - valLoss: 0.44001179933547974 - trainLoss: 0.438833087682724\n",
      "cnt: 0 - valLoss: 0.44001084566116333 - trainLoss: 0.438830703496933\n",
      "cnt: 0 - valLoss: 0.4400097727775574 - trainLoss: 0.4388282597064972\n",
      "cnt: 0 - valLoss: 0.4400087893009186 - trainLoss: 0.4388258159160614\n",
      "cnt: 0 - valLoss: 0.440007746219635 - trainLoss: 0.4388234317302704\n",
      "cnt: 0 - valLoss: 0.440006822347641 - trainLoss: 0.438821017742157\n",
      "cnt: 0 - valLoss: 0.4400057792663574 - trainLoss: 0.4388185441493988\n",
      "cnt: 0 - valLoss: 0.44000476598739624 - trainLoss: 0.4388161599636078\n",
      "cnt: 0 - valLoss: 0.44000378251075745 - trainLoss: 0.4388137459754944\n",
      "cnt: 0 - valLoss: 0.4400027096271515 - trainLoss: 0.43881136178970337\n",
      "cnt: 0 - valLoss: 0.4400016665458679 - trainLoss: 0.4388088881969452\n",
      "cnt: 0 - valLoss: 0.4400005638599396 - trainLoss: 0.4388064742088318\n",
      "cnt: 0 - valLoss: 0.439999520778656 - trainLoss: 0.43880409002304077\n",
      "cnt: 0 - valLoss: 0.43999844789505005 - trainLoss: 0.43880167603492737\n",
      "cnt: 0 - valLoss: 0.43999746441841125 - trainLoss: 0.43879929184913635\n",
      "cnt: 0 - valLoss: 0.4399963915348053 - trainLoss: 0.4387968182563782\n",
      "cnt: 0 - valLoss: 0.4399953782558441 - trainLoss: 0.43879440426826477\n",
      "cnt: 0 - valLoss: 0.43999433517456055 - trainLoss: 0.43879202008247375\n",
      "cnt: 0 - valLoss: 0.4399932622909546 - trainLoss: 0.43878957629203796\n",
      "cnt: 0 - valLoss: 0.4399922788143158 - trainLoss: 0.43878716230392456\n",
      "cnt: 0 - valLoss: 0.4399912655353546 - trainLoss: 0.43878480792045593\n",
      "cnt: 0 - valLoss: 0.43999022245407104 - trainLoss: 0.43878233432769775\n",
      "cnt: 0 - valLoss: 0.4399891197681427 - trainLoss: 0.43877995014190674\n",
      "cnt: 0 - valLoss: 0.43998804688453674 - trainLoss: 0.43877753615379333\n",
      "cnt: 0 - valLoss: 0.4399870038032532 - trainLoss: 0.4387751519680023\n",
      "cnt: 0 - valLoss: 0.43998607993125916 - trainLoss: 0.43877270817756653\n",
      "cnt: 0 - valLoss: 0.4399849474430084 - trainLoss: 0.43877026438713074\n",
      "cnt: 0 - valLoss: 0.43998393416404724 - trainLoss: 0.4387678802013397\n",
      "cnt: 0 - valLoss: 0.43998295068740845 - trainLoss: 0.43876543641090393\n",
      "cnt: 0 - valLoss: 0.4399818778038025 - trainLoss: 0.4387630820274353\n",
      "cnt: 0 - valLoss: 0.4399808347225189 - trainLoss: 0.4387606084346771\n",
      "cnt: 0 - valLoss: 0.4399798512458801 - trainLoss: 0.4387581944465637\n",
      "cnt: 0 - valLoss: 0.43997877836227417 - trainLoss: 0.4387558102607727\n",
      "cnt: 0 - valLoss: 0.4399776756763458 - trainLoss: 0.4387533962726593\n",
      "cnt: 0 - valLoss: 0.43997663259506226 - trainLoss: 0.4387510120868683\n",
      "cnt: 0 - valLoss: 0.4399756193161011 - trainLoss: 0.4387485682964325\n",
      "cnt: 0 - valLoss: 0.4399746358394623 - trainLoss: 0.4387461245059967\n",
      "cnt: 0 - valLoss: 0.4399736523628235 - trainLoss: 0.4387437403202057\n",
      "cnt: 0 - valLoss: 0.43997251987457275 - trainLoss: 0.4387413263320923\n",
      "cnt: 0 - valLoss: 0.4399714469909668 - trainLoss: 0.43873894214630127\n",
      "cnt: 0 - valLoss: 0.43997037410736084 - trainLoss: 0.4387364685535431\n",
      "cnt: 0 - valLoss: 0.4399693012237549 - trainLoss: 0.4387340545654297\n",
      "cnt: 0 - valLoss: 0.4399682283401489 - trainLoss: 0.4387316107749939\n",
      "cnt: 0 - valLoss: 0.43996715545654297 - trainLoss: 0.4387291669845581\n",
      "cnt: 0 - valLoss: 0.43996596336364746 - trainLoss: 0.4387267529964447\n",
      "cnt: 0 - valLoss: 0.4399648904800415 - trainLoss: 0.43872424960136414\n",
      "cnt: 0 - valLoss: 0.43996381759643555 - trainLoss: 0.4387218654155731\n",
      "cnt: 0 - valLoss: 0.4399627149105072 - trainLoss: 0.43871942162513733\n",
      "cnt: 0 - valLoss: 0.4399615526199341 - trainLoss: 0.43871694803237915\n",
      "cnt: 0 - valLoss: 0.43996039032936096 - trainLoss: 0.43871450424194336\n",
      "cnt: 0 - valLoss: 0.4399592876434326 - trainLoss: 0.4387120008468628\n",
      "cnt: 0 - valLoss: 0.4399581253528595 - trainLoss: 0.4387095868587494\n",
      "cnt: 0 - valLoss: 0.43995702266693115 - trainLoss: 0.4387070834636688\n",
      "cnt: 0 - valLoss: 0.43995586037635803 - trainLoss: 0.43870460987091064\n",
      "cnt: 0 - valLoss: 0.43995460867881775 - trainLoss: 0.43870216608047485\n",
      "cnt: 0 - valLoss: 0.4399535059928894 - trainLoss: 0.43869972229003906\n",
      "cnt: 0 - valLoss: 0.43995237350463867 - trainLoss: 0.4386972486972809\n",
      "cnt: 0 - valLoss: 0.43995121121406555 - trainLoss: 0.4386947751045227\n",
      "cnt: 0 - valLoss: 0.4399500787258148 - trainLoss: 0.4386923313140869\n",
      "cnt: 0 - valLoss: 0.4399489164352417 - trainLoss: 0.4386898875236511\n",
      "cnt: 0 - valLoss: 0.4399476945400238 - trainLoss: 0.43868741393089294\n",
      "cnt: 0 - valLoss: 0.43994656205177307 - trainLoss: 0.4386849105358124\n",
      "cnt: 0 - valLoss: 0.4399453401565552 - trainLoss: 0.43868252635002136\n",
      "cnt: 0 - valLoss: 0.43994423747062683 - trainLoss: 0.4386800527572632\n",
      "cnt: 0 - valLoss: 0.4399430453777313 - trainLoss: 0.4386775493621826\n",
      "cnt: 0 - valLoss: 0.43994176387786865 - trainLoss: 0.43867507576942444\n",
      "cnt: 0 - valLoss: 0.43994060158729553 - trainLoss: 0.4386726915836334\n",
      "cnt: 0 - valLoss: 0.4399394392967224 - trainLoss: 0.43867015838623047\n",
      "cnt: 0 - valLoss: 0.4399383068084717 - trainLoss: 0.4386677145957947\n",
      "cnt: 0 - valLoss: 0.43993711471557617 - trainLoss: 0.4386652708053589\n",
      "cnt: 0 - valLoss: 0.43993595242500305 - trainLoss: 0.4386627972126007\n",
      "cnt: 0 - valLoss: 0.4399348199367523 - trainLoss: 0.4386603534221649\n",
      "cnt: 0 - valLoss: 0.4399336576461792 - trainLoss: 0.4386579096317291\n",
      "cnt: 0 - valLoss: 0.4399324953556061 - trainLoss: 0.43865543603897095\n",
      "cnt: 0 - valLoss: 0.43993130326271057 - trainLoss: 0.43865299224853516\n",
      "cnt: 0 - valLoss: 0.4399300813674927 - trainLoss: 0.438650518655777\n",
      "cnt: 0 - valLoss: 0.43992888927459717 - trainLoss: 0.4386480152606964\n",
      "cnt: 0 - valLoss: 0.4399277865886688 - trainLoss: 0.4386456310749054\n",
      "cnt: 0 - valLoss: 0.4399265646934509 - trainLoss: 0.4386431574821472\n",
      "cnt: 0 - valLoss: 0.4399254322052002 - trainLoss: 0.43864065408706665\n",
      "cnt: 0 - valLoss: 0.43992429971694946 - trainLoss: 0.43863821029663086\n",
      "cnt: 0 - valLoss: 0.43992313742637634 - trainLoss: 0.43863579630851746\n",
      "cnt: 0 - valLoss: 0.4399219751358032 - trainLoss: 0.4386332929134369\n",
      "cnt: 0 - valLoss: 0.4399208426475525 - trainLoss: 0.4386308491230011\n",
      "cnt: 0 - valLoss: 0.43991971015930176 - trainLoss: 0.4386284351348877\n",
      "cnt: 0 - valLoss: 0.4399186074733734 - trainLoss: 0.43862593173980713\n",
      "cnt: 0 - valLoss: 0.43991732597351074 - trainLoss: 0.43862348794937134\n",
      "cnt: 0 - valLoss: 0.43991619348526 - trainLoss: 0.43862101435661316\n",
      "cnt: 0 - valLoss: 0.4399150609970093 - trainLoss: 0.43861857056617737\n",
      "cnt: 0 - valLoss: 0.43991389870643616 - trainLoss: 0.4386161267757416\n",
      "cnt: 0 - valLoss: 0.43991273641586304 - trainLoss: 0.4386136531829834\n",
      "cnt: 0 - valLoss: 0.4399115741252899 - trainLoss: 0.4386112093925476\n",
      "cnt: 0 - valLoss: 0.43991050124168396 - trainLoss: 0.43860873579978943\n",
      "cnt: 0 - valLoss: 0.43990933895111084 - trainLoss: 0.43860626220703125\n",
      "cnt: 0 - valLoss: 0.4399081766605377 - trainLoss: 0.43860384821891785\n",
      "cnt: 0 - valLoss: 0.439907044172287 - trainLoss: 0.43860137462615967\n",
      "cnt: 0 - valLoss: 0.4399058222770691 - trainLoss: 0.4385989308357239\n",
      "cnt: 0 - valLoss: 0.4399046301841736 - trainLoss: 0.4385964572429657\n",
      "cnt: 0 - valLoss: 0.43990352749824524 - trainLoss: 0.4385940134525299\n",
      "cnt: 0 - valLoss: 0.4399023950099945 - trainLoss: 0.4385915696620941\n",
      "cnt: 0 - valLoss: 0.4399012327194214 - trainLoss: 0.4385891258716583\n",
      "cnt: 0 - valLoss: 0.43990010023117065 - trainLoss: 0.43858665227890015\n",
      "cnt: 0 - valLoss: 0.4398989677429199 - trainLoss: 0.43858417868614197\n",
      "cnt: 0 - valLoss: 0.4398978054523468 - trainLoss: 0.4385817348957062\n",
      "cnt: 0 - valLoss: 0.43989670276641846 - trainLoss: 0.4385792911052704\n",
      "cnt: 0 - valLoss: 0.4398954510688782 - trainLoss: 0.4385768175125122\n",
      "cnt: 0 - valLoss: 0.43989434838294983 - trainLoss: 0.43857431411743164\n",
      "cnt: 0 - valLoss: 0.4398931860923767 - trainLoss: 0.4385719299316406\n",
      "cnt: 0 - valLoss: 0.43989208340644836 - trainLoss: 0.43856945633888245\n",
      "cnt: 0 - valLoss: 0.43989095091819763 - trainLoss: 0.43856701254844666\n",
      "cnt: 0 - valLoss: 0.4398898482322693 - trainLoss: 0.4385645091533661\n",
      "cnt: 0 - valLoss: 0.43988868594169617 - trainLoss: 0.4385620653629303\n",
      "cnt: 0 - valLoss: 0.4398875832557678 - trainLoss: 0.4385595917701721\n",
      "cnt: 0 - valLoss: 0.4398864507675171 - trainLoss: 0.43855714797973633\n",
      "cnt: 0 - valLoss: 0.43988534808158875 - trainLoss: 0.43855467438697815\n",
      "cnt: 0 - valLoss: 0.43988409638404846 - trainLoss: 0.43855223059654236\n",
      "cnt: 0 - valLoss: 0.4398829936981201 - trainLoss: 0.43854978680610657\n",
      "cnt: 0 - valLoss: 0.4398818612098694 - trainLoss: 0.4385473430156708\n",
      "cnt: 0 - valLoss: 0.43988072872161865 - trainLoss: 0.4385448694229126\n",
      "cnt: 0 - valLoss: 0.4398795962333679 - trainLoss: 0.4385424256324768\n",
      "cnt: 0 - valLoss: 0.4398784935474396 - trainLoss: 0.43853995203971863\n",
      "cnt: 0 - valLoss: 0.43987736105918884 - trainLoss: 0.43853750824928284\n",
      "cnt: 0 - valLoss: 0.4398762583732605 - trainLoss: 0.43853503465652466\n",
      "cnt: 0 - valLoss: 0.43987515568733215 - trainLoss: 0.43853262066841125\n",
      "cnt: 0 - valLoss: 0.4398740231990814 - trainLoss: 0.4385301470756531\n",
      "cnt: 0 - valLoss: 0.4398728311061859 - trainLoss: 0.4385276734828949\n",
      "cnt: 0 - valLoss: 0.4398716986179352 - trainLoss: 0.4385252296924591\n",
      "cnt: 0 - valLoss: 0.43987059593200684 - trainLoss: 0.4385227859020233\n",
      "cnt: 0 - valLoss: 0.4398694932460785 - trainLoss: 0.43852031230926514\n",
      "cnt: 0 - valLoss: 0.43986836075782776 - trainLoss: 0.43851786851882935\n",
      "cnt: 0 - valLoss: 0.4398672878742218 - trainLoss: 0.43851542472839355\n",
      "cnt: 0 - valLoss: 0.43986615538597107 - trainLoss: 0.4385129511356354\n",
      "cnt: 0 - valLoss: 0.4398650825023651 - trainLoss: 0.4385105073451996\n",
      "cnt: 0 - valLoss: 0.4398639500141144 - trainLoss: 0.4385080635547638\n",
      "cnt: 0 - valLoss: 0.43986275792121887 - trainLoss: 0.4385055899620056\n",
      "cnt: 0 - valLoss: 0.4398616552352905 - trainLoss: 0.4385031461715698\n",
      "cnt: 0 - valLoss: 0.4398605227470398 - trainLoss: 0.43850070238113403\n",
      "cnt: 0 - valLoss: 0.43985942006111145 - trainLoss: 0.43849822878837585\n",
      "cnt: 0 - valLoss: 0.4398583471775055 - trainLoss: 0.43849578499794006\n",
      "cnt: 0 - valLoss: 0.43985721468925476 - trainLoss: 0.43849337100982666\n",
      "cnt: 0 - valLoss: 0.4398561120033264 - trainLoss: 0.4384908676147461\n",
      "cnt: 0 - valLoss: 0.43985503911972046 - trainLoss: 0.4384884238243103\n",
      "cnt: 0 - valLoss: 0.43985387682914734 - trainLoss: 0.4384859800338745\n",
      "cnt: 0 - valLoss: 0.43985286355018616 - trainLoss: 0.4384835362434387\n",
      "cnt: 0 - valLoss: 0.4398516118526459 - trainLoss: 0.43848109245300293\n",
      "cnt: 0 - valLoss: 0.43985050916671753 - trainLoss: 0.43847861886024475\n",
      "cnt: 0 - valLoss: 0.4398494362831116 - trainLoss: 0.4384761452674866\n",
      "cnt: 0 - valLoss: 0.43984830379486084 - trainLoss: 0.4384737014770508\n",
      "cnt: 0 - valLoss: 0.4398472309112549 - trainLoss: 0.438471257686615\n",
      "cnt: 0 - valLoss: 0.43984606862068176 - trainLoss: 0.4384688138961792\n",
      "cnt: 0 - valLoss: 0.4398449957370758 - trainLoss: 0.4384663701057434\n",
      "cnt: 0 - valLoss: 0.43984395265579224 - trainLoss: 0.43846389651298523\n",
      "cnt: 0 - valLoss: 0.4398428201675415 - trainLoss: 0.43846145272254944\n",
      "cnt: 0 - valLoss: 0.4398416578769684 - trainLoss: 0.43845900893211365\n",
      "cnt: 0 - valLoss: 0.4398405849933624 - trainLoss: 0.43845653533935547\n",
      "cnt: 0 - valLoss: 0.4398394525051117 - trainLoss: 0.4384540617465973\n",
      "cnt: 0 - valLoss: 0.43983837962150574 - trainLoss: 0.4384516477584839\n",
      "cnt: 0 - valLoss: 0.439837247133255 - trainLoss: 0.4384491741657257\n",
      "cnt: 0 - valLoss: 0.43983617424964905 - trainLoss: 0.4384467303752899\n",
      "cnt: 0 - valLoss: 0.4398351013660431 - trainLoss: 0.43844425678253174\n",
      "cnt: 0 - valLoss: 0.43983399868011475 - trainLoss: 0.43844181299209595\n",
      "cnt: 0 - valLoss: 0.4398329257965088 - trainLoss: 0.43843936920166016\n",
      "cnt: 0 - valLoss: 0.4398317337036133 - trainLoss: 0.43843692541122437\n",
      "cnt: 0 - valLoss: 0.4398306608200073 - trainLoss: 0.4384344518184662\n",
      "cnt: 0 - valLoss: 0.43982958793640137 - trainLoss: 0.4384319484233856\n",
      "cnt: 0 - valLoss: 0.43982845544815063 - trainLoss: 0.4384295642375946\n",
      "cnt: 0 - valLoss: 0.4398273527622223 - trainLoss: 0.4384271204471588\n",
      "cnt: 0 - valLoss: 0.43982627987861633 - trainLoss: 0.43842464685440063\n",
      "cnt: 0 - valLoss: 0.439825177192688 - trainLoss: 0.43842220306396484\n",
      "cnt: 0 - valLoss: 0.4398241341114044 - trainLoss: 0.43841972947120667\n",
      "cnt: 0 - valLoss: 0.4398230314254761 - trainLoss: 0.4384172856807709\n",
      "cnt: 0 - valLoss: 0.4398218095302582 - trainLoss: 0.4384148418903351\n",
      "cnt: 0 - valLoss: 0.4398207366466522 - trainLoss: 0.4384123682975769\n",
      "cnt: 0 - valLoss: 0.43981969356536865 - trainLoss: 0.4384099245071411\n",
      "cnt: 0 - valLoss: 0.4398185610771179 - trainLoss: 0.4384074807167053\n",
      "cnt: 0 - valLoss: 0.43981754779815674 - trainLoss: 0.43840500712394714\n",
      "cnt: 0 - valLoss: 0.439816415309906 - trainLoss: 0.43840259313583374\n",
      "cnt: 0 - valLoss: 0.43981534242630005 - trainLoss: 0.43840011954307556\n",
      "cnt: 0 - valLoss: 0.4398142695426941 - trainLoss: 0.4383976459503174\n",
      "cnt: 0 - valLoss: 0.43981319665908813 - trainLoss: 0.438395231962204\n",
      "cnt: 0 - valLoss: 0.4398120641708374 - trainLoss: 0.4383927583694458\n",
      "cnt: 0 - valLoss: 0.43981099128723145 - trainLoss: 0.4383902847766876\n",
      "cnt: 0 - valLoss: 0.4398099184036255 - trainLoss: 0.43838784098625183\n",
      "cnt: 0 - valLoss: 0.43980878591537476 - trainLoss: 0.43838539719581604\n",
      "cnt: 0 - valLoss: 0.4398077726364136 - trainLoss: 0.43838295340538025\n",
      "cnt: 0 - valLoss: 0.43980664014816284 - trainLoss: 0.43838050961494446\n",
      "cnt: 0 - valLoss: 0.43980562686920166 - trainLoss: 0.4383780360221863\n",
      "cnt: 0 - valLoss: 0.4398044943809509 - trainLoss: 0.4383755922317505\n",
      "cnt: 0 - valLoss: 0.43980348110198975 - trainLoss: 0.4383731484413147\n",
      "cnt: 0 - valLoss: 0.43980228900909424 - trainLoss: 0.4383706748485565\n",
      "cnt: 0 - valLoss: 0.43980124592781067 - trainLoss: 0.4383682608604431\n",
      "cnt: 0 - valLoss: 0.4398001432418823 - trainLoss: 0.43836578726768494\n",
      "cnt: 0 - valLoss: 0.43979907035827637 - trainLoss: 0.43836331367492676\n",
      "cnt: 0 - valLoss: 0.4397979974746704 - trainLoss: 0.43836089968681335\n",
      "cnt: 0 - valLoss: 0.43979692459106445 - trainLoss: 0.4383584260940552\n",
      "cnt: 0 - valLoss: 0.4397958517074585 - trainLoss: 0.438355952501297\n",
      "cnt: 0 - valLoss: 0.43979477882385254 - trainLoss: 0.4383535385131836\n",
      "cnt: 0 - valLoss: 0.4397937059402466 - trainLoss: 0.4383511245250702\n",
      "cnt: 0 - valLoss: 0.4397925138473511 - trainLoss: 0.4383486807346344\n",
      "cnt: 0 - valLoss: 0.4397914707660675 - trainLoss: 0.43834617733955383\n",
      "cnt: 0 - valLoss: 0.43979039788246155 - trainLoss: 0.43834376335144043\n",
      "cnt: 0 - valLoss: 0.4397893249988556 - trainLoss: 0.43834131956100464\n",
      "cnt: 0 - valLoss: 0.43978825211524963 - trainLoss: 0.43833887577056885\n",
      "cnt: 0 - valLoss: 0.4397871792316437 - trainLoss: 0.43833643198013306\n",
      "cnt: 0 - valLoss: 0.4397861659526825 - trainLoss: 0.43833398818969727\n",
      "cnt: 0 - valLoss: 0.43978503346443176 - trainLoss: 0.4383315443992615\n",
      "cnt: 0 - valLoss: 0.4397840201854706 - trainLoss: 0.4383291006088257\n",
      "cnt: 0 - valLoss: 0.43978285789489746 - trainLoss: 0.4383266568183899\n",
      "cnt: 0 - valLoss: 0.4397817850112915 - trainLoss: 0.4383242428302765\n",
      "cnt: 0 - valLoss: 0.43978071212768555 - trainLoss: 0.4383217990398407\n",
      "cnt: 0 - valLoss: 0.4397796392440796 - trainLoss: 0.43831929564476013\n",
      "cnt: 0 - valLoss: 0.4397786259651184 - trainLoss: 0.4383169114589691\n",
      "cnt: 0 - valLoss: 0.4397774934768677 - trainLoss: 0.4383144676685333\n",
      "cnt: 0 - valLoss: 0.4397764205932617 - trainLoss: 0.43831202387809753\n",
      "cnt: 0 - valLoss: 0.43977534770965576 - trainLoss: 0.43830955028533936\n",
      "cnt: 0 - valLoss: 0.4397743344306946 - trainLoss: 0.43830716609954834\n",
      "cnt: 0 - valLoss: 0.43977317214012146 - trainLoss: 0.43830472230911255\n",
      "cnt: 0 - valLoss: 0.4397720992565155 - trainLoss: 0.43830227851867676\n",
      "cnt: 0 - valLoss: 0.43977102637290955 - trainLoss: 0.43829983472824097\n",
      "cnt: 0 - valLoss: 0.4397699534893036 - trainLoss: 0.4382973909378052\n",
      "cnt: 0 - valLoss: 0.43976888060569763 - trainLoss: 0.4382949471473694\n",
      "cnt: 0 - valLoss: 0.43976786732673645 - trainLoss: 0.4382925033569336\n",
      "cnt: 0 - valLoss: 0.4397667944431305 - trainLoss: 0.4382900893688202\n",
      "cnt: 0 - valLoss: 0.43976572155952454 - trainLoss: 0.4382876455783844\n",
      "cnt: 0 - valLoss: 0.4397646486759186 - trainLoss: 0.43828514218330383\n",
      "cnt: 0 - valLoss: 0.43976351618766785 - trainLoss: 0.4382827579975128\n",
      "cnt: 0 - valLoss: 0.4397624731063843 - trainLoss: 0.43828028440475464\n",
      "cnt: 0 - valLoss: 0.4397614002227783 - trainLoss: 0.43827784061431885\n",
      "cnt: 0 - valLoss: 0.43975985050201416 - trainLoss: 0.43827539682388306\n",
      "cnt: 0 - valLoss: 0.43975839018821716 - trainLoss: 0.4382728636264801\n",
      "cnt: 0 - valLoss: 0.439756840467453 - trainLoss: 0.4382703900337219\n",
      "cnt: 0 - valLoss: 0.439755380153656 - trainLoss: 0.43826785683631897\n",
      "cnt: 0 - valLoss: 0.43975383043289185 - trainLoss: 0.438265323638916\n",
      "cnt: 0 - valLoss: 0.4397522211074829 - trainLoss: 0.4382627606391907\n",
      "cnt: 0 - valLoss: 0.43975070118904114 - trainLoss: 0.4382602870464325\n",
      "cnt: 0 - valLoss: 0.43974921107292175 - trainLoss: 0.43825775384902954\n",
      "cnt: 0 - valLoss: 0.43974775075912476 - trainLoss: 0.4382552206516266\n",
      "cnt: 0 - valLoss: 0.4397462010383606 - trainLoss: 0.438252717256546\n",
      "cnt: 0 - valLoss: 0.4397447109222412 - trainLoss: 0.43825018405914307\n",
      "cnt: 0 - valLoss: 0.4397432208061218 - trainLoss: 0.4382476508617401\n",
      "cnt: 0 - valLoss: 0.43974170088768005 - trainLoss: 0.43824517726898193\n",
      "cnt: 0 - valLoss: 0.43974021077156067 - trainLoss: 0.4382426142692566\n",
      "cnt: 0 - valLoss: 0.4397386610507965 - trainLoss: 0.4382401406764984\n",
      "cnt: 0 - valLoss: 0.4397371709346771 - trainLoss: 0.43823763728141785\n",
      "cnt: 0 - valLoss: 0.4397357106208801 - trainLoss: 0.4382350742816925\n",
      "cnt: 0 - valLoss: 0.43973422050476074 - trainLoss: 0.4382326006889343\n",
      "cnt: 0 - valLoss: 0.43973276019096375 - trainLoss: 0.438230037689209\n",
      "cnt: 0 - valLoss: 0.439731240272522 - trainLoss: 0.4382275342941284\n",
      "cnt: 0 - valLoss: 0.4397297501564026 - trainLoss: 0.4382249712944031\n",
      "cnt: 0 - valLoss: 0.4397282898426056 - trainLoss: 0.4382224977016449\n",
      "cnt: 0 - valLoss: 0.43972671031951904 - trainLoss: 0.43821996450424194\n",
      "cnt: 0 - valLoss: 0.43972522020339966 - trainLoss: 0.438217431306839\n",
      "cnt: 0 - valLoss: 0.43972378969192505 - trainLoss: 0.4382149577140808\n",
      "cnt: 0 - valLoss: 0.43972229957580566 - trainLoss: 0.43821239471435547\n",
      "cnt: 0 - valLoss: 0.43972083926200867 - trainLoss: 0.4382099211215973\n",
      "cnt: 0 - valLoss: 0.43971937894821167 - trainLoss: 0.43820732831954956\n",
      "cnt: 0 - valLoss: 0.4397179186344147 - trainLoss: 0.4382048547267914\n",
      "cnt: 0 - valLoss: 0.4397164583206177 - trainLoss: 0.4382023811340332\n",
      "cnt: 0 - valLoss: 0.43971487879753113 - trainLoss: 0.43819981813430786\n",
      "cnt: 0 - valLoss: 0.43971341848373413 - trainLoss: 0.4381973147392273\n",
      "cnt: 0 - valLoss: 0.43971195816993713 - trainLoss: 0.43819481134414673\n",
      "cnt: 0 - valLoss: 0.4397105574607849 - trainLoss: 0.4381922781467438\n",
      "cnt: 0 - valLoss: 0.4397090971469879 - trainLoss: 0.4381897449493408\n",
      "cnt: 0 - valLoss: 0.4397076368331909 - trainLoss: 0.43818721175193787\n",
      "cnt: 0 - valLoss: 0.4397061765193939 - trainLoss: 0.4381847381591797\n",
      "cnt: 0 - valLoss: 0.4397047758102417 - trainLoss: 0.43818217515945435\n",
      "cnt: 0 - valLoss: 0.43970322608947754 - trainLoss: 0.43817970156669617\n",
      "cnt: 0 - valLoss: 0.43970176577568054 - trainLoss: 0.4381771683692932\n",
      "cnt: 0 - valLoss: 0.4397003650665283 - trainLoss: 0.43817463517189026\n",
      "cnt: 0 - valLoss: 0.4396989047527313 - trainLoss: 0.4381721615791321\n",
      "cnt: 0 - valLoss: 0.4396974742412567 - trainLoss: 0.4381696283817291\n",
      "cnt: 0 - valLoss: 0.4396960735321045 - trainLoss: 0.43816709518432617\n",
      "cnt: 0 - valLoss: 0.4396946132183075 - trainLoss: 0.4381645917892456\n",
      "cnt: 0 - valLoss: 0.4396930932998657 - trainLoss: 0.43816205859184265\n",
      "cnt: 0 - valLoss: 0.4396916925907135 - trainLoss: 0.4381595849990845\n",
      "cnt: 0 - valLoss: 0.43969032168388367 - trainLoss: 0.4381570816040039\n",
      "cnt: 0 - valLoss: 0.43968895077705383 - trainLoss: 0.43815457820892334\n",
      "cnt: 0 - valLoss: 0.4396876096725464 - trainLoss: 0.4381520748138428\n",
      "cnt: 0 - valLoss: 0.43968620896339417 - trainLoss: 0.43814951181411743\n",
      "cnt: 0 - valLoss: 0.4396848678588867 - trainLoss: 0.43814706802368164\n",
      "cnt: 0 - valLoss: 0.43968355655670166 - trainLoss: 0.4381445050239563\n",
      "cnt: 0 - valLoss: 0.43968209624290466 - trainLoss: 0.4381420314311981\n",
      "cnt: 0 - valLoss: 0.43968072533607483 - trainLoss: 0.43813952803611755\n",
      "cnt: 0 - valLoss: 0.439679354429245 - trainLoss: 0.4381370544433594\n",
      "cnt: 0 - valLoss: 0.43967798352241516 - trainLoss: 0.4381345212459564\n",
      "cnt: 0 - valLoss: 0.4396766424179077 - trainLoss: 0.43813201785087585\n",
      "cnt: 0 - valLoss: 0.4396752715110779 - trainLoss: 0.4381295144557953\n",
      "cnt: 0 - valLoss: 0.4396739602088928 - trainLoss: 0.4381270408630371\n",
      "cnt: 0 - valLoss: 0.4396724998950958 - trainLoss: 0.43812447786331177\n",
      "cnt: 0 - valLoss: 0.439671128988266 - trainLoss: 0.4381220042705536\n",
      "cnt: 0 - valLoss: 0.43966978788375854 - trainLoss: 0.438119500875473\n",
      "cnt: 0 - valLoss: 0.4396684765815735 - trainLoss: 0.43811699748039246\n",
      "cnt: 0 - valLoss: 0.43966713547706604 - trainLoss: 0.4381144940853119\n",
      "cnt: 0 - valLoss: 0.4396657645702362 - trainLoss: 0.4381119906902313\n",
      "cnt: 0 - valLoss: 0.43966442346572876 - trainLoss: 0.43810948729515076\n",
      "cnt: 0 - valLoss: 0.43966296315193176 - trainLoss: 0.4381069839000702\n",
      "cnt: 0 - valLoss: 0.4396616518497467 - trainLoss: 0.43810445070266724\n",
      "cnt: 0 - valLoss: 0.43966031074523926 - trainLoss: 0.43810197710990906\n",
      "cnt: 0 - valLoss: 0.4396589398384094 - trainLoss: 0.4380994737148285\n",
      "cnt: 0 - valLoss: 0.43965765833854675 - trainLoss: 0.43809694051742554\n",
      "cnt: 0 - valLoss: 0.4396562874317169 - trainLoss: 0.43809446692466736\n",
      "cnt: 0 - valLoss: 0.4396549165248871 - trainLoss: 0.4380919337272644\n",
      "cnt: 0 - valLoss: 0.43965357542037964 - trainLoss: 0.4380894601345062\n",
      "cnt: 0 - valLoss: 0.43965214490890503 - trainLoss: 0.43808698654174805\n",
      "cnt: 0 - valLoss: 0.4396508038043976 - trainLoss: 0.4380844533443451\n",
      "cnt: 0 - valLoss: 0.43964946269989014 - trainLoss: 0.43808192014694214\n",
      "cnt: 0 - valLoss: 0.4396480917930603 - trainLoss: 0.43807944655418396\n",
      "cnt: 0 - valLoss: 0.43964678049087524 - trainLoss: 0.4380769729614258\n",
      "cnt: 0 - valLoss: 0.4396454393863678 - trainLoss: 0.4380744397640228\n",
      "cnt: 0 - valLoss: 0.43964406847953796 - trainLoss: 0.4380719065666199\n",
      "cnt: 0 - valLoss: 0.4396427869796753 - trainLoss: 0.4380694329738617\n",
      "cnt: 0 - valLoss: 0.43964144587516785 - trainLoss: 0.4380669593811035\n",
      "cnt: 0 - valLoss: 0.4396400451660156 - trainLoss: 0.43806445598602295\n",
      "cnt: 0 - valLoss: 0.4396387040615082 - trainLoss: 0.43806198239326477\n",
      "cnt: 0 - valLoss: 0.4396374225616455 - trainLoss: 0.43805941939353943\n",
      "cnt: 0 - valLoss: 0.4396360516548157 - trainLoss: 0.43805694580078125\n",
      "cnt: 0 - valLoss: 0.4396347403526306 - trainLoss: 0.4380544424057007\n",
      "cnt: 0 - valLoss: 0.43963345885276794 - trainLoss: 0.4380519688129425\n",
      "cnt: 0 - valLoss: 0.4396321177482605 - trainLoss: 0.43804940581321716\n",
      "cnt: 0 - valLoss: 0.43963077664375305 - trainLoss: 0.43804696202278137\n",
      "cnt: 0 - valLoss: 0.4396294951438904 - trainLoss: 0.4380444884300232\n",
      "cnt: 0 - valLoss: 0.4396280348300934 - trainLoss: 0.43804195523262024\n",
      "cnt: 0 - valLoss: 0.4396267235279083 - trainLoss: 0.43803948163986206\n",
      "cnt: 0 - valLoss: 0.43962544202804565 - trainLoss: 0.4380369484424591\n",
      "cnt: 0 - valLoss: 0.4396241009235382 - trainLoss: 0.43803441524505615\n",
      "cnt: 0 - valLoss: 0.43962281942367554 - trainLoss: 0.438031941652298\n",
      "cnt: 0 - valLoss: 0.4396214783191681 - trainLoss: 0.4380294680595398\n",
      "cnt: 0 - valLoss: 0.4396201968193054 - trainLoss: 0.43802693486213684\n",
      "cnt: 0 - valLoss: 0.43961891531944275 - trainLoss: 0.43802446126937866\n",
      "cnt: 0 - valLoss: 0.4396175444126129 - trainLoss: 0.4380219280719757\n",
      "cnt: 0 - valLoss: 0.4396161735057831 - trainLoss: 0.43801945447921753\n",
      "cnt: 0 - valLoss: 0.43961483240127563 - trainLoss: 0.43801695108413696\n",
      "cnt: 0 - valLoss: 0.43961355090141296 - trainLoss: 0.4380144476890564\n",
      "cnt: 0 - valLoss: 0.4396122395992279 - trainLoss: 0.4380120038986206\n",
      "cnt: 0 - valLoss: 0.43961092829704285 - trainLoss: 0.4380094110965729\n",
      "cnt: 0 - valLoss: 0.4396096467971802 - trainLoss: 0.4380069375038147\n",
      "cnt: 0 - valLoss: 0.4396083652973175 - trainLoss: 0.4380044639110565\n",
      "cnt: 0 - valLoss: 0.43960705399513245 - trainLoss: 0.43800199031829834\n",
      "cnt: 0 - valLoss: 0.439605712890625 - trainLoss: 0.4379994571208954\n",
      "cnt: 0 - valLoss: 0.43960434198379517 - trainLoss: 0.4379969835281372\n",
      "cnt: 0 - valLoss: 0.43960314989089966 - trainLoss: 0.43799445033073425\n",
      "cnt: 0 - valLoss: 0.439601868391037 - trainLoss: 0.4379919767379761\n",
      "cnt: 0 - valLoss: 0.43960055708885193 - trainLoss: 0.4379894733428955\n",
      "cnt: 0 - valLoss: 0.43959927558898926 - trainLoss: 0.43798699975013733\n",
      "cnt: 0 - valLoss: 0.4395979940891266 - trainLoss: 0.4379844665527344\n",
      "cnt: 0 - valLoss: 0.4395967125892639 - trainLoss: 0.4379819333553314\n",
      "cnt: 0 - valLoss: 0.4395953416824341 - trainLoss: 0.43797945976257324\n",
      "cnt: 0 - valLoss: 0.4395940899848938 - trainLoss: 0.43797698616981506\n",
      "cnt: 0 - valLoss: 0.4395928680896759 - trainLoss: 0.4379745125770569\n",
      "cnt: 0 - valLoss: 0.43959152698516846 - trainLoss: 0.43797197937965393\n",
      "cnt: 0 - valLoss: 0.4395902752876282 - trainLoss: 0.437969446182251\n",
      "cnt: 0 - valLoss: 0.4395889937877655 - trainLoss: 0.4379670023918152\n",
      "cnt: 0 - valLoss: 0.43958771228790283 - trainLoss: 0.43796446919441223\n",
      "cnt: 0 - valLoss: 0.43958646059036255 - trainLoss: 0.43796199560165405\n",
      "cnt: 0 - valLoss: 0.4395850896835327 - trainLoss: 0.4379594624042511\n",
      "cnt: 0 - valLoss: 0.4395838677883148 - trainLoss: 0.4379569888114929\n",
      "cnt: 0 - valLoss: 0.43958255648612976 - trainLoss: 0.43795445561408997\n",
      "cnt: 0 - valLoss: 0.43958133459091187 - trainLoss: 0.4379519820213318\n",
      "cnt: 0 - valLoss: 0.4395800828933716 - trainLoss: 0.4379495084285736\n",
      "cnt: 0 - valLoss: 0.4395788013935089 - trainLoss: 0.43794703483581543\n",
      "cnt: 0 - valLoss: 0.43957754969596863 - trainLoss: 0.4379444420337677\n",
      "cnt: 0 - valLoss: 0.43957626819610596 - trainLoss: 0.4379419982433319\n",
      "cnt: 0 - valLoss: 0.4395749270915985 - trainLoss: 0.43793952465057373\n",
      "cnt: 0 - valLoss: 0.4395737051963806 - trainLoss: 0.4379369914531708\n",
      "cnt: 0 - valLoss: 0.43957242369651794 - trainLoss: 0.4379345178604126\n",
      "cnt: 0 - valLoss: 0.43957117199897766 - trainLoss: 0.4379320442676544\n",
      "cnt: 0 - valLoss: 0.439569890499115 - trainLoss: 0.43792951107025146\n",
      "cnt: 0 - valLoss: 0.4395686686038971 - trainLoss: 0.4379270374774933\n",
      "cnt: 0 - valLoss: 0.4395674467086792 - trainLoss: 0.43792450428009033\n",
      "cnt: 0 - valLoss: 0.43956610560417175 - trainLoss: 0.43792203068733215\n",
      "cnt: 0 - valLoss: 0.4395648241043091 - trainLoss: 0.4379195272922516\n",
      "cnt: 0 - valLoss: 0.4395635724067688 - trainLoss: 0.4379170536994934\n",
      "cnt: 0 - valLoss: 0.4395623505115509 - trainLoss: 0.43791452050209045\n",
      "cnt: 0 - valLoss: 0.4395610988140106 - trainLoss: 0.4379120469093323\n",
      "cnt: 0 - valLoss: 0.43955984711647034 - trainLoss: 0.4379095137119293\n",
      "cnt: 0 - valLoss: 0.43955865502357483 - trainLoss: 0.43790704011917114\n",
      "cnt: 0 - valLoss: 0.43955740332603455 - trainLoss: 0.43790456652641296\n",
      "cnt: 0 - valLoss: 0.43955615162849426 - trainLoss: 0.4379020929336548\n",
      "cnt: 0 - valLoss: 0.43955478072166443 - trainLoss: 0.4378995895385742\n",
      "cnt: 0 - valLoss: 0.4395535886287689 - trainLoss: 0.43789711594581604\n",
      "cnt: 0 - valLoss: 0.4395523965358734 - trainLoss: 0.4378945529460907\n",
      "cnt: 0 - valLoss: 0.43955114483833313 - trainLoss: 0.4378921091556549\n",
      "cnt: 0 - valLoss: 0.43954989314079285 - trainLoss: 0.43788957595825195\n",
      "cnt: 0 - valLoss: 0.43954867124557495 - trainLoss: 0.4378871023654938\n",
      "cnt: 0 - valLoss: 0.43954744935035706 - trainLoss: 0.4378846287727356\n",
      "cnt: 0 - valLoss: 0.439546138048172 - trainLoss: 0.43788209557533264\n",
      "cnt: 0 - valLoss: 0.4395449459552765 - trainLoss: 0.4378795623779297\n",
      "cnt: 0 - valLoss: 0.4395436942577362 - trainLoss: 0.4378771185874939\n",
      "cnt: 0 - valLoss: 0.4395424425601959 - trainLoss: 0.43787458539009094\n",
      "cnt: 0 - valLoss: 0.4395412504673004 - trainLoss: 0.43787211179733276\n",
      "cnt: 0 - valLoss: 0.43953999876976013 - trainLoss: 0.4378696382045746\n",
      "cnt: 0 - valLoss: 0.43953877687454224 - trainLoss: 0.43786710500717163\n",
      "cnt: 0 - valLoss: 0.43953755497932434 - trainLoss: 0.43786463141441345\n",
      "cnt: 0 - valLoss: 0.4395362436771393 - trainLoss: 0.4378620982170105\n",
      "cnt: 0 - valLoss: 0.43953508138656616 - trainLoss: 0.4378596246242523\n",
      "cnt: 0 - valLoss: 0.4395337998867035 - trainLoss: 0.43785715103149414\n",
      "cnt: 0 - valLoss: 0.43953263759613037 - trainLoss: 0.4378546178340912\n",
      "cnt: 0 - valLoss: 0.4395313858985901 - trainLoss: 0.4378521144390106\n",
      "cnt: 0 - valLoss: 0.4395301938056946 - trainLoss: 0.43784964084625244\n",
      "cnt: 0 - valLoss: 0.4395290017127991 - trainLoss: 0.4378471076488495\n",
      "cnt: 0 - valLoss: 0.43952763080596924 - trainLoss: 0.4378446340560913\n",
      "cnt: 0 - valLoss: 0.43952640891075134 - trainLoss: 0.43784216046333313\n",
      "cnt: 0 - valLoss: 0.43952521681785583 - trainLoss: 0.43783968687057495\n",
      "cnt: 0 - valLoss: 0.4395240247249603 - trainLoss: 0.4378371834754944\n",
      "cnt: 0 - valLoss: 0.4395228326320648 - trainLoss: 0.4378346800804138\n",
      "cnt: 0 - valLoss: 0.4395216107368469 - trainLoss: 0.437832236289978\n",
      "cnt: 0 - valLoss: 0.4395204186439514 - trainLoss: 0.4378296434879303\n",
      "cnt: 0 - valLoss: 0.4395192265510559 - trainLoss: 0.4378271698951721\n",
      "cnt: 0 - valLoss: 0.43951794505119324 - trainLoss: 0.43782469630241394\n",
      "cnt: 0 - valLoss: 0.43951669335365295 - trainLoss: 0.43782222270965576\n",
      "cnt: 0 - valLoss: 0.43951553106307983 - trainLoss: 0.4378197193145752\n",
      "cnt: 0 - valLoss: 0.43951427936553955 - trainLoss: 0.43781721591949463\n",
      "cnt: 0 - valLoss: 0.43951308727264404 - trainLoss: 0.43781471252441406\n",
      "cnt: 0 - valLoss: 0.4395119249820709 - trainLoss: 0.4378122091293335\n",
      "cnt: 0 - valLoss: 0.4395107328891754 - trainLoss: 0.43780970573425293\n",
      "cnt: 0 - valLoss: 0.43950939178466797 - trainLoss: 0.43780723214149475\n",
      "cnt: 0 - valLoss: 0.43950825929641724 - trainLoss: 0.4378047585487366\n",
      "cnt: 0 - valLoss: 0.43950700759887695 - trainLoss: 0.437802255153656\n",
      "cnt: 0 - valLoss: 0.43950581550598145 - trainLoss: 0.43779975175857544\n",
      "cnt: 0 - valLoss: 0.43950459361076355 - trainLoss: 0.4377972483634949\n",
      "cnt: 0 - valLoss: 0.43950340151786804 - trainLoss: 0.4377947747707367\n",
      "cnt: 0 - valLoss: 0.4395022392272949 - trainLoss: 0.43779224157333374\n",
      "cnt: 0 - valLoss: 0.43950095772743225 - trainLoss: 0.43778976798057556\n",
      "cnt: 0 - valLoss: 0.43949976563453674 - trainLoss: 0.4377872943878174\n",
      "cnt: 0 - valLoss: 0.43949854373931885 - trainLoss: 0.4377847909927368\n",
      "cnt: 0 - valLoss: 0.4394974112510681 - trainLoss: 0.43778231739997864\n",
      "cnt: 0 - valLoss: 0.439496248960495 - trainLoss: 0.4377797842025757\n",
      "cnt: 0 - valLoss: 0.43949511647224426 - trainLoss: 0.4377773404121399\n",
      "cnt: 0 - valLoss: 0.4394940137863159 - trainLoss: 0.4377748668193817\n",
      "cnt: 0 - valLoss: 0.4394928216934204 - trainLoss: 0.43777233362197876\n",
      "cnt: 0 - valLoss: 0.4394916892051697 - trainLoss: 0.4377698600292206\n",
      "cnt: 0 - valLoss: 0.43949049711227417 - trainLoss: 0.4377674162387848\n",
      "cnt: 0 - valLoss: 0.4394894242286682 - trainLoss: 0.4377649128437042\n",
      "cnt: 0 - valLoss: 0.4394882619380951 - trainLoss: 0.43776240944862366\n",
      "cnt: 0 - valLoss: 0.43948715925216675 - trainLoss: 0.43775996565818787\n",
      "cnt: 0 - valLoss: 0.4394860565662384 - trainLoss: 0.4377574920654297\n",
      "cnt: 0 - valLoss: 0.4394848346710205 - trainLoss: 0.4377550184726715\n",
      "cnt: 0 - valLoss: 0.43948373198509216 - trainLoss: 0.43775251507759094\n",
      "cnt: 0 - valLoss: 0.43948259949684143 - trainLoss: 0.4377500116825104\n",
      "cnt: 0 - valLoss: 0.4394814968109131 - trainLoss: 0.4377475678920746\n",
      "cnt: 0 - valLoss: 0.43948033452033997 - trainLoss: 0.4377450942993164\n",
      "cnt: 0 - valLoss: 0.4394792318344116 - trainLoss: 0.43774256110191345\n",
      "cnt: 0 - valLoss: 0.4394780993461609 - trainLoss: 0.43774011731147766\n",
      "cnt: 0 - valLoss: 0.43947696685791016 - trainLoss: 0.4377375841140747\n",
      "cnt: 0 - valLoss: 0.43947580456733704 - trainLoss: 0.4377351403236389\n",
      "cnt: 0 - valLoss: 0.4394746720790863 - trainLoss: 0.43773266673088074\n",
      "cnt: 0 - valLoss: 0.43947359919548035 - trainLoss: 0.4377301335334778\n",
      "cnt: 0 - valLoss: 0.439472496509552 - trainLoss: 0.4377277195453644\n",
      "cnt: 0 - valLoss: 0.43947139382362366 - trainLoss: 0.4377251863479614\n",
      "cnt: 0 - valLoss: 0.4394702613353729 - trainLoss: 0.43772274255752563\n",
      "cnt: 0 - valLoss: 0.4394690692424774 - trainLoss: 0.4377202093601227\n",
      "cnt: 0 - valLoss: 0.43946799635887146 - trainLoss: 0.4377177655696869\n",
      "cnt: 0 - valLoss: 0.43946683406829834 - trainLoss: 0.4377152919769287\n",
      "cnt: 0 - valLoss: 0.4394657611846924 - trainLoss: 0.43771275877952576\n",
      "cnt: 0 - valLoss: 0.43946465849876404 - trainLoss: 0.43771031498908997\n",
      "cnt: 0 - valLoss: 0.4394635260105133 - trainLoss: 0.4377078413963318\n",
      "cnt: 0 - valLoss: 0.43946248292922974 - trainLoss: 0.4377053678035736\n",
      "cnt: 0 - valLoss: 0.43946126103401184 - trainLoss: 0.43770289421081543\n",
      "cnt: 0 - valLoss: 0.4394601583480835 - trainLoss: 0.43770039081573486\n",
      "cnt: 0 - valLoss: 0.43945902585983276 - trainLoss: 0.4376979172229767\n",
      "cnt: 0 - valLoss: 0.4394579827785492 - trainLoss: 0.4376954436302185\n",
      "cnt: 0 - valLoss: 0.43945690989494324 - trainLoss: 0.43769294023513794\n",
      "cnt: 0 - valLoss: 0.4394557774066925 - trainLoss: 0.43769046664237976\n",
      "cnt: 0 - valLoss: 0.43945467472076416 - trainLoss: 0.4376879930496216\n",
      "cnt: 0 - valLoss: 0.43945348262786865 - trainLoss: 0.437685489654541\n",
      "cnt: 0 - valLoss: 0.4394523501396179 - trainLoss: 0.43768301606178284\n",
      "cnt: 0 - valLoss: 0.43945127725601196 - trainLoss: 0.43768054246902466\n",
      "cnt: 0 - valLoss: 0.439450204372406 - trainLoss: 0.4376780688762665\n",
      "cnt: 0 - valLoss: 0.43944910168647766 - trainLoss: 0.4376755654811859\n",
      "cnt: 0 - valLoss: 0.43944796919822693 - trainLoss: 0.43767309188842773\n",
      "cnt: 0 - valLoss: 0.43944689631462097 - trainLoss: 0.43767061829566956\n",
      "cnt: 0 - valLoss: 0.43944576382637024 - trainLoss: 0.437668114900589\n",
      "cnt: 0 - valLoss: 0.4394446313381195 - trainLoss: 0.4376656413078308\n",
      "cnt: 0 - valLoss: 0.43944355845451355 - trainLoss: 0.43766316771507263\n",
      "cnt: 0 - valLoss: 0.4394424855709076 - trainLoss: 0.43766066431999207\n",
      "cnt: 0 - valLoss: 0.43944141268730164 - trainLoss: 0.4376581907272339\n",
      "cnt: 0 - valLoss: 0.4394403398036957 - trainLoss: 0.4376557171344757\n",
      "cnt: 0 - valLoss: 0.43943923711776733 - trainLoss: 0.43765324354171753\n",
      "cnt: 0 - valLoss: 0.4394380450248718 - trainLoss: 0.43765074014663696\n",
      "cnt: 0 - valLoss: 0.43943700194358826 - trainLoss: 0.4376482665538788\n",
      "cnt: 0 - valLoss: 0.4394358992576599 - trainLoss: 0.437645822763443\n",
      "cnt: 0 - valLoss: 0.43943482637405396 - trainLoss: 0.4376433491706848\n",
      "cnt: 0 - valLoss: 0.4394336938858032 - trainLoss: 0.43764081597328186\n",
      "cnt: 0 - valLoss: 0.43943268060684204 - trainLoss: 0.43763837218284607\n",
      "cnt: 0 - valLoss: 0.4394315779209137 - trainLoss: 0.4376358985900879\n",
      "cnt: 0 - valLoss: 0.4394303560256958 - trainLoss: 0.4376334547996521\n",
      "cnt: 0 - valLoss: 0.43942928314208984 - trainLoss: 0.43763092160224915\n",
      "cnt: 0 - valLoss: 0.4394282102584839 - trainLoss: 0.43762844800949097\n",
      "cnt: 0 - valLoss: 0.43942713737487793 - trainLoss: 0.4376260042190552\n",
      "cnt: 0 - valLoss: 0.439426064491272 - trainLoss: 0.4376234710216522\n",
      "cnt: 0 - valLoss: 0.4394250214099884 - trainLoss: 0.4376210570335388\n",
      "cnt: 0 - valLoss: 0.43942388892173767 - trainLoss: 0.43761855363845825\n",
      "cnt: 0 - valLoss: 0.43942272663116455 - trainLoss: 0.4376160204410553\n",
      "cnt: 0 - valLoss: 0.4394216537475586 - trainLoss: 0.4376136064529419\n",
      "cnt: 0 - valLoss: 0.43942058086395264 - trainLoss: 0.43761107325553894\n",
      "cnt: 0 - valLoss: 0.4394195079803467 - trainLoss: 0.43760862946510315\n",
      "cnt: 0 - valLoss: 0.43941840529441833 - trainLoss: 0.43760615587234497\n",
      "cnt: 0 - valLoss: 0.4394173324108124 - trainLoss: 0.4376036524772644\n",
      "cnt: 0 - valLoss: 0.43941617012023926 - trainLoss: 0.4376011788845062\n",
      "cnt: 0 - valLoss: 0.4394150972366333 - trainLoss: 0.43759870529174805\n",
      "cnt: 0 - valLoss: 0.43941402435302734 - trainLoss: 0.43759623169898987\n",
      "cnt: 0 - valLoss: 0.4394129514694214 - trainLoss: 0.4375937283039093\n",
      "cnt: 0 - valLoss: 0.43941187858581543 - trainLoss: 0.4375912547111511\n",
      "cnt: 0 - valLoss: 0.4394108057022095 - trainLoss: 0.43758878111839294\n",
      "cnt: 0 - valLoss: 0.4394097626209259 - trainLoss: 0.4375862777233124\n",
      "cnt: 0 - valLoss: 0.4394086003303528 - trainLoss: 0.4375838041305542\n",
      "cnt: 0 - valLoss: 0.4394075274467468 - trainLoss: 0.4375813603401184\n",
      "cnt: 0 - valLoss: 0.43940645456314087 - trainLoss: 0.43757888674736023\n",
      "cnt: 0 - valLoss: 0.4394053816795349 - trainLoss: 0.4375763535499573\n",
      "cnt: 0 - valLoss: 0.43940433859825134 - trainLoss: 0.4375738799571991\n",
      "cnt: 0 - valLoss: 0.4394032657146454 - trainLoss: 0.4375714361667633\n",
      "cnt: 0 - valLoss: 0.43940219283103943 - trainLoss: 0.4375689625740051\n",
      "cnt: 0 - valLoss: 0.4394010007381439 - trainLoss: 0.43756645917892456\n",
      "cnt: 0 - valLoss: 0.43939998745918274 - trainLoss: 0.437563955783844\n",
      "cnt: 0 - valLoss: 0.439398854970932 - trainLoss: 0.4375614523887634\n",
      "cnt: 0 - valLoss: 0.4393978714942932 - trainLoss: 0.43755900859832764\n",
      "cnt: 0 - valLoss: 0.43939679861068726 - trainLoss: 0.43755659461021423\n",
      "cnt: 0 - valLoss: 0.4393956959247589 - trainLoss: 0.4375540018081665\n",
      "cnt: 0 - valLoss: 0.4393945634365082 - trainLoss: 0.4375515878200531\n",
      "cnt: 0 - valLoss: 0.4393934905529022 - trainLoss: 0.43754908442497253\n",
      "cnt: 0 - valLoss: 0.43939244747161865 - trainLoss: 0.43754661083221436\n",
      "cnt: 0 - valLoss: 0.4393914043903351 - trainLoss: 0.43754416704177856\n",
      "cnt: 0 - valLoss: 0.43939030170440674 - trainLoss: 0.4375416338443756\n",
      "cnt: 0 - valLoss: 0.43938925862312317 - trainLoss: 0.4375392198562622\n",
      "cnt: 0 - valLoss: 0.439388245344162 - trainLoss: 0.43753668665885925\n",
      "cnt: 0 - valLoss: 0.43938708305358887 - trainLoss: 0.43753424286842346\n",
      "cnt: 0 - valLoss: 0.4393860101699829 - trainLoss: 0.4375317692756653\n",
      "cnt: 0 - valLoss: 0.43938496708869934 - trainLoss: 0.4375292658805847\n",
      "cnt: 0 - valLoss: 0.43938395380973816 - trainLoss: 0.43752679228782654\n",
      "cnt: 0 - valLoss: 0.4393828809261322 - trainLoss: 0.43752431869506836\n",
      "cnt: 0 - valLoss: 0.43938183784484863 - trainLoss: 0.4375218152999878\n",
      "cnt: 0 - valLoss: 0.4393806755542755 - trainLoss: 0.4375193417072296\n",
      "cnt: 0 - valLoss: 0.43937960267066956 - trainLoss: 0.43751686811447144\n",
      "cnt: 0 - valLoss: 0.4393785893917084 - trainLoss: 0.43751439452171326\n",
      "cnt: 0 - valLoss: 0.4393775165081024 - trainLoss: 0.4375118911266327\n",
      "cnt: 0 - valLoss: 0.43937647342681885 - trainLoss: 0.4375094175338745\n",
      "cnt: 0 - valLoss: 0.43937546014785767 - trainLoss: 0.43750694394111633\n",
      "cnt: 0 - valLoss: 0.4393743872642517 - trainLoss: 0.43750444054603577\n",
      "cnt: 0 - valLoss: 0.4393732249736786 - trainLoss: 0.4375019669532776\n",
      "cnt: 0 - valLoss: 0.4393722414970398 - trainLoss: 0.4374995231628418\n",
      "cnt: 0 - valLoss: 0.43937116861343384 - trainLoss: 0.43749698996543884\n",
      "cnt: 0 - valLoss: 0.43937015533447266 - trainLoss: 0.43749457597732544\n",
      "cnt: 0 - valLoss: 0.4393690824508667 - trainLoss: 0.4374920427799225\n",
      "cnt: 0 - valLoss: 0.4393680989742279 - trainLoss: 0.4374896287918091\n",
      "cnt: 0 - valLoss: 0.43936702609062195 - trainLoss: 0.4374871253967285\n",
      "cnt: 0 - valLoss: 0.43936586380004883 - trainLoss: 0.43748462200164795\n",
      "cnt: 0 - valLoss: 0.43936482071876526 - trainLoss: 0.43748220801353455\n",
      "cnt: 0 - valLoss: 0.4393638074398041 - trainLoss: 0.437479704618454\n",
      "cnt: 0 - valLoss: 0.4393627643585205 - trainLoss: 0.4374772310256958\n",
      "cnt: 0 - valLoss: 0.4393617510795593 - trainLoss: 0.4374747574329376\n",
      "cnt: 0 - valLoss: 0.43936067819595337 - trainLoss: 0.43747225403785706\n",
      "cnt: 0 - valLoss: 0.43935954570770264 - trainLoss: 0.4374697804450989\n",
      "cnt: 0 - valLoss: 0.43935853242874146 - trainLoss: 0.4374673068523407\n",
      "cnt: 0 - valLoss: 0.4393574595451355 - trainLoss: 0.4374648630619049\n",
      "cnt: 0 - valLoss: 0.43935641646385193 - trainLoss: 0.43746238946914673\n",
      "cnt: 0 - valLoss: 0.43935540318489075 - trainLoss: 0.4374598562717438\n",
      "cnt: 0 - valLoss: 0.4393543303012848 - trainLoss: 0.437457412481308\n",
      "cnt: 0 - valLoss: 0.4393532872200012 - trainLoss: 0.43745487928390503\n",
      "cnt: 0 - valLoss: 0.43935221433639526 - trainLoss: 0.43745243549346924\n",
      "cnt: 0 - valLoss: 0.4393511712551117 - trainLoss: 0.43744996190071106\n",
      "cnt: 0 - valLoss: 0.43935009837150574 - trainLoss: 0.4374474883079529\n",
      "cnt: 0 - valLoss: 0.43934908509254456 - trainLoss: 0.4374450147151947\n",
      "cnt: 0 - valLoss: 0.439348042011261 - trainLoss: 0.43744251132011414\n",
      "cnt: 0 - valLoss: 0.4393469989299774 - trainLoss: 0.43744006752967834\n",
      "cnt: 0 - valLoss: 0.4393458962440491 - trainLoss: 0.43743759393692017\n",
      "cnt: 0 - valLoss: 0.4393448233604431 - trainLoss: 0.4374350607395172\n",
      "cnt: 0 - valLoss: 0.4393438398838043 - trainLoss: 0.4374326169490814\n",
      "cnt: 0 - valLoss: 0.43934279680252075 - trainLoss: 0.43743014335632324\n",
      "cnt: 0 - valLoss: 0.4393417239189148 - trainLoss: 0.43742766976356506\n",
      "cnt: 0 - valLoss: 0.439340740442276 - trainLoss: 0.4374251961708069\n",
      "cnt: 0 - valLoss: 0.4393397271633148 - trainLoss: 0.4374226927757263\n",
      "cnt: 0 - valLoss: 0.4393385648727417 - trainLoss: 0.43742021918296814\n",
      "cnt: 0 - valLoss: 0.43933752179145813 - trainLoss: 0.43741774559020996\n",
      "cnt: 0 - valLoss: 0.43933650851249695 - trainLoss: 0.4374152421951294\n",
      "cnt: 0 - valLoss: 0.4393354654312134 - trainLoss: 0.4374127686023712\n",
      "cnt: 0 - valLoss: 0.4393344521522522 - trainLoss: 0.43741029500961304\n",
      "cnt: 0 - valLoss: 0.43933340907096863 - trainLoss: 0.43740779161453247\n",
      "cnt: 0 - valLoss: 0.4393323063850403 - trainLoss: 0.4374053180217743\n",
      "cnt: 0 - valLoss: 0.4393312633037567 - trainLoss: 0.4374028742313385\n",
      "cnt: 0 - valLoss: 0.43933025002479553 - trainLoss: 0.43740037083625793\n",
      "cnt: 0 - valLoss: 0.43932920694351196 - trainLoss: 0.43739792704582214\n",
      "cnt: 0 - valLoss: 0.4393281936645508 - trainLoss: 0.4373954236507416\n",
      "cnt: 0 - valLoss: 0.439327210187912 - trainLoss: 0.4373930096626282\n",
      "cnt: 0 - valLoss: 0.43932607769966125 - trainLoss: 0.4373904764652252\n",
      "cnt: 0 - valLoss: 0.43932509422302246 - trainLoss: 0.43738803267478943\n",
      "cnt: 0 - valLoss: 0.43932411074638367 - trainLoss: 0.43738555908203125\n",
      "cnt: 0 - valLoss: 0.4393230676651001 - trainLoss: 0.4373830556869507\n",
      "cnt: 0 - valLoss: 0.4393220841884613 - trainLoss: 0.4373805820941925\n",
      "cnt: 0 - valLoss: 0.4393211007118225 - trainLoss: 0.4373781085014343\n",
      "cnt: 0 - valLoss: 0.4393201172351837 - trainLoss: 0.43737560510635376\n",
      "cnt: 0 - valLoss: 0.43931904435157776 - trainLoss: 0.4373731315135956\n",
      "cnt: 0 - valLoss: 0.4393180310726166 - trainLoss: 0.4373706877231598\n",
      "cnt: 0 - valLoss: 0.439316987991333 - trainLoss: 0.4373681843280792\n",
      "cnt: 0 - valLoss: 0.4393159747123718 - trainLoss: 0.43736568093299866\n",
      "cnt: 0 - valLoss: 0.4393150210380554 - trainLoss: 0.43736323714256287\n",
      "cnt: 0 - valLoss: 0.43931400775909424 - trainLoss: 0.4373607337474823\n",
      "cnt: 0 - valLoss: 0.4393129348754883 - trainLoss: 0.4373582899570465\n",
      "cnt: 0 - valLoss: 0.4393119513988495 - trainLoss: 0.43735578656196594\n",
      "cnt: 0 - valLoss: 0.4393109083175659 - trainLoss: 0.43735331296920776\n",
      "cnt: 0 - valLoss: 0.4393099546432495 - trainLoss: 0.4373508393764496\n",
      "cnt: 0 - valLoss: 0.43930894136428833 - trainLoss: 0.43734830617904663\n",
      "cnt: 0 - valLoss: 0.43930795788764954 - trainLoss: 0.43734586238861084\n",
      "cnt: 0 - valLoss: 0.43930697441101074 - trainLoss: 0.43734341859817505\n",
      "cnt: 0 - valLoss: 0.43930584192276 - trainLoss: 0.43734094500541687\n",
      "cnt: 0 - valLoss: 0.4393048584461212 - trainLoss: 0.4373384118080139\n",
      "cnt: 0 - valLoss: 0.4393038749694824 - trainLoss: 0.4373359978199005\n",
      "cnt: 0 - valLoss: 0.43930286169052124 - trainLoss: 0.43733349442481995\n",
      "cnt: 0 - valLoss: 0.43930181860923767 - trainLoss: 0.43733102083206177\n",
      "cnt: 0 - valLoss: 0.43930089473724365 - trainLoss: 0.4373285472393036\n",
      "cnt: 0 - valLoss: 0.4392997622489929 - trainLoss: 0.437326043844223\n",
      "cnt: 0 - valLoss: 0.4392988085746765 - trainLoss: 0.43732357025146484\n",
      "cnt: 0 - valLoss: 0.43929779529571533 - trainLoss: 0.43732109665870667\n",
      "cnt: 0 - valLoss: 0.43929681181907654 - trainLoss: 0.4373186528682709\n",
      "cnt: 0 - valLoss: 0.43929582834243774 - trainLoss: 0.4373161792755127\n",
      "cnt: 0 - valLoss: 0.43929484486579895 - trainLoss: 0.43731367588043213\n",
      "cnt: 0 - valLoss: 0.439293771982193 - trainLoss: 0.43731123208999634\n",
      "cnt: 0 - valLoss: 0.4392927289009094 - trainLoss: 0.43730875849723816\n",
      "cnt: 0 - valLoss: 0.43929174542427063 - trainLoss: 0.4373062252998352\n",
      "cnt: 0 - valLoss: 0.43929076194763184 - trainLoss: 0.437303751707077\n",
      "cnt: 0 - valLoss: 0.43928977847099304 - trainLoss: 0.43730130791664124\n",
      "cnt: 0 - valLoss: 0.43928879499435425 - trainLoss: 0.43729883432388306\n",
      "cnt: 0 - valLoss: 0.43928781151771545 - trainLoss: 0.4372963607311249\n",
      "cnt: 0 - valLoss: 0.4392867386341095 - trainLoss: 0.4372938573360443\n",
      "cnt: 0 - valLoss: 0.4392856955528259 - trainLoss: 0.43729138374328613\n",
      "cnt: 0 - valLoss: 0.43928471207618713 - trainLoss: 0.43728891015052795\n",
      "cnt: 0 - valLoss: 0.43928372859954834 - trainLoss: 0.4372864067554474\n",
      "cnt: 0 - valLoss: 0.43928271532058716 - trainLoss: 0.4372839331626892\n",
      "cnt: 0 - valLoss: 0.43928176164627075 - trainLoss: 0.43728145956993103\n",
      "cnt: 0 - valLoss: 0.4392806589603424 - trainLoss: 0.43727898597717285\n",
      "cnt: 0 - valLoss: 0.439279705286026 - trainLoss: 0.43727654218673706\n",
      "cnt: 0 - valLoss: 0.4392787218093872 - trainLoss: 0.4372740387916565\n",
      "cnt: 0 - valLoss: 0.43927767872810364 - trainLoss: 0.4372715353965759\n",
      "cnt: 0 - valLoss: 0.43927669525146484 - trainLoss: 0.43726909160614014\n",
      "cnt: 0 - valLoss: 0.4392757713794708 - trainLoss: 0.43726658821105957\n",
      "cnt: 0 - valLoss: 0.4392746388912201 - trainLoss: 0.4372641146183014\n",
      "cnt: 0 - valLoss: 0.4392736256122589 - trainLoss: 0.4372616708278656\n",
      "cnt: 0 - valLoss: 0.4392726719379425 - trainLoss: 0.43725916743278503\n",
      "cnt: 0 - valLoss: 0.4392717480659485 - trainLoss: 0.43725672364234924\n",
      "cnt: 0 - valLoss: 0.4392707049846649 - trainLoss: 0.4372542202472687\n",
      "cnt: 0 - valLoss: 0.4392697215080261 - trainLoss: 0.4372517466545105\n",
      "cnt: 0 - valLoss: 0.43926873803138733 - trainLoss: 0.4372492730617523\n",
      "cnt: 0 - valLoss: 0.43926766514778137 - trainLoss: 0.43724679946899414\n",
      "cnt: 0 - valLoss: 0.4392666816711426 - trainLoss: 0.4372442960739136\n",
      "cnt: 0 - valLoss: 0.4392656981945038 - trainLoss: 0.4372418224811554\n",
      "cnt: 0 - valLoss: 0.439264714717865 - trainLoss: 0.4372393488883972\n",
      "cnt: 0 - valLoss: 0.4392637610435486 - trainLoss: 0.4372369050979614\n",
      "cnt: 0 - valLoss: 0.43926286697387695 - trainLoss: 0.43723440170288086\n",
      "cnt: 0 - valLoss: 0.439261794090271 - trainLoss: 0.4372319281101227\n",
      "cnt: 0 - valLoss: 0.43926092982292175 - trainLoss: 0.4372294545173645\n",
      "cnt: 0 - valLoss: 0.43925997614860535 - trainLoss: 0.4372270107269287\n",
      "cnt: 0 - valLoss: 0.43925905227661133 - trainLoss: 0.43722453713417053\n",
      "cnt: 0 - valLoss: 0.4392581582069397 - trainLoss: 0.43722203373908997\n",
      "cnt: 0 - valLoss: 0.4392572343349457 - trainLoss: 0.43721961975097656\n",
      "cnt: 0 - valLoss: 0.4392561614513397 - trainLoss: 0.437217116355896\n",
      "cnt: 0 - valLoss: 0.4392552673816681 - trainLoss: 0.4372146427631378\n",
      "cnt: 0 - valLoss: 0.4392543435096741 - trainLoss: 0.437212198972702\n",
      "cnt: 0 - valLoss: 0.43925338983535767 - trainLoss: 0.43720972537994385\n",
      "cnt: 0 - valLoss: 0.43925249576568604 - trainLoss: 0.4372072219848633\n",
      "cnt: 0 - valLoss: 0.4392516016960144 - trainLoss: 0.4372047483921051\n",
      "cnt: 0 - valLoss: 0.439250648021698 - trainLoss: 0.4372022747993469\n",
      "cnt: 0 - valLoss: 0.43924960494041443 - trainLoss: 0.43719980120658875\n",
      "cnt: 0 - valLoss: 0.4392487108707428 - trainLoss: 0.43719735741615295\n",
      "cnt: 0 - valLoss: 0.43924781680107117 - trainLoss: 0.43719491362571716\n",
      "cnt: 0 - valLoss: 0.43924686312675476 - trainLoss: 0.437192440032959\n",
      "cnt: 0 - valLoss: 0.43924593925476074 - trainLoss: 0.4371899366378784\n",
      "cnt: 0 - valLoss: 0.4392450451850891 - trainLoss: 0.43718746304512024\n",
      "cnt: 0 - valLoss: 0.43924400210380554 - trainLoss: 0.43718501925468445\n",
      "cnt: 0 - valLoss: 0.4392431080341339 - trainLoss: 0.43718254566192627\n",
      "cnt: 0 - valLoss: 0.4392421543598175 - trainLoss: 0.4371800720691681\n",
      "cnt: 0 - valLoss: 0.4392412304878235 - trainLoss: 0.4371775686740875\n",
      "cnt: 0 - valLoss: 0.4392402768135071 - trainLoss: 0.43717512488365173\n",
      "cnt: 0 - valLoss: 0.4392393231391907 - trainLoss: 0.43717265129089355\n",
      "cnt: 0 - valLoss: 0.4392383098602295 - trainLoss: 0.4371701776981354\n",
      "cnt: 0 - valLoss: 0.4392373561859131 - trainLoss: 0.4371676743030548\n",
      "cnt: 0 - valLoss: 0.4392364025115967 - trainLoss: 0.4371652603149414\n",
      "cnt: 0 - valLoss: 0.43923547863960266 - trainLoss: 0.4371628165245056\n",
      "cnt: 0 - valLoss: 0.43923452496528625 - trainLoss: 0.43716031312942505\n",
      "cnt: 0 - valLoss: 0.43923360109329224 - trainLoss: 0.43715789914131165\n",
      "cnt: 0 - valLoss: 0.43923264741897583 - trainLoss: 0.4371553659439087\n",
      "cnt: 0 - valLoss: 0.43923160433769226 - trainLoss: 0.4371529519557953\n",
      "cnt: 0 - valLoss: 0.43923068046569824 - trainLoss: 0.4371504783630371\n",
      "cnt: 0 - valLoss: 0.43922972679138184 - trainLoss: 0.43714800477027893\n",
      "cnt: 0 - valLoss: 0.43922877311706543 - trainLoss: 0.43714553117752075\n",
      "cnt: 0 - valLoss: 0.4392278492450714 - trainLoss: 0.43714308738708496\n",
      "cnt: 0 - valLoss: 0.439226895570755 - trainLoss: 0.4371405839920044\n",
      "cnt: 0 - valLoss: 0.4392258822917938 - trainLoss: 0.4371381103992462\n",
      "cnt: 0 - valLoss: 0.4392249286174774 - trainLoss: 0.4371356666088104\n",
      "cnt: 0 - valLoss: 0.439223974943161 - trainLoss: 0.43713319301605225\n",
      "cnt: 0 - valLoss: 0.439223051071167 - trainLoss: 0.43713074922561646\n",
      "cnt: 0 - valLoss: 0.4392220973968506 - trainLoss: 0.43712830543518066\n",
      "cnt: 0 - valLoss: 0.4392211437225342 - trainLoss: 0.4371258318424225\n",
      "cnt: 0 - valLoss: 0.4392200708389282 - trainLoss: 0.4371233582496643\n",
      "cnt: 0 - valLoss: 0.43921908736228943 - trainLoss: 0.43712085485458374\n",
      "cnt: 0 - valLoss: 0.4392181634902954 - trainLoss: 0.43711838126182556\n",
      "cnt: 0 - valLoss: 0.439217209815979 - trainLoss: 0.4371159076690674\n",
      "cnt: 0 - valLoss: 0.4392162561416626 - trainLoss: 0.43711337447166443\n",
      "cnt: 0 - valLoss: 0.4392153322696686 - trainLoss: 0.4371108412742615\n",
      "cnt: 0 - valLoss: 0.4392142593860626 - trainLoss: 0.4371083676815033\n",
      "cnt: 0 - valLoss: 0.439213365316391 - trainLoss: 0.4371058940887451\n",
      "cnt: 0 - valLoss: 0.43921250104904175 - trainLoss: 0.43710339069366455\n",
      "cnt: 0 - valLoss: 0.4392116367816925 - trainLoss: 0.43710091710090637\n",
      "cnt: 0 - valLoss: 0.4392107427120209 - trainLoss: 0.4370984435081482\n",
      "cnt: 0 - valLoss: 0.43920984864234924 - trainLoss: 0.4370959401130676\n",
      "cnt: 0 - valLoss: 0.439208984375 - trainLoss: 0.43709343671798706\n",
      "cnt: 0 - valLoss: 0.4392080008983612 - trainLoss: 0.4370909333229065\n",
      "cnt: 0 - valLoss: 0.4392070472240448 - trainLoss: 0.4370884299278259\n",
      "cnt: 0 - valLoss: 0.43920621275901794 - trainLoss: 0.43708598613739014\n",
      "cnt: 0 - valLoss: 0.4392052888870239 - trainLoss: 0.4370834529399872\n",
      "cnt: 0 - valLoss: 0.4392043948173523 - trainLoss: 0.437080979347229\n",
      "cnt: 0 - valLoss: 0.43920353055000305 - trainLoss: 0.43707847595214844\n",
      "cnt: 0 - valLoss: 0.43920254707336426 - trainLoss: 0.43707600235939026\n",
      "cnt: 0 - valLoss: 0.4392016530036926 - trainLoss: 0.4370735287666321\n",
      "cnt: 0 - valLoss: 0.439200758934021 - trainLoss: 0.4370710551738739\n",
      "cnt: 0 - valLoss: 0.43919986486434937 - trainLoss: 0.43706855177879333\n",
      "cnt: 0 - valLoss: 0.4391990005970001 - trainLoss: 0.43706607818603516\n",
      "cnt: 0 - valLoss: 0.4391981065273285 - trainLoss: 0.437063604593277\n",
      "cnt: 0 - valLoss: 0.4391971230506897 - trainLoss: 0.4370611011981964\n",
      "cnt: 0 - valLoss: 0.4391961991786957 - trainLoss: 0.43705862760543823\n",
      "cnt: 0 - valLoss: 0.4391953647136688 - trainLoss: 0.4370560944080353\n",
      "cnt: 0 - valLoss: 0.4391944110393524 - trainLoss: 0.4370536208152771\n",
      "cnt: 0 - valLoss: 0.4391935467720032 - trainLoss: 0.4370511770248413\n",
      "cnt: 0 - valLoss: 0.43919268250465393 - trainLoss: 0.43704861402511597\n",
      "cnt: 0 - valLoss: 0.4391917884349823 - trainLoss: 0.4370461404323578\n",
      "cnt: 0 - valLoss: 0.4391908049583435 - trainLoss: 0.437043696641922\n",
      "cnt: 0 - valLoss: 0.43918994069099426 - trainLoss: 0.43704116344451904\n",
      "cnt: 0 - valLoss: 0.43918901681900024 - trainLoss: 0.43703868985176086\n",
      "cnt: 0 - valLoss: 0.439188152551651 - trainLoss: 0.4370361864566803\n",
      "cnt: 0 - valLoss: 0.43918728828430176 - trainLoss: 0.4370337128639221\n",
      "cnt: 0 - valLoss: 0.43918636441230774 - trainLoss: 0.43703123927116394\n",
      "cnt: 0 - valLoss: 0.43918541073799133 - trainLoss: 0.43702876567840576\n",
      "cnt: 0 - valLoss: 0.4391845464706421 - trainLoss: 0.4370262622833252\n",
      "cnt: 0 - valLoss: 0.43918362259864807 - trainLoss: 0.437023788690567\n",
      "cnt: 0 - valLoss: 0.43918275833129883 - trainLoss: 0.43702125549316406\n",
      "cnt: 0 - valLoss: 0.4391818940639496 - trainLoss: 0.43701881170272827\n",
      "cnt: 0 - valLoss: 0.4391809403896332 - trainLoss: 0.4370163083076477\n",
      "cnt: 0 - valLoss: 0.4391801059246063 - trainLoss: 0.43701380491256714\n",
      "cnt: 0 - valLoss: 0.4391791522502899 - trainLoss: 0.43701133131980896\n",
      "cnt: 0 - valLoss: 0.4391782581806183 - trainLoss: 0.4370088577270508\n",
      "cnt: 0 - valLoss: 0.43917736411094666 - trainLoss: 0.4370063543319702\n",
      "cnt: 0 - valLoss: 0.4391764998435974 - trainLoss: 0.43700385093688965\n",
      "cnt: 0 - valLoss: 0.439175546169281 - trainLoss: 0.43700140714645386\n",
      "cnt: 0 - valLoss: 0.43917468190193176 - trainLoss: 0.4369989335536957\n",
      "cnt: 0 - valLoss: 0.43917369842529297 - trainLoss: 0.4369964301586151\n",
      "cnt: 0 - valLoss: 0.43917280435562134 - trainLoss: 0.43699392676353455\n",
      "cnt: 0 - valLoss: 0.4391719400882721 - trainLoss: 0.436991423368454\n",
      "cnt: 0 - valLoss: 0.4391710162162781 - trainLoss: 0.4369889497756958\n",
      "cnt: 0 - valLoss: 0.43917015194892883 - trainLoss: 0.4369864761829376\n",
      "cnt: 0 - valLoss: 0.4391692876815796 - trainLoss: 0.43698397278785706\n",
      "cnt: 0 - valLoss: 0.4391683042049408 - trainLoss: 0.4369814991950989\n",
      "cnt: 0 - valLoss: 0.43916741013526917 - trainLoss: 0.4369790256023407\n",
      "cnt: 0 - valLoss: 0.43916651606559753 - trainLoss: 0.43697652220726013\n",
      "cnt: 0 - valLoss: 0.4391656219959259 - trainLoss: 0.43697404861450195\n",
      "cnt: 0 - valLoss: 0.43916475772857666 - trainLoss: 0.4369715750217438\n",
      "cnt: 0 - valLoss: 0.43916386365890503 - trainLoss: 0.4369691014289856\n",
      "cnt: 0 - valLoss: 0.439162939786911 - trainLoss: 0.43696656823158264\n",
      "cnt: 0 - valLoss: 0.439162015914917 - trainLoss: 0.43696409463882446\n",
      "cnt: 0 - valLoss: 0.4391610622406006 - trainLoss: 0.43696165084838867\n",
      "cnt: 0 - valLoss: 0.43916019797325134 - trainLoss: 0.4369591474533081\n",
      "cnt: 0 - valLoss: 0.4391593635082245 - trainLoss: 0.43695664405822754\n",
      "cnt: 0 - valLoss: 0.43915843963623047 - trainLoss: 0.4369541108608246\n",
      "cnt: 0 - valLoss: 0.43915754556655884 - trainLoss: 0.4369516670703888\n",
      "cnt: 0 - valLoss: 0.43915653228759766 - trainLoss: 0.43694913387298584\n",
      "cnt: 0 - valLoss: 0.4391556680202484 - trainLoss: 0.43694669008255005\n",
      "cnt: 0 - valLoss: 0.43915480375289917 - trainLoss: 0.4369441866874695\n",
      "cnt: 0 - valLoss: 0.43915390968322754 - trainLoss: 0.4369417428970337\n",
      "cnt: 0 - valLoss: 0.4391530156135559 - trainLoss: 0.43693920969963074\n",
      "cnt: 0 - valLoss: 0.4391521215438843 - trainLoss: 0.43693673610687256\n",
      "cnt: 0 - valLoss: 0.43915125727653503 - trainLoss: 0.4369342625141144\n",
      "cnt: 0 - valLoss: 0.43915027379989624 - trainLoss: 0.4369317591190338\n",
      "cnt: 0 - valLoss: 0.4391493499279022 - trainLoss: 0.43692928552627563\n",
      "cnt: 0 - valLoss: 0.4391484558582306 - trainLoss: 0.43692681193351746\n",
      "cnt: 0 - valLoss: 0.43914759159088135 - trainLoss: 0.4369242787361145\n",
      "cnt: 0 - valLoss: 0.4391466975212097 - trainLoss: 0.4369218349456787\n",
      "cnt: 0 - valLoss: 0.4391458034515381 - trainLoss: 0.43691930174827576\n",
      "cnt: 0 - valLoss: 0.4391448497772217 - trainLoss: 0.43691685795783997\n",
      "cnt: 0 - valLoss: 0.43914392590522766 - trainLoss: 0.4369143545627594\n",
      "cnt: 0 - valLoss: 0.4391430616378784 - trainLoss: 0.4369119107723236\n",
      "cnt: 0 - valLoss: 0.439142107963562 - trainLoss: 0.43690934777259827\n",
      "cnt: 0 - valLoss: 0.43914127349853516 - trainLoss: 0.4369069039821625\n",
      "cnt: 0 - valLoss: 0.4391404092311859 - trainLoss: 0.4369043707847595\n",
      "cnt: 0 - valLoss: 0.4391395151615143 - trainLoss: 0.43690189719200134\n",
      "cnt: 0 - valLoss: 0.4391385316848755 - trainLoss: 0.4368993937969208\n",
      "cnt: 0 - valLoss: 0.43913766741752625 - trainLoss: 0.4368969202041626\n",
      "cnt: 0 - valLoss: 0.43913671374320984 - trainLoss: 0.4368944466114044\n",
      "cnt: 0 - valLoss: 0.4391358494758606 - trainLoss: 0.43689197301864624\n",
      "cnt: 0 - valLoss: 0.43913495540618896 - trainLoss: 0.4368894696235657\n",
      "cnt: 0 - valLoss: 0.43913406133651733 - trainLoss: 0.4368869960308075\n",
      "cnt: 0 - valLoss: 0.43913307785987854 - trainLoss: 0.4368845224380493\n",
      "cnt: 0 - valLoss: 0.4391321837902069 - trainLoss: 0.43688201904296875\n",
      "cnt: 0 - valLoss: 0.43913131952285767 - trainLoss: 0.43687954545021057\n",
      "cnt: 0 - valLoss: 0.43913042545318604 - trainLoss: 0.4368770718574524\n",
      "cnt: 0 - valLoss: 0.4391295313835144 - trainLoss: 0.4368745684623718\n",
      "cnt: 0 - valLoss: 0.43912866711616516 - trainLoss: 0.43687209486961365\n",
      "cnt: 0 - valLoss: 0.43912771344184875 - trainLoss: 0.4368695616722107\n",
      "cnt: 0 - valLoss: 0.43912678956985474 - trainLoss: 0.4368671476840973\n",
      "cnt: 0 - valLoss: 0.43912583589553833 - trainLoss: 0.43686461448669434\n",
      "cnt: 0 - valLoss: 0.4391249716281891 - trainLoss: 0.43686214089393616\n",
      "cnt: 0 - valLoss: 0.43912410736083984 - trainLoss: 0.4368596374988556\n",
      "cnt: 0 - valLoss: 0.4391231834888458 - trainLoss: 0.4368571639060974\n",
      "cnt: 0 - valLoss: 0.4391223192214966 - trainLoss: 0.43685469031333923\n",
      "cnt: 0 - valLoss: 0.439121276140213 - trainLoss: 0.43685218691825867\n",
      "cnt: 0 - valLoss: 0.43912044167518616 - trainLoss: 0.4368496835231781\n",
      "cnt: 0 - valLoss: 0.43911951780319214 - trainLoss: 0.43684718012809753\n",
      "cnt: 0 - valLoss: 0.4391186237335205 - trainLoss: 0.43684470653533936\n",
      "cnt: 0 - valLoss: 0.43911778926849365 - trainLoss: 0.4368422329425812\n",
      "cnt: 0 - valLoss: 0.43911686539649963 - trainLoss: 0.4368397295475006\n",
      "cnt: 0 - valLoss: 0.439115971326828 - trainLoss: 0.43683725595474243\n",
      "cnt: 0 - valLoss: 0.4391150176525116 - trainLoss: 0.43683478236198425\n",
      "cnt: 0 - valLoss: 0.4391140937805176 - trainLoss: 0.4368323087692261\n",
      "cnt: 0 - valLoss: 0.43911319971084595 - trainLoss: 0.4368298053741455\n",
      "cnt: 0 - valLoss: 0.43911227583885193 - trainLoss: 0.43682733178138733\n",
      "cnt: 0 - valLoss: 0.4391114413738251 - trainLoss: 0.43682485818862915\n",
      "cnt: 0 - valLoss: 0.43911057710647583 - trainLoss: 0.4368223547935486\n",
      "cnt: 0 - valLoss: 0.43910953402519226 - trainLoss: 0.436819851398468\n",
      "cnt: 0 - valLoss: 0.4391086995601654 - trainLoss: 0.43681734800338745\n",
      "cnt: 0 - valLoss: 0.4391077756881714 - trainLoss: 0.43681490421295166\n",
      "cnt: 0 - valLoss: 0.43910688161849976 - trainLoss: 0.4368124306201935\n",
      "cnt: 0 - valLoss: 0.4391059875488281 - trainLoss: 0.4368099570274353\n",
      "cnt: 0 - valLoss: 0.4391050934791565 - trainLoss: 0.43680742383003235\n",
      "cnt: 0 - valLoss: 0.43910422921180725 - trainLoss: 0.43680495023727417\n",
      "cnt: 0 - valLoss: 0.43910321593284607 - trainLoss: 0.4368024468421936\n",
      "cnt: 0 - valLoss: 0.4391023516654968 - trainLoss: 0.43679994344711304\n",
      "cnt: 0 - valLoss: 0.4391014575958252 - trainLoss: 0.43679744005203247\n",
      "cnt: 0 - valLoss: 0.4391005337238312 - trainLoss: 0.4367949664592743\n",
      "cnt: 0 - valLoss: 0.4390996992588043 - trainLoss: 0.4367925226688385\n",
      "cnt: 0 - valLoss: 0.4390987753868103 - trainLoss: 0.43679001927375793\n",
      "cnt: 0 - valLoss: 0.43909788131713867 - trainLoss: 0.43678751587867737\n",
      "cnt: 0 - valLoss: 0.4390968680381775 - trainLoss: 0.4367850422859192\n",
      "cnt: 0 - valLoss: 0.43909600377082825 - trainLoss: 0.436782568693161\n",
      "cnt: 0 - valLoss: 0.439095139503479 - trainLoss: 0.43678006529808044\n",
      "cnt: 0 - valLoss: 0.4390941858291626 - trainLoss: 0.43677759170532227\n",
      "cnt: 0 - valLoss: 0.43909329175949097 - trainLoss: 0.4367751181125641\n",
      "cnt: 0 - valLoss: 0.4390924572944641 - trainLoss: 0.4367726147174835\n",
      "cnt: 0 - valLoss: 0.43909144401550293 - trainLoss: 0.43677014112472534\n",
      "cnt: 0 - valLoss: 0.4390906095504761 - trainLoss: 0.43676766753196716\n",
      "cnt: 0 - valLoss: 0.43908965587615967 - trainLoss: 0.436765193939209\n",
      "cnt: 0 - valLoss: 0.43908876180648804 - trainLoss: 0.4367626905441284\n",
      "cnt: 0 - valLoss: 0.4390878677368164 - trainLoss: 0.43676021695137024\n",
      "cnt: 0 - valLoss: 0.43908700346946716 - trainLoss: 0.43675774335861206\n",
      "cnt: 0 - valLoss: 0.43908610939979553 - trainLoss: 0.4367552399635315\n",
      "cnt: 0 - valLoss: 0.43908512592315674 - trainLoss: 0.4367527663707733\n",
      "cnt: 0 - valLoss: 0.4390842020511627 - trainLoss: 0.43675023317337036\n",
      "cnt: 0 - valLoss: 0.4390833079814911 - trainLoss: 0.4367477595806122\n",
      "cnt: 0 - valLoss: 0.43908244371414185 - trainLoss: 0.436745285987854\n",
      "cnt: 0 - valLoss: 0.439081609249115 - trainLoss: 0.43674278259277344\n",
      "cnt: 0 - valLoss: 0.4390806555747986 - trainLoss: 0.43674027919769287\n",
      "cnt: 0 - valLoss: 0.43907976150512695 - trainLoss: 0.4367377758026123\n",
      "cnt: 0 - valLoss: 0.43907877802848816 - trainLoss: 0.4367353022098541\n",
      "cnt: 0 - valLoss: 0.43907788395881653 - trainLoss: 0.43673282861709595\n",
      "cnt: 0 - valLoss: 0.4390769600868225 - trainLoss: 0.43673035502433777\n",
      "cnt: 0 - valLoss: 0.43907612562179565 - trainLoss: 0.4367278516292572\n",
      "cnt: 0 - valLoss: 0.43907520174980164 - trainLoss: 0.436725378036499\n",
      "cnt: 0 - valLoss: 0.4390742778778076 - trainLoss: 0.43672290444374084\n",
      "cnt: 0 - valLoss: 0.4390732944011688 - trainLoss: 0.4367204010486603\n",
      "cnt: 0 - valLoss: 0.4390723705291748 - trainLoss: 0.4367179274559021\n",
      "cnt: 0 - valLoss: 0.4390714764595032 - trainLoss: 0.4367154538631439\n",
      "cnt: 0 - valLoss: 0.43907055258750916 - trainLoss: 0.43671295046806335\n",
      "cnt: 0 - valLoss: 0.43906962871551514 - trainLoss: 0.4367104768753052\n",
      "cnt: 0 - valLoss: 0.4390687346458435 - trainLoss: 0.436708003282547\n",
      "cnt: 0 - valLoss: 0.4390678107738495 - trainLoss: 0.4367055296897888\n",
      "cnt: 0 - valLoss: 0.4390667974948883 - trainLoss: 0.43670302629470825\n",
      "cnt: 0 - valLoss: 0.4390659034252167 - trainLoss: 0.4367005527019501\n",
      "cnt: 0 - valLoss: 0.43906497955322266 - trainLoss: 0.4366980791091919\n",
      "cnt: 0 - valLoss: 0.43906405568122864 - trainLoss: 0.43669557571411133\n",
      "cnt: 0 - valLoss: 0.439063161611557 - trainLoss: 0.43669313192367554\n",
      "cnt: 0 - valLoss: 0.439062237739563 - trainLoss: 0.43669062852859497\n",
      "cnt: 0 - valLoss: 0.43906131386756897 - trainLoss: 0.4366881251335144\n",
      "cnt: 0 - valLoss: 0.4390603303909302 - trainLoss: 0.4366856515407562\n",
      "cnt: 0 - valLoss: 0.43905940651893616 - trainLoss: 0.43668317794799805\n",
      "cnt: 0 - valLoss: 0.43905848264694214 - trainLoss: 0.43668070435523987\n",
      "cnt: 0 - valLoss: 0.4390575885772705 - trainLoss: 0.4366782009601593\n",
      "cnt: 0 - valLoss: 0.4390566945075989 - trainLoss: 0.4366757273674011\n",
      "cnt: 0 - valLoss: 0.43905580043792725 - trainLoss: 0.43667325377464294\n",
      "cnt: 0 - valLoss: 0.4390547573566437 - trainLoss: 0.43667080998420715\n",
      "cnt: 0 - valLoss: 0.43905386328697205 - trainLoss: 0.4366682767868042\n",
      "cnt: 0 - valLoss: 0.4390529692173004 - trainLoss: 0.4366658329963684\n",
      "cnt: 0 - valLoss: 0.4390520751476288 - trainLoss: 0.43666335940361023\n",
      "cnt: 0 - valLoss: 0.43905118107795715 - trainLoss: 0.43666088581085205\n",
      "cnt: 0 - valLoss: 0.43905025720596313 - trainLoss: 0.4366583824157715\n",
      "cnt: 0 - valLoss: 0.4390493631362915 - trainLoss: 0.4366558790206909\n",
      "cnt: 0 - valLoss: 0.4390483498573303 - trainLoss: 0.4366534352302551\n",
      "cnt: 0 - valLoss: 0.4390474259853363 - trainLoss: 0.43665093183517456\n",
      "cnt: 0 - valLoss: 0.4390465319156647 - trainLoss: 0.4366484582424164\n",
      "cnt: 0 - valLoss: 0.43904563784599304 - trainLoss: 0.4366460144519806\n",
      "cnt: 0 - valLoss: 0.4390447437763214 - trainLoss: 0.43664348125457764\n",
      "cnt: 0 - valLoss: 0.439043790102005 - trainLoss: 0.43664100766181946\n",
      "cnt: 0 - valLoss: 0.43904292583465576 - trainLoss: 0.43663856387138367\n",
      "cnt: 0 - valLoss: 0.4390419125556946 - trainLoss: 0.4366360902786255\n",
      "cnt: 0 - valLoss: 0.43904104828834534 - trainLoss: 0.4366336166858673\n",
      "cnt: 0 - valLoss: 0.43904009461402893 - trainLoss: 0.43663111329078674\n",
      "cnt: 0 - valLoss: 0.4390392005443573 - trainLoss: 0.43662863969802856\n",
      "cnt: 0 - valLoss: 0.43903830647468567 - trainLoss: 0.4366261661052704\n",
      "cnt: 0 - valLoss: 0.43903741240501404 - trainLoss: 0.4366236925125122\n",
      "cnt: 0 - valLoss: 0.43903636932373047 - trainLoss: 0.43662118911743164\n",
      "cnt: 0 - valLoss: 0.43903547525405884 - trainLoss: 0.43661871552467346\n",
      "cnt: 0 - valLoss: 0.4390345513820648 - trainLoss: 0.4366162419319153\n",
      "cnt: 0 - valLoss: 0.4390336275100708 - trainLoss: 0.4366137385368347\n",
      "cnt: 0 - valLoss: 0.43903273344039917 - trainLoss: 0.43661126494407654\n",
      "cnt: 0 - valLoss: 0.43903183937072754 - trainLoss: 0.43660879135131836\n",
      "cnt: 0 - valLoss: 0.4390309453010559 - trainLoss: 0.4366062879562378\n",
      "cnt: 0 - valLoss: 0.4390299618244171 - trainLoss: 0.4366038143634796\n",
      "cnt: 0 - valLoss: 0.4390290379524231 - trainLoss: 0.43660134077072144\n",
      "cnt: 0 - valLoss: 0.43902814388275146 - trainLoss: 0.43659886717796326\n",
      "cnt: 0 - valLoss: 0.43902724981307983 - trainLoss: 0.4365963637828827\n",
      "cnt: 0 - valLoss: 0.4390262961387634 - trainLoss: 0.4365938901901245\n",
      "cnt: 0 - valLoss: 0.4390254020690918 - trainLoss: 0.43659141659736633\n",
      "cnt: 0 - valLoss: 0.43902450799942017 - trainLoss: 0.43658891320228577\n",
      "cnt: 0 - valLoss: 0.43902352452278137 - trainLoss: 0.4365864396095276\n",
      "cnt: 0 - valLoss: 0.43902263045310974 - trainLoss: 0.4365839958190918\n",
      "cnt: 0 - valLoss: 0.4390217065811157 - trainLoss: 0.43658146262168884\n",
      "cnt: 0 - valLoss: 0.4390208125114441 - trainLoss: 0.43657898902893066\n",
      "cnt: 0 - valLoss: 0.43901991844177246 - trainLoss: 0.4365765452384949\n",
      "cnt: 0 - valLoss: 0.43901902437210083 - trainLoss: 0.4365740418434143\n",
      "cnt: 0 - valLoss: 0.4390181303024292 - trainLoss: 0.4365715980529785\n",
      "cnt: 0 - valLoss: 0.43901708722114563 - trainLoss: 0.43656906485557556\n",
      "cnt: 0 - valLoss: 0.439016193151474 - trainLoss: 0.43656662106513977\n",
      "cnt: 0 - valLoss: 0.43901532888412476 - trainLoss: 0.4365641474723816\n",
      "cnt: 0 - valLoss: 0.43901440501213074 - trainLoss: 0.4365616738796234\n",
      "cnt: 0 - valLoss: 0.4390134811401367 - trainLoss: 0.43655917048454285\n",
      "cnt: 0 - valLoss: 0.4390125870704651 - trainLoss: 0.43655672669410706\n",
      "cnt: 0 - valLoss: 0.4390116035938263 - trainLoss: 0.4365542531013489\n",
      "cnt: 0 - valLoss: 0.43901070952415466 - trainLoss: 0.4365517795085907\n",
      "cnt: 0 - valLoss: 0.43900981545448303 - trainLoss: 0.43654927611351013\n",
      "cnt: 0 - valLoss: 0.439008891582489 - trainLoss: 0.43654680252075195\n",
      "cnt: 0 - valLoss: 0.439007967710495 - trainLoss: 0.4365443289279938\n",
      "cnt: 0 - valLoss: 0.43900707364082336 - trainLoss: 0.4365418553352356\n",
      "cnt: 0 - valLoss: 0.4390062093734741 - trainLoss: 0.43653935194015503\n",
      "cnt: 0 - valLoss: 0.4390052258968353 - trainLoss: 0.43653687834739685\n",
      "cnt: 0 - valLoss: 0.4390043318271637 - trainLoss: 0.43653440475463867\n",
      "cnt: 0 - valLoss: 0.4390034079551697 - trainLoss: 0.4365319013595581\n",
      "cnt: 0 - valLoss: 0.43900248408317566 - trainLoss: 0.4365294277667999\n",
      "cnt: 0 - valLoss: 0.4390016198158264 - trainLoss: 0.43652695417404175\n",
      "cnt: 0 - valLoss: 0.43900066614151 - trainLoss: 0.4365244507789612\n",
      "cnt: 0 - valLoss: 0.43899983167648315 - trainLoss: 0.436521977186203\n",
      "cnt: 0 - valLoss: 0.4389987885951996 - trainLoss: 0.4365195035934448\n",
      "cnt: 0 - valLoss: 0.43899789452552795 - trainLoss: 0.43651703000068665\n",
      "cnt: 0 - valLoss: 0.4389970004558563 - trainLoss: 0.43651458621025085\n",
      "cnt: 0 - valLoss: 0.4389960765838623 - trainLoss: 0.4365120828151703\n",
      "cnt: 0 - valLoss: 0.4389951825141907 - trainLoss: 0.4365096688270569\n",
      "cnt: 0 - valLoss: 0.43899425864219666 - trainLoss: 0.4365071654319763\n",
      "cnt: 0 - valLoss: 0.4389933943748474 - trainLoss: 0.43650469183921814\n",
      "cnt: 0 - valLoss: 0.43899235129356384 - trainLoss: 0.43650221824645996\n",
      "cnt: 0 - valLoss: 0.438991516828537 - trainLoss: 0.4364997148513794\n",
      "cnt: 0 - valLoss: 0.4389905631542206 - trainLoss: 0.4364972412586212\n",
      "cnt: 0 - valLoss: 0.43898966908454895 - trainLoss: 0.43649476766586304\n",
      "cnt: 0 - valLoss: 0.4389887750148773 - trainLoss: 0.43649226427078247\n",
      "cnt: 0 - valLoss: 0.4389878511428833 - trainLoss: 0.4364897906780243\n",
      "cnt: 0 - valLoss: 0.43898701667785645 - trainLoss: 0.4364873170852661\n",
      "cnt: 0 - valLoss: 0.43898600339889526 - trainLoss: 0.43648484349250793\n",
      "cnt: 0 - valLoss: 0.43898507952690125 - trainLoss: 0.43648234009742737\n",
      "cnt: 0 - valLoss: 0.438984215259552 - trainLoss: 0.4364798665046692\n",
      "cnt: 0 - valLoss: 0.43898332118988037 - trainLoss: 0.436477392911911\n",
      "cnt: 0 - valLoss: 0.43898245692253113 - trainLoss: 0.43647488951683044\n",
      "cnt: 0 - valLoss: 0.4389815628528595 - trainLoss: 0.43647241592407227\n",
      "cnt: 0 - valLoss: 0.4389805793762207 - trainLoss: 0.4364699423313141\n",
      "cnt: 0 - valLoss: 0.4389796853065491 - trainLoss: 0.4364674985408783\n",
      "cnt: 0 - valLoss: 0.43897876143455505 - trainLoss: 0.4364650249481201\n",
      "cnt: 0 - valLoss: 0.4389778673648834 - trainLoss: 0.43646249175071716\n",
      "cnt: 0 - valLoss: 0.4389770030975342 - trainLoss: 0.436460018157959\n",
      "cnt: 0 - valLoss: 0.43897607922554016 - trainLoss: 0.4364575147628784\n",
      "cnt: 0 - valLoss: 0.4389752149581909 - trainLoss: 0.4364550709724426\n",
      "cnt: 0 - valLoss: 0.43897420167922974 - trainLoss: 0.4364526569843292\n",
      "cnt: 0 - valLoss: 0.4389732778072357 - trainLoss: 0.43645015358924866\n",
      "cnt: 0 - valLoss: 0.4389723539352417 - trainLoss: 0.4364476799964905\n",
      "cnt: 0 - valLoss: 0.4389714300632477 - trainLoss: 0.4364452064037323\n",
      "cnt: 0 - valLoss: 0.43897053599357605 - trainLoss: 0.43644270300865173\n",
      "cnt: 0 - valLoss: 0.4389696419239044 - trainLoss: 0.43644022941589355\n",
      "cnt: 0 - valLoss: 0.4389687478542328 - trainLoss: 0.4364377558231354\n",
      "cnt: 0 - valLoss: 0.438967764377594 - trainLoss: 0.4364352524280548\n",
      "cnt: 0 - valLoss: 0.43896687030792236 - trainLoss: 0.43643277883529663\n",
      "cnt: 0 - valLoss: 0.43896588683128357 - trainLoss: 0.43643030524253845\n",
      "cnt: 0 - valLoss: 0.4389650225639343 - trainLoss: 0.4364278316497803\n",
      "cnt: 0 - valLoss: 0.4389640688896179 - trainLoss: 0.4364253878593445\n",
      "cnt: 0 - valLoss: 0.4389631748199463 - trainLoss: 0.4364229440689087\n",
      "cnt: 0 - valLoss: 0.43896228075027466 - trainLoss: 0.4364204704761505\n",
      "cnt: 0 - valLoss: 0.43896129727363586 - trainLoss: 0.43641796708106995\n",
      "cnt: 0 - valLoss: 0.43896034359931946 - trainLoss: 0.43641549348831177\n",
      "cnt: 0 - valLoss: 0.4389594495296478 - trainLoss: 0.4364130198955536\n",
      "cnt: 0 - valLoss: 0.4389585554599762 - trainLoss: 0.436410516500473\n",
      "cnt: 0 - valLoss: 0.4389576315879822 - trainLoss: 0.43640804290771484\n",
      "cnt: 0 - valLoss: 0.43895670771598816 - trainLoss: 0.43640556931495667\n",
      "cnt: 0 - valLoss: 0.4389558434486389 - trainLoss: 0.4364030659198761\n",
      "cnt: 0 - valLoss: 0.43895480036735535 - trainLoss: 0.4364005923271179\n",
      "cnt: 0 - valLoss: 0.43895387649536133 - trainLoss: 0.43639814853668213\n",
      "cnt: 0 - valLoss: 0.4389529824256897 - trainLoss: 0.43639564514160156\n",
      "cnt: 0 - valLoss: 0.4389520585536957 - trainLoss: 0.43639320135116577\n",
      "cnt: 0 - valLoss: 0.4389512240886688 - trainLoss: 0.4363906979560852\n",
      "cnt: 0 - valLoss: 0.4389502704143524 - trainLoss: 0.43638819456100464\n",
      "cnt: 0 - valLoss: 0.4389493763446808 - trainLoss: 0.43638575077056885\n",
      "cnt: 0 - valLoss: 0.4389483332633972 - trainLoss: 0.4363832473754883\n",
      "cnt: 0 - valLoss: 0.4389474391937256 - trainLoss: 0.4363808333873749\n",
      "cnt: 0 - valLoss: 0.4389464855194092 - trainLoss: 0.4363783299922943\n",
      "cnt: 0 - valLoss: 0.4389456510543823 - trainLoss: 0.43637585639953613\n",
      "cnt: 0 - valLoss: 0.4389447271823883 - trainLoss: 0.43637338280677795\n",
      "cnt: 0 - valLoss: 0.4389438331127167 - trainLoss: 0.4363708794116974\n",
      "cnt: 0 - valLoss: 0.4389428198337555 - trainLoss: 0.4363684058189392\n",
      "cnt: 0 - valLoss: 0.43894195556640625 - trainLoss: 0.43636593222618103\n",
      "cnt: 0 - valLoss: 0.43894100189208984 - trainLoss: 0.43636348843574524\n",
      "cnt: 0 - valLoss: 0.4389401078224182 - trainLoss: 0.4363609552383423\n",
      "cnt: 0 - valLoss: 0.4389392137527466 - trainLoss: 0.43635857105255127\n",
      "cnt: 0 - valLoss: 0.4389382600784302 - trainLoss: 0.4363560378551483\n",
      "cnt: 0 - valLoss: 0.4389374256134033 - trainLoss: 0.43635356426239014\n",
      "cnt: 0 - valLoss: 0.43893641233444214 - trainLoss: 0.43635106086730957\n",
      "cnt: 0 - valLoss: 0.4389355182647705 - trainLoss: 0.43634864687919617\n",
      "cnt: 0 - valLoss: 0.4389345943927765 - trainLoss: 0.4363461434841156\n",
      "cnt: 0 - valLoss: 0.43893373012542725 - trainLoss: 0.4363436698913574\n",
      "cnt: 0 - valLoss: 0.43893277645111084 - trainLoss: 0.43634119629859924\n",
      "cnt: 0 - valLoss: 0.4389319121837616 - trainLoss: 0.4363386929035187\n",
      "cnt: 0 - valLoss: 0.4389309883117676 - trainLoss: 0.4363362193107605\n",
      "cnt: 0 - valLoss: 0.438929945230484 - trainLoss: 0.4363337457180023\n",
      "cnt: 0 - valLoss: 0.43892911076545715 - trainLoss: 0.43633130192756653\n",
      "cnt: 0 - valLoss: 0.43892818689346313 - trainLoss: 0.43632882833480835\n",
      "cnt: 0 - valLoss: 0.4389272630214691 - trainLoss: 0.43632638454437256\n",
      "cnt: 0 - valLoss: 0.4389263987541199 - trainLoss: 0.4363239109516144\n",
      "cnt: 0 - valLoss: 0.43892544507980347 - trainLoss: 0.4363214075565338\n",
      "cnt: 0 - valLoss: 0.43892455101013184 - trainLoss: 0.43631893396377563\n",
      "cnt: 0 - valLoss: 0.43892356753349304 - trainLoss: 0.43631646037101746\n",
      "cnt: 0 - valLoss: 0.4389226734638214 - trainLoss: 0.4363139569759369\n",
      "cnt: 0 - valLoss: 0.4389217793941498 - trainLoss: 0.4363114833831787\n",
      "cnt: 0 - valLoss: 0.43892085552215576 - trainLoss: 0.43630900979042053\n",
      "cnt: 0 - valLoss: 0.43891996145248413 - trainLoss: 0.43630656599998474\n",
      "cnt: 0 - valLoss: 0.4389190673828125 - trainLoss: 0.43630409240722656\n",
      "cnt: 0 - valLoss: 0.4389181137084961 - trainLoss: 0.4363015592098236\n",
      "cnt: 0 - valLoss: 0.4389171004295349 - trainLoss: 0.4362991452217102\n",
      "cnt: 0 - valLoss: 0.43891623616218567 - trainLoss: 0.436296671628952\n",
      "cnt: 0 - valLoss: 0.4389152526855469 - trainLoss: 0.43629419803619385\n",
      "cnt: 0 - valLoss: 0.43891429901123047 - trainLoss: 0.4362916946411133\n",
      "cnt: 0 - valLoss: 0.4389133155345917 - trainLoss: 0.4362892210483551\n",
      "cnt: 0 - valLoss: 0.43891239166259766 - trainLoss: 0.4362867772579193\n",
      "cnt: 0 - valLoss: 0.43891143798828125 - trainLoss: 0.43628430366516113\n",
      "cnt: 0 - valLoss: 0.4389103949069977 - trainLoss: 0.43628185987472534\n",
      "cnt: 0 - valLoss: 0.43890947103500366 - trainLoss: 0.43627938628196716\n",
      "cnt: 0 - valLoss: 0.43890851736068726 - trainLoss: 0.436276912689209\n",
      "cnt: 0 - valLoss: 0.43890753388404846 - trainLoss: 0.4362744092941284\n",
      "cnt: 0 - valLoss: 0.43890658020973206 - trainLoss: 0.43627193570137024\n",
      "cnt: 0 - valLoss: 0.4389055371284485 - trainLoss: 0.43626946210861206\n",
      "cnt: 0 - valLoss: 0.4389045238494873 - trainLoss: 0.43626701831817627\n",
      "cnt: 0 - valLoss: 0.4389035999774933 - trainLoss: 0.4362645447254181\n",
      "cnt: 0 - valLoss: 0.4389026463031769 - trainLoss: 0.4362621009349823\n",
      "cnt: 0 - valLoss: 0.4389016628265381 - trainLoss: 0.43625959753990173\n",
      "cnt: 0 - valLoss: 0.4389007091522217 - trainLoss: 0.43625712394714355\n",
      "cnt: 0 - valLoss: 0.43889978528022766 - trainLoss: 0.4362546503543854\n",
      "cnt: 0 - valLoss: 0.43889880180358887 - trainLoss: 0.4362521469593048\n",
      "cnt: 0 - valLoss: 0.43889784812927246 - trainLoss: 0.4362497329711914\n",
      "cnt: 0 - valLoss: 0.43889686465263367 - trainLoss: 0.43624722957611084\n",
      "cnt: 0 - valLoss: 0.43889591097831726 - trainLoss: 0.43624475598335266\n",
      "cnt: 0 - valLoss: 0.43889498710632324 - trainLoss: 0.43624231219291687\n",
      "cnt: 0 - valLoss: 0.4388938546180725 - trainLoss: 0.4362398386001587\n",
      "cnt: 0 - valLoss: 0.4388929307460785 - trainLoss: 0.4362373650074005\n",
      "cnt: 0 - valLoss: 0.4388919770717621 - trainLoss: 0.43623486161231995\n",
      "cnt: 0 - valLoss: 0.4388910233974457 - trainLoss: 0.43623241782188416\n",
      "cnt: 0 - valLoss: 0.4388900101184845 - trainLoss: 0.436229944229126\n",
      "cnt: 0 - valLoss: 0.43888911604881287 - trainLoss: 0.4362275004386902\n",
      "cnt: 0 - valLoss: 0.4388881325721741 - trainLoss: 0.436225026845932\n",
      "cnt: 0 - valLoss: 0.43888717889785767 - trainLoss: 0.43622255325317383\n",
      "cnt: 0 - valLoss: 0.43888622522354126 - trainLoss: 0.43622004985809326\n",
      "cnt: 0 - valLoss: 0.43888524174690247 - trainLoss: 0.4362175762653351\n",
      "cnt: 0 - valLoss: 0.43888431787490845 - trainLoss: 0.4362151324748993\n",
      "cnt: 0 - valLoss: 0.43888336420059204 - trainLoss: 0.4362126588821411\n",
      "cnt: 0 - valLoss: 0.4388822913169861 - trainLoss: 0.43621018528938293\n",
      "cnt: 0 - valLoss: 0.4388813376426697 - trainLoss: 0.43620774149894714\n",
      "cnt: 0 - valLoss: 0.4388803541660309 - trainLoss: 0.4362052381038666\n",
      "cnt: 0 - valLoss: 0.43887946009635925 - trainLoss: 0.4362027645111084\n",
      "cnt: 0 - valLoss: 0.43887844681739807 - trainLoss: 0.4362003207206726\n",
      "cnt: 0 - valLoss: 0.43887749314308167 - trainLoss: 0.43619781732559204\n",
      "cnt: 0 - valLoss: 0.43887653946876526 - trainLoss: 0.4361953139305115\n",
      "cnt: 0 - valLoss: 0.43887561559677124 - trainLoss: 0.4361928701400757\n",
      "cnt: 0 - valLoss: 0.43887466192245483 - trainLoss: 0.4361903965473175\n",
      "cnt: 0 - valLoss: 0.43887367844581604 - trainLoss: 0.4361879527568817\n",
      "cnt: 0 - valLoss: 0.43887272477149963 - trainLoss: 0.43618547916412354\n",
      "cnt: 0 - valLoss: 0.43887174129486084 - trainLoss: 0.43618300557136536\n",
      "cnt: 0 - valLoss: 0.4388708174228668 - trainLoss: 0.4361805021762848\n",
      "cnt: 0 - valLoss: 0.43886974453926086 - trainLoss: 0.436178058385849\n",
      "cnt: 0 - valLoss: 0.43886879086494446 - trainLoss: 0.4361756443977356\n",
      "cnt: 0 - valLoss: 0.43886783719062805 - trainLoss: 0.43617311120033264\n",
      "cnt: 0 - valLoss: 0.43886691331863403 - trainLoss: 0.43617066740989685\n",
      "cnt: 0 - valLoss: 0.4388659596443176 - trainLoss: 0.43616819381713867\n",
      "cnt: 0 - valLoss: 0.4388652443885803 - trainLoss: 0.4361656904220581\n",
      "cnt: 0 - valLoss: 0.4388645589351654 - trainLoss: 0.43616312742233276\n",
      "cnt: 0 - valLoss: 0.4388638734817505 - trainLoss: 0.4361605644226074\n",
      "cnt: 0 - valLoss: 0.43886321783065796 - trainLoss: 0.43615809082984924\n",
      "cnt: 0 - valLoss: 0.43886250257492065 - trainLoss: 0.4361554682254791\n",
      "cnt: 0 - valLoss: 0.43886181712150574 - trainLoss: 0.43615293502807617\n",
      "cnt: 0 - valLoss: 0.4388611316680908 - trainLoss: 0.4361504316329956\n",
      "cnt: 0 - valLoss: 0.43886029720306396 - trainLoss: 0.4361478388309479\n",
      "cnt: 0 - valLoss: 0.43885961174964905 - trainLoss: 0.4361453354358673\n",
      "cnt: 0 - valLoss: 0.43885892629623413 - trainLoss: 0.43614277243614197\n",
      "cnt: 0 - valLoss: 0.4388582110404968 - trainLoss: 0.43614017963409424\n",
      "cnt: 0 - valLoss: 0.4388575255870819 - trainLoss: 0.43613767623901367\n",
      "cnt: 0 - valLoss: 0.438856840133667 - trainLoss: 0.4361351430416107\n",
      "cnt: 0 - valLoss: 0.4388561546802521 - trainLoss: 0.4361325800418854\n",
      "cnt: 0 - valLoss: 0.4388554096221924 - trainLoss: 0.43613001704216003\n",
      "cnt: 0 - valLoss: 0.4388546943664551 - trainLoss: 0.4361274838447571\n",
      "cnt: 0 - valLoss: 0.43885400891304016 - trainLoss: 0.43612492084503174\n",
      "cnt: 0 - valLoss: 0.43885332345962524 - trainLoss: 0.4361223578453064\n",
      "cnt: 0 - valLoss: 0.43885257840156555 - trainLoss: 0.43611982464790344\n",
      "cnt: 0 - valLoss: 0.43885186314582825 - trainLoss: 0.4361172616481781\n",
      "cnt: 0 - valLoss: 0.43885108828544617 - trainLoss: 0.43611466884613037\n",
      "cnt: 0 - valLoss: 0.43885037302970886 - trainLoss: 0.4361121952533722\n",
      "cnt: 0 - valLoss: 0.43884962797164917 - trainLoss: 0.43610963225364685\n",
      "cnt: 0 - valLoss: 0.43884891271591187 - trainLoss: 0.4361070692539215\n",
      "cnt: 0 - valLoss: 0.4388481676578522 - trainLoss: 0.43610450625419617\n",
      "cnt: 0 - valLoss: 0.43884745240211487 - trainLoss: 0.4361020028591156\n",
      "cnt: 0 - valLoss: 0.4388466775417328 - trainLoss: 0.43609943985939026\n",
      "cnt: 0 - valLoss: 0.4388459324836731 - trainLoss: 0.4360969066619873\n",
      "cnt: 0 - valLoss: 0.4388451874256134 - trainLoss: 0.43609434366226196\n",
      "cnt: 0 - valLoss: 0.4388444423675537 - trainLoss: 0.4360918402671814\n",
      "cnt: 0 - valLoss: 0.43884363770484924 - trainLoss: 0.43608924746513367\n",
      "cnt: 0 - valLoss: 0.4388429522514343 - trainLoss: 0.4360867440700531\n",
      "cnt: 0 - valLoss: 0.43884217739105225 - trainLoss: 0.43608421087265015\n",
      "cnt: 0 - valLoss: 0.43884140253067017 - trainLoss: 0.4360816478729248\n",
      "cnt: 0 - valLoss: 0.4388405382633209 - trainLoss: 0.43607908487319946\n",
      "cnt: 0 - valLoss: 0.43883976340293884 - trainLoss: 0.4360765516757965\n",
      "cnt: 0 - valLoss: 0.43883898854255676 - trainLoss: 0.43607401847839355\n",
      "cnt: 0 - valLoss: 0.43883827328681946 - trainLoss: 0.4360714852809906\n",
      "cnt: 0 - valLoss: 0.4388374984264374 - trainLoss: 0.43606892228126526\n",
      "cnt: 0 - valLoss: 0.4388367235660553 - trainLoss: 0.4360663890838623\n",
      "cnt: 0 - valLoss: 0.438836008310318 - trainLoss: 0.43606382608413696\n",
      "cnt: 0 - valLoss: 0.4388352334499359 - trainLoss: 0.4360612630844116\n",
      "cnt: 0 - valLoss: 0.43883445858955383 - trainLoss: 0.43605878949165344\n",
      "cnt: 0 - valLoss: 0.43883368372917175 - trainLoss: 0.4360562264919281\n",
      "cnt: 0 - valLoss: 0.4388328790664673 - trainLoss: 0.43605363368988037\n",
      "cnt: 0 - valLoss: 0.4388321340084076 - trainLoss: 0.4360511302947998\n",
      "cnt: 0 - valLoss: 0.4388313293457031 - trainLoss: 0.43604859709739685\n",
      "cnt: 0 - valLoss: 0.43883055448532104 - trainLoss: 0.4360460638999939\n",
      "cnt: 0 - valLoss: 0.43882977962493896 - trainLoss: 0.43604353070259094\n",
      "cnt: 0 - valLoss: 0.4388289749622345 - trainLoss: 0.4360409677028656\n",
      "cnt: 0 - valLoss: 0.4388282001018524 - trainLoss: 0.43603840470314026\n",
      "cnt: 0 - valLoss: 0.43882742524147034 - trainLoss: 0.4360358715057373\n",
      "cnt: 0 - valLoss: 0.43882665038108826 - trainLoss: 0.43603336811065674\n",
      "cnt: 0 - valLoss: 0.4388258755207062 - trainLoss: 0.4360308051109314\n",
      "cnt: 0 - valLoss: 0.4388251006603241 - trainLoss: 0.43602827191352844\n",
      "cnt: 0 - valLoss: 0.4388243556022644 - trainLoss: 0.4360257387161255\n",
      "cnt: 0 - valLoss: 0.4388234317302704 - trainLoss: 0.43602317571640015\n",
      "cnt: 0 - valLoss: 0.4388226270675659 - trainLoss: 0.4360206425189972\n",
      "cnt: 0 - valLoss: 0.43882185220718384 - trainLoss: 0.43601810932159424\n",
      "cnt: 0 - valLoss: 0.43882107734680176 - trainLoss: 0.4360155463218689\n",
      "cnt: 0 - valLoss: 0.4388203024864197 - trainLoss: 0.43601301312446594\n",
      "cnt: 0 - valLoss: 0.4388195276260376 - trainLoss: 0.4360105097293854\n",
      "cnt: 0 - valLoss: 0.43881869316101074 - trainLoss: 0.43600794672966003\n",
      "cnt: 0 - valLoss: 0.43881791830062866 - trainLoss: 0.4360053539276123\n",
      "cnt: 0 - valLoss: 0.4388170540332794 - trainLoss: 0.43600285053253174\n",
      "cnt: 0 - valLoss: 0.4388161897659302 - trainLoss: 0.43600040674209595\n",
      "cnt: 0 - valLoss: 0.4388154149055481 - trainLoss: 0.4359978437423706\n",
      "cnt: 0 - valLoss: 0.43881458044052124 - trainLoss: 0.4359952509403229\n",
      "cnt: 0 - valLoss: 0.438813716173172 - trainLoss: 0.4359927475452423\n",
      "cnt: 0 - valLoss: 0.43881285190582275 - trainLoss: 0.43599018454551697\n",
      "cnt: 0 - valLoss: 0.4388119876384735 - trainLoss: 0.435987651348114\n",
      "cnt: 0 - valLoss: 0.43881118297576904 - trainLoss: 0.43598511815071106\n",
      "cnt: 0 - valLoss: 0.4388103187084198 - trainLoss: 0.4359825849533081\n",
      "cnt: 0 - valLoss: 0.43880951404571533 - trainLoss: 0.43598002195358276\n",
      "cnt: 0 - valLoss: 0.4388086497783661 - trainLoss: 0.4359775483608246\n",
      "cnt: 0 - valLoss: 0.43880778551101685 - trainLoss: 0.43597498536109924\n",
      "cnt: 0 - valLoss: 0.43880695104599 - trainLoss: 0.4359724521636963\n",
      "cnt: 0 - valLoss: 0.43880608677864075 - trainLoss: 0.43596991896629333\n",
      "cnt: 0 - valLoss: 0.4388052225112915 - trainLoss: 0.435967355966568\n",
      "cnt: 0 - valLoss: 0.43880435824394226 - trainLoss: 0.43596482276916504\n",
      "cnt: 0 - valLoss: 0.4388035535812378 - trainLoss: 0.4359623193740845\n",
      "cnt: 0 - valLoss: 0.43880268931388855 - trainLoss: 0.43595972657203674\n",
      "cnt: 0 - valLoss: 0.4388017952442169 - trainLoss: 0.4359572231769562\n",
      "cnt: 0 - valLoss: 0.43880099058151245 - trainLoss: 0.4359546899795532\n",
      "cnt: 0 - valLoss: 0.4388001561164856 - trainLoss: 0.43595215678215027\n",
      "cnt: 0 - valLoss: 0.4387992322444916 - trainLoss: 0.4359496533870697\n",
      "cnt: 0 - valLoss: 0.4387983977794647 - trainLoss: 0.435947060585022\n",
      "cnt: 0 - valLoss: 0.43879756331443787 - trainLoss: 0.4359445571899414\n",
      "cnt: 0 - valLoss: 0.43879663944244385 - trainLoss: 0.43594202399253845\n",
      "cnt: 0 - valLoss: 0.4387958347797394 - trainLoss: 0.4359394609928131\n",
      "cnt: 0 - valLoss: 0.43879494071006775 - trainLoss: 0.43593692779541016\n",
      "cnt: 0 - valLoss: 0.4387940764427185 - trainLoss: 0.4359343945980072\n",
      "cnt: 0 - valLoss: 0.43879327178001404 - trainLoss: 0.43593183159828186\n",
      "cnt: 0 - valLoss: 0.4387924075126648 - trainLoss: 0.4359293580055237\n",
      "cnt: 0 - valLoss: 0.43879154324531555 - trainLoss: 0.43592679500579834\n",
      "cnt: 0 - valLoss: 0.4387906491756439 - trainLoss: 0.4359242618083954\n",
      "cnt: 0 - valLoss: 0.4387897849082947 - trainLoss: 0.43592172861099243\n",
      "cnt: 0 - valLoss: 0.43878889083862305 - trainLoss: 0.4359191656112671\n",
      "cnt: 0 - valLoss: 0.4387880861759186 - trainLoss: 0.43591663241386414\n",
      "cnt: 0 - valLoss: 0.43878722190856934 - trainLoss: 0.4359140992164612\n",
      "cnt: 0 - valLoss: 0.4387863576412201 - trainLoss: 0.43591153621673584\n",
      "cnt: 0 - valLoss: 0.43878546357154846 - trainLoss: 0.43590906262397766\n",
      "cnt: 0 - valLoss: 0.43878456950187683 - trainLoss: 0.4359064996242523\n",
      "cnt: 0 - valLoss: 0.4387837052345276 - trainLoss: 0.435903936624527\n",
      "cnt: 0 - valLoss: 0.43878284096717834 - trainLoss: 0.4359014332294464\n",
      "cnt: 0 - valLoss: 0.4387820065021515 - trainLoss: 0.43589887022972107\n",
      "cnt: 0 - valLoss: 0.43878114223480225 - trainLoss: 0.4358963072299957\n",
      "cnt: 0 - valLoss: 0.4387802481651306 - trainLoss: 0.43589383363723755\n",
      "cnt: 0 - valLoss: 0.43877938389778137 - trainLoss: 0.4358912706375122\n",
      "cnt: 0 - valLoss: 0.4387785792350769 - trainLoss: 0.43588873744010925\n",
      "cnt: 0 - valLoss: 0.4387776255607605 - trainLoss: 0.4358862042427063\n",
      "cnt: 0 - valLoss: 0.43877676129341125 - trainLoss: 0.43588367104530334\n",
      "cnt: 0 - valLoss: 0.438775897026062 - trainLoss: 0.435881108045578\n",
      "cnt: 0 - valLoss: 0.4387750029563904 - trainLoss: 0.43587857484817505\n",
      "cnt: 0 - valLoss: 0.43877410888671875 - trainLoss: 0.43587610125541687\n",
      "cnt: 0 - valLoss: 0.4387732446193695 - trainLoss: 0.43587353825569153\n",
      "cnt: 0 - valLoss: 0.43877238035202026 - trainLoss: 0.4358709752559662\n",
      "cnt: 0 - valLoss: 0.4387715458869934 - trainLoss: 0.4358684718608856\n",
      "cnt: 0 - valLoss: 0.4387706518173218 - trainLoss: 0.4358659088611603\n",
      "cnt: 0 - valLoss: 0.43876978754997253 - trainLoss: 0.4358634054660797\n",
      "cnt: 0 - valLoss: 0.4387688934803009 - trainLoss: 0.43586087226867676\n",
      "cnt: 0 - valLoss: 0.43876802921295166 - trainLoss: 0.4358583092689514\n",
      "cnt: 0 - valLoss: 0.43876713514328003 - trainLoss: 0.43585577607154846\n",
      "cnt: 0 - valLoss: 0.438766211271286 - trainLoss: 0.4358532428741455\n",
      "cnt: 0 - valLoss: 0.43876537680625916 - trainLoss: 0.43585067987442017\n",
      "cnt: 0 - valLoss: 0.4387645125389099 - trainLoss: 0.435848206281662\n",
      "cnt: 0 - valLoss: 0.4387635588645935 - trainLoss: 0.43584564328193665\n",
      "cnt: 0 - valLoss: 0.43876269459724426 - trainLoss: 0.4358431100845337\n",
      "cnt: 0 - valLoss: 0.4387618601322174 - trainLoss: 0.43584057688713074\n",
      "cnt: 0 - valLoss: 0.43876099586486816 - trainLoss: 0.4358380138874054\n",
      "cnt: 0 - valLoss: 0.43876010179519653 - trainLoss: 0.4358355402946472\n",
      "cnt: 0 - valLoss: 0.4387592077255249 - trainLoss: 0.4358329474925995\n",
      "cnt: 0 - valLoss: 0.43875834345817566 - trainLoss: 0.4358304440975189\n",
      "cnt: 0 - valLoss: 0.43875738978385925 - trainLoss: 0.43582791090011597\n",
      "cnt: 0 - valLoss: 0.43875652551651 - trainLoss: 0.4358253479003906\n",
      "cnt: 0 - valLoss: 0.43875569105148315 - trainLoss: 0.43582287430763245\n",
      "cnt: 0 - valLoss: 0.43875476717948914 - trainLoss: 0.4358202815055847\n",
      "cnt: 0 - valLoss: 0.4387538731098175 - trainLoss: 0.43581777811050415\n",
      "cnt: 0 - valLoss: 0.4387529790401459 - trainLoss: 0.4358152449131012\n",
      "cnt: 0 - valLoss: 0.43875208497047424 - trainLoss: 0.43581268191337585\n",
      "cnt: 0 - valLoss: 0.438751220703125 - trainLoss: 0.4358101785182953\n",
      "cnt: 0 - valLoss: 0.43875032663345337 - trainLoss: 0.43580761551856995\n",
      "cnt: 0 - valLoss: 0.43874943256378174 - trainLoss: 0.4358050525188446\n",
      "cnt: 0 - valLoss: 0.4387485682964325 - trainLoss: 0.4358025789260864\n",
      "cnt: 0 - valLoss: 0.43874767422676086 - trainLoss: 0.43580004572868347\n",
      "cnt: 0 - valLoss: 0.4387468099594116 - trainLoss: 0.4357975125312805\n",
      "cnt: 0 - valLoss: 0.43874591588974 - trainLoss: 0.4357949495315552\n",
      "cnt: 0 - valLoss: 0.43874505162239075 - trainLoss: 0.4357924163341522\n",
      "cnt: 0 - valLoss: 0.4387441575527191 - trainLoss: 0.43578991293907166\n",
      "cnt: 0 - valLoss: 0.4387432336807251 - trainLoss: 0.4357873201370239\n",
      "cnt: 0 - valLoss: 0.43874239921569824 - trainLoss: 0.43578481674194336\n",
      "cnt: 0 - valLoss: 0.43874144554138184 - trainLoss: 0.4357822835445404\n",
      "cnt: 0 - valLoss: 0.4387405812740326 - trainLoss: 0.43577972054481506\n",
      "cnt: 0 - valLoss: 0.43873968720436096 - trainLoss: 0.4357772469520569\n",
      "cnt: 0 - valLoss: 0.4387388229370117 - trainLoss: 0.43577465415000916\n",
      "cnt: 0 - valLoss: 0.4387379288673401 - trainLoss: 0.4357721507549286\n",
      "cnt: 0 - valLoss: 0.43873706459999084 - trainLoss: 0.43576961755752563\n",
      "cnt: 0 - valLoss: 0.4387361407279968 - trainLoss: 0.4357670545578003\n",
      "cnt: 0 - valLoss: 0.4387352764606476 - trainLoss: 0.43576452136039734\n",
      "cnt: 0 - valLoss: 0.43873441219329834 - trainLoss: 0.4357619881629944\n",
      "cnt: 0 - valLoss: 0.43873342871665955 - trainLoss: 0.4357594847679138\n",
      "cnt: 0 - valLoss: 0.4387325048446655 - trainLoss: 0.43575695157051086\n",
      "cnt: 0 - valLoss: 0.4387315809726715 - trainLoss: 0.4357544481754303\n",
      "cnt: 0 - valLoss: 0.4387305974960327 - trainLoss: 0.43575188517570496\n",
      "cnt: 0 - valLoss: 0.4387297034263611 - trainLoss: 0.4357493221759796\n",
      "cnt: 0 - valLoss: 0.4387287497520447 - trainLoss: 0.43574684858322144\n",
      "cnt: 0 - valLoss: 0.43872782588005066 - trainLoss: 0.4357442855834961\n",
      "cnt: 0 - valLoss: 0.43872687220573425 - trainLoss: 0.4357417821884155\n",
      "cnt: 0 - valLoss: 0.43872591853141785 - trainLoss: 0.4357392191886902\n",
      "cnt: 0 - valLoss: 0.43872499465942383 - trainLoss: 0.435736745595932\n",
      "cnt: 0 - valLoss: 0.4387240409851074 - trainLoss: 0.43573418259620667\n",
      "cnt: 0 - valLoss: 0.4387231171131134 - trainLoss: 0.4357316493988037\n",
      "cnt: 0 - valLoss: 0.438722163438797 - trainLoss: 0.43572911620140076\n",
      "cnt: 0 - valLoss: 0.4387212097644806 - trainLoss: 0.4357266128063202\n",
      "cnt: 0 - valLoss: 0.4387202858924866 - trainLoss: 0.43572407960891724\n",
      "cnt: 0 - valLoss: 0.43871933221817017 - trainLoss: 0.4357215464115143\n",
      "cnt: 0 - valLoss: 0.43871837854385376 - trainLoss: 0.43571901321411133\n",
      "cnt: 0 - valLoss: 0.4387173652648926 - trainLoss: 0.43571650981903076\n",
      "cnt: 0 - valLoss: 0.43871623277664185 - trainLoss: 0.4357139766216278\n",
      "cnt: 0 - valLoss: 0.43871521949768066 - trainLoss: 0.43571144342422485\n",
      "cnt: 0 - valLoss: 0.4387141764163971 - trainLoss: 0.4357089102268219\n",
      "cnt: 0 - valLoss: 0.43871307373046875 - trainLoss: 0.43570640683174133\n",
      "cnt: 0 - valLoss: 0.4387120008468628 - trainLoss: 0.4357038736343384\n",
      "cnt: 0 - valLoss: 0.43871092796325684 - trainLoss: 0.43570131063461304\n",
      "cnt: 0 - valLoss: 0.43870991468429565 - trainLoss: 0.43569880723953247\n",
      "cnt: 0 - valLoss: 0.4387088418006897 - trainLoss: 0.4356963038444519\n",
      "cnt: 0 - valLoss: 0.43870776891708374 - trainLoss: 0.43569377064704895\n",
      "cnt: 0 - valLoss: 0.43870672583580017 - trainLoss: 0.4356912076473236\n",
      "cnt: 0 - valLoss: 0.4387056529521942 - trainLoss: 0.43568873405456543\n",
      "cnt: 0 - valLoss: 0.43870458006858826 - trainLoss: 0.4356862008571625\n",
      "cnt: 0 - valLoss: 0.4387035071849823 - trainLoss: 0.4356836676597595\n",
      "cnt: 0 - valLoss: 0.43870243430137634 - trainLoss: 0.43568113446235657\n",
      "cnt: 0 - valLoss: 0.43870142102241516 - trainLoss: 0.435678631067276\n",
      "cnt: 0 - valLoss: 0.4387003481388092 - trainLoss: 0.43567609786987305\n",
      "cnt: 0 - valLoss: 0.43869927525520325 - trainLoss: 0.4356735646724701\n",
      "cnt: 0 - valLoss: 0.4386982321739197 - trainLoss: 0.43567103147506714\n",
      "cnt: 0 - valLoss: 0.4386972188949585 - trainLoss: 0.4356685280799866\n",
      "cnt: 0 - valLoss: 0.43869608640670776 - trainLoss: 0.435666024684906\n",
      "cnt: 0 - valLoss: 0.4386950731277466 - trainLoss: 0.43566346168518066\n",
      "cnt: 0 - valLoss: 0.438694030046463 - trainLoss: 0.4356609880924225\n",
      "cnt: 0 - valLoss: 0.43869292736053467 - trainLoss: 0.43565842509269714\n",
      "cnt: 0 - valLoss: 0.4386919438838959 - trainLoss: 0.4356559216976166\n",
      "cnt: 0 - valLoss: 0.4386908710002899 - trainLoss: 0.435653418302536\n",
      "cnt: 0 - valLoss: 0.43868979811668396 - trainLoss: 0.43565088510513306\n",
      "cnt: 0 - valLoss: 0.438688725233078 - trainLoss: 0.4356483221054077\n",
      "cnt: 0 - valLoss: 0.4386877119541168 - trainLoss: 0.43564578890800476\n",
      "cnt: 0 - valLoss: 0.43868666887283325 - trainLoss: 0.4356432557106018\n",
      "cnt: 0 - valLoss: 0.4386855661869049 - trainLoss: 0.43564078211784363\n",
      "cnt: 0 - valLoss: 0.43868452310562134 - trainLoss: 0.4356382489204407\n",
      "cnt: 0 - valLoss: 0.43868350982666016 - trainLoss: 0.4356357455253601\n",
      "cnt: 0 - valLoss: 0.4386823773384094 - trainLoss: 0.4356331527233124\n",
      "cnt: 0 - valLoss: 0.43868136405944824 - trainLoss: 0.4356306791305542\n",
      "cnt: 0 - valLoss: 0.4386803209781647 - trainLoss: 0.43562811613082886\n",
      "cnt: 0 - valLoss: 0.4386792480945587 - trainLoss: 0.4356255829334259\n",
      "cnt: 0 - valLoss: 0.43867823481559753 - trainLoss: 0.4356231093406677\n",
      "cnt: 0 - valLoss: 0.4386771619319916 - trainLoss: 0.43562057614326477\n",
      "cnt: 0 - valLoss: 0.4386760890483856 - trainLoss: 0.4356180429458618\n",
      "cnt: 0 - valLoss: 0.43867504596710205 - trainLoss: 0.43561553955078125\n",
      "cnt: 0 - valLoss: 0.43867403268814087 - trainLoss: 0.4356130063533783\n",
      "cnt: 0 - valLoss: 0.4386729598045349 - trainLoss: 0.43561047315597534\n",
      "cnt: 0 - valLoss: 0.43867191672325134 - trainLoss: 0.4356079399585724\n",
      "cnt: 0 - valLoss: 0.43867090344429016 - trainLoss: 0.4356054365634918\n",
      "cnt: 0 - valLoss: 0.43866977095603943 - trainLoss: 0.43560293316841125\n",
      "cnt: 0 - valLoss: 0.43866875767707825 - trainLoss: 0.4356003701686859\n",
      "cnt: 0 - valLoss: 0.4386677145957947 - trainLoss: 0.43559789657592773\n",
      "cnt: 0 - valLoss: 0.4386666417121887 - trainLoss: 0.4355953335762024\n",
      "cnt: 0 - valLoss: 0.43866556882858276 - trainLoss: 0.4355928301811218\n",
      "cnt: 0 - valLoss: 0.4386645555496216 - trainLoss: 0.4355902671813965\n",
      "cnt: 0 - valLoss: 0.438663512468338 - trainLoss: 0.4355877935886383\n",
      "cnt: 0 - valLoss: 0.43866243958473206 - trainLoss: 0.43558523058891296\n",
      "cnt: 0 - valLoss: 0.4386613965034485 - trainLoss: 0.4355827569961548\n",
      "cnt: 0 - valLoss: 0.4386603534221649 - trainLoss: 0.43558016419410706\n",
      "cnt: 0 - valLoss: 0.43865931034088135 - trainLoss: 0.4355776906013489\n",
      "cnt: 0 - valLoss: 0.4386582672595978 - trainLoss: 0.43557512760162354\n",
      "cnt: 0 - valLoss: 0.4386572539806366 - trainLoss: 0.43557265400886536\n",
      "cnt: 0 - valLoss: 0.43865615129470825 - trainLoss: 0.4355700612068176\n",
      "cnt: 0 - valLoss: 0.43865513801574707 - trainLoss: 0.43556758761405945\n",
      "cnt: 0 - valLoss: 0.4386540949344635 - trainLoss: 0.4355650246143341\n",
      "cnt: 0 - valLoss: 0.43865302205085754 - trainLoss: 0.4355625510215759\n",
      "cnt: 0 - valLoss: 0.43865200877189636 - trainLoss: 0.4355599582195282\n",
      "cnt: 0 - valLoss: 0.4386509358882904 - trainLoss: 0.43555748462677\n",
      "cnt: 0 - valLoss: 0.43864989280700684 - trainLoss: 0.43555495142936707\n",
      "cnt: 0 - valLoss: 0.4386487901210785 - trainLoss: 0.4355524480342865\n",
      "cnt: 0 - valLoss: 0.4386478364467621 - trainLoss: 0.43554988503456116\n",
      "cnt: 0 - valLoss: 0.43864673376083374 - trainLoss: 0.4355473816394806\n",
      "cnt: 0 - valLoss: 0.43864569067955017 - trainLoss: 0.43554481863975525\n",
      "cnt: 0 - valLoss: 0.438644677400589 - trainLoss: 0.43554234504699707\n",
      "cnt: 0 - valLoss: 0.4386436343193054 - trainLoss: 0.4355398118495941\n",
      "cnt: 0 - valLoss: 0.4386425316333771 - trainLoss: 0.43553727865219116\n",
      "cnt: 0 - valLoss: 0.4386414885520935 - trainLoss: 0.435534805059433\n",
      "cnt: 0 - valLoss: 0.4386404752731323 - trainLoss: 0.43553224205970764\n",
      "cnt: 0 - valLoss: 0.43863940238952637 - trainLoss: 0.4355296790599823\n",
      "cnt: 0 - valLoss: 0.4386383593082428 - trainLoss: 0.43552717566490173\n",
      "cnt: 0 - valLoss: 0.4386373460292816 - trainLoss: 0.43552467226982117\n",
      "cnt: 0 - valLoss: 0.43863627314567566 - trainLoss: 0.4355221390724182\n",
      "cnt: 0 - valLoss: 0.4386352300643921 - trainLoss: 0.43551960587501526\n",
      "cnt: 0 - valLoss: 0.4386342167854309 - trainLoss: 0.4355170726776123\n",
      "cnt: 0 - valLoss: 0.4386330842971802 - trainLoss: 0.4355145990848541\n",
      "cnt: 0 - valLoss: 0.4386321008205414 - trainLoss: 0.4355120360851288\n",
      "cnt: 0 - valLoss: 0.4386310279369354 - trainLoss: 0.4355095624923706\n",
      "cnt: 0 - valLoss: 0.43862995505332947 - trainLoss: 0.4355069696903229\n",
      "cnt: 0 - valLoss: 0.4386289715766907 - trainLoss: 0.4355044960975647\n",
      "cnt: 0 - valLoss: 0.4386279582977295 - trainLoss: 0.43550193309783936\n",
      "cnt: 0 - valLoss: 0.43862682580947876 - trainLoss: 0.4354994595050812\n",
      "cnt: 0 - valLoss: 0.4386258125305176 - trainLoss: 0.43549689650535583\n",
      "cnt: 0 - valLoss: 0.438624769449234 - trainLoss: 0.4354943633079529\n",
      "cnt: 0 - valLoss: 0.4386237561702728 - trainLoss: 0.4354918301105499\n",
      "cnt: 0 - valLoss: 0.43862268328666687 - trainLoss: 0.43548935651779175\n",
      "cnt: 0 - valLoss: 0.4386216104030609 - trainLoss: 0.4354867935180664\n",
      "cnt: 0 - valLoss: 0.43862056732177734 - trainLoss: 0.43548426032066345\n",
      "cnt: 0 - valLoss: 0.43861955404281616 - trainLoss: 0.4354817271232605\n",
      "cnt: 0 - valLoss: 0.4386185109615326 - trainLoss: 0.4354792535305023\n",
      "cnt: 0 - valLoss: 0.43861740827560425 - trainLoss: 0.435476690530777\n",
      "cnt: 0 - valLoss: 0.43861642479896545 - trainLoss: 0.435474157333374\n",
      "cnt: 0 - valLoss: 0.4386153519153595 - trainLoss: 0.43547162413597107\n",
      "cnt: 0 - valLoss: 0.4386143088340759 - trainLoss: 0.4354690909385681\n",
      "cnt: 0 - valLoss: 0.43861329555511475 - trainLoss: 0.43546658754348755\n",
      "cnt: 0 - valLoss: 0.4386122524738312 - trainLoss: 0.435464084148407\n",
      "cnt: 0 - valLoss: 0.4386111795902252 - trainLoss: 0.4354615807533264\n",
      "cnt: 0 - valLoss: 0.43861016631126404 - trainLoss: 0.43545904755592346\n",
      "cnt: 0 - valLoss: 0.43860912322998047 - trainLoss: 0.4354564845561981\n",
      "cnt: 0 - valLoss: 0.4386080503463745 - trainLoss: 0.43545395135879517\n",
      "cnt: 0 - valLoss: 0.43860703706741333 - trainLoss: 0.435451477766037\n",
      "cnt: 0 - valLoss: 0.43860599398612976 - trainLoss: 0.43544894456863403\n",
      "cnt: 0 - valLoss: 0.4386049807071686 - trainLoss: 0.4354464113712311\n",
      "cnt: 0 - valLoss: 0.438603937625885 - trainLoss: 0.4354439079761505\n",
      "cnt: 0 - valLoss: 0.43860286474227905 - trainLoss: 0.4354413151741028\n",
      "cnt: 0 - valLoss: 0.43860185146331787 - trainLoss: 0.4354388415813446\n",
      "cnt: 0 - valLoss: 0.4386007785797119 - trainLoss: 0.43543627858161926\n",
      "cnt: 0 - valLoss: 0.43859973549842834 - trainLoss: 0.4354338049888611\n",
      "cnt: 0 - valLoss: 0.43859872221946716 - trainLoss: 0.43543127179145813\n",
      "cnt: 0 - valLoss: 0.4385976791381836 - trainLoss: 0.4354287385940552\n",
      "cnt: 0 - valLoss: 0.43859660625457764 - trainLoss: 0.4354262053966522\n",
      "cnt: 0 - valLoss: 0.43859559297561646 - trainLoss: 0.43542370200157166\n",
      "cnt: 0 - valLoss: 0.4385945498943329 - trainLoss: 0.4354211091995239\n",
      "cnt: 0 - valLoss: 0.4385935366153717 - trainLoss: 0.43541863560676575\n",
      "cnt: 0 - valLoss: 0.43859246373176575 - trainLoss: 0.4354161024093628\n",
      "cnt: 0 - valLoss: 0.4385914206504822 - trainLoss: 0.4354135990142822\n",
      "cnt: 0 - valLoss: 0.438590407371521 - trainLoss: 0.4354110360145569\n",
      "cnt: 0 - valLoss: 0.4385893642902374 - trainLoss: 0.4354085326194763\n",
      "cnt: 0 - valLoss: 0.43858835101127625 - trainLoss: 0.43540602922439575\n",
      "cnt: 0 - valLoss: 0.4385872781276703 - trainLoss: 0.4354034960269928\n",
      "cnt: 0 - valLoss: 0.4385862350463867 - trainLoss: 0.43540096282958984\n",
      "cnt: 0 - valLoss: 0.43858522176742554 - trainLoss: 0.4353984296321869\n",
      "cnt: 0 - valLoss: 0.43858417868614197 - trainLoss: 0.4353959560394287\n",
      "cnt: 0 - valLoss: 0.4385831654071808 - trainLoss: 0.43539339303970337\n",
      "cnt: 0 - valLoss: 0.43858209252357483 - trainLoss: 0.4353908598423004\n",
      "cnt: 0 - valLoss: 0.43858104944229126 - trainLoss: 0.43538832664489746\n",
      "cnt: 0 - valLoss: 0.4385800361633301 - trainLoss: 0.4353858232498169\n",
      "cnt: 0 - valLoss: 0.4385789930820465 - trainLoss: 0.43538329005241394\n",
      "cnt: 0 - valLoss: 0.4385779798030853 - trainLoss: 0.435380756855011\n",
      "cnt: 0 - valLoss: 0.43857693672180176 - trainLoss: 0.43537822365760803\n",
      "cnt: 0 - valLoss: 0.4385758638381958 - trainLoss: 0.43537572026252747\n",
      "cnt: 0 - valLoss: 0.438574880361557 - trainLoss: 0.4353731870651245\n",
      "cnt: 0 - valLoss: 0.4385738670825958 - trainLoss: 0.43537071347236633\n",
      "cnt: 0 - valLoss: 0.43857282400131226 - trainLoss: 0.4353681206703186\n",
      "cnt: 0 - valLoss: 0.4385718107223511 - trainLoss: 0.43536561727523804\n",
      "cnt: 0 - valLoss: 0.4385707676410675 - trainLoss: 0.4353630840778351\n",
      "cnt: 0 - valLoss: 0.4385697543621063 - trainLoss: 0.43536055088043213\n",
      "cnt: 0 - valLoss: 0.43856871128082275 - trainLoss: 0.43535804748535156\n",
      "cnt: 0 - valLoss: 0.4385676980018616 - trainLoss: 0.4353555142879486\n",
      "cnt: 0 - valLoss: 0.4385667145252228 - trainLoss: 0.43535298109054565\n",
      "cnt: 0 - valLoss: 0.4385656416416168 - trainLoss: 0.4353504478931427\n",
      "cnt: 0 - valLoss: 0.43856459856033325 - trainLoss: 0.43534794449806213\n",
      "cnt: 0 - valLoss: 0.43856361508369446 - trainLoss: 0.4353454113006592\n",
      "cnt: 0 - valLoss: 0.4385625720024109 - trainLoss: 0.4353428781032562\n",
      "cnt: 0 - valLoss: 0.4385615587234497 - trainLoss: 0.4353403151035309\n",
      "cnt: 0 - valLoss: 0.43856051564216614 - trainLoss: 0.4353378415107727\n",
      "cnt: 0 - valLoss: 0.43855950236320496 - trainLoss: 0.43533530831336975\n",
      "cnt: 0 - valLoss: 0.43855851888656616 - trainLoss: 0.4353327751159668\n",
      "cnt: 0 - valLoss: 0.4385574758052826 - trainLoss: 0.4353303015232086\n",
      "cnt: 0 - valLoss: 0.4385564625263214 - trainLoss: 0.4353277385234833\n",
      "cnt: 0 - valLoss: 0.43855541944503784 - trainLoss: 0.4353252351284027\n",
      "cnt: 0 - valLoss: 0.43855440616607666 - trainLoss: 0.43532267212867737\n",
      "cnt: 0 - valLoss: 0.4385533630847931 - trainLoss: 0.4353201985359192\n",
      "cnt: 0 - valLoss: 0.4385523498058319 - trainLoss: 0.43531763553619385\n",
      "cnt: 0 - valLoss: 0.4385513663291931 - trainLoss: 0.4353151321411133\n",
      "cnt: 0 - valLoss: 0.4385503828525543 - trainLoss: 0.43531256914138794\n",
      "cnt: 0 - valLoss: 0.43854933977127075 - trainLoss: 0.43531009554862976\n",
      "cnt: 0 - valLoss: 0.43854832649230957 - trainLoss: 0.4353075623512268\n",
      "cnt: 0 - valLoss: 0.438547283411026 - trainLoss: 0.43530505895614624\n",
      "cnt: 0 - valLoss: 0.4385462701320648 - trainLoss: 0.4353024661540985\n",
      "cnt: 0 - valLoss: 0.43854522705078125 - trainLoss: 0.43529999256134033\n",
      "cnt: 0 - valLoss: 0.4385441839694977 - trainLoss: 0.435297429561615\n",
      "cnt: 0 - valLoss: 0.4385431706905365 - trainLoss: 0.4352949559688568\n",
      "cnt: 0 - valLoss: 0.4385422170162201 - trainLoss: 0.4352923631668091\n",
      "cnt: 0 - valLoss: 0.4385412037372589 - trainLoss: 0.4352898895740509\n",
      "cnt: 0 - valLoss: 0.43854016065597534 - trainLoss: 0.43528732657432556\n",
      "cnt: 0 - valLoss: 0.43853914737701416 - trainLoss: 0.4352847933769226\n",
      "cnt: 0 - valLoss: 0.4385381042957306 - trainLoss: 0.43528231978416443\n",
      "cnt: 0 - valLoss: 0.4385371208190918 - trainLoss: 0.4352797567844391\n",
      "cnt: 0 - valLoss: 0.43853604793548584 - trainLoss: 0.43527722358703613\n",
      "cnt: 0 - valLoss: 0.43853503465652466 - trainLoss: 0.43527474999427795\n",
      "cnt: 0 - valLoss: 0.43853408098220825 - trainLoss: 0.435272216796875\n",
      "cnt: 0 - valLoss: 0.43853306770324707 - trainLoss: 0.43526968359947205\n",
      "cnt: 0 - valLoss: 0.4385320246219635 - trainLoss: 0.4352671205997467\n",
      "cnt: 0 - valLoss: 0.43853095173835754 - trainLoss: 0.4352646470069885\n",
      "cnt: 0 - valLoss: 0.43852993845939636 - trainLoss: 0.43526211380958557\n",
      "cnt: 0 - valLoss: 0.4385289251804352 - trainLoss: 0.4352595806121826\n",
      "cnt: 0 - valLoss: 0.438527911901474 - trainLoss: 0.43525707721710205\n",
      "cnt: 0 - valLoss: 0.43852686882019043 - trainLoss: 0.4352545440196991\n",
      "cnt: 0 - valLoss: 0.43852585554122925 - trainLoss: 0.43525198101997375\n",
      "cnt: 0 - valLoss: 0.43852490186691284 - trainLoss: 0.4352494776248932\n",
      "cnt: 0 - valLoss: 0.4385238289833069 - trainLoss: 0.4352469742298126\n",
      "cnt: 0 - valLoss: 0.4385228157043457 - trainLoss: 0.43524444103240967\n",
      "cnt: 0 - valLoss: 0.43852177262306213 - trainLoss: 0.4352419674396515\n",
      "cnt: 0 - valLoss: 0.43852078914642334 - trainLoss: 0.43523937463760376\n",
      "cnt: 0 - valLoss: 0.4385197162628174 - trainLoss: 0.4352368712425232\n",
      "cnt: 0 - valLoss: 0.4385187327861786 - trainLoss: 0.43523433804512024\n",
      "cnt: 0 - valLoss: 0.4385177493095398 - trainLoss: 0.43523186445236206\n",
      "cnt: 0 - valLoss: 0.43851667642593384 - trainLoss: 0.4352293014526367\n",
      "cnt: 0 - valLoss: 0.43851566314697266 - trainLoss: 0.43522676825523376\n",
      "cnt: 0 - valLoss: 0.4385146200656891 - trainLoss: 0.4352242648601532\n",
      "cnt: 0 - valLoss: 0.4385136365890503 - trainLoss: 0.43522176146507263\n",
      "cnt: 0 - valLoss: 0.43851256370544434 - trainLoss: 0.4352191984653473\n",
      "cnt: 0 - valLoss: 0.43851158022880554 - trainLoss: 0.43521666526794434\n",
      "cnt: 0 - valLoss: 0.438510537147522 - trainLoss: 0.4352141320705414\n",
      "cnt: 0 - valLoss: 0.4385095536708832 - trainLoss: 0.4352116584777832\n",
      "cnt: 0 - valLoss: 0.438508540391922 - trainLoss: 0.43520909547805786\n",
      "cnt: 0 - valLoss: 0.4385075569152832 - trainLoss: 0.4352065920829773\n",
      "cnt: 0 - valLoss: 0.43850651383399963 - trainLoss: 0.43520408868789673\n",
      "cnt: 0 - valLoss: 0.43850550055503845 - trainLoss: 0.4352015554904938\n",
      "cnt: 0 - valLoss: 0.4385044574737549 - trainLoss: 0.43519899249076843\n",
      "cnt: 0 - valLoss: 0.4385034441947937 - trainLoss: 0.43519648909568787\n",
      "cnt: 0 - valLoss: 0.4385024607181549 - trainLoss: 0.4351940155029297\n",
      "cnt: 0 - valLoss: 0.43850141763687134 - trainLoss: 0.43519145250320435\n",
      "cnt: 0 - valLoss: 0.43850043416023254 - trainLoss: 0.43518897891044617\n",
      "cnt: 0 - valLoss: 0.43849942088127136 - trainLoss: 0.43518638610839844\n",
      "cnt: 0 - valLoss: 0.4384983777999878 - trainLoss: 0.43518388271331787\n",
      "cnt: 0 - valLoss: 0.438497394323349 - trainLoss: 0.4351813495159149\n",
      "cnt: 0 - valLoss: 0.43849635124206543 - trainLoss: 0.43517881631851196\n",
      "cnt: 0 - valLoss: 0.43849533796310425 - trainLoss: 0.4351763129234314\n",
      "cnt: 0 - valLoss: 0.4384942948818207 - trainLoss: 0.43517380952835083\n",
      "cnt: 0 - valLoss: 0.4384932816028595 - trainLoss: 0.4351712465286255\n",
      "cnt: 0 - valLoss: 0.4384922981262207 - trainLoss: 0.4351687729358673\n",
      "cnt: 0 - valLoss: 0.43849125504493713 - trainLoss: 0.43516620993614197\n",
      "cnt: 0 - valLoss: 0.43849027156829834 - trainLoss: 0.4351637065410614\n",
      "cnt: 0 - valLoss: 0.43848925828933716 - trainLoss: 0.43516114354133606\n",
      "cnt: 0 - valLoss: 0.4384882152080536 - trainLoss: 0.4351586699485779\n",
      "cnt: 0 - valLoss: 0.4384872019290924 - trainLoss: 0.43515610694885254\n",
      "cnt: 0 - valLoss: 0.43848615884780884 - trainLoss: 0.435153603553772\n",
      "cnt: 0 - valLoss: 0.43848514556884766 - trainLoss: 0.43515104055404663\n",
      "cnt: 0 - valLoss: 0.43848419189453125 - trainLoss: 0.43514856696128845\n",
      "cnt: 0 - valLoss: 0.4384831488132477 - trainLoss: 0.4351460337638855\n",
      "cnt: 0 - valLoss: 0.4384821355342865 - trainLoss: 0.43514350056648254\n",
      "cnt: 0 - valLoss: 0.43848109245300293 - trainLoss: 0.4351409375667572\n",
      "cnt: 0 - valLoss: 0.43848007917404175 - trainLoss: 0.43513840436935425\n",
      "cnt: 0 - valLoss: 0.4384790360927582 - trainLoss: 0.43513593077659607\n",
      "cnt: 0 - valLoss: 0.4384780526161194 - trainLoss: 0.4351333975791931\n",
      "cnt: 0 - valLoss: 0.4384770393371582 - trainLoss: 0.4351308345794678\n",
      "cnt: 0 - valLoss: 0.4384760558605194 - trainLoss: 0.4351283609867096\n",
      "cnt: 0 - valLoss: 0.43847501277923584 - trainLoss: 0.43512582778930664\n",
      "cnt: 0 - valLoss: 0.43847399950027466 - trainLoss: 0.4351233243942261\n",
      "cnt: 0 - valLoss: 0.43847301602363586 - trainLoss: 0.4351207911968231\n",
      "cnt: 0 - valLoss: 0.43847203254699707 - trainLoss: 0.43511825799942017\n",
      "cnt: 0 - valLoss: 0.4384709894657135 - trainLoss: 0.4351157248020172\n",
      "cnt: 0 - valLoss: 0.43846991658210754 - trainLoss: 0.43511322140693665\n",
      "cnt: 0 - valLoss: 0.43846893310546875 - trainLoss: 0.4351106882095337\n",
      "cnt: 0 - valLoss: 0.4384678900241852 - trainLoss: 0.43510815501213074\n",
      "cnt: 0 - valLoss: 0.4384669065475464 - trainLoss: 0.4351056218147278\n",
      "cnt: 0 - valLoss: 0.4384659230709076 - trainLoss: 0.4351031184196472\n",
      "cnt: 0 - valLoss: 0.4384649097919464 - trainLoss: 0.4351005256175995\n",
      "cnt: 0 - valLoss: 0.43846386671066284 - trainLoss: 0.4350980520248413\n",
      "cnt: 0 - valLoss: 0.43846285343170166 - trainLoss: 0.43509551882743835\n",
      "cnt: 0 - valLoss: 0.4384618103504181 - trainLoss: 0.4350930154323578\n",
      "cnt: 0 - valLoss: 0.4384608268737793 - trainLoss: 0.43509048223495483\n",
      "cnt: 0 - valLoss: 0.4384598135948181 - trainLoss: 0.4350879490375519\n",
      "cnt: 0 - valLoss: 0.43845877051353455 - trainLoss: 0.4350854158401489\n",
      "cnt: 0 - valLoss: 0.43845778703689575 - trainLoss: 0.43508291244506836\n",
      "cnt: 0 - valLoss: 0.43845680356025696 - trainLoss: 0.435080349445343\n",
      "cnt: 0 - valLoss: 0.4384557604789734 - trainLoss: 0.43507784605026245\n",
      "cnt: 0 - valLoss: 0.4384547472000122 - trainLoss: 0.4350753426551819\n",
      "cnt: 0 - valLoss: 0.43845370411872864 - trainLoss: 0.43507280945777893\n",
      "cnt: 0 - valLoss: 0.43845272064208984 - trainLoss: 0.4350702464580536\n",
      "cnt: 0 - valLoss: 0.43845170736312866 - trainLoss: 0.435067743062973\n",
      "cnt: 0 - valLoss: 0.4384506642818451 - trainLoss: 0.43506523966789246\n",
      "cnt: 0 - valLoss: 0.4384496510028839 - trainLoss: 0.4350627064704895\n",
      "cnt: 0 - valLoss: 0.4384486675262451 - trainLoss: 0.4350602328777313\n",
      "cnt: 0 - valLoss: 0.43844762444496155 - trainLoss: 0.4350576400756836\n",
      "cnt: 0 - valLoss: 0.43844664096832275 - trainLoss: 0.435055136680603\n",
      "cnt: 0 - valLoss: 0.4384456276893616 - trainLoss: 0.4350526034832001\n",
      "cnt: 0 - valLoss: 0.438444584608078 - trainLoss: 0.4350501298904419\n",
      "cnt: 0 - valLoss: 0.4384436011314392 - trainLoss: 0.43504753708839417\n",
      "cnt: 0 - valLoss: 0.43844252824783325 - trainLoss: 0.4350450336933136\n",
      "cnt: 0 - valLoss: 0.43844154477119446 - trainLoss: 0.43504250049591064\n",
      "cnt: 0 - valLoss: 0.43844056129455566 - trainLoss: 0.43504002690315247\n",
      "cnt: 0 - valLoss: 0.43843957781791687 - trainLoss: 0.4350374639034271\n",
      "cnt: 0 - valLoss: 0.4384385347366333 - trainLoss: 0.43503493070602417\n",
      "cnt: 0 - valLoss: 0.4384375512599945 - trainLoss: 0.4350323975086212\n",
      "cnt: 0 - valLoss: 0.4384365379810333 - trainLoss: 0.43502992391586304\n",
      "cnt: 0 - valLoss: 0.43843549489974976 - trainLoss: 0.4350273609161377\n",
      "cnt: 0 - valLoss: 0.4384344816207886 - trainLoss: 0.43502485752105713\n",
      "cnt: 0 - valLoss: 0.438433438539505 - trainLoss: 0.4350222945213318\n",
      "cnt: 0 - valLoss: 0.4384324252605438 - trainLoss: 0.43501976132392883\n",
      "cnt: 0 - valLoss: 0.43843138217926025 - trainLoss: 0.43501725792884827\n",
      "cnt: 0 - valLoss: 0.43843042850494385 - trainLoss: 0.4350147545337677\n",
      "cnt: 0 - valLoss: 0.43842941522598267 - trainLoss: 0.43501219153404236\n",
      "cnt: 0 - valLoss: 0.4384283721446991 - trainLoss: 0.4350096583366394\n",
      "cnt: 0 - valLoss: 0.4384273886680603 - trainLoss: 0.43500715494155884\n",
      "cnt: 0 - valLoss: 0.4384264051914215 - trainLoss: 0.43500465154647827\n",
      "cnt: 0 - valLoss: 0.43842533230781555 - trainLoss: 0.43500208854675293\n",
      "cnt: 0 - valLoss: 0.43842434883117676 - trainLoss: 0.43499958515167236\n",
      "cnt: 0 - valLoss: 0.4384233355522156 - trainLoss: 0.4349970519542694\n",
      "cnt: 0 - valLoss: 0.438422292470932 - trainLoss: 0.43499454855918884\n",
      "cnt: 0 - valLoss: 0.438421368598938 - trainLoss: 0.4349919855594635\n",
      "cnt: 0 - valLoss: 0.4384203255176544 - trainLoss: 0.4349895119667053\n",
      "cnt: 0 - valLoss: 0.43841931223869324 - trainLoss: 0.43498697876930237\n",
      "cnt: 0 - valLoss: 0.43841829895973206 - trainLoss: 0.4349844753742218\n",
      "cnt: 0 - valLoss: 0.43841731548309326 - trainLoss: 0.43498194217681885\n",
      "cnt: 0 - valLoss: 0.4384163022041321 - trainLoss: 0.4349794089794159\n",
      "cnt: 0 - valLoss: 0.4384153187274933 - trainLoss: 0.43497684597969055\n",
      "cnt: 0 - valLoss: 0.4384142756462097 - trainLoss: 0.4349743723869324\n",
      "cnt: 0 - valLoss: 0.43841326236724854 - trainLoss: 0.4349718391895294\n",
      "cnt: 0 - valLoss: 0.43841227889060974 - trainLoss: 0.43496930599212646\n",
      "cnt: 0 - valLoss: 0.43841123580932617 - trainLoss: 0.4349668323993683\n",
      "cnt: 0 - valLoss: 0.4384102523326874 - trainLoss: 0.43496426939964294\n",
      "cnt: 0 - valLoss: 0.4384092688560486 - trainLoss: 0.43496173620224\n",
      "cnt: 0 - valLoss: 0.4384082555770874 - trainLoss: 0.43495920300483704\n",
      "cnt: 0 - valLoss: 0.43840721249580383 - trainLoss: 0.43495672941207886\n",
      "cnt: 0 - valLoss: 0.43840622901916504 - trainLoss: 0.4349541664123535\n",
      "cnt: 0 - valLoss: 0.43840518593788147 - trainLoss: 0.43495163321495056\n",
      "cnt: 0 - valLoss: 0.4384041726589203 - trainLoss: 0.4349491000175476\n",
      "cnt: 0 - valLoss: 0.4384031891822815 - trainLoss: 0.43494662642478943\n",
      "cnt: 0 - valLoss: 0.4384022057056427 - trainLoss: 0.4349440634250641\n",
      "cnt: 0 - valLoss: 0.4384012222290039 - trainLoss: 0.4349415600299835\n",
      "cnt: 0 - valLoss: 0.43840017914772034 - trainLoss: 0.4349389970302582\n",
      "cnt: 0 - valLoss: 0.43839919567108154 - trainLoss: 0.4349364936351776\n",
      "cnt: 0 - valLoss: 0.43839818239212036 - trainLoss: 0.43493396043777466\n",
      "cnt: 0 - valLoss: 0.4383971393108368 - trainLoss: 0.4349314868450165\n",
      "cnt: 0 - valLoss: 0.438396155834198 - trainLoss: 0.43492889404296875\n",
      "cnt: 0 - valLoss: 0.43839511275291443 - trainLoss: 0.4349263906478882\n",
      "cnt: 0 - valLoss: 0.43839409947395325 - trainLoss: 0.43492385745048523\n",
      "cnt: 0 - valLoss: 0.43839311599731445 - trainLoss: 0.4349213242530823\n",
      "cnt: 0 - valLoss: 0.43839213252067566 - trainLoss: 0.4349188506603241\n",
      "cnt: 0 - valLoss: 0.4383910894393921 - trainLoss: 0.43491628766059875\n",
      "cnt: 0 - valLoss: 0.4383901059627533 - trainLoss: 0.4349137544631958\n",
      "cnt: 0 - valLoss: 0.4383891224861145 - trainLoss: 0.4349112808704376\n",
      "cnt: 0 - valLoss: 0.4383881092071533 - trainLoss: 0.4349086880683899\n",
      "cnt: 0 - valLoss: 0.43838706612586975 - trainLoss: 0.4349062144756317\n",
      "cnt: 0 - valLoss: 0.43838608264923096 - trainLoss: 0.43490365147590637\n",
      "cnt: 0 - valLoss: 0.4383850395679474 - trainLoss: 0.4349011182785034\n",
      "cnt: 0 - valLoss: 0.4383840560913086 - trainLoss: 0.43489864468574524\n",
      "cnt: 0 - valLoss: 0.4383830428123474 - trainLoss: 0.4348961114883423\n",
      "cnt: 0 - valLoss: 0.438382089138031 - trainLoss: 0.43489354848861694\n",
      "cnt: 0 - valLoss: 0.4383810758590698 - trainLoss: 0.434891015291214\n",
      "cnt: 0 - valLoss: 0.43838003277778625 - trainLoss: 0.4348885416984558\n",
      "cnt: 0 - valLoss: 0.4383790194988251 - trainLoss: 0.43488600850105286\n",
      "cnt: 0 - valLoss: 0.4383780360221863 - trainLoss: 0.4348834455013275\n",
      "cnt: 0 - valLoss: 0.4383770227432251 - trainLoss: 0.43488097190856934\n",
      "cnt: 0 - valLoss: 0.4383760094642639 - trainLoss: 0.4348784387111664\n",
      "cnt: 0 - valLoss: 0.43837496638298035 - trainLoss: 0.4348759055137634\n",
      "cnt: 0 - valLoss: 0.43837395310401917 - trainLoss: 0.4348733425140381\n",
      "cnt: 0 - valLoss: 0.43837299942970276 - trainLoss: 0.4348708689212799\n",
      "cnt: 0 - valLoss: 0.43837201595306396 - trainLoss: 0.43486833572387695\n",
      "cnt: 0 - valLoss: 0.4383710026741028 - trainLoss: 0.434865802526474\n",
      "cnt: 0 - valLoss: 0.438370019197464 - trainLoss: 0.43486329913139343\n",
      "cnt: 0 - valLoss: 0.4383689761161804 - trainLoss: 0.4348607659339905\n",
      "cnt: 0 - valLoss: 0.4383679926395416 - trainLoss: 0.43485820293426514\n",
      "cnt: 0 - valLoss: 0.43836697936058044 - trainLoss: 0.43485569953918457\n",
      "cnt: 0 - valLoss: 0.43836596608161926 - trainLoss: 0.434853196144104\n",
      "cnt: 0 - valLoss: 0.4383649528026581 - trainLoss: 0.43485066294670105\n",
      "cnt: 0 - valLoss: 0.4383639097213745 - trainLoss: 0.4348481297492981\n",
      "cnt: 0 - valLoss: 0.4383629858493805 - trainLoss: 0.43484562635421753\n",
      "cnt: 0 - valLoss: 0.4383619427680969 - trainLoss: 0.4348430931568146\n",
      "cnt: 0 - valLoss: 0.43836095929145813 - trainLoss: 0.4348405599594116\n",
      "cnt: 0 - valLoss: 0.43835997581481934 - trainLoss: 0.43483802676200867\n",
      "cnt: 0 - valLoss: 0.43835896253585815 - trainLoss: 0.4348355233669281\n",
      "cnt: 0 - valLoss: 0.4383579194545746 - trainLoss: 0.43483301997184753\n",
      "cnt: 0 - valLoss: 0.4383569359779358 - trainLoss: 0.4348304569721222\n",
      "cnt: 0 - valLoss: 0.438355952501297 - trainLoss: 0.43482792377471924\n",
      "cnt: 0 - valLoss: 0.4383549094200134 - trainLoss: 0.43482542037963867\n",
      "cnt: 0 - valLoss: 0.43835389614105225 - trainLoss: 0.4348228871822357\n",
      "cnt: 0 - valLoss: 0.43835294246673584 - trainLoss: 0.43482035398483276\n",
      "cnt: 0 - valLoss: 0.43835192918777466 - trainLoss: 0.4348178207874298\n",
      "cnt: 0 - valLoss: 0.43835094571113586 - trainLoss: 0.43481531739234924\n",
      "cnt: 0 - valLoss: 0.43834996223449707 - trainLoss: 0.4348128139972687\n",
      "cnt: 0 - valLoss: 0.4383489191532135 - trainLoss: 0.43481025099754333\n",
      "cnt: 0 - valLoss: 0.4383479058742523 - trainLoss: 0.43480774760246277\n",
      "cnt: 0 - valLoss: 0.43834689259529114 - trainLoss: 0.4348052144050598\n",
      "cnt: 0 - valLoss: 0.43834590911865234 - trainLoss: 0.4348026514053345\n",
      "cnt: 0 - valLoss: 0.43834489583969116 - trainLoss: 0.4348001480102539\n",
      "cnt: 0 - valLoss: 0.4383438527584076 - trainLoss: 0.4347976744174957\n",
      "cnt: 0 - valLoss: 0.4383428692817688 - trainLoss: 0.4347951114177704\n",
      "cnt: 0 - valLoss: 0.43834188580513 - trainLoss: 0.4347926378250122\n",
      "cnt: 0 - valLoss: 0.4383408725261688 - trainLoss: 0.43479010462760925\n",
      "cnt: 0 - valLoss: 0.43833988904953003 - trainLoss: 0.4347875416278839\n",
      "cnt: 0 - valLoss: 0.43833887577056885 - trainLoss: 0.43478500843048096\n",
      "cnt: 0 - valLoss: 0.43833789229393005 - trainLoss: 0.434782475233078\n",
      "cnt: 0 - valLoss: 0.43833687901496887 - trainLoss: 0.4347800016403198\n",
      "cnt: 0 - valLoss: 0.4383358955383301 - trainLoss: 0.43477746844291687\n",
      "cnt: 0 - valLoss: 0.4383348524570465 - trainLoss: 0.43477490544319153\n",
      "cnt: 0 - valLoss: 0.4383338391780853 - trainLoss: 0.43477243185043335\n",
      "cnt: 0 - valLoss: 0.43833285570144653 - trainLoss: 0.4347698390483856\n",
      "cnt: 0 - valLoss: 0.4383319020271301 - trainLoss: 0.43476736545562744\n",
      "cnt: 0 - valLoss: 0.43833082914352417 - trainLoss: 0.4347648024559021\n",
      "cnt: 0 - valLoss: 0.4383298456668854 - trainLoss: 0.4347623288631439\n",
      "cnt: 0 - valLoss: 0.4383288323879242 - trainLoss: 0.4347597658634186\n",
      "cnt: 0 - valLoss: 0.438327819108963 - trainLoss: 0.434757262468338\n",
      "cnt: 0 - valLoss: 0.43832680583000183 - trainLoss: 0.43475469946861267\n",
      "cnt: 0 - valLoss: 0.43832582235336304 - trainLoss: 0.4347521662712097\n",
      "cnt: 0 - valLoss: 0.43832483887672424 - trainLoss: 0.43474966287612915\n",
      "cnt: 0 - valLoss: 0.4383237957954407 - trainLoss: 0.4347471296787262\n",
      "cnt: 0 - valLoss: 0.4383227825164795 - trainLoss: 0.43474459648132324\n",
      "cnt: 0 - valLoss: 0.4383217990398407 - trainLoss: 0.4347420632839203\n",
      "cnt: 0 - valLoss: 0.4383208155632019 - trainLoss: 0.4347395598888397\n",
      "cnt: 0 - valLoss: 0.43831977248191833 - trainLoss: 0.43473705649375916\n",
      "cnt: 0 - valLoss: 0.43831875920295715 - trainLoss: 0.4347345530986786\n",
      "cnt: 0 - valLoss: 0.43831780552864075 - trainLoss: 0.43473201990127563\n",
      "cnt: 0 - valLoss: 0.4383167624473572 - trainLoss: 0.4347294569015503\n",
      "cnt: 0 - valLoss: 0.438315749168396 - trainLoss: 0.4347269535064697\n",
      "cnt: 0 - valLoss: 0.4383147060871124 - trainLoss: 0.43472445011138916\n",
      "cnt: 0 - valLoss: 0.4383137822151184 - trainLoss: 0.43472182750701904\n",
      "cnt: 0 - valLoss: 0.43831273913383484 - trainLoss: 0.43471935391426086\n",
      "cnt: 0 - valLoss: 0.43831172585487366 - trainLoss: 0.4347168505191803\n",
      "cnt: 0 - valLoss: 0.4383106827735901 - trainLoss: 0.43471428751945496\n",
      "cnt: 0 - valLoss: 0.43830975890159607 - trainLoss: 0.4347118139266968\n",
      "cnt: 0 - valLoss: 0.4383087456226349 - trainLoss: 0.43470925092697144\n",
      "cnt: 0 - valLoss: 0.4383077621459961 - trainLoss: 0.43470677733421326\n",
      "cnt: 0 - valLoss: 0.4383067488670349 - trainLoss: 0.4347041845321655\n",
      "cnt: 0 - valLoss: 0.43830570578575134 - trainLoss: 0.43470168113708496\n",
      "cnt: 0 - valLoss: 0.43830469250679016 - trainLoss: 0.434699147939682\n",
      "cnt: 0 - valLoss: 0.43830370903015137 - trainLoss: 0.43469667434692383\n",
      "cnt: 0 - valLoss: 0.4383027255535126 - trainLoss: 0.4346940815448761\n",
      "cnt: 0 - valLoss: 0.438301682472229 - trainLoss: 0.43469157814979553\n",
      "cnt: 0 - valLoss: 0.4383006691932678 - trainLoss: 0.4346890449523926\n",
      "cnt: 0 - valLoss: 0.43829968571662903 - trainLoss: 0.4346865117549896\n",
      "cnt: 0 - valLoss: 0.43829867243766785 - trainLoss: 0.43468400835990906\n",
      "cnt: 0 - valLoss: 0.43829768896102905 - trainLoss: 0.4346814751625061\n",
      "cnt: 0 - valLoss: 0.43829670548439026 - trainLoss: 0.43467894196510315\n",
      "cnt: 0 - valLoss: 0.4382956922054291 - trainLoss: 0.43467646837234497\n",
      "cnt: 0 - valLoss: 0.4382946491241455 - trainLoss: 0.43467390537261963\n",
      "cnt: 0 - valLoss: 0.4382936656475067 - trainLoss: 0.4346713721752167\n",
      "cnt: 0 - valLoss: 0.43829265236854553 - trainLoss: 0.4346688389778137\n",
      "cnt: 0 - valLoss: 0.43829166889190674 - trainLoss: 0.43466636538505554\n",
      "cnt: 0 - valLoss: 0.43829062581062317 - trainLoss: 0.4346638023853302\n",
      "cnt: 0 - valLoss: 0.438289612531662 - trainLoss: 0.43466129899024963\n",
      "cnt: 0 - valLoss: 0.4382886588573456 - trainLoss: 0.4346587359905243\n",
      "cnt: 0 - valLoss: 0.438287615776062 - trainLoss: 0.43465620279312134\n",
      "cnt: 0 - valLoss: 0.43828660249710083 - trainLoss: 0.43465369939804077\n",
      "cnt: 0 - valLoss: 0.43828555941581726 - trainLoss: 0.4346511960029602\n",
      "cnt: 0 - valLoss: 0.4382845461368561 - trainLoss: 0.43464863300323486\n",
      "cnt: 0 - valLoss: 0.4382835626602173 - trainLoss: 0.4346461594104767\n",
      "cnt: 0 - valLoss: 0.4382825195789337 - trainLoss: 0.43464359641075134\n",
      "cnt: 0 - valLoss: 0.43828150629997253 - trainLoss: 0.4346410632133484\n",
      "cnt: 0 - valLoss: 0.43828046321868896 - trainLoss: 0.43463853001594543\n",
      "cnt: 0 - valLoss: 0.4382794499397278 - trainLoss: 0.43463602662086487\n",
      "cnt: 0 - valLoss: 0.4382784962654114 - trainLoss: 0.4346334934234619\n",
      "cnt: 0 - valLoss: 0.4382774829864502 - trainLoss: 0.43463101983070374\n",
      "cnt: 0 - valLoss: 0.4382764399051666 - trainLoss: 0.434628427028656\n",
      "cnt: 0 - valLoss: 0.43827542662620544 - trainLoss: 0.4346259534358978\n",
      "cnt: 0 - valLoss: 0.4382743835449219 - trainLoss: 0.4346233904361725\n",
      "cnt: 0 - valLoss: 0.43827342987060547 - trainLoss: 0.4346209168434143\n",
      "cnt: 0 - valLoss: 0.4382724165916443 - trainLoss: 0.4346183240413666\n",
      "cnt: 0 - valLoss: 0.4382713735103607 - trainLoss: 0.4346158504486084\n",
      "cnt: 0 - valLoss: 0.4382703900337219 - trainLoss: 0.43461331725120544\n",
      "cnt: 0 - valLoss: 0.43826937675476074 - trainLoss: 0.4346108138561249\n",
      "cnt: 0 - valLoss: 0.43826839327812195 - trainLoss: 0.43460822105407715\n",
      "cnt: 0 - valLoss: 0.43826740980148315 - trainLoss: 0.43460574746131897\n",
      "cnt: 0 - valLoss: 0.4382663667201996 - trainLoss: 0.43460318446159363\n",
      "cnt: 0 - valLoss: 0.4382653832435608 - trainLoss: 0.43460071086883545\n",
      "cnt: 0 - valLoss: 0.43826431035995483 - trainLoss: 0.4345982074737549\n",
      "cnt: 0 - valLoss: 0.4382633566856384 - trainLoss: 0.43459564447402954\n",
      "cnt: 0 - valLoss: 0.43826234340667725 - trainLoss: 0.43459317088127136\n",
      "cnt: 0 - valLoss: 0.4382613003253937 - trainLoss: 0.434590607881546\n",
      "cnt: 0 - valLoss: 0.4382603168487549 - trainLoss: 0.43458807468414307\n",
      "cnt: 0 - valLoss: 0.4382593035697937 - trainLoss: 0.4345855414867401\n",
      "cnt: 0 - valLoss: 0.4382583498954773 - trainLoss: 0.43458303809165955\n",
      "cnt: 0 - valLoss: 0.4382573664188385 - trainLoss: 0.4345805048942566\n",
      "cnt: 0 - valLoss: 0.4382563531398773 - trainLoss: 0.4345780313014984\n",
      "cnt: 0 - valLoss: 0.43825531005859375 - trainLoss: 0.4345754384994507\n",
      "cnt: 0 - valLoss: 0.4382542669773102 - trainLoss: 0.4345729649066925\n",
      "cnt: 0 - valLoss: 0.438253253698349 - trainLoss: 0.43457040190696716\n",
      "cnt: 0 - valLoss: 0.4382523000240326 - trainLoss: 0.434567928314209\n",
      "cnt: 0 - valLoss: 0.4382512867450714 - trainLoss: 0.43456533551216125\n",
      "cnt: 0 - valLoss: 0.4382503032684326 - trainLoss: 0.4345628619194031\n",
      "cnt: 0 - valLoss: 0.43824926018714905 - trainLoss: 0.43456029891967773\n",
      "cnt: 0 - valLoss: 0.43824827671051025 - trainLoss: 0.43455782532691956\n",
      "cnt: 0 - valLoss: 0.4382472634315491 - trainLoss: 0.4345552921295166\n",
      "cnt: 0 - valLoss: 0.4382462799549103 - trainLoss: 0.43455275893211365\n",
      "cnt: 0 - valLoss: 0.4382452666759491 - trainLoss: 0.4345502257347107\n",
      "cnt: 0 - valLoss: 0.4382442533969879 - trainLoss: 0.4345477223396301\n",
      "cnt: 0 - valLoss: 0.4382432997226715 - trainLoss: 0.4345451891422272\n",
      "cnt: 0 - valLoss: 0.43824222683906555 - trainLoss: 0.4345426559448242\n",
      "cnt: 0 - valLoss: 0.43824124336242676 - trainLoss: 0.43454018235206604\n",
      "cnt: 0 - valLoss: 0.4382402300834656 - trainLoss: 0.4345376193523407\n",
      "cnt: 0 - valLoss: 0.4382392466068268 - trainLoss: 0.43453508615493774\n",
      "cnt: 0 - valLoss: 0.438238263130188 - trainLoss: 0.4345325529575348\n",
      "cnt: 0 - valLoss: 0.4382372200489044 - trainLoss: 0.4345300495624542\n",
      "cnt: 0 - valLoss: 0.4382362365722656 - trainLoss: 0.43452751636505127\n",
      "cnt: 0 - valLoss: 0.43823525309562683 - trainLoss: 0.4345250427722931\n",
      "cnt: 0 - valLoss: 0.43823421001434326 - trainLoss: 0.43452244997024536\n",
      "cnt: 0 - valLoss: 0.43823328614234924 - trainLoss: 0.4345199763774872\n",
      "cnt: 0 - valLoss: 0.4382322430610657 - trainLoss: 0.43451741337776184\n",
      "cnt: 0 - valLoss: 0.4382312297821045 - trainLoss: 0.43451493978500366\n",
      "cnt: 0 - valLoss: 0.4382302463054657 - trainLoss: 0.43451234698295593\n",
      "cnt: 0 - valLoss: 0.43822920322418213 - trainLoss: 0.43450987339019775\n",
      "cnt: 0 - valLoss: 0.43822821974754333 - trainLoss: 0.4345073103904724\n",
      "cnt: 0 - valLoss: 0.43822723627090454 - trainLoss: 0.43450483679771423\n",
      "cnt: 0 - valLoss: 0.43822619318962097 - trainLoss: 0.43450233340263367\n",
      "cnt: 0 - valLoss: 0.4382252097129822 - trainLoss: 0.4344997704029083\n",
      "cnt: 0 - valLoss: 0.438224196434021 - trainLoss: 0.43449723720550537\n",
      "cnt: 0 - valLoss: 0.4382232427597046 - trainLoss: 0.4344947338104248\n",
      "cnt: 0 - valLoss: 0.4382222294807434 - trainLoss: 0.43449220061302185\n",
      "cnt: 0 - valLoss: 0.43822118639945984 - trainLoss: 0.4344896674156189\n",
      "cnt: 0 - valLoss: 0.43822020292282104 - trainLoss: 0.4344871938228607\n",
      "cnt: 0 - valLoss: 0.43821918964385986 - trainLoss: 0.4344846308231354\n",
      "cnt: 0 - valLoss: 0.43821823596954346 - trainLoss: 0.4344820976257324\n",
      "cnt: 0 - valLoss: 0.4382171928882599 - trainLoss: 0.43447956442832947\n",
      "cnt: 0 - valLoss: 0.4382161796092987 - trainLoss: 0.4344770908355713\n",
      "cnt: 0 - valLoss: 0.4382151961326599 - trainLoss: 0.43447452783584595\n",
      "cnt: 0 - valLoss: 0.43821415305137634 - trainLoss: 0.43447205424308777\n",
      "cnt: 0 - valLoss: 0.43821316957473755 - trainLoss: 0.43446946144104004\n",
      "cnt: 0 - valLoss: 0.43821218609809875 - trainLoss: 0.43446698784828186\n",
      "cnt: 0 - valLoss: 0.4382111728191376 - trainLoss: 0.4344645142555237\n",
      "cnt: 0 - valLoss: 0.4382100999355316 - trainLoss: 0.43446195125579834\n",
      "cnt: 0 - valLoss: 0.43820905685424805 - trainLoss: 0.4344594478607178\n",
      "cnt: 0 - valLoss: 0.4382079839706421 - trainLoss: 0.4344569444656372\n",
      "cnt: 0 - valLoss: 0.4382069706916809 - trainLoss: 0.43445441126823425\n",
      "cnt: 0 - valLoss: 0.43820592761039734 - trainLoss: 0.4344518482685089\n",
      "cnt: 0 - valLoss: 0.43820491433143616 - trainLoss: 0.43444934487342834\n",
      "cnt: 0 - valLoss: 0.4382038712501526 - trainLoss: 0.4344468414783478\n",
      "cnt: 0 - valLoss: 0.4382028579711914 - trainLoss: 0.4344443082809448\n",
      "cnt: 0 - valLoss: 0.43820181488990784 - trainLoss: 0.43444183468818665\n",
      "cnt: 0 - valLoss: 0.43820080161094666 - trainLoss: 0.4344392418861389\n",
      "cnt: 0 - valLoss: 0.4381997287273407 - trainLoss: 0.43443676829338074\n",
      "cnt: 0 - valLoss: 0.43819865584373474 - trainLoss: 0.4344342350959778\n",
      "cnt: 0 - valLoss: 0.43819761276245117 - trainLoss: 0.4344317317008972\n",
      "cnt: 0 - valLoss: 0.4381966292858124 - trainLoss: 0.43442919850349426\n",
      "cnt: 0 - valLoss: 0.4381955564022064 - trainLoss: 0.4344266653060913\n",
      "cnt: 0 - valLoss: 0.4381945729255676 - trainLoss: 0.43442413210868835\n",
      "cnt: 0 - valLoss: 0.43819350004196167 - trainLoss: 0.4344216287136078\n",
      "cnt: 0 - valLoss: 0.4381924867630005 - trainLoss: 0.4344191253185272\n",
      "cnt: 0 - valLoss: 0.4381915032863617 - trainLoss: 0.4344165623188019\n",
      "cnt: 0 - valLoss: 0.4381903409957886 - trainLoss: 0.4344140291213989\n",
      "cnt: 0 - valLoss: 0.43818920850753784 - trainLoss: 0.43441152572631836\n",
      "cnt: 0 - valLoss: 0.4381881058216095 - trainLoss: 0.43440890312194824\n",
      "cnt: 0 - valLoss: 0.43818700313568115 - trainLoss: 0.4344063103199005\n",
      "cnt: 0 - valLoss: 0.4381858706474304 - trainLoss: 0.4344037175178528\n",
      "cnt: 0 - valLoss: 0.43818479776382446 - trainLoss: 0.43440115451812744\n",
      "cnt: 0 - valLoss: 0.4381836950778961 - trainLoss: 0.4343985915184021\n",
      "cnt: 0 - valLoss: 0.43818262219429016 - trainLoss: 0.43439602851867676\n",
      "cnt: 0 - valLoss: 0.43818148970603943 - trainLoss: 0.4343934655189514\n",
      "cnt: 0 - valLoss: 0.4381803870201111 - trainLoss: 0.4343909025192261\n",
      "cnt: 0 - valLoss: 0.4381793141365051 - trainLoss: 0.4343882203102112\n",
      "cnt: 0 - valLoss: 0.4381782114505768 - trainLoss: 0.43438565731048584\n",
      "cnt: 0 - valLoss: 0.4381771385669708 - trainLoss: 0.4343831539154053\n",
      "cnt: 0 - valLoss: 0.4381759762763977 - trainLoss: 0.43438053131103516\n",
      "cnt: 0 - valLoss: 0.43817490339279175 - trainLoss: 0.4343779683113098\n",
      "cnt: 0 - valLoss: 0.4381738305091858 - trainLoss: 0.4343754053115845\n",
      "cnt: 0 - valLoss: 0.43817272782325745 - trainLoss: 0.43437281250953674\n",
      "cnt: 0 - valLoss: 0.4381715655326843 - trainLoss: 0.4343702495098114\n",
      "cnt: 0 - valLoss: 0.4381704330444336 - trainLoss: 0.43436768651008606\n",
      "cnt: 0 - valLoss: 0.43816933035850525 - trainLoss: 0.43436506390571594\n",
      "cnt: 0 - valLoss: 0.4381682872772217 - trainLoss: 0.4343624711036682\n",
      "cnt: 0 - valLoss: 0.43816718459129333 - trainLoss: 0.43435990810394287\n",
      "cnt: 0 - valLoss: 0.4381661117076874 - trainLoss: 0.43435734510421753\n",
      "cnt: 0 - valLoss: 0.43816500902175903 - trainLoss: 0.4343547523021698\n",
      "cnt: 0 - valLoss: 0.4381638765335083 - trainLoss: 0.43435218930244446\n",
      "cnt: 0 - valLoss: 0.43816277384757996 - trainLoss: 0.43434959650039673\n",
      "cnt: 0 - valLoss: 0.438161700963974 - trainLoss: 0.434347003698349\n",
      "cnt: 0 - valLoss: 0.43816062808036804 - trainLoss: 0.43434441089630127\n",
      "cnt: 0 - valLoss: 0.4381595253944397 - trainLoss: 0.4343418776988983\n",
      "cnt: 0 - valLoss: 0.43815839290618896 - trainLoss: 0.4343392550945282\n",
      "cnt: 0 - valLoss: 0.438157320022583 - trainLoss: 0.43433669209480286\n",
      "cnt: 0 - valLoss: 0.43815621733665466 - trainLoss: 0.4343341290950775\n",
      "cnt: 0 - valLoss: 0.43815505504608154 - trainLoss: 0.4343315660953522\n",
      "cnt: 0 - valLoss: 0.4381538927555084 - trainLoss: 0.4343288540840149\n",
      "cnt: 0 - valLoss: 0.4381527006626129 - trainLoss: 0.43432626128196716\n",
      "cnt: 0 - valLoss: 0.4381515383720398 - trainLoss: 0.4343235194683075\n",
      "cnt: 0 - valLoss: 0.4381503760814667 - trainLoss: 0.434320867061615\n",
      "cnt: 0 - valLoss: 0.43814921379089355 - trainLoss: 0.4343182146549225\n",
      "cnt: 0 - valLoss: 0.4381480813026428 - trainLoss: 0.43431556224823\n",
      "cnt: 0 - valLoss: 0.4381469488143921 - trainLoss: 0.4343129098415375\n",
      "cnt: 0 - valLoss: 0.43814584612846375 - trainLoss: 0.4343101978302002\n",
      "cnt: 0 - valLoss: 0.4381446838378906 - trainLoss: 0.4343075156211853\n",
      "cnt: 0 - valLoss: 0.4381435215473175 - trainLoss: 0.4343048930168152\n",
      "cnt: 0 - valLoss: 0.43814241886138916 - trainLoss: 0.4343022108078003\n",
      "cnt: 0 - valLoss: 0.43814125657081604 - trainLoss: 0.4342995584011078\n",
      "cnt: 0 - valLoss: 0.4381400942802429 - trainLoss: 0.4342969059944153\n",
      "cnt: 0 - valLoss: 0.4381389915943146 - trainLoss: 0.4342942535877228\n",
      "cnt: 0 - valLoss: 0.43813782930374146 - trainLoss: 0.4342916011810303\n",
      "cnt: 0 - valLoss: 0.43813666701316833 - trainLoss: 0.43428894877433777\n",
      "cnt: 0 - valLoss: 0.43813556432724 - trainLoss: 0.4342862367630005\n",
      "cnt: 0 - valLoss: 0.43813443183898926 - trainLoss: 0.43428361415863037\n",
      "cnt: 0 - valLoss: 0.4381333291530609 - trainLoss: 0.43428096175193787\n",
      "cnt: 0 - valLoss: 0.4381321370601654 - trainLoss: 0.434278279542923\n",
      "cnt: 0 - valLoss: 0.4381310045719147 - trainLoss: 0.43427565693855286\n",
      "cnt: 0 - valLoss: 0.43812990188598633 - trainLoss: 0.4342729151248932\n",
      "cnt: 0 - valLoss: 0.4381287097930908 - trainLoss: 0.4342702627182007\n",
      "cnt: 0 - valLoss: 0.4381275773048401 - trainLoss: 0.4342676103115082\n",
      "cnt: 0 - valLoss: 0.43812647461891174 - trainLoss: 0.4342649579048157\n",
      "cnt: 0 - valLoss: 0.438125342130661 - trainLoss: 0.43426230549812317\n",
      "cnt: 0 - valLoss: 0.4381242096424103 - trainLoss: 0.43425965309143066\n",
      "cnt: 0 - valLoss: 0.43812304735183716 - trainLoss: 0.43425700068473816\n",
      "cnt: 0 - valLoss: 0.4381219148635864 - trainLoss: 0.43425434827804565\n",
      "cnt: 0 - valLoss: 0.4381208121776581 - trainLoss: 0.43425166606903076\n",
      "cnt: 0 - valLoss: 0.43811970949172974 - trainLoss: 0.43424901366233826\n",
      "cnt: 0 - valLoss: 0.43811848759651184 - trainLoss: 0.434246301651001\n",
      "cnt: 0 - valLoss: 0.4381173849105835 - trainLoss: 0.43424364924430847\n",
      "cnt: 0 - valLoss: 0.4381162226200104 - trainLoss: 0.43424099683761597\n",
      "cnt: 0 - valLoss: 0.43811511993408203 - trainLoss: 0.43423834443092346\n",
      "cnt: 0 - valLoss: 0.4381139874458313 - trainLoss: 0.43423569202423096\n",
      "cnt: 0 - valLoss: 0.4381128251552582 - trainLoss: 0.43423303961753845\n",
      "cnt: 0 - valLoss: 0.43811172246932983 - trainLoss: 0.43423038721084595\n",
      "cnt: 0 - valLoss: 0.4381106197834015 - trainLoss: 0.43422767519950867\n",
      "cnt: 0 - valLoss: 0.4381093978881836 - trainLoss: 0.43422502279281616\n",
      "cnt: 0 - valLoss: 0.43810829520225525 - trainLoss: 0.43422237038612366\n",
      "cnt: 0 - valLoss: 0.4381071925163269 - trainLoss: 0.43421971797943115\n",
      "cnt: 0 - valLoss: 0.43810606002807617 - trainLoss: 0.43421706557273865\n",
      "cnt: 0 - valLoss: 0.4381049573421478 - trainLoss: 0.43421441316604614\n",
      "cnt: 0 - valLoss: 0.4381038248538971 - trainLoss: 0.43421173095703125\n",
      "cnt: 0 - valLoss: 0.43810269236564636 - trainLoss: 0.43420907855033875\n",
      "cnt: 0 - valLoss: 0.43810153007507324 - trainLoss: 0.43420636653900146\n",
      "cnt: 0 - valLoss: 0.4381003975868225 - trainLoss: 0.43420377373695374\n",
      "cnt: 0 - valLoss: 0.43809929490089417 - trainLoss: 0.43420109152793884\n",
      "cnt: 0 - valLoss: 0.4380982220172882 - trainLoss: 0.43419840931892395\n",
      "cnt: 0 - valLoss: 0.4380970597267151 - trainLoss: 0.43419575691223145\n",
      "cnt: 0 - valLoss: 0.43809595704078674 - trainLoss: 0.43419304490089417\n",
      "cnt: 0 - valLoss: 0.4380947947502136 - trainLoss: 0.43419039249420166\n",
      "cnt: 0 - valLoss: 0.4380936324596405 - trainLoss: 0.43418774008750916\n",
      "cnt: 0 - valLoss: 0.43809252977371216 - trainLoss: 0.43418508768081665\n",
      "cnt: 0 - valLoss: 0.4380914568901062 - trainLoss: 0.43418243527412415\n",
      "cnt: 0 - valLoss: 0.43809032440185547 - trainLoss: 0.43417978286743164\n",
      "cnt: 0 - valLoss: 0.43808919191360474 - trainLoss: 0.43417713046073914\n",
      "cnt: 0 - valLoss: 0.438088059425354 - trainLoss: 0.43417447805404663\n",
      "cnt: 0 - valLoss: 0.43808695673942566 - trainLoss: 0.4341718256473541\n",
      "cnt: 0 - valLoss: 0.4380858242511749 - trainLoss: 0.4341691732406616\n",
      "cnt: 0 - valLoss: 0.4380846917629242 - trainLoss: 0.43416649103164673\n",
      "cnt: 0 - valLoss: 0.43808361887931824 - trainLoss: 0.4341638386249542\n",
      "cnt: 0 - valLoss: 0.4380824863910675 - trainLoss: 0.43416112661361694\n",
      "cnt: 0 - valLoss: 0.43808138370513916 - trainLoss: 0.43415847420692444\n",
      "cnt: 0 - valLoss: 0.43808022141456604 - trainLoss: 0.43415582180023193\n",
      "cnt: 0 - valLoss: 0.4380790591239929 - trainLoss: 0.43415316939353943\n",
      "cnt: 0 - valLoss: 0.4380779564380646 - trainLoss: 0.4341505169868469\n",
      "cnt: 0 - valLoss: 0.43807682394981384 - trainLoss: 0.4341478645801544\n",
      "cnt: 0 - valLoss: 0.4380757510662079 - trainLoss: 0.4341452121734619\n",
      "cnt: 0 - valLoss: 0.43807464838027954 - trainLoss: 0.43414250016212463\n",
      "cnt: 0 - valLoss: 0.4380735456943512 - trainLoss: 0.4341399073600769\n",
      "cnt: 0 - valLoss: 0.4380723834037781 - trainLoss: 0.4341371953487396\n",
      "cnt: 0 - valLoss: 0.43807125091552734 - trainLoss: 0.43413451313972473\n",
      "cnt: 0 - valLoss: 0.438070148229599 - trainLoss: 0.4341318607330322\n",
      "cnt: 0 - valLoss: 0.43806904554367065 - trainLoss: 0.4341292083263397\n",
      "cnt: 0 - valLoss: 0.4380679130554199 - trainLoss: 0.4341265559196472\n",
      "cnt: 0 - valLoss: 0.43806684017181396 - trainLoss: 0.4341239035129547\n",
      "cnt: 0 - valLoss: 0.438065767288208 - trainLoss: 0.4341212511062622\n",
      "cnt: 0 - valLoss: 0.4380646049976349 - trainLoss: 0.4341185986995697\n",
      "cnt: 0 - valLoss: 0.43806350231170654 - trainLoss: 0.4341158866882324\n",
      "cnt: 0 - valLoss: 0.4380623996257782 - trainLoss: 0.43411320447921753\n",
      "cnt: 0 - valLoss: 0.43806132674217224 - trainLoss: 0.43411052227020264\n",
      "cnt: 0 - valLoss: 0.4380601942539215 - trainLoss: 0.43410786986351013\n",
      "cnt: 0 - valLoss: 0.43805909156799316 - trainLoss: 0.4341052174568176\n",
      "cnt: 0 - valLoss: 0.4380579888820648 - trainLoss: 0.4341025650501251\n",
      "cnt: 0 - valLoss: 0.43805691599845886 - trainLoss: 0.4340999126434326\n",
      "cnt: 0 - valLoss: 0.43805575370788574 - trainLoss: 0.4340972602367401\n",
      "cnt: 0 - valLoss: 0.438054621219635 - trainLoss: 0.4340946078300476\n",
      "cnt: 0 - valLoss: 0.43805354833602905 - trainLoss: 0.4340919256210327\n",
      "cnt: 0 - valLoss: 0.4380524456501007 - trainLoss: 0.43408921360969543\n",
      "cnt: 0 - valLoss: 0.4380512833595276 - trainLoss: 0.4340866208076477\n",
      "cnt: 0 - valLoss: 0.43805018067359924 - trainLoss: 0.4340839087963104\n",
      "cnt: 0 - valLoss: 0.4380490183830261 - trainLoss: 0.4340813457965851\n",
      "cnt: 0 - valLoss: 0.438047856092453 - trainLoss: 0.4340786635875702\n",
      "cnt: 0 - valLoss: 0.4380466938018799 - trainLoss: 0.4340759813785553\n",
      "cnt: 0 - valLoss: 0.43804553151130676 - trainLoss: 0.43407338857650757\n",
      "cnt: 0 - valLoss: 0.4380444288253784 - trainLoss: 0.4340706765651703\n",
      "cnt: 0 - valLoss: 0.4380432665348053 - trainLoss: 0.43406808376312256\n",
      "cnt: 0 - valLoss: 0.4380421042442322 - trainLoss: 0.43406543135643005\n",
      "cnt: 0 - valLoss: 0.43804091215133667 - trainLoss: 0.4340627193450928\n",
      "cnt: 0 - valLoss: 0.4380398094654083 - trainLoss: 0.43406006693840027\n",
      "cnt: 0 - valLoss: 0.4380386769771576 - trainLoss: 0.43405747413635254\n",
      "cnt: 0 - valLoss: 0.4380375146865845 - trainLoss: 0.43405482172966003\n",
      "cnt: 0 - valLoss: 0.43803638219833374 - trainLoss: 0.43405210971832275\n",
      "cnt: 0 - valLoss: 0.4380352199077606 - trainLoss: 0.43404945731163025\n",
      "cnt: 0 - valLoss: 0.4380340576171875 - trainLoss: 0.43404683470726013\n",
      "cnt: 0 - valLoss: 0.4380328953266144 - trainLoss: 0.43404415249824524\n",
      "cnt: 0 - valLoss: 0.43803173303604126 - trainLoss: 0.4340415298938751\n",
      "cnt: 0 - valLoss: 0.4380306303501129 - trainLoss: 0.43403884768486023\n",
      "cnt: 0 - valLoss: 0.43802952766418457 - trainLoss: 0.4340362250804901\n",
      "cnt: 0 - valLoss: 0.43802836537361145 - trainLoss: 0.4340335726737976\n",
      "cnt: 0 - valLoss: 0.43802720308303833 - trainLoss: 0.4340309202671051\n",
      "cnt: 0 - valLoss: 0.4380260705947876 - trainLoss: 0.4340282678604126\n",
      "cnt: 0 - valLoss: 0.43802496790885925 - trainLoss: 0.4340256154537201\n",
      "cnt: 0 - valLoss: 0.43802377581596375 - trainLoss: 0.4340229630470276\n",
      "cnt: 0 - valLoss: 0.438022643327713 - trainLoss: 0.4340203106403351\n",
      "cnt: 0 - valLoss: 0.43802154064178467 - trainLoss: 0.4340176582336426\n",
      "cnt: 0 - valLoss: 0.43802034854888916 - trainLoss: 0.4340150058269501\n",
      "cnt: 0 - valLoss: 0.4380192756652832 - trainLoss: 0.43401235342025757\n",
      "cnt: 0 - valLoss: 0.4380181133747101 - trainLoss: 0.43400970101356506\n",
      "cnt: 0 - valLoss: 0.43801701068878174 - trainLoss: 0.43400704860687256\n",
      "cnt: 0 - valLoss: 0.43801578879356384 - trainLoss: 0.43400439620018005\n",
      "cnt: 0 - valLoss: 0.4380146861076355 - trainLoss: 0.43400174379348755\n",
      "cnt: 0 - valLoss: 0.4380135238170624 - trainLoss: 0.43399909138679504\n",
      "cnt: 0 - valLoss: 0.43801242113113403 - trainLoss: 0.43399643898010254\n",
      "cnt: 0 - valLoss: 0.4380112886428833 - trainLoss: 0.43399378657341003\n",
      "cnt: 0 - valLoss: 0.4380101263523102 - trainLoss: 0.43399113416671753\n",
      "cnt: 0 - valLoss: 0.43800899386405945 - trainLoss: 0.43398845195770264\n",
      "cnt: 0 - valLoss: 0.43800783157348633 - trainLoss: 0.43398579955101013\n",
      "cnt: 0 - valLoss: 0.43800675868988037 - trainLoss: 0.4339831471443176\n",
      "cnt: 0 - valLoss: 0.43800562620162964 - trainLoss: 0.4339805543422699\n",
      "cnt: 0 - valLoss: 0.4380044937133789 - trainLoss: 0.4339779019355774\n",
      "cnt: 0 - valLoss: 0.4380033612251282 - trainLoss: 0.4339752495288849\n",
      "cnt: 0 - valLoss: 0.43800216913223267 - trainLoss: 0.4339725971221924\n",
      "cnt: 0 - valLoss: 0.4380010962486267 - trainLoss: 0.4339699447154999\n",
      "cnt: 0 - valLoss: 0.43799999356269836 - trainLoss: 0.4339672923088074\n",
      "cnt: 0 - valLoss: 0.43799883127212524 - trainLoss: 0.4339645802974701\n",
      "cnt: 0 - valLoss: 0.4379977583885193 - trainLoss: 0.4339619278907776\n",
      "cnt: 0 - valLoss: 0.43799659609794617 - trainLoss: 0.43395930528640747\n",
      "cnt: 0 - valLoss: 0.4379954934120178 - trainLoss: 0.43395671248435974\n",
      "cnt: 0 - valLoss: 0.4379943609237671 - trainLoss: 0.43395406007766724\n",
      "cnt: 0 - valLoss: 0.43799325823783875 - trainLoss: 0.43395134806632996\n",
      "cnt: 0 - valLoss: 0.437992125749588 - trainLoss: 0.4339487552642822\n",
      "cnt: 0 - valLoss: 0.43799102306365967 - trainLoss: 0.43394604325294495\n",
      "cnt: 0 - valLoss: 0.4379899203777313 - trainLoss: 0.43394339084625244\n",
      "cnt: 0 - valLoss: 0.4379887878894806 - trainLoss: 0.43394073843955994\n",
      "cnt: 0 - valLoss: 0.43798768520355225 - trainLoss: 0.4339381456375122\n",
      "cnt: 0 - valLoss: 0.4379865229129791 - trainLoss: 0.4339354336261749\n",
      "cnt: 0 - valLoss: 0.43798545002937317 - trainLoss: 0.4339327812194824\n",
      "cnt: 0 - valLoss: 0.4379843473434448 - trainLoss: 0.4339301586151123\n",
      "cnt: 0 - valLoss: 0.4379832148551941 - trainLoss: 0.4339275062084198\n",
      "cnt: 0 - valLoss: 0.43798211216926575 - trainLoss: 0.4339248538017273\n",
      "cnt: 0 - valLoss: 0.4379810094833374 - trainLoss: 0.4339222013950348\n",
      "cnt: 0 - valLoss: 0.43797987699508667 - trainLoss: 0.4339195489883423\n",
      "cnt: 0 - valLoss: 0.4379787743091583 - trainLoss: 0.4339168667793274\n",
      "cnt: 0 - valLoss: 0.4379776418209076 - trainLoss: 0.4339142441749573\n",
      "cnt: 0 - valLoss: 0.43797653913497925 - trainLoss: 0.43391159176826477\n",
      "cnt: 0 - valLoss: 0.4379754364490509 - trainLoss: 0.43390893936157227\n",
      "cnt: 0 - valLoss: 0.43797430396080017 - trainLoss: 0.43390628695487976\n",
      "cnt: 0 - valLoss: 0.4379732012748718 - trainLoss: 0.43390363454818726\n",
      "cnt: 0 - valLoss: 0.4379720687866211 - trainLoss: 0.43390098214149475\n",
      "cnt: 0 - valLoss: 0.43797099590301514 - trainLoss: 0.43389832973480225\n",
      "cnt: 0 - valLoss: 0.4379698932170868 - trainLoss: 0.43389567732810974\n",
      "cnt: 0 - valLoss: 0.43796879053115845 - trainLoss: 0.43389302492141724\n",
      "cnt: 0 - valLoss: 0.4379676580429077 - trainLoss: 0.43389037251472473\n",
      "cnt: 0 - valLoss: 0.4379664957523346 - trainLoss: 0.4338877201080322\n",
      "cnt: 0 - valLoss: 0.43796542286872864 - trainLoss: 0.4338850677013397\n",
      "cnt: 0 - valLoss: 0.4379643201828003 - trainLoss: 0.4338824152946472\n",
      "cnt: 0 - valLoss: 0.43796324729919434 - trainLoss: 0.4338797628879547\n",
      "cnt: 0 - valLoss: 0.4379620850086212 - trainLoss: 0.4338771104812622\n",
      "cnt: 0 - valLoss: 0.43796101212501526 - trainLoss: 0.4338744580745697\n",
      "cnt: 0 - valLoss: 0.4379599094390869 - trainLoss: 0.4338717758655548\n",
      "cnt: 0 - valLoss: 0.43795880675315857 - trainLoss: 0.4338691234588623\n",
      "cnt: 0 - valLoss: 0.43795767426490784 - trainLoss: 0.4338664710521698\n",
      "cnt: 0 - valLoss: 0.4379565715789795 - trainLoss: 0.4338638186454773\n",
      "cnt: 0 - valLoss: 0.4379555284976959 - trainLoss: 0.4338611662387848\n",
      "cnt: 0 - valLoss: 0.4379543662071228 - trainLoss: 0.4338585138320923\n",
      "cnt: 0 - valLoss: 0.43795329332351685 - trainLoss: 0.4338558614253998\n",
      "cnt: 0 - valLoss: 0.4379521906375885 - trainLoss: 0.4338532090187073\n",
      "cnt: 0 - valLoss: 0.43795108795166016 - trainLoss: 0.43385055661201477\n",
      "cnt: 0 - valLoss: 0.4379499554634094 - trainLoss: 0.43384790420532227\n",
      "cnt: 0 - valLoss: 0.4379488527774811 - trainLoss: 0.43384528160095215\n",
      "cnt: 0 - valLoss: 0.4379478096961975 - trainLoss: 0.43384259939193726\n",
      "cnt: 0 - valLoss: 0.43794670701026917 - trainLoss: 0.43383994698524475\n",
      "cnt: 0 - valLoss: 0.4379456043243408 - trainLoss: 0.43383732438087463\n",
      "cnt: 0 - valLoss: 0.4379444718360901 - trainLoss: 0.43383467197418213\n",
      "cnt: 0 - valLoss: 0.43794336915016174 - trainLoss: 0.43383198976516724\n",
      "cnt: 0 - valLoss: 0.4379422962665558 - trainLoss: 0.4338293671607971\n",
      "cnt: 0 - valLoss: 0.43794116377830505 - trainLoss: 0.4338267147541046\n",
      "cnt: 0 - valLoss: 0.4379400908946991 - trainLoss: 0.4338240325450897\n",
      "cnt: 0 - valLoss: 0.4379390478134155 - trainLoss: 0.4338214099407196\n",
      "cnt: 0 - valLoss: 0.4379378855228424 - trainLoss: 0.4338187575340271\n",
      "cnt: 0 - valLoss: 0.43793681263923645 - trainLoss: 0.4338161051273346\n",
      "cnt: 0 - valLoss: 0.4379357397556305 - trainLoss: 0.4338134229183197\n",
      "cnt: 0 - valLoss: 0.43793460726737976 - trainLoss: 0.4338107407093048\n",
      "cnt: 0 - valLoss: 0.4379335045814514 - trainLoss: 0.4338081479072571\n",
      "cnt: 0 - valLoss: 0.43793243169784546 - trainLoss: 0.4338054955005646\n",
      "cnt: 0 - valLoss: 0.43793126940727234 - trainLoss: 0.4338027834892273\n",
      "cnt: 0 - valLoss: 0.43793025612831116 - trainLoss: 0.4338001310825348\n",
      "cnt: 0 - valLoss: 0.4379291236400604 - trainLoss: 0.4337974786758423\n",
      "cnt: 0 - valLoss: 0.4379280209541321 - trainLoss: 0.43379488587379456\n",
      "cnt: 0 - valLoss: 0.4379269480705261 - trainLoss: 0.43379223346710205\n",
      "cnt: 0 - valLoss: 0.43792587518692017 - trainLoss: 0.43378958106040955\n",
      "cnt: 0 - valLoss: 0.4379247725009918 - trainLoss: 0.43378692865371704\n",
      "cnt: 0 - valLoss: 0.4379236400127411 - trainLoss: 0.4337843060493469\n",
      "cnt: 0 - valLoss: 0.43792253732681274 - trainLoss: 0.43378159403800964\n",
      "cnt: 0 - valLoss: 0.4379214942455292 - trainLoss: 0.43377894163131714\n",
      "cnt: 0 - valLoss: 0.43792039155960083 - trainLoss: 0.43377628922462463\n",
      "cnt: 0 - valLoss: 0.4379193186759949 - trainLoss: 0.43377363681793213\n",
      "cnt: 0 - valLoss: 0.43791821599006653 - trainLoss: 0.4337710440158844\n",
      "cnt: 0 - valLoss: 0.43791714310646057 - trainLoss: 0.4337683320045471\n",
      "cnt: 0 - valLoss: 0.43791601061820984 - trainLoss: 0.4337657392024994\n",
      "cnt: 0 - valLoss: 0.4379149079322815 - trainLoss: 0.4337630867958069\n",
      "cnt: 0 - valLoss: 0.43791377544403076 - trainLoss: 0.4337603747844696\n",
      "cnt: 0 - valLoss: 0.4379126727581024 - trainLoss: 0.4337577521800995\n",
      "cnt: 0 - valLoss: 0.43791162967681885 - trainLoss: 0.433755099773407\n",
      "cnt: 0 - valLoss: 0.4379105269908905 - trainLoss: 0.4337524473667145\n",
      "cnt: 0 - valLoss: 0.43790942430496216 - trainLoss: 0.433749794960022\n",
      "cnt: 0 - valLoss: 0.4379082918167114 - trainLoss: 0.43374714255332947\n",
      "cnt: 0 - valLoss: 0.43790721893310547 - trainLoss: 0.43374449014663696\n",
      "cnt: 0 - valLoss: 0.4379061162471771 - trainLoss: 0.43374183773994446\n",
      "cnt: 0 - valLoss: 0.43790504336357117 - trainLoss: 0.43373918533325195\n",
      "cnt: 0 - valLoss: 0.4379039406776428 - trainLoss: 0.43373653292655945\n",
      "cnt: 0 - valLoss: 0.4379028081893921 - trainLoss: 0.43373388051986694\n",
      "cnt: 0 - valLoss: 0.4379017949104309 - trainLoss: 0.43373122811317444\n",
      "cnt: 0 - valLoss: 0.4379006624221802 - trainLoss: 0.43372857570648193\n",
      "cnt: 0 - valLoss: 0.4378995895385742 - trainLoss: 0.43372592329978943\n",
      "cnt: 0 - valLoss: 0.43789851665496826 - trainLoss: 0.4337232708930969\n",
      "cnt: 0 - valLoss: 0.4378974139690399 - trainLoss: 0.4337206482887268\n",
      "cnt: 0 - valLoss: 0.4378963112831116 - trainLoss: 0.4337179660797119\n",
      "cnt: 0 - valLoss: 0.43789517879486084 - trainLoss: 0.4337153136730194\n",
      "cnt: 0 - valLoss: 0.43789416551589966 - trainLoss: 0.4337126910686493\n",
      "cnt: 0 - valLoss: 0.4378930330276489 - trainLoss: 0.4337100386619568\n",
      "cnt: 0 - valLoss: 0.43789201974868774 - trainLoss: 0.4337073564529419\n",
      "cnt: 0 - valLoss: 0.437890887260437 - trainLoss: 0.4337047040462494\n",
      "cnt: 0 - valLoss: 0.43788978457450867 - trainLoss: 0.4337020516395569\n",
      "cnt: 0 - valLoss: 0.4378887116909027 - trainLoss: 0.43369942903518677\n",
      "cnt: 0 - valLoss: 0.43788763880729675 - trainLoss: 0.4336967170238495\n",
      "cnt: 0 - valLoss: 0.4378865361213684 - trainLoss: 0.43369412422180176\n",
      "cnt: 0 - valLoss: 0.4378854036331177 - trainLoss: 0.43369147181510925\n",
      "cnt: 0 - valLoss: 0.4378843903541565 - trainLoss: 0.43368884921073914\n",
      "cnt: 0 - valLoss: 0.43788325786590576 - trainLoss: 0.43368616700172424\n",
      "cnt: 0 - valLoss: 0.4378821551799774 - trainLoss: 0.4336835443973541\n",
      "cnt: 0 - valLoss: 0.43788111209869385 - trainLoss: 0.43368086218833923\n",
      "cnt: 0 - valLoss: 0.4378800094127655 - trainLoss: 0.4336782395839691\n",
      "cnt: 0 - valLoss: 0.4378789961338043 - trainLoss: 0.4336755573749542\n",
      "cnt: 0 - valLoss: 0.4378778636455536 - trainLoss: 0.4336729049682617\n",
      "cnt: 0 - valLoss: 0.43787676095962524 - trainLoss: 0.4336702227592468\n",
      "cnt: 0 - valLoss: 0.4378756880760193 - trainLoss: 0.4336675703525543\n",
      "cnt: 0 - valLoss: 0.43787461519241333 - trainLoss: 0.4336649775505066\n",
      "cnt: 0 - valLoss: 0.4378734827041626 - trainLoss: 0.4336622655391693\n",
      "cnt: 0 - valLoss: 0.43787243962287903 - trainLoss: 0.4336596727371216\n",
      "cnt: 0 - valLoss: 0.43787136673927307 - trainLoss: 0.4336569607257843\n",
      "cnt: 0 - valLoss: 0.4378703236579895 - trainLoss: 0.4336543083190918\n",
      "cnt: 0 - valLoss: 0.43786922097206116 - trainLoss: 0.43365171551704407\n",
      "cnt: 0 - valLoss: 0.4378681480884552 - trainLoss: 0.4336490035057068\n",
      "cnt: 0 - valLoss: 0.43786707520484924 - trainLoss: 0.43364641070365906\n",
      "cnt: 0 - valLoss: 0.4378659427165985 - trainLoss: 0.4336436986923218\n",
      "cnt: 0 - valLoss: 0.43786486983299255 - trainLoss: 0.4336410462856293\n",
      "cnt: 0 - valLoss: 0.4378637969493866 - trainLoss: 0.43363842368125916\n",
      "cnt: 0 - valLoss: 0.437862753868103 - trainLoss: 0.43363577127456665\n",
      "cnt: 0 - valLoss: 0.4378616213798523 - trainLoss: 0.43363311886787415\n",
      "cnt: 0 - valLoss: 0.43786054849624634 - trainLoss: 0.43363046646118164\n",
      "cnt: 0 - valLoss: 0.437859445810318 - trainLoss: 0.43362781405448914\n",
      "cnt: 0 - valLoss: 0.4378584027290344 - trainLoss: 0.43362516164779663\n",
      "cnt: 0 - valLoss: 0.43785738945007324 - trainLoss: 0.4336225092411041\n",
      "cnt: 0 - valLoss: 0.4378562569618225 - trainLoss: 0.4336198568344116\n",
      "cnt: 0 - valLoss: 0.43785515427589417 - trainLoss: 0.4336172044277191\n",
      "cnt: 0 - valLoss: 0.4378540813922882 - trainLoss: 0.4336145520210266\n",
      "cnt: 0 - valLoss: 0.43785300850868225 - trainLoss: 0.4336118996143341\n",
      "cnt: 0 - valLoss: 0.4378519356250763 - trainLoss: 0.4336092472076416\n",
      "cnt: 0 - valLoss: 0.43785086274147034 - trainLoss: 0.4336065948009491\n",
      "cnt: 0 - valLoss: 0.437849760055542 - trainLoss: 0.4336039423942566\n",
      "cnt: 0 - valLoss: 0.43784868717193604 - trainLoss: 0.4336012899875641\n",
      "cnt: 0 - valLoss: 0.4378476142883301 - trainLoss: 0.4335986375808716\n",
      "cnt: 0 - valLoss: 0.4378465712070465 - trainLoss: 0.4335959851741791\n",
      "cnt: 0 - valLoss: 0.43784549832344055 - trainLoss: 0.4335933327674866\n",
      "cnt: 0 - valLoss: 0.4378444254398346 - trainLoss: 0.43359068036079407\n",
      "cnt: 0 - valLoss: 0.43784335255622864 - trainLoss: 0.43358802795410156\n",
      "cnt: 0 - valLoss: 0.4378422796726227 - trainLoss: 0.43358537554740906\n",
      "cnt: 0 - valLoss: 0.4378412365913391 - trainLoss: 0.43358269333839417\n",
      "cnt: 0 - valLoss: 0.4378401041030884 - trainLoss: 0.43358004093170166\n",
      "cnt: 0 - valLoss: 0.4378390312194824 - trainLoss: 0.43357738852500916\n",
      "cnt: 0 - valLoss: 0.43783801794052124 - trainLoss: 0.43357473611831665\n",
      "cnt: 0 - valLoss: 0.4378368854522705 - trainLoss: 0.43357208371162415\n",
      "cnt: 0 - valLoss: 0.43783581256866455 - trainLoss: 0.43356943130493164\n",
      "cnt: 0 - valLoss: 0.4378347396850586 - trainLoss: 0.43356677889823914\n",
      "cnt: 0 - valLoss: 0.43783366680145264 - trainLoss: 0.43356412649154663\n",
      "cnt: 0 - valLoss: 0.43783265352249146 - trainLoss: 0.4335614740848541\n",
      "cnt: 0 - valLoss: 0.4378315806388855 - trainLoss: 0.4335588216781616\n",
      "cnt: 0 - valLoss: 0.43783050775527954 - trainLoss: 0.4335561692714691\n",
      "cnt: 0 - valLoss: 0.43782946467399597 - trainLoss: 0.4335535168647766\n",
      "cnt: 0 - valLoss: 0.4378284215927124 - trainLoss: 0.4335508644580841\n",
      "cnt: 0 - valLoss: 0.43782728910446167 - trainLoss: 0.4335482120513916\n",
      "cnt: 0 - valLoss: 0.4378262758255005 - trainLoss: 0.4335455596446991\n",
      "cnt: 0 - valLoss: 0.43782520294189453 - trainLoss: 0.4335429072380066\n",
      "cnt: 0 - valLoss: 0.4378241300582886 - trainLoss: 0.4335402548313141\n",
      "cnt: 0 - valLoss: 0.4378230571746826 - trainLoss: 0.4335376024246216\n",
      "cnt: 0 - valLoss: 0.43782198429107666 - trainLoss: 0.4335349500179291\n",
      "cnt: 0 - valLoss: 0.4378209412097931 - trainLoss: 0.4335322976112366\n",
      "cnt: 0 - valLoss: 0.4378199279308319 - trainLoss: 0.43352967500686646\n",
      "cnt: 0 - valLoss: 0.43781888484954834 - trainLoss: 0.43352699279785156\n",
      "cnt: 0 - valLoss: 0.4378178119659424 - trainLoss: 0.43352437019348145\n",
      "cnt: 0 - valLoss: 0.4378167390823364 - trainLoss: 0.43352171778678894\n",
      "cnt: 0 - valLoss: 0.43781572580337524 - trainLoss: 0.43351906538009644\n",
      "cnt: 0 - valLoss: 0.4378146529197693 - trainLoss: 0.43351635336875916\n",
      "cnt: 0 - valLoss: 0.43781358003616333 - trainLoss: 0.43351370096206665\n",
      "cnt: 0 - valLoss: 0.43781253695487976 - trainLoss: 0.4335111379623413\n",
      "cnt: 0 - valLoss: 0.4378114640712738 - trainLoss: 0.4335084557533264\n",
      "cnt: 0 - valLoss: 0.43781039118766785 - trainLoss: 0.4335058033466339\n",
      "cnt: 0 - valLoss: 0.43780937790870667 - trainLoss: 0.4335031509399414\n",
      "cnt: 0 - valLoss: 0.4378083050251007 - trainLoss: 0.4335004985332489\n",
      "cnt: 0 - valLoss: 0.43780723214149475 - trainLoss: 0.4334978461265564\n",
      "cnt: 0 - valLoss: 0.43780621886253357 - trainLoss: 0.4334952235221863\n",
      "cnt: 0 - valLoss: 0.43780517578125 - trainLoss: 0.4334925711154938\n",
      "cnt: 0 - valLoss: 0.43780407309532166 - trainLoss: 0.43348991870880127\n",
      "cnt: 0 - valLoss: 0.43780311942100525 - trainLoss: 0.43348726630210876\n",
      "cnt: 0 - valLoss: 0.4378020465373993 - trainLoss: 0.43348461389541626\n",
      "cnt: 0 - valLoss: 0.4378010630607605 - trainLoss: 0.43348196148872375\n",
      "cnt: 0 - valLoss: 0.4378001093864441 - trainLoss: 0.43347933888435364\n",
      "cnt: 0 - valLoss: 0.4377990961074829 - trainLoss: 0.43347668647766113\n",
      "cnt: 0 - valLoss: 0.4377981126308441 - trainLoss: 0.433474063873291\n",
      "cnt: 0 - valLoss: 0.4377971291542053 - trainLoss: 0.4334714114665985\n",
      "cnt: 0 - valLoss: 0.43779614567756653 - trainLoss: 0.433468759059906\n",
      "cnt: 0 - valLoss: 0.43779513239860535 - trainLoss: 0.4334661066532135\n",
      "cnt: 0 - valLoss: 0.43779411911964417 - trainLoss: 0.433463454246521\n",
      "cnt: 0 - valLoss: 0.43779316544532776 - trainLoss: 0.43346086144447327\n",
      "cnt: 0 - valLoss: 0.4377921521663666 - trainLoss: 0.43345823884010315\n",
      "cnt: 0 - valLoss: 0.4377911686897278 - trainLoss: 0.43345558643341064\n",
      "cnt: 0 - valLoss: 0.4377901256084442 - trainLoss: 0.4334529638290405\n",
      "cnt: 0 - valLoss: 0.4377892017364502 - trainLoss: 0.43345028162002563\n",
      "cnt: 0 - valLoss: 0.437788188457489 - trainLoss: 0.4334476590156555\n",
      "cnt: 0 - valLoss: 0.43778717517852783 - trainLoss: 0.433445006608963\n",
      "cnt: 0 - valLoss: 0.43778619170188904 - trainLoss: 0.4334424138069153\n",
      "cnt: 0 - valLoss: 0.43778520822525024 - trainLoss: 0.4334397614002228\n",
      "cnt: 0 - valLoss: 0.43778422474861145 - trainLoss: 0.4334371089935303\n",
      "cnt: 0 - valLoss: 0.43778324127197266 - trainLoss: 0.43343448638916016\n",
      "cnt: 0 - valLoss: 0.43778225779533386 - trainLoss: 0.43343183398246765\n",
      "cnt: 0 - valLoss: 0.4377812445163727 - trainLoss: 0.43342921137809753\n",
      "cnt: 0 - valLoss: 0.4377802312374115 - trainLoss: 0.43342655897140503\n",
      "cnt: 0 - valLoss: 0.4377792477607727 - trainLoss: 0.4334239065647125\n",
      "cnt: 0 - valLoss: 0.4377782642841339 - trainLoss: 0.43342125415802\n",
      "cnt: 0 - valLoss: 0.4377772808074951 - trainLoss: 0.4334186017513275\n",
      "cnt: 0 - valLoss: 0.4377762973308563 - trainLoss: 0.4334160089492798\n",
      "cnt: 0 - valLoss: 0.43777531385421753 - trainLoss: 0.43341338634490967\n",
      "cnt: 0 - valLoss: 0.43777430057525635 - trainLoss: 0.43341073393821716\n",
      "cnt: 0 - valLoss: 0.43777328729629517 - trainLoss: 0.43340808153152466\n",
      "cnt: 0 - valLoss: 0.43777230381965637 - trainLoss: 0.43340542912483215\n",
      "cnt: 0 - valLoss: 0.4377713203430176 - trainLoss: 0.43340280652046204\n",
      "cnt: 0 - valLoss: 0.437770277261734 - trainLoss: 0.43340015411376953\n",
      "cnt: 0 - valLoss: 0.4377692937850952 - trainLoss: 0.433397501707077\n",
      "cnt: 0 - valLoss: 0.4377683103084564 - trainLoss: 0.4333948791027069\n",
      "cnt: 0 - valLoss: 0.4377673268318176 - trainLoss: 0.4333922266960144\n",
      "cnt: 0 - valLoss: 0.43776634335517883 - trainLoss: 0.4333895742893219\n",
      "cnt: 0 - valLoss: 0.43776535987854004 - trainLoss: 0.4333869218826294\n",
      "cnt: 0 - valLoss: 0.43776437640190125 - trainLoss: 0.43338432908058167\n",
      "cnt: 0 - valLoss: 0.4377633333206177 - trainLoss: 0.43338170647621155\n",
      "cnt: 0 - valLoss: 0.4377623498439789 - trainLoss: 0.43337905406951904\n",
      "cnt: 0 - valLoss: 0.4377613663673401 - trainLoss: 0.43337640166282654\n",
      "cnt: 0 - valLoss: 0.4377603828907013 - trainLoss: 0.43337374925613403\n",
      "cnt: 0 - valLoss: 0.4377593994140625 - trainLoss: 0.4333711266517639\n",
      "cnt: 0 - valLoss: 0.43775835633277893 - trainLoss: 0.4333685338497162\n",
      "cnt: 0 - valLoss: 0.4377574324607849 - trainLoss: 0.4333658218383789\n",
      "cnt: 0 - valLoss: 0.4377564489841461 - trainLoss: 0.4333632290363312\n",
      "cnt: 0 - valLoss: 0.4377554655075073 - trainLoss: 0.43336057662963867\n",
      "cnt: 0 - valLoss: 0.43775448203086853 - trainLoss: 0.43335792422294617\n",
      "cnt: 0 - valLoss: 0.43775349855422974 - trainLoss: 0.43335530161857605\n",
      "cnt: 0 - valLoss: 0.43775248527526855 - trainLoss: 0.43335264921188354\n",
      "cnt: 0 - valLoss: 0.43775150179862976 - trainLoss: 0.4333500266075134\n",
      "cnt: 0 - valLoss: 0.43775051832199097 - trainLoss: 0.4333473742008209\n",
      "cnt: 0 - valLoss: 0.4377495348453522 - trainLoss: 0.4333447217941284\n",
      "cnt: 0 - valLoss: 0.43774861097335815 - trainLoss: 0.4333420693874359\n",
      "cnt: 0 - valLoss: 0.437747597694397 - trainLoss: 0.4333394765853882\n",
      "cnt: 0 - valLoss: 0.4377466142177582 - trainLoss: 0.43333685398101807\n",
      "cnt: 0 - valLoss: 0.43774569034576416 - trainLoss: 0.43333420157432556\n",
      "cnt: 0 - valLoss: 0.4377446472644806 - trainLoss: 0.43333154916763306\n",
      "cnt: 0 - valLoss: 0.4377436637878418 - trainLoss: 0.43332889676094055\n",
      "cnt: 0 - valLoss: 0.437742680311203 - trainLoss: 0.43332627415657043\n",
      "cnt: 0 - valLoss: 0.4377416968345642 - trainLoss: 0.43332362174987793\n",
      "cnt: 0 - valLoss: 0.4377407431602478 - trainLoss: 0.4333210289478302\n",
      "cnt: 0 - valLoss: 0.437739759683609 - trainLoss: 0.4333183765411377\n",
      "cnt: 0 - valLoss: 0.4377387762069702 - trainLoss: 0.4333157241344452\n",
      "cnt: 0 - valLoss: 0.4377377927303314 - trainLoss: 0.4333131015300751\n",
      "cnt: 0 - valLoss: 0.4377368688583374 - trainLoss: 0.43331044912338257\n",
      "cnt: 0 - valLoss: 0.43773582577705383 - trainLoss: 0.4333077371120453\n",
      "cnt: 0 - valLoss: 0.4377348721027374 - trainLoss: 0.43330517411231995\n",
      "cnt: 0 - valLoss: 0.43773388862609863 - trainLoss: 0.43330252170562744\n",
      "cnt: 0 - valLoss: 0.43773290514945984 - trainLoss: 0.43329986929893494\n",
      "cnt: 0 - valLoss: 0.43773192167282104 - trainLoss: 0.43329721689224243\n",
      "cnt: 0 - valLoss: 0.43773093819618225 - trainLoss: 0.4332945644855499\n",
      "cnt: 0 - valLoss: 0.43772995471954346 - trainLoss: 0.4332919418811798\n",
      "cnt: 0 - valLoss: 0.43772897124290466 - trainLoss: 0.4332893490791321\n",
      "cnt: 0 - valLoss: 0.43772801756858826 - trainLoss: 0.4332866966724396\n",
      "cnt: 0 - valLoss: 0.4377270042896271 - trainLoss: 0.43328404426574707\n",
      "cnt: 0 - valLoss: 0.43772605061531067 - trainLoss: 0.43328139185905457\n",
      "cnt: 0 - valLoss: 0.4377250373363495 - trainLoss: 0.43327873945236206\n",
      "cnt: 0 - valLoss: 0.4377240836620331 - trainLoss: 0.4332761764526367\n",
      "cnt: 0 - valLoss: 0.4377231001853943 - trainLoss: 0.4332734942436218\n",
      "cnt: 0 - valLoss: 0.4377221167087555 - trainLoss: 0.4332708418369293\n",
      "cnt: 0 - valLoss: 0.4377211630344391 - trainLoss: 0.4332681894302368\n",
      "cnt: 0 - valLoss: 0.4377201497554779 - trainLoss: 0.4332655370235443\n",
      "cnt: 0 - valLoss: 0.4377191960811615 - trainLoss: 0.43326297402381897\n",
      "cnt: 0 - valLoss: 0.43771815299987793 - trainLoss: 0.43326032161712646\n",
      "cnt: 0 - valLoss: 0.4377172291278839 - trainLoss: 0.4332576394081116\n",
      "cnt: 0 - valLoss: 0.4377162456512451 - trainLoss: 0.43325501680374146\n",
      "cnt: 0 - valLoss: 0.4377152621746063 - trainLoss: 0.43325236439704895\n",
      "cnt: 0 - valLoss: 0.43771427869796753 - trainLoss: 0.43324974179267883\n",
      "cnt: 0 - valLoss: 0.4377133250236511 - trainLoss: 0.43324708938598633\n",
      "cnt: 0 - valLoss: 0.43771234154701233 - trainLoss: 0.4332444369792938\n",
      "cnt: 0 - valLoss: 0.43771129846572876 - trainLoss: 0.4332418441772461\n",
      "cnt: 0 - valLoss: 0.43771037459373474 - trainLoss: 0.4332391917705536\n",
      "cnt: 0 - valLoss: 0.43770942091941833 - trainLoss: 0.4332365393638611\n",
      "cnt: 0 - valLoss: 0.43770840764045715 - trainLoss: 0.4332338869571686\n",
      "cnt: 0 - valLoss: 0.43770739436149597 - trainLoss: 0.43323126435279846\n",
      "cnt: 0 - valLoss: 0.4377064108848572 - trainLoss: 0.43322864174842834\n",
      "cnt: 0 - valLoss: 0.43770548701286316 - trainLoss: 0.43322598934173584\n",
      "cnt: 0 - valLoss: 0.4377044439315796 - trainLoss: 0.43322333693504333\n",
      "cnt: 0 - valLoss: 0.43770352005958557 - trainLoss: 0.4332207441329956\n",
      "cnt: 0 - valLoss: 0.437702476978302 - trainLoss: 0.4332180917263031\n",
      "cnt: 0 - valLoss: 0.4377015233039856 - trainLoss: 0.4332153797149658\n",
      "cnt: 0 - valLoss: 0.4377005398273468 - trainLoss: 0.4332127869129181\n",
      "cnt: 0 - valLoss: 0.437699556350708 - trainLoss: 0.433210164308548\n",
      "cnt: 0 - valLoss: 0.4376985728740692 - trainLoss: 0.43320751190185547\n",
      "cnt: 0 - valLoss: 0.4376975893974304 - trainLoss: 0.43320485949516296\n",
      "cnt: 0 - valLoss: 0.4376966059207916 - trainLoss: 0.43320223689079285\n",
      "cnt: 0 - valLoss: 0.4376956522464752 - trainLoss: 0.43319958448410034\n",
      "cnt: 0 - valLoss: 0.43769463896751404 - trainLoss: 0.43319693207740784\n",
      "cnt: 0 - valLoss: 0.4376935660839081 - trainLoss: 0.4331943392753601\n",
      "cnt: 0 - valLoss: 0.4376924932003021 - trainLoss: 0.4331916570663452\n",
      "cnt: 0 - valLoss: 0.43769142031669617 - trainLoss: 0.4331890046596527\n",
      "cnt: 0 - valLoss: 0.4376903474330902 - trainLoss: 0.433186411857605\n",
      "cnt: 0 - valLoss: 0.43768927454948425 - trainLoss: 0.43318378925323486\n",
      "cnt: 0 - valLoss: 0.4376882016658783 - trainLoss: 0.43318113684654236\n",
      "cnt: 0 - valLoss: 0.43768712878227234 - trainLoss: 0.43317848443984985\n",
      "cnt: 0 - valLoss: 0.43768611550331116 - trainLoss: 0.43317583203315735\n",
      "cnt: 0 - valLoss: 0.4376849830150604 - trainLoss: 0.43317320942878723\n",
      "cnt: 0 - valLoss: 0.43768396973609924 - trainLoss: 0.4331705570220947\n",
      "cnt: 0 - valLoss: 0.4376828968524933 - trainLoss: 0.433167964220047\n",
      "cnt: 0 - valLoss: 0.43768182396888733 - trainLoss: 0.4331653118133545\n",
      "cnt: 0 - valLoss: 0.43768075108528137 - trainLoss: 0.433162659406662\n",
      "cnt: 0 - valLoss: 0.4376796782016754 - trainLoss: 0.4331600069999695\n",
      "cnt: 0 - valLoss: 0.43767860531806946 - trainLoss: 0.43315738439559937\n",
      "cnt: 0 - valLoss: 0.4376775324344635 - trainLoss: 0.4331547021865845\n",
      "cnt: 0 - valLoss: 0.43767645955085754 - trainLoss: 0.43315207958221436\n",
      "cnt: 0 - valLoss: 0.43767544627189636 - trainLoss: 0.43314942717552185\n",
      "cnt: 0 - valLoss: 0.43767431378364563 - trainLoss: 0.43314680457115173\n",
      "cnt: 0 - valLoss: 0.43767330050468445 - trainLoss: 0.43314415216445923\n",
      "cnt: 0 - valLoss: 0.4376722276210785 - trainLoss: 0.4331414997577667\n",
      "cnt: 0 - valLoss: 0.43767115473747253 - trainLoss: 0.433138906955719\n",
      "cnt: 0 - valLoss: 0.4376700818538666 - trainLoss: 0.4331362843513489\n",
      "cnt: 0 - valLoss: 0.4376690089702606 - trainLoss: 0.43313363194465637\n",
      "cnt: 0 - valLoss: 0.43766793608665466 - trainLoss: 0.43313097953796387\n",
      "cnt: 0 - valLoss: 0.43766680359840393 - trainLoss: 0.43312832713127136\n",
      "cnt: 0 - valLoss: 0.43766579031944275 - trainLoss: 0.43312570452690125\n",
      "cnt: 0 - valLoss: 0.4376647174358368 - trainLoss: 0.43312302231788635\n",
      "cnt: 0 - valLoss: 0.43766364455223083 - trainLoss: 0.43312036991119385\n",
      "cnt: 0 - valLoss: 0.4376625716686249 - trainLoss: 0.4331178069114685\n",
      "cnt: 0 - valLoss: 0.4376614987850189 - trainLoss: 0.433115154504776\n",
      "cnt: 0 - valLoss: 0.43766048550605774 - trainLoss: 0.4331125020980835\n",
      "cnt: 0 - valLoss: 0.4376594126224518 - trainLoss: 0.4331098198890686\n",
      "cnt: 0 - valLoss: 0.4376583397388458 - trainLoss: 0.4331071674823761\n",
      "cnt: 0 - valLoss: 0.4376572072505951 - trainLoss: 0.43310457468032837\n",
      "cnt: 0 - valLoss: 0.4376561939716339 - trainLoss: 0.43310192227363586\n",
      "cnt: 0 - valLoss: 0.43765512108802795 - trainLoss: 0.43309929966926575\n",
      "cnt: 0 - valLoss: 0.437654048204422 - trainLoss: 0.433096706867218\n",
      "cnt: 0 - valLoss: 0.4376530051231384 - trainLoss: 0.43309399485588074\n",
      "cnt: 0 - valLoss: 0.43765193223953247 - trainLoss: 0.43309134244918823\n",
      "cnt: 0 - valLoss: 0.4376508593559265 - trainLoss: 0.4330887794494629\n",
      "cnt: 0 - valLoss: 0.43764975666999817 - trainLoss: 0.4330861270427704\n",
      "cnt: 0 - valLoss: 0.437648743391037 - trainLoss: 0.4330834746360779\n",
      "cnt: 0 - valLoss: 0.43764761090278625 - trainLoss: 0.4330808222293854\n",
      "cnt: 0 - valLoss: 0.4376465380191803 - trainLoss: 0.43307819962501526\n",
      "cnt: 0 - valLoss: 0.4376455247402191 - trainLoss: 0.43307554721832275\n",
      "cnt: 0 - valLoss: 0.43764445185661316 - trainLoss: 0.43307289481163025\n",
      "cnt: 0 - valLoss: 0.4376433789730072 - trainLoss: 0.43307021260261536\n",
      "cnt: 0 - valLoss: 0.43764230608940125 - trainLoss: 0.4330676198005676\n",
      "cnt: 0 - valLoss: 0.4376412630081177 - trainLoss: 0.4330649673938751\n",
      "cnt: 0 - valLoss: 0.43764016032218933 - trainLoss: 0.4330623149871826\n",
      "cnt: 0 - valLoss: 0.4376390874385834 - trainLoss: 0.4330596625804901\n",
      "cnt: 0 - valLoss: 0.4376380443572998 - trainLoss: 0.4330570697784424\n",
      "cnt: 0 - valLoss: 0.43763697147369385 - trainLoss: 0.4330544173717499\n",
      "cnt: 0 - valLoss: 0.4376358687877655 - trainLoss: 0.43305179476737976\n",
      "cnt: 0 - valLoss: 0.4376348555088043 - trainLoss: 0.43304914236068726\n",
      "cnt: 0 - valLoss: 0.43763378262519836 - trainLoss: 0.43304648995399475\n",
      "cnt: 0 - valLoss: 0.4376327097415924 - trainLoss: 0.43304383754730225\n",
      "cnt: 0 - valLoss: 0.43763166666030884 - trainLoss: 0.43304121494293213\n",
      "cnt: 0 - valLoss: 0.4376305639743805 - trainLoss: 0.4330385625362396\n",
      "cnt: 0 - valLoss: 0.4376295208930969 - trainLoss: 0.4330359697341919\n",
      "cnt: 0 - valLoss: 0.4376284182071686 - trainLoss: 0.4330333173274994\n",
      "cnt: 0 - valLoss: 0.437627375125885 - trainLoss: 0.4330306649208069\n",
      "cnt: 0 - valLoss: 0.43762627243995667 - trainLoss: 0.4330280125141144\n",
      "cnt: 0 - valLoss: 0.4376252293586731 - trainLoss: 0.43302538990974426\n",
      "cnt: 0 - valLoss: 0.43762412667274475 - trainLoss: 0.43302273750305176\n",
      "cnt: 0 - valLoss: 0.43762311339378357 - trainLoss: 0.43302008509635925\n",
      "cnt: 0 - valLoss: 0.4376220405101776 - trainLoss: 0.43301746249198914\n",
      "cnt: 0 - valLoss: 0.43762096762657166 - trainLoss: 0.43301481008529663\n",
      "cnt: 0 - valLoss: 0.4376198947429657 - trainLoss: 0.4330121576786041\n",
      "cnt: 0 - valLoss: 0.43761882185935974 - trainLoss: 0.4330095052719116\n",
      "cnt: 0 - valLoss: 0.4376177489757538 - trainLoss: 0.4330068826675415\n",
      "cnt: 0 - valLoss: 0.43761661648750305 - trainLoss: 0.4330042004585266\n",
      "cnt: 0 - valLoss: 0.4376155436038971 - trainLoss: 0.4330015778541565\n",
      "cnt: 0 - valLoss: 0.43761444091796875 - trainLoss: 0.43299898505210876\n",
      "cnt: 0 - valLoss: 0.4376133382320404 - trainLoss: 0.43299633264541626\n",
      "cnt: 0 - valLoss: 0.43761226534843445 - trainLoss: 0.43299368023872375\n",
      "cnt: 0 - valLoss: 0.4376111328601837 - trainLoss: 0.43299105763435364\n",
      "cnt: 0 - valLoss: 0.43761005997657776 - trainLoss: 0.43298840522766113\n",
      "cnt: 0 - valLoss: 0.43760889768600464 - trainLoss: 0.432985782623291\n",
      "cnt: 0 - valLoss: 0.43760785460472107 - trainLoss: 0.4329831302165985\n",
      "cnt: 0 - valLoss: 0.43760672211647034 - trainLoss: 0.432980477809906\n",
      "cnt: 0 - valLoss: 0.437605619430542 - trainLoss: 0.4329778850078583\n",
      "cnt: 0 - valLoss: 0.43760448694229126 - trainLoss: 0.43297523260116577\n",
      "cnt: 0 - valLoss: 0.4376033842563629 - trainLoss: 0.43297260999679565\n",
      "cnt: 0 - valLoss: 0.43760234117507935 - trainLoss: 0.43296995759010315\n",
      "cnt: 0 - valLoss: 0.4376012086868286 - trainLoss: 0.43296730518341064\n",
      "cnt: 0 - valLoss: 0.4376000761985779 - trainLoss: 0.43296465277671814\n",
      "cnt: 0 - valLoss: 0.4375990033149719 - trainLoss: 0.43296200037002563\n",
      "cnt: 0 - valLoss: 0.4375979006290436 - trainLoss: 0.4329593777656555\n",
      "cnt: 0 - valLoss: 0.43759676814079285 - trainLoss: 0.4329567849636078\n",
      "cnt: 0 - valLoss: 0.4375956654548645 - trainLoss: 0.4329541325569153\n",
      "cnt: 0 - valLoss: 0.43759456276893616 - trainLoss: 0.4329514801502228\n",
      "cnt: 0 - valLoss: 0.4375934898853302 - trainLoss: 0.4329488277435303\n",
      "cnt: 0 - valLoss: 0.43759241700172424 - trainLoss: 0.43294623494148254\n",
      "cnt: 0 - valLoss: 0.4375912845134735 - trainLoss: 0.43294358253479004\n",
      "cnt: 0 - valLoss: 0.43759018182754517 - trainLoss: 0.43294093012809753\n",
      "cnt: 0 - valLoss: 0.4375890791416168 - trainLoss: 0.43293827772140503\n",
      "cnt: 0 - valLoss: 0.4375879466533661 - trainLoss: 0.4329356849193573\n",
      "cnt: 0 - valLoss: 0.43758687376976013 - trainLoss: 0.4329330325126648\n",
      "cnt: 0 - valLoss: 0.4375857710838318 - trainLoss: 0.4329304099082947\n",
      "cnt: 0 - valLoss: 0.43758463859558105 - trainLoss: 0.4329277575016022\n",
      "cnt: 0 - valLoss: 0.4375835359096527 - trainLoss: 0.43292510509490967\n",
      "cnt: 0 - valLoss: 0.43758246302604675 - trainLoss: 0.43292245268821716\n",
      "cnt: 0 - valLoss: 0.4375813603401184 - trainLoss: 0.43291983008384705\n",
      "cnt: 0 - valLoss: 0.43758028745651245 - trainLoss: 0.43291717767715454\n",
      "cnt: 0 - valLoss: 0.4375792145729065 - trainLoss: 0.4329145848751068\n",
      "cnt: 0 - valLoss: 0.43757808208465576 - trainLoss: 0.4329119324684143\n",
      "cnt: 0 - valLoss: 0.4375769793987274 - trainLoss: 0.4329092800617218\n",
      "cnt: 0 - valLoss: 0.4375758767127991 - trainLoss: 0.4329066574573517\n",
      "cnt: 0 - valLoss: 0.43757474422454834 - trainLoss: 0.4329040050506592\n",
      "cnt: 0 - valLoss: 0.4375736713409424 - trainLoss: 0.43290138244628906\n",
      "cnt: 0 - valLoss: 0.43757256865501404 - trainLoss: 0.43289873003959656\n",
      "cnt: 0 - valLoss: 0.4375714361667633 - trainLoss: 0.43289607763290405\n",
      "cnt: 0 - valLoss: 0.43757039308547974 - trainLoss: 0.43289342522621155\n",
      "cnt: 0 - valLoss: 0.437569260597229 - trainLoss: 0.4328908324241638\n",
      "cnt: 0 - valLoss: 0.43756815791130066 - trainLoss: 0.4328881502151489\n",
      "cnt: 0 - valLoss: 0.4375670850276947 - trainLoss: 0.4328855574131012\n",
      "cnt: 0 - valLoss: 0.43756601214408875 - trainLoss: 0.4328829050064087\n",
      "cnt: 0 - valLoss: 0.437564879655838 - trainLoss: 0.4328802824020386\n",
      "cnt: 0 - valLoss: 0.43756377696990967 - trainLoss: 0.43287762999534607\n",
      "cnt: 0 - valLoss: 0.4375626742839813 - trainLoss: 0.43287497758865356\n",
      "cnt: 0 - valLoss: 0.4375615417957306 - trainLoss: 0.43287232518196106\n",
      "cnt: 0 - valLoss: 0.43756046891212463 - trainLoss: 0.43286970257759094\n",
      "cnt: 0 - valLoss: 0.4375593960285187 - trainLoss: 0.43286705017089844\n",
      "cnt: 0 - valLoss: 0.43755826354026794 - trainLoss: 0.4328644573688507\n",
      "cnt: 0 - valLoss: 0.437557190656662 - trainLoss: 0.4328617453575134\n",
      "cnt: 0 - valLoss: 0.43755605816841125 - trainLoss: 0.4328591525554657\n",
      "cnt: 0 - valLoss: 0.4375549554824829 - trainLoss: 0.4328565299510956\n",
      "cnt: 0 - valLoss: 0.43755388259887695 - trainLoss: 0.4328538775444031\n",
      "cnt: 0 - valLoss: 0.437552809715271 - trainLoss: 0.43285122513771057\n",
      "cnt: 0 - valLoss: 0.43755167722702026 - trainLoss: 0.43284857273101807\n",
      "cnt: 0 - valLoss: 0.4375505745410919 - trainLoss: 0.43284595012664795\n",
      "cnt: 0 - valLoss: 0.43754950165748596 - trainLoss: 0.43284329771995544\n",
      "cnt: 0 - valLoss: 0.43754842877388 - trainLoss: 0.43284064531326294\n",
      "cnt: 0 - valLoss: 0.43754732608795166 - trainLoss: 0.4328380525112152\n",
      "cnt: 0 - valLoss: 0.4375461935997009 - trainLoss: 0.4328354001045227\n",
      "cnt: 0 - valLoss: 0.43754518032073975 - trainLoss: 0.4328327775001526\n",
      "cnt: 0 - valLoss: 0.437544047832489 - trainLoss: 0.4328301250934601\n",
      "cnt: 0 - valLoss: 0.43754294514656067 - trainLoss: 0.4328274726867676\n",
      "cnt: 0 - valLoss: 0.4375418722629547 - trainLoss: 0.43282485008239746\n",
      "cnt: 0 - valLoss: 0.43754076957702637 - trainLoss: 0.43282219767570496\n",
      "cnt: 0 - valLoss: 0.4375396966934204 - trainLoss: 0.43281954526901245\n",
      "cnt: 0 - valLoss: 0.43753862380981445 - trainLoss: 0.4328169524669647\n",
      "cnt: 0 - valLoss: 0.4375375509262085 - trainLoss: 0.4328143000602722\n",
      "cnt: 0 - valLoss: 0.43753641843795776 - trainLoss: 0.4328116774559021\n",
      "cnt: 0 - valLoss: 0.4375353753566742 - trainLoss: 0.4328090250492096\n",
      "cnt: 0 - valLoss: 0.43753424286842346 - trainLoss: 0.4328063726425171\n",
      "cnt: 0 - valLoss: 0.4375331699848175 - trainLoss: 0.432803750038147\n",
      "cnt: 0 - valLoss: 0.43753206729888916 - trainLoss: 0.4328010678291321\n",
      "cnt: 0 - valLoss: 0.4375309944152832 - trainLoss: 0.43279850482940674\n",
      "cnt: 0 - valLoss: 0.43752986192703247 - trainLoss: 0.43279585242271423\n",
      "cnt: 0 - valLoss: 0.4375287890434265 - trainLoss: 0.43279320001602173\n",
      "cnt: 0 - valLoss: 0.43752774596214294 - trainLoss: 0.4327905476093292\n",
      "cnt: 0 - valLoss: 0.437526673078537 - trainLoss: 0.4327878952026367\n",
      "cnt: 0 - valLoss: 0.43752554059028625 - trainLoss: 0.4327852129936218\n",
      "cnt: 0 - valLoss: 0.4375244379043579 - trainLoss: 0.4327826201915741\n",
      "cnt: 0 - valLoss: 0.43752336502075195 - trainLoss: 0.432779997587204\n",
      "cnt: 0 - valLoss: 0.437522292137146 - trainLoss: 0.4327773451805115\n",
      "cnt: 0 - valLoss: 0.43752115964889526 - trainLoss: 0.43277475237846375\n",
      "cnt: 0 - valLoss: 0.4375201463699341 - trainLoss: 0.43277204036712646\n",
      "cnt: 0 - valLoss: 0.43751904368400574 - trainLoss: 0.43276944756507874\n",
      "cnt: 0 - valLoss: 0.4375179708003998 - trainLoss: 0.4327668249607086\n",
      "cnt: 0 - valLoss: 0.43751683831214905 - trainLoss: 0.4327641725540161\n",
      "cnt: 0 - valLoss: 0.4375157654285431 - trainLoss: 0.4327615201473236\n",
      "cnt: 0 - valLoss: 0.43751466274261475 - trainLoss: 0.4327588677406311\n",
      "cnt: 0 - valLoss: 0.4375135898590088 - trainLoss: 0.4327562153339386\n",
      "cnt: 0 - valLoss: 0.43751251697540283 - trainLoss: 0.4327535927295685\n",
      "cnt: 0 - valLoss: 0.4375114440917969 - trainLoss: 0.432750940322876\n",
      "cnt: 0 - valLoss: 0.4375103712081909 - trainLoss: 0.43274831771850586\n",
      "cnt: 0 - valLoss: 0.4375092685222626 - trainLoss: 0.43274566531181335\n",
      "cnt: 0 - valLoss: 0.4375081956386566 - trainLoss: 0.43274301290512085\n",
      "cnt: 0 - valLoss: 0.43750712275505066 - trainLoss: 0.4327404201030731\n",
      "cnt: 0 - valLoss: 0.4375060498714447 - trainLoss: 0.4327377676963806\n",
      "cnt: 0 - valLoss: 0.43750497698783875 - trainLoss: 0.4327351450920105\n",
      "cnt: 0 - valLoss: 0.437503844499588 - trainLoss: 0.432732492685318\n",
      "cnt: 0 - valLoss: 0.43750277161598206 - trainLoss: 0.4327298402786255\n",
      "cnt: 0 - valLoss: 0.4375017285346985 - trainLoss: 0.432727187871933\n",
      "cnt: 0 - valLoss: 0.43750065565109253 - trainLoss: 0.43272456526756287\n",
      "cnt: 0 - valLoss: 0.4374995827674866 - trainLoss: 0.43272191286087036\n",
      "cnt: 0 - valLoss: 0.43749845027923584 - trainLoss: 0.43271926045417786\n",
      "cnt: 0 - valLoss: 0.4374973475933075 - trainLoss: 0.4327166676521301\n",
      "cnt: 0 - valLoss: 0.43749621510505676 - trainLoss: 0.43271392583847046\n",
      "cnt: 0 - valLoss: 0.4374951124191284 - trainLoss: 0.43271127343177795\n",
      "cnt: 0 - valLoss: 0.4374940097332001 - trainLoss: 0.4327085316181183\n",
      "cnt: 0 - valLoss: 0.43749287724494934 - trainLoss: 0.432705819606781\n",
      "cnt: 0 - valLoss: 0.437491774559021 - trainLoss: 0.4327031373977661\n",
      "cnt: 0 - valLoss: 0.43749064207077026 - trainLoss: 0.43270039558410645\n",
      "cnt: 0 - valLoss: 0.4374895393848419 - trainLoss: 0.43269774317741394\n",
      "cnt: 0 - valLoss: 0.4374884366989136 - trainLoss: 0.4326950013637543\n",
      "cnt: 0 - valLoss: 0.43748730421066284 - trainLoss: 0.4326923191547394\n",
      "cnt: 0 - valLoss: 0.4374862015247345 - trainLoss: 0.4326896071434021\n",
      "cnt: 0 - valLoss: 0.43748509883880615 - trainLoss: 0.4326869249343872\n",
      "cnt: 0 - valLoss: 0.4374839663505554 - trainLoss: 0.4326842129230499\n",
      "cnt: 0 - valLoss: 0.4374828636646271 - trainLoss: 0.43268147110939026\n",
      "cnt: 0 - valLoss: 0.43748173117637634 - trainLoss: 0.43267878890037537\n",
      "cnt: 0 - valLoss: 0.4374806582927704 - trainLoss: 0.4326760768890381\n",
      "cnt: 0 - valLoss: 0.43747955560684204 - trainLoss: 0.4326733946800232\n",
      "cnt: 0 - valLoss: 0.4374784529209137 - trainLoss: 0.4326706826686859\n",
      "cnt: 0 - valLoss: 0.43747738003730774 - trainLoss: 0.43266794085502625\n",
      "cnt: 0 - valLoss: 0.4374762177467346 - trainLoss: 0.43266528844833374\n",
      "cnt: 0 - valLoss: 0.43747514486312866 - trainLoss: 0.4326625466346741\n",
      "cnt: 0 - valLoss: 0.43747401237487793 - trainLoss: 0.4326598644256592\n",
      "cnt: 0 - valLoss: 0.4374729096889496 - trainLoss: 0.4326571524143219\n",
      "cnt: 0 - valLoss: 0.43747180700302124 - trainLoss: 0.43265441060066223\n",
      "cnt: 0 - valLoss: 0.4374706745147705 - trainLoss: 0.43265172839164734\n",
      "cnt: 0 - valLoss: 0.43746957182884216 - trainLoss: 0.43264904618263245\n",
      "cnt: 0 - valLoss: 0.43746843934059143 - trainLoss: 0.43264633417129517\n",
      "cnt: 0 - valLoss: 0.43746739625930786 - trainLoss: 0.4326436519622803\n",
      "cnt: 0 - valLoss: 0.43746626377105713 - trainLoss: 0.43264099955558777\n",
      "cnt: 0 - valLoss: 0.43746519088745117 - trainLoss: 0.4326382577419281\n",
      "cnt: 0 - valLoss: 0.4374640882015228 - trainLoss: 0.43263551592826843\n",
      "cnt: 0 - valLoss: 0.4374629557132721 - trainLoss: 0.4326328635215759\n",
      "cnt: 0 - valLoss: 0.43746185302734375 - trainLoss: 0.43263012170791626\n",
      "cnt: 0 - valLoss: 0.4374607503414154 - trainLoss: 0.432627409696579\n",
      "cnt: 0 - valLoss: 0.43745967745780945 - trainLoss: 0.4326247274875641\n",
      "cnt: 0 - valLoss: 0.4374586045742035 - trainLoss: 0.4326220154762268\n",
      "cnt: 0 - valLoss: 0.43745747208595276 - trainLoss: 0.4326193332672119\n",
      "cnt: 0 - valLoss: 0.4374563694000244 - trainLoss: 0.43261659145355225\n",
      "cnt: 0 - valLoss: 0.43745526671409607 - trainLoss: 0.43261387944221497\n",
      "cnt: 0 - valLoss: 0.4374541938304901 - trainLoss: 0.4326111972332001\n",
      "cnt: 0 - valLoss: 0.4374530613422394 - trainLoss: 0.4326084554195404\n",
      "cnt: 0 - valLoss: 0.43745195865631104 - trainLoss: 0.4326058030128479\n",
      "cnt: 0 - valLoss: 0.4374508261680603 - trainLoss: 0.43260306119918823\n",
      "cnt: 0 - valLoss: 0.43744975328445435 - trainLoss: 0.4326004087924957\n",
      "cnt: 0 - valLoss: 0.437448650598526 - trainLoss: 0.43259766697883606\n",
      "cnt: 0 - valLoss: 0.4374476373195648 - trainLoss: 0.43259501457214355\n",
      "cnt: 0 - valLoss: 0.4374465048313141 - trainLoss: 0.4325922727584839\n",
      "cnt: 0 - valLoss: 0.43744540214538574 - trainLoss: 0.4325895309448242\n",
      "cnt: 0 - valLoss: 0.437444269657135 - trainLoss: 0.4325868487358093\n",
      "cnt: 0 - valLoss: 0.43744319677352905 - trainLoss: 0.43258413672447205\n",
      "cnt: 0 - valLoss: 0.4374421238899231 - trainLoss: 0.4325813949108124\n",
      "cnt: 0 - valLoss: 0.43744102120399475 - trainLoss: 0.4325787425041199\n",
      "cnt: 0 - valLoss: 0.43744000792503357 - trainLoss: 0.432576060295105\n",
      "cnt: 0 - valLoss: 0.43743887543678284 - trainLoss: 0.4325733482837677\n",
      "cnt: 0 - valLoss: 0.4374377727508545 - trainLoss: 0.43257060647010803\n",
      "cnt: 0 - valLoss: 0.43743669986724854 - trainLoss: 0.43256792426109314\n",
      "cnt: 0 - valLoss: 0.4374356269836426 - trainLoss: 0.43256521224975586\n",
      "cnt: 0 - valLoss: 0.43743449449539185 - trainLoss: 0.43256253004074097\n",
      "cnt: 0 - valLoss: 0.43743348121643066 - trainLoss: 0.4325598478317261\n",
      "cnt: 0 - valLoss: 0.4374323785305023 - trainLoss: 0.4325571060180664\n",
      "cnt: 0 - valLoss: 0.43743130564689636 - trainLoss: 0.4325544536113739\n",
      "cnt: 0 - valLoss: 0.4374302327632904 - trainLoss: 0.43255171179771423\n",
      "cnt: 0 - valLoss: 0.4374291002750397 - trainLoss: 0.43254899978637695\n",
      "cnt: 0 - valLoss: 0.4374280273914337 - trainLoss: 0.43254631757736206\n",
      "cnt: 0 - valLoss: 0.43742695450782776 - trainLoss: 0.4325435757637024\n",
      "cnt: 0 - valLoss: 0.4374258816242218 - trainLoss: 0.4325409233570099\n",
      "cnt: 0 - valLoss: 0.43742480874061584 - trainLoss: 0.4325381815433502\n",
      "cnt: 0 - valLoss: 0.4374237060546875 - trainLoss: 0.43253546953201294\n",
      "cnt: 0 - valLoss: 0.43742263317108154 - trainLoss: 0.43253278732299805\n",
      "cnt: 0 - valLoss: 0.4374215602874756 - trainLoss: 0.4325300455093384\n",
      "cnt: 0 - valLoss: 0.43742045760154724 - trainLoss: 0.4325273931026459\n",
      "cnt: 0 - valLoss: 0.43741941452026367 - trainLoss: 0.4325246512889862\n",
      "cnt: 0 - valLoss: 0.4374183118343353 - trainLoss: 0.4325219392776489\n",
      "cnt: 0 - valLoss: 0.43741726875305176 - trainLoss: 0.43251925706863403\n",
      "cnt: 0 - valLoss: 0.4374161660671234 - trainLoss: 0.43251657485961914\n",
      "cnt: 0 - valLoss: 0.43741509318351746 - trainLoss: 0.43251386284828186\n",
      "cnt: 0 - valLoss: 0.4374139904975891 - trainLoss: 0.4325111210346222\n",
      "cnt: 0 - valLoss: 0.43741294741630554 - trainLoss: 0.4325084388256073\n",
      "cnt: 0 - valLoss: 0.4374118745326996 - trainLoss: 0.43250572681427\n",
      "cnt: 0 - valLoss: 0.43741077184677124 - trainLoss: 0.43250298500061035\n",
      "cnt: 0 - valLoss: 0.4374096989631653 - trainLoss: 0.43250033259391785\n",
      "cnt: 0 - valLoss: 0.4374086260795593 - trainLoss: 0.4324975907802582\n",
      "cnt: 0 - valLoss: 0.43740758299827576 - trainLoss: 0.4324949383735657\n",
      "cnt: 0 - valLoss: 0.4374065101146698 - trainLoss: 0.432492196559906\n",
      "cnt: 0 - valLoss: 0.43740540742874146 - trainLoss: 0.4324895143508911\n",
      "cnt: 0 - valLoss: 0.4374043345451355 - trainLoss: 0.43248680233955383\n",
      "cnt: 0 - valLoss: 0.43740329146385193 - trainLoss: 0.43248406052589417\n",
      "cnt: 0 - valLoss: 0.4374021887779236 - trainLoss: 0.43248140811920166\n",
      "cnt: 0 - valLoss: 0.4374011158943176 - trainLoss: 0.4324786961078644\n",
      "cnt: 0 - valLoss: 0.43740010261535645 - trainLoss: 0.4324759840965271\n",
      "cnt: 0 - valLoss: 0.4373990297317505 - trainLoss: 0.4324733018875122\n",
      "cnt: 0 - valLoss: 0.43739789724349976 - trainLoss: 0.43247056007385254\n",
      "cnt: 0 - valLoss: 0.4373968839645386 - trainLoss: 0.43246790766716003\n",
      "cnt: 0 - valLoss: 0.43739575147628784 - trainLoss: 0.43246516585350037\n",
      "cnt: 0 - valLoss: 0.43739473819732666 - trainLoss: 0.4324624538421631\n",
      "cnt: 0 - valLoss: 0.4373936057090759 - trainLoss: 0.4324597716331482\n",
      "cnt: 0 - valLoss: 0.43739259243011475 - trainLoss: 0.4324570596218109\n",
      "cnt: 0 - valLoss: 0.437391459941864 - trainLoss: 0.432454377412796\n",
      "cnt: 0 - valLoss: 0.43739044666290283 - trainLoss: 0.43245163559913635\n",
      "cnt: 0 - valLoss: 0.4373893737792969 - trainLoss: 0.43244898319244385\n",
      "cnt: 0 - valLoss: 0.4373883008956909 - trainLoss: 0.4324462413787842\n",
      "cnt: 0 - valLoss: 0.43738722801208496 - trainLoss: 0.4324434995651245\n",
      "cnt: 0 - valLoss: 0.437386155128479 - trainLoss: 0.432440847158432\n",
      "cnt: 0 - valLoss: 0.43738508224487305 - trainLoss: 0.43243810534477234\n",
      "cnt: 0 - valLoss: 0.4373840093612671 - trainLoss: 0.43243542313575745\n",
      "cnt: 0 - valLoss: 0.4373829960823059 - trainLoss: 0.43243271112442017\n",
      "cnt: 0 - valLoss: 0.43738192319869995 - trainLoss: 0.4324299693107605\n",
      "cnt: 0 - valLoss: 0.437380850315094 - trainLoss: 0.432427316904068\n",
      "cnt: 0 - valLoss: 0.43737977743148804 - trainLoss: 0.4324246346950531\n",
      "cnt: 0 - valLoss: 0.4373787045478821 - trainLoss: 0.4324219226837158\n",
      "cnt: 0 - valLoss: 0.4373776614665985 - trainLoss: 0.43241918087005615\n",
      "cnt: 0 - valLoss: 0.43737655878067017 - trainLoss: 0.43241649866104126\n",
      "cnt: 0 - valLoss: 0.4373755156993866 - trainLoss: 0.432413786649704\n",
      "cnt: 0 - valLoss: 0.4373745024204254 - trainLoss: 0.4324110448360443\n",
      "cnt: 0 - valLoss: 0.43737339973449707 - trainLoss: 0.4324083924293518\n",
      "cnt: 0 - valLoss: 0.4373723566532135 - trainLoss: 0.43240565061569214\n",
      "cnt: 0 - valLoss: 0.43737128376960754 - trainLoss: 0.43240296840667725\n",
      "cnt: 0 - valLoss: 0.4373702108860016 - trainLoss: 0.43240025639533997\n",
      "cnt: 0 - valLoss: 0.43736913800239563 - trainLoss: 0.4323975145816803\n",
      "cnt: 0 - valLoss: 0.4373680651187897 - trainLoss: 0.4323948621749878\n",
      "cnt: 0 - valLoss: 0.4373670518398285 - trainLoss: 0.4323921799659729\n",
      "cnt: 0 - valLoss: 0.4373660087585449 - trainLoss: 0.43238943815231323\n",
      "cnt: 0 - valLoss: 0.43736493587493896 - trainLoss: 0.4323867857456207\n",
      "cnt: 0 - valLoss: 0.437363862991333 - trainLoss: 0.43238401412963867\n",
      "cnt: 0 - valLoss: 0.4373628497123718 - trainLoss: 0.43238136172294617\n",
      "cnt: 0 - valLoss: 0.4373617172241211 - trainLoss: 0.4323786199092865\n",
      "cnt: 0 - valLoss: 0.4373607039451599 - trainLoss: 0.4323759078979492\n",
      "cnt: 0 - valLoss: 0.43735963106155396 - trainLoss: 0.4323732256889343\n",
      "cnt: 0 - valLoss: 0.437358558177948 - trainLoss: 0.43237048387527466\n",
      "cnt: 0 - valLoss: 0.43735751509666443 - trainLoss: 0.43236783146858215\n",
      "cnt: 0 - valLoss: 0.43735644221305847 - trainLoss: 0.4323650896549225\n",
      "cnt: 0 - valLoss: 0.4373553991317749 - trainLoss: 0.4323623776435852\n",
      "cnt: 0 - valLoss: 0.43735435605049133 - trainLoss: 0.4323596954345703\n",
      "cnt: 0 - valLoss: 0.437353253364563 - trainLoss: 0.43235695362091064\n",
      "cnt: 0 - valLoss: 0.4373522102832794 - trainLoss: 0.43235430121421814\n",
      "cnt: 0 - valLoss: 0.43735119700431824 - trainLoss: 0.43235155940055847\n",
      "cnt: 0 - valLoss: 0.4373501241207123 - trainLoss: 0.4323488771915436\n",
      "cnt: 0 - valLoss: 0.4373490810394287 - trainLoss: 0.4323461651802063\n",
      "cnt: 0 - valLoss: 0.43734800815582275 - trainLoss: 0.4323434829711914\n",
      "cnt: 0 - valLoss: 0.4373469352722168 - trainLoss: 0.4323407709598541\n",
      "cnt: 0 - valLoss: 0.437345951795578 - trainLoss: 0.43233802914619446\n",
      "cnt: 0 - valLoss: 0.43734487891197205 - trainLoss: 0.43233534693717957\n",
      "cnt: 0 - valLoss: 0.4373438060283661 - trainLoss: 0.4323326349258423\n",
      "cnt: 0 - valLoss: 0.4373427927494049 - trainLoss: 0.4323299527168274\n",
      "cnt: 0 - valLoss: 0.43734174966812134 - trainLoss: 0.4323272407054901\n",
      "cnt: 0 - valLoss: 0.437340646982193 - trainLoss: 0.43232449889183044\n",
      "cnt: 0 - valLoss: 0.4373396635055542 - trainLoss: 0.43232184648513794\n",
      "cnt: 0 - valLoss: 0.43733859062194824 - trainLoss: 0.43231910467147827\n",
      "cnt: 0 - valLoss: 0.4373375475406647 - trainLoss: 0.43231645226478577\n",
      "cnt: 0 - valLoss: 0.4373364746570587 - trainLoss: 0.4323137104511261\n",
      "cnt: 0 - valLoss: 0.4373354911804199 - trainLoss: 0.4323109984397888\n",
      "cnt: 0 - valLoss: 0.4373343884944916 - trainLoss: 0.43230828642845154\n",
      "cnt: 0 - valLoss: 0.437333345413208 - trainLoss: 0.43230560421943665\n",
      "cnt: 0 - valLoss: 0.4373323321342468 - trainLoss: 0.43230289220809937\n",
      "cnt: 0 - valLoss: 0.43733125925064087 - trainLoss: 0.4323002099990845\n",
      "cnt: 0 - valLoss: 0.4373302757740021 - trainLoss: 0.4322974979877472\n",
      "cnt: 0 - valLoss: 0.4373292028903961 - trainLoss: 0.4322948157787323\n",
      "cnt: 0 - valLoss: 0.43732813000679016 - trainLoss: 0.43229207396507263\n",
      "cnt: 0 - valLoss: 0.4373270869255066 - trainLoss: 0.4322894215583801\n",
      "cnt: 0 - valLoss: 0.43732601404190063 - trainLoss: 0.43228667974472046\n",
      "cnt: 0 - valLoss: 0.43732503056526184 - trainLoss: 0.4322839677333832\n",
      "cnt: 0 - valLoss: 0.4373239576816559 - trainLoss: 0.4322812855243683\n",
      "cnt: 0 - valLoss: 0.4373229444026947 - trainLoss: 0.432278573513031\n",
      "cnt: 0 - valLoss: 0.43732187151908875 - trainLoss: 0.4322758913040161\n",
      "cnt: 0 - valLoss: 0.4373208284378052 - trainLoss: 0.43227314949035645\n",
      "cnt: 0 - valLoss: 0.437319815158844 - trainLoss: 0.43227043747901917\n",
      "cnt: 0 - valLoss: 0.4373187720775604 - trainLoss: 0.4322677552700043\n",
      "cnt: 0 - valLoss: 0.43731775879859924 - trainLoss: 0.4322650730609894\n",
      "cnt: 0 - valLoss: 0.4373166859149933 - trainLoss: 0.4322623312473297\n",
      "cnt: 0 - valLoss: 0.4373157024383545 - trainLoss: 0.43225961923599243\n",
      "cnt: 0 - valLoss: 0.43731462955474854 - trainLoss: 0.43225687742233276\n",
      "cnt: 0 - valLoss: 0.4373135566711426 - trainLoss: 0.43225419521331787\n",
      "cnt: 0 - valLoss: 0.437312513589859 - trainLoss: 0.4322514832019806\n",
      "cnt: 0 - valLoss: 0.4373115003108978 - trainLoss: 0.4322488307952881\n",
      "cnt: 0 - valLoss: 0.43731042742729187 - trainLoss: 0.4322460889816284\n",
      "cnt: 0 - valLoss: 0.4373094439506531 - trainLoss: 0.4322434067726135\n",
      "cnt: 0 - valLoss: 0.4373083710670471 - trainLoss: 0.43224069476127625\n",
      "cnt: 0 - valLoss: 0.43730732798576355 - trainLoss: 0.4322379529476166\n",
      "cnt: 0 - valLoss: 0.43730634450912476 - trainLoss: 0.4322352707386017\n",
      "cnt: 0 - valLoss: 0.4373052716255188 - trainLoss: 0.4322325587272644\n",
      "cnt: 0 - valLoss: 0.43730419874191284 - trainLoss: 0.4322298765182495\n",
      "cnt: 0 - valLoss: 0.43730318546295166 - trainLoss: 0.43222716450691223\n",
      "cnt: 0 - valLoss: 0.4373021423816681 - trainLoss: 0.43222445249557495\n",
      "cnt: 0 - valLoss: 0.4373011291027069 - trainLoss: 0.43222180008888245\n",
      "cnt: 0 - valLoss: 0.43730008602142334 - trainLoss: 0.4322190582752228\n",
      "cnt: 0 - valLoss: 0.43729907274246216 - trainLoss: 0.4322163462638855\n",
      "cnt: 0 - valLoss: 0.4372980296611786 - trainLoss: 0.4322136640548706\n",
      "cnt: 0 - valLoss: 0.43729695677757263 - trainLoss: 0.4322109520435333\n",
      "cnt: 0 - valLoss: 0.43729597330093384 - trainLoss: 0.43220826983451843\n",
      "cnt: 0 - valLoss: 0.43729496002197266 - trainLoss: 0.43220552802085876\n",
      "cnt: 0 - valLoss: 0.4372939169406891 - trainLoss: 0.4322028160095215\n",
      "cnt: 0 - valLoss: 0.4372929036617279 - trainLoss: 0.4322001338005066\n",
      "cnt: 0 - valLoss: 0.43729183077812195 - trainLoss: 0.4321973919868469\n",
      "cnt: 0 - valLoss: 0.4372907876968384 - trainLoss: 0.4321947395801544\n",
      "cnt: 0 - valLoss: 0.4372898042201996 - trainLoss: 0.43219199776649475\n",
      "cnt: 0 - valLoss: 0.43728873133659363 - trainLoss: 0.43218928575515747\n",
      "cnt: 0 - valLoss: 0.43728771805763245 - trainLoss: 0.4321866035461426\n",
      "cnt: 0 - valLoss: 0.4372866749763489 - trainLoss: 0.4321838915348053\n",
      "cnt: 0 - valLoss: 0.4372856914997101 - trainLoss: 0.4321812093257904\n",
      "cnt: 0 - valLoss: 0.43728458881378174 - trainLoss: 0.43217846751213074\n",
      "cnt: 0 - valLoss: 0.43728354573249817 - trainLoss: 0.43217578530311584\n",
      "cnt: 0 - valLoss: 0.4372825622558594 - trainLoss: 0.43217307329177856\n",
      "cnt: 0 - valLoss: 0.4372815191745758 - trainLoss: 0.43217039108276367\n",
      "cnt: 0 - valLoss: 0.43728047609329224 - trainLoss: 0.4321676790714264\n",
      "cnt: 0 - valLoss: 0.43727943301200867 - trainLoss: 0.4321649968624115\n",
      "cnt: 0 - valLoss: 0.4372783899307251 - trainLoss: 0.43216225504875183\n",
      "cnt: 0 - valLoss: 0.4372774064540863 - trainLoss: 0.43215954303741455\n",
      "cnt: 0 - valLoss: 0.4372763931751251 - trainLoss: 0.43215686082839966\n",
      "cnt: 0 - valLoss: 0.43727535009384155 - trainLoss: 0.4321541488170624\n",
      "cnt: 0 - valLoss: 0.43727436661720276 - trainLoss: 0.4321514070034027\n",
      "cnt: 0 - valLoss: 0.4372732937335968 - trainLoss: 0.4321487545967102\n",
      "cnt: 0 - valLoss: 0.4372722804546356 - trainLoss: 0.4321460723876953\n",
      "cnt: 0 - valLoss: 0.43727120757102966 - trainLoss: 0.43214336037635803\n",
      "cnt: 0 - valLoss: 0.43727022409439087 - trainLoss: 0.43214064836502075\n",
      "cnt: 0 - valLoss: 0.4372691810131073 - trainLoss: 0.43213793635368347\n",
      "cnt: 0 - valLoss: 0.4372681677341461 - trainLoss: 0.4321352541446686\n",
      "cnt: 0 - valLoss: 0.43726712465286255 - trainLoss: 0.4321325123310089\n",
      "cnt: 0 - valLoss: 0.43726611137390137 - trainLoss: 0.4321298599243164\n",
      "cnt: 0 - valLoss: 0.4372651278972626 - trainLoss: 0.43212711811065674\n",
      "cnt: 0 - valLoss: 0.437264084815979 - trainLoss: 0.43212440609931946\n",
      "cnt: 0 - valLoss: 0.4372630715370178 - trainLoss: 0.43212172389030457\n",
      "cnt: 0 - valLoss: 0.43726202845573425 - trainLoss: 0.4321189820766449\n",
      "cnt: 0 - valLoss: 0.43726101517677307 - trainLoss: 0.4321163296699524\n",
      "cnt: 0 - valLoss: 0.4372599720954895 - trainLoss: 0.4321135878562927\n",
      "cnt: 0 - valLoss: 0.4372589588165283 - trainLoss: 0.43211087584495544\n",
      "cnt: 0 - valLoss: 0.4372578263282776 - trainLoss: 0.43210819363594055\n",
      "cnt: 0 - valLoss: 0.4372568130493164 - trainLoss: 0.4321054518222809\n",
      "cnt: 0 - valLoss: 0.43725574016571045 - trainLoss: 0.4321027994155884\n",
      "cnt: 0 - valLoss: 0.4372546672821045 - trainLoss: 0.4321000874042511\n",
      "cnt: 0 - valLoss: 0.4372536242008209 - trainLoss: 0.4320974051952362\n",
      "cnt: 0 - valLoss: 0.43725255131721497 - trainLoss: 0.43209466338157654\n",
      "cnt: 0 - valLoss: 0.437251478433609 - trainLoss: 0.43209198117256165\n",
      "cnt: 0 - valLoss: 0.4372504651546478 - trainLoss: 0.43208926916122437\n",
      "cnt: 0 - valLoss: 0.43724939227104187 - trainLoss: 0.4320865869522095\n",
      "cnt: 0 - valLoss: 0.4372483193874359 - trainLoss: 0.4320838749408722\n",
      "cnt: 0 - valLoss: 0.43724727630615234 - trainLoss: 0.4320812225341797\n",
      "cnt: 0 - valLoss: 0.43724626302719116 - trainLoss: 0.43207848072052\n",
      "cnt: 0 - valLoss: 0.43724513053894043 - trainLoss: 0.4320757985115051\n",
      "cnt: 0 - valLoss: 0.4372440576553345 - trainLoss: 0.43207308650016785\n",
      "cnt: 0 - valLoss: 0.4372430443763733 - trainLoss: 0.4320703446865082\n",
      "cnt: 0 - valLoss: 0.43724197149276733 - trainLoss: 0.4320676922798157\n",
      "cnt: 0 - valLoss: 0.43724095821380615 - trainLoss: 0.432064950466156\n",
      "cnt: 0 - valLoss: 0.4372398853302002 - trainLoss: 0.4320622980594635\n",
      "cnt: 0 - valLoss: 0.4372388422489166 - trainLoss: 0.43205955624580383\n",
      "cnt: 0 - valLoss: 0.43723776936531067 - trainLoss: 0.43205681443214417\n",
      "cnt: 0 - valLoss: 0.4372367560863495 - trainLoss: 0.43205416202545166\n",
      "cnt: 0 - valLoss: 0.43723568320274353 - trainLoss: 0.43205147981643677\n",
      "cnt: 0 - valLoss: 0.4372346103191376 - trainLoss: 0.4320487678050995\n",
      "cnt: 0 - valLoss: 0.437233567237854 - trainLoss: 0.4320460855960846\n",
      "cnt: 0 - valLoss: 0.4372325539588928 - trainLoss: 0.4320434033870697\n",
      "cnt: 0 - valLoss: 0.43723148107528687 - trainLoss: 0.43204066157341003\n",
      "cnt: 0 - valLoss: 0.4372304379940033 - trainLoss: 0.43203800916671753\n",
      "cnt: 0 - valLoss: 0.43722936511039734 - trainLoss: 0.43203529715538025\n",
      "cnt: 0 - valLoss: 0.43722835183143616 - trainLoss: 0.43203261494636536\n",
      "cnt: 0 - valLoss: 0.4372273087501526 - trainLoss: 0.4320298731327057\n",
      "cnt: 0 - valLoss: 0.43722623586654663 - trainLoss: 0.4320271611213684\n",
      "cnt: 0 - valLoss: 0.43722522258758545 - trainLoss: 0.4320244789123535\n",
      "cnt: 0 - valLoss: 0.4372241497039795 - trainLoss: 0.432021826505661\n",
      "cnt: 0 - valLoss: 0.4372231066226959 - trainLoss: 0.43201908469200134\n",
      "cnt: 0 - valLoss: 0.43722203373908997 - trainLoss: 0.43201637268066406\n",
      "cnt: 0 - valLoss: 0.437220960855484 - trainLoss: 0.43201369047164917\n",
      "cnt: 0 - valLoss: 0.43721985816955566 - trainLoss: 0.43201103806495667\n",
      "cnt: 0 - valLoss: 0.4372187852859497 - trainLoss: 0.43200838565826416\n",
      "cnt: 0 - valLoss: 0.43721774220466614 - trainLoss: 0.4320056736469269\n",
      "cnt: 0 - valLoss: 0.4372166693210602 - trainLoss: 0.432002991437912\n",
      "cnt: 0 - valLoss: 0.43721556663513184 - trainLoss: 0.4320002794265747\n",
      "cnt: 0 - valLoss: 0.43721455335617065 - trainLoss: 0.4319975972175598\n",
      "cnt: 0 - valLoss: 0.4372134804725647 - trainLoss: 0.4319949448108673\n",
      "cnt: 0 - valLoss: 0.43721234798431396 - trainLoss: 0.43199220299720764\n",
      "cnt: 0 - valLoss: 0.437211275100708 - trainLoss: 0.43198955059051514\n",
      "cnt: 0 - valLoss: 0.4372102618217468 - trainLoss: 0.43198689818382263\n",
      "cnt: 0 - valLoss: 0.4372091293334961 - trainLoss: 0.4319842457771301\n",
      "cnt: 0 - valLoss: 0.4372081160545349 - trainLoss: 0.43198150396347046\n",
      "cnt: 0 - valLoss: 0.43720704317092896 - trainLoss: 0.43197885155677795\n",
      "cnt: 0 - valLoss: 0.437205970287323 - trainLoss: 0.4319761395454407\n",
      "cnt: 0 - valLoss: 0.43720486760139465 - trainLoss: 0.4319734573364258\n",
      "cnt: 0 - valLoss: 0.4372038245201111 - trainLoss: 0.4319708049297333\n",
      "cnt: 0 - valLoss: 0.43720272183418274 - trainLoss: 0.431968092918396\n",
      "cnt: 0 - valLoss: 0.43720167875289917 - trainLoss: 0.4319654405117035\n",
      "cnt: 0 - valLoss: 0.4372006058692932 - trainLoss: 0.4319627583026886\n",
      "cnt: 0 - valLoss: 0.43719953298568726 - trainLoss: 0.4319601058959961\n",
      "cnt: 0 - valLoss: 0.4371985197067261 - trainLoss: 0.4319573938846588\n",
      "cnt: 0 - valLoss: 0.43719738721847534 - trainLoss: 0.4319547116756439\n",
      "cnt: 0 - valLoss: 0.4371963143348694 - trainLoss: 0.43195199966430664\n",
      "cnt: 0 - valLoss: 0.4371952414512634 - trainLoss: 0.43194931745529175\n",
      "cnt: 0 - valLoss: 0.43719416856765747 - trainLoss: 0.43194666504859924\n",
      "cnt: 0 - valLoss: 0.4371930956840515 - trainLoss: 0.43194395303726196\n",
      "cnt: 0 - valLoss: 0.43719205260276794 - trainLoss: 0.43194127082824707\n",
      "cnt: 0 - valLoss: 0.437190979719162 - trainLoss: 0.43193861842155457\n",
      "cnt: 0 - valLoss: 0.4371899366378784 - trainLoss: 0.43193596601486206\n",
      "cnt: 0 - valLoss: 0.43718886375427246 - trainLoss: 0.4319332242012024\n",
      "cnt: 0 - valLoss: 0.4371877908706665 - trainLoss: 0.4319305717945099\n",
      "cnt: 0 - valLoss: 0.43718671798706055 - trainLoss: 0.4319278597831726\n",
      "cnt: 0 - valLoss: 0.4371856451034546 - trainLoss: 0.4319252073764801\n",
      "cnt: 0 - valLoss: 0.4371846318244934 - trainLoss: 0.4319225251674652\n",
      "cnt: 0 - valLoss: 0.43718355894088745 - trainLoss: 0.4319198727607727\n",
      "cnt: 0 - valLoss: 0.4371824860572815 - trainLoss: 0.43191713094711304\n",
      "cnt: 0 - valLoss: 0.43718141317367554 - trainLoss: 0.43191447854042053\n",
      "cnt: 0 - valLoss: 0.4371803402900696 - trainLoss: 0.431911826133728\n",
      "cnt: 0 - valLoss: 0.4371792674064636 - trainLoss: 0.43190908432006836\n",
      "cnt: 0 - valLoss: 0.43717822432518005 - trainLoss: 0.43190643191337585\n",
      "cnt: 0 - valLoss: 0.4371771514415741 - trainLoss: 0.43190377950668335\n",
      "cnt: 0 - valLoss: 0.4371761083602905 - trainLoss: 0.4319010376930237\n",
      "cnt: 0 - valLoss: 0.43717503547668457 - trainLoss: 0.4318983852863312\n",
      "cnt: 0 - valLoss: 0.437173992395401 - trainLoss: 0.43189573287963867\n",
      "cnt: 0 - valLoss: 0.43717291951179504 - trainLoss: 0.431892991065979\n",
      "cnt: 0 - valLoss: 0.4371718466281891 - trainLoss: 0.4318903386592865\n",
      "cnt: 0 - valLoss: 0.43717074394226074 - trainLoss: 0.431887686252594\n",
      "cnt: 0 - valLoss: 0.4371697008609772 - trainLoss: 0.4318849444389343\n",
      "cnt: 0 - valLoss: 0.437168687582016 - trainLoss: 0.4318822920322418\n",
      "cnt: 0 - valLoss: 0.43716755509376526 - trainLoss: 0.43187958002090454\n",
      "cnt: 0 - valLoss: 0.4371665418148041 - trainLoss: 0.43187689781188965\n",
      "cnt: 0 - valLoss: 0.4371654987335205 - trainLoss: 0.43187424540519714\n",
      "cnt: 0 - valLoss: 0.43716442584991455 - trainLoss: 0.43187159299850464\n",
      "cnt: 0 - valLoss: 0.4371633529663086 - trainLoss: 0.43186885118484497\n",
      "cnt: 0 - valLoss: 0.4371623396873474 - trainLoss: 0.43186619877815247\n",
      "cnt: 0 - valLoss: 0.4371612071990967 - trainLoss: 0.43186354637145996\n",
      "cnt: 0 - valLoss: 0.4371601939201355 - trainLoss: 0.4318608045578003\n",
      "cnt: 0 - valLoss: 0.43715915083885193 - trainLoss: 0.4318581521511078\n",
      "cnt: 0 - valLoss: 0.43715807795524597 - trainLoss: 0.4318554997444153\n",
      "cnt: 0 - valLoss: 0.4371570348739624 - trainLoss: 0.4318527579307556\n",
      "cnt: 0 - valLoss: 0.43715596199035645 - trainLoss: 0.4318501055240631\n",
      "cnt: 0 - valLoss: 0.4371548891067505 - trainLoss: 0.43184739351272583\n",
      "cnt: 0 - valLoss: 0.4371538460254669 - trainLoss: 0.43184471130371094\n",
      "cnt: 0 - valLoss: 0.43715277314186096 - trainLoss: 0.43184205889701843\n",
      "cnt: 0 - valLoss: 0.437151700258255 - trainLoss: 0.4318394064903259\n",
      "cnt: 0 - valLoss: 0.4371506869792938 - trainLoss: 0.43183666467666626\n",
      "cnt: 0 - valLoss: 0.43714961409568787 - trainLoss: 0.43183401226997375\n",
      "cnt: 0 - valLoss: 0.4371485412120819 - trainLoss: 0.4318313002586365\n",
      "cnt: 0 - valLoss: 0.43714749813079834 - trainLoss: 0.4318286180496216\n",
      "cnt: 0 - valLoss: 0.43714648485183716 - trainLoss: 0.4318259060382843\n",
      "cnt: 0 - valLoss: 0.4371453523635864 - trainLoss: 0.4318232238292694\n",
      "cnt: 0 - valLoss: 0.43714433908462524 - trainLoss: 0.4318205714225769\n",
      "cnt: 0 - valLoss: 0.4371432662010193 - trainLoss: 0.4318178594112396\n",
      "cnt: 0 - valLoss: 0.43714219331741333 - trainLoss: 0.4318152070045471\n",
      "cnt: 0 - valLoss: 0.43714115023612976 - trainLoss: 0.4318125247955322\n",
      "cnt: 0 - valLoss: 0.4371400773525238 - trainLoss: 0.43180981278419495\n",
      "cnt: 0 - valLoss: 0.4371390640735626 - trainLoss: 0.43180716037750244\n",
      "cnt: 0 - valLoss: 0.43713799118995667 - trainLoss: 0.43180447816848755\n",
      "cnt: 0 - valLoss: 0.4371369183063507 - trainLoss: 0.43180182576179504\n",
      "cnt: 0 - valLoss: 0.43713587522506714 - trainLoss: 0.43179911375045776\n",
      "cnt: 0 - valLoss: 0.43713483214378357 - trainLoss: 0.43179643154144287\n",
      "cnt: 0 - valLoss: 0.4371337592601776 - trainLoss: 0.43179377913475037\n",
      "cnt: 0 - valLoss: 0.43713268637657166 - trainLoss: 0.4317910373210907\n",
      "cnt: 0 - valLoss: 0.4371316432952881 - trainLoss: 0.4317883849143982\n",
      "cnt: 0 - valLoss: 0.43713057041168213 - trainLoss: 0.4317857325077057\n",
      "cnt: 0 - valLoss: 0.43712949752807617 - trainLoss: 0.431782990694046\n",
      "cnt: 0 - valLoss: 0.437128484249115 - trainLoss: 0.4317803382873535\n",
      "cnt: 0 - valLoss: 0.43712741136550903 - trainLoss: 0.43177762627601624\n",
      "cnt: 0 - valLoss: 0.4371263384819031 - trainLoss: 0.43177494406700134\n",
      "cnt: 0 - valLoss: 0.4371252954006195 - trainLoss: 0.43177229166030884\n",
      "cnt: 0 - valLoss: 0.43712422251701355 - trainLoss: 0.43176957964897156\n",
      "cnt: 0 - valLoss: 0.4371231496334076 - trainLoss: 0.43176692724227905\n",
      "cnt: 0 - valLoss: 0.4371221363544464 - trainLoss: 0.43176424503326416\n",
      "cnt: 0 - valLoss: 0.43712109327316284 - trainLoss: 0.4317615330219269\n",
      "cnt: 0 - valLoss: 0.43712007999420166 - trainLoss: 0.4317588806152344\n",
      "cnt: 0 - valLoss: 0.4371190071105957 - trainLoss: 0.4317561984062195\n",
      "cnt: 0 - valLoss: 0.43711796402931213 - trainLoss: 0.431753545999527\n",
      "cnt: 0 - valLoss: 0.4371168911457062 - trainLoss: 0.4317508041858673\n",
      "cnt: 0 - valLoss: 0.437115877866745 - trainLoss: 0.4317481517791748\n",
      "cnt: 0 - valLoss: 0.4371148347854614 - trainLoss: 0.4317454993724823\n",
      "cnt: 0 - valLoss: 0.43711376190185547 - trainLoss: 0.43174275755882263\n",
      "cnt: 0 - valLoss: 0.4371127486228943 - trainLoss: 0.4317401051521301\n",
      "cnt: 0 - valLoss: 0.43711167573928833 - trainLoss: 0.43173739314079285\n",
      "cnt: 0 - valLoss: 0.43711063265800476 - trainLoss: 0.43173471093177795\n",
      "cnt: 0 - valLoss: 0.4371095597743988 - trainLoss: 0.4317319989204407\n",
      "cnt: 0 - valLoss: 0.4371085464954376 - trainLoss: 0.43172934651374817\n",
      "cnt: 0 - valLoss: 0.43710750341415405 - trainLoss: 0.43172669410705566\n",
      "cnt: 0 - valLoss: 0.4371064305305481 - trainLoss: 0.43172401189804077\n",
      "cnt: 0 - valLoss: 0.4371054172515869 - trainLoss: 0.4317212998867035\n",
      "cnt: 0 - valLoss: 0.43710437417030334 - trainLoss: 0.4317186176776886\n",
      "cnt: 0 - valLoss: 0.43710336089134216 - trainLoss: 0.4317159652709961\n",
      "cnt: 0 - valLoss: 0.4371022880077362 - trainLoss: 0.4317132532596588\n",
      "cnt: 0 - valLoss: 0.43710124492645264 - trainLoss: 0.4317105710506439\n",
      "cnt: 0 - valLoss: 0.4371001720428467 - trainLoss: 0.4317079186439514\n",
      "cnt: 0 - valLoss: 0.4370991587638855 - trainLoss: 0.43170517683029175\n",
      "cnt: 0 - valLoss: 0.43709811568260193 - trainLoss: 0.43170252442359924\n",
      "cnt: 0 - valLoss: 0.43709710240364075 - trainLoss: 0.43169987201690674\n",
      "cnt: 0 - valLoss: 0.4370960295200348 - trainLoss: 0.43169716000556946\n",
      "cnt: 0 - valLoss: 0.4370949864387512 - trainLoss: 0.43169447779655457\n",
      "cnt: 0 - valLoss: 0.43709391355514526 - trainLoss: 0.4316917657852173\n",
      "cnt: 0 - valLoss: 0.4370929002761841 - trainLoss: 0.4316890835762024\n",
      "cnt: 0 - valLoss: 0.4370918571949005 - trainLoss: 0.4316864311695099\n",
      "cnt: 0 - valLoss: 0.43709084391593933 - trainLoss: 0.4316837191581726\n",
      "cnt: 0 - valLoss: 0.4370897710323334 - trainLoss: 0.4316810369491577\n",
      "cnt: 0 - valLoss: 0.4370887279510498 - trainLoss: 0.4316783845424652\n",
      "cnt: 0 - valLoss: 0.43708765506744385 - trainLoss: 0.4316757321357727\n",
      "cnt: 0 - valLoss: 0.43708664178848267 - trainLoss: 0.4316730201244354\n",
      "cnt: 0 - valLoss: 0.4370855987071991 - trainLoss: 0.43167033791542053\n",
      "cnt: 0 - valLoss: 0.4370845556259155 - trainLoss: 0.431667685508728\n",
      "cnt: 0 - valLoss: 0.43708354234695435 - trainLoss: 0.43166497349739075\n",
      "cnt: 0 - valLoss: 0.4370824694633484 - trainLoss: 0.43166229128837585\n",
      "cnt: 0 - valLoss: 0.4370814561843872 - trainLoss: 0.43165963888168335\n",
      "cnt: 0 - valLoss: 0.43708038330078125 - trainLoss: 0.43165692687034607\n",
      "cnt: 0 - valLoss: 0.4370793402194977 - trainLoss: 0.4316542446613312\n",
      "cnt: 0 - valLoss: 0.4370783269405365 - trainLoss: 0.43165159225463867\n",
      "cnt: 0 - valLoss: 0.43707728385925293 - trainLoss: 0.4316488802433014\n",
      "cnt: 0 - valLoss: 0.437076210975647 - trainLoss: 0.4316461980342865\n",
      "cnt: 0 - valLoss: 0.4370751976966858 - trainLoss: 0.4316434860229492\n",
      "cnt: 0 - valLoss: 0.437074214220047 - trainLoss: 0.4316408336162567\n",
      "cnt: 0 - valLoss: 0.43707314133644104 - trainLoss: 0.4316381514072418\n",
      "cnt: 0 - valLoss: 0.4370720684528351 - trainLoss: 0.4316354990005493\n",
      "cnt: 0 - valLoss: 0.4370710253715515 - trainLoss: 0.43163275718688965\n",
      "cnt: 0 - valLoss: 0.43707001209259033 - trainLoss: 0.43163010478019714\n",
      "cnt: 0 - valLoss: 0.43706896901130676 - trainLoss: 0.43162745237350464\n",
      "cnt: 0 - valLoss: 0.4370678961277008 - trainLoss: 0.43162471055984497\n",
      "cnt: 0 - valLoss: 0.437066912651062 - trainLoss: 0.43162205815315247\n",
      "cnt: 0 - valLoss: 0.43706583976745605 - trainLoss: 0.4316193461418152\n",
      "cnt: 0 - valLoss: 0.4370648264884949 - trainLoss: 0.4316166639328003\n",
      "cnt: 0 - valLoss: 0.4370637834072113 - trainLoss: 0.4316140115261078\n",
      "cnt: 0 - valLoss: 0.43706271052360535 - trainLoss: 0.4316112995147705\n",
      "cnt: 0 - valLoss: 0.43706169724464417 - trainLoss: 0.4316086173057556\n",
      "cnt: 0 - valLoss: 0.4370606541633606 - trainLoss: 0.4316059648990631\n",
      "cnt: 0 - valLoss: 0.4370596408843994 - trainLoss: 0.43160322308540344\n",
      "cnt: 0 - valLoss: 0.43705859780311584 - trainLoss: 0.43160057067871094\n",
      "cnt: 0 - valLoss: 0.43705758452415466 - trainLoss: 0.43159791827201843\n",
      "cnt: 0 - valLoss: 0.4370565414428711 - trainLoss: 0.43159517645835876\n",
      "cnt: 0 - valLoss: 0.43705546855926514 - trainLoss: 0.43159252405166626\n",
      "cnt: 0 - valLoss: 0.43705445528030396 - trainLoss: 0.43158987164497375\n",
      "cnt: 0 - valLoss: 0.4370534121990204 - trainLoss: 0.4315871596336365\n",
      "cnt: 0 - valLoss: 0.4370523989200592 - trainLoss: 0.4315844774246216\n",
      "cnt: 0 - valLoss: 0.43705135583877563 - trainLoss: 0.4315818250179291\n",
      "cnt: 0 - valLoss: 0.4370502829551697 - trainLoss: 0.4315790832042694\n",
      "cnt: 0 - valLoss: 0.4370492696762085 - trainLoss: 0.4315764307975769\n",
      "cnt: 0 - valLoss: 0.4370482265949249 - trainLoss: 0.4315737783908844\n",
      "cnt: 0 - valLoss: 0.43704721331596375 - trainLoss: 0.43157103657722473\n",
      "cnt: 0 - valLoss: 0.4370461702346802 - trainLoss: 0.4315683841705322\n",
      "cnt: 0 - valLoss: 0.437045156955719 - trainLoss: 0.4315657317638397\n",
      "cnt: 0 - valLoss: 0.43704408407211304 - trainLoss: 0.43156298995018005\n",
      "cnt: 0 - valLoss: 0.43704304099082947 - trainLoss: 0.43156033754348755\n",
      "cnt: 0 - valLoss: 0.4370420575141907 - trainLoss: 0.43155762553215027\n",
      "cnt: 0 - valLoss: 0.4370409846305847 - trainLoss: 0.4315549433231354\n",
      "cnt: 0 - valLoss: 0.43703997135162354 - trainLoss: 0.43155229091644287\n",
      "cnt: 0 - valLoss: 0.4370388984680176 - trainLoss: 0.4315495789051056\n",
      "cnt: 0 - valLoss: 0.437037855386734 - trainLoss: 0.4315468966960907\n",
      "cnt: 0 - valLoss: 0.4370368719100952 - trainLoss: 0.4315441846847534\n",
      "cnt: 0 - valLoss: 0.43703579902648926 - trainLoss: 0.4315415322780609\n",
      "cnt: 0 - valLoss: 0.4370347857475281 - trainLoss: 0.431538850069046\n",
      "cnt: 0 - valLoss: 0.4370337426662445 - trainLoss: 0.43153613805770874\n",
      "cnt: 0 - valLoss: 0.4370327293872833 - trainLoss: 0.43153345584869385\n",
      "cnt: 0 - valLoss: 0.43703168630599976 - trainLoss: 0.43153080344200134\n",
      "cnt: 0 - valLoss: 0.4370306134223938 - trainLoss: 0.43152815103530884\n",
      "cnt: 0 - valLoss: 0.437029629945755 - trainLoss: 0.43152540922164917\n",
      "cnt: 0 - valLoss: 0.43702855706214905 - trainLoss: 0.43152275681495667\n",
      "cnt: 0 - valLoss: 0.43702754378318787 - trainLoss: 0.431520015001297\n",
      "cnt: 0 - valLoss: 0.4370265007019043 - trainLoss: 0.4315173625946045\n",
      "cnt: 0 - valLoss: 0.4370254874229431 - trainLoss: 0.431514710187912\n",
      "cnt: 0 - valLoss: 0.43702441453933716 - trainLoss: 0.4315119981765747\n",
      "cnt: 0 - valLoss: 0.4370233714580536 - trainLoss: 0.4315093159675598\n",
      "cnt: 0 - valLoss: 0.4370223581790924 - trainLoss: 0.4315066635608673\n",
      "cnt: 0 - valLoss: 0.43702131509780884 - trainLoss: 0.43150395154953003\n",
      "cnt: 0 - valLoss: 0.43702030181884766 - trainLoss: 0.43150126934051514\n",
      "cnt: 0 - valLoss: 0.4370192587375641 - trainLoss: 0.43149861693382263\n",
      "cnt: 0 - valLoss: 0.4370182454586029 - trainLoss: 0.43149587512016296\n",
      "cnt: 0 - valLoss: 0.43701717257499695 - trainLoss: 0.43149322271347046\n",
      "cnt: 0 - valLoss: 0.4370161294937134 - trainLoss: 0.43149057030677795\n",
      "cnt: 0 - valLoss: 0.4370151162147522 - trainLoss: 0.4314878582954407\n",
      "cnt: 0 - valLoss: 0.43701407313346863 - trainLoss: 0.4314851760864258\n",
      "cnt: 0 - valLoss: 0.43701305985450745 - trainLoss: 0.4314825236797333\n",
      "cnt: 0 - valLoss: 0.4370120167732239 - trainLoss: 0.4314797818660736\n",
      "cnt: 0 - valLoss: 0.4370109438896179 - trainLoss: 0.4314771294593811\n",
      "cnt: 0 - valLoss: 0.4370099604129791 - trainLoss: 0.4314744770526886\n",
      "cnt: 0 - valLoss: 0.43700888752937317 - trainLoss: 0.43147173523902893\n",
      "cnt: 0 - valLoss: 0.437007874250412 - trainLoss: 0.4314690828323364\n",
      "cnt: 0 - valLoss: 0.4370068311691284 - trainLoss: 0.4314664304256439\n",
      "cnt: 0 - valLoss: 0.43700581789016724 - trainLoss: 0.43146371841430664\n",
      "cnt: 0 - valLoss: 0.43700477480888367 - trainLoss: 0.431460976600647\n",
      "cnt: 0 - valLoss: 0.4370037317276001 - trainLoss: 0.43145832419395447\n",
      "cnt: 0 - valLoss: 0.4370027184486389 - trainLoss: 0.4314556419849396\n",
      "cnt: 0 - valLoss: 0.43700167536735535 - trainLoss: 0.4314529299736023\n",
      "cnt: 0 - valLoss: 0.43700066208839417 - trainLoss: 0.4314502775669098\n",
      "cnt: 0 - valLoss: 0.4369996190071106 - trainLoss: 0.4314475953578949\n",
      "cnt: 0 - valLoss: 0.4369986057281494 - trainLoss: 0.4314448833465576\n",
      "cnt: 0 - valLoss: 0.43699756264686584 - trainLoss: 0.4314422011375427\n",
      "cnt: 0 - valLoss: 0.43699654936790466 - trainLoss: 0.4314395487308502\n",
      "cnt: 0 - valLoss: 0.4369955062866211 - trainLoss: 0.43143683671951294\n",
      "cnt: 0 - valLoss: 0.43699443340301514 - trainLoss: 0.43143418431282043\n",
      "cnt: 0 - valLoss: 0.43699344992637634 - trainLoss: 0.43143150210380554\n",
      "cnt: 0 - valLoss: 0.4369923770427704 - trainLoss: 0.43142879009246826\n",
      "cnt: 0 - valLoss: 0.43699130415916443 - trainLoss: 0.43142610788345337\n",
      "cnt: 0 - valLoss: 0.43699032068252563 - trainLoss: 0.43142345547676086\n",
      "cnt: 0 - valLoss: 0.43698930740356445 - trainLoss: 0.4314207136631012\n",
      "cnt: 0 - valLoss: 0.4369882643222809 - trainLoss: 0.4314180612564087\n",
      "cnt: 0 - valLoss: 0.4369872510433197 - trainLoss: 0.431415319442749\n",
      "cnt: 0 - valLoss: 0.43698620796203613 - trainLoss: 0.4314126670360565\n",
      "cnt: 0 - valLoss: 0.43698519468307495 - trainLoss: 0.431410014629364\n",
      "cnt: 0 - valLoss: 0.4369841516017914 - trainLoss: 0.43140730261802673\n",
      "cnt: 0 - valLoss: 0.4369831383228302 - trainLoss: 0.43140462040901184\n",
      "cnt: 0 - valLoss: 0.43698209524154663 - trainLoss: 0.43140196800231934\n",
      "cnt: 0 - valLoss: 0.43698108196258545 - trainLoss: 0.43139922618865967\n",
      "cnt: 0 - valLoss: 0.4369800388813019 - trainLoss: 0.43139657378196716\n",
      "cnt: 0 - valLoss: 0.4369790256023407 - trainLoss: 0.4313938617706299\n",
      "cnt: 0 - valLoss: 0.43697795271873474 - trainLoss: 0.431391179561615\n",
      "cnt: 0 - valLoss: 0.43697696924209595 - trainLoss: 0.4313885271549225\n",
      "cnt: 0 - valLoss: 0.4369759261608124 - trainLoss: 0.4313858151435852\n",
      "cnt: 0 - valLoss: 0.4369749128818512 - trainLoss: 0.4313831329345703\n",
      "cnt: 0 - valLoss: 0.4369738698005676 - trainLoss: 0.43138042092323303\n",
      "cnt: 0 - valLoss: 0.43697285652160645 - trainLoss: 0.43137773871421814\n",
      "cnt: 0 - valLoss: 0.43697184324264526 - trainLoss: 0.43137508630752563\n",
      "cnt: 0 - valLoss: 0.4369708299636841 - trainLoss: 0.43137237429618835\n",
      "cnt: 0 - valLoss: 0.4369697868824005 - trainLoss: 0.43136969208717346\n",
      "cnt: 0 - valLoss: 0.43696877360343933 - trainLoss: 0.43136703968048096\n",
      "cnt: 0 - valLoss: 0.43696773052215576 - trainLoss: 0.4313643276691437\n",
      "cnt: 0 - valLoss: 0.4369667172431946 - trainLoss: 0.4313616454601288\n",
      "cnt: 0 - valLoss: 0.436965674161911 - trainLoss: 0.4313589930534363\n",
      "cnt: 0 - valLoss: 0.4369646906852722 - trainLoss: 0.4313562512397766\n",
      "cnt: 0 - valLoss: 0.43696367740631104 - trainLoss: 0.4313535988330841\n",
      "cnt: 0 - valLoss: 0.43696263432502747 - trainLoss: 0.4313508868217468\n",
      "cnt: 0 - valLoss: 0.4369616210460663 - trainLoss: 0.43134820461273193\n",
      "cnt: 0 - valLoss: 0.4369605779647827 - trainLoss: 0.43134555220603943\n",
      "cnt: 0 - valLoss: 0.4369595944881439 - trainLoss: 0.43134284019470215\n",
      "cnt: 0 - valLoss: 0.43695858120918274 - trainLoss: 0.43134015798568726\n",
      "cnt: 0 - valLoss: 0.43695753812789917 - trainLoss: 0.43133747577667236\n",
      "cnt: 0 - valLoss: 0.436956524848938 - trainLoss: 0.43133479356765747\n",
      "cnt: 0 - valLoss: 0.4369554817676544 - trainLoss: 0.4313321113586426\n",
      "cnt: 0 - valLoss: 0.43695446848869324 - trainLoss: 0.4313294589519501\n",
      "cnt: 0 - valLoss: 0.43695342540740967 - trainLoss: 0.4313267171382904\n",
      "cnt: 0 - valLoss: 0.4369524419307709 - trainLoss: 0.4313240647315979\n",
      "cnt: 0 - valLoss: 0.4369514584541321 - trainLoss: 0.4313213527202606\n",
      "cnt: 0 - valLoss: 0.4369503855705261 - trainLoss: 0.4313186705112457\n",
      "cnt: 0 - valLoss: 0.43694940209388733 - trainLoss: 0.4313160181045532\n",
      "cnt: 0 - valLoss: 0.43694835901260376 - trainLoss: 0.43131330609321594\n",
      "cnt: 0 - valLoss: 0.4369473457336426 - trainLoss: 0.43131062388420105\n",
      "cnt: 0 - valLoss: 0.436946302652359 - trainLoss: 0.43130797147750854\n",
      "cnt: 0 - valLoss: 0.4369452893733978 - trainLoss: 0.43130525946617126\n",
      "cnt: 0 - valLoss: 0.43694430589675903 - trainLoss: 0.43130257725715637\n",
      "cnt: 0 - valLoss: 0.43694326281547546 - trainLoss: 0.43129992485046387\n",
      "cnt: 0 - valLoss: 0.43694227933883667 - trainLoss: 0.4312972128391266\n",
      "cnt: 0 - valLoss: 0.4369412660598755 - trainLoss: 0.4312945306301117\n",
      "cnt: 0 - valLoss: 0.4369402229785919 - trainLoss: 0.4312918484210968\n",
      "cnt: 0 - valLoss: 0.43693920969963074 - trainLoss: 0.4312891364097595\n",
      "cnt: 0 - valLoss: 0.43693819642066956 - trainLoss: 0.431286484003067\n",
      "cnt: 0 - valLoss: 0.4369371831417084 - trainLoss: 0.4312838017940521\n",
      "cnt: 0 - valLoss: 0.4369361400604248 - trainLoss: 0.43128108978271484\n",
      "cnt: 0 - valLoss: 0.436935156583786 - trainLoss: 0.43127843737602234\n",
      "cnt: 0 - valLoss: 0.43693414330482483 - trainLoss: 0.43127569556236267\n",
      "cnt: 0 - valLoss: 0.43693310022354126 - trainLoss: 0.43127304315567017\n",
      "cnt: 0 - valLoss: 0.4369320869445801 - trainLoss: 0.43127039074897766\n",
      "cnt: 0 - valLoss: 0.4369310438632965 - trainLoss: 0.431267648935318\n",
      "cnt: 0 - valLoss: 0.4369300305843353 - trainLoss: 0.4312649965286255\n",
      "cnt: 0 - valLoss: 0.43692904710769653 - trainLoss: 0.4312622547149658\n",
      "cnt: 0 - valLoss: 0.4369279742240906 - trainLoss: 0.4312596023082733\n",
      "cnt: 0 - valLoss: 0.4369269907474518 - trainLoss: 0.4312569200992584\n",
      "cnt: 0 - valLoss: 0.4369259476661682 - trainLoss: 0.43125420808792114\n",
      "cnt: 0 - valLoss: 0.4369249641895294 - trainLoss: 0.43125155568122864\n",
      "cnt: 0 - valLoss: 0.43692395091056824 - trainLoss: 0.43124890327453613\n",
      "cnt: 0 - valLoss: 0.43692290782928467 - trainLoss: 0.43124616146087646\n",
      "cnt: 0 - valLoss: 0.4369218945503235 - trainLoss: 0.43124350905418396\n",
      "cnt: 0 - valLoss: 0.4369208514690399 - trainLoss: 0.4312407672405243\n",
      "cnt: 0 - valLoss: 0.4369198679924011 - trainLoss: 0.4312381148338318\n",
      "cnt: 0 - valLoss: 0.43691879510879517 - trainLoss: 0.4312354624271393\n",
      "cnt: 0 - valLoss: 0.43691781163215637 - trainLoss: 0.4312327206134796\n",
      "cnt: 0 - valLoss: 0.4369168281555176 - trainLoss: 0.4312300682067871\n",
      "cnt: 0 - valLoss: 0.436915785074234 - trainLoss: 0.4312273859977722\n",
      "cnt: 0 - valLoss: 0.4369147717952728 - trainLoss: 0.43122467398643494\n",
      "cnt: 0 - valLoss: 0.43691372871398926 - trainLoss: 0.43122202157974243\n",
      "cnt: 0 - valLoss: 0.4369127154350281 - trainLoss: 0.43121933937072754\n",
      "cnt: 0 - valLoss: 0.4369117319583893 - trainLoss: 0.43121662735939026\n",
      "cnt: 0 - valLoss: 0.4369106888771057 - trainLoss: 0.43121397495269775\n",
      "cnt: 0 - valLoss: 0.4369097054004669 - trainLoss: 0.4312112331390381\n",
      "cnt: 0 - valLoss: 0.43690869212150574 - trainLoss: 0.4312085807323456\n",
      "cnt: 0 - valLoss: 0.43690764904022217 - trainLoss: 0.4312059283256531\n",
      "cnt: 0 - valLoss: 0.4369066655635834 - trainLoss: 0.4312031865119934\n",
      "cnt: 0 - valLoss: 0.4369056224822998 - trainLoss: 0.4312005341053009\n",
      "cnt: 0 - valLoss: 0.4369046092033386 - trainLoss: 0.43119779229164124\n",
      "cnt: 0 - valLoss: 0.43690362572669983 - trainLoss: 0.4311951994895935\n",
      "cnt: 0 - valLoss: 0.43690258264541626 - trainLoss: 0.43119245767593384\n",
      "cnt: 0 - valLoss: 0.4369015693664551 - trainLoss: 0.43118980526924133\n",
      "cnt: 0 - valLoss: 0.4369005858898163 - trainLoss: 0.43118709325790405\n",
      "cnt: 0 - valLoss: 0.4368995130062103 - trainLoss: 0.43118444085121155\n",
      "cnt: 0 - valLoss: 0.43689852952957153 - trainLoss: 0.4311816990375519\n",
      "cnt: 0 - valLoss: 0.43689754605293274 - trainLoss: 0.4311790466308594\n",
      "cnt: 0 - valLoss: 0.43689650297164917 - trainLoss: 0.4311763644218445\n",
      "cnt: 0 - valLoss: 0.436895489692688 - trainLoss: 0.4311736524105072\n",
      "cnt: 0 - valLoss: 0.4368944466114044 - trainLoss: 0.4311710000038147\n",
      "cnt: 0 - valLoss: 0.43689343333244324 - trainLoss: 0.43116825819015503\n",
      "cnt: 0 - valLoss: 0.43689242005348206 - trainLoss: 0.4311656057834625\n",
      "cnt: 0 - valLoss: 0.43689143657684326 - trainLoss: 0.43116292357444763\n",
      "cnt: 0 - valLoss: 0.4368904232978821 - trainLoss: 0.4311602711677551\n",
      "cnt: 0 - valLoss: 0.4368893802165985 - trainLoss: 0.43115755915641785\n",
      "cnt: 0 - valLoss: 0.43688836693763733 - trainLoss: 0.43115487694740295\n",
      "cnt: 0 - valLoss: 0.43688732385635376 - trainLoss: 0.4311521649360657\n",
      "cnt: 0 - valLoss: 0.4368863105773926 - trainLoss: 0.43114951252937317\n",
      "cnt: 0 - valLoss: 0.4368853271007538 - trainLoss: 0.4311468303203583\n",
      "cnt: 0 - valLoss: 0.436884343624115 - trainLoss: 0.431144118309021\n",
      "cnt: 0 - valLoss: 0.4368833005428314 - trainLoss: 0.4311414659023285\n",
      "cnt: 0 - valLoss: 0.43688228726387024 - trainLoss: 0.4311387836933136\n",
      "cnt: 0 - valLoss: 0.43688124418258667 - trainLoss: 0.4311360716819763\n",
      "cnt: 0 - valLoss: 0.4368802607059479 - trainLoss: 0.4311333894729614\n",
      "cnt: 0 - valLoss: 0.4368792772293091 - trainLoss: 0.43113067746162415\n",
      "cnt: 0 - valLoss: 0.4368782341480255 - trainLoss: 0.43112802505493164\n",
      "cnt: 0 - valLoss: 0.43687722086906433 - trainLoss: 0.431125283241272\n",
      "cnt: 0 - valLoss: 0.43687617778778076 - trainLoss: 0.43112263083457947\n",
      "cnt: 0 - valLoss: 0.43687519431114197 - trainLoss: 0.4311199486255646\n",
      "cnt: 0 - valLoss: 0.4368741810321808 - trainLoss: 0.4311172366142273\n",
      "cnt: 0 - valLoss: 0.436873197555542 - trainLoss: 0.4311145544052124\n",
      "cnt: 0 - valLoss: 0.4368721544742584 - trainLoss: 0.4311119019985199\n",
      "cnt: 0 - valLoss: 0.43687117099761963 - trainLoss: 0.4311091899871826\n",
      "cnt: 0 - valLoss: 0.43687015771865845 - trainLoss: 0.4311065375804901\n",
      "cnt: 0 - valLoss: 0.4368691146373749 - trainLoss: 0.43110379576683044\n",
      "cnt: 0 - valLoss: 0.4368680417537689 - trainLoss: 0.43110114336013794\n",
      "cnt: 0 - valLoss: 0.43686702847480774 - trainLoss: 0.43109846115112305\n",
      "cnt: 0 - valLoss: 0.43686598539352417 - trainLoss: 0.43109574913978577\n",
      "cnt: 0 - valLoss: 0.436864972114563 - trainLoss: 0.43109309673309326\n",
      "cnt: 0 - valLoss: 0.4368639290332794 - trainLoss: 0.43109041452407837\n",
      "cnt: 0 - valLoss: 0.43686291575431824 - trainLoss: 0.4310877025127411\n",
      "cnt: 0 - valLoss: 0.43686187267303467 - trainLoss: 0.4310850501060486\n",
      "cnt: 0 - valLoss: 0.4368608593940735 - trainLoss: 0.4310823678970337\n",
      "cnt: 0 - valLoss: 0.43685978651046753 - trainLoss: 0.4310796558856964\n",
      "cnt: 0 - valLoss: 0.43685874342918396 - trainLoss: 0.4310770034790039\n",
      "cnt: 0 - valLoss: 0.4368577301502228 - trainLoss: 0.431074321269989\n",
      "cnt: 0 - valLoss: 0.4368566572666168 - trainLoss: 0.43107160925865173\n",
      "cnt: 0 - valLoss: 0.43685561418533325 - trainLoss: 0.43106895685195923\n",
      "cnt: 0 - valLoss: 0.43685460090637207 - trainLoss: 0.4310663044452667\n",
      "cnt: 0 - valLoss: 0.4368535578250885 - trainLoss: 0.43106356263160706\n",
      "cnt: 0 - valLoss: 0.4368525445461273 - trainLoss: 0.43106091022491455\n",
      "cnt: 0 - valLoss: 0.43685147166252136 - trainLoss: 0.43105822801589966\n",
      "cnt: 0 - valLoss: 0.4368504583835602 - trainLoss: 0.4310555160045624\n",
      "cnt: 0 - valLoss: 0.4368494153022766 - trainLoss: 0.4310528635978699\n",
      "cnt: 0 - valLoss: 0.43684837222099304 - trainLoss: 0.431050181388855\n",
      "cnt: 0 - valLoss: 0.4368473291397095 - trainLoss: 0.4310474693775177\n",
      "cnt: 0 - valLoss: 0.4368463158607483 - trainLoss: 0.4310448169708252\n",
      "cnt: 0 - valLoss: 0.4368452727794647 - trainLoss: 0.4310421347618103\n",
      "cnt: 0 - valLoss: 0.43684422969818115 - trainLoss: 0.431039422750473\n",
      "cnt: 0 - valLoss: 0.4368431866168976 - trainLoss: 0.4310367703437805\n",
      "cnt: 0 - valLoss: 0.436842143535614 - trainLoss: 0.43103402853012085\n",
      "cnt: 0 - valLoss: 0.43684113025665283 - trainLoss: 0.43103137612342834\n",
      "cnt: 0 - valLoss: 0.43684008717536926 - trainLoss: 0.43102872371673584\n",
      "cnt: 0 - valLoss: 0.4368390738964081 - trainLoss: 0.43102604150772095\n",
      "cnt: 0 - valLoss: 0.4368380010128021 - trainLoss: 0.43102332949638367\n",
      "cnt: 0 - valLoss: 0.43683701753616333 - trainLoss: 0.4310206472873688\n",
      "cnt: 0 - valLoss: 0.4368358850479126 - trainLoss: 0.4310179352760315\n",
      "cnt: 0 - valLoss: 0.4368348717689514 - trainLoss: 0.431015282869339\n",
      "cnt: 0 - valLoss: 0.4368338882923126 - trainLoss: 0.4310126304626465\n",
      "cnt: 0 - valLoss: 0.43683281540870667 - trainLoss: 0.4310098886489868\n",
      "cnt: 0 - valLoss: 0.4368317723274231 - trainLoss: 0.4310072362422943\n",
      "cnt: 0 - valLoss: 0.4368307590484619 - trainLoss: 0.4310045838356018\n",
      "cnt: 0 - valLoss: 0.43682971596717834 - trainLoss: 0.4310019016265869\n",
      "cnt: 0 - valLoss: 0.43682870268821716 - trainLoss: 0.43099918961524963\n",
      "cnt: 0 - valLoss: 0.4368276298046112 - trainLoss: 0.43099650740623474\n",
      "cnt: 0 - valLoss: 0.43682658672332764 - trainLoss: 0.43099379539489746\n",
      "cnt: 0 - valLoss: 0.43682557344436646 - trainLoss: 0.43099114298820496\n",
      "cnt: 0 - valLoss: 0.4368245303630829 - trainLoss: 0.43098846077919006\n",
      "cnt: 0 - valLoss: 0.43682345747947693 - trainLoss: 0.43098580837249756\n",
      "cnt: 0 - valLoss: 0.43682244420051575 - trainLoss: 0.4309830963611603\n",
      "cnt: 0 - valLoss: 0.4368214011192322 - trainLoss: 0.4309803545475006\n",
      "cnt: 0 - valLoss: 0.436820387840271 - trainLoss: 0.4309777021408081\n",
      "cnt: 0 - valLoss: 0.4368193447589874 - trainLoss: 0.4309750497341156\n",
      "cnt: 0 - valLoss: 0.43681833148002625 - trainLoss: 0.4309723675251007\n",
      "cnt: 0 - valLoss: 0.4368172585964203 - trainLoss: 0.4309696555137634\n",
      "cnt: 0 - valLoss: 0.4368162751197815 - trainLoss: 0.4309670031070709\n",
      "cnt: 0 - valLoss: 0.43681520223617554 - trainLoss: 0.43096432089805603\n",
      "cnt: 0 - valLoss: 0.43681415915489197 - trainLoss: 0.43096160888671875\n",
      "cnt: 0 - valLoss: 0.436813086271286 - trainLoss: 0.43095895648002625\n",
      "cnt: 0 - valLoss: 0.43681207299232483 - trainLoss: 0.4309562146663666\n",
      "cnt: 0 - valLoss: 0.43681102991104126 - trainLoss: 0.4309535622596741\n",
      "cnt: 0 - valLoss: 0.4368100166320801 - trainLoss: 0.43095090985298157\n",
      "cnt: 0 - valLoss: 0.4368089735507965 - trainLoss: 0.4309482276439667\n",
      "cnt: 0 - valLoss: 0.43680790066719055 - trainLoss: 0.4309455156326294\n",
      "cnt: 0 - valLoss: 0.43680691719055176 - trainLoss: 0.4309428632259369\n",
      "cnt: 0 - valLoss: 0.4368058443069458 - trainLoss: 0.4309401214122772\n",
      "cnt: 0 - valLoss: 0.4368048310279846 - trainLoss: 0.4309374690055847\n",
      "cnt: 0 - valLoss: 0.4368038475513458 - trainLoss: 0.4309348165988922\n",
      "cnt: 0 - valLoss: 0.43680277466773987 - trainLoss: 0.43093207478523254\n",
      "cnt: 0 - valLoss: 0.4368017315864563 - trainLoss: 0.43092942237854004\n",
      "cnt: 0 - valLoss: 0.4368007481098175 - trainLoss: 0.43092674016952515\n",
      "cnt: 0 - valLoss: 0.43679967522621155 - trainLoss: 0.43092408776283264\n",
      "cnt: 0 - valLoss: 0.43679869174957275 - trainLoss: 0.43092137575149536\n",
      "cnt: 0 - valLoss: 0.4367976784706116 - trainLoss: 0.43091869354248047\n",
      "cnt: 0 - valLoss: 0.436796635389328 - trainLoss: 0.4309159815311432\n",
      "cnt: 0 - valLoss: 0.4367956221103668 - trainLoss: 0.4309133291244507\n",
      "cnt: 0 - valLoss: 0.43679454922676086 - trainLoss: 0.430910587310791\n",
      "cnt: 0 - valLoss: 0.43679356575012207 - trainLoss: 0.4309079349040985\n",
      "cnt: 0 - valLoss: 0.4367925226688385 - trainLoss: 0.430905282497406\n",
      "cnt: 0 - valLoss: 0.4367915093898773 - trainLoss: 0.43090254068374634\n",
      "cnt: 0 - valLoss: 0.43679046630859375 - trainLoss: 0.43089988827705383\n",
      "cnt: 0 - valLoss: 0.4367894232273102 - trainLoss: 0.43089723587036133\n",
      "cnt: 0 - valLoss: 0.436788409948349 - trainLoss: 0.43089455366134644\n",
      "cnt: 0 - valLoss: 0.43678736686706543 - trainLoss: 0.43089184165000916\n",
      "cnt: 0 - valLoss: 0.43678635358810425 - trainLoss: 0.43088915944099426\n",
      "cnt: 0 - valLoss: 0.4367853105068207 - trainLoss: 0.43088650703430176\n",
      "cnt: 0 - valLoss: 0.4367842972278595 - trainLoss: 0.4308837950229645\n",
      "cnt: 0 - valLoss: 0.4367833137512207 - trainLoss: 0.430881142616272\n",
      "cnt: 0 - valLoss: 0.43678227066993713 - trainLoss: 0.4308784008026123\n",
      "cnt: 0 - valLoss: 0.4367811977863312 - trainLoss: 0.4308757483959198\n",
      "cnt: 0 - valLoss: 0.4367802143096924 - trainLoss: 0.4308730661869049\n",
      "cnt: 0 - valLoss: 0.4367792010307312 - trainLoss: 0.4308703541755676\n",
      "cnt: 0 - valLoss: 0.4367782175540924 - trainLoss: 0.4308677017688751\n",
      "cnt: 0 - valLoss: 0.43677717447280884 - trainLoss: 0.4308650493621826\n",
      "cnt: 0 - valLoss: 0.43677616119384766 - trainLoss: 0.43086230754852295\n",
      "cnt: 0 - valLoss: 0.4367751181125641 - trainLoss: 0.43085965514183044\n",
      "cnt: 0 - valLoss: 0.4367741048336029 - trainLoss: 0.43085700273513794\n",
      "cnt: 0 - valLoss: 0.43677306175231934 - trainLoss: 0.43085426092147827\n",
      "cnt: 0 - valLoss: 0.43677207827568054 - trainLoss: 0.43085160851478577\n",
      "cnt: 0 - valLoss: 0.436771035194397 - trainLoss: 0.43084895610809326\n",
      "cnt: 0 - valLoss: 0.4367700517177582 - trainLoss: 0.43084627389907837\n",
      "cnt: 0 - valLoss: 0.436769038438797 - trainLoss: 0.43084362149238586\n",
      "cnt: 0 - valLoss: 0.4367679953575134 - trainLoss: 0.4308409094810486\n",
      "cnt: 0 - valLoss: 0.43676698207855225 - trainLoss: 0.4308382570743561\n",
      "cnt: 0 - valLoss: 0.4367659389972687 - trainLoss: 0.4308355748653412\n",
      "cnt: 0 - valLoss: 0.4367649555206299 - trainLoss: 0.4308328628540039\n",
      "cnt: 0 - valLoss: 0.4367639720439911 - trainLoss: 0.4308302104473114\n",
      "cnt: 0 - valLoss: 0.4367629587650299 - trainLoss: 0.4308275580406189\n",
      "cnt: 0 - valLoss: 0.43676191568374634 - trainLoss: 0.430824875831604\n",
      "cnt: 0 - valLoss: 0.43676093220710754 - trainLoss: 0.4308221638202667\n",
      "cnt: 0 - valLoss: 0.43675991892814636 - trainLoss: 0.4308195114135742\n",
      "cnt: 0 - valLoss: 0.4367588758468628 - trainLoss: 0.4308168292045593\n",
      "cnt: 0 - valLoss: 0.4367578625679016 - trainLoss: 0.43081411719322205\n",
      "cnt: 0 - valLoss: 0.43675684928894043 - trainLoss: 0.43081146478652954\n",
      "cnt: 0 - valLoss: 0.43675583600997925 - trainLoss: 0.43080881237983704\n",
      "cnt: 0 - valLoss: 0.4367547929286957 - trainLoss: 0.43080607056617737\n",
      "cnt: 0 - valLoss: 0.4367537796497345 - trainLoss: 0.43080341815948486\n",
      "cnt: 0 - valLoss: 0.4367527365684509 - trainLoss: 0.43080076575279236\n",
      "cnt: 0 - valLoss: 0.4367518126964569 - trainLoss: 0.43079808354377747\n",
      "cnt: 0 - valLoss: 0.43675076961517334 - trainLoss: 0.4307953715324402\n",
      "cnt: 0 - valLoss: 0.43674975633621216 - trainLoss: 0.4307927191257477\n",
      "cnt: 0 - valLoss: 0.4367487132549286 - trainLoss: 0.4307900369167328\n",
      "cnt: 0 - valLoss: 0.4367477297782898 - trainLoss: 0.4307873249053955\n",
      "cnt: 0 - valLoss: 0.4367467164993286 - trainLoss: 0.430784672498703\n",
      "cnt: 0 - valLoss: 0.43674564361572266 - trainLoss: 0.4307820200920105\n",
      "cnt: 0 - valLoss: 0.43674466013908386 - trainLoss: 0.4307793378829956\n",
      "cnt: 0 - valLoss: 0.4367436468601227 - trainLoss: 0.4307766258716583\n",
      "cnt: 0 - valLoss: 0.4367426335811615 - trainLoss: 0.4307739734649658\n",
      "cnt: 0 - valLoss: 0.43674159049987793 - trainLoss: 0.4307713210582733\n",
      "cnt: 0 - valLoss: 0.43674057722091675 - trainLoss: 0.43076857924461365\n",
      "cnt: 0 - valLoss: 0.43673959374427795 - trainLoss: 0.43076592683792114\n",
      "cnt: 0 - valLoss: 0.43673861026763916 - trainLoss: 0.43076327443122864\n",
      "cnt: 0 - valLoss: 0.4367375671863556 - trainLoss: 0.43076062202453613\n",
      "cnt: 0 - valLoss: 0.4367365539073944 - trainLoss: 0.43075788021087646\n",
      "cnt: 0 - valLoss: 0.43673551082611084 - trainLoss: 0.43075522780418396\n",
      "cnt: 0 - valLoss: 0.43673452734947205 - trainLoss: 0.43075257539749146\n",
      "cnt: 0 - valLoss: 0.43673351407051086 - trainLoss: 0.4307498335838318\n",
      "cnt: 0 - valLoss: 0.43673253059387207 - trainLoss: 0.4307471811771393\n",
      "cnt: 0 - valLoss: 0.4367314577102661 - trainLoss: 0.4307445287704468\n",
      "cnt: 0 - valLoss: 0.4367304742336273 - trainLoss: 0.4307418465614319\n",
      "cnt: 0 - valLoss: 0.43672943115234375 - trainLoss: 0.4307391345500946\n",
      "cnt: 0 - valLoss: 0.4367283880710602 - trainLoss: 0.4307364821434021\n",
      "cnt: 0 - valLoss: 0.436727374792099 - trainLoss: 0.4307338297367096\n",
      "cnt: 0 - valLoss: 0.43672633171081543 - trainLoss: 0.4307311475276947\n",
      "cnt: 0 - valLoss: 0.4367254078388214 - trainLoss: 0.4307284355163574\n",
      "cnt: 0 - valLoss: 0.43672436475753784 - trainLoss: 0.4307257831096649\n",
      "cnt: 0 - valLoss: 0.43672335147857666 - trainLoss: 0.43072307109832764\n",
      "cnt: 0 - valLoss: 0.4367223083972931 - trainLoss: 0.43072038888931274\n",
      "cnt: 0 - valLoss: 0.4367212951183319 - trainLoss: 0.43071773648262024\n",
      "cnt: 0 - valLoss: 0.4367203116416931 - trainLoss: 0.43071508407592773\n",
      "cnt: 0 - valLoss: 0.43671926856040955 - trainLoss: 0.43071237206459045\n",
      "cnt: 0 - valLoss: 0.43671828508377075 - trainLoss: 0.43070968985557556\n",
      "cnt: 0 - valLoss: 0.43671727180480957 - trainLoss: 0.43070703744888306\n",
      "cnt: 0 - valLoss: 0.436716228723526 - trainLoss: 0.43070438504219055\n",
      "cnt: 0 - valLoss: 0.4367152452468872 - trainLoss: 0.43070173263549805\n",
      "cnt: 0 - valLoss: 0.4367142617702484 - trainLoss: 0.43069908022880554\n",
      "cnt: 0 - valLoss: 0.43671321868896484 - trainLoss: 0.43069642782211304\n",
      "cnt: 0 - valLoss: 0.43671220541000366 - trainLoss: 0.43069371581077576\n",
      "cnt: 0 - valLoss: 0.43671125173568726 - trainLoss: 0.43069103360176086\n",
      "cnt: 0 - valLoss: 0.4367102384567261 - trainLoss: 0.43068838119506836\n",
      "cnt: 0 - valLoss: 0.4367091953754425 - trainLoss: 0.43068572878837585\n",
      "cnt: 0 - valLoss: 0.4367081820964813 - trainLoss: 0.43068307638168335\n",
      "cnt: 0 - valLoss: 0.4367072284221649 - trainLoss: 0.43068042397499084\n",
      "cnt: 0 - valLoss: 0.43670618534088135 - trainLoss: 0.43067777156829834\n",
      "cnt: 0 - valLoss: 0.43670520186424255 - trainLoss: 0.43067505955696106\n",
      "cnt: 0 - valLoss: 0.43670418858528137 - trainLoss: 0.43067237734794617\n",
      "cnt: 0 - valLoss: 0.4367031455039978 - trainLoss: 0.43066972494125366\n",
      "cnt: 0 - valLoss: 0.436702162027359 - trainLoss: 0.43066707253456116\n",
      "cnt: 0 - valLoss: 0.4367011487483978 - trainLoss: 0.43066439032554626\n",
      "cnt: 0 - valLoss: 0.43670016527175903 - trainLoss: 0.43066173791885376\n",
      "cnt: 0 - valLoss: 0.43669918179512024 - trainLoss: 0.43065908551216125\n",
      "cnt: 0 - valLoss: 0.43669813871383667 - trainLoss: 0.43065640330314636\n",
      "cnt: 0 - valLoss: 0.4366971254348755 - trainLoss: 0.4306536912918091\n",
      "cnt: 0 - valLoss: 0.4366961419582367 - trainLoss: 0.4306510388851166\n",
      "cnt: 0 - valLoss: 0.4366951286792755 - trainLoss: 0.4306483864784241\n",
      "cnt: 0 - valLoss: 0.43669411540031433 - trainLoss: 0.43064573407173157\n",
      "cnt: 0 - valLoss: 0.43669313192367554 - trainLoss: 0.4306430518627167\n",
      "cnt: 0 - valLoss: 0.43669214844703674 - trainLoss: 0.43064042925834656\n",
      "cnt: 0 - valLoss: 0.4366911053657532 - trainLoss: 0.43063774704933167\n",
      "cnt: 0 - valLoss: 0.436690092086792 - trainLoss: 0.4306350350379944\n",
      "cnt: 0 - valLoss: 0.4366890490055084 - trainLoss: 0.4306323826313019\n",
      "cnt: 0 - valLoss: 0.43668806552886963 - trainLoss: 0.4306297302246094\n",
      "cnt: 0 - valLoss: 0.43668708205223083 - trainLoss: 0.43062707781791687\n",
      "cnt: 0 - valLoss: 0.43668606877326965 - trainLoss: 0.43062442541122437\n",
      "cnt: 0 - valLoss: 0.43668505549430847 - trainLoss: 0.43062177300453186\n",
      "cnt: 0 - valLoss: 0.4366840422153473 - trainLoss: 0.43061909079551697\n",
      "cnt: 0 - valLoss: 0.4366830587387085 - trainLoss: 0.43061643838882446\n",
      "cnt: 0 - valLoss: 0.4366820156574249 - trainLoss: 0.43061375617980957\n",
      "cnt: 0 - valLoss: 0.43668100237846375 - trainLoss: 0.4306110739707947\n",
      "cnt: 0 - valLoss: 0.43668001890182495 - trainLoss: 0.4306084215641022\n",
      "cnt: 0 - valLoss: 0.43667903542518616 - trainLoss: 0.43060576915740967\n",
      "cnt: 0 - valLoss: 0.4366779923439026 - trainLoss: 0.4306030571460724\n",
      "cnt: 0 - valLoss: 0.4366770088672638 - trainLoss: 0.43060046434402466\n",
      "cnt: 0 - valLoss: 0.4366759955883026 - trainLoss: 0.4305977523326874\n",
      "cnt: 0 - valLoss: 0.43667498230934143 - trainLoss: 0.4305950701236725\n",
      "cnt: 0 - valLoss: 0.43667396903038025 - trainLoss: 0.43059241771698\n",
      "cnt: 0 - valLoss: 0.4366729259490967 - trainLoss: 0.4305897653102875\n",
      "cnt: 0 - valLoss: 0.4366719424724579 - trainLoss: 0.43058711290359497\n",
      "cnt: 0 - valLoss: 0.4366709291934967 - trainLoss: 0.43058446049690247\n",
      "cnt: 0 - valLoss: 0.4366699457168579 - trainLoss: 0.43058180809020996\n",
      "cnt: 0 - valLoss: 0.43666890263557434 - trainLoss: 0.4305790960788727\n",
      "cnt: 0 - valLoss: 0.43666791915893555 - trainLoss: 0.4305764138698578\n",
      "cnt: 0 - valLoss: 0.43666690587997437 - trainLoss: 0.4305737614631653\n",
      "cnt: 0 - valLoss: 0.43666592240333557 - trainLoss: 0.4305711090564728\n",
      "cnt: 0 - valLoss: 0.436664879322052 - trainLoss: 0.4305683970451355\n",
      "cnt: 0 - valLoss: 0.4366638660430908 - trainLoss: 0.43056580424308777\n",
      "cnt: 0 - valLoss: 0.43666285276412964 - trainLoss: 0.4305631220340729\n",
      "cnt: 0 - valLoss: 0.43666186928749084 - trainLoss: 0.430560439825058\n",
      "cnt: 0 - valLoss: 0.43666085600852966 - trainLoss: 0.4305577874183655\n",
      "cnt: 0 - valLoss: 0.43665987253189087 - trainLoss: 0.4305550754070282\n",
      "cnt: 0 - valLoss: 0.4366588294506073 - trainLoss: 0.4305524230003357\n",
      "cnt: 0 - valLoss: 0.4366578161716461 - trainLoss: 0.4305497705936432\n",
      "cnt: 0 - valLoss: 0.4366568326950073 - trainLoss: 0.4305471181869507\n",
      "cnt: 0 - valLoss: 0.43665578961372375 - trainLoss: 0.4305444657802582\n",
      "cnt: 0 - valLoss: 0.43665480613708496 - trainLoss: 0.4305417239665985\n",
      "cnt: 0 - valLoss: 0.43665382266044617 - trainLoss: 0.430539071559906\n",
      "cnt: 0 - valLoss: 0.4366527795791626 - trainLoss: 0.4305364191532135\n",
      "cnt: 0 - valLoss: 0.4366517663002014 - trainLoss: 0.430533766746521\n",
      "cnt: 0 - valLoss: 0.4366507828235626 - trainLoss: 0.4305311143398285\n",
      "cnt: 0 - valLoss: 0.43664973974227905 - trainLoss: 0.430528461933136\n",
      "cnt: 0 - valLoss: 0.43664872646331787 - trainLoss: 0.4305257797241211\n",
      "cnt: 0 - valLoss: 0.4366477429866791 - trainLoss: 0.4305231273174286\n",
      "cnt: 0 - valLoss: 0.4366466999053955 - trainLoss: 0.4305204153060913\n",
      "cnt: 0 - valLoss: 0.4366457164287567 - trainLoss: 0.4305177628993988\n",
      "cnt: 0 - valLoss: 0.4366447329521179 - trainLoss: 0.4305151104927063\n",
      "cnt: 0 - valLoss: 0.43664371967315674 - trainLoss: 0.4305124580860138\n",
      "cnt: 0 - valLoss: 0.43664267659187317 - trainLoss: 0.4305097758769989\n",
      "cnt: 0 - valLoss: 0.4366416931152344 - trainLoss: 0.4305071234703064\n",
      "cnt: 0 - valLoss: 0.4366407096385956 - trainLoss: 0.4305044114589691\n",
      "cnt: 0 - valLoss: 0.436639666557312 - trainLoss: 0.4305017590522766\n",
      "cnt: 0 - valLoss: 0.43663865327835083 - trainLoss: 0.4304991066455841\n",
      "cnt: 0 - valLoss: 0.43663761019706726 - trainLoss: 0.4304964542388916\n",
      "cnt: 0 - valLoss: 0.43663662672042847 - trainLoss: 0.4304937422275543\n",
      "cnt: 0 - valLoss: 0.4366356432437897 - trainLoss: 0.43049106001853943\n",
      "cnt: 0 - valLoss: 0.4366346299648285 - trainLoss: 0.4304884374141693\n",
      "cnt: 0 - valLoss: 0.4366335868835449 - trainLoss: 0.4304857552051544\n",
      "cnt: 0 - valLoss: 0.43663260340690613 - trainLoss: 0.4304831027984619\n",
      "cnt: 0 - valLoss: 0.43663159012794495 - trainLoss: 0.4304804503917694\n",
      "cnt: 0 - valLoss: 0.4366305470466614 - trainLoss: 0.4304777979850769\n",
      "cnt: 0 - valLoss: 0.4366295635700226 - trainLoss: 0.4304750859737396\n",
      "cnt: 0 - valLoss: 0.436628520488739 - trainLoss: 0.4304724335670471\n",
      "cnt: 0 - valLoss: 0.43662750720977783 - trainLoss: 0.4304697811603546\n",
      "cnt: 0 - valLoss: 0.43662652373313904 - trainLoss: 0.4304670989513397\n",
      "cnt: 0 - valLoss: 0.43662554025650024 - trainLoss: 0.4304644465446472\n",
      "cnt: 0 - valLoss: 0.4366244971752167 - trainLoss: 0.4304617941379547\n",
      "cnt: 0 - valLoss: 0.4366234838962555 - trainLoss: 0.43045908212661743\n",
      "cnt: 0 - valLoss: 0.4366225004196167 - trainLoss: 0.4304564893245697\n",
      "cnt: 0 - valLoss: 0.4366215169429779 - trainLoss: 0.4304537773132324\n",
      "cnt: 0 - valLoss: 0.43662047386169434 - trainLoss: 0.43045109510421753\n",
      "cnt: 0 - valLoss: 0.43661946058273315 - trainLoss: 0.43044841289520264\n",
      "cnt: 0 - valLoss: 0.4366184175014496 - trainLoss: 0.43044576048851013\n",
      "cnt: 0 - valLoss: 0.4366174340248108 - trainLoss: 0.4304431080818176\n",
      "cnt: 0 - valLoss: 0.4366163909435272 - trainLoss: 0.4304404556751251\n",
      "cnt: 0 - valLoss: 0.4366154074668884 - trainLoss: 0.43043777346611023\n",
      "cnt: 0 - valLoss: 0.43661439418792725 - trainLoss: 0.4304351210594177\n",
      "cnt: 0 - valLoss: 0.4366133511066437 - trainLoss: 0.43043240904808044\n",
      "cnt: 0 - valLoss: 0.4366123676300049 - trainLoss: 0.43042975664138794\n",
      "cnt: 0 - valLoss: 0.4366113543510437 - trainLoss: 0.43042710423469543\n",
      "cnt: 0 - valLoss: 0.4366103708744049 - trainLoss: 0.43042442202568054\n",
      "cnt: 0 - valLoss: 0.43660932779312134 - trainLoss: 0.4304217994213104\n",
      "cnt: 0 - valLoss: 0.43660831451416016 - trainLoss: 0.4304191470146179\n",
      "cnt: 0 - valLoss: 0.43660733103752136 - trainLoss: 0.430416464805603\n",
      "cnt: 0 - valLoss: 0.4366062879562378 - trainLoss: 0.43041375279426575\n",
      "cnt: 0 - valLoss: 0.436605304479599 - trainLoss: 0.43041110038757324\n",
      "cnt: 0 - valLoss: 0.43660426139831543 - trainLoss: 0.43040844798088074\n",
      "cnt: 0 - valLoss: 0.43660327792167664 - trainLoss: 0.43040576577186584\n",
      "cnt: 0 - valLoss: 0.4366022050380707 - trainLoss: 0.43040311336517334\n",
      "cnt: 0 - valLoss: 0.43660128116607666 - trainLoss: 0.43040046095848083\n",
      "cnt: 0 - valLoss: 0.4366002380847931 - trainLoss: 0.43039777874946594\n",
      "cnt: 0 - valLoss: 0.4365992248058319 - trainLoss: 0.43039509654045105\n",
      "cnt: 0 - valLoss: 0.43659818172454834 - trainLoss: 0.43039244413375854\n",
      "cnt: 0 - valLoss: 0.43659719824790955 - trainLoss: 0.43038979172706604\n",
      "cnt: 0 - valLoss: 0.43659621477127075 - trainLoss: 0.43038713932037354\n",
      "cnt: 0 - valLoss: 0.43659520149230957 - trainLoss: 0.43038439750671387\n",
      "cnt: 0 - valLoss: 0.43659424781799316 - trainLoss: 0.43038177490234375\n",
      "cnt: 0 - valLoss: 0.4365932047367096 - trainLoss: 0.43037912249565125\n",
      "cnt: 0 - valLoss: 0.4365922212600708 - trainLoss: 0.43037644028663635\n",
      "cnt: 0 - valLoss: 0.436591237783432 - trainLoss: 0.43037378787994385\n",
      "cnt: 0 - valLoss: 0.4365902543067932 - trainLoss: 0.43037113547325134\n",
      "cnt: 0 - valLoss: 0.4365892708301544 - trainLoss: 0.43036848306655884\n",
      "cnt: 0 - valLoss: 0.43658825755119324 - trainLoss: 0.43036577105522156\n",
      "cnt: 0 - valLoss: 0.43658724427223206 - trainLoss: 0.43036308884620667\n",
      "cnt: 0 - valLoss: 0.43658626079559326 - trainLoss: 0.43036043643951416\n",
      "cnt: 0 - valLoss: 0.43658527731895447 - trainLoss: 0.43035778403282166\n",
      "cnt: 0 - valLoss: 0.4365842938423157 - trainLoss: 0.43035513162612915\n",
      "cnt: 0 - valLoss: 0.4365833103656769 - trainLoss: 0.43035247921943665\n",
      "cnt: 0 - valLoss: 0.4365823268890381 - trainLoss: 0.43034982681274414\n",
      "cnt: 0 - valLoss: 0.4365813136100769 - trainLoss: 0.4303470849990845\n",
      "cnt: 0 - valLoss: 0.4365803003311157 - trainLoss: 0.43034443259239197\n",
      "cnt: 0 - valLoss: 0.43657931685447693 - trainLoss: 0.43034178018569946\n",
      "cnt: 0 - valLoss: 0.4365783929824829 - trainLoss: 0.43033909797668457\n",
      "cnt: 0 - valLoss: 0.43657734990119934 - trainLoss: 0.43033644556999207\n",
      "cnt: 0 - valLoss: 0.4365764260292053 - trainLoss: 0.43033379316329956\n",
      "cnt: 0 - valLoss: 0.43657538294792175 - trainLoss: 0.43033114075660706\n",
      "cnt: 0 - valLoss: 0.43657439947128296 - trainLoss: 0.43032848834991455\n",
      "cnt: 0 - valLoss: 0.43657341599464417 - trainLoss: 0.43032583594322205\n",
      "cnt: 0 - valLoss: 0.43657246232032776 - trainLoss: 0.43032318353652954\n",
      "cnt: 0 - valLoss: 0.43657147884368896 - trainLoss: 0.43032053112983704\n",
      "cnt: 0 - valLoss: 0.43657049536705017 - trainLoss: 0.43031787872314453\n",
      "cnt: 0 - valLoss: 0.43656957149505615 - trainLoss: 0.430315226316452\n",
      "cnt: 0 - valLoss: 0.4365685284137726 - trainLoss: 0.4303125739097595\n",
      "cnt: 0 - valLoss: 0.436567485332489 - trainLoss: 0.430309921503067\n",
      "cnt: 0 - valLoss: 0.4365665018558502 - trainLoss: 0.4303072690963745\n",
      "cnt: 0 - valLoss: 0.4365655183792114 - trainLoss: 0.430304616689682\n",
      "cnt: 0 - valLoss: 0.43656450510025024 - trainLoss: 0.4303019046783447\n",
      "cnt: 0 - valLoss: 0.43656355142593384 - trainLoss: 0.430299311876297\n",
      "cnt: 0 - valLoss: 0.43656253814697266 - trainLoss: 0.4302966594696045\n",
      "cnt: 0 - valLoss: 0.43656155467033386 - trainLoss: 0.430294007062912\n",
      "cnt: 0 - valLoss: 0.4365605413913727 - trainLoss: 0.4302913546562195\n",
      "cnt: 0 - valLoss: 0.4365595579147339 - trainLoss: 0.430288702249527\n",
      "cnt: 0 - valLoss: 0.4365585744380951 - trainLoss: 0.4302860498428345\n",
      "cnt: 0 - valLoss: 0.4365575611591339 - trainLoss: 0.43028339743614197\n",
      "cnt: 0 - valLoss: 0.43655651807785034 - trainLoss: 0.4302806854248047\n",
      "cnt: 0 - valLoss: 0.4365555942058563 - trainLoss: 0.43027806282043457\n",
      "cnt: 0 - valLoss: 0.43655455112457275 - trainLoss: 0.43027541041374207\n",
      "cnt: 0 - valLoss: 0.4365535378456116 - trainLoss: 0.43027275800704956\n",
      "cnt: 0 - valLoss: 0.43655258417129517 - trainLoss: 0.43027010560035706\n",
      "cnt: 0 - valLoss: 0.4365515410900116 - trainLoss: 0.43026745319366455\n",
      "cnt: 0 - valLoss: 0.4365506172180176 - trainLoss: 0.43026477098464966\n",
      "cnt: 0 - valLoss: 0.436549574136734 - trainLoss: 0.43026214838027954\n",
      "cnt: 0 - valLoss: 0.4365485608577728 - trainLoss: 0.43025949597358704\n",
      "cnt: 0 - valLoss: 0.43654757738113403 - trainLoss: 0.43025684356689453\n",
      "cnt: 0 - valLoss: 0.43654659390449524 - trainLoss: 0.43025416135787964\n",
      "cnt: 0 - valLoss: 0.43654555082321167 - trainLoss: 0.4302515387535095\n",
      "cnt: 0 - valLoss: 0.43654459714889526 - trainLoss: 0.430248886346817\n",
      "cnt: 0 - valLoss: 0.4365435838699341 - trainLoss: 0.4302462041378021\n",
      "cnt: 0 - valLoss: 0.4365426003932953 - trainLoss: 0.43024352192878723\n",
      "cnt: 0 - valLoss: 0.4365415573120117 - trainLoss: 0.4302408695220947\n",
      "cnt: 0 - valLoss: 0.4365405738353729 - trainLoss: 0.4302382171154022\n",
      "cnt: 0 - valLoss: 0.43653959035873413 - trainLoss: 0.4302356243133545\n",
      "cnt: 0 - valLoss: 0.43653857707977295 - trainLoss: 0.430232971906662\n",
      "cnt: 0 - valLoss: 0.43653759360313416 - trainLoss: 0.4302302598953247\n",
      "cnt: 0 - valLoss: 0.4365365505218506 - trainLoss: 0.4302276074886322\n",
      "cnt: 0 - valLoss: 0.4365355670452118 - trainLoss: 0.4302249550819397\n",
      "cnt: 0 - valLoss: 0.436534583568573 - trainLoss: 0.43022236227989197\n",
      "cnt: 0 - valLoss: 0.4365336000919342 - trainLoss: 0.43021970987319946\n",
      "cnt: 0 - valLoss: 0.4365326166152954 - trainLoss: 0.43021702766418457\n",
      "cnt: 0 - valLoss: 0.43653157353401184 - trainLoss: 0.43021437525749207\n",
      "cnt: 0 - valLoss: 0.43653059005737305 - trainLoss: 0.4302116930484772\n",
      "cnt: 0 - valLoss: 0.43652960658073425 - trainLoss: 0.43020907044410706\n",
      "cnt: 0 - valLoss: 0.43652859330177307 - trainLoss: 0.43020641803741455\n",
      "cnt: 0 - valLoss: 0.4365275502204895 - trainLoss: 0.43020373582839966\n",
      "cnt: 0 - valLoss: 0.4365265369415283 - trainLoss: 0.43020108342170715\n",
      "cnt: 0 - valLoss: 0.4365255832672119 - trainLoss: 0.43019843101501465\n",
      "cnt: 0 - valLoss: 0.43652454018592834 - trainLoss: 0.43019580841064453\n",
      "cnt: 0 - valLoss: 0.43652352690696716 - trainLoss: 0.43019312620162964\n",
      "cnt: 0 - valLoss: 0.43652254343032837 - trainLoss: 0.4301905035972595\n",
      "cnt: 0 - valLoss: 0.4365215599536896 - trainLoss: 0.43018782138824463\n",
      "cnt: 0 - valLoss: 0.43652060627937317 - trainLoss: 0.4301851987838745\n",
      "cnt: 0 - valLoss: 0.4365195333957672 - trainLoss: 0.430182546377182\n",
      "cnt: 0 - valLoss: 0.4365185499191284 - trainLoss: 0.4301798939704895\n",
      "cnt: 0 - valLoss: 0.43651753664016724 - trainLoss: 0.430177241563797\n",
      "cnt: 0 - valLoss: 0.43651652336120605 - trainLoss: 0.4301745295524597\n",
      "cnt: 0 - valLoss: 0.43651553988456726 - trainLoss: 0.4301718771457672\n",
      "cnt: 0 - valLoss: 0.4365145266056061 - trainLoss: 0.4301692843437195\n",
      "cnt: 0 - valLoss: 0.4365135431289673 - trainLoss: 0.4301665723323822\n",
      "cnt: 0 - valLoss: 0.4365125596523285 - trainLoss: 0.4301639199256897\n",
      "cnt: 0 - valLoss: 0.4365115165710449 - trainLoss: 0.43016132712364197\n",
      "cnt: 0 - valLoss: 0.43651050329208374 - trainLoss: 0.43015867471694946\n",
      "cnt: 0 - valLoss: 0.43650951981544495 - trainLoss: 0.4301559627056122\n",
      "cnt: 0 - valLoss: 0.43650853633880615 - trainLoss: 0.43015334010124207\n",
      "cnt: 0 - valLoss: 0.43650752305984497 - trainLoss: 0.43015068769454956\n",
      "cnt: 0 - valLoss: 0.4365065097808838 - trainLoss: 0.43014800548553467\n",
      "cnt: 0 - valLoss: 0.436505526304245 - trainLoss: 0.43014535307884216\n",
      "cnt: 0 - valLoss: 0.4365044832229614 - trainLoss: 0.43014273047447205\n",
      "cnt: 0 - valLoss: 0.43650349974632263 - trainLoss: 0.43014004826545715\n",
      "cnt: 0 - valLoss: 0.43650248646736145 - trainLoss: 0.43013742566108704\n",
      "cnt: 0 - valLoss: 0.43650150299072266 - trainLoss: 0.43013474345207214\n",
      "cnt: 0 - valLoss: 0.43650051951408386 - trainLoss: 0.43013209104537964\n",
      "cnt: 0 - valLoss: 0.4364995062351227 - trainLoss: 0.43012943863868713\n",
      "cnt: 0 - valLoss: 0.4364984929561615 - trainLoss: 0.43012678623199463\n",
      "cnt: 0 - valLoss: 0.43649744987487793 - trainLoss: 0.4301241338253021\n",
      "cnt: 0 - valLoss: 0.43649646639823914 - trainLoss: 0.43012145161628723\n",
      "cnt: 0 - valLoss: 0.43649548292160034 - trainLoss: 0.4301187992095947\n",
      "cnt: 0 - valLoss: 0.43649446964263916 - trainLoss: 0.4301161468029022\n",
      "cnt: 0 - valLoss: 0.4364934265613556 - trainLoss: 0.4301134943962097\n",
      "cnt: 0 - valLoss: 0.4364924430847168 - trainLoss: 0.4301108419895172\n",
      "cnt: 0 - valLoss: 0.436491459608078 - trainLoss: 0.4301081895828247\n",
      "cnt: 0 - valLoss: 0.4364904463291168 - trainLoss: 0.4301055371761322\n",
      "cnt: 0 - valLoss: 0.436489462852478 - trainLoss: 0.4301028847694397\n",
      "cnt: 0 - valLoss: 0.43648841977119446 - trainLoss: 0.4301002323627472\n",
      "cnt: 0 - valLoss: 0.43648743629455566 - trainLoss: 0.4300975799560547\n",
      "cnt: 0 - valLoss: 0.4364863932132721 - trainLoss: 0.4300948977470398\n",
      "cnt: 0 - valLoss: 0.4364853799343109 - trainLoss: 0.4300922453403473\n",
      "cnt: 0 - valLoss: 0.4364843964576721 - trainLoss: 0.4300896227359772\n",
      "cnt: 0 - valLoss: 0.4364834129810333 - trainLoss: 0.43008697032928467\n",
      "cnt: 0 - valLoss: 0.43648236989974976 - trainLoss: 0.43008434772491455\n",
      "cnt: 0 - valLoss: 0.4364813566207886 - trainLoss: 0.43008166551589966\n",
      "cnt: 0 - valLoss: 0.436480313539505 - trainLoss: 0.43007901310920715\n",
      "cnt: 0 - valLoss: 0.436479389667511 - trainLoss: 0.43007636070251465\n",
      "cnt: 0 - valLoss: 0.4364783465862274 - trainLoss: 0.43007370829582214\n",
      "cnt: 0 - valLoss: 0.4364773631095886 - trainLoss: 0.43007105588912964\n",
      "cnt: 0 - valLoss: 0.43647632002830505 - trainLoss: 0.43006840348243713\n",
      "cnt: 0 - valLoss: 0.43647533655166626 - trainLoss: 0.43006575107574463\n",
      "cnt: 0 - valLoss: 0.4364743232727051 - trainLoss: 0.4300630986690521\n",
      "cnt: 0 - valLoss: 0.4364732801914215 - trainLoss: 0.43006041646003723\n",
      "cnt: 0 - valLoss: 0.4364722967147827 - trainLoss: 0.43005773425102234\n",
      "cnt: 0 - valLoss: 0.43647128343582153 - trainLoss: 0.43005508184432983\n",
      "cnt: 0 - valLoss: 0.43647029995918274 - trainLoss: 0.43005242943763733\n",
      "cnt: 0 - valLoss: 0.43646925687789917 - trainLoss: 0.4300498068332672\n",
      "cnt: 0 - valLoss: 0.4364682734012604 - trainLoss: 0.4300471544265747\n",
      "cnt: 0 - valLoss: 0.4364672601222992 - trainLoss: 0.4300445020198822\n",
      "cnt: 0 - valLoss: 0.436466246843338 - trainLoss: 0.4300418198108673\n",
      "cnt: 0 - valLoss: 0.43646523356437683 - trainLoss: 0.4300391674041748\n",
      "cnt: 0 - valLoss: 0.43646425008773804 - trainLoss: 0.4300365447998047\n",
      "cnt: 0 - valLoss: 0.43646326661109924 - trainLoss: 0.4300338625907898\n",
      "cnt: 0 - valLoss: 0.43646231293678284 - trainLoss: 0.4300312399864197\n",
      "cnt: 0 - valLoss: 0.4364612400531769 - trainLoss: 0.4300285875797272\n",
      "cnt: 0 - valLoss: 0.4364602565765381 - trainLoss: 0.43002593517303467\n",
      "cnt: 0 - valLoss: 0.4364592432975769 - trainLoss: 0.43002328276634216\n",
      "cnt: 0 - valLoss: 0.4364582300186157 - trainLoss: 0.4300205707550049\n",
      "cnt: 0 - valLoss: 0.43645721673965454 - trainLoss: 0.43001797795295715\n",
      "cnt: 0 - valLoss: 0.43645623326301575 - trainLoss: 0.43001532554626465\n",
      "cnt: 0 - valLoss: 0.43645524978637695 - trainLoss: 0.43001267313957214\n",
      "cnt: 0 - valLoss: 0.43645426630973816 - trainLoss: 0.43000996112823486\n",
      "cnt: 0 - valLoss: 0.4364532232284546 - trainLoss: 0.43000730872154236\n",
      "cnt: 0 - valLoss: 0.4364522099494934 - trainLoss: 0.43000471591949463\n",
      "cnt: 0 - valLoss: 0.436451256275177 - trainLoss: 0.43000200390815735\n",
      "cnt: 0 - valLoss: 0.4364502429962158 - trainLoss: 0.42999938130378723\n",
      "cnt: 0 - valLoss: 0.43644919991493225 - trainLoss: 0.42999669909477234\n",
      "cnt: 0 - valLoss: 0.43644821643829346 - trainLoss: 0.42999404668807983\n",
      "cnt: 0 - valLoss: 0.43644723296165466 - trainLoss: 0.42999139428138733\n",
      "cnt: 0 - valLoss: 0.4364461898803711 - trainLoss: 0.4299887418746948\n",
      "cnt: 0 - valLoss: 0.4364451766014099 - trainLoss: 0.4299860894680023\n",
      "cnt: 0 - valLoss: 0.4364442229270935 - trainLoss: 0.4299834370613098\n",
      "cnt: 0 - valLoss: 0.4364432096481323 - trainLoss: 0.4299807846546173\n",
      "cnt: 0 - valLoss: 0.43644216656684875 - trainLoss: 0.4299781322479248\n",
      "cnt: 0 - valLoss: 0.43644124269485474 - trainLoss: 0.4299754798412323\n",
      "cnt: 0 - valLoss: 0.43644019961357117 - trainLoss: 0.4299728274345398\n",
      "cnt: 0 - valLoss: 0.4364391565322876 - trainLoss: 0.4299701750278473\n",
      "cnt: 0 - valLoss: 0.4364381730556488 - trainLoss: 0.4299675226211548\n",
      "cnt: 0 - valLoss: 0.43643718957901 - trainLoss: 0.4299648404121399\n",
      "cnt: 0 - valLoss: 0.43643617630004883 - trainLoss: 0.4299621880054474\n",
      "cnt: 0 - valLoss: 0.43643519282341003 - trainLoss: 0.4299595355987549\n",
      "cnt: 0 - valLoss: 0.43643414974212646 - trainLoss: 0.4299568831920624\n",
      "cnt: 0 - valLoss: 0.43643316626548767 - trainLoss: 0.4299542307853699\n",
      "cnt: 0 - valLoss: 0.4364321827888489 - trainLoss: 0.42995157837867737\n",
      "cnt: 0 - valLoss: 0.4364311993122101 - trainLoss: 0.42994892597198486\n",
      "cnt: 0 - valLoss: 0.4364301562309265 - trainLoss: 0.42994627356529236\n",
      "cnt: 0 - valLoss: 0.43642914295196533 - trainLoss: 0.42994362115859985\n",
      "cnt: 0 - valLoss: 0.43642809987068176 - trainLoss: 0.42994096875190735\n",
      "cnt: 0 - valLoss: 0.43642717599868774 - trainLoss: 0.42993831634521484\n",
      "cnt: 0 - valLoss: 0.4364261329174042 - trainLoss: 0.42993566393852234\n",
      "cnt: 0 - valLoss: 0.436425119638443 - trainLoss: 0.4299330413341522\n",
      "cnt: 0 - valLoss: 0.4364241361618042 - trainLoss: 0.42993035912513733\n",
      "cnt: 0 - valLoss: 0.4364231526851654 - trainLoss: 0.4299277067184448\n",
      "cnt: 0 - valLoss: 0.43642210960388184 - trainLoss: 0.4299250543117523\n",
      "cnt: 0 - valLoss: 0.43642112612724304 - trainLoss: 0.4299224019050598\n",
      "cnt: 0 - valLoss: 0.43642017245292664 - trainLoss: 0.4299197494983673\n",
      "cnt: 0 - valLoss: 0.4364192485809326 - trainLoss: 0.4299170970916748\n",
      "cnt: 0 - valLoss: 0.436418354511261 - trainLoss: 0.4299144446849823\n",
      "cnt: 0 - valLoss: 0.4364173412322998 - trainLoss: 0.4299118220806122\n",
      "cnt: 0 - valLoss: 0.4364164471626282 - trainLoss: 0.4299091398715973\n",
      "cnt: 0 - valLoss: 0.43641552329063416 - trainLoss: 0.42990654706954956\n",
      "cnt: 0 - valLoss: 0.43641459941864014 - trainLoss: 0.42990389466285706\n",
      "cnt: 0 - valLoss: 0.4364136755466461 - trainLoss: 0.42990124225616455\n",
      "cnt: 0 - valLoss: 0.4364126920700073 - trainLoss: 0.42989858984947205\n",
      "cnt: 0 - valLoss: 0.4364117681980133 - trainLoss: 0.42989593744277954\n",
      "cnt: 0 - valLoss: 0.4364108443260193 - trainLoss: 0.42989328503608704\n",
      "cnt: 0 - valLoss: 0.43640995025634766 - trainLoss: 0.42989063262939453\n",
      "cnt: 0 - valLoss: 0.43640899658203125 - trainLoss: 0.4298880398273468\n",
      "cnt: 0 - valLoss: 0.43640804290771484 - trainLoss: 0.4298853278160095\n",
      "cnt: 0 - valLoss: 0.4364071190357208 - trainLoss: 0.429882675409317\n",
      "cnt: 0 - valLoss: 0.4364061653614044 - trainLoss: 0.4298800528049469\n",
      "cnt: 0 - valLoss: 0.4364052712917328 - trainLoss: 0.4298774003982544\n",
      "cnt: 0 - valLoss: 0.436404287815094 - trainLoss: 0.42987480759620667\n",
      "cnt: 0 - valLoss: 0.4364033341407776 - trainLoss: 0.42987215518951416\n",
      "cnt: 0 - valLoss: 0.43640244007110596 - trainLoss: 0.42986950278282166\n",
      "cnt: 0 - valLoss: 0.43640148639678955 - trainLoss: 0.42986685037612915\n",
      "cnt: 0 - valLoss: 0.43640056252479553 - trainLoss: 0.42986419796943665\n",
      "cnt: 0 - valLoss: 0.4363996088504791 - trainLoss: 0.42986154556274414\n",
      "cnt: 0 - valLoss: 0.4363986551761627 - trainLoss: 0.42985886335372925\n",
      "cnt: 0 - valLoss: 0.4363977611064911 - trainLoss: 0.42985621094703674\n",
      "cnt: 0 - valLoss: 0.43639683723449707 - trainLoss: 0.42985355854034424\n",
      "cnt: 0 - valLoss: 0.43639588356018066 - trainLoss: 0.4298509657382965\n",
      "cnt: 0 - valLoss: 0.43639492988586426 - trainLoss: 0.429848313331604\n",
      "cnt: 0 - valLoss: 0.43639400601387024 - trainLoss: 0.4298456907272339\n",
      "cnt: 0 - valLoss: 0.43639305233955383 - trainLoss: 0.429843008518219\n",
      "cnt: 0 - valLoss: 0.4363920986652374 - trainLoss: 0.4298403859138489\n",
      "cnt: 0 - valLoss: 0.4363912045955658 - trainLoss: 0.42983773350715637\n",
      "cnt: 0 - valLoss: 0.4363902807235718 - trainLoss: 0.42983508110046387\n",
      "cnt: 0 - valLoss: 0.43638932704925537 - trainLoss: 0.42983242869377136\n",
      "cnt: 0 - valLoss: 0.43638837337493896 - trainLoss: 0.42982977628707886\n",
      "cnt: 0 - valLoss: 0.43638744950294495 - trainLoss: 0.42982712388038635\n",
      "cnt: 0 - valLoss: 0.43638649582862854 - trainLoss: 0.42982447147369385\n",
      "cnt: 0 - valLoss: 0.4363856017589569 - trainLoss: 0.42982181906700134\n",
      "cnt: 0 - valLoss: 0.4363846480846405 - trainLoss: 0.42981916666030884\n",
      "cnt: 0 - valLoss: 0.4363836944103241 - trainLoss: 0.4298166036605835\n",
      "cnt: 0 - valLoss: 0.4363827109336853 - trainLoss: 0.4298138916492462\n",
      "cnt: 0 - valLoss: 0.43638181686401367 - trainLoss: 0.4298112690448761\n",
      "cnt: 0 - valLoss: 0.43638089299201965 - trainLoss: 0.4298085868358612\n",
      "cnt: 0 - valLoss: 0.43637987971305847 - trainLoss: 0.4298059642314911\n",
      "cnt: 0 - valLoss: 0.43637898564338684 - trainLoss: 0.4298033118247986\n",
      "cnt: 0 - valLoss: 0.4363780617713928 - trainLoss: 0.4298006594181061\n",
      "cnt: 0 - valLoss: 0.4363771080970764 - trainLoss: 0.4297980070114136\n",
      "cnt: 0 - valLoss: 0.43637615442276 - trainLoss: 0.42979535460472107\n",
      "cnt: 0 - valLoss: 0.4363751709461212 - trainLoss: 0.42979270219802856\n",
      "cnt: 0 - valLoss: 0.4363742768764496 - trainLoss: 0.42979004979133606\n",
      "cnt: 0 - valLoss: 0.4363733232021332 - trainLoss: 0.42978742718696594\n",
      "cnt: 0 - valLoss: 0.43637239933013916 - trainLoss: 0.42978474497795105\n",
      "cnt: 0 - valLoss: 0.43637144565582275 - trainLoss: 0.42978212237358093\n",
      "cnt: 0 - valLoss: 0.43637049198150635 - trainLoss: 0.4297794699668884\n",
      "cnt: 0 - valLoss: 0.43636950850486755 - trainLoss: 0.4297768175601959\n",
      "cnt: 0 - valLoss: 0.43636858463287354 - trainLoss: 0.4297742247581482\n",
      "cnt: 0 - valLoss: 0.43636763095855713 - trainLoss: 0.4297715723514557\n",
      "cnt: 0 - valLoss: 0.4363667368888855 - trainLoss: 0.4297689199447632\n",
      "cnt: 0 - valLoss: 0.4363657534122467 - trainLoss: 0.4297662675380707\n",
      "cnt: 0 - valLoss: 0.4363647997379303 - trainLoss: 0.4297636151313782\n",
      "cnt: 0 - valLoss: 0.4363638758659363 - trainLoss: 0.4297609329223633\n",
      "cnt: 0 - valLoss: 0.43636295199394226 - trainLoss: 0.4297582805156708\n",
      "cnt: 0 - valLoss: 0.4363619387149811 - trainLoss: 0.42975568771362305\n",
      "cnt: 0 - valLoss: 0.43636104464530945 - trainLoss: 0.42975303530693054\n",
      "cnt: 0 - valLoss: 0.43636009097099304 - trainLoss: 0.4297504127025604\n",
      "cnt: 0 - valLoss: 0.43635913729667664 - trainLoss: 0.42974773049354553\n",
      "cnt: 0 - valLoss: 0.4363582134246826 - trainLoss: 0.4297451078891754\n",
      "cnt: 0 - valLoss: 0.4363572597503662 - trainLoss: 0.4297424256801605\n",
      "cnt: 0 - valLoss: 0.4363563656806946 - trainLoss: 0.4297398030757904\n",
      "cnt: 0 - valLoss: 0.4363553822040558 - trainLoss: 0.4297371506690979\n",
      "cnt: 0 - valLoss: 0.436354398727417 - trainLoss: 0.4297344982624054\n",
      "cnt: 0 - valLoss: 0.4363534450531006 - trainLoss: 0.4297318458557129\n",
      "cnt: 0 - valLoss: 0.43635255098342896 - trainLoss: 0.4297291934490204\n",
      "cnt: 0 - valLoss: 0.43635156750679016 - trainLoss: 0.4297265410423279\n",
      "cnt: 0 - valLoss: 0.43635067343711853 - trainLoss: 0.4297238886356354\n",
      "cnt: 0 - valLoss: 0.4363497197628021 - trainLoss: 0.42972126603126526\n",
      "cnt: 0 - valLoss: 0.43634873628616333 - trainLoss: 0.42971861362457275\n",
      "cnt: 0 - valLoss: 0.4363477826118469 - trainLoss: 0.42971596121788025\n",
      "cnt: 0 - valLoss: 0.4363468587398529 - trainLoss: 0.42971330881118774\n",
      "cnt: 0 - valLoss: 0.4363459050655365 - trainLoss: 0.4297106862068176\n",
      "cnt: 0 - valLoss: 0.4363449513912201 - trainLoss: 0.42970800399780273\n",
      "cnt: 0 - valLoss: 0.4363440275192261 - trainLoss: 0.4297053813934326\n",
      "cnt: 0 - valLoss: 0.43634307384490967 - trainLoss: 0.4297027289867401\n",
      "cnt: 0 - valLoss: 0.4363420903682709 - trainLoss: 0.4297000765800476\n",
      "cnt: 0 - valLoss: 0.4363411068916321 - trainLoss: 0.4296974241733551\n",
      "cnt: 0 - valLoss: 0.43634021282196045 - trainLoss: 0.4296947717666626\n",
      "cnt: 0 - valLoss: 0.43633925914764404 - trainLoss: 0.4296921193599701\n",
      "cnt: 0 - valLoss: 0.43633827567100525 - trainLoss: 0.42968952655792236\n",
      "cnt: 0 - valLoss: 0.43633732199668884 - trainLoss: 0.4296868145465851\n",
      "cnt: 0 - valLoss: 0.4363363981246948 - trainLoss: 0.42968419194221497\n",
      "cnt: 0 - valLoss: 0.4363354444503784 - trainLoss: 0.42968153953552246\n",
      "cnt: 0 - valLoss: 0.4363344609737396 - trainLoss: 0.42967894673347473\n",
      "cnt: 0 - valLoss: 0.436333566904068 - trainLoss: 0.42967623472213745\n",
      "cnt: 0 - valLoss: 0.4363326132297516 - trainLoss: 0.4296736419200897\n",
      "cnt: 0 - valLoss: 0.4363316297531128 - trainLoss: 0.4296709895133972\n",
      "cnt: 0 - valLoss: 0.4363306760787964 - trainLoss: 0.4296683371067047\n",
      "cnt: 0 - valLoss: 0.4363296926021576 - trainLoss: 0.4296656847000122\n",
      "cnt: 0 - valLoss: 0.4363287687301636 - trainLoss: 0.4296630322933197\n",
      "cnt: 0 - valLoss: 0.43632781505584717 - trainLoss: 0.4296603500843048\n",
      "cnt: 0 - valLoss: 0.43632686138153076 - trainLoss: 0.4296576976776123\n",
      "cnt: 0 - valLoss: 0.43632593750953674 - trainLoss: 0.4296550452709198\n",
      "cnt: 0 - valLoss: 0.4363248944282532 - trainLoss: 0.4296523928642273\n",
      "cnt: 0 - valLoss: 0.43632400035858154 - trainLoss: 0.42964980006217957\n",
      "cnt: 0 - valLoss: 0.43632301688194275 - trainLoss: 0.42964714765548706\n",
      "cnt: 0 - valLoss: 0.43632206320762634 - trainLoss: 0.42964449524879456\n",
      "cnt: 0 - valLoss: 0.4363211393356323 - trainLoss: 0.42964187264442444\n",
      "cnt: 0 - valLoss: 0.4363201856613159 - trainLoss: 0.42963919043540955\n",
      "cnt: 0 - valLoss: 0.4363192021846771 - trainLoss: 0.42963653802871704\n",
      "cnt: 0 - valLoss: 0.4363182485103607 - trainLoss: 0.4296339154243469\n",
      "cnt: 0 - valLoss: 0.4363173246383667 - trainLoss: 0.4296312630176544\n",
      "cnt: 0 - valLoss: 0.4363163411617279 - trainLoss: 0.4296286106109619\n",
      "cnt: 0 - valLoss: 0.4363153874874115 - trainLoss: 0.4296259582042694\n",
      "cnt: 0 - valLoss: 0.4363144338130951 - trainLoss: 0.4296233057975769\n",
      "cnt: 0 - valLoss: 0.4363135099411011 - trainLoss: 0.4296206831932068\n",
      "cnt: 0 - valLoss: 0.4363125264644623 - trainLoss: 0.4296180307865143\n",
      "cnt: 0 - valLoss: 0.4363115727901459 - trainLoss: 0.4296153485774994\n",
      "cnt: 0 - valLoss: 0.43631061911582947 - trainLoss: 0.4296127259731293\n",
      "cnt: 0 - valLoss: 0.4363096356391907 - trainLoss: 0.42961007356643677\n",
      "cnt: 0 - valLoss: 0.43630871176719666 - trainLoss: 0.42960745096206665\n",
      "cnt: 0 - valLoss: 0.43630775809288025 - trainLoss: 0.42960479855537415\n",
      "cnt: 0 - valLoss: 0.43630680441856384 - trainLoss: 0.42960214614868164\n",
      "cnt: 0 - valLoss: 0.43630582094192505 - trainLoss: 0.42959949374198914\n",
      "cnt: 0 - valLoss: 0.43630489706993103 - trainLoss: 0.42959684133529663\n",
      "cnt: 0 - valLoss: 0.43630391359329224 - trainLoss: 0.4295941889286041\n",
      "cnt: 0 - valLoss: 0.43630295991897583 - trainLoss: 0.4295915365219116\n",
      "cnt: 0 - valLoss: 0.4363020062446594 - trainLoss: 0.4295888841152191\n",
      "cnt: 0 - valLoss: 0.43630102276802063 - trainLoss: 0.4295862317085266\n",
      "cnt: 0 - valLoss: 0.43630003929138184 - trainLoss: 0.4295836091041565\n",
      "cnt: 0 - valLoss: 0.43629908561706543 - trainLoss: 0.4295809268951416\n",
      "cnt: 0 - valLoss: 0.4362981617450714 - trainLoss: 0.4295782744884491\n",
      "cnt: 0 - valLoss: 0.4362971782684326 - trainLoss: 0.429575651884079\n",
      "cnt: 0 - valLoss: 0.4362962245941162 - trainLoss: 0.4295729994773865\n",
      "cnt: 0 - valLoss: 0.4362952411174774 - trainLoss: 0.42957040667533875\n",
      "cnt: 0 - valLoss: 0.436294287443161 - trainLoss: 0.42956775426864624\n",
      "cnt: 0 - valLoss: 0.436293363571167 - trainLoss: 0.42956510186195374\n",
      "cnt: 0 - valLoss: 0.4362924098968506 - trainLoss: 0.42956241965293884\n",
      "cnt: 0 - valLoss: 0.4362914264202118 - trainLoss: 0.4295598268508911\n",
      "cnt: 0 - valLoss: 0.436290442943573 - trainLoss: 0.42955711483955383\n",
      "cnt: 0 - valLoss: 0.4362894892692566 - trainLoss: 0.42955446243286133\n",
      "cnt: 0 - valLoss: 0.4362885057926178 - trainLoss: 0.4295518100261688\n",
      "cnt: 0 - valLoss: 0.436287522315979 - trainLoss: 0.4295491576194763\n",
      "cnt: 0 - valLoss: 0.4362865686416626 - trainLoss: 0.4295465648174286\n",
      "cnt: 0 - valLoss: 0.4362856447696686 - trainLoss: 0.4295439124107361\n",
      "cnt: 0 - valLoss: 0.4362846612930298 - trainLoss: 0.4295412600040436\n",
      "cnt: 0 - valLoss: 0.4362837076187134 - trainLoss: 0.42953863739967346\n",
      "cnt: 0 - valLoss: 0.4362827241420746 - trainLoss: 0.4295359253883362\n",
      "cnt: 0 - valLoss: 0.4362817704677582 - trainLoss: 0.42953333258628845\n",
      "cnt: 0 - valLoss: 0.4362807869911194 - trainLoss: 0.42953068017959595\n",
      "cnt: 0 - valLoss: 0.43627986311912537 - trainLoss: 0.42952802777290344\n",
      "cnt: 0 - valLoss: 0.4362788796424866 - trainLoss: 0.42952537536621094\n",
      "cnt: 0 - valLoss: 0.43627792596817017 - trainLoss: 0.4295227527618408\n",
      "cnt: 0 - valLoss: 0.43627697229385376 - trainLoss: 0.4295200705528259\n",
      "cnt: 0 - valLoss: 0.43627598881721497 - trainLoss: 0.4295174181461334\n",
      "cnt: 0 - valLoss: 0.43627500534057617 - trainLoss: 0.4295147657394409\n",
      "cnt: 0 - valLoss: 0.43627408146858215 - trainLoss: 0.4295121431350708\n",
      "cnt: 0 - valLoss: 0.43627306818962097 - trainLoss: 0.4295094907283783\n",
      "cnt: 0 - valLoss: 0.43627217411994934 - trainLoss: 0.4295068383216858\n",
      "cnt: 0 - valLoss: 0.43627116084098816 - trainLoss: 0.4295042157173157\n",
      "cnt: 0 - valLoss: 0.43627020716667175 - trainLoss: 0.4295015335083008\n",
      "cnt: 0 - valLoss: 0.43626922369003296 - trainLoss: 0.42949891090393066\n",
      "cnt: 0 - valLoss: 0.43626827001571655 - trainLoss: 0.42949625849723816\n",
      "cnt: 0 - valLoss: 0.43626728653907776 - trainLoss: 0.42949360609054565\n",
      "cnt: 0 - valLoss: 0.43626636266708374 - trainLoss: 0.42949095368385315\n",
      "cnt: 0 - valLoss: 0.43626540899276733 - trainLoss: 0.42948830127716064\n",
      "cnt: 0 - valLoss: 0.43626439571380615 - trainLoss: 0.42948564887046814\n",
      "cnt: 0 - valLoss: 0.43626344203948975 - trainLoss: 0.42948299646377563\n",
      "cnt: 0 - valLoss: 0.43626248836517334 - trainLoss: 0.4294803738594055\n",
      "cnt: 0 - valLoss: 0.43626150488853455 - trainLoss: 0.429477721452713\n",
      "cnt: 0 - valLoss: 0.43626052141189575 - trainLoss: 0.4294750690460205\n",
      "cnt: 0 - valLoss: 0.43625956773757935 - trainLoss: 0.429472416639328\n",
      "cnt: 0 - valLoss: 0.4362586438655853 - trainLoss: 0.4294697642326355\n",
      "cnt: 0 - valLoss: 0.43625766038894653 - trainLoss: 0.42946717143058777\n",
      "cnt: 0 - valLoss: 0.43625667691230774 - trainLoss: 0.4294644892215729\n",
      "cnt: 0 - valLoss: 0.43625572323799133 - trainLoss: 0.429461807012558\n",
      "cnt: 0 - valLoss: 0.43625473976135254 - trainLoss: 0.42945918440818787\n",
      "cnt: 0 - valLoss: 0.43625375628471375 - trainLoss: 0.42945653200149536\n",
      "cnt: 0 - valLoss: 0.43625280261039734 - trainLoss: 0.42945387959480286\n",
      "cnt: 0 - valLoss: 0.43625178933143616 - trainLoss: 0.42945122718811035\n",
      "cnt: 0 - valLoss: 0.43625083565711975 - trainLoss: 0.42944857478141785\n",
      "cnt: 0 - valLoss: 0.43624988198280334 - trainLoss: 0.42944592237472534\n",
      "cnt: 0 - valLoss: 0.43624889850616455 - trainLoss: 0.42944326996803284\n",
      "cnt: 0 - valLoss: 0.43624791502952576 - trainLoss: 0.4294406771659851\n",
      "cnt: 0 - valLoss: 0.43624699115753174 - trainLoss: 0.4294379651546478\n",
      "cnt: 0 - valLoss: 0.43624600768089294 - trainLoss: 0.4294353127479553\n",
      "cnt: 0 - valLoss: 0.43624499440193176 - trainLoss: 0.4294326603412628\n",
      "cnt: 0 - valLoss: 0.43624407052993774 - trainLoss: 0.4294300377368927\n",
      "cnt: 0 - valLoss: 0.43624308705329895 - trainLoss: 0.4294273853302002\n",
      "cnt: 0 - valLoss: 0.43624210357666016 - trainLoss: 0.4294247329235077\n",
      "cnt: 0 - valLoss: 0.43624114990234375 - trainLoss: 0.4294220805168152\n",
      "cnt: 0 - valLoss: 0.43624016642570496 - trainLoss: 0.42941948771476746\n",
      "cnt: 0 - valLoss: 0.43623918294906616 - trainLoss: 0.42941683530807495\n",
      "cnt: 0 - valLoss: 0.43623822927474976 - trainLoss: 0.42941418290138245\n",
      "cnt: 0 - valLoss: 0.43623724579811096 - trainLoss: 0.42941150069236755\n",
      "cnt: 0 - valLoss: 0.43623626232147217 - trainLoss: 0.42940884828567505\n",
      "cnt: 0 - valLoss: 0.43623530864715576 - trainLoss: 0.42940619587898254\n",
      "cnt: 0 - valLoss: 0.43623432517051697 - trainLoss: 0.42940354347229004\n",
      "cnt: 0 - valLoss: 0.4362333416938782 - trainLoss: 0.42940089106559753\n",
      "cnt: 0 - valLoss: 0.43623241782188416 - trainLoss: 0.4293982982635498\n",
      "cnt: 0 - valLoss: 0.4362313747406006 - trainLoss: 0.4293955862522125\n",
      "cnt: 0 - valLoss: 0.4362304210662842 - trainLoss: 0.4293929934501648\n",
      "cnt: 0 - valLoss: 0.43622949719429016 - trainLoss: 0.4293903410434723\n",
      "cnt: 0 - valLoss: 0.43622851371765137 - trainLoss: 0.4293876886367798\n",
      "cnt: 0 - valLoss: 0.4362275302410126 - trainLoss: 0.4293850064277649\n",
      "cnt: 0 - valLoss: 0.43622657656669617 - trainLoss: 0.4293823540210724\n",
      "cnt: 0 - valLoss: 0.4362255930900574 - trainLoss: 0.42937976121902466\n",
      "cnt: 0 - valLoss: 0.4362246096134186 - trainLoss: 0.42937710881233215\n",
      "cnt: 0 - valLoss: 0.4362236261367798 - trainLoss: 0.4293743968009949\n",
      "cnt: 0 - valLoss: 0.436222642660141 - trainLoss: 0.42937180399894714\n",
      "cnt: 0 - valLoss: 0.4362216889858246 - trainLoss: 0.42936915159225464\n",
      "cnt: 0 - valLoss: 0.4362207055091858 - trainLoss: 0.42936649918556213\n",
      "cnt: 0 - valLoss: 0.4362197518348694 - trainLoss: 0.42936384677886963\n",
      "cnt: 0 - valLoss: 0.4362187683582306 - trainLoss: 0.4293611943721771\n",
      "cnt: 0 - valLoss: 0.4362177550792694 - trainLoss: 0.42935851216316223\n",
      "cnt: 0 - valLoss: 0.436216801404953 - trainLoss: 0.4293559193611145\n",
      "cnt: 0 - valLoss: 0.4362158179283142 - trainLoss: 0.4293532073497772\n",
      "cnt: 0 - valLoss: 0.4362148642539978 - trainLoss: 0.4293505549430847\n",
      "cnt: 0 - valLoss: 0.436213880777359 - trainLoss: 0.429347962141037\n",
      "cnt: 0 - valLoss: 0.4362128973007202 - trainLoss: 0.4293452501296997\n",
      "cnt: 0 - valLoss: 0.4362119138240814 - trainLoss: 0.429342657327652\n",
      "cnt: 0 - valLoss: 0.4362109303474426 - trainLoss: 0.4293400049209595\n",
      "cnt: 0 - valLoss: 0.43620994687080383 - trainLoss: 0.42933735251426697\n",
      "cnt: 0 - valLoss: 0.4362089931964874 - trainLoss: 0.42933472990989685\n",
      "cnt: 0 - valLoss: 0.43620800971984863 - trainLoss: 0.42933207750320435\n",
      "cnt: 0 - valLoss: 0.43620702624320984 - trainLoss: 0.42932942509651184\n",
      "cnt: 0 - valLoss: 0.43620604276657104 - trainLoss: 0.42932677268981934\n",
      "cnt: 0 - valLoss: 0.43620505928993225 - trainLoss: 0.42932412028312683\n",
      "cnt: 0 - valLoss: 0.43620410561561584 - trainLoss: 0.4293214678764343\n",
      "cnt: 0 - valLoss: 0.4362031817436218 - trainLoss: 0.4293188154697418\n",
      "cnt: 0 - valLoss: 0.43620219826698303 - trainLoss: 0.4293161928653717\n",
      "cnt: 0 - valLoss: 0.43620121479034424 - trainLoss: 0.4293135106563568\n",
      "cnt: 0 - valLoss: 0.43620017170906067 - trainLoss: 0.4293108582496643\n",
      "cnt: 0 - valLoss: 0.43619921803474426 - trainLoss: 0.4293082058429718\n",
      "cnt: 0 - valLoss: 0.43619823455810547 - trainLoss: 0.4293055832386017\n",
      "cnt: 0 - valLoss: 0.4361972510814667 - trainLoss: 0.4293028712272644\n",
      "cnt: 0 - valLoss: 0.43619632720947266 - trainLoss: 0.4293002784252167\n",
      "cnt: 0 - valLoss: 0.43619534373283386 - trainLoss: 0.42929762601852417\n",
      "cnt: 0 - valLoss: 0.4361943304538727 - trainLoss: 0.42929497361183167\n",
      "cnt: 0 - valLoss: 0.4361933469772339 - trainLoss: 0.42929232120513916\n",
      "cnt: 0 - valLoss: 0.4361923635005951 - trainLoss: 0.42928969860076904\n",
      "cnt: 0 - valLoss: 0.4361914396286011 - trainLoss: 0.42928704619407654\n",
      "cnt: 0 - valLoss: 0.4361903965473175 - trainLoss: 0.42928439378738403\n",
      "cnt: 0 - valLoss: 0.4361894726753235 - trainLoss: 0.42928174138069153\n",
      "cnt: 0 - valLoss: 0.4361884593963623 - trainLoss: 0.429279088973999\n",
      "cnt: 0 - valLoss: 0.4361874759197235 - trainLoss: 0.4292764365673065\n",
      "cnt: 0 - valLoss: 0.4361865520477295 - trainLoss: 0.429273784160614\n",
      "cnt: 0 - valLoss: 0.4361855685710907 - trainLoss: 0.4292711317539215\n",
      "cnt: 0 - valLoss: 0.4361845850944519 - trainLoss: 0.429268479347229\n",
      "cnt: 0 - valLoss: 0.4361835718154907 - trainLoss: 0.4292658269405365\n",
      "cnt: 0 - valLoss: 0.43618258833885193 - trainLoss: 0.429263174533844\n",
      "cnt: 0 - valLoss: 0.4361816644668579 - trainLoss: 0.4292605221271515\n",
      "cnt: 0 - valLoss: 0.43618062138557434 - trainLoss: 0.429257869720459\n",
      "cnt: 0 - valLoss: 0.4361796975135803 - trainLoss: 0.42925524711608887\n",
      "cnt: 0 - valLoss: 0.43617871403694153 - trainLoss: 0.42925259470939636\n",
      "cnt: 0 - valLoss: 0.43617770075798035 - trainLoss: 0.42924997210502625\n",
      "cnt: 0 - valLoss: 0.43617677688598633 - trainLoss: 0.42924731969833374\n",
      "cnt: 0 - valLoss: 0.43617573380470276 - trainLoss: 0.42924463748931885\n",
      "cnt: 0 - valLoss: 0.43617480993270874 - trainLoss: 0.42924201488494873\n",
      "cnt: 0 - valLoss: 0.43617382645606995 - trainLoss: 0.4292393624782562\n",
      "cnt: 0 - valLoss: 0.43617284297943115 - trainLoss: 0.4292367100715637\n",
      "cnt: 0 - valLoss: 0.43617182970046997 - trainLoss: 0.4292340576648712\n",
      "cnt: 0 - valLoss: 0.4361708462238312 - trainLoss: 0.4292314052581787\n",
      "cnt: 0 - valLoss: 0.43616992235183716 - trainLoss: 0.4292287826538086\n",
      "cnt: 0 - valLoss: 0.43616893887519836 - trainLoss: 0.4292261004447937\n",
      "cnt: 0 - valLoss: 0.43616795539855957 - trainLoss: 0.4292234480381012\n",
      "cnt: 0 - valLoss: 0.4361669421195984 - trainLoss: 0.4292207956314087\n",
      "cnt: 0 - valLoss: 0.4361659586429596 - trainLoss: 0.4292181432247162\n",
      "cnt: 0 - valLoss: 0.4361649751663208 - trainLoss: 0.42921552062034607\n",
      "cnt: 0 - valLoss: 0.4361640512943268 - trainLoss: 0.42921286821365356\n",
      "cnt: 0 - valLoss: 0.436163067817688 - trainLoss: 0.42921021580696106\n",
      "cnt: 0 - valLoss: 0.4361620843410492 - trainLoss: 0.42920756340026855\n",
      "cnt: 0 - valLoss: 0.436161071062088 - trainLoss: 0.42920494079589844\n",
      "cnt: 0 - valLoss: 0.4361600875854492 - trainLoss: 0.42920228838920593\n",
      "cnt: 0 - valLoss: 0.4361591041088104 - trainLoss: 0.4291996359825134\n",
      "cnt: 0 - valLoss: 0.4361581802368164 - trainLoss: 0.4291969835758209\n",
      "cnt: 0 - valLoss: 0.4361571967601776 - trainLoss: 0.4291943311691284\n",
      "cnt: 0 - valLoss: 0.43615618348121643 - trainLoss: 0.4291916787624359\n",
      "cnt: 0 - valLoss: 0.43615517020225525 - trainLoss: 0.4291890263557434\n",
      "cnt: 0 - valLoss: 0.43615421652793884 - trainLoss: 0.4291863739490509\n",
      "cnt: 0 - valLoss: 0.43615320324897766 - trainLoss: 0.4291837215423584\n",
      "cnt: 0 - valLoss: 0.43615224957466125 - trainLoss: 0.4291810691356659\n",
      "cnt: 0 - valLoss: 0.43615126609802246 - trainLoss: 0.4291784167289734\n",
      "cnt: 0 - valLoss: 0.43615028262138367 - trainLoss: 0.4291757643222809\n",
      "cnt: 0 - valLoss: 0.4361492395401001 - trainLoss: 0.4291731119155884\n",
      "cnt: 0 - valLoss: 0.4361483156681061 - trainLoss: 0.42917048931121826\n",
      "cnt: 0 - valLoss: 0.4361473321914673 - trainLoss: 0.42916783690452576\n",
      "cnt: 0 - valLoss: 0.4361463487148285 - trainLoss: 0.42916518449783325\n",
      "cnt: 0 - valLoss: 0.4361453652381897 - trainLoss: 0.42916253209114075\n",
      "cnt: 0 - valLoss: 0.4361443817615509 - trainLoss: 0.429159939289093\n",
      "cnt: 0 - valLoss: 0.4361434280872345 - trainLoss: 0.4291572868824005\n",
      "cnt: 0 - valLoss: 0.4361423850059509 - trainLoss: 0.4291546046733856\n",
      "cnt: 0 - valLoss: 0.4361414611339569 - trainLoss: 0.4291519522666931\n",
      "cnt: 0 - valLoss: 0.43614041805267334 - trainLoss: 0.4291492998600006\n",
      "cnt: 0 - valLoss: 0.4361394941806793 - trainLoss: 0.4291466474533081\n",
      "cnt: 0 - valLoss: 0.4361385107040405 - trainLoss: 0.4291439950466156\n",
      "cnt: 0 - valLoss: 0.43613749742507935 - trainLoss: 0.4291413426399231\n",
      "cnt: 0 - valLoss: 0.43613651394844055 - trainLoss: 0.4291386902332306\n",
      "cnt: 0 - valLoss: 0.43613553047180176 - trainLoss: 0.4291360378265381\n",
      "cnt: 0 - valLoss: 0.43613454699516296 - trainLoss: 0.42913344502449036\n",
      "cnt: 0 - valLoss: 0.43613356351852417 - trainLoss: 0.4291307330131531\n",
      "cnt: 0 - valLoss: 0.4361325800418854 - trainLoss: 0.42912808060646057\n",
      "cnt: 0 - valLoss: 0.4361315965652466 - trainLoss: 0.42912542819976807\n",
      "cnt: 0 - valLoss: 0.436130553483963 - trainLoss: 0.42912277579307556\n",
      "cnt: 0 - valLoss: 0.43612954020500183 - trainLoss: 0.42912012338638306\n",
      "cnt: 0 - valLoss: 0.43612849712371826 - trainLoss: 0.4291175603866577\n",
      "cnt: 0 - valLoss: 0.4361274838447571 - trainLoss: 0.4291149079799652\n",
      "cnt: 0 - valLoss: 0.4361264109611511 - trainLoss: 0.4291122555732727\n",
      "cnt: 0 - valLoss: 0.43612536787986755 - trainLoss: 0.4291096329689026\n",
      "cnt: 0 - valLoss: 0.4361242949962616 - trainLoss: 0.4291069507598877\n",
      "cnt: 0 - valLoss: 0.4361232817173004 - trainLoss: 0.4291043281555176\n",
      "cnt: 0 - valLoss: 0.43612223863601685 - trainLoss: 0.42910170555114746\n",
      "cnt: 0 - valLoss: 0.4361211657524109 - trainLoss: 0.42909905314445496\n",
      "cnt: 0 - valLoss: 0.4361201226711273 - trainLoss: 0.42909640073776245\n",
      "cnt: 0 - valLoss: 0.43611910939216614 - trainLoss: 0.42909374833106995\n",
      "cnt: 0 - valLoss: 0.4361180365085602 - trainLoss: 0.4290911555290222\n",
      "cnt: 0 - valLoss: 0.436117023229599 - trainLoss: 0.4290885329246521\n",
      "cnt: 0 - valLoss: 0.43611598014831543 - trainLoss: 0.4290858805179596\n",
      "cnt: 0 - valLoss: 0.43611496686935425 - trainLoss: 0.4290832281112671\n",
      "cnt: 0 - valLoss: 0.4361138641834259 - trainLoss: 0.4290805757045746\n",
      "cnt: 0 - valLoss: 0.43611282110214233 - trainLoss: 0.4290779232978821\n",
      "cnt: 0 - valLoss: 0.43611180782318115 - trainLoss: 0.42907530069351196\n",
      "cnt: 0 - valLoss: 0.4361107647418976 - trainLoss: 0.42907261848449707\n",
      "cnt: 0 - valLoss: 0.436109721660614 - trainLoss: 0.42907005548477173\n",
      "cnt: 0 - valLoss: 0.43610870838165283 - trainLoss: 0.4290674030780792\n",
      "cnt: 0 - valLoss: 0.4361076354980469 - trainLoss: 0.4290647506713867\n",
      "cnt: 0 - valLoss: 0.4361065924167633 - trainLoss: 0.4290620684623718\n",
      "cnt: 0 - valLoss: 0.43610554933547974 - trainLoss: 0.4290594756603241\n",
      "cnt: 0 - valLoss: 0.43610450625419617 - trainLoss: 0.4290568232536316\n",
      "cnt: 0 - valLoss: 0.4361034631729126 - trainLoss: 0.4290541708469391\n",
      "cnt: 0 - valLoss: 0.4361024498939514 - trainLoss: 0.42905154824256897\n",
      "cnt: 0 - valLoss: 0.43610140681266785 - trainLoss: 0.42904895544052124\n",
      "cnt: 0 - valLoss: 0.43610039353370667 - trainLoss: 0.42904624342918396\n",
      "cnt: 0 - valLoss: 0.4360993206501007 - trainLoss: 0.42904362082481384\n",
      "cnt: 0 - valLoss: 0.43609824776649475 - trainLoss: 0.4290410280227661\n",
      "cnt: 0 - valLoss: 0.43609726428985596 - trainLoss: 0.4290383756160736\n",
      "cnt: 0 - valLoss: 0.43609619140625 - trainLoss: 0.4290357232093811\n",
      "cnt: 0 - valLoss: 0.43609514832496643 - trainLoss: 0.4290330708026886\n",
      "cnt: 0 - valLoss: 0.43609413504600525 - trainLoss: 0.4290304183959961\n",
      "cnt: 0 - valLoss: 0.4360930919647217 - trainLoss: 0.429027795791626\n",
      "cnt: 0 - valLoss: 0.4360920786857605 - trainLoss: 0.42902514338493347\n",
      "cnt: 0 - valLoss: 0.43609100580215454 - trainLoss: 0.42902252078056335\n",
      "cnt: 0 - valLoss: 0.4360899329185486 - trainLoss: 0.42901986837387085\n",
      "cnt: 0 - valLoss: 0.4360889196395874 - trainLoss: 0.42901721596717834\n",
      "cnt: 0 - valLoss: 0.43608787655830383 - trainLoss: 0.42901456356048584\n",
      "cnt: 0 - valLoss: 0.43608683347702026 - trainLoss: 0.4290120005607605\n",
      "cnt: 0 - valLoss: 0.4360858201980591 - trainLoss: 0.429009348154068\n",
      "cnt: 0 - valLoss: 0.4360847473144531 - trainLoss: 0.4290066957473755\n",
      "cnt: 0 - valLoss: 0.43608370423316956 - trainLoss: 0.429004043340683\n",
      "cnt: 0 - valLoss: 0.43608272075653076 - trainLoss: 0.42900142073631287\n",
      "cnt: 0 - valLoss: 0.4360817074775696 - trainLoss: 0.42899876832962036\n",
      "cnt: 0 - valLoss: 0.43608057498931885 - trainLoss: 0.42899611592292786\n",
      "cnt: 0 - valLoss: 0.43607956171035767 - trainLoss: 0.4289935231208801\n",
      "cnt: 0 - valLoss: 0.4360785186290741 - trainLoss: 0.4289908707141876\n",
      "cnt: 0 - valLoss: 0.4360775053501129 - trainLoss: 0.4289882183074951\n",
      "cnt: 0 - valLoss: 0.43607646226882935 - trainLoss: 0.428985595703125\n",
      "cnt: 0 - valLoss: 0.43607544898986816 - trainLoss: 0.4289829432964325\n",
      "cnt: 0 - valLoss: 0.4360743761062622 - trainLoss: 0.42898029088974\n",
      "cnt: 0 - valLoss: 0.4360733926296234 - trainLoss: 0.4289776682853699\n",
      "cnt: 0 - valLoss: 0.43607231974601746 - trainLoss: 0.42897501587867737\n",
      "cnt: 0 - valLoss: 0.43607133626937866 - trainLoss: 0.42897236347198486\n",
      "cnt: 0 - valLoss: 0.4360702633857727 - trainLoss: 0.42896977066993713\n",
      "cnt: 0 - valLoss: 0.4360692799091339 - trainLoss: 0.42896711826324463\n",
      "cnt: 0 - valLoss: 0.43606820702552795 - trainLoss: 0.4289644658565521\n",
      "cnt: 0 - valLoss: 0.43606722354888916 - trainLoss: 0.428961843252182\n",
      "cnt: 0 - valLoss: 0.4360661804676056 - trainLoss: 0.4289591908454895\n",
      "cnt: 0 - valLoss: 0.43606510758399963 - trainLoss: 0.428956538438797\n",
      "cnt: 0 - valLoss: 0.43606409430503845 - trainLoss: 0.4289538860321045\n",
      "cnt: 0 - valLoss: 0.43606311082839966 - trainLoss: 0.42895132303237915\n",
      "cnt: 0 - valLoss: 0.4360620677471161 - trainLoss: 0.42894867062568665\n",
      "cnt: 0 - valLoss: 0.43606099486351013 - trainLoss: 0.42894601821899414\n",
      "cnt: 0 - valLoss: 0.43605998158454895 - trainLoss: 0.42894333600997925\n",
      "cnt: 0 - valLoss: 0.43605899810791016 - trainLoss: 0.4289407432079315\n",
      "cnt: 0 - valLoss: 0.4360579550266266 - trainLoss: 0.428938090801239\n",
      "cnt: 0 - valLoss: 0.4360569417476654 - trainLoss: 0.4289354383945465\n",
      "cnt: 0 - valLoss: 0.43605586886405945 - trainLoss: 0.4289328157901764\n",
      "cnt: 0 - valLoss: 0.43605488538742065 - trainLoss: 0.4289301633834839\n",
      "cnt: 0 - valLoss: 0.4360538423061371 - trainLoss: 0.4289275109767914\n",
      "cnt: 0 - valLoss: 0.4360528290271759 - trainLoss: 0.4289248585700989\n",
      "cnt: 0 - valLoss: 0.43605178594589233 - trainLoss: 0.42892223596572876\n",
      "cnt: 0 - valLoss: 0.4360507130622864 - trainLoss: 0.42891964316368103\n",
      "cnt: 0 - valLoss: 0.4360496997833252 - trainLoss: 0.4289169907569885\n",
      "cnt: 0 - valLoss: 0.43604862689971924 - trainLoss: 0.428914338350296\n",
      "cnt: 0 - valLoss: 0.4360475540161133 - trainLoss: 0.4289117753505707\n",
      "cnt: 0 - valLoss: 0.4360464811325073 - trainLoss: 0.4289090633392334\n",
      "cnt: 0 - valLoss: 0.43604543805122375 - trainLoss: 0.4289064407348633\n",
      "cnt: 0 - valLoss: 0.4360444247722626 - trainLoss: 0.4289037883281708\n",
      "cnt: 0 - valLoss: 0.4360433518886566 - trainLoss: 0.42890113592147827\n",
      "cnt: 0 - valLoss: 0.43604227900505066 - trainLoss: 0.42889857292175293\n",
      "cnt: 0 - valLoss: 0.4360412061214447 - trainLoss: 0.4288959205150604\n",
      "cnt: 0 - valLoss: 0.43604013323783875 - trainLoss: 0.4288932681083679\n",
      "cnt: 0 - valLoss: 0.4360390901565552 - trainLoss: 0.4288906157016754\n",
      "cnt: 0 - valLoss: 0.4360380172729492 - trainLoss: 0.4288879632949829\n",
      "cnt: 0 - valLoss: 0.43603700399398804 - trainLoss: 0.4288853406906128\n",
      "cnt: 0 - valLoss: 0.4360359311103821 - trainLoss: 0.42888274788856506\n",
      "cnt: 0 - valLoss: 0.4360348582267761 - trainLoss: 0.42888009548187256\n",
      "cnt: 0 - valLoss: 0.43603378534317017 - trainLoss: 0.42887744307518005\n",
      "cnt: 0 - valLoss: 0.4360327422618866 - trainLoss: 0.42887479066848755\n",
      "cnt: 0 - valLoss: 0.436031699180603 - trainLoss: 0.42887216806411743\n",
      "cnt: 0 - valLoss: 0.43603062629699707 - trainLoss: 0.4288695752620697\n",
      "cnt: 0 - valLoss: 0.4360295832157135 - trainLoss: 0.4288668930530548\n",
      "cnt: 0 - valLoss: 0.4360285699367523 - trainLoss: 0.4288643002510071\n",
      "cnt: 0 - valLoss: 0.43602749705314636 - trainLoss: 0.4288616478443146\n",
      "cnt: 0 - valLoss: 0.4360264241695404 - trainLoss: 0.42885899543762207\n",
      "cnt: 0 - valLoss: 0.43602538108825684 - trainLoss: 0.42885637283325195\n",
      "cnt: 0 - valLoss: 0.43602436780929565 - trainLoss: 0.42885372042655945\n",
      "cnt: 0 - valLoss: 0.4360232949256897 - trainLoss: 0.4288511276245117\n",
      "cnt: 0 - valLoss: 0.43602222204208374 - trainLoss: 0.42884841561317444\n",
      "cnt: 0 - valLoss: 0.43602117896080017 - trainLoss: 0.4288458526134491\n",
      "cnt: 0 - valLoss: 0.4360200762748718 - trainLoss: 0.4288432002067566\n",
      "cnt: 0 - valLoss: 0.43601909279823303 - trainLoss: 0.4288405478000641\n",
      "cnt: 0 - valLoss: 0.4360180199146271 - trainLoss: 0.4288378953933716\n",
      "cnt: 0 - valLoss: 0.4360169470310211 - trainLoss: 0.42883527278900146\n",
      "cnt: 0 - valLoss: 0.43601590394973755 - trainLoss: 0.42883262038230896\n",
      "cnt: 0 - valLoss: 0.4360148310661316 - trainLoss: 0.42882999777793884\n",
      "cnt: 0 - valLoss: 0.4360138177871704 - trainLoss: 0.42882734537124634\n",
      "cnt: 0 - valLoss: 0.43601274490356445 - trainLoss: 0.42882469296455383\n",
      "cnt: 0 - valLoss: 0.4360116720199585 - trainLoss: 0.4288221001625061\n",
      "cnt: 0 - valLoss: 0.4360106289386749 - trainLoss: 0.428819477558136\n",
      "cnt: 0 - valLoss: 0.43600955605506897 - trainLoss: 0.4288168251514435\n",
      "cnt: 0 - valLoss: 0.4360085427761078 - trainLoss: 0.428814172744751\n",
      "cnt: 0 - valLoss: 0.43600746989250183 - trainLoss: 0.42881155014038086\n",
      "cnt: 0 - valLoss: 0.43600642681121826 - trainLoss: 0.42880895733833313\n",
      "cnt: 0 - valLoss: 0.4360054135322571 - trainLoss: 0.4288063049316406\n",
      "cnt: 0 - valLoss: 0.43600428104400635 - trainLoss: 0.4288036525249481\n",
      "cnt: 0 - valLoss: 0.43600326776504517 - trainLoss: 0.4288010001182556\n",
      "cnt: 0 - valLoss: 0.4360021948814392 - trainLoss: 0.4287984371185303\n",
      "cnt: 0 - valLoss: 0.43600112199783325 - trainLoss: 0.428795725107193\n",
      "cnt: 0 - valLoss: 0.43600010871887207 - trainLoss: 0.4287931025028229\n",
      "cnt: 0 - valLoss: 0.4359990656375885 - trainLoss: 0.42879045009613037\n",
      "cnt: 0 - valLoss: 0.43599799275398254 - trainLoss: 0.42878779768943787\n",
      "cnt: 0 - valLoss: 0.43599700927734375 - trainLoss: 0.42878520488739014\n",
      "cnt: 0 - valLoss: 0.4359959065914154 - trainLoss: 0.42878255248069763\n",
      "cnt: 0 - valLoss: 0.43599486351013184 - trainLoss: 0.4287799298763275\n",
      "cnt: 0 - valLoss: 0.43599385023117065 - trainLoss: 0.428777277469635\n",
      "cnt: 0 - valLoss: 0.4359928071498871 - trainLoss: 0.4287746548652649\n",
      "cnt: 0 - valLoss: 0.43599170446395874 - trainLoss: 0.4287720024585724\n",
      "cnt: 0 - valLoss: 0.43599066138267517 - trainLoss: 0.4287693500518799\n",
      "cnt: 0 - valLoss: 0.435989648103714 - trainLoss: 0.42876675724983215\n",
      "cnt: 0 - valLoss: 0.4359886050224304 - trainLoss: 0.42876410484313965\n",
      "cnt: 0 - valLoss: 0.43598753213882446 - trainLoss: 0.42876145243644714\n",
      "cnt: 0 - valLoss: 0.4359864592552185 - trainLoss: 0.428758829832077\n",
      "cnt: 0 - valLoss: 0.4359854459762573 - trainLoss: 0.4287562072277069\n",
      "cnt: 0 - valLoss: 0.43598440289497375 - trainLoss: 0.4287535548210144\n",
      "cnt: 0 - valLoss: 0.4359833300113678 - trainLoss: 0.4287509024143219\n",
      "cnt: 0 - valLoss: 0.43598225712776184 - trainLoss: 0.42874830961227417\n",
      "cnt: 0 - valLoss: 0.43598124384880066 - trainLoss: 0.4287455976009369\n",
      "cnt: 0 - valLoss: 0.4359802007675171 - trainLoss: 0.42874303460121155\n",
      "cnt: 0 - valLoss: 0.4359791874885559 - trainLoss: 0.42874038219451904\n",
      "cnt: 0 - valLoss: 0.43597814440727234 - trainLoss: 0.42873772978782654\n",
      "cnt: 0 - valLoss: 0.4359770715236664 - trainLoss: 0.4287351071834564\n",
      "cnt: 0 - valLoss: 0.4359759986400604 - trainLoss: 0.4287324547767639\n",
      "cnt: 0 - valLoss: 0.43597498536109924 - trainLoss: 0.4287298619747162\n",
      "cnt: 0 - valLoss: 0.4359739124774933 - trainLoss: 0.4287272095680237\n",
      "cnt: 0 - valLoss: 0.4359728693962097 - trainLoss: 0.42872458696365356\n",
      "cnt: 0 - valLoss: 0.43597179651260376 - trainLoss: 0.42872193455696106\n",
      "cnt: 0 - valLoss: 0.4359707832336426 - trainLoss: 0.42871931195259094\n",
      "cnt: 0 - valLoss: 0.435969740152359 - trainLoss: 0.42871662974357605\n",
      "cnt: 0 - valLoss: 0.4359687268733978 - trainLoss: 0.42871400713920593\n",
      "cnt: 0 - valLoss: 0.43596765398979187 - trainLoss: 0.4287113547325134\n",
      "cnt: 0 - valLoss: 0.4359665811061859 - trainLoss: 0.4287087917327881\n",
      "cnt: 0 - valLoss: 0.43596553802490234 - trainLoss: 0.4287061393260956\n",
      "cnt: 0 - valLoss: 0.43596452474594116 - trainLoss: 0.4287034869194031\n",
      "cnt: 0 - valLoss: 0.4359634816646576 - trainLoss: 0.42870083451271057\n",
      "cnt: 0 - valLoss: 0.4359624683856964 - trainLoss: 0.42869821190834045\n",
      "cnt: 0 - valLoss: 0.43596139550209045 - trainLoss: 0.42869552969932556\n",
      "cnt: 0 - valLoss: 0.4359603226184845 - trainLoss: 0.4286929666996002\n",
      "cnt: 0 - valLoss: 0.4359592795372009 - trainLoss: 0.4286903142929077\n",
      "cnt: 0 - valLoss: 0.43595826625823975 - trainLoss: 0.4286876618862152\n",
      "cnt: 0 - valLoss: 0.4359572231769562 - trainLoss: 0.4286850094795227\n",
      "cnt: 0 - valLoss: 0.4359561502933502 - trainLoss: 0.4286823868751526\n",
      "cnt: 0 - valLoss: 0.4359551668167114 - trainLoss: 0.4286797344684601\n",
      "cnt: 0 - valLoss: 0.4359540641307831 - trainLoss: 0.42867711186408997\n",
      "cnt: 0 - valLoss: 0.4359530210494995 - trainLoss: 0.42867445945739746\n",
      "cnt: 0 - valLoss: 0.43595200777053833 - trainLoss: 0.42867180705070496\n",
      "cnt: 0 - valLoss: 0.43595096468925476 - trainLoss: 0.4286692142486572\n",
      "cnt: 0 - valLoss: 0.4359499514102936 - trainLoss: 0.4286665618419647\n",
      "cnt: 0 - valLoss: 0.4359488785266876 - trainLoss: 0.4286639392375946\n",
      "cnt: 0 - valLoss: 0.43594783544540405 - trainLoss: 0.4286612868309021\n",
      "cnt: 0 - valLoss: 0.4359467625617981 - trainLoss: 0.4286586344242096\n",
      "cnt: 0 - valLoss: 0.4359457492828369 - trainLoss: 0.4286560118198395\n",
      "cnt: 0 - valLoss: 0.43594470620155334 - trainLoss: 0.42865341901779175\n",
      "cnt: 0 - valLoss: 0.4359436333179474 - trainLoss: 0.42865076661109924\n",
      "cnt: 0 - valLoss: 0.4359426498413086 - trainLoss: 0.42864811420440674\n",
      "cnt: 0 - valLoss: 0.43594157695770264 - trainLoss: 0.42864546179771423\n",
      "cnt: 0 - valLoss: 0.43594059348106384 - trainLoss: 0.42864280939102173\n",
      "cnt: 0 - valLoss: 0.4359395205974579 - trainLoss: 0.4286401867866516\n",
      "cnt: 0 - valLoss: 0.43593844771385193 - trainLoss: 0.4286375641822815\n",
      "cnt: 0 - valLoss: 0.4359373450279236 - trainLoss: 0.428634911775589\n",
      "cnt: 0 - valLoss: 0.43593624234199524 - trainLoss: 0.4286321699619293\n",
      "cnt: 0 - valLoss: 0.43593502044677734 - trainLoss: 0.42862945795059204\n",
      "cnt: 0 - valLoss: 0.4359338879585266 - trainLoss: 0.4286266565322876\n",
      "cnt: 0 - valLoss: 0.4359327256679535 - trainLoss: 0.4286239743232727\n",
      "cnt: 0 - valLoss: 0.43593159317970276 - trainLoss: 0.4286211431026459\n",
      "cnt: 0 - valLoss: 0.4359304904937744 - trainLoss: 0.4286184012889862\n",
      "cnt: 0 - valLoss: 0.4359293282032013 - trainLoss: 0.42861565947532654\n",
      "cnt: 0 - valLoss: 0.4359281659126282 - trainLoss: 0.42861291766166687\n",
      "cnt: 0 - valLoss: 0.43592706322669983 - trainLoss: 0.4286101758480072\n",
      "cnt: 0 - valLoss: 0.4359259009361267 - trainLoss: 0.42860743403434753\n",
      "cnt: 0 - valLoss: 0.43592479825019836 - trainLoss: 0.42860469222068787\n",
      "cnt: 0 - valLoss: 0.43592363595962524 - trainLoss: 0.4286019206047058\n",
      "cnt: 0 - valLoss: 0.4359224736690521 - trainLoss: 0.42859917879104614\n",
      "cnt: 0 - valLoss: 0.435921311378479 - trainLoss: 0.4285964071750641\n",
      "cnt: 0 - valLoss: 0.43592020869255066 - trainLoss: 0.4285936653614044\n",
      "cnt: 0 - valLoss: 0.4359190762042999 - trainLoss: 0.42859092354774475\n",
      "cnt: 0 - valLoss: 0.4359179437160492 - trainLoss: 0.4285881817340851\n",
      "cnt: 0 - valLoss: 0.4359167814254761 - trainLoss: 0.4285854399204254\n",
      "cnt: 0 - valLoss: 0.43591564893722534 - trainLoss: 0.42858269810676575\n",
      "cnt: 0 - valLoss: 0.4359144866466522 - trainLoss: 0.4285799562931061\n",
      "cnt: 0 - valLoss: 0.4359133541584015 - trainLoss: 0.4285772144794464\n",
      "cnt: 0 - valLoss: 0.43591222167015076 - trainLoss: 0.42857447266578674\n",
      "cnt: 0 - valLoss: 0.4359111189842224 - trainLoss: 0.4285716712474823\n",
      "cnt: 0 - valLoss: 0.4359099864959717 - trainLoss: 0.428568959236145\n",
      "cnt: 0 - valLoss: 0.43590885400772095 - trainLoss: 0.42856618762016296\n",
      "cnt: 0 - valLoss: 0.4359076917171478 - trainLoss: 0.4285634458065033\n",
      "cnt: 0 - valLoss: 0.4359065592288971 - trainLoss: 0.42856070399284363\n",
      "cnt: 0 - valLoss: 0.43590545654296875 - trainLoss: 0.42855796217918396\n",
      "cnt: 0 - valLoss: 0.43590426445007324 - trainLoss: 0.4285552203655243\n",
      "cnt: 0 - valLoss: 0.4359031319618225 - trainLoss: 0.4285524785518646\n",
      "cnt: 0 - valLoss: 0.43590202927589417 - trainLoss: 0.42854973673820496\n",
      "cnt: 0 - valLoss: 0.4359009265899658 - trainLoss: 0.4285469651222229\n",
      "cnt: 0 - valLoss: 0.4358997642993927 - trainLoss: 0.42854422330856323\n",
      "cnt: 0 - valLoss: 0.43589863181114197 - trainLoss: 0.4285414516925812\n",
      "cnt: 0 - valLoss: 0.4358975291252136 - trainLoss: 0.4285387098789215\n",
      "cnt: 0 - valLoss: 0.4358963668346405 - trainLoss: 0.42853596806526184\n",
      "cnt: 0 - valLoss: 0.43589526414871216 - trainLoss: 0.4285332262516022\n",
      "cnt: 0 - valLoss: 0.4358941316604614 - trainLoss: 0.4285304844379425\n",
      "cnt: 0 - valLoss: 0.4358930289745331 - trainLoss: 0.42852774262428284\n",
      "cnt: 0 - valLoss: 0.43589186668395996 - trainLoss: 0.42852500081062317\n",
      "cnt: 0 - valLoss: 0.43589070439338684 - trainLoss: 0.4285222291946411\n",
      "cnt: 0 - valLoss: 0.4358896017074585 - trainLoss: 0.42851948738098145\n",
      "cnt: 0 - valLoss: 0.43588846921920776 - trainLoss: 0.4285167455673218\n",
      "cnt: 0 - valLoss: 0.4358873665332794 - trainLoss: 0.4285139739513397\n",
      "cnt: 0 - valLoss: 0.4358862638473511 - trainLoss: 0.42851123213768005\n",
      "cnt: 0 - valLoss: 0.43588513135910034 - trainLoss: 0.4285084903240204\n",
      "cnt: 0 - valLoss: 0.4358839690685272 - trainLoss: 0.4285057485103607\n",
      "cnt: 0 - valLoss: 0.4358828365802765 - trainLoss: 0.42850300669670105\n",
      "cnt: 0 - valLoss: 0.43588176369667053 - trainLoss: 0.4285002648830414\n",
      "cnt: 0 - valLoss: 0.4358806014060974 - trainLoss: 0.4284975230693817\n",
      "cnt: 0 - valLoss: 0.4358794689178467 - trainLoss: 0.42849478125572205\n",
      "cnt: 0 - valLoss: 0.43587836623191833 - trainLoss: 0.4284919500350952\n",
      "cnt: 0 - valLoss: 0.4358772039413452 - trainLoss: 0.4284892678260803\n",
      "cnt: 0 - valLoss: 0.43587613105773926 - trainLoss: 0.42848649621009827\n",
      "cnt: 0 - valLoss: 0.4358750283718109 - trainLoss: 0.4284837543964386\n",
      "cnt: 0 - valLoss: 0.4358738362789154 - trainLoss: 0.42848101258277893\n",
      "cnt: 0 - valLoss: 0.4358727037906647 - trainLoss: 0.42847827076911926\n",
      "cnt: 0 - valLoss: 0.4358716309070587 - trainLoss: 0.4284755289554596\n",
      "cnt: 0 - valLoss: 0.4358704686164856 - trainLoss: 0.4284727871417999\n",
      "cnt: 0 - valLoss: 0.43586936593055725 - trainLoss: 0.42847001552581787\n",
      "cnt: 0 - valLoss: 0.4358682632446289 - trainLoss: 0.4284672737121582\n",
      "cnt: 0 - valLoss: 0.4358671307563782 - trainLoss: 0.42846447229385376\n",
      "cnt: 0 - valLoss: 0.43586602807044983 - trainLoss: 0.4284617602825165\n",
      "cnt: 0 - valLoss: 0.4358648955821991 - trainLoss: 0.4284590184688568\n",
      "cnt: 0 - valLoss: 0.43586379289627075 - trainLoss: 0.42845627665519714\n",
      "cnt: 0 - valLoss: 0.4358626902103424 - trainLoss: 0.4284535348415375\n",
      "cnt: 0 - valLoss: 0.4358615577220917 - trainLoss: 0.42845073342323303\n",
      "cnt: 0 - valLoss: 0.43586045503616333 - trainLoss: 0.42844802141189575\n",
      "cnt: 0 - valLoss: 0.4358593225479126 - trainLoss: 0.4284452795982361\n",
      "cnt: 0 - valLoss: 0.43585819005966187 - trainLoss: 0.4284425377845764\n",
      "cnt: 0 - valLoss: 0.43585705757141113 - trainLoss: 0.42843979597091675\n",
      "cnt: 0 - valLoss: 0.4358559548854828 - trainLoss: 0.4284369945526123\n",
      "cnt: 0 - valLoss: 0.43585482239723206 - trainLoss: 0.42843425273895264\n",
      "cnt: 0 - valLoss: 0.4358537197113037 - trainLoss: 0.42843154072761536\n",
      "cnt: 0 - valLoss: 0.43585261702537537 - trainLoss: 0.4284287989139557\n",
      "cnt: 0 - valLoss: 0.43585145473480225 - trainLoss: 0.42842599749565125\n",
      "cnt: 0 - valLoss: 0.4358503818511963 - trainLoss: 0.4284232556819916\n",
      "cnt: 0 - valLoss: 0.43584927916526794 - trainLoss: 0.4284205138683319\n",
      "cnt: 0 - valLoss: 0.4358481466770172 - trainLoss: 0.42841780185699463\n",
      "cnt: 0 - valLoss: 0.4358469843864441 - trainLoss: 0.4284150004386902\n",
      "cnt: 0 - valLoss: 0.43584588170051575 - trainLoss: 0.4284122586250305\n",
      "cnt: 0 - valLoss: 0.4358448088169098 - trainLoss: 0.42840951681137085\n",
      "cnt: 0 - valLoss: 0.43584370613098145 - trainLoss: 0.4284067749977112\n",
      "cnt: 0 - valLoss: 0.4358425438404083 - trainLoss: 0.4284040629863739\n",
      "cnt: 0 - valLoss: 0.43584147095680237 - trainLoss: 0.42840132117271423\n",
      "cnt: 0 - valLoss: 0.4358403980731964 - trainLoss: 0.42839857935905457\n",
      "cnt: 0 - valLoss: 0.4358392655849457 - trainLoss: 0.4283957779407501\n",
      "cnt: 0 - valLoss: 0.43583816289901733 - trainLoss: 0.42839300632476807\n",
      "cnt: 0 - valLoss: 0.4358370006084442 - trainLoss: 0.4283902645111084\n",
      "cnt: 0 - valLoss: 0.43583592772483826 - trainLoss: 0.42838752269744873\n",
      "cnt: 0 - valLoss: 0.4358348250389099 - trainLoss: 0.42838478088378906\n",
      "cnt: 0 - valLoss: 0.4358336925506592 - trainLoss: 0.428382009267807\n",
      "cnt: 0 - valLoss: 0.43583258986473083 - trainLoss: 0.4283792972564697\n",
      "cnt: 0 - valLoss: 0.4358314871788025 - trainLoss: 0.42837652564048767\n",
      "cnt: 0 - valLoss: 0.43583035469055176 - trainLoss: 0.4283737540245056\n",
      "cnt: 0 - valLoss: 0.4358292818069458 - trainLoss: 0.42837101221084595\n",
      "cnt: 0 - valLoss: 0.4358281195163727 - trainLoss: 0.4283682703971863\n",
      "cnt: 0 - valLoss: 0.4358270764350891 - trainLoss: 0.4283655285835266\n",
      "cnt: 0 - valLoss: 0.4358259439468384 - trainLoss: 0.42836278676986694\n",
      "cnt: 0 - valLoss: 0.4358248710632324 - trainLoss: 0.4283600151538849\n",
      "cnt: 0 - valLoss: 0.4358237683773041 - trainLoss: 0.42835721373558044\n",
      "cnt: 0 - valLoss: 0.43582263588905334 - trainLoss: 0.42835453152656555\n",
      "cnt: 0 - valLoss: 0.435821533203125 - trainLoss: 0.4283517599105835\n",
      "cnt: 0 - valLoss: 0.43582046031951904 - trainLoss: 0.42834901809692383\n",
      "cnt: 0 - valLoss: 0.4358193576335907 - trainLoss: 0.42834627628326416\n",
      "cnt: 0 - valLoss: 0.4358181953430176 - trainLoss: 0.4283435344696045\n",
      "cnt: 0 - valLoss: 0.435817152261734 - trainLoss: 0.4283407926559448\n",
      "cnt: 0 - valLoss: 0.43581604957580566 - trainLoss: 0.42833805084228516\n",
      "cnt: 0 - valLoss: 0.4358149468898773 - trainLoss: 0.4283352792263031\n",
      "cnt: 0 - valLoss: 0.43581390380859375 - trainLoss: 0.42833253741264343\n",
      "cnt: 0 - valLoss: 0.43581274151802063 - trainLoss: 0.428329735994339\n",
      "cnt: 0 - valLoss: 0.4358116686344147 - trainLoss: 0.4283269941806793\n",
      "cnt: 0 - valLoss: 0.43581056594848633 - trainLoss: 0.42832428216934204\n",
      "cnt: 0 - valLoss: 0.43580949306488037 - trainLoss: 0.4283215403556824\n",
      "cnt: 0 - valLoss: 0.43580836057662964 - trainLoss: 0.4283187985420227\n",
      "cnt: 0 - valLoss: 0.43580731749534607 - trainLoss: 0.42831605672836304\n",
      "cnt: 0 - valLoss: 0.43580618500709534 - trainLoss: 0.4283132553100586\n",
      "cnt: 0 - valLoss: 0.435805082321167 - trainLoss: 0.4283105134963989\n",
      "cnt: 0 - valLoss: 0.43580400943756104 - trainLoss: 0.42830774188041687\n",
      "cnt: 0 - valLoss: 0.4358028769493103 - trainLoss: 0.4283050000667572\n",
      "cnt: 0 - valLoss: 0.43580180406570435 - trainLoss: 0.42830225825309753\n",
      "cnt: 0 - valLoss: 0.4358007311820984 - trainLoss: 0.42829951643943787\n",
      "cnt: 0 - valLoss: 0.43579962849617004 - trainLoss: 0.4282967746257782\n",
      "cnt: 0 - valLoss: 0.4357985556125641 - trainLoss: 0.42829400300979614\n",
      "cnt: 0 - valLoss: 0.43579745292663574 - trainLoss: 0.4282912611961365\n",
      "cnt: 0 - valLoss: 0.4357963800430298 - trainLoss: 0.4282884895801544\n",
      "cnt: 0 - valLoss: 0.43579524755477905 - trainLoss: 0.42828574776649475\n",
      "cnt: 0 - valLoss: 0.4357941746711731 - trainLoss: 0.4282830059528351\n",
      "cnt: 0 - valLoss: 0.43579307198524475 - trainLoss: 0.4282802641391754\n",
      "cnt: 0 - valLoss: 0.4357919692993164 - trainLoss: 0.42827752232551575\n",
      "cnt: 0 - valLoss: 0.43579089641571045 - trainLoss: 0.4282747805118561\n",
      "cnt: 0 - valLoss: 0.4357898235321045 - trainLoss: 0.428272008895874\n",
      "cnt: 0 - valLoss: 0.43578869104385376 - trainLoss: 0.42826929688453674\n",
      "cnt: 0 - valLoss: 0.4357876181602478 - trainLoss: 0.4282664954662323\n",
      "cnt: 0 - valLoss: 0.43578654527664185 - trainLoss: 0.42826375365257263\n",
      "cnt: 0 - valLoss: 0.4357854425907135 - trainLoss: 0.42826101183891296\n",
      "cnt: 0 - valLoss: 0.43578433990478516 - trainLoss: 0.4282582700252533\n",
      "cnt: 0 - valLoss: 0.4357832670211792 - trainLoss: 0.42825552821159363\n",
      "cnt: 0 - valLoss: 0.43578219413757324 - trainLoss: 0.42825278639793396\n",
      "cnt: 0 - valLoss: 0.4357811212539673 - trainLoss: 0.4282500147819519\n",
      "cnt: 0 - valLoss: 0.43577998876571655 - trainLoss: 0.42824727296829224\n",
      "cnt: 0 - valLoss: 0.4357789158821106 - trainLoss: 0.42824453115463257\n",
      "cnt: 0 - valLoss: 0.43577781319618225 - trainLoss: 0.4282417595386505\n",
      "cnt: 0 - valLoss: 0.43577679991722107 - trainLoss: 0.42823901772499084\n",
      "cnt: 0 - valLoss: 0.43577566742897034 - trainLoss: 0.4282362759113312\n",
      "cnt: 0 - valLoss: 0.435774564743042 - trainLoss: 0.4282335340976715\n",
      "cnt: 0 - valLoss: 0.4357735216617584 - trainLoss: 0.42823079228401184\n",
      "cnt: 0 - valLoss: 0.4357724189758301 - trainLoss: 0.4282280504703522\n",
      "cnt: 0 - valLoss: 0.43577128648757935 - trainLoss: 0.4282253086566925\n",
      "cnt: 0 - valLoss: 0.4357702136039734 - trainLoss: 0.42822253704071045\n",
      "cnt: 0 - valLoss: 0.4357691705226898 - trainLoss: 0.4282197952270508\n",
      "cnt: 0 - valLoss: 0.4357680380344391 - trainLoss: 0.42821699380874634\n",
      "cnt: 0 - valLoss: 0.43576696515083313 - trainLoss: 0.42821428179740906\n",
      "cnt: 0 - valLoss: 0.4357658922672272 - trainLoss: 0.4282115399837494\n",
      "cnt: 0 - valLoss: 0.43576478958129883 - trainLoss: 0.4282087981700897\n",
      "cnt: 0 - valLoss: 0.4357636570930481 - trainLoss: 0.42820605635643005\n",
      "cnt: 0 - valLoss: 0.4357626438140869 - trainLoss: 0.4282033145427704\n",
      "cnt: 0 - valLoss: 0.43576157093048096 - trainLoss: 0.4282005727291107\n",
      "cnt: 0 - valLoss: 0.4357604682445526 - trainLoss: 0.42819780111312866\n",
      "cnt: 0 - valLoss: 0.43575939536094666 - trainLoss: 0.4281949996948242\n",
      "cnt: 0 - valLoss: 0.4357583224773407 - trainLoss: 0.42819225788116455\n",
      "cnt: 0 - valLoss: 0.43575724959373474 - trainLoss: 0.42818957567214966\n",
      "cnt: 0 - valLoss: 0.435756117105484 - trainLoss: 0.4281867742538452\n",
      "cnt: 0 - valLoss: 0.43575504422187805 - trainLoss: 0.42818406224250793\n",
      "cnt: 0 - valLoss: 0.4357539713382721 - trainLoss: 0.42818132042884827\n",
      "cnt: 0 - valLoss: 0.43575289845466614 - trainLoss: 0.4281785786151886\n",
      "cnt: 0 - valLoss: 0.4357518255710602 - trainLoss: 0.42817583680152893\n",
      "cnt: 0 - valLoss: 0.4357507824897766 - trainLoss: 0.4281730353832245\n",
      "cnt: 0 - valLoss: 0.4357496500015259 - trainLoss: 0.42817026376724243\n",
      "cnt: 0 - valLoss: 0.4357485771179199 - trainLoss: 0.42816752195358276\n",
      "cnt: 0 - valLoss: 0.4357474744319916 - trainLoss: 0.4281647801399231\n",
      "cnt: 0 - valLoss: 0.4357464015483856 - trainLoss: 0.4281620383262634\n",
      "cnt: 0 - valLoss: 0.43574535846710205 - trainLoss: 0.42815929651260376\n",
      "cnt: 0 - valLoss: 0.4357442855834961 - trainLoss: 0.4281565845012665\n",
      "cnt: 0 - valLoss: 0.43574318289756775 - trainLoss: 0.42815378308296204\n",
      "cnt: 0 - valLoss: 0.4357421100139618 - trainLoss: 0.42815104126930237\n",
      "cnt: 0 - valLoss: 0.43574103713035583 - trainLoss: 0.4281482696533203\n",
      "cnt: 0 - valLoss: 0.4357399344444275 - trainLoss: 0.42814552783966064\n",
      "cnt: 0 - valLoss: 0.4357388913631439 - trainLoss: 0.428142786026001\n",
      "cnt: 0 - valLoss: 0.4357377886772156 - trainLoss: 0.4281400442123413\n",
      "cnt: 0 - valLoss: 0.4357367157936096 - trainLoss: 0.42813730239868164\n",
      "cnt: 0 - valLoss: 0.43573564291000366 - trainLoss: 0.428134560585022\n",
      "cnt: 0 - valLoss: 0.4357345700263977 - trainLoss: 0.4281318187713623\n",
      "cnt: 0 - valLoss: 0.43573349714279175 - trainLoss: 0.42812904715538025\n",
      "cnt: 0 - valLoss: 0.4357323944568634 - trainLoss: 0.4281263053417206\n",
      "cnt: 0 - valLoss: 0.43573135137557983 - trainLoss: 0.4281235337257385\n",
      "cnt: 0 - valLoss: 0.4357302486896515 - trainLoss: 0.42812079191207886\n",
      "cnt: 0 - valLoss: 0.43572917580604553 - trainLoss: 0.4281180500984192\n",
      "cnt: 0 - valLoss: 0.4357281029224396 - trainLoss: 0.4281153082847595\n",
      "cnt: 0 - valLoss: 0.43572697043418884 - trainLoss: 0.4281124770641327\n",
      "cnt: 0 - valLoss: 0.43572598695755005 - trainLoss: 0.4281097948551178\n",
      "cnt: 0 - valLoss: 0.43572500348091125 - trainLoss: 0.42810699343681335\n",
      "cnt: 0 - valLoss: 0.4357239902019501 - trainLoss: 0.42810431122779846\n",
      "cnt: 0 - valLoss: 0.43572303652763367 - trainLoss: 0.4281015694141388\n",
      "cnt: 0 - valLoss: 0.4357219934463501 - trainLoss: 0.4280988276004791\n",
      "cnt: 0 - valLoss: 0.4357209801673889 - trainLoss: 0.42809608578681946\n",
      "cnt: 0 - valLoss: 0.4357200264930725 - trainLoss: 0.42809340357780457\n",
      "cnt: 0 - valLoss: 0.43571901321411133 - trainLoss: 0.4280906021595001\n",
      "cnt: 0 - valLoss: 0.43571802973747253 - trainLoss: 0.42808786034584045\n",
      "cnt: 0 - valLoss: 0.43571704626083374 - trainLoss: 0.42808517813682556\n",
      "cnt: 0 - valLoss: 0.43571606278419495 - trainLoss: 0.4280824065208435\n",
      "cnt: 0 - valLoss: 0.43571507930755615 - trainLoss: 0.42807960510253906\n",
      "cnt: 0 - valLoss: 0.43571406602859497 - trainLoss: 0.42807692289352417\n",
      "cnt: 0 - valLoss: 0.4357130527496338 - trainLoss: 0.4280741810798645\n",
      "cnt: 0 - valLoss: 0.435712069272995 - trainLoss: 0.42807143926620483\n",
      "cnt: 0 - valLoss: 0.4357110857963562 - trainLoss: 0.42806869745254517\n",
      "cnt: 0 - valLoss: 0.4357101023197174 - trainLoss: 0.4280660152435303\n",
      "cnt: 0 - valLoss: 0.43570905923843384 - trainLoss: 0.4280632734298706\n",
      "cnt: 0 - valLoss: 0.43570807576179504 - trainLoss: 0.42806047201156616\n",
      "cnt: 0 - valLoss: 0.43570709228515625 - trainLoss: 0.42805778980255127\n",
      "cnt: 0 - valLoss: 0.4357060492038727 - trainLoss: 0.4280550479888916\n",
      "cnt: 0 - valLoss: 0.43570512533187866 - trainLoss: 0.42805227637290955\n",
      "cnt: 0 - valLoss: 0.4357040822505951 - trainLoss: 0.42804956436157227\n",
      "cnt: 0 - valLoss: 0.4357030689716339 - trainLoss: 0.4280467927455902\n",
      "cnt: 0 - valLoss: 0.4357021152973175 - trainLoss: 0.42804408073425293\n",
      "cnt: 0 - valLoss: 0.4357011020183563 - trainLoss: 0.42804133892059326\n",
      "cnt: 0 - valLoss: 0.43570005893707275 - trainLoss: 0.4280385971069336\n",
      "cnt: 0 - valLoss: 0.43569910526275635 - trainLoss: 0.42803582549095154\n",
      "cnt: 0 - valLoss: 0.43569809198379517 - trainLoss: 0.42803308367729187\n",
      "cnt: 0 - valLoss: 0.4356970489025116 - trainLoss: 0.4280303418636322\n",
      "cnt: 0 - valLoss: 0.4356960654258728 - trainLoss: 0.4280276596546173\n",
      "cnt: 0 - valLoss: 0.435695081949234 - trainLoss: 0.42802491784095764\n",
      "cnt: 0 - valLoss: 0.4356940686702728 - trainLoss: 0.428022176027298\n",
      "cnt: 0 - valLoss: 0.4356931149959564 - trainLoss: 0.4280194044113159\n",
      "cnt: 0 - valLoss: 0.43569210171699524 - trainLoss: 0.42801669239997864\n",
      "cnt: 0 - valLoss: 0.43569111824035645 - trainLoss: 0.42801395058631897\n",
      "cnt: 0 - valLoss: 0.43569010496139526 - trainLoss: 0.4280112087726593\n",
      "cnt: 0 - valLoss: 0.4356890916824341 - trainLoss: 0.42800846695899963\n",
      "cnt: 0 - valLoss: 0.4356881082057953 - trainLoss: 0.42800572514533997\n",
      "cnt: 0 - valLoss: 0.4356871247291565 - trainLoss: 0.4280029833316803\n",
      "cnt: 0 - valLoss: 0.4356861412525177 - trainLoss: 0.42800021171569824\n",
      "cnt: 0 - valLoss: 0.4356851577758789 - trainLoss: 0.42799752950668335\n",
      "cnt: 0 - valLoss: 0.43568411469459534 - trainLoss: 0.4279947876930237\n",
      "cnt: 0 - valLoss: 0.43568310141563416 - trainLoss: 0.427992045879364\n",
      "cnt: 0 - valLoss: 0.43568211793899536 - trainLoss: 0.42798930406570435\n",
      "cnt: 0 - valLoss: 0.4356811046600342 - trainLoss: 0.4279865622520447\n",
      "cnt: 0 - valLoss: 0.4356801211833954 - trainLoss: 0.427983820438385\n",
      "cnt: 0 - valLoss: 0.4356791377067566 - trainLoss: 0.42798104882240295\n",
      "cnt: 0 - valLoss: 0.4356781542301178 - trainLoss: 0.4279783368110657\n",
      "cnt: 0 - valLoss: 0.435677170753479 - trainLoss: 0.4279755651950836\n",
      "cnt: 0 - valLoss: 0.4356761574745178 - trainLoss: 0.42797282338142395\n",
      "cnt: 0 - valLoss: 0.43567517399787903 - trainLoss: 0.4279700815677643\n",
      "cnt: 0 - valLoss: 0.43567416071891785 - trainLoss: 0.4279673993587494\n",
      "cnt: 0 - valLoss: 0.43567314743995667 - trainLoss: 0.4279646575450897\n",
      "cnt: 0 - valLoss: 0.43567219376564026 - trainLoss: 0.4279618561267853\n",
      "cnt: 0 - valLoss: 0.4356711804866791 - trainLoss: 0.4279591739177704\n",
      "cnt: 0 - valLoss: 0.4356701970100403 - trainLoss: 0.4279564321041107\n",
      "cnt: 0 - valLoss: 0.4356691539287567 - trainLoss: 0.42795366048812866\n",
      "cnt: 0 - valLoss: 0.4356681704521179 - trainLoss: 0.4279509484767914\n",
      "cnt: 0 - valLoss: 0.43566715717315674 - trainLoss: 0.4279482066631317\n",
      "cnt: 0 - valLoss: 0.43566620349884033 - trainLoss: 0.42794543504714966\n",
      "cnt: 0 - valLoss: 0.43566516041755676 - trainLoss: 0.42794269323349\n",
      "cnt: 0 - valLoss: 0.4356641471385956 - trainLoss: 0.4279399812221527\n",
      "cnt: 0 - valLoss: 0.4356631934642792 - trainLoss: 0.42793720960617065\n",
      "cnt: 0 - valLoss: 0.4356622099876404 - trainLoss: 0.427934467792511\n",
      "cnt: 0 - valLoss: 0.4356611967086792 - trainLoss: 0.4279317259788513\n",
      "cnt: 0 - valLoss: 0.4356602132320404 - trainLoss: 0.42792895436286926\n",
      "cnt: 0 - valLoss: 0.4356592297554016 - trainLoss: 0.427926242351532\n",
      "cnt: 0 - valLoss: 0.43565818667411804 - trainLoss: 0.4279235303401947\n",
      "cnt: 0 - valLoss: 0.43565720319747925 - trainLoss: 0.42792078852653503\n",
      "cnt: 0 - valLoss: 0.4356561601161957 - trainLoss: 0.42791804671287537\n",
      "cnt: 0 - valLoss: 0.4356551468372345 - trainLoss: 0.4279152452945709\n",
      "cnt: 0 - valLoss: 0.4356541931629181 - trainLoss: 0.42791256308555603\n",
      "cnt: 0 - valLoss: 0.4356532096862793 - trainLoss: 0.4279097616672516\n",
      "cnt: 0 - valLoss: 0.4356522262096405 - trainLoss: 0.4279070794582367\n",
      "cnt: 0 - valLoss: 0.4356512129306793 - trainLoss: 0.42790430784225464\n",
      "cnt: 0 - valLoss: 0.4356502294540405 - trainLoss: 0.42790156602859497\n",
      "cnt: 0 - valLoss: 0.43564921617507935 - trainLoss: 0.4278988242149353\n",
      "cnt: 0 - valLoss: 0.43564820289611816 - trainLoss: 0.42789608240127563\n",
      "cnt: 0 - valLoss: 0.43564721941947937 - trainLoss: 0.42789334058761597\n",
      "cnt: 0 - valLoss: 0.4356461763381958 - trainLoss: 0.4278905987739563\n",
      "cnt: 0 - valLoss: 0.435645192861557 - trainLoss: 0.42788785696029663\n",
      "cnt: 0 - valLoss: 0.4356441795825958 - trainLoss: 0.42788517475128174\n",
      "cnt: 0 - valLoss: 0.43564319610595703 - trainLoss: 0.4278823733329773\n",
      "cnt: 0 - valLoss: 0.43564221262931824 - trainLoss: 0.4278796315193176\n",
      "cnt: 0 - valLoss: 0.43564119935035706 - trainLoss: 0.42787688970565796\n",
      "cnt: 0 - valLoss: 0.43564021587371826 - trainLoss: 0.4278741776943207\n",
      "cnt: 0 - valLoss: 0.4356392025947571 - trainLoss: 0.427871435880661\n",
      "cnt: 0 - valLoss: 0.4356382191181183 - trainLoss: 0.42786869406700134\n",
      "cnt: 0 - valLoss: 0.4356372356414795 - trainLoss: 0.4278659522533417\n",
      "cnt: 0 - valLoss: 0.4356362521648407 - trainLoss: 0.427863210439682\n",
      "cnt: 0 - valLoss: 0.4356352686882019 - trainLoss: 0.42786046862602234\n",
      "cnt: 0 - valLoss: 0.43563422560691833 - trainLoss: 0.42785772681236267\n",
      "cnt: 0 - valLoss: 0.43563321232795715 - trainLoss: 0.427854984998703\n",
      "cnt: 0 - valLoss: 0.43563219904899597 - trainLoss: 0.42785224318504333\n",
      "cnt: 0 - valLoss: 0.4356312155723572 - trainLoss: 0.42784950137138367\n",
      "cnt: 0 - valLoss: 0.4356302320957184 - trainLoss: 0.427846759557724\n",
      "cnt: 0 - valLoss: 0.4356292486190796 - trainLoss: 0.4278440475463867\n",
      "cnt: 0 - valLoss: 0.4356282353401184 - trainLoss: 0.42784130573272705\n",
      "cnt: 0 - valLoss: 0.4356272518634796 - trainLoss: 0.4278385639190674\n",
      "cnt: 0 - valLoss: 0.4356262683868408 - trainLoss: 0.4278358221054077\n",
      "cnt: 0 - valLoss: 0.43562522530555725 - trainLoss: 0.42783308029174805\n",
      "cnt: 0 - valLoss: 0.43562427163124084 - trainLoss: 0.4278303384780884\n",
      "cnt: 0 - valLoss: 0.43562325835227966 - trainLoss: 0.4278275966644287\n",
      "cnt: 0 - valLoss: 0.43562227487564087 - trainLoss: 0.42782482504844666\n",
      "cnt: 0 - valLoss: 0.4356212317943573 - trainLoss: 0.427822083234787\n",
      "cnt: 0 - valLoss: 0.4356202483177185 - trainLoss: 0.4278193712234497\n",
      "cnt: 0 - valLoss: 0.4356192350387573 - trainLoss: 0.42781659960746765\n",
      "cnt: 0 - valLoss: 0.4356182813644409 - trainLoss: 0.42781388759613037\n",
      "cnt: 0 - valLoss: 0.43561726808547974 - trainLoss: 0.4278111755847931\n",
      "cnt: 0 - valLoss: 0.43561625480651855 - trainLoss: 0.4278084337711334\n",
      "cnt: 0 - valLoss: 0.43561527132987976 - trainLoss: 0.4278056025505066\n",
      "cnt: 0 - valLoss: 0.4356142580509186 - trainLoss: 0.4278028905391693\n",
      "cnt: 0 - valLoss: 0.435613214969635 - trainLoss: 0.42780014872550964\n",
      "cnt: 0 - valLoss: 0.4356122314929962 - trainLoss: 0.42779743671417236\n",
      "cnt: 0 - valLoss: 0.4356112480163574 - trainLoss: 0.4277946949005127\n",
      "cnt: 0 - valLoss: 0.4356103241443634 - trainLoss: 0.42779189348220825\n",
      "cnt: 0 - valLoss: 0.43560925126075745 - trainLoss: 0.42778921127319336\n",
      "cnt: 0 - valLoss: 0.43560826778411865 - trainLoss: 0.4277864694595337\n",
      "cnt: 0 - valLoss: 0.43560725450515747 - trainLoss: 0.427783727645874\n",
      "cnt: 0 - valLoss: 0.4356062412261963 - trainLoss: 0.42778098583221436\n",
      "cnt: 0 - valLoss: 0.4356052577495575 - trainLoss: 0.4277782440185547\n",
      "cnt: 0 - valLoss: 0.4356042146682739 - trainLoss: 0.427775502204895\n",
      "cnt: 0 - valLoss: 0.4356032907962799 - trainLoss: 0.4277728199958801\n",
      "cnt: 0 - valLoss: 0.43560224771499634 - trainLoss: 0.4277700185775757\n",
      "cnt: 0 - valLoss: 0.43560126423835754 - trainLoss: 0.427767276763916\n",
      "cnt: 0 - valLoss: 0.43560028076171875 - trainLoss: 0.42776450514793396\n",
      "cnt: 0 - valLoss: 0.4355992376804352 - trainLoss: 0.4277617633342743\n",
      "cnt: 0 - valLoss: 0.435598224401474 - trainLoss: 0.4277590215206146\n",
      "cnt: 0 - valLoss: 0.4355972707271576 - trainLoss: 0.42775633931159973\n",
      "cnt: 0 - valLoss: 0.4355962574481964 - trainLoss: 0.42775359749794006\n",
      "cnt: 0 - valLoss: 0.43559521436691284 - trainLoss: 0.4277507960796356\n",
      "cnt: 0 - valLoss: 0.43559423089027405 - trainLoss: 0.42774808406829834\n",
      "cnt: 0 - valLoss: 0.43559324741363525 - trainLoss: 0.42774534225463867\n",
      "cnt: 0 - valLoss: 0.4355922341346741 - trainLoss: 0.427742600440979\n",
      "cnt: 0 - valLoss: 0.4355912506580353 - trainLoss: 0.42773985862731934\n",
      "cnt: 0 - valLoss: 0.4355902373790741 - trainLoss: 0.42773711681365967\n",
      "cnt: 0 - valLoss: 0.4355892241001129 - trainLoss: 0.427734375\n",
      "cnt: 0 - valLoss: 0.4355882704257965 - trainLoss: 0.42773163318634033\n",
      "cnt: 0 - valLoss: 0.43558719754219055 - trainLoss: 0.42772889137268066\n",
      "cnt: 0 - valLoss: 0.43558621406555176 - trainLoss: 0.427726149559021\n",
      "cnt: 0 - valLoss: 0.43558523058891296 - trainLoss: 0.42772340774536133\n",
      "cnt: 0 - valLoss: 0.43558424711227417 - trainLoss: 0.42772066593170166\n",
      "cnt: 0 - valLoss: 0.435583233833313 - trainLoss: 0.4277179539203644\n",
      "cnt: 0 - valLoss: 0.4355822503566742 - trainLoss: 0.4277152121067047\n",
      "cnt: 0 - valLoss: 0.435581237077713 - trainLoss: 0.42771247029304504\n",
      "cnt: 0 - valLoss: 0.43558022379875183 - trainLoss: 0.4277097284793854\n",
      "cnt: 0 - valLoss: 0.4355792701244354 - trainLoss: 0.4277069866657257\n",
      "cnt: 0 - valLoss: 0.43557825684547424 - trainLoss: 0.42770424485206604\n",
      "cnt: 0 - valLoss: 0.4355772137641907 - trainLoss: 0.427701473236084\n",
      "cnt: 0 - valLoss: 0.4355762004852295 - trainLoss: 0.4276987314224243\n",
      "cnt: 0 - valLoss: 0.4355752468109131 - trainLoss: 0.42769601941108704\n",
      "cnt: 0 - valLoss: 0.4355742335319519 - trainLoss: 0.42769327759742737\n",
      "cnt: 0 - valLoss: 0.4355732202529907 - trainLoss: 0.4276905357837677\n",
      "cnt: 0 - valLoss: 0.43557223677635193 - trainLoss: 0.42768779397010803\n",
      "cnt: 0 - valLoss: 0.43557122349739075 - trainLoss: 0.427685022354126\n",
      "cnt: 0 - valLoss: 0.43557024002075195 - trainLoss: 0.4276822805404663\n",
      "cnt: 0 - valLoss: 0.43556925654411316 - trainLoss: 0.42767953872680664\n",
      "cnt: 0 - valLoss: 0.43556827306747437 - trainLoss: 0.427676796913147\n",
      "cnt: 0 - valLoss: 0.4355672299861908 - trainLoss: 0.4276740550994873\n",
      "cnt: 0 - valLoss: 0.435566246509552 - trainLoss: 0.42767131328582764\n",
      "cnt: 0 - valLoss: 0.4355652332305908 - trainLoss: 0.42766860127449036\n",
      "cnt: 0 - valLoss: 0.43556419014930725 - trainLoss: 0.4276658594608307\n",
      "cnt: 0 - valLoss: 0.43556323647499084 - trainLoss: 0.427663117647171\n",
      "cnt: 0 - valLoss: 0.43556222319602966 - trainLoss: 0.42766037583351135\n",
      "cnt: 0 - valLoss: 0.4355611801147461 - trainLoss: 0.4276576340198517\n",
      "cnt: 0 - valLoss: 0.4355601966381073 - trainLoss: 0.427654892206192\n",
      "cnt: 0 - valLoss: 0.4355592131614685 - trainLoss: 0.42765212059020996\n",
      "cnt: 0 - valLoss: 0.4355581998825073 - trainLoss: 0.4276493787765503\n",
      "cnt: 0 - valLoss: 0.4355572462081909 - trainLoss: 0.4276466369628906\n",
      "cnt: 0 - valLoss: 0.43555623292922974 - trainLoss: 0.42764392495155334\n",
      "cnt: 0 - valLoss: 0.43555521965026855 - trainLoss: 0.4276411533355713\n",
      "cnt: 0 - valLoss: 0.4355542063713074 - trainLoss: 0.4276384115219116\n",
      "cnt: 0 - valLoss: 0.4355532228946686 - trainLoss: 0.42763566970825195\n",
      "cnt: 0 - valLoss: 0.435552179813385 - trainLoss: 0.4276329278945923\n",
      "cnt: 0 - valLoss: 0.4355511963367462 - trainLoss: 0.4276301860809326\n",
      "cnt: 0 - valLoss: 0.43555018305778503 - trainLoss: 0.42762744426727295\n",
      "cnt: 0 - valLoss: 0.43554919958114624 - trainLoss: 0.4276247024536133\n",
      "cnt: 0 - valLoss: 0.43554821610450745 - trainLoss: 0.427621990442276\n",
      "cnt: 0 - valLoss: 0.4355471730232239 - trainLoss: 0.42761924862861633\n",
      "cnt: 0 - valLoss: 0.4355461895465851 - trainLoss: 0.42761650681495667\n",
      "cnt: 0 - valLoss: 0.4355451464653015 - trainLoss: 0.427613765001297\n",
      "cnt: 0 - valLoss: 0.4355442225933075 - trainLoss: 0.42761102318763733\n",
      "cnt: 0 - valLoss: 0.4355431795120239 - trainLoss: 0.42760828137397766\n",
      "cnt: 0 - valLoss: 0.43554216623306274 - trainLoss: 0.427605539560318\n",
      "cnt: 0 - valLoss: 0.43554121255874634 - trainLoss: 0.4276027977466583\n",
      "cnt: 0 - valLoss: 0.43554019927978516 - trainLoss: 0.42760005593299866\n",
      "cnt: 0 - valLoss: 0.4355391561985016 - trainLoss: 0.427597314119339\n",
      "cnt: 0 - valLoss: 0.4355381727218628 - trainLoss: 0.4275945723056793\n",
      "cnt: 0 - valLoss: 0.435537189245224 - trainLoss: 0.42759186029434204\n",
      "cnt: 0 - valLoss: 0.43553614616394043 - trainLoss: 0.4275891184806824\n",
      "cnt: 0 - valLoss: 0.43553513288497925 - trainLoss: 0.4275863766670227\n",
      "cnt: 0 - valLoss: 0.43553414940834045 - trainLoss: 0.42758363485336304\n",
      "cnt: 0 - valLoss: 0.43553316593170166 - trainLoss: 0.42758089303970337\n",
      "cnt: 0 - valLoss: 0.43553218245506287 - trainLoss: 0.4275781512260437\n",
      "cnt: 0 - valLoss: 0.4355311989784241 - trainLoss: 0.42757540941238403\n",
      "cnt: 0 - valLoss: 0.4355301558971405 - trainLoss: 0.427572637796402\n",
      "cnt: 0 - valLoss: 0.4355291724205017 - trainLoss: 0.4275699257850647\n",
      "cnt: 0 - valLoss: 0.4355281591415405 - trainLoss: 0.42756718397140503\n",
      "cnt: 0 - valLoss: 0.43552714586257935 - trainLoss: 0.427564412355423\n",
      "cnt: 0 - valLoss: 0.43552613258361816 - trainLoss: 0.4275616705417633\n",
      "cnt: 0 - valLoss: 0.43552514910697937 - trainLoss: 0.4275589883327484\n",
      "cnt: 0 - valLoss: 0.4355241656303406 - trainLoss: 0.42755618691444397\n",
      "cnt: 0 - valLoss: 0.435523122549057 - trainLoss: 0.4275535047054291\n",
      "cnt: 0 - valLoss: 0.4355221390724182 - trainLoss: 0.4275507628917694\n",
      "cnt: 0 - valLoss: 0.4355211555957794 - trainLoss: 0.4275479316711426\n",
      "cnt: 0 - valLoss: 0.4355201721191406 - trainLoss: 0.4275452196598053\n",
      "cnt: 0 - valLoss: 0.43551912903785706 - trainLoss: 0.427542507648468\n",
      "cnt: 0 - valLoss: 0.43551814556121826 - trainLoss: 0.42753979563713074\n",
      "cnt: 0 - valLoss: 0.4355171322822571 - trainLoss: 0.4275369644165039\n",
      "cnt: 0 - valLoss: 0.4355161488056183 - trainLoss: 0.427534282207489\n",
      "cnt: 0 - valLoss: 0.4355151653289795 - trainLoss: 0.42753154039382935\n",
      "cnt: 0 - valLoss: 0.4355141222476959 - trainLoss: 0.4275287687778473\n",
      "cnt: 0 - valLoss: 0.4355131983757019 - trainLoss: 0.42752605676651\n",
      "cnt: 0 - valLoss: 0.43551212549209595 - trainLoss: 0.42752328515052795\n",
      "cnt: 0 - valLoss: 0.43551114201545715 - trainLoss: 0.4275205433368683\n",
      "cnt: 0 - valLoss: 0.4355100989341736 - trainLoss: 0.4275178015232086\n",
      "cnt: 0 - valLoss: 0.4355091452598572 - trainLoss: 0.42751505970954895\n",
      "cnt: 0 - valLoss: 0.435508131980896 - trainLoss: 0.4275123178958893\n",
      "cnt: 0 - valLoss: 0.4355070888996124 - trainLoss: 0.4275095760822296\n",
      "cnt: 0 - valLoss: 0.43550610542297363 - trainLoss: 0.4275068938732147\n",
      "cnt: 0 - valLoss: 0.43550512194633484 - trainLoss: 0.4275040924549103\n",
      "cnt: 0 - valLoss: 0.43550410866737366 - trainLoss: 0.4275013506412506\n",
      "cnt: 0 - valLoss: 0.4355030655860901 - trainLoss: 0.42749860882759094\n",
      "cnt: 0 - valLoss: 0.4355020821094513 - trainLoss: 0.42749589681625366\n",
      "cnt: 0 - valLoss: 0.4355010986328125 - trainLoss: 0.427493155002594\n",
      "cnt: 0 - valLoss: 0.43550005555152893 - trainLoss: 0.4274904131889343\n",
      "cnt: 0 - valLoss: 0.43549907207489014 - trainLoss: 0.42748767137527466\n",
      "cnt: 0 - valLoss: 0.43549808859825134 - trainLoss: 0.4274848699569702\n",
      "cnt: 0 - valLoss: 0.43549707531929016 - trainLoss: 0.4274821877479553\n",
      "cnt: 0 - valLoss: 0.4354960322380066 - trainLoss: 0.42747944593429565\n",
      "cnt: 0 - valLoss: 0.4354950487613678 - trainLoss: 0.427476704120636\n",
      "cnt: 0 - valLoss: 0.435494065284729 - trainLoss: 0.4274739623069763\n",
      "cnt: 0 - valLoss: 0.4354930818080902 - trainLoss: 0.42747122049331665\n",
      "cnt: 0 - valLoss: 0.4354920983314514 - trainLoss: 0.427468478679657\n",
      "cnt: 0 - valLoss: 0.43549105525016785 - trainLoss: 0.4274657070636749\n",
      "cnt: 0 - valLoss: 0.43549004197120667 - trainLoss: 0.42746302485466003\n",
      "cnt: 0 - valLoss: 0.4354889988899231 - trainLoss: 0.42746028304100037\n",
      "cnt: 0 - valLoss: 0.4354880750179291 - trainLoss: 0.4274574816226959\n",
      "cnt: 0 - valLoss: 0.4354870319366455 - trainLoss: 0.42745473980903625\n",
      "cnt: 0 - valLoss: 0.4354860186576843 - trainLoss: 0.4274519681930542\n",
      "cnt: 0 - valLoss: 0.4354850649833679 - trainLoss: 0.4274492859840393\n",
      "cnt: 0 - valLoss: 0.43548399209976196 - trainLoss: 0.42744654417037964\n",
      "cnt: 0 - valLoss: 0.4354829788208008 - trainLoss: 0.4274437427520752\n",
      "cnt: 0 - valLoss: 0.4354819059371948 - trainLoss: 0.4274410605430603\n",
      "cnt: 0 - valLoss: 0.43548086285591125 - trainLoss: 0.42743831872940063\n",
      "cnt: 0 - valLoss: 0.4354797601699829 - trainLoss: 0.42743557691574097\n",
      "cnt: 0 - valLoss: 0.43547868728637695 - trainLoss: 0.4274328351020813\n",
      "cnt: 0 - valLoss: 0.4354776442050934 - trainLoss: 0.4274301528930664\n",
      "cnt: 0 - valLoss: 0.43547654151916504 - trainLoss: 0.42742741107940674\n",
      "cnt: 0 - valLoss: 0.4354753792285919 - trainLoss: 0.42742466926574707\n",
      "cnt: 0 - valLoss: 0.4354742765426636 - trainLoss: 0.4274219572544098\n",
      "cnt: 0 - valLoss: 0.43547314405441284 - trainLoss: 0.4274192154407501\n",
      "cnt: 0 - valLoss: 0.4354720413684845 - trainLoss: 0.42741647362709045\n",
      "cnt: 0 - valLoss: 0.4354708790779114 - trainLoss: 0.42741379141807556\n",
      "cnt: 0 - valLoss: 0.43546977639198303 - trainLoss: 0.4274110496044159\n",
      "cnt: 0 - valLoss: 0.4354686141014099 - trainLoss: 0.4274083077907562\n",
      "cnt: 0 - valLoss: 0.4354674816131592 - trainLoss: 0.42740556597709656\n",
      "cnt: 0 - valLoss: 0.43546637892723083 - trainLoss: 0.42740288376808167\n",
      "cnt: 0 - valLoss: 0.4354652762413025 - trainLoss: 0.4274001717567444\n",
      "cnt: 0 - valLoss: 0.43546414375305176 - trainLoss: 0.4273974299430847\n",
      "cnt: 0 - valLoss: 0.4354630410671234 - trainLoss: 0.42739468812942505\n",
      "cnt: 0 - valLoss: 0.4354618787765503 - trainLoss: 0.42739197611808777\n",
      "cnt: 0 - valLoss: 0.43546077609062195 - trainLoss: 0.4273892641067505\n",
      "cnt: 0 - valLoss: 0.4354596436023712 - trainLoss: 0.4273865222930908\n",
      "cnt: 0 - valLoss: 0.4354584813117981 - trainLoss: 0.42738378047943115\n",
      "cnt: 0 - valLoss: 0.43545737862586975 - trainLoss: 0.42738109827041626\n",
      "cnt: 0 - valLoss: 0.4354562759399414 - trainLoss: 0.4273783564567566\n",
      "cnt: 0 - valLoss: 0.4354551434516907 - trainLoss: 0.4273756146430969\n",
      "cnt: 0 - valLoss: 0.43545404076576233 - trainLoss: 0.42737287282943726\n",
      "cnt: 0 - valLoss: 0.4354528784751892 - trainLoss: 0.42737019062042236\n",
      "cnt: 0 - valLoss: 0.43545177578926086 - trainLoss: 0.4273674786090851\n",
      "cnt: 0 - valLoss: 0.4354507029056549 - trainLoss: 0.4273647367954254\n",
      "cnt: 0 - valLoss: 0.4354495704174042 - trainLoss: 0.42736199498176575\n",
      "cnt: 0 - valLoss: 0.43544846773147583 - trainLoss: 0.4273592531681061\n",
      "cnt: 0 - valLoss: 0.4354473352432251 - trainLoss: 0.4273565709590912\n",
      "cnt: 0 - valLoss: 0.43544623255729675 - trainLoss: 0.4273538291454315\n",
      "cnt: 0 - valLoss: 0.4354451298713684 - trainLoss: 0.42735111713409424\n",
      "cnt: 0 - valLoss: 0.4354439973831177 - trainLoss: 0.42734837532043457\n",
      "cnt: 0 - valLoss: 0.43544289469718933 - trainLoss: 0.4273456633090973\n",
      "cnt: 0 - valLoss: 0.435441792011261 - trainLoss: 0.42734295129776\n",
      "cnt: 0 - valLoss: 0.43544065952301025 - trainLoss: 0.42734020948410034\n",
      "cnt: 0 - valLoss: 0.4354395568370819 - trainLoss: 0.4273374676704407\n",
      "cnt: 0 - valLoss: 0.4354384243488312 - trainLoss: 0.4273347854614258\n",
      "cnt: 0 - valLoss: 0.43543732166290283 - trainLoss: 0.4273320436477661\n",
      "cnt: 0 - valLoss: 0.4354362487792969 - trainLoss: 0.42732930183410645\n",
      "cnt: 0 - valLoss: 0.43543514609336853 - trainLoss: 0.42732658982276917\n",
      "cnt: 0 - valLoss: 0.4354340136051178 - trainLoss: 0.4273238778114319\n",
      "cnt: 0 - valLoss: 0.43543291091918945 - trainLoss: 0.4273211359977722\n",
      "cnt: 0 - valLoss: 0.4354318380355835 - trainLoss: 0.42731842398643494\n",
      "cnt: 0 - valLoss: 0.43543070554733276 - trainLoss: 0.42731568217277527\n",
      "cnt: 0 - valLoss: 0.4354296028614044 - trainLoss: 0.4273129403591156\n",
      "cnt: 0 - valLoss: 0.4354285001754761 - trainLoss: 0.4273102581501007\n",
      "cnt: 0 - valLoss: 0.4354274570941925 - trainLoss: 0.42730751633644104\n",
      "cnt: 0 - valLoss: 0.4354262948036194 - trainLoss: 0.42730480432510376\n",
      "cnt: 0 - valLoss: 0.4354252219200134 - trainLoss: 0.4273020923137665\n",
      "cnt: 0 - valLoss: 0.4354241192340851 - trainLoss: 0.4272993505001068\n",
      "cnt: 0 - valLoss: 0.43542301654815674 - trainLoss: 0.42729660868644714\n",
      "cnt: 0 - valLoss: 0.4354219436645508 - trainLoss: 0.42729389667510986\n",
      "cnt: 0 - valLoss: 0.43542078137397766 - trainLoss: 0.4272911548614502\n",
      "cnt: 0 - valLoss: 0.4354197382926941 - trainLoss: 0.4272884726524353\n",
      "cnt: 0 - valLoss: 0.43541863560676575 - trainLoss: 0.42728573083877563\n",
      "cnt: 0 - valLoss: 0.4354175329208374 - trainLoss: 0.42728298902511597\n",
      "cnt: 0 - valLoss: 0.43541646003723145 - trainLoss: 0.4272802472114563\n",
      "cnt: 0 - valLoss: 0.4354152977466583 - trainLoss: 0.42727750539779663\n",
      "cnt: 0 - valLoss: 0.43541425466537476 - trainLoss: 0.4272748529911041\n",
      "cnt: 0 - valLoss: 0.4354131519794464 - trainLoss: 0.42727208137512207\n",
      "cnt: 0 - valLoss: 0.4354120194911957 - trainLoss: 0.4272693693637848\n",
      "cnt: 0 - valLoss: 0.43541091680526733 - trainLoss: 0.4272666275501251\n",
      "cnt: 0 - valLoss: 0.43540990352630615 - trainLoss: 0.42726394534111023\n",
      "cnt: 0 - valLoss: 0.4354087710380554 - trainLoss: 0.42726123332977295\n",
      "cnt: 0 - valLoss: 0.4354076683521271 - trainLoss: 0.4272584915161133\n",
      "cnt: 0 - valLoss: 0.43540653586387634 - trainLoss: 0.427255779504776\n",
      "cnt: 0 - valLoss: 0.43540552258491516 - trainLoss: 0.42725303769111633\n",
      "cnt: 0 - valLoss: 0.43540439009666443 - trainLoss: 0.42725029587745667\n",
      "cnt: 0 - valLoss: 0.4354032874107361 - trainLoss: 0.4272475838661194\n",
      "cnt: 0 - valLoss: 0.4354022145271301 - trainLoss: 0.4272449016571045\n",
      "cnt: 0 - valLoss: 0.43540114164352417 - trainLoss: 0.4272421598434448\n",
      "cnt: 0 - valLoss: 0.4354000389575958 - trainLoss: 0.42723941802978516\n",
      "cnt: 0 - valLoss: 0.4353989064693451 - trainLoss: 0.4272366762161255\n",
      "cnt: 0 - valLoss: 0.43539783358573914 - trainLoss: 0.4272339940071106\n",
      "cnt: 0 - valLoss: 0.4353967308998108 - trainLoss: 0.4272312521934509\n",
      "cnt: 0 - valLoss: 0.43539565801620483 - trainLoss: 0.42722854018211365\n",
      "cnt: 0 - valLoss: 0.43539464473724365 - trainLoss: 0.427225798368454\n",
      "cnt: 0 - valLoss: 0.4353935122489929 - trainLoss: 0.4272230565547943\n",
      "cnt: 0 - valLoss: 0.4353924095630646 - trainLoss: 0.4272203743457794\n",
      "cnt: 0 - valLoss: 0.43539127707481384 - trainLoss: 0.42721763253211975\n",
      "cnt: 0 - valLoss: 0.43539026379585266 - trainLoss: 0.42721492052078247\n",
      "cnt: 0 - valLoss: 0.43538913130760193 - trainLoss: 0.4272122085094452\n",
      "cnt: 0 - valLoss: 0.43538805842399597 - trainLoss: 0.4272094666957855\n",
      "cnt: 0 - valLoss: 0.4353870153427124 - trainLoss: 0.42720672488212585\n",
      "cnt: 0 - valLoss: 0.43538588285446167 - trainLoss: 0.4272039830684662\n",
      "cnt: 0 - valLoss: 0.4353848695755005 - trainLoss: 0.4272012710571289\n",
      "cnt: 0 - valLoss: 0.43538373708724976 - trainLoss: 0.427198588848114\n",
      "cnt: 0 - valLoss: 0.4353826344013214 - trainLoss: 0.42719584703445435\n",
      "cnt: 0 - valLoss: 0.43538156151771545 - trainLoss: 0.4271931052207947\n",
      "cnt: 0 - valLoss: 0.4353804886341095 - trainLoss: 0.427190363407135\n",
      "cnt: 0 - valLoss: 0.43537941575050354 - trainLoss: 0.4271876811981201\n",
      "cnt: 0 - valLoss: 0.4353783130645752 - trainLoss: 0.42718493938446045\n",
      "cnt: 0 - valLoss: 0.43537724018096924 - trainLoss: 0.42718222737312317\n",
      "cnt: 0 - valLoss: 0.43537619709968567 - trainLoss: 0.4271794855594635\n",
      "cnt: 0 - valLoss: 0.4353750944137573 - trainLoss: 0.42717674374580383\n",
      "cnt: 0 - valLoss: 0.43537402153015137 - trainLoss: 0.42717400193214417\n",
      "cnt: 0 - valLoss: 0.4353729486465454 - trainLoss: 0.4271713197231293\n",
      "cnt: 0 - valLoss: 0.43537187576293945 - trainLoss: 0.4271685779094696\n",
      "cnt: 0 - valLoss: 0.4353708028793335 - trainLoss: 0.42716583609580994\n",
      "cnt: 0 - valLoss: 0.43536967039108276 - trainLoss: 0.42716315388679504\n",
      "cnt: 0 - valLoss: 0.4353686571121216 - trainLoss: 0.4271604120731354\n",
      "cnt: 0 - valLoss: 0.43536755442619324 - trainLoss: 0.4271577000617981\n",
      "cnt: 0 - valLoss: 0.43536651134490967 - trainLoss: 0.4271549582481384\n",
      "cnt: 0 - valLoss: 0.4353654086589813 - trainLoss: 0.42715221643447876\n",
      "cnt: 0 - valLoss: 0.43536433577537537 - trainLoss: 0.4271495044231415\n",
      "cnt: 0 - valLoss: 0.4353632628917694 - trainLoss: 0.4271467924118042\n",
      "cnt: 0 - valLoss: 0.4353621304035187 - trainLoss: 0.42714405059814453\n",
      "cnt: 0 - valLoss: 0.4353610575199127 - trainLoss: 0.42714130878448486\n",
      "cnt: 0 - valLoss: 0.43535998463630676 - trainLoss: 0.4271385669708252\n",
      "cnt: 0 - valLoss: 0.4353589713573456 - trainLoss: 0.4271359145641327\n",
      "cnt: 0 - valLoss: 0.43535786867141724 - trainLoss: 0.427133172750473\n",
      "cnt: 0 - valLoss: 0.43535682559013367 - trainLoss: 0.42713043093681335\n",
      "cnt: 0 - valLoss: 0.4353557229042053 - trainLoss: 0.4271277189254761\n",
      "cnt: 0 - valLoss: 0.43535467982292175 - trainLoss: 0.4271250069141388\n",
      "cnt: 0 - valLoss: 0.4353536069393158 - trainLoss: 0.4271222651004791\n",
      "cnt: 0 - valLoss: 0.43535253405570984 - trainLoss: 0.42711958289146423\n",
      "cnt: 0 - valLoss: 0.4353514611721039 - trainLoss: 0.42711684107780457\n",
      "cnt: 0 - valLoss: 0.4353503882884979 - trainLoss: 0.4271140992641449\n",
      "cnt: 0 - valLoss: 0.43534931540489197 - trainLoss: 0.4271113872528076\n",
      "cnt: 0 - valLoss: 0.435348242521286 - trainLoss: 0.4271087050437927\n",
      "cnt: 0 - valLoss: 0.43534722924232483 - trainLoss: 0.42710596323013306\n",
      "cnt: 0 - valLoss: 0.4353460967540741 - trainLoss: 0.4271032214164734\n",
      "cnt: 0 - valLoss: 0.4353450834751129 - trainLoss: 0.4271004796028137\n",
      "cnt: 0 - valLoss: 0.43534401059150696 - trainLoss: 0.42709779739379883\n",
      "cnt: 0 - valLoss: 0.4353429675102234 - trainLoss: 0.42709505558013916\n",
      "cnt: 0 - valLoss: 0.4353419244289398 - trainLoss: 0.4270923435688019\n",
      "cnt: 0 - valLoss: 0.4353407919406891 - trainLoss: 0.4270896017551422\n",
      "cnt: 0 - valLoss: 0.4353397786617279 - trainLoss: 0.42708685994148254\n",
      "cnt: 0 - valLoss: 0.43533870577812195 - trainLoss: 0.42708417773246765\n",
      "cnt: 0 - valLoss: 0.4353376626968384 - trainLoss: 0.4270814061164856\n",
      "cnt: 0 - valLoss: 0.4353365898132324 - trainLoss: 0.4270786941051483\n",
      "cnt: 0 - valLoss: 0.43533557653427124 - trainLoss: 0.4270760118961334\n",
      "cnt: 0 - valLoss: 0.4353344440460205 - trainLoss: 0.42707327008247375\n",
      "cnt: 0 - valLoss: 0.4353334307670593 - trainLoss: 0.4270705282688141\n",
      "cnt: 0 - valLoss: 0.43533235788345337 - trainLoss: 0.4270678162574768\n",
      "cnt: 0 - valLoss: 0.4353313148021698 - trainLoss: 0.4270651340484619\n",
      "cnt: 0 - valLoss: 0.43533024191856384 - trainLoss: 0.42706239223480225\n",
      "cnt: 0 - valLoss: 0.43532922863960266 - trainLoss: 0.4270596504211426\n",
      "cnt: 0 - valLoss: 0.4353281557559967 - trainLoss: 0.4270569086074829\n",
      "cnt: 0 - valLoss: 0.43532708287239075 - trainLoss: 0.42705416679382324\n",
      "cnt: 0 - valLoss: 0.4353260397911072 - trainLoss: 0.42705148458480835\n",
      "cnt: 0 - valLoss: 0.435325026512146 - trainLoss: 0.42704877257347107\n",
      "cnt: 0 - valLoss: 0.43532389402389526 - trainLoss: 0.4270460307598114\n",
      "cnt: 0 - valLoss: 0.4353228807449341 - trainLoss: 0.42704328894615173\n",
      "cnt: 0 - valLoss: 0.4353218078613281 - trainLoss: 0.42704060673713684\n",
      "cnt: 0 - valLoss: 0.43532073497772217 - trainLoss: 0.4270378351211548\n",
      "cnt: 0 - valLoss: 0.435319721698761 - trainLoss: 0.4270351231098175\n",
      "cnt: 0 - valLoss: 0.4353186786174774 - trainLoss: 0.4270324409008026\n",
      "cnt: 0 - valLoss: 0.43531760573387146 - trainLoss: 0.42702969908714294\n",
      "cnt: 0 - valLoss: 0.4353165328502655 - trainLoss: 0.4270269572734833\n",
      "cnt: 0 - valLoss: 0.4353155493736267 - trainLoss: 0.4270242154598236\n",
      "cnt: 0 - valLoss: 0.43531447649002075 - trainLoss: 0.42702150344848633\n",
      "cnt: 0 - valLoss: 0.4353134036064148 - trainLoss: 0.42701882123947144\n",
      "cnt: 0 - valLoss: 0.4353123903274536 - trainLoss: 0.42701607942581177\n",
      "cnt: 0 - valLoss: 0.43531131744384766 - trainLoss: 0.4270133376121521\n",
      "cnt: 0 - valLoss: 0.4353102743625641 - trainLoss: 0.42701059579849243\n",
      "cnt: 0 - valLoss: 0.43530920147895813 - trainLoss: 0.42700785398483276\n",
      "cnt: 0 - valLoss: 0.43530818819999695 - trainLoss: 0.42700520157814026\n",
      "cnt: 0 - valLoss: 0.435307115316391 - trainLoss: 0.4270024597644806\n",
      "cnt: 0 - valLoss: 0.4353060722351074 - trainLoss: 0.4269997179508209\n",
      "cnt: 0 - valLoss: 0.43530505895614624 - trainLoss: 0.42699697613716125\n",
      "cnt: 0 - valLoss: 0.4353039860725403 - trainLoss: 0.42699429392814636\n",
      "cnt: 0 - valLoss: 0.4353029429912567 - trainLoss: 0.4269915819168091\n",
      "cnt: 0 - valLoss: 0.43530187010765076 - trainLoss: 0.426988810300827\n",
      "cnt: 0 - valLoss: 0.4353008568286896 - trainLoss: 0.42698612809181213\n",
      "cnt: 0 - valLoss: 0.4352997839450836 - trainLoss: 0.42698338627815247\n",
      "cnt: 0 - valLoss: 0.43529871106147766 - trainLoss: 0.4269806444644928\n",
      "cnt: 0 - valLoss: 0.4352976679801941 - trainLoss: 0.4269779324531555\n",
      "cnt: 0 - valLoss: 0.4352966547012329 - trainLoss: 0.4269752502441406\n",
      "cnt: 0 - valLoss: 0.4352955222129822 - trainLoss: 0.42697250843048096\n",
      "cnt: 0 - valLoss: 0.4352945387363434 - trainLoss: 0.4269697666168213\n",
      "cnt: 0 - valLoss: 0.4352934658527374 - trainLoss: 0.4269670248031616\n",
      "cnt: 0 - valLoss: 0.43529245257377625 - trainLoss: 0.42696434259414673\n",
      "cnt: 0 - valLoss: 0.4352913796901703 - trainLoss: 0.42696160078048706\n",
      "cnt: 0 - valLoss: 0.4352903366088867 - trainLoss: 0.4269588887691498\n",
      "cnt: 0 - valLoss: 0.43528926372528076 - trainLoss: 0.4269561469554901\n",
      "cnt: 0 - valLoss: 0.4352882504463196 - trainLoss: 0.42695340514183044\n",
      "cnt: 0 - valLoss: 0.435287207365036 - trainLoss: 0.42695072293281555\n",
      "cnt: 0 - valLoss: 0.43528619408607483 - trainLoss: 0.42694801092147827\n",
      "cnt: 0 - valLoss: 0.4352850615978241 - trainLoss: 0.4269452691078186\n",
      "cnt: 0 - valLoss: 0.4352840483188629 - trainLoss: 0.4269425570964813\n",
      "cnt: 0 - valLoss: 0.43528300523757935 - trainLoss: 0.42693981528282166\n",
      "cnt: 0 - valLoss: 0.43528199195861816 - trainLoss: 0.426937073469162\n",
      "cnt: 0 - valLoss: 0.4352809190750122 - trainLoss: 0.4269343614578247\n",
      "cnt: 0 - valLoss: 0.43527987599372864 - trainLoss: 0.4269316792488098\n",
      "cnt: 0 - valLoss: 0.4352788031101227 - trainLoss: 0.42692893743515015\n",
      "cnt: 0 - valLoss: 0.4352777898311615 - trainLoss: 0.4269261956214905\n",
      "cnt: 0 - valLoss: 0.43527674674987793 - trainLoss: 0.4269234836101532\n",
      "cnt: 0 - valLoss: 0.435275673866272 - trainLoss: 0.42692071199417114\n",
      "cnt: 0 - valLoss: 0.4352746605873108 - trainLoss: 0.42691802978515625\n",
      "cnt: 0 - valLoss: 0.4352736175060272 - trainLoss: 0.4269152879714966\n",
      "cnt: 0 - valLoss: 0.43527257442474365 - trainLoss: 0.4269125759601593\n",
      "cnt: 0 - valLoss: 0.4352715313434601 - trainLoss: 0.42690983414649963\n",
      "cnt: 0 - valLoss: 0.4352704882621765 - trainLoss: 0.42690709233283997\n",
      "cnt: 0 - valLoss: 0.43526944518089294 - trainLoss: 0.4269044101238251\n",
      "cnt: 0 - valLoss: 0.435268372297287 - trainLoss: 0.4269016981124878\n",
      "cnt: 0 - valLoss: 0.4352673590183258 - trainLoss: 0.4268989861011505\n",
      "cnt: 0 - valLoss: 0.43526631593704224 - trainLoss: 0.42689624428749084\n",
      "cnt: 0 - valLoss: 0.43526527285575867 - trainLoss: 0.4268935024738312\n",
      "cnt: 0 - valLoss: 0.4352642297744751 - trainLoss: 0.4268907606601715\n",
      "cnt: 0 - valLoss: 0.43526318669319153 - trainLoss: 0.42688801884651184\n",
      "cnt: 0 - valLoss: 0.43526217341423035 - trainLoss: 0.42688530683517456\n",
      "cnt: 0 - valLoss: 0.435261070728302 - trainLoss: 0.42688262462615967\n",
      "cnt: 0 - valLoss: 0.4352600574493408 - trainLoss: 0.4268798828125\n",
      "cnt: 0 - valLoss: 0.43525901436805725 - trainLoss: 0.42687714099884033\n",
      "cnt: 0 - valLoss: 0.43525800108909607 - trainLoss: 0.42687445878982544\n",
      "cnt: 0 - valLoss: 0.4352569580078125 - trainLoss: 0.42687171697616577\n",
      "cnt: 0 - valLoss: 0.43525588512420654 - trainLoss: 0.4268689751625061\n",
      "cnt: 0 - valLoss: 0.43525490164756775 - trainLoss: 0.4268662631511688\n",
      "cnt: 0 - valLoss: 0.4352538287639618 - trainLoss: 0.42686352133750916\n",
      "cnt: 0 - valLoss: 0.4352527856826782 - trainLoss: 0.4268607795238495\n",
      "cnt: 0 - valLoss: 0.43525177240371704 - trainLoss: 0.4268580973148346\n",
      "cnt: 0 - valLoss: 0.4352506995201111 - trainLoss: 0.4268554151058197\n",
      "cnt: 0 - valLoss: 0.4352496862411499 - trainLoss: 0.42685261368751526\n",
      "cnt: 0 - valLoss: 0.43524864315986633 - trainLoss: 0.42684993147850037\n",
      "cnt: 0 - valLoss: 0.43524760007858276 - trainLoss: 0.4268471896648407\n",
      "cnt: 0 - valLoss: 0.4352465867996216 - trainLoss: 0.4268444776535034\n",
      "cnt: 0 - valLoss: 0.435245543718338 - trainLoss: 0.42684170603752136\n",
      "cnt: 0 - valLoss: 0.43524453043937683 - trainLoss: 0.4268389940261841\n",
      "cnt: 0 - valLoss: 0.4352434575557709 - trainLoss: 0.4268362820148468\n",
      "cnt: 0 - valLoss: 0.4352424740791321 - trainLoss: 0.4268335700035095\n",
      "cnt: 0 - valLoss: 0.4352414309978485 - trainLoss: 0.42683082818984985\n",
      "cnt: 0 - valLoss: 0.43524035811424255 - trainLoss: 0.4268280863761902\n",
      "cnt: 0 - valLoss: 0.43523934483528137 - trainLoss: 0.4268253445625305\n",
      "cnt: 0 - valLoss: 0.4352383017539978 - trainLoss: 0.4268226623535156\n",
      "cnt: 0 - valLoss: 0.4352372884750366 - trainLoss: 0.42681995034217834\n",
      "cnt: 0 - valLoss: 0.43523624539375305 - trainLoss: 0.4268172085285187\n",
      "cnt: 0 - valLoss: 0.43523523211479187 - trainLoss: 0.4268144965171814\n",
      "cnt: 0 - valLoss: 0.4352341890335083 - trainLoss: 0.42681175470352173\n",
      "cnt: 0 - valLoss: 0.4352331757545471 - trainLoss: 0.42680904269218445\n",
      "cnt: 0 - valLoss: 0.4352321922779083 - trainLoss: 0.42680636048316956\n",
      "cnt: 0 - valLoss: 0.43523114919662476 - trainLoss: 0.4268036186695099\n",
      "cnt: 0 - valLoss: 0.4352301359176636 - trainLoss: 0.4268008768558502\n",
      "cnt: 0 - valLoss: 0.43522909283638 - trainLoss: 0.42679813504219055\n",
      "cnt: 0 - valLoss: 0.4352280795574188 - trainLoss: 0.42679542303085327\n",
      "cnt: 0 - valLoss: 0.43522703647613525 - trainLoss: 0.4267927408218384\n",
      "cnt: 0 - valLoss: 0.4352260231971741 - trainLoss: 0.4267899990081787\n",
      "cnt: 0 - valLoss: 0.4352249801158905 - trainLoss: 0.42678725719451904\n",
      "cnt: 0 - valLoss: 0.4352239668369293 - trainLoss: 0.42678457498550415\n",
      "cnt: 0 - valLoss: 0.43522292375564575 - trainLoss: 0.4267817735671997\n",
      "cnt: 0 - valLoss: 0.43522191047668457 - trainLoss: 0.4267791211605072\n",
      "cnt: 0 - valLoss: 0.435220867395401 - trainLoss: 0.42677637934684753\n",
      "cnt: 0 - valLoss: 0.4352198541164398 - trainLoss: 0.42677363753318787\n",
      "cnt: 0 - valLoss: 0.43521881103515625 - trainLoss: 0.426770955324173\n",
      "cnt: 0 - valLoss: 0.43521785736083984 - trainLoss: 0.4267682433128357\n",
      "cnt: 0 - valLoss: 0.4352167844772339 - trainLoss: 0.42676547169685364\n",
      "cnt: 0 - valLoss: 0.4352157711982727 - trainLoss: 0.42676278948783875\n",
      "cnt: 0 - valLoss: 0.4352147877216339 - trainLoss: 0.4267600476741791\n",
      "cnt: 0 - valLoss: 0.43521374464035034 - trainLoss: 0.4267573058605194\n",
      "cnt: 0 - valLoss: 0.43521273136138916 - trainLoss: 0.42675456404685974\n",
      "cnt: 0 - valLoss: 0.4352116882801056 - trainLoss: 0.4267518222332001\n",
      "cnt: 0 - valLoss: 0.4352106750011444 - trainLoss: 0.42674916982650757\n",
      "cnt: 0 - valLoss: 0.43520963191986084 - trainLoss: 0.4267464280128479\n",
      "cnt: 0 - valLoss: 0.43520864844322205 - trainLoss: 0.42674368619918823\n",
      "cnt: 0 - valLoss: 0.4352075755596161 - trainLoss: 0.42674094438552856\n",
      "cnt: 0 - valLoss: 0.4352065622806549 - trainLoss: 0.42673826217651367\n",
      "cnt: 0 - valLoss: 0.4352056086063385 - trainLoss: 0.426735520362854\n",
      "cnt: 0 - valLoss: 0.4352045953273773 - trainLoss: 0.4267328083515167\n",
      "cnt: 0 - valLoss: 0.43520355224609375 - trainLoss: 0.42673006653785706\n",
      "cnt: 0 - valLoss: 0.4352025091648102 - trainLoss: 0.4267273247241974\n",
      "cnt: 0 - valLoss: 0.435201495885849 - trainLoss: 0.4267246127128601\n",
      "cnt: 0 - valLoss: 0.4352005124092102 - trainLoss: 0.4267219305038452\n",
      "cnt: 0 - valLoss: 0.4351995289325714 - trainLoss: 0.42671921849250793\n",
      "cnt: 0 - valLoss: 0.43519848585128784 - trainLoss: 0.42671650648117065\n",
      "cnt: 0 - valLoss: 0.43519750237464905 - trainLoss: 0.42671382427215576\n",
      "cnt: 0 - valLoss: 0.43519648909568787 - trainLoss: 0.4267111122608185\n",
      "cnt: 0 - valLoss: 0.4351955056190491 - trainLoss: 0.4267084300518036\n",
      "cnt: 0 - valLoss: 0.4351944625377655 - trainLoss: 0.4267056882381439\n",
      "cnt: 0 - valLoss: 0.4351934790611267 - trainLoss: 0.4267030358314514\n",
      "cnt: 0 - valLoss: 0.4351924955844879 - trainLoss: 0.42670029401779175\n",
      "cnt: 0 - valLoss: 0.43519145250320435 - trainLoss: 0.4266975522041321\n",
      "cnt: 0 - valLoss: 0.43519043922424316 - trainLoss: 0.4266948997974396\n",
      "cnt: 0 - valLoss: 0.43518945574760437 - trainLoss: 0.4266921579837799\n",
      "cnt: 0 - valLoss: 0.4351884722709656 - trainLoss: 0.4266894459724426\n",
      "cnt: 0 - valLoss: 0.435187429189682 - trainLoss: 0.42668670415878296\n",
      "cnt: 0 - valLoss: 0.4351864159107208 - trainLoss: 0.42668402194976807\n",
      "cnt: 0 - valLoss: 0.43518543243408203 - trainLoss: 0.4266813397407532\n",
      "cnt: 0 - valLoss: 0.43518444895744324 - trainLoss: 0.4266786277294159\n",
      "cnt: 0 - valLoss: 0.43518340587615967 - trainLoss: 0.4266758859157562\n",
      "cnt: 0 - valLoss: 0.4351823925971985 - trainLoss: 0.42667320370674133\n",
      "cnt: 0 - valLoss: 0.4351813793182373 - trainLoss: 0.42667049169540405\n",
      "cnt: 0 - valLoss: 0.4351803958415985 - trainLoss: 0.42666780948638916\n",
      "cnt: 0 - valLoss: 0.43517938256263733 - trainLoss: 0.4266650974750519\n",
      "cnt: 0 - valLoss: 0.43517833948135376 - trainLoss: 0.4266623556613922\n",
      "cnt: 0 - valLoss: 0.43517735600471497 - trainLoss: 0.4266596734523773\n",
      "cnt: 0 - valLoss: 0.43517637252807617 - trainLoss: 0.42665696144104004\n",
      "cnt: 0 - valLoss: 0.435175359249115 - trainLoss: 0.42665421962738037\n",
      "cnt: 0 - valLoss: 0.4351743757724762 - trainLoss: 0.4266515374183655\n",
      "cnt: 0 - valLoss: 0.4351733922958374 - trainLoss: 0.4266488254070282\n",
      "cnt: 0 - valLoss: 0.43517234921455383 - trainLoss: 0.4266461133956909\n",
      "cnt: 0 - valLoss: 0.43517130613327026 - trainLoss: 0.4266434609889984\n",
      "cnt: 0 - valLoss: 0.43517038226127625 - trainLoss: 0.42664071917533875\n",
      "cnt: 0 - valLoss: 0.4351693391799927 - trainLoss: 0.42663800716400146\n",
      "cnt: 0 - valLoss: 0.4351683259010315 - trainLoss: 0.4266353249549866\n",
      "cnt: 0 - valLoss: 0.4351673424243927 - trainLoss: 0.4266326129436493\n",
      "cnt: 0 - valLoss: 0.43516629934310913 - trainLoss: 0.4266298711299896\n",
      "cnt: 0 - valLoss: 0.43516531586647034 - trainLoss: 0.42662718892097473\n",
      "cnt: 0 - valLoss: 0.43516433238983154 - trainLoss: 0.42662447690963745\n",
      "cnt: 0 - valLoss: 0.43516331911087036 - trainLoss: 0.42662179470062256\n",
      "cnt: 0 - valLoss: 0.4351623058319092 - trainLoss: 0.4266190528869629\n",
      "cnt: 0 - valLoss: 0.435161292552948 - trainLoss: 0.4266163408756256\n",
      "cnt: 0 - valLoss: 0.43516024947166443 - trainLoss: 0.4266136586666107\n",
      "cnt: 0 - valLoss: 0.4351593255996704 - trainLoss: 0.42661091685295105\n",
      "cnt: 0 - valLoss: 0.43515828251838684 - trainLoss: 0.42660823464393616\n",
      "cnt: 0 - valLoss: 0.43515726923942566 - trainLoss: 0.4266055226325989\n",
      "cnt: 0 - valLoss: 0.43515631556510925 - trainLoss: 0.4266027808189392\n",
      "cnt: 0 - valLoss: 0.43515530228614807 - trainLoss: 0.4266000986099243\n",
      "cnt: 0 - valLoss: 0.4351542592048645 - trainLoss: 0.42659738659858704\n",
      "cnt: 0 - valLoss: 0.4351532757282257 - trainLoss: 0.42659470438957214\n",
      "cnt: 0 - valLoss: 0.43515223264694214 - trainLoss: 0.42659199237823486\n",
      "cnt: 0 - valLoss: 0.43515124917030334 - trainLoss: 0.4265892505645752\n",
      "cnt: 0 - valLoss: 0.43515023589134216 - trainLoss: 0.4265865683555603\n",
      "cnt: 0 - valLoss: 0.43514925241470337 - trainLoss: 0.426583856344223\n",
      "cnt: 0 - valLoss: 0.4351482689380646 - trainLoss: 0.42658111453056335\n",
      "cnt: 0 - valLoss: 0.435147225856781 - trainLoss: 0.42657846212387085\n",
      "cnt: 0 - valLoss: 0.4351462423801422 - trainLoss: 0.4265757203102112\n",
      "cnt: 0 - valLoss: 0.4351452589035034 - trainLoss: 0.4265730381011963\n",
      "cnt: 0 - valLoss: 0.43514424562454224 - trainLoss: 0.4265703558921814\n",
      "cnt: 0 - valLoss: 0.43514320254325867 - trainLoss: 0.42656761407852173\n",
      "cnt: 0 - valLoss: 0.4351421594619751 - trainLoss: 0.4265649616718292\n",
      "cnt: 0 - valLoss: 0.4351411759853363 - trainLoss: 0.42656221985816956\n",
      "cnt: 0 - valLoss: 0.4351401627063751 - trainLoss: 0.4265594780445099\n",
      "cnt: 0 - valLoss: 0.43513917922973633 - trainLoss: 0.4265568256378174\n",
      "cnt: 0 - valLoss: 0.43513813614845276 - trainLoss: 0.4265540838241577\n",
      "cnt: 0 - valLoss: 0.4351371228694916 - trainLoss: 0.42655134201049805\n",
      "cnt: 0 - valLoss: 0.435136079788208 - trainLoss: 0.42654862999916077\n",
      "cnt: 0 - valLoss: 0.4351350665092468 - trainLoss: 0.4265459477901459\n",
      "cnt: 0 - valLoss: 0.43513402342796326 - trainLoss: 0.42654329538345337\n",
      "cnt: 0 - valLoss: 0.43513303995132446 - trainLoss: 0.4265405535697937\n",
      "cnt: 0 - valLoss: 0.43513205647468567 - trainLoss: 0.42653781175613403\n",
      "cnt: 0 - valLoss: 0.4351310431957245 - trainLoss: 0.42653515934944153\n",
      "cnt: 0 - valLoss: 0.4351300001144409 - trainLoss: 0.42653241753578186\n",
      "cnt: 0 - valLoss: 0.43512898683547974 - trainLoss: 0.4265296757221222\n",
      "cnt: 0 - valLoss: 0.43512797355651855 - trainLoss: 0.4265270233154297\n",
      "cnt: 0 - valLoss: 0.4351269006729126 - trainLoss: 0.42652428150177\n",
      "cnt: 0 - valLoss: 0.4351259171962738 - trainLoss: 0.4265215992927551\n",
      "cnt: 0 - valLoss: 0.4351249039173126 - trainLoss: 0.42651885747909546\n",
      "cnt: 0 - valLoss: 0.43512392044067383 - trainLoss: 0.4265161454677582\n",
      "cnt: 0 - valLoss: 0.43512287735939026 - trainLoss: 0.4265134632587433\n",
      "cnt: 0 - valLoss: 0.4351218640804291 - trainLoss: 0.426510751247406\n",
      "cnt: 0 - valLoss: 0.4351208209991455 - trainLoss: 0.4265080690383911\n",
      "cnt: 0 - valLoss: 0.4351198077201843 - trainLoss: 0.42650532722473145\n",
      "cnt: 0 - valLoss: 0.4351188540458679 - trainLoss: 0.42650261521339417\n",
      "cnt: 0 - valLoss: 0.43511778116226196 - trainLoss: 0.4264999330043793\n",
      "cnt: 0 - valLoss: 0.43511679768562317 - trainLoss: 0.4264971911907196\n",
      "cnt: 0 - valLoss: 0.435115784406662 - trainLoss: 0.4264945089817047\n",
      "cnt: 0 - valLoss: 0.4351147413253784 - trainLoss: 0.42649176716804504\n",
      "cnt: 0 - valLoss: 0.43511372804641724 - trainLoss: 0.42648911476135254\n",
      "cnt: 0 - valLoss: 0.43511271476745605 - trainLoss: 0.42648637294769287\n",
      "cnt: 0 - valLoss: 0.4351117014884949 - trainLoss: 0.4264836609363556\n",
      "cnt: 0 - valLoss: 0.4351106584072113 - trainLoss: 0.4264809787273407\n",
      "cnt: 0 - valLoss: 0.4351096749305725 - trainLoss: 0.42647823691368103\n",
      "cnt: 0 - valLoss: 0.43510866165161133 - trainLoss: 0.42647552490234375\n",
      "cnt: 0 - valLoss: 0.43510761857032776 - trainLoss: 0.42647284269332886\n",
      "cnt: 0 - valLoss: 0.4351066052913666 - trainLoss: 0.4264701008796692\n",
      "cnt: 0 - valLoss: 0.4351056218147278 - trainLoss: 0.4264673888683319\n",
      "cnt: 0 - valLoss: 0.4351045787334442 - trainLoss: 0.426464706659317\n",
      "cnt: 0 - valLoss: 0.4351035952568054 - trainLoss: 0.4264620244503021\n",
      "cnt: 0 - valLoss: 0.43510258197784424 - trainLoss: 0.42645931243896484\n",
      "cnt: 0 - valLoss: 0.43510159850120544 - trainLoss: 0.4264565706253052\n",
      "cnt: 0 - valLoss: 0.4351005554199219 - trainLoss: 0.4264538288116455\n",
      "cnt: 0 - valLoss: 0.4350995719432831 - trainLoss: 0.426451176404953\n",
      "cnt: 0 - valLoss: 0.4350984990596771 - trainLoss: 0.4264484941959381\n",
      "cnt: 0 - valLoss: 0.43509751558303833 - trainLoss: 0.42644575238227844\n",
      "cnt: 0 - valLoss: 0.43509647250175476 - trainLoss: 0.42644304037094116\n",
      "cnt: 0 - valLoss: 0.4350954592227936 - trainLoss: 0.42644035816192627\n",
      "cnt: 0 - valLoss: 0.4350944757461548 - trainLoss: 0.4264376163482666\n",
      "cnt: 0 - valLoss: 0.4350934326648712 - trainLoss: 0.4264349043369293\n",
      "cnt: 0 - valLoss: 0.4350924491882324 - trainLoss: 0.42643219232559204\n",
      "cnt: 0 - valLoss: 0.43509143590927124 - trainLoss: 0.42642951011657715\n",
      "cnt: 0 - valLoss: 0.43509039282798767 - trainLoss: 0.42642679810523987\n",
      "cnt: 0 - valLoss: 0.4350893795490265 - trainLoss: 0.4264240562915802\n",
      "cnt: 0 - valLoss: 0.4350883364677429 - trainLoss: 0.4264214038848877\n",
      "cnt: 0 - valLoss: 0.4350873827934265 - trainLoss: 0.426418662071228\n",
      "cnt: 0 - valLoss: 0.43508633971214294 - trainLoss: 0.4264160096645355\n",
      "cnt: 0 - valLoss: 0.43508532643318176 - trainLoss: 0.42641326785087585\n",
      "cnt: 0 - valLoss: 0.4350843131542206 - trainLoss: 0.4264105260372162\n",
      "cnt: 0 - valLoss: 0.435083270072937 - trainLoss: 0.4264078140258789\n",
      "cnt: 0 - valLoss: 0.435082346200943 - trainLoss: 0.426405131816864\n",
      "cnt: 0 - valLoss: 0.4350813031196594 - trainLoss: 0.42640241980552673\n",
      "cnt: 0 - valLoss: 0.43508028984069824 - trainLoss: 0.42639973759651184\n",
      "cnt: 0 - valLoss: 0.4350792467594147 - trainLoss: 0.4263969957828522\n",
      "cnt: 0 - valLoss: 0.43507832288742065 - trainLoss: 0.4263942837715149\n",
      "cnt: 0 - valLoss: 0.4350772798061371 - trainLoss: 0.4263916015625\n",
      "cnt: 0 - valLoss: 0.4350762665271759 - trainLoss: 0.4263889193534851\n",
      "cnt: 0 - valLoss: 0.43507522344589233 - trainLoss: 0.42638617753982544\n",
      "cnt: 0 - valLoss: 0.43507421016693115 - trainLoss: 0.42638343572616577\n",
      "cnt: 0 - valLoss: 0.43507319688796997 - trainLoss: 0.4263807237148285\n",
      "cnt: 0 - valLoss: 0.4350721836090088 - trainLoss: 0.4263780415058136\n",
      "cnt: 0 - valLoss: 0.4350711405277252 - trainLoss: 0.4263753294944763\n",
      "cnt: 0 - valLoss: 0.4350701570510864 - trainLoss: 0.4263726472854614\n",
      "cnt: 0 - valLoss: 0.43506914377212524 - trainLoss: 0.42636993527412415\n",
      "cnt: 0 - valLoss: 0.4350681006908417 - trainLoss: 0.4263671934604645\n",
      "cnt: 0 - valLoss: 0.4350671172142029 - trainLoss: 0.4263645112514496\n",
      "cnt: 0 - valLoss: 0.4350661039352417 - trainLoss: 0.4263617992401123\n",
      "cnt: 0 - valLoss: 0.4350651204586029 - trainLoss: 0.42635905742645264\n",
      "cnt: 0 - valLoss: 0.43506407737731934 - trainLoss: 0.42635637521743774\n",
      "cnt: 0 - valLoss: 0.43506309390068054 - trainLoss: 0.42635369300842285\n",
      "cnt: 0 - valLoss: 0.435062050819397 - trainLoss: 0.4263509511947632\n",
      "cnt: 0 - valLoss: 0.4350610375404358 - trainLoss: 0.4263482391834259\n",
      "cnt: 0 - valLoss: 0.435060054063797 - trainLoss: 0.426345556974411\n",
      "cnt: 0 - valLoss: 0.4350590109825134 - trainLoss: 0.42634281516075134\n",
      "cnt: 0 - valLoss: 0.43505802750587463 - trainLoss: 0.42634010314941406\n",
      "cnt: 0 - valLoss: 0.43505701422691345 - trainLoss: 0.42633742094039917\n",
      "cnt: 0 - valLoss: 0.43505603075027466 - trainLoss: 0.4263347089290619\n",
      "cnt: 0 - valLoss: 0.4350549876689911 - trainLoss: 0.4263319671154022\n",
      "cnt: 0 - valLoss: 0.4350540041923523 - trainLoss: 0.42632928490638733\n",
      "cnt: 0 - valLoss: 0.4350529909133911 - trainLoss: 0.42632654309272766\n",
      "cnt: 0 - valLoss: 0.43505194783210754 - trainLoss: 0.42632386088371277\n",
      "cnt: 0 - valLoss: 0.43505096435546875 - trainLoss: 0.4263211488723755\n",
      "cnt: 0 - valLoss: 0.43504998087882996 - trainLoss: 0.4263184666633606\n",
      "cnt: 0 - valLoss: 0.435048907995224 - trainLoss: 0.4263157546520233\n",
      "cnt: 0 - valLoss: 0.43504786491394043 - trainLoss: 0.42631301283836365\n",
      "cnt: 0 - valLoss: 0.4350469410419464 - trainLoss: 0.426310271024704\n",
      "cnt: 0 - valLoss: 0.43504589796066284 - trainLoss: 0.4263076186180115\n",
      "cnt: 0 - valLoss: 0.43504488468170166 - trainLoss: 0.4263049364089966\n",
      "cnt: 0 - valLoss: 0.43504390120506287 - trainLoss: 0.4263021945953369\n",
      "cnt: 0 - valLoss: 0.4350428581237793 - trainLoss: 0.42629948258399963\n",
      "cnt: 0 - valLoss: 0.4350418746471405 - trainLoss: 0.42629680037498474\n",
      "cnt: 0 - valLoss: 0.4350408911705017 - trainLoss: 0.42629408836364746\n",
      "cnt: 0 - valLoss: 0.4350398778915405 - trainLoss: 0.4262913465499878\n",
      "cnt: 0 - valLoss: 0.43503886461257935 - trainLoss: 0.4262886643409729\n",
      "cnt: 0 - valLoss: 0.43503785133361816 - trainLoss: 0.4262859523296356\n",
      "cnt: 0 - valLoss: 0.43503686785697937 - trainLoss: 0.42628324031829834\n",
      "cnt: 0 - valLoss: 0.4350358843803406 - trainLoss: 0.42628052830696106\n",
      "cnt: 0 - valLoss: 0.435034841299057 - trainLoss: 0.42627784609794617\n",
      "cnt: 0 - valLoss: 0.4350338578224182 - trainLoss: 0.4262751042842865\n",
      "cnt: 0 - valLoss: 0.4350328743457794 - trainLoss: 0.4262723922729492\n",
      "cnt: 0 - valLoss: 0.43503186106681824 - trainLoss: 0.4262697100639343\n",
      "cnt: 0 - valLoss: 0.43503084778785706 - trainLoss: 0.4262670576572418\n",
      "cnt: 0 - valLoss: 0.4350298345088959 - trainLoss: 0.4262642562389374\n",
      "cnt: 0 - valLoss: 0.4350288510322571 - trainLoss: 0.4262616038322449\n",
      "cnt: 0 - valLoss: 0.4350278675556183 - trainLoss: 0.4262588620185852\n",
      "cnt: 0 - valLoss: 0.4350268840789795 - trainLoss: 0.4262561798095703\n",
      "cnt: 0 - valLoss: 0.4350258409976959 - trainLoss: 0.42625343799591064\n",
      "cnt: 0 - valLoss: 0.43502485752105713 - trainLoss: 0.42625075578689575\n",
      "cnt: 0 - valLoss: 0.43502387404441833 - trainLoss: 0.42624804377555847\n",
      "cnt: 0 - valLoss: 0.43502289056777954 - trainLoss: 0.4262453019618988\n",
      "cnt: 0 - valLoss: 0.43502184748649597 - trainLoss: 0.4262426197528839\n",
      "cnt: 0 - valLoss: 0.4350208342075348 - trainLoss: 0.42623990774154663\n",
      "cnt: 0 - valLoss: 0.435019850730896 - trainLoss: 0.42623722553253174\n",
      "cnt: 0 - valLoss: 0.4350188672542572 - trainLoss: 0.42623451352119446\n",
      "cnt: 0 - valLoss: 0.43501782417297363 - trainLoss: 0.4262317717075348\n",
      "cnt: 0 - valLoss: 0.43501684069633484 - trainLoss: 0.4262290894985199\n",
      "cnt: 0 - valLoss: 0.43501582741737366 - trainLoss: 0.4262263774871826\n",
      "cnt: 0 - valLoss: 0.43501487374305725 - trainLoss: 0.42622363567352295\n",
      "cnt: 0 - valLoss: 0.43501386046409607 - trainLoss: 0.42622098326683044\n",
      "cnt: 0 - valLoss: 0.4350128471851349 - trainLoss: 0.4262182414531708\n",
      "cnt: 0 - valLoss: 0.4350118637084961 - trainLoss: 0.42621558904647827\n",
      "cnt: 0 - valLoss: 0.4350108504295349 - trainLoss: 0.4262128174304962\n",
      "cnt: 0 - valLoss: 0.4350098669528961 - trainLoss: 0.4262101352214813\n",
      "cnt: 0 - valLoss: 0.43500882387161255 - trainLoss: 0.4262074828147888\n",
      "cnt: 0 - valLoss: 0.43500784039497375 - trainLoss: 0.42620474100112915\n",
      "cnt: 0 - valLoss: 0.4350068271160126 - trainLoss: 0.4262019991874695\n",
      "cnt: 0 - valLoss: 0.43500587344169617 - trainLoss: 0.4261992871761322\n",
      "cnt: 0 - valLoss: 0.4350048303604126 - trainLoss: 0.4261966049671173\n",
      "cnt: 0 - valLoss: 0.4350038170814514 - trainLoss: 0.42619389295578003\n",
      "cnt: 0 - valLoss: 0.4350028336048126 - trainLoss: 0.42619121074676514\n",
      "cnt: 0 - valLoss: 0.43500185012817383 - trainLoss: 0.42618846893310547\n",
      "cnt: 0 - valLoss: 0.43500080704689026 - trainLoss: 0.4261857569217682\n",
      "cnt: 0 - valLoss: 0.43499988317489624 - trainLoss: 0.4261830747127533\n",
      "cnt: 0 - valLoss: 0.43499884009361267 - trainLoss: 0.426180362701416\n",
      "cnt: 0 - valLoss: 0.4349978566169739 - trainLoss: 0.4261776804924011\n",
      "cnt: 0 - valLoss: 0.4349968433380127 - trainLoss: 0.42617493867874146\n",
      "cnt: 0 - valLoss: 0.4349958300590515 - trainLoss: 0.42617225646972656\n",
      "cnt: 0 - valLoss: 0.43499481678009033 - trainLoss: 0.4261695444583893\n",
      "cnt: 0 - valLoss: 0.43499383330345154 - trainLoss: 0.4261668622493744\n",
      "cnt: 0 - valLoss: 0.43499284982681274 - trainLoss: 0.4261641502380371\n",
      "cnt: 0 - valLoss: 0.4349918067455292 - trainLoss: 0.42616140842437744\n",
      "cnt: 0 - valLoss: 0.4349908232688904 - trainLoss: 0.42615872621536255\n",
      "cnt: 0 - valLoss: 0.4349898397922516 - trainLoss: 0.42615601420402527\n",
      "cnt: 0 - valLoss: 0.4349888265132904 - trainLoss: 0.4261532723903656\n",
      "cnt: 0 - valLoss: 0.4349878430366516 - trainLoss: 0.4261506199836731\n",
      "cnt: 0 - valLoss: 0.43498679995536804 - trainLoss: 0.4261478781700134\n",
      "cnt: 0 - valLoss: 0.43498581647872925 - trainLoss: 0.4261452257633209\n",
      "cnt: 0 - valLoss: 0.43498483300209045 - trainLoss: 0.42614245414733887\n",
      "cnt: 0 - valLoss: 0.43498384952545166 - trainLoss: 0.4261397421360016\n",
      "cnt: 0 - valLoss: 0.4349828064441681 - trainLoss: 0.4261370599269867\n",
      "cnt: 0 - valLoss: 0.4349818229675293 - trainLoss: 0.4261343777179718\n",
      "cnt: 0 - valLoss: 0.4349808394908905 - trainLoss: 0.42613163590431213\n",
      "cnt: 0 - valLoss: 0.4349798262119293 - trainLoss: 0.42612898349761963\n",
      "cnt: 0 - valLoss: 0.4349788725376129 - trainLoss: 0.42612624168395996\n",
      "cnt: 0 - valLoss: 0.43497782945632935 - trainLoss: 0.4261235296726227\n",
      "cnt: 0 - valLoss: 0.43497684597969055 - trainLoss: 0.426120787858963\n",
      "cnt: 0 - valLoss: 0.43497586250305176 - trainLoss: 0.4261181056499481\n",
      "cnt: 0 - valLoss: 0.4349748492240906 - trainLoss: 0.4261154532432556\n",
      "cnt: 0 - valLoss: 0.4349738657474518 - trainLoss: 0.42611271142959595\n",
      "cnt: 0 - valLoss: 0.434972882270813 - trainLoss: 0.4261099696159363\n",
      "cnt: 0 - valLoss: 0.4349718987941742 - trainLoss: 0.426107257604599\n",
      "cnt: 0 - valLoss: 0.434970885515213 - trainLoss: 0.4261045753955841\n",
      "cnt: 0 - valLoss: 0.43496987223625183 - trainLoss: 0.42610183358192444\n",
      "cnt: 0 - valLoss: 0.43496888875961304 - trainLoss: 0.42609915137290955\n",
      "cnt: 0 - valLoss: 0.43496790528297424 - trainLoss: 0.42609643936157227\n",
      "cnt: 0 - valLoss: 0.4349668622016907 - trainLoss: 0.4260937571525574\n",
      "cnt: 0 - valLoss: 0.4349658787250519 - trainLoss: 0.4260910451412201\n",
      "cnt: 0 - valLoss: 0.4349648952484131 - trainLoss: 0.4260883033275604\n",
      "cnt: 0 - valLoss: 0.4349639117717743 - trainLoss: 0.42608562111854553\n",
      "cnt: 0 - valLoss: 0.4349629282951355 - trainLoss: 0.42608290910720825\n",
      "cnt: 0 - valLoss: 0.43496188521385193 - trainLoss: 0.4260801672935486\n",
      "cnt: 0 - valLoss: 0.43496090173721313 - trainLoss: 0.4260774850845337\n",
      "cnt: 0 - valLoss: 0.43495991826057434 - trainLoss: 0.4260747730731964\n",
      "cnt: 0 - valLoss: 0.43495893478393555 - trainLoss: 0.4260720908641815\n",
      "cnt: 0 - valLoss: 0.43495792150497437 - trainLoss: 0.42606937885284424\n",
      "cnt: 0 - valLoss: 0.43495693802833557 - trainLoss: 0.42606663703918457\n",
      "cnt: 0 - valLoss: 0.43495598435401917 - trainLoss: 0.42606398463249207\n",
      "cnt: 0 - valLoss: 0.4349549412727356 - trainLoss: 0.4260612726211548\n",
      "cnt: 0 - valLoss: 0.4349539279937744 - trainLoss: 0.4260585606098175\n",
      "cnt: 0 - valLoss: 0.4349529445171356 - trainLoss: 0.42605581879615784\n",
      "cnt: 0 - valLoss: 0.4349519610404968 - trainLoss: 0.42605313658714294\n",
      "cnt: 0 - valLoss: 0.43495097756385803 - trainLoss: 0.42605042457580566\n",
      "cnt: 0 - valLoss: 0.43494999408721924 - trainLoss: 0.42604774236679077\n",
      "cnt: 0 - valLoss: 0.43494895100593567 - trainLoss: 0.4260450005531311\n",
      "cnt: 0 - valLoss: 0.43494799733161926 - trainLoss: 0.4260422885417938\n",
      "cnt: 0 - valLoss: 0.4349469840526581 - trainLoss: 0.42603960633277893\n",
      "cnt: 0 - valLoss: 0.4349460005760193 - trainLoss: 0.42603686451911926\n",
      "cnt: 0 - valLoss: 0.4349450170993805 - trainLoss: 0.426034152507782\n",
      "cnt: 0 - valLoss: 0.4349439740180969 - trainLoss: 0.4260314702987671\n",
      "cnt: 0 - valLoss: 0.4349430501461029 - trainLoss: 0.4260287284851074\n",
      "cnt: 0 - valLoss: 0.43494200706481934 - trainLoss: 0.42602604627609253\n",
      "cnt: 0 - valLoss: 0.43494105339050293 - trainLoss: 0.42602333426475525\n",
      "cnt: 0 - valLoss: 0.43494004011154175 - trainLoss: 0.42602065205574036\n",
      "cnt: 0 - valLoss: 0.4349389970302582 - trainLoss: 0.4260179400444031\n",
      "cnt: 0 - valLoss: 0.43493807315826416 - trainLoss: 0.4260151982307434\n",
      "cnt: 0 - valLoss: 0.4349370300769806 - trainLoss: 0.4260125458240509\n",
      "cnt: 0 - valLoss: 0.4349360466003418 - trainLoss: 0.42600980401039124\n",
      "cnt: 0 - valLoss: 0.434935063123703 - trainLoss: 0.42600706219673157\n",
      "cnt: 0 - valLoss: 0.4349340796470642 - trainLoss: 0.42600440979003906\n",
      "cnt: 0 - valLoss: 0.434933066368103 - trainLoss: 0.4260016679763794\n",
      "cnt: 0 - valLoss: 0.43493205308914185 - trainLoss: 0.4259989857673645\n",
      "cnt: 0 - valLoss: 0.43493106961250305 - trainLoss: 0.42599624395370483\n",
      "cnt: 0 - valLoss: 0.43493008613586426 - trainLoss: 0.42599353194236755\n",
      "cnt: 0 - valLoss: 0.43492916226387024 - trainLoss: 0.42599084973335266\n",
      "cnt: 0 - valLoss: 0.43492811918258667 - trainLoss: 0.42598816752433777\n",
      "cnt: 0 - valLoss: 0.4349271059036255 - trainLoss: 0.4259854257106781\n",
      "cnt: 0 - valLoss: 0.4349261522293091 - trainLoss: 0.4259827136993408\n",
      "cnt: 0 - valLoss: 0.4349251091480255 - trainLoss: 0.4259800314903259\n",
      "cnt: 0 - valLoss: 0.4349241256713867 - trainLoss: 0.42597731947898865\n",
      "cnt: 0 - valLoss: 0.4349231421947479 - trainLoss: 0.425974577665329\n",
      "cnt: 0 - valLoss: 0.43492212891578674 - trainLoss: 0.4259718954563141\n",
      "cnt: 0 - valLoss: 0.43492114543914795 - trainLoss: 0.4259691834449768\n",
      "cnt: 0 - valLoss: 0.43492016196250916 - trainLoss: 0.4259665012359619\n",
      "cnt: 0 - valLoss: 0.43491917848587036 - trainLoss: 0.42596375942230225\n",
      "cnt: 0 - valLoss: 0.4349181652069092 - trainLoss: 0.42596104741096497\n",
      "cnt: 0 - valLoss: 0.4349171817302704 - trainLoss: 0.4259583652019501\n",
      "cnt: 0 - valLoss: 0.4349161982536316 - trainLoss: 0.4259556233882904\n",
      "cnt: 0 - valLoss: 0.4349151849746704 - trainLoss: 0.4259529709815979\n",
      "cnt: 0 - valLoss: 0.43491414189338684 - trainLoss: 0.42595022916793823\n",
      "cnt: 0 - valLoss: 0.4349132180213928 - trainLoss: 0.42594754695892334\n",
      "cnt: 0 - valLoss: 0.43491223454475403 - trainLoss: 0.42594483494758606\n",
      "cnt: 0 - valLoss: 0.43491122126579285 - trainLoss: 0.4259420931339264\n",
      "cnt: 0 - valLoss: 0.43491020798683167 - trainLoss: 0.4259393513202667\n",
      "cnt: 0 - valLoss: 0.43490925431251526 - trainLoss: 0.4259366989135742\n",
      "cnt: 0 - valLoss: 0.4349082410335541 - trainLoss: 0.4259340167045593\n",
      "cnt: 0 - valLoss: 0.4349072575569153 - trainLoss: 0.42593127489089966\n",
      "cnt: 0 - valLoss: 0.4349062740802765 - trainLoss: 0.4259285628795624\n",
      "cnt: 0 - valLoss: 0.4349052309989929 - trainLoss: 0.4259258210659027\n",
      "cnt: 0 - valLoss: 0.4349042475223541 - trainLoss: 0.4259231686592102\n",
      "cnt: 0 - valLoss: 0.43490326404571533 - trainLoss: 0.42592042684555054\n",
      "cnt: 0 - valLoss: 0.4349023103713989 - trainLoss: 0.42591774463653564\n",
      "cnt: 0 - valLoss: 0.43490129709243774 - trainLoss: 0.42591506242752075\n",
      "cnt: 0 - valLoss: 0.4349002540111542 - trainLoss: 0.4259123206138611\n",
      "cnt: 0 - valLoss: 0.43489933013916016 - trainLoss: 0.4259096086025238\n",
      "cnt: 0 - valLoss: 0.4348982870578766 - trainLoss: 0.4259069263935089\n",
      "cnt: 0 - valLoss: 0.4348973035812378 - trainLoss: 0.42590421438217163\n",
      "cnt: 0 - valLoss: 0.434896320104599 - trainLoss: 0.42590153217315674\n",
      "cnt: 0 - valLoss: 0.4348953366279602 - trainLoss: 0.42589879035949707\n",
      "cnt: 0 - valLoss: 0.4348943531513214 - trainLoss: 0.4258960783481598\n",
      "cnt: 0 - valLoss: 0.43489331007003784 - trainLoss: 0.4258933961391449\n",
      "cnt: 0 - valLoss: 0.4348923861980438 - trainLoss: 0.42589065432548523\n",
      "cnt: 0 - valLoss: 0.43489140272140503 - trainLoss: 0.42588794231414795\n",
      "cnt: 0 - valLoss: 0.43489038944244385 - trainLoss: 0.42588526010513306\n",
      "cnt: 0 - valLoss: 0.43488937616348267 - trainLoss: 0.4258825182914734\n",
      "cnt: 0 - valLoss: 0.43488839268684387 - trainLoss: 0.4258798360824585\n",
      "cnt: 0 - valLoss: 0.4348874092102051 - trainLoss: 0.4258771240711212\n",
      "cnt: 0 - valLoss: 0.4348864257335663 - trainLoss: 0.42587438225746155\n",
      "cnt: 0 - valLoss: 0.4348853826522827 - trainLoss: 0.42587172985076904\n",
      "cnt: 0 - valLoss: 0.4348844587802887 - trainLoss: 0.4258689880371094\n",
      "cnt: 0 - valLoss: 0.4348834753036499 - trainLoss: 0.4258663058280945\n",
      "cnt: 0 - valLoss: 0.43488243222236633 - trainLoss: 0.4258635938167572\n",
      "cnt: 0 - valLoss: 0.43488144874572754 - trainLoss: 0.42586085200309753\n",
      "cnt: 0 - valLoss: 0.43488046526908875 - trainLoss: 0.42585816979408264\n",
      "cnt: 0 - valLoss: 0.43487948179244995 - trainLoss: 0.42585545778274536\n",
      "cnt: 0 - valLoss: 0.43487849831581116 - trainLoss: 0.4258527159690857\n",
      "cnt: 0 - valLoss: 0.43487751483917236 - trainLoss: 0.4258500635623932\n",
      "cnt: 0 - valLoss: 0.43487653136253357 - trainLoss: 0.4258473217487335\n",
      "cnt: 0 - valLoss: 0.43487548828125 - trainLoss: 0.42584460973739624\n",
      "cnt: 0 - valLoss: 0.4348745048046112 - trainLoss: 0.42584195733070374\n",
      "cnt: 0 - valLoss: 0.4348735213279724 - trainLoss: 0.42583921551704407\n",
      "cnt: 0 - valLoss: 0.4348725378513336 - trainLoss: 0.4258365035057068\n",
      "cnt: 0 - valLoss: 0.4348715543746948 - trainLoss: 0.4258338212966919\n",
      "cnt: 0 - valLoss: 0.43487057089805603 - trainLoss: 0.4258311092853546\n",
      "cnt: 0 - valLoss: 0.43486958742141724 - trainLoss: 0.42582836747169495\n",
      "cnt: 0 - valLoss: 0.43486854434013367 - trainLoss: 0.42582568526268005\n",
      "cnt: 0 - valLoss: 0.43486759066581726 - trainLoss: 0.4258229434490204\n",
      "cnt: 0 - valLoss: 0.4348665773868561 - trainLoss: 0.4258202910423279\n",
      "cnt: 0 - valLoss: 0.4348656237125397 - trainLoss: 0.4258175492286682\n",
      "cnt: 0 - valLoss: 0.4348646104335785 - trainLoss: 0.42581483721733093\n",
      "cnt: 0 - valLoss: 0.4348636269569397 - trainLoss: 0.42581215500831604\n",
      "cnt: 0 - valLoss: 0.4348626434803009 - trainLoss: 0.42580941319465637\n",
      "cnt: 0 - valLoss: 0.43486160039901733 - trainLoss: 0.42580676078796387\n",
      "cnt: 0 - valLoss: 0.43486061692237854 - trainLoss: 0.4258040189743042\n",
      "cnt: 0 - valLoss: 0.43485963344573975 - trainLoss: 0.42580127716064453\n",
      "cnt: 0 - valLoss: 0.4348585903644562 - trainLoss: 0.42579859495162964\n",
      "cnt: 0 - valLoss: 0.43485766649246216 - trainLoss: 0.42579588294029236\n",
      "cnt: 0 - valLoss: 0.4348566234111786 - trainLoss: 0.42579320073127747\n",
      "cnt: 0 - valLoss: 0.4348556399345398 - trainLoss: 0.4257904887199402\n",
      "cnt: 0 - valLoss: 0.434854656457901 - trainLoss: 0.4257877469062805\n",
      "cnt: 0 - valLoss: 0.4348536431789398 - trainLoss: 0.425785094499588\n",
      "cnt: 0 - valLoss: 0.4348526895046234 - trainLoss: 0.42578235268592834\n",
      "cnt: 0 - valLoss: 0.4348517060279846 - trainLoss: 0.4257796108722687\n",
      "cnt: 0 - valLoss: 0.4348507225513458 - trainLoss: 0.4257769286632538\n",
      "cnt: 0 - valLoss: 0.43484967947006226 - trainLoss: 0.4257742166519165\n",
      "cnt: 0 - valLoss: 0.43484869599342346 - trainLoss: 0.4257715344429016\n",
      "cnt: 0 - valLoss: 0.43484771251678467 - trainLoss: 0.4257688522338867\n",
      "cnt: 0 - valLoss: 0.43484675884246826 - trainLoss: 0.42576611042022705\n",
      "cnt: 0 - valLoss: 0.4348457455635071 - trainLoss: 0.42576339840888977\n",
      "cnt: 0 - valLoss: 0.4348447620868683 - trainLoss: 0.4257607161998749\n",
      "cnt: 0 - valLoss: 0.4348437786102295 - trainLoss: 0.4257580041885376\n",
      "cnt: 0 - valLoss: 0.4348427355289459 - trainLoss: 0.42575526237487793\n",
      "cnt: 0 - valLoss: 0.43484175205230713 - trainLoss: 0.42575258016586304\n",
      "cnt: 0 - valLoss: 0.43484076857566833 - trainLoss: 0.42574983835220337\n",
      "cnt: 0 - valLoss: 0.43483975529670715 - trainLoss: 0.42574718594551086\n",
      "cnt: 0 - valLoss: 0.43483880162239075 - trainLoss: 0.4257444441318512\n",
      "cnt: 0 - valLoss: 0.4348377585411072 - trainLoss: 0.4257417917251587\n",
      "cnt: 0 - valLoss: 0.43483683466911316 - trainLoss: 0.425739049911499\n",
      "cnt: 0 - valLoss: 0.4348357915878296 - trainLoss: 0.42573630809783936\n",
      "cnt: 0 - valLoss: 0.4348347783088684 - trainLoss: 0.42573362588882446\n",
      "cnt: 0 - valLoss: 0.434833824634552 - trainLoss: 0.4257309138774872\n",
      "cnt: 0 - valLoss: 0.4348328411579132 - trainLoss: 0.4257281720638275\n",
      "cnt: 0 - valLoss: 0.4348318576812744 - trainLoss: 0.4257254898548126\n",
      "cnt: 0 - valLoss: 0.43483081459999084 - trainLoss: 0.42572277784347534\n",
      "cnt: 0 - valLoss: 0.43482983112335205 - trainLoss: 0.4257200360298157\n",
      "cnt: 0 - valLoss: 0.43482884764671326 - trainLoss: 0.42571738362312317\n",
      "cnt: 0 - valLoss: 0.43482786417007446 - trainLoss: 0.4257146418094635\n",
      "cnt: 0 - valLoss: 0.43482688069343567 - trainLoss: 0.4257119596004486\n",
      "cnt: 0 - valLoss: 0.4348258674144745 - trainLoss: 0.42570924758911133\n",
      "cnt: 0 - valLoss: 0.4348249137401581 - trainLoss: 0.42570656538009644\n",
      "cnt: 0 - valLoss: 0.4348238706588745 - trainLoss: 0.42570385336875916\n",
      "cnt: 0 - valLoss: 0.4348229467868805 - trainLoss: 0.4257011115550995\n",
      "cnt: 0 - valLoss: 0.4348219037055969 - trainLoss: 0.4256984293460846\n",
      "cnt: 0 - valLoss: 0.4348209798336029 - trainLoss: 0.4256957471370697\n",
      "cnt: 0 - valLoss: 0.43481993675231934 - trainLoss: 0.42569300532341003\n",
      "cnt: 0 - valLoss: 0.43481898307800293 - trainLoss: 0.42569029331207275\n",
      "cnt: 0 - valLoss: 0.43481796979904175 - trainLoss: 0.42568761110305786\n",
      "cnt: 0 - valLoss: 0.4348169267177582 - trainLoss: 0.4256848692893982\n",
      "cnt: 0 - valLoss: 0.43481600284576416 - trainLoss: 0.4256822168827057\n",
      "cnt: 0 - valLoss: 0.4348149597644806 - trainLoss: 0.425679475069046\n",
      "cnt: 0 - valLoss: 0.4348139762878418 - trainLoss: 0.42567673325538635\n",
      "cnt: 0 - valLoss: 0.434812992811203 - trainLoss: 0.42567408084869385\n",
      "cnt: 0 - valLoss: 0.4348120093345642 - trainLoss: 0.4256713390350342\n",
      "cnt: 0 - valLoss: 0.4348110258579254 - trainLoss: 0.4256686270236969\n",
      "cnt: 0 - valLoss: 0.4348100423812866 - trainLoss: 0.42566588521003723\n",
      "cnt: 0 - valLoss: 0.4348090589046478 - trainLoss: 0.42566320300102234\n",
      "cnt: 0 - valLoss: 0.43480807542800903 - trainLoss: 0.42566055059432983\n",
      "cnt: 0 - valLoss: 0.43480709195137024 - trainLoss: 0.42565780878067017\n",
      "cnt: 0 - valLoss: 0.43480610847473145 - trainLoss: 0.4256550669670105\n",
      "cnt: 0 - valLoss: 0.43480509519577026 - trainLoss: 0.4256523847579956\n",
      "cnt: 0 - valLoss: 0.4348040819168091 - trainLoss: 0.4256496727466583\n",
      "cnt: 0 - valLoss: 0.4348031282424927 - trainLoss: 0.42564699053764343\n",
      "cnt: 0 - valLoss: 0.4348021149635315 - trainLoss: 0.42564424872398376\n",
      "cnt: 0 - valLoss: 0.4348011612892151 - trainLoss: 0.4256415367126465\n",
      "cnt: 0 - valLoss: 0.4348001480102539 - trainLoss: 0.4256387948989868\n",
      "cnt: 0 - valLoss: 0.4347991645336151 - trainLoss: 0.4256361126899719\n",
      "cnt: 0 - valLoss: 0.43479815125465393 - trainLoss: 0.42563340067863464\n",
      "cnt: 0 - valLoss: 0.43479716777801514 - trainLoss: 0.42563068866729736\n",
      "cnt: 0 - valLoss: 0.43479618430137634 - trainLoss: 0.42562800645828247\n",
      "cnt: 0 - valLoss: 0.43479520082473755 - trainLoss: 0.4256252944469452\n",
      "cnt: 0 - valLoss: 0.43479421734809875 - trainLoss: 0.4256225824356079\n",
      "cnt: 0 - valLoss: 0.4347932040691376 - trainLoss: 0.425619900226593\n",
      "cnt: 0 - valLoss: 0.43479225039482117 - trainLoss: 0.42561715841293335\n",
      "cnt: 0 - valLoss: 0.4347912669181824 - trainLoss: 0.42561444640159607\n",
      "cnt: 0 - valLoss: 0.4347902834415436 - trainLoss: 0.4256117641925812\n",
      "cnt: 0 - valLoss: 0.4347892999649048 - trainLoss: 0.4256090223789215\n",
      "cnt: 0 - valLoss: 0.434788316488266 - trainLoss: 0.42560628056526184\n",
      "cnt: 0 - valLoss: 0.4347872734069824 - trainLoss: 0.42560362815856934\n",
      "cnt: 0 - valLoss: 0.4347863495349884 - trainLoss: 0.42560088634490967\n",
      "cnt: 0 - valLoss: 0.4347853362560272 - trainLoss: 0.42559823393821716\n",
      "cnt: 0 - valLoss: 0.4347843527793884 - trainLoss: 0.4255954921245575\n",
      "cnt: 0 - valLoss: 0.43478336930274963 - trainLoss: 0.4255927801132202\n",
      "cnt: 0 - valLoss: 0.43478238582611084 - trainLoss: 0.42559006810188293\n",
      "cnt: 0 - valLoss: 0.43478140234947205 - trainLoss: 0.42558741569519043\n",
      "cnt: 0 - valLoss: 0.43478041887283325 - trainLoss: 0.42558467388153076\n",
      "cnt: 0 - valLoss: 0.43477946519851685 - trainLoss: 0.4255819618701935\n",
      "cnt: 0 - valLoss: 0.43477848172187805 - trainLoss: 0.4255792796611786\n",
      "cnt: 0 - valLoss: 0.43477749824523926 - trainLoss: 0.4255765676498413\n",
      "cnt: 0 - valLoss: 0.43477651476860046 - trainLoss: 0.42557382583618164\n",
      "cnt: 0 - valLoss: 0.43477553129196167 - trainLoss: 0.42557114362716675\n",
      "cnt: 0 - valLoss: 0.43477457761764526 - trainLoss: 0.42556843161582947\n",
      "cnt: 0 - valLoss: 0.43477359414100647 - trainLoss: 0.4255656898021698\n",
      "cnt: 0 - valLoss: 0.4347726106643677 - trainLoss: 0.4255630075931549\n",
      "cnt: 0 - valLoss: 0.4347716271877289 - trainLoss: 0.4255602955818176\n",
      "cnt: 0 - valLoss: 0.4347706437110901 - trainLoss: 0.42555755376815796\n",
      "cnt: 0 - valLoss: 0.4347696602344513 - trainLoss: 0.42555487155914307\n",
      "cnt: 0 - valLoss: 0.4347687065601349 - trainLoss: 0.4255521893501282\n",
      "cnt: 0 - valLoss: 0.4347676932811737 - trainLoss: 0.4255494475364685\n",
      "cnt: 0 - valLoss: 0.4347667396068573 - trainLoss: 0.4255467355251312\n",
      "cnt: 0 - valLoss: 0.4347657561302185 - trainLoss: 0.42554405331611633\n",
      "cnt: 0 - valLoss: 0.4347647726535797 - trainLoss: 0.42554131150245667\n",
      "cnt: 0 - valLoss: 0.4347637891769409 - trainLoss: 0.42553865909576416\n",
      "cnt: 0 - valLoss: 0.4347628355026245 - trainLoss: 0.4255359172821045\n",
      "cnt: 0 - valLoss: 0.43476182222366333 - trainLoss: 0.4255332052707672\n",
      "cnt: 0 - valLoss: 0.4347608685493469 - trainLoss: 0.4255305230617523\n",
      "cnt: 0 - valLoss: 0.43475988507270813 - trainLoss: 0.42552781105041504\n",
      "cnt: 0 - valLoss: 0.43475890159606934 - trainLoss: 0.42552506923675537\n",
      "cnt: 0 - valLoss: 0.43475794792175293 - trainLoss: 0.4255223870277405\n",
      "cnt: 0 - valLoss: 0.43475693464279175 - trainLoss: 0.4255196750164032\n",
      "cnt: 0 - valLoss: 0.43475598096847534 - trainLoss: 0.4255169928073883\n",
      "cnt: 0 - valLoss: 0.43475496768951416 - trainLoss: 0.42551425099372864\n",
      "cnt: 0 - valLoss: 0.43475401401519775 - trainLoss: 0.42551156878471375\n",
      "cnt: 0 - valLoss: 0.43475303053855896 - trainLoss: 0.4255088269710541\n",
      "cnt: 0 - valLoss: 0.43475204706192017 - trainLoss: 0.4255061149597168\n",
      "cnt: 0 - valLoss: 0.43475109338760376 - trainLoss: 0.4255034327507019\n",
      "cnt: 0 - valLoss: 0.4347500801086426 - trainLoss: 0.4255007207393646\n",
      "cnt: 0 - valLoss: 0.43474912643432617 - trainLoss: 0.42549797892570496\n",
      "cnt: 0 - valLoss: 0.434748113155365 - trainLoss: 0.42549529671669006\n",
      "cnt: 0 - valLoss: 0.4347471594810486 - trainLoss: 0.4254925847053528\n",
      "cnt: 0 - valLoss: 0.4347461760044098 - trainLoss: 0.4254898428916931\n",
      "cnt: 0 - valLoss: 0.434745192527771 - trainLoss: 0.4254871904850006\n",
      "cnt: 0 - valLoss: 0.4347442090511322 - trainLoss: 0.42548444867134094\n",
      "cnt: 0 - valLoss: 0.4347432255744934 - trainLoss: 0.42548176646232605\n",
      "cnt: 0 - valLoss: 0.434742271900177 - trainLoss: 0.42547908425331116\n",
      "cnt: 0 - valLoss: 0.4347412586212158 - trainLoss: 0.4254763722419739\n",
      "cnt: 0 - valLoss: 0.4347403049468994 - trainLoss: 0.4254736304283142\n",
      "cnt: 0 - valLoss: 0.4347393214702606 - trainLoss: 0.4254709482192993\n",
      "cnt: 0 - valLoss: 0.4347383677959442 - trainLoss: 0.42546823620796204\n",
      "cnt: 0 - valLoss: 0.4347373843193054 - trainLoss: 0.42546549439430237\n",
      "cnt: 0 - valLoss: 0.43473637104034424 - trainLoss: 0.4254628121852875\n",
      "cnt: 0 - valLoss: 0.43473541736602783 - trainLoss: 0.4254601001739502\n",
      "cnt: 0 - valLoss: 0.43473437428474426 - trainLoss: 0.4254574179649353\n",
      "cnt: 0 - valLoss: 0.43473345041275024 - trainLoss: 0.42545467615127563\n",
      "cnt: 0 - valLoss: 0.43473246693611145 - trainLoss: 0.42545196413993835\n",
      "cnt: 0 - valLoss: 0.43473148345947266 - trainLoss: 0.42544931173324585\n",
      "cnt: 0 - valLoss: 0.43473049998283386 - trainLoss: 0.4254465699195862\n",
      "cnt: 0 - valLoss: 0.4347294867038727 - trainLoss: 0.4254438877105713\n",
      "cnt: 0 - valLoss: 0.4347285032272339 - trainLoss: 0.4254411458969116\n",
      "cnt: 0 - valLoss: 0.4347275197505951 - trainLoss: 0.4254384934902191\n",
      "cnt: 0 - valLoss: 0.4347265958786011 - trainLoss: 0.42543575167655945\n",
      "cnt: 0 - valLoss: 0.4347255527973175 - trainLoss: 0.4254330098628998\n",
      "cnt: 0 - valLoss: 0.4347246289253235 - trainLoss: 0.4254303574562073\n",
      "cnt: 0 - valLoss: 0.4347235858440399 - trainLoss: 0.4254276156425476\n",
      "cnt: 0 - valLoss: 0.4347226321697235 - trainLoss: 0.42542487382888794\n",
      "cnt: 0 - valLoss: 0.4347216486930847 - trainLoss: 0.42542222142219543\n",
      "cnt: 0 - valLoss: 0.4347206652164459 - trainLoss: 0.42541947960853577\n",
      "cnt: 0 - valLoss: 0.4347197413444519 - trainLoss: 0.4254167973995209\n",
      "cnt: 0 - valLoss: 0.43471869826316833 - trainLoss: 0.4254140853881836\n",
      "cnt: 0 - valLoss: 0.43471774458885193 - trainLoss: 0.4254114031791687\n",
      "cnt: 0 - valLoss: 0.43471676111221313 - trainLoss: 0.4254086911678314\n",
      "cnt: 0 - valLoss: 0.43471577763557434 - trainLoss: 0.42540597915649414\n",
      "cnt: 0 - valLoss: 0.4347148537635803 - trainLoss: 0.4254032373428345\n",
      "cnt: 0 - valLoss: 0.43471381068229675 - trainLoss: 0.4254005253314972\n",
      "cnt: 0 - valLoss: 0.43471285700798035 - trainLoss: 0.4253978431224823\n",
      "cnt: 0 - valLoss: 0.43471184372901917 - trainLoss: 0.42539510130882263\n",
      "cnt: 0 - valLoss: 0.43471089005470276 - trainLoss: 0.4253924489021301\n",
      "cnt: 0 - valLoss: 0.43470990657806396 - trainLoss: 0.42538970708847046\n",
      "cnt: 0 - valLoss: 0.43470892310142517 - trainLoss: 0.4253869950771332\n",
      "cnt: 0 - valLoss: 0.43470799922943115 - trainLoss: 0.4253843128681183\n",
      "cnt: 0 - valLoss: 0.4347069561481476 - trainLoss: 0.4253815710544586\n",
      "cnt: 0 - valLoss: 0.4347059726715088 - trainLoss: 0.4253789186477661\n",
      "cnt: 0 - valLoss: 0.43470498919487 - trainLoss: 0.42537617683410645\n",
      "cnt: 0 - valLoss: 0.4347040355205536 - trainLoss: 0.4253734350204468\n",
      "cnt: 0 - valLoss: 0.4347030520439148 - trainLoss: 0.4253707826137543\n",
      "cnt: 0 - valLoss: 0.434702068567276 - trainLoss: 0.4253680408000946\n",
      "cnt: 0 - valLoss: 0.4347010552883148 - trainLoss: 0.4253653585910797\n",
      "cnt: 0 - valLoss: 0.4347001016139984 - trainLoss: 0.42536264657974243\n",
      "cnt: 0 - valLoss: 0.434699147939682 - trainLoss: 0.42535996437072754\n",
      "cnt: 0 - valLoss: 0.4346981644630432 - trainLoss: 0.42535725235939026\n",
      "cnt: 0 - valLoss: 0.4346971809864044 - trainLoss: 0.4253545105457306\n",
      "cnt: 0 - valLoss: 0.4346961975097656 - trainLoss: 0.4253517687320709\n",
      "cnt: 0 - valLoss: 0.43469521403312683 - trainLoss: 0.4253491163253784\n",
      "cnt: 0 - valLoss: 0.43469417095184326 - trainLoss: 0.42534637451171875\n",
      "cnt: 0 - valLoss: 0.43469324707984924 - trainLoss: 0.42534369230270386\n",
      "cnt: 0 - valLoss: 0.43469226360321045 - trainLoss: 0.4253409802913666\n",
      "cnt: 0 - valLoss: 0.43469128012657166 - trainLoss: 0.4253382384777069\n",
      "cnt: 0 - valLoss: 0.43469032645225525 - trainLoss: 0.425335556268692\n",
      "cnt: 0 - valLoss: 0.43468934297561646 - trainLoss: 0.4253328740596771\n",
      "cnt: 0 - valLoss: 0.43468835949897766 - trainLoss: 0.42533016204833984\n",
      "cnt: 0 - valLoss: 0.43468740582466125 - trainLoss: 0.42532747983932495\n",
      "cnt: 0 - valLoss: 0.43468642234802246 - trainLoss: 0.4253247380256653\n",
      "cnt: 0 - valLoss: 0.4346854090690613 - trainLoss: 0.425322026014328\n",
      "cnt: 0 - valLoss: 0.4346843957901001 - trainLoss: 0.4253193438053131\n",
      "cnt: 0 - valLoss: 0.4346834719181061 - trainLoss: 0.42531660199165344\n",
      "cnt: 0 - valLoss: 0.4346824884414673 - trainLoss: 0.42531394958496094\n",
      "cnt: 0 - valLoss: 0.4346815049648285 - trainLoss: 0.42531120777130127\n",
      "cnt: 0 - valLoss: 0.4346805214881897 - trainLoss: 0.4253084659576416\n",
      "cnt: 0 - valLoss: 0.4346795380115509 - trainLoss: 0.42530572414398193\n",
      "cnt: 0 - valLoss: 0.4346785843372345 - trainLoss: 0.42530307173728943\n",
      "cnt: 0 - valLoss: 0.4346775412559509 - trainLoss: 0.42530032992362976\n",
      "cnt: 0 - valLoss: 0.4346766173839569 - trainLoss: 0.42529767751693726\n",
      "cnt: 0 - valLoss: 0.4346756339073181 - trainLoss: 0.4252949357032776\n",
      "cnt: 0 - valLoss: 0.4346746504306793 - trainLoss: 0.4252921938896179\n",
      "cnt: 0 - valLoss: 0.4346736967563629 - trainLoss: 0.4252895414829254\n",
      "cnt: 0 - valLoss: 0.43467265367507935 - trainLoss: 0.42528679966926575\n",
      "cnt: 0 - valLoss: 0.4346717298030853 - trainLoss: 0.42528414726257324\n",
      "cnt: 0 - valLoss: 0.4346707761287689 - trainLoss: 0.4252814054489136\n",
      "cnt: 0 - valLoss: 0.43466976284980774 - trainLoss: 0.4252787232398987\n",
      "cnt: 0 - valLoss: 0.43466880917549133 - trainLoss: 0.425275981426239\n",
      "cnt: 0 - valLoss: 0.43466776609420776 - trainLoss: 0.42527326941490173\n",
      "cnt: 0 - valLoss: 0.43466684222221375 - trainLoss: 0.42527058720588684\n",
      "cnt: 0 - valLoss: 0.43466588854789734 - trainLoss: 0.4252678453922272\n",
      "cnt: 0 - valLoss: 0.43466487526893616 - trainLoss: 0.4252651333808899\n",
      "cnt: 0 - valLoss: 0.43466392159461975 - trainLoss: 0.4252624213695526\n",
      "cnt: 0 - valLoss: 0.43466293811798096 - trainLoss: 0.4252597689628601\n",
      "cnt: 0 - valLoss: 0.43466195464134216 - trainLoss: 0.42525702714920044\n",
      "cnt: 0 - valLoss: 0.43466100096702576 - trainLoss: 0.42525431513786316\n",
      "cnt: 0 - valLoss: 0.4346599876880646 - trainLoss: 0.42525163292884827\n",
      "cnt: 0 - valLoss: 0.43465903401374817 - trainLoss: 0.425248920917511\n",
      "cnt: 0 - valLoss: 0.4346580505371094 - trainLoss: 0.4252461791038513\n",
      "cnt: 0 - valLoss: 0.4346570670604706 - trainLoss: 0.4252434968948364\n",
      "cnt: 0 - valLoss: 0.4346560835838318 - trainLoss: 0.42524078488349915\n",
      "cnt: 0 - valLoss: 0.434655100107193 - trainLoss: 0.4252380430698395\n",
      "cnt: 0 - valLoss: 0.4346541464328766 - trainLoss: 0.4252353608608246\n",
      "cnt: 0 - valLoss: 0.4346531629562378 - trainLoss: 0.4252326190471649\n",
      "cnt: 0 - valLoss: 0.4346522092819214 - trainLoss: 0.4252299666404724\n",
      "cnt: 0 - valLoss: 0.4346511960029602 - trainLoss: 0.42522722482681274\n",
      "cnt: 0 - valLoss: 0.4346502125263214 - trainLoss: 0.42522454261779785\n",
      "cnt: 0 - valLoss: 0.434649258852005 - trainLoss: 0.42522183060646057\n",
      "cnt: 0 - valLoss: 0.434648334980011 - trainLoss: 0.4252191483974457\n",
      "cnt: 0 - valLoss: 0.4346473217010498 - trainLoss: 0.4252164363861084\n",
      "cnt: 0 - valLoss: 0.4346463084220886 - trainLoss: 0.42521369457244873\n",
      "cnt: 0 - valLoss: 0.43464532494544983 - trainLoss: 0.42521101236343384\n",
      "cnt: 0 - valLoss: 0.4346443712711334 - trainLoss: 0.42520830035209656\n",
      "cnt: 0 - valLoss: 0.43464338779449463 - trainLoss: 0.4252055585384369\n",
      "cnt: 0 - valLoss: 0.43464240431785583 - trainLoss: 0.4252028167247772\n",
      "cnt: 0 - valLoss: 0.43464145064353943 - trainLoss: 0.4252001643180847\n",
      "cnt: 0 - valLoss: 0.43464043736457825 - trainLoss: 0.42519742250442505\n",
      "cnt: 0 - valLoss: 0.43463948369026184 - trainLoss: 0.42519474029541016\n",
      "cnt: 0 - valLoss: 0.4346385598182678 - trainLoss: 0.4251920282840729\n",
      "cnt: 0 - valLoss: 0.43463751673698425 - trainLoss: 0.4251893162727356\n",
      "cnt: 0 - valLoss: 0.43463656306266785 - trainLoss: 0.4251866042613983\n",
      "cnt: 0 - valLoss: 0.43463554978370667 - trainLoss: 0.4251839220523834\n",
      "cnt: 0 - valLoss: 0.43463459610939026 - trainLoss: 0.42518121004104614\n",
      "cnt: 0 - valLoss: 0.43463367223739624 - trainLoss: 0.4251784682273865\n",
      "cnt: 0 - valLoss: 0.43463262915611267 - trainLoss: 0.4251757860183716\n",
      "cnt: 0 - valLoss: 0.4346316456794739 - trainLoss: 0.4251730442047119\n",
      "cnt: 0 - valLoss: 0.4346306622028351 - trainLoss: 0.4251703917980194\n",
      "cnt: 0 - valLoss: 0.4346297085285187 - trainLoss: 0.42516764998435974\n",
      "cnt: 0 - valLoss: 0.4346287250518799 - trainLoss: 0.42516493797302246\n",
      "cnt: 0 - valLoss: 0.4346277415752411 - trainLoss: 0.42516225576400757\n",
      "cnt: 0 - valLoss: 0.43462681770324707 - trainLoss: 0.4251595139503479\n",
      "cnt: 0 - valLoss: 0.4346258044242859 - trainLoss: 0.4251568615436554\n",
      "cnt: 0 - valLoss: 0.4346247911453247 - trainLoss: 0.4251541197299957\n",
      "cnt: 0 - valLoss: 0.4346238374710083 - trainLoss: 0.42515143752098083\n",
      "cnt: 0 - valLoss: 0.4346228539943695 - trainLoss: 0.42514869570732117\n",
      "cnt: 0 - valLoss: 0.4346219301223755 - trainLoss: 0.4251459836959839\n",
      "cnt: 0 - valLoss: 0.4346208870410919 - trainLoss: 0.425143301486969\n",
      "cnt: 0 - valLoss: 0.4346199333667755 - trainLoss: 0.4251405894756317\n",
      "cnt: 0 - valLoss: 0.43461892008781433 - trainLoss: 0.42513784766197205\n",
      "cnt: 0 - valLoss: 0.4346179664134979 - trainLoss: 0.42513516545295715\n",
      "cnt: 0 - valLoss: 0.43461698293685913 - trainLoss: 0.4251324534416199\n",
      "cnt: 0 - valLoss: 0.43461599946022034 - trainLoss: 0.4251297116279602\n",
      "cnt: 0 - valLoss: 0.43461504578590393 - trainLoss: 0.4251270592212677\n",
      "cnt: 0 - valLoss: 0.43461406230926514 - trainLoss: 0.42512431740760803\n",
      "cnt: 0 - valLoss: 0.43461307883262634 - trainLoss: 0.42512160539627075\n",
      "cnt: 0 - valLoss: 0.43461209535598755 - trainLoss: 0.42511889338493347\n",
      "cnt: 0 - valLoss: 0.43461111187934875 - trainLoss: 0.4251162111759186\n",
      "cnt: 0 - valLoss: 0.43461018800735474 - trainLoss: 0.4251134693622589\n",
      "cnt: 0 - valLoss: 0.43460917472839355 - trainLoss: 0.4251108169555664\n",
      "cnt: 0 - valLoss: 0.43460819125175476 - trainLoss: 0.42510807514190674\n",
      "cnt: 0 - valLoss: 0.4346071779727936 - trainLoss: 0.42510533332824707\n",
      "cnt: 0 - valLoss: 0.4346062242984772 - trainLoss: 0.42510268092155457\n",
      "cnt: 0 - valLoss: 0.4346052408218384 - trainLoss: 0.4250999391078949\n",
      "cnt: 0 - valLoss: 0.4346042573451996 - trainLoss: 0.4250972270965576\n",
      "cnt: 0 - valLoss: 0.4346033036708832 - trainLoss: 0.4250945448875427\n",
      "cnt: 0 - valLoss: 0.434602290391922 - trainLoss: 0.42509180307388306\n",
      "cnt: 0 - valLoss: 0.4346013367176056 - trainLoss: 0.42508915066719055\n",
      "cnt: 0 - valLoss: 0.4346004128456116 - trainLoss: 0.4250864088535309\n",
      "cnt: 0 - valLoss: 0.434599369764328 - trainLoss: 0.4250836670398712\n",
      "cnt: 0 - valLoss: 0.4345984160900116 - trainLoss: 0.4250809848308563\n",
      "cnt: 0 - valLoss: 0.4345974326133728 - trainLoss: 0.42507827281951904\n",
      "cnt: 0 - valLoss: 0.434596449136734 - trainLoss: 0.42507559061050415\n",
      "cnt: 0 - valLoss: 0.43459552526474 - trainLoss: 0.42507287859916687\n",
      "cnt: 0 - valLoss: 0.4345945417881012 - trainLoss: 0.425070196390152\n",
      "cnt: 0 - valLoss: 0.4345935583114624 - trainLoss: 0.4250674545764923\n",
      "cnt: 0 - valLoss: 0.43459251523017883 - trainLoss: 0.42506474256515503\n",
      "cnt: 0 - valLoss: 0.4345915615558624 - trainLoss: 0.42506206035614014\n",
      "cnt: 0 - valLoss: 0.43459057807922363 - trainLoss: 0.4250592589378357\n",
      "cnt: 0 - valLoss: 0.43458959460258484 - trainLoss: 0.4250566065311432\n",
      "cnt: 0 - valLoss: 0.4345886707305908 - trainLoss: 0.4250538647174835\n",
      "cnt: 0 - valLoss: 0.43458765745162964 - trainLoss: 0.425051212310791\n",
      "cnt: 0 - valLoss: 0.43458667397499084 - trainLoss: 0.42504850029945374\n",
      "cnt: 0 - valLoss: 0.43458569049835205 - trainLoss: 0.42504578828811646\n",
      "cnt: 0 - valLoss: 0.43458470702171326 - trainLoss: 0.42504310607910156\n",
      "cnt: 0 - valLoss: 0.43458378314971924 - trainLoss: 0.4250403940677643\n",
      "cnt: 0 - valLoss: 0.43458279967308044 - trainLoss: 0.4250376522541046\n",
      "cnt: 0 - valLoss: 0.43458178639411926 - trainLoss: 0.4250349700450897\n",
      "cnt: 0 - valLoss: 0.43458080291748047 - trainLoss: 0.42503222823143005\n",
      "cnt: 0 - valLoss: 0.4345798194408417 - trainLoss: 0.42502957582473755\n",
      "cnt: 0 - valLoss: 0.43457889556884766 - trainLoss: 0.4250268340110779\n",
      "cnt: 0 - valLoss: 0.43457791209220886 - trainLoss: 0.4250241816043854\n",
      "cnt: 0 - valLoss: 0.43457695841789246 - trainLoss: 0.4250214397907257\n",
      "cnt: 0 - valLoss: 0.43457597494125366 - trainLoss: 0.42501869797706604\n",
      "cnt: 0 - valLoss: 0.43457502126693726 - trainLoss: 0.42501598596572876\n",
      "cnt: 0 - valLoss: 0.4345740079879761 - trainLoss: 0.42501330375671387\n",
      "cnt: 0 - valLoss: 0.43457305431365967 - trainLoss: 0.4250105917453766\n",
      "cnt: 0 - valLoss: 0.4345720410346985 - trainLoss: 0.4250079095363617\n",
      "cnt: 0 - valLoss: 0.4345710873603821 - trainLoss: 0.425005167722702\n",
      "cnt: 0 - valLoss: 0.4345701038837433 - trainLoss: 0.42500248551368713\n",
      "cnt: 0 - valLoss: 0.4345691204071045 - trainLoss: 0.42499977350234985\n",
      "cnt: 0 - valLoss: 0.4345680773258209 - trainLoss: 0.4249970316886902\n",
      "cnt: 0 - valLoss: 0.43456706404685974 - trainLoss: 0.4249943494796753\n",
      "cnt: 0 - valLoss: 0.43456602096557617 - trainLoss: 0.424991637468338\n",
      "cnt: 0 - valLoss: 0.434565007686615 - trainLoss: 0.4249889850616455\n",
      "cnt: 0 - valLoss: 0.4345639646053314 - trainLoss: 0.42498624324798584\n",
      "cnt: 0 - valLoss: 0.43456289172172546 - trainLoss: 0.42498356103897095\n",
      "cnt: 0 - valLoss: 0.4345618188381195 - trainLoss: 0.42498084902763367\n",
      "cnt: 0 - valLoss: 0.4345608055591583 - trainLoss: 0.424978107213974\n",
      "cnt: 0 - valLoss: 0.43455982208251953 - trainLoss: 0.4249754548072815\n",
      "cnt: 0 - valLoss: 0.4345587491989136 - trainLoss: 0.4249727129936218\n",
      "cnt: 0 - valLoss: 0.4345577657222748 - trainLoss: 0.4249700605869293\n",
      "cnt: 0 - valLoss: 0.4345566928386688 - trainLoss: 0.42496734857559204\n",
      "cnt: 0 - valLoss: 0.43455564975738525 - trainLoss: 0.42496466636657715\n",
      "cnt: 0 - valLoss: 0.4345546364784241 - trainLoss: 0.42496198415756226\n",
      "cnt: 0 - valLoss: 0.4345535933971405 - trainLoss: 0.4249592423439026\n",
      "cnt: 0 - valLoss: 0.4345525801181793 - trainLoss: 0.4249565601348877\n",
      "cnt: 0 - valLoss: 0.43455153703689575 - trainLoss: 0.4249538481235504\n",
      "cnt: 0 - valLoss: 0.43455052375793457 - trainLoss: 0.4249511659145355\n",
      "cnt: 0 - valLoss: 0.434549480676651 - trainLoss: 0.424948513507843\n",
      "cnt: 0 - valLoss: 0.43454840779304504 - trainLoss: 0.42494577169418335\n",
      "cnt: 0 - valLoss: 0.43454739451408386 - trainLoss: 0.42494311928749084\n",
      "cnt: 0 - valLoss: 0.4345463514328003 - trainLoss: 0.4249403774738312\n",
      "cnt: 0 - valLoss: 0.43454527854919434 - trainLoss: 0.4249376654624939\n",
      "cnt: 0 - valLoss: 0.43454426527023315 - trainLoss: 0.424934983253479\n",
      "cnt: 0 - valLoss: 0.4345432221889496 - trainLoss: 0.42493224143981934\n",
      "cnt: 0 - valLoss: 0.43454214930534363 - trainLoss: 0.42492958903312683\n",
      "cnt: 0 - valLoss: 0.43454107642173767 - trainLoss: 0.42492684721946716\n",
      "cnt: 0 - valLoss: 0.4345400631427765 - trainLoss: 0.42492419481277466\n",
      "cnt: 0 - valLoss: 0.4345390200614929 - trainLoss: 0.424921452999115\n",
      "cnt: 0 - valLoss: 0.43453794717788696 - trainLoss: 0.4249188005924225\n",
      "cnt: 0 - valLoss: 0.434536874294281 - trainLoss: 0.4249160885810852\n",
      "cnt: 0 - valLoss: 0.4345358610153198 - trainLoss: 0.42491334676742554\n",
      "cnt: 0 - valLoss: 0.43453481793403625 - trainLoss: 0.42491066455841064\n",
      "cnt: 0 - valLoss: 0.4345338046550751 - trainLoss: 0.42490798234939575\n",
      "cnt: 0 - valLoss: 0.4345327317714691 - trainLoss: 0.42490527033805847\n",
      "cnt: 0 - valLoss: 0.43453168869018555 - trainLoss: 0.42490261793136597\n",
      "cnt: 0 - valLoss: 0.43453067541122437 - trainLoss: 0.4248998761177063\n",
      "cnt: 0 - valLoss: 0.4345296025276184 - trainLoss: 0.4248971939086914\n",
      "cnt: 0 - valLoss: 0.4345286190509796 - trainLoss: 0.4248944818973541\n",
      "cnt: 0 - valLoss: 0.43452754616737366 - trainLoss: 0.4248918294906616\n",
      "cnt: 0 - valLoss: 0.4345265030860901 - trainLoss: 0.42488914728164673\n",
      "cnt: 0 - valLoss: 0.4345254898071289 - trainLoss: 0.42488643527030945\n",
      "cnt: 0 - valLoss: 0.43452444672584534 - trainLoss: 0.4248836934566498\n",
      "cnt: 0 - valLoss: 0.4345233738422394 - trainLoss: 0.4248810112476349\n",
      "cnt: 0 - valLoss: 0.4345223605632782 - trainLoss: 0.4248782992362976\n",
      "cnt: 0 - valLoss: 0.43452131748199463 - trainLoss: 0.4248756468296051\n",
      "cnt: 0 - valLoss: 0.43452024459838867 - trainLoss: 0.42487290501594543\n",
      "cnt: 0 - valLoss: 0.4345192313194275 - trainLoss: 0.42487025260925293\n",
      "cnt: 0 - valLoss: 0.4345181882381439 - trainLoss: 0.42486751079559326\n",
      "cnt: 0 - valLoss: 0.43451717495918274 - trainLoss: 0.42486485838890076\n",
      "cnt: 0 - valLoss: 0.43451613187789917 - trainLoss: 0.4248621165752411\n",
      "cnt: 0 - valLoss: 0.434515118598938 - trainLoss: 0.4248594641685486\n",
      "cnt: 0 - valLoss: 0.43451404571533203 - trainLoss: 0.4248567223548889\n",
      "cnt: 0 - valLoss: 0.43451306223869324 - trainLoss: 0.424854040145874\n",
      "cnt: 0 - valLoss: 0.4345119893550873 - trainLoss: 0.42485132813453674\n",
      "cnt: 0 - valLoss: 0.4345109462738037 - trainLoss: 0.42484864592552185\n",
      "cnt: 0 - valLoss: 0.43450987339019775 - trainLoss: 0.42484593391418457\n",
      "cnt: 0 - valLoss: 0.4345088601112366 - trainLoss: 0.42484328150749207\n",
      "cnt: 0 - valLoss: 0.434507817029953 - trainLoss: 0.4248405694961548\n",
      "cnt: 0 - valLoss: 0.4345068037509918 - trainLoss: 0.4248378574848175\n",
      "cnt: 0 - valLoss: 0.43450576066970825 - trainLoss: 0.4248351752758026\n",
      "cnt: 0 - valLoss: 0.43450474739074707 - trainLoss: 0.42483243346214294\n",
      "cnt: 0 - valLoss: 0.4345037043094635 - trainLoss: 0.42482978105545044\n",
      "cnt: 0 - valLoss: 0.4345026910305023 - trainLoss: 0.42482703924179077\n",
      "cnt: 0 - valLoss: 0.43450164794921875 - trainLoss: 0.42482438683509827\n",
      "cnt: 0 - valLoss: 0.4345006048679352 - trainLoss: 0.424821674823761\n",
      "cnt: 0 - valLoss: 0.434499591588974 - trainLoss: 0.4248189926147461\n",
      "cnt: 0 - valLoss: 0.43449854850769043 - trainLoss: 0.4248162508010864\n",
      "cnt: 0 - valLoss: 0.4344974756240845 - trainLoss: 0.42481353878974915\n",
      "cnt: 0 - valLoss: 0.4344964921474457 - trainLoss: 0.42481088638305664\n",
      "cnt: 0 - valLoss: 0.4344954788684845 - trainLoss: 0.42480820417404175\n",
      "cnt: 0 - valLoss: 0.4344944357872009 - trainLoss: 0.42480549216270447\n",
      "cnt: 0 - valLoss: 0.43449336290359497 - trainLoss: 0.4248028099536896\n",
      "cnt: 0 - valLoss: 0.4344923794269562 - trainLoss: 0.4248000681400299\n",
      "cnt: 0 - valLoss: 0.434491366147995 - trainLoss: 0.4247973561286926\n",
      "cnt: 0 - valLoss: 0.43449029326438904 - trainLoss: 0.42479467391967773\n",
      "cnt: 0 - valLoss: 0.43448930978775024 - trainLoss: 0.42479202151298523\n",
      "cnt: 0 - valLoss: 0.4344882667064667 - trainLoss: 0.42478927969932556\n",
      "cnt: 0 - valLoss: 0.4344871938228607 - trainLoss: 0.4247865676879883\n",
      "cnt: 0 - valLoss: 0.4344862103462219 - trainLoss: 0.4247838854789734\n",
      "cnt: 0 - valLoss: 0.43448519706726074 - trainLoss: 0.4247812330722809\n",
      "cnt: 0 - valLoss: 0.4344841539859772 - trainLoss: 0.4247784912586212\n",
      "cnt: 0 - valLoss: 0.434483140707016 - trainLoss: 0.4247758388519287\n",
      "cnt: 0 - valLoss: 0.4344820976257324 - trainLoss: 0.42477309703826904\n",
      "cnt: 0 - valLoss: 0.43448108434677124 - trainLoss: 0.4247703552246094\n",
      "cnt: 0 - valLoss: 0.43448004126548767 - trainLoss: 0.42476770281791687\n",
      "cnt: 0 - valLoss: 0.4344790279865265 - trainLoss: 0.424765020608902\n",
      "cnt: 0 - valLoss: 0.4344779849052429 - trainLoss: 0.4247623085975647\n",
      "cnt: 0 - valLoss: 0.43447697162628174 - trainLoss: 0.4247596263885498\n",
      "cnt: 0 - valLoss: 0.43447598814964294 - trainLoss: 0.4247569143772125\n",
      "cnt: 0 - valLoss: 0.434474915266037 - trainLoss: 0.42475423216819763\n",
      "cnt: 0 - valLoss: 0.4344738721847534 - trainLoss: 0.42475152015686035\n",
      "cnt: 0 - valLoss: 0.4344728887081146 - trainLoss: 0.4247487783432007\n",
      "cnt: 0 - valLoss: 0.43447181582450867 - trainLoss: 0.4247461259365082\n",
      "cnt: 0 - valLoss: 0.43447086215019226 - trainLoss: 0.4247434437274933\n",
      "cnt: 0 - valLoss: 0.4344697892665863 - trainLoss: 0.424740731716156\n",
      "cnt: 0 - valLoss: 0.4344687759876251 - trainLoss: 0.42473798990249634\n",
      "cnt: 0 - valLoss: 0.43446779251098633 - trainLoss: 0.42473530769348145\n",
      "cnt: 0 - valLoss: 0.43446674942970276 - trainLoss: 0.42473259568214417\n",
      "cnt: 0 - valLoss: 0.4344657361507416 - trainLoss: 0.4247299134731293\n",
      "cnt: 0 - valLoss: 0.434464693069458 - trainLoss: 0.4247272312641144\n",
      "cnt: 0 - valLoss: 0.4344636797904968 - trainLoss: 0.4247245490550995\n",
      "cnt: 0 - valLoss: 0.43446263670921326 - trainLoss: 0.4247218370437622\n",
      "cnt: 0 - valLoss: 0.4344616234302521 - trainLoss: 0.4247191250324249\n",
      "cnt: 0 - valLoss: 0.43446066975593567 - trainLoss: 0.42471644282341003\n",
      "cnt: 0 - valLoss: 0.4344596564769745 - trainLoss: 0.42471373081207275\n",
      "cnt: 0 - valLoss: 0.4344586133956909 - trainLoss: 0.42471104860305786\n",
      "cnt: 0 - valLoss: 0.43445760011672974 - trainLoss: 0.42470839619636536\n",
      "cnt: 0 - valLoss: 0.43445655703544617 - trainLoss: 0.4247056543827057\n",
      "cnt: 0 - valLoss: 0.4344555139541626 - trainLoss: 0.4247030019760132\n",
      "cnt: 0 - valLoss: 0.4344545304775238 - trainLoss: 0.4247002601623535\n",
      "cnt: 0 - valLoss: 0.4344535171985626 - trainLoss: 0.42469754815101624\n",
      "cnt: 0 - valLoss: 0.43445247411727905 - trainLoss: 0.42469486594200134\n",
      "cnt: 0 - valLoss: 0.43445149064064026 - trainLoss: 0.42469221353530884\n",
      "cnt: 0 - valLoss: 0.4344504773616791 - trainLoss: 0.42468947172164917\n",
      "cnt: 0 - valLoss: 0.4344494342803955 - trainLoss: 0.42468681931495667\n",
      "cnt: 0 - valLoss: 0.4344484210014343 - trainLoss: 0.424684077501297\n",
      "cnt: 0 - valLoss: 0.43444743752479553 - trainLoss: 0.4246814250946045\n",
      "cnt: 0 - valLoss: 0.43444639444351196 - trainLoss: 0.4246786832809448\n",
      "cnt: 0 - valLoss: 0.4344453811645508 - trainLoss: 0.42467597126960754\n",
      "cnt: 0 - valLoss: 0.434444397687912 - trainLoss: 0.42467328906059265\n",
      "cnt: 0 - valLoss: 0.4344433546066284 - trainLoss: 0.42467057704925537\n",
      "cnt: 0 - valLoss: 0.4344423711299896 - trainLoss: 0.4246678948402405\n",
      "cnt: 0 - valLoss: 0.43444129824638367 - trainLoss: 0.4246651828289032\n",
      "cnt: 0 - valLoss: 0.43444034457206726 - trainLoss: 0.4246625304222107\n",
      "cnt: 0 - valLoss: 0.4344392716884613 - trainLoss: 0.4246598482131958\n",
      "cnt: 0 - valLoss: 0.4344382882118225 - trainLoss: 0.42465710639953613\n",
      "cnt: 0 - valLoss: 0.43443727493286133 - trainLoss: 0.42465442419052124\n",
      "cnt: 0 - valLoss: 0.43443629145622253 - trainLoss: 0.42465171217918396\n",
      "cnt: 0 - valLoss: 0.4344352185726166 - trainLoss: 0.42464902997016907\n",
      "cnt: 0 - valLoss: 0.4344342350959778 - trainLoss: 0.4246463179588318\n",
      "cnt: 0 - valLoss: 0.434433251619339 - trainLoss: 0.4246436357498169\n",
      "cnt: 0 - valLoss: 0.4344322085380554 - trainLoss: 0.4246409237384796\n",
      "cnt: 0 - valLoss: 0.43443119525909424 - trainLoss: 0.4246382415294647\n",
      "cnt: 0 - valLoss: 0.43443021178245544 - trainLoss: 0.42463552951812744\n",
      "cnt: 0 - valLoss: 0.43442919850349426 - trainLoss: 0.4246327877044678\n",
      "cnt: 0 - valLoss: 0.4344281852245331 - trainLoss: 0.42463013529777527\n",
      "cnt: 0 - valLoss: 0.4344271421432495 - trainLoss: 0.4246274530887604\n",
      "cnt: 0 - valLoss: 0.4344261586666107 - trainLoss: 0.4246247410774231\n",
      "cnt: 0 - valLoss: 0.43442514538764954 - trainLoss: 0.4246220588684082\n",
      "cnt: 0 - valLoss: 0.43442416191101074 - trainLoss: 0.4246193468570709\n",
      "cnt: 0 - valLoss: 0.4344231188297272 - trainLoss: 0.42461660504341125\n",
      "cnt: 0 - valLoss: 0.4344221353530884 - trainLoss: 0.42461395263671875\n",
      "cnt: 0 - valLoss: 0.4344211518764496 - trainLoss: 0.4246112108230591\n",
      "cnt: 0 - valLoss: 0.43442028760910034 - trainLoss: 0.4246085584163666\n",
      "cnt: 0 - valLoss: 0.4344194531440735 - trainLoss: 0.4246058762073517\n",
      "cnt: 0 - valLoss: 0.43441858887672424 - trainLoss: 0.4246031641960144\n",
      "cnt: 0 - valLoss: 0.4344175457954407 - trainLoss: 0.4246004819869995\n",
      "cnt: 0 - valLoss: 0.43441668152809143 - trainLoss: 0.42459776997566223\n",
      "cnt: 0 - valLoss: 0.4344158470630646 - trainLoss: 0.42459508776664734\n",
      "cnt: 0 - valLoss: 0.43441498279571533 - trainLoss: 0.42459240555763245\n",
      "cnt: 0 - valLoss: 0.43441393971443176 - trainLoss: 0.42458972334861755\n",
      "cnt: 0 - valLoss: 0.4344131350517273 - trainLoss: 0.4245870113372803\n",
      "cnt: 0 - valLoss: 0.43441227078437805 - trainLoss: 0.42458435893058777\n",
      "cnt: 0 - valLoss: 0.4344114363193512 - trainLoss: 0.4245816171169281\n",
      "cnt: 0 - valLoss: 0.4344103932380676 - trainLoss: 0.4245789647102356\n",
      "cnt: 0 - valLoss: 0.4344095289707184 - trainLoss: 0.4245762228965759\n",
      "cnt: 0 - valLoss: 0.43440869450569153 - trainLoss: 0.4245735704898834\n",
      "cnt: 0 - valLoss: 0.4344078302383423 - trainLoss: 0.42457082867622375\n",
      "cnt: 0 - valLoss: 0.4344068467617035 - trainLoss: 0.4245681166648865\n",
      "cnt: 0 - valLoss: 0.43440598249435425 - trainLoss: 0.4245654344558716\n",
      "cnt: 0 - valLoss: 0.434405118227005 - trainLoss: 0.4245627820491791\n",
      "cnt: 0 - valLoss: 0.43440425395965576 - trainLoss: 0.4245600402355194\n",
      "cnt: 0 - valLoss: 0.4344032406806946 - trainLoss: 0.42455732822418213\n",
      "cnt: 0 - valLoss: 0.43440237641334534 - trainLoss: 0.42455464601516724\n",
      "cnt: 0 - valLoss: 0.4344015121459961 - trainLoss: 0.42455199360847473\n",
      "cnt: 0 - valLoss: 0.43440067768096924 - trainLoss: 0.42454925179481506\n",
      "cnt: 0 - valLoss: 0.43439963459968567 - trainLoss: 0.42454659938812256\n",
      "cnt: 0 - valLoss: 0.4343987703323364 - trainLoss: 0.4245438575744629\n",
      "cnt: 0 - valLoss: 0.43439793586730957 - trainLoss: 0.4245412051677704\n",
      "cnt: 0 - valLoss: 0.4343970715999603 - trainLoss: 0.4245384633541107\n",
      "cnt: 0 - valLoss: 0.43439602851867676 - trainLoss: 0.4245358109474182\n",
      "cnt: 0 - valLoss: 0.4343951940536499 - trainLoss: 0.42453309893608093\n",
      "cnt: 0 - valLoss: 0.43439432978630066 - trainLoss: 0.42453041672706604\n",
      "cnt: 0 - valLoss: 0.4343934655189514 - trainLoss: 0.42452770471572876\n",
      "cnt: 0 - valLoss: 0.43439242243766785 - trainLoss: 0.42452502250671387\n",
      "cnt: 0 - valLoss: 0.434391587972641 - trainLoss: 0.4245223104953766\n",
      "cnt: 0 - valLoss: 0.43439072370529175 - trainLoss: 0.4245196282863617\n",
      "cnt: 0 - valLoss: 0.4343898594379425 - trainLoss: 0.4245169758796692\n",
      "cnt: 0 - valLoss: 0.4343888461589813 - trainLoss: 0.4245142340660095\n",
      "cnt: 0 - valLoss: 0.43438801169395447 - trainLoss: 0.42451155185699463\n",
      "cnt: 0 - valLoss: 0.43438711762428284 - trainLoss: 0.42450883984565735\n",
      "cnt: 0 - valLoss: 0.4343862533569336 - trainLoss: 0.42450618743896484\n",
      "cnt: 0 - valLoss: 0.4343852400779724 - trainLoss: 0.42450350522994995\n",
      "cnt: 0 - valLoss: 0.43438437581062317 - trainLoss: 0.4245007038116455\n",
      "cnt: 0 - valLoss: 0.4343835711479187 - trainLoss: 0.424498051404953\n",
      "cnt: 0 - valLoss: 0.43438267707824707 - trainLoss: 0.4244953989982605\n",
      "cnt: 0 - valLoss: 0.4343816339969635 - trainLoss: 0.4244927167892456\n",
      "cnt: 0 - valLoss: 0.43438076972961426 - trainLoss: 0.4244900047779083\n",
      "cnt: 0 - valLoss: 0.4343799352645874 - trainLoss: 0.42448726296424866\n",
      "cnt: 0 - valLoss: 0.43437907099723816 - trainLoss: 0.42448461055755615\n",
      "cnt: 0 - valLoss: 0.4343780279159546 - trainLoss: 0.42448192834854126\n",
      "cnt: 0 - valLoss: 0.43437716364860535 - trainLoss: 0.424479216337204\n",
      "cnt: 0 - valLoss: 0.4343763291835785 - trainLoss: 0.4244765341281891\n",
      "cnt: 0 - valLoss: 0.43437546491622925 - trainLoss: 0.4244738221168518\n",
      "cnt: 0 - valLoss: 0.43437460064888 - trainLoss: 0.4244711399078369\n",
      "cnt: 0 - valLoss: 0.4343735873699188 - trainLoss: 0.42446842789649963\n",
      "cnt: 0 - valLoss: 0.4343727231025696 - trainLoss: 0.42446577548980713\n",
      "cnt: 0 - valLoss: 0.43437185883522034 - trainLoss: 0.42446303367614746\n",
      "cnt: 0 - valLoss: 0.4343709945678711 - trainLoss: 0.42446035146713257\n",
      "cnt: 0 - valLoss: 0.4343699812889099 - trainLoss: 0.4244576394557953\n",
      "cnt: 0 - valLoss: 0.43436911702156067 - trainLoss: 0.4244549572467804\n",
      "cnt: 0 - valLoss: 0.4343682527542114 - trainLoss: 0.4244523048400879\n",
      "cnt: 0 - valLoss: 0.43436741828918457 - trainLoss: 0.4244495630264282\n",
      "cnt: 0 - valLoss: 0.434366375207901 - trainLoss: 0.42444685101509094\n",
      "cnt: 0 - valLoss: 0.43436551094055176 - trainLoss: 0.42444416880607605\n",
      "cnt: 0 - valLoss: 0.4343646168708801 - trainLoss: 0.42444148659706116\n",
      "cnt: 0 - valLoss: 0.43436381220817566 - trainLoss: 0.42443880438804626\n",
      "cnt: 0 - valLoss: 0.4343627691268921 - trainLoss: 0.42443612217903137\n",
      "cnt: 0 - valLoss: 0.43436190485954285 - trainLoss: 0.4244334399700165\n",
      "cnt: 0 - valLoss: 0.4343610107898712 - trainLoss: 0.4244306981563568\n",
      "cnt: 0 - valLoss: 0.4343601167201996 - trainLoss: 0.42442798614501953\n",
      "cnt: 0 - valLoss: 0.4343593120574951 - trainLoss: 0.42442530393600464\n",
      "cnt: 0 - valLoss: 0.43435826897621155 - trainLoss: 0.42442265152931213\n",
      "cnt: 0 - valLoss: 0.4343573749065399 - trainLoss: 0.42441990971565247\n",
      "cnt: 0 - valLoss: 0.43435657024383545 - trainLoss: 0.42441725730895996\n",
      "cnt: 0 - valLoss: 0.43435564637184143 - trainLoss: 0.4244145154953003\n",
      "cnt: 0 - valLoss: 0.43435466289520264 - trainLoss: 0.4244118630886078\n",
      "cnt: 0 - valLoss: 0.434353768825531 - trainLoss: 0.4244091212749481\n",
      "cnt: 0 - valLoss: 0.43435290455818176 - trainLoss: 0.4244064688682556\n",
      "cnt: 0 - valLoss: 0.4343520700931549 - trainLoss: 0.42440375685691833\n",
      "cnt: 0 - valLoss: 0.43435120582580566 - trainLoss: 0.42440107464790344\n",
      "cnt: 0 - valLoss: 0.4343501627445221 - trainLoss: 0.42439836263656616\n",
      "cnt: 0 - valLoss: 0.43434932827949524 - trainLoss: 0.42439568042755127\n",
      "cnt: 0 - valLoss: 0.4343484044075012 - trainLoss: 0.424392968416214\n",
      "cnt: 0 - valLoss: 0.43434759974479675 - trainLoss: 0.4243902862071991\n",
      "cnt: 0 - valLoss: 0.4343467056751251 - trainLoss: 0.4243875741958618\n",
      "cnt: 0 - valLoss: 0.43434566259384155 - trainLoss: 0.4243848919868469\n",
      "cnt: 0 - valLoss: 0.4343448281288147 - trainLoss: 0.4243822395801544\n",
      "cnt: 0 - valLoss: 0.4343439042568207 - trainLoss: 0.42437949776649475\n",
      "cnt: 0 - valLoss: 0.4343430697917938 - trainLoss: 0.42437684535980225\n",
      "cnt: 0 - valLoss: 0.43434202671051025 - trainLoss: 0.4243741035461426\n",
      "cnt: 0 - valLoss: 0.434341162443161 - trainLoss: 0.4243713915348053\n",
      "cnt: 0 - valLoss: 0.43434032797813416 - trainLoss: 0.4243687093257904\n",
      "cnt: 0 - valLoss: 0.4343394637107849 - trainLoss: 0.4243660569190979\n",
      "cnt: 0 - valLoss: 0.43433859944343567 - trainLoss: 0.4243633449077606\n",
      "cnt: 0 - valLoss: 0.4343375861644745 - trainLoss: 0.4243606626987457\n",
      "cnt: 0 - valLoss: 0.43433672189712524 - trainLoss: 0.42435792088508606\n",
      "cnt: 0 - valLoss: 0.434335857629776 - trainLoss: 0.42435526847839355\n",
      "cnt: 0 - valLoss: 0.43433496356010437 - trainLoss: 0.4243525266647339\n",
      "cnt: 0 - valLoss: 0.4343340992927551 - trainLoss: 0.4243498742580414\n",
      "cnt: 0 - valLoss: 0.43433302640914917 - trainLoss: 0.4243471324443817\n",
      "cnt: 0 - valLoss: 0.4343321621417999 - trainLoss: 0.4243444800376892\n",
      "cnt: 0 - valLoss: 0.43433132767677307 - trainLoss: 0.4243417978286743\n",
      "cnt: 0 - valLoss: 0.43433046340942383 - trainLoss: 0.42433908581733704\n",
      "cnt: 0 - valLoss: 0.4343295991420746 - trainLoss: 0.42433634400367737\n",
      "cnt: 0 - valLoss: 0.4343285858631134 - trainLoss: 0.42433369159698486\n",
      "cnt: 0 - valLoss: 0.4343276619911194 - trainLoss: 0.42433100938796997\n",
      "cnt: 0 - valLoss: 0.43432682752609253 - trainLoss: 0.4243282973766327\n",
      "cnt: 0 - valLoss: 0.4343259632587433 - trainLoss: 0.4243256151676178\n",
      "cnt: 0 - valLoss: 0.43432509899139404 - trainLoss: 0.4243229031562805\n",
      "cnt: 0 - valLoss: 0.43432408571243286 - trainLoss: 0.4243202209472656\n",
      "cnt: 0 - valLoss: 0.4343232214450836 - trainLoss: 0.42431750893592834\n",
      "cnt: 0 - valLoss: 0.434322327375412 - trainLoss: 0.42431482672691345\n",
      "cnt: 0 - valLoss: 0.43432146310806274 - trainLoss: 0.42431211471557617\n",
      "cnt: 0 - valLoss: 0.4343205988407135 - trainLoss: 0.42430946230888367\n",
      "cnt: 0 - valLoss: 0.4343195855617523 - trainLoss: 0.424306720495224\n",
      "cnt: 0 - valLoss: 0.4343187212944031 - trainLoss: 0.4243040382862091\n",
      "cnt: 0 - valLoss: 0.43431776762008667 - trainLoss: 0.4243013262748718\n",
      "cnt: 0 - valLoss: 0.4343169629573822 - trainLoss: 0.4242986738681793\n",
      "cnt: 0 - valLoss: 0.43431591987609863 - trainLoss: 0.42429596185684204\n",
      "cnt: 0 - valLoss: 0.43431511521339417 - trainLoss: 0.42429327964782715\n",
      "cnt: 0 - valLoss: 0.43431422114372253 - trainLoss: 0.42429062724113464\n",
      "cnt: 0 - valLoss: 0.4343133270740509 - trainLoss: 0.42428791522979736\n",
      "cnt: 0 - valLoss: 0.4343123137950897 - trainLoss: 0.42428523302078247\n",
      "cnt: 0 - valLoss: 0.43431147933006287 - trainLoss: 0.4242825508117676\n",
      "cnt: 0 - valLoss: 0.4343106150627136 - trainLoss: 0.4242798984050751\n",
      "cnt: 0 - valLoss: 0.43430963158607483 - trainLoss: 0.4242771863937378\n",
      "cnt: 0 - valLoss: 0.4343087375164032 - trainLoss: 0.4242745041847229\n",
      "cnt: 0 - valLoss: 0.43430787324905396 - trainLoss: 0.4242718517780304\n",
      "cnt: 0 - valLoss: 0.4343069791793823 - trainLoss: 0.4242691397666931\n",
      "cnt: 0 - valLoss: 0.43430599570274353 - trainLoss: 0.4242664575576782\n",
      "cnt: 0 - valLoss: 0.4343050718307495 - trainLoss: 0.4242638051509857\n",
      "cnt: 0 - valLoss: 0.43430423736572266 - trainLoss: 0.42426109313964844\n",
      "cnt: 0 - valLoss: 0.4343031942844391 - trainLoss: 0.42425844073295593\n",
      "cnt: 0 - valLoss: 0.43430233001708984 - trainLoss: 0.42425569891929626\n",
      "cnt: 0 - valLoss: 0.434301495552063 - trainLoss: 0.42425304651260376\n",
      "cnt: 0 - valLoss: 0.43430063128471375 - trainLoss: 0.4242503345012665\n",
      "cnt: 0 - valLoss: 0.4342995882034302 - trainLoss: 0.4242476522922516\n",
      "cnt: 0 - valLoss: 0.4342987537384033 - trainLoss: 0.4242449998855591\n",
      "cnt: 0 - valLoss: 0.4342978894710541 - trainLoss: 0.4242423176765442\n",
      "cnt: 0 - valLoss: 0.4342968463897705 - trainLoss: 0.4242396056652069\n",
      "cnt: 0 - valLoss: 0.43429601192474365 - trainLoss: 0.4242369532585144\n",
      "cnt: 0 - valLoss: 0.43429508805274963 - trainLoss: 0.4242342710494995\n",
      "cnt: 0 - valLoss: 0.4342942535877228 - trainLoss: 0.42423155903816223\n",
      "cnt: 0 - valLoss: 0.4342932403087616 - trainLoss: 0.4242289066314697\n",
      "cnt: 0 - valLoss: 0.43429234623908997 - trainLoss: 0.42422622442245483\n",
      "cnt: 0 - valLoss: 0.43429145216941833 - trainLoss: 0.42422351241111755\n",
      "cnt: 0 - valLoss: 0.4342905879020691 - trainLoss: 0.42422083020210266\n",
      "cnt: 0 - valLoss: 0.4342895746231079 - trainLoss: 0.42421817779541016\n",
      "cnt: 0 - valLoss: 0.43428871035575867 - trainLoss: 0.4242154657840729\n",
      "cnt: 0 - valLoss: 0.4342878460884094 - trainLoss: 0.42421281337738037\n",
      "cnt: 0 - valLoss: 0.43428683280944824 - trainLoss: 0.4242101013660431\n",
      "cnt: 0 - valLoss: 0.434285968542099 - trainLoss: 0.4242074191570282\n",
      "cnt: 0 - valLoss: 0.43428507447242737 - trainLoss: 0.4242047369480133\n",
      "cnt: 0 - valLoss: 0.4342842102050781 - trainLoss: 0.4242020547389984\n",
      "cnt: 0 - valLoss: 0.43428322672843933 - trainLoss: 0.4241993725299835\n",
      "cnt: 0 - valLoss: 0.4342823624610901 - trainLoss: 0.42419669032096863\n",
      "cnt: 0 - valLoss: 0.43428146839141846 - trainLoss: 0.42419400811195374\n",
      "cnt: 0 - valLoss: 0.4342806041240692 - trainLoss: 0.42419132590293884\n",
      "cnt: 0 - valLoss: 0.43427959084510803 - trainLoss: 0.42418867349624634\n",
      "cnt: 0 - valLoss: 0.434278666973114 - trainLoss: 0.42418596148490906\n",
      "cnt: 0 - valLoss: 0.43427783250808716 - trainLoss: 0.42418327927589417\n",
      "cnt: 0 - valLoss: 0.4342769682407379 - trainLoss: 0.42418062686920166\n",
      "cnt: 0 - valLoss: 0.43427592515945435 - trainLoss: 0.4241779148578644\n",
      "cnt: 0 - valLoss: 0.4342750906944275 - trainLoss: 0.4241752326488495\n",
      "cnt: 0 - valLoss: 0.43427422642707825 - trainLoss: 0.424172580242157\n",
      "cnt: 0 - valLoss: 0.4342733323574066 - trainLoss: 0.4241698682308197\n",
      "cnt: 0 - valLoss: 0.43427228927612305 - trainLoss: 0.4241671860218048\n",
      "cnt: 0 - valLoss: 0.4342714250087738 - trainLoss: 0.4241645336151123\n",
      "cnt: 0 - valLoss: 0.4342705309391022 - trainLoss: 0.42416179180145264\n",
      "cnt: 0 - valLoss: 0.43426966667175293 - trainLoss: 0.42415913939476013\n",
      "cnt: 0 - valLoss: 0.43426865339279175 - trainLoss: 0.4241564869880676\n",
      "cnt: 0 - valLoss: 0.4342677891254425 - trainLoss: 0.42415374517440796\n",
      "cnt: 0 - valLoss: 0.43426692485809326 - trainLoss: 0.42415109276771545\n",
      "cnt: 0 - valLoss: 0.4342660903930664 - trainLoss: 0.42414844036102295\n",
      "cnt: 0 - valLoss: 0.43426501750946045 - trainLoss: 0.42414575815200806\n",
      "cnt: 0 - valLoss: 0.43426409363746643 - trainLoss: 0.4241430461406708\n",
      "cnt: 0 - valLoss: 0.4342632591724396 - trainLoss: 0.42414039373397827\n",
      "cnt: 0 - valLoss: 0.43426239490509033 - trainLoss: 0.4241377115249634\n",
      "cnt: 0 - valLoss: 0.43426135182380676 - trainLoss: 0.4241349995136261\n",
      "cnt: 0 - valLoss: 0.4342605173587799 - trainLoss: 0.4241323173046112\n",
      "cnt: 0 - valLoss: 0.43425965309143066 - trainLoss: 0.4241296648979187\n",
      "cnt: 0 - valLoss: 0.4342587888240814 - trainLoss: 0.4241269528865814\n",
      "cnt: 0 - valLoss: 0.43425771594047546 - trainLoss: 0.42412427067756653\n",
      "cnt: 0 - valLoss: 0.4342568516731262 - trainLoss: 0.42412155866622925\n",
      "cnt: 0 - valLoss: 0.43425601720809937 - trainLoss: 0.42411890625953674\n",
      "cnt: 0 - valLoss: 0.43425509333610535 - trainLoss: 0.42411622405052185\n",
      "cnt: 0 - valLoss: 0.43425408005714417 - trainLoss: 0.42411351203918457\n",
      "cnt: 0 - valLoss: 0.4342532157897949 - trainLoss: 0.42411085963249207\n",
      "cnt: 0 - valLoss: 0.4342523217201233 - trainLoss: 0.4241081774234772\n",
      "cnt: 0 - valLoss: 0.43425145745277405 - trainLoss: 0.42410552501678467\n",
      "cnt: 0 - valLoss: 0.43425044417381287 - trainLoss: 0.4241028130054474\n",
      "cnt: 0 - valLoss: 0.43424952030181885 - trainLoss: 0.4241001307964325\n",
      "cnt: 0 - valLoss: 0.434248685836792 - trainLoss: 0.4240974187850952\n",
      "cnt: 0 - valLoss: 0.43424782156944275 - trainLoss: 0.4240947365760803\n",
      "cnt: 0 - valLoss: 0.4342469274997711 - trainLoss: 0.4240920841693878\n",
      "cnt: 0 - valLoss: 0.4342459440231323 - trainLoss: 0.4240894317626953\n",
      "cnt: 0 - valLoss: 0.4342449903488159 - trainLoss: 0.42408671975135803\n",
      "cnt: 0 - valLoss: 0.4342441260814667 - trainLoss: 0.42408400774002075\n",
      "cnt: 0 - valLoss: 0.4342432916164398 - trainLoss: 0.42408132553100586\n",
      "cnt: 0 - valLoss: 0.43424224853515625 - trainLoss: 0.42407867312431335\n",
      "cnt: 0 - valLoss: 0.434241384267807 - trainLoss: 0.42407599091529846\n",
      "cnt: 0 - valLoss: 0.4342404901981354 - trainLoss: 0.4240732789039612\n",
      "cnt: 0 - valLoss: 0.43423962593078613 - trainLoss: 0.4240706264972687\n",
      "cnt: 0 - valLoss: 0.43423861265182495 - trainLoss: 0.4240679442882538\n",
      "cnt: 0 - valLoss: 0.4342377185821533 - trainLoss: 0.4240652918815613\n",
      "cnt: 0 - valLoss: 0.4342368543148041 - trainLoss: 0.424062579870224\n",
      "cnt: 0 - valLoss: 0.43423599004745483 - trainLoss: 0.4240598976612091\n",
      "cnt: 0 - valLoss: 0.4342350959777832 - trainLoss: 0.4240571856498718\n",
      "cnt: 0 - valLoss: 0.4342341125011444 - trainLoss: 0.4240545332431793\n",
      "cnt: 0 - valLoss: 0.434233158826828 - trainLoss: 0.4240518808364868\n",
      "cnt: 0 - valLoss: 0.43423229455947876 - trainLoss: 0.4240491986274719\n",
      "cnt: 0 - valLoss: 0.4342314600944519 - trainLoss: 0.42404648661613464\n",
      "cnt: 0 - valLoss: 0.43423041701316833 - trainLoss: 0.42404380440711975\n",
      "cnt: 0 - valLoss: 0.4342295527458191 - trainLoss: 0.42404109239578247\n",
      "cnt: 0 - valLoss: 0.43422871828079224 - trainLoss: 0.42403843998908997\n",
      "cnt: 0 - valLoss: 0.4342277944087982 - trainLoss: 0.42403578758239746\n",
      "cnt: 0 - valLoss: 0.43422672152519226 - trainLoss: 0.4240330457687378\n",
      "cnt: 0 - valLoss: 0.4342259168624878 - trainLoss: 0.4240303933620453\n",
      "cnt: 0 - valLoss: 0.43422502279281616 - trainLoss: 0.4240277409553528\n",
      "cnt: 0 - valLoss: 0.4342241585254669 - trainLoss: 0.4240250587463379\n",
      "cnt: 0 - valLoss: 0.43422314524650574 - trainLoss: 0.4240223467350006\n",
      "cnt: 0 - valLoss: 0.4342222213745117 - trainLoss: 0.4240196943283081\n",
      "cnt: 0 - valLoss: 0.4342213273048401 - trainLoss: 0.4240170121192932\n",
      "cnt: 0 - valLoss: 0.43422046303749084 - trainLoss: 0.42401430010795593\n",
      "cnt: 0 - valLoss: 0.434219628572464 - trainLoss: 0.42401161789894104\n",
      "cnt: 0 - valLoss: 0.4342185854911804 - trainLoss: 0.42400890588760376\n",
      "cnt: 0 - valLoss: 0.4342177212238312 - trainLoss: 0.42400625348091125\n",
      "cnt: 0 - valLoss: 0.43421682715415955 - trainLoss: 0.42400357127189636\n",
      "cnt: 0 - valLoss: 0.4342159330844879 - trainLoss: 0.42400091886520386\n",
      "cnt: 0 - valLoss: 0.43421489000320435 - trainLoss: 0.4239982068538666\n",
      "cnt: 0 - valLoss: 0.4342140555381775 - trainLoss: 0.4239955544471741\n",
      "cnt: 0 - valLoss: 0.43421313166618347 - trainLoss: 0.4239928126335144\n",
      "cnt: 0 - valLoss: 0.43421223759651184 - trainLoss: 0.4239901602268219\n",
      "cnt: 0 - valLoss: 0.4342113733291626 - trainLoss: 0.4239875078201294\n",
      "cnt: 0 - valLoss: 0.4342103600502014 - trainLoss: 0.4239848554134369\n",
      "cnt: 0 - valLoss: 0.4342094957828522 - trainLoss: 0.4239821135997772\n",
      "cnt: 0 - valLoss: 0.43420860171318054 - trainLoss: 0.4239794611930847\n",
      "cnt: 0 - valLoss: 0.4342077076435089 - trainLoss: 0.4239768087863922\n",
      "cnt: 0 - valLoss: 0.43420666456222534 - trainLoss: 0.42397406697273254\n",
      "cnt: 0 - valLoss: 0.4342058300971985 - trainLoss: 0.42397141456604004\n",
      "cnt: 0 - valLoss: 0.43420490622520447 - trainLoss: 0.42396873235702515\n",
      "cnt: 0 - valLoss: 0.4342040717601776 - trainLoss: 0.42396602034568787\n",
      "cnt: 0 - valLoss: 0.4342031478881836 - trainLoss: 0.42396336793899536\n",
      "cnt: 0 - valLoss: 0.4342021346092224 - trainLoss: 0.42396071553230286\n",
      "cnt: 0 - valLoss: 0.43420127034187317 - trainLoss: 0.42395803332328796\n",
      "cnt: 0 - valLoss: 0.43420031666755676 - trainLoss: 0.4239553213119507\n",
      "cnt: 0 - valLoss: 0.4341994822025299 - trainLoss: 0.423952579498291\n",
      "cnt: 0 - valLoss: 0.43419861793518066 - trainLoss: 0.4239499270915985\n",
      "cnt: 0 - valLoss: 0.4341975748538971 - trainLoss: 0.423947274684906\n",
      "cnt: 0 - valLoss: 0.43419668078422546 - trainLoss: 0.4239445924758911\n",
      "cnt: 0 - valLoss: 0.43419578671455383 - trainLoss: 0.42394188046455383\n",
      "cnt: 0 - valLoss: 0.4341949224472046 - trainLoss: 0.42393922805786133\n",
      "cnt: 0 - valLoss: 0.43419402837753296 - trainLoss: 0.4239365756511688\n",
      "cnt: 0 - valLoss: 0.4341930150985718 - trainLoss: 0.42393383383750916\n",
      "cnt: 0 - valLoss: 0.43419209122657776 - trainLoss: 0.42393118143081665\n",
      "cnt: 0 - valLoss: 0.4341912567615509 - trainLoss: 0.42392849922180176\n",
      "cnt: 0 - valLoss: 0.4341903328895569 - trainLoss: 0.42392584681510925\n",
      "cnt: 0 - valLoss: 0.43418949842453003 - trainLoss: 0.423923134803772\n",
      "cnt: 0 - valLoss: 0.4341884255409241 - trainLoss: 0.42392048239707947\n",
      "cnt: 0 - valLoss: 0.43418750166893005 - trainLoss: 0.4239178001880646\n",
      "cnt: 0 - valLoss: 0.4341866970062256 - trainLoss: 0.4239150881767273\n",
      "cnt: 0 - valLoss: 0.4341857433319092 - trainLoss: 0.4239124357700348\n",
      "cnt: 0 - valLoss: 0.4341849088668823 - trainLoss: 0.4239097535610199\n",
      "cnt: 0 - valLoss: 0.43418386578559875 - trainLoss: 0.4239070415496826\n",
      "cnt: 0 - valLoss: 0.4341829717159271 - trainLoss: 0.4239043891429901\n",
      "cnt: 0 - valLoss: 0.4341820776462555 - trainLoss: 0.42390164732933044\n",
      "cnt: 0 - valLoss: 0.43418121337890625 - trainLoss: 0.42389899492263794\n",
      "cnt: 0 - valLoss: 0.4341803193092346 - trainLoss: 0.42389634251594543\n",
      "cnt: 0 - valLoss: 0.43417924642562866 - trainLoss: 0.42389360070228577\n",
      "cnt: 0 - valLoss: 0.4341783821582794 - trainLoss: 0.42389094829559326\n",
      "cnt: 0 - valLoss: 0.4341774880886078 - trainLoss: 0.42388826608657837\n",
      "cnt: 0 - valLoss: 0.43417659401893616 - trainLoss: 0.42388561367988586\n",
      "cnt: 0 - valLoss: 0.4341757297515869 - trainLoss: 0.42388296127319336\n",
      "cnt: 0 - valLoss: 0.43417468667030334 - trainLoss: 0.4238802492618561\n",
      "cnt: 0 - valLoss: 0.4341737926006317 - trainLoss: 0.4238775670528412\n",
      "cnt: 0 - valLoss: 0.4341728985309601 - trainLoss: 0.4238748550415039\n",
      "cnt: 0 - valLoss: 0.43417203426361084 - trainLoss: 0.4238722026348114\n",
      "cnt: 0 - valLoss: 0.4341711103916168 - trainLoss: 0.4238695502281189\n",
      "cnt: 0 - valLoss: 0.43417006731033325 - trainLoss: 0.42386680841445923\n",
      "cnt: 0 - valLoss: 0.434169203042984 - trainLoss: 0.4238641560077667\n",
      "cnt: 0 - valLoss: 0.4341683089733124 - trainLoss: 0.42386147379875183\n",
      "cnt: 0 - valLoss: 0.43416741490364075 - trainLoss: 0.42385876178741455\n",
      "cnt: 0 - valLoss: 0.4341665506362915 - trainLoss: 0.42385610938072205\n",
      "cnt: 0 - valLoss: 0.43416547775268555 - trainLoss: 0.42385342717170715\n",
      "cnt: 0 - valLoss: 0.4341646134853363 - trainLoss: 0.4238507151603699\n",
      "cnt: 0 - valLoss: 0.4341637194156647 - trainLoss: 0.42384806275367737\n",
      "cnt: 0 - valLoss: 0.43416285514831543 - trainLoss: 0.4238453805446625\n",
      "cnt: 0 - valLoss: 0.4341619610786438 - trainLoss: 0.4238426685333252\n",
      "cnt: 0 - valLoss: 0.43416106700897217 - trainLoss: 0.4238400161266327\n",
      "cnt: 0 - valLoss: 0.434160053730011 - trainLoss: 0.4238373339176178\n",
      "cnt: 0 - valLoss: 0.43415912985801697 - trainLoss: 0.4238346219062805\n",
      "cnt: 0 - valLoss: 0.43415823578834534 - trainLoss: 0.423831969499588\n",
      "cnt: 0 - valLoss: 0.4341573715209961 - trainLoss: 0.4238292872905731\n",
      "cnt: 0 - valLoss: 0.4341564476490021 - trainLoss: 0.4238266348838806\n",
      "cnt: 0 - valLoss: 0.4341554641723633 - trainLoss: 0.42382392287254333\n",
      "cnt: 0 - valLoss: 0.43415454030036926 - trainLoss: 0.42382127046585083\n",
      "cnt: 0 - valLoss: 0.43415364623069763 - trainLoss: 0.42381852865219116\n",
      "cnt: 0 - valLoss: 0.434152752161026 - trainLoss: 0.42381587624549866\n",
      "cnt: 0 - valLoss: 0.43415188789367676 - trainLoss: 0.42381322383880615\n",
      "cnt: 0 - valLoss: 0.4341509938240051 - trainLoss: 0.4238104820251465\n",
      "cnt: 0 - valLoss: 0.43414998054504395 - trainLoss: 0.423807829618454\n",
      "cnt: 0 - valLoss: 0.4341490566730499 - trainLoss: 0.4238051772117615\n",
      "cnt: 0 - valLoss: 0.4341481626033783 - trainLoss: 0.4238024950027466\n",
      "cnt: 0 - valLoss: 0.43414726853370667 - trainLoss: 0.4237997829914093\n",
      "cnt: 0 - valLoss: 0.43414637446403503 - trainLoss: 0.4237971007823944\n",
      "cnt: 0 - valLoss: 0.4341455101966858 - trainLoss: 0.42379438877105713\n",
      "cnt: 0 - valLoss: 0.43414443731307983 - trainLoss: 0.4237917959690094\n",
      "cnt: 0 - valLoss: 0.4341435730457306 - trainLoss: 0.4237890839576721\n",
      "cnt: 0 - valLoss: 0.43414267897605896 - trainLoss: 0.4237864017486572\n",
      "cnt: 0 - valLoss: 0.43414178490638733 - trainLoss: 0.42378368973731995\n",
      "cnt: 0 - valLoss: 0.4341408908367157 - trainLoss: 0.42378103733062744\n",
      "cnt: 0 - valLoss: 0.4341399669647217 - trainLoss: 0.42377835512161255\n",
      "cnt: 0 - valLoss: 0.4341389834880829 - trainLoss: 0.42377570271492004\n",
      "cnt: 0 - valLoss: 0.43413808941841125 - trainLoss: 0.42377299070358276\n",
      "cnt: 0 - valLoss: 0.4341371953487396 - trainLoss: 0.42377033829689026\n",
      "cnt: 0 - valLoss: 0.434136301279068 - trainLoss: 0.4237675964832306\n",
      "cnt: 0 - valLoss: 0.43413540720939636 - trainLoss: 0.4237649440765381\n",
      "cnt: 0 - valLoss: 0.43413448333740234 - trainLoss: 0.4237622618675232\n",
      "cnt: 0 - valLoss: 0.43413347005844116 - trainLoss: 0.4237596094608307\n",
      "cnt: 0 - valLoss: 0.43413257598876953 - trainLoss: 0.4237569570541382\n",
      "cnt: 0 - valLoss: 0.4341316521167755 - trainLoss: 0.4237542450428009\n",
      "cnt: 0 - valLoss: 0.43413081765174866 - trainLoss: 0.423751562833786\n",
      "cnt: 0 - valLoss: 0.43412986397743225 - trainLoss: 0.42374885082244873\n",
      "cnt: 0 - valLoss: 0.434128999710083 - trainLoss: 0.4237461984157562\n",
      "cnt: 0 - valLoss: 0.4341279864311218 - trainLoss: 0.42374351620674133\n",
      "cnt: 0 - valLoss: 0.4341270923614502 - trainLoss: 0.42374080419540405\n",
      "cnt: 0 - valLoss: 0.4341261684894562 - trainLoss: 0.42373815178871155\n",
      "cnt: 0 - valLoss: 0.4341253340244293 - trainLoss: 0.42373549938201904\n",
      "cnt: 0 - valLoss: 0.4341243803501129 - trainLoss: 0.42373284697532654\n",
      "cnt: 0 - valLoss: 0.4341234862804413 - trainLoss: 0.42373010516166687\n",
      "cnt: 0 - valLoss: 0.43412259221076965 - trainLoss: 0.42372745275497437\n",
      "cnt: 0 - valLoss: 0.4341215491294861 - trainLoss: 0.4237247109413147\n",
      "cnt: 0 - valLoss: 0.43412068486213684 - trainLoss: 0.4237220585346222\n",
      "cnt: 0 - valLoss: 0.4341197907924652 - trainLoss: 0.4237194061279297\n",
      "cnt: 0 - valLoss: 0.4341188967227936 - trainLoss: 0.4237167239189148\n",
      "cnt: 0 - valLoss: 0.43411800265312195 - trainLoss: 0.4237140119075775\n",
      "cnt: 0 - valLoss: 0.43411707878112793 - trainLoss: 0.423711359500885\n",
      "cnt: 0 - valLoss: 0.4341162443161011 - trainLoss: 0.4237087070941925\n",
      "cnt: 0 - valLoss: 0.43411511182785034 - trainLoss: 0.42370596528053284\n",
      "cnt: 0 - valLoss: 0.4341142773628235 - trainLoss: 0.42370331287384033\n",
      "cnt: 0 - valLoss: 0.43411341309547424 - trainLoss: 0.4237006604671478\n",
      "cnt: 0 - valLoss: 0.43411245942115784 - trainLoss: 0.42369791865348816\n",
      "cnt: 0 - valLoss: 0.4341115951538086 - trainLoss: 0.42369526624679565\n",
      "cnt: 0 - valLoss: 0.43411070108413696 - trainLoss: 0.42369258403778076\n",
      "cnt: 0 - valLoss: 0.434109628200531 - trainLoss: 0.4236898720264435\n",
      "cnt: 0 - valLoss: 0.4341087341308594 - trainLoss: 0.423687219619751\n",
      "cnt: 0 - valLoss: 0.43410786986351013 - trainLoss: 0.42368456721305847\n",
      "cnt: 0 - valLoss: 0.4341069757938385 - trainLoss: 0.42368191480636597\n",
      "cnt: 0 - valLoss: 0.43410608172416687 - trainLoss: 0.4236792325973511\n",
      "cnt: 0 - valLoss: 0.43410518765449524 - trainLoss: 0.4236765205860138\n",
      "cnt: 0 - valLoss: 0.434104323387146 - trainLoss: 0.4236738681793213\n",
      "cnt: 0 - valLoss: 0.43410342931747437 - trainLoss: 0.4236711263656616\n",
      "cnt: 0 - valLoss: 0.4341023564338684 - trainLoss: 0.4236684739589691\n",
      "cnt: 0 - valLoss: 0.4341014623641968 - trainLoss: 0.42366573214530945\n",
      "cnt: 0 - valLoss: 0.43410053849220276 - trainLoss: 0.42366307973861694\n",
      "cnt: 0 - valLoss: 0.43409964442253113 - trainLoss: 0.42366042733192444\n",
      "cnt: 0 - valLoss: 0.4340987503528595 - trainLoss: 0.42365777492523193\n",
      "cnt: 0 - valLoss: 0.43409785628318787 - trainLoss: 0.42365509271621704\n",
      "cnt: 0 - valLoss: 0.43409693241119385 - trainLoss: 0.42365238070487976\n",
      "cnt: 0 - valLoss: 0.4340960383415222 - trainLoss: 0.42364972829818726\n",
      "cnt: 0 - valLoss: 0.43409496545791626 - trainLoss: 0.42364707589149475\n",
      "cnt: 0 - valLoss: 0.4340941309928894 - trainLoss: 0.42364436388015747\n",
      "cnt: 0 - valLoss: 0.434093177318573 - trainLoss: 0.4236416816711426\n",
      "cnt: 0 - valLoss: 0.43409231305122375 - trainLoss: 0.4236389696598053\n",
      "cnt: 0 - valLoss: 0.43409138917922974 - trainLoss: 0.4236362874507904\n",
      "cnt: 0 - valLoss: 0.4340905249118805 - trainLoss: 0.4236336350440979\n",
      "cnt: 0 - valLoss: 0.43408966064453125 - trainLoss: 0.4236309230327606\n",
      "cnt: 0 - valLoss: 0.43408870697021484 - trainLoss: 0.4236282706260681\n",
      "cnt: 0 - valLoss: 0.43408769369125366 - trainLoss: 0.4236255884170532\n",
      "cnt: 0 - valLoss: 0.43408674001693726 - trainLoss: 0.4236229360103607\n",
      "cnt: 0 - valLoss: 0.434085875749588 - trainLoss: 0.42362022399902344\n",
      "cnt: 0 - valLoss: 0.4340849816799164 - trainLoss: 0.42361754179000854\n",
      "cnt: 0 - valLoss: 0.43408408761024475 - trainLoss: 0.42361488938331604\n",
      "cnt: 0 - valLoss: 0.4340832233428955 - trainLoss: 0.42361223697662354\n",
      "cnt: 0 - valLoss: 0.4340822994709015 - trainLoss: 0.42360952496528625\n",
      "cnt: 0 - valLoss: 0.43408137559890747 - trainLoss: 0.42360684275627136\n",
      "cnt: 0 - valLoss: 0.43408048152923584 - trainLoss: 0.42360419034957886\n",
      "cnt: 0 - valLoss: 0.43407946825027466 - trainLoss: 0.4236014783382416\n",
      "cnt: 0 - valLoss: 0.43407851457595825 - trainLoss: 0.4235987961292267\n",
      "cnt: 0 - valLoss: 0.4340776205062866 - trainLoss: 0.4235961437225342\n",
      "cnt: 0 - valLoss: 0.434076726436615 - trainLoss: 0.4235934913158417\n",
      "cnt: 0 - valLoss: 0.43407586216926575 - trainLoss: 0.423590749502182\n",
      "cnt: 0 - valLoss: 0.43407490849494934 - trainLoss: 0.4235880970954895\n",
      "cnt: 0 - valLoss: 0.4340740442276001 - trainLoss: 0.423585444688797\n",
      "cnt: 0 - valLoss: 0.4340731203556061 - trainLoss: 0.4235827326774597\n",
      "cnt: 0 - valLoss: 0.43407225608825684 - trainLoss: 0.4235800504684448\n",
      "cnt: 0 - valLoss: 0.4340711832046509 - trainLoss: 0.4235773980617523\n",
      "cnt: 0 - valLoss: 0.43407028913497925 - trainLoss: 0.42357468605041504\n",
      "cnt: 0 - valLoss: 0.4340693950653076 - trainLoss: 0.42357203364372253\n",
      "cnt: 0 - valLoss: 0.434068500995636 - trainLoss: 0.42356935143470764\n",
      "cnt: 0 - valLoss: 0.4340675473213196 - trainLoss: 0.42356663942337036\n",
      "cnt: 0 - valLoss: 0.43406668305397034 - trainLoss: 0.42356395721435547\n",
      "cnt: 0 - valLoss: 0.4340657889842987 - trainLoss: 0.42356130480766296\n",
      "cnt: 0 - valLoss: 0.4340648949146271 - trainLoss: 0.42355865240097046\n",
      "cnt: 0 - valLoss: 0.43406403064727783 - trainLoss: 0.4235559403896332\n",
      "cnt: 0 - valLoss: 0.4340630769729614 - trainLoss: 0.4235532879829407\n",
      "cnt: 0 - valLoss: 0.43406206369400024 - trainLoss: 0.4235506057739258\n",
      "cnt: 0 - valLoss: 0.43406111001968384 - trainLoss: 0.4235478937625885\n",
      "cnt: 0 - valLoss: 0.4340602457523346 - trainLoss: 0.423545241355896\n",
      "cnt: 0 - valLoss: 0.4340593218803406 - trainLoss: 0.4235425591468811\n",
      "cnt: 0 - valLoss: 0.43405845761299133 - trainLoss: 0.4235399067401886\n",
      "cnt: 0 - valLoss: 0.4340575039386749 - trainLoss: 0.4235371947288513\n",
      "cnt: 0 - valLoss: 0.4340566098690033 - trainLoss: 0.4235345125198364\n",
      "cnt: 0 - valLoss: 0.43405571579933167 - trainLoss: 0.4235318601131439\n",
      "cnt: 0 - valLoss: 0.4340548515319824 - trainLoss: 0.42352914810180664\n",
      "cnt: 0 - valLoss: 0.43405377864837646 - trainLoss: 0.42352649569511414\n",
      "cnt: 0 - valLoss: 0.43405288457870483 - trainLoss: 0.42352381348609924\n",
      "cnt: 0 - valLoss: 0.4340519905090332 - trainLoss: 0.42352110147476196\n",
      "cnt: 0 - valLoss: 0.4340510964393616 - trainLoss: 0.42351844906806946\n",
      "cnt: 0 - valLoss: 0.43405014276504517 - trainLoss: 0.42351576685905457\n",
      "cnt: 0 - valLoss: 0.4340492784976959 - trainLoss: 0.4235130548477173\n",
      "cnt: 0 - valLoss: 0.4340483546257019 - trainLoss: 0.4235103726387024\n",
      "cnt: 0 - valLoss: 0.43404749035835266 - trainLoss: 0.4235077202320099\n",
      "cnt: 0 - valLoss: 0.43404659628868103 - trainLoss: 0.4235050678253174\n",
      "cnt: 0 - valLoss: 0.4340455234050751 - trainLoss: 0.4235023558139801\n",
      "cnt: 0 - valLoss: 0.43404459953308105 - trainLoss: 0.4234996736049652\n",
      "cnt: 0 - valLoss: 0.4340437054634094 - trainLoss: 0.4234970211982727\n",
      "cnt: 0 - valLoss: 0.4340428411960602 - trainLoss: 0.4234943687915802\n",
      "cnt: 0 - valLoss: 0.43404194712638855 - trainLoss: 0.42349162697792053\n",
      "cnt: 0 - valLoss: 0.4340410530567169 - trainLoss: 0.423488974571228\n",
      "cnt: 0 - valLoss: 0.43404003977775574 - trainLoss: 0.4234863221645355\n",
      "cnt: 0 - valLoss: 0.4340391159057617 - trainLoss: 0.42348361015319824\n",
      "cnt: 0 - valLoss: 0.4340382218360901 - trainLoss: 0.42348095774650574\n",
      "cnt: 0 - valLoss: 0.43403729796409607 - trainLoss: 0.42347827553749084\n",
      "cnt: 0 - valLoss: 0.4340364336967468 - trainLoss: 0.42347562313079834\n",
      "cnt: 0 - valLoss: 0.43403536081314087 - trainLoss: 0.42347297072410583\n",
      "cnt: 0 - valLoss: 0.43403446674346924 - trainLoss: 0.42347025871276855\n",
      "cnt: 0 - valLoss: 0.43403360247612 - trainLoss: 0.42346757650375366\n",
      "cnt: 0 - valLoss: 0.4340326488018036 - trainLoss: 0.42346492409706116\n",
      "cnt: 0 - valLoss: 0.43403178453445435 - trainLoss: 0.4234622120857239\n",
      "cnt: 0 - valLoss: 0.4340308606624603 - trainLoss: 0.423459529876709\n",
      "cnt: 0 - valLoss: 0.43402981758117676 - trainLoss: 0.4234568774700165\n",
      "cnt: 0 - valLoss: 0.4340289235115051 - trainLoss: 0.4234541952610016\n",
      "cnt: 0 - valLoss: 0.4340280294418335 - trainLoss: 0.4234515130519867\n",
      "cnt: 0 - valLoss: 0.43402713537216187 - trainLoss: 0.4234488308429718\n",
      "cnt: 0 - valLoss: 0.43402621150016785 - trainLoss: 0.4234461486339569\n",
      "cnt: 0 - valLoss: 0.434025377035141 - trainLoss: 0.4234434962272644\n",
      "cnt: 0 - valLoss: 0.43402424454689026 - trainLoss: 0.4234408140182495\n",
      "cnt: 0 - valLoss: 0.43402335047721863 - trainLoss: 0.42343810200691223\n",
      "cnt: 0 - valLoss: 0.4340224862098694 - trainLoss: 0.4234354496002197\n",
      "cnt: 0 - valLoss: 0.43402159214019775 - trainLoss: 0.4234327971935272\n",
      "cnt: 0 - valLoss: 0.43402063846588135 - trainLoss: 0.42343011498451233\n",
      "cnt: 0 - valLoss: 0.43401962518692017 - trainLoss: 0.42342740297317505\n",
      "cnt: 0 - valLoss: 0.43401873111724854 - trainLoss: 0.42342475056648254\n",
      "cnt: 0 - valLoss: 0.4340178370475769 - trainLoss: 0.42342206835746765\n",
      "cnt: 0 - valLoss: 0.43401697278022766 - trainLoss: 0.42341941595077515\n",
      "cnt: 0 - valLoss: 0.43401601910591125 - trainLoss: 0.42341670393943787\n",
      "cnt: 0 - valLoss: 0.434015154838562 - trainLoss: 0.42341405153274536\n",
      "cnt: 0 - valLoss: 0.43401405215263367 - trainLoss: 0.42341139912605286\n",
      "cnt: 0 - valLoss: 0.4340131878852844 - trainLoss: 0.4234086573123932\n",
      "cnt: 0 - valLoss: 0.4340122640132904 - trainLoss: 0.4234060049057007\n",
      "cnt: 0 - valLoss: 0.4340113401412964 - trainLoss: 0.4234033524990082\n",
      "cnt: 0 - valLoss: 0.43401044607162476 - trainLoss: 0.4234006702899933\n",
      "cnt: 0 - valLoss: 0.4340095520019531 - trainLoss: 0.4233980178833008\n",
      "cnt: 0 - valLoss: 0.43400850892066956 - trainLoss: 0.4233953058719635\n",
      "cnt: 0 - valLoss: 0.43400758504867554 - trainLoss: 0.423392653465271\n",
      "cnt: 0 - valLoss: 0.4340067207813263 - trainLoss: 0.4233899712562561\n",
      "cnt: 0 - valLoss: 0.43400582671165466 - trainLoss: 0.4233873188495636\n",
      "cnt: 0 - valLoss: 0.43400487303733826 - trainLoss: 0.4233846068382263\n",
      "cnt: 0 - valLoss: 0.4340039789676666 - trainLoss: 0.4233819544315338\n",
      "cnt: 0 - valLoss: 0.4340031147003174 - trainLoss: 0.4233792722225189\n",
      "cnt: 0 - valLoss: 0.4340020418167114 - trainLoss: 0.4233766198158264\n",
      "cnt: 0 - valLoss: 0.4340011477470398 - trainLoss: 0.42337390780448914\n",
      "cnt: 0 - valLoss: 0.43400025367736816 - trainLoss: 0.42337125539779663\n",
      "cnt: 0 - valLoss: 0.43399927020072937 - trainLoss: 0.4233686029911041\n",
      "cnt: 0 - valLoss: 0.4339982867240906 - trainLoss: 0.42336586117744446\n",
      "cnt: 0 - valLoss: 0.43399733304977417 - trainLoss: 0.42336326837539673\n",
      "cnt: 0 - valLoss: 0.43399637937545776 - trainLoss: 0.42336055636405945\n",
      "cnt: 0 - valLoss: 0.43399539589881897 - trainLoss: 0.42335790395736694\n",
      "cnt: 0 - valLoss: 0.43399447202682495 - trainLoss: 0.42335525155067444\n",
      "cnt: 0 - valLoss: 0.43399348855018616 - trainLoss: 0.42335256934165955\n",
      "cnt: 0 - valLoss: 0.43399253487586975 - trainLoss: 0.42334991693496704\n",
      "cnt: 0 - valLoss: 0.43399152159690857 - trainLoss: 0.42334720492362976\n",
      "cnt: 0 - valLoss: 0.43399059772491455 - trainLoss: 0.42334458231925964\n",
      "cnt: 0 - valLoss: 0.43398961424827576 - trainLoss: 0.42334190011024475\n",
      "cnt: 0 - valLoss: 0.43398869037628174 - trainLoss: 0.42333924770355225\n",
      "cnt: 0 - valLoss: 0.43398770689964294 - trainLoss: 0.42333659529685974\n",
      "cnt: 0 - valLoss: 0.43398675322532654 - trainLoss: 0.42333394289016724\n",
      "cnt: 0 - valLoss: 0.43398576974868774 - trainLoss: 0.42333123087882996\n",
      "cnt: 0 - valLoss: 0.43398481607437134 - trainLoss: 0.42332854866981506\n",
      "cnt: 0 - valLoss: 0.43398383259773254 - trainLoss: 0.42332589626312256\n",
      "cnt: 0 - valLoss: 0.43398287892341614 - trainLoss: 0.42332324385643005\n",
      "cnt: 0 - valLoss: 0.43398189544677734 - trainLoss: 0.42332059144973755\n",
      "cnt: 0 - valLoss: 0.43398091197013855 - trainLoss: 0.42331793904304504\n",
      "cnt: 0 - valLoss: 0.43397998809814453 - trainLoss: 0.42331522703170776\n",
      "cnt: 0 - valLoss: 0.43397900462150574 - trainLoss: 0.42331263422966003\n",
      "cnt: 0 - valLoss: 0.43397805094718933 - trainLoss: 0.42330992221832275\n",
      "cnt: 0 - valLoss: 0.43397706747055054 - trainLoss: 0.42330724000930786\n",
      "cnt: 0 - valLoss: 0.43397608399391174 - trainLoss: 0.42330458760261536\n",
      "cnt: 0 - valLoss: 0.43397513031959534 - trainLoss: 0.42330193519592285\n",
      "cnt: 0 - valLoss: 0.43397417664527893 - trainLoss: 0.42329925298690796\n",
      "cnt: 0 - valLoss: 0.4339732527732849 - trainLoss: 0.42329660058021545\n",
      "cnt: 0 - valLoss: 0.4339722990989685 - trainLoss: 0.42329394817352295\n",
      "cnt: 0 - valLoss: 0.4339712858200073 - trainLoss: 0.42329126596450806\n",
      "cnt: 0 - valLoss: 0.4339703321456909 - trainLoss: 0.4232885539531708\n",
      "cnt: 0 - valLoss: 0.4339693486690521 - trainLoss: 0.42328590154647827\n",
      "cnt: 0 - valLoss: 0.4339683949947357 - trainLoss: 0.42328324913978577\n",
      "cnt: 0 - valLoss: 0.4339674115180969 - trainLoss: 0.42328059673309326\n",
      "cnt: 0 - valLoss: 0.4339664876461029 - trainLoss: 0.42327794432640076\n",
      "cnt: 0 - valLoss: 0.4339655339717865 - trainLoss: 0.42327526211738586\n",
      "cnt: 0 - valLoss: 0.4339645802974701 - trainLoss: 0.42327260971069336\n",
      "cnt: 0 - valLoss: 0.4339635968208313 - trainLoss: 0.4232698976993561\n",
      "cnt: 0 - valLoss: 0.4339626133441925 - trainLoss: 0.4232672452926636\n",
      "cnt: 0 - valLoss: 0.4339616894721985 - trainLoss: 0.42326459288597107\n",
      "cnt: 0 - valLoss: 0.4339606761932373 - trainLoss: 0.42326194047927856\n",
      "cnt: 0 - valLoss: 0.4339596927165985 - trainLoss: 0.42325928807258606\n",
      "cnt: 0 - valLoss: 0.4339587688446045 - trainLoss: 0.42325663566589355\n",
      "cnt: 0 - valLoss: 0.4339578151702881 - trainLoss: 0.42325395345687866\n",
      "cnt: 0 - valLoss: 0.4339568614959717 - trainLoss: 0.4232512414455414\n",
      "cnt: 0 - valLoss: 0.4339558780193329 - trainLoss: 0.4232485890388489\n",
      "cnt: 0 - valLoss: 0.4339548945426941 - trainLoss: 0.42324593663215637\n",
      "cnt: 0 - valLoss: 0.4339539706707001 - trainLoss: 0.42324328422546387\n",
      "cnt: 0 - valLoss: 0.43395301699638367 - trainLoss: 0.42324063181877136\n",
      "cnt: 0 - valLoss: 0.43395206332206726 - trainLoss: 0.42323797941207886\n",
      "cnt: 0 - valLoss: 0.4339510500431061 - trainLoss: 0.4232352674007416\n",
      "cnt: 0 - valLoss: 0.4339500963687897 - trainLoss: 0.4232325851917267\n",
      "cnt: 0 - valLoss: 0.43394917249679565 - trainLoss: 0.42322996258735657\n",
      "cnt: 0 - valLoss: 0.43394821882247925 - trainLoss: 0.4232272803783417\n",
      "cnt: 0 - valLoss: 0.43394726514816284 - trainLoss: 0.42322462797164917\n",
      "cnt: 0 - valLoss: 0.43394628167152405 - trainLoss: 0.42322197556495667\n",
      "cnt: 0 - valLoss: 0.43394529819488525 - trainLoss: 0.42321932315826416\n",
      "cnt: 0 - valLoss: 0.43394434452056885 - trainLoss: 0.4232166111469269\n",
      "cnt: 0 - valLoss: 0.43394336104393005 - trainLoss: 0.4232139587402344\n",
      "cnt: 0 - valLoss: 0.4339424669742584 - trainLoss: 0.4232112765312195\n",
      "cnt: 0 - valLoss: 0.43394148349761963 - trainLoss: 0.423208624124527\n",
      "cnt: 0 - valLoss: 0.43394050002098083 - trainLoss: 0.4232059717178345\n",
      "cnt: 0 - valLoss: 0.43393954634666443 - trainLoss: 0.42320331931114197\n",
      "cnt: 0 - valLoss: 0.4339386224746704 - trainLoss: 0.42320066690444946\n",
      "cnt: 0 - valLoss: 0.4339376389980316 - trainLoss: 0.42319798469543457\n",
      "cnt: 0 - valLoss: 0.4339366853237152 - trainLoss: 0.4231953024864197\n",
      "cnt: 0 - valLoss: 0.4339357018470764 - trainLoss: 0.4231926202774048\n",
      "cnt: 0 - valLoss: 0.43393474817276 - trainLoss: 0.42318999767303467\n",
      "cnt: 0 - valLoss: 0.433933824300766 - trainLoss: 0.4231872856616974\n",
      "cnt: 0 - valLoss: 0.4339328408241272 - trainLoss: 0.4231846332550049\n",
      "cnt: 0 - valLoss: 0.4339318871498108 - trainLoss: 0.4231819808483124\n",
      "cnt: 0 - valLoss: 0.433930903673172 - trainLoss: 0.4231793284416199\n",
      "cnt: 0 - valLoss: 0.4339299499988556 - trainLoss: 0.42317667603492737\n",
      "cnt: 0 - valLoss: 0.4339290261268616 - trainLoss: 0.4231739938259125\n",
      "cnt: 0 - valLoss: 0.43392807245254517 - trainLoss: 0.42317134141921997\n",
      "cnt: 0 - valLoss: 0.43392708897590637 - trainLoss: 0.42316868901252747\n",
      "cnt: 0 - valLoss: 0.43392619490623474 - trainLoss: 0.4231659770011902\n",
      "cnt: 0 - valLoss: 0.43392521142959595 - trainLoss: 0.4231633245944977\n",
      "cnt: 0 - valLoss: 0.43392425775527954 - trainLoss: 0.4231606721878052\n",
      "cnt: 0 - valLoss: 0.43392327427864075 - trainLoss: 0.42315801978111267\n",
      "cnt: 0 - valLoss: 0.43392232060432434 - trainLoss: 0.42315536737442017\n",
      "cnt: 0 - valLoss: 0.4339213967323303 - trainLoss: 0.4231526851654053\n",
      "cnt: 0 - valLoss: 0.43392041325569153 - trainLoss: 0.423149973154068\n",
      "cnt: 0 - valLoss: 0.4339194893836975 - trainLoss: 0.4231473207473755\n",
      "cnt: 0 - valLoss: 0.43391847610473633 - trainLoss: 0.423144668340683\n",
      "cnt: 0 - valLoss: 0.4339175224304199 - trainLoss: 0.4231420159339905\n",
      "cnt: 0 - valLoss: 0.4339165985584259 - trainLoss: 0.423139363527298\n",
      "cnt: 0 - valLoss: 0.4339156448841095 - trainLoss: 0.4231366515159607\n",
      "cnt: 0 - valLoss: 0.4339146912097931 - trainLoss: 0.4231339991092682\n",
      "cnt: 0 - valLoss: 0.4339137077331543 - trainLoss: 0.4231313467025757\n",
      "cnt: 0 - valLoss: 0.4339127838611603 - trainLoss: 0.4231286942958832\n",
      "cnt: 0 - valLoss: 0.4339117705821991 - trainLoss: 0.4231260120868683\n",
      "cnt: 0 - valLoss: 0.4339108467102051 - trainLoss: 0.4231233596801758\n",
      "cnt: 0 - valLoss: 0.43390989303588867 - trainLoss: 0.4231207072734833\n",
      "cnt: 0 - valLoss: 0.43390896916389465 - trainLoss: 0.42311805486679077\n",
      "cnt: 0 - valLoss: 0.43390795588493347 - trainLoss: 0.4231153428554535\n",
      "cnt: 0 - valLoss: 0.43390703201293945 - trainLoss: 0.42311275005340576\n",
      "cnt: 0 - valLoss: 0.43390604853630066 - trainLoss: 0.42311009764671326\n",
      "cnt: 0 - valLoss: 0.43390515446662903 - trainLoss: 0.423107385635376\n",
      "cnt: 0 - valLoss: 0.43390414118766785 - trainLoss: 0.4231047034263611\n",
      "cnt: 0 - valLoss: 0.43390321731567383 - trainLoss: 0.4231020510196686\n",
      "cnt: 0 - valLoss: 0.4339022636413574 - trainLoss: 0.4230993986129761\n",
      "cnt: 0 - valLoss: 0.4339013397693634 - trainLoss: 0.4230967164039612\n",
      "cnt: 0 - valLoss: 0.433900386095047 - trainLoss: 0.4230940341949463\n",
      "cnt: 0 - valLoss: 0.4338993430137634 - trainLoss: 0.4230913817882538\n",
      "cnt: 0 - valLoss: 0.4338984489440918 - trainLoss: 0.4230887293815613\n",
      "cnt: 0 - valLoss: 0.433897465467453 - trainLoss: 0.4230860769748688\n",
      "cnt: 0 - valLoss: 0.4338965117931366 - trainLoss: 0.4230833649635315\n",
      "cnt: 0 - valLoss: 0.4338955879211426 - trainLoss: 0.423080712556839\n",
      "cnt: 0 - valLoss: 0.43389463424682617 - trainLoss: 0.4230780601501465\n",
      "cnt: 0 - valLoss: 0.43389371037483215 - trainLoss: 0.423075407743454\n",
      "cnt: 0 - valLoss: 0.43389275670051575 - trainLoss: 0.4230727255344391\n",
      "cnt: 0 - valLoss: 0.43389177322387695 - trainLoss: 0.4230700135231018\n",
      "cnt: 0 - valLoss: 0.4338908791542053 - trainLoss: 0.4230674207210541\n",
      "cnt: 0 - valLoss: 0.4338899254798889 - trainLoss: 0.4230647087097168\n",
      "cnt: 0 - valLoss: 0.4338889420032501 - trainLoss: 0.4230620563030243\n",
      "cnt: 0 - valLoss: 0.4338879883289337 - trainLoss: 0.4230594038963318\n",
      "cnt: 0 - valLoss: 0.4338870644569397 - trainLoss: 0.4230567514896393\n",
      "cnt: 0 - valLoss: 0.4338861107826233 - trainLoss: 0.4230540990829468\n",
      "cnt: 0 - valLoss: 0.43388524651527405 - trainLoss: 0.4230514466762543\n",
      "cnt: 0 - valLoss: 0.43388426303863525 - trainLoss: 0.4230487644672394\n",
      "cnt: 0 - valLoss: 0.43388310074806213 - trainLoss: 0.4230460524559021\n",
      "cnt: 0 - valLoss: 0.43388229608535767 - trainLoss: 0.4230434000492096\n",
      "cnt: 0 - valLoss: 0.43388134241104126 - trainLoss: 0.4230407476425171\n",
      "cnt: 0 - valLoss: 0.4338805079460144 - trainLoss: 0.4230380952358246\n",
      "cnt: 0 - valLoss: 0.43387964367866516 - trainLoss: 0.4230354428291321\n",
      "cnt: 0 - valLoss: 0.43387874960899353 - trainLoss: 0.4230327904224396\n",
      "cnt: 0 - valLoss: 0.4338778257369995 - trainLoss: 0.42303013801574707\n",
      "cnt: 0 - valLoss: 0.43387699127197266 - trainLoss: 0.4230274260044098\n",
      "cnt: 0 - valLoss: 0.43387582898139954 - trainLoss: 0.4230247735977173\n",
      "cnt: 0 - valLoss: 0.4338749349117279 - trainLoss: 0.4230221211910248\n",
      "cnt: 0 - valLoss: 0.43387407064437866 - trainLoss: 0.4230194389820099\n",
      "cnt: 0 - valLoss: 0.4338732063770294 - trainLoss: 0.4230167865753174\n",
      "cnt: 0 - valLoss: 0.4338723123073578 - trainLoss: 0.4230141341686249\n",
      "cnt: 0 - valLoss: 0.43387141823768616 - trainLoss: 0.4230114817619324\n",
      "cnt: 0 - valLoss: 0.4338705539703369 - trainLoss: 0.4230087697505951\n",
      "cnt: 0 - valLoss: 0.4338696599006653 - trainLoss: 0.42300617694854736\n",
      "cnt: 0 - valLoss: 0.43386876583099365 - trainLoss: 0.4230034649372101\n",
      "cnt: 0 - valLoss: 0.43386760354042053 - trainLoss: 0.4230007827281952\n",
      "cnt: 0 - valLoss: 0.4338666796684265 - trainLoss: 0.4229981601238251\n",
      "cnt: 0 - valLoss: 0.43386584520339966 - trainLoss: 0.4229954481124878\n",
      "cnt: 0 - valLoss: 0.4338649809360504 - trainLoss: 0.4229927957057953\n",
      "cnt: 0 - valLoss: 0.43386411666870117 - trainLoss: 0.4229901432991028\n",
      "cnt: 0 - valLoss: 0.43386319279670715 - trainLoss: 0.4229874610900879\n",
      "cnt: 0 - valLoss: 0.4338623285293579 - trainLoss: 0.4229848086833954\n",
      "cnt: 0 - valLoss: 0.43386146426200867 - trainLoss: 0.4229821562767029\n",
      "cnt: 0 - valLoss: 0.43386027216911316 - trainLoss: 0.4229795038700104\n",
      "cnt: 0 - valLoss: 0.4338594079017639 - trainLoss: 0.42297685146331787\n",
      "cnt: 0 - valLoss: 0.4338585436344147 - trainLoss: 0.4229741394519806\n",
      "cnt: 0 - valLoss: 0.43385764956474304 - trainLoss: 0.4229714870452881\n",
      "cnt: 0 - valLoss: 0.4338567852973938 - trainLoss: 0.4229688346385956\n",
      "cnt: 0 - valLoss: 0.43385589122772217 - trainLoss: 0.4229661822319031\n",
      "cnt: 0 - valLoss: 0.4338550269603729 - trainLoss: 0.42296352982521057\n",
      "cnt: 0 - valLoss: 0.4338541328907013 - trainLoss: 0.4229607880115509\n",
      "cnt: 0 - valLoss: 0.43385323882102966 - trainLoss: 0.42295822501182556\n",
      "cnt: 0 - valLoss: 0.43385228514671326 - trainLoss: 0.4229554831981659\n",
      "cnt: 0 - valLoss: 0.433851420879364 - trainLoss: 0.4229528307914734\n",
      "cnt: 0 - valLoss: 0.43385049700737 - trainLoss: 0.4229501783847809\n",
      "cnt: 0 - valLoss: 0.43384963274002075 - trainLoss: 0.4229475259780884\n",
      "cnt: 0 - valLoss: 0.43384844064712524 - trainLoss: 0.4229448735713959\n",
      "cnt: 0 - valLoss: 0.433847576379776 - trainLoss: 0.42294222116470337\n",
      "cnt: 0 - valLoss: 0.43384668231010437 - trainLoss: 0.42293956875801086\n",
      "cnt: 0 - valLoss: 0.43384578824043274 - trainLoss: 0.42293691635131836\n",
      "cnt: 0 - valLoss: 0.4338449239730835 - trainLoss: 0.4229342043399811\n",
      "cnt: 0 - valLoss: 0.43384402990341187 - trainLoss: 0.4229315519332886\n",
      "cnt: 0 - valLoss: 0.4338431656360626 - trainLoss: 0.42292889952659607\n",
      "cnt: 0 - valLoss: 0.433842271566391 - trainLoss: 0.4229262173175812\n",
      "cnt: 0 - valLoss: 0.43384140729904175 - trainLoss: 0.42292356491088867\n",
      "cnt: 0 - valLoss: 0.43384045362472534 - trainLoss: 0.42292091250419617\n",
      "cnt: 0 - valLoss: 0.4338395297527313 - trainLoss: 0.42291826009750366\n",
      "cnt: 0 - valLoss: 0.4338386058807373 - trainLoss: 0.4229155480861664\n",
      "cnt: 0 - valLoss: 0.4338377118110657 - trainLoss: 0.4229128956794739\n",
      "cnt: 0 - valLoss: 0.43383660912513733 - trainLoss: 0.42291024327278137\n",
      "cnt: 0 - valLoss: 0.4338356554508209 - trainLoss: 0.4229075610637665\n",
      "cnt: 0 - valLoss: 0.4338347911834717 - trainLoss: 0.4229048788547516\n",
      "cnt: 0 - valLoss: 0.43383389711380005 - trainLoss: 0.4229022264480591\n",
      "cnt: 0 - valLoss: 0.4338330030441284 - trainLoss: 0.4228995740413666\n",
      "cnt: 0 - valLoss: 0.4338321387767792 - trainLoss: 0.4228969216346741\n",
      "cnt: 0 - valLoss: 0.4338313043117523 - trainLoss: 0.42289426922798157\n",
      "cnt: 0 - valLoss: 0.4338304400444031 - trainLoss: 0.4228915870189667\n",
      "cnt: 0 - valLoss: 0.4338292181491852 - trainLoss: 0.42288896441459656\n",
      "cnt: 0 - valLoss: 0.4338283836841583 - trainLoss: 0.42288628220558167\n",
      "cnt: 0 - valLoss: 0.4338274896144867 - trainLoss: 0.42288362979888916\n",
      "cnt: 0 - valLoss: 0.4338265657424927 - trainLoss: 0.4228809177875519\n",
      "cnt: 0 - valLoss: 0.4338257312774658 - trainLoss: 0.4228782653808594\n",
      "cnt: 0 - valLoss: 0.4338248074054718 - trainLoss: 0.42287561297416687\n",
      "cnt: 0 - valLoss: 0.43382397294044495 - trainLoss: 0.42287296056747437\n",
      "cnt: 0 - valLoss: 0.4338230490684509 - trainLoss: 0.42287030816078186\n",
      "cnt: 0 - valLoss: 0.4338222146034241 - trainLoss: 0.42286765575408936\n",
      "cnt: 0 - valLoss: 0.4338209927082062 - trainLoss: 0.42286497354507446\n",
      "cnt: 0 - valLoss: 0.4338201582431793 - trainLoss: 0.42286235094070435\n",
      "cnt: 0 - valLoss: 0.4338192939758301 - trainLoss: 0.42285969853401184\n",
      "cnt: 0 - valLoss: 0.43381839990615845 - trainLoss: 0.42285698652267456\n",
      "cnt: 0 - valLoss: 0.43381747603416443 - trainLoss: 0.42285430431365967\n",
      "cnt: 0 - valLoss: 0.4338166415691376 - trainLoss: 0.42285165190696716\n",
      "cnt: 0 - valLoss: 0.43381577730178833 - trainLoss: 0.42284899950027466\n",
      "cnt: 0 - valLoss: 0.4338148236274719 - trainLoss: 0.42284634709358215\n",
      "cnt: 0 - valLoss: 0.43381401896476746 - trainLoss: 0.4228436350822449\n",
      "cnt: 0 - valLoss: 0.43381282687187195 - trainLoss: 0.42284104228019714\n",
      "cnt: 0 - valLoss: 0.4338119626045227 - trainLoss: 0.42283838987350464\n",
      "cnt: 0 - valLoss: 0.43381109833717346 - trainLoss: 0.42283567786216736\n",
      "cnt: 0 - valLoss: 0.43381020426750183 - trainLoss: 0.42283299565315247\n",
      "cnt: 0 - valLoss: 0.4338093400001526 - trainLoss: 0.42283034324645996\n",
      "cnt: 0 - valLoss: 0.43380844593048096 - trainLoss: 0.42282769083976746\n",
      "cnt: 0 - valLoss: 0.4338075518608093 - trainLoss: 0.42282503843307495\n",
      "cnt: 0 - valLoss: 0.4338066875934601 - trainLoss: 0.42282238602638245\n",
      "cnt: 0 - valLoss: 0.43380582332611084 - trainLoss: 0.42281967401504517\n",
      "cnt: 0 - valLoss: 0.43380463123321533 - trainLoss: 0.42281705141067505\n",
      "cnt: 0 - valLoss: 0.4338037669658661 - trainLoss: 0.42281439900398254\n",
      "cnt: 0 - valLoss: 0.43380290269851685 - trainLoss: 0.4228116571903229\n",
      "cnt: 0 - valLoss: 0.4338020086288452 - trainLoss: 0.42280906438827515\n",
      "cnt: 0 - valLoss: 0.43380114436149597 - trainLoss: 0.42280635237693787\n",
      "cnt: 0 - valLoss: 0.4338003098964691 - trainLoss: 0.42280369997024536\n",
      "cnt: 0 - valLoss: 0.4337993860244751 - trainLoss: 0.42280104756355286\n",
      "cnt: 0 - valLoss: 0.43379855155944824 - trainLoss: 0.42279839515686035\n",
      "cnt: 0 - valLoss: 0.433797687292099 - trainLoss: 0.42279574275016785\n",
      "cnt: 0 - valLoss: 0.4337967038154602 - trainLoss: 0.42279309034347534\n",
      "cnt: 0 - valLoss: 0.43379583954811096 - trainLoss: 0.42279043793678284\n",
      "cnt: 0 - valLoss: 0.43379467725753784 - trainLoss: 0.42278778553009033\n",
      "cnt: 0 - valLoss: 0.433793842792511 - trainLoss: 0.4227851331233978\n",
      "cnt: 0 - valLoss: 0.43379291892051697 - trainLoss: 0.42278245091438293\n",
      "cnt: 0 - valLoss: 0.4337920844554901 - trainLoss: 0.42277979850769043\n",
      "cnt: 0 - valLoss: 0.4337911605834961 - trainLoss: 0.42277708649635315\n",
      "cnt: 0 - valLoss: 0.43379032611846924 - trainLoss: 0.42277443408966064\n",
      "cnt: 0 - valLoss: 0.4337894022464752 - trainLoss: 0.42277178168296814\n",
      "cnt: 0 - valLoss: 0.43378856778144836 - trainLoss: 0.42276912927627563\n",
      "cnt: 0 - valLoss: 0.43378740549087524 - trainLoss: 0.42276647686958313\n",
      "cnt: 0 - valLoss: 0.433786541223526 - trainLoss: 0.4227638244628906\n",
      "cnt: 0 - valLoss: 0.43378567695617676 - trainLoss: 0.42276111245155334\n",
      "cnt: 0 - valLoss: 0.4337847828865051 - trainLoss: 0.4227585196495056\n",
      "cnt: 0 - valLoss: 0.4337838888168335 - trainLoss: 0.4227558672428131\n",
      "cnt: 0 - valLoss: 0.43378302454948425 - trainLoss: 0.42275315523147583\n",
      "cnt: 0 - valLoss: 0.433782160282135 - trainLoss: 0.4227505624294281\n",
      "cnt: 0 - valLoss: 0.43378132581710815 - trainLoss: 0.4227478504180908\n",
      "cnt: 0 - valLoss: 0.4337804615497589 - trainLoss: 0.4227451682090759\n",
      "cnt: 0 - valLoss: 0.4337792694568634 - trainLoss: 0.4227425158023834\n",
      "cnt: 0 - valLoss: 0.43377840518951416 - trainLoss: 0.4227398633956909\n",
      "cnt: 0 - valLoss: 0.4337775409221649 - trainLoss: 0.4227372109889984\n",
      "cnt: 0 - valLoss: 0.4337766468524933 - trainLoss: 0.4227345287799835\n",
      "cnt: 0 - valLoss: 0.43377578258514404 - trainLoss: 0.422731876373291\n",
      "cnt: 0 - valLoss: 0.4337748885154724 - trainLoss: 0.4227292239665985\n",
      "cnt: 0 - valLoss: 0.43377402424812317 - trainLoss: 0.4227265417575836\n",
      "cnt: 0 - valLoss: 0.4337731599807739 - trainLoss: 0.4227238893508911\n",
      "cnt: 0 - valLoss: 0.4337722659111023 - trainLoss: 0.42272117733955383\n",
      "cnt: 0 - valLoss: 0.43377137184143066 - trainLoss: 0.4227185845375061\n",
      "cnt: 0 - valLoss: 0.43377041816711426 - trainLoss: 0.4227158725261688\n",
      "cnt: 0 - valLoss: 0.4337693154811859 - trainLoss: 0.4227132201194763\n",
      "cnt: 0 - valLoss: 0.43376845121383667 - trainLoss: 0.4227105677127838\n",
      "cnt: 0 - valLoss: 0.43376755714416504 - trainLoss: 0.4227079153060913\n",
      "cnt: 0 - valLoss: 0.4337666630744934 - trainLoss: 0.4227052628993988\n",
      "cnt: 0 - valLoss: 0.43376579880714417 - trainLoss: 0.4227026104927063\n",
      "cnt: 0 - valLoss: 0.4337649345397949 - trainLoss: 0.4226999282836914\n",
      "cnt: 0 - valLoss: 0.4337640404701233 - trainLoss: 0.4226973056793213\n",
      "cnt: 0 - valLoss: 0.43376317620277405 - trainLoss: 0.4226946234703064\n",
      "cnt: 0 - valLoss: 0.4337623119354248 - trainLoss: 0.4226919412612915\n",
      "cnt: 0 - valLoss: 0.4337611794471741 - trainLoss: 0.4226892590522766\n",
      "cnt: 0 - valLoss: 0.43376031517982483 - trainLoss: 0.4226866066455841\n",
      "cnt: 0 - valLoss: 0.4337594509124756 - trainLoss: 0.4226839542388916\n",
      "cnt: 0 - valLoss: 0.4337584972381592 - trainLoss: 0.4226813018321991\n",
      "cnt: 0 - valLoss: 0.4337576627731323 - trainLoss: 0.4226786494255066\n",
      "cnt: 0 - valLoss: 0.4337567985057831 - trainLoss: 0.4226759970188141\n",
      "cnt: 0 - valLoss: 0.43375590443611145 - trainLoss: 0.4226733446121216\n",
      "cnt: 0 - valLoss: 0.4337550103664398 - trainLoss: 0.4226706922054291\n",
      "cnt: 0 - valLoss: 0.4337541460990906 - trainLoss: 0.4226679801940918\n",
      "cnt: 0 - valLoss: 0.43375298380851746 - trainLoss: 0.4226653277873993\n",
      "cnt: 0 - valLoss: 0.4337521195411682 - trainLoss: 0.4226626753807068\n",
      "cnt: 0 - valLoss: 0.4337512254714966 - trainLoss: 0.4226599931716919\n",
      "cnt: 0 - valLoss: 0.43375036120414734 - trainLoss: 0.4226573407649994\n",
      "cnt: 0 - valLoss: 0.4337494969367981 - trainLoss: 0.4226546883583069\n",
      "cnt: 0 - valLoss: 0.4337485730648041 - trainLoss: 0.4226520359516144\n",
      "cnt: 0 - valLoss: 0.43374770879745483 - trainLoss: 0.4226493537425995\n",
      "cnt: 0 - valLoss: 0.4337468445301056 - trainLoss: 0.422646701335907\n",
      "cnt: 0 - valLoss: 0.43374598026275635 - trainLoss: 0.4226440489292145\n",
      "cnt: 0 - valLoss: 0.4337451457977295 - trainLoss: 0.422641396522522\n",
      "cnt: 0 - valLoss: 0.4337439239025116 - trainLoss: 0.42263874411582947\n",
      "cnt: 0 - valLoss: 0.43374308943748474 - trainLoss: 0.4226360023021698\n",
      "cnt: 0 - valLoss: 0.4337421655654907 - trainLoss: 0.42263340950012207\n",
      "cnt: 0 - valLoss: 0.4337412714958191 - trainLoss: 0.4226306974887848\n",
      "cnt: 0 - valLoss: 0.43374043703079224 - trainLoss: 0.4226280450820923\n",
      "cnt: 0 - valLoss: 0.4337395131587982 - trainLoss: 0.4226253926753998\n",
      "cnt: 0 - valLoss: 0.43373870849609375 - trainLoss: 0.4226227402687073\n",
      "cnt: 0 - valLoss: 0.43373775482177734 - trainLoss: 0.42262008786201477\n",
      "cnt: 0 - valLoss: 0.4337369203567505 - trainLoss: 0.42261743545532227\n",
      "cnt: 0 - valLoss: 0.43373605608940125 - trainLoss: 0.42261478304862976\n",
      "cnt: 0 - valLoss: 0.4337348937988281 - trainLoss: 0.42261213064193726\n",
      "cnt: 0 - valLoss: 0.4337339997291565 - trainLoss: 0.42260947823524475\n",
      "cnt: 0 - valLoss: 0.43373313546180725 - trainLoss: 0.42260682582855225\n",
      "cnt: 0 - valLoss: 0.433732271194458 - trainLoss: 0.42260411381721497\n",
      "cnt: 0 - valLoss: 0.4337313771247864 - trainLoss: 0.42260146141052246\n",
      "cnt: 0 - valLoss: 0.43373051285743713 - trainLoss: 0.42259877920150757\n",
      "cnt: 0 - valLoss: 0.4337296187877655 - trainLoss: 0.42259612679481506\n",
      "cnt: 0 - valLoss: 0.43372875452041626 - trainLoss: 0.42259347438812256\n",
      "cnt: 0 - valLoss: 0.4337279200553894 - trainLoss: 0.42259085178375244\n",
      "cnt: 0 - valLoss: 0.4337269961833954 - trainLoss: 0.42258816957473755\n",
      "cnt: 0 - valLoss: 0.43372586369514465 - trainLoss: 0.42258551716804504\n",
      "cnt: 0 - valLoss: 0.4337249994277954 - trainLoss: 0.42258286476135254\n",
      "cnt: 0 - valLoss: 0.43372413516044617 - trainLoss: 0.42258021235466003\n",
      "cnt: 0 - valLoss: 0.43372324109077454 - trainLoss: 0.42257755994796753\n",
      "cnt: 0 - valLoss: 0.4337223470211029 - trainLoss: 0.42257487773895264\n",
      "cnt: 0 - valLoss: 0.43372148275375366 - trainLoss: 0.42257222533226013\n",
      "cnt: 0 - valLoss: 0.43372058868408203 - trainLoss: 0.42256954312324524\n",
      "cnt: 0 - valLoss: 0.433719664812088 - trainLoss: 0.42256689071655273\n",
      "cnt: 0 - valLoss: 0.433718740940094 - trainLoss: 0.42256423830986023\n",
      "cnt: 0 - valLoss: 0.4337177872657776 - trainLoss: 0.4225615859031677\n",
      "cnt: 0 - valLoss: 0.43371686339378357 - trainLoss: 0.4225589334964752\n",
      "cnt: 0 - valLoss: 0.43371593952178955 - trainLoss: 0.4225563108921051\n",
      "cnt: 0 - valLoss: 0.43371501564979553 - trainLoss: 0.4225536286830902\n",
      "cnt: 0 - valLoss: 0.4337140619754791 - trainLoss: 0.4225509762763977\n",
      "cnt: 0 - valLoss: 0.4337131083011627 - trainLoss: 0.4225483536720276\n",
      "cnt: 0 - valLoss: 0.4337121844291687 - trainLoss: 0.4225456714630127\n",
      "cnt: 0 - valLoss: 0.4337112307548523 - trainLoss: 0.4225430488586426\n",
      "cnt: 0 - valLoss: 0.43371033668518066 - trainLoss: 0.4225403368473053\n",
      "cnt: 0 - valLoss: 0.43370938301086426 - trainLoss: 0.42253774404525757\n",
      "cnt: 0 - valLoss: 0.43370845913887024 - trainLoss: 0.42253509163856506\n",
      "cnt: 0 - valLoss: 0.43370750546455383 - trainLoss: 0.42253243923187256\n",
      "cnt: 0 - valLoss: 0.4337065517902374 - trainLoss: 0.42252978682518005\n",
      "cnt: 0 - valLoss: 0.4337056577205658 - trainLoss: 0.42252713441848755\n",
      "cnt: 0 - valLoss: 0.4337047338485718 - trainLoss: 0.42252448201179504\n",
      "cnt: 0 - valLoss: 0.43370378017425537 - trainLoss: 0.42252182960510254\n",
      "cnt: 0 - valLoss: 0.43370282649993896 - trainLoss: 0.42251917719841003\n",
      "cnt: 0 - valLoss: 0.43370193243026733 - trainLoss: 0.42251652479171753\n",
      "cnt: 0 - valLoss: 0.4337009787559509 - trainLoss: 0.42251384258270264\n",
      "cnt: 0 - valLoss: 0.4337000548839569 - trainLoss: 0.42251119017601013\n",
      "cnt: 0 - valLoss: 0.4336991012096405 - trainLoss: 0.4225085377693176\n",
      "cnt: 0 - valLoss: 0.4336981475353241 - trainLoss: 0.4225058853626251\n",
      "cnt: 0 - valLoss: 0.43369725346565247 - trainLoss: 0.4225032329559326\n",
      "cnt: 0 - valLoss: 0.43369632959365845 - trainLoss: 0.4225005805492401\n",
      "cnt: 0 - valLoss: 0.43369540572166443 - trainLoss: 0.4224979281425476\n",
      "cnt: 0 - valLoss: 0.4336944818496704 - trainLoss: 0.4224952757358551\n",
      "cnt: 0 - valLoss: 0.433693528175354 - trainLoss: 0.4224926829338074\n",
      "cnt: 0 - valLoss: 0.4336925745010376 - trainLoss: 0.42249003052711487\n",
      "cnt: 0 - valLoss: 0.43369168043136597 - trainLoss: 0.4224873185157776\n",
      "cnt: 0 - valLoss: 0.43369075655937195 - trainLoss: 0.42248469591140747\n",
      "cnt: 0 - valLoss: 0.43368980288505554 - trainLoss: 0.4224820137023926\n",
      "cnt: 0 - valLoss: 0.4336889088153839 - trainLoss: 0.42247939109802246\n",
      "cnt: 0 - valLoss: 0.4336879551410675 - trainLoss: 0.42247679829597473\n",
      "cnt: 0 - valLoss: 0.4336870312690735 - trainLoss: 0.42247408628463745\n",
      "cnt: 0 - valLoss: 0.4336860775947571 - trainLoss: 0.4224714934825897\n",
      "cnt: 0 - valLoss: 0.43368518352508545 - trainLoss: 0.42246878147125244\n",
      "cnt: 0 - valLoss: 0.43368422985076904 - trainLoss: 0.4224661886692047\n",
      "cnt: 0 - valLoss: 0.43368327617645264 - trainLoss: 0.42246347665786743\n",
      "cnt: 0 - valLoss: 0.43368229269981384 - trainLoss: 0.4224608838558197\n",
      "cnt: 0 - valLoss: 0.43368130922317505 - trainLoss: 0.4224582016468048\n",
      "cnt: 0 - valLoss: 0.43368032574653625 - trainLoss: 0.4224555492401123\n",
      "cnt: 0 - valLoss: 0.43367934226989746 - trainLoss: 0.4224528968334198\n",
      "cnt: 0 - valLoss: 0.43367835879325867 - trainLoss: 0.42245030403137207\n",
      "cnt: 0 - valLoss: 0.4336773753166199 - trainLoss: 0.42244765162467957\n",
      "cnt: 0 - valLoss: 0.4336763918399811 - trainLoss: 0.42244499921798706\n",
      "cnt: 0 - valLoss: 0.4336754083633423 - trainLoss: 0.4224422872066498\n",
      "cnt: 0 - valLoss: 0.4336744248867035 - trainLoss: 0.42243972420692444\n",
      "cnt: 0 - valLoss: 0.4336734414100647 - trainLoss: 0.42243704199790955\n",
      "cnt: 0 - valLoss: 0.4336724579334259 - trainLoss: 0.42243441939353943\n",
      "cnt: 0 - valLoss: 0.4336714446544647 - trainLoss: 0.4224317669868469\n",
      "cnt: 0 - valLoss: 0.4336704611778259 - trainLoss: 0.4224291145801544\n",
      "cnt: 0 - valLoss: 0.43366947770118713 - trainLoss: 0.4224264621734619\n",
      "cnt: 0 - valLoss: 0.4336685538291931 - trainLoss: 0.4224238097667694\n",
      "cnt: 0 - valLoss: 0.43366751074790955 - trainLoss: 0.4224211871623993\n",
      "cnt: 0 - valLoss: 0.4336665868759155 - trainLoss: 0.4224185049533844\n",
      "cnt: 0 - valLoss: 0.43366557359695435 - trainLoss: 0.42241594195365906\n",
      "cnt: 0 - valLoss: 0.43366459012031555 - trainLoss: 0.42241325974464417\n",
      "cnt: 0 - valLoss: 0.43366360664367676 - trainLoss: 0.42241060733795166\n",
      "cnt: 0 - valLoss: 0.43366262316703796 - trainLoss: 0.42240795493125916\n",
      "cnt: 0 - valLoss: 0.43366163969039917 - trainLoss: 0.42240530252456665\n",
      "cnt: 0 - valLoss: 0.43366068601608276 - trainLoss: 0.42240265011787415\n",
      "cnt: 0 - valLoss: 0.43365970253944397 - trainLoss: 0.4224000573158264\n",
      "cnt: 0 - valLoss: 0.4336587190628052 - trainLoss: 0.4223974049091339\n",
      "cnt: 0 - valLoss: 0.43365779519081116 - trainLoss: 0.42239469289779663\n",
      "cnt: 0 - valLoss: 0.4336567521095276 - trainLoss: 0.4223921000957489\n",
      "cnt: 0 - valLoss: 0.43365582823753357 - trainLoss: 0.4223894774913788\n",
      "cnt: 0 - valLoss: 0.4336548149585724 - trainLoss: 0.4223868250846863\n",
      "cnt: 0 - valLoss: 0.4336538314819336 - trainLoss: 0.4223841726779938\n",
      "cnt: 0 - valLoss: 0.4336529076099396 - trainLoss: 0.42238152027130127\n",
      "cnt: 0 - valLoss: 0.4336519241333008 - trainLoss: 0.42237889766693115\n",
      "cnt: 0 - valLoss: 0.433650940656662 - trainLoss: 0.42237624526023865\n",
      "cnt: 0 - valLoss: 0.4336499869823456 - trainLoss: 0.4223736524581909\n",
      "cnt: 0 - valLoss: 0.4336490035057068 - trainLoss: 0.4223710000514984\n",
      "cnt: 0 - valLoss: 0.433648020029068 - trainLoss: 0.4223683178424835\n",
      "cnt: 0 - valLoss: 0.4336470663547516 - trainLoss: 0.422365665435791\n",
      "cnt: 0 - valLoss: 0.4336460828781128 - trainLoss: 0.4223630130290985\n",
      "cnt: 0 - valLoss: 0.433645099401474 - trainLoss: 0.422360360622406\n",
      "cnt: 0 - valLoss: 0.4336441457271576 - trainLoss: 0.4223577082157135\n",
      "cnt: 0 - valLoss: 0.4336432218551636 - trainLoss: 0.42235511541366577\n",
      "cnt: 0 - valLoss: 0.43364217877388 - trainLoss: 0.42235246300697327\n",
      "cnt: 0 - valLoss: 0.433641254901886 - trainLoss: 0.42234984040260315\n",
      "cnt: 0 - valLoss: 0.4336403012275696 - trainLoss: 0.42234718799591064\n",
      "cnt: 0 - valLoss: 0.4336393177509308 - trainLoss: 0.42234453558921814\n",
      "cnt: 0 - valLoss: 0.433638334274292 - trainLoss: 0.42234188318252563\n",
      "cnt: 0 - valLoss: 0.4336373805999756 - trainLoss: 0.42233923077583313\n",
      "cnt: 0 - valLoss: 0.4336363971233368 - trainLoss: 0.4223365783691406\n",
      "cnt: 0 - valLoss: 0.4336354434490204 - trainLoss: 0.4223339557647705\n",
      "cnt: 0 - valLoss: 0.4336344599723816 - trainLoss: 0.422331303358078\n",
      "cnt: 0 - valLoss: 0.4336335361003876 - trainLoss: 0.4223287105560303\n",
      "cnt: 0 - valLoss: 0.4336325526237488 - trainLoss: 0.422325998544693\n",
      "cnt: 0 - valLoss: 0.4336315989494324 - trainLoss: 0.4223233759403229\n",
      "cnt: 0 - valLoss: 0.4336306154727936 - trainLoss: 0.42232072353363037\n",
      "cnt: 0 - valLoss: 0.4336296617984772 - trainLoss: 0.42231807112693787\n",
      "cnt: 0 - valLoss: 0.4336286783218384 - trainLoss: 0.42231547832489014\n",
      "cnt: 0 - valLoss: 0.4336276948451996 - trainLoss: 0.42231282591819763\n",
      "cnt: 0 - valLoss: 0.4336267411708832 - trainLoss: 0.42231011390686035\n",
      "cnt: 0 - valLoss: 0.43362581729888916 - trainLoss: 0.422307550907135\n",
      "cnt: 0 - valLoss: 0.43362486362457275 - trainLoss: 0.4223048985004425\n",
      "cnt: 0 - valLoss: 0.43362390995025635 - trainLoss: 0.42230224609375\n",
      "cnt: 0 - valLoss: 0.43362292647361755 - trainLoss: 0.4222995936870575\n",
      "cnt: 0 - valLoss: 0.43362200260162354 - trainLoss: 0.422296941280365\n",
      "cnt: 0 - valLoss: 0.43362101912498474 - trainLoss: 0.4222942888736725\n",
      "cnt: 0 - valLoss: 0.43362006545066833 - trainLoss: 0.42229166626930237\n",
      "cnt: 0 - valLoss: 0.43361911177635193 - trainLoss: 0.42228901386260986\n",
      "cnt: 0 - valLoss: 0.4336181879043579 - trainLoss: 0.42228642106056213\n",
      "cnt: 0 - valLoss: 0.4336172044277191 - trainLoss: 0.42228376865386963\n",
      "cnt: 0 - valLoss: 0.4336162507534027 - trainLoss: 0.4222811162471771\n",
      "cnt: 0 - valLoss: 0.4336152970790863 - trainLoss: 0.42227843403816223\n",
      "cnt: 0 - valLoss: 0.4336143136024475 - trainLoss: 0.4222758412361145\n",
      "cnt: 0 - valLoss: 0.4336133897304535 - trainLoss: 0.422273188829422\n",
      "cnt: 0 - valLoss: 0.4336124360561371 - trainLoss: 0.4222705662250519\n",
      "cnt: 0 - valLoss: 0.4336114525794983 - trainLoss: 0.422267884016037\n",
      "cnt: 0 - valLoss: 0.4336104989051819 - trainLoss: 0.42226526141166687\n",
      "cnt: 0 - valLoss: 0.43360960483551025 - trainLoss: 0.42226260900497437\n",
      "cnt: 0 - valLoss: 0.43360862135887146 - trainLoss: 0.42225995659828186\n",
      "cnt: 0 - valLoss: 0.43360763788223267 - trainLoss: 0.42225730419158936\n",
      "cnt: 0 - valLoss: 0.43360668420791626 - trainLoss: 0.422254741191864\n",
      "cnt: 0 - valLoss: 0.43360576033592224 - trainLoss: 0.42225202918052673\n",
      "cnt: 0 - valLoss: 0.43360480666160583 - trainLoss: 0.422249436378479\n",
      "cnt: 0 - valLoss: 0.43360385298728943 - trainLoss: 0.4222467839717865\n",
      "cnt: 0 - valLoss: 0.4336029291152954 - trainLoss: 0.422244131565094\n",
      "cnt: 0 - valLoss: 0.433601975440979 - trainLoss: 0.4222414791584015\n",
      "cnt: 0 - valLoss: 0.4336010217666626 - trainLoss: 0.42223885655403137\n",
      "cnt: 0 - valLoss: 0.4336000978946686 - trainLoss: 0.42223620414733887\n",
      "cnt: 0 - valLoss: 0.4335991144180298 - trainLoss: 0.42223355174064636\n",
      "cnt: 0 - valLoss: 0.4335981607437134 - trainLoss: 0.42223089933395386\n",
      "cnt: 0 - valLoss: 0.433597207069397 - trainLoss: 0.42222827672958374\n",
      "cnt: 0 - valLoss: 0.4335962235927582 - trainLoss: 0.42222562432289124\n",
      "cnt: 0 - valLoss: 0.43359529972076416 - trainLoss: 0.42222297191619873\n",
      "cnt: 0 - valLoss: 0.43359434604644775 - trainLoss: 0.4222203195095062\n",
      "cnt: 0 - valLoss: 0.4335934519767761 - trainLoss: 0.4222177267074585\n",
      "cnt: 0 - valLoss: 0.43359246850013733 - trainLoss: 0.4222150444984436\n",
      "cnt: 0 - valLoss: 0.4335915148258209 - trainLoss: 0.4222123920917511\n",
      "cnt: 0 - valLoss: 0.4335906207561493 - trainLoss: 0.42220979928970337\n",
      "cnt: 0 - valLoss: 0.4335896372795105 - trainLoss: 0.42220714688301086\n",
      "cnt: 0 - valLoss: 0.4335886836051941 - trainLoss: 0.42220449447631836\n",
      "cnt: 0 - valLoss: 0.4335877597332001 - trainLoss: 0.42220187187194824\n",
      "cnt: 0 - valLoss: 0.43358680605888367 - trainLoss: 0.42219921946525574\n",
      "cnt: 0 - valLoss: 0.43358591198921204 - trainLoss: 0.4221965968608856\n",
      "cnt: 0 - valLoss: 0.43358492851257324 - trainLoss: 0.4221939444541931\n",
      "cnt: 0 - valLoss: 0.433584064245224 - trainLoss: 0.4221912920475006\n",
      "cnt: 0 - valLoss: 0.4335830807685852 - trainLoss: 0.4221886992454529\n",
      "cnt: 0 - valLoss: 0.4335820972919464 - trainLoss: 0.4221859872341156\n",
      "cnt: 0 - valLoss: 0.4335812032222748 - trainLoss: 0.4221833348274231\n",
      "cnt: 0 - valLoss: 0.433580219745636 - trainLoss: 0.42218077182769775\n",
      "cnt: 0 - valLoss: 0.43357935547828674 - trainLoss: 0.42217811942100525\n",
      "cnt: 0 - valLoss: 0.43357837200164795 - trainLoss: 0.42217546701431274\n",
      "cnt: 0 - valLoss: 0.43357744812965393 - trainLoss: 0.42217281460762024\n",
      "cnt: 0 - valLoss: 0.43357646465301514 - trainLoss: 0.42217016220092773\n",
      "cnt: 0 - valLoss: 0.4335755407810211 - trainLoss: 0.42216750979423523\n",
      "cnt: 0 - valLoss: 0.4335746467113495 - trainLoss: 0.4221648871898651\n",
      "cnt: 0 - valLoss: 0.4335736930370331 - trainLoss: 0.4221622943878174\n",
      "cnt: 0 - valLoss: 0.4335727393627167 - trainLoss: 0.4221596419811249\n",
      "cnt: 0 - valLoss: 0.43357181549072266 - trainLoss: 0.4221569895744324\n",
      "cnt: 0 - valLoss: 0.43357086181640625 - trainLoss: 0.42215433716773987\n",
      "cnt: 0 - valLoss: 0.43356990814208984 - trainLoss: 0.42215168476104736\n",
      "cnt: 0 - valLoss: 0.4335689842700958 - trainLoss: 0.42214900255203247\n",
      "cnt: 0 - valLoss: 0.4335680902004242 - trainLoss: 0.42214635014533997\n",
      "cnt: 0 - valLoss: 0.4335671663284302 - trainLoss: 0.42214375734329224\n",
      "cnt: 0 - valLoss: 0.4335661828517914 - trainLoss: 0.4221411347389221\n",
      "cnt: 0 - valLoss: 0.43356525897979736 - trainLoss: 0.4221384823322296\n",
      "cnt: 0 - valLoss: 0.43356433510780334 - trainLoss: 0.4221358299255371\n",
      "cnt: 0 - valLoss: 0.4335634112358093 - trainLoss: 0.4221332371234894\n",
      "cnt: 0 - valLoss: 0.4335624575614929 - trainLoss: 0.4221305549144745\n",
      "cnt: 0 - valLoss: 0.4335615038871765 - trainLoss: 0.4221278727054596\n",
      "cnt: 0 - valLoss: 0.4335606098175049 - trainLoss: 0.4221252501010895\n",
      "cnt: 0 - valLoss: 0.4335596263408661 - trainLoss: 0.42212265729904175\n",
      "cnt: 0 - valLoss: 0.43355873227119446 - trainLoss: 0.42212000489234924\n",
      "cnt: 0 - valLoss: 0.43355777859687805 - trainLoss: 0.42211735248565674\n",
      "cnt: 0 - valLoss: 0.43355685472488403 - trainLoss: 0.42211470007896423\n",
      "cnt: 0 - valLoss: 0.4335559606552124 - trainLoss: 0.42211204767227173\n",
      "cnt: 0 - valLoss: 0.433555006980896 - trainLoss: 0.4221094250679016\n",
      "cnt: 0 - valLoss: 0.4335540533065796 - trainLoss: 0.4221068024635315\n",
      "cnt: 0 - valLoss: 0.43355312943458557 - trainLoss: 0.422104150056839\n",
      "cnt: 0 - valLoss: 0.43355220556259155 - trainLoss: 0.4221014976501465\n",
      "cnt: 0 - valLoss: 0.43355128169059753 - trainLoss: 0.422098845243454\n",
      "cnt: 0 - valLoss: 0.43355032801628113 - trainLoss: 0.4220961928367615\n",
      "cnt: 0 - valLoss: 0.4335493743419647 - trainLoss: 0.42209354043006897\n",
      "cnt: 0 - valLoss: 0.4335484802722931 - trainLoss: 0.42209097743034363\n",
      "cnt: 0 - valLoss: 0.4335475564002991 - trainLoss: 0.4220883250236511\n",
      "cnt: 0 - valLoss: 0.43354660272598267 - trainLoss: 0.4220856726169586\n",
      "cnt: 0 - valLoss: 0.43354564905166626 - trainLoss: 0.4220830202102661\n",
      "cnt: 0 - valLoss: 0.4335448145866394 - trainLoss: 0.4220803678035736\n",
      "cnt: 0 - valLoss: 0.433543860912323 - trainLoss: 0.4220777153968811\n",
      "cnt: 0 - valLoss: 0.4335429072380066 - trainLoss: 0.422075092792511\n",
      "cnt: 0 - valLoss: 0.43354201316833496 - trainLoss: 0.42207249999046326\n",
      "cnt: 0 - valLoss: 0.43354102969169617 - trainLoss: 0.42206984758377075\n",
      "cnt: 0 - valLoss: 0.43354013562202454 - trainLoss: 0.42206716537475586\n",
      "cnt: 0 - valLoss: 0.4335392415523529 - trainLoss: 0.42206451296806335\n",
      "cnt: 0 - valLoss: 0.4335382878780365 - trainLoss: 0.42206186056137085\n",
      "cnt: 0 - valLoss: 0.4335373342037201 - trainLoss: 0.4220592677593231\n",
      "cnt: 0 - valLoss: 0.43353644013404846 - trainLoss: 0.4220566153526306\n",
      "cnt: 0 - valLoss: 0.43353548645973206 - trainLoss: 0.4220539927482605\n",
      "cnt: 0 - valLoss: 0.43353456258773804 - trainLoss: 0.422051340341568\n",
      "cnt: 0 - valLoss: 0.4335336685180664 - trainLoss: 0.4220486879348755\n",
      "cnt: 0 - valLoss: 0.43353271484375 - trainLoss: 0.42204606533050537\n",
      "cnt: 0 - valLoss: 0.4335317611694336 - trainLoss: 0.4220433831214905\n",
      "cnt: 0 - valLoss: 0.43353092670440674 - trainLoss: 0.42204082012176514\n",
      "cnt: 0 - valLoss: 0.43352997303009033 - trainLoss: 0.42203816771507263\n",
      "cnt: 0 - valLoss: 0.4335290789604187 - trainLoss: 0.4220355153083801\n",
      "cnt: 0 - valLoss: 0.4335281252861023 - trainLoss: 0.4220328629016876\n",
      "cnt: 0 - valLoss: 0.43352723121643066 - trainLoss: 0.4220302104949951\n",
      "cnt: 0 - valLoss: 0.43352627754211426 - trainLoss: 0.4220275580883026\n",
      "cnt: 0 - valLoss: 0.43352535367012024 - trainLoss: 0.4220249652862549\n",
      "cnt: 0 - valLoss: 0.4335244297981262 - trainLoss: 0.4220223128795624\n",
      "cnt: 0 - valLoss: 0.4335235357284546 - trainLoss: 0.4220196604728699\n",
      "cnt: 0 - valLoss: 0.43352261185646057 - trainLoss: 0.42201700806617737\n",
      "cnt: 0 - valLoss: 0.43352165818214417 - trainLoss: 0.42201435565948486\n",
      "cnt: 0 - valLoss: 0.43352076411247253 - trainLoss: 0.4220117926597595\n",
      "cnt: 0 - valLoss: 0.43351981043815613 - trainLoss: 0.422009140253067\n",
      "cnt: 0 - valLoss: 0.4335189163684845 - trainLoss: 0.4220064878463745\n",
      "cnt: 0 - valLoss: 0.43351802229881287 - trainLoss: 0.422003835439682\n",
      "cnt: 0 - valLoss: 0.43351706862449646 - trainLoss: 0.4220011830329895\n",
      "cnt: 0 - valLoss: 0.43351617455482483 - trainLoss: 0.421998530626297\n",
      "cnt: 0 - valLoss: 0.4335152208805084 - trainLoss: 0.4219959080219269\n",
      "cnt: 0 - valLoss: 0.4335142970085144 - trainLoss: 0.42199331521987915\n",
      "cnt: 0 - valLoss: 0.4335133731365204 - trainLoss: 0.42199066281318665\n",
      "cnt: 0 - valLoss: 0.43351247906684875 - trainLoss: 0.42198801040649414\n",
      "cnt: 0 - valLoss: 0.4335115849971771 - trainLoss: 0.42198532819747925\n",
      "cnt: 0 - valLoss: 0.4335106909275055 - trainLoss: 0.42198267579078674\n",
      "cnt: 0 - valLoss: 0.4335097372531891 - trainLoss: 0.42198002338409424\n",
      "cnt: 0 - valLoss: 0.43350887298583984 - trainLoss: 0.4219774305820465\n",
      "cnt: 0 - valLoss: 0.4335079491138458 - trainLoss: 0.4219748079776764\n",
      "cnt: 0 - valLoss: 0.4335069954395294 - trainLoss: 0.4219721555709839\n",
      "cnt: 0 - valLoss: 0.4335061311721802 - trainLoss: 0.42196956276893616\n",
      "cnt: 0 - valLoss: 0.43350520730018616 - trainLoss: 0.42196688055992126\n",
      "cnt: 0 - valLoss: 0.43350428342819214 - trainLoss: 0.42196422815322876\n",
      "cnt: 0 - valLoss: 0.4335033893585205 - trainLoss: 0.42196163535118103\n",
      "cnt: 0 - valLoss: 0.4335024654865265 - trainLoss: 0.4219589829444885\n",
      "cnt: 0 - valLoss: 0.43350154161453247 - trainLoss: 0.421956330537796\n",
      "cnt: 0 - valLoss: 0.43350064754486084 - trainLoss: 0.4219537079334259\n",
      "cnt: 0 - valLoss: 0.4334997236728668 - trainLoss: 0.421951025724411\n",
      "cnt: 0 - valLoss: 0.4334988594055176 - trainLoss: 0.4219484329223633\n",
      "cnt: 0 - valLoss: 0.43349796533584595 - trainLoss: 0.4219457805156708\n",
      "cnt: 0 - valLoss: 0.43349701166152954 - trainLoss: 0.42194312810897827\n",
      "cnt: 0 - valLoss: 0.4334961175918579 - trainLoss: 0.42194047570228577\n",
      "cnt: 0 - valLoss: 0.43349525332450867 - trainLoss: 0.42193782329559326\n",
      "cnt: 0 - valLoss: 0.43349429965019226 - trainLoss: 0.42193523049354553\n",
      "cnt: 0 - valLoss: 0.43349337577819824 - trainLoss: 0.421932578086853\n",
      "cnt: 0 - valLoss: 0.4334924817085266 - trainLoss: 0.4219299554824829\n",
      "cnt: 0 - valLoss: 0.4334915578365326 - trainLoss: 0.4219273030757904\n",
      "cnt: 0 - valLoss: 0.43349066376686096 - trainLoss: 0.4219246506690979\n",
      "cnt: 0 - valLoss: 0.43348976969718933 - trainLoss: 0.4219219982624054\n",
      "cnt: 0 - valLoss: 0.4334888160228729 - trainLoss: 0.4219193756580353\n",
      "cnt: 0 - valLoss: 0.43348798155784607 - trainLoss: 0.4219167232513428\n",
      "cnt: 0 - valLoss: 0.43348702788352966 - trainLoss: 0.42191413044929504\n",
      "cnt: 0 - valLoss: 0.43348607420921326 - trainLoss: 0.42191147804260254\n",
      "cnt: 0 - valLoss: 0.433485209941864 - trainLoss: 0.42190882563591003\n",
      "cnt: 0 - valLoss: 0.43348428606987 - trainLoss: 0.42190617322921753\n",
      "cnt: 0 - valLoss: 0.43348342180252075 - trainLoss: 0.4219035506248474\n",
      "cnt: 0 - valLoss: 0.43348246812820435 - trainLoss: 0.4219009280204773\n",
      "cnt: 0 - valLoss: 0.4334816336631775 - trainLoss: 0.4218982756137848\n",
      "cnt: 0 - valLoss: 0.4334806799888611 - trainLoss: 0.4218956232070923\n",
      "cnt: 0 - valLoss: 0.43347978591918945 - trainLoss: 0.4218929708003998\n",
      "cnt: 0 - valLoss: 0.4334788918495178 - trainLoss: 0.4218903183937073\n",
      "cnt: 0 - valLoss: 0.4334779679775238 - trainLoss: 0.42188772559165955\n",
      "cnt: 0 - valLoss: 0.4334770441055298 - trainLoss: 0.42188510298728943\n",
      "cnt: 0 - valLoss: 0.43347615003585815 - trainLoss: 0.4218824505805969\n",
      "cnt: 0 - valLoss: 0.4334752857685089 - trainLoss: 0.4218797981739044\n",
      "cnt: 0 - valLoss: 0.4334743320941925 - trainLoss: 0.4218771457672119\n",
      "cnt: 0 - valLoss: 0.43347346782684326 - trainLoss: 0.4218745827674866\n",
      "cnt: 0 - valLoss: 0.43347254395484924 - trainLoss: 0.42187193036079407\n",
      "cnt: 0 - valLoss: 0.4334716498851776 - trainLoss: 0.42186927795410156\n",
      "cnt: 0 - valLoss: 0.4334707260131836 - trainLoss: 0.42186662554740906\n",
      "cnt: 0 - valLoss: 0.43346983194351196 - trainLoss: 0.42186394333839417\n",
      "cnt: 0 - valLoss: 0.43346893787384033 - trainLoss: 0.42186129093170166\n",
      "cnt: 0 - valLoss: 0.4334680438041687 - trainLoss: 0.42185869812965393\n",
      "cnt: 0 - valLoss: 0.43346714973449707 - trainLoss: 0.4218560755252838\n",
      "cnt: 0 - valLoss: 0.43346622586250305 - trainLoss: 0.4218534231185913\n",
      "cnt: 0 - valLoss: 0.4334653317928314 - trainLoss: 0.4218507707118988\n",
      "cnt: 0 - valLoss: 0.4334644079208374 - trainLoss: 0.4218481183052063\n",
      "cnt: 0 - valLoss: 0.43346354365348816 - trainLoss: 0.4218454957008362\n",
      "cnt: 0 - valLoss: 0.43346264958381653 - trainLoss: 0.42184290289878845\n",
      "cnt: 0 - valLoss: 0.4334617257118225 - trainLoss: 0.42184025049209595\n",
      "cnt: 0 - valLoss: 0.4334608018398285 - trainLoss: 0.42183759808540344\n",
      "cnt: 0 - valLoss: 0.43345993757247925 - trainLoss: 0.42183494567871094\n",
      "cnt: 0 - valLoss: 0.4334590435028076 - trainLoss: 0.42183229327201843\n",
      "cnt: 0 - valLoss: 0.433458149433136 - trainLoss: 0.4218296706676483\n",
      "cnt: 0 - valLoss: 0.43345722556114197 - trainLoss: 0.4218270182609558\n",
      "cnt: 0 - valLoss: 0.43345633149147034 - trainLoss: 0.4218243956565857\n",
      "cnt: 0 - valLoss: 0.4334554374217987 - trainLoss: 0.42182180285453796\n",
      "cnt: 0 - valLoss: 0.4334545433521271 - trainLoss: 0.42181915044784546\n",
      "cnt: 0 - valLoss: 0.43345364928245544 - trainLoss: 0.42181649804115295\n",
      "cnt: 0 - valLoss: 0.4334527254104614 - trainLoss: 0.42181387543678284\n",
      "cnt: 0 - valLoss: 0.43345189094543457 - trainLoss: 0.42181122303009033\n",
      "cnt: 0 - valLoss: 0.43345093727111816 - trainLoss: 0.4218085706233978\n",
      "cnt: 0 - valLoss: 0.43345004320144653 - trainLoss: 0.4218059480190277\n",
      "cnt: 0 - valLoss: 0.4334491491317749 - trainLoss: 0.4218032956123352\n",
      "cnt: 0 - valLoss: 0.43344828486442566 - trainLoss: 0.4218006432056427\n",
      "cnt: 0 - valLoss: 0.43344733119010925 - trainLoss: 0.42179805040359497\n",
      "cnt: 0 - valLoss: 0.4334464371204376 - trainLoss: 0.42179539799690247\n",
      "cnt: 0 - valLoss: 0.433445543050766 - trainLoss: 0.42179277539253235\n",
      "cnt: 0 - valLoss: 0.43344467878341675 - trainLoss: 0.42179012298583984\n",
      "cnt: 0 - valLoss: 0.4334438145160675 - trainLoss: 0.4217875003814697\n",
      "cnt: 0 - valLoss: 0.4334428906440735 - trainLoss: 0.4217848479747772\n",
      "cnt: 0 - valLoss: 0.43344196677207947 - trainLoss: 0.4217821955680847\n",
      "cnt: 0 - valLoss: 0.43344107270240784 - trainLoss: 0.4217795431613922\n",
      "cnt: 0 - valLoss: 0.4334402084350586 - trainLoss: 0.4217769503593445\n",
      "cnt: 0 - valLoss: 0.4334392845630646 - trainLoss: 0.42177432775497437\n",
      "cnt: 0 - valLoss: 0.43343842029571533 - trainLoss: 0.42177167534828186\n",
      "cnt: 0 - valLoss: 0.4334375560283661 - trainLoss: 0.42176902294158936\n",
      "cnt: 0 - valLoss: 0.43343663215637207 - trainLoss: 0.42176640033721924\n",
      "cnt: 0 - valLoss: 0.4334357678890228 - trainLoss: 0.42176374793052673\n",
      "cnt: 0 - valLoss: 0.4334348738193512 - trainLoss: 0.421761155128479\n",
      "cnt: 0 - valLoss: 0.4334339499473572 - trainLoss: 0.4217585027217865\n",
      "cnt: 0 - valLoss: 0.43343305587768555 - trainLoss: 0.421755850315094\n",
      "cnt: 0 - valLoss: 0.4334321916103363 - trainLoss: 0.4217531979084015\n",
      "cnt: 0 - valLoss: 0.4334312975406647 - trainLoss: 0.42175057530403137\n",
      "cnt: 0 - valLoss: 0.43343037366867065 - trainLoss: 0.42174795269966125\n",
      "cnt: 0 - valLoss: 0.4334295094013214 - trainLoss: 0.42174530029296875\n",
      "cnt: 0 - valLoss: 0.43342864513397217 - trainLoss: 0.42174264788627625\n",
      "cnt: 0 - valLoss: 0.43342769145965576 - trainLoss: 0.4217400550842285\n",
      "cnt: 0 - valLoss: 0.4334268569946289 - trainLoss: 0.421737402677536\n",
      "cnt: 0 - valLoss: 0.4334259331226349 - trainLoss: 0.4217347800731659\n",
      "cnt: 0 - valLoss: 0.43342503905296326 - trainLoss: 0.4217321276664734\n",
      "cnt: 0 - valLoss: 0.433424174785614 - trainLoss: 0.4217294752597809\n",
      "cnt: 0 - valLoss: 0.43342325091362 - trainLoss: 0.42172685265541077\n",
      "cnt: 0 - valLoss: 0.43342238664627075 - trainLoss: 0.42172420024871826\n",
      "cnt: 0 - valLoss: 0.4334214925765991 - trainLoss: 0.42172160744667053\n",
      "cnt: 0 - valLoss: 0.4334205985069275 - trainLoss: 0.421718955039978\n",
      "cnt: 0 - valLoss: 0.43341973423957825 - trainLoss: 0.4217163026332855\n",
      "cnt: 0 - valLoss: 0.4334188401699066 - trainLoss: 0.4217136800289154\n",
      "cnt: 0 - valLoss: 0.4334179162979126 - trainLoss: 0.4217110276222229\n",
      "cnt: 0 - valLoss: 0.43341702222824097 - trainLoss: 0.4217084050178528\n",
      "cnt: 0 - valLoss: 0.4334161877632141 - trainLoss: 0.4217057526111603\n",
      "cnt: 0 - valLoss: 0.4334152638912201 - trainLoss: 0.42170315980911255\n",
      "cnt: 0 - valLoss: 0.43341436982154846 - trainLoss: 0.42170050740242004\n",
      "cnt: 0 - valLoss: 0.4334135055541992 - trainLoss: 0.4216978847980499\n",
      "cnt: 0 - valLoss: 0.4334126114845276 - trainLoss: 0.4216952323913574\n",
      "cnt: 0 - valLoss: 0.43341174721717834 - trainLoss: 0.4216925799846649\n",
      "cnt: 0 - valLoss: 0.4334108531475067 - trainLoss: 0.4216899275779724\n",
      "cnt: 0 - valLoss: 0.4334099590778351 - trainLoss: 0.4216872751712799\n",
      "cnt: 0 - valLoss: 0.43340909481048584 - trainLoss: 0.42168471217155457\n",
      "cnt: 0 - valLoss: 0.4334082007408142 - trainLoss: 0.42168205976486206\n",
      "cnt: 0 - valLoss: 0.4334073066711426 - trainLoss: 0.42167940735816956\n",
      "cnt: 0 - valLoss: 0.43340644240379333 - trainLoss: 0.42167675495147705\n",
      "cnt: 0 - valLoss: 0.43340548872947693 - trainLoss: 0.42167413234710693\n",
      "cnt: 0 - valLoss: 0.4334046542644501 - trainLoss: 0.42167147994041443\n",
      "cnt: 0 - valLoss: 0.43340378999710083 - trainLoss: 0.4216688573360443\n",
      "cnt: 0 - valLoss: 0.4334028363227844 - trainLoss: 0.4216662049293518\n",
      "cnt: 0 - valLoss: 0.4334019720554352 - trainLoss: 0.4216635525226593\n",
      "cnt: 0 - valLoss: 0.43340107798576355 - trainLoss: 0.4216609597206116\n",
      "cnt: 0 - valLoss: 0.4334001839160919 - trainLoss: 0.42165830731391907\n",
      "cnt: 0 - valLoss: 0.4333993196487427 - trainLoss: 0.42165568470954895\n",
      "cnt: 0 - valLoss: 0.43339842557907104 - trainLoss: 0.42165303230285645\n",
      "cnt: 0 - valLoss: 0.4333975315093994 - trainLoss: 0.42165037989616394\n",
      "cnt: 0 - valLoss: 0.43339666724205017 - trainLoss: 0.4216477572917938\n",
      "cnt: 0 - valLoss: 0.4333958029747009 - trainLoss: 0.4216451048851013\n",
      "cnt: 0 - valLoss: 0.4333949089050293 - trainLoss: 0.4216425120830536\n",
      "cnt: 0 - valLoss: 0.43339401483535767 - trainLoss: 0.4216398596763611\n",
      "cnt: 0 - valLoss: 0.4333931505680084 - trainLoss: 0.4216372072696686\n",
      "cnt: 0 - valLoss: 0.4333922564983368 - trainLoss: 0.4216345548629761\n",
      "cnt: 0 - valLoss: 0.43339136242866516 - trainLoss: 0.42163193225860596\n",
      "cnt: 0 - valLoss: 0.4333904981613159 - trainLoss: 0.42162930965423584\n",
      "cnt: 0 - valLoss: 0.4333896040916443 - trainLoss: 0.42162665724754333\n",
      "cnt: 0 - valLoss: 0.43338871002197266 - trainLoss: 0.42162400484085083\n",
      "cnt: 0 - valLoss: 0.4333878457546234 - trainLoss: 0.4216214120388031\n",
      "cnt: 0 - valLoss: 0.4333869516849518 - trainLoss: 0.4216187596321106\n",
      "cnt: 0 - valLoss: 0.43338602781295776 - trainLoss: 0.4216161370277405\n",
      "cnt: 0 - valLoss: 0.4333851933479309 - trainLoss: 0.421613484621048\n",
      "cnt: 0 - valLoss: 0.4333842992782593 - trainLoss: 0.42161083221435547\n",
      "cnt: 0 - valLoss: 0.43338343501091003 - trainLoss: 0.42160817980766296\n",
      "cnt: 0 - valLoss: 0.4333825409412384 - trainLoss: 0.42160555720329285\n",
      "cnt: 0 - valLoss: 0.4333816170692444 - trainLoss: 0.4216029644012451\n",
      "cnt: 0 - valLoss: 0.43338072299957275 - trainLoss: 0.4216003119945526\n",
      "cnt: 0 - valLoss: 0.4333798587322235 - trainLoss: 0.4215976595878601\n",
      "cnt: 0 - valLoss: 0.43337902426719666 - trainLoss: 0.42159503698349\n",
      "cnt: 0 - valLoss: 0.43337810039520264 - trainLoss: 0.4215923845767975\n",
      "cnt: 0 - valLoss: 0.433377206325531 - trainLoss: 0.42158976197242737\n",
      "cnt: 0 - valLoss: 0.43337634205818176 - trainLoss: 0.42158710956573486\n",
      "cnt: 0 - valLoss: 0.43337544798851013 - trainLoss: 0.42158445715904236\n",
      "cnt: 0 - valLoss: 0.4333745837211609 - trainLoss: 0.42158186435699463\n",
      "cnt: 0 - valLoss: 0.43337368965148926 - trainLoss: 0.4215792119503021\n",
      "cnt: 0 - valLoss: 0.4333728551864624 - trainLoss: 0.421576589345932\n",
      "cnt: 0 - valLoss: 0.4333719313144684 - trainLoss: 0.4215739369392395\n",
      "cnt: 0 - valLoss: 0.43337103724479675 - trainLoss: 0.421571284532547\n",
      "cnt: 0 - valLoss: 0.4333701729774475 - trainLoss: 0.4215686619281769\n",
      "cnt: 0 - valLoss: 0.43336933851242065 - trainLoss: 0.4215660095214844\n",
      "cnt: 0 - valLoss: 0.43336838483810425 - trainLoss: 0.42156341671943665\n",
      "cnt: 0 - valLoss: 0.433367520570755 - trainLoss: 0.42156076431274414\n",
      "cnt: 0 - valLoss: 0.43336665630340576 - trainLoss: 0.42155808210372925\n",
      "cnt: 0 - valLoss: 0.43336576223373413 - trainLoss: 0.4215554893016815\n",
      "cnt: 0 - valLoss: 0.4333648979663849 - trainLoss: 0.4215528666973114\n",
      "cnt: 0 - valLoss: 0.43336406350135803 - trainLoss: 0.4215502142906189\n",
      "cnt: 0 - valLoss: 0.433363139629364 - trainLoss: 0.4215475618839264\n",
      "cnt: 0 - valLoss: 0.43336230516433716 - trainLoss: 0.4215449094772339\n",
      "cnt: 0 - valLoss: 0.4333614110946655 - trainLoss: 0.42154231667518616\n",
      "cnt: 0 - valLoss: 0.43336057662963867 - trainLoss: 0.42153963446617126\n",
      "cnt: 0 - valLoss: 0.43335965275764465 - trainLoss: 0.42153704166412354\n",
      "cnt: 0 - valLoss: 0.4333587884902954 - trainLoss: 0.42153438925743103\n",
      "cnt: 0 - valLoss: 0.43335792422294617 - trainLoss: 0.4215317368507385\n",
      "cnt: 0 - valLoss: 0.4333570599555969 - trainLoss: 0.4215291142463684\n",
      "cnt: 0 - valLoss: 0.4333561658859253 - trainLoss: 0.4215265214443207\n",
      "cnt: 0 - valLoss: 0.43335527181625366 - trainLoss: 0.4215238690376282\n",
      "cnt: 0 - valLoss: 0.4333544075489044 - trainLoss: 0.4215211868286133\n",
      "cnt: 0 - valLoss: 0.4333535432815552 - trainLoss: 0.42151859402656555\n",
      "cnt: 0 - valLoss: 0.4333527088165283 - trainLoss: 0.42151597142219543\n",
      "cnt: 0 - valLoss: 0.4333518445491791 - trainLoss: 0.42151331901550293\n",
      "cnt: 0 - valLoss: 0.43335095047950745 - trainLoss: 0.4215106666088104\n",
      "cnt: 0 - valLoss: 0.4333500266075134 - trainLoss: 0.4215080738067627\n",
      "cnt: 0 - valLoss: 0.4333491921424866 - trainLoss: 0.4215054214000702\n",
      "cnt: 0 - valLoss: 0.43334832787513733 - trainLoss: 0.4215027391910553\n",
      "cnt: 0 - valLoss: 0.4333474636077881 - trainLoss: 0.42150014638900757\n",
      "cnt: 0 - valLoss: 0.43334659934043884 - trainLoss: 0.42149749398231506\n",
      "cnt: 0 - valLoss: 0.4333457052707672 - trainLoss: 0.42149484157562256\n",
      "cnt: 0 - valLoss: 0.4333448112010956 - trainLoss: 0.4214922785758972\n",
      "cnt: 0 - valLoss: 0.43334394693374634 - trainLoss: 0.4214896261692047\n",
      "cnt: 0 - valLoss: 0.4333430826663971 - trainLoss: 0.4214870035648346\n",
      "cnt: 0 - valLoss: 0.43334224820137024 - trainLoss: 0.4214843213558197\n",
      "cnt: 0 - valLoss: 0.433341383934021 - trainLoss: 0.4214816987514496\n",
      "cnt: 0 - valLoss: 0.4333404302597046 - trainLoss: 0.42147907614707947\n",
      "cnt: 0 - valLoss: 0.43333956599235535 - trainLoss: 0.42147642374038696\n",
      "cnt: 0 - valLoss: 0.4333387315273285 - trainLoss: 0.42147377133369446\n",
      "cnt: 0 - valLoss: 0.43333786725997925 - trainLoss: 0.4214712083339691\n",
      "cnt: 0 - valLoss: 0.43333700299263 - trainLoss: 0.4214685261249542\n",
      "cnt: 0 - valLoss: 0.433336079120636 - trainLoss: 0.4214659035205841\n",
      "cnt: 0 - valLoss: 0.43333521485328674 - trainLoss: 0.4214632511138916\n",
      "cnt: 0 - valLoss: 0.4333343505859375 - trainLoss: 0.4214606285095215\n",
      "cnt: 0 - valLoss: 0.43333348631858826 - trainLoss: 0.42145803570747375\n",
      "cnt: 0 - valLoss: 0.4333325922489166 - trainLoss: 0.42145538330078125\n",
      "cnt: 0 - valLoss: 0.4333317279815674 - trainLoss: 0.42145273089408875\n",
      "cnt: 0 - valLoss: 0.43333083391189575 - trainLoss: 0.42145010828971863\n",
      "cnt: 0 - valLoss: 0.4333299696445465 - trainLoss: 0.4214474558830261\n",
      "cnt: 0 - valLoss: 0.43332919478416443 - trainLoss: 0.421444833278656\n",
      "cnt: 0 - valLoss: 0.4333283603191376 - trainLoss: 0.4214421808719635\n",
      "cnt: 0 - valLoss: 0.4333275854587555 - trainLoss: 0.42143958806991577\n",
      "cnt: 0 - valLoss: 0.43332675099372864 - trainLoss: 0.42143696546554565\n",
      "cnt: 0 - valLoss: 0.43332594633102417 - trainLoss: 0.42143431305885315\n",
      "cnt: 0 - valLoss: 0.4333251714706421 - trainLoss: 0.42143166065216064\n",
      "cnt: 0 - valLoss: 0.4333243668079376 - trainLoss: 0.4214290380477905\n",
      "cnt: 0 - valLoss: 0.43332356214523315 - trainLoss: 0.4214264452457428\n",
      "cnt: 0 - valLoss: 0.4333227872848511 - trainLoss: 0.4214237928390503\n",
      "cnt: 0 - valLoss: 0.43332192301750183 - trainLoss: 0.4214211404323578\n",
      "cnt: 0 - valLoss: 0.43332114815711975 - trainLoss: 0.42141857743263245\n",
      "cnt: 0 - valLoss: 0.4333203434944153 - trainLoss: 0.42141589522361755\n",
      "cnt: 0 - valLoss: 0.4333195090293884 - trainLoss: 0.42141324281692505\n",
      "cnt: 0 - valLoss: 0.43331873416900635 - trainLoss: 0.42141059041023254\n",
      "cnt: 0 - valLoss: 0.4333179295063019 - trainLoss: 0.42140793800354004\n",
      "cnt: 0 - valLoss: 0.4333171248435974 - trainLoss: 0.4214053452014923\n",
      "cnt: 0 - valLoss: 0.43331634998321533 - trainLoss: 0.4214026927947998\n",
      "cnt: 0 - valLoss: 0.4333154857158661 - trainLoss: 0.4214000701904297\n",
      "cnt: 0 - valLoss: 0.433314710855484 - trainLoss: 0.4213974177837372\n",
      "cnt: 0 - valLoss: 0.43331390619277954 - trainLoss: 0.42139479517936707\n",
      "cnt: 0 - valLoss: 0.4333131015300751 - trainLoss: 0.42139214277267456\n",
      "cnt: 0 - valLoss: 0.433312326669693 - trainLoss: 0.42138954997062683\n",
      "cnt: 0 - valLoss: 0.43331146240234375 - trainLoss: 0.4213868975639343\n",
      "cnt: 0 - valLoss: 0.43331068754196167 - trainLoss: 0.4213842749595642\n",
      "cnt: 0 - valLoss: 0.4333098828792572 - trainLoss: 0.4213816821575165\n",
      "cnt: 0 - valLoss: 0.43330904841423035 - trainLoss: 0.4213789999485016\n",
      "cnt: 0 - valLoss: 0.4333082139492035 - trainLoss: 0.4213763475418091\n",
      "cnt: 0 - valLoss: 0.4333074390888214 - trainLoss: 0.4213736951351166\n",
      "cnt: 0 - valLoss: 0.43330666422843933 - trainLoss: 0.42137110233306885\n",
      "cnt: 0 - valLoss: 0.4333057999610901 - trainLoss: 0.42136847972869873\n",
      "cnt: 0 - valLoss: 0.433305025100708 - trainLoss: 0.4213658273220062\n",
      "cnt: 0 - valLoss: 0.43330419063568115 - trainLoss: 0.4213632345199585\n",
      "cnt: 0 - valLoss: 0.4333034157752991 - trainLoss: 0.4213605523109436\n",
      "cnt: 0 - valLoss: 0.4333025813102722 - trainLoss: 0.4213579595088959\n",
      "cnt: 0 - valLoss: 0.43330177664756775 - trainLoss: 0.42135530710220337\n",
      "cnt: 0 - valLoss: 0.4333009719848633 - trainLoss: 0.42135265469551086\n",
      "cnt: 0 - valLoss: 0.4333001375198364 - trainLoss: 0.4213500916957855\n",
      "cnt: 0 - valLoss: 0.43329933285713196 - trainLoss: 0.421347439289093\n",
      "cnt: 0 - valLoss: 0.4332985281944275 - trainLoss: 0.4213447868824005\n",
      "cnt: 0 - valLoss: 0.43329769372940063 - trainLoss: 0.4213421642780304\n",
      "cnt: 0 - valLoss: 0.43329688906669617 - trainLoss: 0.4213395416736603\n",
      "cnt: 0 - valLoss: 0.4332960844039917 - trainLoss: 0.4213368892669678\n",
      "cnt: 0 - valLoss: 0.4332953095436096 - trainLoss: 0.42133423686027527\n",
      "cnt: 0 - valLoss: 0.4332944452762604 - trainLoss: 0.42133164405822754\n",
      "cnt: 0 - valLoss: 0.4332936704158783 - trainLoss: 0.4213290214538574\n",
      "cnt: 0 - valLoss: 0.43329286575317383 - trainLoss: 0.4213263690471649\n",
      "cnt: 0 - valLoss: 0.433292031288147 - trainLoss: 0.4213237166404724\n",
      "cnt: 0 - valLoss: 0.4332912266254425 - trainLoss: 0.4213210642337799\n",
      "cnt: 0 - valLoss: 0.4332904517650604 - trainLoss: 0.4213184416294098\n",
      "cnt: 0 - valLoss: 0.43328961730003357 - trainLoss: 0.42131584882736206\n",
      "cnt: 0 - valLoss: 0.4332887828350067 - trainLoss: 0.42131322622299194\n",
      "cnt: 0 - valLoss: 0.43328797817230225 - trainLoss: 0.42131057381629944\n",
      "cnt: 0 - valLoss: 0.4332871735095978 - trainLoss: 0.42130792140960693\n",
      "cnt: 0 - valLoss: 0.4332863390445709 - trainLoss: 0.4213052988052368\n",
      "cnt: 0 - valLoss: 0.43328553438186646 - trainLoss: 0.4213026463985443\n",
      "cnt: 0 - valLoss: 0.433284729719162 - trainLoss: 0.4213000535964966\n",
      "cnt: 0 - valLoss: 0.4332839548587799 - trainLoss: 0.42129743099212646\n",
      "cnt: 0 - valLoss: 0.43328312039375305 - trainLoss: 0.42129477858543396\n",
      "cnt: 0 - valLoss: 0.4332822859287262 - trainLoss: 0.42129212617874146\n",
      "cnt: 0 - valLoss: 0.43328145146369934 - trainLoss: 0.42128950357437134\n",
      "cnt: 0 - valLoss: 0.43328067660331726 - trainLoss: 0.42128685116767883\n",
      "cnt: 0 - valLoss: 0.4332798421382904 - trainLoss: 0.4212842583656311\n",
      "cnt: 0 - valLoss: 0.4332790672779083 - trainLoss: 0.4212816059589386\n",
      "cnt: 0 - valLoss: 0.4332782030105591 - trainLoss: 0.4212789833545685\n",
      "cnt: 0 - valLoss: 0.433277428150177 - trainLoss: 0.421276330947876\n",
      "cnt: 0 - valLoss: 0.43327656388282776 - trainLoss: 0.42127370834350586\n",
      "cnt: 0 - valLoss: 0.4332757592201233 - trainLoss: 0.42127105593681335\n",
      "cnt: 0 - valLoss: 0.4332749545574188 - trainLoss: 0.4212684631347656\n",
      "cnt: 0 - valLoss: 0.43327417969703674 - trainLoss: 0.4212658107280731\n",
      "cnt: 0 - valLoss: 0.4332733452320099 - trainLoss: 0.421263188123703\n",
      "cnt: 0 - valLoss: 0.4332725405693054 - trainLoss: 0.4212605357170105\n",
      "cnt: 0 - valLoss: 0.4332716763019562 - trainLoss: 0.42125794291496277\n",
      "cnt: 0 - valLoss: 0.4332709014415741 - trainLoss: 0.42125532031059265\n",
      "cnt: 0 - valLoss: 0.43327006697654724 - trainLoss: 0.42125266790390015\n",
      "cnt: 0 - valLoss: 0.43326929211616516 - trainLoss: 0.42125004529953003\n",
      "cnt: 0 - valLoss: 0.4332684576511383 - trainLoss: 0.4212474524974823\n",
      "cnt: 0 - valLoss: 0.43326765298843384 - trainLoss: 0.4212448000907898\n",
      "cnt: 0 - valLoss: 0.4332667887210846 - trainLoss: 0.4212421476840973\n",
      "cnt: 0 - valLoss: 0.4332660436630249 - trainLoss: 0.4212395250797272\n",
      "cnt: 0 - valLoss: 0.43326520919799805 - trainLoss: 0.42123687267303467\n",
      "cnt: 0 - valLoss: 0.4332644045352936 - trainLoss: 0.42123425006866455\n",
      "cnt: 0 - valLoss: 0.4332635998725891 - trainLoss: 0.4212316572666168\n",
      "cnt: 0 - valLoss: 0.43326276540756226 - trainLoss: 0.4212290346622467\n",
      "cnt: 0 - valLoss: 0.433261901140213 - trainLoss: 0.4212263822555542\n",
      "cnt: 0 - valLoss: 0.43326109647750854 - trainLoss: 0.4212237298488617\n",
      "cnt: 0 - valLoss: 0.4332602918148041 - trainLoss: 0.4212211072444916\n",
      "cnt: 0 - valLoss: 0.433259516954422 - trainLoss: 0.4212184548377991\n",
      "cnt: 0 - valLoss: 0.43325865268707275 - trainLoss: 0.42121586203575134\n",
      "cnt: 0 - valLoss: 0.4332578778266907 - trainLoss: 0.42121320962905884\n",
      "cnt: 0 - valLoss: 0.4332570731639862 - trainLoss: 0.42121055722236633\n",
      "cnt: 0 - valLoss: 0.43325626850128174 - trainLoss: 0.4212079644203186\n",
      "cnt: 0 - valLoss: 0.4332554042339325 - trainLoss: 0.4212053120136261\n",
      "cnt: 0 - valLoss: 0.433254599571228 - trainLoss: 0.4212026596069336\n",
      "cnt: 0 - valLoss: 0.43325376510620117 - trainLoss: 0.42120006680488586\n",
      "cnt: 0 - valLoss: 0.4332529604434967 - trainLoss: 0.42119744420051575\n",
      "cnt: 0 - valLoss: 0.43325215578079224 - trainLoss: 0.42119479179382324\n",
      "cnt: 0 - valLoss: 0.4332513213157654 - trainLoss: 0.42119213938713074\n",
      "cnt: 0 - valLoss: 0.4332505166530609 - trainLoss: 0.4211895167827606\n",
      "cnt: 0 - valLoss: 0.43324965238571167 - trainLoss: 0.4211869239807129\n",
      "cnt: 0 - valLoss: 0.4332488775253296 - trainLoss: 0.4211843013763428\n",
      "cnt: 0 - valLoss: 0.43324801325798035 - trainLoss: 0.42118164896965027\n",
      "cnt: 0 - valLoss: 0.43324726819992065 - trainLoss: 0.42117899656295776\n",
      "cnt: 0 - valLoss: 0.4332464337348938 - trainLoss: 0.42117640376091003\n",
      "cnt: 0 - valLoss: 0.43324562907218933 - trainLoss: 0.42117375135421753\n",
      "cnt: 0 - valLoss: 0.43324482440948486 - trainLoss: 0.4211711287498474\n",
      "cnt: 0 - valLoss: 0.433243989944458 - trainLoss: 0.4211685061454773\n",
      "cnt: 0 - valLoss: 0.43324315547943115 - trainLoss: 0.4211658537387848\n",
      "cnt: 0 - valLoss: 0.4332422912120819 - trainLoss: 0.42116326093673706\n",
      "cnt: 0 - valLoss: 0.43324151635169983 - trainLoss: 0.42116060853004456\n",
      "cnt: 0 - valLoss: 0.4332406520843506 - trainLoss: 0.42115798592567444\n",
      "cnt: 0 - valLoss: 0.4332398772239685 - trainLoss: 0.42115533351898193\n",
      "cnt: 0 - valLoss: 0.43323907256126404 - trainLoss: 0.4211527109146118\n",
      "cnt: 0 - valLoss: 0.43323826789855957 - trainLoss: 0.4211501181125641\n",
      "cnt: 0 - valLoss: 0.4332374036312103 - trainLoss: 0.4211474657058716\n",
      "cnt: 0 - valLoss: 0.43323656916618347 - trainLoss: 0.4211448132991791\n",
      "cnt: 0 - valLoss: 0.433235764503479 - trainLoss: 0.42114225029945374\n",
      "cnt: 0 - valLoss: 0.4332349896430969 - trainLoss: 0.42113956809043884\n",
      "cnt: 0 - valLoss: 0.4332341253757477 - trainLoss: 0.42113691568374634\n",
      "cnt: 0 - valLoss: 0.4332333207130432 - trainLoss: 0.4211343228816986\n",
      "cnt: 0 - valLoss: 0.43323251605033875 - trainLoss: 0.4211317002773285\n",
      "cnt: 0 - valLoss: 0.4332317113876343 - trainLoss: 0.421129047870636\n",
      "cnt: 0 - valLoss: 0.4332308769226074 - trainLoss: 0.4211263954639435\n",
      "cnt: 0 - valLoss: 0.43323007225990295 - trainLoss: 0.42112380266189575\n",
      "cnt: 0 - valLoss: 0.4332292675971985 - trainLoss: 0.42112118005752563\n",
      "cnt: 0 - valLoss: 0.43322843313217163 - trainLoss: 0.42111852765083313\n",
      "cnt: 0 - valLoss: 0.43322762846946716 - trainLoss: 0.421115905046463\n",
      "cnt: 0 - valLoss: 0.4332267642021179 - trainLoss: 0.4211133122444153\n",
      "cnt: 0 - valLoss: 0.43322598934173584 - trainLoss: 0.4211106598377228\n",
      "cnt: 0 - valLoss: 0.4332251250743866 - trainLoss: 0.4211080074310303\n",
      "cnt: 0 - valLoss: 0.43322432041168213 - trainLoss: 0.42110535502433777\n",
      "cnt: 0 - valLoss: 0.43322351574897766 - trainLoss: 0.42110276222229004\n",
      "cnt: 0 - valLoss: 0.4332226812839508 - trainLoss: 0.42110010981559753\n",
      "cnt: 0 - valLoss: 0.43322187662124634 - trainLoss: 0.4210975170135498\n",
      "cnt: 0 - valLoss: 0.43322107195854187 - trainLoss: 0.4210948646068573\n",
      "cnt: 0 - valLoss: 0.4332202672958374 - trainLoss: 0.4210922420024872\n",
      "cnt: 0 - valLoss: 0.43321943283081055 - trainLoss: 0.4210895895957947\n",
      "cnt: 0 - valLoss: 0.4332186281681061 - trainLoss: 0.42108696699142456\n",
      "cnt: 0 - valLoss: 0.43321776390075684 - trainLoss: 0.42108431458473206\n",
      "cnt: 0 - valLoss: 0.43321698904037476 - trainLoss: 0.4210817217826843\n",
      "cnt: 0 - valLoss: 0.4332161247730255 - trainLoss: 0.4210790693759918\n",
      "cnt: 0 - valLoss: 0.43321532011032104 - trainLoss: 0.4210764467716217\n",
      "cnt: 0 - valLoss: 0.4332145154476166 - trainLoss: 0.4210737943649292\n",
      "cnt: 0 - valLoss: 0.43321365118026733 - trainLoss: 0.4210711717605591\n",
      "cnt: 0 - valLoss: 0.43321287631988525 - trainLoss: 0.42106857895851135\n",
      "cnt: 0 - valLoss: 0.4332120716571808 - trainLoss: 0.42106595635414124\n",
      "cnt: 0 - valLoss: 0.43321123719215393 - trainLoss: 0.42106330394744873\n",
      "cnt: 0 - valLoss: 0.43321043252944946 - trainLoss: 0.421060711145401\n",
      "cnt: 0 - valLoss: 0.433209627866745 - trainLoss: 0.4210580885410309\n",
      "cnt: 0 - valLoss: 0.4332088530063629 - trainLoss: 0.4210554361343384\n",
      "cnt: 0 - valLoss: 0.43320798873901367 - trainLoss: 0.42105281352996826\n",
      "cnt: 0 - valLoss: 0.4332071840763092 - trainLoss: 0.42105016112327576\n",
      "cnt: 0 - valLoss: 0.43320637941360474 - trainLoss: 0.421047568321228\n",
      "cnt: 0 - valLoss: 0.4332055449485779 - trainLoss: 0.4210449159145355\n",
      "cnt: 0 - valLoss: 0.4332047402858734 - trainLoss: 0.4210422933101654\n",
      "cnt: 0 - valLoss: 0.43320393562316895 - trainLoss: 0.4210396707057953\n",
      "cnt: 0 - valLoss: 0.4332031011581421 - trainLoss: 0.4210370182991028\n",
      "cnt: 0 - valLoss: 0.43320223689079285 - trainLoss: 0.42103442549705505\n",
      "cnt: 0 - valLoss: 0.43320149183273315 - trainLoss: 0.42103180289268494\n",
      "cnt: 0 - valLoss: 0.4332006573677063 - trainLoss: 0.42102915048599243\n",
      "cnt: 0 - valLoss: 0.43319985270500183 - trainLoss: 0.4210265576839447\n",
      "cnt: 0 - valLoss: 0.43319904804229736 - trainLoss: 0.4210238754749298\n",
      "cnt: 0 - valLoss: 0.4331982135772705 - trainLoss: 0.4210212826728821\n",
      "cnt: 0 - valLoss: 0.43319737911224365 - trainLoss: 0.4210186302661896\n",
      "cnt: 0 - valLoss: 0.4331966042518616 - trainLoss: 0.42101606726646423\n",
      "cnt: 0 - valLoss: 0.4331957697868347 - trainLoss: 0.42101341485977173\n",
      "cnt: 0 - valLoss: 0.43319496512413025 - trainLoss: 0.4210107624530792\n",
      "cnt: 0 - valLoss: 0.4331941604614258 - trainLoss: 0.4210081398487091\n",
      "cnt: 0 - valLoss: 0.4331933259963989 - trainLoss: 0.421005517244339\n",
      "cnt: 0 - valLoss: 0.43319249153137207 - trainLoss: 0.4210028648376465\n",
      "cnt: 0 - valLoss: 0.43319171667099 - trainLoss: 0.42100030183792114\n",
      "cnt: 0 - valLoss: 0.43319085240364075 - trainLoss: 0.42099764943122864\n",
      "cnt: 0 - valLoss: 0.4331900477409363 - trainLoss: 0.42099499702453613\n",
      "cnt: 0 - valLoss: 0.4331892132759094 - trainLoss: 0.42099234461784363\n",
      "cnt: 0 - valLoss: 0.4331883490085602 - trainLoss: 0.4209897220134735\n",
      "cnt: 0 - valLoss: 0.4331876039505005 - trainLoss: 0.4209871292114258\n",
      "cnt: 0 - valLoss: 0.43318676948547363 - trainLoss: 0.4209844768047333\n",
      "cnt: 0 - valLoss: 0.43318596482276917 - trainLoss: 0.42098191380500793\n",
      "cnt: 0 - valLoss: 0.43318507075309753 - trainLoss: 0.42097926139831543\n",
      "cnt: 0 - valLoss: 0.4331841766834259 - trainLoss: 0.42097657918930054\n",
      "cnt: 0 - valLoss: 0.43318331241607666 - trainLoss: 0.4209740161895752\n",
      "cnt: 0 - valLoss: 0.4331824481487274 - trainLoss: 0.4209713637828827\n",
      "cnt: 0 - valLoss: 0.4331815540790558 - trainLoss: 0.42096880078315735\n",
      "cnt: 0 - valLoss: 0.43318066000938416 - trainLoss: 0.42096614837646484\n",
      "cnt: 0 - valLoss: 0.4331797957420349 - trainLoss: 0.4209635257720947\n",
      "cnt: 0 - valLoss: 0.43317893147468567 - trainLoss: 0.420960932970047\n",
      "cnt: 0 - valLoss: 0.43317803740501404 - trainLoss: 0.4209582805633545\n",
      "cnt: 0 - valLoss: 0.4331771731376648 - trainLoss: 0.4209556579589844\n",
      "cnt: 0 - valLoss: 0.43317627906799316 - trainLoss: 0.42095306515693665\n",
      "cnt: 0 - valLoss: 0.4331754148006439 - trainLoss: 0.42095044255256653\n",
      "cnt: 0 - valLoss: 0.4331745505332947 - trainLoss: 0.4209478199481964\n",
      "cnt: 0 - valLoss: 0.43317365646362305 - trainLoss: 0.4209451675415039\n",
      "cnt: 0 - valLoss: 0.4331727623939514 - trainLoss: 0.4209425747394562\n",
      "cnt: 0 - valLoss: 0.4331718981266022 - trainLoss: 0.42093995213508606\n",
      "cnt: 0 - valLoss: 0.43317100405693054 - trainLoss: 0.42093732953071594\n",
      "cnt: 0 - valLoss: 0.4331701099872589 - trainLoss: 0.4209347367286682\n",
      "cnt: 0 - valLoss: 0.43316924571990967 - trainLoss: 0.4209320843219757\n",
      "cnt: 0 - valLoss: 0.4331683814525604 - trainLoss: 0.42092952132225037\n",
      "cnt: 0 - valLoss: 0.43316754698753357 - trainLoss: 0.42092686891555786\n",
      "cnt: 0 - valLoss: 0.43316662311553955 - trainLoss: 0.42092424631118774\n",
      "cnt: 0 - valLoss: 0.4331657886505127 - trainLoss: 0.4209216237068176\n",
      "cnt: 0 - valLoss: 0.4331648647785187 - trainLoss: 0.4209190607070923\n",
      "cnt: 0 - valLoss: 0.4331640303134918 - trainLoss: 0.4209164083003998\n",
      "cnt: 0 - valLoss: 0.4331631064414978 - trainLoss: 0.4209137558937073\n",
      "cnt: 0 - valLoss: 0.43316227197647095 - trainLoss: 0.42091116309165955\n",
      "cnt: 0 - valLoss: 0.43316134810447693 - trainLoss: 0.42090854048728943\n",
      "cnt: 0 - valLoss: 0.4331605136394501 - trainLoss: 0.4209059774875641\n",
      "cnt: 0 - valLoss: 0.43315964937210083 - trainLoss: 0.4209033250808716\n",
      "cnt: 0 - valLoss: 0.4331587851047516 - trainLoss: 0.4209006726741791\n",
      "cnt: 0 - valLoss: 0.43315789103507996 - trainLoss: 0.42089810967445374\n",
      "cnt: 0 - valLoss: 0.4331569969654083 - trainLoss: 0.42089542746543884\n",
      "cnt: 0 - valLoss: 0.4331561326980591 - trainLoss: 0.4208928346633911\n",
      "cnt: 0 - valLoss: 0.43315526843070984 - trainLoss: 0.420890212059021\n",
      "cnt: 0 - valLoss: 0.4331544041633606 - trainLoss: 0.42088761925697327\n",
      "cnt: 0 - valLoss: 0.43315356969833374 - trainLoss: 0.42088499665260315\n",
      "cnt: 0 - valLoss: 0.4331526458263397 - trainLoss: 0.42088234424591064\n",
      "cnt: 0 - valLoss: 0.43315181136131287 - trainLoss: 0.4208797812461853\n",
      "cnt: 0 - valLoss: 0.43315088748931885 - trainLoss: 0.4208771288394928\n",
      "cnt: 0 - valLoss: 0.4331500828266144 - trainLoss: 0.4208745062351227\n",
      "cnt: 0 - valLoss: 0.43314918875694275 - trainLoss: 0.42087191343307495\n",
      "cnt: 0 - valLoss: 0.4331483244895935 - trainLoss: 0.42086926102638245\n",
      "cnt: 0 - valLoss: 0.43314746022224426 - trainLoss: 0.4208666682243347\n",
      "cnt: 0 - valLoss: 0.4331466257572174 - trainLoss: 0.420864075422287\n",
      "cnt: 0 - valLoss: 0.4331457018852234 - trainLoss: 0.4208614230155945\n",
      "cnt: 0 - valLoss: 0.43314486742019653 - trainLoss: 0.42085880041122437\n",
      "cnt: 0 - valLoss: 0.4331440031528473 - trainLoss: 0.42085617780685425\n",
      "cnt: 0 - valLoss: 0.43314313888549805 - trainLoss: 0.4208535850048065\n",
      "cnt: 0 - valLoss: 0.4331422448158264 - trainLoss: 0.420850932598114\n",
      "cnt: 0 - valLoss: 0.4331413805484772 - trainLoss: 0.42084836959838867\n",
      "cnt: 0 - valLoss: 0.43314051628112793 - trainLoss: 0.42084571719169617\n",
      "cnt: 0 - valLoss: 0.4331396818161011 - trainLoss: 0.42084312438964844\n",
      "cnt: 0 - valLoss: 0.43313881754875183 - trainLoss: 0.42084047198295593\n",
      "cnt: 0 - valLoss: 0.4331379532814026 - trainLoss: 0.4208378791809082\n",
      "cnt: 0 - valLoss: 0.43313708901405334 - trainLoss: 0.4208352565765381\n",
      "cnt: 0 - valLoss: 0.4331361949443817 - trainLoss: 0.4208326041698456\n",
      "cnt: 0 - valLoss: 0.43313533067703247 - trainLoss: 0.42083001136779785\n",
      "cnt: 0 - valLoss: 0.4331344962120056 - trainLoss: 0.42082738876342773\n",
      "cnt: 0 - valLoss: 0.43313363194465637 - trainLoss: 0.4208248257637024\n",
      "cnt: 0 - valLoss: 0.43313276767730713 - trainLoss: 0.4208221733570099\n",
      "cnt: 0 - valLoss: 0.43313196301460266 - trainLoss: 0.4208195209503174\n",
      "cnt: 0 - valLoss: 0.43313106894493103 - trainLoss: 0.42081689834594727\n",
      "cnt: 0 - valLoss: 0.4331302046775818 - trainLoss: 0.42081427574157715\n",
      "cnt: 0 - valLoss: 0.43312934041023254 - trainLoss: 0.4208117127418518\n",
      "cnt: 0 - valLoss: 0.4331284761428833 - trainLoss: 0.4208090603351593\n",
      "cnt: 0 - valLoss: 0.43312758207321167 - trainLoss: 0.4208064675331116\n",
      "cnt: 0 - valLoss: 0.4331267178058624 - trainLoss: 0.42080384492874146\n",
      "cnt: 0 - valLoss: 0.43312588334083557 - trainLoss: 0.42080119252204895\n",
      "cnt: 0 - valLoss: 0.4331250488758087 - trainLoss: 0.4207986295223236\n",
      "cnt: 0 - valLoss: 0.4331241846084595 - trainLoss: 0.4207959771156311\n",
      "cnt: 0 - valLoss: 0.43312329053878784 - trainLoss: 0.420793354511261\n",
      "cnt: 0 - valLoss: 0.433122456073761 - trainLoss: 0.42079076170921326\n",
      "cnt: 0 - valLoss: 0.43312159180641174 - trainLoss: 0.42078813910484314\n",
      "cnt: 0 - valLoss: 0.4331207275390625 - trainLoss: 0.420785516500473\n",
      "cnt: 0 - valLoss: 0.43311986327171326 - trainLoss: 0.4207829236984253\n",
      "cnt: 0 - valLoss: 0.433118999004364 - trainLoss: 0.4207802712917328\n",
      "cnt: 0 - valLoss: 0.43311816453933716 - trainLoss: 0.42077764868736267\n",
      "cnt: 0 - valLoss: 0.4331173002719879 - trainLoss: 0.42077502608299255\n",
      "cnt: 0 - valLoss: 0.43311643600463867 - trainLoss: 0.4207724332809448\n",
      "cnt: 0 - valLoss: 0.43311557173728943 - trainLoss: 0.4207697808742523\n",
      "cnt: 0 - valLoss: 0.4331147372722626 - trainLoss: 0.420767217874527\n",
      "cnt: 0 - valLoss: 0.43311387300491333 - trainLoss: 0.4207645654678345\n",
      "cnt: 0 - valLoss: 0.4331130087375641 - trainLoss: 0.42076197266578674\n",
      "cnt: 0 - valLoss: 0.43311214447021484 - trainLoss: 0.42075932025909424\n",
      "cnt: 0 - valLoss: 0.433111310005188 - trainLoss: 0.4207567572593689\n",
      "cnt: 0 - valLoss: 0.43311044573783875 - trainLoss: 0.4207541048526764\n",
      "cnt: 0 - valLoss: 0.4331095814704895 - trainLoss: 0.4207514524459839\n",
      "cnt: 0 - valLoss: 0.43310871720314026 - trainLoss: 0.42074885964393616\n",
      "cnt: 0 - valLoss: 0.4331078827381134 - trainLoss: 0.42074623703956604\n",
      "cnt: 0 - valLoss: 0.43310701847076416 - trainLoss: 0.4207436740398407\n",
      "cnt: 0 - valLoss: 0.4331061542034149 - trainLoss: 0.4207410216331482\n",
      "cnt: 0 - valLoss: 0.4331052899360657 - trainLoss: 0.42073845863342285\n",
      "cnt: 0 - valLoss: 0.43310442566871643 - trainLoss: 0.42073577642440796\n",
      "cnt: 0 - valLoss: 0.43310362100601196 - trainLoss: 0.42073318362236023\n",
      "cnt: 0 - valLoss: 0.43310272693634033 - trainLoss: 0.4207305610179901\n",
      "cnt: 0 - valLoss: 0.43310192227363586 - trainLoss: 0.4207279086112976\n",
      "cnt: 0 - valLoss: 0.43310099840164185 - trainLoss: 0.4207253158092499\n",
      "cnt: 0 - valLoss: 0.4331001937389374 - trainLoss: 0.42072269320487976\n",
      "cnt: 0 - valLoss: 0.43309932947158813 - trainLoss: 0.4207201302051544\n",
      "cnt: 0 - valLoss: 0.4330984950065613 - trainLoss: 0.4207174777984619\n",
      "cnt: 0 - valLoss: 0.43309763073921204 - trainLoss: 0.4207148551940918\n",
      "cnt: 0 - valLoss: 0.4330967664718628 - trainLoss: 0.42071226239204407\n",
      "cnt: 0 - valLoss: 0.43309590220451355 - trainLoss: 0.42070960998535156\n",
      "cnt: 0 - valLoss: 0.4330950677394867 - trainLoss: 0.42070698738098145\n",
      "cnt: 0 - valLoss: 0.43309420347213745 - trainLoss: 0.42070436477661133\n",
      "cnt: 0 - valLoss: 0.4330933690071106 - trainLoss: 0.4207017123699188\n",
      "cnt: 0 - valLoss: 0.43309247493743896 - trainLoss: 0.4206991493701935\n",
      "cnt: 0 - valLoss: 0.4330916702747345 - trainLoss: 0.42069655656814575\n",
      "cnt: 0 - valLoss: 0.43309080600738525 - trainLoss: 0.42069393396377563\n",
      "cnt: 0 - valLoss: 0.433089941740036 - trainLoss: 0.42069128155708313\n",
      "cnt: 0 - valLoss: 0.43308910727500916 - trainLoss: 0.4206887185573578\n",
      "cnt: 0 - valLoss: 0.4330882430076599 - trainLoss: 0.4206860661506653\n",
      "cnt: 0 - valLoss: 0.43308737874031067 - trainLoss: 0.4206834137439728\n",
      "cnt: 0 - valLoss: 0.4330865740776062 - trainLoss: 0.42068082094192505\n",
      "cnt: 0 - valLoss: 0.43308570981025696 - trainLoss: 0.42067816853523254\n",
      "cnt: 0 - valLoss: 0.4330848157405853 - trainLoss: 0.4206756055355072\n",
      "cnt: 0 - valLoss: 0.43308398127555847 - trainLoss: 0.4206729531288147\n",
      "cnt: 0 - valLoss: 0.4330831468105316 - trainLoss: 0.42067036032676697\n",
      "cnt: 0 - valLoss: 0.43308231234550476 - trainLoss: 0.42066773772239685\n",
      "cnt: 0 - valLoss: 0.43308141827583313 - trainLoss: 0.42066511511802673\n",
      "cnt: 0 - valLoss: 0.43308061361312866 - trainLoss: 0.420662522315979\n",
      "cnt: 0 - valLoss: 0.4330797493457794 - trainLoss: 0.4206598997116089\n",
      "cnt: 0 - valLoss: 0.4330788850784302 - trainLoss: 0.42065730690956116\n",
      "cnt: 0 - valLoss: 0.4330780506134033 - trainLoss: 0.42065462470054626\n",
      "cnt: 0 - valLoss: 0.4330771863460541 - trainLoss: 0.4206520617008209\n",
      "cnt: 0 - valLoss: 0.43307632207870483 - trainLoss: 0.4206494092941284\n",
      "cnt: 0 - valLoss: 0.43307551741600037 - trainLoss: 0.4206468164920807\n",
      "cnt: 0 - valLoss: 0.43307459354400635 - trainLoss: 0.42064419388771057\n",
      "cnt: 0 - valLoss: 0.4330737590789795 - trainLoss: 0.42064157128334045\n",
      "cnt: 0 - valLoss: 0.43307289481163025 - trainLoss: 0.4206389784812927\n",
      "cnt: 0 - valLoss: 0.4330720901489258 - trainLoss: 0.4206363260746002\n",
      "cnt: 0 - valLoss: 0.4330712556838989 - trainLoss: 0.4206337630748749\n",
      "cnt: 0 - valLoss: 0.43307042121887207 - trainLoss: 0.42063114047050476\n",
      "cnt: 0 - valLoss: 0.4330695569515228 - trainLoss: 0.42062848806381226\n",
      "cnt: 0 - valLoss: 0.4330686926841736 - trainLoss: 0.42062586545944214\n",
      "cnt: 0 - valLoss: 0.43306782841682434 - trainLoss: 0.4206232726573944\n",
      "cnt: 0 - valLoss: 0.4330669641494751 - trainLoss: 0.4206206500530243\n",
      "cnt: 0 - valLoss: 0.43306612968444824 - trainLoss: 0.42061805725097656\n",
      "cnt: 0 - valLoss: 0.433065265417099 - trainLoss: 0.42061543464660645\n",
      "cnt: 0 - valLoss: 0.43306446075439453 - trainLoss: 0.4206128716468811\n",
      "cnt: 0 - valLoss: 0.4330635964870453 - trainLoss: 0.4206102192401886\n",
      "cnt: 0 - valLoss: 0.43306273221969604 - trainLoss: 0.4206075668334961\n",
      "cnt: 0 - valLoss: 0.4330619275569916 - trainLoss: 0.42060500383377075\n",
      "cnt: 0 - valLoss: 0.43306106328964233 - trainLoss: 0.42060232162475586\n",
      "cnt: 0 - valLoss: 0.4330601990222931 - trainLoss: 0.4205997586250305\n",
      "cnt: 0 - valLoss: 0.43305933475494385 - trainLoss: 0.420597106218338\n",
      "cnt: 0 - valLoss: 0.433058500289917 - trainLoss: 0.4205945134162903\n",
      "cnt: 0 - valLoss: 0.43305763602256775 - trainLoss: 0.42059189081192017\n",
      "cnt: 0 - valLoss: 0.4330567717552185 - trainLoss: 0.42058926820755005\n",
      "cnt: 0 - valLoss: 0.43305596709251404 - trainLoss: 0.4205866754055023\n",
      "cnt: 0 - valLoss: 0.4330551028251648 - trainLoss: 0.4205840528011322\n",
      "cnt: 0 - valLoss: 0.4330542981624603 - trainLoss: 0.4205814599990845\n",
      "cnt: 0 - valLoss: 0.4330534338951111 - trainLoss: 0.42057883739471436\n",
      "cnt: 0 - valLoss: 0.43305256962776184 - trainLoss: 0.42057618498802185\n",
      "cnt: 0 - valLoss: 0.4330517053604126 - trainLoss: 0.42057356238365173\n",
      "cnt: 0 - valLoss: 0.43305087089538574 - trainLoss: 0.4205709993839264\n",
      "cnt: 0 - valLoss: 0.4330500364303589 - trainLoss: 0.42056840658187866\n",
      "cnt: 0 - valLoss: 0.43304914236068726 - trainLoss: 0.42056578397750854\n",
      "cnt: 0 - valLoss: 0.4330483376979828 - trainLoss: 0.42056313157081604\n",
      "cnt: 0 - valLoss: 0.4330475330352783 - trainLoss: 0.4205605089664459\n",
      "cnt: 0 - valLoss: 0.4330466687679291 - trainLoss: 0.4205579161643982\n",
      "cnt: 0 - valLoss: 0.43304580450057983 - trainLoss: 0.4205552935600281\n",
      "cnt: 0 - valLoss: 0.4330449402332306 - trainLoss: 0.42055267095565796\n",
      "cnt: 0 - valLoss: 0.43304407596588135 - trainLoss: 0.42055001854896545\n",
      "cnt: 0 - valLoss: 0.4330432713031769 - trainLoss: 0.4205474555492401\n",
      "cnt: 0 - valLoss: 0.43304240703582764 - trainLoss: 0.4205448627471924\n",
      "cnt: 0 - valLoss: 0.43304160237312317 - trainLoss: 0.42054224014282227\n",
      "cnt: 0 - valLoss: 0.4330407381057739 - trainLoss: 0.42053958773612976\n",
      "cnt: 0 - valLoss: 0.43303990364074707 - trainLoss: 0.4205370247364044\n",
      "cnt: 0 - valLoss: 0.433039128780365 - trainLoss: 0.4205344021320343\n",
      "cnt: 0 - valLoss: 0.43303826451301575 - trainLoss: 0.4205318093299866\n",
      "cnt: 0 - valLoss: 0.4330374002456665 - trainLoss: 0.42052915692329407\n",
      "cnt: 0 - valLoss: 0.43303653597831726 - trainLoss: 0.42052656412124634\n",
      "cnt: 0 - valLoss: 0.4330357313156128 - trainLoss: 0.4205239713191986\n",
      "cnt: 0 - valLoss: 0.4330349266529083 - trainLoss: 0.4205213189125061\n",
      "cnt: 0 - valLoss: 0.4330340623855591 - trainLoss: 0.420518696308136\n",
      "cnt: 0 - valLoss: 0.43303319811820984 - trainLoss: 0.42051610350608826\n",
      "cnt: 0 - valLoss: 0.43303239345550537 - trainLoss: 0.42051348090171814\n",
      "cnt: 0 - valLoss: 0.4330315887928009 - trainLoss: 0.420510858297348\n",
      "cnt: 0 - valLoss: 0.43303072452545166 - trainLoss: 0.4205082654953003\n",
      "cnt: 0 - valLoss: 0.4330298602581024 - trainLoss: 0.4205056130886078\n",
      "cnt: 0 - valLoss: 0.43302905559539795 - trainLoss: 0.42050305008888245\n",
      "cnt: 0 - valLoss: 0.4330281913280487 - trainLoss: 0.42050036787986755\n",
      "cnt: 0 - valLoss: 0.43302738666534424 - trainLoss: 0.4204978048801422\n",
      "cnt: 0 - valLoss: 0.433026522397995 - trainLoss: 0.4204951524734497\n",
      "cnt: 0 - valLoss: 0.43302565813064575 - trainLoss: 0.42049258947372437\n",
      "cnt: 0 - valLoss: 0.4330248534679413 - trainLoss: 0.42048993706703186\n",
      "cnt: 0 - valLoss: 0.43302401900291443 - trainLoss: 0.42048731446266174\n",
      "cnt: 0 - valLoss: 0.4330231845378876 - trainLoss: 0.420484721660614\n",
      "cnt: 0 - valLoss: 0.43302232027053833 - trainLoss: 0.42048215866088867\n",
      "cnt: 0 - valLoss: 0.43302151560783386 - trainLoss: 0.42047950625419617\n",
      "cnt: 0 - valLoss: 0.433020681142807 - trainLoss: 0.42047688364982605\n",
      "cnt: 0 - valLoss: 0.43301981687545776 - trainLoss: 0.42047426104545593\n",
      "cnt: 0 - valLoss: 0.4330190122127533 - trainLoss: 0.4204716086387634\n",
      "cnt: 0 - valLoss: 0.43301820755004883 - trainLoss: 0.4204690456390381\n",
      "cnt: 0 - valLoss: 0.4330173432826996 - trainLoss: 0.4204663932323456\n",
      "cnt: 0 - valLoss: 0.43301647901535034 - trainLoss: 0.42046383023262024\n",
      "cnt: 0 - valLoss: 0.43301570415496826 - trainLoss: 0.42046117782592773\n",
      "cnt: 0 - valLoss: 0.4330148696899414 - trainLoss: 0.4204586148262024\n",
      "cnt: 0 - valLoss: 0.43301400542259216 - trainLoss: 0.4204559624195099\n",
      "cnt: 0 - valLoss: 0.4330132007598877 - trainLoss: 0.42045339941978455\n",
      "cnt: 0 - valLoss: 0.43301236629486084 - trainLoss: 0.42045074701309204\n",
      "cnt: 0 - valLoss: 0.4330115020275116 - trainLoss: 0.4204481542110443\n",
      "cnt: 0 - valLoss: 0.43301069736480713 - trainLoss: 0.4204455018043518\n",
      "cnt: 0 - valLoss: 0.43300989270210266 - trainLoss: 0.42044293880462646\n",
      "cnt: 0 - valLoss: 0.4330090582370758 - trainLoss: 0.42044034600257874\n",
      "cnt: 0 - valLoss: 0.43300825357437134 - trainLoss: 0.4204377233982086\n",
      "cnt: 0 - valLoss: 0.4330073893070221 - trainLoss: 0.4204350709915161\n",
      "cnt: 0 - valLoss: 0.43300655484199524 - trainLoss: 0.42043250799179077\n",
      "cnt: 0 - valLoss: 0.4330057203769684 - trainLoss: 0.42042985558509827\n",
      "cnt: 0 - valLoss: 0.4330049157142639 - trainLoss: 0.42042723298072815\n",
      "cnt: 0 - valLoss: 0.4330040514469147 - trainLoss: 0.42042461037635803\n",
      "cnt: 0 - valLoss: 0.4330032467842102 - trainLoss: 0.4204220473766327\n",
      "cnt: 0 - valLoss: 0.43300244212150574 - trainLoss: 0.4204193949699402\n",
      "cnt: 0 - valLoss: 0.4330015778541565 - trainLoss: 0.42041680216789246\n",
      "cnt: 0 - valLoss: 0.43300071358680725 - trainLoss: 0.42041417956352234\n",
      "cnt: 0 - valLoss: 0.4329999089241028 - trainLoss: 0.4204115569591522\n",
      "cnt: 0 - valLoss: 0.4329990744590759 - trainLoss: 0.4204089641571045\n",
      "cnt: 0 - valLoss: 0.43299826979637146 - trainLoss: 0.4204063415527344\n",
      "cnt: 0 - valLoss: 0.432997465133667 - trainLoss: 0.42040374875068665\n",
      "cnt: 0 - valLoss: 0.43299660086631775 - trainLoss: 0.42040112614631653\n",
      "cnt: 0 - valLoss: 0.4329957365989685 - trainLoss: 0.4203985035419464\n",
      "cnt: 0 - valLoss: 0.43299493193626404 - trainLoss: 0.4203958511352539\n",
      "cnt: 0 - valLoss: 0.43299412727355957 - trainLoss: 0.42039328813552856\n",
      "cnt: 0 - valLoss: 0.4329932630062103 - trainLoss: 0.4203907251358032\n",
      "cnt: 0 - valLoss: 0.43299248814582825 - trainLoss: 0.4203880727291107\n",
      "cnt: 0 - valLoss: 0.432991623878479 - trainLoss: 0.4203854203224182\n",
      "cnt: 0 - valLoss: 0.43299081921577454 - trainLoss: 0.42038285732269287\n",
      "cnt: 0 - valLoss: 0.4329899847507477 - trainLoss: 0.42038020491600037\n",
      "cnt: 0 - valLoss: 0.4329891502857208 - trainLoss: 0.42037761211395264\n",
      "cnt: 0 - valLoss: 0.4329882860183716 - trainLoss: 0.4203750193119049\n",
      "cnt: 0 - valLoss: 0.4329875111579895 - trainLoss: 0.4203723967075348\n",
      "cnt: 0 - valLoss: 0.43298664689064026 - trainLoss: 0.42036980390548706\n",
      "cnt: 0 - valLoss: 0.4329858422279358 - trainLoss: 0.42036718130111694\n",
      "cnt: 0 - valLoss: 0.4329850375652313 - trainLoss: 0.42036452889442444\n",
      "cnt: 0 - valLoss: 0.4329841732978821 - trainLoss: 0.4203619658946991\n",
      "cnt: 0 - valLoss: 0.4329833686351776 - trainLoss: 0.4203593134880066\n",
      "cnt: 0 - valLoss: 0.43298253417015076 - trainLoss: 0.42035675048828125\n",
      "cnt: 0 - valLoss: 0.4329816699028015 - trainLoss: 0.42035409808158875\n",
      "cnt: 0 - valLoss: 0.43298083543777466 - trainLoss: 0.42035147547721863\n",
      "cnt: 0 - valLoss: 0.4329800605773926 - trainLoss: 0.4203489124774933\n",
      "cnt: 0 - valLoss: 0.43297919631004333 - trainLoss: 0.4203462600708008\n",
      "cnt: 0 - valLoss: 0.4329783320426941 - trainLoss: 0.42034363746643066\n",
      "cnt: 0 - valLoss: 0.432977557182312 - trainLoss: 0.4203410744667053\n",
      "cnt: 0 - valLoss: 0.43297672271728516 - trainLoss: 0.4203384220600128\n",
      "cnt: 0 - valLoss: 0.4329758882522583 - trainLoss: 0.4203358590602875\n",
      "cnt: 0 - valLoss: 0.43297508358955383 - trainLoss: 0.42033320665359497\n",
      "cnt: 0 - valLoss: 0.4329742193222046 - trainLoss: 0.42033058404922485\n",
      "cnt: 0 - valLoss: 0.4329734444618225 - trainLoss: 0.4203279912471771\n",
      "cnt: 0 - valLoss: 0.43297260999679565 - trainLoss: 0.4203253984451294\n",
      "cnt: 0 - valLoss: 0.4329717755317688 - trainLoss: 0.4203227460384369\n",
      "cnt: 0 - valLoss: 0.43297097086906433 - trainLoss: 0.42032018303871155\n",
      "cnt: 0 - valLoss: 0.4329701066017151 - trainLoss: 0.4203175902366638\n",
      "cnt: 0 - valLoss: 0.43296924233436584 - trainLoss: 0.4203149080276489\n",
      "cnt: 0 - valLoss: 0.43296849727630615 - trainLoss: 0.4203123152256012\n",
      "cnt: 0 - valLoss: 0.4329676330089569 - trainLoss: 0.42030975222587585\n",
      "cnt: 0 - valLoss: 0.43296679854393005 - trainLoss: 0.42030709981918335\n",
      "cnt: 0 - valLoss: 0.4329659938812256 - trainLoss: 0.4203045070171356\n",
      "cnt: 0 - valLoss: 0.43296512961387634 - trainLoss: 0.4203019142150879\n",
      "cnt: 0 - valLoss: 0.43296435475349426 - trainLoss: 0.4202992618083954\n",
      "cnt: 0 - valLoss: 0.4329635202884674 - trainLoss: 0.42029663920402527\n",
      "cnt: 0 - valLoss: 0.43296265602111816 - trainLoss: 0.4202940762042999\n",
      "cnt: 0 - valLoss: 0.4329618811607361 - trainLoss: 0.4202914237976074\n",
      "cnt: 0 - valLoss: 0.43296101689338684 - trainLoss: 0.4202888607978821\n",
      "cnt: 0 - valLoss: 0.43296024203300476 - trainLoss: 0.4202862083911896\n",
      "cnt: 0 - valLoss: 0.4329594075679779 - trainLoss: 0.42028364539146423\n",
      "cnt: 0 - valLoss: 0.43295854330062866 - trainLoss: 0.4202810227870941\n",
      "cnt: 0 - valLoss: 0.4329577684402466 - trainLoss: 0.420278400182724\n",
      "cnt: 0 - valLoss: 0.43295690417289734 - trainLoss: 0.42027580738067627\n",
      "cnt: 0 - valLoss: 0.43295609951019287 - trainLoss: 0.42027318477630615\n",
      "cnt: 0 - valLoss: 0.4329552948474884 - trainLoss: 0.42027053236961365\n",
      "cnt: 0 - valLoss: 0.43295443058013916 - trainLoss: 0.4202679693698883\n",
      "cnt: 0 - valLoss: 0.4329535961151123 - trainLoss: 0.4202653467655182\n",
      "cnt: 0 - valLoss: 0.43295279145240784 - trainLoss: 0.42026275396347046\n",
      "cnt: 0 - valLoss: 0.4329519271850586 - trainLoss: 0.4202601909637451\n",
      "cnt: 0 - valLoss: 0.4329511225223541 - trainLoss: 0.4202575385570526\n",
      "cnt: 0 - valLoss: 0.43295031785964966 - trainLoss: 0.4202549159526825\n",
      "cnt: 0 - valLoss: 0.4329494833946228 - trainLoss: 0.4202522933483124\n",
      "cnt: 0 - valLoss: 0.43294864892959595 - trainLoss: 0.42024970054626465\n",
      "cnt: 0 - valLoss: 0.4329478144645691 - trainLoss: 0.42024707794189453\n",
      "cnt: 0 - valLoss: 0.4329470098018646 - trainLoss: 0.4202444851398468\n",
      "cnt: 0 - valLoss: 0.43294620513916016 - trainLoss: 0.4202418625354767\n",
      "cnt: 0 - valLoss: 0.4329453408718109 - trainLoss: 0.42023923993110657\n",
      "cnt: 0 - valLoss: 0.43294453620910645 - trainLoss: 0.42023664712905884\n",
      "cnt: 0 - valLoss: 0.4329437017440796 - trainLoss: 0.4202340245246887\n",
      "cnt: 0 - valLoss: 0.4329428970813751 - trainLoss: 0.4202314019203186\n",
      "cnt: 0 - valLoss: 0.43294209241867065 - trainLoss: 0.42022883892059326\n",
      "cnt: 0 - valLoss: 0.4329412281513214 - trainLoss: 0.42022618651390076\n",
      "cnt: 0 - valLoss: 0.43294039368629456 - trainLoss: 0.4202236235141754\n",
      "cnt: 0 - valLoss: 0.4329395890235901 - trainLoss: 0.4202209711074829\n",
      "cnt: 0 - valLoss: 0.43293872475624084 - trainLoss: 0.42021840810775757\n",
      "cnt: 0 - valLoss: 0.43293797969818115 - trainLoss: 0.42021575570106506\n",
      "cnt: 0 - valLoss: 0.4329371154308319 - trainLoss: 0.4202131927013397\n",
      "cnt: 0 - valLoss: 0.43293628096580505 - trainLoss: 0.4202105402946472\n",
      "cnt: 0 - valLoss: 0.4329354763031006 - trainLoss: 0.4202079176902771\n",
      "cnt: 0 - valLoss: 0.43293461203575134 - trainLoss: 0.420205295085907\n",
      "cnt: 0 - valLoss: 0.4329338073730469 - trainLoss: 0.42020273208618164\n",
      "cnt: 0 - valLoss: 0.4329330027103424 - trainLoss: 0.42020007967948914\n",
      "cnt: 0 - valLoss: 0.4329322278499603 - trainLoss: 0.4201975166797638\n",
      "cnt: 0 - valLoss: 0.4329313635826111 - trainLoss: 0.4201948940753937\n",
      "cnt: 0 - valLoss: 0.43293049931526184 - trainLoss: 0.42019230127334595\n",
      "cnt: 0 - valLoss: 0.4329296946525574 - trainLoss: 0.42018964886665344\n",
      "cnt: 0 - valLoss: 0.4329288899898529 - trainLoss: 0.4201870858669281\n",
      "cnt: 0 - valLoss: 0.43292805552482605 - trainLoss: 0.420184463262558\n",
      "cnt: 0 - valLoss: 0.4329272508621216 - trainLoss: 0.42018184065818787\n",
      "cnt: 0 - valLoss: 0.43292638659477234 - trainLoss: 0.42017924785614014\n",
      "cnt: 0 - valLoss: 0.43292558193206787 - trainLoss: 0.42017662525177\n",
      "cnt: 0 - valLoss: 0.4329247772693634 - trainLoss: 0.4201740324497223\n",
      "cnt: 0 - valLoss: 0.43292391300201416 - trainLoss: 0.4201714098453522\n",
      "cnt: 0 - valLoss: 0.4329230785369873 - trainLoss: 0.42016875743865967\n",
      "cnt: 0 - valLoss: 0.43292227387428284 - trainLoss: 0.4201661944389343\n",
      "cnt: 0 - valLoss: 0.43292146921157837 - trainLoss: 0.4201635718345642\n",
      "cnt: 0 - valLoss: 0.4329206347465515 - trainLoss: 0.4201609790325165\n",
      "cnt: 0 - valLoss: 0.43291983008384705 - trainLoss: 0.4201582968235016\n",
      "cnt: 0 - valLoss: 0.4329190254211426 - trainLoss: 0.42015573382377625\n",
      "cnt: 0 - valLoss: 0.4329181909561157 - trainLoss: 0.42015308141708374\n",
      "cnt: 0 - valLoss: 0.43291738629341125 - trainLoss: 0.4201505780220032\n",
      "cnt: 0 - valLoss: 0.432916522026062 - trainLoss: 0.4201478958129883\n",
      "cnt: 0 - valLoss: 0.4329157769680023 - trainLoss: 0.42014530301094055\n",
      "cnt: 0 - valLoss: 0.4329149127006531 - trainLoss: 0.42014268040657043\n",
      "cnt: 0 - valLoss: 0.43291404843330383 - trainLoss: 0.4201400876045227\n",
      "cnt: 0 - valLoss: 0.43291327357292175 - trainLoss: 0.42013752460479736\n",
      "cnt: 0 - valLoss: 0.4329124093055725 - trainLoss: 0.42013490200042725\n",
      "cnt: 0 - valLoss: 0.43291160464286804 - trainLoss: 0.42013227939605713\n",
      "cnt: 0 - valLoss: 0.4329107999801636 - trainLoss: 0.4201296269893646\n",
      "cnt: 0 - valLoss: 0.4329099655151367 - trainLoss: 0.4201270639896393\n",
      "cnt: 0 - valLoss: 0.43290916085243225 - trainLoss: 0.4201244115829468\n",
      "cnt: 0 - valLoss: 0.4329083561897278 - trainLoss: 0.42012184858322144\n",
      "cnt: 0 - valLoss: 0.43290749192237854 - trainLoss: 0.4201192259788513\n",
      "cnt: 0 - valLoss: 0.4329066872596741 - trainLoss: 0.4201166331768036\n",
      "cnt: 0 - valLoss: 0.4329058527946472 - trainLoss: 0.42011401057243347\n",
      "cnt: 0 - valLoss: 0.43290504813194275 - trainLoss: 0.42011138796806335\n",
      "cnt: 0 - valLoss: 0.4329042434692383 - trainLoss: 0.4201087951660156\n",
      "cnt: 0 - valLoss: 0.4329034090042114 - trainLoss: 0.4201061725616455\n",
      "cnt: 0 - valLoss: 0.43290257453918457 - trainLoss: 0.42010360956192017\n",
      "cnt: 0 - valLoss: 0.4329017400741577 - trainLoss: 0.42010095715522766\n",
      "cnt: 0 - valLoss: 0.43290093541145325 - trainLoss: 0.4200983941555023\n",
      "cnt: 0 - valLoss: 0.4329001307487488 - trainLoss: 0.4200957417488098\n",
      "cnt: 0 - valLoss: 0.4328992962837219 - trainLoss: 0.4200931787490845\n",
      "cnt: 0 - valLoss: 0.4328984320163727 - trainLoss: 0.42009052634239197\n",
      "cnt: 0 - valLoss: 0.432897686958313 - trainLoss: 0.42008793354034424\n",
      "cnt: 0 - valLoss: 0.43289682269096375 - trainLoss: 0.4200853407382965\n",
      "cnt: 0 - valLoss: 0.43289604783058167 - trainLoss: 0.4200827181339264\n",
      "cnt: 0 - valLoss: 0.4328951835632324 - trainLoss: 0.42008012533187866\n",
      "cnt: 0 - valLoss: 0.43289437890052795 - trainLoss: 0.42007750272750854\n",
      "cnt: 0 - valLoss: 0.4328935742378235 - trainLoss: 0.4200748801231384\n",
      "cnt: 0 - valLoss: 0.43289273977279663 - trainLoss: 0.4200722873210907\n",
      "cnt: 0 - valLoss: 0.43289193511009216 - trainLoss: 0.4200696647167206\n",
      "cnt: 0 - valLoss: 0.4328910708427429 - trainLoss: 0.42006707191467285\n",
      "cnt: 0 - valLoss: 0.43289029598236084 - trainLoss: 0.42006444931030273\n",
      "cnt: 0 - valLoss: 0.4328894317150116 - trainLoss: 0.4200618267059326\n",
      "cnt: 0 - valLoss: 0.43288862705230713 - trainLoss: 0.4200592637062073\n",
      "cnt: 0 - valLoss: 0.43288782238960266 - trainLoss: 0.42005661129951477\n",
      "cnt: 0 - valLoss: 0.4328869879245758 - trainLoss: 0.42005404829978943\n",
      "cnt: 0 - valLoss: 0.43288618326187134 - trainLoss: 0.4200513958930969\n",
      "cnt: 0 - valLoss: 0.4328853189945221 - trainLoss: 0.4200488328933716\n",
      "cnt: 0 - valLoss: 0.4328845739364624 - trainLoss: 0.4200461804866791\n",
      "cnt: 0 - valLoss: 0.43288370966911316 - trainLoss: 0.42004361748695374\n",
      "cnt: 0 - valLoss: 0.4328829348087311 - trainLoss: 0.4200409948825836\n",
      "cnt: 0 - valLoss: 0.43288207054138184 - trainLoss: 0.4200383722782135\n",
      "cnt: 0 - valLoss: 0.43288126587867737 - trainLoss: 0.42003580927848816\n",
      "cnt: 0 - valLoss: 0.4328804314136505 - trainLoss: 0.42003315687179565\n",
      "cnt: 0 - valLoss: 0.4328796863555908 - trainLoss: 0.4200305938720703\n",
      "cnt: 0 - valLoss: 0.4328788220882416 - trainLoss: 0.4200279414653778\n",
      "cnt: 0 - valLoss: 0.4328779876232147 - trainLoss: 0.42002537846565247\n",
      "cnt: 0 - valLoss: 0.43287718296051025 - trainLoss: 0.42002275586128235\n",
      "cnt: 0 - valLoss: 0.4328763782978058 - trainLoss: 0.42002013325691223\n",
      "cnt: 0 - valLoss: 0.43287554383277893 - trainLoss: 0.4200174808502197\n",
      "cnt: 0 - valLoss: 0.4328747093677521 - trainLoss: 0.4200149178504944\n",
      "cnt: 0 - valLoss: 0.43287393450737 - trainLoss: 0.42001232504844666\n",
      "cnt: 0 - valLoss: 0.4328731298446655 - trainLoss: 0.42000970244407654\n",
      "cnt: 0 - valLoss: 0.43287229537963867 - trainLoss: 0.4200070798397064\n",
      "cnt: 0 - valLoss: 0.43287143111228943 - trainLoss: 0.4200044870376587\n",
      "cnt: 0 - valLoss: 0.43287068605422974 - trainLoss: 0.42000192403793335\n",
      "cnt: 0 - valLoss: 0.4328698217868805 - trainLoss: 0.41999927163124084\n",
      "cnt: 0 - valLoss: 0.4328690469264984 - trainLoss: 0.4199966788291931\n",
      "cnt: 0 - valLoss: 0.43286824226379395 - trainLoss: 0.4199940860271454\n",
      "cnt: 0 - valLoss: 0.4328674077987671 - trainLoss: 0.41999146342277527\n",
      "cnt: 0 - valLoss: 0.43286654353141785 - trainLoss: 0.41998881101608276\n",
      "cnt: 0 - valLoss: 0.4328657388687134 - trainLoss: 0.4199862480163574\n",
      "cnt: 0 - valLoss: 0.4328649342060089 - trainLoss: 0.4199836254119873\n",
      "cnt: 0 - valLoss: 0.43286415934562683 - trainLoss: 0.4199810326099396\n",
      "cnt: 0 - valLoss: 0.4328632950782776 - trainLoss: 0.41997841000556946\n",
      "cnt: 0 - valLoss: 0.4328625202178955 - trainLoss: 0.41997581720352173\n",
      "cnt: 0 - valLoss: 0.43286168575286865 - trainLoss: 0.419973224401474\n",
      "cnt: 0 - valLoss: 0.4328609108924866 - trainLoss: 0.4199705719947815\n",
      "cnt: 0 - valLoss: 0.4328600764274597 - trainLoss: 0.41996800899505615\n",
      "cnt: 0 - valLoss: 0.43285927176475525 - trainLoss: 0.41996535658836365\n",
      "cnt: 0 - valLoss: 0.432858407497406 - trainLoss: 0.4199627935886383\n",
      "cnt: 0 - valLoss: 0.43285760283470154 - trainLoss: 0.4199601709842682\n",
      "cnt: 0 - valLoss: 0.43285679817199707 - trainLoss: 0.41995757818222046\n",
      "cnt: 0 - valLoss: 0.432856023311615 - trainLoss: 0.4199550151824951\n",
      "cnt: 0 - valLoss: 0.43285515904426575 - trainLoss: 0.4199523627758026\n",
      "cnt: 0 - valLoss: 0.43285438418388367 - trainLoss: 0.4199497699737549\n",
      "cnt: 0 - valLoss: 0.4328535199165344 - trainLoss: 0.4199471175670624\n",
      "cnt: 0 - valLoss: 0.43285274505615234 - trainLoss: 0.41994455456733704\n",
      "cnt: 0 - valLoss: 0.4328519105911255 - trainLoss: 0.41994190216064453\n",
      "cnt: 0 - valLoss: 0.4328511357307434 - trainLoss: 0.4199393391609192\n",
      "cnt: 0 - valLoss: 0.43285036087036133 - trainLoss: 0.4199367165565491\n",
      "cnt: 0 - valLoss: 0.4328494966030121 - trainLoss: 0.41993412375450134\n",
      "cnt: 0 - valLoss: 0.43284872174263 - trainLoss: 0.419931560754776\n",
      "cnt: 0 - valLoss: 0.43284785747528076 - trainLoss: 0.4199289083480835\n",
      "cnt: 0 - valLoss: 0.43284711241722107 - trainLoss: 0.41992631554603577\n",
      "cnt: 0 - valLoss: 0.4328462779521942 - trainLoss: 0.41992372274398804\n",
      "cnt: 0 - valLoss: 0.43284547328948975 - trainLoss: 0.4199211001396179\n",
      "cnt: 0 - valLoss: 0.43284469842910767 - trainLoss: 0.4199185073375702\n",
      "cnt: 0 - valLoss: 0.4328438937664032 - trainLoss: 0.4199158847332001\n",
      "cnt: 0 - valLoss: 0.43284305930137634 - trainLoss: 0.41991332173347473\n",
      "cnt: 0 - valLoss: 0.43284228444099426 - trainLoss: 0.4199106693267822\n",
      "cnt: 0 - valLoss: 0.4328414499759674 - trainLoss: 0.4199081063270569\n",
      "cnt: 0 - valLoss: 0.4328406751155853 - trainLoss: 0.41990548372268677\n",
      "cnt: 0 - valLoss: 0.4328398108482361 - trainLoss: 0.41990286111831665\n",
      "cnt: 0 - valLoss: 0.432839035987854 - trainLoss: 0.41990020871162415\n",
      "cnt: 0 - valLoss: 0.4328382611274719 - trainLoss: 0.4198976457118988\n",
      "cnt: 0 - valLoss: 0.4328373968601227 - trainLoss: 0.41989508271217346\n",
      "cnt: 0 - valLoss: 0.4328365921974182 - trainLoss: 0.41989243030548096\n",
      "cnt: 0 - valLoss: 0.43283581733703613 - trainLoss: 0.4198898673057556\n",
      "cnt: 0 - valLoss: 0.43283501267433167 - trainLoss: 0.4198872447013855\n",
      "cnt: 0 - valLoss: 0.4328342378139496 - trainLoss: 0.41988465189933777\n",
      "cnt: 0 - valLoss: 0.43283337354660034 - trainLoss: 0.4198819696903229\n",
      "cnt: 0 - valLoss: 0.43283259868621826 - trainLoss: 0.41987940669059753\n",
      "cnt: 0 - valLoss: 0.4328317642211914 - trainLoss: 0.4198768436908722\n",
      "cnt: 0 - valLoss: 0.4328309893608093 - trainLoss: 0.41987425088882446\n",
      "cnt: 0 - valLoss: 0.43283021450042725 - trainLoss: 0.41987162828445435\n",
      "cnt: 0 - valLoss: 0.432829350233078 - trainLoss: 0.41986900568008423\n",
      "cnt: 0 - valLoss: 0.4328285753726959 - trainLoss: 0.4198664128780365\n",
      "cnt: 0 - valLoss: 0.43282777070999146 - trainLoss: 0.41986384987831116\n",
      "cnt: 0 - valLoss: 0.432826966047287 - trainLoss: 0.41986116766929626\n",
      "cnt: 0 - valLoss: 0.4328261911869049 - trainLoss: 0.41985857486724854\n",
      "cnt: 0 - valLoss: 0.43282535672187805 - trainLoss: 0.4198560118675232\n",
      "cnt: 0 - valLoss: 0.4328245520591736 - trainLoss: 0.4198533892631531\n",
      "cnt: 0 - valLoss: 0.43282368779182434 - trainLoss: 0.41985076665878296\n",
      "cnt: 0 - valLoss: 0.43282291293144226 - trainLoss: 0.41984817385673523\n",
      "cnt: 0 - valLoss: 0.4328220784664154 - trainLoss: 0.4198455512523651\n",
      "cnt: 0 - valLoss: 0.4328213036060333 - trainLoss: 0.4198429584503174\n",
      "cnt: 0 - valLoss: 0.43282046914100647 - trainLoss: 0.41984033584594727\n",
      "cnt: 0 - valLoss: 0.4328197240829468 - trainLoss: 0.41983771324157715\n",
      "cnt: 0 - valLoss: 0.4328188896179199 - trainLoss: 0.4198351502418518\n",
      "cnt: 0 - valLoss: 0.43281808495521545 - trainLoss: 0.4198324978351593\n",
      "cnt: 0 - valLoss: 0.432817280292511 - trainLoss: 0.41982993483543396\n",
      "cnt: 0 - valLoss: 0.43281644582748413 - trainLoss: 0.4198273718357086\n",
      "cnt: 0 - valLoss: 0.43281564116477966 - trainLoss: 0.4198247194290161\n",
      "cnt: 0 - valLoss: 0.4328148365020752 - trainLoss: 0.41982215642929077\n",
      "cnt: 0 - valLoss: 0.43281400203704834 - trainLoss: 0.41981950402259827\n",
      "cnt: 0 - valLoss: 0.43281322717666626 - trainLoss: 0.41981691122055054\n",
      "cnt: 0 - valLoss: 0.4328124225139618 - trainLoss: 0.4198143482208252\n",
      "cnt: 0 - valLoss: 0.4328116178512573 - trainLoss: 0.4198116958141327\n",
      "cnt: 0 - valLoss: 0.43281078338623047 - trainLoss: 0.41980910301208496\n",
      "cnt: 0 - valLoss: 0.432809978723526 - trainLoss: 0.41980648040771484\n",
      "cnt: 0 - valLoss: 0.43280917406082153 - trainLoss: 0.4198039174079895\n",
      "cnt: 0 - valLoss: 0.4328083395957947 - trainLoss: 0.4198012948036194\n",
      "cnt: 0 - valLoss: 0.4328075647354126 - trainLoss: 0.41979870200157166\n",
      "cnt: 0 - valLoss: 0.4328068196773529 - trainLoss: 0.4197961091995239\n",
      "cnt: 0 - valLoss: 0.43280595541000366 - trainLoss: 0.4197934567928314\n",
      "cnt: 0 - valLoss: 0.4328051805496216 - trainLoss: 0.4197908937931061\n",
      "cnt: 0 - valLoss: 0.4328043758869171 - trainLoss: 0.41978830099105835\n",
      "cnt: 0 - valLoss: 0.43280354142189026 - trainLoss: 0.41978567838668823\n",
      "cnt: 0 - valLoss: 0.4328027367591858 - trainLoss: 0.4197830557823181\n",
      "cnt: 0 - valLoss: 0.4328019320964813 - trainLoss: 0.4197804629802704\n",
      "cnt: 0 - valLoss: 0.43280109763145447 - trainLoss: 0.41977789998054504\n",
      "cnt: 0 - valLoss: 0.43280029296875 - trainLoss: 0.41977524757385254\n",
      "cnt: 0 - valLoss: 0.4327995181083679 - trainLoss: 0.4197726547718048\n",
      "cnt: 0 - valLoss: 0.43279874324798584 - trainLoss: 0.4197700619697571\n",
      "cnt: 0 - valLoss: 0.4327978789806366 - trainLoss: 0.41976743936538696\n",
      "cnt: 0 - valLoss: 0.4327971339225769 - trainLoss: 0.4197648763656616\n",
      "cnt: 0 - valLoss: 0.43279626965522766 - trainLoss: 0.4197622239589691\n",
      "cnt: 0 - valLoss: 0.4327954947948456 - trainLoss: 0.4197596609592438\n",
      "cnt: 0 - valLoss: 0.4327947199344635 - trainLoss: 0.41975703835487366\n",
      "cnt: 0 - valLoss: 0.43279385566711426 - trainLoss: 0.4197544455528259\n",
      "cnt: 0 - valLoss: 0.4327930808067322 - trainLoss: 0.4197518527507782\n",
      "cnt: 0 - valLoss: 0.4327922761440277 - trainLoss: 0.41974925994873047\n",
      "cnt: 0 - valLoss: 0.43279147148132324 - trainLoss: 0.41974663734436035\n",
      "cnt: 0 - valLoss: 0.43279069662094116 - trainLoss: 0.4197440445423126\n",
      "cnt: 0 - valLoss: 0.4327898323535919 - trainLoss: 0.4197414219379425\n",
      "cnt: 0 - valLoss: 0.43278905749320984 - trainLoss: 0.41973876953125\n",
      "cnt: 0 - valLoss: 0.43278828263282776 - trainLoss: 0.41973620653152466\n",
      "cnt: 0 - valLoss: 0.4327874481678009 - trainLoss: 0.41973358392715454\n",
      "cnt: 0 - valLoss: 0.4327866733074188 - trainLoss: 0.4197310507297516\n",
      "cnt: 0 - valLoss: 0.43278583884239197 - trainLoss: 0.4197283983230591\n",
      "cnt: 0 - valLoss: 0.4327850341796875 - trainLoss: 0.41972580552101135\n",
      "cnt: 0 - valLoss: 0.43278422951698303 - trainLoss: 0.41972318291664124\n",
      "cnt: 0 - valLoss: 0.43278345465660095 - trainLoss: 0.4197206199169159\n",
      "cnt: 0 - valLoss: 0.4327826201915741 - trainLoss: 0.4197179973125458\n",
      "cnt: 0 - valLoss: 0.43278181552886963 - trainLoss: 0.41971540451049805\n",
      "cnt: 0 - valLoss: 0.43278101086616516 - trainLoss: 0.41971278190612793\n",
      "cnt: 0 - valLoss: 0.4327801764011383 - trainLoss: 0.4197101891040802\n",
      "cnt: 0 - valLoss: 0.43277934193611145 - trainLoss: 0.4197075664997101\n",
      "cnt: 0 - valLoss: 0.4327785074710846 - trainLoss: 0.4197048842906952\n",
      "cnt: 0 - valLoss: 0.43277767300605774 - trainLoss: 0.4197022020816803\n",
      "cnt: 0 - valLoss: 0.4327768087387085 - trainLoss: 0.4196995496749878\n",
      "cnt: 0 - valLoss: 0.43277594447135925 - trainLoss: 0.4196968972682953\n",
      "cnt: 0 - valLoss: 0.4327751398086548 - trainLoss: 0.4196942448616028\n",
      "cnt: 0 - valLoss: 0.43277427554130554 - trainLoss: 0.4196915626525879\n",
      "cnt: 0 - valLoss: 0.4327734708786011 - trainLoss: 0.4196889102458954\n",
      "cnt: 0 - valLoss: 0.43277260661125183 - trainLoss: 0.41968628764152527\n",
      "cnt: 0 - valLoss: 0.4327717423439026 - trainLoss: 0.4196836054325104\n",
      "cnt: 0 - valLoss: 0.4327709376811981 - trainLoss: 0.41968095302581787\n",
      "cnt: 0 - valLoss: 0.43277013301849365 - trainLoss: 0.4196782410144806\n",
      "cnt: 0 - valLoss: 0.4327692687511444 - trainLoss: 0.4196755886077881\n",
      "cnt: 0 - valLoss: 0.43276840448379517 - trainLoss: 0.4196729362010956\n",
      "cnt: 0 - valLoss: 0.4327675998210907 - trainLoss: 0.4196702837944031\n",
      "cnt: 0 - valLoss: 0.43276676535606384 - trainLoss: 0.41966763138771057\n",
      "cnt: 0 - valLoss: 0.432765930891037 - trainLoss: 0.41966497898101807\n",
      "cnt: 0 - valLoss: 0.43276506662368774 - trainLoss: 0.41966232657432556\n",
      "cnt: 0 - valLoss: 0.4327642321586609 - trainLoss: 0.41965967416763306\n",
      "cnt: 0 - valLoss: 0.43276339769363403 - trainLoss: 0.41965702176094055\n",
      "cnt: 0 - valLoss: 0.4327625632286072 - trainLoss: 0.41965436935424805\n",
      "cnt: 0 - valLoss: 0.4327617287635803 - trainLoss: 0.41965171694755554\n",
      "cnt: 0 - valLoss: 0.43276089429855347 - trainLoss: 0.41964906454086304\n",
      "cnt: 0 - valLoss: 0.4327600598335266 - trainLoss: 0.41964635252952576\n",
      "cnt: 0 - valLoss: 0.43275922536849976 - trainLoss: 0.41964367032051086\n",
      "cnt: 0 - valLoss: 0.4327583611011505 - trainLoss: 0.41964101791381836\n",
      "cnt: 0 - valLoss: 0.43275755643844604 - trainLoss: 0.41963836550712585\n",
      "cnt: 0 - valLoss: 0.4327566921710968 - trainLoss: 0.41963574290275574\n",
      "cnt: 0 - valLoss: 0.43275588750839233 - trainLoss: 0.41963306069374084\n",
      "cnt: 0 - valLoss: 0.4327550232410431 - trainLoss: 0.41963040828704834\n",
      "cnt: 0 - valLoss: 0.4327542185783386 - trainLoss: 0.41962775588035583\n",
      "cnt: 0 - valLoss: 0.43275341391563416 - trainLoss: 0.41962507367134094\n",
      "cnt: 0 - valLoss: 0.4327525496482849 - trainLoss: 0.41962242126464844\n",
      "cnt: 0 - valLoss: 0.43275168538093567 - trainLoss: 0.41961976885795593\n",
      "cnt: 0 - valLoss: 0.4327509105205536 - trainLoss: 0.4196171164512634\n",
      "cnt: 0 - valLoss: 0.43275004625320435 - trainLoss: 0.4196144640445709\n",
      "cnt: 0 - valLoss: 0.4327492117881775 - trainLoss: 0.4196118116378784\n",
      "cnt: 0 - valLoss: 0.43274834752082825 - trainLoss: 0.4196091294288635\n",
      "cnt: 0 - valLoss: 0.43274757266044617 - trainLoss: 0.4196065068244934\n",
      "cnt: 0 - valLoss: 0.4327467083930969 - trainLoss: 0.4196038246154785\n",
      "cnt: 0 - valLoss: 0.43274590373039246 - trainLoss: 0.4196012020111084\n",
      "cnt: 0 - valLoss: 0.4327450394630432 - trainLoss: 0.4195985198020935\n",
      "cnt: 0 - valLoss: 0.43274423480033875 - trainLoss: 0.4195958077907562\n",
      "cnt: 0 - valLoss: 0.4327433705329895 - trainLoss: 0.4195932149887085\n",
      "cnt: 0 - valLoss: 0.4327425956726074 - trainLoss: 0.4195905327796936\n",
      "cnt: 0 - valLoss: 0.4327417314052582 - trainLoss: 0.4195878505706787\n",
      "cnt: 0 - valLoss: 0.4327409267425537 - trainLoss: 0.4195851981639862\n",
      "cnt: 0 - valLoss: 0.43274006247520447 - trainLoss: 0.4195825457572937\n",
      "cnt: 0 - valLoss: 0.4327392578125 - trainLoss: 0.4195798933506012\n",
      "cnt: 0 - valLoss: 0.43273839354515076 - trainLoss: 0.4195772409439087\n",
      "cnt: 0 - valLoss: 0.4327375888824463 - trainLoss: 0.4195745885372162\n",
      "cnt: 0 - valLoss: 0.4327367842197418 - trainLoss: 0.4195719361305237\n",
      "cnt: 0 - valLoss: 0.4327359199523926 - trainLoss: 0.4195692837238312\n",
      "cnt: 0 - valLoss: 0.4327350854873657 - trainLoss: 0.41956663131713867\n",
      "cnt: 0 - valLoss: 0.43273428082466125 - trainLoss: 0.4195639193058014\n",
      "cnt: 0 - valLoss: 0.4327334761619568 - trainLoss: 0.41956132650375366\n",
      "cnt: 0 - valLoss: 0.43273261189460754 - trainLoss: 0.419558584690094\n",
      "cnt: 0 - valLoss: 0.4327318072319031 - trainLoss: 0.41955599188804626\n",
      "cnt: 0 - valLoss: 0.4327309727668762 - trainLoss: 0.41955330967903137\n",
      "cnt: 0 - valLoss: 0.43273016810417175 - trainLoss: 0.41955065727233887\n",
      "cnt: 0 - valLoss: 0.43272921442985535 - trainLoss: 0.41954800486564636\n",
      "cnt: 0 - valLoss: 0.4327283203601837 - trainLoss: 0.41954535245895386\n",
      "cnt: 0 - valLoss: 0.4327274262905121 - trainLoss: 0.41954270005226135\n",
      "cnt: 0 - valLoss: 0.43272653222084045 - trainLoss: 0.41954004764556885\n",
      "cnt: 0 - valLoss: 0.4327256381511688 - trainLoss: 0.41953742504119873\n",
      "cnt: 0 - valLoss: 0.4327247142791748 - trainLoss: 0.41953474283218384\n",
      "cnt: 0 - valLoss: 0.4327238202095032 - trainLoss: 0.4195321202278137\n",
      "cnt: 0 - valLoss: 0.43272289633750916 - trainLoss: 0.4195294678211212\n",
      "cnt: 0 - valLoss: 0.4327220320701599 - trainLoss: 0.4195268154144287\n",
      "cnt: 0 - valLoss: 0.4327210783958435 - trainLoss: 0.4195241630077362\n",
      "cnt: 0 - valLoss: 0.43272021412849426 - trainLoss: 0.4195215106010437\n",
      "cnt: 0 - valLoss: 0.43271932005882263 - trainLoss: 0.4195188879966736\n",
      "cnt: 0 - valLoss: 0.4327183961868286 - trainLoss: 0.4195162355899811\n",
      "cnt: 0 - valLoss: 0.4327174723148346 - trainLoss: 0.4195135831832886\n",
      "cnt: 0 - valLoss: 0.43271663784980774 - trainLoss: 0.41951093077659607\n",
      "cnt: 0 - valLoss: 0.43271568417549133 - trainLoss: 0.41950827836990356\n",
      "cnt: 0 - valLoss: 0.4327148199081421 - trainLoss: 0.41950562596321106\n",
      "cnt: 0 - valLoss: 0.43271389603614807 - trainLoss: 0.41950300335884094\n",
      "cnt: 0 - valLoss: 0.43271303176879883 - trainLoss: 0.41950035095214844\n",
      "cnt: 0 - valLoss: 0.4327121376991272 - trainLoss: 0.41949769854545593\n",
      "cnt: 0 - valLoss: 0.4327112138271332 - trainLoss: 0.4194950461387634\n",
      "cnt: 0 - valLoss: 0.43271031975746155 - trainLoss: 0.4194924533367157\n",
      "cnt: 0 - valLoss: 0.4327094256877899 - trainLoss: 0.4194897413253784\n",
      "cnt: 0 - valLoss: 0.4327085614204407 - trainLoss: 0.4194870889186859\n",
      "cnt: 0 - valLoss: 0.43270766735076904 - trainLoss: 0.4194844961166382\n",
      "cnt: 0 - valLoss: 0.4327067732810974 - trainLoss: 0.4194818437099457\n",
      "cnt: 0 - valLoss: 0.43270590901374817 - trainLoss: 0.4194791913032532\n",
      "cnt: 0 - valLoss: 0.43270495533943176 - trainLoss: 0.4194765090942383\n",
      "cnt: 0 - valLoss: 0.43270406126976013 - trainLoss: 0.41947391629219055\n",
      "cnt: 0 - valLoss: 0.43270325660705566 - trainLoss: 0.41947126388549805\n",
      "cnt: 0 - valLoss: 0.43270230293273926 - trainLoss: 0.41946861147880554\n",
      "cnt: 0 - valLoss: 0.4327014684677124 - trainLoss: 0.41946595907211304\n",
      "cnt: 0 - valLoss: 0.4327005445957184 - trainLoss: 0.41946330666542053\n",
      "cnt: 0 - valLoss: 0.43269965052604675 - trainLoss: 0.419460654258728\n",
      "cnt: 0 - valLoss: 0.4326987862586975 - trainLoss: 0.4194580018520355\n",
      "cnt: 0 - valLoss: 0.4326978921890259 - trainLoss: 0.419455349445343\n",
      "cnt: 0 - valLoss: 0.43269699811935425 - trainLoss: 0.4194527268409729\n",
      "cnt: 0 - valLoss: 0.432696133852005 - trainLoss: 0.4194500148296356\n",
      "cnt: 0 - valLoss: 0.43269526958465576 - trainLoss: 0.4194473624229431\n",
      "cnt: 0 - valLoss: 0.43269437551498413 - trainLoss: 0.4194447696208954\n",
      "cnt: 0 - valLoss: 0.4326934814453125 - trainLoss: 0.4194421172142029\n",
      "cnt: 0 - valLoss: 0.43269261717796326 - trainLoss: 0.4194394648075104\n",
      "cnt: 0 - valLoss: 0.432691752910614 - trainLoss: 0.41943684220314026\n",
      "cnt: 0 - valLoss: 0.43269091844558716 - trainLoss: 0.41943418979644775\n",
      "cnt: 0 - valLoss: 0.4326900541782379 - trainLoss: 0.41943153738975525\n",
      "cnt: 0 - valLoss: 0.4326891601085663 - trainLoss: 0.41942888498306274\n",
      "cnt: 0 - valLoss: 0.43268826603889465 - trainLoss: 0.41942623257637024\n",
      "cnt: 0 - valLoss: 0.4326874017715454 - trainLoss: 0.41942358016967773\n",
      "cnt: 0 - valLoss: 0.43268653750419617 - trainLoss: 0.41942092776298523\n",
      "cnt: 0 - valLoss: 0.43268564343452454 - trainLoss: 0.4194183051586151\n",
      "cnt: 0 - valLoss: 0.4326847791671753 - trainLoss: 0.4194156527519226\n",
      "cnt: 0 - valLoss: 0.43268391489982605 - trainLoss: 0.4194130003452301\n",
      "cnt: 0 - valLoss: 0.4326830208301544 - trainLoss: 0.4194103479385376\n",
      "cnt: 0 - valLoss: 0.4326821565628052 - trainLoss: 0.41940775513648987\n",
      "cnt: 0 - valLoss: 0.4326813220977783 - trainLoss: 0.41940510272979736\n",
      "cnt: 0 - valLoss: 0.4326803982257843 - trainLoss: 0.41940242052078247\n",
      "cnt: 0 - valLoss: 0.43267956376075745 - trainLoss: 0.41939976811408997\n",
      "cnt: 0 - valLoss: 0.4326786994934082 - trainLoss: 0.41939711570739746\n",
      "cnt: 0 - valLoss: 0.4326778054237366 - trainLoss: 0.41939446330070496\n",
      "cnt: 0 - valLoss: 0.43267694115638733 - trainLoss: 0.4193918704986572\n",
      "cnt: 0 - valLoss: 0.4326760768890381 - trainLoss: 0.4193892180919647\n",
      "cnt: 0 - valLoss: 0.43267521262168884 - trainLoss: 0.4193865954875946\n",
      "cnt: 0 - valLoss: 0.432674378156662 - trainLoss: 0.4193839430809021\n",
      "cnt: 0 - valLoss: 0.4326734244823456 - trainLoss: 0.4193812906742096\n",
      "cnt: 0 - valLoss: 0.43267256021499634 - trainLoss: 0.4193786382675171\n",
      "cnt: 0 - valLoss: 0.43267175555229187 - trainLoss: 0.4193759858608246\n",
      "cnt: 0 - valLoss: 0.43267086148262024 - trainLoss: 0.41937336325645447\n",
      "cnt: 0 - valLoss: 0.432669997215271 - trainLoss: 0.4193706810474396\n",
      "cnt: 0 - valLoss: 0.43266913294792175 - trainLoss: 0.41936805844306946\n",
      "cnt: 0 - valLoss: 0.4326682686805725 - trainLoss: 0.41936540603637695\n",
      "cnt: 0 - valLoss: 0.43266743421554565 - trainLoss: 0.4193628132343292\n",
      "cnt: 0 - valLoss: 0.4326665699481964 - trainLoss: 0.4193601608276367\n",
      "cnt: 0 - valLoss: 0.4326656758785248 - trainLoss: 0.4193574786186218\n",
      "cnt: 0 - valLoss: 0.43266481161117554 - trainLoss: 0.4193548262119293\n",
      "cnt: 0 - valLoss: 0.4326639473438263 - trainLoss: 0.4193522334098816\n",
      "cnt: 0 - valLoss: 0.43266308307647705 - trainLoss: 0.4193495810031891\n",
      "cnt: 0 - valLoss: 0.4326621890068054 - trainLoss: 0.4193468689918518\n",
      "cnt: 0 - valLoss: 0.432661235332489 - trainLoss: 0.4193442165851593\n",
      "cnt: 0 - valLoss: 0.43266040086746216 - trainLoss: 0.41934147477149963\n",
      "cnt: 0 - valLoss: 0.43265944719314575 - trainLoss: 0.41933879256248474\n",
      "cnt: 0 - valLoss: 0.4326585829257965 - trainLoss: 0.41933608055114746\n",
      "cnt: 0 - valLoss: 0.43265774846076965 - trainLoss: 0.4193333387374878\n",
      "cnt: 0 - valLoss: 0.43265679478645325 - trainLoss: 0.4193306863307953\n",
      "cnt: 0 - valLoss: 0.4326559007167816 - trainLoss: 0.4193279445171356\n",
      "cnt: 0 - valLoss: 0.4326549768447876 - trainLoss: 0.4193252623081207\n",
      "cnt: 0 - valLoss: 0.43265408277511597 - trainLoss: 0.41932258009910583\n",
      "cnt: 0 - valLoss: 0.4326532483100891 - trainLoss: 0.41931986808776855\n",
      "cnt: 0 - valLoss: 0.4326522946357727 - trainLoss: 0.4193171262741089\n",
      "cnt: 0 - valLoss: 0.4326514005661011 - trainLoss: 0.419314444065094\n",
      "cnt: 0 - valLoss: 0.43265053629875183 - trainLoss: 0.4193117320537567\n",
      "cnt: 0 - valLoss: 0.4326495826244354 - trainLoss: 0.4193090498447418\n",
      "cnt: 0 - valLoss: 0.43264874815940857 - trainLoss: 0.41930630803108215\n",
      "cnt: 0 - valLoss: 0.43264782428741455 - trainLoss: 0.41930365562438965\n",
      "cnt: 0 - valLoss: 0.4326469302177429 - trainLoss: 0.41930091381073\n",
      "cnt: 0 - valLoss: 0.4326460361480713 - trainLoss: 0.4192982017993927\n",
      "cnt: 0 - valLoss: 0.43264517188072205 - trainLoss: 0.4192955195903778\n",
      "cnt: 0 - valLoss: 0.4326442778110504 - trainLoss: 0.41929277777671814\n",
      "cnt: 0 - valLoss: 0.4326433837413788 - trainLoss: 0.41929006576538086\n",
      "cnt: 0 - valLoss: 0.43264248967170715 - trainLoss: 0.41928738355636597\n",
      "cnt: 0 - valLoss: 0.43264156579971313 - trainLoss: 0.4192847013473511\n",
      "cnt: 0 - valLoss: 0.4326406717300415 - trainLoss: 0.4192819893360138\n",
      "cnt: 0 - valLoss: 0.43263980746269226 - trainLoss: 0.4192792475223541\n",
      "cnt: 0 - valLoss: 0.43263891339302063 - trainLoss: 0.4192765951156616\n",
      "cnt: 0 - valLoss: 0.432638019323349 - trainLoss: 0.41927385330200195\n",
      "cnt: 0 - valLoss: 0.43263715505599976 - trainLoss: 0.41927117109298706\n",
      "cnt: 0 - valLoss: 0.4326362907886505 - trainLoss: 0.4192684590816498\n",
      "cnt: 0 - valLoss: 0.4326353669166565 - trainLoss: 0.4192657768726349\n",
      "cnt: 0 - valLoss: 0.43263450264930725 - trainLoss: 0.4192630350589752\n",
      "cnt: 0 - valLoss: 0.43263372778892517 - trainLoss: 0.4192604124546051\n",
      "cnt: 0 - valLoss: 0.43263304233551025 - trainLoss: 0.4192579388618469\n",
      "cnt: 0 - valLoss: 0.43263232707977295 - trainLoss: 0.41925549507141113\n",
      "cnt: 0 - valLoss: 0.43263158202171326 - trainLoss: 0.41925305128097534\n",
      "cnt: 0 - valLoss: 0.43263086676597595 - trainLoss: 0.41925057768821716\n",
      "cnt: 0 - valLoss: 0.43263012170791626 - trainLoss: 0.419248104095459\n",
      "cnt: 0 - valLoss: 0.43262943625450134 - trainLoss: 0.4192456007003784\n",
      "cnt: 0 - valLoss: 0.43262866139411926 - trainLoss: 0.4192431569099426\n",
      "cnt: 0 - valLoss: 0.43262797594070435 - trainLoss: 0.4192407429218292\n",
      "cnt: 0 - valLoss: 0.43262723088264465 - trainLoss: 0.41923823952674866\n",
      "cnt: 0 - valLoss: 0.43262654542922974 - trainLoss: 0.41923579573631287\n",
      "cnt: 0 - valLoss: 0.4326257109642029 - trainLoss: 0.4192333221435547\n",
      "cnt: 0 - valLoss: 0.4326249063014984 - trainLoss: 0.4192308783531189\n",
      "cnt: 0 - valLoss: 0.43262413144111633 - trainLoss: 0.4192284047603607\n",
      "cnt: 0 - valLoss: 0.4326232671737671 - trainLoss: 0.4192259609699249\n",
      "cnt: 0 - valLoss: 0.43262240290641785 - trainLoss: 0.41922351717948914\n",
      "cnt: 0 - valLoss: 0.43262165784835815 - trainLoss: 0.41922104358673096\n",
      "cnt: 0 - valLoss: 0.4326208233833313 - trainLoss: 0.4192185699939728\n",
      "cnt: 0 - valLoss: 0.43261995911598206 - trainLoss: 0.4192160665988922\n",
      "cnt: 0 - valLoss: 0.4326191544532776 - trainLoss: 0.41921359300613403\n",
      "cnt: 0 - valLoss: 0.4326183795928955 - trainLoss: 0.419211208820343\n",
      "cnt: 0 - valLoss: 0.43261754512786865 - trainLoss: 0.41920870542526245\n",
      "cnt: 0 - valLoss: 0.4326167106628418 - trainLoss: 0.4192062318325043\n",
      "cnt: 0 - valLoss: 0.43261590600013733 - trainLoss: 0.4192037880420685\n",
      "cnt: 0 - valLoss: 0.4326150417327881 - trainLoss: 0.4192013144493103\n",
      "cnt: 0 - valLoss: 0.432614266872406 - trainLoss: 0.4191988408565521\n",
      "cnt: 0 - valLoss: 0.43261340260505676 - trainLoss: 0.41919639706611633\n",
      "cnt: 0 - valLoss: 0.4326125979423523 - trainLoss: 0.41919389367103577\n",
      "cnt: 0 - valLoss: 0.4326117932796478 - trainLoss: 0.41919147968292236\n",
      "cnt: 0 - valLoss: 0.4326109290122986 - trainLoss: 0.4191889762878418\n",
      "cnt: 0 - valLoss: 0.4326101243495941 - trainLoss: 0.419186532497406\n",
      "cnt: 0 - valLoss: 0.43260934948921204 - trainLoss: 0.4191840589046478\n",
      "cnt: 0 - valLoss: 0.4326084554195404 - trainLoss: 0.41918161511421204\n",
      "cnt: 0 - valLoss: 0.4326076805591583 - trainLoss: 0.41917917132377625\n",
      "cnt: 0 - valLoss: 0.43260684609413147 - trainLoss: 0.41917669773101807\n",
      "cnt: 0 - valLoss: 0.432606041431427 - trainLoss: 0.4191742241382599\n",
      "cnt: 0 - valLoss: 0.43260517716407776 - trainLoss: 0.4191717207431793\n",
      "cnt: 0 - valLoss: 0.4326044023036957 - trainLoss: 0.4191693067550659\n",
      "cnt: 0 - valLoss: 0.4326035678386688 - trainLoss: 0.4191668629646301\n",
      "cnt: 0 - valLoss: 0.43260273337364197 - trainLoss: 0.41916435956954956\n",
      "cnt: 0 - valLoss: 0.4326019287109375 - trainLoss: 0.4191618859767914\n",
      "cnt: 0 - valLoss: 0.43260106444358826 - trainLoss: 0.4191594123840332\n",
      "cnt: 0 - valLoss: 0.4326001703739166 - trainLoss: 0.41915690898895264\n",
      "cnt: 0 - valLoss: 0.4325993061065674 - trainLoss: 0.41915440559387207\n",
      "cnt: 0 - valLoss: 0.43259841203689575 - trainLoss: 0.4191518723964691\n",
      "cnt: 0 - valLoss: 0.4325975179672241 - trainLoss: 0.4191493093967438\n",
      "cnt: 0 - valLoss: 0.4325966536998749 - trainLoss: 0.4191468358039856\n",
      "cnt: 0 - valLoss: 0.43259578943252563 - trainLoss: 0.41914430260658264\n",
      "cnt: 0 - valLoss: 0.4325949549674988 - trainLoss: 0.4191417694091797\n",
      "cnt: 0 - valLoss: 0.43259403109550476 - trainLoss: 0.4191392958164215\n",
      "cnt: 0 - valLoss: 0.4325931966304779 - trainLoss: 0.41913676261901855\n",
      "cnt: 0 - valLoss: 0.4325922727584839 - trainLoss: 0.4191342294216156\n",
      "cnt: 0 - valLoss: 0.43259143829345703 - trainLoss: 0.41913172602653503\n",
      "cnt: 0 - valLoss: 0.432590514421463 - trainLoss: 0.4191291928291321\n",
      "cnt: 0 - valLoss: 0.43258967995643616 - trainLoss: 0.4191267192363739\n",
      "cnt: 0 - valLoss: 0.4325888156890869 - trainLoss: 0.41912412643432617\n",
      "cnt: 0 - valLoss: 0.4325879216194153 - trainLoss: 0.419121652841568\n",
      "cnt: 0 - valLoss: 0.43258705735206604 - trainLoss: 0.41911908984184265\n",
      "cnt: 0 - valLoss: 0.4325861930847168 - trainLoss: 0.4191166162490845\n",
      "cnt: 0 - valLoss: 0.43258529901504517 - trainLoss: 0.4191140830516815\n",
      "cnt: 0 - valLoss: 0.4325844347476959 - trainLoss: 0.41911154985427856\n",
      "cnt: 0 - valLoss: 0.4325835406780243 - trainLoss: 0.4191090166568756\n",
      "cnt: 0 - valLoss: 0.43258267641067505 - trainLoss: 0.41910654306411743\n",
      "cnt: 0 - valLoss: 0.4325817823410034 - trainLoss: 0.4191040098667145\n",
      "cnt: 0 - valLoss: 0.4325809180736542 - trainLoss: 0.4191015362739563\n",
      "cnt: 0 - valLoss: 0.4325800836086273 - trainLoss: 0.41909897327423096\n",
      "cnt: 0 - valLoss: 0.4325791597366333 - trainLoss: 0.419096440076828\n",
      "cnt: 0 - valLoss: 0.43257835507392883 - trainLoss: 0.41909390687942505\n",
      "cnt: 0 - valLoss: 0.4325774013996124 - trainLoss: 0.41909143328666687\n",
      "cnt: 0 - valLoss: 0.43257656693458557 - trainLoss: 0.4190889000892639\n",
      "cnt: 0 - valLoss: 0.43257570266723633 - trainLoss: 0.41908639669418335\n",
      "cnt: 0 - valLoss: 0.4325748085975647 - trainLoss: 0.4190838634967804\n",
      "cnt: 0 - valLoss: 0.4325738847255707 - trainLoss: 0.4190813899040222\n",
      "cnt: 0 - valLoss: 0.4325730502605438 - trainLoss: 0.41907885670661926\n",
      "cnt: 0 - valLoss: 0.4325721859931946 - trainLoss: 0.4190762937068939\n",
      "cnt: 0 - valLoss: 0.43257132172584534 - trainLoss: 0.41907379031181335\n",
      "cnt: 0 - valLoss: 0.43257036805152893 - trainLoss: 0.4190713167190552\n",
      "cnt: 0 - valLoss: 0.43256956338882446 - trainLoss: 0.41906875371932983\n",
      "cnt: 0 - valLoss: 0.43256866931915283 - trainLoss: 0.4190662205219269\n",
      "cnt: 0 - valLoss: 0.4325678050518036 - trainLoss: 0.4190637469291687\n",
      "cnt: 0 - valLoss: 0.43256691098213196 - trainLoss: 0.41906121373176575\n",
      "cnt: 0 - valLoss: 0.4325660467147827 - trainLoss: 0.4190586805343628\n",
      "cnt: 0 - valLoss: 0.43256518244743347 - trainLoss: 0.4190561771392822\n",
      "cnt: 0 - valLoss: 0.43256428837776184 - trainLoss: 0.4190536439418793\n",
      "cnt: 0 - valLoss: 0.4325634241104126 - trainLoss: 0.4190511107444763\n",
      "cnt: 0 - valLoss: 0.4325625002384186 - trainLoss: 0.41904863715171814\n",
      "cnt: 0 - valLoss: 0.4325615167617798 - trainLoss: 0.4190461039543152\n",
      "cnt: 0 - valLoss: 0.43256062269210815 - trainLoss: 0.41904357075691223\n",
      "cnt: 0 - valLoss: 0.43255966901779175 - trainLoss: 0.41904109716415405\n",
      "cnt: 0 - valLoss: 0.43255868554115295 - trainLoss: 0.4190385639667511\n",
      "cnt: 0 - valLoss: 0.4325577914714813 - trainLoss: 0.41903606057167053\n",
      "cnt: 0 - valLoss: 0.4325568377971649 - trainLoss: 0.41903355717658997\n",
      "cnt: 0 - valLoss: 0.4325558841228485 - trainLoss: 0.4190310537815094\n",
      "cnt: 0 - valLoss: 0.4325549602508545 - trainLoss: 0.41902855038642883\n",
      "cnt: 0 - valLoss: 0.4325540065765381 - trainLoss: 0.41902604699134827\n",
      "cnt: 0 - valLoss: 0.4325530529022217 - trainLoss: 0.4190235137939453\n",
      "cnt: 0 - valLoss: 0.43255212903022766 - trainLoss: 0.41902098059654236\n",
      "cnt: 0 - valLoss: 0.43255117535591125 - trainLoss: 0.4190185070037842\n",
      "cnt: 0 - valLoss: 0.43255019187927246 - trainLoss: 0.4190159738063812\n",
      "cnt: 0 - valLoss: 0.43254929780960083 - trainLoss: 0.41901344060897827\n",
      "cnt: 0 - valLoss: 0.4325483441352844 - trainLoss: 0.4190109670162201\n",
      "cnt: 0 - valLoss: 0.4325474202632904 - trainLoss: 0.41900843381881714\n",
      "cnt: 0 - valLoss: 0.432546466588974 - trainLoss: 0.4190059304237366\n",
      "cnt: 0 - valLoss: 0.4325455129146576 - trainLoss: 0.419003427028656\n",
      "cnt: 0 - valLoss: 0.43254461884498596 - trainLoss: 0.4190009534358978\n",
      "cnt: 0 - valLoss: 0.43254363536834717 - trainLoss: 0.4189984202384949\n",
      "cnt: 0 - valLoss: 0.43254274129867554 - trainLoss: 0.4189959168434143\n",
      "cnt: 0 - valLoss: 0.43254175782203674 - trainLoss: 0.41899338364601135\n",
      "cnt: 0 - valLoss: 0.43254080414772034 - trainLoss: 0.4189909100532532\n",
      "cnt: 0 - valLoss: 0.4325399100780487 - trainLoss: 0.4189883768558502\n",
      "cnt: 0 - valLoss: 0.4325389266014099 - trainLoss: 0.41898590326309204\n",
      "cnt: 0 - valLoss: 0.4325380325317383 - trainLoss: 0.4189833104610443\n",
      "cnt: 0 - valLoss: 0.4325370788574219 - trainLoss: 0.41898083686828613\n",
      "cnt: 0 - valLoss: 0.43253612518310547 - trainLoss: 0.41897836327552795\n",
      "cnt: 0 - valLoss: 0.43253520131111145 - trainLoss: 0.418975830078125\n",
      "cnt: 0 - valLoss: 0.43253424763679504 - trainLoss: 0.4189733564853668\n",
      "cnt: 0 - valLoss: 0.4325333535671234 - trainLoss: 0.41897082328796387\n",
      "cnt: 0 - valLoss: 0.432532399892807 - trainLoss: 0.4189682602882385\n",
      "cnt: 0 - valLoss: 0.432531476020813 - trainLoss: 0.41896575689315796\n",
      "cnt: 0 - valLoss: 0.4325305223464966 - trainLoss: 0.4189632833003998\n",
      "cnt: 0 - valLoss: 0.4325295686721802 - trainLoss: 0.4189607501029968\n",
      "cnt: 0 - valLoss: 0.43252867460250854 - trainLoss: 0.41895824670791626\n",
      "cnt: 0 - valLoss: 0.4325277805328369 - trainLoss: 0.4189557433128357\n",
      "cnt: 0 - valLoss: 0.4325268268585205 - trainLoss: 0.4189532697200775\n",
      "cnt: 0 - valLoss: 0.4325259029865265 - trainLoss: 0.41895073652267456\n",
      "cnt: 0 - valLoss: 0.43252497911453247 - trainLoss: 0.418948233127594\n",
      "cnt: 0 - valLoss: 0.4325239956378937 - trainLoss: 0.4189457297325134\n",
      "cnt: 0 - valLoss: 0.43252310156822205 - trainLoss: 0.41894322633743286\n",
      "cnt: 0 - valLoss: 0.432522177696228 - trainLoss: 0.4189406931400299\n",
      "cnt: 0 - valLoss: 0.432521253824234 - trainLoss: 0.41893821954727173\n",
      "cnt: 0 - valLoss: 0.43252032995224 - trainLoss: 0.41893571615219116\n",
      "cnt: 0 - valLoss: 0.4325193762779236 - trainLoss: 0.4189332127571106\n",
      "cnt: 0 - valLoss: 0.43251851201057434 - trainLoss: 0.41893067955970764\n",
      "cnt: 0 - valLoss: 0.4325175881385803 - trainLoss: 0.4189281463623047\n",
      "cnt: 0 - valLoss: 0.4325166344642639 - trainLoss: 0.41892561316490173\n",
      "cnt: 0 - valLoss: 0.4325157403945923 - trainLoss: 0.41892313957214355\n",
      "cnt: 0 - valLoss: 0.4325147867202759 - trainLoss: 0.4189206659793854\n",
      "cnt: 0 - valLoss: 0.4325140714645386 - trainLoss: 0.4189182221889496\n",
      "cnt: 0 - valLoss: 0.43251338601112366 - trainLoss: 0.41891589760780334\n",
      "cnt: 0 - valLoss: 0.4325126111507416 - trainLoss: 0.4189136326313019\n",
      "cnt: 0 - valLoss: 0.4325118660926819 - trainLoss: 0.418911337852478\n",
      "cnt: 0 - valLoss: 0.4325111508369446 - trainLoss: 0.4189090132713318\n",
      "cnt: 0 - valLoss: 0.4325103759765625 - trainLoss: 0.41890671849250793\n",
      "cnt: 0 - valLoss: 0.4325096607208252 - trainLoss: 0.4189043939113617\n",
      "cnt: 0 - valLoss: 0.4325089156627655 - trainLoss: 0.41890206933021545\n",
      "cnt: 0 - valLoss: 0.4325081408023834 - trainLoss: 0.4188997745513916\n",
      "cnt: 0 - valLoss: 0.4325074255466461 - trainLoss: 0.41889744997024536\n",
      "cnt: 0 - valLoss: 0.4325066804885864 - trainLoss: 0.4188951551914215\n",
      "cnt: 0 - valLoss: 0.4325059652328491 - trainLoss: 0.41889289021492004\n",
      "cnt: 0 - valLoss: 0.43250522017478943 - trainLoss: 0.4188905656337738\n",
      "cnt: 0 - valLoss: 0.43250447511672974 - trainLoss: 0.41888827085494995\n",
      "cnt: 0 - valLoss: 0.43250373005867004 - trainLoss: 0.4188859462738037\n",
      "cnt: 0 - valLoss: 0.43250301480293274 - trainLoss: 0.41888362169265747\n",
      "cnt: 0 - valLoss: 0.43250223994255066 - trainLoss: 0.4188813269138336\n",
      "cnt: 0 - valLoss: 0.43250149488449097 - trainLoss: 0.41887906193733215\n",
      "cnt: 0 - valLoss: 0.4325007200241089 - trainLoss: 0.4188767373561859\n",
      "cnt: 0 - valLoss: 0.4325000047683716 - trainLoss: 0.4188743829727173\n",
      "cnt: 0 - valLoss: 0.4324992895126343 - trainLoss: 0.4188721179962158\n",
      "cnt: 0 - valLoss: 0.4324985146522522 - trainLoss: 0.41886982321739197\n",
      "cnt: 0 - valLoss: 0.4324977397918701 - trainLoss: 0.4188674986362457\n",
      "cnt: 0 - valLoss: 0.4324969947338104 - trainLoss: 0.4188651740550995\n",
      "cnt: 0 - valLoss: 0.43249621987342834 - trainLoss: 0.41886287927627563\n",
      "cnt: 0 - valLoss: 0.43249550461769104 - trainLoss: 0.41886061429977417\n",
      "cnt: 0 - valLoss: 0.43249472975730896 - trainLoss: 0.41885828971862793\n",
      "cnt: 0 - valLoss: 0.43249401450157166 - trainLoss: 0.4188559949398041\n",
      "cnt: 0 - valLoss: 0.4324932396411896 - trainLoss: 0.41885367035865784\n",
      "cnt: 0 - valLoss: 0.4324924647808075 - trainLoss: 0.41885140538215637\n",
      "cnt: 0 - valLoss: 0.4324916899204254 - trainLoss: 0.41884908080101013\n",
      "cnt: 0 - valLoss: 0.4324909448623657 - trainLoss: 0.4188467860221863\n",
      "cnt: 0 - valLoss: 0.43249019980430603 - trainLoss: 0.41884446144104004\n",
      "cnt: 0 - valLoss: 0.43248945474624634 - trainLoss: 0.4188421666622162\n",
      "cnt: 0 - valLoss: 0.43248867988586426 - trainLoss: 0.41883984208106995\n",
      "cnt: 0 - valLoss: 0.4324879050254822 - trainLoss: 0.4188375473022461\n",
      "cnt: 0 - valLoss: 0.4324871301651001 - trainLoss: 0.41883528232574463\n",
      "cnt: 0 - valLoss: 0.4324863851070404 - trainLoss: 0.4188329577445984\n",
      "cnt: 0 - valLoss: 0.4324856102466583 - trainLoss: 0.41883063316345215\n",
      "cnt: 0 - valLoss: 0.43248486518859863 - trainLoss: 0.4188283383846283\n",
      "cnt: 0 - valLoss: 0.43248409032821655 - trainLoss: 0.41882607340812683\n",
      "cnt: 0 - valLoss: 0.4324833154678345 - trainLoss: 0.4188237190246582\n",
      "cnt: 0 - valLoss: 0.4324825704097748 - trainLoss: 0.41882145404815674\n",
      "cnt: 0 - valLoss: 0.4324817955493927 - trainLoss: 0.4188191294670105\n",
      "cnt: 0 - valLoss: 0.432481050491333 - trainLoss: 0.41881683468818665\n",
      "cnt: 0 - valLoss: 0.4324802756309509 - trainLoss: 0.4188145101070404\n",
      "cnt: 0 - valLoss: 0.43247950077056885 - trainLoss: 0.41881218552589417\n",
      "cnt: 0 - valLoss: 0.43247875571250916 - trainLoss: 0.4188098907470703\n",
      "cnt: 0 - valLoss: 0.4324779808521271 - trainLoss: 0.41880765557289124\n",
      "cnt: 0 - valLoss: 0.4324771463871002 - trainLoss: 0.4188053011894226\n",
      "cnt: 0 - valLoss: 0.43247634172439575 - trainLoss: 0.41880300641059875\n",
      "cnt: 0 - valLoss: 0.4324754774570465 - trainLoss: 0.4188007414340973\n",
      "cnt: 0 - valLoss: 0.43247464299201965 - trainLoss: 0.41879841685295105\n",
      "cnt: 0 - valLoss: 0.4324737787246704 - trainLoss: 0.4187960922718048\n",
      "cnt: 0 - valLoss: 0.43247300386428833 - trainLoss: 0.41879382729530334\n",
      "cnt: 0 - valLoss: 0.4324721395969391 - trainLoss: 0.4187915623188019\n",
      "cnt: 0 - valLoss: 0.43247127532958984 - trainLoss: 0.418789267539978\n",
      "cnt: 0 - valLoss: 0.4324704706668854 - trainLoss: 0.4187869429588318\n",
      "cnt: 0 - valLoss: 0.43246960639953613 - trainLoss: 0.4187846779823303\n",
      "cnt: 0 - valLoss: 0.43246880173683167 - trainLoss: 0.41878241300582886\n",
      "cnt: 0 - valLoss: 0.4324679374694824 - trainLoss: 0.4187800884246826\n",
      "cnt: 0 - valLoss: 0.4324670732021332 - trainLoss: 0.41877779364585876\n",
      "cnt: 0 - valLoss: 0.4324662387371063 - trainLoss: 0.4187754690647125\n",
      "cnt: 0 - valLoss: 0.43246546387672424 - trainLoss: 0.41877320408821106\n",
      "cnt: 0 - valLoss: 0.432464599609375 - trainLoss: 0.4187709391117096\n",
      "cnt: 0 - valLoss: 0.43246373534202576 - trainLoss: 0.41876861453056335\n",
      "cnt: 0 - valLoss: 0.4324629306793213 - trainLoss: 0.4187663197517395\n",
      "cnt: 0 - valLoss: 0.43246206641197205 - trainLoss: 0.41876405477523804\n",
      "cnt: 0 - valLoss: 0.4324612617492676 - trainLoss: 0.4187617301940918\n",
      "cnt: 0 - valLoss: 0.43246039748191833 - trainLoss: 0.41875943541526794\n",
      "cnt: 0 - valLoss: 0.4324595332145691 - trainLoss: 0.4187571704387665\n",
      "cnt: 0 - valLoss: 0.43245869874954224 - trainLoss: 0.41875484585762024\n",
      "cnt: 0 - valLoss: 0.4324578642845154 - trainLoss: 0.418752521276474\n",
      "cnt: 0 - valLoss: 0.43245700001716614 - trainLoss: 0.41875025629997253\n",
      "cnt: 0 - valLoss: 0.43245619535446167 - trainLoss: 0.4187479615211487\n",
      "cnt: 0 - valLoss: 0.4324553310871124 - trainLoss: 0.4187456965446472\n",
      "cnt: 0 - valLoss: 0.43245452642440796 - trainLoss: 0.418743371963501\n",
      "cnt: 0 - valLoss: 0.4324537217617035 - trainLoss: 0.4187411069869995\n",
      "cnt: 0 - valLoss: 0.43245285749435425 - trainLoss: 0.41873884201049805\n",
      "cnt: 0 - valLoss: 0.432451993227005 - trainLoss: 0.4187365174293518\n",
      "cnt: 0 - valLoss: 0.43245118856430054 - trainLoss: 0.41873422265052795\n",
      "cnt: 0 - valLoss: 0.4324503242969513 - trainLoss: 0.4187318980693817\n",
      "cnt: 0 - valLoss: 0.4324495196342468 - trainLoss: 0.41872960329055786\n",
      "cnt: 0 - valLoss: 0.4324486553668976 - trainLoss: 0.4187273681163788\n",
      "cnt: 0 - valLoss: 0.43244779109954834 - trainLoss: 0.41872504353523254\n",
      "cnt: 0 - valLoss: 0.4324469268321991 - trainLoss: 0.4187227487564087\n",
      "cnt: 0 - valLoss: 0.43244609236717224 - trainLoss: 0.4187204837799072\n",
      "cnt: 0 - valLoss: 0.43244531750679016 - trainLoss: 0.418718159198761\n",
      "cnt: 0 - valLoss: 0.4324444532394409 - trainLoss: 0.4187158942222595\n",
      "cnt: 0 - valLoss: 0.4324435889720917 - trainLoss: 0.4187135696411133\n",
      "cnt: 0 - valLoss: 0.4324427545070648 - trainLoss: 0.4187113046646118\n",
      "cnt: 0 - valLoss: 0.43244192004203796 - trainLoss: 0.41870900988578796\n",
      "cnt: 0 - valLoss: 0.4324411153793335 - trainLoss: 0.4187066853046417\n",
      "cnt: 0 - valLoss: 0.43244025111198425 - trainLoss: 0.41870442032814026\n",
      "cnt: 0 - valLoss: 0.432439386844635 - trainLoss: 0.4187021553516388\n",
      "cnt: 0 - valLoss: 0.43243855237960815 - trainLoss: 0.41869989037513733\n",
      "cnt: 0 - valLoss: 0.4324376881122589 - trainLoss: 0.4186975657939911\n",
      "cnt: 0 - valLoss: 0.43243685364723206 - trainLoss: 0.41869527101516724\n",
      "cnt: 0 - valLoss: 0.4324360489845276 - trainLoss: 0.41869300603866577\n",
      "cnt: 0 - valLoss: 0.43243518471717834 - trainLoss: 0.41869068145751953\n",
      "cnt: 0 - valLoss: 0.4324343502521515 - trainLoss: 0.41868841648101807\n",
      "cnt: 0 - valLoss: 0.43243348598480225 - trainLoss: 0.4186860918998718\n",
      "cnt: 0 - valLoss: 0.432432621717453 - trainLoss: 0.41868382692337036\n",
      "cnt: 0 - valLoss: 0.43243175745010376 - trainLoss: 0.4186815321445465\n",
      "cnt: 0 - valLoss: 0.4324309825897217 - trainLoss: 0.41867926716804504\n",
      "cnt: 0 - valLoss: 0.4324301481246948 - trainLoss: 0.4186769425868988\n",
      "cnt: 0 - valLoss: 0.4324292838573456 - trainLoss: 0.41867467761039734\n",
      "cnt: 0 - valLoss: 0.43242841958999634 - trainLoss: 0.4186723530292511\n",
      "cnt: 0 - valLoss: 0.4324275553226471 - trainLoss: 0.41867008805274963\n",
      "cnt: 0 - valLoss: 0.4324267506599426 - trainLoss: 0.4186677932739258\n",
      "cnt: 0 - valLoss: 0.4324258863925934 - trainLoss: 0.4186655282974243\n",
      "cnt: 0 - valLoss: 0.4324250817298889 - trainLoss: 0.4186632037162781\n",
      "cnt: 0 - valLoss: 0.4324241876602173 - trainLoss: 0.4186609089374542\n",
      "cnt: 0 - valLoss: 0.43242335319519043 - trainLoss: 0.41865861415863037\n",
      "cnt: 0 - valLoss: 0.4324225187301636 - trainLoss: 0.4186563193798065\n",
      "cnt: 0 - valLoss: 0.43242165446281433 - trainLoss: 0.41865405440330505\n",
      "cnt: 0 - valLoss: 0.4324207901954651 - trainLoss: 0.4186517298221588\n",
      "cnt: 0 - valLoss: 0.432420015335083 - trainLoss: 0.41864946484565735\n",
      "cnt: 0 - valLoss: 0.43241918087005615 - trainLoss: 0.4186471700668335\n",
      "cnt: 0 - valLoss: 0.4324183166027069 - trainLoss: 0.41864487528800964\n",
      "cnt: 0 - valLoss: 0.43241745233535767 - trainLoss: 0.4186425805091858\n",
      "cnt: 0 - valLoss: 0.4324165880680084 - trainLoss: 0.4186403155326843\n",
      "cnt: 0 - valLoss: 0.43241578340530396 - trainLoss: 0.4186379909515381\n",
      "cnt: 0 - valLoss: 0.4324149191379547 - trainLoss: 0.4186357259750366\n",
      "cnt: 0 - valLoss: 0.43241405487060547 - trainLoss: 0.41863346099853516\n",
      "cnt: 0 - valLoss: 0.432413250207901 - trainLoss: 0.4186311364173889\n",
      "cnt: 0 - valLoss: 0.43241238594055176 - trainLoss: 0.41862884163856506\n",
      "cnt: 0 - valLoss: 0.4324115514755249 - trainLoss: 0.4186265766620636\n",
      "cnt: 0 - valLoss: 0.43241068720817566 - trainLoss: 0.41862431168556213\n",
      "cnt: 0 - valLoss: 0.4324098527431488 - trainLoss: 0.4186219871044159\n",
      "cnt: 0 - valLoss: 0.43240901827812195 - trainLoss: 0.41861969232559204\n",
      "cnt: 0 - valLoss: 0.4324081838130951 - trainLoss: 0.4186173975467682\n",
      "cnt: 0 - valLoss: 0.43240734934806824 - trainLoss: 0.41861510276794434\n",
      "cnt: 0 - valLoss: 0.432406485080719 - trainLoss: 0.41861283779144287\n",
      "cnt: 0 - valLoss: 0.43240565061569214 - trainLoss: 0.4186105728149414\n",
      "cnt: 0 - valLoss: 0.4324048161506653 - trainLoss: 0.4186082184314728\n",
      "cnt: 0 - valLoss: 0.4324039816856384 - trainLoss: 0.4186059832572937\n",
      "cnt: 0 - valLoss: 0.4324031472206116 - trainLoss: 0.41860365867614746\n",
      "cnt: 0 - valLoss: 0.43240228295326233 - trainLoss: 0.418601393699646\n",
      "cnt: 0 - valLoss: 0.43240147829055786 - trainLoss: 0.41859912872314453\n",
      "cnt: 0 - valLoss: 0.4324006140232086 - trainLoss: 0.4185968339443207\n",
      "cnt: 0 - valLoss: 0.43239977955818176 - trainLoss: 0.4185945391654968\n",
      "cnt: 0 - valLoss: 0.4323989450931549 - trainLoss: 0.418592244386673\n",
      "cnt: 0 - valLoss: 0.43239808082580566 - trainLoss: 0.4185899794101715\n",
      "cnt: 0 - valLoss: 0.4323973059654236 - trainLoss: 0.41858765482902527\n",
      "cnt: 0 - valLoss: 0.43239641189575195 - trainLoss: 0.4185853898525238\n",
      "cnt: 0 - valLoss: 0.4323955774307251 - trainLoss: 0.41858309507369995\n",
      "cnt: 0 - valLoss: 0.43239474296569824 - trainLoss: 0.4185808002948761\n",
      "cnt: 0 - valLoss: 0.4323939085006714 - trainLoss: 0.41857850551605225\n",
      "cnt: 0 - valLoss: 0.4323931038379669 - trainLoss: 0.4185762405395508\n",
      "cnt: 0 - valLoss: 0.4323922395706177 - trainLoss: 0.4185739755630493\n",
      "cnt: 0 - valLoss: 0.4323914051055908 - trainLoss: 0.4185716509819031\n",
      "cnt: 0 - valLoss: 0.43239057064056396 - trainLoss: 0.4185693562030792\n",
      "cnt: 0 - valLoss: 0.4323897063732147 - trainLoss: 0.41856706142425537\n",
      "cnt: 0 - valLoss: 0.43238890171051025 - trainLoss: 0.4185647964477539\n",
      "cnt: 0 - valLoss: 0.432388037443161 - trainLoss: 0.41856250166893005\n",
      "cnt: 0 - valLoss: 0.43238720297813416 - trainLoss: 0.4185601770877838\n",
      "cnt: 0 - valLoss: 0.4323863685131073 - trainLoss: 0.41855791211128235\n",
      "cnt: 0 - valLoss: 0.43238553404808044 - trainLoss: 0.4185556471347809\n",
      "cnt: 0 - valLoss: 0.4323846995830536 - trainLoss: 0.41855332255363464\n",
      "cnt: 0 - valLoss: 0.43238383531570435 - trainLoss: 0.4185510575771332\n",
      "cnt: 0 - valLoss: 0.4323830008506775 - trainLoss: 0.4185487627983093\n",
      "cnt: 0 - valLoss: 0.43238216638565063 - trainLoss: 0.41854649782180786\n",
      "cnt: 0 - valLoss: 0.43238136172294617 - trainLoss: 0.4185441732406616\n",
      "cnt: 0 - valLoss: 0.4323804974555969 - trainLoss: 0.41854193806648254\n",
      "cnt: 0 - valLoss: 0.43237969279289246 - trainLoss: 0.4185396432876587\n",
      "cnt: 0 - valLoss: 0.4323787987232208 - trainLoss: 0.4185373783111572\n",
      "cnt: 0 - valLoss: 0.43237796425819397 - trainLoss: 0.418535053730011\n",
      "cnt: 0 - valLoss: 0.4323771595954895 - trainLoss: 0.41853275895118713\n",
      "cnt: 0 - valLoss: 0.43237629532814026 - trainLoss: 0.4185304641723633\n",
      "cnt: 0 - valLoss: 0.4323754906654358 - trainLoss: 0.41852816939353943\n",
      "cnt: 0 - valLoss: 0.43237462639808655 - trainLoss: 0.41852590441703796\n",
      "cnt: 0 - valLoss: 0.4323737621307373 - trainLoss: 0.4185236394405365\n",
      "cnt: 0 - valLoss: 0.43237295746803284 - trainLoss: 0.41852131485939026\n",
      "cnt: 0 - valLoss: 0.4323720932006836 - trainLoss: 0.4185190498828888\n",
      "cnt: 0 - valLoss: 0.43237125873565674 - trainLoss: 0.41851672530174255\n",
      "cnt: 0 - valLoss: 0.4323703944683075 - trainLoss: 0.4185144603252411\n",
      "cnt: 0 - valLoss: 0.432369589805603 - trainLoss: 0.41851216554641724\n",
      "cnt: 0 - valLoss: 0.4323687255382538 - trainLoss: 0.41850993037223816\n",
      "cnt: 0 - valLoss: 0.43236786127090454 - trainLoss: 0.41850757598876953\n",
      "cnt: 0 - valLoss: 0.4323670566082001 - trainLoss: 0.41850531101226807\n",
      "cnt: 0 - valLoss: 0.43236619234085083 - trainLoss: 0.4185030460357666\n",
      "cnt: 0 - valLoss: 0.4323653280735016 - trainLoss: 0.41850078105926514\n",
      "cnt: 0 - valLoss: 0.43236446380615234 - trainLoss: 0.4184984564781189\n",
      "cnt: 0 - valLoss: 0.4323636293411255 - trainLoss: 0.41849616169929504\n",
      "cnt: 0 - valLoss: 0.43236279487609863 - trainLoss: 0.4184938967227936\n",
      "cnt: 0 - valLoss: 0.4323619604110718 - trainLoss: 0.4184916019439697\n",
      "cnt: 0 - valLoss: 0.4323611259460449 - trainLoss: 0.4184893071651459\n",
      "cnt: 0 - valLoss: 0.4323602616786957 - trainLoss: 0.4184870421886444\n",
      "cnt: 0 - valLoss: 0.4323594272136688 - trainLoss: 0.41848477721214294\n",
      "cnt: 0 - valLoss: 0.4323585629463196 - trainLoss: 0.4184824526309967\n",
      "cnt: 0 - valLoss: 0.43235769867897034 - trainLoss: 0.41848015785217285\n",
      "cnt: 0 - valLoss: 0.43235689401626587 - trainLoss: 0.418477863073349\n",
      "cnt: 0 - valLoss: 0.4323560297489166 - trainLoss: 0.41847559809684753\n",
      "cnt: 0 - valLoss: 0.43235522508621216 - trainLoss: 0.4184733033180237\n",
      "cnt: 0 - valLoss: 0.4323543608188629 - trainLoss: 0.4184710681438446\n",
      "cnt: 0 - valLoss: 0.43235349655151367 - trainLoss: 0.41846877336502075\n",
      "cnt: 0 - valLoss: 0.43235263228416443 - trainLoss: 0.4184664785861969\n",
      "cnt: 0 - valLoss: 0.4323517978191376 - trainLoss: 0.41846418380737305\n",
      "cnt: 0 - valLoss: 0.4323509633541107 - trainLoss: 0.4184619188308716\n",
      "cnt: 0 - valLoss: 0.43235012888908386 - trainLoss: 0.41845959424972534\n",
      "cnt: 0 - valLoss: 0.432349294424057 - trainLoss: 0.4184573292732239\n",
      "cnt: 0 - valLoss: 0.43234843015670776 - trainLoss: 0.41845500469207764\n",
      "cnt: 0 - valLoss: 0.4323475956916809 - trainLoss: 0.41845273971557617\n",
      "cnt: 0 - valLoss: 0.43234676122665405 - trainLoss: 0.4184504449367523\n",
      "cnt: 0 - valLoss: 0.4323458671569824 - trainLoss: 0.41844820976257324\n",
      "cnt: 0 - valLoss: 0.43234509229660034 - trainLoss: 0.4184459149837494\n",
      "cnt: 0 - valLoss: 0.4323441982269287 - trainLoss: 0.41844362020492554\n",
      "cnt: 0 - valLoss: 0.43234339356422424 - trainLoss: 0.4184413552284241\n",
      "cnt: 0 - valLoss: 0.432342529296875 - trainLoss: 0.4184390604496002\n",
      "cnt: 0 - valLoss: 0.43234166502952576 - trainLoss: 0.418436735868454\n",
      "cnt: 0 - valLoss: 0.4323408603668213 - trainLoss: 0.4184344708919525\n",
      "cnt: 0 - valLoss: 0.4323400557041168 - trainLoss: 0.41843217611312866\n",
      "cnt: 0 - valLoss: 0.4323391318321228 - trainLoss: 0.4184298813343048\n",
      "cnt: 0 - valLoss: 0.43233832716941833 - trainLoss: 0.41842761635780334\n",
      "cnt: 0 - valLoss: 0.4323374629020691 - trainLoss: 0.4184253513813019\n",
      "cnt: 0 - valLoss: 0.4323366582393646 - trainLoss: 0.418423056602478\n",
      "cnt: 0 - valLoss: 0.4323357939720154 - trainLoss: 0.41842079162597656\n",
      "cnt: 0 - valLoss: 0.4323349893093109 - trainLoss: 0.4184184968471527\n",
      "cnt: 0 - valLoss: 0.43233412504196167 - trainLoss: 0.41841620206832886\n",
      "cnt: 0 - valLoss: 0.4323332607746124 - trainLoss: 0.4184138774871826\n",
      "cnt: 0 - valLoss: 0.43233242630958557 - trainLoss: 0.4184116721153259\n",
      "cnt: 0 - valLoss: 0.4323315918445587 - trainLoss: 0.4184093475341797\n",
      "cnt: 0 - valLoss: 0.4323307275772095 - trainLoss: 0.4184070825576782\n",
      "cnt: 0 - valLoss: 0.432329922914505 - trainLoss: 0.41840481758117676\n",
      "cnt: 0 - valLoss: 0.4323290288448334 - trainLoss: 0.4184024930000305\n",
      "cnt: 0 - valLoss: 0.4323282241821289 - trainLoss: 0.41840019822120667\n",
      "cnt: 0 - valLoss: 0.43232738971710205 - trainLoss: 0.4183979332447052\n",
      "cnt: 0 - valLoss: 0.4323265552520752 - trainLoss: 0.41839566826820374\n",
      "cnt: 0 - valLoss: 0.43232572078704834 - trainLoss: 0.4183933436870575\n",
      "cnt: 0 - valLoss: 0.4323248565196991 - trainLoss: 0.41839107871055603\n",
      "cnt: 0 - valLoss: 0.43232402205467224 - trainLoss: 0.41838881373405457\n",
      "cnt: 0 - valLoss: 0.4323233664035797 - trainLoss: 0.4183865487575531\n",
      "cnt: 0 - valLoss: 0.43232274055480957 - trainLoss: 0.4183844327926636\n",
      "cnt: 0 - valLoss: 0.43232208490371704 - trainLoss: 0.41838228702545166\n",
      "cnt: 0 - valLoss: 0.4323213994503021 - trainLoss: 0.41838017106056213\n",
      "cnt: 0 - valLoss: 0.4323207437992096 - trainLoss: 0.4183780252933502\n",
      "cnt: 0 - valLoss: 0.43232011795043945 - trainLoss: 0.4183758795261383\n",
      "cnt: 0 - valLoss: 0.4323194622993469 - trainLoss: 0.41837379336357117\n",
      "cnt: 0 - valLoss: 0.432318776845932 - trainLoss: 0.41837167739868164\n",
      "cnt: 0 - valLoss: 0.43231818079948425 - trainLoss: 0.4183695316314697\n",
      "cnt: 0 - valLoss: 0.43231749534606934 - trainLoss: 0.4183674156665802\n",
      "cnt: 0 - valLoss: 0.4323168694972992 - trainLoss: 0.4183652997016907\n",
      "cnt: 0 - valLoss: 0.43231621384620667 - trainLoss: 0.41836312413215637\n",
      "cnt: 0 - valLoss: 0.43231552839279175 - trainLoss: 0.4183610677719116\n",
      "cnt: 0 - valLoss: 0.432314932346344 - trainLoss: 0.4183589220046997\n",
      "cnt: 0 - valLoss: 0.4323142468929291 - trainLoss: 0.4183567762374878\n",
      "cnt: 0 - valLoss: 0.43231356143951416 - trainLoss: 0.41835466027259827\n",
      "cnt: 0 - valLoss: 0.43231290578842163 - trainLoss: 0.41835251450538635\n",
      "cnt: 0 - valLoss: 0.4323122799396515 - trainLoss: 0.4183504581451416\n",
      "cnt: 0 - valLoss: 0.4323115944862366 - trainLoss: 0.4183483123779297\n",
      "cnt: 0 - valLoss: 0.43231090903282166 - trainLoss: 0.4183461666107178\n",
      "cnt: 0 - valLoss: 0.43231022357940674 - trainLoss: 0.41834408044815063\n",
      "cnt: 0 - valLoss: 0.4323095679283142 - trainLoss: 0.4183419346809387\n",
      "cnt: 0 - valLoss: 0.4323089122772217 - trainLoss: 0.4183398485183716\n",
      "cnt: 0 - valLoss: 0.43230822682380676 - trainLoss: 0.41833770275115967\n",
      "cnt: 0 - valLoss: 0.43230754137039185 - trainLoss: 0.41833561658859253\n",
      "cnt: 0 - valLoss: 0.43230685591697693 - trainLoss: 0.418333500623703\n",
      "cnt: 0 - valLoss: 0.4323062300682068 - trainLoss: 0.4183313250541687\n",
      "cnt: 0 - valLoss: 0.43230554461479187 - trainLoss: 0.41832923889160156\n",
      "cnt: 0 - valLoss: 0.43230488896369934 - trainLoss: 0.41832712292671204\n",
      "cnt: 0 - valLoss: 0.4323042035102844 - trainLoss: 0.4183249771595001\n",
      "cnt: 0 - valLoss: 0.4323035180568695 - trainLoss: 0.4183228611946106\n",
      "cnt: 0 - valLoss: 0.4323028326034546 - trainLoss: 0.41832074522972107\n",
      "cnt: 0 - valLoss: 0.4323021471500397 - trainLoss: 0.41831859946250916\n",
      "cnt: 0 - valLoss: 0.43230146169662476 - trainLoss: 0.418316513299942\n",
      "cnt: 0 - valLoss: 0.43230077624320984 - trainLoss: 0.4183143675327301\n",
      "cnt: 0 - valLoss: 0.4323000907897949 - trainLoss: 0.41831228137016296\n",
      "cnt: 0 - valLoss: 0.43229940533638 - trainLoss: 0.41831013560295105\n",
      "cnt: 0 - valLoss: 0.4322987198829651 - trainLoss: 0.41830798983573914\n",
      "cnt: 0 - valLoss: 0.43229803442955017 - trainLoss: 0.4183058440685272\n",
      "cnt: 0 - valLoss: 0.43229734897613525 - trainLoss: 0.4183037579059601\n",
      "cnt: 0 - valLoss: 0.43229666352272034 - trainLoss: 0.41830167174339294\n",
      "cnt: 0 - valLoss: 0.4322959780693054 - trainLoss: 0.41829952597618103\n",
      "cnt: 0 - valLoss: 0.4322952926158905 - trainLoss: 0.4182974100112915\n",
      "cnt: 0 - valLoss: 0.4322946071624756 - trainLoss: 0.41829532384872437\n",
      "cnt: 0 - valLoss: 0.43229392170906067 - trainLoss: 0.41829314827919006\n",
      "cnt: 0 - valLoss: 0.43229320645332336 - trainLoss: 0.41829103231430054\n",
      "cnt: 0 - valLoss: 0.43229252099990845 - trainLoss: 0.418288916349411\n",
      "cnt: 0 - valLoss: 0.43229177594184875 - trainLoss: 0.4182868003845215\n",
      "cnt: 0 - valLoss: 0.4322911500930786 - trainLoss: 0.41828471422195435\n",
      "cnt: 0 - valLoss: 0.4322904050350189 - trainLoss: 0.41828256845474243\n",
      "cnt: 0 - valLoss: 0.432289719581604 - trainLoss: 0.4182804226875305\n",
      "cnt: 0 - valLoss: 0.4322890341281891 - trainLoss: 0.4182783365249634\n",
      "cnt: 0 - valLoss: 0.43228834867477417 - trainLoss: 0.41827619075775146\n",
      "cnt: 0 - valLoss: 0.43228766322135925 - trainLoss: 0.4182741045951843\n",
      "cnt: 0 - valLoss: 0.4322868883609772 - trainLoss: 0.4182719588279724\n",
      "cnt: 0 - valLoss: 0.43228620290756226 - trainLoss: 0.4182698130607605\n",
      "cnt: 0 - valLoss: 0.43228551745414734 - trainLoss: 0.41826772689819336\n",
      "cnt: 0 - valLoss: 0.4322848320007324 - trainLoss: 0.41826561093330383\n",
      "cnt: 0 - valLoss: 0.4322841465473175 - trainLoss: 0.4182634949684143\n",
      "cnt: 0 - valLoss: 0.4322834312915802 - trainLoss: 0.4182613492012024\n",
      "cnt: 0 - valLoss: 0.4322827458381653 - trainLoss: 0.4182592034339905\n",
      "cnt: 0 - valLoss: 0.4322820007801056 - trainLoss: 0.4182571470737457\n",
      "cnt: 0 - valLoss: 0.4322813153266907 - trainLoss: 0.4182550013065338\n",
      "cnt: 0 - valLoss: 0.43228062987327576 - trainLoss: 0.4182528555393219\n",
      "cnt: 0 - valLoss: 0.43227994441986084 - trainLoss: 0.41825076937675476\n",
      "cnt: 0 - valLoss: 0.43227922916412354 - trainLoss: 0.4182486832141876\n",
      "cnt: 0 - valLoss: 0.43227848410606384 - trainLoss: 0.4182465374469757\n",
      "cnt: 0 - valLoss: 0.4322777986526489 - trainLoss: 0.4182444214820862\n",
      "cnt: 0 - valLoss: 0.432277113199234 - trainLoss: 0.41824230551719666\n",
      "cnt: 0 - valLoss: 0.4322763979434967 - trainLoss: 0.41824018955230713\n",
      "cnt: 0 - valLoss: 0.4322757124900818 - trainLoss: 0.4182380437850952\n",
      "cnt: 0 - valLoss: 0.4322749674320221 - trainLoss: 0.4182359278202057\n",
      "cnt: 0 - valLoss: 0.4322742819786072 - trainLoss: 0.41823387145996094\n",
      "cnt: 0 - valLoss: 0.4322735667228699 - trainLoss: 0.418231725692749\n",
      "cnt: 0 - valLoss: 0.4322728216648102 - trainLoss: 0.4182295799255371\n",
      "cnt: 0 - valLoss: 0.4322721064090729 - trainLoss: 0.41822749376296997\n",
      "cnt: 0 - valLoss: 0.43227145075798035 - trainLoss: 0.41822537779808044\n",
      "cnt: 0 - valLoss: 0.43227070569992065 - trainLoss: 0.4182232618331909\n",
      "cnt: 0 - valLoss: 0.43227002024650574 - trainLoss: 0.4182211458683014\n",
      "cnt: 0 - valLoss: 0.43226924538612366 - trainLoss: 0.4182189702987671\n",
      "cnt: 0 - valLoss: 0.43226879835128784 - trainLoss: 0.41821691393852234\n",
      "cnt: 0 - valLoss: 0.43226829171180725 - trainLoss: 0.41821494698524475\n",
      "cnt: 0 - valLoss: 0.4322677254676819 - trainLoss: 0.418212890625\n",
      "cnt: 0 - valLoss: 0.4322672188282013 - trainLoss: 0.4182109236717224\n",
      "cnt: 0 - valLoss: 0.4322667419910431 - trainLoss: 0.4182089567184448\n",
      "cnt: 0 - valLoss: 0.4322662353515625 - trainLoss: 0.41820693016052246\n",
      "cnt: 0 - valLoss: 0.4322657287120819 - trainLoss: 0.4182049334049225\n",
      "cnt: 0 - valLoss: 0.43226519227027893 - trainLoss: 0.4182029068470001\n",
      "cnt: 0 - valLoss: 0.43226468563079834 - trainLoss: 0.41820091009140015\n",
      "cnt: 0 - valLoss: 0.43226414918899536 - trainLoss: 0.41819894313812256\n",
      "cnt: 0 - valLoss: 0.4322636127471924 - trainLoss: 0.4181969165802002\n",
      "cnt: 0 - valLoss: 0.4322631359100342 - trainLoss: 0.4181949496269226\n",
      "cnt: 0 - valLoss: 0.4322625994682312 - trainLoss: 0.41819295287132263\n",
      "cnt: 0 - valLoss: 0.4322620630264282 - trainLoss: 0.41819098591804504\n",
      "cnt: 0 - valLoss: 0.43226152658462524 - trainLoss: 0.4181889295578003\n",
      "cnt: 0 - valLoss: 0.43226101994514465 - trainLoss: 0.4181869626045227\n",
      "cnt: 0 - valLoss: 0.4322604835033417 - trainLoss: 0.41818493604660034\n",
      "cnt: 0 - valLoss: 0.4322599768638611 - trainLoss: 0.41818296909332275\n",
      "cnt: 0 - valLoss: 0.4322594106197357 - trainLoss: 0.4181809723377228\n",
      "cnt: 0 - valLoss: 0.43225887417793274 - trainLoss: 0.4181790053844452\n",
      "cnt: 0 - valLoss: 0.43225833773612976 - trainLoss: 0.41817694902420044\n",
      "cnt: 0 - valLoss: 0.4322578012943268 - trainLoss: 0.41817498207092285\n",
      "cnt: 0 - valLoss: 0.4322572350502014 - trainLoss: 0.4181729853153229\n",
      "cnt: 0 - valLoss: 0.4322567284107208 - trainLoss: 0.4181709587574005\n",
      "cnt: 0 - valLoss: 0.43225619196891785 - trainLoss: 0.41816896200180054\n",
      "cnt: 0 - valLoss: 0.43225565552711487 - trainLoss: 0.41816699504852295\n",
      "cnt: 0 - valLoss: 0.4322550892829895 - trainLoss: 0.418164998292923\n",
      "cnt: 0 - valLoss: 0.43225452303886414 - trainLoss: 0.4181629717350006\n",
      "cnt: 0 - valLoss: 0.4322540760040283 - trainLoss: 0.418161004781723\n",
      "cnt: 0 - valLoss: 0.4322534501552582 - trainLoss: 0.41815903782844543\n",
      "cnt: 0 - valLoss: 0.4322529435157776 - trainLoss: 0.41815704107284546\n",
      "cnt: 0 - valLoss: 0.43225234746932983 - trainLoss: 0.4181550145149231\n",
      "cnt: 0 - valLoss: 0.43225184082984924 - trainLoss: 0.4181530177593231\n",
      "cnt: 0 - valLoss: 0.4322512447834015 - trainLoss: 0.4181510806083679\n",
      "cnt: 0 - valLoss: 0.4322507083415985 - trainLoss: 0.41814902424812317\n",
      "cnt: 0 - valLoss: 0.43225011229515076 - trainLoss: 0.4181470572948456\n",
      "cnt: 0 - valLoss: 0.43224960565567017 - trainLoss: 0.418145090341568\n",
      "cnt: 0 - valLoss: 0.4322490096092224 - trainLoss: 0.418143093585968\n",
      "cnt: 0 - valLoss: 0.43224844336509705 - trainLoss: 0.41814106702804565\n",
      "cnt: 0 - valLoss: 0.4322478771209717 - trainLoss: 0.41813910007476807\n",
      "cnt: 0 - valLoss: 0.4322473704814911 - trainLoss: 0.4181371033191681\n",
      "cnt: 0 - valLoss: 0.43224677443504333 - trainLoss: 0.4181350767612457\n",
      "cnt: 0 - valLoss: 0.4322461783885956 - trainLoss: 0.41813310980796814\n",
      "cnt: 0 - valLoss: 0.4322455823421478 - trainLoss: 0.4181310832500458\n",
      "cnt: 0 - valLoss: 0.43224507570266724 - trainLoss: 0.4181290864944458\n",
      "cnt: 0 - valLoss: 0.4322444498538971 - trainLoss: 0.4181271195411682\n",
      "cnt: 0 - valLoss: 0.4322439134120941 - trainLoss: 0.4181251525878906\n",
      "cnt: 0 - valLoss: 0.43224331736564636 - trainLoss: 0.4181230962276459\n",
      "cnt: 0 - valLoss: 0.432242751121521 - trainLoss: 0.4181211292743683\n",
      "cnt: 0 - valLoss: 0.43224215507507324 - trainLoss: 0.4181191623210907\n",
      "cnt: 0 - valLoss: 0.4322415590286255 - trainLoss: 0.41811713576316833\n",
      "cnt: 0 - valLoss: 0.4322410225868225 - trainLoss: 0.41811516880989075\n",
      "cnt: 0 - valLoss: 0.43224042654037476 - trainLoss: 0.41811317205429077\n",
      "cnt: 0 - valLoss: 0.432239830493927 - trainLoss: 0.4181112051010132\n",
      "cnt: 0 - valLoss: 0.43223923444747925 - trainLoss: 0.4181092381477356\n",
      "cnt: 0 - valLoss: 0.4322386384010315 - trainLoss: 0.4181072413921356\n",
      "cnt: 0 - valLoss: 0.43223804235458374 - trainLoss: 0.41810521483421326\n",
      "cnt: 0 - valLoss: 0.4322374761104584 - trainLoss: 0.4181032180786133\n",
      "cnt: 0 - valLoss: 0.432236909866333 - trainLoss: 0.4181012511253357\n",
      "cnt: 0 - valLoss: 0.43223631381988525 - trainLoss: 0.4180992841720581\n",
      "cnt: 0 - valLoss: 0.4322356879711151 - trainLoss: 0.41809728741645813\n",
      "cnt: 0 - valLoss: 0.43223512172698975 - trainLoss: 0.41809526085853577\n",
      "cnt: 0 - valLoss: 0.432234525680542 - trainLoss: 0.4180932939052582\n",
      "cnt: 0 - valLoss: 0.43223392963409424 - trainLoss: 0.4180913269519806\n",
      "cnt: 0 - valLoss: 0.4322333037853241 - trainLoss: 0.41808927059173584\n",
      "cnt: 0 - valLoss: 0.43223270773887634 - trainLoss: 0.41808730363845825\n",
      "cnt: 0 - valLoss: 0.4322321116924286 - trainLoss: 0.41808533668518066\n",
      "cnt: 0 - valLoss: 0.43223151564598083 - trainLoss: 0.4180833697319031\n",
      "cnt: 0 - valLoss: 0.43223094940185547 - trainLoss: 0.4180813133716583\n",
      "cnt: 0 - valLoss: 0.4322303235530853 - trainLoss: 0.41807934641838074\n",
      "cnt: 0 - valLoss: 0.4322297275066376 - trainLoss: 0.41807737946510315\n",
      "cnt: 0 - valLoss: 0.4322291314601898 - trainLoss: 0.41807541251182556\n",
      "cnt: 0 - valLoss: 0.4322285056114197 - trainLoss: 0.4180733859539032\n",
      "cnt: 0 - valLoss: 0.4322279095649719 - trainLoss: 0.4180714190006256\n",
      "cnt: 0 - valLoss: 0.43222731351852417 - trainLoss: 0.41806942224502563\n",
      "cnt: 0 - valLoss: 0.4322267174720764 - trainLoss: 0.41806745529174805\n",
      "cnt: 0 - valLoss: 0.4322260618209839 - trainLoss: 0.41806548833847046\n",
      "cnt: 0 - valLoss: 0.43222546577453613 - trainLoss: 0.4180634617805481\n",
      "cnt: 0 - valLoss: 0.43222492933273315 - trainLoss: 0.4180614650249481\n",
      "cnt: 0 - valLoss: 0.4322242736816406 - trainLoss: 0.41805949807167053\n",
      "cnt: 0 - valLoss: 0.43222367763519287 - trainLoss: 0.41805753111839294\n",
      "cnt: 0 - valLoss: 0.43222299218177795 - trainLoss: 0.41805556416511536\n",
      "cnt: 0 - valLoss: 0.4322224259376526 - trainLoss: 0.4180535078048706\n",
      "cnt: 0 - valLoss: 0.43222180008888245 - trainLoss: 0.418051540851593\n",
      "cnt: 0 - valLoss: 0.4322211742401123 - trainLoss: 0.41804957389831543\n",
      "cnt: 0 - valLoss: 0.43222054839134216 - trainLoss: 0.41804757714271545\n",
      "cnt: 0 - valLoss: 0.4322199523448944 - trainLoss: 0.41804561018943787\n",
      "cnt: 0 - valLoss: 0.43221935629844666 - trainLoss: 0.4180436432361603\n",
      "cnt: 0 - valLoss: 0.43221867084503174 - trainLoss: 0.4180415868759155\n",
      "cnt: 0 - valLoss: 0.4322180449962616 - trainLoss: 0.41803961992263794\n",
      "cnt: 0 - valLoss: 0.43221744894981384 - trainLoss: 0.41803765296936035\n",
      "cnt: 0 - valLoss: 0.4322168231010437 - trainLoss: 0.41803568601608276\n",
      "cnt: 0 - valLoss: 0.43221616744995117 - trainLoss: 0.4180336892604828\n",
      "cnt: 0 - valLoss: 0.4322155714035034 - trainLoss: 0.4180317223072052\n",
      "cnt: 0 - valLoss: 0.4322149157524109 - trainLoss: 0.4180297553539276\n",
      "cnt: 0 - valLoss: 0.43221431970596313 - trainLoss: 0.41802772879600525\n",
      "cnt: 0 - valLoss: 0.432213693857193 - trainLoss: 0.41802576184272766\n",
      "cnt: 0 - valLoss: 0.43221309781074524 - trainLoss: 0.4180237948894501\n",
      "cnt: 0 - valLoss: 0.4322124421596527 - trainLoss: 0.4180218279361725\n",
      "cnt: 0 - valLoss: 0.4322117865085602 - trainLoss: 0.4180198013782501\n",
      "cnt: 0 - valLoss: 0.43221116065979004 - trainLoss: 0.41801783442497253\n",
      "cnt: 0 - valLoss: 0.4322105050086975 - trainLoss: 0.41801583766937256\n",
      "cnt: 0 - valLoss: 0.43220987915992737 - trainLoss: 0.41801387071609497\n",
      "cnt: 0 - valLoss: 0.43220922350883484 - trainLoss: 0.4180119037628174\n",
      "cnt: 0 - valLoss: 0.4322086274623871 - trainLoss: 0.418009877204895\n",
      "cnt: 0 - valLoss: 0.43220794200897217 - trainLoss: 0.41800791025161743\n",
      "cnt: 0 - valLoss: 0.4322073459625244 - trainLoss: 0.41800591349601746\n",
      "cnt: 0 - valLoss: 0.4322066605091095 - trainLoss: 0.41800394654273987\n",
      "cnt: 0 - valLoss: 0.43220600485801697 - trainLoss: 0.4180019497871399\n",
      "cnt: 0 - valLoss: 0.4322053790092468 - trainLoss: 0.4179999828338623\n",
      "cnt: 0 - valLoss: 0.4322047829627991 - trainLoss: 0.4179980158805847\n",
      "cnt: 0 - valLoss: 0.43220409750938416 - trainLoss: 0.41799601912498474\n",
      "cnt: 0 - valLoss: 0.43220341205596924 - trainLoss: 0.41799405217170715\n",
      "cnt: 0 - valLoss: 0.4322027862071991 - trainLoss: 0.4179920256137848\n",
      "cnt: 0 - valLoss: 0.4322021007537842 - trainLoss: 0.4179900586605072\n",
      "cnt: 0 - valLoss: 0.43220141530036926 - trainLoss: 0.4179880917072296\n",
      "cnt: 0 - valLoss: 0.4322008192539215 - trainLoss: 0.417986124753952\n",
      "cnt: 0 - valLoss: 0.43220019340515137 - trainLoss: 0.41798415780067444\n",
      "cnt: 0 - valLoss: 0.43219953775405884 - trainLoss: 0.41798219084739685\n",
      "cnt: 0 - valLoss: 0.4321988523006439 - trainLoss: 0.4179801344871521\n",
      "cnt: 0 - valLoss: 0.432198166847229 - trainLoss: 0.4179781675338745\n",
      "cnt: 0 - valLoss: 0.43219754099845886 - trainLoss: 0.4179762303829193\n",
      "cnt: 0 - valLoss: 0.43219688534736633 - trainLoss: 0.41797423362731934\n",
      "cnt: 0 - valLoss: 0.4321961998939514 - trainLoss: 0.41797226667404175\n",
      "cnt: 0 - valLoss: 0.4321955442428589 - trainLoss: 0.41797029972076416\n",
      "cnt: 0 - valLoss: 0.43219491839408875 - trainLoss: 0.4179683327674866\n",
      "cnt: 0 - valLoss: 0.43219423294067383 - trainLoss: 0.417966365814209\n",
      "cnt: 0 - valLoss: 0.4321935772895813 - trainLoss: 0.41796430945396423\n",
      "cnt: 0 - valLoss: 0.4321928918361664 - trainLoss: 0.41796237230300903\n",
      "cnt: 0 - valLoss: 0.43219220638275146 - trainLoss: 0.41796037554740906\n",
      "cnt: 0 - valLoss: 0.4321915805339813 - trainLoss: 0.4179583787918091\n",
      "cnt: 0 - valLoss: 0.4321909248828888 - trainLoss: 0.4179564118385315\n",
      "cnt: 0 - valLoss: 0.4321902394294739 - trainLoss: 0.4179544448852539\n",
      "cnt: 0 - valLoss: 0.4321895241737366 - trainLoss: 0.41795244812965393\n",
      "cnt: 0 - valLoss: 0.43218889832496643 - trainLoss: 0.41795048117637634\n",
      "cnt: 0 - valLoss: 0.4321882128715515 - trainLoss: 0.41794851422309875\n",
      "cnt: 0 - valLoss: 0.4321875274181366 - trainLoss: 0.41794654726982117\n",
      "cnt: 0 - valLoss: 0.43218690156936646 - trainLoss: 0.4179445207118988\n",
      "cnt: 0 - valLoss: 0.43218615651130676 - trainLoss: 0.4179425537586212\n",
      "cnt: 0 - valLoss: 0.4321855306625366 - trainLoss: 0.41794058680534363\n",
      "cnt: 0 - valLoss: 0.4321848750114441 - trainLoss: 0.41793861985206604\n",
      "cnt: 0 - valLoss: 0.4321841895580292 - trainLoss: 0.41793665289878845\n",
      "cnt: 0 - valLoss: 0.43218350410461426 - trainLoss: 0.41793468594551086\n",
      "cnt: 0 - valLoss: 0.43218281865119934 - trainLoss: 0.4179327189922333\n",
      "cnt: 0 - valLoss: 0.4321821331977844 - trainLoss: 0.4179306924343109\n",
      "cnt: 0 - valLoss: 0.4321814477443695 - trainLoss: 0.4179287254810333\n",
      "cnt: 0 - valLoss: 0.4321807622909546 - trainLoss: 0.41792672872543335\n",
      "cnt: 0 - valLoss: 0.4321800768375397 - trainLoss: 0.41792479157447815\n",
      "cnt: 0 - valLoss: 0.43217939138412476 - trainLoss: 0.4179227948188782\n",
      "cnt: 0 - valLoss: 0.43217870593070984 - trainLoss: 0.4179207980632782\n",
      "cnt: 0 - valLoss: 0.4321780800819397 - trainLoss: 0.4179188311100006\n",
      "cnt: 0 - valLoss: 0.4321773946285248 - trainLoss: 0.417916864156723\n",
      "cnt: 0 - valLoss: 0.43217673897743225 - trainLoss: 0.41791489720344543\n",
      "cnt: 0 - valLoss: 0.43217602372169495 - trainLoss: 0.41791293025016785\n",
      "cnt: 0 - valLoss: 0.4321753680706024 - trainLoss: 0.41791096329689026\n",
      "cnt: 0 - valLoss: 0.4321746826171875 - trainLoss: 0.4179089665412903\n",
      "cnt: 0 - valLoss: 0.4321739971637726 - trainLoss: 0.4179070293903351\n",
      "cnt: 0 - valLoss: 0.4321732819080353 - trainLoss: 0.4179050326347351\n",
      "cnt: 0 - valLoss: 0.43217259645462036 - trainLoss: 0.41790300607681274\n",
      "cnt: 0 - valLoss: 0.43217194080352783 - trainLoss: 0.41790103912353516\n",
      "cnt: 0 - valLoss: 0.4321712553501129 - trainLoss: 0.41789907217025757\n",
      "cnt: 0 - valLoss: 0.432170569896698 - trainLoss: 0.41789710521698\n",
      "cnt: 0 - valLoss: 0.4321698844432831 - trainLoss: 0.4178951382637024\n",
      "cnt: 0 - valLoss: 0.43216919898986816 - trainLoss: 0.4178931713104248\n",
      "cnt: 0 - valLoss: 0.43216845393180847 - trainLoss: 0.4178912043571472\n",
      "cnt: 0 - valLoss: 0.43216782808303833 - trainLoss: 0.41788923740386963\n",
      "cnt: 0 - valLoss: 0.4321671426296234 - trainLoss: 0.41788721084594727\n",
      "cnt: 0 - valLoss: 0.43216636776924133 - trainLoss: 0.4178852438926697\n",
      "cnt: 0 - valLoss: 0.4321656823158264 - trainLoss: 0.4178832769393921\n",
      "cnt: 0 - valLoss: 0.43216508626937866 - trainLoss: 0.4178813099861145\n",
      "cnt: 0 - valLoss: 0.43216440081596375 - trainLoss: 0.41787928342819214\n",
      "cnt: 0 - valLoss: 0.43216371536254883 - trainLoss: 0.4178773760795593\n",
      "cnt: 0 - valLoss: 0.4321630299091339 - trainLoss: 0.41787540912628174\n",
      "cnt: 0 - valLoss: 0.43216243386268616 - trainLoss: 0.41787344217300415\n",
      "cnt: 0 - valLoss: 0.43216168880462646 - trainLoss: 0.41787147521972656\n",
      "cnt: 0 - valLoss: 0.4321610629558563 - trainLoss: 0.4178694784641266\n",
      "cnt: 0 - valLoss: 0.4321603775024414 - trainLoss: 0.4178674817085266\n",
      "cnt: 0 - valLoss: 0.4321597218513489 - trainLoss: 0.417865514755249\n",
      "cnt: 0 - valLoss: 0.43215903639793396 - trainLoss: 0.41786354780197144\n",
      "cnt: 0 - valLoss: 0.43215835094451904 - trainLoss: 0.41786158084869385\n",
      "cnt: 0 - valLoss: 0.4321576952934265 - trainLoss: 0.41785961389541626\n",
      "cnt: 0 - valLoss: 0.4321570098400116 - trainLoss: 0.41785764694213867\n",
      "cnt: 0 - valLoss: 0.43215638399124146 - trainLoss: 0.4178556799888611\n",
      "cnt: 0 - valLoss: 0.43215563893318176 - trainLoss: 0.4178537130355835\n",
      "cnt: 0 - valLoss: 0.4321550130844116 - trainLoss: 0.4178517162799835\n",
      "cnt: 0 - valLoss: 0.43215426802635193 - trainLoss: 0.41784974932670593\n",
      "cnt: 0 - valLoss: 0.4321536421775818 - trainLoss: 0.41784778237342834\n",
      "cnt: 0 - valLoss: 0.43215295672416687 - trainLoss: 0.41784581542015076\n",
      "cnt: 0 - valLoss: 0.43215230107307434 - trainLoss: 0.41784384846687317\n",
      "cnt: 0 - valLoss: 0.43215158581733704 - trainLoss: 0.4178418815135956\n",
      "cnt: 0 - valLoss: 0.4321509301662445 - trainLoss: 0.417839914560318\n",
      "cnt: 0 - valLoss: 0.43215030431747437 - trainLoss: 0.4178379476070404\n",
      "cnt: 0 - valLoss: 0.4321495592594147 - trainLoss: 0.4178359806537628\n",
      "cnt: 0 - valLoss: 0.43214887380599976 - trainLoss: 0.41783401370048523\n",
      "cnt: 0 - valLoss: 0.43214818835258484 - trainLoss: 0.41783204674720764\n",
      "cnt: 0 - valLoss: 0.4321475625038147 - trainLoss: 0.41783007979393005\n",
      "cnt: 0 - valLoss: 0.432146817445755 - trainLoss: 0.41782811284065247\n",
      "cnt: 0 - valLoss: 0.4321461319923401 - trainLoss: 0.4178261458873749\n",
      "cnt: 0 - valLoss: 0.4321454167366028 - trainLoss: 0.4178241789340973\n",
      "cnt: 0 - valLoss: 0.43214476108551025 - trainLoss: 0.4178221523761749\n",
      "cnt: 0 - valLoss: 0.4321441352367401 - trainLoss: 0.4178202152252197\n",
      "cnt: 0 - valLoss: 0.4321433901786804 - trainLoss: 0.41781824827194214\n",
      "cnt: 0 - valLoss: 0.4321427643299103 - trainLoss: 0.41781628131866455\n",
      "cnt: 0 - valLoss: 0.4321420192718506 - trainLoss: 0.41781431436538696\n",
      "cnt: 0 - valLoss: 0.43214139342308044 - trainLoss: 0.4178123474121094\n",
      "cnt: 0 - valLoss: 0.43214061856269836 - trainLoss: 0.4178103804588318\n",
      "cnt: 0 - valLoss: 0.43213996291160583 - trainLoss: 0.4178084135055542\n",
      "cnt: 0 - valLoss: 0.4321392774581909 - trainLoss: 0.4178064167499542\n",
      "cnt: 0 - valLoss: 0.432138592004776 - trainLoss: 0.417804479598999\n",
      "cnt: 0 - valLoss: 0.4321379065513611 - trainLoss: 0.41780251264572144\n",
      "cnt: 0 - valLoss: 0.432137131690979 - trainLoss: 0.41780054569244385\n",
      "cnt: 0 - valLoss: 0.4321364462375641 - trainLoss: 0.41779857873916626\n",
      "cnt: 0 - valLoss: 0.43213576078414917 - trainLoss: 0.41779661178588867\n",
      "cnt: 0 - valLoss: 0.43213507533073425 - trainLoss: 0.4177946448326111\n",
      "cnt: 0 - valLoss: 0.43213436007499695 - trainLoss: 0.4177926778793335\n",
      "cnt: 0 - valLoss: 0.43213367462158203 - trainLoss: 0.4177906811237335\n",
      "cnt: 0 - valLoss: 0.4321330189704895 - trainLoss: 0.41778871417045593\n",
      "cnt: 0 - valLoss: 0.4321323037147522 - trainLoss: 0.41778674721717834\n",
      "cnt: 0 - valLoss: 0.4321315586566925 - trainLoss: 0.41778478026390076\n",
      "cnt: 0 - valLoss: 0.4321308732032776 - trainLoss: 0.41778281331062317\n",
      "cnt: 0 - valLoss: 0.43213018774986267 - trainLoss: 0.4177808463573456\n",
      "cnt: 0 - valLoss: 0.43212950229644775 - trainLoss: 0.417778879404068\n",
      "cnt: 0 - valLoss: 0.43212878704071045 - trainLoss: 0.4177769124507904\n",
      "cnt: 0 - valLoss: 0.43212810158729553 - trainLoss: 0.4177749454975128\n",
      "cnt: 0 - valLoss: 0.43212735652923584 - trainLoss: 0.41777297854423523\n",
      "cnt: 0 - valLoss: 0.4321266710758209 - trainLoss: 0.41777101159095764\n",
      "cnt: 0 - valLoss: 0.4321259558200836 - trainLoss: 0.41776907444000244\n",
      "cnt: 0 - valLoss: 0.4321252107620239 - trainLoss: 0.41776707768440247\n",
      "cnt: 0 - valLoss: 0.432124525308609 - trainLoss: 0.4177651107311249\n",
      "cnt: 0 - valLoss: 0.4321238398551941 - trainLoss: 0.4177631437778473\n",
      "cnt: 0 - valLoss: 0.4321231245994568 - trainLoss: 0.4177612066268921\n",
      "cnt: 0 - valLoss: 0.4321223795413971 - trainLoss: 0.4177592396736145\n",
      "cnt: 0 - valLoss: 0.4321216940879822 - trainLoss: 0.4177572727203369\n",
      "cnt: 0 - valLoss: 0.43212100863456726 - trainLoss: 0.4177553355693817\n",
      "cnt: 0 - valLoss: 0.43212029337882996 - trainLoss: 0.41775333881378174\n",
      "cnt: 0 - valLoss: 0.43211954832077026 - trainLoss: 0.41775140166282654\n",
      "cnt: 0 - valLoss: 0.43211886286735535 - trainLoss: 0.41774943470954895\n",
      "cnt: 0 - valLoss: 0.43211814761161804 - trainLoss: 0.41774746775627136\n",
      "cnt: 0 - valLoss: 0.43211743235588074 - trainLoss: 0.4177455008029938\n",
      "cnt: 0 - valLoss: 0.4321167469024658 - trainLoss: 0.4177435338497162\n",
      "cnt: 0 - valLoss: 0.4321160614490509 - trainLoss: 0.4177415668964386\n",
      "cnt: 0 - valLoss: 0.4321152865886688 - trainLoss: 0.417739599943161\n",
      "cnt: 0 - valLoss: 0.4321146011352539 - trainLoss: 0.4177376329898834\n",
      "cnt: 0 - valLoss: 0.4321138262748718 - trainLoss: 0.41773566603660583\n",
      "cnt: 0 - valLoss: 0.4321131706237793 - trainLoss: 0.41773366928100586\n",
      "cnt: 0 - valLoss: 0.4321124851703644 - trainLoss: 0.41773170232772827\n",
      "cnt: 0 - valLoss: 0.4321117699146271 - trainLoss: 0.4177297353744507\n",
      "cnt: 0 - valLoss: 0.43211108446121216 - trainLoss: 0.4177277684211731\n",
      "cnt: 0 - valLoss: 0.4321103096008301 - trainLoss: 0.4177258610725403\n",
      "cnt: 0 - valLoss: 0.43210962414741516 - trainLoss: 0.4177238941192627\n",
      "cnt: 0 - valLoss: 0.43210893869400024 - trainLoss: 0.4177219271659851\n",
      "cnt: 0 - valLoss: 0.43210816383361816 - trainLoss: 0.41771993041038513\n",
      "cnt: 0 - valLoss: 0.43210747838020325 - trainLoss: 0.41771796345710754\n",
      "cnt: 0 - valLoss: 0.43210670351982117 - trainLoss: 0.41771605610847473\n",
      "cnt: 0 - valLoss: 0.43210601806640625 - trainLoss: 0.41771408915519714\n",
      "cnt: 0 - valLoss: 0.43210533261299133 - trainLoss: 0.4177120625972748\n",
      "cnt: 0 - valLoss: 0.43210455775260925 - trainLoss: 0.41771015524864197\n",
      "cnt: 0 - valLoss: 0.43210387229919434 - trainLoss: 0.4177081882953644\n",
      "cnt: 0 - valLoss: 0.43210309743881226 - trainLoss: 0.4177061915397644\n",
      "cnt: 0 - valLoss: 0.43210241198539734 - trainLoss: 0.4177042245864868\n",
      "cnt: 0 - valLoss: 0.43210163712501526 - trainLoss: 0.41770225763320923\n",
      "cnt: 0 - valLoss: 0.4321008622646332 - trainLoss: 0.41770029067993164\n",
      "cnt: 0 - valLoss: 0.43210017681121826 - trainLoss: 0.41769832372665405\n",
      "cnt: 0 - valLoss: 0.43209949135780334 - trainLoss: 0.41769635677337646\n",
      "cnt: 0 - valLoss: 0.43209874629974365 - trainLoss: 0.41769441962242126\n",
      "cnt: 0 - valLoss: 0.43209803104400635 - trainLoss: 0.4176924228668213\n",
      "cnt: 0 - valLoss: 0.43209728598594666 - trainLoss: 0.4176904857158661\n",
      "cnt: 0 - valLoss: 0.43209654092788696 - trainLoss: 0.4176884889602661\n",
      "cnt: 0 - valLoss: 0.43209585547447205 - trainLoss: 0.4176865518093109\n",
      "cnt: 0 - valLoss: 0.43209508061408997 - trainLoss: 0.4176846444606781\n",
      "cnt: 0 - valLoss: 0.4320943057537079 - trainLoss: 0.4176826775074005\n",
      "cnt: 0 - valLoss: 0.4320935010910034 - trainLoss: 0.41768068075180054\n",
      "cnt: 0 - valLoss: 0.43209272623062134 - trainLoss: 0.41767871379852295\n",
      "cnt: 0 - valLoss: 0.43209195137023926 - trainLoss: 0.41767674684524536\n",
      "cnt: 0 - valLoss: 0.4320911765098572 - trainLoss: 0.4176747798919678\n",
      "cnt: 0 - valLoss: 0.4320903420448303 - trainLoss: 0.4176728129386902\n",
      "cnt: 0 - valLoss: 0.43208956718444824 - trainLoss: 0.4176709055900574\n",
      "cnt: 0 - valLoss: 0.43208882212638855 - trainLoss: 0.4176689386367798\n",
      "cnt: 0 - valLoss: 0.4320879876613617 - trainLoss: 0.4176669418811798\n",
      "cnt: 0 - valLoss: 0.432087242603302 - trainLoss: 0.417665034532547\n",
      "cnt: 0 - valLoss: 0.4320864677429199 - trainLoss: 0.4176630675792694\n",
      "cnt: 0 - valLoss: 0.43208569288253784 - trainLoss: 0.4176611006259918\n",
      "cnt: 0 - valLoss: 0.432084858417511 - trainLoss: 0.41765913367271423\n",
      "cnt: 0 - valLoss: 0.4320840835571289 - trainLoss: 0.41765719652175903\n",
      "cnt: 0 - valLoss: 0.4320833086967468 - trainLoss: 0.41765522956848145\n",
      "cnt: 0 - valLoss: 0.43208247423171997 - trainLoss: 0.41765326261520386\n",
      "cnt: 0 - valLoss: 0.43208175897598267 - trainLoss: 0.41765129566192627\n",
      "cnt: 0 - valLoss: 0.4320808947086334 - trainLoss: 0.41764935851097107\n",
      "cnt: 0 - valLoss: 0.43208011984825134 - trainLoss: 0.4176473915576935\n",
      "cnt: 0 - valLoss: 0.43207934498786926 - trainLoss: 0.4176454246044159\n",
      "cnt: 0 - valLoss: 0.4320785403251648 - trainLoss: 0.4176434576511383\n",
      "cnt: 0 - valLoss: 0.4320778250694275 - trainLoss: 0.4176415205001831\n",
      "cnt: 0 - valLoss: 0.43207696080207825 - trainLoss: 0.41763952374458313\n",
      "cnt: 0 - valLoss: 0.43207621574401855 - trainLoss: 0.41763758659362793\n",
      "cnt: 0 - valLoss: 0.4320754110813141 - trainLoss: 0.41763561964035034\n",
      "cnt: 0 - valLoss: 0.4320746064186096 - trainLoss: 0.41763371229171753\n",
      "cnt: 0 - valLoss: 0.43207383155822754 - trainLoss: 0.41763171553611755\n",
      "cnt: 0 - valLoss: 0.43207305669784546 - trainLoss: 0.41762974858283997\n",
      "cnt: 0 - valLoss: 0.43207234144210815 - trainLoss: 0.4176277816295624\n",
      "cnt: 0 - valLoss: 0.4320714771747589 - trainLoss: 0.41762587428092957\n",
      "cnt: 0 - valLoss: 0.43207070231437683 - trainLoss: 0.41762393712997437\n",
      "cnt: 0 - valLoss: 0.43206989765167236 - trainLoss: 0.4176219701766968\n",
      "cnt: 0 - valLoss: 0.4320691227912903 - trainLoss: 0.4176200032234192\n",
      "cnt: 0 - valLoss: 0.4320683777332306 - trainLoss: 0.4176180362701416\n",
      "cnt: 0 - valLoss: 0.43206751346588135 - trainLoss: 0.417616069316864\n",
      "cnt: 0 - valLoss: 0.4320667088031769 - trainLoss: 0.4176141321659088\n",
      "cnt: 0 - valLoss: 0.4320659935474396 - trainLoss: 0.4176121652126312\n",
      "cnt: 0 - valLoss: 0.4320650100708008 - trainLoss: 0.4176102578639984\n",
      "cnt: 0 - valLoss: 0.4320642352104187 - trainLoss: 0.41760826110839844\n",
      "cnt: 0 - valLoss: 0.4320632517337799 - trainLoss: 0.41760629415512085\n",
      "cnt: 0 - valLoss: 0.4320622980594635 - trainLoss: 0.41760432720184326\n",
      "cnt: 0 - valLoss: 0.4320613443851471 - trainLoss: 0.41760241985321045\n",
      "cnt: 0 - valLoss: 0.4320603609085083 - trainLoss: 0.41760045289993286\n",
      "cnt: 0 - valLoss: 0.4320595860481262 - trainLoss: 0.41759851574897766\n",
      "cnt: 0 - valLoss: 0.4320586621761322 - trainLoss: 0.4175965487957001\n",
      "cnt: 0 - valLoss: 0.4320576786994934 - trainLoss: 0.4175945818424225\n",
      "cnt: 0 - valLoss: 0.43205681443214417 - trainLoss: 0.4175926148891449\n",
      "cnt: 0 - valLoss: 0.4320557713508606 - trainLoss: 0.4175906777381897\n",
      "cnt: 0 - valLoss: 0.4320547878742218 - trainLoss: 0.4175887703895569\n",
      "cnt: 0 - valLoss: 0.4320540726184845 - trainLoss: 0.4175867736339569\n",
      "cnt: 0 - valLoss: 0.4320531189441681 - trainLoss: 0.4175848066806793\n",
      "cnt: 0 - valLoss: 0.4320521950721741 - trainLoss: 0.41758283972740173\n",
      "cnt: 0 - valLoss: 0.43205124139785767 - trainLoss: 0.4175809323787689\n",
      "cnt: 0 - valLoss: 0.43205028772354126 - trainLoss: 0.4175789952278137\n",
      "cnt: 0 - valLoss: 0.43204936385154724 - trainLoss: 0.41757702827453613\n",
      "cnt: 0 - valLoss: 0.43204832077026367 - trainLoss: 0.41757506132125854\n",
      "cnt: 0 - valLoss: 0.4320475459098816 - trainLoss: 0.41757312417030334\n",
      "cnt: 0 - valLoss: 0.43204665184020996 - trainLoss: 0.41757115721702576\n",
      "cnt: 0 - valLoss: 0.43204566836357117 - trainLoss: 0.41756919026374817\n",
      "cnt: 0 - valLoss: 0.43204477429389954 - trainLoss: 0.41756728291511536\n",
      "cnt: 0 - valLoss: 0.43204382061958313 - trainLoss: 0.41756531596183777\n",
      "cnt: 0 - valLoss: 0.4320428967475891 - trainLoss: 0.4175633192062378\n",
      "cnt: 0 - valLoss: 0.43204206228256226 - trainLoss: 0.417561411857605\n",
      "cnt: 0 - valLoss: 0.43204113841056824 - trainLoss: 0.4175594747066498\n",
      "cnt: 0 - valLoss: 0.43204018473625183 - trainLoss: 0.4175575077533722\n",
      "cnt: 0 - valLoss: 0.4320392310619354 - trainLoss: 0.4175555408000946\n",
      "cnt: 0 - valLoss: 0.4320383071899414 - trainLoss: 0.4175536036491394\n",
      "cnt: 0 - valLoss: 0.4320373833179474 - trainLoss: 0.4175516366958618\n",
      "cnt: 0 - valLoss: 0.4320366680622101 - trainLoss: 0.41754966974258423\n",
      "cnt: 0 - valLoss: 0.4320356249809265 - trainLoss: 0.4175477623939514\n",
      "cnt: 0 - valLoss: 0.4320347011089325 - trainLoss: 0.41754579544067383\n",
      "cnt: 0 - valLoss: 0.4320337474346161 - trainLoss: 0.41754382848739624\n",
      "cnt: 0 - valLoss: 0.43203285336494446 - trainLoss: 0.41754189133644104\n",
      "cnt: 0 - valLoss: 0.43203186988830566 - trainLoss: 0.41753995418548584\n",
      "cnt: 0 - valLoss: 0.4320310652256012 - trainLoss: 0.41753795742988586\n",
      "cnt: 0 - valLoss: 0.4320300817489624 - trainLoss: 0.4175359904766083\n",
      "cnt: 0 - valLoss: 0.43202921748161316 - trainLoss: 0.4175339639186859\n",
      "cnt: 0 - valLoss: 0.43202823400497437 - trainLoss: 0.4175319969654083\n",
      "cnt: 0 - valLoss: 0.4320273995399475 - trainLoss: 0.41753003001213074\n",
      "cnt: 0 - valLoss: 0.4320264160633087 - trainLoss: 0.41752806305885315\n",
      "cnt: 0 - valLoss: 0.43202561140060425 - trainLoss: 0.41752609610557556\n",
      "cnt: 0 - valLoss: 0.4320245683193207 - trainLoss: 0.417524129152298\n",
      "cnt: 0 - valLoss: 0.432023823261261 - trainLoss: 0.4175221025943756\n",
      "cnt: 0 - valLoss: 0.4320227801799774 - trainLoss: 0.417520135641098\n",
      "cnt: 0 - valLoss: 0.43202200531959534 - trainLoss: 0.4175182282924652\n",
      "cnt: 0 - valLoss: 0.43202102184295654 - trainLoss: 0.41751620173454285\n",
      "cnt: 0 - valLoss: 0.43202000856399536 - trainLoss: 0.41751423478126526\n",
      "cnt: 0 - valLoss: 0.4320191740989685 - trainLoss: 0.41751226782798767\n",
      "cnt: 0 - valLoss: 0.43201836943626404 - trainLoss: 0.4175103008747101\n",
      "cnt: 0 - valLoss: 0.43201738595962524 - trainLoss: 0.4175083339214325\n",
      "cnt: 0 - valLoss: 0.43201643228530884 - trainLoss: 0.41750630736351013\n",
      "cnt: 0 - valLoss: 0.4320155680179596 - trainLoss: 0.41750434041023254\n",
      "cnt: 0 - valLoss: 0.4320145845413208 - trainLoss: 0.41750237345695496\n",
      "cnt: 0 - valLoss: 0.43201377987861633 - trainLoss: 0.41750040650367737\n",
      "cnt: 0 - valLoss: 0.43201279640197754 - trainLoss: 0.4174984395503998\n",
      "cnt: 0 - valLoss: 0.43201202154159546 - trainLoss: 0.4174964725971222\n",
      "cnt: 0 - valLoss: 0.4320109486579895 - trainLoss: 0.4174945056438446\n",
      "cnt: 0 - valLoss: 0.43201014399528503 - trainLoss: 0.41749250888824463\n",
      "cnt: 0 - valLoss: 0.43200916051864624 - trainLoss: 0.41749054193496704\n",
      "cnt: 0 - valLoss: 0.43200838565826416 - trainLoss: 0.41748854517936707\n",
      "cnt: 0 - valLoss: 0.43200740218162537 - trainLoss: 0.4174865782260895\n",
      "cnt: 0 - valLoss: 0.4320065677165985 - trainLoss: 0.4174845814704895\n",
      "cnt: 0 - valLoss: 0.43200555443763733 - trainLoss: 0.4174826145172119\n",
      "cnt: 0 - valLoss: 0.43200477957725525 - trainLoss: 0.4174806475639343\n",
      "cnt: 0 - valLoss: 0.43200379610061646 - trainLoss: 0.41747868061065674\n",
      "cnt: 0 - valLoss: 0.432002991437912 - trainLoss: 0.41747671365737915\n",
      "cnt: 0 - valLoss: 0.4320019781589508 - trainLoss: 0.41747474670410156\n",
      "cnt: 0 - valLoss: 0.4320009648799896 - trainLoss: 0.4174727499485016\n",
      "cnt: 0 - valLoss: 0.43200016021728516 - trainLoss: 0.417470782995224\n",
      "cnt: 0 - valLoss: 0.4319993853569031 - trainLoss: 0.417468786239624\n",
      "cnt: 0 - valLoss: 0.4319984018802643 - trainLoss: 0.41746681928634644\n",
      "cnt: 0 - valLoss: 0.4319974184036255 - trainLoss: 0.41746485233306885\n",
      "cnt: 0 - valLoss: 0.4319966435432434 - trainLoss: 0.41746288537979126\n",
      "cnt: 0 - valLoss: 0.43199560046195984 - trainLoss: 0.4174608588218689\n",
      "cnt: 0 - valLoss: 0.43199479579925537 - trainLoss: 0.4174588918685913\n",
      "cnt: 0 - valLoss: 0.4319938123226166 - trainLoss: 0.4174569249153137\n",
      "cnt: 0 - valLoss: 0.4319930374622345 - trainLoss: 0.41745495796203613\n",
      "cnt: 0 - valLoss: 0.4319919943809509 - trainLoss: 0.41745299100875854\n",
      "cnt: 0 - valLoss: 0.43199118971824646 - trainLoss: 0.41745102405548096\n",
      "cnt: 0 - valLoss: 0.43199020624160767 - trainLoss: 0.41744905710220337\n",
      "cnt: 0 - valLoss: 0.4319893419742584 - trainLoss: 0.417447030544281\n",
      "cnt: 0 - valLoss: 0.4319884777069092 - trainLoss: 0.41744503378868103\n",
      "cnt: 0 - valLoss: 0.4319874048233032 - trainLoss: 0.41744306683540344\n",
      "cnt: 0 - valLoss: 0.43198657035827637 - trainLoss: 0.4174410402774811\n",
      "cnt: 0 - valLoss: 0.4319857358932495 - trainLoss: 0.4174390435218811\n",
      "cnt: 0 - valLoss: 0.43198490142822266 - trainLoss: 0.4174370765686035\n",
      "cnt: 0 - valLoss: 0.43198394775390625 - trainLoss: 0.41743502020835876\n",
      "cnt: 0 - valLoss: 0.4319829046726227 - trainLoss: 0.4174330532550812\n",
      "cnt: 0 - valLoss: 0.431982159614563 - trainLoss: 0.4174310863018036\n",
      "cnt: 0 - valLoss: 0.43198129534721375 - trainLoss: 0.41742902994155884\n",
      "cnt: 0 - valLoss: 0.43198034167289734 - trainLoss: 0.41742706298828125\n",
      "cnt: 0 - valLoss: 0.43197953701019287 - trainLoss: 0.4174250364303589\n",
      "cnt: 0 - valLoss: 0.4319784939289093 - trainLoss: 0.4174230396747589\n",
      "cnt: 0 - valLoss: 0.43197768926620483 - trainLoss: 0.41742101311683655\n",
      "cnt: 0 - valLoss: 0.4319767951965332 - trainLoss: 0.41741904616355896\n",
      "cnt: 0 - valLoss: 0.43197593092918396 - trainLoss: 0.417417049407959\n",
      "cnt: 0 - valLoss: 0.43197494745254517 - trainLoss: 0.4174150228500366\n",
      "cnt: 0 - valLoss: 0.4319740831851959 - trainLoss: 0.41741302609443665\n",
      "cnt: 0 - valLoss: 0.4319731593132019 - trainLoss: 0.4174109995365143\n",
      "cnt: 0 - valLoss: 0.4319723844528198 - trainLoss: 0.4174090325832367\n",
      "cnt: 0 - valLoss: 0.43197134137153625 - trainLoss: 0.4174070358276367\n",
      "cnt: 0 - valLoss: 0.431970477104187 - trainLoss: 0.41740500926971436\n",
      "cnt: 0 - valLoss: 0.43196964263916016 - trainLoss: 0.4174030125141144\n",
      "cnt: 0 - valLoss: 0.4319687783718109 - trainLoss: 0.4174010157585144\n",
      "cnt: 0 - valLoss: 0.43196773529052734 - trainLoss: 0.41739901900291443\n",
      "cnt: 0 - valLoss: 0.4319669306278229 - trainLoss: 0.41739699244499207\n",
      "cnt: 0 - valLoss: 0.43196606636047363 - trainLoss: 0.4173950254917145\n",
      "cnt: 0 - valLoss: 0.4319649934768677 - trainLoss: 0.4173930287361145\n",
      "cnt: 0 - valLoss: 0.4319641888141632 - trainLoss: 0.41739100217819214\n",
      "cnt: 0 - valLoss: 0.43196338415145874 - trainLoss: 0.41738900542259216\n",
      "cnt: 0 - valLoss: 0.4319625198841095 - trainLoss: 0.4173870384693146\n",
      "cnt: 0 - valLoss: 0.4319614768028259 - trainLoss: 0.4173850119113922\n",
      "cnt: 0 - valLoss: 0.4319606423377991 - trainLoss: 0.41738301515579224\n",
      "cnt: 0 - valLoss: 0.43195977807044983 - trainLoss: 0.41738104820251465\n",
      "cnt: 0 - valLoss: 0.43195897340774536 - trainLoss: 0.4173790216445923\n",
      "cnt: 0 - valLoss: 0.43195804953575134 - trainLoss: 0.4173770248889923\n",
      "cnt: 0 - valLoss: 0.43195703625679016 - trainLoss: 0.41737499833106995\n",
      "cnt: 0 - valLoss: 0.4319562017917633 - trainLoss: 0.41737300157546997\n",
      "cnt: 0 - valLoss: 0.43195539712905884 - trainLoss: 0.4173710346221924\n",
      "cnt: 0 - valLoss: 0.4319545328617096 - trainLoss: 0.41736900806427\n",
      "cnt: 0 - valLoss: 0.4319535195827484 - trainLoss: 0.41736701130867004\n",
      "cnt: 0 - valLoss: 0.43195265531539917 - trainLoss: 0.41736504435539246\n",
      "cnt: 0 - valLoss: 0.4319518804550171 - trainLoss: 0.4173630177974701\n",
      "cnt: 0 - valLoss: 0.43195077776908875 - trainLoss: 0.4173610210418701\n",
      "cnt: 0 - valLoss: 0.43195000290870667 - trainLoss: 0.41735899448394775\n",
      "cnt: 0 - valLoss: 0.4319491386413574 - trainLoss: 0.41735702753067017\n",
      "cnt: 0 - valLoss: 0.43194833397865295 - trainLoss: 0.4173550307750702\n",
      "cnt: 0 - valLoss: 0.431947261095047 - trainLoss: 0.4173530042171478\n",
      "cnt: 0 - valLoss: 0.43194645643234253 - trainLoss: 0.41735103726387024\n",
      "cnt: 0 - valLoss: 0.4319456219673157 - trainLoss: 0.4173490107059479\n",
      "cnt: 0 - valLoss: 0.4319448173046112 - trainLoss: 0.4173470437526703\n",
      "cnt: 0 - valLoss: 0.43194374442100525 - trainLoss: 0.4173450469970703\n",
      "cnt: 0 - valLoss: 0.4319429397583008 - trainLoss: 0.41734302043914795\n",
      "cnt: 0 - valLoss: 0.4319421052932739 - trainLoss: 0.417341023683548\n",
      "cnt: 0 - valLoss: 0.43194127082824707 - trainLoss: 0.4173389971256256\n",
      "cnt: 0 - valLoss: 0.4319402277469635 - trainLoss: 0.417337030172348\n",
      "cnt: 0 - valLoss: 0.43193942308425903 - trainLoss: 0.41733503341674805\n",
      "cnt: 0 - valLoss: 0.4319385886192322 - trainLoss: 0.41733306646347046\n",
      "cnt: 0 - valLoss: 0.4319377839565277 - trainLoss: 0.4173310399055481\n",
      "cnt: 0 - valLoss: 0.43193671107292175 - trainLoss: 0.4173290431499481\n",
      "cnt: 0 - valLoss: 0.4319359064102173 - trainLoss: 0.41732707619667053\n",
      "cnt: 0 - valLoss: 0.43193507194519043 - trainLoss: 0.41732504963874817\n",
      "cnt: 0 - valLoss: 0.4319342374801636 - trainLoss: 0.4173230528831482\n",
      "cnt: 0 - valLoss: 0.43193319439888 - trainLoss: 0.4173210859298706\n",
      "cnt: 0 - valLoss: 0.43193238973617554 - trainLoss: 0.41731905937194824\n",
      "cnt: 0 - valLoss: 0.43193158507347107 - trainLoss: 0.41731706261634827\n",
      "cnt: 0 - valLoss: 0.4319307208061218 - trainLoss: 0.4173150956630707\n",
      "cnt: 0 - valLoss: 0.43192967772483826 - trainLoss: 0.4173130393028259\n",
      "cnt: 0 - valLoss: 0.4319288730621338 - trainLoss: 0.41731107234954834\n",
      "cnt: 0 - valLoss: 0.4319280683994293 - trainLoss: 0.417309045791626\n",
      "cnt: 0 - valLoss: 0.43192723393440247 - trainLoss: 0.417307049036026\n",
      "cnt: 0 - valLoss: 0.4319261610507965 - trainLoss: 0.4173050820827484\n",
      "cnt: 0 - valLoss: 0.43192535638809204 - trainLoss: 0.41730302572250366\n",
      "cnt: 0 - valLoss: 0.4319245517253876 - trainLoss: 0.4173010587692261\n",
      "cnt: 0 - valLoss: 0.4319237172603607 - trainLoss: 0.4172990620136261\n",
      "cnt: 0 - valLoss: 0.4319227337837219 - trainLoss: 0.4172970652580261\n",
      "cnt: 0 - valLoss: 0.4319218695163727 - trainLoss: 0.41729503870010376\n",
      "cnt: 0 - valLoss: 0.4319210648536682 - trainLoss: 0.41729307174682617\n",
      "cnt: 0 - valLoss: 0.43192026019096375 - trainLoss: 0.4172910153865814\n",
      "cnt: 0 - valLoss: 0.4319192171096802 - trainLoss: 0.41728904843330383\n",
      "cnt: 0 - valLoss: 0.4319183826446533 - trainLoss: 0.41728705167770386\n",
      "cnt: 0 - valLoss: 0.43191754817962646 - trainLoss: 0.41728508472442627\n",
      "cnt: 0 - valLoss: 0.431916743516922 - trainLoss: 0.4172830581665039\n",
      "cnt: 0 - valLoss: 0.4319157004356384 - trainLoss: 0.41728106141090393\n",
      "cnt: 0 - valLoss: 0.43191489577293396 - trainLoss: 0.41727909445762634\n",
      "cnt: 0 - valLoss: 0.4319140911102295 - trainLoss: 0.417277067899704\n",
      "cnt: 0 - valLoss: 0.43191325664520264 - trainLoss: 0.4172751009464264\n",
      "cnt: 0 - valLoss: 0.43191224336624146 - trainLoss: 0.4172731041908264\n",
      "cnt: 0 - valLoss: 0.431911438703537 - trainLoss: 0.41727113723754883\n",
      "cnt: 0 - valLoss: 0.4319106638431549 - trainLoss: 0.41726911067962646\n",
      "cnt: 0 - valLoss: 0.4319096803665161 - trainLoss: 0.4172671139240265\n",
      "cnt: 0 - valLoss: 0.43190884590148926 - trainLoss: 0.4172650873661041\n",
      "cnt: 0 - valLoss: 0.4319080114364624 - trainLoss: 0.41726312041282654\n",
      "cnt: 0 - valLoss: 0.43190717697143555 - trainLoss: 0.41726112365722656\n",
      "cnt: 0 - valLoss: 0.43190625309944153 - trainLoss: 0.4172591269016266\n",
      "cnt: 0 - valLoss: 0.4319054186344147 - trainLoss: 0.4172571301460266\n",
      "cnt: 0 - valLoss: 0.43190455436706543 - trainLoss: 0.41725510358810425\n",
      "cnt: 0 - valLoss: 0.43190380930900574 - trainLoss: 0.4172531068325043\n",
      "cnt: 0 - valLoss: 0.43190276622772217 - trainLoss: 0.4172511398792267\n",
      "cnt: 0 - valLoss: 0.4319019913673401 - trainLoss: 0.4172491133213043\n",
      "cnt: 0 - valLoss: 0.43190112709999084 - trainLoss: 0.41724714636802673\n",
      "cnt: 0 - valLoss: 0.4319003224372864 - trainLoss: 0.41724514961242676\n",
      "cnt: 0 - valLoss: 0.4318993389606476 - trainLoss: 0.41724318265914917\n",
      "cnt: 0 - valLoss: 0.4318985641002655 - trainLoss: 0.4172412157058716\n",
      "cnt: 0 - valLoss: 0.43189769983291626 - trainLoss: 0.41723915934562683\n",
      "cnt: 0 - valLoss: 0.4318971037864685 - trainLoss: 0.41723722219467163\n",
      "cnt: 0 - valLoss: 0.4318963885307312 - trainLoss: 0.4172353148460388\n",
      "cnt: 0 - valLoss: 0.4318956136703491 - trainLoss: 0.417233407497406\n",
      "cnt: 0 - valLoss: 0.43189483880996704 - trainLoss: 0.4172315299510956\n",
      "cnt: 0 - valLoss: 0.43189412355422974 - trainLoss: 0.41722965240478516\n",
      "cnt: 0 - valLoss: 0.4318932890892029 - trainLoss: 0.41722777485847473\n",
      "cnt: 0 - valLoss: 0.4318925142288208 - trainLoss: 0.4172258973121643\n",
      "cnt: 0 - valLoss: 0.4318917989730835 - trainLoss: 0.4172239899635315\n",
      "cnt: 0 - valLoss: 0.4318910241127014 - trainLoss: 0.41722211241722107\n",
      "cnt: 0 - valLoss: 0.43189024925231934 - trainLoss: 0.41722020506858826\n",
      "cnt: 0 - valLoss: 0.43188947439193726 - trainLoss: 0.41721829771995544\n",
      "cnt: 0 - valLoss: 0.43188875913619995 - trainLoss: 0.41721639037132263\n",
      "cnt: 0 - valLoss: 0.43188801407814026 - trainLoss: 0.4172145128250122\n",
      "cnt: 0 - valLoss: 0.4318872094154358 - trainLoss: 0.4172126054763794\n",
      "cnt: 0 - valLoss: 0.4318864047527313 - trainLoss: 0.41721072793006897\n",
      "cnt: 0 - valLoss: 0.43188565969467163 - trainLoss: 0.41720885038375854\n",
      "cnt: 0 - valLoss: 0.43188488483428955 - trainLoss: 0.4172069728374481\n",
      "cnt: 0 - valLoss: 0.43188416957855225 - trainLoss: 0.4172050356864929\n",
      "cnt: 0 - valLoss: 0.43188339471817017 - trainLoss: 0.4172031581401825\n",
      "cnt: 0 - valLoss: 0.43188267946243286 - trainLoss: 0.41720131039619446\n",
      "cnt: 0 - valLoss: 0.4318819046020508 - trainLoss: 0.41719940304756165\n",
      "cnt: 0 - valLoss: 0.4318811595439911 - trainLoss: 0.41719749569892883\n",
      "cnt: 0 - valLoss: 0.431880384683609 - trainLoss: 0.4171956181526184\n",
      "cnt: 0 - valLoss: 0.43187960982322693 - trainLoss: 0.4171937108039856\n",
      "cnt: 0 - valLoss: 0.43187886476516724 - trainLoss: 0.4171918034553528\n",
      "cnt: 0 - valLoss: 0.43187808990478516 - trainLoss: 0.41718992590904236\n",
      "cnt: 0 - valLoss: 0.43187734484672546 - trainLoss: 0.41718801856040955\n",
      "cnt: 0 - valLoss: 0.4318765699863434 - trainLoss: 0.4171861708164215\n",
      "cnt: 0 - valLoss: 0.4318758547306061 - trainLoss: 0.4171842336654663\n",
      "cnt: 0 - valLoss: 0.431875079870224 - trainLoss: 0.41718238592147827\n",
      "cnt: 0 - valLoss: 0.4318743050098419 - trainLoss: 0.41718050837516785\n",
      "cnt: 0 - valLoss: 0.43187353014945984 - trainLoss: 0.41717854142189026\n",
      "cnt: 0 - valLoss: 0.4318728446960449 - trainLoss: 0.4171766936779022\n",
      "cnt: 0 - valLoss: 0.43187206983566284 - trainLoss: 0.4171748161315918\n",
      "cnt: 0 - valLoss: 0.43187129497528076 - trainLoss: 0.417172908782959\n",
      "cnt: 0 - valLoss: 0.4318704605102539 - trainLoss: 0.41717100143432617\n",
      "cnt: 0 - valLoss: 0.431869775056839 - trainLoss: 0.41716912388801575\n",
      "cnt: 0 - valLoss: 0.4318690001964569 - trainLoss: 0.4171672463417053\n",
      "cnt: 0 - valLoss: 0.43186822533607483 - trainLoss: 0.4171653687953949\n",
      "cnt: 0 - valLoss: 0.43186748027801514 - trainLoss: 0.4171634912490845\n",
      "cnt: 0 - valLoss: 0.43186676502227783 - trainLoss: 0.41716158390045166\n",
      "cnt: 0 - valLoss: 0.43186599016189575 - trainLoss: 0.41715970635414124\n",
      "cnt: 0 - valLoss: 0.43186521530151367 - trainLoss: 0.4171578288078308\n",
      "cnt: 0 - valLoss: 0.4318644404411316 - trainLoss: 0.4171559512615204\n",
      "cnt: 0 - valLoss: 0.4318636655807495 - trainLoss: 0.4171540141105652\n",
      "cnt: 0 - valLoss: 0.4318629205226898 - trainLoss: 0.41715216636657715\n",
      "cnt: 0 - valLoss: 0.43186214566230774 - trainLoss: 0.41715025901794434\n",
      "cnt: 0 - valLoss: 0.43186140060424805 - trainLoss: 0.4171483814716339\n",
      "cnt: 0 - valLoss: 0.43186062574386597 - trainLoss: 0.4171465337276459\n",
      "cnt: 0 - valLoss: 0.43185991048812866 - trainLoss: 0.4171445965766907\n",
      "cnt: 0 - valLoss: 0.4318591356277466 - trainLoss: 0.41714268922805786\n",
      "cnt: 0 - valLoss: 0.4318583607673645 - trainLoss: 0.4171408414840698\n",
      "cnt: 0 - valLoss: 0.4318575859069824 - trainLoss: 0.4171389043331146\n",
      "cnt: 0 - valLoss: 0.43185681104660034 - trainLoss: 0.4171370267868042\n",
      "cnt: 0 - valLoss: 0.4318561255931854 - trainLoss: 0.4171351492404938\n",
      "cnt: 0 - valLoss: 0.43185532093048096 - trainLoss: 0.41713327169418335\n",
      "cnt: 0 - valLoss: 0.43185460567474365 - trainLoss: 0.41713136434555054\n",
      "cnt: 0 - valLoss: 0.4318537712097168 - trainLoss: 0.4171294867992401\n",
      "cnt: 0 - valLoss: 0.4318530559539795 - trainLoss: 0.4171276092529297\n",
      "cnt: 0 - valLoss: 0.4318522810935974 - trainLoss: 0.41712573170661926\n",
      "cnt: 0 - valLoss: 0.43185150623321533 - trainLoss: 0.41712385416030884\n",
      "cnt: 0 - valLoss: 0.43185073137283325 - trainLoss: 0.4171219766139984\n",
      "cnt: 0 - valLoss: 0.43185001611709595 - trainLoss: 0.4171200692653656\n",
      "cnt: 0 - valLoss: 0.43184924125671387 - trainLoss: 0.4171181917190552\n",
      "cnt: 0 - valLoss: 0.4318484663963318 - trainLoss: 0.41711631417274475\n",
      "cnt: 0 - valLoss: 0.4318477213382721 - trainLoss: 0.41711437702178955\n",
      "cnt: 0 - valLoss: 0.4318469762802124 - trainLoss: 0.4171124994754791\n",
      "cnt: 0 - valLoss: 0.4318462014198303 - trainLoss: 0.4171105623245239\n",
      "cnt: 0 - valLoss: 0.43184536695480347 - trainLoss: 0.4171087443828583\n",
      "cnt: 0 - valLoss: 0.43184465169906616 - trainLoss: 0.41710686683654785\n",
      "cnt: 0 - valLoss: 0.4318438470363617 - trainLoss: 0.41710495948791504\n",
      "cnt: 0 - valLoss: 0.431843101978302 - trainLoss: 0.4171030819416046\n",
      "cnt: 0 - valLoss: 0.4318423271179199 - trainLoss: 0.4171011745929718\n",
      "cnt: 0 - valLoss: 0.43184155225753784 - trainLoss: 0.417099267244339\n",
      "cnt: 0 - valLoss: 0.43184077739715576 - trainLoss: 0.41709738969802856\n",
      "cnt: 0 - valLoss: 0.43184003233909607 - trainLoss: 0.41709551215171814\n",
      "cnt: 0 - valLoss: 0.4318392872810364 - trainLoss: 0.4170936346054077\n",
      "cnt: 0 - valLoss: 0.4318385124206543 - trainLoss: 0.4170917868614197\n",
      "cnt: 0 - valLoss: 0.431837797164917 - trainLoss: 0.4170898497104645\n",
      "cnt: 0 - valLoss: 0.4318370223045349 - trainLoss: 0.41708797216415405\n",
      "cnt: 0 - valLoss: 0.43183624744415283 - trainLoss: 0.417086124420166\n",
      "cnt: 0 - valLoss: 0.43183547258377075 - trainLoss: 0.4170842170715332\n",
      "cnt: 0 - valLoss: 0.4318346679210663 - trainLoss: 0.4170823395252228\n",
      "cnt: 0 - valLoss: 0.4318339228630066 - trainLoss: 0.41708043217658997\n",
      "cnt: 0 - valLoss: 0.4318331480026245 - trainLoss: 0.41707855463027954\n",
      "cnt: 0 - valLoss: 0.4318324029445648 - trainLoss: 0.4170766770839691\n",
      "cnt: 0 - valLoss: 0.43183162808418274 - trainLoss: 0.4170747995376587\n",
      "cnt: 0 - valLoss: 0.43183085322380066 - trainLoss: 0.41707292199134827\n",
      "cnt: 0 - valLoss: 0.43183010816574097 - trainLoss: 0.41707101464271545\n",
      "cnt: 0 - valLoss: 0.43182939291000366 - trainLoss: 0.41706913709640503\n",
      "cnt: 0 - valLoss: 0.4318285882472992 - trainLoss: 0.4170672595500946\n",
      "cnt: 0 - valLoss: 0.4318278431892395 - trainLoss: 0.4170653820037842\n",
      "cnt: 0 - valLoss: 0.4318270683288574 - trainLoss: 0.41706350445747375\n",
      "cnt: 0 - valLoss: 0.43182629346847534 - trainLoss: 0.41706159710884094\n",
      "cnt: 0 - valLoss: 0.43182551860809326 - trainLoss: 0.4170597195625305\n",
      "cnt: 0 - valLoss: 0.43182477355003357 - trainLoss: 0.4170578420162201\n",
      "cnt: 0 - valLoss: 0.4318239986896515 - trainLoss: 0.41705596446990967\n",
      "cnt: 0 - valLoss: 0.4318232238292694 - trainLoss: 0.41705408692359924\n",
      "cnt: 0 - valLoss: 0.43182244896888733 - trainLoss: 0.4170522093772888\n",
      "cnt: 0 - valLoss: 0.43182167410850525 - trainLoss: 0.417050302028656\n",
      "cnt: 0 - valLoss: 0.43182089924812317 - trainLoss: 0.4170484244823456\n",
      "cnt: 0 - valLoss: 0.4318201243877411 - trainLoss: 0.41704654693603516\n",
      "cnt: 0 - valLoss: 0.43181943893432617 - trainLoss: 0.41704466938972473\n",
      "cnt: 0 - valLoss: 0.4318186342716217 - trainLoss: 0.41704273223876953\n",
      "cnt: 0 - valLoss: 0.4318178594112396 - trainLoss: 0.4170408844947815\n",
      "cnt: 0 - valLoss: 0.4318171441555023 - trainLoss: 0.41703900694847107\n",
      "cnt: 0 - valLoss: 0.43181636929512024 - trainLoss: 0.41703712940216064\n",
      "cnt: 0 - valLoss: 0.43181562423706055 - trainLoss: 0.4170352518558502\n",
      "cnt: 0 - valLoss: 0.43181484937667847 - trainLoss: 0.4170333743095398\n",
      "cnt: 0 - valLoss: 0.43181413412094116 - trainLoss: 0.417031466960907\n",
      "cnt: 0 - valLoss: 0.4318133294582367 - trainLoss: 0.41702958941459656\n",
      "cnt: 0 - valLoss: 0.43181249499320984 - trainLoss: 0.41702771186828613\n",
      "cnt: 0 - valLoss: 0.43181177973747253 - trainLoss: 0.4170258343219757\n",
      "cnt: 0 - valLoss: 0.43181100487709045 - trainLoss: 0.4170239567756653\n",
      "cnt: 0 - valLoss: 0.4318102300167084 - trainLoss: 0.41702204942703247\n",
      "cnt: 0 - valLoss: 0.43180951476097107 - trainLoss: 0.41702017188072205\n",
      "cnt: 0 - valLoss: 0.431808739900589 - trainLoss: 0.4170182943344116\n",
      "cnt: 0 - valLoss: 0.4318079650402069 - trainLoss: 0.4170164167881012\n",
      "cnt: 0 - valLoss: 0.43180719017982483 - trainLoss: 0.41701453924179077\n",
      "cnt: 0 - valLoss: 0.43180641531944275 - trainLoss: 0.41701263189315796\n",
      "cnt: 0 - valLoss: 0.43180570006370544 - trainLoss: 0.41701075434684753\n",
      "cnt: 0 - valLoss: 0.43180492520332336 - trainLoss: 0.4170088768005371\n",
      "cnt: 0 - valLoss: 0.4318041503429413 - trainLoss: 0.4170069992542267\n",
      "cnt: 0 - valLoss: 0.4318034052848816 - trainLoss: 0.41700512170791626\n",
      "cnt: 0 - valLoss: 0.4318026304244995 - trainLoss: 0.41700324416160583\n",
      "cnt: 0 - valLoss: 0.4318018853664398 - trainLoss: 0.4170013964176178\n",
      "cnt: 0 - valLoss: 0.43180111050605774 - trainLoss: 0.4169994592666626\n",
      "cnt: 0 - valLoss: 0.4318002760410309 - trainLoss: 0.4169975817203522\n",
      "cnt: 0 - valLoss: 0.4317995607852936 - trainLoss: 0.41699570417404175\n",
      "cnt: 0 - valLoss: 0.4317987859249115 - trainLoss: 0.4169938266277313\n",
      "cnt: 0 - valLoss: 0.4317980110645294 - trainLoss: 0.4169919192790985\n",
      "cnt: 0 - valLoss: 0.43179723620414734 - trainLoss: 0.4169900417327881\n",
      "cnt: 0 - valLoss: 0.43179646134376526 - trainLoss: 0.41698816418647766\n",
      "cnt: 0 - valLoss: 0.4317956864833832 - trainLoss: 0.41698628664016724\n",
      "cnt: 0 - valLoss: 0.4317949712276459 - trainLoss: 0.4169844388961792\n",
      "cnt: 0 - valLoss: 0.4317941963672638 - trainLoss: 0.4169825613498688\n",
      "cnt: 0 - valLoss: 0.4317934811115265 - trainLoss: 0.41698068380355835\n",
      "cnt: 0 - valLoss: 0.4317927062511444 - trainLoss: 0.41697877645492554\n",
      "cnt: 0 - valLoss: 0.43179193139076233 - trainLoss: 0.4169768989086151\n",
      "cnt: 0 - valLoss: 0.43179115653038025 - trainLoss: 0.4169750213623047\n",
      "cnt: 0 - valLoss: 0.43179038166999817 - trainLoss: 0.41697314381599426\n",
      "cnt: 0 - valLoss: 0.43178966641426086 - trainLoss: 0.41697126626968384\n",
      "cnt: 0 - valLoss: 0.4317888915538788 - trainLoss: 0.4169693887233734\n",
      "cnt: 0 - valLoss: 0.4317881166934967 - trainLoss: 0.4169675409793854\n",
      "cnt: 0 - valLoss: 0.43178731203079224 - trainLoss: 0.4169656038284302\n",
      "cnt: 0 - valLoss: 0.43178653717041016 - trainLoss: 0.41696372628211975\n",
      "cnt: 0 - valLoss: 0.4317857623100281 - trainLoss: 0.4169618487358093\n",
      "cnt: 0 - valLoss: 0.431784987449646 - trainLoss: 0.4169600009918213\n",
      "cnt: 0 - valLoss: 0.4317842423915863 - trainLoss: 0.4169580638408661\n",
      "cnt: 0 - valLoss: 0.4317834973335266 - trainLoss: 0.41695618629455566\n",
      "cnt: 0 - valLoss: 0.43178272247314453 - trainLoss: 0.4169543385505676\n",
      "cnt: 0 - valLoss: 0.43178197741508484 - trainLoss: 0.4169524610042572\n",
      "cnt: 0 - valLoss: 0.43178120255470276 - trainLoss: 0.4169505834579468\n",
      "cnt: 0 - valLoss: 0.4317804276943207 - trainLoss: 0.41694870591163635\n",
      "cnt: 0 - valLoss: 0.431779682636261 - trainLoss: 0.4169468283653259\n",
      "cnt: 0 - valLoss: 0.4317789375782013 - trainLoss: 0.4169449210166931\n",
      "cnt: 0 - valLoss: 0.4317781329154968 - trainLoss: 0.4169430434703827\n",
      "cnt: 0 - valLoss: 0.43177738785743713 - trainLoss: 0.41694116592407227\n",
      "cnt: 0 - valLoss: 0.43177661299705505 - trainLoss: 0.41693928837776184\n",
      "cnt: 0 - valLoss: 0.43177589774131775 - trainLoss: 0.4169374108314514\n",
      "cnt: 0 - valLoss: 0.43177512288093567 - trainLoss: 0.4169355034828186\n",
      "cnt: 0 - valLoss: 0.4317743480205536 - trainLoss: 0.4169336259365082\n",
      "cnt: 0 - valLoss: 0.4317735731601715 - trainLoss: 0.41693174839019775\n",
      "cnt: 0 - valLoss: 0.43177279829978943 - trainLoss: 0.4169299006462097\n",
      "cnt: 0 - valLoss: 0.43177199363708496 - trainLoss: 0.4169280230998993\n",
      "cnt: 0 - valLoss: 0.43177127838134766 - trainLoss: 0.41692614555358887\n",
      "cnt: 0 - valLoss: 0.4317705035209656 - trainLoss: 0.41692426800727844\n",
      "cnt: 0 - valLoss: 0.4317697286605835 - trainLoss: 0.416922390460968\n",
      "cnt: 0 - valLoss: 0.4317689538002014 - trainLoss: 0.4169204533100128\n",
      "cnt: 0 - valLoss: 0.43176817893981934 - trainLoss: 0.4169186055660248\n",
      "cnt: 0 - valLoss: 0.43176740407943726 - trainLoss: 0.41691672801971436\n",
      "cnt: 0 - valLoss: 0.4317666292190552 - trainLoss: 0.41691485047340393\n",
      "cnt: 0 - valLoss: 0.4317658543586731 - trainLoss: 0.4169130027294159\n",
      "cnt: 0 - valLoss: 0.4317651391029358 - trainLoss: 0.41691112518310547\n",
      "cnt: 0 - valLoss: 0.4317643642425537 - trainLoss: 0.41690924763679504\n",
      "cnt: 0 - valLoss: 0.43176358938217163 - trainLoss: 0.41690734028816223\n",
      "cnt: 0 - valLoss: 0.4317628741264343 - trainLoss: 0.4169054627418518\n",
      "cnt: 0 - valLoss: 0.43176209926605225 - trainLoss: 0.4169035851955414\n",
      "cnt: 0 - valLoss: 0.43176132440567017 - trainLoss: 0.41690170764923096\n",
      "cnt: 0 - valLoss: 0.4317605495452881 - trainLoss: 0.41689983010292053\n",
      "cnt: 0 - valLoss: 0.431759774684906 - trainLoss: 0.4168979525566101\n",
      "cnt: 0 - valLoss: 0.4317589998245239 - trainLoss: 0.4168960452079773\n",
      "cnt: 0 - valLoss: 0.43175822496414185 - trainLoss: 0.41689422726631165\n",
      "cnt: 0 - valLoss: 0.43175747990608215 - trainLoss: 0.41689229011535645\n",
      "cnt: 0 - valLoss: 0.43175673484802246 - trainLoss: 0.4168904423713684\n",
      "cnt: 0 - valLoss: 0.4317559599876404 - trainLoss: 0.416888564825058\n",
      "cnt: 0 - valLoss: 0.43175509572029114 - trainLoss: 0.41688668727874756\n",
      "cnt: 0 - valLoss: 0.43175438046455383 - trainLoss: 0.41688480973243713\n",
      "cnt: 0 - valLoss: 0.43175360560417175 - trainLoss: 0.4168829619884491\n",
      "cnt: 0 - valLoss: 0.4317528307437897 - trainLoss: 0.41688108444213867\n",
      "cnt: 0 - valLoss: 0.4317520558834076 - trainLoss: 0.41687917709350586\n",
      "cnt: 0 - valLoss: 0.4317512810230255 - trainLoss: 0.41687729954719543\n",
      "cnt: 0 - valLoss: 0.4317505359649658 - trainLoss: 0.416875422000885\n",
      "cnt: 0 - valLoss: 0.43174976110458374 - trainLoss: 0.4168735444545746\n",
      "cnt: 0 - valLoss: 0.43174898624420166 - trainLoss: 0.41687166690826416\n",
      "cnt: 0 - valLoss: 0.43174824118614197 - trainLoss: 0.41686978936195374\n",
      "cnt: 0 - valLoss: 0.4317474365234375 - trainLoss: 0.4168678820133209\n",
      "cnt: 0 - valLoss: 0.4317466616630554 - trainLoss: 0.4168660640716553\n",
      "cnt: 0 - valLoss: 0.43174588680267334 - trainLoss: 0.41686415672302246\n",
      "cnt: 0 - valLoss: 0.43174517154693604 - trainLoss: 0.4168623387813568\n",
      "cnt: 0 - valLoss: 0.43174439668655396 - trainLoss: 0.416860431432724\n",
      "cnt: 0 - valLoss: 0.43174365162849426 - trainLoss: 0.4168585538864136\n",
      "cnt: 0 - valLoss: 0.4317428469657898 - trainLoss: 0.41685667634010315\n",
      "cnt: 0 - valLoss: 0.4317420721054077 - trainLoss: 0.4168547987937927\n",
      "cnt: 0 - valLoss: 0.43174129724502563 - trainLoss: 0.4168529510498047\n",
      "cnt: 0 - valLoss: 0.43174058198928833 - trainLoss: 0.41685107350349426\n",
      "cnt: 0 - valLoss: 0.43173977732658386 - trainLoss: 0.41684919595718384\n",
      "cnt: 0 - valLoss: 0.43173903226852417 - trainLoss: 0.41684725880622864\n",
      "cnt: 0 - valLoss: 0.4317382574081421 - trainLoss: 0.4168454110622406\n",
      "cnt: 0 - valLoss: 0.43173748254776 - trainLoss: 0.41684359312057495\n",
      "cnt: 0 - valLoss: 0.43173667788505554 - trainLoss: 0.41684168577194214\n",
      "cnt: 0 - valLoss: 0.43173596262931824 - trainLoss: 0.4168397784233093\n",
      "cnt: 0 - valLoss: 0.4317351281642914 - trainLoss: 0.4168379306793213\n",
      "cnt: 0 - valLoss: 0.4317343235015869 - trainLoss: 0.41683605313301086\n",
      "cnt: 0 - valLoss: 0.4317335784435272 - trainLoss: 0.41683417558670044\n",
      "cnt: 0 - valLoss: 0.4317328631877899 - trainLoss: 0.4168322682380676\n",
      "cnt: 0 - valLoss: 0.43173208832740784 - trainLoss: 0.4168303906917572\n",
      "cnt: 0 - valLoss: 0.43173131346702576 - trainLoss: 0.41682854294776917\n",
      "cnt: 0 - valLoss: 0.4317305386066437 - trainLoss: 0.4168267250061035\n",
      "cnt: 0 - valLoss: 0.4317297637462616 - trainLoss: 0.4168248474597931\n",
      "cnt: 0 - valLoss: 0.4317290186882019 - trainLoss: 0.4168229401111603\n",
      "cnt: 0 - valLoss: 0.4317282736301422 - trainLoss: 0.41682106256484985\n",
      "cnt: 0 - valLoss: 0.43172749876976013 - trainLoss: 0.41681918501853943\n",
      "cnt: 0 - valLoss: 0.43172672390937805 - trainLoss: 0.416817307472229\n",
      "cnt: 0 - valLoss: 0.43172600865364075 - trainLoss: 0.4168154299259186\n",
      "cnt: 0 - valLoss: 0.43172523379325867 - trainLoss: 0.41681352257728577\n",
      "cnt: 0 - valLoss: 0.4317244589328766 - trainLoss: 0.4168117046356201\n",
      "cnt: 0 - valLoss: 0.4317237436771393 - trainLoss: 0.4168097972869873\n",
      "cnt: 0 - valLoss: 0.4317229092121124 - trainLoss: 0.4168079197406769\n",
      "cnt: 0 - valLoss: 0.4317221939563751 - trainLoss: 0.41680607199668884\n",
      "cnt: 0 - valLoss: 0.43172144889831543 - trainLoss: 0.4168041944503784\n",
      "cnt: 0 - valLoss: 0.43172070384025574 - trainLoss: 0.416802316904068\n",
      "cnt: 0 - valLoss: 0.43171992897987366 - trainLoss: 0.41680043935775757\n",
      "cnt: 0 - valLoss: 0.4317191541194916 - trainLoss: 0.41679856181144714\n",
      "cnt: 0 - valLoss: 0.4317183792591095 - trainLoss: 0.4167966842651367\n",
      "cnt: 0 - valLoss: 0.4317176342010498 - trainLoss: 0.4167948365211487\n",
      "cnt: 0 - valLoss: 0.4317168891429901 - trainLoss: 0.41679295897483826\n",
      "cnt: 0 - valLoss: 0.43171611428260803 - trainLoss: 0.4167911112308502\n",
      "cnt: 0 - valLoss: 0.43171536922454834 - trainLoss: 0.4167892336845398\n",
      "cnt: 0 - valLoss: 0.43171459436416626 - trainLoss: 0.416787326335907\n",
      "cnt: 0 - valLoss: 0.4317138195037842 - trainLoss: 0.41678544878959656\n",
      "cnt: 0 - valLoss: 0.4317130744457245 - trainLoss: 0.41678357124328613\n",
      "cnt: 0 - valLoss: 0.4317122995853424 - trainLoss: 0.4167816936969757\n",
      "cnt: 0 - valLoss: 0.4317115247249603 - trainLoss: 0.41677984595298767\n",
      "cnt: 0 - valLoss: 0.43171074986457825 - trainLoss: 0.41677799820899963\n",
      "cnt: 0 - valLoss: 0.43171000480651855 - trainLoss: 0.4167761206626892\n",
      "cnt: 0 - valLoss: 0.43170925974845886 - trainLoss: 0.4167742431163788\n",
      "cnt: 0 - valLoss: 0.4317084848880768 - trainLoss: 0.41677236557006836\n",
      "cnt: 0 - valLoss: 0.4317077398300171 - trainLoss: 0.41677048802375793\n",
      "cnt: 0 - valLoss: 0.431706964969635 - trainLoss: 0.4167685806751251\n",
      "cnt: 0 - valLoss: 0.4317062497138977 - trainLoss: 0.4167667031288147\n",
      "cnt: 0 - valLoss: 0.4317054748535156 - trainLoss: 0.41676485538482666\n",
      "cnt: 0 - valLoss: 0.4317047595977783 - trainLoss: 0.416763037443161\n",
      "cnt: 0 - valLoss: 0.43170398473739624 - trainLoss: 0.4167611300945282\n",
      "cnt: 0 - valLoss: 0.43170320987701416 - trainLoss: 0.4167592525482178\n",
      "cnt: 0 - valLoss: 0.4317024350166321 - trainLoss: 0.41675737500190735\n",
      "cnt: 0 - valLoss: 0.43170166015625 - trainLoss: 0.4167555272579193\n",
      "cnt: 0 - valLoss: 0.4317009449005127 - trainLoss: 0.4167536497116089\n",
      "cnt: 0 - valLoss: 0.4317001700401306 - trainLoss: 0.41675177216529846\n",
      "cnt: 0 - valLoss: 0.4316994249820709 - trainLoss: 0.4167499244213104\n",
      "cnt: 0 - valLoss: 0.43169865012168884 - trainLoss: 0.416748046875\n",
      "cnt: 0 - valLoss: 0.43169787526130676 - trainLoss: 0.4167461693286896\n",
      "cnt: 0 - valLoss: 0.43169713020324707 - trainLoss: 0.41674432158470154\n",
      "cnt: 0 - valLoss: 0.431696355342865 - trainLoss: 0.4167424440383911\n",
      "cnt: 0 - valLoss: 0.4316956102848053 - trainLoss: 0.4167405962944031\n",
      "cnt: 0 - valLoss: 0.4316948354244232 - trainLoss: 0.41673871874809265\n",
      "cnt: 0 - valLoss: 0.4316941201686859 - trainLoss: 0.4167368412017822\n",
      "cnt: 0 - valLoss: 0.43169334530830383 - trainLoss: 0.4167349636554718\n",
      "cnt: 0 - valLoss: 0.43169257044792175 - trainLoss: 0.416733056306839\n",
      "cnt: 0 - valLoss: 0.4316917955875397 - trainLoss: 0.41673123836517334\n",
      "cnt: 0 - valLoss: 0.43169108033180237 - trainLoss: 0.4167293310165405\n",
      "cnt: 0 - valLoss: 0.4316902458667755 - trainLoss: 0.4167275130748749\n",
      "cnt: 0 - valLoss: 0.4316895306110382 - trainLoss: 0.41672560572624207\n",
      "cnt: 0 - valLoss: 0.4316888153553009 - trainLoss: 0.41672372817993164\n",
      "cnt: 0 - valLoss: 0.4316880404949188 - trainLoss: 0.4167218506336212\n",
      "cnt: 0 - valLoss: 0.43168726563453674 - trainLoss: 0.4167200028896332\n",
      "cnt: 0 - valLoss: 0.43168652057647705 - trainLoss: 0.41671818494796753\n",
      "cnt: 0 - valLoss: 0.4316857159137726 - trainLoss: 0.4167162775993347\n",
      "cnt: 0 - valLoss: 0.4316850006580353 - trainLoss: 0.4167144000530243\n",
      "cnt: 0 - valLoss: 0.4316842555999756 - trainLoss: 0.41671252250671387\n",
      "cnt: 0 - valLoss: 0.4316834807395935 - trainLoss: 0.41671064496040344\n",
      "cnt: 0 - valLoss: 0.4316827058792114 - trainLoss: 0.4167087972164154\n",
      "cnt: 0 - valLoss: 0.43168193101882935 - trainLoss: 0.41670694947242737\n",
      "cnt: 0 - valLoss: 0.43168118596076965 - trainLoss: 0.41670507192611694\n",
      "cnt: 0 - valLoss: 0.4316804111003876 - trainLoss: 0.4167031943798065\n",
      "cnt: 0 - valLoss: 0.4316796660423279 - trainLoss: 0.4167013168334961\n",
      "cnt: 0 - valLoss: 0.4316788613796234 - trainLoss: 0.41669946908950806\n",
      "cnt: 0 - valLoss: 0.43167808651924133 - trainLoss: 0.41669759154319763\n",
      "cnt: 0 - valLoss: 0.43167731165885925 - trainLoss: 0.4166957437992096\n",
      "cnt: 0 - valLoss: 0.4316765367984772 - trainLoss: 0.41669386625289917\n",
      "cnt: 0 - valLoss: 0.43167585134506226 - trainLoss: 0.41669198870658875\n",
      "cnt: 0 - valLoss: 0.4316750764846802 - trainLoss: 0.41669008135795593\n",
      "cnt: 0 - valLoss: 0.4316743016242981 - trainLoss: 0.4166882634162903\n",
      "cnt: 0 - valLoss: 0.4316735565662384 - trainLoss: 0.41668641567230225\n",
      "cnt: 0 - valLoss: 0.4316727817058563 - trainLoss: 0.4166845381259918\n",
      "cnt: 0 - valLoss: 0.43167203664779663 - trainLoss: 0.4166826605796814\n",
      "cnt: 0 - valLoss: 0.43167126178741455 - trainLoss: 0.4166807532310486\n",
      "cnt: 0 - valLoss: 0.43167054653167725 - trainLoss: 0.41667893528938293\n",
      "cnt: 0 - valLoss: 0.43166977167129517 - trainLoss: 0.4166770875453949\n",
      "cnt: 0 - valLoss: 0.4316689968109131 - trainLoss: 0.4166752099990845\n",
      "cnt: 0 - valLoss: 0.4316682815551758 - trainLoss: 0.41667330265045166\n",
      "cnt: 0 - valLoss: 0.4316675066947937 - trainLoss: 0.416671484708786\n",
      "cnt: 0 - valLoss: 0.4316667318344116 - trainLoss: 0.4166695475578308\n",
      "cnt: 0 - valLoss: 0.43166595697402954 - trainLoss: 0.4166676998138428\n",
      "cnt: 0 - valLoss: 0.43166524171829224 - trainLoss: 0.4166658818721771\n",
      "cnt: 0 - valLoss: 0.43166446685791016 - trainLoss: 0.4166640341281891\n",
      "cnt: 0 - valLoss: 0.43166372179985046 - trainLoss: 0.4166620969772339\n",
      "cnt: 0 - valLoss: 0.4316629469394684 - trainLoss: 0.41666024923324585\n",
      "cnt: 0 - valLoss: 0.4316621720790863 - trainLoss: 0.4166583716869354\n",
      "cnt: 0 - valLoss: 0.431661456823349 - trainLoss: 0.4166565239429474\n",
      "cnt: 0 - valLoss: 0.4316606819629669 - trainLoss: 0.41665464639663696\n",
      "cnt: 0 - valLoss: 0.43165990710258484 - trainLoss: 0.41665276885032654\n",
      "cnt: 0 - valLoss: 0.43165913224220276 - trainLoss: 0.4166509211063385\n",
      "cnt: 0 - valLoss: 0.4316583573818207 - trainLoss: 0.4166490435600281\n",
      "cnt: 0 - valLoss: 0.43165767192840576 - trainLoss: 0.41664719581604004\n",
      "cnt: 0 - valLoss: 0.4316568672657013 - trainLoss: 0.4166453182697296\n",
      "cnt: 0 - valLoss: 0.431656152009964 - trainLoss: 0.4166434407234192\n",
      "cnt: 0 - valLoss: 0.4316553771495819 - trainLoss: 0.41664156317710876\n",
      "cnt: 0 - valLoss: 0.43165460228919983 - trainLoss: 0.41663968563079834\n",
      "cnt: 0 - valLoss: 0.43165385723114014 - trainLoss: 0.4166378378868103\n",
      "cnt: 0 - valLoss: 0.43165314197540283 - trainLoss: 0.41663599014282227\n",
      "cnt: 0 - valLoss: 0.43165236711502075 - trainLoss: 0.41663411259651184\n",
      "cnt: 0 - valLoss: 0.43165159225463867 - trainLoss: 0.4166322350502014\n",
      "cnt: 0 - valLoss: 0.4316508173942566 - trainLoss: 0.4166303873062134\n",
      "cnt: 0 - valLoss: 0.4316500425338745 - trainLoss: 0.41662853956222534\n",
      "cnt: 0 - valLoss: 0.4316492974758148 - trainLoss: 0.4166266620159149\n",
      "cnt: 0 - valLoss: 0.4316485524177551 - trainLoss: 0.4166247844696045\n",
      "cnt: 0 - valLoss: 0.4316478371620178 - trainLoss: 0.41662293672561646\n",
      "cnt: 0 - valLoss: 0.43164706230163574 - trainLoss: 0.41662105917930603\n",
      "cnt: 0 - valLoss: 0.43164628744125366 - trainLoss: 0.4166191816329956\n",
      "cnt: 0 - valLoss: 0.4316455125808716 - trainLoss: 0.41661733388900757\n",
      "cnt: 0 - valLoss: 0.4316447973251343 - trainLoss: 0.41661545634269714\n",
      "cnt: 0 - valLoss: 0.4316440224647522 - trainLoss: 0.4166136085987091\n",
      "cnt: 0 - valLoss: 0.4316432476043701 - trainLoss: 0.4166117310523987\n",
      "cnt: 0 - valLoss: 0.4316425025463104 - trainLoss: 0.41660988330841064\n",
      "cnt: 0 - valLoss: 0.43164172768592834 - trainLoss: 0.4166080057621002\n",
      "cnt: 0 - valLoss: 0.43164098262786865 - trainLoss: 0.4166061282157898\n",
      "cnt: 0 - valLoss: 0.43164023756980896 - trainLoss: 0.41660428047180176\n",
      "cnt: 0 - valLoss: 0.4316394627094269 - trainLoss: 0.41660234332084656\n",
      "cnt: 0 - valLoss: 0.4316387474536896 - trainLoss: 0.4166005551815033\n",
      "cnt: 0 - valLoss: 0.4316379725933075 - trainLoss: 0.41659867763519287\n",
      "cnt: 0 - valLoss: 0.4316371977329254 - trainLoss: 0.41659682989120483\n",
      "cnt: 0 - valLoss: 0.43163642287254333 - trainLoss: 0.4165949523448944\n",
      "cnt: 0 - valLoss: 0.43163564801216125 - trainLoss: 0.416593074798584\n",
      "cnt: 0 - valLoss: 0.43163493275642395 - trainLoss: 0.41659122705459595\n",
      "cnt: 0 - valLoss: 0.43163418769836426 - trainLoss: 0.4165893495082855\n",
      "cnt: 0 - valLoss: 0.4316334128379822 - trainLoss: 0.4165875017642975\n",
      "cnt: 0 - valLoss: 0.4316326379776001 - trainLoss: 0.41658565402030945\n",
      "cnt: 0 - valLoss: 0.4316318929195404 - trainLoss: 0.41658371686935425\n",
      "cnt: 0 - valLoss: 0.4316311180591583 - trainLoss: 0.4165818989276886\n",
      "cnt: 0 - valLoss: 0.43163034319877625 - trainLoss: 0.4165800213813782\n",
      "cnt: 0 - valLoss: 0.43162956833839417 - trainLoss: 0.41657811403274536\n",
      "cnt: 0 - valLoss: 0.43162888288497925 - trainLoss: 0.4165762960910797\n",
      "cnt: 0 - valLoss: 0.43162801861763 - trainLoss: 0.4165744483470917\n",
      "cnt: 0 - valLoss: 0.4316272437572479 - trainLoss: 0.41657257080078125\n",
      "cnt: 0 - valLoss: 0.4316265285015106 - trainLoss: 0.41657066345214844\n",
      "cnt: 0 - valLoss: 0.43162575364112854 - trainLoss: 0.4165688455104828\n",
      "cnt: 0 - valLoss: 0.43162500858306885 - trainLoss: 0.41656699776649475\n",
      "cnt: 0 - valLoss: 0.43162426352500916 - trainLoss: 0.4165651202201843\n",
      "cnt: 0 - valLoss: 0.4316234886646271 - trainLoss: 0.4165632426738739\n",
      "cnt: 0 - valLoss: 0.431622713804245 - trainLoss: 0.41656139492988586\n",
      "cnt: 0 - valLoss: 0.4316219687461853 - trainLoss: 0.4165595471858978\n",
      "cnt: 0 - valLoss: 0.4316211938858032 - trainLoss: 0.4165576696395874\n",
      "cnt: 0 - valLoss: 0.43162044882774353 - trainLoss: 0.416555792093277\n",
      "cnt: 0 - valLoss: 0.43161970376968384 - trainLoss: 0.41655394434928894\n",
      "cnt: 0 - valLoss: 0.43161892890930176 - trainLoss: 0.4165520668029785\n",
      "cnt: 0 - valLoss: 0.4316181540489197 - trainLoss: 0.4165502190589905\n",
      "cnt: 0 - valLoss: 0.4316174387931824 - trainLoss: 0.41654834151268005\n",
      "cnt: 0 - valLoss: 0.4316166937351227 - trainLoss: 0.416546493768692\n",
      "cnt: 0 - valLoss: 0.431615948677063 - trainLoss: 0.416544646024704\n",
      "cnt: 0 - valLoss: 0.4316151738166809 - trainLoss: 0.41654276847839355\n",
      "cnt: 0 - valLoss: 0.43161439895629883 - trainLoss: 0.4165409207344055\n",
      "cnt: 0 - valLoss: 0.43161365389823914 - trainLoss: 0.4165390133857727\n",
      "cnt: 0 - valLoss: 0.43161293864250183 - trainLoss: 0.41653719544410706\n",
      "cnt: 0 - valLoss: 0.43161216378211975 - trainLoss: 0.41653528809547424\n",
      "cnt: 0 - valLoss: 0.43161138892173767 - trainLoss: 0.4165334403514862\n",
      "cnt: 0 - valLoss: 0.4316106140613556 - trainLoss: 0.4165315628051758\n",
      "cnt: 0 - valLoss: 0.4316099286079407 - trainLoss: 0.41652971506118774\n",
      "cnt: 0 - valLoss: 0.4316091537475586 - trainLoss: 0.4165278673171997\n",
      "cnt: 0 - valLoss: 0.4316083788871765 - trainLoss: 0.4165259897708893\n",
      "cnt: 0 - valLoss: 0.4316076636314392 - trainLoss: 0.41652414202690125\n",
      "cnt: 0 - valLoss: 0.43160688877105713 - trainLoss: 0.4165223240852356\n",
      "cnt: 0 - valLoss: 0.4316061735153198 - trainLoss: 0.4165204167366028\n",
      "cnt: 0 - valLoss: 0.43160539865493774 - trainLoss: 0.41651853919029236\n",
      "cnt: 0 - valLoss: 0.43160465359687805 - trainLoss: 0.4165166914463043\n",
      "cnt: 0 - valLoss: 0.43160393834114075 - trainLoss: 0.41651487350463867\n",
      "cnt: 0 - valLoss: 0.43160316348075867 - trainLoss: 0.41651296615600586\n",
      "cnt: 0 - valLoss: 0.4316023886203766 - trainLoss: 0.41651108860969543\n",
      "cnt: 0 - valLoss: 0.4316016733646393 - trainLoss: 0.4165093004703522\n",
      "cnt: 0 - valLoss: 0.4316009283065796 - trainLoss: 0.41650742292404175\n",
      "cnt: 0 - valLoss: 0.4316001534461975 - trainLoss: 0.4165055453777313\n",
      "cnt: 0 - valLoss: 0.4315994381904602 - trainLoss: 0.4165036380290985\n",
      "cnt: 0 - valLoss: 0.4315986633300781 - trainLoss: 0.41650184988975525\n",
      "cnt: 0 - valLoss: 0.43159788846969604 - trainLoss: 0.4164999723434448\n",
      "cnt: 0 - valLoss: 0.43159717321395874 - trainLoss: 0.4164980947971344\n",
      "cnt: 0 - valLoss: 0.43159639835357666 - trainLoss: 0.41649624705314636\n",
      "cnt: 0 - valLoss: 0.43159565329551697 - trainLoss: 0.4164943993091583\n",
      "cnt: 0 - valLoss: 0.43159493803977966 - trainLoss: 0.4164925217628479\n",
      "cnt: 0 - valLoss: 0.4315941631793976 - trainLoss: 0.41649067401885986\n",
      "cnt: 0 - valLoss: 0.4315933883190155 - trainLoss: 0.41648879647254944\n",
      "cnt: 0 - valLoss: 0.4315927028656006 - trainLoss: 0.416486918926239\n",
      "cnt: 0 - valLoss: 0.4315919280052185 - trainLoss: 0.416485071182251\n",
      "cnt: 0 - valLoss: 0.4315911531448364 - trainLoss: 0.41648322343826294\n",
      "cnt: 0 - valLoss: 0.4315904378890991 - trainLoss: 0.4164813458919525\n",
      "cnt: 0 - valLoss: 0.43158966302871704 - trainLoss: 0.4164794981479645\n",
      "cnt: 0 - valLoss: 0.43158894777297974 - trainLoss: 0.41647762060165405\n",
      "cnt: 0 - valLoss: 0.43158817291259766 - trainLoss: 0.416475772857666\n",
      "cnt: 0 - valLoss: 0.43158742785453796 - trainLoss: 0.41647395491600037\n",
      "cnt: 0 - valLoss: 0.43158671259880066 - trainLoss: 0.41647204756736755\n",
      "cnt: 0 - valLoss: 0.4315859377384186 - trainLoss: 0.4164702296257019\n",
      "cnt: 0 - valLoss: 0.4315851926803589 - trainLoss: 0.4164683520793915\n",
      "cnt: 0 - valLoss: 0.4315844774246216 - trainLoss: 0.41646650433540344\n",
      "cnt: 0 - valLoss: 0.4315837025642395 - trainLoss: 0.416464626789093\n",
      "cnt: 0 - valLoss: 0.4315829277038574 - trainLoss: 0.4164627194404602\n",
      "cnt: 0 - valLoss: 0.43158215284347534 - trainLoss: 0.41646093130111694\n",
      "cnt: 0 - valLoss: 0.4315814673900604 - trainLoss: 0.4164590537548065\n",
      "cnt: 0 - valLoss: 0.43158069252967834 - trainLoss: 0.4164572060108185\n",
      "cnt: 0 - valLoss: 0.43157997727394104 - trainLoss: 0.41645532846450806\n",
      "cnt: 0 - valLoss: 0.43157920241355896 - trainLoss: 0.41645348072052\n",
      "cnt: 0 - valLoss: 0.43157848715782166 - trainLoss: 0.4164516031742096\n",
      "cnt: 0 - valLoss: 0.4315777122974396 - trainLoss: 0.41644975543022156\n",
      "cnt: 0 - valLoss: 0.4315769672393799 - trainLoss: 0.4164479076862335\n",
      "cnt: 0 - valLoss: 0.4315761923789978 - trainLoss: 0.4164460301399231\n",
      "cnt: 0 - valLoss: 0.4315754771232605 - trainLoss: 0.41644421219825745\n",
      "cnt: 0 - valLoss: 0.4315747022628784 - trainLoss: 0.41644230484962463\n",
      "cnt: 0 - valLoss: 0.4315739870071411 - trainLoss: 0.416440486907959\n",
      "cnt: 0 - valLoss: 0.43157321214675903 - trainLoss: 0.41643857955932617\n",
      "cnt: 0 - valLoss: 0.43157246708869934 - trainLoss: 0.4164367616176605\n",
      "cnt: 0 - valLoss: 0.43157169222831726 - trainLoss: 0.4164349138736725\n",
      "cnt: 0 - valLoss: 0.43157097697257996 - trainLoss: 0.41643303632736206\n",
      "cnt: 0 - valLoss: 0.43157023191452026 - trainLoss: 0.41643112897872925\n",
      "cnt: 0 - valLoss: 0.43156951665878296 - trainLoss: 0.4164293110370636\n",
      "cnt: 0 - valLoss: 0.4315687119960785 - trainLoss: 0.41642746329307556\n",
      "cnt: 0 - valLoss: 0.4315680265426636 - trainLoss: 0.4164256155490875\n",
      "cnt: 0 - valLoss: 0.4315672516822815 - trainLoss: 0.4164237380027771\n",
      "cnt: 0 - valLoss: 0.4315664768218994 - trainLoss: 0.4164218604564667\n",
      "cnt: 0 - valLoss: 0.4315657317638397 - trainLoss: 0.41642001271247864\n",
      "cnt: 0 - valLoss: 0.4315650165081024 - trainLoss: 0.4164181649684906\n",
      "cnt: 0 - valLoss: 0.43156424164772034 - trainLoss: 0.41641634702682495\n",
      "cnt: 0 - valLoss: 0.43156346678733826 - trainLoss: 0.41641443967819214\n",
      "cnt: 0 - valLoss: 0.43156275153160095 - trainLoss: 0.4164125621318817\n",
      "cnt: 0 - valLoss: 0.4315619170665741 - trainLoss: 0.4164107143878937\n",
      "cnt: 0 - valLoss: 0.4315611720085144 - trainLoss: 0.416408896446228\n",
      "cnt: 0 - valLoss: 0.4315603971481323 - trainLoss: 0.4164069890975952\n",
      "cnt: 0 - valLoss: 0.43155956268310547 - trainLoss: 0.4164051115512848\n",
      "cnt: 0 - valLoss: 0.431558758020401 - trainLoss: 0.41640329360961914\n",
      "cnt: 0 - valLoss: 0.4315579831600189 - trainLoss: 0.4164014458656311\n",
      "cnt: 0 - valLoss: 0.43155720829963684 - trainLoss: 0.41639959812164307\n",
      "cnt: 0 - valLoss: 0.43155643343925476 - trainLoss: 0.41639775037765503\n",
      "cnt: 0 - valLoss: 0.4315556585788727 - trainLoss: 0.4163958728313446\n",
      "cnt: 0 - valLoss: 0.431554913520813 - trainLoss: 0.41639402508735657\n",
      "cnt: 0 - valLoss: 0.4315541386604309 - trainLoss: 0.4163922071456909\n",
      "cnt: 0 - valLoss: 0.43155330419540405 - trainLoss: 0.4163902997970581\n",
      "cnt: 0 - valLoss: 0.4315524995326996 - trainLoss: 0.41638848185539246\n",
      "cnt: 0 - valLoss: 0.4315517842769623 - trainLoss: 0.41638657450675964\n",
      "cnt: 0 - valLoss: 0.4315510094165802 - trainLoss: 0.416384756565094\n",
      "cnt: 0 - valLoss: 0.43155017495155334 - trainLoss: 0.4163828492164612\n",
      "cnt: 0 - valLoss: 0.43154942989349365 - trainLoss: 0.41638103127479553\n",
      "cnt: 0 - valLoss: 0.4315486550331116 - trainLoss: 0.4163791835308075\n",
      "cnt: 0 - valLoss: 0.4315478801727295 - trainLoss: 0.41637730598449707\n",
      "cnt: 0 - valLoss: 0.43154704570770264 - trainLoss: 0.41637545824050903\n",
      "cnt: 0 - valLoss: 0.43154630064964294 - trainLoss: 0.416373610496521\n",
      "cnt: 0 - valLoss: 0.43154552578926086 - trainLoss: 0.41637173295021057\n",
      "cnt: 0 - valLoss: 0.4315447509288788 - trainLoss: 0.41636988520622253\n",
      "cnt: 0 - valLoss: 0.43154391646385193 - trainLoss: 0.4163680672645569\n",
      "cnt: 0 - valLoss: 0.43154317140579224 - trainLoss: 0.4163661599159241\n",
      "cnt: 0 - valLoss: 0.43154239654541016 - trainLoss: 0.4163643419742584\n",
      "cnt: 0 - valLoss: 0.4315416216850281 - trainLoss: 0.4163624346256256\n",
      "cnt: 0 - valLoss: 0.431540846824646 - trainLoss: 0.41636061668395996\n",
      "cnt: 0 - valLoss: 0.43154004216194153 - trainLoss: 0.4163587689399719\n",
      "cnt: 0 - valLoss: 0.43153926730155945 - trainLoss: 0.4163569211959839\n",
      "cnt: 0 - valLoss: 0.43153849244117737 - trainLoss: 0.41635507345199585\n",
      "cnt: 0 - valLoss: 0.4315376579761505 - trainLoss: 0.4163531959056854\n",
      "cnt: 0 - valLoss: 0.4315369129180908 - trainLoss: 0.4163513481616974\n",
      "cnt: 0 - valLoss: 0.43153613805770874 - trainLoss: 0.41634947061538696\n",
      "cnt: 0 - valLoss: 0.43153536319732666 - trainLoss: 0.41634759306907654\n",
      "cnt: 0 - valLoss: 0.4315345883369446 - trainLoss: 0.4163458049297333\n",
      "cnt: 0 - valLoss: 0.4315337836742401 - trainLoss: 0.41634392738342285\n",
      "cnt: 0 - valLoss: 0.43153300881385803 - trainLoss: 0.4163420796394348\n",
      "cnt: 0 - valLoss: 0.4315321743488312 - trainLoss: 0.4163402318954468\n",
      "cnt: 0 - valLoss: 0.4315313994884491 - trainLoss: 0.41633835434913635\n",
      "cnt: 0 - valLoss: 0.4315306544303894 - trainLoss: 0.4163365066051483\n",
      "cnt: 0 - valLoss: 0.4315298795700073 - trainLoss: 0.4163346290588379\n",
      "cnt: 0 - valLoss: 0.43152904510498047 - trainLoss: 0.41633278131484985\n",
      "cnt: 0 - valLoss: 0.43152832984924316 - trainLoss: 0.4163309335708618\n",
      "cnt: 0 - valLoss: 0.4315275251865387 - trainLoss: 0.4163290560245514\n",
      "cnt: 0 - valLoss: 0.4315267503261566 - trainLoss: 0.41632720828056335\n",
      "cnt: 0 - valLoss: 0.43152591586112976 - trainLoss: 0.41632533073425293\n",
      "cnt: 0 - valLoss: 0.4315251410007477 - trainLoss: 0.41632354259490967\n",
      "cnt: 0 - valLoss: 0.431524395942688 - trainLoss: 0.41632166504859924\n",
      "cnt: 0 - valLoss: 0.4315236210823059 - trainLoss: 0.4163198173046112\n",
      "cnt: 0 - valLoss: 0.43152284622192383 - trainLoss: 0.41631796956062317\n",
      "cnt: 0 - valLoss: 0.43152207136154175 - trainLoss: 0.41631609201431274\n",
      "cnt: 0 - valLoss: 0.4315212666988373 - trainLoss: 0.4163143038749695\n",
      "cnt: 0 - valLoss: 0.4315204918384552 - trainLoss: 0.41631242632865906\n",
      "cnt: 0 - valLoss: 0.43151965737342834 - trainLoss: 0.41631051898002625\n",
      "cnt: 0 - valLoss: 0.43151891231536865 - trainLoss: 0.4163087010383606\n",
      "cnt: 0 - valLoss: 0.43151816725730896 - trainLoss: 0.4163067936897278\n",
      "cnt: 0 - valLoss: 0.4315173625946045 - trainLoss: 0.41630497574806213\n",
      "cnt: 0 - valLoss: 0.4315165877342224 - trainLoss: 0.4163031578063965\n",
      "cnt: 0 - valLoss: 0.43151581287384033 - trainLoss: 0.41630128026008606\n",
      "cnt: 0 - valLoss: 0.43151503801345825 - trainLoss: 0.416299432516098\n",
      "cnt: 0 - valLoss: 0.43151426315307617 - trainLoss: 0.4162976145744324\n",
      "cnt: 0 - valLoss: 0.4315134882926941 - trainLoss: 0.41629570722579956\n",
      "cnt: 0 - valLoss: 0.4315126836299896 - trainLoss: 0.4162938892841339\n",
      "cnt: 0 - valLoss: 0.43151190876960754 - trainLoss: 0.4162920415401459\n",
      "cnt: 0 - valLoss: 0.43151113390922546 - trainLoss: 0.41629016399383545\n",
      "cnt: 0 - valLoss: 0.43151041865348816 - trainLoss: 0.4162883460521698\n",
      "cnt: 0 - valLoss: 0.4315095841884613 - trainLoss: 0.4162864685058594\n",
      "cnt: 0 - valLoss: 0.4315088391304016 - trainLoss: 0.41628462076187134\n",
      "cnt: 0 - valLoss: 0.43150806427001953 - trainLoss: 0.4162828028202057\n",
      "cnt: 0 - valLoss: 0.43150728940963745 - trainLoss: 0.4162808954715729\n",
      "cnt: 0 - valLoss: 0.43150654435157776 - trainLoss: 0.41627901792526245\n",
      "cnt: 0 - valLoss: 0.4315057694911957 - trainLoss: 0.4162772297859192\n",
      "cnt: 0 - valLoss: 0.4315049648284912 - trainLoss: 0.41627535223960876\n",
      "cnt: 0 - valLoss: 0.4315042495727539 - trainLoss: 0.4162735342979431\n",
      "cnt: 0 - valLoss: 0.4315034747123718 - trainLoss: 0.4162716567516327\n",
      "cnt: 0 - valLoss: 0.43150269985198975 - trainLoss: 0.41626977920532227\n",
      "cnt: 0 - valLoss: 0.43150192499160767 - trainLoss: 0.416267991065979\n",
      "cnt: 0 - valLoss: 0.4315011501312256 - trainLoss: 0.4162661135196686\n",
      "cnt: 0 - valLoss: 0.4315003752708435 - trainLoss: 0.41626426577568054\n",
      "cnt: 0 - valLoss: 0.4314996004104614 - trainLoss: 0.4162624180316925\n",
      "cnt: 0 - valLoss: 0.43149882555007935 - trainLoss: 0.4162605404853821\n",
      "cnt: 0 - valLoss: 0.43149808049201965 - trainLoss: 0.41625869274139404\n",
      "cnt: 0 - valLoss: 0.4314973056316376 - trainLoss: 0.416256844997406\n",
      "cnt: 0 - valLoss: 0.4314965307712555 - trainLoss: 0.4162549674510956\n",
      "cnt: 0 - valLoss: 0.4314957559108734 - trainLoss: 0.4162531793117523\n",
      "cnt: 0 - valLoss: 0.43149498105049133 - trainLoss: 0.4162513017654419\n",
      "cnt: 0 - valLoss: 0.43149420619010925 - trainLoss: 0.4162493944168091\n",
      "cnt: 0 - valLoss: 0.4314934313297272 - trainLoss: 0.4162476062774658\n",
      "cnt: 0 - valLoss: 0.4314926564693451 - trainLoss: 0.4162457287311554\n",
      "cnt: 0 - valLoss: 0.431491881608963 - trainLoss: 0.41624388098716736\n",
      "cnt: 0 - valLoss: 0.4314911365509033 - trainLoss: 0.4162420332431793\n",
      "cnt: 0 - valLoss: 0.43149030208587646 - trainLoss: 0.41624021530151367\n",
      "cnt: 0 - valLoss: 0.43148958683013916 - trainLoss: 0.41623836755752563\n",
      "cnt: 0 - valLoss: 0.4314888119697571 - trainLoss: 0.4162365198135376\n",
      "cnt: 0 - valLoss: 0.431488037109375 - trainLoss: 0.41623467206954956\n",
      "cnt: 0 - valLoss: 0.4314872622489929 - trainLoss: 0.41623276472091675\n",
      "cnt: 0 - valLoss: 0.43148648738861084 - trainLoss: 0.4162309467792511\n",
      "cnt: 0 - valLoss: 0.43148571252822876 - trainLoss: 0.41622912883758545\n",
      "cnt: 0 - valLoss: 0.4314849376678467 - trainLoss: 0.41622722148895264\n",
      "cnt: 0 - valLoss: 0.431484192609787 - trainLoss: 0.416225403547287\n",
      "cnt: 0 - valLoss: 0.4314834177494049 - trainLoss: 0.41622355580329895\n",
      "cnt: 0 - valLoss: 0.4314826428890228 - trainLoss: 0.4162216782569885\n",
      "cnt: 0 - valLoss: 0.43148186802864075 - trainLoss: 0.4162198603153229\n",
      "cnt: 0 - valLoss: 0.43148109316825867 - trainLoss: 0.41621798276901245\n",
      "cnt: 0 - valLoss: 0.4314803183078766 - trainLoss: 0.4162161946296692\n",
      "cnt: 0 - valLoss: 0.4314795434474945 - trainLoss: 0.41621431708335876\n",
      "cnt: 0 - valLoss: 0.4314787685871124 - trainLoss: 0.41621243953704834\n",
      "cnt: 0 - valLoss: 0.43147799372673035 - trainLoss: 0.4162106215953827\n",
      "cnt: 0 - valLoss: 0.43147724866867065 - trainLoss: 0.41620874404907227\n",
      "cnt: 0 - valLoss: 0.4314764738082886 - trainLoss: 0.41620689630508423\n",
      "cnt: 0 - valLoss: 0.4314756989479065 - trainLoss: 0.4162050783634186\n",
      "cnt: 0 - valLoss: 0.4314749240875244 - trainLoss: 0.41620317101478577\n",
      "cnt: 0 - valLoss: 0.43147414922714233 - trainLoss: 0.4162013828754425\n",
      "cnt: 0 - valLoss: 0.43147343397140503 - trainLoss: 0.4161995053291321\n",
      "cnt: 0 - valLoss: 0.4314725995063782 - trainLoss: 0.41619765758514404\n",
      "cnt: 0 - valLoss: 0.4314718246459961 - trainLoss: 0.416195809841156\n",
      "cnt: 0 - valLoss: 0.431471049785614 - trainLoss: 0.4161939322948456\n",
      "cnt: 0 - valLoss: 0.4314703047275543 - trainLoss: 0.4161921441555023\n",
      "cnt: 0 - valLoss: 0.43146952986717224 - trainLoss: 0.4161902666091919\n",
      "cnt: 0 - valLoss: 0.43146875500679016 - trainLoss: 0.41618841886520386\n",
      "cnt: 0 - valLoss: 0.43146800994873047 - trainLoss: 0.41618654131889343\n",
      "cnt: 0 - valLoss: 0.4314672350883484 - trainLoss: 0.4161847233772278\n",
      "cnt: 0 - valLoss: 0.4314664304256439 - trainLoss: 0.41618290543556213\n",
      "cnt: 0 - valLoss: 0.4314657151699066 - trainLoss: 0.4161809980869293\n",
      "cnt: 0 - valLoss: 0.43146494030952454 - trainLoss: 0.41617918014526367\n",
      "cnt: 0 - valLoss: 0.4314641058444977 - trainLoss: 0.41617733240127563\n",
      "cnt: 0 - valLoss: 0.4314633905887604 - trainLoss: 0.4161754846572876\n",
      "cnt: 0 - valLoss: 0.4314625859260559 - trainLoss: 0.4161736071109772\n",
      "cnt: 0 - valLoss: 0.43146181106567383 - trainLoss: 0.41617175936698914\n",
      "cnt: 0 - valLoss: 0.43146103620529175 - trainLoss: 0.4161699116230011\n",
      "cnt: 0 - valLoss: 0.43146029114723206 - trainLoss: 0.41616809368133545\n",
      "cnt: 0 - valLoss: 0.43145954608917236 - trainLoss: 0.4161662459373474\n",
      "cnt: 0 - valLoss: 0.4314587712287903 - trainLoss: 0.4161643981933594\n",
      "cnt: 0 - valLoss: 0.4314579367637634 - trainLoss: 0.41616255044937134\n",
      "cnt: 0 - valLoss: 0.43145716190338135 - trainLoss: 0.4161607325077057\n",
      "cnt: 0 - valLoss: 0.43145644664764404 - trainLoss: 0.4161588251590729\n",
      "cnt: 0 - valLoss: 0.4314556419849396 - trainLoss: 0.4161570072174072\n",
      "cnt: 0 - valLoss: 0.4314548671245575 - trainLoss: 0.4161551594734192\n",
      "cnt: 0 - valLoss: 0.4314541220664978 - trainLoss: 0.41615331172943115\n",
      "cnt: 0 - valLoss: 0.4314533472061157 - trainLoss: 0.4161514639854431\n",
      "cnt: 0 - valLoss: 0.43145260214805603 - trainLoss: 0.4161495864391327\n",
      "cnt: 0 - valLoss: 0.43145182728767395 - trainLoss: 0.41614770889282227\n",
      "cnt: 0 - valLoss: 0.43145105242729187 - trainLoss: 0.416145920753479\n",
      "cnt: 0 - valLoss: 0.4314502775669098 - trainLoss: 0.41614407300949097\n",
      "cnt: 0 - valLoss: 0.4314495325088501 - trainLoss: 0.41614222526550293\n",
      "cnt: 0 - valLoss: 0.4314487874507904 - trainLoss: 0.4161403477191925\n",
      "cnt: 0 - valLoss: 0.4314480125904083 - trainLoss: 0.41613855957984924\n",
      "cnt: 0 - valLoss: 0.43144723773002625 - trainLoss: 0.4161366820335388\n",
      "cnt: 0 - valLoss: 0.43144646286964417 - trainLoss: 0.416134774684906\n",
      "cnt: 0 - valLoss: 0.4314456880092621 - trainLoss: 0.41613298654556274\n",
      "cnt: 0 - valLoss: 0.43144491314888 - trainLoss: 0.4161311089992523\n",
      "cnt: 0 - valLoss: 0.4314441382884979 - trainLoss: 0.4161292612552643\n",
      "cnt: 0 - valLoss: 0.43144336342811584 - trainLoss: 0.41612741351127625\n",
      "cnt: 0 - valLoss: 0.43144261837005615 - trainLoss: 0.4161255955696106\n",
      "cnt: 0 - valLoss: 0.4314418435096741 - trainLoss: 0.41612374782562256\n",
      "cnt: 0 - valLoss: 0.4314410984516144 - trainLoss: 0.4161219000816345\n",
      "cnt: 0 - valLoss: 0.4314403235912323 - trainLoss: 0.4161200225353241\n",
      "cnt: 0 - valLoss: 0.4314395487308502 - trainLoss: 0.41611823439598083\n",
      "cnt: 0 - valLoss: 0.4314388036727905 - trainLoss: 0.416116327047348\n",
      "cnt: 0 - valLoss: 0.43143802881240845 - trainLoss: 0.4161145091056824\n",
      "cnt: 0 - valLoss: 0.43143728375434875 - trainLoss: 0.41611266136169434\n",
      "cnt: 0 - valLoss: 0.4314365088939667 - trainLoss: 0.4161108136177063\n",
      "cnt: 0 - valLoss: 0.4314357340335846 - trainLoss: 0.4161089360713959\n",
      "cnt: 0 - valLoss: 0.4314349293708801 - trainLoss: 0.41610708832740784\n",
      "cnt: 0 - valLoss: 0.4314342141151428 - trainLoss: 0.4161052405834198\n",
      "cnt: 0 - valLoss: 0.43143343925476074 - trainLoss: 0.41610342264175415\n",
      "cnt: 0 - valLoss: 0.43143266439437866 - trainLoss: 0.4161015748977661\n",
      "cnt: 0 - valLoss: 0.4314318895339966 - trainLoss: 0.4160997271537781\n",
      "cnt: 0 - valLoss: 0.4314311146736145 - trainLoss: 0.41609787940979004\n",
      "cnt: 0 - valLoss: 0.4314303398132324 - trainLoss: 0.4160960018634796\n",
      "cnt: 0 - valLoss: 0.43142956495285034 - trainLoss: 0.41609421372413635\n",
      "cnt: 0 - valLoss: 0.43142884969711304 - trainLoss: 0.4160923361778259\n",
      "cnt: 0 - valLoss: 0.43142807483673096 - trainLoss: 0.4160904884338379\n",
      "cnt: 0 - valLoss: 0.4314272701740265 - trainLoss: 0.41608864068984985\n",
      "cnt: 0 - valLoss: 0.4314265251159668 - trainLoss: 0.4160867929458618\n",
      "cnt: 0 - valLoss: 0.4314257502555847 - trainLoss: 0.41608497500419617\n",
      "cnt: 0 - valLoss: 0.43142497539520264 - trainLoss: 0.41608306765556335\n",
      "cnt: 0 - valLoss: 0.43142426013946533 - trainLoss: 0.4160812795162201\n",
      "cnt: 0 - valLoss: 0.43142348527908325 - trainLoss: 0.41607940196990967\n",
      "cnt: 0 - valLoss: 0.43142271041870117 - trainLoss: 0.41607755422592163\n",
      "cnt: 0 - valLoss: 0.4314219355583191 - trainLoss: 0.4160757064819336\n",
      "cnt: 0 - valLoss: 0.431421160697937 - trainLoss: 0.41607391834259033\n",
      "cnt: 0 - valLoss: 0.4314204156398773 - trainLoss: 0.4160720407962799\n",
      "cnt: 0 - valLoss: 0.4314196705818176 - trainLoss: 0.41607019305229187\n",
      "cnt: 0 - valLoss: 0.43141886591911316 - trainLoss: 0.41606834530830383\n",
      "cnt: 0 - valLoss: 0.43141812086105347 - trainLoss: 0.4160665273666382\n",
      "cnt: 0 - valLoss: 0.4314173460006714 - trainLoss: 0.41606467962265015\n",
      "cnt: 0 - valLoss: 0.4314166009426117 - trainLoss: 0.4160628020763397\n",
      "cnt: 0 - valLoss: 0.4314158260822296 - trainLoss: 0.4160609841346741\n",
      "cnt: 0 - valLoss: 0.43141505122184753 - trainLoss: 0.41605910658836365\n",
      "cnt: 0 - valLoss: 0.43141430616378784 - trainLoss: 0.4160573184490204\n",
      "cnt: 0 - valLoss: 0.43141353130340576 - trainLoss: 0.41605544090270996\n",
      "cnt: 0 - valLoss: 0.43141278624534607 - trainLoss: 0.4160535931587219\n",
      "cnt: 0 - valLoss: 0.431412011384964 - trainLoss: 0.4160517454147339\n",
      "cnt: 0 - valLoss: 0.4314112365245819 - trainLoss: 0.41604989767074585\n",
      "cnt: 0 - valLoss: 0.4314104914665222 - trainLoss: 0.4160480797290802\n",
      "cnt: 0 - valLoss: 0.43140968680381775 - trainLoss: 0.41604623198509216\n",
      "cnt: 0 - valLoss: 0.43140897154808044 - trainLoss: 0.4160443842411041\n",
      "cnt: 0 - valLoss: 0.43140819668769836 - trainLoss: 0.4160425066947937\n",
      "cnt: 0 - valLoss: 0.4314074218273163 - trainLoss: 0.41604065895080566\n",
      "cnt: 0 - valLoss: 0.4314066767692566 - trainLoss: 0.4160388112068176\n",
      "cnt: 0 - valLoss: 0.4314058721065521 - trainLoss: 0.416036993265152\n",
      "cnt: 0 - valLoss: 0.4314051568508148 - trainLoss: 0.41603514552116394\n",
      "cnt: 0 - valLoss: 0.43140432238578796 - trainLoss: 0.4160332977771759\n",
      "cnt: 0 - valLoss: 0.43140360713005066 - trainLoss: 0.41603145003318787\n",
      "cnt: 0 - valLoss: 0.4314028322696686 - trainLoss: 0.41602957248687744\n",
      "cnt: 0 - valLoss: 0.4314020872116089 - trainLoss: 0.4160277247428894\n",
      "cnt: 0 - valLoss: 0.4314013421535492 - trainLoss: 0.41602590680122375\n",
      "cnt: 0 - valLoss: 0.4314005970954895 - trainLoss: 0.4160240888595581\n",
      "cnt: 0 - valLoss: 0.4313998222351074 - trainLoss: 0.4160222113132477\n",
      "cnt: 0 - valLoss: 0.43139904737472534 - trainLoss: 0.41602036356925964\n",
      "cnt: 0 - valLoss: 0.43139827251434326 - trainLoss: 0.416018545627594\n",
      "cnt: 0 - valLoss: 0.43139755725860596 - trainLoss: 0.41601669788360596\n",
      "cnt: 0 - valLoss: 0.4313967823982239 - trainLoss: 0.4160148501396179\n",
      "cnt: 0 - valLoss: 0.4313960671424866 - trainLoss: 0.4160130023956299\n",
      "cnt: 0 - valLoss: 0.4313952922821045 - trainLoss: 0.41601118445396423\n",
      "cnt: 0 - valLoss: 0.4313945472240448 - trainLoss: 0.4160093665122986\n",
      "cnt: 0 - valLoss: 0.43139374256134033 - trainLoss: 0.41600748896598816\n",
      "cnt: 0 - valLoss: 0.4313930571079254 - trainLoss: 0.41600561141967773\n",
      "cnt: 0 - valLoss: 0.43139228224754333 - trainLoss: 0.4160038232803345\n",
      "cnt: 0 - valLoss: 0.43139150738716125 - trainLoss: 0.41600191593170166\n",
      "cnt: 0 - valLoss: 0.4313907325267792 - trainLoss: 0.416000097990036\n",
      "cnt: 0 - valLoss: 0.4313899576663971 - trainLoss: 0.415998250246048\n",
      "cnt: 0 - valLoss: 0.4313892722129822 - trainLoss: 0.4159964621067047\n",
      "cnt: 0 - valLoss: 0.4313884973526001 - trainLoss: 0.4159945547580719\n",
      "cnt: 0 - valLoss: 0.4313877522945404 - trainLoss: 0.41599276661872864\n",
      "cnt: 0 - valLoss: 0.4313869774341583 - trainLoss: 0.4159909188747406\n",
      "cnt: 0 - valLoss: 0.43138623237609863 - trainLoss: 0.41598910093307495\n",
      "cnt: 0 - valLoss: 0.43138545751571655 - trainLoss: 0.41598719358444214\n",
      "cnt: 0 - valLoss: 0.43138474225997925 - trainLoss: 0.4159854054450989\n",
      "cnt: 0 - valLoss: 0.43138396739959717 - trainLoss: 0.41598352789878845\n",
      "cnt: 0 - valLoss: 0.43138325214385986 - trainLoss: 0.4159817397594452\n",
      "cnt: 0 - valLoss: 0.4313824772834778 - trainLoss: 0.4159798324108124\n",
      "cnt: 0 - valLoss: 0.4313817024230957 - trainLoss: 0.4159780442714691\n",
      "cnt: 0 - valLoss: 0.431380957365036 - trainLoss: 0.4159761667251587\n",
      "cnt: 0 - valLoss: 0.43138018250465393 - trainLoss: 0.41597437858581543\n",
      "cnt: 0 - valLoss: 0.43137943744659424 - trainLoss: 0.4159724712371826\n",
      "cnt: 0 - valLoss: 0.4313787519931793 - trainLoss: 0.41597068309783936\n",
      "cnt: 0 - valLoss: 0.43137797713279724 - trainLoss: 0.41596880555152893\n",
      "cnt: 0 - valLoss: 0.43137720227241516 - trainLoss: 0.4159669876098633\n",
      "cnt: 0 - valLoss: 0.43137645721435547 - trainLoss: 0.41596511006355286\n",
      "cnt: 0 - valLoss: 0.431375652551651 - trainLoss: 0.4159633219242096\n",
      "cnt: 0 - valLoss: 0.4313749372959137 - trainLoss: 0.41596144437789917\n",
      "cnt: 0 - valLoss: 0.4313741624355316 - trainLoss: 0.41595959663391113\n",
      "cnt: 0 - valLoss: 0.43137338757514954 - trainLoss: 0.4159577488899231\n",
      "cnt: 0 - valLoss: 0.43137264251708984 - trainLoss: 0.41595596075057983\n",
      "cnt: 0 - valLoss: 0.43137192726135254 - trainLoss: 0.4159541130065918\n",
      "cnt: 0 - valLoss: 0.43137115240097046 - trainLoss: 0.41595223546028137\n",
      "cnt: 0 - valLoss: 0.4313703775405884 - trainLoss: 0.4159504473209381\n",
      "cnt: 0 - valLoss: 0.4313696622848511 - trainLoss: 0.4159485995769501\n",
      "cnt: 0 - valLoss: 0.431368887424469 - trainLoss: 0.41594672203063965\n",
      "cnt: 0 - valLoss: 0.4313681125640869 - trainLoss: 0.415944904088974\n",
      "cnt: 0 - valLoss: 0.4313673675060272 - trainLoss: 0.41594308614730835\n",
      "cnt: 0 - valLoss: 0.43136662244796753 - trainLoss: 0.4159412384033203\n",
      "cnt: 0 - valLoss: 0.43136587738990784 - trainLoss: 0.4159393608570099\n",
      "cnt: 0 - valLoss: 0.43136510252952576 - trainLoss: 0.41593754291534424\n",
      "cnt: 0 - valLoss: 0.4313643276691437 - trainLoss: 0.4159357249736786\n",
      "cnt: 0 - valLoss: 0.43136361241340637 - trainLoss: 0.41593387722969055\n",
      "cnt: 0 - valLoss: 0.4313628673553467 - trainLoss: 0.4159320294857025\n",
      "cnt: 0 - valLoss: 0.431362122297287 - trainLoss: 0.4159301817417145\n",
      "cnt: 0 - valLoss: 0.4313613474369049 - trainLoss: 0.41592836380004883\n",
      "cnt: 0 - valLoss: 0.4313606023788452 - trainLoss: 0.4159265160560608\n",
      "cnt: 0 - valLoss: 0.4313598871231079 - trainLoss: 0.41592466831207275\n",
      "cnt: 0 - valLoss: 0.43135911226272583 - trainLoss: 0.4159228205680847\n",
      "cnt: 0 - valLoss: 0.43135833740234375 - trainLoss: 0.41592100262641907\n",
      "cnt: 0 - valLoss: 0.43135762214660645 - trainLoss: 0.41591915488243103\n",
      "cnt: 0 - valLoss: 0.43135687708854675 - trainLoss: 0.415917307138443\n",
      "cnt: 0 - valLoss: 0.4313561022281647 - trainLoss: 0.41591545939445496\n",
      "cnt: 0 - valLoss: 0.4313553273677826 - trainLoss: 0.4159136712551117\n",
      "cnt: 0 - valLoss: 0.4313546121120453 - trainLoss: 0.41591179370880127\n",
      "cnt: 0 - valLoss: 0.4313538670539856 - trainLoss: 0.41590994596481323\n",
      "cnt: 0 - valLoss: 0.4313531517982483 - trainLoss: 0.4159080982208252\n",
      "cnt: 0 - valLoss: 0.431352436542511 - trainLoss: 0.41590631008148193\n",
      "cnt: 0 - valLoss: 0.4313516914844513 - trainLoss: 0.4159044325351715\n",
      "cnt: 0 - valLoss: 0.431350976228714 - trainLoss: 0.41590258479118347\n",
      "cnt: 0 - valLoss: 0.4313502907752991 - trainLoss: 0.4159007966518402\n",
      "cnt: 0 - valLoss: 0.431349515914917 - trainLoss: 0.4158989489078522\n",
      "cnt: 0 - valLoss: 0.4313488304615021 - trainLoss: 0.41589710116386414\n",
      "cnt: 0 - valLoss: 0.43134814500808716 - trainLoss: 0.4158952534198761\n",
      "cnt: 0 - valLoss: 0.4313473701477051 - trainLoss: 0.41589343547821045\n",
      "cnt: 0 - valLoss: 0.43134668469429016 - trainLoss: 0.4158915877342224\n",
      "cnt: 0 - valLoss: 0.43134593963623047 - trainLoss: 0.4158897399902344\n",
      "cnt: 0 - valLoss: 0.43134522438049316 - trainLoss: 0.41588789224624634\n",
      "cnt: 0 - valLoss: 0.43134453892707825 - trainLoss: 0.4158860743045807\n",
      "cnt: 0 - valLoss: 0.43134379386901855 - trainLoss: 0.41588422656059265\n",
      "cnt: 0 - valLoss: 0.43134307861328125 - trainLoss: 0.4158823788166046\n",
      "cnt: 0 - valLoss: 0.43134236335754395 - trainLoss: 0.4158805310726166\n",
      "cnt: 0 - valLoss: 0.43134167790412903 - trainLoss: 0.4158787429332733\n",
      "cnt: 0 - valLoss: 0.43134093284606934 - trainLoss: 0.4158768653869629\n",
      "cnt: 0 - valLoss: 0.43134021759033203 - trainLoss: 0.41587507724761963\n",
      "cnt: 0 - valLoss: 0.4313395321369171 - trainLoss: 0.4158732295036316\n",
      "cnt: 0 - valLoss: 0.4313387870788574 - trainLoss: 0.41587138175964355\n",
      "cnt: 0 - valLoss: 0.4313380718231201 - trainLoss: 0.4158695340156555\n",
      "cnt: 0 - valLoss: 0.4313373863697052 - trainLoss: 0.41586771607398987\n",
      "cnt: 0 - valLoss: 0.4313366413116455 - trainLoss: 0.4158658981323242\n",
      "cnt: 0 - valLoss: 0.4313359260559082 - trainLoss: 0.4158640205860138\n",
      "cnt: 0 - valLoss: 0.4313351809978485 - trainLoss: 0.41586217284202576\n",
      "cnt: 0 - valLoss: 0.4313344657421112 - trainLoss: 0.4158603549003601\n",
      "cnt: 0 - valLoss: 0.4313337802886963 - trainLoss: 0.41585853695869446\n",
      "cnt: 0 - valLoss: 0.4313330352306366 - trainLoss: 0.41585665941238403\n",
      "cnt: 0 - valLoss: 0.4313322901725769 - trainLoss: 0.41585487127304077\n",
      "cnt: 0 - valLoss: 0.431331604719162 - trainLoss: 0.41585302352905273\n",
      "cnt: 0 - valLoss: 0.43133091926574707 - trainLoss: 0.4158511757850647\n",
      "cnt: 0 - valLoss: 0.4313300848007202 - trainLoss: 0.41584932804107666\n",
      "cnt: 0 - valLoss: 0.43132928013801575 - trainLoss: 0.415847510099411\n",
      "cnt: 0 - valLoss: 0.43132850527763367 - trainLoss: 0.41584569215774536\n",
      "cnt: 0 - valLoss: 0.4313276410102844 - trainLoss: 0.41584381461143494\n",
      "cnt: 0 - valLoss: 0.43132686614990234 - trainLoss: 0.4158420264720917\n",
      "cnt: 0 - valLoss: 0.4313260614871979 - trainLoss: 0.41584017872810364\n",
      "cnt: 0 - valLoss: 0.4313252568244934 - trainLoss: 0.4158383309841156\n",
      "cnt: 0 - valLoss: 0.43132448196411133 - trainLoss: 0.41583654284477234\n",
      "cnt: 0 - valLoss: 0.43132370710372925 - trainLoss: 0.4158346951007843\n",
      "cnt: 0 - valLoss: 0.4313229024410248 - trainLoss: 0.41583284735679626\n",
      "cnt: 0 - valLoss: 0.4313221275806427 - trainLoss: 0.4158310294151306\n",
      "cnt: 0 - valLoss: 0.43132129311561584 - trainLoss: 0.4158291816711426\n",
      "cnt: 0 - valLoss: 0.43132054805755615 - trainLoss: 0.4158273935317993\n",
      "cnt: 0 - valLoss: 0.4313197135925293 - trainLoss: 0.41582557559013367\n",
      "cnt: 0 - valLoss: 0.43131890892982483 - trainLoss: 0.41582369804382324\n",
      "cnt: 0 - valLoss: 0.43131813406944275 - trainLoss: 0.41582190990448\n",
      "cnt: 0 - valLoss: 0.43131735920906067 - trainLoss: 0.41582003235816956\n",
      "cnt: 0 - valLoss: 0.4313165545463562 - trainLoss: 0.4158182144165039\n",
      "cnt: 0 - valLoss: 0.4313157796859741 - trainLoss: 0.41581642627716064\n",
      "cnt: 0 - valLoss: 0.43131497502326965 - trainLoss: 0.4158145785331726\n",
      "cnt: 0 - valLoss: 0.4313142001628876 - trainLoss: 0.41581273078918457\n",
      "cnt: 0 - valLoss: 0.4313134253025055 - trainLoss: 0.4158109128475189\n",
      "cnt: 0 - valLoss: 0.43131259083747864 - trainLoss: 0.41580909490585327\n",
      "cnt: 0 - valLoss: 0.43131184577941895 - trainLoss: 0.4158072769641876\n",
      "cnt: 0 - valLoss: 0.4313110113143921 - trainLoss: 0.4158054292201996\n",
      "cnt: 0 - valLoss: 0.43131023645401 - trainLoss: 0.41580358147621155\n",
      "cnt: 0 - valLoss: 0.43130946159362793 - trainLoss: 0.4158017933368683\n",
      "cnt: 0 - valLoss: 0.43130871653556824 - trainLoss: 0.41579994559288025\n",
      "cnt: 0 - valLoss: 0.4313078820705414 - trainLoss: 0.4157980978488922\n",
      "cnt: 0 - valLoss: 0.4313070774078369 - trainLoss: 0.41579630970954895\n",
      "cnt: 0 - valLoss: 0.43130630254745483 - trainLoss: 0.4157944619655609\n",
      "cnt: 0 - valLoss: 0.43130552768707275 - trainLoss: 0.4157926142215729\n",
      "cnt: 0 - valLoss: 0.4313047230243683 - trainLoss: 0.4157908260822296\n",
      "cnt: 0 - valLoss: 0.4313039481639862 - trainLoss: 0.4157889783382416\n",
      "cnt: 0 - valLoss: 0.43130314350128174 - trainLoss: 0.4157871603965759\n",
      "cnt: 0 - valLoss: 0.43130236864089966 - trainLoss: 0.4157853126525879\n",
      "cnt: 0 - valLoss: 0.4313015937805176 - trainLoss: 0.41578346490859985\n",
      "cnt: 0 - valLoss: 0.4313008189201355 - trainLoss: 0.4157816767692566\n",
      "cnt: 0 - valLoss: 0.4313000440597534 - trainLoss: 0.41577985882759094\n",
      "cnt: 0 - valLoss: 0.43129926919937134 - trainLoss: 0.4157780408859253\n",
      "cnt: 0 - valLoss: 0.4312984049320221 - trainLoss: 0.41577619314193726\n",
      "cnt: 0 - valLoss: 0.4312976896762848 - trainLoss: 0.4157743453979492\n",
      "cnt: 0 - valLoss: 0.4312968850135803 - trainLoss: 0.41577255725860596\n",
      "cnt: 0 - valLoss: 0.43129613995552063 - trainLoss: 0.4157707095146179\n",
      "cnt: 0 - valLoss: 0.43129536509513855 - trainLoss: 0.4157688319683075\n",
      "cnt: 0 - valLoss: 0.43129459023475647 - trainLoss: 0.41576704382896423\n",
      "cnt: 0 - valLoss: 0.4312938451766968 - trainLoss: 0.4157652258872986\n",
      "cnt: 0 - valLoss: 0.4312930107116699 - trainLoss: 0.41576340794563293\n",
      "cnt: 0 - valLoss: 0.43129223585128784 - trainLoss: 0.4157615602016449\n",
      "cnt: 0 - valLoss: 0.43129146099090576 - trainLoss: 0.41575974225997925\n",
      "cnt: 0 - valLoss: 0.43129071593284607 - trainLoss: 0.4157579243183136\n",
      "cnt: 0 - valLoss: 0.431289941072464 - trainLoss: 0.41575607657432556\n",
      "cnt: 0 - valLoss: 0.4312891662120819 - trainLoss: 0.4157542884349823\n",
      "cnt: 0 - valLoss: 0.43128839135169983 - trainLoss: 0.4157523810863495\n",
      "cnt: 0 - valLoss: 0.43128761649131775 - trainLoss: 0.4157505929470062\n",
      "cnt: 0 - valLoss: 0.43128684163093567 - trainLoss: 0.41574880480766296\n",
      "cnt: 0 - valLoss: 0.4312860369682312 - trainLoss: 0.4157469570636749\n",
      "cnt: 0 - valLoss: 0.4312852919101715 - trainLoss: 0.4157451093196869\n",
      "cnt: 0 - valLoss: 0.43128451704978943 - trainLoss: 0.41574332118034363\n",
      "cnt: 0 - valLoss: 0.43128371238708496 - trainLoss: 0.4157414436340332\n",
      "cnt: 0 - valLoss: 0.4312829375267029 - trainLoss: 0.41573962569236755\n",
      "cnt: 0 - valLoss: 0.4312822222709656 - trainLoss: 0.4157378375530243\n",
      "cnt: 0 - valLoss: 0.4312814474105835 - trainLoss: 0.41573596000671387\n",
      "cnt: 0 - valLoss: 0.4312806725502014 - trainLoss: 0.4157341718673706\n",
      "cnt: 0 - valLoss: 0.43127989768981934 - trainLoss: 0.41573232412338257\n",
      "cnt: 0 - valLoss: 0.43127912282943726 - trainLoss: 0.41573047637939453\n",
      "cnt: 0 - valLoss: 0.4312783479690552 - trainLoss: 0.41572868824005127\n",
      "cnt: 0 - valLoss: 0.4312775433063507 - trainLoss: 0.41572684049606323\n",
      "cnt: 0 - valLoss: 0.43127676844596863 - trainLoss: 0.41572505235671997\n",
      "cnt: 0 - valLoss: 0.4312760531902313 - trainLoss: 0.41572320461273193\n",
      "cnt: 0 - valLoss: 0.43127527832984924 - trainLoss: 0.41572141647338867\n",
      "cnt: 0 - valLoss: 0.4312744438648224 - trainLoss: 0.41571950912475586\n",
      "cnt: 0 - valLoss: 0.4312736988067627 - trainLoss: 0.4157177209854126\n",
      "cnt: 0 - valLoss: 0.43127286434173584 - trainLoss: 0.41571587324142456\n",
      "cnt: 0 - valLoss: 0.43127208948135376 - trainLoss: 0.4157140552997589\n",
      "cnt: 0 - valLoss: 0.4312712848186493 - trainLoss: 0.41571223735809326\n",
      "cnt: 0 - valLoss: 0.431270569562912 - trainLoss: 0.41571044921875\n",
      "cnt: 0 - valLoss: 0.43126970529556274 - trainLoss: 0.41570860147476196\n",
      "cnt: 0 - valLoss: 0.43126893043518066 - trainLoss: 0.4157067537307739\n",
      "cnt: 0 - valLoss: 0.4312681257724762 - trainLoss: 0.41570496559143066\n",
      "cnt: 0 - valLoss: 0.4312673509120941 - trainLoss: 0.4157031178474426\n",
      "cnt: 0 - valLoss: 0.43126657605171204 - trainLoss: 0.415701299905777\n",
      "cnt: 0 - valLoss: 0.4312657415866852 - trainLoss: 0.41569948196411133\n",
      "cnt: 0 - valLoss: 0.4312649965286255 - trainLoss: 0.4156976640224457\n",
      "cnt: 0 - valLoss: 0.4312642216682434 - trainLoss: 0.41569584608078003\n",
      "cnt: 0 - valLoss: 0.43126338720321655 - trainLoss: 0.4156940281391144\n",
      "cnt: 0 - valLoss: 0.4312626123428345 - trainLoss: 0.41569218039512634\n",
      "cnt: 0 - valLoss: 0.43126180768013 - trainLoss: 0.4156903624534607\n",
      "cnt: 0 - valLoss: 0.4312610328197479 - trainLoss: 0.41568854451179504\n",
      "cnt: 0 - valLoss: 0.43126025795936584 - trainLoss: 0.4156867265701294\n",
      "cnt: 0 - valLoss: 0.4312594532966614 - trainLoss: 0.41568493843078613\n",
      "cnt: 0 - valLoss: 0.4312587380409241 - trainLoss: 0.4156830608844757\n",
      "cnt: 0 - valLoss: 0.43125787377357483 - trainLoss: 0.41568127274513245\n",
      "cnt: 0 - valLoss: 0.43125712871551514 - trainLoss: 0.4156794548034668\n",
      "cnt: 0 - valLoss: 0.43125632405281067 - trainLoss: 0.41567760705947876\n",
      "cnt: 0 - valLoss: 0.4312555491924286 - trainLoss: 0.4156757891178131\n",
      "cnt: 0 - valLoss: 0.4312547743320465 - trainLoss: 0.4156739413738251\n",
      "cnt: 0 - valLoss: 0.43125393986701965 - trainLoss: 0.41567209362983704\n",
      "cnt: 0 - valLoss: 0.4312531650066376 - trainLoss: 0.41567033529281616\n",
      "cnt: 0 - valLoss: 0.4312523901462555 - trainLoss: 0.4156685173511505\n",
      "cnt: 0 - valLoss: 0.4312516152858734 - trainLoss: 0.41566669940948486\n",
      "cnt: 0 - valLoss: 0.43125084042549133 - trainLoss: 0.4156648516654968\n",
      "cnt: 0 - valLoss: 0.43125006556510925 - trainLoss: 0.4156630337238312\n",
      "cnt: 0 - valLoss: 0.4312492907047272 - trainLoss: 0.4156612157821655\n",
      "cnt: 0 - valLoss: 0.4312484860420227 - trainLoss: 0.4156593978404999\n",
      "cnt: 0 - valLoss: 0.43124768137931824 - trainLoss: 0.41565755009651184\n",
      "cnt: 0 - valLoss: 0.43124690651893616 - trainLoss: 0.4156557619571686\n",
      "cnt: 0 - valLoss: 0.4312461316585541 - trainLoss: 0.41565391421318054\n",
      "cnt: 0 - valLoss: 0.431245356798172 - trainLoss: 0.4156520962715149\n",
      "cnt: 0 - valLoss: 0.4312445819377899 - trainLoss: 0.41565027832984924\n",
      "cnt: 0 - valLoss: 0.43124380707740784 - trainLoss: 0.4156484305858612\n",
      "cnt: 0 - valLoss: 0.43124303221702576 - trainLoss: 0.41564664244651794\n",
      "cnt: 0 - valLoss: 0.4312421679496765 - trainLoss: 0.4156447947025299\n",
      "cnt: 0 - valLoss: 0.4312414526939392 - trainLoss: 0.41564300656318665\n",
      "cnt: 0 - valLoss: 0.43124064803123474 - trainLoss: 0.4156411588191986\n",
      "cnt: 0 - valLoss: 0.43123987317085266 - trainLoss: 0.41563931107521057\n",
      "cnt: 0 - valLoss: 0.4312390983104706 - trainLoss: 0.4156375527381897\n",
      "cnt: 0 - valLoss: 0.4312383234500885 - trainLoss: 0.41563570499420166\n",
      "cnt: 0 - valLoss: 0.4312375485897064 - trainLoss: 0.415633887052536\n",
      "cnt: 0 - valLoss: 0.43123677372932434 - trainLoss: 0.415632039308548\n",
      "cnt: 0 - valLoss: 0.43123599886894226 - trainLoss: 0.4156302511692047\n",
      "cnt: 0 - valLoss: 0.4312352240085602 - trainLoss: 0.41562843322753906\n",
      "cnt: 0 - valLoss: 0.4312344789505005 - trainLoss: 0.4156266152858734\n",
      "cnt: 0 - valLoss: 0.4312337040901184 - trainLoss: 0.4156247675418854\n",
      "cnt: 0 - valLoss: 0.43123292922973633 - trainLoss: 0.4156229496002197\n",
      "cnt: 0 - valLoss: 0.43123215436935425 - trainLoss: 0.4156211316585541\n",
      "cnt: 0 - valLoss: 0.43123137950897217 - trainLoss: 0.41561928391456604\n",
      "cnt: 0 - valLoss: 0.4312306046485901 - trainLoss: 0.4156174957752228\n",
      "cnt: 0 - valLoss: 0.431229829788208 - trainLoss: 0.41561564803123474\n",
      "cnt: 0 - valLoss: 0.4312290549278259 - trainLoss: 0.4156138598918915\n",
      "cnt: 0 - valLoss: 0.43122828006744385 - trainLoss: 0.41561201214790344\n",
      "cnt: 0 - valLoss: 0.43122756481170654 - trainLoss: 0.4156101942062378\n",
      "cnt: 0 - valLoss: 0.4312267601490021 - trainLoss: 0.41560837626457214\n",
      "cnt: 0 - valLoss: 0.4312260150909424 - trainLoss: 0.4156065285205841\n",
      "cnt: 0 - valLoss: 0.4312252402305603 - trainLoss: 0.41560474038124084\n",
      "cnt: 0 - valLoss: 0.4312244653701782 - trainLoss: 0.4156029224395752\n",
      "cnt: 0 - valLoss: 0.4312237501144409 - trainLoss: 0.41560113430023193\n",
      "cnt: 0 - valLoss: 0.43122297525405884 - trainLoss: 0.4155992865562439\n",
      "cnt: 0 - valLoss: 0.43122220039367676 - trainLoss: 0.41559743881225586\n",
      "cnt: 0 - valLoss: 0.43122148513793945 - trainLoss: 0.4155956506729126\n",
      "cnt: 0 - valLoss: 0.4312207102775574 - trainLoss: 0.41559380292892456\n",
      "cnt: 0 - valLoss: 0.4312199354171753 - trainLoss: 0.4155919849872589\n",
      "cnt: 0 - valLoss: 0.4312191605567932 - trainLoss: 0.41559016704559326\n",
      "cnt: 0 - valLoss: 0.43121838569641113 - trainLoss: 0.41558837890625\n",
      "cnt: 0 - valLoss: 0.43121767044067383 - trainLoss: 0.41558659076690674\n",
      "cnt: 0 - valLoss: 0.43121689558029175 - trainLoss: 0.4155847430229187\n",
      "cnt: 0 - valLoss: 0.43121612071990967 - trainLoss: 0.41558289527893066\n",
      "cnt: 0 - valLoss: 0.4312153458595276 - trainLoss: 0.4155811071395874\n",
      "cnt: 0 - valLoss: 0.4312145709991455 - trainLoss: 0.41557925939559937\n",
      "cnt: 0 - valLoss: 0.4312138557434082 - trainLoss: 0.41557741165161133\n",
      "cnt: 0 - valLoss: 0.4312130808830261 - trainLoss: 0.41557562351226807\n",
      "cnt: 0 - valLoss: 0.43121233582496643 - trainLoss: 0.4155738353729248\n",
      "cnt: 0 - valLoss: 0.43121159076690674 - trainLoss: 0.41557201743125916\n",
      "cnt: 0 - valLoss: 0.43121081590652466 - trainLoss: 0.4155701994895935\n",
      "cnt: 0 - valLoss: 0.4312100410461426 - trainLoss: 0.41556835174560547\n",
      "cnt: 0 - valLoss: 0.4312092661857605 - trainLoss: 0.4155665636062622\n",
      "cnt: 0 - valLoss: 0.4312084913253784 - trainLoss: 0.41556471586227417\n",
      "cnt: 0 - valLoss: 0.4312077760696411 - trainLoss: 0.4155628979206085\n",
      "cnt: 0 - valLoss: 0.43120700120925903 - trainLoss: 0.41556107997894287\n",
      "cnt: 0 - valLoss: 0.43120625615119934 - trainLoss: 0.4155592620372772\n",
      "cnt: 0 - valLoss: 0.43120548129081726 - trainLoss: 0.41555747389793396\n",
      "cnt: 0 - valLoss: 0.4312047064304352 - trainLoss: 0.41555559635162354\n",
      "cnt: 0 - valLoss: 0.4312039911746979 - trainLoss: 0.4155538082122803\n",
      "cnt: 0 - valLoss: 0.4312032163143158 - trainLoss: 0.4155519902706146\n",
      "cnt: 0 - valLoss: 0.4312025010585785 - trainLoss: 0.41555020213127136\n",
      "cnt: 0 - valLoss: 0.4312017261981964 - trainLoss: 0.4155484139919281\n",
      "cnt: 0 - valLoss: 0.43120095133781433 - trainLoss: 0.41554656624794006\n",
      "cnt: 0 - valLoss: 0.43120017647743225 - trainLoss: 0.415544718503952\n",
      "cnt: 0 - valLoss: 0.43119946122169495 - trainLoss: 0.41554293036460876\n",
      "cnt: 0 - valLoss: 0.43119871616363525 - trainLoss: 0.4155411124229431\n",
      "cnt: 0 - valLoss: 0.4311979413032532 - trainLoss: 0.41553929448127747\n",
      "cnt: 0 - valLoss: 0.4311971664428711 - trainLoss: 0.41553744673728943\n",
      "cnt: 0 - valLoss: 0.4311964511871338 - trainLoss: 0.41553565859794617\n",
      "cnt: 0 - valLoss: 0.4311956763267517 - trainLoss: 0.4155338406562805\n",
      "cnt: 0 - valLoss: 0.4311949610710144 - trainLoss: 0.41553202271461487\n",
      "cnt: 0 - valLoss: 0.4311941862106323 - trainLoss: 0.4155302047729492\n",
      "cnt: 0 - valLoss: 0.43119344115257263 - trainLoss: 0.4155283570289612\n",
      "cnt: 0 - valLoss: 0.43119263648986816 - trainLoss: 0.4155265688896179\n",
      "cnt: 0 - valLoss: 0.43119195103645325 - trainLoss: 0.41552478075027466\n",
      "cnt: 0 - valLoss: 0.43119117617607117 - trainLoss: 0.41552290320396423\n",
      "cnt: 0 - valLoss: 0.4311904013156891 - trainLoss: 0.4155210852622986\n",
      "cnt: 0 - valLoss: 0.431189626455307 - trainLoss: 0.4155192971229553\n",
      "cnt: 0 - valLoss: 0.4311888515949249 - trainLoss: 0.41551750898361206\n",
      "cnt: 0 - valLoss: 0.43118816614151 - trainLoss: 0.415515661239624\n",
      "cnt: 0 - valLoss: 0.43118739128112793 - trainLoss: 0.415513813495636\n",
      "cnt: 0 - valLoss: 0.4311866760253906 - trainLoss: 0.4155120253562927\n",
      "cnt: 0 - valLoss: 0.43118590116500854 - trainLoss: 0.41551023721694946\n",
      "cnt: 0 - valLoss: 0.43118518590927124 - trainLoss: 0.4155083894729614\n",
      "cnt: 0 - valLoss: 0.43118441104888916 - trainLoss: 0.4155065715312958\n",
      "cnt: 0 - valLoss: 0.4311836361885071 - trainLoss: 0.4155047535896301\n",
      "cnt: 0 - valLoss: 0.43118295073509216 - trainLoss: 0.4155029356479645\n",
      "cnt: 0 - valLoss: 0.4311821460723877 - trainLoss: 0.41550111770629883\n",
      "cnt: 0 - valLoss: 0.431181401014328 - trainLoss: 0.41549935936927795\n",
      "cnt: 0 - valLoss: 0.4311806857585907 - trainLoss: 0.41549748182296753\n",
      "cnt: 0 - valLoss: 0.431179940700531 - trainLoss: 0.4154956638813019\n",
      "cnt: 0 - valLoss: 0.4311791658401489 - trainLoss: 0.4154938757419586\n",
      "cnt: 0 - valLoss: 0.43117839097976685 - trainLoss: 0.41549208760261536\n",
      "cnt: 0 - valLoss: 0.43117764592170715 - trainLoss: 0.4154902398586273\n",
      "cnt: 0 - valLoss: 0.43117690086364746 - trainLoss: 0.41548845171928406\n",
      "cnt: 0 - valLoss: 0.43117618560791016 - trainLoss: 0.415486603975296\n",
      "cnt: 0 - valLoss: 0.4311754107475281 - trainLoss: 0.41548478603363037\n",
      "cnt: 0 - valLoss: 0.4311746656894684 - trainLoss: 0.4154829978942871\n",
      "cnt: 0 - valLoss: 0.4311739504337311 - trainLoss: 0.4154811203479767\n",
      "cnt: 0 - valLoss: 0.431173175573349 - trainLoss: 0.4154793322086334\n",
      "cnt: 0 - valLoss: 0.4311724603176117 - trainLoss: 0.4154775142669678\n",
      "cnt: 0 - valLoss: 0.431171715259552 - trainLoss: 0.4154757261276245\n",
      "cnt: 0 - valLoss: 0.4311709403991699 - trainLoss: 0.4154738783836365\n",
      "cnt: 0 - valLoss: 0.4311702251434326 - trainLoss: 0.4154720902442932\n",
      "cnt: 0 - valLoss: 0.43116945028305054 - trainLoss: 0.4154702425003052\n",
      "cnt: 0 - valLoss: 0.43116870522499084 - trainLoss: 0.4154684543609619\n",
      "cnt: 0 - valLoss: 0.43116796016693115 - trainLoss: 0.4154666066169739\n",
      "cnt: 0 - valLoss: 0.43116721510887146 - trainLoss: 0.4154648184776306\n",
      "cnt: 0 - valLoss: 0.4311664402484894 - trainLoss: 0.4154629707336426\n",
      "cnt: 0 - valLoss: 0.4311657249927521 - trainLoss: 0.4154611825942993\n",
      "cnt: 0 - valLoss: 0.43116503953933716 - trainLoss: 0.41545936465263367\n",
      "cnt: 0 - valLoss: 0.4311642646789551 - trainLoss: 0.415457546710968\n",
      "cnt: 0 - valLoss: 0.4311635196208954 - trainLoss: 0.41545569896698\n",
      "cnt: 0 - valLoss: 0.4311627447605133 - trainLoss: 0.4154539406299591\n",
      "cnt: 0 - valLoss: 0.4311619997024536 - trainLoss: 0.41545209288597107\n",
      "cnt: 0 - valLoss: 0.4311612546443939 - trainLoss: 0.4154503047466278\n",
      "cnt: 0 - valLoss: 0.4311605393886566 - trainLoss: 0.41544845700263977\n",
      "cnt: 0 - valLoss: 0.43115976452827454 - trainLoss: 0.4154466688632965\n",
      "cnt: 0 - valLoss: 0.43115901947021484 - trainLoss: 0.41544482111930847\n",
      "cnt: 0 - valLoss: 0.43115824460983276 - trainLoss: 0.4154430329799652\n",
      "cnt: 0 - valLoss: 0.43115755915641785 - trainLoss: 0.41544121503829956\n",
      "cnt: 0 - valLoss: 0.43115681409835815 - trainLoss: 0.4154393970966339\n",
      "cnt: 0 - valLoss: 0.43115612864494324 - trainLoss: 0.41543757915496826\n",
      "cnt: 0 - valLoss: 0.43115535378456116 - trainLoss: 0.415435791015625\n",
      "cnt: 0 - valLoss: 0.43115466833114624 - trainLoss: 0.41543394327163696\n",
      "cnt: 0 - valLoss: 0.43115389347076416 - trainLoss: 0.4154321551322937\n",
      "cnt: 0 - valLoss: 0.43115314841270447 - trainLoss: 0.41543036699295044\n",
      "cnt: 0 - valLoss: 0.43115243315696716 - trainLoss: 0.4154285192489624\n",
      "cnt: 0 - valLoss: 0.43115168809890747 - trainLoss: 0.41542673110961914\n",
      "cnt: 0 - valLoss: 0.43115097284317017 - trainLoss: 0.4154248833656311\n",
      "cnt: 0 - valLoss: 0.43115025758743286 - trainLoss: 0.41542306542396545\n",
      "cnt: 0 - valLoss: 0.4311494827270508 - trainLoss: 0.4154212772846222\n",
      "cnt: 0 - valLoss: 0.4311487376689911 - trainLoss: 0.41541942954063416\n",
      "cnt: 0 - valLoss: 0.4311480224132538 - trainLoss: 0.4154176414012909\n",
      "cnt: 0 - valLoss: 0.4311472773551941 - trainLoss: 0.41541579365730286\n",
      "cnt: 0 - valLoss: 0.4311465620994568 - trainLoss: 0.4154140055179596\n",
      "cnt: 0 - valLoss: 0.4311458170413971 - trainLoss: 0.41541221737861633\n",
      "cnt: 0 - valLoss: 0.4311451017856598 - trainLoss: 0.4154103994369507\n",
      "cnt: 0 - valLoss: 0.4311443567276001 - trainLoss: 0.41540858149528503\n",
      "cnt: 0 - valLoss: 0.4311436414718628 - trainLoss: 0.4154067635536194\n",
      "cnt: 0 - valLoss: 0.4311428666114807 - trainLoss: 0.41540494561195374\n",
      "cnt: 0 - valLoss: 0.4311421811580658 - trainLoss: 0.4154031276702881\n",
      "cnt: 0 - valLoss: 0.4311414659023285 - trainLoss: 0.4154013395309448\n",
      "cnt: 0 - valLoss: 0.4311406910419464 - trainLoss: 0.41539955139160156\n",
      "cnt: 0 - valLoss: 0.4311400055885315 - trainLoss: 0.4153977036476135\n",
      "cnt: 0 - valLoss: 0.4311392605304718 - trainLoss: 0.4153958857059479\n",
      "cnt: 0 - valLoss: 0.4311385452747345 - trainLoss: 0.4153940677642822\n",
      "cnt: 0 - valLoss: 0.4311377704143524 - trainLoss: 0.4153922498226166\n",
      "cnt: 0 - valLoss: 0.4311370849609375 - trainLoss: 0.4153904616832733\n",
      "cnt: 0 - valLoss: 0.4311363101005554 - trainLoss: 0.4153886139392853\n",
      "cnt: 0 - valLoss: 0.4311356246471405 - trainLoss: 0.415386825799942\n",
      "cnt: 0 - valLoss: 0.4311348497867584 - trainLoss: 0.415384978055954\n",
      "cnt: 0 - valLoss: 0.4311341643333435 - trainLoss: 0.4153831899166107\n",
      "cnt: 0 - valLoss: 0.4311334490776062 - trainLoss: 0.41538140177726746\n",
      "cnt: 0 - valLoss: 0.4311327040195465 - trainLoss: 0.4153795838356018\n",
      "cnt: 0 - valLoss: 0.4311319887638092 - trainLoss: 0.41537776589393616\n",
      "cnt: 0 - valLoss: 0.4311312437057495 - trainLoss: 0.4153759479522705\n",
      "cnt: 0 - valLoss: 0.4311305582523346 - trainLoss: 0.41537415981292725\n",
      "cnt: 0 - valLoss: 0.4311298131942749 - trainLoss: 0.4153723120689392\n",
      "cnt: 0 - valLoss: 0.4311290681362152 - trainLoss: 0.41537052392959595\n",
      "cnt: 0 - valLoss: 0.4311283528804779 - trainLoss: 0.4153686761856079\n",
      "cnt: 0 - valLoss: 0.431127667427063 - trainLoss: 0.41536688804626465\n",
      "cnt: 0 - valLoss: 0.4311269223690033 - trainLoss: 0.415365070104599\n",
      "cnt: 0 - valLoss: 0.431126207113266 - trainLoss: 0.41536325216293335\n",
      "cnt: 0 - valLoss: 0.4311254322528839 - trainLoss: 0.4153614342212677\n",
      "cnt: 0 - valLoss: 0.431124746799469 - trainLoss: 0.41535964608192444\n",
      "cnt: 0 - valLoss: 0.4311240017414093 - trainLoss: 0.4153577983379364\n",
      "cnt: 0 - valLoss: 0.431123286485672 - trainLoss: 0.41535598039627075\n",
      "cnt: 0 - valLoss: 0.4311225414276123 - trainLoss: 0.4153541624546051\n",
      "cnt: 0 - valLoss: 0.431121826171875 - trainLoss: 0.41535237431526184\n",
      "cnt: 0 - valLoss: 0.4311211407184601 - trainLoss: 0.4153505861759186\n",
      "cnt: 0 - valLoss: 0.4311204254627228 - trainLoss: 0.41534876823425293\n",
      "cnt: 0 - valLoss: 0.4311196804046631 - trainLoss: 0.41534698009490967\n",
      "cnt: 0 - valLoss: 0.4311189651489258 - trainLoss: 0.4153451919555664\n",
      "cnt: 0 - valLoss: 0.4311182200908661 - trainLoss: 0.41534334421157837\n",
      "cnt: 0 - valLoss: 0.4311175048351288 - trainLoss: 0.41534149646759033\n",
      "cnt: 0 - valLoss: 0.4311167597770691 - trainLoss: 0.41533970832824707\n",
      "cnt: 0 - valLoss: 0.4311160743236542 - trainLoss: 0.4153378903865814\n",
      "cnt: 0 - valLoss: 0.4311152994632721 - trainLoss: 0.41533610224723816\n",
      "cnt: 0 - valLoss: 0.4311146140098572 - trainLoss: 0.4153342545032501\n",
      "cnt: 0 - valLoss: 0.4311138391494751 - trainLoss: 0.41533246636390686\n",
      "cnt: 0 - valLoss: 0.4311131536960602 - trainLoss: 0.4153306186199188\n",
      "cnt: 0 - valLoss: 0.4311124384403229 - trainLoss: 0.41532883048057556\n",
      "cnt: 0 - valLoss: 0.43111172318458557 - trainLoss: 0.4153270423412323\n",
      "cnt: 0 - valLoss: 0.43111103773117065 - trainLoss: 0.41532519459724426\n",
      "cnt: 0 - valLoss: 0.4311102628707886 - trainLoss: 0.415323406457901\n",
      "cnt: 0 - valLoss: 0.43110957741737366 - trainLoss: 0.41532158851623535\n",
      "cnt: 0 - valLoss: 0.43110883235931396 - trainLoss: 0.4153197705745697\n",
      "cnt: 0 - valLoss: 0.43110811710357666 - trainLoss: 0.41531795263290405\n",
      "cnt: 0 - valLoss: 0.43110743165016174 - trainLoss: 0.4153161644935608\n",
      "cnt: 0 - valLoss: 0.43110665678977966 - trainLoss: 0.41531431674957275\n",
      "cnt: 0 - valLoss: 0.43110591173171997 - trainLoss: 0.4153125286102295\n",
      "cnt: 0 - valLoss: 0.43110522627830505 - trainLoss: 0.41531071066856384\n",
      "cnt: 0 - valLoss: 0.43110451102256775 - trainLoss: 0.4153089225292206\n",
      "cnt: 0 - valLoss: 0.43110382556915283 - trainLoss: 0.41530707478523254\n",
      "cnt: 0 - valLoss: 0.4311031103134155 - trainLoss: 0.4153052866458893\n",
      "cnt: 0 - valLoss: 0.43110236525535583 - trainLoss: 0.415303498506546\n",
      "cnt: 0 - valLoss: 0.43110164999961853 - trainLoss: 0.415301650762558\n",
      "cnt: 0 - valLoss: 0.4311009645462036 - trainLoss: 0.4152998626232147\n",
      "cnt: 0 - valLoss: 0.43110018968582153 - trainLoss: 0.4152980446815491\n",
      "cnt: 0 - valLoss: 0.43109944462776184 - trainLoss: 0.4152962267398834\n",
      "cnt: 0 - valLoss: 0.4310987591743469 - trainLoss: 0.4152944087982178\n",
      "cnt: 0 - valLoss: 0.4310980439186096 - trainLoss: 0.4152926206588745\n",
      "cnt: 0 - valLoss: 0.4310972988605499 - trainLoss: 0.4152907729148865\n",
      "cnt: 0 - valLoss: 0.431096613407135 - trainLoss: 0.4152890145778656\n",
      "cnt: 0 - valLoss: 0.43109583854675293 - trainLoss: 0.41528719663619995\n",
      "cnt: 0 - valLoss: 0.431095153093338 - trainLoss: 0.4152853488922119\n",
      "cnt: 0 - valLoss: 0.4310944676399231 - trainLoss: 0.41528353095054626\n",
      "cnt: 0 - valLoss: 0.4310937523841858 - trainLoss: 0.415281742811203\n",
      "cnt: 0 - valLoss: 0.4310930371284485 - trainLoss: 0.41527995467185974\n",
      "cnt: 0 - valLoss: 0.43109235167503357 - trainLoss: 0.4152781069278717\n",
      "cnt: 0 - valLoss: 0.4310915768146515 - trainLoss: 0.41527631878852844\n",
      "cnt: 0 - valLoss: 0.4310908913612366 - trainLoss: 0.4152745008468628\n",
      "cnt: 0 - valLoss: 0.4310901165008545 - trainLoss: 0.41527271270751953\n",
      "cnt: 0 - valLoss: 0.43108946084976196 - trainLoss: 0.4152708649635315\n",
      "cnt: 0 - valLoss: 0.43108874559402466 - trainLoss: 0.41526907682418823\n",
      "cnt: 0 - valLoss: 0.4310879707336426 - trainLoss: 0.41526728868484497\n",
      "cnt: 0 - valLoss: 0.43108728528022766 - trainLoss: 0.41526544094085693\n",
      "cnt: 0 - valLoss: 0.43108659982681274 - trainLoss: 0.41526365280151367\n",
      "cnt: 0 - valLoss: 0.43108585476875305 - trainLoss: 0.41526180505752563\n",
      "cnt: 0 - valLoss: 0.43108513951301575 - trainLoss: 0.41526004672050476\n",
      "cnt: 0 - valLoss: 0.43108445405960083 - trainLoss: 0.4152582585811615\n",
      "cnt: 0 - valLoss: 0.43108370900154114 - trainLoss: 0.41525641083717346\n",
      "cnt: 0 - valLoss: 0.43108299374580383 - trainLoss: 0.4152546226978302\n",
      "cnt: 0 - valLoss: 0.4310823082923889 - trainLoss: 0.41525280475616455\n",
      "cnt: 0 - valLoss: 0.431081622838974 - trainLoss: 0.4152509868144989\n",
      "cnt: 0 - valLoss: 0.4310809075832367 - trainLoss: 0.41524913907051086\n",
      "cnt: 0 - valLoss: 0.431080162525177 - trainLoss: 0.41524738073349\n",
      "cnt: 0 - valLoss: 0.4310794472694397 - trainLoss: 0.41524559259414673\n",
      "cnt: 0 - valLoss: 0.4310787618160248 - trainLoss: 0.4152437746524811\n",
      "cnt: 0 - valLoss: 0.4310780167579651 - trainLoss: 0.41524195671081543\n",
      "cnt: 0 - valLoss: 0.43107733130455017 - trainLoss: 0.4152401387691498\n",
      "cnt: 0 - valLoss: 0.43107661604881287 - trainLoss: 0.4152383506298065\n",
      "cnt: 0 - valLoss: 0.4310758709907532 - trainLoss: 0.41523656249046326\n",
      "cnt: 0 - valLoss: 0.43107518553733826 - trainLoss: 0.4152347147464752\n",
      "cnt: 0 - valLoss: 0.43107450008392334 - trainLoss: 0.4152328670024872\n",
      "cnt: 0 - valLoss: 0.43107378482818604 - trainLoss: 0.4152311086654663\n",
      "cnt: 0 - valLoss: 0.43107303977012634 - trainLoss: 0.41522926092147827\n",
      "cnt: 0 - valLoss: 0.4310723543167114 - trainLoss: 0.415227472782135\n",
      "cnt: 0 - valLoss: 0.4310716688632965 - trainLoss: 0.41522568464279175\n",
      "cnt: 0 - valLoss: 0.4310709536075592 - trainLoss: 0.4152238667011261\n",
      "cnt: 0 - valLoss: 0.4310702085494995 - trainLoss: 0.41522207856178284\n",
      "cnt: 0 - valLoss: 0.4310695230960846 - trainLoss: 0.4152202308177948\n",
      "cnt: 0 - valLoss: 0.4310688376426697 - trainLoss: 0.41521841287612915\n",
      "cnt: 0 - valLoss: 0.43106815218925476 - trainLoss: 0.4152165949344635\n",
      "cnt: 0 - valLoss: 0.43106746673583984 - trainLoss: 0.41521480679512024\n",
      "cnt: 0 - valLoss: 0.43106675148010254 - trainLoss: 0.415213018655777\n",
      "cnt: 0 - valLoss: 0.43106600642204285 - trainLoss: 0.41521120071411133\n",
      "cnt: 0 - valLoss: 0.43106532096862793 - trainLoss: 0.41520941257476807\n",
      "cnt: 0 - valLoss: 0.431064635515213 - trainLoss: 0.4152076244354248\n",
      "cnt: 0 - valLoss: 0.4310639500617981 - trainLoss: 0.41520577669143677\n",
      "cnt: 0 - valLoss: 0.4310632348060608 - trainLoss: 0.41520392894744873\n",
      "cnt: 0 - valLoss: 0.4310625493526459 - trainLoss: 0.41520214080810547\n",
      "cnt: 0 - valLoss: 0.43106186389923096 - trainLoss: 0.4152003526687622\n",
      "cnt: 0 - valLoss: 0.43106114864349365 - trainLoss: 0.41519853472709656\n",
      "cnt: 0 - valLoss: 0.43106043338775635 - trainLoss: 0.4151967465877533\n",
      "cnt: 0 - valLoss: 0.43105974793434143 - trainLoss: 0.41519495844841003\n",
      "cnt: 0 - valLoss: 0.4310590624809265 - trainLoss: 0.415193110704422\n",
      "cnt: 0 - valLoss: 0.4310583472251892 - trainLoss: 0.41519132256507874\n",
      "cnt: 0 - valLoss: 0.4310576617717743 - trainLoss: 0.4151895046234131\n",
      "cnt: 0 - valLoss: 0.4310569763183594 - trainLoss: 0.4151877164840698\n",
      "cnt: 0 - valLoss: 0.43105626106262207 - trainLoss: 0.41518592834472656\n",
      "cnt: 0 - valLoss: 0.43105557560920715 - trainLoss: 0.4151841104030609\n",
      "cnt: 0 - valLoss: 0.43105489015579224 - trainLoss: 0.4151822626590729\n",
      "cnt: 0 - valLoss: 0.4310542047023773 - trainLoss: 0.4151804745197296\n",
      "cnt: 0 - valLoss: 0.4310534596443176 - trainLoss: 0.41517868638038635\n",
      "cnt: 0 - valLoss: 0.4310527741909027 - trainLoss: 0.4151768982410431\n",
      "cnt: 0 - valLoss: 0.4310520887374878 - trainLoss: 0.41517508029937744\n",
      "cnt: 0 - valLoss: 0.4310513734817505 - trainLoss: 0.4151732921600342\n",
      "cnt: 0 - valLoss: 0.43105068802833557 - trainLoss: 0.4151715040206909\n",
      "cnt: 0 - valLoss: 0.43105000257492065 - trainLoss: 0.4151695966720581\n",
      "cnt: 0 - valLoss: 0.43104931712150574 - trainLoss: 0.41516783833503723\n",
      "cnt: 0 - valLoss: 0.4310486316680908 - trainLoss: 0.4151660203933716\n",
      "cnt: 0 - valLoss: 0.43104788661003113 - trainLoss: 0.4151642620563507\n",
      "cnt: 0 - valLoss: 0.4310472011566162 - trainLoss: 0.41516247391700745\n",
      "cnt: 0 - valLoss: 0.4310464859008789 - trainLoss: 0.4151606261730194\n",
      "cnt: 0 - valLoss: 0.431045800447464 - trainLoss: 0.41515880823135376\n",
      "cnt: 0 - valLoss: 0.4310451149940491 - trainLoss: 0.4151570200920105\n",
      "cnt: 0 - valLoss: 0.43104442954063416 - trainLoss: 0.41515523195266724\n",
      "cnt: 0 - valLoss: 0.43104374408721924 - trainLoss: 0.4151534140110016\n",
      "cnt: 0 - valLoss: 0.4310430586338043 - trainLoss: 0.4151516258716583\n",
      "cnt: 0 - valLoss: 0.43104231357574463 - trainLoss: 0.41514983773231506\n",
      "cnt: 0 - valLoss: 0.4310416281223297 - trainLoss: 0.4151480495929718\n",
      "cnt: 0 - valLoss: 0.4310409128665924 - trainLoss: 0.41514623165130615\n",
      "cnt: 0 - valLoss: 0.4310402572154999 - trainLoss: 0.4151443839073181\n",
      "cnt: 0 - valLoss: 0.4310395419597626 - trainLoss: 0.41514259576797485\n",
      "cnt: 0 - valLoss: 0.43103885650634766 - trainLoss: 0.4151408076286316\n",
      "cnt: 0 - valLoss: 0.43103817105293274 - trainLoss: 0.41513895988464355\n",
      "cnt: 0 - valLoss: 0.43103742599487305 - trainLoss: 0.4151371717453003\n",
      "cnt: 0 - valLoss: 0.43103674054145813 - trainLoss: 0.41513535380363464\n",
      "cnt: 0 - valLoss: 0.4310360550880432 - trainLoss: 0.4151335656642914\n",
      "cnt: 0 - valLoss: 0.4310353696346283 - trainLoss: 0.4151317775249481\n",
      "cnt: 0 - valLoss: 0.4310346841812134 - trainLoss: 0.41512995958328247\n",
      "cnt: 0 - valLoss: 0.43103399872779846 - trainLoss: 0.4151281714439392\n",
      "cnt: 0 - valLoss: 0.43103328347206116 - trainLoss: 0.41512632369995117\n",
      "cnt: 0 - valLoss: 0.43103259801864624 - trainLoss: 0.4151245653629303\n",
      "cnt: 0 - valLoss: 0.4310319125652313 - trainLoss: 0.41512274742126465\n",
      "cnt: 0 - valLoss: 0.4310312271118164 - trainLoss: 0.415120929479599\n",
      "cnt: 0 - valLoss: 0.4310305416584015 - trainLoss: 0.41511914134025574\n",
      "cnt: 0 - valLoss: 0.4310298562049866 - trainLoss: 0.4151172935962677\n",
      "cnt: 0 - valLoss: 0.4310291111469269 - trainLoss: 0.41511550545692444\n",
      "cnt: 0 - valLoss: 0.43102842569351196 - trainLoss: 0.4151137173175812\n",
      "cnt: 0 - valLoss: 0.43102774024009705 - trainLoss: 0.4151118993759155\n",
      "cnt: 0 - valLoss: 0.43102705478668213 - trainLoss: 0.41511011123657227\n",
      "cnt: 0 - valLoss: 0.4310263693332672 - trainLoss: 0.415108323097229\n",
      "cnt: 0 - valLoss: 0.4310256540775299 - trainLoss: 0.41510650515556335\n",
      "cnt: 0 - valLoss: 0.431024968624115 - trainLoss: 0.4151047170162201\n",
      "cnt: 0 - valLoss: 0.4310242831707001 - trainLoss: 0.41510292887687683\n",
      "cnt: 0 - valLoss: 0.43102359771728516 - trainLoss: 0.4151010811328888\n",
      "cnt: 0 - valLoss: 0.43102291226387024 - trainLoss: 0.4150993227958679\n",
      "cnt: 0 - valLoss: 0.4310222268104553 - trainLoss: 0.4150974750518799\n",
      "cnt: 0 - valLoss: 0.4310215413570404 - trainLoss: 0.4150957465171814\n",
      "cnt: 0 - valLoss: 0.4310208559036255 - trainLoss: 0.41509389877319336\n",
      "cnt: 0 - valLoss: 0.43102017045021057 - trainLoss: 0.4150920510292053\n",
      "cnt: 0 - valLoss: 0.43101948499679565 - trainLoss: 0.41509029269218445\n",
      "cnt: 0 - valLoss: 0.43101873993873596 - trainLoss: 0.4150884449481964\n",
      "cnt: 0 - valLoss: 0.43101802468299866 - trainLoss: 0.41508668661117554\n",
      "cnt: 0 - valLoss: 0.43101736903190613 - trainLoss: 0.4150848984718323\n",
      "cnt: 0 - valLoss: 0.4310166835784912 - trainLoss: 0.41508305072784424\n",
      "cnt: 0 - valLoss: 0.4310159683227539 - trainLoss: 0.415081262588501\n",
      "cnt: 0 - valLoss: 0.431015282869339 - trainLoss: 0.4150794744491577\n",
      "cnt: 0 - valLoss: 0.4310145974159241 - trainLoss: 0.41507765650749207\n",
      "cnt: 0 - valLoss: 0.43101391196250916 - trainLoss: 0.4150758683681488\n",
      "cnt: 0 - valLoss: 0.43101322650909424 - trainLoss: 0.41507408022880554\n",
      "cnt: 0 - valLoss: 0.4310125410556793 - trainLoss: 0.4150722622871399\n",
      "cnt: 0 - valLoss: 0.4310118556022644 - trainLoss: 0.41507047414779663\n",
      "cnt: 0 - valLoss: 0.4310111701488495 - trainLoss: 0.41506868600845337\n",
      "cnt: 0 - valLoss: 0.4310104250907898 - trainLoss: 0.41506683826446533\n",
      "cnt: 0 - valLoss: 0.43100979924201965 - trainLoss: 0.41506505012512207\n",
      "cnt: 0 - valLoss: 0.43100911378860474 - trainLoss: 0.4150632321834564\n",
      "cnt: 0 - valLoss: 0.43100836873054504 - trainLoss: 0.41506144404411316\n",
      "cnt: 0 - valLoss: 0.4310076832771301 - trainLoss: 0.4150595963001251\n",
      "cnt: 0 - valLoss: 0.4310070276260376 - trainLoss: 0.41505783796310425\n",
      "cnt: 0 - valLoss: 0.4310063123703003 - trainLoss: 0.4150560200214386\n",
      "cnt: 0 - valLoss: 0.431005597114563 - trainLoss: 0.4150542616844177\n",
      "cnt: 0 - valLoss: 0.43100494146347046 - trainLoss: 0.4150524139404297\n",
      "cnt: 0 - valLoss: 0.43100425601005554 - trainLoss: 0.4150506258010864\n",
      "cnt: 0 - valLoss: 0.4310035705566406 - trainLoss: 0.41504886746406555\n",
      "cnt: 0 - valLoss: 0.4310028851032257 - trainLoss: 0.4150470197200775\n",
      "cnt: 0 - valLoss: 0.4310022294521332 - trainLoss: 0.41504523158073425\n",
      "cnt: 0 - valLoss: 0.4310015141963959 - trainLoss: 0.4150434136390686\n",
      "cnt: 0 - valLoss: 0.43100082874298096 - trainLoss: 0.41504162549972534\n",
      "cnt: 0 - valLoss: 0.43100011348724365 - trainLoss: 0.4150398373603821\n",
      "cnt: 0 - valLoss: 0.4309994578361511 - trainLoss: 0.4150380492210388\n",
      "cnt: 0 - valLoss: 0.4309987723827362 - trainLoss: 0.41503623127937317\n",
      "cnt: 0 - valLoss: 0.4309980869293213 - trainLoss: 0.4150344431400299\n",
      "cnt: 0 - valLoss: 0.4309973418712616 - trainLoss: 0.41503259539604187\n",
      "cnt: 0 - valLoss: 0.43099671602249146 - trainLoss: 0.415030837059021\n",
      "cnt: 0 - valLoss: 0.43099603056907654 - trainLoss: 0.41502904891967773\n",
      "cnt: 0 - valLoss: 0.4309953451156616 - trainLoss: 0.4150272607803345\n",
      "cnt: 0 - valLoss: 0.43099460005760193 - trainLoss: 0.41502541303634644\n",
      "cnt: 0 - valLoss: 0.430993914604187 - trainLoss: 0.4150236248970032\n",
      "cnt: 0 - valLoss: 0.43099328875541687 - trainLoss: 0.4150218665599823\n",
      "cnt: 0 - valLoss: 0.4309925436973572 - trainLoss: 0.41502001881599426\n",
      "cnt: 0 - valLoss: 0.43099191784858704 - trainLoss: 0.415018230676651\n",
      "cnt: 0 - valLoss: 0.4309912621974945 - trainLoss: 0.41501641273498535\n",
      "cnt: 0 - valLoss: 0.4309905469417572 - trainLoss: 0.4150145947933197\n",
      "cnt: 0 - valLoss: 0.4309898614883423 - trainLoss: 0.41501283645629883\n",
      "cnt: 0 - valLoss: 0.43098917603492737 - trainLoss: 0.4150110185146332\n",
      "cnt: 0 - valLoss: 0.4309884309768677 - trainLoss: 0.4150092303752899\n",
      "cnt: 0 - valLoss: 0.43098780512809753 - trainLoss: 0.4150073826313019\n",
      "cnt: 0 - valLoss: 0.4309871196746826 - trainLoss: 0.4150055944919586\n",
      "cnt: 0 - valLoss: 0.4309864342212677 - trainLoss: 0.41500380635261536\n",
      "cnt: 0 - valLoss: 0.43098577857017517 - trainLoss: 0.4150019884109497\n",
      "cnt: 0 - valLoss: 0.43098509311676025 - trainLoss: 0.41500020027160645\n",
      "cnt: 0 - valLoss: 0.43098440766334534 - trainLoss: 0.4149984121322632\n",
      "cnt: 0 - valLoss: 0.4309837222099304 - trainLoss: 0.41499659419059753\n",
      "cnt: 0 - valLoss: 0.4309830367565155 - trainLoss: 0.4149948060512543\n",
      "cnt: 0 - valLoss: 0.4309823513031006 - trainLoss: 0.414993017911911\n",
      "cnt: 0 - valLoss: 0.43098166584968567 - trainLoss: 0.41499119997024536\n",
      "cnt: 0 - valLoss: 0.43098098039627075 - trainLoss: 0.4149894118309021\n",
      "cnt: 0 - valLoss: 0.4309803247451782 - trainLoss: 0.41498762369155884\n",
      "cnt: 0 - valLoss: 0.4309796094894409 - trainLoss: 0.4149858057498932\n",
      "cnt: 0 - valLoss: 0.4309789538383484 - trainLoss: 0.4149840176105499\n",
      "cnt: 0 - valLoss: 0.43097832798957825 - trainLoss: 0.4149821698665619\n",
      "cnt: 0 - valLoss: 0.43097764253616333 - trainLoss: 0.414980411529541\n",
      "cnt: 0 - valLoss: 0.4309769570827484 - trainLoss: 0.41497859358787537\n",
      "cnt: 0 - valLoss: 0.4309762716293335 - trainLoss: 0.4149767756462097\n",
      "cnt: 0 - valLoss: 0.4309755861759186 - trainLoss: 0.41497501730918884\n",
      "cnt: 0 - valLoss: 0.43097490072250366 - trainLoss: 0.4149731993675232\n",
      "cnt: 0 - valLoss: 0.43097424507141113 - trainLoss: 0.41497138142585754\n",
      "cnt: 0 - valLoss: 0.430973619222641 - trainLoss: 0.4149695932865143\n",
      "cnt: 0 - valLoss: 0.4309729337692261 - trainLoss: 0.414967805147171\n",
      "cnt: 0 - valLoss: 0.43097224831581116 - trainLoss: 0.414965957403183\n",
      "cnt: 0 - valLoss: 0.43097156286239624 - trainLoss: 0.4149641692638397\n",
      "cnt: 0 - valLoss: 0.4309708774089813 - trainLoss: 0.4149623513221741\n",
      "cnt: 0 - valLoss: 0.4309701919555664 - trainLoss: 0.4149605631828308\n",
      "cnt: 0 - valLoss: 0.4309695363044739 - trainLoss: 0.4149588644504547\n",
      "cnt: 0 - valLoss: 0.43096888065338135 - trainLoss: 0.4149570167064667\n",
      "cnt: 0 - valLoss: 0.43096816539764404 - trainLoss: 0.4149552285671234\n",
      "cnt: 0 - valLoss: 0.4309675097465515 - trainLoss: 0.41495341062545776\n",
      "cnt: 0 - valLoss: 0.4309668242931366 - trainLoss: 0.4149516224861145\n",
      "cnt: 0 - valLoss: 0.4309661388397217 - trainLoss: 0.41494983434677124\n",
      "cnt: 0 - valLoss: 0.4309656322002411 - trainLoss: 0.4149480164051056\n",
      "cnt: 0 - valLoss: 0.43096500635147095 - trainLoss: 0.41494622826576233\n",
      "cnt: 0 - valLoss: 0.4309643507003784 - trainLoss: 0.4149443805217743\n",
      "cnt: 0 - valLoss: 0.4309636354446411 - trainLoss: 0.4149426221847534\n",
      "cnt: 0 - valLoss: 0.4309631586074829 - trainLoss: 0.41494080424308777\n",
      "cnt: 0 - valLoss: 0.430962473154068 - trainLoss: 0.4149390459060669\n",
      "cnt: 0 - valLoss: 0.4309617877006531 - trainLoss: 0.41493722796440125\n",
      "cnt: 0 - valLoss: 0.43096110224723816 - trainLoss: 0.414935439825058\n",
      "cnt: 0 - valLoss: 0.43096041679382324 - trainLoss: 0.4149336516857147\n",
      "cnt: 0 - valLoss: 0.4309599697589874 - trainLoss: 0.4149318337440491\n",
      "cnt: 0 - valLoss: 0.4309592545032501 - trainLoss: 0.4149300158023834\n",
      "cnt: 0 - valLoss: 0.4309585988521576 - trainLoss: 0.4149281978607178\n",
      "cnt: 0 - valLoss: 0.4309579133987427 - trainLoss: 0.4149264395236969\n",
      "cnt: 0 - valLoss: 0.4309574067592621 - trainLoss: 0.41492465138435364\n",
      "cnt: 0 - valLoss: 0.43095672130584717 - trainLoss: 0.4149228632450104\n",
      "cnt: 0 - valLoss: 0.43095606565475464 - trainLoss: 0.41492101550102234\n",
      "cnt: 0 - valLoss: 0.43095535039901733 - trainLoss: 0.41491925716400146\n",
      "cnt: 0 - valLoss: 0.43095487356185913 - trainLoss: 0.4149174690246582\n",
      "cnt: 0 - valLoss: 0.430954247713089 - trainLoss: 0.41491565108299255\n",
      "cnt: 0 - valLoss: 0.4309535622596741 - trainLoss: 0.4149138629436493\n",
      "cnt: 0 - valLoss: 0.43095287680625916 - trainLoss: 0.41491201519966125\n",
      "cnt: 0 - valLoss: 0.4309523403644562 - trainLoss: 0.41491028666496277\n",
      "cnt: 0 - valLoss: 0.43095165491104126 - trainLoss: 0.41490843892097473\n",
      "cnt: 0 - valLoss: 0.4309510290622711 - trainLoss: 0.4149066209793091\n",
      "cnt: 0 - valLoss: 0.4309502840042114 - trainLoss: 0.4149048328399658\n",
      "cnt: 0 - valLoss: 0.430949866771698 - trainLoss: 0.41490307450294495\n",
      "cnt: 0 - valLoss: 0.4309491813182831 - trainLoss: 0.4149012267589569\n",
      "cnt: 0 - valLoss: 0.43094849586486816 - trainLoss: 0.41489943861961365\n",
      "cnt: 0 - valLoss: 0.43094781041145325 - trainLoss: 0.4148976504802704\n",
      "cnt: 0 - valLoss: 0.43094730377197266 - trainLoss: 0.4148958623409271\n",
      "cnt: 0 - valLoss: 0.4309466481208801 - trainLoss: 0.4148940443992615\n",
      "cnt: 0 - valLoss: 0.4309459626674652 - trainLoss: 0.4148922860622406\n",
      "cnt: 0 - valLoss: 0.4309452772140503 - trainLoss: 0.41489046812057495\n",
      "cnt: 0 - valLoss: 0.4309448003768921 - trainLoss: 0.4148886501789093\n",
      "cnt: 0 - valLoss: 0.4309441149234772 - trainLoss: 0.41488686203956604\n",
      "cnt: 0 - valLoss: 0.43094342947006226 - trainLoss: 0.4148850739002228\n",
      "cnt: 0 - valLoss: 0.43094274401664734 - trainLoss: 0.41488325595855713\n",
      "cnt: 0 - valLoss: 0.43094226717948914 - trainLoss: 0.41488146781921387\n",
      "cnt: 0 - valLoss: 0.4309415817260742 - trainLoss: 0.4148796796798706\n",
      "cnt: 0 - valLoss: 0.4309408962726593 - trainLoss: 0.41487786173820496\n",
      "cnt: 0 - valLoss: 0.43094027042388916 - trainLoss: 0.4148760735988617\n",
      "cnt: 0 - valLoss: 0.43093976378440857 - trainLoss: 0.41487428545951843\n",
      "cnt: 0 - valLoss: 0.43093907833099365 - trainLoss: 0.4148724675178528\n",
      "cnt: 0 - valLoss: 0.43093836307525635 - trainLoss: 0.4148707091808319\n",
      "cnt: 0 - valLoss: 0.43093767762184143 - trainLoss: 0.41486892104148865\n",
      "cnt: 0 - valLoss: 0.430937260389328 - trainLoss: 0.4148671329021454\n",
      "cnt: 0 - valLoss: 0.4309365749359131 - trainLoss: 0.4148653447628021\n",
      "cnt: 0 - valLoss: 0.43093588948249817 - trainLoss: 0.4148634970188141\n",
      "cnt: 0 - valLoss: 0.4309353828430176 - trainLoss: 0.4148617386817932\n",
      "cnt: 0 - valLoss: 0.43093469738960266 - trainLoss: 0.41485995054244995\n",
      "cnt: 0 - valLoss: 0.4309341013431549 - trainLoss: 0.4148581326007843\n",
      "cnt: 0 - valLoss: 0.4309333264827728 - trainLoss: 0.41485628485679626\n",
      "cnt: 0 - valLoss: 0.4309328496456146 - trainLoss: 0.414854496717453\n",
      "cnt: 0 - valLoss: 0.4309324622154236 - trainLoss: 0.4148527979850769\n",
      "cnt: 0 - valLoss: 0.4309321343898773 - trainLoss: 0.41485103964805603\n",
      "cnt: 0 - valLoss: 0.4309316873550415 - trainLoss: 0.41484931111335754\n",
      "cnt: 0 - valLoss: 0.43093129992485046 - trainLoss: 0.41484761238098145\n",
      "cnt: 0 - valLoss: 0.4309309124946594 - trainLoss: 0.41484585404396057\n",
      "cnt: 0 - valLoss: 0.4309305250644684 - trainLoss: 0.4148441553115845\n",
      "cnt: 0 - valLoss: 0.43093013763427734 - trainLoss: 0.414842426776886\n",
      "cnt: 0 - valLoss: 0.4309297204017639 - trainLoss: 0.4148407280445099\n",
      "cnt: 0 - valLoss: 0.4309293329715729 - trainLoss: 0.414838969707489\n",
      "cnt: 0 - valLoss: 0.43092894554138184 - trainLoss: 0.4148372411727905\n",
      "cnt: 0 - valLoss: 0.4309285283088684 - trainLoss: 0.41483554244041443\n",
      "cnt: 0 - valLoss: 0.43092817068099976 - trainLoss: 0.41483378410339355\n",
      "cnt: 0 - valLoss: 0.4309277832508087 - trainLoss: 0.41483208537101746\n",
      "cnt: 0 - valLoss: 0.4309272766113281 - trainLoss: 0.41483035683631897\n",
      "cnt: 0 - valLoss: 0.4309267997741699 - trainLoss: 0.41482865810394287\n",
      "cnt: 0 - valLoss: 0.4309263825416565 - trainLoss: 0.4148269295692444\n",
      "cnt: 0 - valLoss: 0.4309258759021759 - trainLoss: 0.4148252308368683\n",
      "cnt: 0 - valLoss: 0.4309253394603729 - trainLoss: 0.4148235023021698\n",
      "cnt: 0 - valLoss: 0.4309248626232147 - trainLoss: 0.4148218035697937\n",
      "cnt: 0 - valLoss: 0.4309244155883789 - trainLoss: 0.4148200750350952\n",
      "cnt: 0 - valLoss: 0.4309239685535431 - trainLoss: 0.4148183763027191\n",
      "cnt: 0 - valLoss: 0.4309234619140625 - trainLoss: 0.41481661796569824\n",
      "cnt: 0 - valLoss: 0.4309229552745819 - trainLoss: 0.41481491923332214\n",
      "cnt: 0 - valLoss: 0.4309224784374237 - trainLoss: 0.41481325030326843\n",
      "cnt: 0 - valLoss: 0.4309220016002655 - trainLoss: 0.41481149196624756\n",
      "cnt: 0 - valLoss: 0.4309215247631073 - trainLoss: 0.41480982303619385\n",
      "cnt: 0 - valLoss: 0.4309210181236267 - trainLoss: 0.41480809450149536\n",
      "cnt: 0 - valLoss: 0.4309205412864685 - trainLoss: 0.4148063361644745\n",
      "cnt: 0 - valLoss: 0.4309200644493103 - trainLoss: 0.4148046374320984\n",
      "cnt: 0 - valLoss: 0.4309196174144745 - trainLoss: 0.4148029386997223\n",
      "cnt: 0 - valLoss: 0.4309190809726715 - trainLoss: 0.4148012101650238\n",
      "cnt: 0 - valLoss: 0.4309186041355133 - trainLoss: 0.4147995114326477\n",
      "cnt: 0 - valLoss: 0.4309181571006775 - trainLoss: 0.4147977828979492\n",
      "cnt: 0 - valLoss: 0.4309176206588745 - trainLoss: 0.4147961139678955\n",
      "cnt: 0 - valLoss: 0.4309171736240387 - trainLoss: 0.41479435563087463\n",
      "cnt: 0 - valLoss: 0.4309166967868805 - trainLoss: 0.41479265689849854\n",
      "cnt: 0 - valLoss: 0.4309161901473999 - trainLoss: 0.41479092836380005\n",
      "cnt: 0 - valLoss: 0.4309156537055969 - trainLoss: 0.41478922963142395\n",
      "cnt: 0 - valLoss: 0.4309151768684387 - trainLoss: 0.41478756070137024\n",
      "cnt: 0 - valLoss: 0.4309147298336029 - trainLoss: 0.41478586196899414\n",
      "cnt: 0 - valLoss: 0.4309141933917999 - trainLoss: 0.41478410363197327\n",
      "cnt: 0 - valLoss: 0.4309137761592865 - trainLoss: 0.4147823750972748\n",
      "cnt: 0 - valLoss: 0.4309132695198059 - trainLoss: 0.4147806763648987\n",
      "cnt: 0 - valLoss: 0.43091273307800293 - trainLoss: 0.4147789478302002\n",
      "cnt: 0 - valLoss: 0.43091222643852234 - trainLoss: 0.4147772789001465\n",
      "cnt: 0 - valLoss: 0.43091174960136414 - trainLoss: 0.4147755205631256\n",
      "cnt: 0 - valLoss: 0.4309113025665283 - trainLoss: 0.4147738516330719\n",
      "cnt: 0 - valLoss: 0.43091076612472534 - trainLoss: 0.4147721529006958\n",
      "cnt: 0 - valLoss: 0.43091025948524475 - trainLoss: 0.4147704541683197\n",
      "cnt: 0 - valLoss: 0.43090975284576416 - trainLoss: 0.4147687256336212\n",
      "cnt: 0 - valLoss: 0.43090927600860596 - trainLoss: 0.4147670269012451\n",
      "cnt: 0 - valLoss: 0.43090879917144775 - trainLoss: 0.41476529836654663\n",
      "cnt: 0 - valLoss: 0.43090829253196716 - trainLoss: 0.41476354002952576\n",
      "cnt: 0 - valLoss: 0.4309077858924866 - trainLoss: 0.41476187109947205\n",
      "cnt: 0 - valLoss: 0.43090730905532837 - trainLoss: 0.41476017236709595\n",
      "cnt: 0 - valLoss: 0.43090683221817017 - trainLoss: 0.4147584140300751\n",
      "cnt: 0 - valLoss: 0.4309063255786896 - trainLoss: 0.41475674510002136\n",
      "cnt: 0 - valLoss: 0.4309057891368866 - trainLoss: 0.4147550165653229\n",
      "cnt: 0 - valLoss: 0.4309053421020508 - trainLoss: 0.4147533178329468\n",
      "cnt: 0 - valLoss: 0.4309048056602478 - trainLoss: 0.4147516191005707\n",
      "cnt: 0 - valLoss: 0.430904358625412 - trainLoss: 0.4147498905658722\n",
      "cnt: 0 - valLoss: 0.4309043884277344 - trainLoss: 0.4147481918334961\n",
      "cnt: 1 - valLoss: 0.43090441823005676 - trainLoss: 0.41474637389183044\n",
      "cnt: 2 - valLoss: 0.43090447783470154 - trainLoss: 0.4147445559501648\n",
      "cnt: 3 - valLoss: 0.4309045076370239 - trainLoss: 0.41474273800849915\n",
      "cnt: 4 - valLoss: 0.4309045672416687 - trainLoss: 0.4147409200668335\n",
      "cnt: 5 - valLoss: 0.4309045970439911 - trainLoss: 0.41473910212516785\n",
      "cnt: 6 - valLoss: 0.43090465664863586 - trainLoss: 0.414737343788147\n",
      "cnt: 7 - valLoss: 0.43090468645095825 - trainLoss: 0.4147355258464813\n",
      "cnt: 8 - valLoss: 0.430904746055603 - trainLoss: 0.4147337079048157\n",
      "cnt: 9 - valLoss: 0.430904746055603 - trainLoss: 0.4147319197654724\n",
      "cnt: 10 - valLoss: 0.4309047758579254 - trainLoss: 0.41473013162612915\n",
      "cnt: 11 - valLoss: 0.4309048056602478 - trainLoss: 0.4147283136844635\n",
      "cnt: 12 - valLoss: 0.4309048056602478 - trainLoss: 0.41472649574279785\n",
      "cnt: 13 - valLoss: 0.4309048652648926 - trainLoss: 0.414724737405777\n",
      "cnt: 14 - valLoss: 0.4309048652648926 - trainLoss: 0.41472288966178894\n",
      "cnt: 15 - valLoss: 0.4309048652648926 - trainLoss: 0.4147211015224457\n",
      "cnt: 16 - valLoss: 0.43090489506721497 - trainLoss: 0.4147193431854248\n",
      "cnt: 17 - valLoss: 0.4309048652648926 - trainLoss: 0.41471749544143677\n",
      "cnt: 18 - valLoss: 0.4309048652648926 - trainLoss: 0.4147157073020935\n",
      "cnt: 19 - valLoss: 0.4309048652648926 - trainLoss: 0.41471385955810547\n",
      "cnt: 20 - valLoss: 0.4309048652648926 - trainLoss: 0.4147120714187622\n",
      "cnt: 21 - valLoss: 0.4309048652648926 - trainLoss: 0.41471025347709656\n",
      "cnt: 22 - valLoss: 0.4309048652648926 - trainLoss: 0.4147084951400757\n",
      "cnt: 23 - valLoss: 0.4309048056602478 - trainLoss: 0.41470667719841003\n",
      "cnt: 24 - valLoss: 0.4309048056602478 - trainLoss: 0.4147048592567444\n",
      "cnt: 25 - valLoss: 0.4309048056602478 - trainLoss: 0.4147030711174011\n",
      "cnt: 26 - valLoss: 0.430904746055603 - trainLoss: 0.41470128297805786\n",
      "cnt: 27 - valLoss: 0.430904746055603 - trainLoss: 0.4146994650363922\n",
      "cnt: 28 - valLoss: 0.43090468645095825 - trainLoss: 0.41469767689704895\n",
      "cnt: 29 - valLoss: 0.43090465664863586 - trainLoss: 0.4146958887577057\n",
      "cnt: 30 - valLoss: 0.43090465664863586 - trainLoss: 0.41469407081604004\n",
      "cnt: 31 - valLoss: 0.4309045672416687 - trainLoss: 0.4146922826766968\n",
      "cnt: 32 - valLoss: 0.4309045076370239 - trainLoss: 0.4146904945373535\n",
      "cnt: 33 - valLoss: 0.43090447783470154 - trainLoss: 0.41468867659568787\n",
      "cnt: 34 - valLoss: 0.43090441823005676 - trainLoss: 0.4146868884563446\n",
      "cnt: 35 - valLoss: 0.43090441823005676 - trainLoss: 0.41468510031700134\n",
      "cnt: 36 - valLoss: 0.430904358625412 - trainLoss: 0.4146832823753357\n",
      "cnt: 0 - valLoss: 0.4309042990207672 - trainLoss: 0.41468149423599243\n",
      "cnt: 0 - valLoss: 0.4309042692184448 - trainLoss: 0.41467970609664917\n",
      "cnt: 0 - valLoss: 0.43090417981147766 - trainLoss: 0.4146778881549835\n",
      "cnt: 0 - valLoss: 0.4309041202068329 - trainLoss: 0.41467610001564026\n",
      "cnt: 0 - valLoss: 0.43090400099754333 - trainLoss: 0.414674311876297\n",
      "cnt: 0 - valLoss: 0.43090397119522095 - trainLoss: 0.41467252373695374\n",
      "cnt: 0 - valLoss: 0.4309038817882538 - trainLoss: 0.4146707057952881\n",
      "cnt: 0 - valLoss: 0.430903822183609 - trainLoss: 0.4146689176559448\n",
      "cnt: 0 - valLoss: 0.43090373277664185 - trainLoss: 0.41466712951660156\n",
      "cnt: 0 - valLoss: 0.43090367317199707 - trainLoss: 0.4146653115749359\n",
      "cnt: 0 - valLoss: 0.4309035837650299 - trainLoss: 0.41466352343559265\n",
      "cnt: 0 - valLoss: 0.43090343475341797 - trainLoss: 0.4146617650985718\n",
      "cnt: 0 - valLoss: 0.4309034049510956 - trainLoss: 0.41465991735458374\n",
      "cnt: 0 - valLoss: 0.43090328574180603 - trainLoss: 0.41465821862220764\n",
      "cnt: 0 - valLoss: 0.43090322613716125 - trainLoss: 0.4146563708782196\n",
      "cnt: 0 - valLoss: 0.4309031367301941 - trainLoss: 0.41465458273887634\n",
      "cnt: 0 - valLoss: 0.43090298771858215 - trainLoss: 0.4146527647972107\n",
      "cnt: 0 - valLoss: 0.430902898311615 - trainLoss: 0.4146510362625122\n",
      "cnt: 0 - valLoss: 0.4309028089046478 - trainLoss: 0.41464918851852417\n",
      "cnt: 0 - valLoss: 0.43090271949768066 - trainLoss: 0.4146474301815033\n",
      "cnt: 0 - valLoss: 0.43090254068374634 - trainLoss: 0.41464558243751526\n",
      "cnt: 0 - valLoss: 0.4309024512767792 - trainLoss: 0.4146438241004944\n",
      "cnt: 0 - valLoss: 0.4309023320674896 - trainLoss: 0.4146420359611511\n",
      "cnt: 0 - valLoss: 0.4309022128582001 - trainLoss: 0.41464024782180786\n",
      "cnt: 0 - valLoss: 0.4309021234512329 - trainLoss: 0.4146384298801422\n",
      "cnt: 0 - valLoss: 0.43090197443962097 - trainLoss: 0.41463667154312134\n",
      "cnt: 0 - valLoss: 0.4309018552303314 - trainLoss: 0.4146348536014557\n",
      "cnt: 0 - valLoss: 0.43090173602104187 - trainLoss: 0.4146330952644348\n",
      "cnt: 0 - valLoss: 0.43090155720710754 - trainLoss: 0.41463127732276917\n",
      "cnt: 0 - valLoss: 0.4309014678001404 - trainLoss: 0.4146294891834259\n",
      "cnt: 0 - valLoss: 0.43090134859085083 - trainLoss: 0.41462770104408264\n",
      "cnt: 0 - valLoss: 0.4309011697769165 - trainLoss: 0.4146259129047394\n",
      "cnt: 0 - valLoss: 0.43090105056762695 - trainLoss: 0.4146241545677185\n",
      "cnt: 0 - valLoss: 0.4309009313583374 - trainLoss: 0.41462233662605286\n",
      "cnt: 0 - valLoss: 0.43090078234672546 - trainLoss: 0.4146205484867096\n",
      "cnt: 0 - valLoss: 0.43090060353279114 - trainLoss: 0.41461876034736633\n",
      "cnt: 0 - valLoss: 0.4309004545211792 - trainLoss: 0.4146169424057007\n",
      "cnt: 0 - valLoss: 0.43090030550956726 - trainLoss: 0.4146151840686798\n",
      "cnt: 0 - valLoss: 0.4309001564979553 - trainLoss: 0.4146134555339813\n",
      "cnt: 0 - valLoss: 0.430899977684021 - trainLoss: 0.4146116077899933\n",
      "cnt: 0 - valLoss: 0.43089985847473145 - trainLoss: 0.4146098494529724\n",
      "cnt: 0 - valLoss: 0.4308996796607971 - trainLoss: 0.41460806131362915\n",
      "cnt: 0 - valLoss: 0.4308995306491852 - trainLoss: 0.4146062433719635\n",
      "cnt: 0 - valLoss: 0.43089932203292847 - trainLoss: 0.41460445523262024\n",
      "cnt: 0 - valLoss: 0.43089917302131653 - trainLoss: 0.414602667093277\n",
      "cnt: 0 - valLoss: 0.4308989942073822 - trainLoss: 0.41460084915161133\n",
      "cnt: 0 - valLoss: 0.4308988153934479 - trainLoss: 0.41459906101226807\n",
      "cnt: 0 - valLoss: 0.43089863657951355 - trainLoss: 0.4145973026752472\n",
      "cnt: 0 - valLoss: 0.4308984875679016 - trainLoss: 0.41459551453590393\n",
      "cnt: 0 - valLoss: 0.4308982491493225 - trainLoss: 0.41459375619888306\n",
      "cnt: 0 - valLoss: 0.43089810013771057 - trainLoss: 0.4145919680595398\n",
      "cnt: 0 - valLoss: 0.43089792132377625 - trainLoss: 0.41459015011787415\n",
      "cnt: 0 - valLoss: 0.4308977425098419 - trainLoss: 0.4145883619785309\n",
      "cnt: 0 - valLoss: 0.4308975338935852 - trainLoss: 0.41458660364151\n",
      "cnt: 0 - valLoss: 0.4308973550796509 - trainLoss: 0.41458484530448914\n",
      "cnt: 0 - valLoss: 0.43089714646339417 - trainLoss: 0.4145830571651459\n",
      "cnt: 0 - valLoss: 0.43089696764945984 - trainLoss: 0.4145812690258026\n",
      "cnt: 0 - valLoss: 0.43089672923088074 - trainLoss: 0.41457945108413696\n",
      "cnt: 0 - valLoss: 0.4308965504169464 - trainLoss: 0.4145776629447937\n",
      "cnt: 0 - valLoss: 0.4308963418006897 - trainLoss: 0.4145759046077728\n",
      "cnt: 0 - valLoss: 0.43089616298675537 - trainLoss: 0.41457411646842957\n",
      "cnt: 0 - valLoss: 0.43089595437049866 - trainLoss: 0.4145723581314087\n",
      "cnt: 0 - valLoss: 0.43089577555656433 - trainLoss: 0.41457056999206543\n",
      "cnt: 0 - valLoss: 0.4308955669403076 - trainLoss: 0.4145687520503998\n",
      "cnt: 0 - valLoss: 0.4308953583240509 - trainLoss: 0.4145669639110565\n",
      "cnt: 0 - valLoss: 0.4308951199054718 - trainLoss: 0.41456520557403564\n",
      "cnt: 0 - valLoss: 0.43089497089385986 - trainLoss: 0.41456344723701477\n",
      "cnt: 0 - valLoss: 0.43089473247528076 - trainLoss: 0.4145616590976715\n",
      "cnt: 0 - valLoss: 0.43089452385902405 - trainLoss: 0.41455984115600586\n",
      "cnt: 0 - valLoss: 0.43089431524276733 - trainLoss: 0.4145580530166626\n",
      "cnt: 0 - valLoss: 0.430894136428833 - trainLoss: 0.4145562946796417\n",
      "cnt: 0 - valLoss: 0.4308939278125763 - trainLoss: 0.41455453634262085\n",
      "cnt: 0 - valLoss: 0.4308936595916748 - trainLoss: 0.4145527482032776\n",
      "cnt: 0 - valLoss: 0.4308934509754181 - trainLoss: 0.4145509600639343\n",
      "cnt: 0 - valLoss: 0.4308932423591614 - trainLoss: 0.4145491421222687\n",
      "cnt: 0 - valLoss: 0.43089303374290466 - trainLoss: 0.4145473539829254\n",
      "cnt: 0 - valLoss: 0.4308927655220032 - trainLoss: 0.41454559564590454\n",
      "cnt: 0 - valLoss: 0.43089255690574646 - trainLoss: 0.41454383730888367\n",
      "cnt: 0 - valLoss: 0.43089228868484497 - trainLoss: 0.4145420491695404\n",
      "cnt: 0 - valLoss: 0.43089208006858826 - trainLoss: 0.41454026103019714\n",
      "cnt: 0 - valLoss: 0.43089184165000916 - trainLoss: 0.41453850269317627\n",
      "cnt: 0 - valLoss: 0.43089160323143005 - trainLoss: 0.4145366847515106\n",
      "cnt: 0 - valLoss: 0.43089136481285095 - trainLoss: 0.41453489661216736\n",
      "cnt: 0 - valLoss: 0.43089115619659424 - trainLoss: 0.4145331382751465\n",
      "cnt: 0 - valLoss: 0.43089088797569275 - trainLoss: 0.4145313799381256\n",
      "cnt: 0 - valLoss: 0.43089061975479126 - trainLoss: 0.41452959179878235\n",
      "cnt: 0 - valLoss: 0.43089038133621216 - trainLoss: 0.4145278334617615\n",
      "cnt: 0 - valLoss: 0.43089011311531067 - trainLoss: 0.4145260453224182\n",
      "cnt: 0 - valLoss: 0.43088990449905396 - trainLoss: 0.41452425718307495\n",
      "cnt: 0 - valLoss: 0.43088963627815247 - trainLoss: 0.4145224392414093\n",
      "cnt: 0 - valLoss: 0.43088942766189575 - trainLoss: 0.4145206809043884\n",
      "cnt: 0 - valLoss: 0.4308891296386719 - trainLoss: 0.41451892256736755\n",
      "cnt: 0 - valLoss: 0.43088892102241516 - trainLoss: 0.4145171344280243\n",
      "cnt: 0 - valLoss: 0.43088865280151367 - trainLoss: 0.4145153760910034\n",
      "cnt: 0 - valLoss: 0.43088841438293457 - trainLoss: 0.41451361775398254\n",
      "cnt: 0 - valLoss: 0.43088802695274353 - trainLoss: 0.4145118296146393\n",
      "cnt: 0 - valLoss: 0.4308876395225525 - trainLoss: 0.414510041475296\n",
      "cnt: 0 - valLoss: 0.43088728189468384 - trainLoss: 0.41450822353363037\n",
      "cnt: 0 - valLoss: 0.4308869540691376 - trainLoss: 0.4145064949989319\n",
      "cnt: 0 - valLoss: 0.43088656663894653 - trainLoss: 0.414504736661911\n",
      "cnt: 0 - valLoss: 0.4308861792087555 - trainLoss: 0.41450291872024536\n",
      "cnt: 0 - valLoss: 0.43088582158088684 - trainLoss: 0.4145011603832245\n",
      "cnt: 0 - valLoss: 0.4308854043483734 - trainLoss: 0.414499431848526\n",
      "cnt: 0 - valLoss: 0.43088504672050476 - trainLoss: 0.41449761390686035\n",
      "cnt: 0 - valLoss: 0.4308847188949585 - trainLoss: 0.4144958555698395\n",
      "cnt: 0 - valLoss: 0.4308842718601227 - trainLoss: 0.4144940972328186\n",
      "cnt: 0 - valLoss: 0.4308839440345764 - trainLoss: 0.41449230909347534\n",
      "cnt: 0 - valLoss: 0.430883526802063 - trainLoss: 0.41449055075645447\n",
      "cnt: 0 - valLoss: 0.43088316917419434 - trainLoss: 0.4144887924194336\n",
      "cnt: 0 - valLoss: 0.4308827519416809 - trainLoss: 0.41448700428009033\n",
      "cnt: 0 - valLoss: 0.43088245391845703 - trainLoss: 0.41448524594306946\n",
      "cnt: 0 - valLoss: 0.4308820068836212 - trainLoss: 0.4144834876060486\n",
      "cnt: 0 - valLoss: 0.4308816194534302 - trainLoss: 0.4144816994667053\n",
      "cnt: 0 - valLoss: 0.4308812916278839 - trainLoss: 0.41447994112968445\n",
      "cnt: 0 - valLoss: 0.4308808445930481 - trainLoss: 0.4144781827926636\n",
      "cnt: 0 - valLoss: 0.43088045716285706 - trainLoss: 0.4144763946533203\n",
      "cnt: 0 - valLoss: 0.4308800995349884 - trainLoss: 0.41447463631629944\n",
      "cnt: 0 - valLoss: 0.43087971210479736 - trainLoss: 0.41447287797927856\n",
      "cnt: 0 - valLoss: 0.4308793246746063 - trainLoss: 0.4144710898399353\n",
      "cnt: 0 - valLoss: 0.4308788776397705 - trainLoss: 0.41446933150291443\n",
      "cnt: 0 - valLoss: 0.43087849020957947 - trainLoss: 0.41446757316589355\n",
      "cnt: 0 - valLoss: 0.4308781027793884 - trainLoss: 0.4144657850265503\n",
      "cnt: 0 - valLoss: 0.430877685546875 - trainLoss: 0.4144640266895294\n",
      "cnt: 0 - valLoss: 0.43087729811668396 - trainLoss: 0.41446226835250854\n",
      "cnt: 0 - valLoss: 0.4308769106864929 - trainLoss: 0.4144604802131653\n",
      "cnt: 0 - valLoss: 0.4308764934539795 - trainLoss: 0.4144587218761444\n",
      "cnt: 0 - valLoss: 0.43087610602378845 - trainLoss: 0.41445696353912354\n",
      "cnt: 0 - valLoss: 0.43087565898895264 - trainLoss: 0.41445520520210266\n",
      "cnt: 0 - valLoss: 0.4308752715587616 - trainLoss: 0.4144534468650818\n",
      "cnt: 0 - valLoss: 0.43087485432624817 - trainLoss: 0.41445159912109375\n",
      "cnt: 0 - valLoss: 0.43087443709373474 - trainLoss: 0.4144498407840729\n",
      "cnt: 0 - valLoss: 0.4308740496635437 - trainLoss: 0.4144481420516968\n",
      "cnt: 0 - valLoss: 0.43087366223335266 - trainLoss: 0.4144463539123535\n",
      "cnt: 0 - valLoss: 0.43087318539619446 - trainLoss: 0.41444453597068787\n",
      "cnt: 0 - valLoss: 0.4308727979660034 - trainLoss: 0.41444283723831177\n",
      "cnt: 0 - valLoss: 0.43087238073349 - trainLoss: 0.4144410490989685\n",
      "cnt: 0 - valLoss: 0.4308719336986542 - trainLoss: 0.41443923115730286\n",
      "cnt: 0 - valLoss: 0.43087151646614075 - trainLoss: 0.41443753242492676\n",
      "cnt: 0 - valLoss: 0.4308710992336273 - trainLoss: 0.4144357442855835\n",
      "cnt: 0 - valLoss: 0.4308706521987915 - trainLoss: 0.4144339859485626\n",
      "cnt: 0 - valLoss: 0.4308702349662781 - trainLoss: 0.41443222761154175\n",
      "cnt: 0 - valLoss: 0.43086978793144226 - trainLoss: 0.4144304692745209\n",
      "cnt: 0 - valLoss: 0.43086937069892883 - trainLoss: 0.4144287109375\n",
      "cnt: 0 - valLoss: 0.4308689534664154 - trainLoss: 0.41442692279815674\n",
      "cnt: 0 - valLoss: 0.4308685064315796 - trainLoss: 0.4144251048564911\n",
      "cnt: 0 - valLoss: 0.43086808919906616 - trainLoss: 0.414423406124115\n",
      "cnt: 0 - valLoss: 0.43086764216423035 - trainLoss: 0.41442161798477173\n",
      "cnt: 0 - valLoss: 0.4308672249317169 - trainLoss: 0.41441985964775085\n",
      "cnt: 0 - valLoss: 0.4308668076992035 - trainLoss: 0.41441810131073\n",
      "cnt: 0 - valLoss: 0.4308663606643677 - trainLoss: 0.4144163131713867\n",
      "cnt: 0 - valLoss: 0.4308658838272095 - trainLoss: 0.41441455483436584\n",
      "cnt: 0 - valLoss: 0.43086543679237366 - trainLoss: 0.41441279649734497\n",
      "cnt: 0 - valLoss: 0.43086498975753784 - trainLoss: 0.4144110381603241\n",
      "cnt: 0 - valLoss: 0.4308645725250244 - trainLoss: 0.41440925002098083\n",
      "cnt: 0 - valLoss: 0.430864155292511 - trainLoss: 0.41440749168395996\n",
      "cnt: 0 - valLoss: 0.4308636784553528 - trainLoss: 0.4144057333469391\n",
      "cnt: 0 - valLoss: 0.4308632016181946 - trainLoss: 0.4144039750099182\n",
      "cnt: 0 - valLoss: 0.43086278438568115 - trainLoss: 0.41440218687057495\n",
      "cnt: 0 - valLoss: 0.43086233735084534 - trainLoss: 0.4144004285335541\n",
      "cnt: 0 - valLoss: 0.4308619201183319 - trainLoss: 0.4143986701965332\n",
      "cnt: 0 - valLoss: 0.4308614730834961 - trainLoss: 0.41439685225486755\n",
      "cnt: 0 - valLoss: 0.43086105585098267 - trainLoss: 0.41439512372016907\n",
      "cnt: 0 - valLoss: 0.43086057901382446 - trainLoss: 0.4143933653831482\n",
      "cnt: 0 - valLoss: 0.43086016178131104 - trainLoss: 0.4143916070461273\n",
      "cnt: 0 - valLoss: 0.4308597147464752 - trainLoss: 0.41438984870910645\n",
      "cnt: 0 - valLoss: 0.4308592677116394 - trainLoss: 0.4143880605697632\n",
      "cnt: 0 - valLoss: 0.4308588206768036 - trainLoss: 0.4143863022327423\n",
      "cnt: 0 - valLoss: 0.43085840344429016 - trainLoss: 0.41438454389572144\n",
      "cnt: 0 - valLoss: 0.43085792660713196 - trainLoss: 0.41438278555870056\n",
      "cnt: 0 - valLoss: 0.43085750937461853 - trainLoss: 0.4143810272216797\n",
      "cnt: 0 - valLoss: 0.4308570623397827 - trainLoss: 0.4143792688846588\n",
      "cnt: 0 - valLoss: 0.4308565855026245 - trainLoss: 0.41437751054763794\n",
      "cnt: 0 - valLoss: 0.4308561682701111 - trainLoss: 0.4143757224082947\n",
      "cnt: 0 - valLoss: 0.4308556616306305 - trainLoss: 0.4143739640712738\n",
      "cnt: 0 - valLoss: 0.4308552145957947 - trainLoss: 0.41437220573425293\n",
      "cnt: 0 - valLoss: 0.43085479736328125 - trainLoss: 0.41437044739723206\n",
      "cnt: 0 - valLoss: 0.4308543801307678 - trainLoss: 0.4143686890602112\n",
      "cnt: 0 - valLoss: 0.4308539032936096 - trainLoss: 0.4143669009208679\n",
      "cnt: 0 - valLoss: 0.4308534562587738 - trainLoss: 0.4143652021884918\n",
      "cnt: 0 - valLoss: 0.430853009223938 - trainLoss: 0.41436338424682617\n",
      "cnt: 0 - valLoss: 0.4308525323867798 - trainLoss: 0.4143616259098053\n",
      "cnt: 0 - valLoss: 0.4308520555496216 - trainLoss: 0.41435983777046204\n",
      "cnt: 0 - valLoss: 0.43085163831710815 - trainLoss: 0.41435813903808594\n",
      "cnt: 0 - valLoss: 0.43085119128227234 - trainLoss: 0.41435638070106506\n",
      "cnt: 0 - valLoss: 0.43085068464279175 - trainLoss: 0.4143545925617218\n",
      "cnt: 0 - valLoss: 0.4308502674102783 - trainLoss: 0.4143528342247009\n",
      "cnt: 0 - valLoss: 0.4308498203754425 - trainLoss: 0.41435107588768005\n",
      "cnt: 0 - valLoss: 0.4308493137359619 - trainLoss: 0.4143493175506592\n",
      "cnt: 0 - valLoss: 0.4308488965034485 - trainLoss: 0.4143475592136383\n",
      "cnt: 0 - valLoss: 0.4308484196662903 - trainLoss: 0.41434577107429504\n",
      "cnt: 0 - valLoss: 0.4308479428291321 - trainLoss: 0.41434404253959656\n",
      "cnt: 0 - valLoss: 0.4308474659919739 - trainLoss: 0.4143422544002533\n",
      "cnt: 0 - valLoss: 0.4308469891548157 - trainLoss: 0.4143405258655548\n",
      "cnt: 0 - valLoss: 0.43084651231765747 - trainLoss: 0.41433873772621155\n",
      "cnt: 0 - valLoss: 0.43084606528282166 - trainLoss: 0.4143369793891907\n",
      "cnt: 0 - valLoss: 0.43084561824798584 - trainLoss: 0.4143352210521698\n",
      "cnt: 0 - valLoss: 0.43084511160850525 - trainLoss: 0.41433343291282654\n",
      "cnt: 0 - valLoss: 0.4308446943759918 - trainLoss: 0.41433173418045044\n",
      "cnt: 0 - valLoss: 0.43084415793418884 - trainLoss: 0.4143299162387848\n",
      "cnt: 0 - valLoss: 0.4308437407016754 - trainLoss: 0.4143282175064087\n",
      "cnt: 0 - valLoss: 0.4308432340621948 - trainLoss: 0.41432642936706543\n",
      "cnt: 0 - valLoss: 0.4308427572250366 - trainLoss: 0.41432467103004456\n",
      "cnt: 0 - valLoss: 0.4308422803878784 - trainLoss: 0.4143229126930237\n",
      "cnt: 0 - valLoss: 0.4308418035507202 - trainLoss: 0.41432109475135803\n",
      "cnt: 0 - valLoss: 0.430841326713562 - trainLoss: 0.41431939601898193\n",
      "cnt: 0 - valLoss: 0.4308408796787262 - trainLoss: 0.41431763768196106\n",
      "cnt: 0 - valLoss: 0.430840402841568 - trainLoss: 0.4143158793449402\n",
      "cnt: 0 - valLoss: 0.4308398962020874 - trainLoss: 0.4143140912055969\n",
      "cnt: 0 - valLoss: 0.4308394491672516 - trainLoss: 0.41431233286857605\n",
      "cnt: 0 - valLoss: 0.430838942527771 - trainLoss: 0.4143105745315552\n",
      "cnt: 0 - valLoss: 0.4308384656906128 - trainLoss: 0.4143088161945343\n",
      "cnt: 0 - valLoss: 0.4308379888534546 - trainLoss: 0.4143070578575134\n",
      "cnt: 0 - valLoss: 0.430837482213974 - trainLoss: 0.41430529952049255\n",
      "cnt: 0 - valLoss: 0.4308369755744934 - trainLoss: 0.41430357098579407\n",
      "cnt: 0 - valLoss: 0.4308365285396576 - trainLoss: 0.4143017530441284\n",
      "cnt: 0 - valLoss: 0.4308360815048218 - trainLoss: 0.4143000543117523\n",
      "cnt: 0 - valLoss: 0.4308355450630188 - trainLoss: 0.41429829597473145\n",
      "cnt: 0 - valLoss: 0.4308350682258606 - trainLoss: 0.41429653763771057\n",
      "cnt: 0 - valLoss: 0.43083456158638 - trainLoss: 0.4142947494983673\n",
      "cnt: 0 - valLoss: 0.4308340847492218 - trainLoss: 0.4142930209636688\n",
      "cnt: 0 - valLoss: 0.430833637714386 - trainLoss: 0.41429123282432556\n",
      "cnt: 0 - valLoss: 0.430833101272583 - trainLoss: 0.41428953409194946\n",
      "cnt: 0 - valLoss: 0.4308325946331024 - trainLoss: 0.4142877161502838\n",
      "cnt: 0 - valLoss: 0.4308321177959442 - trainLoss: 0.41428595781326294\n",
      "cnt: 0 - valLoss: 0.430831640958786 - trainLoss: 0.41428419947624207\n",
      "cnt: 0 - valLoss: 0.4308311343193054 - trainLoss: 0.4142824709415436\n",
      "cnt: 0 - valLoss: 0.4308306574821472 - trainLoss: 0.4142807126045227\n",
      "cnt: 0 - valLoss: 0.430830180644989 - trainLoss: 0.41427895426750183\n",
      "cnt: 0 - valLoss: 0.4308296740055084 - trainLoss: 0.41427719593048096\n",
      "cnt: 0 - valLoss: 0.43082916736602783 - trainLoss: 0.4142754077911377\n",
      "cnt: 0 - valLoss: 0.43082869052886963 - trainLoss: 0.4142736792564392\n",
      "cnt: 0 - valLoss: 0.43082818388938904 - trainLoss: 0.41427192091941833\n",
      "cnt: 0 - valLoss: 0.43082770705223083 - trainLoss: 0.41427016258239746\n",
      "cnt: 0 - valLoss: 0.43082720041275024 - trainLoss: 0.4142684042453766\n",
      "cnt: 0 - valLoss: 0.43082669377326965 - trainLoss: 0.4142666757106781\n",
      "cnt: 0 - valLoss: 0.4308261573314667 - trainLoss: 0.41426485776901245\n",
      "cnt: 0 - valLoss: 0.4308256506919861 - trainLoss: 0.41426315903663635\n",
      "cnt: 0 - valLoss: 0.4308251440525055 - trainLoss: 0.4142613708972931\n",
      "cnt: 0 - valLoss: 0.4308246672153473 - trainLoss: 0.4142596423625946\n",
      "cnt: 0 - valLoss: 0.4308241009712219 - trainLoss: 0.41425785422325134\n",
      "cnt: 0 - valLoss: 0.4308236241340637 - trainLoss: 0.41425612568855286\n",
      "cnt: 0 - valLoss: 0.43082311749458313 - trainLoss: 0.4142543375492096\n",
      "cnt: 0 - valLoss: 0.43082261085510254 - trainLoss: 0.4142526388168335\n",
      "cnt: 0 - valLoss: 0.43082210421562195 - trainLoss: 0.4142508804798126\n",
      "cnt: 0 - valLoss: 0.43082162737846375 - trainLoss: 0.41424912214279175\n",
      "cnt: 0 - valLoss: 0.43082112073898315 - trainLoss: 0.4142473638057709\n",
      "cnt: 0 - valLoss: 0.4308205544948578 - trainLoss: 0.41424560546875\n",
      "cnt: 0 - valLoss: 0.4308200478553772 - trainLoss: 0.41424381732940674\n",
      "cnt: 0 - valLoss: 0.430819571018219 - trainLoss: 0.41424205899238586\n",
      "cnt: 0 - valLoss: 0.43081897497177124 - trainLoss: 0.414240300655365\n",
      "cnt: 0 - valLoss: 0.43081849813461304 - trainLoss: 0.4142385721206665\n",
      "cnt: 0 - valLoss: 0.43081799149513245 - trainLoss: 0.41423678398132324\n",
      "cnt: 0 - valLoss: 0.4308174252510071 - trainLoss: 0.41423502564430237\n",
      "cnt: 0 - valLoss: 0.4308169186115265 - trainLoss: 0.41423332691192627\n",
      "cnt: 0 - valLoss: 0.4308163821697235 - trainLoss: 0.4142315089702606\n",
      "cnt: 0 - valLoss: 0.4308158755302429 - trainLoss: 0.41422978043556213\n",
      "cnt: 0 - valLoss: 0.43081536889076233 - trainLoss: 0.41422802209854126\n",
      "cnt: 0 - valLoss: 0.43081480264663696 - trainLoss: 0.4142262637615204\n",
      "cnt: 0 - valLoss: 0.43081429600715637 - trainLoss: 0.4142245054244995\n",
      "cnt: 0 - valLoss: 0.43081381916999817 - trainLoss: 0.41422274708747864\n",
      "cnt: 0 - valLoss: 0.4308132529258728 - trainLoss: 0.41422104835510254\n",
      "cnt: 0 - valLoss: 0.4308127164840698 - trainLoss: 0.4142192304134369\n",
      "cnt: 0 - valLoss: 0.43081218004226685 - trainLoss: 0.414217472076416\n",
      "cnt: 0 - valLoss: 0.43081167340278625 - trainLoss: 0.41421574354171753\n",
      "cnt: 0 - valLoss: 0.43081116676330566 - trainLoss: 0.41421398520469666\n",
      "cnt: 0 - valLoss: 0.4308106601238251 - trainLoss: 0.4142122268676758\n",
      "cnt: 0 - valLoss: 0.4308100938796997 - trainLoss: 0.41421040892601013\n",
      "cnt: 0 - valLoss: 0.43080952763557434 - trainLoss: 0.41420871019363403\n",
      "cnt: 0 - valLoss: 0.43080905079841614 - trainLoss: 0.41420695185661316\n",
      "cnt: 0 - valLoss: 0.43080854415893555 - trainLoss: 0.4142051935195923\n",
      "cnt: 0 - valLoss: 0.4308079779148102 - trainLoss: 0.4142034351825714\n",
      "cnt: 0 - valLoss: 0.4308074712753296 - trainLoss: 0.41420167684555054\n",
      "cnt: 0 - valLoss: 0.430806964635849 - trainLoss: 0.41419997811317444\n",
      "cnt: 0 - valLoss: 0.4308064579963684 - trainLoss: 0.4141981899738312\n",
      "cnt: 0 - valLoss: 0.43080592155456543 - trainLoss: 0.4141964316368103\n",
      "cnt: 0 - valLoss: 0.43080541491508484 - trainLoss: 0.41419467329978943\n",
      "cnt: 0 - valLoss: 0.43080490827560425 - trainLoss: 0.41419291496276855\n",
      "cnt: 0 - valLoss: 0.43080440163612366 - trainLoss: 0.4141911566257477\n",
      "cnt: 0 - valLoss: 0.4308038651943207 - trainLoss: 0.4141893982887268\n",
      "cnt: 0 - valLoss: 0.4308033585548401 - trainLoss: 0.4141876995563507\n",
      "cnt: 0 - valLoss: 0.4308027923107147 - trainLoss: 0.41418594121932983\n",
      "cnt: 0 - valLoss: 0.4308023452758789 - trainLoss: 0.4141841530799866\n",
      "cnt: 0 - valLoss: 0.43080177903175354 - trainLoss: 0.4141824245452881\n",
      "cnt: 0 - valLoss: 0.43080127239227295 - trainLoss: 0.4141806364059448\n",
      "cnt: 0 - valLoss: 0.43080073595046997 - trainLoss: 0.41417890787124634\n",
      "cnt: 0 - valLoss: 0.430800199508667 - trainLoss: 0.41417717933654785\n",
      "cnt: 0 - valLoss: 0.430799663066864 - trainLoss: 0.4141753613948822\n",
      "cnt: 0 - valLoss: 0.4307991564273834 - trainLoss: 0.4141736626625061\n",
      "cnt: 0 - valLoss: 0.43079861998558044 - trainLoss: 0.41417190432548523\n",
      "cnt: 0 - valLoss: 0.43079808354377747 - trainLoss: 0.41417014598846436\n",
      "cnt: 0 - valLoss: 0.4307975769042969 - trainLoss: 0.4141683876514435\n",
      "cnt: 0 - valLoss: 0.4307970106601715 - trainLoss: 0.4141666293144226\n",
      "cnt: 0 - valLoss: 0.4307965040206909 - trainLoss: 0.41416487097740173\n",
      "cnt: 0 - valLoss: 0.4307959973812103 - trainLoss: 0.41416311264038086\n",
      "cnt: 0 - valLoss: 0.43079543113708496 - trainLoss: 0.4141613841056824\n",
      "cnt: 0 - valLoss: 0.43079495429992676 - trainLoss: 0.4141596257686615\n",
      "cnt: 0 - valLoss: 0.4307944178581238 - trainLoss: 0.4141578674316406\n",
      "cnt: 0 - valLoss: 0.4307938516139984 - trainLoss: 0.41415613889694214\n",
      "cnt: 0 - valLoss: 0.4307933449745178 - trainLoss: 0.4141543507575989\n",
      "cnt: 0 - valLoss: 0.43079280853271484 - trainLoss: 0.4141526520252228\n",
      "cnt: 0 - valLoss: 0.43079227209091187 - trainLoss: 0.4141508936882019\n",
      "cnt: 0 - valLoss: 0.4307917058467865 - trainLoss: 0.41414913535118103\n",
      "cnt: 0 - valLoss: 0.4307911992073059 - trainLoss: 0.41414737701416016\n",
      "cnt: 0 - valLoss: 0.43079066276550293 - trainLoss: 0.4141456186771393\n",
      "cnt: 0 - valLoss: 0.43079012632369995 - trainLoss: 0.4141439199447632\n",
      "cnt: 0 - valLoss: 0.430789589881897 - trainLoss: 0.41414210200309753\n",
      "cnt: 0 - valLoss: 0.430789053440094 - trainLoss: 0.41414040327072144\n",
      "cnt: 0 - valLoss: 0.4307885468006134 - trainLoss: 0.4141386151313782\n",
      "cnt: 0 - valLoss: 0.43078798055648804 - trainLoss: 0.4141368567943573\n",
      "cnt: 0 - valLoss: 0.43078741431236267 - trainLoss: 0.4141350984573364\n",
      "cnt: 0 - valLoss: 0.4307869076728821 - trainLoss: 0.41413336992263794\n",
      "cnt: 0 - valLoss: 0.4307864010334015 - trainLoss: 0.4141315817832947\n",
      "cnt: 0 - valLoss: 0.4307858347892761 - trainLoss: 0.4141298830509186\n",
      "cnt: 0 - valLoss: 0.43078532814979553 - trainLoss: 0.4141281247138977\n",
      "cnt: 0 - valLoss: 0.43078476190567017 - trainLoss: 0.41412636637687683\n",
      "cnt: 0 - valLoss: 0.4307842552661896 - trainLoss: 0.41412460803985596\n",
      "cnt: 0 - valLoss: 0.4307836890220642 - trainLoss: 0.4141228497028351\n",
      "cnt: 0 - valLoss: 0.4307831823825836 - trainLoss: 0.414121150970459\n",
      "cnt: 0 - valLoss: 0.43078261613845825 - trainLoss: 0.41411933302879333\n",
      "cnt: 0 - valLoss: 0.43078210949897766 - trainLoss: 0.41411757469177246\n",
      "cnt: 0 - valLoss: 0.4307815432548523 - trainLoss: 0.4141158163547516\n",
      "cnt: 0 - valLoss: 0.43078097701072693 - trainLoss: 0.4141141176223755\n",
      "cnt: 0 - valLoss: 0.43078047037124634 - trainLoss: 0.4141123294830322\n",
      "cnt: 0 - valLoss: 0.43077990412712097 - trainLoss: 0.41411060094833374\n",
      "cnt: 0 - valLoss: 0.430779367685318 - trainLoss: 0.41410884261131287\n",
      "cnt: 0 - valLoss: 0.4307788014411926 - trainLoss: 0.4141071140766144\n",
      "cnt: 0 - valLoss: 0.43077823519706726 - trainLoss: 0.4141053855419159\n",
      "cnt: 0 - valLoss: 0.43077772855758667 - trainLoss: 0.41410359740257263\n",
      "cnt: 0 - valLoss: 0.4307771623134613 - trainLoss: 0.41410186886787415\n",
      "cnt: 0 - valLoss: 0.4307766258716583 - trainLoss: 0.4141000807285309\n",
      "cnt: 0 - valLoss: 0.43077605962753296 - trainLoss: 0.41409832239151\n",
      "cnt: 0 - valLoss: 0.4307754933834076 - trainLoss: 0.4140966236591339\n",
      "cnt: 0 - valLoss: 0.430774986743927 - trainLoss: 0.41409486532211304\n",
      "cnt: 0 - valLoss: 0.43077442049980164 - trainLoss: 0.41409310698509216\n",
      "cnt: 0 - valLoss: 0.43077388405799866 - trainLoss: 0.4140913486480713\n",
      "cnt: 0 - valLoss: 0.4307733178138733 - trainLoss: 0.4140895903110504\n",
      "cnt: 0 - valLoss: 0.4307727515697479 - trainLoss: 0.41408783197402954\n",
      "cnt: 0 - valLoss: 0.43077221512794495 - trainLoss: 0.41408607363700867\n",
      "cnt: 0 - valLoss: 0.4307716488838196 - trainLoss: 0.4140843152999878\n",
      "cnt: 0 - valLoss: 0.4307710826396942 - trainLoss: 0.4140826165676117\n",
      "cnt: 0 - valLoss: 0.43077051639556885 - trainLoss: 0.4140808582305908\n",
      "cnt: 0 - valLoss: 0.43077000975608826 - trainLoss: 0.41407909989356995\n",
      "cnt: 0 - valLoss: 0.4307694733142853 - trainLoss: 0.41407740116119385\n",
      "cnt: 0 - valLoss: 0.43076884746551514 - trainLoss: 0.4140755832195282\n",
      "cnt: 0 - valLoss: 0.43076831102371216 - trainLoss: 0.4140738844871521\n",
      "cnt: 0 - valLoss: 0.4307677447795868 - trainLoss: 0.4140721261501312\n",
      "cnt: 0 - valLoss: 0.4307671785354614 - trainLoss: 0.41407036781311035\n",
      "cnt: 0 - valLoss: 0.43076667189598083 - trainLoss: 0.4140686094760895\n",
      "cnt: 0 - valLoss: 0.4307660758495331 - trainLoss: 0.4140668511390686\n",
      "cnt: 0 - valLoss: 0.4307655692100525 - trainLoss: 0.4140651524066925\n",
      "cnt: 0 - valLoss: 0.43076497316360474 - trainLoss: 0.41406339406967163\n",
      "cnt: 0 - valLoss: 0.43076440691947937 - trainLoss: 0.41406163573265076\n",
      "cnt: 0 - valLoss: 0.430763840675354 - trainLoss: 0.4140598773956299\n",
      "cnt: 0 - valLoss: 0.43076327443122864 - trainLoss: 0.4140581488609314\n",
      "cnt: 0 - valLoss: 0.43076273798942566 - trainLoss: 0.4140564203262329\n",
      "cnt: 0 - valLoss: 0.4307621717453003 - trainLoss: 0.41405466198921204\n",
      "cnt: 0 - valLoss: 0.4307616055011749 - trainLoss: 0.41405290365219116\n",
      "cnt: 0 - valLoss: 0.43076106905937195 - trainLoss: 0.41405120491981506\n",
      "cnt: 0 - valLoss: 0.4307605028152466 - trainLoss: 0.4140494167804718\n",
      "cnt: 0 - valLoss: 0.4307599365711212 - trainLoss: 0.4140476882457733\n",
      "cnt: 0 - valLoss: 0.43075940012931824 - trainLoss: 0.41404592990875244\n",
      "cnt: 0 - valLoss: 0.4307587742805481 - trainLoss: 0.41404417157173157\n",
      "cnt: 0 - valLoss: 0.4307582676410675 - trainLoss: 0.4140424132347107\n",
      "cnt: 0 - valLoss: 0.43075767159461975 - trainLoss: 0.4140406847000122\n",
      "cnt: 0 - valLoss: 0.43075716495513916 - trainLoss: 0.4140389561653137\n",
      "cnt: 0 - valLoss: 0.4307565689086914 - trainLoss: 0.41403719782829285\n",
      "cnt: 0 - valLoss: 0.43075597286224365 - trainLoss: 0.414035439491272\n",
      "cnt: 0 - valLoss: 0.4307554364204407 - trainLoss: 0.4140336811542511\n",
      "cnt: 0 - valLoss: 0.4307548999786377 - trainLoss: 0.4140319526195526\n",
      "cnt: 0 - valLoss: 0.43075433373451233 - trainLoss: 0.4140302240848541\n",
      "cnt: 0 - valLoss: 0.4307537376880646 - trainLoss: 0.41402846574783325\n",
      "cnt: 0 - valLoss: 0.4307531714439392 - trainLoss: 0.4140267074108124\n",
      "cnt: 0 - valLoss: 0.43075260519981384 - trainLoss: 0.4140250086784363\n",
      "cnt: 0 - valLoss: 0.43075206875801086 - trainLoss: 0.4140232503414154\n",
      "cnt: 0 - valLoss: 0.4307515025138855 - trainLoss: 0.41402149200439453\n",
      "cnt: 0 - valLoss: 0.43075090646743774 - trainLoss: 0.41401973366737366\n",
      "cnt: 0 - valLoss: 0.4307503402233124 - trainLoss: 0.4140179753303528\n",
      "cnt: 0 - valLoss: 0.430749773979187 - trainLoss: 0.4140162169933319\n",
      "cnt: 0 - valLoss: 0.43074923753738403 - trainLoss: 0.4140144884586334\n",
      "cnt: 0 - valLoss: 0.43074867129325867 - trainLoss: 0.41401275992393494\n",
      "cnt: 0 - valLoss: 0.4307480752468109 - trainLoss: 0.41401106119155884\n",
      "cnt: 0 - valLoss: 0.4307475686073303 - trainLoss: 0.4140092432498932\n",
      "cnt: 0 - valLoss: 0.4307469427585602 - trainLoss: 0.4140075445175171\n",
      "cnt: 0 - valLoss: 0.4307463467121124 - trainLoss: 0.4140057861804962\n",
      "cnt: 0 - valLoss: 0.43074581027030945 - trainLoss: 0.41400402784347534\n",
      "cnt: 0 - valLoss: 0.4307452440261841 - trainLoss: 0.41400232911109924\n",
      "cnt: 0 - valLoss: 0.4307446777820587 - trainLoss: 0.4140005111694336\n",
      "cnt: 0 - valLoss: 0.43074414134025574 - trainLoss: 0.4139988124370575\n",
      "cnt: 0 - valLoss: 0.4307435154914856 - trainLoss: 0.4139971137046814\n",
      "cnt: 0 - valLoss: 0.4307429790496826 - trainLoss: 0.41399529576301575\n",
      "cnt: 0 - valLoss: 0.43074241280555725 - trainLoss: 0.41399359703063965\n",
      "cnt: 0 - valLoss: 0.4307418167591095 - trainLoss: 0.413991779088974\n",
      "cnt: 0 - valLoss: 0.43074125051498413 - trainLoss: 0.4139900803565979\n",
      "cnt: 0 - valLoss: 0.43074071407318115 - trainLoss: 0.413988322019577\n",
      "cnt: 0 - valLoss: 0.430740088224411 - trainLoss: 0.41398656368255615\n",
      "cnt: 0 - valLoss: 0.43073955178260803 - trainLoss: 0.41398486495018005\n",
      "cnt: 0 - valLoss: 0.4307389557361603 - trainLoss: 0.4139831066131592\n",
      "cnt: 0 - valLoss: 0.4307383894920349 - trainLoss: 0.4139813482761383\n",
      "cnt: 0 - valLoss: 0.43073779344558716 - trainLoss: 0.4139796495437622\n",
      "cnt: 0 - valLoss: 0.4307372570037842 - trainLoss: 0.41397789120674133\n",
      "cnt: 0 - valLoss: 0.4307366609573364 - trainLoss: 0.41397613286972046\n",
      "cnt: 0 - valLoss: 0.43073606491088867 - trainLoss: 0.413974404335022\n",
      "cnt: 0 - valLoss: 0.4307355582714081 - trainLoss: 0.4139726161956787\n",
      "cnt: 0 - valLoss: 0.43073490262031555 - trainLoss: 0.4139709174633026\n",
      "cnt: 0 - valLoss: 0.4307343661785126 - trainLoss: 0.4139691889286041\n",
      "cnt: 0 - valLoss: 0.4307337999343872 - trainLoss: 0.41396743059158325\n",
      "cnt: 0 - valLoss: 0.43073323369026184 - trainLoss: 0.4139656722545624\n",
      "cnt: 0 - valLoss: 0.4307326376438141 - trainLoss: 0.4139639139175415\n",
      "cnt: 0 - valLoss: 0.4307320713996887 - trainLoss: 0.413962185382843\n",
      "cnt: 0 - valLoss: 0.43073147535324097 - trainLoss: 0.41396045684814453\n",
      "cnt: 0 - valLoss: 0.4307308793067932 - trainLoss: 0.41395869851112366\n",
      "cnt: 0 - valLoss: 0.43073031306266785 - trainLoss: 0.4139569401741028\n",
      "cnt: 0 - valLoss: 0.4307297170162201 - trainLoss: 0.4139552414417267\n",
      "cnt: 0 - valLoss: 0.4307291805744171 - trainLoss: 0.4139534831047058\n",
      "cnt: 0 - valLoss: 0.4307285249233246 - trainLoss: 0.41395172476768494\n",
      "cnt: 0 - valLoss: 0.430728018283844 - trainLoss: 0.41395002603530884\n",
      "cnt: 0 - valLoss: 0.43072742223739624 - trainLoss: 0.41394826769828796\n",
      "cnt: 0 - valLoss: 0.4307268559932709 - trainLoss: 0.4139465093612671\n",
      "cnt: 0 - valLoss: 0.4307262599468231 - trainLoss: 0.4139447510242462\n",
      "cnt: 0 - valLoss: 0.43072566390037537 - trainLoss: 0.4139430522918701\n",
      "cnt: 0 - valLoss: 0.43072509765625 - trainLoss: 0.41394129395484924\n",
      "cnt: 0 - valLoss: 0.43072450160980225 - trainLoss: 0.41393953561782837\n",
      "cnt: 0 - valLoss: 0.4307239055633545 - trainLoss: 0.4139378070831299\n",
      "cnt: 0 - valLoss: 0.43072330951690674 - trainLoss: 0.4139360785484314\n",
      "cnt: 0 - valLoss: 0.43072277307510376 - trainLoss: 0.4139343500137329\n",
      "cnt: 0 - valLoss: 0.430722177028656 - trainLoss: 0.41393259167671204\n",
      "cnt: 0 - valLoss: 0.43072158098220825 - trainLoss: 0.41393083333969116\n",
      "cnt: 0 - valLoss: 0.4307209849357605 - trainLoss: 0.41392913460731506\n",
      "cnt: 0 - valLoss: 0.43072041869163513 - trainLoss: 0.4139273762702942\n",
      "cnt: 0 - valLoss: 0.4307197332382202 - trainLoss: 0.4139256179332733\n",
      "cnt: 0 - valLoss: 0.4307190477848053 - trainLoss: 0.4139239192008972\n",
      "cnt: 0 - valLoss: 0.4307183623313904 - trainLoss: 0.41392216086387634\n",
      "cnt: 0 - valLoss: 0.43071773648262024 - trainLoss: 0.41392043232917786\n",
      "cnt: 0 - valLoss: 0.4307170510292053 - trainLoss: 0.413918673992157\n",
      "cnt: 0 - valLoss: 0.4307163655757904 - trainLoss: 0.4139169752597809\n",
      "cnt: 0 - valLoss: 0.4307156801223755 - trainLoss: 0.4139152765274048\n",
      "cnt: 0 - valLoss: 0.43071502447128296 - trainLoss: 0.4139135479927063\n",
      "cnt: 0 - valLoss: 0.43071433901786804 - trainLoss: 0.41391175985336304\n",
      "cnt: 0 - valLoss: 0.4307136535644531 - trainLoss: 0.4139100909233093\n",
      "cnt: 0 - valLoss: 0.4307129979133606 - trainLoss: 0.41390833258628845\n",
      "cnt: 0 - valLoss: 0.4307123124599457 - trainLoss: 0.41390660405158997\n",
      "cnt: 0 - valLoss: 0.43071162700653076 - trainLoss: 0.4139048755168915\n",
      "cnt: 0 - valLoss: 0.4307110011577606 - trainLoss: 0.413903146982193\n",
      "cnt: 0 - valLoss: 0.4307103157043457 - trainLoss: 0.4139013886451721\n",
      "cnt: 0 - valLoss: 0.4307096302509308 - trainLoss: 0.413899689912796\n",
      "cnt: 0 - valLoss: 0.43070897459983826 - trainLoss: 0.41389796137809753\n",
      "cnt: 0 - valLoss: 0.4307080805301666 - trainLoss: 0.41389626264572144\n",
      "cnt: 0 - valLoss: 0.43070727586746216 - trainLoss: 0.41389450430870056\n",
      "cnt: 0 - valLoss: 0.4307064116001129 - trainLoss: 0.41389280557632446\n",
      "cnt: 0 - valLoss: 0.43070554733276367 - trainLoss: 0.4138910472393036\n",
      "cnt: 0 - valLoss: 0.43070468306541443 - trainLoss: 0.4138893187046051\n",
      "cnt: 0 - valLoss: 0.4307038486003876 - trainLoss: 0.413887619972229\n",
      "cnt: 0 - valLoss: 0.43070298433303833 - trainLoss: 0.41388586163520813\n",
      "cnt: 0 - valLoss: 0.4307021200656891 - trainLoss: 0.41388413310050964\n",
      "cnt: 0 - valLoss: 0.43070125579833984 - trainLoss: 0.41388240456581116\n",
      "cnt: 0 - valLoss: 0.430700421333313 - trainLoss: 0.41388067603111267\n",
      "cnt: 0 - valLoss: 0.43069955706596375 - trainLoss: 0.4138789176940918\n",
      "cnt: 0 - valLoss: 0.4306986927986145 - trainLoss: 0.4138772189617157\n",
      "cnt: 0 - valLoss: 0.43069782853126526 - trainLoss: 0.4138754606246948\n",
      "cnt: 0 - valLoss: 0.4306969940662384 - trainLoss: 0.4138737916946411\n",
      "cnt: 0 - valLoss: 0.43069612979888916 - trainLoss: 0.41387203335762024\n",
      "cnt: 0 - valLoss: 0.4306952655315399 - trainLoss: 0.41387033462524414\n",
      "cnt: 0 - valLoss: 0.4306944012641907 - trainLoss: 0.41386860609054565\n",
      "cnt: 0 - valLoss: 0.4306935966014862 - trainLoss: 0.4138668477535248\n",
      "cnt: 0 - valLoss: 0.4306927025318146 - trainLoss: 0.4138651490211487\n",
      "cnt: 0 - valLoss: 0.4306918680667877 - trainLoss: 0.4138633906841278\n",
      "cnt: 0 - valLoss: 0.43069103360176086 - trainLoss: 0.4138616621494293\n",
      "cnt: 0 - valLoss: 0.4306901693344116 - trainLoss: 0.4138599634170532\n",
      "cnt: 0 - valLoss: 0.4306893050670624 - trainLoss: 0.41385820508003235\n",
      "cnt: 0 - valLoss: 0.4306885004043579 - trainLoss: 0.41385650634765625\n",
      "cnt: 0 - valLoss: 0.43068763613700867 - trainLoss: 0.41385477781295776\n",
      "cnt: 0 - valLoss: 0.4306867718696594 - trainLoss: 0.4138530194759369\n",
      "cnt: 0 - valLoss: 0.4306859076023102 - trainLoss: 0.4138513207435608\n",
      "cnt: 0 - valLoss: 0.4306850731372833 - trainLoss: 0.4138495922088623\n",
      "cnt: 0 - valLoss: 0.4306842088699341 - trainLoss: 0.4138478934764862\n",
      "cnt: 0 - valLoss: 0.43068334460258484 - trainLoss: 0.41384613513946533\n",
      "cnt: 0 - valLoss: 0.4306824803352356 - trainLoss: 0.41384437680244446\n",
      "cnt: 0 - valLoss: 0.43068164587020874 - trainLoss: 0.41384267807006836\n",
      "cnt: 0 - valLoss: 0.4306807816028595 - trainLoss: 0.4138409495353699\n",
      "cnt: 0 - valLoss: 0.43067991733551025 - trainLoss: 0.4138392508029938\n",
      "cnt: 0 - valLoss: 0.430679053068161 - trainLoss: 0.4138375222682953\n",
      "cnt: 0 - valLoss: 0.43067824840545654 - trainLoss: 0.4138357937335968\n",
      "cnt: 0 - valLoss: 0.4306773841381073 - trainLoss: 0.4138340353965759\n",
      "cnt: 0 - valLoss: 0.43067654967308044 - trainLoss: 0.41383230686187744\n",
      "cnt: 0 - valLoss: 0.4306756854057312 - trainLoss: 0.41383060812950134\n",
      "cnt: 0 - valLoss: 0.43067485094070435 - trainLoss: 0.41382884979248047\n",
      "cnt: 0 - valLoss: 0.4306739568710327 - trainLoss: 0.413827121257782\n",
      "cnt: 0 - valLoss: 0.43067309260368347 - trainLoss: 0.4138253927230835\n",
      "cnt: 0 - valLoss: 0.430672287940979 - trainLoss: 0.4138237237930298\n",
      "cnt: 0 - valLoss: 0.43067142367362976 - trainLoss: 0.4138219654560089\n",
      "cnt: 0 - valLoss: 0.4306705892086029 - trainLoss: 0.4138202369213104\n",
      "cnt: 0 - valLoss: 0.43066972494125366 - trainLoss: 0.41381847858428955\n",
      "cnt: 0 - valLoss: 0.4306688606739044 - trainLoss: 0.41381677985191345\n",
      "cnt: 0 - valLoss: 0.4306679964065552 - trainLoss: 0.41381505131721497\n",
      "cnt: 0 - valLoss: 0.4306671917438507 - trainLoss: 0.4138133227825165\n",
      "cnt: 0 - valLoss: 0.43066638708114624 - trainLoss: 0.413811594247818\n",
      "cnt: 0 - valLoss: 0.430665522813797 - trainLoss: 0.4138098955154419\n",
      "cnt: 0 - valLoss: 0.43066465854644775 - trainLoss: 0.413808137178421\n",
      "cnt: 0 - valLoss: 0.4306637942790985 - trainLoss: 0.41380640864372253\n",
      "cnt: 0 - valLoss: 0.43066295981407166 - trainLoss: 0.41380470991134644\n",
      "cnt: 0 - valLoss: 0.4306620955467224 - trainLoss: 0.41380295157432556\n",
      "cnt: 0 - valLoss: 0.43066129088401794 - trainLoss: 0.41380128264427185\n",
      "cnt: 0 - valLoss: 0.4306604564189911 - trainLoss: 0.413799524307251\n",
      "cnt: 0 - valLoss: 0.43065959215164185 - trainLoss: 0.4137978255748749\n",
      "cnt: 0 - valLoss: 0.430658757686615 - trainLoss: 0.413796067237854\n",
      "cnt: 0 - valLoss: 0.43065792322158813 - trainLoss: 0.4137943387031555\n",
      "cnt: 0 - valLoss: 0.4306570887565613 - trainLoss: 0.4137926697731018\n",
      "cnt: 0 - valLoss: 0.4306562542915344 - trainLoss: 0.41379091143608093\n",
      "cnt: 0 - valLoss: 0.4306553900241852 - trainLoss: 0.41378921270370483\n",
      "cnt: 0 - valLoss: 0.4306545555591583 - trainLoss: 0.41378745436668396\n",
      "cnt: 0 - valLoss: 0.4306536912918091 - trainLoss: 0.41378575563430786\n",
      "cnt: 0 - valLoss: 0.4306528866291046 - trainLoss: 0.4137840270996094\n",
      "cnt: 0 - valLoss: 0.43065202236175537 - trainLoss: 0.4137823283672333\n",
      "cnt: 0 - valLoss: 0.43065115809440613 - trainLoss: 0.4137805998325348\n",
      "cnt: 0 - valLoss: 0.43065035343170166 - trainLoss: 0.4137788414955139\n",
      "cnt: 0 - valLoss: 0.4306494891643524 - trainLoss: 0.4137771427631378\n",
      "cnt: 0 - valLoss: 0.4306486248970032 - trainLoss: 0.41377538442611694\n",
      "cnt: 0 - valLoss: 0.43064776062965393 - trainLoss: 0.41377368569374084\n",
      "cnt: 0 - valLoss: 0.43064695596694946 - trainLoss: 0.41377195715904236\n",
      "cnt: 0 - valLoss: 0.430646151304245 - trainLoss: 0.41377025842666626\n",
      "cnt: 0 - valLoss: 0.43064528703689575 - trainLoss: 0.4137685000896454\n",
      "cnt: 0 - valLoss: 0.4306444227695465 - trainLoss: 0.4137668311595917\n",
      "cnt: 0 - valLoss: 0.43064358830451965 - trainLoss: 0.4137650728225708\n",
      "cnt: 0 - valLoss: 0.4306427538394928 - trainLoss: 0.4137633740901947\n",
      "cnt: 0 - valLoss: 0.43064188957214355 - trainLoss: 0.41376161575317383\n",
      "cnt: 0 - valLoss: 0.4306410849094391 - trainLoss: 0.41375988721847534\n",
      "cnt: 0 - valLoss: 0.43064022064208984 - trainLoss: 0.41375818848609924\n",
      "cnt: 0 - valLoss: 0.430639386177063 - trainLoss: 0.41375645995140076\n",
      "cnt: 0 - valLoss: 0.43063855171203613 - trainLoss: 0.41375476121902466\n",
      "cnt: 0 - valLoss: 0.43063774704933167 - trainLoss: 0.4137530028820038\n",
      "cnt: 0 - valLoss: 0.4306368827819824 - trainLoss: 0.4137512445449829\n",
      "cnt: 0 - valLoss: 0.43063607811927795 - trainLoss: 0.4137495458126068\n",
      "cnt: 0 - valLoss: 0.4306352138519287 - trainLoss: 0.4137478172779083\n",
      "cnt: 0 - valLoss: 0.43063434958457947 - trainLoss: 0.4137461185455322\n",
      "cnt: 0 - valLoss: 0.430633544921875 - trainLoss: 0.41374439001083374\n",
      "cnt: 0 - valLoss: 0.43063274025917053 - trainLoss: 0.41374269127845764\n",
      "cnt: 0 - valLoss: 0.4306318759918213 - trainLoss: 0.41374096274375916\n",
      "cnt: 0 - valLoss: 0.4306310713291168 - trainLoss: 0.41373926401138306\n",
      "cnt: 0 - valLoss: 0.4306302070617676 - trainLoss: 0.4137375056743622\n",
      "cnt: 0 - valLoss: 0.43062934279441833 - trainLoss: 0.4137358069419861\n",
      "cnt: 0 - valLoss: 0.43062853813171387 - trainLoss: 0.4137340486049652\n",
      "cnt: 0 - valLoss: 0.430627703666687 - trainLoss: 0.4137323200702667\n",
      "cnt: 0 - valLoss: 0.43062686920166016 - trainLoss: 0.4137306213378906\n",
      "cnt: 0 - valLoss: 0.4306260943412781 - trainLoss: 0.41372889280319214\n",
      "cnt: 0 - valLoss: 0.43062523007392883 - trainLoss: 0.41372719407081604\n",
      "cnt: 0 - valLoss: 0.4306243658065796 - trainLoss: 0.41372546553611755\n",
      "cnt: 0 - valLoss: 0.4306235611438751 - trainLoss: 0.41372373700141907\n",
      "cnt: 0 - valLoss: 0.43062275648117065 - trainLoss: 0.4137220084667206\n",
      "cnt: 0 - valLoss: 0.4306218922138214 - trainLoss: 0.4137203097343445\n",
      "cnt: 0 - valLoss: 0.43062105774879456 - trainLoss: 0.413718581199646\n",
      "cnt: 0 - valLoss: 0.4306202530860901 - trainLoss: 0.4137168824672699\n",
      "cnt: 0 - valLoss: 0.43061938881874084 - trainLoss: 0.413715124130249\n",
      "cnt: 0 - valLoss: 0.4306185841560364 - trainLoss: 0.41371339559555054\n",
      "cnt: 0 - valLoss: 0.43061771988868713 - trainLoss: 0.41371169686317444\n",
      "cnt: 0 - valLoss: 0.43061691522598267 - trainLoss: 0.41370999813079834\n",
      "cnt: 0 - valLoss: 0.4306160509586334 - trainLoss: 0.41370826959609985\n",
      "cnt: 0 - valLoss: 0.43061524629592896 - trainLoss: 0.41370657086372375\n",
      "cnt: 0 - valLoss: 0.4306144416332245 - trainLoss: 0.4137048125267029\n",
      "cnt: 0 - valLoss: 0.43061357736587524 - trainLoss: 0.4137030839920044\n",
      "cnt: 0 - valLoss: 0.4306127429008484 - trainLoss: 0.4137013852596283\n",
      "cnt: 0 - valLoss: 0.4306119382381439 - trainLoss: 0.4136996269226074\n",
      "cnt: 0 - valLoss: 0.4306110739707947 - trainLoss: 0.4136979281902313\n",
      "cnt: 0 - valLoss: 0.4306102693080902 - trainLoss: 0.41369619965553284\n",
      "cnt: 0 - valLoss: 0.43060946464538574 - trainLoss: 0.41369450092315674\n",
      "cnt: 0 - valLoss: 0.4306086301803589 - trainLoss: 0.41369277238845825\n",
      "cnt: 0 - valLoss: 0.43060779571533203 - trainLoss: 0.4136910140514374\n",
      "cnt: 0 - valLoss: 0.4306069314479828 - trainLoss: 0.4136893153190613\n",
      "cnt: 0 - valLoss: 0.4306061267852783 - trainLoss: 0.4136875867843628\n",
      "cnt: 0 - valLoss: 0.43060529232025146 - trainLoss: 0.4136858880519867\n",
      "cnt: 0 - valLoss: 0.430604487657547 - trainLoss: 0.4136841297149658\n",
      "cnt: 0 - valLoss: 0.43060362339019775 - trainLoss: 0.4136824607849121\n",
      "cnt: 0 - valLoss: 0.4306028187274933 - trainLoss: 0.41368070244789124\n",
      "cnt: 0 - valLoss: 0.43060198426246643 - trainLoss: 0.41367900371551514\n",
      "cnt: 0 - valLoss: 0.43060117959976196 - trainLoss: 0.41367727518081665\n",
      "cnt: 0 - valLoss: 0.4306003749370575 - trainLoss: 0.41367557644844055\n",
      "cnt: 0 - valLoss: 0.43059951066970825 - trainLoss: 0.41367384791374207\n",
      "cnt: 0 - valLoss: 0.430598646402359 - trainLoss: 0.41367214918136597\n",
      "cnt: 0 - valLoss: 0.43059787154197693 - trainLoss: 0.41367045044898987\n",
      "cnt: 0 - valLoss: 0.4305970370769501 - trainLoss: 0.413668692111969\n",
      "cnt: 0 - valLoss: 0.43059617280960083 - trainLoss: 0.4136669635772705\n",
      "cnt: 0 - valLoss: 0.43059536814689636 - trainLoss: 0.4136652648448944\n",
      "cnt: 0 - valLoss: 0.4305945336818695 - trainLoss: 0.41366350650787354\n",
      "cnt: 0 - valLoss: 0.43059366941452026 - trainLoss: 0.41366177797317505\n",
      "cnt: 0 - valLoss: 0.43059292435646057 - trainLoss: 0.41366007924079895\n",
      "cnt: 0 - valLoss: 0.43059206008911133 - trainLoss: 0.41365838050842285\n",
      "cnt: 0 - valLoss: 0.4305912256240845 - trainLoss: 0.41365665197372437\n",
      "cnt: 0 - valLoss: 0.43059042096138 - trainLoss: 0.41365495324134827\n",
      "cnt: 0 - valLoss: 0.43058955669403076 - trainLoss: 0.4136532247066498\n",
      "cnt: 0 - valLoss: 0.4305887520313263 - trainLoss: 0.4136515259742737\n",
      "cnt: 0 - valLoss: 0.4305879473686218 - trainLoss: 0.41364970803260803\n",
      "cnt: 0 - valLoss: 0.43058711290359497 - trainLoss: 0.4136480391025543\n",
      "cnt: 0 - valLoss: 0.430586576461792 - trainLoss: 0.41364631056785583\n",
      "cnt: 0 - valLoss: 0.43058571219444275 - trainLoss: 0.4136446416378021\n",
      "cnt: 0 - valLoss: 0.43058520555496216 - trainLoss: 0.41364291310310364\n",
      "cnt: 0 - valLoss: 0.4305846095085144 - trainLoss: 0.4136412441730499\n",
      "cnt: 0 - valLoss: 0.43058404326438904 - trainLoss: 0.4136395752429962\n",
      "cnt: 0 - valLoss: 0.43058347702026367 - trainLoss: 0.41363781690597534\n",
      "cnt: 0 - valLoss: 0.4305829703807831 - trainLoss: 0.41363614797592163\n",
      "cnt: 0 - valLoss: 0.4305823743343353 - trainLoss: 0.4136344790458679\n",
      "cnt: 0 - valLoss: 0.43058186769485474 - trainLoss: 0.4136327803134918\n",
      "cnt: 0 - valLoss: 0.4305812418460846 - trainLoss: 0.41363105177879333\n",
      "cnt: 0 - valLoss: 0.4305807054042816 - trainLoss: 0.41362935304641724\n",
      "cnt: 0 - valLoss: 0.43058013916015625 - trainLoss: 0.4136276841163635\n",
      "cnt: 0 - valLoss: 0.430579274892807 - trainLoss: 0.41362595558166504\n",
      "cnt: 0 - valLoss: 0.4305787682533264 - trainLoss: 0.41362428665161133\n",
      "cnt: 0 - valLoss: 0.43057817220687866 - trainLoss: 0.41362258791923523\n",
      "cnt: 0 - valLoss: 0.4305776059627533 - trainLoss: 0.4136209189891815\n",
      "cnt: 0 - valLoss: 0.43057703971862793 - trainLoss: 0.41361919045448303\n",
      "cnt: 0 - valLoss: 0.4305764436721802 - trainLoss: 0.4136175215244293\n",
      "cnt: 0 - valLoss: 0.4305759370326996 - trainLoss: 0.4136158227920532\n",
      "cnt: 0 - valLoss: 0.43057534098625183 - trainLoss: 0.4136141240596771\n",
      "cnt: 0 - valLoss: 0.43057477474212646 - trainLoss: 0.4136124551296234\n",
      "cnt: 0 - valLoss: 0.4305742383003235 - trainLoss: 0.4136107861995697\n",
      "cnt: 0 - valLoss: 0.4305736720561981 - trainLoss: 0.4136090576648712\n",
      "cnt: 0 - valLoss: 0.43057310581207275 - trainLoss: 0.4136073589324951\n",
      "cnt: 0 - valLoss: 0.430572509765625 - trainLoss: 0.4136057198047638\n",
      "cnt: 0 - valLoss: 0.43057164549827576 - trainLoss: 0.4136039614677429\n",
      "cnt: 0 - valLoss: 0.4305711090564728 - trainLoss: 0.4136022925376892\n",
      "cnt: 0 - valLoss: 0.4305705428123474 - trainLoss: 0.4136005938053131\n",
      "cnt: 0 - valLoss: 0.43056994676589966 - trainLoss: 0.4135989248752594\n",
      "cnt: 0 - valLoss: 0.4305694103240967 - trainLoss: 0.4135972559452057\n",
      "cnt: 0 - valLoss: 0.4305688142776489 - trainLoss: 0.4135955274105072\n",
      "cnt: 0 - valLoss: 0.43056821823120117 - trainLoss: 0.4135938286781311\n",
      "cnt: 0 - valLoss: 0.4305676519870758 - trainLoss: 0.4135921597480774\n",
      "cnt: 0 - valLoss: 0.4305671155452728 - trainLoss: 0.4135904908180237\n",
      "cnt: 0 - valLoss: 0.4305665194988251 - trainLoss: 0.4135887622833252\n",
      "cnt: 0 - valLoss: 0.4305659532546997 - trainLoss: 0.4135870635509491\n",
      "cnt: 0 - valLoss: 0.43056535720825195 - trainLoss: 0.4135853350162506\n",
      "cnt: 0 - valLoss: 0.4305647909641266 - trainLoss: 0.4135837256908417\n",
      "cnt: 0 - valLoss: 0.43056419491767883 - trainLoss: 0.4135819971561432\n",
      "cnt: 0 - valLoss: 0.43056362867355347 - trainLoss: 0.4135802984237671\n",
      "cnt: 0 - valLoss: 0.4305627942085266 - trainLoss: 0.4135785698890686\n",
      "cnt: 0 - valLoss: 0.43056216835975647 - trainLoss: 0.41357696056365967\n",
      "cnt: 0 - valLoss: 0.4305616319179535 - trainLoss: 0.4135752320289612\n",
      "cnt: 0 - valLoss: 0.43056103587150574 - trainLoss: 0.4135735332965851\n",
      "cnt: 0 - valLoss: 0.4305604100227356 - trainLoss: 0.413571834564209\n",
      "cnt: 0 - valLoss: 0.430559903383255 - trainLoss: 0.41357019543647766\n",
      "cnt: 0 - valLoss: 0.43055930733680725 - trainLoss: 0.41356849670410156\n",
      "cnt: 0 - valLoss: 0.4305587112903595 - trainLoss: 0.4135667681694031\n",
      "cnt: 0 - valLoss: 0.43055811524391174 - trainLoss: 0.413565069437027\n",
      "cnt: 0 - valLoss: 0.4305575489997864 - trainLoss: 0.41356343030929565\n",
      "cnt: 0 - valLoss: 0.4305569529533386 - trainLoss: 0.41356173157691956\n",
      "cnt: 0 - valLoss: 0.43055635690689087 - trainLoss: 0.41356006264686584\n",
      "cnt: 0 - valLoss: 0.4305557608604431 - trainLoss: 0.41355833411216736\n",
      "cnt: 0 - valLoss: 0.43055519461631775 - trainLoss: 0.41355663537979126\n",
      "cnt: 0 - valLoss: 0.4305546283721924 - trainLoss: 0.41355496644973755\n",
      "cnt: 0 - valLoss: 0.43055400252342224 - trainLoss: 0.41355323791503906\n",
      "cnt: 0 - valLoss: 0.43055346608161926 - trainLoss: 0.41355153918266296\n",
      "cnt: 0 - valLoss: 0.4305528402328491 - trainLoss: 0.41354987025260925\n",
      "cnt: 0 - valLoss: 0.4305519759654999 - trainLoss: 0.41354820132255554\n",
      "cnt: 0 - valLoss: 0.4305513799190521 - trainLoss: 0.41354647278785706\n",
      "cnt: 0 - valLoss: 0.43055081367492676 - trainLoss: 0.41354480385780334\n",
      "cnt: 0 - valLoss: 0.430550217628479 - trainLoss: 0.41354313492774963\n",
      "cnt: 0 - valLoss: 0.43054962158203125 - trainLoss: 0.41354143619537354\n",
      "cnt: 0 - valLoss: 0.4305490255355835 - trainLoss: 0.4135397672653198\n",
      "cnt: 0 - valLoss: 0.43054842948913574 - trainLoss: 0.41353803873062134\n",
      "cnt: 0 - valLoss: 0.430547833442688 - trainLoss: 0.4135363698005676\n",
      "cnt: 0 - valLoss: 0.43054720759391785 - trainLoss: 0.41353467106819153\n",
      "cnt: 0 - valLoss: 0.4305466115474701 - trainLoss: 0.4135330021381378\n",
      "cnt: 0 - valLoss: 0.4305460751056671 - trainLoss: 0.4135313332080841\n",
      "cnt: 0 - valLoss: 0.4305454194545746 - trainLoss: 0.4135296046733856\n",
      "cnt: 0 - valLoss: 0.43054482340812683 - trainLoss: 0.4135279059410095\n",
      "cnt: 0 - valLoss: 0.4305442273616791 - trainLoss: 0.4135262668132782\n",
      "cnt: 0 - valLoss: 0.4305436313152313 - trainLoss: 0.4135245680809021\n",
      "cnt: 0 - valLoss: 0.43054303526878357 - trainLoss: 0.413522869348526\n",
      "cnt: 0 - valLoss: 0.4305424094200134 - trainLoss: 0.4135212302207947\n",
      "cnt: 0 - valLoss: 0.4305418133735657 - trainLoss: 0.4135195314884186\n",
      "cnt: 0 - valLoss: 0.4305412173271179 - trainLoss: 0.4135178029537201\n",
      "cnt: 0 - valLoss: 0.43054062128067017 - trainLoss: 0.4135161340236664\n",
      "cnt: 0 - valLoss: 0.43053996562957764 - trainLoss: 0.4135144352912903\n",
      "cnt: 0 - valLoss: 0.43053942918777466 - trainLoss: 0.4135127663612366\n",
      "cnt: 0 - valLoss: 0.43053877353668213 - trainLoss: 0.4135110378265381\n",
      "cnt: 0 - valLoss: 0.43053820729255676 - trainLoss: 0.4135093688964844\n",
      "cnt: 0 - valLoss: 0.43053752183914185 - trainLoss: 0.41350769996643066\n",
      "cnt: 0 - valLoss: 0.4305369257926941 - trainLoss: 0.41350600123405457\n",
      "cnt: 0 - valLoss: 0.43053609132766724 - trainLoss: 0.41350433230400085\n",
      "cnt: 0 - valLoss: 0.4305354654788971 - trainLoss: 0.41350260376930237\n",
      "cnt: 0 - valLoss: 0.43053486943244934 - trainLoss: 0.41350093483924866\n",
      "cnt: 0 - valLoss: 0.4305342435836792 - trainLoss: 0.41349923610687256\n",
      "cnt: 0 - valLoss: 0.43053364753723145 - trainLoss: 0.41349759697914124\n",
      "cnt: 0 - valLoss: 0.4305329918861389 - trainLoss: 0.41349589824676514\n",
      "cnt: 0 - valLoss: 0.43053239583969116 - trainLoss: 0.41349416971206665\n",
      "cnt: 0 - valLoss: 0.4305317997932434 - trainLoss: 0.41349250078201294\n",
      "cnt: 0 - valLoss: 0.43053120374679565 - trainLoss: 0.41349083185195923\n",
      "cnt: 0 - valLoss: 0.4305305480957031 - trainLoss: 0.41348913311958313\n",
      "cnt: 0 - valLoss: 0.43052995204925537 - trainLoss: 0.4134874641895294\n",
      "cnt: 0 - valLoss: 0.43052929639816284 - trainLoss: 0.4134857952594757\n",
      "cnt: 0 - valLoss: 0.4305287003517151 - trainLoss: 0.4134840667247772\n",
      "cnt: 0 - valLoss: 0.43052810430526733 - trainLoss: 0.4134823977947235\n",
      "cnt: 0 - valLoss: 0.4305275082588196 - trainLoss: 0.4134807288646698\n",
      "cnt: 0 - valLoss: 0.4305269122123718 - trainLoss: 0.4134790301322937\n",
      "cnt: 0 - valLoss: 0.4305262565612793 - trainLoss: 0.4134773015975952\n",
      "cnt: 0 - valLoss: 0.43052563071250916 - trainLoss: 0.4134756922721863\n",
      "cnt: 0 - valLoss: 0.430525004863739 - trainLoss: 0.4134739637374878\n",
      "cnt: 0 - valLoss: 0.43052440881729126 - trainLoss: 0.4134722650051117\n",
      "cnt: 0 - valLoss: 0.4305238127708435 - trainLoss: 0.413470596075058\n",
      "cnt: 0 - valLoss: 0.4305231273174286 - trainLoss: 0.4134689271450043\n",
      "cnt: 0 - valLoss: 0.43052253127098083 - trainLoss: 0.4134672284126282\n",
      "cnt: 0 - valLoss: 0.4305219352245331 - trainLoss: 0.41346555948257446\n",
      "cnt: 0 - valLoss: 0.43052127957344055 - trainLoss: 0.413463830947876\n",
      "cnt: 0 - valLoss: 0.4305206537246704 - trainLoss: 0.41346216201782227\n",
      "cnt: 0 - valLoss: 0.43052005767822266 - trainLoss: 0.41346049308776855\n",
      "cnt: 0 - valLoss: 0.4305194020271301 - trainLoss: 0.41345882415771484\n",
      "cnt: 0 - valLoss: 0.4305188059806824 - trainLoss: 0.41345712542533875\n",
      "cnt: 0 - valLoss: 0.43051815032958984 - trainLoss: 0.41345545649528503\n",
      "cnt: 0 - valLoss: 0.4305175542831421 - trainLoss: 0.41345372796058655\n",
      "cnt: 0 - valLoss: 0.43051692843437195 - trainLoss: 0.41345205903053284\n",
      "cnt: 0 - valLoss: 0.4305162727832794 - trainLoss: 0.41345036029815674\n",
      "cnt: 0 - valLoss: 0.4305156469345093 - trainLoss: 0.4134487211704254\n",
      "cnt: 0 - valLoss: 0.43051502108573914 - trainLoss: 0.4134470224380493\n",
      "cnt: 0 - valLoss: 0.430514395236969 - trainLoss: 0.4134453535079956\n",
      "cnt: 0 - valLoss: 0.43051379919052124 - trainLoss: 0.4134436845779419\n",
      "cnt: 0 - valLoss: 0.4305131137371063 - trainLoss: 0.4134419560432434\n",
      "cnt: 0 - valLoss: 0.43051251769065857 - trainLoss: 0.4134402871131897\n",
      "cnt: 0 - valLoss: 0.4305118918418884 - trainLoss: 0.413438618183136\n",
      "cnt: 0 - valLoss: 0.4305112659931183 - trainLoss: 0.4134369194507599\n",
      "cnt: 0 - valLoss: 0.43051061034202576 - trainLoss: 0.4134351909160614\n",
      "cnt: 0 - valLoss: 0.430510014295578 - trainLoss: 0.41343358159065247\n",
      "cnt: 0 - valLoss: 0.4305093288421631 - trainLoss: 0.413431853055954\n",
      "cnt: 0 - valLoss: 0.43050873279571533 - trainLoss: 0.41343018412590027\n",
      "cnt: 0 - valLoss: 0.4305081367492676 - trainLoss: 0.41342848539352417\n",
      "cnt: 0 - valLoss: 0.43050748109817505 - trainLoss: 0.41342681646347046\n",
      "cnt: 0 - valLoss: 0.4305068552494049 - trainLoss: 0.41342517733573914\n",
      "cnt: 0 - valLoss: 0.4305061995983124 - trainLoss: 0.41342347860336304\n",
      "cnt: 0 - valLoss: 0.43050557374954224 - trainLoss: 0.4134218096733093\n",
      "cnt: 0 - valLoss: 0.4305049479007721 - trainLoss: 0.41342008113861084\n",
      "cnt: 0 - valLoss: 0.43050432205200195 - trainLoss: 0.41341841220855713\n",
      "cnt: 0 - valLoss: 0.4305036664009094 - trainLoss: 0.4134167432785034\n",
      "cnt: 0 - valLoss: 0.43050307035446167 - trainLoss: 0.4134150743484497\n",
      "cnt: 0 - valLoss: 0.43050238490104675 - trainLoss: 0.4134133756160736\n",
      "cnt: 0 - valLoss: 0.430501788854599 - trainLoss: 0.4134117066860199\n",
      "cnt: 0 - valLoss: 0.43050113320350647 - trainLoss: 0.4134100377559662\n",
      "cnt: 0 - valLoss: 0.43050050735473633 - trainLoss: 0.4134083092212677\n",
      "cnt: 0 - valLoss: 0.4304998219013214 - trainLoss: 0.413406640291214\n",
      "cnt: 0 - valLoss: 0.4304990768432617 - trainLoss: 0.4134049713611603\n",
      "cnt: 0 - valLoss: 0.4304983913898468 - trainLoss: 0.41340336203575134\n",
      "cnt: 0 - valLoss: 0.4304976761341095 - trainLoss: 0.41340163350105286\n",
      "cnt: 0 - valLoss: 0.4304969906806946 - trainLoss: 0.41339996457099915\n",
      "cnt: 0 - valLoss: 0.43049630522727966 - trainLoss: 0.41339829564094543\n",
      "cnt: 0 - valLoss: 0.43049561977386475 - trainLoss: 0.4133966863155365\n",
      "cnt: 0 - valLoss: 0.43049487471580505 - trainLoss: 0.413394957780838\n",
      "cnt: 0 - valLoss: 0.43049418926239014 - trainLoss: 0.4133932888507843\n",
      "cnt: 0 - valLoss: 0.43049347400665283 - trainLoss: 0.41339167952537537\n",
      "cnt: 0 - valLoss: 0.4304927885532379 - trainLoss: 0.4133899509906769\n",
      "cnt: 0 - valLoss: 0.430492103099823 - trainLoss: 0.41338834166526794\n",
      "cnt: 0 - valLoss: 0.4304913580417633 - trainLoss: 0.41338667273521423\n",
      "cnt: 0 - valLoss: 0.4304906725883484 - trainLoss: 0.4133850038051605\n",
      "cnt: 0 - valLoss: 0.4304899573326111 - trainLoss: 0.4133833348751068\n",
      "cnt: 0 - valLoss: 0.43048927187919617 - trainLoss: 0.4133816659450531\n",
      "cnt: 0 - valLoss: 0.43048858642578125 - trainLoss: 0.4133799970149994\n",
      "cnt: 0 - valLoss: 0.43048790097236633 - trainLoss: 0.4133783280849457\n",
      "cnt: 0 - valLoss: 0.43048718571662903 - trainLoss: 0.41337665915489197\n",
      "cnt: 0 - valLoss: 0.4304865002632141 - trainLoss: 0.41337499022483826\n",
      "cnt: 0 - valLoss: 0.4304857552051544 - trainLoss: 0.41337332129478455\n",
      "cnt: 0 - valLoss: 0.4304850697517395 - trainLoss: 0.4133716821670532\n",
      "cnt: 0 - valLoss: 0.4304843544960022 - trainLoss: 0.4133699834346771\n",
      "cnt: 0 - valLoss: 0.4304836690425873 - trainLoss: 0.4133683443069458\n",
      "cnt: 0 - valLoss: 0.4304829239845276 - trainLoss: 0.4133666455745697\n",
      "cnt: 0 - valLoss: 0.43048229813575745 - trainLoss: 0.4133650064468384\n",
      "cnt: 0 - valLoss: 0.43048155307769775 - trainLoss: 0.41336333751678467\n",
      "cnt: 0 - valLoss: 0.43048086762428284 - trainLoss: 0.41336166858673096\n",
      "cnt: 0 - valLoss: 0.43048015236854553 - trainLoss: 0.41335999965667725\n",
      "cnt: 0 - valLoss: 0.43047940731048584 - trainLoss: 0.41335833072662354\n",
      "cnt: 0 - valLoss: 0.4304787814617157 - trainLoss: 0.4133566915988922\n",
      "cnt: 0 - valLoss: 0.430478036403656 - trainLoss: 0.4133549928665161\n",
      "cnt: 0 - valLoss: 0.4304773509502411 - trainLoss: 0.4133533537387848\n",
      "cnt: 0 - valLoss: 0.43047666549682617 - trainLoss: 0.4133516550064087\n",
      "cnt: 0 - valLoss: 0.43047598004341125 - trainLoss: 0.41335001587867737\n",
      "cnt: 0 - valLoss: 0.43047526478767395 - trainLoss: 0.41334831714630127\n",
      "cnt: 0 - valLoss: 0.43047451972961426 - trainLoss: 0.41334667801856995\n",
      "cnt: 0 - valLoss: 0.43047383427619934 - trainLoss: 0.41334497928619385\n",
      "cnt: 0 - valLoss: 0.4304731488227844 - trainLoss: 0.4133433401584625\n",
      "cnt: 0 - valLoss: 0.4304724633693695 - trainLoss: 0.4133416712284088\n",
      "cnt: 0 - valLoss: 0.4304717481136322 - trainLoss: 0.4133400022983551\n",
      "cnt: 0 - valLoss: 0.4304710626602173 - trainLoss: 0.4133383333683014\n",
      "cnt: 0 - valLoss: 0.4304703176021576 - trainLoss: 0.4133366644382477\n",
      "cnt: 0 - valLoss: 0.4304696321487427 - trainLoss: 0.41333505511283875\n",
      "cnt: 0 - valLoss: 0.43046894669532776 - trainLoss: 0.41333338618278503\n",
      "cnt: 0 - valLoss: 0.43046826124191284 - trainLoss: 0.4133317172527313\n",
      "cnt: 0 - valLoss: 0.4304675757884979 - trainLoss: 0.4133300483226776\n",
      "cnt: 0 - valLoss: 0.430466890335083 - trainLoss: 0.4133283793926239\n",
      "cnt: 0 - valLoss: 0.4304661750793457 - trainLoss: 0.4133267104625702\n",
      "cnt: 0 - valLoss: 0.4304654896259308 - trainLoss: 0.4133250415325165\n",
      "cnt: 0 - valLoss: 0.4304647445678711 - trainLoss: 0.41332340240478516\n",
      "cnt: 0 - valLoss: 0.4304640591144562 - trainLoss: 0.41332170367240906\n",
      "cnt: 0 - valLoss: 0.43046337366104126 - trainLoss: 0.41332006454467773\n",
      "cnt: 0 - valLoss: 0.43046268820762634 - trainLoss: 0.413318395614624\n",
      "cnt: 0 - valLoss: 0.4304620027542114 - trainLoss: 0.4133167266845703\n",
      "cnt: 0 - valLoss: 0.4304612874984741 - trainLoss: 0.4133150577545166\n",
      "cnt: 0 - valLoss: 0.4304606020450592 - trainLoss: 0.4133133888244629\n",
      "cnt: 0 - valLoss: 0.4304599165916443 - trainLoss: 0.4133117198944092\n",
      "cnt: 0 - valLoss: 0.43045923113822937 - trainLoss: 0.41331005096435547\n",
      "cnt: 0 - valLoss: 0.4304584860801697 - trainLoss: 0.41330841183662415\n",
      "cnt: 0 - valLoss: 0.43045780062675476 - trainLoss: 0.41330671310424805\n",
      "cnt: 0 - valLoss: 0.43045711517333984 - trainLoss: 0.4133050739765167\n",
      "cnt: 0 - valLoss: 0.4304564297199249 - trainLoss: 0.413303405046463\n",
      "cnt: 0 - valLoss: 0.43045574426651 - trainLoss: 0.4133017361164093\n",
      "cnt: 0 - valLoss: 0.4304550290107727 - trainLoss: 0.41330012679100037\n",
      "cnt: 0 - valLoss: 0.430454283952713 - trainLoss: 0.4132983982563019\n",
      "cnt: 0 - valLoss: 0.43045365810394287 - trainLoss: 0.41329672932624817\n",
      "cnt: 0 - valLoss: 0.43045297265052795 - trainLoss: 0.41329506039619446\n",
      "cnt: 0 - valLoss: 0.43045222759246826 - trainLoss: 0.4132934510707855\n",
      "cnt: 0 - valLoss: 0.43045154213905334 - trainLoss: 0.4132918119430542\n",
      "cnt: 0 - valLoss: 0.4304508566856384 - trainLoss: 0.4132901132106781\n",
      "cnt: 0 - valLoss: 0.4304501712322235 - trainLoss: 0.4132884442806244\n",
      "cnt: 0 - valLoss: 0.4304494559764862 - trainLoss: 0.41328680515289307\n",
      "cnt: 0 - valLoss: 0.4304487705230713 - trainLoss: 0.41328513622283936\n",
      "cnt: 0 - valLoss: 0.4304480254650116 - trainLoss: 0.41328346729278564\n",
      "cnt: 0 - valLoss: 0.43044742941856384 - trainLoss: 0.41328179836273193\n",
      "cnt: 0 - valLoss: 0.43044665455818176 - trainLoss: 0.4132801294326782\n",
      "cnt: 0 - valLoss: 0.43044596910476685 - trainLoss: 0.4132784605026245\n",
      "cnt: 0 - valLoss: 0.43044528365135193 - trainLoss: 0.4132768213748932\n",
      "cnt: 0 - valLoss: 0.430444598197937 - trainLoss: 0.4132751524448395\n",
      "cnt: 0 - valLoss: 0.4304439127445221 - trainLoss: 0.41327348351478577\n",
      "cnt: 0 - valLoss: 0.4304432272911072 - trainLoss: 0.41327181458473206\n",
      "cnt: 0 - valLoss: 0.43044254183769226 - trainLoss: 0.41327014565467834\n",
      "cnt: 0 - valLoss: 0.43044185638427734 - trainLoss: 0.41326847672462463\n",
      "cnt: 0 - valLoss: 0.4304411709308624 - trainLoss: 0.4132668077945709\n",
      "cnt: 0 - valLoss: 0.4304404854774475 - trainLoss: 0.413265198469162\n",
      "cnt: 0 - valLoss: 0.4304398000240326 - trainLoss: 0.41326355934143066\n",
      "cnt: 0 - valLoss: 0.4304391145706177 - trainLoss: 0.41326186060905457\n",
      "cnt: 0 - valLoss: 0.43043839931488037 - trainLoss: 0.41326022148132324\n",
      "cnt: 0 - valLoss: 0.43043774366378784 - trainLoss: 0.41325855255126953\n",
      "cnt: 0 - valLoss: 0.43043702840805054 - trainLoss: 0.4132568836212158\n",
      "cnt: 0 - valLoss: 0.4304363429546356 - trainLoss: 0.4132552742958069\n",
      "cnt: 0 - valLoss: 0.4304355978965759 - trainLoss: 0.4132535457611084\n",
      "cnt: 0 - valLoss: 0.4304349720478058 - trainLoss: 0.4132518768310547\n",
      "cnt: 0 - valLoss: 0.4304342269897461 - trainLoss: 0.41325026750564575\n",
      "cnt: 0 - valLoss: 0.4304335415363312 - trainLoss: 0.41324859857559204\n",
      "cnt: 0 - valLoss: 0.43043285608291626 - trainLoss: 0.4132469594478607\n",
      "cnt: 0 - valLoss: 0.43043217062950134 - trainLoss: 0.413245290517807\n",
      "cnt: 0 - valLoss: 0.4304314851760864 - trainLoss: 0.4132436215877533\n",
      "cnt: 0 - valLoss: 0.4304307997226715 - trainLoss: 0.4132419526576996\n",
      "cnt: 0 - valLoss: 0.4304301142692566 - trainLoss: 0.41324031352996826\n",
      "cnt: 0 - valLoss: 0.4304294288158417 - trainLoss: 0.41323864459991455\n",
      "cnt: 0 - valLoss: 0.43042874336242676 - trainLoss: 0.41323697566986084\n",
      "cnt: 0 - valLoss: 0.43042802810668945 - trainLoss: 0.41323530673980713\n",
      "cnt: 0 - valLoss: 0.43042734265327454 - trainLoss: 0.4132336378097534\n",
      "cnt: 0 - valLoss: 0.4304266571998596 - trainLoss: 0.4132320284843445\n",
      "cnt: 0 - valLoss: 0.4304259121417999 - trainLoss: 0.41323035955429077\n",
      "cnt: 0 - valLoss: 0.4304252862930298 - trainLoss: 0.4132286310195923\n",
      "cnt: 0 - valLoss: 0.4304245412349701 - trainLoss: 0.41322702169418335\n",
      "cnt: 0 - valLoss: 0.43042391538619995 - trainLoss: 0.413225382566452\n",
      "cnt: 0 - valLoss: 0.43042322993278503 - trainLoss: 0.4132236838340759\n",
      "cnt: 0 - valLoss: 0.43042248487472534 - trainLoss: 0.4132220447063446\n",
      "cnt: 0 - valLoss: 0.4304217994213104 - trainLoss: 0.4132204055786133\n",
      "cnt: 0 - valLoss: 0.4304211139678955 - trainLoss: 0.4132187068462372\n",
      "cnt: 0 - valLoss: 0.43042048811912537 - trainLoss: 0.41321706771850586\n",
      "cnt: 0 - valLoss: 0.4304197430610657 - trainLoss: 0.41321539878845215\n",
      "cnt: 0 - valLoss: 0.43041905760765076 - trainLoss: 0.41321372985839844\n",
      "cnt: 0 - valLoss: 0.43041837215423584 - trainLoss: 0.4132121205329895\n",
      "cnt: 0 - valLoss: 0.4304176867008209 - trainLoss: 0.413210391998291\n",
      "cnt: 0 - valLoss: 0.430417001247406 - trainLoss: 0.4132087826728821\n",
      "cnt: 0 - valLoss: 0.4304163157939911 - trainLoss: 0.41320714354515076\n",
      "cnt: 0 - valLoss: 0.4304156005382538 - trainLoss: 0.41320544481277466\n",
      "cnt: 0 - valLoss: 0.43041491508483887 - trainLoss: 0.41320380568504333\n",
      "cnt: 0 - valLoss: 0.43041422963142395 - trainLoss: 0.4132021367549896\n",
      "cnt: 0 - valLoss: 0.43041354417800903 - trainLoss: 0.4132004678249359\n",
      "cnt: 0 - valLoss: 0.4304128885269165 - trainLoss: 0.413198858499527\n",
      "cnt: 0 - valLoss: 0.4304122030735016 - trainLoss: 0.4131971299648285\n",
      "cnt: 0 - valLoss: 0.4304114878177643 - trainLoss: 0.41319552063941956\n",
      "cnt: 0 - valLoss: 0.43041080236434937 - trainLoss: 0.41319388151168823\n",
      "cnt: 0 - valLoss: 0.43041011691093445 - trainLoss: 0.41319218277931213\n",
      "cnt: 0 - valLoss: 0.43040937185287476 - trainLoss: 0.4131905436515808\n",
      "cnt: 0 - valLoss: 0.4304087460041046 - trainLoss: 0.4131889045238495\n",
      "cnt: 0 - valLoss: 0.4304080009460449 - trainLoss: 0.4131872057914734\n",
      "cnt: 0 - valLoss: 0.43040731549263 - trainLoss: 0.41318556666374207\n",
      "cnt: 0 - valLoss: 0.4304066300392151 - trainLoss: 0.41318389773368835\n",
      "cnt: 0 - valLoss: 0.43040594458580017 - trainLoss: 0.41318222880363464\n",
      "cnt: 0 - valLoss: 0.43040525913238525 - trainLoss: 0.4131806194782257\n",
      "cnt: 0 - valLoss: 0.43040457367897034 - trainLoss: 0.413178950548172\n",
      "cnt: 0 - valLoss: 0.4304038882255554 - trainLoss: 0.4131772816181183\n",
      "cnt: 0 - valLoss: 0.4304032027721405 - trainLoss: 0.4131756126880646\n",
      "cnt: 0 - valLoss: 0.4304025173187256 - trainLoss: 0.41317397356033325\n",
      "cnt: 0 - valLoss: 0.43040183186531067 - trainLoss: 0.41317230463027954\n",
      "cnt: 0 - valLoss: 0.43040114641189575 - trainLoss: 0.41317063570022583\n",
      "cnt: 0 - valLoss: 0.43040046095848083 - trainLoss: 0.4131689667701721\n",
      "cnt: 0 - valLoss: 0.4303997755050659 - trainLoss: 0.4131673574447632\n",
      "cnt: 0 - valLoss: 0.430399090051651 - trainLoss: 0.4131656885147095\n",
      "cnt: 0 - valLoss: 0.4303983747959137 - trainLoss: 0.41316401958465576\n",
      "cnt: 0 - valLoss: 0.43039771914482117 - trainLoss: 0.41316238045692444\n",
      "cnt: 0 - valLoss: 0.43039703369140625 - trainLoss: 0.4131607115268707\n",
      "cnt: 0 - valLoss: 0.43039631843566895 - trainLoss: 0.413159042596817\n",
      "cnt: 0 - valLoss: 0.4303956627845764 - trainLoss: 0.4131574034690857\n",
      "cnt: 0 - valLoss: 0.4303949773311615 - trainLoss: 0.413155734539032\n",
      "cnt: 0 - valLoss: 0.4303942620754242 - trainLoss: 0.41315406560897827\n",
      "cnt: 0 - valLoss: 0.4303935766220093 - trainLoss: 0.41315239667892456\n",
      "cnt: 0 - valLoss: 0.430392861366272 - trainLoss: 0.41315072774887085\n",
      "cnt: 0 - valLoss: 0.43039214611053467 - trainLoss: 0.4131491184234619\n",
      "cnt: 0 - valLoss: 0.43039146065711975 - trainLoss: 0.4131474494934082\n",
      "cnt: 0 - valLoss: 0.4303908050060272 - trainLoss: 0.4131457805633545\n",
      "cnt: 0 - valLoss: 0.4303900897502899 - trainLoss: 0.41314414143562317\n",
      "cnt: 0 - valLoss: 0.430389404296875 - trainLoss: 0.41314247250556946\n",
      "cnt: 0 - valLoss: 0.4303887188434601 - trainLoss: 0.41314080357551575\n",
      "cnt: 0 - valLoss: 0.43038803339004517 - trainLoss: 0.4131391942501068\n",
      "cnt: 0 - valLoss: 0.43038737773895264 - trainLoss: 0.4131374657154083\n",
      "cnt: 0 - valLoss: 0.43038666248321533 - trainLoss: 0.4131358563899994\n",
      "cnt: 0 - valLoss: 0.4303859770298004 - trainLoss: 0.41313421726226807\n",
      "cnt: 0 - valLoss: 0.4303852915763855 - trainLoss: 0.41313251852989197\n",
      "cnt: 0 - valLoss: 0.4303846061229706 - trainLoss: 0.41313087940216064\n",
      "cnt: 0 - valLoss: 0.43038392066955566 - trainLoss: 0.4131292402744293\n",
      "cnt: 0 - valLoss: 0.43038323521614075 - trainLoss: 0.4131275415420532\n",
      "cnt: 0 - valLoss: 0.43038254976272583 - trainLoss: 0.4131259620189667\n",
      "cnt: 0 - valLoss: 0.4303818643093109 - trainLoss: 0.41312429308891296\n",
      "cnt: 0 - valLoss: 0.430381178855896 - trainLoss: 0.41312262415885925\n",
      "cnt: 0 - valLoss: 0.4303804934024811 - trainLoss: 0.41312095522880554\n",
      "cnt: 0 - valLoss: 0.43037980794906616 - trainLoss: 0.4131193161010742\n",
      "cnt: 0 - valLoss: 0.43037912249565125 - trainLoss: 0.4131176471710205\n",
      "cnt: 0 - valLoss: 0.43037843704223633 - trainLoss: 0.4131160378456116\n",
      "cnt: 0 - valLoss: 0.4303777515888214 - trainLoss: 0.41311436891555786\n",
      "cnt: 0 - valLoss: 0.4303770661354065 - trainLoss: 0.41311269998550415\n",
      "cnt: 0 - valLoss: 0.4303763806819916 - trainLoss: 0.41311103105545044\n",
      "cnt: 0 - valLoss: 0.43037569522857666 - trainLoss: 0.4131093919277191\n",
      "cnt: 0 - valLoss: 0.43037500977516174 - trainLoss: 0.4131077229976654\n",
      "cnt: 0 - valLoss: 0.4303743243217468 - trainLoss: 0.4131060540676117\n",
      "cnt: 0 - valLoss: 0.4303736388683319 - trainLoss: 0.41310441493988037\n",
      "cnt: 0 - valLoss: 0.430372953414917 - trainLoss: 0.41310274600982666\n",
      "cnt: 0 - valLoss: 0.43037229776382446 - trainLoss: 0.4131011366844177\n",
      "cnt: 0 - valLoss: 0.43037158250808716 - trainLoss: 0.413099467754364\n",
      "cnt: 0 - valLoss: 0.43037089705467224 - trainLoss: 0.4130978286266327\n",
      "cnt: 0 - valLoss: 0.4303702116012573 - trainLoss: 0.413096159696579\n",
      "cnt: 0 - valLoss: 0.4303695261478424 - trainLoss: 0.41309449076652527\n",
      "cnt: 0 - valLoss: 0.4303688406944275 - trainLoss: 0.41309288144111633\n",
      "cnt: 0 - valLoss: 0.4303681552410126 - trainLoss: 0.4130912125110626\n",
      "cnt: 0 - valLoss: 0.43036746978759766 - trainLoss: 0.4130895435810089\n",
      "cnt: 0 - valLoss: 0.43036678433418274 - trainLoss: 0.4130879044532776\n",
      "cnt: 0 - valLoss: 0.4303661286830902 - trainLoss: 0.4130862355232239\n",
      "cnt: 0 - valLoss: 0.4303654432296753 - trainLoss: 0.41308456659317017\n",
      "cnt: 0 - valLoss: 0.4303647577762604 - trainLoss: 0.41308292746543884\n",
      "cnt: 0 - valLoss: 0.43036407232284546 - trainLoss: 0.41308125853538513\n",
      "cnt: 0 - valLoss: 0.43036338686943054 - trainLoss: 0.4130796492099762\n",
      "cnt: 0 - valLoss: 0.430362731218338 - trainLoss: 0.4130779802799225\n",
      "cnt: 0 - valLoss: 0.4303620159626007 - trainLoss: 0.41307634115219116\n",
      "cnt: 0 - valLoss: 0.4303613305091858 - trainLoss: 0.41307467222213745\n",
      "cnt: 0 - valLoss: 0.43036067485809326 - trainLoss: 0.41307300329208374\n",
      "cnt: 0 - valLoss: 0.43035998940467834 - trainLoss: 0.4130713939666748\n",
      "cnt: 0 - valLoss: 0.4303593039512634 - trainLoss: 0.4130697250366211\n",
      "cnt: 0 - valLoss: 0.4303586184978485 - trainLoss: 0.4130680561065674\n",
      "cnt: 0 - valLoss: 0.4303579330444336 - trainLoss: 0.41306641697883606\n",
      "cnt: 0 - valLoss: 0.4303572475910187 - trainLoss: 0.41306474804878235\n",
      "cnt: 0 - valLoss: 0.43035656213760376 - trainLoss: 0.41306307911872864\n",
      "cnt: 0 - valLoss: 0.43035587668418884 - trainLoss: 0.4130614697933197\n",
      "cnt: 0 - valLoss: 0.4303551912307739 - trainLoss: 0.4130597710609436\n",
      "cnt: 0 - valLoss: 0.430354505777359 - trainLoss: 0.41305816173553467\n",
      "cnt: 0 - valLoss: 0.43035387992858887 - trainLoss: 0.41305649280548096\n",
      "cnt: 0 - valLoss: 0.43035319447517395 - trainLoss: 0.41305482387542725\n",
      "cnt: 0 - valLoss: 0.43035250902175903 - trainLoss: 0.4130531847476959\n",
      "cnt: 0 - valLoss: 0.4303518533706665 - trainLoss: 0.4130515158176422\n",
      "cnt: 0 - valLoss: 0.4303511679172516 - trainLoss: 0.4130499064922333\n",
      "cnt: 0 - valLoss: 0.43035048246383667 - trainLoss: 0.41304826736450195\n",
      "cnt: 0 - valLoss: 0.43034979701042175 - trainLoss: 0.41304659843444824\n",
      "cnt: 0 - valLoss: 0.43034911155700684 - trainLoss: 0.41304492950439453\n",
      "cnt: 0 - valLoss: 0.4303484261035919 - trainLoss: 0.4130432605743408\n",
      "cnt: 0 - valLoss: 0.430347740650177 - trainLoss: 0.4130416512489319\n",
      "cnt: 0 - valLoss: 0.4303470849990845 - trainLoss: 0.4130399823188782\n",
      "cnt: 0 - valLoss: 0.43034639954566956 - trainLoss: 0.41303831338882446\n",
      "cnt: 0 - valLoss: 0.43034571409225464 - trainLoss: 0.4130367040634155\n",
      "cnt: 0 - valLoss: 0.4303450882434845 - trainLoss: 0.41303500533103943\n",
      "cnt: 0 - valLoss: 0.4303444027900696 - trainLoss: 0.4130333662033081\n",
      "cnt: 0 - valLoss: 0.43034371733665466 - trainLoss: 0.4130316972732544\n",
      "cnt: 0 - valLoss: 0.43034303188323975 - trainLoss: 0.41303008794784546\n",
      "cnt: 0 - valLoss: 0.43034234642982483 - trainLoss: 0.41302841901779175\n",
      "cnt: 0 - valLoss: 0.4303417503833771 - trainLoss: 0.4130267798900604\n",
      "cnt: 0 - valLoss: 0.4303410053253174 - trainLoss: 0.4130251109600067\n",
      "cnt: 0 - valLoss: 0.43034037947654724 - trainLoss: 0.413023442029953\n",
      "cnt: 0 - valLoss: 0.43033963441848755 - trainLoss: 0.41302183270454407\n",
      "cnt: 0 - valLoss: 0.43033894896507263 - trainLoss: 0.41302019357681274\n",
      "cnt: 0 - valLoss: 0.4303383231163025 - trainLoss: 0.41301852464675903\n",
      "cnt: 0 - valLoss: 0.4303376376628876 - trainLoss: 0.4130168557167053\n",
      "cnt: 0 - valLoss: 0.43033695220947266 - trainLoss: 0.413015216588974\n",
      "cnt: 0 - valLoss: 0.43033626675605774 - trainLoss: 0.41301360726356506\n",
      "cnt: 0 - valLoss: 0.4303356111049652 - trainLoss: 0.41301193833351135\n",
      "cnt: 0 - valLoss: 0.4303349256515503 - trainLoss: 0.41301026940345764\n",
      "cnt: 0 - valLoss: 0.43033427000045776 - trainLoss: 0.4130086302757263\n",
      "cnt: 0 - valLoss: 0.43033358454704285 - trainLoss: 0.4130069613456726\n",
      "cnt: 0 - valLoss: 0.43033289909362793 - trainLoss: 0.4130052924156189\n",
      "cnt: 0 - valLoss: 0.4303322732448578 - trainLoss: 0.41300368309020996\n",
      "cnt: 0 - valLoss: 0.43033158779144287 - trainLoss: 0.41300204396247864\n",
      "cnt: 0 - valLoss: 0.43033093214035034 - trainLoss: 0.41300034523010254\n",
      "cnt: 0 - valLoss: 0.4303302466869354 - trainLoss: 0.4129987061023712\n",
      "cnt: 0 - valLoss: 0.4303295612335205 - trainLoss: 0.4129970669746399\n",
      "cnt: 0 - valLoss: 0.43032893538475037 - trainLoss: 0.4129953980445862\n",
      "cnt: 0 - valLoss: 0.43032824993133545 - trainLoss: 0.41299378871917725\n",
      "cnt: 0 - valLoss: 0.4303275942802429 - trainLoss: 0.41299211978912354\n",
      "cnt: 0 - valLoss: 0.430326908826828 - trainLoss: 0.4129904806613922\n",
      "cnt: 0 - valLoss: 0.43032628297805786 - trainLoss: 0.4129888117313385\n",
      "cnt: 0 - valLoss: 0.43032553791999817 - trainLoss: 0.41298720240592957\n",
      "cnt: 0 - valLoss: 0.430324912071228 - trainLoss: 0.41298553347587585\n",
      "cnt: 0 - valLoss: 0.4303241968154907 - trainLoss: 0.41298389434814453\n",
      "cnt: 0 - valLoss: 0.4303235709667206 - trainLoss: 0.4129822850227356\n",
      "cnt: 0 - valLoss: 0.43032288551330566 - trainLoss: 0.4129805564880371\n",
      "cnt: 0 - valLoss: 0.43032220005989075 - trainLoss: 0.4129789471626282\n",
      "cnt: 0 - valLoss: 0.4303215444087982 - trainLoss: 0.41297730803489685\n",
      "cnt: 0 - valLoss: 0.4303208589553833 - trainLoss: 0.41297563910484314\n",
      "cnt: 0 - valLoss: 0.43032023310661316 - trainLoss: 0.4129739999771118\n",
      "cnt: 0 - valLoss: 0.43031954765319824 - trainLoss: 0.4129723310470581\n",
      "cnt: 0 - valLoss: 0.4303188621997833 - trainLoss: 0.41297072172164917\n",
      "cnt: 0 - valLoss: 0.4303182065486908 - trainLoss: 0.41296908259391785\n",
      "cnt: 0 - valLoss: 0.43031758069992065 - trainLoss: 0.41296741366386414\n",
      "cnt: 0 - valLoss: 0.43031689524650574 - trainLoss: 0.4129657447338104\n",
      "cnt: 0 - valLoss: 0.4303162395954132 - trainLoss: 0.4129641354084015\n",
      "cnt: 0 - valLoss: 0.4303155541419983 - trainLoss: 0.41296249628067017\n",
      "cnt: 0 - valLoss: 0.4303148686885834 - trainLoss: 0.41296082735061646\n",
      "cnt: 0 - valLoss: 0.43031418323516846 - trainLoss: 0.41295915842056274\n",
      "cnt: 0 - valLoss: 0.4303135275840759 - trainLoss: 0.4129575192928314\n",
      "cnt: 0 - valLoss: 0.4303129017353058 - trainLoss: 0.4129558503627777\n",
      "cnt: 0 - valLoss: 0.43031221628189087 - trainLoss: 0.4129542410373688\n",
      "cnt: 0 - valLoss: 0.43031153082847595 - trainLoss: 0.41295260190963745\n",
      "cnt: 0 - valLoss: 0.4303108751773834 - trainLoss: 0.41295093297958374\n",
      "cnt: 0 - valLoss: 0.4303101897239685 - trainLoss: 0.41294926404953003\n",
      "cnt: 0 - valLoss: 0.43030959367752075 - trainLoss: 0.4129475951194763\n",
      "cnt: 0 - valLoss: 0.43030887842178345 - trainLoss: 0.4129459857940674\n",
      "cnt: 0 - valLoss: 0.4303082227706909 - trainLoss: 0.41294434666633606\n",
      "cnt: 0 - valLoss: 0.430307537317276 - trainLoss: 0.41294267773628235\n",
      "cnt: 0 - valLoss: 0.43030688166618347 - trainLoss: 0.4129410684108734\n",
      "cnt: 0 - valLoss: 0.43030625581741333 - trainLoss: 0.4129393994808197\n",
      "cnt: 0 - valLoss: 0.4303055703639984 - trainLoss: 0.4129377603530884\n",
      "cnt: 0 - valLoss: 0.4303048849105835 - trainLoss: 0.41293612122535706\n",
      "cnt: 0 - valLoss: 0.43030422925949097 - trainLoss: 0.4129345118999481\n",
      "cnt: 0 - valLoss: 0.4303036034107208 - trainLoss: 0.4129328429698944\n",
      "cnt: 0 - valLoss: 0.4303029179573059 - trainLoss: 0.4129311740398407\n",
      "cnt: 0 - valLoss: 0.430302232503891 - trainLoss: 0.4129295349121094\n",
      "cnt: 0 - valLoss: 0.4303015470504761 - trainLoss: 0.41292792558670044\n",
      "cnt: 0 - valLoss: 0.4303009510040283 - trainLoss: 0.41292625665664673\n",
      "cnt: 0 - valLoss: 0.4303002655506134 - trainLoss: 0.412924587726593\n",
      "cnt: 0 - valLoss: 0.4302995800971985 - trainLoss: 0.4129229485988617\n",
      "cnt: 0 - valLoss: 0.43029892444610596 - trainLoss: 0.41292130947113037\n",
      "cnt: 0 - valLoss: 0.4302982687950134 - trainLoss: 0.41291964054107666\n",
      "cnt: 0 - valLoss: 0.4302975833415985 - trainLoss: 0.41291797161102295\n",
      "cnt: 0 - valLoss: 0.4302968978881836 - trainLoss: 0.412916362285614\n",
      "cnt: 0 - valLoss: 0.43029627203941345 - trainLoss: 0.4129147231578827\n",
      "cnt: 0 - valLoss: 0.4302956163883209 - trainLoss: 0.412913054227829\n",
      "cnt: 0 - valLoss: 0.430294930934906 - trainLoss: 0.41291144490242004\n",
      "cnt: 0 - valLoss: 0.4302942454814911 - trainLoss: 0.4129098057746887\n",
      "cnt: 0 - valLoss: 0.43029361963272095 - trainLoss: 0.412908136844635\n",
      "cnt: 0 - valLoss: 0.4302929639816284 - trainLoss: 0.4129065275192261\n",
      "cnt: 0 - valLoss: 0.4302922785282135 - trainLoss: 0.41290488839149475\n",
      "cnt: 0 - valLoss: 0.4302915930747986 - trainLoss: 0.41290321946144104\n",
      "cnt: 0 - valLoss: 0.43029090762138367 - trainLoss: 0.41290155053138733\n",
      "cnt: 0 - valLoss: 0.4302903115749359 - trainLoss: 0.412899911403656\n",
      "cnt: 0 - valLoss: 0.4302895665168762 - trainLoss: 0.41289830207824707\n",
      "cnt: 0 - valLoss: 0.4302889406681061 - trainLoss: 0.41289663314819336\n",
      "cnt: 0 - valLoss: 0.43028828501701355 - trainLoss: 0.41289499402046204\n",
      "cnt: 0 - valLoss: 0.43028759956359863 - trainLoss: 0.4128933250904083\n",
      "cnt: 0 - valLoss: 0.4302869737148285 - trainLoss: 0.4128917157649994\n",
      "cnt: 0 - valLoss: 0.4302862882614136 - trainLoss: 0.41289007663726807\n",
      "cnt: 0 - valLoss: 0.43028563261032104 - trainLoss: 0.41288840770721436\n",
      "cnt: 0 - valLoss: 0.43028491735458374 - trainLoss: 0.41288673877716064\n",
      "cnt: 0 - valLoss: 0.430284321308136 - trainLoss: 0.4128850996494293\n",
      "cnt: 0 - valLoss: 0.43028363585472107 - trainLoss: 0.4128834307193756\n",
      "cnt: 0 - valLoss: 0.43028298020362854 - trainLoss: 0.41288185119628906\n",
      "cnt: 0 - valLoss: 0.4302822947502136 - trainLoss: 0.41288018226623535\n",
      "cnt: 0 - valLoss: 0.4302816390991211 - trainLoss: 0.41287851333618164\n",
      "cnt: 0 - valLoss: 0.4302809536457062 - trainLoss: 0.4128769040107727\n",
      "cnt: 0 - valLoss: 0.4302803575992584 - trainLoss: 0.4128752648830414\n",
      "cnt: 0 - valLoss: 0.4302796423435211 - trainLoss: 0.41287359595298767\n",
      "cnt: 0 - valLoss: 0.4302789866924286 - trainLoss: 0.4128720164299011\n",
      "cnt: 0 - valLoss: 0.43027836084365845 - trainLoss: 0.41287028789520264\n",
      "cnt: 0 - valLoss: 0.4302777051925659 - trainLoss: 0.4128686785697937\n",
      "cnt: 0 - valLoss: 0.430277019739151 - trainLoss: 0.41286700963974\n",
      "cnt: 0 - valLoss: 0.43027636408805847 - trainLoss: 0.41286537051200867\n",
      "cnt: 0 - valLoss: 0.43027564883232117 - trainLoss: 0.41286376118659973\n",
      "cnt: 0 - valLoss: 0.43027499318122864 - trainLoss: 0.412862092256546\n",
      "cnt: 0 - valLoss: 0.4302743673324585 - trainLoss: 0.4128604531288147\n",
      "cnt: 0 - valLoss: 0.4302736818790436 - trainLoss: 0.41285884380340576\n",
      "cnt: 0 - valLoss: 0.43027302622795105 - trainLoss: 0.41285717487335205\n",
      "cnt: 0 - valLoss: 0.4302724003791809 - trainLoss: 0.4128555357456207\n",
      "cnt: 0 - valLoss: 0.430271714925766 - trainLoss: 0.412853866815567\n",
      "cnt: 0 - valLoss: 0.43027105927467346 - trainLoss: 0.4128522276878357\n",
      "cnt: 0 - valLoss: 0.4302704334259033 - trainLoss: 0.41285061836242676\n",
      "cnt: 0 - valLoss: 0.4302697479724884 - trainLoss: 0.41284894943237305\n",
      "cnt: 0 - valLoss: 0.4302690625190735 - trainLoss: 0.4128473103046417\n",
      "cnt: 0 - valLoss: 0.43026837706565857 - trainLoss: 0.412845641374588\n",
      "cnt: 0 - valLoss: 0.4302677512168884 - trainLoss: 0.4128440320491791\n",
      "cnt: 0 - valLoss: 0.4302670657634735 - trainLoss: 0.41284236311912537\n",
      "cnt: 0 - valLoss: 0.43026643991470337 - trainLoss: 0.41284072399139404\n",
      "cnt: 0 - valLoss: 0.43026578426361084 - trainLoss: 0.4128391146659851\n",
      "cnt: 0 - valLoss: 0.4302650988101959 - trainLoss: 0.4128374755382538\n",
      "cnt: 0 - valLoss: 0.4302644729614258 - trainLoss: 0.4128358066082001\n",
      "cnt: 0 - valLoss: 0.43026378750801086 - trainLoss: 0.41283416748046875\n",
      "cnt: 0 - valLoss: 0.43026313185691833 - trainLoss: 0.41283249855041504\n",
      "cnt: 0 - valLoss: 0.4302624464035034 - trainLoss: 0.4128308892250061\n",
      "cnt: 0 - valLoss: 0.4302617907524109 - trainLoss: 0.4128292202949524\n",
      "cnt: 0 - valLoss: 0.43026116490364075 - trainLoss: 0.41282758116722107\n",
      "cnt: 0 - valLoss: 0.43026047945022583 - trainLoss: 0.41282597184181213\n",
      "cnt: 0 - valLoss: 0.4302597939968109 - trainLoss: 0.4128243029117584\n",
      "cnt: 0 - valLoss: 0.430259108543396 - trainLoss: 0.4128226637840271\n",
      "cnt: 0 - valLoss: 0.43025851249694824 - trainLoss: 0.4128209948539734\n",
      "cnt: 0 - valLoss: 0.4302578270435333 - trainLoss: 0.41281935572624207\n",
      "cnt: 0 - valLoss: 0.4302571713924408 - trainLoss: 0.41281768679618835\n",
      "cnt: 0 - valLoss: 0.43025654554367065 - trainLoss: 0.4128161072731018\n",
      "cnt: 0 - valLoss: 0.43025586009025574 - trainLoss: 0.41281449794769287\n",
      "cnt: 0 - valLoss: 0.4302552044391632 - trainLoss: 0.4128127694129944\n",
      "cnt: 0 - valLoss: 0.4302545189857483 - trainLoss: 0.41281116008758545\n",
      "cnt: 0 - valLoss: 0.43025386333465576 - trainLoss: 0.41280949115753174\n",
      "cnt: 0 - valLoss: 0.43025317788124084 - trainLoss: 0.4128078520298004\n",
      "cnt: 0 - valLoss: 0.4302525520324707 - trainLoss: 0.4128062427043915\n",
      "cnt: 0 - valLoss: 0.4302518665790558 - trainLoss: 0.41280460357666016\n",
      "cnt: 0 - valLoss: 0.43025118112564087 - trainLoss: 0.41280293464660645\n",
      "cnt: 0 - valLoss: 0.43025052547454834 - trainLoss: 0.4128012955188751\n",
      "cnt: 0 - valLoss: 0.4302498996257782 - trainLoss: 0.4127996265888214\n",
      "cnt: 0 - valLoss: 0.43024924397468567 - trainLoss: 0.4127980172634125\n",
      "cnt: 0 - valLoss: 0.43024855852127075 - trainLoss: 0.41279634833335876\n",
      "cnt: 0 - valLoss: 0.4302479028701782 - trainLoss: 0.41279470920562744\n",
      "cnt: 0 - valLoss: 0.4302472770214081 - trainLoss: 0.4127930998802185\n",
      "cnt: 0 - valLoss: 0.43024659156799316 - trainLoss: 0.4127914607524872\n",
      "cnt: 0 - valLoss: 0.43024593591690063 - trainLoss: 0.41278982162475586\n",
      "cnt: 0 - valLoss: 0.4302452504634857 - trainLoss: 0.41278815269470215\n",
      "cnt: 0 - valLoss: 0.4302446246147156 - trainLoss: 0.41278648376464844\n",
      "cnt: 0 - valLoss: 0.43024396896362305 - trainLoss: 0.4127848744392395\n",
      "cnt: 0 - valLoss: 0.4302433431148529 - trainLoss: 0.4127832353115082\n",
      "cnt: 0 - valLoss: 0.430242657661438 - trainLoss: 0.41278162598609924\n",
      "cnt: 0 - valLoss: 0.43024197220802307 - trainLoss: 0.41277995705604553\n",
      "cnt: 0 - valLoss: 0.43024131655693054 - trainLoss: 0.4127783179283142\n",
      "cnt: 0 - valLoss: 0.430240660905838 - trainLoss: 0.4127767086029053\n",
      "cnt: 0 - valLoss: 0.4302399754524231 - trainLoss: 0.41277503967285156\n",
      "cnt: 0 - valLoss: 0.43023934960365295 - trainLoss: 0.41277340054512024\n",
      "cnt: 0 - valLoss: 0.43023866415023804 - trainLoss: 0.4127717614173889\n",
      "cnt: 0 - valLoss: 0.4302380084991455 - trainLoss: 0.4127700924873352\n",
      "cnt: 0 - valLoss: 0.43023738265037537 - trainLoss: 0.4127684235572815\n",
      "cnt: 0 - valLoss: 0.43023672699928284 - trainLoss: 0.41276684403419495\n",
      "cnt: 0 - valLoss: 0.4302360415458679 - trainLoss: 0.41276517510414124\n",
      "cnt: 0 - valLoss: 0.4302354156970978 - trainLoss: 0.4127635657787323\n",
      "cnt: 0 - valLoss: 0.43023476004600525 - trainLoss: 0.4127618968486786\n",
      "cnt: 0 - valLoss: 0.43023407459259033 - trainLoss: 0.41276025772094727\n",
      "cnt: 0 - valLoss: 0.4302333891391754 - trainLoss: 0.41275861859321594\n",
      "cnt: 0 - valLoss: 0.43023279309272766 - trainLoss: 0.412757009267807\n",
      "cnt: 0 - valLoss: 0.43023210763931274 - trainLoss: 0.4127553403377533\n",
      "cnt: 0 - valLoss: 0.4302314519882202 - trainLoss: 0.412753701210022\n",
      "cnt: 0 - valLoss: 0.4302307665348053 - trainLoss: 0.41275209188461304\n",
      "cnt: 0 - valLoss: 0.43023014068603516 - trainLoss: 0.4127504229545593\n",
      "cnt: 0 - valLoss: 0.4302295446395874 - trainLoss: 0.412748783826828\n",
      "cnt: 0 - valLoss: 0.4302288293838501 - trainLoss: 0.41274717450141907\n",
      "cnt: 0 - valLoss: 0.43022820353507996 - trainLoss: 0.41274550557136536\n",
      "cnt: 0 - valLoss: 0.4302275478839874 - trainLoss: 0.41274386644363403\n",
      "cnt: 0 - valLoss: 0.4302268624305725 - trainLoss: 0.4127422273159027\n",
      "cnt: 0 - valLoss: 0.43022626638412476 - trainLoss: 0.4127406179904938\n",
      "cnt: 0 - valLoss: 0.43022558093070984 - trainLoss: 0.41273894906044006\n",
      "cnt: 0 - valLoss: 0.4302249550819397 - trainLoss: 0.41273730993270874\n",
      "cnt: 0 - valLoss: 0.43022429943084717 - trainLoss: 0.41273564100265503\n",
      "cnt: 0 - valLoss: 0.43022361397743225 - trainLoss: 0.4127340316772461\n",
      "cnt: 0 - valLoss: 0.4302230179309845 - trainLoss: 0.41273239254951477\n",
      "cnt: 0 - valLoss: 0.43022236227989197 - trainLoss: 0.41273078322410583\n",
      "cnt: 0 - valLoss: 0.43022167682647705 - trainLoss: 0.4127291440963745\n",
      "cnt: 0 - valLoss: 0.4302210509777069 - trainLoss: 0.4127274751663208\n",
      "cnt: 0 - valLoss: 0.43022045493125916 - trainLoss: 0.4127258360385895\n",
      "cnt: 0 - valLoss: 0.43021976947784424 - trainLoss: 0.41272422671318054\n",
      "cnt: 0 - valLoss: 0.4302191436290741 - trainLoss: 0.41272255778312683\n",
      "cnt: 0 - valLoss: 0.4302184581756592 - trainLoss: 0.4127209186553955\n",
      "cnt: 0 - valLoss: 0.43021777272224426 - trainLoss: 0.4127193093299866\n",
      "cnt: 0 - valLoss: 0.4302171766757965 - trainLoss: 0.41271767020225525\n",
      "cnt: 0 - valLoss: 0.4302164912223816 - trainLoss: 0.41271600127220154\n",
      "cnt: 0 - valLoss: 0.43021589517593384 - trainLoss: 0.412714421749115\n",
      "cnt: 0 - valLoss: 0.4302152693271637 - trainLoss: 0.4127126932144165\n",
      "cnt: 0 - valLoss: 0.4302145838737488 - trainLoss: 0.41271111369132996\n",
      "cnt: 0 - valLoss: 0.43021392822265625 - trainLoss: 0.41270944476127625\n",
      "cnt: 0 - valLoss: 0.4302132725715637 - trainLoss: 0.4127078354358673\n",
      "cnt: 0 - valLoss: 0.4302126467227936 - trainLoss: 0.412706196308136\n",
      "cnt: 0 - valLoss: 0.4302120506763458 - trainLoss: 0.4127045273780823\n",
      "cnt: 0 - valLoss: 0.4302113652229309 - trainLoss: 0.41270291805267334\n",
      "cnt: 0 - valLoss: 0.4302107095718384 - trainLoss: 0.412701278924942\n",
      "cnt: 0 - valLoss: 0.43021008372306824 - trainLoss: 0.4126996397972107\n",
      "cnt: 0 - valLoss: 0.4302094280719757 - trainLoss: 0.412697970867157\n",
      "cnt: 0 - valLoss: 0.4302087724208832 - trainLoss: 0.41269636154174805\n",
      "cnt: 0 - valLoss: 0.43020814657211304 - trainLoss: 0.4126947224140167\n",
      "cnt: 0 - valLoss: 0.4302074909210205 - trainLoss: 0.4126931130886078\n",
      "cnt: 0 - valLoss: 0.43020686507225037 - trainLoss: 0.41269147396087646\n",
      "cnt: 0 - valLoss: 0.4302062690258026 - trainLoss: 0.41268986463546753\n",
      "cnt: 0 - valLoss: 0.4302056133747101 - trainLoss: 0.4126881957054138\n",
      "cnt: 0 - valLoss: 0.43020492792129517 - trainLoss: 0.4126865565776825\n",
      "cnt: 0 - valLoss: 0.4302043318748474 - trainLoss: 0.4126848876476288\n",
      "cnt: 0 - valLoss: 0.4302035868167877 - trainLoss: 0.41268324851989746\n",
      "cnt: 0 - valLoss: 0.4302028715610504 - trainLoss: 0.4126816391944885\n",
      "cnt: 0 - valLoss: 0.4302021265029907 - trainLoss: 0.4126800000667572\n",
      "cnt: 0 - valLoss: 0.4302014410495758 - trainLoss: 0.41267839074134827\n",
      "cnt: 0 - valLoss: 0.4302007257938385 - trainLoss: 0.41267675161361694\n",
      "cnt: 0 - valLoss: 0.4302000403404236 - trainLoss: 0.4126751124858856\n",
      "cnt: 0 - valLoss: 0.4301993250846863 - trainLoss: 0.4126735031604767\n",
      "cnt: 0 - valLoss: 0.4301985800266266 - trainLoss: 0.41267186403274536\n",
      "cnt: 0 - valLoss: 0.43019789457321167 - trainLoss: 0.4126702547073364\n",
      "cnt: 0 - valLoss: 0.43019720911979675 - trainLoss: 0.4126686155796051\n",
      "cnt: 0 - valLoss: 0.4301964342594147 - trainLoss: 0.41266700625419617\n",
      "cnt: 0 - valLoss: 0.43019574880599976 - trainLoss: 0.41266536712646484\n",
      "cnt: 0 - valLoss: 0.43019506335258484 - trainLoss: 0.4126637279987335\n",
      "cnt: 0 - valLoss: 0.43019434809684753 - trainLoss: 0.4126621186733246\n",
      "cnt: 0 - valLoss: 0.43019360303878784 - trainLoss: 0.41266047954559326\n",
      "cnt: 0 - valLoss: 0.4301929175853729 - trainLoss: 0.4126588702201843\n",
      "cnt: 0 - valLoss: 0.430192232131958 - trainLoss: 0.4126572906970978\n",
      "cnt: 0 - valLoss: 0.4301915466785431 - trainLoss: 0.41265562176704407\n",
      "cnt: 0 - valLoss: 0.4301908314228058 - trainLoss: 0.41265401244163513\n",
      "cnt: 0 - valLoss: 0.4301900863647461 - trainLoss: 0.4126523435115814\n",
      "cnt: 0 - valLoss: 0.4301894009113312 - trainLoss: 0.4126507341861725\n",
      "cnt: 0 - valLoss: 0.43018871545791626 - trainLoss: 0.41264909505844116\n",
      "cnt: 0 - valLoss: 0.43018803000450134 - trainLoss: 0.4126475155353546\n",
      "cnt: 0 - valLoss: 0.43018731474876404 - trainLoss: 0.4126458466053009\n",
      "cnt: 0 - valLoss: 0.4301866292953491 - trainLoss: 0.41264423727989197\n",
      "cnt: 0 - valLoss: 0.4301859736442566 - trainLoss: 0.41264262795448303\n",
      "cnt: 0 - valLoss: 0.4301852583885193 - trainLoss: 0.4126410186290741\n",
      "cnt: 0 - valLoss: 0.43018457293510437 - trainLoss: 0.4126393795013428\n",
      "cnt: 0 - valLoss: 0.43018388748168945 - trainLoss: 0.41263777017593384\n",
      "cnt: 0 - valLoss: 0.43018320202827454 - trainLoss: 0.4126361310482025\n",
      "cnt: 0 - valLoss: 0.4301825165748596 - trainLoss: 0.4126345217227936\n",
      "cnt: 0 - valLoss: 0.4301818311214447 - trainLoss: 0.41263288259506226\n",
      "cnt: 0 - valLoss: 0.430181086063385 - trainLoss: 0.4126313030719757\n",
      "cnt: 0 - valLoss: 0.43018046021461487 - trainLoss: 0.4126296639442444\n",
      "cnt: 0 - valLoss: 0.4301797151565552 - trainLoss: 0.41262805461883545\n",
      "cnt: 0 - valLoss: 0.43017902970314026 - trainLoss: 0.4126264154911041\n",
      "cnt: 0 - valLoss: 0.43017834424972534 - trainLoss: 0.4126248061656952\n",
      "cnt: 0 - valLoss: 0.4301776587963104 - trainLoss: 0.41262316703796387\n",
      "cnt: 0 - valLoss: 0.4301769733428955 - trainLoss: 0.41262149810791016\n",
      "cnt: 0 - valLoss: 0.4301762878894806 - trainLoss: 0.4126199185848236\n",
      "cnt: 0 - valLoss: 0.4301756024360657 - trainLoss: 0.4126182794570923\n",
      "cnt: 0 - valLoss: 0.43017491698265076 - trainLoss: 0.41261667013168335\n",
      "cnt: 0 - valLoss: 0.43017423152923584 - trainLoss: 0.412615031003952\n",
      "cnt: 0 - valLoss: 0.4301735460758209 - trainLoss: 0.4126134216785431\n",
      "cnt: 0 - valLoss: 0.4301729202270508 - trainLoss: 0.41261178255081177\n",
      "cnt: 0 - valLoss: 0.4301721751689911 - trainLoss: 0.41261014342308044\n",
      "cnt: 0 - valLoss: 0.43017154932022095 - trainLoss: 0.4126085340976715\n",
      "cnt: 0 - valLoss: 0.43017080426216125 - trainLoss: 0.4126068949699402\n",
      "cnt: 0 - valLoss: 0.43017011880874634 - trainLoss: 0.41260528564453125\n",
      "cnt: 0 - valLoss: 0.4301694333553314 - trainLoss: 0.4126036465167999\n",
      "cnt: 0 - valLoss: 0.4301687479019165 - trainLoss: 0.4126020073890686\n",
      "cnt: 0 - valLoss: 0.4301680624485016 - trainLoss: 0.41260042786598206\n",
      "cnt: 0 - valLoss: 0.43016737699508667 - trainLoss: 0.4125988185405731\n",
      "cnt: 0 - valLoss: 0.43016675114631653 - trainLoss: 0.4125971496105194\n",
      "cnt: 0 - valLoss: 0.4301660656929016 - trainLoss: 0.4125955104827881\n",
      "cnt: 0 - valLoss: 0.4301653802394867 - trainLoss: 0.41259393095970154\n",
      "cnt: 0 - valLoss: 0.4301646947860718 - trainLoss: 0.4125922918319702\n",
      "cnt: 0 - valLoss: 0.4301639795303345 - trainLoss: 0.4125906825065613\n",
      "cnt: 0 - valLoss: 0.43016329407691956 - trainLoss: 0.41258904337882996\n",
      "cnt: 0 - valLoss: 0.4301626682281494 - trainLoss: 0.412587434053421\n",
      "cnt: 0 - valLoss: 0.4301619231700897 - trainLoss: 0.4125857949256897\n",
      "cnt: 0 - valLoss: 0.43016132712364197 - trainLoss: 0.41258418560028076\n",
      "cnt: 0 - valLoss: 0.43016061186790466 - trainLoss: 0.4125825762748718\n",
      "cnt: 0 - valLoss: 0.43015992641448975 - trainLoss: 0.4125809669494629\n",
      "cnt: 0 - valLoss: 0.4301592707633972 - trainLoss: 0.41257932782173157\n",
      "cnt: 0 - valLoss: 0.4301585555076599 - trainLoss: 0.41257771849632263\n",
      "cnt: 0 - valLoss: 0.4301578998565674 - trainLoss: 0.4125760793685913\n",
      "cnt: 0 - valLoss: 0.43015721440315247 - trainLoss: 0.4125744700431824\n",
      "cnt: 0 - valLoss: 0.4301565885543823 - trainLoss: 0.41257283091545105\n",
      "cnt: 0 - valLoss: 0.4301559031009674 - trainLoss: 0.4125711917877197\n",
      "cnt: 0 - valLoss: 0.4301552176475525 - trainLoss: 0.4125695824623108\n",
      "cnt: 0 - valLoss: 0.4301545321941376 - trainLoss: 0.41256794333457947\n",
      "cnt: 0 - valLoss: 0.43015384674072266 - trainLoss: 0.4125663638114929\n",
      "cnt: 0 - valLoss: 0.4301531910896301 - trainLoss: 0.4125646948814392\n",
      "cnt: 0 - valLoss: 0.4301525056362152 - trainLoss: 0.41256311535835266\n",
      "cnt: 0 - valLoss: 0.4301518201828003 - trainLoss: 0.41256147623062134\n",
      "cnt: 0 - valLoss: 0.43015116453170776 - trainLoss: 0.4125598669052124\n",
      "cnt: 0 - valLoss: 0.43015047907829285 - trainLoss: 0.4125581979751587\n",
      "cnt: 0 - valLoss: 0.43014979362487793 - trainLoss: 0.41255661845207214\n",
      "cnt: 0 - valLoss: 0.430149108171463 - trainLoss: 0.4125549793243408\n",
      "cnt: 0 - valLoss: 0.43014848232269287 - trainLoss: 0.4125533998012543\n",
      "cnt: 0 - valLoss: 0.43014782667160034 - trainLoss: 0.41255176067352295\n",
      "cnt: 0 - valLoss: 0.4301471412181854 - trainLoss: 0.412550151348114\n",
      "cnt: 0 - valLoss: 0.4301464557647705 - trainLoss: 0.4125485122203827\n",
      "cnt: 0 - valLoss: 0.43014582991600037 - trainLoss: 0.41254690289497375\n",
      "cnt: 0 - valLoss: 0.43014517426490784 - trainLoss: 0.41254526376724243\n",
      "cnt: 0 - valLoss: 0.4301444888114929 - trainLoss: 0.4125436544418335\n",
      "cnt: 0 - valLoss: 0.430143803358078 - trainLoss: 0.41254204511642456\n",
      "cnt: 0 - valLoss: 0.43014317750930786 - trainLoss: 0.41254037618637085\n",
      "cnt: 0 - valLoss: 0.43014252185821533 - trainLoss: 0.4125387668609619\n",
      "cnt: 0 - valLoss: 0.4301418364048004 - trainLoss: 0.41253718733787537\n",
      "cnt: 0 - valLoss: 0.4301411807537079 - trainLoss: 0.41253554821014404\n",
      "cnt: 0 - valLoss: 0.43014055490493774 - trainLoss: 0.4125339388847351\n",
      "cnt: 0 - valLoss: 0.4301398694515228 - trainLoss: 0.41253232955932617\n",
      "cnt: 0 - valLoss: 0.4301392138004303 - trainLoss: 0.41253072023391724\n",
      "cnt: 0 - valLoss: 0.43013858795166016 - trainLoss: 0.4125290811061859\n",
      "cnt: 0 - valLoss: 0.43013790249824524 - trainLoss: 0.412527471780777\n",
      "cnt: 0 - valLoss: 0.4301372170448303 - trainLoss: 0.41252583265304565\n",
      "cnt: 0 - valLoss: 0.4301365911960602 - trainLoss: 0.4125242233276367\n",
      "cnt: 0 - valLoss: 0.43013590574264526 - trainLoss: 0.4125226140022278\n",
      "cnt: 0 - valLoss: 0.43013522028923035 - trainLoss: 0.4125209450721741\n",
      "cnt: 0 - valLoss: 0.4301345944404602 - trainLoss: 0.4125193655490875\n",
      "cnt: 0 - valLoss: 0.4301339387893677 - trainLoss: 0.4125176966190338\n",
      "cnt: 0 - valLoss: 0.43013325333595276 - trainLoss: 0.4125160872936249\n",
      "cnt: 0 - valLoss: 0.4301326274871826 - trainLoss: 0.41251447796821594\n",
      "cnt: 0 - valLoss: 0.4301319718360901 - trainLoss: 0.412512868642807\n",
      "cnt: 0 - valLoss: 0.43013134598731995 - trainLoss: 0.41251128911972046\n",
      "cnt: 0 - valLoss: 0.43013066053390503 - trainLoss: 0.41250964999198914\n",
      "cnt: 0 - valLoss: 0.4301300048828125 - trainLoss: 0.4125080406665802\n",
      "cnt: 0 - valLoss: 0.4301293194293976 - trainLoss: 0.4125064015388489\n",
      "cnt: 0 - valLoss: 0.43012866377830505 - trainLoss: 0.41250476241111755\n",
      "cnt: 0 - valLoss: 0.4301280379295349 - trainLoss: 0.4125031530857086\n",
      "cnt: 0 - valLoss: 0.43012735247612 - trainLoss: 0.41250157356262207\n",
      "cnt: 0 - valLoss: 0.43012675642967224 - trainLoss: 0.41249993443489075\n",
      "cnt: 0 - valLoss: 0.4301260709762573 - trainLoss: 0.4124983549118042\n",
      "cnt: 0 - valLoss: 0.4301254153251648 - trainLoss: 0.4124966859817505\n",
      "cnt: 0 - valLoss: 0.4301247298717499 - trainLoss: 0.41249510645866394\n",
      "cnt: 0 - valLoss: 0.4301241338253021 - trainLoss: 0.4124934673309326\n",
      "cnt: 0 - valLoss: 0.4301234483718872 - trainLoss: 0.4124918580055237\n",
      "cnt: 0 - valLoss: 0.4301227927207947 - trainLoss: 0.41249021887779236\n",
      "cnt: 0 - valLoss: 0.43012216687202454 - trainLoss: 0.4124886095523834\n",
      "cnt: 0 - valLoss: 0.4301214814186096 - trainLoss: 0.4124870002269745\n",
      "cnt: 0 - valLoss: 0.4301208257675171 - trainLoss: 0.4124853312969208\n",
      "cnt: 0 - valLoss: 0.430120050907135 - trainLoss: 0.41248375177383423\n",
      "cnt: 0 - valLoss: 0.43011924624443054 - trainLoss: 0.4124821722507477\n",
      "cnt: 0 - valLoss: 0.4301184415817261 - trainLoss: 0.41248056292533875\n",
      "cnt: 0 - valLoss: 0.430117666721344 - trainLoss: 0.4124789237976074\n",
      "cnt: 0 - valLoss: 0.43011683225631714 - trainLoss: 0.4124773442745209\n",
      "cnt: 0 - valLoss: 0.43011608719825745 - trainLoss: 0.4124757647514343\n",
      "cnt: 0 - valLoss: 0.43011531233787537 - trainLoss: 0.412474125623703\n",
      "cnt: 0 - valLoss: 0.4301144778728485 - trainLoss: 0.41247251629829407\n",
      "cnt: 0 - valLoss: 0.43011370301246643 - trainLoss: 0.4124709665775299\n",
      "cnt: 0 - valLoss: 0.43011289834976196 - trainLoss: 0.4124693274497986\n",
      "cnt: 0 - valLoss: 0.4301120936870575 - trainLoss: 0.41246771812438965\n",
      "cnt: 0 - valLoss: 0.4301113188266754 - trainLoss: 0.4124661386013031\n",
      "cnt: 0 - valLoss: 0.43011054396629333 - trainLoss: 0.41246452927589417\n",
      "cnt: 0 - valLoss: 0.43010976910591125 - trainLoss: 0.41246291995048523\n",
      "cnt: 0 - valLoss: 0.4301089942455292 - trainLoss: 0.4124612808227539\n",
      "cnt: 0 - valLoss: 0.4301082193851471 - trainLoss: 0.41245970129966736\n",
      "cnt: 0 - valLoss: 0.4301074147224426 - trainLoss: 0.4124581217765808\n",
      "cnt: 0 - valLoss: 0.43010661005973816 - trainLoss: 0.4124564826488495\n",
      "cnt: 0 - valLoss: 0.4301058351993561 - trainLoss: 0.41245490312576294\n",
      "cnt: 0 - valLoss: 0.4301050901412964 - trainLoss: 0.412453293800354\n",
      "cnt: 0 - valLoss: 0.4301043450832367 - trainLoss: 0.41245171427726746\n",
      "cnt: 0 - valLoss: 0.43010351061820984 - trainLoss: 0.41245007514953613\n",
      "cnt: 0 - valLoss: 0.43010273575782776 - trainLoss: 0.4124484956264496\n",
      "cnt: 0 - valLoss: 0.43010202050209045 - trainLoss: 0.41244691610336304\n",
      "cnt: 0 - valLoss: 0.430101215839386 - trainLoss: 0.4124452769756317\n",
      "cnt: 0 - valLoss: 0.4301004707813263 - trainLoss: 0.4124436676502228\n",
      "cnt: 0 - valLoss: 0.4300996959209442 - trainLoss: 0.4124421179294586\n",
      "cnt: 0 - valLoss: 0.43009892106056213 - trainLoss: 0.4124404788017273\n",
      "cnt: 0 - valLoss: 0.43009814620018005 - trainLoss: 0.41243886947631836\n",
      "cnt: 0 - valLoss: 0.43009740114212036 - trainLoss: 0.41243723034858704\n",
      "cnt: 0 - valLoss: 0.4300966262817383 - trainLoss: 0.4124356508255005\n",
      "cnt: 0 - valLoss: 0.4300958812236786 - trainLoss: 0.41243407130241394\n",
      "cnt: 0 - valLoss: 0.4300951063632965 - trainLoss: 0.4124324917793274\n",
      "cnt: 0 - valLoss: 0.43009433150291443 - trainLoss: 0.41243085265159607\n",
      "cnt: 0 - valLoss: 0.43009358644485474 - trainLoss: 0.4124292731285095\n",
      "cnt: 0 - valLoss: 0.43009281158447266 - trainLoss: 0.4124276340007782\n",
      "cnt: 0 - valLoss: 0.43009206652641296 - trainLoss: 0.41242605447769165\n",
      "cnt: 0 - valLoss: 0.4300912916660309 - trainLoss: 0.4124244451522827\n",
      "cnt: 0 - valLoss: 0.4300905764102936 - trainLoss: 0.41242286562919617\n",
      "cnt: 0 - valLoss: 0.4300898015499115 - trainLoss: 0.41242122650146484\n",
      "cnt: 0 - valLoss: 0.4300890266895294 - trainLoss: 0.4124196469783783\n",
      "cnt: 0 - valLoss: 0.4300883412361145 - trainLoss: 0.41241806745529175\n",
      "cnt: 0 - valLoss: 0.4300875663757324 - trainLoss: 0.4124164283275604\n",
      "cnt: 0 - valLoss: 0.43008679151535034 - trainLoss: 0.4124148488044739\n",
      "cnt: 0 - valLoss: 0.43008601665496826 - trainLoss: 0.41241320967674255\n",
      "cnt: 0 - valLoss: 0.43008533120155334 - trainLoss: 0.412411630153656\n",
      "cnt: 0 - valLoss: 0.43008458614349365 - trainLoss: 0.41241002082824707\n",
      "cnt: 0 - valLoss: 0.43008384108543396 - trainLoss: 0.4124084413051605\n",
      "cnt: 0 - valLoss: 0.43008312582969666 - trainLoss: 0.4124068319797516\n",
      "cnt: 0 - valLoss: 0.4300823509693146 - trainLoss: 0.41240522265434265\n",
      "cnt: 0 - valLoss: 0.4300816059112549 - trainLoss: 0.4124036431312561\n",
      "cnt: 0 - valLoss: 0.4300808906555176 - trainLoss: 0.41240206360816956\n",
      "cnt: 0 - valLoss: 0.4300801157951355 - trainLoss: 0.41240042448043823\n",
      "cnt: 0 - valLoss: 0.4300793707370758 - trainLoss: 0.4123987853527069\n",
      "cnt: 0 - valLoss: 0.4300786554813385 - trainLoss: 0.41239726543426514\n",
      "cnt: 0 - valLoss: 0.4300779402256012 - trainLoss: 0.4123956263065338\n",
      "cnt: 0 - valLoss: 0.4300771951675415 - trainLoss: 0.41239404678344727\n",
      "cnt: 0 - valLoss: 0.4300764799118042 - trainLoss: 0.4123924672603607\n",
      "cnt: 0 - valLoss: 0.4300757348537445 - trainLoss: 0.4123908281326294\n",
      "cnt: 0 - valLoss: 0.4300749599933624 - trainLoss: 0.41238921880722046\n",
      "cnt: 0 - valLoss: 0.4300742745399475 - trainLoss: 0.4123876392841339\n",
      "cnt: 0 - valLoss: 0.43007349967956543 - trainLoss: 0.41238605976104736\n",
      "cnt: 0 - valLoss: 0.4300728142261505 - trainLoss: 0.41238442063331604\n",
      "cnt: 0 - valLoss: 0.4300720691680908 - trainLoss: 0.4123828411102295\n",
      "cnt: 0 - valLoss: 0.4300713837146759 - trainLoss: 0.41238126158714294\n",
      "cnt: 0 - valLoss: 0.4300706088542938 - trainLoss: 0.4123796224594116\n",
      "cnt: 0 - valLoss: 0.4300699234008789 - trainLoss: 0.41237807273864746\n",
      "cnt: 0 - valLoss: 0.4300691783428192 - trainLoss: 0.4123764634132385\n",
      "cnt: 0 - valLoss: 0.4300684630870819 - trainLoss: 0.4123748242855072\n",
      "cnt: 0 - valLoss: 0.4300677180290222 - trainLoss: 0.41237324476242065\n",
      "cnt: 0 - valLoss: 0.4300670027732849 - trainLoss: 0.4123716354370117\n",
      "cnt: 0 - valLoss: 0.4300662577152252 - trainLoss: 0.4123700261116028\n",
      "cnt: 0 - valLoss: 0.4300655424594879 - trainLoss: 0.41236844658851624\n",
      "cnt: 0 - valLoss: 0.430064857006073 - trainLoss: 0.4123668670654297\n",
      "cnt: 0 - valLoss: 0.4300641119480133 - trainLoss: 0.41236525774002075\n",
      "cnt: 0 - valLoss: 0.430063396692276 - trainLoss: 0.41236361861228943\n",
      "cnt: 0 - valLoss: 0.4300627112388611 - trainLoss: 0.41236206889152527\n",
      "cnt: 0 - valLoss: 0.430061936378479 - trainLoss: 0.41236045956611633\n",
      "cnt: 0 - valLoss: 0.4300612509250641 - trainLoss: 0.4123588800430298\n",
      "cnt: 0 - valLoss: 0.43006056547164917 - trainLoss: 0.41235727071762085\n",
      "cnt: 0 - valLoss: 0.4300597906112671 - trainLoss: 0.4123556613922119\n",
      "cnt: 0 - valLoss: 0.4300591051578522 - trainLoss: 0.41235408186912537\n",
      "cnt: 0 - valLoss: 0.43005838990211487 - trainLoss: 0.4123525023460388\n",
      "cnt: 0 - valLoss: 0.43005770444869995 - trainLoss: 0.4123508632183075\n",
      "cnt: 0 - valLoss: 0.43005701899528503 - trainLoss: 0.41234928369522095\n",
      "cnt: 0 - valLoss: 0.43005627393722534 - trainLoss: 0.4123477041721344\n",
      "cnt: 0 - valLoss: 0.4300555884838104 - trainLoss: 0.4123460650444031\n",
      "cnt: 0 - valLoss: 0.4300548732280731 - trainLoss: 0.41234445571899414\n",
      "cnt: 0 - valLoss: 0.4300541877746582 - trainLoss: 0.41234290599823\n",
      "cnt: 0 - valLoss: 0.4300535023212433 - trainLoss: 0.41234126687049866\n",
      "cnt: 0 - valLoss: 0.4300527572631836 - trainLoss: 0.4123397469520569\n",
      "cnt: 0 - valLoss: 0.43005213141441345 - trainLoss: 0.41233810782432556\n",
      "cnt: 0 - valLoss: 0.43005138635635376 - trainLoss: 0.412336528301239\n",
      "cnt: 0 - valLoss: 0.43005070090293884 - trainLoss: 0.4123348891735077\n",
      "cnt: 0 - valLoss: 0.4300500154495239 - trainLoss: 0.41233330965042114\n",
      "cnt: 0 - valLoss: 0.4300493001937866 - trainLoss: 0.4123317301273346\n",
      "cnt: 0 - valLoss: 0.4300486445426941 - trainLoss: 0.41233015060424805\n",
      "cnt: 0 - valLoss: 0.4300479590892792 - trainLoss: 0.4123285114765167\n",
      "cnt: 0 - valLoss: 0.43004724383354187 - trainLoss: 0.4123269021511078\n",
      "cnt: 0 - valLoss: 0.43004655838012695 - trainLoss: 0.41232532262802124\n",
      "cnt: 0 - valLoss: 0.4300459027290344 - trainLoss: 0.4123237133026123\n",
      "cnt: 0 - valLoss: 0.4300451874732971 - trainLoss: 0.41232213377952576\n",
      "cnt: 0 - valLoss: 0.4300445318222046 - trainLoss: 0.4123205244541168\n",
      "cnt: 0 - valLoss: 0.4300438165664673 - trainLoss: 0.4123189449310303\n",
      "cnt: 0 - valLoss: 0.43004316091537476 - trainLoss: 0.41231733560562134\n",
      "cnt: 0 - valLoss: 0.43004247546195984 - trainLoss: 0.4123157560825348\n",
      "cnt: 0 - valLoss: 0.4300417900085449 - trainLoss: 0.41231414675712585\n",
      "cnt: 0 - valLoss: 0.43004110455513 - trainLoss: 0.4123125672340393\n",
      "cnt: 0 - valLoss: 0.4300404191017151 - trainLoss: 0.41231095790863037\n",
      "cnt: 0 - valLoss: 0.43003973364830017 - trainLoss: 0.41230934858322144\n",
      "cnt: 0 - valLoss: 0.43003910779953003 - trainLoss: 0.4123077690601349\n",
      "cnt: 0 - valLoss: 0.4300384223461151 - trainLoss: 0.41230618953704834\n",
      "cnt: 0 - valLoss: 0.4300377666950226 - trainLoss: 0.412304550409317\n",
      "cnt: 0 - valLoss: 0.43003708124160767 - trainLoss: 0.41230297088623047\n",
      "cnt: 0 - valLoss: 0.43003639578819275 - trainLoss: 0.4123013913631439\n",
      "cnt: 0 - valLoss: 0.43003571033477783 - trainLoss: 0.4122997522354126\n",
      "cnt: 0 - valLoss: 0.4300350248813629 - trainLoss: 0.41229820251464844\n",
      "cnt: 0 - valLoss: 0.43003442883491516 - trainLoss: 0.4122965931892395\n",
      "cnt: 0 - valLoss: 0.43003368377685547 - trainLoss: 0.41229504346847534\n",
      "cnt: 0 - valLoss: 0.43003299832344055 - trainLoss: 0.4122934341430664\n",
      "cnt: 0 - valLoss: 0.4300323724746704 - trainLoss: 0.4122917950153351\n",
      "cnt: 0 - valLoss: 0.4300316870212555 - trainLoss: 0.41229021549224854\n",
      "cnt: 0 - valLoss: 0.43003109097480774 - trainLoss: 0.412288635969162\n",
      "cnt: 0 - valLoss: 0.43003031611442566 - trainLoss: 0.41228705644607544\n",
      "cnt: 0 - valLoss: 0.4300297200679779 - trainLoss: 0.4122854173183441\n",
      "cnt: 0 - valLoss: 0.430029034614563 - trainLoss: 0.41228383779525757\n",
      "cnt: 0 - valLoss: 0.43002834916114807 - trainLoss: 0.41228219866752625\n",
      "cnt: 0 - valLoss: 0.43002766370773315 - trainLoss: 0.4122806191444397\n",
      "cnt: 0 - valLoss: 0.430027037858963 - trainLoss: 0.41227903962135315\n",
      "cnt: 0 - valLoss: 0.4300263524055481 - trainLoss: 0.4122774600982666\n",
      "cnt: 0 - valLoss: 0.4300256669521332 - trainLoss: 0.4122758209705353\n",
      "cnt: 0 - valLoss: 0.43002498149871826 - trainLoss: 0.41227424144744873\n",
      "cnt: 0 - valLoss: 0.4300243556499481 - trainLoss: 0.4122726619243622\n",
      "cnt: 0 - valLoss: 0.4300236999988556 - trainLoss: 0.41227108240127563\n",
      "cnt: 0 - valLoss: 0.4300230145454407 - trainLoss: 0.4122694432735443\n",
      "cnt: 0 - valLoss: 0.43002232909202576 - trainLoss: 0.41226786375045776\n",
      "cnt: 0 - valLoss: 0.4300217032432556 - trainLoss: 0.4122662842273712\n",
      "cnt: 0 - valLoss: 0.4300210177898407 - trainLoss: 0.41226470470428467\n",
      "cnt: 0 - valLoss: 0.4300203323364258 - trainLoss: 0.4122631251811981\n",
      "cnt: 0 - valLoss: 0.43001964688301086 - trainLoss: 0.4122615456581116\n",
      "cnt: 0 - valLoss: 0.43001899123191833 - trainLoss: 0.41225993633270264\n",
      "cnt: 0 - valLoss: 0.4300183057785034 - trainLoss: 0.4122583270072937\n",
      "cnt: 0 - valLoss: 0.4300176203250885 - trainLoss: 0.41225677728652954\n",
      "cnt: 0 - valLoss: 0.4300169348716736 - trainLoss: 0.4122551679611206\n",
      "cnt: 0 - valLoss: 0.43001624941825867 - trainLoss: 0.41225358843803406\n",
      "cnt: 0 - valLoss: 0.43001559376716614 - trainLoss: 0.4122519791126251\n",
      "cnt: 0 - valLoss: 0.430014967918396 - trainLoss: 0.4122503995895386\n",
      "cnt: 0 - valLoss: 0.4300142228603363 - trainLoss: 0.4122488796710968\n",
      "cnt: 0 - valLoss: 0.43001359701156616 - trainLoss: 0.4122472405433655\n",
      "cnt: 0 - valLoss: 0.43001291155815125 - trainLoss: 0.41224566102027893\n",
      "cnt: 0 - valLoss: 0.43001222610473633 - trainLoss: 0.4122440814971924\n",
      "cnt: 0 - valLoss: 0.4300115704536438 - trainLoss: 0.4122425317764282\n",
      "cnt: 0 - valLoss: 0.43001094460487366 - trainLoss: 0.4122408628463745\n",
      "cnt: 0 - valLoss: 0.43001025915145874 - trainLoss: 0.41223931312561035\n",
      "cnt: 0 - valLoss: 0.4300095736980438 - trainLoss: 0.4122377336025238\n",
      "cnt: 0 - valLoss: 0.4300089180469513 - trainLoss: 0.41223615407943726\n",
      "cnt: 0 - valLoss: 0.43000829219818115 - trainLoss: 0.4122345745563507\n",
      "cnt: 0 - valLoss: 0.43000757694244385 - trainLoss: 0.41223299503326416\n",
      "cnt: 0 - valLoss: 0.4300069808959961 - trainLoss: 0.4122314155101776\n",
      "cnt: 0 - valLoss: 0.4300062656402588 - trainLoss: 0.4122298061847687\n",
      "cnt: 0 - valLoss: 0.43000560998916626 - trainLoss: 0.41222822666168213\n",
      "cnt: 0 - valLoss: 0.4300049841403961 - trainLoss: 0.4122266471385956\n",
      "cnt: 0 - valLoss: 0.4300043284893036 - trainLoss: 0.41222506761550903\n",
      "cnt: 0 - valLoss: 0.43000364303588867 - trainLoss: 0.4122234880924225\n",
      "cnt: 0 - valLoss: 0.43000301718711853 - trainLoss: 0.41222184896469116\n",
      "cnt: 0 - valLoss: 0.430002361536026 - trainLoss: 0.4122203290462494\n",
      "cnt: 0 - valLoss: 0.4300016760826111 - trainLoss: 0.41221868991851807\n",
      "cnt: 0 - valLoss: 0.43000102043151855 - trainLoss: 0.4122171401977539\n",
      "cnt: 0 - valLoss: 0.4300003945827484 - trainLoss: 0.41221553087234497\n",
      "cnt: 0 - valLoss: 0.4299997389316559 - trainLoss: 0.4122139811515808\n",
      "cnt: 0 - valLoss: 0.42999911308288574 - trainLoss: 0.41221240162849426\n",
      "cnt: 0 - valLoss: 0.4299984276294708 - trainLoss: 0.4122108221054077\n",
      "cnt: 0 - valLoss: 0.4299977719783783 - trainLoss: 0.4122091829776764\n",
      "cnt: 0 - valLoss: 0.42999714612960815 - trainLoss: 0.41220763325691223\n",
      "cnt: 0 - valLoss: 0.42999646067619324 - trainLoss: 0.41220611333847046\n",
      "cnt: 0 - valLoss: 0.4299958348274231 - trainLoss: 0.41220444440841675\n",
      "cnt: 0 - valLoss: 0.42999520897865295 - trainLoss: 0.4122028946876526\n",
      "cnt: 0 - valLoss: 0.42999452352523804 - trainLoss: 0.41220131516456604\n",
      "cnt: 0 - valLoss: 0.4299938678741455 - trainLoss: 0.4121996760368347\n",
      "cnt: 0 - valLoss: 0.42999324202537537 - trainLoss: 0.41219815611839294\n",
      "cnt: 0 - valLoss: 0.42999258637428284 - trainLoss: 0.4121965765953064\n",
      "cnt: 0 - valLoss: 0.4299919009208679 - trainLoss: 0.4121949374675751\n",
      "cnt: 0 - valLoss: 0.42999130487442017 - trainLoss: 0.4121933579444885\n",
      "cnt: 0 - valLoss: 0.42999064922332764 - trainLoss: 0.41219180822372437\n",
      "cnt: 0 - valLoss: 0.4299900233745575 - trainLoss: 0.41219019889831543\n",
      "cnt: 0 - valLoss: 0.42998942732810974 - trainLoss: 0.4121885895729065\n",
      "cnt: 0 - valLoss: 0.4299887418746948 - trainLoss: 0.4121870696544647\n",
      "cnt: 0 - valLoss: 0.4299880862236023 - trainLoss: 0.4121854901313782\n",
      "cnt: 0 - valLoss: 0.42998746037483215 - trainLoss: 0.41218385100364685\n",
      "cnt: 0 - valLoss: 0.4299868047237396 - trainLoss: 0.4121822714805603\n",
      "cnt: 0 - valLoss: 0.42998620867729187 - trainLoss: 0.41218069195747375\n",
      "cnt: 0 - valLoss: 0.42998552322387695 - trainLoss: 0.4121791422367096\n",
      "cnt: 0 - valLoss: 0.4299848675727844 - trainLoss: 0.41217750310897827\n",
      "cnt: 0 - valLoss: 0.4299842417240143 - trainLoss: 0.4121759831905365\n",
      "cnt: 0 - valLoss: 0.42998358607292175 - trainLoss: 0.41217440366744995\n",
      "cnt: 0 - valLoss: 0.429982990026474 - trainLoss: 0.412172794342041\n",
      "cnt: 0 - valLoss: 0.42998233437538147 - trainLoss: 0.41217121481895447\n",
      "cnt: 0 - valLoss: 0.42998170852661133 - trainLoss: 0.4121696352958679\n",
      "cnt: 0 - valLoss: 0.4299810230731964 - trainLoss: 0.412168025970459\n",
      "cnt: 0 - valLoss: 0.42998045682907104 - trainLoss: 0.4121664762496948\n",
      "cnt: 0 - valLoss: 0.4299798309803009 - trainLoss: 0.4121648967266083\n",
      "cnt: 0 - valLoss: 0.4299791753292084 - trainLoss: 0.41216331720352173\n",
      "cnt: 0 - valLoss: 0.42997851967811584 - trainLoss: 0.4121617078781128\n",
      "cnt: 0 - valLoss: 0.4299778938293457 - trainLoss: 0.41216012835502625\n",
      "cnt: 0 - valLoss: 0.42997729778289795 - trainLoss: 0.4121585488319397\n",
      "cnt: 0 - valLoss: 0.4299767017364502 - trainLoss: 0.41215696930885315\n",
      "cnt: 0 - valLoss: 0.42997604608535767 - trainLoss: 0.4121553897857666\n",
      "cnt: 0 - valLoss: 0.4299754500389099 - trainLoss: 0.41215381026268005\n",
      "cnt: 0 - valLoss: 0.429974764585495 - trainLoss: 0.4121522307395935\n",
      "cnt: 0 - valLoss: 0.42997416853904724 - trainLoss: 0.41215062141418457\n",
      "cnt: 0 - valLoss: 0.4299735724925995 - trainLoss: 0.412149041891098\n",
      "cnt: 0 - valLoss: 0.42997291684150696 - trainLoss: 0.4121474623680115\n",
      "cnt: 0 - valLoss: 0.4299723207950592 - trainLoss: 0.4121458828449249\n",
      "cnt: 0 - valLoss: 0.4299716651439667 - trainLoss: 0.41214433312416077\n",
      "cnt: 0 - valLoss: 0.4299710690975189 - trainLoss: 0.41214272379875183\n",
      "cnt: 0 - valLoss: 0.4299704432487488 - trainLoss: 0.4121411442756653\n",
      "cnt: 0 - valLoss: 0.42996981739997864 - trainLoss: 0.41213956475257874\n",
      "cnt: 0 - valLoss: 0.4299691915512085 - trainLoss: 0.4121380150318146\n",
      "cnt: 0 - valLoss: 0.42996862530708313 - trainLoss: 0.412136435508728\n",
      "cnt: 0 - valLoss: 0.429967999458313 - trainLoss: 0.4121348559856415\n",
      "cnt: 0 - valLoss: 0.42996737360954285 - trainLoss: 0.41213324666023254\n",
      "cnt: 0 - valLoss: 0.4299667775630951 - trainLoss: 0.4121316373348236\n",
      "cnt: 0 - valLoss: 0.42996618151664734 - trainLoss: 0.41213008761405945\n",
      "cnt: 0 - valLoss: 0.4299654960632324 - trainLoss: 0.4121285080909729\n",
      "cnt: 0 - valLoss: 0.42996490001678467 - trainLoss: 0.4121268689632416\n",
      "cnt: 0 - valLoss: 0.4299643039703369 - trainLoss: 0.41212528944015503\n",
      "cnt: 0 - valLoss: 0.42996370792388916 - trainLoss: 0.41212376952171326\n",
      "cnt: 0 - valLoss: 0.4299631118774414 - trainLoss: 0.4121221601963043\n",
      "cnt: 0 - valLoss: 0.42996251583099365 - trainLoss: 0.4121205806732178\n",
      "cnt: 0 - valLoss: 0.4299618899822235 - trainLoss: 0.4121190011501312\n",
      "cnt: 0 - valLoss: 0.42996126413345337 - trainLoss: 0.4121174216270447\n",
      "cnt: 0 - valLoss: 0.42996060848236084 - trainLoss: 0.41211584210395813\n",
      "cnt: 0 - valLoss: 0.4299600124359131 - trainLoss: 0.4121142625808716\n",
      "cnt: 0 - valLoss: 0.42995941638946533 - trainLoss: 0.41211268305778503\n",
      "cnt: 0 - valLoss: 0.4299588203430176 - trainLoss: 0.4121110737323761\n",
      "cnt: 0 - valLoss: 0.4299582242965698 - trainLoss: 0.4121095538139343\n",
      "cnt: 0 - valLoss: 0.42995765805244446 - trainLoss: 0.412107914686203\n",
      "cnt: 0 - valLoss: 0.42995700240135193 - trainLoss: 0.41210636496543884\n",
      "cnt: 0 - valLoss: 0.4299564063549042 - trainLoss: 0.4121047854423523\n",
      "cnt: 0 - valLoss: 0.4299558103084564 - trainLoss: 0.41210320591926575\n",
      "cnt: 0 - valLoss: 0.42995521426200867 - trainLoss: 0.4121015965938568\n",
      "cnt: 0 - valLoss: 0.4299546182155609 - trainLoss: 0.41210004687309265\n",
      "cnt: 0 - valLoss: 0.42995402216911316 - trainLoss: 0.4120984971523285\n",
      "cnt: 0 - valLoss: 0.4299534261226654 - trainLoss: 0.41209688782691956\n",
      "cnt: 0 - valLoss: 0.42995280027389526 - trainLoss: 0.4120952785015106\n",
      "cnt: 0 - valLoss: 0.4299522042274475 - trainLoss: 0.4120936989784241\n",
      "cnt: 0 - valLoss: 0.42995160818099976 - trainLoss: 0.4120921194553375\n",
      "cnt: 0 - valLoss: 0.429951012134552 - trainLoss: 0.41209059953689575\n",
      "cnt: 0 - valLoss: 0.42995041608810425 - trainLoss: 0.4120889902114868\n",
      "cnt: 0 - valLoss: 0.4299498200416565 - trainLoss: 0.4120873808860779\n",
      "cnt: 0 - valLoss: 0.42994922399520874 - trainLoss: 0.4120858311653137\n",
      "cnt: 0 - valLoss: 0.429948627948761 - trainLoss: 0.4120842218399048\n",
      "cnt: 0 - valLoss: 0.42994800209999084 - trainLoss: 0.4120826721191406\n",
      "cnt: 0 - valLoss: 0.4299474060535431 - trainLoss: 0.4120810925960541\n",
      "cnt: 0 - valLoss: 0.42994681000709534 - trainLoss: 0.41207951307296753\n",
      "cnt: 0 - valLoss: 0.4299462139606476 - trainLoss: 0.4120779037475586\n",
      "cnt: 0 - valLoss: 0.4299456477165222 - trainLoss: 0.41207632422447205\n",
      "cnt: 0 - valLoss: 0.42994505167007446 - trainLoss: 0.4120748043060303\n",
      "cnt: 0 - valLoss: 0.4299444258213043 - trainLoss: 0.41207316517829895\n",
      "cnt: 0 - valLoss: 0.42994385957717896 - trainLoss: 0.4120715856552124\n",
      "cnt: 0 - valLoss: 0.4299432635307312 - trainLoss: 0.41207000613212585\n",
      "cnt: 0 - valLoss: 0.42994269728660583 - trainLoss: 0.4120684564113617\n",
      "cnt: 0 - valLoss: 0.4299421012401581 - trainLoss: 0.41206681728363037\n",
      "cnt: 0 - valLoss: 0.42994144558906555 - trainLoss: 0.4120652973651886\n",
      "cnt: 0 - valLoss: 0.4299409091472626 - trainLoss: 0.41206374764442444\n",
      "cnt: 0 - valLoss: 0.4299403131008148 - trainLoss: 0.4120621085166931\n",
      "cnt: 0 - valLoss: 0.4299396872520447 - trainLoss: 0.41206052899360657\n",
      "cnt: 0 - valLoss: 0.4299390912055969 - trainLoss: 0.41205894947052\n",
      "cnt: 0 - valLoss: 0.42993849515914917 - trainLoss: 0.41205736994743347\n",
      "cnt: 0 - valLoss: 0.4299378991127014 - trainLoss: 0.4120558202266693\n",
      "cnt: 0 - valLoss: 0.42993730306625366 - trainLoss: 0.41205424070358276\n",
      "cnt: 0 - valLoss: 0.4299367368221283 - trainLoss: 0.4120526611804962\n",
      "cnt: 0 - valLoss: 0.42993617057800293 - trainLoss: 0.41205108165740967\n",
      "cnt: 0 - valLoss: 0.429935485124588 - trainLoss: 0.4120495021343231\n",
      "cnt: 0 - valLoss: 0.42993485927581787 - trainLoss: 0.4120479226112366\n",
      "cnt: 0 - valLoss: 0.42993420362472534 - trainLoss: 0.4120463728904724\n",
      "cnt: 0 - valLoss: 0.4299335181713104 - trainLoss: 0.4120447337627411\n",
      "cnt: 0 - valLoss: 0.42993292212486267 - trainLoss: 0.41204312443733215\n",
      "cnt: 0 - valLoss: 0.42993223667144775 - trainLoss: 0.412041574716568\n",
      "cnt: 0 - valLoss: 0.429931640625 - trainLoss: 0.41203999519348145\n",
      "cnt: 0 - valLoss: 0.4299309551715851 - trainLoss: 0.4120384156703949\n",
      "cnt: 0 - valLoss: 0.42993035912513733 - trainLoss: 0.4120367765426636\n",
      "cnt: 0 - valLoss: 0.4299297034740448 - trainLoss: 0.4120352566242218\n",
      "cnt: 0 - valLoss: 0.42992907762527466 - trainLoss: 0.4120336174964905\n",
      "cnt: 0 - valLoss: 0.4299284815788269 - trainLoss: 0.41203203797340393\n",
      "cnt: 0 - valLoss: 0.4299278259277344 - trainLoss: 0.41203048825263977\n",
      "cnt: 0 - valLoss: 0.42992717027664185 - trainLoss: 0.4120289087295532\n",
      "cnt: 0 - valLoss: 0.4299265742301941 - trainLoss: 0.4120273292064667\n",
      "cnt: 0 - valLoss: 0.4299258887767792 - trainLoss: 0.4120257496833801\n",
      "cnt: 0 - valLoss: 0.4299252927303314 - trainLoss: 0.4120241105556488\n",
      "cnt: 0 - valLoss: 0.42992469668388367 - trainLoss: 0.41202253103256226\n",
      "cnt: 0 - valLoss: 0.42992401123046875 - trainLoss: 0.4120209515094757\n",
      "cnt: 0 - valLoss: 0.429923415184021 - trainLoss: 0.41201940178871155\n",
      "cnt: 0 - valLoss: 0.4299227297306061 - trainLoss: 0.4120177924633026\n",
      "cnt: 0 - valLoss: 0.4299221336841583 - trainLoss: 0.41201624274253845\n",
      "cnt: 0 - valLoss: 0.42992156744003296 - trainLoss: 0.4120146632194519\n",
      "cnt: 0 - valLoss: 0.42992088198661804 - trainLoss: 0.41201308369636536\n",
      "cnt: 0 - valLoss: 0.4299202859401703 - trainLoss: 0.41201144456863403\n",
      "cnt: 0 - valLoss: 0.42991963028907776 - trainLoss: 0.4120098948478699\n",
      "cnt: 0 - valLoss: 0.42991903424263 - trainLoss: 0.41200828552246094\n",
      "cnt: 0 - valLoss: 0.42991840839385986 - trainLoss: 0.4120067358016968\n",
      "cnt: 0 - valLoss: 0.42991775274276733 - trainLoss: 0.41200509667396545\n",
      "cnt: 0 - valLoss: 0.4299171566963196 - trainLoss: 0.4120035171508789\n",
      "cnt: 0 - valLoss: 0.42991650104522705 - trainLoss: 0.41200199723243713\n",
      "cnt: 0 - valLoss: 0.4299159049987793 - trainLoss: 0.4120003879070282\n",
      "cnt: 0 - valLoss: 0.42991527915000916 - trainLoss: 0.41199877858161926\n",
      "cnt: 0 - valLoss: 0.429914653301239 - trainLoss: 0.4119972288608551\n",
      "cnt: 0 - valLoss: 0.42991402745246887 - trainLoss: 0.41199564933776855\n",
      "cnt: 0 - valLoss: 0.42991337180137634 - trainLoss: 0.411994069814682\n",
      "cnt: 0 - valLoss: 0.4299127757549286 - trainLoss: 0.41199249029159546\n",
      "cnt: 0 - valLoss: 0.42991217970848083 - trainLoss: 0.4119909107685089\n",
      "cnt: 0 - valLoss: 0.4299115836620331 - trainLoss: 0.4119892716407776\n",
      "cnt: 0 - valLoss: 0.42991092801094055 - trainLoss: 0.4119877219200134\n",
      "cnt: 0 - valLoss: 0.4299103021621704 - trainLoss: 0.4119861125946045\n",
      "cnt: 0 - valLoss: 0.42990970611572266 - trainLoss: 0.41198456287384033\n",
      "cnt: 0 - valLoss: 0.4299091100692749 - trainLoss: 0.4119829833507538\n",
      "cnt: 0 - valLoss: 0.42990848422050476 - trainLoss: 0.41198140382766724\n",
      "cnt: 0 - valLoss: 0.429907888174057 - trainLoss: 0.4119798243045807\n",
      "cnt: 0 - valLoss: 0.42990726232528687 - trainLoss: 0.41197824478149414\n",
      "cnt: 0 - valLoss: 0.4299066662788391 - trainLoss: 0.4119766354560852\n",
      "cnt: 0 - valLoss: 0.4299060106277466 - trainLoss: 0.41197505593299866\n",
      "cnt: 0 - valLoss: 0.42990541458129883 - trainLoss: 0.4119734764099121\n",
      "cnt: 0 - valLoss: 0.4299048185348511 - trainLoss: 0.41197189688682556\n",
      "cnt: 0 - valLoss: 0.42990416288375854 - trainLoss: 0.411970317363739\n",
      "cnt: 0 - valLoss: 0.4299035668373108 - trainLoss: 0.41196873784065247\n",
      "cnt: 0 - valLoss: 0.42990297079086304 - trainLoss: 0.4119671583175659\n",
      "cnt: 0 - valLoss: 0.4299023747444153 - trainLoss: 0.411965548992157\n",
      "cnt: 0 - valLoss: 0.42990171909332275 - trainLoss: 0.41196396946907043\n",
      "cnt: 0 - valLoss: 0.429901123046875 - trainLoss: 0.4119623899459839\n",
      "cnt: 0 - valLoss: 0.42990052700042725 - trainLoss: 0.4119608402252197\n",
      "cnt: 0 - valLoss: 0.4298999309539795 - trainLoss: 0.4119592308998108\n",
      "cnt: 0 - valLoss: 0.42989933490753174 - trainLoss: 0.41195765137672424\n",
      "cnt: 0 - valLoss: 0.4298987090587616 - trainLoss: 0.4119560718536377\n",
      "cnt: 0 - valLoss: 0.42989811301231384 - trainLoss: 0.41195452213287354\n",
      "cnt: 0 - valLoss: 0.4298974871635437 - trainLoss: 0.4119528830051422\n",
      "cnt: 0 - valLoss: 0.42989692091941833 - trainLoss: 0.41195136308670044\n",
      "cnt: 0 - valLoss: 0.4298962652683258 - trainLoss: 0.4119497537612915\n",
      "cnt: 0 - valLoss: 0.42989563941955566 - trainLoss: 0.41194817423820496\n",
      "cnt: 0 - valLoss: 0.4298950731754303 - trainLoss: 0.411946564912796\n",
      "cnt: 0 - valLoss: 0.42989447712898254 - trainLoss: 0.41194501519203186\n",
      "cnt: 0 - valLoss: 0.4298938512802124 - trainLoss: 0.4119434356689453\n",
      "cnt: 0 - valLoss: 0.42989322543144226 - trainLoss: 0.41194185614585876\n",
      "cnt: 0 - valLoss: 0.4298926293849945 - trainLoss: 0.41194021701812744\n",
      "cnt: 0 - valLoss: 0.42989203333854675 - trainLoss: 0.4119386672973633\n",
      "cnt: 0 - valLoss: 0.429891437292099 - trainLoss: 0.41193708777427673\n",
      "cnt: 0 - valLoss: 0.42989084124565125 - trainLoss: 0.4119355082511902\n",
      "cnt: 0 - valLoss: 0.4298902750015259 - trainLoss: 0.41193392872810364\n",
      "cnt: 0 - valLoss: 0.4298896789550781 - trainLoss: 0.4119323492050171\n",
      "cnt: 0 - valLoss: 0.4298890233039856 - trainLoss: 0.41193076968193054\n",
      "cnt: 0 - valLoss: 0.42988842725753784 - trainLoss: 0.411929190158844\n",
      "cnt: 0 - valLoss: 0.4298878312110901 - trainLoss: 0.41192761063575745\n",
      "cnt: 0 - valLoss: 0.42988723516464233 - trainLoss: 0.4119260013103485\n",
      "cnt: 0 - valLoss: 0.4298866391181946 - trainLoss: 0.41192442178726196\n",
      "cnt: 0 - valLoss: 0.4298860430717468 - trainLoss: 0.4119228422641754\n",
      "cnt: 0 - valLoss: 0.42988547682762146 - trainLoss: 0.41192129254341125\n",
      "cnt: 0 - valLoss: 0.4298848807811737 - trainLoss: 0.4119196832180023\n",
      "cnt: 0 - valLoss: 0.42988428473472595 - trainLoss: 0.41191813349723816\n",
      "cnt: 0 - valLoss: 0.4298837184906006 - trainLoss: 0.4119165539741516\n",
      "cnt: 0 - valLoss: 0.42988312244415283 - trainLoss: 0.41191497445106506\n",
      "cnt: 0 - valLoss: 0.4298825263977051 - trainLoss: 0.4119133949279785\n",
      "cnt: 0 - valLoss: 0.4298819601535797 - trainLoss: 0.41191181540489197\n",
      "cnt: 0 - valLoss: 0.4298812747001648 - trainLoss: 0.41191020607948303\n",
      "cnt: 0 - valLoss: 0.42988064885139465 - trainLoss: 0.41190868616104126\n",
      "cnt: 0 - valLoss: 0.42987996339797974 - trainLoss: 0.4119071066379547\n",
      "cnt: 0 - valLoss: 0.4298793077468872 - trainLoss: 0.4119054675102234\n",
      "cnt: 0 - valLoss: 0.4298786520957947 - trainLoss: 0.41190391778945923\n",
      "cnt: 0 - valLoss: 0.42987796664237976 - trainLoss: 0.41190239787101746\n",
      "cnt: 0 - valLoss: 0.429877370595932 - trainLoss: 0.41190075874328613\n",
      "cnt: 0 - valLoss: 0.4298766851425171 - trainLoss: 0.411899209022522\n",
      "cnt: 0 - valLoss: 0.42987605929374695 - trainLoss: 0.4118976294994354\n",
      "cnt: 0 - valLoss: 0.4298754036426544 - trainLoss: 0.4118960499763489\n",
      "cnt: 0 - valLoss: 0.4298747181892395 - trainLoss: 0.4118945002555847\n",
      "cnt: 0 - valLoss: 0.42987412214279175 - trainLoss: 0.4118928909301758\n",
      "cnt: 0 - valLoss: 0.42987343668937683 - trainLoss: 0.4118913412094116\n",
      "cnt: 0 - valLoss: 0.4298728406429291 - trainLoss: 0.4118897020816803\n",
      "cnt: 0 - valLoss: 0.42987215518951416 - trainLoss: 0.4118881821632385\n",
      "cnt: 0 - valLoss: 0.4298715591430664 - trainLoss: 0.411886602640152\n",
      "cnt: 0 - valLoss: 0.4298708736896515 - trainLoss: 0.41188502311706543\n",
      "cnt: 0 - valLoss: 0.42987021803855896 - trainLoss: 0.41188347339630127\n",
      "cnt: 0 - valLoss: 0.42986956238746643 - trainLoss: 0.41188183426856995\n",
      "cnt: 0 - valLoss: 0.4298689365386963 - trainLoss: 0.4118803143501282\n",
      "cnt: 0 - valLoss: 0.42986828088760376 - trainLoss: 0.41187867522239685\n",
      "cnt: 0 - valLoss: 0.429867684841156 - trainLoss: 0.4118771255016327\n",
      "cnt: 0 - valLoss: 0.42986705899238586 - trainLoss: 0.4118756055831909\n",
      "cnt: 0 - valLoss: 0.42986640334129333 - trainLoss: 0.4118739664554596\n",
      "cnt: 0 - valLoss: 0.4298657178878784 - trainLoss: 0.41187241673469543\n",
      "cnt: 0 - valLoss: 0.42986512184143066 - trainLoss: 0.41187089681625366\n",
      "cnt: 0 - valLoss: 0.42986446619033813 - trainLoss: 0.41186925768852234\n",
      "cnt: 0 - valLoss: 0.429863840341568 - trainLoss: 0.4118677079677582\n",
      "cnt: 0 - valLoss: 0.42986318469047546 - trainLoss: 0.41186612844467163\n",
      "cnt: 0 - valLoss: 0.4298625886440277 - trainLoss: 0.4118645489215851\n",
      "cnt: 0 - valLoss: 0.4298619329929352 - trainLoss: 0.41186296939849854\n",
      "cnt: 0 - valLoss: 0.4298613369464874 - trainLoss: 0.411861389875412\n",
      "cnt: 0 - valLoss: 0.4298606514930725 - trainLoss: 0.4118598401546478\n",
      "cnt: 0 - valLoss: 0.42986005544662476 - trainLoss: 0.4118582606315613\n",
      "cnt: 0 - valLoss: 0.429859459400177 - trainLoss: 0.41185668110847473\n",
      "cnt: 0 - valLoss: 0.4298588037490845 - trainLoss: 0.4118551015853882\n",
      "cnt: 0 - valLoss: 0.42985817790031433 - trainLoss: 0.41185349225997925\n",
      "cnt: 0 - valLoss: 0.4298575818538666 - trainLoss: 0.4118519723415375\n",
      "cnt: 0 - valLoss: 0.42985692620277405 - trainLoss: 0.4118503928184509\n",
      "cnt: 0 - valLoss: 0.4298563003540039 - trainLoss: 0.41184884309768677\n",
      "cnt: 0 - valLoss: 0.42985570430755615 - trainLoss: 0.4118472635746002\n",
      "cnt: 0 - valLoss: 0.429855078458786 - trainLoss: 0.41184568405151367\n",
      "cnt: 0 - valLoss: 0.4298543930053711 - trainLoss: 0.4118441343307495\n",
      "cnt: 0 - valLoss: 0.42985379695892334 - trainLoss: 0.4118424952030182\n",
      "cnt: 0 - valLoss: 0.4298532009124756 - trainLoss: 0.41184091567993164\n",
      "cnt: 0 - valLoss: 0.42985257506370544 - trainLoss: 0.41183939576148987\n",
      "cnt: 0 - valLoss: 0.4298519492149353 - trainLoss: 0.41183778643608093\n",
      "cnt: 0 - valLoss: 0.42985132336616516 - trainLoss: 0.4118362069129944\n",
      "cnt: 0 - valLoss: 0.4298507273197174 - trainLoss: 0.41183462738990784\n",
      "cnt: 0 - valLoss: 0.42985013127326965 - trainLoss: 0.4118330776691437\n",
      "cnt: 0 - valLoss: 0.4298494756221771 - trainLoss: 0.41183149814605713\n",
      "cnt: 0 - valLoss: 0.42984887957572937 - trainLoss: 0.41182997822761536\n",
      "cnt: 0 - valLoss: 0.42984822392463684 - trainLoss: 0.41182833909988403\n",
      "cnt: 0 - valLoss: 0.4298476278781891 - trainLoss: 0.4118267893791199\n",
      "cnt: 0 - valLoss: 0.42984703183174133 - trainLoss: 0.4118252694606781\n",
      "cnt: 0 - valLoss: 0.4298464357852936 - trainLoss: 0.4118236303329468\n",
      "cnt: 0 - valLoss: 0.42984578013420105 - trainLoss: 0.4118220806121826\n",
      "cnt: 0 - valLoss: 0.4298451840877533 - trainLoss: 0.4118204712867737\n",
      "cnt: 0 - valLoss: 0.42984455823898315 - trainLoss: 0.4118189215660095\n",
      "cnt: 0 - valLoss: 0.429843932390213 - trainLoss: 0.411817342042923\n",
      "cnt: 0 - valLoss: 0.42984333634376526 - trainLoss: 0.4118157625198364\n",
      "cnt: 0 - valLoss: 0.4298427402973175 - trainLoss: 0.41181421279907227\n",
      "cnt: 0 - valLoss: 0.42984214425086975 - trainLoss: 0.4118126332759857\n",
      "cnt: 0 - valLoss: 0.429841548204422 - trainLoss: 0.41181105375289917\n",
      "cnt: 0 - valLoss: 0.42984095215797424 - trainLoss: 0.411809504032135\n",
      "cnt: 0 - valLoss: 0.4298403561115265 - trainLoss: 0.41180792450904846\n",
      "cnt: 0 - valLoss: 0.42983970046043396 - trainLoss: 0.4118063449859619\n",
      "cnt: 0 - valLoss: 0.4298391342163086 - trainLoss: 0.41180476546287537\n",
      "cnt: 0 - valLoss: 0.42983853816986084 - trainLoss: 0.4118031859397888\n",
      "cnt: 0 - valLoss: 0.4298379421234131 - trainLoss: 0.41180163621902466\n",
      "cnt: 0 - valLoss: 0.42983731627464294 - trainLoss: 0.4118000864982605\n",
      "cnt: 0 - valLoss: 0.4298367500305176 - trainLoss: 0.41179847717285156\n",
      "cnt: 0 - valLoss: 0.4298361539840698 - trainLoss: 0.4117969274520874\n",
      "cnt: 0 - valLoss: 0.42983555793762207 - trainLoss: 0.41179537773132324\n",
      "cnt: 0 - valLoss: 0.42983493208885193 - trainLoss: 0.4117937982082367\n",
      "cnt: 0 - valLoss: 0.4298343360424042 - trainLoss: 0.41179221868515015\n",
      "cnt: 0 - valLoss: 0.4298337399959564 - trainLoss: 0.4117906391620636\n",
      "cnt: 0 - valLoss: 0.42983314394950867 - trainLoss: 0.41178905963897705\n",
      "cnt: 0 - valLoss: 0.4298325479030609 - trainLoss: 0.4117875099182129\n",
      "cnt: 0 - valLoss: 0.42983195185661316 - trainLoss: 0.41178593039512634\n",
      "cnt: 0 - valLoss: 0.4298313856124878 - trainLoss: 0.4117843508720398\n",
      "cnt: 0 - valLoss: 0.42983078956604004 - trainLoss: 0.41178280115127563\n",
      "cnt: 0 - valLoss: 0.4298301935195923 - trainLoss: 0.4117812514305115\n",
      "cnt: 0 - valLoss: 0.42982959747314453 - trainLoss: 0.41177964210510254\n",
      "cnt: 0 - valLoss: 0.42982903122901917 - trainLoss: 0.4117780923843384\n",
      "cnt: 0 - valLoss: 0.4298284351825714 - trainLoss: 0.4117765426635742\n",
      "cnt: 0 - valLoss: 0.42982783913612366 - trainLoss: 0.41177496314048767\n",
      "cnt: 0 - valLoss: 0.4298272430896759 - trainLoss: 0.4117733836174011\n",
      "cnt: 0 - valLoss: 0.42982667684555054 - trainLoss: 0.4117718040943146\n",
      "cnt: 0 - valLoss: 0.4298260807991028 - trainLoss: 0.4117702543735504\n",
      "cnt: 0 - valLoss: 0.42982548475265503 - trainLoss: 0.41176867485046387\n",
      "cnt: 0 - valLoss: 0.42982491850852966 - trainLoss: 0.4117671251296997\n",
      "cnt: 0 - valLoss: 0.4298243224620819 - trainLoss: 0.41176551580429077\n",
      "cnt: 0 - valLoss: 0.42982372641563416 - trainLoss: 0.4117639660835266\n",
      "cnt: 0 - valLoss: 0.429823100566864 - trainLoss: 0.41176241636276245\n",
      "cnt: 0 - valLoss: 0.4298225939273834 - trainLoss: 0.4117608368396759\n",
      "cnt: 0 - valLoss: 0.42982199788093567 - trainLoss: 0.41175925731658936\n",
      "cnt: 0 - valLoss: 0.4298214018344879 - trainLoss: 0.4117577075958252\n",
      "cnt: 0 - valLoss: 0.42982083559036255 - trainLoss: 0.41175612807273865\n",
      "cnt: 0 - valLoss: 0.42982029914855957 - trainLoss: 0.4117545485496521\n",
      "cnt: 0 - valLoss: 0.42981967329978943 - trainLoss: 0.41175299882888794\n",
      "cnt: 0 - valLoss: 0.4298190772533417 - trainLoss: 0.411751389503479\n",
      "cnt: 0 - valLoss: 0.4298185408115387 - trainLoss: 0.41174983978271484\n",
      "cnt: 0 - valLoss: 0.42981797456741333 - trainLoss: 0.4117482900619507\n",
      "cnt: 0 - valLoss: 0.4298173785209656 - trainLoss: 0.41174668073654175\n",
      "cnt: 0 - valLoss: 0.4298167824745178 - trainLoss: 0.4117451310157776\n",
      "cnt: 0 - valLoss: 0.4298161566257477 - trainLoss: 0.4117435812950134\n",
      "cnt: 0 - valLoss: 0.4298156499862671 - trainLoss: 0.4117419719696045\n",
      "cnt: 0 - valLoss: 0.42981505393981934 - trainLoss: 0.41174042224884033\n",
      "cnt: 0 - valLoss: 0.42981448769569397 - trainLoss: 0.41173887252807617\n",
      "cnt: 0 - valLoss: 0.429813951253891 - trainLoss: 0.41173726320266724\n",
      "cnt: 0 - valLoss: 0.42981335520744324 - trainLoss: 0.4117357134819031\n",
      "cnt: 0 - valLoss: 0.42981278896331787 - trainLoss: 0.4117341637611389\n",
      "cnt: 0 - valLoss: 0.4298122227191925 - trainLoss: 0.41173258423805237\n",
      "cnt: 0 - valLoss: 0.42981162667274475 - trainLoss: 0.4117310047149658\n",
      "cnt: 0 - valLoss: 0.429811030626297 - trainLoss: 0.4117294251918793\n",
      "cnt: 0 - valLoss: 0.4298105239868164 - trainLoss: 0.4117278754711151\n",
      "cnt: 0 - valLoss: 0.42980992794036865 - trainLoss: 0.41172629594802856\n",
      "cnt: 0 - valLoss: 0.4298093616962433 - trainLoss: 0.411724716424942\n",
      "cnt: 0 - valLoss: 0.42980876564979553 - trainLoss: 0.41172316670417786\n",
      "cnt: 0 - valLoss: 0.42980822920799255 - trainLoss: 0.4117215871810913\n",
      "cnt: 0 - valLoss: 0.4298076331615448 - trainLoss: 0.41172000765800476\n",
      "cnt: 0 - valLoss: 0.4298070967197418 - trainLoss: 0.4117184579372406\n",
      "cnt: 0 - valLoss: 0.4298064708709717 - trainLoss: 0.41171687841415405\n",
      "cnt: 0 - valLoss: 0.4298059642314911 - trainLoss: 0.4117153286933899\n",
      "cnt: 0 - valLoss: 0.42980536818504333 - trainLoss: 0.41171374917030334\n",
      "cnt: 0 - valLoss: 0.42980480194091797 - trainLoss: 0.4117121696472168\n",
      "cnt: 0 - valLoss: 0.429804265499115 - trainLoss: 0.41171061992645264\n",
      "cnt: 0 - valLoss: 0.42980366945266724 - trainLoss: 0.4117090404033661\n",
      "cnt: 0 - valLoss: 0.42980310320854187 - trainLoss: 0.41170746088027954\n",
      "cnt: 0 - valLoss: 0.4298025369644165 - trainLoss: 0.41170594096183777\n",
      "cnt: 0 - valLoss: 0.42980197072029114 - trainLoss: 0.41170433163642883\n",
      "cnt: 0 - valLoss: 0.42980143427848816 - trainLoss: 0.4117027521133423\n",
      "cnt: 0 - valLoss: 0.4298008680343628 - trainLoss: 0.4117012321949005\n",
      "cnt: 0 - valLoss: 0.4298003017902374 - trainLoss: 0.4116996228694916\n",
      "cnt: 0 - valLoss: 0.42979979515075684 - trainLoss: 0.4116981029510498\n",
      "cnt: 0 - valLoss: 0.4297991991043091 - trainLoss: 0.41169652342796326\n",
      "cnt: 0 - valLoss: 0.4297986924648285 - trainLoss: 0.4116949141025543\n",
      "cnt: 0 - valLoss: 0.4297981262207031 - trainLoss: 0.41169339418411255\n",
      "cnt: 0 - valLoss: 0.42979755997657776 - trainLoss: 0.411691814661026\n",
      "cnt: 0 - valLoss: 0.4297970235347748 - trainLoss: 0.41169020533561707\n",
      "cnt: 0 - valLoss: 0.4297964572906494 - trainLoss: 0.4116886258125305\n",
      "cnt: 0 - valLoss: 0.42979589104652405 - trainLoss: 0.41168710589408875\n",
      "cnt: 0 - valLoss: 0.42979535460472107 - trainLoss: 0.4116855561733246\n",
      "cnt: 0 - valLoss: 0.4297948181629181 - trainLoss: 0.4116840064525604\n",
      "cnt: 0 - valLoss: 0.4297942817211151 - trainLoss: 0.4116824269294739\n",
      "cnt: 0 - valLoss: 0.42979371547698975 - trainLoss: 0.41168084740638733\n",
      "cnt: 0 - valLoss: 0.4297931492328644 - trainLoss: 0.41167929768562317\n",
      "cnt: 0 - valLoss: 0.4297926425933838 - trainLoss: 0.4116777181625366\n",
      "cnt: 0 - valLoss: 0.4297920763492584 - trainLoss: 0.4116761386394501\n",
      "cnt: 0 - valLoss: 0.42979153990745544 - trainLoss: 0.4116745889186859\n",
      "cnt: 0 - valLoss: 0.4297909736633301 - trainLoss: 0.41167300939559937\n",
      "cnt: 0 - valLoss: 0.4297904670238495 - trainLoss: 0.4116714596748352\n",
      "cnt: 0 - valLoss: 0.4297899007797241 - trainLoss: 0.41166988015174866\n",
      "cnt: 0 - valLoss: 0.42978933453559875 - trainLoss: 0.4116683602333069\n",
      "cnt: 0 - valLoss: 0.42978882789611816 - trainLoss: 0.41166672110557556\n",
      "cnt: 0 - valLoss: 0.4297882616519928 - trainLoss: 0.4116651713848114\n",
      "cnt: 0 - valLoss: 0.4297877252101898 - trainLoss: 0.41166365146636963\n",
      "cnt: 0 - valLoss: 0.42978715896606445 - trainLoss: 0.4116620421409607\n",
      "cnt: 0 - valLoss: 0.4297865927219391 - trainLoss: 0.41166046261787415\n",
      "cnt: 0 - valLoss: 0.4297860860824585 - trainLoss: 0.4116589426994324\n",
      "cnt: 0 - valLoss: 0.4297855794429779 - trainLoss: 0.4116573929786682\n",
      "cnt: 0 - valLoss: 0.42978495359420776 - trainLoss: 0.41165581345558167\n",
      "cnt: 0 - valLoss: 0.4297844469547272 - trainLoss: 0.4116542339324951\n",
      "cnt: 0 - valLoss: 0.4297839403152466 - trainLoss: 0.41165268421173096\n",
      "cnt: 0 - valLoss: 0.4297833740711212 - trainLoss: 0.4116511344909668\n",
      "cnt: 0 - valLoss: 0.42978283762931824 - trainLoss: 0.41164952516555786\n",
      "cnt: 0 - valLoss: 0.42978230118751526 - trainLoss: 0.4116479754447937\n",
      "cnt: 0 - valLoss: 0.4297817647457123 - trainLoss: 0.41164639592170715\n",
      "cnt: 0 - valLoss: 0.4297811985015869 - trainLoss: 0.411644846200943\n",
      "cnt: 0 - valLoss: 0.4297806918621063 - trainLoss: 0.41164326667785645\n",
      "cnt: 0 - valLoss: 0.42978012561798096 - trainLoss: 0.4116417169570923\n",
      "cnt: 0 - valLoss: 0.42977961897850037 - trainLoss: 0.41164013743400574\n",
      "cnt: 0 - valLoss: 0.429779052734375 - trainLoss: 0.4116385877132416\n",
      "cnt: 0 - valLoss: 0.4297785460948944 - trainLoss: 0.41163700819015503\n",
      "cnt: 0 - valLoss: 0.42977797985076904 - trainLoss: 0.4116354286670685\n",
      "cnt: 0 - valLoss: 0.4297774136066437 - trainLoss: 0.41163384914398193\n",
      "cnt: 0 - valLoss: 0.4297769069671631 - trainLoss: 0.4116322994232178\n",
      "cnt: 0 - valLoss: 0.4297764003276825 - trainLoss: 0.411630779504776\n",
      "cnt: 0 - valLoss: 0.42977580428123474 - trainLoss: 0.41162917017936707\n",
      "cnt: 0 - valLoss: 0.42977526783943176 - trainLoss: 0.4116276502609253\n",
      "cnt: 0 - valLoss: 0.42977476119995117 - trainLoss: 0.41162607073783875\n",
      "cnt: 0 - valLoss: 0.4297742545604706 - trainLoss: 0.4116245210170746\n",
      "cnt: 0 - valLoss: 0.42977374792099 - trainLoss: 0.41162288188934326\n",
      "cnt: 0 - valLoss: 0.42977315187454224 - trainLoss: 0.4116213619709015\n",
      "cnt: 0 - valLoss: 0.42977261543273926 - trainLoss: 0.41161981225013733\n",
      "cnt: 0 - valLoss: 0.42977210879325867 - trainLoss: 0.4116182327270508\n",
      "cnt: 0 - valLoss: 0.4297715425491333 - trainLoss: 0.41161665320396423\n",
      "cnt: 0 - valLoss: 0.4297710359096527 - trainLoss: 0.4116151034832001\n",
      "cnt: 0 - valLoss: 0.42977046966552734 - trainLoss: 0.4116135537624359\n",
      "cnt: 0 - valLoss: 0.42976996302604675 - trainLoss: 0.411611944437027\n",
      "cnt: 0 - valLoss: 0.4297693967819214 - trainLoss: 0.4116103947162628\n",
      "cnt: 0 - valLoss: 0.4297688901424408 - trainLoss: 0.41160884499549866\n",
      "cnt: 0 - valLoss: 0.42976832389831543 - trainLoss: 0.4116072654724121\n",
      "cnt: 0 - valLoss: 0.42976781725883484 - trainLoss: 0.41160571575164795\n",
      "cnt: 0 - valLoss: 0.4297672510147095 - trainLoss: 0.4116041362285614\n",
      "cnt: 0 - valLoss: 0.4297667443752289 - trainLoss: 0.41160255670547485\n",
      "cnt: 0 - valLoss: 0.4297662079334259 - trainLoss: 0.4116010069847107\n",
      "cnt: 0 - valLoss: 0.4297656714916229 - trainLoss: 0.41159942746162415\n",
      "cnt: 0 - valLoss: 0.42976513504981995 - trainLoss: 0.4115979075431824\n",
      "cnt: 0 - valLoss: 0.42976459860801697 - trainLoss: 0.4115963578224182\n",
      "cnt: 0 - valLoss: 0.4297640919685364 - trainLoss: 0.41159477829933167\n",
      "cnt: 0 - valLoss: 0.429763525724411 - trainLoss: 0.4115931987762451\n",
      "cnt: 0 - valLoss: 0.4297630190849304 - trainLoss: 0.41159164905548096\n",
      "cnt: 0 - valLoss: 0.42976251244544983 - trainLoss: 0.4115900695323944\n",
      "cnt: 0 - valLoss: 0.42976194620132446 - trainLoss: 0.41158849000930786\n",
      "cnt: 0 - valLoss: 0.42976143956184387 - trainLoss: 0.4115869402885437\n",
      "cnt: 0 - valLoss: 0.4297609329223633 - trainLoss: 0.41158539056777954\n",
      "cnt: 0 - valLoss: 0.4297603666782379 - trainLoss: 0.411583811044693\n",
      "cnt: 0 - valLoss: 0.4297598600387573 - trainLoss: 0.41158223152160645\n",
      "cnt: 0 - valLoss: 0.42975932359695435 - trainLoss: 0.4115806818008423\n",
      "cnt: 0 - valLoss: 0.42975881695747375 - trainLoss: 0.4115791618824005\n",
      "cnt: 0 - valLoss: 0.4297582507133484 - trainLoss: 0.4115775525569916\n",
      "cnt: 0 - valLoss: 0.4297577440738678 - trainLoss: 0.41157597303390503\n",
      "cnt: 0 - valLoss: 0.4297572076320648 - trainLoss: 0.4115743935108185\n",
      "cnt: 0 - valLoss: 0.42975667119026184 - trainLoss: 0.4115728437900543\n",
      "cnt: 0 - valLoss: 0.42975616455078125 - trainLoss: 0.4115712642669678\n",
      "cnt: 0 - valLoss: 0.42975565791130066 - trainLoss: 0.411569744348526\n",
      "cnt: 0 - valLoss: 0.4297551214694977 - trainLoss: 0.41156819462776184\n",
      "cnt: 0 - valLoss: 0.4297546148300171 - trainLoss: 0.4115666151046753\n",
      "cnt: 0 - valLoss: 0.4297541081905365 - trainLoss: 0.41156503558158875\n",
      "cnt: 0 - valLoss: 0.4297536015510559 - trainLoss: 0.4115634858608246\n",
      "cnt: 0 - valLoss: 0.42975306510925293 - trainLoss: 0.4115619361400604\n",
      "cnt: 0 - valLoss: 0.42975255846977234 - trainLoss: 0.4115603566169739\n",
      "cnt: 0 - valLoss: 0.42975205183029175 - trainLoss: 0.4115588068962097\n",
      "cnt: 0 - valLoss: 0.4297514855861664 - trainLoss: 0.41155722737312317\n",
      "cnt: 0 - valLoss: 0.4297509789466858 - trainLoss: 0.4115557074546814\n",
      "cnt: 0 - valLoss: 0.4297505021095276 - trainLoss: 0.41155409812927246\n",
      "cnt: 0 - valLoss: 0.429749995470047 - trainLoss: 0.4115525186061859\n",
      "cnt: 0 - valLoss: 0.4297494888305664 - trainLoss: 0.41155099868774414\n",
      "cnt: 0 - valLoss: 0.42974892258644104 - trainLoss: 0.4115493893623352\n",
      "cnt: 0 - valLoss: 0.42974841594696045 - trainLoss: 0.41154786944389343\n",
      "cnt: 0 - valLoss: 0.42974793910980225 - trainLoss: 0.4115462899208069\n",
      "cnt: 0 - valLoss: 0.42974743247032166 - trainLoss: 0.4115447402000427\n",
      "cnt: 0 - valLoss: 0.4297468960285187 - trainLoss: 0.41154319047927856\n",
      "cnt: 0 - valLoss: 0.4297463893890381 - trainLoss: 0.411541610956192\n",
      "cnt: 0 - valLoss: 0.4297458827495575 - trainLoss: 0.41154003143310547\n",
      "cnt: 0 - valLoss: 0.4297453761100769 - trainLoss: 0.4115384817123413\n",
      "cnt: 0 - valLoss: 0.42974475026130676 - trainLoss: 0.41153693199157715\n",
      "cnt: 0 - valLoss: 0.42974424362182617 - trainLoss: 0.4115353226661682\n",
      "cnt: 0 - valLoss: 0.4297436475753784 - trainLoss: 0.41153377294540405\n",
      "cnt: 0 - valLoss: 0.42974305152893066 - trainLoss: 0.4115322232246399\n",
      "cnt: 0 - valLoss: 0.4297424554824829 - trainLoss: 0.41153061389923096\n",
      "cnt: 0 - valLoss: 0.42974185943603516 - trainLoss: 0.4115290641784668\n",
      "cnt: 0 - valLoss: 0.4297412931919098 - trainLoss: 0.41152748465538025\n",
      "cnt: 0 - valLoss: 0.4297407269477844 - trainLoss: 0.4115259051322937\n",
      "cnt: 0 - valLoss: 0.42974013090133667 - trainLoss: 0.41152435541152954\n",
      "cnt: 0 - valLoss: 0.4297395646572113 - trainLoss: 0.411522775888443\n",
      "cnt: 0 - valLoss: 0.4297390282154083 - trainLoss: 0.41152119636535645\n",
      "cnt: 0 - valLoss: 0.42973843216896057 - trainLoss: 0.4115196168422699\n",
      "cnt: 0 - valLoss: 0.42973780632019043 - trainLoss: 0.41151806712150574\n",
      "cnt: 0 - valLoss: 0.42973729968070984 - trainLoss: 0.4115164875984192\n",
      "cnt: 0 - valLoss: 0.4297367036342621 - trainLoss: 0.41151490807533264\n",
      "cnt: 0 - valLoss: 0.4297361373901367 - trainLoss: 0.4115133285522461\n",
      "cnt: 0 - valLoss: 0.42973554134368896 - trainLoss: 0.41151177883148193\n",
      "cnt: 0 - valLoss: 0.429735004901886 - trainLoss: 0.4115101993083954\n",
      "cnt: 0 - valLoss: 0.4297344386577606 - trainLoss: 0.41150861978530884\n",
      "cnt: 0 - valLoss: 0.42973384261131287 - trainLoss: 0.4115070700645447\n",
      "cnt: 0 - valLoss: 0.4297332763671875 - trainLoss: 0.41150549054145813\n",
      "cnt: 0 - valLoss: 0.42973268032073975 - trainLoss: 0.4115039110183716\n",
      "cnt: 0 - valLoss: 0.42973217368125916 - trainLoss: 0.41150233149528503\n",
      "cnt: 0 - valLoss: 0.429731547832489 - trainLoss: 0.4115007817745209\n",
      "cnt: 0 - valLoss: 0.42973101139068604 - trainLoss: 0.4114992022514343\n",
      "cnt: 0 - valLoss: 0.42973044514656067 - trainLoss: 0.41149765253067017\n",
      "cnt: 0 - valLoss: 0.4297298491001129 - trainLoss: 0.41149601340293884\n",
      "cnt: 0 - valLoss: 0.4297293424606323 - trainLoss: 0.41149449348449707\n",
      "cnt: 0 - valLoss: 0.42972874641418457 - trainLoss: 0.4114929139614105\n",
      "cnt: 0 - valLoss: 0.42972812056541443 - trainLoss: 0.4114913046360016\n",
      "cnt: 0 - valLoss: 0.42972758412361145 - trainLoss: 0.4114897847175598\n",
      "cnt: 0 - valLoss: 0.4297270178794861 - trainLoss: 0.41148820519447327\n",
      "cnt: 0 - valLoss: 0.4297265112400055 - trainLoss: 0.4114866554737091\n",
      "cnt: 0 - valLoss: 0.42972591519355774 - trainLoss: 0.4114850163459778\n",
      "cnt: 0 - valLoss: 0.4297252893447876 - trainLoss: 0.411483496427536\n",
      "cnt: 0 - valLoss: 0.429724782705307 - trainLoss: 0.41148191690444946\n",
      "cnt: 0 - valLoss: 0.42972424626350403 - trainLoss: 0.4114803075790405\n",
      "cnt: 0 - valLoss: 0.4297236204147339 - trainLoss: 0.41147878766059875\n",
      "cnt: 0 - valLoss: 0.4297230839729309 - trainLoss: 0.4114772081375122\n",
      "cnt: 0 - valLoss: 0.42972248792648315 - trainLoss: 0.41147559881210327\n",
      "cnt: 0 - valLoss: 0.4297219514846802 - trainLoss: 0.4114740192890167\n",
      "cnt: 0 - valLoss: 0.4297213554382324 - trainLoss: 0.41147249937057495\n",
      "cnt: 0 - valLoss: 0.42972078919410706 - trainLoss: 0.411470890045166\n",
      "cnt: 0 - valLoss: 0.4297202527523041 - trainLoss: 0.41146931052207947\n",
      "cnt: 0 - valLoss: 0.4297196865081787 - trainLoss: 0.4114677309989929\n",
      "cnt: 0 - valLoss: 0.42971912026405334 - trainLoss: 0.41146615147590637\n",
      "cnt: 0 - valLoss: 0.42971861362457275 - trainLoss: 0.4114646017551422\n",
      "cnt: 0 - valLoss: 0.429718017578125 - trainLoss: 0.41146308183670044\n",
      "cnt: 0 - valLoss: 0.42971742153167725 - trainLoss: 0.4114614725112915\n",
      "cnt: 0 - valLoss: 0.42971691489219666 - trainLoss: 0.41145989298820496\n",
      "cnt: 0 - valLoss: 0.4297163486480713 - trainLoss: 0.4114583134651184\n",
      "cnt: 0 - valLoss: 0.4297157824039459 - trainLoss: 0.41145673394203186\n",
      "cnt: 0 - valLoss: 0.42971527576446533 - trainLoss: 0.4114551842212677\n",
      "cnt: 0 - valLoss: 0.4297146797180176 - trainLoss: 0.41145360469818115\n",
      "cnt: 0 - valLoss: 0.429714173078537 - trainLoss: 0.4114520847797394\n",
      "cnt: 0 - valLoss: 0.42971354722976685 - trainLoss: 0.41145047545433044\n",
      "cnt: 0 - valLoss: 0.42971304059028625 - trainLoss: 0.4114488959312439\n",
      "cnt: 0 - valLoss: 0.4297124743461609 - trainLoss: 0.41144731640815735\n",
      "cnt: 0 - valLoss: 0.4297119379043579 - trainLoss: 0.4114457666873932\n",
      "cnt: 0 - valLoss: 0.42971137166023254 - trainLoss: 0.4114442467689514\n",
      "cnt: 0 - valLoss: 0.4297108054161072 - trainLoss: 0.4114426076412201\n",
      "cnt: 0 - valLoss: 0.4297102987766266 - trainLoss: 0.41144105792045593\n",
      "cnt: 0 - valLoss: 0.4297097325325012 - trainLoss: 0.411439448595047\n",
      "cnt: 0 - valLoss: 0.42970919609069824 - trainLoss: 0.41143789887428284\n",
      "cnt: 0 - valLoss: 0.4297086298465729 - trainLoss: 0.4114363491535187\n",
      "cnt: 0 - valLoss: 0.4297080636024475 - trainLoss: 0.41143476963043213\n",
      "cnt: 0 - valLoss: 0.4297075569629669 - trainLoss: 0.4114331901073456\n",
      "cnt: 0 - valLoss: 0.42970699071884155 - trainLoss: 0.4114316403865814\n",
      "cnt: 0 - valLoss: 0.4297064542770386 - trainLoss: 0.4114300608634949\n",
      "cnt: 0 - valLoss: 0.4297059178352356 - trainLoss: 0.4114284813404083\n",
      "cnt: 0 - valLoss: 0.429705411195755 - trainLoss: 0.4114269018173218\n",
      "cnt: 0 - valLoss: 0.42970481514930725 - trainLoss: 0.4114253520965576\n",
      "cnt: 0 - valLoss: 0.42970430850982666 - trainLoss: 0.41142377257347107\n",
      "cnt: 0 - valLoss: 0.42970380187034607 - trainLoss: 0.4114222228527069\n",
      "cnt: 0 - valLoss: 0.4297031760215759 - trainLoss: 0.41142064332962036\n",
      "cnt: 0 - valLoss: 0.42970266938209534 - trainLoss: 0.4114191234111786\n",
      "cnt: 0 - valLoss: 0.42970216274261475 - trainLoss: 0.41141754388809204\n",
      "cnt: 0 - valLoss: 0.4297015964984894 - trainLoss: 0.4114159345626831\n",
      "cnt: 0 - valLoss: 0.4297010898590088 - trainLoss: 0.41141435503959656\n",
      "cnt: 0 - valLoss: 0.4297005236148834 - trainLoss: 0.4114128351211548\n",
      "cnt: 0 - valLoss: 0.42970001697540283 - trainLoss: 0.41141122579574585\n",
      "cnt: 0 - valLoss: 0.42969945073127747 - trainLoss: 0.4114096462726593\n",
      "cnt: 0 - valLoss: 0.4296989142894745 - trainLoss: 0.41140812635421753\n",
      "cnt: 0 - valLoss: 0.4296983778476715 - trainLoss: 0.4114064872264862\n",
      "cnt: 0 - valLoss: 0.4296978712081909 - trainLoss: 0.4114049971103668\n",
      "cnt: 0 - valLoss: 0.4296973645687103 - trainLoss: 0.4114034175872803\n",
      "cnt: 0 - valLoss: 0.42969685792922974 - trainLoss: 0.41140180826187134\n",
      "cnt: 0 - valLoss: 0.42969629168510437 - trainLoss: 0.4114002287387848\n",
      "cnt: 0 - valLoss: 0.429695725440979 - trainLoss: 0.411398708820343\n",
      "cnt: 0 - valLoss: 0.4296952188014984 - trainLoss: 0.4113970994949341\n",
      "cnt: 0 - valLoss: 0.4296947121620178 - trainLoss: 0.4113955795764923\n",
      "cnt: 0 - valLoss: 0.42969417572021484 - trainLoss: 0.41139400005340576\n",
      "cnt: 0 - valLoss: 0.42969366908073425 - trainLoss: 0.4113923907279968\n",
      "cnt: 0 - valLoss: 0.42969316244125366 - trainLoss: 0.4113908112049103\n",
      "cnt: 0 - valLoss: 0.42969265580177307 - trainLoss: 0.4113892912864685\n",
      "cnt: 0 - valLoss: 0.4296920895576477 - trainLoss: 0.41138768196105957\n",
      "cnt: 0 - valLoss: 0.4296915829181671 - trainLoss: 0.411386102437973\n",
      "cnt: 0 - valLoss: 0.4296911060810089 - trainLoss: 0.41138458251953125\n",
      "cnt: 0 - valLoss: 0.42969053983688354 - trainLoss: 0.4113830029964447\n",
      "cnt: 0 - valLoss: 0.42969003319740295 - trainLoss: 0.41138139367103577\n",
      "cnt: 0 - valLoss: 0.42968952655792236 - trainLoss: 0.411379873752594\n",
      "cnt: 0 - valLoss: 0.4296889901161194 - trainLoss: 0.41137832403182983\n",
      "cnt: 0 - valLoss: 0.4296884834766388 - trainLoss: 0.4113767445087433\n",
      "cnt: 0 - valLoss: 0.4296879172325134 - trainLoss: 0.41137516498565674\n",
      "cnt: 0 - valLoss: 0.42968741059303284 - trainLoss: 0.4113736152648926\n",
      "cnt: 0 - valLoss: 0.42968690395355225 - trainLoss: 0.41137203574180603\n",
      "cnt: 0 - valLoss: 0.42968642711639404 - trainLoss: 0.4113704562187195\n",
      "cnt: 0 - valLoss: 0.42968592047691345 - trainLoss: 0.4113689064979553\n",
      "cnt: 0 - valLoss: 0.4296853542327881 - trainLoss: 0.41136735677719116\n",
      "cnt: 0 - valLoss: 0.4296848475933075 - trainLoss: 0.4113657474517822\n",
      "cnt: 0 - valLoss: 0.4296843409538269 - trainLoss: 0.41136419773101807\n",
      "cnt: 0 - valLoss: 0.4296838045120239 - trainLoss: 0.4113626480102539\n",
      "cnt: 0 - valLoss: 0.42968329787254333 - trainLoss: 0.41136106848716736\n",
      "cnt: 0 - valLoss: 0.42968279123306274 - trainLoss: 0.4113594889640808\n",
      "cnt: 0 - valLoss: 0.42968228459358215 - trainLoss: 0.41135790944099426\n",
      "cnt: 0 - valLoss: 0.4296817481517792 - trainLoss: 0.4113563597202301\n",
      "cnt: 0 - valLoss: 0.42968127131462097 - trainLoss: 0.41135478019714355\n",
      "cnt: 0 - valLoss: 0.429680734872818 - trainLoss: 0.4113532304763794\n",
      "cnt: 0 - valLoss: 0.4296802580356598 - trainLoss: 0.41135165095329285\n",
      "cnt: 0 - valLoss: 0.4296796917915344 - trainLoss: 0.4113500714302063\n",
      "cnt: 0 - valLoss: 0.42967918515205383 - trainLoss: 0.41134852170944214\n",
      "cnt: 0 - valLoss: 0.42967867851257324 - trainLoss: 0.41134700179100037\n",
      "cnt: 0 - valLoss: 0.42967814207077026 - trainLoss: 0.4113454222679138\n",
      "cnt: 0 - valLoss: 0.4296776354312897 - trainLoss: 0.4113438129425049\n",
      "cnt: 0 - valLoss: 0.42967715859413147 - trainLoss: 0.4113422930240631\n",
      "cnt: 0 - valLoss: 0.4296766221523285 - trainLoss: 0.41134071350097656\n",
      "cnt: 0 - valLoss: 0.4296760857105255 - trainLoss: 0.4113391041755676\n",
      "cnt: 0 - valLoss: 0.4296755790710449 - trainLoss: 0.41133758425712585\n",
      "cnt: 0 - valLoss: 0.42967507243156433 - trainLoss: 0.4113360047340393\n",
      "cnt: 0 - valLoss: 0.42967456579208374 - trainLoss: 0.41133439540863037\n",
      "cnt: 0 - valLoss: 0.42967408895492554 - trainLoss: 0.4113328754901886\n",
      "cnt: 0 - valLoss: 0.42967358231544495 - trainLoss: 0.41133129596710205\n",
      "cnt: 0 - valLoss: 0.42967304587364197 - trainLoss: 0.4113297462463379\n",
      "cnt: 0 - valLoss: 0.4296725392341614 - trainLoss: 0.41132816672325134\n",
      "cnt: 0 - valLoss: 0.4296720325946808 - trainLoss: 0.4113266170024872\n",
      "cnt: 0 - valLoss: 0.4296715259552002 - trainLoss: 0.41132503747940063\n",
      "cnt: 0 - valLoss: 0.429671049118042 - trainLoss: 0.4113234579563141\n",
      "cnt: 0 - valLoss: 0.429670512676239 - trainLoss: 0.4113219082355499\n",
      "cnt: 0 - valLoss: 0.4296700060367584 - trainLoss: 0.4113203287124634\n",
      "cnt: 0 - valLoss: 0.42966949939727783 - trainLoss: 0.41131874918937683\n",
      "cnt: 0 - valLoss: 0.42966902256011963 - trainLoss: 0.41131719946861267\n",
      "cnt: 0 - valLoss: 0.42966845631599426 - trainLoss: 0.4113156497478485\n",
      "cnt: 0 - valLoss: 0.42966800928115845 - trainLoss: 0.41131407022476196\n",
      "cnt: 0 - valLoss: 0.42966747283935547 - trainLoss: 0.4113124907016754\n",
      "cnt: 0 - valLoss: 0.42966702580451965 - trainLoss: 0.41131094098091125\n",
      "cnt: 0 - valLoss: 0.4296664893627167 - trainLoss: 0.4113093614578247\n",
      "cnt: 0 - valLoss: 0.4296659529209137 - trainLoss: 0.41130778193473816\n",
      "cnt: 0 - valLoss: 0.4296654760837555 - trainLoss: 0.411306232213974\n",
      "cnt: 0 - valLoss: 0.4296649694442749 - trainLoss: 0.4113047122955322\n",
      "cnt: 0 - valLoss: 0.4296644330024719 - trainLoss: 0.4113030731678009\n",
      "cnt: 0 - valLoss: 0.42966392636299133 - trainLoss: 0.41130152344703674\n",
      "cnt: 0 - valLoss: 0.42966344952583313 - trainLoss: 0.41130000352859497\n",
      "cnt: 0 - valLoss: 0.4296629726886749 - trainLoss: 0.4112984240055084\n",
      "cnt: 0 - valLoss: 0.42966243624687195 - trainLoss: 0.4112968146800995\n",
      "cnt: 0 - valLoss: 0.42966195940971375 - trainLoss: 0.4112952947616577\n",
      "cnt: 0 - valLoss: 0.42966145277023315 - trainLoss: 0.41129374504089355\n",
      "cnt: 0 - valLoss: 0.4296609163284302 - trainLoss: 0.41129210591316223\n",
      "cnt: 0 - valLoss: 0.429660439491272 - trainLoss: 0.41129058599472046\n",
      "cnt: 0 - valLoss: 0.4296599328517914 - trainLoss: 0.4112890362739563\n",
      "cnt: 0 - valLoss: 0.4296594560146332 - trainLoss: 0.41128745675086975\n",
      "cnt: 0 - valLoss: 0.4296589493751526 - trainLoss: 0.4112858772277832\n",
      "cnt: 0 - valLoss: 0.429658442735672 - trainLoss: 0.41128432750701904\n",
      "cnt: 0 - valLoss: 0.4296579658985138 - trainLoss: 0.4112827777862549\n",
      "cnt: 0 - valLoss: 0.4296574592590332 - trainLoss: 0.41128116846084595\n",
      "cnt: 0 - valLoss: 0.4296569526195526 - trainLoss: 0.4112796187400818\n",
      "cnt: 0 - valLoss: 0.42965641617774963 - trainLoss: 0.4112780690193176\n",
      "cnt: 0 - valLoss: 0.42965593934059143 - trainLoss: 0.4112764894962311\n",
      "cnt: 0 - valLoss: 0.42965543270111084 - trainLoss: 0.41127490997314453\n",
      "cnt: 0 - valLoss: 0.42965492606163025 - trainLoss: 0.41127336025238037\n",
      "cnt: 0 - valLoss: 0.42965441942214966 - trainLoss: 0.4112718403339386\n",
      "cnt: 0 - valLoss: 0.42965394258499146 - trainLoss: 0.41127026081085205\n",
      "cnt: 0 - valLoss: 0.42965346574783325 - trainLoss: 0.4112686514854431\n",
      "cnt: 0 - valLoss: 0.42965295910835266 - trainLoss: 0.41126713156700134\n",
      "cnt: 0 - valLoss: 0.42965245246887207 - trainLoss: 0.4112655818462372\n",
      "cnt: 0 - valLoss: 0.4296519160270691 - trainLoss: 0.41126400232315063\n",
      "cnt: 0 - valLoss: 0.4296514391899109 - trainLoss: 0.4112624228000641\n",
      "cnt: 0 - valLoss: 0.4296509325504303 - trainLoss: 0.4112608730792999\n",
      "cnt: 0 - valLoss: 0.4296504557132721 - trainLoss: 0.41125932335853577\n",
      "cnt: 0 - valLoss: 0.4296500086784363 - trainLoss: 0.4112577438354492\n",
      "cnt: 0 - valLoss: 0.4296494722366333 - trainLoss: 0.41125616431236267\n",
      "cnt: 0 - valLoss: 0.4296489655971527 - trainLoss: 0.4112546145915985\n",
      "cnt: 0 - valLoss: 0.4296484887599945 - trainLoss: 0.41125303506851196\n",
      "cnt: 0 - valLoss: 0.4296479821205139 - trainLoss: 0.4112514555454254\n",
      "cnt: 0 - valLoss: 0.4296475052833557 - trainLoss: 0.41124990582466125\n",
      "cnt: 0 - valLoss: 0.4296469986438751 - trainLoss: 0.4112483859062195\n",
      "cnt: 0 - valLoss: 0.4296465218067169 - trainLoss: 0.4112468361854553\n",
      "cnt: 0 - valLoss: 0.42964601516723633 - trainLoss: 0.411245197057724\n",
      "cnt: 0 - valLoss: 0.4296455383300781 - trainLoss: 0.4112436771392822\n",
      "cnt: 0 - valLoss: 0.4296450614929199 - trainLoss: 0.41124212741851807\n",
      "cnt: 0 - valLoss: 0.42964455485343933 - trainLoss: 0.4112405478954315\n",
      "cnt: 0 - valLoss: 0.42964407801628113 - trainLoss: 0.41123899817466736\n",
      "cnt: 0 - valLoss: 0.4296436011791229 - trainLoss: 0.4112374186515808\n",
      "cnt: 0 - valLoss: 0.42964309453964233 - trainLoss: 0.41123586893081665\n",
      "cnt: 0 - valLoss: 0.42964258790016174 - trainLoss: 0.4112343490123749\n",
      "cnt: 0 - valLoss: 0.42964211106300354 - trainLoss: 0.41123270988464355\n",
      "cnt: 0 - valLoss: 0.42964163422584534 - trainLoss: 0.4112311601638794\n",
      "cnt: 0 - valLoss: 0.42964112758636475 - trainLoss: 0.4112296402454376\n",
      "cnt: 0 - valLoss: 0.42964062094688416 - trainLoss: 0.41122809052467346\n",
      "cnt: 0 - valLoss: 0.42964014410972595 - trainLoss: 0.41122645139694214\n",
      "cnt: 0 - valLoss: 0.42963966727256775 - trainLoss: 0.41122493147850037\n",
      "cnt: 0 - valLoss: 0.42963916063308716 - trainLoss: 0.4112233817577362\n",
      "cnt: 0 - valLoss: 0.42963871359825134 - trainLoss: 0.41122180223464966\n",
      "cnt: 0 - valLoss: 0.42963820695877075 - trainLoss: 0.4112202525138855\n",
      "cnt: 0 - valLoss: 0.42963770031929016 - trainLoss: 0.41121867299079895\n",
      "cnt: 0 - valLoss: 0.42963722348213196 - trainLoss: 0.4112171232700348\n",
      "cnt: 0 - valLoss: 0.42963674664497375 - trainLoss: 0.411215603351593\n",
      "cnt: 0 - valLoss: 0.42963624000549316 - trainLoss: 0.4112139642238617\n",
      "cnt: 0 - valLoss: 0.42963582277297974 - trainLoss: 0.4112124741077423\n",
      "cnt: 0 - valLoss: 0.42963528633117676 - trainLoss: 0.41121089458465576\n",
      "cnt: 0 - valLoss: 0.42963480949401855 - trainLoss: 0.4112093448638916\n",
      "cnt: 0 - valLoss: 0.42963430285453796 - trainLoss: 0.41120776534080505\n",
      "cnt: 0 - valLoss: 0.42963382601737976 - trainLoss: 0.4112062156200409\n",
      "cnt: 0 - valLoss: 0.42963331937789917 - trainLoss: 0.41120463609695435\n",
      "cnt: 0 - valLoss: 0.42963284254074097 - trainLoss: 0.4112030863761902\n",
      "cnt: 0 - valLoss: 0.42963236570358276 - trainLoss: 0.4112015664577484\n",
      "cnt: 0 - valLoss: 0.42963191866874695 - trainLoss: 0.4111999571323395\n",
      "cnt: 0 - valLoss: 0.42963144183158875 - trainLoss: 0.41119837760925293\n",
      "cnt: 0 - valLoss: 0.42963093519210815 - trainLoss: 0.41119685769081116\n",
      "cnt: 0 - valLoss: 0.42963048815727234 - trainLoss: 0.411195307970047\n",
      "cnt: 0 - valLoss: 0.42963001132011414 - trainLoss: 0.41119372844696045\n",
      "cnt: 0 - valLoss: 0.42962950468063354 - trainLoss: 0.4111921489238739\n",
      "cnt: 0 - valLoss: 0.42962899804115295 - trainLoss: 0.41119059920310974\n",
      "cnt: 0 - valLoss: 0.42962852120399475 - trainLoss: 0.4111890494823456\n",
      "cnt: 0 - valLoss: 0.42962804436683655 - trainLoss: 0.4111874997615814\n",
      "cnt: 0 - valLoss: 0.42962756752967834 - trainLoss: 0.4111858904361725\n",
      "cnt: 0 - valLoss: 0.42962712049484253 - trainLoss: 0.4111843407154083\n",
      "cnt: 0 - valLoss: 0.4296266436576843 - trainLoss: 0.41118279099464417\n",
      "cnt: 0 - valLoss: 0.42962610721588135 - trainLoss: 0.4111812114715576\n",
      "cnt: 0 - valLoss: 0.42962566018104553 - trainLoss: 0.41117963194847107\n",
      "cnt: 0 - valLoss: 0.42962518334388733 - trainLoss: 0.4111780822277069\n",
      "cnt: 0 - valLoss: 0.4296247065067291 - trainLoss: 0.41117656230926514\n",
      "cnt: 0 - valLoss: 0.4296242296695709 - trainLoss: 0.411175012588501\n",
      "cnt: 0 - valLoss: 0.42962372303009033 - trainLoss: 0.41117340326309204\n",
      "cnt: 0 - valLoss: 0.42962324619293213 - trainLoss: 0.4111718535423279\n",
      "cnt: 0 - valLoss: 0.4296227693557739 - trainLoss: 0.4111703038215637\n",
      "cnt: 0 - valLoss: 0.42962226271629333 - trainLoss: 0.4111687242984772\n",
      "cnt: 0 - valLoss: 0.42962178587913513 - trainLoss: 0.411167174577713\n",
      "cnt: 0 - valLoss: 0.42962130904197693 - trainLoss: 0.41116559505462646\n",
      "cnt: 0 - valLoss: 0.4296208918094635 - trainLoss: 0.4111640453338623\n",
      "cnt: 0 - valLoss: 0.4296203851699829 - trainLoss: 0.41116246581077576\n",
      "cnt: 0 - valLoss: 0.4296199083328247 - trainLoss: 0.411160945892334\n",
      "cnt: 0 - valLoss: 0.4296194016933441 - trainLoss: 0.41115933656692505\n",
      "cnt: 0 - valLoss: 0.4296189248561859 - trainLoss: 0.4111578166484833\n",
      "cnt: 0 - valLoss: 0.4296184480190277 - trainLoss: 0.41115623712539673\n",
      "cnt: 0 - valLoss: 0.4296179711818695 - trainLoss: 0.4111546277999878\n",
      "cnt: 0 - valLoss: 0.4296174645423889 - trainLoss: 0.411153107881546\n",
      "cnt: 0 - valLoss: 0.4296170473098755 - trainLoss: 0.41115155816078186\n",
      "cnt: 0 - valLoss: 0.4296165108680725 - trainLoss: 0.4111500084400177\n",
      "cnt: 0 - valLoss: 0.4296160638332367 - trainLoss: 0.4111484885215759\n",
      "cnt: 0 - valLoss: 0.4296155869960785 - trainLoss: 0.4111469089984894\n",
      "cnt: 0 - valLoss: 0.4296151101589203 - trainLoss: 0.41114529967308044\n",
      "cnt: 0 - valLoss: 0.4296146333217621 - trainLoss: 0.41114377975463867\n",
      "cnt: 0 - valLoss: 0.4296141266822815 - trainLoss: 0.4111422002315521\n",
      "cnt: 0 - valLoss: 0.4296136796474457 - trainLoss: 0.41114065051078796\n",
      "cnt: 0 - valLoss: 0.4296131730079651 - trainLoss: 0.4111390709877014\n",
      "cnt: 0 - valLoss: 0.4296126961708069 - trainLoss: 0.41113752126693726\n",
      "cnt: 0 - valLoss: 0.42961224913597107 - trainLoss: 0.4111359715461731\n",
      "cnt: 0 - valLoss: 0.42961177229881287 - trainLoss: 0.41113439202308655\n",
      "cnt: 0 - valLoss: 0.42961129546165466 - trainLoss: 0.4111328125\n",
      "cnt: 0 - valLoss: 0.42961081862449646 - trainLoss: 0.41113126277923584\n",
      "cnt: 0 - valLoss: 0.42961034178733826 - trainLoss: 0.41112974286079407\n",
      "cnt: 0 - valLoss: 0.42960983514785767 - trainLoss: 0.41112813353538513\n",
      "cnt: 0 - valLoss: 0.42960941791534424 - trainLoss: 0.41112661361694336\n",
      "cnt: 0 - valLoss: 0.42960894107818604 - trainLoss: 0.4111250340938568\n",
      "cnt: 0 - valLoss: 0.42960846424102783 - trainLoss: 0.41112348437309265\n",
      "cnt: 0 - valLoss: 0.42960795760154724 - trainLoss: 0.4111219346523285\n",
      "cnt: 0 - valLoss: 0.42960748076438904 - trainLoss: 0.41112035512924194\n",
      "cnt: 0 - valLoss: 0.42960700392723083 - trainLoss: 0.4111187756061554\n",
      "cnt: 0 - valLoss: 0.42960652709007263 - trainLoss: 0.41111722588539124\n",
      "cnt: 0 - valLoss: 0.42960605025291443 - trainLoss: 0.41111570596694946\n",
      "cnt: 0 - valLoss: 0.4296056032180786 - trainLoss: 0.4111140966415405\n",
      "cnt: 0 - valLoss: 0.4296051263809204 - trainLoss: 0.411112517118454\n",
      "cnt: 0 - valLoss: 0.4296046495437622 - trainLoss: 0.4111109972000122\n",
      "cnt: 0 - valLoss: 0.429604172706604 - trainLoss: 0.41110944747924805\n",
      "cnt: 0 - valLoss: 0.4296037554740906 - trainLoss: 0.4111078679561615\n",
      "cnt: 0 - valLoss: 0.4296032190322876 - trainLoss: 0.41110628843307495\n",
      "cnt: 0 - valLoss: 0.429602712392807 - trainLoss: 0.4111047387123108\n",
      "cnt: 0 - valLoss: 0.4296022951602936 - trainLoss: 0.41110315918922424\n",
      "cnt: 0 - valLoss: 0.4296018183231354 - trainLoss: 0.4111016094684601\n",
      "cnt: 0 - valLoss: 0.4296013414859772 - trainLoss: 0.41110002994537354\n",
      "cnt: 0 - valLoss: 0.42960092425346375 - trainLoss: 0.4110984802246094\n",
      "cnt: 0 - valLoss: 0.42960041761398315 - trainLoss: 0.4110969305038452\n",
      "cnt: 0 - valLoss: 0.42959994077682495 - trainLoss: 0.41109541058540344\n",
      "cnt: 0 - valLoss: 0.42959946393966675 - trainLoss: 0.4110938310623169\n",
      "cnt: 0 - valLoss: 0.4295990467071533 - trainLoss: 0.41109228134155273\n",
      "cnt: 0 - valLoss: 0.42959851026535034 - trainLoss: 0.4110907018184662\n",
      "cnt: 0 - valLoss: 0.42959803342819214 - trainLoss: 0.411089152097702\n",
      "cnt: 0 - valLoss: 0.4295975863933563 - trainLoss: 0.41108760237693787\n",
      "cnt: 0 - valLoss: 0.4295971393585205 - trainLoss: 0.4110860228538513\n",
      "cnt: 0 - valLoss: 0.4295966625213623 - trainLoss: 0.41108444333076477\n",
      "cnt: 0 - valLoss: 0.4295961558818817 - trainLoss: 0.4110828936100006\n",
      "cnt: 0 - valLoss: 0.4295956790447235 - trainLoss: 0.41108137369155884\n",
      "cnt: 0 - valLoss: 0.4295952320098877 - trainLoss: 0.4110797941684723\n",
      "cnt: 0 - valLoss: 0.4295947551727295 - trainLoss: 0.41107818484306335\n",
      "cnt: 0 - valLoss: 0.4295942783355713 - trainLoss: 0.4110766649246216\n",
      "cnt: 0 - valLoss: 0.4295938014984131 - trainLoss: 0.4110751152038574\n",
      "cnt: 0 - valLoss: 0.4295932948589325 - trainLoss: 0.41107356548309326\n",
      "cnt: 0 - valLoss: 0.4295928478240967 - trainLoss: 0.4110719859600067\n",
      "cnt: 0 - valLoss: 0.4295923411846161 - trainLoss: 0.41107043623924255\n",
      "cnt: 0 - valLoss: 0.42959192395210266 - trainLoss: 0.411068856716156\n",
      "cnt: 0 - valLoss: 0.42959144711494446 - trainLoss: 0.41106733679771423\n",
      "cnt: 0 - valLoss: 0.42959097027778625 - trainLoss: 0.4110657274723053\n",
      "cnt: 0 - valLoss: 0.42959049344062805 - trainLoss: 0.4110642075538635\n",
      "cnt: 0 - valLoss: 0.42959004640579224 - trainLoss: 0.411062628030777\n",
      "cnt: 0 - valLoss: 0.42958950996398926 - trainLoss: 0.4110610783100128\n",
      "cnt: 0 - valLoss: 0.42958909273147583 - trainLoss: 0.41105952858924866\n",
      "cnt: 0 - valLoss: 0.42958858609199524 - trainLoss: 0.4110579490661621\n",
      "cnt: 0 - valLoss: 0.4295881390571594 - trainLoss: 0.41105639934539795\n",
      "cnt: 0 - valLoss: 0.4295876622200012 - trainLoss: 0.4110548198223114\n",
      "cnt: 0 - valLoss: 0.4295872151851654 - trainLoss: 0.41105329990386963\n",
      "cnt: 0 - valLoss: 0.4295867383480072 - trainLoss: 0.41105175018310547\n",
      "cnt: 0 - valLoss: 0.429586261510849 - trainLoss: 0.4110501706600189\n",
      "cnt: 0 - valLoss: 0.4295857846736908 - trainLoss: 0.41104862093925476\n",
      "cnt: 0 - valLoss: 0.4295853078365326 - trainLoss: 0.4110470414161682\n",
      "cnt: 0 - valLoss: 0.4295848608016968 - trainLoss: 0.41104549169540405\n",
      "cnt: 0 - valLoss: 0.4295843839645386 - trainLoss: 0.4110439419746399\n",
      "cnt: 0 - valLoss: 0.42958390712738037 - trainLoss: 0.41104236245155334\n",
      "cnt: 0 - valLoss: 0.42958343029022217 - trainLoss: 0.4110408425331116\n",
      "cnt: 0 - valLoss: 0.42958301305770874 - trainLoss: 0.4110392928123474\n",
      "cnt: 0 - valLoss: 0.42958253622055054 - trainLoss: 0.41103771328926086\n",
      "cnt: 0 - valLoss: 0.42958205938339233 - trainLoss: 0.4110361635684967\n",
      "cnt: 0 - valLoss: 0.42958158254623413 - trainLoss: 0.41103461384773254\n",
      "cnt: 0 - valLoss: 0.4295811057090759 - trainLoss: 0.411033034324646\n",
      "cnt: 0 - valLoss: 0.4295806884765625 - trainLoss: 0.4110315144062042\n",
      "cnt: 0 - valLoss: 0.4295801818370819 - trainLoss: 0.4110299050807953\n",
      "cnt: 0 - valLoss: 0.4295797348022461 - trainLoss: 0.4110283851623535\n",
      "cnt: 0 - valLoss: 0.4295792877674103 - trainLoss: 0.41102683544158936\n",
      "cnt: 0 - valLoss: 0.42957884073257446 - trainLoss: 0.4110252559185028\n",
      "cnt: 0 - valLoss: 0.42957836389541626 - trainLoss: 0.41102370619773865\n",
      "cnt: 0 - valLoss: 0.42957791686058044 - trainLoss: 0.4110221564769745\n",
      "cnt: 0 - valLoss: 0.42957744002342224 - trainLoss: 0.41102057695388794\n",
      "cnt: 0 - valLoss: 0.4295769929885864 - trainLoss: 0.41101905703544617\n",
      "cnt: 0 - valLoss: 0.42957648634910583 - trainLoss: 0.41101744771003723\n",
      "cnt: 0 - valLoss: 0.42957600951194763 - trainLoss: 0.41101592779159546\n",
      "cnt: 0 - valLoss: 0.4295755922794342 - trainLoss: 0.4110143780708313\n",
      "cnt: 0 - valLoss: 0.429575115442276 - trainLoss: 0.41101279854774475\n",
      "cnt: 0 - valLoss: 0.4295746982097626 - trainLoss: 0.4110112488269806\n",
      "cnt: 0 - valLoss: 0.42957422137260437 - trainLoss: 0.4110097289085388\n",
      "cnt: 0 - valLoss: 0.42957374453544617 - trainLoss: 0.41100817918777466\n",
      "cnt: 0 - valLoss: 0.4295732378959656 - trainLoss: 0.4110065996646881\n",
      "cnt: 0 - valLoss: 0.42957279086112976 - trainLoss: 0.41100502014160156\n",
      "cnt: 0 - valLoss: 0.42957234382629395 - trainLoss: 0.4110034704208374\n",
      "cnt: 0 - valLoss: 0.42957186698913574 - trainLoss: 0.41100192070007324\n",
      "cnt: 0 - valLoss: 0.4295714199542999 - trainLoss: 0.4110003709793091\n",
      "cnt: 0 - valLoss: 0.4295709729194641 - trainLoss: 0.4109988510608673\n",
      "cnt: 0 - valLoss: 0.4295704960823059 - trainLoss: 0.41099727153778076\n",
      "cnt: 0 - valLoss: 0.4295700192451477 - trainLoss: 0.4109957218170166\n",
      "cnt: 0 - valLoss: 0.4295696020126343 - trainLoss: 0.41099414229393005\n",
      "cnt: 0 - valLoss: 0.4295691251754761 - trainLoss: 0.4109925925731659\n",
      "cnt: 0 - valLoss: 0.42956864833831787 - trainLoss: 0.41099104285240173\n",
      "cnt: 0 - valLoss: 0.42956817150115967 - trainLoss: 0.4109894633293152\n",
      "cnt: 0 - valLoss: 0.42956769466400146 - trainLoss: 0.4109879434108734\n",
      "cnt: 0 - valLoss: 0.42956727743148804 - trainLoss: 0.41098639369010925\n",
      "cnt: 0 - valLoss: 0.42956680059432983 - trainLoss: 0.4109848141670227\n",
      "cnt: 0 - valLoss: 0.42956632375717163 - trainLoss: 0.41098326444625854\n",
      "cnt: 0 - valLoss: 0.4295658469200134 - trainLoss: 0.410981684923172\n",
      "cnt: 0 - valLoss: 0.4295653998851776 - trainLoss: 0.41098013520240784\n",
      "cnt: 0 - valLoss: 0.4295649528503418 - trainLoss: 0.4109785556793213\n",
      "cnt: 0 - valLoss: 0.4295644760131836 - trainLoss: 0.41097700595855713\n",
      "cnt: 0 - valLoss: 0.4295640289783478 - trainLoss: 0.41097548604011536\n",
      "cnt: 0 - valLoss: 0.42956358194351196 - trainLoss: 0.4109739363193512\n",
      "cnt: 0 - valLoss: 0.42956310510635376 - trainLoss: 0.41097238659858704\n",
      "cnt: 0 - valLoss: 0.42956259846687317 - trainLoss: 0.4109708070755005\n",
      "cnt: 0 - valLoss: 0.42956218123435974 - trainLoss: 0.41096922755241394\n",
      "cnt: 0 - valLoss: 0.42956170439720154 - trainLoss: 0.4109676778316498\n",
      "cnt: 0 - valLoss: 0.42956122756004333 - trainLoss: 0.4109661877155304\n",
      "cnt: 0 - valLoss: 0.4295608103275299 - trainLoss: 0.4109645485877991\n",
      "cnt: 0 - valLoss: 0.4295603334903717 - trainLoss: 0.4109630584716797\n",
      "cnt: 0 - valLoss: 0.4295598566532135 - trainLoss: 0.4109615087509155\n",
      "cnt: 0 - valLoss: 0.4295594394207001 - trainLoss: 0.41095998883247375\n",
      "cnt: 0 - valLoss: 0.42955896258354187 - trainLoss: 0.41095834970474243\n",
      "cnt: 0 - valLoss: 0.42955848574638367 - trainLoss: 0.41095679998397827\n",
      "cnt: 0 - valLoss: 0.42955800890922546 - trainLoss: 0.4109552800655365\n",
      "cnt: 0 - valLoss: 0.42955753207206726 - trainLoss: 0.41095373034477234\n",
      "cnt: 0 - valLoss: 0.42955711483955383 - trainLoss: 0.4109521806240082\n",
      "cnt: 0 - valLoss: 0.42955663800239563 - trainLoss: 0.41095060110092163\n",
      "cnt: 0 - valLoss: 0.4295561611652374 - trainLoss: 0.41094905138015747\n",
      "cnt: 0 - valLoss: 0.429555743932724 - trainLoss: 0.4109474718570709\n",
      "cnt: 0 - valLoss: 0.4295552372932434 - trainLoss: 0.41094595193862915\n",
      "cnt: 0 - valLoss: 0.4295547902584076 - trainLoss: 0.410944402217865\n",
      "cnt: 0 - valLoss: 0.4295543432235718 - trainLoss: 0.41094285249710083\n",
      "cnt: 0 - valLoss: 0.4295538663864136 - trainLoss: 0.41094133257865906\n",
      "cnt: 0 - valLoss: 0.42955341935157776 - trainLoss: 0.4109397232532501\n",
      "cnt: 0 - valLoss: 0.42955294251441956 - trainLoss: 0.41093820333480835\n",
      "cnt: 0 - valLoss: 0.42955249547958374 - trainLoss: 0.4109366238117218\n",
      "cnt: 0 - valLoss: 0.4295520484447479 - trainLoss: 0.41093507409095764\n",
      "cnt: 0 - valLoss: 0.42955154180526733 - trainLoss: 0.4109335243701935\n",
      "cnt: 0 - valLoss: 0.4295511245727539 - trainLoss: 0.4109319746494293\n",
      "cnt: 0 - valLoss: 0.4295506775379181 - trainLoss: 0.4109303951263428\n",
      "cnt: 0 - valLoss: 0.4295502007007599 - trainLoss: 0.4109288156032562\n",
      "cnt: 0 - valLoss: 0.4295497536659241 - trainLoss: 0.41092732548713684\n",
      "cnt: 0 - valLoss: 0.42954927682876587 - trainLoss: 0.4109257757663727\n",
      "cnt: 0 - valLoss: 0.42954882979393005 - trainLoss: 0.41092419624328613\n",
      "cnt: 0 - valLoss: 0.42954838275909424 - trainLoss: 0.410922646522522\n",
      "cnt: 0 - valLoss: 0.42954784631729126 - trainLoss: 0.4109211266040802\n",
      "cnt: 0 - valLoss: 0.42954742908477783 - trainLoss: 0.41091957688331604\n",
      "cnt: 0 - valLoss: 0.42954695224761963 - trainLoss: 0.4109179973602295\n",
      "cnt: 0 - valLoss: 0.4295465350151062 - trainLoss: 0.41091641783714294\n",
      "cnt: 0 - valLoss: 0.429546058177948 - trainLoss: 0.41091489791870117\n",
      "cnt: 0 - valLoss: 0.42954564094543457 - trainLoss: 0.4109133183956146\n",
      "cnt: 0 - valLoss: 0.42954516410827637 - trainLoss: 0.41091179847717285\n",
      "cnt: 0 - valLoss: 0.42954468727111816 - trainLoss: 0.4109102487564087\n",
      "cnt: 0 - valLoss: 0.42954427003860474 - trainLoss: 0.41090869903564453\n",
      "cnt: 0 - valLoss: 0.4295438230037689 - trainLoss: 0.41090714931488037\n",
      "cnt: 0 - valLoss: 0.4295433461666107 - trainLoss: 0.4109055697917938\n",
      "cnt: 0 - valLoss: 0.4295428991317749 - trainLoss: 0.41090404987335205\n",
      "cnt: 0 - valLoss: 0.4295424520969391 - trainLoss: 0.4109025001525879\n",
      "cnt: 0 - valLoss: 0.4295419752597809 - trainLoss: 0.41090092062950134\n",
      "cnt: 0 - valLoss: 0.4295414984226227 - trainLoss: 0.4108993709087372\n",
      "cnt: 0 - valLoss: 0.42954108119010925 - trainLoss: 0.410897821187973\n",
      "cnt: 0 - valLoss: 0.42954060435295105 - trainLoss: 0.41089630126953125\n",
      "cnt: 0 - valLoss: 0.42954012751579285 - trainLoss: 0.4108947217464447\n",
      "cnt: 0 - valLoss: 0.42953968048095703 - trainLoss: 0.41089311242103577\n",
      "cnt: 0 - valLoss: 0.42953920364379883 - trainLoss: 0.410891592502594\n",
      "cnt: 0 - valLoss: 0.429538756608963 - trainLoss: 0.41089004278182983\n",
      "cnt: 0 - valLoss: 0.4295383095741272 - trainLoss: 0.4108884930610657\n",
      "cnt: 0 - valLoss: 0.4295378625392914 - trainLoss: 0.4108869731426239\n",
      "cnt: 0 - valLoss: 0.4295373857021332 - trainLoss: 0.41088542342185974\n",
      "cnt: 0 - valLoss: 0.42953696846961975 - trainLoss: 0.4108838737010956\n",
      "cnt: 0 - valLoss: 0.42953649163246155 - trainLoss: 0.41088229417800903\n",
      "cnt: 0 - valLoss: 0.42953601479530334 - trainLoss: 0.41088080406188965\n",
      "cnt: 0 - valLoss: 0.4295355975627899 - trainLoss: 0.4108792245388031\n",
      "cnt: 0 - valLoss: 0.4295351207256317 - trainLoss: 0.41087761521339417\n",
      "cnt: 0 - valLoss: 0.4295346438884735 - trainLoss: 0.4108760952949524\n",
      "cnt: 0 - valLoss: 0.4295342266559601 - trainLoss: 0.41087454557418823\n",
      "cnt: 0 - valLoss: 0.4295337498188019 - trainLoss: 0.4108729958534241\n",
      "cnt: 0 - valLoss: 0.42953333258628845 - trainLoss: 0.4108714759349823\n",
      "cnt: 0 - valLoss: 0.42953285574913025 - trainLoss: 0.41086992621421814\n",
      "cnt: 0 - valLoss: 0.42953237891197205 - trainLoss: 0.4108683466911316\n",
      "cnt: 0 - valLoss: 0.42953190207481384 - trainLoss: 0.41086679697036743\n",
      "cnt: 0 - valLoss: 0.4295314848423004 - trainLoss: 0.41086524724960327\n",
      "cnt: 0 - valLoss: 0.4295310080051422 - trainLoss: 0.4108636677265167\n",
      "cnt: 0 - valLoss: 0.4295305907726288 - trainLoss: 0.41086214780807495\n",
      "cnt: 0 - valLoss: 0.4295301139354706 - trainLoss: 0.4108605980873108\n",
      "cnt: 0 - valLoss: 0.42952969670295715 - trainLoss: 0.41085904836654663\n",
      "cnt: 0 - valLoss: 0.4295291602611542 - trainLoss: 0.41085749864578247\n",
      "cnt: 0 - valLoss: 0.42952874302864075 - trainLoss: 0.4108559191226959\n",
      "cnt: 0 - valLoss: 0.4295283257961273 - trainLoss: 0.41085439920425415\n",
      "cnt: 0 - valLoss: 0.42952778935432434 - trainLoss: 0.41085284948349\n",
      "cnt: 0 - valLoss: 0.4295273721218109 - trainLoss: 0.41085129976272583\n",
      "cnt: 0 - valLoss: 0.4295268952846527 - trainLoss: 0.4108497202396393\n",
      "cnt: 0 - valLoss: 0.4295264780521393 - trainLoss: 0.4108481705188751\n",
      "cnt: 0 - valLoss: 0.4295260012149811 - trainLoss: 0.41084665060043335\n",
      "cnt: 0 - valLoss: 0.4295255243778229 - trainLoss: 0.4108451008796692\n",
      "cnt: 0 - valLoss: 0.42952510714530945 - trainLoss: 0.41084355115890503\n",
      "cnt: 0 - valLoss: 0.42952463030815125 - trainLoss: 0.41084203124046326\n",
      "cnt: 0 - valLoss: 0.42952418327331543 - trainLoss: 0.4108404815196991\n",
      "cnt: 0 - valLoss: 0.4295237362384796 - trainLoss: 0.41083890199661255\n",
      "cnt: 0 - valLoss: 0.4295232892036438 - trainLoss: 0.4108373522758484\n",
      "cnt: 0 - valLoss: 0.4295228123664856 - trainLoss: 0.41083580255508423\n",
      "cnt: 0 - valLoss: 0.42952239513397217 - trainLoss: 0.4108342230319977\n",
      "cnt: 0 - valLoss: 0.42952191829681396 - trainLoss: 0.4108326733112335\n",
      "cnt: 0 - valLoss: 0.42952144145965576 - trainLoss: 0.41083115339279175\n",
      "cnt: 0 - valLoss: 0.42952099442481995 - trainLoss: 0.4108296036720276\n",
      "cnt: 0 - valLoss: 0.42952051758766174 - trainLoss: 0.41082802414894104\n",
      "cnt: 0 - valLoss: 0.4295200705528259 - trainLoss: 0.4108264744281769\n",
      "cnt: 0 - valLoss: 0.4295196235179901 - trainLoss: 0.4108249545097351\n",
      "cnt: 0 - valLoss: 0.4295191764831543 - trainLoss: 0.41082340478897095\n",
      "cnt: 0 - valLoss: 0.4295186996459961 - trainLoss: 0.4108218550682068\n",
      "cnt: 0 - valLoss: 0.42951828241348267 - trainLoss: 0.41082027554512024\n",
      "cnt: 0 - valLoss: 0.42951780557632446 - trainLoss: 0.4108187258243561\n",
      "cnt: 0 - valLoss: 0.42951732873916626 - trainLoss: 0.4108172059059143\n",
      "cnt: 0 - valLoss: 0.42951691150665283 - trainLoss: 0.41081565618515015\n",
      "cnt: 0 - valLoss: 0.42951643466949463 - trainLoss: 0.410814106464386\n",
      "cnt: 0 - valLoss: 0.4295159578323364 - trainLoss: 0.4108125567436218\n",
      "cnt: 0 - valLoss: 0.429515540599823 - trainLoss: 0.4108109772205353\n",
      "cnt: 0 - valLoss: 0.42951512336730957 - trainLoss: 0.41080939769744873\n",
      "cnt: 0 - valLoss: 0.42951464653015137 - trainLoss: 0.41080784797668457\n",
      "cnt: 0 - valLoss: 0.42951416969299316 - trainLoss: 0.4108063280582428\n",
      "cnt: 0 - valLoss: 0.42951375246047974 - trainLoss: 0.41080477833747864\n",
      "cnt: 0 - valLoss: 0.42951327562332153 - trainLoss: 0.4108032286167145\n",
      "cnt: 0 - valLoss: 0.4295128285884857 - trainLoss: 0.4108017086982727\n",
      "cnt: 0 - valLoss: 0.4295123815536499 - trainLoss: 0.41080015897750854\n",
      "cnt: 0 - valLoss: 0.4295119047164917 - trainLoss: 0.410798579454422\n",
      "cnt: 0 - valLoss: 0.4295114278793335 - trainLoss: 0.41079702973365784\n",
      "cnt: 0 - valLoss: 0.4295109808444977 - trainLoss: 0.4107954502105713\n",
      "cnt: 0 - valLoss: 0.42951056361198425 - trainLoss: 0.4107939600944519\n",
      "cnt: 0 - valLoss: 0.42951008677482605 - trainLoss: 0.41079238057136536\n",
      "cnt: 0 - valLoss: 0.4295096695423126 - trainLoss: 0.4107908308506012\n",
      "cnt: 0 - valLoss: 0.4295091927051544 - trainLoss: 0.41078928112983704\n",
      "cnt: 0 - valLoss: 0.429508775472641 - trainLoss: 0.4107877314090729\n",
      "cnt: 0 - valLoss: 0.429508239030838 - trainLoss: 0.4107862114906311\n",
      "cnt: 0 - valLoss: 0.429507851600647 - trainLoss: 0.41078463196754456\n",
      "cnt: 0 - valLoss: 0.42950740456581116 - trainLoss: 0.4107831120491028\n",
      "cnt: 0 - valLoss: 0.42950695753097534 - trainLoss: 0.410781592130661\n",
      "cnt: 0 - valLoss: 0.42950648069381714 - trainLoss: 0.41078001260757446\n",
      "cnt: 0 - valLoss: 0.4295060336589813 - trainLoss: 0.4107784628868103\n",
      "cnt: 0 - valLoss: 0.4295055866241455 - trainLoss: 0.41077688336372375\n",
      "cnt: 0 - valLoss: 0.4295051097869873 - trainLoss: 0.4107753336429596\n",
      "cnt: 0 - valLoss: 0.4295046925544739 - trainLoss: 0.41077378392219543\n",
      "cnt: 0 - valLoss: 0.4295042157173157 - trainLoss: 0.41077226400375366\n",
      "cnt: 0 - valLoss: 0.42950379848480225 - trainLoss: 0.4107707142829895\n",
      "cnt: 0 - valLoss: 0.42950335144996643 - trainLoss: 0.41076916456222534\n",
      "cnt: 0 - valLoss: 0.4295029044151306 - trainLoss: 0.4107676148414612\n",
      "cnt: 0 - valLoss: 0.4295024275779724 - trainLoss: 0.4107660949230194\n",
      "cnt: 0 - valLoss: 0.4295019805431366 - trainLoss: 0.41076454520225525\n",
      "cnt: 0 - valLoss: 0.42950156331062317 - trainLoss: 0.4107629954814911\n",
      "cnt: 0 - valLoss: 0.42950108647346497 - trainLoss: 0.41076141595840454\n",
      "cnt: 0 - valLoss: 0.42950060963630676 - trainLoss: 0.41075989603996277\n",
      "cnt: 0 - valLoss: 0.42950019240379333 - trainLoss: 0.4107583463191986\n",
      "cnt: 0 - valLoss: 0.4294997751712799 - trainLoss: 0.41075676679611206\n",
      "cnt: 0 - valLoss: 0.42949923872947693 - trainLoss: 0.4107552170753479\n",
      "cnt: 0 - valLoss: 0.4294988214969635 - trainLoss: 0.41075366735458374\n",
      "cnt: 0 - valLoss: 0.4294984042644501 - trainLoss: 0.41075214743614197\n",
      "cnt: 0 - valLoss: 0.42949795722961426 - trainLoss: 0.4107505977153778\n",
      "cnt: 0 - valLoss: 0.42949748039245605 - trainLoss: 0.41074904799461365\n",
      "cnt: 0 - valLoss: 0.4294970631599426 - trainLoss: 0.4107474982738495\n",
      "cnt: 0 - valLoss: 0.4294965863227844 - trainLoss: 0.4107459783554077\n",
      "cnt: 0 - valLoss: 0.4294961988925934 - trainLoss: 0.41074442863464355\n",
      "cnt: 0 - valLoss: 0.4294957220554352 - trainLoss: 0.4107428789138794\n",
      "cnt: 0 - valLoss: 0.42949530482292175 - trainLoss: 0.4107413589954376\n",
      "cnt: 0 - valLoss: 0.42949482798576355 - trainLoss: 0.4107397794723511\n",
      "cnt: 0 - valLoss: 0.4294944107532501 - trainLoss: 0.4107382297515869\n",
      "cnt: 0 - valLoss: 0.4294939339160919 - trainLoss: 0.41073668003082275\n",
      "cnt: 0 - valLoss: 0.4294935166835785 - trainLoss: 0.4107351303100586\n",
      "cnt: 0 - valLoss: 0.4294930696487427 - trainLoss: 0.4107336103916168\n",
      "cnt: 0 - valLoss: 0.42949265241622925 - trainLoss: 0.4107320308685303\n",
      "cnt: 0 - valLoss: 0.4294922351837158 - trainLoss: 0.4107305109500885\n",
      "cnt: 0 - valLoss: 0.42949178814888 - trainLoss: 0.41072899103164673\n",
      "cnt: 0 - valLoss: 0.4294912815093994 - trainLoss: 0.4107273817062378\n",
      "cnt: 0 - valLoss: 0.429490864276886 - trainLoss: 0.410725861787796\n",
      "cnt: 0 - valLoss: 0.42949041724205017 - trainLoss: 0.41072431206703186\n",
      "cnt: 0 - valLoss: 0.42949000000953674 - trainLoss: 0.4107227623462677\n",
      "cnt: 0 - valLoss: 0.42948952317237854 - trainLoss: 0.4107212424278259\n",
      "cnt: 0 - valLoss: 0.4294891357421875 - trainLoss: 0.41071969270706177\n",
      "cnt: 0 - valLoss: 0.4294886589050293 - trainLoss: 0.4107181429862976\n",
      "cnt: 0 - valLoss: 0.4294881820678711 - trainLoss: 0.41071662306785583\n",
      "cnt: 0 - valLoss: 0.42948776483535767 - trainLoss: 0.4107150733470917\n",
      "cnt: 0 - valLoss: 0.42948728799819946 - trainLoss: 0.4107135236263275\n",
      "cnt: 0 - valLoss: 0.4294869005680084 - trainLoss: 0.41071197390556335\n",
      "cnt: 0 - valLoss: 0.42948639392852783 - trainLoss: 0.4107104539871216\n",
      "cnt: 0 - valLoss: 0.4294859766960144 - trainLoss: 0.41070887446403503\n",
      "cnt: 0 - valLoss: 0.4294855296611786 - trainLoss: 0.4107073247432709\n",
      "cnt: 0 - valLoss: 0.42948511242866516 - trainLoss: 0.4107057750225067\n",
      "cnt: 0 - valLoss: 0.42948463559150696 - trainLoss: 0.41070422530174255\n",
      "cnt: 0 - valLoss: 0.42948421835899353 - trainLoss: 0.4107027053833008\n",
      "cnt: 0 - valLoss: 0.4294837415218353 - trainLoss: 0.4107012152671814\n",
      "cnt: 0 - valLoss: 0.4294832944869995 - trainLoss: 0.41069966554641724\n",
      "cnt: 0 - valLoss: 0.4294828772544861 - trainLoss: 0.4106980264186859\n",
      "cnt: 0 - valLoss: 0.42948246002197266 - trainLoss: 0.41069653630256653\n",
      "cnt: 0 - valLoss: 0.42948198318481445 - trainLoss: 0.41069498658180237\n",
      "cnt: 0 - valLoss: 0.42948153614997864 - trainLoss: 0.4106934666633606\n",
      "cnt: 0 - valLoss: 0.4294810891151428 - trainLoss: 0.41069185733795166\n",
      "cnt: 0 - valLoss: 0.429480642080307 - trainLoss: 0.4106903374195099\n",
      "cnt: 0 - valLoss: 0.4294802248477936 - trainLoss: 0.4106887876987457\n",
      "cnt: 0 - valLoss: 0.4294797480106354 - trainLoss: 0.41068723797798157\n",
      "cnt: 0 - valLoss: 0.42947933077812195 - trainLoss: 0.4106857180595398\n",
      "cnt: 0 - valLoss: 0.42947885394096375 - trainLoss: 0.41068416833877563\n",
      "cnt: 0 - valLoss: 0.42947840690612793 - trainLoss: 0.4106826186180115\n",
      "cnt: 0 - valLoss: 0.4294779896736145 - trainLoss: 0.4106810986995697\n",
      "cnt: 0 - valLoss: 0.4294775724411011 - trainLoss: 0.41067954897880554\n",
      "cnt: 0 - valLoss: 0.42947709560394287 - trainLoss: 0.4106779992580414\n",
      "cnt: 0 - valLoss: 0.42947664856910706 - trainLoss: 0.4106764495372772\n",
      "cnt: 0 - valLoss: 0.42947620153427124 - trainLoss: 0.41067492961883545\n",
      "cnt: 0 - valLoss: 0.4294757544994354 - trainLoss: 0.4106733798980713\n",
      "cnt: 0 - valLoss: 0.429475337266922 - trainLoss: 0.41067183017730713\n",
      "cnt: 0 - valLoss: 0.4294748604297638 - trainLoss: 0.41067031025886536\n",
      "cnt: 0 - valLoss: 0.42947444319725037 - trainLoss: 0.4106687605381012\n",
      "cnt: 0 - valLoss: 0.42947396636009216 - trainLoss: 0.41066721081733704\n",
      "cnt: 0 - valLoss: 0.42947351932525635 - trainLoss: 0.4106656312942505\n",
      "cnt: 0 - valLoss: 0.4294731020927429 - trainLoss: 0.41066408157348633\n",
      "cnt: 0 - valLoss: 0.4294726252555847 - trainLoss: 0.41066256165504456\n",
      "cnt: 0 - valLoss: 0.4294722080230713 - trainLoss: 0.4106610119342804\n",
      "cnt: 0 - valLoss: 0.42947179079055786 - trainLoss: 0.410659521818161\n",
      "cnt: 0 - valLoss: 0.42947131395339966 - trainLoss: 0.41065794229507446\n",
      "cnt: 0 - valLoss: 0.42947086691856384 - trainLoss: 0.4106563925743103\n",
      "cnt: 0 - valLoss: 0.4294704496860504 - trainLoss: 0.41065484285354614\n",
      "cnt: 0 - valLoss: 0.429470032453537 - trainLoss: 0.410653293132782\n",
      "cnt: 0 - valLoss: 0.4294695556163788 - trainLoss: 0.41065171360969543\n",
      "cnt: 0 - valLoss: 0.4294690787792206 - trainLoss: 0.41065019369125366\n",
      "cnt: 0 - valLoss: 0.42946866154670715 - trainLoss: 0.4106486439704895\n",
      "cnt: 0 - valLoss: 0.42946821451187134 - trainLoss: 0.41064709424972534\n",
      "cnt: 0 - valLoss: 0.4294677972793579 - trainLoss: 0.41064560413360596\n",
      "cnt: 0 - valLoss: 0.4294673204421997 - trainLoss: 0.4106440246105194\n",
      "cnt: 0 - valLoss: 0.4294668436050415 - trainLoss: 0.41064247488975525\n",
      "cnt: 0 - valLoss: 0.4294664263725281 - trainLoss: 0.4106409251689911\n",
      "cnt: 0 - valLoss: 0.42946597933769226 - trainLoss: 0.4106394052505493\n",
      "cnt: 0 - valLoss: 0.42946556210517883 - trainLoss: 0.41063785552978516\n",
      "cnt: 0 - valLoss: 0.4294651448726654 - trainLoss: 0.410636305809021\n",
      "cnt: 0 - valLoss: 0.4294646680355072 - trainLoss: 0.4106347858905792\n",
      "cnt: 0 - valLoss: 0.429464191198349 - trainLoss: 0.41063323616981506\n",
      "cnt: 0 - valLoss: 0.42946377396583557 - trainLoss: 0.4106316864490509\n",
      "cnt: 0 - valLoss: 0.42946332693099976 - trainLoss: 0.41063013672828674\n",
      "cnt: 0 - valLoss: 0.42946290969848633 - trainLoss: 0.41062861680984497\n",
      "cnt: 0 - valLoss: 0.4294624626636505 - trainLoss: 0.4106270670890808\n",
      "cnt: 0 - valLoss: 0.4294620156288147 - trainLoss: 0.41062551736831665\n",
      "cnt: 0 - valLoss: 0.4294615685939789 - trainLoss: 0.4106239974498749\n",
      "cnt: 0 - valLoss: 0.42946115136146545 - trainLoss: 0.4106224477291107\n",
      "cnt: 0 - valLoss: 0.42946067452430725 - trainLoss: 0.41062089800834656\n",
      "cnt: 0 - valLoss: 0.4294602572917938 - trainLoss: 0.4106193780899048\n",
      "cnt: 0 - valLoss: 0.4294597804546356 - trainLoss: 0.4106178283691406\n",
      "cnt: 0 - valLoss: 0.4294593334197998 - trainLoss: 0.41061627864837646\n",
      "cnt: 0 - valLoss: 0.4294589161872864 - trainLoss: 0.4106147289276123\n",
      "cnt: 0 - valLoss: 0.4294584393501282 - trainLoss: 0.41061314940452576\n",
      "cnt: 0 - valLoss: 0.42945802211761475 - trainLoss: 0.41061165928840637\n",
      "cnt: 0 - valLoss: 0.42945757508277893 - trainLoss: 0.4106101095676422\n",
      "cnt: 0 - valLoss: 0.4294571578502655 - trainLoss: 0.41060858964920044\n",
      "cnt: 0 - valLoss: 0.4294567406177521 - trainLoss: 0.4106070399284363\n",
      "cnt: 0 - valLoss: 0.42945626378059387 - trainLoss: 0.4106054902076721\n",
      "cnt: 0 - valLoss: 0.42945578694343567 - trainLoss: 0.41060394048690796\n",
      "cnt: 0 - valLoss: 0.42945539951324463 - trainLoss: 0.4106024205684662\n",
      "cnt: 0 - valLoss: 0.4294549226760864 - trainLoss: 0.410600870847702\n",
      "cnt: 0 - valLoss: 0.429454505443573 - trainLoss: 0.4105992913246155\n",
      "cnt: 0 - valLoss: 0.4294540286064148 - trainLoss: 0.4105978012084961\n",
      "cnt: 0 - valLoss: 0.42945364117622375 - trainLoss: 0.41059622168540955\n",
      "cnt: 0 - valLoss: 0.42945316433906555 - trainLoss: 0.4105946719646454\n",
      "cnt: 0 - valLoss: 0.4294527471065521 - trainLoss: 0.4105931222438812\n",
      "cnt: 0 - valLoss: 0.4294523298740387 - trainLoss: 0.41059163212776184\n",
      "cnt: 0 - valLoss: 0.4294518530368805 - trainLoss: 0.4105900526046753\n",
      "cnt: 0 - valLoss: 0.4294514060020447 - trainLoss: 0.41058850288391113\n",
      "cnt: 0 - valLoss: 0.42945098876953125 - trainLoss: 0.410586953163147\n",
      "cnt: 0 - valLoss: 0.42945051193237305 - trainLoss: 0.4105854630470276\n",
      "cnt: 0 - valLoss: 0.429450124502182 - trainLoss: 0.4105839133262634\n",
      "cnt: 0 - valLoss: 0.4294496178627014 - trainLoss: 0.4105823338031769\n",
      "cnt: 0 - valLoss: 0.4294492304325104 - trainLoss: 0.4105808138847351\n",
      "cnt: 0 - valLoss: 0.42944881319999695 - trainLoss: 0.41057926416397095\n",
      "cnt: 0 - valLoss: 0.42944833636283875 - trainLoss: 0.4105777144432068\n",
      "cnt: 0 - valLoss: 0.42944788932800293 - trainLoss: 0.4105762243270874\n",
      "cnt: 0 - valLoss: 0.4294474720954895 - trainLoss: 0.41057464480400085\n",
      "cnt: 0 - valLoss: 0.4294470548629761 - trainLoss: 0.4105730950832367\n",
      "cnt: 0 - valLoss: 0.42944657802581787 - trainLoss: 0.41057154536247253\n",
      "cnt: 0 - valLoss: 0.42944613099098206 - trainLoss: 0.41057002544403076\n",
      "cnt: 0 - valLoss: 0.42944568395614624 - trainLoss: 0.410568505525589\n",
      "cnt: 0 - valLoss: 0.4294452369213104 - trainLoss: 0.4105669856071472\n",
      "cnt: 0 - valLoss: 0.429444819688797 - trainLoss: 0.4105653762817383\n",
      "cnt: 0 - valLoss: 0.4294443428516388 - trainLoss: 0.4105638861656189\n",
      "cnt: 0 - valLoss: 0.42944392561912537 - trainLoss: 0.41056230664253235\n",
      "cnt: 0 - valLoss: 0.4294435381889343 - trainLoss: 0.4105607569217682\n",
      "cnt: 0 - valLoss: 0.4294430613517761 - trainLoss: 0.4105592668056488\n",
      "cnt: 0 - valLoss: 0.4294425845146179 - trainLoss: 0.41055768728256226\n",
      "cnt: 0 - valLoss: 0.4294421672821045 - trainLoss: 0.4105561375617981\n",
      "cnt: 0 - valLoss: 0.4294417202472687 - trainLoss: 0.4105546474456787\n",
      "cnt: 0 - valLoss: 0.42944127321243286 - trainLoss: 0.41055309772491455\n",
      "cnt: 0 - valLoss: 0.42944082617759705 - trainLoss: 0.4105515778064728\n",
      "cnt: 0 - valLoss: 0.42944034934043884 - trainLoss: 0.4105500280857086\n",
      "cnt: 0 - valLoss: 0.4294399321079254 - trainLoss: 0.41054847836494446\n",
      "cnt: 0 - valLoss: 0.4294394552707672 - trainLoss: 0.4105468988418579\n",
      "cnt: 0 - valLoss: 0.4294390380382538 - trainLoss: 0.4105454087257385\n",
      "cnt: 0 - valLoss: 0.42943865060806274 - trainLoss: 0.41054385900497437\n",
      "cnt: 0 - valLoss: 0.42943817377090454 - trainLoss: 0.4105423092842102\n",
      "cnt: 0 - valLoss: 0.42943769693374634 - trainLoss: 0.41054078936576843\n",
      "cnt: 0 - valLoss: 0.4294372797012329 - trainLoss: 0.4105392396450043\n",
      "cnt: 0 - valLoss: 0.4294368028640747 - trainLoss: 0.4105376601219177\n",
      "cnt: 0 - valLoss: 0.4294363856315613 - trainLoss: 0.41053617000579834\n",
      "cnt: 0 - valLoss: 0.42943599820137024 - trainLoss: 0.4105346202850342\n",
      "cnt: 0 - valLoss: 0.42943552136421204 - trainLoss: 0.41053307056427\n",
      "cnt: 0 - valLoss: 0.42943504452705383 - trainLoss: 0.41053152084350586\n",
      "cnt: 0 - valLoss: 0.4294346272945404 - trainLoss: 0.4105300009250641\n",
      "cnt: 0 - valLoss: 0.4294341802597046 - trainLoss: 0.4105284512042999\n",
      "cnt: 0 - valLoss: 0.4294337034225464 - trainLoss: 0.41052690148353577\n",
      "cnt: 0 - valLoss: 0.42943328619003296 - trainLoss: 0.410525381565094\n",
      "cnt: 0 - valLoss: 0.42943286895751953 - trainLoss: 0.41052383184432983\n",
      "cnt: 0 - valLoss: 0.42943239212036133 - trainLoss: 0.4105222821235657\n",
      "cnt: 0 - valLoss: 0.4294319450855255 - trainLoss: 0.4105207622051239\n",
      "cnt: 0 - valLoss: 0.4294314980506897 - trainLoss: 0.41051921248435974\n",
      "cnt: 0 - valLoss: 0.4294310510158539 - trainLoss: 0.4105176627635956\n",
      "cnt: 0 - valLoss: 0.42943063378334045 - trainLoss: 0.4105161130428314\n",
      "cnt: 0 - valLoss: 0.42943015694618225 - trainLoss: 0.41051459312438965\n",
      "cnt: 0 - valLoss: 0.4294297397136688 - trainLoss: 0.4105130434036255\n",
      "cnt: 0 - valLoss: 0.429429292678833 - trainLoss: 0.41051149368286133\n",
      "cnt: 0 - valLoss: 0.4294288754463196 - trainLoss: 0.41050997376441956\n",
      "cnt: 0 - valLoss: 0.4294283092021942 - trainLoss: 0.4105084240436554\n",
      "cnt: 0 - valLoss: 0.4294278025627136 - trainLoss: 0.41050687432289124\n",
      "cnt: 0 - valLoss: 0.42942723631858826 - trainLoss: 0.41050535440444946\n",
      "cnt: 0 - valLoss: 0.42942675948143005 - trainLoss: 0.4105037450790405\n",
      "cnt: 0 - valLoss: 0.4294262230396271 - trainLoss: 0.41050225496292114\n",
      "cnt: 0 - valLoss: 0.4294256865978241 - trainLoss: 0.4105006754398346\n",
      "cnt: 0 - valLoss: 0.4294251799583435 - trainLoss: 0.41049912571907043\n",
      "cnt: 0 - valLoss: 0.4294246733188629 - trainLoss: 0.4104975461959839\n",
      "cnt: 0 - valLoss: 0.42942410707473755 - trainLoss: 0.4104959964752197\n",
      "cnt: 0 - valLoss: 0.42942363023757935 - trainLoss: 0.41049447655677795\n",
      "cnt: 0 - valLoss: 0.42942309379577637 - trainLoss: 0.4104929268360138\n",
      "cnt: 0 - valLoss: 0.429422527551651 - trainLoss: 0.41049134731292725\n",
      "cnt: 0 - valLoss: 0.4294220209121704 - trainLoss: 0.4104897975921631\n",
      "cnt: 0 - valLoss: 0.4294215142726898 - trainLoss: 0.41048821806907654\n",
      "cnt: 0 - valLoss: 0.42942097783088684 - trainLoss: 0.4104866683483124\n",
      "cnt: 0 - valLoss: 0.42942047119140625 - trainLoss: 0.4104851484298706\n",
      "cnt: 0 - valLoss: 0.42941996455192566 - trainLoss: 0.41048359870910645\n",
      "cnt: 0 - valLoss: 0.4294194281101227 - trainLoss: 0.4104820489883423\n",
      "cnt: 0 - valLoss: 0.4294189214706421 - trainLoss: 0.41048046946525574\n",
      "cnt: 0 - valLoss: 0.4294184148311615 - trainLoss: 0.4104789197444916\n",
      "cnt: 0 - valLoss: 0.42941781878471375 - trainLoss: 0.41047734022140503\n",
      "cnt: 0 - valLoss: 0.42941734194755554 - trainLoss: 0.41047582030296326\n",
      "cnt: 0 - valLoss: 0.4294167757034302 - trainLoss: 0.4104742705821991\n",
      "cnt: 0 - valLoss: 0.4294162690639496 - trainLoss: 0.41047272086143494\n",
      "cnt: 0 - valLoss: 0.4294157922267914 - trainLoss: 0.4104711413383484\n",
      "cnt: 0 - valLoss: 0.4294152557849884 - trainLoss: 0.41046956181526184\n",
      "cnt: 0 - valLoss: 0.4294147193431854 - trainLoss: 0.41046807169914246\n",
      "cnt: 0 - valLoss: 0.42941421270370483 - trainLoss: 0.4104664623737335\n",
      "cnt: 0 - valLoss: 0.42941370606422424 - trainLoss: 0.41046494245529175\n",
      "cnt: 0 - valLoss: 0.42941319942474365 - trainLoss: 0.4104633927345276\n",
      "cnt: 0 - valLoss: 0.4294126629829407 - trainLoss: 0.4104618430137634\n",
      "cnt: 0 - valLoss: 0.4294121563434601 - trainLoss: 0.4104602634906769\n",
      "cnt: 0 - valLoss: 0.4294116497039795 - trainLoss: 0.41045868396759033\n",
      "cnt: 0 - valLoss: 0.4294111132621765 - trainLoss: 0.41045719385147095\n",
      "cnt: 0 - valLoss: 0.4294106066226959 - trainLoss: 0.4104556143283844\n",
      "cnt: 0 - valLoss: 0.42941009998321533 - trainLoss: 0.41045406460762024\n",
      "cnt: 0 - valLoss: 0.42940959334373474 - trainLoss: 0.4104525148868561\n",
      "cnt: 0 - valLoss: 0.42940905690193176 - trainLoss: 0.4104509949684143\n",
      "cnt: 0 - valLoss: 0.42940855026245117 - trainLoss: 0.41044938564300537\n",
      "cnt: 0 - valLoss: 0.4294080436229706 - trainLoss: 0.4104478657245636\n",
      "cnt: 0 - valLoss: 0.42940753698349 - trainLoss: 0.41044631600379944\n",
      "cnt: 0 - valLoss: 0.429407000541687 - trainLoss: 0.4104447662830353\n",
      "cnt: 0 - valLoss: 0.4294064939022064 - trainLoss: 0.41044318675994873\n",
      "cnt: 0 - valLoss: 0.42940598726272583 - trainLoss: 0.41044163703918457\n",
      "cnt: 0 - valLoss: 0.42940548062324524 - trainLoss: 0.4104401171207428\n",
      "cnt: 0 - valLoss: 0.42940494418144226 - trainLoss: 0.41043856739997864\n",
      "cnt: 0 - valLoss: 0.42940443754196167 - trainLoss: 0.4104369878768921\n",
      "cnt: 0 - valLoss: 0.4294039309024811 - trainLoss: 0.41043543815612793\n",
      "cnt: 0 - valLoss: 0.4294034242630005 - trainLoss: 0.41043391823768616\n",
      "cnt: 0 - valLoss: 0.4294028878211975 - trainLoss: 0.4104323089122772\n",
      "cnt: 0 - valLoss: 0.4294024407863617 - trainLoss: 0.41043078899383545\n",
      "cnt: 0 - valLoss: 0.4294019043445587 - trainLoss: 0.4104292392730713\n",
      "cnt: 0 - valLoss: 0.4294013977050781 - trainLoss: 0.41042768955230713\n",
      "cnt: 0 - valLoss: 0.42940089106559753 - trainLoss: 0.41042616963386536\n",
      "cnt: 0 - valLoss: 0.42940035462379456 - trainLoss: 0.4104245603084564\n",
      "cnt: 0 - valLoss: 0.42939990758895874 - trainLoss: 0.41042304039001465\n",
      "cnt: 0 - valLoss: 0.4293993413448334 - trainLoss: 0.4104214608669281\n",
      "cnt: 0 - valLoss: 0.42939886450767517 - trainLoss: 0.41041991114616394\n",
      "cnt: 0 - valLoss: 0.4293983578681946 - trainLoss: 0.4104183614253998\n",
      "cnt: 0 - valLoss: 0.429397851228714 - trainLoss: 0.4104168117046356\n",
      "cnt: 0 - valLoss: 0.4293973743915558 - trainLoss: 0.4104152321815491\n",
      "cnt: 0 - valLoss: 0.4293968677520752 - trainLoss: 0.4104137122631073\n",
      "cnt: 0 - valLoss: 0.4293963313102722 - trainLoss: 0.41041216254234314\n",
      "cnt: 0 - valLoss: 0.429395854473114 - trainLoss: 0.410410612821579\n",
      "cnt: 0 - valLoss: 0.4293953478336334 - trainLoss: 0.41040903329849243\n",
      "cnt: 0 - valLoss: 0.42939484119415283 - trainLoss: 0.41040748357772827\n",
      "cnt: 0 - valLoss: 0.42939433455467224 - trainLoss: 0.4104059636592865\n",
      "cnt: 0 - valLoss: 0.42939379811286926 - trainLoss: 0.41040441393852234\n",
      "cnt: 0 - valLoss: 0.42939335107803345 - trainLoss: 0.4104028642177582\n",
      "cnt: 0 - valLoss: 0.42939281463623047 - trainLoss: 0.4104013442993164\n",
      "cnt: 0 - valLoss: 0.42939236760139465 - trainLoss: 0.41039973497390747\n",
      "cnt: 0 - valLoss: 0.4293918311595917 - trainLoss: 0.4103982150554657\n",
      "cnt: 0 - valLoss: 0.4293913245201111 - trainLoss: 0.41039663553237915\n",
      "cnt: 0 - valLoss: 0.4293908476829529 - trainLoss: 0.410395085811615\n",
      "cnt: 0 - valLoss: 0.4293903410434723 - trainLoss: 0.41039353609085083\n",
      "cnt: 0 - valLoss: 0.4293898344039917 - trainLoss: 0.41039201617240906\n",
      "cnt: 0 - valLoss: 0.4293892979621887 - trainLoss: 0.4103904664516449\n",
      "cnt: 0 - valLoss: 0.4293888509273529 - trainLoss: 0.41038891673088074\n",
      "cnt: 0 - valLoss: 0.4293883144855499 - trainLoss: 0.4103873670101166\n",
      "cnt: 0 - valLoss: 0.42938780784606934 - trainLoss: 0.41038575768470764\n",
      "cnt: 0 - valLoss: 0.4293873906135559 - trainLoss: 0.4103842079639435\n",
      "cnt: 0 - valLoss: 0.42938685417175293 - trainLoss: 0.4103827178478241\n",
      "cnt: 0 - valLoss: 0.42938634753227234 - trainLoss: 0.41038113832473755\n",
      "cnt: 0 - valLoss: 0.42938584089279175 - trainLoss: 0.4103795886039734\n",
      "cnt: 0 - valLoss: 0.42938536405563354 - trainLoss: 0.41037803888320923\n",
      "cnt: 0 - valLoss: 0.42938488721847534 - trainLoss: 0.41037651896476746\n",
      "cnt: 0 - valLoss: 0.42938438057899475 - trainLoss: 0.4103749692440033\n",
      "cnt: 0 - valLoss: 0.42938387393951416 - trainLoss: 0.41037338972091675\n",
      "cnt: 0 - valLoss: 0.42938339710235596 - trainLoss: 0.4103718101978302\n",
      "cnt: 0 - valLoss: 0.42938289046287537 - trainLoss: 0.41037026047706604\n",
      "cnt: 0 - valLoss: 0.4293823540210724 - trainLoss: 0.4103687107563019\n",
      "cnt: 0 - valLoss: 0.4293819069862366 - trainLoss: 0.4103671908378601\n",
      "cnt: 0 - valLoss: 0.4293813705444336 - trainLoss: 0.41036564111709595\n",
      "cnt: 0 - valLoss: 0.429380863904953 - trainLoss: 0.4103640913963318\n",
      "cnt: 0 - valLoss: 0.4293803572654724 - trainLoss: 0.4103625416755676\n",
      "cnt: 0 - valLoss: 0.4293798506259918 - trainLoss: 0.4103609621524811\n",
      "cnt: 0 - valLoss: 0.429379403591156 - trainLoss: 0.4103594422340393\n",
      "cnt: 0 - valLoss: 0.4293788969516754 - trainLoss: 0.41035789251327515\n",
      "cnt: 0 - valLoss: 0.4293783903121948 - trainLoss: 0.410356342792511\n",
      "cnt: 0 - valLoss: 0.42937785387039185 - trainLoss: 0.4103547930717468\n",
      "cnt: 0 - valLoss: 0.42937734723091125 - trainLoss: 0.4103532135486603\n",
      "cnt: 0 - valLoss: 0.4293769299983978 - trainLoss: 0.4103516936302185\n",
      "cnt: 0 - valLoss: 0.42937642335891724 - trainLoss: 0.41035014390945435\n",
      "cnt: 0 - valLoss: 0.42937588691711426 - trainLoss: 0.4103485941886902\n",
      "cnt: 0 - valLoss: 0.42937538027763367 - trainLoss: 0.4103470742702484\n",
      "cnt: 0 - valLoss: 0.42937490344047546 - trainLoss: 0.4103454649448395\n",
      "cnt: 0 - valLoss: 0.4293743968009949 - trainLoss: 0.41034388542175293\n",
      "cnt: 0 - valLoss: 0.42937391996383667 - trainLoss: 0.41034239530563354\n",
      "cnt: 0 - valLoss: 0.42937344312667847 - trainLoss: 0.410340815782547\n",
      "cnt: 0 - valLoss: 0.4293729364871979 - trainLoss: 0.41033926606178284\n",
      "cnt: 0 - valLoss: 0.4293724596500397 - trainLoss: 0.4103377163410187\n",
      "cnt: 0 - valLoss: 0.4293719530105591 - trainLoss: 0.4103361964225769\n",
      "cnt: 0 - valLoss: 0.4293714463710785 - trainLoss: 0.41033464670181274\n",
      "cnt: 0 - valLoss: 0.4293709695339203 - trainLoss: 0.4103330969810486\n",
      "cnt: 0 - valLoss: 0.4293704628944397 - trainLoss: 0.4103315770626068\n",
      "cnt: 0 - valLoss: 0.4293699860572815 - trainLoss: 0.41033002734184265\n",
      "cnt: 0 - valLoss: 0.4293694794178009 - trainLoss: 0.4103284478187561\n",
      "cnt: 0 - valLoss: 0.4293689429759979 - trainLoss: 0.41032689809799194\n",
      "cnt: 0 - valLoss: 0.4293684661388397 - trainLoss: 0.4103253483772278\n",
      "cnt: 0 - valLoss: 0.4293680191040039 - trainLoss: 0.41032376885414124\n",
      "cnt: 0 - valLoss: 0.4293674826622009 - trainLoss: 0.41032227873802185\n",
      "cnt: 0 - valLoss: 0.42936697602272034 - trainLoss: 0.4103206992149353\n",
      "cnt: 0 - valLoss: 0.4293665587902069 - trainLoss: 0.41031914949417114\n",
      "cnt: 0 - valLoss: 0.42936602234840393 - trainLoss: 0.410317599773407\n",
      "cnt: 0 - valLoss: 0.42936551570892334 - trainLoss: 0.4103160798549652\n",
      "cnt: 0 - valLoss: 0.42936503887176514 - trainLoss: 0.41031453013420105\n",
      "cnt: 0 - valLoss: 0.42936450242996216 - trainLoss: 0.4103129506111145\n",
      "cnt: 0 - valLoss: 0.42936405539512634 - trainLoss: 0.41031140089035034\n",
      "cnt: 0 - valLoss: 0.42936354875564575 - trainLoss: 0.4103098213672638\n",
      "cnt: 0 - valLoss: 0.42936304211616516 - trainLoss: 0.4103083312511444\n",
      "cnt: 0 - valLoss: 0.42936256527900696 - trainLoss: 0.41030675172805786\n",
      "cnt: 0 - valLoss: 0.42936208844184875 - trainLoss: 0.4103052020072937\n",
      "cnt: 0 - valLoss: 0.42936161160469055 - trainLoss: 0.41030365228652954\n",
      "cnt: 0 - valLoss: 0.4293610751628876 - trainLoss: 0.41030213236808777\n",
      "cnt: 0 - valLoss: 0.42936062812805176 - trainLoss: 0.4103005826473236\n",
      "cnt: 0 - valLoss: 0.42936012148857117 - trainLoss: 0.41029903292655945\n",
      "cnt: 0 - valLoss: 0.42935964465141296 - trainLoss: 0.4102974534034729\n",
      "cnt: 0 - valLoss: 0.4293590784072876 - trainLoss: 0.4102959632873535\n",
      "cnt: 0 - valLoss: 0.4293586313724518 - trainLoss: 0.41029441356658936\n",
      "cnt: 0 - valLoss: 0.4293581545352936 - trainLoss: 0.4102928340435028\n",
      "cnt: 0 - valLoss: 0.429357647895813 - trainLoss: 0.41029125452041626\n",
      "cnt: 0 - valLoss: 0.42935711145401 - trainLoss: 0.4102897047996521\n",
      "cnt: 0 - valLoss: 0.4293566644191742 - trainLoss: 0.41028815507888794\n",
      "cnt: 0 - valLoss: 0.429356187582016 - trainLoss: 0.41028663516044617\n",
      "cnt: 0 - valLoss: 0.429355651140213 - trainLoss: 0.410285085439682\n",
      "cnt: 0 - valLoss: 0.4293551445007324 - trainLoss: 0.41028353571891785\n",
      "cnt: 0 - valLoss: 0.4293546676635742 - trainLoss: 0.4102820158004761\n",
      "cnt: 0 - valLoss: 0.4293542206287384 - trainLoss: 0.4102804660797119\n",
      "cnt: 0 - valLoss: 0.4293536841869354 - trainLoss: 0.41027891635894775\n",
      "cnt: 0 - valLoss: 0.4293532073497772 - trainLoss: 0.4102773368358612\n",
      "cnt: 0 - valLoss: 0.42935270071029663 - trainLoss: 0.41027578711509705\n",
      "cnt: 0 - valLoss: 0.4293522238731384 - trainLoss: 0.4102742075920105\n",
      "cnt: 0 - valLoss: 0.42935171723365784 - trainLoss: 0.4102727174758911\n",
      "cnt: 0 - valLoss: 0.42935124039649963 - trainLoss: 0.41027116775512695\n",
      "cnt: 0 - valLoss: 0.42935076355934143 - trainLoss: 0.4102696180343628\n",
      "cnt: 0 - valLoss: 0.42935025691986084 - trainLoss: 0.41026803851127625\n",
      "cnt: 0 - valLoss: 0.42934978008270264 - trainLoss: 0.4102665185928345\n",
      "cnt: 0 - valLoss: 0.4293493330478668 - trainLoss: 0.4102649688720703\n",
      "cnt: 0 - valLoss: 0.42934879660606384 - trainLoss: 0.41026341915130615\n",
      "cnt: 0 - valLoss: 0.429348349571228 - trainLoss: 0.4102618992328644\n",
      "cnt: 0 - valLoss: 0.4293478727340698 - trainLoss: 0.4102603495121002\n",
      "cnt: 0 - valLoss: 0.42934733629226685 - trainLoss: 0.41025876998901367\n",
      "cnt: 0 - valLoss: 0.42934688925743103 - trainLoss: 0.4102572500705719\n",
      "cnt: 0 - valLoss: 0.42934635281562805 - trainLoss: 0.41025567054748535\n",
      "cnt: 0 - valLoss: 0.42934590578079224 - trainLoss: 0.4102541506290436\n",
      "cnt: 0 - valLoss: 0.42934536933898926 - trainLoss: 0.4102526009082794\n",
      "cnt: 0 - valLoss: 0.42934495210647583 - trainLoss: 0.41025105118751526\n",
      "cnt: 0 - valLoss: 0.42934444546699524 - trainLoss: 0.4102494716644287\n",
      "cnt: 0 - valLoss: 0.42934390902519226 - trainLoss: 0.41024792194366455\n",
      "cnt: 0 - valLoss: 0.42934346199035645 - trainLoss: 0.4102464020252228\n",
      "cnt: 0 - valLoss: 0.42934298515319824 - trainLoss: 0.4102448523044586\n",
      "cnt: 0 - valLoss: 0.42934244871139526 - trainLoss: 0.41024330258369446\n",
      "cnt: 0 - valLoss: 0.42934200167655945 - trainLoss: 0.4102417528629303\n",
      "cnt: 0 - valLoss: 0.42934152483940125 - trainLoss: 0.4102402329444885\n",
      "cnt: 0 - valLoss: 0.42934104800224304 - trainLoss: 0.41023868322372437\n",
      "cnt: 0 - valLoss: 0.4293403923511505 - trainLoss: 0.4102371037006378\n",
      "cnt: 0 - valLoss: 0.42933976650238037 - trainLoss: 0.41023561358451843\n",
      "cnt: 0 - valLoss: 0.42933911085128784 - trainLoss: 0.4102340340614319\n",
      "cnt: 0 - valLoss: 0.4293384850025177 - trainLoss: 0.4102325141429901\n",
      "cnt: 0 - valLoss: 0.42933782935142517 - trainLoss: 0.41023099422454834\n",
      "cnt: 0 - valLoss: 0.4293372333049774 - trainLoss: 0.4102294445037842\n",
      "cnt: 0 - valLoss: 0.4293365776538849 - trainLoss: 0.41022789478302\n",
      "cnt: 0 - valLoss: 0.42933595180511475 - trainLoss: 0.41022634506225586\n",
      "cnt: 0 - valLoss: 0.4293352961540222 - trainLoss: 0.4102248251438141\n",
      "cnt: 0 - valLoss: 0.4293346703052521 - trainLoss: 0.4102232754230499\n",
      "cnt: 0 - valLoss: 0.4293340742588043 - trainLoss: 0.41022172570228577\n",
      "cnt: 0 - valLoss: 0.4293334484100342 - trainLoss: 0.410220205783844\n",
      "cnt: 0 - valLoss: 0.42933276295661926 - trainLoss: 0.41021865606307983\n",
      "cnt: 0 - valLoss: 0.4293321669101715 - trainLoss: 0.4102171063423157\n",
      "cnt: 0 - valLoss: 0.42933154106140137 - trainLoss: 0.4102155864238739\n",
      "cnt: 0 - valLoss: 0.42933088541030884 - trainLoss: 0.41021403670310974\n",
      "cnt: 0 - valLoss: 0.4293302893638611 - trainLoss: 0.4102124869823456\n",
      "cnt: 0 - valLoss: 0.42932969331741333 - trainLoss: 0.4102109372615814\n",
      "cnt: 0 - valLoss: 0.4293290078639984 - trainLoss: 0.41020941734313965\n",
      "cnt: 0 - valLoss: 0.42932841181755066 - trainLoss: 0.4102078676223755\n",
      "cnt: 0 - valLoss: 0.4293278157711029 - trainLoss: 0.41020631790161133\n",
      "cnt: 0 - valLoss: 0.42932718992233276 - trainLoss: 0.41020479798316956\n",
      "cnt: 0 - valLoss: 0.42932650446891785 - trainLoss: 0.4102032482624054\n",
      "cnt: 0 - valLoss: 0.4293259084224701 - trainLoss: 0.41020169854164124\n",
      "cnt: 0 - valLoss: 0.42932531237602234 - trainLoss: 0.41020017862319946\n",
      "cnt: 0 - valLoss: 0.4293246865272522 - trainLoss: 0.4101986289024353\n",
      "cnt: 0 - valLoss: 0.42932403087615967 - trainLoss: 0.41019707918167114\n",
      "cnt: 0 - valLoss: 0.4293234348297119 - trainLoss: 0.410195529460907\n",
      "cnt: 0 - valLoss: 0.42932283878326416 - trainLoss: 0.4101940095424652\n",
      "cnt: 0 - valLoss: 0.4293222427368164 - trainLoss: 0.41019245982170105\n",
      "cnt: 0 - valLoss: 0.4293215870857239 - trainLoss: 0.4101909101009369\n",
      "cnt: 0 - valLoss: 0.4293210208415985 - trainLoss: 0.4101893901824951\n",
      "cnt: 0 - valLoss: 0.42932042479515076 - trainLoss: 0.41018784046173096\n",
      "cnt: 0 - valLoss: 0.4293197989463806 - trainLoss: 0.4101863503456116\n",
      "cnt: 0 - valLoss: 0.4293191432952881 - trainLoss: 0.4101848006248474\n",
      "cnt: 0 - valLoss: 0.4293185770511627 - trainLoss: 0.41018325090408325\n",
      "cnt: 0 - valLoss: 0.4293179512023926 - trainLoss: 0.4101817309856415\n",
      "cnt: 0 - valLoss: 0.4293173551559448 - trainLoss: 0.41018012166023254\n",
      "cnt: 0 - valLoss: 0.42931675910949707 - trainLoss: 0.41017863154411316\n",
      "cnt: 0 - valLoss: 0.42931613326072693 - trainLoss: 0.4101771414279938\n",
      "cnt: 0 - valLoss: 0.4293155372142792 - trainLoss: 0.4101755917072296\n",
      "cnt: 0 - valLoss: 0.4293149411678314 - trainLoss: 0.41017404198646545\n",
      "cnt: 0 - valLoss: 0.42931437492370605 - trainLoss: 0.4101724624633789\n",
      "cnt: 0 - valLoss: 0.4293137788772583 - trainLoss: 0.4101709723472595\n",
      "cnt: 0 - valLoss: 0.42931318283081055 - trainLoss: 0.41016942262649536\n",
      "cnt: 0 - valLoss: 0.4293125867843628 - trainLoss: 0.4101679027080536\n",
      "cnt: 0 - valLoss: 0.42931199073791504 - trainLoss: 0.41016635298728943\n",
      "cnt: 0 - valLoss: 0.4293113946914673 - trainLoss: 0.41016480326652527\n",
      "cnt: 0 - valLoss: 0.42931079864501953 - trainLoss: 0.4101632833480835\n",
      "cnt: 0 - valLoss: 0.42931023240089417 - trainLoss: 0.41016173362731934\n",
      "cnt: 0 - valLoss: 0.4293096363544464 - trainLoss: 0.4101601839065552\n",
      "cnt: 0 - valLoss: 0.42930904030799866 - trainLoss: 0.410158634185791\n",
      "cnt: 0 - valLoss: 0.4293084740638733 - trainLoss: 0.41015711426734924\n",
      "cnt: 0 - valLoss: 0.42930781841278076 - trainLoss: 0.4101555645465851\n",
      "cnt: 0 - valLoss: 0.429307222366333 - trainLoss: 0.4101540744304657\n",
      "cnt: 0 - valLoss: 0.42930668592453003 - trainLoss: 0.41015252470970154\n",
      "cnt: 0 - valLoss: 0.42930611968040466 - trainLoss: 0.410150945186615\n",
      "cnt: 0 - valLoss: 0.4293055236339569 - trainLoss: 0.4101494550704956\n",
      "cnt: 0 - valLoss: 0.42930492758750916 - trainLoss: 0.41014790534973145\n",
      "cnt: 0 - valLoss: 0.4293043613433838 - trainLoss: 0.41014641523361206\n",
      "cnt: 0 - valLoss: 0.42930376529693604 - trainLoss: 0.4101448655128479\n",
      "cnt: 0 - valLoss: 0.42930319905281067 - trainLoss: 0.41014331579208374\n",
      "cnt: 0 - valLoss: 0.4293026030063629 - trainLoss: 0.41014179587364197\n",
      "cnt: 0 - valLoss: 0.42930203676223755 - trainLoss: 0.4101402461528778\n",
      "cnt: 0 - valLoss: 0.4293014407157898 - trainLoss: 0.41013869643211365\n",
      "cnt: 0 - valLoss: 0.42930087447166443 - trainLoss: 0.4101371467113495\n",
      "cnt: 0 - valLoss: 0.4293002784252167 - trainLoss: 0.4101356267929077\n",
      "cnt: 0 - valLoss: 0.4292997419834137 - trainLoss: 0.41013407707214355\n",
      "cnt: 0 - valLoss: 0.42929911613464355 - trainLoss: 0.4101325273513794\n",
      "cnt: 0 - valLoss: 0.4292985796928406 - trainLoss: 0.41013103723526\n",
      "cnt: 0 - valLoss: 0.4292980134487152 - trainLoss: 0.41012948751449585\n",
      "cnt: 0 - valLoss: 0.42929744720458984 - trainLoss: 0.4101279079914093\n",
      "cnt: 0 - valLoss: 0.4292968511581421 - trainLoss: 0.4101264178752899\n",
      "cnt: 0 - valLoss: 0.42929625511169434 - trainLoss: 0.41012483835220337\n",
      "cnt: 0 - valLoss: 0.42929574847221375 - trainLoss: 0.410123348236084\n",
      "cnt: 0 - valLoss: 0.429295152425766 - trainLoss: 0.4101217985153198\n",
      "cnt: 0 - valLoss: 0.4292945861816406 - trainLoss: 0.41012030839920044\n",
      "cnt: 0 - valLoss: 0.42929401993751526 - trainLoss: 0.4101187586784363\n",
      "cnt: 0 - valLoss: 0.4292934238910675 - trainLoss: 0.4101172089576721\n",
      "cnt: 0 - valLoss: 0.42929285764694214 - trainLoss: 0.41011565923690796\n",
      "cnt: 0 - valLoss: 0.4292922616004944 - trainLoss: 0.4101141393184662\n",
      "cnt: 0 - valLoss: 0.4292917251586914 - trainLoss: 0.410112589597702\n",
      "cnt: 0 - valLoss: 0.42929115891456604 - trainLoss: 0.41011103987693787\n",
      "cnt: 0 - valLoss: 0.4292905926704407 - trainLoss: 0.4101095497608185\n",
      "cnt: 0 - valLoss: 0.4292900562286377 - trainLoss: 0.41010797023773193\n",
      "cnt: 0 - valLoss: 0.42928943037986755 - trainLoss: 0.41010648012161255\n",
      "cnt: 0 - valLoss: 0.4292888939380646 - trainLoss: 0.4101049304008484\n",
      "cnt: 0 - valLoss: 0.4292883574962616 - trainLoss: 0.41010338068008423\n",
      "cnt: 0 - valLoss: 0.42928776144981384 - trainLoss: 0.41010186076164246\n",
      "cnt: 0 - valLoss: 0.42928725481033325 - trainLoss: 0.4101003110408783\n",
      "cnt: 0 - valLoss: 0.4292866885662079 - trainLoss: 0.4100988209247589\n",
      "cnt: 0 - valLoss: 0.4292861521244049 - trainLoss: 0.41009727120399475\n",
      "cnt: 0 - valLoss: 0.42928555607795715 - trainLoss: 0.4100957214832306\n",
      "cnt: 0 - valLoss: 0.4292850196361542 - trainLoss: 0.4100942015647888\n",
      "cnt: 0 - valLoss: 0.4292844235897064 - trainLoss: 0.41009265184402466\n",
      "cnt: 0 - valLoss: 0.42928391695022583 - trainLoss: 0.4100911021232605\n",
      "cnt: 0 - valLoss: 0.4292833209037781 - trainLoss: 0.4100896120071411\n",
      "cnt: 0 - valLoss: 0.4292827844619751 - trainLoss: 0.41008806228637695\n",
      "cnt: 0 - valLoss: 0.4292822778224945 - trainLoss: 0.4100865125656128\n",
      "cnt: 0 - valLoss: 0.42928168177604675 - trainLoss: 0.4100850224494934\n",
      "cnt: 0 - valLoss: 0.42928117513656616 - trainLoss: 0.41008347272872925\n",
      "cnt: 0 - valLoss: 0.42928066849708557 - trainLoss: 0.4100819528102875\n",
      "cnt: 0 - valLoss: 0.42928004264831543 - trainLoss: 0.4100804030895233\n",
      "cnt: 0 - valLoss: 0.42927950620651245 - trainLoss: 0.41007885336875916\n",
      "cnt: 0 - valLoss: 0.4292789697647095 - trainLoss: 0.4100773334503174\n",
      "cnt: 0 - valLoss: 0.4292784333229065 - trainLoss: 0.4100758135318756\n",
      "cnt: 0 - valLoss: 0.4292779266834259 - trainLoss: 0.41007429361343384\n",
      "cnt: 0 - valLoss: 0.42927730083465576 - trainLoss: 0.4100727438926697\n",
      "cnt: 0 - valLoss: 0.42927679419517517 - trainLoss: 0.4100711941719055\n",
      "cnt: 0 - valLoss: 0.4292762279510498 - trainLoss: 0.41006967425346375\n",
      "cnt: 0 - valLoss: 0.4292756915092468 - trainLoss: 0.410068154335022\n",
      "cnt: 0 - valLoss: 0.42927515506744385 - trainLoss: 0.4100666344165802\n",
      "cnt: 0 - valLoss: 0.42927461862564087 - trainLoss: 0.41006508469581604\n",
      "cnt: 0 - valLoss: 0.4292740523815155 - trainLoss: 0.4100635349750519\n",
      "cnt: 0 - valLoss: 0.4292735755443573 - trainLoss: 0.4100620150566101\n",
      "cnt: 0 - valLoss: 0.4292730391025543 - trainLoss: 0.41006046533584595\n",
      "cnt: 0 - valLoss: 0.42927247285842896 - trainLoss: 0.41005897521972656\n",
      "cnt: 0 - valLoss: 0.42927196621894836 - trainLoss: 0.4100574254989624\n",
      "cnt: 0 - valLoss: 0.429271399974823 - trainLoss: 0.41005587577819824\n",
      "cnt: 0 - valLoss: 0.4292708933353424 - trainLoss: 0.4100543260574341\n",
      "cnt: 0 - valLoss: 0.42927032709121704 - trainLoss: 0.4100528359413147\n",
      "cnt: 0 - valLoss: 0.42926982045173645 - trainLoss: 0.41005128622055054\n",
      "cnt: 0 - valLoss: 0.4292692542076111 - trainLoss: 0.41004976630210876\n",
      "cnt: 0 - valLoss: 0.4292687475681305 - trainLoss: 0.4100482165813446\n",
      "cnt: 0 - valLoss: 0.4292681813240051 - trainLoss: 0.41004666686058044\n",
      "cnt: 0 - valLoss: 0.4292677044868469 - trainLoss: 0.41004517674446106\n",
      "cnt: 0 - valLoss: 0.42926710844039917 - trainLoss: 0.4100436270236969\n",
      "cnt: 0 - valLoss: 0.4292666018009186 - trainLoss: 0.4100421369075775\n",
      "cnt: 0 - valLoss: 0.429266095161438 - trainLoss: 0.41004055738449097\n",
      "cnt: 0 - valLoss: 0.429265558719635 - trainLoss: 0.4100390672683716\n",
      "cnt: 0 - valLoss: 0.42926502227783203 - trainLoss: 0.4100375175476074\n",
      "cnt: 0 - valLoss: 0.42926448583602905 - trainLoss: 0.41003596782684326\n",
      "cnt: 0 - valLoss: 0.42926397919654846 - trainLoss: 0.4100344479084015\n",
      "cnt: 0 - valLoss: 0.4292634129524231 - trainLoss: 0.41003289818763733\n",
      "cnt: 0 - valLoss: 0.4292629063129425 - trainLoss: 0.41003140807151794\n",
      "cnt: 0 - valLoss: 0.4292623996734619 - trainLoss: 0.4100298285484314\n",
      "cnt: 0 - valLoss: 0.4292618930339813 - trainLoss: 0.4100283086299896\n",
      "cnt: 0 - valLoss: 0.42926132678985596 - trainLoss: 0.41002678871154785\n",
      "cnt: 0 - valLoss: 0.42926084995269775 - trainLoss: 0.4100252687931061\n",
      "cnt: 0 - valLoss: 0.42926025390625 - trainLoss: 0.4100237488746643\n",
      "cnt: 0 - valLoss: 0.4292597770690918 - trainLoss: 0.41002219915390015\n",
      "cnt: 0 - valLoss: 0.42925921082496643 - trainLoss: 0.410020649433136\n",
      "cnt: 0 - valLoss: 0.42925870418548584 - trainLoss: 0.4100191593170166\n",
      "cnt: 0 - valLoss: 0.42925819754600525 - trainLoss: 0.41001760959625244\n",
      "cnt: 0 - valLoss: 0.42925769090652466 - trainLoss: 0.4100160598754883\n",
      "cnt: 0 - valLoss: 0.4292571544647217 - trainLoss: 0.4100145697593689\n",
      "cnt: 0 - valLoss: 0.4292566478252411 - trainLoss: 0.4100130498409271\n",
      "cnt: 0 - valLoss: 0.4292560815811157 - trainLoss: 0.41001150012016296\n",
      "cnt: 0 - valLoss: 0.4292556345462799 - trainLoss: 0.4100099503993988\n",
      "cnt: 0 - valLoss: 0.42925506830215454 - trainLoss: 0.41000840067863464\n",
      "cnt: 0 - valLoss: 0.42925456166267395 - trainLoss: 0.41000688076019287\n",
      "cnt: 0 - valLoss: 0.4292539954185486 - trainLoss: 0.4100053608417511\n",
      "cnt: 0 - valLoss: 0.4292535185813904 - trainLoss: 0.4100038707256317\n",
      "cnt: 0 - valLoss: 0.4292530119419098 - trainLoss: 0.41000232100486755\n",
      "cnt: 0 - valLoss: 0.4292524456977844 - trainLoss: 0.4100008010864258\n",
      "cnt: 0 - valLoss: 0.42925193905830383 - trainLoss: 0.4099992513656616\n",
      "cnt: 0 - valLoss: 0.42925143241882324 - trainLoss: 0.40999770164489746\n",
      "cnt: 0 - valLoss: 0.42925089597702026 - trainLoss: 0.4099961817264557\n",
      "cnt: 0 - valLoss: 0.4292503893375397 - trainLoss: 0.40999463200569153\n",
      "cnt: 0 - valLoss: 0.42924991250038147 - trainLoss: 0.40999314188957214\n",
      "cnt: 0 - valLoss: 0.4292494058609009 - trainLoss: 0.409991592168808\n",
      "cnt: 0 - valLoss: 0.4292488992214203 - trainLoss: 0.4099900424480438\n",
      "cnt: 0 - valLoss: 0.4292483329772949 - trainLoss: 0.40998855233192444\n",
      "cnt: 0 - valLoss: 0.42924782633781433 - trainLoss: 0.4099870026111603\n",
      "cnt: 0 - valLoss: 0.42924734950065613 - trainLoss: 0.4099854826927185\n",
      "cnt: 0 - valLoss: 0.42924684286117554 - trainLoss: 0.40998393297195435\n",
      "cnt: 0 - valLoss: 0.42924633622169495 - trainLoss: 0.4099823832511902\n",
      "cnt: 0 - valLoss: 0.42924579977989197 - trainLoss: 0.4099808931350708\n",
      "cnt: 0 - valLoss: 0.429245263338089 - trainLoss: 0.40997934341430664\n",
      "cnt: 0 - valLoss: 0.4292447865009308 - trainLoss: 0.40997785329818726\n",
      "cnt: 0 - valLoss: 0.4292442798614502 - trainLoss: 0.4099763035774231\n",
      "cnt: 0 - valLoss: 0.4292437434196472 - trainLoss: 0.4099747836589813\n",
      "cnt: 0 - valLoss: 0.42924320697784424 - trainLoss: 0.40997323393821716\n",
      "cnt: 0 - valLoss: 0.42924273014068604 - trainLoss: 0.4099717438220978\n",
      "cnt: 0 - valLoss: 0.42924222350120544 - trainLoss: 0.4099701941013336\n",
      "cnt: 0 - valLoss: 0.42924174666404724 - trainLoss: 0.40996864438056946\n",
      "cnt: 0 - valLoss: 0.42924121022224426 - trainLoss: 0.4099670946598053\n",
      "cnt: 0 - valLoss: 0.42924070358276367 - trainLoss: 0.4099656045436859\n",
      "cnt: 0 - valLoss: 0.42924022674560547 - trainLoss: 0.40996408462524414\n",
      "cnt: 0 - valLoss: 0.4292397201061249 - trainLoss: 0.40996253490448\n",
      "cnt: 0 - valLoss: 0.4292392432689667 - trainLoss: 0.4099610447883606\n",
      "cnt: 0 - valLoss: 0.4292387366294861 - trainLoss: 0.40995949506759644\n",
      "cnt: 0 - valLoss: 0.4292382299900055 - trainLoss: 0.4099579453468323\n",
      "cnt: 0 - valLoss: 0.4292377233505249 - trainLoss: 0.4099564552307129\n",
      "cnt: 0 - valLoss: 0.4292372465133667 - trainLoss: 0.40995490550994873\n",
      "cnt: 0 - valLoss: 0.42923668026924133 - trainLoss: 0.40995335578918457\n",
      "cnt: 0 - valLoss: 0.4292362630367279 - trainLoss: 0.4099518358707428\n",
      "cnt: 0 - valLoss: 0.4292357265949249 - trainLoss: 0.4099503457546234\n",
      "cnt: 0 - valLoss: 0.42923521995544434 - trainLoss: 0.40994879603385925\n",
      "cnt: 0 - valLoss: 0.42923474311828613 - trainLoss: 0.4099472463130951\n",
      "cnt: 0 - valLoss: 0.42923423647880554 - trainLoss: 0.4099457561969757\n",
      "cnt: 0 - valLoss: 0.42923375964164734 - trainLoss: 0.40994420647621155\n",
      "cnt: 0 - valLoss: 0.429233193397522 - trainLoss: 0.4099426567554474\n",
      "cnt: 0 - valLoss: 0.42923274636268616 - trainLoss: 0.4099411368370056\n",
      "cnt: 0 - valLoss: 0.4292322099208832 - trainLoss: 0.40993961691856384\n",
      "cnt: 0 - valLoss: 0.42923176288604736 - trainLoss: 0.40993812680244446\n",
      "cnt: 0 - valLoss: 0.4292312264442444 - trainLoss: 0.4099365770816803\n",
      "cnt: 0 - valLoss: 0.42923077940940857 - trainLoss: 0.4099350571632385\n",
      "cnt: 0 - valLoss: 0.4292302429676056 - trainLoss: 0.40993350744247437\n",
      "cnt: 0 - valLoss: 0.429229736328125 - trainLoss: 0.409932017326355\n",
      "cnt: 0 - valLoss: 0.4292292594909668 - trainLoss: 0.40993043780326843\n",
      "cnt: 0 - valLoss: 0.4292287528514862 - trainLoss: 0.40992891788482666\n",
      "cnt: 0 - valLoss: 0.4292282462120056 - trainLoss: 0.4099273979663849\n",
      "cnt: 0 - valLoss: 0.4292277693748474 - trainLoss: 0.4099258482456207\n",
      "cnt: 0 - valLoss: 0.4292272925376892 - trainLoss: 0.40992435812950134\n",
      "cnt: 0 - valLoss: 0.4292267858982086 - trainLoss: 0.4099228084087372\n",
      "cnt: 0 - valLoss: 0.4292263090610504 - trainLoss: 0.4099213182926178\n",
      "cnt: 0 - valLoss: 0.4292258024215698 - trainLoss: 0.40991973876953125\n",
      "cnt: 0 - valLoss: 0.42922526597976685 - trainLoss: 0.4099182188510895\n",
      "cnt: 0 - valLoss: 0.42922481894493103 - trainLoss: 0.4099166989326477\n",
      "cnt: 0 - valLoss: 0.4292243421077728 - trainLoss: 0.40991517901420593\n",
      "cnt: 0 - valLoss: 0.42922383546829224 - trainLoss: 0.40991365909576416\n",
      "cnt: 0 - valLoss: 0.42922329902648926 - trainLoss: 0.409912109375\n",
      "cnt: 0 - valLoss: 0.42922288179397583 - trainLoss: 0.40991055965423584\n",
      "cnt: 0 - valLoss: 0.42922237515449524 - trainLoss: 0.40990906953811646\n",
      "cnt: 0 - valLoss: 0.42922189831733704 - trainLoss: 0.40990757942199707\n",
      "cnt: 0 - valLoss: 0.42922139167785645 - trainLoss: 0.4099060297012329\n",
      "cnt: 0 - valLoss: 0.42922091484069824 - trainLoss: 0.40990447998046875\n",
      "cnt: 0 - valLoss: 0.42922037839889526 - trainLoss: 0.409902960062027\n",
      "cnt: 0 - valLoss: 0.42921993136405945 - trainLoss: 0.4099014401435852\n",
      "cnt: 0 - valLoss: 0.42921939492225647 - trainLoss: 0.40989986062049866\n",
      "cnt: 0 - valLoss: 0.42921894788742065 - trainLoss: 0.40989840030670166\n",
      "cnt: 0 - valLoss: 0.4292184114456177 - trainLoss: 0.4098968803882599\n",
      "cnt: 0 - valLoss: 0.42921799421310425 - trainLoss: 0.4098953306674957\n",
      "cnt: 0 - valLoss: 0.42921751737594604 - trainLoss: 0.40989378094673157\n",
      "cnt: 0 - valLoss: 0.42921701073646545 - trainLoss: 0.4098922610282898\n",
      "cnt: 0 - valLoss: 0.42921653389930725 - trainLoss: 0.4098908007144928\n",
      "cnt: 0 - valLoss: 0.42921602725982666 - trainLoss: 0.40988925099372864\n",
      "cnt: 0 - valLoss: 0.42921555042266846 - trainLoss: 0.4098877012729645\n",
      "cnt: 0 - valLoss: 0.42921507358551025 - trainLoss: 0.4098861813545227\n",
      "cnt: 0 - valLoss: 0.42921456694602966 - trainLoss: 0.40988466143608093\n",
      "cnt: 0 - valLoss: 0.42921409010887146 - trainLoss: 0.40988314151763916\n",
      "cnt: 0 - valLoss: 0.42921361327171326 - trainLoss: 0.409881591796875\n",
      "cnt: 0 - valLoss: 0.42921310663223267 - trainLoss: 0.4098801016807556\n",
      "cnt: 0 - valLoss: 0.42921268939971924 - trainLoss: 0.40987855195999146\n",
      "cnt: 0 - valLoss: 0.42921215295791626 - trainLoss: 0.4098770022392273\n",
      "cnt: 0 - valLoss: 0.42921170592308044 - trainLoss: 0.4098755121231079\n",
      "cnt: 0 - valLoss: 0.42921122908592224 - trainLoss: 0.40987393260002136\n",
      "cnt: 0 - valLoss: 0.42921069264411926 - trainLoss: 0.40987247228622437\n",
      "cnt: 0 - valLoss: 0.42921027541160583 - trainLoss: 0.4098709225654602\n",
      "cnt: 0 - valLoss: 0.42920976877212524 - trainLoss: 0.40986940264701843\n",
      "cnt: 0 - valLoss: 0.42920929193496704 - trainLoss: 0.40986788272857666\n",
      "cnt: 0 - valLoss: 0.42920881509780884 - trainLoss: 0.4098663032054901\n",
      "cnt: 0 - valLoss: 0.4292083978652954 - trainLoss: 0.4098648130893707\n",
      "cnt: 0 - valLoss: 0.42920783162117004 - trainLoss: 0.40986332297325134\n",
      "cnt: 0 - valLoss: 0.42920735478401184 - trainLoss: 0.4098617732524872\n",
      "cnt: 0 - valLoss: 0.4292069375514984 - trainLoss: 0.409860223531723\n",
      "cnt: 0 - valLoss: 0.4292064309120178 - trainLoss: 0.40985870361328125\n",
      "cnt: 0 - valLoss: 0.4292059540748596 - trainLoss: 0.4098571836948395\n",
      "cnt: 0 - valLoss: 0.4292054772377014 - trainLoss: 0.4098556637763977\n",
      "cnt: 0 - valLoss: 0.4292050004005432 - trainLoss: 0.40985414385795593\n",
      "cnt: 0 - valLoss: 0.429204523563385 - trainLoss: 0.40985262393951416\n",
      "cnt: 0 - valLoss: 0.4292040765285492 - trainLoss: 0.4098511040210724\n",
      "cnt: 0 - valLoss: 0.429203599691391 - trainLoss: 0.4098495841026306\n",
      "cnt: 0 - valLoss: 0.429203063249588 - trainLoss: 0.40984803438186646\n",
      "cnt: 0 - valLoss: 0.4292026460170746 - trainLoss: 0.40984654426574707\n",
      "cnt: 0 - valLoss: 0.4292021691799164 - trainLoss: 0.4098449945449829\n",
      "cnt: 0 - valLoss: 0.4292016923427582 - trainLoss: 0.40984344482421875\n",
      "cnt: 0 - valLoss: 0.42920124530792236 - trainLoss: 0.409841924905777\n",
      "cnt: 0 - valLoss: 0.42920076847076416 - trainLoss: 0.4098404049873352\n",
      "cnt: 0 - valLoss: 0.42920029163360596 - trainLoss: 0.4098389148712158\n",
      "cnt: 0 - valLoss: 0.42919981479644775 - trainLoss: 0.40983736515045166\n",
      "cnt: 0 - valLoss: 0.4291993975639343 - trainLoss: 0.4098358452320099\n",
      "cnt: 0 - valLoss: 0.42919886112213135 - trainLoss: 0.4098343849182129\n",
      "cnt: 0 - valLoss: 0.42919841408729553 - trainLoss: 0.40983283519744873\n",
      "cnt: 0 - valLoss: 0.42919793725013733 - trainLoss: 0.4098312556743622\n",
      "cnt: 0 - valLoss: 0.4291974902153015 - trainLoss: 0.4098297655582428\n",
      "cnt: 0 - valLoss: 0.4291970431804657 - trainLoss: 0.40982821583747864\n",
      "cnt: 0 - valLoss: 0.4291965067386627 - trainLoss: 0.4098266661167145\n",
      "cnt: 0 - valLoss: 0.4291960597038269 - trainLoss: 0.4098251760005951\n",
      "cnt: 0 - valLoss: 0.4291956126689911 - trainLoss: 0.40982362627983093\n",
      "cnt: 0 - valLoss: 0.4291951060295105 - trainLoss: 0.40982213616371155\n",
      "cnt: 0 - valLoss: 0.42919468879699707 - trainLoss: 0.4098205864429474\n",
      "cnt: 0 - valLoss: 0.4291941523551941 - trainLoss: 0.4098190665245056\n",
      "cnt: 0 - valLoss: 0.4291936755180359 - trainLoss: 0.40981754660606384\n",
      "cnt: 0 - valLoss: 0.42919325828552246 - trainLoss: 0.40981602668762207\n",
      "cnt: 0 - valLoss: 0.42919278144836426 - trainLoss: 0.4098145067691803\n",
      "cnt: 0 - valLoss: 0.42919230461120605 - trainLoss: 0.4098129868507385\n",
      "cnt: 0 - valLoss: 0.42919185757637024 - trainLoss: 0.40981143712997437\n",
      "cnt: 0 - valLoss: 0.4291914105415344 - trainLoss: 0.409809947013855\n",
      "cnt: 0 - valLoss: 0.42919090390205383 - trainLoss: 0.4098083972930908\n",
      "cnt: 0 - valLoss: 0.4291904866695404 - trainLoss: 0.40980690717697144\n",
      "cnt: 0 - valLoss: 0.4291900098323822 - trainLoss: 0.4098053574562073\n",
      "cnt: 0 - valLoss: 0.429189532995224 - trainLoss: 0.4098038077354431\n",
      "cnt: 0 - valLoss: 0.42918911576271057 - trainLoss: 0.40980231761932373\n",
      "cnt: 0 - valLoss: 0.42918863892555237 - trainLoss: 0.40980076789855957\n",
      "cnt: 0 - valLoss: 0.42918816208839417 - trainLoss: 0.4097992777824402\n",
      "cnt: 0 - valLoss: 0.42918768525123596 - trainLoss: 0.4097977578639984\n",
      "cnt: 0 - valLoss: 0.42918720841407776 - trainLoss: 0.40979623794555664\n",
      "cnt: 0 - valLoss: 0.42918679118156433 - trainLoss: 0.40979471802711487\n",
      "cnt: 0 - valLoss: 0.42918622493743896 - trainLoss: 0.4097931683063507\n",
      "cnt: 0 - valLoss: 0.4291857182979584 - trainLoss: 0.4097916781902313\n",
      "cnt: 0 - valLoss: 0.4291852116584778 - trainLoss: 0.40979012846946716\n",
      "cnt: 0 - valLoss: 0.4291846454143524 - trainLoss: 0.409788578748703\n",
      "cnt: 0 - valLoss: 0.4291841387748718 - trainLoss: 0.40978702902793884\n",
      "cnt: 0 - valLoss: 0.42918360233306885 - trainLoss: 0.40978550910949707\n",
      "cnt: 0 - valLoss: 0.42918309569358826 - trainLoss: 0.4097839593887329\n",
      "cnt: 0 - valLoss: 0.42918258905410767 - trainLoss: 0.40978240966796875\n",
      "cnt: 0 - valLoss: 0.4291820228099823 - trainLoss: 0.409780889749527\n",
      "cnt: 0 - valLoss: 0.4291815161705017 - trainLoss: 0.4097793400287628\n",
      "cnt: 0 - valLoss: 0.4291810095310211 - trainLoss: 0.40977779030799866\n",
      "cnt: 0 - valLoss: 0.4291805028915405 - trainLoss: 0.4097762703895569\n",
      "cnt: 0 - valLoss: 0.42917993664741516 - trainLoss: 0.4097747206687927\n",
      "cnt: 0 - valLoss: 0.42917943000793457 - trainLoss: 0.40977323055267334\n",
      "cnt: 0 - valLoss: 0.4291788935661316 - trainLoss: 0.4097716808319092\n",
      "cnt: 0 - valLoss: 0.429178386926651 - trainLoss: 0.409770131111145\n",
      "cnt: 0 - valLoss: 0.4291778802871704 - trainLoss: 0.40976855158805847\n",
      "cnt: 0 - valLoss: 0.4291773736476898 - trainLoss: 0.4097670614719391\n",
      "cnt: 0 - valLoss: 0.42917683720588684 - trainLoss: 0.4097655117511749\n",
      "cnt: 0 - valLoss: 0.42917630076408386 - trainLoss: 0.40976396203041077\n",
      "cnt: 0 - valLoss: 0.4291757345199585 - trainLoss: 0.4097624719142914\n",
      "cnt: 0 - valLoss: 0.4291752278804779 - trainLoss: 0.4097609221935272\n",
      "cnt: 0 - valLoss: 0.4291747510433197 - trainLoss: 0.4097593426704407\n",
      "cnt: 0 - valLoss: 0.42917418479919434 - trainLoss: 0.4097578525543213\n",
      "cnt: 0 - valLoss: 0.42917367815971375 - trainLoss: 0.40975630283355713\n",
      "cnt: 0 - valLoss: 0.42917317152023315 - trainLoss: 0.40975478291511536\n",
      "cnt: 0 - valLoss: 0.4291726350784302 - trainLoss: 0.4097532331943512\n",
      "cnt: 0 - valLoss: 0.4291721284389496 - trainLoss: 0.40975168347358704\n",
      "cnt: 0 - valLoss: 0.429171621799469 - trainLoss: 0.4097501337528229\n",
      "cnt: 0 - valLoss: 0.4291711151599884 - trainLoss: 0.4097486138343811\n",
      "cnt: 0 - valLoss: 0.4291705787181854 - trainLoss: 0.40974706411361694\n",
      "cnt: 0 - valLoss: 0.42917007207870483 - trainLoss: 0.4097455143928528\n",
      "cnt: 0 - valLoss: 0.42916956543922424 - trainLoss: 0.409743994474411\n",
      "cnt: 0 - valLoss: 0.42916905879974365 - trainLoss: 0.40974244475364685\n",
      "cnt: 0 - valLoss: 0.4291685223579407 - trainLoss: 0.4097408950328827\n",
      "cnt: 0 - valLoss: 0.4291679859161377 - trainLoss: 0.4097393751144409\n",
      "cnt: 0 - valLoss: 0.4291675090789795 - trainLoss: 0.40973782539367676\n",
      "cnt: 0 - valLoss: 0.4291669726371765 - trainLoss: 0.4097363352775574\n",
      "cnt: 0 - valLoss: 0.42916643619537354 - trainLoss: 0.4097347855567932\n",
      "cnt: 0 - valLoss: 0.42916595935821533 - trainLoss: 0.40973323583602905\n",
      "cnt: 0 - valLoss: 0.42916545271873474 - trainLoss: 0.4097316861152649\n",
      "cnt: 0 - valLoss: 0.42916491627693176 - trainLoss: 0.4097301959991455\n",
      "cnt: 0 - valLoss: 0.42916440963745117 - trainLoss: 0.40972867608070374\n",
      "cnt: 0 - valLoss: 0.4291639029979706 - trainLoss: 0.4097270667552948\n",
      "cnt: 0 - valLoss: 0.42916339635849 - trainLoss: 0.4097255766391754\n",
      "cnt: 0 - valLoss: 0.429162859916687 - trainLoss: 0.40972402691841125\n",
      "cnt: 0 - valLoss: 0.4291623532772064 - trainLoss: 0.4097225069999695\n",
      "cnt: 0 - valLoss: 0.42916184663772583 - trainLoss: 0.4097209572792053\n",
      "cnt: 0 - valLoss: 0.4291613698005676 - trainLoss: 0.40971940755844116\n",
      "cnt: 0 - valLoss: 0.42916086316108704 - trainLoss: 0.4097178876399994\n",
      "cnt: 0 - valLoss: 0.42916035652160645 - trainLoss: 0.40971633791923523\n",
      "cnt: 0 - valLoss: 0.42915982007980347 - trainLoss: 0.40971478819847107\n",
      "cnt: 0 - valLoss: 0.4291593134403229 - trainLoss: 0.4097132980823517\n",
      "cnt: 0 - valLoss: 0.4291588068008423 - trainLoss: 0.40971171855926514\n",
      "cnt: 0 - valLoss: 0.4291583001613617 - trainLoss: 0.40971022844314575\n",
      "cnt: 0 - valLoss: 0.4291578233242035 - trainLoss: 0.4097086191177368\n",
      "cnt: 0 - valLoss: 0.4291572868824005 - trainLoss: 0.40970712900161743\n",
      "cnt: 0 - valLoss: 0.4291567802429199 - trainLoss: 0.40970557928085327\n",
      "cnt: 0 - valLoss: 0.42915627360343933 - trainLoss: 0.4097040593624115\n",
      "cnt: 0 - valLoss: 0.42915579676628113 - trainLoss: 0.40970250964164734\n",
      "cnt: 0 - valLoss: 0.4291553199291229 - trainLoss: 0.40970101952552795\n",
      "cnt: 0 - valLoss: 0.42915481328964233 - trainLoss: 0.4096994698047638\n",
      "cnt: 0 - valLoss: 0.42915430665016174 - trainLoss: 0.40969792008399963\n",
      "cnt: 0 - valLoss: 0.42915380001068115 - trainLoss: 0.40969640016555786\n",
      "cnt: 0 - valLoss: 0.4291532635688782 - trainLoss: 0.4096948504447937\n",
      "cnt: 0 - valLoss: 0.4291527569293976 - trainLoss: 0.40969330072402954\n",
      "cnt: 0 - valLoss: 0.4291522800922394 - trainLoss: 0.40969178080558777\n",
      "cnt: 0 - valLoss: 0.4291518032550812 - trainLoss: 0.4096902310848236\n",
      "cnt: 0 - valLoss: 0.4291512966156006 - trainLoss: 0.40968868136405945\n",
      "cnt: 0 - valLoss: 0.42915078997612 - trainLoss: 0.40968719124794006\n",
      "cnt: 0 - valLoss: 0.4291502833366394 - trainLoss: 0.4096856415271759\n",
      "cnt: 0 - valLoss: 0.4291497468948364 - trainLoss: 0.40968406200408936\n",
      "cnt: 0 - valLoss: 0.4291492700576782 - trainLoss: 0.40968257188796997\n",
      "cnt: 0 - valLoss: 0.4291488230228424 - trainLoss: 0.4096810221672058\n",
      "cnt: 0 - valLoss: 0.42914828658103943 - trainLoss: 0.4096795320510864\n",
      "cnt: 0 - valLoss: 0.42914777994155884 - trainLoss: 0.40967798233032227\n",
      "cnt: 0 - valLoss: 0.42914727330207825 - trainLoss: 0.4096764028072357\n",
      "cnt: 0 - valLoss: 0.42914676666259766 - trainLoss: 0.40967485308647156\n",
      "cnt: 0 - valLoss: 0.42914631962776184 - trainLoss: 0.4096733629703522\n",
      "cnt: 0 - valLoss: 0.42914581298828125 - trainLoss: 0.409671813249588\n",
      "cnt: 0 - valLoss: 0.42914530634880066 - trainLoss: 0.40967029333114624\n",
      "cnt: 0 - valLoss: 0.4291447699069977 - trainLoss: 0.4096687436103821\n",
      "cnt: 0 - valLoss: 0.42914432287216187 - trainLoss: 0.4096671938896179\n",
      "cnt: 0 - valLoss: 0.4291437864303589 - trainLoss: 0.40966564416885376\n",
      "cnt: 0 - valLoss: 0.42914333939552307 - trainLoss: 0.409664124250412\n",
      "cnt: 0 - valLoss: 0.4291428029537201 - trainLoss: 0.4096626043319702\n",
      "cnt: 0 - valLoss: 0.4291423559188843 - trainLoss: 0.40966108441352844\n",
      "cnt: 0 - valLoss: 0.4291418194770813 - trainLoss: 0.4096595346927643\n",
      "cnt: 0 - valLoss: 0.4291413426399231 - trainLoss: 0.4096580445766449\n",
      "cnt: 0 - valLoss: 0.4291408360004425 - trainLoss: 0.40965649485588074\n",
      "cnt: 0 - valLoss: 0.4291403293609619 - trainLoss: 0.4096549451351166\n",
      "cnt: 0 - valLoss: 0.4291399121284485 - trainLoss: 0.4096534252166748\n",
      "cnt: 0 - valLoss: 0.4291393756866455 - trainLoss: 0.40965187549591064\n",
      "cnt: 0 - valLoss: 0.4291388690471649 - trainLoss: 0.4096503257751465\n",
      "cnt: 0 - valLoss: 0.4291383922100067 - trainLoss: 0.4096488058567047\n",
      "cnt: 0 - valLoss: 0.4291379153728485 - trainLoss: 0.40964728593826294\n",
      "cnt: 0 - valLoss: 0.4291374087333679 - trainLoss: 0.4096457064151764\n",
      "cnt: 0 - valLoss: 0.4291369915008545 - trainLoss: 0.409644216299057\n",
      "cnt: 0 - valLoss: 0.4291364550590515 - trainLoss: 0.40964266657829285\n",
      "cnt: 0 - valLoss: 0.4291359484195709 - trainLoss: 0.4096411466598511\n",
      "cnt: 0 - valLoss: 0.4291355311870575 - trainLoss: 0.4096396267414093\n",
      "cnt: 0 - valLoss: 0.4291350245475769 - trainLoss: 0.40963810682296753\n",
      "cnt: 0 - valLoss: 0.4291345477104187 - trainLoss: 0.40963655710220337\n",
      "cnt: 0 - valLoss: 0.4291340708732605 - trainLoss: 0.4096350073814392\n",
      "cnt: 0 - valLoss: 0.4291335642337799 - trainLoss: 0.40963345766067505\n",
      "cnt: 0 - valLoss: 0.4291330873966217 - trainLoss: 0.4096319377422333\n",
      "cnt: 0 - valLoss: 0.4291326105594635 - trainLoss: 0.4096304178237915\n",
      "cnt: 0 - valLoss: 0.4291321337223053 - trainLoss: 0.40962889790534973\n",
      "cnt: 0 - valLoss: 0.42913150787353516 - trainLoss: 0.40962734818458557\n",
      "cnt: 0 - valLoss: 0.42913082242012024 - trainLoss: 0.4096257984638214\n",
      "cnt: 0 - valLoss: 0.4291301667690277 - trainLoss: 0.40962421894073486\n",
      "cnt: 0 - valLoss: 0.4291295111179352 - trainLoss: 0.4096226990222931\n",
      "cnt: 0 - valLoss: 0.4291289746761322 - trainLoss: 0.40962114930152893\n",
      "cnt: 0 - valLoss: 0.4291283190250397 - trainLoss: 0.40961959958076477\n",
      "cnt: 0 - valLoss: 0.4291278123855591 - trainLoss: 0.4096180200576782\n",
      "cnt: 0 - valLoss: 0.42912715673446655 - trainLoss: 0.40961647033691406\n",
      "cnt: 0 - valLoss: 0.4291265308856964 - trainLoss: 0.4096149504184723\n",
      "cnt: 0 - valLoss: 0.4291258454322815 - trainLoss: 0.40961334109306335\n",
      "cnt: 0 - valLoss: 0.42912527918815613 - trainLoss: 0.4096118211746216\n",
      "cnt: 0 - valLoss: 0.429124653339386 - trainLoss: 0.40961024165153503\n",
      "cnt: 0 - valLoss: 0.42912396788597107 - trainLoss: 0.4096086919307709\n",
      "cnt: 0 - valLoss: 0.4291234314441681 - trainLoss: 0.4096071422100067\n",
      "cnt: 0 - valLoss: 0.4291227459907532 - trainLoss: 0.40960559248924255\n",
      "cnt: 0 - valLoss: 0.4291221499443054 - trainLoss: 0.409604012966156\n",
      "cnt: 0 - valLoss: 0.42912155389785767 - trainLoss: 0.40960249304771423\n",
      "cnt: 0 - valLoss: 0.42912086844444275 - trainLoss: 0.4096009433269501\n",
      "cnt: 0 - valLoss: 0.429120272397995 - trainLoss: 0.4095993936061859\n",
      "cnt: 0 - valLoss: 0.42911967635154724 - trainLoss: 0.40959787368774414\n",
      "cnt: 0 - valLoss: 0.4291190207004547 - trainLoss: 0.4095962643623352\n",
      "cnt: 0 - valLoss: 0.42911839485168457 - trainLoss: 0.40959468483924866\n",
      "cnt: 0 - valLoss: 0.4291178286075592 - trainLoss: 0.4095931947231293\n",
      "cnt: 0 - valLoss: 0.4291171729564667 - trainLoss: 0.4095916152000427\n",
      "cnt: 0 - valLoss: 0.4291165769100189 - trainLoss: 0.40959012508392334\n",
      "cnt: 0 - valLoss: 0.429115891456604 - trainLoss: 0.4095885157585144\n",
      "cnt: 0 - valLoss: 0.4291153848171234 - trainLoss: 0.40958699584007263\n",
      "cnt: 0 - valLoss: 0.4291146993637085 - trainLoss: 0.40958544611930847\n",
      "cnt: 0 - valLoss: 0.42911410331726074 - trainLoss: 0.4095838665962219\n",
      "cnt: 0 - valLoss: 0.429113507270813 - trainLoss: 0.40958231687545776\n",
      "cnt: 0 - valLoss: 0.42911288142204285 - trainLoss: 0.4095807671546936\n",
      "cnt: 0 - valLoss: 0.4291122853755951 - trainLoss: 0.40957918763160706\n",
      "cnt: 0 - valLoss: 0.4291115999221802 - trainLoss: 0.4095776677131653\n",
      "cnt: 0 - valLoss: 0.4291110038757324 - trainLoss: 0.4095761179924011\n",
      "cnt: 0 - valLoss: 0.42911049723625183 - trainLoss: 0.40957456827163696\n",
      "cnt: 0 - valLoss: 0.4291098415851593 - trainLoss: 0.4095730483531952\n",
      "cnt: 0 - valLoss: 0.42910921573638916 - trainLoss: 0.40957149863243103\n",
      "cnt: 0 - valLoss: 0.42910856008529663 - trainLoss: 0.4095699191093445\n",
      "cnt: 0 - valLoss: 0.42910805344581604 - trainLoss: 0.4095683693885803\n",
      "cnt: 0 - valLoss: 0.4291073977947235 - trainLoss: 0.4095667898654938\n",
      "cnt: 0 - valLoss: 0.42910677194595337 - trainLoss: 0.4095652401447296\n",
      "cnt: 0 - valLoss: 0.42910611629486084 - trainLoss: 0.40956369042396545\n",
      "cnt: 0 - valLoss: 0.4291055202484131 - trainLoss: 0.4095621705055237\n",
      "cnt: 0 - valLoss: 0.42910492420196533 - trainLoss: 0.40956059098243713\n",
      "cnt: 0 - valLoss: 0.4291043281555176 - trainLoss: 0.409559041261673\n",
      "cnt: 0 - valLoss: 0.4291037321090698 - trainLoss: 0.4095574915409088\n",
      "cnt: 0 - valLoss: 0.4291030764579773 - trainLoss: 0.40955597162246704\n",
      "cnt: 0 - valLoss: 0.42910248041152954 - trainLoss: 0.4095543622970581\n",
      "cnt: 0 - valLoss: 0.429101824760437 - trainLoss: 0.40955284237861633\n",
      "cnt: 0 - valLoss: 0.4291013181209564 - trainLoss: 0.4095512926578522\n",
      "cnt: 0 - valLoss: 0.42910072207450867 - trainLoss: 0.409549742937088\n",
      "cnt: 0 - valLoss: 0.42910006642341614 - trainLoss: 0.40954822301864624\n",
      "cnt: 0 - valLoss: 0.429099440574646 - trainLoss: 0.4095466136932373\n",
      "cnt: 0 - valLoss: 0.42909884452819824 - trainLoss: 0.40954509377479553\n",
      "cnt: 0 - valLoss: 0.4290982484817505 - trainLoss: 0.40954354405403137\n",
      "cnt: 0 - valLoss: 0.42909759283065796 - trainLoss: 0.4095419943332672\n",
      "cnt: 0 - valLoss: 0.4290970265865326 - trainLoss: 0.40954047441482544\n",
      "cnt: 0 - valLoss: 0.42909643054008484 - trainLoss: 0.4095388650894165\n",
      "cnt: 0 - valLoss: 0.4290958344936371 - trainLoss: 0.40953734517097473\n",
      "cnt: 0 - valLoss: 0.42909523844718933 - trainLoss: 0.40953579545021057\n",
      "cnt: 0 - valLoss: 0.4290946424007416 - trainLoss: 0.409534215927124\n",
      "cnt: 0 - valLoss: 0.4290940463542938 - trainLoss: 0.40953272581100464\n",
      "cnt: 0 - valLoss: 0.4290933907032013 - trainLoss: 0.4095311462879181\n",
      "cnt: 0 - valLoss: 0.42909279465675354 - trainLoss: 0.40952959656715393\n",
      "cnt: 0 - valLoss: 0.4290921986103058 - trainLoss: 0.40952804684638977\n",
      "cnt: 0 - valLoss: 0.4290916323661804 - trainLoss: 0.4095264971256256\n",
      "cnt: 0 - valLoss: 0.42909103631973267 - trainLoss: 0.40952497720718384\n",
      "cnt: 0 - valLoss: 0.4290904402732849 - trainLoss: 0.4095233976840973\n",
      "cnt: 0 - valLoss: 0.42908987402915955 - trainLoss: 0.40952184796333313\n",
      "cnt: 0 - valLoss: 0.4290892779827118 - trainLoss: 0.40952029824256897\n",
      "cnt: 0 - valLoss: 0.42908868193626404 - trainLoss: 0.4095187485218048\n",
      "cnt: 0 - valLoss: 0.4290880858898163 - trainLoss: 0.40951722860336304\n",
      "cnt: 0 - valLoss: 0.4290875196456909 - trainLoss: 0.4095156490802765\n",
      "cnt: 0 - valLoss: 0.42908692359924316 - trainLoss: 0.40951409935951233\n",
      "cnt: 0 - valLoss: 0.4290863275527954 - trainLoss: 0.40951254963874817\n",
      "cnt: 0 - valLoss: 0.42908573150634766 - trainLoss: 0.4095110297203064\n",
      "cnt: 0 - valLoss: 0.4290850758552551 - trainLoss: 0.40950947999954224\n",
      "cnt: 0 - valLoss: 0.42908450961112976 - trainLoss: 0.4095079004764557\n",
      "cnt: 0 - valLoss: 0.429083913564682 - trainLoss: 0.40950635075569153\n",
      "cnt: 0 - valLoss: 0.42908331751823425 - trainLoss: 0.40950480103492737\n",
      "cnt: 0 - valLoss: 0.4290827214717865 - trainLoss: 0.4095032811164856\n",
      "cnt: 0 - valLoss: 0.42908212542533875 - trainLoss: 0.40950173139572144\n",
      "cnt: 0 - valLoss: 0.4290815591812134 - trainLoss: 0.4095001816749573\n",
      "cnt: 0 - valLoss: 0.429080992937088 - trainLoss: 0.4094986319541931\n",
      "cnt: 0 - valLoss: 0.42908036708831787 - trainLoss: 0.40949705243110657\n",
      "cnt: 0 - valLoss: 0.4290798008441925 - trainLoss: 0.4094955623149872\n",
      "cnt: 0 - valLoss: 0.42907920479774475 - trainLoss: 0.40949398279190063\n",
      "cnt: 0 - valLoss: 0.429078608751297 - trainLoss: 0.4094924330711365\n",
      "cnt: 0 - valLoss: 0.42907804250717163 - trainLoss: 0.4094909131526947\n",
      "cnt: 0 - valLoss: 0.4290774464607239 - trainLoss: 0.40948936343193054\n",
      "cnt: 0 - valLoss: 0.4290768802165985 - trainLoss: 0.409487783908844\n",
      "cnt: 0 - valLoss: 0.42907625436782837 - trainLoss: 0.40948623418807983\n",
      "cnt: 0 - valLoss: 0.429075688123703 - trainLoss: 0.4094846844673157\n",
      "cnt: 0 - valLoss: 0.42907509207725525 - trainLoss: 0.4094831645488739\n",
      "cnt: 0 - valLoss: 0.4290745258331299 - trainLoss: 0.40948161482810974\n",
      "cnt: 0 - valLoss: 0.4290739893913269 - trainLoss: 0.4094800651073456\n",
      "cnt: 0 - valLoss: 0.42907336354255676 - trainLoss: 0.4094785153865814\n",
      "cnt: 0 - valLoss: 0.429072767496109 - trainLoss: 0.40947699546813965\n",
      "cnt: 0 - valLoss: 0.42907217144966125 - trainLoss: 0.4094754457473755\n",
      "cnt: 0 - valLoss: 0.4290716052055359 - trainLoss: 0.40947389602661133\n",
      "cnt: 0 - valLoss: 0.4290710687637329 - trainLoss: 0.4094723165035248\n",
      "cnt: 0 - valLoss: 0.42907047271728516 - trainLoss: 0.4094708263874054\n",
      "cnt: 0 - valLoss: 0.4290699064731598 - trainLoss: 0.40946924686431885\n",
      "cnt: 0 - valLoss: 0.42906925082206726 - trainLoss: 0.4094676971435547\n",
      "cnt: 0 - valLoss: 0.42906874418258667 - trainLoss: 0.4094661474227905\n",
      "cnt: 0 - valLoss: 0.4290681481361389 - trainLoss: 0.40946462750434875\n",
      "cnt: 0 - valLoss: 0.42906755208969116 - trainLoss: 0.4094630777835846\n",
      "cnt: 0 - valLoss: 0.4290669858455658 - trainLoss: 0.40946149826049805\n",
      "cnt: 0 - valLoss: 0.42906641960144043 - trainLoss: 0.4094599485397339\n",
      "cnt: 0 - valLoss: 0.4290658235549927 - trainLoss: 0.4094583988189697\n",
      "cnt: 0 - valLoss: 0.4290653169155121 - trainLoss: 0.40945687890052795\n",
      "cnt: 0 - valLoss: 0.42906472086906433 - trainLoss: 0.4094553291797638\n",
      "cnt: 0 - valLoss: 0.4290641248226166 - trainLoss: 0.40945377945899963\n",
      "cnt: 0 - valLoss: 0.429063618183136 - trainLoss: 0.40945225954055786\n",
      "cnt: 0 - valLoss: 0.42906299233436584 - trainLoss: 0.4094507098197937\n",
      "cnt: 0 - valLoss: 0.42906245589256287 - trainLoss: 0.40944916009902954\n",
      "cnt: 0 - valLoss: 0.4290618896484375 - trainLoss: 0.409447580575943\n",
      "cnt: 0 - valLoss: 0.42906132340431213 - trainLoss: 0.4094460904598236\n",
      "cnt: 0 - valLoss: 0.4290607273578644 - trainLoss: 0.40944451093673706\n",
      "cnt: 0 - valLoss: 0.429060161113739 - trainLoss: 0.4094429910182953\n",
      "cnt: 0 - valLoss: 0.42905956506729126 - trainLoss: 0.40944141149520874\n",
      "cnt: 0 - valLoss: 0.4290590286254883 - trainLoss: 0.40943989157676697\n",
      "cnt: 0 - valLoss: 0.4290584623813629 - trainLoss: 0.40943828225135803\n",
      "cnt: 0 - valLoss: 0.42905789613723755 - trainLoss: 0.40943676233291626\n",
      "cnt: 0 - valLoss: 0.42905735969543457 - trainLoss: 0.4094352424144745\n",
      "cnt: 0 - valLoss: 0.4290567934513092 - trainLoss: 0.40943366289138794\n",
      "cnt: 0 - valLoss: 0.42905622720718384 - trainLoss: 0.40943217277526855\n",
      "cnt: 0 - valLoss: 0.42905566096305847 - trainLoss: 0.4094306230545044\n",
      "cnt: 0 - valLoss: 0.4290551245212555 - trainLoss: 0.40942904353141785\n",
      "cnt: 0 - valLoss: 0.42905452847480774 - trainLoss: 0.4094275236129761\n",
      "cnt: 0 - valLoss: 0.4290539622306824 - trainLoss: 0.4094259738922119\n",
      "cnt: 0 - valLoss: 0.429053395986557 - trainLoss: 0.40942442417144775\n",
      "cnt: 0 - valLoss: 0.4290528893470764 - trainLoss: 0.4094228744506836\n",
      "cnt: 0 - valLoss: 0.42905229330062866 - trainLoss: 0.4094213545322418\n",
      "cnt: 0 - valLoss: 0.42905178666114807 - trainLoss: 0.40941980481147766\n",
      "cnt: 0 - valLoss: 0.4290512204170227 - trainLoss: 0.4094182550907135\n",
      "cnt: 0 - valLoss: 0.42905065417289734 - trainLoss: 0.40941673517227173\n",
      "cnt: 0 - valLoss: 0.42905014753341675 - trainLoss: 0.4094151258468628\n",
      "cnt: 0 - valLoss: 0.4290495812892914 - trainLoss: 0.4094136357307434\n",
      "cnt: 0 - valLoss: 0.4290490448474884 - trainLoss: 0.40941205620765686\n",
      "cnt: 0 - valLoss: 0.4290485084056854 - trainLoss: 0.4094105660915375\n",
      "cnt: 0 - valLoss: 0.42904791235923767 - trainLoss: 0.4094090163707733\n",
      "cnt: 0 - valLoss: 0.4290474057197571 - trainLoss: 0.40940743684768677\n",
      "cnt: 0 - valLoss: 0.4290468394756317 - trainLoss: 0.4094058871269226\n",
      "cnt: 0 - valLoss: 0.4290463328361511 - trainLoss: 0.40940436720848083\n",
      "cnt: 0 - valLoss: 0.42904576659202576 - trainLoss: 0.40940284729003906\n",
      "cnt: 0 - valLoss: 0.4290452301502228 - trainLoss: 0.4094012677669525\n",
      "cnt: 0 - valLoss: 0.4290446937084198 - trainLoss: 0.40939971804618835\n",
      "cnt: 0 - valLoss: 0.4290441572666168 - trainLoss: 0.4093981385231018\n",
      "cnt: 0 - valLoss: 0.42904362082481384 - trainLoss: 0.4093966484069824\n",
      "cnt: 0 - valLoss: 0.42904311418533325 - trainLoss: 0.4093950688838959\n",
      "cnt: 0 - valLoss: 0.4290425479412079 - trainLoss: 0.4093935191631317\n",
      "cnt: 0 - valLoss: 0.4290420114994049 - trainLoss: 0.40939196944236755\n",
      "cnt: 0 - valLoss: 0.42904147505760193 - trainLoss: 0.4093904495239258\n",
      "cnt: 0 - valLoss: 0.42904096841812134 - trainLoss: 0.4093888998031616\n",
      "cnt: 0 - valLoss: 0.4290403723716736 - trainLoss: 0.40938735008239746\n",
      "cnt: 0 - valLoss: 0.429039865732193 - trainLoss: 0.4093858301639557\n",
      "cnt: 0 - valLoss: 0.4290393590927124 - trainLoss: 0.40938428044319153\n",
      "cnt: 0 - valLoss: 0.4290388226509094 - trainLoss: 0.40938273072242737\n",
      "cnt: 0 - valLoss: 0.42903828620910645 - trainLoss: 0.4093812108039856\n",
      "cnt: 0 - valLoss: 0.4290377199649811 - trainLoss: 0.40937966108322144\n",
      "cnt: 0 - valLoss: 0.4290372133255005 - trainLoss: 0.4093781113624573\n",
      "cnt: 0 - valLoss: 0.4290366768836975 - trainLoss: 0.4093765616416931\n",
      "cnt: 0 - valLoss: 0.42903614044189453 - trainLoss: 0.40937504172325134\n",
      "cnt: 0 - valLoss: 0.42903557419776917 - trainLoss: 0.4093734920024872\n",
      "cnt: 0 - valLoss: 0.4290350675582886 - trainLoss: 0.409371942281723\n",
      "cnt: 0 - valLoss: 0.4290345311164856 - trainLoss: 0.40937042236328125\n",
      "cnt: 0 - valLoss: 0.429034024477005 - trainLoss: 0.4093688726425171\n",
      "cnt: 0 - valLoss: 0.42903345823287964 - trainLoss: 0.40936732292175293\n",
      "cnt: 0 - valLoss: 0.42903292179107666 - trainLoss: 0.40936580300331116\n",
      "cnt: 0 - valLoss: 0.42903241515159607 - trainLoss: 0.409364253282547\n",
      "cnt: 0 - valLoss: 0.4290318787097931 - trainLoss: 0.40936267375946045\n",
      "cnt: 0 - valLoss: 0.4290313720703125 - trainLoss: 0.4093611538410187\n",
      "cnt: 0 - valLoss: 0.4290308654308319 - trainLoss: 0.4093596339225769\n",
      "cnt: 0 - valLoss: 0.42903029918670654 - trainLoss: 0.40935808420181274\n",
      "cnt: 0 - valLoss: 0.42902979254722595 - trainLoss: 0.4093565344810486\n",
      "cnt: 0 - valLoss: 0.42902928590774536 - trainLoss: 0.4093550145626068\n",
      "cnt: 0 - valLoss: 0.42902871966362 - trainLoss: 0.40935346484184265\n",
      "cnt: 0 - valLoss: 0.4290282130241394 - trainLoss: 0.4093518853187561\n",
      "cnt: 0 - valLoss: 0.4290276765823364 - trainLoss: 0.40935033559799194\n",
      "cnt: 0 - valLoss: 0.42902714014053345 - trainLoss: 0.4093487858772278\n",
      "cnt: 0 - valLoss: 0.42902666330337524 - trainLoss: 0.409347265958786\n",
      "cnt: 0 - valLoss: 0.42902615666389465 - trainLoss: 0.40934574604034424\n",
      "cnt: 0 - valLoss: 0.4290255904197693 - trainLoss: 0.4093441665172577\n",
      "cnt: 0 - valLoss: 0.4290250837802887 - trainLoss: 0.4093426465988159\n",
      "cnt: 0 - valLoss: 0.42902451753616333 - trainLoss: 0.40934109687805176\n",
      "cnt: 0 - valLoss: 0.42902401089668274 - trainLoss: 0.4093395471572876\n",
      "cnt: 0 - valLoss: 0.42902347445487976 - trainLoss: 0.40933799743652344\n",
      "cnt: 0 - valLoss: 0.42902296781539917 - trainLoss: 0.40933647751808167\n",
      "cnt: 0 - valLoss: 0.4290224611759186 - trainLoss: 0.4093349277973175\n",
      "cnt: 0 - valLoss: 0.429021954536438 - trainLoss: 0.40933337807655334\n",
      "cnt: 0 - valLoss: 0.429021418094635 - trainLoss: 0.4093318581581116\n",
      "cnt: 0 - valLoss: 0.4290209114551544 - trainLoss: 0.4093303084373474\n",
      "cnt: 0 - valLoss: 0.42902034521102905 - trainLoss: 0.40932875871658325\n",
      "cnt: 0 - valLoss: 0.42901983857154846 - trainLoss: 0.4093272387981415\n",
      "cnt: 0 - valLoss: 0.42901936173439026 - trainLoss: 0.4093256890773773\n",
      "cnt: 0 - valLoss: 0.4290188252925873 - trainLoss: 0.40932413935661316\n",
      "cnt: 0 - valLoss: 0.4290182590484619 - trainLoss: 0.409322589635849\n",
      "cnt: 0 - valLoss: 0.4290177822113037 - trainLoss: 0.4093210697174072\n",
      "cnt: 0 - valLoss: 0.42901721596717834 - trainLoss: 0.40931951999664307\n",
      "cnt: 0 - valLoss: 0.42901670932769775 - trainLoss: 0.4093179702758789\n",
      "cnt: 0 - valLoss: 0.42901620268821716 - trainLoss: 0.40931645035743713\n",
      "cnt: 0 - valLoss: 0.4290156960487366 - trainLoss: 0.409314900636673\n",
      "cnt: 0 - valLoss: 0.4290151596069336 - trainLoss: 0.4093133509159088\n",
      "cnt: 0 - valLoss: 0.429014652967453 - trainLoss: 0.40931183099746704\n",
      "cnt: 0 - valLoss: 0.4290141463279724 - trainLoss: 0.4093102812767029\n",
      "cnt: 0 - valLoss: 0.4290136396884918 - trainLoss: 0.4093087315559387\n",
      "cnt: 0 - valLoss: 0.42901310324668884 - trainLoss: 0.40930724143981934\n",
      "cnt: 0 - valLoss: 0.42901259660720825 - trainLoss: 0.4093056619167328\n",
      "cnt: 0 - valLoss: 0.42901208996772766 - trainLoss: 0.40930411219596863\n",
      "cnt: 0 - valLoss: 0.4290115237236023 - trainLoss: 0.40930256247520447\n",
      "cnt: 0 - valLoss: 0.4290110468864441 - trainLoss: 0.4093010425567627\n",
      "cnt: 0 - valLoss: 0.4290105402469635 - trainLoss: 0.40929949283599854\n",
      "cnt: 0 - valLoss: 0.4290100336074829 - trainLoss: 0.40929800271987915\n",
      "cnt: 0 - valLoss: 0.4290095269680023 - trainLoss: 0.4092963933944702\n",
      "cnt: 0 - valLoss: 0.42900899052619934 - trainLoss: 0.40929490327835083\n",
      "cnt: 0 - valLoss: 0.42900845408439636 - trainLoss: 0.4092933237552643\n",
      "cnt: 0 - valLoss: 0.42900797724723816 - trainLoss: 0.4092917740345001\n",
      "cnt: 0 - valLoss: 0.4290074408054352 - trainLoss: 0.40929028391838074\n",
      "cnt: 0 - valLoss: 0.4290069043636322 - trainLoss: 0.4092887043952942\n",
      "cnt: 0 - valLoss: 0.429006427526474 - trainLoss: 0.4092872142791748\n",
      "cnt: 0 - valLoss: 0.4290059208869934 - trainLoss: 0.40928566455841064\n",
      "cnt: 0 - valLoss: 0.42900535464286804 - trainLoss: 0.4092841148376465\n",
      "cnt: 0 - valLoss: 0.42900487780570984 - trainLoss: 0.40928253531455994\n",
      "cnt: 0 - valLoss: 0.42900437116622925 - trainLoss: 0.40928104519844055\n",
      "cnt: 0 - valLoss: 0.42900386452674866 - trainLoss: 0.409279465675354\n",
      "cnt: 0 - valLoss: 0.4290033280849457 - trainLoss: 0.40927794575691223\n",
      "cnt: 0 - valLoss: 0.4290028214454651 - trainLoss: 0.40927642583847046\n",
      "cnt: 0 - valLoss: 0.4290023148059845 - trainLoss: 0.4092748761177063\n",
      "cnt: 0 - valLoss: 0.4290018081665039 - trainLoss: 0.40927332639694214\n",
      "cnt: 0 - valLoss: 0.4290012717247009 - trainLoss: 0.40927180647850037\n",
      "cnt: 0 - valLoss: 0.42900076508522034 - trainLoss: 0.4092702567577362\n",
      "cnt: 0 - valLoss: 0.42900028824806213 - trainLoss: 0.40926870703697205\n",
      "cnt: 0 - valLoss: 0.42899978160858154 - trainLoss: 0.4092671871185303\n",
      "cnt: 0 - valLoss: 0.42899927496910095 - trainLoss: 0.4092656373977661\n",
      "cnt: 0 - valLoss: 0.42899879813194275 - trainLoss: 0.40926408767700195\n",
      "cnt: 0 - valLoss: 0.4289982318878174 - trainLoss: 0.4092625379562378\n",
      "cnt: 0 - valLoss: 0.4289977550506592 - trainLoss: 0.409261018037796\n",
      "cnt: 0 - valLoss: 0.42899730801582336 - trainLoss: 0.40925946831703186\n",
      "cnt: 0 - valLoss: 0.4289967715740204 - trainLoss: 0.4092579185962677\n",
      "cnt: 0 - valLoss: 0.4289962649345398 - trainLoss: 0.4092563986778259\n",
      "cnt: 0 - valLoss: 0.4289957880973816 - trainLoss: 0.40925484895706177\n",
      "cnt: 0 - valLoss: 0.428995281457901 - trainLoss: 0.4092533588409424\n",
      "cnt: 0 - valLoss: 0.4289948046207428 - trainLoss: 0.40925177931785583\n",
      "cnt: 0 - valLoss: 0.4289942979812622 - trainLoss: 0.4092502295970917\n",
      "cnt: 0 - valLoss: 0.4289937913417816 - trainLoss: 0.4092486798763275\n",
      "cnt: 0 - valLoss: 0.4289933145046234 - trainLoss: 0.40924718976020813\n",
      "cnt: 0 - valLoss: 0.4289928078651428 - trainLoss: 0.40924564003944397\n",
      "cnt: 0 - valLoss: 0.4289923310279846 - trainLoss: 0.4092440903186798\n",
      "cnt: 0 - valLoss: 0.42899182438850403 - trainLoss: 0.40924251079559326\n",
      "cnt: 0 - valLoss: 0.4289913475513458 - trainLoss: 0.4092410206794739\n",
      "cnt: 0 - valLoss: 0.42899081110954285 - trainLoss: 0.4092394709587097\n",
      "cnt: 0 - valLoss: 0.42899036407470703 - trainLoss: 0.40923798084259033\n",
      "cnt: 0 - valLoss: 0.42898988723754883 - trainLoss: 0.40923643112182617\n",
      "cnt: 0 - valLoss: 0.42898938059806824 - trainLoss: 0.4092349112033844\n",
      "cnt: 0 - valLoss: 0.42898884415626526 - trainLoss: 0.40923336148262024\n",
      "cnt: 0 - valLoss: 0.42898836731910706 - trainLoss: 0.4092318117618561\n",
      "cnt: 0 - valLoss: 0.42898792028427124 - trainLoss: 0.4092302918434143\n",
      "cnt: 0 - valLoss: 0.42898738384246826 - trainLoss: 0.40922874212265015\n",
      "cnt: 0 - valLoss: 0.42898687720298767 - trainLoss: 0.409227192401886\n",
      "cnt: 0 - valLoss: 0.4289863705635071 - trainLoss: 0.4092256426811218\n",
      "cnt: 0 - valLoss: 0.4289858937263489 - trainLoss: 0.40922412276268005\n",
      "cnt: 0 - valLoss: 0.4289854168891907 - trainLoss: 0.4092226028442383\n",
      "cnt: 0 - valLoss: 0.42898494005203247 - trainLoss: 0.40922102332115173\n",
      "cnt: 0 - valLoss: 0.4289844334125519 - trainLoss: 0.40921950340270996\n",
      "cnt: 0 - valLoss: 0.4289839565753937 - trainLoss: 0.4092179834842682\n",
      "cnt: 0 - valLoss: 0.4289834499359131 - trainLoss: 0.40921640396118164\n",
      "cnt: 0 - valLoss: 0.4289829730987549 - trainLoss: 0.40921488404273987\n",
      "cnt: 0 - valLoss: 0.4289824664592743 - trainLoss: 0.4092133641242981\n",
      "cnt: 0 - valLoss: 0.4289819896221161 - trainLoss: 0.4092118442058563\n",
      "cnt: 0 - valLoss: 0.4289814829826355 - trainLoss: 0.40921032428741455\n",
      "cnt: 0 - valLoss: 0.4289809763431549 - trainLoss: 0.409208744764328\n",
      "cnt: 0 - valLoss: 0.4289805293083191 - trainLoss: 0.4092072546482086\n",
      "cnt: 0 - valLoss: 0.4289800226688385 - trainLoss: 0.40920570492744446\n",
      "cnt: 0 - valLoss: 0.4289795458316803 - trainLoss: 0.4092041552066803\n",
      "cnt: 0 - valLoss: 0.4289790391921997 - trainLoss: 0.4092026352882385\n",
      "cnt: 0 - valLoss: 0.4289785623550415 - trainLoss: 0.40920108556747437\n",
      "cnt: 0 - valLoss: 0.4289780855178833 - trainLoss: 0.4091995358467102\n",
      "cnt: 0 - valLoss: 0.4289776086807251 - trainLoss: 0.40919801592826843\n",
      "cnt: 0 - valLoss: 0.4289771616458893 - trainLoss: 0.4091964662075043\n",
      "cnt: 0 - valLoss: 0.4289766252040863 - trainLoss: 0.4091949164867401\n",
      "cnt: 0 - valLoss: 0.4289761781692505 - trainLoss: 0.4091934263706207\n",
      "cnt: 0 - valLoss: 0.4289757013320923 - trainLoss: 0.40919187664985657\n",
      "cnt: 0 - valLoss: 0.4289751946926117 - trainLoss: 0.4091903567314148\n",
      "cnt: 0 - valLoss: 0.4289747178554535 - trainLoss: 0.40918880701065063\n",
      "cnt: 0 - valLoss: 0.4289741814136505 - trainLoss: 0.4091872572898865\n",
      "cnt: 0 - valLoss: 0.4289737641811371 - trainLoss: 0.4091857671737671\n",
      "cnt: 0 - valLoss: 0.4289732575416565 - trainLoss: 0.40918418765068054\n",
      "cnt: 0 - valLoss: 0.4289727807044983 - trainLoss: 0.40918269753456116\n",
      "cnt: 0 - valLoss: 0.4289722740650177 - trainLoss: 0.409181147813797\n",
      "cnt: 0 - valLoss: 0.4289717972278595 - trainLoss: 0.40917959809303284\n",
      "cnt: 0 - valLoss: 0.4289713203907013 - trainLoss: 0.4091780483722687\n",
      "cnt: 0 - valLoss: 0.4289708435535431 - trainLoss: 0.4091765284538269\n",
      "cnt: 0 - valLoss: 0.4289703667163849 - trainLoss: 0.40917497873306274\n",
      "cnt: 0 - valLoss: 0.4289699196815491 - trainLoss: 0.4091734290122986\n",
      "cnt: 0 - valLoss: 0.42896944284439087 - trainLoss: 0.4091719090938568\n",
      "cnt: 0 - valLoss: 0.4289688766002655 - trainLoss: 0.40917038917541504\n",
      "cnt: 0 - valLoss: 0.4289684593677521 - trainLoss: 0.4091688096523285\n",
      "cnt: 0 - valLoss: 0.4289679229259491 - trainLoss: 0.4091673195362091\n",
      "cnt: 0 - valLoss: 0.4289674758911133 - trainLoss: 0.40916576981544495\n",
      "cnt: 0 - valLoss: 0.4289669990539551 - trainLoss: 0.4091642498970032\n",
      "cnt: 0 - valLoss: 0.4289664924144745 - trainLoss: 0.409162700176239\n",
      "cnt: 0 - valLoss: 0.42896604537963867 - trainLoss: 0.40916121006011963\n",
      "cnt: 0 - valLoss: 0.4289655387401581 - trainLoss: 0.40915966033935547\n",
      "cnt: 0 - valLoss: 0.4289650619029999 - trainLoss: 0.4091581106185913\n",
      "cnt: 0 - valLoss: 0.4289645850658417 - trainLoss: 0.40915656089782715\n",
      "cnt: 0 - valLoss: 0.4289640784263611 - trainLoss: 0.4091550409793854\n",
      "cnt: 0 - valLoss: 0.4289636015892029 - trainLoss: 0.4091534912586212\n",
      "cnt: 0 - valLoss: 0.4289631247520447 - trainLoss: 0.40915200114250183\n",
      "cnt: 0 - valLoss: 0.4289626181125641 - trainLoss: 0.4091504216194153\n",
      "cnt: 0 - valLoss: 0.42896220088005066 - trainLoss: 0.4091489017009735\n",
      "cnt: 0 - valLoss: 0.4289616644382477 - trainLoss: 0.40914738178253174\n",
      "cnt: 0 - valLoss: 0.42896124720573425 - trainLoss: 0.40914586186408997\n",
      "cnt: 0 - valLoss: 0.42896074056625366 - trainLoss: 0.4091443419456482\n",
      "cnt: 0 - valLoss: 0.42896026372909546 - trainLoss: 0.40914279222488403\n",
      "cnt: 0 - valLoss: 0.42895978689193726 - trainLoss: 0.4091412425041199\n",
      "cnt: 0 - valLoss: 0.42895928025245667 - trainLoss: 0.4091397225856781\n",
      "cnt: 0 - valLoss: 0.42895880341529846 - trainLoss: 0.40913817286491394\n",
      "cnt: 0 - valLoss: 0.42895832657814026 - trainLoss: 0.4091366231441498\n",
      "cnt: 0 - valLoss: 0.42895784974098206 - trainLoss: 0.4091351330280304\n",
      "cnt: 0 - valLoss: 0.42895740270614624 - trainLoss: 0.40913358330726624\n",
      "cnt: 0 - valLoss: 0.4289569556713104 - trainLoss: 0.40913206338882446\n",
      "cnt: 0 - valLoss: 0.42895644903182983 - trainLoss: 0.4091305136680603\n",
      "cnt: 0 - valLoss: 0.42895594239234924 - trainLoss: 0.40912896394729614\n",
      "cnt: 0 - valLoss: 0.4289554953575134 - trainLoss: 0.409127414226532\n",
      "cnt: 0 - valLoss: 0.42895498871803284 - trainLoss: 0.4091259241104126\n",
      "cnt: 0 - valLoss: 0.4289545714855194 - trainLoss: 0.40912437438964844\n",
      "cnt: 0 - valLoss: 0.42895403504371643 - trainLoss: 0.40912285447120667\n",
      "cnt: 0 - valLoss: 0.428953617811203 - trainLoss: 0.4091213345527649\n",
      "cnt: 0 - valLoss: 0.4289531111717224 - trainLoss: 0.40911975502967834\n",
      "cnt: 0 - valLoss: 0.4289526343345642 - trainLoss: 0.4091182351112366\n",
      "cnt: 0 - valLoss: 0.428952157497406 - trainLoss: 0.4091167151927948\n",
      "cnt: 0 - valLoss: 0.4289516806602478 - trainLoss: 0.409115195274353\n",
      "cnt: 0 - valLoss: 0.428951233625412 - trainLoss: 0.40911364555358887\n",
      "cnt: 0 - valLoss: 0.428950697183609 - trainLoss: 0.4091121554374695\n",
      "cnt: 0 - valLoss: 0.4289502799510956 - trainLoss: 0.40911057591438293\n",
      "cnt: 0 - valLoss: 0.428949773311615 - trainLoss: 0.40910905599594116\n",
      "cnt: 0 - valLoss: 0.4289493262767792 - trainLoss: 0.4091075360774994\n",
      "cnt: 0 - valLoss: 0.4289488196372986 - trainLoss: 0.40910598635673523\n",
      "cnt: 0 - valLoss: 0.4289483428001404 - trainLoss: 0.40910443663597107\n",
      "cnt: 0 - valLoss: 0.4289478659629822 - trainLoss: 0.4091028869152069\n",
      "cnt: 0 - valLoss: 0.4289473593235016 - trainLoss: 0.4091013967990875\n",
      "cnt: 0 - valLoss: 0.42894694209098816 - trainLoss: 0.40909987688064575\n",
      "cnt: 0 - valLoss: 0.42894646525382996 - trainLoss: 0.4090983271598816\n",
      "cnt: 0 - valLoss: 0.42894598841667175 - trainLoss: 0.4090968370437622\n",
      "cnt: 0 - valLoss: 0.42894551157951355 - trainLoss: 0.40909522771835327\n",
      "cnt: 0 - valLoss: 0.42894503474235535 - trainLoss: 0.4090937376022339\n",
      "cnt: 0 - valLoss: 0.42894452810287476 - trainLoss: 0.4090921878814697\n",
      "cnt: 0 - valLoss: 0.42894411087036133 - trainLoss: 0.40909066796302795\n",
      "cnt: 0 - valLoss: 0.42894360423088074 - trainLoss: 0.4090891182422638\n",
      "cnt: 0 - valLoss: 0.42894312739372253 - trainLoss: 0.4090876281261444\n",
      "cnt: 0 - valLoss: 0.4289427697658539 - trainLoss: 0.40908604860305786\n",
      "cnt: 0 - valLoss: 0.42894238233566284 - trainLoss: 0.4090845286846161\n",
      "cnt: 0 - valLoss: 0.4289420545101166 - trainLoss: 0.4090830385684967\n",
      "cnt: 0 - valLoss: 0.42894166707992554 - trainLoss: 0.40908148884773254\n",
      "cnt: 0 - valLoss: 0.4289413094520569 - trainLoss: 0.40907996892929077\n",
      "cnt: 0 - valLoss: 0.42894092202186584 - trainLoss: 0.4090784192085266\n",
      "cnt: 0 - valLoss: 0.4289405941963196 - trainLoss: 0.40907686948776245\n",
      "cnt: 0 - valLoss: 0.42894017696380615 - trainLoss: 0.40907537937164307\n",
      "cnt: 0 - valLoss: 0.4289398193359375 - trainLoss: 0.4090738892555237\n",
      "cnt: 0 - valLoss: 0.42893946170806885 - trainLoss: 0.4090723395347595\n",
      "cnt: 0 - valLoss: 0.4289391040802002 - trainLoss: 0.40907078981399536\n",
      "cnt: 0 - valLoss: 0.42893871665000916 - trainLoss: 0.4090692698955536\n",
      "cnt: 0 - valLoss: 0.4289383590221405 - trainLoss: 0.40906772017478943\n",
      "cnt: 0 - valLoss: 0.42893797159194946 - trainLoss: 0.40906623005867004\n",
      "cnt: 0 - valLoss: 0.4289375841617584 - trainLoss: 0.40906471014022827\n",
      "cnt: 0 - valLoss: 0.42893725633621216 - trainLoss: 0.4090631902217865\n",
      "cnt: 0 - valLoss: 0.4289368689060211 - trainLoss: 0.40906164050102234\n",
      "cnt: 0 - valLoss: 0.4289364814758301 - trainLoss: 0.40906015038490295\n",
      "cnt: 0 - valLoss: 0.4289361238479614 - trainLoss: 0.4090586006641388\n",
      "cnt: 0 - valLoss: 0.428935706615448 - trainLoss: 0.40905705094337463\n",
      "cnt: 0 - valLoss: 0.42893534898757935 - trainLoss: 0.40905556082725525\n",
      "cnt: 0 - valLoss: 0.4289349615573883 - trainLoss: 0.4090540111064911\n",
      "cnt: 0 - valLoss: 0.42893463373184204 - trainLoss: 0.4090524911880493\n",
      "cnt: 0 - valLoss: 0.428934246301651 - trainLoss: 0.40905094146728516\n",
      "cnt: 0 - valLoss: 0.42893385887145996 - trainLoss: 0.409049391746521\n",
      "cnt: 0 - valLoss: 0.4289334714412689 - trainLoss: 0.4090479016304016\n",
      "cnt: 0 - valLoss: 0.42893314361572266 - trainLoss: 0.4090464115142822\n",
      "cnt: 0 - valLoss: 0.4289327561855316 - trainLoss: 0.40904486179351807\n",
      "cnt: 0 - valLoss: 0.4289323687553406 - trainLoss: 0.4090433120727539\n",
      "cnt: 0 - valLoss: 0.4289320111274719 - trainLoss: 0.40904179215431213\n",
      "cnt: 0 - valLoss: 0.4289315938949585 - trainLoss: 0.40904027223587036\n",
      "cnt: 0 - valLoss: 0.42893123626708984 - trainLoss: 0.4090387523174286\n",
      "cnt: 0 - valLoss: 0.4289308190345764 - trainLoss: 0.40903720259666443\n",
      "cnt: 0 - valLoss: 0.42893046140670776 - trainLoss: 0.40903571248054504\n",
      "cnt: 0 - valLoss: 0.4289301037788391 - trainLoss: 0.4090341627597809\n",
      "cnt: 0 - valLoss: 0.42892971634864807 - trainLoss: 0.4090326130390167\n",
      "cnt: 0 - valLoss: 0.42892932891845703 - trainLoss: 0.40903112292289734\n",
      "cnt: 0 - valLoss: 0.4289289712905884 - trainLoss: 0.4090295732021332\n",
      "cnt: 0 - valLoss: 0.42892858386039734 - trainLoss: 0.4090280532836914\n",
      "cnt: 0 - valLoss: 0.4289281666278839 - trainLoss: 0.40902653336524963\n",
      "cnt: 0 - valLoss: 0.42892780900001526 - trainLoss: 0.40902501344680786\n",
      "cnt: 0 - valLoss: 0.428927481174469 - trainLoss: 0.4090234637260437\n",
      "cnt: 0 - valLoss: 0.4289270341396332 - trainLoss: 0.4090219736099243\n",
      "cnt: 0 - valLoss: 0.4289267063140869 - trainLoss: 0.40902045369148254\n",
      "cnt: 0 - valLoss: 0.4289262890815735 - trainLoss: 0.40901893377304077\n",
      "cnt: 0 - valLoss: 0.4289259612560272 - trainLoss: 0.4090173840522766\n",
      "cnt: 0 - valLoss: 0.4289255440235138 - trainLoss: 0.40901583433151245\n",
      "cnt: 0 - valLoss: 0.42892515659332275 - trainLoss: 0.40901434421539307\n",
      "cnt: 0 - valLoss: 0.4289248287677765 - trainLoss: 0.4090127944946289\n",
      "cnt: 0 - valLoss: 0.4289243817329407 - trainLoss: 0.4090113043785095\n",
      "cnt: 0 - valLoss: 0.4289240539073944 - trainLoss: 0.40900975465774536\n",
      "cnt: 0 - valLoss: 0.42892366647720337 - trainLoss: 0.4090082347393036\n",
      "cnt: 0 - valLoss: 0.42892327904701233 - trainLoss: 0.40900668501853943\n",
      "cnt: 0 - valLoss: 0.4289228916168213 - trainLoss: 0.40900519490242004\n",
      "cnt: 0 - valLoss: 0.42892250418663025 - trainLoss: 0.4090036451816559\n",
      "cnt: 0 - valLoss: 0.4289221167564392 - trainLoss: 0.4090021550655365\n",
      "cnt: 0 - valLoss: 0.42892178893089294 - trainLoss: 0.40900060534477234\n",
      "cnt: 0 - valLoss: 0.4289214015007019 - trainLoss: 0.4089990556240082\n",
      "cnt: 0 - valLoss: 0.42892101407051086 - trainLoss: 0.4089975357055664\n",
      "cnt: 0 - valLoss: 0.4289206266403198 - trainLoss: 0.40899601578712463\n",
      "cnt: 0 - valLoss: 0.4289202392101288 - trainLoss: 0.40899449586868286\n",
      "cnt: 0 - valLoss: 0.42891985177993774 - trainLoss: 0.4089929759502411\n",
      "cnt: 0 - valLoss: 0.4289194643497467 - trainLoss: 0.4089914560317993\n",
      "cnt: 0 - valLoss: 0.42891907691955566 - trainLoss: 0.40898990631103516\n",
      "cnt: 0 - valLoss: 0.4289186894893646 - trainLoss: 0.40898841619491577\n",
      "cnt: 0 - valLoss: 0.4289183020591736 - trainLoss: 0.4089868664741516\n",
      "cnt: 0 - valLoss: 0.42891791462898254 - trainLoss: 0.4089853763580322\n",
      "cnt: 0 - valLoss: 0.4289175868034363 - trainLoss: 0.40898382663726807\n",
      "cnt: 0 - valLoss: 0.42891719937324524 - trainLoss: 0.4089822769165039\n",
      "cnt: 0 - valLoss: 0.4289167523384094 - trainLoss: 0.4089807868003845\n",
      "cnt: 0 - valLoss: 0.42891642451286316 - trainLoss: 0.40897929668426514\n",
      "cnt: 0 - valLoss: 0.4289160370826721 - trainLoss: 0.408977746963501\n",
      "cnt: 0 - valLoss: 0.4289156496524811 - trainLoss: 0.4089761972427368\n",
      "cnt: 0 - valLoss: 0.42891526222229004 - trainLoss: 0.40897467732429504\n",
      "cnt: 0 - valLoss: 0.428914874792099 - trainLoss: 0.4089731276035309\n",
      "cnt: 0 - valLoss: 0.42891448736190796 - trainLoss: 0.4089716374874115\n",
      "cnt: 0 - valLoss: 0.4289140999317169 - trainLoss: 0.4089701175689697\n",
      "cnt: 0 - valLoss: 0.42891377210617065 - trainLoss: 0.40896859765052795\n",
      "cnt: 0 - valLoss: 0.4289133846759796 - trainLoss: 0.4089670479297638\n",
      "cnt: 0 - valLoss: 0.4289129972457886 - trainLoss: 0.40896549820899963\n",
      "cnt: 0 - valLoss: 0.42891255021095276 - trainLoss: 0.40896397829055786\n",
      "cnt: 0 - valLoss: 0.4289122223854065 - trainLoss: 0.4089624583721161\n",
      "cnt: 0 - valLoss: 0.4289117753505707 - trainLoss: 0.4089609682559967\n",
      "cnt: 0 - valLoss: 0.42891138792037964 - trainLoss: 0.40895941853523254\n",
      "cnt: 0 - valLoss: 0.428911030292511 - trainLoss: 0.40895789861679077\n",
      "cnt: 0 - valLoss: 0.42891064286231995 - trainLoss: 0.408956378698349\n",
      "cnt: 0 - valLoss: 0.4289102554321289 - trainLoss: 0.4089548587799072\n",
      "cnt: 0 - valLoss: 0.42890989780426025 - trainLoss: 0.40895330905914307\n",
      "cnt: 0 - valLoss: 0.4289094805717468 - trainLoss: 0.4089518189430237\n",
      "cnt: 0 - valLoss: 0.4289090931415558 - trainLoss: 0.4089502692222595\n",
      "cnt: 0 - valLoss: 0.42890870571136475 - trainLoss: 0.40894877910614014\n",
      "cnt: 0 - valLoss: 0.4289083182811737 - trainLoss: 0.408947229385376\n",
      "cnt: 0 - valLoss: 0.42890793085098267 - trainLoss: 0.4089457392692566\n",
      "cnt: 0 - valLoss: 0.42890751361846924 - trainLoss: 0.40894418954849243\n",
      "cnt: 0 - valLoss: 0.4289071261882782 - trainLoss: 0.40894269943237305\n",
      "cnt: 0 - valLoss: 0.42890673875808716 - trainLoss: 0.4089411199092865\n",
      "cnt: 0 - valLoss: 0.4289063513278961 - trainLoss: 0.4089396595954895\n",
      "cnt: 0 - valLoss: 0.4289059638977051 - trainLoss: 0.40893810987472534\n",
      "cnt: 0 - valLoss: 0.42890551686286926 - trainLoss: 0.4089365601539612\n",
      "cnt: 0 - valLoss: 0.4289051294326782 - trainLoss: 0.4089350402355194\n",
      "cnt: 0 - valLoss: 0.42890480160713196 - trainLoss: 0.40893352031707764\n",
      "cnt: 0 - valLoss: 0.42890438437461853 - trainLoss: 0.40893203020095825\n",
      "cnt: 0 - valLoss: 0.4289039373397827 - trainLoss: 0.4089305102825165\n",
      "cnt: 0 - valLoss: 0.42890360951423645 - trainLoss: 0.4089289605617523\n",
      "cnt: 0 - valLoss: 0.4289032220840454 - trainLoss: 0.40892747044563293\n",
      "cnt: 0 - valLoss: 0.4289027750492096 - trainLoss: 0.4089259207248688\n",
      "cnt: 0 - valLoss: 0.42890244722366333 - trainLoss: 0.4089243710041046\n",
      "cnt: 0 - valLoss: 0.4289020299911499 - trainLoss: 0.40892288088798523\n",
      "cnt: 0 - valLoss: 0.42890164256095886 - trainLoss: 0.40892133116722107\n",
      "cnt: 0 - valLoss: 0.4289012551307678 - trainLoss: 0.4089197814464569\n",
      "cnt: 0 - valLoss: 0.4289008677005768 - trainLoss: 0.4089183509349823\n",
      "cnt: 0 - valLoss: 0.42890048027038574 - trainLoss: 0.40891680121421814\n",
      "cnt: 0 - valLoss: 0.4289000332355499 - trainLoss: 0.408915251493454\n",
      "cnt: 0 - valLoss: 0.4288996458053589 - trainLoss: 0.4089137315750122\n",
      "cnt: 0 - valLoss: 0.42889925837516785 - trainLoss: 0.40891218185424805\n",
      "cnt: 0 - valLoss: 0.4288988411426544 - trainLoss: 0.40891069173812866\n",
      "cnt: 0 - valLoss: 0.42889851331710815 - trainLoss: 0.4089091420173645\n",
      "cnt: 0 - valLoss: 0.4288981258869171 - trainLoss: 0.4089076519012451\n",
      "cnt: 0 - valLoss: 0.4288977384567261 - trainLoss: 0.40890610218048096\n",
      "cnt: 0 - valLoss: 0.42889729142189026 - trainLoss: 0.4089046120643616\n",
      "cnt: 0 - valLoss: 0.4288969039916992 - trainLoss: 0.40890300273895264\n",
      "cnt: 0 - valLoss: 0.4288965165615082 - trainLoss: 0.408901572227478\n",
      "cnt: 0 - valLoss: 0.42889609932899475 - trainLoss: 0.40890002250671387\n",
      "cnt: 0 - valLoss: 0.4288957715034485 - trainLoss: 0.4088984727859497\n",
      "cnt: 0 - valLoss: 0.42889532446861267 - trainLoss: 0.4088969826698303\n",
      "cnt: 0 - valLoss: 0.42889493703842163 - trainLoss: 0.40889549255371094\n",
      "cnt: 0 - valLoss: 0.4288945496082306 - trainLoss: 0.4088939428329468\n",
      "cnt: 0 - valLoss: 0.42889416217803955 - trainLoss: 0.4088923931121826\n",
      "cnt: 0 - valLoss: 0.4288937747478485 - trainLoss: 0.40889090299606323\n",
      "cnt: 0 - valLoss: 0.42889338731765747 - trainLoss: 0.4088893532752991\n",
      "cnt: 0 - valLoss: 0.42889299988746643 - trainLoss: 0.4088878631591797\n",
      "cnt: 0 - valLoss: 0.4288926422595978 - trainLoss: 0.4088863134384155\n",
      "cnt: 0 - valLoss: 0.42889225482940674 - trainLoss: 0.40888479351997375\n",
      "cnt: 0 - valLoss: 0.4288918077945709 - trainLoss: 0.408883273601532\n",
      "cnt: 0 - valLoss: 0.4288914203643799 - trainLoss: 0.4088817834854126\n",
      "cnt: 0 - valLoss: 0.42889103293418884 - trainLoss: 0.40888023376464844\n",
      "cnt: 0 - valLoss: 0.4288906157016754 - trainLoss: 0.40887871384620667\n",
      "cnt: 0 - valLoss: 0.4288902282714844 - trainLoss: 0.4088771939277649\n",
      "cnt: 0 - valLoss: 0.42888984084129333 - trainLoss: 0.4088756740093231\n",
      "cnt: 0 - valLoss: 0.4288894534111023 - trainLoss: 0.40887418389320374\n",
      "cnt: 0 - valLoss: 0.42888906598091125 - trainLoss: 0.4088726341724396\n",
      "cnt: 0 - valLoss: 0.4288886487483978 - trainLoss: 0.4088710844516754\n",
      "cnt: 0 - valLoss: 0.4288882613182068 - trainLoss: 0.40886959433555603\n",
      "cnt: 0 - valLoss: 0.42888787388801575 - trainLoss: 0.40886804461479187\n",
      "cnt: 0 - valLoss: 0.4288874566555023 - trainLoss: 0.4088664948940277\n",
      "cnt: 0 - valLoss: 0.42888709902763367 - trainLoss: 0.4088650047779083\n",
      "cnt: 0 - valLoss: 0.42888668179512024 - trainLoss: 0.40886351466178894\n",
      "cnt: 0 - valLoss: 0.4288862943649292 - trainLoss: 0.4088619351387024\n",
      "cnt: 0 - valLoss: 0.42888590693473816 - trainLoss: 0.4088604748249054\n",
      "cnt: 0 - valLoss: 0.42888545989990234 - trainLoss: 0.40885892510414124\n",
      "cnt: 0 - valLoss: 0.4288850724697113 - trainLoss: 0.40885740518569946\n",
      "cnt: 0 - valLoss: 0.4288846552371979 - trainLoss: 0.4088558852672577\n",
      "cnt: 0 - valLoss: 0.42888426780700684 - trainLoss: 0.4088543951511383\n",
      "cnt: 0 - valLoss: 0.4288838505744934 - trainLoss: 0.40885284543037415\n",
      "cnt: 0 - valLoss: 0.42888346314430237 - trainLoss: 0.4088513255119324\n",
      "cnt: 0 - valLoss: 0.42888307571411133 - trainLoss: 0.4088498055934906\n",
      "cnt: 0 - valLoss: 0.4288826286792755 - trainLoss: 0.40884828567504883\n",
      "cnt: 0 - valLoss: 0.42888230085372925 - trainLoss: 0.40884676575660706\n",
      "cnt: 0 - valLoss: 0.4288818836212158 - trainLoss: 0.4088452458381653\n",
      "cnt: 0 - valLoss: 0.4288814961910248 - trainLoss: 0.4088437259197235\n",
      "cnt: 0 - valLoss: 0.42888104915618896 - trainLoss: 0.40884220600128174\n",
      "cnt: 0 - valLoss: 0.4288806617259979 - trainLoss: 0.40884068608283997\n",
      "cnt: 0 - valLoss: 0.4288802742958069 - trainLoss: 0.4088391661643982\n",
      "cnt: 0 - valLoss: 0.42887985706329346 - trainLoss: 0.40883761644363403\n",
      "cnt: 0 - valLoss: 0.4288794696331024 - trainLoss: 0.40883615612983704\n",
      "cnt: 0 - valLoss: 0.428879052400589 - trainLoss: 0.4088346064090729\n",
      "cnt: 0 - valLoss: 0.42887866497039795 - trainLoss: 0.4088330864906311\n",
      "cnt: 0 - valLoss: 0.42887821793556213 - trainLoss: 0.40883153676986694\n",
      "cnt: 0 - valLoss: 0.4288778305053711 - trainLoss: 0.40883004665374756\n",
      "cnt: 0 - valLoss: 0.42887744307518005 - trainLoss: 0.4088285565376282\n",
      "cnt: 0 - valLoss: 0.428877055644989 - trainLoss: 0.408827006816864\n",
      "cnt: 0 - valLoss: 0.4288766384124756 - trainLoss: 0.40882551670074463\n",
      "cnt: 0 - valLoss: 0.42887625098228455 - trainLoss: 0.40882396697998047\n",
      "cnt: 0 - valLoss: 0.4288758337497711 - trainLoss: 0.4088224768638611\n",
      "cnt: 0 - valLoss: 0.4288754463195801 - trainLoss: 0.4088209271430969\n",
      "cnt: 0 - valLoss: 0.42887499928474426 - trainLoss: 0.40881943702697754\n",
      "cnt: 0 - valLoss: 0.42887458205223083 - trainLoss: 0.4088178873062134\n",
      "cnt: 0 - valLoss: 0.42887425422668457 - trainLoss: 0.408816397190094\n",
      "cnt: 0 - valLoss: 0.42887380719184875 - trainLoss: 0.40881484746932983\n",
      "cnt: 0 - valLoss: 0.4288734197616577 - trainLoss: 0.40881338715553284\n",
      "cnt: 0 - valLoss: 0.4288730323314667 - trainLoss: 0.4088118076324463\n",
      "cnt: 0 - valLoss: 0.42887261509895325 - trainLoss: 0.4088103175163269\n",
      "cnt: 0 - valLoss: 0.4288721978664398 - trainLoss: 0.40880876779556274\n",
      "cnt: 0 - valLoss: 0.4288718104362488 - trainLoss: 0.4088072180747986\n",
      "cnt: 0 - valLoss: 0.42887142300605774 - trainLoss: 0.4088057279586792\n",
      "cnt: 0 - valLoss: 0.4288710355758667 - trainLoss: 0.4088042378425598\n",
      "cnt: 0 - valLoss: 0.4288705885410309 - trainLoss: 0.40880268812179565\n",
      "cnt: 0 - valLoss: 0.42887017130851746 - trainLoss: 0.4088011384010315\n",
      "cnt: 0 - valLoss: 0.4288697838783264 - trainLoss: 0.4087996482849121\n",
      "cnt: 0 - valLoss: 0.428869366645813 - trainLoss: 0.4087981581687927\n",
      "cnt: 0 - valLoss: 0.4288689196109772 - trainLoss: 0.40879660844802856\n",
      "cnt: 0 - valLoss: 0.4288685917854309 - trainLoss: 0.4087950587272644\n",
      "cnt: 0 - valLoss: 0.4288681447505951 - trainLoss: 0.408793568611145\n",
      "cnt: 0 - valLoss: 0.42886772751808167 - trainLoss: 0.40879207849502563\n",
      "cnt: 0 - valLoss: 0.4288673400878906 - trainLoss: 0.4087905287742615\n",
      "cnt: 0 - valLoss: 0.4288669526576996 - trainLoss: 0.4087890386581421\n",
      "cnt: 0 - valLoss: 0.42886653542518616 - trainLoss: 0.40878748893737793\n",
      "cnt: 0 - valLoss: 0.4288661777973175 - trainLoss: 0.40878599882125854\n",
      "cnt: 0 - valLoss: 0.4288657605648041 - trainLoss: 0.4087844491004944\n",
      "cnt: 0 - valLoss: 0.42886531352996826 - trainLoss: 0.408782958984375\n",
      "cnt: 0 - valLoss: 0.42886489629745483 - trainLoss: 0.4087814688682556\n",
      "cnt: 0 - valLoss: 0.4288645088672638 - trainLoss: 0.40877991914749146\n",
      "cnt: 0 - valLoss: 0.42886412143707275 - trainLoss: 0.4087783694267273\n",
      "cnt: 0 - valLoss: 0.4288637042045593 - trainLoss: 0.4087768793106079\n",
      "cnt: 0 - valLoss: 0.4288633167743683 - trainLoss: 0.40877532958984375\n",
      "cnt: 0 - valLoss: 0.42886292934417725 - trainLoss: 0.40877383947372437\n",
      "cnt: 0 - valLoss: 0.42886248230934143 - trainLoss: 0.4087722897529602\n",
      "cnt: 0 - valLoss: 0.428862065076828 - trainLoss: 0.4087708592414856\n",
      "cnt: 0 - valLoss: 0.42886167764663696 - trainLoss: 0.40876930952072144\n",
      "cnt: 0 - valLoss: 0.42886126041412354 - trainLoss: 0.4087677597999573\n",
      "cnt: 0 - valLoss: 0.4288608729839325 - trainLoss: 0.4087662696838379\n",
      "cnt: 0 - valLoss: 0.4288604259490967 - trainLoss: 0.4087647795677185\n",
      "cnt: 0 - valLoss: 0.42886000871658325 - trainLoss: 0.40876322984695435\n",
      "cnt: 0 - valLoss: 0.4288596212863922 - trainLoss: 0.40876173973083496\n",
      "cnt: 0 - valLoss: 0.42885923385620117 - trainLoss: 0.4087602198123932\n",
      "cnt: 0 - valLoss: 0.42885881662368774 - trainLoss: 0.4087586998939514\n",
      "cnt: 0 - valLoss: 0.42885836958885193 - trainLoss: 0.40875715017318726\n",
      "cnt: 0 - valLoss: 0.4288579523563385 - trainLoss: 0.40875566005706787\n",
      "cnt: 0 - valLoss: 0.42885756492614746 - trainLoss: 0.4087541103363037\n",
      "cnt: 0 - valLoss: 0.4288571774959564 - trainLoss: 0.4087526202201843\n",
      "cnt: 0 - valLoss: 0.428856760263443 - trainLoss: 0.40875107049942017\n",
      "cnt: 0 - valLoss: 0.4288563132286072 - trainLoss: 0.408749520778656\n",
      "cnt: 0 - valLoss: 0.42885592579841614 - trainLoss: 0.4087480902671814\n",
      "cnt: 0 - valLoss: 0.4288555085659027 - trainLoss: 0.40874654054641724\n",
      "cnt: 0 - valLoss: 0.42885512113571167 - trainLoss: 0.40874505043029785\n",
      "cnt: 0 - valLoss: 0.42885470390319824 - trainLoss: 0.4087435305118561\n",
      "cnt: 0 - valLoss: 0.4288543164730072 - trainLoss: 0.4087420105934143\n",
      "cnt: 0 - valLoss: 0.4288538694381714 - trainLoss: 0.40874046087265015\n",
      "cnt: 0 - valLoss: 0.42885345220565796 - trainLoss: 0.40873897075653076\n",
      "cnt: 0 - valLoss: 0.4288530647754669 - trainLoss: 0.408737450838089\n",
      "cnt: 0 - valLoss: 0.4288526475429535 - trainLoss: 0.4087359309196472\n",
      "cnt: 0 - valLoss: 0.42885226011276245 - trainLoss: 0.40873441100120544\n",
      "cnt: 0 - valLoss: 0.4288518726825714 - trainLoss: 0.40873289108276367\n",
      "cnt: 0 - valLoss: 0.4288514256477356 - trainLoss: 0.4087313711643219\n",
      "cnt: 0 - valLoss: 0.42885100841522217 - trainLoss: 0.4087298512458801\n",
      "cnt: 0 - valLoss: 0.42885059118270874 - trainLoss: 0.40872833132743835\n",
      "cnt: 0 - valLoss: 0.4288501441478729 - trainLoss: 0.40872684121131897\n",
      "cnt: 0 - valLoss: 0.42884981632232666 - trainLoss: 0.4087252914905548\n",
      "cnt: 0 - valLoss: 0.42884933948516846 - trainLoss: 0.4087238013744354\n",
      "cnt: 0 - valLoss: 0.4288489520549774 - trainLoss: 0.40872231125831604\n",
      "cnt: 0 - valLoss: 0.428848534822464 - trainLoss: 0.4087207615375519\n",
      "cnt: 0 - valLoss: 0.42884814739227295 - trainLoss: 0.4087192416191101\n",
      "cnt: 0 - valLoss: 0.4288477599620819 - trainLoss: 0.40871772170066833\n",
      "cnt: 0 - valLoss: 0.4288473129272461 - trainLoss: 0.40871623158454895\n",
      "cnt: 0 - valLoss: 0.42884689569473267 - trainLoss: 0.4087146818637848\n",
      "cnt: 0 - valLoss: 0.42884647846221924 - trainLoss: 0.408713161945343\n",
      "cnt: 0 - valLoss: 0.4288460314273834 - trainLoss: 0.408711701631546\n",
      "cnt: 0 - valLoss: 0.4288456439971924 - trainLoss: 0.40871015191078186\n",
      "cnt: 0 - valLoss: 0.42884525656700134 - trainLoss: 0.4087086021900177\n",
      "cnt: 0 - valLoss: 0.4288448393344879 - trainLoss: 0.4087071120738983\n",
      "cnt: 0 - valLoss: 0.4288444221019745 - trainLoss: 0.40870562195777893\n",
      "cnt: 0 - valLoss: 0.42884397506713867 - trainLoss: 0.40870407223701477\n",
      "cnt: 0 - valLoss: 0.42884355783462524 - trainLoss: 0.4087025821208954\n",
      "cnt: 0 - valLoss: 0.42884311079978943 - trainLoss: 0.408701092004776\n",
      "cnt: 0 - valLoss: 0.4288427233695984 - trainLoss: 0.40869954228401184\n",
      "cnt: 0 - valLoss: 0.42884236574172974 - trainLoss: 0.40869805216789246\n",
      "cnt: 0 - valLoss: 0.4288419187068939 - trainLoss: 0.4086965024471283\n",
      "cnt: 0 - valLoss: 0.4288415014743805 - trainLoss: 0.4086950123310089\n",
      "cnt: 0 - valLoss: 0.4288410544395447 - trainLoss: 0.40869346261024475\n",
      "cnt: 0 - valLoss: 0.42884063720703125 - trainLoss: 0.40869197249412537\n",
      "cnt: 0 - valLoss: 0.4288402497768402 - trainLoss: 0.4086904525756836\n",
      "cnt: 0 - valLoss: 0.42883986234664917 - trainLoss: 0.4086889326572418\n",
      "cnt: 0 - valLoss: 0.42883944511413574 - trainLoss: 0.40868741273880005\n",
      "cnt: 0 - valLoss: 0.4288389980792999 - trainLoss: 0.40868592262268066\n",
      "cnt: 0 - valLoss: 0.4288385808467865 - trainLoss: 0.4086843729019165\n",
      "cnt: 0 - valLoss: 0.42883816361427307 - trainLoss: 0.4086828827857971\n",
      "cnt: 0 - valLoss: 0.42883777618408203 - trainLoss: 0.40868133306503296\n",
      "cnt: 0 - valLoss: 0.428837388753891 - trainLoss: 0.4086798429489136\n",
      "cnt: 0 - valLoss: 0.4288369417190552 - trainLoss: 0.4086783230304718\n",
      "cnt: 0 - valLoss: 0.42883652448654175 - trainLoss: 0.40867680311203003\n",
      "cnt: 0 - valLoss: 0.4288361072540283 - trainLoss: 0.40867531299591064\n",
      "cnt: 0 - valLoss: 0.4288356602191925 - trainLoss: 0.4086737632751465\n",
      "cnt: 0 - valLoss: 0.42883527278900146 - trainLoss: 0.4086722731590271\n",
      "cnt: 0 - valLoss: 0.42883485555648804 - trainLoss: 0.4086707830429077\n",
      "cnt: 0 - valLoss: 0.428834468126297 - trainLoss: 0.40866923332214355\n",
      "cnt: 0 - valLoss: 0.42883405089378357 - trainLoss: 0.40866774320602417\n",
      "cnt: 0 - valLoss: 0.42883360385894775 - trainLoss: 0.4086662530899048\n",
      "cnt: 0 - valLoss: 0.4288331866264343 - trainLoss: 0.4086647033691406\n",
      "cnt: 0 - valLoss: 0.4288327395915985 - trainLoss: 0.40866315364837646\n",
      "cnt: 0 - valLoss: 0.4288323223590851 - trainLoss: 0.40866169333457947\n",
      "cnt: 0 - valLoss: 0.42883193492889404 - trainLoss: 0.4086601734161377\n",
      "cnt: 0 - valLoss: 0.428831547498703 - trainLoss: 0.40865862369537354\n",
      "cnt: 0 - valLoss: 0.4288311302661896 - trainLoss: 0.40865713357925415\n",
      "cnt: 0 - valLoss: 0.42883068323135376 - trainLoss: 0.40865558385849\n",
      "cnt: 0 - valLoss: 0.42883026599884033 - trainLoss: 0.408654123544693\n",
      "cnt: 0 - valLoss: 0.4288298785686493 - trainLoss: 0.40865257382392883\n",
      "cnt: 0 - valLoss: 0.42882946133613586 - trainLoss: 0.40865108370780945\n",
      "cnt: 0 - valLoss: 0.4288290739059448 - trainLoss: 0.40864959359169006\n",
      "cnt: 0 - valLoss: 0.428828626871109 - trainLoss: 0.4086481034755707\n",
      "cnt: 0 - valLoss: 0.4288282096385956 - trainLoss: 0.4086465537548065\n",
      "cnt: 0 - valLoss: 0.42882779240608215 - trainLoss: 0.40864500403404236\n",
      "cnt: 0 - valLoss: 0.4288274049758911 - trainLoss: 0.408643513917923\n",
      "cnt: 0 - valLoss: 0.4288269579410553 - trainLoss: 0.4086420238018036\n",
      "cnt: 0 - valLoss: 0.42882657051086426 - trainLoss: 0.40864047408103943\n",
      "cnt: 0 - valLoss: 0.42882615327835083 - trainLoss: 0.40863898396492004\n",
      "cnt: 0 - valLoss: 0.4288257360458374 - trainLoss: 0.40863746404647827\n",
      "cnt: 0 - valLoss: 0.4288252890110016 - trainLoss: 0.4086359441280365\n",
      "cnt: 0 - valLoss: 0.42882487177848816 - trainLoss: 0.4086344242095947\n",
      "cnt: 0 - valLoss: 0.4288244843482971 - trainLoss: 0.40863293409347534\n",
      "cnt: 0 - valLoss: 0.4288240075111389 - trainLoss: 0.4086313843727112\n",
      "cnt: 0 - valLoss: 0.42882364988327026 - trainLoss: 0.4086298942565918\n",
      "cnt: 0 - valLoss: 0.42882323265075684 - trainLoss: 0.4086284041404724\n",
      "cnt: 0 - valLoss: 0.4288228154182434 - trainLoss: 0.40862685441970825\n",
      "cnt: 0 - valLoss: 0.4288223683834076 - trainLoss: 0.40862536430358887\n",
      "cnt: 0 - valLoss: 0.42882195115089417 - trainLoss: 0.4086238741874695\n",
      "cnt: 0 - valLoss: 0.42882153391838074 - trainLoss: 0.4086223244667053\n",
      "cnt: 0 - valLoss: 0.4288211464881897 - trainLoss: 0.40862077474594116\n",
      "cnt: 0 - valLoss: 0.4288206696510315 - trainLoss: 0.40861931443214417\n",
      "cnt: 0 - valLoss: 0.42882028222084045 - trainLoss: 0.4086177945137024\n",
      "cnt: 0 - valLoss: 0.4288198947906494 - trainLoss: 0.4086162745952606\n",
      "cnt: 0 - valLoss: 0.428819477558136 - trainLoss: 0.40861475467681885\n",
      "cnt: 0 - valLoss: 0.42881903052330017 - trainLoss: 0.40861326456069946\n",
      "cnt: 0 - valLoss: 0.42881861329078674 - trainLoss: 0.4086117446422577\n",
      "cnt: 0 - valLoss: 0.4288181662559509 - trainLoss: 0.4086102545261383\n",
      "cnt: 0 - valLoss: 0.42881783843040466 - trainLoss: 0.40860870480537415\n",
      "cnt: 0 - valLoss: 0.42881739139556885 - trainLoss: 0.40860724449157715\n",
      "cnt: 0 - valLoss: 0.42881694436073303 - trainLoss: 0.4086057245731354\n",
      "cnt: 0 - valLoss: 0.428816556930542 - trainLoss: 0.4086041748523712\n",
      "cnt: 0 - valLoss: 0.4288161098957062 - trainLoss: 0.40860268473625183\n",
      "cnt: 0 - valLoss: 0.42881572246551514 - trainLoss: 0.40860119462013245\n",
      "cnt: 0 - valLoss: 0.4288153052330017 - trainLoss: 0.4085996448993683\n",
      "cnt: 0 - valLoss: 0.4288148880004883 - trainLoss: 0.4085981845855713\n",
      "cnt: 0 - valLoss: 0.42881450057029724 - trainLoss: 0.40859663486480713\n",
      "cnt: 0 - valLoss: 0.4288140535354614 - trainLoss: 0.40859511494636536\n",
      "cnt: 0 - valLoss: 0.428813636302948 - trainLoss: 0.4085935950279236\n",
      "cnt: 0 - valLoss: 0.42881321907043457 - trainLoss: 0.4085921049118042\n",
      "cnt: 0 - valLoss: 0.42881277203559875 - trainLoss: 0.40859055519104004\n",
      "cnt: 0 - valLoss: 0.4288123846054077 - trainLoss: 0.40858906507492065\n",
      "cnt: 0 - valLoss: 0.4288119673728943 - trainLoss: 0.40858757495880127\n",
      "cnt: 0 - valLoss: 0.42881157994270325 - trainLoss: 0.4085860252380371\n",
      "cnt: 0 - valLoss: 0.42881110310554504 - trainLoss: 0.4085845351219177\n",
      "cnt: 0 - valLoss: 0.428810715675354 - trainLoss: 0.40858304500579834\n",
      "cnt: 0 - valLoss: 0.4288102984428406 - trainLoss: 0.4085814952850342\n",
      "cnt: 0 - valLoss: 0.42880985140800476 - trainLoss: 0.4085800051689148\n",
      "cnt: 0 - valLoss: 0.4288094639778137 - trainLoss: 0.408578485250473\n",
      "cnt: 0 - valLoss: 0.4288090467453003 - trainLoss: 0.40857699513435364\n",
      "cnt: 0 - valLoss: 0.42880865931510925 - trainLoss: 0.40857550501823425\n",
      "cnt: 0 - valLoss: 0.42880818247795105 - trainLoss: 0.4085739552974701\n",
      "cnt: 0 - valLoss: 0.42880779504776 - trainLoss: 0.4085724651813507\n",
      "cnt: 0 - valLoss: 0.4288073778152466 - trainLoss: 0.40857091546058655\n",
      "cnt: 0 - valLoss: 0.42880696058273315 - trainLoss: 0.40856942534446716\n",
      "cnt: 0 - valLoss: 0.42880651354789734 - trainLoss: 0.4085679352283478\n",
      "cnt: 0 - valLoss: 0.4288060963153839 - trainLoss: 0.408566415309906\n",
      "cnt: 0 - valLoss: 0.4288056492805481 - trainLoss: 0.40856489539146423\n",
      "cnt: 0 - valLoss: 0.42880526185035706 - trainLoss: 0.4085633456707001\n",
      "cnt: 0 - valLoss: 0.42880484461784363 - trainLoss: 0.4085618853569031\n",
      "cnt: 0 - valLoss: 0.4288044571876526 - trainLoss: 0.4085603952407837\n",
      "cnt: 0 - valLoss: 0.4288039803504944 - trainLoss: 0.4085589051246643\n",
      "cnt: 0 - valLoss: 0.42880359292030334 - trainLoss: 0.40855735540390015\n",
      "cnt: 0 - valLoss: 0.4288031756877899 - trainLoss: 0.40855589509010315\n",
      "cnt: 0 - valLoss: 0.4288027584552765 - trainLoss: 0.408554345369339\n",
      "cnt: 0 - valLoss: 0.4288023114204407 - trainLoss: 0.4085528254508972\n",
      "cnt: 0 - valLoss: 0.42880189418792725 - trainLoss: 0.4085513651371002\n",
      "cnt: 0 - valLoss: 0.42880144715309143 - trainLoss: 0.40854978561401367\n",
      "cnt: 0 - valLoss: 0.428801029920578 - trainLoss: 0.4085482656955719\n",
      "cnt: 0 - valLoss: 0.42880064249038696 - trainLoss: 0.4085468351840973\n",
      "cnt: 0 - valLoss: 0.42880016565322876 - trainLoss: 0.40854528546333313\n",
      "cnt: 0 - valLoss: 0.4287997782230377 - trainLoss: 0.40854373574256897\n",
      "cnt: 0 - valLoss: 0.4287993609905243 - trainLoss: 0.4085422456264496\n",
      "cnt: 0 - valLoss: 0.42879894375801086 - trainLoss: 0.4085407555103302\n",
      "cnt: 0 - valLoss: 0.42879849672317505 - trainLoss: 0.4085392951965332\n",
      "cnt: 0 - valLoss: 0.4287980794906616 - trainLoss: 0.40853774547576904\n",
      "cnt: 0 - valLoss: 0.4287976324558258 - trainLoss: 0.4085361957550049\n",
      "cnt: 0 - valLoss: 0.4287972152233124 - trainLoss: 0.4085347652435303\n",
      "cnt: 0 - valLoss: 0.42879682779312134 - trainLoss: 0.4085332155227661\n",
      "cnt: 0 - valLoss: 0.42879635095596313 - trainLoss: 0.40853172540664673\n",
      "cnt: 0 - valLoss: 0.4287959635257721 - trainLoss: 0.40853020548820496\n",
      "cnt: 0 - valLoss: 0.42879554629325867 - trainLoss: 0.4085286855697632\n",
      "cnt: 0 - valLoss: 0.42879512906074524 - trainLoss: 0.4085272252559662\n",
      "cnt: 0 - valLoss: 0.4287946820259094 - trainLoss: 0.408525675535202\n",
      "cnt: 0 - valLoss: 0.4287942945957184 - trainLoss: 0.40852412581443787\n",
      "cnt: 0 - valLoss: 0.4287938177585602 - trainLoss: 0.4085226356983185\n",
      "cnt: 0 - valLoss: 0.42879346013069153 - trainLoss: 0.4085211455821991\n",
      "cnt: 0 - valLoss: 0.4287930130958557 - trainLoss: 0.4085196554660797\n",
      "cnt: 0 - valLoss: 0.4287925958633423 - trainLoss: 0.40851813554763794\n",
      "cnt: 0 - valLoss: 0.42879214882850647 - trainLoss: 0.40851661562919617\n",
      "cnt: 0 - valLoss: 0.42879173159599304 - trainLoss: 0.4085150957107544\n",
      "cnt: 0 - valLoss: 0.428791344165802 - trainLoss: 0.408513605594635\n",
      "cnt: 0 - valLoss: 0.4287908673286438 - trainLoss: 0.40851205587387085\n",
      "cnt: 0 - valLoss: 0.42879045009613037 - trainLoss: 0.40851056575775146\n",
      "cnt: 0 - valLoss: 0.42879006266593933 - trainLoss: 0.4085090756416321\n",
      "cnt: 0 - valLoss: 0.4287896454334259 - trainLoss: 0.4085075855255127\n",
      "cnt: 0 - valLoss: 0.4287891983985901 - trainLoss: 0.40850603580474854\n",
      "cnt: 0 - valLoss: 0.42878878116607666 - trainLoss: 0.40850454568862915\n",
      "cnt: 0 - valLoss: 0.42878833413124084 - trainLoss: 0.40850308537483215\n",
      "cnt: 0 - valLoss: 0.4287879168987274 - trainLoss: 0.408501535654068\n",
      "cnt: 0 - valLoss: 0.428787499666214 - trainLoss: 0.40849998593330383\n",
      "cnt: 0 - valLoss: 0.4287870526313782 - trainLoss: 0.4084985554218292\n",
      "cnt: 0 - valLoss: 0.42878663539886475 - trainLoss: 0.40849700570106506\n",
      "cnt: 0 - valLoss: 0.4287862479686737 - trainLoss: 0.4084955155849457\n",
      "cnt: 0 - valLoss: 0.4287857711315155 - trainLoss: 0.4084939956665039\n",
      "cnt: 0 - valLoss: 0.42878538370132446 - trainLoss: 0.4084925055503845\n",
      "cnt: 0 - valLoss: 0.42878496646881104 - trainLoss: 0.40849101543426514\n",
      "cnt: 0 - valLoss: 0.4287845194339752 - trainLoss: 0.408489465713501\n",
      "cnt: 0 - valLoss: 0.4287841022014618 - trainLoss: 0.4084879755973816\n",
      "cnt: 0 - valLoss: 0.4287836253643036 - trainLoss: 0.4084864854812622\n",
      "cnt: 0 - valLoss: 0.42878323793411255 - trainLoss: 0.40848496556282043\n",
      "cnt: 0 - valLoss: 0.4287828505039215 - trainLoss: 0.40848347544670105\n",
      "cnt: 0 - valLoss: 0.4287824332714081 - trainLoss: 0.4084819257259369\n",
      "cnt: 0 - valLoss: 0.4287819564342499 - trainLoss: 0.4084804654121399\n",
      "cnt: 0 - valLoss: 0.42878153920173645 - trainLoss: 0.4084789454936981\n",
      "cnt: 0 - valLoss: 0.42878109216690063 - trainLoss: 0.40847739577293396\n",
      "cnt: 0 - valLoss: 0.4287807047367096 - trainLoss: 0.4084759056568146\n",
      "cnt: 0 - valLoss: 0.42878028750419617 - trainLoss: 0.4084744155406952\n",
      "cnt: 0 - valLoss: 0.42877987027168274 - trainLoss: 0.4084729552268982\n",
      "cnt: 0 - valLoss: 0.4287794232368469 - trainLoss: 0.40847140550613403\n",
      "cnt: 0 - valLoss: 0.4287790060043335 - trainLoss: 0.40846991539001465\n",
      "cnt: 0 - valLoss: 0.4287785589694977 - trainLoss: 0.4084683954715729\n",
      "cnt: 0 - valLoss: 0.42877820134162903 - trainLoss: 0.4084668755531311\n",
      "cnt: 0 - valLoss: 0.4287777245044708 - trainLoss: 0.4084653854370117\n",
      "cnt: 0 - valLoss: 0.428777277469635 - trainLoss: 0.40846386551856995\n",
      "cnt: 0 - valLoss: 0.4287768602371216 - trainLoss: 0.40846237540245056\n",
      "cnt: 0 - valLoss: 0.42877647280693054 - trainLoss: 0.4084608256816864\n",
      "cnt: 0 - valLoss: 0.4287760853767395 - trainLoss: 0.408459335565567\n",
      "cnt: 0 - valLoss: 0.4287756085395813 - trainLoss: 0.40845784544944763\n",
      "cnt: 0 - valLoss: 0.42877519130706787 - trainLoss: 0.40845632553100586\n",
      "cnt: 0 - valLoss: 0.42877474427223206 - trainLoss: 0.4084548056125641\n",
      "cnt: 0 - valLoss: 0.4287743866443634 - trainLoss: 0.4084533452987671\n",
      "cnt: 0 - valLoss: 0.4287739396095276 - trainLoss: 0.40845179557800293\n",
      "cnt: 0 - valLoss: 0.42877352237701416 - trainLoss: 0.40845033526420593\n",
      "cnt: 0 - valLoss: 0.42877307534217834 - trainLoss: 0.40844881534576416\n",
      "cnt: 0 - valLoss: 0.42877262830734253 - trainLoss: 0.4084472954273224\n",
      "cnt: 0 - valLoss: 0.4287721812725067 - trainLoss: 0.408445805311203\n",
      "cnt: 0 - valLoss: 0.4287717938423157 - trainLoss: 0.40844425559043884\n",
      "cnt: 0 - valLoss: 0.42877137660980225 - trainLoss: 0.40844282507896423\n",
      "cnt: 0 - valLoss: 0.42877089977264404 - trainLoss: 0.4084412753582001\n",
      "cnt: 0 - valLoss: 0.4287704825401306 - trainLoss: 0.4084397852420807\n",
      "cnt: 0 - valLoss: 0.4287700355052948 - trainLoss: 0.4084382653236389\n",
      "cnt: 0 - valLoss: 0.42876961827278137 - trainLoss: 0.40843677520751953\n",
      "cnt: 0 - valLoss: 0.42876920104026794 - trainLoss: 0.40843528509140015\n",
      "cnt: 0 - valLoss: 0.42876875400543213 - trainLoss: 0.40843379497528076\n",
      "cnt: 0 - valLoss: 0.4287683367729187 - trainLoss: 0.408432275056839\n",
      "cnt: 0 - valLoss: 0.4287678599357605 - trainLoss: 0.4084307551383972\n",
      "cnt: 0 - valLoss: 0.42876747250556946 - trainLoss: 0.4084292948246002\n",
      "cnt: 0 - valLoss: 0.42876705527305603 - trainLoss: 0.40842774510383606\n",
      "cnt: 0 - valLoss: 0.4287665784358978 - trainLoss: 0.4084262549877167\n",
      "cnt: 0 - valLoss: 0.428766131401062 - trainLoss: 0.4084247648715973\n",
      "cnt: 0 - valLoss: 0.4287657141685486 - trainLoss: 0.40842321515083313\n",
      "cnt: 0 - valLoss: 0.42876529693603516 - trainLoss: 0.40842175483703613\n",
      "cnt: 0 - valLoss: 0.42876484990119934 - trainLoss: 0.408420205116272\n",
      "cnt: 0 - valLoss: 0.4287644326686859 - trainLoss: 0.4084187150001526\n",
      "cnt: 0 - valLoss: 0.4287639856338501 - trainLoss: 0.4084172248840332\n",
      "cnt: 0 - valLoss: 0.42876356840133667 - trainLoss: 0.4084157347679138\n",
      "cnt: 0 - valLoss: 0.42876315116882324 - trainLoss: 0.40841421484947205\n",
      "cnt: 0 - valLoss: 0.4287627041339874 - trainLoss: 0.4084126949310303\n",
      "cnt: 0 - valLoss: 0.428762286901474 - trainLoss: 0.4084112346172333\n",
      "cnt: 0 - valLoss: 0.42876186966896057 - trainLoss: 0.4084096848964691\n",
      "cnt: 0 - valLoss: 0.42876142263412476 - trainLoss: 0.40840819478034973\n",
      "cnt: 0 - valLoss: 0.42876094579696655 - trainLoss: 0.40840667486190796\n",
      "cnt: 0 - valLoss: 0.4287605583667755 - trainLoss: 0.4084051549434662\n",
      "cnt: 0 - valLoss: 0.4287601411342621 - trainLoss: 0.4084036946296692\n",
      "cnt: 0 - valLoss: 0.42875972390174866 - trainLoss: 0.40840214490890503\n",
      "cnt: 0 - valLoss: 0.42875924706459045 - trainLoss: 0.40840068459510803\n",
      "cnt: 0 - valLoss: 0.42875880002975464 - trainLoss: 0.40839916467666626\n",
      "cnt: 0 - valLoss: 0.4287583529949188 - trainLoss: 0.4083976447582245\n",
      "cnt: 0 - valLoss: 0.4287579655647278 - trainLoss: 0.4083961546421051\n",
      "cnt: 0 - valLoss: 0.42875751852989197 - trainLoss: 0.4083946645259857\n",
      "cnt: 0 - valLoss: 0.42875710129737854 - trainLoss: 0.40839317440986633\n",
      "cnt: 0 - valLoss: 0.42875662446022034 - trainLoss: 0.4083916246891022\n",
      "cnt: 0 - valLoss: 0.4287562072277069 - trainLoss: 0.4083901345729828\n",
      "cnt: 0 - valLoss: 0.4287557601928711 - trainLoss: 0.408388614654541\n",
      "cnt: 0 - valLoss: 0.42875537276268005 - trainLoss: 0.40838712453842163\n",
      "cnt: 0 - valLoss: 0.42875492572784424 - trainLoss: 0.40838563442230225\n",
      "cnt: 0 - valLoss: 0.4287544786930084 - trainLoss: 0.4083840847015381\n",
      "cnt: 0 - valLoss: 0.428754061460495 - trainLoss: 0.4083826243877411\n",
      "cnt: 0 - valLoss: 0.4287536144256592 - trainLoss: 0.4083811342716217\n",
      "cnt: 0 - valLoss: 0.42875319719314575 - trainLoss: 0.4083796441555023\n",
      "cnt: 0 - valLoss: 0.4287527799606323 - trainLoss: 0.40837809443473816\n",
      "cnt: 0 - valLoss: 0.4287523329257965 - trainLoss: 0.40837663412094116\n",
      "cnt: 0 - valLoss: 0.4287519156932831 - trainLoss: 0.4083751142024994\n",
      "cnt: 0 - valLoss: 0.42875149846076965 - trainLoss: 0.40837356448173523\n",
      "cnt: 0 - valLoss: 0.42875105142593384 - trainLoss: 0.40837210416793823\n",
      "cnt: 0 - valLoss: 0.4287506341934204 - trainLoss: 0.4083705544471741\n",
      "cnt: 0 - valLoss: 0.4287501871585846 - trainLoss: 0.40836912393569946\n",
      "cnt: 0 - valLoss: 0.4287497401237488 - trainLoss: 0.4083675742149353\n",
      "cnt: 0 - valLoss: 0.42874929308891296 - trainLoss: 0.4083661139011383\n",
      "cnt: 0 - valLoss: 0.42874887585639954 - trainLoss: 0.40836456418037415\n",
      "cnt: 0 - valLoss: 0.4287484288215637 - trainLoss: 0.40836310386657715\n",
      "cnt: 0 - valLoss: 0.4287479817867279 - trainLoss: 0.4083615839481354\n",
      "cnt: 0 - valLoss: 0.42874759435653687 - trainLoss: 0.4083600640296936\n",
      "cnt: 0 - valLoss: 0.42874711751937866 - trainLoss: 0.4083585739135742\n",
      "cnt: 0 - valLoss: 0.42874667048454285 - trainLoss: 0.40835705399513245\n",
      "cnt: 0 - valLoss: 0.4287463128566742 - trainLoss: 0.40835559368133545\n",
      "cnt: 0 - valLoss: 0.428745836019516 - trainLoss: 0.4083540737628937\n",
      "cnt: 0 - valLoss: 0.4287453889846802 - trainLoss: 0.4083525836467743\n",
      "cnt: 0 - valLoss: 0.42874497175216675 - trainLoss: 0.40835103392601013\n",
      "cnt: 0 - valLoss: 0.4287445545196533 - trainLoss: 0.40834954380989075\n",
      "cnt: 0 - valLoss: 0.4287441074848175 - trainLoss: 0.40834805369377136\n",
      "cnt: 0 - valLoss: 0.4287436902523041 - trainLoss: 0.4083465039730072\n",
      "cnt: 0 - valLoss: 0.42874324321746826 - trainLoss: 0.4083450436592102\n",
      "cnt: 0 - valLoss: 0.42874279618263245 - trainLoss: 0.4083435535430908\n",
      "cnt: 0 - valLoss: 0.4287424087524414 - trainLoss: 0.40834206342697144\n",
      "cnt: 0 - valLoss: 0.4287419617176056 - trainLoss: 0.4083405137062073\n",
      "cnt: 0 - valLoss: 0.4287414848804474 - trainLoss: 0.4083390533924103\n",
      "cnt: 0 - valLoss: 0.42874106764793396 - trainLoss: 0.4083375334739685\n",
      "cnt: 0 - valLoss: 0.4287406802177429 - trainLoss: 0.40833601355552673\n",
      "cnt: 0 - valLoss: 0.4287402033805847 - trainLoss: 0.40833452343940735\n",
      "cnt: 0 - valLoss: 0.4287397861480713 - trainLoss: 0.40833303332328796\n",
      "cnt: 0 - valLoss: 0.42873936891555786 - trainLoss: 0.4083315432071686\n",
      "cnt: 0 - valLoss: 0.42873892188072205 - trainLoss: 0.4083300232887268\n",
      "cnt: 0 - valLoss: 0.42873844504356384 - trainLoss: 0.4083285331726074\n",
      "cnt: 0 - valLoss: 0.4287380576133728 - trainLoss: 0.40832704305648804\n",
      "cnt: 0 - valLoss: 0.4287376403808594 - trainLoss: 0.40832552313804626\n",
      "cnt: 0 - valLoss: 0.42873716354370117 - trainLoss: 0.4083240032196045\n",
      "cnt: 0 - valLoss: 0.42873674631118774 - trainLoss: 0.4083225429058075\n",
      "cnt: 0 - valLoss: 0.42873629927635193 - trainLoss: 0.40832099318504333\n",
      "cnt: 0 - valLoss: 0.4287358820438385 - trainLoss: 0.40831953287124634\n",
      "cnt: 0 - valLoss: 0.4287354648113251 - trainLoss: 0.40831801295280457\n",
      "cnt: 0 - valLoss: 0.42873501777648926 - trainLoss: 0.40831655263900757\n",
      "cnt: 0 - valLoss: 0.42873460054397583 - trainLoss: 0.4083150029182434\n",
      "cnt: 0 - valLoss: 0.4287341237068176 - trainLoss: 0.4083135426044464\n",
      "cnt: 0 - valLoss: 0.4287337064743042 - trainLoss: 0.40831202268600464\n",
      "cnt: 0 - valLoss: 0.4287332594394684 - trainLoss: 0.40831050276756287\n",
      "cnt: 0 - valLoss: 0.42873284220695496 - trainLoss: 0.4083090126514435\n",
      "cnt: 0 - valLoss: 0.42873242497444153 - trainLoss: 0.4083074629306793\n",
      "cnt: 0 - valLoss: 0.4287319779396057 - trainLoss: 0.40830597281455994\n",
      "cnt: 0 - valLoss: 0.4287315011024475 - trainLoss: 0.40830448269844055\n",
      "cnt: 0 - valLoss: 0.4287310838699341 - trainLoss: 0.40830299258232117\n",
      "cnt: 0 - valLoss: 0.42873066663742065 - trainLoss: 0.4083014726638794\n",
      "cnt: 0 - valLoss: 0.42873021960258484 - trainLoss: 0.40829998254776\n",
      "cnt: 0 - valLoss: 0.42872974276542664 - trainLoss: 0.4082984924316406\n",
      "cnt: 0 - valLoss: 0.4287293255329132 - trainLoss: 0.40829700231552124\n",
      "cnt: 0 - valLoss: 0.4287289083003998 - trainLoss: 0.40829548239707947\n",
      "cnt: 0 - valLoss: 0.4287284314632416 - trainLoss: 0.4082939922809601\n",
      "cnt: 0 - valLoss: 0.42872798442840576 - trainLoss: 0.4082925021648407\n",
      "cnt: 0 - valLoss: 0.42872756719589233 - trainLoss: 0.4082909822463989\n",
      "cnt: 0 - valLoss: 0.4287271499633789 - trainLoss: 0.40828949213027954\n",
      "cnt: 0 - valLoss: 0.4287267029285431 - trainLoss: 0.40828797221183777\n",
      "cnt: 0 - valLoss: 0.42872628569602966 - trainLoss: 0.40828651189804077\n",
      "cnt: 0 - valLoss: 0.42872580885887146 - trainLoss: 0.4082849621772766\n",
      "cnt: 0 - valLoss: 0.42872539162635803 - trainLoss: 0.4082834720611572\n",
      "cnt: 0 - valLoss: 0.4287249445915222 - trainLoss: 0.40828195214271545\n",
      "cnt: 0 - valLoss: 0.4287245273590088 - trainLoss: 0.40828052163124084\n",
      "cnt: 0 - valLoss: 0.4287240505218506 - trainLoss: 0.4082790017127991\n",
      "cnt: 0 - valLoss: 0.42872363328933716 - trainLoss: 0.4082774817943573\n",
      "cnt: 0 - valLoss: 0.42872318625450134 - trainLoss: 0.4082759618759155\n",
      "cnt: 0 - valLoss: 0.4287227690219879 - trainLoss: 0.40827447175979614\n",
      "cnt: 0 - valLoss: 0.4287223517894745 - trainLoss: 0.40827298164367676\n",
      "cnt: 0 - valLoss: 0.4287218749523163 - trainLoss: 0.40827152132987976\n",
      "cnt: 0 - valLoss: 0.42872142791748047 - trainLoss: 0.4082699716091156\n",
      "cnt: 0 - valLoss: 0.42872101068496704 - trainLoss: 0.40826845169067383\n",
      "cnt: 0 - valLoss: 0.4287205934524536 - trainLoss: 0.40826699137687683\n",
      "cnt: 0 - valLoss: 0.4287201166152954 - trainLoss: 0.40826550126075745\n",
      "cnt: 0 - valLoss: 0.4287196695804596 - trainLoss: 0.4082639813423157\n",
      "cnt: 0 - valLoss: 0.42871925234794617 - trainLoss: 0.4082624912261963\n",
      "cnt: 0 - valLoss: 0.42871883511543274 - trainLoss: 0.4082610011100769\n",
      "cnt: 0 - valLoss: 0.4287183880805969 - trainLoss: 0.40825945138931274\n",
      "cnt: 0 - valLoss: 0.4287179112434387 - trainLoss: 0.40825799107551575\n",
      "cnt: 0 - valLoss: 0.4287174940109253 - trainLoss: 0.4082564413547516\n",
      "cnt: 0 - valLoss: 0.42871707677841187 - trainLoss: 0.4082549512386322\n",
      "cnt: 0 - valLoss: 0.42871659994125366 - trainLoss: 0.4082534909248352\n",
      "cnt: 0 - valLoss: 0.42871615290641785 - trainLoss: 0.40825197100639343\n",
      "cnt: 0 - valLoss: 0.4287157356739044 - trainLoss: 0.40825045108795166\n",
      "cnt: 0 - valLoss: 0.4287153482437134 - trainLoss: 0.4082489609718323\n",
      "cnt: 0 - valLoss: 0.4287148714065552 - trainLoss: 0.4082474708557129\n",
      "cnt: 0 - valLoss: 0.42871445417404175 - trainLoss: 0.4082459807395935\n",
      "cnt: 0 - valLoss: 0.42871397733688354 - trainLoss: 0.40824446082115173\n",
      "cnt: 0 - valLoss: 0.4287135601043701 - trainLoss: 0.40824297070503235\n",
      "cnt: 0 - valLoss: 0.4287131726741791 - trainLoss: 0.40824148058891296\n",
      "cnt: 0 - valLoss: 0.4287126958370209 - trainLoss: 0.40824002027511597\n",
      "cnt: 0 - valLoss: 0.42871221899986267 - trainLoss: 0.4082384705543518\n",
      "cnt: 0 - valLoss: 0.42871180176734924 - trainLoss: 0.4082370102405548\n",
      "cnt: 0 - valLoss: 0.4287113547325134 - trainLoss: 0.40823549032211304\n",
      "cnt: 0 - valLoss: 0.4287109375 - trainLoss: 0.40823397040367126\n",
      "cnt: 0 - valLoss: 0.4287105202674866 - trainLoss: 0.4082324802875519\n",
      "cnt: 0 - valLoss: 0.42871007323265076 - trainLoss: 0.4082309901714325\n",
      "cnt: 0 - valLoss: 0.42870959639549255 - trainLoss: 0.4082295000553131\n",
      "cnt: 0 - valLoss: 0.4287092089653015 - trainLoss: 0.40822798013687134\n",
      "cnt: 0 - valLoss: 0.4287087619304657 - trainLoss: 0.40822649002075195\n",
      "cnt: 0 - valLoss: 0.4287083148956299 - trainLoss: 0.40822499990463257\n",
      "cnt: 0 - valLoss: 0.42870789766311646 - trainLoss: 0.4082235097885132\n",
      "cnt: 0 - valLoss: 0.428707480430603 - trainLoss: 0.4082219898700714\n",
      "cnt: 0 - valLoss: 0.4287070035934448 - trainLoss: 0.408220499753952\n",
      "cnt: 0 - valLoss: 0.428706556558609 - trainLoss: 0.40821900963783264\n",
      "cnt: 0 - valLoss: 0.4287061393260956 - trainLoss: 0.40821751952171326\n",
      "cnt: 0 - valLoss: 0.42870572209358215 - trainLoss: 0.4082159698009491\n",
      "cnt: 0 - valLoss: 0.42870524525642395 - trainLoss: 0.4082145094871521\n",
      "cnt: 0 - valLoss: 0.4287048578262329 - trainLoss: 0.40821295976638794\n",
      "cnt: 0 - valLoss: 0.4287043809890747 - trainLoss: 0.40821149945259094\n",
      "cnt: 0 - valLoss: 0.4287039637565613 - trainLoss: 0.40821000933647156\n",
      "cnt: 0 - valLoss: 0.4287034869194031 - trainLoss: 0.4082085192203522\n",
      "cnt: 0 - valLoss: 0.42870303988456726 - trainLoss: 0.4082070291042328\n",
      "cnt: 0 - valLoss: 0.42870262265205383 - trainLoss: 0.408205509185791\n",
      "cnt: 0 - valLoss: 0.4287022054195404 - trainLoss: 0.40820398926734924\n",
      "cnt: 0 - valLoss: 0.4287017583847046 - trainLoss: 0.40820252895355225\n",
      "cnt: 0 - valLoss: 0.42870134115219116 - trainLoss: 0.40820103883743286\n",
      "cnt: 0 - valLoss: 0.42870086431503296 - trainLoss: 0.4081995189189911\n",
      "cnt: 0 - valLoss: 0.42870044708251953 - trainLoss: 0.4081980288028717\n",
      "cnt: 0 - valLoss: 0.42869997024536133 - trainLoss: 0.40819647908210754\n",
      "cnt: 0 - valLoss: 0.4286995232105255 - trainLoss: 0.40819498896598816\n",
      "cnt: 0 - valLoss: 0.4286991059780121 - trainLoss: 0.40819352865219116\n",
      "cnt: 0 - valLoss: 0.42869868874549866 - trainLoss: 0.4081920087337494\n",
      "cnt: 0 - valLoss: 0.42869824171066284 - trainLoss: 0.4081905484199524\n",
      "cnt: 0 - valLoss: 0.42869776487350464 - trainLoss: 0.40818899869918823\n",
      "cnt: 0 - valLoss: 0.4286973178386688 - trainLoss: 0.40818753838539124\n",
      "cnt: 0 - valLoss: 0.4286969304084778 - trainLoss: 0.40818604826927185\n",
      "cnt: 0 - valLoss: 0.4286964535713196 - trainLoss: 0.40818455815315247\n",
      "cnt: 0 - valLoss: 0.42869603633880615 - trainLoss: 0.4081830382347107\n",
      "cnt: 0 - valLoss: 0.42869555950164795 - trainLoss: 0.4081815481185913\n",
      "cnt: 0 - valLoss: 0.4286951720714569 - trainLoss: 0.4081800580024719\n",
      "cnt: 0 - valLoss: 0.42869463562965393 - trainLoss: 0.40817856788635254\n",
      "cnt: 0 - valLoss: 0.4286942780017853 - trainLoss: 0.4081770181655884\n",
      "cnt: 0 - valLoss: 0.42869383096694946 - trainLoss: 0.4081755578517914\n",
      "cnt: 0 - valLoss: 0.42869335412979126 - trainLoss: 0.408174067735672\n",
      "cnt: 0 - valLoss: 0.42869293689727783 - trainLoss: 0.4081725776195526\n",
      "cnt: 0 - valLoss: 0.4286925196647644 - trainLoss: 0.40817102789878845\n",
      "cnt: 0 - valLoss: 0.4286920428276062 - trainLoss: 0.40816956758499146\n",
      "cnt: 0 - valLoss: 0.4286915957927704 - trainLoss: 0.40816807746887207\n",
      "cnt: 0 - valLoss: 0.42869117856025696 - trainLoss: 0.4081665575504303\n",
      "cnt: 0 - valLoss: 0.42869070172309875 - trainLoss: 0.4081650674343109\n",
      "cnt: 0 - valLoss: 0.4286902844905853 - trainLoss: 0.40816357731819153\n",
      "cnt: 0 - valLoss: 0.4286898076534271 - trainLoss: 0.40816208720207214\n",
      "cnt: 0 - valLoss: 0.4286894202232361 - trainLoss: 0.40816056728363037\n",
      "cnt: 0 - valLoss: 0.4286889433860779 - trainLoss: 0.4081590473651886\n",
      "cnt: 0 - valLoss: 0.42868855595588684 - trainLoss: 0.4081575870513916\n",
      "cnt: 0 - valLoss: 0.42868807911872864 - trainLoss: 0.4081560969352722\n",
      "cnt: 0 - valLoss: 0.4286876320838928 - trainLoss: 0.40815457701683044\n",
      "cnt: 0 - valLoss: 0.428687185049057 - trainLoss: 0.40815308690071106\n",
      "cnt: 0 - valLoss: 0.4286867678165436 - trainLoss: 0.4081515967845917\n",
      "cnt: 0 - valLoss: 0.42868632078170776 - trainLoss: 0.4081501364707947\n",
      "cnt: 0 - valLoss: 0.42868587374687195 - trainLoss: 0.4081485867500305\n",
      "cnt: 0 - valLoss: 0.42868542671203613 - trainLoss: 0.40814709663391113\n",
      "cnt: 0 - valLoss: 0.4286850094795227 - trainLoss: 0.40814560651779175\n",
      "cnt: 0 - valLoss: 0.4286845326423645 - trainLoss: 0.40814411640167236\n",
      "cnt: 0 - valLoss: 0.4286841154098511 - trainLoss: 0.4081425964832306\n",
      "cnt: 0 - valLoss: 0.42868366837501526 - trainLoss: 0.4081411361694336\n",
      "cnt: 0 - valLoss: 0.42868325114250183 - trainLoss: 0.4081396162509918\n",
      "cnt: 0 - valLoss: 0.42868277430534363 - trainLoss: 0.40813809633255005\n",
      "cnt: 0 - valLoss: 0.4286823868751526 - trainLoss: 0.40813660621643066\n",
      "cnt: 0 - valLoss: 0.4286819100379944 - trainLoss: 0.40813514590263367\n",
      "cnt: 0 - valLoss: 0.42868146300315857 - trainLoss: 0.4081336557865143\n",
      "cnt: 0 - valLoss: 0.42868101596832275 - trainLoss: 0.4081321656703949\n",
      "cnt: 0 - valLoss: 0.4286805987358093 - trainLoss: 0.4081306755542755\n",
      "cnt: 0 - valLoss: 0.4286801517009735 - trainLoss: 0.40812915563583374\n",
      "cnt: 0 - valLoss: 0.4286797344684601 - trainLoss: 0.40812763571739197\n",
      "cnt: 0 - valLoss: 0.4286792576313019 - trainLoss: 0.40812617540359497\n",
      "cnt: 0 - valLoss: 0.42867884039878845 - trainLoss: 0.408124715089798\n",
      "cnt: 0 - valLoss: 0.42867836356163025 - trainLoss: 0.4081231653690338\n",
      "cnt: 0 - valLoss: 0.4286779463291168 - trainLoss: 0.40812167525291443\n",
      "cnt: 0 - valLoss: 0.428677499294281 - trainLoss: 0.40812018513679504\n",
      "cnt: 0 - valLoss: 0.4286770820617676 - trainLoss: 0.40811866521835327\n",
      "cnt: 0 - valLoss: 0.42867663502693176 - trainLoss: 0.4081171751022339\n",
      "cnt: 0 - valLoss: 0.42867621779441833 - trainLoss: 0.4081156849861145\n",
      "cnt: 0 - valLoss: 0.42867574095726013 - trainLoss: 0.4081141948699951\n",
      "cnt: 0 - valLoss: 0.42867526412010193 - trainLoss: 0.40811267495155334\n",
      "cnt: 0 - valLoss: 0.4286748766899109 - trainLoss: 0.40811118483543396\n",
      "cnt: 0 - valLoss: 0.4286744296550751 - trainLoss: 0.40810972452163696\n",
      "cnt: 0 - valLoss: 0.42867398262023926 - trainLoss: 0.4081082344055176\n",
      "cnt: 0 - valLoss: 0.42867356538772583 - trainLoss: 0.4081066846847534\n",
      "cnt: 0 - valLoss: 0.4286731481552124 - trainLoss: 0.4081052243709564\n",
      "cnt: 0 - valLoss: 0.4286726713180542 - trainLoss: 0.40810373425483704\n",
      "cnt: 0 - valLoss: 0.42867228388786316 - trainLoss: 0.40810224413871765\n",
      "cnt: 0 - valLoss: 0.42867180705070496 - trainLoss: 0.40810075402259827\n",
      "cnt: 0 - valLoss: 0.42867133021354675 - trainLoss: 0.4080992341041565\n",
      "cnt: 0 - valLoss: 0.4286709129810333 - trainLoss: 0.4080977439880371\n",
      "cnt: 0 - valLoss: 0.4286704659461975 - trainLoss: 0.4080962538719177\n",
      "cnt: 0 - valLoss: 0.4286700487136841 - trainLoss: 0.40809476375579834\n",
      "cnt: 0 - valLoss: 0.42866963148117065 - trainLoss: 0.40809324383735657\n",
      "cnt: 0 - valLoss: 0.4286690950393677 - trainLoss: 0.4080917537212372\n",
      "cnt: 0 - valLoss: 0.4286687672138214 - trainLoss: 0.4080902636051178\n",
      "cnt: 0 - valLoss: 0.4286682903766632 - trainLoss: 0.4080887734889984\n",
      "cnt: 0 - valLoss: 0.428667813539505 - trainLoss: 0.4080873131752014\n",
      "cnt: 0 - valLoss: 0.4286673963069916 - trainLoss: 0.40808579325675964\n",
      "cnt: 0 - valLoss: 0.4286669194698334 - trainLoss: 0.40808430314064026\n",
      "cnt: 0 - valLoss: 0.42866653203964233 - trainLoss: 0.4080828130245209\n",
      "cnt: 0 - valLoss: 0.42866605520248413 - trainLoss: 0.4080813229084015\n",
      "cnt: 0 - valLoss: 0.4286656379699707 - trainLoss: 0.4080798625946045\n",
      "cnt: 0 - valLoss: 0.4286651611328125 - trainLoss: 0.40807831287384033\n",
      "cnt: 0 - valLoss: 0.4286647439002991 - trainLoss: 0.40807685256004333\n",
      "cnt: 0 - valLoss: 0.42866429686546326 - trainLoss: 0.40807533264160156\n",
      "cnt: 0 - valLoss: 0.42866387963294983 - trainLoss: 0.40807387232780457\n",
      "cnt: 0 - valLoss: 0.4286634027957916 - trainLoss: 0.4080723226070404\n",
      "cnt: 0 - valLoss: 0.4286629855632782 - trainLoss: 0.4080708622932434\n",
      "cnt: 0 - valLoss: 0.4286625385284424 - trainLoss: 0.40806931257247925\n",
      "cnt: 0 - valLoss: 0.4286620616912842 - trainLoss: 0.40806788206100464\n",
      "cnt: 0 - valLoss: 0.42866164445877075 - trainLoss: 0.40806636214256287\n",
      "cnt: 0 - valLoss: 0.4286612272262573 - trainLoss: 0.4080648720264435\n",
      "cnt: 0 - valLoss: 0.4286607801914215 - trainLoss: 0.4080634117126465\n",
      "cnt: 0 - valLoss: 0.4286603629589081 - trainLoss: 0.4080618917942047\n",
      "cnt: 0 - valLoss: 0.42865994572639465 - trainLoss: 0.4080604314804077\n",
      "cnt: 0 - valLoss: 0.42865946888923645 - trainLoss: 0.40805888175964355\n",
      "cnt: 0 - valLoss: 0.42865899205207825 - trainLoss: 0.40805742144584656\n",
      "cnt: 0 - valLoss: 0.4286585748195648 - trainLoss: 0.4080559015274048\n",
      "cnt: 0 - valLoss: 0.4286580979824066 - trainLoss: 0.4080544412136078\n",
      "cnt: 0 - valLoss: 0.4286577105522156 - trainLoss: 0.408052921295166\n",
      "cnt: 0 - valLoss: 0.4286572337150574 - trainLoss: 0.40805143117904663\n",
      "cnt: 0 - valLoss: 0.42865675687789917 - trainLoss: 0.40804994106292725\n",
      "cnt: 0 - valLoss: 0.42865633964538574 - trainLoss: 0.40804845094680786\n",
      "cnt: 0 - valLoss: 0.4286558926105499 - trainLoss: 0.40804699063301086\n",
      "cnt: 0 - valLoss: 0.42865538597106934 - trainLoss: 0.4080454409122467\n",
      "cnt: 0 - valLoss: 0.4286549985408783 - trainLoss: 0.4080439805984497\n",
      "cnt: 0 - valLoss: 0.4286545217037201 - trainLoss: 0.4080425202846527\n",
      "cnt: 0 - valLoss: 0.42865410447120667 - trainLoss: 0.40804100036621094\n",
      "cnt: 0 - valLoss: 0.42865368723869324 - trainLoss: 0.40803948044776917\n",
      "cnt: 0 - valLoss: 0.42865321040153503 - trainLoss: 0.4080379903316498\n",
      "cnt: 0 - valLoss: 0.4286527633666992 - trainLoss: 0.4080365002155304\n",
      "cnt: 0 - valLoss: 0.4286523163318634 - trainLoss: 0.4080350399017334\n",
      "cnt: 0 - valLoss: 0.4286518692970276 - trainLoss: 0.408033549785614\n",
      "cnt: 0 - valLoss: 0.4286513924598694 - trainLoss: 0.40803205966949463\n",
      "cnt: 0 - valLoss: 0.42865097522735596 - trainLoss: 0.40803053975105286\n",
      "cnt: 0 - valLoss: 0.42865049839019775 - trainLoss: 0.40802904963493347\n",
      "cnt: 0 - valLoss: 0.4286500811576843 - trainLoss: 0.4080275595188141\n",
      "cnt: 0 - valLoss: 0.4286496341228485 - trainLoss: 0.4080260992050171\n",
      "cnt: 0 - valLoss: 0.4286492168903351 - trainLoss: 0.4080246090888977\n",
      "cnt: 0 - valLoss: 0.4286487400531769 - trainLoss: 0.40802305936813354\n",
      "cnt: 0 - valLoss: 0.4286482632160187 - trainLoss: 0.40802159905433655\n",
      "cnt: 0 - valLoss: 0.42864784598350525 - trainLoss: 0.40802010893821716\n",
      "cnt: 0 - valLoss: 0.4286474287509918 - trainLoss: 0.4080186188220978\n",
      "cnt: 0 - valLoss: 0.42864689230918884 - trainLoss: 0.408017098903656\n",
      "cnt: 0 - valLoss: 0.4286464750766754 - trainLoss: 0.4080156087875366\n",
      "cnt: 0 - valLoss: 0.428646057844162 - trainLoss: 0.40801411867141724\n",
      "cnt: 0 - valLoss: 0.42864561080932617 - trainLoss: 0.40801262855529785\n",
      "cnt: 0 - valLoss: 0.42864513397216797 - trainLoss: 0.4080111086368561\n",
      "cnt: 0 - valLoss: 0.42864471673965454 - trainLoss: 0.4080096483230591\n",
      "cnt: 0 - valLoss: 0.42864423990249634 - trainLoss: 0.4080081582069397\n",
      "cnt: 0 - valLoss: 0.4286438226699829 - trainLoss: 0.4080066680908203\n",
      "cnt: 0 - valLoss: 0.4286433458328247 - trainLoss: 0.4080051779747009\n",
      "cnt: 0 - valLoss: 0.4286428689956665 - trainLoss: 0.40800365805625916\n",
      "cnt: 0 - valLoss: 0.4286424517631531 - trainLoss: 0.40800222754478455\n",
      "cnt: 0 - valLoss: 0.4286419749259949 - trainLoss: 0.4080007076263428\n",
      "cnt: 0 - valLoss: 0.42864155769348145 - trainLoss: 0.4079992175102234\n",
      "cnt: 0 - valLoss: 0.42864111065864563 - trainLoss: 0.40799766778945923\n",
      "cnt: 0 - valLoss: 0.4286406338214874 - trainLoss: 0.40799620747566223\n",
      "cnt: 0 - valLoss: 0.4286401867866516 - trainLoss: 0.40799471735954285\n",
      "cnt: 0 - valLoss: 0.42863979935646057 - trainLoss: 0.40799322724342346\n",
      "cnt: 0 - valLoss: 0.42863932251930237 - trainLoss: 0.40799176692962646\n",
      "cnt: 0 - valLoss: 0.42863884568214417 - trainLoss: 0.4079902172088623\n",
      "cnt: 0 - valLoss: 0.42863842844963074 - trainLoss: 0.4079887866973877\n",
      "cnt: 0 - valLoss: 0.42863795161247253 - trainLoss: 0.4079872667789459\n",
      "cnt: 0 - valLoss: 0.4286375045776367 - trainLoss: 0.4079858064651489\n",
      "cnt: 0 - valLoss: 0.4286370575428009 - trainLoss: 0.40798428654670715\n",
      "cnt: 0 - valLoss: 0.4286366105079651 - trainLoss: 0.40798282623291016\n",
      "cnt: 0 - valLoss: 0.4286361336708069 - trainLoss: 0.40798133611679077\n",
      "cnt: 0 - valLoss: 0.42863574624061584 - trainLoss: 0.407979816198349\n",
      "cnt: 0 - valLoss: 0.42863523960113525 - trainLoss: 0.4079783260822296\n",
      "cnt: 0 - valLoss: 0.4286348223686218 - trainLoss: 0.4079768657684326\n",
      "cnt: 0 - valLoss: 0.428634375333786 - trainLoss: 0.40797534584999084\n",
      "cnt: 0 - valLoss: 0.4286339282989502 - trainLoss: 0.4079738259315491\n",
      "cnt: 0 - valLoss: 0.4286334812641144 - trainLoss: 0.40797239542007446\n",
      "cnt: 0 - valLoss: 0.4286330044269562 - trainLoss: 0.4079708755016327\n",
      "cnt: 0 - valLoss: 0.42863255739212036 - trainLoss: 0.4079693853855133\n",
      "cnt: 0 - valLoss: 0.4286321699619293 - trainLoss: 0.4079678952693939\n",
      "cnt: 0 - valLoss: 0.42863163352012634 - trainLoss: 0.40796637535095215\n",
      "cnt: 0 - valLoss: 0.4286312162876129 - trainLoss: 0.40796494483947754\n",
      "cnt: 0 - valLoss: 0.4286307990550995 - trainLoss: 0.40796342492103577\n",
      "cnt: 0 - valLoss: 0.4286303222179413 - trainLoss: 0.4079619348049164\n",
      "cnt: 0 - valLoss: 0.4286298453807831 - trainLoss: 0.407960444688797\n",
      "cnt: 0 - valLoss: 0.42862942814826965 - trainLoss: 0.407958984375\n",
      "cnt: 0 - valLoss: 0.42862898111343384 - trainLoss: 0.4079574942588806\n",
      "cnt: 0 - valLoss: 0.42862850427627563 - trainLoss: 0.40795597434043884\n",
      "cnt: 0 - valLoss: 0.4286280572414398 - trainLoss: 0.40795448422431946\n",
      "cnt: 0 - valLoss: 0.428627610206604 - trainLoss: 0.4079529941082001\n",
      "cnt: 0 - valLoss: 0.4286271333694458 - trainLoss: 0.4079515039920807\n",
      "cnt: 0 - valLoss: 0.4286267161369324 - trainLoss: 0.4079499840736389\n",
      "cnt: 0 - valLoss: 0.42862623929977417 - trainLoss: 0.4079485535621643\n",
      "cnt: 0 - valLoss: 0.42862582206726074 - trainLoss: 0.40794703364372253\n",
      "cnt: 0 - valLoss: 0.4286253750324249 - trainLoss: 0.40794554352760315\n",
      "cnt: 0 - valLoss: 0.42862486839294434 - trainLoss: 0.40794405341148376\n",
      "cnt: 0 - valLoss: 0.4286244511604309 - trainLoss: 0.4079425632953644\n",
      "cnt: 0 - valLoss: 0.4286239743232727 - trainLoss: 0.4079411029815674\n",
      "cnt: 0 - valLoss: 0.4286234974861145 - trainLoss: 0.4079395532608032\n",
      "cnt: 0 - valLoss: 0.4286230802536011 - trainLoss: 0.4079380929470062\n",
      "cnt: 0 - valLoss: 0.42862263321876526 - trainLoss: 0.40793663263320923\n",
      "cnt: 0 - valLoss: 0.42862221598625183 - trainLoss: 0.40793511271476746\n",
      "cnt: 0 - valLoss: 0.4286217987537384 - trainLoss: 0.40793365240097046\n",
      "cnt: 0 - valLoss: 0.4286212623119354 - trainLoss: 0.40793219208717346\n",
      "cnt: 0 - valLoss: 0.428620845079422 - trainLoss: 0.4079306423664093\n",
      "cnt: 0 - valLoss: 0.42862042784690857 - trainLoss: 0.4079291820526123\n",
      "cnt: 0 - valLoss: 0.42861998081207275 - trainLoss: 0.40792766213417053\n",
      "cnt: 0 - valLoss: 0.42861950397491455 - trainLoss: 0.40792620182037354\n",
      "cnt: 0 - valLoss: 0.42861902713775635 - trainLoss: 0.40792474150657654\n",
      "cnt: 0 - valLoss: 0.4286186099052429 - trainLoss: 0.40792325139045715\n",
      "cnt: 0 - valLoss: 0.4286181926727295 - trainLoss: 0.40792176127433777\n",
      "cnt: 0 - valLoss: 0.4286177158355713 - trainLoss: 0.407920241355896\n",
      "cnt: 0 - valLoss: 0.4286172389984131 - trainLoss: 0.4079187512397766\n",
      "cnt: 0 - valLoss: 0.42861682176589966 - trainLoss: 0.4079172909259796\n",
      "cnt: 0 - valLoss: 0.42861637473106384 - trainLoss: 0.40791574120521545\n",
      "cnt: 0 - valLoss: 0.428615927696228 - trainLoss: 0.40791431069374084\n",
      "cnt: 0 - valLoss: 0.4286154508590698 - trainLoss: 0.4079127907752991\n",
      "cnt: 0 - valLoss: 0.428615003824234 - trainLoss: 0.4079113006591797\n",
      "cnt: 0 - valLoss: 0.4286145269870758 - trainLoss: 0.4079098403453827\n",
      "cnt: 0 - valLoss: 0.42861407995224 - trainLoss: 0.4079083502292633\n",
      "cnt: 0 - valLoss: 0.4286136329174042 - trainLoss: 0.4079068601131439\n",
      "cnt: 0 - valLoss: 0.42861321568489075 - trainLoss: 0.40790534019470215\n",
      "cnt: 0 - valLoss: 0.42861273884773254 - trainLoss: 0.40790385007858276\n",
      "cnt: 0 - valLoss: 0.42861226201057434 - trainLoss: 0.40790238976478577\n",
      "cnt: 0 - valLoss: 0.4286118447780609 - trainLoss: 0.4079008996486664\n",
      "cnt: 0 - valLoss: 0.4286113679409027 - trainLoss: 0.407899409532547\n",
      "cnt: 0 - valLoss: 0.4286109507083893 - trainLoss: 0.40789794921875\n",
      "cnt: 0 - valLoss: 0.4286104738712311 - trainLoss: 0.40789639949798584\n",
      "cnt: 0 - valLoss: 0.4286099970340729 - trainLoss: 0.40789493918418884\n",
      "cnt: 0 - valLoss: 0.42860957980155945 - trainLoss: 0.40789341926574707\n",
      "cnt: 0 - valLoss: 0.42860910296440125 - trainLoss: 0.4078919589519501\n",
      "cnt: 0 - valLoss: 0.42860865592956543 - trainLoss: 0.4078904986381531\n",
      "cnt: 0 - valLoss: 0.428608238697052 - trainLoss: 0.4078890085220337\n",
      "cnt: 0 - valLoss: 0.4286077618598938 - trainLoss: 0.4078875184059143\n",
      "cnt: 0 - valLoss: 0.4286072850227356 - trainLoss: 0.4078860580921173\n",
      "cnt: 0 - valLoss: 0.42860686779022217 - trainLoss: 0.40788453817367554\n",
      "cnt: 0 - valLoss: 0.42860645055770874 - trainLoss: 0.40788304805755615\n",
      "cnt: 0 - valLoss: 0.42860597372055054 - trainLoss: 0.40788155794143677\n",
      "cnt: 0 - valLoss: 0.4286055266857147 - trainLoss: 0.4078800678253174\n",
      "cnt: 0 - valLoss: 0.4286050796508789 - trainLoss: 0.4078786075115204\n",
      "cnt: 0 - valLoss: 0.4286046326160431 - trainLoss: 0.407877117395401\n",
      "cnt: 0 - valLoss: 0.4286041557788849 - trainLoss: 0.40787559747695923\n",
      "cnt: 0 - valLoss: 0.4286037087440491 - trainLoss: 0.40787413716316223\n",
      "cnt: 0 - valLoss: 0.42860326170921326 - trainLoss: 0.40787264704704285\n",
      "cnt: 0 - valLoss: 0.42860284447669983 - trainLoss: 0.40787115693092346\n",
      "cnt: 0 - valLoss: 0.428602397441864 - trainLoss: 0.4078696668148041\n",
      "cnt: 0 - valLoss: 0.4286019802093506 - trainLoss: 0.4078681468963623\n",
      "cnt: 0 - valLoss: 0.42860156297683716 - trainLoss: 0.4078667163848877\n",
      "cnt: 0 - valLoss: 0.42860108613967896 - trainLoss: 0.4078651964664459\n",
      "cnt: 0 - valLoss: 0.4286006987094879 - trainLoss: 0.4078637361526489\n",
      "cnt: 0 - valLoss: 0.4286002814769745 - trainLoss: 0.40786224603652954\n",
      "cnt: 0 - valLoss: 0.4285998046398163 - trainLoss: 0.40786078572273254\n",
      "cnt: 0 - valLoss: 0.42859935760498047 - trainLoss: 0.40785926580429077\n",
      "cnt: 0 - valLoss: 0.42859897017478943 - trainLoss: 0.4078578054904938\n",
      "cnt: 0 - valLoss: 0.4285985231399536 - trainLoss: 0.4078562557697296\n",
      "cnt: 0 - valLoss: 0.4285980761051178 - trainLoss: 0.4078547954559326\n",
      "cnt: 0 - valLoss: 0.42859765887260437 - trainLoss: 0.4078533351421356\n",
      "cnt: 0 - valLoss: 0.42859721183776855 - trainLoss: 0.40785184502601624\n",
      "cnt: 0 - valLoss: 0.4285967946052551 - trainLoss: 0.40785035490989685\n",
      "cnt: 0 - valLoss: 0.4285963773727417 - trainLoss: 0.40784889459609985\n",
      "cnt: 0 - valLoss: 0.4285959005355835 - trainLoss: 0.40784740447998047\n",
      "cnt: 0 - valLoss: 0.42859551310539246 - trainLoss: 0.4078459143638611\n",
      "cnt: 0 - valLoss: 0.42859503626823425 - trainLoss: 0.4078444540500641\n",
      "cnt: 0 - valLoss: 0.4285946190357208 - trainLoss: 0.4078429639339447\n",
      "cnt: 0 - valLoss: 0.428594172000885 - trainLoss: 0.40784144401550293\n",
      "cnt: 0 - valLoss: 0.4285937547683716 - trainLoss: 0.40783998370170593\n",
      "cnt: 0 - valLoss: 0.42859333753585815 - trainLoss: 0.40783846378326416\n",
      "cnt: 0 - valLoss: 0.42859289050102234 - trainLoss: 0.40783700346946716\n",
      "cnt: 0 - valLoss: 0.4285924732685089 - trainLoss: 0.4078355133533478\n",
      "cnt: 0 - valLoss: 0.4285920262336731 - trainLoss: 0.4078340530395508\n",
      "cnt: 0 - valLoss: 0.42859160900115967 - trainLoss: 0.4078325629234314\n",
      "cnt: 0 - valLoss: 0.42859119176864624 - trainLoss: 0.4078311026096344\n",
      "cnt: 0 - valLoss: 0.4285907447338104 - trainLoss: 0.40782955288887024\n",
      "cnt: 0 - valLoss: 0.428590327501297 - trainLoss: 0.40782809257507324\n",
      "cnt: 0 - valLoss: 0.42858991026878357 - trainLoss: 0.40782663226127625\n",
      "cnt: 0 - valLoss: 0.42858946323394775 - trainLoss: 0.40782514214515686\n",
      "cnt: 0 - valLoss: 0.42858898639678955 - trainLoss: 0.4078236520290375\n",
      "cnt: 0 - valLoss: 0.4285885691642761 - trainLoss: 0.4078221619129181\n",
      "cnt: 0 - valLoss: 0.4285881519317627 - trainLoss: 0.4078207015991211\n",
      "cnt: 0 - valLoss: 0.4285877048969269 - trainLoss: 0.4078191816806793\n",
      "cnt: 0 - valLoss: 0.42858728766441345 - trainLoss: 0.40781769156455994\n",
      "cnt: 0 - valLoss: 0.42858684062957764 - trainLoss: 0.40781620144844055\n",
      "cnt: 0 - valLoss: 0.4285864233970642 - trainLoss: 0.40781474113464355\n",
      "cnt: 0 - valLoss: 0.4285860061645508 - trainLoss: 0.40781325101852417\n",
      "cnt: 0 - valLoss: 0.42858555912971497 - trainLoss: 0.4078117609024048\n",
      "cnt: 0 - valLoss: 0.42858508229255676 - trainLoss: 0.4078103005886078\n",
      "cnt: 0 - valLoss: 0.42858466506004333 - trainLoss: 0.40780875086784363\n",
      "cnt: 0 - valLoss: 0.4285842478275299 - trainLoss: 0.40780729055404663\n",
      "cnt: 0 - valLoss: 0.4285838007926941 - trainLoss: 0.40780580043792725\n",
      "cnt: 0 - valLoss: 0.42858338356018066 - trainLoss: 0.40780431032180786\n",
      "cnt: 0 - valLoss: 0.42858296632766724 - trainLoss: 0.40780285000801086\n",
      "cnt: 0 - valLoss: 0.4285825192928314 - trainLoss: 0.40780138969421387\n",
      "cnt: 0 - valLoss: 0.428582102060318 - trainLoss: 0.4077998399734497\n",
      "cnt: 0 - valLoss: 0.4285816550254822 - trainLoss: 0.4077983796596527\n",
      "cnt: 0 - valLoss: 0.42858120799064636 - trainLoss: 0.4077969491481781\n",
      "cnt: 0 - valLoss: 0.42858076095581055 - trainLoss: 0.40779542922973633\n",
      "cnt: 0 - valLoss: 0.4285803437232971 - trainLoss: 0.40779393911361694\n",
      "cnt: 0 - valLoss: 0.4285798966884613 - trainLoss: 0.40779247879981995\n",
      "cnt: 0 - valLoss: 0.4285794794559479 - trainLoss: 0.40779101848602295\n",
      "cnt: 0 - valLoss: 0.42857906222343445 - trainLoss: 0.4077894985675812\n",
      "cnt: 0 - valLoss: 0.42857858538627625 - trainLoss: 0.4077880382537842\n",
      "cnt: 0 - valLoss: 0.42857813835144043 - trainLoss: 0.4077865481376648\n",
      "cnt: 0 - valLoss: 0.4285777807235718 - trainLoss: 0.407785028219223\n",
      "cnt: 0 - valLoss: 0.4285773038864136 - trainLoss: 0.4077835977077484\n",
      "cnt: 0 - valLoss: 0.42857685685157776 - trainLoss: 0.40778207778930664\n",
      "cnt: 0 - valLoss: 0.42857643961906433 - trainLoss: 0.40778058767318726\n",
      "cnt: 0 - valLoss: 0.4285760223865509 - trainLoss: 0.40777909755706787\n",
      "cnt: 0 - valLoss: 0.4285755753517151 - trainLoss: 0.4077776372432709\n",
      "cnt: 0 - valLoss: 0.42857515811920166 - trainLoss: 0.4077761471271515\n",
      "cnt: 0 - valLoss: 0.42857471108436584 - trainLoss: 0.4077746868133545\n",
      "cnt: 0 - valLoss: 0.4285742938518524 - trainLoss: 0.4077731966972351\n",
      "cnt: 0 - valLoss: 0.428573876619339 - trainLoss: 0.40777167677879333\n",
      "cnt: 0 - valLoss: 0.428573340177536 - trainLoss: 0.40777021646499634\n",
      "cnt: 0 - valLoss: 0.4285729229450226 - trainLoss: 0.40776878595352173\n",
      "cnt: 0 - valLoss: 0.42857250571250916 - trainLoss: 0.40776723623275757\n",
      "cnt: 0 - valLoss: 0.42857205867767334 - trainLoss: 0.40776577591896057\n",
      "cnt: 0 - valLoss: 0.4285716414451599 - trainLoss: 0.4077643156051636\n",
      "cnt: 0 - valLoss: 0.4285711944103241 - trainLoss: 0.4077627956867218\n",
      "cnt: 0 - valLoss: 0.4285707473754883 - trainLoss: 0.4077613353729248\n",
      "cnt: 0 - valLoss: 0.42857030034065247 - trainLoss: 0.40775981545448303\n",
      "cnt: 0 - valLoss: 0.42856988310813904 - trainLoss: 0.4077583849430084\n",
      "cnt: 0 - valLoss: 0.4285694360733032 - trainLoss: 0.40775686502456665\n",
      "cnt: 0 - valLoss: 0.4285690188407898 - trainLoss: 0.40775537490844727\n",
      "cnt: 0 - valLoss: 0.4285685420036316 - trainLoss: 0.40775391459465027\n",
      "cnt: 0 - valLoss: 0.42856815457344055 - trainLoss: 0.4077524244785309\n",
      "cnt: 0 - valLoss: 0.42856770753860474 - trainLoss: 0.4077509641647339\n",
      "cnt: 0 - valLoss: 0.4285672605037689 - trainLoss: 0.4077494740486145\n",
      "cnt: 0 - valLoss: 0.4285668432712555 - trainLoss: 0.4077479839324951\n",
      "cnt: 0 - valLoss: 0.4285663962364197 - trainLoss: 0.4077465236186981\n",
      "cnt: 0 - valLoss: 0.42856597900390625 - trainLoss: 0.40774503350257874\n",
      "cnt: 0 - valLoss: 0.42856550216674805 - trainLoss: 0.40774351358413696\n",
      "cnt: 0 - valLoss: 0.428565114736557 - trainLoss: 0.40774205327033997\n",
      "cnt: 0 - valLoss: 0.4285646080970764 - trainLoss: 0.4077405631542206\n",
      "cnt: 0 - valLoss: 0.428564190864563 - trainLoss: 0.4077390730381012\n",
      "cnt: 0 - valLoss: 0.4285637438297272 - trainLoss: 0.4077375829219818\n",
      "cnt: 0 - valLoss: 0.42856332659721375 - trainLoss: 0.4077361226081848\n",
      "cnt: 0 - valLoss: 0.42856287956237793 - trainLoss: 0.4077346622943878\n",
      "cnt: 0 - valLoss: 0.4285624623298645 - trainLoss: 0.40773317217826843\n",
      "cnt: 0 - valLoss: 0.4285620450973511 - trainLoss: 0.40773165225982666\n",
      "cnt: 0 - valLoss: 0.42856159806251526 - trainLoss: 0.40773022174835205\n",
      "cnt: 0 - valLoss: 0.42856112122535706 - trainLoss: 0.4077287018299103\n",
      "cnt: 0 - valLoss: 0.4285607635974884 - trainLoss: 0.4077272415161133\n",
      "cnt: 0 - valLoss: 0.4285602867603302 - trainLoss: 0.4077257215976715\n",
      "cnt: 0 - valLoss: 0.428559809923172 - trainLoss: 0.4077242612838745\n",
      "cnt: 0 - valLoss: 0.42855939269065857 - trainLoss: 0.4077228009700775\n",
      "cnt: 0 - valLoss: 0.42855894565582275 - trainLoss: 0.40772131085395813\n",
      "cnt: 0 - valLoss: 0.4285585284233093 - trainLoss: 0.40771982073783875\n",
      "cnt: 0 - valLoss: 0.4285580813884735 - trainLoss: 0.40771836042404175\n",
      "cnt: 0 - valLoss: 0.4285576641559601 - trainLoss: 0.40771687030792236\n",
      "cnt: 0 - valLoss: 0.42855724692344666 - trainLoss: 0.40771540999412537\n",
      "cnt: 0 - valLoss: 0.42855677008628845 - trainLoss: 0.4077138602733612\n",
      "cnt: 0 - valLoss: 0.4285563826560974 - trainLoss: 0.4077123999595642\n",
      "cnt: 0 - valLoss: 0.4285559058189392 - trainLoss: 0.4077109098434448\n",
      "cnt: 0 - valLoss: 0.428555428981781 - trainLoss: 0.4077094495296478\n",
      "cnt: 0 - valLoss: 0.4285550117492676 - trainLoss: 0.40770798921585083\n",
      "cnt: 0 - valLoss: 0.42855456471443176 - trainLoss: 0.40770649909973145\n",
      "cnt: 0 - valLoss: 0.42855414748191833 - trainLoss: 0.40770500898361206\n",
      "cnt: 0 - valLoss: 0.4285537302494049 - trainLoss: 0.40770354866981506\n",
      "cnt: 0 - valLoss: 0.4285532832145691 - trainLoss: 0.4077020585536957\n",
      "cnt: 0 - valLoss: 0.4285528063774109 - trainLoss: 0.4077005386352539\n",
      "cnt: 0 - valLoss: 0.42855238914489746 - trainLoss: 0.4076990783214569\n",
      "cnt: 0 - valLoss: 0.42855191230773926 - trainLoss: 0.4076975882053375\n",
      "cnt: 0 - valLoss: 0.42855149507522583 - trainLoss: 0.4076961278915405\n",
      "cnt: 0 - valLoss: 0.4285510778427124 - trainLoss: 0.40769463777542114\n",
      "cnt: 0 - valLoss: 0.4285506308078766 - trainLoss: 0.40769317746162415\n",
      "cnt: 0 - valLoss: 0.42855021357536316 - trainLoss: 0.4076916575431824\n",
      "cnt: 0 - valLoss: 0.42854976654052734 - trainLoss: 0.4076901972293854\n",
      "cnt: 0 - valLoss: 0.4285493493080139 - trainLoss: 0.4076887369155884\n",
      "cnt: 0 - valLoss: 0.4285489320755005 - trainLoss: 0.407687246799469\n",
      "cnt: 0 - valLoss: 0.4285484552383423 - trainLoss: 0.4076857268810272\n",
      "cnt: 0 - valLoss: 0.42854800820350647 - trainLoss: 0.4076842963695526\n",
      "cnt: 0 - valLoss: 0.42854759097099304 - trainLoss: 0.40768277645111084\n",
      "cnt: 0 - valLoss: 0.4285471737384796 - trainLoss: 0.40768128633499146\n",
      "cnt: 0 - valLoss: 0.4285466969013214 - trainLoss: 0.40767982602119446\n",
      "cnt: 0 - valLoss: 0.4285462498664856 - trainLoss: 0.4076783359050751\n",
      "cnt: 0 - valLoss: 0.42854583263397217 - trainLoss: 0.4076768755912781\n",
      "cnt: 0 - valLoss: 0.42854541540145874 - trainLoss: 0.4076753854751587\n",
      "cnt: 0 - valLoss: 0.4285449683666229 - trainLoss: 0.4076739251613617\n",
      "cnt: 0 - valLoss: 0.4285445511341095 - trainLoss: 0.4076724648475647\n",
      "cnt: 0 - valLoss: 0.42854413390159607 - trainLoss: 0.40767091512680054\n",
      "cnt: 0 - valLoss: 0.42854368686676025 - trainLoss: 0.4076694846153259\n",
      "cnt: 0 - valLoss: 0.42854318022727966 - trainLoss: 0.40766802430152893\n",
      "cnt: 0 - valLoss: 0.42854273319244385 - trainLoss: 0.40766653418540955\n",
      "cnt: 0 - valLoss: 0.4285423755645752 - trainLoss: 0.40766507387161255\n",
      "cnt: 0 - valLoss: 0.428541898727417 - trainLoss: 0.4076635539531708\n",
      "cnt: 0 - valLoss: 0.4285414516925812 - trainLoss: 0.4076620638370514\n",
      "cnt: 0 - valLoss: 0.42854103446006775 - trainLoss: 0.4076606035232544\n",
      "cnt: 0 - valLoss: 0.4285406172275543 - trainLoss: 0.407659113407135\n",
      "cnt: 0 - valLoss: 0.4285401701927185 - trainLoss: 0.407657653093338\n",
      "cnt: 0 - valLoss: 0.4285397529602051 - trainLoss: 0.407656192779541\n",
      "cnt: 0 - valLoss: 0.42853930592536926 - trainLoss: 0.40765470266342163\n",
      "cnt: 0 - valLoss: 0.42853888869285583 - trainLoss: 0.40765321254730225\n",
      "cnt: 0 - valLoss: 0.42853841185569763 - trainLoss: 0.40765172243118286\n",
      "cnt: 0 - valLoss: 0.4285379946231842 - trainLoss: 0.40765029191970825\n",
      "cnt: 0 - valLoss: 0.4285375475883484 - trainLoss: 0.4076487720012665\n",
      "cnt: 0 - valLoss: 0.42853713035583496 - trainLoss: 0.4076473116874695\n",
      "cnt: 0 - valLoss: 0.42853671312332153 - trainLoss: 0.4076458513736725\n",
      "cnt: 0 - valLoss: 0.42853623628616333 - trainLoss: 0.4076443612575531\n",
      "cnt: 0 - valLoss: 0.4285358488559723 - trainLoss: 0.40764284133911133\n",
      "cnt: 0 - valLoss: 0.4285353720188141 - trainLoss: 0.4076414108276367\n",
      "cnt: 0 - valLoss: 0.42853495478630066 - trainLoss: 0.4076399505138397\n",
      "cnt: 0 - valLoss: 0.42853450775146484 - trainLoss: 0.40763840079307556\n",
      "cnt: 0 - valLoss: 0.4285340905189514 - trainLoss: 0.40763694047927856\n",
      "cnt: 0 - valLoss: 0.428533673286438 - trainLoss: 0.40763548016548157\n",
      "cnt: 0 - valLoss: 0.4285331964492798 - trainLoss: 0.4076339900493622\n",
      "cnt: 0 - valLoss: 0.42853274941444397 - trainLoss: 0.4076324999332428\n",
      "cnt: 0 - valLoss: 0.42853236198425293 - trainLoss: 0.4076310396194458\n",
      "cnt: 0 - valLoss: 0.4285319149494171 - trainLoss: 0.4076295495033264\n",
      "cnt: 0 - valLoss: 0.4285315275192261 - trainLoss: 0.4076280891895294\n",
      "cnt: 0 - valLoss: 0.4285309910774231 - trainLoss: 0.4076266288757324\n",
      "cnt: 0 - valLoss: 0.42853057384490967 - trainLoss: 0.40762513875961304\n",
      "cnt: 0 - valLoss: 0.42853015661239624 - trainLoss: 0.40762361884117126\n",
      "cnt: 0 - valLoss: 0.4285297095775604 - trainLoss: 0.40762218832969666\n",
      "cnt: 0 - valLoss: 0.428529292345047 - trainLoss: 0.4076206684112549\n",
      "cnt: 0 - valLoss: 0.42852887511253357 - trainLoss: 0.4076192378997803\n",
      "cnt: 0 - valLoss: 0.42852842807769775 - trainLoss: 0.4076177179813385\n",
      "cnt: 0 - valLoss: 0.4285280108451843 - trainLoss: 0.4076162576675415\n",
      "cnt: 0 - valLoss: 0.4285275638103485 - trainLoss: 0.4076147675514221\n",
      "cnt: 0 - valLoss: 0.4285271465778351 - trainLoss: 0.4076133072376251\n",
      "cnt: 0 - valLoss: 0.4285266697406769 - trainLoss: 0.4076118767261505\n",
      "cnt: 0 - valLoss: 0.42852625250816345 - trainLoss: 0.40761032700538635\n",
      "cnt: 0 - valLoss: 0.42852580547332764 - trainLoss: 0.40760886669158936\n",
      "cnt: 0 - valLoss: 0.4285253882408142 - trainLoss: 0.40760737657546997\n",
      "cnt: 0 - valLoss: 0.428524911403656 - trainLoss: 0.407605916261673\n",
      "cnt: 0 - valLoss: 0.4285244941711426 - trainLoss: 0.4076044261455536\n",
      "cnt: 0 - valLoss: 0.42852404713630676 - trainLoss: 0.4076029658317566\n",
      "cnt: 0 - valLoss: 0.42852362990379333 - trainLoss: 0.4076015055179596\n",
      "cnt: 0 - valLoss: 0.4285232126712799 - trainLoss: 0.4076000154018402\n",
      "cnt: 0 - valLoss: 0.4285227358341217 - trainLoss: 0.40759849548339844\n",
      "cnt: 0 - valLoss: 0.4285222887992859 - trainLoss: 0.40759706497192383\n",
      "cnt: 0 - valLoss: 0.4285218417644501 - trainLoss: 0.40759560465812683\n",
      "cnt: 0 - valLoss: 0.42852145433425903 - trainLoss: 0.40759411454200745\n",
      "cnt: 0 - valLoss: 0.42852097749710083 - trainLoss: 0.4075925946235657\n",
      "cnt: 0 - valLoss: 0.4285205602645874 - trainLoss: 0.4075911343097687\n",
      "cnt: 0 - valLoss: 0.4285201132297516 - trainLoss: 0.40758970379829407\n",
      "cnt: 0 - valLoss: 0.42851969599723816 - trainLoss: 0.4075881540775299\n",
      "cnt: 0 - valLoss: 0.42851921916007996 - trainLoss: 0.4075867235660553\n",
      "cnt: 0 - valLoss: 0.4285188317298889 - trainLoss: 0.4075852334499359\n",
      "cnt: 0 - valLoss: 0.4285183548927307 - trainLoss: 0.4075837731361389\n",
      "cnt: 0 - valLoss: 0.4285179376602173 - trainLoss: 0.40758228302001953\n",
      "cnt: 0 - valLoss: 0.42851749062538147 - trainLoss: 0.40758079290390015\n",
      "cnt: 0 - valLoss: 0.42851704359054565 - trainLoss: 0.40757933259010315\n",
      "cnt: 0 - valLoss: 0.42851659655570984 - trainLoss: 0.40757787227630615\n",
      "cnt: 0 - valLoss: 0.4285161793231964 - trainLoss: 0.40757638216018677\n",
      "cnt: 0 - valLoss: 0.4285157322883606 - trainLoss: 0.40757492184638977\n",
      "cnt: 0 - valLoss: 0.42851531505584717 - trainLoss: 0.4075734317302704\n",
      "cnt: 0 - valLoss: 0.42851489782333374 - trainLoss: 0.4075719714164734\n",
      "cnt: 0 - valLoss: 0.4285144507884979 - trainLoss: 0.4075705111026764\n",
      "cnt: 0 - valLoss: 0.42851394414901733 - trainLoss: 0.407569020986557\n",
      "cnt: 0 - valLoss: 0.4285135269165039 - trainLoss: 0.4075675308704376\n",
      "cnt: 0 - valLoss: 0.4285130798816681 - trainLoss: 0.4075660705566406\n",
      "cnt: 0 - valLoss: 0.42851266264915466 - trainLoss: 0.40756452083587646\n",
      "cnt: 0 - valLoss: 0.42851218581199646 - trainLoss: 0.40756312012672424\n",
      "cnt: 0 - valLoss: 0.4285117983818054 - trainLoss: 0.40756160020828247\n",
      "cnt: 0 - valLoss: 0.428511381149292 - trainLoss: 0.40756016969680786\n",
      "cnt: 0 - valLoss: 0.4285109043121338 - trainLoss: 0.4075586199760437\n",
      "cnt: 0 - valLoss: 0.42851048707962036 - trainLoss: 0.4075571596622467\n",
      "cnt: 0 - valLoss: 0.42851001024246216 - trainLoss: 0.4075556993484497\n",
      "cnt: 0 - valLoss: 0.42850956320762634 - trainLoss: 0.4075542390346527\n",
      "cnt: 0 - valLoss: 0.4285091459751129 - trainLoss: 0.4075527489185333\n",
      "cnt: 0 - valLoss: 0.4285087287425995 - trainLoss: 0.40755128860473633\n",
      "cnt: 0 - valLoss: 0.42850828170776367 - trainLoss: 0.40754979848861694\n",
      "cnt: 0 - valLoss: 0.42850786447525024 - trainLoss: 0.40754833817481995\n",
      "cnt: 0 - valLoss: 0.42850741744041443 - trainLoss: 0.40754687786102295\n",
      "cnt: 0 - valLoss: 0.4285069704055786 - trainLoss: 0.40754538774490356\n",
      "cnt: 0 - valLoss: 0.4285064935684204 - trainLoss: 0.4075438976287842\n",
      "cnt: 0 - valLoss: 0.42850610613822937 - trainLoss: 0.4075424373149872\n",
      "cnt: 0 - valLoss: 0.42850562930107117 - trainLoss: 0.4075409770011902\n",
      "cnt: 0 - valLoss: 0.42850521206855774 - trainLoss: 0.4075394868850708\n",
      "cnt: 0 - valLoss: 0.4285047650337219 - trainLoss: 0.4075379967689514\n",
      "cnt: 0 - valLoss: 0.4285042881965637 - trainLoss: 0.4075365364551544\n",
      "cnt: 0 - valLoss: 0.4285038709640503 - trainLoss: 0.4075350761413574\n",
      "cnt: 0 - valLoss: 0.4285033941268921 - trainLoss: 0.40753358602523804\n",
      "cnt: 0 - valLoss: 0.42850297689437866 - trainLoss: 0.40753206610679626\n",
      "cnt: 0 - valLoss: 0.42850252985954285 - trainLoss: 0.40753063559532166\n",
      "cnt: 0 - valLoss: 0.4285021126270294 - trainLoss: 0.40752917528152466\n",
      "cnt: 0 - valLoss: 0.428501695394516 - trainLoss: 0.4075276851654053\n",
      "cnt: 0 - valLoss: 0.4285012483596802 - trainLoss: 0.4075262248516083\n",
      "cnt: 0 - valLoss: 0.428500771522522 - trainLoss: 0.4075247049331665\n",
      "cnt: 0 - valLoss: 0.42850035429000854 - trainLoss: 0.4075232744216919\n",
      "cnt: 0 - valLoss: 0.42849987745285034 - trainLoss: 0.4075217545032501\n",
      "cnt: 0 - valLoss: 0.4284994602203369 - trainLoss: 0.4075203537940979\n",
      "cnt: 0 - valLoss: 0.4284990429878235 - trainLoss: 0.40751880407333374\n",
      "cnt: 0 - valLoss: 0.42849859595298767 - trainLoss: 0.40751734375953674\n",
      "cnt: 0 - valLoss: 0.42849811911582947 - trainLoss: 0.40751591324806213\n",
      "cnt: 0 - valLoss: 0.42849770188331604 - trainLoss: 0.40751439332962036\n",
      "cnt: 0 - valLoss: 0.42849722504615784 - trainLoss: 0.40751299262046814\n",
      "cnt: 0 - valLoss: 0.4284968078136444 - trainLoss: 0.407511442899704\n",
      "cnt: 0 - valLoss: 0.4284963607788086 - trainLoss: 0.407509982585907\n",
      "cnt: 0 - valLoss: 0.42849594354629517 - trainLoss: 0.4075085520744324\n",
      "cnt: 0 - valLoss: 0.42849546670913696 - trainLoss: 0.4075070321559906\n",
      "cnt: 0 - valLoss: 0.42849504947662354 - trainLoss: 0.4075055718421936\n",
      "cnt: 0 - valLoss: 0.4284946024417877 - trainLoss: 0.4075040817260742\n",
      "cnt: 0 - valLoss: 0.4284941554069519 - trainLoss: 0.4075026214122772\n",
      "cnt: 0 - valLoss: 0.4284937083721161 - trainLoss: 0.40750113129615784\n",
      "cnt: 0 - valLoss: 0.42849329113960266 - trainLoss: 0.40749967098236084\n",
      "cnt: 0 - valLoss: 0.42849284410476685 - trainLoss: 0.40749821066856384\n",
      "cnt: 0 - valLoss: 0.42849239706993103 - trainLoss: 0.40749672055244446\n",
      "cnt: 0 - valLoss: 0.4284919500350952 - trainLoss: 0.4074952304363251\n",
      "cnt: 0 - valLoss: 0.4284915328025818 - trainLoss: 0.40749382972717285\n",
      "cnt: 0 - valLoss: 0.4284910559654236 - trainLoss: 0.4074922800064087\n",
      "cnt: 0 - valLoss: 0.42849063873291016 - trainLoss: 0.4074908196926117\n",
      "cnt: 0 - valLoss: 0.42849019169807434 - trainLoss: 0.4074893593788147\n",
      "cnt: 0 - valLoss: 0.42848971486091614 - trainLoss: 0.4074878990650177\n",
      "cnt: 0 - valLoss: 0.4284892976284027 - trainLoss: 0.4074864089488983\n",
      "cnt: 0 - valLoss: 0.4284888803958893 - trainLoss: 0.40748491883277893\n",
      "cnt: 0 - valLoss: 0.42848843336105347 - trainLoss: 0.40748345851898193\n",
      "cnt: 0 - valLoss: 0.42848795652389526 - trainLoss: 0.40748199820518494\n",
      "cnt: 0 - valLoss: 0.42848753929138184 - trainLoss: 0.40748053789138794\n",
      "cnt: 0 - valLoss: 0.4284871220588684 - trainLoss: 0.40747901797294617\n",
      "cnt: 0 - valLoss: 0.4284866452217102 - trainLoss: 0.40747755765914917\n",
      "cnt: 0 - valLoss: 0.428486168384552 - trainLoss: 0.4074760973453522\n",
      "cnt: 0 - valLoss: 0.4284857511520386 - trainLoss: 0.4074746370315552\n",
      "cnt: 0 - valLoss: 0.42848530411720276 - trainLoss: 0.407473087310791\n",
      "cnt: 0 - valLoss: 0.42848488688468933 - trainLoss: 0.4074716567993164\n",
      "cnt: 0 - valLoss: 0.4284844696521759 - trainLoss: 0.4074701964855194\n",
      "cnt: 0 - valLoss: 0.4284840226173401 - trainLoss: 0.4074687361717224\n",
      "cnt: 0 - valLoss: 0.4284835457801819 - trainLoss: 0.407467246055603\n",
      "cnt: 0 - valLoss: 0.42848309874534607 - trainLoss: 0.40746572613716125\n",
      "cnt: 0 - valLoss: 0.42848265171051025 - trainLoss: 0.40746432542800903\n",
      "cnt: 0 - valLoss: 0.4284822344779968 - trainLoss: 0.40746283531188965\n",
      "cnt: 0 - valLoss: 0.428481787443161 - trainLoss: 0.40746137499809265\n",
      "cnt: 0 - valLoss: 0.4284813702106476 - trainLoss: 0.40745988488197327\n",
      "cnt: 0 - valLoss: 0.42848095297813416 - trainLoss: 0.4074583649635315\n",
      "cnt: 0 - valLoss: 0.42848047614097595 - trainLoss: 0.4074569642543793\n",
      "cnt: 0 - valLoss: 0.42847999930381775 - trainLoss: 0.4074554741382599\n",
      "cnt: 0 - valLoss: 0.4284795820713043 - trainLoss: 0.4074540138244629\n",
      "cnt: 0 - valLoss: 0.4284791350364685 - trainLoss: 0.4074525535106659\n",
      "cnt: 0 - valLoss: 0.4284787178039551 - trainLoss: 0.4074510633945465\n",
      "cnt: 0 - valLoss: 0.42847827076911926 - trainLoss: 0.4074495732784271\n",
      "cnt: 0 - valLoss: 0.42847785353660583 - trainLoss: 0.4074481129646301\n",
      "cnt: 0 - valLoss: 0.42847737669944763 - trainLoss: 0.40744665265083313\n",
      "cnt: 0 - valLoss: 0.42847689986228943 - trainLoss: 0.40744516253471375\n",
      "cnt: 0 - valLoss: 0.428476482629776 - trainLoss: 0.40744370222091675\n",
      "cnt: 0 - valLoss: 0.4284760653972626 - trainLoss: 0.40744221210479736\n",
      "cnt: 0 - valLoss: 0.42847561836242676 - trainLoss: 0.40744075179100037\n",
      "cnt: 0 - valLoss: 0.42847514152526855 - trainLoss: 0.40743929147720337\n",
      "cnt: 0 - valLoss: 0.4284747242927551 - trainLoss: 0.407437801361084\n",
      "cnt: 0 - valLoss: 0.4284742474555969 - trainLoss: 0.407436341047287\n",
      "cnt: 0 - valLoss: 0.4284738302230835 - trainLoss: 0.4074348211288452\n",
      "cnt: 0 - valLoss: 0.4284733831882477 - trainLoss: 0.4074333906173706\n",
      "cnt: 0 - valLoss: 0.42847293615341187 - trainLoss: 0.4074319303035736\n",
      "cnt: 0 - valLoss: 0.42847248911857605 - trainLoss: 0.4074304699897766\n",
      "cnt: 0 - valLoss: 0.42847201228141785 - trainLoss: 0.4074290096759796\n",
      "cnt: 0 - valLoss: 0.4284715950489044 - trainLoss: 0.40742751955986023\n",
      "cnt: 0 - valLoss: 0.428471177816391 - trainLoss: 0.40742602944374084\n",
      "cnt: 0 - valLoss: 0.4284707009792328 - trainLoss: 0.40742456912994385\n",
      "cnt: 0 - valLoss: 0.4284702241420746 - trainLoss: 0.40742310881614685\n",
      "cnt: 0 - valLoss: 0.42846980690956116 - trainLoss: 0.40742164850234985\n",
      "cnt: 0 - valLoss: 0.42846935987472534 - trainLoss: 0.40742015838623047\n",
      "cnt: 0 - valLoss: 0.4284689426422119 - trainLoss: 0.40741869807243347\n",
      "cnt: 0 - valLoss: 0.4284685254096985 - trainLoss: 0.4074172079563141\n",
      "cnt: 0 - valLoss: 0.4284680485725403 - trainLoss: 0.4074157476425171\n",
      "cnt: 0 - valLoss: 0.4284675717353821 - trainLoss: 0.4074142575263977\n",
      "cnt: 0 - valLoss: 0.42846715450286865 - trainLoss: 0.4074127972126007\n",
      "cnt: 0 - valLoss: 0.42846670746803284 - trainLoss: 0.4074113070964813\n",
      "cnt: 0 - valLoss: 0.4284662902355194 - trainLoss: 0.4074098765850067\n",
      "cnt: 0 - valLoss: 0.4284658133983612 - trainLoss: 0.40740838646888733\n",
      "cnt: 0 - valLoss: 0.4284653961658478 - trainLoss: 0.40740689635276794\n",
      "cnt: 0 - valLoss: 0.4284649193286896 - trainLoss: 0.40740543603897095\n",
      "cnt: 0 - valLoss: 0.42846447229385376 - trainLoss: 0.40740397572517395\n",
      "cnt: 0 - valLoss: 0.42846405506134033 - trainLoss: 0.40740251541137695\n",
      "cnt: 0 - valLoss: 0.4284636378288269 - trainLoss: 0.40740102529525757\n",
      "cnt: 0 - valLoss: 0.4284631907939911 - trainLoss: 0.40739956498146057\n",
      "cnt: 0 - valLoss: 0.4284626841545105 - trainLoss: 0.4073981046676636\n",
      "cnt: 0 - valLoss: 0.42846226692199707 - trainLoss: 0.4073966145515442\n",
      "cnt: 0 - valLoss: 0.42846181988716125 - trainLoss: 0.4073951542377472\n",
      "cnt: 0 - valLoss: 0.4284614026546478 - trainLoss: 0.4073936641216278\n",
      "cnt: 0 - valLoss: 0.428460955619812 - trainLoss: 0.4073922038078308\n",
      "cnt: 0 - valLoss: 0.4284605085849762 - trainLoss: 0.4073907434940338\n",
      "cnt: 0 - valLoss: 0.4284600615501404 - trainLoss: 0.4073892831802368\n",
      "cnt: 0 - valLoss: 0.4284595847129822 - trainLoss: 0.40738779306411743\n",
      "cnt: 0 - valLoss: 0.42845916748046875 - trainLoss: 0.40738633275032043\n",
      "cnt: 0 - valLoss: 0.4284587502479553 - trainLoss: 0.40738484263420105\n",
      "cnt: 0 - valLoss: 0.4284583032131195 - trainLoss: 0.40738338232040405\n",
      "cnt: 0 - valLoss: 0.4284578859806061 - trainLoss: 0.40738192200660706\n",
      "cnt: 0 - valLoss: 0.4284574091434479 - trainLoss: 0.40738043189048767\n",
      "cnt: 0 - valLoss: 0.42845699191093445 - trainLoss: 0.4073789715766907\n",
      "cnt: 0 - valLoss: 0.42845654487609863 - trainLoss: 0.4073775112628937\n",
      "cnt: 0 - valLoss: 0.42845606803894043 - trainLoss: 0.4073760211467743\n",
      "cnt: 0 - valLoss: 0.428455650806427 - trainLoss: 0.4073745608329773\n",
      "cnt: 0 - valLoss: 0.4284552335739136 - trainLoss: 0.4073731005191803\n",
      "cnt: 0 - valLoss: 0.42845478653907776 - trainLoss: 0.4073716700077057\n",
      "cnt: 0 - valLoss: 0.42845430970191956 - trainLoss: 0.4073701500892639\n",
      "cnt: 0 - valLoss: 0.42845389246940613 - trainLoss: 0.40736865997314453\n",
      "cnt: 0 - valLoss: 0.4284534752368927 - trainLoss: 0.40736719965934753\n",
      "cnt: 0 - valLoss: 0.4284529983997345 - trainLoss: 0.40736573934555054\n",
      "cnt: 0 - valLoss: 0.42845258116722107 - trainLoss: 0.4073643088340759\n",
      "cnt: 0 - valLoss: 0.42845213413238525 - trainLoss: 0.40736284852027893\n",
      "cnt: 0 - valLoss: 0.4284517168998718 - trainLoss: 0.40736135840415955\n",
      "cnt: 0 - valLoss: 0.4284512400627136 - trainLoss: 0.40735989809036255\n",
      "cnt: 0 - valLoss: 0.4284508228302002 - trainLoss: 0.4073583781719208\n",
      "cnt: 0 - valLoss: 0.428450345993042 - trainLoss: 0.40735694766044617\n",
      "cnt: 0 - valLoss: 0.4284498989582062 - trainLoss: 0.40735548734664917\n",
      "cnt: 0 - valLoss: 0.42844948172569275 - trainLoss: 0.4073540270328522\n",
      "cnt: 0 - valLoss: 0.4284490644931793 - trainLoss: 0.4073525667190552\n",
      "cnt: 0 - valLoss: 0.4284485876560211 - trainLoss: 0.4073510766029358\n",
      "cnt: 0 - valLoss: 0.4284482002258301 - trainLoss: 0.4073496162891388\n",
      "cnt: 0 - valLoss: 0.4284477233886719 - trainLoss: 0.4073481261730194\n",
      "cnt: 0 - valLoss: 0.42844730615615845 - trainLoss: 0.4073466658592224\n",
      "cnt: 0 - valLoss: 0.42844682931900024 - trainLoss: 0.4073452055454254\n",
      "cnt: 0 - valLoss: 0.42844638228416443 - trainLoss: 0.4073437452316284\n",
      "cnt: 0 - valLoss: 0.428445965051651 - trainLoss: 0.40734222531318665\n",
      "cnt: 0 - valLoss: 0.4284454882144928 - trainLoss: 0.40734076499938965\n",
      "cnt: 0 - valLoss: 0.42844510078430176 - trainLoss: 0.40733930468559265\n",
      "cnt: 0 - valLoss: 0.42844462394714355 - trainLoss: 0.40733784437179565\n",
      "cnt: 0 - valLoss: 0.4284442067146301 - trainLoss: 0.40733638405799866\n",
      "cnt: 0 - valLoss: 0.4284437298774719 - trainLoss: 0.40733492374420166\n",
      "cnt: 0 - valLoss: 0.4284433126449585 - trainLoss: 0.4073334336280823\n",
      "cnt: 0 - valLoss: 0.4284428656101227 - trainLoss: 0.40733203291893005\n",
      "cnt: 0 - valLoss: 0.42844244837760925 - trainLoss: 0.4073305130004883\n",
      "cnt: 0 - valLoss: 0.4284420311450958 - trainLoss: 0.4073290228843689\n",
      "cnt: 0 - valLoss: 0.42844149470329285 - trainLoss: 0.4073275625705719\n",
      "cnt: 0 - valLoss: 0.4284411668777466 - trainLoss: 0.4073260724544525\n",
      "cnt: 0 - valLoss: 0.428440660238266 - trainLoss: 0.4073246717453003\n",
      "cnt: 0 - valLoss: 0.4284402132034302 - trainLoss: 0.4073232114315033\n",
      "cnt: 0 - valLoss: 0.42843979597091675 - trainLoss: 0.4073217213153839\n",
      "cnt: 0 - valLoss: 0.4284393787384033 - trainLoss: 0.40732020139694214\n",
      "cnt: 0 - valLoss: 0.4284389019012451 - trainLoss: 0.40731877088546753\n",
      "cnt: 0 - valLoss: 0.4284384548664093 - trainLoss: 0.40731731057167053\n",
      "cnt: 0 - valLoss: 0.4284380376338959 - trainLoss: 0.40731585025787354\n",
      "cnt: 0 - valLoss: 0.42843762040138245 - trainLoss: 0.40731436014175415\n",
      "cnt: 0 - valLoss: 0.42843714356422424 - trainLoss: 0.40731289982795715\n",
      "cnt: 0 - valLoss: 0.4284366965293884 - trainLoss: 0.40731140971183777\n",
      "cnt: 0 - valLoss: 0.428436279296875 - trainLoss: 0.40730994939804077\n",
      "cnt: 0 - valLoss: 0.4284358024597168 - trainLoss: 0.4073084890842438\n",
      "cnt: 0 - valLoss: 0.42843538522720337 - trainLoss: 0.4073070287704468\n",
      "cnt: 0 - valLoss: 0.42843493819236755 - trainLoss: 0.4073055684566498\n",
      "cnt: 0 - valLoss: 0.42843449115753174 - trainLoss: 0.4073041081428528\n",
      "cnt: 0 - valLoss: 0.4284340441226959 - trainLoss: 0.4073026180267334\n",
      "cnt: 0 - valLoss: 0.4284336268901825 - trainLoss: 0.4073011577129364\n",
      "cnt: 0 - valLoss: 0.4284331500530243 - trainLoss: 0.4072996973991394\n",
      "cnt: 0 - valLoss: 0.42843273282051086 - trainLoss: 0.40729820728302\n",
      "cnt: 0 - valLoss: 0.42843228578567505 - trainLoss: 0.407296746969223\n",
      "cnt: 0 - valLoss: 0.42843180894851685 - trainLoss: 0.4072953164577484\n",
      "cnt: 0 - valLoss: 0.4284313917160034 - trainLoss: 0.4072938561439514\n",
      "cnt: 0 - valLoss: 0.42843097448349 - trainLoss: 0.4072923958301544\n",
      "cnt: 0 - valLoss: 0.4284305274486542 - trainLoss: 0.40729090571403503\n",
      "cnt: 0 - valLoss: 0.42843005061149597 - trainLoss: 0.40728944540023804\n",
      "cnt: 0 - valLoss: 0.42842963337898254 - trainLoss: 0.40728792548179626\n",
      "cnt: 0 - valLoss: 0.42842915654182434 - trainLoss: 0.40728649497032166\n",
      "cnt: 0 - valLoss: 0.4284287393093109 - trainLoss: 0.40728503465652466\n",
      "cnt: 0 - valLoss: 0.4284282922744751 - trainLoss: 0.40728357434272766\n",
      "cnt: 0 - valLoss: 0.42842787504196167 - trainLoss: 0.4072820842266083\n",
      "cnt: 0 - valLoss: 0.42842745780944824 - trainLoss: 0.4072805643081665\n",
      "cnt: 0 - valLoss: 0.42842698097229004 - trainLoss: 0.4072791635990143\n",
      "cnt: 0 - valLoss: 0.42842650413513184 - trainLoss: 0.4072776734828949\n",
      "cnt: 0 - valLoss: 0.4284260869026184 - trainLoss: 0.4072762131690979\n",
      "cnt: 0 - valLoss: 0.4284256398677826 - trainLoss: 0.4072747528553009\n",
      "cnt: 0 - valLoss: 0.42842522263526917 - trainLoss: 0.4072732925415039\n",
      "cnt: 0 - valLoss: 0.42842474579811096 - trainLoss: 0.4072718322277069\n",
      "cnt: 0 - valLoss: 0.42842432856559753 - trainLoss: 0.4072703421115875\n",
      "cnt: 0 - valLoss: 0.4284238815307617 - trainLoss: 0.4072688817977905\n",
      "cnt: 0 - valLoss: 0.4284234344959259 - trainLoss: 0.40726739168167114\n",
      "cnt: 0 - valLoss: 0.4284229874610901 - trainLoss: 0.40726593136787415\n",
      "cnt: 0 - valLoss: 0.42842257022857666 - trainLoss: 0.40726447105407715\n",
      "cnt: 0 - valLoss: 0.42842209339141846 - trainLoss: 0.40726304054260254\n",
      "cnt: 0 - valLoss: 0.42842167615890503 - trainLoss: 0.40726158022880554\n",
      "cnt: 0 - valLoss: 0.4284211993217468 - trainLoss: 0.40726009011268616\n",
      "cnt: 0 - valLoss: 0.428420752286911 - trainLoss: 0.4072585701942444\n",
      "cnt: 0 - valLoss: 0.4284203052520752 - trainLoss: 0.40725716948509216\n",
      "cnt: 0 - valLoss: 0.4284198582172394 - trainLoss: 0.40725570917129517\n",
      "cnt: 0 - valLoss: 0.42841944098472595 - trainLoss: 0.4072542190551758\n",
      "cnt: 0 - valLoss: 0.42841896414756775 - trainLoss: 0.4072527587413788\n",
      "cnt: 0 - valLoss: 0.4284185469150543 - trainLoss: 0.4072512984275818\n",
      "cnt: 0 - valLoss: 0.4284180700778961 - trainLoss: 0.4072498381137848\n",
      "cnt: 0 - valLoss: 0.4284176826477051 - trainLoss: 0.4072483479976654\n",
      "cnt: 0 - valLoss: 0.4284172058105469 - trainLoss: 0.4072468876838684\n",
      "cnt: 0 - valLoss: 0.42841672897338867 - trainLoss: 0.4072454273700714\n",
      "cnt: 0 - valLoss: 0.42841631174087524 - trainLoss: 0.407243937253952\n",
      "cnt: 0 - valLoss: 0.42841586470603943 - trainLoss: 0.4072425365447998\n",
      "cnt: 0 - valLoss: 0.428415447473526 - trainLoss: 0.40724101662635803\n",
      "cnt: 0 - valLoss: 0.4284149706363678 - trainLoss: 0.4072395861148834\n",
      "cnt: 0 - valLoss: 0.42841455340385437 - trainLoss: 0.4072381258010864\n",
      "cnt: 0 - valLoss: 0.42841407656669617 - trainLoss: 0.40723663568496704\n",
      "cnt: 0 - valLoss: 0.4284136891365051 - trainLoss: 0.40723517537117004\n",
      "cnt: 0 - valLoss: 0.4284132122993469 - trainLoss: 0.40723371505737305\n",
      "cnt: 0 - valLoss: 0.4284127950668335 - trainLoss: 0.40723222494125366\n",
      "cnt: 0 - valLoss: 0.4284123182296753 - trainLoss: 0.40723079442977905\n",
      "cnt: 0 - valLoss: 0.42841193079948425 - trainLoss: 0.40722930431365967\n",
      "cnt: 0 - valLoss: 0.42841145396232605 - trainLoss: 0.40722784399986267\n",
      "cnt: 0 - valLoss: 0.4284110367298126 - trainLoss: 0.4072263836860657\n",
      "cnt: 0 - valLoss: 0.4284105598926544 - trainLoss: 0.4072248935699463\n",
      "cnt: 0 - valLoss: 0.4284101724624634 - trainLoss: 0.40722349286079407\n",
      "cnt: 0 - valLoss: 0.4284096956253052 - trainLoss: 0.4072219729423523\n",
      "cnt: 0 - valLoss: 0.42840927839279175 - trainLoss: 0.4072205126285553\n",
      "cnt: 0 - valLoss: 0.4284088611602783 - trainLoss: 0.4072190225124359\n",
      "cnt: 0 - valLoss: 0.4284084141254425 - trainLoss: 0.4072176218032837\n",
      "cnt: 0 - valLoss: 0.4284079074859619 - trainLoss: 0.4072161316871643\n",
      "cnt: 0 - valLoss: 0.4284075200557709 - trainLoss: 0.4072146713733673\n",
      "cnt: 0 - valLoss: 0.42840704321861267 - trainLoss: 0.4072132110595703\n",
      "cnt: 0 - valLoss: 0.42840662598609924 - trainLoss: 0.4072117507457733\n",
      "cnt: 0 - valLoss: 0.4284061789512634 - trainLoss: 0.4072102904319763\n",
      "cnt: 0 - valLoss: 0.4284057319164276 - trainLoss: 0.4072088301181793\n",
      "cnt: 0 - valLoss: 0.4284052848815918 - trainLoss: 0.40720734000205994\n",
      "cnt: 0 - valLoss: 0.42840486764907837 - trainLoss: 0.40720584988594055\n",
      "cnt: 0 - valLoss: 0.42840439081192017 - trainLoss: 0.40720438957214355\n",
      "cnt: 0 - valLoss: 0.42840397357940674 - trainLoss: 0.40720298886299133\n",
      "cnt: 0 - valLoss: 0.4284035265445709 - trainLoss: 0.40720146894454956\n",
      "cnt: 0 - valLoss: 0.4284031093120575 - trainLoss: 0.40720003843307495\n",
      "cnt: 0 - valLoss: 0.4284026622772217 - trainLoss: 0.40719857811927795\n",
      "cnt: 0 - valLoss: 0.42840221524238586 - trainLoss: 0.40719711780548096\n",
      "cnt: 0 - valLoss: 0.42840173840522766 - trainLoss: 0.4071956276893616\n",
      "cnt: 0 - valLoss: 0.4284013509750366 - trainLoss: 0.4071941673755646\n",
      "cnt: 0 - valLoss: 0.4284008741378784 - trainLoss: 0.4071927070617676\n",
      "cnt: 0 - valLoss: 0.428400456905365 - trainLoss: 0.4071912467479706\n",
      "cnt: 0 - valLoss: 0.4284000098705292 - trainLoss: 0.4071897864341736\n",
      "cnt: 0 - valLoss: 0.42839959263801575 - trainLoss: 0.4071883261203766\n",
      "cnt: 0 - valLoss: 0.4283991754055023 - trainLoss: 0.4071868360042572\n",
      "cnt: 0 - valLoss: 0.4283986985683441 - trainLoss: 0.4071853756904602\n",
      "cnt: 0 - valLoss: 0.4283982515335083 - trainLoss: 0.4071839451789856\n",
      "cnt: 0 - valLoss: 0.4283977746963501 - trainLoss: 0.4071824848651886\n",
      "cnt: 0 - valLoss: 0.42839735746383667 - trainLoss: 0.4071810245513916\n",
      "cnt: 0 - valLoss: 0.42839694023132324 - trainLoss: 0.4071795642375946\n",
      "cnt: 0 - valLoss: 0.4283964931964874 - trainLoss: 0.4071781039237976\n",
      "cnt: 0 - valLoss: 0.428396075963974 - trainLoss: 0.4071766138076782\n",
      "cnt: 0 - valLoss: 0.4283955991268158 - trainLoss: 0.4071751534938812\n",
      "cnt: 0 - valLoss: 0.4283951222896576 - trainLoss: 0.40717369318008423\n",
      "cnt: 0 - valLoss: 0.42839473485946655 - trainLoss: 0.40717220306396484\n",
      "cnt: 0 - valLoss: 0.42839428782463074 - trainLoss: 0.40717074275016785\n",
      "cnt: 0 - valLoss: 0.4283938407897949 - trainLoss: 0.40716928243637085\n",
      "cnt: 0 - valLoss: 0.4283934235572815 - trainLoss: 0.40716785192489624\n",
      "cnt: 0 - valLoss: 0.4283929765224457 - trainLoss: 0.40716639161109924\n",
      "cnt: 0 - valLoss: 0.42839252948760986 - trainLoss: 0.40716493129730225\n",
      "cnt: 0 - valLoss: 0.42839208245277405 - trainLoss: 0.40716347098350525\n",
      "cnt: 0 - valLoss: 0.42839160561561584 - trainLoss: 0.40716201066970825\n",
      "cnt: 0 - valLoss: 0.4283911883831024 - trainLoss: 0.40716052055358887\n",
      "cnt: 0 - valLoss: 0.428390771150589 - trainLoss: 0.40715911984443665\n",
      "cnt: 0 - valLoss: 0.4283902943134308 - trainLoss: 0.4071575999259949\n",
      "cnt: 0 - valLoss: 0.42838990688323975 - trainLoss: 0.4071561098098755\n",
      "cnt: 0 - valLoss: 0.42838943004608154 - trainLoss: 0.4071546494960785\n",
      "cnt: 0 - valLoss: 0.4283890128135681 - trainLoss: 0.4071531891822815\n",
      "cnt: 0 - valLoss: 0.4283885359764099 - trainLoss: 0.4071517586708069\n",
      "cnt: 0 - valLoss: 0.4283880889415741 - trainLoss: 0.4071502983570099\n",
      "cnt: 0 - valLoss: 0.42838767170906067 - trainLoss: 0.4071488380432129\n",
      "cnt: 0 - valLoss: 0.42838725447654724 - trainLoss: 0.4071473777294159\n",
      "cnt: 0 - valLoss: 0.4283868074417114 - trainLoss: 0.4071458876132965\n",
      "cnt: 0 - valLoss: 0.4283863306045532 - trainLoss: 0.4071444571018219\n",
      "cnt: 0 - valLoss: 0.4283858835697174 - trainLoss: 0.4071429669857025\n",
      "cnt: 0 - valLoss: 0.4283854365348816 - trainLoss: 0.4071415066719055\n",
      "cnt: 0 - valLoss: 0.42838501930236816 - trainLoss: 0.4071400463581085\n",
      "cnt: 0 - valLoss: 0.42838454246520996 - trainLoss: 0.4071386158466339\n",
      "cnt: 0 - valLoss: 0.42838406562805176 - trainLoss: 0.4071371555328369\n",
      "cnt: 0 - valLoss: 0.4283836781978607 - trainLoss: 0.4071356952190399\n",
      "cnt: 0 - valLoss: 0.4283832311630249 - trainLoss: 0.40713420510292053\n",
      "cnt: 0 - valLoss: 0.42838284373283386 - trainLoss: 0.40713274478912354\n",
      "cnt: 0 - valLoss: 0.42838236689567566 - trainLoss: 0.40713128447532654\n",
      "cnt: 0 - valLoss: 0.42838191986083984 - trainLoss: 0.40712982416152954\n",
      "cnt: 0 - valLoss: 0.42838147282600403 - trainLoss: 0.40712836384773254\n",
      "cnt: 0 - valLoss: 0.4283810257911682 - trainLoss: 0.40712693333625793\n",
      "cnt: 0 - valLoss: 0.42838054895401 - trainLoss: 0.40712547302246094\n",
      "cnt: 0 - valLoss: 0.4283801317214966 - trainLoss: 0.40712401270866394\n",
      "cnt: 0 - valLoss: 0.42837971448898315 - trainLoss: 0.40712252259254456\n",
      "cnt: 0 - valLoss: 0.42837926745414734 - trainLoss: 0.40712106227874756\n",
      "cnt: 0 - valLoss: 0.4283788502216339 - trainLoss: 0.40711960196495056\n",
      "cnt: 0 - valLoss: 0.4283784031867981 - trainLoss: 0.4071181118488312\n",
      "cnt: 0 - valLoss: 0.42837798595428467 - trainLoss: 0.40711668133735657\n",
      "cnt: 0 - valLoss: 0.42837750911712646 - trainLoss: 0.40711522102355957\n",
      "cnt: 0 - valLoss: 0.42837709188461304 - trainLoss: 0.4071137309074402\n",
      "cnt: 0 - valLoss: 0.42837661504745483 - trainLoss: 0.4071122705936432\n",
      "cnt: 0 - valLoss: 0.4283761978149414 - trainLoss: 0.4071108400821686\n",
      "cnt: 0 - valLoss: 0.4283757507801056 - trainLoss: 0.4071093797683716\n",
      "cnt: 0 - valLoss: 0.42837533354759216 - trainLoss: 0.4071079194545746\n",
      "cnt: 0 - valLoss: 0.42837488651275635 - trainLoss: 0.4071064591407776\n",
      "cnt: 0 - valLoss: 0.4283744692802429 - trainLoss: 0.4071049988269806\n",
      "cnt: 0 - valLoss: 0.4283740520477295 - trainLoss: 0.4071035385131836\n",
      "cnt: 0 - valLoss: 0.4283736050128937 - trainLoss: 0.407102108001709\n",
      "cnt: 0 - valLoss: 0.42837315797805786 - trainLoss: 0.407100647687912\n",
      "cnt: 0 - valLoss: 0.42837271094322205 - trainLoss: 0.4070991277694702\n",
      "cnt: 0 - valLoss: 0.42837223410606384 - trainLoss: 0.407097727060318\n",
      "cnt: 0 - valLoss: 0.4283718168735504 - trainLoss: 0.407096266746521\n",
      "cnt: 0 - valLoss: 0.428371399641037 - trainLoss: 0.4070947766304016\n",
      "cnt: 0 - valLoss: 0.42837095260620117 - trainLoss: 0.4070933759212494\n",
      "cnt: 0 - valLoss: 0.42837047576904297 - trainLoss: 0.4070918560028076\n",
      "cnt: 0 - valLoss: 0.42837002873420715 - trainLoss: 0.4070903956890106\n",
      "cnt: 0 - valLoss: 0.42836958169937134 - trainLoss: 0.407088965177536\n",
      "cnt: 0 - valLoss: 0.4283691644668579 - trainLoss: 0.407087504863739\n",
      "cnt: 0 - valLoss: 0.4283687174320221 - trainLoss: 0.407086044549942\n",
      "cnt: 0 - valLoss: 0.42836830019950867 - trainLoss: 0.407084584236145\n",
      "cnt: 0 - valLoss: 0.42836788296699524 - trainLoss: 0.40708309412002563\n",
      "cnt: 0 - valLoss: 0.4283674359321594 - trainLoss: 0.40708163380622864\n",
      "cnt: 0 - valLoss: 0.42836692929267883 - trainLoss: 0.40708017349243164\n",
      "cnt: 0 - valLoss: 0.4283665418624878 - trainLoss: 0.40707871317863464\n",
      "cnt: 0 - valLoss: 0.4283660650253296 - trainLoss: 0.40707728266716003\n",
      "cnt: 0 - valLoss: 0.42836564779281616 - trainLoss: 0.40707582235336304\n",
      "cnt: 0 - valLoss: 0.42836520075798035 - trainLoss: 0.40707436203956604\n",
      "cnt: 0 - valLoss: 0.4283647835254669 - trainLoss: 0.40707290172576904\n",
      "cnt: 0 - valLoss: 0.4283643066883087 - trainLoss: 0.40707144141197205\n",
      "cnt: 0 - valLoss: 0.4283638894557953 - trainLoss: 0.40706998109817505\n",
      "cnt: 0 - valLoss: 0.4283634424209595 - trainLoss: 0.40706855058670044\n",
      "cnt: 0 - valLoss: 0.42836299538612366 - trainLoss: 0.40706709027290344\n",
      "cnt: 0 - valLoss: 0.42836254835128784 - trainLoss: 0.40706562995910645\n",
      "cnt: 0 - valLoss: 0.4283621311187744 - trainLoss: 0.40706413984298706\n",
      "cnt: 0 - valLoss: 0.4283616542816162 - trainLoss: 0.40706270933151245\n",
      "cnt: 0 - valLoss: 0.42836126685142517 - trainLoss: 0.40706124901771545\n",
      "cnt: 0 - valLoss: 0.42836079001426697 - trainLoss: 0.40705981850624084\n",
      "cnt: 0 - valLoss: 0.42836037278175354 - trainLoss: 0.40705835819244385\n",
      "cnt: 0 - valLoss: 0.42835989594459534 - trainLoss: 0.40705689787864685\n",
      "cnt: 0 - valLoss: 0.4283594787120819 - trainLoss: 0.40705540776252747\n",
      "cnt: 0 - valLoss: 0.4283590316772461 - trainLoss: 0.40705394744873047\n",
      "cnt: 0 - valLoss: 0.42835861444473267 - trainLoss: 0.40705248713493347\n",
      "cnt: 0 - valLoss: 0.42835819721221924 - trainLoss: 0.4070510268211365\n",
      "cnt: 0 - valLoss: 0.4283577501773834 - trainLoss: 0.40704962611198425\n",
      "cnt: 0 - valLoss: 0.4283572733402252 - trainLoss: 0.40704813599586487\n",
      "cnt: 0 - valLoss: 0.4283568561077118 - trainLoss: 0.40704670548439026\n",
      "cnt: 0 - valLoss: 0.4283563792705536 - trainLoss: 0.4070451557636261\n",
      "cnt: 0 - valLoss: 0.42835596203804016 - trainLoss: 0.4070437550544739\n",
      "cnt: 0 - valLoss: 0.42835548520088196 - trainLoss: 0.4070422947406769\n",
      "cnt: 0 - valLoss: 0.4283550977706909 - trainLoss: 0.4070408046245575\n",
      "cnt: 0 - valLoss: 0.4283546209335327 - trainLoss: 0.4070394039154053\n",
      "cnt: 0 - valLoss: 0.4283542037010193 - trainLoss: 0.4070378839969635\n",
      "cnt: 0 - valLoss: 0.4283537268638611 - trainLoss: 0.4070364236831665\n",
      "cnt: 0 - valLoss: 0.42835330963134766 - trainLoss: 0.4070349931716919\n",
      "cnt: 0 - valLoss: 0.42835286259651184 - trainLoss: 0.4070335626602173\n",
      "cnt: 0 - valLoss: 0.4283524453639984 - trainLoss: 0.4070320725440979\n",
      "cnt: 0 - valLoss: 0.4283519983291626 - trainLoss: 0.4070306122303009\n",
      "cnt: 0 - valLoss: 0.42835158109664917 - trainLoss: 0.4070292115211487\n",
      "cnt: 0 - valLoss: 0.42835110425949097 - trainLoss: 0.4070276916027069\n",
      "cnt: 0 - valLoss: 0.42835068702697754 - trainLoss: 0.4070262610912323\n",
      "cnt: 0 - valLoss: 0.42835021018981934 - trainLoss: 0.4070248007774353\n",
      "cnt: 0 - valLoss: 0.4283497929573059 - trainLoss: 0.4070233404636383\n",
      "cnt: 0 - valLoss: 0.4283493459224701 - trainLoss: 0.4070218801498413\n",
      "cnt: 0 - valLoss: 0.4283488988876343 - trainLoss: 0.4070204198360443\n",
      "cnt: 0 - valLoss: 0.4283486604690552 - trainLoss: 0.4070189893245697\n",
      "cnt: 0 - valLoss: 0.4283484220504761 - trainLoss: 0.4070175290107727\n",
      "cnt: 0 - valLoss: 0.42834797501564026 - trainLoss: 0.4070160686969757\n",
      "cnt: 0 - valLoss: 0.42834773659706116 - trainLoss: 0.4070146083831787\n",
      "cnt: 0 - valLoss: 0.42834728956222534 - trainLoss: 0.4070131480693817\n",
      "cnt: 0 - valLoss: 0.42834705114364624 - trainLoss: 0.4070116877555847\n",
      "cnt: 0 - valLoss: 0.4283466041088104 - trainLoss: 0.4070102572441101\n",
      "cnt: 0 - valLoss: 0.4283463656902313 - trainLoss: 0.4070087969303131\n",
      "cnt: 0 - valLoss: 0.42834609746932983 - trainLoss: 0.4070073366165161\n",
      "cnt: 0 - valLoss: 0.4283456802368164 - trainLoss: 0.4070058763027191\n",
      "cnt: 0 - valLoss: 0.4283454120159149 - trainLoss: 0.4070044159889221\n",
      "cnt: 0 - valLoss: 0.4283449947834015 - trainLoss: 0.4070029556751251\n",
      "cnt: 0 - valLoss: 0.4283447265625 - trainLoss: 0.4070015251636505\n",
      "cnt: 0 - valLoss: 0.4283443093299866 - trainLoss: 0.4070000648498535\n",
      "cnt: 0 - valLoss: 0.4283440411090851 - trainLoss: 0.4069986045360565\n",
      "cnt: 0 - valLoss: 0.4283437728881836 - trainLoss: 0.4069971442222595\n",
      "cnt: 0 - valLoss: 0.4283433258533478 - trainLoss: 0.4069956839084625\n",
      "cnt: 0 - valLoss: 0.4283430874347687 - trainLoss: 0.4069942235946655\n",
      "cnt: 0 - valLoss: 0.42834267020225525 - trainLoss: 0.4069927930831909\n",
      "cnt: 0 - valLoss: 0.42834240198135376 - trainLoss: 0.4069913327693939\n",
      "cnt: 0 - valLoss: 0.42834195494651794 - trainLoss: 0.4069898724555969\n",
      "cnt: 0 - valLoss: 0.42834171652793884 - trainLoss: 0.4069884121417999\n",
      "cnt: 0 - valLoss: 0.42834141850471497 - trainLoss: 0.40698695182800293\n",
      "cnt: 0 - valLoss: 0.42834094166755676 - trainLoss: 0.40698549151420593\n",
      "cnt: 0 - valLoss: 0.42834070324897766 - trainLoss: 0.4069840610027313\n",
      "cnt: 0 - valLoss: 0.42834025621414185 - trainLoss: 0.4069826006889343\n",
      "cnt: 0 - valLoss: 0.42834004759788513 - trainLoss: 0.40698114037513733\n",
      "cnt: 0 - valLoss: 0.42833974957466125 - trainLoss: 0.40697968006134033\n",
      "cnt: 0 - valLoss: 0.4283393323421478 - trainLoss: 0.40697821974754333\n",
      "cnt: 0 - valLoss: 0.42833906412124634 - trainLoss: 0.40697675943374634\n",
      "cnt: 0 - valLoss: 0.42833855748176575 - trainLoss: 0.40697532892227173\n",
      "cnt: 0 - valLoss: 0.42833828926086426 - trainLoss: 0.40697386860847473\n",
      "cnt: 0 - valLoss: 0.42833778262138367 - trainLoss: 0.40697240829467773\n",
      "cnt: 0 - valLoss: 0.4283375144004822 - trainLoss: 0.40697094798088074\n",
      "cnt: 0 - valLoss: 0.4283372759819031 - trainLoss: 0.40696948766708374\n",
      "cnt: 0 - valLoss: 0.4283367395401001 - trainLoss: 0.40696802735328674\n",
      "cnt: 0 - valLoss: 0.428336501121521 - trainLoss: 0.40696659684181213\n",
      "cnt: 0 - valLoss: 0.4283359944820404 - trainLoss: 0.40696513652801514\n",
      "cnt: 0 - valLoss: 0.4283357262611389 - trainLoss: 0.40696367621421814\n",
      "cnt: 0 - valLoss: 0.4283354580402374 - trainLoss: 0.40696221590042114\n",
      "cnt: 0 - valLoss: 0.42833495140075684 - trainLoss: 0.40696075558662415\n",
      "cnt: 0 - valLoss: 0.42833468317985535 - trainLoss: 0.40695929527282715\n",
      "cnt: 0 - valLoss: 0.42833417654037476 - trainLoss: 0.40695786476135254\n",
      "cnt: 0 - valLoss: 0.42833393812179565 - trainLoss: 0.40695640444755554\n",
      "cnt: 0 - valLoss: 0.42833366990089417 - trainLoss: 0.40695494413375854\n",
      "cnt: 0 - valLoss: 0.4283331632614136 - trainLoss: 0.40695348381996155\n",
      "cnt: 0 - valLoss: 0.4283328652381897 - trainLoss: 0.40695202350616455\n",
      "cnt: 0 - valLoss: 0.4283323884010315 - trainLoss: 0.40695056319236755\n",
      "cnt: 0 - valLoss: 0.42833212018013 - trainLoss: 0.40694913268089294\n",
      "cnt: 0 - valLoss: 0.42833179235458374 - trainLoss: 0.40694767236709595\n",
      "cnt: 0 - valLoss: 0.42833131551742554 - trainLoss: 0.40694621205329895\n",
      "cnt: 0 - valLoss: 0.42833104729652405 - trainLoss: 0.40694475173950195\n",
      "cnt: 0 - valLoss: 0.42833054065704346 - trainLoss: 0.40694329142570496\n",
      "cnt: 0 - valLoss: 0.4283302426338196 - trainLoss: 0.40694189071655273\n",
      "cnt: 0 - valLoss: 0.4283299744129181 - trainLoss: 0.40694043040275574\n",
      "cnt: 0 - valLoss: 0.4283294975757599 - trainLoss: 0.40693897008895874\n",
      "cnt: 0 - valLoss: 0.428329199552536 - trainLoss: 0.40693750977516174\n",
      "cnt: 0 - valLoss: 0.42832890152931213 - trainLoss: 0.40693601965904236\n",
      "cnt: 0 - valLoss: 0.42832842469215393 - trainLoss: 0.40693455934524536\n",
      "cnt: 0 - valLoss: 0.42832812666893005 - trainLoss: 0.40693315863609314\n",
      "cnt: 0 - valLoss: 0.42832762002944946 - trainLoss: 0.40693166851997375\n",
      "cnt: 0 - valLoss: 0.4283273220062256 - trainLoss: 0.40693023800849915\n",
      "cnt: 0 - valLoss: 0.4283270537853241 - trainLoss: 0.40692877769470215\n",
      "cnt: 0 - valLoss: 0.4283265471458435 - trainLoss: 0.40692734718322754\n",
      "cnt: 0 - valLoss: 0.42832624912261963 - trainLoss: 0.40692588686943054\n",
      "cnt: 0 - valLoss: 0.4283257722854614 - trainLoss: 0.40692442655563354\n",
      "cnt: 0 - valLoss: 0.42832547426223755 - trainLoss: 0.4069230258464813\n",
      "cnt: 0 - valLoss: 0.42832517623901367 - trainLoss: 0.40692147612571716\n",
      "cnt: 0 - valLoss: 0.4283246695995331 - trainLoss: 0.40692004561424255\n",
      "cnt: 0 - valLoss: 0.4283243715763092 - trainLoss: 0.40691864490509033\n",
      "cnt: 0 - valLoss: 0.4283240735530853 - trainLoss: 0.40691715478897095\n",
      "cnt: 0 - valLoss: 0.42832356691360474 - trainLoss: 0.40691566467285156\n",
      "cnt: 0 - valLoss: 0.42832323908805847 - trainLoss: 0.40691423416137695\n",
      "cnt: 0 - valLoss: 0.4283229410648346 - trainLoss: 0.40691277384757996\n",
      "cnt: 0 - valLoss: 0.428322434425354 - trainLoss: 0.40691131353378296\n",
      "cnt: 0 - valLoss: 0.4283221364021301 - trainLoss: 0.40690991282463074\n",
      "cnt: 0 - valLoss: 0.42832183837890625 - trainLoss: 0.40690842270851135\n",
      "cnt: 0 - valLoss: 0.42832133173942566 - trainLoss: 0.40690696239471436\n",
      "cnt: 0 - valLoss: 0.42832106351852417 - trainLoss: 0.40690556168556213\n",
      "cnt: 0 - valLoss: 0.4283207356929779 - trainLoss: 0.40690410137176514\n",
      "cnt: 0 - valLoss: 0.4283201992511749 - trainLoss: 0.40690264105796814\n",
      "cnt: 0 - valLoss: 0.42831990122795105 - trainLoss: 0.40690118074417114\n",
      "cnt: 0 - valLoss: 0.4283196032047272 - trainLoss: 0.40689972043037415\n",
      "cnt: 0 - valLoss: 0.4283190965652466 - trainLoss: 0.40689826011657715\n",
      "cnt: 0 - valLoss: 0.4283187985420227 - trainLoss: 0.40689682960510254\n",
      "cnt: 0 - valLoss: 0.42831850051879883 - trainLoss: 0.40689536929130554\n",
      "cnt: 0 - valLoss: 0.42831793427467346 - trainLoss: 0.40689390897750854\n",
      "cnt: 0 - valLoss: 0.4283176362514496 - trainLoss: 0.40689244866371155\n",
      "cnt: 0 - valLoss: 0.4283173680305481 - trainLoss: 0.40689098834991455\n",
      "cnt: 0 - valLoss: 0.4283168613910675 - trainLoss: 0.40688952803611755\n",
      "cnt: 0 - valLoss: 0.42831653356552124 - trainLoss: 0.40688812732696533\n",
      "cnt: 0 - valLoss: 0.4283161759376526 - trainLoss: 0.40688663721084595\n",
      "cnt: 0 - valLoss: 0.428315669298172 - trainLoss: 0.40688517689704895\n",
      "cnt: 0 - valLoss: 0.4283153712749481 - trainLoss: 0.40688371658325195\n",
      "cnt: 0 - valLoss: 0.42831507325172424 - trainLoss: 0.40688231587409973\n",
      "cnt: 0 - valLoss: 0.42831456661224365 - trainLoss: 0.40688085556030273\n",
      "cnt: 0 - valLoss: 0.428314208984375 - trainLoss: 0.40687939524650574\n",
      "cnt: 0 - valLoss: 0.4283139109611511 - trainLoss: 0.4068779945373535\n",
      "cnt: 0 - valLoss: 0.42831340432167053 - trainLoss: 0.4068765342235565\n",
      "cnt: 0 - valLoss: 0.42831310629844666 - trainLoss: 0.4068750739097595\n",
      "cnt: 0 - valLoss: 0.4283128082752228 - trainLoss: 0.4068736135959625\n",
      "cnt: 0 - valLoss: 0.4283122420310974 - trainLoss: 0.4068721532821655\n",
      "cnt: 0 - valLoss: 0.42831194400787354 - trainLoss: 0.4068707227706909\n",
      "cnt: 0 - valLoss: 0.42831164598464966 - trainLoss: 0.40686920285224915\n",
      "cnt: 0 - valLoss: 0.4283111095428467 - trainLoss: 0.4068678021430969\n",
      "cnt: 0 - valLoss: 0.4283107817173004 - trainLoss: 0.4068663418292999\n",
      "cnt: 0 - valLoss: 0.42831048369407654 - trainLoss: 0.40686485171318054\n",
      "cnt: 0 - valLoss: 0.42831000685691833 - trainLoss: 0.40686339139938354\n",
      "cnt: 0 - valLoss: 0.42830967903137207 - trainLoss: 0.4068619906902313\n",
      "cnt: 0 - valLoss: 0.4283093512058258 - trainLoss: 0.4068605303764343\n",
      "cnt: 0 - valLoss: 0.4283088445663452 - trainLoss: 0.40685907006263733\n",
      "cnt: 0 - valLoss: 0.42830854654312134 - trainLoss: 0.40685760974884033\n",
      "cnt: 0 - valLoss: 0.4283082187175751 - trainLoss: 0.40685614943504333\n",
      "cnt: 0 - valLoss: 0.4283076822757721 - trainLoss: 0.40685468912124634\n",
      "cnt: 0 - valLoss: 0.42830735445022583 - trainLoss: 0.40685325860977173\n",
      "cnt: 0 - valLoss: 0.42830705642700195 - trainLoss: 0.40685179829597473\n",
      "cnt: 0 - valLoss: 0.4283064901828766 - trainLoss: 0.40685033798217773\n",
      "cnt: 0 - valLoss: 0.4283061921596527 - trainLoss: 0.4068489372730255\n",
      "cnt: 0 - valLoss: 0.42830589413642883 - trainLoss: 0.4068474769592285\n",
      "cnt: 0 - valLoss: 0.4283055365085602 - trainLoss: 0.4068460166454315\n",
      "cnt: 0 - valLoss: 0.4283050298690796 - trainLoss: 0.4068445563316345\n",
      "cnt: 0 - valLoss: 0.4283047318458557 - trainLoss: 0.4068430960178375\n",
      "cnt: 0 - valLoss: 0.42830443382263184 - trainLoss: 0.4068416953086853\n",
      "cnt: 0 - valLoss: 0.42830386757850647 - trainLoss: 0.4068402051925659\n",
      "cnt: 0 - valLoss: 0.4283035695552826 - trainLoss: 0.4068387746810913\n",
      "cnt: 0 - valLoss: 0.42830324172973633 - trainLoss: 0.4068372845649719\n",
      "cnt: 0 - valLoss: 0.42830273509025574 - trainLoss: 0.4068358838558197\n",
      "cnt: 0 - valLoss: 0.4283023774623871 - trainLoss: 0.4068344235420227\n",
      "cnt: 0 - valLoss: 0.4283020794391632 - trainLoss: 0.4068329632282257\n",
      "cnt: 0 - valLoss: 0.42830151319503784 - trainLoss: 0.4068315029144287\n",
      "cnt: 0 - valLoss: 0.42830121517181396 - trainLoss: 0.4068300426006317\n",
      "cnt: 0 - valLoss: 0.4283009171485901 - trainLoss: 0.4068285822868347\n",
      "cnt: 0 - valLoss: 0.4283005893230438 - trainLoss: 0.4068271517753601\n",
      "cnt: 0 - valLoss: 0.42830002307891846 - trainLoss: 0.4068256914615631\n",
      "cnt: 0 - valLoss: 0.4282997250556946 - trainLoss: 0.4068242311477661\n",
      "cnt: 0 - valLoss: 0.4282993674278259 - trainLoss: 0.4068227708339691\n",
      "cnt: 0 - valLoss: 0.42829886078834534 - trainLoss: 0.4068213701248169\n",
      "cnt: 0 - valLoss: 0.4282985329627991 - trainLoss: 0.4068199098110199\n",
      "cnt: 0 - valLoss: 0.4282981753349304 - trainLoss: 0.4068184494972229\n",
      "cnt: 0 - valLoss: 0.42829784750938416 - trainLoss: 0.4068169593811035\n",
      "cnt: 0 - valLoss: 0.4282973110675812 - trainLoss: 0.4068155288696289\n",
      "cnt: 0 - valLoss: 0.4282969832420349 - trainLoss: 0.4068140685558319\n",
      "cnt: 0 - valLoss: 0.42829668521881104 - trainLoss: 0.4068126678466797\n",
      "cnt: 0 - valLoss: 0.42829611897468567 - trainLoss: 0.4068112075328827\n",
      "cnt: 0 - valLoss: 0.4282957911491394 - trainLoss: 0.4068097472190857\n",
      "cnt: 0 - valLoss: 0.4282954931259155 - trainLoss: 0.4068083167076111\n",
      "cnt: 0 - valLoss: 0.42829516530036926 - trainLoss: 0.4068068563938141\n",
      "cnt: 0 - valLoss: 0.4282946288585663 - trainLoss: 0.4068053960800171\n",
      "cnt: 0 - valLoss: 0.42829427123069763 - trainLoss: 0.4068039357662201\n",
      "cnt: 0 - valLoss: 0.42829394340515137 - trainLoss: 0.4068024754524231\n",
      "cnt: 0 - valLoss: 0.4282935857772827 - trainLoss: 0.4068010747432709\n",
      "cnt: 0 - valLoss: 0.4282930791378021 - trainLoss: 0.4067996144294739\n",
      "cnt: 0 - valLoss: 0.42829278111457825 - trainLoss: 0.4067981541156769\n",
      "cnt: 0 - valLoss: 0.4282924234867096 - trainLoss: 0.4067966938018799\n",
      "cnt: 0 - valLoss: 0.4282918870449066 - trainLoss: 0.40679529309272766\n",
      "cnt: 0 - valLoss: 0.42829152941703796 - trainLoss: 0.40679383277893066\n",
      "cnt: 0 - valLoss: 0.4282912313938141 - trainLoss: 0.40679237246513367\n",
      "cnt: 0 - valLoss: 0.4282909035682678 - trainLoss: 0.40679094195365906\n",
      "cnt: 0 - valLoss: 0.42829033732414246 - trainLoss: 0.40678951144218445\n",
      "cnt: 0 - valLoss: 0.4282899796962738 - trainLoss: 0.40678802132606506\n",
      "cnt: 0 - valLoss: 0.4282896816730499 - trainLoss: 0.40678656101226807\n",
      "cnt: 0 - valLoss: 0.42828935384750366 - trainLoss: 0.40678510069847107\n",
      "cnt: 0 - valLoss: 0.4282887876033783 - trainLoss: 0.4067836403846741\n",
      "cnt: 0 - valLoss: 0.42828845977783203 - trainLoss: 0.40678223967552185\n",
      "cnt: 0 - valLoss: 0.4282881021499634 - trainLoss: 0.40678077936172485\n",
      "cnt: 0 - valLoss: 0.4282878041267395 - trainLoss: 0.40677931904792786\n",
      "cnt: 0 - valLoss: 0.42828723788261414 - trainLoss: 0.40677791833877563\n",
      "cnt: 0 - valLoss: 0.42828693985939026 - trainLoss: 0.40677645802497864\n",
      "cnt: 0 - valLoss: 0.4282865524291992 - trainLoss: 0.40677499771118164\n",
      "cnt: 0 - valLoss: 0.42828601598739624 - trainLoss: 0.40677353739738464\n",
      "cnt: 0 - valLoss: 0.4282856583595276 - trainLoss: 0.40677210688591003\n",
      "cnt: 0 - valLoss: 0.4282853603363037 - trainLoss: 0.40677064657211304\n",
      "cnt: 0 - valLoss: 0.42828503251075745 - trainLoss: 0.40676918625831604\n",
      "cnt: 0 - valLoss: 0.4282844662666321 - trainLoss: 0.40676772594451904\n",
      "cnt: 0 - valLoss: 0.4282841682434082 - trainLoss: 0.4067663252353668\n",
      "cnt: 0 - valLoss: 0.42828381061553955 - trainLoss: 0.4067648649215698\n",
      "cnt: 0 - valLoss: 0.4282834231853485 - trainLoss: 0.4067634344100952\n",
      "cnt: 0 - valLoss: 0.4282829165458679 - trainLoss: 0.4067620038986206\n",
      "cnt: 0 - valLoss: 0.42828258872032166 - trainLoss: 0.4067605435848236\n",
      "cnt: 0 - valLoss: 0.428282231092453 - trainLoss: 0.4067590832710266\n",
      "cnt: 0 - valLoss: 0.42828190326690674 - trainLoss: 0.4067576229572296\n",
      "cnt: 0 - valLoss: 0.42828133702278137 - trainLoss: 0.4067561626434326\n",
      "cnt: 0 - valLoss: 0.4282810389995575 - trainLoss: 0.4067547023296356\n",
      "cnt: 0 - valLoss: 0.42828068137168884 - trainLoss: 0.406753271818161\n",
      "cnt: 0 - valLoss: 0.4282802939414978 - trainLoss: 0.4067518413066864\n",
      "cnt: 0 - valLoss: 0.4282797574996948 - trainLoss: 0.4067503809928894\n",
      "cnt: 0 - valLoss: 0.42827939987182617 - trainLoss: 0.4067489504814148\n",
      "cnt: 0 - valLoss: 0.4282790720462799 - trainLoss: 0.4067475199699402\n",
      "cnt: 0 - valLoss: 0.42827877402305603 - trainLoss: 0.4067460596561432\n",
      "cnt: 0 - valLoss: 0.42827820777893066 - trainLoss: 0.4067446291446686\n",
      "cnt: 0 - valLoss: 0.428277850151062 - trainLoss: 0.4067431688308716\n",
      "cnt: 0 - valLoss: 0.42827752232551575 - trainLoss: 0.4067417085170746\n",
      "cnt: 0 - valLoss: 0.4282771646976471 - trainLoss: 0.4067402482032776\n",
      "cnt: 0 - valLoss: 0.4282766282558441 - trainLoss: 0.4067387878894806\n",
      "cnt: 0 - valLoss: 0.42827627062797546 - trainLoss: 0.40673738718032837\n",
      "cnt: 0 - valLoss: 0.4282758831977844 - trainLoss: 0.40673592686653137\n",
      "cnt: 0 - valLoss: 0.42827555537223816 - trainLoss: 0.4067344665527344\n",
      "cnt: 0 - valLoss: 0.4282750189304352 - trainLoss: 0.4067330062389374\n",
      "cnt: 0 - valLoss: 0.4282746911048889 - trainLoss: 0.40673160552978516\n",
      "cnt: 0 - valLoss: 0.42827433347702026 - trainLoss: 0.40673014521598816\n",
      "cnt: 0 - valLoss: 0.428274005651474 - trainLoss: 0.40672868490219116\n",
      "cnt: 0 - valLoss: 0.4282734990119934 - trainLoss: 0.40672722458839417\n",
      "cnt: 0 - valLoss: 0.42827311158180237 - trainLoss: 0.40672582387924194\n",
      "cnt: 0 - valLoss: 0.42827272415161133 - trainLoss: 0.40672436356544495\n",
      "cnt: 0 - valLoss: 0.42827242612838745 - trainLoss: 0.4067229628562927\n",
      "cnt: 0 - valLoss: 0.4282718598842621 - trainLoss: 0.40672147274017334\n",
      "cnt: 0 - valLoss: 0.4282715320587158 - trainLoss: 0.40672004222869873\n",
      "cnt: 0 - valLoss: 0.42827117443084717 - trainLoss: 0.40671858191490173\n",
      "cnt: 0 - valLoss: 0.4282708466053009 - trainLoss: 0.4067171514034271\n",
      "cnt: 0 - valLoss: 0.42827048897743225 - trainLoss: 0.4067156910896301\n",
      "cnt: 0 - valLoss: 0.4282698929309845 - trainLoss: 0.40671423077583313\n",
      "cnt: 0 - valLoss: 0.42826953530311584 - trainLoss: 0.4067128002643585\n",
      "cnt: 0 - valLoss: 0.4282692074775696 - trainLoss: 0.4067113697528839\n",
      "cnt: 0 - valLoss: 0.4282688498497009 - trainLoss: 0.4067099392414093\n",
      "cnt: 0 - valLoss: 0.42826831340789795 - trainLoss: 0.4067084789276123\n",
      "cnt: 0 - valLoss: 0.4282679259777069 - trainLoss: 0.4067070484161377\n",
      "cnt: 0 - valLoss: 0.42826756834983826 - trainLoss: 0.4067055881023407\n",
      "cnt: 0 - valLoss: 0.428267240524292 - trainLoss: 0.4067041277885437\n",
      "cnt: 0 - valLoss: 0.42826688289642334 - trainLoss: 0.4067026674747467\n",
      "cnt: 0 - valLoss: 0.42826634645462036 - trainLoss: 0.4067012667655945\n",
      "cnt: 0 - valLoss: 0.4282659590244293 - trainLoss: 0.4066998064517975\n",
      "cnt: 0 - valLoss: 0.42826560139656067 - trainLoss: 0.4066983461380005\n",
      "cnt: 0 - valLoss: 0.4282652735710144 - trainLoss: 0.40669694542884827\n",
      "cnt: 0 - valLoss: 0.42826470732688904 - trainLoss: 0.40669548511505127\n",
      "cnt: 0 - valLoss: 0.428264319896698 - trainLoss: 0.4066940248012543\n",
      "cnt: 0 - valLoss: 0.42826396226882935 - trainLoss: 0.4066925644874573\n",
      "cnt: 0 - valLoss: 0.4282636344432831 - trainLoss: 0.40669116377830505\n",
      "cnt: 0 - valLoss: 0.42826327681541443 - trainLoss: 0.40668970346450806\n",
      "cnt: 0 - valLoss: 0.42826277017593384 - trainLoss: 0.40668824315071106\n",
      "cnt: 0 - valLoss: 0.4282623529434204 - trainLoss: 0.40668678283691406\n",
      "cnt: 0 - valLoss: 0.42826199531555176 - trainLoss: 0.40668538212776184\n",
      "cnt: 0 - valLoss: 0.4282616674900055 - trainLoss: 0.40668392181396484\n",
      "cnt: 0 - valLoss: 0.4282611608505249 - trainLoss: 0.40668246150016785\n",
      "cnt: 0 - valLoss: 0.4282607138156891 - trainLoss: 0.40668100118637085\n",
      "cnt: 0 - valLoss: 0.4282603859901428 - trainLoss: 0.40667957067489624\n",
      "cnt: 0 - valLoss: 0.42826002836227417 - trainLoss: 0.40667814016342163\n",
      "cnt: 0 - valLoss: 0.4282597005367279 - trainLoss: 0.40667665004730225\n",
      "cnt: 0 - valLoss: 0.42825913429260254 - trainLoss: 0.40667521953582764\n",
      "cnt: 0 - valLoss: 0.4282587468624115 - trainLoss: 0.406673789024353\n",
      "cnt: 0 - valLoss: 0.42825838923454285 - trainLoss: 0.4066723585128784\n",
      "cnt: 0 - valLoss: 0.4282580614089966 - trainLoss: 0.4066708981990814\n",
      "cnt: 0 - valLoss: 0.42825746536254883 - trainLoss: 0.4066694676876068\n",
      "cnt: 0 - valLoss: 0.4282571077346802 - trainLoss: 0.4066680073738098\n",
      "cnt: 0 - valLoss: 0.4282567799091339 - trainLoss: 0.4066665768623352\n",
      "cnt: 0 - valLoss: 0.42825639247894287 - trainLoss: 0.4066651463508606\n",
      "cnt: 0 - valLoss: 0.4282560348510742 - trainLoss: 0.4066636860370636\n",
      "cnt: 0 - valLoss: 0.42825549840927124 - trainLoss: 0.406662255525589\n",
      "cnt: 0 - valLoss: 0.4282551407814026 - trainLoss: 0.4066608250141144\n",
      "cnt: 0 - valLoss: 0.42825475335121155 - trainLoss: 0.4066593647003174\n",
      "cnt: 0 - valLoss: 0.4282543659210205 - trainLoss: 0.4066579043865204\n",
      "cnt: 0 - valLoss: 0.42825403809547424 - trainLoss: 0.4066564738750458\n",
      "cnt: 0 - valLoss: 0.4282534718513489 - trainLoss: 0.40665507316589355\n",
      "cnt: 0 - valLoss: 0.42825308442115784 - trainLoss: 0.40665358304977417\n",
      "cnt: 0 - valLoss: 0.4282527565956116 - trainLoss: 0.40665215253829956\n",
      "cnt: 0 - valLoss: 0.4282523989677429 - trainLoss: 0.40665072202682495\n",
      "cnt: 0 - valLoss: 0.4282520115375519 - trainLoss: 0.40664926171302795\n",
      "cnt: 0 - valLoss: 0.4282514452934265 - trainLoss: 0.40664783120155334\n",
      "cnt: 0 - valLoss: 0.42825111746788025 - trainLoss: 0.40664640069007874\n",
      "cnt: 0 - valLoss: 0.4282507300376892 - trainLoss: 0.40664494037628174\n",
      "cnt: 0 - valLoss: 0.42825034260749817 - trainLoss: 0.40664348006248474\n",
      "cnt: 0 - valLoss: 0.4282500147819519 - trainLoss: 0.40664204955101013\n",
      "cnt: 0 - valLoss: 0.4282494783401489 - trainLoss: 0.4066406190395355\n",
      "cnt: 0 - valLoss: 0.4282490611076355 - trainLoss: 0.4066391587257385\n",
      "cnt: 0 - valLoss: 0.42824870347976685 - trainLoss: 0.40663769841194153\n",
      "cnt: 0 - valLoss: 0.4282483160495758 - trainLoss: 0.40663623809814453\n",
      "cnt: 0 - valLoss: 0.42824798822402954 - trainLoss: 0.4066348373889923\n",
      "cnt: 0 - valLoss: 0.4282473921775818 - trainLoss: 0.4066334068775177\n",
      "cnt: 0 - valLoss: 0.42824703454971313 - trainLoss: 0.4066319763660431\n",
      "cnt: 0 - valLoss: 0.4282466471195221 - trainLoss: 0.4066305160522461\n",
      "cnt: 0 - valLoss: 0.42824631929397583 - trainLoss: 0.4066290557384491\n",
      "cnt: 0 - valLoss: 0.4282459616661072 - trainLoss: 0.4066275954246521\n",
      "cnt: 0 - valLoss: 0.4282453656196594 - trainLoss: 0.4066261947154999\n",
      "cnt: 0 - valLoss: 0.42824503779411316 - trainLoss: 0.4066247344017029\n",
      "cnt: 0 - valLoss: 0.42824459075927734 - trainLoss: 0.4066232740879059\n",
      "cnt: 0 - valLoss: 0.4282442629337311 - trainLoss: 0.40662187337875366\n",
      "cnt: 0 - valLoss: 0.4282439053058624 - trainLoss: 0.40662041306495667\n",
      "cnt: 0 - valLoss: 0.4282433092594147 - trainLoss: 0.40661895275115967\n",
      "cnt: 0 - valLoss: 0.4282429814338684 - trainLoss: 0.40661749243736267\n",
      "cnt: 0 - valLoss: 0.42824262380599976 - trainLoss: 0.4066160321235657\n",
      "cnt: 0 - valLoss: 0.42824220657348633 - trainLoss: 0.40661466121673584\n",
      "cnt: 0 - valLoss: 0.4282418489456177 - trainLoss: 0.40661320090293884\n",
      "cnt: 0 - valLoss: 0.4282413125038147 - trainLoss: 0.40661177039146423\n",
      "cnt: 0 - valLoss: 0.42824092507362366 - trainLoss: 0.40661031007766724\n",
      "cnt: 0 - valLoss: 0.4282405376434326 - trainLoss: 0.4066088795661926\n",
      "cnt: 0 - valLoss: 0.42824018001556396 - trainLoss: 0.406607449054718\n",
      "cnt: 0 - valLoss: 0.4282397925853729 - trainLoss: 0.406605988740921\n",
      "cnt: 0 - valLoss: 0.42823925614356995 - trainLoss: 0.4066045582294464\n",
      "cnt: 0 - valLoss: 0.4282388985157013 - trainLoss: 0.4066031277179718\n",
      "cnt: 0 - valLoss: 0.42823851108551025 - trainLoss: 0.4066016674041748\n",
      "cnt: 0 - valLoss: 0.4282380938529968 - trainLoss: 0.4066002368927002\n",
      "cnt: 0 - valLoss: 0.4282377362251282 - trainLoss: 0.4065987765789032\n",
      "cnt: 0 - valLoss: 0.4282374083995819 - trainLoss: 0.4065973460674286\n",
      "cnt: 0 - valLoss: 0.42823684215545654 - trainLoss: 0.406595915555954\n",
      "cnt: 0 - valLoss: 0.4282364547252655 - trainLoss: 0.406594455242157\n",
      "cnt: 0 - valLoss: 0.4282360374927521 - trainLoss: 0.4065930247306824\n",
      "cnt: 0 - valLoss: 0.4282356798648834 - trainLoss: 0.4065915644168854\n",
      "cnt: 0 - valLoss: 0.42823535203933716 - trainLoss: 0.40659013390541077\n",
      "cnt: 0 - valLoss: 0.4282347559928894 - trainLoss: 0.40658873319625854\n",
      "cnt: 0 - valLoss: 0.42823439836502075 - trainLoss: 0.40658727288246155\n",
      "cnt: 0 - valLoss: 0.4282340109348297 - trainLoss: 0.40658581256866455\n",
      "cnt: 0 - valLoss: 0.42823362350463867 - trainLoss: 0.40658435225486755\n",
      "cnt: 0 - valLoss: 0.4282332956790924 - trainLoss: 0.40658295154571533\n",
      "cnt: 0 - valLoss: 0.4282328486442566 - trainLoss: 0.40658149123191833\n",
      "cnt: 0 - valLoss: 0.428232342004776 - trainLoss: 0.4065800905227661\n",
      "cnt: 0 - valLoss: 0.42823195457458496 - trainLoss: 0.4065786302089691\n",
      "cnt: 0 - valLoss: 0.4282315671443939 - trainLoss: 0.4065771698951721\n",
      "cnt: 0 - valLoss: 0.42823123931884766 - trainLoss: 0.4065757095813751\n",
      "cnt: 0 - valLoss: 0.42823079228401184 - trainLoss: 0.4065743088722229\n",
      "cnt: 0 - valLoss: 0.42823028564453125 - trainLoss: 0.4065729081630707\n",
      "cnt: 0 - valLoss: 0.4282298982143402 - trainLoss: 0.4065714478492737\n",
      "cnt: 0 - valLoss: 0.42822951078414917 - trainLoss: 0.4065699875354767\n",
      "cnt: 0 - valLoss: 0.42822912335395813 - trainLoss: 0.40656858682632446\n",
      "cnt: 0 - valLoss: 0.4282287359237671 - trainLoss: 0.4065670669078827\n",
      "cnt: 0 - valLoss: 0.4282284080982208 - trainLoss: 0.40656566619873047\n",
      "cnt: 0 - valLoss: 0.42822781205177307 - trainLoss: 0.40656420588493347\n",
      "cnt: 0 - valLoss: 0.42822742462158203 - trainLoss: 0.40656280517578125\n",
      "cnt: 0 - valLoss: 0.428227037191391 - trainLoss: 0.40656134486198425\n",
      "cnt: 0 - valLoss: 0.42822667956352234 - trainLoss: 0.40655988454818726\n",
      "cnt: 0 - valLoss: 0.4282262921333313 - trainLoss: 0.40655848383903503\n",
      "cnt: 0 - valLoss: 0.42822590470314026 - trainLoss: 0.40655702352523804\n",
      "cnt: 0 - valLoss: 0.4282253682613373 - trainLoss: 0.4065555930137634\n",
      "cnt: 0 - valLoss: 0.42822498083114624 - trainLoss: 0.4065541625022888\n",
      "cnt: 0 - valLoss: 0.4282245934009552 - trainLoss: 0.4065527021884918\n",
      "cnt: 0 - valLoss: 0.42822420597076416 - trainLoss: 0.4065512716770172\n",
      "cnt: 0 - valLoss: 0.4282238185405731 - trainLoss: 0.4065498113632202\n",
      "cnt: 0 - valLoss: 0.4282234311103821 - trainLoss: 0.4065483808517456\n",
      "cnt: 0 - valLoss: 0.4282230734825134 - trainLoss: 0.406546950340271\n",
      "cnt: 0 - valLoss: 0.4282224774360657 - trainLoss: 0.406545490026474\n",
      "cnt: 0 - valLoss: 0.42822209000587463 - trainLoss: 0.4065440893173218\n",
      "cnt: 0 - valLoss: 0.4282217025756836 - trainLoss: 0.40654268860816956\n",
      "cnt: 0 - valLoss: 0.42822137475013733 - trainLoss: 0.4065411686897278\n",
      "cnt: 0 - valLoss: 0.4282209277153015 - trainLoss: 0.40653976798057556\n",
      "cnt: 0 - valLoss: 0.42822057008743286 - trainLoss: 0.40653830766677856\n",
      "cnt: 0 - valLoss: 0.4282200038433075 - trainLoss: 0.40653684735298157\n",
      "cnt: 0 - valLoss: 0.42821961641311646 - trainLoss: 0.40653538703918457\n",
      "cnt: 0 - valLoss: 0.4282192289829254 - trainLoss: 0.40653398633003235\n",
      "cnt: 0 - valLoss: 0.42821887135505676 - trainLoss: 0.4065325856208801\n",
      "cnt: 0 - valLoss: 0.4282184839248657 - trainLoss: 0.40653112530708313\n",
      "cnt: 0 - valLoss: 0.42821812629699707 - trainLoss: 0.40652966499328613\n",
      "cnt: 0 - valLoss: 0.42821773886680603 - trainLoss: 0.40652820467948914\n",
      "cnt: 0 - valLoss: 0.4282171130180359 - trainLoss: 0.4065268039703369\n",
      "cnt: 0 - valLoss: 0.42821675539016724 - trainLoss: 0.4065253734588623\n",
      "cnt: 0 - valLoss: 0.4282163679599762 - trainLoss: 0.4065239429473877\n",
      "cnt: 0 - valLoss: 0.42821598052978516 - trainLoss: 0.4065224826335907\n",
      "cnt: 0 - valLoss: 0.4282156229019165 - trainLoss: 0.4065210521221161\n",
      "cnt: 0 - valLoss: 0.4282152056694031 - trainLoss: 0.40651965141296387\n",
      "cnt: 0 - valLoss: 0.4282148480415344 - trainLoss: 0.40651819109916687\n",
      "cnt: 0 - valLoss: 0.428214430809021 - trainLoss: 0.4065167307853699\n",
      "cnt: 0 - valLoss: 0.42821383476257324 - trainLoss: 0.40651533007621765\n",
      "cnt: 0 - valLoss: 0.4282134771347046 - trainLoss: 0.40651386976242065\n",
      "cnt: 0 - valLoss: 0.42821308970451355 - trainLoss: 0.40651246905326843\n",
      "cnt: 0 - valLoss: 0.4282127022743225 - trainLoss: 0.40651100873947144\n",
      "cnt: 0 - valLoss: 0.42821231484413147 - trainLoss: 0.4065095782279968\n",
      "cnt: 0 - valLoss: 0.42821192741394043 - trainLoss: 0.4065081477165222\n",
      "cnt: 0 - valLoss: 0.4282115697860718 - trainLoss: 0.4065066874027252\n",
      "cnt: 0 - valLoss: 0.4282110035419464 - trainLoss: 0.4065052568912506\n",
      "cnt: 0 - valLoss: 0.4282105565071106 - trainLoss: 0.4065038561820984\n",
      "cnt: 0 - valLoss: 0.42821022868156433 - trainLoss: 0.406502366065979\n",
      "cnt: 0 - valLoss: 0.4282098114490509 - trainLoss: 0.4065009355545044\n",
      "cnt: 0 - valLoss: 0.42820945382118225 - trainLoss: 0.4064995348453522\n",
      "cnt: 0 - valLoss: 0.4282090365886688 - trainLoss: 0.4064980745315552\n",
      "cnt: 0 - valLoss: 0.42820867896080017 - trainLoss: 0.4064966142177582\n",
      "cnt: 0 - valLoss: 0.42820829153060913 - trainLoss: 0.40649521350860596\n",
      "cnt: 0 - valLoss: 0.42820775508880615 - trainLoss: 0.40649375319480896\n",
      "cnt: 0 - valLoss: 0.42820730805397034 - trainLoss: 0.40649235248565674\n",
      "cnt: 0 - valLoss: 0.4282069802284241 - trainLoss: 0.40649089217185974\n",
      "cnt: 0 - valLoss: 0.42820653319358826 - trainLoss: 0.40648943185806274\n",
      "cnt: 0 - valLoss: 0.428206205368042 - trainLoss: 0.4064880311489105\n",
      "cnt: 0 - valLoss: 0.4282057583332062 - trainLoss: 0.4064866006374359\n",
      "cnt: 0 - valLoss: 0.4282054305076599 - trainLoss: 0.4064851403236389\n",
      "cnt: 0 - valLoss: 0.42820483446121216 - trainLoss: 0.4064837396144867\n",
      "cnt: 0 - valLoss: 0.4282044470310211 - trainLoss: 0.4064822793006897\n",
      "cnt: 0 - valLoss: 0.4282040596008301 - trainLoss: 0.4064808189868927\n",
      "cnt: 0 - valLoss: 0.4282037019729614 - trainLoss: 0.4064794182777405\n",
      "cnt: 0 - valLoss: 0.428203284740448 - trainLoss: 0.4064779579639435\n",
      "cnt: 0 - valLoss: 0.42820289731025696 - trainLoss: 0.40647655725479126\n",
      "cnt: 0 - valLoss: 0.4282025098800659 - trainLoss: 0.40647509694099426\n",
      "cnt: 0 - valLoss: 0.4282021224498749 - trainLoss: 0.40647369623184204\n",
      "cnt: 0 - valLoss: 0.42820173501968384 - trainLoss: 0.40647223591804504\n",
      "cnt: 0 - valLoss: 0.42820116877555847 - trainLoss: 0.40647080540657043\n",
      "cnt: 0 - valLoss: 0.42820075154304504 - trainLoss: 0.40646934509277344\n",
      "cnt: 0 - valLoss: 0.428200364112854 - trainLoss: 0.40646791458129883\n",
      "cnt: 0 - valLoss: 0.42820003628730774 - trainLoss: 0.4064664840698242\n",
      "cnt: 0 - valLoss: 0.4281996488571167 - trainLoss: 0.4064650237560272\n",
      "cnt: 0 - valLoss: 0.42819926142692566 - trainLoss: 0.406463623046875\n",
      "cnt: 0 - valLoss: 0.42819881439208984 - trainLoss: 0.406462162733078\n",
      "cnt: 0 - valLoss: 0.4281984865665436 - trainLoss: 0.4064607620239258\n",
      "cnt: 0 - valLoss: 0.4281978905200958 - trainLoss: 0.4064593017101288\n",
      "cnt: 0 - valLoss: 0.4281975030899048 - trainLoss: 0.40645790100097656\n",
      "cnt: 0 - valLoss: 0.42819711565971375 - trainLoss: 0.40645644068717957\n",
      "cnt: 0 - valLoss: 0.42819666862487793 - trainLoss: 0.40645498037338257\n",
      "cnt: 0 - valLoss: 0.42819634079933167 - trainLoss: 0.40645354986190796\n",
      "cnt: 0 - valLoss: 0.42819592356681824 - trainLoss: 0.40645211935043335\n",
      "cnt: 0 - valLoss: 0.4281955659389496 - trainLoss: 0.40645068883895874\n",
      "cnt: 0 - valLoss: 0.42819514870643616 - trainLoss: 0.4064492881298065\n",
      "cnt: 0 - valLoss: 0.4281947910785675 - trainLoss: 0.4064478278160095\n",
      "cnt: 0 - valLoss: 0.42819419503211975 - trainLoss: 0.4064463675022125\n",
      "cnt: 0 - valLoss: 0.4281938076019287 - trainLoss: 0.4064449071884155\n",
      "cnt: 0 - valLoss: 0.42819342017173767 - trainLoss: 0.4064435362815857\n",
      "cnt: 0 - valLoss: 0.42819300293922424 - trainLoss: 0.4064421057701111\n",
      "cnt: 0 - valLoss: 0.4281926453113556 - trainLoss: 0.4064406752586365\n",
      "cnt: 0 - valLoss: 0.42819222807884216 - trainLoss: 0.4064392149448395\n",
      "cnt: 0 - valLoss: 0.42819178104400635 - trainLoss: 0.40643781423568726\n",
      "cnt: 0 - valLoss: 0.4281914234161377 - trainLoss: 0.40643635392189026\n",
      "cnt: 0 - valLoss: 0.42819103598594666 - trainLoss: 0.40643489360809326\n",
      "cnt: 0 - valLoss: 0.428190678358078 - trainLoss: 0.40643349289894104\n",
      "cnt: 0 - valLoss: 0.42819005250930786 - trainLoss: 0.40643203258514404\n",
      "cnt: 0 - valLoss: 0.4281896650791168 - trainLoss: 0.4064306318759918\n",
      "cnt: 0 - valLoss: 0.4281892776489258 - trainLoss: 0.4064291715621948\n",
      "cnt: 0 - valLoss: 0.42818889021873474 - trainLoss: 0.4064277112483978\n",
      "cnt: 0 - valLoss: 0.4281885027885437 - trainLoss: 0.4064263105392456\n",
      "cnt: 0 - valLoss: 0.42818811535835266 - trainLoss: 0.4064248502254486\n",
      "cnt: 0 - valLoss: 0.42818766832351685 - trainLoss: 0.406423419713974\n",
      "cnt: 0 - valLoss: 0.4281872808933258 - trainLoss: 0.4064219892024994\n",
      "cnt: 0 - valLoss: 0.42818692326545715 - trainLoss: 0.4064205586910248\n",
      "cnt: 0 - valLoss: 0.42818647623062134 - trainLoss: 0.40641915798187256\n",
      "cnt: 0 - valLoss: 0.42818590998649597 - trainLoss: 0.40641769766807556\n",
      "cnt: 0 - valLoss: 0.4281855523586273 - trainLoss: 0.40641623735427856\n",
      "cnt: 0 - valLoss: 0.4281851053237915 - trainLoss: 0.40641486644744873\n",
      "cnt: 0 - valLoss: 0.42818471789360046 - trainLoss: 0.40641340613365173\n",
      "cnt: 0 - valLoss: 0.4281843304634094 - trainLoss: 0.4064119756221771\n",
      "cnt: 0 - valLoss: 0.4281839430332184 - trainLoss: 0.4064105153083801\n",
      "cnt: 0 - valLoss: 0.42818355560302734 - trainLoss: 0.4064090847969055\n",
      "cnt: 0 - valLoss: 0.4281831681728363 - trainLoss: 0.4064076244831085\n",
      "cnt: 0 - valLoss: 0.4281827509403229 - trainLoss: 0.4064062237739563\n",
      "cnt: 0 - valLoss: 0.42818236351013184 - trainLoss: 0.4064047634601593\n",
      "cnt: 0 - valLoss: 0.4281817376613617 - trainLoss: 0.4064033627510071\n",
      "cnt: 0 - valLoss: 0.42818138003349304 - trainLoss: 0.40640193223953247\n",
      "cnt: 0 - valLoss: 0.428180992603302 - trainLoss: 0.40640050172805786\n",
      "cnt: 0 - valLoss: 0.4281805753707886 - trainLoss: 0.40639904141426086\n",
      "cnt: 0 - valLoss: 0.42818018794059753 - trainLoss: 0.40639761090278625\n",
      "cnt: 0 - valLoss: 0.4281798005104065 - trainLoss: 0.40639621019363403\n",
      "cnt: 0 - valLoss: 0.4281793534755707 - trainLoss: 0.40639474987983704\n",
      "cnt: 0 - valLoss: 0.4281790256500244 - trainLoss: 0.40639328956604004\n",
      "cnt: 0 - valLoss: 0.428178608417511 - trainLoss: 0.40639185905456543\n",
      "cnt: 0 - valLoss: 0.42817822098731995 - trainLoss: 0.4063904881477356\n",
      "cnt: 0 - valLoss: 0.4281778335571289 - trainLoss: 0.4063890278339386\n",
      "cnt: 0 - valLoss: 0.4281771779060364 - trainLoss: 0.4063875675201416\n",
      "cnt: 0 - valLoss: 0.42817679047584534 - trainLoss: 0.4063861072063446\n",
      "cnt: 0 - valLoss: 0.4281764030456543 - trainLoss: 0.4063847064971924\n",
      "cnt: 0 - valLoss: 0.42817601561546326 - trainLoss: 0.4063832759857178\n",
      "cnt: 0 - valLoss: 0.42817559838294983 - trainLoss: 0.4063818156719208\n",
      "cnt: 0 - valLoss: 0.4281752109527588 - trainLoss: 0.40638041496276855\n",
      "cnt: 0 - valLoss: 0.42817482352256775 - trainLoss: 0.40637895464897156\n",
      "cnt: 0 - valLoss: 0.4281744062900543 - trainLoss: 0.40637755393981934\n",
      "cnt: 0 - valLoss: 0.42817404866218567 - trainLoss: 0.40637609362602234\n",
      "cnt: 0 - valLoss: 0.42817363142967224 - trainLoss: 0.4063746929168701\n",
      "cnt: 0 - valLoss: 0.4281731843948364 - trainLoss: 0.4063732326030731\n",
      "cnt: 0 - valLoss: 0.42817258834838867 - trainLoss: 0.4063718020915985\n",
      "cnt: 0 - valLoss: 0.42817217111587524 - trainLoss: 0.4063703715801239\n",
      "cnt: 0 - valLoss: 0.4281718134880066 - trainLoss: 0.40636900067329407\n",
      "cnt: 0 - valLoss: 0.42817142605781555 - trainLoss: 0.40636754035949707\n",
      "cnt: 0 - valLoss: 0.42817097902297974 - trainLoss: 0.4063660800457001\n",
      "cnt: 0 - valLoss: 0.4281706213951111 - trainLoss: 0.40636467933654785\n",
      "cnt: 0 - valLoss: 0.42817020416259766 - trainLoss: 0.40636321902275085\n",
      "cnt: 0 - valLoss: 0.4281698167324066 - trainLoss: 0.40636178851127625\n",
      "cnt: 0 - valLoss: 0.4281694293022156 - trainLoss: 0.40636032819747925\n",
      "cnt: 0 - valLoss: 0.42816898226737976 - trainLoss: 0.406358927488327\n",
      "cnt: 0 - valLoss: 0.4281685948371887 - trainLoss: 0.40635746717453003\n",
      "cnt: 0 - valLoss: 0.42816799879074097 - trainLoss: 0.4063560664653778\n",
      "cnt: 0 - valLoss: 0.42816758155822754 - trainLoss: 0.4063546061515808\n",
      "cnt: 0 - valLoss: 0.4281671941280365 - trainLoss: 0.4063532054424286\n",
      "cnt: 0 - valLoss: 0.42816677689552307 - trainLoss: 0.406351774930954\n",
      "cnt: 0 - valLoss: 0.42816632986068726 - trainLoss: 0.406350314617157\n",
      "cnt: 0 - valLoss: 0.428166002035141 - trainLoss: 0.40634891390800476\n",
      "cnt: 0 - valLoss: 0.4281655550003052 - trainLoss: 0.40634751319885254\n",
      "cnt: 0 - valLoss: 0.42816513776779175 - trainLoss: 0.40634605288505554\n",
      "cnt: 0 - valLoss: 0.4281647503376007 - trainLoss: 0.40634459257125854\n",
      "cnt: 0 - valLoss: 0.42816436290740967 - trainLoss: 0.4063431918621063\n",
      "cnt: 0 - valLoss: 0.42816394567489624 - trainLoss: 0.4063417315483093\n",
      "cnt: 0 - valLoss: 0.4281634986400604 - trainLoss: 0.4063403010368347\n",
      "cnt: 0 - valLoss: 0.42816296219825745 - trainLoss: 0.4063388705253601\n",
      "cnt: 0 - valLoss: 0.42816251516342163 - trainLoss: 0.4063374400138855\n",
      "cnt: 0 - valLoss: 0.4281621277332306 - trainLoss: 0.4063359797000885\n",
      "cnt: 0 - valLoss: 0.42816171050071716 - trainLoss: 0.4063345789909363\n",
      "cnt: 0 - valLoss: 0.4281613230705261 - trainLoss: 0.4063331186771393\n",
      "cnt: 0 - valLoss: 0.4281609356403351 - trainLoss: 0.40633171796798706\n",
      "cnt: 0 - valLoss: 0.42816051840782166 - trainLoss: 0.40633028745651245\n",
      "cnt: 0 - valLoss: 0.42816007137298584 - trainLoss: 0.40632882714271545\n",
      "cnt: 0 - valLoss: 0.4281596839427948 - trainLoss: 0.40632739663124084\n",
      "cnt: 0 - valLoss: 0.42815929651260376 - trainLoss: 0.40632596611976624\n",
      "cnt: 0 - valLoss: 0.42815887928009033 - trainLoss: 0.406324565410614\n",
      "cnt: 0 - valLoss: 0.4281584918498993 - trainLoss: 0.406323105096817\n",
      "cnt: 0 - valLoss: 0.42815810441970825 - trainLoss: 0.4063217043876648\n",
      "cnt: 0 - valLoss: 0.4281576871871948 - trainLoss: 0.4063202738761902\n",
      "cnt: 0 - valLoss: 0.4281570315361023 - trainLoss: 0.4063188135623932\n",
      "cnt: 0 - valLoss: 0.42815664410591125 - trainLoss: 0.4063173830509186\n",
      "cnt: 0 - valLoss: 0.4281562268733978 - trainLoss: 0.40631601214408875\n",
      "cnt: 0 - valLoss: 0.4281558394432068 - trainLoss: 0.40631455183029175\n",
      "cnt: 0 - valLoss: 0.42815545201301575 - trainLoss: 0.40631312131881714\n",
      "cnt: 0 - valLoss: 0.4281550347805023 - trainLoss: 0.40631163120269775\n",
      "cnt: 0 - valLoss: 0.4281545877456665 - trainLoss: 0.4063102602958679\n",
      "cnt: 0 - valLoss: 0.4281541705131531 - trainLoss: 0.4063087999820709\n",
      "cnt: 0 - valLoss: 0.42815378308296204 - trainLoss: 0.4063073992729187\n",
      "cnt: 0 - valLoss: 0.428153395652771 - trainLoss: 0.4063059985637665\n",
      "cnt: 0 - valLoss: 0.4281529486179352 - trainLoss: 0.4063045382499695\n",
      "cnt: 0 - valLoss: 0.42815259099006653 - trainLoss: 0.4063031077384949\n",
      "cnt: 0 - valLoss: 0.4281521439552307 - trainLoss: 0.4063016474246979\n",
      "cnt: 0 - valLoss: 0.4281517565250397 - trainLoss: 0.40630024671554565\n",
      "cnt: 0 - valLoss: 0.42815133929252625 - trainLoss: 0.40629878640174866\n",
      "cnt: 0 - valLoss: 0.4281507432460785 - trainLoss: 0.40629732608795166\n",
      "cnt: 0 - valLoss: 0.4281502962112427 - trainLoss: 0.40629592537879944\n",
      "cnt: 0 - valLoss: 0.42814987897872925 - trainLoss: 0.4062945246696472\n",
      "cnt: 0 - valLoss: 0.4281494915485382 - trainLoss: 0.4062930643558502\n",
      "cnt: 0 - valLoss: 0.4281490743160248 - trainLoss: 0.4062916338443756\n",
      "cnt: 0 - valLoss: 0.42814868688583374 - trainLoss: 0.4062902331352234\n",
      "cnt: 0 - valLoss: 0.4281482398509979 - trainLoss: 0.4062887728214264\n",
      "cnt: 0 - valLoss: 0.4281478524208069 - trainLoss: 0.40628737211227417\n",
      "cnt: 0 - valLoss: 0.42814740538597107 - trainLoss: 0.40628594160079956\n",
      "cnt: 0 - valLoss: 0.42814701795578003 - trainLoss: 0.40628451108932495\n",
      "cnt: 0 - valLoss: 0.4281465709209442 - trainLoss: 0.40628308057785034\n",
      "cnt: 0 - valLoss: 0.4281461834907532 - trainLoss: 0.40628162026405334\n",
      "cnt: 0 - valLoss: 0.42814579606056213 - trainLoss: 0.4062802195549011\n",
      "cnt: 0 - valLoss: 0.4281453788280487 - trainLoss: 0.4062788188457489\n",
      "cnt: 0 - valLoss: 0.42814499139785767 - trainLoss: 0.4062773585319519\n",
      "cnt: 0 - valLoss: 0.42814457416534424 - trainLoss: 0.4062759280204773\n",
      "cnt: 0 - valLoss: 0.4281441271305084 - trainLoss: 0.4062744677066803\n",
      "cnt: 0 - valLoss: 0.428143709897995 - trainLoss: 0.4062730669975281\n",
      "cnt: 0 - valLoss: 0.42814311385154724 - trainLoss: 0.40627166628837585\n",
      "cnt: 0 - valLoss: 0.4281426668167114 - trainLoss: 0.40627020597457886\n",
      "cnt: 0 - valLoss: 0.4281422793865204 - trainLoss: 0.40626877546310425\n",
      "cnt: 0 - valLoss: 0.42814186215400696 - trainLoss: 0.406267374753952\n",
      "cnt: 0 - valLoss: 0.42814144492149353 - trainLoss: 0.40626591444015503\n",
      "cnt: 0 - valLoss: 0.4281409978866577 - trainLoss: 0.4062645137310028\n",
      "cnt: 0 - valLoss: 0.4281406104564667 - trainLoss: 0.4062630534172058\n",
      "cnt: 0 - valLoss: 0.42814019322395325 - trainLoss: 0.4062616527080536\n",
      "cnt: 0 - valLoss: 0.4281397759914398 - trainLoss: 0.4062601923942566\n",
      "cnt: 0 - valLoss: 0.4281393885612488 - trainLoss: 0.406258761882782\n",
      "cnt: 0 - valLoss: 0.42813894152641296 - trainLoss: 0.40625736117362976\n",
      "cnt: 0 - valLoss: 0.42813852429389954 - trainLoss: 0.40625590085983276\n",
      "cnt: 0 - valLoss: 0.4281381368637085 - trainLoss: 0.40625450015068054\n",
      "cnt: 0 - valLoss: 0.4281376898288727 - trainLoss: 0.40625306963920593\n",
      "cnt: 0 - valLoss: 0.42813733220100403 - trainLoss: 0.4062516391277313\n",
      "cnt: 0 - valLoss: 0.4281368851661682 - trainLoss: 0.4062502086162567\n",
      "cnt: 0 - valLoss: 0.4281364977359772 - trainLoss: 0.4062488079071045\n",
      "cnt: 0 - valLoss: 0.42813608050346375 - trainLoss: 0.4062473475933075\n",
      "cnt: 0 - valLoss: 0.42813563346862793 - trainLoss: 0.4062459468841553\n",
      "cnt: 0 - valLoss: 0.4281352162361145 - trainLoss: 0.40624451637268066\n",
      "cnt: 0 - valLoss: 0.42813482880592346 - trainLoss: 0.40624305605888367\n",
      "cnt: 0 - valLoss: 0.42813441157341003 - trainLoss: 0.40624162554740906\n",
      "cnt: 0 - valLoss: 0.4281337559223175 - trainLoss: 0.4062402546405792\n",
      "cnt: 0 - valLoss: 0.42813336849212646 - trainLoss: 0.4062387943267822\n",
      "cnt: 0 - valLoss: 0.42813289165496826 - trainLoss: 0.40623733401298523\n",
      "cnt: 0 - valLoss: 0.4281325042247772 - trainLoss: 0.4062359631061554\n",
      "cnt: 0 - valLoss: 0.42813214659690857 - trainLoss: 0.4062345027923584\n",
      "cnt: 0 - valLoss: 0.42813169956207275 - trainLoss: 0.4062330424785614\n",
      "cnt: 0 - valLoss: 0.4281312823295593 - trainLoss: 0.40623167157173157\n",
      "cnt: 0 - valLoss: 0.4281308948993683 - trainLoss: 0.40623021125793457\n",
      "cnt: 0 - valLoss: 0.42813044786453247 - trainLoss: 0.40622878074645996\n",
      "cnt: 0 - valLoss: 0.42813003063201904 - trainLoss: 0.40622735023498535\n",
      "cnt: 0 - valLoss: 0.428129643201828 - trainLoss: 0.40622594952583313\n",
      "cnt: 0 - valLoss: 0.4281292259693146 - trainLoss: 0.4062245190143585\n",
      "cnt: 0 - valLoss: 0.42812877893447876 - trainLoss: 0.4062230885028839\n",
      "cnt: 0 - valLoss: 0.42812836170196533 - trainLoss: 0.4062216579914093\n",
      "cnt: 0 - valLoss: 0.4281279444694519 - trainLoss: 0.4062201976776123\n",
      "cnt: 0 - valLoss: 0.42812755703926086 - trainLoss: 0.40621882677078247\n",
      "cnt: 0 - valLoss: 0.42812711000442505 - trainLoss: 0.40621739625930786\n",
      "cnt: 0 - valLoss: 0.428126722574234 - trainLoss: 0.40621596574783325\n",
      "cnt: 0 - valLoss: 0.4281263053417206 - trainLoss: 0.40621450543403625\n",
      "cnt: 0 - valLoss: 0.42812588810920715 - trainLoss: 0.40621310472488403\n",
      "cnt: 0 - valLoss: 0.4281255006790161 - trainLoss: 0.4062117040157318\n",
      "cnt: 0 - valLoss: 0.4281250536441803 - trainLoss: 0.4062102735042572\n",
      "cnt: 0 - valLoss: 0.42812463641166687 - trainLoss: 0.4062088131904602\n",
      "cnt: 0 - valLoss: 0.42812424898147583 - trainLoss: 0.4062073826789856\n",
      "cnt: 0 - valLoss: 0.4281238317489624 - trainLoss: 0.406205952167511\n",
      "cnt: 0 - valLoss: 0.4281233847141266 - trainLoss: 0.40620455145835876\n",
      "cnt: 0 - valLoss: 0.42812296748161316 - trainLoss: 0.40620312094688416\n",
      "cnt: 0 - valLoss: 0.4281225800514221 - trainLoss: 0.40620172023773193\n",
      "cnt: 0 - valLoss: 0.4281221032142639 - trainLoss: 0.40620025992393494\n",
      "cnt: 0 - valLoss: 0.4281217157840729 - trainLoss: 0.4061988592147827\n",
      "cnt: 0 - valLoss: 0.42812106013298035 - trainLoss: 0.4061974287033081\n",
      "cnt: 0 - valLoss: 0.4281206429004669 - trainLoss: 0.4061959981918335\n",
      "cnt: 0 - valLoss: 0.4281202554702759 - trainLoss: 0.4061945676803589\n",
      "cnt: 0 - valLoss: 0.42811983823776245 - trainLoss: 0.40619316697120667\n",
      "cnt: 0 - valLoss: 0.4281194508075714 - trainLoss: 0.40619170665740967\n",
      "cnt: 0 - valLoss: 0.4281189739704132 - trainLoss: 0.40619033575057983\n",
      "cnt: 0 - valLoss: 0.42811858654022217 - trainLoss: 0.40618887543678284\n",
      "cnt: 0 - valLoss: 0.42811816930770874 - trainLoss: 0.40618741512298584\n",
      "cnt: 0 - valLoss: 0.4281177222728729 - trainLoss: 0.406186044216156\n",
      "cnt: 0 - valLoss: 0.4281173050403595 - trainLoss: 0.4061846137046814\n",
      "cnt: 0 - valLoss: 0.42811691761016846 - trainLoss: 0.4061831533908844\n",
      "cnt: 0 - valLoss: 0.42811644077301025 - trainLoss: 0.40618178248405457\n",
      "cnt: 0 - valLoss: 0.4281160533428192 - trainLoss: 0.40618032217025757\n",
      "cnt: 0 - valLoss: 0.4281156361103058 - trainLoss: 0.40617889165878296\n",
      "cnt: 0 - valLoss: 0.42811524868011475 - trainLoss: 0.40617749094963074\n",
      "cnt: 0 - valLoss: 0.42811480164527893 - trainLoss: 0.4061760902404785\n",
      "cnt: 0 - valLoss: 0.4281143844127655 - trainLoss: 0.4061746299266815\n",
      "cnt: 0 - valLoss: 0.4281139671802521 - trainLoss: 0.4061731994152069\n",
      "cnt: 0 - valLoss: 0.42811352014541626 - trainLoss: 0.4061717689037323\n",
      "cnt: 0 - valLoss: 0.4281131327152252 - trainLoss: 0.40617039799690247\n",
      "cnt: 0 - valLoss: 0.4281127452850342 - trainLoss: 0.40616893768310547\n",
      "cnt: 0 - valLoss: 0.42811229825019836 - trainLoss: 0.40616750717163086\n",
      "cnt: 0 - valLoss: 0.4281119108200073 - trainLoss: 0.40616607666015625\n",
      "cnt: 0 - valLoss: 0.4281114637851715 - trainLoss: 0.40616464614868164\n",
      "cnt: 0 - valLoss: 0.4281110465526581 - trainLoss: 0.4061632454395294\n",
      "cnt: 0 - valLoss: 0.42811062932014465 - trainLoss: 0.4061618149280548\n",
      "cnt: 0 - valLoss: 0.42811018228530884 - trainLoss: 0.4061603844165802\n",
      "cnt: 0 - valLoss: 0.4281097650527954 - trainLoss: 0.4061589539051056\n",
      "cnt: 0 - valLoss: 0.42810937762260437 - trainLoss: 0.40615755319595337\n",
      "cnt: 0 - valLoss: 0.42810899019241333 - trainLoss: 0.40615612268447876\n",
      "cnt: 0 - valLoss: 0.4281085133552551 - trainLoss: 0.40615472197532654\n",
      "cnt: 0 - valLoss: 0.4281080961227417 - trainLoss: 0.40615326166152954\n",
      "cnt: 0 - valLoss: 0.42810770869255066 - trainLoss: 0.4061518609523773\n",
      "cnt: 0 - valLoss: 0.42810723185539246 - trainLoss: 0.4061504304409027\n",
      "cnt: 0 - valLoss: 0.4281068444252014 - trainLoss: 0.4061489999294281\n",
      "cnt: 0 - valLoss: 0.428106427192688 - trainLoss: 0.40614762902259827\n",
      "cnt: 0 - valLoss: 0.4281059801578522 - trainLoss: 0.40614616870880127\n",
      "cnt: 0 - valLoss: 0.42810556292533875 - trainLoss: 0.40614473819732666\n",
      "cnt: 0 - valLoss: 0.42810511589050293 - trainLoss: 0.40614333748817444\n",
      "cnt: 0 - valLoss: 0.4281046986579895 - trainLoss: 0.40614187717437744\n",
      "cnt: 0 - valLoss: 0.42810431122779846 - trainLoss: 0.4061404764652252\n",
      "cnt: 0 - valLoss: 0.42810383439064026 - trainLoss: 0.4061390459537506\n",
      "cnt: 0 - valLoss: 0.4281034469604492 - trainLoss: 0.406137615442276\n",
      "cnt: 0 - valLoss: 0.4281029999256134 - trainLoss: 0.4061361849308014\n",
      "cnt: 0 - valLoss: 0.4281025528907776 - trainLoss: 0.40613478422164917\n",
      "cnt: 0 - valLoss: 0.42810213565826416 - trainLoss: 0.40613335371017456\n",
      "cnt: 0 - valLoss: 0.4281017482280731 - trainLoss: 0.40613195300102234\n",
      "cnt: 0 - valLoss: 0.4281013011932373 - trainLoss: 0.40613049268722534\n",
      "cnt: 0 - valLoss: 0.42810094356536865 - trainLoss: 0.4061291217803955\n",
      "cnt: 0 - valLoss: 0.42810046672821045 - trainLoss: 0.4061276614665985\n",
      "cnt: 0 - valLoss: 0.4281000792980194 - trainLoss: 0.4061262607574463\n",
      "cnt: 0 - valLoss: 0.4280996322631836 - trainLoss: 0.4061248004436493\n",
      "cnt: 0 - valLoss: 0.42809924483299255 - trainLoss: 0.40612339973449707\n",
      "cnt: 0 - valLoss: 0.42809879779815674 - trainLoss: 0.40612196922302246\n",
      "cnt: 0 - valLoss: 0.4280983507633209 - trainLoss: 0.40612056851387024\n",
      "cnt: 0 - valLoss: 0.42809802293777466 - trainLoss: 0.40611910820007324\n",
      "cnt: 0 - valLoss: 0.42809757590293884 - trainLoss: 0.4061177372932434\n",
      "cnt: 0 - valLoss: 0.4280971586704254 - trainLoss: 0.4061162769794464\n",
      "cnt: 0 - valLoss: 0.428096741437912 - trainLoss: 0.4061148762702942\n",
      "cnt: 0 - valLoss: 0.42809629440307617 - trainLoss: 0.40611347556114197\n",
      "cnt: 0 - valLoss: 0.42809590697288513 - trainLoss: 0.40611201524734497\n",
      "cnt: 0 - valLoss: 0.4280954897403717 - trainLoss: 0.40611064434051514\n",
      "cnt: 0 - valLoss: 0.4280950427055359 - trainLoss: 0.40610918402671814\n",
      "cnt: 0 - valLoss: 0.42809462547302246 - trainLoss: 0.4061077833175659\n",
      "cnt: 0 - valLoss: 0.4280942380428314 - trainLoss: 0.4061063528060913\n",
      "cnt: 0 - valLoss: 0.428093820810318 - trainLoss: 0.4061048924922943\n",
      "cnt: 0 - valLoss: 0.4280933737754822 - trainLoss: 0.4061035215854645\n",
      "cnt: 0 - valLoss: 0.42809295654296875 - trainLoss: 0.40610209107398987\n",
      "cnt: 0 - valLoss: 0.4280925393104553 - trainLoss: 0.40610066056251526\n",
      "cnt: 0 - valLoss: 0.4280920922756195 - trainLoss: 0.40609925985336304\n",
      "cnt: 0 - valLoss: 0.42809170484542847 - trainLoss: 0.4060978293418884\n",
      "cnt: 0 - valLoss: 0.42809128761291504 - trainLoss: 0.4060963988304138\n",
      "cnt: 0 - valLoss: 0.428090900182724 - trainLoss: 0.4060949683189392\n",
      "cnt: 0 - valLoss: 0.42809048295021057 - trainLoss: 0.4060935080051422\n",
      "cnt: 0 - valLoss: 0.42809003591537476 - trainLoss: 0.4060921370983124\n",
      "cnt: 0 - valLoss: 0.42808961868286133 - trainLoss: 0.40609070658683777\n",
      "cnt: 0 - valLoss: 0.4280891716480255 - trainLoss: 0.40608933568000793\n",
      "cnt: 0 - valLoss: 0.4280887544155121 - trainLoss: 0.40608787536621094\n",
      "cnt: 0 - valLoss: 0.42808833718299866 - trainLoss: 0.40608644485473633\n",
      "cnt: 0 - valLoss: 0.42808789014816284 - trainLoss: 0.4060850441455841\n",
      "cnt: 0 - valLoss: 0.4280875027179718 - trainLoss: 0.4060835838317871\n",
      "cnt: 0 - valLoss: 0.4280870854854584 - trainLoss: 0.4060821831226349\n",
      "cnt: 0 - valLoss: 0.42808660864830017 - trainLoss: 0.4060807526111603\n",
      "cnt: 0 - valLoss: 0.42808622121810913 - trainLoss: 0.40607935190200806\n",
      "cnt: 0 - valLoss: 0.4280858337879181 - trainLoss: 0.40607795119285583\n",
      "cnt: 0 - valLoss: 0.4280853569507599 - trainLoss: 0.4060765206813812\n",
      "cnt: 0 - valLoss: 0.42808493971824646 - trainLoss: 0.40607506036758423\n",
      "cnt: 0 - valLoss: 0.4280845522880554 - trainLoss: 0.406073659658432\n",
      "cnt: 0 - valLoss: 0.428084135055542 - trainLoss: 0.4060722589492798\n",
      "cnt: 0 - valLoss: 0.4280836880207062 - trainLoss: 0.4060708284378052\n",
      "cnt: 0 - valLoss: 0.42808327078819275 - trainLoss: 0.4060693681240082\n",
      "cnt: 0 - valLoss: 0.4280828535556793 - trainLoss: 0.40606799721717834\n",
      "cnt: 0 - valLoss: 0.4280824065208435 - trainLoss: 0.40606656670570374\n",
      "cnt: 0 - valLoss: 0.4280819892883301 - trainLoss: 0.4060651957988739\n",
      "cnt: 0 - valLoss: 0.42808154225349426 - trainLoss: 0.4060637354850769\n",
      "cnt: 0 - valLoss: 0.42808112502098083 - trainLoss: 0.4060623049736023\n",
      "cnt: 0 - valLoss: 0.4280807077884674 - trainLoss: 0.4060609042644501\n",
      "cnt: 0 - valLoss: 0.42808032035827637 - trainLoss: 0.4060594439506531\n",
      "cnt: 0 - valLoss: 0.42807987332344055 - trainLoss: 0.40605804324150085\n",
      "cnt: 0 - valLoss: 0.4280794560909271 - trainLoss: 0.40605661273002625\n",
      "cnt: 0 - valLoss: 0.4280790388584137 - trainLoss: 0.406055212020874\n",
      "cnt: 0 - valLoss: 0.42807865142822266 - trainLoss: 0.4060538113117218\n",
      "cnt: 0 - valLoss: 0.42807820439338684 - trainLoss: 0.4060523808002472\n",
      "cnt: 0 - valLoss: 0.42807772755622864 - trainLoss: 0.40605098009109497\n",
      "cnt: 0 - valLoss: 0.4280773401260376 - trainLoss: 0.406049519777298\n",
      "cnt: 0 - valLoss: 0.4280768930912018 - trainLoss: 0.40604814887046814\n",
      "cnt: 0 - valLoss: 0.42807644605636597 - trainLoss: 0.40604668855667114\n",
      "cnt: 0 - valLoss: 0.4280760586261749 - trainLoss: 0.40604522824287415\n",
      "cnt: 0 - valLoss: 0.4280756413936615 - trainLoss: 0.4060438573360443\n",
      "cnt: 0 - valLoss: 0.42807522416114807 - trainLoss: 0.4060424268245697\n",
      "cnt: 0 - valLoss: 0.42807477712631226 - trainLoss: 0.40604105591773987\n",
      "cnt: 0 - valLoss: 0.42807435989379883 - trainLoss: 0.40603959560394287\n",
      "cnt: 0 - valLoss: 0.428073912858963 - trainLoss: 0.40603816509246826\n",
      "cnt: 0 - valLoss: 0.4280734956264496 - trainLoss: 0.40603676438331604\n",
      "cnt: 0 - valLoss: 0.42807307839393616 - trainLoss: 0.40603530406951904\n",
      "cnt: 0 - valLoss: 0.4280726909637451 - trainLoss: 0.4060339331626892\n",
      "cnt: 0 - valLoss: 0.4280722141265869 - trainLoss: 0.4060324728488922\n",
      "cnt: 0 - valLoss: 0.4280717968940735 - trainLoss: 0.40603107213974\n",
      "cnt: 0 - valLoss: 0.42807140946388245 - trainLoss: 0.40602967143058777\n",
      "cnt: 0 - valLoss: 0.42807096242904663 - trainLoss: 0.40602824091911316\n",
      "cnt: 0 - valLoss: 0.4280705451965332 - trainLoss: 0.40602678060531616\n",
      "cnt: 0 - valLoss: 0.4280700981616974 - trainLoss: 0.40602537989616394\n",
      "cnt: 0 - valLoss: 0.42806971073150635 - trainLoss: 0.4060240089893341\n",
      "cnt: 0 - valLoss: 0.42806926369667053 - trainLoss: 0.4060225486755371\n",
      "cnt: 0 - valLoss: 0.4280688762664795 - trainLoss: 0.4060211777687073\n",
      "cnt: 0 - valLoss: 0.4280684292316437 - trainLoss: 0.4060197174549103\n",
      "cnt: 0 - valLoss: 0.42806801199913025 - trainLoss: 0.40601831674575806\n",
      "cnt: 0 - valLoss: 0.4280675947666168 - trainLoss: 0.40601691603660583\n",
      "cnt: 0 - valLoss: 0.428067147731781 - trainLoss: 0.40601545572280884\n",
      "cnt: 0 - valLoss: 0.4280666708946228 - trainLoss: 0.406014084815979\n",
      "cnt: 0 - valLoss: 0.42806628346443176 - trainLoss: 0.406012624502182\n",
      "cnt: 0 - valLoss: 0.42806586623191833 - trainLoss: 0.4060112535953522\n",
      "cnt: 0 - valLoss: 0.4280654489994049 - trainLoss: 0.4060097932815552\n",
      "cnt: 0 - valLoss: 0.4280650019645691 - trainLoss: 0.40600839257240295\n",
      "cnt: 0 - valLoss: 0.42806458473205566 - trainLoss: 0.40600696206092834\n",
      "cnt: 0 - valLoss: 0.42806416749954224 - trainLoss: 0.40600553154945374\n",
      "cnt: 0 - valLoss: 0.4280637204647064 - trainLoss: 0.4060041606426239\n",
      "cnt: 0 - valLoss: 0.428063303232193 - trainLoss: 0.4060027003288269\n",
      "cnt: 0 - valLoss: 0.4280628561973572 - trainLoss: 0.4060012698173523\n",
      "cnt: 0 - valLoss: 0.42806246876716614 - trainLoss: 0.4059998691082001\n",
      "cnt: 0 - valLoss: 0.4280620217323303 - trainLoss: 0.40599846839904785\n",
      "cnt: 0 - valLoss: 0.4280615746974945 - trainLoss: 0.40599703788757324\n",
      "cnt: 0 - valLoss: 0.42806118726730347 - trainLoss: 0.40599557757377625\n",
      "cnt: 0 - valLoss: 0.42806071043014526 - trainLoss: 0.4059942066669464\n",
      "cnt: 0 - valLoss: 0.42806029319763184 - trainLoss: 0.4059928059577942\n",
      "cnt: 0 - valLoss: 0.4280599057674408 - trainLoss: 0.4059913456439972\n",
      "cnt: 0 - valLoss: 0.4280594289302826 - trainLoss: 0.40598994493484497\n",
      "cnt: 0 - valLoss: 0.42805904150009155 - trainLoss: 0.40598857402801514\n",
      "cnt: 0 - valLoss: 0.42805859446525574 - trainLoss: 0.40598711371421814\n",
      "cnt: 0 - valLoss: 0.4280581474304199 - trainLoss: 0.4059857130050659\n",
      "cnt: 0 - valLoss: 0.4280577301979065 - trainLoss: 0.4059842824935913\n",
      "cnt: 0 - valLoss: 0.42805734276771545 - trainLoss: 0.4059828817844391\n",
      "cnt: 0 - valLoss: 0.42805689573287964 - trainLoss: 0.4059814512729645\n",
      "cnt: 0 - valLoss: 0.4280564486980438 - trainLoss: 0.40598002076148987\n",
      "cnt: 0 - valLoss: 0.428056001663208 - trainLoss: 0.40597859025001526\n",
      "cnt: 0 - valLoss: 0.4280555844306946 - trainLoss: 0.40597718954086304\n",
      "cnt: 0 - valLoss: 0.42805516719818115 - trainLoss: 0.4059757590293884\n",
      "cnt: 0 - valLoss: 0.4280547797679901 - trainLoss: 0.4059743583202362\n",
      "cnt: 0 - valLoss: 0.4280543029308319 - trainLoss: 0.405972957611084\n",
      "cnt: 0 - valLoss: 0.4280538558959961 - trainLoss: 0.4059715270996094\n",
      "cnt: 0 - valLoss: 0.42805346846580505 - trainLoss: 0.40597012639045715\n",
      "cnt: 0 - valLoss: 0.42805302143096924 - trainLoss: 0.40596869587898254\n",
      "cnt: 0 - valLoss: 0.4280525743961334 - trainLoss: 0.40596726536750793\n",
      "cnt: 0 - valLoss: 0.42805215716362 - trainLoss: 0.4059658944606781\n",
      "cnt: 0 - valLoss: 0.4280517101287842 - trainLoss: 0.4059644341468811\n",
      "cnt: 0 - valLoss: 0.42805129289627075 - trainLoss: 0.4059630036354065\n",
      "cnt: 0 - valLoss: 0.4280508756637573 - trainLoss: 0.4059616029262543\n",
      "cnt: 0 - valLoss: 0.4280503988265991 - trainLoss: 0.40596020221710205\n",
      "cnt: 0 - valLoss: 0.4280499517917633 - trainLoss: 0.40595877170562744\n",
      "cnt: 0 - valLoss: 0.4280495345592499 - trainLoss: 0.4059573709964752\n",
      "cnt: 0 - valLoss: 0.42804911732673645 - trainLoss: 0.4059559404850006\n",
      "cnt: 0 - valLoss: 0.42804867029190063 - trainLoss: 0.405954509973526\n",
      "cnt: 0 - valLoss: 0.4280482232570648 - trainLoss: 0.40595313906669617\n",
      "cnt: 0 - valLoss: 0.428047776222229 - trainLoss: 0.40595167875289917\n",
      "cnt: 0 - valLoss: 0.4280473589897156 - trainLoss: 0.40595030784606934\n",
      "cnt: 0 - valLoss: 0.42804691195487976 - trainLoss: 0.40594884753227234\n",
      "cnt: 0 - valLoss: 0.42804649472236633 - trainLoss: 0.4059474468231201\n",
      "cnt: 0 - valLoss: 0.4280460774898529 - trainLoss: 0.4059460163116455\n",
      "cnt: 0 - valLoss: 0.4280456304550171 - trainLoss: 0.4059446156024933\n",
      "cnt: 0 - valLoss: 0.42804521322250366 - trainLoss: 0.4059431850910187\n",
      "cnt: 0 - valLoss: 0.42804476618766785 - trainLoss: 0.40594175457954407\n",
      "cnt: 0 - valLoss: 0.4280443489551544 - trainLoss: 0.40594038367271423\n",
      "cnt: 0 - valLoss: 0.428043931722641 - trainLoss: 0.4059389531612396\n",
      "cnt: 0 - valLoss: 0.4280434548854828 - trainLoss: 0.4059374928474426\n",
      "cnt: 0 - valLoss: 0.42804306745529175 - trainLoss: 0.4059361219406128\n",
      "cnt: 0 - valLoss: 0.42804259061813354 - trainLoss: 0.4059346914291382\n",
      "cnt: 0 - valLoss: 0.4280421733856201 - trainLoss: 0.4059332609176636\n",
      "cnt: 0 - valLoss: 0.4280417263507843 - trainLoss: 0.40593186020851135\n",
      "cnt: 0 - valLoss: 0.4280412793159485 - trainLoss: 0.40593042969703674\n",
      "cnt: 0 - valLoss: 0.42804083228111267 - trainLoss: 0.4059290289878845\n",
      "cnt: 0 - valLoss: 0.42804044485092163 - trainLoss: 0.4059276282787323\n",
      "cnt: 0 - valLoss: 0.4280399680137634 - trainLoss: 0.4059261977672577\n",
      "cnt: 0 - valLoss: 0.42803955078125 - trainLoss: 0.40592479705810547\n",
      "cnt: 0 - valLoss: 0.4280391335487366 - trainLoss: 0.40592336654663086\n",
      "cnt: 0 - valLoss: 0.42803868651390076 - trainLoss: 0.40592196583747864\n",
      "cnt: 0 - valLoss: 0.42803826928138733 - trainLoss: 0.4059205949306488\n",
      "cnt: 0 - valLoss: 0.4280377924442291 - trainLoss: 0.4059191346168518\n",
      "cnt: 0 - valLoss: 0.4280373752117157 - trainLoss: 0.4059176743030548\n",
      "cnt: 0 - valLoss: 0.4280369281768799 - trainLoss: 0.40591633319854736\n",
      "cnt: 0 - valLoss: 0.42803654074668884 - trainLoss: 0.40591490268707275\n",
      "cnt: 0 - valLoss: 0.428036093711853 - trainLoss: 0.40591350197792053\n",
      "cnt: 0 - valLoss: 0.4280356466770172 - trainLoss: 0.40591204166412354\n",
      "cnt: 0 - valLoss: 0.4280352294445038 - trainLoss: 0.4059106707572937\n",
      "cnt: 0 - valLoss: 0.4280347526073456 - trainLoss: 0.4059092104434967\n",
      "cnt: 0 - valLoss: 0.42803433537483215 - trainLoss: 0.40590783953666687\n",
      "cnt: 0 - valLoss: 0.42803388833999634 - trainLoss: 0.4059063792228699\n",
      "cnt: 0 - valLoss: 0.4280334711074829 - trainLoss: 0.40590500831604004\n",
      "cnt: 0 - valLoss: 0.4280330240726471 - trainLoss: 0.4059036076068878\n",
      "cnt: 0 - valLoss: 0.42803260684013367 - trainLoss: 0.4059021472930908\n",
      "cnt: 0 - valLoss: 0.42803218960762024 - trainLoss: 0.4059007465839386\n",
      "cnt: 0 - valLoss: 0.4280317425727844 - trainLoss: 0.405899316072464\n",
      "cnt: 0 - valLoss: 0.428031325340271 - trainLoss: 0.40589791536331177\n",
      "cnt: 0 - valLoss: 0.4280308485031128 - trainLoss: 0.40589651465415955\n",
      "cnt: 0 - valLoss: 0.42803046107292175 - trainLoss: 0.40589508414268494\n",
      "cnt: 0 - valLoss: 0.42802998423576355 - trainLoss: 0.4058936834335327\n",
      "cnt: 0 - valLoss: 0.42802950739860535 - trainLoss: 0.4058922529220581\n",
      "cnt: 0 - valLoss: 0.4280290901660919 - trainLoss: 0.4058908224105835\n",
      "cnt: 0 - valLoss: 0.4280287027359009 - trainLoss: 0.40588945150375366\n",
      "cnt: 0 - valLoss: 0.4280282258987427 - trainLoss: 0.40588802099227905\n",
      "cnt: 0 - valLoss: 0.42802780866622925 - trainLoss: 0.40588662028312683\n",
      "cnt: 0 - valLoss: 0.4280273914337158 - trainLoss: 0.4058851897716522\n",
      "cnt: 0 - valLoss: 0.42802694439888 - trainLoss: 0.4058837890625\n",
      "cnt: 0 - valLoss: 0.4280265271663666 - trainLoss: 0.4058823883533478\n",
      "cnt: 0 - valLoss: 0.42802608013153076 - trainLoss: 0.40588095784187317\n",
      "cnt: 0 - valLoss: 0.42802566289901733 - trainLoss: 0.40587955713272095\n",
      "cnt: 0 - valLoss: 0.42802518606185913 - trainLoss: 0.40587809681892395\n",
      "cnt: 0 - valLoss: 0.4280247986316681 - trainLoss: 0.4058767259120941\n",
      "cnt: 0 - valLoss: 0.42802438139915466 - trainLoss: 0.4058753252029419\n",
      "cnt: 0 - valLoss: 0.4280238747596741 - trainLoss: 0.4058738648891449\n",
      "cnt: 0 - valLoss: 0.42802348732948303 - trainLoss: 0.4058724343776703\n",
      "cnt: 0 - valLoss: 0.42802301049232483 - trainLoss: 0.40587103366851807\n",
      "cnt: 0 - valLoss: 0.428022563457489 - trainLoss: 0.40586966276168823\n",
      "cnt: 0 - valLoss: 0.42802220582962036 - trainLoss: 0.405868262052536\n",
      "cnt: 0 - valLoss: 0.42802172899246216 - trainLoss: 0.4058668315410614\n",
      "cnt: 0 - valLoss: 0.42802128195762634 - trainLoss: 0.4058653712272644\n",
      "cnt: 0 - valLoss: 0.4280208647251129 - trainLoss: 0.4058639705181122\n",
      "cnt: 0 - valLoss: 0.4280204474925995 - trainLoss: 0.40586256980895996\n",
      "cnt: 0 - valLoss: 0.42802000045776367 - trainLoss: 0.4058611989021301\n",
      "cnt: 0 - valLoss: 0.42801952362060547 - trainLoss: 0.40585973858833313\n",
      "cnt: 0 - valLoss: 0.42801910638809204 - trainLoss: 0.4058583676815033\n",
      "cnt: 0 - valLoss: 0.4280186891555786 - trainLoss: 0.4058569669723511\n",
      "cnt: 0 - valLoss: 0.4280182421207428 - trainLoss: 0.40585553646087646\n",
      "cnt: 0 - valLoss: 0.4280177652835846 - trainLoss: 0.40585407614707947\n",
      "cnt: 0 - valLoss: 0.42801734805107117 - trainLoss: 0.40585267543792725\n",
      "cnt: 0 - valLoss: 0.42801693081855774 - trainLoss: 0.40585124492645264\n",
      "cnt: 0 - valLoss: 0.4280164837837219 - trainLoss: 0.4058498442173004\n",
      "cnt: 0 - valLoss: 0.4280160069465637 - trainLoss: 0.4058484435081482\n",
      "cnt: 0 - valLoss: 0.4280155897140503 - trainLoss: 0.40584707260131836\n",
      "cnt: 0 - valLoss: 0.42801517248153687 - trainLoss: 0.40584564208984375\n",
      "cnt: 0 - valLoss: 0.42801469564437866 - trainLoss: 0.40584424138069153\n",
      "cnt: 0 - valLoss: 0.42801424860954285 - trainLoss: 0.40584278106689453\n",
      "cnt: 0 - valLoss: 0.4280138313770294 - trainLoss: 0.4058414101600647\n",
      "cnt: 0 - valLoss: 0.4280133545398712 - trainLoss: 0.4058399498462677\n",
      "cnt: 0 - valLoss: 0.4280129671096802 - trainLoss: 0.40583857893943787\n",
      "cnt: 0 - valLoss: 0.428012490272522 - trainLoss: 0.40583717823028564\n",
      "cnt: 0 - valLoss: 0.42801207304000854 - trainLoss: 0.4058357775211334\n",
      "cnt: 0 - valLoss: 0.4280116558074951 - trainLoss: 0.4058343470096588\n",
      "cnt: 0 - valLoss: 0.4280111789703369 - trainLoss: 0.4058329463005066\n",
      "cnt: 0 - valLoss: 0.4280107617378235 - trainLoss: 0.405831515789032\n",
      "cnt: 0 - valLoss: 0.42801031470298767 - trainLoss: 0.40583011507987976\n",
      "cnt: 0 - valLoss: 0.42800983786582947 - trainLoss: 0.40582871437072754\n",
      "cnt: 0 - valLoss: 0.42800942063331604 - trainLoss: 0.40582728385925293\n",
      "cnt: 0 - valLoss: 0.4280090034008026 - trainLoss: 0.40582582354545593\n",
      "cnt: 0 - valLoss: 0.4280085265636444 - trainLoss: 0.4058244228363037\n",
      "cnt: 0 - valLoss: 0.4280080795288086 - trainLoss: 0.4058230519294739\n",
      "cnt: 0 - valLoss: 0.42800766229629517 - trainLoss: 0.40582165122032166\n",
      "cnt: 0 - valLoss: 0.42800724506378174 - trainLoss: 0.40582022070884705\n",
      "cnt: 0 - valLoss: 0.4280067980289459 - trainLoss: 0.4058188199996948\n",
      "cnt: 0 - valLoss: 0.4280063807964325 - trainLoss: 0.405817449092865\n",
      "cnt: 0 - valLoss: 0.4280059039592743 - trainLoss: 0.405815988779068\n",
      "cnt: 0 - valLoss: 0.4280054271221161 - trainLoss: 0.40581461787223816\n",
      "cnt: 0 - valLoss: 0.42800503969192505 - trainLoss: 0.40581315755844116\n",
      "cnt: 0 - valLoss: 0.42800456285476685 - trainLoss: 0.40581175684928894\n",
      "cnt: 0 - valLoss: 0.4280041456222534 - trainLoss: 0.4058103561401367\n",
      "cnt: 0 - valLoss: 0.42800372838974 - trainLoss: 0.4058089256286621\n",
      "cnt: 0 - valLoss: 0.4280032813549042 - trainLoss: 0.4058075249195099\n",
      "cnt: 0 - valLoss: 0.42800286412239075 - trainLoss: 0.4058060944080353\n",
      "cnt: 0 - valLoss: 0.42800238728523254 - trainLoss: 0.40580469369888306\n",
      "cnt: 0 - valLoss: 0.42800191044807434 - trainLoss: 0.40580329298973083\n",
      "cnt: 0 - valLoss: 0.4280014932155609 - trainLoss: 0.4058018624782562\n",
      "cnt: 0 - valLoss: 0.4280010461807251 - trainLoss: 0.4058004915714264\n",
      "cnt: 0 - valLoss: 0.42800062894821167 - trainLoss: 0.4057990312576294\n",
      "cnt: 0 - valLoss: 0.42800021171569824 - trainLoss: 0.40579766035079956\n",
      "cnt: 0 - valLoss: 0.42799973487854004 - trainLoss: 0.40579625964164734\n",
      "cnt: 0 - valLoss: 0.42799925804138184 - trainLoss: 0.4057948589324951\n",
      "cnt: 0 - valLoss: 0.4279988706111908 - trainLoss: 0.4057934284210205\n",
      "cnt: 0 - valLoss: 0.4279983937740326 - trainLoss: 0.4057920277118683\n",
      "cnt: 0 - valLoss: 0.42799797654151917 - trainLoss: 0.4057905673980713\n",
      "cnt: 0 - valLoss: 0.42799755930900574 - trainLoss: 0.40578919649124146\n",
      "cnt: 0 - valLoss: 0.4279971122741699 - trainLoss: 0.40578779578208923\n",
      "cnt: 0 - valLoss: 0.4279966354370117 - trainLoss: 0.40578633546829224\n",
      "cnt: 0 - valLoss: 0.4279962182044983 - trainLoss: 0.4057849645614624\n",
      "cnt: 0 - valLoss: 0.42799580097198486 - trainLoss: 0.4057835340499878\n",
      "cnt: 0 - valLoss: 0.42799532413482666 - trainLoss: 0.40578213334083557\n",
      "cnt: 0 - valLoss: 0.42799487709999084 - trainLoss: 0.40578073263168335\n",
      "cnt: 0 - valLoss: 0.4279944598674774 - trainLoss: 0.4057793617248535\n",
      "cnt: 0 - valLoss: 0.427994042634964 - trainLoss: 0.4057779014110565\n",
      "cnt: 0 - valLoss: 0.427993506193161 - trainLoss: 0.4057764708995819\n",
      "cnt: 0 - valLoss: 0.42799311876296997 - trainLoss: 0.4057750701904297\n",
      "cnt: 0 - valLoss: 0.42799270153045654 - trainLoss: 0.40577366948127747\n",
      "cnt: 0 - valLoss: 0.42799222469329834 - trainLoss: 0.40577223896980286\n",
      "cnt: 0 - valLoss: 0.4279918074607849 - trainLoss: 0.405770868062973\n",
      "cnt: 0 - valLoss: 0.4279913604259491 - trainLoss: 0.4057694673538208\n",
      "cnt: 0 - valLoss: 0.4279909133911133 - trainLoss: 0.4057680666446686\n",
      "cnt: 0 - valLoss: 0.4279904365539551 - trainLoss: 0.40576669573783875\n",
      "cnt: 0 - valLoss: 0.42798998951911926 - trainLoss: 0.40576523542404175\n",
      "cnt: 0 - valLoss: 0.42798957228660583 - trainLoss: 0.40576380491256714\n",
      "cnt: 0 - valLoss: 0.4279891550540924 - trainLoss: 0.4057624042034149\n",
      "cnt: 0 - valLoss: 0.4279887080192566 - trainLoss: 0.4057610332965851\n",
      "cnt: 0 - valLoss: 0.427988201379776 - trainLoss: 0.4057595729827881\n",
      "cnt: 0 - valLoss: 0.4279877841472626 - trainLoss: 0.40575820207595825\n",
      "cnt: 0 - valLoss: 0.42798733711242676 - trainLoss: 0.40575674176216125\n",
      "cnt: 0 - valLoss: 0.42798691987991333 - trainLoss: 0.4057553708553314\n",
      "cnt: 0 - valLoss: 0.4279865026473999 - trainLoss: 0.4057539403438568\n",
      "cnt: 0 - valLoss: 0.4279860258102417 - trainLoss: 0.405752569437027\n",
      "cnt: 0 - valLoss: 0.4279855787754059 - trainLoss: 0.40575110912323\n",
      "cnt: 0 - valLoss: 0.4279851019382477 - trainLoss: 0.40574973821640015\n",
      "cnt: 0 - valLoss: 0.42798468470573425 - trainLoss: 0.40574830770492554\n",
      "cnt: 0 - valLoss: 0.4279842674732208 - trainLoss: 0.4057469069957733\n",
      "cnt: 0 - valLoss: 0.4279837906360626 - trainLoss: 0.4057455062866211\n",
      "cnt: 0 - valLoss: 0.4279833734035492 - trainLoss: 0.4057440757751465\n",
      "cnt: 0 - valLoss: 0.427982896566391 - trainLoss: 0.40574267506599426\n",
      "cnt: 0 - valLoss: 0.4279824495315552 - trainLoss: 0.40574127435684204\n",
      "cnt: 0 - valLoss: 0.42798203229904175 - trainLoss: 0.40573984384536743\n",
      "cnt: 0 - valLoss: 0.4279816150665283 - trainLoss: 0.4057384431362152\n",
      "cnt: 0 - valLoss: 0.4279811680316925 - trainLoss: 0.4057370126247406\n",
      "cnt: 0 - valLoss: 0.4279807507991791 - trainLoss: 0.40573564171791077\n",
      "cnt: 0 - valLoss: 0.4279802739620209 - trainLoss: 0.40573424100875854\n",
      "cnt: 0 - valLoss: 0.42797979712486267 - trainLoss: 0.40573278069496155\n",
      "cnt: 0 - valLoss: 0.42797937989234924 - trainLoss: 0.4057313799858093\n",
      "cnt: 0 - valLoss: 0.4279789328575134 - trainLoss: 0.4057300090789795\n",
      "cnt: 0 - valLoss: 0.4279784858226776 - trainLoss: 0.4057285785675049\n",
      "cnt: 0 - valLoss: 0.4279780387878418 - trainLoss: 0.40572717785835266\n",
      "cnt: 0 - valLoss: 0.42797762155532837 - trainLoss: 0.40572577714920044\n",
      "cnt: 0 - valLoss: 0.42797714471817017 - trainLoss: 0.40572434663772583\n",
      "cnt: 0 - valLoss: 0.42797672748565674 - trainLoss: 0.405722975730896\n",
      "cnt: 0 - valLoss: 0.4279762804508209 - trainLoss: 0.4057215750217438\n",
      "cnt: 0 - valLoss: 0.4279758632183075 - trainLoss: 0.40572014451026917\n",
      "cnt: 0 - valLoss: 0.4279753863811493 - trainLoss: 0.40571874380111694\n",
      "cnt: 0 - valLoss: 0.42797496914863586 - trainLoss: 0.4057173430919647\n",
      "cnt: 0 - valLoss: 0.42797449231147766 - trainLoss: 0.4057159125804901\n",
      "cnt: 0 - valLoss: 0.42797404527664185 - trainLoss: 0.4057145118713379\n",
      "cnt: 0 - valLoss: 0.4279736280441284 - trainLoss: 0.4057130813598633\n",
      "cnt: 0 - valLoss: 0.4279731512069702 - trainLoss: 0.40571168065071106\n",
      "cnt: 0 - valLoss: 0.4279727339744568 - trainLoss: 0.40571027994155884\n",
      "cnt: 0 - valLoss: 0.42797228693962097 - trainLoss: 0.405708909034729\n",
      "cnt: 0 - valLoss: 0.42797183990478516 - trainLoss: 0.405707448720932\n",
      "cnt: 0 - valLoss: 0.42797139286994934 - trainLoss: 0.4057060778141022\n",
      "cnt: 0 - valLoss: 0.42797091603279114 - trainLoss: 0.4057046175003052\n",
      "cnt: 0 - valLoss: 0.4279705286026001 - trainLoss: 0.40570324659347534\n",
      "cnt: 0 - valLoss: 0.4279700219631195 - trainLoss: 0.40570178627967834\n",
      "cnt: 0 - valLoss: 0.4279696047306061 - trainLoss: 0.4057004153728485\n",
      "cnt: 0 - valLoss: 0.42796915769577026 - trainLoss: 0.4056990444660187\n",
      "cnt: 0 - valLoss: 0.42796874046325684 - trainLoss: 0.40569761395454407\n",
      "cnt: 0 - valLoss: 0.4279683232307434 - trainLoss: 0.40569624304771423\n",
      "cnt: 0 - valLoss: 0.4279678761959076 - trainLoss: 0.4056948125362396\n",
      "cnt: 0 - valLoss: 0.4279674291610718 - trainLoss: 0.4056934118270874\n",
      "cnt: 0 - valLoss: 0.4279669523239136 - trainLoss: 0.4056919813156128\n",
      "cnt: 0 - valLoss: 0.42796650528907776 - trainLoss: 0.40569058060646057\n",
      "cnt: 0 - valLoss: 0.42796602845191956 - trainLoss: 0.40568917989730835\n",
      "cnt: 0 - valLoss: 0.4279656708240509 - trainLoss: 0.40568774938583374\n",
      "cnt: 0 - valLoss: 0.4279651343822479 - trainLoss: 0.4056863486766815\n",
      "cnt: 0 - valLoss: 0.4279647171497345 - trainLoss: 0.4056849181652069\n",
      "cnt: 0 - valLoss: 0.42796429991722107 - trainLoss: 0.40568357706069946\n",
      "cnt: 0 - valLoss: 0.42796385288238525 - trainLoss: 0.40568214654922485\n",
      "cnt: 0 - valLoss: 0.4279634356498718 - trainLoss: 0.40568074584007263\n",
      "cnt: 0 - valLoss: 0.4279629588127136 - trainLoss: 0.405679315328598\n",
      "cnt: 0 - valLoss: 0.4279625415802002 - trainLoss: 0.4056779146194458\n",
      "cnt: 0 - valLoss: 0.427962064743042 - trainLoss: 0.40567654371261597\n",
      "cnt: 0 - valLoss: 0.4279616177082062 - trainLoss: 0.40567514300346375\n",
      "cnt: 0 - valLoss: 0.42796117067337036 - trainLoss: 0.40567371249198914\n",
      "cnt: 0 - valLoss: 0.42796072363853455 - trainLoss: 0.40567225217819214\n",
      "cnt: 0 - valLoss: 0.42796024680137634 - trainLoss: 0.4056708812713623\n",
      "cnt: 0 - valLoss: 0.4279598295688629 - trainLoss: 0.4056694805622101\n",
      "cnt: 0 - valLoss: 0.4279594123363495 - trainLoss: 0.40566807985305786\n",
      "cnt: 0 - valLoss: 0.42795896530151367 - trainLoss: 0.405666708946228\n",
      "cnt: 0 - valLoss: 0.42795848846435547 - trainLoss: 0.40566524863243103\n",
      "cnt: 0 - valLoss: 0.42795807123184204 - trainLoss: 0.4056638777256012\n",
      "cnt: 0 - valLoss: 0.42795759439468384 - trainLoss: 0.4056624472141266\n",
      "cnt: 0 - valLoss: 0.4279572069644928 - trainLoss: 0.40566104650497437\n",
      "cnt: 0 - valLoss: 0.4279567301273346 - trainLoss: 0.40565964579582214\n",
      "cnt: 0 - valLoss: 0.4279562830924988 - trainLoss: 0.40565821528434753\n",
      "cnt: 0 - valLoss: 0.4279558062553406 - trainLoss: 0.4056568443775177\n",
      "cnt: 0 - valLoss: 0.42795535922050476 - trainLoss: 0.4056554138660431\n",
      "cnt: 0 - valLoss: 0.42795494198799133 - trainLoss: 0.40565404295921326\n",
      "cnt: 0 - valLoss: 0.4279545247554779 - trainLoss: 0.40565261244773865\n",
      "cnt: 0 - valLoss: 0.4279540777206421 - trainLoss: 0.4056512117385864\n",
      "cnt: 0 - valLoss: 0.4279535710811615 - trainLoss: 0.4056498408317566\n",
      "cnt: 0 - valLoss: 0.42795315384864807 - trainLoss: 0.4056483805179596\n",
      "cnt: 0 - valLoss: 0.42795270681381226 - trainLoss: 0.40564700961112976\n",
      "cnt: 0 - valLoss: 0.42795222997665405 - trainLoss: 0.40564554929733276\n",
      "cnt: 0 - valLoss: 0.427951842546463 - trainLoss: 0.40564417839050293\n",
      "cnt: 0 - valLoss: 0.4279513955116272 - trainLoss: 0.4056427776813507\n",
      "cnt: 0 - valLoss: 0.427950918674469 - trainLoss: 0.4056413471698761\n",
      "cnt: 0 - valLoss: 0.4279504716396332 - trainLoss: 0.4056399464607239\n",
      "cnt: 0 - valLoss: 0.42795002460479736 - trainLoss: 0.40563854575157166\n",
      "cnt: 0 - valLoss: 0.42794957756996155 - trainLoss: 0.4056371748447418\n",
      "cnt: 0 - valLoss: 0.4279491603374481 - trainLoss: 0.4056357145309448\n",
      "cnt: 0 - valLoss: 0.4279487133026123 - trainLoss: 0.405634343624115\n",
      "cnt: 0 - valLoss: 0.4279482662677765 - trainLoss: 0.40563294291496277\n",
      "cnt: 0 - valLoss: 0.4279478192329407 - trainLoss: 0.40563151240348816\n",
      "cnt: 0 - valLoss: 0.42794740200042725 - trainLoss: 0.4056301414966583\n",
      "cnt: 0 - valLoss: 0.42794695496559143 - trainLoss: 0.40562868118286133\n",
      "cnt: 0 - valLoss: 0.4279465079307556 - trainLoss: 0.4056273102760315\n",
      "cnt: 0 - valLoss: 0.4279460310935974 - trainLoss: 0.4056259095668793\n",
      "cnt: 0 - valLoss: 0.4279455840587616 - trainLoss: 0.40562450885772705\n",
      "cnt: 0 - valLoss: 0.42794516682624817 - trainLoss: 0.40562307834625244\n",
      "cnt: 0 - valLoss: 0.42794468998908997 - trainLoss: 0.4056216776371002\n",
      "cnt: 0 - valLoss: 0.42794421315193176 - trainLoss: 0.4056203067302704\n",
      "cnt: 0 - valLoss: 0.42794379591941833 - trainLoss: 0.4056188464164734\n",
      "cnt: 0 - valLoss: 0.4279433786869049 - trainLoss: 0.40561747550964355\n",
      "cnt: 0 - valLoss: 0.4279429018497467 - trainLoss: 0.40561607480049133\n",
      "cnt: 0 - valLoss: 0.42794251441955566 - trainLoss: 0.4056146442890167\n",
      "cnt: 0 - valLoss: 0.4279420077800751 - trainLoss: 0.4056132435798645\n",
      "cnt: 0 - valLoss: 0.42794156074523926 - trainLoss: 0.40561187267303467\n",
      "cnt: 0 - valLoss: 0.42794114351272583 - trainLoss: 0.40561047196388245\n",
      "cnt: 0 - valLoss: 0.4279407262802124 - trainLoss: 0.40560904145240784\n",
      "cnt: 0 - valLoss: 0.4279402494430542 - trainLoss: 0.4056076407432556\n",
      "cnt: 0 - valLoss: 0.4279398024082184 - trainLoss: 0.4056062698364258\n",
      "cnt: 0 - valLoss: 0.42793938517570496 - trainLoss: 0.4056048095226288\n",
      "cnt: 0 - valLoss: 0.42793890833854675 - trainLoss: 0.40560343861579895\n",
      "cnt: 0 - valLoss: 0.4279384911060333 - trainLoss: 0.40560203790664673\n",
      "cnt: 0 - valLoss: 0.4279380440711975 - trainLoss: 0.4056006073951721\n",
      "cnt: 0 - valLoss: 0.4279376268386841 - trainLoss: 0.4055992364883423\n",
      "cnt: 0 - valLoss: 0.4279371500015259 - trainLoss: 0.4055977761745453\n",
      "cnt: 0 - valLoss: 0.4279366731643677 - trainLoss: 0.40559640526771545\n",
      "cnt: 0 - valLoss: 0.42793625593185425 - trainLoss: 0.40559500455856323\n",
      "cnt: 0 - valLoss: 0.4279358386993408 - trainLoss: 0.405593603849411\n",
      "cnt: 0 - valLoss: 0.427935391664505 - trainLoss: 0.4055922329425812\n",
      "cnt: 0 - valLoss: 0.4279349148273468 - trainLoss: 0.4055907726287842\n",
      "cnt: 0 - valLoss: 0.4279344975948334 - trainLoss: 0.40558940172195435\n",
      "cnt: 0 - valLoss: 0.42793402075767517 - trainLoss: 0.4055880308151245\n",
      "cnt: 0 - valLoss: 0.42793360352516174 - trainLoss: 0.4055865705013275\n",
      "cnt: 0 - valLoss: 0.4279331564903259 - trainLoss: 0.4055851995944977\n",
      "cnt: 0 - valLoss: 0.4279327094554901 - trainLoss: 0.40558379888534546\n",
      "cnt: 0 - valLoss: 0.4279322624206543 - trainLoss: 0.40558236837387085\n",
      "cnt: 0 - valLoss: 0.4279317855834961 - trainLoss: 0.40558096766471863\n",
      "cnt: 0 - valLoss: 0.42793139815330505 - trainLoss: 0.4055795669555664\n",
      "cnt: 0 - valLoss: 0.42793095111846924 - trainLoss: 0.4055781960487366\n",
      "cnt: 0 - valLoss: 0.4279305040836334 - trainLoss: 0.4055767357349396\n",
      "cnt: 0 - valLoss: 0.42793008685112 - trainLoss: 0.40557536482810974\n",
      "cnt: 0 - valLoss: 0.4279296100139618 - trainLoss: 0.4055739939212799\n",
      "cnt: 0 - valLoss: 0.4279291331768036 - trainLoss: 0.4055725336074829\n",
      "cnt: 0 - valLoss: 0.42792871594429016 - trainLoss: 0.4055711627006531\n",
      "cnt: 0 - valLoss: 0.42792826890945435 - trainLoss: 0.40556976199150085\n",
      "cnt: 0 - valLoss: 0.42792782187461853 - trainLoss: 0.40556833148002625\n",
      "cnt: 0 - valLoss: 0.4279273450374603 - trainLoss: 0.405566930770874\n",
      "cnt: 0 - valLoss: 0.4279269576072693 - trainLoss: 0.4055655300617218\n",
      "cnt: 0 - valLoss: 0.4279264807701111 - trainLoss: 0.40556415915489197\n",
      "cnt: 0 - valLoss: 0.42792606353759766 - trainLoss: 0.40556272864341736\n",
      "cnt: 0 - valLoss: 0.42792558670043945 - trainLoss: 0.40556132793426514\n",
      "cnt: 0 - valLoss: 0.42792513966560364 - trainLoss: 0.4055598974227905\n",
      "cnt: 0 - valLoss: 0.4279246926307678 - trainLoss: 0.4055584967136383\n",
      "cnt: 0 - valLoss: 0.427924245595932 - trainLoss: 0.40555712580680847\n",
      "cnt: 0 - valLoss: 0.4279238283634186 - trainLoss: 0.40555572509765625\n",
      "cnt: 0 - valLoss: 0.42792338132858276 - trainLoss: 0.40555429458618164\n",
      "cnt: 0 - valLoss: 0.42792293429374695 - trainLoss: 0.4055529236793518\n",
      "cnt: 0 - valLoss: 0.42792248725891113 - trainLoss: 0.4055515229701996\n",
      "cnt: 0 - valLoss: 0.42792198061943054 - trainLoss: 0.40555012226104736\n",
      "cnt: 0 - valLoss: 0.4279215633869171 - trainLoss: 0.40554869174957275\n",
      "cnt: 0 - valLoss: 0.4279211163520813 - trainLoss: 0.40554729104042053\n",
      "cnt: 0 - valLoss: 0.4279206395149231 - trainLoss: 0.4055459201335907\n",
      "cnt: 0 - valLoss: 0.42792022228240967 - trainLoss: 0.40554454922676086\n",
      "cnt: 0 - valLoss: 0.42791980504989624 - trainLoss: 0.40554308891296387\n",
      "cnt: 0 - valLoss: 0.42791926860809326 - trainLoss: 0.40554171800613403\n",
      "cnt: 0 - valLoss: 0.42791885137557983 - trainLoss: 0.40554025769233704\n",
      "cnt: 0 - valLoss: 0.4279184341430664 - trainLoss: 0.4055388867855072\n",
      "cnt: 0 - valLoss: 0.4279179871082306 - trainLoss: 0.40553751587867737\n",
      "cnt: 0 - valLoss: 0.42791756987571716 - trainLoss: 0.40553611516952515\n",
      "cnt: 0 - valLoss: 0.4279170632362366 - trainLoss: 0.40553468465805054\n",
      "cnt: 0 - valLoss: 0.42791661620140076 - trainLoss: 0.4055332839488983\n",
      "cnt: 0 - valLoss: 0.42791619896888733 - trainLoss: 0.4055318832397461\n",
      "cnt: 0 - valLoss: 0.4279157519340515 - trainLoss: 0.40553051233291626\n",
      "cnt: 0 - valLoss: 0.4279153048992157 - trainLoss: 0.40552908182144165\n",
      "cnt: 0 - valLoss: 0.4279148280620575 - trainLoss: 0.40552768111228943\n",
      "cnt: 0 - valLoss: 0.4279143810272217 - trainLoss: 0.4055263102054596\n",
      "cnt: 0 - valLoss: 0.42791396379470825 - trainLoss: 0.4055249094963074\n",
      "cnt: 0 - valLoss: 0.4279135465621948 - trainLoss: 0.40552347898483276\n",
      "cnt: 0 - valLoss: 0.42791301012039185 - trainLoss: 0.40552210807800293\n",
      "cnt: 0 - valLoss: 0.4279125928878784 - trainLoss: 0.40552064776420593\n",
      "cnt: 0 - valLoss: 0.427912175655365 - trainLoss: 0.4055192768573761\n",
      "cnt: 0 - valLoss: 0.4279116988182068 - trainLoss: 0.40551790595054626\n",
      "cnt: 0 - valLoss: 0.42791125178337097 - trainLoss: 0.40551647543907166\n",
      "cnt: 0 - valLoss: 0.42791080474853516 - trainLoss: 0.4055151045322418\n",
      "cnt: 0 - valLoss: 0.42791035771369934 - trainLoss: 0.4055136442184448\n",
      "cnt: 0 - valLoss: 0.4279099404811859 - trainLoss: 0.405512273311615\n",
      "cnt: 0 - valLoss: 0.4279094934463501 - trainLoss: 0.40551087260246277\n",
      "cnt: 0 - valLoss: 0.4279090464115143 - trainLoss: 0.40550944209098816\n",
      "cnt: 0 - valLoss: 0.4279085695743561 - trainLoss: 0.4055080711841583\n",
      "cnt: 0 - valLoss: 0.42790812253952026 - trainLoss: 0.4055066704750061\n",
      "cnt: 0 - valLoss: 0.42790770530700684 - trainLoss: 0.4055052399635315\n",
      "cnt: 0 - valLoss: 0.42790722846984863 - trainLoss: 0.40550386905670166\n",
      "cnt: 0 - valLoss: 0.42790675163269043 - trainLoss: 0.40550246834754944\n",
      "cnt: 0 - valLoss: 0.427906334400177 - trainLoss: 0.4055010676383972\n",
      "cnt: 0 - valLoss: 0.4279055595397949 - trainLoss: 0.4054996371269226\n",
      "cnt: 0 - valLoss: 0.4279051721096039 - trainLoss: 0.4054982662200928\n",
      "cnt: 0 - valLoss: 0.42790478467941284 - trainLoss: 0.40549686551094055\n",
      "cnt: 0 - valLoss: 0.4279043972492218 - trainLoss: 0.4054954946041107\n",
      "cnt: 0 - valLoss: 0.42790400981903076 - trainLoss: 0.4054940938949585\n",
      "cnt: 0 - valLoss: 0.4279036819934845 - trainLoss: 0.4054926633834839\n",
      "cnt: 0 - valLoss: 0.42790329456329346 - trainLoss: 0.40549129247665405\n",
      "cnt: 0 - valLoss: 0.4279029369354248 - trainLoss: 0.40548983216285706\n",
      "cnt: 0 - valLoss: 0.4279025197029114 - trainLoss: 0.4054884612560272\n",
      "cnt: 0 - valLoss: 0.4279021918773651 - trainLoss: 0.405487060546875\n",
      "cnt: 0 - valLoss: 0.42790141701698303 - trainLoss: 0.40548568964004517\n",
      "cnt: 0 - valLoss: 0.427901029586792 - trainLoss: 0.40548428893089294\n",
      "cnt: 0 - valLoss: 0.42790067195892334 - trainLoss: 0.4054829180240631\n",
      "cnt: 0 - valLoss: 0.4279003441333771 - trainLoss: 0.4054814577102661\n",
      "cnt: 0 - valLoss: 0.42789995670318604 - trainLoss: 0.4054800868034363\n",
      "cnt: 0 - valLoss: 0.4278995990753174 - trainLoss: 0.40547868609428406\n",
      "cnt: 0 - valLoss: 0.42789921164512634 - trainLoss: 0.40547725558280945\n",
      "cnt: 0 - valLoss: 0.4278988838195801 - trainLoss: 0.4054758846759796\n",
      "cnt: 0 - valLoss: 0.4278985261917114 - trainLoss: 0.4054744839668274\n",
      "cnt: 0 - valLoss: 0.427898108959198 - trainLoss: 0.4054730534553528\n",
      "cnt: 0 - valLoss: 0.4278973340988159 - trainLoss: 0.40547168254852295\n",
      "cnt: 0 - valLoss: 0.42789700627326965 - trainLoss: 0.40547025203704834\n",
      "cnt: 0 - valLoss: 0.427896648645401 - trainLoss: 0.4054688811302185\n",
      "cnt: 0 - valLoss: 0.42789632081985474 - trainLoss: 0.40546751022338867\n",
      "cnt: 0 - valLoss: 0.4278959631919861 - trainLoss: 0.40546607971191406\n",
      "cnt: 0 - valLoss: 0.42789557576179504 - trainLoss: 0.40546467900276184\n",
      "cnt: 0 - valLoss: 0.4278952479362488 - trainLoss: 0.405463308095932\n",
      "cnt: 0 - valLoss: 0.42789486050605774 - trainLoss: 0.405461847782135\n",
      "cnt: 0 - valLoss: 0.4278945028781891 - trainLoss: 0.4054604768753052\n",
      "cnt: 0 - valLoss: 0.42789411544799805 - trainLoss: 0.40545910596847534\n",
      "cnt: 0 - valLoss: 0.427893728017807 - trainLoss: 0.4054577052593231\n",
      "cnt: 0 - valLoss: 0.4278929531574249 - trainLoss: 0.4054562747478485\n",
      "cnt: 0 - valLoss: 0.42789265513420105 - trainLoss: 0.4054549038410187\n",
      "cnt: 0 - valLoss: 0.42789226770401 - trainLoss: 0.40545350313186646\n",
      "cnt: 0 - valLoss: 0.42789193987846375 - trainLoss: 0.4054521322250366\n",
      "cnt: 0 - valLoss: 0.4278915822505951 - trainLoss: 0.4054507315158844\n",
      "cnt: 0 - valLoss: 0.42789119482040405 - trainLoss: 0.4054492712020874\n",
      "cnt: 0 - valLoss: 0.427890807390213 - trainLoss: 0.40544790029525757\n",
      "cnt: 0 - valLoss: 0.42789047956466675 - trainLoss: 0.40544646978378296\n",
      "cnt: 0 - valLoss: 0.4278901219367981 - trainLoss: 0.4054451286792755\n",
      "cnt: 0 - valLoss: 0.42788973450660706 - trainLoss: 0.4054436981678009\n",
      "cnt: 0 - valLoss: 0.4278894066810608 - trainLoss: 0.40544232726097107\n",
      "cnt: 0 - valLoss: 0.4278886020183563 - trainLoss: 0.40544092655181885\n",
      "cnt: 0 - valLoss: 0.42788830399513245 - trainLoss: 0.405439555644989\n",
      "cnt: 0 - valLoss: 0.4278879165649414 - trainLoss: 0.4054381251335144\n",
      "cnt: 0 - valLoss: 0.42788755893707275 - trainLoss: 0.4054367244243622\n",
      "cnt: 0 - valLoss: 0.4278871715068817 - trainLoss: 0.40543532371520996\n",
      "cnt: 0 - valLoss: 0.4278867840766907 - trainLoss: 0.4054339528083801\n",
      "cnt: 0 - valLoss: 0.4278864562511444 - trainLoss: 0.4054325222969055\n",
      "cnt: 0 - valLoss: 0.42788609862327576 - trainLoss: 0.4054311215877533\n",
      "cnt: 0 - valLoss: 0.4278857111930847 - trainLoss: 0.4054297208786011\n",
      "cnt: 0 - valLoss: 0.42788538336753845 - trainLoss: 0.40542834997177124\n",
      "cnt: 0 - valLoss: 0.4278850257396698 - trainLoss: 0.40542691946029663\n",
      "cnt: 0 - valLoss: 0.42788422107696533 - trainLoss: 0.4054255485534668\n",
      "cnt: 0 - valLoss: 0.4278838634490967 - trainLoss: 0.40542417764663696\n",
      "cnt: 0 - valLoss: 0.4278835356235504 - trainLoss: 0.40542277693748474\n",
      "cnt: 0 - valLoss: 0.4278831481933594 - trainLoss: 0.40542134642601013\n",
      "cnt: 0 - valLoss: 0.4278827905654907 - trainLoss: 0.4054199755191803\n",
      "cnt: 0 - valLoss: 0.42788246273994446 - trainLoss: 0.4054185748100281\n",
      "cnt: 0 - valLoss: 0.4278820753097534 - trainLoss: 0.40541717410087585\n",
      "cnt: 0 - valLoss: 0.42788174748420715 - trainLoss: 0.405415803194046\n",
      "cnt: 0 - valLoss: 0.4278813600540161 - trainLoss: 0.4054143726825714\n",
      "cnt: 0 - valLoss: 0.4278809726238251 - trainLoss: 0.4054129719734192\n",
      "cnt: 0 - valLoss: 0.4278806149959564 - trainLoss: 0.40541160106658936\n",
      "cnt: 0 - valLoss: 0.42788028717041016 - trainLoss: 0.40541014075279236\n",
      "cnt: 0 - valLoss: 0.4278795123100281 - trainLoss: 0.4054087698459625\n",
      "cnt: 0 - valLoss: 0.42787912487983704 - trainLoss: 0.4054073989391327\n",
      "cnt: 0 - valLoss: 0.427878737449646 - trainLoss: 0.40540599822998047\n",
      "cnt: 0 - valLoss: 0.42787837982177734 - trainLoss: 0.40540462732315063\n",
      "cnt: 0 - valLoss: 0.4278779923915863 - trainLoss: 0.4054032266139984\n",
      "cnt: 0 - valLoss: 0.4278776943683624 - trainLoss: 0.4054018557071686\n",
      "cnt: 0 - valLoss: 0.4278773069381714 - trainLoss: 0.40540042519569397\n",
      "cnt: 0 - valLoss: 0.42787691950798035 - trainLoss: 0.40539902448654175\n",
      "cnt: 0 - valLoss: 0.4278765916824341 - trainLoss: 0.4053976535797119\n",
      "cnt: 0 - valLoss: 0.42787620425224304 - trainLoss: 0.4053962230682373\n",
      "cnt: 0 - valLoss: 0.4278758764266968 - trainLoss: 0.4053948223590851\n",
      "cnt: 0 - valLoss: 0.4278750419616699 - trainLoss: 0.40539345145225525\n",
      "cnt: 0 - valLoss: 0.42787471413612366 - trainLoss: 0.405392050743103\n",
      "cnt: 0 - valLoss: 0.427874356508255 - trainLoss: 0.4053906202316284\n",
      "cnt: 0 - valLoss: 0.42787396907806396 - trainLoss: 0.4053892493247986\n",
      "cnt: 0 - valLoss: 0.4278736412525177 - trainLoss: 0.40538784861564636\n",
      "cnt: 0 - valLoss: 0.42787325382232666 - trainLoss: 0.40538644790649414\n",
      "cnt: 0 - valLoss: 0.427872896194458 - trainLoss: 0.4053850769996643\n",
      "cnt: 0 - valLoss: 0.42787256836891174 - trainLoss: 0.4053836464881897\n",
      "cnt: 0 - valLoss: 0.4278721809387207 - trainLoss: 0.4053822457790375\n",
      "cnt: 0 - valLoss: 0.42787179350852966 - trainLoss: 0.40538087487220764\n",
      "cnt: 0 - valLoss: 0.427871435880661 - trainLoss: 0.4053795039653778\n",
      "cnt: 0 - valLoss: 0.42787110805511475 - trainLoss: 0.405378133058548\n",
      "cnt: 0 - valLoss: 0.4278703033924103 - trainLoss: 0.405376672744751\n",
      "cnt: 0 - valLoss: 0.42786991596221924 - trainLoss: 0.40537530183792114\n",
      "cnt: 0 - valLoss: 0.4278695583343506 - trainLoss: 0.4053739011287689\n",
      "cnt: 0 - valLoss: 0.4278692305088043 - trainLoss: 0.4053724706172943\n",
      "cnt: 0 - valLoss: 0.42786869406700134 - trainLoss: 0.4053710997104645\n",
      "cnt: 0 - valLoss: 0.4278682768344879 - trainLoss: 0.40536969900131226\n",
      "cnt: 0 - valLoss: 0.4278678596019745 - trainLoss: 0.40536829829216003\n",
      "cnt: 0 - valLoss: 0.42786741256713867 - trainLoss: 0.4053669571876526\n",
      "cnt: 0 - valLoss: 0.42786699533462524 - trainLoss: 0.40536555647850037\n",
      "cnt: 0 - valLoss: 0.42786651849746704 - trainLoss: 0.40536418557167053\n",
      "cnt: 0 - valLoss: 0.4278661012649536 - trainLoss: 0.4053628146648407\n",
      "cnt: 0 - valLoss: 0.4278656244277954 - trainLoss: 0.4053613841533661\n",
      "cnt: 0 - valLoss: 0.4278651773929596 - trainLoss: 0.40535998344421387\n",
      "cnt: 0 - valLoss: 0.42786476016044617 - trainLoss: 0.40535861253738403\n",
      "cnt: 0 - valLoss: 0.42786434292793274 - trainLoss: 0.4053572118282318\n",
      "cnt: 0 - valLoss: 0.42786386609077454 - trainLoss: 0.405355840921402\n",
      "cnt: 0 - valLoss: 0.42786338925361633 - trainLoss: 0.40535441040992737\n",
      "cnt: 0 - valLoss: 0.4278629720211029 - trainLoss: 0.40535303950309753\n",
      "cnt: 0 - valLoss: 0.4278625249862671 - trainLoss: 0.4053516685962677\n",
      "cnt: 0 - valLoss: 0.42786210775375366 - trainLoss: 0.4053502678871155\n",
      "cnt: 0 - valLoss: 0.42786160111427307 - trainLoss: 0.40534889698028564\n",
      "cnt: 0 - valLoss: 0.42786115407943726 - trainLoss: 0.4053474962711334\n",
      "cnt: 0 - valLoss: 0.42786073684692383 - trainLoss: 0.4053461253643036\n",
      "cnt: 0 - valLoss: 0.427860289812088 - trainLoss: 0.405344694852829\n",
      "cnt: 0 - valLoss: 0.4278598725795746 - trainLoss: 0.40534332394599915\n",
      "cnt: 0 - valLoss: 0.42785945534706116 - trainLoss: 0.4053419232368469\n",
      "cnt: 0 - valLoss: 0.42785897850990295 - trainLoss: 0.4053405523300171\n",
      "cnt: 0 - valLoss: 0.42785850167274475 - trainLoss: 0.40533918142318726\n",
      "cnt: 0 - valLoss: 0.4278580844402313 - trainLoss: 0.40533772110939026\n",
      "cnt: 0 - valLoss: 0.4278576374053955 - trainLoss: 0.4053363502025604\n",
      "cnt: 0 - valLoss: 0.4278572201728821 - trainLoss: 0.4053349792957306\n",
      "cnt: 0 - valLoss: 0.42785680294036865 - trainLoss: 0.40533360838890076\n",
      "cnt: 0 - valLoss: 0.42785632610321045 - trainLoss: 0.40533220767974854\n",
      "cnt: 0 - valLoss: 0.42785587906837463 - trainLoss: 0.4053307771682739\n",
      "cnt: 0 - valLoss: 0.42785540223121643 - trainLoss: 0.4053294360637665\n",
      "cnt: 0 - valLoss: 0.427854984998703 - trainLoss: 0.40532806515693665\n",
      "cnt: 0 - valLoss: 0.4278545677661896 - trainLoss: 0.40532663464546204\n",
      "cnt: 0 - valLoss: 0.42785412073135376 - trainLoss: 0.4053252339363098\n",
      "cnt: 0 - valLoss: 0.42785367369651794 - trainLoss: 0.40532386302948\n",
      "cnt: 0 - valLoss: 0.42785322666168213 - trainLoss: 0.40532249212265015\n",
      "cnt: 0 - valLoss: 0.4278528094291687 - trainLoss: 0.4053211212158203\n",
      "cnt: 0 - valLoss: 0.4278523325920105 - trainLoss: 0.4053197205066681\n",
      "cnt: 0 - valLoss: 0.42785191535949707 - trainLoss: 0.40531834959983826\n",
      "cnt: 0 - valLoss: 0.42785146832466125 - trainLoss: 0.40531691908836365\n",
      "cnt: 0 - valLoss: 0.42785099148750305 - trainLoss: 0.4053155481815338\n",
      "cnt: 0 - valLoss: 0.427850604057312 - trainLoss: 0.405314177274704\n",
      "cnt: 0 - valLoss: 0.4278501570224762 - trainLoss: 0.40531277656555176\n",
      "cnt: 0 - valLoss: 0.4278497099876404 - trainLoss: 0.4053114056587219\n",
      "cnt: 0 - valLoss: 0.4278492331504822 - trainLoss: 0.4053100049495697\n",
      "cnt: 0 - valLoss: 0.42784881591796875 - trainLoss: 0.40530863404273987\n",
      "cnt: 0 - valLoss: 0.4278483986854553 - trainLoss: 0.40530720353126526\n",
      "cnt: 0 - valLoss: 0.4278479516506195 - trainLoss: 0.40530580282211304\n",
      "cnt: 0 - valLoss: 0.4278475344181061 - trainLoss: 0.4053044319152832\n",
      "cnt: 0 - valLoss: 0.4278470575809479 - trainLoss: 0.40530306100845337\n",
      "cnt: 0 - valLoss: 0.42784664034843445 - trainLoss: 0.40530163049697876\n",
      "cnt: 0 - valLoss: 0.42784619331359863 - trainLoss: 0.40530022978782654\n",
      "cnt: 0 - valLoss: 0.42784571647644043 - trainLoss: 0.4052988588809967\n",
      "cnt: 0 - valLoss: 0.427845299243927 - trainLoss: 0.40529748797416687\n",
      "cnt: 0 - valLoss: 0.4278448820114136 - trainLoss: 0.40529608726501465\n",
      "cnt: 0 - valLoss: 0.42784443497657776 - trainLoss: 0.40529465675354004\n",
      "cnt: 0 - valLoss: 0.42784395813941956 - trainLoss: 0.405293345451355\n",
      "cnt: 0 - valLoss: 0.42784354090690613 - trainLoss: 0.40529191493988037\n",
      "cnt: 0 - valLoss: 0.4278431236743927 - trainLoss: 0.40529054403305054\n",
      "cnt: 0 - valLoss: 0.4278426468372345 - trainLoss: 0.4052891433238983\n",
      "cnt: 0 - valLoss: 0.42784222960472107 - trainLoss: 0.4052877724170685\n",
      "cnt: 0 - valLoss: 0.42784178256988525 - trainLoss: 0.40528637170791626\n",
      "cnt: 0 - valLoss: 0.4278413653373718 - trainLoss: 0.4052850008010864\n",
      "cnt: 0 - valLoss: 0.427840918302536 - trainLoss: 0.4052836298942566\n",
      "cnt: 0 - valLoss: 0.4278405010700226 - trainLoss: 0.405282199382782\n",
      "cnt: 0 - valLoss: 0.4278400242328644 - trainLoss: 0.40528082847595215\n",
      "cnt: 0 - valLoss: 0.42783963680267334 - trainLoss: 0.4052794277667999\n",
      "cnt: 0 - valLoss: 0.42783915996551514 - trainLoss: 0.4052780568599701\n",
      "cnt: 0 - valLoss: 0.4278387129306793 - trainLoss: 0.40527668595314026\n",
      "cnt: 0 - valLoss: 0.4278383255004883 - trainLoss: 0.40527528524398804\n",
      "cnt: 0 - valLoss: 0.42783787846565247 - trainLoss: 0.4052739143371582\n",
      "cnt: 0 - valLoss: 0.42783740162849426 - trainLoss: 0.4052724838256836\n",
      "cnt: 0 - valLoss: 0.4278370141983032 - trainLoss: 0.40527111291885376\n",
      "cnt: 0 - valLoss: 0.4278365671634674 - trainLoss: 0.4052697420120239\n",
      "cnt: 0 - valLoss: 0.42783617973327637 - trainLoss: 0.4052684009075165\n",
      "cnt: 0 - valLoss: 0.42783570289611816 - trainLoss: 0.40526703000068665\n",
      "cnt: 0 - valLoss: 0.42783528566360474 - trainLoss: 0.40526559948921204\n",
      "cnt: 0 - valLoss: 0.4278348386287689 - trainLoss: 0.4052641987800598\n",
      "cnt: 0 - valLoss: 0.4278344213962555 - trainLoss: 0.40526282787323\n",
      "cnt: 0 - valLoss: 0.4278339743614197 - trainLoss: 0.40526145696640015\n",
      "cnt: 0 - valLoss: 0.42783355712890625 - trainLoss: 0.4052600860595703\n",
      "cnt: 0 - valLoss: 0.4278331398963928 - trainLoss: 0.4052586853504181\n",
      "cnt: 0 - valLoss: 0.427832692861557 - trainLoss: 0.40525731444358826\n",
      "cnt: 0 - valLoss: 0.4278322756290436 - trainLoss: 0.40525588393211365\n",
      "cnt: 0 - valLoss: 0.42783182859420776 - trainLoss: 0.4052545726299286\n",
      "cnt: 0 - valLoss: 0.42783141136169434 - trainLoss: 0.4052531123161316\n",
      "cnt: 0 - valLoss: 0.4278309941291809 - trainLoss: 0.40525177121162415\n",
      "cnt: 0 - valLoss: 0.4278305470943451 - trainLoss: 0.4052503705024719\n",
      "cnt: 0 - valLoss: 0.42783015966415405 - trainLoss: 0.4052489697933197\n",
      "cnt: 0 - valLoss: 0.4278297424316406 - trainLoss: 0.40524759888648987\n",
      "cnt: 0 - valLoss: 0.4278292655944824 - trainLoss: 0.40524622797966003\n",
      "cnt: 0 - valLoss: 0.427828848361969 - trainLoss: 0.4052448570728302\n",
      "cnt: 0 - valLoss: 0.42782846093177795 - trainLoss: 0.4052434265613556\n",
      "cnt: 0 - valLoss: 0.42782801389694214 - trainLoss: 0.40524205565452576\n",
      "cnt: 0 - valLoss: 0.4278275966644287 - trainLoss: 0.4052406847476959\n",
      "cnt: 0 - valLoss: 0.42782720923423767 - trainLoss: 0.4052392840385437\n",
      "cnt: 0 - valLoss: 0.42782679200172424 - trainLoss: 0.40523794293403625\n",
      "cnt: 0 - valLoss: 0.42782631516456604 - trainLoss: 0.40523654222488403\n",
      "cnt: 0 - valLoss: 0.4278258979320526 - trainLoss: 0.4052351713180542\n",
      "cnt: 0 - valLoss: 0.4278255105018616 - trainLoss: 0.40523380041122437\n",
      "cnt: 0 - valLoss: 0.42782506346702576 - trainLoss: 0.40523239970207214\n",
      "cnt: 0 - valLoss: 0.42782464623451233 - trainLoss: 0.40523096919059753\n",
      "cnt: 0 - valLoss: 0.4278241991996765 - trainLoss: 0.4052296578884125\n",
      "cnt: 0 - valLoss: 0.4278237819671631 - trainLoss: 0.40522822737693787\n",
      "cnt: 0 - valLoss: 0.42782336473464966 - trainLoss: 0.40522685647010803\n",
      "cnt: 0 - valLoss: 0.42782291769981384 - trainLoss: 0.4052254557609558\n",
      "cnt: 0 - valLoss: 0.4278225004673004 - trainLoss: 0.405224084854126\n",
      "cnt: 0 - valLoss: 0.4278221130371094 - trainLoss: 0.40522271394729614\n",
      "cnt: 0 - valLoss: 0.42782172560691833 - trainLoss: 0.4052213132381439\n",
      "cnt: 0 - valLoss: 0.42782124876976013 - trainLoss: 0.4052199423313141\n",
      "cnt: 0 - valLoss: 0.4278208315372467 - trainLoss: 0.40521857142448425\n",
      "cnt: 0 - valLoss: 0.42782044410705566 - trainLoss: 0.4052172005176544\n",
      "cnt: 0 - valLoss: 0.42782002687454224 - trainLoss: 0.4052158296108246\n",
      "cnt: 0 - valLoss: 0.42781955003738403 - trainLoss: 0.40521442890167236\n",
      "cnt: 0 - valLoss: 0.4278191030025482 - trainLoss: 0.40521299839019775\n",
      "cnt: 0 - valLoss: 0.4278187155723572 - trainLoss: 0.4052116274833679\n",
      "cnt: 0 - valLoss: 0.42781829833984375 - trainLoss: 0.40521031618118286\n",
      "cnt: 0 - valLoss: 0.4278179109096527 - trainLoss: 0.40520888566970825\n",
      "cnt: 0 - valLoss: 0.4278174340724945 - trainLoss: 0.4052075147628784\n",
      "cnt: 0 - valLoss: 0.42781704664230347 - trainLoss: 0.4052061140537262\n",
      "cnt: 0 - valLoss: 0.42781662940979004 - trainLoss: 0.40520474314689636\n",
      "cnt: 0 - valLoss: 0.427816241979599 - trainLoss: 0.40520334243774414\n",
      "cnt: 0 - valLoss: 0.42781582474708557 - trainLoss: 0.4052019715309143\n",
      "cnt: 0 - valLoss: 0.42781537771224976 - trainLoss: 0.4052006006240845\n",
      "cnt: 0 - valLoss: 0.42781496047973633 - trainLoss: 0.40519922971725464\n",
      "cnt: 0 - valLoss: 0.4278145134449005 - trainLoss: 0.4051978588104248\n",
      "cnt: 0 - valLoss: 0.4278140962123871 - trainLoss: 0.4051964282989502\n",
      "cnt: 0 - valLoss: 0.42781370878219604 - trainLoss: 0.40519505739212036\n",
      "cnt: 0 - valLoss: 0.4278132915496826 - trainLoss: 0.40519365668296814\n",
      "cnt: 0 - valLoss: 0.4278128147125244 - trainLoss: 0.4051922857761383\n",
      "cnt: 0 - valLoss: 0.42781245708465576 - trainLoss: 0.40519091486930847\n",
      "cnt: 0 - valLoss: 0.42781203985214233 - trainLoss: 0.40518951416015625\n",
      "cnt: 0 - valLoss: 0.4278116226196289 - trainLoss: 0.4051881730556488\n",
      "cnt: 0 - valLoss: 0.4278111755847931 - trainLoss: 0.4051867723464966\n",
      "cnt: 0 - valLoss: 0.42781075835227966 - trainLoss: 0.40518540143966675\n",
      "cnt: 0 - valLoss: 0.4278103709220886 - trainLoss: 0.40518397092819214\n",
      "cnt: 0 - valLoss: 0.4278099536895752 - trainLoss: 0.4051826000213623\n",
      "cnt: 0 - valLoss: 0.4278095066547394 - trainLoss: 0.40518122911453247\n",
      "cnt: 0 - valLoss: 0.42780908942222595 - trainLoss: 0.40517985820770264\n",
      "cnt: 0 - valLoss: 0.4278087019920349 - trainLoss: 0.4051785171031952\n",
      "cnt: 0 - valLoss: 0.4278082549571991 - trainLoss: 0.40517714619636536\n",
      "cnt: 0 - valLoss: 0.42780783772468567 - trainLoss: 0.40517571568489075\n",
      "cnt: 0 - valLoss: 0.42780745029449463 - trainLoss: 0.4051743447780609\n",
      "cnt: 0 - valLoss: 0.4278070330619812 - trainLoss: 0.4051729440689087\n",
      "cnt: 0 - valLoss: 0.4278065860271454 - trainLoss: 0.40517157316207886\n",
      "cnt: 0 - valLoss: 0.42780616879463196 - trainLoss: 0.405170202255249\n",
      "cnt: 0 - valLoss: 0.4278057813644409 - trainLoss: 0.4051688313484192\n",
      "cnt: 0 - valLoss: 0.4278053641319275 - trainLoss: 0.40516746044158936\n",
      "cnt: 0 - valLoss: 0.4278049170970917 - trainLoss: 0.40516605973243713\n",
      "cnt: 0 - valLoss: 0.42780452966690063 - trainLoss: 0.4051646888256073\n",
      "cnt: 0 - valLoss: 0.4278041124343872 - trainLoss: 0.4051632583141327\n",
      "cnt: 0 - valLoss: 0.42780372500419617 - trainLoss: 0.40516188740730286\n",
      "cnt: 0 - valLoss: 0.42780330777168274 - trainLoss: 0.405160516500473\n",
      "cnt: 0 - valLoss: 0.4278028607368469 - trainLoss: 0.4051591455936432\n",
      "cnt: 0 - valLoss: 0.4278024435043335 - trainLoss: 0.40515774488449097\n",
      "cnt: 0 - valLoss: 0.42780205607414246 - trainLoss: 0.40515637397766113\n",
      "cnt: 0 - valLoss: 0.42780163884162903 - trainLoss: 0.4051550030708313\n",
      "cnt: 0 - valLoss: 0.427801251411438 - trainLoss: 0.40515363216400146\n",
      "cnt: 0 - valLoss: 0.4278008043766022 - trainLoss: 0.40515223145484924\n",
      "cnt: 0 - valLoss: 0.42780038714408875 - trainLoss: 0.4051508605480194\n",
      "cnt: 0 - valLoss: 0.42779994010925293 - trainLoss: 0.4051494896411896\n",
      "cnt: 0 - valLoss: 0.4277995228767395 - trainLoss: 0.40514805912971497\n",
      "cnt: 0 - valLoss: 0.42779913544654846 - trainLoss: 0.40514668822288513\n",
      "cnt: 0 - valLoss: 0.42779871821403503 - trainLoss: 0.4051453173160553\n",
      "cnt: 0 - valLoss: 0.427798330783844 - trainLoss: 0.4051439166069031\n",
      "cnt: 0 - valLoss: 0.4277978837490082 - trainLoss: 0.405142605304718\n",
      "cnt: 0 - valLoss: 0.42779746651649475 - trainLoss: 0.4051411747932434\n",
      "cnt: 0 - valLoss: 0.4277970492839813 - trainLoss: 0.4051398038864136\n",
      "cnt: 0 - valLoss: 0.4277966618537903 - trainLoss: 0.40513843297958374\n",
      "cnt: 0 - valLoss: 0.42779621481895447 - trainLoss: 0.4051370620727539\n",
      "cnt: 0 - valLoss: 0.4277958273887634 - trainLoss: 0.4051356613636017\n",
      "cnt: 0 - valLoss: 0.42779541015625 - trainLoss: 0.40513429045677185\n",
      "cnt: 0 - valLoss: 0.4277949929237366 - trainLoss: 0.405132919549942\n",
      "cnt: 0 - valLoss: 0.42779454588890076 - trainLoss: 0.4051315486431122\n",
      "cnt: 0 - valLoss: 0.4277941584587097 - trainLoss: 0.40513014793395996\n",
      "cnt: 0 - valLoss: 0.4277937412261963 - trainLoss: 0.4051287770271301\n",
      "cnt: 0 - valLoss: 0.42779335379600525 - trainLoss: 0.4051274061203003\n",
      "cnt: 0 - valLoss: 0.4277929365634918 - trainLoss: 0.40512603521347046\n",
      "cnt: 0 - valLoss: 0.427792489528656 - trainLoss: 0.40512460470199585\n",
      "cnt: 0 - valLoss: 0.42779210209846497 - trainLoss: 0.405123233795166\n",
      "cnt: 0 - valLoss: 0.42779168486595154 - trainLoss: 0.4051218628883362\n",
      "cnt: 0 - valLoss: 0.4277912378311157 - trainLoss: 0.40512046217918396\n",
      "cnt: 0 - valLoss: 0.42779088020324707 - trainLoss: 0.4051190912723541\n",
      "cnt: 0 - valLoss: 0.42779043316841125 - trainLoss: 0.4051177203655243\n",
      "cnt: 0 - valLoss: 0.4277900159358978 - trainLoss: 0.40511634945869446\n",
      "cnt: 0 - valLoss: 0.4277896285057068 - trainLoss: 0.4051149785518646\n",
      "cnt: 0 - valLoss: 0.42778924107551575 - trainLoss: 0.4051136076450348\n",
      "cnt: 0 - valLoss: 0.4277888238430023 - trainLoss: 0.40511223673820496\n",
      "cnt: 0 - valLoss: 0.4277883768081665 - trainLoss: 0.40511083602905273\n",
      "cnt: 0 - valLoss: 0.4277879595756531 - trainLoss: 0.4051094651222229\n",
      "cnt: 0 - valLoss: 0.42778757214546204 - trainLoss: 0.4051080644130707\n",
      "cnt: 0 - valLoss: 0.427787184715271 - trainLoss: 0.40510669350624084\n",
      "cnt: 0 - valLoss: 0.4277867376804352 - trainLoss: 0.405105322599411\n",
      "cnt: 0 - valLoss: 0.42778632044792175 - trainLoss: 0.4051039516925812\n",
      "cnt: 0 - valLoss: 0.4277859330177307 - trainLoss: 0.40510258078575134\n",
      "cnt: 0 - valLoss: 0.4277855455875397 - trainLoss: 0.4051012098789215\n",
      "cnt: 0 - valLoss: 0.42778512835502625 - trainLoss: 0.4050997793674469\n",
      "cnt: 0 - valLoss: 0.4277847409248352 - trainLoss: 0.40509840846061707\n",
      "cnt: 0 - valLoss: 0.42778435349464417 - trainLoss: 0.40509703755378723\n",
      "cnt: 0 - valLoss: 0.42778393626213074 - trainLoss: 0.4050956964492798\n",
      "cnt: 0 - valLoss: 0.4277834892272949 - trainLoss: 0.4050942659378052\n",
      "cnt: 0 - valLoss: 0.4277831017971039 - trainLoss: 0.40509289503097534\n",
      "cnt: 0 - valLoss: 0.42778271436691284 - trainLoss: 0.4050915241241455\n",
      "cnt: 0 - valLoss: 0.4277823269367218 - trainLoss: 0.4050901532173157\n",
      "cnt: 0 - valLoss: 0.427781879901886 - trainLoss: 0.40508875250816345\n",
      "cnt: 0 - valLoss: 0.42778149247169495 - trainLoss: 0.405087411403656\n",
      "cnt: 0 - valLoss: 0.4277811050415039 - trainLoss: 0.4050860106945038\n",
      "cnt: 0 - valLoss: 0.4277806580066681 - trainLoss: 0.40508466958999634\n",
      "cnt: 0 - valLoss: 0.42778024077415466 - trainLoss: 0.4050832688808441\n",
      "cnt: 0 - valLoss: 0.4277798533439636 - trainLoss: 0.4050818681716919\n",
      "cnt: 0 - valLoss: 0.4277794659137726 - trainLoss: 0.40508049726486206\n",
      "cnt: 0 - valLoss: 0.42777904868125916 - trainLoss: 0.4050791263580322\n",
      "cnt: 0 - valLoss: 0.4277786612510681 - trainLoss: 0.4050777554512024\n",
      "cnt: 0 - valLoss: 0.4277782738208771 - trainLoss: 0.40507638454437256\n",
      "cnt: 0 - valLoss: 0.42777782678604126 - trainLoss: 0.40507495403289795\n",
      "cnt: 0 - valLoss: 0.4277774393558502 - trainLoss: 0.4050736427307129\n",
      "cnt: 0 - valLoss: 0.4277770519256592 - trainLoss: 0.40507227182388306\n",
      "cnt: 0 - valLoss: 0.42777663469314575 - trainLoss: 0.40507087111473083\n",
      "cnt: 0 - valLoss: 0.4277762174606323 - trainLoss: 0.405069500207901\n",
      "cnt: 0 - valLoss: 0.4277757704257965 - trainLoss: 0.40506812930107117\n",
      "cnt: 0 - valLoss: 0.42777544260025024 - trainLoss: 0.40506669878959656\n",
      "cnt: 0 - valLoss: 0.42777499556541443 - trainLoss: 0.4050653278827667\n",
      "cnt: 0 - valLoss: 0.427774578332901 - trainLoss: 0.4050639569759369\n",
      "cnt: 0 - valLoss: 0.42777419090270996 - trainLoss: 0.40506258606910706\n",
      "cnt: 0 - valLoss: 0.4277738034725189 - trainLoss: 0.4050612151622772\n",
      "cnt: 0 - valLoss: 0.4277733862400055 - trainLoss: 0.4050598442554474\n",
      "cnt: 0 - valLoss: 0.42777299880981445 - trainLoss: 0.40505844354629517\n",
      "cnt: 0 - valLoss: 0.4277726113796234 - trainLoss: 0.4050571322441101\n",
      "cnt: 0 - valLoss: 0.4277721643447876 - trainLoss: 0.4050557017326355\n",
      "cnt: 0 - valLoss: 0.42777183651924133 - trainLoss: 0.40505433082580566\n",
      "cnt: 0 - valLoss: 0.42777135968208313 - trainLoss: 0.40505293011665344\n",
      "cnt: 0 - valLoss: 0.4277709722518921 - trainLoss: 0.4050515592098236\n",
      "cnt: 0 - valLoss: 0.42777058482170105 - trainLoss: 0.4050501883029938\n",
      "cnt: 0 - valLoss: 0.42777019739151 - trainLoss: 0.40504881739616394\n",
      "cnt: 0 - valLoss: 0.4277697801589966 - trainLoss: 0.4050474464893341\n",
      "cnt: 0 - valLoss: 0.42776939272880554 - trainLoss: 0.4050460755825043\n",
      "cnt: 0 - valLoss: 0.4277689754962921 - trainLoss: 0.40504470467567444\n",
      "cnt: 0 - valLoss: 0.4277685880661011 - trainLoss: 0.4050433039665222\n",
      "cnt: 0 - valLoss: 0.42776814103126526 - trainLoss: 0.40504196286201477\n",
      "cnt: 0 - valLoss: 0.42776772379875183 - trainLoss: 0.40504056215286255\n",
      "cnt: 0 - valLoss: 0.4277673661708832 - trainLoss: 0.4050391912460327\n",
      "cnt: 0 - valLoss: 0.42776694893836975 - trainLoss: 0.4050378203392029\n",
      "cnt: 0 - valLoss: 0.4277665615081787 - trainLoss: 0.40503644943237305\n",
      "cnt: 0 - valLoss: 0.42776617407798767 - trainLoss: 0.40503501892089844\n",
      "cnt: 0 - valLoss: 0.42776575684547424 - trainLoss: 0.4050336480140686\n",
      "cnt: 0 - valLoss: 0.4277653694152832 - trainLoss: 0.40503230690956116\n",
      "cnt: 0 - valLoss: 0.42776498198509216 - trainLoss: 0.4050309360027313\n",
      "cnt: 0 - valLoss: 0.42776453495025635 - trainLoss: 0.4050295650959015\n",
      "cnt: 0 - valLoss: 0.4277642071247101 - trainLoss: 0.40502819418907166\n",
      "cnt: 0 - valLoss: 0.42776378989219666 - trainLoss: 0.40502676367759705\n",
      "cnt: 0 - valLoss: 0.42776334285736084 - trainLoss: 0.4050253927707672\n",
      "cnt: 0 - valLoss: 0.4277630150318146 - trainLoss: 0.4050240218639374\n",
      "cnt: 0 - valLoss: 0.4277626574039459 - trainLoss: 0.40502265095710754\n",
      "cnt: 0 - valLoss: 0.42776232957839966 - trainLoss: 0.4050212800502777\n",
      "cnt: 0 - valLoss: 0.4277620315551758 - trainLoss: 0.4050199091434479\n",
      "cnt: 0 - valLoss: 0.42776167392730713 - trainLoss: 0.4050185978412628\n",
      "cnt: 0 - valLoss: 0.42776134610176086 - trainLoss: 0.4050171971321106\n",
      "cnt: 0 - valLoss: 0.427761048078537 - trainLoss: 0.40501585602760315\n",
      "cnt: 0 - valLoss: 0.42776066064834595 - trainLoss: 0.4050144851207733\n",
      "cnt: 0 - valLoss: 0.4277602732181549 - trainLoss: 0.4050131142139435\n",
      "cnt: 0 - valLoss: 0.42775997519493103 - trainLoss: 0.4050118029117584\n",
      "cnt: 0 - valLoss: 0.42775967717170715 - trainLoss: 0.4050103724002838\n",
      "cnt: 0 - valLoss: 0.4277592897415161 - trainLoss: 0.405009001493454\n",
      "cnt: 0 - valLoss: 0.42775893211364746 - trainLoss: 0.4050076901912689\n",
      "cnt: 0 - valLoss: 0.4277586340904236 - trainLoss: 0.4050062596797943\n",
      "cnt: 0 - valLoss: 0.42775824666023254 - trainLoss: 0.40500494837760925\n",
      "cnt: 0 - valLoss: 0.42775794863700867 - trainLoss: 0.4050035774707794\n",
      "cnt: 0 - valLoss: 0.4277576208114624 - trainLoss: 0.4050022065639496\n",
      "cnt: 0 - valLoss: 0.42775726318359375 - trainLoss: 0.40500083565711975\n",
      "cnt: 0 - valLoss: 0.4277568757534027 - trainLoss: 0.4049994945526123\n",
      "cnt: 0 - valLoss: 0.42775657773017883 - trainLoss: 0.40499812364578247\n",
      "cnt: 0 - valLoss: 0.4277562201023102 - trainLoss: 0.40499675273895264\n",
      "cnt: 0 - valLoss: 0.42775586247444153 - trainLoss: 0.4049954116344452\n",
      "cnt: 0 - valLoss: 0.4277555048465729 - trainLoss: 0.40499404072761536\n",
      "cnt: 0 - valLoss: 0.427755206823349 - trainLoss: 0.4049926698207855\n",
      "cnt: 0 - valLoss: 0.42775484919548035 - trainLoss: 0.4049912989139557\n",
      "cnt: 0 - valLoss: 0.4277544319629669 - trainLoss: 0.40498995780944824\n",
      "cnt: 0 - valLoss: 0.42775413393974304 - trainLoss: 0.4049885869026184\n",
      "cnt: 0 - valLoss: 0.4277538061141968 - trainLoss: 0.4049872159957886\n",
      "cnt: 0 - valLoss: 0.4277534782886505 - trainLoss: 0.40498584508895874\n",
      "cnt: 0 - valLoss: 0.4277530908584595 - trainLoss: 0.4049845337867737\n",
      "cnt: 0 - valLoss: 0.4277527630329132 - trainLoss: 0.40498316287994385\n",
      "cnt: 0 - valLoss: 0.42775240540504456 - trainLoss: 0.404981791973114\n",
      "cnt: 0 - valLoss: 0.4277520775794983 - trainLoss: 0.4049804210662842\n",
      "cnt: 0 - valLoss: 0.42775169014930725 - trainLoss: 0.40497905015945435\n",
      "cnt: 0 - valLoss: 0.4277513921260834 - trainLoss: 0.4049776792526245\n",
      "cnt: 0 - valLoss: 0.4277510344982147 - trainLoss: 0.4049763083457947\n",
      "cnt: 0 - valLoss: 0.42775067687034607 - trainLoss: 0.40497496724128723\n",
      "cnt: 0 - valLoss: 0.4277503192424774 - trainLoss: 0.4049736261367798\n",
      "cnt: 0 - valLoss: 0.42774999141693115 - trainLoss: 0.40497225522994995\n",
      "cnt: 0 - valLoss: 0.4277496337890625 - trainLoss: 0.4049709141254425\n",
      "cnt: 0 - valLoss: 0.42774927616119385 - trainLoss: 0.40496954321861267\n",
      "cnt: 0 - valLoss: 0.4277489483356476 - trainLoss: 0.40496817231178284\n",
      "cnt: 0 - valLoss: 0.42774859070777893 - trainLoss: 0.404966801404953\n",
      "cnt: 0 - valLoss: 0.42774826288223267 - trainLoss: 0.40496543049812317\n",
      "cnt: 0 - valLoss: 0.427747905254364 - trainLoss: 0.40496405959129333\n",
      "cnt: 0 - valLoss: 0.42774757742881775 - trainLoss: 0.4049626886844635\n",
      "cnt: 0 - valLoss: 0.4277472198009491 - trainLoss: 0.40496131777763367\n",
      "cnt: 0 - valLoss: 0.42774686217308044 - trainLoss: 0.4049600064754486\n",
      "cnt: 0 - valLoss: 0.4277465045452118 - trainLoss: 0.4049586355686188\n",
      "cnt: 0 - valLoss: 0.4277461767196655 - trainLoss: 0.40495726466178894\n",
      "cnt: 0 - valLoss: 0.4277458190917969 - trainLoss: 0.4049558937549591\n",
      "cnt: 0 - valLoss: 0.4277454614639282 - trainLoss: 0.40495455265045166\n",
      "cnt: 0 - valLoss: 0.42774510383605957 - trainLoss: 0.4049531817436218\n",
      "cnt: 0 - valLoss: 0.4277447760105133 - trainLoss: 0.4049518406391144\n",
      "cnt: 0 - valLoss: 0.42774441838264465 - trainLoss: 0.40495046973228455\n",
      "cnt: 0 - valLoss: 0.427744060754776 - trainLoss: 0.4049491286277771\n",
      "cnt: 0 - valLoss: 0.42774373292922974 - trainLoss: 0.40494775772094727\n",
      "cnt: 0 - valLoss: 0.4277433753013611 - trainLoss: 0.40494638681411743\n",
      "cnt: 0 - valLoss: 0.4277430474758148 - trainLoss: 0.4049450755119324\n",
      "cnt: 0 - valLoss: 0.42774268984794617 - trainLoss: 0.40494370460510254\n",
      "cnt: 0 - valLoss: 0.4277423620223999 - trainLoss: 0.4049423336982727\n",
      "cnt: 0 - valLoss: 0.42774200439453125 - trainLoss: 0.40494096279144287\n",
      "cnt: 0 - valLoss: 0.4277416169643402 - trainLoss: 0.40493959188461304\n",
      "cnt: 0 - valLoss: 0.42774131894111633 - trainLoss: 0.4049382507801056\n",
      "cnt: 0 - valLoss: 0.4277409315109253 - trainLoss: 0.40493687987327576\n",
      "cnt: 0 - valLoss: 0.42774060368537903 - trainLoss: 0.4049355089664459\n",
      "cnt: 0 - valLoss: 0.427740216255188 - trainLoss: 0.40493419766426086\n",
      "cnt: 0 - valLoss: 0.4277399182319641 - trainLoss: 0.40493282675743103\n",
      "cnt: 0 - valLoss: 0.42773953080177307 - trainLoss: 0.4049314558506012\n",
      "cnt: 0 - valLoss: 0.4277391731739044 - trainLoss: 0.40493008494377136\n",
      "cnt: 0 - valLoss: 0.42773884534835815 - trainLoss: 0.40492871403694153\n",
      "cnt: 0 - valLoss: 0.4277384579181671 - trainLoss: 0.4049273431301117\n",
      "cnt: 0 - valLoss: 0.42773815989494324 - trainLoss: 0.40492597222328186\n",
      "cnt: 0 - valLoss: 0.4277377724647522 - trainLoss: 0.404924601316452\n",
      "cnt: 0 - valLoss: 0.42773738503456116 - trainLoss: 0.40492329001426697\n",
      "cnt: 0 - valLoss: 0.4277370870113373 - trainLoss: 0.4049219489097595\n",
      "cnt: 0 - valLoss: 0.42773669958114624 - trainLoss: 0.4049205780029297\n",
      "cnt: 0 - valLoss: 0.4277363419532776 - trainLoss: 0.40491920709609985\n",
      "cnt: 0 - valLoss: 0.4277360141277313 - trainLoss: 0.40491783618927\n",
      "cnt: 0 - valLoss: 0.42773565649986267 - trainLoss: 0.4049164652824402\n",
      "cnt: 0 - valLoss: 0.4277353286743164 - trainLoss: 0.40491509437561035\n",
      "cnt: 0 - valLoss: 0.42773497104644775 - trainLoss: 0.4049137234687805\n",
      "cnt: 0 - valLoss: 0.4277345538139343 - trainLoss: 0.40491241216659546\n",
      "cnt: 0 - valLoss: 0.42773425579071045 - trainLoss: 0.4049110412597656\n",
      "cnt: 0 - valLoss: 0.4277338981628418 - trainLoss: 0.4049097001552582\n",
      "cnt: 0 - valLoss: 0.42773351073265076 - trainLoss: 0.40490832924842834\n",
      "cnt: 0 - valLoss: 0.4277331829071045 - trainLoss: 0.4049069583415985\n",
      "cnt: 0 - valLoss: 0.42773282527923584 - trainLoss: 0.4049055874347687\n",
      "cnt: 0 - valLoss: 0.4277324974536896 - trainLoss: 0.40490421652793884\n",
      "cnt: 0 - valLoss: 0.4277321398258209 - trainLoss: 0.4049028754234314\n",
      "cnt: 0 - valLoss: 0.4277317523956299 - trainLoss: 0.40490150451660156\n",
      "cnt: 0 - valLoss: 0.4277314245700836 - trainLoss: 0.4049001634120941\n",
      "cnt: 0 - valLoss: 0.4277310371398926 - trainLoss: 0.4048987627029419\n",
      "cnt: 0 - valLoss: 0.4277307391166687 - trainLoss: 0.4048974812030792\n",
      "cnt: 0 - valLoss: 0.42773035168647766 - trainLoss: 0.4048961102962494\n",
      "cnt: 0 - valLoss: 0.427729994058609 - trainLoss: 0.40489473938941956\n",
      "cnt: 0 - valLoss: 0.42772960662841797 - trainLoss: 0.4048933684825897\n",
      "cnt: 0 - valLoss: 0.4277292788028717 - trainLoss: 0.4048919975757599\n",
      "cnt: 0 - valLoss: 0.42772892117500305 - trainLoss: 0.40489062666893005\n",
      "cnt: 0 - valLoss: 0.4277285933494568 - trainLoss: 0.4048892557621002\n",
      "cnt: 0 - valLoss: 0.42772820591926575 - trainLoss: 0.4048879146575928\n",
      "cnt: 0 - valLoss: 0.4277278482913971 - trainLoss: 0.40488654375076294\n",
      "cnt: 0 - valLoss: 0.42772746086120605 - trainLoss: 0.4048852324485779\n",
      "cnt: 0 - valLoss: 0.4277271330356598 - trainLoss: 0.40488386154174805\n",
      "cnt: 0 - valLoss: 0.42772674560546875 - trainLoss: 0.4048824906349182\n",
      "cnt: 0 - valLoss: 0.4277263879776001 - trainLoss: 0.40488114953041077\n",
      "cnt: 0 - valLoss: 0.42772606015205383 - trainLoss: 0.40487974882125854\n",
      "cnt: 0 - valLoss: 0.4277257025241852 - trainLoss: 0.4048784077167511\n",
      "cnt: 0 - valLoss: 0.42772534489631653 - trainLoss: 0.40487703680992126\n",
      "cnt: 0 - valLoss: 0.4277249872684479 - trainLoss: 0.4048757255077362\n",
      "cnt: 0 - valLoss: 0.42772459983825684 - trainLoss: 0.40487435460090637\n",
      "cnt: 0 - valLoss: 0.42772427201271057 - trainLoss: 0.40487298369407654\n",
      "cnt: 0 - valLoss: 0.42772388458251953 - trainLoss: 0.4048716127872467\n",
      "cnt: 0 - valLoss: 0.4277235269546509 - trainLoss: 0.40487024188041687\n",
      "cnt: 0 - valLoss: 0.42772313952445984 - trainLoss: 0.4048689305782318\n",
      "cnt: 0 - valLoss: 0.4277228116989136 - trainLoss: 0.404867559671402\n",
      "cnt: 0 - valLoss: 0.4277224540710449 - trainLoss: 0.40486621856689453\n",
      "cnt: 0 - valLoss: 0.42772212624549866 - trainLoss: 0.4048648476600647\n",
      "cnt: 0 - valLoss: 0.4277217388153076 - trainLoss: 0.40486347675323486\n",
      "cnt: 0 - valLoss: 0.4277213513851166 - trainLoss: 0.40486210584640503\n",
      "cnt: 0 - valLoss: 0.4277209937572479 - trainLoss: 0.4048607349395752\n",
      "cnt: 0 - valLoss: 0.4277206063270569 - trainLoss: 0.40485942363739014\n",
      "cnt: 0 - valLoss: 0.4277202785015106 - trainLoss: 0.4048580825328827\n",
      "cnt: 0 - valLoss: 0.4277198910713196 - trainLoss: 0.40485671162605286\n",
      "cnt: 0 - valLoss: 0.4277195334434509 - trainLoss: 0.40485531091690063\n",
      "cnt: 0 - valLoss: 0.4277191460132599 - trainLoss: 0.4048539698123932\n",
      "cnt: 0 - valLoss: 0.4277188181877136 - trainLoss: 0.40485259890556335\n",
      "cnt: 0 - valLoss: 0.4277184307575226 - trainLoss: 0.4048512279987335\n",
      "cnt: 0 - valLoss: 0.42771804332733154 - trainLoss: 0.40484991669654846\n",
      "cnt: 0 - valLoss: 0.4277177155017853 - trainLoss: 0.40484854578971863\n",
      "cnt: 0 - valLoss: 0.42771732807159424 - trainLoss: 0.4048472046852112\n",
      "cnt: 0 - valLoss: 0.4277169704437256 - trainLoss: 0.40484586358070374\n",
      "cnt: 0 - valLoss: 0.42771655321121216 - trainLoss: 0.4048444926738739\n",
      "cnt: 0 - valLoss: 0.4277161955833435 - trainLoss: 0.40484312176704407\n",
      "cnt: 0 - valLoss: 0.42771586775779724 - trainLoss: 0.40484175086021423\n",
      "cnt: 0 - valLoss: 0.4277154803276062 - trainLoss: 0.4048403799533844\n",
      "cnt: 0 - valLoss: 0.42771509289741516 - trainLoss: 0.40483903884887695\n",
      "cnt: 0 - valLoss: 0.4277147054672241 - trainLoss: 0.4048376679420471\n",
      "cnt: 0 - valLoss: 0.4277143180370331 - trainLoss: 0.40483635663986206\n",
      "cnt: 0 - valLoss: 0.42771396040916443 - trainLoss: 0.4048349857330322\n",
      "cnt: 0 - valLoss: 0.4277135729789734 - trainLoss: 0.4048336148262024\n",
      "cnt: 0 - valLoss: 0.4277132451534271 - trainLoss: 0.40483224391937256\n",
      "cnt: 0 - valLoss: 0.4277128577232361 - trainLoss: 0.4048308730125427\n",
      "cnt: 0 - valLoss: 0.42771247029304504 - trainLoss: 0.4048295319080353\n",
      "cnt: 0 - valLoss: 0.4277121424674988 - trainLoss: 0.4048282206058502\n",
      "cnt: 0 - valLoss: 0.42771175503730774 - trainLoss: 0.4048268496990204\n",
      "cnt: 0 - valLoss: 0.4277113676071167 - trainLoss: 0.40482547879219055\n",
      "cnt: 0 - valLoss: 0.42771098017692566 - trainLoss: 0.4048241078853607\n",
      "cnt: 0 - valLoss: 0.427710622549057 - trainLoss: 0.40482276678085327\n",
      "cnt: 0 - valLoss: 0.42771023511886597 - trainLoss: 0.40482139587402344\n",
      "cnt: 0 - valLoss: 0.4277098476886749 - trainLoss: 0.4048200249671936\n",
      "cnt: 0 - valLoss: 0.42770951986312866 - trainLoss: 0.40481871366500854\n",
      "cnt: 0 - valLoss: 0.4277091324329376 - trainLoss: 0.4048173427581787\n",
      "cnt: 0 - valLoss: 0.4277087450027466 - trainLoss: 0.4048159718513489\n",
      "cnt: 0 - valLoss: 0.42770838737487793 - trainLoss: 0.40481460094451904\n",
      "cnt: 0 - valLoss: 0.4277080297470093 - trainLoss: 0.404813289642334\n",
      "cnt: 0 - valLoss: 0.42770764231681824 - trainLoss: 0.40481191873550415\n",
      "cnt: 0 - valLoss: 0.4277072846889496 - trainLoss: 0.4048105776309967\n",
      "cnt: 0 - valLoss: 0.4277069568634033 - trainLoss: 0.40480920672416687\n",
      "cnt: 0 - valLoss: 0.4277065098285675 - trainLoss: 0.40480783581733704\n",
      "cnt: 0 - valLoss: 0.42770618200302124 - trainLoss: 0.404806524515152\n",
      "cnt: 0 - valLoss: 0.4277057945728302 - trainLoss: 0.40480515360832214\n",
      "cnt: 0 - valLoss: 0.42770540714263916 - trainLoss: 0.4048037827014923\n",
      "cnt: 0 - valLoss: 0.4277050495147705 - trainLoss: 0.40480244159698486\n",
      "cnt: 0 - valLoss: 0.42770472168922424 - trainLoss: 0.40480107069015503\n",
      "cnt: 0 - valLoss: 0.4277042746543884 - trainLoss: 0.4047996997833252\n",
      "cnt: 0 - valLoss: 0.42770394682884216 - trainLoss: 0.40479832887649536\n",
      "cnt: 0 - valLoss: 0.4277035593986511 - trainLoss: 0.4047970175743103\n",
      "cnt: 0 - valLoss: 0.4277031719684601 - trainLoss: 0.40479564666748047\n",
      "cnt: 0 - valLoss: 0.42770278453826904 - trainLoss: 0.40479427576065063\n",
      "cnt: 0 - valLoss: 0.4277024567127228 - trainLoss: 0.4047929346561432\n",
      "cnt: 0 - valLoss: 0.42770206928253174 - trainLoss: 0.40479156374931335\n",
      "cnt: 0 - valLoss: 0.4277017116546631 - trainLoss: 0.4047902524471283\n",
      "cnt: 0 - valLoss: 0.42770132422447205 - trainLoss: 0.4047888517379761\n",
      "cnt: 0 - valLoss: 0.427700936794281 - trainLoss: 0.40478751063346863\n",
      "cnt: 0 - valLoss: 0.42770054936408997 - trainLoss: 0.4047861695289612\n",
      "cnt: 0 - valLoss: 0.4277001619338989 - trainLoss: 0.40478482842445374\n",
      "cnt: 0 - valLoss: 0.42769983410835266 - trainLoss: 0.4047834575176239\n",
      "cnt: 0 - valLoss: 0.4276994466781616 - trainLoss: 0.40478211641311646\n",
      "cnt: 0 - valLoss: 0.42769908905029297 - trainLoss: 0.4047807455062866\n",
      "cnt: 0 - valLoss: 0.42769870162010193 - trainLoss: 0.4047793745994568\n",
      "cnt: 0 - valLoss: 0.4276983141899109 - trainLoss: 0.40477806329727173\n",
      "cnt: 0 - valLoss: 0.4276979863643646 - trainLoss: 0.4047766923904419\n",
      "cnt: 0 - valLoss: 0.4276975989341736 - trainLoss: 0.40477532148361206\n",
      "cnt: 0 - valLoss: 0.42769721150398254 - trainLoss: 0.4047739803791046\n",
      "cnt: 0 - valLoss: 0.4276968836784363 - trainLoss: 0.4047726094722748\n",
      "cnt: 0 - valLoss: 0.42769649624824524 - trainLoss: 0.40477123856544495\n",
      "cnt: 0 - valLoss: 0.4276961386203766 - trainLoss: 0.4047699272632599\n",
      "cnt: 0 - valLoss: 0.42769575119018555 - trainLoss: 0.40476855635643005\n",
      "cnt: 0 - valLoss: 0.4276953637599945 - trainLoss: 0.4047671854496002\n",
      "cnt: 0 - valLoss: 0.42769497632980347 - trainLoss: 0.4047658443450928\n",
      "cnt: 0 - valLoss: 0.4276946485042572 - trainLoss: 0.40476447343826294\n",
      "cnt: 0 - valLoss: 0.42769426107406616 - trainLoss: 0.4047631621360779\n",
      "cnt: 0 - valLoss: 0.4276938736438751 - trainLoss: 0.40476179122924805\n",
      "cnt: 0 - valLoss: 0.42769351601600647 - trainLoss: 0.4047604203224182\n",
      "cnt: 0 - valLoss: 0.42769312858581543 - trainLoss: 0.40475907921791077\n",
      "cnt: 0 - valLoss: 0.4276927709579468 - trainLoss: 0.4047577679157257\n",
      "cnt: 0 - valLoss: 0.4276924133300781 - trainLoss: 0.4047563970088959\n",
      "cnt: 0 - valLoss: 0.4276920258998871 - trainLoss: 0.40475502610206604\n",
      "cnt: 0 - valLoss: 0.4276916980743408 - trainLoss: 0.4047536849975586\n",
      "cnt: 0 - valLoss: 0.427691251039505 - trainLoss: 0.40475228428840637\n",
      "cnt: 0 - valLoss: 0.42769092321395874 - trainLoss: 0.4047509431838989\n",
      "cnt: 0 - valLoss: 0.4276905655860901 - trainLoss: 0.40474963188171387\n",
      "cnt: 0 - valLoss: 0.42769017815589905 - trainLoss: 0.40474826097488403\n",
      "cnt: 0 - valLoss: 0.427689790725708 - trainLoss: 0.4047468900680542\n",
      "cnt: 0 - valLoss: 0.42768940329551697 - trainLoss: 0.40474551916122437\n",
      "cnt: 0 - valLoss: 0.4276890158653259 - trainLoss: 0.4047442078590393\n",
      "cnt: 0 - valLoss: 0.42768868803977966 - trainLoss: 0.40474286675453186\n",
      "cnt: 0 - valLoss: 0.4276883006095886 - trainLoss: 0.404741495847702\n",
      "cnt: 0 - valLoss: 0.4276879131793976 - trainLoss: 0.4047401249408722\n",
      "cnt: 0 - valLoss: 0.42768755555152893 - trainLoss: 0.40473881363868713\n",
      "cnt: 0 - valLoss: 0.4276871979236603 - trainLoss: 0.4047374427318573\n",
      "cnt: 0 - valLoss: 0.42768681049346924 - trainLoss: 0.40473610162734985\n",
      "cnt: 0 - valLoss: 0.4276864528656006 - trainLoss: 0.40473473072052\n",
      "cnt: 0 - valLoss: 0.42768603563308716 - trainLoss: 0.4047333598136902\n",
      "cnt: 0 - valLoss: 0.4276857376098633 - trainLoss: 0.40473198890686035\n",
      "cnt: 0 - valLoss: 0.42768535017967224 - trainLoss: 0.4047306776046753\n",
      "cnt: 0 - valLoss: 0.4276849627494812 - trainLoss: 0.40472933650016785\n",
      "cnt: 0 - valLoss: 0.42768457531929016 - trainLoss: 0.404727965593338\n",
      "cnt: 0 - valLoss: 0.4276842176914215 - trainLoss: 0.4047265946865082\n",
      "cnt: 0 - valLoss: 0.42768383026123047 - trainLoss: 0.40472522377967834\n",
      "cnt: 0 - valLoss: 0.42768344283103943 - trainLoss: 0.4047239124774933\n",
      "cnt: 0 - valLoss: 0.42768311500549316 - trainLoss: 0.40472254157066345\n",
      "cnt: 0 - valLoss: 0.4276827275753021 - trainLoss: 0.404721200466156\n",
      "cnt: 0 - valLoss: 0.4276823401451111 - trainLoss: 0.40471982955932617\n",
      "cnt: 0 - valLoss: 0.4276820123195648 - trainLoss: 0.40471845865249634\n",
      "cnt: 0 - valLoss: 0.4276816248893738 - trainLoss: 0.4047171473503113\n",
      "cnt: 0 - valLoss: 0.42768123745918274 - trainLoss: 0.40471577644348145\n",
      "cnt: 0 - valLoss: 0.4276808500289917 - trainLoss: 0.404714435338974\n",
      "cnt: 0 - valLoss: 0.42768046259880066 - trainLoss: 0.40471306443214417\n",
      "cnt: 0 - valLoss: 0.427680104970932 - trainLoss: 0.4047117531299591\n",
      "cnt: 0 - valLoss: 0.4276796877384186 - trainLoss: 0.4047103822231293\n",
      "cnt: 0 - valLoss: 0.4276793301105499 - trainLoss: 0.4047090411186218\n",
      "cnt: 0 - valLoss: 0.4276789426803589 - trainLoss: 0.4047077000141144\n",
      "cnt: 0 - valLoss: 0.4276786148548126 - trainLoss: 0.40470632910728455\n",
      "cnt: 0 - valLoss: 0.4276782274246216 - trainLoss: 0.4047049880027771\n",
      "cnt: 0 - valLoss: 0.42767783999443054 - trainLoss: 0.40470367670059204\n",
      "cnt: 0 - valLoss: 0.4276774525642395 - trainLoss: 0.4047023057937622\n",
      "cnt: 0 - valLoss: 0.42767712473869324 - trainLoss: 0.4047009348869324\n",
      "cnt: 0 - valLoss: 0.4276767373085022 - trainLoss: 0.40469956398010254\n",
      "cnt: 0 - valLoss: 0.42767634987831116 - trainLoss: 0.40469828248023987\n",
      "cnt: 0 - valLoss: 0.4276759624481201 - trainLoss: 0.40469685196876526\n",
      "cnt: 0 - valLoss: 0.4276755750179291 - trainLoss: 0.4046955406665802\n",
      "cnt: 0 - valLoss: 0.4276752173900604 - trainLoss: 0.40469416975975037\n",
      "cnt: 0 - valLoss: 0.427674800157547 - trainLoss: 0.4046928286552429\n",
      "cnt: 0 - valLoss: 0.42767444252967834 - trainLoss: 0.4046914577484131\n",
      "cnt: 0 - valLoss: 0.4276740550994873 - trainLoss: 0.404690146446228\n",
      "cnt: 0 - valLoss: 0.42767369747161865 - trainLoss: 0.4046887755393982\n",
      "cnt: 0 - valLoss: 0.4276733100414276 - trainLoss: 0.40468743443489075\n",
      "cnt: 0 - valLoss: 0.42767295241355896 - trainLoss: 0.4046860635280609\n",
      "cnt: 0 - valLoss: 0.4276725649833679 - trainLoss: 0.4046846926212311\n",
      "cnt: 0 - valLoss: 0.4276721775531769 - trainLoss: 0.404683381319046\n",
      "cnt: 0 - valLoss: 0.42767179012298584 - trainLoss: 0.4046820104122162\n",
      "cnt: 0 - valLoss: 0.4276714622974396 - trainLoss: 0.40468066930770874\n",
      "cnt: 0 - valLoss: 0.42767101526260376 - trainLoss: 0.4046792685985565\n",
      "cnt: 0 - valLoss: 0.4276706874370575 - trainLoss: 0.40467798709869385\n",
      "cnt: 0 - valLoss: 0.42767030000686646 - trainLoss: 0.404676616191864\n",
      "cnt: 0 - valLoss: 0.4276699125766754 - trainLoss: 0.40467527508735657\n",
      "cnt: 0 - valLoss: 0.4276695251464844 - trainLoss: 0.40467390418052673\n",
      "cnt: 0 - valLoss: 0.4276691675186157 - trainLoss: 0.4046725928783417\n",
      "cnt: 0 - valLoss: 0.42766880989074707 - trainLoss: 0.40467122197151184\n",
      "cnt: 0 - valLoss: 0.4276684522628784 - trainLoss: 0.404669851064682\n",
      "cnt: 0 - valLoss: 0.427668035030365 - trainLoss: 0.40466850996017456\n",
      "cnt: 0 - valLoss: 0.42766764760017395 - trainLoss: 0.4046671390533447\n",
      "cnt: 0 - valLoss: 0.4276672601699829 - trainLoss: 0.40466582775115967\n",
      "cnt: 0 - valLoss: 0.4276668131351471 - trainLoss: 0.40466445684432983\n",
      "cnt: 0 - valLoss: 0.42766642570495605 - trainLoss: 0.4046631157398224\n",
      "cnt: 0 - valLoss: 0.4276660084724426 - trainLoss: 0.40466180443763733\n",
      "cnt: 0 - valLoss: 0.4276656210422516 - trainLoss: 0.4046604335308075\n",
      "cnt: 0 - valLoss: 0.42766520380973816 - trainLoss: 0.40465906262397766\n",
      "cnt: 0 - valLoss: 0.4276648461818695 - trainLoss: 0.4046577215194702\n",
      "cnt: 0 - valLoss: 0.4276643693447113 - trainLoss: 0.40465641021728516\n",
      "cnt: 0 - valLoss: 0.42766398191452026 - trainLoss: 0.4046550393104553\n",
      "cnt: 0 - valLoss: 0.42766356468200684 - trainLoss: 0.4046536684036255\n",
      "cnt: 0 - valLoss: 0.42766323685646057 - trainLoss: 0.40465235710144043\n",
      "cnt: 0 - valLoss: 0.42766278982162476 - trainLoss: 0.4046509861946106\n",
      "cnt: 0 - valLoss: 0.4276624023914337 - trainLoss: 0.40464964509010315\n",
      "cnt: 0 - valLoss: 0.4276620149612427 - trainLoss: 0.4046482741832733\n",
      "cnt: 0 - valLoss: 0.42766159772872925 - trainLoss: 0.40464696288108826\n",
      "cnt: 0 - valLoss: 0.4276612102985382 - trainLoss: 0.4046455919742584\n",
      "cnt: 0 - valLoss: 0.42766082286834717 - trainLoss: 0.404644250869751\n",
      "cnt: 0 - valLoss: 0.42766040563583374 - trainLoss: 0.40464287996292114\n",
      "cnt: 0 - valLoss: 0.4276599586009979 - trainLoss: 0.4046415686607361\n",
      "cnt: 0 - valLoss: 0.4276595711708069 - trainLoss: 0.40464022755622864\n",
      "cnt: 0 - valLoss: 0.42765918374061584 - trainLoss: 0.4046388566493988\n",
      "cnt: 0 - valLoss: 0.4276587665081024 - trainLoss: 0.40463748574256897\n",
      "cnt: 0 - valLoss: 0.427658349275589 - trainLoss: 0.4046361744403839\n",
      "cnt: 0 - valLoss: 0.42765796184539795 - trainLoss: 0.40463483333587646\n",
      "cnt: 0 - valLoss: 0.4276575744152069 - trainLoss: 0.40463346242904663\n",
      "cnt: 0 - valLoss: 0.42765718698501587 - trainLoss: 0.4046321511268616\n",
      "cnt: 0 - valLoss: 0.42765679955482483 - trainLoss: 0.4046308100223541\n",
      "cnt: 0 - valLoss: 0.427656352519989 - trainLoss: 0.4046294093132019\n",
      "cnt: 0 - valLoss: 0.4276559352874756 - trainLoss: 0.40462806820869446\n",
      "cnt: 0 - valLoss: 0.42765554785728455 - trainLoss: 0.4046267569065094\n",
      "cnt: 0 - valLoss: 0.4276551604270935 - trainLoss: 0.40462538599967957\n",
      "cnt: 0 - valLoss: 0.4276547431945801 - trainLoss: 0.4046240448951721\n",
      "cnt: 0 - valLoss: 0.42765435576438904 - trainLoss: 0.40462273359298706\n",
      "cnt: 0 - valLoss: 0.427653968334198 - trainLoss: 0.4046213626861572\n",
      "cnt: 0 - valLoss: 0.42765355110168457 - trainLoss: 0.4046200215816498\n",
      "cnt: 0 - valLoss: 0.42765310406684875 - trainLoss: 0.40461865067481995\n",
      "cnt: 0 - valLoss: 0.4276527166366577 - trainLoss: 0.4046173393726349\n",
      "cnt: 0 - valLoss: 0.4276523292064667 - trainLoss: 0.40461596846580505\n",
      "cnt: 0 - valLoss: 0.42765191197395325 - trainLoss: 0.4046145975589752\n",
      "cnt: 0 - valLoss: 0.4276515245437622 - trainLoss: 0.4046132564544678\n",
      "cnt: 0 - valLoss: 0.42765113711357117 - trainLoss: 0.4046119451522827\n",
      "cnt: 0 - valLoss: 0.42765071988105774 - trainLoss: 0.4046105742454529\n",
      "cnt: 0 - valLoss: 0.4276503622531891 - trainLoss: 0.40460923314094543\n",
      "cnt: 0 - valLoss: 0.42764994502067566 - trainLoss: 0.4046078622341156\n",
      "cnt: 0 - valLoss: 0.42764949798583984 - trainLoss: 0.40460655093193054\n",
      "cnt: 0 - valLoss: 0.4276491105556488 - trainLoss: 0.4046052098274231\n",
      "cnt: 0 - valLoss: 0.42764872312545776 - trainLoss: 0.40460383892059326\n",
      "cnt: 0 - valLoss: 0.42764830589294434 - trainLoss: 0.4046024680137634\n",
      "cnt: 0 - valLoss: 0.4276479184627533 - trainLoss: 0.40460115671157837\n",
      "cnt: 0 - valLoss: 0.42764750123023987 - trainLoss: 0.4045998156070709\n",
      "cnt: 0 - valLoss: 0.42764711380004883 - trainLoss: 0.4045984447002411\n",
      "cnt: 0 - valLoss: 0.4276467263698578 - trainLoss: 0.40459707379341125\n",
      "cnt: 0 - valLoss: 0.427646279335022 - trainLoss: 0.4045957922935486\n",
      "cnt: 0 - valLoss: 0.4276459217071533 - trainLoss: 0.40459442138671875\n",
      "cnt: 0 - valLoss: 0.4276455342769623 - trainLoss: 0.4045930504798889\n",
      "cnt: 0 - valLoss: 0.42764514684677124 - trainLoss: 0.40459173917770386\n",
      "cnt: 0 - valLoss: 0.4276446998119354 - trainLoss: 0.4045903980731964\n",
      "cnt: 0 - valLoss: 0.4276443123817444 - trainLoss: 0.4045890271663666\n",
      "cnt: 0 - valLoss: 0.42764392495155334 - trainLoss: 0.40458765625953674\n",
      "cnt: 0 - valLoss: 0.4276435077190399 - trainLoss: 0.4045862853527069\n",
      "cnt: 0 - valLoss: 0.4276431202888489 - trainLoss: 0.40458500385284424\n",
      "cnt: 0 - valLoss: 0.42764273285865784 - trainLoss: 0.4045836329460144\n",
      "cnt: 0 - valLoss: 0.4276423454284668 - trainLoss: 0.40458232164382935\n",
      "cnt: 0 - valLoss: 0.42764192819595337 - trainLoss: 0.4045809805393219\n",
      "cnt: 0 - valLoss: 0.42764154076576233 - trainLoss: 0.40457960963249207\n",
      "cnt: 0 - valLoss: 0.4276410937309265 - trainLoss: 0.404578298330307\n",
      "cnt: 0 - valLoss: 0.42764073610305786 - trainLoss: 0.4045769274234772\n",
      "cnt: 0 - valLoss: 0.4276403486728668 - trainLoss: 0.4045755863189697\n",
      "cnt: 0 - valLoss: 0.4276399612426758 - trainLoss: 0.4045742154121399\n",
      "cnt: 0 - valLoss: 0.42763957381248474 - trainLoss: 0.40457290410995483\n",
      "cnt: 0 - valLoss: 0.4276391267776489 - trainLoss: 0.4045715630054474\n",
      "cnt: 0 - valLoss: 0.4276387393474579 - trainLoss: 0.40457019209861755\n",
      "cnt: 0 - valLoss: 0.42763832211494446 - trainLoss: 0.4045688509941101\n",
      "cnt: 0 - valLoss: 0.4276379346847534 - trainLoss: 0.40456750988960266\n",
      "cnt: 0 - valLoss: 0.4276375472545624 - trainLoss: 0.4045661687850952\n",
      "cnt: 0 - valLoss: 0.42763715982437134 - trainLoss: 0.40456482768058777\n",
      "cnt: 0 - valLoss: 0.4276367425918579 - trainLoss: 0.4045634865760803\n",
      "cnt: 0 - valLoss: 0.42763635516166687 - trainLoss: 0.4045621454715729\n",
      "cnt: 0 - valLoss: 0.42763596773147583 - trainLoss: 0.40456080436706543\n",
      "cnt: 0 - valLoss: 0.4276355803012848 - trainLoss: 0.404559463262558\n",
      "cnt: 0 - valLoss: 0.42763516306877136 - trainLoss: 0.40455809235572815\n",
      "cnt: 0 - valLoss: 0.4276347756385803 - trainLoss: 0.4045567810535431\n",
      "cnt: 0 - valLoss: 0.4276343882083893 - trainLoss: 0.40455541014671326\n",
      "cnt: 0 - valLoss: 0.42763400077819824 - trainLoss: 0.4045540690422058\n",
      "cnt: 0 - valLoss: 0.4276335537433624 - trainLoss: 0.40455275774002075\n",
      "cnt: 0 - valLoss: 0.4276331663131714 - trainLoss: 0.4045513868331909\n",
      "cnt: 0 - valLoss: 0.42763277888298035 - trainLoss: 0.40455004572868347\n",
      "cnt: 0 - valLoss: 0.4276324212551117 - trainLoss: 0.4045487344264984\n",
      "cnt: 0 - valLoss: 0.42763203382492065 - trainLoss: 0.4045473635196686\n",
      "cnt: 0 - valLoss: 0.42763158679008484 - trainLoss: 0.40454602241516113\n",
      "cnt: 0 - valLoss: 0.4276311993598938 - trainLoss: 0.4045446515083313\n",
      "cnt: 0 - valLoss: 0.42763081192970276 - trainLoss: 0.40454334020614624\n",
      "cnt: 0 - valLoss: 0.42763039469718933 - trainLoss: 0.4045419692993164\n",
      "cnt: 0 - valLoss: 0.4276300072669983 - trainLoss: 0.40454068779945374\n",
      "cnt: 0 - valLoss: 0.42762961983680725 - trainLoss: 0.4045393168926239\n",
      "cnt: 0 - valLoss: 0.4276292026042938 - trainLoss: 0.40453794598579407\n",
      "cnt: 0 - valLoss: 0.42762884497642517 - trainLoss: 0.40453657507896423\n",
      "cnt: 0 - valLoss: 0.42762842774391174 - trainLoss: 0.40453529357910156\n",
      "cnt: 0 - valLoss: 0.4276280403137207 - trainLoss: 0.40453392267227173\n",
      "cnt: 0 - valLoss: 0.42762765288352966 - trainLoss: 0.4045325815677643\n",
      "cnt: 0 - valLoss: 0.42762720584869385 - trainLoss: 0.4045312702655792\n",
      "cnt: 0 - valLoss: 0.4276268482208252 - trainLoss: 0.4045298993587494\n",
      "cnt: 0 - valLoss: 0.42762646079063416 - trainLoss: 0.40452852845191956\n",
      "cnt: 0 - valLoss: 0.42762601375579834 - trainLoss: 0.4045271873474121\n",
      "cnt: 0 - valLoss: 0.4276256859302521 - trainLoss: 0.40452587604522705\n",
      "cnt: 0 - valLoss: 0.42762523889541626 - trainLoss: 0.4045245051383972\n",
      "cnt: 0 - valLoss: 0.42762491106987 - trainLoss: 0.40452316403388977\n",
      "cnt: 0 - valLoss: 0.4276244640350342 - trainLoss: 0.4045218527317047\n",
      "cnt: 0 - valLoss: 0.42762404680252075 - trainLoss: 0.4045204818248749\n",
      "cnt: 0 - valLoss: 0.4276237189769745 - trainLoss: 0.40451914072036743\n",
      "cnt: 0 - valLoss: 0.42762327194213867 - trainLoss: 0.4045178294181824\n",
      "cnt: 0 - valLoss: 0.42762288451194763 - trainLoss: 0.40451645851135254\n",
      "cnt: 0 - valLoss: 0.4276224970817566 - trainLoss: 0.4045151174068451\n",
      "cnt: 0 - valLoss: 0.42762210965156555 - trainLoss: 0.40451380610466003\n",
      "cnt: 0 - valLoss: 0.4276217222213745 - trainLoss: 0.4045124351978302\n",
      "cnt: 0 - valLoss: 0.4276213049888611 - trainLoss: 0.40451109409332275\n",
      "cnt: 0 - valLoss: 0.42762091755867004 - trainLoss: 0.4045097231864929\n",
      "cnt: 0 - valLoss: 0.427620530128479 - trainLoss: 0.40450841188430786\n",
      "cnt: 0 - valLoss: 0.4276201128959656 - trainLoss: 0.4045070707798004\n",
      "cnt: 0 - valLoss: 0.4276197552680969 - trainLoss: 0.4045056998729706\n",
      "cnt: 0 - valLoss: 0.4276193380355835 - trainLoss: 0.4045043885707855\n",
      "cnt: 0 - valLoss: 0.42761895060539246 - trainLoss: 0.4045030176639557\n",
      "cnt: 0 - valLoss: 0.4276185631752014 - trainLoss: 0.404501736164093\n",
      "cnt: 0 - valLoss: 0.427618145942688 - trainLoss: 0.4045003652572632\n",
      "cnt: 0 - valLoss: 0.4276176989078522 - trainLoss: 0.40449902415275574\n",
      "cnt: 0 - valLoss: 0.4276173710823059 - trainLoss: 0.4044976532459259\n",
      "cnt: 0 - valLoss: 0.4276169240474701 - trainLoss: 0.40449634194374084\n",
      "cnt: 0 - valLoss: 0.42761659622192383 - trainLoss: 0.404494971036911\n",
      "cnt: 0 - valLoss: 0.427616149187088 - trainLoss: 0.40449368953704834\n",
      "cnt: 0 - valLoss: 0.42761582136154175 - trainLoss: 0.4044923186302185\n",
      "cnt: 0 - valLoss: 0.4276154041290283 - trainLoss: 0.40449097752571106\n",
      "cnt: 0 - valLoss: 0.4276149570941925 - trainLoss: 0.404489666223526\n",
      "cnt: 0 - valLoss: 0.42761462926864624 - trainLoss: 0.40448829531669617\n",
      "cnt: 0 - valLoss: 0.4276142418384552 - trainLoss: 0.40448692440986633\n",
      "cnt: 0 - valLoss: 0.42761385440826416 - trainLoss: 0.40448564291000366\n",
      "cnt: 0 - valLoss: 0.42761340737342834 - trainLoss: 0.40448427200317383\n",
      "cnt: 0 - valLoss: 0.4276130199432373 - trainLoss: 0.404482901096344\n",
      "cnt: 0 - valLoss: 0.42761266231536865 - trainLoss: 0.40448155999183655\n",
      "cnt: 0 - valLoss: 0.4276122748851776 - trainLoss: 0.4044802486896515\n",
      "cnt: 0 - valLoss: 0.4276118874549866 - trainLoss: 0.40447887778282166\n",
      "cnt: 0 - valLoss: 0.42761144042015076 - trainLoss: 0.404477596282959\n",
      "cnt: 0 - valLoss: 0.4276110529899597 - trainLoss: 0.40447622537612915\n",
      "cnt: 0 - valLoss: 0.4276106655597687 - trainLoss: 0.4044748544692993\n",
      "cnt: 0 - valLoss: 0.42761027812957764 - trainLoss: 0.40447357296943665\n",
      "cnt: 0 - valLoss: 0.4276098906993866 - trainLoss: 0.4044722020626068\n",
      "cnt: 0 - valLoss: 0.42760947346687317 - trainLoss: 0.404470831155777\n",
      "cnt: 0 - valLoss: 0.4276091456413269 - trainLoss: 0.4044695496559143\n",
      "cnt: 0 - valLoss: 0.4276086986064911 - trainLoss: 0.4044681787490845\n",
      "cnt: 0 - valLoss: 0.42760828137397766 - trainLoss: 0.404466837644577\n",
      "cnt: 0 - valLoss: 0.4276078939437866 - trainLoss: 0.40446552634239197\n",
      "cnt: 0 - valLoss: 0.4276075065135956 - trainLoss: 0.40446415543556213\n",
      "cnt: 0 - valLoss: 0.42760708928108215 - trainLoss: 0.4044628143310547\n",
      "cnt: 0 - valLoss: 0.4276067018508911 - trainLoss: 0.40446150302886963\n",
      "cnt: 0 - valLoss: 0.4276063144207001 - trainLoss: 0.4044601619243622\n",
      "cnt: 0 - valLoss: 0.42760586738586426 - trainLoss: 0.40445879101753235\n",
      "cnt: 0 - valLoss: 0.427605539560318 - trainLoss: 0.4044574797153473\n",
      "cnt: 0 - valLoss: 0.4276050925254822 - trainLoss: 0.40445613861083984\n",
      "cnt: 0 - valLoss: 0.4276047646999359 - trainLoss: 0.40445476770401\n",
      "cnt: 0 - valLoss: 0.4276043176651001 - trainLoss: 0.40445345640182495\n",
      "cnt: 0 - valLoss: 0.42760396003723145 - trainLoss: 0.4044520854949951\n",
      "cnt: 0 - valLoss: 0.42760351300239563 - trainLoss: 0.40445080399513245\n",
      "cnt: 0 - valLoss: 0.42760318517684937 - trainLoss: 0.4044494330883026\n",
      "cnt: 0 - valLoss: 0.42760273814201355 - trainLoss: 0.40444809198379517\n",
      "cnt: 0 - valLoss: 0.4276023507118225 - trainLoss: 0.4044467806816101\n",
      "cnt: 0 - valLoss: 0.42760196328163147 - trainLoss: 0.4044454097747803\n",
      "cnt: 0 - valLoss: 0.42760154604911804 - trainLoss: 0.4044440686702728\n",
      "cnt: 0 - valLoss: 0.427601158618927 - trainLoss: 0.40444275736808777\n",
      "cnt: 0 - valLoss: 0.42760077118873596 - trainLoss: 0.40444138646125793\n",
      "cnt: 0 - valLoss: 0.42760035395622253 - trainLoss: 0.4044400453567505\n",
      "cnt: 0 - valLoss: 0.4275999963283539 - trainLoss: 0.40443873405456543\n",
      "cnt: 0 - valLoss: 0.4275995194911957 - trainLoss: 0.4044373631477356\n",
      "cnt: 0 - valLoss: 0.4275991916656494 - trainLoss: 0.40443602204322815\n",
      "cnt: 0 - valLoss: 0.4275988042354584 - trainLoss: 0.4044347107410431\n",
      "cnt: 0 - valLoss: 0.42759841680526733 - trainLoss: 0.40443333983421326\n",
      "cnt: 0 - valLoss: 0.4275979995727539 - trainLoss: 0.4044319987297058\n",
      "cnt: 0 - valLoss: 0.42759761214256287 - trainLoss: 0.40443068742752075\n",
      "cnt: 0 - valLoss: 0.4275972247123718 - trainLoss: 0.4044293165206909\n",
      "cnt: 0 - valLoss: 0.427596777677536 - trainLoss: 0.40442800521850586\n",
      "cnt: 0 - valLoss: 0.42759644985198975 - trainLoss: 0.4044266641139984\n",
      "cnt: 0 - valLoss: 0.42759600281715393 - trainLoss: 0.4044252932071686\n",
      "cnt: 0 - valLoss: 0.4275956451892853 - trainLoss: 0.4044239819049835\n",
      "cnt: 0 - valLoss: 0.42759519815444946 - trainLoss: 0.4044226408004761\n",
      "cnt: 0 - valLoss: 0.4275948107242584 - trainLoss: 0.40442129969596863\n",
      "cnt: 0 - valLoss: 0.42759448289871216 - trainLoss: 0.4044199585914612\n",
      "cnt: 0 - valLoss: 0.42759403586387634 - trainLoss: 0.40441861748695374\n",
      "cnt: 0 - valLoss: 0.4275936186313629 - trainLoss: 0.4044173061847687\n",
      "cnt: 0 - valLoss: 0.42759326100349426 - trainLoss: 0.40441593527793884\n",
      "cnt: 0 - valLoss: 0.4275928735733032 - trainLoss: 0.4044145941734314\n",
      "cnt: 0 - valLoss: 0.42759251594543457 - trainLoss: 0.40441325306892395\n",
      "cnt: 0 - valLoss: 0.42759206891059875 - trainLoss: 0.4044119119644165\n",
      "cnt: 0 - valLoss: 0.4275917410850525 - trainLoss: 0.40441057085990906\n",
      "cnt: 0 - valLoss: 0.4275912940502167 - trainLoss: 0.4044092297554016\n",
      "cnt: 0 - valLoss: 0.42759090662002563 - trainLoss: 0.40440788865089417\n",
      "cnt: 0 - valLoss: 0.4275905191898346 - trainLoss: 0.4044065773487091\n",
      "cnt: 0 - valLoss: 0.42759013175964355 - trainLoss: 0.40440523624420166\n",
      "cnt: 0 - valLoss: 0.42758968472480774 - trainLoss: 0.4044039249420166\n",
      "cnt: 0 - valLoss: 0.4275893270969391 - trainLoss: 0.40440255403518677\n",
      "cnt: 0 - valLoss: 0.42758893966674805 - trainLoss: 0.4044012129306793\n",
      "cnt: 0 - valLoss: 0.427588552236557 - trainLoss: 0.40439990162849426\n",
      "cnt: 0 - valLoss: 0.42758816480636597 - trainLoss: 0.40439853072166443\n",
      "cnt: 0 - valLoss: 0.4275877773761749 - trainLoss: 0.404397189617157\n",
      "cnt: 0 - valLoss: 0.4275873601436615 - trainLoss: 0.4043958783149719\n",
      "cnt: 0 - valLoss: 0.42758700251579285 - trainLoss: 0.4043945372104645\n",
      "cnt: 0 - valLoss: 0.4275865852832794 - trainLoss: 0.40439316630363464\n",
      "cnt: 0 - valLoss: 0.42758625745773315 - trainLoss: 0.404391884803772\n",
      "cnt: 0 - valLoss: 0.4275858700275421 - trainLoss: 0.4043905735015869\n",
      "cnt: 0 - valLoss: 0.4275854825973511 - trainLoss: 0.4043892025947571\n",
      "cnt: 0 - valLoss: 0.42758503556251526 - trainLoss: 0.40438786149024963\n",
      "cnt: 0 - valLoss: 0.427584707736969 - trainLoss: 0.4043864905834198\n",
      "cnt: 0 - valLoss: 0.4275842607021332 - trainLoss: 0.40438517928123474\n",
      "cnt: 0 - valLoss: 0.42758387327194214 - trainLoss: 0.4043838381767273\n",
      "cnt: 0 - valLoss: 0.42758405208587646 - trainLoss: 0.40438252687454224\n",
      "cnt: 1 - valLoss: 0.4275842607021332 - trainLoss: 0.4043810963630676\n",
      "cnt: 2 - valLoss: 0.4275844097137451 - trainLoss: 0.4043796956539154\n",
      "cnt: 3 - valLoss: 0.42758452892303467 - trainLoss: 0.4043782949447632\n",
      "cnt: 4 - valLoss: 0.427584707736969 - trainLoss: 0.4043768644332886\n",
      "cnt: 5 - valLoss: 0.4275848865509033 - trainLoss: 0.40437546372413635\n",
      "cnt: 6 - valLoss: 0.42758503556251526 - trainLoss: 0.40437403321266174\n",
      "cnt: 7 - valLoss: 0.4275851845741272 - trainLoss: 0.4043726325035095\n",
      "cnt: 8 - valLoss: 0.42758533358573914 - trainLoss: 0.4043712615966797\n",
      "cnt: 9 - valLoss: 0.4275854825973511 - trainLoss: 0.4043698012828827\n",
      "cnt: 10 - valLoss: 0.427585631608963 - trainLoss: 0.40436840057373047\n",
      "cnt: 11 - valLoss: 0.4275857210159302 - trainLoss: 0.40436697006225586\n",
      "cnt: 12 - valLoss: 0.4275858700275421 - trainLoss: 0.4043656289577484\n",
      "cnt: 13 - valLoss: 0.42758598923683167 - trainLoss: 0.4043641686439514\n",
      "cnt: 14 - valLoss: 0.42758607864379883 - trainLoss: 0.4043627977371216\n",
      "cnt: 15 - valLoss: 0.4275861978530884 - trainLoss: 0.404361367225647\n",
      "cnt: 16 - valLoss: 0.42758631706237793 - trainLoss: 0.40435996651649475\n",
      "cnt: 17 - valLoss: 0.4275864064693451 - trainLoss: 0.40435856580734253\n",
      "cnt: 18 - valLoss: 0.42758655548095703 - trainLoss: 0.4043571352958679\n",
      "cnt: 19 - valLoss: 0.4275866448879242 - trainLoss: 0.4043557345867157\n",
      "cnt: 20 - valLoss: 0.42758676409721375 - trainLoss: 0.4043543040752411\n",
      "cnt: 21 - valLoss: 0.42758679389953613 - trainLoss: 0.40435290336608887\n",
      "cnt: 22 - valLoss: 0.42758694291114807 - trainLoss: 0.40435150265693665\n",
      "cnt: 23 - valLoss: 0.42758700251579285 - trainLoss: 0.4043501317501068\n",
      "cnt: 24 - valLoss: 0.42758709192276 - trainLoss: 0.4043487012386322\n",
      "cnt: 25 - valLoss: 0.4275871813297272 - trainLoss: 0.40434730052948\n",
      "cnt: 26 - valLoss: 0.42758727073669434 - trainLoss: 0.40434587001800537\n",
      "cnt: 27 - valLoss: 0.4275873303413391 - trainLoss: 0.40434446930885315\n",
      "cnt: 28 - valLoss: 0.42758744955062866 - trainLoss: 0.4043430984020233\n",
      "cnt: 29 - valLoss: 0.4275875389575958 - trainLoss: 0.4043416380882263\n",
      "cnt: 30 - valLoss: 0.4275875389575958 - trainLoss: 0.4043402671813965\n",
      "cnt: 31 - valLoss: 0.427587628364563 - trainLoss: 0.40433886647224426\n",
      "cnt: 32 - valLoss: 0.42758768796920776 - trainLoss: 0.40433746576309204\n",
      "cnt: 33 - valLoss: 0.42758774757385254 - trainLoss: 0.4043360948562622\n",
      "cnt: 34 - valLoss: 0.4275877773761749 - trainLoss: 0.4043346643447876\n",
      "cnt: 35 - valLoss: 0.4275878667831421 - trainLoss: 0.4043332636356354\n",
      "cnt: 36 - valLoss: 0.4275878667831421 - trainLoss: 0.40433183312416077\n",
      "cnt: 37 - valLoss: 0.42758795619010925 - trainLoss: 0.40433043241500854\n",
      "cnt: 38 - valLoss: 0.42758801579475403 - trainLoss: 0.4043290615081787\n",
      "cnt: 39 - valLoss: 0.4275880455970764 - trainLoss: 0.4043276607990265\n",
      "cnt: 40 - valLoss: 0.4275880455970764 - trainLoss: 0.4043262302875519\n",
      "cnt: 41 - valLoss: 0.4275880753993988 - trainLoss: 0.40432482957839966\n",
      "cnt: 42 - valLoss: 0.4275880753993988 - trainLoss: 0.4043234586715698\n",
      "cnt: 43 - valLoss: 0.4275880455970764 - trainLoss: 0.4043220281600952\n",
      "cnt: 44 - valLoss: 0.4275880455970764 - trainLoss: 0.40432068705558777\n",
      "cnt: 45 - valLoss: 0.42758801579475403 - trainLoss: 0.40431925654411316\n",
      "cnt: 46 - valLoss: 0.42758795619010925 - trainLoss: 0.40431785583496094\n",
      "cnt: 47 - valLoss: 0.42758795619010925 - trainLoss: 0.40431642532348633\n",
      "cnt: 48 - valLoss: 0.42758792638778687 - trainLoss: 0.4043150544166565\n",
      "cnt: 49 - valLoss: 0.4275878667831421 - trainLoss: 0.4043136537075043\n",
      "cnt: 50 - valLoss: 0.4275878667831421 - trainLoss: 0.40431228280067444\n",
      "cnt: 51 - valLoss: 0.4275877773761749 - trainLoss: 0.4043109118938446\n",
      "cnt: 52 - valLoss: 0.4275877773761749 - trainLoss: 0.4043094515800476\n",
      "cnt: 53 - valLoss: 0.42758768796920776 - trainLoss: 0.4043080806732178\n",
      "cnt: 54 - valLoss: 0.42758768796920776 - trainLoss: 0.40430667996406555\n",
      "cnt: 55 - valLoss: 0.427587628364563 - trainLoss: 0.40430524945259094\n",
      "cnt: 56 - valLoss: 0.4275875687599182 - trainLoss: 0.4043039083480835\n",
      "cnt: 57 - valLoss: 0.4275875389575958 - trainLoss: 0.4043024778366089\n",
      "cnt: 58 - valLoss: 0.42758744955062866 - trainLoss: 0.40430116653442383\n",
      "cnt: 59 - valLoss: 0.4275873899459839 - trainLoss: 0.40429970622062683\n",
      "cnt: 60 - valLoss: 0.4275873303413391 - trainLoss: 0.404298335313797\n",
      "cnt: 61 - valLoss: 0.42758727073669434 - trainLoss: 0.40429696440696716\n",
      "cnt: 62 - valLoss: 0.42758694291114807 - trainLoss: 0.40429550409317017\n",
      "cnt: 63 - valLoss: 0.4275866746902466 - trainLoss: 0.40429413318634033\n",
      "cnt: 64 - valLoss: 0.42758631706237793 - trainLoss: 0.4042927622795105\n",
      "cnt: 65 - valLoss: 0.42758601903915405 - trainLoss: 0.40429139137268066\n",
      "cnt: 66 - valLoss: 0.4275857210159302 - trainLoss: 0.40429002046585083\n",
      "cnt: 67 - valLoss: 0.4275853931903839 - trainLoss: 0.4042886197566986\n",
      "cnt: 68 - valLoss: 0.42758509516716003 - trainLoss: 0.404287189245224\n",
      "cnt: 69 - valLoss: 0.42758479714393616 - trainLoss: 0.40428581833839417\n",
      "cnt: 70 - valLoss: 0.4275844395160675 - trainLoss: 0.4042844772338867\n",
      "cnt: 71 - valLoss: 0.42758414149284363 - trainLoss: 0.4042831063270569\n",
      "cnt: 72 - valLoss: 0.42758381366729736 - trainLoss: 0.4042816758155823\n",
      "cnt: 0 - valLoss: 0.4275835156440735 - trainLoss: 0.40428027510643005\n",
      "cnt: 0 - valLoss: 0.42758315801620483 - trainLoss: 0.4042789041996002\n",
      "cnt: 0 - valLoss: 0.42758288979530334 - trainLoss: 0.4042775332927704\n",
      "cnt: 0 - valLoss: 0.4275825619697571 - trainLoss: 0.40427616238594055\n",
      "cnt: 0 - valLoss: 0.4275822043418884 - trainLoss: 0.40427473187446594\n",
      "cnt: 0 - valLoss: 0.42758190631866455 - trainLoss: 0.4042734205722809\n",
      "cnt: 0 - valLoss: 0.4275815784931183 - trainLoss: 0.4042719602584839\n",
      "cnt: 0 - valLoss: 0.42758122086524963 - trainLoss: 0.40427058935165405\n",
      "cnt: 0 - valLoss: 0.42758092284202576 - trainLoss: 0.4042692184448242\n",
      "cnt: 0 - valLoss: 0.4275805950164795 - trainLoss: 0.4042678475379944\n",
      "cnt: 0 - valLoss: 0.42758023738861084 - trainLoss: 0.40426644682884216\n",
      "cnt: 0 - valLoss: 0.42757993936538696 - trainLoss: 0.4042651057243347\n",
      "cnt: 0 - valLoss: 0.4275796115398407 - trainLoss: 0.4042636752128601\n",
      "cnt: 0 - valLoss: 0.4275793135166168 - trainLoss: 0.4042623043060303\n",
      "cnt: 0 - valLoss: 0.4275789260864258 - trainLoss: 0.40426093339920044\n",
      "cnt: 0 - valLoss: 0.4275786280632019 - trainLoss: 0.4042595624923706\n",
      "cnt: 0 - valLoss: 0.42757827043533325 - trainLoss: 0.404258131980896\n",
      "cnt: 0 - valLoss: 0.427577942609787 - trainLoss: 0.40425676107406616\n",
      "cnt: 0 - valLoss: 0.42757758498191833 - trainLoss: 0.40425536036491394\n",
      "cnt: 0 - valLoss: 0.4275771975517273 - trainLoss: 0.4042539894580841\n",
      "cnt: 0 - valLoss: 0.4275768995285034 - trainLoss: 0.4042526185512543\n",
      "cnt: 0 - valLoss: 0.42757657170295715 - trainLoss: 0.40425124764442444\n",
      "cnt: 0 - valLoss: 0.4275762140750885 - trainLoss: 0.4042498469352722\n",
      "cnt: 0 - valLoss: 0.42757582664489746 - trainLoss: 0.4042484164237976\n",
      "cnt: 0 - valLoss: 0.4275754988193512 - trainLoss: 0.40424710512161255\n",
      "cnt: 0 - valLoss: 0.42757511138916016 - trainLoss: 0.40424567461013794\n",
      "cnt: 0 - valLoss: 0.4275747537612915 - trainLoss: 0.4042443037033081\n",
      "cnt: 0 - valLoss: 0.42757442593574524 - trainLoss: 0.40424293279647827\n",
      "cnt: 0 - valLoss: 0.4275740683078766 - trainLoss: 0.40424156188964844\n",
      "cnt: 0 - valLoss: 0.4275737404823303 - trainLoss: 0.4042401909828186\n",
      "cnt: 0 - valLoss: 0.4275733530521393 - trainLoss: 0.4042387902736664\n",
      "cnt: 0 - valLoss: 0.42757299542427063 - trainLoss: 0.40423738956451416\n",
      "cnt: 0 - valLoss: 0.4275726079940796 - trainLoss: 0.4042360186576843\n",
      "cnt: 0 - valLoss: 0.42757222056388855 - trainLoss: 0.4042346477508545\n",
      "cnt: 0 - valLoss: 0.4275718927383423 - trainLoss: 0.40423327684402466\n",
      "cnt: 0 - valLoss: 0.42757150530815125 - trainLoss: 0.4042319059371948\n",
      "cnt: 0 - valLoss: 0.4275711476802826 - trainLoss: 0.4042304754257202\n",
      "cnt: 0 - valLoss: 0.42757081985473633 - trainLoss: 0.40422913432121277\n",
      "cnt: 0 - valLoss: 0.4275704324245453 - trainLoss: 0.40422770380973816\n",
      "cnt: 0 - valLoss: 0.42757004499435425 - trainLoss: 0.4042263329029083\n",
      "cnt: 0 - valLoss: 0.4275696575641632 - trainLoss: 0.4042249619960785\n",
      "cnt: 0 - valLoss: 0.42756927013397217 - trainLoss: 0.40422359108924866\n",
      "cnt: 0 - valLoss: 0.4275689423084259 - trainLoss: 0.4042222201824188\n",
      "cnt: 0 - valLoss: 0.42756855487823486 - trainLoss: 0.4042208194732666\n",
      "cnt: 0 - valLoss: 0.4275681674480438 - trainLoss: 0.40421944856643677\n",
      "cnt: 0 - valLoss: 0.42756780982017517 - trainLoss: 0.40421804785728455\n",
      "cnt: 0 - valLoss: 0.42756739258766174 - trainLoss: 0.4042166769504547\n",
      "cnt: 0 - valLoss: 0.4275670349597931 - trainLoss: 0.4042153060436249\n",
      "cnt: 0 - valLoss: 0.4275667071342468 - trainLoss: 0.40421387553215027\n",
      "cnt: 0 - valLoss: 0.427566260099411 - trainLoss: 0.4042125642299652\n",
      "cnt: 0 - valLoss: 0.42756587266921997 - trainLoss: 0.4042111933231354\n",
      "cnt: 0 - valLoss: 0.42756548523902893 - trainLoss: 0.40420976281166077\n",
      "cnt: 0 - valLoss: 0.4275651276111603 - trainLoss: 0.40420839190483093\n",
      "cnt: 0 - valLoss: 0.42756474018096924 - trainLoss: 0.4042069911956787\n",
      "cnt: 0 - valLoss: 0.4275643825531006 - trainLoss: 0.4042056202888489\n",
      "cnt: 0 - valLoss: 0.42756396532058716 - trainLoss: 0.40420424938201904\n",
      "cnt: 0 - valLoss: 0.4275636076927185 - trainLoss: 0.4042028784751892\n",
      "cnt: 0 - valLoss: 0.4275631904602051 - trainLoss: 0.4042015075683594\n",
      "cnt: 0 - valLoss: 0.42756280303001404 - trainLoss: 0.40420013666152954\n",
      "cnt: 0 - valLoss: 0.427562415599823 - trainLoss: 0.4041987657546997\n",
      "cnt: 0 - valLoss: 0.42756205797195435 - trainLoss: 0.4041973054409027\n",
      "cnt: 0 - valLoss: 0.4275616407394409 - trainLoss: 0.4041959345340729\n",
      "cnt: 0 - valLoss: 0.42756131291389465 - trainLoss: 0.4041946232318878\n",
      "cnt: 0 - valLoss: 0.42756086587905884 - trainLoss: 0.4041932225227356\n",
      "cnt: 0 - valLoss: 0.4275604784488678 - trainLoss: 0.40419185161590576\n",
      "cnt: 0 - valLoss: 0.42756009101867676 - trainLoss: 0.4041904807090759\n",
      "cnt: 0 - valLoss: 0.42755967378616333 - trainLoss: 0.4041891098022461\n",
      "cnt: 0 - valLoss: 0.4275593161582947 - trainLoss: 0.40418773889541626\n",
      "cnt: 0 - valLoss: 0.42755889892578125 - trainLoss: 0.4041863679885864\n",
      "cnt: 0 - valLoss: 0.4275584816932678 - trainLoss: 0.4041849374771118\n",
      "cnt: 0 - valLoss: 0.4275580942630768 - trainLoss: 0.4041835367679596\n",
      "cnt: 0 - valLoss: 0.42755770683288574 - trainLoss: 0.40418216586112976\n",
      "cnt: 0 - valLoss: 0.4275572597980499 - trainLoss: 0.4041807949542999\n",
      "cnt: 0 - valLoss: 0.4275568723678589 - trainLoss: 0.4041794240474701\n",
      "cnt: 0 - valLoss: 0.42755648493766785 - trainLoss: 0.40417805314064026\n",
      "cnt: 0 - valLoss: 0.4275560677051544 - trainLoss: 0.40417665243148804\n",
      "cnt: 0 - valLoss: 0.427555650472641 - trainLoss: 0.4041753113269806\n",
      "cnt: 0 - valLoss: 0.42755526304244995 - trainLoss: 0.40417391061782837\n",
      "cnt: 0 - valLoss: 0.42755481600761414 - trainLoss: 0.40417253971099854\n",
      "cnt: 0 - valLoss: 0.4275544285774231 - trainLoss: 0.4041711688041687\n",
      "cnt: 0 - valLoss: 0.42755401134490967 - trainLoss: 0.4041697680950165\n",
      "cnt: 0 - valLoss: 0.42755359411239624 - trainLoss: 0.40416842699050903\n",
      "cnt: 0 - valLoss: 0.4275532066822052 - trainLoss: 0.4041670262813568\n",
      "cnt: 0 - valLoss: 0.42755281925201416 - trainLoss: 0.404165655374527\n",
      "cnt: 0 - valLoss: 0.42755237221717834 - trainLoss: 0.40416428446769714\n",
      "cnt: 0 - valLoss: 0.4275519549846649 - trainLoss: 0.4041629135608673\n",
      "cnt: 0 - valLoss: 0.4275515377521515 - trainLoss: 0.4041614830493927\n",
      "cnt: 0 - valLoss: 0.42755115032196045 - trainLoss: 0.40416011214256287\n",
      "cnt: 0 - valLoss: 0.42755070328712463 - trainLoss: 0.40415874123573303\n",
      "cnt: 0 - valLoss: 0.4275502860546112 - trainLoss: 0.4041573405265808\n",
      "cnt: 0 - valLoss: 0.42754989862442017 - trainLoss: 0.40415602922439575\n",
      "cnt: 0 - valLoss: 0.42754948139190674 - trainLoss: 0.40415459871292114\n",
      "cnt: 0 - valLoss: 0.4275490343570709 - trainLoss: 0.4041532278060913\n",
      "cnt: 0 - valLoss: 0.4275486171245575 - trainLoss: 0.4041518568992615\n",
      "cnt: 0 - valLoss: 0.4275481700897217 - trainLoss: 0.40415048599243164\n",
      "cnt: 0 - valLoss: 0.42754775285720825 - trainLoss: 0.4041491150856018\n",
      "cnt: 0 - valLoss: 0.4275473654270172 - trainLoss: 0.4041477143764496\n",
      "cnt: 0 - valLoss: 0.427546888589859 - trainLoss: 0.40414634346961975\n",
      "cnt: 0 - valLoss: 0.4275464713573456 - trainLoss: 0.4041450023651123\n",
      "cnt: 0 - valLoss: 0.42754608392715454 - trainLoss: 0.4041436016559601\n",
      "cnt: 0 - valLoss: 0.42754560708999634 - trainLoss: 0.40414223074913025\n",
      "cnt: 0 - valLoss: 0.4275452196598053 - trainLoss: 0.404140830039978\n",
      "cnt: 0 - valLoss: 0.4275447428226471 - trainLoss: 0.4041394591331482\n",
      "cnt: 0 - valLoss: 0.42754432559013367 - trainLoss: 0.40413808822631836\n",
      "cnt: 0 - valLoss: 0.42754390835762024 - trainLoss: 0.4041367173194885\n",
      "cnt: 0 - valLoss: 0.4275434613227844 - trainLoss: 0.4041353464126587\n",
      "cnt: 0 - valLoss: 0.427543044090271 - trainLoss: 0.40413397550582886\n",
      "cnt: 0 - valLoss: 0.4275425672531128 - trainLoss: 0.404132604598999\n",
      "cnt: 0 - valLoss: 0.42754215002059937 - trainLoss: 0.4041312336921692\n",
      "cnt: 0 - valLoss: 0.4275417625904083 - trainLoss: 0.40412986278533936\n",
      "cnt: 0 - valLoss: 0.4275412857532501 - trainLoss: 0.4041284918785095\n",
      "cnt: 0 - valLoss: 0.4275408089160919 - trainLoss: 0.4041270911693573\n",
      "cnt: 0 - valLoss: 0.4275403916835785 - trainLoss: 0.40412572026252747\n",
      "cnt: 0 - valLoss: 0.4275399446487427 - trainLoss: 0.40412428975105286\n",
      "cnt: 0 - valLoss: 0.42753952741622925 - trainLoss: 0.404122918844223\n",
      "cnt: 0 - valLoss: 0.42753905057907104 - trainLoss: 0.4041215479373932\n",
      "cnt: 0 - valLoss: 0.4275386333465576 - trainLoss: 0.40412017703056335\n",
      "cnt: 0 - valLoss: 0.4275381863117218 - trainLoss: 0.4041188061237335\n",
      "cnt: 0 - valLoss: 0.427537739276886 - trainLoss: 0.4041174650192261\n",
      "cnt: 0 - valLoss: 0.42753729224205017 - trainLoss: 0.40411609411239624\n",
      "cnt: 0 - valLoss: 0.42753687500953674 - trainLoss: 0.4041147232055664\n",
      "cnt: 0 - valLoss: 0.42753639817237854 - trainLoss: 0.4041133522987366\n",
      "cnt: 0 - valLoss: 0.4275359809398651 - trainLoss: 0.40411198139190674\n",
      "cnt: 0 - valLoss: 0.4275355041027069 - trainLoss: 0.40411055088043213\n",
      "cnt: 0 - valLoss: 0.4275350570678711 - trainLoss: 0.4041091799736023\n",
      "cnt: 0 - valLoss: 0.4275346100330353 - trainLoss: 0.40410780906677246\n",
      "cnt: 0 - valLoss: 0.4275341331958771 - trainLoss: 0.4041064381599426\n",
      "cnt: 0 - valLoss: 0.42753368616104126 - trainLoss: 0.4041050672531128\n",
      "cnt: 0 - valLoss: 0.42753326892852783 - trainLoss: 0.40410369634628296\n",
      "cnt: 0 - valLoss: 0.42753279209136963 - trainLoss: 0.4041023552417755\n",
      "cnt: 0 - valLoss: 0.4275323152542114 - trainLoss: 0.4041009843349457\n",
      "cnt: 0 - valLoss: 0.427531898021698 - trainLoss: 0.40409961342811584\n",
      "cnt: 0 - valLoss: 0.4275314211845398 - trainLoss: 0.40409818291664124\n",
      "cnt: 0 - valLoss: 0.42753100395202637 - trainLoss: 0.4040968120098114\n",
      "cnt: 0 - valLoss: 0.42753052711486816 - trainLoss: 0.40409544110298157\n",
      "cnt: 0 - valLoss: 0.42753010988235474 - trainLoss: 0.40409407019615173\n",
      "cnt: 0 - valLoss: 0.42752957344055176 - trainLoss: 0.4040926992893219\n",
      "cnt: 0 - valLoss: 0.42752915620803833 - trainLoss: 0.40409132838249207\n",
      "cnt: 0 - valLoss: 0.4275286793708801 - trainLoss: 0.40408995747566223\n",
      "cnt: 0 - valLoss: 0.4275282025337219 - trainLoss: 0.4040886163711548\n",
      "cnt: 0 - valLoss: 0.4275277853012085 - trainLoss: 0.40408724546432495\n",
      "cnt: 0 - valLoss: 0.4275273084640503 - trainLoss: 0.40408581495285034\n",
      "cnt: 0 - valLoss: 0.4275268316268921 - trainLoss: 0.4040845036506653\n",
      "cnt: 0 - valLoss: 0.4275263547897339 - trainLoss: 0.4040830731391907\n",
      "cnt: 0 - valLoss: 0.42752590775489807 - trainLoss: 0.40408170223236084\n",
      "cnt: 0 - valLoss: 0.42752546072006226 - trainLoss: 0.4040803909301758\n",
      "cnt: 0 - valLoss: 0.42752495408058167 - trainLoss: 0.40407902002334595\n",
      "cnt: 0 - valLoss: 0.42752453684806824 - trainLoss: 0.40407758951187134\n",
      "cnt: 0 - valLoss: 0.42752406001091003 - trainLoss: 0.4040762186050415\n",
      "cnt: 0 - valLoss: 0.42752358317375183 - trainLoss: 0.40407487750053406\n",
      "cnt: 0 - valLoss: 0.4275231659412384 - trainLoss: 0.4040735065937042\n",
      "cnt: 0 - valLoss: 0.4275226294994354 - trainLoss: 0.4040720760822296\n",
      "cnt: 0 - valLoss: 0.427522212266922 - trainLoss: 0.4040707051753998\n",
      "cnt: 0 - valLoss: 0.42752179503440857 - trainLoss: 0.40406933426856995\n",
      "cnt: 0 - valLoss: 0.4275212585926056 - trainLoss: 0.4040680229663849\n",
      "cnt: 0 - valLoss: 0.4275207817554474 - trainLoss: 0.4040665924549103\n",
      "cnt: 0 - valLoss: 0.4275203347206116 - trainLoss: 0.40406522154808044\n",
      "cnt: 0 - valLoss: 0.42751985788345337 - trainLoss: 0.4040638506412506\n",
      "cnt: 0 - valLoss: 0.42751941084861755 - trainLoss: 0.4040624797344208\n",
      "cnt: 0 - valLoss: 0.42751890420913696 - trainLoss: 0.40406110882759094\n",
      "cnt: 0 - valLoss: 0.42751842737197876 - trainLoss: 0.4040597081184387\n",
      "cnt: 0 - valLoss: 0.42751801013946533 - trainLoss: 0.40405839681625366\n",
      "cnt: 0 - valLoss: 0.42751750349998474 - trainLoss: 0.40405702590942383\n",
      "cnt: 0 - valLoss: 0.4275170564651489 - trainLoss: 0.404055655002594\n",
      "cnt: 0 - valLoss: 0.42751654982566833 - trainLoss: 0.40405428409576416\n",
      "cnt: 0 - valLoss: 0.42751607298851013 - trainLoss: 0.4040529131889343\n",
      "cnt: 0 - valLoss: 0.42751559615135193 - trainLoss: 0.4040515422821045\n",
      "cnt: 0 - valLoss: 0.4275151491165161 - trainLoss: 0.40405017137527466\n",
      "cnt: 0 - valLoss: 0.4275146722793579 - trainLoss: 0.4040488004684448\n",
      "cnt: 0 - valLoss: 0.4275141954421997 - trainLoss: 0.404047429561615\n",
      "cnt: 0 - valLoss: 0.4275137186050415 - trainLoss: 0.40404602885246277\n",
      "cnt: 0 - valLoss: 0.4275132417678833 - trainLoss: 0.4040446877479553\n",
      "cnt: 0 - valLoss: 0.4275127649307251 - trainLoss: 0.4040432870388031\n",
      "cnt: 0 - valLoss: 0.4275122582912445 - trainLoss: 0.40404194593429565\n",
      "cnt: 0 - valLoss: 0.4275118410587311 - trainLoss: 0.4040405750274658\n",
      "cnt: 0 - valLoss: 0.4275113344192505 - trainLoss: 0.404039204120636\n",
      "cnt: 0 - valLoss: 0.4275108873844147 - trainLoss: 0.40403783321380615\n",
      "cnt: 0 - valLoss: 0.4275103807449341 - trainLoss: 0.40403643250465393\n",
      "cnt: 0 - valLoss: 0.4275099039077759 - trainLoss: 0.4040350615978241\n",
      "cnt: 0 - valLoss: 0.4275094270706177 - trainLoss: 0.40403369069099426\n",
      "cnt: 0 - valLoss: 0.4275089502334595 - trainLoss: 0.40403231978416443\n",
      "cnt: 0 - valLoss: 0.4275084435939789 - trainLoss: 0.4040309488773346\n",
      "cnt: 0 - valLoss: 0.4275079667568207 - trainLoss: 0.40402957797050476\n",
      "cnt: 0 - valLoss: 0.42750751972198486 - trainLoss: 0.4040282666683197\n",
      "cnt: 0 - valLoss: 0.42750704288482666 - trainLoss: 0.4040268361568451\n",
      "cnt: 0 - valLoss: 0.42750653624534607 - trainLoss: 0.40402546525001526\n",
      "cnt: 0 - valLoss: 0.42750608921051025 - trainLoss: 0.4040240943431854\n",
      "cnt: 0 - valLoss: 0.42750558257102966 - trainLoss: 0.4040227234363556\n",
      "cnt: 0 - valLoss: 0.4275050759315491 - trainLoss: 0.40402135252952576\n",
      "cnt: 0 - valLoss: 0.42750459909439087 - trainLoss: 0.4040200412273407\n",
      "cnt: 0 - valLoss: 0.42750412225723267 - trainLoss: 0.4040186107158661\n",
      "cnt: 0 - valLoss: 0.4275036156177521 - trainLoss: 0.40401723980903625\n",
      "cnt: 0 - valLoss: 0.42750313878059387 - trainLoss: 0.4040158689022064\n",
      "cnt: 0 - valLoss: 0.42750266194343567 - trainLoss: 0.4040144979953766\n",
      "cnt: 0 - valLoss: 0.4275021553039551 - trainLoss: 0.40401318669319153\n",
      "cnt: 0 - valLoss: 0.4275016784667969 - trainLoss: 0.4040117859840393\n",
      "cnt: 0 - valLoss: 0.42750120162963867 - trainLoss: 0.4040104150772095\n",
      "cnt: 0 - valLoss: 0.42750072479248047 - trainLoss: 0.40400904417037964\n",
      "cnt: 0 - valLoss: 0.4275002181529999 - trainLoss: 0.40400761365890503\n",
      "cnt: 0 - valLoss: 0.4274997413158417 - trainLoss: 0.40400630235671997\n",
      "cnt: 0 - valLoss: 0.4274992346763611 - trainLoss: 0.4040049612522125\n",
      "cnt: 0 - valLoss: 0.4274987280368805 - trainLoss: 0.4040035605430603\n",
      "cnt: 0 - valLoss: 0.4274982511997223 - trainLoss: 0.40400218963623047\n",
      "cnt: 0 - valLoss: 0.4274977743625641 - trainLoss: 0.40400081872940063\n",
      "cnt: 0 - valLoss: 0.4274972677230835 - trainLoss: 0.4039994478225708\n",
      "cnt: 0 - valLoss: 0.4274967908859253 - trainLoss: 0.40399807691574097\n",
      "cnt: 0 - valLoss: 0.4274962842464447 - trainLoss: 0.40399670600891113\n",
      "cnt: 0 - valLoss: 0.4274958074092865 - trainLoss: 0.4039953052997589\n",
      "cnt: 0 - valLoss: 0.4274953007698059 - trainLoss: 0.4039939343929291\n",
      "cnt: 0 - valLoss: 0.42749476432800293 - trainLoss: 0.40399259328842163\n",
      "cnt: 0 - valLoss: 0.42749425768852234 - trainLoss: 0.4039912223815918\n",
      "cnt: 0 - valLoss: 0.4274938404560089 - trainLoss: 0.40398985147476196\n",
      "cnt: 0 - valLoss: 0.4274933338165283 - trainLoss: 0.40398845076560974\n",
      "cnt: 0 - valLoss: 0.42749279737472534 - trainLoss: 0.4039871096611023\n",
      "cnt: 0 - valLoss: 0.42749229073524475 - trainLoss: 0.40398573875427246\n",
      "cnt: 0 - valLoss: 0.42749181389808655 - trainLoss: 0.4039843678474426\n",
      "cnt: 0 - valLoss: 0.42749133706092834 - trainLoss: 0.4039829969406128\n",
      "cnt: 0 - valLoss: 0.42749083042144775 - trainLoss: 0.40398162603378296\n",
      "cnt: 0 - valLoss: 0.42749032378196716 - trainLoss: 0.4039802849292755\n",
      "cnt: 0 - valLoss: 0.42748984694480896 - trainLoss: 0.4039789140224457\n",
      "cnt: 0 - valLoss: 0.42748934030532837 - trainLoss: 0.40397754311561584\n",
      "cnt: 0 - valLoss: 0.42748886346817017 - trainLoss: 0.403976172208786\n",
      "cnt: 0 - valLoss: 0.4274883568286896 - trainLoss: 0.4039748013019562\n",
      "cnt: 0 - valLoss: 0.4274878203868866 - trainLoss: 0.40397346019744873\n",
      "cnt: 0 - valLoss: 0.427487313747406 - trainLoss: 0.4039720892906189\n",
      "cnt: 0 - valLoss: 0.4274868071079254 - trainLoss: 0.40397071838378906\n",
      "cnt: 0 - valLoss: 0.4274863004684448 - trainLoss: 0.40396934747695923\n",
      "cnt: 0 - valLoss: 0.4274858236312866 - trainLoss: 0.403967946767807\n",
      "cnt: 0 - valLoss: 0.42748522758483887 - trainLoss: 0.40396660566329956\n",
      "cnt: 0 - valLoss: 0.42748457193374634 - trainLoss: 0.4039652347564697\n",
      "cnt: 0 - valLoss: 0.42748400568962097 - trainLoss: 0.4039638638496399\n",
      "cnt: 0 - valLoss: 0.42748337984085083 - trainLoss: 0.40396252274513245\n",
      "cnt: 0 - valLoss: 0.4274827837944031 - trainLoss: 0.4039611518383026\n",
      "cnt: 0 - valLoss: 0.4274821877479553 - trainLoss: 0.4039597809314728\n",
      "cnt: 0 - valLoss: 0.4274815618991852 - trainLoss: 0.40395843982696533\n",
      "cnt: 0 - valLoss: 0.4274809658527374 - trainLoss: 0.4039570689201355\n",
      "cnt: 0 - valLoss: 0.4274803698062897 - trainLoss: 0.40395569801330566\n",
      "cnt: 0 - valLoss: 0.4274797737598419 - trainLoss: 0.4039543867111206\n",
      "cnt: 0 - valLoss: 0.42747917771339417 - trainLoss: 0.40395301580429077\n",
      "cnt: 0 - valLoss: 0.4274785816669464 - trainLoss: 0.40395164489746094\n",
      "cnt: 0 - valLoss: 0.42747798562049866 - trainLoss: 0.4039502739906311\n",
      "cnt: 0 - valLoss: 0.4274773895740509 - trainLoss: 0.40394890308380127\n",
      "cnt: 0 - valLoss: 0.42747676372528076 - trainLoss: 0.4039475619792938\n",
      "cnt: 0 - valLoss: 0.427476167678833 - trainLoss: 0.40394625067710876\n",
      "cnt: 0 - valLoss: 0.42747557163238525 - trainLoss: 0.40394482016563416\n",
      "cnt: 0 - valLoss: 0.4274749755859375 - trainLoss: 0.4039435088634491\n",
      "cnt: 0 - valLoss: 0.42747437953948975 - trainLoss: 0.40394213795661926\n",
      "cnt: 0 - valLoss: 0.427473783493042 - trainLoss: 0.40394076704978943\n",
      "cnt: 0 - valLoss: 0.42747318744659424 - trainLoss: 0.4039393961429596\n",
      "cnt: 0 - valLoss: 0.4274725615978241 - trainLoss: 0.40393805503845215\n",
      "cnt: 0 - valLoss: 0.42747196555137634 - trainLoss: 0.4039367139339447\n",
      "cnt: 0 - valLoss: 0.4274713695049286 - trainLoss: 0.40393537282943726\n",
      "cnt: 0 - valLoss: 0.42747077345848083 - trainLoss: 0.4039340019226074\n",
      "cnt: 0 - valLoss: 0.4274701774120331 - trainLoss: 0.4039326012134552\n",
      "cnt: 0 - valLoss: 0.4274695813655853 - trainLoss: 0.40393126010894775\n",
      "cnt: 0 - valLoss: 0.4274689853191376 - trainLoss: 0.4039298892021179\n",
      "cnt: 0 - valLoss: 0.4274683892726898 - trainLoss: 0.40392857789993286\n",
      "cnt: 0 - valLoss: 0.42746782302856445 - trainLoss: 0.403927206993103\n",
      "cnt: 0 - valLoss: 0.4274671673774719 - trainLoss: 0.4039258360862732\n",
      "cnt: 0 - valLoss: 0.42746657133102417 - trainLoss: 0.40392449498176575\n",
      "cnt: 0 - valLoss: 0.4274660050868988 - trainLoss: 0.4039231240749359\n",
      "cnt: 0 - valLoss: 0.42746540904045105 - trainLoss: 0.4039217531681061\n",
      "cnt: 0 - valLoss: 0.4274648129940033 - trainLoss: 0.40392038226127625\n",
      "cnt: 0 - valLoss: 0.42746424674987793 - trainLoss: 0.4039190113544464\n",
      "cnt: 0 - valLoss: 0.4274636507034302 - trainLoss: 0.40391770005226135\n",
      "cnt: 0 - valLoss: 0.4274630546569824 - trainLoss: 0.4039163291454315\n",
      "cnt: 0 - valLoss: 0.42746245861053467 - trainLoss: 0.4039149582386017\n",
      "cnt: 0 - valLoss: 0.4274618625640869 - trainLoss: 0.40391361713409424\n",
      "cnt: 0 - valLoss: 0.42746129631996155 - trainLoss: 0.4039122462272644\n",
      "cnt: 0 - valLoss: 0.4274607002735138 - trainLoss: 0.40391093492507935\n",
      "cnt: 0 - valLoss: 0.4274601340293884 - trainLoss: 0.4039095640182495\n",
      "cnt: 0 - valLoss: 0.4274595379829407 - trainLoss: 0.4039081931114197\n",
      "cnt: 0 - valLoss: 0.4274589419364929 - trainLoss: 0.40390685200691223\n",
      "cnt: 0 - valLoss: 0.42745834589004517 - trainLoss: 0.4039055109024048\n",
      "cnt: 0 - valLoss: 0.4274577796459198 - trainLoss: 0.40390413999557495\n",
      "cnt: 0 - valLoss: 0.42745718359947205 - trainLoss: 0.4039027988910675\n",
      "cnt: 0 - valLoss: 0.4274566173553467 - trainLoss: 0.4039013981819153\n",
      "cnt: 0 - valLoss: 0.4274560213088989 - trainLoss: 0.40390005707740784\n",
      "cnt: 0 - valLoss: 0.42745542526245117 - trainLoss: 0.4038987457752228\n",
      "cnt: 0 - valLoss: 0.4274548292160034 - trainLoss: 0.40389737486839294\n",
      "cnt: 0 - valLoss: 0.4274543225765228 - trainLoss: 0.4038960039615631\n",
      "cnt: 0 - valLoss: 0.4274536669254303 - trainLoss: 0.4038946330547333\n",
      "cnt: 0 - valLoss: 0.4274531304836273 - trainLoss: 0.40389326214790344\n",
      "cnt: 0 - valLoss: 0.4274525046348572 - trainLoss: 0.403891921043396\n",
      "cnt: 0 - valLoss: 0.4274519681930542 - trainLoss: 0.40389060974121094\n",
      "cnt: 0 - valLoss: 0.42745140194892883 - trainLoss: 0.40388917922973633\n",
      "cnt: 0 - valLoss: 0.4274508059024811 - trainLoss: 0.40388786792755127\n",
      "cnt: 0 - valLoss: 0.4274502098560333 - trainLoss: 0.40388649702072144\n",
      "cnt: 0 - valLoss: 0.42744964361190796 - trainLoss: 0.403885155916214\n",
      "cnt: 0 - valLoss: 0.4274490475654602 - trainLoss: 0.40388375520706177\n",
      "cnt: 0 - valLoss: 0.42744845151901245 - trainLoss: 0.4038824141025543\n",
      "cnt: 0 - valLoss: 0.4274478852748871 - trainLoss: 0.40388110280036926\n",
      "cnt: 0 - valLoss: 0.42744728922843933 - trainLoss: 0.40387973189353943\n",
      "cnt: 0 - valLoss: 0.42744672298431396 - trainLoss: 0.4038783609867096\n",
      "cnt: 0 - valLoss: 0.427446186542511 - trainLoss: 0.40387699007987976\n",
      "cnt: 0 - valLoss: 0.42744556069374084 - trainLoss: 0.4038756787776947\n",
      "cnt: 0 - valLoss: 0.42744502425193787 - trainLoss: 0.40387433767318726\n",
      "cnt: 0 - valLoss: 0.4274444282054901 - trainLoss: 0.4038729667663574\n",
      "cnt: 0 - valLoss: 0.42744386196136475 - trainLoss: 0.4038715958595276\n",
      "cnt: 0 - valLoss: 0.427443265914917 - trainLoss: 0.40387022495269775\n",
      "cnt: 0 - valLoss: 0.4274426996707916 - trainLoss: 0.4038688540458679\n",
      "cnt: 0 - valLoss: 0.42744213342666626 - trainLoss: 0.40386754274368286\n",
      "cnt: 0 - valLoss: 0.4274415373802185 - trainLoss: 0.403866171836853\n",
      "cnt: 0 - valLoss: 0.42744094133377075 - trainLoss: 0.4038648009300232\n",
      "cnt: 0 - valLoss: 0.4274403750896454 - trainLoss: 0.40386345982551575\n",
      "cnt: 0 - valLoss: 0.4274398386478424 - trainLoss: 0.4038620889186859\n",
      "cnt: 0 - valLoss: 0.42743924260139465 - trainLoss: 0.40386077761650085\n",
      "cnt: 0 - valLoss: 0.4274386763572693 - trainLoss: 0.403859406709671\n",
      "cnt: 0 - valLoss: 0.4274381101131439 - trainLoss: 0.4038580358028412\n",
      "cnt: 0 - valLoss: 0.42743751406669617 - trainLoss: 0.40385669469833374\n",
      "cnt: 0 - valLoss: 0.4274369180202484 - trainLoss: 0.4038553237915039\n",
      "cnt: 0 - valLoss: 0.42743635177612305 - trainLoss: 0.4038539528846741\n",
      "cnt: 0 - valLoss: 0.4274357855319977 - trainLoss: 0.40385258197784424\n",
      "cnt: 0 - valLoss: 0.4274351894855499 - trainLoss: 0.4038512706756592\n",
      "cnt: 0 - valLoss: 0.4274345934391022 - trainLoss: 0.40384989976882935\n",
      "cnt: 0 - valLoss: 0.4274340569972992 - trainLoss: 0.4038485586643219\n",
      "cnt: 0 - valLoss: 0.42743349075317383 - trainLoss: 0.40384718775749207\n",
      "cnt: 0 - valLoss: 0.4274328947067261 - trainLoss: 0.40384581685066223\n",
      "cnt: 0 - valLoss: 0.4274323284626007 - trainLoss: 0.4038445055484772\n",
      "cnt: 0 - valLoss: 0.42743176221847534 - trainLoss: 0.40384313464164734\n",
      "cnt: 0 - valLoss: 0.4274311661720276 - trainLoss: 0.4038417637348175\n",
      "cnt: 0 - valLoss: 0.4274305999279022 - trainLoss: 0.40384045243263245\n",
      "cnt: 0 - valLoss: 0.42743006348609924 - trainLoss: 0.4038390815258026\n",
      "cnt: 0 - valLoss: 0.4274294674396515 - trainLoss: 0.4038377106189728\n",
      "cnt: 0 - valLoss: 0.42742884159088135 - trainLoss: 0.40383636951446533\n",
      "cnt: 0 - valLoss: 0.42742830514907837 - trainLoss: 0.4038349986076355\n",
      "cnt: 0 - valLoss: 0.427427738904953 - trainLoss: 0.40383362770080566\n",
      "cnt: 0 - valLoss: 0.42742714285850525 - trainLoss: 0.4038323163986206\n",
      "cnt: 0 - valLoss: 0.4274265766143799 - trainLoss: 0.40383094549179077\n",
      "cnt: 0 - valLoss: 0.4274260401725769 - trainLoss: 0.40382957458496094\n",
      "cnt: 0 - valLoss: 0.42742541432380676 - trainLoss: 0.4038282334804535\n",
      "cnt: 0 - valLoss: 0.4274248778820038 - trainLoss: 0.40382686257362366\n",
      "cnt: 0 - valLoss: 0.42742428183555603 - trainLoss: 0.4038255512714386\n",
      "cnt: 0 - valLoss: 0.42742371559143066 - trainLoss: 0.40382418036460876\n",
      "cnt: 0 - valLoss: 0.4274231493473053 - trainLoss: 0.40382280945777893\n",
      "cnt: 0 - valLoss: 0.42742255330085754 - trainLoss: 0.4038214385509491\n",
      "cnt: 0 - valLoss: 0.4274219870567322 - trainLoss: 0.40382006764411926\n",
      "cnt: 0 - valLoss: 0.4274213910102844 - trainLoss: 0.4038187265396118\n",
      "cnt: 0 - valLoss: 0.42742079496383667 - trainLoss: 0.40381741523742676\n",
      "cnt: 0 - valLoss: 0.4274202883243561 - trainLoss: 0.4038160443305969\n",
      "cnt: 0 - valLoss: 0.4274196922779083 - trainLoss: 0.4038146734237671\n",
      "cnt: 0 - valLoss: 0.42741912603378296 - trainLoss: 0.40381330251693726\n",
      "cnt: 0 - valLoss: 0.4274185299873352 - trainLoss: 0.4038119614124298\n",
      "cnt: 0 - valLoss: 0.42741796374320984 - trainLoss: 0.40381062030792236\n",
      "cnt: 0 - valLoss: 0.4274173974990845 - trainLoss: 0.4038092792034149\n",
      "cnt: 0 - valLoss: 0.4274168610572815 - trainLoss: 0.4038079082965851\n",
      "cnt: 0 - valLoss: 0.42741626501083374 - trainLoss: 0.40380653738975525\n",
      "cnt: 0 - valLoss: 0.42741572856903076 - trainLoss: 0.4038052260875702\n",
      "cnt: 0 - valLoss: 0.427415132522583 - trainLoss: 0.40380385518074036\n",
      "cnt: 0 - valLoss: 0.42741459608078003 - trainLoss: 0.4038025140762329\n",
      "cnt: 0 - valLoss: 0.42741402983665466 - trainLoss: 0.4038011431694031\n",
      "cnt: 0 - valLoss: 0.4274134635925293 - trainLoss: 0.40379977226257324\n",
      "cnt: 0 - valLoss: 0.42741289734840393 - trainLoss: 0.4037984609603882\n",
      "cnt: 0 - valLoss: 0.4274123013019562 - trainLoss: 0.40379709005355835\n",
      "cnt: 0 - valLoss: 0.4274117052555084 - trainLoss: 0.4037957489490509\n",
      "cnt: 0 - valLoss: 0.42741116881370544 - trainLoss: 0.40379437804222107\n",
      "cnt: 0 - valLoss: 0.4274106025695801 - trainLoss: 0.40379300713539124\n",
      "cnt: 0 - valLoss: 0.4274100363254547 - trainLoss: 0.4037916362285614\n",
      "cnt: 0 - valLoss: 0.42740947008132935 - trainLoss: 0.40379032492637634\n",
      "cnt: 0 - valLoss: 0.42740893363952637 - trainLoss: 0.4037889838218689\n",
      "cnt: 0 - valLoss: 0.427408367395401 - trainLoss: 0.40378761291503906\n",
      "cnt: 0 - valLoss: 0.42740780115127563 - trainLoss: 0.40378624200820923\n",
      "cnt: 0 - valLoss: 0.4274072051048279 - trainLoss: 0.4037848711013794\n",
      "cnt: 0 - valLoss: 0.4274066984653473 - trainLoss: 0.40378355979919434\n",
      "cnt: 0 - valLoss: 0.4274061322212219 - trainLoss: 0.4037821888923645\n",
      "cnt: 0 - valLoss: 0.42740562558174133 - trainLoss: 0.40378084778785706\n",
      "cnt: 0 - valLoss: 0.42740511894226074 - trainLoss: 0.4037794768810272\n",
      "cnt: 0 - valLoss: 0.4274045526981354 - trainLoss: 0.4037781059741974\n",
      "cnt: 0 - valLoss: 0.42740398645401 - trainLoss: 0.4037768244743347\n",
      "cnt: 0 - valLoss: 0.4274034798145294 - trainLoss: 0.4037754535675049\n",
      "cnt: 0 - valLoss: 0.42740297317504883 - trainLoss: 0.40377408266067505\n",
      "cnt: 0 - valLoss: 0.4274023771286011 - trainLoss: 0.4037727117538452\n",
      "cnt: 0 - valLoss: 0.4274018406867981 - trainLoss: 0.40377140045166016\n",
      "cnt: 0 - valLoss: 0.4274013340473175 - trainLoss: 0.4037700295448303\n",
      "cnt: 0 - valLoss: 0.42740103602409363 - trainLoss: 0.4037686884403229\n",
      "cnt: 0 - valLoss: 0.42740046977996826 - trainLoss: 0.40376734733581543\n",
      "cnt: 0 - valLoss: 0.42739996314048767 - trainLoss: 0.4037659764289856\n",
      "cnt: 0 - valLoss: 0.4273996353149414 - trainLoss: 0.40376463532447815\n",
      "cnt: 0 - valLoss: 0.4273990988731384 - trainLoss: 0.4037632644176483\n",
      "cnt: 0 - valLoss: 0.42739859223365784 - trainLoss: 0.40376195311546326\n",
      "cnt: 0 - valLoss: 0.4273982644081116 - trainLoss: 0.4037605822086334\n",
      "cnt: 0 - valLoss: 0.4273977279663086 - trainLoss: 0.403759241104126\n",
      "cnt: 0 - valLoss: 0.427397221326828 - trainLoss: 0.40375787019729614\n",
      "cnt: 0 - valLoss: 0.42739665508270264 - trainLoss: 0.4037565588951111\n",
      "cnt: 0 - valLoss: 0.42739635705947876 - trainLoss: 0.40375518798828125\n",
      "cnt: 0 - valLoss: 0.42739585041999817 - trainLoss: 0.4037538170814514\n",
      "cnt: 0 - valLoss: 0.4273952543735504 - trainLoss: 0.40375247597694397\n",
      "cnt: 0 - valLoss: 0.4273949861526489 - trainLoss: 0.4037511646747589\n",
      "cnt: 0 - valLoss: 0.42739444971084595 - trainLoss: 0.4037497937679291\n",
      "cnt: 0 - valLoss: 0.4273938834667206 - trainLoss: 0.40374845266342163\n",
      "cnt: 0 - valLoss: 0.4273935854434967 - trainLoss: 0.4037470519542694\n",
      "cnt: 0 - valLoss: 0.42739301919937134 - trainLoss: 0.40374571084976196\n",
      "cnt: 0 - valLoss: 0.42739251255989075 - trainLoss: 0.4037443995475769\n",
      "cnt: 0 - valLoss: 0.4273921549320221 - trainLoss: 0.40374302864074707\n",
      "cnt: 0 - valLoss: 0.4273916482925415 - trainLoss: 0.4037416875362396\n",
      "cnt: 0 - valLoss: 0.42739108204841614 - trainLoss: 0.4037403166294098\n",
      "cnt: 0 - valLoss: 0.42739078402519226 - trainLoss: 0.40373894572257996\n",
      "cnt: 0 - valLoss: 0.4273901879787445 - trainLoss: 0.4037376344203949\n",
      "cnt: 0 - valLoss: 0.4273896813392639 - trainLoss: 0.40373626351356506\n",
      "cnt: 0 - valLoss: 0.42738938331604004 - trainLoss: 0.4037349224090576\n",
      "cnt: 0 - valLoss: 0.4273888170719147 - trainLoss: 0.4037335515022278\n",
      "cnt: 0 - valLoss: 0.4273885190486908 - trainLoss: 0.4037322402000427\n",
      "cnt: 0 - valLoss: 0.42738795280456543 - trainLoss: 0.4037308692932129\n",
      "cnt: 0 - valLoss: 0.42738741636276245 - trainLoss: 0.40372952818870544\n",
      "cnt: 0 - valLoss: 0.4273871183395386 - trainLoss: 0.4037282168865204\n",
      "cnt: 0 - valLoss: 0.4273865520954132 - trainLoss: 0.40372684597969055\n",
      "cnt: 0 - valLoss: 0.42738598585128784 - trainLoss: 0.4037254750728607\n",
      "cnt: 0 - valLoss: 0.42738568782806396 - trainLoss: 0.40372413396835327\n",
      "cnt: 0 - valLoss: 0.427385151386261 - trainLoss: 0.40372276306152344\n",
      "cnt: 0 - valLoss: 0.4273845851421356 - trainLoss: 0.4037214517593384\n",
      "cnt: 0 - valLoss: 0.42738428711891174 - trainLoss: 0.40372008085250854\n",
      "cnt: 0 - valLoss: 0.4273837208747864 - trainLoss: 0.4037187099456787\n",
      "cnt: 0 - valLoss: 0.427383154630661 - trainLoss: 0.40371736884117126\n",
      "cnt: 0 - valLoss: 0.42738285660743713 - trainLoss: 0.4037160575389862\n",
      "cnt: 0 - valLoss: 0.42738232016563416 - trainLoss: 0.40371468663215637\n",
      "cnt: 0 - valLoss: 0.4273819625377655 - trainLoss: 0.4037133455276489\n",
      "cnt: 0 - valLoss: 0.4273814558982849 - trainLoss: 0.4037120044231415\n",
      "cnt: 0 - valLoss: 0.42738085985183716 - trainLoss: 0.40371066331863403\n",
      "cnt: 0 - valLoss: 0.42738059163093567 - trainLoss: 0.4037092924118042\n",
      "cnt: 0 - valLoss: 0.4273799955844879 - trainLoss: 0.40370798110961914\n",
      "cnt: 0 - valLoss: 0.4273794889450073 - trainLoss: 0.4037066102027893\n",
      "cnt: 0 - valLoss: 0.42737913131713867 - trainLoss: 0.40370526909828186\n",
      "cnt: 0 - valLoss: 0.4273785650730133 - trainLoss: 0.403703898191452\n",
      "cnt: 0 - valLoss: 0.42737826704978943 - trainLoss: 0.4037025272846222\n",
      "cnt: 0 - valLoss: 0.4273776710033417 - trainLoss: 0.40370121598243713\n",
      "cnt: 0 - valLoss: 0.4273771643638611 - trainLoss: 0.4036998748779297\n",
      "cnt: 0 - valLoss: 0.4273768365383148 - trainLoss: 0.40369850397109985\n",
      "cnt: 0 - valLoss: 0.42737630009651184 - trainLoss: 0.4036971926689148\n",
      "cnt: 0 - valLoss: 0.4273757040500641 - trainLoss: 0.40369585156440735\n",
      "cnt: 0 - valLoss: 0.4273754358291626 - trainLoss: 0.4036944508552551\n",
      "cnt: 0 - valLoss: 0.42737483978271484 - trainLoss: 0.4036931097507477\n",
      "cnt: 0 - valLoss: 0.4273745119571686 - trainLoss: 0.40369173884391785\n",
      "cnt: 0 - valLoss: 0.427374005317688 - trainLoss: 0.4036904275417328\n",
      "cnt: 0 - valLoss: 0.4273734390735626 - trainLoss: 0.40368908643722534\n",
      "cnt: 0 - valLoss: 0.42737314105033875 - trainLoss: 0.4036877155303955\n",
      "cnt: 0 - valLoss: 0.427372545003891 - trainLoss: 0.40368640422821045\n",
      "cnt: 0 - valLoss: 0.4273719787597656 - trainLoss: 0.403685063123703\n",
      "cnt: 0 - valLoss: 0.42737168073654175 - trainLoss: 0.40368369221687317\n",
      "cnt: 0 - valLoss: 0.427371084690094 - trainLoss: 0.40368232131004333\n",
      "cnt: 0 - valLoss: 0.4273707866668701 - trainLoss: 0.4036810100078583\n",
      "cnt: 0 - valLoss: 0.42737022042274475 - trainLoss: 0.40367966890335083\n",
      "cnt: 0 - valLoss: 0.4273696541786194 - trainLoss: 0.403678297996521\n",
      "cnt: 0 - valLoss: 0.4273693561553955 - trainLoss: 0.40367692708969116\n",
      "cnt: 0 - valLoss: 0.42736876010894775 - trainLoss: 0.4036756157875061\n",
      "cnt: 0 - valLoss: 0.42736852169036865 - trainLoss: 0.40367427468299866\n",
      "cnt: 0 - valLoss: 0.4273678958415985 - trainLoss: 0.4036729037761688\n",
      "cnt: 0 - valLoss: 0.42736735939979553 - trainLoss: 0.403671532869339\n",
      "cnt: 0 - valLoss: 0.4273670017719269 - trainLoss: 0.40367022156715393\n",
      "cnt: 0 - valLoss: 0.4273664355278015 - trainLoss: 0.4036688506603241\n",
      "cnt: 0 - valLoss: 0.42736583948135376 - trainLoss: 0.40366750955581665\n",
      "cnt: 0 - valLoss: 0.4273655414581299 - trainLoss: 0.4036661982536316\n",
      "cnt: 0 - valLoss: 0.4273650050163269 - trainLoss: 0.40366482734680176\n",
      "cnt: 0 - valLoss: 0.42736464738845825 - trainLoss: 0.4036634862422943\n",
      "cnt: 0 - valLoss: 0.4273640811443329 - trainLoss: 0.4036621153354645\n",
      "cnt: 0 - valLoss: 0.4273635447025299 - trainLoss: 0.4036608040332794\n",
      "cnt: 0 - valLoss: 0.42736318707466125 - trainLoss: 0.4036594331264496\n",
      "cnt: 0 - valLoss: 0.4273626208305359 - trainLoss: 0.40365809202194214\n",
      "cnt: 0 - valLoss: 0.427362322807312 - trainLoss: 0.4036567211151123\n",
      "cnt: 0 - valLoss: 0.42736172676086426 - trainLoss: 0.40365540981292725\n",
      "cnt: 0 - valLoss: 0.4273611903190613 - trainLoss: 0.4036540687084198\n",
      "cnt: 0 - valLoss: 0.4273608922958374 - trainLoss: 0.40365269780158997\n",
      "cnt: 0 - valLoss: 0.42736026644706726 - trainLoss: 0.40365132689476013\n",
      "cnt: 0 - valLoss: 0.427359938621521 - trainLoss: 0.4036500155925751\n",
      "cnt: 0 - valLoss: 0.4273594319820404 - trainLoss: 0.4036486744880676\n",
      "cnt: 0 - valLoss: 0.42735880613327026 - trainLoss: 0.4036473035812378\n",
      "cnt: 0 - valLoss: 0.4273585081100464 - trainLoss: 0.40364599227905273\n",
      "cnt: 0 - valLoss: 0.42735791206359863 - trainLoss: 0.4036446213722229\n",
      "cnt: 0 - valLoss: 0.42735758423805237 - trainLoss: 0.40364328026771545\n",
      "cnt: 0 - valLoss: 0.427357017993927 - trainLoss: 0.4036419093608856\n",
      "cnt: 0 - valLoss: 0.4273565113544464 - trainLoss: 0.40364059805870056\n",
      "cnt: 0 - valLoss: 0.42735615372657776 - trainLoss: 0.4036392271518707\n",
      "cnt: 0 - valLoss: 0.42735555768013 - trainLoss: 0.4036378860473633\n",
      "cnt: 0 - valLoss: 0.42735522985458374 - trainLoss: 0.40363654494285583\n",
      "cnt: 0 - valLoss: 0.42735469341278076 - trainLoss: 0.4036352038383484\n",
      "cnt: 0 - valLoss: 0.427354097366333 - trainLoss: 0.40363386273384094\n",
      "cnt: 0 - valLoss: 0.42735376954078674 - trainLoss: 0.4036325216293335\n",
      "cnt: 0 - valLoss: 0.42735326290130615 - trainLoss: 0.40363118052482605\n",
      "cnt: 0 - valLoss: 0.4273529052734375 - trainLoss: 0.4036298096179962\n",
      "cnt: 0 - valLoss: 0.42735230922698975 - trainLoss: 0.40362849831581116\n",
      "cnt: 0 - valLoss: 0.4273517429828644 - trainLoss: 0.4036271274089813\n",
      "cnt: 0 - valLoss: 0.4273514151573181 - trainLoss: 0.4036257863044739\n",
      "cnt: 0 - valLoss: 0.42735084891319275 - trainLoss: 0.4036244750022888\n",
      "cnt: 0 - valLoss: 0.4273504912853241 - trainLoss: 0.403623104095459\n",
      "cnt: 0 - valLoss: 0.4273499548435211 - trainLoss: 0.40362176299095154\n",
      "cnt: 0 - valLoss: 0.42734938859939575 - trainLoss: 0.4036204516887665\n",
      "cnt: 0 - valLoss: 0.4273490607738495 - trainLoss: 0.40361908078193665\n",
      "cnt: 0 - valLoss: 0.42734843492507935 - trainLoss: 0.4036177098751068\n",
      "cnt: 0 - valLoss: 0.42734813690185547 - trainLoss: 0.40361636877059937\n",
      "cnt: 0 - valLoss: 0.4273475408554077 - trainLoss: 0.4036150574684143\n",
      "cnt: 0 - valLoss: 0.42734700441360474 - trainLoss: 0.4036136865615845\n",
      "cnt: 0 - valLoss: 0.4273466467857361 - trainLoss: 0.403612345457077\n",
      "cnt: 0 - valLoss: 0.4273460805416107 - trainLoss: 0.40361103415489197\n",
      "cnt: 0 - valLoss: 0.42734575271606445 - trainLoss: 0.40360966324806213\n",
      "cnt: 0 - valLoss: 0.4273451864719391 - trainLoss: 0.4036083221435547\n",
      "cnt: 0 - valLoss: 0.42734459042549133 - trainLoss: 0.40360701084136963\n",
      "cnt: 0 - valLoss: 0.42734429240226746 - trainLoss: 0.4036056399345398\n",
      "cnt: 0 - valLoss: 0.4273436963558197 - trainLoss: 0.40360429883003235\n",
      "cnt: 0 - valLoss: 0.42734333872795105 - trainLoss: 0.4036029279232025\n",
      "cnt: 0 - valLoss: 0.42734280228614807 - trainLoss: 0.40360161662101746\n",
      "cnt: 0 - valLoss: 0.4273424446582794 - trainLoss: 0.4036002457141876\n",
      "cnt: 0 - valLoss: 0.42734187841415405 - trainLoss: 0.4035988748073578\n",
      "cnt: 0 - valLoss: 0.4273412823677063 - trainLoss: 0.40359753370285034\n",
      "cnt: 0 - valLoss: 0.4273409843444824 - trainLoss: 0.4035962224006653\n",
      "cnt: 0 - valLoss: 0.42734038829803467 - trainLoss: 0.40359485149383545\n",
      "cnt: 0 - valLoss: 0.4273400604724884 - trainLoss: 0.4035935699939728\n",
      "cnt: 0 - valLoss: 0.42733949422836304 - trainLoss: 0.40359219908714294\n",
      "cnt: 0 - valLoss: 0.42733892798423767 - trainLoss: 0.4035908579826355\n",
      "cnt: 0 - valLoss: 0.4273386001586914 - trainLoss: 0.40358948707580566\n",
      "cnt: 0 - valLoss: 0.42733800411224365 - trainLoss: 0.4035881757736206\n",
      "cnt: 0 - valLoss: 0.4273376762866974 - trainLoss: 0.40358680486679077\n",
      "cnt: 0 - valLoss: 0.4273371398448944 - trainLoss: 0.40358543395996094\n",
      "cnt: 0 - valLoss: 0.4273366928100586 - trainLoss: 0.4035840928554535\n",
      "cnt: 0 - valLoss: 0.427336186170578 - trainLoss: 0.40358278155326843\n",
      "cnt: 0 - valLoss: 0.42733559012413025 - trainLoss: 0.4035814106464386\n",
      "cnt: 0 - valLoss: 0.42733529210090637 - trainLoss: 0.40358006954193115\n",
      "cnt: 0 - valLoss: 0.427334725856781 - trainLoss: 0.4035787582397461\n",
      "cnt: 0 - valLoss: 0.42733439803123474 - trainLoss: 0.40357738733291626\n",
      "cnt: 0 - valLoss: 0.427333801984787 - trainLoss: 0.4035760462284088\n",
      "cnt: 0 - valLoss: 0.42733344435691833 - trainLoss: 0.40357473492622375\n",
      "cnt: 0 - valLoss: 0.42733287811279297 - trainLoss: 0.4035733640193939\n",
      "cnt: 0 - valLoss: 0.4273322820663452 - trainLoss: 0.4035720229148865\n",
      "cnt: 0 - valLoss: 0.42733198404312134 - trainLoss: 0.4035707116127014\n",
      "cnt: 0 - valLoss: 0.4273313879966736 - trainLoss: 0.4035693407058716\n",
      "cnt: 0 - valLoss: 0.4273310601711273 - trainLoss: 0.40356796979904175\n",
      "cnt: 0 - valLoss: 0.42733052372932434 - trainLoss: 0.4035666882991791\n",
      "cnt: 0 - valLoss: 0.4273301362991333 - trainLoss: 0.40356531739234924\n",
      "cnt: 0 - valLoss: 0.4273295998573303 - trainLoss: 0.4035639464855194\n",
      "cnt: 0 - valLoss: 0.4273289740085602 - trainLoss: 0.40356260538101196\n",
      "cnt: 0 - valLoss: 0.4273286461830139 - trainLoss: 0.4035612940788269\n",
      "cnt: 0 - valLoss: 0.42732805013656616 - trainLoss: 0.40355992317199707\n",
      "cnt: 0 - valLoss: 0.4273276925086975 - trainLoss: 0.4035585820674896\n",
      "cnt: 0 - valLoss: 0.42732709646224976 - trainLoss: 0.40355727076530457\n",
      "cnt: 0 - valLoss: 0.42732685804367065 - trainLoss: 0.40355589985847473\n",
      "cnt: 0 - valLoss: 0.4273262023925781 - trainLoss: 0.4035545289516449\n",
      "cnt: 0 - valLoss: 0.42732563614845276 - trainLoss: 0.4035532474517822\n",
      "cnt: 0 - valLoss: 0.4273253083229065 - trainLoss: 0.4035518765449524\n",
      "cnt: 0 - valLoss: 0.42732471227645874 - trainLoss: 0.4035505950450897\n",
      "cnt: 0 - valLoss: 0.4273243546485901 - trainLoss: 0.4035491645336151\n",
      "cnt: 0 - valLoss: 0.42732375860214233 - trainLoss: 0.40354785323143005\n",
      "cnt: 0 - valLoss: 0.42732346057891846 - trainLoss: 0.4035464823246002\n",
      "cnt: 0 - valLoss: 0.4273228049278259 - trainLoss: 0.40354520082473755\n",
      "cnt: 0 - valLoss: 0.42732229828834534 - trainLoss: 0.4035438299179077\n",
      "cnt: 0 - valLoss: 0.4273219704627991 - trainLoss: 0.40354248881340027\n",
      "cnt: 0 - valLoss: 0.42732134461402893 - trainLoss: 0.4035411775112152\n",
      "cnt: 0 - valLoss: 0.42732101678848267 - trainLoss: 0.4035398066043854\n",
      "cnt: 0 - valLoss: 0.4273204207420349 - trainLoss: 0.40353846549987793\n",
      "cnt: 0 - valLoss: 0.42732006311416626 - trainLoss: 0.40353715419769287\n",
      "cnt: 0 - valLoss: 0.4273194670677185 - trainLoss: 0.40353578329086304\n",
      "cnt: 0 - valLoss: 0.42731913924217224 - trainLoss: 0.40353450179100037\n",
      "cnt: 0 - valLoss: 0.4273185431957245 - trainLoss: 0.40353307127952576\n",
      "cnt: 0 - valLoss: 0.42731818556785583 - trainLoss: 0.4035317599773407\n",
      "cnt: 0 - valLoss: 0.42731761932373047 - trainLoss: 0.40353041887283325\n",
      "cnt: 0 - valLoss: 0.4273170232772827 - trainLoss: 0.4035291075706482\n",
      "cnt: 0 - valLoss: 0.42731669545173645 - trainLoss: 0.40352773666381836\n",
      "cnt: 0 - valLoss: 0.4273161292076111 - trainLoss: 0.4035263955593109\n",
      "cnt: 0 - valLoss: 0.4273158013820648 - trainLoss: 0.40352508425712585\n",
      "cnt: 0 - valLoss: 0.4273151755332947 - trainLoss: 0.403523713350296\n",
      "cnt: 0 - valLoss: 0.4273148477077484 - trainLoss: 0.4035223722457886\n",
      "cnt: 0 - valLoss: 0.42731425166130066 - trainLoss: 0.40352100133895874\n",
      "cnt: 0 - valLoss: 0.427313894033432 - trainLoss: 0.4035196900367737\n",
      "cnt: 0 - valLoss: 0.42731329798698425 - trainLoss: 0.40351831912994385\n",
      "cnt: 0 - valLoss: 0.4273127317428589 - trainLoss: 0.4035170376300812\n",
      "cnt: 0 - valLoss: 0.4273124039173126 - trainLoss: 0.40351566672325134\n",
      "cnt: 0 - valLoss: 0.42731180787086487 - trainLoss: 0.4035143256187439\n",
      "cnt: 0 - valLoss: 0.4273114502429962 - trainLoss: 0.40351301431655884\n",
      "cnt: 0 - valLoss: 0.42731091380119324 - trainLoss: 0.403511643409729\n",
      "cnt: 0 - valLoss: 0.4273105561733246 - trainLoss: 0.40351030230522156\n",
      "cnt: 0 - valLoss: 0.42730996012687683 - trainLoss: 0.4035089313983917\n",
      "cnt: 0 - valLoss: 0.4273096024990082 - trainLoss: 0.40350762009620667\n",
      "cnt: 0 - valLoss: 0.4273090064525604 - trainLoss: 0.4035062789916992\n",
      "cnt: 0 - valLoss: 0.42730841040611267 - trainLoss: 0.40350496768951416\n",
      "cnt: 0 - valLoss: 0.42730799317359924 - trainLoss: 0.4035035967826843\n",
      "cnt: 0 - valLoss: 0.4273075461387634 - trainLoss: 0.40350231528282166\n",
      "cnt: 0 - valLoss: 0.42730712890625 - trainLoss: 0.4035009443759918\n",
      "cnt: 0 - valLoss: 0.4273064434528351 - trainLoss: 0.4034996032714844\n",
      "cnt: 0 - valLoss: 0.42730602622032166 - trainLoss: 0.4034982919692993\n",
      "cnt: 0 - valLoss: 0.42730557918548584 - trainLoss: 0.40349695086479187\n",
      "cnt: 0 - valLoss: 0.42730510234832764 - trainLoss: 0.4034956395626068\n",
      "cnt: 0 - valLoss: 0.4273046851158142 - trainLoss: 0.403494268655777\n",
      "cnt: 0 - valLoss: 0.4273039996623993 - trainLoss: 0.4034929871559143\n",
      "cnt: 0 - valLoss: 0.42730358242988586 - trainLoss: 0.4034916162490845\n",
      "cnt: 0 - valLoss: 0.42730313539505005 - trainLoss: 0.4034903347492218\n",
      "cnt: 0 - valLoss: 0.42730265855789185 - trainLoss: 0.40348896384239197\n",
      "cnt: 0 - valLoss: 0.4273022711277008 - trainLoss: 0.4034876227378845\n",
      "cnt: 0 - valLoss: 0.4273015558719635 - trainLoss: 0.40348631143569946\n",
      "cnt: 0 - valLoss: 0.4273011386394501 - trainLoss: 0.40348494052886963\n",
      "cnt: 0 - valLoss: 0.42730069160461426 - trainLoss: 0.40348362922668457\n",
      "cnt: 0 - valLoss: 0.42730027437210083 - trainLoss: 0.4034822881221771\n",
      "cnt: 0 - valLoss: 0.4272998571395874 - trainLoss: 0.40348097681999207\n",
      "cnt: 0 - valLoss: 0.4272994101047516 - trainLoss: 0.40347960591316223\n",
      "cnt: 0 - valLoss: 0.42729872465133667 - trainLoss: 0.40347832441329956\n",
      "cnt: 0 - valLoss: 0.42729824781417847 - trainLoss: 0.4034769535064697\n",
      "cnt: 0 - valLoss: 0.4272978603839874 - trainLoss: 0.40347567200660706\n",
      "cnt: 0 - valLoss: 0.4272974133491516 - trainLoss: 0.4034743010997772\n",
      "cnt: 0 - valLoss: 0.4272969365119934 - trainLoss: 0.4034729301929474\n",
      "cnt: 0 - valLoss: 0.4272962808609009 - trainLoss: 0.4034716486930847\n",
      "cnt: 0 - valLoss: 0.42729586362838745 - trainLoss: 0.40347033739089966\n",
      "cnt: 0 - valLoss: 0.42729541659355164 - trainLoss: 0.4034689962863922\n",
      "cnt: 0 - valLoss: 0.4272949695587158 - trainLoss: 0.4034676253795624\n",
      "cnt: 0 - valLoss: 0.42729452252388 - trainLoss: 0.4034663438796997\n",
      "cnt: 0 - valLoss: 0.4272938370704651 - trainLoss: 0.4034649729728699\n",
      "cnt: 0 - valLoss: 0.42729341983795166 - trainLoss: 0.4034636616706848\n",
      "cnt: 0 - valLoss: 0.42729297280311584 - trainLoss: 0.40346232056617737\n",
      "cnt: 0 - valLoss: 0.4272925555706024 - trainLoss: 0.4034610092639923\n",
      "cnt: 0 - valLoss: 0.427292138338089 - trainLoss: 0.40345966815948486\n",
      "cnt: 0 - valLoss: 0.4272914528846741 - trainLoss: 0.4034583568572998\n",
      "cnt: 0 - valLoss: 0.42729100584983826 - trainLoss: 0.40345701575279236\n",
      "cnt: 0 - valLoss: 0.42729058861732483 - trainLoss: 0.4034557044506073\n",
      "cnt: 0 - valLoss: 0.4272901117801666 - trainLoss: 0.40345436334609985\n",
      "cnt: 0 - valLoss: 0.4272896945476532 - trainLoss: 0.40345299243927\n",
      "cnt: 0 - valLoss: 0.4272892475128174 - trainLoss: 0.40345168113708496\n",
      "cnt: 0 - valLoss: 0.42728856205940247 - trainLoss: 0.4034503996372223\n",
      "cnt: 0 - valLoss: 0.42728814482688904 - trainLoss: 0.40344905853271484\n",
      "cnt: 0 - valLoss: 0.42728766798973083 - trainLoss: 0.403447687625885\n",
      "cnt: 0 - valLoss: 0.4272872805595398 - trainLoss: 0.40344637632369995\n",
      "cnt: 0 - valLoss: 0.4272868037223816 - trainLoss: 0.4034450650215149\n",
      "cnt: 0 - valLoss: 0.42728617787361145 - trainLoss: 0.40344372391700745\n",
      "cnt: 0 - valLoss: 0.42728573083877563 - trainLoss: 0.4034423828125\n",
      "cnt: 0 - valLoss: 0.4272852838039398 - trainLoss: 0.40344104170799255\n",
      "cnt: 0 - valLoss: 0.427284836769104 - trainLoss: 0.4034397304058075\n",
      "cnt: 0 - valLoss: 0.4272844195365906 - trainLoss: 0.4034384489059448\n",
      "cnt: 0 - valLoss: 0.42728373408317566 - trainLoss: 0.403437077999115\n",
      "cnt: 0 - valLoss: 0.42728328704833984 - trainLoss: 0.40343573689460754\n",
      "cnt: 0 - valLoss: 0.4272828996181488 - trainLoss: 0.4034344255924225\n",
      "cnt: 0 - valLoss: 0.4272824823856354 - trainLoss: 0.4034331440925598\n",
      "cnt: 0 - valLoss: 0.4272820055484772 - trainLoss: 0.40343177318573\n",
      "cnt: 0 - valLoss: 0.42728158831596375 - trainLoss: 0.40343043208122253\n",
      "cnt: 0 - valLoss: 0.4272809326648712 - trainLoss: 0.4034291207790375\n",
      "cnt: 0 - valLoss: 0.4272805154323578 - trainLoss: 0.40342777967453003\n",
      "cnt: 0 - valLoss: 0.427280068397522 - trainLoss: 0.40342646837234497\n",
      "cnt: 0 - valLoss: 0.42727965116500854 - trainLoss: 0.4034251272678375\n",
      "cnt: 0 - valLoss: 0.42727917432785034 - trainLoss: 0.40342381596565247\n",
      "cnt: 0 - valLoss: 0.4272785484790802 - trainLoss: 0.403422474861145\n",
      "cnt: 0 - valLoss: 0.4272781014442444 - trainLoss: 0.40342116355895996\n",
      "cnt: 0 - valLoss: 0.42727765440940857 - trainLoss: 0.4034198224544525\n",
      "cnt: 0 - valLoss: 0.42727726697921753 - trainLoss: 0.40341851115226746\n",
      "cnt: 0 - valLoss: 0.4272767901420593 - trainLoss: 0.40341717004776\n",
      "cnt: 0 - valLoss: 0.4272761344909668 - trainLoss: 0.40341585874557495\n",
      "cnt: 0 - valLoss: 0.42727571725845337 - trainLoss: 0.4034145176410675\n",
      "cnt: 0 - valLoss: 0.42727527022361755 - trainLoss: 0.40341320633888245\n",
      "cnt: 0 - valLoss: 0.4272748529911041 - trainLoss: 0.403411865234375\n",
      "cnt: 0 - valLoss: 0.4272744357585907 - trainLoss: 0.40341052412986755\n",
      "cnt: 0 - valLoss: 0.4272739887237549 - trainLoss: 0.4034092426300049\n",
      "cnt: 0 - valLoss: 0.4272732734680176 - trainLoss: 0.40340787172317505\n",
      "cnt: 0 - valLoss: 0.42727288603782654 - trainLoss: 0.4034065902233124\n",
      "cnt: 0 - valLoss: 0.4272724390029907 - trainLoss: 0.40340521931648254\n",
      "cnt: 0 - valLoss: 0.42727208137512207 - trainLoss: 0.4034039080142975\n",
      "cnt: 0 - valLoss: 0.4272715449333191 - trainLoss: 0.4034026265144348\n",
      "cnt: 0 - valLoss: 0.42727091908454895 - trainLoss: 0.403401255607605\n",
      "cnt: 0 - valLoss: 0.42727047204971313 - trainLoss: 0.40339991450309753\n",
      "cnt: 0 - valLoss: 0.4272700548171997 - trainLoss: 0.4033986032009125\n",
      "cnt: 0 - valLoss: 0.42726966738700867 - trainLoss: 0.40339726209640503\n",
      "cnt: 0 - valLoss: 0.42726919054985046 - trainLoss: 0.40339598059654236\n",
      "cnt: 0 - valLoss: 0.42726877331733704 - trainLoss: 0.4033946096897125\n",
      "cnt: 0 - valLoss: 0.4272680878639221 - trainLoss: 0.40339329838752747\n",
      "cnt: 0 - valLoss: 0.4272676408290863 - trainLoss: 0.4033920168876648\n",
      "cnt: 0 - valLoss: 0.42726725339889526 - trainLoss: 0.40339064598083496\n",
      "cnt: 0 - valLoss: 0.42726680636405945 - trainLoss: 0.4033893048763275\n",
      "cnt: 0 - valLoss: 0.42726635932922363 - trainLoss: 0.40338799357414246\n",
      "cnt: 0 - valLoss: 0.4272656738758087 - trainLoss: 0.403386652469635\n",
      "cnt: 0 - valLoss: 0.4272652864456177 - trainLoss: 0.40338537096977234\n",
      "cnt: 0 - valLoss: 0.42726486921310425 - trainLoss: 0.4033840000629425\n",
      "cnt: 0 - valLoss: 0.42726439237594604 - trainLoss: 0.40338271856307983\n",
      "cnt: 0 - valLoss: 0.4272639751434326 - trainLoss: 0.4033813774585724\n",
      "cnt: 0 - valLoss: 0.4272635281085968 - trainLoss: 0.40338006615638733\n",
      "cnt: 0 - valLoss: 0.42726290225982666 - trainLoss: 0.4033786952495575\n",
      "cnt: 0 - valLoss: 0.42726242542266846 - trainLoss: 0.4033774137496948\n",
      "cnt: 0 - valLoss: 0.42726200819015503 - trainLoss: 0.4033760726451874\n",
      "cnt: 0 - valLoss: 0.427261620759964 - trainLoss: 0.4033747613430023\n",
      "cnt: 0 - valLoss: 0.4272611439228058 - trainLoss: 0.4033734202384949\n",
      "cnt: 0 - valLoss: 0.42726069688796997 - trainLoss: 0.4033721089363098\n",
      "cnt: 0 - valLoss: 0.42726001143455505 - trainLoss: 0.40337076783180237\n",
      "cnt: 0 - valLoss: 0.4272595942020416 - trainLoss: 0.4033694565296173\n",
      "cnt: 0 - valLoss: 0.4272592067718506 - trainLoss: 0.4033680856227875\n",
      "cnt: 0 - valLoss: 0.4272587299346924 - trainLoss: 0.4033668041229248\n",
      "cnt: 0 - valLoss: 0.4272582530975342 - trainLoss: 0.40336546301841736\n",
      "cnt: 0 - valLoss: 0.42725762724876404 - trainLoss: 0.4033641517162323\n",
      "cnt: 0 - valLoss: 0.4272571802139282 - trainLoss: 0.40336287021636963\n",
      "cnt: 0 - valLoss: 0.4272567629814148 - trainLoss: 0.4033614993095398\n",
      "cnt: 0 - valLoss: 0.42725634574890137 - trainLoss: 0.40336015820503235\n",
      "cnt: 0 - valLoss: 0.42725589871406555 - trainLoss: 0.4033588469028473\n",
      "cnt: 0 - valLoss: 0.4272555112838745 - trainLoss: 0.40335753560066223\n",
      "cnt: 0 - valLoss: 0.4272548258304596 - trainLoss: 0.4033561944961548\n",
      "cnt: 0 - valLoss: 0.42725440859794617 - trainLoss: 0.40335485339164734\n",
      "cnt: 0 - valLoss: 0.42725399136543274 - trainLoss: 0.40335357189178467\n",
      "cnt: 0 - valLoss: 0.42725351452827454 - trainLoss: 0.4033522307872772\n",
      "cnt: 0 - valLoss: 0.4272531270980835 - trainLoss: 0.4033508598804474\n",
      "cnt: 0 - valLoss: 0.4272526502609253 - trainLoss: 0.4033495783805847\n",
      "cnt: 0 - valLoss: 0.42725199460983276 - trainLoss: 0.40334826707839966\n",
      "cnt: 0 - valLoss: 0.42725157737731934 - trainLoss: 0.4033469259738922\n",
      "cnt: 0 - valLoss: 0.42725124955177307 - trainLoss: 0.40334561467170715\n",
      "cnt: 0 - valLoss: 0.4272509813308716 - trainLoss: 0.4033442735671997\n",
      "cnt: 0 - valLoss: 0.4272504150867462 - trainLoss: 0.40334299206733704\n",
      "cnt: 0 - valLoss: 0.42724987864494324 - trainLoss: 0.403341680765152\n",
      "cnt: 0 - valLoss: 0.427249550819397 - trainLoss: 0.40334033966064453\n",
      "cnt: 0 - valLoss: 0.427249014377594 - trainLoss: 0.4033390283584595\n",
      "cnt: 0 - valLoss: 0.4272487163543701 - trainLoss: 0.403337687253952\n",
      "cnt: 0 - valLoss: 0.42724815011024475 - trainLoss: 0.40333637595176697\n",
      "cnt: 0 - valLoss: 0.42724764347076416 - trainLoss: 0.4033350348472595\n",
      "cnt: 0 - valLoss: 0.4272472858428955 - trainLoss: 0.40333375334739685\n",
      "cnt: 0 - valLoss: 0.4272467792034149 - trainLoss: 0.4033324122428894\n",
      "cnt: 0 - valLoss: 0.42724648118019104 - trainLoss: 0.40333110094070435\n",
      "cnt: 0 - valLoss: 0.4272459149360657 - trainLoss: 0.4033297598361969\n",
      "cnt: 0 - valLoss: 0.4272455871105194 - trainLoss: 0.40332844853401184\n",
      "cnt: 0 - valLoss: 0.42724505066871643 - trainLoss: 0.4033271074295044\n",
      "cnt: 0 - valLoss: 0.42724454402923584 - trainLoss: 0.40332579612731934\n",
      "cnt: 0 - valLoss: 0.4272443652153015 - trainLoss: 0.40332451462745667\n",
      "cnt: 0 - valLoss: 0.4272439479827881 - trainLoss: 0.4033231735229492\n",
      "cnt: 0 - valLoss: 0.42724353075027466 - trainLoss: 0.40332189202308655\n",
      "cnt: 0 - valLoss: 0.42724308371543884 - trainLoss: 0.4033205807209015\n",
      "cnt: 0 - valLoss: 0.4272426664829254 - trainLoss: 0.40331923961639404\n",
      "cnt: 0 - valLoss: 0.427242249250412 - trainLoss: 0.403317928314209\n",
      "cnt: 0 - valLoss: 0.42724180221557617 - trainLoss: 0.40331658720970154\n",
      "cnt: 0 - valLoss: 0.42724138498306274 - trainLoss: 0.4033152759075165\n",
      "cnt: 0 - valLoss: 0.42724093794822693 - trainLoss: 0.4033139646053314\n",
      "cnt: 0 - valLoss: 0.4272405207157135 - trainLoss: 0.40331265330314636\n",
      "cnt: 0 - valLoss: 0.4272401034832001 - trainLoss: 0.4033113121986389\n",
      "cnt: 0 - valLoss: 0.42723965644836426 - trainLoss: 0.40331003069877625\n",
      "cnt: 0 - valLoss: 0.42723923921585083 - trainLoss: 0.4033086597919464\n",
      "cnt: 0 - valLoss: 0.4272388219833374 - trainLoss: 0.40330737829208374\n",
      "cnt: 0 - valLoss: 0.4272383749485016 - trainLoss: 0.4033060669898987\n",
      "cnt: 0 - valLoss: 0.42723795771598816 - trainLoss: 0.40330472588539124\n",
      "cnt: 0 - valLoss: 0.42723751068115234 - trainLoss: 0.4033034145832062\n",
      "cnt: 0 - valLoss: 0.4272370934486389 - trainLoss: 0.4033021330833435\n",
      "cnt: 0 - valLoss: 0.4272366762161255 - trainLoss: 0.40330079197883606\n",
      "cnt: 0 - valLoss: 0.4272362291812897 - trainLoss: 0.403299480676651\n",
      "cnt: 0 - valLoss: 0.42723581194877625 - trainLoss: 0.40329813957214355\n",
      "cnt: 0 - valLoss: 0.42723536491394043 - trainLoss: 0.4032968580722809\n",
      "cnt: 0 - valLoss: 0.4272349178791046 - trainLoss: 0.40329551696777344\n",
      "cnt: 0 - valLoss: 0.4272345304489136 - trainLoss: 0.4032942056655884\n",
      "cnt: 0 - valLoss: 0.42723408341407776 - trainLoss: 0.40329286456108093\n",
      "cnt: 0 - valLoss: 0.42723360657691956 - trainLoss: 0.40329158306121826\n",
      "cnt: 0 - valLoss: 0.42723348736763 - trainLoss: 0.4032902717590332\n",
      "cnt: 0 - valLoss: 0.4272330105304718 - trainLoss: 0.40328899025917053\n",
      "cnt: 0 - valLoss: 0.4272325932979584 - trainLoss: 0.4032876193523407\n",
      "cnt: 0 - valLoss: 0.42723217606544495 - trainLoss: 0.403286337852478\n",
      "cnt: 0 - valLoss: 0.4272317886352539 - trainLoss: 0.4032849669456482\n",
      "cnt: 0 - valLoss: 0.4272313416004181 - trainLoss: 0.4032836854457855\n",
      "cnt: 0 - valLoss: 0.4272308647632599 - trainLoss: 0.40328240394592285\n",
      "cnt: 0 - valLoss: 0.42723044753074646 - trainLoss: 0.4032810628414154\n",
      "cnt: 0 - valLoss: 0.42723003029823303 - trainLoss: 0.40327972173690796\n",
      "cnt: 0 - valLoss: 0.4272295832633972 - trainLoss: 0.4032784402370453\n",
      "cnt: 0 - valLoss: 0.4272291958332062 - trainLoss: 0.40327712893486023\n",
      "cnt: 0 - valLoss: 0.42722877860069275 - trainLoss: 0.4032757878303528\n",
      "cnt: 0 - valLoss: 0.4272283613681793 - trainLoss: 0.4032744765281677\n",
      "cnt: 0 - valLoss: 0.42722782492637634 - trainLoss: 0.4032731354236603\n",
      "cnt: 0 - valLoss: 0.4272274374961853 - trainLoss: 0.4032718539237976\n",
      "cnt: 0 - valLoss: 0.4272270202636719 - trainLoss: 0.40327054262161255\n",
      "cnt: 0 - valLoss: 0.42722654342651367 - trainLoss: 0.4032692015171051\n",
      "cnt: 0 - valLoss: 0.42722615599632263 - trainLoss: 0.40326792001724243\n",
      "cnt: 0 - valLoss: 0.42722567915916443 - trainLoss: 0.40326663851737976\n",
      "cnt: 0 - valLoss: 0.427225261926651 - trainLoss: 0.4032652676105499\n",
      "cnt: 0 - valLoss: 0.4272248446941376 - trainLoss: 0.40326395630836487\n",
      "cnt: 0 - valLoss: 0.42722439765930176 - trainLoss: 0.4032626450061798\n",
      "cnt: 0 - valLoss: 0.42722398042678833 - trainLoss: 0.40326133370399475\n",
      "cnt: 0 - valLoss: 0.4272235631942749 - trainLoss: 0.4032599925994873\n",
      "cnt: 0 - valLoss: 0.4272230863571167 - trainLoss: 0.40325868129730225\n",
      "cnt: 0 - valLoss: 0.42722293734550476 - trainLoss: 0.4032573997974396\n",
      "cnt: 0 - valLoss: 0.42722249031066895 - trainLoss: 0.40325602889060974\n",
      "cnt: 0 - valLoss: 0.42722204327583313 - trainLoss: 0.40325474739074707\n",
      "cnt: 0 - valLoss: 0.4272216260433197 - trainLoss: 0.4032534658908844\n",
      "cnt: 0 - valLoss: 0.4272211790084839 - trainLoss: 0.40325212478637695\n",
      "cnt: 0 - valLoss: 0.42722076177597046 - trainLoss: 0.4032508134841919\n",
      "cnt: 0 - valLoss: 0.42722028493881226 - trainLoss: 0.4032495319843292\n",
      "cnt: 0 - valLoss: 0.42721986770629883 - trainLoss: 0.4032481610774994\n",
      "cnt: 0 - valLoss: 0.427219420671463 - trainLoss: 0.4032468795776367\n",
      "cnt: 0 - valLoss: 0.4272190034389496 - trainLoss: 0.4032455384731293\n",
      "cnt: 0 - valLoss: 0.42721858620643616 - trainLoss: 0.4032441973686218\n",
      "cnt: 0 - valLoss: 0.42721813917160034 - trainLoss: 0.40324291586875916\n",
      "cnt: 0 - valLoss: 0.4272177219390869 - trainLoss: 0.4032416045665741\n",
      "cnt: 0 - valLoss: 0.4272173047065735 - trainLoss: 0.40324026346206665\n",
      "cnt: 0 - valLoss: 0.42721685767173767 - trainLoss: 0.403238981962204\n",
      "cnt: 0 - valLoss: 0.4272163510322571 - trainLoss: 0.4032376706600189\n",
      "cnt: 0 - valLoss: 0.42721596360206604 - trainLoss: 0.4032363295555115\n",
      "cnt: 0 - valLoss: 0.42721548676490784 - trainLoss: 0.4032350182533264\n",
      "cnt: 0 - valLoss: 0.4272150695323944 - trainLoss: 0.40323373675346375\n",
      "cnt: 0 - valLoss: 0.4272146224975586 - trainLoss: 0.4032323956489563\n",
      "cnt: 0 - valLoss: 0.42721447348594666 - trainLoss: 0.40323108434677124\n",
      "cnt: 0 - valLoss: 0.42721402645111084 - trainLoss: 0.4032297432422638\n",
      "cnt: 0 - valLoss: 0.4272136092185974 - trainLoss: 0.4032284617424011\n",
      "cnt: 0 - valLoss: 0.4272131621837616 - trainLoss: 0.40322718024253845\n",
      "cnt: 0 - valLoss: 0.4272127151489258 - trainLoss: 0.4032258093357086\n",
      "cnt: 0 - valLoss: 0.42721226811408997 - trainLoss: 0.40322452783584595\n",
      "cnt: 0 - valLoss: 0.42721185088157654 - trainLoss: 0.4032232463359833\n",
      "cnt: 0 - valLoss: 0.42721137404441833 - trainLoss: 0.40322190523147583\n",
      "cnt: 0 - valLoss: 0.4272109568119049 - trainLoss: 0.40322059392929077\n",
      "cnt: 0 - valLoss: 0.42720967531204224 - trainLoss: 0.4032192528247833\n",
      "cnt: 0 - valLoss: 0.4272085428237915 - trainLoss: 0.4032178819179535\n",
      "cnt: 0 - valLoss: 0.42720746994018555 - trainLoss: 0.40321657061576843\n",
      "cnt: 0 - valLoss: 0.42720645666122437 - trainLoss: 0.4032151997089386\n",
      "cnt: 0 - valLoss: 0.4272053837776184 - trainLoss: 0.40321382880210876\n",
      "cnt: 0 - valLoss: 0.42720434069633484 - trainLoss: 0.40321245789527893\n",
      "cnt: 0 - valLoss: 0.42720332741737366 - trainLoss: 0.4032111167907715\n",
      "cnt: 0 - valLoss: 0.4272022843360901 - trainLoss: 0.40320974588394165\n",
      "cnt: 0 - valLoss: 0.4272012710571289 - trainLoss: 0.4032084345817566\n",
      "cnt: 0 - valLoss: 0.4272002875804901 - trainLoss: 0.403207004070282\n",
      "cnt: 0 - valLoss: 0.42719924449920654 - trainLoss: 0.4032056927680969\n",
      "cnt: 0 - valLoss: 0.42719826102256775 - trainLoss: 0.4032043516635895\n",
      "cnt: 0 - valLoss: 0.4271972179412842 - trainLoss: 0.4032030403614044\n",
      "cnt: 0 - valLoss: 0.427196204662323 - trainLoss: 0.4032016694545746\n",
      "cnt: 0 - valLoss: 0.42719516158103943 - trainLoss: 0.40320029854774475\n",
      "cnt: 0 - valLoss: 0.42719414830207825 - trainLoss: 0.4031989276409149\n",
      "cnt: 0 - valLoss: 0.42719319462776184 - trainLoss: 0.4031975567340851\n",
      "cnt: 0 - valLoss: 0.4271922707557678 - trainLoss: 0.40319621562957764\n",
      "cnt: 0 - valLoss: 0.4271913468837738 - trainLoss: 0.4031948745250702\n",
      "cnt: 0 - valLoss: 0.4271904528141022 - trainLoss: 0.40319350361824036\n",
      "cnt: 0 - valLoss: 0.42718955874443054 - trainLoss: 0.4031921625137329\n",
      "cnt: 0 - valLoss: 0.4271886646747589 - trainLoss: 0.40319085121154785\n",
      "cnt: 0 - valLoss: 0.42718780040740967 - trainLoss: 0.403189480304718\n",
      "cnt: 0 - valLoss: 0.42718684673309326 - trainLoss: 0.4031881093978882\n",
      "cnt: 0 - valLoss: 0.4271860122680664 - trainLoss: 0.40318676829338074\n",
      "cnt: 0 - valLoss: 0.4271850883960724 - trainLoss: 0.4031853973865509\n",
      "cnt: 0 - valLoss: 0.42718425393104553 - trainLoss: 0.40318408608436584\n",
      "cnt: 0 - valLoss: 0.4271833300590515 - trainLoss: 0.403182715177536\n",
      "cnt: 0 - valLoss: 0.42718249559402466 - trainLoss: 0.40318137407302856\n",
      "cnt: 0 - valLoss: 0.4271816313266754 - trainLoss: 0.40318000316619873\n",
      "cnt: 0 - valLoss: 0.42718076705932617 - trainLoss: 0.40317869186401367\n",
      "cnt: 0 - valLoss: 0.42717987298965454 - trainLoss: 0.4031773507595062\n",
      "cnt: 0 - valLoss: 0.4271790087223053 - trainLoss: 0.4031759798526764\n",
      "cnt: 0 - valLoss: 0.42717811465263367 - trainLoss: 0.40317466855049133\n",
      "cnt: 0 - valLoss: 0.4271772503852844 - trainLoss: 0.4031732976436615\n",
      "cnt: 0 - valLoss: 0.4271763861179352 - trainLoss: 0.40317195653915405\n",
      "cnt: 0 - valLoss: 0.4271755516529083 - trainLoss: 0.4031705856323242\n",
      "cnt: 0 - valLoss: 0.4271746873855591 - trainLoss: 0.4031692147254944\n",
      "cnt: 0 - valLoss: 0.42717382311820984 - trainLoss: 0.4031679034233093\n",
      "cnt: 0 - valLoss: 0.4271729588508606 - trainLoss: 0.4031665623188019\n",
      "cnt: 0 - valLoss: 0.42717212438583374 - trainLoss: 0.40316519141197205\n",
      "cnt: 0 - valLoss: 0.4271712899208069 - trainLoss: 0.403163880109787\n",
      "cnt: 0 - valLoss: 0.42717039585113525 - trainLoss: 0.40316250920295715\n",
      "cnt: 0 - valLoss: 0.4271695911884308 - trainLoss: 0.4031611680984497\n",
      "cnt: 0 - valLoss: 0.42716875672340393 - trainLoss: 0.4031597971916199\n",
      "cnt: 0 - valLoss: 0.4271679222583771 - trainLoss: 0.40315842628479004\n",
      "cnt: 0 - valLoss: 0.42716705799102783 - trainLoss: 0.403157114982605\n",
      "cnt: 0 - valLoss: 0.4271661937236786 - trainLoss: 0.40315577387809753\n",
      "cnt: 0 - valLoss: 0.4271653890609741 - trainLoss: 0.4031544029712677\n",
      "cnt: 0 - valLoss: 0.42716458439826965 - trainLoss: 0.40315309166908264\n",
      "cnt: 0 - valLoss: 0.4271637201309204 - trainLoss: 0.4031517207622528\n",
      "cnt: 0 - valLoss: 0.42716288566589355 - trainLoss: 0.40315037965774536\n",
      "cnt: 0 - valLoss: 0.4271620810031891 - trainLoss: 0.4031490087509155\n",
      "cnt: 0 - valLoss: 0.42716121673583984 - trainLoss: 0.40314769744873047\n",
      "cnt: 0 - valLoss: 0.4271604120731354 - trainLoss: 0.403146356344223\n",
      "cnt: 0 - valLoss: 0.4271596074104309 - trainLoss: 0.4031449854373932\n",
      "cnt: 0 - valLoss: 0.42715883255004883 - trainLoss: 0.40314361453056335\n",
      "cnt: 0 - valLoss: 0.4271579682826996 - trainLoss: 0.4031423032283783\n",
      "cnt: 0 - valLoss: 0.4271571636199951 - trainLoss: 0.40314096212387085\n",
      "cnt: 0 - valLoss: 0.42715632915496826 - trainLoss: 0.403139591217041\n",
      "cnt: 0 - valLoss: 0.4271555244922638 - trainLoss: 0.4031382203102112\n",
      "cnt: 0 - valLoss: 0.4271547198295593 - trainLoss: 0.4031369388103485\n",
      "cnt: 0 - valLoss: 0.42715394496917725 - trainLoss: 0.4031355679035187\n",
      "cnt: 0 - valLoss: 0.4271531403064728 - trainLoss: 0.40313419699668884\n",
      "cnt: 0 - valLoss: 0.4271523058414459 - trainLoss: 0.4031328856945038\n",
      "cnt: 0 - valLoss: 0.42715150117874146 - trainLoss: 0.40313154458999634\n",
      "cnt: 0 - valLoss: 0.4271507263183594 - trainLoss: 0.4031301736831665\n",
      "cnt: 0 - valLoss: 0.4271499216556549 - trainLoss: 0.40312883257865906\n",
      "cnt: 0 - valLoss: 0.42714908719062805 - trainLoss: 0.403127521276474\n",
      "cnt: 0 - valLoss: 0.42714831233024597 - trainLoss: 0.40312615036964417\n",
      "cnt: 0 - valLoss: 0.4271475672721863 - trainLoss: 0.4031248092651367\n",
      "cnt: 0 - valLoss: 0.4271467328071594 - trainLoss: 0.4031234681606293\n",
      "cnt: 0 - valLoss: 0.42714595794677734 - trainLoss: 0.4031221270561218\n",
      "cnt: 0 - valLoss: 0.42714518308639526 - trainLoss: 0.4031207859516144\n",
      "cnt: 0 - valLoss: 0.4271443784236908 - trainLoss: 0.40311944484710693\n",
      "cnt: 0 - valLoss: 0.42714357376098633 - trainLoss: 0.4031181037425995\n",
      "cnt: 0 - valLoss: 0.42714282870292664 - trainLoss: 0.40311676263809204\n",
      "cnt: 0 - valLoss: 0.42714202404022217 - trainLoss: 0.4031154215335846\n",
      "cnt: 0 - valLoss: 0.4271412491798401 - trainLoss: 0.40311405062675476\n",
      "cnt: 0 - valLoss: 0.427140474319458 - trainLoss: 0.4031127393245697\n",
      "cnt: 0 - valLoss: 0.4271396994590759 - trainLoss: 0.40311136841773987\n",
      "cnt: 0 - valLoss: 0.42713892459869385 - trainLoss: 0.4031100273132324\n",
      "cnt: 0 - valLoss: 0.42713817954063416 - trainLoss: 0.4031086564064026\n",
      "cnt: 0 - valLoss: 0.4271374046802521 - trainLoss: 0.40310734510421753\n",
      "cnt: 0 - valLoss: 0.42713662981987 - trainLoss: 0.4031059741973877\n",
      "cnt: 0 - valLoss: 0.4271358549594879 - trainLoss: 0.40310466289520264\n",
      "cnt: 0 - valLoss: 0.42713508009910583 - trainLoss: 0.4031033217906952\n",
      "cnt: 0 - valLoss: 0.42713430523872375 - trainLoss: 0.40310195088386536\n",
      "cnt: 0 - valLoss: 0.4271335303783417 - trainLoss: 0.4031006097793579\n",
      "cnt: 0 - valLoss: 0.4271327555179596 - trainLoss: 0.40309929847717285\n",
      "cnt: 0 - valLoss: 0.4271320402622223 - trainLoss: 0.403097927570343\n",
      "cnt: 0 - valLoss: 0.4271312654018402 - trainLoss: 0.4030965566635132\n",
      "cnt: 0 - valLoss: 0.4271305501461029 - trainLoss: 0.4030952751636505\n",
      "cnt: 0 - valLoss: 0.4271297752857208 - trainLoss: 0.4030939042568207\n",
      "cnt: 0 - valLoss: 0.42712894082069397 - trainLoss: 0.40309256315231323\n",
      "cnt: 0 - valLoss: 0.42712825536727905 - trainLoss: 0.4030911922454834\n",
      "cnt: 0 - valLoss: 0.427127480506897 - trainLoss: 0.40308988094329834\n",
      "cnt: 0 - valLoss: 0.42712676525115967 - trainLoss: 0.4030885398387909\n",
      "cnt: 0 - valLoss: 0.4271259903907776 - trainLoss: 0.40308716893196106\n",
      "cnt: 0 - valLoss: 0.4271252751350403 - trainLoss: 0.403085857629776\n",
      "cnt: 0 - valLoss: 0.4271245300769806 - trainLoss: 0.40308448672294617\n",
      "cnt: 0 - valLoss: 0.4271238148212433 - trainLoss: 0.4030832052230835\n",
      "cnt: 0 - valLoss: 0.4271230697631836 - trainLoss: 0.40308183431625366\n",
      "cnt: 0 - valLoss: 0.4271222949028015 - trainLoss: 0.40308046340942383\n",
      "cnt: 0 - valLoss: 0.4271215796470642 - trainLoss: 0.4030791223049164\n",
      "cnt: 0 - valLoss: 0.4271208643913269 - trainLoss: 0.4030778110027313\n",
      "cnt: 0 - valLoss: 0.4271201193332672 - trainLoss: 0.4030764400959015\n",
      "cnt: 0 - valLoss: 0.4271194040775299 - trainLoss: 0.40307509899139404\n",
      "cnt: 0 - valLoss: 0.4271186590194702 - trainLoss: 0.403073787689209\n",
      "cnt: 0 - valLoss: 0.4271179437637329 - trainLoss: 0.40307241678237915\n",
      "cnt: 0 - valLoss: 0.42711716890335083 - trainLoss: 0.4030710756778717\n",
      "cnt: 0 - valLoss: 0.4271164834499359 - trainLoss: 0.40306976437568665\n",
      "cnt: 0 - valLoss: 0.4271157383918762 - trainLoss: 0.4030683934688568\n",
      "cnt: 0 - valLoss: 0.4271150231361389 - trainLoss: 0.40306705236434937\n",
      "cnt: 0 - valLoss: 0.42711424827575684 - trainLoss: 0.4030657410621643\n",
      "cnt: 0 - valLoss: 0.4271136224269867 - trainLoss: 0.4030643701553345\n",
      "cnt: 0 - valLoss: 0.427112877368927 - trainLoss: 0.4030630886554718\n",
      "cnt: 0 - valLoss: 0.4271121025085449 - trainLoss: 0.40306171774864197\n",
      "cnt: 0 - valLoss: 0.42711141705513 - trainLoss: 0.40306034684181213\n",
      "cnt: 0 - valLoss: 0.4271107017993927 - trainLoss: 0.4030590057373047\n",
      "cnt: 0 - valLoss: 0.4271100163459778 - trainLoss: 0.40305769443511963\n",
      "cnt: 0 - valLoss: 0.4271092712879181 - trainLoss: 0.4030563235282898\n",
      "cnt: 0 - valLoss: 0.4271085858345032 - trainLoss: 0.40305498242378235\n",
      "cnt: 0 - valLoss: 0.42710790038108826 - trainLoss: 0.4030536711215973\n",
      "cnt: 0 - valLoss: 0.42710718512535095 - trainLoss: 0.40305230021476746\n",
      "cnt: 0 - valLoss: 0.42710644006729126 - trainLoss: 0.40305095911026\n",
      "cnt: 0 - valLoss: 0.42710575461387634 - trainLoss: 0.40304964780807495\n",
      "cnt: 0 - valLoss: 0.42710503935813904 - trainLoss: 0.4030483067035675\n",
      "cnt: 0 - valLoss: 0.4271043539047241 - trainLoss: 0.40304699540138245\n",
      "cnt: 0 - valLoss: 0.4271036684513092 - trainLoss: 0.4030456244945526\n",
      "cnt: 0 - valLoss: 0.4271029233932495 - trainLoss: 0.40304428339004517\n",
      "cnt: 0 - valLoss: 0.4271022379398346 - trainLoss: 0.4030429720878601\n",
      "cnt: 0 - valLoss: 0.4271015524864197 - trainLoss: 0.4030416011810303\n",
      "cnt: 0 - valLoss: 0.42710086703300476 - trainLoss: 0.4030402600765228\n",
      "cnt: 0 - valLoss: 0.42710018157958984 - trainLoss: 0.40303894877433777\n",
      "cnt: 0 - valLoss: 0.4270994961261749 - trainLoss: 0.40303757786750793\n",
      "cnt: 0 - valLoss: 0.42709881067276 - trainLoss: 0.4030362665653229\n",
      "cnt: 0 - valLoss: 0.4270980954170227 - trainLoss: 0.40303492546081543\n",
      "cnt: 0 - valLoss: 0.427097350358963 - trainLoss: 0.403033584356308\n",
      "cnt: 0 - valLoss: 0.42709672451019287 - trainLoss: 0.40303224325180054\n",
      "cnt: 0 - valLoss: 0.42709603905677795 - trainLoss: 0.4030309021472931\n",
      "cnt: 0 - valLoss: 0.42709535360336304 - trainLoss: 0.40302956104278564\n",
      "cnt: 0 - valLoss: 0.4270946681499481 - trainLoss: 0.4030282199382782\n",
      "cnt: 0 - valLoss: 0.4270939826965332 - trainLoss: 0.40302687883377075\n",
      "cnt: 0 - valLoss: 0.4270932972431183 - trainLoss: 0.4030255675315857\n",
      "cnt: 0 - valLoss: 0.42709261178970337 - trainLoss: 0.40302419662475586\n",
      "cnt: 0 - valLoss: 0.42709192633628845 - trainLoss: 0.4030228853225708\n",
      "cnt: 0 - valLoss: 0.4270912706851959 - trainLoss: 0.40302151441574097\n",
      "cnt: 0 - valLoss: 0.427090585231781 - trainLoss: 0.4030201733112335\n",
      "cnt: 0 - valLoss: 0.42708995938301086 - trainLoss: 0.40301886200904846\n",
      "cnt: 0 - valLoss: 0.42708927392959595 - trainLoss: 0.403017520904541\n",
      "cnt: 0 - valLoss: 0.4270886182785034 - trainLoss: 0.40301620960235596\n",
      "cnt: 0 - valLoss: 0.4270879328250885 - trainLoss: 0.4030148684978485\n",
      "cnt: 0 - valLoss: 0.4270872473716736 - trainLoss: 0.4030134975910187\n",
      "cnt: 0 - valLoss: 0.42708659172058105 - trainLoss: 0.4030121862888336\n",
      "cnt: 0 - valLoss: 0.42708590626716614 - trainLoss: 0.40301084518432617\n",
      "cnt: 0 - valLoss: 0.427085280418396 - trainLoss: 0.4030095338821411\n",
      "cnt: 0 - valLoss: 0.4270845949649811 - trainLoss: 0.4030081629753113\n",
      "cnt: 0 - valLoss: 0.42708390951156616 - trainLoss: 0.40300682187080383\n",
      "cnt: 0 - valLoss: 0.42708325386047363 - trainLoss: 0.4030055105686188\n",
      "cnt: 0 - valLoss: 0.4270825684070587 - trainLoss: 0.40300416946411133\n",
      "cnt: 0 - valLoss: 0.4270819425582886 - trainLoss: 0.4030027985572815\n",
      "cnt: 0 - valLoss: 0.42708128690719604 - trainLoss: 0.40300148725509644\n",
      "cnt: 0 - valLoss: 0.4270806610584259 - trainLoss: 0.403000146150589\n",
      "cnt: 0 - valLoss: 0.4270800054073334 - trainLoss: 0.40299883484840393\n",
      "cnt: 0 - valLoss: 0.42707934975624084 - trainLoss: 0.4029974937438965\n",
      "cnt: 0 - valLoss: 0.4270786643028259 - trainLoss: 0.40299612283706665\n",
      "cnt: 0 - valLoss: 0.4270780384540558 - trainLoss: 0.402994841337204\n",
      "cnt: 0 - valLoss: 0.42707738280296326 - trainLoss: 0.40299347043037415\n",
      "cnt: 0 - valLoss: 0.4270767569541931 - trainLoss: 0.4029921591281891\n",
      "cnt: 0 - valLoss: 0.4270761013031006 - trainLoss: 0.40299081802368164\n",
      "cnt: 0 - valLoss: 0.42707541584968567 - trainLoss: 0.4029894471168518\n",
      "cnt: 0 - valLoss: 0.4270748198032379 - trainLoss: 0.40298813581466675\n",
      "cnt: 0 - valLoss: 0.427074134349823 - trainLoss: 0.4029867947101593\n",
      "cnt: 0 - valLoss: 0.42707353830337524 - trainLoss: 0.40298548340797424\n",
      "cnt: 0 - valLoss: 0.4270728528499603 - trainLoss: 0.4029841423034668\n",
      "cnt: 0 - valLoss: 0.4270722568035126 - trainLoss: 0.40298277139663696\n",
      "cnt: 0 - valLoss: 0.42707157135009766 - trainLoss: 0.4029814600944519\n",
      "cnt: 0 - valLoss: 0.4270709753036499 - trainLoss: 0.40298011898994446\n",
      "cnt: 0 - valLoss: 0.4270703196525574 - trainLoss: 0.4029787480831146\n",
      "cnt: 0 - valLoss: 0.42706966400146484 - trainLoss: 0.40297746658325195\n",
      "cnt: 0 - valLoss: 0.4270690381526947 - trainLoss: 0.4029760956764221\n",
      "cnt: 0 - valLoss: 0.4270683825016022 - trainLoss: 0.40297481417655945\n",
      "cnt: 0 - valLoss: 0.42706775665283203 - trainLoss: 0.4029734432697296\n",
      "cnt: 0 - valLoss: 0.4270671010017395 - trainLoss: 0.4029720723628998\n",
      "cnt: 0 - valLoss: 0.42706650495529175 - trainLoss: 0.4029707908630371\n",
      "cnt: 0 - valLoss: 0.4270658493041992 - trainLoss: 0.4029694199562073\n",
      "cnt: 0 - valLoss: 0.4270652234554291 - trainLoss: 0.4029681086540222\n",
      "cnt: 0 - valLoss: 0.42706456780433655 - trainLoss: 0.40296676754951477\n",
      "cnt: 0 - valLoss: 0.4270639419555664 - trainLoss: 0.4029654562473297\n",
      "cnt: 0 - valLoss: 0.42706334590911865 - trainLoss: 0.40296411514282227\n",
      "cnt: 0 - valLoss: 0.4270626902580261 - trainLoss: 0.40296274423599243\n",
      "cnt: 0 - valLoss: 0.4270620346069336 - trainLoss: 0.40296146273612976\n",
      "cnt: 0 - valLoss: 0.42706140875816345 - trainLoss: 0.4029600918292999\n",
      "cnt: 0 - valLoss: 0.4270608127117157 - trainLoss: 0.4029587209224701\n",
      "cnt: 0 - valLoss: 0.42706015706062317 - trainLoss: 0.40295740962028503\n",
      "cnt: 0 - valLoss: 0.4270595610141754 - trainLoss: 0.4029560685157776\n",
      "cnt: 0 - valLoss: 0.4270589053630829 - trainLoss: 0.4029547870159149\n",
      "cnt: 0 - valLoss: 0.42705830931663513 - trainLoss: 0.4029534161090851\n",
      "cnt: 0 - valLoss: 0.427057683467865 - trainLoss: 0.40295207500457764\n",
      "cnt: 0 - valLoss: 0.42705708742141724 - trainLoss: 0.4029507637023926\n",
      "cnt: 0 - valLoss: 0.4270564317703247 - trainLoss: 0.40294939279556274\n",
      "cnt: 0 - valLoss: 0.42705583572387695 - trainLoss: 0.4029481112957001\n",
      "cnt: 0 - valLoss: 0.4270551800727844 - trainLoss: 0.40294674038887024\n",
      "cnt: 0 - valLoss: 0.42705458402633667 - trainLoss: 0.4029453992843628\n",
      "cnt: 0 - valLoss: 0.42705395817756653 - trainLoss: 0.40294408798217773\n",
      "cnt: 0 - valLoss: 0.4270533323287964 - trainLoss: 0.4029427468776703\n",
      "cnt: 0 - valLoss: 0.42705270648002625 - trainLoss: 0.40294143557548523\n",
      "cnt: 0 - valLoss: 0.4270521104335785 - trainLoss: 0.4029400944709778\n",
      "cnt: 0 - valLoss: 0.42705151438713074 - trainLoss: 0.40293872356414795\n",
      "cnt: 0 - valLoss: 0.4270508885383606 - trainLoss: 0.4029374122619629\n",
      "cnt: 0 - valLoss: 0.42705029249191284 - trainLoss: 0.40293607115745544\n",
      "cnt: 0 - valLoss: 0.4270496964454651 - trainLoss: 0.4029347598552704\n",
      "cnt: 0 - valLoss: 0.42704910039901733 - trainLoss: 0.40293338894844055\n",
      "cnt: 0 - valLoss: 0.4270485043525696 - trainLoss: 0.4029320478439331\n",
      "cnt: 0 - valLoss: 0.42704784870147705 - trainLoss: 0.40293073654174805\n",
      "cnt: 0 - valLoss: 0.4270473122596741 - trainLoss: 0.4029293954372406\n",
      "cnt: 0 - valLoss: 0.42704665660858154 - trainLoss: 0.40292808413505554\n",
      "cnt: 0 - valLoss: 0.4270460605621338 - trainLoss: 0.4029267132282257\n",
      "cnt: 0 - valLoss: 0.42704546451568604 - trainLoss: 0.40292543172836304\n",
      "cnt: 0 - valLoss: 0.42704489827156067 - trainLoss: 0.4029240608215332\n",
      "cnt: 0 - valLoss: 0.4270443022251129 - trainLoss: 0.40292277932167053\n",
      "cnt: 0 - valLoss: 0.42704370617866516 - trainLoss: 0.4029214084148407\n",
      "cnt: 0 - valLoss: 0.4270431101322174 - trainLoss: 0.40292006731033325\n",
      "cnt: 0 - valLoss: 0.42704251408576965 - trainLoss: 0.4029187560081482\n",
      "cnt: 0 - valLoss: 0.4270418882369995 - trainLoss: 0.40291741490364075\n",
      "cnt: 0 - valLoss: 0.42704135179519653 - trainLoss: 0.4029160439968109\n",
      "cnt: 0 - valLoss: 0.427040696144104 - trainLoss: 0.40291476249694824\n",
      "cnt: 0 - valLoss: 0.42704012989997864 - trainLoss: 0.4029133915901184\n",
      "cnt: 0 - valLoss: 0.4270395338535309 - trainLoss: 0.40291208028793335\n",
      "cnt: 0 - valLoss: 0.42703893780708313 - trainLoss: 0.4029107391834259\n",
      "cnt: 0 - valLoss: 0.42703837156295776 - trainLoss: 0.40290942788124084\n",
      "cnt: 0 - valLoss: 0.42703777551651 - trainLoss: 0.402908056974411\n",
      "cnt: 0 - valLoss: 0.42703717947006226 - trainLoss: 0.40290677547454834\n",
      "cnt: 0 - valLoss: 0.4270365834236145 - trainLoss: 0.4029054045677185\n",
      "cnt: 0 - valLoss: 0.42703601717948914 - trainLoss: 0.40290412306785583\n",
      "cnt: 0 - valLoss: 0.427035391330719 - trainLoss: 0.4029027819633484\n",
      "cnt: 0 - valLoss: 0.4270348846912384 - trainLoss: 0.40290141105651855\n",
      "cnt: 0 - valLoss: 0.42703425884246826 - trainLoss: 0.4029000997543335\n",
      "cnt: 0 - valLoss: 0.4270336627960205 - trainLoss: 0.40289872884750366\n",
      "cnt: 0 - valLoss: 0.42703306674957275 - trainLoss: 0.4028974175453186\n",
      "cnt: 0 - valLoss: 0.42703256011009216 - trainLoss: 0.40289607644081116\n",
      "cnt: 0 - valLoss: 0.4270319640636444 - trainLoss: 0.4028947651386261\n",
      "cnt: 0 - valLoss: 0.42703136801719666 - trainLoss: 0.40289345383644104\n",
      "cnt: 0 - valLoss: 0.4270307421684265 - trainLoss: 0.4028921127319336\n",
      "cnt: 0 - valLoss: 0.42703020572662354 - trainLoss: 0.40289074182510376\n",
      "cnt: 0 - valLoss: 0.42702963948249817 - trainLoss: 0.4028894603252411\n",
      "cnt: 0 - valLoss: 0.4270290434360504 - trainLoss: 0.40288808941841125\n",
      "cnt: 0 - valLoss: 0.42702844738960266 - trainLoss: 0.4028867185115814\n",
      "cnt: 0 - valLoss: 0.4270278811454773 - trainLoss: 0.40288543701171875\n",
      "cnt: 0 - valLoss: 0.42702731490135193 - trainLoss: 0.4028841257095337\n",
      "cnt: 0 - valLoss: 0.4270267188549042 - trainLoss: 0.40288278460502625\n",
      "cnt: 0 - valLoss: 0.4270261824131012 - trainLoss: 0.4028814136981964\n",
      "cnt: 0 - valLoss: 0.42702561616897583 - trainLoss: 0.40288010239601135\n",
      "cnt: 0 - valLoss: 0.4270250201225281 - trainLoss: 0.4028787612915039\n",
      "cnt: 0 - valLoss: 0.4270244538784027 - trainLoss: 0.40287747979164124\n",
      "cnt: 0 - valLoss: 0.42702388763427734 - trainLoss: 0.4028761088848114\n",
      "cnt: 0 - valLoss: 0.4270232915878296 - trainLoss: 0.40287479758262634\n",
      "cnt: 0 - valLoss: 0.4270227551460266 - trainLoss: 0.4028734564781189\n",
      "cnt: 0 - valLoss: 0.42702218890190125 - trainLoss: 0.40287214517593384\n",
      "cnt: 0 - valLoss: 0.4270216226577759 - trainLoss: 0.4028708040714264\n",
      "cnt: 0 - valLoss: 0.4270210266113281 - trainLoss: 0.40286949276924133\n",
      "cnt: 0 - valLoss: 0.42702043056488037 - trainLoss: 0.4028681516647339\n",
      "cnt: 0 - valLoss: 0.427019864320755 - trainLoss: 0.40286678075790405\n",
      "cnt: 0 - valLoss: 0.4270193576812744 - trainLoss: 0.402865469455719\n",
      "cnt: 0 - valLoss: 0.42701876163482666 - trainLoss: 0.40286412835121155\n",
      "cnt: 0 - valLoss: 0.4270181953907013 - trainLoss: 0.4028628170490265\n",
      "cnt: 0 - valLoss: 0.4270176291465759 - trainLoss: 0.40286147594451904\n",
      "cnt: 0 - valLoss: 0.4270170331001282 - trainLoss: 0.402860164642334\n",
      "cnt: 0 - valLoss: 0.4270164966583252 - trainLoss: 0.40285882353782654\n",
      "cnt: 0 - valLoss: 0.42701593041419983 - trainLoss: 0.4028575122356415\n",
      "cnt: 0 - valLoss: 0.42701536417007446 - trainLoss: 0.40285617113113403\n",
      "cnt: 0 - valLoss: 0.4270147979259491 - trainLoss: 0.4028548300266266\n",
      "cnt: 0 - valLoss: 0.42701420187950134 - trainLoss: 0.40285351872444153\n",
      "cnt: 0 - valLoss: 0.42701366543769836 - trainLoss: 0.4028521478176117\n",
      "cnt: 0 - valLoss: 0.427013099193573 - trainLoss: 0.402850866317749\n",
      "cnt: 0 - valLoss: 0.42701253294944763 - trainLoss: 0.4028494954109192\n",
      "cnt: 0 - valLoss: 0.42701199650764465 - trainLoss: 0.40284815430641174\n",
      "cnt: 0 - valLoss: 0.4270114302635193 - trainLoss: 0.4028468728065491\n",
      "cnt: 0 - valLoss: 0.4270108640193939 - trainLoss: 0.402845561504364\n",
      "cnt: 0 - valLoss: 0.42701026797294617 - trainLoss: 0.4028441905975342\n",
      "cnt: 0 - valLoss: 0.4270097613334656 - trainLoss: 0.40284284949302673\n",
      "cnt: 0 - valLoss: 0.4270091652870178 - trainLoss: 0.4028415381908417\n",
      "cnt: 0 - valLoss: 0.42700862884521484 - trainLoss: 0.40284016728401184\n",
      "cnt: 0 - valLoss: 0.4270080327987671 - trainLoss: 0.40283888578414917\n",
      "cnt: 0 - valLoss: 0.4270075261592865 - trainLoss: 0.4028375446796417\n",
      "cnt: 0 - valLoss: 0.42700695991516113 - trainLoss: 0.40283623337745667\n",
      "cnt: 0 - valLoss: 0.42700642347335815 - trainLoss: 0.4028348922729492\n",
      "cnt: 0 - valLoss: 0.4270058572292328 - trainLoss: 0.4028335213661194\n",
      "cnt: 0 - valLoss: 0.4270052909851074 - trainLoss: 0.4028322100639343\n",
      "cnt: 0 - valLoss: 0.42700472474098206 - trainLoss: 0.40283092856407166\n",
      "cnt: 0 - valLoss: 0.4270041882991791 - trainLoss: 0.4028295874595642\n",
      "cnt: 0 - valLoss: 0.4270036816596985 - trainLoss: 0.40282827615737915\n",
      "cnt: 0 - valLoss: 0.42700305581092834 - trainLoss: 0.4028269052505493\n",
      "cnt: 0 - valLoss: 0.42700254917144775 - trainLoss: 0.40282562375068665\n",
      "cnt: 0 - valLoss: 0.4270019829273224 - trainLoss: 0.4028242528438568\n",
      "cnt: 0 - valLoss: 0.4270014464855194 - trainLoss: 0.40282297134399414\n",
      "cnt: 0 - valLoss: 0.42700091004371643 - trainLoss: 0.4028216004371643\n",
      "cnt: 0 - valLoss: 0.42700037360191345 - trainLoss: 0.40282025933265686\n",
      "cnt: 0 - valLoss: 0.4269998073577881 - trainLoss: 0.4028189480304718\n",
      "cnt: 0 - valLoss: 0.4269992411136627 - trainLoss: 0.40281760692596436\n",
      "cnt: 0 - valLoss: 0.42699873447418213 - trainLoss: 0.4028162658214569\n",
      "cnt: 0 - valLoss: 0.42699816823005676 - trainLoss: 0.40281498432159424\n",
      "cnt: 0 - valLoss: 0.4269976317882538 - trainLoss: 0.4028136134147644\n",
      "cnt: 0 - valLoss: 0.4269970953464508 - trainLoss: 0.40281230211257935\n",
      "cnt: 0 - valLoss: 0.4269965589046478 - trainLoss: 0.4028109610080719\n",
      "cnt: 0 - valLoss: 0.42699605226516724 - trainLoss: 0.40280964970588684\n",
      "cnt: 0 - valLoss: 0.42699548602104187 - trainLoss: 0.4028083086013794\n",
      "cnt: 0 - valLoss: 0.4269949197769165 - trainLoss: 0.40280699729919434\n",
      "cnt: 0 - valLoss: 0.4269944131374359 - trainLoss: 0.4028056561946869\n",
      "cnt: 0 - valLoss: 0.4269939064979553 - trainLoss: 0.40280434489250183\n",
      "cnt: 0 - valLoss: 0.42699337005615234 - trainLoss: 0.4028030037879944\n",
      "cnt: 0 - valLoss: 0.4269927740097046 - trainLoss: 0.4028016924858093\n",
      "cnt: 0 - valLoss: 0.426992267370224 - trainLoss: 0.4028003513813019\n",
      "cnt: 0 - valLoss: 0.4269917607307434 - trainLoss: 0.40279898047447205\n",
      "cnt: 0 - valLoss: 0.42699119448661804 - trainLoss: 0.4027976989746094\n",
      "cnt: 0 - valLoss: 0.42699068784713745 - trainLoss: 0.4027963876724243\n",
      "cnt: 0 - valLoss: 0.4269901216030121 - trainLoss: 0.40279504656791687\n",
      "cnt: 0 - valLoss: 0.4269896149635315 - trainLoss: 0.4027937352657318\n",
      "cnt: 0 - valLoss: 0.4269891083240509 - trainLoss: 0.40279239416122437\n",
      "cnt: 0 - valLoss: 0.42698854207992554 - trainLoss: 0.40279102325439453\n",
      "cnt: 0 - valLoss: 0.42698803544044495 - trainLoss: 0.40278974175453186\n",
      "cnt: 0 - valLoss: 0.42698749899864197 - trainLoss: 0.4027884304523468\n",
      "cnt: 0 - valLoss: 0.426986962556839 - trainLoss: 0.40278708934783936\n",
      "cnt: 0 - valLoss: 0.426986426115036 - trainLoss: 0.4027857482433319\n",
      "cnt: 0 - valLoss: 0.4269859194755554 - trainLoss: 0.40278443694114685\n",
      "cnt: 0 - valLoss: 0.42698541283607483 - trainLoss: 0.402783066034317\n",
      "cnt: 0 - valLoss: 0.42698490619659424 - trainLoss: 0.40278178453445435\n",
      "cnt: 0 - valLoss: 0.42698436975479126 - trainLoss: 0.4027804136276245\n",
      "cnt: 0 - valLoss: 0.4269838333129883 - trainLoss: 0.40277913212776184\n",
      "cnt: 0 - valLoss: 0.4269832968711853 - trainLoss: 0.4027777910232544\n",
      "cnt: 0 - valLoss: 0.4269827604293823 - trainLoss: 0.40277647972106934\n",
      "cnt: 0 - valLoss: 0.42698222398757935 - trainLoss: 0.4027751386165619\n",
      "cnt: 0 - valLoss: 0.42698171734809875 - trainLoss: 0.40277382731437683\n",
      "cnt: 0 - valLoss: 0.42698121070861816 - trainLoss: 0.4027724862098694\n",
      "cnt: 0 - valLoss: 0.4269807040691376 - trainLoss: 0.4027711749076843\n",
      "cnt: 0 - valLoss: 0.4269801378250122 - trainLoss: 0.4027698338031769\n",
      "cnt: 0 - valLoss: 0.4269796311855316 - trainLoss: 0.4027685225009918\n",
      "cnt: 0 - valLoss: 0.42697909474372864 - trainLoss: 0.4027671813964844\n",
      "cnt: 0 - valLoss: 0.42697858810424805 - trainLoss: 0.4027658700942993\n",
      "cnt: 0 - valLoss: 0.4269780218601227 - trainLoss: 0.40276452898979187\n",
      "cnt: 0 - valLoss: 0.4269775152206421 - trainLoss: 0.4027632176876068\n",
      "cnt: 0 - valLoss: 0.42697709798812866 - trainLoss: 0.40276187658309937\n",
      "cnt: 0 - valLoss: 0.4269765019416809 - trainLoss: 0.4027605652809143\n",
      "cnt: 0 - valLoss: 0.4269760549068451 - trainLoss: 0.40275922417640686\n",
      "cnt: 0 - valLoss: 0.42697545886039734 - trainLoss: 0.4027579128742218\n",
      "cnt: 0 - valLoss: 0.42697498202323914 - trainLoss: 0.40275657176971436\n",
      "cnt: 0 - valLoss: 0.42697447538375854 - trainLoss: 0.4027552306652069\n",
      "cnt: 0 - valLoss: 0.42697396874427795 - trainLoss: 0.40275391936302185\n",
      "cnt: 0 - valLoss: 0.4269734025001526 - trainLoss: 0.4027525782585144\n",
      "cnt: 0 - valLoss: 0.426972895860672 - trainLoss: 0.40275126695632935\n",
      "cnt: 0 - valLoss: 0.4269723892211914 - trainLoss: 0.4027499258518219\n",
      "cnt: 0 - valLoss: 0.4269718527793884 - trainLoss: 0.40274861454963684\n",
      "cnt: 0 - valLoss: 0.42697134613990784 - trainLoss: 0.4027472734451294\n",
      "cnt: 0 - valLoss: 0.42697083950042725 - trainLoss: 0.40274596214294434\n",
      "cnt: 0 - valLoss: 0.42697033286094666 - trainLoss: 0.4027446210384369\n",
      "cnt: 0 - valLoss: 0.4269697964191437 - trainLoss: 0.4027433395385742\n",
      "cnt: 0 - valLoss: 0.4269692897796631 - trainLoss: 0.4027419686317444\n",
      "cnt: 0 - valLoss: 0.4269687831401825 - trainLoss: 0.4027406871318817\n",
      "cnt: 0 - valLoss: 0.4269682765007019 - trainLoss: 0.4027393162250519\n",
      "cnt: 0 - valLoss: 0.4269677996635437 - trainLoss: 0.4027380347251892\n",
      "cnt: 0 - valLoss: 0.4269672632217407 - trainLoss: 0.4027366638183594\n",
      "cnt: 0 - valLoss: 0.42696675658226013 - trainLoss: 0.4027353823184967\n",
      "cnt: 0 - valLoss: 0.42696624994277954 - trainLoss: 0.40273401141166687\n",
      "cnt: 0 - valLoss: 0.42696574330329895 - trainLoss: 0.4027327299118042\n",
      "cnt: 0 - valLoss: 0.4269651770591736 - trainLoss: 0.40273141860961914\n",
      "cnt: 0 - valLoss: 0.4269647002220154 - trainLoss: 0.4027300775051117\n",
      "cnt: 0 - valLoss: 0.4269641935825348 - trainLoss: 0.40272873640060425\n",
      "cnt: 0 - valLoss: 0.4269636869430542 - trainLoss: 0.4027274250984192\n",
      "cnt: 0 - valLoss: 0.426963210105896 - trainLoss: 0.40272608399391174\n",
      "cnt: 0 - valLoss: 0.4269627034664154 - trainLoss: 0.4027247726917267\n",
      "cnt: 0 - valLoss: 0.4269621670246124 - trainLoss: 0.40272343158721924\n",
      "cnt: 0 - valLoss: 0.42696166038513184 - trainLoss: 0.4027221202850342\n",
      "cnt: 0 - valLoss: 0.42696115374565125 - trainLoss: 0.40272077918052673\n",
      "cnt: 0 - valLoss: 0.42696064710617065 - trainLoss: 0.4027194678783417\n",
      "cnt: 0 - valLoss: 0.4269601106643677 - trainLoss: 0.40271812677383423\n",
      "cnt: 0 - valLoss: 0.4269596040248871 - trainLoss: 0.40271681547164917\n",
      "cnt: 0 - valLoss: 0.4269591271877289 - trainLoss: 0.4027155339717865\n",
      "cnt: 0 - valLoss: 0.4269586205482483 - trainLoss: 0.40271416306495667\n",
      "cnt: 0 - valLoss: 0.4269581437110901 - trainLoss: 0.4027128219604492\n",
      "cnt: 0 - valLoss: 0.4269576370716095 - trainLoss: 0.40271154046058655\n",
      "cnt: 0 - valLoss: 0.4269571304321289 - trainLoss: 0.4027102291584015\n",
      "cnt: 0 - valLoss: 0.4269565939903259 - trainLoss: 0.40270885825157166\n",
      "cnt: 0 - valLoss: 0.42695608735084534 - trainLoss: 0.402707576751709\n",
      "cnt: 0 - valLoss: 0.42695561051368713 - trainLoss: 0.40270620584487915\n",
      "cnt: 0 - valLoss: 0.42695513367652893 - trainLoss: 0.4027049243450165\n",
      "cnt: 0 - valLoss: 0.42695462703704834 - trainLoss: 0.40270355343818665\n",
      "cnt: 0 - valLoss: 0.42695412039756775 - trainLoss: 0.4027022421360016\n",
      "cnt: 0 - valLoss: 0.42695361375808716 - trainLoss: 0.40270093083381653\n",
      "cnt: 0 - valLoss: 0.4269530773162842 - trainLoss: 0.4026995897293091\n",
      "cnt: 0 - valLoss: 0.42695266008377075 - trainLoss: 0.4026983082294464\n",
      "cnt: 0 - valLoss: 0.4269520938396454 - trainLoss: 0.4026969373226166\n",
      "cnt: 0 - valLoss: 0.42695164680480957 - trainLoss: 0.4026956558227539\n",
      "cnt: 0 - valLoss: 0.4269511103630066 - trainLoss: 0.4026942849159241\n",
      "cnt: 0 - valLoss: 0.4269506335258484 - trainLoss: 0.402692973613739\n",
      "cnt: 0 - valLoss: 0.4269500970840454 - trainLoss: 0.40269169211387634\n",
      "cnt: 0 - valLoss: 0.4269496202468872 - trainLoss: 0.4026903510093689\n",
      "cnt: 0 - valLoss: 0.426949143409729 - trainLoss: 0.40268898010253906\n",
      "cnt: 0 - valLoss: 0.4269486367702484 - trainLoss: 0.4026876986026764\n",
      "cnt: 0 - valLoss: 0.4269481301307678 - trainLoss: 0.40268638730049133\n",
      "cnt: 0 - valLoss: 0.4269476532936096 - trainLoss: 0.4026850461959839\n",
      "cnt: 0 - valLoss: 0.4269471764564514 - trainLoss: 0.40268373489379883\n",
      "cnt: 0 - valLoss: 0.4269466698169708 - trainLoss: 0.4026823937892914\n",
      "cnt: 0 - valLoss: 0.42694613337516785 - trainLoss: 0.4026811122894287\n",
      "cnt: 0 - valLoss: 0.42694562673568726 - trainLoss: 0.4026797413825989\n",
      "cnt: 0 - valLoss: 0.42694514989852905 - trainLoss: 0.4026784598827362\n",
      "cnt: 0 - valLoss: 0.42694470286369324 - trainLoss: 0.40267708897590637\n",
      "cnt: 0 - valLoss: 0.42694416642189026 - trainLoss: 0.4026758074760437\n",
      "cnt: 0 - valLoss: 0.42694365978240967 - trainLoss: 0.40267446637153625\n",
      "cnt: 0 - valLoss: 0.42694318294525146 - trainLoss: 0.4026731550693512\n",
      "cnt: 0 - valLoss: 0.4269426763057709 - trainLoss: 0.40267181396484375\n",
      "cnt: 0 - valLoss: 0.42694219946861267 - trainLoss: 0.4026705026626587\n",
      "cnt: 0 - valLoss: 0.4269416928291321 - trainLoss: 0.40266913175582886\n",
      "cnt: 0 - valLoss: 0.4269411861896515 - trainLoss: 0.4026678502559662\n",
      "cnt: 0 - valLoss: 0.4269407093524933 - trainLoss: 0.40266650915145874\n",
      "cnt: 0 - valLoss: 0.4269402325153351 - trainLoss: 0.4026651978492737\n",
      "cnt: 0 - valLoss: 0.4269397258758545 - trainLoss: 0.40266385674476624\n",
      "cnt: 0 - valLoss: 0.4269391894340515 - trainLoss: 0.40266257524490356\n",
      "cnt: 0 - valLoss: 0.4269386827945709 - trainLoss: 0.4026612639427185\n",
      "cnt: 0 - valLoss: 0.4269382655620575 - trainLoss: 0.40265989303588867\n",
      "cnt: 0 - valLoss: 0.4269377589225769 - trainLoss: 0.402658611536026\n",
      "cnt: 0 - valLoss: 0.4269372224807739 - trainLoss: 0.40265727043151855\n",
      "cnt: 0 - valLoss: 0.4269367456436157 - trainLoss: 0.4026559591293335\n",
      "cnt: 0 - valLoss: 0.4269362986087799 - trainLoss: 0.40265461802482605\n",
      "cnt: 0 - valLoss: 0.42693576216697693 - trainLoss: 0.4026532769203186\n",
      "cnt: 0 - valLoss: 0.42693525552749634 - trainLoss: 0.40265196561813354\n",
      "cnt: 0 - valLoss: 0.4269348382949829 - trainLoss: 0.4026506841182709\n",
      "cnt: 0 - valLoss: 0.4269343316555023 - trainLoss: 0.4026493430137634\n",
      "cnt: 0 - valLoss: 0.42693379521369934 - trainLoss: 0.4026479721069336\n",
      "cnt: 0 - valLoss: 0.42693331837654114 - trainLoss: 0.4026466906070709\n",
      "cnt: 0 - valLoss: 0.4269328713417053 - trainLoss: 0.40264537930488586\n",
      "cnt: 0 - valLoss: 0.42693233489990234 - trainLoss: 0.4026440382003784\n",
      "cnt: 0 - valLoss: 0.42693188786506653 - trainLoss: 0.40264272689819336\n",
      "cnt: 0 - valLoss: 0.4269314110279083 - trainLoss: 0.4026413857936859\n",
      "cnt: 0 - valLoss: 0.42693087458610535 - trainLoss: 0.40264007449150085\n",
      "cnt: 0 - valLoss: 0.42693036794662476 - trainLoss: 0.4026387929916382\n",
      "cnt: 0 - valLoss: 0.42692995071411133 - trainLoss: 0.40263745188713074\n",
      "cnt: 0 - valLoss: 0.42692944407463074 - trainLoss: 0.4026361405849457\n",
      "cnt: 0 - valLoss: 0.42692896723747253 - trainLoss: 0.40263476967811584\n",
      "cnt: 0 - valLoss: 0.42692849040031433 - trainLoss: 0.4026334881782532\n",
      "cnt: 0 - valLoss: 0.42692801356315613 - trainLoss: 0.4026321470737457\n",
      "cnt: 0 - valLoss: 0.4269275367259979 - trainLoss: 0.4026308059692383\n",
      "cnt: 0 - valLoss: 0.42692703008651733 - trainLoss: 0.4026294946670532\n",
      "cnt: 0 - valLoss: 0.42692655324935913 - trainLoss: 0.4026281535625458\n",
      "cnt: 0 - valLoss: 0.42692604660987854 - trainLoss: 0.4026268720626831\n",
      "cnt: 0 - valLoss: 0.42692556977272034 - trainLoss: 0.40262550115585327\n",
      "cnt: 0 - valLoss: 0.42692506313323975 - trainLoss: 0.4026241898536682\n",
      "cnt: 0 - valLoss: 0.42692458629608154 - trainLoss: 0.40262290835380554\n",
      "cnt: 0 - valLoss: 0.42692410945892334 - trainLoss: 0.4026215672492981\n",
      "cnt: 0 - valLoss: 0.4269236922264099 - trainLoss: 0.40262025594711304\n",
      "cnt: 0 - valLoss: 0.4269231855869293 - trainLoss: 0.4026189148426056\n",
      "cnt: 0 - valLoss: 0.42692264914512634 - trainLoss: 0.40261760354042053\n",
      "cnt: 0 - valLoss: 0.4269222319126129 - trainLoss: 0.4026162624359131\n",
      "cnt: 0 - valLoss: 0.4269217252731323 - trainLoss: 0.4026149809360504\n",
      "cnt: 0 - valLoss: 0.4269212484359741 - trainLoss: 0.40261366963386536\n",
      "cnt: 0 - valLoss: 0.4269207715988159 - trainLoss: 0.4026123285293579\n",
      "cnt: 0 - valLoss: 0.4269203543663025 - trainLoss: 0.40261101722717285\n",
      "cnt: 0 - valLoss: 0.4269198179244995 - trainLoss: 0.4026096761226654\n",
      "cnt: 0 - valLoss: 0.4269193708896637 - trainLoss: 0.40260833501815796\n",
      "cnt: 0 - valLoss: 0.4269188940525055 - trainLoss: 0.4026070535182953\n",
      "cnt: 0 - valLoss: 0.4269184470176697 - trainLoss: 0.40260574221611023\n",
      "cnt: 0 - valLoss: 0.4269179403781891 - trainLoss: 0.4026044011116028\n",
      "cnt: 0 - valLoss: 0.42691752314567566 - trainLoss: 0.4026030898094177\n",
      "cnt: 0 - valLoss: 0.4269169867038727 - trainLoss: 0.4026017487049103\n",
      "cnt: 0 - valLoss: 0.42691656947135925 - trainLoss: 0.4026004374027252\n",
      "cnt: 0 - valLoss: 0.42691609263420105 - trainLoss: 0.40259915590286255\n",
      "cnt: 0 - valLoss: 0.42691561579704285 - trainLoss: 0.4025978147983551\n",
      "cnt: 0 - valLoss: 0.42691516876220703 - trainLoss: 0.40259650349617004\n",
      "cnt: 0 - valLoss: 0.42691469192504883 - trainLoss: 0.4025951623916626\n",
      "cnt: 0 - valLoss: 0.42691418528556824 - trainLoss: 0.4025938808917999\n",
      "cnt: 0 - valLoss: 0.4269137382507324 - trainLoss: 0.40259256958961487\n",
      "cnt: 0 - valLoss: 0.4269132614135742 - trainLoss: 0.4025912284851074\n",
      "cnt: 0 - valLoss: 0.4269128143787384 - trainLoss: 0.40258991718292236\n",
      "cnt: 0 - valLoss: 0.4269123375415802 - trainLoss: 0.4025886058807373\n",
      "cnt: 0 - valLoss: 0.426911860704422 - trainLoss: 0.40258729457855225\n",
      "cnt: 0 - valLoss: 0.4269113838672638 - trainLoss: 0.4025859534740448\n",
      "cnt: 0 - valLoss: 0.4269109070301056 - trainLoss: 0.40258464217185974\n",
      "cnt: 0 - valLoss: 0.42691048979759216 - trainLoss: 0.40258336067199707\n",
      "cnt: 0 - valLoss: 0.4269099831581116 - trainLoss: 0.4025820195674896\n",
      "cnt: 0 - valLoss: 0.42690953612327576 - trainLoss: 0.40258070826530457\n",
      "cnt: 0 - valLoss: 0.42690905928611755 - trainLoss: 0.4025793671607971\n",
      "cnt: 0 - valLoss: 0.42690861225128174 - trainLoss: 0.40257805585861206\n",
      "cnt: 0 - valLoss: 0.42690813541412354 - trainLoss: 0.4025767743587494\n",
      "cnt: 0 - valLoss: 0.42690765857696533 - trainLoss: 0.40257543325424194\n",
      "cnt: 0 - valLoss: 0.42690718173980713 - trainLoss: 0.4025741517543793\n",
      "cnt: 0 - valLoss: 0.4269067049026489 - trainLoss: 0.4025728106498718\n",
      "cnt: 0 - valLoss: 0.4269062876701355 - trainLoss: 0.40257149934768677\n",
      "cnt: 0 - valLoss: 0.4269057810306549 - trainLoss: 0.4025701582431793\n",
      "cnt: 0 - valLoss: 0.4269053339958191 - trainLoss: 0.40256887674331665\n",
      "cnt: 0 - valLoss: 0.4269048571586609 - trainLoss: 0.4025675058364868\n",
      "cnt: 0 - valLoss: 0.4269044101238251 - trainLoss: 0.40256622433662415\n",
      "cnt: 0 - valLoss: 0.42690396308898926 - trainLoss: 0.4025649130344391\n",
      "cnt: 0 - valLoss: 0.42690345644950867 - trainLoss: 0.4025636315345764\n",
      "cnt: 0 - valLoss: 0.42690303921699524 - trainLoss: 0.40256229043006897\n",
      "cnt: 0 - valLoss: 0.42690256237983704 - trainLoss: 0.4025609791278839\n",
      "cnt: 0 - valLoss: 0.4269021153450012 - trainLoss: 0.40255963802337646\n",
      "cnt: 0 - valLoss: 0.42690160870552063 - trainLoss: 0.4025583565235138\n",
      "cnt: 0 - valLoss: 0.4269011318683624 - trainLoss: 0.40255704522132874\n",
      "cnt: 0 - valLoss: 0.426900714635849 - trainLoss: 0.4025557041168213\n",
      "cnt: 0 - valLoss: 0.4269002377986908 - trainLoss: 0.40255433320999146\n",
      "cnt: 0 - valLoss: 0.4268997609615326 - trainLoss: 0.4025530517101288\n",
      "cnt: 0 - valLoss: 0.4268993139266968 - trainLoss: 0.4025517702102661\n",
      "cnt: 0 - valLoss: 0.42689886689186096 - trainLoss: 0.40255042910575867\n",
      "cnt: 0 - valLoss: 0.42689839005470276 - trainLoss: 0.4025491178035736\n",
      "cnt: 0 - valLoss: 0.42689797282218933 - trainLoss: 0.40254783630371094\n",
      "cnt: 0 - valLoss: 0.42689746618270874 - trainLoss: 0.4025464951992035\n",
      "cnt: 0 - valLoss: 0.4268970191478729 - trainLoss: 0.40254518389701843\n",
      "cnt: 0 - valLoss: 0.4268965423107147 - trainLoss: 0.40254390239715576\n",
      "cnt: 0 - valLoss: 0.4268960952758789 - trainLoss: 0.4025425612926483\n",
      "cnt: 0 - valLoss: 0.4268956482410431 - trainLoss: 0.40254124999046326\n",
      "cnt: 0 - valLoss: 0.4268951714038849 - trainLoss: 0.4025399386882782\n",
      "cnt: 0 - valLoss: 0.4268947243690491 - trainLoss: 0.40253859758377075\n",
      "cnt: 0 - valLoss: 0.42689424753189087 - trainLoss: 0.4025372862815857\n",
      "cnt: 0 - valLoss: 0.42689377069473267 - trainLoss: 0.40253597497940063\n",
      "cnt: 0 - valLoss: 0.42689335346221924 - trainLoss: 0.40253469347953796\n",
      "cnt: 0 - valLoss: 0.4268929064273834 - trainLoss: 0.4025333523750305\n",
      "cnt: 0 - valLoss: 0.42689239978790283 - trainLoss: 0.40253204107284546\n",
      "cnt: 0 - valLoss: 0.4268919825553894 - trainLoss: 0.402530699968338\n",
      "cnt: 0 - valLoss: 0.4268915355205536 - trainLoss: 0.40252941846847534\n",
      "cnt: 0 - valLoss: 0.4268910586833954 - trainLoss: 0.4025280475616455\n",
      "cnt: 0 - valLoss: 0.42689061164855957 - trainLoss: 0.40252676606178284\n",
      "cnt: 0 - valLoss: 0.42689013481140137 - trainLoss: 0.40252548456192017\n",
      "cnt: 0 - valLoss: 0.42688968777656555 - trainLoss: 0.4025241732597351\n",
      "cnt: 0 - valLoss: 0.42688918113708496 - trainLoss: 0.40252283215522766\n",
      "cnt: 0 - valLoss: 0.42688876390457153 - trainLoss: 0.402521550655365\n",
      "cnt: 0 - valLoss: 0.42688828706741333 - trainLoss: 0.40252017974853516\n",
      "cnt: 0 - valLoss: 0.4268878102302551 - trainLoss: 0.4025188982486725\n",
      "cnt: 0 - valLoss: 0.4268873929977417 - trainLoss: 0.4025176167488098\n",
      "cnt: 0 - valLoss: 0.4268869161605835 - trainLoss: 0.40251624584198\n",
      "cnt: 0 - valLoss: 0.4268864393234253 - trainLoss: 0.4025149643421173\n",
      "cnt: 0 - valLoss: 0.42688602209091187 - trainLoss: 0.40251362323760986\n",
      "cnt: 0 - valLoss: 0.42688554525375366 - trainLoss: 0.4025123119354248\n",
      "cnt: 0 - valLoss: 0.42688509821891785 - trainLoss: 0.40251103043556213\n",
      "cnt: 0 - valLoss: 0.42688465118408203 - trainLoss: 0.4025096893310547\n",
      "cnt: 0 - valLoss: 0.42688417434692383 - trainLoss: 0.40250837802886963\n",
      "cnt: 0 - valLoss: 0.426883727312088 - trainLoss: 0.40250706672668457\n",
      "cnt: 0 - valLoss: 0.4268832802772522 - trainLoss: 0.4025057256221771\n",
      "cnt: 0 - valLoss: 0.4268828332424164 - trainLoss: 0.40250441431999207\n",
      "cnt: 0 - valLoss: 0.4268823564052582 - trainLoss: 0.4025031328201294\n",
      "cnt: 0 - valLoss: 0.42688193917274475 - trainLoss: 0.40250176191329956\n",
      "cnt: 0 - valLoss: 0.42688146233558655 - trainLoss: 0.4025004804134369\n",
      "cnt: 0 - valLoss: 0.42688098549842834 - trainLoss: 0.4024991989135742\n",
      "cnt: 0 - valLoss: 0.4268805682659149 - trainLoss: 0.4024978280067444\n",
      "cnt: 0 - valLoss: 0.4268800914287567 - trainLoss: 0.4024965465068817\n",
      "cnt: 0 - valLoss: 0.4268796145915985 - trainLoss: 0.40249523520469666\n",
      "cnt: 0 - valLoss: 0.4268791973590851 - trainLoss: 0.402493953704834\n",
      "cnt: 0 - valLoss: 0.4268787205219269 - trainLoss: 0.40249261260032654\n",
      "cnt: 0 - valLoss: 0.42687830328941345 - trainLoss: 0.4024913012981415\n",
      "cnt: 0 - valLoss: 0.42687782645225525 - trainLoss: 0.4024899899959564\n",
      "cnt: 0 - valLoss: 0.4268774092197418 - trainLoss: 0.40248867869377136\n",
      "cnt: 0 - valLoss: 0.4268769323825836 - trainLoss: 0.4024873375892639\n",
      "cnt: 0 - valLoss: 0.4268764555454254 - trainLoss: 0.40248602628707886\n",
      "cnt: 0 - valLoss: 0.426876038312912 - trainLoss: 0.4024847447872162\n",
      "cnt: 0 - valLoss: 0.4268755614757538 - trainLoss: 0.40248340368270874\n",
      "cnt: 0 - valLoss: 0.42687511444091797 - trainLoss: 0.4024820923805237\n",
      "cnt: 0 - valLoss: 0.42687469720840454 - trainLoss: 0.402480810880661\n",
      "cnt: 0 - valLoss: 0.4268742799758911 - trainLoss: 0.4024794399738312\n",
      "cnt: 0 - valLoss: 0.4268738031387329 - trainLoss: 0.4024781584739685\n",
      "cnt: 0 - valLoss: 0.4268733561038971 - trainLoss: 0.40247681736946106\n",
      "cnt: 0 - valLoss: 0.4268729090690613 - trainLoss: 0.4024755358695984\n",
      "cnt: 0 - valLoss: 0.4268724322319031 - trainLoss: 0.40247419476509094\n",
      "cnt: 0 - valLoss: 0.42687198519706726 - trainLoss: 0.40247291326522827\n",
      "cnt: 0 - valLoss: 0.42687156796455383 - trainLoss: 0.4024716019630432\n",
      "cnt: 0 - valLoss: 0.42687106132507324 - trainLoss: 0.40247026085853577\n",
      "cnt: 0 - valLoss: 0.4268706142902374 - trainLoss: 0.4024689793586731\n",
      "cnt: 0 - valLoss: 0.426870197057724 - trainLoss: 0.40246760845184326\n",
      "cnt: 0 - valLoss: 0.42686977982521057 - trainLoss: 0.4024663269519806\n",
      "cnt: 0 - valLoss: 0.42686933279037476 - trainLoss: 0.4024650454521179\n",
      "cnt: 0 - valLoss: 0.42686885595321655 - trainLoss: 0.4024636745452881\n",
      "cnt: 0 - valLoss: 0.42686840891838074 - trainLoss: 0.4024623930454254\n",
      "cnt: 0 - valLoss: 0.4268679618835449 - trainLoss: 0.40246108174324036\n",
      "cnt: 0 - valLoss: 0.4268675446510315 - trainLoss: 0.4024597406387329\n",
      "cnt: 0 - valLoss: 0.4268670678138733 - trainLoss: 0.40245845913887024\n",
      "cnt: 0 - valLoss: 0.4268665909767151 - trainLoss: 0.4024571180343628\n",
      "cnt: 0 - valLoss: 0.42686617374420166 - trainLoss: 0.40245580673217773\n",
      "cnt: 0 - valLoss: 0.42686569690704346 - trainLoss: 0.40245452523231506\n",
      "cnt: 0 - valLoss: 0.4268653094768524 - trainLoss: 0.4024531841278076\n",
      "cnt: 0 - valLoss: 0.4268648028373718 - trainLoss: 0.40245187282562256\n",
      "cnt: 0 - valLoss: 0.426864355802536 - trainLoss: 0.4024505913257599\n",
      "cnt: 0 - valLoss: 0.4268639385700226 - trainLoss: 0.4024493098258972\n",
      "cnt: 0 - valLoss: 0.42686352133750916 - trainLoss: 0.40244796872138977\n",
      "cnt: 0 - valLoss: 0.42686307430267334 - trainLoss: 0.4024466574192047\n",
      "cnt: 0 - valLoss: 0.42686259746551514 - trainLoss: 0.40244537591934204\n",
      "cnt: 0 - valLoss: 0.4268621802330017 - trainLoss: 0.4024440050125122\n",
      "cnt: 0 - valLoss: 0.4268617033958435 - trainLoss: 0.40244269371032715\n",
      "cnt: 0 - valLoss: 0.4268612861633301 - trainLoss: 0.4024413824081421\n",
      "cnt: 0 - valLoss: 0.4268608093261719 - trainLoss: 0.40244004130363464\n",
      "cnt: 0 - valLoss: 0.42686042189598083 - trainLoss: 0.402438759803772\n",
      "cnt: 0 - valLoss: 0.42685994505882263 - trainLoss: 0.4024374783039093\n",
      "cnt: 0 - valLoss: 0.4268595278263092 - trainLoss: 0.40243610739707947\n",
      "cnt: 0 - valLoss: 0.426859050989151 - trainLoss: 0.4024348258972168\n",
      "cnt: 0 - valLoss: 0.4268586337566376 - trainLoss: 0.4024335443973541\n",
      "cnt: 0 - valLoss: 0.42685818672180176 - trainLoss: 0.40243223309516907\n",
      "cnt: 0 - valLoss: 0.42685770988464355 - trainLoss: 0.4024308919906616\n",
      "cnt: 0 - valLoss: 0.4268572926521301 - trainLoss: 0.40242958068847656\n",
      "cnt: 0 - valLoss: 0.4268568754196167 - trainLoss: 0.4024282395839691\n",
      "cnt: 0 - valLoss: 0.4268563985824585 - trainLoss: 0.40242695808410645\n",
      "cnt: 0 - valLoss: 0.4268559515476227 - trainLoss: 0.402425616979599\n",
      "cnt: 0 - valLoss: 0.42685550451278687 - trainLoss: 0.40242433547973633\n",
      "cnt: 0 - valLoss: 0.4268551170825958 - trainLoss: 0.40242302417755127\n",
      "cnt: 0 - valLoss: 0.4268546402454376 - trainLoss: 0.4024217426776886\n",
      "cnt: 0 - valLoss: 0.4268541634082794 - trainLoss: 0.40242037177085876\n",
      "cnt: 0 - valLoss: 0.426853746175766 - trainLoss: 0.4024190902709961\n",
      "cnt: 0 - valLoss: 0.4268532991409302 - trainLoss: 0.4024178087711334\n",
      "cnt: 0 - valLoss: 0.42685288190841675 - trainLoss: 0.4024164378643036\n",
      "cnt: 0 - valLoss: 0.4268524646759033 - trainLoss: 0.4024151563644409\n",
      "cnt: 0 - valLoss: 0.4268520176410675 - trainLoss: 0.40241384506225586\n",
      "cnt: 0 - valLoss: 0.4268515408039093 - trainLoss: 0.4024125337600708\n",
      "cnt: 0 - valLoss: 0.4268511235713959 - trainLoss: 0.40241119265556335\n",
      "cnt: 0 - valLoss: 0.42685070633888245 - trainLoss: 0.4024099111557007\n",
      "cnt: 0 - valLoss: 0.42685022950172424 - trainLoss: 0.402408629655838\n",
      "cnt: 0 - valLoss: 0.4268497824668884 - trainLoss: 0.40240731835365295\n",
      "cnt: 0 - valLoss: 0.426849365234375 - trainLoss: 0.4024059772491455\n",
      "cnt: 0 - valLoss: 0.4268488883972168 - trainLoss: 0.40240469574928284\n",
      "cnt: 0 - valLoss: 0.42684850096702576 - trainLoss: 0.402403324842453\n",
      "cnt: 0 - valLoss: 0.42684802412986755 - trainLoss: 0.40240204334259033\n",
      "cnt: 0 - valLoss: 0.42684757709503174 - trainLoss: 0.40240076184272766\n",
      "cnt: 0 - valLoss: 0.4268471896648407 - trainLoss: 0.4023993909358978\n",
      "cnt: 0 - valLoss: 0.4268467426300049 - trainLoss: 0.40239810943603516\n",
      "cnt: 0 - valLoss: 0.4268462657928467 - trainLoss: 0.4023968279361725\n",
      "cnt: 0 - valLoss: 0.42684584856033325 - trainLoss: 0.40239548683166504\n",
      "cnt: 0 - valLoss: 0.4268454313278198 - trainLoss: 0.40239417552948\n",
      "cnt: 0 - valLoss: 0.4268449544906616 - trainLoss: 0.4023928940296173\n",
      "cnt: 0 - valLoss: 0.4268445074558258 - trainLoss: 0.4023915231227875\n",
      "cnt: 0 - valLoss: 0.4268440902233124 - trainLoss: 0.4023902416229248\n",
      "cnt: 0 - valLoss: 0.42684367299079895 - trainLoss: 0.40238896012306213\n",
      "cnt: 0 - valLoss: 0.42684322595596313 - trainLoss: 0.4023876190185547\n",
      "cnt: 0 - valLoss: 0.4268428087234497 - trainLoss: 0.40238630771636963\n",
      "cnt: 0 - valLoss: 0.4268423318862915 - trainLoss: 0.40238499641418457\n",
      "cnt: 0 - valLoss: 0.4268419146537781 - trainLoss: 0.4023836851119995\n",
      "cnt: 0 - valLoss: 0.42684146761894226 - trainLoss: 0.40238240361213684\n",
      "cnt: 0 - valLoss: 0.42684105038642883 - trainLoss: 0.4023810625076294\n",
      "cnt: 0 - valLoss: 0.4268406331539154 - trainLoss: 0.40237975120544434\n",
      "cnt: 0 - valLoss: 0.4268401563167572 - trainLoss: 0.40237846970558167\n",
      "cnt: 0 - valLoss: 0.4268397092819214 - trainLoss: 0.4023771286010742\n",
      "cnt: 0 - valLoss: 0.42683929204940796 - trainLoss: 0.40237584710121155\n",
      "cnt: 0 - valLoss: 0.42683887481689453 - trainLoss: 0.4023745357990265\n",
      "cnt: 0 - valLoss: 0.42683839797973633 - trainLoss: 0.4023732542991638\n",
      "cnt: 0 - valLoss: 0.4268379509449005 - trainLoss: 0.40237191319465637\n",
      "cnt: 0 - valLoss: 0.4268375337123871 - trainLoss: 0.4023705720901489\n",
      "cnt: 0 - valLoss: 0.42683711647987366 - trainLoss: 0.40236929059028625\n",
      "cnt: 0 - valLoss: 0.42683666944503784 - trainLoss: 0.4023679792881012\n",
      "cnt: 0 - valLoss: 0.4268362522125244 - trainLoss: 0.40236663818359375\n",
      "cnt: 0 - valLoss: 0.426835834980011 - trainLoss: 0.4023653566837311\n",
      "cnt: 0 - valLoss: 0.4268353581428528 - trainLoss: 0.4023640751838684\n",
      "cnt: 0 - valLoss: 0.4268348813056946 - trainLoss: 0.40236276388168335\n",
      "cnt: 0 - valLoss: 0.42683449387550354 - trainLoss: 0.4023614227771759\n",
      "cnt: 0 - valLoss: 0.42683401703834534 - trainLoss: 0.40236011147499084\n",
      "cnt: 0 - valLoss: 0.4268335998058319 - trainLoss: 0.4023588299751282\n",
      "cnt: 0 - valLoss: 0.4268331527709961 - trainLoss: 0.4023574888706207\n",
      "cnt: 0 - valLoss: 0.42683273553848267 - trainLoss: 0.4023561477661133\n",
      "cnt: 0 - valLoss: 0.42683231830596924 - trainLoss: 0.4023548662662506\n",
      "cnt: 0 - valLoss: 0.4268318712711334 - trainLoss: 0.40235358476638794\n",
      "cnt: 0 - valLoss: 0.42683145403862 - trainLoss: 0.4023522734642029\n",
      "cnt: 0 - valLoss: 0.4268310070037842 - trainLoss: 0.4023509919643402\n",
      "cnt: 0 - valLoss: 0.42683055996894836 - trainLoss: 0.40234965085983276\n",
      "cnt: 0 - valLoss: 0.4268301725387573 - trainLoss: 0.4023483693599701\n",
      "cnt: 0 - valLoss: 0.4268296957015991 - trainLoss: 0.40234705805778503\n",
      "cnt: 0 - valLoss: 0.4268292486667633 - trainLoss: 0.4023457169532776\n",
      "cnt: 0 - valLoss: 0.4268288314342499 - trainLoss: 0.4023444354534149\n",
      "cnt: 0 - valLoss: 0.4268283545970917 - trainLoss: 0.40234315395355225\n",
      "cnt: 0 - valLoss: 0.42682793736457825 - trainLoss: 0.4023418128490448\n",
      "cnt: 0 - valLoss: 0.4268275201320648 - trainLoss: 0.40234050154685974\n",
      "cnt: 0 - valLoss: 0.426827073097229 - trainLoss: 0.40233922004699707\n",
      "cnt: 0 - valLoss: 0.4268266558647156 - trainLoss: 0.4023378789424896\n",
      "cnt: 0 - valLoss: 0.42682620882987976 - trainLoss: 0.40233656764030457\n",
      "cnt: 0 - valLoss: 0.42682579159736633 - trainLoss: 0.4023352861404419\n",
      "cnt: 0 - valLoss: 0.4268253743648529 - trainLoss: 0.40233394503593445\n",
      "cnt: 0 - valLoss: 0.4268249273300171 - trainLoss: 0.4023326635360718\n",
      "cnt: 0 - valLoss: 0.4268244504928589 - trainLoss: 0.4023313522338867\n",
      "cnt: 0 - valLoss: 0.42682406306266785 - trainLoss: 0.40233004093170166\n",
      "cnt: 0 - valLoss: 0.42682361602783203 - trainLoss: 0.402328759431839\n",
      "cnt: 0 - valLoss: 0.42682313919067383 - trainLoss: 0.40232744812965393\n",
      "cnt: 0 - valLoss: 0.4268227517604828 - trainLoss: 0.4023261070251465\n",
      "cnt: 0 - valLoss: 0.426822304725647 - trainLoss: 0.4023248255252838\n",
      "cnt: 0 - valLoss: 0.42682185769081116 - trainLoss: 0.40232351422309875\n",
      "cnt: 0 - valLoss: 0.42682141065597534 - trainLoss: 0.4023221731185913\n",
      "cnt: 0 - valLoss: 0.4268209934234619 - trainLoss: 0.40232089161872864\n",
      "cnt: 0 - valLoss: 0.4268205761909485 - trainLoss: 0.4023195803165436\n",
      "cnt: 0 - valLoss: 0.42682012915611267 - trainLoss: 0.4023182690143585\n",
      "cnt: 0 - valLoss: 0.42681971192359924 - trainLoss: 0.40231698751449585\n",
      "cnt: 0 - valLoss: 0.4268192648887634 - trainLoss: 0.4023156762123108\n",
      "cnt: 0 - valLoss: 0.4268188178539276 - trainLoss: 0.4023143947124481\n",
      "cnt: 0 - valLoss: 0.4268183708190918 - trainLoss: 0.4023130536079407\n",
      "cnt: 0 - valLoss: 0.42681798338890076 - trainLoss: 0.4023117423057556\n",
      "cnt: 0 - valLoss: 0.42681750655174255 - trainLoss: 0.40231046080589294\n",
      "cnt: 0 - valLoss: 0.4268171191215515 - trainLoss: 0.4023091197013855\n",
      "cnt: 0 - valLoss: 0.4268166124820709 - trainLoss: 0.4023078382015228\n",
      "cnt: 0 - valLoss: 0.4268161952495575 - trainLoss: 0.40230652689933777\n",
      "cnt: 0 - valLoss: 0.42681580781936646 - trainLoss: 0.4023052155971527\n",
      "cnt: 0 - valLoss: 0.42681533098220825 - trainLoss: 0.40230393409729004\n",
      "cnt: 0 - valLoss: 0.4268149137496948 - trainLoss: 0.40230265259742737\n",
      "cnt: 0 - valLoss: 0.426814466714859 - trainLoss: 0.40230128169059753\n",
      "cnt: 0 - valLoss: 0.4268140494823456 - trainLoss: 0.40230000019073486\n",
      "cnt: 0 - valLoss: 0.42681363224983215 - trainLoss: 0.4022986888885498\n",
      "cnt: 0 - valLoss: 0.42681318521499634 - trainLoss: 0.40229740738868713\n",
      "cnt: 0 - valLoss: 0.4268127679824829 - trainLoss: 0.40229612588882446\n",
      "cnt: 0 - valLoss: 0.4268123209476471 - trainLoss: 0.402294784784317\n",
      "cnt: 0 - valLoss: 0.42681190371513367 - trainLoss: 0.40229350328445435\n",
      "cnt: 0 - valLoss: 0.42681142687797546 - trainLoss: 0.4022921621799469\n",
      "cnt: 0 - valLoss: 0.4268110394477844 - trainLoss: 0.40229085087776184\n",
      "cnt: 0 - valLoss: 0.426810622215271 - trainLoss: 0.40228956937789917\n",
      "cnt: 0 - valLoss: 0.4268101453781128 - trainLoss: 0.4022882878780365\n",
      "cnt: 0 - valLoss: 0.42680972814559937 - trainLoss: 0.40228694677352905\n",
      "cnt: 0 - valLoss: 0.4268093407154083 - trainLoss: 0.402285635471344\n",
      "cnt: 0 - valLoss: 0.4268088638782501 - trainLoss: 0.4022843539714813\n",
      "cnt: 0 - valLoss: 0.4268084466457367 - trainLoss: 0.40228304266929626\n",
      "cnt: 0 - valLoss: 0.4268079996109009 - trainLoss: 0.4022817313671112\n",
      "cnt: 0 - valLoss: 0.42680758237838745 - trainLoss: 0.40228039026260376\n",
      "cnt: 0 - valLoss: 0.42680710554122925 - trainLoss: 0.4022791087627411\n",
      "cnt: 0 - valLoss: 0.4268067181110382 - trainLoss: 0.4022778272628784\n",
      "cnt: 0 - valLoss: 0.42680624127388 - trainLoss: 0.40227654576301575\n",
      "cnt: 0 - valLoss: 0.42680585384368896 - trainLoss: 0.4022751748561859\n",
      "cnt: 0 - valLoss: 0.42680543661117554 - trainLoss: 0.40227389335632324\n",
      "cnt: 0 - valLoss: 0.42680495977401733 - trainLoss: 0.40227261185646057\n",
      "cnt: 0 - valLoss: 0.4268045425415039 - trainLoss: 0.4022713005542755\n",
      "cnt: 0 - valLoss: 0.4268040955066681 - trainLoss: 0.40226995944976807\n",
      "cnt: 0 - valLoss: 0.42680367827415466 - trainLoss: 0.4022686779499054\n",
      "cnt: 0 - valLoss: 0.4268032908439636 - trainLoss: 0.4022673964500427\n",
      "cnt: 0 - valLoss: 0.4268028140068054 - trainLoss: 0.40226611495018005\n",
      "cnt: 0 - valLoss: 0.426802396774292 - trainLoss: 0.4022647440433502\n",
      "cnt: 0 - valLoss: 0.42680200934410095 - trainLoss: 0.40226346254348755\n",
      "cnt: 0 - valLoss: 0.42680156230926514 - trainLoss: 0.4022621810436249\n",
      "cnt: 0 - valLoss: 0.4268011152744293 - trainLoss: 0.4022608995437622\n",
      "cnt: 0 - valLoss: 0.4268006682395935 - trainLoss: 0.40225955843925476\n",
      "cnt: 0 - valLoss: 0.4268002510070801 - trainLoss: 0.4022582471370697\n",
      "cnt: 0 - valLoss: 0.42679980397224426 - trainLoss: 0.40225693583488464\n",
      "cnt: 0 - valLoss: 0.42679938673973083 - trainLoss: 0.4022556245326996\n",
      "cnt: 0 - valLoss: 0.4267989695072174 - trainLoss: 0.4022543430328369\n",
      "cnt: 0 - valLoss: 0.4267985224723816 - trainLoss: 0.40225306153297424\n",
      "cnt: 0 - valLoss: 0.42679810523986816 - trainLoss: 0.4022517204284668\n",
      "cnt: 0 - valLoss: 0.42679768800735474 - trainLoss: 0.4022504389286041\n",
      "cnt: 0 - valLoss: 0.4267972409725189 - trainLoss: 0.40224912762641907\n",
      "cnt: 0 - valLoss: 0.4267968237400055 - trainLoss: 0.4022477865219116\n",
      "cnt: 0 - valLoss: 0.4267963767051697 - trainLoss: 0.40224650502204895\n",
      "cnt: 0 - valLoss: 0.42679595947265625 - trainLoss: 0.4022452235221863\n",
      "cnt: 0 - valLoss: 0.4267955422401428 - trainLoss: 0.4022439420223236\n",
      "cnt: 0 - valLoss: 0.426795095205307 - trainLoss: 0.40224260091781616\n",
      "cnt: 0 - valLoss: 0.4267946779727936 - trainLoss: 0.4022412896156311\n",
      "cnt: 0 - valLoss: 0.42679423093795776 - trainLoss: 0.40224000811576843\n",
      "cnt: 0 - valLoss: 0.42679381370544434 - trainLoss: 0.402238667011261\n",
      "cnt: 0 - valLoss: 0.4267933964729309 - trainLoss: 0.4022373855113983\n",
      "cnt: 0 - valLoss: 0.4267929494380951 - trainLoss: 0.40223610401153564\n",
      "cnt: 0 - valLoss: 0.42679253220558167 - trainLoss: 0.4022347629070282\n",
      "cnt: 0 - valLoss: 0.42679211497306824 - trainLoss: 0.40223345160484314\n",
      "cnt: 0 - valLoss: 0.4267916679382324 - trainLoss: 0.40223217010498047\n",
      "cnt: 0 - valLoss: 0.426791250705719 - trainLoss: 0.4022308886051178\n",
      "cnt: 0 - valLoss: 0.4267908036708832 - trainLoss: 0.40222954750061035\n",
      "cnt: 0 - valLoss: 0.42679038643836975 - trainLoss: 0.4022282660007477\n",
      "cnt: 0 - valLoss: 0.4267899692058563 - trainLoss: 0.4022269546985626\n",
      "cnt: 0 - valLoss: 0.4267895221710205 - trainLoss: 0.4022256135940552\n",
      "cnt: 0 - valLoss: 0.4267890453338623 - trainLoss: 0.4022243320941925\n",
      "cnt: 0 - valLoss: 0.42678868770599365 - trainLoss: 0.40222305059432983\n",
      "cnt: 0 - valLoss: 0.42678824067115784 - trainLoss: 0.40222176909446716\n",
      "cnt: 0 - valLoss: 0.4267878234386444 - trainLoss: 0.4022204875946045\n",
      "cnt: 0 - valLoss: 0.4267873764038086 - trainLoss: 0.40221911668777466\n",
      "cnt: 0 - valLoss: 0.42678695917129517 - trainLoss: 0.402217835187912\n",
      "cnt: 0 - valLoss: 0.42678654193878174 - trainLoss: 0.4022165536880493\n",
      "cnt: 0 - valLoss: 0.4267861545085907 - trainLoss: 0.40221527218818665\n",
      "cnt: 0 - valLoss: 0.4267856776714325 - trainLoss: 0.4022139310836792\n",
      "cnt: 0 - valLoss: 0.42678529024124146 - trainLoss: 0.40221261978149414\n",
      "cnt: 0 - valLoss: 0.426784873008728 - trainLoss: 0.4022113084793091\n",
      "cnt: 0 - valLoss: 0.4267844259738922 - trainLoss: 0.4022100269794464\n",
      "cnt: 0 - valLoss: 0.42678403854370117 - trainLoss: 0.40220871567726135\n",
      "cnt: 0 - valLoss: 0.42678356170654297 - trainLoss: 0.4022074341773987\n",
      "cnt: 0 - valLoss: 0.42678317427635193 - trainLoss: 0.40220609307289124\n",
      "cnt: 0 - valLoss: 0.4267827570438385 - trainLoss: 0.40220481157302856\n",
      "cnt: 0 - valLoss: 0.4267823398113251 - trainLoss: 0.4022035300731659\n",
      "cnt: 0 - valLoss: 0.42678189277648926 - trainLoss: 0.40220221877098083\n",
      "cnt: 0 - valLoss: 0.42678147554397583 - trainLoss: 0.4022009074687958\n",
      "cnt: 0 - valLoss: 0.4267810583114624 - trainLoss: 0.4021995961666107\n",
      "cnt: 0 - valLoss: 0.42678067088127136 - trainLoss: 0.40219831466674805\n",
      "cnt: 0 - valLoss: 0.42678022384643555 - trainLoss: 0.4021970331668854\n",
      "cnt: 0 - valLoss: 0.4267798364162445 - trainLoss: 0.40219569206237793\n",
      "cnt: 0 - valLoss: 0.4267794191837311 - trainLoss: 0.40219441056251526\n",
      "cnt: 0 - valLoss: 0.42677897214889526 - trainLoss: 0.4021930992603302\n",
      "cnt: 0 - valLoss: 0.42677855491638184 - trainLoss: 0.40219181776046753\n",
      "cnt: 0 - valLoss: 0.4267781376838684 - trainLoss: 0.4021904766559601\n",
      "cnt: 0 - valLoss: 0.4267776906490326 - trainLoss: 0.4021891951560974\n",
      "cnt: 0 - valLoss: 0.42677730321884155 - trainLoss: 0.40218791365623474\n",
      "cnt: 0 - valLoss: 0.4267768859863281 - trainLoss: 0.4021865725517273\n",
      "cnt: 0 - valLoss: 0.4267764687538147 - trainLoss: 0.4021852910518646\n",
      "cnt: 0 - valLoss: 0.4267760217189789 - trainLoss: 0.40218397974967957\n",
      "cnt: 0 - valLoss: 0.42677560448646545 - trainLoss: 0.4021826982498169\n",
      "cnt: 0 - valLoss: 0.4267752170562744 - trainLoss: 0.40218135714530945\n",
      "cnt: 0 - valLoss: 0.426774799823761 - trainLoss: 0.4021800756454468\n",
      "cnt: 0 - valLoss: 0.42677435278892517 - trainLoss: 0.4021787941455841\n",
      "cnt: 0 - valLoss: 0.42677393555641174 - trainLoss: 0.40217751264572144\n",
      "cnt: 0 - valLoss: 0.4267735481262207 - trainLoss: 0.402176171541214\n",
      "cnt: 0 - valLoss: 0.4267731010913849 - trainLoss: 0.4021748900413513\n",
      "cnt: 0 - valLoss: 0.42677268385887146 - trainLoss: 0.40217357873916626\n",
      "cnt: 0 - valLoss: 0.4267722964286804 - trainLoss: 0.4021722972393036\n",
      "cnt: 0 - valLoss: 0.426771879196167 - trainLoss: 0.4021710157394409\n",
      "cnt: 0 - valLoss: 0.4267714321613312 - trainLoss: 0.40216970443725586\n",
      "cnt: 0 - valLoss: 0.42677101492881775 - trainLoss: 0.4021683931350708\n",
      "cnt: 0 - valLoss: 0.4267705976963043 - trainLoss: 0.40216705203056335\n",
      "cnt: 0 - valLoss: 0.4267701506614685 - trainLoss: 0.4021657705307007\n",
      "cnt: 0 - valLoss: 0.42676976323127747 - trainLoss: 0.402164489030838\n",
      "cnt: 0 - valLoss: 0.4267693758010864 - trainLoss: 0.40216317772865295\n",
      "cnt: 0 - valLoss: 0.426768958568573 - trainLoss: 0.40216192603111267\n",
      "cnt: 0 - valLoss: 0.42676854133605957 - trainLoss: 0.4021606147289276\n",
      "cnt: 0 - valLoss: 0.42676809430122375 - trainLoss: 0.40215927362442017\n",
      "cnt: 0 - valLoss: 0.4267676770687103 - trainLoss: 0.4021579921245575\n",
      "cnt: 0 - valLoss: 0.4267672896385193 - trainLoss: 0.4021567106246948\n",
      "cnt: 0 - valLoss: 0.42676684260368347 - trainLoss: 0.4021553695201874\n",
      "cnt: 0 - valLoss: 0.42676642537117004 - trainLoss: 0.4021540880203247\n",
      "cnt: 0 - valLoss: 0.426766037940979 - trainLoss: 0.40215277671813965\n",
      "cnt: 0 - valLoss: 0.4267656207084656 - trainLoss: 0.402151495218277\n",
      "cnt: 0 - valLoss: 0.42676517367362976 - trainLoss: 0.4021502137184143\n",
      "cnt: 0 - valLoss: 0.42676475644111633 - trainLoss: 0.40214887261390686\n",
      "cnt: 0 - valLoss: 0.4267643690109253 - trainLoss: 0.4021475911140442\n",
      "cnt: 0 - valLoss: 0.42676395177841187 - trainLoss: 0.4021463096141815\n",
      "cnt: 0 - valLoss: 0.42676350474357605 - trainLoss: 0.4021449685096741\n",
      "cnt: 0 - valLoss: 0.426763117313385 - trainLoss: 0.4021436870098114\n",
      "cnt: 0 - valLoss: 0.4267627000808716 - trainLoss: 0.40214240550994873\n",
      "cnt: 0 - valLoss: 0.42676228284835815 - trainLoss: 0.40214109420776367\n",
      "cnt: 0 - valLoss: 0.42676183581352234 - trainLoss: 0.402139812707901\n",
      "cnt: 0 - valLoss: 0.4267614483833313 - trainLoss: 0.40213847160339355\n",
      "cnt: 0 - valLoss: 0.4267614185810089 - trainLoss: 0.4021371901035309\n",
      "cnt: 0 - valLoss: 0.4267614185810089 - trainLoss: 0.40213581919670105\n",
      "cnt: 0 - valLoss: 0.4267614185810089 - trainLoss: 0.4021344780921936\n",
      "cnt: 0 - valLoss: 0.4267614185810089 - trainLoss: 0.40213313698768616\n",
      "cnt: 0 - valLoss: 0.4267614185810089 - trainLoss: 0.4021317958831787\n",
      "cnt: 0 - valLoss: 0.4267614185810089 - trainLoss: 0.40213045477867126\n",
      "cnt: 0 - valLoss: 0.42676135897636414 - trainLoss: 0.4021291136741638\n",
      "cnt: 0 - valLoss: 0.42676135897636414 - trainLoss: 0.40212777256965637\n",
      "cnt: 0 - valLoss: 0.42676132917404175 - trainLoss: 0.40212640166282654\n",
      "cnt: 0 - valLoss: 0.426761269569397 - trainLoss: 0.4021250903606415\n",
      "cnt: 0 - valLoss: 0.426761269569397 - trainLoss: 0.40212374925613403\n",
      "cnt: 0 - valLoss: 0.4267612397670746 - trainLoss: 0.4021223783493042\n",
      "cnt: 0 - valLoss: 0.4267612397670746 - trainLoss: 0.40212106704711914\n",
      "cnt: 0 - valLoss: 0.4267612099647522 - trainLoss: 0.4021197259426117\n",
      "cnt: 0 - valLoss: 0.4267611503601074 - trainLoss: 0.4021183252334595\n",
      "cnt: 0 - valLoss: 0.42676112055778503 - trainLoss: 0.402116984128952\n",
      "cnt: 0 - valLoss: 0.42676106095314026 - trainLoss: 0.40211567282676697\n",
      "cnt: 0 - valLoss: 0.42676106095314026 - trainLoss: 0.40211430191993713\n",
      "cnt: 0 - valLoss: 0.4267609715461731 - trainLoss: 0.4021129608154297\n",
      "cnt: 0 - valLoss: 0.4267609119415283 - trainLoss: 0.40211158990859985\n",
      "cnt: 0 - valLoss: 0.4267609119415283 - trainLoss: 0.4021103084087372\n",
      "cnt: 0 - valLoss: 0.42676082253456116 - trainLoss: 0.40210890769958496\n",
      "cnt: 0 - valLoss: 0.426760733127594 - trainLoss: 0.4021075665950775\n",
      "cnt: 0 - valLoss: 0.4267606735229492 - trainLoss: 0.40210625529289246\n",
      "cnt: 0 - valLoss: 0.42676064372062683 - trainLoss: 0.402104914188385\n",
      "cnt: 0 - valLoss: 0.42676055431365967 - trainLoss: 0.40210360288619995\n",
      "cnt: 0 - valLoss: 0.4267604649066925 - trainLoss: 0.4021022319793701\n",
      "cnt: 0 - valLoss: 0.4267604351043701 - trainLoss: 0.4021008610725403\n",
      "cnt: 0 - valLoss: 0.42676037549972534 - trainLoss: 0.40209951996803284\n",
      "cnt: 0 - valLoss: 0.4267602562904358 - trainLoss: 0.4020982086658478\n",
      "cnt: 0 - valLoss: 0.4267602264881134 - trainLoss: 0.40209686756134033\n",
      "cnt: 0 - valLoss: 0.42676007747650146 - trainLoss: 0.4020954966545105\n",
      "cnt: 0 - valLoss: 0.4267599880695343 - trainLoss: 0.40209418535232544\n",
      "cnt: 0 - valLoss: 0.4267599582672119 - trainLoss: 0.402092844247818\n",
      "cnt: 0 - valLoss: 0.42675986886024475 - trainLoss: 0.40209147334098816\n",
      "cnt: 0 - valLoss: 0.4267597794532776 - trainLoss: 0.4020901620388031\n",
      "cnt: 0 - valLoss: 0.42675966024398804 - trainLoss: 0.40208882093429565\n",
      "cnt: 0 - valLoss: 0.4267595708370209 - trainLoss: 0.4020874500274658\n",
      "cnt: 0 - valLoss: 0.4267594814300537 - trainLoss: 0.40208613872528076\n",
      "cnt: 0 - valLoss: 0.42675936222076416 - trainLoss: 0.4020847976207733\n",
      "cnt: 0 - valLoss: 0.426759272813797 - trainLoss: 0.4020834267139435\n",
      "cnt: 0 - valLoss: 0.42675918340682983 - trainLoss: 0.4020821154117584\n",
      "cnt: 0 - valLoss: 0.42675909399986267 - trainLoss: 0.402080774307251\n",
      "cnt: 0 - valLoss: 0.4267590045928955 - trainLoss: 0.40207940340042114\n",
      "cnt: 0 - valLoss: 0.42675885558128357 - trainLoss: 0.4020780920982361\n",
      "cnt: 0 - valLoss: 0.4267587661743164 - trainLoss: 0.40207675099372864\n",
      "cnt: 0 - valLoss: 0.42675867676734924 - trainLoss: 0.4020753800868988\n",
      "cnt: 0 - valLoss: 0.4267585277557373 - trainLoss: 0.40207409858703613\n",
      "cnt: 0 - valLoss: 0.42675840854644775 - trainLoss: 0.4020727276802063\n",
      "cnt: 0 - valLoss: 0.4267583191394806 - trainLoss: 0.40207141637802124\n",
      "cnt: 0 - valLoss: 0.42675817012786865 - trainLoss: 0.4020700752735138\n",
      "cnt: 0 - valLoss: 0.4267580211162567 - trainLoss: 0.40206870436668396\n",
      "cnt: 0 - valLoss: 0.42675790190696716 - trainLoss: 0.4020673334598541\n",
      "cnt: 0 - valLoss: 0.4267577826976776 - trainLoss: 0.40206605195999146\n",
      "cnt: 0 - valLoss: 0.4267576336860657 - trainLoss: 0.4020646810531616\n",
      "cnt: 0 - valLoss: 0.42675745487213135 - trainLoss: 0.4020633101463318\n",
      "cnt: 0 - valLoss: 0.4267573356628418 - trainLoss: 0.40206199884414673\n",
      "cnt: 0 - valLoss: 0.42675721645355225 - trainLoss: 0.4020606577396393\n",
      "cnt: 0 - valLoss: 0.4267570376396179 - trainLoss: 0.4020593464374542\n",
      "cnt: 0 - valLoss: 0.42675691843032837 - trainLoss: 0.4020580053329468\n",
      "cnt: 0 - valLoss: 0.42675673961639404 - trainLoss: 0.40205663442611694\n",
      "cnt: 0 - valLoss: 0.4267566204071045 - trainLoss: 0.4020553529262543\n",
      "cnt: 0 - valLoss: 0.42675647139549255 - trainLoss: 0.40205398201942444\n",
      "cnt: 0 - valLoss: 0.4267563223838806 - trainLoss: 0.4020526111125946\n",
      "cnt: 0 - valLoss: 0.4267561733722687 - trainLoss: 0.40205129981040955\n",
      "cnt: 0 - valLoss: 0.42675602436065674 - trainLoss: 0.4020499587059021\n",
      "cnt: 0 - valLoss: 0.4267558455467224 - trainLoss: 0.40204864740371704\n",
      "cnt: 0 - valLoss: 0.42675572633743286 - trainLoss: 0.4020473062992096\n",
      "cnt: 0 - valLoss: 0.42675554752349854 - trainLoss: 0.40204593539237976\n",
      "cnt: 0 - valLoss: 0.4267553985118866 - trainLoss: 0.4020446538925171\n",
      "cnt: 0 - valLoss: 0.4267551898956299 - trainLoss: 0.40204328298568726\n",
      "cnt: 0 - valLoss: 0.42675507068634033 - trainLoss: 0.4020419418811798\n",
      "cnt: 0 - valLoss: 0.426754891872406 - trainLoss: 0.40204063057899475\n",
      "cnt: 0 - valLoss: 0.4267547130584717 - trainLoss: 0.4020392894744873\n",
      "cnt: 0 - valLoss: 0.42675456404685974 - trainLoss: 0.40203797817230225\n",
      "cnt: 0 - valLoss: 0.4267543852329254 - trainLoss: 0.4020366072654724\n",
      "cnt: 0 - valLoss: 0.4267542064189911 - trainLoss: 0.40203526616096497\n",
      "cnt: 0 - valLoss: 0.42675402760505676 - trainLoss: 0.4020339548587799\n",
      "cnt: 0 - valLoss: 0.4267538785934448 - trainLoss: 0.40203261375427246\n",
      "cnt: 0 - valLoss: 0.4267536401748657 - trainLoss: 0.4020313024520874\n",
      "cnt: 0 - valLoss: 0.42675352096557617 - trainLoss: 0.40202993154525757\n",
      "cnt: 0 - valLoss: 0.42675334215164185 - trainLoss: 0.4020285904407501\n",
      "cnt: 0 - valLoss: 0.4267531931400299 - trainLoss: 0.40202727913856506\n",
      "cnt: 0 - valLoss: 0.4267529547214508 - trainLoss: 0.4020259380340576\n",
      "cnt: 0 - valLoss: 0.4267527461051941 - trainLoss: 0.40202462673187256\n",
      "cnt: 0 - valLoss: 0.42675259709358215 - trainLoss: 0.4020232558250427\n",
      "cnt: 0 - valLoss: 0.42675235867500305 - trainLoss: 0.4020219147205353\n",
      "cnt: 0 - valLoss: 0.42675215005874634 - trainLoss: 0.4020206034183502\n",
      "cnt: 0 - valLoss: 0.426751971244812 - trainLoss: 0.4020192623138428\n",
      "cnt: 0 - valLoss: 0.4267518222332001 - trainLoss: 0.40201789140701294\n",
      "cnt: 0 - valLoss: 0.4267515540122986 - trainLoss: 0.4020165801048279\n",
      "cnt: 0 - valLoss: 0.42675137519836426 - trainLoss: 0.40201523900032043\n",
      "cnt: 0 - valLoss: 0.42675113677978516 - trainLoss: 0.4020139276981354\n",
      "cnt: 0 - valLoss: 0.42675095796585083 - trainLoss: 0.40201255679130554\n",
      "cnt: 0 - valLoss: 0.42675068974494934 - trainLoss: 0.40201127529144287\n",
      "cnt: 0 - valLoss: 0.4267505407333374 - trainLoss: 0.40200990438461304\n",
      "cnt: 0 - valLoss: 0.4267502725124359 - trainLoss: 0.40200862288475037\n",
      "cnt: 0 - valLoss: 0.4267500638961792 - trainLoss: 0.40200725197792053\n",
      "cnt: 0 - valLoss: 0.4267498254776001 - trainLoss: 0.4020059108734131\n",
      "cnt: 0 - valLoss: 0.4267496168613434 - trainLoss: 0.402004599571228\n",
      "cnt: 0 - valLoss: 0.42674940824508667 - trainLoss: 0.4020032584667206\n",
      "cnt: 0 - valLoss: 0.4267491400241852 - trainLoss: 0.4020019471645355\n",
      "cnt: 0 - valLoss: 0.42674893140792847 - trainLoss: 0.4020005762577057\n",
      "cnt: 0 - valLoss: 0.42674872279167175 - trainLoss: 0.401999294757843\n",
      "cnt: 0 - valLoss: 0.42674851417541504 - trainLoss: 0.40199795365333557\n",
      "cnt: 0 - valLoss: 0.4267483055591583 - trainLoss: 0.40199658274650574\n",
      "cnt: 0 - valLoss: 0.42674803733825684 - trainLoss: 0.4019952714443207\n",
      "cnt: 0 - valLoss: 0.4267478287220001 - trainLoss: 0.4019939601421356\n",
      "cnt: 0 - valLoss: 0.42674756050109863 - trainLoss: 0.4019926190376282\n",
      "cnt: 0 - valLoss: 0.4267473518848419 - trainLoss: 0.40199124813079834\n",
      "cnt: 0 - valLoss: 0.4267471432685852 - trainLoss: 0.4019899368286133\n",
      "cnt: 0 - valLoss: 0.4267468750476837 - trainLoss: 0.40198859572410583\n",
      "cnt: 0 - valLoss: 0.426746666431427 - trainLoss: 0.4019872844219208\n",
      "cnt: 0 - valLoss: 0.4267463982105255 - trainLoss: 0.4019859731197357\n",
      "cnt: 0 - valLoss: 0.4267461895942688 - trainLoss: 0.40198463201522827\n",
      "cnt: 0 - valLoss: 0.4267459809780121 - trainLoss: 0.40198326110839844\n",
      "cnt: 0 - valLoss: 0.4267457127571106 - trainLoss: 0.4019818902015686\n",
      "cnt: 0 - valLoss: 0.4267454743385315 - trainLoss: 0.40198060870170593\n",
      "cnt: 0 - valLoss: 0.42674520611763 - trainLoss: 0.4019792973995209\n",
      "cnt: 0 - valLoss: 0.4267449676990509 - trainLoss: 0.4019779562950134\n",
      "cnt: 0 - valLoss: 0.4267447292804718 - trainLoss: 0.40197664499282837\n",
      "cnt: 0 - valLoss: 0.4267444908618927 - trainLoss: 0.4019753038883209\n",
      "cnt: 0 - valLoss: 0.4267441928386688 - trainLoss: 0.4019739329814911\n",
      "cnt: 0 - valLoss: 0.4267439544200897 - trainLoss: 0.4019726514816284\n",
      "cnt: 0 - valLoss: 0.426743745803833 - trainLoss: 0.40197134017944336\n",
      "cnt: 0 - valLoss: 0.42674341797828674 - trainLoss: 0.4019699990749359\n",
      "cnt: 0 - valLoss: 0.42674320936203003 - trainLoss: 0.40196868777275085\n",
      "cnt: 0 - valLoss: 0.42674294114112854 - trainLoss: 0.4019673466682434\n",
      "cnt: 0 - valLoss: 0.42674264311790466 - trainLoss: 0.4019659757614136\n",
      "cnt: 0 - valLoss: 0.42674243450164795 - trainLoss: 0.4019646644592285\n",
      "cnt: 0 - valLoss: 0.4267421364784241 - trainLoss: 0.40196332335472107\n",
      "cnt: 0 - valLoss: 0.4267418682575226 - trainLoss: 0.40196195244789124\n",
      "cnt: 0 - valLoss: 0.4267415702342987 - trainLoss: 0.4019606411457062\n",
      "cnt: 0 - valLoss: 0.4267413020133972 - trainLoss: 0.40195930004119873\n",
      "cnt: 0 - valLoss: 0.4267410635948181 - trainLoss: 0.40195801854133606\n",
      "cnt: 0 - valLoss: 0.42674076557159424 - trainLoss: 0.4019566476345062\n",
      "cnt: 0 - valLoss: 0.426740825176239 - trainLoss: 0.40195536613464355\n",
      "cnt: 1 - valLoss: 0.4267405867576599 - trainLoss: 0.4019539952278137\n",
      "cnt: 0 - valLoss: 0.42674028873443604 - trainLoss: 0.40195268392562866\n",
      "cnt: 0 - valLoss: 0.4267403781414032 - trainLoss: 0.4019513428211212\n",
      "cnt: 1 - valLoss: 0.4267401099205017 - trainLoss: 0.40195003151893616\n",
      "cnt: 0 - valLoss: 0.4267401397228241 - trainLoss: 0.4019486904144287\n",
      "cnt: 1 - valLoss: 0.426739901304245 - trainLoss: 0.40194734930992126\n",
      "cnt: 0 - valLoss: 0.42673954367637634 - trainLoss: 0.4019460380077362\n",
      "cnt: 0 - valLoss: 0.4267396330833435 - trainLoss: 0.40194469690322876\n",
      "cnt: 1 - valLoss: 0.4267393946647644 - trainLoss: 0.4019433856010437\n",
      "cnt: 0 - valLoss: 0.4267390966415405 - trainLoss: 0.40194201469421387\n",
      "cnt: 0 - valLoss: 0.4267391264438629 - trainLoss: 0.4019407331943512\n",
      "cnt: 1 - valLoss: 0.42673876881599426 - trainLoss: 0.40193936228752136\n",
      "cnt: 0 - valLoss: 0.42673853039741516 - trainLoss: 0.4019380807876587\n",
      "cnt: 0 - valLoss: 0.4267386198043823 - trainLoss: 0.40193670988082886\n",
      "cnt: 1 - valLoss: 0.42673826217651367 - trainLoss: 0.4019354283809662\n",
      "cnt: 0 - valLoss: 0.42673835158348083 - trainLoss: 0.40193405747413635\n",
      "cnt: 1 - valLoss: 0.42673802375793457 - trainLoss: 0.4019327163696289\n",
      "cnt: 0 - valLoss: 0.4267377555370331 - trainLoss: 0.40193140506744385\n",
      "cnt: 0 - valLoss: 0.42673778533935547 - trainLoss: 0.4019300639629364\n",
      "cnt: 1 - valLoss: 0.4267374575138092 - trainLoss: 0.40192878246307373\n",
      "cnt: 0 - valLoss: 0.4267374873161316 - trainLoss: 0.40192747116088867\n",
      "cnt: 1 - valLoss: 0.4267372488975525 - trainLoss: 0.40192610025405884\n",
      "cnt: 0 - valLoss: 0.42673689126968384 - trainLoss: 0.40192481875419617\n",
      "cnt: 0 - valLoss: 0.4267369508743286 - trainLoss: 0.40192344784736633\n",
      "cnt: 1 - valLoss: 0.4267365634441376 - trainLoss: 0.40192216634750366\n",
      "cnt: 0 - valLoss: 0.42673665285110474 - trainLoss: 0.40192079544067383\n",
      "cnt: 1 - valLoss: 0.42673632502555847 - trainLoss: 0.40191951394081116\n",
      "cnt: 0 - valLoss: 0.42673638463020325 - trainLoss: 0.4019181430339813\n",
      "cnt: 1 - valLoss: 0.4267360270023346 - trainLoss: 0.40191683173179626\n",
      "cnt: 0 - valLoss: 0.42673569917678833 - trainLoss: 0.4019154906272888\n",
      "cnt: 0 - valLoss: 0.4267357289791107 - trainLoss: 0.40191414952278137\n",
      "cnt: 1 - valLoss: 0.42673540115356445 - trainLoss: 0.4019128084182739\n",
      "cnt: 0 - valLoss: 0.42673543095588684 - trainLoss: 0.40191152691841125\n",
      "cnt: 1 - valLoss: 0.4267350435256958 - trainLoss: 0.4019101560115814\n",
      "cnt: 0 - valLoss: 0.4267351031303406 - trainLoss: 0.40190884470939636\n",
      "cnt: 1 - valLoss: 0.4267347455024719 - trainLoss: 0.4019075632095337\n",
      "cnt: 0 - valLoss: 0.4267343580722809 - trainLoss: 0.40190622210502625\n",
      "cnt: 0 - valLoss: 0.42673441767692566 - trainLoss: 0.4019049108028412\n",
      "cnt: 1 - valLoss: 0.426734060049057 - trainLoss: 0.40190356969833374\n",
      "cnt: 0 - valLoss: 0.426734060049057 - trainLoss: 0.4019021987915039\n",
      "cnt: 0 - valLoss: 0.42673373222351074 - trainLoss: 0.40190091729164124\n",
      "cnt: 0 - valLoss: 0.42673373222351074 - trainLoss: 0.4018996059894562\n",
      "cnt: 0 - valLoss: 0.4267333745956421 - trainLoss: 0.40189826488494873\n",
      "cnt: 0 - valLoss: 0.4267333745956421 - trainLoss: 0.4018968939781189\n",
      "cnt: 0 - valLoss: 0.42673298716545105 - trainLoss: 0.4018956124782562\n",
      "cnt: 0 - valLoss: 0.4267330467700958 - trainLoss: 0.40189430117607117\n",
      "cnt: 1 - valLoss: 0.4267326593399048 - trainLoss: 0.4018929600715637\n",
      "cnt: 0 - valLoss: 0.42673259973526 - trainLoss: 0.40189167857170105\n",
      "cnt: 0 - valLoss: 0.42673227190971375 - trainLoss: 0.4018903076648712\n",
      "cnt: 0 - valLoss: 0.42673230171203613 - trainLoss: 0.40188902616500854\n",
      "cnt: 1 - valLoss: 0.4267319142818451 - trainLoss: 0.4018876552581787\n",
      "cnt: 0 - valLoss: 0.4267318844795227 - trainLoss: 0.40188637375831604\n",
      "cnt: 0 - valLoss: 0.42673149704933167 - trainLoss: 0.4018850028514862\n",
      "cnt: 0 - valLoss: 0.42673152685165405 - trainLoss: 0.40188366174697876\n",
      "cnt: 1 - valLoss: 0.426731139421463 - trainLoss: 0.4018823504447937\n",
      "cnt: 0 - valLoss: 0.426731139421463 - trainLoss: 0.40188106894493103\n",
      "cnt: 0 - valLoss: 0.4267307221889496 - trainLoss: 0.4018797278404236\n",
      "cnt: 0 - valLoss: 0.4267307221889496 - trainLoss: 0.4018784165382385\n",
      "cnt: 0 - valLoss: 0.4267303943634033 - trainLoss: 0.4018770754337311\n",
      "cnt: 0 - valLoss: 0.42673030495643616 - trainLoss: 0.401875764131546\n",
      "cnt: 0 - valLoss: 0.4267299473285675 - trainLoss: 0.4018744230270386\n",
      "cnt: 0 - valLoss: 0.4267299175262451 - trainLoss: 0.4018731117248535\n",
      "cnt: 0 - valLoss: 0.4267295300960541 - trainLoss: 0.40187177062034607\n",
      "cnt: 0 - valLoss: 0.4267294704914093 - trainLoss: 0.401870459318161\n",
      "cnt: 0 - valLoss: 0.42672908306121826 - trainLoss: 0.40186917781829834\n",
      "cnt: 0 - valLoss: 0.4267290532588959 - trainLoss: 0.4018678069114685\n",
      "cnt: 0 - valLoss: 0.42672866582870483 - trainLoss: 0.40186652541160583\n",
      "cnt: 0 - valLoss: 0.42672863602638245 - trainLoss: 0.401865154504776\n",
      "cnt: 0 - valLoss: 0.4267282485961914 - trainLoss: 0.40186384320259094\n",
      "cnt: 0 - valLoss: 0.4267282485961914 - trainLoss: 0.4018625319004059\n",
      "cnt: 0 - valLoss: 0.4267278015613556 - trainLoss: 0.40186119079589844\n",
      "cnt: 0 - valLoss: 0.4267277121543884 - trainLoss: 0.4018598794937134\n",
      "cnt: 0 - valLoss: 0.4267277121543884 - trainLoss: 0.40185853838920593\n",
      "cnt: 0 - valLoss: 0.426727294921875 - trainLoss: 0.4018572270870209\n",
      "cnt: 0 - valLoss: 0.4267272651195526 - trainLoss: 0.4018558859825134\n",
      "cnt: 0 - valLoss: 0.4267268180847168 - trainLoss: 0.40185460448265076\n",
      "cnt: 0 - valLoss: 0.4267267882823944 - trainLoss: 0.4018532931804657\n",
      "cnt: 0 - valLoss: 0.42672640085220337 - trainLoss: 0.40185195207595825\n",
      "cnt: 0 - valLoss: 0.4267263412475586 - trainLoss: 0.4018506705760956\n",
      "cnt: 0 - valLoss: 0.42672625184059143 - trainLoss: 0.40184929966926575\n",
      "cnt: 0 - valLoss: 0.4267258942127228 - trainLoss: 0.4018479883670807\n",
      "cnt: 0 - valLoss: 0.4267258048057556 - trainLoss: 0.40184664726257324\n",
      "cnt: 0 - valLoss: 0.4267253577709198 - trainLoss: 0.40184536576271057\n",
      "cnt: 0 - valLoss: 0.4267253279685974 - trainLoss: 0.4018440544605255\n",
      "cnt: 0 - valLoss: 0.4267248809337616 - trainLoss: 0.40184271335601807\n",
      "cnt: 0 - valLoss: 0.4267250597476959 - trainLoss: 0.4018413722515106\n",
      "cnt: 1 - valLoss: 0.4267248213291168 - trainLoss: 0.40184006094932556\n",
      "cnt: 0 - valLoss: 0.4267246425151825 - trainLoss: 0.4018387794494629\n",
      "cnt: 0 - valLoss: 0.4267244338989258 - trainLoss: 0.4018374979496002\n",
      "cnt: 0 - valLoss: 0.4267241954803467 - trainLoss: 0.40183621644973755\n",
      "cnt: 0 - valLoss: 0.4267239570617676 - trainLoss: 0.40183496475219727\n",
      "cnt: 0 - valLoss: 0.42672374844551086 - trainLoss: 0.4018336534500122\n",
      "cnt: 0 - valLoss: 0.42672351002693176 - trainLoss: 0.40183234214782715\n",
      "cnt: 0 - valLoss: 0.42672330141067505 - trainLoss: 0.4018310606479645\n",
      "cnt: 0 - valLoss: 0.426723450422287 - trainLoss: 0.4018297791481018\n",
      "cnt: 1 - valLoss: 0.4267232120037079 - trainLoss: 0.40182849764823914\n",
      "cnt: 0 - valLoss: 0.4267229735851288 - trainLoss: 0.40182721614837646\n",
      "cnt: 0 - valLoss: 0.42672276496887207 - trainLoss: 0.4018259346485138\n",
      "cnt: 0 - valLoss: 0.4267224967479706 - trainLoss: 0.4018246531486511\n",
      "cnt: 0 - valLoss: 0.42672228813171387 - trainLoss: 0.4018233120441437\n",
      "cnt: 0 - valLoss: 0.4267220199108124 - trainLoss: 0.401822030544281\n",
      "cnt: 0 - valLoss: 0.42672181129455566 - trainLoss: 0.40182074904441833\n",
      "cnt: 0 - valLoss: 0.42672160267829895 - trainLoss: 0.40181946754455566\n",
      "cnt: 0 - valLoss: 0.42672139406204224 - trainLoss: 0.401818186044693\n",
      "cnt: 0 - valLoss: 0.42672106623649597 - trainLoss: 0.4018169045448303\n",
      "cnt: 0 - valLoss: 0.4267212152481079 - trainLoss: 0.4018155634403229\n",
      "cnt: 1 - valLoss: 0.4267209470272064 - trainLoss: 0.4018142819404602\n",
      "cnt: 0 - valLoss: 0.4267207086086273 - trainLoss: 0.40181300044059753\n",
      "cnt: 0 - valLoss: 0.4267204701900482 - trainLoss: 0.40181171894073486\n",
      "cnt: 0 - valLoss: 0.4267202317714691 - trainLoss: 0.4018104374408722\n",
      "cnt: 0 - valLoss: 0.4267200231552124 - trainLoss: 0.4018091559410095\n",
      "cnt: 0 - valLoss: 0.4267197549343109 - trainLoss: 0.40180787444114685\n",
      "cnt: 0 - valLoss: 0.4267194867134094 - trainLoss: 0.4018065929412842\n",
      "cnt: 0 - valLoss: 0.4267192482948303 - trainLoss: 0.4018053114414215\n",
      "cnt: 0 - valLoss: 0.4267193078994751 - trainLoss: 0.40180402994155884\n",
      "cnt: 1 - valLoss: 0.426719069480896 - trainLoss: 0.40180274844169617\n",
      "cnt: 0 - valLoss: 0.4267188012599945 - trainLoss: 0.4018014669418335\n",
      "cnt: 0 - valLoss: 0.4267185628414154 - trainLoss: 0.40180015563964844\n",
      "cnt: 0 - valLoss: 0.4267182946205139 - trainLoss: 0.40179887413978577\n",
      "cnt: 0 - valLoss: 0.42671799659729004 - trainLoss: 0.4017975628376007\n",
      "cnt: 0 - valLoss: 0.42671769857406616 - trainLoss: 0.4017963111400604\n",
      "cnt: 0 - valLoss: 0.4267178773880005 - trainLoss: 0.40179499983787537\n",
      "cnt: 1 - valLoss: 0.426717609167099 - trainLoss: 0.4017937183380127\n",
      "cnt: 0 - valLoss: 0.42671725153923035 - trainLoss: 0.40179240703582764\n",
      "cnt: 0 - valLoss: 0.42671701312065125 - trainLoss: 0.40179112553596497\n",
      "cnt: 0 - valLoss: 0.42671674489974976 - trainLoss: 0.4017898440361023\n",
      "cnt: 0 - valLoss: 0.4267164468765259 - trainLoss: 0.4017885625362396\n",
      "cnt: 0 - valLoss: 0.42671656608581543 - trainLoss: 0.40178728103637695\n",
      "cnt: 1 - valLoss: 0.42671623826026917 - trainLoss: 0.4017859995365143\n",
      "cnt: 0 - valLoss: 0.4267159700393677 - trainLoss: 0.4017846882343292\n",
      "cnt: 0 - valLoss: 0.4267156720161438 - trainLoss: 0.40178343653678894\n",
      "cnt: 0 - valLoss: 0.4267154335975647 - trainLoss: 0.40178215503692627\n",
      "cnt: 0 - valLoss: 0.4267151355743408 - trainLoss: 0.4017808735370636\n",
      "cnt: 0 - valLoss: 0.4267151951789856 - trainLoss: 0.4017795920372009\n",
      "cnt: 1 - valLoss: 0.4267149567604065 - trainLoss: 0.4017782509326935\n",
      "cnt: 0 - valLoss: 0.4267146587371826 - trainLoss: 0.4017769694328308\n",
      "cnt: 0 - valLoss: 0.42671436071395874 - trainLoss: 0.40177568793296814\n",
      "cnt: 0 - valLoss: 0.4267140030860901 - trainLoss: 0.40177443623542786\n",
      "cnt: 0 - valLoss: 0.426713764667511 - trainLoss: 0.4017731249332428\n",
      "cnt: 0 - valLoss: 0.42671382427215576 - trainLoss: 0.4017718434333801\n",
      "cnt: 1 - valLoss: 0.42671358585357666 - trainLoss: 0.40177056193351746\n",
      "cnt: 0 - valLoss: 0.426713228225708 - trainLoss: 0.4017692804336548\n",
      "cnt: 0 - valLoss: 0.42671293020248413 - trainLoss: 0.4017679691314697\n",
      "cnt: 0 - valLoss: 0.42671263217926025 - trainLoss: 0.40176668763160706\n",
      "cnt: 0 - valLoss: 0.4267127215862274 - trainLoss: 0.4017654061317444\n",
      "cnt: 1 - valLoss: 0.42671242356300354 - trainLoss: 0.4017641246318817\n",
      "cnt: 0 - valLoss: 0.42671212553977966 - trainLoss: 0.40176284313201904\n",
      "cnt: 0 - valLoss: 0.4267118275165558 - trainLoss: 0.40176156163215637\n",
      "cnt: 0 - valLoss: 0.4267115294933319 - trainLoss: 0.4017602801322937\n",
      "cnt: 0 - valLoss: 0.4267116189002991 - trainLoss: 0.40175899863243103\n",
      "cnt: 1 - valLoss: 0.4267112612724304 - trainLoss: 0.40175771713256836\n",
      "cnt: 0 - valLoss: 0.42671096324920654 - trainLoss: 0.4017564356327057\n",
      "cnt: 0 - valLoss: 0.42671066522598267 - trainLoss: 0.4017551839351654\n",
      "cnt: 0 - valLoss: 0.4267103672027588 - trainLoss: 0.40175384283065796\n",
      "cnt: 0 - valLoss: 0.4267103970050812 - trainLoss: 0.4017525613307953\n",
      "cnt: 1 - valLoss: 0.4267100989818573 - trainLoss: 0.4017512798309326\n",
      "cnt: 0 - valLoss: 0.42670977115631104 - trainLoss: 0.40174999833106995\n",
      "cnt: 0 - valLoss: 0.42670950293540955 - trainLoss: 0.4017487168312073\n",
      "cnt: 0 - valLoss: 0.4267095625400543 - trainLoss: 0.4017474353313446\n",
      "cnt: 1 - valLoss: 0.42670920491218567 - trainLoss: 0.4017461836338043\n",
      "cnt: 0 - valLoss: 0.4267088770866394 - trainLoss: 0.40174490213394165\n",
      "cnt: 0 - valLoss: 0.4267085790634155 - trainLoss: 0.4017435908317566\n",
      "cnt: 0 - valLoss: 0.4267086088657379 - trainLoss: 0.4017423093318939\n",
      "cnt: 1 - valLoss: 0.42670831084251404 - trainLoss: 0.40174102783203125\n",
      "cnt: 0 - valLoss: 0.4267079532146454 - trainLoss: 0.40173977613449097\n",
      "cnt: 0 - valLoss: 0.4267076551914215 - trainLoss: 0.4017384946346283\n",
      "cnt: 0 - valLoss: 0.42670732736587524 - trainLoss: 0.40173715353012085\n",
      "cnt: 0 - valLoss: 0.42670735716819763 - trainLoss: 0.4017358720302582\n",
      "cnt: 1 - valLoss: 0.42670705914497375 - trainLoss: 0.4017345905303955\n",
      "cnt: 0 - valLoss: 0.4267067313194275 - trainLoss: 0.4017333686351776\n",
      "cnt: 0 - valLoss: 0.4267067611217499 - trainLoss: 0.40173202753067017\n",
      "cnt: 1 - valLoss: 0.4267064332962036 - trainLoss: 0.4017307460308075\n",
      "cnt: 0 - valLoss: 0.42670607566833496 - trainLoss: 0.4017294645309448\n",
      "cnt: 0 - valLoss: 0.4267057478427887 - trainLoss: 0.40172821283340454\n",
      "cnt: 0 - valLoss: 0.4267057776451111 - trainLoss: 0.40172693133354187\n",
      "cnt: 1 - valLoss: 0.4267054796218872 - trainLoss: 0.4017256498336792\n",
      "cnt: 0 - valLoss: 0.42670509219169617 - trainLoss: 0.40172436833381653\n",
      "cnt: 0 - valLoss: 0.4267047643661499 - trainLoss: 0.40172311663627625\n",
      "cnt: 0 - valLoss: 0.4267047941684723 - trainLoss: 0.4017218351364136\n",
      "cnt: 1 - valLoss: 0.4267044961452484 - trainLoss: 0.4017205238342285\n",
      "cnt: 0 - valLoss: 0.4267041087150574 - trainLoss: 0.40171924233436584\n",
      "cnt: 0 - valLoss: 0.4267037510871887 - trainLoss: 0.40171799063682556\n",
      "cnt: 0 - valLoss: 0.4267038106918335 - trainLoss: 0.4017167091369629\n",
      "cnt: 1 - valLoss: 0.42670345306396484 - trainLoss: 0.4017154276371002\n",
      "cnt: 0 - valLoss: 0.4267031252384186 - trainLoss: 0.40171414613723755\n",
      "cnt: 0 - valLoss: 0.4267031252384186 - trainLoss: 0.4017128646373749\n",
      "cnt: 0 - valLoss: 0.4267027676105499 - trainLoss: 0.4017115831375122\n",
      "cnt: 0 - valLoss: 0.4267023801803589 - trainLoss: 0.40171027183532715\n",
      "cnt: 0 - valLoss: 0.42670243978500366 - trainLoss: 0.4017089903354645\n",
      "cnt: 1 - valLoss: 0.426702082157135 - trainLoss: 0.4017077088356018\n",
      "cnt: 0 - valLoss: 0.42670169472694397 - trainLoss: 0.40170642733573914\n",
      "cnt: 0 - valLoss: 0.4267013669013977 - trainLoss: 0.40170514583587646\n",
      "cnt: 0 - valLoss: 0.4267013669013977 - trainLoss: 0.4017038643360138\n",
      "cnt: 0 - valLoss: 0.42670097947120667 - trainLoss: 0.4017025828361511\n",
      "cnt: 0 - valLoss: 0.426700621843338 - trainLoss: 0.40170133113861084\n",
      "cnt: 0 - valLoss: 0.426700621843338 - trainLoss: 0.4017000198364258\n",
      "cnt: 0 - valLoss: 0.426700234413147 - trainLoss: 0.4016987681388855\n",
      "cnt: 0 - valLoss: 0.4266998767852783 - trainLoss: 0.4016974866390228\n",
      "cnt: 0 - valLoss: 0.4266998767852783 - trainLoss: 0.40169623494148254\n",
      "cnt: 0 - valLoss: 0.42669951915740967 - trainLoss: 0.4016949534416199\n",
      "cnt: 0 - valLoss: 0.42669913172721863 - trainLoss: 0.4016936719417572\n",
      "cnt: 0 - valLoss: 0.42669910192489624 - trainLoss: 0.40169239044189453\n",
      "cnt: 0 - valLoss: 0.4266987442970276 - trainLoss: 0.40169110894203186\n",
      "cnt: 0 - valLoss: 0.42669835686683655 - trainLoss: 0.4016898274421692\n",
      "cnt: 0 - valLoss: 0.42669835686683655 - trainLoss: 0.4016885459423065\n",
      "cnt: 0 - valLoss: 0.4266979694366455 - trainLoss: 0.40168726444244385\n",
      "cnt: 0 - valLoss: 0.42669758200645447 - trainLoss: 0.40168601274490356\n",
      "cnt: 0 - valLoss: 0.4266975522041321 - trainLoss: 0.4016847014427185\n",
      "cnt: 0 - valLoss: 0.42669716477394104 - trainLoss: 0.4016834497451782\n",
      "cnt: 0 - valLoss: 0.4266968071460724 - trainLoss: 0.40168216824531555\n",
      "cnt: 0 - valLoss: 0.42669677734375 - trainLoss: 0.40168091654777527\n",
      "cnt: 0 - valLoss: 0.42669641971588135 - trainLoss: 0.4016796350479126\n",
      "cnt: 0 - valLoss: 0.42669638991355896 - trainLoss: 0.4016783535480499\n",
      "cnt: 0 - valLoss: 0.4266960024833679 - trainLoss: 0.40167707204818726\n",
      "cnt: 0 - valLoss: 0.4266955852508545 - trainLoss: 0.4016757905483246\n",
      "cnt: 0 - valLoss: 0.4266956150531769 - trainLoss: 0.4016745090484619\n",
      "cnt: 1 - valLoss: 0.42669519782066345 - trainLoss: 0.40167325735092163\n",
      "cnt: 0 - valLoss: 0.4266948103904724 - trainLoss: 0.40167197585105896\n",
      "cnt: 0 - valLoss: 0.4266948103904724 - trainLoss: 0.4016706943511963\n",
      "cnt: 0 - valLoss: 0.4266943633556366 - trainLoss: 0.4016694128513336\n",
      "cnt: 0 - valLoss: 0.42669394612312317 - trainLoss: 0.40166816115379333\n",
      "cnt: 0 - valLoss: 0.42669394612312317 - trainLoss: 0.4016668498516083\n",
      "cnt: 0 - valLoss: 0.42669352889060974 - trainLoss: 0.401665598154068\n",
      "cnt: 0 - valLoss: 0.42669352889060974 - trainLoss: 0.4016643166542053\n",
      "cnt: 0 - valLoss: 0.4266930818557739 - trainLoss: 0.40166303515434265\n",
      "cnt: 0 - valLoss: 0.4266926646232605 - trainLoss: 0.40166175365448\n",
      "cnt: 0 - valLoss: 0.4266926646232605 - trainLoss: 0.4016605019569397\n",
      "cnt: 0 - valLoss: 0.42669227719306946 - trainLoss: 0.401659220457077\n",
      "cnt: 0 - valLoss: 0.42669224739074707 - trainLoss: 0.40165793895721436\n",
      "cnt: 0 - valLoss: 0.42669180035591125 - trainLoss: 0.4016566872596741\n",
      "cnt: 0 - valLoss: 0.4266913831233978 - trainLoss: 0.4016554057598114\n",
      "cnt: 0 - valLoss: 0.4266913831233978 - trainLoss: 0.40165412425994873\n",
      "cnt: 0 - valLoss: 0.426690936088562 - trainLoss: 0.40165284276008606\n",
      "cnt: 0 - valLoss: 0.426690936088562 - trainLoss: 0.4016515612602234\n",
      "cnt: 0 - valLoss: 0.4266904890537262 - trainLoss: 0.4016503095626831\n",
      "cnt: 0 - valLoss: 0.42669010162353516 - trainLoss: 0.40164902806282043\n",
      "cnt: 0 - valLoss: 0.426690012216568 - trainLoss: 0.40164774656295776\n",
      "cnt: 0 - valLoss: 0.42668962478637695 - trainLoss: 0.40164652466773987\n",
      "cnt: 0 - valLoss: 0.4266895651817322 - trainLoss: 0.4016452133655548\n",
      "cnt: 0 - valLoss: 0.42668914794921875 - trainLoss: 0.40164393186569214\n",
      "cnt: 0 - valLoss: 0.4266887307167053 - trainLoss: 0.40164265036582947\n",
      "cnt: 0 - valLoss: 0.4266887307167053 - trainLoss: 0.4016414284706116\n",
      "cnt: 0 - valLoss: 0.4266882538795471 - trainLoss: 0.4016400873661041\n",
      "cnt: 0 - valLoss: 0.42668819427490234 - trainLoss: 0.40163883566856384\n",
      "cnt: 0 - valLoss: 0.4266877770423889 - trainLoss: 0.40163755416870117\n",
      "cnt: 0 - valLoss: 0.42668774724006653 - trainLoss: 0.4016363322734833\n",
      "cnt: 0 - valLoss: 0.4266873002052307 - trainLoss: 0.40163499116897583\n",
      "cnt: 0 - valLoss: 0.4266868829727173 - trainLoss: 0.40163376927375793\n",
      "cnt: 0 - valLoss: 0.4266868233680725 - trainLoss: 0.40163251757621765\n",
      "cnt: 0 - valLoss: 0.4266864061355591 - trainLoss: 0.4016311764717102\n",
      "cnt: 0 - valLoss: 0.4266863763332367 - trainLoss: 0.40162989497184753\n",
      "cnt: 0 - valLoss: 0.4266858994960785 - trainLoss: 0.40162867307662964\n",
      "cnt: 0 - valLoss: 0.4266858398914337 - trainLoss: 0.40162739157676697\n",
      "cnt: 0 - valLoss: 0.4266853630542755 - trainLoss: 0.4016260802745819\n",
      "cnt: 0 - valLoss: 0.4266849756240845 - trainLoss: 0.401624858379364\n",
      "cnt: 0 - valLoss: 0.4266849160194397 - trainLoss: 0.40162357687950134\n",
      "cnt: 0 - valLoss: 0.4266844391822815 - trainLoss: 0.40162229537963867\n",
      "cnt: 0 - valLoss: 0.4266844391822815 - trainLoss: 0.4016210436820984\n",
      "cnt: 0 - valLoss: 0.4266839921474457 - trainLoss: 0.4016197621822357\n",
      "cnt: 0 - valLoss: 0.4266839325428009 - trainLoss: 0.40161848068237305\n",
      "cnt: 0 - valLoss: 0.4266833961009979 - trainLoss: 0.4016171991825104\n",
      "cnt: 0 - valLoss: 0.4266833961009979 - trainLoss: 0.4016159474849701\n",
      "cnt: 0 - valLoss: 0.4266829192638397 - trainLoss: 0.4016146659851074\n",
      "cnt: 0 - valLoss: 0.42668288946151733 - trainLoss: 0.40161338448524475\n",
      "cnt: 0 - valLoss: 0.42668241262435913 - trainLoss: 0.40161213278770447\n",
      "cnt: 0 - valLoss: 0.4266819357872009 - trainLoss: 0.4016108512878418\n",
      "cnt: 0 - valLoss: 0.42668190598487854 - trainLoss: 0.4016096293926239\n",
      "cnt: 0 - valLoss: 0.42668142914772034 - trainLoss: 0.40160828828811646\n",
      "cnt: 0 - valLoss: 0.4266813397407532 - trainLoss: 0.40160703659057617\n",
      "cnt: 0 - valLoss: 0.42668092250823975 - trainLoss: 0.4016057550907135\n",
      "cnt: 0 - valLoss: 0.4266808331012726 - trainLoss: 0.4016045331954956\n",
      "cnt: 0 - valLoss: 0.42668041586875916 - trainLoss: 0.40160325169563293\n",
      "cnt: 0 - valLoss: 0.426680326461792 - trainLoss: 0.4016019403934479\n",
      "cnt: 0 - valLoss: 0.4266798794269562 - trainLoss: 0.4016006588935852\n",
      "cnt: 0 - valLoss: 0.426679790019989 - trainLoss: 0.4015994369983673\n",
      "cnt: 0 - valLoss: 0.4266793429851532 - trainLoss: 0.40159815549850464\n",
      "cnt: 0 - valLoss: 0.42667925357818604 - trainLoss: 0.40159687399864197\n",
      "cnt: 0 - valLoss: 0.42667877674102783 - trainLoss: 0.4015955626964569\n",
      "cnt: 0 - valLoss: 0.42667874693870544 - trainLoss: 0.401594340801239\n",
      "cnt: 0 - valLoss: 0.42667827010154724 - trainLoss: 0.40159305930137634\n",
      "cnt: 0 - valLoss: 0.4266781806945801 - trainLoss: 0.40159180760383606\n",
      "cnt: 0 - valLoss: 0.4266777038574219 - trainLoss: 0.4015905261039734\n",
      "cnt: 0 - valLoss: 0.42667722702026367 - trainLoss: 0.4015892446041107\n",
      "cnt: 0 - valLoss: 0.4266771376132965 - trainLoss: 0.40158799290657043\n",
      "cnt: 0 - valLoss: 0.4266767203807831 - trainLoss: 0.40158671140670776\n",
      "cnt: 0 - valLoss: 0.4266766309738159 - trainLoss: 0.40158548951148987\n",
      "cnt: 0 - valLoss: 0.4266761541366577 - trainLoss: 0.4015841782093048\n",
      "cnt: 0 - valLoss: 0.42667606472969055 - trainLoss: 0.40158289670944214\n",
      "cnt: 0 - valLoss: 0.42667561769485474 - trainLoss: 0.40158161520957947\n",
      "cnt: 0 - valLoss: 0.4266755282878876 - trainLoss: 0.4015803933143616\n",
      "cnt: 0 - valLoss: 0.42667508125305176 - trainLoss: 0.4015791118144989\n",
      "cnt: 0 - valLoss: 0.4266749322414398 - trainLoss: 0.4015778601169586\n",
      "cnt: 0 - valLoss: 0.426674485206604 - trainLoss: 0.40157657861709595\n",
      "cnt: 0 - valLoss: 0.42667439579963684 - trainLoss: 0.4015752971172333\n",
      "cnt: 0 - valLoss: 0.4266739785671234 - trainLoss: 0.4015740156173706\n",
      "cnt: 0 - valLoss: 0.42667385935783386 - trainLoss: 0.40157273411750793\n",
      "cnt: 0 - valLoss: 0.42667338252067566 - trainLoss: 0.40157148241996765\n",
      "cnt: 0 - valLoss: 0.4266732931137085 - trainLoss: 0.40157023072242737\n",
      "cnt: 0 - valLoss: 0.4266727864742279 - trainLoss: 0.4015689492225647\n",
      "cnt: 0 - valLoss: 0.42667269706726074 - trainLoss: 0.401567667722702\n",
      "cnt: 0 - valLoss: 0.4266722500324249 - trainLoss: 0.40156641602516174\n",
      "cnt: 0 - valLoss: 0.42667216062545776 - trainLoss: 0.4015651047229767\n",
      "cnt: 0 - valLoss: 0.4266716539859772 - trainLoss: 0.4015638530254364\n",
      "cnt: 0 - valLoss: 0.4266715347766876 - trainLoss: 0.4015626311302185\n",
      "cnt: 0 - valLoss: 0.4266710579395294 - trainLoss: 0.40156134963035583\n",
      "cnt: 0 - valLoss: 0.42667096853256226 - trainLoss: 0.4015600383281708\n",
      "cnt: 0 - valLoss: 0.42667046189308167 - trainLoss: 0.4015587568283081\n",
      "cnt: 0 - valLoss: 0.4266703426837921 - trainLoss: 0.4015575349330902\n",
      "cnt: 0 - valLoss: 0.42667028307914734 - trainLoss: 0.4015562832355499\n",
      "cnt: 0 - valLoss: 0.42666977643966675 - trainLoss: 0.40155500173568726\n",
      "cnt: 0 - valLoss: 0.42666956782341003 - trainLoss: 0.401553750038147\n",
      "cnt: 0 - valLoss: 0.4266689121723175 - trainLoss: 0.4015524685382843\n",
      "cnt: 0 - valLoss: 0.4266686737537384 - trainLoss: 0.40155118703842163\n",
      "cnt: 0 - valLoss: 0.4266684353351593 - trainLoss: 0.40154996514320374\n",
      "cnt: 0 - valLoss: 0.4266682267189026 - trainLoss: 0.40154871344566345\n",
      "cnt: 0 - valLoss: 0.4266679883003235 - trainLoss: 0.40154746174812317\n",
      "cnt: 0 - valLoss: 0.42666780948638916 - trainLoss: 0.4015461802482605\n",
      "cnt: 0 - valLoss: 0.4266672134399414 - trainLoss: 0.4015449285507202\n",
      "cnt: 0 - valLoss: 0.4266669452190399 - trainLoss: 0.4015437066555023\n",
      "cnt: 0 - valLoss: 0.4266666769981384 - trainLoss: 0.40154242515563965\n",
      "cnt: 0 - valLoss: 0.4266664683818817 - trainLoss: 0.40154117345809937\n",
      "cnt: 0 - valLoss: 0.426666259765625 - trainLoss: 0.4015399217605591\n",
      "cnt: 0 - valLoss: 0.4266659915447235 - trainLoss: 0.4015386402606964\n",
      "cnt: 0 - valLoss: 0.42666536569595337 - trainLoss: 0.4015374183654785\n",
      "cnt: 0 - valLoss: 0.42666515707969666 - trainLoss: 0.40153616666793823\n",
      "cnt: 0 - valLoss: 0.42666488885879517 - trainLoss: 0.40153488516807556\n",
      "cnt: 0 - valLoss: 0.4266646206378937 - trainLoss: 0.4015336334705353\n",
      "cnt: 0 - valLoss: 0.42666441202163696 - trainLoss: 0.4015324115753174\n",
      "cnt: 0 - valLoss: 0.42666420340538025 - trainLoss: 0.4015311300754547\n",
      "cnt: 0 - valLoss: 0.4266635477542877 - trainLoss: 0.40152987837791443\n",
      "cnt: 0 - valLoss: 0.426663339138031 - trainLoss: 0.40152862668037415\n",
      "cnt: 0 - valLoss: 0.4266631007194519 - trainLoss: 0.4015273451805115\n",
      "cnt: 0 - valLoss: 0.4266628324985504 - trainLoss: 0.4015261232852936\n",
      "cnt: 0 - valLoss: 0.4266626238822937 - trainLoss: 0.4015248715877533\n",
      "cnt: 0 - valLoss: 0.4266623258590698 - trainLoss: 0.4015235900878906\n",
      "cnt: 0 - valLoss: 0.4266620874404907 - trainLoss: 0.40152233839035034\n",
      "cnt: 0 - valLoss: 0.4266614615917206 - trainLoss: 0.40152111649513245\n",
      "cnt: 0 - valLoss: 0.42666125297546387 - trainLoss: 0.40151986479759216\n",
      "cnt: 0 - valLoss: 0.4266609847545624 - trainLoss: 0.4015186131000519\n",
      "cnt: 0 - valLoss: 0.4266607165336609 - trainLoss: 0.4015173316001892\n",
      "cnt: 0 - valLoss: 0.4266605079174042 - trainLoss: 0.4015160799026489\n",
      "cnt: 0 - valLoss: 0.4266602694988251 - trainLoss: 0.40151485800743103\n",
      "cnt: 0 - valLoss: 0.4266600012779236 - trainLoss: 0.40151360630989075\n",
      "cnt: 0 - valLoss: 0.42665940523147583 - trainLoss: 0.4015123248100281\n",
      "cnt: 0 - valLoss: 0.42665913701057434 - trainLoss: 0.4015110731124878\n",
      "cnt: 0 - valLoss: 0.42665889859199524 - trainLoss: 0.4015097916126251\n",
      "cnt: 0 - valLoss: 0.42665863037109375 - trainLoss: 0.4015085697174072\n",
      "cnt: 0 - valLoss: 0.42665836215019226 - trainLoss: 0.40150731801986694\n",
      "cnt: 0 - valLoss: 0.42665812373161316 - trainLoss: 0.40150606632232666\n",
      "cnt: 0 - valLoss: 0.42665791511535645 - trainLoss: 0.40150484442710876\n",
      "cnt: 0 - valLoss: 0.4266572594642639 - trainLoss: 0.4015035629272461\n",
      "cnt: 0 - valLoss: 0.4266569912433624 - trainLoss: 0.4015023112297058\n",
      "cnt: 0 - valLoss: 0.4266567528247833 - trainLoss: 0.4015010595321655\n",
      "cnt: 0 - valLoss: 0.42665648460388184 - trainLoss: 0.40149977803230286\n",
      "cnt: 0 - valLoss: 0.4266562759876251 - trainLoss: 0.40149858593940735\n",
      "cnt: 0 - valLoss: 0.42665600776672363 - trainLoss: 0.4014973044395447\n",
      "cnt: 0 - valLoss: 0.42665576934814453 - trainLoss: 0.4014960527420044\n",
      "cnt: 0 - valLoss: 0.42665550112724304 - trainLoss: 0.4014948308467865\n",
      "cnt: 0 - valLoss: 0.4266548454761505 - trainLoss: 0.40149354934692383\n",
      "cnt: 0 - valLoss: 0.4266546368598938 - trainLoss: 0.40149229764938354\n",
      "cnt: 0 - valLoss: 0.4266543388366699 - trainLoss: 0.40149104595184326\n",
      "cnt: 0 - valLoss: 0.4266541004180908 - trainLoss: 0.40148982405662537\n",
      "cnt: 0 - valLoss: 0.42665383219718933 - trainLoss: 0.4014885425567627\n",
      "cnt: 0 - valLoss: 0.42665356397628784 - trainLoss: 0.4014872908592224\n",
      "cnt: 0 - valLoss: 0.42665332555770874 - trainLoss: 0.40148600935935974\n",
      "cnt: 0 - valLoss: 0.42665305733680725 - trainLoss: 0.40148475766181946\n",
      "cnt: 0 - valLoss: 0.4266524612903595 - trainLoss: 0.40148353576660156\n",
      "cnt: 0 - valLoss: 0.426652193069458 - trainLoss: 0.4014822840690613\n",
      "cnt: 0 - valLoss: 0.4266519546508789 - trainLoss: 0.401481032371521\n",
      "cnt: 0 - valLoss: 0.4266516864299774 - trainLoss: 0.4014797508716583\n",
      "cnt: 0 - valLoss: 0.42665138840675354 - trainLoss: 0.40147852897644043\n",
      "cnt: 0 - valLoss: 0.42665112018585205 - trainLoss: 0.40147727727890015\n",
      "cnt: 0 - valLoss: 0.42665088176727295 - trainLoss: 0.40147602558135986\n",
      "cnt: 0 - valLoss: 0.42665061354637146 - trainLoss: 0.4014747440814972\n",
      "cnt: 0 - valLoss: 0.42664995789527893 - trainLoss: 0.4014734923839569\n",
      "cnt: 0 - valLoss: 0.42664971947669983 - trainLoss: 0.401472270488739\n",
      "cnt: 0 - valLoss: 0.42664945125579834 - trainLoss: 0.40147101879119873\n",
      "cnt: 0 - valLoss: 0.42664921283721924 - trainLoss: 0.40146979689598083\n",
      "cnt: 0 - valLoss: 0.42664891481399536 - trainLoss: 0.40146854519844055\n",
      "cnt: 0 - valLoss: 0.4266485869884491 - trainLoss: 0.4014672636985779\n",
      "cnt: 0 - valLoss: 0.42664834856987 - trainLoss: 0.4014660120010376\n",
      "cnt: 0 - valLoss: 0.4266480803489685 - trainLoss: 0.4014647901058197\n",
      "cnt: 0 - valLoss: 0.4266478419303894 - trainLoss: 0.4014635384082794\n",
      "cnt: 0 - valLoss: 0.4266471564769745 - trainLoss: 0.40146225690841675\n",
      "cnt: 0 - valLoss: 0.426646888256073 - trainLoss: 0.40146100521087646\n",
      "cnt: 0 - valLoss: 0.4266466200351715 - trainLoss: 0.4014597535133362\n",
      "cnt: 0 - valLoss: 0.4266463816165924 - trainLoss: 0.4014585316181183\n",
      "cnt: 0 - valLoss: 0.42664608359336853 - trainLoss: 0.4014572501182556\n",
      "cnt: 0 - valLoss: 0.42664581537246704 - trainLoss: 0.40145599842071533\n",
      "cnt: 0 - valLoss: 0.42664551734924316 - trainLoss: 0.40145474672317505\n",
      "cnt: 0 - valLoss: 0.4266452491283417 - trainLoss: 0.40145352482795715\n",
      "cnt: 0 - valLoss: 0.4266450107097626 - trainLoss: 0.4014522433280945\n",
      "cnt: 0 - valLoss: 0.4266447126865387 - trainLoss: 0.4014509916305542\n",
      "cnt: 0 - valLoss: 0.4266444146633148 - trainLoss: 0.4014497399330139\n",
      "cnt: 0 - valLoss: 0.42664414644241333 - trainLoss: 0.401448518037796\n",
      "cnt: 0 - valLoss: 0.4266434907913208 - trainLoss: 0.40144726634025574\n",
      "cnt: 0 - valLoss: 0.4266431927680969 - trainLoss: 0.40144601464271545\n",
      "cnt: 0 - valLoss: 0.4266429543495178 - trainLoss: 0.40144479274749756\n",
      "cnt: 0 - valLoss: 0.42664265632629395 - trainLoss: 0.4014435112476349\n",
      "cnt: 0 - valLoss: 0.4266423285007477 - trainLoss: 0.4014422595500946\n",
      "cnt: 0 - valLoss: 0.4266420900821686 - trainLoss: 0.40144097805023193\n",
      "cnt: 0 - valLoss: 0.4266418218612671 - trainLoss: 0.4014397859573364\n",
      "cnt: 0 - valLoss: 0.4266414940357208 - trainLoss: 0.40143850445747375\n",
      "cnt: 0 - valLoss: 0.42664122581481934 - trainLoss: 0.40143725275993347\n",
      "cnt: 0 - valLoss: 0.42664092779159546 - trainLoss: 0.4014360010623932\n",
      "cnt: 0 - valLoss: 0.42664065957069397 - trainLoss: 0.4014347791671753\n",
      "cnt: 0 - valLoss: 0.42664042115211487 - trainLoss: 0.4014334976673126\n",
      "cnt: 0 - valLoss: 0.426640123128891 - trainLoss: 0.40143224596977234\n",
      "cnt: 0 - valLoss: 0.4266398549079895 - trainLoss: 0.40143099427223206\n",
      "cnt: 0 - valLoss: 0.4266391694545746 - trainLoss: 0.40142977237701416\n",
      "cnt: 0 - valLoss: 0.4266388416290283 - trainLoss: 0.4014285206794739\n",
      "cnt: 0 - valLoss: 0.42663857340812683 - trainLoss: 0.4014272391796112\n",
      "cnt: 0 - valLoss: 0.42663830518722534 - trainLoss: 0.4014259874820709\n",
      "cnt: 0 - valLoss: 0.42663806676864624 - trainLoss: 0.401424765586853\n",
      "cnt: 0 - valLoss: 0.4266377091407776 - trainLoss: 0.40142351388931274\n",
      "cnt: 0 - valLoss: 0.4266374707221985 - trainLoss: 0.40142226219177246\n",
      "cnt: 0 - valLoss: 0.4266371428966522 - trainLoss: 0.40142104029655457\n",
      "cnt: 0 - valLoss: 0.42663684487342834 - trainLoss: 0.4014197587966919\n",
      "cnt: 0 - valLoss: 0.42663660645484924 - trainLoss: 0.4014185070991516\n",
      "cnt: 0 - valLoss: 0.42663630843162537 - trainLoss: 0.40141725540161133\n",
      "cnt: 0 - valLoss: 0.4266360104084015 - trainLoss: 0.40141597390174866\n",
      "cnt: 0 - valLoss: 0.4266357421875 - trainLoss: 0.40141478180885315\n",
      "cnt: 0 - valLoss: 0.4266354441642761 - trainLoss: 0.4014135003089905\n",
      "cnt: 0 - valLoss: 0.42663514614105225 - trainLoss: 0.4014122486114502\n",
      "cnt: 0 - valLoss: 0.42663484811782837 - trainLoss: 0.4014110267162323\n",
      "cnt: 0 - valLoss: 0.4266345500946045 - trainLoss: 0.40140974521636963\n",
      "cnt: 0 - valLoss: 0.4266338646411896 - trainLoss: 0.40140852332115173\n",
      "cnt: 0 - valLoss: 0.4266335964202881 - trainLoss: 0.40140724182128906\n",
      "cnt: 0 - valLoss: 0.4266333281993866 - trainLoss: 0.40140604972839355\n",
      "cnt: 0 - valLoss: 0.42663300037384033 - trainLoss: 0.4014047682285309\n",
      "cnt: 0 - valLoss: 0.42663270235061646 - trainLoss: 0.4014035165309906\n",
      "cnt: 0 - valLoss: 0.4266324043273926 - trainLoss: 0.4014022946357727\n",
      "cnt: 0 - valLoss: 0.4266321361064911 - trainLoss: 0.4014010429382324\n",
      "cnt: 0 - valLoss: 0.4266318380832672 - trainLoss: 0.40139979124069214\n",
      "cnt: 0 - valLoss: 0.4266315698623657 - trainLoss: 0.40139850974082947\n",
      "cnt: 0 - valLoss: 0.42663124203681946 - trainLoss: 0.40139731764793396\n",
      "cnt: 0 - valLoss: 0.4266309440135956 - trainLoss: 0.4013960361480713\n",
      "cnt: 0 - valLoss: 0.4266306757926941 - trainLoss: 0.4013947546482086\n",
      "cnt: 0 - valLoss: 0.4266303479671478 - trainLoss: 0.4013935625553131\n",
      "cnt: 0 - valLoss: 0.42663007974624634 - trainLoss: 0.40139228105545044\n",
      "cnt: 0 - valLoss: 0.42662978172302246 - trainLoss: 0.40139105916023254\n",
      "cnt: 0 - valLoss: 0.4266294836997986 - trainLoss: 0.4013897776603699\n",
      "cnt: 0 - valLoss: 0.4266291856765747 - trainLoss: 0.40138858556747437\n",
      "cnt: 0 - valLoss: 0.42662888765335083 - trainLoss: 0.4013873040676117\n",
      "cnt: 0 - valLoss: 0.42662858963012695 - trainLoss: 0.401386022567749\n",
      "cnt: 0 - valLoss: 0.4266282916069031 - trainLoss: 0.4013848304748535\n",
      "cnt: 0 - valLoss: 0.4266279339790344 - trainLoss: 0.40138354897499084\n",
      "cnt: 0 - valLoss: 0.4266276955604553 - trainLoss: 0.40138229727745056\n",
      "cnt: 0 - valLoss: 0.426627516746521 - trainLoss: 0.4013810455799103\n",
      "cnt: 0 - valLoss: 0.42662692070007324 - trainLoss: 0.40137985348701477\n",
      "cnt: 0 - valLoss: 0.4266267418861389 - trainLoss: 0.40137866139411926\n",
      "cnt: 0 - valLoss: 0.4266265630722046 - trainLoss: 0.401377409696579\n",
      "cnt: 0 - valLoss: 0.42662638425827026 - trainLoss: 0.4013761878013611\n",
      "cnt: 0 - valLoss: 0.4266262352466583 - trainLoss: 0.4013749659061432\n",
      "cnt: 0 - valLoss: 0.4266260266304016 - trainLoss: 0.4013736844062805\n",
      "cnt: 0 - valLoss: 0.42662546038627625 - trainLoss: 0.401372492313385\n",
      "cnt: 0 - valLoss: 0.42662525177001953 - trainLoss: 0.4013713002204895\n",
      "cnt: 0 - valLoss: 0.4266250729560852 - trainLoss: 0.4013700485229492\n",
      "cnt: 0 - valLoss: 0.4266248643398285 - trainLoss: 0.4013688266277313\n",
      "cnt: 0 - valLoss: 0.42662468552589417 - trainLoss: 0.4013676047325134\n",
      "cnt: 0 - valLoss: 0.42662450671195984 - trainLoss: 0.40136638283729553\n",
      "cnt: 0 - valLoss: 0.4266239106655121 - trainLoss: 0.40136513113975525\n",
      "cnt: 0 - valLoss: 0.42662370204925537 - trainLoss: 0.40136393904685974\n",
      "cnt: 0 - valLoss: 0.42662349343299866 - trainLoss: 0.40136274695396423\n",
      "cnt: 0 - valLoss: 0.42662331461906433 - trainLoss: 0.40136146545410156\n",
      "cnt: 0 - valLoss: 0.42662304639816284 - trainLoss: 0.40136024355888367\n",
      "cnt: 0 - valLoss: 0.4266227185726166 - trainLoss: 0.40135905146598816\n",
      "cnt: 0 - valLoss: 0.4266224503517151 - trainLoss: 0.40135785937309265\n",
      "cnt: 0 - valLoss: 0.4266221523284912 - trainLoss: 0.40135660767555237\n",
      "cnt: 0 - valLoss: 0.42662185430526733 - trainLoss: 0.4013553857803345\n",
      "cnt: 0 - valLoss: 0.42662155628204346 - trainLoss: 0.4013541638851166\n",
      "cnt: 0 - valLoss: 0.4266212582588196 - trainLoss: 0.40135297179222107\n",
      "cnt: 0 - valLoss: 0.4266209602355957 - trainLoss: 0.40135177969932556\n",
      "cnt: 0 - valLoss: 0.4266206622123718 - trainLoss: 0.40135058760643005\n",
      "cnt: 0 - valLoss: 0.42662036418914795 - trainLoss: 0.4013493061065674\n",
      "cnt: 0 - valLoss: 0.4266200661659241 - trainLoss: 0.4013480842113495\n",
      "cnt: 0 - valLoss: 0.4266197085380554 - trainLoss: 0.401346892118454\n",
      "cnt: 0 - valLoss: 0.42661944031715393 - trainLoss: 0.40134570002555847\n",
      "cnt: 0 - valLoss: 0.42661911249160767 - trainLoss: 0.4013444483280182\n",
      "cnt: 0 - valLoss: 0.4266188442707062 - trainLoss: 0.4013432562351227\n",
      "cnt: 0 - valLoss: 0.4266185164451599 - trainLoss: 0.4013420343399048\n",
      "cnt: 0 - valLoss: 0.42661815881729126 - trainLoss: 0.4013408124446869\n",
      "cnt: 0 - valLoss: 0.42661792039871216 - trainLoss: 0.4013396203517914\n",
      "cnt: 0 - valLoss: 0.4266176223754883 - trainLoss: 0.4013383686542511\n",
      "cnt: 0 - valLoss: 0.42661726474761963 - trainLoss: 0.4013371765613556\n",
      "cnt: 0 - valLoss: 0.42661696672439575 - trainLoss: 0.4013359844684601\n",
      "cnt: 0 - valLoss: 0.4266166388988495 - trainLoss: 0.4013347327709198\n",
      "cnt: 0 - valLoss: 0.426616370677948 - trainLoss: 0.4013335406780243\n",
      "cnt: 0 - valLoss: 0.42661601305007935 - trainLoss: 0.4013323485851288\n",
      "cnt: 0 - valLoss: 0.42661571502685547 - trainLoss: 0.4013311564922333\n",
      "cnt: 0 - valLoss: 0.4266154170036316 - trainLoss: 0.401329904794693\n",
      "cnt: 0 - valLoss: 0.4266150891780853 - trainLoss: 0.4013287425041199\n",
      "cnt: 0 - valLoss: 0.42661482095718384 - trainLoss: 0.401327520608902\n",
      "cnt: 0 - valLoss: 0.4266144931316376 - trainLoss: 0.4013262689113617\n",
      "cnt: 0 - valLoss: 0.4266141355037689 - trainLoss: 0.4013250768184662\n",
      "cnt: 0 - valLoss: 0.42661383748054504 - trainLoss: 0.4013238847255707\n",
      "cnt: 0 - valLoss: 0.42661353945732117 - trainLoss: 0.4013226330280304\n",
      "cnt: 0 - valLoss: 0.4266132116317749 - trainLoss: 0.4013214409351349\n",
      "cnt: 0 - valLoss: 0.4266129434108734 - trainLoss: 0.4013202488422394\n",
      "cnt: 0 - valLoss: 0.42661258578300476 - trainLoss: 0.4013190269470215\n",
      "cnt: 0 - valLoss: 0.4266122579574585 - trainLoss: 0.4013178050518036\n",
      "cnt: 0 - valLoss: 0.426611989736557 - trainLoss: 0.4013165831565857\n",
      "cnt: 0 - valLoss: 0.42661166191101074 - trainLoss: 0.4013153612613678\n",
      "cnt: 0 - valLoss: 0.4266113042831421 - trainLoss: 0.4013141989707947\n",
      "cnt: 0 - valLoss: 0.426611065864563 - trainLoss: 0.4013129472732544\n",
      "cnt: 0 - valLoss: 0.42661070823669434 - trainLoss: 0.4013117253780365\n",
      "cnt: 0 - valLoss: 0.42661038041114807 - trainLoss: 0.4013105034828186\n",
      "cnt: 0 - valLoss: 0.4266101121902466 - trainLoss: 0.4013093113899231\n",
      "cnt: 0 - valLoss: 0.42660975456237793 - trainLoss: 0.4013081192970276\n",
      "cnt: 0 - valLoss: 0.42660945653915405 - trainLoss: 0.4013068675994873\n",
      "cnt: 0 - valLoss: 0.4266091585159302 - trainLoss: 0.4013057351112366\n",
      "cnt: 0 - valLoss: 0.4266088306903839 - trainLoss: 0.4013044536113739\n",
      "cnt: 0 - valLoss: 0.4266083836555481 - trainLoss: 0.4013032913208008\n",
      "cnt: 0 - valLoss: 0.42660805583000183 - trainLoss: 0.4013020396232605\n",
      "cnt: 0 - valLoss: 0.4266076683998108 - trainLoss: 0.401300847530365\n",
      "cnt: 0 - valLoss: 0.42660728096961975 - trainLoss: 0.4012996554374695\n",
      "cnt: 0 - valLoss: 0.4266068637371063 - trainLoss: 0.4012984335422516\n",
      "cnt: 0 - valLoss: 0.42660650610923767 - trainLoss: 0.4012972414493561\n",
      "cnt: 0 - valLoss: 0.42660608887672424 - trainLoss: 0.4012960195541382\n",
      "cnt: 0 - valLoss: 0.4266057312488556 - trainLoss: 0.4012947976589203\n",
      "cnt: 0 - valLoss: 0.42660531401634216 - trainLoss: 0.4012936055660248\n",
      "cnt: 0 - valLoss: 0.4266049265861511 - trainLoss: 0.4012924134731293\n",
      "cnt: 0 - valLoss: 0.42660456895828247 - trainLoss: 0.401291161775589\n",
      "cnt: 0 - valLoss: 0.42660418152809143 - trainLoss: 0.4012899696826935\n",
      "cnt: 0 - valLoss: 0.4266038239002228 - trainLoss: 0.401288777589798\n",
      "cnt: 0 - valLoss: 0.42660343647003174 - trainLoss: 0.40128758549690247\n",
      "cnt: 0 - valLoss: 0.4266029894351959 - trainLoss: 0.4012863337993622\n",
      "cnt: 0 - valLoss: 0.42660266160964966 - trainLoss: 0.40128517150878906\n",
      "cnt: 0 - valLoss: 0.42660221457481384 - trainLoss: 0.40128397941589355\n",
      "cnt: 0 - valLoss: 0.4266018867492676 - trainLoss: 0.40128272771835327\n",
      "cnt: 0 - valLoss: 0.42660149931907654 - trainLoss: 0.4012815058231354\n",
      "cnt: 0 - valLoss: 0.4266011118888855 - trainLoss: 0.40128031373023987\n",
      "cnt: 0 - valLoss: 0.42660069465637207 - trainLoss: 0.401279091835022\n",
      "cnt: 0 - valLoss: 0.42660030722618103 - trainLoss: 0.40127789974212646\n",
      "cnt: 0 - valLoss: 0.4265999495983124 - trainLoss: 0.40127670764923096\n",
      "cnt: 0 - valLoss: 0.42659953236579895 - trainLoss: 0.40127551555633545\n",
      "cnt: 0 - valLoss: 0.4265991449356079 - trainLoss: 0.40127429366111755\n",
      "cnt: 0 - valLoss: 0.42659875750541687 - trainLoss: 0.40127307176589966\n",
      "cnt: 0 - valLoss: 0.42659837007522583 - trainLoss: 0.40127187967300415\n",
      "cnt: 0 - valLoss: 0.4265979826450348 - trainLoss: 0.40127065777778625\n",
      "cnt: 0 - valLoss: 0.42659759521484375 - trainLoss: 0.40126943588256836\n",
      "cnt: 0 - valLoss: 0.4265972375869751 - trainLoss: 0.40126824378967285\n",
      "cnt: 0 - valLoss: 0.42659682035446167 - trainLoss: 0.40126702189445496\n",
      "cnt: 0 - valLoss: 0.42659643292427063 - trainLoss: 0.40126582980155945\n",
      "cnt: 0 - valLoss: 0.4265960454940796 - trainLoss: 0.40126463770866394\n",
      "cnt: 0 - valLoss: 0.4265957176685333 - trainLoss: 0.40126344561576843\n",
      "cnt: 0 - valLoss: 0.4265952706336975 - trainLoss: 0.40126222372055054\n",
      "cnt: 0 - valLoss: 0.42659488320350647 - trainLoss: 0.40126103162765503\n",
      "cnt: 0 - valLoss: 0.42659449577331543 - trainLoss: 0.40125980973243713\n",
      "cnt: 0 - valLoss: 0.42659416794776917 - trainLoss: 0.401258647441864\n",
      "cnt: 0 - valLoss: 0.42659375071525574 - trainLoss: 0.40125739574432373\n",
      "cnt: 0 - valLoss: 0.4265933930873871 - trainLoss: 0.40125617384910583\n",
      "cnt: 0 - valLoss: 0.42659300565719604 - trainLoss: 0.40125495195388794\n",
      "cnt: 0 - valLoss: 0.4265925884246826 - trainLoss: 0.40125375986099243\n",
      "cnt: 0 - valLoss: 0.4265922009944916 - trainLoss: 0.40125250816345215\n",
      "cnt: 0 - valLoss: 0.42659181356430054 - trainLoss: 0.4012513756752014\n",
      "cnt: 0 - valLoss: 0.4265914261341095 - trainLoss: 0.4012501537799835\n",
      "cnt: 0 - valLoss: 0.42659103870391846 - trainLoss: 0.4012489318847656\n",
      "cnt: 0 - valLoss: 0.4265906512737274 - trainLoss: 0.4012477695941925\n",
      "cnt: 0 - valLoss: 0.426590234041214 - trainLoss: 0.4012465178966522\n",
      "cnt: 0 - valLoss: 0.42658987641334534 - trainLoss: 0.4012453258037567\n",
      "cnt: 0 - valLoss: 0.4265894889831543 - trainLoss: 0.4012441337108612\n",
      "cnt: 0 - valLoss: 0.42658910155296326 - trainLoss: 0.4012428820133209\n",
      "cnt: 0 - valLoss: 0.426588773727417 - trainLoss: 0.4012416899204254\n",
      "cnt: 0 - valLoss: 0.4265880882740021 - trainLoss: 0.4012404978275299\n",
      "cnt: 0 - valLoss: 0.42658740282058716 - trainLoss: 0.4012393057346344\n",
      "cnt: 0 - valLoss: 0.4265870451927185 - trainLoss: 0.4012380838394165\n",
      "cnt: 0 - valLoss: 0.4265863597393036 - trainLoss: 0.401236891746521\n",
      "cnt: 0 - valLoss: 0.4265860319137573 - trainLoss: 0.4012356996536255\n",
      "cnt: 0 - valLoss: 0.42658543586730957 - trainLoss: 0.4012344479560852\n",
      "cnt: 0 - valLoss: 0.42658475041389465 - trainLoss: 0.4012333154678345\n",
      "cnt: 0 - valLoss: 0.4265843629837036 - trainLoss: 0.4012320637702942\n",
      "cnt: 0 - valLoss: 0.4265837073326111 - trainLoss: 0.4012308716773987\n",
      "cnt: 0 - valLoss: 0.42658331990242004 - trainLoss: 0.4012296795845032\n",
      "cnt: 0 - valLoss: 0.4265826940536499 - trainLoss: 0.4012284576892853\n",
      "cnt: 0 - valLoss: 0.4265820384025574 - trainLoss: 0.4012272357940674\n",
      "cnt: 0 - valLoss: 0.4265816807746887 - trainLoss: 0.40122607350349426\n",
      "cnt: 0 - valLoss: 0.4265810549259186 - trainLoss: 0.401224821805954\n",
      "cnt: 0 - valLoss: 0.4265806972980499 - trainLoss: 0.40122362971305847\n",
      "cnt: 0 - valLoss: 0.426580011844635 - trainLoss: 0.40122243762016296\n",
      "cnt: 0 - valLoss: 0.42657941579818726 - trainLoss: 0.40122124552726746\n",
      "cnt: 0 - valLoss: 0.426579087972641 - trainLoss: 0.40122002363204956\n",
      "cnt: 0 - valLoss: 0.4265784025192261 - trainLoss: 0.40121883153915405\n",
      "cnt: 0 - valLoss: 0.4265778064727783 - trainLoss: 0.40121763944625854\n",
      "cnt: 0 - valLoss: 0.42657744884490967 - trainLoss: 0.40121644735336304\n",
      "cnt: 0 - valLoss: 0.42657679319381714 - trainLoss: 0.40121525526046753\n",
      "cnt: 0 - valLoss: 0.4265764355659485 - trainLoss: 0.40121400356292725\n",
      "cnt: 0 - valLoss: 0.4265758693218231 - trainLoss: 0.40121281147003174\n",
      "cnt: 0 - valLoss: 0.4265752136707306 - trainLoss: 0.40121155977249146\n",
      "cnt: 0 - valLoss: 0.42657479643821716 - trainLoss: 0.40121036767959595\n",
      "cnt: 0 - valLoss: 0.4265742003917694 - trainLoss: 0.4012092053890228\n",
      "cnt: 0 - valLoss: 0.42657360434532166 - trainLoss: 0.4012080132961273\n",
      "cnt: 0 - valLoss: 0.42657333612442017 - trainLoss: 0.40120676159858704\n",
      "cnt: 0 - valLoss: 0.42657291889190674 - trainLoss: 0.4012055993080139\n",
      "cnt: 0 - valLoss: 0.42657238245010376 - trainLoss: 0.4012044072151184\n",
      "cnt: 0 - valLoss: 0.42657187581062317 - trainLoss: 0.4012032151222229\n",
      "cnt: 0 - valLoss: 0.42657139897346497 - trainLoss: 0.4012019634246826\n",
      "cnt: 0 - valLoss: 0.42657092213630676 - trainLoss: 0.4012008309364319\n",
      "cnt: 0 - valLoss: 0.42657041549682617 - trainLoss: 0.4011995792388916\n",
      "cnt: 0 - valLoss: 0.42656999826431274 - trainLoss: 0.4011983871459961\n",
      "cnt: 0 - valLoss: 0.42656949162483215 - trainLoss: 0.4011971652507782\n",
      "cnt: 0 - valLoss: 0.4265689551830292 - trainLoss: 0.4011959731578827\n",
      "cnt: 0 - valLoss: 0.42656847834587097 - trainLoss: 0.40119481086730957\n",
      "cnt: 0 - valLoss: 0.42656803131103516 - trainLoss: 0.40119361877441406\n",
      "cnt: 0 - valLoss: 0.42656755447387695 - trainLoss: 0.40119239687919617\n",
      "cnt: 0 - valLoss: 0.42656704783439636 - trainLoss: 0.40119117498397827\n",
      "cnt: 0 - valLoss: 0.42656657099723816 - trainLoss: 0.40119004249572754\n",
      "cnt: 0 - valLoss: 0.42656609416007996 - trainLoss: 0.40118879079818726\n",
      "cnt: 0 - valLoss: 0.42656561732292175 - trainLoss: 0.40118762850761414\n",
      "cnt: 0 - valLoss: 0.42656511068344116 - trainLoss: 0.40118643641471863\n",
      "cnt: 0 - valLoss: 0.42656466364860535 - trainLoss: 0.40118518471717834\n",
      "cnt: 0 - valLoss: 0.42656415700912476 - trainLoss: 0.40118399262428284\n",
      "cnt: 0 - valLoss: 0.42656368017196655 - trainLoss: 0.4011828303337097\n",
      "cnt: 0 - valLoss: 0.42656323313713074 - trainLoss: 0.4011816084384918\n",
      "cnt: 0 - valLoss: 0.42656269669532776 - trainLoss: 0.4011803865432739\n",
      "cnt: 0 - valLoss: 0.42656227946281433 - trainLoss: 0.4011791944503784\n",
      "cnt: 0 - valLoss: 0.42656177282333374 - trainLoss: 0.4011780023574829\n",
      "cnt: 0 - valLoss: 0.42656129598617554 - trainLoss: 0.4011768400669098\n",
      "cnt: 0 - valLoss: 0.42656081914901733 - trainLoss: 0.4011756181716919\n",
      "cnt: 0 - valLoss: 0.42656034231185913 - trainLoss: 0.401174396276474\n",
      "cnt: 0 - valLoss: 0.4265598654747009 - trainLoss: 0.4011732041835785\n",
      "cnt: 0 - valLoss: 0.42655935883522034 - trainLoss: 0.40117204189300537\n",
      "cnt: 0 - valLoss: 0.4265589416027069 - trainLoss: 0.40117084980010986\n",
      "cnt: 0 - valLoss: 0.42655840516090393 - trainLoss: 0.40116962790489197\n",
      "cnt: 0 - valLoss: 0.4265579879283905 - trainLoss: 0.40116846561431885\n",
      "cnt: 0 - valLoss: 0.4265575110912323 - trainLoss: 0.40116727352142334\n",
      "cnt: 0 - valLoss: 0.4265570342540741 - trainLoss: 0.40116605162620544\n",
      "cnt: 0 - valLoss: 0.4265565872192383 - trainLoss: 0.40116485953330994\n",
      "cnt: 0 - valLoss: 0.4265561103820801 - trainLoss: 0.40116363763809204\n",
      "cnt: 0 - valLoss: 0.4265556335449219 - trainLoss: 0.4011624753475189\n",
      "cnt: 0 - valLoss: 0.42655515670776367 - trainLoss: 0.40116122364997864\n",
      "cnt: 0 - valLoss: 0.42655473947525024 - trainLoss: 0.4011600613594055\n",
      "cnt: 0 - valLoss: 0.42655423283576965 - trainLoss: 0.40115886926651\n",
      "cnt: 0 - valLoss: 0.42655378580093384 - trainLoss: 0.4011576771736145\n",
      "cnt: 0 - valLoss: 0.42655330896377563 - trainLoss: 0.4011564254760742\n",
      "cnt: 0 - valLoss: 0.4265528619289398 - trainLoss: 0.4011552631855011\n",
      "cnt: 0 - valLoss: 0.42655232548713684 - trainLoss: 0.4011540710926056\n",
      "cnt: 0 - valLoss: 0.4265519082546234 - trainLoss: 0.4011528789997101\n",
      "cnt: 0 - valLoss: 0.4265514612197876 - trainLoss: 0.4011516869068146\n",
      "cnt: 0 - valLoss: 0.426550954580307 - trainLoss: 0.40115049481391907\n",
      "cnt: 0 - valLoss: 0.4265504777431488 - trainLoss: 0.40114927291870117\n",
      "cnt: 0 - valLoss: 0.426550030708313 - trainLoss: 0.40114808082580566\n",
      "cnt: 0 - valLoss: 0.4265495836734772 - trainLoss: 0.40114691853523254\n",
      "cnt: 0 - valLoss: 0.42654910683631897 - trainLoss: 0.40114569664001465\n",
      "cnt: 0 - valLoss: 0.42654865980148315 - trainLoss: 0.40114453434944153\n",
      "cnt: 0 - valLoss: 0.42654818296432495 - trainLoss: 0.40114328265190125\n",
      "cnt: 0 - valLoss: 0.42654770612716675 - trainLoss: 0.4011421501636505\n",
      "cnt: 0 - valLoss: 0.4265472888946533 - trainLoss: 0.40114089846611023\n",
      "cnt: 0 - valLoss: 0.4265468120574951 - trainLoss: 0.4011397063732147\n",
      "cnt: 0 - valLoss: 0.4265463352203369 - trainLoss: 0.4011385440826416\n",
      "cnt: 0 - valLoss: 0.4265459179878235 - trainLoss: 0.4011372923851013\n",
      "cnt: 0 - valLoss: 0.42654547095298767 - trainLoss: 0.4011361300945282\n",
      "cnt: 0 - valLoss: 0.42654499411582947 - trainLoss: 0.4011349081993103\n",
      "cnt: 0 - valLoss: 0.42654454708099365 - trainLoss: 0.4011337459087372\n",
      "cnt: 0 - valLoss: 0.42654410004615784 - trainLoss: 0.4011324942111969\n",
      "cnt: 0 - valLoss: 0.4265436828136444 - trainLoss: 0.40113136172294617\n",
      "cnt: 0 - valLoss: 0.42654314637184143 - trainLoss: 0.40113013982772827\n",
      "cnt: 0 - valLoss: 0.426542729139328 - trainLoss: 0.40112894773483276\n",
      "cnt: 0 - valLoss: 0.4265422523021698 - trainLoss: 0.40112775564193726\n",
      "cnt: 0 - valLoss: 0.42654183506965637 - trainLoss: 0.40112656354904175\n",
      "cnt: 0 - valLoss: 0.42654141783714294 - trainLoss: 0.40112537145614624\n",
      "cnt: 0 - valLoss: 0.42654094099998474 - trainLoss: 0.40112414956092834\n",
      "cnt: 0 - valLoss: 0.4265404939651489 - trainLoss: 0.40112295746803284\n",
      "cnt: 0 - valLoss: 0.4265400171279907 - trainLoss: 0.40112176537513733\n",
      "cnt: 0 - valLoss: 0.4265395700931549 - trainLoss: 0.4011205732822418\n",
      "cnt: 0 - valLoss: 0.4265391230583191 - trainLoss: 0.4011193513870239\n",
      "cnt: 0 - valLoss: 0.42653870582580566 - trainLoss: 0.4011181592941284\n",
      "cnt: 0 - valLoss: 0.42653822898864746 - trainLoss: 0.4011169672012329\n",
      "cnt: 0 - valLoss: 0.42653781175613403 - trainLoss: 0.4011157751083374\n",
      "cnt: 0 - valLoss: 0.42653733491897583 - trainLoss: 0.4011145830154419\n",
      "cnt: 0 - valLoss: 0.4265369176864624 - trainLoss: 0.401113361120224\n",
      "cnt: 0 - valLoss: 0.4265364408493042 - trainLoss: 0.4011121690273285\n",
      "cnt: 0 - valLoss: 0.4265359938144684 - trainLoss: 0.401110976934433\n",
      "cnt: 0 - valLoss: 0.4265355169773102 - trainLoss: 0.4011097848415375\n",
      "cnt: 0 - valLoss: 0.42653509974479675 - trainLoss: 0.40110862255096436\n",
      "cnt: 0 - valLoss: 0.4265346825122833 - trainLoss: 0.40110743045806885\n",
      "cnt: 0 - valLoss: 0.4265342056751251 - trainLoss: 0.40110623836517334\n",
      "cnt: 0 - valLoss: 0.4265337884426117 - trainLoss: 0.40110501646995544\n",
      "cnt: 0 - valLoss: 0.4265333116054535 - trainLoss: 0.4011038839817047\n",
      "cnt: 0 - valLoss: 0.4265328645706177 - trainLoss: 0.40110263228416443\n",
      "cnt: 0 - valLoss: 0.42653244733810425 - trainLoss: 0.4011014401912689\n",
      "cnt: 0 - valLoss: 0.4265320301055908 - trainLoss: 0.4011002779006958\n",
      "cnt: 0 - valLoss: 0.4265315532684326 - trainLoss: 0.4010990858078003\n",
      "cnt: 0 - valLoss: 0.4265311062335968 - trainLoss: 0.4010978937149048\n",
      "cnt: 0 - valLoss: 0.426530659198761 - trainLoss: 0.4010966718196869\n",
      "cnt: 0 - valLoss: 0.42653021216392517 - trainLoss: 0.401095449924469\n",
      "cnt: 0 - valLoss: 0.42652979493141174 - trainLoss: 0.4010942876338959\n",
      "cnt: 0 - valLoss: 0.4265293478965759 - trainLoss: 0.40109309554100037\n",
      "cnt: 0 - valLoss: 0.4265289306640625 - trainLoss: 0.40109187364578247\n",
      "cnt: 0 - valLoss: 0.4265284538269043 - trainLoss: 0.40109068155288696\n",
      "cnt: 0 - valLoss: 0.42652803659439087 - trainLoss: 0.40108948945999146\n",
      "cnt: 0 - valLoss: 0.42652758955955505 - trainLoss: 0.40108832716941833\n",
      "cnt: 0 - valLoss: 0.42652714252471924 - trainLoss: 0.4010871350765228\n",
      "cnt: 0 - valLoss: 0.4265267550945282 - trainLoss: 0.4010859429836273\n",
      "cnt: 0 - valLoss: 0.42652627825737 - trainLoss: 0.4010847508907318\n",
      "cnt: 0 - valLoss: 0.4265258312225342 - trainLoss: 0.4010835886001587\n",
      "cnt: 0 - valLoss: 0.42652541399002075 - trainLoss: 0.4010823965072632\n",
      "cnt: 0 - valLoss: 0.4265249967575073 - trainLoss: 0.4010811448097229\n",
      "cnt: 0 - valLoss: 0.4265245497226715 - trainLoss: 0.4010799825191498\n",
      "cnt: 0 - valLoss: 0.4265240728855133 - trainLoss: 0.4010787904262543\n",
      "cnt: 0 - valLoss: 0.4265236556529999 - trainLoss: 0.40107759833335876\n",
      "cnt: 0 - valLoss: 0.42652326822280884 - trainLoss: 0.40107640624046326\n",
      "cnt: 0 - valLoss: 0.4265228509902954 - trainLoss: 0.40107518434524536\n",
      "cnt: 0 - valLoss: 0.4265223741531372 - trainLoss: 0.40107399225234985\n",
      "cnt: 0 - valLoss: 0.4265219569206238 - trainLoss: 0.40107280015945435\n",
      "cnt: 0 - valLoss: 0.42652150988578796 - trainLoss: 0.40107160806655884\n",
      "cnt: 0 - valLoss: 0.42652103304862976 - trainLoss: 0.4010704457759857\n",
      "cnt: 0 - valLoss: 0.42652061581611633 - trainLoss: 0.40106919407844543\n",
      "cnt: 0 - valLoss: 0.4265201985836029 - trainLoss: 0.4010680019855499\n",
      "cnt: 0 - valLoss: 0.4265197515487671 - trainLoss: 0.4010668396949768\n",
      "cnt: 0 - valLoss: 0.42651933431625366 - trainLoss: 0.4010656476020813\n",
      "cnt: 0 - valLoss: 0.42651888728141785 - trainLoss: 0.4010644853115082\n",
      "cnt: 0 - valLoss: 0.4265184700489044 - trainLoss: 0.40106329321861267\n",
      "cnt: 0 - valLoss: 0.426518052816391 - trainLoss: 0.4010620415210724\n",
      "cnt: 0 - valLoss: 0.4265176057815552 - trainLoss: 0.40106090903282166\n",
      "cnt: 0 - valLoss: 0.42651718854904175 - trainLoss: 0.40105968713760376\n",
      "cnt: 0 - valLoss: 0.4265167713165283 - trainLoss: 0.40105849504470825\n",
      "cnt: 0 - valLoss: 0.4265162944793701 - trainLoss: 0.40105730295181274\n",
      "cnt: 0 - valLoss: 0.4265158474445343 - trainLoss: 0.40105611085891724\n",
      "cnt: 0 - valLoss: 0.42651546001434326 - trainLoss: 0.40105491876602173\n",
      "cnt: 0 - valLoss: 0.42651501297950745 - trainLoss: 0.40105369687080383\n",
      "cnt: 0 - valLoss: 0.42651456594467163 - trainLoss: 0.4010525047779083\n",
      "cnt: 0 - valLoss: 0.4265141487121582 - trainLoss: 0.4010513126850128\n",
      "cnt: 0 - valLoss: 0.42651376128196716 - trainLoss: 0.4010501503944397\n",
      "cnt: 0 - valLoss: 0.42651328444480896 - trainLoss: 0.4010489583015442\n",
      "cnt: 0 - valLoss: 0.4265128970146179 - trainLoss: 0.40104779601097107\n",
      "cnt: 0 - valLoss: 0.4265124797821045 - trainLoss: 0.40104660391807556\n",
      "cnt: 0 - valLoss: 0.4265120327472687 - trainLoss: 0.40104541182518005\n",
      "cnt: 0 - valLoss: 0.42651161551475525 - trainLoss: 0.40104421973228455\n",
      "cnt: 0 - valLoss: 0.4265111982822418 - trainLoss: 0.40104299783706665\n",
      "cnt: 0 - valLoss: 0.4265107214450836 - trainLoss: 0.40104180574417114\n",
      "cnt: 0 - valLoss: 0.4265102744102478 - trainLoss: 0.401040643453598\n",
      "cnt: 0 - valLoss: 0.42650988698005676 - trainLoss: 0.4010394215583801\n",
      "cnt: 0 - valLoss: 0.42650946974754333 - trainLoss: 0.401038259267807\n",
      "cnt: 0 - valLoss: 0.4265090525150299 - trainLoss: 0.4010370075702667\n",
      "cnt: 0 - valLoss: 0.4265086054801941 - trainLoss: 0.4010358154773712\n",
      "cnt: 0 - valLoss: 0.42650818824768066 - trainLoss: 0.4010346531867981\n",
      "cnt: 0 - valLoss: 0.42650777101516724 - trainLoss: 0.4010334610939026\n",
      "cnt: 0 - valLoss: 0.4265073239803314 - trainLoss: 0.4010322690010071\n",
      "cnt: 0 - valLoss: 0.426506906747818 - trainLoss: 0.4010310769081116\n",
      "cnt: 0 - valLoss: 0.4265064597129822 - trainLoss: 0.40102991461753845\n",
      "cnt: 0 - valLoss: 0.42650607228279114 - trainLoss: 0.40102872252464294\n",
      "cnt: 0 - valLoss: 0.4265056252479553 - trainLoss: 0.40102750062942505\n",
      "cnt: 0 - valLoss: 0.4265052378177643 - trainLoss: 0.4010263681411743\n",
      "cnt: 0 - valLoss: 0.42650479078292847 - trainLoss: 0.40102511644363403\n",
      "cnt: 0 - valLoss: 0.42650437355041504 - trainLoss: 0.4010239541530609\n",
      "cnt: 0 - valLoss: 0.426503986120224 - trainLoss: 0.4010227620601654\n",
      "cnt: 0 - valLoss: 0.42650356888771057 - trainLoss: 0.4010215699672699\n",
      "cnt: 0 - valLoss: 0.42650312185287476 - trainLoss: 0.4010203778743744\n",
      "cnt: 0 - valLoss: 0.42650270462036133 - trainLoss: 0.4010191559791565\n",
      "cnt: 0 - valLoss: 0.4265022575855255 - trainLoss: 0.40101802349090576\n",
      "cnt: 0 - valLoss: 0.4265018403530121 - trainLoss: 0.40101680159568787\n",
      "cnt: 0 - valLoss: 0.42650142312049866 - trainLoss: 0.40101560950279236\n",
      "cnt: 0 - valLoss: 0.42650097608566284 - trainLoss: 0.40101441740989685\n",
      "cnt: 0 - valLoss: 0.4265005588531494 - trainLoss: 0.40101322531700134\n",
      "cnt: 0 - valLoss: 0.426500141620636 - trainLoss: 0.4010120630264282\n",
      "cnt: 0 - valLoss: 0.42649969458580017 - trainLoss: 0.4010108709335327\n",
      "cnt: 0 - valLoss: 0.42649930715560913 - trainLoss: 0.4010096788406372\n",
      "cnt: 0 - valLoss: 0.4264988899230957 - trainLoss: 0.4010085165500641\n",
      "cnt: 0 - valLoss: 0.4264984428882599 - trainLoss: 0.4010073244571686\n",
      "cnt: 0 - valLoss: 0.42649802565574646 - trainLoss: 0.4010061025619507\n",
      "cnt: 0 - valLoss: 0.4264976382255554 - trainLoss: 0.4010049104690552\n",
      "cnt: 0 - valLoss: 0.426497220993042 - trainLoss: 0.40100371837615967\n",
      "cnt: 0 - valLoss: 0.4264967739582062 - trainLoss: 0.40100255608558655\n",
      "cnt: 0 - valLoss: 0.42649635672569275 - trainLoss: 0.40100136399269104\n",
      "cnt: 0 - valLoss: 0.4264959394931793 - trainLoss: 0.40100017189979553\n",
      "cnt: 0 - valLoss: 0.4264954924583435 - trainLoss: 0.40099895000457764\n",
      "cnt: 0 - valLoss: 0.4264950752258301 - trainLoss: 0.4009978175163269\n",
      "cnt: 0 - valLoss: 0.42649468779563904 - trainLoss: 0.4009966254234314\n",
      "cnt: 0 - valLoss: 0.426494300365448 - trainLoss: 0.4009954035282135\n",
      "cnt: 0 - valLoss: 0.42649388313293457 - trainLoss: 0.400994211435318\n",
      "cnt: 0 - valLoss: 0.42649340629577637 - trainLoss: 0.4009930193424225\n",
      "cnt: 0 - valLoss: 0.4264930188655853 - trainLoss: 0.400991827249527\n",
      "cnt: 0 - valLoss: 0.4264925718307495 - trainLoss: 0.4009906053543091\n",
      "cnt: 0 - valLoss: 0.4264921545982361 - trainLoss: 0.40098947286605835\n",
      "cnt: 0 - valLoss: 0.42649176716804504 - trainLoss: 0.40098831057548523\n",
      "cnt: 0 - valLoss: 0.4264913499355316 - trainLoss: 0.40098705887794495\n",
      "cnt: 0 - valLoss: 0.4264909029006958 - trainLoss: 0.4009858965873718\n",
      "cnt: 0 - valLoss: 0.42649051547050476 - trainLoss: 0.4009847044944763\n",
      "cnt: 0 - valLoss: 0.42649009823799133 - trainLoss: 0.4009835124015808\n",
      "cnt: 0 - valLoss: 0.4264896810054779 - trainLoss: 0.4009823203086853\n",
      "cnt: 0 - valLoss: 0.4264892339706421 - trainLoss: 0.4009811282157898\n",
      "cnt: 0 - valLoss: 0.42648884654045105 - trainLoss: 0.4009799659252167\n",
      "cnt: 0 - valLoss: 0.42648836970329285 - trainLoss: 0.40097877383232117\n",
      "cnt: 0 - valLoss: 0.4264879524707794 - trainLoss: 0.40097755193710327\n",
      "cnt: 0 - valLoss: 0.4264875650405884 - trainLoss: 0.40097641944885254\n",
      "cnt: 0 - valLoss: 0.42648714780807495 - trainLoss: 0.40097519755363464\n",
      "cnt: 0 - valLoss: 0.4264867603778839 - trainLoss: 0.40097400546073914\n",
      "cnt: 0 - valLoss: 0.42648619413375854 - trainLoss: 0.40097281336784363\n",
      "cnt: 0 - valLoss: 0.42648568749427795 - trainLoss: 0.4009716510772705\n",
      "cnt: 0 - valLoss: 0.42648518085479736 - trainLoss: 0.4009704887866974\n",
      "cnt: 0 - valLoss: 0.4264846444129944 - trainLoss: 0.4009692966938019\n",
      "cnt: 0 - valLoss: 0.4264841377735138 - trainLoss: 0.40096810460090637\n",
      "cnt: 0 - valLoss: 0.4264836609363556 - trainLoss: 0.40096694231033325\n",
      "cnt: 0 - valLoss: 0.426483154296875 - trainLoss: 0.40096575021743774\n",
      "cnt: 0 - valLoss: 0.42648258805274963 - trainLoss: 0.40096455812454224\n",
      "cnt: 0 - valLoss: 0.42648211121559143 - trainLoss: 0.40096336603164673\n",
      "cnt: 0 - valLoss: 0.42648160457611084 - trainLoss: 0.4009622037410736\n",
      "cnt: 0 - valLoss: 0.42648106813430786 - trainLoss: 0.4009610116481781\n",
      "cnt: 0 - valLoss: 0.42648059129714966 - trainLoss: 0.400959849357605\n",
      "cnt: 0 - valLoss: 0.4264800548553467 - trainLoss: 0.4009586572647095\n",
      "cnt: 0 - valLoss: 0.4264795482158661 - trainLoss: 0.40095749497413635\n",
      "cnt: 0 - valLoss: 0.4264790415763855 - trainLoss: 0.40095630288124084\n",
      "cnt: 0 - valLoss: 0.4264785647392273 - trainLoss: 0.4009551405906677\n",
      "cnt: 0 - valLoss: 0.4264780580997467 - trainLoss: 0.4009539484977722\n",
      "cnt: 0 - valLoss: 0.4264775812625885 - trainLoss: 0.4009527862071991\n",
      "cnt: 0 - valLoss: 0.4264770746231079 - trainLoss: 0.4009515941143036\n",
      "cnt: 0 - valLoss: 0.4264765679836273 - trainLoss: 0.4009503722190857\n",
      "cnt: 0 - valLoss: 0.42647603154182434 - trainLoss: 0.4009491801261902\n",
      "cnt: 0 - valLoss: 0.42647552490234375 - trainLoss: 0.40094801783561707\n",
      "cnt: 0 - valLoss: 0.42647504806518555 - trainLoss: 0.40094688534736633\n",
      "cnt: 0 - valLoss: 0.42647457122802734 - trainLoss: 0.40094566345214844\n",
      "cnt: 0 - valLoss: 0.42647406458854675 - trainLoss: 0.40094447135925293\n",
      "cnt: 0 - valLoss: 0.42647355794906616 - trainLoss: 0.4009433090686798\n",
      "cnt: 0 - valLoss: 0.42647308111190796 - trainLoss: 0.4009421765804291\n",
      "cnt: 0 - valLoss: 0.42647260427474976 - trainLoss: 0.4009409546852112\n",
      "cnt: 0 - valLoss: 0.42647209763526917 - trainLoss: 0.4009397625923157\n",
      "cnt: 0 - valLoss: 0.42647162079811096 - trainLoss: 0.40093860030174255\n",
      "cnt: 0 - valLoss: 0.42647111415863037 - trainLoss: 0.40093740820884705\n",
      "cnt: 0 - valLoss: 0.42647063732147217 - trainLoss: 0.4009362459182739\n",
      "cnt: 0 - valLoss: 0.4264701306819916 - trainLoss: 0.4009350538253784\n",
      "cnt: 0 - valLoss: 0.4264696538448334 - trainLoss: 0.4009338617324829\n",
      "cnt: 0 - valLoss: 0.4264691472053528 - trainLoss: 0.4009326994419098\n",
      "cnt: 0 - valLoss: 0.4264686703681946 - trainLoss: 0.4009315073490143\n",
      "cnt: 0 - valLoss: 0.4264681935310364 - trainLoss: 0.40093034505844116\n",
      "cnt: 0 - valLoss: 0.4264676868915558 - trainLoss: 0.40092915296554565\n",
      "cnt: 0 - valLoss: 0.4264672100543976 - trainLoss: 0.40092796087265015\n",
      "cnt: 0 - valLoss: 0.4264667332172394 - trainLoss: 0.400926798582077\n",
      "cnt: 0 - valLoss: 0.4264662563800812 - trainLoss: 0.4009256064891815\n",
      "cnt: 0 - valLoss: 0.4264657497406006 - trainLoss: 0.4009244441986084\n",
      "cnt: 0 - valLoss: 0.4264652729034424 - trainLoss: 0.4009232521057129\n",
      "cnt: 0 - valLoss: 0.4264647662639618 - trainLoss: 0.40092208981513977\n",
      "cnt: 0 - valLoss: 0.4264642000198364 - trainLoss: 0.40092092752456665\n",
      "cnt: 0 - valLoss: 0.42646369338035583 - trainLoss: 0.40091973543167114\n",
      "cnt: 0 - valLoss: 0.42646321654319763 - trainLoss: 0.40091854333877563\n",
      "cnt: 0 - valLoss: 0.42646273970603943 - trainLoss: 0.4009173810482025\n",
      "cnt: 0 - valLoss: 0.42646220326423645 - trainLoss: 0.400916188955307\n",
      "cnt: 0 - valLoss: 0.42646166682243347 - trainLoss: 0.4009150266647339\n",
      "cnt: 0 - valLoss: 0.4264611601829529 - trainLoss: 0.4009138345718384\n",
      "cnt: 0 - valLoss: 0.4264606237411499 - trainLoss: 0.40091264247894287\n",
      "cnt: 0 - valLoss: 0.4264601767063141 - trainLoss: 0.40091150999069214\n",
      "cnt: 0 - valLoss: 0.4264596700668335 - trainLoss: 0.40091031789779663\n",
      "cnt: 0 - valLoss: 0.4264591634273529 - trainLoss: 0.4009091258049011\n",
      "cnt: 0 - valLoss: 0.4264586269855499 - trainLoss: 0.400907963514328\n",
      "cnt: 0 - valLoss: 0.42645812034606934 - trainLoss: 0.4009068012237549\n",
      "cnt: 0 - valLoss: 0.4264577031135559 - trainLoss: 0.4009056091308594\n",
      "cnt: 0 - valLoss: 0.42645716667175293 - trainLoss: 0.40090441703796387\n",
      "cnt: 0 - valLoss: 0.42645666003227234 - trainLoss: 0.40090322494506836\n",
      "cnt: 0 - valLoss: 0.426456093788147 - trainLoss: 0.40090206265449524\n",
      "cnt: 0 - valLoss: 0.42645564675331116 - trainLoss: 0.4009009003639221\n",
      "cnt: 0 - valLoss: 0.42645516991615295 - trainLoss: 0.4008997082710266\n",
      "cnt: 0 - valLoss: 0.42645466327667236 - trainLoss: 0.4008985161781311\n",
      "cnt: 0 - valLoss: 0.42645418643951416 - trainLoss: 0.400897353887558\n",
      "cnt: 0 - valLoss: 0.42645367980003357 - trainLoss: 0.40089619159698486\n",
      "cnt: 0 - valLoss: 0.42645320296287537 - trainLoss: 0.40089502930641174\n",
      "cnt: 0 - valLoss: 0.42645272612571716 - trainLoss: 0.40089383721351624\n",
      "cnt: 0 - valLoss: 0.4264522194862366 - trainLoss: 0.4008926451206207\n",
      "cnt: 0 - valLoss: 0.42645174264907837 - trainLoss: 0.4008914828300476\n",
      "cnt: 0 - valLoss: 0.4264512360095978 - trainLoss: 0.4008902907371521\n",
      "cnt: 0 - valLoss: 0.42645078897476196 - trainLoss: 0.4008890986442566\n",
      "cnt: 0 - valLoss: 0.42645028233528137 - trainLoss: 0.40088796615600586\n",
      "cnt: 0 - valLoss: 0.42644980549812317 - trainLoss: 0.40088677406311035\n",
      "cnt: 0 - valLoss: 0.4264492988586426 - trainLoss: 0.40088558197021484\n",
      "cnt: 0 - valLoss: 0.426448792219162 - trainLoss: 0.4008844196796417\n",
      "cnt: 0 - valLoss: 0.42644834518432617 - trainLoss: 0.4008832573890686\n",
      "cnt: 0 - valLoss: 0.4264478385448456 - trainLoss: 0.4008820652961731\n",
      "cnt: 0 - valLoss: 0.4264473617076874 - trainLoss: 0.4008808732032776\n",
      "cnt: 0 - valLoss: 0.42644715309143066 - trainLoss: 0.40087971091270447\n",
      "cnt: 0 - valLoss: 0.4264466464519501 - trainLoss: 0.40087857842445374\n",
      "cnt: 0 - valLoss: 0.42644616961479187 - trainLoss: 0.40087735652923584\n",
      "cnt: 0 - valLoss: 0.4264456629753113 - trainLoss: 0.40087616443634033\n",
      "cnt: 0 - valLoss: 0.42644548416137695 - trainLoss: 0.4008750021457672\n",
      "cnt: 0 - valLoss: 0.42644497752189636 - trainLoss: 0.4008738696575165\n",
      "cnt: 0 - valLoss: 0.42644450068473816 - trainLoss: 0.4008726477622986\n",
      "cnt: 0 - valLoss: 0.4264439642429352 - trainLoss: 0.4008714556694031\n",
      "cnt: 0 - valLoss: 0.42644381523132324 - trainLoss: 0.40087029337882996\n",
      "cnt: 0 - valLoss: 0.42644327878952026 - trainLoss: 0.4008691608905792\n",
      "cnt: 0 - valLoss: 0.4264427721500397 - trainLoss: 0.40086793899536133\n",
      "cnt: 0 - valLoss: 0.42644229531288147 - trainLoss: 0.4008667469024658\n",
      "cnt: 0 - valLoss: 0.42644214630126953 - trainLoss: 0.4008656442165375\n",
      "cnt: 0 - valLoss: 0.42644160985946655 - trainLoss: 0.40086445212364197\n",
      "cnt: 0 - valLoss: 0.42644116282463074 - trainLoss: 0.4008632302284241\n",
      "cnt: 0 - valLoss: 0.42644068598747253 - trainLoss: 0.40086203813552856\n",
      "cnt: 0 - valLoss: 0.42644020915031433 - trainLoss: 0.40086087584495544\n",
      "cnt: 0 - valLoss: 0.42643994092941284 - trainLoss: 0.4008597433567047\n",
      "cnt: 0 - valLoss: 0.42643943428993225 - trainLoss: 0.4008585214614868\n",
      "cnt: 0 - valLoss: 0.4264390170574188 - trainLoss: 0.4008573293685913\n",
      "cnt: 0 - valLoss: 0.42643848061561584 - trainLoss: 0.40085622668266296\n",
      "cnt: 0 - valLoss: 0.4264383316040039 - trainLoss: 0.40085503458976746\n",
      "cnt: 0 - valLoss: 0.4264378845691681 - trainLoss: 0.40085381269454956\n",
      "cnt: 0 - valLoss: 0.4264373779296875 - trainLoss: 0.40085268020629883\n",
      "cnt: 0 - valLoss: 0.4264369010925293 - trainLoss: 0.4008515179157257\n",
      "cnt: 0 - valLoss: 0.4264364242553711 - trainLoss: 0.4008503258228302\n",
      "cnt: 0 - valLoss: 0.426436185836792 - trainLoss: 0.4008491635322571\n",
      "cnt: 0 - valLoss: 0.4264357089996338 - trainLoss: 0.4008479714393616\n",
      "cnt: 0 - valLoss: 0.4264352321624756 - trainLoss: 0.40084680914878845\n",
      "cnt: 0 - valLoss: 0.42643481492996216 - trainLoss: 0.40084561705589294\n",
      "cnt: 0 - valLoss: 0.42643433809280396 - trainLoss: 0.4008444547653198\n",
      "cnt: 0 - valLoss: 0.42643406987190247 - trainLoss: 0.4008432924747467\n",
      "cnt: 0 - valLoss: 0.42643359303474426 - trainLoss: 0.4008421003818512\n",
      "cnt: 0 - valLoss: 0.42643314599990845 - trainLoss: 0.4008409082889557\n",
      "cnt: 0 - valLoss: 0.42643266916275024 - trainLoss: 0.40083974599838257\n",
      "cnt: 0 - valLoss: 0.42643246054649353 - trainLoss: 0.40083858370780945\n",
      "cnt: 0 - valLoss: 0.4264319837093353 - trainLoss: 0.40083739161491394\n",
      "cnt: 0 - valLoss: 0.4264315068721771 - trainLoss: 0.40083619952201843\n",
      "cnt: 0 - valLoss: 0.4264310896396637 - trainLoss: 0.4008350372314453\n",
      "cnt: 0 - valLoss: 0.4264305531978607 - trainLoss: 0.4008338749408722\n",
      "cnt: 0 - valLoss: 0.4264304041862488 - trainLoss: 0.4008326828479767\n",
      "cnt: 0 - valLoss: 0.4264299273490906 - trainLoss: 0.40083152055740356\n",
      "cnt: 0 - valLoss: 0.4264294505119324 - trainLoss: 0.40083035826683044\n",
      "cnt: 0 - valLoss: 0.4264289438724518 - trainLoss: 0.40082916617393494\n",
      "cnt: 0 - valLoss: 0.4264284670352936 - trainLoss: 0.40082797408103943\n",
      "cnt: 0 - valLoss: 0.42642834782600403 - trainLoss: 0.4008268117904663\n",
      "cnt: 0 - valLoss: 0.4264278709888458 - trainLoss: 0.4008256494998932\n",
      "cnt: 0 - valLoss: 0.4264273941516876 - trainLoss: 0.40082451701164246\n",
      "cnt: 0 - valLoss: 0.4264269173145294 - trainLoss: 0.40082329511642456\n",
      "cnt: 0 - valLoss: 0.4264264404773712 - trainLoss: 0.40082216262817383\n",
      "cnt: 0 - valLoss: 0.4264262318611145 - trainLoss: 0.40082094073295593\n",
      "cnt: 0 - valLoss: 0.4264258146286011 - trainLoss: 0.4008197486400604\n",
      "cnt: 0 - valLoss: 0.42642533779144287 - trainLoss: 0.4008186459541321\n",
      "cnt: 0 - valLoss: 0.42642486095428467 - trainLoss: 0.4008174538612366\n",
      "cnt: 0 - valLoss: 0.4264245927333832 - trainLoss: 0.4008162319660187\n",
      "cnt: 0 - valLoss: 0.42642417550086975 - trainLoss: 0.40081509947776794\n",
      "cnt: 0 - valLoss: 0.4264237582683563 - trainLoss: 0.4008139371871948\n",
      "cnt: 0 - valLoss: 0.4264233112335205 - trainLoss: 0.4008127450942993\n",
      "cnt: 0 - valLoss: 0.4264228045940399 - trainLoss: 0.4008115828037262\n",
      "cnt: 0 - valLoss: 0.4264223277568817 - trainLoss: 0.4008103907108307\n",
      "cnt: 0 - valLoss: 0.4264221489429474 - trainLoss: 0.40080922842025757\n",
      "cnt: 0 - valLoss: 0.4264217019081116 - trainLoss: 0.40080806612968445\n",
      "cnt: 0 - valLoss: 0.42642122507095337 - trainLoss: 0.40080687403678894\n",
      "cnt: 0 - valLoss: 0.42642077803611755 - trainLoss: 0.4008057117462158\n",
      "cnt: 0 - valLoss: 0.4264206290245056 - trainLoss: 0.4008045196533203\n",
      "cnt: 0 - valLoss: 0.4264201521873474 - trainLoss: 0.4008033573627472\n",
      "cnt: 0 - valLoss: 0.4264197051525116 - trainLoss: 0.4008021950721741\n",
      "cnt: 0 - valLoss: 0.4264192581176758 - trainLoss: 0.40080100297927856\n",
      "cnt: 0 - valLoss: 0.42641881108283997 - trainLoss: 0.40079981088638306\n",
      "cnt: 0 - valLoss: 0.42641833424568176 - trainLoss: 0.4007987082004547\n",
      "cnt: 0 - valLoss: 0.4264181852340698 - trainLoss: 0.4007974863052368\n",
      "cnt: 0 - valLoss: 0.4264177083969116 - trainLoss: 0.4007963538169861\n",
      "cnt: 0 - valLoss: 0.4264172613620758 - trainLoss: 0.40079519152641296\n",
      "cnt: 0 - valLoss: 0.42641681432724 - trainLoss: 0.40079399943351746\n",
      "cnt: 0 - valLoss: 0.4264163672924042 - trainLoss: 0.40079277753829956\n",
      "cnt: 0 - valLoss: 0.42641595005989075 - trainLoss: 0.40079164505004883\n",
      "cnt: 0 - valLoss: 0.42641574144363403 - trainLoss: 0.4007904827594757\n",
      "cnt: 0 - valLoss: 0.42641526460647583 - trainLoss: 0.4007892906665802\n",
      "cnt: 0 - valLoss: 0.4264148473739624 - trainLoss: 0.4007880687713623\n",
      "cnt: 0 - valLoss: 0.4264144003391266 - trainLoss: 0.40078696608543396\n",
      "cnt: 0 - valLoss: 0.4264139235019684 - trainLoss: 0.40078577399253845\n",
      "cnt: 0 - valLoss: 0.4264134466648102 - trainLoss: 0.40078461170196533\n",
      "cnt: 0 - valLoss: 0.42641332745552063 - trainLoss: 0.4007834196090698\n",
      "cnt: 0 - valLoss: 0.4264128506183624 - trainLoss: 0.4007822573184967\n",
      "cnt: 0 - valLoss: 0.426412433385849 - trainLoss: 0.4007810652256012\n",
      "cnt: 0 - valLoss: 0.42641201615333557 - trainLoss: 0.4007799029350281\n",
      "cnt: 0 - valLoss: 0.42641156911849976 - trainLoss: 0.40077874064445496\n",
      "cnt: 0 - valLoss: 0.42641115188598633 - trainLoss: 0.4007776081562042\n",
      "cnt: 0 - valLoss: 0.4264109432697296 - trainLoss: 0.40077638626098633\n",
      "cnt: 0 - valLoss: 0.4264104664325714 - trainLoss: 0.4007752537727356\n",
      "cnt: 0 - valLoss: 0.4264100193977356 - trainLoss: 0.4007740318775177\n",
      "cnt: 0 - valLoss: 0.42640960216522217 - trainLoss: 0.40077289938926697\n",
      "cnt: 0 - valLoss: 0.42640918493270874 - trainLoss: 0.40077173709869385\n",
      "cnt: 0 - valLoss: 0.4264087378978729 - trainLoss: 0.4007705748081207\n",
      "cnt: 0 - valLoss: 0.4264083206653595 - trainLoss: 0.40076932311058044\n",
      "cnt: 0 - valLoss: 0.42640814185142517 - trainLoss: 0.4007682204246521\n",
      "cnt: 0 - valLoss: 0.42640772461891174 - trainLoss: 0.4007670283317566\n",
      "cnt: 0 - valLoss: 0.42640724778175354 - trainLoss: 0.40076586604118347\n",
      "cnt: 0 - valLoss: 0.4264068305492401 - trainLoss: 0.40076467394828796\n",
      "cnt: 0 - valLoss: 0.4264063537120819 - trainLoss: 0.40076351165771484\n",
      "cnt: 0 - valLoss: 0.42640620470046997 - trainLoss: 0.4007623493671417\n",
      "cnt: 0 - valLoss: 0.42640575766563416 - trainLoss: 0.4007611870765686\n",
      "cnt: 0 - valLoss: 0.42640531063079834 - trainLoss: 0.4007599949836731\n",
      "cnt: 0 - valLoss: 0.42640483379364014 - trainLoss: 0.4007588028907776\n",
      "cnt: 0 - valLoss: 0.42640450596809387 - trainLoss: 0.40075770020484924\n",
      "cnt: 0 - valLoss: 0.42640402913093567 - trainLoss: 0.40075650811195374\n",
      "cnt: 0 - valLoss: 0.42640361189842224 - trainLoss: 0.40075528621673584\n",
      "cnt: 0 - valLoss: 0.4264034330844879 - trainLoss: 0.40075409412384033\n",
      "cnt: 0 - valLoss: 0.4264030158519745 - trainLoss: 0.400752991437912\n",
      "cnt: 0 - valLoss: 0.42640256881713867 - trainLoss: 0.4007517993450165\n",
      "cnt: 0 - valLoss: 0.42640215158462524 - trainLoss: 0.40075063705444336\n",
      "cnt: 0 - valLoss: 0.42640170454978943 - trainLoss: 0.40074947476387024\n",
      "cnt: 0 - valLoss: 0.426401287317276 - trainLoss: 0.40074828267097473\n",
      "cnt: 0 - valLoss: 0.4264010787010193 - trainLoss: 0.400747150182724\n",
      "cnt: 0 - valLoss: 0.42640063166618347 - trainLoss: 0.4007459580898285\n",
      "cnt: 0 - valLoss: 0.4264002740383148 - trainLoss: 0.400744765996933\n",
      "cnt: 0 - valLoss: 0.426399827003479 - trainLoss: 0.4007435739040375\n",
      "cnt: 0 - valLoss: 0.4263994097709656 - trainLoss: 0.40074244141578674\n",
      "cnt: 0 - valLoss: 0.42639896273612976 - trainLoss: 0.40074124932289124\n",
      "cnt: 0 - valLoss: 0.42639854550361633 - trainLoss: 0.4007400870323181\n",
      "cnt: 0 - valLoss: 0.426398366689682 - trainLoss: 0.4007389545440674\n",
      "cnt: 0 - valLoss: 0.4263979494571686 - trainLoss: 0.4007377326488495\n",
      "cnt: 0 - valLoss: 0.42639750242233276 - trainLoss: 0.400736540555954\n",
      "cnt: 0 - valLoss: 0.42639708518981934 - trainLoss: 0.40073543787002563\n",
      "cnt: 0 - valLoss: 0.4263966679573059 - trainLoss: 0.4007342457771301\n",
      "cnt: 0 - valLoss: 0.42639631032943726 - trainLoss: 0.400733083486557\n",
      "cnt: 0 - valLoss: 0.42639583349227905 - trainLoss: 0.4007319211959839\n",
      "cnt: 0 - valLoss: 0.4263954162597656 - trainLoss: 0.4007307291030884\n",
      "cnt: 0 - valLoss: 0.4263952374458313 - trainLoss: 0.40072956681251526\n",
      "cnt: 0 - valLoss: 0.42639485001564026 - trainLoss: 0.40072837471961975\n",
      "cnt: 0 - valLoss: 0.42639437317848206 - trainLoss: 0.40072721242904663\n",
      "cnt: 0 - valLoss: 0.42639395594596863 - trainLoss: 0.4007261097431183\n",
      "cnt: 0 - valLoss: 0.4263935387134552 - trainLoss: 0.4007249176502228\n",
      "cnt: 0 - valLoss: 0.4263930916786194 - trainLoss: 0.4007236957550049\n",
      "cnt: 0 - valLoss: 0.42639297246932983 - trainLoss: 0.4007225036621094\n",
      "cnt: 0 - valLoss: 0.42639249563217163 - trainLoss: 0.40072140097618103\n",
      "cnt: 0 - valLoss: 0.4263921082019806 - trainLoss: 0.4007202088832855\n",
      "cnt: 0 - valLoss: 0.42639172077178955 - trainLoss: 0.4007190465927124\n",
      "cnt: 0 - valLoss: 0.4263913035392761 - trainLoss: 0.4007178544998169\n",
      "cnt: 0 - valLoss: 0.4263908267021179 - trainLoss: 0.4007166922092438\n",
      "cnt: 0 - valLoss: 0.4263904392719269 - trainLoss: 0.40071552991867065\n",
      "cnt: 0 - valLoss: 0.42639002203941345 - trainLoss: 0.40071436762809753\n",
      "cnt: 0 - valLoss: 0.4263898730278015 - trainLoss: 0.400713175535202\n",
      "cnt: 0 - valLoss: 0.4263894557952881 - trainLoss: 0.4007120132446289\n",
      "cnt: 0 - valLoss: 0.42638903856277466 - trainLoss: 0.4007108807563782\n",
      "cnt: 0 - valLoss: 0.42638859152793884 - trainLoss: 0.4007096588611603\n",
      "cnt: 0 - valLoss: 0.4263881742954254 - trainLoss: 0.40070855617523193\n",
      "cnt: 0 - valLoss: 0.426387757062912 - trainLoss: 0.4007073640823364\n",
      "cnt: 0 - valLoss: 0.42638739943504333 - trainLoss: 0.4007061719894409\n",
      "cnt: 0 - valLoss: 0.4263869822025299 - trainLoss: 0.4007050096988678\n",
      "cnt: 0 - valLoss: 0.4263868033885956 - trainLoss: 0.4007038474082947\n",
      "cnt: 0 - valLoss: 0.42638638615608215 - trainLoss: 0.40070265531539917\n",
      "cnt: 0 - valLoss: 0.42638593912124634 - trainLoss: 0.40070149302482605\n",
      "cnt: 0 - valLoss: 0.4263855218887329 - trainLoss: 0.40070033073425293\n",
      "cnt: 0 - valLoss: 0.4263850748538971 - trainLoss: 0.4006991684436798\n",
      "cnt: 0 - valLoss: 0.42638468742370605 - trainLoss: 0.4006980359554291\n",
      "cnt: 0 - valLoss: 0.4263842701911926 - trainLoss: 0.4006968140602112\n",
      "cnt: 0 - valLoss: 0.4263841509819031 - trainLoss: 0.4006956219673157\n",
      "cnt: 0 - valLoss: 0.42638370394706726 - trainLoss: 0.40069451928138733\n",
      "cnt: 0 - valLoss: 0.42638328671455383 - trainLoss: 0.4006933271884918\n",
      "cnt: 0 - valLoss: 0.4263828694820404 - trainLoss: 0.4006921052932739\n",
      "cnt: 0 - valLoss: 0.42638251185417175 - trainLoss: 0.4006910026073456\n",
      "cnt: 0 - valLoss: 0.4263821244239807 - trainLoss: 0.4006898105144501\n",
      "cnt: 0 - valLoss: 0.4263816475868225 - trainLoss: 0.40068870782852173\n",
      "cnt: 0 - valLoss: 0.42638126015663147 - trainLoss: 0.40068748593330383\n",
      "cnt: 0 - valLoss: 0.4263811409473419 - trainLoss: 0.4006862938404083\n",
      "cnt: 0 - valLoss: 0.4263807237148285 - trainLoss: 0.4006851315498352\n",
      "cnt: 0 - valLoss: 0.4263802766799927 - trainLoss: 0.4006839990615845\n",
      "cnt: 0 - valLoss: 0.42637988924980164 - trainLoss: 0.40068283677101135\n",
      "cnt: 0 - valLoss: 0.4263795018196106 - trainLoss: 0.40068167448043823\n",
      "cnt: 0 - valLoss: 0.42637908458709717 - trainLoss: 0.4006805121898651\n",
      "cnt: 0 - valLoss: 0.42637866735458374 - trainLoss: 0.4006793797016144\n",
      "cnt: 0 - valLoss: 0.4263783097267151 - trainLoss: 0.4006781578063965\n",
      "cnt: 0 - valLoss: 0.42637792229652405 - trainLoss: 0.40067702531814575\n",
      "cnt: 0 - valLoss: 0.4263778030872345 - trainLoss: 0.40067586302757263\n",
      "cnt: 0 - valLoss: 0.42637738585472107 - trainLoss: 0.4006746709346771\n",
      "cnt: 0 - valLoss: 0.42637693881988525 - trainLoss: 0.400673508644104\n",
      "cnt: 0 - valLoss: 0.4263765513896942 - trainLoss: 0.4006723463535309\n",
      "cnt: 0 - valLoss: 0.4263761639595032 - trainLoss: 0.40067118406295776\n",
      "cnt: 0 - valLoss: 0.42637574672698975 - trainLoss: 0.40067002177238464\n",
      "cnt: 0 - valLoss: 0.42637529969215393 - trainLoss: 0.40066882967948914\n",
      "cnt: 0 - valLoss: 0.4263749420642853 - trainLoss: 0.400667667388916\n",
      "cnt: 0 - valLoss: 0.42637479305267334 - trainLoss: 0.4006664752960205\n",
      "cnt: 0 - valLoss: 0.4263743758201599 - trainLoss: 0.4006653130054474\n",
      "cnt: 0 - valLoss: 0.42637401819229126 - trainLoss: 0.40066418051719666\n",
      "cnt: 0 - valLoss: 0.42637360095977783 - trainLoss: 0.40066301822662354\n",
      "cnt: 0 - valLoss: 0.4263732135295868 - trainLoss: 0.400661826133728\n",
      "cnt: 0 - valLoss: 0.42637282609939575 - trainLoss: 0.4006606936454773\n",
      "cnt: 0 - valLoss: 0.4263724386692047 - trainLoss: 0.4006595015525818\n",
      "cnt: 0 - valLoss: 0.4263720214366913 - trainLoss: 0.40065833926200867\n",
      "cnt: 0 - valLoss: 0.42637163400650024 - trainLoss: 0.40065720677375793\n",
      "cnt: 0 - valLoss: 0.4263714849948883 - trainLoss: 0.40065598487854004\n",
      "cnt: 0 - valLoss: 0.4263710677623749 - trainLoss: 0.4006548523902893\n",
      "cnt: 0 - valLoss: 0.42637068033218384 - trainLoss: 0.4006536900997162\n",
      "cnt: 0 - valLoss: 0.4263702630996704 - trainLoss: 0.40065252780914307\n",
      "cnt: 0 - valLoss: 0.42636987566947937 - trainLoss: 0.40065136551856995\n",
      "cnt: 0 - valLoss: 0.42636948823928833 - trainLoss: 0.40065017342567444\n",
      "cnt: 0 - valLoss: 0.4263690710067749 - trainLoss: 0.4006490111351013\n",
      "cnt: 0 - valLoss: 0.42636871337890625 - trainLoss: 0.4006478488445282\n",
      "cnt: 0 - valLoss: 0.4263682961463928 - trainLoss: 0.4006466567516327\n",
      "cnt: 0 - valLoss: 0.42636793851852417 - trainLoss: 0.40064555406570435\n",
      "cnt: 0 - valLoss: 0.42636775970458984 - trainLoss: 0.40064436197280884\n",
      "cnt: 0 - valLoss: 0.4263673722743988 - trainLoss: 0.4006431996822357\n",
      "cnt: 0 - valLoss: 0.42636698484420776 - trainLoss: 0.4006420373916626\n",
      "cnt: 0 - valLoss: 0.42636656761169434 - trainLoss: 0.4006408751010895\n",
      "cnt: 0 - valLoss: 0.4263661503791809 - trainLoss: 0.40063968300819397\n",
      "cnt: 0 - valLoss: 0.42636579275131226 - trainLoss: 0.40063852071762085\n",
      "cnt: 0 - valLoss: 0.42636537551879883 - trainLoss: 0.4006373882293701\n",
      "cnt: 0 - valLoss: 0.4263649880886078 - trainLoss: 0.400636225938797\n",
      "cnt: 0 - valLoss: 0.42636486887931824 - trainLoss: 0.4006350636482239\n",
      "cnt: 0 - valLoss: 0.4263644218444824 - trainLoss: 0.40063387155532837\n",
      "cnt: 0 - valLoss: 0.4263640344142914 - trainLoss: 0.40063270926475525\n",
      "cnt: 0 - valLoss: 0.42636364698410034 - trainLoss: 0.40063154697418213\n",
      "cnt: 0 - valLoss: 0.4263633191585541 - trainLoss: 0.4006304144859314\n",
      "cnt: 0 - valLoss: 0.42636287212371826 - trainLoss: 0.4006291925907135\n",
      "cnt: 0 - valLoss: 0.4263624846935272 - trainLoss: 0.40062808990478516\n",
      "cnt: 0 - valLoss: 0.42636212706565857 - trainLoss: 0.40062689781188965\n",
      "cnt: 0 - valLoss: 0.42636168003082275 - trainLoss: 0.40062573552131653\n",
      "cnt: 0 - valLoss: 0.4263613522052765 - trainLoss: 0.4006245732307434\n",
      "cnt: 0 - valLoss: 0.42636117339134216 - trainLoss: 0.4006233811378479\n",
      "cnt: 0 - valLoss: 0.4263607859611511 - trainLoss: 0.40062227845191956\n",
      "cnt: 0 - valLoss: 0.4263603985309601 - trainLoss: 0.40062105655670166\n",
      "cnt: 0 - valLoss: 0.42635998129844666 - trainLoss: 0.4006199240684509\n",
      "cnt: 0 - valLoss: 0.4263595938682556 - trainLoss: 0.4006187617778778\n",
      "cnt: 0 - valLoss: 0.4263592064380646 - trainLoss: 0.4006175994873047\n",
      "cnt: 0 - valLoss: 0.42635881900787354 - trainLoss: 0.4006164073944092\n",
      "cnt: 0 - valLoss: 0.4263584315776825 - trainLoss: 0.40061524510383606\n",
      "cnt: 0 - valLoss: 0.42635807394981384 - trainLoss: 0.40061408281326294\n",
      "cnt: 0 - valLoss: 0.4263576865196228 - trainLoss: 0.4006129503250122\n",
      "cnt: 0 - valLoss: 0.42635729908943176 - trainLoss: 0.4006117880344391\n",
      "cnt: 0 - valLoss: 0.4263571798801422 - trainLoss: 0.40061062574386597\n",
      "cnt: 0 - valLoss: 0.42635679244995117 - trainLoss: 0.40060943365097046\n",
      "cnt: 0 - valLoss: 0.42635640501976013 - trainLoss: 0.40060827136039734\n",
      "cnt: 0 - valLoss: 0.4263560175895691 - trainLoss: 0.4006071090698242\n",
      "cnt: 0 - valLoss: 0.42635560035705566 - trainLoss: 0.4006059467792511\n",
      "cnt: 0 - valLoss: 0.426355242729187 - trainLoss: 0.40060481429100037\n",
      "cnt: 0 - valLoss: 0.42635485529899597 - trainLoss: 0.40060359239578247\n",
      "cnt: 0 - valLoss: 0.4263544976711273 - trainLoss: 0.40060245990753174\n",
      "cnt: 0 - valLoss: 0.4263540506362915 - trainLoss: 0.4006012976169586\n",
      "cnt: 0 - valLoss: 0.42635372281074524 - trainLoss: 0.4006001353263855\n",
      "cnt: 0 - valLoss: 0.4263533651828766 - trainLoss: 0.4005989730358124\n",
      "cnt: 0 - valLoss: 0.42635297775268555 - trainLoss: 0.40059778094291687\n",
      "cnt: 0 - valLoss: 0.4263525903224945 - trainLoss: 0.40059661865234375\n",
      "cnt: 0 - valLoss: 0.42635250091552734 - trainLoss: 0.400595486164093\n",
      "cnt: 0 - valLoss: 0.4263520836830139 - trainLoss: 0.4005943536758423\n",
      "cnt: 0 - valLoss: 0.42635172605514526 - trainLoss: 0.4005931615829468\n",
      "cnt: 0 - valLoss: 0.42635130882263184 - trainLoss: 0.40059199929237366\n",
      "cnt: 0 - valLoss: 0.42635098099708557 - trainLoss: 0.40059080719947815\n",
      "cnt: 0 - valLoss: 0.42635059356689453 - trainLoss: 0.4005897045135498\n",
      "cnt: 0 - valLoss: 0.4263502061367035 - trainLoss: 0.4005884826183319\n",
      "cnt: 0 - valLoss: 0.42634981870651245 - trainLoss: 0.4005873501300812\n",
      "cnt: 0 - valLoss: 0.4263494312763214 - trainLoss: 0.40058618783950806\n",
      "cnt: 0 - valLoss: 0.42634904384613037 - trainLoss: 0.40058502554893494\n",
      "cnt: 0 - valLoss: 0.4263486862182617 - trainLoss: 0.40058383345603943\n",
      "cnt: 0 - valLoss: 0.42634856700897217 - trainLoss: 0.4005826711654663\n",
      "cnt: 0 - valLoss: 0.42634817957878113 - trainLoss: 0.4005815088748932\n",
      "cnt: 0 - valLoss: 0.4263477921485901 - trainLoss: 0.40058037638664246\n",
      "cnt: 0 - valLoss: 0.42634740471839905 - trainLoss: 0.40057921409606934\n",
      "cnt: 0 - valLoss: 0.4263470768928528 - trainLoss: 0.4005780518054962\n",
      "cnt: 0 - valLoss: 0.42634668946266174 - trainLoss: 0.4005768895149231\n",
      "cnt: 0 - valLoss: 0.4263463318347931 - trainLoss: 0.4005756974220276\n",
      "cnt: 0 - valLoss: 0.42634594440460205 - trainLoss: 0.40057453513145447\n",
      "cnt: 0 - valLoss: 0.426345556974411 - trainLoss: 0.40057340264320374\n",
      "cnt: 0 - valLoss: 0.42634516954421997 - trainLoss: 0.4005722403526306\n",
      "cnt: 0 - valLoss: 0.42634478211402893 - trainLoss: 0.4005710780620575\n",
      "cnt: 0 - valLoss: 0.42634445428848267 - trainLoss: 0.4005699157714844\n",
      "cnt: 0 - valLoss: 0.4263440668582916 - trainLoss: 0.40056875348091125\n",
      "cnt: 0 - valLoss: 0.4263436794281006 - trainLoss: 0.40056756138801575\n",
      "cnt: 0 - valLoss: 0.42634356021881104 - trainLoss: 0.4005663990974426\n",
      "cnt: 0 - valLoss: 0.42634317278862 - trainLoss: 0.4005652666091919\n",
      "cnt: 0 - valLoss: 0.42634281516075134 - trainLoss: 0.4005641043186188\n",
      "cnt: 0 - valLoss: 0.4263424277305603 - trainLoss: 0.40056294202804565\n",
      "cnt: 0 - valLoss: 0.42634204030036926 - trainLoss: 0.40056174993515015\n",
      "cnt: 0 - valLoss: 0.4263417422771454 - trainLoss: 0.400560587644577\n",
      "cnt: 0 - valLoss: 0.42634135484695435 - trainLoss: 0.4005594253540039\n",
      "cnt: 0 - valLoss: 0.4263409674167633 - trainLoss: 0.4005582928657532\n",
      "cnt: 0 - valLoss: 0.42634060978889465 - trainLoss: 0.40055713057518005\n",
      "cnt: 0 - valLoss: 0.4263402223587036 - trainLoss: 0.40055596828460693\n",
      "cnt: 0 - valLoss: 0.42633986473083496 - trainLoss: 0.4005548059940338\n",
      "cnt: 0 - valLoss: 0.4263394773006439 - trainLoss: 0.4005536437034607\n",
      "cnt: 0 - valLoss: 0.4263390898704529 - trainLoss: 0.40055251121520996\n",
      "cnt: 0 - valLoss: 0.4263387620449066 - trainLoss: 0.40055134892463684\n",
      "cnt: 0 - valLoss: 0.4263386130332947 - trainLoss: 0.4005501866340637\n",
      "cnt: 0 - valLoss: 0.42633822560310364 - trainLoss: 0.4005489945411682\n",
      "cnt: 0 - valLoss: 0.4263378381729126 - trainLoss: 0.4005478322505951\n",
      "cnt: 0 - valLoss: 0.42633751034736633 - trainLoss: 0.400546669960022\n",
      "cnt: 0 - valLoss: 0.4263371229171753 - trainLoss: 0.40054553747177124\n",
      "cnt: 0 - valLoss: 0.42633670568466187 - trainLoss: 0.4005443751811981\n",
      "cnt: 0 - valLoss: 0.426336407661438 - trainLoss: 0.4005431830883026\n",
      "cnt: 0 - valLoss: 0.42633605003356934 - trainLoss: 0.4005420207977295\n",
      "cnt: 0 - valLoss: 0.42633572220802307 - trainLoss: 0.40054085850715637\n",
      "cnt: 0 - valLoss: 0.42633533477783203 - trainLoss: 0.40053969621658325\n",
      "cnt: 0 - valLoss: 0.426334947347641 - trainLoss: 0.40053853392601013\n",
      "cnt: 0 - valLoss: 0.42633458971977234 - trainLoss: 0.4005374014377594\n",
      "cnt: 0 - valLoss: 0.4263342022895813 - trainLoss: 0.4005362391471863\n",
      "cnt: 0 - valLoss: 0.42633381485939026 - trainLoss: 0.40053507685661316\n",
      "cnt: 0 - valLoss: 0.4263337254524231 - trainLoss: 0.40053391456604004\n",
      "cnt: 0 - valLoss: 0.42633330821990967 - trainLoss: 0.40053272247314453\n",
      "cnt: 0 - valLoss: 0.4263329803943634 - trainLoss: 0.4005315601825714\n",
      "cnt: 0 - valLoss: 0.42633259296417236 - trainLoss: 0.4005304276943207\n",
      "cnt: 0 - valLoss: 0.4263322055339813 - trainLoss: 0.40052926540374756\n",
      "cnt: 0 - valLoss: 0.42633184790611267 - trainLoss: 0.40052810311317444\n",
      "cnt: 0 - valLoss: 0.42633146047592163 - trainLoss: 0.4005269408226013\n",
      "cnt: 0 - valLoss: 0.4263310730457306 - trainLoss: 0.4005257785320282\n",
      "cnt: 0 - valLoss: 0.4263307452201843 - trainLoss: 0.40052464604377747\n",
      "cnt: 0 - valLoss: 0.4263303577899933 - trainLoss: 0.40052348375320435\n",
      "cnt: 0 - valLoss: 0.42632997035980225 - trainLoss: 0.40052229166030884\n",
      "cnt: 0 - valLoss: 0.4263296127319336 - trainLoss: 0.4005211591720581\n",
      "cnt: 0 - valLoss: 0.42632922530174255 - trainLoss: 0.4005200266838074\n",
      "cnt: 0 - valLoss: 0.4263288378715515 - trainLoss: 0.4005188047885895\n",
      "cnt: 0 - valLoss: 0.4263288378715515 - trainLoss: 0.40051767230033875\n",
      "cnt: 0 - valLoss: 0.42632848024368286 - trainLoss: 0.4005165100097656\n",
      "cnt: 0 - valLoss: 0.4263280928134918 - trainLoss: 0.4005153477191925\n",
      "cnt: 0 - valLoss: 0.42632773518562317 - trainLoss: 0.400514155626297\n",
      "cnt: 0 - valLoss: 0.42632734775543213 - trainLoss: 0.40051302313804626\n",
      "cnt: 0 - valLoss: 0.4263269603252411 - trainLoss: 0.40051183104515076\n",
      "cnt: 0 - valLoss: 0.4263266324996948 - trainLoss: 0.4005107283592224\n",
      "cnt: 0 - valLoss: 0.4263262450695038 - trainLoss: 0.4005095660686493\n",
      "cnt: 0 - valLoss: 0.42632588744163513 - trainLoss: 0.4005083739757538\n",
      "cnt: 0 - valLoss: 0.4263255000114441 - trainLoss: 0.40050721168518066\n",
      "cnt: 0 - valLoss: 0.4263251721858978 - trainLoss: 0.40050604939460754\n",
      "cnt: 0 - valLoss: 0.4263247847557068 - trainLoss: 0.4005049169063568\n",
      "cnt: 0 - valLoss: 0.42632439732551575 - trainLoss: 0.4005037546157837\n",
      "cnt: 0 - valLoss: 0.4263240396976471 - trainLoss: 0.40050259232521057\n",
      "cnt: 0 - valLoss: 0.42632371187210083 - trainLoss: 0.40050143003463745\n",
      "cnt: 0 - valLoss: 0.4263233244419098 - trainLoss: 0.4005002975463867\n",
      "cnt: 0 - valLoss: 0.42632293701171875 - trainLoss: 0.4004991352558136\n",
      "cnt: 0 - valLoss: 0.4263228476047516 - trainLoss: 0.4004979729652405\n",
      "cnt: 0 - valLoss: 0.4263225197792053 - trainLoss: 0.40049678087234497\n",
      "cnt: 0 - valLoss: 0.42632216215133667 - trainLoss: 0.40049564838409424\n",
      "cnt: 0 - valLoss: 0.42632174491882324 - trainLoss: 0.40049445629119873\n",
      "cnt: 0 - valLoss: 0.4263213872909546 - trainLoss: 0.4004933536052704\n",
      "cnt: 0 - valLoss: 0.4263210594654083 - trainLoss: 0.4004921615123749\n",
      "cnt: 0 - valLoss: 0.4263207018375397 - trainLoss: 0.40049099922180176\n",
      "cnt: 0 - valLoss: 0.42632028460502625 - trainLoss: 0.40048983693122864\n",
      "cnt: 0 - valLoss: 0.4263199269771576 - trainLoss: 0.4004886746406555\n",
      "cnt: 0 - valLoss: 0.42631959915161133 - trainLoss: 0.4004875421524048\n",
      "cnt: 0 - valLoss: 0.4263192117214203 - trainLoss: 0.40048637986183167\n",
      "cnt: 0 - valLoss: 0.4263189136981964 - trainLoss: 0.40048521757125854\n",
      "cnt: 0 - valLoss: 0.42631852626800537 - trainLoss: 0.40048402547836304\n",
      "cnt: 0 - valLoss: 0.4263181686401367 - trainLoss: 0.4004828929901123\n",
      "cnt: 0 - valLoss: 0.42631784081459045 - trainLoss: 0.4004817008972168\n",
      "cnt: 0 - valLoss: 0.4263174533843994 - trainLoss: 0.4004805386066437\n",
      "cnt: 0 - valLoss: 0.42631709575653076 - trainLoss: 0.40047940611839294\n",
      "cnt: 0 - valLoss: 0.426317036151886 - trainLoss: 0.4004782736301422\n",
      "cnt: 0 - valLoss: 0.42631658911705017 - trainLoss: 0.4004770815372467\n",
      "cnt: 0 - valLoss: 0.4263162612915039 - trainLoss: 0.40047597885131836\n",
      "cnt: 0 - valLoss: 0.42631590366363525 - trainLoss: 0.40047478675842285\n",
      "cnt: 0 - valLoss: 0.4263155162334442 - trainLoss: 0.40047362446784973\n",
      "cnt: 0 - valLoss: 0.42631518840789795 - trainLoss: 0.4004724621772766\n",
      "cnt: 0 - valLoss: 0.4263148009777069 - trainLoss: 0.40047135949134827\n",
      "cnt: 0 - valLoss: 0.42631444334983826 - trainLoss: 0.40047013759613037\n",
      "cnt: 0 - valLoss: 0.426314115524292 - trainLoss: 0.400469034910202\n",
      "cnt: 0 - valLoss: 0.42631375789642334 - trainLoss: 0.4004678428173065\n",
      "cnt: 0 - valLoss: 0.4263133406639099 - trainLoss: 0.4004666805267334\n",
      "cnt: 0 - valLoss: 0.42631304264068604 - trainLoss: 0.4004655182361603\n",
      "cnt: 0 - valLoss: 0.426312655210495 - trainLoss: 0.40046432614326477\n",
      "cnt: 0 - valLoss: 0.42631229758262634 - trainLoss: 0.4004632234573364\n",
      "cnt: 0 - valLoss: 0.42631199955940247 - trainLoss: 0.4004620611667633\n",
      "cnt: 0 - valLoss: 0.4263116717338562 - trainLoss: 0.4004608988761902\n",
      "cnt: 0 - valLoss: 0.42631128430366516 - trainLoss: 0.40045973658561707\n",
      "cnt: 0 - valLoss: 0.4263109266757965 - trainLoss: 0.40045860409736633\n",
      "cnt: 0 - valLoss: 0.42631053924560547 - trainLoss: 0.4004574418067932\n",
      "cnt: 0 - valLoss: 0.4263104498386383 - trainLoss: 0.4004562795162201\n",
      "cnt: 0 - valLoss: 0.42631012201309204 - trainLoss: 0.400455117225647\n",
      "cnt: 0 - valLoss: 0.426309734582901 - trainLoss: 0.40045398473739624\n",
      "cnt: 0 - valLoss: 0.42630940675735474 - trainLoss: 0.40045276284217834\n",
      "cnt: 0 - valLoss: 0.4263090491294861 - trainLoss: 0.40045166015625\n",
      "cnt: 0 - valLoss: 0.4263087213039398 - trainLoss: 0.4004504680633545\n",
      "cnt: 0 - valLoss: 0.4263083338737488 - trainLoss: 0.40044933557510376\n",
      "cnt: 0 - valLoss: 0.4263079762458801 - trainLoss: 0.40044814348220825\n",
      "cnt: 0 - valLoss: 0.42630764842033386 - trainLoss: 0.4004470407962799\n",
      "cnt: 0 - valLoss: 0.4263072609901428 - trainLoss: 0.4004458785057068\n",
      "cnt: 0 - valLoss: 0.42630690336227417 - trainLoss: 0.40044471621513367\n",
      "cnt: 0 - valLoss: 0.4263065755367279 - trainLoss: 0.40044352412223816\n",
      "cnt: 0 - valLoss: 0.42630618810653687 - trainLoss: 0.4004424214363098\n",
      "cnt: 0 - valLoss: 0.426305890083313 - trainLoss: 0.4004412293434143\n",
      "cnt: 0 - valLoss: 0.42630553245544434 - trainLoss: 0.4004400670528412\n",
      "cnt: 0 - valLoss: 0.4263051450252533 - trainLoss: 0.40043890476226807\n",
      "cnt: 0 - valLoss: 0.42630481719970703 - trainLoss: 0.40043774247169495\n",
      "cnt: 0 - valLoss: 0.4263044595718384 - trainLoss: 0.4004365801811218\n",
      "cnt: 0 - valLoss: 0.42630407214164734 - trainLoss: 0.4004354774951935\n",
      "cnt: 0 - valLoss: 0.4263039827346802 - trainLoss: 0.400434285402298\n",
      "cnt: 0 - valLoss: 0.4263036549091339 - trainLoss: 0.40043318271636963\n",
      "cnt: 0 - valLoss: 0.42630329728126526 - trainLoss: 0.40043196082115173\n",
      "cnt: 0 - valLoss: 0.426302969455719 - trainLoss: 0.400430828332901\n",
      "cnt: 0 - valLoss: 0.42630261182785034 - trainLoss: 0.4004296660423279\n",
      "cnt: 0 - valLoss: 0.4263022243976593 - trainLoss: 0.40042850375175476\n",
      "cnt: 0 - valLoss: 0.42630189657211304 - trainLoss: 0.40042734146118164\n",
      "cnt: 0 - valLoss: 0.4263015389442444 - trainLoss: 0.4004261791706085\n",
      "cnt: 0 - valLoss: 0.42630115151405334 - trainLoss: 0.4004250466823578\n",
      "cnt: 0 - valLoss: 0.4263008236885071 - trainLoss: 0.40042388439178467\n",
      "cnt: 0 - valLoss: 0.4263004660606384 - trainLoss: 0.40042272210121155\n",
      "cnt: 0 - valLoss: 0.42630013823509216 - trainLoss: 0.4004215598106384\n",
      "cnt: 0 - valLoss: 0.4262997806072235 - trainLoss: 0.4004204273223877\n",
      "cnt: 0 - valLoss: 0.42629939317703247 - trainLoss: 0.4004192054271698\n",
      "cnt: 0 - valLoss: 0.4262990951538086 - trainLoss: 0.40041810274124146\n",
      "cnt: 0 - valLoss: 0.42629876732826233 - trainLoss: 0.40041694045066833\n",
      "cnt: 0 - valLoss: 0.4262984097003937 - trainLoss: 0.4004157781600952\n",
      "cnt: 0 - valLoss: 0.4262980818748474 - trainLoss: 0.4004146456718445\n",
      "cnt: 0 - valLoss: 0.42629772424697876 - trainLoss: 0.40041348338127136\n",
      "cnt: 0 - valLoss: 0.4262973964214325 - trainLoss: 0.40041232109069824\n",
      "cnt: 0 - valLoss: 0.42629703879356384 - trainLoss: 0.4004111588001251\n",
      "cnt: 0 - valLoss: 0.4262967109680176 - trainLoss: 0.4004100263118744\n",
      "cnt: 0 - valLoss: 0.4262963533401489 - trainLoss: 0.40040886402130127\n",
      "cnt: 0 - valLoss: 0.42629626393318176 - trainLoss: 0.40040770173072815\n",
      "cnt: 0 - valLoss: 0.4262958765029907 - trainLoss: 0.40040653944015503\n",
      "cnt: 0 - valLoss: 0.42629557847976685 - trainLoss: 0.4004054367542267\n",
      "cnt: 0 - valLoss: 0.4262951910495758 - trainLoss: 0.4004042446613312\n",
      "cnt: 0 - valLoss: 0.42629486322402954 - trainLoss: 0.40040311217308044\n",
      "cnt: 0 - valLoss: 0.4262945055961609 - trainLoss: 0.40040192008018494\n",
      "cnt: 0 - valLoss: 0.42629414796829224 - trainLoss: 0.4004007577896118\n",
      "cnt: 0 - valLoss: 0.4262937903404236 - trainLoss: 0.4003996253013611\n",
      "cnt: 0 - valLoss: 0.4262934625148773 - trainLoss: 0.40039849281311035\n",
      "cnt: 0 - valLoss: 0.42629310488700867 - trainLoss: 0.40039730072021484\n",
      "cnt: 0 - valLoss: 0.4262927770614624 - trainLoss: 0.4003961980342865\n",
      "cnt: 0 - valLoss: 0.42629241943359375 - trainLoss: 0.4003949761390686\n",
      "cnt: 0 - valLoss: 0.4262920618057251 - trainLoss: 0.40039384365081787\n",
      "cnt: 0 - valLoss: 0.42629173398017883 - trainLoss: 0.40039268136024475\n",
      "cnt: 0 - valLoss: 0.4262913763523102 - trainLoss: 0.4003915786743164\n",
      "cnt: 0 - valLoss: 0.4262910485267639 - trainLoss: 0.4003903567790985\n",
      "cnt: 0 - valLoss: 0.42629075050354004 - trainLoss: 0.4003892242908478\n",
      "cnt: 0 - valLoss: 0.426290363073349 - trainLoss: 0.40038806200027466\n",
      "cnt: 0 - valLoss: 0.42629000544548035 - trainLoss: 0.4003869295120239\n",
      "cnt: 0 - valLoss: 0.4262896776199341 - trainLoss: 0.4003857970237732\n",
      "cnt: 0 - valLoss: 0.42628931999206543 - trainLoss: 0.4003846347332001\n",
      "cnt: 0 - valLoss: 0.42628929018974304 - trainLoss: 0.40038353204727173\n",
      "cnt: 0 - valLoss: 0.426288902759552 - trainLoss: 0.40038231015205383\n",
      "cnt: 0 - valLoss: 0.42628857493400574 - trainLoss: 0.4003811776638031\n",
      "cnt: 0 - valLoss: 0.4262882173061371 - trainLoss: 0.40038001537323\n",
      "cnt: 0 - valLoss: 0.4262878894805908 - trainLoss: 0.4003788232803345\n",
      "cnt: 0 - valLoss: 0.42628753185272217 - trainLoss: 0.40037769079208374\n",
      "cnt: 0 - valLoss: 0.4262872040271759 - trainLoss: 0.40037649869918823\n",
      "cnt: 0 - valLoss: 0.42628684639930725 - trainLoss: 0.4003753960132599\n",
      "cnt: 0 - valLoss: 0.426286518573761 - trainLoss: 0.40037423372268677\n",
      "cnt: 0 - valLoss: 0.42628616094589233 - trainLoss: 0.40037307143211365\n",
      "cnt: 0 - valLoss: 0.42628583312034607 - trainLoss: 0.4003719091415405\n",
      "cnt: 0 - valLoss: 0.4262854754924774 - trainLoss: 0.4003707766532898\n",
      "cnt: 0 - valLoss: 0.4262850880622864 - trainLoss: 0.40036964416503906\n",
      "cnt: 0 - valLoss: 0.4262847900390625 - trainLoss: 0.40036845207214355\n",
      "cnt: 0 - valLoss: 0.42628440260887146 - trainLoss: 0.4003673493862152\n",
      "cnt: 0 - valLoss: 0.4262841045856476 - trainLoss: 0.4003661572933197\n",
      "cnt: 0 - valLoss: 0.42628374695777893 - trainLoss: 0.40036502480506897\n",
      "cnt: 0 - valLoss: 0.42628341913223267 - trainLoss: 0.40036386251449585\n",
      "cnt: 0 - valLoss: 0.4262830317020416 - trainLoss: 0.4003627300262451\n",
      "cnt: 0 - valLoss: 0.42628273367881775 - trainLoss: 0.4003615081310272\n",
      "cnt: 0 - valLoss: 0.4262823760509491 - trainLoss: 0.4003604054450989\n",
      "cnt: 0 - valLoss: 0.42628204822540283 - trainLoss: 0.40035921335220337\n",
      "cnt: 0 - valLoss: 0.42628195881843567 - trainLoss: 0.40035808086395264\n",
      "cnt: 0 - valLoss: 0.4262816309928894 - trainLoss: 0.4003569483757019\n",
      "cnt: 0 - valLoss: 0.42628127336502075 - trainLoss: 0.4003557860851288\n",
      "cnt: 0 - valLoss: 0.4262809455394745 - trainLoss: 0.40035462379455566\n",
      "cnt: 0 - valLoss: 0.42628058791160583 - trainLoss: 0.40035346150398254\n",
      "cnt: 0 - valLoss: 0.4262802004814148 - trainLoss: 0.4003523290157318\n",
      "cnt: 0 - valLoss: 0.4262799024581909 - trainLoss: 0.4003511667251587\n",
      "cnt: 0 - valLoss: 0.42627957463264465 - trainLoss: 0.40035003423690796\n",
      "cnt: 0 - valLoss: 0.4262791872024536 - trainLoss: 0.40034884214401245\n",
      "cnt: 0 - valLoss: 0.42627888917922974 - trainLoss: 0.4003477096557617\n",
      "cnt: 0 - valLoss: 0.4262785315513611 - trainLoss: 0.4003465473651886\n",
      "cnt: 0 - valLoss: 0.4262782037258148 - trainLoss: 0.4003453850746155\n",
      "cnt: 0 - valLoss: 0.42627784609794617 - trainLoss: 0.40034428238868713\n",
      "cnt: 0 - valLoss: 0.4262775182723999 - trainLoss: 0.400343120098114\n",
      "cnt: 0 - valLoss: 0.42627716064453125 - trainLoss: 0.4003419578075409\n",
      "cnt: 0 - valLoss: 0.4262768030166626 - trainLoss: 0.4003407955169678\n",
      "cnt: 0 - valLoss: 0.4262765645980835 - trainLoss: 0.40033966302871704\n",
      "cnt: 0 - valLoss: 0.42627620697021484 - trainLoss: 0.4003385007381439\n",
      "cnt: 0 - valLoss: 0.4262758791446686 - trainLoss: 0.4003373682498932\n",
      "cnt: 0 - valLoss: 0.4262755215167999 - trainLoss: 0.4003361761569977\n",
      "cnt: 0 - valLoss: 0.42627519369125366 - trainLoss: 0.40033507347106934\n",
      "cnt: 0 - valLoss: 0.426274836063385 - trainLoss: 0.4003339111804962\n",
      "cnt: 0 - valLoss: 0.42627453804016113 - trainLoss: 0.4003327488899231\n",
      "cnt: 0 - valLoss: 0.42627421021461487 - trainLoss: 0.40033161640167236\n",
      "cnt: 0 - valLoss: 0.4262738525867462 - trainLoss: 0.40033045411109924\n",
      "cnt: 0 - valLoss: 0.42627352476119995 - trainLoss: 0.4003292918205261\n",
      "cnt: 0 - valLoss: 0.4262732267379761 - trainLoss: 0.400328129529953\n",
      "cnt: 0 - valLoss: 0.4262728691101074 - trainLoss: 0.4003269672393799\n",
      "cnt: 0 - valLoss: 0.42627254128456116 - trainLoss: 0.40032583475112915\n",
      "cnt: 0 - valLoss: 0.426272451877594 - trainLoss: 0.4003247022628784\n",
      "cnt: 0 - valLoss: 0.4262719750404358 - trainLoss: 0.4003235399723053\n",
      "cnt: 0 - valLoss: 0.42627155780792236 - trainLoss: 0.40032240748405457\n",
      "cnt: 0 - valLoss: 0.4262711703777313 - trainLoss: 0.40032127499580383\n",
      "cnt: 0 - valLoss: 0.426270991563797 - trainLoss: 0.4003200829029083\n",
      "cnt: 0 - valLoss: 0.42627057433128357 - trainLoss: 0.40031898021698\n",
      "cnt: 0 - valLoss: 0.42627018690109253 - trainLoss: 0.4003177881240845\n",
      "cnt: 0 - valLoss: 0.4262700080871582 - trainLoss: 0.40031665563583374\n",
      "cnt: 0 - valLoss: 0.42626953125 - trainLoss: 0.4003154933452606\n",
      "cnt: 0 - valLoss: 0.42626914381980896 - trainLoss: 0.4003143608570099\n",
      "cnt: 0 - valLoss: 0.42626893520355225 - trainLoss: 0.40031322836875916\n",
      "cnt: 0 - valLoss: 0.4262685477733612 - trainLoss: 0.4003120958805084\n",
      "cnt: 0 - valLoss: 0.4262681305408478 - trainLoss: 0.4003109335899353\n",
      "cnt: 0 - valLoss: 0.42626798152923584 - trainLoss: 0.4003097712993622\n",
      "cnt: 0 - valLoss: 0.4262675642967224 - trainLoss: 0.40030866861343384\n",
      "cnt: 0 - valLoss: 0.4262671172618866 - trainLoss: 0.4003075063228607\n",
      "cnt: 0 - valLoss: 0.42626699805259705 - trainLoss: 0.4003064036369324\n",
      "cnt: 0 - valLoss: 0.4262665808200836 - trainLoss: 0.4003051817417145\n",
      "cnt: 0 - valLoss: 0.4262661337852478 - trainLoss: 0.40030404925346375\n",
      "cnt: 0 - valLoss: 0.42626601457595825 - trainLoss: 0.4003028869628906\n",
      "cnt: 0 - valLoss: 0.4262655973434448 - trainLoss: 0.4003017544746399\n",
      "cnt: 0 - valLoss: 0.426265150308609 - trainLoss: 0.40030065178871155\n",
      "cnt: 0 - valLoss: 0.4262649416923523 - trainLoss: 0.40029945969581604\n",
      "cnt: 0 - valLoss: 0.42626455426216125 - trainLoss: 0.4002983570098877\n",
      "cnt: 0 - valLoss: 0.4262641370296478 - trainLoss: 0.40029722452163696\n",
      "cnt: 0 - valLoss: 0.4262639880180359 - trainLoss: 0.40029603242874146\n",
      "cnt: 0 - valLoss: 0.4262635409832001 - trainLoss: 0.40029487013816833\n",
      "cnt: 0 - valLoss: 0.42626315355300903 - trainLoss: 0.40029376745224\n",
      "cnt: 0 - valLoss: 0.4262630045413971 - trainLoss: 0.40029260516166687\n",
      "cnt: 0 - valLoss: 0.42626258730888367 - trainLoss: 0.40029144287109375\n",
      "cnt: 0 - valLoss: 0.42626217007637024 - trainLoss: 0.400290310382843\n",
      "cnt: 0 - valLoss: 0.4262620210647583 - trainLoss: 0.4002891778945923\n",
      "cnt: 0 - valLoss: 0.4262616038322449 - trainLoss: 0.40028807520866394\n",
      "cnt: 0 - valLoss: 0.42626118659973145 - trainLoss: 0.40028688311576843\n",
      "cnt: 0 - valLoss: 0.4262610375881195 - trainLoss: 0.4002857506275177\n",
      "cnt: 0 - valLoss: 0.42626065015792847 - trainLoss: 0.4002845585346222\n",
      "cnt: 0 - valLoss: 0.42626017332077026 - trainLoss: 0.40028345584869385\n",
      "cnt: 0 - valLoss: 0.4262601137161255 - trainLoss: 0.4002822935581207\n",
      "cnt: 0 - valLoss: 0.4262596666812897 - trainLoss: 0.4002811908721924\n",
      "cnt: 0 - valLoss: 0.42625924944877625 - trainLoss: 0.40028002858161926\n",
      "cnt: 0 - valLoss: 0.4262591302394867 - trainLoss: 0.40027886629104614\n",
      "cnt: 0 - valLoss: 0.42625874280929565 - trainLoss: 0.400277704000473\n",
      "cnt: 0 - valLoss: 0.42625826597213745 - trainLoss: 0.4002765715122223\n",
      "cnt: 0 - valLoss: 0.4262581169605255 - trainLoss: 0.40027543902397156\n",
      "cnt: 0 - valLoss: 0.4262576997280121 - trainLoss: 0.40027427673339844\n",
      "cnt: 0 - valLoss: 0.42625758051872253 - trainLoss: 0.4002731442451477\n",
      "cnt: 0 - valLoss: 0.4262571930885315 - trainLoss: 0.4002719819545746\n",
      "cnt: 0 - valLoss: 0.4262567162513733 - trainLoss: 0.40027087926864624\n",
      "cnt: 0 - valLoss: 0.42625632882118225 - trainLoss: 0.4002697169780731\n",
      "cnt: 0 - valLoss: 0.4262562096118927 - trainLoss: 0.4002685844898224\n",
      "cnt: 0 - valLoss: 0.4262557625770569 - trainLoss: 0.40026745200157166\n",
      "cnt: 0 - valLoss: 0.42625537514686584 - trainLoss: 0.40026623010635376\n",
      "cnt: 0 - valLoss: 0.4262552559375763 - trainLoss: 0.4002651274204254\n",
      "cnt: 0 - valLoss: 0.4262547791004181 - trainLoss: 0.40026402473449707\n",
      "cnt: 0 - valLoss: 0.42625439167022705 - trainLoss: 0.40026286244392395\n",
      "cnt: 0 - valLoss: 0.4262543022632599 - trainLoss: 0.40026170015335083\n",
      "cnt: 0 - valLoss: 0.42625388503074646 - trainLoss: 0.4002605974674225\n",
      "cnt: 0 - valLoss: 0.42625346779823303 - trainLoss: 0.40025943517684937\n",
      "cnt: 0 - valLoss: 0.4262533187866211 - trainLoss: 0.40025827288627625\n",
      "cnt: 0 - valLoss: 0.42625293135643005 - trainLoss: 0.4002571403980255\n",
      "cnt: 0 - valLoss: 0.426252543926239 - trainLoss: 0.4002559781074524\n",
      "cnt: 0 - valLoss: 0.4262523949146271 - trainLoss: 0.40025484561920166\n",
      "cnt: 0 - valLoss: 0.42625194787979126 - trainLoss: 0.40025365352630615\n",
      "cnt: 0 - valLoss: 0.426251620054245 - trainLoss: 0.4002525508403778\n",
      "cnt: 0 - valLoss: 0.42625144124031067 - trainLoss: 0.40025144815444946\n",
      "cnt: 0 - valLoss: 0.42625102400779724 - trainLoss: 0.40025028586387634\n",
      "cnt: 0 - valLoss: 0.4262506663799286 - trainLoss: 0.4002491235733032\n",
      "cnt: 0 - valLoss: 0.42625024914741516 - trainLoss: 0.4002479612827301\n",
      "cnt: 0 - valLoss: 0.4262501001358032 - trainLoss: 0.40024685859680176\n",
      "cnt: 0 - valLoss: 0.42624974250793457 - trainLoss: 0.40024569630622864\n",
      "cnt: 0 - valLoss: 0.42624935507774353 - trainLoss: 0.4002445936203003\n",
      "cnt: 0 - valLoss: 0.4262492060661316 - trainLoss: 0.4002434015274048\n",
      "cnt: 0 - valLoss: 0.42624878883361816 - trainLoss: 0.40024226903915405\n",
      "cnt: 0 - valLoss: 0.4262484312057495 - trainLoss: 0.40024110674858093\n",
      "cnt: 0 - valLoss: 0.4262482821941376 - trainLoss: 0.4002399742603302\n",
      "cnt: 0 - valLoss: 0.42624783515930176 - trainLoss: 0.40023884177207947\n",
      "cnt: 0 - valLoss: 0.4262474477291107 - trainLoss: 0.40023770928382874\n",
      "cnt: 0 - valLoss: 0.42624732851982117 - trainLoss: 0.400236576795578\n",
      "cnt: 0 - valLoss: 0.4262469410896301 - trainLoss: 0.4002354145050049\n",
      "cnt: 0 - valLoss: 0.4262465536594391 - trainLoss: 0.40023428201675415\n",
      "cnt: 0 - valLoss: 0.4262462258338928 - trainLoss: 0.40023311972618103\n",
      "cnt: 0 - valLoss: 0.4262460470199585 - trainLoss: 0.4002319872379303\n",
      "cnt: 0 - valLoss: 0.4262455999851227 - trainLoss: 0.40023085474967957\n",
      "cnt: 0 - valLoss: 0.42624524235725403 - trainLoss: 0.40022969245910645\n",
      "cnt: 0 - valLoss: 0.4262450933456421 - trainLoss: 0.4002285897731781\n",
      "cnt: 0 - valLoss: 0.4262447655200958 - trainLoss: 0.400227427482605\n",
      "cnt: 0 - valLoss: 0.42624431848526 - trainLoss: 0.40022626519203186\n",
      "cnt: 0 - valLoss: 0.42624416947364807 - trainLoss: 0.4002251625061035\n",
      "cnt: 0 - valLoss: 0.4262438118457794 - trainLoss: 0.4002240002155304\n",
      "cnt: 0 - valLoss: 0.4262434244155884 - trainLoss: 0.4002228379249573\n",
      "cnt: 0 - valLoss: 0.42624300718307495 - trainLoss: 0.40022173523902893\n",
      "cnt: 0 - valLoss: 0.426242858171463 - trainLoss: 0.4002205729484558\n",
      "cnt: 0 - valLoss: 0.42624253034591675 - trainLoss: 0.4002194106578827\n",
      "cnt: 0 - valLoss: 0.4262421131134033 - trainLoss: 0.40021824836730957\n",
      "cnt: 0 - valLoss: 0.426241934299469 - trainLoss: 0.4002171456813812\n",
      "cnt: 0 - valLoss: 0.42624154686927795 - trainLoss: 0.4002159833908081\n",
      "cnt: 0 - valLoss: 0.4262411594390869 - trainLoss: 0.40021488070487976\n",
      "cnt: 0 - valLoss: 0.4262407720088959 - trainLoss: 0.40021368861198425\n",
      "cnt: 0 - valLoss: 0.4262406527996063 - trainLoss: 0.4002125561237335\n",
      "cnt: 0 - valLoss: 0.42624029517173767 - trainLoss: 0.4002114534378052\n",
      "cnt: 0 - valLoss: 0.42623987793922424 - trainLoss: 0.40021026134490967\n",
      "cnt: 0 - valLoss: 0.4262397885322571 - trainLoss: 0.4002091586589813\n",
      "cnt: 0 - valLoss: 0.4262393116950989 - trainLoss: 0.4002079963684082\n",
      "cnt: 0 - valLoss: 0.426239013671875 - trainLoss: 0.40020686388015747\n",
      "cnt: 0 - valLoss: 0.4262385964393616 - trainLoss: 0.40020573139190674\n",
      "cnt: 0 - valLoss: 0.42623844742774963 - trainLoss: 0.4002045691013336\n",
      "cnt: 0 - valLoss: 0.4262380599975586 - trainLoss: 0.4002034068107605\n",
      "cnt: 0 - valLoss: 0.42623767256736755 - trainLoss: 0.40020230412483215\n",
      "cnt: 0 - valLoss: 0.426237553358078 - trainLoss: 0.40020114183425903\n",
      "cnt: 0 - valLoss: 0.42623716592788696 - trainLoss: 0.4001999795436859\n",
      "cnt: 0 - valLoss: 0.4262367784976959 - trainLoss: 0.40019887685775757\n",
      "cnt: 0 - valLoss: 0.4262363910675049 - trainLoss: 0.4001977741718292\n",
      "cnt: 0 - valLoss: 0.42623627185821533 - trainLoss: 0.4001966118812561\n",
      "cnt: 0 - valLoss: 0.4262358546257019 - trainLoss: 0.400195449590683\n",
      "cnt: 0 - valLoss: 0.42623549699783325 - trainLoss: 0.40019428730010986\n",
      "cnt: 0 - valLoss: 0.4262353777885437 - trainLoss: 0.4001931846141815\n",
      "cnt: 0 - valLoss: 0.42623499035835266 - trainLoss: 0.4001920223236084\n",
      "cnt: 0 - valLoss: 0.4262346029281616 - trainLoss: 0.40019091963768005\n",
      "cnt: 0 - valLoss: 0.42623448371887207 - trainLoss: 0.40018972754478455\n",
      "cnt: 0 - valLoss: 0.4262341260910034 - trainLoss: 0.4001885950565338\n",
      "cnt: 0 - valLoss: 0.42623370885849 - trainLoss: 0.40018749237060547\n",
      "cnt: 0 - valLoss: 0.4262332618236542 - trainLoss: 0.40018633008003235\n",
      "cnt: 0 - valLoss: 0.426233172416687 - trainLoss: 0.40018516778945923\n",
      "cnt: 0 - valLoss: 0.42623278498649597 - trainLoss: 0.4001840651035309\n",
      "cnt: 0 - valLoss: 0.4262324273586273 - trainLoss: 0.40018290281295776\n",
      "cnt: 0 - valLoss: 0.42623233795166016 - trainLoss: 0.40018174052238464\n",
      "cnt: 0 - valLoss: 0.4262319505214691 - trainLoss: 0.4001806378364563\n",
      "cnt: 0 - valLoss: 0.4262315630912781 - trainLoss: 0.4001794755458832\n",
      "cnt: 0 - valLoss: 0.42623111605644226 - trainLoss: 0.40017834305763245\n",
      "cnt: 0 - valLoss: 0.4262310266494751 - trainLoss: 0.4001771807670593\n",
      "cnt: 0 - valLoss: 0.42623066902160645 - trainLoss: 0.4001760482788086\n",
      "cnt: 0 - valLoss: 0.4262302815914154 - trainLoss: 0.40017491579055786\n",
      "cnt: 0 - valLoss: 0.42623013257980347 - trainLoss: 0.40017378330230713\n",
      "cnt: 0 - valLoss: 0.4262297451496124 - trainLoss: 0.4001726806163788\n",
      "cnt: 0 - valLoss: 0.42622941732406616 - trainLoss: 0.40017151832580566\n",
      "cnt: 0 - valLoss: 0.42622897028923035 - trainLoss: 0.40017035603523254\n",
      "cnt: 0 - valLoss: 0.42622891068458557 - trainLoss: 0.4001692235469818\n",
      "cnt: 0 - valLoss: 0.42622846364974976 - trainLoss: 0.4001680612564087\n",
      "cnt: 0 - valLoss: 0.4262281358242035 - trainLoss: 0.40016692876815796\n",
      "cnt: 0 - valLoss: 0.42622795701026917 - trainLoss: 0.4001658260822296\n",
      "cnt: 0 - valLoss: 0.4262275993824005 - trainLoss: 0.4001646339893341\n",
      "cnt: 0 - valLoss: 0.4262271821498871 - trainLoss: 0.40016353130340576\n",
      "cnt: 0 - valLoss: 0.4262268543243408 - trainLoss: 0.40016239881515503\n",
      "cnt: 0 - valLoss: 0.42622676491737366 - trainLoss: 0.4001612365245819\n",
      "cnt: 0 - valLoss: 0.4262263774871826 - trainLoss: 0.4001601040363312\n",
      "cnt: 0 - valLoss: 0.42622601985931396 - trainLoss: 0.40015897154808044\n",
      "cnt: 0 - valLoss: 0.42622560262680054 - trainLoss: 0.4001578390598297\n",
      "cnt: 0 - valLoss: 0.426225483417511 - trainLoss: 0.4001566767692566\n",
      "cnt: 0 - valLoss: 0.42622512578964233 - trainLoss: 0.40015554428100586\n",
      "cnt: 0 - valLoss: 0.42622479796409607 - trainLoss: 0.4001544117927551\n",
      "cnt: 0 - valLoss: 0.42622461915016174 - trainLoss: 0.4001532793045044\n",
      "cnt: 0 - valLoss: 0.4262242317199707 - trainLoss: 0.40015214681625366\n",
      "cnt: 0 - valLoss: 0.42622384428977966 - trainLoss: 0.40015101432800293\n",
      "cnt: 0 - valLoss: 0.4262237548828125 - trainLoss: 0.4001498520374298\n",
      "cnt: 0 - valLoss: 0.42622339725494385 - trainLoss: 0.40014874935150146\n",
      "cnt: 0 - valLoss: 0.4262229800224304 - trainLoss: 0.40014758706092834\n",
      "cnt: 0 - valLoss: 0.42622265219688416 - trainLoss: 0.4001464545726776\n",
      "cnt: 0 - valLoss: 0.426222562789917 - trainLoss: 0.4001453220844269\n",
      "cnt: 0 - valLoss: 0.4262221157550812 - trainLoss: 0.40014415979385376\n",
      "cnt: 0 - valLoss: 0.4262217879295349 - trainLoss: 0.4001430571079254\n",
      "cnt: 0 - valLoss: 0.4262213408946991 - trainLoss: 0.40014195442199707\n",
      "cnt: 0 - valLoss: 0.4262212812900543 - trainLoss: 0.40014079213142395\n",
      "cnt: 0 - valLoss: 0.4262208938598633 - trainLoss: 0.4001396894454956\n",
      "cnt: 0 - valLoss: 0.42622050642967224 - trainLoss: 0.4001385271549225\n",
      "cnt: 0 - valLoss: 0.4262203574180603 - trainLoss: 0.40013736486434937\n",
      "cnt: 0 - valLoss: 0.4262200593948364 - trainLoss: 0.40013620257377625\n",
      "cnt: 0 - valLoss: 0.4262196719646454 - trainLoss: 0.4001350998878479\n",
      "cnt: 0 - valLoss: 0.42621928453445435 - trainLoss: 0.40013399720191956\n",
      "cnt: 0 - valLoss: 0.42621922492980957 - trainLoss: 0.40013277530670166\n",
      "cnt: 0 - valLoss: 0.42621877789497375 - trainLoss: 0.4001316726207733\n",
      "cnt: 0 - valLoss: 0.4262184500694275 - trainLoss: 0.40013056993484497\n",
      "cnt: 0 - valLoss: 0.4262180030345917 - trainLoss: 0.40012940764427185\n",
      "cnt: 0 - valLoss: 0.4262179732322693 - trainLoss: 0.4001283049583435\n",
      "cnt: 0 - valLoss: 0.42621758580207825 - trainLoss: 0.4001271426677704\n",
      "cnt: 0 - valLoss: 0.4262171685695648 - trainLoss: 0.40012603998184204\n",
      "cnt: 0 - valLoss: 0.42621707916259766 - trainLoss: 0.4001248776912689\n",
      "cnt: 0 - valLoss: 0.42621663212776184 - trainLoss: 0.4001237154006958\n",
      "cnt: 0 - valLoss: 0.4262163043022156 - trainLoss: 0.40012261271476746\n",
      "cnt: 0 - valLoss: 0.4262159466743469 - trainLoss: 0.40012145042419434\n",
      "cnt: 0 - valLoss: 0.4262157678604126 - trainLoss: 0.4001203179359436\n",
      "cnt: 0 - valLoss: 0.42621544003486633 - trainLoss: 0.40011918544769287\n",
      "cnt: 0 - valLoss: 0.4262150824069977 - trainLoss: 0.40011805295944214\n",
      "cnt: 0 - valLoss: 0.42621496319770813 - trainLoss: 0.4001169502735138\n",
      "cnt: 0 - valLoss: 0.4262145757675171 - trainLoss: 0.4001157879829407\n",
      "cnt: 0 - valLoss: 0.42621418833732605 - trainLoss: 0.40011462569236755\n",
      "cnt: 0 - valLoss: 0.4262138605117798 - trainLoss: 0.4001135230064392\n",
      "cnt: 0 - valLoss: 0.42621371150016785 - trainLoss: 0.4001123607158661\n",
      "cnt: 0 - valLoss: 0.4262132942676544 - trainLoss: 0.40011125802993774\n",
      "cnt: 0 - valLoss: 0.42621296644210815 - trainLoss: 0.4001100957393646\n",
      "cnt: 0 - valLoss: 0.426212877035141 - trainLoss: 0.4001089930534363\n",
      "cnt: 0 - valLoss: 0.42621248960494995 - trainLoss: 0.40010783076286316\n",
      "cnt: 0 - valLoss: 0.4262121021747589 - trainLoss: 0.40010666847229004\n",
      "cnt: 0 - valLoss: 0.42621174454689026 - trainLoss: 0.4001055657863617\n",
      "cnt: 0 - valLoss: 0.4262116253376007 - trainLoss: 0.40010446310043335\n",
      "cnt: 0 - valLoss: 0.42621123790740967 - trainLoss: 0.4001033306121826\n",
      "cnt: 0 - valLoss: 0.42621085047721863 - trainLoss: 0.4001021385192871\n",
      "cnt: 0 - valLoss: 0.4262107312679291 - trainLoss: 0.40010103583335876\n",
      "cnt: 0 - valLoss: 0.4262103736400604 - trainLoss: 0.40009987354278564\n",
      "cnt: 0 - valLoss: 0.42621004581451416 - trainLoss: 0.4000987708568573\n",
      "cnt: 0 - valLoss: 0.42620959877967834 - trainLoss: 0.4000976085662842\n",
      "cnt: 0 - valLoss: 0.42620953917503357 - trainLoss: 0.40009650588035583\n",
      "cnt: 0 - valLoss: 0.42620915174484253 - trainLoss: 0.4000953733921051\n",
      "cnt: 0 - valLoss: 0.4262087941169739 - trainLoss: 0.400094211101532\n",
      "cnt: 0 - valLoss: 0.42620840668678284 - trainLoss: 0.40009307861328125\n",
      "cnt: 0 - valLoss: 0.4262082874774933 - trainLoss: 0.4000919461250305\n",
      "cnt: 0 - valLoss: 0.42620790004730225 - trainLoss: 0.4000908434391022\n",
      "cnt: 0 - valLoss: 0.4262075424194336 - trainLoss: 0.40008968114852905\n",
      "cnt: 0 - valLoss: 0.42620745301246643 - trainLoss: 0.4000885784626007\n",
      "cnt: 0 - valLoss: 0.426207035779953 - trainLoss: 0.4000874161720276\n",
      "cnt: 0 - valLoss: 0.42620670795440674 - trainLoss: 0.40008625388145447\n",
      "cnt: 0 - valLoss: 0.4262063503265381 - trainLoss: 0.4000851511955261\n",
      "cnt: 0 - valLoss: 0.4262062609195709 - trainLoss: 0.400083988904953\n",
      "cnt: 0 - valLoss: 0.4262058436870575 - trainLoss: 0.40008288621902466\n",
      "cnt: 0 - valLoss: 0.42620545625686646 - trainLoss: 0.4000817537307739\n",
      "cnt: 0 - valLoss: 0.4262051582336426 - trainLoss: 0.4000806212425232\n",
      "cnt: 0 - valLoss: 0.4262050688266754 - trainLoss: 0.4000794589519501\n",
      "cnt: 0 - valLoss: 0.426204651594162 - trainLoss: 0.40007835626602173\n",
      "cnt: 0 - valLoss: 0.42620426416397095 - trainLoss: 0.4000771939754486\n",
      "cnt: 0 - valLoss: 0.42620420455932617 - trainLoss: 0.4000760614871979\n",
      "cnt: 0 - valLoss: 0.42620381712913513 - trainLoss: 0.40007492899894714\n",
      "cnt: 0 - valLoss: 0.4262034296989441 - trainLoss: 0.4000737965106964\n",
      "cnt: 0 - valLoss: 0.42620304226875305 - trainLoss: 0.40007269382476807\n",
      "cnt: 0 - valLoss: 0.42620301246643066 - trainLoss: 0.40007153153419495\n",
      "cnt: 0 - valLoss: 0.4262026250362396 - trainLoss: 0.4000704288482666\n",
      "cnt: 0 - valLoss: 0.4262022376060486 - trainLoss: 0.4000692665576935\n",
      "cnt: 0 - valLoss: 0.42620185017585754 - trainLoss: 0.40006816387176514\n",
      "cnt: 0 - valLoss: 0.4262017607688904 - trainLoss: 0.40006697177886963\n",
      "cnt: 0 - valLoss: 0.42620137333869934 - trainLoss: 0.4000658392906189\n",
      "cnt: 0 - valLoss: 0.4262010455131531 - trainLoss: 0.40006473660469055\n",
      "cnt: 0 - valLoss: 0.42620065808296204 - trainLoss: 0.4000636339187622\n",
      "cnt: 0 - valLoss: 0.4262005686759949 - trainLoss: 0.4000624716281891\n",
      "cnt: 0 - valLoss: 0.42620018124580383 - trainLoss: 0.40006130933761597\n",
      "cnt: 0 - valLoss: 0.4261998236179352 - trainLoss: 0.4000602066516876\n",
      "cnt: 0 - valLoss: 0.42619946599006653 - trainLoss: 0.4000590741634369\n",
      "cnt: 0 - valLoss: 0.42619937658309937 - trainLoss: 0.40005794167518616\n",
      "cnt: 0 - valLoss: 0.4261989891529083 - trainLoss: 0.4000568091869354\n",
      "cnt: 0 - valLoss: 0.42619872093200684 - trainLoss: 0.4000556468963623\n",
      "cnt: 0 - valLoss: 0.4261983036994934 - trainLoss: 0.40005454421043396\n",
      "cnt: 0 - valLoss: 0.42619821429252625 - trainLoss: 0.4000534415245056\n",
      "cnt: 0 - valLoss: 0.4261978566646576 - trainLoss: 0.4000522494316101\n",
      "cnt: 0 - valLoss: 0.42619752883911133 - trainLoss: 0.4000511169433594\n",
      "cnt: 0 - valLoss: 0.42619743943214417 - trainLoss: 0.40005001425743103\n",
      "cnt: 0 - valLoss: 0.42619702219963074 - trainLoss: 0.4000488519668579\n",
      "cnt: 0 - valLoss: 0.4261966645717621 - trainLoss: 0.40004774928092957\n",
      "cnt: 0 - valLoss: 0.42619627714157104 - trainLoss: 0.40004658699035645\n",
      "cnt: 0 - valLoss: 0.42619624733924866 - trainLoss: 0.4000454843044281\n",
      "cnt: 0 - valLoss: 0.4261958599090576 - trainLoss: 0.40004435181617737\n",
      "cnt: 0 - valLoss: 0.4261954724788666 - trainLoss: 0.40004318952560425\n",
      "cnt: 0 - valLoss: 0.42619508504867554 - trainLoss: 0.4000420570373535\n",
      "cnt: 0 - valLoss: 0.42619502544403076 - trainLoss: 0.4000409245491028\n",
      "cnt: 0 - valLoss: 0.4261946380138397 - trainLoss: 0.40003979206085205\n",
      "cnt: 0 - valLoss: 0.42619428038597107 - trainLoss: 0.40003862977027893\n",
      "cnt: 0 - valLoss: 0.4261939227581024 - trainLoss: 0.4000374972820282\n",
      "cnt: 0 - valLoss: 0.42619383335113525 - trainLoss: 0.40003639459609985\n",
      "cnt: 0 - valLoss: 0.4261934161186218 - trainLoss: 0.4000352919101715\n",
      "cnt: 0 - valLoss: 0.42619314789772034 - trainLoss: 0.4000341296195984\n",
      "cnt: 0 - valLoss: 0.4261930286884308 - trainLoss: 0.40003302693367004\n",
      "cnt: 0 - valLoss: 0.42619264125823975 - trainLoss: 0.4000318646430969\n",
      "cnt: 0 - valLoss: 0.4261922836303711 - trainLoss: 0.4000307619571686\n",
      "cnt: 0 - valLoss: 0.42619195580482483 - trainLoss: 0.40002959966659546\n",
      "cnt: 0 - valLoss: 0.42619186639785767 - trainLoss: 0.40002843737602234\n",
      "cnt: 0 - valLoss: 0.42619144916534424 - trainLoss: 0.4000273644924164\n",
      "cnt: 0 - valLoss: 0.4261910915374756 - trainLoss: 0.40002620220184326\n",
      "cnt: 0 - valLoss: 0.4261907637119293 - trainLoss: 0.4000250995159149\n",
      "cnt: 0 - valLoss: 0.42619067430496216 - trainLoss: 0.4000239074230194\n",
      "cnt: 0 - valLoss: 0.4261903166770935 - trainLoss: 0.40002283453941345\n",
      "cnt: 0 - valLoss: 0.42618998885154724 - trainLoss: 0.40002167224884033\n",
      "cnt: 0 - valLoss: 0.4261896014213562 - trainLoss: 0.400020569562912\n",
      "cnt: 0 - valLoss: 0.42618951201438904 - trainLoss: 0.40001940727233887\n",
      "cnt: 0 - valLoss: 0.42618921399116516 - trainLoss: 0.4000183045864105\n",
      "cnt: 0 - valLoss: 0.4261888265609741 - trainLoss: 0.4000171422958374\n",
      "cnt: 0 - valLoss: 0.4261884391307831 - trainLoss: 0.40001603960990906\n",
      "cnt: 0 - valLoss: 0.4261883795261383 - trainLoss: 0.40001487731933594\n",
      "cnt: 0 - valLoss: 0.42618802189826965 - trainLoss: 0.40001380443573\n",
      "cnt: 0 - valLoss: 0.426187664270401 - trainLoss: 0.4000126123428345\n",
      "cnt: 0 - valLoss: 0.42618733644485474 - trainLoss: 0.40001147985458374\n",
      "cnt: 0 - valLoss: 0.4261872470378876 - trainLoss: 0.4000103771686554\n",
      "cnt: 0 - valLoss: 0.42618680000305176 - trainLoss: 0.40000927448272705\n",
      "cnt: 0 - valLoss: 0.4261865019798279 - trainLoss: 0.40000805258750916\n",
      "cnt: 0 - valLoss: 0.4261861741542816 - trainLoss: 0.4000069797039032\n",
      "cnt: 0 - valLoss: 0.42618608474731445 - trainLoss: 0.40000584721565247\n",
      "cnt: 0 - valLoss: 0.4261856973171234 - trainLoss: 0.40000471472740173\n",
      "cnt: 0 - valLoss: 0.42618533968925476 - trainLoss: 0.400003582239151\n",
      "cnt: 0 - valLoss: 0.4261850416660309 - trainLoss: 0.40000244975090027\n",
      "cnt: 0 - valLoss: 0.4261849522590637 - trainLoss: 0.40000128746032715\n",
      "cnt: 0 - valLoss: 0.4261845648288727 - trainLoss: 0.4000001847743988\n",
      "cnt: 0 - valLoss: 0.42618420720100403 - trainLoss: 0.3999990224838257\n",
      "cnt: 0 - valLoss: 0.42618387937545776 - trainLoss: 0.39999788999557495\n",
      "cnt: 0 - valLoss: 0.426183819770813 - trainLoss: 0.3999967575073242\n",
      "cnt: 0 - valLoss: 0.42618343234062195 - trainLoss: 0.3999956548213959\n",
      "cnt: 0 - valLoss: 0.4261830747127533 - trainLoss: 0.39999455213546753\n",
      "cnt: 0 - valLoss: 0.4261827766895294 - trainLoss: 0.3999934196472168\n",
      "cnt: 0 - valLoss: 0.42618244886398315 - trainLoss: 0.3999922573566437\n",
      "cnt: 0 - valLoss: 0.4261822998523712 - trainLoss: 0.39999112486839294\n",
      "cnt: 0 - valLoss: 0.4261819124221802 - trainLoss: 0.3999899923801422\n",
      "cnt: 0 - valLoss: 0.4261815845966339 - trainLoss: 0.39998888969421387\n",
      "cnt: 0 - valLoss: 0.4261815845966339 - trainLoss: 0.39998772740364075\n",
      "cnt: 0 - valLoss: 0.42618119716644287 - trainLoss: 0.3999866247177124\n",
      "cnt: 0 - valLoss: 0.4261808395385742 - trainLoss: 0.39998552203178406\n",
      "cnt: 0 - valLoss: 0.4261804521083832 - trainLoss: 0.39998435974121094\n",
      "cnt: 0 - valLoss: 0.4261804223060608 - trainLoss: 0.3999832272529602\n",
      "cnt: 0 - valLoss: 0.42618003487586975 - trainLoss: 0.3999820947647095\n",
      "cnt: 0 - valLoss: 0.4261797070503235 - trainLoss: 0.39998096227645874\n",
      "cnt: 0 - valLoss: 0.42617934942245483 - trainLoss: 0.3999797999858856\n",
      "cnt: 0 - valLoss: 0.42617902159690857 - trainLoss: 0.3999786972999573\n",
      "cnt: 0 - valLoss: 0.4261789619922638 - trainLoss: 0.39997759461402893\n",
      "cnt: 0 - valLoss: 0.42617857456207275 - trainLoss: 0.3999764621257782\n",
      "cnt: 0 - valLoss: 0.4261782467365265 - trainLoss: 0.39997532963752747\n",
      "cnt: 0 - valLoss: 0.42617785930633545 - trainLoss: 0.39997416734695435\n",
      "cnt: 0 - valLoss: 0.4261777997016907 - trainLoss: 0.399973064661026\n",
      "cnt: 0 - valLoss: 0.4261774718761444 - trainLoss: 0.39997193217277527\n",
      "cnt: 0 - valLoss: 0.42617711424827576 - trainLoss: 0.3999708294868469\n",
      "cnt: 0 - valLoss: 0.4261767268180847 - trainLoss: 0.3999696671962738\n",
      "cnt: 0 - valLoss: 0.42617669701576233 - trainLoss: 0.3999685049057007\n",
      "cnt: 0 - valLoss: 0.4261762499809265 - trainLoss: 0.39996740221977234\n",
      "cnt: 0 - valLoss: 0.42617595195770264 - trainLoss: 0.399966299533844\n",
      "cnt: 0 - valLoss: 0.42617562413215637 - trainLoss: 0.3999651372432709\n",
      "cnt: 0 - valLoss: 0.4261755049228668 - trainLoss: 0.39996403455734253\n",
      "cnt: 0 - valLoss: 0.42617514729499817 - trainLoss: 0.3999628722667694\n",
      "cnt: 0 - valLoss: 0.4261748194694519 - trainLoss: 0.3999617397785187\n",
      "cnt: 0 - valLoss: 0.426174521446228 - trainLoss: 0.39996063709259033\n",
      "cnt: 0 - valLoss: 0.42617443203926086 - trainLoss: 0.399959534406662\n",
      "cnt: 0 - valLoss: 0.4261740446090698 - trainLoss: 0.3999583423137665\n",
      "cnt: 0 - valLoss: 0.42617368698120117 - trainLoss: 0.39995720982551575\n",
      "cnt: 0 - valLoss: 0.4261733889579773 - trainLoss: 0.3999561071395874\n",
      "cnt: 0 - valLoss: 0.42617300152778625 - trainLoss: 0.39995500445365906\n",
      "cnt: 0 - valLoss: 0.4261729121208191 - trainLoss: 0.3999538719654083\n",
      "cnt: 0 - valLoss: 0.4261725842952728 - trainLoss: 0.3999527096748352\n",
      "cnt: 0 - valLoss: 0.4261722266674042 - trainLoss: 0.39995160698890686\n",
      "cnt: 0 - valLoss: 0.4261722266674042 - trainLoss: 0.39995044469833374\n",
      "cnt: 0 - valLoss: 0.42617180943489075 - trainLoss: 0.3999493420124054\n",
      "cnt: 0 - valLoss: 0.4261714518070221 - trainLoss: 0.39994823932647705\n",
      "cnt: 0 - valLoss: 0.42617112398147583 - trainLoss: 0.39994707703590393\n",
      "cnt: 0 - valLoss: 0.4261707663536072 - trainLoss: 0.3999459445476532\n",
      "cnt: 0 - valLoss: 0.4261707067489624 - trainLoss: 0.39994481205940247\n",
      "cnt: 0 - valLoss: 0.42617034912109375 - trainLoss: 0.39994364976882935\n",
      "cnt: 0 - valLoss: 0.4261699914932251 - trainLoss: 0.3999425768852234\n",
      "cnt: 0 - valLoss: 0.42616966366767883 - trainLoss: 0.39994141459465027\n",
      "cnt: 0 - valLoss: 0.42616957426071167 - trainLoss: 0.3999403119087219\n",
      "cnt: 0 - valLoss: 0.4261692464351654 - trainLoss: 0.3999392092227936\n",
      "cnt: 0 - valLoss: 0.42616888880729675 - trainLoss: 0.39993804693222046\n",
      "cnt: 0 - valLoss: 0.4261685609817505 - trainLoss: 0.3999369144439697\n",
      "cnt: 0 - valLoss: 0.4261684715747833 - trainLoss: 0.399935781955719\n",
      "cnt: 0 - valLoss: 0.4261681139469147 - trainLoss: 0.39993464946746826\n",
      "cnt: 0 - valLoss: 0.4261677861213684 - trainLoss: 0.39993351697921753\n",
      "cnt: 0 - valLoss: 0.42616742849349976 - trainLoss: 0.3999323844909668\n",
      "cnt: 0 - valLoss: 0.4261673092842102 - trainLoss: 0.39993128180503845\n",
      "cnt: 0 - valLoss: 0.42616695165634155 - trainLoss: 0.39993011951446533\n",
      "cnt: 0 - valLoss: 0.4261666238307953 - trainLoss: 0.3999290466308594\n",
      "cnt: 0 - valLoss: 0.4261663258075714 - trainLoss: 0.39992788434028625\n",
      "cnt: 0 - valLoss: 0.42616596817970276 - trainLoss: 0.3999267816543579\n",
      "cnt: 0 - valLoss: 0.4261658489704132 - trainLoss: 0.3999256193637848\n",
      "cnt: 0 - valLoss: 0.42616549134254456 - trainLoss: 0.39992451667785645\n",
      "cnt: 0 - valLoss: 0.4261651337146759 - trainLoss: 0.3999233543872833\n",
      "cnt: 0 - valLoss: 0.4261651337146759 - trainLoss: 0.399922251701355\n",
      "cnt: 0 - valLoss: 0.42616477608680725 - trainLoss: 0.39992111921310425\n",
      "cnt: 0 - valLoss: 0.426164448261261 - trainLoss: 0.3999199867248535\n",
      "cnt: 0 - valLoss: 0.42616409063339233 - trainLoss: 0.3999188542366028\n",
      "cnt: 0 - valLoss: 0.42616376280784607 - trainLoss: 0.39991775155067444\n",
      "cnt: 0 - valLoss: 0.4261636734008789 - trainLoss: 0.3999166488647461\n",
      "cnt: 0 - valLoss: 0.4261632263660431 - trainLoss: 0.399915486574173\n",
      "cnt: 0 - valLoss: 0.426162987947464 - trainLoss: 0.39991438388824463\n",
      "cnt: 0 - valLoss: 0.42616263031959534 - trainLoss: 0.3999132215976715\n",
      "cnt: 0 - valLoss: 0.426162451505661 - trainLoss: 0.3999120891094208\n",
      "cnt: 0 - valLoss: 0.4261620044708252 - trainLoss: 0.39991098642349243\n",
      "cnt: 0 - valLoss: 0.42616185545921326 - trainLoss: 0.3999098837375641\n",
      "cnt: 0 - valLoss: 0.42616137862205505 - trainLoss: 0.39990875124931335\n",
      "cnt: 0 - valLoss: 0.4261612296104431 - trainLoss: 0.399907648563385\n",
      "cnt: 0 - valLoss: 0.4261607825756073 - trainLoss: 0.39990654587745667\n",
      "cnt: 0 - valLoss: 0.42616063356399536 - trainLoss: 0.39990538358688354\n",
      "cnt: 0 - valLoss: 0.4261604845523834 - trainLoss: 0.3999042212963104\n",
      "cnt: 0 - valLoss: 0.4261600077152252 - trainLoss: 0.3999031186103821\n",
      "cnt: 0 - valLoss: 0.42615988850593567 - trainLoss: 0.39990201592445374\n",
      "cnt: 0 - valLoss: 0.42615947127342224 - trainLoss: 0.399900883436203\n",
      "cnt: 0 - valLoss: 0.4261592924594879 - trainLoss: 0.39989978075027466\n",
      "cnt: 0 - valLoss: 0.4261588156223297 - trainLoss: 0.3998986482620239\n",
      "cnt: 0 - valLoss: 0.42615869641304016 - trainLoss: 0.3998975157737732\n",
      "cnt: 0 - valLoss: 0.42615821957588196 - trainLoss: 0.39989644289016724\n",
      "cnt: 0 - valLoss: 0.4261581003665924 - trainLoss: 0.3998952805995941\n",
      "cnt: 0 - valLoss: 0.4261576533317566 - trainLoss: 0.399894118309021\n",
      "cnt: 0 - valLoss: 0.42615750432014465 - trainLoss: 0.39989301562309265\n",
      "cnt: 0 - valLoss: 0.4261573255062103 - trainLoss: 0.3998919427394867\n",
      "cnt: 0 - valLoss: 0.4261568784713745 - trainLoss: 0.3998907804489136\n",
      "cnt: 0 - valLoss: 0.4261567294597626 - trainLoss: 0.39988967776298523\n",
      "cnt: 0 - valLoss: 0.42615634202957153 - trainLoss: 0.3998885750770569\n",
      "cnt: 0 - valLoss: 0.4261561334133148 - trainLoss: 0.39988744258880615\n",
      "cnt: 0 - valLoss: 0.426155686378479 - trainLoss: 0.3998863399028778\n",
      "cnt: 0 - valLoss: 0.42615559697151184 - trainLoss: 0.3998851776123047\n",
      "cnt: 0 - valLoss: 0.42615509033203125 - trainLoss: 0.39988407492637634\n",
      "cnt: 0 - valLoss: 0.4261550009250641 - trainLoss: 0.3998829126358032\n",
      "cnt: 0 - valLoss: 0.42615458369255066 - trainLoss: 0.3998818099498749\n",
      "cnt: 0 - valLoss: 0.42615440487861633 - trainLoss: 0.39988067746162415\n",
      "cnt: 0 - valLoss: 0.4261541962623596 - trainLoss: 0.3998795747756958\n",
      "cnt: 0 - valLoss: 0.4261538088321686 - trainLoss: 0.39987847208976746\n",
      "cnt: 0 - valLoss: 0.42615363001823425 - trainLoss: 0.3998773396015167\n",
      "cnt: 0 - valLoss: 0.4261532127857208 - trainLoss: 0.3998762369155884\n",
      "cnt: 0 - valLoss: 0.4261530339717865 - trainLoss: 0.39987513422966003\n",
      "cnt: 0 - valLoss: 0.42615261673927307 - trainLoss: 0.3998739719390869\n",
      "cnt: 0 - valLoss: 0.42615243792533875 - trainLoss: 0.3998728394508362\n",
      "cnt: 0 - valLoss: 0.42615199089050293 - trainLoss: 0.39987173676490784\n",
      "cnt: 0 - valLoss: 0.42615193128585815 - trainLoss: 0.3998706340789795\n",
      "cnt: 0 - valLoss: 0.42615145444869995 - trainLoss: 0.39986947178840637\n",
      "cnt: 0 - valLoss: 0.426151305437088 - trainLoss: 0.3998683989048004\n",
      "cnt: 0 - valLoss: 0.4261511564254761 - trainLoss: 0.3998672366142273\n",
      "cnt: 0 - valLoss: 0.42615070939064026 - trainLoss: 0.39986613392829895\n",
      "cnt: 0 - valLoss: 0.4261505603790283 - trainLoss: 0.3998650312423706\n",
      "cnt: 0 - valLoss: 0.4261501133441925 - trainLoss: 0.3998638987541199\n",
      "cnt: 0 - valLoss: 0.42614999413490295 - trainLoss: 0.39986279606819153\n",
      "cnt: 0 - valLoss: 0.42614954710006714 - trainLoss: 0.3998616933822632\n",
      "cnt: 0 - valLoss: 0.4261494278907776 - trainLoss: 0.39986056089401245\n",
      "cnt: 0 - valLoss: 0.4261489510536194 - trainLoss: 0.3998594284057617\n",
      "cnt: 0 - valLoss: 0.42614883184432983 - trainLoss: 0.399858295917511\n",
      "cnt: 0 - valLoss: 0.4261484146118164 - trainLoss: 0.39985713362693787\n",
      "cnt: 0 - valLoss: 0.42614826560020447 - trainLoss: 0.3998560607433319\n",
      "cnt: 0 - valLoss: 0.42614784836769104 - trainLoss: 0.3998549282550812\n",
      "cnt: 0 - valLoss: 0.4261477291584015 - trainLoss: 0.39985379576683044\n",
      "cnt: 0 - valLoss: 0.42614755034446716 - trainLoss: 0.3998526930809021\n",
      "cnt: 0 - valLoss: 0.42614710330963135 - trainLoss: 0.39985159039497375\n",
      "cnt: 0 - valLoss: 0.4261469542980194 - trainLoss: 0.39985042810440063\n",
      "cnt: 0 - valLoss: 0.4261465072631836 - trainLoss: 0.3998493552207947\n",
      "cnt: 0 - valLoss: 0.42614638805389404 - trainLoss: 0.39984825253486633\n",
      "cnt: 0 - valLoss: 0.4261459708213806 - trainLoss: 0.3998471200466156\n",
      "cnt: 0 - valLoss: 0.4261458218097687 - trainLoss: 0.39984601736068726\n",
      "cnt: 0 - valLoss: 0.42614540457725525 - trainLoss: 0.39984485507011414\n",
      "cnt: 0 - valLoss: 0.4261452257633209 - trainLoss: 0.399843692779541\n",
      "cnt: 0 - valLoss: 0.4261448383331299 - trainLoss: 0.39984264969825745\n",
      "cnt: 0 - valLoss: 0.42614468932151794 - trainLoss: 0.3998414874076843\n",
      "cnt: 0 - valLoss: 0.4261443018913269 - trainLoss: 0.3998403549194336\n",
      "cnt: 0 - valLoss: 0.4261441230773926 - trainLoss: 0.39983928203582764\n",
      "cnt: 0 - valLoss: 0.42614367604255676 - trainLoss: 0.3998381495475769\n",
      "cnt: 0 - valLoss: 0.4261435568332672 - trainLoss: 0.39983701705932617\n",
      "cnt: 0 - valLoss: 0.42614316940307617 - trainLoss: 0.3998359143733978\n",
      "cnt: 0 - valLoss: 0.42614296078681946 - trainLoss: 0.3998348116874695\n",
      "cnt: 0 - valLoss: 0.4261428415775299 - trainLoss: 0.39983367919921875\n",
      "cnt: 0 - valLoss: 0.4261423945426941 - trainLoss: 0.3998325765132904\n",
      "cnt: 0 - valLoss: 0.42614230513572693 - trainLoss: 0.3998314142227173\n",
      "cnt: 0 - valLoss: 0.4261418581008911 - trainLoss: 0.39983031153678894\n",
      "cnt: 0 - valLoss: 0.4261417090892792 - trainLoss: 0.3998292088508606\n",
      "cnt: 0 - valLoss: 0.42614129185676575 - trainLoss: 0.39982807636260986\n",
      "cnt: 0 - valLoss: 0.4261411726474762 - trainLoss: 0.39982691407203674\n",
      "cnt: 0 - valLoss: 0.4261407256126404 - trainLoss: 0.3998258113861084\n",
      "cnt: 0 - valLoss: 0.42614054679870605 - trainLoss: 0.39982470870018005\n",
      "cnt: 0 - valLoss: 0.4261402189731598 - trainLoss: 0.3998235762119293\n",
      "cnt: 0 - valLoss: 0.4261400103569031 - trainLoss: 0.399822473526001\n",
      "cnt: 0 - valLoss: 0.4261396527290344 - trainLoss: 0.39982137084007263\n",
      "cnt: 0 - valLoss: 0.4261394739151001 - trainLoss: 0.3998202383518219\n",
      "cnt: 0 - valLoss: 0.42613905668258667 - trainLoss: 0.39981913566589355\n",
      "cnt: 0 - valLoss: 0.42613887786865234 - trainLoss: 0.39981797337532043\n",
      "cnt: 0 - valLoss: 0.4261384904384613 - trainLoss: 0.3998168706893921\n",
      "cnt: 0 - valLoss: 0.42613837122917175 - trainLoss: 0.39981576800346375\n",
      "cnt: 0 - valLoss: 0.4261379539966583 - trainLoss: 0.399814635515213\n",
      "cnt: 0 - valLoss: 0.4261378049850464 - trainLoss: 0.39981353282928467\n",
      "cnt: 0 - valLoss: 0.42613741755485535 - trainLoss: 0.3998124301433563\n",
      "cnt: 0 - valLoss: 0.4261372685432434 - trainLoss: 0.3998112976551056\n",
      "cnt: 0 - valLoss: 0.4261368215084076 - trainLoss: 0.39981019496917725\n",
      "cnt: 0 - valLoss: 0.42613673210144043 - trainLoss: 0.3998090326786041\n",
      "cnt: 0 - valLoss: 0.426136314868927 - trainLoss: 0.3998079299926758\n",
      "cnt: 0 - valLoss: 0.42613619565963745 - trainLoss: 0.39980679750442505\n",
      "cnt: 0 - valLoss: 0.4261360466480255 - trainLoss: 0.3998056948184967\n",
      "cnt: 0 - valLoss: 0.4261356294155121 - trainLoss: 0.39980459213256836\n",
      "cnt: 0 - valLoss: 0.42613545060157776 - trainLoss: 0.3998034596443176\n",
      "cnt: 0 - valLoss: 0.4261350631713867 - trainLoss: 0.3998023569583893\n",
      "cnt: 0 - valLoss: 0.42613494396209717 - trainLoss: 0.39980119466781616\n",
      "cnt: 0 - valLoss: 0.42613452672958374 - trainLoss: 0.3998001217842102\n",
      "cnt: 0 - valLoss: 0.4261343777179718 - trainLoss: 0.3997989892959595\n",
      "cnt: 0 - valLoss: 0.42613399028778076 - trainLoss: 0.39979785680770874\n",
      "cnt: 0 - valLoss: 0.4261338412761688 - trainLoss: 0.3997967839241028\n",
      "cnt: 0 - valLoss: 0.426133394241333 - trainLoss: 0.3997955918312073\n",
      "cnt: 0 - valLoss: 0.42613327503204346 - trainLoss: 0.3997945189476013\n",
      "cnt: 0 - valLoss: 0.4261328876018524 - trainLoss: 0.3997933566570282\n",
      "cnt: 0 - valLoss: 0.4261327087879181 - trainLoss: 0.39979231357574463\n",
      "cnt: 0 - valLoss: 0.4261323809623718 - trainLoss: 0.3997911810874939\n",
      "cnt: 0 - valLoss: 0.4261322021484375 - trainLoss: 0.3997900187969208\n",
      "cnt: 0 - valLoss: 0.4261317849159241 - trainLoss: 0.39978891611099243\n",
      "cnt: 0 - valLoss: 0.42613163590431213 - trainLoss: 0.3997878134250641\n",
      "cnt: 0 - valLoss: 0.4261312484741211 - trainLoss: 0.39978668093681335\n",
      "cnt: 0 - valLoss: 0.42613112926483154 - trainLoss: 0.399785578250885\n",
      "cnt: 0 - valLoss: 0.4261307120323181 - trainLoss: 0.39978447556495667\n",
      "cnt: 0 - valLoss: 0.4261305630207062 - trainLoss: 0.39978334307670593\n",
      "cnt: 0 - valLoss: 0.42613017559051514 - trainLoss: 0.3997822403907776\n",
      "cnt: 0 - valLoss: 0.4261300563812256 - trainLoss: 0.39978113770484924\n",
      "cnt: 0 - valLoss: 0.42612963914871216 - trainLoss: 0.3997800052165985\n",
      "cnt: 0 - valLoss: 0.4261294901371002 - trainLoss: 0.39977890253067017\n",
      "cnt: 0 - valLoss: 0.4261291027069092 - trainLoss: 0.3997777998447418\n",
      "cnt: 0 - valLoss: 0.42612895369529724 - trainLoss: 0.3997766375541687\n",
      "cnt: 0 - valLoss: 0.4261288642883301 - trainLoss: 0.39977553486824036\n",
      "cnt: 0 - valLoss: 0.42612841725349426 - trainLoss: 0.3997744023799896\n",
      "cnt: 0 - valLoss: 0.4261282980442047 - trainLoss: 0.3997732400894165\n",
      "cnt: 0 - valLoss: 0.42612791061401367 - trainLoss: 0.39977219700813293\n",
      "cnt: 0 - valLoss: 0.42612749338150024 - trainLoss: 0.3997710347175598\n",
      "cnt: 0 - valLoss: 0.4261274039745331 - trainLoss: 0.39976996183395386\n",
      "cnt: 0 - valLoss: 0.42612719535827637 - trainLoss: 0.39976879954338074\n",
      "cnt: 0 - valLoss: 0.4261268377304077 - trainLoss: 0.3997676968574524\n",
      "cnt: 0 - valLoss: 0.4261264204978943 - trainLoss: 0.39976656436920166\n",
      "cnt: 0 - valLoss: 0.42612630128860474 - trainLoss: 0.3997654616832733\n",
      "cnt: 0 - valLoss: 0.4261259138584137 - trainLoss: 0.39976435899734497\n",
      "cnt: 0 - valLoss: 0.42612576484680176 - trainLoss: 0.39976322650909424\n",
      "cnt: 0 - valLoss: 0.4261256158351898 - trainLoss: 0.3997621238231659\n",
      "cnt: 0 - valLoss: 0.426125168800354 - trainLoss: 0.39976102113723755\n",
      "cnt: 0 - valLoss: 0.42612504959106445 - trainLoss: 0.3997598886489868\n",
      "cnt: 0 - valLoss: 0.42612460255622864 - trainLoss: 0.39975878596305847\n",
      "cnt: 0 - valLoss: 0.4261244833469391 - trainLoss: 0.3997576832771301\n",
      "cnt: 0 - valLoss: 0.42612406611442566 - trainLoss: 0.3997565507888794\n",
      "cnt: 0 - valLoss: 0.42612388730049133 - trainLoss: 0.39975544810295105\n",
      "cnt: 0 - valLoss: 0.4261235296726227 - trainLoss: 0.3997543454170227\n",
      "cnt: 0 - valLoss: 0.42612338066101074 - trainLoss: 0.399753212928772\n",
      "cnt: 0 - valLoss: 0.4261229932308197 - trainLoss: 0.39975211024284363\n",
      "cnt: 0 - valLoss: 0.42612284421920776 - trainLoss: 0.3997510075569153\n",
      "cnt: 0 - valLoss: 0.42612239718437195 - trainLoss: 0.39974987506866455\n",
      "cnt: 0 - valLoss: 0.4261223077774048 - trainLoss: 0.3997487723827362\n",
      "cnt: 0 - valLoss: 0.4261218309402466 - trainLoss: 0.39974766969680786\n",
      "cnt: 0 - valLoss: 0.4261217415332794 - trainLoss: 0.39974653720855713\n",
      "cnt: 0 - valLoss: 0.426121324300766 - trainLoss: 0.3997454047203064\n",
      "cnt: 0 - valLoss: 0.42612123489379883 - trainLoss: 0.39974433183670044\n",
      "cnt: 0 - valLoss: 0.426120787858963 - trainLoss: 0.3997431695461273\n",
      "cnt: 0 - valLoss: 0.4261206388473511 - trainLoss: 0.39974209666252136\n",
      "cnt: 0 - valLoss: 0.4261205494403839 - trainLoss: 0.39974093437194824\n",
      "cnt: 0 - valLoss: 0.4261201024055481 - trainLoss: 0.3997398316860199\n",
      "cnt: 0 - valLoss: 0.42611998319625854 - trainLoss: 0.39973869919776917\n",
      "cnt: 0 - valLoss: 0.4261195659637451 - trainLoss: 0.3997375965118408\n",
      "cnt: 0 - valLoss: 0.4261194169521332 - trainLoss: 0.3997364938259125\n",
      "cnt: 0 - valLoss: 0.42611899971961975 - trainLoss: 0.39973536133766174\n",
      "cnt: 0 - valLoss: 0.4261188805103302 - trainLoss: 0.399734228849411\n",
      "cnt: 0 - valLoss: 0.42611849308013916 - trainLoss: 0.39973315596580505\n",
      "cnt: 0 - valLoss: 0.4261183440685272 - trainLoss: 0.3997320234775543\n",
      "cnt: 0 - valLoss: 0.42611798644065857 - trainLoss: 0.399730920791626\n",
      "cnt: 0 - valLoss: 0.42611780762672424 - trainLoss: 0.39972981810569763\n",
      "cnt: 0 - valLoss: 0.4261173605918884 - trainLoss: 0.3997286856174469\n",
      "cnt: 0 - valLoss: 0.42611730098724365 - trainLoss: 0.39972755312919617\n",
      "cnt: 0 - valLoss: 0.42611682415008545 - trainLoss: 0.39972642064094543\n",
      "cnt: 0 - valLoss: 0.4261167347431183 - trainLoss: 0.3997253179550171\n",
      "cnt: 0 - valLoss: 0.4261162579059601 - trainLoss: 0.39972424507141113\n",
      "cnt: 0 - valLoss: 0.4261162281036377 - trainLoss: 0.3997231423854828\n",
      "cnt: 0 - valLoss: 0.4261157512664795 - trainLoss: 0.39972198009490967\n",
      "cnt: 0 - valLoss: 0.42611560225486755 - trainLoss: 0.3997208774089813\n",
      "cnt: 0 - valLoss: 0.4261152148246765 - trainLoss: 0.39971980452537537\n",
      "cnt: 0 - valLoss: 0.4261150658130646 - trainLoss: 0.39971864223480225\n",
      "cnt: 0 - valLoss: 0.4261149764060974 - trainLoss: 0.3997175693511963\n",
      "cnt: 0 - valLoss: 0.42611458897590637 - trainLoss: 0.39971640706062317\n",
      "cnt: 0 - valLoss: 0.42611441016197205 - trainLoss: 0.3997153043746948\n",
      "cnt: 0 - valLoss: 0.426114022731781 - trainLoss: 0.3997142016887665\n",
      "cnt: 0 - valLoss: 0.4261138439178467 - trainLoss: 0.3997131288051605\n",
      "cnt: 0 - valLoss: 0.4261135160923004 - trainLoss: 0.3997119665145874\n",
      "cnt: 0 - valLoss: 0.4261133372783661 - trainLoss: 0.39971086382865906\n",
      "cnt: 0 - valLoss: 0.42611294984817505 - trainLoss: 0.3997097313404083\n",
      "cnt: 0 - valLoss: 0.4261128306388855 - trainLoss: 0.39970862865448\n",
      "cnt: 0 - valLoss: 0.42611241340637207 - trainLoss: 0.39970749616622925\n",
      "cnt: 0 - valLoss: 0.4261123239994049 - trainLoss: 0.3997063934803009\n",
      "cnt: 0 - valLoss: 0.4261118769645691 - trainLoss: 0.39970529079437256\n",
      "cnt: 0 - valLoss: 0.42611178755760193 - trainLoss: 0.3997042179107666\n",
      "cnt: 0 - valLoss: 0.4261113703250885 - trainLoss: 0.3997030556201935\n",
      "cnt: 0 - valLoss: 0.4261111915111542 - trainLoss: 0.3997019827365875\n",
      "cnt: 0 - valLoss: 0.4261108636856079 - trainLoss: 0.3997008204460144\n",
      "cnt: 0 - valLoss: 0.4261106848716736 - trainLoss: 0.39969971776008606\n",
      "cnt: 0 - valLoss: 0.42611029744148254 - trainLoss: 0.3996986448764801\n",
      "cnt: 0 - valLoss: 0.4261102080345154 - trainLoss: 0.39969754219055176\n",
      "cnt: 0 - valLoss: 0.42610979080200195 - trainLoss: 0.39969637989997864\n",
      "cnt: 0 - valLoss: 0.4261096715927124 - trainLoss: 0.3996952772140503\n",
      "cnt: 0 - valLoss: 0.42610928416252136 - trainLoss: 0.39969414472579956\n",
      "cnt: 0 - valLoss: 0.4261091351509094 - trainLoss: 0.3996930420398712\n",
      "cnt: 0 - valLoss: 0.4261087477207184 - trainLoss: 0.39969193935394287\n",
      "cnt: 0 - valLoss: 0.4261086583137512 - trainLoss: 0.39969080686569214\n",
      "cnt: 0 - valLoss: 0.4261082112789154 - trainLoss: 0.3996897339820862\n",
      "cnt: 0 - valLoss: 0.42610812187194824 - trainLoss: 0.39968860149383545\n",
      "cnt: 0 - valLoss: 0.4261077344417572 - trainLoss: 0.3996874690055847\n",
      "cnt: 0 - valLoss: 0.42610758543014526 - trainLoss: 0.39968639612197876\n",
      "cnt: 0 - valLoss: 0.42610716819763184 - trainLoss: 0.399685263633728\n",
      "cnt: 0 - valLoss: 0.4261070787906647 - trainLoss: 0.3996841311454773\n",
      "cnt: 0 - valLoss: 0.42610666155815125 - trainLoss: 0.39968302845954895\n",
      "cnt: 0 - valLoss: 0.4261065721511841 - trainLoss: 0.3996819257736206\n",
      "cnt: 0 - valLoss: 0.42610615491867065 - trainLoss: 0.3996807932853699\n",
      "cnt: 0 - valLoss: 0.4261060655117035 - trainLoss: 0.39967969059944153\n",
      "cnt: 0 - valLoss: 0.4261056184768677 - trainLoss: 0.39967861771583557\n",
      "cnt: 0 - valLoss: 0.4261055290699005 - trainLoss: 0.39967745542526245\n",
      "cnt: 0 - valLoss: 0.4261051118373871 - trainLoss: 0.3996763825416565\n",
      "cnt: 0 - valLoss: 0.42610499262809753 - trainLoss: 0.39967527985572815\n",
      "cnt: 0 - valLoss: 0.4261046051979065 - trainLoss: 0.3996741771697998\n",
      "cnt: 0 - valLoss: 0.42610451579093933 - trainLoss: 0.3996730446815491\n",
      "cnt: 0 - valLoss: 0.4261040985584259 - trainLoss: 0.39967191219329834\n",
      "cnt: 0 - valLoss: 0.42610400915145874 - trainLoss: 0.3996708393096924\n",
      "cnt: 0 - valLoss: 0.4261035621166229 - trainLoss: 0.39966970682144165\n",
      "cnt: 0 - valLoss: 0.42610347270965576 - trainLoss: 0.3996686041355133\n",
      "cnt: 0 - valLoss: 0.4261030852794647 - trainLoss: 0.3996674418449402\n",
      "cnt: 0 - valLoss: 0.4261029362678528 - trainLoss: 0.39966633915901184\n",
      "cnt: 0 - valLoss: 0.42610254883766174 - trainLoss: 0.3996652662754059\n",
      "cnt: 0 - valLoss: 0.4261024594306946 - trainLoss: 0.39966416358947754\n",
      "cnt: 0 - valLoss: 0.42610207200050354 - trainLoss: 0.3996630012989044\n",
      "cnt: 0 - valLoss: 0.4261018931865692 - trainLoss: 0.39966192841529846\n",
      "cnt: 0 - valLoss: 0.4261015057563782 - trainLoss: 0.3996608257293701\n",
      "cnt: 0 - valLoss: 0.426101416349411 - trainLoss: 0.3996596932411194\n",
      "cnt: 0 - valLoss: 0.4261009991168976 - trainLoss: 0.39965859055519104\n",
      "cnt: 0 - valLoss: 0.4261009097099304 - trainLoss: 0.3996574878692627\n",
      "cnt: 0 - valLoss: 0.426100492477417 - trainLoss: 0.39965641498565674\n",
      "cnt: 0 - valLoss: 0.42610034346580505 - trainLoss: 0.3996552526950836\n",
      "cnt: 0 - valLoss: 0.4261000156402588 - trainLoss: 0.3996541500091553\n",
      "cnt: 0 - valLoss: 0.42609989643096924 - trainLoss: 0.3996530771255493\n",
      "cnt: 0 - valLoss: 0.4260994493961334 - trainLoss: 0.3996519446372986\n",
      "cnt: 0 - valLoss: 0.42609935998916626 - trainLoss: 0.39965081214904785\n",
      "cnt: 0 - valLoss: 0.42609894275665283 - trainLoss: 0.3996496796607971\n",
      "cnt: 0 - valLoss: 0.42609885334968567 - trainLoss: 0.39964860677719116\n",
      "cnt: 0 - valLoss: 0.42609846591949463 - trainLoss: 0.3996475040912628\n",
      "cnt: 0 - valLoss: 0.4260983467102051 - trainLoss: 0.3996464014053345\n",
      "cnt: 0 - valLoss: 0.4260979890823364 - trainLoss: 0.39964526891708374\n",
      "cnt: 0 - valLoss: 0.4260978400707245 - trainLoss: 0.3996441662311554\n",
      "cnt: 0 - valLoss: 0.42609745264053345 - trainLoss: 0.39964306354522705\n",
      "cnt: 0 - valLoss: 0.4260973632335663 - trainLoss: 0.3996419310569763\n",
      "cnt: 0 - valLoss: 0.42609697580337524 - trainLoss: 0.399640828371048\n",
      "cnt: 0 - valLoss: 0.4260968267917633 - trainLoss: 0.39963972568511963\n",
      "cnt: 0 - valLoss: 0.4260964095592499 - trainLoss: 0.39963865280151367\n",
      "cnt: 0 - valLoss: 0.4260963201522827 - trainLoss: 0.39963749051094055\n",
      "cnt: 0 - valLoss: 0.42609599232673645 - trainLoss: 0.3996363878250122\n",
      "cnt: 0 - valLoss: 0.4260958433151245 - trainLoss: 0.39963531494140625\n",
      "cnt: 0 - valLoss: 0.4260954260826111 - trainLoss: 0.3996341824531555\n",
      "cnt: 0 - valLoss: 0.4260953366756439 - trainLoss: 0.3996330797672272\n",
      "cnt: 0 - valLoss: 0.4260949492454529 - trainLoss: 0.3996320068836212\n",
      "cnt: 0 - valLoss: 0.42609483003616333 - trainLoss: 0.39963090419769287\n",
      "cnt: 0 - valLoss: 0.4260944426059723 - trainLoss: 0.39962974190711975\n",
      "cnt: 0 - valLoss: 0.4260944128036499 - trainLoss: 0.3996286392211914\n",
      "cnt: 0 - valLoss: 0.4260939657688141 - trainLoss: 0.39962756633758545\n",
      "cnt: 0 - valLoss: 0.4260938763618469 - trainLoss: 0.3996264338493347\n",
      "cnt: 0 - valLoss: 0.4260934591293335 - trainLoss: 0.39962533116340637\n",
      "cnt: 0 - valLoss: 0.42609333992004395 - trainLoss: 0.399624228477478\n",
      "cnt: 0 - valLoss: 0.4260929524898529 - trainLoss: 0.3996230959892273\n",
      "cnt: 0 - valLoss: 0.42609286308288574 - trainLoss: 0.39962199330329895\n",
      "cnt: 0 - valLoss: 0.4260924160480499 - trainLoss: 0.3996208906173706\n",
      "cnt: 0 - valLoss: 0.42609238624572754 - trainLoss: 0.39961981773376465\n",
      "cnt: 0 - valLoss: 0.4260919690132141 - trainLoss: 0.3996186852455139\n",
      "cnt: 0 - valLoss: 0.4260918200016022 - trainLoss: 0.3996175527572632\n",
      "cnt: 0 - valLoss: 0.4260914921760559 - trainLoss: 0.39961642026901245\n",
      "cnt: 0 - valLoss: 0.42609134316444397 - trainLoss: 0.3996153473854065\n",
      "cnt: 0 - valLoss: 0.42609095573425293 - trainLoss: 0.39961427450180054\n",
      "cnt: 0 - valLoss: 0.42609089612960815 - trainLoss: 0.3996131420135498\n",
      "cnt: 0 - valLoss: 0.4260905086994171 - trainLoss: 0.39961206912994385\n",
      "cnt: 0 - valLoss: 0.42609041929244995 - trainLoss: 0.3996109068393707\n",
      "cnt: 0 - valLoss: 0.4260900318622589 - trainLoss: 0.39960983395576477\n",
      "cnt: 0 - valLoss: 0.4260898530483246 - trainLoss: 0.3996087312698364\n",
      "cnt: 0 - valLoss: 0.4260895252227783 - trainLoss: 0.3996075987815857\n",
      "cnt: 0 - valLoss: 0.4260891377925873 - trainLoss: 0.39960649609565735\n",
      "cnt: 0 - valLoss: 0.42608898878097534 - trainLoss: 0.399605393409729\n",
      "cnt: 0 - valLoss: 0.4260886013507843 - trainLoss: 0.39960426092147827\n",
      "cnt: 0 - valLoss: 0.42608851194381714 - trainLoss: 0.3996031582355499\n",
      "cnt: 0 - valLoss: 0.4260881543159485 - trainLoss: 0.39960208535194397\n",
      "cnt: 0 - valLoss: 0.4260880649089813 - trainLoss: 0.39960092306137085\n",
      "cnt: 0 - valLoss: 0.4260876774787903 - trainLoss: 0.3995998501777649\n",
      "cnt: 0 - valLoss: 0.42608752846717834 - trainLoss: 0.39959874749183655\n",
      "cnt: 0 - valLoss: 0.4260871410369873 - trainLoss: 0.3995976448059082\n",
      "cnt: 0 - valLoss: 0.4260871112346649 - trainLoss: 0.39959657192230225\n",
      "cnt: 0 - valLoss: 0.4260866940021515 - trainLoss: 0.3995954692363739\n",
      "cnt: 0 - valLoss: 0.4260866045951843 - trainLoss: 0.39959433674812317\n",
      "cnt: 0 - valLoss: 0.4260862469673157 - trainLoss: 0.3995932340621948\n",
      "cnt: 0 - valLoss: 0.42608606815338135 - trainLoss: 0.3995921313762665\n",
      "cnt: 0 - valLoss: 0.4260857105255127 - trainLoss: 0.39959099888801575\n",
      "cnt: 0 - valLoss: 0.4260856509208679 - trainLoss: 0.3995898962020874\n",
      "cnt: 0 - valLoss: 0.4260852336883545 - trainLoss: 0.39958882331848145\n",
      "cnt: 0 - valLoss: 0.4260851740837097 - trainLoss: 0.3995877206325531\n",
      "cnt: 0 - valLoss: 0.4260847568511963 - trainLoss: 0.39958658814430237\n",
      "cnt: 0 - valLoss: 0.42608463764190674 - trainLoss: 0.3995855152606964\n",
      "cnt: 0 - valLoss: 0.4260842800140381 - trainLoss: 0.39958441257476807\n",
      "cnt: 0 - valLoss: 0.42608416080474854 - trainLoss: 0.3995833098888397\n",
      "cnt: 0 - valLoss: 0.4260837733745575 - trainLoss: 0.3995821475982666\n",
      "cnt: 0 - valLoss: 0.42608368396759033 - trainLoss: 0.39958107471466064\n",
      "cnt: 0 - valLoss: 0.4260832965373993 - trainLoss: 0.3995799720287323\n",
      "cnt: 0 - valLoss: 0.42608317732810974 - trainLoss: 0.39957883954048157\n",
      "cnt: 0 - valLoss: 0.4260828197002411 - trainLoss: 0.3995777368545532\n",
      "cnt: 0 - valLoss: 0.4260827302932739 - trainLoss: 0.3995766341686249\n",
      "cnt: 0 - valLoss: 0.4260823428630829 - trainLoss: 0.39957550168037415\n",
      "cnt: 0 - valLoss: 0.42608222365379333 - trainLoss: 0.3995743989944458\n",
      "cnt: 0 - valLoss: 0.42608189582824707 - trainLoss: 0.39957332611083984\n",
      "cnt: 0 - valLoss: 0.42608174681663513 - trainLoss: 0.3995722234249115\n",
      "cnt: 0 - valLoss: 0.4260813593864441 - trainLoss: 0.39957115054130554\n",
      "cnt: 0 - valLoss: 0.42608126997947693 - trainLoss: 0.3995700180530548\n",
      "cnt: 0 - valLoss: 0.42608094215393066 - trainLoss: 0.3995688855648041\n",
      "cnt: 0 - valLoss: 0.42608052492141724 - trainLoss: 0.3995678126811981\n",
      "cnt: 0 - valLoss: 0.4260804355144501 - trainLoss: 0.3995666801929474\n",
      "cnt: 0 - valLoss: 0.4260800778865814 - trainLoss: 0.39956557750701904\n",
      "cnt: 0 - valLoss: 0.42607995867729187 - trainLoss: 0.3995644748210907\n",
      "cnt: 0 - valLoss: 0.4260796010494232 - trainLoss: 0.39956334233283997\n",
      "cnt: 0 - valLoss: 0.42607948184013367 - trainLoss: 0.3995622992515564\n",
      "cnt: 0 - valLoss: 0.42607906460762024 - trainLoss: 0.3995611369609833\n",
      "cnt: 0 - valLoss: 0.42607900500297546 - trainLoss: 0.3995600640773773\n",
      "cnt: 0 - valLoss: 0.4260786175727844 - trainLoss: 0.3995589315891266\n",
      "cnt: 0 - valLoss: 0.42607852816581726 - trainLoss: 0.399557888507843\n",
      "cnt: 0 - valLoss: 0.426078200340271 - trainLoss: 0.3995567560195923\n",
      "cnt: 0 - valLoss: 0.42607808113098145 - trainLoss: 0.39955565333366394\n",
      "cnt: 0 - valLoss: 0.42607763409614563 - trainLoss: 0.3995545506477356\n",
      "cnt: 0 - valLoss: 0.42607760429382324 - trainLoss: 0.39955341815948486\n",
      "cnt: 0 - valLoss: 0.4260772168636322 - trainLoss: 0.3995523154735565\n",
      "cnt: 0 - valLoss: 0.42607706785202026 - trainLoss: 0.3995512127876282\n",
      "cnt: 0 - valLoss: 0.426076740026474 - trainLoss: 0.3995501399040222\n",
      "cnt: 0 - valLoss: 0.42607665061950684 - trainLoss: 0.3995490074157715\n",
      "cnt: 0 - valLoss: 0.4260762631893158 - trainLoss: 0.39954790472984314\n",
      "cnt: 0 - valLoss: 0.42607614398002625 - trainLoss: 0.3995468020439148\n",
      "cnt: 0 - valLoss: 0.4260757863521576 - trainLoss: 0.39954566955566406\n",
      "cnt: 0 - valLoss: 0.42607569694519043 - trainLoss: 0.3995445668697357\n",
      "cnt: 0 - valLoss: 0.4260753393173218 - trainLoss: 0.39954349398612976\n",
      "cnt: 0 - valLoss: 0.42607519030570984 - trainLoss: 0.3995423913002014\n",
      "cnt: 0 - valLoss: 0.42607489228248596 - trainLoss: 0.3995412588119507\n",
      "cnt: 0 - valLoss: 0.42607471346855164 - trainLoss: 0.39954015612602234\n",
      "cnt: 0 - valLoss: 0.4260743260383606 - trainLoss: 0.3995390832424164\n",
      "cnt: 0 - valLoss: 0.4260742962360382 - trainLoss: 0.39953798055648804\n",
      "cnt: 0 - valLoss: 0.42607390880584717 - trainLoss: 0.3995368480682373\n",
      "cnt: 0 - valLoss: 0.42607381939888 - trainLoss: 0.39953580498695374\n",
      "cnt: 0 - valLoss: 0.42607349157333374 - trainLoss: 0.399534672498703\n",
      "cnt: 0 - valLoss: 0.4260730445384979 - trainLoss: 0.39953356981277466\n",
      "cnt: 0 - valLoss: 0.42607301473617554 - trainLoss: 0.3995324373245239\n",
      "cnt: 0 - valLoss: 0.4260726273059845 - trainLoss: 0.3995313346385956\n",
      "cnt: 0 - valLoss: 0.42607250809669495 - trainLoss: 0.39953023195266724\n",
      "cnt: 0 - valLoss: 0.42607221007347107 - trainLoss: 0.3995290994644165\n",
      "cnt: 0 - valLoss: 0.42607206106185913 - trainLoss: 0.39952805638313293\n",
      "cnt: 0 - valLoss: 0.4260716736316681 - trainLoss: 0.3995269238948822\n",
      "cnt: 0 - valLoss: 0.4260716438293457 - trainLoss: 0.39952582120895386\n",
      "cnt: 0 - valLoss: 0.4260711967945099 - trainLoss: 0.3995247185230255\n",
      "cnt: 0 - valLoss: 0.4260711669921875 - trainLoss: 0.39952364563941956\n",
      "cnt: 0 - valLoss: 0.42607077956199646 - trainLoss: 0.3995225131511688\n",
      "cnt: 0 - valLoss: 0.4260706603527069 - trainLoss: 0.3995214104652405\n",
      "cnt: 0 - valLoss: 0.42607030272483826 - trainLoss: 0.3995203375816345\n",
      "cnt: 0 - valLoss: 0.4260702133178711 - trainLoss: 0.3995192348957062\n",
      "cnt: 0 - valLoss: 0.42606979608535767 - trainLoss: 0.39951810240745544\n",
      "cnt: 0 - valLoss: 0.42606979608535767 - trainLoss: 0.3995169997215271\n",
      "cnt: 0 - valLoss: 0.42606937885284424 - trainLoss: 0.39951592683792114\n",
      "cnt: 0 - valLoss: 0.4260690212249756 - trainLoss: 0.3995148241519928\n",
      "cnt: 0 - valLoss: 0.4260689318180084 - trainLoss: 0.39951369166374207\n",
      "cnt: 0 - valLoss: 0.4260685443878174 - trainLoss: 0.3995125889778137\n",
      "cnt: 0 - valLoss: 0.426068514585495 - trainLoss: 0.3995114862918854\n",
      "cnt: 0 - valLoss: 0.4260680675506592 - trainLoss: 0.39951035380363464\n",
      "cnt: 0 - valLoss: 0.4260680079460144 - trainLoss: 0.3995092511177063\n",
      "cnt: 0 - valLoss: 0.42606762051582336 - trainLoss: 0.39950817823410034\n",
      "cnt: 0 - valLoss: 0.4260675609111786 - trainLoss: 0.3995071053504944\n",
      "cnt: 0 - valLoss: 0.42606717348098755 - trainLoss: 0.39950594305992126\n",
      "cnt: 0 - valLoss: 0.4260670840740204 - trainLoss: 0.3995048999786377\n",
      "cnt: 0 - valLoss: 0.42606669664382935 - trainLoss: 0.39950376749038696\n",
      "cnt: 0 - valLoss: 0.42606663703918457 - trainLoss: 0.3995026648044586\n",
      "cnt: 0 - valLoss: 0.4260662794113159 - trainLoss: 0.3995015621185303\n",
      "cnt: 0 - valLoss: 0.42606619000434875 - trainLoss: 0.39950042963027954\n",
      "cnt: 0 - valLoss: 0.4260657727718353 - trainLoss: 0.3994993269443512\n",
      "cnt: 0 - valLoss: 0.42606571316719055 - trainLoss: 0.39949825406074524\n",
      "cnt: 0 - valLoss: 0.4260653257369995 - trainLoss: 0.3994971513748169\n",
      "cnt: 0 - valLoss: 0.42606526613235474 - trainLoss: 0.39949601888656616\n",
      "cnt: 0 - valLoss: 0.4260649085044861 - trainLoss: 0.3994949460029602\n",
      "cnt: 0 - valLoss: 0.42606449127197266 - trainLoss: 0.39949384331703186\n",
      "cnt: 0 - valLoss: 0.4260644018650055 - trainLoss: 0.3994927406311035\n",
      "cnt: 0 - valLoss: 0.42606404423713684 - trainLoss: 0.3994916081428528\n",
      "cnt: 0 - valLoss: 0.4260639548301697 - trainLoss: 0.3994905352592468\n",
      "cnt: 0 - valLoss: 0.42606356739997864 - trainLoss: 0.3994894325733185\n",
      "cnt: 0 - valLoss: 0.42606350779533386 - trainLoss: 0.39948832988739014\n",
      "cnt: 0 - valLoss: 0.4260631203651428 - trainLoss: 0.3994872570037842\n",
      "cnt: 0 - valLoss: 0.42606303095817566 - trainLoss: 0.39948615431785583\n",
      "cnt: 0 - valLoss: 0.426062673330307 - trainLoss: 0.3994850218296051\n",
      "cnt: 0 - valLoss: 0.42606255412101746 - trainLoss: 0.39948391914367676\n",
      "cnt: 0 - valLoss: 0.4260621964931488 - trainLoss: 0.3994828164577484\n",
      "cnt: 0 - valLoss: 0.42606213688850403 - trainLoss: 0.39948177337646484\n",
      "cnt: 0 - valLoss: 0.426061749458313 - trainLoss: 0.3994806110858917\n",
      "cnt: 0 - valLoss: 0.4260616600513458 - trainLoss: 0.39947953820228577\n",
      "cnt: 0 - valLoss: 0.4260613024234772 - trainLoss: 0.3994784355163574\n",
      "cnt: 0 - valLoss: 0.4260611832141876 - trainLoss: 0.3994773328304291\n",
      "cnt: 0 - valLoss: 0.4260607957839966 - trainLoss: 0.39947620034217834\n",
      "cnt: 0 - valLoss: 0.4260607063770294 - trainLoss: 0.39947509765625\n",
      "cnt: 0 - valLoss: 0.42606037855148315 - trainLoss: 0.39947402477264404\n",
      "cnt: 0 - valLoss: 0.4260600209236145 - trainLoss: 0.3994729220867157\n",
      "cnt: 0 - valLoss: 0.42605990171432495 - trainLoss: 0.39947184920310974\n",
      "cnt: 0 - valLoss: 0.4260595142841339 - trainLoss: 0.3994706869125366\n",
      "cnt: 0 - valLoss: 0.42605942487716675 - trainLoss: 0.39946961402893066\n",
      "cnt: 0 - valLoss: 0.4260590672492981 - trainLoss: 0.3994685113430023\n",
      "cnt: 0 - valLoss: 0.4260590076446533 - trainLoss: 0.39946743845939636\n",
      "cnt: 0 - valLoss: 0.4260586202144623 - trainLoss: 0.399466335773468\n",
      "cnt: 0 - valLoss: 0.4260585308074951 - trainLoss: 0.3994652032852173\n",
      "cnt: 0 - valLoss: 0.4260581433773041 - trainLoss: 0.39946404099464417\n",
      "cnt: 0 - valLoss: 0.4260580837726593 - trainLoss: 0.399463027715683\n",
      "cnt: 0 - valLoss: 0.42605769634246826 - trainLoss: 0.39946192502975464\n",
      "cnt: 0 - valLoss: 0.4260575771331787 - trainLoss: 0.3994607925415039\n",
      "cnt: 0 - valLoss: 0.42605727910995483 - trainLoss: 0.39945968985557556\n",
      "cnt: 0 - valLoss: 0.4260571599006653 - trainLoss: 0.3994586169719696\n",
      "cnt: 0 - valLoss: 0.42605680227279663 - trainLoss: 0.39945751428604126\n",
      "cnt: 0 - valLoss: 0.42605677247047424 - trainLoss: 0.3994563817977905\n",
      "cnt: 0 - valLoss: 0.4260563254356384 - trainLoss: 0.39945530891418457\n",
      "cnt: 0 - valLoss: 0.4260559380054474 - trainLoss: 0.3994542062282562\n",
      "cnt: 0 - valLoss: 0.4260558784008026 - trainLoss: 0.3994531035423279\n",
      "cnt: 0 - valLoss: 0.4260554909706116 - trainLoss: 0.39945197105407715\n",
      "cnt: 0 - valLoss: 0.4260554909706116 - trainLoss: 0.3994509279727936\n",
      "cnt: 0 - valLoss: 0.42605504393577576 - trainLoss: 0.39944979548454285\n",
      "cnt: 0 - valLoss: 0.4260549545288086 - trainLoss: 0.3994487226009369\n",
      "cnt: 0 - valLoss: 0.42605462670326233 - trainLoss: 0.39944761991500854\n",
      "cnt: 0 - valLoss: 0.42605453729629517 - trainLoss: 0.3994465172290802\n",
      "cnt: 0 - valLoss: 0.4260541796684265 - trainLoss: 0.39944538474082947\n",
      "cnt: 0 - valLoss: 0.42605412006378174 - trainLoss: 0.3994442820549011\n",
      "cnt: 0 - valLoss: 0.4260537326335907 - trainLoss: 0.39944320917129517\n",
      "cnt: 0 - valLoss: 0.42605364322662354 - trainLoss: 0.3994421064853668\n",
      "cnt: 0 - valLoss: 0.4260532557964325 - trainLoss: 0.3994409739971161\n",
      "cnt: 0 - valLoss: 0.4260531961917877 - trainLoss: 0.39943990111351013\n",
      "cnt: 0 - valLoss: 0.4260528087615967 - trainLoss: 0.3994387984275818\n",
      "cnt: 0 - valLoss: 0.4260524809360504 - trainLoss: 0.39943769574165344\n",
      "cnt: 0 - valLoss: 0.42605239152908325 - trainLoss: 0.3994365632534027\n",
      "cnt: 0 - valLoss: 0.4260520040988922 - trainLoss: 0.39943552017211914\n",
      "cnt: 0 - valLoss: 0.4260519742965698 - trainLoss: 0.3994343876838684\n",
      "cnt: 0 - valLoss: 0.4260515868663788 - trainLoss: 0.39943331480026245\n",
      "cnt: 0 - valLoss: 0.42605143785476685 - trainLoss: 0.3994322121143341\n",
      "cnt: 0 - valLoss: 0.42605113983154297 - trainLoss: 0.39943110942840576\n",
      "cnt: 0 - valLoss: 0.4260510206222534 - trainLoss: 0.39942997694015503\n",
      "cnt: 0 - valLoss: 0.42605069279670715 - trainLoss: 0.3994289040565491\n",
      "cnt: 0 - valLoss: 0.42605060338974 - trainLoss: 0.3994278311729431\n",
      "cnt: 0 - valLoss: 0.42605024576187134 - trainLoss: 0.39942672848701477\n",
      "cnt: 0 - valLoss: 0.4260498583316803 - trainLoss: 0.3994256258010864\n",
      "cnt: 0 - valLoss: 0.4260498285293579 - trainLoss: 0.3994244933128357\n",
      "cnt: 0 - valLoss: 0.42604944109916687 - trainLoss: 0.39942339062690735\n",
      "cnt: 0 - valLoss: 0.4260493814945221 - trainLoss: 0.3994223177433014\n",
      "cnt: 0 - valLoss: 0.42604905366897583 - trainLoss: 0.39942121505737305\n",
      "cnt: 0 - valLoss: 0.4260488748550415 - trainLoss: 0.3994201421737671\n",
      "cnt: 0 - valLoss: 0.42604854702949524 - trainLoss: 0.39941903948783875\n",
      "cnt: 0 - valLoss: 0.4260484576225281 - trainLoss: 0.399417906999588\n",
      "cnt: 0 - valLoss: 0.4260480999946594 - trainLoss: 0.39941683411598206\n",
      "cnt: 0 - valLoss: 0.42604801058769226 - trainLoss: 0.3994157314300537\n",
      "cnt: 0 - valLoss: 0.426047682762146 - trainLoss: 0.39941462874412537\n",
      "cnt: 0 - valLoss: 0.42604729533195496 - trainLoss: 0.3994135558605194\n",
      "cnt: 0 - valLoss: 0.4260472357273102 - trainLoss: 0.3994124233722687\n",
      "cnt: 0 - valLoss: 0.4260469079017639 - trainLoss: 0.39941132068634033\n",
      "cnt: 0 - valLoss: 0.42604678869247437 - trainLoss: 0.3994102478027344\n",
      "cnt: 0 - valLoss: 0.4260464012622833 - trainLoss: 0.39940914511680603\n",
      "cnt: 0 - valLoss: 0.42604634165763855 - trainLoss: 0.3994080126285553\n",
      "cnt: 0 - valLoss: 0.4260459542274475 - trainLoss: 0.39940696954727173\n",
      "cnt: 0 - valLoss: 0.4260459244251251 - trainLoss: 0.399405837059021\n",
      "cnt: 0 - valLoss: 0.4260455369949341 - trainLoss: 0.39940473437309265\n",
      "cnt: 0 - valLoss: 0.4260454475879669 - trainLoss: 0.3994036614894867\n",
      "cnt: 0 - valLoss: 0.42604511976242065 - trainLoss: 0.3994024991989136\n",
      "cnt: 0 - valLoss: 0.4260450303554535 - trainLoss: 0.3994014263153076\n",
      "cnt: 0 - valLoss: 0.42604464292526245 - trainLoss: 0.39940035343170166\n",
      "cnt: 0 - valLoss: 0.4260443449020386 - trainLoss: 0.3993992507457733\n",
      "cnt: 0 - valLoss: 0.4260442554950714 - trainLoss: 0.39939814805984497\n",
      "cnt: 0 - valLoss: 0.42604386806488037 - trainLoss: 0.399397075176239\n",
      "cnt: 0 - valLoss: 0.4260438084602356 - trainLoss: 0.3993959426879883\n",
      "cnt: 0 - valLoss: 0.42604342103004456 - trainLoss: 0.3993948996067047\n",
      "cnt: 0 - valLoss: 0.426043301820755 - trainLoss: 0.399393767118454\n",
      "cnt: 0 - valLoss: 0.42604297399520874 - trainLoss: 0.39939266443252563\n",
      "cnt: 0 - valLoss: 0.42604291439056396 - trainLoss: 0.3993915617465973\n",
      "cnt: 0 - valLoss: 0.4260425269603729 - trainLoss: 0.39939048886299133\n",
      "cnt: 0 - valLoss: 0.42604243755340576 - trainLoss: 0.3993893563747406\n",
      "cnt: 0 - valLoss: 0.4260421097278595 - trainLoss: 0.39938828349113464\n",
      "cnt: 0 - valLoss: 0.42604175209999084 - trainLoss: 0.3993871808052063\n",
      "cnt: 0 - valLoss: 0.42604169249534607 - trainLoss: 0.39938607811927795\n",
      "cnt: 0 - valLoss: 0.4260413348674774 - trainLoss: 0.3993849456310272\n",
      "cnt: 0 - valLoss: 0.42604121565818787 - trainLoss: 0.39938387274742126\n",
      "cnt: 0 - valLoss: 0.426040917634964 - trainLoss: 0.3993827700614929\n",
      "cnt: 0 - valLoss: 0.4260408282279968 - trainLoss: 0.39938169717788696\n",
      "cnt: 0 - valLoss: 0.426040381193161 - trainLoss: 0.3993805944919586\n",
      "cnt: 0 - valLoss: 0.426040381193161 - trainLoss: 0.3993794918060303\n",
      "cnt: 0 - valLoss: 0.4260399639606476 - trainLoss: 0.3993784189224243\n",
      "cnt: 0 - valLoss: 0.4260399341583252 - trainLoss: 0.3993772864341736\n",
      "cnt: 0 - valLoss: 0.42603954672813416 - trainLoss: 0.3993762135505676\n",
      "cnt: 0 - valLoss: 0.4260391891002655 - trainLoss: 0.3993751108646393\n",
      "cnt: 0 - valLoss: 0.42603909969329834 - trainLoss: 0.39937400817871094\n",
      "cnt: 0 - valLoss: 0.4260387718677521 - trainLoss: 0.3993728756904602\n",
      "cnt: 0 - valLoss: 0.4260386824607849 - trainLoss: 0.39937180280685425\n",
      "cnt: 0 - valLoss: 0.42603832483291626 - trainLoss: 0.3993707001209259\n",
      "cnt: 0 - valLoss: 0.4260382354259491 - trainLoss: 0.39936962723731995\n",
      "cnt: 0 - valLoss: 0.42603790760040283 - trainLoss: 0.3993685245513916\n",
      "cnt: 0 - valLoss: 0.4260377883911133 - trainLoss: 0.39936745166778564\n",
      "cnt: 0 - valLoss: 0.4260374903678894 - trainLoss: 0.3993663489818573\n",
      "cnt: 0 - valLoss: 0.42603713274002075 - trainLoss: 0.39936527609825134\n",
      "cnt: 0 - valLoss: 0.4260370135307312 - trainLoss: 0.3993641436100006\n",
      "cnt: 0 - valLoss: 0.4260367155075073 - trainLoss: 0.39936304092407227\n",
      "cnt: 0 - valLoss: 0.4260365664958954 - trainLoss: 0.3993619680404663\n",
      "cnt: 0 - valLoss: 0.42603617906570435 - trainLoss: 0.3993608057498932\n",
      "cnt: 0 - valLoss: 0.42603617906570435 - trainLoss: 0.399359792470932\n",
      "cnt: 0 - valLoss: 0.4260357916355133 - trainLoss: 0.39935868978500366\n",
      "cnt: 0 - valLoss: 0.42603567242622375 - trainLoss: 0.39935755729675293\n",
      "cnt: 0 - valLoss: 0.4260353744029999 - trainLoss: 0.399356484413147\n",
      "cnt: 0 - valLoss: 0.4260352849960327 - trainLoss: 0.39935538172721863\n",
      "cnt: 0 - valLoss: 0.4260348975658417 - trainLoss: 0.3993542790412903\n",
      "cnt: 0 - valLoss: 0.4260348975658417 - trainLoss: 0.3993532061576843\n",
      "cnt: 0 - valLoss: 0.42603451013565063 - trainLoss: 0.3993520736694336\n",
      "cnt: 0 - valLoss: 0.4260341227054596 - trainLoss: 0.39935097098350525\n",
      "cnt: 0 - valLoss: 0.4260341227054596 - trainLoss: 0.3993498980998993\n",
      "cnt: 0 - valLoss: 0.42603370547294617 - trainLoss: 0.39934879541397095\n",
      "cnt: 0 - valLoss: 0.426033616065979 - trainLoss: 0.399347722530365\n",
      "cnt: 0 - valLoss: 0.42603328824043274 - trainLoss: 0.39934661984443665\n",
      "cnt: 0 - valLoss: 0.4260331988334656 - trainLoss: 0.3993454873561859\n",
      "cnt: 0 - valLoss: 0.4260328412055969 - trainLoss: 0.39934441447257996\n",
      "cnt: 0 - valLoss: 0.42603281140327454 - trainLoss: 0.3993433117866516\n",
      "cnt: 0 - valLoss: 0.4260324537754059 - trainLoss: 0.39934223890304565\n",
      "cnt: 0 - valLoss: 0.42603206634521484 - trainLoss: 0.3993411362171173\n",
      "cnt: 0 - valLoss: 0.4260319769382477 - trainLoss: 0.3993400037288666\n",
      "cnt: 0 - valLoss: 0.4260316491127014 - trainLoss: 0.3993389308452606\n",
      "cnt: 0 - valLoss: 0.42603155970573425 - trainLoss: 0.3993378281593323\n",
      "cnt: 0 - valLoss: 0.426031231880188 - trainLoss: 0.3993367552757263\n",
      "cnt: 0 - valLoss: 0.4260311424732208 - trainLoss: 0.399335652589798\n",
      "cnt: 0 - valLoss: 0.4260307848453522 - trainLoss: 0.39933454990386963\n",
      "cnt: 0 - valLoss: 0.426030695438385 - trainLoss: 0.39933347702026367\n",
      "cnt: 0 - valLoss: 0.42603036761283875 - trainLoss: 0.3993324041366577\n",
      "cnt: 0 - valLoss: 0.4260300099849701 - trainLoss: 0.399331271648407\n",
      "cnt: 0 - valLoss: 0.42602992057800293 - trainLoss: 0.39933016896247864\n",
      "cnt: 0 - valLoss: 0.42602959275245667 - trainLoss: 0.3993290662765503\n",
      "cnt: 0 - valLoss: 0.4260295033454895 - trainLoss: 0.39932799339294434\n",
      "cnt: 0 - valLoss: 0.42602911591529846 - trainLoss: 0.3993269205093384\n",
      "cnt: 0 - valLoss: 0.4260290861129761 - trainLoss: 0.39932581782341003\n",
      "cnt: 0 - valLoss: 0.4260287284851074 - trainLoss: 0.3993246853351593\n",
      "cnt: 0 - valLoss: 0.42602863907814026 - trainLoss: 0.39932358264923096\n",
      "cnt: 0 - valLoss: 0.426028311252594 - trainLoss: 0.399322509765625\n",
      "cnt: 0 - valLoss: 0.42602822184562683 - trainLoss: 0.39932143688201904\n",
      "cnt: 0 - valLoss: 0.4260278344154358 - trainLoss: 0.3993203341960907\n",
      "cnt: 0 - valLoss: 0.4260278046131134 - trainLoss: 0.39931920170783997\n",
      "cnt: 0 - valLoss: 0.42602744698524475 - trainLoss: 0.3993181586265564\n",
      "cnt: 0 - valLoss: 0.4260270595550537 - trainLoss: 0.39931702613830566\n",
      "cnt: 0 - valLoss: 0.4260270297527313 - trainLoss: 0.3993159234523773\n",
      "cnt: 0 - valLoss: 0.42602667212486267 - trainLoss: 0.39931485056877136\n",
      "cnt: 0 - valLoss: 0.4260265827178955 - trainLoss: 0.399313747882843\n",
      "cnt: 0 - valLoss: 0.42602625489234924 - trainLoss: 0.3993126153945923\n",
      "cnt: 0 - valLoss: 0.4260261654853821 - trainLoss: 0.3993116021156311\n",
      "cnt: 0 - valLoss: 0.4260258078575134 - trainLoss: 0.39931046962738037\n",
      "cnt: 0 - valLoss: 0.42602574825286865 - trainLoss: 0.399309366941452\n",
      "cnt: 0 - valLoss: 0.426025390625 - trainLoss: 0.39930829405784607\n",
      "cnt: 0 - valLoss: 0.42602500319480896 - trainLoss: 0.3993071913719177\n",
      "cnt: 0 - valLoss: 0.4260249733924866 - trainLoss: 0.3993060886859894\n",
      "cnt: 0 - valLoss: 0.4260246157646179 - trainLoss: 0.3993050158023834\n",
      "cnt: 0 - valLoss: 0.42602452635765076 - trainLoss: 0.3993038833141327\n",
      "cnt: 0 - valLoss: 0.4260241389274597 - trainLoss: 0.39930281043052673\n",
      "cnt: 0 - valLoss: 0.42602410912513733 - trainLoss: 0.3993017375469208\n",
      "cnt: 0 - valLoss: 0.4260237514972687 - trainLoss: 0.39930063486099243\n",
      "cnt: 0 - valLoss: 0.4260236620903015 - trainLoss: 0.3992995321750641\n",
      "cnt: 0 - valLoss: 0.42602333426475525 - trainLoss: 0.39929839968681335\n",
      "cnt: 0 - valLoss: 0.4260229766368866 - trainLoss: 0.3992973566055298\n",
      "cnt: 0 - valLoss: 0.4260229170322418 - trainLoss: 0.39929628372192383\n",
      "cnt: 0 - valLoss: 0.42602255940437317 - trainLoss: 0.3992951512336731\n",
      "cnt: 0 - valLoss: 0.426022469997406 - trainLoss: 0.39929404854774475\n",
      "cnt: 0 - valLoss: 0.42602214217185974 - trainLoss: 0.3992929756641388\n",
      "cnt: 0 - valLoss: 0.4260220527648926 - trainLoss: 0.39929187297821045\n",
      "cnt: 0 - valLoss: 0.4260216951370239 - trainLoss: 0.3992907404899597\n",
      "cnt: 0 - valLoss: 0.4260215759277344 - trainLoss: 0.39928966760635376\n",
      "cnt: 0 - valLoss: 0.4260212182998657 - trainLoss: 0.3992886245250702\n",
      "cnt: 0 - valLoss: 0.42602118849754333 - trainLoss: 0.39928749203681946\n",
      "cnt: 0 - valLoss: 0.42602086067199707 - trainLoss: 0.3992864191532135\n",
      "cnt: 0 - valLoss: 0.4260205030441284 - trainLoss: 0.39928531646728516\n",
      "cnt: 0 - valLoss: 0.42602041363716125 - trainLoss: 0.3992842137813568\n",
      "cnt: 0 - valLoss: 0.426020085811615 - trainLoss: 0.39928314089775085\n",
      "cnt: 0 - valLoss: 0.4260200262069702 - trainLoss: 0.3992820680141449\n",
      "cnt: 0 - valLoss: 0.4260196387767792 - trainLoss: 0.39928093552589417\n",
      "cnt: 0 - valLoss: 0.426019549369812 - trainLoss: 0.3992798328399658\n",
      "cnt: 0 - valLoss: 0.42601922154426575 - trainLoss: 0.3992787301540375\n",
      "cnt: 0 - valLoss: 0.4260191321372986 - trainLoss: 0.3992776572704315\n",
      "cnt: 0 - valLoss: 0.4260188043117523 - trainLoss: 0.39927658438682556\n",
      "cnt: 0 - valLoss: 0.42601871490478516 - trainLoss: 0.3992754817008972\n",
      "cnt: 0 - valLoss: 0.4260184168815613 - trainLoss: 0.39927440881729126\n",
      "cnt: 0 - valLoss: 0.42601802945137024 - trainLoss: 0.3992732763290405\n",
      "cnt: 0 - valLoss: 0.4260179400444031 - trainLoss: 0.39927220344543457\n",
      "cnt: 0 - valLoss: 0.4260176718235016 - trainLoss: 0.3992711007595062\n",
      "cnt: 0 - valLoss: 0.42601749300956726 - trainLoss: 0.39927002787590027\n",
      "cnt: 0 - valLoss: 0.426017165184021 - trainLoss: 0.39926886558532715\n",
      "cnt: 0 - valLoss: 0.4260171055793762 - trainLoss: 0.39926785230636597\n",
      "cnt: 0 - valLoss: 0.42601677775382996 - trainLoss: 0.3992667496204376\n",
      "cnt: 0 - valLoss: 0.4260166585445404 - trainLoss: 0.3992656171321869\n",
      "cnt: 0 - valLoss: 0.42601636052131653 - trainLoss: 0.39926454424858093\n",
      "cnt: 0 - valLoss: 0.4260159730911255 - trainLoss: 0.39926350116729736\n",
      "cnt: 0 - valLoss: 0.4260159134864807 - trainLoss: 0.39926236867904663\n",
      "cnt: 0 - valLoss: 0.4260155260562897 - trainLoss: 0.3992612659931183\n",
      "cnt: 0 - valLoss: 0.4260154962539673 - trainLoss: 0.39926013350486755\n",
      "cnt: 0 - valLoss: 0.42601513862609863 - trainLoss: 0.399259090423584\n",
      "cnt: 0 - valLoss: 0.4260150194168091 - trainLoss: 0.399258017539978\n",
      "cnt: 0 - valLoss: 0.4260147511959076 - trainLoss: 0.3992568850517273\n",
      "cnt: 0 - valLoss: 0.42601466178894043 - trainLoss: 0.39925581216812134\n",
      "cnt: 0 - valLoss: 0.4260143041610718 - trainLoss: 0.399254709482193\n",
      "cnt: 0 - valLoss: 0.426014244556427 - trainLoss: 0.39925363659858704\n",
      "cnt: 0 - valLoss: 0.42601391673088074 - trainLoss: 0.3992525339126587\n",
      "cnt: 0 - valLoss: 0.4260135591030121 - trainLoss: 0.39925146102905273\n",
      "cnt: 0 - valLoss: 0.4260134696960449 - trainLoss: 0.3992503583431244\n",
      "cnt: 0 - valLoss: 0.42601314187049866 - trainLoss: 0.39924928545951843\n",
      "cnt: 0 - valLoss: 0.4260130524635315 - trainLoss: 0.3992481529712677\n",
      "cnt: 0 - valLoss: 0.42601266503334045 - trainLoss: 0.39924705028533936\n",
      "cnt: 0 - valLoss: 0.4260126054286957 - trainLoss: 0.3992459774017334\n",
      "cnt: 0 - valLoss: 0.4260122776031494 - trainLoss: 0.39924490451812744\n",
      "cnt: 0 - valLoss: 0.42601218819618225 - trainLoss: 0.3992438018321991\n",
      "cnt: 0 - valLoss: 0.4260118901729584 - trainLoss: 0.39924269914627075\n",
      "cnt: 0 - valLoss: 0.42601150274276733 - trainLoss: 0.3992416262626648\n",
      "cnt: 0 - valLoss: 0.42601141333580017 - trainLoss: 0.39924049377441406\n",
      "cnt: 0 - valLoss: 0.4260111153125763 - trainLoss: 0.3992394208908081\n",
      "cnt: 0 - valLoss: 0.42601099610328674 - trainLoss: 0.39923831820487976\n",
      "cnt: 0 - valLoss: 0.4260106384754181 - trainLoss: 0.3992372155189514\n",
      "cnt: 0 - valLoss: 0.4260106086730957 - trainLoss: 0.39923617243766785\n",
      "cnt: 0 - valLoss: 0.42601022124290466 - trainLoss: 0.3992350697517395\n",
      "cnt: 0 - valLoss: 0.4260101616382599 - trainLoss: 0.39923396706581116\n",
      "cnt: 0 - valLoss: 0.42600977420806885 - trainLoss: 0.3992328941822052\n",
      "cnt: 0 - valLoss: 0.42600977420806885 - trainLoss: 0.39923182129859924\n",
      "cnt: 0 - valLoss: 0.4260094165802002 - trainLoss: 0.3992306888103485\n",
      "cnt: 0 - valLoss: 0.42600902915000916 - trainLoss: 0.39922958612442017\n",
      "cnt: 0 - valLoss: 0.42600902915000916 - trainLoss: 0.399228572845459\n",
      "cnt: 0 - valLoss: 0.4260086715221405 - trainLoss: 0.39922744035720825\n",
      "cnt: 0 - valLoss: 0.42600858211517334 - trainLoss: 0.3992263376712799\n",
      "cnt: 0 - valLoss: 0.4260082542896271 - trainLoss: 0.39922526478767395\n",
      "cnt: 0 - valLoss: 0.4260081648826599 - trainLoss: 0.399224191904068\n",
      "cnt: 0 - valLoss: 0.42600780725479126 - trainLoss: 0.39922308921813965\n",
      "cnt: 0 - valLoss: 0.426007479429245 - trainLoss: 0.3992219567298889\n",
      "cnt: 0 - valLoss: 0.4260074198246002 - trainLoss: 0.39922088384628296\n",
      "cnt: 0 - valLoss: 0.4260070323944092 - trainLoss: 0.3992197811603546\n",
      "cnt: 0 - valLoss: 0.4260070323944092 - trainLoss: 0.39921870827674866\n",
      "cnt: 0 - valLoss: 0.4260067045688629 - trainLoss: 0.3992176353931427\n",
      "cnt: 0 - valLoss: 0.42600661516189575 - trainLoss: 0.39921653270721436\n",
      "cnt: 0 - valLoss: 0.4260062873363495 - trainLoss: 0.399215430021286\n",
      "cnt: 0 - valLoss: 0.4260061979293823 - trainLoss: 0.39921435713768005\n",
      "cnt: 0 - valLoss: 0.42600584030151367 - trainLoss: 0.3992132246494293\n",
      "cnt: 0 - valLoss: 0.4260055124759674 - trainLoss: 0.399212121963501\n",
      "cnt: 0 - valLoss: 0.42600545287132263 - trainLoss: 0.3992111086845398\n",
      "cnt: 0 - valLoss: 0.42600512504577637 - trainLoss: 0.39920997619628906\n",
      "cnt: 0 - valLoss: 0.4260050356388092 - trainLoss: 0.3992088735103607\n",
      "cnt: 0 - valLoss: 0.42600464820861816 - trainLoss: 0.39920780062675476\n",
      "cnt: 0 - valLoss: 0.42600464820861816 - trainLoss: 0.3992067277431488\n",
      "cnt: 0 - valLoss: 0.4260042905807495 - trainLoss: 0.39920562505722046\n",
      "cnt: 0 - valLoss: 0.42600396275520325 - trainLoss: 0.3992044925689697\n",
      "cnt: 0 - valLoss: 0.42600390315055847 - trainLoss: 0.39920347929000854\n",
      "cnt: 0 - valLoss: 0.4260035455226898 - trainLoss: 0.3992023766040802\n",
      "cnt: 0 - valLoss: 0.42600348591804504 - trainLoss: 0.39920124411582947\n",
      "cnt: 0 - valLoss: 0.42600318789482117 - trainLoss: 0.3992001712322235\n",
      "cnt: 0 - valLoss: 0.426003098487854 - trainLoss: 0.39919909834861755\n",
      "cnt: 0 - valLoss: 0.42600277066230774 - trainLoss: 0.3991979956626892\n",
      "cnt: 0 - valLoss: 0.4260026812553406 - trainLoss: 0.39919692277908325\n",
      "cnt: 0 - valLoss: 0.4260023236274719 - trainLoss: 0.39919576048851013\n",
      "cnt: 0 - valLoss: 0.42600199580192566 - trainLoss: 0.39919474720954895\n",
      "cnt: 0 - valLoss: 0.4260019361972809 - trainLoss: 0.3991936445236206\n",
      "cnt: 0 - valLoss: 0.4260016083717346 - trainLoss: 0.3991925120353699\n",
      "cnt: 0 - valLoss: 0.42600154876708984 - trainLoss: 0.3991914987564087\n",
      "cnt: 0 - valLoss: 0.4260011613368988 - trainLoss: 0.39919036626815796\n",
      "cnt: 0 - valLoss: 0.4260011315345764 - trainLoss: 0.3991892635822296\n",
      "cnt: 0 - valLoss: 0.42600077390670776 - trainLoss: 0.39918819069862366\n",
      "cnt: 0 - valLoss: 0.4260004460811615 - trainLoss: 0.3991870880126953\n",
      "cnt: 0 - valLoss: 0.4260004460811615 - trainLoss: 0.39918601512908936\n",
      "cnt: 0 - valLoss: 0.42600008845329285 - trainLoss: 0.3991849422454834\n",
      "cnt: 0 - valLoss: 0.42600002884864807 - trainLoss: 0.39918383955955505\n",
      "cnt: 0 - valLoss: 0.4259996712207794 - trainLoss: 0.3991827666759491\n",
      "cnt: 0 - valLoss: 0.4259996712207794 - trainLoss: 0.39918166399002075\n",
      "cnt: 0 - valLoss: 0.42599934339523315 - trainLoss: 0.3991805911064148\n",
      "cnt: 0 - valLoss: 0.425999253988266 - trainLoss: 0.39917945861816406\n",
      "cnt: 0 - valLoss: 0.4259989857673645 - trainLoss: 0.3991783857345581\n",
      "cnt: 0 - valLoss: 0.4259985685348511 - trainLoss: 0.39917728304862976\n",
      "cnt: 0 - valLoss: 0.4259985685348511 - trainLoss: 0.3991762101650238\n",
      "cnt: 0 - valLoss: 0.4259982705116272 - trainLoss: 0.39917513728141785\n",
      "cnt: 0 - valLoss: 0.4259982109069824 - trainLoss: 0.3991740345954895\n",
      "cnt: 0 - valLoss: 0.42599788308143616 - trainLoss: 0.39917293190956116\n",
      "cnt: 0 - valLoss: 0.42599788308143616 - trainLoss: 0.3991717994213104\n",
      "cnt: 0 - valLoss: 0.4259974956512451 - trainLoss: 0.39917078614234924\n",
      "cnt: 0 - valLoss: 0.42599713802337646 - trainLoss: 0.3991696536540985\n",
      "cnt: 0 - valLoss: 0.4259970486164093 - trainLoss: 0.39916858077049255\n",
      "cnt: 0 - valLoss: 0.4259967505931854 - trainLoss: 0.3991674780845642\n",
      "cnt: 0 - valLoss: 0.42599672079086304 - trainLoss: 0.39916640520095825\n",
      "cnt: 0 - valLoss: 0.4259963631629944 - trainLoss: 0.3991653323173523\n",
      "cnt: 0 - valLoss: 0.425996333360672 - trainLoss: 0.39916422963142395\n",
      "cnt: 0 - valLoss: 0.42599597573280334 - trainLoss: 0.399163156747818\n",
      "cnt: 0 - valLoss: 0.42599591612815857 - trainLoss: 0.39916205406188965\n",
      "cnt: 0 - valLoss: 0.4259955883026123 - trainLoss: 0.3991609811782837\n",
      "cnt: 0 - valLoss: 0.42599526047706604 - trainLoss: 0.39915990829467773\n",
      "cnt: 0 - valLoss: 0.42599523067474365 - trainLoss: 0.3991588056087494\n",
      "cnt: 0 - valLoss: 0.425994873046875 - trainLoss: 0.39915767312049866\n",
      "cnt: 0 - valLoss: 0.425994873046875 - trainLoss: 0.3991566002368927\n",
      "cnt: 0 - valLoss: 0.42599448561668396 - trainLoss: 0.39915552735328674\n",
      "cnt: 0 - valLoss: 0.4259944558143616 - trainLoss: 0.3991544246673584\n",
      "cnt: 0 - valLoss: 0.4259940981864929 - trainLoss: 0.39915335178375244\n",
      "cnt: 0 - valLoss: 0.42599406838417053 - trainLoss: 0.3991522490978241\n",
      "cnt: 0 - valLoss: 0.42599377036094666 - trainLoss: 0.39915117621421814\n",
      "cnt: 0 - valLoss: 0.4259933829307556 - trainLoss: 0.3991500735282898\n",
      "cnt: 0 - valLoss: 0.4259933829307556 - trainLoss: 0.39914900064468384\n",
      "cnt: 0 - valLoss: 0.4259929955005646 - trainLoss: 0.3991478681564331\n",
      "cnt: 0 - valLoss: 0.4259929955005646 - trainLoss: 0.39914679527282715\n",
      "cnt: 0 - valLoss: 0.4259926378726959 - trainLoss: 0.3991457521915436\n",
      "cnt: 0 - valLoss: 0.42599254846572876 - trainLoss: 0.39914461970329285\n",
      "cnt: 0 - valLoss: 0.4259922206401825 - trainLoss: 0.3991435468196869\n",
      "cnt: 0 - valLoss: 0.4259922206401825 - trainLoss: 0.39914244413375854\n",
      "cnt: 0 - valLoss: 0.42599186301231384 - trainLoss: 0.3991413712501526\n",
      "cnt: 0 - valLoss: 0.4259915351867676 - trainLoss: 0.39914029836654663\n",
      "cnt: 0 - valLoss: 0.4259914755821228 - trainLoss: 0.3991391956806183\n",
      "cnt: 0 - valLoss: 0.4259911775588989 - trainLoss: 0.39913806319236755\n",
      "cnt: 0 - valLoss: 0.42599108815193176 - trainLoss: 0.39913704991340637\n",
      "cnt: 0 - valLoss: 0.4259907901287079 - trainLoss: 0.399135947227478\n",
      "cnt: 0 - valLoss: 0.4259907007217407 - trainLoss: 0.3991348147392273\n",
      "cnt: 0 - valLoss: 0.42599040269851685 - trainLoss: 0.39913374185562134\n",
      "cnt: 0 - valLoss: 0.4259900748729706 - trainLoss: 0.39913269877433777\n",
      "cnt: 0 - valLoss: 0.4259900748729706 - trainLoss: 0.39913156628608704\n",
      "cnt: 0 - valLoss: 0.42598971724510193 - trainLoss: 0.3991304934024811\n",
      "cnt: 0 - valLoss: 0.42598965764045715 - trainLoss: 0.3991294205188751\n",
      "cnt: 0 - valLoss: 0.4259893000125885 - trainLoss: 0.39912834763526917\n",
      "cnt: 0 - valLoss: 0.4259893000125885 - trainLoss: 0.39912721514701843\n",
      "cnt: 0 - valLoss: 0.42598891258239746 - trainLoss: 0.39912617206573486\n",
      "cnt: 0 - valLoss: 0.4259888827800751 - trainLoss: 0.3991250693798065\n",
      "cnt: 0 - valLoss: 0.42598849534988403 - trainLoss: 0.3991239666938782\n",
      "cnt: 0 - valLoss: 0.42598819732666016 - trainLoss: 0.3991228938102722\n",
      "cnt: 0 - valLoss: 0.4259881377220154 - trainLoss: 0.39912182092666626\n",
      "cnt: 0 - valLoss: 0.4259878098964691 - trainLoss: 0.3991207480430603\n",
      "cnt: 0 - valLoss: 0.42598775029182434 - trainLoss: 0.39911961555480957\n",
      "cnt: 0 - valLoss: 0.42598745226860046 - trainLoss: 0.399118572473526\n",
      "cnt: 0 - valLoss: 0.4259874224662781 - trainLoss: 0.39911743998527527\n",
      "cnt: 0 - valLoss: 0.42598703503608704 - trainLoss: 0.3991163372993469\n",
      "cnt: 0 - valLoss: 0.42598703503608704 - trainLoss: 0.39911526441574097\n",
      "cnt: 0 - valLoss: 0.4259866774082184 - trainLoss: 0.399114191532135\n",
      "cnt: 0 - valLoss: 0.42598628997802734 - trainLoss: 0.39911311864852905\n",
      "cnt: 0 - valLoss: 0.4259863495826721 - trainLoss: 0.3991120457649231\n",
      "cnt: 1 - valLoss: 0.4259859621524811 - trainLoss: 0.39911094307899475\n",
      "cnt: 0 - valLoss: 0.4259859025478363 - trainLoss: 0.3991098701953888\n",
      "cnt: 0 - valLoss: 0.4259856045246124 - trainLoss: 0.39910876750946045\n",
      "cnt: 0 - valLoss: 0.42598527669906616 - trainLoss: 0.3991076946258545\n",
      "cnt: 0 - valLoss: 0.4259853959083557 - trainLoss: 0.39910662174224854\n",
      "cnt: 1 - valLoss: 0.425985187292099 - trainLoss: 0.3991055190563202\n",
      "cnt: 0 - valLoss: 0.4259850084781647 - trainLoss: 0.39910444617271423\n",
      "cnt: 0 - valLoss: 0.42598482966423035 - trainLoss: 0.3991033136844635\n",
      "cnt: 0 - valLoss: 0.4259846806526184 - trainLoss: 0.39910224080085754\n",
      "cnt: 0 - valLoss: 0.4259844124317169 - trainLoss: 0.3991011679172516\n",
      "cnt: 0 - valLoss: 0.42598429322242737 - trainLoss: 0.39910006523132324\n",
      "cnt: 0 - valLoss: 0.42598411440849304 - trainLoss: 0.3990989923477173\n",
      "cnt: 0 - valLoss: 0.4259839355945587 - trainLoss: 0.39909791946411133\n",
      "cnt: 0 - valLoss: 0.425983726978302 - trainLoss: 0.39909684658050537\n",
      "cnt: 0 - valLoss: 0.4259835183620453 - trainLoss: 0.3990958034992218\n",
      "cnt: 0 - valLoss: 0.42598333954811096 - trainLoss: 0.39909467101097107\n",
      "cnt: 0 - valLoss: 0.4259832203388214 - trainLoss: 0.3990935683250427\n",
      "cnt: 0 - valLoss: 0.4259830117225647 - trainLoss: 0.39909252524375916\n",
      "cnt: 0 - valLoss: 0.4259827733039856 - trainLoss: 0.3990914225578308\n",
      "cnt: 0 - valLoss: 0.42598140239715576 - trainLoss: 0.39909034967422485\n",
      "cnt: 0 - valLoss: 0.42598047852516174 - trainLoss: 0.39908918738365173\n",
      "cnt: 0 - valLoss: 0.4259794354438782 - trainLoss: 0.399088054895401\n",
      "cnt: 0 - valLoss: 0.4259786307811737 - trainLoss: 0.3990868926048279\n",
      "cnt: 0 - valLoss: 0.42597806453704834 - trainLoss: 0.3990858197212219\n",
      "cnt: 0 - valLoss: 0.42597755789756775 - trainLoss: 0.3990847170352936\n",
      "cnt: 0 - valLoss: 0.42597696185112 - trainLoss: 0.39908358454704285\n",
      "cnt: 0 - valLoss: 0.4259764552116394 - trainLoss: 0.3990824818611145\n",
      "cnt: 0 - valLoss: 0.42597588896751404 - trainLoss: 0.39908140897750854\n",
      "cnt: 0 - valLoss: 0.42597532272338867 - trainLoss: 0.3990803062915802\n",
      "cnt: 0 - valLoss: 0.4259748160839081 - trainLoss: 0.39907917380332947\n",
      "cnt: 0 - valLoss: 0.4259743094444275 - trainLoss: 0.3990780711174011\n",
      "cnt: 0 - valLoss: 0.4259737730026245 - trainLoss: 0.3990769684314728\n",
      "cnt: 0 - valLoss: 0.42597317695617676 - trainLoss: 0.39907583594322205\n",
      "cnt: 0 - valLoss: 0.42597267031669617 - trainLoss: 0.3990747630596161\n",
      "cnt: 0 - valLoss: 0.4259721636772156 - trainLoss: 0.39907366037368774\n",
      "cnt: 0 - valLoss: 0.4259716272354126 - trainLoss: 0.3990725576877594\n",
      "cnt: 0 - valLoss: 0.42597103118896484 - trainLoss: 0.39907142519950867\n",
      "cnt: 0 - valLoss: 0.42597058415412903 - trainLoss: 0.3990703225135803\n",
      "cnt: 0 - valLoss: 0.42597001791000366 - trainLoss: 0.39906924962997437\n",
      "cnt: 0 - valLoss: 0.42596951127052307 - trainLoss: 0.399068146944046\n",
      "cnt: 0 - valLoss: 0.4259689748287201 - trainLoss: 0.3990670144557953\n",
      "cnt: 0 - valLoss: 0.4259684681892395 - trainLoss: 0.39906591176986694\n",
      "cnt: 0 - valLoss: 0.4259679615497589 - trainLoss: 0.399064838886261\n",
      "cnt: 0 - valLoss: 0.4259674549102783 - trainLoss: 0.39906373620033264\n",
      "cnt: 0 - valLoss: 0.42596691846847534 - trainLoss: 0.3990626037120819\n",
      "cnt: 0 - valLoss: 0.42596638202667236 - trainLoss: 0.39906150102615356\n",
      "cnt: 0 - valLoss: 0.4259658455848694 - trainLoss: 0.3990604281425476\n",
      "cnt: 0 - valLoss: 0.42596539855003357 - trainLoss: 0.39905932545661926\n",
      "cnt: 0 - valLoss: 0.4259648621082306 - trainLoss: 0.3990582227706909\n",
      "cnt: 0 - valLoss: 0.42596435546875 - trainLoss: 0.3990570902824402\n",
      "cnt: 0 - valLoss: 0.42596396803855896 - trainLoss: 0.39905601739883423\n",
      "cnt: 0 - valLoss: 0.42596355080604553 - trainLoss: 0.3990549147129059\n",
      "cnt: 0 - valLoss: 0.4259631037712097 - trainLoss: 0.39905381202697754\n",
      "cnt: 0 - valLoss: 0.4259627163410187 - trainLoss: 0.3990526795387268\n",
      "cnt: 0 - valLoss: 0.42596229910850525 - trainLoss: 0.39905157685279846\n",
      "cnt: 0 - valLoss: 0.4259619414806366 - trainLoss: 0.3990505039691925\n",
      "cnt: 0 - valLoss: 0.42596161365509033 - trainLoss: 0.39904940128326416\n",
      "cnt: 0 - valLoss: 0.42596110701560974 - trainLoss: 0.3990482687950134\n",
      "cnt: 0 - valLoss: 0.4259607493877411 - trainLoss: 0.39904719591140747\n",
      "cnt: 0 - valLoss: 0.42596036195755005 - trainLoss: 0.39904606342315674\n",
      "cnt: 0 - valLoss: 0.425959974527359 - trainLoss: 0.39904502034187317\n",
      "cnt: 0 - valLoss: 0.4259594976902008 - trainLoss: 0.3990439176559448\n",
      "cnt: 0 - valLoss: 0.42595916986465454 - trainLoss: 0.3990428149700165\n",
      "cnt: 0 - valLoss: 0.4259587824344635 - trainLoss: 0.39904168248176575\n",
      "cnt: 0 - valLoss: 0.4259583652019501 - trainLoss: 0.3990405797958374\n",
      "cnt: 0 - valLoss: 0.4259580075740814 - trainLoss: 0.39903950691223145\n",
      "cnt: 0 - valLoss: 0.425957590341568 - trainLoss: 0.3990384042263031\n",
      "cnt: 0 - valLoss: 0.42595720291137695 - trainLoss: 0.39903733134269714\n",
      "cnt: 0 - valLoss: 0.4259568154811859 - trainLoss: 0.3990361988544464\n",
      "cnt: 0 - valLoss: 0.42595645785331726 - trainLoss: 0.39903509616851807\n",
      "cnt: 0 - valLoss: 0.42595604062080383 - trainLoss: 0.3990339934825897\n",
      "cnt: 0 - valLoss: 0.4259556531906128 - trainLoss: 0.399032860994339\n",
      "cnt: 0 - valLoss: 0.42595526576042175 - trainLoss: 0.3990318477153778\n",
      "cnt: 0 - valLoss: 0.4259549379348755 - trainLoss: 0.39903074502944946\n",
      "cnt: 0 - valLoss: 0.4259544909000397 - trainLoss: 0.39902958273887634\n",
      "cnt: 0 - valLoss: 0.42595407366752625 - trainLoss: 0.3990285098552704\n",
      "cnt: 0 - valLoss: 0.4259537160396576 - trainLoss: 0.39902740716934204\n",
      "cnt: 0 - valLoss: 0.42595338821411133 - trainLoss: 0.3990262746810913\n",
      "cnt: 0 - valLoss: 0.4259530007839203 - trainLoss: 0.39902520179748535\n",
      "cnt: 0 - valLoss: 0.4259525537490845 - trainLoss: 0.3990241289138794\n",
      "cnt: 0 - valLoss: 0.4259521961212158 - trainLoss: 0.39902302622795105\n",
      "cnt: 0 - valLoss: 0.42595183849334717 - trainLoss: 0.3990219235420227\n",
      "cnt: 0 - valLoss: 0.42595142126083374 - trainLoss: 0.399020791053772\n",
      "cnt: 0 - valLoss: 0.4259510636329651 - trainLoss: 0.39901968836784363\n",
      "cnt: 0 - valLoss: 0.42595064640045166 - trainLoss: 0.39901861548423767\n",
      "cnt: 0 - valLoss: 0.425950288772583 - trainLoss: 0.3990175127983093\n",
      "cnt: 0 - valLoss: 0.4259498715400696 - trainLoss: 0.39901643991470337\n",
      "cnt: 0 - valLoss: 0.4259495139122009 - trainLoss: 0.39901530742645264\n",
      "cnt: 0 - valLoss: 0.42594918608665466 - trainLoss: 0.3990142047405243\n",
      "cnt: 0 - valLoss: 0.4259487986564636 - trainLoss: 0.39901313185691833\n",
      "cnt: 0 - valLoss: 0.4259484112262726 - trainLoss: 0.39901202917099\n",
      "cnt: 0 - valLoss: 0.42594802379608154 - trainLoss: 0.39901095628738403\n",
      "cnt: 0 - valLoss: 0.4259476363658905 - trainLoss: 0.3990097939968109\n",
      "cnt: 0 - valLoss: 0.42594730854034424 - trainLoss: 0.39900872111320496\n",
      "cnt: 0 - valLoss: 0.4259469211101532 - trainLoss: 0.3990076184272766\n",
      "cnt: 0 - valLoss: 0.42594656348228455 - trainLoss: 0.39900651574134827\n",
      "cnt: 0 - valLoss: 0.4259461760520935 - trainLoss: 0.3990054428577423\n",
      "cnt: 0 - valLoss: 0.42594584822654724 - trainLoss: 0.39900436997413635\n",
      "cnt: 0 - valLoss: 0.4259454011917114 - trainLoss: 0.3990032374858856\n",
      "cnt: 0 - valLoss: 0.42594507336616516 - trainLoss: 0.3990021347999573\n",
      "cnt: 0 - valLoss: 0.4259446859359741 - trainLoss: 0.39900103211402893\n",
      "cnt: 0 - valLoss: 0.42594438791275024 - trainLoss: 0.398999959230423\n",
      "cnt: 0 - valLoss: 0.42594394087791443 - trainLoss: 0.398998886346817\n",
      "cnt: 0 - valLoss: 0.42594361305236816 - trainLoss: 0.3989977240562439\n",
      "cnt: 0 - valLoss: 0.4259432554244995 - trainLoss: 0.39899665117263794\n",
      "cnt: 0 - valLoss: 0.4259428381919861 - trainLoss: 0.3989955484867096\n",
      "cnt: 0 - valLoss: 0.4259425103664398 - trainLoss: 0.39899444580078125\n",
      "cnt: 0 - valLoss: 0.42594215273857117 - trainLoss: 0.3989933133125305\n",
      "cnt: 0 - valLoss: 0.4259418249130249 - trainLoss: 0.39899224042892456\n",
      "cnt: 0 - valLoss: 0.42594143748283386 - trainLoss: 0.3989911675453186\n",
      "cnt: 0 - valLoss: 0.4259410500526428 - trainLoss: 0.39899006485939026\n",
      "cnt: 0 - valLoss: 0.42594069242477417 - trainLoss: 0.3989889621734619\n",
      "cnt: 0 - valLoss: 0.4259403645992279 - trainLoss: 0.39898788928985596\n",
      "cnt: 0 - valLoss: 0.42594000697135925 - trainLoss: 0.39898681640625\n",
      "cnt: 0 - valLoss: 0.425939679145813 - trainLoss: 0.3989856541156769\n",
      "cnt: 0 - valLoss: 0.42593932151794434 - trainLoss: 0.3989845812320709\n",
      "cnt: 0 - valLoss: 0.4259389042854309 - trainLoss: 0.3989834785461426\n",
      "cnt: 0 - valLoss: 0.42593860626220703 - trainLoss: 0.39898237586021423\n",
      "cnt: 0 - valLoss: 0.4259382486343384 - trainLoss: 0.3989813029766083\n",
      "cnt: 0 - valLoss: 0.42593786120414734 - trainLoss: 0.39898017048835754\n",
      "cnt: 0 - valLoss: 0.4259375333786011 - trainLoss: 0.3989790976047516\n",
      "cnt: 0 - valLoss: 0.4259371757507324 - trainLoss: 0.39897799491882324\n",
      "cnt: 0 - valLoss: 0.42593684792518616 - trainLoss: 0.3989768922328949\n",
      "cnt: 0 - valLoss: 0.4259364604949951 - trainLoss: 0.39897575974464417\n",
      "cnt: 0 - valLoss: 0.42593616247177124 - trainLoss: 0.3989747166633606\n",
      "cnt: 0 - valLoss: 0.4259358048439026 - trainLoss: 0.39897358417510986\n",
      "cnt: 0 - valLoss: 0.42593541741371155 - trainLoss: 0.3989725112915039\n",
      "cnt: 0 - valLoss: 0.42593511939048767 - trainLoss: 0.39897140860557556\n",
      "cnt: 0 - valLoss: 0.4259347915649414 - trainLoss: 0.3989703357219696\n",
      "cnt: 0 - valLoss: 0.42593449354171753 - trainLoss: 0.39896926283836365\n",
      "cnt: 0 - valLoss: 0.4259341061115265 - trainLoss: 0.3989681601524353\n",
      "cnt: 0 - valLoss: 0.4259338080883026 - trainLoss: 0.39896702766418457\n",
      "cnt: 0 - valLoss: 0.42593348026275635 - trainLoss: 0.398965984582901\n",
      "cnt: 0 - valLoss: 0.4259331226348877 - trainLoss: 0.39896485209465027\n",
      "cnt: 0 - valLoss: 0.42593279480934143 - trainLoss: 0.3989637494087219\n",
      "cnt: 0 - valLoss: 0.42593246698379517 - trainLoss: 0.3989626467227936\n",
      "cnt: 0 - valLoss: 0.4259321093559265 - trainLoss: 0.3989615738391876\n",
      "cnt: 0 - valLoss: 0.42593178153038025 - trainLoss: 0.3989604413509369\n",
      "cnt: 0 - valLoss: 0.4259314239025116 - trainLoss: 0.39895936846733093\n",
      "cnt: 0 - valLoss: 0.4259311854839325 - trainLoss: 0.39895832538604736\n",
      "cnt: 0 - valLoss: 0.42593079805374146 - trainLoss: 0.39895716309547424\n",
      "cnt: 0 - valLoss: 0.4259304404258728 - trainLoss: 0.3989560902118683\n",
      "cnt: 0 - valLoss: 0.4259301424026489 - trainLoss: 0.39895495772361755\n",
      "cnt: 0 - valLoss: 0.42592981457710266 - trainLoss: 0.398953914642334\n",
      "cnt: 0 - valLoss: 0.425929456949234 - trainLoss: 0.39895278215408325\n",
      "cnt: 0 - valLoss: 0.4259292185306549 - trainLoss: 0.3989517092704773\n",
      "cnt: 0 - valLoss: 0.42592886090278625 - trainLoss: 0.39895060658454895\n",
      "cnt: 0 - valLoss: 0.42592853307724 - trainLoss: 0.3989495038986206\n",
      "cnt: 0 - valLoss: 0.42592817544937134 - trainLoss: 0.39894843101501465\n",
      "cnt: 0 - valLoss: 0.42592787742614746 - trainLoss: 0.3989473581314087\n",
      "cnt: 0 - valLoss: 0.4259274899959564 - trainLoss: 0.39894622564315796\n",
      "cnt: 0 - valLoss: 0.4259272515773773 - trainLoss: 0.3989451229572296\n",
      "cnt: 0 - valLoss: 0.42592689394950867 - trainLoss: 0.39894405007362366\n",
      "cnt: 0 - valLoss: 0.4259265661239624 - trainLoss: 0.3989429473876953\n",
      "cnt: 0 - valLoss: 0.42592623829841614 - trainLoss: 0.39894184470176697\n",
      "cnt: 0 - valLoss: 0.42592594027519226 - trainLoss: 0.398940771818161\n",
      "cnt: 0 - valLoss: 0.4259255528450012 - trainLoss: 0.3989396393299103\n",
      "cnt: 0 - valLoss: 0.4259253144264221 - trainLoss: 0.3989385664463043\n",
      "cnt: 0 - valLoss: 0.42592495679855347 - trainLoss: 0.398937463760376\n",
      "cnt: 0 - valLoss: 0.4259246289730072 - trainLoss: 0.39893639087677\n",
      "cnt: 0 - valLoss: 0.4259243309497833 - trainLoss: 0.3989352881908417\n",
      "cnt: 0 - valLoss: 0.42592403292655945 - trainLoss: 0.39893415570259094\n",
      "cnt: 0 - valLoss: 0.42592373490333557 - trainLoss: 0.3989331126213074\n",
      "cnt: 0 - valLoss: 0.42592334747314453 - trainLoss: 0.39893198013305664\n",
      "cnt: 0 - valLoss: 0.42592304944992065 - trainLoss: 0.3989308774471283\n",
      "cnt: 0 - valLoss: 0.4259227514266968 - trainLoss: 0.39892980456352234\n",
      "cnt: 0 - valLoss: 0.4259223937988281 - trainLoss: 0.398928701877594\n",
      "cnt: 0 - valLoss: 0.42592209577560425 - trainLoss: 0.39892762899398804\n",
      "cnt: 0 - valLoss: 0.42592179775238037 - trainLoss: 0.3989265561103821\n",
      "cnt: 0 - valLoss: 0.4259214997291565 - trainLoss: 0.39892545342445374\n",
      "cnt: 0 - valLoss: 0.42592114210128784 - trainLoss: 0.398924320936203\n",
      "cnt: 0 - valLoss: 0.42592084407806396 - trainLoss: 0.39892324805259705\n",
      "cnt: 0 - valLoss: 0.4259204566478729 - trainLoss: 0.3989221453666687\n",
      "cnt: 0 - valLoss: 0.4259202182292938 - trainLoss: 0.39892107248306274\n",
      "cnt: 0 - valLoss: 0.42591992020606995 - trainLoss: 0.3989199697971344\n",
      "cnt: 0 - valLoss: 0.4259195625782013 - trainLoss: 0.39891883730888367\n",
      "cnt: 0 - valLoss: 0.4259192645549774 - trainLoss: 0.3989178240299225\n",
      "cnt: 0 - valLoss: 0.4259189963340759 - trainLoss: 0.39891672134399414\n",
      "cnt: 0 - valLoss: 0.42591866850852966 - trainLoss: 0.3989155888557434\n",
      "cnt: 0 - valLoss: 0.4259183704853058 - trainLoss: 0.39891448616981506\n",
      "cnt: 0 - valLoss: 0.4259180724620819 - trainLoss: 0.3989134132862091\n",
      "cnt: 0 - valLoss: 0.42591771483421326 - trainLoss: 0.39891234040260315\n",
      "cnt: 0 - valLoss: 0.42591747641563416 - trainLoss: 0.3989112377166748\n",
      "cnt: 0 - valLoss: 0.4259171783924103 - trainLoss: 0.3989101052284241\n",
      "cnt: 0 - valLoss: 0.425916850566864 - trainLoss: 0.3989090025424957\n",
      "cnt: 0 - valLoss: 0.42591652274131775 - trainLoss: 0.39890792965888977\n",
      "cnt: 0 - valLoss: 0.42591625452041626 - trainLoss: 0.3989068567752838\n",
      "cnt: 0 - valLoss: 0.42591592669487 - trainLoss: 0.39890575408935547\n",
      "cnt: 0 - valLoss: 0.4259156584739685 - trainLoss: 0.3989046812057495\n",
      "cnt: 0 - valLoss: 0.42591533064842224 - trainLoss: 0.39890357851982117\n",
      "cnt: 0 - valLoss: 0.4259149730205536 - trainLoss: 0.3989025056362152\n",
      "cnt: 0 - valLoss: 0.4259147346019745 - trainLoss: 0.3989013731479645\n",
      "cnt: 0 - valLoss: 0.4259144067764282 - trainLoss: 0.39890027046203613\n",
      "cnt: 0 - valLoss: 0.42591410875320435 - trainLoss: 0.3988991975784302\n",
      "cnt: 0 - valLoss: 0.42591381072998047 - trainLoss: 0.3988981246948242\n",
      "cnt: 0 - valLoss: 0.42591357231140137 - trainLoss: 0.3988970220088959\n",
      "cnt: 0 - valLoss: 0.4259132146835327 - trainLoss: 0.39889591932296753\n",
      "cnt: 0 - valLoss: 0.42591291666030884 - trainLoss: 0.3988948464393616\n",
      "cnt: 0 - valLoss: 0.42591267824172974 - trainLoss: 0.39889371395111084\n",
      "cnt: 0 - valLoss: 0.4259123206138611 - trainLoss: 0.3988926410675049\n",
      "cnt: 0 - valLoss: 0.4259120523929596 - trainLoss: 0.39889153838157654\n",
      "cnt: 0 - valLoss: 0.4259117543697357 - trainLoss: 0.3988904356956482\n",
      "cnt: 0 - valLoss: 0.42591142654418945 - trainLoss: 0.39888936281204224\n",
      "cnt: 0 - valLoss: 0.4259111285209656 - trainLoss: 0.3988882303237915\n",
      "cnt: 0 - valLoss: 0.4259108603000641 - trainLoss: 0.39888718724250793\n",
      "cnt: 0 - valLoss: 0.4259105622768402 - trainLoss: 0.398886114358902\n",
      "cnt: 0 - valLoss: 0.42591026425361633 - trainLoss: 0.39888498187065125\n",
      "cnt: 0 - valLoss: 0.42590999603271484 - trainLoss: 0.3988839089870453\n",
      "cnt: 0 - valLoss: 0.4259096682071686 - trainLoss: 0.39888280630111694\n",
      "cnt: 0 - valLoss: 0.4259093999862671 - trainLoss: 0.3988817036151886\n",
      "cnt: 0 - valLoss: 0.4259091019630432 - trainLoss: 0.39888063073158264\n",
      "cnt: 0 - valLoss: 0.42590880393981934 - trainLoss: 0.3988795578479767\n",
      "cnt: 0 - valLoss: 0.42590850591659546 - trainLoss: 0.39887845516204834\n",
      "cnt: 0 - valLoss: 0.4259082078933716 - trainLoss: 0.3988773822784424\n",
      "cnt: 0 - valLoss: 0.4259079098701477 - trainLoss: 0.39887624979019165\n",
      "cnt: 0 - valLoss: 0.4259076416492462 - trainLoss: 0.3988751769065857\n",
      "cnt: 0 - valLoss: 0.42590734362602234 - trainLoss: 0.39887407422065735\n",
      "cnt: 0 - valLoss: 0.42590704560279846 - trainLoss: 0.3988730013370514\n",
      "cnt: 0 - valLoss: 0.425906777381897 - trainLoss: 0.39887189865112305\n",
      "cnt: 0 - valLoss: 0.4259064793586731 - trainLoss: 0.3988708257675171\n",
      "cnt: 0 - valLoss: 0.425906240940094 - trainLoss: 0.39886972308158875\n",
      "cnt: 0 - valLoss: 0.4259059429168701 - trainLoss: 0.3988686501979828\n",
      "cnt: 0 - valLoss: 0.42590564489364624 - trainLoss: 0.39886751770973206\n",
      "cnt: 0 - valLoss: 0.42590537667274475 - trainLoss: 0.3988664448261261\n",
      "cnt: 0 - valLoss: 0.42590510845184326 - trainLoss: 0.39886534214019775\n",
      "cnt: 0 - valLoss: 0.425904780626297 - trainLoss: 0.3988642394542694\n",
      "cnt: 0 - valLoss: 0.4259045124053955 - trainLoss: 0.39886316657066345\n",
      "cnt: 0 - valLoss: 0.4259042739868164 - trainLoss: 0.3988620936870575\n",
      "cnt: 0 - valLoss: 0.4259040057659149 - trainLoss: 0.39886099100112915\n",
      "cnt: 0 - valLoss: 0.42590370774269104 - trainLoss: 0.3988598585128784\n",
      "cnt: 0 - valLoss: 0.42590340971946716 - trainLoss: 0.39885878562927246\n",
      "cnt: 0 - valLoss: 0.4259031414985657 - trainLoss: 0.3988576829433441\n",
      "cnt: 0 - valLoss: 0.4259029030799866 - trainLoss: 0.39885661005973816\n",
      "cnt: 0 - valLoss: 0.4259026050567627 - trainLoss: 0.3988555073738098\n",
      "cnt: 0 - valLoss: 0.4259023368358612 - trainLoss: 0.39885443449020386\n",
      "cnt: 0 - valLoss: 0.4259020686149597 - trainLoss: 0.3988533318042755\n",
      "cnt: 0 - valLoss: 0.42590177059173584 - trainLoss: 0.39885225892066956\n",
      "cnt: 0 - valLoss: 0.42590153217315674 - trainLoss: 0.3988511264324188\n",
      "cnt: 0 - valLoss: 0.42590126395225525 - trainLoss: 0.39885005354881287\n",
      "cnt: 0 - valLoss: 0.42590096592903137 - trainLoss: 0.3988489806652069\n",
      "cnt: 0 - valLoss: 0.4259006977081299 - trainLoss: 0.39884793758392334\n",
      "cnt: 0 - valLoss: 0.4259003698825836 - trainLoss: 0.3988468050956726\n",
      "cnt: 0 - valLoss: 0.4259001612663269 - trainLoss: 0.39884573221206665\n",
      "cnt: 0 - valLoss: 0.4258998930454254 - trainLoss: 0.3988446295261383\n",
      "cnt: 0 - valLoss: 0.42589953541755676 - trainLoss: 0.39884355664253235\n",
      "cnt: 0 - valLoss: 0.42589929699897766 - trainLoss: 0.398842453956604\n",
      "cnt: 0 - valLoss: 0.42589908838272095 - trainLoss: 0.39884138107299805\n",
      "cnt: 0 - valLoss: 0.42589879035949707 - trainLoss: 0.3988402783870697\n",
      "cnt: 0 - valLoss: 0.4258985221385956 - trainLoss: 0.39883920550346375\n",
      "cnt: 0 - valLoss: 0.42589831352233887 - trainLoss: 0.3988381326198578\n",
      "cnt: 0 - valLoss: 0.4258979558944702 - trainLoss: 0.39883700013160706\n",
      "cnt: 0 - valLoss: 0.4258977174758911 - trainLoss: 0.3988358974456787\n",
      "cnt: 0 - valLoss: 0.4258974492549896 - trainLoss: 0.39883479475975037\n",
      "cnt: 0 - valLoss: 0.42589715123176575 - trainLoss: 0.3988337516784668\n",
      "cnt: 0 - valLoss: 0.4258969724178314 - trainLoss: 0.39883264899253845\n",
      "cnt: 0 - valLoss: 0.42589664459228516 - trainLoss: 0.3988315463066101\n",
      "cnt: 0 - valLoss: 0.42589637637138367 - trainLoss: 0.3988304138183594\n",
      "cnt: 0 - valLoss: 0.4258961081504822 - trainLoss: 0.3988294005393982\n",
      "cnt: 0 - valLoss: 0.4258958697319031 - trainLoss: 0.39882826805114746\n",
      "cnt: 0 - valLoss: 0.4258956015110016 - trainLoss: 0.3988271653652191\n",
      "cnt: 0 - valLoss: 0.4258953034877777 - trainLoss: 0.39882609248161316\n",
      "cnt: 0 - valLoss: 0.425895094871521 - trainLoss: 0.3988250195980072\n",
      "cnt: 0 - valLoss: 0.4258947968482971 - trainLoss: 0.39882391691207886\n",
      "cnt: 0 - valLoss: 0.42589449882507324 - trainLoss: 0.3988228738307953\n",
      "cnt: 0 - valLoss: 0.4258943200111389 - trainLoss: 0.39882177114486694\n",
      "cnt: 0 - valLoss: 0.42589396238327026 - trainLoss: 0.3988206684589386\n",
      "cnt: 0 - valLoss: 0.42589372396469116 - trainLoss: 0.39881959557533264\n",
      "cnt: 0 - valLoss: 0.42589351534843445 - trainLoss: 0.3988185226917267\n",
      "cnt: 0 - valLoss: 0.42589321732521057 - trainLoss: 0.39881742000579834\n",
      "cnt: 0 - valLoss: 0.4258929491043091 - trainLoss: 0.3988162875175476\n",
      "cnt: 0 - valLoss: 0.4258926808834076 - trainLoss: 0.39881521463394165\n",
      "cnt: 0 - valLoss: 0.4258923828601837 - trainLoss: 0.3988141417503357\n",
      "cnt: 0 - valLoss: 0.425892174243927 - trainLoss: 0.39881303906440735\n",
      "cnt: 0 - valLoss: 0.4258919060230255 - trainLoss: 0.398811936378479\n",
      "cnt: 0 - valLoss: 0.4258916676044464 - trainLoss: 0.39881086349487305\n",
      "cnt: 0 - valLoss: 0.4258913993835449 - trainLoss: 0.3988097906112671\n",
      "cnt: 0 - valLoss: 0.42589110136032104 - trainLoss: 0.39880868792533875\n",
      "cnt: 0 - valLoss: 0.42589083313941956 - trainLoss: 0.3988076150417328\n",
      "cnt: 0 - valLoss: 0.42589059472084045 - trainLoss: 0.39880654215812683\n",
      "cnt: 0 - valLoss: 0.4258902966976166 - trainLoss: 0.3988054096698761\n",
      "cnt: 0 - valLoss: 0.42589011788368225 - trainLoss: 0.39880436658859253\n",
      "cnt: 0 - valLoss: 0.425889790058136 - trainLoss: 0.3988032937049866\n",
      "cnt: 0 - valLoss: 0.4258895516395569 - trainLoss: 0.39880216121673584\n",
      "cnt: 0 - valLoss: 0.42588934302330017 - trainLoss: 0.3988010585308075\n",
      "cnt: 0 - valLoss: 0.4258890151977539 - trainLoss: 0.39879995584487915\n",
      "cnt: 0 - valLoss: 0.4258887767791748 - trainLoss: 0.3987989127635956\n",
      "cnt: 0 - valLoss: 0.4258884787559509 - trainLoss: 0.39879781007766724\n",
      "cnt: 0 - valLoss: 0.4258882403373718 - trainLoss: 0.3987967371940613\n",
      "cnt: 0 - valLoss: 0.4258880317211151 - trainLoss: 0.39879563450813293\n",
      "cnt: 0 - valLoss: 0.4258877635002136 - trainLoss: 0.398794561624527\n",
      "cnt: 0 - valLoss: 0.42588749527931213 - trainLoss: 0.39879342913627625\n",
      "cnt: 0 - valLoss: 0.42588725686073303 - trainLoss: 0.39879241585731506\n",
      "cnt: 0 - valLoss: 0.42588695883750916 - trainLoss: 0.3987913131713867\n",
      "cnt: 0 - valLoss: 0.42588669061660767 - trainLoss: 0.398790180683136\n",
      "cnt: 0 - valLoss: 0.4258864223957062 - trainLoss: 0.39878910779953003\n",
      "cnt: 0 - valLoss: 0.42588621377944946 - trainLoss: 0.3987880349159241\n",
      "cnt: 0 - valLoss: 0.42588597536087036 - trainLoss: 0.3987869322299957\n",
      "cnt: 0 - valLoss: 0.4258856475353241 - trainLoss: 0.3987858295440674\n",
      "cnt: 0 - valLoss: 0.4258854389190674 - trainLoss: 0.3987847864627838\n",
      "cnt: 0 - valLoss: 0.4258851408958435 - trainLoss: 0.39878368377685547\n",
      "cnt: 0 - valLoss: 0.4258849024772644 - trainLoss: 0.3987825810909271\n",
      "cnt: 0 - valLoss: 0.4258846640586853 - trainLoss: 0.39878150820732117\n",
      "cnt: 0 - valLoss: 0.4258844256401062 - trainLoss: 0.3987804353237152\n",
      "cnt: 0 - valLoss: 0.4258842170238495 - trainLoss: 0.3987793028354645\n",
      "cnt: 0 - valLoss: 0.42588385939598083 - trainLoss: 0.3987782299518585\n",
      "cnt: 0 - valLoss: 0.4258836805820465 - trainLoss: 0.39877718687057495\n",
      "cnt: 0 - valLoss: 0.4258834421634674 - trainLoss: 0.39877602458000183\n",
      "cnt: 0 - valLoss: 0.4258831739425659 - trainLoss: 0.3987749516963959\n",
      "cnt: 0 - valLoss: 0.42588290572166443 - trainLoss: 0.3987738788127899\n",
      "cnt: 0 - valLoss: 0.4258826673030853 - trainLoss: 0.39877280592918396\n",
      "cnt: 0 - valLoss: 0.42588239908218384 - trainLoss: 0.3987717032432556\n",
      "cnt: 0 - valLoss: 0.4258821904659271 - trainLoss: 0.39877063035964966\n",
      "cnt: 0 - valLoss: 0.42588189244270325 - trainLoss: 0.3987695574760437\n",
      "cnt: 0 - valLoss: 0.4258817136287689 - trainLoss: 0.39876845479011536\n",
      "cnt: 0 - valLoss: 0.42588141560554504 - trainLoss: 0.3987673223018646\n",
      "cnt: 0 - valLoss: 0.42588114738464355 - trainLoss: 0.39876630902290344\n",
      "cnt: 0 - valLoss: 0.42588093876838684 - trainLoss: 0.3987651765346527\n",
      "cnt: 0 - valLoss: 0.42588064074516296 - trainLoss: 0.39876413345336914\n",
      "cnt: 0 - valLoss: 0.42588043212890625 - trainLoss: 0.3987630009651184\n",
      "cnt: 0 - valLoss: 0.42588016390800476 - trainLoss: 0.39876192808151245\n",
      "cnt: 0 - valLoss: 0.42587992548942566 - trainLoss: 0.3987608253955841\n",
      "cnt: 0 - valLoss: 0.42587971687316895 - trainLoss: 0.39875975251197815\n",
      "cnt: 0 - valLoss: 0.4258793890476227 - trainLoss: 0.3987586796283722\n",
      "cnt: 0 - valLoss: 0.42587918043136597 - trainLoss: 0.39875757694244385\n",
      "cnt: 0 - valLoss: 0.4258788824081421 - trainLoss: 0.3987565040588379\n",
      "cnt: 0 - valLoss: 0.42587870359420776 - trainLoss: 0.39875540137290955\n",
      "cnt: 0 - valLoss: 0.42587846517562866 - trainLoss: 0.3987543284893036\n",
      "cnt: 0 - valLoss: 0.4258781969547272 - trainLoss: 0.39875319600105286\n",
      "cnt: 0 - valLoss: 0.42587798833847046 - trainLoss: 0.3987521231174469\n",
      "cnt: 0 - valLoss: 0.4258776903152466 - trainLoss: 0.39875105023384094\n",
      "cnt: 0 - valLoss: 0.42587748169898987 - trainLoss: 0.39875003695487976\n",
      "cnt: 0 - valLoss: 0.42587727308273315 - trainLoss: 0.3987489342689514\n",
      "cnt: 0 - valLoss: 0.42587700486183167 - trainLoss: 0.3987478017807007\n",
      "cnt: 0 - valLoss: 0.4258767366409302 - trainLoss: 0.3987467288970947\n",
      "cnt: 0 - valLoss: 0.4258764982223511 - trainLoss: 0.39874568581581116\n",
      "cnt: 0 - valLoss: 0.425876259803772 - trainLoss: 0.3987446129322052\n",
      "cnt: 0 - valLoss: 0.42587605118751526 - trainLoss: 0.3987434506416321\n",
      "cnt: 0 - valLoss: 0.42587581276893616 - trainLoss: 0.3987424075603485\n",
      "cnt: 0 - valLoss: 0.42587554454803467 - trainLoss: 0.39874130487442017\n",
      "cnt: 0 - valLoss: 0.4258752763271332 - trainLoss: 0.398740291595459\n",
      "cnt: 0 - valLoss: 0.42587512731552124 - trainLoss: 0.39873915910720825\n",
      "cnt: 0 - valLoss: 0.42587485909461975 - trainLoss: 0.3987380564212799\n",
      "cnt: 0 - valLoss: 0.42587459087371826 - trainLoss: 0.39873698353767395\n",
      "cnt: 0 - valLoss: 0.42587435245513916 - trainLoss: 0.398735910654068\n",
      "cnt: 0 - valLoss: 0.42587414383888245 - trainLoss: 0.39873480796813965\n",
      "cnt: 0 - valLoss: 0.42587387561798096 - trainLoss: 0.3987337350845337\n",
      "cnt: 0 - valLoss: 0.42587360739707947 - trainLoss: 0.39873266220092773\n",
      "cnt: 0 - valLoss: 0.42587339878082275 - trainLoss: 0.3987315595149994\n",
      "cnt: 0 - valLoss: 0.42587316036224365 - trainLoss: 0.3987305164337158\n",
      "cnt: 0 - valLoss: 0.42587289214134216 - trainLoss: 0.3987294137477875\n",
      "cnt: 0 - valLoss: 0.42587271332740784 - trainLoss: 0.39872828125953674\n",
      "cnt: 0 - valLoss: 0.4258723855018616 - trainLoss: 0.39872726798057556\n",
      "cnt: 0 - valLoss: 0.42587220668792725 - trainLoss: 0.3987261652946472\n",
      "cnt: 0 - valLoss: 0.42587193846702576 - trainLoss: 0.3987250328063965\n",
      "cnt: 0 - valLoss: 0.42587170004844666 - trainLoss: 0.3987240195274353\n",
      "cnt: 0 - valLoss: 0.42587152123451233 - trainLoss: 0.39872288703918457\n",
      "cnt: 0 - valLoss: 0.42587122321128845 - trainLoss: 0.398721843957901\n",
      "cnt: 0 - valLoss: 0.42587101459503174 - trainLoss: 0.39872077107429504\n",
      "cnt: 0 - valLoss: 0.42587077617645264 - trainLoss: 0.3987196385860443\n",
      "cnt: 0 - valLoss: 0.42587053775787354 - trainLoss: 0.39871856570243835\n",
      "cnt: 0 - valLoss: 0.4258703291416168 - trainLoss: 0.3987175226211548\n",
      "cnt: 0 - valLoss: 0.42587006092071533 - trainLoss: 0.39871639013290405\n",
      "cnt: 0 - valLoss: 0.4258698523044586 - trainLoss: 0.3987152874469757\n",
      "cnt: 0 - valLoss: 0.4258696436882019 - trainLoss: 0.39871424436569214\n",
      "cnt: 0 - valLoss: 0.425869345664978 - trainLoss: 0.3987131416797638\n",
      "cnt: 0 - valLoss: 0.4258691072463989 - trainLoss: 0.39871206879615784\n",
      "cnt: 0 - valLoss: 0.4258688688278198 - trainLoss: 0.3987109959125519\n",
      "cnt: 0 - valLoss: 0.4258686304092407 - trainLoss: 0.39870989322662354\n",
      "cnt: 0 - valLoss: 0.425868421792984 - trainLoss: 0.39870885014533997\n",
      "cnt: 0 - valLoss: 0.4258681833744049 - trainLoss: 0.3987077474594116\n",
      "cnt: 0 - valLoss: 0.4258679449558258 - trainLoss: 0.3987066447734833\n",
      "cnt: 0 - valLoss: 0.4258677065372467 - trainLoss: 0.3987055718898773\n",
      "cnt: 0 - valLoss: 0.42586749792099 - trainLoss: 0.39870449900627136\n",
      "cnt: 0 - valLoss: 0.4258671998977661 - trainLoss: 0.398703396320343\n",
      "cnt: 0 - valLoss: 0.425866961479187 - trainLoss: 0.39870235323905945\n",
      "cnt: 0 - valLoss: 0.4258668124675751 - trainLoss: 0.3987012505531311\n",
      "cnt: 0 - valLoss: 0.4258665442466736 - trainLoss: 0.39870017766952515\n",
      "cnt: 0 - valLoss: 0.42586633563041687 - trainLoss: 0.3986991047859192\n",
      "cnt: 0 - valLoss: 0.42586612701416016 - trainLoss: 0.39869800209999084\n",
      "cnt: 0 - valLoss: 0.42586588859558105 - trainLoss: 0.3986969590187073\n",
      "cnt: 0 - valLoss: 0.42586567997932434 - trainLoss: 0.39869585633277893\n",
      "cnt: 0 - valLoss: 0.42586544156074524 - trainLoss: 0.398694783449173\n",
      "cnt: 0 - valLoss: 0.4258652627468109 - trainLoss: 0.39869368076324463\n",
      "cnt: 0 - valLoss: 0.42586496472358704 - trainLoss: 0.39869260787963867\n",
      "cnt: 0 - valLoss: 0.4258647859096527 - trainLoss: 0.3986915349960327\n",
      "cnt: 0 - valLoss: 0.4258645176887512 - trainLoss: 0.39869046211242676\n",
      "cnt: 0 - valLoss: 0.4258643090724945 - trainLoss: 0.3986893892288208\n",
      "cnt: 0 - valLoss: 0.4258641302585602 - trainLoss: 0.39868828654289246\n",
      "cnt: 0 - valLoss: 0.4258638918399811 - trainLoss: 0.3986872136592865\n",
      "cnt: 0 - valLoss: 0.42586368322372437 - trainLoss: 0.39868614077568054\n",
      "cnt: 0 - valLoss: 0.4258634150028229 - trainLoss: 0.3986850678920746\n",
      "cnt: 0 - valLoss: 0.42586320638656616 - trainLoss: 0.39868396520614624\n",
      "cnt: 0 - valLoss: 0.42586302757263184 - trainLoss: 0.3986828923225403\n",
      "cnt: 0 - valLoss: 0.42586275935173035 - trainLoss: 0.3986818194389343\n",
      "cnt: 0 - valLoss: 0.42586252093315125 - trainLoss: 0.39868074655532837\n",
      "cnt: 0 - valLoss: 0.4258623421192169 - trainLoss: 0.39867961406707764\n",
      "cnt: 0 - valLoss: 0.4258621335029602 - trainLoss: 0.39867857098579407\n",
      "cnt: 0 - valLoss: 0.42586183547973633 - trainLoss: 0.3986774981021881\n",
      "cnt: 0 - valLoss: 0.425861656665802 - trainLoss: 0.3986763656139374\n",
      "cnt: 0 - valLoss: 0.4258614778518677 - trainLoss: 0.3986752927303314\n",
      "cnt: 0 - valLoss: 0.4258612394332886 - trainLoss: 0.39867424964904785\n",
      "cnt: 0 - valLoss: 0.42586106061935425 - trainLoss: 0.3986731767654419\n",
      "cnt: 0 - valLoss: 0.42586079239845276 - trainLoss: 0.39867204427719116\n",
      "cnt: 0 - valLoss: 0.42586058378219604 - trainLoss: 0.3986709713935852\n",
      "cnt: 0 - valLoss: 0.42586037516593933 - trainLoss: 0.39866989850997925\n",
      "cnt: 0 - valLoss: 0.42586010694503784 - trainLoss: 0.3986688554286957\n",
      "cnt: 0 - valLoss: 0.4258599579334259 - trainLoss: 0.3986677825450897\n",
      "cnt: 0 - valLoss: 0.4258596897125244 - trainLoss: 0.398666650056839\n",
      "cnt: 0 - valLoss: 0.4258594810962677 - trainLoss: 0.39866557717323303\n",
      "cnt: 0 - valLoss: 0.4258592128753662 - trainLoss: 0.39866453409194946\n",
      "cnt: 0 - valLoss: 0.4258590340614319 - trainLoss: 0.39866340160369873\n",
      "cnt: 0 - valLoss: 0.42585882544517517 - trainLoss: 0.39866238832473755\n",
      "cnt: 0 - valLoss: 0.42585858702659607 - trainLoss: 0.3986612558364868\n",
      "cnt: 0 - valLoss: 0.42585840821266174 - trainLoss: 0.39866018295288086\n",
      "cnt: 0 - valLoss: 0.42585813999176025 - trainLoss: 0.3986591398715973\n",
      "cnt: 0 - valLoss: 0.4258579611778259 - trainLoss: 0.39865800738334656\n",
      "cnt: 0 - valLoss: 0.4258577227592468 - trainLoss: 0.3986569344997406\n",
      "cnt: 0 - valLoss: 0.4258575439453125 - trainLoss: 0.39865586161613464\n",
      "cnt: 0 - valLoss: 0.4258573353290558 - trainLoss: 0.3986547589302063\n",
      "cnt: 0 - valLoss: 0.4258570671081543 - trainLoss: 0.39865368604660034\n",
      "cnt: 0 - valLoss: 0.4258568584918976 - trainLoss: 0.3986526131629944\n",
      "cnt: 0 - valLoss: 0.4258565902709961 - trainLoss: 0.3986515402793884\n",
      "cnt: 0 - valLoss: 0.42585644125938416 - trainLoss: 0.39865046739578247\n",
      "cnt: 0 - valLoss: 0.42585617303848267 - trainLoss: 0.3986493647098541\n",
      "cnt: 0 - valLoss: 0.42585599422454834 - trainLoss: 0.39864829182624817\n",
      "cnt: 0 - valLoss: 0.425855815410614 - trainLoss: 0.3986472189426422\n",
      "cnt: 0 - valLoss: 0.4258555769920349 - trainLoss: 0.39864614605903625\n",
      "cnt: 0 - valLoss: 0.4258553683757782 - trainLoss: 0.3986450731754303\n",
      "cnt: 0 - valLoss: 0.4258551299571991 - trainLoss: 0.39864397048950195\n",
      "cnt: 0 - valLoss: 0.4258549213409424 - trainLoss: 0.398642897605896\n",
      "cnt: 0 - valLoss: 0.4258546829223633 - trainLoss: 0.39864182472229004\n",
      "cnt: 0 - valLoss: 0.4258544445037842 - trainLoss: 0.3986407220363617\n",
      "cnt: 0 - valLoss: 0.42585429549217224 - trainLoss: 0.3986397087574005\n",
      "cnt: 0 - valLoss: 0.42585402727127075 - trainLoss: 0.39863863587379456\n",
      "cnt: 0 - valLoss: 0.4258538484573364 - trainLoss: 0.3986375033855438\n",
      "cnt: 0 - valLoss: 0.4258536100387573 - trainLoss: 0.39863643050193787\n",
      "cnt: 0 - valLoss: 0.425853431224823 - trainLoss: 0.3986353576183319\n",
      "cnt: 0 - valLoss: 0.4258531630039215 - trainLoss: 0.39863431453704834\n",
      "cnt: 0 - valLoss: 0.42585301399230957 - trainLoss: 0.3986331820487976\n",
      "cnt: 0 - valLoss: 0.42585277557373047 - trainLoss: 0.39863210916519165\n",
      "cnt: 0 - valLoss: 0.42585253715515137 - trainLoss: 0.3986310362815857\n",
      "cnt: 0 - valLoss: 0.42585235834121704 - trainLoss: 0.3986299932003021\n",
      "cnt: 0 - valLoss: 0.4258521497249603 - trainLoss: 0.3986288607120514\n",
      "cnt: 0 - valLoss: 0.425851970911026 - trainLoss: 0.3986278474330902\n",
      "cnt: 0 - valLoss: 0.4258517026901245 - trainLoss: 0.3986267149448395\n",
      "cnt: 0 - valLoss: 0.4258514940738678 - trainLoss: 0.3986256420612335\n",
      "cnt: 0 - valLoss: 0.4258512854576111 - trainLoss: 0.39862459897994995\n",
      "cnt: 0 - valLoss: 0.4258510172367096 - trainLoss: 0.398623526096344\n",
      "cnt: 0 - valLoss: 0.42585086822509766 - trainLoss: 0.39862239360809326\n",
      "cnt: 0 - valLoss: 0.42585060000419617 - trainLoss: 0.3986213207244873\n",
      "cnt: 0 - valLoss: 0.42585042119026184 - trainLoss: 0.39862027764320374\n",
      "cnt: 0 - valLoss: 0.4258502125740051 - trainLoss: 0.3986192047595978\n",
      "cnt: 0 - valLoss: 0.4258500039577484 - trainLoss: 0.3986181318759918\n",
      "cnt: 0 - valLoss: 0.4258497953414917 - trainLoss: 0.3986169993877411\n",
      "cnt: 0 - valLoss: 0.4258495569229126 - trainLoss: 0.39861592650413513\n",
      "cnt: 0 - valLoss: 0.4258493185043335 - trainLoss: 0.39861488342285156\n",
      "cnt: 0 - valLoss: 0.42584913969039917 - trainLoss: 0.3986138105392456\n",
      "cnt: 0 - valLoss: 0.42584896087646484 - trainLoss: 0.39861273765563965\n",
      "cnt: 0 - valLoss: 0.42584872245788574 - trainLoss: 0.3986116051673889\n",
      "cnt: 0 - valLoss: 0.4258485436439514 - trainLoss: 0.39861059188842773\n",
      "cnt: 0 - valLoss: 0.4258482754230499 - trainLoss: 0.3986094892024994\n",
      "cnt: 0 - valLoss: 0.425848126411438 - trainLoss: 0.3986084461212158\n",
      "cnt: 0 - valLoss: 0.4258478581905365 - trainLoss: 0.3986073434352875\n",
      "cnt: 0 - valLoss: 0.4258476793766022 - trainLoss: 0.3986062705516815\n",
      "cnt: 0 - valLoss: 0.42584744095802307 - trainLoss: 0.3986051678657532\n",
      "cnt: 0 - valLoss: 0.42584726214408875 - trainLoss: 0.3986040949821472\n",
      "cnt: 0 - valLoss: 0.4258470833301544 - trainLoss: 0.39860302209854126\n",
      "cnt: 0 - valLoss: 0.42584681510925293 - trainLoss: 0.3986019492149353\n",
      "cnt: 0 - valLoss: 0.425846666097641 - trainLoss: 0.39860087633132935\n",
      "cnt: 0 - valLoss: 0.4258463978767395 - trainLoss: 0.3985998034477234\n",
      "cnt: 0 - valLoss: 0.4258462190628052 - trainLoss: 0.39859870076179504\n",
      "cnt: 0 - valLoss: 0.4258459806442261 - trainLoss: 0.3985976278781891\n",
      "cnt: 0 - valLoss: 0.42584580183029175 - trainLoss: 0.39859655499458313\n",
      "cnt: 0 - valLoss: 0.42584553360939026 - trainLoss: 0.3985954523086548\n",
      "cnt: 0 - valLoss: 0.4258453845977783 - trainLoss: 0.3985944092273712\n",
      "cnt: 0 - valLoss: 0.425845205783844 - trainLoss: 0.39859330654144287\n",
      "cnt: 0 - valLoss: 0.4258449375629425 - trainLoss: 0.3985922634601593\n",
      "cnt: 0 - valLoss: 0.4258447587490082 - trainLoss: 0.39859116077423096\n",
      "cnt: 0 - valLoss: 0.42584455013275146 - trainLoss: 0.398590087890625\n",
      "cnt: 0 - valLoss: 0.42584434151649475 - trainLoss: 0.39858901500701904\n",
      "cnt: 0 - valLoss: 0.42584407329559326 - trainLoss: 0.3985879421234131\n",
      "cnt: 0 - valLoss: 0.4258439242839813 - trainLoss: 0.39858686923980713\n",
      "cnt: 0 - valLoss: 0.4258436858654022 - trainLoss: 0.3985857665538788\n",
      "cnt: 0 - valLoss: 0.4258435368537903 - trainLoss: 0.3985847234725952\n",
      "cnt: 0 - valLoss: 0.42584332823753357 - trainLoss: 0.39858362078666687\n",
      "cnt: 0 - valLoss: 0.42584308981895447 - trainLoss: 0.3985825479030609\n",
      "cnt: 0 - valLoss: 0.42584294080734253 - trainLoss: 0.39858147501945496\n",
      "cnt: 0 - valLoss: 0.4258427023887634 - trainLoss: 0.398580402135849\n",
      "cnt: 0 - valLoss: 0.4258425533771515 - trainLoss: 0.39857935905456543\n",
      "cnt: 0 - valLoss: 0.42584228515625 - trainLoss: 0.3985782265663147\n",
      "cnt: 0 - valLoss: 0.4258420765399933 - trainLoss: 0.39857715368270874\n",
      "cnt: 0 - valLoss: 0.4258418679237366 - trainLoss: 0.3985760807991028\n",
      "cnt: 0 - valLoss: 0.42584168910980225 - trainLoss: 0.3985750675201416\n",
      "cnt: 0 - valLoss: 0.4258415102958679 - trainLoss: 0.3985739052295685\n",
      "cnt: 0 - valLoss: 0.42584124207496643 - trainLoss: 0.3985728919506073\n",
      "cnt: 0 - valLoss: 0.4258410930633545 - trainLoss: 0.39857181906700134\n",
      "cnt: 0 - valLoss: 0.425840824842453 - trainLoss: 0.3985706865787506\n",
      "cnt: 0 - valLoss: 0.4258406460285187 - trainLoss: 0.39856964349746704\n",
      "cnt: 0 - valLoss: 0.4258404076099396 - trainLoss: 0.3985685706138611\n",
      "cnt: 0 - valLoss: 0.42584022879600525 - trainLoss: 0.3985674977302551\n",
      "cnt: 0 - valLoss: 0.42584002017974854 - trainLoss: 0.39856642484664917\n",
      "cnt: 0 - valLoss: 0.4258398413658142 - trainLoss: 0.3985653519630432\n",
      "cnt: 0 - valLoss: 0.4258396327495575 - trainLoss: 0.39856427907943726\n",
      "cnt: 0 - valLoss: 0.42583945393562317 - trainLoss: 0.3985632061958313\n",
      "cnt: 0 - valLoss: 0.42583924531936646 - trainLoss: 0.39856213331222534\n",
      "cnt: 0 - valLoss: 0.42583897709846497 - trainLoss: 0.398561030626297\n",
      "cnt: 0 - valLoss: 0.4258388578891754 - trainLoss: 0.39855995774269104\n",
      "cnt: 0 - valLoss: 0.4258385896682739 - trainLoss: 0.3985588848590851\n",
      "cnt: 0 - valLoss: 0.425838440656662 - trainLoss: 0.3985578119754791\n",
      "cnt: 0 - valLoss: 0.4258381724357605 - trainLoss: 0.39855673909187317\n",
      "cnt: 0 - valLoss: 0.42583799362182617 - trainLoss: 0.3985556662082672\n",
      "cnt: 0 - valLoss: 0.42583778500556946 - trainLoss: 0.39855459332466125\n",
      "cnt: 0 - valLoss: 0.42583760619163513 - trainLoss: 0.3985534906387329\n",
      "cnt: 0 - valLoss: 0.4258373975753784 - trainLoss: 0.39855247735977173\n",
      "cnt: 0 - valLoss: 0.4258371889591217 - trainLoss: 0.398551344871521\n",
      "cnt: 0 - valLoss: 0.425836980342865 - trainLoss: 0.39855027198791504\n",
      "cnt: 0 - valLoss: 0.42583680152893066 - trainLoss: 0.3985491991043091\n",
      "cnt: 0 - valLoss: 0.42583662271499634 - trainLoss: 0.3985481560230255\n",
      "cnt: 0 - valLoss: 0.42583638429641724 - trainLoss: 0.39854708313941956\n",
      "cnt: 0 - valLoss: 0.4258362054824829 - trainLoss: 0.3985459506511688\n",
      "cnt: 0 - valLoss: 0.4258359372615814 - trainLoss: 0.39854487776756287\n",
      "cnt: 0 - valLoss: 0.4258357584476471 - trainLoss: 0.3985438644886017\n",
      "cnt: 0 - valLoss: 0.4258355498313904 - trainLoss: 0.39854276180267334\n",
      "cnt: 0 - valLoss: 0.42583534121513367 - trainLoss: 0.39854171872138977\n",
      "cnt: 0 - valLoss: 0.42583516240119934 - trainLoss: 0.3985406160354614\n",
      "cnt: 0 - valLoss: 0.42583492398262024 - trainLoss: 0.39853954315185547\n",
      "cnt: 0 - valLoss: 0.4258347749710083 - trainLoss: 0.3985384702682495\n",
      "cnt: 0 - valLoss: 0.4258345365524292 - trainLoss: 0.39853739738464355\n",
      "cnt: 0 - valLoss: 0.4258343577384949 - trainLoss: 0.3985363245010376\n",
      "cnt: 0 - valLoss: 0.42583414912223816 - trainLoss: 0.39853525161743164\n",
      "cnt: 0 - valLoss: 0.42583394050598145 - trainLoss: 0.3985341489315033\n",
      "cnt: 0 - valLoss: 0.42583370208740234 - trainLoss: 0.39853307604789734\n",
      "cnt: 0 - valLoss: 0.4258335530757904 - trainLoss: 0.3985320031642914\n",
      "cnt: 0 - valLoss: 0.4258333742618561 - trainLoss: 0.3985309302806854\n",
      "cnt: 0 - valLoss: 0.42583316564559937 - trainLoss: 0.39852985739707947\n",
      "cnt: 0 - valLoss: 0.42583292722702026 - trainLoss: 0.3985287845134735\n",
      "cnt: 0 - valLoss: 0.42583271861076355 - trainLoss: 0.39852771162986755\n",
      "cnt: 0 - valLoss: 0.4258325695991516 - trainLoss: 0.398526668548584\n",
      "cnt: 0 - valLoss: 0.4258323311805725 - trainLoss: 0.398525595664978\n",
      "cnt: 0 - valLoss: 0.42583218216896057 - trainLoss: 0.3985244631767273\n",
      "cnt: 0 - valLoss: 0.42583194375038147 - trainLoss: 0.3985234498977661\n",
      "cnt: 0 - valLoss: 0.42583173513412476 - trainLoss: 0.39852234721183777\n",
      "cnt: 0 - valLoss: 0.42583155632019043 - trainLoss: 0.3985213041305542\n",
      "cnt: 0 - valLoss: 0.42583131790161133 - trainLoss: 0.39852020144462585\n",
      "cnt: 0 - valLoss: 0.425831139087677 - trainLoss: 0.3985191285610199\n",
      "cnt: 0 - valLoss: 0.4258309602737427 - trainLoss: 0.39851805567741394\n",
      "cnt: 0 - valLoss: 0.4258307218551636 - trainLoss: 0.39851701259613037\n",
      "cnt: 0 - valLoss: 0.42583054304122925 - trainLoss: 0.398515909910202\n",
      "cnt: 0 - valLoss: 0.42583027482032776 - trainLoss: 0.39851483702659607\n",
      "cnt: 0 - valLoss: 0.4258301556110382 - trainLoss: 0.3985137641429901\n",
      "cnt: 0 - valLoss: 0.4258299469947815 - trainLoss: 0.39851269125938416\n",
      "cnt: 0 - valLoss: 0.42582976818084717 - trainLoss: 0.398511677980423\n",
      "cnt: 0 - valLoss: 0.42582958936691284 - trainLoss: 0.39851051568984985\n",
      "cnt: 0 - valLoss: 0.42582935094833374 - trainLoss: 0.39850950241088867\n",
      "cnt: 0 - valLoss: 0.42582911252975464 - trainLoss: 0.3985084295272827\n",
      "cnt: 0 - valLoss: 0.4258289635181427 - trainLoss: 0.398507297039032\n",
      "cnt: 0 - valLoss: 0.4258287847042084 - trainLoss: 0.3985062837600708\n",
      "cnt: 0 - valLoss: 0.4258285164833069 - trainLoss: 0.39850521087646484\n",
      "cnt: 0 - valLoss: 0.42582839727401733 - trainLoss: 0.3985041081905365\n",
      "cnt: 0 - valLoss: 0.425828218460083 - trainLoss: 0.39850306510925293\n",
      "cnt: 0 - valLoss: 0.42582806944847107 - trainLoss: 0.3985019624233246\n",
      "cnt: 0 - valLoss: 0.42582789063453674 - trainLoss: 0.398500919342041\n",
      "cnt: 0 - valLoss: 0.4258277118206024 - trainLoss: 0.39849981665611267\n",
      "cnt: 0 - valLoss: 0.4258275330066681 - trainLoss: 0.3984987437725067\n",
      "cnt: 0 - valLoss: 0.42582741379737854 - trainLoss: 0.39849767088890076\n",
      "cnt: 0 - valLoss: 0.425827294588089 - trainLoss: 0.3984965980052948\n",
      "cnt: 0 - valLoss: 0.4258270561695099 - trainLoss: 0.39849552512168884\n",
      "cnt: 0 - valLoss: 0.42582693696022034 - trainLoss: 0.3984944820404053\n",
      "cnt: 0 - valLoss: 0.4258267283439636 - trainLoss: 0.3984934091567993\n",
      "cnt: 0 - valLoss: 0.4258266091346741 - trainLoss: 0.39849233627319336\n",
      "cnt: 0 - valLoss: 0.42582643032073975 - trainLoss: 0.3984912633895874\n",
      "cnt: 0 - valLoss: 0.4258262515068054 - trainLoss: 0.39849019050598145\n",
      "cnt: 0 - valLoss: 0.4258260726928711 - trainLoss: 0.3984891176223755\n",
      "cnt: 0 - valLoss: 0.42582592368125916 - trainLoss: 0.39848804473876953\n",
      "cnt: 0 - valLoss: 0.42582574486732483 - trainLoss: 0.3984869718551636\n",
      "cnt: 0 - valLoss: 0.4258255660533905 - trainLoss: 0.3984858989715576\n",
      "cnt: 0 - valLoss: 0.4258253872394562 - trainLoss: 0.39848482608795166\n",
      "cnt: 0 - valLoss: 0.4258252680301666 - trainLoss: 0.3984837830066681\n",
      "cnt: 0 - valLoss: 0.4258250594139099 - trainLoss: 0.39848271012306213\n",
      "cnt: 0 - valLoss: 0.4258248805999756 - trainLoss: 0.3984816372394562\n",
      "cnt: 0 - valLoss: 0.4258247911930084 - trainLoss: 0.3984805643558502\n",
      "cnt: 0 - valLoss: 0.4258246123790741 - trainLoss: 0.39847949147224426\n",
      "cnt: 0 - valLoss: 0.42582446336746216 - trainLoss: 0.3984784185886383\n",
      "cnt: 0 - valLoss: 0.42582425475120544 - trainLoss: 0.3984774053096771\n",
      "cnt: 0 - valLoss: 0.4258241057395935 - trainLoss: 0.3984762728214264\n",
      "cnt: 0 - valLoss: 0.4258239269256592 - trainLoss: 0.3984752595424652\n",
      "cnt: 0 - valLoss: 0.42582377791404724 - trainLoss: 0.3984741270542145\n",
      "cnt: 0 - valLoss: 0.4258235692977905 - trainLoss: 0.3984730541706085\n",
      "cnt: 0 - valLoss: 0.4258234202861786 - trainLoss: 0.39847201108932495\n",
      "cnt: 0 - valLoss: 0.42582324147224426 - trainLoss: 0.3984709680080414\n",
      "cnt: 0 - valLoss: 0.4258230924606323 - trainLoss: 0.3984698951244354\n",
      "cnt: 0 - valLoss: 0.425822913646698 - trainLoss: 0.39846882224082947\n",
      "cnt: 0 - valLoss: 0.42582273483276367 - trainLoss: 0.3984677195549011\n",
      "cnt: 0 - valLoss: 0.4258226156234741 - trainLoss: 0.39846667647361755\n",
      "cnt: 0 - valLoss: 0.4258224368095398 - trainLoss: 0.398465633392334\n",
      "cnt: 0 - valLoss: 0.42582231760025024 - trainLoss: 0.398464560508728\n",
      "cnt: 0 - valLoss: 0.42582204937934875 - trainLoss: 0.3984634280204773\n",
      "cnt: 0 - valLoss: 0.4258219599723816 - trainLoss: 0.3984624147415161\n",
      "cnt: 0 - valLoss: 0.4258217513561249 - trainLoss: 0.39846134185791016\n",
      "cnt: 0 - valLoss: 0.42582157254219055 - trainLoss: 0.3984602689743042\n",
      "cnt: 0 - valLoss: 0.42582136392593384 - trainLoss: 0.39845919609069824\n",
      "cnt: 0 - valLoss: 0.4258212745189667 - trainLoss: 0.3984581232070923\n",
      "cnt: 0 - valLoss: 0.42582106590270996 - trainLoss: 0.39845705032348633\n",
      "cnt: 0 - valLoss: 0.4258209466934204 - trainLoss: 0.39845597743988037\n",
      "cnt: 0 - valLoss: 0.4258206784725189 - trainLoss: 0.3984549641609192\n",
      "cnt: 0 - valLoss: 0.42582058906555176 - trainLoss: 0.39845386147499084\n",
      "cnt: 0 - valLoss: 0.4258204400539398 - trainLoss: 0.3984527289867401\n",
      "cnt: 0 - valLoss: 0.4258202612400055 - trainLoss: 0.39845171570777893\n",
      "cnt: 0 - valLoss: 0.42582008242607117 - trainLoss: 0.398450642824173\n",
      "cnt: 0 - valLoss: 0.42581990361213684 - trainLoss: 0.398449569940567\n",
      "cnt: 0 - valLoss: 0.4258197546005249 - trainLoss: 0.39844849705696106\n",
      "cnt: 0 - valLoss: 0.4258195757865906 - trainLoss: 0.3984474241733551\n",
      "cnt: 0 - valLoss: 0.42581939697265625 - trainLoss: 0.39844632148742676\n",
      "cnt: 0 - valLoss: 0.42581918835639954 - trainLoss: 0.39844533801078796\n",
      "cnt: 0 - valLoss: 0.4258190393447876 - trainLoss: 0.39844420552253723\n",
      "cnt: 0 - valLoss: 0.42581889033317566 - trainLoss: 0.39844316244125366\n",
      "cnt: 0 - valLoss: 0.42581871151924133 - trainLoss: 0.3984421193599701\n",
      "cnt: 0 - valLoss: 0.4258185029029846 - trainLoss: 0.39844101667404175\n",
      "cnt: 0 - valLoss: 0.4258183538913727 - trainLoss: 0.3984399735927582\n",
      "cnt: 0 - valLoss: 0.42581820487976074 - trainLoss: 0.3984389007091522\n",
      "cnt: 0 - valLoss: 0.4258180260658264 - trainLoss: 0.39843782782554626\n",
      "cnt: 0 - valLoss: 0.42581790685653687 - trainLoss: 0.3984367847442627\n",
      "cnt: 0 - valLoss: 0.42581766843795776 - trainLoss: 0.39843571186065674\n",
      "cnt: 0 - valLoss: 0.4258175194263458 - trainLoss: 0.3984346389770508\n",
      "cnt: 0 - valLoss: 0.4258173406124115 - trainLoss: 0.3984335660934448\n",
      "cnt: 0 - valLoss: 0.42581722140312195 - trainLoss: 0.39843249320983887\n",
      "cnt: 0 - valLoss: 0.42581698298454285 - trainLoss: 0.3984314203262329\n",
      "cnt: 0 - valLoss: 0.4258168339729309 - trainLoss: 0.39843040704727173\n",
      "cnt: 0 - valLoss: 0.4258166551589966 - trainLoss: 0.398429274559021\n",
      "cnt: 0 - valLoss: 0.42581647634506226 - trainLoss: 0.39842820167541504\n",
      "cnt: 0 - valLoss: 0.42581629753112793 - trainLoss: 0.3984271287918091\n",
      "cnt: 0 - valLoss: 0.4258161783218384 - trainLoss: 0.3984260857105255\n",
      "cnt: 0 - valLoss: 0.42581596970558167 - trainLoss: 0.39842504262924194\n",
      "cnt: 0 - valLoss: 0.42581579089164734 - trainLoss: 0.398423969745636\n",
      "cnt: 0 - valLoss: 0.425815612077713 - trainLoss: 0.39842289686203003\n",
      "cnt: 0 - valLoss: 0.4258154630661011 - trainLoss: 0.3984218239784241\n",
      "cnt: 0 - valLoss: 0.42581531405448914 - trainLoss: 0.3984207510948181\n",
      "cnt: 0 - valLoss: 0.4258151650428772 - trainLoss: 0.39841970801353455\n",
      "cnt: 0 - valLoss: 0.42581501603126526 - trainLoss: 0.3984186351299286\n",
      "cnt: 0 - valLoss: 0.42581480741500854 - trainLoss: 0.39841756224632263\n",
      "cnt: 0 - valLoss: 0.4258146286010742 - trainLoss: 0.39841651916503906\n",
      "cnt: 0 - valLoss: 0.4258144199848175 - trainLoss: 0.3984154164791107\n",
      "cnt: 0 - valLoss: 0.42581430077552795 - trainLoss: 0.39841437339782715\n",
      "cnt: 0 - valLoss: 0.42581409215927124 - trainLoss: 0.3984132707118988\n",
      "cnt: 0 - valLoss: 0.4258140027523041 - trainLoss: 0.3984122574329376\n",
      "cnt: 0 - valLoss: 0.42581379413604736 - trainLoss: 0.39841118454933167\n",
      "cnt: 0 - valLoss: 0.42581355571746826 - trainLoss: 0.3984101116657257\n",
      "cnt: 0 - valLoss: 0.4258134067058563 - trainLoss: 0.39840906858444214\n",
      "cnt: 0 - valLoss: 0.4258132576942444 - trainLoss: 0.3984079658985138\n",
      "cnt: 0 - valLoss: 0.42581304907798767 - trainLoss: 0.39840689301490784\n",
      "cnt: 0 - valLoss: 0.4258129298686981 - trainLoss: 0.3984058201313019\n",
      "cnt: 0 - valLoss: 0.42581281065940857 - trainLoss: 0.3984047472476959\n",
      "cnt: 0 - valLoss: 0.42581257224082947 - trainLoss: 0.39840373396873474\n",
      "cnt: 0 - valLoss: 0.4258124530315399 - trainLoss: 0.3984026610851288\n",
      "cnt: 0 - valLoss: 0.4258122444152832 - trainLoss: 0.39840155839920044\n",
      "cnt: 0 - valLoss: 0.42581212520599365 - trainLoss: 0.39840051531791687\n",
      "cnt: 0 - valLoss: 0.42581188678741455 - trainLoss: 0.3983994424343109\n",
      "cnt: 0 - valLoss: 0.425811767578125 - trainLoss: 0.39839836955070496\n",
      "cnt: 0 - valLoss: 0.4258115589618683 - trainLoss: 0.3983973562717438\n",
      "cnt: 0 - valLoss: 0.42581138014793396 - trainLoss: 0.39839625358581543\n",
      "cnt: 0 - valLoss: 0.42581120133399963 - trainLoss: 0.3983951807022095\n",
      "cnt: 0 - valLoss: 0.4258110523223877 - trainLoss: 0.3983941078186035\n",
      "cnt: 0 - valLoss: 0.4258108139038086 - trainLoss: 0.39839306473731995\n",
      "cnt: 0 - valLoss: 0.42581072449684143 - trainLoss: 0.398391991853714\n",
      "cnt: 0 - valLoss: 0.42581048607826233 - trainLoss: 0.39839091897010803\n",
      "cnt: 0 - valLoss: 0.4258103668689728 - trainLoss: 0.39838987588882446\n",
      "cnt: 0 - valLoss: 0.42581018805503845 - trainLoss: 0.3983888030052185\n",
      "cnt: 0 - valLoss: 0.4258100390434265 - trainLoss: 0.39838775992393494\n",
      "cnt: 0 - valLoss: 0.4258098900318146 - trainLoss: 0.3983866572380066\n",
      "cnt: 0 - valLoss: 0.42580968141555786 - trainLoss: 0.398385614156723\n",
      "cnt: 0 - valLoss: 0.4258095324039459 - trainLoss: 0.39838454127311707\n",
      "cnt: 0 - valLoss: 0.4258093237876892 - trainLoss: 0.3983834385871887\n",
      "cnt: 0 - valLoss: 0.42580920457839966 - trainLoss: 0.39838242530822754\n",
      "cnt: 0 - valLoss: 0.42580893635749817 - trainLoss: 0.3983813524246216\n",
      "cnt: 0 - valLoss: 0.4258088171482086 - trainLoss: 0.3983802795410156\n",
      "cnt: 0 - valLoss: 0.4258086085319519 - trainLoss: 0.39837920665740967\n",
      "cnt: 0 - valLoss: 0.42580845952033997 - trainLoss: 0.3983781337738037\n",
      "cnt: 0 - valLoss: 0.42580825090408325 - trainLoss: 0.39837706089019775\n",
      "cnt: 0 - valLoss: 0.4258081614971161 - trainLoss: 0.3983760476112366\n",
      "cnt: 0 - valLoss: 0.4258079528808594 - trainLoss: 0.3983749747276306\n",
      "cnt: 0 - valLoss: 0.42580777406692505 - trainLoss: 0.39837390184402466\n",
      "cnt: 0 - valLoss: 0.4258076548576355 - trainLoss: 0.3983727693557739\n",
      "cnt: 0 - valLoss: 0.4258074462413788 - trainLoss: 0.39837175607681274\n",
      "cnt: 0 - valLoss: 0.42580729722976685 - trainLoss: 0.39837074279785156\n",
      "cnt: 0 - valLoss: 0.42580708861351013 - trainLoss: 0.39836961030960083\n",
      "cnt: 0 - valLoss: 0.4258069097995758 - trainLoss: 0.39836859703063965\n",
      "cnt: 0 - valLoss: 0.4258067011833191 - trainLoss: 0.3983675241470337\n",
      "cnt: 0 - valLoss: 0.42580658197402954 - trainLoss: 0.39836645126342773\n",
      "cnt: 0 - valLoss: 0.4258063733577728 - trainLoss: 0.3983653783798218\n",
      "cnt: 0 - valLoss: 0.4258062243461609 - trainLoss: 0.3983643054962158\n",
      "cnt: 0 - valLoss: 0.4258060157299042 - trainLoss: 0.39836323261260986\n",
      "cnt: 0 - valLoss: 0.42580586671829224 - trainLoss: 0.3983621597290039\n",
      "cnt: 0 - valLoss: 0.4258056879043579 - trainLoss: 0.39836108684539795\n",
      "cnt: 0 - valLoss: 0.42580553889274597 - trainLoss: 0.39836007356643677\n",
      "cnt: 0 - valLoss: 0.42580533027648926 - trainLoss: 0.3983590006828308\n",
      "cnt: 0 - valLoss: 0.4258051812648773 - trainLoss: 0.39835792779922485\n",
      "cnt: 0 - valLoss: 0.4258050322532654 - trainLoss: 0.3983568549156189\n",
      "cnt: 0 - valLoss: 0.42580482363700867 - trainLoss: 0.39835578203201294\n",
      "cnt: 0 - valLoss: 0.42580464482307434 - trainLoss: 0.398354709148407\n",
      "cnt: 0 - valLoss: 0.4258044958114624 - trainLoss: 0.3983536660671234\n",
      "cnt: 0 - valLoss: 0.4258043169975281 - trainLoss: 0.39835262298583984\n",
      "cnt: 0 - valLoss: 0.42580410838127136 - trainLoss: 0.3983515501022339\n",
      "cnt: 0 - valLoss: 0.4258039593696594 - trainLoss: 0.39835047721862793\n",
      "cnt: 0 - valLoss: 0.4258037507534027 - trainLoss: 0.398349404335022\n",
      "cnt: 0 - valLoss: 0.42580363154411316 - trainLoss: 0.3983483910560608\n",
      "cnt: 0 - valLoss: 0.42580342292785645 - trainLoss: 0.39834728837013245\n",
      "cnt: 0 - valLoss: 0.4258032739162445 - trainLoss: 0.3983462452888489\n",
      "cnt: 0 - valLoss: 0.4258030652999878 - trainLoss: 0.3983451724052429\n",
      "cnt: 0 - valLoss: 0.42580288648605347 - trainLoss: 0.39834409952163696\n",
      "cnt: 0 - valLoss: 0.42580267786979675 - trainLoss: 0.398343026638031\n",
      "cnt: 0 - valLoss: 0.4258024990558624 - trainLoss: 0.39834195375442505\n",
      "cnt: 0 - valLoss: 0.42580240964889526 - trainLoss: 0.39834094047546387\n",
      "cnt: 0 - valLoss: 0.42580220103263855 - trainLoss: 0.3983398377895355\n",
      "cnt: 0 - valLoss: 0.425802081823349 - trainLoss: 0.39833879470825195\n",
      "cnt: 0 - valLoss: 0.4258018136024475 - trainLoss: 0.398337721824646\n",
      "cnt: 0 - valLoss: 0.42580169439315796 - trainLoss: 0.39833664894104004\n",
      "cnt: 0 - valLoss: 0.42580148577690125 - trainLoss: 0.39833563566207886\n",
      "cnt: 0 - valLoss: 0.4258013665676117 - trainLoss: 0.3983345329761505\n",
      "cnt: 0 - valLoss: 0.4258011281490326 - trainLoss: 0.39833346009254456\n",
      "cnt: 0 - valLoss: 0.42580100893974304 - trainLoss: 0.398332417011261\n",
      "cnt: 0 - valLoss: 0.42580080032348633 - trainLoss: 0.39833134412765503\n",
      "cnt: 0 - valLoss: 0.4258006811141968 - trainLoss: 0.39833033084869385\n",
      "cnt: 0 - valLoss: 0.4258004426956177 - trainLoss: 0.3983292579650879\n",
      "cnt: 0 - valLoss: 0.4258003234863281 - trainLoss: 0.39832815527915955\n",
      "cnt: 0 - valLoss: 0.4258001148700714 - trainLoss: 0.398327112197876\n",
      "cnt: 0 - valLoss: 0.4257999360561371 - trainLoss: 0.39832603931427\n",
      "cnt: 0 - valLoss: 0.42579975724220276 - trainLoss: 0.39832496643066406\n",
      "cnt: 0 - valLoss: 0.4257996082305908 - trainLoss: 0.3983238935470581\n",
      "cnt: 0 - valLoss: 0.4257994294166565 - trainLoss: 0.3983228802680969\n",
      "cnt: 0 - valLoss: 0.42579925060272217 - trainLoss: 0.3983217775821686\n",
      "cnt: 0 - valLoss: 0.4257991313934326 - trainLoss: 0.398320734500885\n",
      "cnt: 0 - valLoss: 0.4257989227771759 - trainLoss: 0.39831966161727905\n",
      "cnt: 0 - valLoss: 0.42579877376556396 - trainLoss: 0.3983185887336731\n",
      "cnt: 0 - valLoss: 0.42579856514930725 - trainLoss: 0.39831751585006714\n",
      "cnt: 0 - valLoss: 0.4257983863353729 - trainLoss: 0.39831650257110596\n",
      "cnt: 0 - valLoss: 0.425798237323761 - trainLoss: 0.3983154296875\n",
      "cnt: 0 - valLoss: 0.42579805850982666 - trainLoss: 0.39831435680389404\n",
      "cnt: 0 - valLoss: 0.42579787969589233 - trainLoss: 0.3983132839202881\n",
      "cnt: 0 - valLoss: 0.425797700881958 - trainLoss: 0.3983122706413269\n",
      "cnt: 0 - valLoss: 0.42579755187034607 - trainLoss: 0.39831113815307617\n",
      "cnt: 0 - valLoss: 0.42579737305641174 - trainLoss: 0.398310124874115\n",
      "cnt: 0 - valLoss: 0.4257971942424774 - trainLoss: 0.39830905199050903\n",
      "cnt: 0 - valLoss: 0.4257970154285431 - trainLoss: 0.3983079791069031\n",
      "cnt: 0 - valLoss: 0.42579686641693115 - trainLoss: 0.3983069658279419\n",
      "cnt: 0 - valLoss: 0.4257966876029968 - trainLoss: 0.39830583333969116\n",
      "cnt: 0 - valLoss: 0.4257965087890625 - trainLoss: 0.39830482006073\n",
      "cnt: 0 - valLoss: 0.4257963299751282 - trainLoss: 0.398303747177124\n",
      "cnt: 0 - valLoss: 0.42579615116119385 - trainLoss: 0.39830267429351807\n",
      "cnt: 0 - valLoss: 0.4257960021495819 - trainLoss: 0.3983016014099121\n",
      "cnt: 0 - valLoss: 0.4257957935333252 - trainLoss: 0.39830052852630615\n",
      "cnt: 0 - valLoss: 0.42579564452171326 - trainLoss: 0.39829951524734497\n",
      "cnt: 0 - valLoss: 0.42579546570777893 - trainLoss: 0.398298442363739\n",
      "cnt: 0 - valLoss: 0.425795316696167 - trainLoss: 0.39829736948013306\n",
      "cnt: 0 - valLoss: 0.42579513788223267 - trainLoss: 0.3982962965965271\n",
      "cnt: 0 - valLoss: 0.42579495906829834 - trainLoss: 0.39829522371292114\n",
      "cnt: 0 - valLoss: 0.4257948398590088 - trainLoss: 0.39829421043395996\n",
      "cnt: 0 - valLoss: 0.4257946312427521 - trainLoss: 0.39829307794570923\n",
      "cnt: 0 - valLoss: 0.42579448223114014 - trainLoss: 0.39829206466674805\n",
      "cnt: 0 - valLoss: 0.4257942736148834 - trainLoss: 0.3982910215854645\n",
      "cnt: 0 - valLoss: 0.4257940948009491 - trainLoss: 0.39828991889953613\n",
      "cnt: 0 - valLoss: 0.42579394578933716 - trainLoss: 0.39828890562057495\n",
      "cnt: 0 - valLoss: 0.42579376697540283 - trainLoss: 0.3982878625392914\n",
      "cnt: 0 - valLoss: 0.4257935881614685 - trainLoss: 0.39828675985336304\n",
      "cnt: 0 - valLoss: 0.4257934093475342 - trainLoss: 0.39828571677207947\n",
      "cnt: 0 - valLoss: 0.42579326033592224 - trainLoss: 0.3982846438884735\n",
      "cnt: 0 - valLoss: 0.4257930815219879 - trainLoss: 0.39828357100486755\n",
      "cnt: 0 - valLoss: 0.4257929027080536 - trainLoss: 0.39828255772590637\n",
      "cnt: 0 - valLoss: 0.42579272389411926 - trainLoss: 0.398281455039978\n",
      "cnt: 0 - valLoss: 0.4257925748825073 - trainLoss: 0.39828041195869446\n",
      "cnt: 0 - valLoss: 0.4257924258708954 - trainLoss: 0.3982793390750885\n",
      "cnt: 0 - valLoss: 0.42579221725463867 - trainLoss: 0.39827826619148254\n",
      "cnt: 0 - valLoss: 0.4257920980453491 - trainLoss: 0.39827725291252136\n",
      "cnt: 0 - valLoss: 0.4257918894290924 - trainLoss: 0.398276150226593\n",
      "cnt: 0 - valLoss: 0.42579174041748047 - trainLoss: 0.39827510714530945\n",
      "cnt: 0 - valLoss: 0.4257916212081909 - trainLoss: 0.3982740342617035\n",
      "cnt: 0 - valLoss: 0.4257914125919342 - trainLoss: 0.39827296137809753\n",
      "cnt: 0 - valLoss: 0.42579129338264465 - trainLoss: 0.39827194809913635\n",
      "cnt: 0 - valLoss: 0.4257911145687103 - trainLoss: 0.3982708752155304\n",
      "cnt: 0 - valLoss: 0.425790935754776 - trainLoss: 0.39826980233192444\n",
      "cnt: 0 - valLoss: 0.4257907569408417 - trainLoss: 0.39826878905296326\n",
      "cnt: 0 - valLoss: 0.42579060792922974 - trainLoss: 0.3982676565647125\n",
      "cnt: 0 - valLoss: 0.4257904291152954 - trainLoss: 0.39826664328575134\n",
      "cnt: 0 - valLoss: 0.4257902503013611 - trainLoss: 0.3982655704021454\n",
      "cnt: 0 - valLoss: 0.42579007148742676 - trainLoss: 0.39826449751853943\n",
      "cnt: 0 - valLoss: 0.4257899224758148 - trainLoss: 0.39826345443725586\n",
      "cnt: 0 - valLoss: 0.4257897436618805 - trainLoss: 0.3982623517513275\n",
      "cnt: 0 - valLoss: 0.42578956484794617 - trainLoss: 0.39826133847236633\n",
      "cnt: 0 - valLoss: 0.42578938603401184 - trainLoss: 0.3982602655887604\n",
      "cnt: 0 - valLoss: 0.4257892370223999 - trainLoss: 0.3982592225074768\n",
      "cnt: 0 - valLoss: 0.4257890582084656 - trainLoss: 0.39825814962387085\n",
      "cnt: 0 - valLoss: 0.42578887939453125 - trainLoss: 0.39825713634490967\n",
      "cnt: 0 - valLoss: 0.4257887005805969 - trainLoss: 0.3982560336589813\n",
      "cnt: 0 - valLoss: 0.4257885217666626 - trainLoss: 0.39825499057769775\n",
      "cnt: 0 - valLoss: 0.42578837275505066 - trainLoss: 0.3982539176940918\n",
      "cnt: 0 - valLoss: 0.42578819394111633 - trainLoss: 0.39825284481048584\n",
      "cnt: 0 - valLoss: 0.4257880747318268 - trainLoss: 0.39825183153152466\n",
      "cnt: 0 - valLoss: 0.4257878363132477 - trainLoss: 0.3982507586479187\n",
      "cnt: 0 - valLoss: 0.42578771710395813 - trainLoss: 0.39824968576431274\n",
      "cnt: 0 - valLoss: 0.4257875084877014 - trainLoss: 0.39824867248535156\n",
      "cnt: 0 - valLoss: 0.4257873296737671 - trainLoss: 0.39824753999710083\n",
      "cnt: 0 - valLoss: 0.42578715085983276 - trainLoss: 0.39824652671813965\n",
      "cnt: 0 - valLoss: 0.4257870316505432 - trainLoss: 0.3982454538345337\n",
      "cnt: 0 - valLoss: 0.4257868230342865 - trainLoss: 0.39824438095092773\n",
      "cnt: 0 - valLoss: 0.4257866442203522 - trainLoss: 0.39824333786964417\n",
      "cnt: 0 - valLoss: 0.42578646540641785 - trainLoss: 0.3982422947883606\n",
      "cnt: 0 - valLoss: 0.4257863461971283 - trainLoss: 0.39824122190475464\n",
      "cnt: 0 - valLoss: 0.4257861375808716 - trainLoss: 0.39824017882347107\n",
      "cnt: 0 - valLoss: 0.42578595876693726 - trainLoss: 0.3982390761375427\n",
      "cnt: 0 - valLoss: 0.42578577995300293 - trainLoss: 0.39823803305625916\n",
      "cnt: 0 - valLoss: 0.425785630941391 - trainLoss: 0.3982369601726532\n",
      "cnt: 0 - valLoss: 0.42578545212745667 - trainLoss: 0.398235946893692\n",
      "cnt: 0 - valLoss: 0.4257853329181671 - trainLoss: 0.39823487401008606\n",
      "cnt: 0 - valLoss: 0.425785094499588 - trainLoss: 0.3982338011264801\n",
      "cnt: 0 - valLoss: 0.4257849454879761 - trainLoss: 0.39823272824287415\n",
      "cnt: 0 - valLoss: 0.42578476667404175 - trainLoss: 0.39823171496391296\n",
      "cnt: 0 - valLoss: 0.4257846474647522 - trainLoss: 0.398230642080307\n",
      "cnt: 0 - valLoss: 0.4257844090461731 - trainLoss: 0.39822956919670105\n",
      "cnt: 0 - valLoss: 0.42578428983688354 - trainLoss: 0.39822855591773987\n",
      "cnt: 0 - valLoss: 0.42578408122062683 - trainLoss: 0.3982274830341339\n",
      "cnt: 0 - valLoss: 0.4257839620113373 - trainLoss: 0.39822641015052795\n",
      "cnt: 0 - valLoss: 0.4257837235927582 - trainLoss: 0.3982253670692444\n",
      "cnt: 0 - valLoss: 0.4257836639881134 - trainLoss: 0.3982242941856384\n",
      "cnt: 0 - valLoss: 0.4257834851741791 - trainLoss: 0.39822322130203247\n",
      "cnt: 0 - valLoss: 0.42578327655792236 - trainLoss: 0.3982221782207489\n",
      "cnt: 0 - valLoss: 0.42578309774398804 - trainLoss: 0.39822113513946533\n",
      "cnt: 0 - valLoss: 0.4257829189300537 - trainLoss: 0.3982200622558594\n",
      "cnt: 0 - valLoss: 0.42578279972076416 - trainLoss: 0.3982190489768982\n",
      "cnt: 0 - valLoss: 0.42578259110450745 - trainLoss: 0.39821797609329224\n",
      "cnt: 0 - valLoss: 0.4257824420928955 - trainLoss: 0.3982169032096863\n",
      "cnt: 0 - valLoss: 0.4257822334766388 - trainLoss: 0.3982158601284027\n",
      "cnt: 0 - valLoss: 0.42578211426734924 - trainLoss: 0.39821475744247437\n",
      "cnt: 0 - valLoss: 0.4257819354534149 - trainLoss: 0.3982137441635132\n",
      "cnt: 0 - valLoss: 0.4257810711860657 - trainLoss: 0.3982126712799072\n",
      "cnt: 0 - valLoss: 0.4257809817790985 - trainLoss: 0.39821159839630127\n",
      "cnt: 0 - valLoss: 0.4257808327674866 - trainLoss: 0.3982105553150177\n",
      "cnt: 0 - valLoss: 0.4257800579071045 - trainLoss: 0.39820948243141174\n",
      "cnt: 0 - valLoss: 0.42577987909317017 - trainLoss: 0.39820846915245056\n",
      "cnt: 0 - valLoss: 0.4257797598838806 - trainLoss: 0.3982073962688446\n",
      "cnt: 0 - valLoss: 0.42577892541885376 - trainLoss: 0.3982063829898834\n",
      "cnt: 0 - valLoss: 0.4257788062095642 - trainLoss: 0.39820533990859985\n",
      "cnt: 0 - valLoss: 0.42577868700027466 - trainLoss: 0.3982042670249939\n",
      "cnt: 0 - valLoss: 0.4257778525352478 - trainLoss: 0.39820319414138794\n",
      "cnt: 0 - valLoss: 0.42577773332595825 - trainLoss: 0.39820218086242676\n",
      "cnt: 0 - valLoss: 0.4257776141166687 - trainLoss: 0.3982011079788208\n",
      "cnt: 0 - valLoss: 0.4257768392562866 - trainLoss: 0.39820003509521484\n",
      "cnt: 0 - valLoss: 0.42577672004699707 - trainLoss: 0.39819902181625366\n",
      "cnt: 0 - valLoss: 0.42577654123306274 - trainLoss: 0.3981979489326477\n",
      "cnt: 0 - valLoss: 0.42577576637268066 - trainLoss: 0.39819687604904175\n",
      "cnt: 0 - valLoss: 0.4257756769657135 - trainLoss: 0.3981958329677582\n",
      "cnt: 0 - valLoss: 0.4257754981517792 - trainLoss: 0.3981947600841522\n",
      "cnt: 0 - valLoss: 0.4257747232913971 - trainLoss: 0.39819374680519104\n",
      "cnt: 0 - valLoss: 0.4257746636867523 - trainLoss: 0.3981926739215851\n",
      "cnt: 0 - valLoss: 0.4257745146751404 - trainLoss: 0.3981916606426239\n",
      "cnt: 0 - valLoss: 0.4257737100124359 - trainLoss: 0.39819058775901794\n",
      "cnt: 0 - valLoss: 0.42577362060546875 - trainLoss: 0.398189514875412\n",
      "cnt: 0 - valLoss: 0.4257735311985016 - trainLoss: 0.3981884717941284\n",
      "cnt: 0 - valLoss: 0.42577266693115234 - trainLoss: 0.39818745851516724\n",
      "cnt: 0 - valLoss: 0.4257725775241852 - trainLoss: 0.3981863856315613\n",
      "cnt: 0 - valLoss: 0.4257725179195404 - trainLoss: 0.3981853127479553\n",
      "cnt: 0 - valLoss: 0.4257717430591583 - trainLoss: 0.39818429946899414\n",
      "cnt: 0 - valLoss: 0.42577165365219116 - trainLoss: 0.39818325638771057\n",
      "cnt: 0 - valLoss: 0.4257708787918091 - trainLoss: 0.3981821835041046\n",
      "cnt: 0 - valLoss: 0.42577075958251953 - trainLoss: 0.39818111062049866\n",
      "cnt: 0 - valLoss: 0.4257706105709076 - trainLoss: 0.3981800973415375\n",
      "cnt: 0 - valLoss: 0.4257698357105255 - trainLoss: 0.3981790244579315\n",
      "cnt: 0 - valLoss: 0.42576977610588074 - trainLoss: 0.39817798137664795\n",
      "cnt: 0 - valLoss: 0.4257696866989136 - trainLoss: 0.3981769382953644\n",
      "cnt: 0 - valLoss: 0.4257689118385315 - trainLoss: 0.3981758952140808\n",
      "cnt: 0 - valLoss: 0.42576882243156433 - trainLoss: 0.39817482233047485\n",
      "cnt: 0 - valLoss: 0.4257687032222748 - trainLoss: 0.3981737494468689\n",
      "cnt: 0 - valLoss: 0.4257679283618927 - trainLoss: 0.3981727361679077\n",
      "cnt: 0 - valLoss: 0.42576783895492554 - trainLoss: 0.39817169308662415\n",
      "cnt: 0 - valLoss: 0.4257677495479584 - trainLoss: 0.3981706202030182\n",
      "cnt: 0 - valLoss: 0.4257669746875763 - trainLoss: 0.39816954731941223\n",
      "cnt: 0 - valLoss: 0.4257669448852539 - trainLoss: 0.39816853404045105\n",
      "cnt: 0 - valLoss: 0.4257661700248718 - trainLoss: 0.39816752076148987\n",
      "cnt: 0 - valLoss: 0.42576608061790466 - trainLoss: 0.3981664478778839\n",
      "cnt: 0 - valLoss: 0.4257659614086151 - trainLoss: 0.39816537499427795\n",
      "cnt: 0 - valLoss: 0.4257652163505554 - trainLoss: 0.3981643319129944\n",
      "cnt: 0 - valLoss: 0.42576512694358826 - trainLoss: 0.3981633186340332\n",
      "cnt: 0 - valLoss: 0.42576509714126587 - trainLoss: 0.39816224575042725\n",
      "cnt: 0 - valLoss: 0.4257643520832062 - trainLoss: 0.3981611728668213\n",
      "cnt: 0 - valLoss: 0.425764262676239 - trainLoss: 0.3981601595878601\n",
      "cnt: 0 - valLoss: 0.42576420307159424 - trainLoss: 0.39815908670425415\n",
      "cnt: 0 - valLoss: 0.42576345801353455 - trainLoss: 0.3981580436229706\n",
      "cnt: 0 - valLoss: 0.4257633686065674 - trainLoss: 0.3981570303440094\n",
      "cnt: 0 - valLoss: 0.4257626533508301 - trainLoss: 0.39815595746040344\n",
      "cnt: 0 - valLoss: 0.4257625639438629 - trainLoss: 0.3981548845767975\n",
      "cnt: 0 - valLoss: 0.4257625341415405 - trainLoss: 0.3981538414955139\n",
      "cnt: 0 - valLoss: 0.42576178908348083 - trainLoss: 0.39815276861190796\n",
      "cnt: 0 - valLoss: 0.42576169967651367 - trainLoss: 0.3981517553329468\n",
      "cnt: 0 - valLoss: 0.4257616698741913 - trainLoss: 0.3981506824493408\n",
      "cnt: 0 - valLoss: 0.4257609248161316 - trainLoss: 0.398149698972702\n",
      "cnt: 0 - valLoss: 0.42576083540916443 - trainLoss: 0.3981485962867737\n",
      "cnt: 0 - valLoss: 0.4257601499557495 - trainLoss: 0.3981475532054901\n",
      "cnt: 0 - valLoss: 0.42576009035110474 - trainLoss: 0.39814653992652893\n",
      "cnt: 0 - valLoss: 0.42576003074645996 - trainLoss: 0.39814549684524536\n",
      "cnt: 0 - valLoss: 0.42575931549072266 - trainLoss: 0.3981444239616394\n",
      "cnt: 0 - valLoss: 0.4257592558860779 - trainLoss: 0.3981434106826782\n",
      "cnt: 0 - valLoss: 0.4257591664791107 - trainLoss: 0.39814233779907227\n",
      "cnt: 0 - valLoss: 0.4257584810256958 - trainLoss: 0.3981412649154663\n",
      "cnt: 0 - valLoss: 0.42575836181640625 - trainLoss: 0.3981402516365051\n",
      "cnt: 0 - valLoss: 0.42575767636299133 - trainLoss: 0.39813917875289917\n",
      "cnt: 0 - valLoss: 0.42575764656066895 - trainLoss: 0.3981381356716156\n",
      "cnt: 0 - valLoss: 0.42575758695602417 - trainLoss: 0.3981371223926544\n",
      "cnt: 0 - valLoss: 0.42575687170028687 - trainLoss: 0.39813607931137085\n",
      "cnt: 0 - valLoss: 0.4257568120956421 - trainLoss: 0.3981350064277649\n",
      "cnt: 0 - valLoss: 0.4257561266422272 - trainLoss: 0.3981339931488037\n",
      "cnt: 0 - valLoss: 0.4257560968399048 - trainLoss: 0.39813292026519775\n",
      "cnt: 0 - valLoss: 0.4257560074329376 - trainLoss: 0.3981319069862366\n",
      "cnt: 0 - valLoss: 0.4257553219795227 - trainLoss: 0.398130863904953\n",
      "cnt: 0 - valLoss: 0.42575526237487793 - trainLoss: 0.39812979102134705\n",
      "cnt: 0 - valLoss: 0.425754576921463 - trainLoss: 0.3981287181377411\n",
      "cnt: 0 - valLoss: 0.425754576921463 - trainLoss: 0.3981277048587799\n",
      "cnt: 0 - valLoss: 0.42575451731681824 - trainLoss: 0.39812663197517395\n",
      "cnt: 0 - valLoss: 0.42575377225875854 - trainLoss: 0.39812564849853516\n",
      "cnt: 0 - valLoss: 0.42575374245643616 - trainLoss: 0.3981245756149292\n",
      "cnt: 0 - valLoss: 0.42575374245643616 - trainLoss: 0.39812350273132324\n",
      "cnt: 0 - valLoss: 0.42575305700302124 - trainLoss: 0.39812248945236206\n",
      "cnt: 0 - valLoss: 0.4257529675960541 - trainLoss: 0.3981214463710785\n",
      "cnt: 0 - valLoss: 0.42575228214263916 - trainLoss: 0.39812037348747253\n",
      "cnt: 0 - valLoss: 0.4257522225379944 - trainLoss: 0.3981193006038666\n",
      "cnt: 0 - valLoss: 0.425752192735672 - trainLoss: 0.3981182873249054\n",
      "cnt: 0 - valLoss: 0.4257515072822571 - trainLoss: 0.3981172442436218\n",
      "cnt: 0 - valLoss: 0.4257515072822571 - trainLoss: 0.39811620116233826\n",
      "cnt: 0 - valLoss: 0.4257507622241974 - trainLoss: 0.3981151580810547\n",
      "cnt: 0 - valLoss: 0.42575082182884216 - trainLoss: 0.3981141448020935\n",
      "cnt: 1 - valLoss: 0.425750732421875 - trainLoss: 0.39811307191848755\n",
      "cnt: 0 - valLoss: 0.4257500469684601 - trainLoss: 0.398112028837204\n",
      "cnt: 0 - valLoss: 0.4257500469684601 - trainLoss: 0.3981110155582428\n",
      "cnt: 0 - valLoss: 0.4257493317127228 - trainLoss: 0.39810997247695923\n",
      "cnt: 0 - valLoss: 0.425749272108078 - trainLoss: 0.39810889959335327\n",
      "cnt: 0 - valLoss: 0.4257492423057556 - trainLoss: 0.3981078565120697\n",
      "cnt: 0 - valLoss: 0.4257485866546631 - trainLoss: 0.39810681343078613\n",
      "cnt: 0 - valLoss: 0.4257485866546631 - trainLoss: 0.39810580015182495\n",
      "cnt: 0 - valLoss: 0.42574790120124817 - trainLoss: 0.3981047570705414\n",
      "cnt: 0 - valLoss: 0.4257478713989258 - trainLoss: 0.3981036841869354\n",
      "cnt: 0 - valLoss: 0.4257478713989258 - trainLoss: 0.39810261130332947\n",
      "cnt: 0 - valLoss: 0.42574721574783325 - trainLoss: 0.3981015980243683\n",
      "cnt: 0 - valLoss: 0.42574718594551086 - trainLoss: 0.3981005549430847\n",
      "cnt: 0 - valLoss: 0.42574653029441833 - trainLoss: 0.39809954166412354\n",
      "cnt: 0 - valLoss: 0.42574653029441833 - trainLoss: 0.3980984687805176\n",
      "cnt: 0 - valLoss: 0.4257458448410034 - trainLoss: 0.3980974555015564\n",
      "cnt: 0 - valLoss: 0.4257458448410034 - trainLoss: 0.39809638261795044\n",
      "cnt: 0 - valLoss: 0.42574581503868103 - trainLoss: 0.3980953097343445\n",
      "cnt: 0 - valLoss: 0.4257451593875885 - trainLoss: 0.3980942666530609\n",
      "cnt: 0 - valLoss: 0.4257451593875885 - trainLoss: 0.39809325337409973\n",
      "cnt: 0 - valLoss: 0.4257444739341736 - trainLoss: 0.3980921804904938\n",
      "cnt: 0 - valLoss: 0.4257444739341736 - trainLoss: 0.3980911374092102\n",
      "cnt: 0 - valLoss: 0.42574450373649597 - trainLoss: 0.398090124130249\n",
      "cnt: 1 - valLoss: 0.42574387788772583 - trainLoss: 0.39808908104896545\n",
      "cnt: 0 - valLoss: 0.42574387788772583 - trainLoss: 0.3980880379676819\n",
      "cnt: 0 - valLoss: 0.4257431924343109 - trainLoss: 0.3980869948863983\n",
      "cnt: 0 - valLoss: 0.4257431924343109 - trainLoss: 0.39808592200279236\n",
      "cnt: 0 - valLoss: 0.4257425367832184 - trainLoss: 0.3980849087238312\n",
      "cnt: 0 - valLoss: 0.42574259638786316 - trainLoss: 0.3980838656425476\n",
      "cnt: 1 - valLoss: 0.4257425367832184 - trainLoss: 0.39808279275894165\n",
      "cnt: 0 - valLoss: 0.42574191093444824 - trainLoss: 0.39808180928230286\n",
      "cnt: 0 - valLoss: 0.42574191093444824 - trainLoss: 0.3980807065963745\n",
      "cnt: 0 - valLoss: 0.4257412254810333 - trainLoss: 0.39807966351509094\n",
      "cnt: 0 - valLoss: 0.4257412552833557 - trainLoss: 0.39807865023612976\n",
      "cnt: 1 - valLoss: 0.42574062943458557 - trainLoss: 0.3980776369571686\n",
      "cnt: 0 - valLoss: 0.42574062943458557 - trainLoss: 0.3980765640735626\n",
      "cnt: 0 - valLoss: 0.42574062943458557 - trainLoss: 0.39807552099227905\n",
      "cnt: 0 - valLoss: 0.42573997378349304 - trainLoss: 0.3980744481086731\n",
      "cnt: 0 - valLoss: 0.42574000358581543 - trainLoss: 0.3980734348297119\n",
      "cnt: 1 - valLoss: 0.4257393181324005 - trainLoss: 0.39807239174842834\n",
      "cnt: 0 - valLoss: 0.4257393777370453 - trainLoss: 0.39807137846946716\n",
      "cnt: 1 - valLoss: 0.42573872208595276 - trainLoss: 0.3980703055858612\n",
      "cnt: 0 - valLoss: 0.42573878169059753 - trainLoss: 0.39806926250457764\n",
      "cnt: 1 - valLoss: 0.42573878169059753 - trainLoss: 0.39806821942329407\n",
      "cnt: 2 - valLoss: 0.425738126039505 - trainLoss: 0.3980671763420105\n",
      "cnt: 0 - valLoss: 0.425738126039505 - trainLoss: 0.3980661630630493\n",
      "cnt: 0 - valLoss: 0.42573752999305725 - trainLoss: 0.39806511998176575\n",
      "cnt: 0 - valLoss: 0.42573752999305725 - trainLoss: 0.3980640470981598\n",
      "cnt: 0 - valLoss: 0.4257369339466095 - trainLoss: 0.3980630338191986\n",
      "cnt: 0 - valLoss: 0.4257369339466095 - trainLoss: 0.39806196093559265\n",
      "cnt: 0 - valLoss: 0.4257369637489319 - trainLoss: 0.3980609178543091\n",
      "cnt: 1 - valLoss: 0.42573633790016174 - trainLoss: 0.3980599045753479\n",
      "cnt: 0 - valLoss: 0.42573636770248413 - trainLoss: 0.39805883169174194\n",
      "cnt: 1 - valLoss: 0.425735741853714 - trainLoss: 0.39805784821510315\n",
      "cnt: 0 - valLoss: 0.4257357716560364 - trainLoss: 0.3980567753314972\n",
      "cnt: 1 - valLoss: 0.42573511600494385 - trainLoss: 0.39805570244789124\n",
      "cnt: 0 - valLoss: 0.4257351756095886 - trainLoss: 0.39805468916893005\n",
      "cnt: 1 - valLoss: 0.4257345199584961 - trainLoss: 0.3980536460876465\n",
      "cnt: 0 - valLoss: 0.42573457956314087 - trainLoss: 0.3980525732040405\n",
      "cnt: 1 - valLoss: 0.42573460936546326 - trainLoss: 0.39805155992507935\n",
      "cnt: 2 - valLoss: 0.4257340133190155 - trainLoss: 0.3980504870414734\n",
      "cnt: 0 - valLoss: 0.4257340133190155 - trainLoss: 0.3980494737625122\n",
      "cnt: 0 - valLoss: 0.42573341727256775 - trainLoss: 0.39804843068122864\n",
      "cnt: 0 - valLoss: 0.42573344707489014 - trainLoss: 0.39804741740226746\n",
      "cnt: 1 - valLoss: 0.42573282122612 - trainLoss: 0.3980463445186615\n",
      "cnt: 0 - valLoss: 0.4257328510284424 - trainLoss: 0.3980453610420227\n",
      "cnt: 1 - valLoss: 0.42573222517967224 - trainLoss: 0.398044228553772\n",
      "cnt: 0 - valLoss: 0.42573225498199463 - trainLoss: 0.3980432450771332\n",
      "cnt: 1 - valLoss: 0.4257323443889618 - trainLoss: 0.3980421721935272\n",
      "cnt: 2 - valLoss: 0.42573168873786926 - trainLoss: 0.39804115891456604\n",
      "cnt: 0 - valLoss: 0.42573174834251404 - trainLoss: 0.39804011583328247\n",
      "cnt: 1 - valLoss: 0.4257310926914215 - trainLoss: 0.3980390727519989\n",
      "cnt: 0 - valLoss: 0.4257311522960663 - trainLoss: 0.39803802967071533\n",
      "cnt: 1 - valLoss: 0.42573055624961853 - trainLoss: 0.3980369567871094\n",
      "cnt: 0 - valLoss: 0.4257305860519409 - trainLoss: 0.3980359435081482\n",
      "cnt: 1 - valLoss: 0.42572999000549316 - trainLoss: 0.3980349004268646\n",
      "cnt: 0 - valLoss: 0.4257300794124603 - trainLoss: 0.39803388714790344\n",
      "cnt: 1 - valLoss: 0.4257300794124603 - trainLoss: 0.3980328142642975\n",
      "cnt: 2 - valLoss: 0.4257294833660126 - trainLoss: 0.3980318307876587\n",
      "cnt: 0 - valLoss: 0.42572951316833496 - trainLoss: 0.39803069829940796\n",
      "cnt: 1 - valLoss: 0.4257289171218872 - trainLoss: 0.3980296850204468\n",
      "cnt: 0 - valLoss: 0.4257289469242096 - trainLoss: 0.3980286717414856\n",
      "cnt: 1 - valLoss: 0.42572835087776184 - trainLoss: 0.398027628660202\n",
      "cnt: 0 - valLoss: 0.425728440284729 - trainLoss: 0.39802661538124084\n",
      "cnt: 1 - valLoss: 0.42572784423828125 - trainLoss: 0.3980255424976349\n",
      "cnt: 0 - valLoss: 0.4257279336452484 - trainLoss: 0.3980244994163513\n",
      "cnt: 1 - valLoss: 0.42572733759880066 - trainLoss: 0.3980235159397125\n",
      "cnt: 0 - valLoss: 0.42572733759880066 - trainLoss: 0.39802244305610657\n",
      "cnt: 0 - valLoss: 0.4257267415523529 - trainLoss: 0.3980213701725006\n",
      "cnt: 0 - valLoss: 0.4257268011569977 - trainLoss: 0.39802035689353943\n",
      "cnt: 1 - valLoss: 0.42572689056396484 - trainLoss: 0.39801931381225586\n",
      "cnt: 2 - valLoss: 0.4257262945175171 - trainLoss: 0.3980183005332947\n",
      "cnt: 0 - valLoss: 0.4257262945175171 - trainLoss: 0.3980172276496887\n",
      "cnt: 0 - valLoss: 0.4257257580757141 - trainLoss: 0.39801615476608276\n",
      "cnt: 0 - valLoss: 0.4257257878780365 - trainLoss: 0.3980151414871216\n",
      "cnt: 1 - valLoss: 0.42572519183158875 - trainLoss: 0.398014098405838\n",
      "cnt: 0 - valLoss: 0.4257252812385559 - trainLoss: 0.39801308512687683\n",
      "cnt: 1 - valLoss: 0.4257248342037201 - trainLoss: 0.39801204204559326\n",
      "cnt: 0 - valLoss: 0.4257250726222992 - trainLoss: 0.3980110287666321\n",
      "cnt: 1 - valLoss: 0.425724595785141 - trainLoss: 0.3980099856853485\n",
      "cnt: 0 - valLoss: 0.4257248342037201 - trainLoss: 0.39800897240638733\n",
      "cnt: 1 - valLoss: 0.4257243871688843 - trainLoss: 0.39800792932510376\n",
      "cnt: 0 - valLoss: 0.42572394013404846 - trainLoss: 0.3980069160461426\n",
      "cnt: 0 - valLoss: 0.4257241487503052 - trainLoss: 0.3980058431625366\n",
      "cnt: 1 - valLoss: 0.42572373151779175 - trainLoss: 0.39800482988357544\n",
      "cnt: 0 - valLoss: 0.4257239103317261 - trainLoss: 0.39800384640693665\n",
      "cnt: 1 - valLoss: 0.42572352290153503 - trainLoss: 0.3980028033256531\n",
      "cnt: 0 - valLoss: 0.42572304606437683 - trainLoss: 0.3980017304420471\n",
      "cnt: 0 - valLoss: 0.42572325468063354 - trainLoss: 0.39800071716308594\n",
      "cnt: 1 - valLoss: 0.4257228374481201 - trainLoss: 0.39799973368644714\n",
      "cnt: 0 - valLoss: 0.42572304606437683 - trainLoss: 0.3979986608028412\n",
      "cnt: 1 - valLoss: 0.4257226288318634 - trainLoss: 0.3979976177215576\n",
      "cnt: 0 - valLoss: 0.4257221817970276 - trainLoss: 0.39799660444259644\n",
      "cnt: 0 - valLoss: 0.4257223904132843 - trainLoss: 0.39799556136131287\n",
      "cnt: 1 - valLoss: 0.4257219433784485 - trainLoss: 0.3979944884777069\n",
      "cnt: 0 - valLoss: 0.4257221817970276 - trainLoss: 0.3979935050010681\n",
      "cnt: 1 - valLoss: 0.4257217049598694 - trainLoss: 0.39799246191978455\n",
      "cnt: 0 - valLoss: 0.4257219731807709 - trainLoss: 0.397991418838501\n",
      "cnt: 1 - valLoss: 0.42572155594825745 - trainLoss: 0.3979904353618622\n",
      "cnt: 0 - valLoss: 0.42572107911109924 - trainLoss: 0.3979893624782562\n",
      "cnt: 0 - valLoss: 0.42572131752967834 - trainLoss: 0.39798834919929504\n",
      "cnt: 1 - valLoss: 0.4257209002971649 - trainLoss: 0.3979873061180115\n",
      "cnt: 0 - valLoss: 0.42572107911109924 - trainLoss: 0.3979862928390503\n",
      "cnt: 1 - valLoss: 0.4257206320762634 - trainLoss: 0.3979852497577667\n",
      "cnt: 0 - valLoss: 0.42572021484375 - trainLoss: 0.39798423647880554\n",
      "cnt: 0 - valLoss: 0.4257204830646515 - trainLoss: 0.397983193397522\n",
      "cnt: 1 - valLoss: 0.4257200360298157 - trainLoss: 0.3979821801185608\n",
      "cnt: 0 - valLoss: 0.4257202446460724 - trainLoss: 0.3979811370372772\n",
      "cnt: 1 - valLoss: 0.42571985721588135 - trainLoss: 0.39798012375831604\n",
      "cnt: 0 - valLoss: 0.42571941018104553 - trainLoss: 0.3979790508747101\n",
      "cnt: 0 - valLoss: 0.42571961879730225 - trainLoss: 0.3979780673980713\n",
      "cnt: 1 - valLoss: 0.4257192313671112 - trainLoss: 0.3979770541191101\n",
      "cnt: 0 - valLoss: 0.4257194399833679 - trainLoss: 0.39797601103782654\n",
      "cnt: 1 - valLoss: 0.4257190227508545 - trainLoss: 0.39797499775886536\n",
      "cnt: 0 - valLoss: 0.4257185757160187 - trainLoss: 0.3979739248752594\n",
      "cnt: 0 - valLoss: 0.42571884393692017 - trainLoss: 0.3979729413986206\n",
      "cnt: 1 - valLoss: 0.42571842670440674 - trainLoss: 0.39797189831733704\n",
      "cnt: 0 - valLoss: 0.42571863532066345 - trainLoss: 0.3979708254337311\n",
      "cnt: 1 - valLoss: 0.42571818828582764 - trainLoss: 0.3979698121547699\n",
      "cnt: 0 - valLoss: 0.4257178008556366 - trainLoss: 0.39796876907348633\n",
      "cnt: 0 - valLoss: 0.4257180392742157 - trainLoss: 0.39796778559684753\n",
      "cnt: 1 - valLoss: 0.4257175922393799 - trainLoss: 0.39796677231788635\n",
      "cnt: 0 - valLoss: 0.4257178008556366 - trainLoss: 0.3979657292366028\n",
      "cnt: 1 - valLoss: 0.42571744322776794 - trainLoss: 0.3979646563529968\n",
      "cnt: 0 - valLoss: 0.4257170557975769 - trainLoss: 0.39796367287635803\n",
      "cnt: 0 - valLoss: 0.4257172644138336 - trainLoss: 0.39796262979507446\n",
      "cnt: 1 - valLoss: 0.4257168173789978 - trainLoss: 0.3979615867137909\n",
      "cnt: 0 - valLoss: 0.42571642994880676 - trainLoss: 0.3979605734348297\n",
      "cnt: 0 - valLoss: 0.4257166087627411 - trainLoss: 0.39795953035354614\n",
      "cnt: 1 - valLoss: 0.42571622133255005 - trainLoss: 0.39795851707458496\n",
      "cnt: 0 - valLoss: 0.42571642994880676 - trainLoss: 0.3979574739933014\n",
      "cnt: 1 - valLoss: 0.4257161021232605 - trainLoss: 0.39795640110969543\n",
      "cnt: 0 - valLoss: 0.4257156252861023 - trainLoss: 0.39795541763305664\n",
      "cnt: 0 - valLoss: 0.42571592330932617 - trainLoss: 0.39795440435409546\n",
      "cnt: 1 - valLoss: 0.42571550607681274 - trainLoss: 0.3979533612728119\n",
      "cnt: 0 - valLoss: 0.42571571469306946 - trainLoss: 0.3979523479938507\n",
      "cnt: 1 - valLoss: 0.4257153272628784 - trainLoss: 0.3979513645172119\n",
      "cnt: 0 - valLoss: 0.425714910030365 - trainLoss: 0.39795029163360596\n",
      "cnt: 0 - valLoss: 0.4257151484489441 - trainLoss: 0.3979492485523224\n",
      "cnt: 1 - valLoss: 0.42571473121643066 - trainLoss: 0.3979482352733612\n",
      "cnt: 0 - valLoss: 0.42571499943733215 - trainLoss: 0.39794719219207764\n",
      "cnt: 1 - valLoss: 0.42571455240249634 - trainLoss: 0.39794617891311646\n",
      "cnt: 0 - valLoss: 0.4257142245769501 - trainLoss: 0.3979451060295105\n",
      "cnt: 0 - valLoss: 0.425714373588562 - trainLoss: 0.3979440927505493\n",
      "cnt: 1 - valLoss: 0.42571404576301575 - trainLoss: 0.39794304966926575\n",
      "cnt: 0 - valLoss: 0.4257136285305023 - trainLoss: 0.39794203639030457\n",
      "cnt: 0 - valLoss: 0.4257138669490814 - trainLoss: 0.39794105291366577\n",
      "cnt: 1 - valLoss: 0.425713449716568 - trainLoss: 0.3979399800300598\n",
      "cnt: 0 - valLoss: 0.42571374773979187 - trainLoss: 0.39793893694877625\n",
      "cnt: 1 - valLoss: 0.42571330070495605 - trainLoss: 0.39793792366981506\n",
      "cnt: 0 - valLoss: 0.4257129430770874 - trainLoss: 0.3979368805885315\n",
      "cnt: 0 - valLoss: 0.4257131814956665 - trainLoss: 0.3979358971118927\n",
      "cnt: 1 - valLoss: 0.42571279406547546 - trainLoss: 0.3979348838329315\n",
      "cnt: 0 - valLoss: 0.4257124066352844 - trainLoss: 0.39793381094932556\n",
      "cnt: 0 - valLoss: 0.4257126748561859 - trainLoss: 0.39793282747268677\n",
      "cnt: 1 - valLoss: 0.4257122874259949 - trainLoss: 0.3979317545890808\n",
      "cnt: 0 - valLoss: 0.42571255564689636 - trainLoss: 0.397930771112442\n",
      "cnt: 1 - valLoss: 0.4257121682167053 - trainLoss: 0.39792975783348083\n",
      "cnt: 0 - valLoss: 0.4257117211818695 - trainLoss: 0.3979286849498749\n",
      "cnt: 0 - valLoss: 0.425711989402771 - trainLoss: 0.3979277014732361\n",
      "cnt: 1 - valLoss: 0.42571163177490234 - trainLoss: 0.3979266583919525\n",
      "cnt: 0 - valLoss: 0.4257112443447113 - trainLoss: 0.39792558550834656\n",
      "cnt: 0 - valLoss: 0.4257114827632904 - trainLoss: 0.39792460203170776\n",
      "cnt: 1 - valLoss: 0.42571109533309937 - trainLoss: 0.3979235887527466\n",
      "cnt: 0 - valLoss: 0.42571139335632324 - trainLoss: 0.397922545671463\n",
      "cnt: 1 - valLoss: 0.4257110059261322 - trainLoss: 0.39792153239250183\n",
      "cnt: 0 - valLoss: 0.42571061849594116 - trainLoss: 0.39792048931121826\n",
      "cnt: 0 - valLoss: 0.42571085691452026 - trainLoss: 0.39791950583457947\n",
      "cnt: 1 - valLoss: 0.4257104992866516 - trainLoss: 0.3979184329509735\n",
      "cnt: 0 - valLoss: 0.42571011185646057 - trainLoss: 0.39791741967201233\n",
      "cnt: 0 - valLoss: 0.4257103502750397 - trainLoss: 0.39791637659072876\n",
      "cnt: 1 - valLoss: 0.4257100224494934 - trainLoss: 0.3979153633117676\n",
      "cnt: 0 - valLoss: 0.4257102310657501 - trainLoss: 0.3979143798351288\n",
      "cnt: 1 - valLoss: 0.4257098436355591 - trainLoss: 0.3979133367538452\n",
      "cnt: 0 - valLoss: 0.42570948600769043 - trainLoss: 0.39791232347488403\n",
      "cnt: 0 - valLoss: 0.4257097542285919 - trainLoss: 0.3979112505912781\n",
      "cnt: 1 - valLoss: 0.4257093369960785 - trainLoss: 0.3979102671146393\n",
      "cnt: 0 - valLoss: 0.42570897936820984 - trainLoss: 0.3979092538356781\n",
      "cnt: 0 - valLoss: 0.42570924758911133 - trainLoss: 0.39790818095207214\n",
      "cnt: 1 - valLoss: 0.4257088601589203 - trainLoss: 0.39790719747543335\n",
      "cnt: 0 - valLoss: 0.42570847272872925 - trainLoss: 0.3979061245918274\n",
      "cnt: 0 - valLoss: 0.4257087707519531 - trainLoss: 0.3979051411151886\n",
      "cnt: 1 - valLoss: 0.4257083833217621 - trainLoss: 0.39790409803390503\n",
      "cnt: 0 - valLoss: 0.4257086515426636 - trainLoss: 0.39790308475494385\n",
      "cnt: 1 - valLoss: 0.42570826411247253 - trainLoss: 0.3979020416736603\n",
      "cnt: 0 - valLoss: 0.4257078766822815 - trainLoss: 0.3979010283946991\n",
      "cnt: 0 - valLoss: 0.4257081151008606 - trainLoss: 0.3978999853134155\n",
      "cnt: 1 - valLoss: 0.42570778727531433 - trainLoss: 0.39789897203445435\n",
      "cnt: 0 - valLoss: 0.4257073998451233 - trainLoss: 0.3978979289531708\n",
      "cnt: 0 - valLoss: 0.4257076680660248 - trainLoss: 0.3978969156742096\n",
      "cnt: 1 - valLoss: 0.42570728063583374 - trainLoss: 0.3978959023952484\n",
      "cnt: 0 - valLoss: 0.4257069230079651 - trainLoss: 0.39789485931396484\n",
      "cnt: 0 - valLoss: 0.4257071912288666 - trainLoss: 0.39789384603500366\n",
      "cnt: 1 - valLoss: 0.42570680379867554 - trainLoss: 0.3978928029537201\n",
      "cnt: 0 - valLoss: 0.4257071018218994 - trainLoss: 0.3978917896747589\n",
      "cnt: 1 - valLoss: 0.4257067143917084 - trainLoss: 0.39789074659347534\n",
      "cnt: 0 - valLoss: 0.42570632696151733 - trainLoss: 0.39788973331451416\n",
      "cnt: 0 - valLoss: 0.42570674419403076 - trainLoss: 0.3978886902332306\n",
      "cnt: 1 - valLoss: 0.42570653557777405 - trainLoss: 0.3978876769542694\n",
      "cnt: 2 - valLoss: 0.42570632696151733 - trainLoss: 0.3978866934776306\n",
      "cnt: 0 - valLoss: 0.42570605874061584 - trainLoss: 0.39788565039634705\n",
      "cnt: 0 - valLoss: 0.42570650577545166 - trainLoss: 0.39788463711738586\n",
      "cnt: 1 - valLoss: 0.42570629715919495 - trainLoss: 0.3978835940361023\n",
      "cnt: 2 - valLoss: 0.42570602893829346 - trainLoss: 0.3978826105594635\n",
      "cnt: 0 - valLoss: 0.42570582032203674 - trainLoss: 0.3978815972805023\n",
      "cnt: 0 - valLoss: 0.425706148147583 - trainLoss: 0.39788055419921875\n",
      "cnt: 1 - valLoss: 0.42570599913597107 - trainLoss: 0.39787954092025757\n",
      "cnt: 2 - valLoss: 0.4257057309150696 - trainLoss: 0.3978785574436188\n",
      "cnt: 0 - valLoss: 0.42570552229881287 - trainLoss: 0.3978775143623352\n",
      "cnt: 0 - valLoss: 0.42570531368255615 - trainLoss: 0.397876501083374\n",
      "cnt: 0 - valLoss: 0.4257057309150696 - trainLoss: 0.39787545800209045\n",
      "cnt: 1 - valLoss: 0.4257054626941681 - trainLoss: 0.39787447452545166\n",
      "cnt: 2 - valLoss: 0.42570531368255615 - trainLoss: 0.3978734612464905\n",
      "cnt: 0 - valLoss: 0.42570504546165466 - trainLoss: 0.3978724181652069\n",
      "cnt: 0 - valLoss: 0.4257054626941681 - trainLoss: 0.3978714048862457\n",
      "cnt: 1 - valLoss: 0.425705224275589 - trainLoss: 0.39787042140960693\n",
      "cnt: 2 - valLoss: 0.42570504546165466 - trainLoss: 0.397869348526001\n",
      "cnt: 0 - valLoss: 0.4257047772407532 - trainLoss: 0.3978683650493622\n",
      "cnt: 0 - valLoss: 0.425705224275589 - trainLoss: 0.3978673815727234\n",
      "cnt: 1 - valLoss: 0.42570504546165466 - trainLoss: 0.3978663682937622\n",
      "cnt: 2 - valLoss: 0.4257047772407532 - trainLoss: 0.39786529541015625\n",
      "cnt: 0 - valLoss: 0.4257045388221741 - trainLoss: 0.39786431193351746\n",
      "cnt: 0 - valLoss: 0.4257049858570099 - trainLoss: 0.3978632688522339\n",
      "cnt: 1 - valLoss: 0.4257047474384308 - trainLoss: 0.3978622555732727\n",
      "cnt: 2 - valLoss: 0.4257045388221741 - trainLoss: 0.3978612720966339\n",
      "cnt: 0 - valLoss: 0.42570430040359497 - trainLoss: 0.39786022901535034\n",
      "cnt: 0 - valLoss: 0.42570409178733826 - trainLoss: 0.39785921573638916\n",
      "cnt: 0 - valLoss: 0.4257045388221741 - trainLoss: 0.39785823225975037\n",
      "cnt: 1 - valLoss: 0.4257042706012726 - trainLoss: 0.3978571891784668\n",
      "cnt: 2 - valLoss: 0.42570406198501587 - trainLoss: 0.3978561758995056\n",
      "cnt: 0 - valLoss: 0.42570385336875916 - trainLoss: 0.39785513281822205\n",
      "cnt: 0 - valLoss: 0.4257042706012726 - trainLoss: 0.39785411953926086\n",
      "cnt: 1 - valLoss: 0.42570406198501587 - trainLoss: 0.39785313606262207\n",
      "cnt: 2 - valLoss: 0.42570385336875916 - trainLoss: 0.3978520631790161\n",
      "cnt: 0 - valLoss: 0.42570367455482483 - trainLoss: 0.3978510797023773\n",
      "cnt: 0 - valLoss: 0.42570406198501587 - trainLoss: 0.3978500962257385\n",
      "cnt: 1 - valLoss: 0.42570385336875916 - trainLoss: 0.39784905314445496\n",
      "cnt: 2 - valLoss: 0.42570361495018005 - trainLoss: 0.3978480398654938\n",
      "cnt: 0 - valLoss: 0.42570340633392334 - trainLoss: 0.3978469967842102\n",
      "cnt: 0 - valLoss: 0.4257031977176666 - trainLoss: 0.397845983505249\n",
      "cnt: 0 - valLoss: 0.42570358514785767 - trainLoss: 0.39784494042396545\n",
      "cnt: 1 - valLoss: 0.42570340633392334 - trainLoss: 0.39784395694732666\n",
      "cnt: 2 - valLoss: 0.42570316791534424 - trainLoss: 0.3978429436683655\n",
      "cnt: 0 - valLoss: 0.4257029891014099 - trainLoss: 0.3978419005870819\n",
      "cnt: 0 - valLoss: 0.42570340633392334 - trainLoss: 0.3978408873081207\n",
      "cnt: 1 - valLoss: 0.42570316791534424 - trainLoss: 0.39783990383148193\n",
      "cnt: 2 - valLoss: 0.4257029891014099 - trainLoss: 0.39783889055252075\n",
      "cnt: 0 - valLoss: 0.4257027804851532 - trainLoss: 0.3978378474712372\n",
      "cnt: 0 - valLoss: 0.4257025420665741 - trainLoss: 0.397836834192276\n",
      "cnt: 0 - valLoss: 0.4257029891014099 - trainLoss: 0.3978358507156372\n",
      "cnt: 1 - valLoss: 0.4257027208805084 - trainLoss: 0.39783480763435364\n",
      "cnt: 2 - valLoss: 0.4257025420665741 - trainLoss: 0.39783379435539246\n",
      "cnt: 0 - valLoss: 0.425702303647995 - trainLoss: 0.39783281087875366\n",
      "cnt: 0 - valLoss: 0.4257028102874756 - trainLoss: 0.3978317677974701\n",
      "cnt: 1 - valLoss: 0.4257025420665741 - trainLoss: 0.3978307545185089\n",
      "cnt: 2 - valLoss: 0.4257023334503174 - trainLoss: 0.39782971143722534\n",
      "cnt: 3 - valLoss: 0.42570212483406067 - trainLoss: 0.39782872796058655\n",
      "cnt: 0 - valLoss: 0.42570191621780396 - trainLoss: 0.39782774448394775\n",
      "cnt: 0 - valLoss: 0.42570239305496216 - trainLoss: 0.3978267312049866\n",
      "cnt: 1 - valLoss: 0.42570221424102783 - trainLoss: 0.397825688123703\n",
      "cnt: 2 - valLoss: 0.42570194602012634 - trainLoss: 0.3978246748447418\n",
      "cnt: 3 - valLoss: 0.42570191621780396 - trainLoss: 0.39782363176345825\n",
      "cnt: 0 - valLoss: 0.4257018566131592 - trainLoss: 0.39782261848449707\n",
      "cnt: 0 - valLoss: 0.4257024824619293 - trainLoss: 0.39782166481018066\n",
      "cnt: 1 - valLoss: 0.42570239305496216 - trainLoss: 0.3978206515312195\n",
      "cnt: 2 - valLoss: 0.4257013201713562 - trainLoss: 0.3978196084499359\n",
      "cnt: 0 - valLoss: 0.42570093274116516 - trainLoss: 0.39781853556632996\n",
      "cnt: 0 - valLoss: 0.4257010221481323 - trainLoss: 0.397817462682724\n",
      "cnt: 1 - valLoss: 0.4257003664970398 - trainLoss: 0.3978164494037628\n",
      "cnt: 0 - valLoss: 0.42570042610168457 - trainLoss: 0.39781540632247925\n",
      "cnt: 1 - valLoss: 0.42569980025291443 - trainLoss: 0.39781439304351807\n",
      "cnt: 0 - valLoss: 0.42570003867149353 - trainLoss: 0.3978133797645569\n",
      "cnt: 1 - valLoss: 0.4256995618343353 - trainLoss: 0.3978123366832733\n",
      "cnt: 0 - valLoss: 0.4256998896598816 - trainLoss: 0.39781132340431213\n",
      "cnt: 1 - valLoss: 0.4256995618343353 - trainLoss: 0.39781028032302856\n",
      "cnt: 0 - valLoss: 0.4256992042064667 - trainLoss: 0.39780929684638977\n",
      "cnt: 0 - valLoss: 0.4256995916366577 - trainLoss: 0.3978082239627838\n",
      "cnt: 1 - valLoss: 0.42569929361343384 - trainLoss: 0.397807240486145\n",
      "cnt: 2 - valLoss: 0.4256989657878876 - trainLoss: 0.39780622720718384\n",
      "cnt: 0 - valLoss: 0.4256986677646637 - trainLoss: 0.39780518412590027\n",
      "cnt: 0 - valLoss: 0.4256989657878876 - trainLoss: 0.3978042006492615\n",
      "cnt: 1 - valLoss: 0.4256986677646637 - trainLoss: 0.3978031873703003\n",
      "cnt: 0 - valLoss: 0.4256983697414398 - trainLoss: 0.3978021442890167\n",
      "cnt: 0 - valLoss: 0.4256986975669861 - trainLoss: 0.39780113101005554\n",
      "cnt: 1 - valLoss: 0.4256983697414398 - trainLoss: 0.39780014753341675\n",
      "cnt: 0 - valLoss: 0.42569804191589355 - trainLoss: 0.3977991044521332\n",
      "cnt: 0 - valLoss: 0.4256984293460846 - trainLoss: 0.397798091173172\n",
      "cnt: 1 - valLoss: 0.42569804191589355 - trainLoss: 0.3977970480918884\n",
      "cnt: 0 - valLoss: 0.4256977438926697 - trainLoss: 0.39779606461524963\n",
      "cnt: 0 - valLoss: 0.42569810152053833 - trainLoss: 0.39779505133628845\n",
      "cnt: 1 - valLoss: 0.42569780349731445 - trainLoss: 0.3977940082550049\n",
      "cnt: 2 - valLoss: 0.4256975054740906 - trainLoss: 0.3977929949760437\n",
      "cnt: 0 - valLoss: 0.4256971478462219 - trainLoss: 0.39779195189476013\n",
      "cnt: 0 - valLoss: 0.4256975054740906 - trainLoss: 0.39779093861579895\n",
      "cnt: 1 - valLoss: 0.4256972074508667 - trainLoss: 0.39778998494148254\n",
      "cnt: 2 - valLoss: 0.4256969094276428 - trainLoss: 0.39778897166252136\n",
      "cnt: 0 - valLoss: 0.4256972372531891 - trainLoss: 0.3977878987789154\n",
      "cnt: 1 - valLoss: 0.4256969094276428 - trainLoss: 0.3977869153022766\n",
      "cnt: 0 - valLoss: 0.42569661140441895 - trainLoss: 0.39778590202331543\n",
      "cnt: 0 - valLoss: 0.4256969392299652 - trainLoss: 0.39778485894203186\n",
      "cnt: 1 - valLoss: 0.42569664120674133 - trainLoss: 0.3977838456630707\n",
      "cnt: 2 - valLoss: 0.4256962835788727 - trainLoss: 0.3977828025817871\n",
      "cnt: 0 - valLoss: 0.4256966710090637 - trainLoss: 0.3977817893028259\n",
      "cnt: 1 - valLoss: 0.42569637298583984 - trainLoss: 0.3977808356285095\n",
      "cnt: 2 - valLoss: 0.42569607496261597 - trainLoss: 0.39777976274490356\n",
      "cnt: 0 - valLoss: 0.4256957769393921 - trainLoss: 0.39777877926826477\n",
      "cnt: 0 - valLoss: 0.42569613456726074 - trainLoss: 0.3977777659893036\n",
      "cnt: 1 - valLoss: 0.4256957769393921 - trainLoss: 0.39777672290802\n",
      "cnt: 0 - valLoss: 0.425695538520813 - trainLoss: 0.39777570962905884\n",
      "cnt: 0 - valLoss: 0.42569586634635925 - trainLoss: 0.39777466654777527\n",
      "cnt: 1 - valLoss: 0.425695538520813 - trainLoss: 0.39777374267578125\n",
      "cnt: 0 - valLoss: 0.4256952702999115 - trainLoss: 0.3977726697921753\n",
      "cnt: 0 - valLoss: 0.4256955683231354 - trainLoss: 0.3977716863155365\n",
      "cnt: 1 - valLoss: 0.4256952702999115 - trainLoss: 0.39777064323425293\n",
      "cnt: 0 - valLoss: 0.42569500207901 - trainLoss: 0.39776962995529175\n",
      "cnt: 0 - valLoss: 0.42569467425346375 - trainLoss: 0.3977685868740082\n",
      "cnt: 0 - valLoss: 0.42569500207901 - trainLoss: 0.3977676033973694\n",
      "cnt: 1 - valLoss: 0.4256947636604309 - trainLoss: 0.3977665901184082\n",
      "cnt: 2 - valLoss: 0.42569440603256226 - trainLoss: 0.3977656066417694\n",
      "cnt: 0 - valLoss: 0.4256947636604309 - trainLoss: 0.39776456356048584\n",
      "cnt: 1 - valLoss: 0.4256944954395294 - trainLoss: 0.39776355028152466\n",
      "cnt: 2 - valLoss: 0.42569419741630554 - trainLoss: 0.3977625072002411\n",
      "cnt: 0 - valLoss: 0.42569389939308167 - trainLoss: 0.3977614939212799\n",
      "cnt: 0 - valLoss: 0.42569422721862793 - trainLoss: 0.39776045083999634\n",
      "cnt: 1 - valLoss: 0.42569398880004883 - trainLoss: 0.3977595269680023\n",
      "cnt: 2 - valLoss: 0.4256936311721802 - trainLoss: 0.39775845408439636\n",
      "cnt: 0 - valLoss: 0.4256940186023712 - trainLoss: 0.39775747060775757\n",
      "cnt: 1 - valLoss: 0.42569372057914734 - trainLoss: 0.397756427526474\n",
      "cnt: 2 - valLoss: 0.42569348216056824 - trainLoss: 0.3977554142475128\n",
      "cnt: 0 - valLoss: 0.4256931245326996 - trainLoss: 0.39775437116622925\n",
      "cnt: 0 - valLoss: 0.425693541765213 - trainLoss: 0.39775338768959045\n",
      "cnt: 1 - valLoss: 0.42569321393966675 - trainLoss: 0.3977523744106293\n",
      "cnt: 2 - valLoss: 0.42569294571876526 - trainLoss: 0.3977513909339905\n",
      "cnt: 0 - valLoss: 0.4256933033466339 - trainLoss: 0.3977503478527069\n",
      "cnt: 1 - valLoss: 0.4256930351257324 - trainLoss: 0.3977493345737457\n",
      "cnt: 2 - valLoss: 0.42569273710250854 - trainLoss: 0.39774832129478455\n",
      "cnt: 0 - valLoss: 0.42569243907928467 - trainLoss: 0.397747278213501\n",
      "cnt: 0 - valLoss: 0.4256928265094757 - trainLoss: 0.39774632453918457\n",
      "cnt: 1 - valLoss: 0.42569252848625183 - trainLoss: 0.3977453112602234\n",
      "cnt: 2 - valLoss: 0.42569223046302795 - trainLoss: 0.39774423837661743\n",
      "cnt: 0 - valLoss: 0.425692617893219 - trainLoss: 0.39774325489997864\n",
      "cnt: 1 - valLoss: 0.4256923198699951 - trainLoss: 0.39774224162101746\n",
      "cnt: 2 - valLoss: 0.42569202184677124 - trainLoss: 0.3977411985397339\n",
      "cnt: 0 - valLoss: 0.42569175362586975 - trainLoss: 0.3977402150630951\n",
      "cnt: 0 - valLoss: 0.4256921112537384 - trainLoss: 0.3977392017841339\n",
      "cnt: 1 - valLoss: 0.4256918430328369 - trainLoss: 0.3977382183074951\n",
      "cnt: 2 - valLoss: 0.42569154500961304 - trainLoss: 0.39773717522621155\n",
      "cnt: 0 - valLoss: 0.4256919324398041 - trainLoss: 0.39773616194725037\n",
      "cnt: 1 - valLoss: 0.4256916642189026 - trainLoss: 0.3977351188659668\n",
      "cnt: 2 - valLoss: 0.4256913363933563 - trainLoss: 0.397734135389328\n",
      "cnt: 0 - valLoss: 0.42569106817245483 - trainLoss: 0.3977331221103668\n",
      "cnt: 0 - valLoss: 0.4256914258003235 - trainLoss: 0.397732138633728\n",
      "cnt: 1 - valLoss: 0.425691157579422 - trainLoss: 0.39773109555244446\n",
      "cnt: 2 - valLoss: 0.42569079995155334 - trainLoss: 0.39773011207580566\n",
      "cnt: 0 - valLoss: 0.42569056153297424 - trainLoss: 0.3977290391921997\n",
      "cnt: 0 - valLoss: 0.42569097876548767 - trainLoss: 0.3977280557155609\n",
      "cnt: 1 - valLoss: 0.4256906807422638 - trainLoss: 0.39772704243659973\n",
      "cnt: 2 - valLoss: 0.4256903827190399 - trainLoss: 0.39772605895996094\n",
      "cnt: 0 - valLoss: 0.42569074034690857 - trainLoss: 0.39772501587867737\n",
      "cnt: 1 - valLoss: 0.4256904721260071 - trainLoss: 0.3977240025997162\n",
      "cnt: 2 - valLoss: 0.4256902039051056 - trainLoss: 0.3977230191230774\n",
      "cnt: 0 - valLoss: 0.4256898760795593 - trainLoss: 0.39772194623947144\n",
      "cnt: 0 - valLoss: 0.42569029331207275 - trainLoss: 0.39772096276283264\n",
      "cnt: 1 - valLoss: 0.42569005489349365 - trainLoss: 0.39771997928619385\n",
      "cnt: 2 - valLoss: 0.4256897270679474 - trainLoss: 0.3977189362049103\n",
      "cnt: 0 - valLoss: 0.4256894290447235 - trainLoss: 0.3977179229259491\n",
      "cnt: 0 - valLoss: 0.42568981647491455 - trainLoss: 0.3977169394493103\n",
      "cnt: 1 - valLoss: 0.42568957805633545 - trainLoss: 0.3977159559726715\n",
      "cnt: 2 - valLoss: 0.4256892800331116 - trainLoss: 0.39771491289138794\n",
      "cnt: 0 - valLoss: 0.4256896674633026 - trainLoss: 0.39771389961242676\n",
      "cnt: 1 - valLoss: 0.42568933963775635 - trainLoss: 0.3977128565311432\n",
      "cnt: 2 - valLoss: 0.42568910121917725 - trainLoss: 0.397711843252182\n",
      "cnt: 0 - valLoss: 0.42568880319595337 - trainLoss: 0.3977108597755432\n",
      "cnt: 0 - valLoss: 0.4256892204284668 - trainLoss: 0.39770981669425964\n",
      "cnt: 1 - valLoss: 0.4256889224052429 - trainLoss: 0.39770883321762085\n",
      "cnt: 2 - valLoss: 0.42568865418434143 - trainLoss: 0.39770781993865967\n",
      "cnt: 0 - valLoss: 0.42568835616111755 - trainLoss: 0.3977067768573761\n",
      "cnt: 0 - valLoss: 0.4256887435913086 - trainLoss: 0.3977057933807373\n",
      "cnt: 1 - valLoss: 0.4256885051727295 - trainLoss: 0.3977048099040985\n",
      "cnt: 2 - valLoss: 0.4256882071495056 - trainLoss: 0.39770379662513733\n",
      "cnt: 0 - valLoss: 0.42568859457969666 - trainLoss: 0.39770275354385376\n",
      "cnt: 1 - valLoss: 0.42568832635879517 - trainLoss: 0.3977017402648926\n",
      "cnt: 2 - valLoss: 0.4256880283355713 - trainLoss: 0.3977007567882538\n",
      "cnt: 0 - valLoss: 0.4256877303123474 - trainLoss: 0.3976996839046478\n",
      "cnt: 0 - valLoss: 0.42568814754486084 - trainLoss: 0.3976987302303314\n",
      "cnt: 1 - valLoss: 0.42568784952163696 - trainLoss: 0.39769771695137024\n",
      "cnt: 2 - valLoss: 0.4256875514984131 - trainLoss: 0.39769670367240906\n",
      "cnt: 0 - valLoss: 0.42568734288215637 - trainLoss: 0.3976956903934479\n",
      "cnt: 0 - valLoss: 0.4256877303123474 - trainLoss: 0.3976946473121643\n",
      "cnt: 1 - valLoss: 0.4256874620914459 - trainLoss: 0.3976936936378479\n",
      "cnt: 2 - valLoss: 0.4256872236728668 - trainLoss: 0.3976926803588867\n",
      "cnt: 0 - valLoss: 0.42568686604499817 - trainLoss: 0.39769163727760315\n",
      "cnt: 0 - valLoss: 0.4256872534751892 - trainLoss: 0.39769062399864197\n",
      "cnt: 1 - valLoss: 0.4256869852542877 - trainLoss: 0.3976896405220032\n",
      "cnt: 2 - valLoss: 0.42568668723106384 - trainLoss: 0.3976885974407196\n",
      "cnt: 0 - valLoss: 0.42568713426589966 - trainLoss: 0.3976876139640808\n",
      "cnt: 1 - valLoss: 0.42568686604499817 - trainLoss: 0.39768660068511963\n",
      "cnt: 2 - valLoss: 0.4256865680217743 - trainLoss: 0.39768555760383606\n",
      "cnt: 0 - valLoss: 0.4256862699985504 - trainLoss: 0.3976845443248749\n",
      "cnt: 0 - valLoss: 0.42568668723106384 - trainLoss: 0.3976835608482361\n",
      "cnt: 1 - valLoss: 0.42568644881248474 - trainLoss: 0.3976825773715973\n",
      "cnt: 2 - valLoss: 0.42568615078926086 - trainLoss: 0.3976815342903137\n",
      "cnt: 0 - valLoss: 0.4256858825683594 - trainLoss: 0.39768052101135254\n",
      "cnt: 0 - valLoss: 0.4256862699985504 - trainLoss: 0.39767947793006897\n",
      "cnt: 1 - valLoss: 0.4256860017776489 - trainLoss: 0.3976784944534302\n",
      "cnt: 2 - valLoss: 0.4256857633590698 - trainLoss: 0.397677481174469\n",
      "cnt: 0 - valLoss: 0.42568546533584595 - trainLoss: 0.3976764976978302\n",
      "cnt: 0 - valLoss: 0.425685852766037 - trainLoss: 0.39767545461654663\n",
      "cnt: 1 - valLoss: 0.4256856143474579 - trainLoss: 0.39767444133758545\n",
      "cnt: 2 - valLoss: 0.425685316324234 - trainLoss: 0.3976733982563019\n",
      "cnt: 0 - valLoss: 0.4256850779056549 - trainLoss: 0.39767247438430786\n",
      "cnt: 0 - valLoss: 0.42568546533584595 - trainLoss: 0.3976714313030243\n",
      "cnt: 1 - valLoss: 0.42568522691726685 - trainLoss: 0.3976704180240631\n",
      "cnt: 2 - valLoss: 0.42568498849868774 - trainLoss: 0.39766937494277954\n",
      "cnt: 0 - valLoss: 0.42568472027778625 - trainLoss: 0.39766839146614075\n",
      "cnt: 0 - valLoss: 0.4256850779056549 - trainLoss: 0.39766740798950195\n",
      "cnt: 1 - valLoss: 0.4256848096847534 - trainLoss: 0.39766639471054077\n",
      "cnt: 2 - valLoss: 0.42568454146385193 - trainLoss: 0.3976653516292572\n",
      "cnt: 0 - valLoss: 0.4256843030452728 - trainLoss: 0.397664338350296\n",
      "cnt: 0 - valLoss: 0.42568469047546387 - trainLoss: 0.3976633548736572\n",
      "cnt: 1 - valLoss: 0.42568448185920715 - trainLoss: 0.39766237139701843\n",
      "cnt: 2 - valLoss: 0.42568421363830566 - trainLoss: 0.39766132831573486\n",
      "cnt: 0 - valLoss: 0.4256846010684967 - trainLoss: 0.39766034483909607\n",
      "cnt: 1 - valLoss: 0.4256843328475952 - trainLoss: 0.3976593613624573\n",
      "cnt: 2 - valLoss: 0.42568403482437134 - trainLoss: 0.3976582884788513\n",
      "cnt: 0 - valLoss: 0.4256838262081146 - trainLoss: 0.3976573050022125\n",
      "cnt: 0 - valLoss: 0.42568421363830566 - trainLoss: 0.39765629172325134\n",
      "cnt: 1 - valLoss: 0.4256839454174042 - trainLoss: 0.39765530824661255\n",
      "cnt: 2 - valLoss: 0.42568373680114746 - trainLoss: 0.397654265165329\n",
      "cnt: 0 - valLoss: 0.42568346858024597 - trainLoss: 0.3976532518863678\n",
      "cnt: 0 - valLoss: 0.425683856010437 - trainLoss: 0.39765220880508423\n",
      "cnt: 1 - valLoss: 0.4256836175918579 - trainLoss: 0.39765122532844543\n",
      "cnt: 2 - valLoss: 0.4256833493709564 - trainLoss: 0.39765024185180664\n",
      "cnt: 0 - valLoss: 0.4256831109523773 - trainLoss: 0.39764922857284546\n",
      "cnt: 0 - valLoss: 0.42568352818489075 - trainLoss: 0.3976481854915619\n",
      "cnt: 1 - valLoss: 0.42568325996398926 - trainLoss: 0.3976472020149231\n",
      "cnt: 2 - valLoss: 0.42568302154541016 - trainLoss: 0.3976462185382843\n",
      "cnt: 0 - valLoss: 0.42568275332450867 - trainLoss: 0.3976452052593231\n",
      "cnt: 0 - valLoss: 0.4256831705570221 - trainLoss: 0.39764416217803955\n",
      "cnt: 1 - valLoss: 0.425682932138443 - trainLoss: 0.39764317870140076\n",
      "cnt: 2 - valLoss: 0.4256826639175415 - trainLoss: 0.3976421654224396\n",
      "cnt: 0 - valLoss: 0.4256824254989624 - trainLoss: 0.3976411819458008\n",
      "cnt: 0 - valLoss: 0.4256821870803833 - trainLoss: 0.3976401090621948\n",
      "cnt: 0 - valLoss: 0.42568257451057434 - trainLoss: 0.3976391553878784\n",
      "cnt: 1 - valLoss: 0.42568233609199524 - trainLoss: 0.39763814210891724\n",
      "cnt: 2 - valLoss: 0.42568209767341614 - trainLoss: 0.39763709902763367\n",
      "cnt: 0 - valLoss: 0.42568185925483704 - trainLoss: 0.3976361155509949\n",
      "cnt: 0 - valLoss: 0.42568227648735046 - trainLoss: 0.3976351022720337\n",
      "cnt: 1 - valLoss: 0.42568203806877136 - trainLoss: 0.3976341187953949\n",
      "cnt: 2 - valLoss: 0.42568179965019226 - trainLoss: 0.39763307571411133\n",
      "cnt: 0 - valLoss: 0.42568156123161316 - trainLoss: 0.39763209223747253\n",
      "cnt: 0 - valLoss: 0.4256819784641266 - trainLoss: 0.39763107895851135\n",
      "cnt: 1 - valLoss: 0.4256817102432251 - trainLoss: 0.39763009548187256\n",
      "cnt: 2 - valLoss: 0.425681471824646 - trainLoss: 0.397629052400589\n",
      "cnt: 0 - valLoss: 0.4256812036037445 - trainLoss: 0.3976280689239502\n",
      "cnt: 0 - valLoss: 0.4256816506385803 - trainLoss: 0.397627055644989\n",
      "cnt: 1 - valLoss: 0.42568138241767883 - trainLoss: 0.3976260721683502\n",
      "cnt: 2 - valLoss: 0.4256811738014221 - trainLoss: 0.39762502908706665\n",
      "cnt: 0 - valLoss: 0.4256809651851654 - trainLoss: 0.39762401580810547\n",
      "cnt: 0 - valLoss: 0.42568138241767883 - trainLoss: 0.3976230323314667\n",
      "cnt: 1 - valLoss: 0.42568108439445496 - trainLoss: 0.3976219892501831\n",
      "cnt: 2 - valLoss: 0.42568087577819824 - trainLoss: 0.3976210057735443\n",
      "cnt: 0 - valLoss: 0.42568066716194153 - trainLoss: 0.39761999249458313\n",
      "cnt: 0 - valLoss: 0.42568039894104004 - trainLoss: 0.39761894941329956\n",
      "cnt: 0 - valLoss: 0.42568081617355347 - trainLoss: 0.39761796593666077\n",
      "cnt: 1 - valLoss: 0.42568060755729675 - trainLoss: 0.397616982460022\n",
      "cnt: 2 - valLoss: 0.42568033933639526 - trainLoss: 0.3976159691810608\n",
      "cnt: 0 - valLoss: 0.42568013072013855 - trainLoss: 0.397614985704422\n",
      "cnt: 0 - valLoss: 0.42568057775497437 - trainLoss: 0.3976140022277832\n",
      "cnt: 1 - valLoss: 0.4256803095340729 - trainLoss: 0.39761295914649963\n",
      "cnt: 2 - valLoss: 0.4256800413131714 - trainLoss: 0.39761194586753845\n",
      "cnt: 0 - valLoss: 0.4256798326969147 - trainLoss: 0.39761096239089966\n",
      "cnt: 0 - valLoss: 0.4256802797317505 - trainLoss: 0.3976099193096161\n",
      "cnt: 1 - valLoss: 0.4256800413131714 - trainLoss: 0.3976089358329773\n",
      "cnt: 2 - valLoss: 0.4256798028945923 - trainLoss: 0.3976079225540161\n",
      "cnt: 0 - valLoss: 0.42567959427833557 - trainLoss: 0.3976069688796997\n",
      "cnt: 0 - valLoss: 0.42567935585975647 - trainLoss: 0.39760589599609375\n",
      "cnt: 0 - valLoss: 0.4256797432899475 - trainLoss: 0.39760491251945496\n",
      "cnt: 1 - valLoss: 0.4256795346736908 - trainLoss: 0.3976038992404938\n",
      "cnt: 2 - valLoss: 0.4256793260574341 - trainLoss: 0.397602915763855\n",
      "cnt: 0 - valLoss: 0.4256790578365326 - trainLoss: 0.3976018726825714\n",
      "cnt: 0 - valLoss: 0.4256795048713684 - trainLoss: 0.3976008892059326\n",
      "cnt: 1 - valLoss: 0.4256792962551117 - trainLoss: 0.39759987592697144\n",
      "cnt: 2 - valLoss: 0.4256790280342102 - trainLoss: 0.39759889245033264\n",
      "cnt: 0 - valLoss: 0.4256788194179535 - trainLoss: 0.3975978493690491\n",
      "cnt: 0 - valLoss: 0.4256792366504669 - trainLoss: 0.3975968658924103\n",
      "cnt: 1 - valLoss: 0.4256790280342102 - trainLoss: 0.3975958526134491\n",
      "cnt: 2 - valLoss: 0.4256787598133087 - trainLoss: 0.3975948691368103\n",
      "cnt: 0 - valLoss: 0.425678551197052 - trainLoss: 0.39759382605552673\n",
      "cnt: 0 - valLoss: 0.4256782829761505 - trainLoss: 0.39759284257888794\n",
      "cnt: 0 - valLoss: 0.42567873001098633 - trainLoss: 0.39759182929992676\n",
      "cnt: 1 - valLoss: 0.4256785213947296 - trainLoss: 0.39759084582328796\n",
      "cnt: 2 - valLoss: 0.4256782829761505 - trainLoss: 0.39758986234664917\n",
      "cnt: 0 - valLoss: 0.4256780743598938 - trainLoss: 0.3975888192653656\n",
      "cnt: 0 - valLoss: 0.42567864060401917 - trainLoss: 0.3975878357887268\n",
      "cnt: 1 - valLoss: 0.42567843198776245 - trainLoss: 0.3975868225097656\n",
      "cnt: 2 - valLoss: 0.4256782829761505 - trainLoss: 0.39758583903312683\n",
      "cnt: 3 - valLoss: 0.42567822337150574 - trainLoss: 0.39758479595184326\n",
      "cnt: 4 - valLoss: 0.42567822337150574 - trainLoss: 0.3975837826728821\n",
      "cnt: 5 - valLoss: 0.4256780743598938 - trainLoss: 0.3975828289985657\n",
      "cnt: 0 - valLoss: 0.4256780445575714 - trainLoss: 0.3975818157196045\n",
      "cnt: 0 - valLoss: 0.4256778955459595 - trainLoss: 0.3975808322429657\n",
      "cnt: 0 - valLoss: 0.4256785213947296 - trainLoss: 0.3975798487663269\n",
      "cnt: 1 - valLoss: 0.42567846179008484 - trainLoss: 0.39757880568504333\n",
      "cnt: 2 - valLoss: 0.4256783425807953 - trainLoss: 0.39757782220840454\n",
      "cnt: 3 - valLoss: 0.4256782531738281 - trainLoss: 0.39757683873176575\n",
      "cnt: 4 - valLoss: 0.42567816376686096 - trainLoss: 0.39757582545280457\n",
      "cnt: 5 - valLoss: 0.4256781339645386 - trainLoss: 0.39757484197616577\n",
      "cnt: 6 - valLoss: 0.4256780445575714 - trainLoss: 0.397573858499527\n",
      "cnt: 7 - valLoss: 0.42567795515060425 - trainLoss: 0.3975728154182434\n",
      "cnt: 8 - valLoss: 0.4256778657436371 - trainLoss: 0.3975718319416046\n",
      "cnt: 0 - valLoss: 0.4256777763366699 - trainLoss: 0.3975708484649658\n",
      "cnt: 0 - valLoss: 0.4256783723831177 - trainLoss: 0.39756983518600464\n",
      "cnt: 1 - valLoss: 0.4256782829761505 - trainLoss: 0.39756885170936584\n",
      "cnt: 2 - valLoss: 0.42567822337150574 - trainLoss: 0.39756786823272705\n",
      "cnt: 3 - valLoss: 0.4256781339645386 - trainLoss: 0.3975668251514435\n",
      "cnt: 4 - valLoss: 0.4256780445575714 - trainLoss: 0.3975658416748047\n",
      "cnt: 5 - valLoss: 0.42567795515060425 - trainLoss: 0.3975648581981659\n",
      "cnt: 6 - valLoss: 0.4256778657436371 - trainLoss: 0.3975638449192047\n",
      "cnt: 7 - valLoss: 0.4256777763366699 - trainLoss: 0.3975628912448883\n",
      "cnt: 0 - valLoss: 0.42567768692970276 - trainLoss: 0.3975618779659271\n",
      "cnt: 0 - valLoss: 0.4256775975227356 - trainLoss: 0.39756083488464355\n",
      "cnt: 0 - valLoss: 0.42567822337150574 - trainLoss: 0.39755985140800476\n",
      "cnt: 1 - valLoss: 0.4256781339645386 - trainLoss: 0.39755886793136597\n",
      "cnt: 2 - valLoss: 0.42567798495292664 - trainLoss: 0.3975578844547272\n",
      "cnt: 3 - valLoss: 0.42567795515060425 - trainLoss: 0.3975568413734436\n",
      "cnt: 4 - valLoss: 0.42567795515060425 - trainLoss: 0.3975558578968048\n",
      "cnt: 5 - valLoss: 0.42567795515060425 - trainLoss: 0.39755484461784363\n",
      "cnt: 6 - valLoss: 0.42567795515060425 - trainLoss: 0.3975538909435272\n",
      "cnt: 7 - valLoss: 0.42567795515060425 - trainLoss: 0.3975529670715332\n",
      "cnt: 8 - valLoss: 0.42567795515060425 - trainLoss: 0.39755192399024963\n",
      "cnt: 9 - valLoss: 0.4256778955459595 - trainLoss: 0.3975510001182556\n",
      "cnt: 10 - valLoss: 0.42567795515060425 - trainLoss: 0.3975500166416168\n",
      "cnt: 11 - valLoss: 0.42567795515060425 - trainLoss: 0.397549033164978\n",
      "cnt: 12 - valLoss: 0.42567795515060425 - trainLoss: 0.39754804968833923\n",
      "cnt: 13 - valLoss: 0.42567795515060425 - trainLoss: 0.39754706621170044\n",
      "cnt: 14 - valLoss: 0.4256778955459595 - trainLoss: 0.39754608273506165\n",
      "cnt: 15 - valLoss: 0.4256778955459595 - trainLoss: 0.39754509925842285\n",
      "cnt: 16 - valLoss: 0.42567795515060425 - trainLoss: 0.39754414558410645\n",
      "cnt: 17 - valLoss: 0.4256778955459595 - trainLoss: 0.3975431025028229\n",
      "cnt: 18 - valLoss: 0.42567795515060425 - trainLoss: 0.39754217863082886\n",
      "cnt: 19 - valLoss: 0.42567795515060425 - trainLoss: 0.39754119515419006\n",
      "cnt: 20 - valLoss: 0.42567795515060425 - trainLoss: 0.39754021167755127\n",
      "cnt: 21 - valLoss: 0.4256778955459595 - trainLoss: 0.39753925800323486\n",
      "cnt: 22 - valLoss: 0.4256778955459595 - trainLoss: 0.3975382447242737\n",
      "cnt: 23 - valLoss: 0.4256778955459595 - trainLoss: 0.3975372910499573\n",
      "cnt: 24 - valLoss: 0.4256778955459595 - trainLoss: 0.3975363075733185\n",
      "cnt: 25 - valLoss: 0.4256778657436371 - trainLoss: 0.3975352942943573\n",
      "cnt: 26 - valLoss: 0.4256778955459595 - trainLoss: 0.3975343406200409\n",
      "cnt: 27 - valLoss: 0.42567795515060425 - trainLoss: 0.3975333571434021\n",
      "cnt: 28 - valLoss: 0.4256778955459595 - trainLoss: 0.3975324034690857\n",
      "cnt: 29 - valLoss: 0.4256778955459595 - trainLoss: 0.3975313901901245\n",
      "cnt: 30 - valLoss: 0.4256778955459595 - trainLoss: 0.3975304365158081\n",
      "cnt: 31 - valLoss: 0.42567795515060425 - trainLoss: 0.3975294530391693\n",
      "cnt: 32 - valLoss: 0.4256778955459595 - trainLoss: 0.3975284695625305\n",
      "cnt: 33 - valLoss: 0.4256778955459595 - trainLoss: 0.3975274860858917\n",
      "cnt: 34 - valLoss: 0.425678551197052 - trainLoss: 0.39752650260925293\n",
      "cnt: 35 - valLoss: 0.425678551197052 - trainLoss: 0.39752551913261414\n",
      "cnt: 36 - valLoss: 0.425678551197052 - trainLoss: 0.39752453565597534\n",
      "cnt: 37 - valLoss: 0.4256785213947296 - trainLoss: 0.3975236117839813\n",
      "cnt: 38 - valLoss: 0.4256785213947296 - trainLoss: 0.39752256870269775\n",
      "cnt: 39 - valLoss: 0.425678551197052 - trainLoss: 0.39752164483070374\n",
      "cnt: 40 - valLoss: 0.4256785213947296 - trainLoss: 0.39752063155174255\n",
      "cnt: 41 - valLoss: 0.42567846179008484 - trainLoss: 0.39751964807510376\n",
      "cnt: 42 - valLoss: 0.42567846179008484 - trainLoss: 0.39751872420310974\n",
      "cnt: 43 - valLoss: 0.42567846179008484 - trainLoss: 0.39751768112182617\n",
      "cnt: 44 - valLoss: 0.42567846179008484 - trainLoss: 0.39751675724983215\n",
      "cnt: 45 - valLoss: 0.42567846179008484 - trainLoss: 0.39751577377319336\n",
      "cnt: 46 - valLoss: 0.42567846179008484 - trainLoss: 0.39751479029655457\n",
      "cnt: 47 - valLoss: 0.42567843198776245 - trainLoss: 0.39751383662223816\n",
      "cnt: 48 - valLoss: 0.42567843198776245 - trainLoss: 0.397512823343277\n",
      "cnt: 49 - valLoss: 0.42567843198776245 - trainLoss: 0.39751186966896057\n",
      "cnt: 50 - valLoss: 0.4256783723831177 - trainLoss: 0.3975108861923218\n",
      "cnt: 51 - valLoss: 0.4256783723831177 - trainLoss: 0.397509902715683\n",
      "cnt: 52 - valLoss: 0.4256783723831177 - trainLoss: 0.3975089490413666\n",
      "cnt: 53 - valLoss: 0.4256783723831177 - trainLoss: 0.3975079655647278\n",
      "cnt: 54 - valLoss: 0.4256783723831177 - trainLoss: 0.397506982088089\n",
      "cnt: 55 - valLoss: 0.4256783723831177 - trainLoss: 0.3975059986114502\n",
      "cnt: 56 - valLoss: 0.4256783723831177 - trainLoss: 0.3975050151348114\n",
      "cnt: 57 - valLoss: 0.4256783425807953 - trainLoss: 0.3975040912628174\n",
      "cnt: 58 - valLoss: 0.4256783425807953 - trainLoss: 0.3975031077861786\n",
      "cnt: 59 - valLoss: 0.4256783723831177 - trainLoss: 0.3975021243095398\n",
      "cnt: 60 - valLoss: 0.4256783723831177 - trainLoss: 0.3975010812282562\n",
      "cnt: 61 - valLoss: 0.4256783425807953 - trainLoss: 0.3975001573562622\n",
      "cnt: 62 - valLoss: 0.4256782829761505 - trainLoss: 0.3974992036819458\n",
      "cnt: 63 - valLoss: 0.4256783425807953 - trainLoss: 0.397498220205307\n",
      "cnt: 64 - valLoss: 0.4256782829761505 - trainLoss: 0.39749717712402344\n",
      "cnt: 65 - valLoss: 0.4256782829761505 - trainLoss: 0.3974962532520294\n",
      "cnt: 66 - valLoss: 0.4256782829761505 - trainLoss: 0.3974952697753906\n",
      "cnt: 67 - valLoss: 0.4256782829761505 - trainLoss: 0.39749428629875183\n",
      "cnt: 68 - valLoss: 0.4256782829761505 - trainLoss: 0.3974933326244354\n",
      "cnt: 69 - valLoss: 0.4256782829761505 - trainLoss: 0.39749234914779663\n",
      "cnt: 70 - valLoss: 0.4256782829761505 - trainLoss: 0.39749136567115784\n",
      "cnt: 71 - valLoss: 0.4256782829761505 - trainLoss: 0.39749038219451904\n",
      "cnt: 72 - valLoss: 0.4256782829761505 - trainLoss: 0.39748939871788025\n",
      "cnt: 73 - valLoss: 0.4256782531738281 - trainLoss: 0.39748844504356384\n",
      "cnt: 74 - valLoss: 0.4256782829761505 - trainLoss: 0.39748746156692505\n",
      "cnt: 75 - valLoss: 0.4256782829761505 - trainLoss: 0.39748647809028625\n",
      "cnt: 76 - valLoss: 0.4256782531738281 - trainLoss: 0.39748549461364746\n",
      "cnt: 77 - valLoss: 0.4256782531738281 - trainLoss: 0.39748451113700867\n",
      "cnt: 78 - valLoss: 0.42567822337150574 - trainLoss: 0.39748358726501465\n",
      "cnt: 79 - valLoss: 0.42567822337150574 - trainLoss: 0.39748260378837585\n",
      "cnt: 80 - valLoss: 0.4256782531738281 - trainLoss: 0.39748165011405945\n",
      "cnt: 81 - valLoss: 0.42567822337150574 - trainLoss: 0.39748063683509827\n",
      "cnt: 82 - valLoss: 0.42567822337150574 - trainLoss: 0.39747968316078186\n",
      "cnt: 83 - valLoss: 0.42567822337150574 - trainLoss: 0.3974786698818207\n",
      "cnt: 84 - valLoss: 0.42567822337150574 - trainLoss: 0.3974777162075043\n",
      "cnt: 85 - valLoss: 0.42567822337150574 - trainLoss: 0.3974767327308655\n",
      "cnt: 86 - valLoss: 0.42567822337150574 - trainLoss: 0.3974757492542267\n",
      "cnt: 87 - valLoss: 0.42567822337150574 - trainLoss: 0.3974747955799103\n",
      "cnt: 88 - valLoss: 0.42567822337150574 - trainLoss: 0.3974738121032715\n",
      "cnt: 89 - valLoss: 0.42567816376686096 - trainLoss: 0.3974728286266327\n",
      "cnt: 90 - valLoss: 0.4256781339645386 - trainLoss: 0.39747190475463867\n",
      "cnt: 91 - valLoss: 0.42567816376686096 - trainLoss: 0.3974709212779999\n",
      "cnt: 92 - valLoss: 0.4256781339645386 - trainLoss: 0.3974699378013611\n",
      "cnt: 93 - valLoss: 0.4256781339645386 - trainLoss: 0.3974689841270447\n",
      "cnt: 94 - valLoss: 0.4256781339645386 - trainLoss: 0.3974679708480835\n",
      "cnt: 95 - valLoss: 0.4256781339645386 - trainLoss: 0.3974670171737671\n",
      "cnt: 96 - valLoss: 0.4256781339645386 - trainLoss: 0.3974660634994507\n",
      "cnt: 97 - valLoss: 0.4256781339645386 - trainLoss: 0.3974650502204895\n",
      "cnt: 98 - valLoss: 0.4256780743598938 - trainLoss: 0.3974640667438507\n",
      "cnt: 99 - valLoss: 0.4256780743598938 - trainLoss: 0.3974631130695343\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1481d6a90>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def trainAI(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    trainLoss=0\n",
    "    for X, y in dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        trainLoss +=loss.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    return trainLoss\n",
    "\n",
    "def valAI(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    valLoss =0\n",
    "    with torch.no_grad():\n",
    "        for X ,y  in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "            # for i in loss:\n",
    "            valLoss+= loss.item()\n",
    "    return valLoss\n",
    "\n",
    "trainLoss=[]\n",
    "valLoss=[]\n",
    "bestModel = model\n",
    "bestLoss = float('inf')\n",
    "cnt=0\n",
    "\n",
    "while(cnt<100):\n",
    "    trainLoss.append(trainAI(trainLoader, model, loss_fn, optimizer))\n",
    "    valLoss.append(valAI(valLoader, model, loss_fn))\n",
    "\n",
    "    print(f'cnt: {cnt} - valLoss: {valLoss[-1]} - trainLoss: {trainLoss[-1]}')\n",
    "    if bestLoss<valLoss[-1]:\n",
    "        cnt+=1\n",
    "    else:\n",
    "        cnt = 0\n",
    "        bestLoss = valLoss[-1]\n",
    "        bestModel = model\n",
    "\n",
    "plt.plot(trainLoss,label='trainLoss')\n",
    "plt.plot(valLoss,label='valLoss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>passengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>1305</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>1306</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>1307</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>1308</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>1309</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     passengerId  Survived\n",
       "0            892         0\n",
       "1            893         0\n",
       "2            894         0\n",
       "3            895         0\n",
       "4            896         0\n",
       "..           ...       ...\n",
       "413         1305         0\n",
       "414         1306         1\n",
       "415         1307         0\n",
       "416         1308         0\n",
       "417         1309         0\n",
       "\n",
       "[418 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def testAI(dataloader, model):\n",
    "    model.eval()\n",
    "    result = []\n",
    "    y=892\n",
    "    with torch.no_grad():\n",
    "        for X ,_  in dataloader:\n",
    "            X  = X.to(device)\n",
    "            pred = model(X)\n",
    "            for i in pred:\n",
    "                result.append([y,torch.argmax(i).item()])\n",
    "                y+=1\n",
    "    return result\n",
    "\n",
    "result = testAI(testLoader, bestModel)\n",
    "result = pd.DataFrame(result)\n",
    "result = result.astype(int)\n",
    "result.columns=['passengerId','Survived']\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('result.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
