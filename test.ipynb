{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Youseff, Mr. Gerious</td>\n",
       "      <td>male</td>\n",
       "      <td>45.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2628</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>176</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Klasen, Mr. Klas Albin</td>\n",
       "      <td>male</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>350404</td>\n",
       "      <td>7.8542</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>829</th>\n",
       "      <td>830</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Stone, Mrs. George Nelson (Martha Evelyn)</td>\n",
       "      <td>female</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>113572</td>\n",
       "      <td>80.0000</td>\n",
       "      <td>B28</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass                                       Name  \\\n",
       "203          204         0       3                       Youseff, Mr. Gerious   \n",
       "175          176         0       3                     Klasen, Mr. Klas Albin   \n",
       "829          830         1       1  Stone, Mrs. George Nelson (Martha Evelyn)   \n",
       "\n",
       "        Sex   Age  SibSp  Parch  Ticket     Fare Cabin Embarked  \n",
       "203    male  45.5      0      0    2628   7.2250   NaN        C  \n",
       "175    male  18.0      1      1  350404   7.8542   NaN        S  \n",
       "829  female  62.0      0      0  113572  80.0000   B28      NaN  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF = pd.read_csv('train.csv')\n",
    "print(trainDF.info())\n",
    "trainDF.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 418 entries, 0 to 417\n",
      "Data columns (total 11 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  418 non-null    int64  \n",
      " 1   Pclass       418 non-null    int64  \n",
      " 2   Name         418 non-null    object \n",
      " 3   Sex          418 non-null    object \n",
      " 4   Age          332 non-null    float64\n",
      " 5   SibSp        418 non-null    int64  \n",
      " 6   Parch        418 non-null    int64  \n",
      " 7   Ticket       418 non-null    object \n",
      " 8   Fare         417 non-null    float64\n",
      " 9   Cabin        91 non-null     object \n",
      " 10  Embarked     418 non-null    object \n",
      "dtypes: float64(2), int64(4), object(5)\n",
      "memory usage: 36.0+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>901</td>\n",
       "      <td>3</td>\n",
       "      <td>Davies, Mr. John Samuel</td>\n",
       "      <td>male</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>A/4 48871</td>\n",
       "      <td>24.1500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>1226</td>\n",
       "      <td>3</td>\n",
       "      <td>Cor, Mr. Ivan</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>349229</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>933</td>\n",
       "      <td>1</td>\n",
       "      <td>Franklin, Mr. Thomas Parham</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>113778</td>\n",
       "      <td>26.5500</td>\n",
       "      <td>D34</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Pclass                         Name   Sex   Age  SibSp  \\\n",
       "9            901       3      Davies, Mr. John Samuel  male  21.0      2   \n",
       "334         1226       3                Cor, Mr. Ivan  male  27.0      0   \n",
       "41           933       1  Franklin, Mr. Thomas Parham  male   NaN      0   \n",
       "\n",
       "     Parch     Ticket     Fare Cabin Embarked  \n",
       "9        0  A/4 48871  24.1500   NaN        S  \n",
       "334      0     349229   7.8958   NaN        S  \n",
       "41       0     113778  26.5500   D34        S  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDF = pd.read_csv('test.csv')\n",
    "print(testDF.info())\n",
    "testDF.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Survived</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.338481</td>\n",
       "      <td>-0.077221</td>\n",
       "      <td>-0.035322</td>\n",
       "      <td>0.081629</td>\n",
       "      <td>0.257307</td>\n",
       "      <td>0.543351</td>\n",
       "      <td>-0.543351</td>\n",
       "      <td>0.168240</td>\n",
       "      <td>0.003650</td>\n",
       "      <td>-0.155660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pclass</th>\n",
       "      <td>-0.338481</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.408106</td>\n",
       "      <td>0.060832</td>\n",
       "      <td>0.018322</td>\n",
       "      <td>-0.558629</td>\n",
       "      <td>-0.124617</td>\n",
       "      <td>0.124617</td>\n",
       "      <td>-0.269658</td>\n",
       "      <td>0.230491</td>\n",
       "      <td>0.096335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>-0.077221</td>\n",
       "      <td>-0.408106</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.243699</td>\n",
       "      <td>-0.150917</td>\n",
       "      <td>0.178740</td>\n",
       "      <td>-0.063645</td>\n",
       "      <td>0.063645</td>\n",
       "      <td>0.085777</td>\n",
       "      <td>-0.019458</td>\n",
       "      <td>-0.075972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SibSp</th>\n",
       "      <td>-0.035322</td>\n",
       "      <td>0.060832</td>\n",
       "      <td>-0.243699</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.373587</td>\n",
       "      <td>0.160238</td>\n",
       "      <td>0.109609</td>\n",
       "      <td>-0.109609</td>\n",
       "      <td>-0.048396</td>\n",
       "      <td>-0.048678</td>\n",
       "      <td>0.075198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Parch</th>\n",
       "      <td>0.081629</td>\n",
       "      <td>0.018322</td>\n",
       "      <td>-0.150917</td>\n",
       "      <td>0.373587</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.221539</td>\n",
       "      <td>0.213125</td>\n",
       "      <td>-0.213125</td>\n",
       "      <td>-0.008635</td>\n",
       "      <td>-0.100943</td>\n",
       "      <td>0.073258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fare</th>\n",
       "      <td>0.257307</td>\n",
       "      <td>-0.558629</td>\n",
       "      <td>0.178740</td>\n",
       "      <td>0.160238</td>\n",
       "      <td>0.221539</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.185523</td>\n",
       "      <td>-0.185523</td>\n",
       "      <td>0.286269</td>\n",
       "      <td>-0.130059</td>\n",
       "      <td>-0.172683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sex_female</th>\n",
       "      <td>0.543351</td>\n",
       "      <td>-0.124617</td>\n",
       "      <td>-0.063645</td>\n",
       "      <td>0.109609</td>\n",
       "      <td>0.213125</td>\n",
       "      <td>0.185523</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.066564</td>\n",
       "      <td>0.088651</td>\n",
       "      <td>-0.119504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sex_male</th>\n",
       "      <td>-0.543351</td>\n",
       "      <td>0.124617</td>\n",
       "      <td>0.063645</td>\n",
       "      <td>-0.109609</td>\n",
       "      <td>-0.213125</td>\n",
       "      <td>-0.185523</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.066564</td>\n",
       "      <td>-0.088651</td>\n",
       "      <td>0.119504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Embarked_C</th>\n",
       "      <td>0.168240</td>\n",
       "      <td>-0.269658</td>\n",
       "      <td>0.085777</td>\n",
       "      <td>-0.048396</td>\n",
       "      <td>-0.008635</td>\n",
       "      <td>0.286269</td>\n",
       "      <td>0.066564</td>\n",
       "      <td>-0.066564</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.164166</td>\n",
       "      <td>-0.775441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Embarked_Q</th>\n",
       "      <td>0.003650</td>\n",
       "      <td>0.230491</td>\n",
       "      <td>-0.019458</td>\n",
       "      <td>-0.048678</td>\n",
       "      <td>-0.100943</td>\n",
       "      <td>-0.130059</td>\n",
       "      <td>0.088651</td>\n",
       "      <td>-0.088651</td>\n",
       "      <td>-0.164166</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.489874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Embarked_S</th>\n",
       "      <td>-0.155660</td>\n",
       "      <td>0.096335</td>\n",
       "      <td>-0.075972</td>\n",
       "      <td>0.075198</td>\n",
       "      <td>0.073258</td>\n",
       "      <td>-0.172683</td>\n",
       "      <td>-0.119504</td>\n",
       "      <td>0.119504</td>\n",
       "      <td>-0.775441</td>\n",
       "      <td>-0.489874</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Survived    Pclass       Age     SibSp     Parch      Fare  \\\n",
       "Survived    1.000000 -0.338481 -0.077221 -0.035322  0.081629  0.257307   \n",
       "Pclass     -0.338481  1.000000 -0.408106  0.060832  0.018322 -0.558629   \n",
       "Age        -0.077221 -0.408106  1.000000 -0.243699 -0.150917  0.178740   \n",
       "SibSp      -0.035322  0.060832 -0.243699  1.000000  0.373587  0.160238   \n",
       "Parch       0.081629  0.018322 -0.150917  0.373587  1.000000  0.221539   \n",
       "Fare        0.257307 -0.558629  0.178740  0.160238  0.221539  1.000000   \n",
       "Sex_female  0.543351 -0.124617 -0.063645  0.109609  0.213125  0.185523   \n",
       "Sex_male   -0.543351  0.124617  0.063645 -0.109609 -0.213125 -0.185523   \n",
       "Embarked_C  0.168240 -0.269658  0.085777 -0.048396 -0.008635  0.286269   \n",
       "Embarked_Q  0.003650  0.230491 -0.019458 -0.048678 -0.100943 -0.130059   \n",
       "Embarked_S -0.155660  0.096335 -0.075972  0.075198  0.073258 -0.172683   \n",
       "\n",
       "            Sex_female  Sex_male  Embarked_C  Embarked_Q  Embarked_S  \n",
       "Survived      0.543351 -0.543351    0.168240    0.003650   -0.155660  \n",
       "Pclass       -0.124617  0.124617   -0.269658    0.230491    0.096335  \n",
       "Age          -0.063645  0.063645    0.085777   -0.019458   -0.075972  \n",
       "SibSp         0.109609 -0.109609   -0.048396   -0.048678    0.075198  \n",
       "Parch         0.213125 -0.213125   -0.008635   -0.100943    0.073258  \n",
       "Fare          0.185523 -0.185523    0.286269   -0.130059   -0.172683  \n",
       "Sex_female    1.000000 -1.000000    0.066564    0.088651   -0.119504  \n",
       "Sex_male     -1.000000  1.000000   -0.066564   -0.088651    0.119504  \n",
       "Embarked_C    0.066564 -0.066564    1.000000   -0.164166   -0.775441  \n",
       "Embarked_Q    0.088651 -0.088651   -0.164166    1.000000   -0.489874  \n",
       "Embarked_S   -0.119504  0.119504   -0.775441   -0.489874    1.000000  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatDF = pd.concat([trainDF,testDF])\n",
    "concatDF = concatDF.reset_index(drop=True)\n",
    "pd.get_dummies(concatDF,columns=['Sex','Embarked']).drop(columns=['PassengerId','Name','Ticket','Cabin']).corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p1/3vvm71l55lddcz4t_gf8bd1r0000gn/T/ipykernel_57870/1256574800.py:2: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  embarked = concatDF[['Pclass','Fare','Embarked','PassengerId']].groupby(['Pclass','Fare','Embarked']).count()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pclass</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"9\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">(-0.001, 8.662]</th>\n",
       "      <th>C</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">(8.662, 26.0]</th>\n",
       "      <th>C</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">(26.0, 512.329]</th>\n",
       "      <th>C</th>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S</th>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"9\" valign=\"top\">2</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">(-0.001, 8.662]</th>\n",
       "      <th>C</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">(8.662, 26.0]</th>\n",
       "      <th>C</th>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S</th>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">(26.0, 512.329]</th>\n",
       "      <th>C</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S</th>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"9\" valign=\"top\">3</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">(-0.001, 8.662]</th>\n",
       "      <th>C</th>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q</th>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S</th>\n",
       "      <td>291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">(8.662, 26.0]</th>\n",
       "      <th>C</th>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S</th>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">(26.0, 512.329]</th>\n",
       "      <th>C</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S</th>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 PassengerId\n",
       "Pclass Fare            Embarked             \n",
       "1      (-0.001, 8.662] C                   0\n",
       "                       Q                   0\n",
       "                       S                   8\n",
       "       (8.662, 26.0]   C                   2\n",
       "                       Q                   0\n",
       "                       S                  12\n",
       "       (26.0, 512.329] C                 139\n",
       "                       Q                   3\n",
       "                       S                 157\n",
       "2      (-0.001, 8.662] C                   0\n",
       "                       Q                   0\n",
       "                       S                   6\n",
       "       (8.662, 26.0]   C                  17\n",
       "                       Q                   7\n",
       "                       S                 187\n",
       "       (26.0, 512.329] C                  11\n",
       "                       Q                   0\n",
       "                       S                  49\n",
       "3      (-0.001, 8.662] C                  58\n",
       "                       Q                  91\n",
       "                       S                 291\n",
       "       (8.662, 26.0]   C                  43\n",
       "                       Q                  16\n",
       "                       S                 144\n",
       "       (26.0, 512.329] C                   0\n",
       "                       Q                   6\n",
       "                       S                  59"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatDF['Fare']=pd.qcut(concatDF['Fare'],3)\n",
    "embarked = concatDF[['Pclass','Fare','Embarked','PassengerId']].groupby(['Pclass','Fare','Embarked']).count()\n",
    "embarked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     891 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>329</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Goldsmith, Mrs. Frank John (Emily Alice Brown)</td>\n",
       "      <td>female</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>363291</td>\n",
       "      <td>20.525</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>734</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Berriman, Mr. William John</td>\n",
       "      <td>male</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28425</td>\n",
       "      <td>13.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>796</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Otter, Mr. Richard</td>\n",
       "      <td>male</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28213</td>\n",
       "      <td>13.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass  \\\n",
       "328          329         1       3   \n",
       "733          734         0       2   \n",
       "795          796         0       2   \n",
       "\n",
       "                                               Name     Sex   Age  SibSp  \\\n",
       "328  Goldsmith, Mrs. Frank John (Emily Alice Brown)  female  31.0      1   \n",
       "733                      Berriman, Mr. William John    male  23.0      0   \n",
       "795                              Otter, Mr. Richard    male  39.0      0   \n",
       "\n",
       "     Parch  Ticket    Fare Cabin Embarked  \n",
       "328      1  363291  20.525   NaN        S  \n",
       "733      0   28425  13.000   NaN        S  \n",
       "795      0   28213  13.000   NaN        S  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train1DF = pd.read_csv('train1.csv')\n",
    "print(train1DF.info())\n",
    "train1DF.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1309 entries, 0 to 417\n",
      "Data columns (total 11 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  1309 non-null   int64  \n",
      " 1   Survived     891 non-null    float64\n",
      " 2   Fare         1308 non-null   float64\n",
      " 3   Pclass_1     1309 non-null   bool   \n",
      " 4   Pclass_2     1309 non-null   bool   \n",
      " 5   Pclass_3     1309 non-null   bool   \n",
      " 6   Sex_female   1309 non-null   bool   \n",
      " 7   Sex_male     1309 non-null   bool   \n",
      " 8   Embarked_C   1309 non-null   bool   \n",
      " 9   Embarked_Q   1309 non-null   bool   \n",
      " 10  Embarked_S   1309 non-null   bool   \n",
      "dtypes: bool(8), float64(2), int64(1)\n",
      "memory usage: 51.1 KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Pclass_1</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Embarked_C</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>1136</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>510</td>\n",
       "      <td>1.0</td>\n",
       "      <td>56.4958</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>971</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived     Fare  Pclass_1  Pclass_2  Pclass_3  Sex_female  \\\n",
       "244         1136       NaN  23.4500     False     False      True       False   \n",
       "509          510       1.0  56.4958     False     False      True       False   \n",
       "79           971       NaN   7.7500     False     False      True        True   \n",
       "\n",
       "     Sex_male  Embarked_C  Embarked_Q  Embarked_S  \n",
       "244      True       False       False        True  \n",
       "509      True       False       False        True  \n",
       "79      False       False        True       False  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatDF = pd.concat([train1DF,testDF])\n",
    "concatDF= concatDF[['PassengerId','Survived','Pclass','Fare','Sex','Embarked']]\n",
    "concatDF = pd.get_dummies(concatDF,columns=['Pclass','Sex','Embarked'])\n",
    "print(concatDF.info())\n",
    "concatDF.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>824</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Moor, Mrs. (Beila)</td>\n",
       "      <td>female</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>392096</td>\n",
       "      <td>12.4750</td>\n",
       "      <td>E121</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Skoog, Master. Harald</td>\n",
       "      <td>male</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>347088</td>\n",
       "      <td>27.9000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>286</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Stankovic, Mr. Ivan</td>\n",
       "      <td>male</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>349239</td>\n",
       "      <td>8.6625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass                   Name     Sex   Age  \\\n",
       "823          824         1       3     Moor, Mrs. (Beila)  female  27.0   \n",
       "63            64         0       3  Skoog, Master. Harald    male   4.0   \n",
       "285          286         0       3    Stankovic, Mr. Ivan    male  33.0   \n",
       "\n",
       "     SibSp  Parch  Ticket     Fare Cabin Embarked  \n",
       "823      0      1  392096  12.4750  E121        S  \n",
       "63       3      2  347088  27.9000   NaN        S  \n",
       "285      0      0  349239   8.6625   NaN        C  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "trainDF = concatDF[:len(trainDF)]\n",
    "testDF = concatDF[len(trainDF):]\n",
    "train1DF.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([7.2500, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000]),\n",
       " tensor([1., 0.]))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, DF:pd.DataFrame):\n",
    "        self.PassengerId= DF['PassengerId'].values\n",
    "        self.Servived = pd.get_dummies(DF['Survived'].astype(float)).values\n",
    "        DF = DF.drop(columns=['PassengerId','Survived'])\n",
    "        self.data = DF.astype(float).values\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.PassengerId)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        x = torch.FloatTensor(self.data[idx])\n",
    "        y = torch.FloatTensor(self.Servived[idx])\n",
    "        return x, y\n",
    "\n",
    "\n",
    "dataSet = MyDataset(DF=trainDF)\n",
    "testSet = MyDataset(DF=testDF)\n",
    "dataSet[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainSet, valSet = torch.utils.data.random_split(dataSet,(0.8,0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 26.5500,   1.0000,   0.0000,  ...,   0.0000,   0.0000,   1.0000],\n",
       "         [  7.7500,   0.0000,   0.0000,  ...,   0.0000,   1.0000,   0.0000],\n",
       "         [227.5250,   1.0000,   0.0000,  ...,   1.0000,   0.0000,   0.0000],\n",
       "         ...,\n",
       "         [  7.8792,   0.0000,   0.0000,  ...,   0.0000,   1.0000,   0.0000],\n",
       "         [ 21.0750,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   1.0000],\n",
       "         [  7.2250,   0.0000,   0.0000,  ...,   1.0000,   0.0000,   0.0000]]),\n",
       " tensor([[1., 0.],\n",
       "         [0., 1.],\n",
       "         [1., 0.],\n",
       "         ...,\n",
       "         [0., 1.],\n",
       "         [1., 0.],\n",
       "         [1., 0.]])]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainLoader= torch.utils.data.DataLoader(trainSet,batch_size=2048,sampler=torch.utils.data.RandomSampler(trainSet))\n",
    "valLoader= torch.utils.data.DataLoader(valSet,batch_size=2048,sampler=torch.utils.data.RandomSampler(valSet))\n",
    "testLoader = torch.utils.data.DataLoader(testSet,batch_size=2048)\n",
    "next(iter(trainLoader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n",
      "NeuralNetwork(\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=9, out_features=8, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=8, out_features=7, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=7, out_features=6, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=6, out_features=5, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=5, out_features=4, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Linear(in_features=4, out_features=3, bias=True)\n",
      "    (11): ReLU()\n",
      "    (12): Linear(in_features=3, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = torch.nn.Sequential(\n",
    "            torch.nn.Linear(9, 8),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(8, 7),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(7, 6),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(6, 5),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(5, 4),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(4, 3),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(3, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnt: 0 - valLoss: 0.7042537927627563 - trainLoss: 0.7045736908912659\n",
      "cnt: 0 - valLoss: 0.7042015194892883 - trainLoss: 0.7045247554779053\n",
      "cnt: 0 - valLoss: 0.7041487097740173 - trainLoss: 0.7044757604598999\n",
      "cnt: 0 - valLoss: 0.7040960788726807 - trainLoss: 0.7044262886047363\n",
      "cnt: 0 - valLoss: 0.7040436863899231 - trainLoss: 0.7043768763542175\n",
      "cnt: 0 - valLoss: 0.7039912939071655 - trainLoss: 0.7043277621269226\n",
      "cnt: 0 - valLoss: 0.7039399147033691 - trainLoss: 0.7042786478996277\n",
      "cnt: 0 - valLoss: 0.7038882970809937 - trainLoss: 0.7042296528816223\n",
      "cnt: 0 - valLoss: 0.7038368582725525 - trainLoss: 0.7041808366775513\n",
      "cnt: 0 - valLoss: 0.7037851810455322 - trainLoss: 0.7041320204734802\n",
      "cnt: 0 - valLoss: 0.7037336230278015 - trainLoss: 0.7040831446647644\n",
      "cnt: 0 - valLoss: 0.7036821842193604 - trainLoss: 0.7040342688560486\n",
      "cnt: 0 - valLoss: 0.7036306262016296 - trainLoss: 0.7039855718612671\n",
      "cnt: 0 - valLoss: 0.7035790085792542 - trainLoss: 0.703936755657196\n",
      "cnt: 0 - valLoss: 0.7035271525382996 - trainLoss: 0.7038879990577698\n",
      "cnt: 0 - valLoss: 0.7034755945205688 - trainLoss: 0.7038387656211853\n",
      "cnt: 0 - valLoss: 0.7034244537353516 - trainLoss: 0.7037897706031799\n",
      "cnt: 0 - valLoss: 0.7033734917640686 - trainLoss: 0.7037408351898193\n",
      "cnt: 0 - valLoss: 0.7033231258392334 - trainLoss: 0.7036920189857483\n",
      "cnt: 0 - valLoss: 0.7032727003097534 - trainLoss: 0.7036433815956116\n",
      "cnt: 0 - valLoss: 0.703222393989563 - trainLoss: 0.7035946846008301\n",
      "cnt: 0 - valLoss: 0.7031724452972412 - trainLoss: 0.7035460472106934\n",
      "cnt: 0 - valLoss: 0.7031225562095642 - trainLoss: 0.7034974694252014\n",
      "cnt: 0 - valLoss: 0.7030728459358215 - trainLoss: 0.7034490704536438\n",
      "cnt: 0 - valLoss: 0.7030230164527893 - trainLoss: 0.7034007906913757\n",
      "cnt: 0 - valLoss: 0.7029736042022705 - trainLoss: 0.7033523917198181\n",
      "cnt: 0 - valLoss: 0.7029243111610413 - trainLoss: 0.7033039331436157\n",
      "cnt: 0 - valLoss: 0.7028754353523254 - trainLoss: 0.7032555341720581\n",
      "cnt: 0 - valLoss: 0.7028263807296753 - trainLoss: 0.70320725440979\n",
      "cnt: 0 - valLoss: 0.7027773857116699 - trainLoss: 0.7031587958335876\n",
      "cnt: 0 - valLoss: 0.7027288675308228 - trainLoss: 0.7031105160713196\n",
      "cnt: 0 - valLoss: 0.7026802897453308 - trainLoss: 0.7030626535415649\n",
      "cnt: 0 - valLoss: 0.7026316523551941 - trainLoss: 0.7030149698257446\n",
      "cnt: 0 - valLoss: 0.7025831341743469 - trainLoss: 0.7029671669006348\n",
      "cnt: 0 - valLoss: 0.7025347352027893 - trainLoss: 0.7029194831848145\n",
      "cnt: 0 - valLoss: 0.7024862766265869 - trainLoss: 0.7028719782829285\n",
      "cnt: 0 - valLoss: 0.7024381756782532 - trainLoss: 0.7028244733810425\n",
      "cnt: 0 - valLoss: 0.7023902535438538 - trainLoss: 0.7027770280838013\n",
      "cnt: 0 - valLoss: 0.7023423910140991 - trainLoss: 0.7027297616004944\n",
      "cnt: 0 - valLoss: 0.7022947072982788 - trainLoss: 0.702682614326477\n",
      "cnt: 0 - valLoss: 0.7022470235824585 - trainLoss: 0.7026354074478149\n",
      "cnt: 0 - valLoss: 0.7021994590759277 - trainLoss: 0.7025883793830872\n",
      "cnt: 0 - valLoss: 0.7021521329879761 - trainLoss: 0.7025414109230042\n",
      "cnt: 0 - valLoss: 0.7021047472953796 - trainLoss: 0.7024945020675659\n",
      "cnt: 0 - valLoss: 0.7020575404167175 - trainLoss: 0.7024476528167725\n",
      "cnt: 0 - valLoss: 0.7020103335380554 - trainLoss: 0.7024009823799133\n",
      "cnt: 0 - valLoss: 0.7019631862640381 - trainLoss: 0.7023541927337646\n",
      "cnt: 0 - valLoss: 0.701915979385376 - trainLoss: 0.7023073434829712\n",
      "cnt: 0 - valLoss: 0.7018689513206482 - trainLoss: 0.7022606730461121\n",
      "cnt: 0 - valLoss: 0.7018221020698547 - trainLoss: 0.7022140026092529\n",
      "cnt: 0 - valLoss: 0.7017751932144165 - trainLoss: 0.7021674513816833\n",
      "cnt: 0 - valLoss: 0.7017287015914917 - trainLoss: 0.7021211385726929\n",
      "cnt: 0 - valLoss: 0.7016822695732117 - trainLoss: 0.7020751237869263\n",
      "cnt: 0 - valLoss: 0.7016359567642212 - trainLoss: 0.7020291090011597\n",
      "cnt: 0 - valLoss: 0.7015896439552307 - trainLoss: 0.7019833326339722\n",
      "cnt: 0 - valLoss: 0.701543390750885 - trainLoss: 0.7019376158714294\n",
      "cnt: 0 - valLoss: 0.7014971971511841 - trainLoss: 0.7018919587135315\n",
      "cnt: 0 - valLoss: 0.7014510631561279 - trainLoss: 0.7018462419509888\n",
      "cnt: 0 - valLoss: 0.7014049291610718 - trainLoss: 0.7018006443977356\n",
      "cnt: 0 - valLoss: 0.70135897397995 - trainLoss: 0.701755166053772\n",
      "cnt: 0 - valLoss: 0.7013131380081177 - trainLoss: 0.7017097473144531\n",
      "cnt: 0 - valLoss: 0.7012671828269958 - trainLoss: 0.7016644477844238\n",
      "cnt: 0 - valLoss: 0.7012214064598083 - trainLoss: 0.7016191482543945\n",
      "cnt: 0 - valLoss: 0.7011756300926208 - trainLoss: 0.7015739679336548\n",
      "cnt: 0 - valLoss: 0.7011299729347229 - trainLoss: 0.7015289068222046\n",
      "cnt: 0 - valLoss: 0.7010843753814697 - trainLoss: 0.7014839053153992\n",
      "cnt: 0 - valLoss: 0.7010388374328613 - trainLoss: 0.7014390230178833\n",
      "cnt: 0 - valLoss: 0.7009935975074768 - trainLoss: 0.701394259929657\n",
      "cnt: 0 - valLoss: 0.7009482979774475 - trainLoss: 0.701349675655365\n",
      "cnt: 0 - valLoss: 0.700903058052063 - trainLoss: 0.701305091381073\n",
      "cnt: 0 - valLoss: 0.7008577585220337 - trainLoss: 0.7012606263160706\n",
      "cnt: 0 - valLoss: 0.7008123397827148 - trainLoss: 0.7012159824371338\n",
      "cnt: 0 - valLoss: 0.700767457485199 - trainLoss: 0.701171338558197\n",
      "cnt: 0 - valLoss: 0.7007227540016174 - trainLoss: 0.7011271119117737\n",
      "cnt: 0 - valLoss: 0.7006786465644836 - trainLoss: 0.7010830044746399\n",
      "cnt: 0 - valLoss: 0.7006338834762573 - trainLoss: 0.7010392546653748\n",
      "cnt: 0 - valLoss: 0.7005892395973206 - trainLoss: 0.7009949684143066\n",
      "cnt: 0 - valLoss: 0.700544536113739 - trainLoss: 0.7009507417678833\n",
      "cnt: 0 - valLoss: 0.7005004286766052 - trainLoss: 0.7009067535400391\n",
      "cnt: 0 - valLoss: 0.7004563212394714 - trainLoss: 0.7008633017539978\n",
      "cnt: 0 - valLoss: 0.7004124522209167 - trainLoss: 0.7008198499679565\n",
      "cnt: 0 - valLoss: 0.7003684639930725 - trainLoss: 0.7007765173912048\n",
      "cnt: 0 - valLoss: 0.7003245949745178 - trainLoss: 0.7007332444190979\n",
      "cnt: 0 - valLoss: 0.7002806663513184 - trainLoss: 0.7006899118423462\n",
      "cnt: 0 - valLoss: 0.7002364993095398 - trainLoss: 0.7006465196609497\n",
      "cnt: 0 - valLoss: 0.7001924514770508 - trainLoss: 0.7006027698516846\n",
      "cnt: 0 - valLoss: 0.700148344039917 - trainLoss: 0.7005591988563538\n",
      "cnt: 0 - valLoss: 0.7001044154167175 - trainLoss: 0.700515627861023\n",
      "cnt: 0 - valLoss: 0.7000604271888733 - trainLoss: 0.7004719972610474\n",
      "cnt: 0 - valLoss: 0.7000162601470947 - trainLoss: 0.7004284262657166\n",
      "cnt: 0 - valLoss: 0.6999720931053162 - trainLoss: 0.7003846168518066\n",
      "cnt: 0 - valLoss: 0.699928343296051 - trainLoss: 0.7003408670425415\n",
      "cnt: 0 - valLoss: 0.6998848915100098 - trainLoss: 0.7002973556518555\n",
      "cnt: 0 - valLoss: 0.6998415589332581 - trainLoss: 0.7002538442611694\n",
      "cnt: 0 - valLoss: 0.6997983455657959 - trainLoss: 0.7002106308937073\n",
      "cnt: 0 - valLoss: 0.6997560262680054 - trainLoss: 0.7001672983169556\n",
      "cnt: 0 - valLoss: 0.699713945388794 - trainLoss: 0.7001250982284546\n",
      "cnt: 0 - valLoss: 0.6996719241142273 - trainLoss: 0.7000830769538879\n",
      "cnt: 0 - valLoss: 0.6996299624443054 - trainLoss: 0.7000411152839661\n",
      "cnt: 0 - valLoss: 0.6995879411697388 - trainLoss: 0.6999990940093994\n",
      "cnt: 0 - valLoss: 0.6995460987091064 - trainLoss: 0.6999573111534119\n",
      "cnt: 0 - valLoss: 0.6995041966438293 - trainLoss: 0.6999154090881348\n",
      "cnt: 0 - valLoss: 0.699462354183197 - trainLoss: 0.6998736262321472\n",
      "cnt: 0 - valLoss: 0.6994205713272095 - trainLoss: 0.6998317837715149\n",
      "cnt: 0 - valLoss: 0.6993788480758667 - trainLoss: 0.6997900009155273\n",
      "cnt: 0 - valLoss: 0.6993371248245239 - trainLoss: 0.6997483372688293\n",
      "cnt: 0 - valLoss: 0.699295699596405 - trainLoss: 0.6997066736221313\n",
      "cnt: 0 - valLoss: 0.6992546916007996 - trainLoss: 0.6996651887893677\n",
      "cnt: 0 - valLoss: 0.6992142200469971 - trainLoss: 0.6996239423751831\n",
      "cnt: 0 - valLoss: 0.6991741061210632 - trainLoss: 0.6995828151702881\n",
      "cnt: 0 - valLoss: 0.6991341710090637 - trainLoss: 0.6995419859886169\n",
      "cnt: 0 - valLoss: 0.6990943551063538 - trainLoss: 0.6995011568069458\n",
      "cnt: 0 - valLoss: 0.6990549564361572 - trainLoss: 0.6994603872299194\n",
      "cnt: 0 - valLoss: 0.6990155577659607 - trainLoss: 0.6994198560714722\n",
      "cnt: 0 - valLoss: 0.6989762783050537 - trainLoss: 0.6993793249130249\n",
      "cnt: 0 - valLoss: 0.698936939239502 - trainLoss: 0.6993388533592224\n",
      "cnt: 0 - valLoss: 0.6988977193832397 - trainLoss: 0.6992985010147095\n",
      "cnt: 0 - valLoss: 0.6988585591316223 - trainLoss: 0.6992581486701965\n",
      "cnt: 0 - valLoss: 0.6988193392753601 - trainLoss: 0.6992177367210388\n",
      "cnt: 0 - valLoss: 0.6987802982330322 - trainLoss: 0.6991775035858154\n",
      "cnt: 0 - valLoss: 0.6987411975860596 - trainLoss: 0.699137270450592\n",
      "cnt: 0 - valLoss: 0.6987021565437317 - trainLoss: 0.6990970969200134\n",
      "cnt: 0 - valLoss: 0.6986632347106934 - trainLoss: 0.6990569829940796\n",
      "cnt: 0 - valLoss: 0.6986242532730103 - trainLoss: 0.6990169286727905\n",
      "cnt: 0 - valLoss: 0.6985853910446167 - trainLoss: 0.6989768743515015\n",
      "cnt: 0 - valLoss: 0.6985465288162231 - trainLoss: 0.6989367604255676\n",
      "cnt: 0 - valLoss: 0.6985076665878296 - trainLoss: 0.6988968253135681\n",
      "cnt: 0 - valLoss: 0.6984689235687256 - trainLoss: 0.6988568902015686\n",
      "cnt: 0 - valLoss: 0.6984301209449768 - trainLoss: 0.6988169550895691\n",
      "cnt: 0 - valLoss: 0.698391318321228 - trainLoss: 0.6987770199775696\n",
      "cnt: 0 - valLoss: 0.6983526945114136 - trainLoss: 0.6987370848655701\n",
      "cnt: 0 - valLoss: 0.6983141303062439 - trainLoss: 0.6986972689628601\n",
      "cnt: 0 - valLoss: 0.6982754468917847 - trainLoss: 0.6986575126647949\n",
      "cnt: 0 - valLoss: 0.6982369422912598 - trainLoss: 0.6986178159713745\n",
      "cnt: 0 - valLoss: 0.6981989145278931 - trainLoss: 0.6985781192779541\n",
      "cnt: 0 - valLoss: 0.6981618404388428 - trainLoss: 0.6985390782356262\n",
      "cnt: 0 - valLoss: 0.6981247663497925 - trainLoss: 0.6985009908676147\n",
      "cnt: 0 - valLoss: 0.6980878114700317 - trainLoss: 0.698462963104248\n",
      "cnt: 0 - valLoss: 0.698050856590271 - trainLoss: 0.6984249353408813\n",
      "cnt: 0 - valLoss: 0.6980138421058655 - trainLoss: 0.6983869671821594\n",
      "cnt: 0 - valLoss: 0.6979770660400391 - trainLoss: 0.6983489990234375\n",
      "cnt: 0 - valLoss: 0.6979403495788574 - trainLoss: 0.6983112692832947\n",
      "cnt: 0 - valLoss: 0.6979036927223206 - trainLoss: 0.6982735395431519\n",
      "cnt: 0 - valLoss: 0.6978670358657837 - trainLoss: 0.6982359290122986\n",
      "cnt: 0 - valLoss: 0.6978307366371155 - trainLoss: 0.6981982588768005\n",
      "cnt: 0 - valLoss: 0.6977945566177368 - trainLoss: 0.6981607675552368\n",
      "cnt: 0 - valLoss: 0.6977584362030029 - trainLoss: 0.6981233954429626\n",
      "cnt: 0 - valLoss: 0.6977223753929138 - trainLoss: 0.6980860233306885\n",
      "cnt: 0 - valLoss: 0.6976863741874695 - trainLoss: 0.6980487108230591\n",
      "cnt: 0 - valLoss: 0.6976502537727356 - trainLoss: 0.6980114579200745\n",
      "cnt: 0 - valLoss: 0.6976142525672913 - trainLoss: 0.6979742050170898\n",
      "cnt: 0 - valLoss: 0.6975783705711365 - trainLoss: 0.6979369521141052\n",
      "cnt: 0 - valLoss: 0.6975423693656921 - trainLoss: 0.6978998184204102\n",
      "cnt: 0 - valLoss: 0.6975065469741821 - trainLoss: 0.6978626847267151\n",
      "cnt: 0 - valLoss: 0.6974707841873169 - trainLoss: 0.6978256702423096\n",
      "cnt: 0 - valLoss: 0.6974349021911621 - trainLoss: 0.6977885961532593\n",
      "cnt: 0 - valLoss: 0.6973992586135864 - trainLoss: 0.6977515816688538\n",
      "cnt: 0 - valLoss: 0.6973634958267212 - trainLoss: 0.697714626789093\n",
      "cnt: 0 - valLoss: 0.6973279118537903 - trainLoss: 0.697677731513977\n",
      "cnt: 0 - valLoss: 0.6972922682762146 - trainLoss: 0.6976408958435059\n",
      "cnt: 0 - valLoss: 0.6972566843032837 - trainLoss: 0.6976040005683899\n",
      "cnt: 0 - valLoss: 0.6972211599349976 - trainLoss: 0.6975672245025635\n",
      "cnt: 0 - valLoss: 0.6971856951713562 - trainLoss: 0.6975305080413818\n",
      "cnt: 0 - valLoss: 0.6971502304077148 - trainLoss: 0.6974937915802002\n",
      "cnt: 0 - valLoss: 0.6971147656440735 - trainLoss: 0.6974570751190186\n",
      "cnt: 0 - valLoss: 0.6970794200897217 - trainLoss: 0.6974204182624817\n",
      "cnt: 0 - valLoss: 0.6970441341400146 - trainLoss: 0.6973839402198792\n",
      "cnt: 0 - valLoss: 0.6970088481903076 - trainLoss: 0.6973474025726318\n",
      "cnt: 0 - valLoss: 0.6969735622406006 - trainLoss: 0.6973108649253845\n",
      "cnt: 0 - valLoss: 0.6969383955001831 - trainLoss: 0.697274386882782\n",
      "cnt: 0 - valLoss: 0.6969032883644104 - trainLoss: 0.6972379684448242\n",
      "cnt: 0 - valLoss: 0.6968681216239929 - trainLoss: 0.6972016096115112\n",
      "cnt: 0 - valLoss: 0.696833074092865 - trainLoss: 0.6971651911735535\n",
      "cnt: 0 - valLoss: 0.6967980265617371 - trainLoss: 0.6971288919448853\n",
      "cnt: 0 - valLoss: 0.6967630386352539 - trainLoss: 0.6970927119255066\n",
      "cnt: 0 - valLoss: 0.6967281103134155 - trainLoss: 0.6970564126968384\n",
      "cnt: 0 - valLoss: 0.6966931819915771 - trainLoss: 0.6970202326774597\n",
      "cnt: 0 - valLoss: 0.6966583728790283 - trainLoss: 0.6969841718673706\n",
      "cnt: 0 - valLoss: 0.6966235041618347 - trainLoss: 0.6969480514526367\n",
      "cnt: 0 - valLoss: 0.6965886950492859 - trainLoss: 0.6969119310379028\n",
      "cnt: 0 - valLoss: 0.6965540051460266 - trainLoss: 0.6968759298324585\n",
      "cnt: 0 - valLoss: 0.6965192556381226 - trainLoss: 0.6968399882316589\n",
      "cnt: 0 - valLoss: 0.6964845657348633 - trainLoss: 0.6968040466308594\n",
      "cnt: 0 - valLoss: 0.6964499354362488 - trainLoss: 0.6967681646347046\n",
      "cnt: 0 - valLoss: 0.6964153051376343 - trainLoss: 0.6967323422431946\n",
      "cnt: 0 - valLoss: 0.6963807940483093 - trainLoss: 0.6966965198516846\n",
      "cnt: 0 - valLoss: 0.6963462829589844 - trainLoss: 0.6966607570648193\n",
      "cnt: 0 - valLoss: 0.696311891078949 - trainLoss: 0.6966251134872437\n",
      "cnt: 0 - valLoss: 0.6962776184082031 - trainLoss: 0.6965895891189575\n",
      "cnt: 0 - valLoss: 0.6962433457374573 - trainLoss: 0.6965541839599609\n",
      "cnt: 0 - valLoss: 0.6962091326713562 - trainLoss: 0.6965187191963196\n",
      "cnt: 0 - valLoss: 0.6961749792098999 - trainLoss: 0.6964834332466125\n",
      "cnt: 0 - valLoss: 0.6961408257484436 - trainLoss: 0.6964480876922607\n",
      "cnt: 0 - valLoss: 0.6961067318916321 - trainLoss: 0.6964128017425537\n",
      "cnt: 0 - valLoss: 0.6960726976394653 - trainLoss: 0.6963775157928467\n",
      "cnt: 0 - valLoss: 0.6960386037826538 - trainLoss: 0.6963423490524292\n",
      "cnt: 0 - valLoss: 0.6960046291351318 - trainLoss: 0.6963071823120117\n",
      "cnt: 0 - valLoss: 0.6959707140922546 - trainLoss: 0.696272075176239\n",
      "cnt: 0 - valLoss: 0.6959367990493774 - trainLoss: 0.6962369680404663\n",
      "cnt: 0 - valLoss: 0.695902943611145 - trainLoss: 0.6962019205093384\n",
      "cnt: 0 - valLoss: 0.6958690881729126 - trainLoss: 0.6961669325828552\n",
      "cnt: 0 - valLoss: 0.6958352327346802 - trainLoss: 0.6961319446563721\n",
      "cnt: 0 - valLoss: 0.6958015561103821 - trainLoss: 0.6960970163345337\n",
      "cnt: 0 - valLoss: 0.6957678198814392 - trainLoss: 0.6960621476173401\n",
      "cnt: 0 - valLoss: 0.6957340836524963 - trainLoss: 0.6960272192955017\n",
      "cnt: 0 - valLoss: 0.695700466632843 - trainLoss: 0.6959924697875977\n",
      "cnt: 0 - valLoss: 0.6956669092178345 - trainLoss: 0.695957601070404\n",
      "cnt: 0 - valLoss: 0.6956332921981812 - trainLoss: 0.6959229111671448\n",
      "cnt: 0 - valLoss: 0.6955997943878174 - trainLoss: 0.6958881616592407\n",
      "cnt: 0 - valLoss: 0.6955663561820984 - trainLoss: 0.6958535313606262\n",
      "cnt: 0 - valLoss: 0.6955328583717346 - trainLoss: 0.6958189010620117\n",
      "cnt: 0 - valLoss: 0.6954994797706604 - trainLoss: 0.695784330368042\n",
      "cnt: 0 - valLoss: 0.6954660415649414 - trainLoss: 0.695749819278717\n",
      "cnt: 0 - valLoss: 0.695432722568512 - trainLoss: 0.6957152485847473\n",
      "cnt: 0 - valLoss: 0.6953994035720825 - trainLoss: 0.6956808567047119\n",
      "cnt: 0 - valLoss: 0.6953662037849426 - trainLoss: 0.6956464052200317\n",
      "cnt: 0 - valLoss: 0.695332944393158 - trainLoss: 0.6956120133399963\n",
      "cnt: 0 - valLoss: 0.6952998638153076 - trainLoss: 0.6955776810646057\n",
      "cnt: 0 - valLoss: 0.6952667236328125 - trainLoss: 0.6955433487892151\n",
      "cnt: 0 - valLoss: 0.6952335834503174 - trainLoss: 0.695509135723114\n",
      "cnt: 0 - valLoss: 0.695200502872467 - trainLoss: 0.6954748630523682\n",
      "cnt: 0 - valLoss: 0.6951675415039062 - trainLoss: 0.6954407095909119\n",
      "cnt: 0 - valLoss: 0.6951345801353455 - trainLoss: 0.6954065561294556\n",
      "cnt: 0 - valLoss: 0.6951016187667847 - trainLoss: 0.6953724026679993\n",
      "cnt: 0 - valLoss: 0.6950687170028687 - trainLoss: 0.6953383684158325\n",
      "cnt: 0 - valLoss: 0.6950358152389526 - trainLoss: 0.695304274559021\n",
      "cnt: 0 - valLoss: 0.6950029730796814 - trainLoss: 0.6952703595161438\n",
      "cnt: 0 - valLoss: 0.6949701905250549 - trainLoss: 0.695236325263977\n",
      "cnt: 0 - valLoss: 0.6949374675750732 - trainLoss: 0.6952024102210999\n",
      "cnt: 0 - valLoss: 0.6949048042297363 - trainLoss: 0.6951685547828674\n",
      "cnt: 0 - valLoss: 0.6948720812797546 - trainLoss: 0.695134699344635\n",
      "cnt: 0 - valLoss: 0.6948394179344177 - trainLoss: 0.6951008439064026\n",
      "cnt: 0 - valLoss: 0.6948068141937256 - trainLoss: 0.6950670480728149\n",
      "cnt: 0 - valLoss: 0.6947742104530334 - trainLoss: 0.6950333714485168\n",
      "cnt: 0 - valLoss: 0.6947416663169861 - trainLoss: 0.694999635219574\n",
      "cnt: 0 - valLoss: 0.6947092413902283 - trainLoss: 0.6949660181999207\n",
      "cnt: 0 - valLoss: 0.6946767568588257 - trainLoss: 0.6949323415756226\n",
      "cnt: 0 - valLoss: 0.6946442723274231 - trainLoss: 0.694898784160614\n",
      "cnt: 0 - valLoss: 0.6946119070053101 - trainLoss: 0.6948652863502502\n",
      "cnt: 0 - valLoss: 0.694579541683197 - trainLoss: 0.6948317289352417\n",
      "cnt: 0 - valLoss: 0.6945472359657288 - trainLoss: 0.6947982311248779\n",
      "cnt: 0 - valLoss: 0.6945149898529053 - trainLoss: 0.6947647929191589\n",
      "cnt: 0 - valLoss: 0.694482684135437 - trainLoss: 0.6947314143180847\n",
      "cnt: 0 - valLoss: 0.6944504976272583 - trainLoss: 0.6946980357170105\n",
      "cnt: 0 - valLoss: 0.6944182515144348 - trainLoss: 0.694664716720581\n",
      "cnt: 0 - valLoss: 0.6943861842155457 - trainLoss: 0.6946313977241516\n",
      "cnt: 0 - valLoss: 0.6943540573120117 - trainLoss: 0.6945981383323669\n",
      "cnt: 0 - valLoss: 0.6943220496177673 - trainLoss: 0.6945648789405823\n",
      "cnt: 0 - valLoss: 0.6942899823188782 - trainLoss: 0.6945317387580872\n",
      "cnt: 0 - valLoss: 0.6942580342292786 - trainLoss: 0.694498598575592\n",
      "cnt: 0 - valLoss: 0.6942260265350342 - trainLoss: 0.6944654583930969\n",
      "cnt: 0 - valLoss: 0.6941941380500793 - trainLoss: 0.6944324374198914\n",
      "cnt: 0 - valLoss: 0.6941622495651245 - trainLoss: 0.6943994164466858\n",
      "cnt: 0 - valLoss: 0.6941303610801697 - trainLoss: 0.6943663954734802\n",
      "cnt: 0 - valLoss: 0.6940985321998596 - trainLoss: 0.6943334341049194\n",
      "cnt: 0 - valLoss: 0.6940667629241943 - trainLoss: 0.6943005323410034\n",
      "cnt: 0 - valLoss: 0.6940350532531738 - trainLoss: 0.6942676305770874\n",
      "cnt: 0 - valLoss: 0.6940033435821533 - trainLoss: 0.6942347288131714\n",
      "cnt: 0 - valLoss: 0.6939716339111328 - trainLoss: 0.6942019462585449\n",
      "cnt: 0 - valLoss: 0.6939401030540466 - trainLoss: 0.6941691637039185\n",
      "cnt: 0 - valLoss: 0.6939084529876709 - trainLoss: 0.694136381149292\n",
      "cnt: 0 - valLoss: 0.6938768625259399 - trainLoss: 0.6941037178039551\n",
      "cnt: 0 - valLoss: 0.6938453316688538 - trainLoss: 0.6940710544586182\n",
      "cnt: 0 - valLoss: 0.6938138604164124 - trainLoss: 0.694038450717926\n",
      "cnt: 0 - valLoss: 0.693782389163971 - trainLoss: 0.6940057873725891\n",
      "cnt: 0 - valLoss: 0.6937509775161743 - trainLoss: 0.6939732432365417\n",
      "cnt: 0 - valLoss: 0.6937196254730225 - trainLoss: 0.6939406991004944\n",
      "cnt: 0 - valLoss: 0.6936882734298706 - trainLoss: 0.6939082741737366\n",
      "cnt: 0 - valLoss: 0.6936569213867188 - trainLoss: 0.6938758492469788\n",
      "cnt: 0 - valLoss: 0.6936256885528564 - trainLoss: 0.6938433647155762\n",
      "cnt: 0 - valLoss: 0.6935944557189941 - trainLoss: 0.6938109993934631\n",
      "cnt: 0 - valLoss: 0.6935632228851318 - trainLoss: 0.6937786340713501\n",
      "cnt: 0 - valLoss: 0.6935320496559143 - trainLoss: 0.6937463879585266\n",
      "cnt: 0 - valLoss: 0.6935008764266968 - trainLoss: 0.6937141418457031\n",
      "cnt: 0 - valLoss: 0.6934698224067688 - trainLoss: 0.6936818361282349\n",
      "cnt: 0 - valLoss: 0.6934387683868408 - trainLoss: 0.6936497092247009\n",
      "cnt: 0 - valLoss: 0.6934077143669128 - trainLoss: 0.693617582321167\n",
      "cnt: 0 - valLoss: 0.6933767795562744 - trainLoss: 0.6935853958129883\n",
      "cnt: 0 - valLoss: 0.693345844745636 - trainLoss: 0.6935532689094543\n",
      "cnt: 0 - valLoss: 0.6933149099349976 - trainLoss: 0.69352126121521\n",
      "cnt: 0 - valLoss: 0.6932839751243591 - trainLoss: 0.6934892535209656\n",
      "cnt: 0 - valLoss: 0.6932531595230103 - trainLoss: 0.6934572458267212\n",
      "cnt: 0 - valLoss: 0.6932223439216614 - trainLoss: 0.6934253573417664\n",
      "cnt: 0 - valLoss: 0.6931915283203125 - trainLoss: 0.6933934092521667\n",
      "cnt: 0 - valLoss: 0.6931607723236084 - trainLoss: 0.6933615207672119\n",
      "cnt: 0 - valLoss: 0.6931300759315491 - trainLoss: 0.6933296918869019\n",
      "cnt: 0 - valLoss: 0.6930993795394897 - trainLoss: 0.6932979226112366\n",
      "cnt: 0 - valLoss: 0.6930687427520752 - trainLoss: 0.6932660937309265\n",
      "cnt: 0 - valLoss: 0.6930381655693054 - trainLoss: 0.693234384059906\n",
      "cnt: 0 - valLoss: 0.6930075883865356 - trainLoss: 0.6932026147842407\n",
      "cnt: 0 - valLoss: 0.6929770708084106 - trainLoss: 0.6931710243225098\n",
      "cnt: 0 - valLoss: 0.6929465532302856 - trainLoss: 0.6931394338607788\n",
      "cnt: 0 - valLoss: 0.6929160952568054 - trainLoss: 0.6931077837944031\n",
      "cnt: 0 - valLoss: 0.6928855776786804 - trainLoss: 0.6930762529373169\n",
      "cnt: 0 - valLoss: 0.6928552389144897 - trainLoss: 0.6930447220802307\n",
      "cnt: 0 - valLoss: 0.6928248405456543 - trainLoss: 0.6930133104324341\n",
      "cnt: 0 - valLoss: 0.6927945613861084 - trainLoss: 0.6929817795753479\n",
      "cnt: 0 - valLoss: 0.6927642822265625 - trainLoss: 0.6929503679275513\n",
      "cnt: 0 - valLoss: 0.6927340030670166 - trainLoss: 0.6929190158843994\n",
      "cnt: 0 - valLoss: 0.6927037835121155 - trainLoss: 0.6928876638412476\n",
      "cnt: 0 - valLoss: 0.6926735639572144 - trainLoss: 0.6928563714027405\n",
      "cnt: 0 - valLoss: 0.692643404006958 - trainLoss: 0.6928251385688782\n",
      "cnt: 0 - valLoss: 0.6926133036613464 - trainLoss: 0.6927939057350159\n",
      "cnt: 0 - valLoss: 0.6925832629203796 - trainLoss: 0.6927627325057983\n",
      "cnt: 0 - valLoss: 0.6925532221794128 - trainLoss: 0.6927315592765808\n",
      "cnt: 0 - valLoss: 0.692523181438446 - trainLoss: 0.6927003860473633\n",
      "cnt: 0 - valLoss: 0.692493200302124 - trainLoss: 0.6926693320274353\n",
      "cnt: 0 - valLoss: 0.6924632787704468 - trainLoss: 0.6926382780075073\n",
      "cnt: 0 - valLoss: 0.6924333572387695 - trainLoss: 0.6926072239875793\n",
      "cnt: 0 - valLoss: 0.6924034953117371 - trainLoss: 0.6925762295722961\n",
      "cnt: 0 - valLoss: 0.6923736333847046 - trainLoss: 0.6925452947616577\n",
      "cnt: 0 - valLoss: 0.6923438310623169 - trainLoss: 0.6925143599510193\n",
      "cnt: 0 - valLoss: 0.692314088344574 - trainLoss: 0.6924834251403809\n",
      "cnt: 0 - valLoss: 0.692284345626831 - trainLoss: 0.6924525499343872\n",
      "cnt: 0 - valLoss: 0.6922546029090881 - trainLoss: 0.6924217343330383\n",
      "cnt: 0 - valLoss: 0.69222491979599 - trainLoss: 0.6923909783363342\n",
      "cnt: 0 - valLoss: 0.6921952366828918 - trainLoss: 0.6923602223396301\n",
      "cnt: 0 - valLoss: 0.6921656131744385 - trainLoss: 0.692329466342926\n",
      "cnt: 0 - valLoss: 0.6921360492706299 - trainLoss: 0.6922987699508667\n",
      "cnt: 0 - valLoss: 0.6921065449714661 - trainLoss: 0.6922681331634521\n",
      "cnt: 0 - valLoss: 0.6920769810676575 - trainLoss: 0.6922374963760376\n",
      "cnt: 0 - valLoss: 0.6920474767684937 - trainLoss: 0.6922069191932678\n",
      "cnt: 0 - valLoss: 0.6920180320739746 - trainLoss: 0.6921764016151428\n",
      "cnt: 0 - valLoss: 0.6919886469841003 - trainLoss: 0.6921458840370178\n",
      "cnt: 0 - valLoss: 0.6919592618942261 - trainLoss: 0.692115306854248\n",
      "cnt: 0 - valLoss: 0.6919299364089966 - trainLoss: 0.6920849084854126\n",
      "cnt: 0 - valLoss: 0.6919007301330566 - trainLoss: 0.6920545101165771\n",
      "cnt: 0 - valLoss: 0.6918714046478271 - trainLoss: 0.6920241713523865\n",
      "cnt: 0 - valLoss: 0.6918421983718872 - trainLoss: 0.6919938325881958\n",
      "cnt: 0 - valLoss: 0.6918129920959473 - trainLoss: 0.6919635534286499\n",
      "cnt: 0 - valLoss: 0.6917837858200073 - trainLoss: 0.6919332146644592\n",
      "cnt: 0 - valLoss: 0.6917546987533569 - trainLoss: 0.6919029951095581\n",
      "cnt: 0 - valLoss: 0.6917255520820618 - trainLoss: 0.6918728351593018\n",
      "cnt: 0 - valLoss: 0.6916964650154114 - trainLoss: 0.6918426752090454\n",
      "cnt: 0 - valLoss: 0.6916674375534058 - trainLoss: 0.6918125152587891\n",
      "cnt: 0 - valLoss: 0.6916384696960449 - trainLoss: 0.6917824149131775\n",
      "cnt: 0 - valLoss: 0.6916095018386841 - trainLoss: 0.6917523145675659\n",
      "cnt: 0 - valLoss: 0.6915805339813232 - trainLoss: 0.6917222738265991\n",
      "cnt: 0 - valLoss: 0.6915516257286072 - trainLoss: 0.6916922926902771\n",
      "cnt: 0 - valLoss: 0.6915227770805359 - trainLoss: 0.6916623711585999\n",
      "cnt: 0 - valLoss: 0.6914939284324646 - trainLoss: 0.6916324496269226\n",
      "cnt: 0 - valLoss: 0.6914650797843933 - trainLoss: 0.6916024684906006\n",
      "cnt: 0 - valLoss: 0.6914363503456116 - trainLoss: 0.6915726661682129\n",
      "cnt: 0 - valLoss: 0.6914075613021851 - trainLoss: 0.6915428042411804\n",
      "cnt: 0 - valLoss: 0.6913788318634033 - trainLoss: 0.6915130615234375\n",
      "cnt: 0 - valLoss: 0.6913502216339111 - trainLoss: 0.6914832592010498\n",
      "cnt: 0 - valLoss: 0.6913215517997742 - trainLoss: 0.6914535164833069\n",
      "cnt: 0 - valLoss: 0.6912930011749268 - trainLoss: 0.691423773765564\n",
      "cnt: 0 - valLoss: 0.6912643909454346 - trainLoss: 0.6913941502571106\n",
      "cnt: 0 - valLoss: 0.6912358999252319 - trainLoss: 0.691364586353302\n",
      "cnt: 0 - valLoss: 0.6912074089050293 - trainLoss: 0.6913349628448486\n",
      "cnt: 0 - valLoss: 0.6911789774894714 - trainLoss: 0.69130539894104\n",
      "cnt: 0 - valLoss: 0.6911505460739136 - trainLoss: 0.6912758946418762\n",
      "cnt: 0 - valLoss: 0.6911220550537109 - trainLoss: 0.6912463307380676\n",
      "cnt: 0 - valLoss: 0.6910938024520874 - trainLoss: 0.6912168860435486\n",
      "cnt: 0 - valLoss: 0.6910654306411743 - trainLoss: 0.6911874413490295\n",
      "cnt: 0 - valLoss: 0.691037118434906 - trainLoss: 0.6911580562591553\n",
      "cnt: 0 - valLoss: 0.6910088658332825 - trainLoss: 0.6911287307739258\n",
      "cnt: 0 - valLoss: 0.6909806728363037 - trainLoss: 0.6910993456840515\n",
      "cnt: 0 - valLoss: 0.6909525394439697 - trainLoss: 0.6910700798034668\n",
      "cnt: 0 - valLoss: 0.6909242868423462 - trainLoss: 0.6910408139228821\n",
      "cnt: 0 - valLoss: 0.690896213054657 - trainLoss: 0.6910115480422974\n",
      "cnt: 0 - valLoss: 0.690868079662323 - trainLoss: 0.6909823417663574\n",
      "cnt: 0 - valLoss: 0.6908400654792786 - trainLoss: 0.6909531950950623\n",
      "cnt: 0 - valLoss: 0.6908120512962341 - trainLoss: 0.6909240484237671\n",
      "cnt: 0 - valLoss: 0.6907840967178345 - trainLoss: 0.6908949613571167\n",
      "cnt: 0 - valLoss: 0.6907560229301453 - trainLoss: 0.6908658742904663\n",
      "cnt: 0 - valLoss: 0.6907281279563904 - trainLoss: 0.6908368468284607\n",
      "cnt: 0 - valLoss: 0.6907002329826355 - trainLoss: 0.6908078789710999\n",
      "cnt: 0 - valLoss: 0.6906723976135254 - trainLoss: 0.6907788515090942\n",
      "cnt: 0 - valLoss: 0.6906446218490601 - trainLoss: 0.6907499432563782\n",
      "cnt: 0 - valLoss: 0.69061678647995 - trainLoss: 0.6907210350036621\n",
      "cnt: 0 - valLoss: 0.6905890703201294 - trainLoss: 0.6906921863555908\n",
      "cnt: 0 - valLoss: 0.6905612945556641 - trainLoss: 0.6906632781028748\n",
      "cnt: 0 - valLoss: 0.6905335187911987 - trainLoss: 0.690634548664093\n",
      "cnt: 0 - valLoss: 0.690505862236023 - trainLoss: 0.6906057596206665\n",
      "cnt: 0 - valLoss: 0.6904782652854919 - trainLoss: 0.6905769109725952\n",
      "cnt: 0 - valLoss: 0.6904506683349609 - trainLoss: 0.690548300743103\n",
      "cnt: 0 - valLoss: 0.6904230713844299 - trainLoss: 0.6905196309089661\n",
      "cnt: 0 - valLoss: 0.6903954744338989 - trainLoss: 0.6904909610748291\n",
      "cnt: 0 - valLoss: 0.6903679370880127 - trainLoss: 0.6904622912406921\n",
      "cnt: 0 - valLoss: 0.6903404593467712 - trainLoss: 0.6904337406158447\n",
      "cnt: 0 - valLoss: 0.6903130412101746 - trainLoss: 0.6904051899909973\n",
      "cnt: 0 - valLoss: 0.6902856230735779 - trainLoss: 0.6903766393661499\n",
      "cnt: 0 - valLoss: 0.690258264541626 - trainLoss: 0.690348207950592\n",
      "cnt: 0 - valLoss: 0.6902307868003845 - trainLoss: 0.6903197169303894\n",
      "cnt: 0 - valLoss: 0.6902034878730774 - trainLoss: 0.6902912855148315\n",
      "cnt: 0 - valLoss: 0.6901761889457703 - trainLoss: 0.6902628540992737\n",
      "cnt: 0 - valLoss: 0.6901488900184631 - trainLoss: 0.6902344822883606\n",
      "cnt: 0 - valLoss: 0.6901217103004456 - trainLoss: 0.6902061700820923\n",
      "cnt: 0 - valLoss: 0.690094530582428 - trainLoss: 0.690177857875824\n",
      "cnt: 0 - valLoss: 0.6900672912597656 - trainLoss: 0.6901496052742004\n",
      "cnt: 0 - valLoss: 0.6900401711463928 - trainLoss: 0.6901213526725769\n",
      "cnt: 0 - valLoss: 0.69001305103302 - trainLoss: 0.6900931596755981\n",
      "cnt: 0 - valLoss: 0.6899859309196472 - trainLoss: 0.6900649666786194\n",
      "cnt: 0 - valLoss: 0.6899588704109192 - trainLoss: 0.6900368332862854\n",
      "cnt: 0 - valLoss: 0.6899318695068359 - trainLoss: 0.6900087594985962\n",
      "cnt: 0 - valLoss: 0.6899048089981079 - trainLoss: 0.6899806261062622\n",
      "cnt: 0 - valLoss: 0.6898778676986694 - trainLoss: 0.689952552318573\n",
      "cnt: 0 - valLoss: 0.6898508667945862 - trainLoss: 0.6899245381355286\n",
      "cnt: 0 - valLoss: 0.6898240447044373 - trainLoss: 0.6898965239524841\n",
      "cnt: 0 - valLoss: 0.6897971630096436 - trainLoss: 0.6898685693740845\n",
      "cnt: 0 - valLoss: 0.6897703409194946 - trainLoss: 0.6898406147956848\n",
      "cnt: 0 - valLoss: 0.6897435784339905 - trainLoss: 0.6898127198219299\n",
      "cnt: 0 - valLoss: 0.6897167563438416 - trainLoss: 0.6897848844528198\n",
      "cnt: 0 - valLoss: 0.6896899938583374 - trainLoss: 0.6897569894790649\n",
      "cnt: 0 - valLoss: 0.6896633505821228 - trainLoss: 0.6897292137145996\n",
      "cnt: 0 - valLoss: 0.6896366477012634 - trainLoss: 0.6897014379501343\n",
      "cnt: 0 - valLoss: 0.6896100640296936 - trainLoss: 0.6896737217903137\n",
      "cnt: 0 - valLoss: 0.6895833611488342 - trainLoss: 0.6896460056304932\n",
      "cnt: 0 - valLoss: 0.6895567774772644 - trainLoss: 0.6896182894706726\n",
      "cnt: 0 - valLoss: 0.6895301938056946 - trainLoss: 0.6895906329154968\n",
      "cnt: 0 - valLoss: 0.6895037293434143 - trainLoss: 0.6895630359649658\n",
      "cnt: 0 - valLoss: 0.6894772052764893 - trainLoss: 0.6895354986190796\n",
      "cnt: 0 - valLoss: 0.6894508004188538 - trainLoss: 0.6895079016685486\n",
      "cnt: 0 - valLoss: 0.6894243359565735 - trainLoss: 0.6894803643226624\n",
      "cnt: 0 - valLoss: 0.689397931098938 - trainLoss: 0.6894529461860657\n",
      "cnt: 0 - valLoss: 0.6893715858459473 - trainLoss: 0.6894254088401794\n",
      "cnt: 0 - valLoss: 0.6893451809883118 - trainLoss: 0.6893979907035828\n",
      "cnt: 0 - valLoss: 0.6893188953399658 - trainLoss: 0.6893706321716309\n",
      "cnt: 0 - valLoss: 0.6892926692962646 - trainLoss: 0.689343273639679\n",
      "cnt: 0 - valLoss: 0.6892663836479187 - trainLoss: 0.689315915107727\n",
      "cnt: 0 - valLoss: 0.6892402172088623 - trainLoss: 0.6892886757850647\n",
      "cnt: 0 - valLoss: 0.6892139911651611 - trainLoss: 0.6892613768577576\n",
      "cnt: 0 - valLoss: 0.6891878843307495 - trainLoss: 0.6892341375350952\n",
      "cnt: 0 - valLoss: 0.6891617178916931 - trainLoss: 0.6892069578170776\n",
      "cnt: 0 - valLoss: 0.6891356706619263 - trainLoss: 0.6891797780990601\n",
      "cnt: 0 - valLoss: 0.6891095638275146 - trainLoss: 0.6891526579856873\n",
      "cnt: 0 - valLoss: 0.6890835762023926 - trainLoss: 0.6891255378723145\n",
      "cnt: 0 - valLoss: 0.6890575289726257 - trainLoss: 0.6890984773635864\n",
      "cnt: 0 - valLoss: 0.6890316009521484 - trainLoss: 0.6890714168548584\n",
      "cnt: 0 - valLoss: 0.6890056729316711 - trainLoss: 0.6890444755554199\n",
      "cnt: 0 - valLoss: 0.6889797449111938 - trainLoss: 0.6890174746513367\n",
      "cnt: 0 - valLoss: 0.6889538764953613 - trainLoss: 0.6889905333518982\n",
      "cnt: 0 - valLoss: 0.6889280080795288 - trainLoss: 0.6889635324478149\n",
      "cnt: 0 - valLoss: 0.6889022588729858 - trainLoss: 0.688936710357666\n",
      "cnt: 0 - valLoss: 0.6888764500617981 - trainLoss: 0.6889098882675171\n",
      "cnt: 0 - valLoss: 0.6888507008552551 - trainLoss: 0.6888830065727234\n",
      "cnt: 0 - valLoss: 0.6888250112533569 - trainLoss: 0.6888561844825745\n",
      "cnt: 0 - valLoss: 0.6887993216514587 - trainLoss: 0.6888294816017151\n",
      "cnt: 0 - valLoss: 0.6887735724449158 - trainLoss: 0.6888027191162109\n",
      "cnt: 0 - valLoss: 0.6887479424476624 - trainLoss: 0.6887760162353516\n",
      "cnt: 0 - valLoss: 0.6887223720550537 - trainLoss: 0.6887493133544922\n",
      "cnt: 0 - valLoss: 0.6886967420578003 - trainLoss: 0.6887226700782776\n",
      "cnt: 0 - valLoss: 0.6886711716651917 - trainLoss: 0.688696026802063\n",
      "cnt: 0 - valLoss: 0.6886457204818726 - trainLoss: 0.6886695027351379\n",
      "cnt: 0 - valLoss: 0.6886201500892639 - trainLoss: 0.6886429190635681\n",
      "cnt: 0 - valLoss: 0.6885946989059448 - trainLoss: 0.6886163949966431\n",
      "cnt: 0 - valLoss: 0.6885692477226257 - trainLoss: 0.688589870929718\n",
      "cnt: 0 - valLoss: 0.6885438561439514 - trainLoss: 0.6885634064674377\n",
      "cnt: 0 - valLoss: 0.6885184645652771 - trainLoss: 0.6885369420051575\n",
      "cnt: 0 - valLoss: 0.6884931325912476 - trainLoss: 0.688510537147522\n",
      "cnt: 0 - valLoss: 0.688467800617218 - trainLoss: 0.6884841918945312\n",
      "cnt: 0 - valLoss: 0.6884424686431885 - trainLoss: 0.6884577870368958\n",
      "cnt: 0 - valLoss: 0.6884172558784485 - trainLoss: 0.688431441783905\n",
      "cnt: 0 - valLoss: 0.6883920431137085 - trainLoss: 0.6884052157402039\n",
      "cnt: 0 - valLoss: 0.6883668303489685 - trainLoss: 0.6883788704872131\n",
      "cnt: 0 - valLoss: 0.6883416175842285 - trainLoss: 0.6883527040481567\n",
      "cnt: 0 - valLoss: 0.6883165240287781 - trainLoss: 0.6883264780044556\n",
      "cnt: 0 - valLoss: 0.6882913708686829 - trainLoss: 0.6883003115653992\n",
      "cnt: 0 - valLoss: 0.6882662773132324 - trainLoss: 0.6882742047309875\n",
      "cnt: 0 - valLoss: 0.688241183757782 - trainLoss: 0.6882479786872864\n",
      "cnt: 0 - valLoss: 0.6882161498069763 - trainLoss: 0.6882219910621643\n",
      "cnt: 0 - valLoss: 0.6881911754608154 - trainLoss: 0.6881958842277527\n",
      "cnt: 0 - valLoss: 0.6881662607192993 - trainLoss: 0.6881698369979858\n",
      "cnt: 0 - valLoss: 0.6881412267684937 - trainLoss: 0.6881438493728638\n",
      "cnt: 0 - valLoss: 0.6881163716316223 - trainLoss: 0.6881178617477417\n",
      "cnt: 0 - valLoss: 0.6880913972854614 - trainLoss: 0.6880919337272644\n",
      "cnt: 0 - valLoss: 0.6880666017532349 - trainLoss: 0.6880660653114319\n",
      "cnt: 0 - valLoss: 0.6880417466163635 - trainLoss: 0.6880401372909546\n",
      "cnt: 0 - valLoss: 0.6880170702934265 - trainLoss: 0.6880143284797668\n",
      "cnt: 0 - valLoss: 0.6879923343658447 - trainLoss: 0.6879885196685791\n",
      "cnt: 0 - valLoss: 0.6879675984382629 - trainLoss: 0.6879627704620361\n",
      "cnt: 0 - valLoss: 0.6879428625106812 - trainLoss: 0.6879370212554932\n",
      "cnt: 0 - valLoss: 0.6879183053970337 - trainLoss: 0.6879112720489502\n",
      "cnt: 0 - valLoss: 0.6878936290740967 - trainLoss: 0.6878856420516968\n",
      "cnt: 0 - valLoss: 0.6878690123558044 - trainLoss: 0.6878599524497986\n",
      "cnt: 0 - valLoss: 0.6878445148468018 - trainLoss: 0.6878343820571899\n",
      "cnt: 0 - valLoss: 0.6878199577331543 - trainLoss: 0.6878086924552917\n",
      "cnt: 0 - valLoss: 0.6877954602241516 - trainLoss: 0.6877832412719727\n",
      "cnt: 0 - valLoss: 0.6877709627151489 - trainLoss: 0.6877575516700745\n",
      "cnt: 0 - valLoss: 0.6877464652061462 - trainLoss: 0.6877321004867554\n",
      "cnt: 0 - valLoss: 0.6877220869064331 - trainLoss: 0.6877066493034363\n",
      "cnt: 0 - valLoss: 0.6876976490020752 - trainLoss: 0.6876811385154724\n",
      "cnt: 0 - valLoss: 0.6876733303070068 - trainLoss: 0.6876558065414429\n",
      "cnt: 0 - valLoss: 0.6876490116119385 - trainLoss: 0.6876303553581238\n",
      "cnt: 0 - valLoss: 0.6876246929168701 - trainLoss: 0.6876049637794495\n",
      "cnt: 0 - valLoss: 0.6876004338264465 - trainLoss: 0.6875796318054199\n",
      "cnt: 0 - valLoss: 0.6875761151313782 - trainLoss: 0.6875542998313904\n",
      "cnt: 0 - valLoss: 0.6875518560409546 - trainLoss: 0.6875290274620056\n",
      "cnt: 0 - valLoss: 0.6875277161598206 - trainLoss: 0.6875037550926208\n",
      "cnt: 0 - valLoss: 0.6875035166740417 - trainLoss: 0.6874785423278809\n",
      "cnt: 0 - valLoss: 0.6874793171882629 - trainLoss: 0.6874533295631409\n",
      "cnt: 0 - valLoss: 0.6874552369117737 - trainLoss: 0.6874281167984009\n",
      "cnt: 0 - valLoss: 0.6874310970306396 - trainLoss: 0.6874030232429504\n",
      "cnt: 0 - valLoss: 0.6874070763587952 - trainLoss: 0.6873778700828552\n",
      "cnt: 0 - valLoss: 0.6873830556869507 - trainLoss: 0.6873527765274048\n",
      "cnt: 0 - valLoss: 0.6873589158058167 - trainLoss: 0.6873276829719543\n",
      "cnt: 0 - valLoss: 0.6873349547386169 - trainLoss: 0.6873025894165039\n",
      "cnt: 0 - valLoss: 0.687311053276062 - trainLoss: 0.6872776746749878\n",
      "cnt: 0 - valLoss: 0.6872870922088623 - trainLoss: 0.6872526407241821\n",
      "cnt: 0 - valLoss: 0.6872632503509521 - trainLoss: 0.687227725982666\n",
      "cnt: 0 - valLoss: 0.6872392892837524 - trainLoss: 0.6872028112411499\n",
      "cnt: 0 - valLoss: 0.6872154474258423 - trainLoss: 0.6871778964996338\n",
      "cnt: 0 - valLoss: 0.6871916055679321 - trainLoss: 0.6871529817581177\n",
      "cnt: 0 - valLoss: 0.687167763710022 - trainLoss: 0.6871281266212463\n",
      "cnt: 0 - valLoss: 0.6871441006660461 - trainLoss: 0.6871033310890198\n",
      "cnt: 0 - valLoss: 0.687120258808136 - trainLoss: 0.6870784759521484\n",
      "cnt: 0 - valLoss: 0.6870965361595154 - trainLoss: 0.6870537400245667\n",
      "cnt: 0 - valLoss: 0.6870728731155396 - trainLoss: 0.6870290040969849\n",
      "cnt: 0 - valLoss: 0.6870492100715637 - trainLoss: 0.6870043277740479\n",
      "cnt: 0 - valLoss: 0.6870255470275879 - trainLoss: 0.6869795918464661\n",
      "cnt: 0 - valLoss: 0.6870019435882568 - trainLoss: 0.6869549751281738\n",
      "cnt: 0 - valLoss: 0.6869783997535706 - trainLoss: 0.6869302988052368\n",
      "cnt: 0 - valLoss: 0.6869548559188843 - trainLoss: 0.6869057416915894\n",
      "cnt: 0 - valLoss: 0.686931312084198 - trainLoss: 0.6868811249732971\n",
      "cnt: 0 - valLoss: 0.6869077682495117 - trainLoss: 0.6868566274642944\n",
      "cnt: 0 - valLoss: 0.6868842244148254 - trainLoss: 0.6868321299552917\n",
      "cnt: 0 - valLoss: 0.6868607997894287 - trainLoss: 0.6868076324462891\n",
      "cnt: 0 - valLoss: 0.686837375164032 - trainLoss: 0.6867831349372864\n",
      "cnt: 0 - valLoss: 0.68681401014328 - trainLoss: 0.6867587566375732\n",
      "cnt: 0 - valLoss: 0.6867905855178833 - trainLoss: 0.6867343783378601\n",
      "cnt: 0 - valLoss: 0.6867672801017761 - trainLoss: 0.6867099404335022\n",
      "cnt: 0 - valLoss: 0.686743974685669 - trainLoss: 0.6866855621337891\n",
      "cnt: 0 - valLoss: 0.6867206692695618 - trainLoss: 0.6866613030433655\n",
      "cnt: 0 - valLoss: 0.6866973638534546 - trainLoss: 0.6866369843482971\n",
      "cnt: 0 - valLoss: 0.6866741180419922 - trainLoss: 0.6866127252578735\n",
      "cnt: 0 - valLoss: 0.6866509318351746 - trainLoss: 0.6865884065628052\n",
      "cnt: 0 - valLoss: 0.6866277456283569 - trainLoss: 0.6865642070770264\n",
      "cnt: 0 - valLoss: 0.6866045594215393 - trainLoss: 0.6865400671958923\n",
      "cnt: 0 - valLoss: 0.6865814328193665 - trainLoss: 0.6865159273147583\n",
      "cnt: 0 - valLoss: 0.6865583062171936 - trainLoss: 0.6864917874336243\n",
      "cnt: 0 - valLoss: 0.6865352392196655 - trainLoss: 0.6864676475524902\n",
      "cnt: 0 - valLoss: 0.6865121722221375 - trainLoss: 0.686443567276001\n",
      "cnt: 0 - valLoss: 0.6864891052246094 - trainLoss: 0.6864194869995117\n",
      "cnt: 0 - valLoss: 0.6864661574363708 - trainLoss: 0.6863954067230225\n",
      "cnt: 0 - valLoss: 0.6864430904388428 - trainLoss: 0.6863714456558228\n",
      "cnt: 0 - valLoss: 0.686420202255249 - trainLoss: 0.686347484588623\n",
      "cnt: 0 - valLoss: 0.6863972544670105 - trainLoss: 0.6863235831260681\n",
      "cnt: 0 - valLoss: 0.6863744258880615 - trainLoss: 0.6862995624542236\n",
      "cnt: 0 - valLoss: 0.686351478099823 - trainLoss: 0.6862756609916687\n",
      "cnt: 0 - valLoss: 0.686328649520874 - trainLoss: 0.6862518191337585\n",
      "cnt: 0 - valLoss: 0.686305820941925 - trainLoss: 0.6862279176712036\n",
      "cnt: 0 - valLoss: 0.6862829923629761 - trainLoss: 0.6862041354179382\n",
      "cnt: 0 - valLoss: 0.6862602233886719 - trainLoss: 0.6861802935600281\n",
      "cnt: 0 - valLoss: 0.6862374544143677 - trainLoss: 0.6861565709114075\n",
      "cnt: 0 - valLoss: 0.6862147450447083 - trainLoss: 0.6861328482627869\n",
      "cnt: 0 - valLoss: 0.6861920952796936 - trainLoss: 0.6861091256141663\n",
      "cnt: 0 - valLoss: 0.6861693859100342 - trainLoss: 0.6860854029655457\n",
      "cnt: 0 - valLoss: 0.6861467957496643 - trainLoss: 0.6860617995262146\n",
      "cnt: 0 - valLoss: 0.6861241459846497 - trainLoss: 0.6860381364822388\n",
      "cnt: 0 - valLoss: 0.686101496219635 - trainLoss: 0.6860145330429077\n",
      "cnt: 0 - valLoss: 0.6860788464546204 - trainLoss: 0.6859908699989319\n",
      "cnt: 0 - valLoss: 0.6860563158988953 - trainLoss: 0.6859673857688904\n",
      "cnt: 0 - valLoss: 0.6860337853431702 - trainLoss: 0.6859437227249146\n",
      "cnt: 0 - valLoss: 0.6860112547874451 - trainLoss: 0.6859201788902283\n",
      "cnt: 0 - valLoss: 0.6859887838363647 - trainLoss: 0.6858966946601868\n",
      "cnt: 0 - valLoss: 0.6859662532806396 - trainLoss: 0.6858732104301453\n",
      "cnt: 0 - valLoss: 0.6859438419342041 - trainLoss: 0.6858498454093933\n",
      "cnt: 0 - valLoss: 0.6859214305877686 - trainLoss: 0.6858263611793518\n",
      "cnt: 0 - valLoss: 0.685899019241333 - trainLoss: 0.6858029365539551\n",
      "cnt: 0 - valLoss: 0.6858766674995422 - trainLoss: 0.6857795715332031\n",
      "cnt: 0 - valLoss: 0.6858543753623962 - trainLoss: 0.6857562065124512\n",
      "cnt: 0 - valLoss: 0.6858320832252502 - trainLoss: 0.685732901096344\n",
      "cnt: 0 - valLoss: 0.6858097314834595 - trainLoss: 0.6857095956802368\n",
      "cnt: 0 - valLoss: 0.6857874989509583 - trainLoss: 0.6856863498687744\n",
      "cnt: 0 - valLoss: 0.6857653260231018 - trainLoss: 0.6856630444526672\n",
      "cnt: 0 - valLoss: 0.6857430934906006 - trainLoss: 0.6856398582458496\n",
      "cnt: 0 - valLoss: 0.6857209205627441 - trainLoss: 0.6856167316436768\n",
      "cnt: 0 - valLoss: 0.6856987476348877 - trainLoss: 0.6855935454368591\n",
      "cnt: 0 - valLoss: 0.6856765747070312 - trainLoss: 0.6855703592300415\n",
      "cnt: 0 - valLoss: 0.6856545209884644 - trainLoss: 0.6855472326278687\n",
      "cnt: 0 - valLoss: 0.6856324076652527 - trainLoss: 0.6855241656303406\n",
      "cnt: 0 - valLoss: 0.6856103539466858 - trainLoss: 0.6855010390281677\n",
      "cnt: 0 - valLoss: 0.6855883002281189 - trainLoss: 0.6854780316352844\n",
      "cnt: 0 - valLoss: 0.6855663061141968 - trainLoss: 0.6854550838470459\n",
      "cnt: 0 - valLoss: 0.6855443716049194 - trainLoss: 0.6854320764541626\n",
      "cnt: 0 - valLoss: 0.6855223774909973 - trainLoss: 0.6854090690612793\n",
      "cnt: 0 - valLoss: 0.68550044298172 - trainLoss: 0.6853861808776855\n",
      "cnt: 0 - valLoss: 0.6854785680770874 - trainLoss: 0.685363233089447\n",
      "cnt: 0 - valLoss: 0.6854566335678101 - trainLoss: 0.685340404510498\n",
      "cnt: 0 - valLoss: 0.6854348182678223 - trainLoss: 0.6853175163269043\n",
      "cnt: 0 - valLoss: 0.6854129433631897 - trainLoss: 0.6852946877479553\n",
      "cnt: 0 - valLoss: 0.6853911876678467 - trainLoss: 0.6852718591690063\n",
      "cnt: 0 - valLoss: 0.6853694319725037 - trainLoss: 0.6852491497993469\n",
      "cnt: 0 - valLoss: 0.6853476166725159 - trainLoss: 0.685226321220398\n",
      "cnt: 0 - valLoss: 0.6853258609771729 - trainLoss: 0.6852036118507385\n",
      "cnt: 0 - valLoss: 0.6853041648864746 - trainLoss: 0.6851809024810791\n",
      "cnt: 0 - valLoss: 0.6852825284004211 - trainLoss: 0.6851582527160645\n",
      "cnt: 0 - valLoss: 0.6852608323097229 - trainLoss: 0.685135543346405\n",
      "cnt: 0 - valLoss: 0.6852391958236694 - trainLoss: 0.6851128935813904\n",
      "cnt: 0 - valLoss: 0.6852176189422607 - trainLoss: 0.6850903630256653\n",
      "cnt: 0 - valLoss: 0.6851961016654968 - trainLoss: 0.6850677728652954\n",
      "cnt: 0 - valLoss: 0.6851744651794434 - trainLoss: 0.6850451827049255\n",
      "cnt: 0 - valLoss: 0.6851529479026794 - trainLoss: 0.6850227117538452\n",
      "cnt: 0 - valLoss: 0.6851314306259155 - trainLoss: 0.6850001811981201\n",
      "cnt: 0 - valLoss: 0.6851099133491516 - trainLoss: 0.6849777102470398\n",
      "cnt: 0 - valLoss: 0.6850885152816772 - trainLoss: 0.6849552989006042\n",
      "cnt: 0 - valLoss: 0.6850670576095581 - trainLoss: 0.6849327683448792\n",
      "cnt: 0 - valLoss: 0.685045599937439 - trainLoss: 0.6849103569984436\n",
      "cnt: 0 - valLoss: 0.6850242018699646 - trainLoss: 0.6848880052566528\n",
      "cnt: 0 - valLoss: 0.685002863407135 - trainLoss: 0.6848656535148621\n",
      "cnt: 0 - valLoss: 0.6849815249443054 - trainLoss: 0.6848433017730713\n",
      "cnt: 0 - valLoss: 0.6849601864814758 - trainLoss: 0.6848210692405701\n",
      "cnt: 0 - valLoss: 0.684938907623291 - trainLoss: 0.6847987174987793\n",
      "cnt: 0 - valLoss: 0.6849176287651062 - trainLoss: 0.6847764253616333\n",
      "cnt: 0 - valLoss: 0.6848963499069214 - trainLoss: 0.6847541928291321\n",
      "cnt: 0 - valLoss: 0.6848751306533813 - trainLoss: 0.6847319602966309\n",
      "cnt: 0 - valLoss: 0.6848539710044861 - trainLoss: 0.6847097873687744\n",
      "cnt: 0 - valLoss: 0.6848328113555908 - trainLoss: 0.684687614440918\n",
      "cnt: 0 - valLoss: 0.6848116517066956 - trainLoss: 0.6846655011177063\n",
      "cnt: 0 - valLoss: 0.6847905516624451 - trainLoss: 0.6846433877944946\n",
      "cnt: 0 - valLoss: 0.6847693920135498 - trainLoss: 0.6846213340759277\n",
      "cnt: 0 - valLoss: 0.6847482919692993 - trainLoss: 0.6845992207527161\n",
      "cnt: 0 - valLoss: 0.6847272515296936 - trainLoss: 0.684577226638794\n",
      "cnt: 0 - valLoss: 0.6847062706947327 - trainLoss: 0.6845552325248718\n",
      "cnt: 0 - valLoss: 0.6846852898597717 - trainLoss: 0.6845332384109497\n",
      "cnt: 0 - valLoss: 0.684664249420166 - trainLoss: 0.6845111846923828\n",
      "cnt: 0 - valLoss: 0.6846433281898499 - trainLoss: 0.6844893097877502\n",
      "cnt: 0 - valLoss: 0.6846224069595337 - trainLoss: 0.6844674348831177\n",
      "cnt: 0 - valLoss: 0.6846014857292175 - trainLoss: 0.6844455599784851\n",
      "cnt: 0 - valLoss: 0.6845806241035461 - trainLoss: 0.6844236254692078\n",
      "cnt: 0 - valLoss: 0.6845597624778748 - trainLoss: 0.6844017505645752\n",
      "cnt: 0 - valLoss: 0.6845389604568481 - trainLoss: 0.6843799948692322\n",
      "cnt: 0 - valLoss: 0.6845180988311768 - trainLoss: 0.6843581795692444\n",
      "cnt: 0 - valLoss: 0.6844972968101501 - trainLoss: 0.6843364238739014\n",
      "cnt: 0 - valLoss: 0.6844765543937683 - trainLoss: 0.6843147277832031\n",
      "cnt: 0 - valLoss: 0.6844558119773865 - trainLoss: 0.6842929720878601\n",
      "cnt: 0 - valLoss: 0.6844350695610046 - trainLoss: 0.6842712759971619\n",
      "cnt: 0 - valLoss: 0.6844143867492676 - trainLoss: 0.6842495203018188\n",
      "cnt: 0 - valLoss: 0.6843936443328857 - trainLoss: 0.6842279434204102\n",
      "cnt: 0 - valLoss: 0.6843729615211487 - trainLoss: 0.6842062473297119\n",
      "cnt: 0 - valLoss: 0.6843523383140564 - trainLoss: 0.6841846108436584\n",
      "cnt: 0 - valLoss: 0.6843317151069641 - trainLoss: 0.6841630935668945\n",
      "cnt: 0 - valLoss: 0.6843112111091614 - trainLoss: 0.6841415166854858\n",
      "cnt: 0 - valLoss: 0.6842907071113586 - trainLoss: 0.6841199398040771\n",
      "cnt: 0 - valLoss: 0.6842700839042664 - trainLoss: 0.684098482131958\n",
      "cnt: 0 - valLoss: 0.6842496395111084 - trainLoss: 0.6840770840644836\n",
      "cnt: 0 - valLoss: 0.6842291951179504 - trainLoss: 0.6840555667877197\n",
      "cnt: 0 - valLoss: 0.6842087507247925 - trainLoss: 0.6840341687202454\n",
      "cnt: 0 - valLoss: 0.6841883063316345 - trainLoss: 0.684012770652771\n",
      "cnt: 0 - valLoss: 0.6841680407524109 - trainLoss: 0.6839914917945862\n",
      "cnt: 0 - valLoss: 0.6841475963592529 - trainLoss: 0.6839701533317566\n",
      "cnt: 0 - valLoss: 0.6841273307800293 - trainLoss: 0.683948814868927\n",
      "cnt: 0 - valLoss: 0.6841070055961609 - trainLoss: 0.6839275360107422\n",
      "cnt: 0 - valLoss: 0.6840867400169373 - trainLoss: 0.6839063167572021\n",
      "cnt: 0 - valLoss: 0.6840664744377136 - trainLoss: 0.6838850378990173\n",
      "cnt: 0 - valLoss: 0.68404620885849 - trainLoss: 0.6838638186454773\n",
      "cnt: 0 - valLoss: 0.6840259432792664 - trainLoss: 0.683842658996582\n",
      "cnt: 0 - valLoss: 0.6840057969093323 - trainLoss: 0.683821439743042\n",
      "cnt: 0 - valLoss: 0.6839856505393982 - trainLoss: 0.6838003396987915\n",
      "cnt: 0 - valLoss: 0.6839654445648193 - trainLoss: 0.6837791800498962\n",
      "cnt: 0 - valLoss: 0.68394535779953 - trainLoss: 0.6837580800056458\n",
      "cnt: 0 - valLoss: 0.6839252710342407 - trainLoss: 0.68373703956604\n",
      "cnt: 0 - valLoss: 0.6839051246643066 - trainLoss: 0.6837160587310791\n",
      "cnt: 0 - valLoss: 0.6838850975036621 - trainLoss: 0.6836950182914734\n",
      "cnt: 0 - valLoss: 0.6838650703430176 - trainLoss: 0.6836739778518677\n",
      "cnt: 0 - valLoss: 0.6838451027870178 - trainLoss: 0.6836529970169067\n",
      "cnt: 0 - valLoss: 0.6838250756263733 - trainLoss: 0.6836320161819458\n",
      "cnt: 0 - valLoss: 0.6838051676750183 - trainLoss: 0.6836111545562744\n",
      "cnt: 0 - valLoss: 0.6837852001190186 - trainLoss: 0.6835902333259583\n",
      "cnt: 0 - valLoss: 0.6837652921676636 - trainLoss: 0.6835693717002869\n",
      "cnt: 0 - valLoss: 0.6837453842163086 - trainLoss: 0.6835484504699707\n",
      "cnt: 0 - valLoss: 0.6837255358695984 - trainLoss: 0.6835277080535889\n",
      "cnt: 0 - valLoss: 0.6837056875228882 - trainLoss: 0.6835068464279175\n",
      "cnt: 0 - valLoss: 0.683685839176178 - trainLoss: 0.6834861636161804\n",
      "cnt: 0 - valLoss: 0.6836660504341125 - trainLoss: 0.6834653615951538\n",
      "cnt: 0 - valLoss: 0.6836463212966919 - trainLoss: 0.6834445595741272\n",
      "cnt: 0 - valLoss: 0.6836265921592712 - trainLoss: 0.6834239363670349\n",
      "cnt: 0 - valLoss: 0.6836068034172058 - trainLoss: 0.6834031939506531\n",
      "cnt: 0 - valLoss: 0.6835871338844299 - trainLoss: 0.683382511138916\n",
      "cnt: 0 - valLoss: 0.683567464351654 - trainLoss: 0.683361828327179\n",
      "cnt: 0 - valLoss: 0.683547854423523 - trainLoss: 0.6833412051200867\n",
      "cnt: 0 - valLoss: 0.6835281848907471 - trainLoss: 0.6833206415176392\n",
      "cnt: 0 - valLoss: 0.6835085153579712 - trainLoss: 0.6833000183105469\n",
      "cnt: 0 - valLoss: 0.6834890246391296 - trainLoss: 0.6832795143127441\n",
      "cnt: 0 - valLoss: 0.6834694743156433 - trainLoss: 0.6832590103149414\n",
      "cnt: 0 - valLoss: 0.683449923992157 - trainLoss: 0.6832385659217834\n",
      "cnt: 0 - valLoss: 0.6834303736686707 - trainLoss: 0.6832179427146912\n",
      "cnt: 0 - valLoss: 0.6834108829498291 - trainLoss: 0.683197557926178\n",
      "cnt: 0 - valLoss: 0.6833914518356323 - trainLoss: 0.6831770539283752\n",
      "cnt: 0 - valLoss: 0.6833720207214355 - trainLoss: 0.6831567287445068\n",
      "cnt: 0 - valLoss: 0.6833525896072388 - trainLoss: 0.6831362843513489\n",
      "cnt: 0 - valLoss: 0.683333158493042 - trainLoss: 0.6831160187721252\n",
      "cnt: 0 - valLoss: 0.68331378698349 - trainLoss: 0.6830955743789673\n",
      "cnt: 0 - valLoss: 0.683294415473938 - trainLoss: 0.6830752491950989\n",
      "cnt: 0 - valLoss: 0.683275043964386 - trainLoss: 0.6830549836158752\n",
      "cnt: 0 - valLoss: 0.6832557320594788 - trainLoss: 0.6830346584320068\n",
      "cnt: 0 - valLoss: 0.6832365393638611 - trainLoss: 0.6830143928527832\n",
      "cnt: 0 - valLoss: 0.6832172274589539 - trainLoss: 0.6829941868782043\n",
      "cnt: 0 - valLoss: 0.6831980347633362 - trainLoss: 0.6829739809036255\n",
      "cnt: 0 - valLoss: 0.6831787824630737 - trainLoss: 0.6829538941383362\n",
      "cnt: 0 - valLoss: 0.683159589767456 - trainLoss: 0.6829336881637573\n",
      "cnt: 0 - valLoss: 0.6831404566764832 - trainLoss: 0.6829134821891785\n",
      "cnt: 0 - valLoss: 0.6831212639808655 - trainLoss: 0.6828933954238892\n",
      "cnt: 0 - valLoss: 0.6831021904945374 - trainLoss: 0.6828733682632446\n",
      "cnt: 0 - valLoss: 0.6830831170082092 - trainLoss: 0.6828533411026001\n",
      "cnt: 0 - valLoss: 0.6830640435218811 - trainLoss: 0.6828332543373108\n",
      "cnt: 0 - valLoss: 0.683044970035553 - trainLoss: 0.6828132271766663\n",
      "cnt: 0 - valLoss: 0.6830258965492249 - trainLoss: 0.6827932596206665\n",
      "cnt: 0 - valLoss: 0.6830068826675415 - trainLoss: 0.682773232460022\n",
      "cnt: 0 - valLoss: 0.6829879283905029 - trainLoss: 0.682753324508667\n",
      "cnt: 0 - valLoss: 0.6829689741134644 - trainLoss: 0.682733416557312\n",
      "cnt: 0 - valLoss: 0.6829500794410706 - trainLoss: 0.682713508605957\n",
      "cnt: 0 - valLoss: 0.682931125164032 - trainLoss: 0.6826935410499573\n",
      "cnt: 0 - valLoss: 0.6829122304916382 - trainLoss: 0.6826737523078918\n",
      "cnt: 0 - valLoss: 0.6828933358192444 - trainLoss: 0.6826539039611816\n",
      "cnt: 0 - valLoss: 0.6828745603561401 - trainLoss: 0.6826340556144714\n",
      "cnt: 0 - valLoss: 0.6828557848930359 - trainLoss: 0.682614266872406\n",
      "cnt: 0 - valLoss: 0.6828370094299316 - trainLoss: 0.6825944781303406\n",
      "cnt: 0 - valLoss: 0.6828182339668274 - trainLoss: 0.6825748085975647\n",
      "cnt: 0 - valLoss: 0.6827994585037231 - trainLoss: 0.6825550198554993\n",
      "cnt: 0 - valLoss: 0.6827807426452637 - trainLoss: 0.6825353503227234\n",
      "cnt: 0 - valLoss: 0.682762086391449 - trainLoss: 0.6825156807899475\n",
      "cnt: 0 - valLoss: 0.6827433705329895 - trainLoss: 0.6824960112571716\n",
      "cnt: 0 - valLoss: 0.6827247142791748 - trainLoss: 0.6824763417243958\n",
      "cnt: 0 - valLoss: 0.6827060580253601 - trainLoss: 0.6824567914009094\n",
      "cnt: 0 - valLoss: 0.6826874613761902 - trainLoss: 0.6824371814727783\n",
      "cnt: 0 - valLoss: 0.6826688647270203 - trainLoss: 0.682417631149292\n",
      "cnt: 0 - valLoss: 0.6826502680778503 - trainLoss: 0.6823980808258057\n",
      "cnt: 0 - valLoss: 0.6826317310333252 - trainLoss: 0.6823785901069641\n",
      "cnt: 0 - valLoss: 0.6826132535934448 - trainLoss: 0.6823590993881226\n",
      "cnt: 0 - valLoss: 0.6825947165489197 - trainLoss: 0.6823395490646362\n",
      "cnt: 0 - valLoss: 0.6825762987136841 - trainLoss: 0.6823201179504395\n",
      "cnt: 0 - valLoss: 0.6825578808784485 - trainLoss: 0.6823006868362427\n",
      "cnt: 0 - valLoss: 0.6825393438339233 - trainLoss: 0.6822812557220459\n",
      "cnt: 0 - valLoss: 0.6825209259986877 - trainLoss: 0.6822618246078491\n",
      "cnt: 0 - valLoss: 0.6825025677680969 - trainLoss: 0.6822425127029419\n",
      "cnt: 0 - valLoss: 0.6824842095375061 - trainLoss: 0.6822232007980347\n",
      "cnt: 0 - valLoss: 0.6824658513069153 - trainLoss: 0.6822038888931274\n",
      "cnt: 0 - valLoss: 0.6824475526809692 - trainLoss: 0.6821845769882202\n",
      "cnt: 0 - valLoss: 0.6824291944503784 - trainLoss: 0.682165265083313\n",
      "cnt: 0 - valLoss: 0.6824109554290771 - trainLoss: 0.6821460127830505\n",
      "cnt: 0 - valLoss: 0.6823926568031311 - trainLoss: 0.6821268200874329\n",
      "cnt: 0 - valLoss: 0.6823744177818298 - trainLoss: 0.6821075677871704\n",
      "cnt: 0 - valLoss: 0.6823561787605286 - trainLoss: 0.682088315486908\n",
      "cnt: 0 - valLoss: 0.6823379993438721 - trainLoss: 0.6820691823959351\n",
      "cnt: 0 - valLoss: 0.6823198199272156 - trainLoss: 0.6820500493049622\n",
      "cnt: 0 - valLoss: 0.6823016405105591 - trainLoss: 0.6820309162139893\n",
      "cnt: 0 - valLoss: 0.6822834610939026 - trainLoss: 0.6820118427276611\n",
      "cnt: 0 - valLoss: 0.6822654008865356 - trainLoss: 0.6819927096366882\n",
      "cnt: 0 - valLoss: 0.6822472214698792 - trainLoss: 0.6819736361503601\n",
      "cnt: 0 - valLoss: 0.682229220867157 - trainLoss: 0.6819546222686768\n",
      "cnt: 0 - valLoss: 0.6822112202644348 - trainLoss: 0.6819356083869934\n",
      "cnt: 0 - valLoss: 0.6821931004524231 - trainLoss: 0.6819165945053101\n",
      "cnt: 0 - valLoss: 0.6821750998497009 - trainLoss: 0.6818975806236267\n",
      "cnt: 0 - valLoss: 0.682157039642334 - trainLoss: 0.6818786859512329\n",
      "cnt: 0 - valLoss: 0.6821390986442566 - trainLoss: 0.6818597316741943\n",
      "cnt: 0 - valLoss: 0.6821210980415344 - trainLoss: 0.6818408370018005\n",
      "cnt: 0 - valLoss: 0.6821032762527466 - trainLoss: 0.681821882724762\n",
      "cnt: 0 - valLoss: 0.6820852756500244 - trainLoss: 0.6818029880523682\n",
      "cnt: 0 - valLoss: 0.6820674538612366 - trainLoss: 0.6817841529846191\n",
      "cnt: 0 - valLoss: 0.6820495128631592 - trainLoss: 0.6817653775215149\n",
      "cnt: 0 - valLoss: 0.6820316910743713 - trainLoss: 0.6817465424537659\n",
      "cnt: 0 - valLoss: 0.6820138692855835 - trainLoss: 0.6817277669906616\n",
      "cnt: 0 - valLoss: 0.6819961071014404 - trainLoss: 0.6817090511322021\n",
      "cnt: 0 - valLoss: 0.6819782853126526 - trainLoss: 0.6816902756690979\n",
      "cnt: 0 - valLoss: 0.6819605231285095 - trainLoss: 0.6816715598106384\n",
      "cnt: 0 - valLoss: 0.6819428205490112 - trainLoss: 0.681652843952179\n",
      "cnt: 0 - valLoss: 0.6819250583648682 - trainLoss: 0.6816341876983643\n",
      "cnt: 0 - valLoss: 0.6819074153900146 - trainLoss: 0.6816155314445496\n",
      "cnt: 0 - valLoss: 0.6818896532058716 - trainLoss: 0.6815969347953796\n",
      "cnt: 0 - valLoss: 0.6818720698356628 - trainLoss: 0.6815782785415649\n",
      "cnt: 0 - valLoss: 0.6818543076515198 - trainLoss: 0.681559681892395\n",
      "cnt: 0 - valLoss: 0.681836724281311 - trainLoss: 0.6815411448478699\n",
      "cnt: 0 - valLoss: 0.6818190813064575 - trainLoss: 0.6815226078033447\n",
      "cnt: 0 - valLoss: 0.6818015575408936 - trainLoss: 0.6815040707588196\n",
      "cnt: 0 - valLoss: 0.6817839741706848 - trainLoss: 0.6814855337142944\n",
      "cnt: 0 - valLoss: 0.6817664504051208 - trainLoss: 0.6814669966697693\n",
      "cnt: 0 - valLoss: 0.6817489862442017 - trainLoss: 0.6814486384391785\n",
      "cnt: 0 - valLoss: 0.6817314028739929 - trainLoss: 0.6814301013946533\n",
      "cnt: 0 - valLoss: 0.6817139983177185 - trainLoss: 0.6814117431640625\n",
      "cnt: 0 - valLoss: 0.6816965341567993 - trainLoss: 0.6813933253288269\n",
      "cnt: 0 - valLoss: 0.6816790699958801 - trainLoss: 0.6813749074935913\n",
      "cnt: 0 - valLoss: 0.6816616058349609 - trainLoss: 0.6813566088676453\n",
      "cnt: 0 - valLoss: 0.6816442012786865 - trainLoss: 0.6813382506370544\n",
      "cnt: 0 - valLoss: 0.6816267967224121 - trainLoss: 0.6813199520111084\n",
      "cnt: 0 - valLoss: 0.6816095113754272 - trainLoss: 0.6813016533851624\n",
      "cnt: 0 - valLoss: 0.6815921664237976 - trainLoss: 0.6812833547592163\n",
      "cnt: 0 - valLoss: 0.681574821472168 - trainLoss: 0.6812650561332703\n",
      "cnt: 0 - valLoss: 0.6815574765205383 - trainLoss: 0.6812468767166138\n",
      "cnt: 0 - valLoss: 0.6815402507781982 - trainLoss: 0.6812286376953125\n",
      "cnt: 0 - valLoss: 0.6815229654312134 - trainLoss: 0.6812103986740112\n",
      "cnt: 0 - valLoss: 0.6815056800842285 - trainLoss: 0.6811922192573547\n",
      "cnt: 0 - valLoss: 0.6814884543418884 - trainLoss: 0.681174099445343\n",
      "cnt: 0 - valLoss: 0.6814713478088379 - trainLoss: 0.6811559200286865\n",
      "cnt: 0 - valLoss: 0.6814541816711426 - trainLoss: 0.6811378598213196\n",
      "cnt: 0 - valLoss: 0.6814369559288025 - trainLoss: 0.6811197400093079\n",
      "cnt: 0 - valLoss: 0.681419849395752 - trainLoss: 0.6811016798019409\n",
      "cnt: 0 - valLoss: 0.6814026832580566 - trainLoss: 0.681083619594574\n",
      "cnt: 0 - valLoss: 0.6813855767250061 - trainLoss: 0.681065559387207\n",
      "cnt: 0 - valLoss: 0.6813685297966003 - trainLoss: 0.6810475587844849\n",
      "cnt: 0 - valLoss: 0.6813514232635498 - trainLoss: 0.6810296177864075\n",
      "cnt: 0 - valLoss: 0.6813344359397888 - trainLoss: 0.6810116171836853\n",
      "cnt: 0 - valLoss: 0.6813173890113831 - trainLoss: 0.6809936165809631\n",
      "cnt: 0 - valLoss: 0.6813004016876221 - trainLoss: 0.6809756755828857\n",
      "cnt: 0 - valLoss: 0.6812833547592163 - trainLoss: 0.6809577941894531\n",
      "cnt: 0 - valLoss: 0.6812664270401001 - trainLoss: 0.6809398531913757\n",
      "cnt: 0 - valLoss: 0.6812494397163391 - trainLoss: 0.6809220910072327\n",
      "cnt: 0 - valLoss: 0.6812325119972229 - trainLoss: 0.6809041500091553\n",
      "cnt: 0 - valLoss: 0.6812156438827515 - trainLoss: 0.6808862686157227\n",
      "cnt: 0 - valLoss: 0.6811988353729248 - trainLoss: 0.6808685064315796\n",
      "cnt: 0 - valLoss: 0.6811819076538086 - trainLoss: 0.6808507442474365\n",
      "cnt: 0 - valLoss: 0.6811650991439819 - trainLoss: 0.6808329820632935\n",
      "cnt: 0 - valLoss: 0.6811482310295105 - trainLoss: 0.6808152198791504\n",
      "cnt: 0 - valLoss: 0.6811314225196838 - trainLoss: 0.6807974576950073\n",
      "cnt: 0 - valLoss: 0.6811146140098572 - trainLoss: 0.680779755115509\n",
      "cnt: 0 - valLoss: 0.6810978651046753 - trainLoss: 0.6807620525360107\n",
      "cnt: 0 - valLoss: 0.6810811161994934 - trainLoss: 0.680744469165802\n",
      "cnt: 0 - valLoss: 0.6810643672943115 - trainLoss: 0.6807267069816589\n",
      "cnt: 0 - valLoss: 0.6810476779937744 - trainLoss: 0.6807090640068054\n",
      "cnt: 0 - valLoss: 0.6810309886932373 - trainLoss: 0.6806914806365967\n",
      "cnt: 0 - valLoss: 0.6810142993927002 - trainLoss: 0.6806738376617432\n",
      "cnt: 0 - valLoss: 0.6809976696968079 - trainLoss: 0.6806562542915344\n",
      "cnt: 0 - valLoss: 0.6809810400009155 - trainLoss: 0.6806386709213257\n",
      "cnt: 0 - valLoss: 0.6809643507003784 - trainLoss: 0.6806211471557617\n",
      "cnt: 0 - valLoss: 0.6809477806091309 - trainLoss: 0.6806036829948425\n",
      "cnt: 0 - valLoss: 0.6809312105178833 - trainLoss: 0.6805861592292786\n",
      "cnt: 0 - valLoss: 0.6809146404266357 - trainLoss: 0.6805686354637146\n",
      "cnt: 0 - valLoss: 0.6808980703353882 - trainLoss: 0.6805511713027954\n",
      "cnt: 0 - valLoss: 0.6808815598487854 - trainLoss: 0.680533766746521\n",
      "cnt: 0 - valLoss: 0.6808651089668274 - trainLoss: 0.6805163025856018\n",
      "cnt: 0 - valLoss: 0.6808485984802246 - trainLoss: 0.6804989576339722\n",
      "cnt: 0 - valLoss: 0.6808320879936218 - trainLoss: 0.6804815530776978\n",
      "cnt: 0 - valLoss: 0.6808156371116638 - trainLoss: 0.6804641485214233\n",
      "cnt: 0 - valLoss: 0.6807993054389954 - trainLoss: 0.6804468035697937\n",
      "cnt: 0 - valLoss: 0.6807828545570374 - trainLoss: 0.6804294586181641\n",
      "cnt: 0 - valLoss: 0.6807665228843689 - trainLoss: 0.6804121732711792\n",
      "cnt: 0 - valLoss: 0.6807500720024109 - trainLoss: 0.6803949475288391\n",
      "cnt: 0 - valLoss: 0.6807337403297424 - trainLoss: 0.6803776621818542\n",
      "cnt: 0 - valLoss: 0.680717408657074 - trainLoss: 0.6803603768348694\n",
      "cnt: 0 - valLoss: 0.6807010769844055 - trainLoss: 0.6803431510925293\n",
      "cnt: 0 - valLoss: 0.6806848049163818 - trainLoss: 0.6803259253501892\n",
      "cnt: 0 - valLoss: 0.6806684732437134 - trainLoss: 0.6803087592124939\n",
      "cnt: 0 - valLoss: 0.6806522011756897 - trainLoss: 0.6802915930747986\n",
      "cnt: 0 - valLoss: 0.6806359887123108 - trainLoss: 0.6802744269371033\n",
      "cnt: 0 - valLoss: 0.6806197166442871 - trainLoss: 0.680257260799408\n",
      "cnt: 0 - valLoss: 0.680603563785553 - trainLoss: 0.6802402138710022\n",
      "cnt: 0 - valLoss: 0.6805874109268188 - trainLoss: 0.6802230477333069\n",
      "cnt: 0 - valLoss: 0.6805711984634399 - trainLoss: 0.6802059412002563\n",
      "cnt: 0 - valLoss: 0.6805551052093506 - trainLoss: 0.6801888942718506\n",
      "cnt: 0 - valLoss: 0.6805389523506165 - trainLoss: 0.6801719069480896\n",
      "cnt: 0 - valLoss: 0.6805228590965271 - trainLoss: 0.6801549196243286\n",
      "cnt: 0 - valLoss: 0.680506706237793 - trainLoss: 0.6801378726959229\n",
      "cnt: 0 - valLoss: 0.6804907321929932 - trainLoss: 0.6801208853721619\n",
      "cnt: 0 - valLoss: 0.6804746389389038 - trainLoss: 0.6801039576530457\n",
      "cnt: 0 - valLoss: 0.6804586052894592 - trainLoss: 0.6800870299339294\n",
      "cnt: 0 - valLoss: 0.6804426312446594 - trainLoss: 0.680070161819458\n",
      "cnt: 0 - valLoss: 0.6804266571998596 - trainLoss: 0.6800532341003418\n",
      "cnt: 0 - valLoss: 0.6804106831550598 - trainLoss: 0.6800363659858704\n",
      "cnt: 0 - valLoss: 0.68039470911026 - trainLoss: 0.6800194978713989\n",
      "cnt: 0 - valLoss: 0.6803787350654602 - trainLoss: 0.6800026893615723\n",
      "cnt: 0 - valLoss: 0.6803628206253052 - trainLoss: 0.6799858808517456\n",
      "cnt: 0 - valLoss: 0.6803469061851501 - trainLoss: 0.679969072341919\n",
      "cnt: 0 - valLoss: 0.6803310513496399 - trainLoss: 0.6799523830413818\n",
      "cnt: 0 - valLoss: 0.6803152561187744 - trainLoss: 0.6799355745315552\n",
      "cnt: 0 - valLoss: 0.6802994012832642 - trainLoss: 0.6799188256263733\n",
      "cnt: 0 - valLoss: 0.6802835464477539 - trainLoss: 0.6799021363258362\n",
      "cnt: 0 - valLoss: 0.6802677512168884 - trainLoss: 0.6798854470252991\n",
      "cnt: 0 - valLoss: 0.6802520155906677 - trainLoss: 0.6798686981201172\n",
      "cnt: 0 - valLoss: 0.6802361011505127 - trainLoss: 0.6798521280288696\n",
      "cnt: 0 - valLoss: 0.6802204251289368 - trainLoss: 0.6798354983329773\n",
      "cnt: 0 - valLoss: 0.6802046895027161 - trainLoss: 0.6798188090324402\n",
      "cnt: 0 - valLoss: 0.6801889538764954 - trainLoss: 0.6798022389411926\n",
      "cnt: 0 - valLoss: 0.6801732778549194 - trainLoss: 0.6797856688499451\n",
      "cnt: 0 - valLoss: 0.6801576018333435 - trainLoss: 0.6797690987586975\n",
      "cnt: 0 - valLoss: 0.6801419258117676 - trainLoss: 0.6797525882720947\n",
      "cnt: 0 - valLoss: 0.6801263093948364 - trainLoss: 0.6797360181808472\n",
      "cnt: 0 - valLoss: 0.6801106929779053 - trainLoss: 0.6797195672988892\n",
      "cnt: 0 - valLoss: 0.6800950765609741 - trainLoss: 0.6797030568122864\n",
      "cnt: 0 - valLoss: 0.680079460144043 - trainLoss: 0.6796865463256836\n",
      "cnt: 0 - valLoss: 0.6800639033317566 - trainLoss: 0.6796700954437256\n",
      "cnt: 0 - valLoss: 0.6800483465194702 - trainLoss: 0.6796537041664124\n",
      "cnt: 0 - valLoss: 0.6800328493118286 - trainLoss: 0.6796372532844543\n",
      "cnt: 0 - valLoss: 0.680017352104187 - trainLoss: 0.6796208620071411\n",
      "cnt: 0 - valLoss: 0.6800017952919006 - trainLoss: 0.6796045303344727\n",
      "cnt: 0 - valLoss: 0.6799863576889038 - trainLoss: 0.6795881390571594\n",
      "cnt: 0 - valLoss: 0.679970920085907 - trainLoss: 0.6795717477798462\n",
      "cnt: 0 - valLoss: 0.6799554228782654 - trainLoss: 0.6795554757118225\n",
      "cnt: 0 - valLoss: 0.6799400448799133 - trainLoss: 0.679539144039154\n",
      "cnt: 0 - valLoss: 0.6799246072769165 - trainLoss: 0.6795229315757751\n",
      "cnt: 0 - valLoss: 0.6799092888832092 - trainLoss: 0.6795065999031067\n",
      "cnt: 0 - valLoss: 0.6798938512802124 - trainLoss: 0.679490327835083\n",
      "cnt: 0 - valLoss: 0.6798784732818604 - trainLoss: 0.6794741749763489\n",
      "cnt: 0 - valLoss: 0.6798631548881531 - trainLoss: 0.6794579029083252\n",
      "cnt: 0 - valLoss: 0.679847776889801 - trainLoss: 0.6794417500495911\n",
      "cnt: 0 - valLoss: 0.6798325181007385 - trainLoss: 0.6794255375862122\n",
      "cnt: 0 - valLoss: 0.679817259311676 - trainLoss: 0.679409384727478\n",
      "cnt: 0 - valLoss: 0.6798019409179688 - trainLoss: 0.6793931722640991\n",
      "cnt: 0 - valLoss: 0.6797866821289062 - trainLoss: 0.6793770790100098\n",
      "cnt: 0 - valLoss: 0.6797714829444885 - trainLoss: 0.6793609857559204\n",
      "cnt: 0 - valLoss: 0.679756224155426 - trainLoss: 0.679344892501831\n",
      "cnt: 0 - valLoss: 0.6797411441802979 - trainLoss: 0.6793289184570312\n",
      "cnt: 0 - valLoss: 0.6797258853912354 - trainLoss: 0.6793128848075867\n",
      "cnt: 0 - valLoss: 0.6797107458114624 - trainLoss: 0.6792967915534973\n",
      "cnt: 0 - valLoss: 0.6796956658363342 - trainLoss: 0.6792808175086975\n",
      "cnt: 0 - valLoss: 0.6796804666519165 - trainLoss: 0.6792647838592529\n",
      "cnt: 0 - valLoss: 0.6796653866767883 - trainLoss: 0.6792488098144531\n",
      "cnt: 0 - valLoss: 0.6796502470970154 - trainLoss: 0.6792328357696533\n",
      "cnt: 0 - valLoss: 0.679635226726532 - trainLoss: 0.6792169213294983\n",
      "cnt: 0 - valLoss: 0.6796201467514038 - trainLoss: 0.6792010068893433\n",
      "cnt: 0 - valLoss: 0.6796051263809204 - trainLoss: 0.6791850924491882\n",
      "cnt: 0 - valLoss: 0.679590106010437 - trainLoss: 0.679169237613678\n",
      "cnt: 0 - valLoss: 0.6795751452445984 - trainLoss: 0.679153323173523\n",
      "cnt: 0 - valLoss: 0.6795600652694702 - trainLoss: 0.6791375279426575\n",
      "cnt: 0 - valLoss: 0.6795451641082764 - trainLoss: 0.679121732711792\n",
      "cnt: 0 - valLoss: 0.6795302033424377 - trainLoss: 0.679105818271637\n",
      "cnt: 0 - valLoss: 0.6795153021812439 - trainLoss: 0.6790900826454163\n",
      "cnt: 0 - valLoss: 0.6795003414154053 - trainLoss: 0.6790743470191956\n",
      "cnt: 0 - valLoss: 0.6794854402542114 - trainLoss: 0.6790585517883301\n",
      "cnt: 0 - valLoss: 0.6794705986976624 - trainLoss: 0.6790428161621094\n",
      "cnt: 0 - valLoss: 0.6794556975364685 - trainLoss: 0.6790270805358887\n",
      "cnt: 0 - valLoss: 0.6794408559799194 - trainLoss: 0.6790114045143127\n",
      "cnt: 0 - valLoss: 0.6794260144233704 - trainLoss: 0.678995668888092\n",
      "cnt: 0 - valLoss: 0.6794112324714661 - trainLoss: 0.6789799928665161\n",
      "cnt: 0 - valLoss: 0.6793964505195618 - trainLoss: 0.6789644360542297\n",
      "cnt: 0 - valLoss: 0.6793816089630127 - trainLoss: 0.6789487600326538\n",
      "cnt: 0 - valLoss: 0.6793668270111084 - trainLoss: 0.6789331436157227\n",
      "cnt: 0 - valLoss: 0.6793521642684937 - trainLoss: 0.6789175271987915\n",
      "cnt: 0 - valLoss: 0.6793373823165894 - trainLoss: 0.6789019703865051\n",
      "cnt: 0 - valLoss: 0.6793227195739746 - trainLoss: 0.678886353969574\n",
      "cnt: 0 - valLoss: 0.6793080568313599 - trainLoss: 0.6788708567619324\n",
      "cnt: 0 - valLoss: 0.6792933344841003 - trainLoss: 0.678855299949646\n",
      "cnt: 0 - valLoss: 0.6792786717414856 - trainLoss: 0.6788398027420044\n",
      "cnt: 0 - valLoss: 0.6792640686035156 - trainLoss: 0.6788243055343628\n",
      "cnt: 0 - valLoss: 0.6792494654655457 - trainLoss: 0.678808867931366\n",
      "cnt: 0 - valLoss: 0.6792348027229309 - trainLoss: 0.6787933707237244\n",
      "cnt: 0 - valLoss: 0.6792201995849609 - trainLoss: 0.6787779927253723\n",
      "cnt: 0 - valLoss: 0.6792056560516357 - trainLoss: 0.6787624955177307\n",
      "cnt: 0 - valLoss: 0.6791910529136658 - trainLoss: 0.6787471771240234\n",
      "cnt: 0 - valLoss: 0.6791765689849854 - trainLoss: 0.6787316799163818\n",
      "cnt: 0 - valLoss: 0.6791620850563049 - trainLoss: 0.6787163615226746\n",
      "cnt: 0 - valLoss: 0.6791475415229797 - trainLoss: 0.6787009835243225\n",
      "cnt: 0 - valLoss: 0.6791330575942993 - trainLoss: 0.6786856651306152\n",
      "cnt: 0 - valLoss: 0.6791185736656189 - trainLoss: 0.678670346736908\n",
      "cnt: 0 - valLoss: 0.6791041493415833 - trainLoss: 0.6786550879478455\n",
      "cnt: 0 - valLoss: 0.6790897250175476 - trainLoss: 0.678639829158783\n",
      "cnt: 0 - valLoss: 0.6790753602981567 - trainLoss: 0.6786245107650757\n",
      "cnt: 0 - valLoss: 0.6790608763694763 - trainLoss: 0.6786092519760132\n",
      "cnt: 0 - valLoss: 0.6790465712547302 - trainLoss: 0.6785939931869507\n",
      "cnt: 0 - valLoss: 0.6790321469306946 - trainLoss: 0.678578794002533\n",
      "cnt: 0 - valLoss: 0.6790178418159485 - trainLoss: 0.6785635948181152\n",
      "cnt: 0 - valLoss: 0.6790034770965576 - trainLoss: 0.6785484552383423\n",
      "cnt: 0 - valLoss: 0.6789891719818115 - trainLoss: 0.6785333156585693\n",
      "cnt: 0 - valLoss: 0.6789748668670654 - trainLoss: 0.6785181164741516\n",
      "cnt: 0 - valLoss: 0.6789605617523193 - trainLoss: 0.6785030364990234\n",
      "cnt: 0 - valLoss: 0.678946316242218 - trainLoss: 0.6784878373146057\n",
      "cnt: 0 - valLoss: 0.6789321303367615 - trainLoss: 0.6784728169441223\n",
      "cnt: 0 - valLoss: 0.6789178848266602 - trainLoss: 0.6784576773643494\n",
      "cnt: 0 - valLoss: 0.6789036393165588 - trainLoss: 0.6784427165985107\n",
      "cnt: 0 - valLoss: 0.6788893938064575 - trainLoss: 0.6784276366233826\n",
      "cnt: 0 - valLoss: 0.6788752675056458 - trainLoss: 0.6784126162528992\n",
      "cnt: 0 - valLoss: 0.678861141204834 - trainLoss: 0.678397536277771\n",
      "cnt: 0 - valLoss: 0.6788469552993774 - trainLoss: 0.6783825755119324\n",
      "cnt: 0 - valLoss: 0.6788328289985657 - trainLoss: 0.6783676147460938\n",
      "cnt: 0 - valLoss: 0.6788187623023987 - trainLoss: 0.6783527135848999\n",
      "cnt: 0 - valLoss: 0.6788045763969421 - trainLoss: 0.6783377528190613\n",
      "cnt: 0 - valLoss: 0.6787905097007751 - trainLoss: 0.6783227920532227\n",
      "cnt: 0 - valLoss: 0.6787764430046082 - trainLoss: 0.6783078908920288\n",
      "cnt: 0 - valLoss: 0.6787623763084412 - trainLoss: 0.6782930493354797\n",
      "cnt: 0 - valLoss: 0.6787483096122742 - trainLoss: 0.6782781481742859\n",
      "cnt: 0 - valLoss: 0.6787343621253967 - trainLoss: 0.678263247013092\n",
      "cnt: 0 - valLoss: 0.6787202954292297 - trainLoss: 0.678248405456543\n",
      "cnt: 0 - valLoss: 0.6787064075469971 - trainLoss: 0.6782336235046387\n",
      "cnt: 0 - valLoss: 0.6786924004554749 - trainLoss: 0.6782187819480896\n",
      "cnt: 0 - valLoss: 0.6786783337593079 - trainLoss: 0.6782039999961853\n",
      "cnt: 0 - valLoss: 0.6786644458770752 - trainLoss: 0.678189218044281\n",
      "cnt: 0 - valLoss: 0.6786505579948425 - trainLoss: 0.6781744360923767\n",
      "cnt: 0 - valLoss: 0.6786367297172546 - trainLoss: 0.6781597137451172\n",
      "cnt: 0 - valLoss: 0.6786227226257324 - trainLoss: 0.6781449913978577\n",
      "cnt: 0 - valLoss: 0.6786088943481445 - trainLoss: 0.6781302690505981\n",
      "cnt: 0 - valLoss: 0.6785950660705566 - trainLoss: 0.6781156659126282\n",
      "cnt: 0 - valLoss: 0.678581178188324 - trainLoss: 0.6781009435653687\n",
      "cnt: 0 - valLoss: 0.6785674095153809 - trainLoss: 0.6780862808227539\n",
      "cnt: 0 - valLoss: 0.6785535216331482 - trainLoss: 0.6780716776847839\n",
      "cnt: 0 - valLoss: 0.6785397529602051 - trainLoss: 0.6780570149421692\n",
      "cnt: 0 - valLoss: 0.6785259246826172 - trainLoss: 0.6780423521995544\n",
      "cnt: 0 - valLoss: 0.6785122156143188 - trainLoss: 0.6780278086662292\n",
      "cnt: 0 - valLoss: 0.6784984469413757 - trainLoss: 0.6780132055282593\n",
      "cnt: 0 - valLoss: 0.6784847378730774 - trainLoss: 0.6779987215995789\n",
      "cnt: 0 - valLoss: 0.6784709692001343 - trainLoss: 0.6779840588569641\n",
      "cnt: 0 - valLoss: 0.6784573197364807 - trainLoss: 0.6779696345329285\n",
      "cnt: 0 - valLoss: 0.6784436702728271 - trainLoss: 0.6779550313949585\n",
      "cnt: 0 - valLoss: 0.6784300208091736 - trainLoss: 0.6779406070709229\n",
      "cnt: 0 - valLoss: 0.6784163117408752 - trainLoss: 0.6779261231422424\n",
      "cnt: 0 - valLoss: 0.6784027218818665 - trainLoss: 0.677911639213562\n",
      "cnt: 0 - valLoss: 0.6783890724182129 - trainLoss: 0.6778972148895264\n",
      "cnt: 0 - valLoss: 0.6783754229545593 - trainLoss: 0.6778827905654907\n",
      "cnt: 0 - valLoss: 0.6783618927001953 - trainLoss: 0.6778683662414551\n",
      "cnt: 0 - valLoss: 0.6783483028411865 - trainLoss: 0.6778539419174194\n",
      "cnt: 0 - valLoss: 0.6783347129821777 - trainLoss: 0.6778395771980286\n",
      "cnt: 0 - valLoss: 0.6783212423324585 - trainLoss: 0.6778252124786377\n",
      "cnt: 0 - valLoss: 0.6783075928688049 - trainLoss: 0.6778108477592468\n",
      "cnt: 0 - valLoss: 0.6782941818237305 - trainLoss: 0.6777965426445007\n",
      "cnt: 0 - valLoss: 0.6782805919647217 - trainLoss: 0.6777822375297546\n",
      "cnt: 0 - valLoss: 0.6782671213150024 - trainLoss: 0.6777679324150085\n",
      "cnt: 0 - valLoss: 0.678253710269928 - trainLoss: 0.6777536273002625\n",
      "cnt: 0 - valLoss: 0.6782402992248535 - trainLoss: 0.6777393221855164\n",
      "cnt: 0 - valLoss: 0.6782268285751343 - trainLoss: 0.6777250170707703\n",
      "cnt: 0 - valLoss: 0.6782134175300598 - trainLoss: 0.6777108311653137\n",
      "cnt: 0 - valLoss: 0.6782000064849854 - trainLoss: 0.6776966452598572\n",
      "cnt: 0 - valLoss: 0.6781866550445557 - trainLoss: 0.6776823997497559\n",
      "cnt: 0 - valLoss: 0.6781732439994812 - trainLoss: 0.6776682138442993\n",
      "cnt: 0 - valLoss: 0.6781598925590515 - trainLoss: 0.6776540875434875\n",
      "cnt: 0 - valLoss: 0.6781465411186218 - trainLoss: 0.6776399612426758\n",
      "cnt: 0 - valLoss: 0.6781332492828369 - trainLoss: 0.6776257157325745\n",
      "cnt: 0 - valLoss: 0.6781198978424072 - trainLoss: 0.6776116490364075\n",
      "cnt: 0 - valLoss: 0.6781066060066223 - trainLoss: 0.6775975227355957\n",
      "cnt: 0 - valLoss: 0.6780933141708374 - trainLoss: 0.6775834560394287\n",
      "cnt: 0 - valLoss: 0.6780800819396973 - trainLoss: 0.6775693893432617\n",
      "cnt: 0 - valLoss: 0.6780668497085571 - trainLoss: 0.6775553226470947\n",
      "cnt: 0 - valLoss: 0.678053617477417 - trainLoss: 0.6775412559509277\n",
      "cnt: 0 - valLoss: 0.6780403852462769 - trainLoss: 0.6775271892547607\n",
      "cnt: 0 - valLoss: 0.6780272126197815 - trainLoss: 0.6775132417678833\n",
      "cnt: 0 - valLoss: 0.6780140399932861 - trainLoss: 0.6774992346763611\n",
      "cnt: 0 - valLoss: 0.6780008673667908 - trainLoss: 0.6774852871894836\n",
      "cnt: 0 - valLoss: 0.6779876947402954 - trainLoss: 0.6774712800979614\n",
      "cnt: 0 - valLoss: 0.6779745221138 - trainLoss: 0.677457332611084\n",
      "cnt: 0 - valLoss: 0.6779614686965942 - trainLoss: 0.6774433851242065\n",
      "cnt: 0 - valLoss: 0.6779482960700989 - trainLoss: 0.6774294376373291\n",
      "cnt: 0 - valLoss: 0.6779352426528931 - trainLoss: 0.6774156093597412\n",
      "cnt: 0 - valLoss: 0.6779221296310425 - trainLoss: 0.6774016618728638\n",
      "cnt: 0 - valLoss: 0.6779090762138367 - trainLoss: 0.6773878335952759\n",
      "cnt: 0 - valLoss: 0.6778960227966309 - trainLoss: 0.6773738861083984\n",
      "cnt: 0 - valLoss: 0.6778829097747803 - trainLoss: 0.6773600578308105\n",
      "cnt: 0 - valLoss: 0.677869975566864 - trainLoss: 0.6773462891578674\n",
      "cnt: 0 - valLoss: 0.677856981754303 - trainLoss: 0.6773324608802795\n",
      "cnt: 0 - valLoss: 0.6778439879417419 - trainLoss: 0.6773185729980469\n",
      "cnt: 0 - valLoss: 0.6778310537338257 - trainLoss: 0.6773048639297485\n",
      "cnt: 0 - valLoss: 0.6778180599212646 - trainLoss: 0.6772910952568054\n",
      "cnt: 0 - valLoss: 0.6778051257133484 - trainLoss: 0.6772773265838623\n",
      "cnt: 0 - valLoss: 0.6777921319007874 - trainLoss: 0.6772636771202087\n",
      "cnt: 0 - valLoss: 0.6777792572975159 - trainLoss: 0.6772499084472656\n",
      "cnt: 0 - valLoss: 0.6777663230895996 - trainLoss: 0.6772361397743225\n",
      "cnt: 0 - valLoss: 0.6777535080909729 - trainLoss: 0.677222490310669\n",
      "cnt: 0 - valLoss: 0.6777405738830566 - trainLoss: 0.6772088408470154\n",
      "cnt: 0 - valLoss: 0.6777277588844299 - trainLoss: 0.6771951913833618\n",
      "cnt: 0 - valLoss: 0.6777148842811584 - trainLoss: 0.6771815419197083\n",
      "cnt: 0 - valLoss: 0.6777021288871765 - trainLoss: 0.6771679520606995\n",
      "cnt: 0 - valLoss: 0.6776893138885498 - trainLoss: 0.6771542429924011\n",
      "cnt: 0 - valLoss: 0.6776764392852783 - trainLoss: 0.6771406531333923\n",
      "cnt: 0 - valLoss: 0.6776637434959412 - trainLoss: 0.6771271228790283\n",
      "cnt: 0 - valLoss: 0.6776509284973145 - trainLoss: 0.6771134734153748\n",
      "cnt: 0 - valLoss: 0.6776381731033325 - trainLoss: 0.6771000623703003\n",
      "cnt: 0 - valLoss: 0.6776254177093506 - trainLoss: 0.6770864725112915\n",
      "cnt: 0 - valLoss: 0.6776127219200134 - trainLoss: 0.6770729422569275\n",
      "cnt: 0 - valLoss: 0.6776000261306763 - trainLoss: 0.6770594716072083\n",
      "cnt: 0 - valLoss: 0.6775873303413391 - trainLoss: 0.6770459413528442\n",
      "cnt: 0 - valLoss: 0.6775746941566467 - trainLoss: 0.677032470703125\n",
      "cnt: 0 - valLoss: 0.6775619983673096 - trainLoss: 0.6770190000534058\n",
      "cnt: 0 - valLoss: 0.6775493025779724 - trainLoss: 0.6770055294036865\n",
      "cnt: 0 - valLoss: 0.6775367259979248 - trainLoss: 0.6769921779632568\n",
      "cnt: 0 - valLoss: 0.6775240898132324 - trainLoss: 0.6769787669181824\n",
      "cnt: 0 - valLoss: 0.67751145362854 - trainLoss: 0.6769653558731079\n",
      "cnt: 0 - valLoss: 0.6774988770484924 - trainLoss: 0.6769519448280334\n",
      "cnt: 0 - valLoss: 0.6774863004684448 - trainLoss: 0.6769385933876038\n",
      "cnt: 0 - valLoss: 0.6774737238883972 - trainLoss: 0.6769252419471741\n",
      "cnt: 0 - valLoss: 0.6774611473083496 - trainLoss: 0.6769118905067444\n",
      "cnt: 0 - valLoss: 0.6774486899375916 - trainLoss: 0.6768985390663147\n",
      "cnt: 0 - valLoss: 0.677436113357544 - trainLoss: 0.6768852472305298\n",
      "cnt: 0 - valLoss: 0.6774236559867859 - trainLoss: 0.6768718957901001\n",
      "cnt: 0 - valLoss: 0.6774111390113831 - trainLoss: 0.67685866355896\n",
      "cnt: 0 - valLoss: 0.677398681640625 - trainLoss: 0.6768454313278198\n",
      "cnt: 0 - valLoss: 0.6773862242698669 - trainLoss: 0.6768321990966797\n",
      "cnt: 0 - valLoss: 0.6773738265037537 - trainLoss: 0.6768189072608948\n",
      "cnt: 0 - valLoss: 0.6773613691329956 - trainLoss: 0.6768057346343994\n",
      "cnt: 0 - valLoss: 0.6773489713668823 - trainLoss: 0.6767925024032593\n",
      "cnt: 0 - valLoss: 0.6773365139961243 - trainLoss: 0.6767792701721191\n",
      "cnt: 0 - valLoss: 0.6773242354393005 - trainLoss: 0.6767661571502686\n",
      "cnt: 0 - valLoss: 0.6773118376731873 - trainLoss: 0.676753044128418\n",
      "cnt: 0 - valLoss: 0.6772994995117188 - trainLoss: 0.6767398118972778\n",
      "cnt: 0 - valLoss: 0.6772871613502502 - trainLoss: 0.676726758480072\n",
      "cnt: 0 - valLoss: 0.6772748231887817 - trainLoss: 0.6767135858535767\n",
      "cnt: 0 - valLoss: 0.6772624850273132 - trainLoss: 0.6767004728317261\n",
      "cnt: 0 - valLoss: 0.6772502064704895 - trainLoss: 0.6766873598098755\n",
      "cnt: 0 - valLoss: 0.6772379279136658 - trainLoss: 0.6766743659973145\n",
      "cnt: 0 - valLoss: 0.6772257089614868 - trainLoss: 0.6766613125801086\n",
      "cnt: 0 - valLoss: 0.6772134304046631 - trainLoss: 0.6766482591629028\n",
      "cnt: 0 - valLoss: 0.6772012114524841 - trainLoss: 0.6766353249549866\n",
      "cnt: 0 - valLoss: 0.6771889925003052 - trainLoss: 0.676622211933136\n",
      "cnt: 0 - valLoss: 0.6771767139434814 - trainLoss: 0.676609218120575\n",
      "cnt: 0 - valLoss: 0.6771645545959473 - trainLoss: 0.6765962839126587\n",
      "cnt: 0 - valLoss: 0.6771523356437683 - trainLoss: 0.6765832901000977\n",
      "cnt: 0 - valLoss: 0.6771402359008789 - trainLoss: 0.6765702962875366\n",
      "cnt: 0 - valLoss: 0.6771280169487 - trainLoss: 0.6765573620796204\n",
      "cnt: 0 - valLoss: 0.6771159172058105 - trainLoss: 0.6765444874763489\n",
      "cnt: 0 - valLoss: 0.6771038174629211 - trainLoss: 0.6765315532684326\n",
      "cnt: 0 - valLoss: 0.677091658115387 - trainLoss: 0.6765186190605164\n",
      "cnt: 0 - valLoss: 0.6770795583724976 - trainLoss: 0.6765058040618896\n",
      "cnt: 0 - valLoss: 0.6770675182342529 - trainLoss: 0.6764929294586182\n",
      "cnt: 0 - valLoss: 0.6770554184913635 - trainLoss: 0.6764800548553467\n",
      "cnt: 0 - valLoss: 0.6770433783531189 - trainLoss: 0.67646723985672\n",
      "cnt: 0 - valLoss: 0.6770313382148743 - trainLoss: 0.6764544248580933\n",
      "cnt: 0 - valLoss: 0.6770192980766296 - trainLoss: 0.6764416098594666\n",
      "cnt: 0 - valLoss: 0.677007257938385 - trainLoss: 0.6764287948608398\n",
      "cnt: 0 - valLoss: 0.6769953370094299 - trainLoss: 0.6764160990715027\n",
      "cnt: 0 - valLoss: 0.6769832372665405 - trainLoss: 0.676403284072876\n",
      "cnt: 0 - valLoss: 0.6769713759422302 - trainLoss: 0.676390528678894\n",
      "cnt: 0 - valLoss: 0.6769593358039856 - trainLoss: 0.6763778328895569\n",
      "cnt: 0 - valLoss: 0.6769474148750305 - trainLoss: 0.6763650178909302\n",
      "cnt: 0 - valLoss: 0.6769354939460754 - trainLoss: 0.6763523817062378\n",
      "cnt: 0 - valLoss: 0.6769235730171204 - trainLoss: 0.6763396263122559\n",
      "cnt: 0 - valLoss: 0.6769117116928101 - trainLoss: 0.6763269901275635\n",
      "cnt: 0 - valLoss: 0.676899790763855 - trainLoss: 0.6763143539428711\n",
      "cnt: 0 - valLoss: 0.6768878698348999 - trainLoss: 0.6763017177581787\n",
      "cnt: 0 - valLoss: 0.6768760085105896 - trainLoss: 0.6762890219688416\n",
      "cnt: 0 - valLoss: 0.6768642067909241 - trainLoss: 0.676276445388794\n",
      "cnt: 0 - valLoss: 0.6768524050712585 - trainLoss: 0.6762638092041016\n",
      "cnt: 0 - valLoss: 0.6768405437469482 - trainLoss: 0.6762511730194092\n",
      "cnt: 0 - valLoss: 0.6768287420272827 - trainLoss: 0.6762386560440063\n",
      "cnt: 0 - valLoss: 0.676816999912262 - trainLoss: 0.6762260794639587\n",
      "cnt: 0 - valLoss: 0.6768051981925964 - trainLoss: 0.6762135028839111\n",
      "cnt: 0 - valLoss: 0.6767933964729309 - trainLoss: 0.6762009859085083\n",
      "cnt: 0 - valLoss: 0.6767816543579102 - trainLoss: 0.6761884689331055\n",
      "cnt: 0 - valLoss: 0.6767699122428894 - trainLoss: 0.6761759519577026\n",
      "cnt: 0 - valLoss: 0.6767581701278687 - trainLoss: 0.6761634945869446\n",
      "cnt: 0 - valLoss: 0.6767464876174927 - trainLoss: 0.6761510372161865\n",
      "cnt: 0 - valLoss: 0.6767348051071167 - trainLoss: 0.6761385202407837\n",
      "cnt: 0 - valLoss: 0.6767231225967407 - trainLoss: 0.6761260628700256\n",
      "cnt: 0 - valLoss: 0.6767114400863647 - trainLoss: 0.6761136651039124\n",
      "cnt: 0 - valLoss: 0.6766998171806335 - trainLoss: 0.6761012077331543\n",
      "cnt: 0 - valLoss: 0.6766881346702576 - trainLoss: 0.676088809967041\n",
      "cnt: 0 - valLoss: 0.6766765117645264 - trainLoss: 0.676076352596283\n",
      "cnt: 0 - valLoss: 0.6766648292541504 - trainLoss: 0.6760640144348145\n",
      "cnt: 0 - valLoss: 0.6766532063484192 - trainLoss: 0.6760516166687012\n",
      "cnt: 0 - valLoss: 0.6766416430473328 - trainLoss: 0.6760392785072327\n",
      "cnt: 0 - valLoss: 0.6766301989555359 - trainLoss: 0.6760269999504089\n",
      "cnt: 0 - valLoss: 0.6766185760498047 - trainLoss: 0.6760146617889404\n",
      "cnt: 0 - valLoss: 0.6766069531440735 - trainLoss: 0.6760023832321167\n",
      "cnt: 0 - valLoss: 0.6765954494476318 - trainLoss: 0.6759900450706482\n",
      "cnt: 0 - valLoss: 0.6765838861465454 - trainLoss: 0.6759777665138245\n",
      "cnt: 0 - valLoss: 0.6765723824501038 - trainLoss: 0.6759654879570007\n",
      "cnt: 0 - valLoss: 0.6765608787536621 - trainLoss: 0.6759532690048218\n",
      "cnt: 0 - valLoss: 0.6765494346618652 - trainLoss: 0.675940990447998\n",
      "cnt: 0 - valLoss: 0.6765379905700684 - trainLoss: 0.6759287714958191\n",
      "cnt: 0 - valLoss: 0.6765265464782715 - trainLoss: 0.6759165525436401\n",
      "cnt: 0 - valLoss: 0.6765150427818298 - trainLoss: 0.675904393196106\n",
      "cnt: 0 - valLoss: 0.6765035390853882 - trainLoss: 0.6758921146392822\n",
      "cnt: 0 - valLoss: 0.6764922142028809 - trainLoss: 0.6758800148963928\n",
      "cnt: 0 - valLoss: 0.676480770111084 - trainLoss: 0.6758677959442139\n",
      "cnt: 0 - valLoss: 0.6764693856239319 - trainLoss: 0.6758556962013245\n",
      "cnt: 0 - valLoss: 0.6764580607414246 - trainLoss: 0.6758434772491455\n",
      "cnt: 0 - valLoss: 0.6764466166496277 - trainLoss: 0.6758314371109009\n",
      "cnt: 0 - valLoss: 0.6764352917671204 - trainLoss: 0.6758193373680115\n",
      "cnt: 0 - valLoss: 0.676423966884613 - trainLoss: 0.6758071780204773\n",
      "cnt: 0 - valLoss: 0.6764126420021057 - trainLoss: 0.6757951378822327\n",
      "cnt: 0 - valLoss: 0.6764013171195984 - trainLoss: 0.6757830381393433\n",
      "cnt: 0 - valLoss: 0.6763899922370911 - trainLoss: 0.6757709980010986\n",
      "cnt: 0 - valLoss: 0.6763787269592285 - trainLoss: 0.6757590174674988\n",
      "cnt: 0 - valLoss: 0.676367461681366 - trainLoss: 0.6757469773292542\n",
      "cnt: 0 - valLoss: 0.6763562560081482 - trainLoss: 0.6757349967956543\n",
      "cnt: 0 - valLoss: 0.6763449907302856 - trainLoss: 0.6757229566574097\n",
      "cnt: 0 - valLoss: 0.6763336658477783 - trainLoss: 0.6757109761238098\n",
      "cnt: 0 - valLoss: 0.6763224601745605 - trainLoss: 0.67569899559021\n",
      "cnt: 0 - valLoss: 0.6763112545013428 - trainLoss: 0.6756870150566101\n",
      "cnt: 0 - valLoss: 0.6763001084327698 - trainLoss: 0.675675094127655\n",
      "cnt: 0 - valLoss: 0.676288902759552 - trainLoss: 0.6756632328033447\n",
      "cnt: 0 - valLoss: 0.676277756690979 - trainLoss: 0.6756512522697449\n",
      "cnt: 0 - valLoss: 0.676266610622406 - trainLoss: 0.6756393313407898\n",
      "cnt: 0 - valLoss: 0.6762554049491882 - trainLoss: 0.6756275296211243\n",
      "cnt: 0 - valLoss: 0.6762442588806152 - trainLoss: 0.6756155490875244\n",
      "cnt: 0 - valLoss: 0.676233172416687 - trainLoss: 0.6756036877632141\n",
      "cnt: 0 - valLoss: 0.6762220859527588 - trainLoss: 0.6755918860435486\n",
      "cnt: 0 - valLoss: 0.6762109994888306 - trainLoss: 0.6755800843238831\n",
      "cnt: 0 - valLoss: 0.6761998534202576 - trainLoss: 0.675568163394928\n",
      "cnt: 0 - valLoss: 0.6761888861656189 - trainLoss: 0.6755564212799072\n",
      "cnt: 0 - valLoss: 0.6761777997016907 - trainLoss: 0.6755446195602417\n",
      "cnt: 0 - valLoss: 0.6761667132377625 - trainLoss: 0.6755327582359314\n",
      "cnt: 0 - valLoss: 0.6761557459831238 - trainLoss: 0.6755210757255554\n",
      "cnt: 0 - valLoss: 0.6761446595191956 - trainLoss: 0.6755092740058899\n",
      "cnt: 0 - valLoss: 0.6761337518692017 - trainLoss: 0.6754975318908691\n",
      "cnt: 0 - valLoss: 0.6761226654052734 - trainLoss: 0.6754858493804932\n",
      "cnt: 0 - valLoss: 0.6761116981506348 - trainLoss: 0.6754741072654724\n",
      "cnt: 0 - valLoss: 0.6761007308959961 - trainLoss: 0.6754623651504517\n",
      "cnt: 0 - valLoss: 0.6760897636413574 - trainLoss: 0.6754507422447205\n",
      "cnt: 0 - valLoss: 0.6760788559913635 - trainLoss: 0.6754390001296997\n",
      "cnt: 0 - valLoss: 0.6760678887367249 - trainLoss: 0.6754273772239685\n",
      "cnt: 0 - valLoss: 0.676056981086731 - trainLoss: 0.6754157543182373\n",
      "cnt: 0 - valLoss: 0.6760460734367371 - trainLoss: 0.6754040718078613\n",
      "cnt: 0 - valLoss: 0.6760352253913879 - trainLoss: 0.6753924489021301\n",
      "cnt: 0 - valLoss: 0.676024317741394 - trainLoss: 0.6753808259963989\n",
      "cnt: 0 - valLoss: 0.6760134696960449 - trainLoss: 0.6753692626953125\n",
      "cnt: 0 - valLoss: 0.6760026216506958 - trainLoss: 0.6753576993942261\n",
      "cnt: 0 - valLoss: 0.6759917736053467 - trainLoss: 0.6753461360931396\n",
      "cnt: 0 - valLoss: 0.6759809255599976 - trainLoss: 0.6753345131874084\n",
      "cnt: 0 - valLoss: 0.6759701371192932 - trainLoss: 0.6753230094909668\n",
      "cnt: 0 - valLoss: 0.6759593486785889 - trainLoss: 0.6753113865852356\n",
      "cnt: 0 - valLoss: 0.6759485602378845 - trainLoss: 0.6752999424934387\n",
      "cnt: 0 - valLoss: 0.6759377121925354 - trainLoss: 0.6752884387969971\n",
      "cnt: 0 - valLoss: 0.6759270429611206 - trainLoss: 0.6752769351005554\n",
      "cnt: 0 - valLoss: 0.6759162545204163 - trainLoss: 0.6752654314041138\n",
      "cnt: 0 - valLoss: 0.6759054660797119 - trainLoss: 0.6752540469169617\n",
      "cnt: 0 - valLoss: 0.6758947372436523 - trainLoss: 0.6752424836158752\n",
      "cnt: 0 - valLoss: 0.6758840084075928 - trainLoss: 0.6752310991287231\n",
      "cnt: 0 - valLoss: 0.675873339176178 - trainLoss: 0.675219714641571\n",
      "cnt: 0 - valLoss: 0.6758626699447632 - trainLoss: 0.6752082109451294\n",
      "cnt: 0 - valLoss: 0.6758520007133484 - trainLoss: 0.6751968264579773\n",
      "cnt: 0 - valLoss: 0.6758412718772888 - trainLoss: 0.6751854419708252\n",
      "cnt: 0 - valLoss: 0.675830602645874 - trainLoss: 0.6751740574836731\n",
      "cnt: 0 - valLoss: 0.6758201122283936 - trainLoss: 0.6751627326011658\n",
      "cnt: 0 - valLoss: 0.675809383392334 - trainLoss: 0.6751514077186584\n",
      "cnt: 0 - valLoss: 0.6757987141609192 - trainLoss: 0.6751400232315063\n",
      "cnt: 0 - valLoss: 0.675788164138794 - trainLoss: 0.675128698348999\n",
      "cnt: 0 - valLoss: 0.6757775545120239 - trainLoss: 0.6751173734664917\n",
      "cnt: 0 - valLoss: 0.6757670044898987 - trainLoss: 0.6751059889793396\n",
      "cnt: 0 - valLoss: 0.6757564544677734 - trainLoss: 0.675094723701477\n",
      "cnt: 0 - valLoss: 0.6757458448410034 - trainLoss: 0.6750835180282593\n",
      "cnt: 0 - valLoss: 0.6757352948188782 - trainLoss: 0.675072193145752\n",
      "cnt: 0 - valLoss: 0.6757248044013977 - trainLoss: 0.6750609874725342\n",
      "cnt: 0 - valLoss: 0.6757142543792725 - trainLoss: 0.6750497221946716\n",
      "cnt: 0 - valLoss: 0.675703763961792 - trainLoss: 0.6750384569168091\n",
      "cnt: 0 - valLoss: 0.6756932139396667 - trainLoss: 0.6750272512435913\n",
      "cnt: 0 - valLoss: 0.675682783126831 - trainLoss: 0.6750161051750183\n",
      "cnt: 0 - valLoss: 0.6756722927093506 - trainLoss: 0.6750048995018005\n",
      "cnt: 0 - valLoss: 0.6756618618965149 - trainLoss: 0.6749937534332275\n",
      "cnt: 0 - valLoss: 0.6756513714790344 - trainLoss: 0.6749825477600098\n",
      "cnt: 0 - valLoss: 0.6756409406661987 - trainLoss: 0.6749714612960815\n",
      "cnt: 0 - valLoss: 0.6756305694580078 - trainLoss: 0.6749602556228638\n",
      "cnt: 0 - valLoss: 0.6756201982498169 - trainLoss: 0.6749491095542908\n",
      "cnt: 0 - valLoss: 0.6756097078323364 - trainLoss: 0.6749379634857178\n",
      "cnt: 0 - valLoss: 0.6755993962287903 - trainLoss: 0.6749269366264343\n",
      "cnt: 0 - valLoss: 0.6755889654159546 - trainLoss: 0.6749157905578613\n",
      "cnt: 0 - valLoss: 0.6755785942077637 - trainLoss: 0.6749047636985779\n",
      "cnt: 0 - valLoss: 0.6755682826042175 - trainLoss: 0.6748936772346497\n",
      "cnt: 0 - valLoss: 0.6755579113960266 - trainLoss: 0.6748826503753662\n",
      "cnt: 0 - valLoss: 0.6755475401878357 - trainLoss: 0.6748716235160828\n",
      "cnt: 0 - valLoss: 0.6755373477935791 - trainLoss: 0.6748605370521545\n",
      "cnt: 0 - valLoss: 0.6755269765853882 - trainLoss: 0.6748495697975159\n",
      "cnt: 0 - valLoss: 0.6755167245864868 - trainLoss: 0.6748385429382324\n",
      "cnt: 0 - valLoss: 0.6755064129829407 - trainLoss: 0.6748275756835938\n",
      "cnt: 0 - valLoss: 0.6754961609840393 - trainLoss: 0.6748166084289551\n",
      "cnt: 0 - valLoss: 0.6754859089851379 - trainLoss: 0.6748056411743164\n",
      "cnt: 0 - valLoss: 0.6754757165908813 - trainLoss: 0.6747946739196777\n",
      "cnt: 0 - valLoss: 0.67546546459198 - trainLoss: 0.6747837662696838\n",
      "cnt: 0 - valLoss: 0.6754552721977234 - trainLoss: 0.6747728586196899\n",
      "cnt: 0 - valLoss: 0.675445020198822 - trainLoss: 0.6747618913650513\n",
      "cnt: 0 - valLoss: 0.6754347681999207 - trainLoss: 0.6747509837150574\n",
      "cnt: 0 - valLoss: 0.6754245758056641 - trainLoss: 0.6747400760650635\n",
      "cnt: 0 - valLoss: 0.675414502620697 - trainLoss: 0.6747292280197144\n",
      "cnt: 0 - valLoss: 0.6754042506217957 - trainLoss: 0.6747183799743652\n",
      "cnt: 0 - valLoss: 0.6753941178321838 - trainLoss: 0.6747074723243713\n",
      "cnt: 0 - valLoss: 0.675383985042572 - trainLoss: 0.674696683883667\n",
      "cnt: 0 - valLoss: 0.6753738522529602 - trainLoss: 0.6746858358383179\n",
      "cnt: 0 - valLoss: 0.6753637790679932 - trainLoss: 0.6746749877929688\n",
      "cnt: 0 - valLoss: 0.6753537058830261 - trainLoss: 0.6746641993522644\n",
      "cnt: 0 - valLoss: 0.6753435730934143 - trainLoss: 0.6746534109115601\n",
      "cnt: 0 - valLoss: 0.6753334999084473 - trainLoss: 0.6746426224708557\n",
      "cnt: 0 - valLoss: 0.6753234267234802 - trainLoss: 0.6746318340301514\n",
      "cnt: 0 - valLoss: 0.6753133535385132 - trainLoss: 0.6746211051940918\n",
      "cnt: 0 - valLoss: 0.6753033399581909 - trainLoss: 0.6746103763580322\n",
      "cnt: 0 - valLoss: 0.6752932667732239 - trainLoss: 0.6745996475219727\n",
      "cnt: 0 - valLoss: 0.6752832531929016 - trainLoss: 0.6745888590812683\n",
      "cnt: 0 - valLoss: 0.6752733588218689 - trainLoss: 0.6745781898498535\n",
      "cnt: 0 - valLoss: 0.6752632856369019 - trainLoss: 0.674567461013794\n",
      "cnt: 0 - valLoss: 0.6752533316612244 - trainLoss: 0.6745567917823792\n",
      "cnt: 0 - valLoss: 0.6752433180809021 - trainLoss: 0.6745461821556091\n",
      "cnt: 0 - valLoss: 0.6752333641052246 - trainLoss: 0.6745354533195496\n",
      "cnt: 0 - valLoss: 0.6752234101295471 - trainLoss: 0.6745247840881348\n",
      "cnt: 0 - valLoss: 0.6752135157585144 - trainLoss: 0.6745141744613647\n",
      "cnt: 0 - valLoss: 0.6752035617828369 - trainLoss: 0.6745035648345947\n",
      "cnt: 0 - valLoss: 0.6751936078071594 - trainLoss: 0.6744928956031799\n",
      "cnt: 0 - valLoss: 0.6751837730407715 - trainLoss: 0.6744822859764099\n",
      "cnt: 0 - valLoss: 0.675173819065094 - trainLoss: 0.6744717359542847\n",
      "cnt: 0 - valLoss: 0.675163984298706 - trainLoss: 0.6744611859321594\n",
      "cnt: 0 - valLoss: 0.6751540899276733 - trainLoss: 0.6744506359100342\n",
      "cnt: 0 - valLoss: 0.6751442551612854 - trainLoss: 0.6744400858879089\n",
      "cnt: 0 - valLoss: 0.6751343607902527 - trainLoss: 0.6744295358657837\n",
      "cnt: 0 - valLoss: 0.6751245260238647 - trainLoss: 0.6744189858436584\n",
      "cnt: 0 - valLoss: 0.6751146912574768 - trainLoss: 0.674408495426178\n",
      "cnt: 0 - valLoss: 0.6751049160957336 - trainLoss: 0.6743979454040527\n",
      "cnt: 0 - valLoss: 0.6750950813293457 - trainLoss: 0.6743874549865723\n",
      "cnt: 0 - valLoss: 0.6750853657722473 - trainLoss: 0.6743770241737366\n",
      "cnt: 0 - valLoss: 0.6750755906105042 - trainLoss: 0.6743664741516113\n",
      "cnt: 0 - valLoss: 0.675065815448761 - trainLoss: 0.6743561029434204\n",
      "cnt: 0 - valLoss: 0.6750560402870178 - trainLoss: 0.6743456125259399\n",
      "cnt: 0 - valLoss: 0.6750463247299194 - trainLoss: 0.6743351817131042\n",
      "cnt: 0 - valLoss: 0.6750365495681763 - trainLoss: 0.6743247509002686\n",
      "cnt: 0 - valLoss: 0.6750267744064331 - trainLoss: 0.6743143200874329\n",
      "cnt: 0 - valLoss: 0.6750171184539795 - trainLoss: 0.6743038892745972\n",
      "cnt: 0 - valLoss: 0.6750074028968811 - trainLoss: 0.674293577671051\n",
      "cnt: 0 - valLoss: 0.6749976873397827 - trainLoss: 0.6742832064628601\n",
      "cnt: 0 - valLoss: 0.6749880313873291 - trainLoss: 0.6742727756500244\n",
      "cnt: 0 - valLoss: 0.6749783754348755 - trainLoss: 0.6742624640464783\n",
      "cnt: 0 - valLoss: 0.6749687194824219 - trainLoss: 0.6742520928382874\n",
      "cnt: 0 - valLoss: 0.6749590635299683 - trainLoss: 0.6742417812347412\n",
      "cnt: 0 - valLoss: 0.6749494671821594 - trainLoss: 0.6742314696311951\n",
      "cnt: 0 - valLoss: 0.6749398112297058 - trainLoss: 0.6742211580276489\n",
      "cnt: 0 - valLoss: 0.6749302744865417 - trainLoss: 0.6742108464241028\n",
      "cnt: 0 - valLoss: 0.6749206781387329 - trainLoss: 0.6742005944252014\n",
      "cnt: 0 - valLoss: 0.6749110221862793 - trainLoss: 0.6741902828216553\n",
      "cnt: 0 - valLoss: 0.6749014854431152 - trainLoss: 0.6741800308227539\n",
      "cnt: 0 - valLoss: 0.6748919486999512 - trainLoss: 0.6741697788238525\n",
      "cnt: 0 - valLoss: 0.6748823523521423 - trainLoss: 0.6741595268249512\n",
      "cnt: 0 - valLoss: 0.6748728156089783 - trainLoss: 0.6741493344306946\n",
      "cnt: 0 - valLoss: 0.6748632788658142 - trainLoss: 0.674139142036438\n",
      "cnt: 0 - valLoss: 0.6748537421226501 - trainLoss: 0.6741288900375366\n",
      "cnt: 0 - valLoss: 0.6748442053794861 - trainLoss: 0.67411869764328\n",
      "cnt: 0 - valLoss: 0.6748347282409668 - trainLoss: 0.6741085052490234\n",
      "cnt: 0 - valLoss: 0.6748252511024475 - trainLoss: 0.6740983724594116\n",
      "cnt: 0 - valLoss: 0.6748157739639282 - trainLoss: 0.674088180065155\n",
      "cnt: 0 - valLoss: 0.6748062968254089 - trainLoss: 0.6740780472755432\n",
      "cnt: 0 - valLoss: 0.6747968792915344 - trainLoss: 0.6740679144859314\n",
      "cnt: 0 - valLoss: 0.6747874021530151 - trainLoss: 0.6740577816963196\n",
      "cnt: 0 - valLoss: 0.6747779250144958 - trainLoss: 0.6740476489067078\n",
      "cnt: 0 - valLoss: 0.6747685670852661 - trainLoss: 0.6740375757217407\n",
      "cnt: 0 - valLoss: 0.6747591495513916 - trainLoss: 0.6740275025367737\n",
      "cnt: 0 - valLoss: 0.6747497320175171 - trainLoss: 0.6740173697471619\n",
      "cnt: 0 - valLoss: 0.6747403740882874 - trainLoss: 0.6740073561668396\n",
      "cnt: 0 - valLoss: 0.6747309565544128 - trainLoss: 0.6739973425865173\n",
      "cnt: 0 - valLoss: 0.6747215986251831 - trainLoss: 0.6739872694015503\n",
      "cnt: 0 - valLoss: 0.6747123003005981 - trainLoss: 0.6739771366119385\n",
      "cnt: 0 - valLoss: 0.6747029423713684 - trainLoss: 0.673967182636261\n",
      "cnt: 0 - valLoss: 0.6746935844421387 - trainLoss: 0.6739572286605835\n",
      "cnt: 0 - valLoss: 0.6746842265129089 - trainLoss: 0.6739472150802612\n",
      "cnt: 0 - valLoss: 0.674674928188324 - trainLoss: 0.673937201499939\n",
      "cnt: 0 - valLoss: 0.674665629863739 - trainLoss: 0.6739272475242615\n",
      "cnt: 0 - valLoss: 0.674656331539154 - trainLoss: 0.673917293548584\n",
      "cnt: 0 - valLoss: 0.6746470928192139 - trainLoss: 0.6739072799682617\n",
      "cnt: 0 - valLoss: 0.6746377944946289 - trainLoss: 0.673897385597229\n",
      "cnt: 0 - valLoss: 0.674628496170044 - trainLoss: 0.6738874316215515\n",
      "cnt: 0 - valLoss: 0.6746193170547485 - trainLoss: 0.6738775372505188\n",
      "cnt: 0 - valLoss: 0.6746100187301636 - trainLoss: 0.6738675832748413\n",
      "cnt: 0 - valLoss: 0.6746007800102234 - trainLoss: 0.6738576889038086\n",
      "cnt: 0 - valLoss: 0.674591600894928 - trainLoss: 0.6738477945327759\n",
      "cnt: 0 - valLoss: 0.6745823621749878 - trainLoss: 0.6738379001617432\n",
      "cnt: 0 - valLoss: 0.6745732426643372 - trainLoss: 0.6738280653953552\n",
      "cnt: 0 - valLoss: 0.674564003944397 - trainLoss: 0.6738181710243225\n",
      "cnt: 0 - valLoss: 0.6745548248291016 - trainLoss: 0.6738083958625793\n",
      "cnt: 0 - valLoss: 0.6745456457138062 - trainLoss: 0.6737985014915466\n",
      "cnt: 0 - valLoss: 0.6745365262031555 - trainLoss: 0.6737887263298035\n",
      "cnt: 0 - valLoss: 0.6745273470878601 - trainLoss: 0.6737788319587708\n",
      "cnt: 0 - valLoss: 0.6745182275772095 - trainLoss: 0.6737690567970276\n",
      "cnt: 0 - valLoss: 0.6745091080665588 - trainLoss: 0.6737592816352844\n",
      "cnt: 0 - valLoss: 0.674500048160553 - trainLoss: 0.673749566078186\n",
      "cnt: 0 - valLoss: 0.6744908690452576 - trainLoss: 0.6737397909164429\n",
      "cnt: 0 - valLoss: 0.6744818091392517 - trainLoss: 0.6737299561500549\n",
      "cnt: 0 - valLoss: 0.6744727492332458 - trainLoss: 0.6737202405929565\n",
      "cnt: 0 - valLoss: 0.67446368932724 - trainLoss: 0.6737105250358582\n",
      "cnt: 0 - valLoss: 0.6744545698165894 - trainLoss: 0.6737008094787598\n",
      "cnt: 0 - valLoss: 0.6744455099105835 - trainLoss: 0.6736910939216614\n",
      "cnt: 0 - valLoss: 0.6744365692138672 - trainLoss: 0.673681378364563\n",
      "cnt: 0 - valLoss: 0.6744274497032166 - trainLoss: 0.6736717224121094\n",
      "cnt: 0 - valLoss: 0.6744185090065002 - trainLoss: 0.6736620664596558\n",
      "cnt: 0 - valLoss: 0.6744095087051392 - trainLoss: 0.6736524105072021\n",
      "cnt: 0 - valLoss: 0.6744005084037781 - trainLoss: 0.6736426949501038\n",
      "cnt: 0 - valLoss: 0.674391508102417 - trainLoss: 0.6736330389976501\n",
      "cnt: 0 - valLoss: 0.6743825078010559 - trainLoss: 0.6736233830451965\n",
      "cnt: 0 - valLoss: 0.6743735671043396 - trainLoss: 0.6736137866973877\n",
      "cnt: 0 - valLoss: 0.6743645668029785 - trainLoss: 0.6736041903495789\n",
      "cnt: 0 - valLoss: 0.674355685710907 - trainLoss: 0.67359459400177\n",
      "cnt: 0 - valLoss: 0.6743466854095459 - trainLoss: 0.6735849976539612\n",
      "cnt: 0 - valLoss: 0.6743378639221191 - trainLoss: 0.6735754013061523\n",
      "cnt: 0 - valLoss: 0.6743288636207581 - trainLoss: 0.6735658049583435\n",
      "cnt: 0 - valLoss: 0.6743199825286865 - trainLoss: 0.6735563278198242\n",
      "cnt: 0 - valLoss: 0.674311101436615 - trainLoss: 0.6735467314720154\n",
      "cnt: 0 - valLoss: 0.6743022799491882 - trainLoss: 0.6735371351242065\n",
      "cnt: 0 - valLoss: 0.6742933988571167 - trainLoss: 0.6735275983810425\n",
      "cnt: 0 - valLoss: 0.6742845177650452 - trainLoss: 0.673518180847168\n",
      "cnt: 0 - valLoss: 0.6742756366729736 - trainLoss: 0.6735085844993591\n",
      "cnt: 0 - valLoss: 0.6742668151855469 - trainLoss: 0.6734991669654846\n",
      "cnt: 0 - valLoss: 0.6742580533027649 - trainLoss: 0.6734896302223206\n",
      "cnt: 0 - valLoss: 0.6742491722106934 - trainLoss: 0.6734801530838013\n",
      "cnt: 0 - valLoss: 0.6742402911186218 - trainLoss: 0.6734707355499268\n",
      "cnt: 0 - valLoss: 0.6742315292358398 - trainLoss: 0.6734612584114075\n",
      "cnt: 0 - valLoss: 0.6742227673530579 - trainLoss: 0.673451840877533\n",
      "cnt: 0 - valLoss: 0.6742139458656311 - trainLoss: 0.6734424233436584\n",
      "cnt: 0 - valLoss: 0.6742052435874939 - trainLoss: 0.6734330058097839\n",
      "cnt: 0 - valLoss: 0.6741964817047119 - trainLoss: 0.6734235882759094\n",
      "cnt: 0 - valLoss: 0.6741877198219299 - trainLoss: 0.6734141707420349\n",
      "cnt: 0 - valLoss: 0.6741790175437927 - trainLoss: 0.6734047532081604\n",
      "cnt: 0 - valLoss: 0.6741703152656555 - trainLoss: 0.6733953952789307\n",
      "cnt: 0 - valLoss: 0.6741614937782288 - trainLoss: 0.6733860373497009\n",
      "cnt: 0 - valLoss: 0.6741528511047363 - trainLoss: 0.6733766794204712\n",
      "cnt: 0 - valLoss: 0.6741441488265991 - trainLoss: 0.6733673214912415\n",
      "cnt: 0 - valLoss: 0.6741354465484619 - trainLoss: 0.6733579635620117\n",
      "cnt: 0 - valLoss: 0.6741268038749695 - trainLoss: 0.673348605632782\n",
      "cnt: 0 - valLoss: 0.6741181015968323 - trainLoss: 0.673339307308197\n",
      "cnt: 0 - valLoss: 0.6741094589233398 - trainLoss: 0.6733300089836121\n",
      "cnt: 0 - valLoss: 0.6741008758544922 - trainLoss: 0.6733207702636719\n",
      "cnt: 0 - valLoss: 0.674092173576355 - trainLoss: 0.6733114123344421\n",
      "cnt: 0 - valLoss: 0.6740835905075073 - trainLoss: 0.673302173614502\n",
      "cnt: 0 - valLoss: 0.6740749478340149 - trainLoss: 0.673292875289917\n",
      "cnt: 0 - valLoss: 0.6740663647651672 - trainLoss: 0.6732836961746216\n",
      "cnt: 0 - valLoss: 0.6740577816963196 - trainLoss: 0.6732744574546814\n",
      "cnt: 0 - valLoss: 0.6740492582321167 - trainLoss: 0.6732652187347412\n",
      "cnt: 0 - valLoss: 0.6740406155586243 - trainLoss: 0.673255980014801\n",
      "cnt: 0 - valLoss: 0.6740320324897766 - trainLoss: 0.6732468008995056\n",
      "cnt: 0 - valLoss: 0.6740235686302185 - trainLoss: 0.6732375621795654\n",
      "cnt: 0 - valLoss: 0.6740150451660156 - trainLoss: 0.67322838306427\n",
      "cnt: 0 - valLoss: 0.6740064024925232 - trainLoss: 0.6732192039489746\n",
      "cnt: 0 - valLoss: 0.6739979386329651 - trainLoss: 0.6732100248336792\n",
      "cnt: 0 - valLoss: 0.673989474773407 - trainLoss: 0.6732008457183838\n",
      "cnt: 0 - valLoss: 0.6739809513092041 - trainLoss: 0.6731917262077332\n",
      "cnt: 0 - valLoss: 0.6739724278450012 - trainLoss: 0.6731825470924377\n",
      "cnt: 0 - valLoss: 0.6739639639854431 - trainLoss: 0.6731734275817871\n",
      "cnt: 0 - valLoss: 0.673955500125885 - trainLoss: 0.6731643080711365\n",
      "cnt: 0 - valLoss: 0.6739470362663269 - trainLoss: 0.6731551885604858\n",
      "cnt: 0 - valLoss: 0.6739385724067688 - trainLoss: 0.67314612865448\n",
      "cnt: 0 - valLoss: 0.6739301085472107 - trainLoss: 0.6731370091438293\n",
      "cnt: 0 - valLoss: 0.6739217042922974 - trainLoss: 0.6731279492378235\n",
      "cnt: 0 - valLoss: 0.6739132404327393 - trainLoss: 0.6731188893318176\n",
      "cnt: 0 - valLoss: 0.6739048957824707 - trainLoss: 0.6731098294258118\n",
      "cnt: 0 - valLoss: 0.6738965511322021 - trainLoss: 0.6731007695198059\n",
      "cnt: 0 - valLoss: 0.673888087272644 - trainLoss: 0.6730917096138\n",
      "cnt: 0 - valLoss: 0.6738796830177307 - trainLoss: 0.673082709312439\n",
      "cnt: 0 - valLoss: 0.6738713979721069 - trainLoss: 0.6730736494064331\n",
      "cnt: 0 - valLoss: 0.6738629341125488 - trainLoss: 0.6730647087097168\n",
      "cnt: 0 - valLoss: 0.6738545894622803 - trainLoss: 0.6730556488037109\n",
      "cnt: 0 - valLoss: 0.6738463044166565 - trainLoss: 0.6730467081069946\n",
      "cnt: 0 - valLoss: 0.6738379597663879 - trainLoss: 0.6730377078056335\n",
      "cnt: 0 - valLoss: 0.6738295555114746 - trainLoss: 0.6730287075042725\n",
      "cnt: 0 - valLoss: 0.6738212704658508 - trainLoss: 0.6730198264122009\n",
      "cnt: 0 - valLoss: 0.6738130450248718 - trainLoss: 0.6730108857154846\n",
      "cnt: 0 - valLoss: 0.6738047003746033 - trainLoss: 0.6730019450187683\n",
      "cnt: 0 - valLoss: 0.6737964153289795 - trainLoss: 0.6729930639266968\n",
      "cnt: 0 - valLoss: 0.6737881898880005 - trainLoss: 0.6729840636253357\n",
      "cnt: 0 - valLoss: 0.6737799048423767 - trainLoss: 0.6729751825332642\n",
      "cnt: 0 - valLoss: 0.6737716197967529 - trainLoss: 0.6729663014411926\n",
      "cnt: 0 - valLoss: 0.6737634539604187 - trainLoss: 0.6729574203491211\n",
      "cnt: 0 - valLoss: 0.6737551689147949 - trainLoss: 0.6729484796524048\n",
      "cnt: 0 - valLoss: 0.6737469434738159 - trainLoss: 0.672939658164978\n",
      "cnt: 0 - valLoss: 0.6737387180328369 - trainLoss: 0.6729308366775513\n",
      "cnt: 0 - valLoss: 0.6737305521965027 - trainLoss: 0.6729219555854797\n",
      "cnt: 0 - valLoss: 0.6737223267555237 - trainLoss: 0.672913134098053\n",
      "cnt: 0 - valLoss: 0.6737142205238342 - trainLoss: 0.6729043126106262\n",
      "cnt: 0 - valLoss: 0.6737059950828552 - trainLoss: 0.6728954911231995\n",
      "cnt: 0 - valLoss: 0.673697829246521 - trainLoss: 0.6728866696357727\n",
      "cnt: 0 - valLoss: 0.6736896634101868 - trainLoss: 0.6728779077529907\n",
      "cnt: 0 - valLoss: 0.6736815571784973 - trainLoss: 0.672869086265564\n",
      "cnt: 0 - valLoss: 0.6736733317375183 - trainLoss: 0.672860324382782\n",
      "cnt: 0 - valLoss: 0.6736652255058289 - trainLoss: 0.6728515625\n",
      "cnt: 0 - valLoss: 0.6736571192741394 - trainLoss: 0.672842800617218\n",
      "cnt: 0 - valLoss: 0.6736489534378052 - trainLoss: 0.672834038734436\n",
      "cnt: 0 - valLoss: 0.6736409068107605 - trainLoss: 0.6728253960609436\n",
      "cnt: 0 - valLoss: 0.673632800579071 - trainLoss: 0.6728165745735168\n",
      "cnt: 0 - valLoss: 0.6736246943473816 - trainLoss: 0.6728079319000244\n",
      "cnt: 0 - valLoss: 0.6736166477203369 - trainLoss: 0.6727991700172424\n",
      "cnt: 0 - valLoss: 0.6736086010932922 - trainLoss: 0.6727904677391052\n",
      "cnt: 0 - valLoss: 0.6736005544662476 - trainLoss: 0.6727818250656128\n",
      "cnt: 0 - valLoss: 0.6735924482345581 - trainLoss: 0.6727731227874756\n",
      "cnt: 0 - valLoss: 0.6735844612121582 - trainLoss: 0.6727644801139832\n",
      "cnt: 0 - valLoss: 0.6735764145851135 - trainLoss: 0.672755777835846\n",
      "cnt: 0 - valLoss: 0.6735684275627136 - trainLoss: 0.6727471351623535\n",
      "cnt: 0 - valLoss: 0.673560380935669 - trainLoss: 0.6727384924888611\n",
      "cnt: 0 - valLoss: 0.6735524535179138 - trainLoss: 0.6727299094200134\n",
      "cnt: 0 - valLoss: 0.6735444068908691 - trainLoss: 0.672721266746521\n",
      "cnt: 0 - valLoss: 0.6735364198684692 - trainLoss: 0.6727126240730286\n",
      "cnt: 0 - valLoss: 0.6735284328460693 - trainLoss: 0.6727040410041809\n",
      "cnt: 0 - valLoss: 0.6735204458236694 - trainLoss: 0.6726954579353333\n",
      "cnt: 0 - valLoss: 0.6735125780105591 - trainLoss: 0.6726868748664856\n",
      "cnt: 0 - valLoss: 0.6735045909881592 - trainLoss: 0.6726782917976379\n",
      "cnt: 0 - valLoss: 0.673496663570404 - trainLoss: 0.6726697683334351\n",
      "cnt: 0 - valLoss: 0.6734887361526489 - trainLoss: 0.6726611852645874\n",
      "cnt: 0 - valLoss: 0.6734808683395386 - trainLoss: 0.6726526618003845\n",
      "cnt: 0 - valLoss: 0.6734729409217834 - trainLoss: 0.6726440787315369\n",
      "cnt: 0 - valLoss: 0.6734650135040283 - trainLoss: 0.6726356148719788\n",
      "cnt: 0 - valLoss: 0.673457145690918 - trainLoss: 0.6726270914077759\n",
      "cnt: 0 - valLoss: 0.6734492778778076 - trainLoss: 0.672618567943573\n",
      "cnt: 0 - valLoss: 0.6734414100646973 - trainLoss: 0.6726101040840149\n",
      "cnt: 0 - valLoss: 0.6734335422515869 - trainLoss: 0.672601580619812\n",
      "cnt: 0 - valLoss: 0.6734256744384766 - trainLoss: 0.6725931167602539\n",
      "cnt: 0 - valLoss: 0.673417866230011 - trainLoss: 0.672584593296051\n",
      "cnt: 0 - valLoss: 0.6734100580215454 - trainLoss: 0.6725761890411377\n",
      "cnt: 0 - valLoss: 0.6734021902084351 - trainLoss: 0.6725677251815796\n",
      "cnt: 0 - valLoss: 0.6733943819999695 - trainLoss: 0.672559380531311\n",
      "cnt: 0 - valLoss: 0.6733865737915039 - trainLoss: 0.6725508570671082\n",
      "cnt: 0 - valLoss: 0.6733787655830383 - trainLoss: 0.6725425124168396\n",
      "cnt: 0 - valLoss: 0.6733710169792175 - trainLoss: 0.6725340485572815\n",
      "cnt: 0 - valLoss: 0.6733632683753967 - trainLoss: 0.6725256443023682\n",
      "cnt: 0 - valLoss: 0.6733554601669312 - trainLoss: 0.6725172996520996\n",
      "cnt: 0 - valLoss: 0.6733477115631104 - trainLoss: 0.6725088953971863\n",
      "cnt: 0 - valLoss: 0.6733399033546448 - trainLoss: 0.672500491142273\n",
      "cnt: 0 - valLoss: 0.6733322143554688 - trainLoss: 0.6724921464920044\n",
      "cnt: 0 - valLoss: 0.6733245253562927 - trainLoss: 0.6724837422370911\n",
      "cnt: 0 - valLoss: 0.6733167767524719 - trainLoss: 0.6724753975868225\n",
      "cnt: 0 - valLoss: 0.6733090281486511 - trainLoss: 0.6724671125411987\n",
      "cnt: 0 - valLoss: 0.6733013391494751 - trainLoss: 0.6724587678909302\n",
      "cnt: 0 - valLoss: 0.6732936501502991 - trainLoss: 0.6724504828453064\n",
      "cnt: 0 - valLoss: 0.673285961151123 - trainLoss: 0.6724421977996826\n",
      "cnt: 0 - valLoss: 0.673278272151947 - trainLoss: 0.6724338531494141\n",
      "cnt: 0 - valLoss: 0.673270583152771 - trainLoss: 0.6724256277084351\n",
      "cnt: 0 - valLoss: 0.673262894153595 - trainLoss: 0.6724173426628113\n",
      "cnt: 0 - valLoss: 0.6732553243637085 - trainLoss: 0.6724090576171875\n",
      "cnt: 0 - valLoss: 0.6732476353645325 - trainLoss: 0.6724008321762085\n",
      "cnt: 0 - valLoss: 0.673240065574646 - trainLoss: 0.6723924875259399\n",
      "cnt: 0 - valLoss: 0.6732323169708252 - trainLoss: 0.6723843216896057\n",
      "cnt: 0 - valLoss: 0.6732248067855835 - trainLoss: 0.6723760366439819\n",
      "cnt: 0 - valLoss: 0.6732171177864075 - trainLoss: 0.6723678112030029\n",
      "cnt: 0 - valLoss: 0.673209547996521 - trainLoss: 0.6723595857620239\n",
      "cnt: 0 - valLoss: 0.6732019782066345 - trainLoss: 0.6723513603210449\n",
      "cnt: 0 - valLoss: 0.673194408416748 - trainLoss: 0.6723432540893555\n",
      "cnt: 0 - valLoss: 0.6731868386268616 - trainLoss: 0.6723350882530212\n",
      "cnt: 0 - valLoss: 0.6731792688369751 - trainLoss: 0.672326922416687\n",
      "cnt: 0 - valLoss: 0.6731717586517334 - trainLoss: 0.672318696975708\n",
      "cnt: 0 - valLoss: 0.6731641292572021 - trainLoss: 0.6723105311393738\n",
      "cnt: 0 - valLoss: 0.6731566190719604 - trainLoss: 0.6723024249076843\n",
      "cnt: 0 - valLoss: 0.6731491088867188 - trainLoss: 0.6722942590713501\n",
      "cnt: 0 - valLoss: 0.6731415390968323 - trainLoss: 0.6722860932350159\n",
      "cnt: 0 - valLoss: 0.6731340885162354 - trainLoss: 0.6722780466079712\n",
      "cnt: 0 - valLoss: 0.6731265783309937 - trainLoss: 0.672269880771637\n",
      "cnt: 0 - valLoss: 0.6731191277503967 - trainLoss: 0.6722617745399475\n",
      "cnt: 0 - valLoss: 0.673111617565155 - trainLoss: 0.6722537279129028\n",
      "cnt: 0 - valLoss: 0.6731041669845581 - trainLoss: 0.6722455620765686\n",
      "cnt: 0 - valLoss: 0.6730966567993164 - trainLoss: 0.6722375154495239\n",
      "cnt: 0 - valLoss: 0.6730892062187195 - trainLoss: 0.6722294688224792\n",
      "cnt: 0 - valLoss: 0.6730817556381226 - trainLoss: 0.6722214221954346\n",
      "cnt: 0 - valLoss: 0.6730743050575256 - trainLoss: 0.6722133755683899\n",
      "cnt: 0 - valLoss: 0.6730668544769287 - trainLoss: 0.6722053289413452\n",
      "cnt: 0 - valLoss: 0.6730594635009766 - trainLoss: 0.6721972823143005\n",
      "cnt: 0 - valLoss: 0.6730520725250244 - trainLoss: 0.6721892356872559\n",
      "cnt: 0 - valLoss: 0.6730446815490723 - trainLoss: 0.672181248664856\n",
      "cnt: 0 - valLoss: 0.6730372309684753 - trainLoss: 0.672173261642456\n",
      "cnt: 0 - valLoss: 0.6730298399925232 - trainLoss: 0.6721652746200562\n",
      "cnt: 0 - valLoss: 0.6730225086212158 - trainLoss: 0.6721572279930115\n",
      "cnt: 0 - valLoss: 0.6730150580406189 - trainLoss: 0.6721493005752563\n",
      "cnt: 0 - valLoss: 0.6730077266693115 - trainLoss: 0.6721413731575012\n",
      "cnt: 0 - valLoss: 0.6730003952980042 - trainLoss: 0.6721333265304565\n",
      "cnt: 0 - valLoss: 0.6729930639266968 - trainLoss: 0.6721254587173462\n",
      "cnt: 0 - valLoss: 0.6729857325553894 - trainLoss: 0.6721175312995911\n",
      "cnt: 0 - valLoss: 0.6729783415794373 - trainLoss: 0.6721095442771912\n",
      "cnt: 0 - valLoss: 0.6729710698127747 - trainLoss: 0.672101616859436\n",
      "cnt: 0 - valLoss: 0.6729637980461121 - trainLoss: 0.6720936894416809\n",
      "cnt: 0 - valLoss: 0.6729564666748047 - trainLoss: 0.6720858216285706\n",
      "cnt: 0 - valLoss: 0.6729491353034973 - trainLoss: 0.6720778942108154\n",
      "cnt: 0 - valLoss: 0.6729418039321899 - trainLoss: 0.6720700263977051\n",
      "cnt: 0 - valLoss: 0.6729345917701721 - trainLoss: 0.6720621585845947\n",
      "cnt: 0 - valLoss: 0.6729273200035095 - trainLoss: 0.6720542311668396\n",
      "cnt: 0 - valLoss: 0.6729200482368469 - trainLoss: 0.672046422958374\n",
      "cnt: 0 - valLoss: 0.6729127764701843 - trainLoss: 0.6720386147499084\n",
      "cnt: 0 - valLoss: 0.6729055643081665 - trainLoss: 0.6720306873321533\n",
      "cnt: 0 - valLoss: 0.6728982925415039 - trainLoss: 0.672022819519043\n",
      "cnt: 0 - valLoss: 0.6728910803794861 - trainLoss: 0.6720150709152222\n",
      "cnt: 0 - valLoss: 0.6728838682174683 - trainLoss: 0.6720072627067566\n",
      "cnt: 0 - valLoss: 0.6728766560554504 - trainLoss: 0.671999454498291\n",
      "cnt: 0 - valLoss: 0.6728694438934326 - trainLoss: 0.6719915866851807\n",
      "cnt: 0 - valLoss: 0.6728622913360596 - trainLoss: 0.6719837784767151\n",
      "cnt: 0 - valLoss: 0.6728550791740417 - trainLoss: 0.6719760894775391\n",
      "cnt: 0 - valLoss: 0.6728479266166687 - trainLoss: 0.6719682812690735\n",
      "cnt: 0 - valLoss: 0.6728407740592957 - trainLoss: 0.6719605326652527\n",
      "cnt: 0 - valLoss: 0.6728336215019226 - trainLoss: 0.6719527840614319\n",
      "cnt: 0 - valLoss: 0.6728264689445496 - trainLoss: 0.6719450354576111\n",
      "cnt: 0 - valLoss: 0.6728193163871765 - trainLoss: 0.6719372868537903\n",
      "cnt: 0 - valLoss: 0.6728121042251587 - trainLoss: 0.6719295382499695\n",
      "cnt: 0 - valLoss: 0.6728050112724304 - trainLoss: 0.6719218492507935\n",
      "cnt: 0 - valLoss: 0.6727979183197021 - trainLoss: 0.6719141006469727\n",
      "cnt: 0 - valLoss: 0.6727907657623291 - trainLoss: 0.6719064116477966\n",
      "cnt: 0 - valLoss: 0.6727836728096008 - trainLoss: 0.6718987226486206\n",
      "cnt: 0 - valLoss: 0.6727766394615173 - trainLoss: 0.6718910336494446\n",
      "cnt: 0 - valLoss: 0.6727695465087891 - trainLoss: 0.6718833446502686\n",
      "cnt: 0 - valLoss: 0.6727624535560608 - trainLoss: 0.6718756556510925\n",
      "cnt: 0 - valLoss: 0.6727553606033325 - trainLoss: 0.671868085861206\n",
      "cnt: 0 - valLoss: 0.672748327255249 - trainLoss: 0.6718603372573853\n",
      "cnt: 0 - valLoss: 0.6727412939071655 - trainLoss: 0.671852707862854\n",
      "cnt: 0 - valLoss: 0.672734260559082 - trainLoss: 0.6718450784683228\n",
      "cnt: 0 - valLoss: 0.6727271676063538 - trainLoss: 0.6718374490737915\n",
      "cnt: 0 - valLoss: 0.6727201342582703 - trainLoss: 0.6718298196792603\n",
      "cnt: 0 - valLoss: 0.6727131009101868 - trainLoss: 0.6718222498893738\n",
      "cnt: 0 - valLoss: 0.672706127166748 - trainLoss: 0.6718146204948425\n",
      "cnt: 0 - valLoss: 0.6726991534233093 - trainLoss: 0.671807050704956\n",
      "cnt: 0 - valLoss: 0.6726921200752258 - trainLoss: 0.6717994809150696\n",
      "cnt: 0 - valLoss: 0.6726850867271423 - trainLoss: 0.6717918515205383\n",
      "cnt: 0 - valLoss: 0.6726781725883484 - trainLoss: 0.6717843413352966\n",
      "cnt: 0 - valLoss: 0.6726711988449097 - trainLoss: 0.6717767119407654\n",
      "cnt: 0 - valLoss: 0.6726641654968262 - trainLoss: 0.6717692017555237\n",
      "cnt: 0 - valLoss: 0.6726572513580322 - trainLoss: 0.6717616319656372\n",
      "cnt: 0 - valLoss: 0.6726503372192383 - trainLoss: 0.6717541217803955\n",
      "cnt: 0 - valLoss: 0.6726433634757996 - trainLoss: 0.6717466115951538\n",
      "cnt: 0 - valLoss: 0.6726364493370056 - trainLoss: 0.6717390418052673\n",
      "cnt: 0 - valLoss: 0.6726294755935669 - trainLoss: 0.6717315912246704\n",
      "cnt: 0 - valLoss: 0.6726226210594177 - trainLoss: 0.6717240810394287\n",
      "cnt: 0 - valLoss: 0.672615647315979 - trainLoss: 0.671716570854187\n",
      "cnt: 0 - valLoss: 0.6726087927818298 - trainLoss: 0.6717091202735901\n",
      "cnt: 0 - valLoss: 0.6726018786430359 - trainLoss: 0.6717016696929932\n",
      "cnt: 0 - valLoss: 0.6725950837135315 - trainLoss: 0.6716941595077515\n",
      "cnt: 0 - valLoss: 0.6725881695747375 - trainLoss: 0.6716866493225098\n",
      "cnt: 0 - valLoss: 0.6725813150405884 - trainLoss: 0.6716792583465576\n",
      "cnt: 0 - valLoss: 0.6725744009017944 - trainLoss: 0.6716717481613159\n",
      "cnt: 0 - valLoss: 0.67256760597229 - trainLoss: 0.6716644167900085\n",
      "cnt: 0 - valLoss: 0.6725607514381409 - trainLoss: 0.6716569066047668\n",
      "cnt: 0 - valLoss: 0.6725539565086365 - trainLoss: 0.6716495156288147\n",
      "cnt: 0 - valLoss: 0.6725471019744873 - trainLoss: 0.6716421842575073\n",
      "cnt: 0 - valLoss: 0.6725402474403381 - trainLoss: 0.6716347336769104\n",
      "cnt: 0 - valLoss: 0.6725334525108337 - trainLoss: 0.6716273427009583\n",
      "cnt: 0 - valLoss: 0.6725266575813293 - trainLoss: 0.6716200113296509\n",
      "cnt: 0 - valLoss: 0.6725199222564697 - trainLoss: 0.671612560749054\n",
      "cnt: 0 - valLoss: 0.6725130677223206 - trainLoss: 0.6716052889823914\n",
      "cnt: 0 - valLoss: 0.6725063323974609 - trainLoss: 0.6715978980064392\n",
      "cnt: 0 - valLoss: 0.6724994778633118 - trainLoss: 0.6715905070304871\n",
      "cnt: 0 - valLoss: 0.6724927425384521 - trainLoss: 0.6715831756591797\n",
      "cnt: 0 - valLoss: 0.6724860668182373 - trainLoss: 0.6715758442878723\n",
      "cnt: 0 - valLoss: 0.6724792718887329 - trainLoss: 0.6715685129165649\n",
      "cnt: 0 - valLoss: 0.6724725365638733 - trainLoss: 0.6715611815452576\n",
      "cnt: 0 - valLoss: 0.6724658012390137 - trainLoss: 0.671553909778595\n",
      "cnt: 0 - valLoss: 0.672459065914154 - trainLoss: 0.6715466380119324\n",
      "cnt: 0 - valLoss: 0.6724523305892944 - trainLoss: 0.6715392470359802\n",
      "cnt: 0 - valLoss: 0.6724455952644348 - trainLoss: 0.6715319752693176\n",
      "cnt: 0 - valLoss: 0.6724389791488647 - trainLoss: 0.671524703502655\n",
      "cnt: 0 - valLoss: 0.6724322438240051 - trainLoss: 0.6715174913406372\n",
      "cnt: 0 - valLoss: 0.6724255681037903 - trainLoss: 0.6715101599693298\n",
      "cnt: 0 - valLoss: 0.6724188923835754 - trainLoss: 0.671502947807312\n",
      "cnt: 0 - valLoss: 0.6724122166633606 - trainLoss: 0.6714956760406494\n",
      "cnt: 0 - valLoss: 0.6724055409431458 - trainLoss: 0.6714885234832764\n",
      "cnt: 0 - valLoss: 0.6723989248275757 - trainLoss: 0.6714812517166138\n",
      "cnt: 0 - valLoss: 0.6723922491073608 - trainLoss: 0.6714739799499512\n",
      "cnt: 0 - valLoss: 0.6723856329917908 - trainLoss: 0.6714668273925781\n",
      "cnt: 0 - valLoss: 0.6723789572715759 - trainLoss: 0.6714596152305603\n",
      "cnt: 0 - valLoss: 0.6723723411560059 - trainLoss: 0.6714524030685425\n",
      "cnt: 0 - valLoss: 0.6723657846450806 - trainLoss: 0.6714452505111694\n",
      "cnt: 0 - valLoss: 0.6723591685295105 - trainLoss: 0.6714380383491516\n",
      "cnt: 0 - valLoss: 0.6723525524139404 - trainLoss: 0.6714308857917786\n",
      "cnt: 0 - valLoss: 0.6723459362983704 - trainLoss: 0.6714236736297607\n",
      "cnt: 0 - valLoss: 0.6723393797874451 - trainLoss: 0.6714165806770325\n",
      "cnt: 0 - valLoss: 0.672332763671875 - trainLoss: 0.6714094281196594\n",
      "cnt: 0 - valLoss: 0.6723262667655945 - trainLoss: 0.6714022755622864\n",
      "cnt: 0 - valLoss: 0.6723196506500244 - trainLoss: 0.6713951230049133\n",
      "cnt: 0 - valLoss: 0.6723130941390991 - trainLoss: 0.6713880300521851\n",
      "cnt: 0 - valLoss: 0.6723065376281738 - trainLoss: 0.671380877494812\n",
      "cnt: 0 - valLoss: 0.6723000407218933 - trainLoss: 0.6713737845420837\n",
      "cnt: 0 - valLoss: 0.672293484210968 - trainLoss: 0.6713666915893555\n",
      "cnt: 0 - valLoss: 0.6722869277000427 - trainLoss: 0.6713595986366272\n",
      "cnt: 0 - valLoss: 0.672280490398407 - trainLoss: 0.6713525056838989\n",
      "cnt: 0 - valLoss: 0.6722739338874817 - trainLoss: 0.6713454723358154\n",
      "cnt: 0 - valLoss: 0.6722674369812012 - trainLoss: 0.6713383793830872\n",
      "cnt: 0 - valLoss: 0.6722609400749207 - trainLoss: 0.6713312864303589\n",
      "cnt: 0 - valLoss: 0.6722545027732849 - trainLoss: 0.6713242530822754\n",
      "cnt: 0 - valLoss: 0.6722480058670044 - trainLoss: 0.6713172793388367\n",
      "cnt: 0 - valLoss: 0.6722415089607239 - trainLoss: 0.6713101863861084\n",
      "cnt: 0 - valLoss: 0.6722350120544434 - trainLoss: 0.6713031530380249\n",
      "cnt: 0 - valLoss: 0.6722285747528076 - trainLoss: 0.6712961792945862\n",
      "cnt: 0 - valLoss: 0.6722220778465271 - trainLoss: 0.6712891459465027\n",
      "cnt: 0 - valLoss: 0.6722156405448914 - trainLoss: 0.6712821125984192\n",
      "cnt: 0 - valLoss: 0.6722092628479004 - trainLoss: 0.6712751388549805\n",
      "cnt: 0 - valLoss: 0.6722028255462646 - trainLoss: 0.6712681651115417\n",
      "cnt: 0 - valLoss: 0.6721963882446289 - trainLoss: 0.6712611317634583\n",
      "cnt: 0 - valLoss: 0.6721900105476379 - trainLoss: 0.6712542176246643\n",
      "cnt: 0 - valLoss: 0.672183632850647 - trainLoss: 0.6712472438812256\n",
      "cnt: 0 - valLoss: 0.6721771955490112 - trainLoss: 0.6712402701377869\n",
      "cnt: 0 - valLoss: 0.6721708178520203 - trainLoss: 0.6712332963943481\n",
      "cnt: 0 - valLoss: 0.6721644401550293 - trainLoss: 0.6712263822555542\n",
      "cnt: 0 - valLoss: 0.6721580624580383 - trainLoss: 0.6712194681167603\n",
      "cnt: 0 - valLoss: 0.6721517443656921 - trainLoss: 0.6712124943733215\n",
      "cnt: 0 - valLoss: 0.6721453070640564 - trainLoss: 0.6712055802345276\n",
      "cnt: 0 - valLoss: 0.6721389889717102 - trainLoss: 0.6711986660957336\n",
      "cnt: 0 - valLoss: 0.672132670879364 - trainLoss: 0.6711918115615845\n",
      "cnt: 0 - valLoss: 0.672126293182373 - trainLoss: 0.6711848378181458\n",
      "cnt: 0 - valLoss: 0.6721199750900269 - trainLoss: 0.6711780428886414\n",
      "cnt: 0 - valLoss: 0.6721136569976807 - trainLoss: 0.6711710691452026\n",
      "cnt: 0 - valLoss: 0.6721073389053345 - trainLoss: 0.6711642146110535\n",
      "cnt: 0 - valLoss: 0.6721010208129883 - trainLoss: 0.6711573600769043\n",
      "cnt: 0 - valLoss: 0.6720947027206421 - trainLoss: 0.6711505651473999\n",
      "cnt: 0 - valLoss: 0.6720884442329407 - trainLoss: 0.6711437106132507\n",
      "cnt: 0 - valLoss: 0.6720821261405945 - trainLoss: 0.6711368560791016\n",
      "cnt: 0 - valLoss: 0.6720758676528931 - trainLoss: 0.6711300015449524\n",
      "cnt: 0 - valLoss: 0.6720696091651917 - trainLoss: 0.671123206615448\n",
      "cnt: 0 - valLoss: 0.6720633506774902 - trainLoss: 0.6711163520812988\n",
      "cnt: 0 - valLoss: 0.6720570921897888 - trainLoss: 0.6711095571517944\n",
      "cnt: 0 - valLoss: 0.6720508337020874 - trainLoss: 0.6711027026176453\n",
      "cnt: 0 - valLoss: 0.672044575214386 - trainLoss: 0.6710959672927856\n",
      "cnt: 0 - valLoss: 0.6720383167266846 - trainLoss: 0.6710891723632812\n",
      "cnt: 0 - valLoss: 0.6720321178436279 - trainLoss: 0.6710823774337769\n",
      "cnt: 0 - valLoss: 0.6720258593559265 - trainLoss: 0.6710755825042725\n",
      "cnt: 0 - valLoss: 0.6720197200775146 - trainLoss: 0.6710688471794128\n",
      "cnt: 0 - valLoss: 0.672013521194458 - trainLoss: 0.6710621118545532\n",
      "cnt: 0 - valLoss: 0.6720072627067566 - trainLoss: 0.6710553765296936\n",
      "cnt: 0 - valLoss: 0.6720011234283447 - trainLoss: 0.671048641204834\n",
      "cnt: 0 - valLoss: 0.6719949245452881 - trainLoss: 0.6710418462753296\n",
      "cnt: 0 - valLoss: 0.6719887256622314 - trainLoss: 0.6710351705551147\n",
      "cnt: 0 - valLoss: 0.6719825863838196 - trainLoss: 0.6710284352302551\n",
      "cnt: 0 - valLoss: 0.6719763875007629 - trainLoss: 0.6710216999053955\n",
      "cnt: 0 - valLoss: 0.6719702482223511 - trainLoss: 0.6710149645805359\n",
      "cnt: 0 - valLoss: 0.6719641089439392 - trainLoss: 0.671008288860321\n",
      "cnt: 0 - valLoss: 0.6719579696655273 - trainLoss: 0.6710016131401062\n",
      "cnt: 0 - valLoss: 0.6719518899917603 - trainLoss: 0.6709949374198914\n",
      "cnt: 0 - valLoss: 0.6719457507133484 - trainLoss: 0.6709882616996765\n",
      "cnt: 0 - valLoss: 0.6719395518302917 - trainLoss: 0.6709815859794617\n",
      "cnt: 0 - valLoss: 0.6719334721565247 - trainLoss: 0.6709749102592468\n",
      "cnt: 0 - valLoss: 0.6719273924827576 - trainLoss: 0.6709682941436768\n",
      "cnt: 0 - valLoss: 0.6719213128089905 - trainLoss: 0.6709616184234619\n",
      "cnt: 0 - valLoss: 0.6719152331352234 - trainLoss: 0.6709550023078918\n",
      "cnt: 0 - valLoss: 0.6719090938568115 - trainLoss: 0.670948326587677\n",
      "cnt: 0 - valLoss: 0.6719030141830444 - trainLoss: 0.6709417700767517\n",
      "cnt: 0 - valLoss: 0.6718969345092773 - trainLoss: 0.6709350943565369\n",
      "cnt: 0 - valLoss: 0.671890914440155 - trainLoss: 0.6709284782409668\n",
      "cnt: 0 - valLoss: 0.6718848347663879 - trainLoss: 0.6709219217300415\n",
      "cnt: 0 - valLoss: 0.6718787550926208 - trainLoss: 0.6709153056144714\n",
      "cnt: 0 - valLoss: 0.6718727350234985 - trainLoss: 0.6709086894989014\n",
      "cnt: 0 - valLoss: 0.6718666553497314 - trainLoss: 0.6709021329879761\n",
      "cnt: 0 - valLoss: 0.6718606948852539 - trainLoss: 0.670895516872406\n",
      "cnt: 0 - valLoss: 0.6718546748161316 - trainLoss: 0.6708890199661255\n",
      "cnt: 0 - valLoss: 0.6718486547470093 - trainLoss: 0.6708824038505554\n",
      "cnt: 0 - valLoss: 0.6718426942825317 - trainLoss: 0.6708759069442749\n",
      "cnt: 0 - valLoss: 0.6718366146087646 - trainLoss: 0.6708693504333496\n",
      "cnt: 0 - valLoss: 0.6718306541442871 - trainLoss: 0.6708628535270691\n",
      "cnt: 0 - valLoss: 0.6718246936798096 - trainLoss: 0.6708562970161438\n",
      "cnt: 0 - valLoss: 0.6718186736106873 - trainLoss: 0.6708498001098633\n",
      "cnt: 0 - valLoss: 0.6718126535415649 - trainLoss: 0.670843243598938\n",
      "cnt: 0 - valLoss: 0.6718067526817322 - trainLoss: 0.6708367466926575\n",
      "cnt: 0 - valLoss: 0.6718007922172546 - trainLoss: 0.6708303093910217\n",
      "cnt: 0 - valLoss: 0.6717948317527771 - trainLoss: 0.6708238124847412\n",
      "cnt: 0 - valLoss: 0.6717889308929443 - trainLoss: 0.6708172559738159\n",
      "cnt: 0 - valLoss: 0.6717829704284668 - trainLoss: 0.6708107590675354\n",
      "cnt: 0 - valLoss: 0.6717770099639893 - trainLoss: 0.6708043813705444\n",
      "cnt: 0 - valLoss: 0.6717711091041565 - trainLoss: 0.6707978844642639\n",
      "cnt: 0 - valLoss: 0.6717652082443237 - trainLoss: 0.6707914471626282\n",
      "cnt: 0 - valLoss: 0.6717592477798462 - trainLoss: 0.6707849502563477\n",
      "cnt: 0 - valLoss: 0.6717534065246582 - trainLoss: 0.6707785725593567\n",
      "cnt: 0 - valLoss: 0.6717474460601807 - trainLoss: 0.670772135257721\n",
      "cnt: 0 - valLoss: 0.6717416048049927 - trainLoss: 0.6707656383514404\n",
      "cnt: 0 - valLoss: 0.6717357039451599 - trainLoss: 0.6707592606544495\n",
      "cnt: 0 - valLoss: 0.6717298626899719 - trainLoss: 0.6707528233528137\n",
      "cnt: 0 - valLoss: 0.6717240214347839 - trainLoss: 0.6707464456558228\n",
      "cnt: 0 - valLoss: 0.6717181205749512 - trainLoss: 0.6707400679588318\n",
      "cnt: 0 - valLoss: 0.6717122793197632 - trainLoss: 0.6707336902618408\n",
      "cnt: 0 - valLoss: 0.6717064380645752 - trainLoss: 0.6707272529602051\n",
      "cnt: 0 - valLoss: 0.6717005968093872 - trainLoss: 0.6707208752632141\n",
      "cnt: 0 - valLoss: 0.6716946959495544 - trainLoss: 0.6707144975662231\n",
      "cnt: 0 - valLoss: 0.6716889142990112 - trainLoss: 0.670708179473877\n",
      "cnt: 0 - valLoss: 0.6716830730438232 - trainLoss: 0.670701801776886\n",
      "cnt: 0 - valLoss: 0.6716773509979248 - trainLoss: 0.6706955432891846\n",
      "cnt: 0 - valLoss: 0.6716715097427368 - trainLoss: 0.6706891655921936\n",
      "cnt: 0 - valLoss: 0.6716657280921936 - trainLoss: 0.6706828474998474\n",
      "cnt: 0 - valLoss: 0.6716599464416504 - trainLoss: 0.6706764698028564\n",
      "cnt: 0 - valLoss: 0.6716541647911072 - trainLoss: 0.6706701517105103\n",
      "cnt: 0 - valLoss: 0.6716483235359192 - trainLoss: 0.6706638932228088\n",
      "cnt: 0 - valLoss: 0.671642541885376 - trainLoss: 0.6706575751304626\n",
      "cnt: 0 - valLoss: 0.6716368794441223 - trainLoss: 0.6706512570381165\n",
      "cnt: 0 - valLoss: 0.6716310977935791 - trainLoss: 0.6706449389457703\n",
      "cnt: 0 - valLoss: 0.6716253161430359 - trainLoss: 0.6706387400627136\n",
      "cnt: 0 - valLoss: 0.6716195344924927 - trainLoss: 0.6706324815750122\n",
      "cnt: 0 - valLoss: 0.6716138124465942 - trainLoss: 0.6706261038780212\n",
      "cnt: 0 - valLoss: 0.6716080904006958 - trainLoss: 0.6706199049949646\n",
      "cnt: 0 - valLoss: 0.6716024279594421 - trainLoss: 0.6706136465072632\n",
      "cnt: 0 - valLoss: 0.6715966463088989 - trainLoss: 0.6706073880195618\n",
      "cnt: 0 - valLoss: 0.6715909838676453 - trainLoss: 0.6706011295318604\n",
      "cnt: 0 - valLoss: 0.6715852618217468 - trainLoss: 0.6705949306488037\n",
      "cnt: 0 - valLoss: 0.6715795397758484 - trainLoss: 0.6705886721611023\n",
      "cnt: 0 - valLoss: 0.6715738773345947 - trainLoss: 0.6705824732780457\n",
      "cnt: 0 - valLoss: 0.6715682148933411 - trainLoss: 0.670576274394989\n",
      "cnt: 0 - valLoss: 0.6715624332427979 - trainLoss: 0.6705700755119324\n",
      "cnt: 0 - valLoss: 0.671556830406189 - trainLoss: 0.6705638766288757\n",
      "cnt: 0 - valLoss: 0.6715511083602905 - trainLoss: 0.6705576777458191\n",
      "cnt: 0 - valLoss: 0.6715455055236816 - trainLoss: 0.6705514788627625\n",
      "cnt: 0 - valLoss: 0.671539843082428 - trainLoss: 0.6705452799797058\n",
      "cnt: 0 - valLoss: 0.6715341806411743 - trainLoss: 0.6705390810966492\n",
      "cnt: 0 - valLoss: 0.6715285778045654 - trainLoss: 0.6705330014228821\n",
      "cnt: 0 - valLoss: 0.6715229749679565 - trainLoss: 0.6705268025398254\n",
      "cnt: 0 - valLoss: 0.6715173125267029 - trainLoss: 0.6705206632614136\n",
      "cnt: 0 - valLoss: 0.671511709690094 - trainLoss: 0.6705145239830017\n",
      "cnt: 0 - valLoss: 0.6715061068534851 - trainLoss: 0.6705083847045898\n",
      "cnt: 0 - valLoss: 0.6715004444122314 - trainLoss: 0.6705021858215332\n",
      "cnt: 0 - valLoss: 0.6714949011802673 - trainLoss: 0.6704961657524109\n",
      "cnt: 0 - valLoss: 0.6714892387390137 - trainLoss: 0.6704899668693542\n",
      "cnt: 0 - valLoss: 0.6714836955070496 - trainLoss: 0.6704838871955872\n",
      "cnt: 0 - valLoss: 0.6714780926704407 - trainLoss: 0.6704778075218201\n",
      "cnt: 0 - valLoss: 0.6714725494384766 - trainLoss: 0.670471727848053\n",
      "cnt: 0 - valLoss: 0.6714670062065125 - trainLoss: 0.6704656481742859\n",
      "cnt: 0 - valLoss: 0.6714614033699036 - trainLoss: 0.670459508895874\n",
      "cnt: 0 - valLoss: 0.6714559197425842 - trainLoss: 0.6704534888267517\n",
      "cnt: 0 - valLoss: 0.6714503169059753 - trainLoss: 0.6704474091529846\n",
      "cnt: 0 - valLoss: 0.6714447736740112 - trainLoss: 0.6704413294792175\n",
      "cnt: 0 - valLoss: 0.6714392900466919 - trainLoss: 0.6704353094100952\n",
      "cnt: 0 - valLoss: 0.671433687210083 - trainLoss: 0.6704292297363281\n",
      "cnt: 0 - valLoss: 0.6714282035827637 - trainLoss: 0.670423150062561\n",
      "cnt: 0 - valLoss: 0.6714227199554443 - trainLoss: 0.6704171895980835\n",
      "cnt: 0 - valLoss: 0.6714171767234802 - trainLoss: 0.6704111099243164\n",
      "cnt: 0 - valLoss: 0.6714116930961609 - trainLoss: 0.6704050898551941\n",
      "cnt: 0 - valLoss: 0.6714061498641968 - trainLoss: 0.6703991293907166\n",
      "cnt: 0 - valLoss: 0.6714006662368774 - trainLoss: 0.6703930497169495\n",
      "cnt: 0 - valLoss: 0.6713951826095581 - trainLoss: 0.6703870296478271\n",
      "cnt: 0 - valLoss: 0.6713896989822388 - trainLoss: 0.6703810691833496\n",
      "cnt: 0 - valLoss: 0.6713842153549194 - trainLoss: 0.6703750491142273\n",
      "cnt: 0 - valLoss: 0.6713788509368896 - trainLoss: 0.6703690886497498\n",
      "cnt: 0 - valLoss: 0.6713733673095703 - trainLoss: 0.6703631281852722\n",
      "cnt: 0 - valLoss: 0.671367883682251 - trainLoss: 0.6703571677207947\n",
      "cnt: 0 - valLoss: 0.6713624596595764 - trainLoss: 0.6703511476516724\n",
      "cnt: 0 - valLoss: 0.6713570356369019 - trainLoss: 0.6703452467918396\n",
      "cnt: 0 - valLoss: 0.6713515520095825 - trainLoss: 0.6703392863273621\n",
      "cnt: 0 - valLoss: 0.671346127986908 - trainLoss: 0.6703333258628845\n",
      "cnt: 0 - valLoss: 0.6713407039642334 - trainLoss: 0.6703274250030518\n",
      "cnt: 0 - valLoss: 0.6713353395462036 - trainLoss: 0.6703214645385742\n",
      "cnt: 0 - valLoss: 0.671329915523529 - trainLoss: 0.6703155636787415\n",
      "cnt: 0 - valLoss: 0.6713245511054993 - trainLoss: 0.6703096628189087\n",
      "cnt: 0 - valLoss: 0.6713191270828247 - trainLoss: 0.6703037619590759\n",
      "cnt: 0 - valLoss: 0.6713137626647949 - trainLoss: 0.6702978610992432\n",
      "cnt: 0 - valLoss: 0.6713083386421204 - trainLoss: 0.6702919602394104\n",
      "cnt: 0 - valLoss: 0.6713030338287354 - trainLoss: 0.6702861189842224\n",
      "cnt: 0 - valLoss: 0.6712976694107056 - trainLoss: 0.6702801585197449\n",
      "cnt: 0 - valLoss: 0.671292245388031 - trainLoss: 0.6702743172645569\n",
      "cnt: 0 - valLoss: 0.6712868809700012 - trainLoss: 0.6702684164047241\n",
      "cnt: 0 - valLoss: 0.6712815761566162 - trainLoss: 0.6702625751495361\n",
      "cnt: 0 - valLoss: 0.6712762117385864 - trainLoss: 0.6702567338943481\n",
      "cnt: 0 - valLoss: 0.6712708473205566 - trainLoss: 0.6702508926391602\n",
      "cnt: 0 - valLoss: 0.6712655425071716 - trainLoss: 0.6702450513839722\n",
      "cnt: 0 - valLoss: 0.6712601780891418 - trainLoss: 0.6702392101287842\n",
      "cnt: 0 - valLoss: 0.6712548732757568 - trainLoss: 0.6702333688735962\n",
      "cnt: 0 - valLoss: 0.6712495684623718 - trainLoss: 0.670227587223053\n",
      "cnt: 0 - valLoss: 0.6712442636489868 - trainLoss: 0.670221745967865\n",
      "cnt: 0 - valLoss: 0.6712389588356018 - trainLoss: 0.670215904712677\n",
      "cnt: 0 - valLoss: 0.6712337136268616 - trainLoss: 0.6702101230621338\n",
      "cnt: 0 - valLoss: 0.6712284088134766 - trainLoss: 0.6702042818069458\n",
      "cnt: 0 - valLoss: 0.6712231636047363 - trainLoss: 0.6701985597610474\n",
      "cnt: 0 - valLoss: 0.6712178587913513 - trainLoss: 0.6701927781105042\n",
      "cnt: 0 - valLoss: 0.6712126135826111 - trainLoss: 0.6701869964599609\n",
      "cnt: 0 - valLoss: 0.6712073087692261 - trainLoss: 0.6701812744140625\n",
      "cnt: 0 - valLoss: 0.6712020635604858 - trainLoss: 0.6701754331588745\n",
      "cnt: 0 - valLoss: 0.6711968183517456 - trainLoss: 0.6701696515083313\n",
      "cnt: 0 - valLoss: 0.6711915731430054 - trainLoss: 0.6701639890670776\n",
      "cnt: 0 - valLoss: 0.6711863279342651 - trainLoss: 0.6701582074165344\n",
      "cnt: 0 - valLoss: 0.6711810827255249 - trainLoss: 0.6701524257659912\n",
      "cnt: 0 - valLoss: 0.6711758375167847 - trainLoss: 0.670146644115448\n",
      "cnt: 0 - valLoss: 0.671170711517334 - trainLoss: 0.6701410412788391\n",
      "cnt: 0 - valLoss: 0.671165406703949 - trainLoss: 0.6701352596282959\n",
      "cnt: 0 - valLoss: 0.6711602210998535 - trainLoss: 0.6701295375823975\n",
      "cnt: 0 - valLoss: 0.6711550354957581 - trainLoss: 0.6701238751411438\n",
      "cnt: 0 - valLoss: 0.6711498498916626 - trainLoss: 0.6701181530952454\n",
      "cnt: 0 - valLoss: 0.6711446642875671 - trainLoss: 0.6701124310493469\n",
      "cnt: 0 - valLoss: 0.6711394786834717 - trainLoss: 0.6701067686080933\n",
      "cnt: 0 - valLoss: 0.6711342334747314 - trainLoss: 0.6701011061668396\n",
      "cnt: 0 - valLoss: 0.6711291074752808 - trainLoss: 0.6700954437255859\n",
      "cnt: 0 - valLoss: 0.6711239814758301 - trainLoss: 0.6700897216796875\n",
      "cnt: 0 - valLoss: 0.6711187362670898 - trainLoss: 0.6700841188430786\n",
      "cnt: 0 - valLoss: 0.6711136102676392 - trainLoss: 0.670078456401825\n",
      "cnt: 0 - valLoss: 0.6711084842681885 - trainLoss: 0.6700727343559265\n",
      "cnt: 0 - valLoss: 0.671103298664093 - trainLoss: 0.6700671315193176\n",
      "cnt: 0 - valLoss: 0.6710981726646423 - trainLoss: 0.670061469078064\n",
      "cnt: 0 - valLoss: 0.6710931062698364 - trainLoss: 0.6700558662414551\n",
      "cnt: 0 - valLoss: 0.6710879802703857 - trainLoss: 0.6700502634048462\n",
      "cnt: 0 - valLoss: 0.6710828542709351 - trainLoss: 0.6700446009635925\n",
      "cnt: 0 - valLoss: 0.6710777282714844 - trainLoss: 0.6700389981269836\n",
      "cnt: 0 - valLoss: 0.6710726022720337 - trainLoss: 0.6700333952903748\n",
      "cnt: 0 - valLoss: 0.671067476272583 - trainLoss: 0.6700277924537659\n",
      "cnt: 0 - valLoss: 0.6710624098777771 - trainLoss: 0.670022189617157\n",
      "cnt: 0 - valLoss: 0.6710573434829712 - trainLoss: 0.6700166463851929\n",
      "cnt: 0 - valLoss: 0.6710522770881653 - trainLoss: 0.6700111031532288\n",
      "cnt: 0 - valLoss: 0.6710471510887146 - trainLoss: 0.6700055003166199\n",
      "cnt: 0 - valLoss: 0.6710420846939087 - trainLoss: 0.669999897480011\n",
      "cnt: 0 - valLoss: 0.6710370779037476 - trainLoss: 0.6699943542480469\n",
      "cnt: 0 - valLoss: 0.6710320115089417 - trainLoss: 0.6699888110160828\n",
      "cnt: 0 - valLoss: 0.6710268259048462 - trainLoss: 0.6699832081794739\n",
      "cnt: 0 - valLoss: 0.6710218787193298 - trainLoss: 0.6699776649475098\n",
      "cnt: 0 - valLoss: 0.6710168123245239 - trainLoss: 0.6699721813201904\n",
      "cnt: 0 - valLoss: 0.6710118055343628 - trainLoss: 0.6699666380882263\n",
      "cnt: 0 - valLoss: 0.6710067391395569 - trainLoss: 0.6699610948562622\n",
      "cnt: 0 - valLoss: 0.6710017323493958 - trainLoss: 0.6699556112289429\n",
      "cnt: 0 - valLoss: 0.6709967255592346 - trainLoss: 0.6699500679969788\n",
      "cnt: 0 - valLoss: 0.6709917187690735 - trainLoss: 0.6699445843696594\n",
      "cnt: 0 - valLoss: 0.6709867119789124 - trainLoss: 0.6699391007423401\n",
      "cnt: 0 - valLoss: 0.6709817051887512 - trainLoss: 0.6699336171150208\n",
      "cnt: 0 - valLoss: 0.6709766983985901 - trainLoss: 0.6699281334877014\n",
      "cnt: 0 - valLoss: 0.6709717512130737 - trainLoss: 0.6699225306510925\n",
      "cnt: 0 - valLoss: 0.6709667444229126 - trainLoss: 0.669917106628418\n",
      "cnt: 0 - valLoss: 0.6709617972373962 - trainLoss: 0.6699116230010986\n",
      "cnt: 0 - valLoss: 0.6709567904472351 - trainLoss: 0.6699061989784241\n",
      "cnt: 0 - valLoss: 0.6709518432617188 - trainLoss: 0.6699007153511047\n",
      "cnt: 0 - valLoss: 0.6709468960762024 - trainLoss: 0.6698952317237854\n",
      "cnt: 0 - valLoss: 0.6709418892860413 - trainLoss: 0.6698898673057556\n",
      "cnt: 0 - valLoss: 0.6709370017051697 - trainLoss: 0.6698843836784363\n",
      "cnt: 0 - valLoss: 0.6709320545196533 - trainLoss: 0.6698789596557617\n",
      "cnt: 0 - valLoss: 0.6709270477294922 - trainLoss: 0.6698735356330872\n",
      "cnt: 0 - valLoss: 0.6709221601486206 - trainLoss: 0.6698681116104126\n",
      "cnt: 0 - valLoss: 0.6709172129631042 - trainLoss: 0.6698627471923828\n",
      "cnt: 0 - valLoss: 0.6709123253822327 - trainLoss: 0.6698573231697083\n",
      "cnt: 0 - valLoss: 0.6709073781967163 - trainLoss: 0.6698518991470337\n",
      "cnt: 0 - valLoss: 0.6709025502204895 - trainLoss: 0.6698464751243591\n",
      "cnt: 0 - valLoss: 0.6708976030349731 - trainLoss: 0.6698411703109741\n",
      "cnt: 0 - valLoss: 0.6708927154541016 - trainLoss: 0.6698356866836548\n",
      "cnt: 0 - valLoss: 0.67088782787323 - trainLoss: 0.6698303818702698\n",
      "cnt: 0 - valLoss: 0.6708829402923584 - trainLoss: 0.6698249578475952\n",
      "cnt: 0 - valLoss: 0.6708780527114868 - trainLoss: 0.6698195934295654\n",
      "cnt: 0 - valLoss: 0.6708731651306152 - trainLoss: 0.6698142290115356\n",
      "cnt: 0 - valLoss: 0.6708682775497437 - trainLoss: 0.6698088645935059\n",
      "cnt: 0 - valLoss: 0.6708634495735168 - trainLoss: 0.6698035001754761\n",
      "cnt: 0 - valLoss: 0.6708585619926453 - trainLoss: 0.6697981953620911\n",
      "cnt: 0 - valLoss: 0.6708537936210632 - trainLoss: 0.669792890548706\n",
      "cnt: 0 - valLoss: 0.6708489060401917 - trainLoss: 0.669787585735321\n",
      "cnt: 0 - valLoss: 0.6708440780639648 - trainLoss: 0.669782280921936\n",
      "cnt: 0 - valLoss: 0.6708391904830933 - trainLoss: 0.6697768568992615\n",
      "cnt: 0 - valLoss: 0.6708343625068665 - trainLoss: 0.6697715520858765\n",
      "cnt: 0 - valLoss: 0.6708295345306396 - trainLoss: 0.6697662472724915\n",
      "cnt: 0 - valLoss: 0.6708247065544128 - trainLoss: 0.6697610020637512\n",
      "cnt: 0 - valLoss: 0.6708199381828308 - trainLoss: 0.6697556376457214\n",
      "cnt: 0 - valLoss: 0.670815110206604 - trainLoss: 0.6697503924369812\n",
      "cnt: 0 - valLoss: 0.670810341835022 - trainLoss: 0.6697450876235962\n",
      "cnt: 0 - valLoss: 0.6708055138587952 - trainLoss: 0.6697397828102112\n",
      "cnt: 0 - valLoss: 0.6708007454872131 - trainLoss: 0.669734537601471\n",
      "cnt: 0 - valLoss: 0.6707959771156311 - trainLoss: 0.6697292923927307\n",
      "cnt: 0 - valLoss: 0.6707911491394043 - trainLoss: 0.6697239875793457\n",
      "cnt: 0 - valLoss: 0.6707863807678223 - trainLoss: 0.6697187423706055\n",
      "cnt: 0 - valLoss: 0.6707816123962402 - trainLoss: 0.6697134971618652\n",
      "cnt: 0 - valLoss: 0.670776903629303 - trainLoss: 0.6697083115577698\n",
      "cnt: 0 - valLoss: 0.670772135257721 - trainLoss: 0.6697030663490295\n",
      "cnt: 0 - valLoss: 0.6707673668861389 - trainLoss: 0.6696978211402893\n",
      "cnt: 0 - valLoss: 0.6707626581192017 - trainLoss: 0.6696925163269043\n",
      "cnt: 0 - valLoss: 0.6707578301429749 - trainLoss: 0.6696873903274536\n",
      "cnt: 0 - valLoss: 0.6707531213760376 - trainLoss: 0.6696821451187134\n",
      "cnt: 0 - valLoss: 0.6707484126091003 - trainLoss: 0.6696769595146179\n",
      "cnt: 0 - valLoss: 0.6707437038421631 - trainLoss: 0.6696717739105225\n",
      "cnt: 0 - valLoss: 0.6707389950752258 - trainLoss: 0.669666588306427\n",
      "cnt: 0 - valLoss: 0.6707342863082886 - trainLoss: 0.6696614027023315\n",
      "cnt: 0 - valLoss: 0.6707295775413513 - trainLoss: 0.6696561574935913\n",
      "cnt: 0 - valLoss: 0.6707248687744141 - trainLoss: 0.6696510314941406\n",
      "cnt: 0 - valLoss: 0.6707201600074768 - trainLoss: 0.6696458458900452\n",
      "cnt: 0 - valLoss: 0.6707155108451843 - trainLoss: 0.6696406602859497\n",
      "cnt: 0 - valLoss: 0.6707108020782471 - trainLoss: 0.6696354746818542\n",
      "cnt: 0 - valLoss: 0.6707061529159546 - trainLoss: 0.6696303486824036\n",
      "cnt: 0 - valLoss: 0.6707014441490173 - trainLoss: 0.6696252226829529\n",
      "cnt: 0 - valLoss: 0.6706967949867249 - trainLoss: 0.6696200370788574\n",
      "cnt: 0 - valLoss: 0.6706921458244324 - trainLoss: 0.6696149110794067\n",
      "cnt: 0 - valLoss: 0.6706874966621399 - trainLoss: 0.669609785079956\n",
      "cnt: 0 - valLoss: 0.6706828474998474 - trainLoss: 0.6696045994758606\n",
      "cnt: 0 - valLoss: 0.6706781983375549 - trainLoss: 0.6695995330810547\n",
      "cnt: 0 - valLoss: 0.6706734895706177 - trainLoss: 0.6695944666862488\n",
      "cnt: 0 - valLoss: 0.6706688404083252 - trainLoss: 0.6695892810821533\n",
      "cnt: 0 - valLoss: 0.6706642508506775 - trainLoss: 0.6695842742919922\n",
      "cnt: 0 - valLoss: 0.6706596612930298 - trainLoss: 0.6695791482925415\n",
      "cnt: 0 - valLoss: 0.6706550121307373 - trainLoss: 0.6695740222930908\n",
      "cnt: 0 - valLoss: 0.6706504225730896 - trainLoss: 0.6695689558982849\n",
      "cnt: 0 - valLoss: 0.6706458330154419 - trainLoss: 0.6695638298988342\n",
      "cnt: 0 - valLoss: 0.6706411838531494 - trainLoss: 0.6695587635040283\n",
      "cnt: 0 - valLoss: 0.6706366539001465 - trainLoss: 0.6695536971092224\n",
      "cnt: 0 - valLoss: 0.670632004737854 - trainLoss: 0.6695486307144165\n",
      "cnt: 0 - valLoss: 0.6706274151802063 - trainLoss: 0.6695435643196106\n",
      "cnt: 0 - valLoss: 0.6706228852272034 - trainLoss: 0.6695385575294495\n",
      "cnt: 0 - valLoss: 0.6706182956695557 - trainLoss: 0.6695334911346436\n",
      "cnt: 0 - valLoss: 0.670613706111908 - trainLoss: 0.6695284247398376\n",
      "cnt: 0 - valLoss: 0.6706091165542603 - trainLoss: 0.6695234179496765\n",
      "cnt: 0 - valLoss: 0.6706045866012573 - trainLoss: 0.6695183515548706\n",
      "cnt: 0 - valLoss: 0.6706000566482544 - trainLoss: 0.6695134043693542\n",
      "cnt: 0 - valLoss: 0.6705954670906067 - trainLoss: 0.6695083379745483\n",
      "cnt: 0 - valLoss: 0.6705909371376038 - trainLoss: 0.6695033311843872\n",
      "cnt: 0 - valLoss: 0.6705864071846008 - trainLoss: 0.6694983243942261\n",
      "cnt: 0 - valLoss: 0.6705818772315979 - trainLoss: 0.6694933176040649\n",
      "cnt: 0 - valLoss: 0.6705774068832397 - trainLoss: 0.6694883108139038\n",
      "cnt: 0 - valLoss: 0.6705727577209473 - trainLoss: 0.6694833040237427\n",
      "cnt: 0 - valLoss: 0.6705682873725891 - trainLoss: 0.6694782972335815\n",
      "cnt: 0 - valLoss: 0.6705637574195862 - trainLoss: 0.66947340965271\n",
      "cnt: 0 - valLoss: 0.670559287071228 - trainLoss: 0.6694684624671936\n",
      "cnt: 0 - valLoss: 0.6705547571182251 - trainLoss: 0.6694634556770325\n",
      "cnt: 0 - valLoss: 0.6705502867698669 - trainLoss: 0.6694585084915161\n",
      "cnt: 0 - valLoss: 0.6705458164215088 - trainLoss: 0.669453501701355\n",
      "cnt: 0 - valLoss: 0.6705412864685059 - trainLoss: 0.6694485545158386\n",
      "cnt: 0 - valLoss: 0.6705368757247925 - trainLoss: 0.6694436073303223\n",
      "cnt: 0 - valLoss: 0.6705324053764343 - trainLoss: 0.6694386601448059\n",
      "cnt: 0 - valLoss: 0.6705278754234314 - trainLoss: 0.6694337725639343\n",
      "cnt: 0 - valLoss: 0.6705234050750732 - trainLoss: 0.669428825378418\n",
      "cnt: 0 - valLoss: 0.6705189943313599 - trainLoss: 0.6694238185882568\n",
      "cnt: 0 - valLoss: 0.6705144643783569 - trainLoss: 0.6694189310073853\n",
      "cnt: 0 - valLoss: 0.6705100536346436 - trainLoss: 0.6694140434265137\n",
      "cnt: 0 - valLoss: 0.6705056428909302 - trainLoss: 0.6694090962409973\n",
      "cnt: 0 - valLoss: 0.670501172542572 - trainLoss: 0.6694042086601257\n",
      "cnt: 0 - valLoss: 0.6704967617988586 - trainLoss: 0.6693993210792542\n",
      "cnt: 0 - valLoss: 0.6704923510551453 - trainLoss: 0.6693944334983826\n",
      "cnt: 0 - valLoss: 0.6704878807067871 - trainLoss: 0.6693896055221558\n",
      "cnt: 0 - valLoss: 0.6704834699630737 - trainLoss: 0.6693847179412842\n",
      "cnt: 0 - valLoss: 0.6704790592193604 - trainLoss: 0.6693798303604126\n",
      "cnt: 0 - valLoss: 0.670474648475647 - trainLoss: 0.669374942779541\n",
      "cnt: 0 - valLoss: 0.6704702377319336 - trainLoss: 0.6693700551986694\n",
      "cnt: 0 - valLoss: 0.670465886592865 - trainLoss: 0.6693651676177979\n",
      "cnt: 0 - valLoss: 0.6704614758491516 - trainLoss: 0.669360339641571\n",
      "cnt: 0 - valLoss: 0.6704570651054382 - trainLoss: 0.6693554520606995\n",
      "cnt: 0 - valLoss: 0.6704526543617249 - trainLoss: 0.6693506240844727\n",
      "cnt: 0 - valLoss: 0.670448362827301 - trainLoss: 0.6693458557128906\n",
      "cnt: 0 - valLoss: 0.6704439520835876 - trainLoss: 0.669340968132019\n",
      "cnt: 0 - valLoss: 0.6704395413398743 - trainLoss: 0.669336199760437\n",
      "cnt: 0 - valLoss: 0.6704352498054504 - trainLoss: 0.6693313121795654\n",
      "cnt: 0 - valLoss: 0.6704308390617371 - trainLoss: 0.6693265438079834\n",
      "cnt: 0 - valLoss: 0.6704265475273132 - trainLoss: 0.6693217158317566\n",
      "cnt: 0 - valLoss: 0.6704221367835999 - trainLoss: 0.6693168878555298\n",
      "cnt: 0 - valLoss: 0.670417845249176 - trainLoss: 0.6693121194839478\n",
      "cnt: 0 - valLoss: 0.6704134941101074 - trainLoss: 0.669307291507721\n",
      "cnt: 0 - valLoss: 0.6704091429710388 - trainLoss: 0.6693025231361389\n",
      "cnt: 0 - valLoss: 0.6704047918319702 - trainLoss: 0.6692977547645569\n",
      "cnt: 0 - valLoss: 0.6704004406929016 - trainLoss: 0.6692929267883301\n",
      "cnt: 0 - valLoss: 0.6703961491584778 - trainLoss: 0.669288158416748\n",
      "cnt: 0 - valLoss: 0.6703917980194092 - trainLoss: 0.669283390045166\n",
      "cnt: 0 - valLoss: 0.6703875064849854 - trainLoss: 0.6692786812782288\n",
      "cnt: 0 - valLoss: 0.6703832149505615 - trainLoss: 0.6692739129066467\n",
      "cnt: 0 - valLoss: 0.6703789234161377 - trainLoss: 0.6692691445350647\n",
      "cnt: 0 - valLoss: 0.6703746318817139 - trainLoss: 0.6692644357681274\n",
      "cnt: 0 - valLoss: 0.6703702807426453 - trainLoss: 0.6692596673965454\n",
      "cnt: 0 - valLoss: 0.6703660488128662 - trainLoss: 0.6692549586296082\n",
      "cnt: 0 - valLoss: 0.6703616976737976 - trainLoss: 0.6692502498626709\n",
      "cnt: 0 - valLoss: 0.6703574657440186 - trainLoss: 0.6692454814910889\n",
      "cnt: 0 - valLoss: 0.6703531742095947 - trainLoss: 0.6692407131195068\n",
      "cnt: 0 - valLoss: 0.6703489422798157 - trainLoss: 0.6692360043525696\n",
      "cnt: 0 - valLoss: 0.6703446507453918 - trainLoss: 0.6692312955856323\n",
      "cnt: 0 - valLoss: 0.6703404784202576 - trainLoss: 0.6692265868186951\n",
      "cnt: 0 - valLoss: 0.6703361868858337 - trainLoss: 0.6692219376564026\n",
      "cnt: 0 - valLoss: 0.6703319549560547 - trainLoss: 0.6692172288894653\n",
      "cnt: 0 - valLoss: 0.6703276634216309 - trainLoss: 0.6692125797271729\n",
      "cnt: 0 - valLoss: 0.6703234314918518 - trainLoss: 0.6692078709602356\n",
      "cnt: 0 - valLoss: 0.6703191995620728 - trainLoss: 0.6692032217979431\n",
      "cnt: 0 - valLoss: 0.6703149676322937 - trainLoss: 0.6691985130310059\n",
      "cnt: 0 - valLoss: 0.6703107953071594 - trainLoss: 0.6691938638687134\n",
      "cnt: 0 - valLoss: 0.6703065037727356 - trainLoss: 0.6691892147064209\n",
      "cnt: 0 - valLoss: 0.6703023314476013 - trainLoss: 0.6691845655441284\n",
      "cnt: 0 - valLoss: 0.6702980995178223 - trainLoss: 0.6691798567771912\n",
      "cnt: 0 - valLoss: 0.670293927192688 - trainLoss: 0.6691752672195435\n",
      "cnt: 0 - valLoss: 0.6702896952629089 - trainLoss: 0.669170618057251\n",
      "cnt: 0 - valLoss: 0.6702855825424194 - trainLoss: 0.6691659092903137\n",
      "cnt: 0 - valLoss: 0.6702813506126404 - trainLoss: 0.669161319732666\n",
      "cnt: 0 - valLoss: 0.6702771782875061 - trainLoss: 0.6691567301750183\n",
      "cnt: 0 - valLoss: 0.6702730059623718 - trainLoss: 0.6691520810127258\n",
      "cnt: 0 - valLoss: 0.6702687740325928 - trainLoss: 0.6691474914550781\n",
      "cnt: 0 - valLoss: 0.6702646613121033 - trainLoss: 0.6691429018974304\n",
      "cnt: 0 - valLoss: 0.670260488986969 - trainLoss: 0.6691382527351379\n",
      "cnt: 0 - valLoss: 0.6702562570571899 - trainLoss: 0.6691336631774902\n",
      "cnt: 0 - valLoss: 0.6702521443367004 - trainLoss: 0.6691290736198425\n",
      "cnt: 0 - valLoss: 0.6702480316162109 - trainLoss: 0.6691244840621948\n",
      "cnt: 0 - valLoss: 0.6702439188957214 - trainLoss: 0.6691198945045471\n",
      "cnt: 0 - valLoss: 0.6702397465705872 - trainLoss: 0.6691153049468994\n",
      "cnt: 0 - valLoss: 0.6702356338500977 - trainLoss: 0.6691107749938965\n",
      "cnt: 0 - valLoss: 0.6702315211296082 - trainLoss: 0.669106125831604\n",
      "cnt: 0 - valLoss: 0.6702272891998291 - trainLoss: 0.6691015958786011\n",
      "cnt: 0 - valLoss: 0.6702231764793396 - trainLoss: 0.6690970659255981\n",
      "cnt: 0 - valLoss: 0.6702190637588501 - trainLoss: 0.6690925359725952\n",
      "cnt: 0 - valLoss: 0.6702150106430054 - trainLoss: 0.6690879464149475\n",
      "cnt: 0 - valLoss: 0.6702108979225159 - trainLoss: 0.6690834164619446\n",
      "cnt: 0 - valLoss: 0.6702067852020264 - trainLoss: 0.6690788269042969\n",
      "cnt: 0 - valLoss: 0.6702027320861816 - trainLoss: 0.669074296951294\n",
      "cnt: 0 - valLoss: 0.6701986193656921 - trainLoss: 0.669069766998291\n",
      "cnt: 0 - valLoss: 0.6701945662498474 - trainLoss: 0.6690652370452881\n",
      "cnt: 0 - valLoss: 0.6701903939247131 - trainLoss: 0.6690607666969299\n",
      "cnt: 0 - valLoss: 0.6701863408088684 - trainLoss: 0.669056236743927\n",
      "cnt: 0 - valLoss: 0.6701822876930237 - trainLoss: 0.6690517067909241\n",
      "cnt: 0 - valLoss: 0.6701781749725342 - trainLoss: 0.6690472364425659\n",
      "cnt: 0 - valLoss: 0.6701741814613342 - trainLoss: 0.669042706489563\n",
      "cnt: 0 - valLoss: 0.6701700687408447 - trainLoss: 0.6690382361412048\n",
      "cnt: 0 - valLoss: 0.670166015625 - trainLoss: 0.6690337657928467\n",
      "cnt: 0 - valLoss: 0.6701619625091553 - trainLoss: 0.6690292358398438\n",
      "cnt: 0 - valLoss: 0.6701579093933105 - trainLoss: 0.6690247654914856\n",
      "cnt: 0 - valLoss: 0.6701539158821106 - trainLoss: 0.6690202951431274\n",
      "cnt: 0 - valLoss: 0.6701498627662659 - trainLoss: 0.6690158247947693\n",
      "cnt: 0 - valLoss: 0.6701458096504211 - trainLoss: 0.6690114140510559\n",
      "cnt: 0 - valLoss: 0.6701418161392212 - trainLoss: 0.6690069437026978\n",
      "cnt: 0 - valLoss: 0.6701377630233765 - trainLoss: 0.6690024733543396\n",
      "cnt: 0 - valLoss: 0.6701337695121765 - trainLoss: 0.6689980626106262\n",
      "cnt: 0 - valLoss: 0.6701297760009766 - trainLoss: 0.6689935922622681\n",
      "cnt: 0 - valLoss: 0.6701257228851318 - trainLoss: 0.6689891219139099\n",
      "cnt: 0 - valLoss: 0.6701217293739319 - trainLoss: 0.6689847111701965\n",
      "cnt: 0 - valLoss: 0.6701177358627319 - trainLoss: 0.6689802408218384\n",
      "cnt: 0 - valLoss: 0.670113742351532 - trainLoss: 0.6689758896827698\n",
      "cnt: 0 - valLoss: 0.6701098084449768 - trainLoss: 0.6689714193344116\n",
      "cnt: 0 - valLoss: 0.6701057553291321 - trainLoss: 0.668967068195343\n",
      "cnt: 0 - valLoss: 0.6701018214225769 - trainLoss: 0.6689625978469849\n",
      "cnt: 0 - valLoss: 0.670097827911377 - trainLoss: 0.6689581871032715\n",
      "cnt: 0 - valLoss: 0.6700938940048218 - trainLoss: 0.6689538359642029\n",
      "cnt: 0 - valLoss: 0.6700899600982666 - trainLoss: 0.6689494252204895\n",
      "cnt: 0 - valLoss: 0.6700859665870667 - trainLoss: 0.6689450144767761\n",
      "cnt: 0 - valLoss: 0.6700820326805115 - trainLoss: 0.6689406633377075\n",
      "cnt: 0 - valLoss: 0.6700780391693115 - trainLoss: 0.6689362525939941\n",
      "cnt: 0 - valLoss: 0.6700741052627563 - trainLoss: 0.6689319014549255\n",
      "cnt: 0 - valLoss: 0.6700701117515564 - trainLoss: 0.6689274907112122\n",
      "cnt: 0 - valLoss: 0.670066237449646 - trainLoss: 0.6689231395721436\n",
      "cnt: 0 - valLoss: 0.6700623035430908 - trainLoss: 0.668918788433075\n",
      "cnt: 0 - valLoss: 0.6700583696365356 - trainLoss: 0.6689144372940063\n",
      "cnt: 0 - valLoss: 0.6700544357299805 - trainLoss: 0.668910026550293\n",
      "cnt: 0 - valLoss: 0.6700505614280701 - trainLoss: 0.6689057350158691\n",
      "cnt: 0 - valLoss: 0.6700466275215149 - trainLoss: 0.6689013838768005\n",
      "cnt: 0 - valLoss: 0.6700427532196045 - trainLoss: 0.6688970923423767\n",
      "cnt: 0 - valLoss: 0.6700388193130493 - trainLoss: 0.6688927412033081\n",
      "cnt: 0 - valLoss: 0.6700349450111389 - trainLoss: 0.6688884496688843\n",
      "cnt: 0 - valLoss: 0.6700310707092285 - trainLoss: 0.6688840985298157\n",
      "cnt: 0 - valLoss: 0.6700271368026733 - trainLoss: 0.6688798069953918\n",
      "cnt: 0 - valLoss: 0.6700232028961182 - trainLoss: 0.668875515460968\n",
      "cnt: 0 - valLoss: 0.6700193285942078 - trainLoss: 0.6688711643218994\n",
      "cnt: 0 - valLoss: 0.6700155138969421 - trainLoss: 0.6688668727874756\n",
      "cnt: 0 - valLoss: 0.6700116395950317 - trainLoss: 0.6688625812530518\n",
      "cnt: 0 - valLoss: 0.6700077056884766 - trainLoss: 0.6688582301139832\n",
      "cnt: 0 - valLoss: 0.6700038313865662 - trainLoss: 0.6688539981842041\n",
      "cnt: 0 - valLoss: 0.6700000166893005 - trainLoss: 0.6688497066497803\n",
      "cnt: 0 - valLoss: 0.6699961423873901 - trainLoss: 0.6688454151153564\n",
      "cnt: 0 - valLoss: 0.6699922680854797 - trainLoss: 0.6688411831855774\n",
      "cnt: 0 - valLoss: 0.6699884533882141 - trainLoss: 0.6688368916511536\n",
      "cnt: 0 - valLoss: 0.6699845790863037 - trainLoss: 0.6688326597213745\n",
      "cnt: 0 - valLoss: 0.6699808239936829 - trainLoss: 0.6688283681869507\n",
      "cnt: 0 - valLoss: 0.6699769496917725 - trainLoss: 0.6688240766525269\n",
      "cnt: 0 - valLoss: 0.6699731349945068 - trainLoss: 0.6688198447227478\n",
      "cnt: 0 - valLoss: 0.6699692606925964 - trainLoss: 0.6688156127929688\n",
      "cnt: 0 - valLoss: 0.6699655055999756 - trainLoss: 0.6688114404678345\n",
      "cnt: 0 - valLoss: 0.6699616312980652 - trainLoss: 0.6688071489334106\n",
      "cnt: 0 - valLoss: 0.6699578762054443 - trainLoss: 0.6688029170036316\n",
      "cnt: 0 - valLoss: 0.6699540615081787 - trainLoss: 0.6687987446784973\n",
      "cnt: 0 - valLoss: 0.6699502468109131 - trainLoss: 0.6687944531440735\n",
      "cnt: 0 - valLoss: 0.6699464917182922 - trainLoss: 0.6687902808189392\n",
      "cnt: 0 - valLoss: 0.6699426770210266 - trainLoss: 0.6687860488891602\n",
      "cnt: 0 - valLoss: 0.669938862323761 - trainLoss: 0.6687818765640259\n",
      "cnt: 0 - valLoss: 0.6699351072311401 - trainLoss: 0.6687776446342468\n",
      "cnt: 0 - valLoss: 0.6699312925338745 - trainLoss: 0.6687734127044678\n",
      "cnt: 0 - valLoss: 0.6699275374412537 - trainLoss: 0.6687692403793335\n",
      "cnt: 0 - valLoss: 0.6699237823486328 - trainLoss: 0.6687650084495544\n",
      "cnt: 0 - valLoss: 0.669920027256012 - trainLoss: 0.6687608957290649\n",
      "cnt: 0 - valLoss: 0.6699162125587463 - trainLoss: 0.6687567234039307\n",
      "cnt: 0 - valLoss: 0.6699125170707703 - trainLoss: 0.6687525510787964\n",
      "cnt: 0 - valLoss: 0.6699087619781494 - trainLoss: 0.6687483191490173\n",
      "cnt: 0 - valLoss: 0.6699050068855286 - trainLoss: 0.6687442064285278\n",
      "cnt: 0 - valLoss: 0.6699011921882629 - trainLoss: 0.6687400341033936\n",
      "cnt: 0 - valLoss: 0.6698975563049316 - trainLoss: 0.6687358617782593\n",
      "cnt: 0 - valLoss: 0.669893741607666 - trainLoss: 0.6687317490577698\n",
      "cnt: 0 - valLoss: 0.6698900461196899 - trainLoss: 0.6687275767326355\n",
      "cnt: 0 - valLoss: 0.6698862910270691 - trainLoss: 0.6687234044075012\n",
      "cnt: 0 - valLoss: 0.669882595539093 - trainLoss: 0.6687192916870117\n",
      "cnt: 0 - valLoss: 0.6698789000511169 - trainLoss: 0.668715238571167\n",
      "cnt: 0 - valLoss: 0.6698751449584961 - trainLoss: 0.6687110662460327\n",
      "cnt: 0 - valLoss: 0.66987144947052 - trainLoss: 0.6687068939208984\n",
      "cnt: 0 - valLoss: 0.6698676943778992 - trainLoss: 0.6687027812004089\n",
      "cnt: 0 - valLoss: 0.6698639988899231 - trainLoss: 0.6686986684799194\n",
      "cnt: 0 - valLoss: 0.669860303401947 - trainLoss: 0.6686945557594299\n",
      "cnt: 0 - valLoss: 0.6698566675186157 - trainLoss: 0.6686904430389404\n",
      "cnt: 0 - valLoss: 0.6698529720306396 - trainLoss: 0.6686863899230957\n",
      "cnt: 0 - valLoss: 0.6698492765426636 - trainLoss: 0.6686822772026062\n",
      "cnt: 0 - valLoss: 0.6698456406593323 - trainLoss: 0.6686781644821167\n",
      "cnt: 0 - valLoss: 0.6698418855667114 - trainLoss: 0.668674111366272\n",
      "cnt: 0 - valLoss: 0.6698382496833801 - trainLoss: 0.6686699986457825\n",
      "cnt: 0 - valLoss: 0.6698346138000488 - trainLoss: 0.6686659455299377\n",
      "cnt: 0 - valLoss: 0.6698309183120728 - trainLoss: 0.668661892414093\n",
      "cnt: 0 - valLoss: 0.6698272824287415 - trainLoss: 0.6686578392982483\n",
      "cnt: 0 - valLoss: 0.6698236465454102 - trainLoss: 0.6686537265777588\n",
      "cnt: 0 - valLoss: 0.6698199510574341 - trainLoss: 0.6686496138572693\n",
      "cnt: 0 - valLoss: 0.6698163151741028 - trainLoss: 0.6686456799507141\n",
      "cnt: 0 - valLoss: 0.6698126792907715 - trainLoss: 0.6686415672302246\n",
      "cnt: 0 - valLoss: 0.6698090434074402 - trainLoss: 0.6686375737190247\n",
      "cnt: 0 - valLoss: 0.6698054671287537 - trainLoss: 0.6686335206031799\n",
      "cnt: 0 - valLoss: 0.6698017716407776 - trainLoss: 0.6686294078826904\n",
      "cnt: 0 - valLoss: 0.6697981953620911 - trainLoss: 0.6686254143714905\n",
      "cnt: 0 - valLoss: 0.6697945594787598 - trainLoss: 0.6686213612556458\n",
      "cnt: 0 - valLoss: 0.6697909832000732 - trainLoss: 0.6686173677444458\n",
      "cnt: 0 - valLoss: 0.6697873473167419 - trainLoss: 0.6686133146286011\n",
      "cnt: 0 - valLoss: 0.6697837114334106 - trainLoss: 0.6686093807220459\n",
      "cnt: 0 - valLoss: 0.6697800755500793 - trainLoss: 0.6686052680015564\n",
      "cnt: 0 - valLoss: 0.6697764992713928 - trainLoss: 0.6686013340950012\n",
      "cnt: 0 - valLoss: 0.6697728633880615 - trainLoss: 0.6685973405838013\n",
      "cnt: 0 - valLoss: 0.6697693467140198 - trainLoss: 0.6685932874679565\n",
      "cnt: 0 - valLoss: 0.6697657108306885 - trainLoss: 0.6685893535614014\n",
      "cnt: 0 - valLoss: 0.6697621941566467 - trainLoss: 0.6685853600502014\n",
      "cnt: 0 - valLoss: 0.6697585582733154 - trainLoss: 0.6685813665390015\n",
      "cnt: 0 - valLoss: 0.6697550415992737 - trainLoss: 0.6685773730278015\n",
      "cnt: 0 - valLoss: 0.6697514653205872 - trainLoss: 0.6685733795166016\n",
      "cnt: 0 - valLoss: 0.6697478294372559 - trainLoss: 0.6685694456100464\n",
      "cnt: 0 - valLoss: 0.6697443127632141 - trainLoss: 0.6685655117034912\n",
      "cnt: 0 - valLoss: 0.6697407364845276 - trainLoss: 0.668561577796936\n",
      "cnt: 0 - valLoss: 0.6697372198104858 - trainLoss: 0.6685576438903809\n",
      "cnt: 0 - valLoss: 0.6697336435317993 - trainLoss: 0.6685536503791809\n",
      "cnt: 0 - valLoss: 0.6697301268577576 - trainLoss: 0.668549656867981\n",
      "cnt: 0 - valLoss: 0.6697266101837158 - trainLoss: 0.6685457229614258\n",
      "cnt: 0 - valLoss: 0.6697230339050293 - trainLoss: 0.6685417890548706\n",
      "cnt: 0 - valLoss: 0.6697194576263428 - trainLoss: 0.6685378551483154\n",
      "cnt: 0 - valLoss: 0.669715940952301 - trainLoss: 0.6685339212417603\n",
      "cnt: 0 - valLoss: 0.6697124242782593 - trainLoss: 0.6685299873352051\n",
      "cnt: 0 - valLoss: 0.6697089076042175 - trainLoss: 0.6685260534286499\n",
      "cnt: 0 - valLoss: 0.6697054505348206 - trainLoss: 0.6685221195220947\n",
      "cnt: 0 - valLoss: 0.6697019338607788 - trainLoss: 0.6685182452201843\n",
      "cnt: 0 - valLoss: 0.6696984171867371 - trainLoss: 0.6685143113136292\n",
      "cnt: 0 - valLoss: 0.6696949005126953 - trainLoss: 0.668510377407074\n",
      "cnt: 0 - valLoss: 0.6696913838386536 - trainLoss: 0.6685065627098083\n",
      "cnt: 0 - valLoss: 0.6696878671646118 - trainLoss: 0.6685026288032532\n",
      "cnt: 0 - valLoss: 0.6696843504905701 - trainLoss: 0.668498694896698\n",
      "cnt: 0 - valLoss: 0.6696808934211731 - trainLoss: 0.6684948801994324\n",
      "cnt: 0 - valLoss: 0.6696774959564209 - trainLoss: 0.6684909462928772\n",
      "cnt: 0 - valLoss: 0.6696739792823792 - trainLoss: 0.6684870719909668\n",
      "cnt: 0 - valLoss: 0.6696704626083374 - trainLoss: 0.6684831976890564\n",
      "cnt: 0 - valLoss: 0.6696670055389404 - trainLoss: 0.6684793829917908\n",
      "cnt: 0 - valLoss: 0.6696634888648987 - trainLoss: 0.6684755086898804\n",
      "cnt: 0 - valLoss: 0.6696600914001465 - trainLoss: 0.66847163438797\n",
      "cnt: 0 - valLoss: 0.6696565747261047 - trainLoss: 0.6684677600860596\n",
      "cnt: 0 - valLoss: 0.6696531772613525 - trainLoss: 0.6684638857841492\n",
      "cnt: 0 - valLoss: 0.6696497201919556 - trainLoss: 0.6684600114822388\n",
      "cnt: 0 - valLoss: 0.6696462631225586 - trainLoss: 0.6684561967849731\n",
      "cnt: 0 - valLoss: 0.6696428656578064 - trainLoss: 0.6684523820877075\n",
      "cnt: 0 - valLoss: 0.6696394085884094 - trainLoss: 0.6684485673904419\n",
      "cnt: 0 - valLoss: 0.6696359515190125 - trainLoss: 0.6684446930885315\n",
      "cnt: 0 - valLoss: 0.6696325540542603 - trainLoss: 0.6684408783912659\n",
      "cnt: 0 - valLoss: 0.6696290969848633 - trainLoss: 0.6684370636940002\n",
      "cnt: 0 - valLoss: 0.6696256399154663 - trainLoss: 0.6684332489967346\n",
      "cnt: 0 - valLoss: 0.6696222424507141 - trainLoss: 0.668429434299469\n",
      "cnt: 0 - valLoss: 0.6696188449859619 - trainLoss: 0.6684256196022034\n",
      "cnt: 0 - valLoss: 0.6696153879165649 - trainLoss: 0.6684218049049377\n",
      "cnt: 0 - valLoss: 0.6696119904518127 - trainLoss: 0.6684179902076721\n",
      "cnt: 0 - valLoss: 0.6696085929870605 - trainLoss: 0.6684142351150513\n",
      "cnt: 0 - valLoss: 0.6696051955223083 - trainLoss: 0.6684103608131409\n",
      "cnt: 0 - valLoss: 0.6696017980575562 - trainLoss: 0.66840660572052\n",
      "cnt: 0 - valLoss: 0.6695984601974487 - trainLoss: 0.6684028506278992\n",
      "cnt: 0 - valLoss: 0.6695950031280518 - trainLoss: 0.6683990955352783\n",
      "cnt: 0 - valLoss: 0.6695916652679443 - trainLoss: 0.6683952808380127\n",
      "cnt: 0 - valLoss: 0.6695882678031921 - trainLoss: 0.6683915257453918\n",
      "cnt: 0 - valLoss: 0.6695848703384399 - trainLoss: 0.6683877110481262\n",
      "cnt: 0 - valLoss: 0.6695815324783325 - trainLoss: 0.6683840155601501\n",
      "cnt: 0 - valLoss: 0.6695781946182251 - trainLoss: 0.6683802604675293\n",
      "cnt: 0 - valLoss: 0.6695747971534729 - trainLoss: 0.6683765053749084\n",
      "cnt: 0 - valLoss: 0.6695714592933655 - trainLoss: 0.6683726906776428\n",
      "cnt: 0 - valLoss: 0.6695681214332581 - trainLoss: 0.668368935585022\n",
      "cnt: 0 - valLoss: 0.6695647239685059 - trainLoss: 0.6683652400970459\n",
      "cnt: 0 - valLoss: 0.6695613861083984 - trainLoss: 0.6683614253997803\n",
      "cnt: 0 - valLoss: 0.669558048248291 - trainLoss: 0.6683577299118042\n",
      "cnt: 0 - valLoss: 0.6695547103881836 - trainLoss: 0.6683539748191833\n",
      "cnt: 0 - valLoss: 0.6695513129234314 - trainLoss: 0.6683502793312073\n",
      "cnt: 0 - valLoss: 0.669547975063324 - trainLoss: 0.6683465838432312\n",
      "cnt: 0 - valLoss: 0.6695446968078613 - trainLoss: 0.6683428287506104\n",
      "cnt: 0 - valLoss: 0.6695413589477539 - trainLoss: 0.6683391332626343\n",
      "cnt: 0 - valLoss: 0.6695380806922913 - trainLoss: 0.6683354377746582\n",
      "cnt: 0 - valLoss: 0.6695347428321838 - trainLoss: 0.6683316826820374\n",
      "cnt: 0 - valLoss: 0.6695314049720764 - trainLoss: 0.668328046798706\n",
      "cnt: 0 - valLoss: 0.6695281267166138 - trainLoss: 0.66832435131073\n",
      "cnt: 0 - valLoss: 0.6695247888565063 - trainLoss: 0.6683205962181091\n",
      "cnt: 0 - valLoss: 0.6695214509963989 - trainLoss: 0.6683169007301331\n",
      "cnt: 0 - valLoss: 0.669518232345581 - trainLoss: 0.668313205242157\n",
      "cnt: 0 - valLoss: 0.6695148944854736 - trainLoss: 0.6683095693588257\n",
      "cnt: 0 - valLoss: 0.669511616230011 - trainLoss: 0.6683058738708496\n",
      "cnt: 0 - valLoss: 0.6695083379745483 - trainLoss: 0.6683022379875183\n",
      "cnt: 0 - valLoss: 0.6695050597190857 - trainLoss: 0.6682985424995422\n",
      "cnt: 0 - valLoss: 0.6695017218589783 - trainLoss: 0.6682949066162109\n",
      "cnt: 0 - valLoss: 0.6694985032081604 - trainLoss: 0.6682911515235901\n",
      "cnt: 0 - valLoss: 0.6694952249526978 - trainLoss: 0.6682875752449036\n",
      "cnt: 0 - valLoss: 0.6694919466972351 - trainLoss: 0.6682839393615723\n",
      "cnt: 0 - valLoss: 0.6694887280464172 - trainLoss: 0.6682802438735962\n",
      "cnt: 0 - valLoss: 0.6694854497909546 - trainLoss: 0.6682766079902649\n",
      "cnt: 0 - valLoss: 0.6694821715354919 - trainLoss: 0.6682729721069336\n",
      "cnt: 0 - valLoss: 0.6694789528846741 - trainLoss: 0.6682692766189575\n",
      "cnt: 0 - valLoss: 0.6694756150245667 - trainLoss: 0.668265700340271\n",
      "cnt: 0 - valLoss: 0.6694724559783936 - trainLoss: 0.6682620644569397\n",
      "cnt: 0 - valLoss: 0.6694691777229309 - trainLoss: 0.6682584285736084\n",
      "cnt: 0 - valLoss: 0.6694658994674683 - trainLoss: 0.6682548522949219\n",
      "cnt: 0 - valLoss: 0.6694626808166504 - trainLoss: 0.6682512164115906\n",
      "cnt: 0 - valLoss: 0.6694594621658325 - trainLoss: 0.6682475805282593\n",
      "cnt: 0 - valLoss: 0.6694562435150146 - trainLoss: 0.6682440042495728\n",
      "cnt: 0 - valLoss: 0.6694530248641968 - trainLoss: 0.6682403683662415\n",
      "cnt: 0 - valLoss: 0.6694498062133789 - trainLoss: 0.6682367920875549\n",
      "cnt: 0 - valLoss: 0.669446587562561 - trainLoss: 0.6682331562042236\n",
      "cnt: 0 - valLoss: 0.6694433689117432 - trainLoss: 0.6682295799255371\n",
      "cnt: 0 - valLoss: 0.6694401502609253 - trainLoss: 0.6682260036468506\n",
      "cnt: 0 - valLoss: 0.6694369316101074 - trainLoss: 0.6682223677635193\n",
      "cnt: 0 - valLoss: 0.6694337725639343 - trainLoss: 0.6682188510894775\n",
      "cnt: 0 - valLoss: 0.6694305539131165 - trainLoss: 0.668215274810791\n",
      "cnt: 0 - valLoss: 0.6694273948669434 - trainLoss: 0.6682116985321045\n",
      "cnt: 0 - valLoss: 0.6694242358207703 - trainLoss: 0.668208122253418\n",
      "cnt: 0 - valLoss: 0.6694210171699524 - trainLoss: 0.6682044863700867\n",
      "cnt: 0 - valLoss: 0.6694178581237793 - trainLoss: 0.6682009696960449\n",
      "cnt: 0 - valLoss: 0.6694146394729614 - trainLoss: 0.6681974530220032\n",
      "cnt: 0 - valLoss: 0.6694114804267883 - trainLoss: 0.6681938767433167\n",
      "cnt: 0 - valLoss: 0.6694083213806152 - trainLoss: 0.6681903600692749\n",
      "cnt: 0 - valLoss: 0.6694051623344421 - trainLoss: 0.6681867837905884\n",
      "cnt: 0 - valLoss: 0.6694019436836243 - trainLoss: 0.6681832075119019\n",
      "cnt: 0 - valLoss: 0.669398844242096 - trainLoss: 0.6681796908378601\n",
      "cnt: 0 - valLoss: 0.6693956255912781 - trainLoss: 0.6681761741638184\n",
      "cnt: 0 - valLoss: 0.6693925261497498 - trainLoss: 0.6681726574897766\n",
      "cnt: 0 - valLoss: 0.6693893671035767 - trainLoss: 0.6681690812110901\n",
      "cnt: 0 - valLoss: 0.6693862080574036 - trainLoss: 0.6681656241416931\n",
      "cnt: 0 - valLoss: 0.6693831086158752 - trainLoss: 0.6681620478630066\n",
      "cnt: 0 - valLoss: 0.6693799495697021 - trainLoss: 0.6681585907936096\n",
      "cnt: 0 - valLoss: 0.6693767309188843 - trainLoss: 0.6681550741195679\n",
      "cnt: 0 - valLoss: 0.6693736910820007 - trainLoss: 0.6681515574455261\n",
      "cnt: 0 - valLoss: 0.6693705320358276 - trainLoss: 0.6681479811668396\n",
      "cnt: 0 - valLoss: 0.6693674325942993 - trainLoss: 0.6681445837020874\n",
      "cnt: 0 - valLoss: 0.669364333152771 - trainLoss: 0.6681410670280457\n",
      "cnt: 0 - valLoss: 0.6693612337112427 - trainLoss: 0.6681375503540039\n",
      "cnt: 0 - valLoss: 0.6693580746650696 - trainLoss: 0.6681340336799622\n",
      "cnt: 0 - valLoss: 0.6693549156188965 - trainLoss: 0.66813063621521\n",
      "cnt: 0 - valLoss: 0.6693518161773682 - trainLoss: 0.6681271195411682\n",
      "cnt: 0 - valLoss: 0.6693487167358398 - trainLoss: 0.6681236028671265\n",
      "cnt: 0 - valLoss: 0.6693456172943115 - trainLoss: 0.6681202054023743\n",
      "cnt: 0 - valLoss: 0.669342577457428 - trainLoss: 0.6681166887283325\n",
      "cnt: 0 - valLoss: 0.6693394780158997 - trainLoss: 0.6681132316589355\n",
      "cnt: 0 - valLoss: 0.6693363785743713 - trainLoss: 0.6681097745895386\n",
      "cnt: 0 - valLoss: 0.669333279132843 - trainLoss: 0.6681063175201416\n",
      "cnt: 0 - valLoss: 0.6693302392959595 - trainLoss: 0.6681029200553894\n",
      "cnt: 0 - valLoss: 0.6693271398544312 - trainLoss: 0.6680994033813477\n",
      "cnt: 0 - valLoss: 0.6693240404129028 - trainLoss: 0.6680959463119507\n",
      "cnt: 0 - valLoss: 0.6693209409713745 - trainLoss: 0.6680924892425537\n",
      "cnt: 0 - valLoss: 0.6693178415298462 - trainLoss: 0.6680890917778015\n",
      "cnt: 0 - valLoss: 0.6693148016929626 - trainLoss: 0.6680856943130493\n",
      "cnt: 0 - valLoss: 0.6693117618560791 - trainLoss: 0.6680821776390076\n",
      "cnt: 0 - valLoss: 0.6693087220191956 - trainLoss: 0.6680787801742554\n",
      "cnt: 0 - valLoss: 0.6693056225776672 - trainLoss: 0.6680753231048584\n",
      "cnt: 0 - valLoss: 0.6693025827407837 - trainLoss: 0.6680719256401062\n",
      "cnt: 0 - valLoss: 0.6692995429039001 - trainLoss: 0.668068528175354\n",
      "cnt: 0 - valLoss: 0.6692965030670166 - trainLoss: 0.668065071105957\n",
      "cnt: 0 - valLoss: 0.6692934632301331 - trainLoss: 0.6680617332458496\n",
      "cnt: 0 - valLoss: 0.6692904233932495 - trainLoss: 0.6680582761764526\n",
      "cnt: 0 - valLoss: 0.669287383556366 - trainLoss: 0.6680549383163452\n",
      "cnt: 0 - valLoss: 0.6692843437194824 - trainLoss: 0.6680514812469482\n",
      "cnt: 0 - valLoss: 0.6692813634872437 - trainLoss: 0.668048083782196\n",
      "cnt: 0 - valLoss: 0.6692783236503601 - trainLoss: 0.6680446863174438\n",
      "cnt: 0 - valLoss: 0.6692752838134766 - trainLoss: 0.6680413484573364\n",
      "cnt: 0 - valLoss: 0.669272243976593 - trainLoss: 0.6680378913879395\n",
      "cnt: 0 - valLoss: 0.6692692637443542 - trainLoss: 0.668034553527832\n",
      "cnt: 0 - valLoss: 0.6692662835121155 - trainLoss: 0.6680312156677246\n",
      "cnt: 0 - valLoss: 0.6692632436752319 - trainLoss: 0.6680278182029724\n",
      "cnt: 0 - valLoss: 0.6692602634429932 - trainLoss: 0.668024480342865\n",
      "cnt: 0 - valLoss: 0.6692572236061096 - trainLoss: 0.6680210828781128\n",
      "cnt: 0 - valLoss: 0.6692542433738708 - trainLoss: 0.6680176854133606\n",
      "cnt: 0 - valLoss: 0.6692512631416321 - trainLoss: 0.6680143475532532\n",
      "cnt: 0 - valLoss: 0.6692482829093933 - trainLoss: 0.6680110096931458\n",
      "cnt: 0 - valLoss: 0.6692452430725098 - trainLoss: 0.6680076122283936\n",
      "cnt: 0 - valLoss: 0.6692423224449158 - trainLoss: 0.6680043339729309\n",
      "cnt: 0 - valLoss: 0.6692392826080322 - trainLoss: 0.6680009961128235\n",
      "cnt: 0 - valLoss: 0.6692363619804382 - trainLoss: 0.6679976582527161\n",
      "cnt: 0 - valLoss: 0.6692333221435547 - trainLoss: 0.6679943203926086\n",
      "cnt: 0 - valLoss: 0.6692304015159607 - trainLoss: 0.6679909825325012\n",
      "cnt: 0 - valLoss: 0.6692274212837219 - trainLoss: 0.6679876446723938\n",
      "cnt: 0 - valLoss: 0.6692245006561279 - trainLoss: 0.6679843068122864\n",
      "cnt: 0 - valLoss: 0.6692215204238892 - trainLoss: 0.667980968952179\n",
      "cnt: 0 - valLoss: 0.6692185401916504 - trainLoss: 0.6679776906967163\n",
      "cnt: 0 - valLoss: 0.6692156195640564 - trainLoss: 0.6679744124412537\n",
      "cnt: 0 - valLoss: 0.6692126989364624 - trainLoss: 0.6679710745811462\n",
      "cnt: 0 - valLoss: 0.6692096590995789 - trainLoss: 0.6679677367210388\n",
      "cnt: 0 - valLoss: 0.6692067980766296 - trainLoss: 0.6679644584655762\n",
      "cnt: 0 - valLoss: 0.6692038774490356 - trainLoss: 0.6679611206054688\n",
      "cnt: 0 - valLoss: 0.6692008972167969 - trainLoss: 0.6679579019546509\n",
      "cnt: 0 - valLoss: 0.6691979169845581 - trainLoss: 0.6679545640945435\n",
      "cnt: 0 - valLoss: 0.6691949963569641 - trainLoss: 0.6679512858390808\n",
      "cnt: 0 - valLoss: 0.6691921353340149 - trainLoss: 0.6679480075836182\n",
      "cnt: 0 - valLoss: 0.6691892147064209 - trainLoss: 0.6679446697235107\n",
      "cnt: 0 - valLoss: 0.6691862940788269 - trainLoss: 0.6679414510726929\n",
      "cnt: 0 - valLoss: 0.6691833734512329 - trainLoss: 0.6679381728172302\n",
      "cnt: 0 - valLoss: 0.6691804528236389 - trainLoss: 0.6679348945617676\n",
      "cnt: 0 - valLoss: 0.6691775321960449 - trainLoss: 0.6679316163063049\n",
      "cnt: 0 - valLoss: 0.6691746115684509 - trainLoss: 0.6679283976554871\n",
      "cnt: 0 - valLoss: 0.6691718101501465 - trainLoss: 0.6679251194000244\n",
      "cnt: 0 - valLoss: 0.6691688299179077 - trainLoss: 0.6679218411445618\n",
      "cnt: 0 - valLoss: 0.6691659688949585 - trainLoss: 0.6679185628890991\n",
      "cnt: 0 - valLoss: 0.6691631078720093 - trainLoss: 0.6679153442382812\n",
      "cnt: 0 - valLoss: 0.6691602468490601 - trainLoss: 0.6679120659828186\n",
      "cnt: 0 - valLoss: 0.6691573262214661 - trainLoss: 0.6679089069366455\n",
      "cnt: 0 - valLoss: 0.6691544055938721 - trainLoss: 0.6679056286811829\n",
      "cnt: 0 - valLoss: 0.6691515445709229 - trainLoss: 0.667902410030365\n",
      "cnt: 0 - valLoss: 0.6691486239433289 - trainLoss: 0.6678991913795471\n",
      "cnt: 0 - valLoss: 0.6691457629203796 - trainLoss: 0.6678959727287292\n",
      "cnt: 0 - valLoss: 0.6691429018974304 - trainLoss: 0.6678927540779114\n",
      "cnt: 0 - valLoss: 0.669140100479126 - trainLoss: 0.6678895354270935\n",
      "cnt: 0 - valLoss: 0.669137179851532 - trainLoss: 0.6678863167762756\n",
      "cnt: 0 - valLoss: 0.6691343188285828 - trainLoss: 0.6678830981254578\n",
      "cnt: 0 - valLoss: 0.6691315174102783 - trainLoss: 0.6678798794746399\n",
      "cnt: 0 - valLoss: 0.6691286563873291 - trainLoss: 0.667876660823822\n",
      "cnt: 0 - valLoss: 0.6691257953643799 - trainLoss: 0.6678735017776489\n",
      "cnt: 0 - valLoss: 0.6691229343414307 - trainLoss: 0.667870283126831\n",
      "cnt: 0 - valLoss: 0.6691200733184814 - trainLoss: 0.6678670644760132\n",
      "cnt: 0 - valLoss: 0.669117271900177 - trainLoss: 0.6678639054298401\n",
      "cnt: 0 - valLoss: 0.6691144704818726 - trainLoss: 0.667860746383667\n",
      "cnt: 0 - valLoss: 0.6691116094589233 - trainLoss: 0.6678575277328491\n",
      "cnt: 0 - valLoss: 0.6691088080406189 - trainLoss: 0.667854368686676\n",
      "cnt: 0 - valLoss: 0.6691059470176697 - trainLoss: 0.6678511500358582\n",
      "cnt: 0 - valLoss: 0.6691031455993652 - trainLoss: 0.6678479909896851\n",
      "cnt: 0 - valLoss: 0.669100284576416 - trainLoss: 0.6678447723388672\n",
      "cnt: 0 - valLoss: 0.6690974831581116 - trainLoss: 0.6678416132926941\n",
      "cnt: 0 - valLoss: 0.6690947413444519 - trainLoss: 0.6678385138511658\n",
      "cnt: 0 - valLoss: 0.6690918803215027 - trainLoss: 0.6678353548049927\n",
      "cnt: 0 - valLoss: 0.6690890789031982 - trainLoss: 0.6678321957588196\n",
      "cnt: 0 - valLoss: 0.6690862774848938 - trainLoss: 0.6678290367126465\n",
      "cnt: 0 - valLoss: 0.6690834760665894 - trainLoss: 0.6678258776664734\n",
      "cnt: 0 - valLoss: 0.6690806746482849 - trainLoss: 0.6678227186203003\n",
      "cnt: 0 - valLoss: 0.6690779328346252 - trainLoss: 0.6678195595741272\n",
      "cnt: 0 - valLoss: 0.6690751314163208 - trainLoss: 0.6678164601325989\n",
      "cnt: 0 - valLoss: 0.6690723299980164 - trainLoss: 0.6678133010864258\n",
      "cnt: 0 - valLoss: 0.6690695285797119 - trainLoss: 0.6678101420402527\n",
      "cnt: 0 - valLoss: 0.6690667867660522 - trainLoss: 0.6678071022033691\n",
      "cnt: 0 - valLoss: 0.6690640449523926 - trainLoss: 0.667803943157196\n",
      "cnt: 0 - valLoss: 0.6690611839294434 - trainLoss: 0.667800784111023\n",
      "cnt: 0 - valLoss: 0.6690584421157837 - trainLoss: 0.6677977442741394\n",
      "cnt: 0 - valLoss: 0.669055700302124 - trainLoss: 0.6677945852279663\n",
      "cnt: 0 - valLoss: 0.6690529584884644 - trainLoss: 0.667791485786438\n",
      "cnt: 0 - valLoss: 0.6690501570701599 - trainLoss: 0.6677883863449097\n",
      "cnt: 0 - valLoss: 0.6690474152565002 - trainLoss: 0.6677852869033813\n",
      "cnt: 0 - valLoss: 0.6690446138381958 - trainLoss: 0.6677821278572083\n",
      "cnt: 0 - valLoss: 0.6690418720245361 - trainLoss: 0.6677790284156799\n",
      "cnt: 0 - valLoss: 0.6690391302108765 - trainLoss: 0.6677759885787964\n",
      "cnt: 0 - valLoss: 0.669036328792572 - trainLoss: 0.6677728891372681\n",
      "cnt: 0 - valLoss: 0.6690335869789124 - trainLoss: 0.6677697896957397\n",
      "cnt: 0 - valLoss: 0.6690309643745422 - trainLoss: 0.6677666902542114\n",
      "cnt: 0 - valLoss: 0.6690282225608826 - trainLoss: 0.6677635908126831\n",
      "cnt: 0 - valLoss: 0.6690254807472229 - trainLoss: 0.6677605509757996\n",
      "cnt: 0 - valLoss: 0.6690227389335632 - trainLoss: 0.6677574515342712\n",
      "cnt: 0 - valLoss: 0.6690199971199036 - trainLoss: 0.6677544116973877\n",
      "cnt: 0 - valLoss: 0.6690172553062439 - trainLoss: 0.6677513122558594\n",
      "cnt: 0 - valLoss: 0.6690145134925842 - trainLoss: 0.6677482724189758\n",
      "cnt: 0 - valLoss: 0.6690118312835693 - trainLoss: 0.6677452325820923\n",
      "cnt: 0 - valLoss: 0.6690091490745544 - trainLoss: 0.667742133140564\n",
      "cnt: 0 - valLoss: 0.6690064072608948 - trainLoss: 0.6677390933036804\n",
      "cnt: 0 - valLoss: 0.6690036654472351 - trainLoss: 0.6677360534667969\n",
      "cnt: 0 - valLoss: 0.669001042842865 - trainLoss: 0.6677330136299133\n",
      "cnt: 0 - valLoss: 0.6689983010292053 - trainLoss: 0.6677299737930298\n",
      "cnt: 0 - valLoss: 0.6689956188201904 - trainLoss: 0.6677269339561462\n",
      "cnt: 0 - valLoss: 0.6689929366111755 - trainLoss: 0.6677239537239075\n",
      "cnt: 0 - valLoss: 0.6689902544021606 - trainLoss: 0.6677208542823792\n",
      "cnt: 0 - valLoss: 0.668987512588501 - trainLoss: 0.6677178740501404\n",
      "cnt: 0 - valLoss: 0.6689848303794861 - trainLoss: 0.6677148342132568\n",
      "cnt: 0 - valLoss: 0.6689821481704712 - trainLoss: 0.6677117943763733\n",
      "cnt: 0 - valLoss: 0.6689795255661011 - trainLoss: 0.6677087545394897\n",
      "cnt: 0 - valLoss: 0.6689767837524414 - trainLoss: 0.667705774307251\n",
      "cnt: 0 - valLoss: 0.6689741611480713 - trainLoss: 0.6677027344703674\n",
      "cnt: 0 - valLoss: 0.6689714789390564 - trainLoss: 0.6676996946334839\n",
      "cnt: 0 - valLoss: 0.6689687967300415 - trainLoss: 0.6676967144012451\n",
      "cnt: 0 - valLoss: 0.6689661145210266 - trainLoss: 0.6676937341690063\n",
      "cnt: 0 - valLoss: 0.6689634919166565 - trainLoss: 0.6676906943321228\n",
      "cnt: 0 - valLoss: 0.6689608097076416 - trainLoss: 0.667687714099884\n",
      "cnt: 0 - valLoss: 0.6689581871032715 - trainLoss: 0.6676847338676453\n",
      "cnt: 0 - valLoss: 0.6689555048942566 - trainLoss: 0.6676816940307617\n",
      "cnt: 0 - valLoss: 0.6689528822898865 - trainLoss: 0.667678713798523\n",
      "cnt: 0 - valLoss: 0.6689502000808716 - trainLoss: 0.667675793170929\n",
      "cnt: 0 - valLoss: 0.6689476370811462 - trainLoss: 0.6676727533340454\n",
      "cnt: 0 - valLoss: 0.6689449548721313 - trainLoss: 0.6676698327064514\n",
      "cnt: 0 - valLoss: 0.6689423322677612 - trainLoss: 0.6676668524742126\n",
      "cnt: 0 - valLoss: 0.6689397096633911 - trainLoss: 0.6676638722419739\n",
      "cnt: 0 - valLoss: 0.668937087059021 - trainLoss: 0.6676608920097351\n",
      "cnt: 0 - valLoss: 0.6689344048500061 - trainLoss: 0.6676579117774963\n",
      "cnt: 0 - valLoss: 0.668931782245636 - trainLoss: 0.6676549911499023\n",
      "cnt: 0 - valLoss: 0.6689292192459106 - trainLoss: 0.6676520109176636\n",
      "cnt: 0 - valLoss: 0.6689265966415405 - trainLoss: 0.6676490902900696\n",
      "cnt: 0 - valLoss: 0.6689239740371704 - trainLoss: 0.6676461100578308\n",
      "cnt: 0 - valLoss: 0.6689213514328003 - trainLoss: 0.6676431894302368\n",
      "cnt: 0 - valLoss: 0.6689186692237854 - trainLoss: 0.667640209197998\n",
      "cnt: 0 - valLoss: 0.6689161062240601 - trainLoss: 0.667637288570404\n",
      "cnt: 0 - valLoss: 0.6689135432243347 - trainLoss: 0.6676343679428101\n",
      "cnt: 0 - valLoss: 0.6689109206199646 - trainLoss: 0.6676313877105713\n",
      "cnt: 0 - valLoss: 0.6689082980155945 - trainLoss: 0.6676284670829773\n",
      "cnt: 0 - valLoss: 0.6689057350158691 - trainLoss: 0.6676255464553833\n",
      "cnt: 0 - valLoss: 0.668903112411499 - trainLoss: 0.6676226258277893\n",
      "cnt: 0 - valLoss: 0.6689005494117737 - trainLoss: 0.6676197052001953\n",
      "cnt: 0 - valLoss: 0.6688979268074036 - trainLoss: 0.6676167249679565\n",
      "cnt: 0 - valLoss: 0.6688953638076782 - trainLoss: 0.6676138639450073\n",
      "cnt: 0 - valLoss: 0.6688928604125977 - trainLoss: 0.6676109433174133\n",
      "cnt: 0 - valLoss: 0.6688902378082275 - trainLoss: 0.6676080226898193\n",
      "cnt: 0 - valLoss: 0.6688876748085022 - trainLoss: 0.6676051020622253\n",
      "cnt: 0 - valLoss: 0.6688851118087769 - trainLoss: 0.6676022410392761\n",
      "cnt: 0 - valLoss: 0.6688825488090515 - trainLoss: 0.6675993800163269\n",
      "cnt: 0 - valLoss: 0.6688799858093262 - trainLoss: 0.6675963997840881\n",
      "cnt: 0 - valLoss: 0.668877363204956 - trainLoss: 0.6675935387611389\n",
      "cnt: 0 - valLoss: 0.6688748598098755 - trainLoss: 0.6675906181335449\n",
      "cnt: 0 - valLoss: 0.6688722968101501 - trainLoss: 0.6675877571105957\n",
      "cnt: 0 - valLoss: 0.6688697338104248 - trainLoss: 0.6675848960876465\n",
      "cnt: 0 - valLoss: 0.6688671708106995 - trainLoss: 0.6675819754600525\n",
      "cnt: 0 - valLoss: 0.6688646674156189 - trainLoss: 0.6675790548324585\n",
      "cnt: 0 - valLoss: 0.6688621044158936 - trainLoss: 0.667576253414154\n",
      "cnt: 0 - valLoss: 0.6688595414161682 - trainLoss: 0.6675733327865601\n",
      "cnt: 0 - valLoss: 0.6688570380210876 - trainLoss: 0.6675705313682556\n",
      "cnt: 0 - valLoss: 0.6688544750213623 - trainLoss: 0.6675676107406616\n",
      "cnt: 0 - valLoss: 0.6688519716262817 - trainLoss: 0.6675648093223572\n",
      "cnt: 0 - valLoss: 0.6688494086265564 - trainLoss: 0.667561948299408\n",
      "cnt: 0 - valLoss: 0.6688469052314758 - trainLoss: 0.667559027671814\n",
      "cnt: 0 - valLoss: 0.6688444018363953 - trainLoss: 0.6675561666488647\n",
      "cnt: 0 - valLoss: 0.6688418388366699 - trainLoss: 0.6675534248352051\n",
      "cnt: 0 - valLoss: 0.6688392758369446 - trainLoss: 0.6675505042076111\n",
      "cnt: 0 - valLoss: 0.668836772441864 - trainLoss: 0.6675476431846619\n",
      "cnt: 0 - valLoss: 0.6688343286514282 - trainLoss: 0.6675448417663574\n",
      "cnt: 0 - valLoss: 0.6688318252563477 - trainLoss: 0.667542040348053\n",
      "cnt: 0 - valLoss: 0.6688292622566223 - trainLoss: 0.6675391793251038\n",
      "cnt: 0 - valLoss: 0.6688268184661865 - trainLoss: 0.6675363183021545\n",
      "cnt: 0 - valLoss: 0.6688242554664612 - trainLoss: 0.6675335168838501\n",
      "cnt: 0 - valLoss: 0.6688218116760254 - trainLoss: 0.6675306558609009\n",
      "cnt: 0 - valLoss: 0.6688193082809448 - trainLoss: 0.6675278544425964\n",
      "cnt: 0 - valLoss: 0.6688168048858643 - trainLoss: 0.667525053024292\n",
      "cnt: 0 - valLoss: 0.6688143014907837 - trainLoss: 0.6675222516059875\n",
      "cnt: 0 - valLoss: 0.6688118577003479 - trainLoss: 0.6675193905830383\n",
      "cnt: 0 - valLoss: 0.6688093543052673 - trainLoss: 0.6675165891647339\n",
      "cnt: 0 - valLoss: 0.6688068509101868 - trainLoss: 0.6675137877464294\n",
      "cnt: 0 - valLoss: 0.668804407119751 - trainLoss: 0.6675110459327698\n",
      "cnt: 0 - valLoss: 0.6688019037246704 - trainLoss: 0.6675081849098206\n",
      "cnt: 0 - valLoss: 0.6687994003295898 - trainLoss: 0.6675053834915161\n",
      "cnt: 0 - valLoss: 0.6687970161437988 - trainLoss: 0.6675026416778564\n",
      "cnt: 0 - valLoss: 0.6687945127487183 - trainLoss: 0.667499840259552\n",
      "cnt: 0 - valLoss: 0.6687920093536377 - trainLoss: 0.6674970388412476\n",
      "cnt: 0 - valLoss: 0.6687896251678467 - trainLoss: 0.6674942374229431\n",
      "cnt: 0 - valLoss: 0.6687871813774109 - trainLoss: 0.6674914956092834\n",
      "cnt: 0 - valLoss: 0.6687846779823303 - trainLoss: 0.667488694190979\n",
      "cnt: 0 - valLoss: 0.6687822937965393 - trainLoss: 0.6674858927726746\n",
      "cnt: 0 - valLoss: 0.6687797904014587 - trainLoss: 0.6674831509590149\n",
      "cnt: 0 - valLoss: 0.6687774062156677 - trainLoss: 0.6674804091453552\n",
      "cnt: 0 - valLoss: 0.6687749028205872 - trainLoss: 0.6674776673316956\n",
      "cnt: 0 - valLoss: 0.6687725186347961 - trainLoss: 0.6674748659133911\n",
      "cnt: 0 - valLoss: 0.6687700152397156 - trainLoss: 0.6674720644950867\n",
      "cnt: 0 - valLoss: 0.6687675714492798 - trainLoss: 0.667469322681427\n",
      "cnt: 0 - valLoss: 0.668765127658844 - trainLoss: 0.6674665808677673\n",
      "cnt: 0 - valLoss: 0.668762743473053 - trainLoss: 0.6674638390541077\n",
      "cnt: 0 - valLoss: 0.668760359287262 - trainLoss: 0.667461097240448\n",
      "cnt: 0 - valLoss: 0.6687579154968262 - trainLoss: 0.6674583554267883\n",
      "cnt: 0 - valLoss: 0.6687554717063904 - trainLoss: 0.6674556136131287\n",
      "cnt: 0 - valLoss: 0.6687530279159546 - trainLoss: 0.667452871799469\n",
      "cnt: 0 - valLoss: 0.6687506437301636 - trainLoss: 0.6674501299858093\n",
      "cnt: 0 - valLoss: 0.6687482595443726 - trainLoss: 0.6674473881721497\n",
      "cnt: 0 - valLoss: 0.6687458157539368 - trainLoss: 0.66744464635849\n",
      "cnt: 0 - valLoss: 0.6687434315681458 - trainLoss: 0.6674419641494751\n",
      "cnt: 0 - valLoss: 0.6687410473823547 - trainLoss: 0.6674392223358154\n",
      "cnt: 0 - valLoss: 0.668738603591919 - trainLoss: 0.6674364805221558\n",
      "cnt: 0 - valLoss: 0.6687362194061279 - trainLoss: 0.6674337387084961\n",
      "cnt: 0 - valLoss: 0.6687338352203369 - trainLoss: 0.6674310564994812\n",
      "cnt: 0 - valLoss: 0.6687314510345459 - trainLoss: 0.6674283146858215\n",
      "cnt: 0 - valLoss: 0.6687290072441101 - trainLoss: 0.6674255728721619\n",
      "cnt: 0 - valLoss: 0.6687266826629639 - trainLoss: 0.6674229502677917\n",
      "cnt: 0 - valLoss: 0.6687242984771729 - trainLoss: 0.6674202084541321\n",
      "cnt: 0 - valLoss: 0.6687219142913818 - trainLoss: 0.6674175262451172\n",
      "cnt: 0 - valLoss: 0.6687195301055908 - trainLoss: 0.6674147844314575\n",
      "cnt: 0 - valLoss: 0.6687171459197998 - trainLoss: 0.6674121022224426\n",
      "cnt: 0 - valLoss: 0.6687148213386536 - trainLoss: 0.6674094200134277\n",
      "cnt: 0 - valLoss: 0.6687123775482178 - trainLoss: 0.6674067378044128\n",
      "cnt: 0 - valLoss: 0.6687101125717163 - trainLoss: 0.667404055595398\n",
      "cnt: 0 - valLoss: 0.6687076687812805 - trainLoss: 0.6674013733863831\n",
      "cnt: 0 - valLoss: 0.6687052845954895 - trainLoss: 0.6673986911773682\n",
      "cnt: 0 - valLoss: 0.6687029004096985 - trainLoss: 0.6673959493637085\n",
      "cnt: 0 - valLoss: 0.668700635433197 - trainLoss: 0.6673933863639832\n",
      "cnt: 0 - valLoss: 0.668698251247406 - trainLoss: 0.6673906445503235\n",
      "cnt: 0 - valLoss: 0.668695867061615 - trainLoss: 0.6673880219459534\n",
      "cnt: 0 - valLoss: 0.6686935424804688 - trainLoss: 0.6673853397369385\n",
      "cnt: 0 - valLoss: 0.6686911582946777 - trainLoss: 0.6673827171325684\n",
      "cnt: 0 - valLoss: 0.6686888933181763 - trainLoss: 0.6673800349235535\n",
      "cnt: 0 - valLoss: 0.6686865091323853 - trainLoss: 0.6673773527145386\n",
      "cnt: 0 - valLoss: 0.668684184551239 - trainLoss: 0.6673747301101685\n",
      "cnt: 0 - valLoss: 0.668681800365448 - trainLoss: 0.6673721075057983\n",
      "cnt: 0 - valLoss: 0.6686794757843018 - trainLoss: 0.6673694252967834\n",
      "cnt: 0 - valLoss: 0.6686771512031555 - trainLoss: 0.6673668026924133\n",
      "cnt: 0 - valLoss: 0.6686748266220093 - trainLoss: 0.6673641204833984\n",
      "cnt: 0 - valLoss: 0.668672502040863 - trainLoss: 0.6673614978790283\n",
      "cnt: 0 - valLoss: 0.6686702370643616 - trainLoss: 0.6673588752746582\n",
      "cnt: 0 - valLoss: 0.6686679124832153 - trainLoss: 0.6673561930656433\n",
      "cnt: 0 - valLoss: 0.6686655879020691 - trainLoss: 0.667353630065918\n",
      "cnt: 0 - valLoss: 0.6686632633209229 - trainLoss: 0.6673509478569031\n",
      "cnt: 0 - valLoss: 0.6686609387397766 - trainLoss: 0.6673483848571777\n",
      "cnt: 0 - valLoss: 0.6686586737632751 - trainLoss: 0.6673457026481628\n",
      "cnt: 0 - valLoss: 0.6686563491821289 - trainLoss: 0.6673430800437927\n",
      "cnt: 0 - valLoss: 0.6686540246009827 - trainLoss: 0.6673405170440674\n",
      "cnt: 0 - valLoss: 0.6686517596244812 - trainLoss: 0.6673378348350525\n",
      "cnt: 0 - valLoss: 0.668649435043335 - trainLoss: 0.6673352718353271\n",
      "cnt: 0 - valLoss: 0.6686471700668335 - trainLoss: 0.667332649230957\n",
      "cnt: 0 - valLoss: 0.6686448454856873 - trainLoss: 0.6673300862312317\n",
      "cnt: 0 - valLoss: 0.668642520904541 - trainLoss: 0.6673274040222168\n",
      "cnt: 0 - valLoss: 0.6686403155326843 - trainLoss: 0.6673248410224915\n",
      "cnt: 0 - valLoss: 0.6686379909515381 - trainLoss: 0.6673222780227661\n",
      "cnt: 0 - valLoss: 0.6686356663703918 - trainLoss: 0.667319655418396\n",
      "cnt: 0 - valLoss: 0.6686334609985352 - trainLoss: 0.6673170328140259\n",
      "cnt: 0 - valLoss: 0.6686311364173889 - trainLoss: 0.6673144698143005\n",
      "cnt: 0 - valLoss: 0.6686288714408875 - trainLoss: 0.6673119068145752\n",
      "cnt: 0 - valLoss: 0.668626606464386 - trainLoss: 0.6673093438148499\n",
      "cnt: 0 - valLoss: 0.6686243414878845 - trainLoss: 0.6673067212104797\n",
      "cnt: 0 - valLoss: 0.6686220765113831 - trainLoss: 0.6673042178153992\n",
      "cnt: 0 - valLoss: 0.6686198115348816 - trainLoss: 0.667301595211029\n",
      "cnt: 0 - valLoss: 0.6686175465583801 - trainLoss: 0.6672989726066589\n",
      "cnt: 0 - valLoss: 0.6686153411865234 - trainLoss: 0.6672964096069336\n",
      "cnt: 0 - valLoss: 0.668613076210022 - trainLoss: 0.6672938466072083\n",
      "cnt: 0 - valLoss: 0.6686108112335205 - trainLoss: 0.6672913432121277\n",
      "cnt: 0 - valLoss: 0.668608546257019 - trainLoss: 0.6672887802124023\n",
      "cnt: 0 - valLoss: 0.6686063408851624 - trainLoss: 0.667286217212677\n",
      "cnt: 0 - valLoss: 0.6686040759086609 - trainLoss: 0.6672836542129517\n",
      "cnt: 0 - valLoss: 0.6686018109321594 - trainLoss: 0.6672810912132263\n",
      "cnt: 0 - valLoss: 0.6685996055603027 - trainLoss: 0.6672785878181458\n",
      "cnt: 0 - valLoss: 0.6685973405838013 - trainLoss: 0.6672760248184204\n",
      "cnt: 0 - valLoss: 0.6685950756072998 - trainLoss: 0.6672734618186951\n",
      "cnt: 0 - valLoss: 0.6685928702354431 - trainLoss: 0.6672708988189697\n",
      "cnt: 0 - valLoss: 0.6685906052589417 - trainLoss: 0.6672683358192444\n",
      "cnt: 0 - valLoss: 0.6685883402824402 - trainLoss: 0.6672658324241638\n",
      "cnt: 0 - valLoss: 0.6685861945152283 - trainLoss: 0.6672633290290833\n",
      "cnt: 0 - valLoss: 0.6685839295387268 - trainLoss: 0.6672608256340027\n",
      "cnt: 0 - valLoss: 0.6685817241668701 - trainLoss: 0.6672582626342773\n",
      "cnt: 0 - valLoss: 0.6685795187950134 - trainLoss: 0.6672557592391968\n",
      "cnt: 0 - valLoss: 0.6685773134231567 - trainLoss: 0.6672532558441162\n",
      "cnt: 0 - valLoss: 0.6685750484466553 - trainLoss: 0.6672506928443909\n",
      "cnt: 0 - valLoss: 0.6685729026794434 - trainLoss: 0.6672481894493103\n",
      "cnt: 0 - valLoss: 0.6685706973075867 - trainLoss: 0.6672456860542297\n",
      "cnt: 0 - valLoss: 0.66856849193573 - trainLoss: 0.6672431826591492\n",
      "cnt: 0 - valLoss: 0.6685662269592285 - trainLoss: 0.6672406792640686\n",
      "cnt: 0 - valLoss: 0.6685640811920166 - trainLoss: 0.667238175868988\n",
      "cnt: 0 - valLoss: 0.6685618758201599 - trainLoss: 0.6672356724739075\n",
      "cnt: 0 - valLoss: 0.6685596704483032 - trainLoss: 0.6672331690788269\n",
      "cnt: 0 - valLoss: 0.6685575246810913 - trainLoss: 0.6672306656837463\n",
      "cnt: 0 - valLoss: 0.6685552597045898 - trainLoss: 0.6672281622886658\n",
      "cnt: 0 - valLoss: 0.6685531139373779 - trainLoss: 0.6672256588935852\n",
      "cnt: 0 - valLoss: 0.6685509085655212 - trainLoss: 0.6672232151031494\n",
      "cnt: 0 - valLoss: 0.6685487627983093 - trainLoss: 0.6672206521034241\n",
      "cnt: 0 - valLoss: 0.6685465574264526 - trainLoss: 0.6672182083129883\n",
      "cnt: 0 - valLoss: 0.6685444116592407 - trainLoss: 0.6672157049179077\n",
      "cnt: 0 - valLoss: 0.668542206287384 - trainLoss: 0.6672132611274719\n",
      "cnt: 0 - valLoss: 0.6685400009155273 - trainLoss: 0.6672107577323914\n",
      "cnt: 0 - valLoss: 0.6685378551483154 - trainLoss: 0.6672082543373108\n",
      "cnt: 0 - valLoss: 0.6685357093811035 - trainLoss: 0.667205810546875\n",
      "cnt: 0 - valLoss: 0.6685335040092468 - trainLoss: 0.6672033667564392\n",
      "cnt: 0 - valLoss: 0.6685313582420349 - trainLoss: 0.6672008633613586\n",
      "cnt: 0 - valLoss: 0.6685291528701782 - trainLoss: 0.6671984195709229\n",
      "cnt: 0 - valLoss: 0.6685270071029663 - trainLoss: 0.6671960353851318\n",
      "cnt: 0 - valLoss: 0.6685248613357544 - trainLoss: 0.6671935319900513\n",
      "cnt: 0 - valLoss: 0.6685227155685425 - trainLoss: 0.6671910881996155\n",
      "cnt: 0 - valLoss: 0.6685205698013306 - trainLoss: 0.6671885848045349\n",
      "cnt: 0 - valLoss: 0.6685184240341187 - trainLoss: 0.6671861410140991\n",
      "cnt: 0 - valLoss: 0.6685162782669067 - trainLoss: 0.6671836972236633\n",
      "cnt: 0 - valLoss: 0.6685141324996948 - trainLoss: 0.6671812534332275\n",
      "cnt: 0 - valLoss: 0.6685119867324829 - trainLoss: 0.6671788096427917\n",
      "cnt: 0 - valLoss: 0.668509840965271 - trainLoss: 0.6671764254570007\n",
      "cnt: 0 - valLoss: 0.6685076951980591 - trainLoss: 0.6671739220619202\n",
      "cnt: 0 - valLoss: 0.6685055494308472 - trainLoss: 0.6671715378761292\n",
      "cnt: 0 - valLoss: 0.6685034036636353 - trainLoss: 0.6671690940856934\n",
      "cnt: 0 - valLoss: 0.6685013175010681 - trainLoss: 0.6671666502952576\n",
      "cnt: 0 - valLoss: 0.6684991717338562 - trainLoss: 0.6671642661094666\n",
      "cnt: 0 - valLoss: 0.6684970855712891 - trainLoss: 0.667161762714386\n",
      "cnt: 0 - valLoss: 0.6684948801994324 - trainLoss: 0.667159378528595\n",
      "cnt: 0 - valLoss: 0.6684927940368652 - trainLoss: 0.6671569347381592\n",
      "cnt: 0 - valLoss: 0.6684906482696533 - trainLoss: 0.6671545505523682\n",
      "cnt: 0 - valLoss: 0.6684885621070862 - trainLoss: 0.6671521663665771\n",
      "cnt: 0 - valLoss: 0.6684864163398743 - trainLoss: 0.6671497225761414\n",
      "cnt: 0 - valLoss: 0.6684843897819519 - trainLoss: 0.6671472787857056\n",
      "cnt: 0 - valLoss: 0.66848224401474 - trainLoss: 0.6671448945999146\n",
      "cnt: 0 - valLoss: 0.6684801578521729 - trainLoss: 0.6671425104141235\n",
      "cnt: 0 - valLoss: 0.6684780120849609 - trainLoss: 0.6671401262283325\n",
      "cnt: 0 - valLoss: 0.6684759259223938 - trainLoss: 0.6671377420425415\n",
      "cnt: 0 - valLoss: 0.6684738397598267 - trainLoss: 0.6671352982521057\n",
      "cnt: 0 - valLoss: 0.6684716939926147 - trainLoss: 0.6671329140663147\n",
      "cnt: 0 - valLoss: 0.6684696078300476 - trainLoss: 0.6671305298805237\n",
      "cnt: 0 - valLoss: 0.6684675216674805 - trainLoss: 0.6671281456947327\n",
      "cnt: 0 - valLoss: 0.6684654355049133 - trainLoss: 0.6671257615089417\n",
      "cnt: 0 - valLoss: 0.6684633493423462 - trainLoss: 0.6671233773231506\n",
      "cnt: 0 - valLoss: 0.6684613227844238 - trainLoss: 0.6671209931373596\n",
      "cnt: 0 - valLoss: 0.6684592366218567 - trainLoss: 0.6671186089515686\n",
      "cnt: 0 - valLoss: 0.6684570908546448 - trainLoss: 0.6671162247657776\n",
      "cnt: 0 - valLoss: 0.6684550046920776 - trainLoss: 0.6671138405799866\n",
      "cnt: 0 - valLoss: 0.6684529185295105 - trainLoss: 0.6671114563941956\n",
      "cnt: 0 - valLoss: 0.6684508323669434 - trainLoss: 0.6671091318130493\n",
      "cnt: 0 - valLoss: 0.668448805809021 - trainLoss: 0.6671067476272583\n",
      "cnt: 0 - valLoss: 0.6684467196464539 - trainLoss: 0.6671044230461121\n",
      "cnt: 0 - valLoss: 0.6684447526931763 - trainLoss: 0.667102038860321\n",
      "cnt: 0 - valLoss: 0.6684426069259644 - trainLoss: 0.6670997142791748\n",
      "cnt: 0 - valLoss: 0.668440580368042 - trainLoss: 0.6670973300933838\n",
      "cnt: 0 - valLoss: 0.6684384942054749 - trainLoss: 0.6670950055122375\n",
      "cnt: 0 - valLoss: 0.6684364676475525 - trainLoss: 0.6670926213264465\n",
      "cnt: 0 - valLoss: 0.6684343814849854 - trainLoss: 0.6670902967453003\n",
      "cnt: 0 - valLoss: 0.668432354927063 - trainLoss: 0.667087972164154\n",
      "cnt: 0 - valLoss: 0.6684302687644958 - trainLoss: 0.667085587978363\n",
      "cnt: 0 - valLoss: 0.6684282422065735 - trainLoss: 0.6670832633972168\n",
      "cnt: 0 - valLoss: 0.6684261560440063 - trainLoss: 0.6670809388160706\n",
      "cnt: 0 - valLoss: 0.668424129486084 - trainLoss: 0.6670786142349243\n",
      "cnt: 0 - valLoss: 0.6684221029281616 - trainLoss: 0.6670762300491333\n",
      "cnt: 0 - valLoss: 0.6684200763702393 - trainLoss: 0.6670739650726318\n",
      "cnt: 0 - valLoss: 0.6684180498123169 - trainLoss: 0.6670716404914856\n",
      "cnt: 0 - valLoss: 0.6684160232543945 - trainLoss: 0.6670692563056946\n",
      "cnt: 0 - valLoss: 0.6684139966964722 - trainLoss: 0.6670669913291931\n",
      "cnt: 0 - valLoss: 0.6684119701385498 - trainLoss: 0.6670646667480469\n",
      "cnt: 0 - valLoss: 0.6684099435806274 - trainLoss: 0.6670623421669006\n",
      "cnt: 0 - valLoss: 0.6684079170227051 - trainLoss: 0.6670600771903992\n",
      "cnt: 0 - valLoss: 0.6684058308601379 - trainLoss: 0.6670577526092529\n",
      "cnt: 0 - valLoss: 0.6684038639068604 - trainLoss: 0.6670554280281067\n",
      "cnt: 0 - valLoss: 0.668401837348938 - trainLoss: 0.6670531034469604\n",
      "cnt: 0 - valLoss: 0.6683998703956604 - trainLoss: 0.667050838470459\n",
      "cnt: 0 - valLoss: 0.6683977842330933 - trainLoss: 0.6670485138893127\n",
      "cnt: 0 - valLoss: 0.6683957576751709 - trainLoss: 0.6670461893081665\n",
      "cnt: 0 - valLoss: 0.6683937907218933 - trainLoss: 0.6670438647270203\n",
      "cnt: 0 - valLoss: 0.6683918237686157 - trainLoss: 0.6670415997505188\n",
      "cnt: 0 - valLoss: 0.6683897972106934 - trainLoss: 0.6670393347740173\n",
      "cnt: 0 - valLoss: 0.668387770652771 - trainLoss: 0.6670370697975159\n",
      "cnt: 0 - valLoss: 0.6683858036994934 - trainLoss: 0.6670348048210144\n",
      "cnt: 0 - valLoss: 0.668383777141571 - trainLoss: 0.6670324206352234\n",
      "cnt: 0 - valLoss: 0.6683818101882935 - trainLoss: 0.6670302152633667\n",
      "cnt: 0 - valLoss: 0.6683797836303711 - trainLoss: 0.6670278906822205\n",
      "cnt: 0 - valLoss: 0.6683778166770935 - trainLoss: 0.667025625705719\n",
      "cnt: 0 - valLoss: 0.6683757901191711 - trainLoss: 0.6670233607292175\n",
      "cnt: 0 - valLoss: 0.6683738231658936 - trainLoss: 0.6670210957527161\n",
      "cnt: 0 - valLoss: 0.668371856212616 - trainLoss: 0.6670188307762146\n",
      "cnt: 0 - valLoss: 0.6683698296546936 - trainLoss: 0.6670165657997131\n",
      "cnt: 0 - valLoss: 0.668367862701416 - trainLoss: 0.6670143008232117\n",
      "cnt: 0 - valLoss: 0.6683659553527832 - trainLoss: 0.6670120358467102\n",
      "cnt: 0 - valLoss: 0.6683639287948608 - trainLoss: 0.667009711265564\n",
      "cnt: 0 - valLoss: 0.6683619618415833 - trainLoss: 0.6670075058937073\n",
      "cnt: 0 - valLoss: 0.6683599948883057 - trainLoss: 0.6670052409172058\n",
      "cnt: 0 - valLoss: 0.6683580875396729 - trainLoss: 0.6670030355453491\n",
      "cnt: 0 - valLoss: 0.6683561205863953 - trainLoss: 0.6670007705688477\n",
      "cnt: 0 - valLoss: 0.6683540940284729 - trainLoss: 0.6669985055923462\n",
      "cnt: 0 - valLoss: 0.6683521270751953 - trainLoss: 0.6669962406158447\n",
      "cnt: 0 - valLoss: 0.6683502197265625 - trainLoss: 0.666994035243988\n",
      "cnt: 0 - valLoss: 0.6683481931686401 - trainLoss: 0.6669917702674866\n",
      "cnt: 0 - valLoss: 0.6683462858200073 - trainLoss: 0.6669895052909851\n",
      "cnt: 0 - valLoss: 0.6683443188667297 - trainLoss: 0.6669872999191284\n",
      "cnt: 0 - valLoss: 0.6683423519134521 - trainLoss: 0.6669850945472717\n",
      "cnt: 0 - valLoss: 0.6683404445648193 - trainLoss: 0.6669828295707703\n",
      "cnt: 0 - valLoss: 0.6683384776115417 - trainLoss: 0.6669806241989136\n",
      "cnt: 0 - valLoss: 0.6683365702629089 - trainLoss: 0.6669784188270569\n",
      "cnt: 0 - valLoss: 0.6683346033096313 - trainLoss: 0.6669761538505554\n",
      "cnt: 0 - valLoss: 0.6683326363563538 - trainLoss: 0.6669739484786987\n",
      "cnt: 0 - valLoss: 0.6683307886123657 - trainLoss: 0.666971743106842\n",
      "cnt: 0 - valLoss: 0.6683287620544434 - trainLoss: 0.6669694781303406\n",
      "cnt: 0 - valLoss: 0.6683268547058105 - trainLoss: 0.6669672727584839\n",
      "cnt: 0 - valLoss: 0.668324887752533 - trainLoss: 0.6669650673866272\n",
      "cnt: 0 - valLoss: 0.6683229804039001 - trainLoss: 0.6669629216194153\n",
      "cnt: 0 - valLoss: 0.6683210730552673 - trainLoss: 0.6669606566429138\n",
      "cnt: 0 - valLoss: 0.6683191657066345 - trainLoss: 0.6669584512710571\n",
      "cnt: 0 - valLoss: 0.6683171987533569 - trainLoss: 0.6669563055038452\n",
      "cnt: 0 - valLoss: 0.6683153510093689 - trainLoss: 0.6669541001319885\n",
      "cnt: 0 - valLoss: 0.6683133840560913 - trainLoss: 0.6669518947601318\n",
      "cnt: 0 - valLoss: 0.6683114767074585 - trainLoss: 0.6669496893882751\n",
      "cnt: 0 - valLoss: 0.6683095693588257 - trainLoss: 0.6669474840164185\n",
      "cnt: 0 - valLoss: 0.6683076620101929 - trainLoss: 0.6669452786445618\n",
      "cnt: 0 - valLoss: 0.6683057546615601 - trainLoss: 0.6669431328773499\n",
      "cnt: 0 - valLoss: 0.6683038473129272 - trainLoss: 0.6669409275054932\n",
      "cnt: 0 - valLoss: 0.6683019399642944 - trainLoss: 0.6669387221336365\n",
      "cnt: 0 - valLoss: 0.6683000922203064 - trainLoss: 0.6669365763664246\n",
      "cnt: 0 - valLoss: 0.6682981252670288 - trainLoss: 0.6669344305992126\n",
      "cnt: 0 - valLoss: 0.6682962775230408 - trainLoss: 0.6669321656227112\n",
      "cnt: 0 - valLoss: 0.6682943105697632 - trainLoss: 0.6669300198554993\n",
      "cnt: 0 - valLoss: 0.6682924628257751 - trainLoss: 0.6669278740882874\n",
      "cnt: 0 - valLoss: 0.6682905554771423 - trainLoss: 0.6669257283210754\n",
      "cnt: 0 - valLoss: 0.6682886481285095 - trainLoss: 0.6669235229492188\n",
      "cnt: 0 - valLoss: 0.6682868003845215 - trainLoss: 0.6669213771820068\n",
      "cnt: 0 - valLoss: 0.6682848930358887 - trainLoss: 0.6669192314147949\n",
      "cnt: 0 - valLoss: 0.6682830452919006 - trainLoss: 0.666917085647583\n",
      "cnt: 0 - valLoss: 0.6682811379432678 - trainLoss: 0.6669148802757263\n",
      "cnt: 0 - valLoss: 0.668279230594635 - trainLoss: 0.6669127941131592\n",
      "cnt: 0 - valLoss: 0.668277382850647 - trainLoss: 0.6669105887413025\n",
      "cnt: 0 - valLoss: 0.6682755351066589 - trainLoss: 0.6669084429740906\n",
      "cnt: 0 - valLoss: 0.6682736873626709 - trainLoss: 0.6669063568115234\n",
      "cnt: 0 - valLoss: 0.6682717800140381 - trainLoss: 0.6669042110443115\n",
      "cnt: 0 - valLoss: 0.66826993227005 - trainLoss: 0.6669020652770996\n",
      "cnt: 0 - valLoss: 0.6682680249214172 - trainLoss: 0.6668999195098877\n",
      "cnt: 0 - valLoss: 0.668266236782074 - trainLoss: 0.6668977737426758\n",
      "cnt: 0 - valLoss: 0.6682643294334412 - trainLoss: 0.6668956279754639\n",
      "cnt: 0 - valLoss: 0.6682624220848083 - trainLoss: 0.666893482208252\n",
      "cnt: 0 - valLoss: 0.6682606339454651 - trainLoss: 0.66689133644104\n",
      "cnt: 0 - valLoss: 0.6682587265968323 - trainLoss: 0.6668891906738281\n",
      "cnt: 0 - valLoss: 0.668256938457489 - trainLoss: 0.666887104511261\n",
      "cnt: 0 - valLoss: 0.6682550311088562 - trainLoss: 0.6668850183486938\n",
      "cnt: 0 - valLoss: 0.6682531833648682 - trainLoss: 0.6668828129768372\n",
      "cnt: 0 - valLoss: 0.6682513952255249 - trainLoss: 0.66688072681427\n",
      "cnt: 0 - valLoss: 0.6682494878768921 - trainLoss: 0.6668785810470581\n",
      "cnt: 0 - valLoss: 0.6682476997375488 - trainLoss: 0.6668765544891357\n",
      "cnt: 0 - valLoss: 0.668245792388916 - trainLoss: 0.6668744087219238\n",
      "cnt: 0 - valLoss: 0.6682440042495728 - trainLoss: 0.6668722629547119\n",
      "cnt: 0 - valLoss: 0.6682420969009399 - trainLoss: 0.6668701767921448\n",
      "cnt: 0 - valLoss: 0.6682403087615967 - trainLoss: 0.6668680906295776\n",
      "cnt: 0 - valLoss: 0.6682384610176086 - trainLoss: 0.6668659448623657\n",
      "cnt: 0 - valLoss: 0.6682366728782654 - trainLoss: 0.6668638586997986\n",
      "cnt: 0 - valLoss: 0.6682348251342773 - trainLoss: 0.6668617725372314\n",
      "cnt: 0 - valLoss: 0.6682329773902893 - trainLoss: 0.6668596863746643\n",
      "cnt: 0 - valLoss: 0.6682311296463013 - trainLoss: 0.6668576002120972\n",
      "cnt: 0 - valLoss: 0.6682294011116028 - trainLoss: 0.6668554544448853\n",
      "cnt: 0 - valLoss: 0.6682275533676147 - trainLoss: 0.6668533682823181\n",
      "cnt: 0 - valLoss: 0.6682256460189819 - trainLoss: 0.666851282119751\n",
      "cnt: 0 - valLoss: 0.6682239770889282 - trainLoss: 0.6668491959571838\n",
      "cnt: 0 - valLoss: 0.6682220697402954 - trainLoss: 0.6668471097946167\n",
      "cnt: 0 - valLoss: 0.6682202816009521 - trainLoss: 0.6668450236320496\n",
      "cnt: 0 - valLoss: 0.6682184338569641 - trainLoss: 0.6668429374694824\n",
      "cnt: 0 - valLoss: 0.6682166457176208 - trainLoss: 0.6668409109115601\n",
      "cnt: 0 - valLoss: 0.6682148575782776 - trainLoss: 0.6668387651443481\n",
      "cnt: 0 - valLoss: 0.6682130694389343 - trainLoss: 0.6668367385864258\n",
      "cnt: 0 - valLoss: 0.6682112812995911 - trainLoss: 0.6668346524238586\n",
      "cnt: 0 - valLoss: 0.668209433555603 - trainLoss: 0.6668326258659363\n",
      "cnt: 0 - valLoss: 0.6682076454162598 - trainLoss: 0.6668305397033691\n",
      "cnt: 0 - valLoss: 0.6682058572769165 - trainLoss: 0.666828453540802\n",
      "cnt: 0 - valLoss: 0.6682040691375732 - trainLoss: 0.6668264865875244\n",
      "cnt: 0 - valLoss: 0.66820228099823 - trainLoss: 0.6668243408203125\n",
      "cnt: 0 - valLoss: 0.6682004332542419 - trainLoss: 0.6668223142623901\n",
      "cnt: 0 - valLoss: 0.6681987047195435 - trainLoss: 0.666820228099823\n",
      "cnt: 0 - valLoss: 0.6681969165802002 - trainLoss: 0.6668181419372559\n",
      "cnt: 0 - valLoss: 0.6681951284408569 - trainLoss: 0.6668161153793335\n",
      "cnt: 0 - valLoss: 0.6681933403015137 - trainLoss: 0.6668140888214111\n",
      "cnt: 0 - valLoss: 0.6681915521621704 - trainLoss: 0.6668120622634888\n",
      "cnt: 0 - valLoss: 0.6681897640228271 - trainLoss: 0.6668100357055664\n",
      "cnt: 0 - valLoss: 0.6681880354881287 - trainLoss: 0.6668079495429993\n",
      "cnt: 0 - valLoss: 0.6681862473487854 - trainLoss: 0.6668059229850769\n",
      "cnt: 0 - valLoss: 0.6681844592094421 - trainLoss: 0.6668038964271545\n",
      "cnt: 0 - valLoss: 0.6681827306747437 - trainLoss: 0.6668018698692322\n",
      "cnt: 0 - valLoss: 0.6681809425354004 - trainLoss: 0.6667998433113098\n",
      "cnt: 0 - valLoss: 0.6681792140007019 - trainLoss: 0.6667977571487427\n",
      "cnt: 0 - valLoss: 0.6681774258613586 - trainLoss: 0.6667957901954651\n",
      "cnt: 0 - valLoss: 0.6681756973266602 - trainLoss: 0.6667937636375427\n",
      "cnt: 0 - valLoss: 0.6681739091873169 - trainLoss: 0.6667916774749756\n",
      "cnt: 0 - valLoss: 0.6681721806526184 - trainLoss: 0.666789710521698\n",
      "cnt: 0 - valLoss: 0.6681703925132751 - trainLoss: 0.6667876839637756\n",
      "cnt: 0 - valLoss: 0.6681686639785767 - trainLoss: 0.6667856574058533\n",
      "cnt: 0 - valLoss: 0.6681669354438782 - trainLoss: 0.6667836308479309\n",
      "cnt: 0 - valLoss: 0.6681651473045349 - trainLoss: 0.6667816638946533\n",
      "cnt: 0 - valLoss: 0.6681633591651917 - trainLoss: 0.666779637336731\n",
      "cnt: 0 - valLoss: 0.6681616306304932 - trainLoss: 0.6667775511741638\n",
      "cnt: 0 - valLoss: 0.6681599617004395 - trainLoss: 0.6667755842208862\n",
      "cnt: 0 - valLoss: 0.6681581735610962 - trainLoss: 0.6667736172676086\n",
      "cnt: 0 - valLoss: 0.6681564450263977 - trainLoss: 0.6667715907096863\n",
      "cnt: 0 - valLoss: 0.6681546568870544 - trainLoss: 0.6667696237564087\n",
      "cnt: 0 - valLoss: 0.668152928352356 - trainLoss: 0.6667676568031311\n",
      "cnt: 0 - valLoss: 0.6681511998176575 - trainLoss: 0.666765570640564\n",
      "cnt: 0 - valLoss: 0.668149471282959 - trainLoss: 0.6667636036872864\n",
      "cnt: 0 - valLoss: 0.6681477427482605 - trainLoss: 0.6667616367340088\n",
      "cnt: 0 - valLoss: 0.668146014213562 - trainLoss: 0.6667596697807312\n",
      "cnt: 0 - valLoss: 0.6681442856788635 - trainLoss: 0.6667577028274536\n",
      "cnt: 0 - valLoss: 0.6681426167488098 - trainLoss: 0.666755735874176\n",
      "cnt: 0 - valLoss: 0.6681408882141113 - trainLoss: 0.6667537093162537\n",
      "cnt: 0 - valLoss: 0.6681391596794128 - trainLoss: 0.6667517423629761\n",
      "cnt: 0 - valLoss: 0.6681374311447144 - trainLoss: 0.6667497754096985\n",
      "cnt: 0 - valLoss: 0.6681357622146606 - trainLoss: 0.6667478084564209\n",
      "cnt: 0 - valLoss: 0.6681340336799622 - trainLoss: 0.6667457818984985\n",
      "cnt: 0 - valLoss: 0.6681323051452637 - trainLoss: 0.666743814945221\n",
      "cnt: 0 - valLoss: 0.6681305766105652 - trainLoss: 0.6667419075965881\n",
      "cnt: 0 - valLoss: 0.6681289076805115 - trainLoss: 0.6667399406433105\n",
      "cnt: 0 - valLoss: 0.668127179145813 - trainLoss: 0.666737973690033\n",
      "cnt: 0 - valLoss: 0.6681255102157593 - trainLoss: 0.6667359471321106\n",
      "cnt: 0 - valLoss: 0.6681238412857056 - trainLoss: 0.666733980178833\n",
      "cnt: 0 - valLoss: 0.6681221127510071 - trainLoss: 0.6667320728302002\n",
      "cnt: 0 - valLoss: 0.6681203842163086 - trainLoss: 0.6667301058769226\n",
      "cnt: 0 - valLoss: 0.6681187152862549 - trainLoss: 0.6667280793190002\n",
      "cnt: 0 - valLoss: 0.6681170463562012 - trainLoss: 0.6667262315750122\n",
      "cnt: 0 - valLoss: 0.6681153178215027 - trainLoss: 0.6667242646217346\n",
      "cnt: 0 - valLoss: 0.668113648891449 - trainLoss: 0.666722297668457\n",
      "cnt: 0 - valLoss: 0.6681119799613953 - trainLoss: 0.6667203307151794\n",
      "cnt: 0 - valLoss: 0.6681102514266968 - trainLoss: 0.6667184233665466\n",
      "cnt: 0 - valLoss: 0.6681085824966431 - trainLoss: 0.666716456413269\n",
      "cnt: 0 - valLoss: 0.6681069135665894 - trainLoss: 0.6667145490646362\n",
      "cnt: 0 - valLoss: 0.6681052446365356 - trainLoss: 0.6667125821113586\n",
      "cnt: 0 - valLoss: 0.6681035757064819 - trainLoss: 0.666710615158081\n",
      "cnt: 0 - valLoss: 0.6681019067764282 - trainLoss: 0.666708767414093\n",
      "cnt: 0 - valLoss: 0.6681001782417297 - trainLoss: 0.6667068004608154\n",
      "cnt: 0 - valLoss: 0.668098509311676 - trainLoss: 0.6667048335075378\n",
      "cnt: 0 - valLoss: 0.6680968403816223 - trainLoss: 0.666702926158905\n",
      "cnt: 0 - valLoss: 0.6680951714515686 - trainLoss: 0.666701078414917\n",
      "cnt: 0 - valLoss: 0.6680935025215149 - trainLoss: 0.6666991114616394\n",
      "cnt: 0 - valLoss: 0.668091893196106 - trainLoss: 0.6666972041130066\n",
      "cnt: 0 - valLoss: 0.6680902242660522 - trainLoss: 0.6666952967643738\n",
      "cnt: 0 - valLoss: 0.6680885553359985 - trainLoss: 0.6666933298110962\n",
      "cnt: 0 - valLoss: 0.6680868864059448 - trainLoss: 0.6666914820671082\n",
      "cnt: 0 - valLoss: 0.6680852770805359 - trainLoss: 0.6666895151138306\n",
      "cnt: 0 - valLoss: 0.6680836081504822 - trainLoss: 0.6666876077651978\n",
      "cnt: 0 - valLoss: 0.6680819392204285 - trainLoss: 0.6666857004165649\n",
      "cnt: 0 - valLoss: 0.6680802702903748 - trainLoss: 0.6666838526725769\n",
      "cnt: 0 - valLoss: 0.6680786609649658 - trainLoss: 0.6666819453239441\n",
      "cnt: 0 - valLoss: 0.6680769920349121 - trainLoss: 0.6666800379753113\n",
      "cnt: 0 - valLoss: 0.6680753231048584 - trainLoss: 0.6666781306266785\n",
      "cnt: 0 - valLoss: 0.6680736541748047 - trainLoss: 0.6666762232780457\n",
      "cnt: 0 - valLoss: 0.6680720448493958 - trainLoss: 0.6666743159294128\n",
      "cnt: 0 - valLoss: 0.6680704355239868 - trainLoss: 0.66667240858078\n",
      "cnt: 0 - valLoss: 0.6680687665939331 - trainLoss: 0.6666705012321472\n",
      "cnt: 0 - valLoss: 0.6680671572685242 - trainLoss: 0.6666686534881592\n",
      "cnt: 0 - valLoss: 0.6680655479431152 - trainLoss: 0.6666667461395264\n",
      "cnt: 0 - valLoss: 0.6680638790130615 - trainLoss: 0.6666648983955383\n",
      "cnt: 0 - valLoss: 0.6680622696876526 - trainLoss: 0.6666629314422607\n",
      "cnt: 0 - valLoss: 0.6680606603622437 - trainLoss: 0.6666611433029175\n",
      "cnt: 0 - valLoss: 0.6680589914321899 - trainLoss: 0.6666592359542847\n",
      "cnt: 0 - valLoss: 0.6680573225021362 - trainLoss: 0.6666573286056519\n",
      "cnt: 0 - valLoss: 0.6680557727813721 - trainLoss: 0.6666554808616638\n",
      "cnt: 0 - valLoss: 0.6680541038513184 - trainLoss: 0.6666536331176758\n",
      "cnt: 0 - valLoss: 0.6680524945259094 - trainLoss: 0.666651725769043\n",
      "cnt: 0 - valLoss: 0.6680508852005005 - trainLoss: 0.6666498780250549\n",
      "cnt: 0 - valLoss: 0.6680492162704468 - trainLoss: 0.6666479706764221\n",
      "cnt: 0 - valLoss: 0.6680476665496826 - trainLoss: 0.6666461229324341\n",
      "cnt: 0 - valLoss: 0.6680460572242737 - trainLoss: 0.666644275188446\n",
      "cnt: 0 - valLoss: 0.6680444478988647 - trainLoss: 0.666642427444458\n",
      "cnt: 0 - valLoss: 0.6680428385734558 - trainLoss: 0.6666405200958252\n",
      "cnt: 0 - valLoss: 0.6680412888526917 - trainLoss: 0.6666386723518372\n",
      "cnt: 0 - valLoss: 0.6680396199226379 - trainLoss: 0.6666368246078491\n",
      "cnt: 0 - valLoss: 0.668038010597229 - trainLoss: 0.6666350364685059\n",
      "cnt: 0 - valLoss: 0.6680364012718201 - trainLoss: 0.6666331887245178\n",
      "cnt: 0 - valLoss: 0.6680348515510559 - trainLoss: 0.666631281375885\n",
      "cnt: 0 - valLoss: 0.668033242225647 - trainLoss: 0.6666293740272522\n",
      "cnt: 0 - valLoss: 0.668031632900238 - trainLoss: 0.6666275858879089\n",
      "cnt: 0 - valLoss: 0.6680300235748291 - trainLoss: 0.6666257381439209\n",
      "cnt: 0 - valLoss: 0.6680284142494202 - trainLoss: 0.6666239500045776\n",
      "cnt: 0 - valLoss: 0.668026864528656 - trainLoss: 0.6666221022605896\n",
      "cnt: 0 - valLoss: 0.6680252552032471 - trainLoss: 0.6666202545166016\n",
      "cnt: 0 - valLoss: 0.6680236458778381 - trainLoss: 0.6666184663772583\n",
      "cnt: 0 - valLoss: 0.668022096157074 - trainLoss: 0.6666165590286255\n",
      "cnt: 0 - valLoss: 0.6680205464363098 - trainLoss: 0.6666147708892822\n",
      "cnt: 0 - valLoss: 0.6680189371109009 - trainLoss: 0.6666129231452942\n",
      "cnt: 0 - valLoss: 0.6680173277854919 - trainLoss: 0.6666110754013062\n",
      "cnt: 0 - valLoss: 0.6680158376693726 - trainLoss: 0.6666092872619629\n",
      "cnt: 0 - valLoss: 0.6680141687393188 - trainLoss: 0.6666074395179749\n",
      "cnt: 0 - valLoss: 0.6680126190185547 - trainLoss: 0.6666055917739868\n",
      "cnt: 0 - valLoss: 0.6680110692977905 - trainLoss: 0.6666038036346436\n",
      "cnt: 0 - valLoss: 0.6680094599723816 - trainLoss: 0.6666020154953003\n",
      "cnt: 0 - valLoss: 0.6680079102516174 - trainLoss: 0.6666001677513123\n",
      "cnt: 0 - valLoss: 0.6680063605308533 - trainLoss: 0.6665983200073242\n",
      "cnt: 0 - valLoss: 0.6680047512054443 - trainLoss: 0.666596531867981\n",
      "cnt: 0 - valLoss: 0.6680032014846802 - trainLoss: 0.6665947437286377\n",
      "cnt: 0 - valLoss: 0.668001651763916 - trainLoss: 0.6665928959846497\n",
      "cnt: 0 - valLoss: 0.6680001020431519 - trainLoss: 0.6665911078453064\n",
      "cnt: 0 - valLoss: 0.6679984927177429 - trainLoss: 0.6665893197059631\n",
      "cnt: 0 - valLoss: 0.6679969429969788 - trainLoss: 0.6665875315666199\n",
      "cnt: 0 - valLoss: 0.6679953932762146 - trainLoss: 0.6665856838226318\n",
      "cnt: 0 - valLoss: 0.6679938435554504 - trainLoss: 0.6665838360786438\n",
      "cnt: 0 - valLoss: 0.667992353439331 - trainLoss: 0.6665821671485901\n",
      "cnt: 0 - valLoss: 0.6679907441139221 - trainLoss: 0.6665803790092468\n",
      "cnt: 0 - valLoss: 0.6679892539978027 - trainLoss: 0.6665785312652588\n",
      "cnt: 0 - valLoss: 0.6679877042770386 - trainLoss: 0.6665767431259155\n",
      "cnt: 0 - valLoss: 0.6679860949516296 - trainLoss: 0.6665749549865723\n",
      "cnt: 0 - valLoss: 0.6679846048355103 - trainLoss: 0.666573166847229\n",
      "cnt: 0 - valLoss: 0.6679830551147461 - trainLoss: 0.6665713787078857\n",
      "cnt: 0 - valLoss: 0.6679815053939819 - trainLoss: 0.6665695905685425\n",
      "cnt: 0 - valLoss: 0.6679799556732178 - trainLoss: 0.6665678024291992\n",
      "cnt: 0 - valLoss: 0.6679784655570984 - trainLoss: 0.6665660738945007\n",
      "cnt: 0 - valLoss: 0.6679768562316895 - trainLoss: 0.6665642857551575\n",
      "cnt: 0 - valLoss: 0.6679753661155701 - trainLoss: 0.6665624976158142\n",
      "cnt: 0 - valLoss: 0.6679738759994507 - trainLoss: 0.6665606498718262\n",
      "cnt: 0 - valLoss: 0.6679723262786865 - trainLoss: 0.6665589213371277\n",
      "cnt: 0 - valLoss: 0.6679708361625671 - trainLoss: 0.6665571331977844\n",
      "cnt: 0 - valLoss: 0.667969286441803 - trainLoss: 0.6665554046630859\n",
      "cnt: 0 - valLoss: 0.6679677367210388 - trainLoss: 0.6665536165237427\n",
      "cnt: 0 - valLoss: 0.6679661870002747 - trainLoss: 0.6665518283843994\n",
      "cnt: 0 - valLoss: 0.6679646968841553 - trainLoss: 0.6665501594543457\n",
      "cnt: 0 - valLoss: 0.6679631471633911 - trainLoss: 0.6665483117103577\n",
      "cnt: 0 - valLoss: 0.6679616570472717 - trainLoss: 0.6665465831756592\n",
      "cnt: 0 - valLoss: 0.6679601669311523 - trainLoss: 0.6665447950363159\n",
      "cnt: 0 - valLoss: 0.6679586172103882 - trainLoss: 0.6665430068969727\n",
      "cnt: 0 - valLoss: 0.667957067489624 - trainLoss: 0.666541337966919\n",
      "cnt: 0 - valLoss: 0.6679556369781494 - trainLoss: 0.6665395498275757\n",
      "cnt: 0 - valLoss: 0.6679540872573853 - trainLoss: 0.6665378212928772\n",
      "cnt: 0 - valLoss: 0.6679525375366211 - trainLoss: 0.6665360331535339\n",
      "cnt: 0 - valLoss: 0.6679511070251465 - trainLoss: 0.6665343046188354\n",
      "cnt: 0 - valLoss: 0.6679495573043823 - trainLoss: 0.6665325164794922\n",
      "cnt: 0 - valLoss: 0.6679480671882629 - trainLoss: 0.6665308475494385\n",
      "cnt: 0 - valLoss: 0.6679465770721436 - trainLoss: 0.66652911901474\n",
      "cnt: 0 - valLoss: 0.6679450869560242 - trainLoss: 0.6665273904800415\n",
      "cnt: 0 - valLoss: 0.6679435968399048 - trainLoss: 0.666525661945343\n",
      "cnt: 0 - valLoss: 0.6679420471191406 - trainLoss: 0.6665238738059998\n",
      "cnt: 0 - valLoss: 0.667940616607666 - trainLoss: 0.6665221452713013\n",
      "cnt: 0 - valLoss: 0.6679390668869019 - trainLoss: 0.6665204167366028\n",
      "cnt: 0 - valLoss: 0.6679375767707825 - trainLoss: 0.6665186882019043\n",
      "cnt: 0 - valLoss: 0.6679360866546631 - trainLoss: 0.6665169596672058\n",
      "cnt: 0 - valLoss: 0.6679346561431885 - trainLoss: 0.6665152311325073\n",
      "cnt: 0 - valLoss: 0.6679331064224243 - trainLoss: 0.6665135622024536\n",
      "cnt: 0 - valLoss: 0.6679316163063049 - trainLoss: 0.6665118336677551\n",
      "cnt: 0 - valLoss: 0.6679301857948303 - trainLoss: 0.6665100455284119\n",
      "cnt: 0 - valLoss: 0.6679286956787109 - trainLoss: 0.6665083765983582\n",
      "cnt: 0 - valLoss: 0.6679272055625916 - trainLoss: 0.6665066480636597\n",
      "cnt: 0 - valLoss: 0.6679257750511169 - trainLoss: 0.6665049195289612\n",
      "cnt: 0 - valLoss: 0.6679242849349976 - trainLoss: 0.6665032505989075\n",
      "cnt: 0 - valLoss: 0.6679227948188782 - trainLoss: 0.666501522064209\n",
      "cnt: 0 - valLoss: 0.667921245098114 - trainLoss: 0.6664997935295105\n",
      "cnt: 0 - valLoss: 0.6679198145866394 - trainLoss: 0.666498064994812\n",
      "cnt: 0 - valLoss: 0.66791832447052 - trainLoss: 0.6664963960647583\n",
      "cnt: 0 - valLoss: 0.6679168939590454 - trainLoss: 0.6664947271347046\n",
      "cnt: 0 - valLoss: 0.667915403842926 - trainLoss: 0.6664929986000061\n",
      "cnt: 0 - valLoss: 0.6679139733314514 - trainLoss: 0.6664912700653076\n",
      "cnt: 0 - valLoss: 0.6679125428199768 - trainLoss: 0.6664895415306091\n",
      "cnt: 0 - valLoss: 0.6679110527038574 - trainLoss: 0.6664878726005554\n",
      "cnt: 0 - valLoss: 0.667909562587738 - trainLoss: 0.6664862036705017\n",
      "cnt: 0 - valLoss: 0.6679081320762634 - trainLoss: 0.6664844751358032\n",
      "cnt: 0 - valLoss: 0.6679067015647888 - trainLoss: 0.6664828658103943\n",
      "cnt: 0 - valLoss: 0.6679052114486694 - trainLoss: 0.6664811372756958\n",
      "cnt: 0 - valLoss: 0.6679037809371948 - trainLoss: 0.6664794087409973\n",
      "cnt: 0 - valLoss: 0.6679022908210754 - trainLoss: 0.6664777398109436\n",
      "cnt: 0 - valLoss: 0.6679008603096008 - trainLoss: 0.6664760708808899\n",
      "cnt: 0 - valLoss: 0.6678993701934814 - trainLoss: 0.6664744019508362\n",
      "cnt: 0 - valLoss: 0.6678979992866516 - trainLoss: 0.6664727330207825\n",
      "cnt: 0 - valLoss: 0.6678965091705322 - trainLoss: 0.6664710640907288\n",
      "cnt: 0 - valLoss: 0.6678951382637024 - trainLoss: 0.6664693355560303\n",
      "cnt: 0 - valLoss: 0.667893648147583 - trainLoss: 0.6664676666259766\n",
      "cnt: 0 - valLoss: 0.6678922176361084 - trainLoss: 0.6664659976959229\n",
      "cnt: 0 - valLoss: 0.667890727519989 - trainLoss: 0.6664643287658691\n",
      "cnt: 0 - valLoss: 0.6678892970085144 - trainLoss: 0.6664626598358154\n",
      "cnt: 0 - valLoss: 0.6678879261016846 - trainLoss: 0.6664609909057617\n",
      "cnt: 0 - valLoss: 0.6678864359855652 - trainLoss: 0.666459321975708\n",
      "cnt: 0 - valLoss: 0.6678850650787354 - trainLoss: 0.6664576530456543\n",
      "cnt: 0 - valLoss: 0.6678836345672607 - trainLoss: 0.6664560437202454\n",
      "cnt: 0 - valLoss: 0.6678821444511414 - trainLoss: 0.6664543151855469\n",
      "cnt: 0 - valLoss: 0.6678807139396667 - trainLoss: 0.6664527058601379\n",
      "cnt: 0 - valLoss: 0.6678793430328369 - trainLoss: 0.6664509773254395\n",
      "cnt: 0 - valLoss: 0.6678779125213623 - trainLoss: 0.6664493680000305\n",
      "cnt: 0 - valLoss: 0.6678764820098877 - trainLoss: 0.6664477586746216\n",
      "cnt: 0 - valLoss: 0.6678750514984131 - trainLoss: 0.6664460897445679\n",
      "cnt: 0 - valLoss: 0.6678736209869385 - trainLoss: 0.6664444208145142\n",
      "cnt: 0 - valLoss: 0.6678722500801086 - trainLoss: 0.6664428114891052\n",
      "cnt: 0 - valLoss: 0.6678707599639893 - trainLoss: 0.6664411425590515\n",
      "cnt: 0 - valLoss: 0.6678693890571594 - trainLoss: 0.6664394736289978\n",
      "cnt: 0 - valLoss: 0.6678679585456848 - trainLoss: 0.6664378046989441\n",
      "cnt: 0 - valLoss: 0.6678665280342102 - trainLoss: 0.6664361953735352\n",
      "cnt: 0 - valLoss: 0.6678652167320251 - trainLoss: 0.6664345860481262\n",
      "cnt: 0 - valLoss: 0.6678637266159058 - trainLoss: 0.6664329171180725\n",
      "cnt: 0 - valLoss: 0.6678622961044312 - trainLoss: 0.6664313077926636\n",
      "cnt: 0 - valLoss: 0.6678609251976013 - trainLoss: 0.6664296388626099\n",
      "cnt: 0 - valLoss: 0.6678595542907715 - trainLoss: 0.6664280295372009\n",
      "cnt: 0 - valLoss: 0.6678580641746521 - trainLoss: 0.6664263606071472\n",
      "cnt: 0 - valLoss: 0.6678566932678223 - trainLoss: 0.6664248108863831\n",
      "cnt: 0 - valLoss: 0.6678553223609924 - trainLoss: 0.6664230823516846\n",
      "cnt: 0 - valLoss: 0.6678539514541626 - trainLoss: 0.6664215326309204\n",
      "cnt: 0 - valLoss: 0.667852520942688 - trainLoss: 0.6664198637008667\n",
      "cnt: 0 - valLoss: 0.6678511500358582 - trainLoss: 0.6664182543754578\n",
      "cnt: 0 - valLoss: 0.6678497791290283 - trainLoss: 0.6664167046546936\n",
      "cnt: 0 - valLoss: 0.6678483486175537 - trainLoss: 0.6664150357246399\n",
      "cnt: 0 - valLoss: 0.6678469181060791 - trainLoss: 0.666413426399231\n",
      "cnt: 0 - valLoss: 0.6678455471992493 - trainLoss: 0.666411817073822\n",
      "cnt: 0 - valLoss: 0.6678441762924194 - trainLoss: 0.6664101481437683\n",
      "cnt: 0 - valLoss: 0.6678428053855896 - trainLoss: 0.6664085388183594\n",
      "cnt: 0 - valLoss: 0.6678414344787598 - trainLoss: 0.6664069294929504\n",
      "cnt: 0 - valLoss: 0.6678400635719299 - trainLoss: 0.6664053797721863\n",
      "cnt: 0 - valLoss: 0.6678386926651001 - trainLoss: 0.6664037108421326\n",
      "cnt: 0 - valLoss: 0.6678372621536255 - trainLoss: 0.6664021015167236\n",
      "cnt: 0 - valLoss: 0.6678358912467957 - trainLoss: 0.6664004921913147\n",
      "cnt: 0 - valLoss: 0.6678345203399658 - trainLoss: 0.6663989424705505\n",
      "cnt: 0 - valLoss: 0.667833149433136 - trainLoss: 0.6663973331451416\n",
      "cnt: 0 - valLoss: 0.6678317785263062 - trainLoss: 0.6663957238197327\n",
      "cnt: 0 - valLoss: 0.6678304076194763 - trainLoss: 0.6663941144943237\n",
      "cnt: 0 - valLoss: 0.6678290367126465 - trainLoss: 0.6663925647735596\n",
      "cnt: 0 - valLoss: 0.6678276658058167 - trainLoss: 0.6663909554481506\n",
      "cnt: 0 - valLoss: 0.6678262948989868 - trainLoss: 0.6663893461227417\n",
      "cnt: 0 - valLoss: 0.667824923992157 - trainLoss: 0.6663877367973328\n",
      "cnt: 0 - valLoss: 0.6678236126899719 - trainLoss: 0.6663861870765686\n",
      "cnt: 0 - valLoss: 0.6678222417831421 - trainLoss: 0.6663845777511597\n",
      "cnt: 0 - valLoss: 0.6678208112716675 - trainLoss: 0.6663829684257507\n",
      "cnt: 0 - valLoss: 0.6678194999694824 - trainLoss: 0.6663814187049866\n",
      "cnt: 0 - valLoss: 0.6678181290626526 - trainLoss: 0.6663798093795776\n",
      "cnt: 0 - valLoss: 0.6678167581558228 - trainLoss: 0.6663782596588135\n",
      "cnt: 0 - valLoss: 0.6678153872489929 - trainLoss: 0.6663765907287598\n",
      "cnt: 0 - valLoss: 0.6678140759468079 - trainLoss: 0.6663751006126404\n",
      "cnt: 0 - valLoss: 0.667812705039978 - trainLoss: 0.6663735508918762\n",
      "cnt: 0 - valLoss: 0.6678113341331482 - trainLoss: 0.6663719415664673\n",
      "cnt: 0 - valLoss: 0.6678099632263184 - trainLoss: 0.6663703918457031\n",
      "cnt: 0 - valLoss: 0.6678085923194885 - trainLoss: 0.6663687825202942\n",
      "cnt: 0 - valLoss: 0.6678073406219482 - trainLoss: 0.66636723279953\n",
      "cnt: 0 - valLoss: 0.6678060293197632 - trainLoss: 0.6663656830787659\n",
      "cnt: 0 - valLoss: 0.6678045988082886 - trainLoss: 0.6663641333580017\n",
      "cnt: 0 - valLoss: 0.6678032875061035 - trainLoss: 0.6663625836372375\n",
      "cnt: 0 - valLoss: 0.6678019165992737 - trainLoss: 0.6663609743118286\n",
      "cnt: 0 - valLoss: 0.6678006052970886 - trainLoss: 0.6663594245910645\n",
      "cnt: 0 - valLoss: 0.6677992939949036 - trainLoss: 0.6663578748703003\n",
      "cnt: 0 - valLoss: 0.6677979230880737 - trainLoss: 0.6663563251495361\n",
      "cnt: 0 - valLoss: 0.6677966117858887 - trainLoss: 0.6663547158241272\n",
      "cnt: 0 - valLoss: 0.6677953004837036 - trainLoss: 0.6663532257080078\n",
      "cnt: 0 - valLoss: 0.6677939891815186 - trainLoss: 0.6663516163825989\n",
      "cnt: 0 - valLoss: 0.667792558670044 - trainLoss: 0.6663500666618347\n",
      "cnt: 0 - valLoss: 0.6677913069725037 - trainLoss: 0.6663485765457153\n",
      "cnt: 0 - valLoss: 0.6677899956703186 - trainLoss: 0.6663469672203064\n",
      "cnt: 0 - valLoss: 0.6677886247634888 - trainLoss: 0.666345477104187\n",
      "cnt: 0 - valLoss: 0.6677873134613037 - trainLoss: 0.6663439273834229\n",
      "cnt: 0 - valLoss: 0.6677860021591187 - trainLoss: 0.6663423776626587\n",
      "cnt: 0 - valLoss: 0.6677846908569336 - trainLoss: 0.6663408279418945\n",
      "cnt: 0 - valLoss: 0.6677833199501038 - trainLoss: 0.6663392782211304\n",
      "cnt: 0 - valLoss: 0.6677820682525635 - trainLoss: 0.6663377285003662\n",
      "cnt: 0 - valLoss: 0.6677807569503784 - trainLoss: 0.6663362383842468\n",
      "cnt: 0 - valLoss: 0.6677793860435486 - trainLoss: 0.6663346886634827\n",
      "cnt: 0 - valLoss: 0.6677780747413635 - trainLoss: 0.6663331389427185\n",
      "cnt: 0 - valLoss: 0.6677767634391785 - trainLoss: 0.6663315892219543\n",
      "cnt: 0 - valLoss: 0.6677755117416382 - trainLoss: 0.666330099105835\n",
      "cnt: 0 - valLoss: 0.6677741408348083 - trainLoss: 0.6663285493850708\n",
      "cnt: 0 - valLoss: 0.6677727699279785 - trainLoss: 0.6663270592689514\n",
      "cnt: 0 - valLoss: 0.6677715182304382 - trainLoss: 0.6663255095481873\n",
      "cnt: 0 - valLoss: 0.6677702069282532 - trainLoss: 0.6663239598274231\n",
      "cnt: 0 - valLoss: 0.6677689552307129 - trainLoss: 0.6663224697113037\n",
      "cnt: 0 - valLoss: 0.6677675843238831 - trainLoss: 0.6663209199905396\n",
      "cnt: 0 - valLoss: 0.6677663326263428 - trainLoss: 0.6663194298744202\n",
      "cnt: 0 - valLoss: 0.6677649617195129 - trainLoss: 0.666317880153656\n",
      "cnt: 0 - valLoss: 0.6677637100219727 - trainLoss: 0.6663163900375366\n",
      "cnt: 0 - valLoss: 0.6677624583244324 - trainLoss: 0.6663148999214172\n",
      "cnt: 0 - valLoss: 0.6677611470222473 - trainLoss: 0.6663133502006531\n",
      "cnt: 0 - valLoss: 0.6677598357200623 - trainLoss: 0.6663118600845337\n",
      "cnt: 0 - valLoss: 0.667758584022522 - trainLoss: 0.6663103699684143\n",
      "cnt: 0 - valLoss: 0.6677572727203369 - trainLoss: 0.6663088798522949\n",
      "cnt: 0 - valLoss: 0.6677559614181519 - trainLoss: 0.6663073897361755\n",
      "cnt: 0 - valLoss: 0.6677547097206116 - trainLoss: 0.6663058400154114\n",
      "cnt: 0 - valLoss: 0.6677533984184265 - trainLoss: 0.666304349899292\n",
      "cnt: 0 - valLoss: 0.6677521467208862 - trainLoss: 0.6663028001785278\n",
      "cnt: 0 - valLoss: 0.6677508354187012 - trainLoss: 0.6663013696670532\n",
      "cnt: 0 - valLoss: 0.6677495837211609 - trainLoss: 0.6662998199462891\n",
      "cnt: 0 - valLoss: 0.6677483320236206 - trainLoss: 0.6662983298301697\n",
      "cnt: 0 - valLoss: 0.6677469611167908 - trainLoss: 0.6662968397140503\n",
      "cnt: 0 - valLoss: 0.6677457690238953 - trainLoss: 0.6662953495979309\n",
      "cnt: 0 - valLoss: 0.6677443981170654 - trainLoss: 0.6662938594818115\n",
      "cnt: 0 - valLoss: 0.6677431464195251 - trainLoss: 0.6662923693656921\n",
      "cnt: 0 - valLoss: 0.6677418947219849 - trainLoss: 0.6662908792495728\n",
      "cnt: 0 - valLoss: 0.6677405834197998 - trainLoss: 0.6662893891334534\n",
      "cnt: 0 - valLoss: 0.6677393317222595 - trainLoss: 0.6662879586219788\n",
      "cnt: 0 - valLoss: 0.6677380800247192 - trainLoss: 0.6662864089012146\n",
      "cnt: 0 - valLoss: 0.6677367687225342 - trainLoss: 0.6662849187850952\n",
      "cnt: 0 - valLoss: 0.6677355170249939 - trainLoss: 0.6662834882736206\n",
      "cnt: 0 - valLoss: 0.6677343249320984 - trainLoss: 0.6662819981575012\n",
      "cnt: 0 - valLoss: 0.6677330136299133 - trainLoss: 0.6662805080413818\n",
      "cnt: 0 - valLoss: 0.6677317023277283 - trainLoss: 0.6662790179252625\n",
      "cnt: 0 - valLoss: 0.6677305102348328 - trainLoss: 0.6662775874137878\n",
      "cnt: 0 - valLoss: 0.6677292585372925 - trainLoss: 0.6662760972976685\n",
      "cnt: 0 - valLoss: 0.6677280068397522 - trainLoss: 0.6662746071815491\n",
      "cnt: 0 - valLoss: 0.6677266955375671 - trainLoss: 0.6662731170654297\n",
      "cnt: 0 - valLoss: 0.6677255034446716 - trainLoss: 0.6662716865539551\n",
      "cnt: 0 - valLoss: 0.6677242517471313 - trainLoss: 0.6662701964378357\n",
      "cnt: 0 - valLoss: 0.6677230000495911 - trainLoss: 0.6662687659263611\n",
      "cnt: 0 - valLoss: 0.6677218079566956 - trainLoss: 0.6662672758102417\n",
      "cnt: 0 - valLoss: 0.6677204370498657 - trainLoss: 0.6662658452987671\n",
      "cnt: 0 - valLoss: 0.6677192449569702 - trainLoss: 0.6662644147872925\n",
      "cnt: 0 - valLoss: 0.6677179932594299 - trainLoss: 0.6662629246711731\n",
      "cnt: 0 - valLoss: 0.6677167415618896 - trainLoss: 0.6662614941596985\n",
      "cnt: 0 - valLoss: 0.6677155494689941 - trainLoss: 0.6662600040435791\n",
      "cnt: 0 - valLoss: 0.6677142977714539 - trainLoss: 0.6662585735321045\n",
      "cnt: 0 - valLoss: 0.6677130460739136 - trainLoss: 0.6662570834159851\n",
      "cnt: 0 - valLoss: 0.6677117347717285 - trainLoss: 0.6662556529045105\n",
      "cnt: 0 - valLoss: 0.667710542678833 - trainLoss: 0.6662542223930359\n",
      "cnt: 0 - valLoss: 0.6677093505859375 - trainLoss: 0.6662527322769165\n",
      "cnt: 0 - valLoss: 0.6677080392837524 - trainLoss: 0.6662513017654419\n",
      "cnt: 0 - valLoss: 0.6677068471908569 - trainLoss: 0.6662498712539673\n",
      "cnt: 0 - valLoss: 0.6677056550979614 - trainLoss: 0.6662484407424927\n",
      "cnt: 0 - valLoss: 0.6677044034004211 - trainLoss: 0.6662470102310181\n",
      "cnt: 0 - valLoss: 0.6677031517028809 - trainLoss: 0.6662455797195435\n",
      "cnt: 0 - valLoss: 0.6677019596099854 - trainLoss: 0.6662440896034241\n",
      "cnt: 0 - valLoss: 0.6677007079124451 - trainLoss: 0.6662426590919495\n",
      "cnt: 0 - valLoss: 0.6676995158195496 - trainLoss: 0.6662412881851196\n",
      "cnt: 0 - valLoss: 0.6676982641220093 - trainLoss: 0.6662397980690002\n",
      "cnt: 0 - valLoss: 0.6676970720291138 - trainLoss: 0.6662383675575256\n",
      "cnt: 0 - valLoss: 0.6676958203315735 - trainLoss: 0.666236937046051\n",
      "cnt: 0 - valLoss: 0.667694628238678 - trainLoss: 0.6662355661392212\n",
      "cnt: 0 - valLoss: 0.6676933765411377 - trainLoss: 0.6662340760231018\n",
      "cnt: 0 - valLoss: 0.6676921844482422 - trainLoss: 0.6662326455116272\n",
      "cnt: 0 - valLoss: 0.6676909923553467 - trainLoss: 0.6662312746047974\n",
      "cnt: 0 - valLoss: 0.6676897406578064 - trainLoss: 0.6662298440933228\n",
      "cnt: 0 - valLoss: 0.6676885485649109 - trainLoss: 0.6662284135818481\n",
      "cnt: 0 - valLoss: 0.6676872968673706 - trainLoss: 0.6662269830703735\n",
      "cnt: 0 - valLoss: 0.6676861047744751 - trainLoss: 0.6662255525588989\n",
      "cnt: 0 - valLoss: 0.6676849126815796 - trainLoss: 0.6662241816520691\n",
      "cnt: 0 - valLoss: 0.6676837205886841 - trainLoss: 0.6662226915359497\n",
      "cnt: 0 - valLoss: 0.6676825284957886 - trainLoss: 0.6662213206291199\n",
      "cnt: 0 - valLoss: 0.6676813364028931 - trainLoss: 0.6662198901176453\n",
      "cnt: 0 - valLoss: 0.667680025100708 - trainLoss: 0.6662185192108154\n",
      "cnt: 0 - valLoss: 0.6676788926124573 - trainLoss: 0.6662171483039856\n",
      "cnt: 0 - valLoss: 0.6676777005195618 - trainLoss: 0.6662156581878662\n",
      "cnt: 0 - valLoss: 0.6676765084266663 - trainLoss: 0.6662142872810364\n",
      "cnt: 0 - valLoss: 0.667675256729126 - trainLoss: 0.6662128567695618\n",
      "cnt: 0 - valLoss: 0.6676741242408752 - trainLoss: 0.6662114262580872\n",
      "cnt: 0 - valLoss: 0.6676729321479797 - trainLoss: 0.6662100553512573\n",
      "cnt: 0 - valLoss: 0.6676717400550842 - trainLoss: 0.6662086248397827\n",
      "cnt: 0 - valLoss: 0.667670488357544 - trainLoss: 0.6662072539329529\n",
      "cnt: 0 - valLoss: 0.6676692962646484 - trainLoss: 0.6662058234214783\n",
      "cnt: 0 - valLoss: 0.6676681041717529 - trainLoss: 0.6662044525146484\n",
      "cnt: 0 - valLoss: 0.6676669716835022 - trainLoss: 0.6662030816078186\n",
      "cnt: 0 - valLoss: 0.6676657795906067 - trainLoss: 0.6662017107009888\n",
      "cnt: 0 - valLoss: 0.6676645874977112 - trainLoss: 0.6662002801895142\n",
      "cnt: 0 - valLoss: 0.6676633954048157 - trainLoss: 0.6661988496780396\n",
      "cnt: 0 - valLoss: 0.6676622033119202 - trainLoss: 0.6661975383758545\n",
      "cnt: 0 - valLoss: 0.6676610708236694 - trainLoss: 0.6661960482597351\n",
      "cnt: 0 - valLoss: 0.6676598191261292 - trainLoss: 0.66619473695755\n",
      "cnt: 0 - valLoss: 0.6676586270332336 - trainLoss: 0.6661933660507202\n",
      "cnt: 0 - valLoss: 0.6676574945449829 - trainLoss: 0.6661919951438904\n",
      "cnt: 0 - valLoss: 0.6676563024520874 - trainLoss: 0.6661905646324158\n",
      "cnt: 0 - valLoss: 0.6676551103591919 - trainLoss: 0.6661892533302307\n",
      "cnt: 0 - valLoss: 0.6676539778709412 - trainLoss: 0.6661878228187561\n",
      "cnt: 0 - valLoss: 0.6676527857780457 - trainLoss: 0.666186511516571\n",
      "cnt: 0 - valLoss: 0.6676516532897949 - trainLoss: 0.6661850810050964\n",
      "cnt: 0 - valLoss: 0.6676504611968994 - trainLoss: 0.6661837100982666\n",
      "cnt: 0 - valLoss: 0.6676492691040039 - trainLoss: 0.666182279586792\n",
      "cnt: 0 - valLoss: 0.6676480770111084 - trainLoss: 0.6661809682846069\n",
      "cnt: 0 - valLoss: 0.6676469445228577 - trainLoss: 0.6661795973777771\n",
      "cnt: 0 - valLoss: 0.6676457524299622 - trainLoss: 0.6661782264709473\n",
      "cnt: 0 - valLoss: 0.6676445603370667 - trainLoss: 0.6661767959594727\n",
      "cnt: 0 - valLoss: 0.6676434278488159 - trainLoss: 0.6661754250526428\n",
      "cnt: 0 - valLoss: 0.6676422953605652 - trainLoss: 0.666174054145813\n",
      "cnt: 0 - valLoss: 0.6676411032676697 - trainLoss: 0.6661728024482727\n",
      "cnt: 0 - valLoss: 0.667639970779419 - trainLoss: 0.6661713719367981\n",
      "cnt: 0 - valLoss: 0.6676387786865234 - trainLoss: 0.666170060634613\n",
      "cnt: 0 - valLoss: 0.6676376461982727 - trainLoss: 0.6661686897277832\n",
      "cnt: 0 - valLoss: 0.667636513710022 - trainLoss: 0.6661673188209534\n",
      "cnt: 0 - valLoss: 0.6676353216171265 - trainLoss: 0.6661659479141235\n",
      "cnt: 0 - valLoss: 0.6676341891288757 - trainLoss: 0.6661646366119385\n",
      "cnt: 0 - valLoss: 0.667633056640625 - trainLoss: 0.6661632657051086\n",
      "cnt: 0 - valLoss: 0.6676319241523743 - trainLoss: 0.6661618947982788\n",
      "cnt: 0 - valLoss: 0.6676307320594788 - trainLoss: 0.666160523891449\n",
      "cnt: 0 - valLoss: 0.667629599571228 - trainLoss: 0.6661591529846191\n",
      "cnt: 0 - valLoss: 0.6676284074783325 - trainLoss: 0.6661578416824341\n",
      "cnt: 0 - valLoss: 0.6676273345947266 - trainLoss: 0.666156530380249\n",
      "cnt: 0 - valLoss: 0.667626142501831 - trainLoss: 0.6661551594734192\n",
      "cnt: 0 - valLoss: 0.6676250100135803 - trainLoss: 0.6661537885665894\n",
      "cnt: 0 - valLoss: 0.6676238775253296 - trainLoss: 0.6661524772644043\n",
      "cnt: 0 - valLoss: 0.6676227450370789 - trainLoss: 0.6661511063575745\n",
      "cnt: 0 - valLoss: 0.6676216125488281 - trainLoss: 0.6661497950553894\n",
      "cnt: 0 - valLoss: 0.6676204800605774 - trainLoss: 0.6661484241485596\n",
      "cnt: 0 - valLoss: 0.6676193475723267 - trainLoss: 0.6661471128463745\n",
      "cnt: 0 - valLoss: 0.6676182150840759 - trainLoss: 0.6661457419395447\n",
      "cnt: 0 - valLoss: 0.6676170229911804 - trainLoss: 0.6661444306373596\n",
      "cnt: 0 - valLoss: 0.6676158905029297 - trainLoss: 0.6661430597305298\n",
      "cnt: 0 - valLoss: 0.6676148176193237 - trainLoss: 0.6661418080329895\n",
      "cnt: 0 - valLoss: 0.6676136255264282 - trainLoss: 0.6661404967308044\n",
      "cnt: 0 - valLoss: 0.6676125526428223 - trainLoss: 0.6661391258239746\n",
      "cnt: 0 - valLoss: 0.6676114797592163 - trainLoss: 0.6661377549171448\n",
      "cnt: 0 - valLoss: 0.667610228061676 - trainLoss: 0.6661365032196045\n",
      "cnt: 0 - valLoss: 0.6676091551780701 - trainLoss: 0.6661351919174194\n",
      "cnt: 0 - valLoss: 0.6676080226898193 - trainLoss: 0.6661338210105896\n",
      "cnt: 0 - valLoss: 0.6676068902015686 - trainLoss: 0.6661325097084045\n",
      "cnt: 0 - valLoss: 0.6676058173179626 - trainLoss: 0.6661311984062195\n",
      "cnt: 0 - valLoss: 0.6676046252250671 - trainLoss: 0.6661298871040344\n",
      "cnt: 0 - valLoss: 0.6676035523414612 - trainLoss: 0.6661285161972046\n",
      "cnt: 0 - valLoss: 0.6676024794578552 - trainLoss: 0.6661272644996643\n",
      "cnt: 0 - valLoss: 0.6676013469696045 - trainLoss: 0.6661259531974792\n",
      "cnt: 0 - valLoss: 0.667600154876709 - trainLoss: 0.6661245822906494\n",
      "cnt: 0 - valLoss: 0.667599081993103 - trainLoss: 0.6661233305931091\n",
      "cnt: 0 - valLoss: 0.6675980091094971 - trainLoss: 0.6661220192909241\n",
      "cnt: 0 - valLoss: 0.6675968766212463 - trainLoss: 0.666120707988739\n",
      "cnt: 0 - valLoss: 0.6675957441329956 - trainLoss: 0.666119396686554\n",
      "cnt: 0 - valLoss: 0.6675946712493896 - trainLoss: 0.6661180257797241\n",
      "cnt: 0 - valLoss: 0.6675935387611389 - trainLoss: 0.6661167740821838\n",
      "cnt: 0 - valLoss: 0.6675924062728882 - trainLoss: 0.6661154627799988\n",
      "cnt: 0 - valLoss: 0.6675913333892822 - trainLoss: 0.6661141514778137\n",
      "cnt: 0 - valLoss: 0.6675902009010315 - trainLoss: 0.6661128997802734\n",
      "cnt: 0 - valLoss: 0.6675890684127808 - trainLoss: 0.6661115288734436\n",
      "cnt: 0 - valLoss: 0.6675879955291748 - trainLoss: 0.6661102771759033\n",
      "cnt: 0 - valLoss: 0.6675869226455688 - trainLoss: 0.6661089062690735\n",
      "cnt: 0 - valLoss: 0.6675857305526733 - trainLoss: 0.6661076545715332\n",
      "cnt: 0 - valLoss: 0.6675847172737122 - trainLoss: 0.6661063432693481\n",
      "cnt: 0 - valLoss: 0.6675835847854614 - trainLoss: 0.6661050915718079\n",
      "cnt: 0 - valLoss: 0.6675825119018555 - trainLoss: 0.6661037802696228\n",
      "cnt: 0 - valLoss: 0.6675814390182495 - trainLoss: 0.6661024689674377\n",
      "cnt: 0 - valLoss: 0.6675803661346436 - trainLoss: 0.6661012172698975\n",
      "cnt: 0 - valLoss: 0.6675792932510376 - trainLoss: 0.6660999059677124\n",
      "cnt: 0 - valLoss: 0.6675782203674316 - trainLoss: 0.6660986542701721\n",
      "cnt: 0 - valLoss: 0.6675770878791809 - trainLoss: 0.6660973429679871\n",
      "cnt: 0 - valLoss: 0.6675759553909302 - trainLoss: 0.666096031665802\n",
      "cnt: 0 - valLoss: 0.667574942111969 - trainLoss: 0.6660947799682617\n",
      "cnt: 0 - valLoss: 0.6675738096237183 - trainLoss: 0.6660935282707214\n",
      "cnt: 0 - valLoss: 0.6675727367401123 - trainLoss: 0.6660922169685364\n",
      "cnt: 0 - valLoss: 0.6675716638565063 - trainLoss: 0.6660909652709961\n",
      "cnt: 0 - valLoss: 0.6675705909729004 - trainLoss: 0.666089653968811\n",
      "cnt: 0 - valLoss: 0.6675695776939392 - trainLoss: 0.6660884022712708\n",
      "cnt: 0 - valLoss: 0.6675684452056885 - trainLoss: 0.6660870909690857\n",
      "cnt: 0 - valLoss: 0.6675673723220825 - trainLoss: 0.6660858988761902\n",
      "cnt: 0 - valLoss: 0.6675662994384766 - trainLoss: 0.6660846471786499\n",
      "cnt: 0 - valLoss: 0.6675652265548706 - trainLoss: 0.6660833358764648\n",
      "cnt: 0 - valLoss: 0.6675641536712646 - trainLoss: 0.6660820245742798\n",
      "cnt: 0 - valLoss: 0.6675631403923035 - trainLoss: 0.6660807728767395\n",
      "cnt: 0 - valLoss: 0.6675620675086975 - trainLoss: 0.6660794615745544\n",
      "cnt: 0 - valLoss: 0.6675609350204468 - trainLoss: 0.6660782694816589\n",
      "cnt: 0 - valLoss: 0.6675599217414856 - trainLoss: 0.6660770177841187\n",
      "cnt: 0 - valLoss: 0.6675588488578796 - trainLoss: 0.6660757064819336\n",
      "cnt: 0 - valLoss: 0.6675577759742737 - trainLoss: 0.6660744547843933\n",
      "cnt: 0 - valLoss: 0.6675567626953125 - trainLoss: 0.6660732626914978\n",
      "cnt: 0 - valLoss: 0.6675556302070618 - trainLoss: 0.6660719513893127\n",
      "cnt: 0 - valLoss: 0.6675546169281006 - trainLoss: 0.6660706400871277\n",
      "cnt: 0 - valLoss: 0.6675534844398499 - trainLoss: 0.6660694479942322\n",
      "cnt: 0 - valLoss: 0.6675524711608887 - trainLoss: 0.6660681962966919\n",
      "cnt: 0 - valLoss: 0.6675514578819275 - trainLoss: 0.6660669445991516\n",
      "cnt: 0 - valLoss: 0.6675504446029663 - trainLoss: 0.6660656332969666\n",
      "cnt: 0 - valLoss: 0.6675493717193604 - trainLoss: 0.666064441204071\n",
      "cnt: 0 - valLoss: 0.6675482988357544 - trainLoss: 0.6660631895065308\n",
      "cnt: 0 - valLoss: 0.6675472259521484 - trainLoss: 0.6660619378089905\n",
      "cnt: 0 - valLoss: 0.6675462126731873 - trainLoss: 0.6660606265068054\n",
      "cnt: 0 - valLoss: 0.6675451397895813 - trainLoss: 0.6660594344139099\n",
      "cnt: 0 - valLoss: 0.6675440669059753 - trainLoss: 0.6660582423210144\n",
      "cnt: 0 - valLoss: 0.6675430536270142 - trainLoss: 0.6660569310188293\n",
      "cnt: 0 - valLoss: 0.667542040348053 - trainLoss: 0.6660557389259338\n",
      "cnt: 0 - valLoss: 0.667540967464447 - trainLoss: 0.6660544872283936\n",
      "cnt: 0 - valLoss: 0.6675398945808411 - trainLoss: 0.6660531759262085\n",
      "cnt: 0 - valLoss: 0.6675388813018799 - trainLoss: 0.666051983833313\n",
      "cnt: 0 - valLoss: 0.6675378084182739 - trainLoss: 0.6660507917404175\n",
      "cnt: 0 - valLoss: 0.6675367951393127 - trainLoss: 0.6660495400428772\n",
      "cnt: 0 - valLoss: 0.6675357818603516 - trainLoss: 0.6660483479499817\n",
      "cnt: 0 - valLoss: 0.6675347089767456 - trainLoss: 0.6660470366477966\n",
      "cnt: 0 - valLoss: 0.6675336956977844 - trainLoss: 0.6660458445549011\n",
      "cnt: 0 - valLoss: 0.6675326824188232 - trainLoss: 0.6660445928573608\n",
      "cnt: 0 - valLoss: 0.6675316095352173 - trainLoss: 0.6660433411598206\n",
      "cnt: 0 - valLoss: 0.6675305962562561 - trainLoss: 0.666042149066925\n",
      "cnt: 0 - valLoss: 0.6675295233726501 - trainLoss: 0.6660408973693848\n",
      "cnt: 0 - valLoss: 0.6675284504890442 - trainLoss: 0.6660397052764893\n",
      "cnt: 0 - valLoss: 0.6675274968147278 - trainLoss: 0.666038453578949\n",
      "cnt: 0 - valLoss: 0.6675264835357666 - trainLoss: 0.6660372614860535\n",
      "cnt: 0 - valLoss: 0.6675254106521606 - trainLoss: 0.666036069393158\n",
      "cnt: 0 - valLoss: 0.6675243377685547 - trainLoss: 0.6660348176956177\n",
      "cnt: 0 - valLoss: 0.6675233840942383 - trainLoss: 0.6660336256027222\n",
      "cnt: 0 - valLoss: 0.6675223708152771 - trainLoss: 0.6660323739051819\n",
      "cnt: 0 - valLoss: 0.6675213575363159 - trainLoss: 0.6660311818122864\n",
      "cnt: 0 - valLoss: 0.66752028465271 - trainLoss: 0.6660299301147461\n",
      "cnt: 0 - valLoss: 0.6675193309783936 - trainLoss: 0.6660287380218506\n",
      "cnt: 0 - valLoss: 0.6675182580947876 - trainLoss: 0.6660275459289551\n",
      "cnt: 0 - valLoss: 0.6675172448158264 - trainLoss: 0.6660263538360596\n",
      "cnt: 0 - valLoss: 0.6675162315368652 - trainLoss: 0.6660251021385193\n",
      "cnt: 0 - valLoss: 0.6675151586532593 - trainLoss: 0.6660239100456238\n",
      "cnt: 0 - valLoss: 0.6675142049789429 - trainLoss: 0.6660227179527283\n",
      "cnt: 0 - valLoss: 0.6675131916999817 - trainLoss: 0.666021466255188\n",
      "cnt: 0 - valLoss: 0.6675121784210205 - trainLoss: 0.6660202741622925\n",
      "cnt: 0 - valLoss: 0.6675111651420593 - trainLoss: 0.6660190224647522\n",
      "cnt: 0 - valLoss: 0.6675101518630981 - trainLoss: 0.6660178303718567\n",
      "cnt: 0 - valLoss: 0.6675090789794922 - trainLoss: 0.6660166382789612\n",
      "cnt: 0 - valLoss: 0.6675081849098206 - trainLoss: 0.6660154461860657\n",
      "cnt: 0 - valLoss: 0.6675071120262146 - trainLoss: 0.6660143136978149\n",
      "cnt: 0 - valLoss: 0.6675061583518982 - trainLoss: 0.6660130620002747\n",
      "cnt: 0 - valLoss: 0.6675050854682922 - trainLoss: 0.6660118699073792\n",
      "cnt: 0 - valLoss: 0.6675041317939758 - trainLoss: 0.6660106778144836\n",
      "cnt: 0 - valLoss: 0.6675031185150146 - trainLoss: 0.6660094857215881\n",
      "cnt: 0 - valLoss: 0.6675021648406982 - trainLoss: 0.6660082936286926\n",
      "cnt: 0 - valLoss: 0.6675011515617371 - trainLoss: 0.6660071015357971\n",
      "cnt: 0 - valLoss: 0.6675000786781311 - trainLoss: 0.6660059094429016\n",
      "cnt: 0 - valLoss: 0.6674990653991699 - trainLoss: 0.6660047769546509\n",
      "cnt: 0 - valLoss: 0.6674981117248535 - trainLoss: 0.6660035252571106\n",
      "cnt: 0 - valLoss: 0.6674970984458923 - trainLoss: 0.6660023331642151\n",
      "cnt: 0 - valLoss: 0.6674961447715759 - trainLoss: 0.6660012006759644\n",
      "cnt: 0 - valLoss: 0.6674951314926147 - trainLoss: 0.6660000085830688\n",
      "cnt: 0 - valLoss: 0.6674941182136536 - trainLoss: 0.6659988164901733\n",
      "cnt: 0 - valLoss: 0.6674931645393372 - trainLoss: 0.6659976243972778\n",
      "cnt: 0 - valLoss: 0.667492151260376 - trainLoss: 0.6659964323043823\n",
      "cnt: 0 - valLoss: 0.6674911379814148 - trainLoss: 0.6659952402114868\n",
      "cnt: 0 - valLoss: 0.6674901843070984 - trainLoss: 0.6659940481185913\n",
      "cnt: 0 - valLoss: 0.667489230632782 - trainLoss: 0.6659929156303406\n",
      "cnt: 0 - valLoss: 0.667488157749176 - trainLoss: 0.6659917235374451\n",
      "cnt: 0 - valLoss: 0.6674872040748596 - trainLoss: 0.6659905910491943\n",
      "cnt: 0 - valLoss: 0.6674861907958984 - trainLoss: 0.6659893989562988\n",
      "cnt: 0 - valLoss: 0.667485237121582 - trainLoss: 0.6659882068634033\n",
      "cnt: 0 - valLoss: 0.6674842238426208 - trainLoss: 0.6659870147705078\n",
      "cnt: 0 - valLoss: 0.6674832701683044 - trainLoss: 0.6659859418869019\n",
      "cnt: 0 - valLoss: 0.6674822568893433 - trainLoss: 0.6659847497940063\n",
      "cnt: 0 - valLoss: 0.6674813032150269 - trainLoss: 0.6659834980964661\n",
      "cnt: 0 - valLoss: 0.6674803495407104 - trainLoss: 0.6659823656082153\n",
      "cnt: 0 - valLoss: 0.6674793362617493 - trainLoss: 0.6659812331199646\n",
      "cnt: 0 - valLoss: 0.6674783825874329 - trainLoss: 0.6659800410270691\n",
      "cnt: 0 - valLoss: 0.6674773693084717 - trainLoss: 0.6659789085388184\n",
      "cnt: 0 - valLoss: 0.6674764156341553 - trainLoss: 0.6659777164459229\n",
      "cnt: 0 - valLoss: 0.6674755215644836 - trainLoss: 0.6659765839576721\n",
      "cnt: 0 - valLoss: 0.6674745082855225 - trainLoss: 0.6659754514694214\n",
      "cnt: 0 - valLoss: 0.667473554611206 - trainLoss: 0.6659742593765259\n",
      "cnt: 0 - valLoss: 0.6674725413322449 - trainLoss: 0.6659730672836304\n",
      "cnt: 0 - valLoss: 0.6674715280532837 - trainLoss: 0.6659719347953796\n",
      "cnt: 0 - valLoss: 0.6674706339836121 - trainLoss: 0.6659707427024841\n",
      "cnt: 0 - valLoss: 0.6674696803092957 - trainLoss: 0.6659696698188782\n",
      "cnt: 0 - valLoss: 0.6674686670303345 - trainLoss: 0.6659684777259827\n",
      "cnt: 0 - valLoss: 0.6674677133560181 - trainLoss: 0.6659673452377319\n",
      "cnt: 0 - valLoss: 0.6674667596817017 - trainLoss: 0.6659662127494812\n",
      "cnt: 0 - valLoss: 0.6674658060073853 - trainLoss: 0.6659650206565857\n",
      "cnt: 0 - valLoss: 0.6674648523330688 - trainLoss: 0.6659639477729797\n",
      "cnt: 0 - valLoss: 0.6674638986587524 - trainLoss: 0.6659627556800842\n",
      "cnt: 0 - valLoss: 0.6674628853797913 - trainLoss: 0.6659616231918335\n",
      "cnt: 0 - valLoss: 0.6674619317054749 - trainLoss: 0.665960431098938\n",
      "cnt: 0 - valLoss: 0.6674609780311584 - trainLoss: 0.6659592986106873\n",
      "cnt: 0 - valLoss: 0.667460024356842 - trainLoss: 0.6659581661224365\n",
      "cnt: 0 - valLoss: 0.6674591302871704 - trainLoss: 0.6659570932388306\n",
      "cnt: 0 - valLoss: 0.6674581170082092 - trainLoss: 0.6659559011459351\n",
      "cnt: 0 - valLoss: 0.6674572229385376 - trainLoss: 0.6659547686576843\n",
      "cnt: 0 - valLoss: 0.6674562096595764 - trainLoss: 0.6659536361694336\n",
      "cnt: 0 - valLoss: 0.66745525598526 - trainLoss: 0.6659525036811829\n",
      "cnt: 0 - valLoss: 0.6674543023109436 - trainLoss: 0.6659513711929321\n",
      "cnt: 0 - valLoss: 0.667453408241272 - trainLoss: 0.6659502387046814\n",
      "cnt: 0 - valLoss: 0.6674524545669556 - trainLoss: 0.6659490466117859\n",
      "cnt: 0 - valLoss: 0.6674515008926392 - trainLoss: 0.6659479737281799\n",
      "cnt: 0 - valLoss: 0.667450487613678 - trainLoss: 0.6659468412399292\n",
      "cnt: 0 - valLoss: 0.6674495935440063 - trainLoss: 0.6659457087516785\n",
      "cnt: 0 - valLoss: 0.6674486398696899 - trainLoss: 0.6659446358680725\n",
      "cnt: 0 - valLoss: 0.6674477458000183 - trainLoss: 0.665943443775177\n",
      "cnt: 0 - valLoss: 0.6674467921257019 - trainLoss: 0.665942370891571\n",
      "cnt: 0 - valLoss: 0.6674458384513855 - trainLoss: 0.6659411787986755\n",
      "cnt: 0 - valLoss: 0.6674448847770691 - trainLoss: 0.6659401059150696\n",
      "cnt: 0 - valLoss: 0.6674439311027527 - trainLoss: 0.6659389734268188\n",
      "cnt: 0 - valLoss: 0.667443037033081 - trainLoss: 0.6659378409385681\n",
      "cnt: 0 - valLoss: 0.6674420833587646 - trainLoss: 0.6659367084503174\n",
      "cnt: 0 - valLoss: 0.6674411296844482 - trainLoss: 0.6659355759620667\n",
      "cnt: 0 - valLoss: 0.6674401760101318 - trainLoss: 0.6659345030784607\n",
      "cnt: 0 - valLoss: 0.6674392223358154 - trainLoss: 0.66593337059021\n",
      "cnt: 0 - valLoss: 0.667438268661499 - trainLoss: 0.6659322381019592\n",
      "cnt: 0 - valLoss: 0.6674373745918274 - trainLoss: 0.6659311056137085\n",
      "cnt: 0 - valLoss: 0.667436420917511 - trainLoss: 0.6659300327301025\n",
      "cnt: 0 - valLoss: 0.6674355268478394 - trainLoss: 0.6659289598464966\n",
      "cnt: 0 - valLoss: 0.6674346327781677 - trainLoss: 0.6659277677536011\n",
      "cnt: 0 - valLoss: 0.6674336791038513 - trainLoss: 0.6659266948699951\n",
      "cnt: 0 - valLoss: 0.6674327254295349 - trainLoss: 0.6659256219863892\n",
      "cnt: 0 - valLoss: 0.6674317717552185 - trainLoss: 0.6659244894981384\n",
      "cnt: 0 - valLoss: 0.6674308776855469 - trainLoss: 0.6659234166145325\n",
      "cnt: 0 - valLoss: 0.6674299836158752 - trainLoss: 0.6659223437309265\n",
      "cnt: 0 - valLoss: 0.6674290299415588 - trainLoss: 0.6659212112426758\n",
      "cnt: 0 - valLoss: 0.6674281358718872 - trainLoss: 0.665920078754425\n",
      "cnt: 0 - valLoss: 0.6674272418022156 - trainLoss: 0.6659190058708191\n",
      "cnt: 0 - valLoss: 0.6674262881278992 - trainLoss: 0.6659179329872131\n",
      "cnt: 0 - valLoss: 0.6674253940582275 - trainLoss: 0.6659167408943176\n",
      "cnt: 0 - valLoss: 0.6674244403839111 - trainLoss: 0.6659157276153564\n",
      "cnt: 0 - valLoss: 0.6674235463142395 - trainLoss: 0.6659145951271057\n",
      "cnt: 0 - valLoss: 0.6674225926399231 - trainLoss: 0.665913462638855\n",
      "cnt: 0 - valLoss: 0.6674216389656067 - trainLoss: 0.665912389755249\n",
      "cnt: 0 - valLoss: 0.6674208045005798 - trainLoss: 0.6659113168716431\n",
      "cnt: 0 - valLoss: 0.6674198508262634 - trainLoss: 0.6659101843833923\n",
      "cnt: 0 - valLoss: 0.6674189567565918 - trainLoss: 0.6659091114997864\n",
      "cnt: 0 - valLoss: 0.6674180626869202 - trainLoss: 0.6659080386161804\n",
      "cnt: 0 - valLoss: 0.6674171090126038 - trainLoss: 0.6659069657325745\n",
      "cnt: 0 - valLoss: 0.6674162745475769 - trainLoss: 0.6659058332443237\n",
      "cnt: 0 - valLoss: 0.6674153208732605 - trainLoss: 0.6659047603607178\n",
      "cnt: 0 - valLoss: 0.6674144268035889 - trainLoss: 0.665903627872467\n",
      "cnt: 0 - valLoss: 0.6674135327339172 - trainLoss: 0.6659026145935059\n",
      "cnt: 0 - valLoss: 0.6674126386642456 - trainLoss: 0.6659015417098999\n",
      "cnt: 0 - valLoss: 0.667411744594574 - trainLoss: 0.665900468826294\n",
      "cnt: 0 - valLoss: 0.6674108505249023 - trainLoss: 0.665899395942688\n",
      "cnt: 0 - valLoss: 0.6674098968505859 - trainLoss: 0.665898323059082\n",
      "cnt: 0 - valLoss: 0.6674090623855591 - trainLoss: 0.6658972501754761\n",
      "cnt: 0 - valLoss: 0.6674081087112427 - trainLoss: 0.6658961176872253\n",
      "cnt: 0 - valLoss: 0.6674072742462158 - trainLoss: 0.6658951044082642\n",
      "cnt: 0 - valLoss: 0.6674063801765442 - trainLoss: 0.6658939719200134\n",
      "cnt: 0 - valLoss: 0.6674054861068726 - trainLoss: 0.6658929586410522\n",
      "cnt: 0 - valLoss: 0.6674045324325562 - trainLoss: 0.6658918261528015\n",
      "cnt: 0 - valLoss: 0.6674036383628845 - trainLoss: 0.6658908128738403\n",
      "cnt: 0 - valLoss: 0.6674028038978577 - trainLoss: 0.6658897399902344\n",
      "cnt: 0 - valLoss: 0.6674019694328308 - trainLoss: 0.6658886075019836\n",
      "cnt: 0 - valLoss: 0.6674010157585144 - trainLoss: 0.6658875346183777\n",
      "cnt: 0 - valLoss: 0.6674001216888428 - trainLoss: 0.6658865213394165\n",
      "cnt: 0 - valLoss: 0.6673992872238159 - trainLoss: 0.6658854484558105\n",
      "cnt: 0 - valLoss: 0.6673983931541443 - trainLoss: 0.6658843755722046\n",
      "cnt: 0 - valLoss: 0.6673975586891174 - trainLoss: 0.6658833622932434\n",
      "cnt: 0 - valLoss: 0.667396605014801 - trainLoss: 0.6658822298049927\n",
      "cnt: 0 - valLoss: 0.6673957705497742 - trainLoss: 0.6658811569213867\n",
      "cnt: 0 - valLoss: 0.6673949360847473 - trainLoss: 0.6658801436424255\n",
      "cnt: 0 - valLoss: 0.6673939824104309 - trainLoss: 0.6658790707588196\n",
      "cnt: 0 - valLoss: 0.6673930883407593 - trainLoss: 0.6658780574798584\n",
      "cnt: 0 - valLoss: 0.6673923134803772 - trainLoss: 0.6658769845962524\n",
      "cnt: 0 - valLoss: 0.6673914194107056 - trainLoss: 0.6658759117126465\n",
      "cnt: 0 - valLoss: 0.6673905253410339 - trainLoss: 0.6658748984336853\n",
      "cnt: 0 - valLoss: 0.6673896312713623 - trainLoss: 0.6658738851547241\n",
      "cnt: 0 - valLoss: 0.6673887968063354 - trainLoss: 0.6658727526664734\n",
      "cnt: 0 - valLoss: 0.6673879027366638 - trainLoss: 0.6658717393875122\n",
      "cnt: 0 - valLoss: 0.6673870086669922 - trainLoss: 0.665870726108551\n",
      "cnt: 0 - valLoss: 0.6673861742019653 - trainLoss: 0.6658696532249451\n",
      "cnt: 0 - valLoss: 0.6673853397369385 - trainLoss: 0.6658685803413391\n",
      "cnt: 0 - valLoss: 0.6673844456672668 - trainLoss: 0.6658675074577332\n",
      "cnt: 0 - valLoss: 0.66738361120224 - trainLoss: 0.665866494178772\n",
      "cnt: 0 - valLoss: 0.6673827171325684 - trainLoss: 0.6658654808998108\n",
      "cnt: 0 - valLoss: 0.6673818826675415 - trainLoss: 0.6658644080162048\n",
      "cnt: 0 - valLoss: 0.6673810482025146 - trainLoss: 0.6658633351325989\n",
      "cnt: 0 - valLoss: 0.667380154132843 - trainLoss: 0.6658623218536377\n",
      "cnt: 0 - valLoss: 0.6673792600631714 - trainLoss: 0.6658613085746765\n",
      "cnt: 0 - valLoss: 0.6673783659934998 - trainLoss: 0.6658602356910706\n",
      "cnt: 0 - valLoss: 0.6673775315284729 - trainLoss: 0.6658592224121094\n",
      "cnt: 0 - valLoss: 0.6673766374588013 - trainLoss: 0.6658581495285034\n",
      "cnt: 0 - valLoss: 0.6673758029937744 - trainLoss: 0.6658570766448975\n",
      "cnt: 0 - valLoss: 0.6673749685287476 - trainLoss: 0.665856122970581\n",
      "cnt: 0 - valLoss: 0.6673740744590759 - trainLoss: 0.6658550500869751\n",
      "cnt: 0 - valLoss: 0.6673732995986938 - trainLoss: 0.6658540368080139\n",
      "cnt: 0 - valLoss: 0.667372465133667 - trainLoss: 0.665852963924408\n",
      "cnt: 0 - valLoss: 0.6673715710639954 - trainLoss: 0.6658519506454468\n",
      "cnt: 0 - valLoss: 0.6673706769943237 - trainLoss: 0.6658509373664856\n",
      "cnt: 0 - valLoss: 0.6673699021339417 - trainLoss: 0.6658499240875244\n",
      "cnt: 0 - valLoss: 0.66736900806427 - trainLoss: 0.6658488512039185\n",
      "cnt: 0 - valLoss: 0.6673681735992432 - trainLoss: 0.665847897529602\n",
      "cnt: 0 - valLoss: 0.6673673391342163 - trainLoss: 0.6658468842506409\n",
      "cnt: 0 - valLoss: 0.6673665642738342 - trainLoss: 0.6658458113670349\n",
      "cnt: 0 - valLoss: 0.6673656702041626 - trainLoss: 0.6658447980880737\n",
      "cnt: 0 - valLoss: 0.6673648357391357 - trainLoss: 0.6658438444137573\n",
      "cnt: 0 - valLoss: 0.6673639416694641 - trainLoss: 0.6658427715301514\n",
      "cnt: 0 - valLoss: 0.6673631072044373 - trainLoss: 0.6658416986465454\n",
      "cnt: 0 - valLoss: 0.6673622727394104 - trainLoss: 0.665840744972229\n",
      "cnt: 0 - valLoss: 0.6673614382743835 - trainLoss: 0.6658397316932678\n",
      "cnt: 0 - valLoss: 0.6673606038093567 - trainLoss: 0.6658386588096619\n",
      "cnt: 0 - valLoss: 0.6673597693443298 - trainLoss: 0.6658376455307007\n",
      "cnt: 0 - valLoss: 0.667358934879303 - trainLoss: 0.6658366918563843\n",
      "cnt: 0 - valLoss: 0.6673581004142761 - trainLoss: 0.6658356189727783\n",
      "cnt: 0 - valLoss: 0.6673572659492493 - trainLoss: 0.6658346652984619\n",
      "cnt: 0 - valLoss: 0.6673564314842224 - trainLoss: 0.6658336520195007\n",
      "cnt: 0 - valLoss: 0.6673555970191956 - trainLoss: 0.6658326387405396\n",
      "cnt: 0 - valLoss: 0.6673547029495239 - trainLoss: 0.6658316254615784\n",
      "cnt: 0 - valLoss: 0.6673538684844971 - trainLoss: 0.6658305525779724\n",
      "cnt: 0 - valLoss: 0.667353093624115 - trainLoss: 0.665829598903656\n",
      "cnt: 0 - valLoss: 0.6673522591590881 - trainLoss: 0.6658285856246948\n",
      "cnt: 0 - valLoss: 0.667351484298706 - trainLoss: 0.6658275723457336\n",
      "cnt: 0 - valLoss: 0.6673505902290344 - trainLoss: 0.6658266186714172\n",
      "cnt: 0 - valLoss: 0.6673497557640076 - trainLoss: 0.665825605392456\n",
      "cnt: 0 - valLoss: 0.6673489809036255 - trainLoss: 0.6658245325088501\n",
      "cnt: 0 - valLoss: 0.6673480868339539 - trainLoss: 0.6658236384391785\n",
      "cnt: 0 - valLoss: 0.6673473119735718 - trainLoss: 0.6658225655555725\n",
      "cnt: 0 - valLoss: 0.6673464775085449 - trainLoss: 0.6658215522766113\n",
      "cnt: 0 - valLoss: 0.6673456430435181 - trainLoss: 0.6658205986022949\n",
      "cnt: 0 - valLoss: 0.667344868183136 - trainLoss: 0.6658195853233337\n",
      "cnt: 0 - valLoss: 0.6673440337181091 - trainLoss: 0.6658186316490173\n",
      "cnt: 0 - valLoss: 0.6673431396484375 - trainLoss: 0.6658175587654114\n",
      "cnt: 0 - valLoss: 0.6673423647880554 - trainLoss: 0.665816605091095\n",
      "cnt: 0 - valLoss: 0.6673415303230286 - trainLoss: 0.6658156514167786\n",
      "cnt: 0 - valLoss: 0.6673407554626465 - trainLoss: 0.6658145785331726\n",
      "cnt: 0 - valLoss: 0.6673399806022644 - trainLoss: 0.6658136248588562\n",
      "cnt: 0 - valLoss: 0.6673390865325928 - trainLoss: 0.665812611579895\n",
      "cnt: 0 - valLoss: 0.6673382520675659 - trainLoss: 0.6658116579055786\n",
      "cnt: 0 - valLoss: 0.6673374772071838 - trainLoss: 0.6658106446266174\n",
      "cnt: 0 - valLoss: 0.667336642742157 - trainLoss: 0.665809690952301\n",
      "cnt: 0 - valLoss: 0.6673358678817749 - trainLoss: 0.6658086776733398\n",
      "cnt: 0 - valLoss: 0.667335033416748 - trainLoss: 0.6658077239990234\n",
      "cnt: 0 - valLoss: 0.6673341989517212 - trainLoss: 0.6658067107200623\n",
      "cnt: 0 - valLoss: 0.6673334240913391 - trainLoss: 0.6658056974411011\n",
      "cnt: 0 - valLoss: 0.6673325896263123 - trainLoss: 0.6658047437667847\n",
      "cnt: 0 - valLoss: 0.6673318147659302 - trainLoss: 0.6658037304878235\n",
      "cnt: 0 - valLoss: 0.6673309803009033 - trainLoss: 0.6658028364181519\n",
      "cnt: 0 - valLoss: 0.6673302054405212 - trainLoss: 0.6658017635345459\n",
      "cnt: 0 - valLoss: 0.6673293113708496 - trainLoss: 0.6658008694648743\n",
      "cnt: 0 - valLoss: 0.6673285961151123 - trainLoss: 0.6657997965812683\n",
      "cnt: 0 - valLoss: 0.6673278212547302 - trainLoss: 0.6657989025115967\n",
      "cnt: 0 - valLoss: 0.6673269867897034 - trainLoss: 0.6657978892326355\n",
      "cnt: 0 - valLoss: 0.6673261523246765 - trainLoss: 0.6657969355583191\n",
      "cnt: 0 - valLoss: 0.6673253178596497 - trainLoss: 0.6657959818840027\n",
      "cnt: 0 - valLoss: 0.6673246026039124 - trainLoss: 0.6657950282096863\n",
      "cnt: 0 - valLoss: 0.6673237681388855 - trainLoss: 0.6657940149307251\n",
      "cnt: 0 - valLoss: 0.6673229336738586 - trainLoss: 0.6657930612564087\n",
      "cnt: 0 - valLoss: 0.6673221588134766 - trainLoss: 0.6657920479774475\n",
      "cnt: 0 - valLoss: 0.6673213839530945 - trainLoss: 0.6657910943031311\n",
      "cnt: 0 - valLoss: 0.6673205494880676 - trainLoss: 0.6657901406288147\n",
      "cnt: 0 - valLoss: 0.6673197746276855 - trainLoss: 0.6657891869544983\n",
      "cnt: 0 - valLoss: 0.6673189401626587 - trainLoss: 0.6657882332801819\n",
      "cnt: 0 - valLoss: 0.6673181653022766 - trainLoss: 0.6657872200012207\n",
      "cnt: 0 - valLoss: 0.6673174500465393 - trainLoss: 0.6657862663269043\n",
      "cnt: 0 - valLoss: 0.6673166751861572 - trainLoss: 0.6657853722572327\n",
      "cnt: 0 - valLoss: 0.6673158407211304 - trainLoss: 0.6657843589782715\n",
      "cnt: 0 - valLoss: 0.6673150062561035 - trainLoss: 0.6657834053039551\n",
      "cnt: 0 - valLoss: 0.6673142313957214 - trainLoss: 0.6657824516296387\n",
      "cnt: 0 - valLoss: 0.6673133969306946 - trainLoss: 0.6657814979553223\n",
      "cnt: 0 - valLoss: 0.6673126816749573 - trainLoss: 0.6657805442810059\n",
      "cnt: 0 - valLoss: 0.6673119068145752 - trainLoss: 0.6657795906066895\n",
      "cnt: 0 - valLoss: 0.6673110723495483 - trainLoss: 0.665778636932373\n",
      "cnt: 0 - valLoss: 0.6673102974891663 - trainLoss: 0.6657776236534119\n",
      "cnt: 0 - valLoss: 0.6673095226287842 - trainLoss: 0.6657767295837402\n",
      "cnt: 0 - valLoss: 0.6673087477684021 - trainLoss: 0.6657757759094238\n",
      "cnt: 0 - valLoss: 0.66730797290802 - trainLoss: 0.6657747626304626\n",
      "cnt: 0 - valLoss: 0.6673071384429932 - trainLoss: 0.6657738089561462\n",
      "cnt: 0 - valLoss: 0.6673063635826111 - trainLoss: 0.6657729148864746\n",
      "cnt: 0 - valLoss: 0.667305588722229 - trainLoss: 0.6657719612121582\n",
      "cnt: 0 - valLoss: 0.6673048138618469 - trainLoss: 0.665770947933197\n",
      "cnt: 0 - valLoss: 0.6673040390014648 - trainLoss: 0.6657700538635254\n",
      "cnt: 0 - valLoss: 0.6673032641410828 - trainLoss: 0.665769100189209\n",
      "cnt: 0 - valLoss: 0.6673024892807007 - trainLoss: 0.6657681465148926\n",
      "cnt: 0 - valLoss: 0.6673017144203186 - trainLoss: 0.6657671928405762\n",
      "cnt: 0 - valLoss: 0.6673009395599365 - trainLoss: 0.6657662987709045\n",
      "cnt: 0 - valLoss: 0.6673001646995544 - trainLoss: 0.6657653450965881\n",
      "cnt: 0 - valLoss: 0.6672993898391724 - trainLoss: 0.6657643914222717\n",
      "cnt: 0 - valLoss: 0.6672986745834351 - trainLoss: 0.6657634973526001\n",
      "cnt: 0 - valLoss: 0.6672977805137634 - trainLoss: 0.6657624840736389\n",
      "cnt: 0 - valLoss: 0.6672970056533813 - trainLoss: 0.6657615900039673\n",
      "cnt: 0 - valLoss: 0.6672963500022888 - trainLoss: 0.6657606363296509\n",
      "cnt: 0 - valLoss: 0.6672954559326172 - trainLoss: 0.6657597422599792\n",
      "cnt: 0 - valLoss: 0.6672946810722351 - trainLoss: 0.6657587885856628\n",
      "cnt: 0 - valLoss: 0.6672939658164978 - trainLoss: 0.6657578945159912\n",
      "cnt: 0 - valLoss: 0.6672932505607605 - trainLoss: 0.6657569408416748\n",
      "cnt: 0 - valLoss: 0.6672924757003784 - trainLoss: 0.6657559871673584\n",
      "cnt: 0 - valLoss: 0.6672917008399963 - trainLoss: 0.665755033493042\n",
      "cnt: 0 - valLoss: 0.6672909259796143 - trainLoss: 0.6657541990280151\n",
      "cnt: 0 - valLoss: 0.6672900915145874 - trainLoss: 0.665753185749054\n",
      "cnt: 0 - valLoss: 0.6672893762588501 - trainLoss: 0.6657522916793823\n",
      "cnt: 0 - valLoss: 0.667288601398468 - trainLoss: 0.6657513380050659\n",
      "cnt: 0 - valLoss: 0.6672878265380859 - trainLoss: 0.6657504439353943\n",
      "cnt: 0 - valLoss: 0.6672870516777039 - trainLoss: 0.6657495498657227\n",
      "cnt: 0 - valLoss: 0.6672863364219666 - trainLoss: 0.6657485961914062\n",
      "cnt: 0 - valLoss: 0.6672855615615845 - trainLoss: 0.6657477021217346\n",
      "cnt: 0 - valLoss: 0.6672847867012024 - trainLoss: 0.6657467484474182\n",
      "cnt: 0 - valLoss: 0.6672840714454651 - trainLoss: 0.6657458543777466\n",
      "cnt: 0 - valLoss: 0.667283296585083 - trainLoss: 0.6657449007034302\n",
      "cnt: 0 - valLoss: 0.6672825217247009 - trainLoss: 0.6657440066337585\n",
      "cnt: 0 - valLoss: 0.6672818064689636 - trainLoss: 0.6657430529594421\n",
      "cnt: 0 - valLoss: 0.6672810912132263 - trainLoss: 0.6657421588897705\n",
      "cnt: 0 - valLoss: 0.6672803163528442 - trainLoss: 0.6657412648200989\n",
      "cnt: 0 - valLoss: 0.6672795414924622 - trainLoss: 0.6657403111457825\n",
      "cnt: 0 - valLoss: 0.6672788262367249 - trainLoss: 0.6657393574714661\n",
      "cnt: 0 - valLoss: 0.6672780513763428 - trainLoss: 0.6657384634017944\n",
      "cnt: 0 - valLoss: 0.6672772765159607 - trainLoss: 0.6657375693321228\n",
      "cnt: 0 - valLoss: 0.6672765612602234 - trainLoss: 0.6657366156578064\n",
      "cnt: 0 - valLoss: 0.6672757863998413 - trainLoss: 0.6657357215881348\n",
      "cnt: 0 - valLoss: 0.667275071144104 - trainLoss: 0.6657348275184631\n",
      "cnt: 0 - valLoss: 0.6672743558883667 - trainLoss: 0.6657339930534363\n",
      "cnt: 0 - valLoss: 0.6672735810279846 - trainLoss: 0.6657330393791199\n",
      "cnt: 0 - valLoss: 0.6672728657722473 - trainLoss: 0.6657320857048035\n",
      "cnt: 0 - valLoss: 0.6672720909118652 - trainLoss: 0.6657312512397766\n",
      "cnt: 0 - valLoss: 0.6672713160514832 - trainLoss: 0.6657302975654602\n",
      "cnt: 0 - valLoss: 0.6672706007957458 - trainLoss: 0.6657293438911438\n",
      "cnt: 0 - valLoss: 0.6672698259353638 - trainLoss: 0.6657285094261169\n",
      "cnt: 0 - valLoss: 0.6672690510749817 - trainLoss: 0.6657276153564453\n",
      "cnt: 0 - valLoss: 0.6672683954238892 - trainLoss: 0.6657267212867737\n",
      "cnt: 0 - valLoss: 0.6672676801681519 - trainLoss: 0.665725827217102\n",
      "cnt: 0 - valLoss: 0.6672669053077698 - trainLoss: 0.6657249331474304\n",
      "cnt: 0 - valLoss: 0.6672661304473877 - trainLoss: 0.665723979473114\n",
      "cnt: 0 - valLoss: 0.6672654151916504 - trainLoss: 0.6657231450080872\n",
      "cnt: 0 - valLoss: 0.6672646999359131 - trainLoss: 0.6657221913337708\n",
      "cnt: 0 - valLoss: 0.667263925075531 - trainLoss: 0.6657212972640991\n",
      "cnt: 0 - valLoss: 0.6672632694244385 - trainLoss: 0.6657204031944275\n",
      "cnt: 0 - valLoss: 0.6672624945640564 - trainLoss: 0.6657195091247559\n",
      "cnt: 0 - valLoss: 0.6672617793083191 - trainLoss: 0.665718674659729\n",
      "cnt: 0 - valLoss: 0.6672610640525818 - trainLoss: 0.6657177209854126\n",
      "cnt: 0 - valLoss: 0.6672603487968445 - trainLoss: 0.6657168865203857\n",
      "cnt: 0 - valLoss: 0.6672595739364624 - trainLoss: 0.6657159924507141\n",
      "cnt: 0 - valLoss: 0.6672588586807251 - trainLoss: 0.6657150983810425\n",
      "cnt: 0 - valLoss: 0.6672581434249878 - trainLoss: 0.6657142043113708\n",
      "cnt: 0 - valLoss: 0.6672574281692505 - trainLoss: 0.6657132506370544\n",
      "cnt: 0 - valLoss: 0.6672566533088684 - trainLoss: 0.6657124161720276\n",
      "cnt: 0 - valLoss: 0.6672559380531311 - trainLoss: 0.665711522102356\n",
      "cnt: 0 - valLoss: 0.6672552227973938 - trainLoss: 0.6657106280326843\n",
      "cnt: 0 - valLoss: 0.6672545075416565 - trainLoss: 0.6657097935676575\n",
      "cnt: 0 - valLoss: 0.6672537922859192 - trainLoss: 0.6657088398933411\n",
      "cnt: 0 - valLoss: 0.6672530174255371 - trainLoss: 0.6657079458236694\n",
      "cnt: 0 - valLoss: 0.6672523021697998 - trainLoss: 0.6657071113586426\n",
      "cnt: 0 - valLoss: 0.6672515869140625 - trainLoss: 0.6657062768936157\n",
      "cnt: 0 - valLoss: 0.6672508716583252 - trainLoss: 0.6657053828239441\n",
      "cnt: 0 - valLoss: 0.6672501564025879 - trainLoss: 0.6657044291496277\n",
      "cnt: 0 - valLoss: 0.6672494411468506 - trainLoss: 0.6657035946846008\n",
      "cnt: 0 - valLoss: 0.6672487258911133 - trainLoss: 0.6657027006149292\n",
      "cnt: 0 - valLoss: 0.667248010635376 - trainLoss: 0.6657018065452576\n",
      "cnt: 0 - valLoss: 0.6672472357749939 - trainLoss: 0.6657009720802307\n",
      "cnt: 0 - valLoss: 0.6672465205192566 - trainLoss: 0.6657000780105591\n",
      "cnt: 0 - valLoss: 0.6672458648681641 - trainLoss: 0.6656992435455322\n",
      "cnt: 0 - valLoss: 0.667245090007782 - trainLoss: 0.6656983494758606\n",
      "cnt: 0 - valLoss: 0.6672444343566895 - trainLoss: 0.665697455406189\n",
      "cnt: 0 - valLoss: 0.6672437191009521 - trainLoss: 0.6656966209411621\n",
      "cnt: 0 - valLoss: 0.6672429442405701 - trainLoss: 0.6656957864761353\n",
      "cnt: 0 - valLoss: 0.6672423481941223 - trainLoss: 0.6656948924064636\n",
      "cnt: 0 - valLoss: 0.6672415733337402 - trainLoss: 0.665693998336792\n",
      "cnt: 0 - valLoss: 0.6672408580780029 - trainLoss: 0.6656931638717651\n",
      "cnt: 0 - valLoss: 0.6672402024269104 - trainLoss: 0.6656922698020935\n",
      "cnt: 0 - valLoss: 0.6672394275665283 - trainLoss: 0.6656913757324219\n",
      "cnt: 0 - valLoss: 0.667238712310791 - trainLoss: 0.6656904816627502\n",
      "cnt: 0 - valLoss: 0.6672380566596985 - trainLoss: 0.6656896471977234\n",
      "cnt: 0 - valLoss: 0.6672373414039612 - trainLoss: 0.6656888127326965\n",
      "cnt: 0 - valLoss: 0.6672366261482239 - trainLoss: 0.6656879186630249\n",
      "cnt: 0 - valLoss: 0.6672359108924866 - trainLoss: 0.665687084197998\n",
      "cnt: 0 - valLoss: 0.6672351956367493 - trainLoss: 0.6656862497329712\n",
      "cnt: 0 - valLoss: 0.6672345399856567 - trainLoss: 0.6656853556632996\n",
      "cnt: 0 - valLoss: 0.6672338247299194 - trainLoss: 0.6656845211982727\n",
      "cnt: 0 - valLoss: 0.6672331094741821 - trainLoss: 0.6656836271286011\n",
      "cnt: 0 - valLoss: 0.6672323942184448 - trainLoss: 0.6656827926635742\n",
      "cnt: 0 - valLoss: 0.6672316789627075 - trainLoss: 0.6656818985939026\n",
      "cnt: 0 - valLoss: 0.6672309637069702 - trainLoss: 0.6656810641288757\n",
      "cnt: 0 - valLoss: 0.6672303080558777 - trainLoss: 0.6656801700592041\n",
      "cnt: 0 - valLoss: 0.6672295331954956 - trainLoss: 0.665679395198822\n",
      "cnt: 0 - valLoss: 0.6672289371490479 - trainLoss: 0.6656785011291504\n",
      "cnt: 0 - valLoss: 0.6672281622886658 - trainLoss: 0.6656776070594788\n",
      "cnt: 0 - valLoss: 0.6672275066375732 - trainLoss: 0.6656768321990967\n",
      "cnt: 0 - valLoss: 0.6672267913818359 - trainLoss: 0.665675938129425\n",
      "cnt: 0 - valLoss: 0.6672261357307434 - trainLoss: 0.6656751036643982\n",
      "cnt: 0 - valLoss: 0.6672254204750061 - trainLoss: 0.6656742691993713\n",
      "cnt: 0 - valLoss: 0.6672247052192688 - trainLoss: 0.6656733751296997\n",
      "cnt: 0 - valLoss: 0.6672239899635315 - trainLoss: 0.6656725406646729\n",
      "cnt: 0 - valLoss: 0.6672232747077942 - trainLoss: 0.6656716465950012\n",
      "cnt: 0 - valLoss: 0.6672226190567017 - trainLoss: 0.6656708121299744\n",
      "cnt: 0 - valLoss: 0.6672219634056091 - trainLoss: 0.6656700372695923\n",
      "cnt: 0 - valLoss: 0.6672212481498718 - trainLoss: 0.6656692028045654\n",
      "cnt: 0 - valLoss: 0.6672205328941345 - trainLoss: 0.6656683087348938\n",
      "cnt: 0 - valLoss: 0.667219877243042 - trainLoss: 0.6656674742698669\n",
      "cnt: 0 - valLoss: 0.6672192215919495 - trainLoss: 0.6656666398048401\n",
      "cnt: 0 - valLoss: 0.6672185063362122 - trainLoss: 0.6656658053398132\n",
      "cnt: 0 - valLoss: 0.6672177910804749 - trainLoss: 0.6656649112701416\n",
      "cnt: 0 - valLoss: 0.6672171354293823 - trainLoss: 0.6656640768051147\n",
      "cnt: 0 - valLoss: 0.667216420173645 - trainLoss: 0.6656633019447327\n",
      "cnt: 0 - valLoss: 0.6672157645225525 - trainLoss: 0.665662407875061\n",
      "cnt: 0 - valLoss: 0.6672150492668152 - trainLoss: 0.665661633014679\n",
      "cnt: 0 - valLoss: 0.6672143936157227 - trainLoss: 0.6656607389450073\n",
      "cnt: 0 - valLoss: 0.6672137379646301 - trainLoss: 0.6656599640846252\n",
      "cnt: 0 - valLoss: 0.6672130227088928 - trainLoss: 0.6656591296195984\n",
      "cnt: 0 - valLoss: 0.6672123670578003 - trainLoss: 0.6656582355499268\n",
      "cnt: 0 - valLoss: 0.667211651802063 - trainLoss: 0.6656574606895447\n",
      "cnt: 0 - valLoss: 0.6672109961509705 - trainLoss: 0.6656566262245178\n",
      "cnt: 0 - valLoss: 0.6672103404998779 - trainLoss: 0.665655791759491\n",
      "cnt: 0 - valLoss: 0.6672095656394958 - trainLoss: 0.6656549572944641\n",
      "cnt: 0 - valLoss: 0.6672089695930481 - trainLoss: 0.6656541228294373\n",
      "cnt: 0 - valLoss: 0.6672083139419556 - trainLoss: 0.6656532883644104\n",
      "cnt: 0 - valLoss: 0.6672075986862183 - trainLoss: 0.6656525135040283\n",
      "cnt: 0 - valLoss: 0.6672069430351257 - trainLoss: 0.6656516194343567\n",
      "cnt: 0 - valLoss: 0.6672062277793884 - trainLoss: 0.6656507849693298\n",
      "cnt: 0 - valLoss: 0.6672055721282959 - trainLoss: 0.665649950504303\n",
      "cnt: 0 - valLoss: 0.6672049164772034 - trainLoss: 0.6656491756439209\n",
      "cnt: 0 - valLoss: 0.6672042012214661 - trainLoss: 0.665648341178894\n",
      "cnt: 0 - valLoss: 0.6672036051750183 - trainLoss: 0.6656475067138672\n",
      "cnt: 0 - valLoss: 0.667202889919281 - trainLoss: 0.6656466722488403\n",
      "cnt: 0 - valLoss: 0.6672022342681885 - trainLoss: 0.6656458973884583\n",
      "cnt: 0 - valLoss: 0.667201578617096 - trainLoss: 0.6656450629234314\n",
      "cnt: 0 - valLoss: 0.6672009229660034 - trainLoss: 0.6656442284584045\n",
      "cnt: 0 - valLoss: 0.6672002673149109 - trainLoss: 0.6656433939933777\n",
      "cnt: 0 - valLoss: 0.6671995520591736 - trainLoss: 0.6656425595283508\n",
      "cnt: 0 - valLoss: 0.667198896408081 - trainLoss: 0.6656417846679688\n",
      "cnt: 0 - valLoss: 0.6671982407569885 - trainLoss: 0.6656409502029419\n",
      "cnt: 0 - valLoss: 0.6671975255012512 - trainLoss: 0.665640115737915\n",
      "cnt: 0 - valLoss: 0.6671968698501587 - trainLoss: 0.665639340877533\n",
      "cnt: 0 - valLoss: 0.6671962738037109 - trainLoss: 0.6656385660171509\n",
      "cnt: 0 - valLoss: 0.6671955585479736 - trainLoss: 0.665637731552124\n",
      "cnt: 0 - valLoss: 0.6671949028968811 - trainLoss: 0.6656368970870972\n",
      "cnt: 0 - valLoss: 0.6671942472457886 - trainLoss: 0.6656360626220703\n",
      "cnt: 0 - valLoss: 0.6671935319900513 - trainLoss: 0.6656352281570435\n",
      "cnt: 0 - valLoss: 0.6671928763389587 - trainLoss: 0.6656344532966614\n",
      "cnt: 0 - valLoss: 0.667192280292511 - trainLoss: 0.6656336784362793\n",
      "cnt: 0 - valLoss: 0.6671915650367737 - trainLoss: 0.6656327843666077\n",
      "cnt: 0 - valLoss: 0.6671909093856812 - trainLoss: 0.6656320095062256\n",
      "cnt: 0 - valLoss: 0.6671902537345886 - trainLoss: 0.6656312346458435\n",
      "cnt: 0 - valLoss: 0.6671895980834961 - trainLoss: 0.6656304001808167\n",
      "cnt: 0 - valLoss: 0.6671889424324036 - trainLoss: 0.6656296253204346\n",
      "cnt: 0 - valLoss: 0.6671883463859558 - trainLoss: 0.6656288504600525\n",
      "cnt: 0 - valLoss: 0.6671876311302185 - trainLoss: 0.6656280159950256\n",
      "cnt: 0 - valLoss: 0.6671870350837708 - trainLoss: 0.6656271815299988\n",
      "cnt: 0 - valLoss: 0.6671863794326782 - trainLoss: 0.6656264066696167\n",
      "cnt: 0 - valLoss: 0.6671857237815857 - trainLoss: 0.6656256318092346\n",
      "cnt: 0 - valLoss: 0.6671850085258484 - trainLoss: 0.6656248569488525\n",
      "cnt: 0 - valLoss: 0.6671843528747559 - trainLoss: 0.6656240224838257\n",
      "cnt: 0 - valLoss: 0.6671837568283081 - trainLoss: 0.6656232476234436\n",
      "cnt: 0 - valLoss: 0.6671831011772156 - trainLoss: 0.665622353553772\n",
      "cnt: 0 - valLoss: 0.667182445526123 - trainLoss: 0.6656215786933899\n",
      "cnt: 0 - valLoss: 0.6671817898750305 - trainLoss: 0.6656208038330078\n",
      "cnt: 0 - valLoss: 0.667181134223938 - trainLoss: 0.6656200289726257\n",
      "cnt: 0 - valLoss: 0.6671804189682007 - trainLoss: 0.6656191945075989\n",
      "cnt: 0 - valLoss: 0.6671798229217529 - trainLoss: 0.6656184196472168\n",
      "cnt: 0 - valLoss: 0.6671791672706604 - trainLoss: 0.6656175851821899\n",
      "cnt: 0 - valLoss: 0.6671785116195679 - trainLoss: 0.6656168699264526\n",
      "cnt: 0 - valLoss: 0.6671779155731201 - trainLoss: 0.6656160950660706\n",
      "cnt: 0 - valLoss: 0.6671772003173828 - trainLoss: 0.6656152606010437\n",
      "cnt: 0 - valLoss: 0.6671766042709351 - trainLoss: 0.6656144857406616\n",
      "cnt: 0 - valLoss: 0.6671760082244873 - trainLoss: 0.6656136512756348\n",
      "cnt: 0 - valLoss: 0.6671753525733948 - trainLoss: 0.6656128764152527\n",
      "cnt: 0 - valLoss: 0.6671746969223022 - trainLoss: 0.6656121015548706\n",
      "cnt: 0 - valLoss: 0.6671739816665649 - trainLoss: 0.6656113266944885\n",
      "cnt: 0 - valLoss: 0.6671733856201172 - trainLoss: 0.6656105518341064\n",
      "cnt: 0 - valLoss: 0.6671727299690247 - trainLoss: 0.6656097173690796\n",
      "cnt: 0 - valLoss: 0.6671720743179321 - trainLoss: 0.6656089425086975\n",
      "cnt: 0 - valLoss: 0.6671714186668396 - trainLoss: 0.6656081676483154\n",
      "cnt: 0 - valLoss: 0.6671708226203918 - trainLoss: 0.6656073927879333\n",
      "cnt: 0 - valLoss: 0.6671702265739441 - trainLoss: 0.6656066179275513\n",
      "cnt: 0 - valLoss: 0.6671695709228516 - trainLoss: 0.6656058430671692\n",
      "cnt: 0 - valLoss: 0.667168915271759 - trainLoss: 0.6656050682067871\n",
      "cnt: 0 - valLoss: 0.6671682596206665 - trainLoss: 0.6656042337417603\n",
      "cnt: 0 - valLoss: 0.667167603969574 - trainLoss: 0.665603518486023\n",
      "cnt: 0 - valLoss: 0.6671669483184814 - trainLoss: 0.6656026840209961\n",
      "cnt: 0 - valLoss: 0.6671663522720337 - trainLoss: 0.665601909160614\n",
      "cnt: 0 - valLoss: 0.6671657562255859 - trainLoss: 0.6656011343002319\n",
      "cnt: 0 - valLoss: 0.6671651601791382 - trainLoss: 0.6656004190444946\n",
      "cnt: 0 - valLoss: 0.6671645045280457 - trainLoss: 0.6655995845794678\n",
      "cnt: 0 - valLoss: 0.6671638488769531 - trainLoss: 0.6655988097190857\n",
      "cnt: 0 - valLoss: 0.6671631932258606 - trainLoss: 0.6655980348587036\n",
      "cnt: 0 - valLoss: 0.6671625971794128 - trainLoss: 0.6655972599983215\n",
      "cnt: 0 - valLoss: 0.6671619415283203 - trainLoss: 0.6655964851379395\n",
      "cnt: 0 - valLoss: 0.6671613454818726 - trainLoss: 0.6655957698822021\n",
      "cnt: 0 - valLoss: 0.6671607494354248 - trainLoss: 0.6655949950218201\n",
      "cnt: 0 - valLoss: 0.6671600341796875 - trainLoss: 0.665594220161438\n",
      "cnt: 0 - valLoss: 0.6671594381332397 - trainLoss: 0.6655934453010559\n",
      "cnt: 0 - valLoss: 0.6671587824821472 - trainLoss: 0.6655926704406738\n",
      "cnt: 0 - valLoss: 0.6671581268310547 - trainLoss: 0.6655918955802917\n",
      "cnt: 0 - valLoss: 0.6671575307846069 - trainLoss: 0.6655911207199097\n",
      "cnt: 0 - valLoss: 0.6671569347381592 - trainLoss: 0.6655903458595276\n",
      "cnt: 0 - valLoss: 0.6671562790870667 - trainLoss: 0.6655895709991455\n",
      "cnt: 0 - valLoss: 0.6671557426452637 - trainLoss: 0.6655887961387634\n",
      "cnt: 0 - valLoss: 0.6671550869941711 - trainLoss: 0.6655880212783813\n",
      "cnt: 0 - valLoss: 0.6671544313430786 - trainLoss: 0.6655872464179993\n",
      "cnt: 0 - valLoss: 0.6671538352966309 - trainLoss: 0.6655864715576172\n",
      "cnt: 0 - valLoss: 0.6671531796455383 - trainLoss: 0.6655857563018799\n",
      "cnt: 0 - valLoss: 0.6671525835990906 - trainLoss: 0.6655849814414978\n",
      "cnt: 0 - valLoss: 0.6671519875526428 - trainLoss: 0.6655842661857605\n",
      "cnt: 0 - valLoss: 0.6671513915061951 - trainLoss: 0.6655834913253784\n",
      "cnt: 0 - valLoss: 0.6671507954597473 - trainLoss: 0.6655827164649963\n",
      "cnt: 0 - valLoss: 0.66715008020401 - trainLoss: 0.6655819416046143\n",
      "cnt: 0 - valLoss: 0.6671494841575623 - trainLoss: 0.6655811667442322\n",
      "cnt: 0 - valLoss: 0.6671488881111145 - trainLoss: 0.6655804514884949\n",
      "cnt: 0 - valLoss: 0.6671482920646667 - trainLoss: 0.6655797362327576\n",
      "cnt: 0 - valLoss: 0.6671476364135742 - trainLoss: 0.6655789017677307\n",
      "cnt: 0 - valLoss: 0.6671470403671265 - trainLoss: 0.6655781865119934\n",
      "cnt: 0 - valLoss: 0.6671463847160339 - trainLoss: 0.6655774116516113\n",
      "cnt: 0 - valLoss: 0.6671457886695862 - trainLoss: 0.6655766367912292\n",
      "cnt: 0 - valLoss: 0.6671451926231384 - trainLoss: 0.6655758619308472\n",
      "cnt: 0 - valLoss: 0.6671445965766907 - trainLoss: 0.6655751466751099\n",
      "cnt: 0 - valLoss: 0.6671440005302429 - trainLoss: 0.6655744314193726\n",
      "cnt: 0 - valLoss: 0.6671433448791504 - trainLoss: 0.6655736565589905\n",
      "cnt: 0 - valLoss: 0.6671428084373474 - trainLoss: 0.6655729413032532\n",
      "cnt: 0 - valLoss: 0.6671420931816101 - trainLoss: 0.6655722260475159\n",
      "cnt: 0 - valLoss: 0.6671414971351624 - trainLoss: 0.6655714511871338\n",
      "cnt: 0 - valLoss: 0.6671409010887146 - trainLoss: 0.6655706763267517\n",
      "cnt: 0 - valLoss: 0.6671403050422668 - trainLoss: 0.6655699610710144\n",
      "cnt: 0 - valLoss: 0.6671397089958191 - trainLoss: 0.6655691862106323\n",
      "cnt: 0 - valLoss: 0.6671391129493713 - trainLoss: 0.6655684113502502\n",
      "cnt: 0 - valLoss: 0.6671385169029236 - trainLoss: 0.6655676364898682\n",
      "cnt: 0 - valLoss: 0.6671379208564758 - trainLoss: 0.6655669808387756\n",
      "cnt: 0 - valLoss: 0.6671372056007385 - trainLoss: 0.6655662059783936\n",
      "cnt: 0 - valLoss: 0.6671366691589355 - trainLoss: 0.6655654311180115\n",
      "cnt: 0 - valLoss: 0.6671361327171326 - trainLoss: 0.665564775466919\n",
      "cnt: 0 - valLoss: 0.66713547706604 - trainLoss: 0.6655640006065369\n",
      "cnt: 0 - valLoss: 0.6671349406242371 - trainLoss: 0.6655632257461548\n",
      "cnt: 0 - valLoss: 0.6671342253684998 - trainLoss: 0.6655625104904175\n",
      "cnt: 0 - valLoss: 0.6671336889266968 - trainLoss: 0.6655617356300354\n",
      "cnt: 0 - valLoss: 0.667133092880249 - trainLoss: 0.6655609607696533\n",
      "cnt: 0 - valLoss: 0.6671324968338013 - trainLoss: 0.665560245513916\n",
      "cnt: 0 - valLoss: 0.6671319007873535 - trainLoss: 0.6655595302581787\n",
      "cnt: 0 - valLoss: 0.667131245136261 - trainLoss: 0.6655588746070862\n",
      "cnt: 0 - valLoss: 0.6671306490898132 - trainLoss: 0.6655580997467041\n",
      "cnt: 0 - valLoss: 0.6671300530433655 - trainLoss: 0.665557324886322\n",
      "cnt: 0 - valLoss: 0.6671294569969177 - trainLoss: 0.6655566096305847\n",
      "cnt: 0 - valLoss: 0.6671289205551147 - trainLoss: 0.6655558347702026\n",
      "cnt: 0 - valLoss: 0.667128324508667 - trainLoss: 0.6655551195144653\n",
      "cnt: 0 - valLoss: 0.6671277284622192 - trainLoss: 0.665554404258728\n",
      "cnt: 0 - valLoss: 0.6671271324157715 - trainLoss: 0.6655536890029907\n",
      "cnt: 0 - valLoss: 0.6671265363693237 - trainLoss: 0.6655529141426086\n",
      "cnt: 0 - valLoss: 0.667125940322876 - trainLoss: 0.6655521392822266\n",
      "cnt: 0 - valLoss: 0.6671253442764282 - trainLoss: 0.665551483631134\n",
      "cnt: 0 - valLoss: 0.6671246886253357 - trainLoss: 0.6655507683753967\n",
      "cnt: 0 - valLoss: 0.6671240925788879 - trainLoss: 0.6655500531196594\n",
      "cnt: 0 - valLoss: 0.6671234965324402 - trainLoss: 0.6655492782592773\n",
      "cnt: 0 - valLoss: 0.6671229004859924 - trainLoss: 0.66554856300354\n",
      "cnt: 0 - valLoss: 0.6671223640441895 - trainLoss: 0.6655478477478027\n",
      "cnt: 0 - valLoss: 0.6671217679977417 - trainLoss: 0.6655471324920654\n",
      "cnt: 0 - valLoss: 0.667121171951294 - trainLoss: 0.6655464172363281\n",
      "cnt: 0 - valLoss: 0.6671205759048462 - trainLoss: 0.665545642375946\n",
      "cnt: 0 - valLoss: 0.6671199798583984 - trainLoss: 0.6655449271202087\n",
      "cnt: 0 - valLoss: 0.6671193838119507 - trainLoss: 0.6655442118644714\n",
      "cnt: 0 - valLoss: 0.6671188473701477 - trainLoss: 0.6655434966087341\n",
      "cnt: 0 - valLoss: 0.6671183109283447 - trainLoss: 0.6655427813529968\n",
      "cnt: 0 - valLoss: 0.667117714881897 - trainLoss: 0.6655420660972595\n",
      "cnt: 0 - valLoss: 0.6671170592308044 - trainLoss: 0.665541410446167\n",
      "cnt: 0 - valLoss: 0.6671164631843567 - trainLoss: 0.6655406355857849\n",
      "cnt: 0 - valLoss: 0.6671159267425537 - trainLoss: 0.6655399203300476\n",
      "cnt: 0 - valLoss: 0.667115330696106 - trainLoss: 0.6655392050743103\n",
      "cnt: 0 - valLoss: 0.667114794254303 - trainLoss: 0.665538489818573\n",
      "cnt: 0 - valLoss: 0.6671141982078552 - trainLoss: 0.6655377745628357\n",
      "cnt: 0 - valLoss: 0.6671136021614075 - trainLoss: 0.6655369997024536\n",
      "cnt: 0 - valLoss: 0.6671129465103149 - trainLoss: 0.6655363440513611\n",
      "cnt: 0 - valLoss: 0.6671124696731567 - trainLoss: 0.6655356287956238\n",
      "cnt: 0 - valLoss: 0.667111873626709 - trainLoss: 0.6655348539352417\n",
      "cnt: 0 - valLoss: 0.6671112775802612 - trainLoss: 0.6655341982841492\n",
      "cnt: 0 - valLoss: 0.6671106815338135 - trainLoss: 0.6655334830284119\n",
      "cnt: 0 - valLoss: 0.6671100854873657 - trainLoss: 0.6655327081680298\n",
      "cnt: 0 - valLoss: 0.6671095490455627 - trainLoss: 0.6655320525169373\n",
      "cnt: 0 - valLoss: 0.6671090126037598 - trainLoss: 0.6655313968658447\n",
      "cnt: 0 - valLoss: 0.6671083569526672 - trainLoss: 0.6655306220054626\n",
      "cnt: 0 - valLoss: 0.6671077609062195 - trainLoss: 0.6655299067497253\n",
      "cnt: 0 - valLoss: 0.6671072244644165 - trainLoss: 0.6655292510986328\n",
      "cnt: 0 - valLoss: 0.6671066284179688 - trainLoss: 0.6655284762382507\n",
      "cnt: 0 - valLoss: 0.667106032371521 - trainLoss: 0.6655277609825134\n",
      "cnt: 0 - valLoss: 0.667105495929718 - trainLoss: 0.6655271053314209\n",
      "cnt: 0 - valLoss: 0.6671048998832703 - trainLoss: 0.6655263900756836\n",
      "cnt: 0 - valLoss: 0.6671043634414673 - trainLoss: 0.6655257344245911\n",
      "cnt: 0 - valLoss: 0.6671038269996643 - trainLoss: 0.665524959564209\n",
      "cnt: 0 - valLoss: 0.6671032309532166 - trainLoss: 0.6655242443084717\n",
      "cnt: 0 - valLoss: 0.6671026349067688 - trainLoss: 0.6655235886573792\n",
      "cnt: 0 - valLoss: 0.6671020984649658 - trainLoss: 0.6655228734016418\n",
      "cnt: 0 - valLoss: 0.6671015024185181 - trainLoss: 0.6655221581459045\n",
      "cnt: 0 - valLoss: 0.6671009063720703 - trainLoss: 0.665521502494812\n",
      "cnt: 0 - valLoss: 0.6671003699302673 - trainLoss: 0.6655207276344299\n",
      "cnt: 0 - valLoss: 0.6670998334884644 - trainLoss: 0.6655200719833374\n",
      "cnt: 0 - valLoss: 0.6670992374420166 - trainLoss: 0.6655193567276001\n",
      "cnt: 0 - valLoss: 0.6670986413955688 - trainLoss: 0.6655187010765076\n",
      "cnt: 0 - valLoss: 0.6670981645584106 - trainLoss: 0.6655179858207703\n",
      "cnt: 0 - valLoss: 0.6670975685119629 - trainLoss: 0.665517270565033\n",
      "cnt: 0 - valLoss: 0.6670969724655151 - trainLoss: 0.6655166149139404\n",
      "cnt: 0 - valLoss: 0.6670964360237122 - trainLoss: 0.6655158996582031\n",
      "cnt: 0 - valLoss: 0.6670958399772644 - trainLoss: 0.6655151844024658\n",
      "cnt: 0 - valLoss: 0.6670953035354614 - trainLoss: 0.6655145287513733\n",
      "cnt: 0 - valLoss: 0.6670947074890137 - trainLoss: 0.665513813495636\n",
      "cnt: 0 - valLoss: 0.6670941710472107 - trainLoss: 0.6655130982398987\n",
      "cnt: 0 - valLoss: 0.6670935750007629 - trainLoss: 0.6655123829841614\n",
      "cnt: 0 - valLoss: 0.6670930981636047 - trainLoss: 0.6655117869377136\n",
      "cnt: 0 - valLoss: 0.667092502117157 - trainLoss: 0.6655110716819763\n",
      "cnt: 0 - valLoss: 0.667091965675354 - trainLoss: 0.6655102968215942\n",
      "cnt: 0 - valLoss: 0.6670913696289062 - trainLoss: 0.6655096411705017\n",
      "cnt: 0 - valLoss: 0.6670908331871033 - trainLoss: 0.6655089855194092\n",
      "cnt: 0 - valLoss: 0.6670902967453003 - trainLoss: 0.6655082702636719\n",
      "cnt: 0 - valLoss: 0.6670897603034973 - trainLoss: 0.6655075550079346\n",
      "cnt: 0 - valLoss: 0.6670891642570496 - trainLoss: 0.665506899356842\n",
      "cnt: 0 - valLoss: 0.6670886278152466 - trainLoss: 0.6655062437057495\n",
      "cnt: 0 - valLoss: 0.6670880913734436 - trainLoss: 0.6655055284500122\n",
      "cnt: 0 - valLoss: 0.6670874953269958 - trainLoss: 0.6655048131942749\n",
      "cnt: 0 - valLoss: 0.6670870184898376 - trainLoss: 0.6655041575431824\n",
      "cnt: 0 - valLoss: 0.6670864224433899 - trainLoss: 0.6655035018920898\n",
      "cnt: 0 - valLoss: 0.6670858860015869 - trainLoss: 0.6655028462409973\n",
      "cnt: 0 - valLoss: 0.6670852899551392 - trainLoss: 0.66550213098526\n",
      "cnt: 0 - valLoss: 0.6670847535133362 - trainLoss: 0.6655014753341675\n",
      "cnt: 0 - valLoss: 0.6670842170715332 - trainLoss: 0.665500819683075\n",
      "cnt: 0 - valLoss: 0.6670836806297302 - trainLoss: 0.6655001044273376\n",
      "cnt: 0 - valLoss: 0.6670831441879272 - trainLoss: 0.6654994487762451\n",
      "cnt: 0 - valLoss: 0.6670826077461243 - trainLoss: 0.6654987335205078\n",
      "cnt: 0 - valLoss: 0.6670820116996765 - trainLoss: 0.6654980778694153\n",
      "cnt: 0 - valLoss: 0.6670815348625183 - trainLoss: 0.665497362613678\n",
      "cnt: 0 - valLoss: 0.6670809388160706 - trainLoss: 0.6654967069625854\n",
      "cnt: 0 - valLoss: 0.6670804023742676 - trainLoss: 0.6654960513114929\n",
      "cnt: 0 - valLoss: 0.6670798659324646 - trainLoss: 0.6654953956604004\n",
      "cnt: 0 - valLoss: 0.6670792698860168 - trainLoss: 0.6654946804046631\n",
      "cnt: 0 - valLoss: 0.6670787930488586 - trainLoss: 0.6654940247535706\n",
      "cnt: 0 - valLoss: 0.6670782566070557 - trainLoss: 0.665493369102478\n",
      "cnt: 0 - valLoss: 0.6670777201652527 - trainLoss: 0.6654926538467407\n",
      "cnt: 0 - valLoss: 0.6670771241188049 - trainLoss: 0.6654919981956482\n",
      "cnt: 0 - valLoss: 0.6670766472816467 - trainLoss: 0.6654913425445557\n",
      "cnt: 0 - valLoss: 0.667076051235199 - trainLoss: 0.6654906868934631\n",
      "cnt: 0 - valLoss: 0.6670755743980408 - trainLoss: 0.6654899716377258\n",
      "cnt: 0 - valLoss: 0.667074978351593 - trainLoss: 0.6654893159866333\n",
      "cnt: 0 - valLoss: 0.66707444190979 - trainLoss: 0.6654886603355408\n",
      "cnt: 0 - valLoss: 0.6670739054679871 - trainLoss: 0.6654879450798035\n",
      "cnt: 0 - valLoss: 0.6670733690261841 - trainLoss: 0.6654873490333557\n",
      "cnt: 0 - valLoss: 0.6670728325843811 - trainLoss: 0.6654866337776184\n",
      "cnt: 0 - valLoss: 0.6670722365379333 - trainLoss: 0.6654859781265259\n",
      "cnt: 0 - valLoss: 0.6670717597007751 - trainLoss: 0.6654853224754333\n",
      "cnt: 0 - valLoss: 0.6670712232589722 - trainLoss: 0.6654846668243408\n",
      "cnt: 0 - valLoss: 0.6670706868171692 - trainLoss: 0.6654840111732483\n",
      "cnt: 0 - valLoss: 0.6670701503753662 - trainLoss: 0.6654833555221558\n",
      "cnt: 0 - valLoss: 0.6670696139335632 - trainLoss: 0.6654826998710632\n",
      "cnt: 0 - valLoss: 0.667069137096405 - trainLoss: 0.6654819846153259\n",
      "cnt: 0 - valLoss: 0.6670685410499573 - trainLoss: 0.6654813885688782\n",
      "cnt: 0 - valLoss: 0.6670680046081543 - trainLoss: 0.6654806733131409\n",
      "cnt: 0 - valLoss: 0.6670674681663513 - trainLoss: 0.6654800176620483\n",
      "cnt: 0 - valLoss: 0.6670669913291931 - trainLoss: 0.6654793620109558\n",
      "cnt: 0 - valLoss: 0.6670663952827454 - trainLoss: 0.6654787063598633\n",
      "cnt: 0 - valLoss: 0.6670659184455872 - trainLoss: 0.6654780507087708\n",
      "cnt: 0 - valLoss: 0.6670653820037842 - trainLoss: 0.6654773950576782\n",
      "cnt: 0 - valLoss: 0.6670648455619812 - trainLoss: 0.6654767394065857\n",
      "cnt: 0 - valLoss: 0.6670643091201782 - trainLoss: 0.6654760837554932\n",
      "cnt: 0 - valLoss: 0.6670637726783752 - trainLoss: 0.6654753684997559\n",
      "cnt: 0 - valLoss: 0.667063295841217 - trainLoss: 0.6654747128486633\n",
      "cnt: 0 - valLoss: 0.6670627593994141 - trainLoss: 0.6654741168022156\n",
      "cnt: 0 - valLoss: 0.6670622229576111 - trainLoss: 0.6654734015464783\n",
      "cnt: 0 - valLoss: 0.6670616865158081 - trainLoss: 0.6654728055000305\n",
      "cnt: 0 - valLoss: 0.6670611500740051 - trainLoss: 0.665472149848938\n",
      "cnt: 0 - valLoss: 0.6670606136322021 - trainLoss: 0.6654715538024902\n",
      "cnt: 0 - valLoss: 0.6670600771903992 - trainLoss: 0.6654708385467529\n",
      "cnt: 0 - valLoss: 0.6670595407485962 - trainLoss: 0.6654701828956604\n",
      "cnt: 0 - valLoss: 0.667059063911438 - trainLoss: 0.6654695868492126\n",
      "cnt: 0 - valLoss: 0.6670585870742798 - trainLoss: 0.6654688715934753\n",
      "cnt: 0 - valLoss: 0.6670580506324768 - trainLoss: 0.6654682159423828\n",
      "cnt: 0 - valLoss: 0.6670575141906738 - trainLoss: 0.6654676198959351\n",
      "cnt: 0 - valLoss: 0.6670569777488708 - trainLoss: 0.6654669642448425\n",
      "cnt: 0 - valLoss: 0.6670565009117126 - trainLoss: 0.66546630859375\n",
      "cnt: 0 - valLoss: 0.6670559048652649 - trainLoss: 0.6654656529426575\n",
      "cnt: 0 - valLoss: 0.6670554280281067 - trainLoss: 0.6654649972915649\n",
      "cnt: 0 - valLoss: 0.6670548915863037 - trainLoss: 0.6654643416404724\n",
      "cnt: 0 - valLoss: 0.6670543551445007 - trainLoss: 0.6654637455940247\n",
      "cnt: 0 - valLoss: 0.6670538187026978 - trainLoss: 0.6654630303382874\n",
      "cnt: 0 - valLoss: 0.6670533418655396 - trainLoss: 0.6654624342918396\n",
      "cnt: 0 - valLoss: 0.6670528650283813 - trainLoss: 0.6654617786407471\n",
      "cnt: 0 - valLoss: 0.6670523285865784 - trainLoss: 0.6654611825942993\n",
      "cnt: 0 - valLoss: 0.6670517921447754 - trainLoss: 0.665460467338562\n",
      "cnt: 0 - valLoss: 0.6670512557029724 - trainLoss: 0.6654598712921143\n",
      "cnt: 0 - valLoss: 0.6670507192611694 - trainLoss: 0.665459156036377\n",
      "cnt: 0 - valLoss: 0.6670502424240112 - trainLoss: 0.665458619594574\n",
      "cnt: 0 - valLoss: 0.667049765586853 - trainLoss: 0.6654579043388367\n",
      "cnt: 0 - valLoss: 0.66704922914505 - trainLoss: 0.6654573082923889\n",
      "cnt: 0 - valLoss: 0.6670486927032471 - trainLoss: 0.6654566526412964\n",
      "cnt: 0 - valLoss: 0.6670481562614441 - trainLoss: 0.6654560565948486\n",
      "cnt: 0 - valLoss: 0.6670476794242859 - trainLoss: 0.6654554009437561\n",
      "cnt: 0 - valLoss: 0.6670471429824829 - trainLoss: 0.6654547452926636\n",
      "cnt: 0 - valLoss: 0.6670466065406799 - trainLoss: 0.6654541492462158\n",
      "cnt: 0 - valLoss: 0.6670461297035217 - trainLoss: 0.6654534935951233\n",
      "cnt: 0 - valLoss: 0.6670456528663635 - trainLoss: 0.6654528379440308\n",
      "cnt: 0 - valLoss: 0.6670451164245605 - trainLoss: 0.6654521822929382\n",
      "cnt: 0 - valLoss: 0.6670445799827576 - trainLoss: 0.6654515862464905\n",
      "cnt: 0 - valLoss: 0.6670441627502441 - trainLoss: 0.665450930595398\n",
      "cnt: 0 - valLoss: 0.6670436263084412 - trainLoss: 0.6654503345489502\n",
      "cnt: 0 - valLoss: 0.6670430898666382 - trainLoss: 0.6654496788978577\n",
      "cnt: 0 - valLoss: 0.66704261302948 - trainLoss: 0.6654490828514099\n",
      "cnt: 0 - valLoss: 0.667042076587677 - trainLoss: 0.6654484272003174\n",
      "cnt: 0 - valLoss: 0.667041540145874 - trainLoss: 0.6654477715492249\n",
      "cnt: 0 - valLoss: 0.6670410633087158 - trainLoss: 0.6654471158981323\n",
      "cnt: 0 - valLoss: 0.6670405268669128 - trainLoss: 0.6654465198516846\n",
      "cnt: 0 - valLoss: 0.6670400500297546 - trainLoss: 0.6654459238052368\n",
      "cnt: 0 - valLoss: 0.6670395731925964 - trainLoss: 0.6654452681541443\n",
      "cnt: 0 - valLoss: 0.6670390367507935 - trainLoss: 0.6654446721076965\n",
      "cnt: 0 - valLoss: 0.6670385599136353 - trainLoss: 0.6654440760612488\n",
      "cnt: 0 - valLoss: 0.6670380234718323 - trainLoss: 0.6654434204101562\n",
      "cnt: 0 - valLoss: 0.6670374870300293 - trainLoss: 0.6654427647590637\n",
      "cnt: 0 - valLoss: 0.6670370101928711 - trainLoss: 0.665442168712616\n",
      "cnt: 0 - valLoss: 0.6670365333557129 - trainLoss: 0.6654415130615234\n",
      "cnt: 0 - valLoss: 0.6670359969139099 - trainLoss: 0.6654408574104309\n",
      "cnt: 0 - valLoss: 0.6670355200767517 - trainLoss: 0.6654402613639832\n",
      "cnt: 0 - valLoss: 0.6670350432395935 - trainLoss: 0.6654396653175354\n",
      "cnt: 0 - valLoss: 0.6670345067977905 - trainLoss: 0.6654390096664429\n",
      "cnt: 0 - valLoss: 0.6670340299606323 - trainLoss: 0.6654384136199951\n",
      "cnt: 0 - valLoss: 0.6670334935188293 - trainLoss: 0.6654378175735474\n",
      "cnt: 0 - valLoss: 0.6670330166816711 - trainLoss: 0.6654371619224548\n",
      "cnt: 0 - valLoss: 0.6670325398445129 - trainLoss: 0.6654365062713623\n",
      "cnt: 0 - valLoss: 0.6670320630073547 - trainLoss: 0.6654359698295593\n",
      "cnt: 0 - valLoss: 0.6670315265655518 - trainLoss: 0.6654353141784668\n",
      "cnt: 0 - valLoss: 0.6670310497283936 - trainLoss: 0.665434718132019\n",
      "cnt: 0 - valLoss: 0.6670305728912354 - trainLoss: 0.6654340624809265\n",
      "cnt: 0 - valLoss: 0.6670300960540771 - trainLoss: 0.6654334664344788\n",
      "cnt: 0 - valLoss: 0.6670295596122742 - trainLoss: 0.6654328107833862\n",
      "cnt: 0 - valLoss: 0.6670290231704712 - trainLoss: 0.6654322147369385\n",
      "cnt: 0 - valLoss: 0.6670286059379578 - trainLoss: 0.6654316186904907\n",
      "cnt: 0 - valLoss: 0.6670280694961548 - trainLoss: 0.665431022644043\n",
      "cnt: 0 - valLoss: 0.6670275926589966 - trainLoss: 0.6654303669929504\n",
      "cnt: 0 - valLoss: 0.6670270562171936 - trainLoss: 0.6654297709465027\n",
      "cnt: 0 - valLoss: 0.6670265793800354 - trainLoss: 0.6654291749000549\n",
      "cnt: 0 - valLoss: 0.6670261025428772 - trainLoss: 0.6654285788536072\n",
      "cnt: 0 - valLoss: 0.667025625705719 - trainLoss: 0.6654278635978699\n",
      "cnt: 0 - valLoss: 0.667025089263916 - trainLoss: 0.6654273271560669\n",
      "cnt: 0 - valLoss: 0.6670246124267578 - trainLoss: 0.6654267311096191\n",
      "cnt: 0 - valLoss: 0.6670241355895996 - trainLoss: 0.6654260754585266\n",
      "cnt: 0 - valLoss: 0.6670237183570862 - trainLoss: 0.6654254794120789\n",
      "cnt: 0 - valLoss: 0.6670231223106384 - trainLoss: 0.6654248833656311\n",
      "cnt: 0 - valLoss: 0.6670226454734802 - trainLoss: 0.6654242873191833\n",
      "cnt: 0 - valLoss: 0.667022168636322 - trainLoss: 0.6654236912727356\n",
      "cnt: 0 - valLoss: 0.6670216917991638 - trainLoss: 0.6654230952262878\n",
      "cnt: 0 - valLoss: 0.6670212745666504 - trainLoss: 0.6654224991798401\n",
      "cnt: 0 - valLoss: 0.6670207381248474 - trainLoss: 0.6654219031333923\n",
      "cnt: 0 - valLoss: 0.6670202612876892 - trainLoss: 0.665421187877655\n",
      "cnt: 0 - valLoss: 0.6670197248458862 - trainLoss: 0.6654207110404968\n",
      "cnt: 0 - valLoss: 0.667019248008728 - trainLoss: 0.6654200553894043\n",
      "cnt: 0 - valLoss: 0.6670187711715698 - trainLoss: 0.6654193997383118\n",
      "cnt: 0 - valLoss: 0.6670183539390564 - trainLoss: 0.665418803691864\n",
      "cnt: 0 - valLoss: 0.6670178174972534 - trainLoss: 0.6654182076454163\n",
      "cnt: 0 - valLoss: 0.6670173406600952 - trainLoss: 0.6654176115989685\n",
      "cnt: 0 - valLoss: 0.667016863822937 - trainLoss: 0.6654170155525208\n",
      "cnt: 0 - valLoss: 0.6670163869857788 - trainLoss: 0.665416419506073\n",
      "cnt: 0 - valLoss: 0.6670159101486206 - trainLoss: 0.6654158234596252\n",
      "cnt: 0 - valLoss: 0.6670154333114624 - trainLoss: 0.6654152274131775\n",
      "cnt: 0 - valLoss: 0.6670148968696594 - trainLoss: 0.6654146313667297\n",
      "cnt: 0 - valLoss: 0.6670144200325012 - trainLoss: 0.6654139757156372\n",
      "cnt: 0 - valLoss: 0.667013943195343 - trainLoss: 0.6654134392738342\n",
      "cnt: 0 - valLoss: 0.6670134663581848 - trainLoss: 0.6654127836227417\n",
      "cnt: 0 - valLoss: 0.6670129895210266 - trainLoss: 0.665412187576294\n",
      "cnt: 0 - valLoss: 0.6670125126838684 - trainLoss: 0.6654115915298462\n",
      "cnt: 0 - valLoss: 0.6670120358467102 - trainLoss: 0.6654110550880432\n",
      "cnt: 0 - valLoss: 0.667011559009552 - trainLoss: 0.6654103994369507\n",
      "cnt: 0 - valLoss: 0.6670111417770386 - trainLoss: 0.6654098033905029\n",
      "cnt: 0 - valLoss: 0.6670106053352356 - trainLoss: 0.6654093265533447\n",
      "cnt: 0 - valLoss: 0.6670101284980774 - trainLoss: 0.6654086709022522\n",
      "cnt: 0 - valLoss: 0.6670096516609192 - trainLoss: 0.6654080748558044\n",
      "cnt: 0 - valLoss: 0.6670092344284058 - trainLoss: 0.6654074788093567\n",
      "cnt: 0 - valLoss: 0.6670087575912476 - trainLoss: 0.6654068827629089\n",
      "cnt: 0 - valLoss: 0.6670082211494446 - trainLoss: 0.6654062867164612\n",
      "cnt: 0 - valLoss: 0.6670078039169312 - trainLoss: 0.6654056906700134\n",
      "cnt: 0 - valLoss: 0.6670072674751282 - trainLoss: 0.6654050946235657\n",
      "cnt: 0 - valLoss: 0.66700679063797 - trainLoss: 0.6654044985771179\n",
      "cnt: 0 - valLoss: 0.6670063138008118 - trainLoss: 0.6654039621353149\n",
      "cnt: 0 - valLoss: 0.6670058369636536 - trainLoss: 0.6654033064842224\n",
      "cnt: 0 - valLoss: 0.6670053601264954 - trainLoss: 0.6654027700424194\n",
      "cnt: 0 - valLoss: 0.6670049428939819 - trainLoss: 0.6654021739959717\n",
      "cnt: 0 - valLoss: 0.6670044660568237 - trainLoss: 0.6654015779495239\n",
      "cnt: 0 - valLoss: 0.6670039892196655 - trainLoss: 0.6654009819030762\n",
      "cnt: 0 - valLoss: 0.6670035123825073 - trainLoss: 0.6654003858566284\n",
      "cnt: 0 - valLoss: 0.6670030355453491 - trainLoss: 0.6653997898101807\n",
      "cnt: 0 - valLoss: 0.6670025587081909 - trainLoss: 0.6653991937637329\n",
      "cnt: 0 - valLoss: 0.6670020818710327 - trainLoss: 0.6653986573219299\n",
      "cnt: 0 - valLoss: 0.6670016050338745 - trainLoss: 0.6653980612754822\n",
      "cnt: 0 - valLoss: 0.6670011878013611 - trainLoss: 0.6653974652290344\n",
      "cnt: 0 - valLoss: 0.6670006513595581 - trainLoss: 0.6653968691825867\n",
      "cnt: 0 - valLoss: 0.6670002341270447 - trainLoss: 0.6653963327407837\n",
      "cnt: 0 - valLoss: 0.6669997572898865 - trainLoss: 0.6653956770896912\n",
      "cnt: 0 - valLoss: 0.6669992804527283 - trainLoss: 0.6653951406478882\n",
      "cnt: 0 - valLoss: 0.6669988036155701 - trainLoss: 0.6653945446014404\n",
      "cnt: 0 - valLoss: 0.6669983863830566 - trainLoss: 0.6653939485549927\n",
      "cnt: 0 - valLoss: 0.6669979095458984 - trainLoss: 0.6653934121131897\n",
      "cnt: 0 - valLoss: 0.6669974327087402 - trainLoss: 0.6653928160667419\n",
      "cnt: 0 - valLoss: 0.666996955871582 - trainLoss: 0.665392279624939\n",
      "cnt: 0 - valLoss: 0.6669965386390686 - trainLoss: 0.6653916835784912\n",
      "cnt: 0 - valLoss: 0.6669960618019104 - trainLoss: 0.6653910875320435\n",
      "cnt: 0 - valLoss: 0.6669955849647522 - trainLoss: 0.6653904914855957\n",
      "cnt: 0 - valLoss: 0.666995108127594 - trainLoss: 0.6653899550437927\n",
      "cnt: 0 - valLoss: 0.6669946908950806 - trainLoss: 0.665389358997345\n",
      "cnt: 0 - valLoss: 0.6669942140579224 - trainLoss: 0.6653887629508972\n",
      "cnt: 0 - valLoss: 0.6669937372207642 - trainLoss: 0.6653881669044495\n",
      "cnt: 0 - valLoss: 0.6669933199882507 - trainLoss: 0.6653876304626465\n",
      "cnt: 0 - valLoss: 0.6669927835464478 - trainLoss: 0.6653870344161987\n",
      "cnt: 0 - valLoss: 0.6669923663139343 - trainLoss: 0.6653864979743958\n",
      "cnt: 0 - valLoss: 0.6669918894767761 - trainLoss: 0.665385901927948\n",
      "cnt: 0 - valLoss: 0.6669914722442627 - trainLoss: 0.6653853058815002\n",
      "cnt: 0 - valLoss: 0.6669909358024597 - trainLoss: 0.6653847098350525\n",
      "cnt: 0 - valLoss: 0.6669905185699463 - trainLoss: 0.6653842329978943\n",
      "cnt: 0 - valLoss: 0.6669900417327881 - trainLoss: 0.6653836369514465\n",
      "cnt: 0 - valLoss: 0.6669896245002747 - trainLoss: 0.6653831005096436\n",
      "cnt: 0 - valLoss: 0.6669890880584717 - trainLoss: 0.6653825044631958\n",
      "cnt: 0 - valLoss: 0.6669886708259583 - trainLoss: 0.6653819680213928\n",
      "cnt: 0 - valLoss: 0.6669881939888 - trainLoss: 0.6653813719749451\n",
      "cnt: 0 - valLoss: 0.6669877767562866 - trainLoss: 0.6653807759284973\n",
      "cnt: 0 - valLoss: 0.6669873595237732 - trainLoss: 0.6653801798820496\n",
      "cnt: 0 - valLoss: 0.6669868230819702 - trainLoss: 0.6653796434402466\n",
      "cnt: 0 - valLoss: 0.666986346244812 - trainLoss: 0.6653791069984436\n",
      "cnt: 0 - valLoss: 0.6669859290122986 - trainLoss: 0.6653785109519958\n",
      "cnt: 0 - valLoss: 0.6669855117797852 - trainLoss: 0.6653779745101929\n",
      "cnt: 0 - valLoss: 0.6669850945472717 - trainLoss: 0.6653773784637451\n",
      "cnt: 0 - valLoss: 0.6669845581054688 - trainLoss: 0.6653767824172974\n",
      "cnt: 0 - valLoss: 0.6669841408729553 - trainLoss: 0.6653762459754944\n",
      "cnt: 0 - valLoss: 0.6669837236404419 - trainLoss: 0.6653756499290466\n",
      "cnt: 0 - valLoss: 0.6669832468032837 - trainLoss: 0.6653751134872437\n",
      "cnt: 0 - valLoss: 0.6669828295707703 - trainLoss: 0.6653745174407959\n",
      "cnt: 0 - valLoss: 0.6669823527336121 - trainLoss: 0.6653739809989929\n",
      "cnt: 0 - valLoss: 0.6669818758964539 - trainLoss: 0.6653734445571899\n",
      "cnt: 0 - valLoss: 0.6669813990592957 - trainLoss: 0.6653728485107422\n",
      "cnt: 0 - valLoss: 0.6669809818267822 - trainLoss: 0.6653723120689392\n",
      "cnt: 0 - valLoss: 0.666980504989624 - trainLoss: 0.6653717756271362\n",
      "cnt: 0 - valLoss: 0.6669800877571106 - trainLoss: 0.6653711795806885\n",
      "cnt: 0 - valLoss: 0.6669796705245972 - trainLoss: 0.6653705835342407\n",
      "cnt: 0 - valLoss: 0.666979193687439 - trainLoss: 0.6653701066970825\n",
      "cnt: 0 - valLoss: 0.6669787168502808 - trainLoss: 0.6653695106506348\n",
      "cnt: 0 - valLoss: 0.6669782996177673 - trainLoss: 0.665368914604187\n",
      "cnt: 0 - valLoss: 0.6669778823852539 - trainLoss: 0.665368378162384\n",
      "cnt: 0 - valLoss: 0.6669774055480957 - trainLoss: 0.665367841720581\n",
      "cnt: 0 - valLoss: 0.6669769287109375 - trainLoss: 0.6653672456741333\n",
      "cnt: 0 - valLoss: 0.6669765114784241 - trainLoss: 0.6653667688369751\n",
      "cnt: 0 - valLoss: 0.6669760942459106 - trainLoss: 0.6653661727905273\n",
      "cnt: 0 - valLoss: 0.6669756174087524 - trainLoss: 0.6653656363487244\n",
      "cnt: 0 - valLoss: 0.666975200176239 - trainLoss: 0.6653650999069214\n",
      "cnt: 0 - valLoss: 0.6669747233390808 - trainLoss: 0.6653645038604736\n",
      "cnt: 0 - valLoss: 0.6669743061065674 - trainLoss: 0.6653640270233154\n",
      "cnt: 0 - valLoss: 0.6669738292694092 - trainLoss: 0.6653634309768677\n",
      "cnt: 0 - valLoss: 0.6669734716415405 - trainLoss: 0.6653628349304199\n",
      "cnt: 0 - valLoss: 0.6669730544090271 - trainLoss: 0.6653623580932617\n",
      "cnt: 0 - valLoss: 0.6669725775718689 - trainLoss: 0.665361762046814\n",
      "cnt: 0 - valLoss: 0.6669721007347107 - trainLoss: 0.6653611660003662\n",
      "cnt: 0 - valLoss: 0.6669716835021973 - trainLoss: 0.665360689163208\n",
      "cnt: 0 - valLoss: 0.6669712662696838 - trainLoss: 0.6653600931167603\n",
      "cnt: 0 - valLoss: 0.6669707298278809 - trainLoss: 0.6653595566749573\n",
      "cnt: 0 - valLoss: 0.6669703722000122 - trainLoss: 0.6653590202331543\n",
      "cnt: 0 - valLoss: 0.666969895362854 - trainLoss: 0.6653584241867065\n",
      "cnt: 0 - valLoss: 0.6669694781303406 - trainLoss: 0.6653579473495483\n",
      "cnt: 0 - valLoss: 0.6669690608978271 - trainLoss: 0.6653573513031006\n",
      "cnt: 0 - valLoss: 0.666968584060669 - trainLoss: 0.6653568744659424\n",
      "cnt: 0 - valLoss: 0.6669681668281555 - trainLoss: 0.6653563380241394\n",
      "cnt: 0 - valLoss: 0.6669677495956421 - trainLoss: 0.6653556823730469\n",
      "cnt: 0 - valLoss: 0.6669673323631287 - trainLoss: 0.6653552055358887\n",
      "cnt: 0 - valLoss: 0.6669669151306152 - trainLoss: 0.6653546690940857\n",
      "cnt: 0 - valLoss: 0.666966438293457 - trainLoss: 0.6653540730476379\n",
      "cnt: 0 - valLoss: 0.6669659614562988 - trainLoss: 0.6653535962104797\n",
      "cnt: 0 - valLoss: 0.6669655442237854 - trainLoss: 0.665353000164032\n",
      "cnt: 0 - valLoss: 0.666965126991272 - trainLoss: 0.6653525233268738\n",
      "cnt: 0 - valLoss: 0.6669646501541138 - trainLoss: 0.665351927280426\n",
      "cnt: 0 - valLoss: 0.6669643521308899 - trainLoss: 0.6653514504432678\n",
      "cnt: 0 - valLoss: 0.6669638156890869 - trainLoss: 0.6653508543968201\n",
      "cnt: 0 - valLoss: 0.6669633984565735 - trainLoss: 0.6653503179550171\n",
      "cnt: 0 - valLoss: 0.6669629812240601 - trainLoss: 0.6653497815132141\n",
      "cnt: 0 - valLoss: 0.6669625043869019 - trainLoss: 0.6653491854667664\n",
      "cnt: 0 - valLoss: 0.6669620871543884 - trainLoss: 0.6653487086296082\n",
      "cnt: 0 - valLoss: 0.666961669921875 - trainLoss: 0.6653481721878052\n",
      "cnt: 0 - valLoss: 0.6669612526893616 - trainLoss: 0.6653476357460022\n",
      "cnt: 0 - valLoss: 0.6669608354568481 - trainLoss: 0.6653470396995544\n",
      "cnt: 0 - valLoss: 0.6669604182243347 - trainLoss: 0.6653465628623962\n",
      "cnt: 0 - valLoss: 0.6669599413871765 - trainLoss: 0.6653460264205933\n",
      "cnt: 0 - valLoss: 0.6669595241546631 - trainLoss: 0.6653454899787903\n",
      "cnt: 0 - valLoss: 0.6669591665267944 - trainLoss: 0.6653449535369873\n",
      "cnt: 0 - valLoss: 0.6669586896896362 - trainLoss: 0.6653444766998291\n",
      "cnt: 0 - valLoss: 0.666958212852478 - trainLoss: 0.6653438806533813\n",
      "cnt: 0 - valLoss: 0.6669577956199646 - trainLoss: 0.6653434038162231\n",
      "cnt: 0 - valLoss: 0.666957437992096 - trainLoss: 0.6653428077697754\n",
      "cnt: 0 - valLoss: 0.6669570207595825 - trainLoss: 0.6653422713279724\n",
      "cnt: 0 - valLoss: 0.6669565439224243 - trainLoss: 0.6653417348861694\n",
      "cnt: 0 - valLoss: 0.6669561862945557 - trainLoss: 0.6653412580490112\n",
      "cnt: 0 - valLoss: 0.6669557094573975 - trainLoss: 0.6653407216072083\n",
      "cnt: 0 - valLoss: 0.666955292224884 - trainLoss: 0.6653401255607605\n",
      "cnt: 0 - valLoss: 0.6669548749923706 - trainLoss: 0.6653396487236023\n",
      "cnt: 0 - valLoss: 0.6669543981552124 - trainLoss: 0.6653391122817993\n",
      "cnt: 0 - valLoss: 0.666953980922699 - trainLoss: 0.6653385758399963\n",
      "cnt: 0 - valLoss: 0.6669535636901855 - trainLoss: 0.6653380393981934\n",
      "cnt: 0 - valLoss: 0.6669532060623169 - trainLoss: 0.6653375625610352\n",
      "cnt: 0 - valLoss: 0.6669527292251587 - trainLoss: 0.6653370261192322\n",
      "cnt: 0 - valLoss: 0.6669523119926453 - trainLoss: 0.6653364300727844\n",
      "cnt: 0 - valLoss: 0.6669518947601318 - trainLoss: 0.6653359532356262\n",
      "cnt: 0 - valLoss: 0.6669515371322632 - trainLoss: 0.6653354167938232\n",
      "cnt: 0 - valLoss: 0.666951060295105 - trainLoss: 0.6653348803520203\n",
      "cnt: 0 - valLoss: 0.6669505834579468 - trainLoss: 0.6653343439102173\n",
      "cnt: 0 - valLoss: 0.6669502854347229 - trainLoss: 0.6653338670730591\n",
      "cnt: 0 - valLoss: 0.6669498682022095 - trainLoss: 0.6653333306312561\n",
      "cnt: 0 - valLoss: 0.6669493913650513 - trainLoss: 0.6653327941894531\n",
      "cnt: 0 - valLoss: 0.6669489741325378 - trainLoss: 0.6653322577476501\n",
      "cnt: 0 - valLoss: 0.6669486165046692 - trainLoss: 0.6653317213058472\n",
      "cnt: 0 - valLoss: 0.666948139667511 - trainLoss: 0.665331244468689\n",
      "cnt: 0 - valLoss: 0.6669477224349976 - trainLoss: 0.665330708026886\n",
      "cnt: 0 - valLoss: 0.6669473052024841 - trainLoss: 0.665330171585083\n",
      "cnt: 0 - valLoss: 0.6669469475746155 - trainLoss: 0.6653296947479248\n",
      "cnt: 0 - valLoss: 0.6669464707374573 - trainLoss: 0.6653291583061218\n",
      "cnt: 0 - valLoss: 0.6669460535049438 - trainLoss: 0.6653286218643188\n",
      "cnt: 0 - valLoss: 0.6669456362724304 - trainLoss: 0.6653281450271606\n",
      "cnt: 0 - valLoss: 0.6669452786445618 - trainLoss: 0.6653276085853577\n",
      "cnt: 0 - valLoss: 0.6669448018074036 - trainLoss: 0.6653270721435547\n",
      "cnt: 0 - valLoss: 0.6669443845748901 - trainLoss: 0.6653265357017517\n",
      "cnt: 0 - valLoss: 0.6669440269470215 - trainLoss: 0.6653259992599487\n",
      "cnt: 0 - valLoss: 0.6669436097145081 - trainLoss: 0.6653254628181458\n",
      "cnt: 0 - valLoss: 0.6669431924819946 - trainLoss: 0.6653250455856323\n",
      "cnt: 0 - valLoss: 0.6669427752494812 - trainLoss: 0.6653244495391846\n",
      "cnt: 0 - valLoss: 0.6669423580169678 - trainLoss: 0.6653239727020264\n",
      "cnt: 0 - valLoss: 0.6669419407844543 - trainLoss: 0.6653234362602234\n",
      "cnt: 0 - valLoss: 0.6669415235519409 - trainLoss: 0.6653229594230652\n",
      "cnt: 0 - valLoss: 0.6669411063194275 - trainLoss: 0.6653224229812622\n",
      "cnt: 0 - valLoss: 0.6669407486915588 - trainLoss: 0.6653218865394592\n",
      "cnt: 0 - valLoss: 0.6669402718544006 - trainLoss: 0.665321409702301\n",
      "cnt: 0 - valLoss: 0.666939914226532 - trainLoss: 0.6653209328651428\n",
      "cnt: 0 - valLoss: 0.6669394969940186 - trainLoss: 0.6653203368186951\n",
      "cnt: 0 - valLoss: 0.6669391393661499 - trainLoss: 0.6653198599815369\n",
      "cnt: 0 - valLoss: 0.6669387221336365 - trainLoss: 0.6653193235397339\n",
      "cnt: 0 - valLoss: 0.6669382452964783 - trainLoss: 0.6653188467025757\n",
      "cnt: 0 - valLoss: 0.6669378876686096 - trainLoss: 0.6653183102607727\n",
      "cnt: 0 - valLoss: 0.6669374704360962 - trainLoss: 0.6653178334236145\n",
      "cnt: 0 - valLoss: 0.6669370532035828 - trainLoss: 0.6653172969818115\n",
      "cnt: 0 - valLoss: 0.6669366359710693 - trainLoss: 0.6653168201446533\n",
      "cnt: 0 - valLoss: 0.6669362783432007 - trainLoss: 0.6653163433074951\n",
      "cnt: 0 - valLoss: 0.6669358611106873 - trainLoss: 0.6653158068656921\n",
      "cnt: 0 - valLoss: 0.6669354438781738 - trainLoss: 0.6653153300285339\n",
      "cnt: 0 - valLoss: 0.6669350266456604 - trainLoss: 0.665314793586731\n",
      "cnt: 0 - valLoss: 0.666934609413147 - trainLoss: 0.665314257144928\n",
      "cnt: 0 - valLoss: 0.6669342517852783 - trainLoss: 0.6653137803077698\n",
      "cnt: 0 - valLoss: 0.6669337749481201 - trainLoss: 0.6653132438659668\n",
      "cnt: 0 - valLoss: 0.6669333577156067 - trainLoss: 0.6653127074241638\n",
      "cnt: 0 - valLoss: 0.666933000087738 - trainLoss: 0.6653122305870056\n",
      "cnt: 0 - valLoss: 0.6669325828552246 - trainLoss: 0.6653116941452026\n",
      "cnt: 0 - valLoss: 0.6669321656227112 - trainLoss: 0.6653112173080444\n",
      "cnt: 0 - valLoss: 0.6669318079948425 - trainLoss: 0.6653107404708862\n",
      "cnt: 0 - valLoss: 0.6669313907623291 - trainLoss: 0.6653102040290833\n",
      "cnt: 0 - valLoss: 0.6669309735298157 - trainLoss: 0.665309727191925\n",
      "cnt: 0 - valLoss: 0.6669305562973022 - trainLoss: 0.6653092503547668\n",
      "cnt: 0 - valLoss: 0.6669301986694336 - trainLoss: 0.6653087139129639\n",
      "cnt: 0 - valLoss: 0.6669297814369202 - trainLoss: 0.6653081774711609\n",
      "cnt: 0 - valLoss: 0.6669294238090515 - trainLoss: 0.6653077006340027\n",
      "cnt: 0 - valLoss: 0.6669290065765381 - trainLoss: 0.6653072237968445\n",
      "cnt: 0 - valLoss: 0.6669285893440247 - trainLoss: 0.6653066873550415\n",
      "cnt: 0 - valLoss: 0.666928231716156 - trainLoss: 0.6653062105178833\n",
      "cnt: 0 - valLoss: 0.6669278144836426 - trainLoss: 0.6653057336807251\n",
      "cnt: 0 - valLoss: 0.6669273972511292 - trainLoss: 0.6653051972389221\n",
      "cnt: 0 - valLoss: 0.6669269800186157 - trainLoss: 0.6653046607971191\n",
      "cnt: 0 - valLoss: 0.6669266223907471 - trainLoss: 0.6653042435646057\n",
      "cnt: 0 - valLoss: 0.6669262051582336 - trainLoss: 0.6653037071228027\n",
      "cnt: 0 - valLoss: 0.6669257879257202 - trainLoss: 0.6653031706809998\n",
      "cnt: 0 - valLoss: 0.6669254302978516 - trainLoss: 0.6653026938438416\n",
      "cnt: 0 - valLoss: 0.6669249534606934 - trainLoss: 0.6653022170066833\n",
      "cnt: 0 - valLoss: 0.6669245958328247 - trainLoss: 0.6653016805648804\n",
      "cnt: 0 - valLoss: 0.666924238204956 - trainLoss: 0.6653012633323669\n",
      "cnt: 0 - valLoss: 0.6669238805770874 - trainLoss: 0.665300726890564\n",
      "cnt: 0 - valLoss: 0.666923463344574 - trainLoss: 0.6653002500534058\n",
      "cnt: 0 - valLoss: 0.6669229865074158 - trainLoss: 0.6652997136116028\n",
      "cnt: 0 - valLoss: 0.6669226884841919 - trainLoss: 0.6652992963790894\n",
      "cnt: 0 - valLoss: 0.6669222116470337 - trainLoss: 0.6652987599372864\n",
      "cnt: 0 - valLoss: 0.6669219136238098 - trainLoss: 0.6652982831001282\n",
      "cnt: 0 - valLoss: 0.6669214367866516 - trainLoss: 0.66529780626297\n",
      "cnt: 0 - valLoss: 0.6669210195541382 - trainLoss: 0.665297269821167\n",
      "cnt: 0 - valLoss: 0.6669207215309143 - trainLoss: 0.6652967929840088\n",
      "cnt: 0 - valLoss: 0.6669202446937561 - trainLoss: 0.6652963161468506\n",
      "cnt: 0 - valLoss: 0.6669198870658875 - trainLoss: 0.6652957797050476\n",
      "cnt: 0 - valLoss: 0.666919469833374 - trainLoss: 0.6652953028678894\n",
      "cnt: 0 - valLoss: 0.6669190526008606 - trainLoss: 0.6652948260307312\n",
      "cnt: 0 - valLoss: 0.6669186949729919 - trainLoss: 0.665294349193573\n",
      "cnt: 0 - valLoss: 0.6669183373451233 - trainLoss: 0.6652938723564148\n",
      "cnt: 0 - valLoss: 0.6669179797172546 - trainLoss: 0.6652933955192566\n",
      "cnt: 0 - valLoss: 0.6669175028800964 - trainLoss: 0.6652928590774536\n",
      "cnt: 0 - valLoss: 0.6669171452522278 - trainLoss: 0.6652923822402954\n",
      "cnt: 0 - valLoss: 0.6669167280197144 - trainLoss: 0.6652919054031372\n",
      "cnt: 0 - valLoss: 0.6669164299964905 - trainLoss: 0.665291428565979\n",
      "cnt: 0 - valLoss: 0.6669159531593323 - trainLoss: 0.665290892124176\n",
      "cnt: 0 - valLoss: 0.6669155955314636 - trainLoss: 0.6652904152870178\n",
      "cnt: 0 - valLoss: 0.6669151782989502 - trainLoss: 0.6652899384498596\n",
      "cnt: 0 - valLoss: 0.6669148802757263 - trainLoss: 0.6652894616127014\n",
      "cnt: 0 - valLoss: 0.6669144630432129 - trainLoss: 0.6652889847755432\n",
      "cnt: 0 - valLoss: 0.6669140458106995 - trainLoss: 0.665288507938385\n",
      "cnt: 0 - valLoss: 0.6669136881828308 - trainLoss: 0.6652880311012268\n",
      "cnt: 0 - valLoss: 0.6669132709503174 - trainLoss: 0.6652875542640686\n",
      "cnt: 0 - valLoss: 0.6669129133224487 - trainLoss: 0.6652870774269104\n",
      "cnt: 0 - valLoss: 0.6669124364852905 - trainLoss: 0.6652866005897522\n",
      "cnt: 0 - valLoss: 0.6669121384620667 - trainLoss: 0.665286123752594\n",
      "cnt: 0 - valLoss: 0.6669117212295532 - trainLoss: 0.6652855277061462\n",
      "cnt: 0 - valLoss: 0.6669113636016846 - trainLoss: 0.6652851104736328\n",
      "cnt: 0 - valLoss: 0.6669109463691711 - trainLoss: 0.6652846336364746\n",
      "cnt: 0 - valLoss: 0.6669105887413025 - trainLoss: 0.6652841567993164\n",
      "cnt: 0 - valLoss: 0.6669102311134338 - trainLoss: 0.665283739566803\n",
      "cnt: 0 - valLoss: 0.6669098138809204 - trainLoss: 0.665283203125\n",
      "cnt: 0 - valLoss: 0.666909396648407 - trainLoss: 0.6652827262878418\n",
      "cnt: 0 - valLoss: 0.6669090986251831 - trainLoss: 0.6652822494506836\n",
      "cnt: 0 - valLoss: 0.6669086217880249 - trainLoss: 0.6652817726135254\n",
      "cnt: 0 - valLoss: 0.6669082641601562 - trainLoss: 0.6652812957763672\n",
      "cnt: 0 - valLoss: 0.6669079065322876 - trainLoss: 0.665280818939209\n",
      "cnt: 0 - valLoss: 0.6669074892997742 - trainLoss: 0.6652803421020508\n",
      "cnt: 0 - valLoss: 0.6669070720672607 - trainLoss: 0.6652798652648926\n",
      "cnt: 0 - valLoss: 0.6669067740440369 - trainLoss: 0.6652793884277344\n",
      "cnt: 0 - valLoss: 0.6669063568115234 - trainLoss: 0.6652789115905762\n",
      "cnt: 0 - valLoss: 0.66690593957901 - trainLoss: 0.665278434753418\n",
      "cnt: 0 - valLoss: 0.6669055819511414 - trainLoss: 0.6652779579162598\n",
      "cnt: 0 - valLoss: 0.6669052243232727 - trainLoss: 0.6652774810791016\n",
      "cnt: 0 - valLoss: 0.6669048070907593 - trainLoss: 0.6652770042419434\n",
      "cnt: 0 - valLoss: 0.6669044494628906 - trainLoss: 0.6652765274047852\n",
      "cnt: 0 - valLoss: 0.666904091835022 - trainLoss: 0.6652759909629822\n",
      "cnt: 0 - valLoss: 0.6669036746025085 - trainLoss: 0.6652755737304688\n",
      "cnt: 0 - valLoss: 0.6669033169746399 - trainLoss: 0.6652750968933105\n",
      "cnt: 0 - valLoss: 0.6669029593467712 - trainLoss: 0.6652746200561523\n",
      "cnt: 0 - valLoss: 0.6669025421142578 - trainLoss: 0.6652741432189941\n",
      "cnt: 0 - valLoss: 0.6669021844863892 - trainLoss: 0.6652737259864807\n",
      "cnt: 0 - valLoss: 0.6669018268585205 - trainLoss: 0.6652731895446777\n",
      "cnt: 0 - valLoss: 0.6669014096260071 - trainLoss: 0.6652727723121643\n",
      "cnt: 0 - valLoss: 0.6669009923934937 - trainLoss: 0.6652722358703613\n",
      "cnt: 0 - valLoss: 0.666900634765625 - trainLoss: 0.6652718186378479\n",
      "cnt: 0 - valLoss: 0.6669002771377563 - trainLoss: 0.6652713418006897\n",
      "cnt: 0 - valLoss: 0.6668999195098877 - trainLoss: 0.6652708649635315\n",
      "cnt: 0 - valLoss: 0.6668995022773743 - trainLoss: 0.6652703881263733\n",
      "cnt: 0 - valLoss: 0.6668992042541504 - trainLoss: 0.6652699112892151\n",
      "cnt: 0 - valLoss: 0.6668987274169922 - trainLoss: 0.6652694344520569\n",
      "cnt: 0 - valLoss: 0.6668983101844788 - trainLoss: 0.6652690172195435\n",
      "cnt: 0 - valLoss: 0.6668980121612549 - trainLoss: 0.6652684807777405\n",
      "cnt: 0 - valLoss: 0.6668976545333862 - trainLoss: 0.665268063545227\n",
      "cnt: 0 - valLoss: 0.6668972969055176 - trainLoss: 0.6652675271034241\n",
      "cnt: 0 - valLoss: 0.6668968796730042 - trainLoss: 0.6652671098709106\n",
      "cnt: 0 - valLoss: 0.6668965220451355 - trainLoss: 0.6652666926383972\n",
      "cnt: 0 - valLoss: 0.6668961048126221 - trainLoss: 0.665266215801239\n",
      "cnt: 0 - valLoss: 0.6668958067893982 - trainLoss: 0.6652657389640808\n",
      "cnt: 0 - valLoss: 0.6668953895568848 - trainLoss: 0.6652652621269226\n",
      "cnt: 0 - valLoss: 0.6668950915336609 - trainLoss: 0.6652647852897644\n",
      "cnt: 0 - valLoss: 0.6668946146965027 - trainLoss: 0.665264368057251\n",
      "cnt: 0 - valLoss: 0.6668941974639893 - trainLoss: 0.665263831615448\n",
      "cnt: 0 - valLoss: 0.6668938994407654 - trainLoss: 0.6652634143829346\n",
      "cnt: 0 - valLoss: 0.6668935418128967 - trainLoss: 0.6652629375457764\n",
      "cnt: 0 - valLoss: 0.6668931245803833 - trainLoss: 0.6652624607086182\n",
      "cnt: 0 - valLoss: 0.6668927669525146 - trainLoss: 0.6652620434761047\n",
      "cnt: 0 - valLoss: 0.666892409324646 - trainLoss: 0.6652615666389465\n",
      "cnt: 0 - valLoss: 0.6668919920921326 - trainLoss: 0.6652610898017883\n",
      "cnt: 0 - valLoss: 0.6668916940689087 - trainLoss: 0.6652606129646301\n",
      "cnt: 0 - valLoss: 0.6668912768363953 - trainLoss: 0.6652601957321167\n",
      "cnt: 0 - valLoss: 0.6668909192085266 - trainLoss: 0.6652597188949585\n",
      "cnt: 0 - valLoss: 0.666890561580658 - trainLoss: 0.6652592420578003\n",
      "cnt: 0 - valLoss: 0.6668902039527893 - trainLoss: 0.6652588248252869\n",
      "cnt: 0 - valLoss: 0.6668898463249207 - trainLoss: 0.6652582883834839\n",
      "cnt: 0 - valLoss: 0.6668894290924072 - trainLoss: 0.6652578711509705\n",
      "cnt: 0 - valLoss: 0.6668890714645386 - trainLoss: 0.665257453918457\n",
      "cnt: 0 - valLoss: 0.6668887138366699 - trainLoss: 0.665256917476654\n",
      "cnt: 0 - valLoss: 0.6668883562088013 - trainLoss: 0.6652565002441406\n",
      "cnt: 0 - valLoss: 0.6668879985809326 - trainLoss: 0.6652560830116272\n",
      "cnt: 0 - valLoss: 0.666887640953064 - trainLoss: 0.6652555465698242\n",
      "cnt: 0 - valLoss: 0.6668872833251953 - trainLoss: 0.6652551293373108\n",
      "cnt: 0 - valLoss: 0.6668868064880371 - trainLoss: 0.6652547121047974\n",
      "cnt: 0 - valLoss: 0.6668865084648132 - trainLoss: 0.6652541756629944\n",
      "cnt: 0 - valLoss: 0.6668861508369446 - trainLoss: 0.665253758430481\n",
      "cnt: 0 - valLoss: 0.6668857336044312 - trainLoss: 0.6652532815933228\n",
      "cnt: 0 - valLoss: 0.6668854355812073 - trainLoss: 0.6652528643608093\n",
      "cnt: 0 - valLoss: 0.6668850779533386 - trainLoss: 0.6652523875236511\n",
      "cnt: 0 - valLoss: 0.6668846607208252 - trainLoss: 0.6652519702911377\n",
      "cnt: 0 - valLoss: 0.6668843626976013 - trainLoss: 0.6652514934539795\n",
      "cnt: 0 - valLoss: 0.6668839454650879 - trainLoss: 0.6652510762214661\n",
      "cnt: 0 - valLoss: 0.6668835878372192 - trainLoss: 0.6652505993843079\n",
      "cnt: 0 - valLoss: 0.6668832302093506 - trainLoss: 0.6652501821517944\n",
      "cnt: 0 - valLoss: 0.6668828725814819 - trainLoss: 0.6652497053146362\n",
      "cnt: 0 - valLoss: 0.6668825149536133 - trainLoss: 0.665249228477478\n",
      "cnt: 0 - valLoss: 0.6668820977210999 - trainLoss: 0.6652488112449646\n",
      "cnt: 0 - valLoss: 0.666881799697876 - trainLoss: 0.6652483344078064\n",
      "cnt: 0 - valLoss: 0.6668814420700073 - trainLoss: 0.6652478575706482\n",
      "cnt: 0 - valLoss: 0.6668810248374939 - trainLoss: 0.6652474403381348\n",
      "cnt: 0 - valLoss: 0.66688072681427 - trainLoss: 0.6652469635009766\n",
      "cnt: 0 - valLoss: 0.6668803095817566 - trainLoss: 0.6652465462684631\n",
      "cnt: 0 - valLoss: 0.6668799519538879 - trainLoss: 0.6652460694313049\n",
      "cnt: 0 - valLoss: 0.6668795943260193 - trainLoss: 0.6652456521987915\n",
      "cnt: 0 - valLoss: 0.6668792366981506 - trainLoss: 0.6652451753616333\n",
      "cnt: 0 - valLoss: 0.666878879070282 - trainLoss: 0.6652447581291199\n",
      "cnt: 0 - valLoss: 0.6668785214424133 - trainLoss: 0.6652442812919617\n",
      "cnt: 0 - valLoss: 0.6668781042098999 - trainLoss: 0.6652438044548035\n",
      "cnt: 0 - valLoss: 0.666877806186676 - trainLoss: 0.66524338722229\n",
      "cnt: 0 - valLoss: 0.6668775081634521 - trainLoss: 0.6652429699897766\n",
      "cnt: 0 - valLoss: 0.666877031326294 - trainLoss: 0.6652424335479736\n",
      "cnt: 0 - valLoss: 0.6668767333030701 - trainLoss: 0.665242075920105\n",
      "cnt: 0 - valLoss: 0.6668763756752014 - trainLoss: 0.6652415990829468\n",
      "cnt: 0 - valLoss: 0.6668760180473328 - trainLoss: 0.6652411818504333\n",
      "cnt: 0 - valLoss: 0.6668756604194641 - trainLoss: 0.6652407646179199\n",
      "cnt: 0 - valLoss: 0.6668753027915955 - trainLoss: 0.6652402877807617\n",
      "cnt: 0 - valLoss: 0.6668749451637268 - trainLoss: 0.6652398705482483\n",
      "cnt: 0 - valLoss: 0.6668745875358582 - trainLoss: 0.6652393937110901\n",
      "cnt: 0 - valLoss: 0.6668742299079895 - trainLoss: 0.6652389168739319\n",
      "cnt: 0 - valLoss: 0.6668739318847656 - trainLoss: 0.6652384996414185\n",
      "cnt: 0 - valLoss: 0.666873574256897 - trainLoss: 0.665238082408905\n",
      "cnt: 0 - valLoss: 0.6668732166290283 - trainLoss: 0.6652376651763916\n",
      "cnt: 0 - valLoss: 0.6668727993965149 - trainLoss: 0.6652371883392334\n",
      "cnt: 0 - valLoss: 0.6668724417686462 - trainLoss: 0.66523677110672\n",
      "cnt: 0 - valLoss: 0.6668721437454224 - trainLoss: 0.6652362942695618\n",
      "cnt: 0 - valLoss: 0.6668717265129089 - trainLoss: 0.6652358770370483\n",
      "cnt: 0 - valLoss: 0.6668714284896851 - trainLoss: 0.6652354598045349\n",
      "cnt: 0 - valLoss: 0.6668710112571716 - trainLoss: 0.6652349829673767\n",
      "cnt: 0 - valLoss: 0.6668707132339478 - trainLoss: 0.6652345061302185\n",
      "cnt: 0 - valLoss: 0.6668703556060791 - trainLoss: 0.6652340888977051\n",
      "cnt: 0 - valLoss: 0.6668700575828552 - trainLoss: 0.6652336716651917\n",
      "cnt: 0 - valLoss: 0.6668696403503418 - trainLoss: 0.6652331948280334\n",
      "cnt: 0 - valLoss: 0.6668692827224731 - trainLoss: 0.6652328372001648\n",
      "cnt: 0 - valLoss: 0.6668689250946045 - trainLoss: 0.6652323603630066\n",
      "cnt: 0 - valLoss: 0.6668686270713806 - trainLoss: 0.6652319431304932\n",
      "cnt: 0 - valLoss: 0.6668682098388672 - trainLoss: 0.6652315258979797\n",
      "cnt: 0 - valLoss: 0.6668679118156433 - trainLoss: 0.6652310490608215\n",
      "cnt: 0 - valLoss: 0.6668675541877747 - trainLoss: 0.6652306318283081\n",
      "cnt: 0 - valLoss: 0.666867196559906 - trainLoss: 0.6652302145957947\n",
      "cnt: 0 - valLoss: 0.6668668389320374 - trainLoss: 0.6652297973632812\n",
      "cnt: 0 - valLoss: 0.6668664813041687 - trainLoss: 0.6652292609214783\n",
      "cnt: 0 - valLoss: 0.6668660640716553 - trainLoss: 0.6652289032936096\n",
      "cnt: 0 - valLoss: 0.6668657660484314 - trainLoss: 0.6652284860610962\n",
      "cnt: 0 - valLoss: 0.6668654680252075 - trainLoss: 0.665228009223938\n",
      "cnt: 0 - valLoss: 0.6668651103973389 - trainLoss: 0.6652275323867798\n",
      "cnt: 0 - valLoss: 0.6668646931648254 - trainLoss: 0.6652271151542664\n",
      "cnt: 0 - valLoss: 0.6668643951416016 - trainLoss: 0.6652266979217529\n",
      "cnt: 0 - valLoss: 0.6668640375137329 - trainLoss: 0.6652262806892395\n",
      "cnt: 0 - valLoss: 0.6668636798858643 - trainLoss: 0.6652258634567261\n",
      "cnt: 0 - valLoss: 0.6668633222579956 - trainLoss: 0.6652254462242126\n",
      "cnt: 0 - valLoss: 0.6668630242347717 - trainLoss: 0.6652249693870544\n",
      "cnt: 0 - valLoss: 0.6668626666069031 - trainLoss: 0.665224552154541\n",
      "cnt: 0 - valLoss: 0.6668623089790344 - trainLoss: 0.6652241349220276\n",
      "cnt: 0 - valLoss: 0.6668619513511658 - trainLoss: 0.6652237176895142\n",
      "cnt: 0 - valLoss: 0.6668616533279419 - trainLoss: 0.6652233004570007\n",
      "cnt: 0 - valLoss: 0.6668612957000732 - trainLoss: 0.6652228236198425\n",
      "cnt: 0 - valLoss: 0.6668609380722046 - trainLoss: 0.6652224063873291\n",
      "cnt: 0 - valLoss: 0.6668605804443359 - trainLoss: 0.6652219295501709\n",
      "cnt: 0 - valLoss: 0.6668602824211121 - trainLoss: 0.6652215719223022\n",
      "cnt: 0 - valLoss: 0.6668599247932434 - trainLoss: 0.6652211546897888\n",
      "cnt: 0 - valLoss: 0.6668595671653748 - trainLoss: 0.6652207374572754\n",
      "cnt: 0 - valLoss: 0.6668592095375061 - trainLoss: 0.6652202606201172\n",
      "cnt: 0 - valLoss: 0.6668589115142822 - trainLoss: 0.6652198433876038\n",
      "cnt: 0 - valLoss: 0.6668585538864136 - trainLoss: 0.6652194261550903\n",
      "cnt: 0 - valLoss: 0.6668581366539001 - trainLoss: 0.6652190089225769\n",
      "cnt: 0 - valLoss: 0.6668578386306763 - trainLoss: 0.6652185916900635\n",
      "cnt: 0 - valLoss: 0.6668575406074524 - trainLoss: 0.6652181148529053\n",
      "cnt: 0 - valLoss: 0.666857123374939 - trainLoss: 0.6652176976203918\n",
      "cnt: 0 - valLoss: 0.6668568253517151 - trainLoss: 0.6652172803878784\n",
      "cnt: 0 - valLoss: 0.6668564677238464 - trainLoss: 0.665216863155365\n",
      "cnt: 0 - valLoss: 0.6668561697006226 - trainLoss: 0.6652164459228516\n",
      "cnt: 0 - valLoss: 0.6668558120727539 - trainLoss: 0.6652160286903381\n",
      "cnt: 0 - valLoss: 0.6668554544448853 - trainLoss: 0.6652155518531799\n",
      "cnt: 0 - valLoss: 0.6668550968170166 - trainLoss: 0.6652151346206665\n",
      "cnt: 0 - valLoss: 0.6668547987937927 - trainLoss: 0.6652147769927979\n",
      "cnt: 0 - valLoss: 0.6668543815612793 - trainLoss: 0.6652143597602844\n",
      "cnt: 0 - valLoss: 0.6668540835380554 - trainLoss: 0.6652138829231262\n",
      "cnt: 0 - valLoss: 0.6668537259101868 - trainLoss: 0.6652134656906128\n",
      "cnt: 0 - valLoss: 0.6668534278869629 - trainLoss: 0.6652130484580994\n",
      "cnt: 0 - valLoss: 0.6668530702590942 - trainLoss: 0.6652126312255859\n",
      "cnt: 0 - valLoss: 0.6668527126312256 - trainLoss: 0.6652122139930725\n",
      "cnt: 0 - valLoss: 0.6668523550033569 - trainLoss: 0.6652117967605591\n",
      "cnt: 0 - valLoss: 0.6668520569801331 - trainLoss: 0.6652113795280457\n",
      "cnt: 0 - valLoss: 0.6668517589569092 - trainLoss: 0.6652109026908875\n",
      "cnt: 0 - valLoss: 0.6668514013290405 - trainLoss: 0.6652105450630188\n",
      "cnt: 0 - valLoss: 0.6668510437011719 - trainLoss: 0.6652100682258606\n",
      "cnt: 0 - valLoss: 0.666850745677948 - trainLoss: 0.6652096509933472\n",
      "cnt: 0 - valLoss: 0.6668503880500793 - trainLoss: 0.6652092933654785\n",
      "cnt: 0 - valLoss: 0.6668500304222107 - trainLoss: 0.6652088761329651\n",
      "cnt: 0 - valLoss: 0.666849672794342 - trainLoss: 0.6652083992958069\n",
      "cnt: 0 - valLoss: 0.6668493747711182 - trainLoss: 0.6652080416679382\n",
      "cnt: 0 - valLoss: 0.6668490171432495 - trainLoss: 0.6652076244354248\n",
      "cnt: 0 - valLoss: 0.6668487191200256 - trainLoss: 0.6652072072029114\n",
      "cnt: 0 - valLoss: 0.666848361492157 - trainLoss: 0.6652067303657532\n",
      "cnt: 0 - valLoss: 0.6668480038642883 - trainLoss: 0.6652063131332397\n",
      "cnt: 0 - valLoss: 0.6668477058410645 - trainLoss: 0.6652059555053711\n",
      "cnt: 0 - valLoss: 0.6668474078178406 - trainLoss: 0.6652055382728577\n",
      "cnt: 0 - valLoss: 0.6668469905853271 - trainLoss: 0.6652050614356995\n",
      "cnt: 0 - valLoss: 0.6668466329574585 - trainLoss: 0.6652047038078308\n",
      "cnt: 0 - valLoss: 0.6668463349342346 - trainLoss: 0.6652042865753174\n",
      "cnt: 0 - valLoss: 0.6668460369110107 - trainLoss: 0.665203869342804\n",
      "cnt: 0 - valLoss: 0.6668456792831421 - trainLoss: 0.6652034521102905\n",
      "cnt: 0 - valLoss: 0.6668453216552734 - trainLoss: 0.6652030348777771\n",
      "cnt: 0 - valLoss: 0.6668450236320496 - trainLoss: 0.6652026176452637\n",
      "cnt: 0 - valLoss: 0.6668446660041809 - trainLoss: 0.6652022004127502\n",
      "cnt: 0 - valLoss: 0.666844367980957 - trainLoss: 0.6652017831802368\n",
      "cnt: 0 - valLoss: 0.6668439507484436 - trainLoss: 0.6652013659477234\n",
      "cnt: 0 - valLoss: 0.6668437123298645 - trainLoss: 0.66520094871521\n",
      "cnt: 0 - valLoss: 0.6668433547019958 - trainLoss: 0.6652005314826965\n",
      "cnt: 0 - valLoss: 0.666843056678772 - trainLoss: 0.6652001738548279\n",
      "cnt: 0 - valLoss: 0.6668426990509033 - trainLoss: 0.6651997566223145\n",
      "cnt: 0 - valLoss: 0.6668423414230347 - trainLoss: 0.665199339389801\n",
      "cnt: 0 - valLoss: 0.666841983795166 - trainLoss: 0.6651989221572876\n",
      "cnt: 0 - valLoss: 0.6668416857719421 - trainLoss: 0.665198564529419\n",
      "cnt: 0 - valLoss: 0.6668413877487183 - trainLoss: 0.6651980876922607\n",
      "cnt: 0 - valLoss: 0.6668410301208496 - trainLoss: 0.6651976704597473\n",
      "cnt: 0 - valLoss: 0.6668407320976257 - trainLoss: 0.6651972532272339\n",
      "cnt: 0 - valLoss: 0.6668403744697571 - trainLoss: 0.6651968955993652\n",
      "cnt: 0 - valLoss: 0.6668400168418884 - trainLoss: 0.665196418762207\n",
      "cnt: 0 - valLoss: 0.6668397188186646 - trainLoss: 0.6651960015296936\n",
      "cnt: 0 - valLoss: 0.6668393611907959 - trainLoss: 0.6651957035064697\n",
      "cnt: 0 - valLoss: 0.666839063167572 - trainLoss: 0.6651952266693115\n",
      "cnt: 0 - valLoss: 0.6668387651443481 - trainLoss: 0.6651948094367981\n",
      "cnt: 0 - valLoss: 0.6668384671211243 - trainLoss: 0.6651943922042847\n",
      "cnt: 0 - valLoss: 0.6668381094932556 - trainLoss: 0.6651939749717712\n",
      "cnt: 0 - valLoss: 0.666837751865387 - trainLoss: 0.6651936173439026\n",
      "cnt: 0 - valLoss: 0.6668374538421631 - trainLoss: 0.6651932001113892\n",
      "cnt: 0 - valLoss: 0.6668370962142944 - trainLoss: 0.6651927828788757\n",
      "cnt: 0 - valLoss: 0.6668367981910706 - trainLoss: 0.6651924252510071\n",
      "cnt: 0 - valLoss: 0.6668364405632019 - trainLoss: 0.6651920080184937\n",
      "cnt: 0 - valLoss: 0.666836142539978 - trainLoss: 0.6651915907859802\n",
      "cnt: 0 - valLoss: 0.6668358445167542 - trainLoss: 0.6651911735534668\n",
      "cnt: 0 - valLoss: 0.6668354868888855 - trainLoss: 0.6651908159255981\n",
      "cnt: 0 - valLoss: 0.6668351292610168 - trainLoss: 0.6651903390884399\n",
      "cnt: 0 - valLoss: 0.6668348908424377 - trainLoss: 0.6651899814605713\n",
      "cnt: 0 - valLoss: 0.6668344736099243 - trainLoss: 0.6651896238327026\n",
      "cnt: 0 - valLoss: 0.6668341755867004 - trainLoss: 0.6651892066001892\n",
      "cnt: 0 - valLoss: 0.6668338775634766 - trainLoss: 0.6651887893676758\n",
      "cnt: 0 - valLoss: 0.6668335795402527 - trainLoss: 0.6651883721351624\n",
      "cnt: 0 - valLoss: 0.666833221912384 - trainLoss: 0.6651879549026489\n",
      "cnt: 0 - valLoss: 0.6668328642845154 - trainLoss: 0.6651875972747803\n",
      "cnt: 0 - valLoss: 0.6668325662612915 - trainLoss: 0.6651871800422668\n",
      "cnt: 0 - valLoss: 0.6668322086334229 - trainLoss: 0.6651867032051086\n",
      "cnt: 0 - valLoss: 0.666831910610199 - trainLoss: 0.6651864051818848\n",
      "cnt: 0 - valLoss: 0.6668316125869751 - trainLoss: 0.6651859283447266\n",
      "cnt: 0 - valLoss: 0.6668312549591064 - trainLoss: 0.6651855111122131\n",
      "cnt: 0 - valLoss: 0.6668309569358826 - trainLoss: 0.6651852130889893\n",
      "cnt: 0 - valLoss: 0.6668306589126587 - trainLoss: 0.665184736251831\n",
      "cnt: 0 - valLoss: 0.66683030128479 - trainLoss: 0.6651843190193176\n",
      "cnt: 0 - valLoss: 0.6668300032615662 - trainLoss: 0.665183961391449\n",
      "cnt: 0 - valLoss: 0.6668296456336975 - trainLoss: 0.6651835441589355\n",
      "cnt: 0 - valLoss: 0.6668293476104736 - trainLoss: 0.6651831865310669\n",
      "cnt: 0 - valLoss: 0.666828989982605 - trainLoss: 0.6651827692985535\n",
      "cnt: 0 - valLoss: 0.6668286919593811 - trainLoss: 0.66518235206604\n",
      "cnt: 0 - valLoss: 0.6668283939361572 - trainLoss: 0.6651819944381714\n",
      "cnt: 0 - valLoss: 0.6668280959129333 - trainLoss: 0.665181577205658\n",
      "cnt: 0 - valLoss: 0.6668277978897095 - trainLoss: 0.6651811599731445\n",
      "cnt: 0 - valLoss: 0.6668274402618408 - trainLoss: 0.6651808023452759\n",
      "cnt: 0 - valLoss: 0.6668270826339722 - trainLoss: 0.6651803851127625\n",
      "cnt: 0 - valLoss: 0.6668268442153931 - trainLoss: 0.665179967880249\n",
      "cnt: 0 - valLoss: 0.6668264865875244 - trainLoss: 0.6651796102523804\n",
      "cnt: 0 - valLoss: 0.6668261289596558 - trainLoss: 0.6651791930198669\n",
      "cnt: 0 - valLoss: 0.6668258309364319 - trainLoss: 0.6651788353919983\n",
      "cnt: 0 - valLoss: 0.6668254733085632 - trainLoss: 0.6651783585548401\n",
      "cnt: 0 - valLoss: 0.6668251752853394 - trainLoss: 0.6651780605316162\n",
      "cnt: 0 - valLoss: 0.6668248772621155 - trainLoss: 0.6651776432991028\n",
      "cnt: 0 - valLoss: 0.6668245792388916 - trainLoss: 0.6651772260665894\n",
      "cnt: 0 - valLoss: 0.6668242812156677 - trainLoss: 0.6651768684387207\n",
      "cnt: 0 - valLoss: 0.6668239235877991 - trainLoss: 0.6651764512062073\n",
      "cnt: 0 - valLoss: 0.6668236255645752 - trainLoss: 0.6651760935783386\n",
      "cnt: 0 - valLoss: 0.6668233275413513 - trainLoss: 0.6651756763458252\n",
      "cnt: 0 - valLoss: 0.6668230295181274 - trainLoss: 0.6651753187179565\n",
      "cnt: 0 - valLoss: 0.6668227314949036 - trainLoss: 0.6651749014854431\n",
      "cnt: 0 - valLoss: 0.6668223738670349 - trainLoss: 0.6651744842529297\n",
      "cnt: 0 - valLoss: 0.6668220162391663 - trainLoss: 0.665174126625061\n",
      "cnt: 0 - valLoss: 0.6668217778205872 - trainLoss: 0.6651737093925476\n",
      "cnt: 0 - valLoss: 0.6668214201927185 - trainLoss: 0.665173351764679\n",
      "cnt: 0 - valLoss: 0.6668210625648499 - trainLoss: 0.6651729345321655\n",
      "cnt: 0 - valLoss: 0.666820764541626 - trainLoss: 0.6651725172996521\n",
      "cnt: 0 - valLoss: 0.6668204665184021 - trainLoss: 0.6651721596717834\n",
      "cnt: 0 - valLoss: 0.6668201684951782 - trainLoss: 0.6651718020439148\n",
      "cnt: 0 - valLoss: 0.6668198704719543 - trainLoss: 0.6651713848114014\n",
      "cnt: 0 - valLoss: 0.6668195128440857 - trainLoss: 0.6651709675788879\n",
      "cnt: 0 - valLoss: 0.6668192148208618 - trainLoss: 0.6651706099510193\n",
      "cnt: 0 - valLoss: 0.6668189167976379 - trainLoss: 0.6651701927185059\n",
      "cnt: 0 - valLoss: 0.6668186187744141 - trainLoss: 0.6651698350906372\n",
      "cnt: 0 - valLoss: 0.6668183207511902 - trainLoss: 0.6651694178581238\n",
      "cnt: 0 - valLoss: 0.6668180227279663 - trainLoss: 0.6651690602302551\n",
      "cnt: 0 - valLoss: 0.6668176651000977 - trainLoss: 0.6651686429977417\n",
      "cnt: 0 - valLoss: 0.6668173670768738 - trainLoss: 0.6651683449745178\n",
      "cnt: 0 - valLoss: 0.6668170690536499 - trainLoss: 0.6651678681373596\n",
      "cnt: 0 - valLoss: 0.6668167114257812 - trainLoss: 0.6651674509048462\n",
      "cnt: 0 - valLoss: 0.6668164134025574 - trainLoss: 0.6651671528816223\n",
      "cnt: 0 - valLoss: 0.6668161749839783 - trainLoss: 0.6651666760444641\n",
      "cnt: 0 - valLoss: 0.6668157577514648 - trainLoss: 0.6651663780212402\n",
      "cnt: 0 - valLoss: 0.6668154001235962 - trainLoss: 0.6651659607887268\n",
      "cnt: 0 - valLoss: 0.6668151617050171 - trainLoss: 0.6651656031608582\n",
      "cnt: 0 - valLoss: 0.6668148040771484 - trainLoss: 0.6651652455329895\n",
      "cnt: 0 - valLoss: 0.6668145656585693 - trainLoss: 0.6651648283004761\n",
      "cnt: 0 - valLoss: 0.6668142676353455 - trainLoss: 0.6651644110679626\n",
      "cnt: 0 - valLoss: 0.6668139696121216 - trainLoss: 0.665164053440094\n",
      "cnt: 0 - valLoss: 0.6668136119842529 - trainLoss: 0.6651636362075806\n",
      "cnt: 0 - valLoss: 0.666813313961029 - trainLoss: 0.6651632785797119\n",
      "cnt: 0 - valLoss: 0.6668130159378052 - trainLoss: 0.6651628613471985\n",
      "cnt: 0 - valLoss: 0.6668126583099365 - trainLoss: 0.6651625633239746\n",
      "cnt: 0 - valLoss: 0.6668124198913574 - trainLoss: 0.6651621460914612\n",
      "cnt: 0 - valLoss: 0.6668120622634888 - trainLoss: 0.6651617884635925\n",
      "cnt: 0 - valLoss: 0.6668117642402649 - trainLoss: 0.6651613712310791\n",
      "cnt: 0 - valLoss: 0.666811466217041 - trainLoss: 0.6651610136032104\n",
      "cnt: 0 - valLoss: 0.6668111681938171 - trainLoss: 0.665160596370697\n",
      "cnt: 0 - valLoss: 0.6668108701705933 - trainLoss: 0.6651602983474731\n",
      "cnt: 0 - valLoss: 0.6668105125427246 - trainLoss: 0.6651598811149597\n",
      "cnt: 0 - valLoss: 0.6668102145195007 - trainLoss: 0.6651595234870911\n",
      "cnt: 0 - valLoss: 0.6668099164962769 - trainLoss: 0.6651591062545776\n",
      "cnt: 0 - valLoss: 0.666809618473053 - trainLoss: 0.665158748626709\n",
      "cnt: 0 - valLoss: 0.6668093204498291 - trainLoss: 0.6651583313941956\n",
      "cnt: 0 - valLoss: 0.6668090224266052 - trainLoss: 0.6651580333709717\n",
      "cnt: 0 - valLoss: 0.6668087244033813 - trainLoss: 0.6651576161384583\n",
      "cnt: 0 - valLoss: 0.6668084263801575 - trainLoss: 0.6651572585105896\n",
      "cnt: 0 - valLoss: 0.6668080687522888 - trainLoss: 0.6651568412780762\n",
      "cnt: 0 - valLoss: 0.6668077707290649 - trainLoss: 0.6651564836502075\n",
      "cnt: 0 - valLoss: 0.6668074727058411 - trainLoss: 0.6651560664176941\n",
      "cnt: 0 - valLoss: 0.6668071746826172 - trainLoss: 0.6651557683944702\n",
      "cnt: 0 - valLoss: 0.6668068766593933 - trainLoss: 0.6651553511619568\n",
      "cnt: 0 - valLoss: 0.6668065786361694 - trainLoss: 0.6651549935340881\n",
      "cnt: 0 - valLoss: 0.6668062806129456 - trainLoss: 0.6651546359062195\n",
      "cnt: 0 - valLoss: 0.6668059825897217 - trainLoss: 0.665154218673706\n",
      "cnt: 0 - valLoss: 0.666805624961853 - trainLoss: 0.6651538610458374\n",
      "cnt: 0 - valLoss: 0.6668053865432739 - trainLoss: 0.6651535034179688\n",
      "cnt: 0 - valLoss: 0.6668050289154053 - trainLoss: 0.6651530861854553\n",
      "cnt: 0 - valLoss: 0.6668047904968262 - trainLoss: 0.6651527285575867\n",
      "cnt: 0 - valLoss: 0.6668044328689575 - trainLoss: 0.6651524305343628\n",
      "cnt: 0 - valLoss: 0.6668041944503784 - trainLoss: 0.6651520133018494\n",
      "cnt: 0 - valLoss: 0.6668038368225098 - trainLoss: 0.6651516556739807\n",
      "cnt: 0 - valLoss: 0.6668035387992859 - trainLoss: 0.6651512980461121\n",
      "cnt: 0 - valLoss: 0.666803240776062 - trainLoss: 0.6651508808135986\n",
      "cnt: 0 - valLoss: 0.6668030023574829 - trainLoss: 0.66515052318573\n",
      "cnt: 0 - valLoss: 0.6668026447296143 - trainLoss: 0.6651501059532166\n",
      "cnt: 0 - valLoss: 0.6668023467063904 - trainLoss: 0.6651497483253479\n",
      "cnt: 0 - valLoss: 0.6668020486831665 - trainLoss: 0.6651493906974792\n",
      "cnt: 0 - valLoss: 0.6668018102645874 - trainLoss: 0.6651490330696106\n",
      "cnt: 0 - valLoss: 0.6668015122413635 - trainLoss: 0.6651486158370972\n",
      "cnt: 0 - valLoss: 0.6668011546134949 - trainLoss: 0.6651483178138733\n",
      "cnt: 0 - valLoss: 0.666800856590271 - trainLoss: 0.6651479005813599\n",
      "cnt: 0 - valLoss: 0.6668005585670471 - trainLoss: 0.665147602558136\n",
      "cnt: 0 - valLoss: 0.6668002605438232 - trainLoss: 0.6651471853256226\n",
      "cnt: 0 - valLoss: 0.6668000221252441 - trainLoss: 0.6651467680931091\n",
      "cnt: 0 - valLoss: 0.6667997241020203 - trainLoss: 0.6651464104652405\n",
      "cnt: 0 - valLoss: 0.6667993664741516 - trainLoss: 0.6651460528373718\n",
      "cnt: 0 - valLoss: 0.6667990684509277 - trainLoss: 0.6651456952095032\n",
      "cnt: 0 - valLoss: 0.6667987704277039 - trainLoss: 0.6651453375816345\n",
      "cnt: 0 - valLoss: 0.66679847240448 - trainLoss: 0.6651449799537659\n",
      "cnt: 0 - valLoss: 0.6667981743812561 - trainLoss: 0.6651446223258972\n",
      "cnt: 0 - valLoss: 0.6667978763580322 - trainLoss: 0.6651442646980286\n",
      "cnt: 0 - valLoss: 0.6667975783348083 - trainLoss: 0.6651438474655151\n",
      "cnt: 0 - valLoss: 0.6667972803115845 - trainLoss: 0.6651434898376465\n",
      "cnt: 0 - valLoss: 0.6667969226837158 - trainLoss: 0.6651431322097778\n",
      "cnt: 0 - valLoss: 0.6667967438697815 - trainLoss: 0.665142834186554\n",
      "cnt: 0 - valLoss: 0.6667963862419128 - trainLoss: 0.6651424169540405\n",
      "cnt: 0 - valLoss: 0.6667961478233337 - trainLoss: 0.6651420593261719\n",
      "cnt: 0 - valLoss: 0.6667957901954651 - trainLoss: 0.6651416420936584\n",
      "cnt: 0 - valLoss: 0.6667954325675964 - trainLoss: 0.6651413440704346\n",
      "cnt: 0 - valLoss: 0.6667951941490173 - trainLoss: 0.6651409268379211\n",
      "cnt: 0 - valLoss: 0.6667949557304382 - trainLoss: 0.6651405692100525\n",
      "cnt: 0 - valLoss: 0.6667946577072144 - trainLoss: 0.6651402711868286\n",
      "cnt: 0 - valLoss: 0.6667943596839905 - trainLoss: 0.6651398539543152\n",
      "cnt: 0 - valLoss: 0.6667940020561218 - trainLoss: 0.6651394963264465\n",
      "cnt: 0 - valLoss: 0.6667937636375427 - trainLoss: 0.6651391983032227\n",
      "cnt: 0 - valLoss: 0.6667934656143188 - trainLoss: 0.6651387810707092\n",
      "cnt: 0 - valLoss: 0.6667931079864502 - trainLoss: 0.6651384234428406\n",
      "cnt: 0 - valLoss: 0.6667928099632263 - trainLoss: 0.6651380658149719\n",
      "cnt: 0 - valLoss: 0.666792631149292 - trainLoss: 0.6651376485824585\n",
      "cnt: 0 - valLoss: 0.6667922735214233 - trainLoss: 0.6651373505592346\n",
      "cnt: 0 - valLoss: 0.6667919754981995 - trainLoss: 0.6651369333267212\n",
      "cnt: 0 - valLoss: 0.6667916178703308 - trainLoss: 0.6651366353034973\n",
      "cnt: 0 - valLoss: 0.6667913198471069 - trainLoss: 0.6651362180709839\n",
      "cnt: 0 - valLoss: 0.6667911410331726 - trainLoss: 0.6651358604431152\n",
      "cnt: 0 - valLoss: 0.6667908430099487 - trainLoss: 0.6651355624198914\n",
      "cnt: 0 - valLoss: 0.6667905449867249 - trainLoss: 0.6651351451873779\n",
      "cnt: 0 - valLoss: 0.666790246963501 - trainLoss: 0.6651347875595093\n",
      "cnt: 0 - valLoss: 0.6667899489402771 - trainLoss: 0.6651344299316406\n",
      "cnt: 0 - valLoss: 0.6667896509170532 - trainLoss: 0.665134072303772\n",
      "cnt: 0 - valLoss: 0.6667892932891846 - trainLoss: 0.6651337146759033\n",
      "cnt: 0 - valLoss: 0.6667890548706055 - trainLoss: 0.6651333570480347\n",
      "cnt: 0 - valLoss: 0.6667887568473816 - trainLoss: 0.665132999420166\n",
      "cnt: 0 - valLoss: 0.6667885184288025 - trainLoss: 0.6651326417922974\n",
      "cnt: 0 - valLoss: 0.6667881608009338 - trainLoss: 0.6651322841644287\n",
      "cnt: 0 - valLoss: 0.6667879223823547 - trainLoss: 0.6651319265365601\n",
      "cnt: 0 - valLoss: 0.6667876243591309 - trainLoss: 0.6651316285133362\n",
      "cnt: 0 - valLoss: 0.666787326335907 - trainLoss: 0.6651312112808228\n",
      "cnt: 0 - valLoss: 0.6667870283126831 - trainLoss: 0.6651309132575989\n",
      "cnt: 0 - valLoss: 0.6667867302894592 - trainLoss: 0.6651304960250854\n",
      "cnt: 0 - valLoss: 0.6667864322662354 - trainLoss: 0.6651301980018616\n",
      "cnt: 0 - valLoss: 0.6667861342430115 - trainLoss: 0.6651297807693481\n",
      "cnt: 0 - valLoss: 0.6667858362197876 - trainLoss: 0.6651294827461243\n",
      "cnt: 0 - valLoss: 0.6667855381965637 - trainLoss: 0.6651290655136108\n",
      "cnt: 0 - valLoss: 0.6667852401733398 - trainLoss: 0.665128767490387\n",
      "cnt: 0 - valLoss: 0.6667850017547607 - trainLoss: 0.6651284098625183\n",
      "cnt: 0 - valLoss: 0.6667847037315369 - trainLoss: 0.6651279926300049\n",
      "cnt: 0 - valLoss: 0.666784405708313 - trainLoss: 0.6651276350021362\n",
      "cnt: 0 - valLoss: 0.6667841672897339 - trainLoss: 0.6651273369789124\n",
      "cnt: 0 - valLoss: 0.6667838096618652 - trainLoss: 0.6651269793510437\n",
      "cnt: 0 - valLoss: 0.6667835116386414 - trainLoss: 0.665126621723175\n",
      "cnt: 0 - valLoss: 0.6667832732200623 - trainLoss: 0.6651262640953064\n",
      "cnt: 0 - valLoss: 0.6667829155921936 - trainLoss: 0.665125846862793\n",
      "cnt: 0 - valLoss: 0.6667826771736145 - trainLoss: 0.6651255488395691\n",
      "cnt: 0 - valLoss: 0.6667824387550354 - trainLoss: 0.6651251912117004\n",
      "cnt: 0 - valLoss: 0.6667820811271667 - trainLoss: 0.6651248931884766\n",
      "cnt: 0 - valLoss: 0.6667818427085876 - trainLoss: 0.6651244759559631\n",
      "cnt: 0 - valLoss: 0.6667815446853638 - trainLoss: 0.6651241779327393\n",
      "cnt: 0 - valLoss: 0.6667812466621399 - trainLoss: 0.6651237607002258\n",
      "cnt: 0 - valLoss: 0.666780948638916 - trainLoss: 0.665123462677002\n",
      "cnt: 0 - valLoss: 0.6667806506156921 - trainLoss: 0.6651231050491333\n",
      "cnt: 0 - valLoss: 0.666780412197113 - trainLoss: 0.6651227474212646\n",
      "cnt: 0 - valLoss: 0.6667801141738892 - trainLoss: 0.6651224493980408\n",
      "cnt: 0 - valLoss: 0.6667798757553101 - trainLoss: 0.6651220321655273\n",
      "cnt: 0 - valLoss: 0.6667795777320862 - trainLoss: 0.6651217341423035\n",
      "cnt: 0 - valLoss: 0.6667792797088623 - trainLoss: 0.6651213765144348\n",
      "cnt: 0 - valLoss: 0.6667789816856384 - trainLoss: 0.6651209592819214\n",
      "cnt: 0 - valLoss: 0.6667786240577698 - trainLoss: 0.6651206612586975\n",
      "cnt: 0 - valLoss: 0.6667783856391907 - trainLoss: 0.6651203036308289\n",
      "cnt: 0 - valLoss: 0.6667781472206116 - trainLoss: 0.6651199460029602\n",
      "cnt: 0 - valLoss: 0.6667778491973877 - trainLoss: 0.6651195883750916\n",
      "cnt: 0 - valLoss: 0.6667775511741638 - trainLoss: 0.6651192307472229\n",
      "cnt: 0 - valLoss: 0.6667772531509399 - trainLoss: 0.665118932723999\n",
      "cnt: 0 - valLoss: 0.6667769551277161 - trainLoss: 0.6651185154914856\n",
      "cnt: 0 - valLoss: 0.6667766571044922 - trainLoss: 0.6651182174682617\n",
      "cnt: 0 - valLoss: 0.6667764186859131 - trainLoss: 0.6651179194450378\n",
      "cnt: 0 - valLoss: 0.666776180267334 - trainLoss: 0.6651175022125244\n",
      "cnt: 0 - valLoss: 0.6667758226394653 - trainLoss: 0.6651172041893005\n",
      "cnt: 0 - valLoss: 0.6667755842208862 - trainLoss: 0.6651168465614319\n",
      "cnt: 0 - valLoss: 0.6667752861976624 - trainLoss: 0.665116548538208\n",
      "cnt: 0 - valLoss: 0.6667750477790833 - trainLoss: 0.6651161313056946\n",
      "cnt: 0 - valLoss: 0.6667746901512146 - trainLoss: 0.6651157736778259\n",
      "cnt: 0 - valLoss: 0.6667744517326355 - trainLoss: 0.6651154160499573\n",
      "cnt: 0 - valLoss: 0.6667741537094116 - trainLoss: 0.6651150584220886\n",
      "cnt: 0 - valLoss: 0.6667739152908325 - trainLoss: 0.6651147603988647\n",
      "cnt: 0 - valLoss: 0.6667736172676086 - trainLoss: 0.6651144027709961\n",
      "cnt: 0 - valLoss: 0.6667733192443848 - trainLoss: 0.6651140451431274\n",
      "cnt: 0 - valLoss: 0.6667730212211609 - trainLoss: 0.6651136875152588\n",
      "cnt: 0 - valLoss: 0.666772723197937 - trainLoss: 0.6651133894920349\n",
      "cnt: 0 - valLoss: 0.6667724847793579 - trainLoss: 0.6651130318641663\n",
      "cnt: 0 - valLoss: 0.666772186756134 - trainLoss: 0.6651127338409424\n",
      "cnt: 0 - valLoss: 0.6667718887329102 - trainLoss: 0.665112316608429\n",
      "cnt: 0 - valLoss: 0.666771650314331 - trainLoss: 0.6651120185852051\n",
      "cnt: 0 - valLoss: 0.6667712926864624 - trainLoss: 0.6651116609573364\n",
      "cnt: 0 - valLoss: 0.6667709946632385 - trainLoss: 0.6651113033294678\n",
      "cnt: 0 - valLoss: 0.6667707562446594 - trainLoss: 0.6651109457015991\n",
      "cnt: 0 - valLoss: 0.6667705774307251 - trainLoss: 0.6651106476783752\n",
      "cnt: 0 - valLoss: 0.6667702198028564 - trainLoss: 0.6651102900505066\n",
      "cnt: 0 - valLoss: 0.6667699813842773 - trainLoss: 0.6651099324226379\n",
      "cnt: 0 - valLoss: 0.6667696833610535 - trainLoss: 0.6651095747947693\n",
      "cnt: 0 - valLoss: 0.6667693853378296 - trainLoss: 0.6651092767715454\n",
      "cnt: 0 - valLoss: 0.6667691469192505 - trainLoss: 0.6651089191436768\n",
      "cnt: 0 - valLoss: 0.6667688488960266 - trainLoss: 0.6651086211204529\n",
      "cnt: 0 - valLoss: 0.6667685508728027 - trainLoss: 0.6651082038879395\n",
      "cnt: 0 - valLoss: 0.6667682528495789 - trainLoss: 0.6651079058647156\n",
      "cnt: 0 - valLoss: 0.6667680144309998 - trainLoss: 0.6651075482368469\n",
      "cnt: 0 - valLoss: 0.6667677164077759 - trainLoss: 0.6651073098182678\n",
      "cnt: 0 - valLoss: 0.6667674779891968 - trainLoss: 0.6651068925857544\n",
      "cnt: 0 - valLoss: 0.6667671799659729 - trainLoss: 0.6651065349578857\n",
      "cnt: 0 - valLoss: 0.666766881942749 - trainLoss: 0.6651061773300171\n",
      "cnt: 0 - valLoss: 0.6667665839195251 - trainLoss: 0.6651058793067932\n",
      "cnt: 0 - valLoss: 0.6667664051055908 - trainLoss: 0.6651055216789246\n",
      "cnt: 0 - valLoss: 0.6667660474777222 - trainLoss: 0.6651052236557007\n",
      "cnt: 0 - valLoss: 0.6667658090591431 - trainLoss: 0.665104866027832\n",
      "cnt: 0 - valLoss: 0.6667655110359192 - trainLoss: 0.6651045680046082\n",
      "cnt: 0 - valLoss: 0.6667652130126953 - trainLoss: 0.6651042103767395\n",
      "cnt: 0 - valLoss: 0.6667649149894714 - trainLoss: 0.6651038527488708\n",
      "cnt: 0 - valLoss: 0.6667646765708923 - trainLoss: 0.6651034951210022\n",
      "cnt: 0 - valLoss: 0.6667644381523132 - trainLoss: 0.6651031970977783\n",
      "cnt: 0 - valLoss: 0.6667641401290894 - trainLoss: 0.6651028394699097\n",
      "cnt: 0 - valLoss: 0.6667638421058655 - trainLoss: 0.6651026010513306\n",
      "cnt: 0 - valLoss: 0.6667635440826416 - trainLoss: 0.6651021838188171\n",
      "cnt: 0 - valLoss: 0.6667633056640625 - trainLoss: 0.6651018261909485\n",
      "cnt: 0 - valLoss: 0.6667630076408386 - trainLoss: 0.6651015281677246\n",
      "cnt: 0 - valLoss: 0.6667627692222595 - trainLoss: 0.6651011109352112\n",
      "cnt: 0 - valLoss: 0.6667624711990356 - trainLoss: 0.6651008129119873\n",
      "cnt: 0 - valLoss: 0.6667621731758118 - trainLoss: 0.6651004552841187\n",
      "cnt: 0 - valLoss: 0.6667619347572327 - trainLoss: 0.6651001572608948\n",
      "cnt: 0 - valLoss: 0.6667616963386536 - trainLoss: 0.6650998592376709\n",
      "cnt: 0 - valLoss: 0.6667613983154297 - trainLoss: 0.6650995016098022\n",
      "cnt: 0 - valLoss: 0.6667611002922058 - trainLoss: 0.6650991439819336\n",
      "cnt: 0 - valLoss: 0.6667608022689819 - trainLoss: 0.6650987863540649\n",
      "cnt: 0 - valLoss: 0.6667605638504028 - trainLoss: 0.6650985479354858\n",
      "cnt: 0 - valLoss: 0.6667603254318237 - trainLoss: 0.6650981903076172\n",
      "cnt: 0 - valLoss: 0.6667600274085999 - trainLoss: 0.6650978922843933\n",
      "cnt: 0 - valLoss: 0.666759729385376 - trainLoss: 0.6650975346565247\n",
      "cnt: 0 - valLoss: 0.6667594313621521 - trainLoss: 0.665097177028656\n",
      "cnt: 0 - valLoss: 0.666759192943573 - trainLoss: 0.6650968194007874\n",
      "cnt: 0 - valLoss: 0.6667589545249939 - trainLoss: 0.6650965213775635\n",
      "cnt: 0 - valLoss: 0.66675865650177 - trainLoss: 0.6650961637496948\n",
      "cnt: 0 - valLoss: 0.6667583584785461 - trainLoss: 0.6650958061218262\n",
      "cnt: 0 - valLoss: 0.666758120059967 - trainLoss: 0.6650955080986023\n",
      "cnt: 0 - valLoss: 0.6667578220367432 - trainLoss: 0.6650952100753784\n",
      "cnt: 0 - valLoss: 0.6667575240135193 - trainLoss: 0.6650948524475098\n",
      "cnt: 0 - valLoss: 0.6667572855949402 - trainLoss: 0.6650945544242859\n",
      "cnt: 0 - valLoss: 0.6667569875717163 - trainLoss: 0.6650941967964172\n",
      "cnt: 0 - valLoss: 0.6667566895484924 - trainLoss: 0.6650938391685486\n",
      "cnt: 0 - valLoss: 0.6667564511299133 - trainLoss: 0.6650934815406799\n",
      "cnt: 0 - valLoss: 0.6667562127113342 - trainLoss: 0.665093183517456\n",
      "cnt: 0 - valLoss: 0.6667558550834656 - trainLoss: 0.6650928854942322\n",
      "cnt: 0 - valLoss: 0.6667556166648865 - trainLoss: 0.6650925278663635\n",
      "cnt: 0 - valLoss: 0.6667553186416626 - trainLoss: 0.6650922298431396\n",
      "cnt: 0 - valLoss: 0.6667550802230835 - trainLoss: 0.665091872215271\n",
      "cnt: 0 - valLoss: 0.6667548418045044 - trainLoss: 0.6650915145874023\n",
      "cnt: 0 - valLoss: 0.6667545437812805 - trainLoss: 0.6650912165641785\n",
      "cnt: 0 - valLoss: 0.6667542457580566 - trainLoss: 0.6650908589363098\n",
      "cnt: 0 - valLoss: 0.6667539477348328 - trainLoss: 0.6650905013084412\n",
      "cnt: 0 - valLoss: 0.6667537093162537 - trainLoss: 0.6650902032852173\n",
      "cnt: 0 - valLoss: 0.6667534112930298 - trainLoss: 0.6650898456573486\n",
      "cnt: 0 - valLoss: 0.6667531132698059 - trainLoss: 0.66508948802948\n",
      "cnt: 0 - valLoss: 0.6667528748512268 - trainLoss: 0.6650892496109009\n",
      "cnt: 0 - valLoss: 0.6667525768280029 - trainLoss: 0.6650888919830322\n",
      "cnt: 0 - valLoss: 0.6667523384094238 - trainLoss: 0.6650885343551636\n",
      "cnt: 0 - valLoss: 0.6667520403862 - trainLoss: 0.6650881767272949\n",
      "cnt: 0 - valLoss: 0.6667518019676208 - trainLoss: 0.6650879383087158\n",
      "cnt: 0 - valLoss: 0.666751503944397 - trainLoss: 0.6650875806808472\n",
      "cnt: 0 - valLoss: 0.6667512059211731 - trainLoss: 0.6650872826576233\n",
      "cnt: 0 - valLoss: 0.6667509078979492 - trainLoss: 0.6650869250297546\n",
      "cnt: 0 - valLoss: 0.6667506694793701 - trainLoss: 0.6650866270065308\n",
      "cnt: 0 - valLoss: 0.666750431060791 - trainLoss: 0.6650862097740173\n",
      "cnt: 0 - valLoss: 0.6667501330375671 - trainLoss: 0.6650859117507935\n",
      "cnt: 0 - valLoss: 0.666749894618988 - trainLoss: 0.6650856137275696\n",
      "cnt: 0 - valLoss: 0.6667495965957642 - trainLoss: 0.6650851964950562\n",
      "cnt: 0 - valLoss: 0.6667493581771851 - trainLoss: 0.665084958076477\n",
      "cnt: 0 - valLoss: 0.6667490601539612 - trainLoss: 0.6650846004486084\n",
      "cnt: 0 - valLoss: 0.6667487621307373 - trainLoss: 0.6650842428207397\n",
      "cnt: 0 - valLoss: 0.6667484641075134 - trainLoss: 0.6650839447975159\n",
      "cnt: 0 - valLoss: 0.6667482852935791 - trainLoss: 0.665083646774292\n",
      "cnt: 0 - valLoss: 0.6667479872703552 - trainLoss: 0.6650833487510681\n",
      "cnt: 0 - valLoss: 0.6667476892471313 - trainLoss: 0.6650829911231995\n",
      "cnt: 0 - valLoss: 0.6667474508285522 - trainLoss: 0.6650826334953308\n",
      "cnt: 0 - valLoss: 0.6667471528053284 - trainLoss: 0.6650822758674622\n",
      "cnt: 0 - valLoss: 0.6667469143867493 - trainLoss: 0.6650819778442383\n",
      "cnt: 0 - valLoss: 0.6667466163635254 - trainLoss: 0.6650816798210144\n",
      "cnt: 0 - valLoss: 0.6667463183403015 - trainLoss: 0.6650813221931458\n",
      "cnt: 0 - valLoss: 0.6667460799217224 - trainLoss: 0.6650810241699219\n",
      "cnt: 0 - valLoss: 0.6667458415031433 - trainLoss: 0.6650806665420532\n",
      "cnt: 0 - valLoss: 0.6667454838752747 - trainLoss: 0.6650804281234741\n",
      "cnt: 0 - valLoss: 0.6667452454566956 - trainLoss: 0.6650800704956055\n",
      "cnt: 0 - valLoss: 0.6667449474334717 - trainLoss: 0.6650797128677368\n",
      "cnt: 0 - valLoss: 0.6667447090148926 - trainLoss: 0.6650793552398682\n",
      "cnt: 0 - valLoss: 0.6667444705963135 - trainLoss: 0.6650791168212891\n",
      "cnt: 0 - valLoss: 0.6667441725730896 - trainLoss: 0.6650787591934204\n",
      "cnt: 0 - valLoss: 0.6667439341545105 - trainLoss: 0.6650784611701965\n",
      "cnt: 0 - valLoss: 0.6667436361312866 - trainLoss: 0.6650781035423279\n",
      "cnt: 0 - valLoss: 0.6667433977127075 - trainLoss: 0.6650778651237488\n",
      "cnt: 0 - valLoss: 0.6667430996894836 - trainLoss: 0.6650773882865906\n",
      "cnt: 0 - valLoss: 0.6667428016662598 - trainLoss: 0.6650771498680115\n",
      "cnt: 0 - valLoss: 0.6667425632476807 - trainLoss: 0.6650767922401428\n",
      "cnt: 0 - valLoss: 0.6667423248291016 - trainLoss: 0.665076494216919\n",
      "cnt: 0 - valLoss: 0.6667420268058777 - trainLoss: 0.6650761365890503\n",
      "cnt: 0 - valLoss: 0.6667417883872986 - trainLoss: 0.6650758981704712\n",
      "cnt: 0 - valLoss: 0.6667414903640747 - trainLoss: 0.6650755405426025\n",
      "cnt: 0 - valLoss: 0.6667412519454956 - trainLoss: 0.6650751829147339\n",
      "cnt: 0 - valLoss: 0.6667409539222717 - trainLoss: 0.66507488489151\n",
      "cnt: 0 - valLoss: 0.6667406558990479 - trainLoss: 0.6650745868682861\n",
      "cnt: 0 - valLoss: 0.6667404174804688 - trainLoss: 0.6650742292404175\n",
      "cnt: 0 - valLoss: 0.6667401790618896 - trainLoss: 0.6650739312171936\n",
      "cnt: 0 - valLoss: 0.6667398810386658 - trainLoss: 0.6650736331939697\n",
      "cnt: 0 - valLoss: 0.6667395830154419 - trainLoss: 0.6650732755661011\n",
      "cnt: 0 - valLoss: 0.6667393445968628 - trainLoss: 0.6650729775428772\n",
      "cnt: 0 - valLoss: 0.6667391061782837 - trainLoss: 0.6650726795196533\n",
      "cnt: 0 - valLoss: 0.6667388677597046 - trainLoss: 0.6650723218917847\n",
      "cnt: 0 - valLoss: 0.6667385101318359 - trainLoss: 0.6650720238685608\n",
      "cnt: 0 - valLoss: 0.6667382717132568 - trainLoss: 0.6650716662406921\n",
      "cnt: 0 - valLoss: 0.6667380332946777 - trainLoss: 0.6650713682174683\n",
      "cnt: 0 - valLoss: 0.6667377352714539 - trainLoss: 0.6650710701942444\n",
      "cnt: 0 - valLoss: 0.6667374968528748 - trainLoss: 0.6650707125663757\n",
      "cnt: 0 - valLoss: 0.6667372584342957 - trainLoss: 0.6650704145431519\n",
      "cnt: 0 - valLoss: 0.6667369604110718 - trainLoss: 0.665070116519928\n",
      "cnt: 0 - valLoss: 0.6667367219924927 - trainLoss: 0.6650698184967041\n",
      "cnt: 0 - valLoss: 0.6667364239692688 - trainLoss: 0.6650695204734802\n",
      "cnt: 0 - valLoss: 0.6667361855506897 - trainLoss: 0.6650691628456116\n",
      "cnt: 0 - valLoss: 0.6667358875274658 - trainLoss: 0.6650688648223877\n",
      "cnt: 0 - valLoss: 0.6667355895042419 - trainLoss: 0.6650685667991638\n",
      "cnt: 0 - valLoss: 0.6667354106903076 - trainLoss: 0.6650682091712952\n",
      "cnt: 0 - valLoss: 0.6667351126670837 - trainLoss: 0.6650679111480713\n",
      "cnt: 0 - valLoss: 0.6667348742485046 - trainLoss: 0.6650675535202026\n",
      "cnt: 0 - valLoss: 0.6667345762252808 - trainLoss: 0.6650672554969788\n",
      "cnt: 0 - valLoss: 0.6667343378067017 - trainLoss: 0.6650668978691101\n",
      "cnt: 0 - valLoss: 0.6667340993881226 - trainLoss: 0.6650665998458862\n",
      "cnt: 0 - valLoss: 0.6667338013648987 - trainLoss: 0.6650663018226624\n",
      "cnt: 0 - valLoss: 0.6667335629463196 - trainLoss: 0.6650660037994385\n",
      "cnt: 0 - valLoss: 0.6667332649230957 - trainLoss: 0.6650657057762146\n",
      "cnt: 0 - valLoss: 0.6667329668998718 - trainLoss: 0.665065348148346\n",
      "cnt: 0 - valLoss: 0.6667327284812927 - trainLoss: 0.6650651097297668\n",
      "cnt: 0 - valLoss: 0.6667325496673584 - trainLoss: 0.6650647521018982\n",
      "cnt: 0 - valLoss: 0.6667322516441345 - trainLoss: 0.6650643944740295\n",
      "cnt: 0 - valLoss: 0.6667319536209106 - trainLoss: 0.6650641560554504\n",
      "cnt: 0 - valLoss: 0.6667317152023315 - trainLoss: 0.6650637984275818\n",
      "cnt: 0 - valLoss: 0.6667314767837524 - trainLoss: 0.6650635600090027\n",
      "cnt: 0 - valLoss: 0.6667311787605286 - trainLoss: 0.665063202381134\n",
      "cnt: 0 - valLoss: 0.6667309403419495 - trainLoss: 0.6650628447532654\n",
      "cnt: 0 - valLoss: 0.6667307019233704 - trainLoss: 0.6650625467300415\n",
      "cnt: 0 - valLoss: 0.6667303442955017 - trainLoss: 0.6650622487068176\n",
      "cnt: 0 - valLoss: 0.6667301058769226 - trainLoss: 0.665061891078949\n",
      "cnt: 0 - valLoss: 0.6667298674583435 - trainLoss: 0.6650616526603699\n",
      "cnt: 0 - valLoss: 0.6667296290397644 - trainLoss: 0.6650612950325012\n",
      "cnt: 0 - valLoss: 0.6667293310165405 - trainLoss: 0.6650609970092773\n",
      "cnt: 0 - valLoss: 0.6667290329933167 - trainLoss: 0.6650606989860535\n",
      "cnt: 0 - valLoss: 0.6667288541793823 - trainLoss: 0.6650604009628296\n",
      "cnt: 0 - valLoss: 0.6667285561561584 - trainLoss: 0.6650601029396057\n",
      "cnt: 0 - valLoss: 0.6667283177375793 - trainLoss: 0.6650598049163818\n",
      "cnt: 0 - valLoss: 0.6667280793190002 - trainLoss: 0.6650594472885132\n",
      "cnt: 0 - valLoss: 0.6667277812957764 - trainLoss: 0.6650592088699341\n",
      "cnt: 0 - valLoss: 0.6667275428771973 - trainLoss: 0.6650588512420654\n",
      "cnt: 0 - valLoss: 0.6667272448539734 - trainLoss: 0.6650585532188416\n",
      "cnt: 0 - valLoss: 0.6667270064353943 - trainLoss: 0.6650582551956177\n",
      "cnt: 0 - valLoss: 0.6667267680168152 - trainLoss: 0.665057897567749\n",
      "cnt: 0 - valLoss: 0.6667264699935913 - trainLoss: 0.6650575399398804\n",
      "cnt: 0 - valLoss: 0.6667261719703674 - trainLoss: 0.6650573015213013\n",
      "cnt: 0 - valLoss: 0.6667259931564331 - trainLoss: 0.6650569438934326\n",
      "cnt: 0 - valLoss: 0.6667256951332092 - trainLoss: 0.6650567054748535\n",
      "cnt: 0 - valLoss: 0.6667253971099854 - trainLoss: 0.6650563478469849\n",
      "cnt: 0 - valLoss: 0.666725218296051 - trainLoss: 0.665056049823761\n",
      "cnt: 0 - valLoss: 0.6667249798774719 - trainLoss: 0.6650557518005371\n",
      "cnt: 0 - valLoss: 0.666724681854248 - trainLoss: 0.6650554537773132\n",
      "cnt: 0 - valLoss: 0.6667243838310242 - trainLoss: 0.6650551557540894\n",
      "cnt: 0 - valLoss: 0.6667241454124451 - trainLoss: 0.6650547981262207\n",
      "cnt: 0 - valLoss: 0.6667238473892212 - trainLoss: 0.6650545001029968\n",
      "cnt: 0 - valLoss: 0.6667236089706421 - trainLoss: 0.665054202079773\n",
      "cnt: 0 - valLoss: 0.666723370552063 - trainLoss: 0.6650538444519043\n",
      "cnt: 0 - valLoss: 0.6667231321334839 - trainLoss: 0.6650536060333252\n",
      "cnt: 0 - valLoss: 0.6667228937149048 - trainLoss: 0.6650532484054565\n",
      "cnt: 0 - valLoss: 0.6667225956916809 - trainLoss: 0.6650530099868774\n",
      "cnt: 0 - valLoss: 0.6667223572731018 - trainLoss: 0.6650526523590088\n",
      "cnt: 0 - valLoss: 0.6667220592498779 - trainLoss: 0.6650523543357849\n",
      "cnt: 0 - valLoss: 0.6667218804359436 - trainLoss: 0.665052056312561\n",
      "cnt: 0 - valLoss: 0.6667215824127197 - trainLoss: 0.6650517582893372\n",
      "cnt: 0 - valLoss: 0.6667213439941406 - trainLoss: 0.6650514602661133\n",
      "cnt: 0 - valLoss: 0.6667210459709167 - trainLoss: 0.6650511622428894\n",
      "cnt: 0 - valLoss: 0.6667208075523376 - trainLoss: 0.6650508642196655\n",
      "cnt: 0 - valLoss: 0.6667205095291138 - trainLoss: 0.6650505065917969\n",
      "cnt: 0 - valLoss: 0.6667203307151794 - trainLoss: 0.6650502681732178\n",
      "cnt: 0 - valLoss: 0.6667200326919556 - trainLoss: 0.6650499105453491\n",
      "cnt: 0 - valLoss: 0.6667197942733765 - trainLoss: 0.6650496125221252\n",
      "cnt: 0 - valLoss: 0.6667195558547974 - trainLoss: 0.6650493144989014\n",
      "cnt: 0 - valLoss: 0.6667193174362183 - trainLoss: 0.6650490164756775\n",
      "cnt: 0 - valLoss: 0.6667189598083496 - trainLoss: 0.6650487184524536\n",
      "cnt: 0 - valLoss: 0.6667187213897705 - trainLoss: 0.6650484204292297\n",
      "cnt: 0 - valLoss: 0.6667185425758362 - trainLoss: 0.6650481224060059\n",
      "cnt: 0 - valLoss: 0.6667182445526123 - trainLoss: 0.665047824382782\n",
      "cnt: 0 - valLoss: 0.6667179465293884 - trainLoss: 0.6650475263595581\n",
      "cnt: 0 - valLoss: 0.6667177677154541 - trainLoss: 0.6650471687316895\n",
      "cnt: 0 - valLoss: 0.6667174696922302 - trainLoss: 0.6650468707084656\n",
      "cnt: 0 - valLoss: 0.6667171716690063 - trainLoss: 0.6650465726852417\n",
      "cnt: 0 - valLoss: 0.666716992855072 - trainLoss: 0.6650462746620178\n",
      "cnt: 0 - valLoss: 0.6667167544364929 - trainLoss: 0.6650460362434387\n",
      "cnt: 0 - valLoss: 0.6667165160179138 - trainLoss: 0.6650456786155701\n",
      "cnt: 0 - valLoss: 0.6667162179946899 - trainLoss: 0.6650453805923462\n",
      "cnt: 0 - valLoss: 0.6667159795761108 - trainLoss: 0.6650450825691223\n",
      "cnt: 0 - valLoss: 0.6667157411575317 - trainLoss: 0.6650447845458984\n",
      "cnt: 0 - valLoss: 0.6667154431343079 - trainLoss: 0.6650444865226746\n",
      "cnt: 0 - valLoss: 0.666715145111084 - trainLoss: 0.6650441884994507\n",
      "cnt: 0 - valLoss: 0.6667149662971497 - trainLoss: 0.665043830871582\n",
      "cnt: 0 - valLoss: 0.6667146682739258 - trainLoss: 0.6650435924530029\n",
      "cnt: 0 - valLoss: 0.6667144298553467 - trainLoss: 0.665043294429779\n",
      "cnt: 0 - valLoss: 0.6667141914367676 - trainLoss: 0.6650429964065552\n",
      "cnt: 0 - valLoss: 0.6667139530181885 - trainLoss: 0.6650426387786865\n",
      "cnt: 0 - valLoss: 0.6667136549949646 - trainLoss: 0.6650424003601074\n",
      "cnt: 0 - valLoss: 0.6667134165763855 - trainLoss: 0.6650420427322388\n",
      "cnt: 0 - valLoss: 0.6667131781578064 - trainLoss: 0.6650418043136597\n",
      "cnt: 0 - valLoss: 0.6667128801345825 - trainLoss: 0.665041446685791\n",
      "cnt: 0 - valLoss: 0.6667126417160034 - trainLoss: 0.6650412082672119\n",
      "cnt: 0 - valLoss: 0.6667124032974243 - trainLoss: 0.6650408506393433\n",
      "cnt: 0 - valLoss: 0.6667121052742004 - trainLoss: 0.6650406122207642\n",
      "cnt: 0 - valLoss: 0.6667118668556213 - trainLoss: 0.6650403141975403\n",
      "cnt: 0 - valLoss: 0.666711688041687 - trainLoss: 0.6650400161743164\n",
      "cnt: 0 - valLoss: 0.6667113900184631 - trainLoss: 0.6650396585464478\n",
      "cnt: 0 - valLoss: 0.6667110919952393 - trainLoss: 0.6650394201278687\n",
      "cnt: 0 - valLoss: 0.6667109131813049 - trainLoss: 0.6650391221046448\n",
      "cnt: 0 - valLoss: 0.666710615158081 - trainLoss: 0.6650388240814209\n",
      "cnt: 0 - valLoss: 0.666710376739502 - trainLoss: 0.665038526058197\n",
      "cnt: 0 - valLoss: 0.6667101383209229 - trainLoss: 0.6650382280349731\n",
      "cnt: 0 - valLoss: 0.666709840297699 - trainLoss: 0.6650379300117493\n",
      "cnt: 0 - valLoss: 0.6667096018791199 - trainLoss: 0.6650376319885254\n",
      "cnt: 0 - valLoss: 0.6667093634605408 - trainLoss: 0.6650373339653015\n",
      "cnt: 0 - valLoss: 0.6667091250419617 - trainLoss: 0.6650370955467224\n",
      "cnt: 0 - valLoss: 0.6667088866233826 - trainLoss: 0.6650367379188538\n",
      "cnt: 0 - valLoss: 0.6667085886001587 - trainLoss: 0.6650363802909851\n",
      "cnt: 0 - valLoss: 0.6667082905769348 - trainLoss: 0.665036141872406\n",
      "cnt: 0 - valLoss: 0.6667081117630005 - trainLoss: 0.6650359034538269\n",
      "cnt: 0 - valLoss: 0.6667078137397766 - trainLoss: 0.665035605430603\n",
      "cnt: 0 - valLoss: 0.6667075157165527 - trainLoss: 0.6650351881980896\n",
      "cnt: 0 - valLoss: 0.6667073369026184 - trainLoss: 0.6650349497795105\n",
      "cnt: 0 - valLoss: 0.6667070388793945 - trainLoss: 0.6650346517562866\n",
      "cnt: 0 - valLoss: 0.6667068004608154 - trainLoss: 0.6650343537330627\n",
      "cnt: 0 - valLoss: 0.6667065024375916 - trainLoss: 0.6650341153144836\n",
      "cnt: 0 - valLoss: 0.6667063236236572 - trainLoss: 0.665033757686615\n",
      "cnt: 0 - valLoss: 0.6667059659957886 - trainLoss: 0.6650335192680359\n",
      "cnt: 0 - valLoss: 0.6667057275772095 - trainLoss: 0.6650331616401672\n",
      "cnt: 0 - valLoss: 0.6667054891586304 - trainLoss: 0.6650328636169434\n",
      "cnt: 0 - valLoss: 0.6667052507400513 - trainLoss: 0.6650326251983643\n",
      "cnt: 0 - valLoss: 0.6667049527168274 - trainLoss: 0.6650323271751404\n",
      "cnt: 0 - valLoss: 0.6667047739028931 - trainLoss: 0.6650320291519165\n",
      "cnt: 0 - valLoss: 0.6667044162750244 - trainLoss: 0.6650316715240479\n",
      "cnt: 0 - valLoss: 0.6667041778564453 - trainLoss: 0.665031373500824\n",
      "cnt: 0 - valLoss: 0.6667039394378662 - trainLoss: 0.6650310754776001\n",
      "cnt: 0 - valLoss: 0.6667036414146423 - trainLoss: 0.665030837059021\n",
      "cnt: 0 - valLoss: 0.6667034029960632 - trainLoss: 0.6650304794311523\n",
      "cnt: 0 - valLoss: 0.6667032241821289 - trainLoss: 0.6650302410125732\n",
      "cnt: 0 - valLoss: 0.666702926158905 - trainLoss: 0.6650299429893494\n",
      "cnt: 0 - valLoss: 0.6667026281356812 - trainLoss: 0.6650296449661255\n",
      "cnt: 0 - valLoss: 0.666702389717102 - trainLoss: 0.6650293469429016\n",
      "cnt: 0 - valLoss: 0.666702151298523 - trainLoss: 0.6650290489196777\n",
      "cnt: 0 - valLoss: 0.6667019128799438 - trainLoss: 0.6650288105010986\n",
      "cnt: 0 - valLoss: 0.6667016744613647 - trainLoss: 0.66502845287323\n",
      "cnt: 0 - valLoss: 0.6667013764381409 - trainLoss: 0.6650282144546509\n",
      "cnt: 0 - valLoss: 0.6667011380195618 - trainLoss: 0.665027916431427\n",
      "cnt: 0 - valLoss: 0.6667008399963379 - trainLoss: 0.6650276184082031\n",
      "cnt: 0 - valLoss: 0.6667006015777588 - trainLoss: 0.6650272607803345\n",
      "cnt: 0 - valLoss: 0.6667003631591797 - trainLoss: 0.6650269627571106\n",
      "cnt: 0 - valLoss: 0.6667001247406006 - trainLoss: 0.6650266647338867\n",
      "cnt: 0 - valLoss: 0.6666998863220215 - trainLoss: 0.6650263667106628\n",
      "cnt: 0 - valLoss: 0.6666995882987976 - trainLoss: 0.6650261282920837\n",
      "cnt: 0 - valLoss: 0.6666992902755737 - trainLoss: 0.6650258302688599\n",
      "cnt: 0 - valLoss: 0.6666989922523499 - trainLoss: 0.6650255918502808\n",
      "cnt: 0 - valLoss: 0.6666987538337708 - trainLoss: 0.6650252938270569\n",
      "cnt: 0 - valLoss: 0.6666985750198364 - trainLoss: 0.665024995803833\n",
      "cnt: 0 - valLoss: 0.6666982769966125 - trainLoss: 0.6650246977806091\n",
      "cnt: 0 - valLoss: 0.6666980385780334 - trainLoss: 0.6650243401527405\n",
      "cnt: 0 - valLoss: 0.6666977405548096 - trainLoss: 0.6650241017341614\n",
      "cnt: 0 - valLoss: 0.6666975021362305 - trainLoss: 0.6650238037109375\n",
      "cnt: 0 - valLoss: 0.6666973233222961 - trainLoss: 0.6650235056877136\n",
      "cnt: 0 - valLoss: 0.6666970252990723 - trainLoss: 0.6650232076644897\n",
      "cnt: 0 - valLoss: 0.6666967868804932 - trainLoss: 0.6650229096412659\n",
      "cnt: 0 - valLoss: 0.6666964888572693 - trainLoss: 0.665022611618042\n",
      "cnt: 0 - valLoss: 0.6666962504386902 - trainLoss: 0.6650223731994629\n",
      "cnt: 0 - valLoss: 0.6666960120201111 - trainLoss: 0.665022075176239\n",
      "cnt: 0 - valLoss: 0.6666957139968872 - trainLoss: 0.6650217771530151\n",
      "cnt: 0 - valLoss: 0.6666954755783081 - trainLoss: 0.6650214791297913\n",
      "cnt: 0 - valLoss: 0.666695237159729 - trainLoss: 0.6650212407112122\n",
      "cnt: 0 - valLoss: 0.6666949987411499 - trainLoss: 0.6650209426879883\n",
      "cnt: 0 - valLoss: 0.666694700717926 - trainLoss: 0.6650206446647644\n",
      "cnt: 0 - valLoss: 0.6666944622993469 - trainLoss: 0.6650202870368958\n",
      "cnt: 0 - valLoss: 0.6666942834854126 - trainLoss: 0.6650200486183167\n",
      "cnt: 0 - valLoss: 0.6666939854621887 - trainLoss: 0.6650197505950928\n",
      "cnt: 0 - valLoss: 0.6666937470436096 - trainLoss: 0.6650194525718689\n",
      "cnt: 0 - valLoss: 0.6666934490203857 - trainLoss: 0.6650192141532898\n",
      "cnt: 0 - valLoss: 0.6666932106018066 - trainLoss: 0.6650189161300659\n",
      "cnt: 0 - valLoss: 0.6666929721832275 - trainLoss: 0.665018618106842\n",
      "cnt: 0 - valLoss: 0.6666927337646484 - trainLoss: 0.6650183200836182\n",
      "cnt: 0 - valLoss: 0.6666924357414246 - trainLoss: 0.6650180220603943\n",
      "cnt: 0 - valLoss: 0.6666922569274902 - trainLoss: 0.6650177240371704\n",
      "cnt: 0 - valLoss: 0.6666919589042664 - trainLoss: 0.6650174260139465\n",
      "cnt: 0 - valLoss: 0.6666917204856873 - trainLoss: 0.6650171875953674\n",
      "cnt: 0 - valLoss: 0.6666914820671082 - trainLoss: 0.6650169491767883\n",
      "cnt: 0 - valLoss: 0.6666911840438843 - trainLoss: 0.6650166511535645\n",
      "cnt: 0 - valLoss: 0.6666909456253052 - trainLoss: 0.6650163531303406\n",
      "cnt: 0 - valLoss: 0.6666907072067261 - trainLoss: 0.6650160551071167\n",
      "cnt: 0 - valLoss: 0.6666905283927917 - trainLoss: 0.6650157570838928\n",
      "cnt: 0 - valLoss: 0.6666901707649231 - trainLoss: 0.665015459060669\n",
      "cnt: 0 - valLoss: 0.6666899919509888 - trainLoss: 0.6650152206420898\n",
      "cnt: 0 - valLoss: 0.6666896939277649 - trainLoss: 0.665014922618866\n",
      "cnt: 0 - valLoss: 0.6666894555091858 - trainLoss: 0.6650146842002869\n",
      "cnt: 0 - valLoss: 0.6666892170906067 - trainLoss: 0.665014386177063\n",
      "cnt: 0 - valLoss: 0.6666889190673828 - trainLoss: 0.6650140881538391\n",
      "cnt: 0 - valLoss: 0.6666887402534485 - trainLoss: 0.6650137901306152\n",
      "cnt: 0 - valLoss: 0.6666884422302246 - trainLoss: 0.6650134921073914\n",
      "cnt: 0 - valLoss: 0.6666882038116455 - trainLoss: 0.6650131940841675\n",
      "cnt: 0 - valLoss: 0.6666879653930664 - trainLoss: 0.6650129556655884\n",
      "cnt: 0 - valLoss: 0.6666877269744873 - trainLoss: 0.6650126576423645\n",
      "cnt: 0 - valLoss: 0.6666874289512634 - trainLoss: 0.6650123596191406\n",
      "cnt: 0 - valLoss: 0.6666871905326843 - trainLoss: 0.6650121212005615\n",
      "cnt: 0 - valLoss: 0.6666869521141052 - trainLoss: 0.6650118231773376\n",
      "cnt: 0 - valLoss: 0.6666867733001709 - trainLoss: 0.6650115251541138\n",
      "cnt: 0 - valLoss: 0.666686475276947 - trainLoss: 0.6650112867355347\n",
      "cnt: 0 - valLoss: 0.6666862368583679 - trainLoss: 0.6650109887123108\n",
      "cnt: 0 - valLoss: 0.6666859984397888 - trainLoss: 0.6650106906890869\n",
      "cnt: 0 - valLoss: 0.6666857004165649 - trainLoss: 0.6650104522705078\n",
      "cnt: 0 - valLoss: 0.6666854619979858 - trainLoss: 0.6650101542472839\n",
      "cnt: 0 - valLoss: 0.6666852235794067 - trainLoss: 0.6650098562240601\n",
      "cnt: 0 - valLoss: 0.6666849851608276 - trainLoss: 0.6650095582008362\n",
      "cnt: 0 - valLoss: 0.6666848063468933 - trainLoss: 0.6650093197822571\n",
      "cnt: 0 - valLoss: 0.6666844487190247 - trainLoss: 0.6650090217590332\n",
      "cnt: 0 - valLoss: 0.6666842103004456 - trainLoss: 0.6650087833404541\n",
      "cnt: 0 - valLoss: 0.6666840314865112 - trainLoss: 0.6650084257125854\n",
      "cnt: 0 - valLoss: 0.6666837334632874 - trainLoss: 0.6650081872940063\n",
      "cnt: 0 - valLoss: 0.6666834950447083 - trainLoss: 0.6650078892707825\n",
      "cnt: 0 - valLoss: 0.6666832566261292 - trainLoss: 0.6650075912475586\n",
      "cnt: 0 - valLoss: 0.66668301820755 - trainLoss: 0.6650073528289795\n",
      "cnt: 0 - valLoss: 0.6666827201843262 - trainLoss: 0.6650070548057556\n",
      "cnt: 0 - valLoss: 0.6666825413703918 - trainLoss: 0.6650068163871765\n",
      "cnt: 0 - valLoss: 0.666682243347168 - trainLoss: 0.6650065183639526\n",
      "cnt: 0 - valLoss: 0.6666820645332336 - trainLoss: 0.6650062799453735\n",
      "cnt: 0 - valLoss: 0.6666817665100098 - trainLoss: 0.6650059223175049\n",
      "cnt: 0 - valLoss: 0.6666815280914307 - trainLoss: 0.665005624294281\n",
      "cnt: 0 - valLoss: 0.6666812896728516 - trainLoss: 0.6650054454803467\n",
      "cnt: 0 - valLoss: 0.6666809916496277 - trainLoss: 0.665005087852478\n",
      "cnt: 0 - valLoss: 0.6666808128356934 - trainLoss: 0.6650048494338989\n",
      "cnt: 0 - valLoss: 0.6666805148124695 - trainLoss: 0.665004551410675\n",
      "cnt: 0 - valLoss: 0.6666803359985352 - trainLoss: 0.6650042533874512\n",
      "cnt: 0 - valLoss: 0.6666800379753113 - trainLoss: 0.6650040149688721\n",
      "cnt: 0 - valLoss: 0.6666797399520874 - trainLoss: 0.6650037169456482\n",
      "cnt: 0 - valLoss: 0.6666795611381531 - trainLoss: 0.6650034189224243\n",
      "cnt: 0 - valLoss: 0.666679322719574 - trainLoss: 0.6650031805038452\n",
      "cnt: 0 - valLoss: 0.6666790843009949 - trainLoss: 0.6650028824806213\n",
      "cnt: 0 - valLoss: 0.6666788458824158 - trainLoss: 0.6650025844573975\n",
      "cnt: 0 - valLoss: 0.6666785478591919 - trainLoss: 0.6650023460388184\n",
      "cnt: 0 - valLoss: 0.6666783690452576 - trainLoss: 0.6650021076202393\n",
      "cnt: 0 - valLoss: 0.6666780710220337 - trainLoss: 0.6650018095970154\n",
      "cnt: 0 - valLoss: 0.6666778326034546 - trainLoss: 0.6650015115737915\n",
      "cnt: 0 - valLoss: 0.6666775941848755 - trainLoss: 0.6650012135505676\n",
      "cnt: 0 - valLoss: 0.6666772961616516 - trainLoss: 0.6650009751319885\n",
      "cnt: 0 - valLoss: 0.6666771173477173 - trainLoss: 0.6650006175041199\n",
      "cnt: 0 - valLoss: 0.6666768789291382 - trainLoss: 0.6650004386901855\n",
      "cnt: 0 - valLoss: 0.6666766405105591 - trainLoss: 0.6650001406669617\n",
      "cnt: 0 - valLoss: 0.66667640209198 - trainLoss: 0.6649998426437378\n",
      "cnt: 0 - valLoss: 0.6666761040687561 - trainLoss: 0.6649996042251587\n",
      "cnt: 0 - valLoss: 0.666675865650177 - trainLoss: 0.6649993062019348\n",
      "cnt: 0 - valLoss: 0.6666756272315979 - trainLoss: 0.6649990081787109\n",
      "cnt: 0 - valLoss: 0.666675329208374 - trainLoss: 0.6649987697601318\n",
      "cnt: 0 - valLoss: 0.6666751503944397 - trainLoss: 0.664998471736908\n",
      "cnt: 0 - valLoss: 0.6666749119758606 - trainLoss: 0.6649982333183289\n",
      "cnt: 0 - valLoss: 0.6666746735572815 - trainLoss: 0.6649978756904602\n",
      "cnt: 0 - valLoss: 0.6666744351387024 - trainLoss: 0.6649976968765259\n",
      "cnt: 0 - valLoss: 0.6666741967201233 - trainLoss: 0.664997398853302\n",
      "cnt: 0 - valLoss: 0.6666739583015442 - trainLoss: 0.6649971008300781\n",
      "cnt: 0 - valLoss: 0.6666737198829651 - trainLoss: 0.664996862411499\n",
      "cnt: 0 - valLoss: 0.6666734218597412 - trainLoss: 0.6649966239929199\n",
      "cnt: 0 - valLoss: 0.6666731834411621 - trainLoss: 0.664996325969696\n",
      "cnt: 0 - valLoss: 0.666672945022583 - trainLoss: 0.6649960279464722\n",
      "cnt: 0 - valLoss: 0.6666727662086487 - trainLoss: 0.6649957299232483\n",
      "cnt: 0 - valLoss: 0.6666724681854248 - trainLoss: 0.6649954915046692\n",
      "cnt: 0 - valLoss: 0.6666722893714905 - trainLoss: 0.6649952530860901\n",
      "cnt: 0 - valLoss: 0.6666719913482666 - trainLoss: 0.6649949550628662\n",
      "cnt: 0 - valLoss: 0.6666717529296875 - trainLoss: 0.6649946570396423\n",
      "cnt: 0 - valLoss: 0.6666715145111084 - trainLoss: 0.6649944186210632\n",
      "cnt: 0 - valLoss: 0.6666712760925293 - trainLoss: 0.6649941205978394\n",
      "cnt: 0 - valLoss: 0.6666710376739502 - trainLoss: 0.6649938225746155\n",
      "cnt: 0 - valLoss: 0.6666707396507263 - trainLoss: 0.6649935841560364\n",
      "cnt: 0 - valLoss: 0.6666705012321472 - trainLoss: 0.6649932861328125\n",
      "cnt: 0 - valLoss: 0.6666703224182129 - trainLoss: 0.6649930477142334\n",
      "cnt: 0 - valLoss: 0.6666700839996338 - trainLoss: 0.6649927496910095\n",
      "cnt: 0 - valLoss: 0.6666698455810547 - trainLoss: 0.6649925112724304\n",
      "cnt: 0 - valLoss: 0.6666695475578308 - trainLoss: 0.6649922132492065\n",
      "cnt: 0 - valLoss: 0.6666693091392517 - trainLoss: 0.6649919748306274\n",
      "cnt: 0 - valLoss: 0.6666690707206726 - trainLoss: 0.6649916172027588\n",
      "cnt: 0 - valLoss: 0.6666688323020935 - trainLoss: 0.6649913787841797\n",
      "cnt: 0 - valLoss: 0.6666686534881592 - trainLoss: 0.6649911403656006\n",
      "cnt: 0 - valLoss: 0.6666682958602905 - trainLoss: 0.6649908423423767\n",
      "cnt: 0 - valLoss: 0.6666680574417114 - trainLoss: 0.6649906039237976\n",
      "cnt: 0 - valLoss: 0.6666678786277771 - trainLoss: 0.6649903655052185\n",
      "cnt: 0 - valLoss: 0.666667640209198 - trainLoss: 0.6649901270866394\n",
      "cnt: 0 - valLoss: 0.6666674017906189 - trainLoss: 0.6649897694587708\n",
      "cnt: 0 - valLoss: 0.6666671633720398 - trainLoss: 0.6649894714355469\n",
      "cnt: 0 - valLoss: 0.6666669249534607 - trainLoss: 0.6649892330169678\n",
      "cnt: 0 - valLoss: 0.6666666269302368 - trainLoss: 0.6649890542030334\n",
      "cnt: 0 - valLoss: 0.6666663885116577 - trainLoss: 0.6649887561798096\n",
      "cnt: 0 - valLoss: 0.6666662096977234 - trainLoss: 0.6649884581565857\n",
      "cnt: 0 - valLoss: 0.6666659712791443 - trainLoss: 0.6649882197380066\n",
      "cnt: 0 - valLoss: 0.6666657328605652 - trainLoss: 0.6649879217147827\n",
      "cnt: 0 - valLoss: 0.6666654348373413 - trainLoss: 0.6649876236915588\n",
      "cnt: 0 - valLoss: 0.666665256023407 - trainLoss: 0.6649874448776245\n",
      "cnt: 0 - valLoss: 0.6666649580001831 - trainLoss: 0.6649870872497559\n",
      "cnt: 0 - valLoss: 0.666664719581604 - trainLoss: 0.6649868488311768\n",
      "cnt: 0 - valLoss: 0.6666644811630249 - trainLoss: 0.6649866104125977\n",
      "cnt: 0 - valLoss: 0.6666643023490906 - trainLoss: 0.6649863123893738\n",
      "cnt: 0 - valLoss: 0.6666640639305115 - trainLoss: 0.6649860143661499\n",
      "cnt: 0 - valLoss: 0.6666637659072876 - trainLoss: 0.6649857759475708\n",
      "cnt: 0 - valLoss: 0.6666635870933533 - trainLoss: 0.6649855375289917\n",
      "cnt: 0 - valLoss: 0.6666633486747742 - trainLoss: 0.6649852395057678\n",
      "cnt: 0 - valLoss: 0.6666630506515503 - trainLoss: 0.6649850010871887\n",
      "cnt: 0 - valLoss: 0.6666628122329712 - trainLoss: 0.6649847030639648\n",
      "cnt: 0 - valLoss: 0.6666626334190369 - trainLoss: 0.6649844646453857\n",
      "cnt: 0 - valLoss: 0.6666623950004578 - trainLoss: 0.6649841666221619\n",
      "cnt: 0 - valLoss: 0.6666620969772339 - trainLoss: 0.6649839282035828\n",
      "cnt: 0 - valLoss: 0.6666618585586548 - trainLoss: 0.6649836897850037\n",
      "cnt: 0 - valLoss: 0.6666616201400757 - trainLoss: 0.6649833917617798\n",
      "cnt: 0 - valLoss: 0.6666614413261414 - trainLoss: 0.6649830937385559\n",
      "cnt: 0 - valLoss: 0.6666611433029175 - trainLoss: 0.6649828553199768\n",
      "cnt: 0 - valLoss: 0.6666609048843384 - trainLoss: 0.6649826169013977\n",
      "cnt: 0 - valLoss: 0.6666606664657593 - trainLoss: 0.6649823188781738\n",
      "cnt: 0 - valLoss: 0.6666604280471802 - trainLoss: 0.6649820804595947\n",
      "cnt: 0 - valLoss: 0.6666601896286011 - trainLoss: 0.6649817824363708\n",
      "cnt: 0 - valLoss: 0.666659951210022 - trainLoss: 0.664981484413147\n",
      "cnt: 0 - valLoss: 0.6666597127914429 - trainLoss: 0.6649813055992126\n",
      "cnt: 0 - valLoss: 0.6666594743728638 - trainLoss: 0.6649810075759888\n",
      "cnt: 0 - valLoss: 0.6666592359542847 - trainLoss: 0.6649807691574097\n",
      "cnt: 0 - valLoss: 0.6666589975357056 - trainLoss: 0.6649804711341858\n",
      "cnt: 0 - valLoss: 0.6666588187217712 - trainLoss: 0.6649802327156067\n",
      "cnt: 0 - valLoss: 0.6666585206985474 - trainLoss: 0.6649799346923828\n",
      "cnt: 0 - valLoss: 0.6666582822799683 - trainLoss: 0.6649796962738037\n",
      "cnt: 0 - valLoss: 0.6666581034660339 - trainLoss: 0.6649793982505798\n",
      "cnt: 0 - valLoss: 0.6666578054428101 - trainLoss: 0.6649791598320007\n",
      "cnt: 0 - valLoss: 0.6666576266288757 - trainLoss: 0.6649789214134216\n",
      "cnt: 0 - valLoss: 0.6666573286056519 - trainLoss: 0.6649786233901978\n",
      "cnt: 0 - valLoss: 0.6666570901870728 - trainLoss: 0.6649783849716187\n",
      "cnt: 0 - valLoss: 0.6666568517684937 - trainLoss: 0.6649780869483948\n",
      "cnt: 0 - valLoss: 0.6666566133499146 - trainLoss: 0.6649777889251709\n",
      "cnt: 0 - valLoss: 0.6666564345359802 - trainLoss: 0.6649776101112366\n",
      "cnt: 0 - valLoss: 0.6666561961174011 - trainLoss: 0.6649773716926575\n",
      "cnt: 0 - valLoss: 0.666655957698822 - trainLoss: 0.6649770736694336\n",
      "cnt: 0 - valLoss: 0.6666557788848877 - trainLoss: 0.6649768352508545\n",
      "cnt: 0 - valLoss: 0.6666554808616638 - trainLoss: 0.6649764776229858\n",
      "cnt: 0 - valLoss: 0.6666552424430847 - trainLoss: 0.6649762988090515\n",
      "cnt: 0 - valLoss: 0.6666550636291504 - trainLoss: 0.6649760007858276\n",
      "cnt: 0 - valLoss: 0.6666548252105713 - trainLoss: 0.6649757623672485\n",
      "cnt: 0 - valLoss: 0.6666545867919922 - trainLoss: 0.6649755239486694\n",
      "cnt: 0 - valLoss: 0.6666543483734131 - trainLoss: 0.6649752259254456\n",
      "cnt: 0 - valLoss: 0.666654109954834 - trainLoss: 0.6649749875068665\n",
      "cnt: 0 - valLoss: 0.6666539311408997 - trainLoss: 0.6649747490882874\n",
      "cnt: 0 - valLoss: 0.6666536927223206 - trainLoss: 0.6649744510650635\n",
      "cnt: 0 - valLoss: 0.6666534543037415 - trainLoss: 0.6649742126464844\n",
      "cnt: 0 - valLoss: 0.6666532158851624 - trainLoss: 0.6649739742279053\n",
      "cnt: 0 - valLoss: 0.6666529774665833 - trainLoss: 0.6649736762046814\n",
      "cnt: 0 - valLoss: 0.6666527390480042 - trainLoss: 0.6649734377861023\n",
      "cnt: 0 - valLoss: 0.6666525602340698 - trainLoss: 0.6649731397628784\n",
      "cnt: 0 - valLoss: 0.6666523218154907 - trainLoss: 0.6649729609489441\n",
      "cnt: 0 - valLoss: 0.6666520237922668 - trainLoss: 0.6649726629257202\n",
      "cnt: 0 - valLoss: 0.6666518449783325 - trainLoss: 0.6649723649024963\n",
      "cnt: 0 - valLoss: 0.6666516065597534 - trainLoss: 0.664972186088562\n",
      "cnt: 0 - valLoss: 0.6666513681411743 - trainLoss: 0.6649718880653381\n",
      "cnt: 0 - valLoss: 0.6666511297225952 - trainLoss: 0.664971649646759\n",
      "cnt: 0 - valLoss: 0.6666509509086609 - trainLoss: 0.6649714112281799\n",
      "cnt: 0 - valLoss: 0.666650652885437 - trainLoss: 0.664971113204956\n",
      "cnt: 0 - valLoss: 0.6666504740715027 - trainLoss: 0.6649708151817322\n",
      "cnt: 0 - valLoss: 0.6666502356529236 - trainLoss: 0.6649705767631531\n",
      "cnt: 0 - valLoss: 0.6666499972343445 - trainLoss: 0.664970338344574\n",
      "cnt: 0 - valLoss: 0.6666496992111206 - trainLoss: 0.6649700403213501\n",
      "cnt: 0 - valLoss: 0.6666494607925415 - trainLoss: 0.664969801902771\n",
      "cnt: 0 - valLoss: 0.6666492223739624 - trainLoss: 0.6649695038795471\n",
      "cnt: 0 - valLoss: 0.6666490435600281 - trainLoss: 0.6649693250656128\n",
      "cnt: 0 - valLoss: 0.666648805141449 - trainLoss: 0.6649689674377441\n",
      "cnt: 0 - valLoss: 0.6666485071182251 - trainLoss: 0.6649687886238098\n",
      "cnt: 0 - valLoss: 0.6666483283042908 - trainLoss: 0.6649685502052307\n",
      "cnt: 0 - valLoss: 0.6666480898857117 - trainLoss: 0.6649682521820068\n",
      "cnt: 0 - valLoss: 0.6666478514671326 - trainLoss: 0.6649680137634277\n",
      "cnt: 0 - valLoss: 0.6666475534439087 - trainLoss: 0.6649677753448486\n",
      "cnt: 0 - valLoss: 0.6666474342346191 - trainLoss: 0.6649674773216248\n",
      "cnt: 0 - valLoss: 0.6666471362113953 - trainLoss: 0.6649672389030457\n",
      "cnt: 0 - valLoss: 0.6666468977928162 - trainLoss: 0.6649670004844666\n",
      "cnt: 0 - valLoss: 0.6666466593742371 - trainLoss: 0.6649667024612427\n",
      "cnt: 0 - valLoss: 0.6666464805603027 - trainLoss: 0.6649664640426636\n",
      "cnt: 0 - valLoss: 0.6666461825370789 - trainLoss: 0.6649662256240845\n",
      "cnt: 0 - valLoss: 0.6666460037231445 - trainLoss: 0.6649659276008606\n",
      "cnt: 0 - valLoss: 0.6666457653045654 - trainLoss: 0.6649656891822815\n",
      "cnt: 0 - valLoss: 0.6666455864906311 - trainLoss: 0.6649654507637024\n",
      "cnt: 0 - valLoss: 0.6666452884674072 - trainLoss: 0.6649652123451233\n",
      "cnt: 0 - valLoss: 0.6666451096534729 - trainLoss: 0.6649649143218994\n",
      "cnt: 0 - valLoss: 0.666644811630249 - trainLoss: 0.6649646162986755\n",
      "cnt: 0 - valLoss: 0.6666445732116699 - trainLoss: 0.6649644374847412\n",
      "cnt: 0 - valLoss: 0.6666443347930908 - trainLoss: 0.6649641394615173\n",
      "cnt: 0 - valLoss: 0.6666440963745117 - trainLoss: 0.6649639010429382\n",
      "cnt: 0 - valLoss: 0.6666439175605774 - trainLoss: 0.6649636626243591\n",
      "cnt: 0 - valLoss: 0.6666436791419983 - trainLoss: 0.6649633646011353\n",
      "cnt: 0 - valLoss: 0.6666434407234192 - trainLoss: 0.6649631261825562\n",
      "cnt: 0 - valLoss: 0.6666432023048401 - trainLoss: 0.664962887763977\n",
      "cnt: 0 - valLoss: 0.666642963886261 - trainLoss: 0.6649625897407532\n",
      "cnt: 0 - valLoss: 0.6666427254676819 - trainLoss: 0.6649623513221741\n",
      "cnt: 0 - valLoss: 0.6666425466537476 - trainLoss: 0.664962112903595\n",
      "cnt: 0 - valLoss: 0.6666422486305237 - trainLoss: 0.6649618744850159\n",
      "cnt: 0 - valLoss: 0.6666420698165894 - trainLoss: 0.664961576461792\n",
      "cnt: 0 - valLoss: 0.6666417717933655 - trainLoss: 0.6649613380432129\n",
      "cnt: 0 - valLoss: 0.6666415929794312 - trainLoss: 0.6649610996246338\n",
      "cnt: 0 - valLoss: 0.666641354560852 - trainLoss: 0.6649608016014099\n",
      "cnt: 0 - valLoss: 0.666641116142273 - trainLoss: 0.6649605631828308\n",
      "cnt: 0 - valLoss: 0.6666408777236938 - trainLoss: 0.6649603247642517\n",
      "cnt: 0 - valLoss: 0.6666406393051147 - trainLoss: 0.6649600863456726\n",
      "cnt: 0 - valLoss: 0.6666404008865356 - trainLoss: 0.6649597883224487\n",
      "cnt: 0 - valLoss: 0.6666402220726013 - trainLoss: 0.6649595499038696\n",
      "cnt: 0 - valLoss: 0.6666399836540222 - trainLoss: 0.6649592518806458\n",
      "cnt: 0 - valLoss: 0.6666397452354431 - trainLoss: 0.6649590730667114\n",
      "cnt: 0 - valLoss: 0.666639506816864 - trainLoss: 0.6649588346481323\n",
      "cnt: 0 - valLoss: 0.6666392683982849 - trainLoss: 0.6649585962295532\n",
      "cnt: 0 - valLoss: 0.6666390299797058 - trainLoss: 0.6649582386016846\n",
      "cnt: 0 - valLoss: 0.6666388511657715 - trainLoss: 0.6649580597877502\n",
      "cnt: 0 - valLoss: 0.6666385531425476 - trainLoss: 0.6649578213691711\n",
      "cnt: 0 - valLoss: 0.6666383147239685 - trainLoss: 0.664957582950592\n",
      "cnt: 0 - valLoss: 0.6666381359100342 - trainLoss: 0.6649572849273682\n",
      "cnt: 0 - valLoss: 0.6666378974914551 - trainLoss: 0.6649570465087891\n",
      "cnt: 0 - valLoss: 0.666637659072876 - trainLoss: 0.6649567484855652\n",
      "cnt: 0 - valLoss: 0.6666373610496521 - trainLoss: 0.6649565696716309\n",
      "cnt: 0 - valLoss: 0.6666371822357178 - trainLoss: 0.6649562120437622\n",
      "cnt: 0 - valLoss: 0.6666369438171387 - trainLoss: 0.6649560332298279\n",
      "cnt: 0 - valLoss: 0.6666367053985596 - trainLoss: 0.664955735206604\n",
      "cnt: 0 - valLoss: 0.6666364669799805 - trainLoss: 0.6649554967880249\n",
      "cnt: 0 - valLoss: 0.6666362881660461 - trainLoss: 0.664955198764801\n",
      "cnt: 0 - valLoss: 0.6666359901428223 - trainLoss: 0.6649550199508667\n",
      "cnt: 0 - valLoss: 0.6666358113288879 - trainLoss: 0.6649547219276428\n",
      "cnt: 0 - valLoss: 0.6666355729103088 - trainLoss: 0.6649544835090637\n",
      "cnt: 0 - valLoss: 0.6666353344917297 - trainLoss: 0.6649542450904846\n",
      "cnt: 0 - valLoss: 0.6666351556777954 - trainLoss: 0.6649539470672607\n",
      "cnt: 0 - valLoss: 0.6666348576545715 - trainLoss: 0.6649537086486816\n",
      "cnt: 0 - valLoss: 0.6666346192359924 - trainLoss: 0.6649534702301025\n",
      "cnt: 0 - valLoss: 0.6666344404220581 - trainLoss: 0.6649532318115234\n",
      "cnt: 0 - valLoss: 0.666634202003479 - trainLoss: 0.6649529933929443\n",
      "cnt: 0 - valLoss: 0.6666339039802551 - trainLoss: 0.6649527549743652\n",
      "cnt: 0 - valLoss: 0.6666336059570312 - trainLoss: 0.6649525165557861\n",
      "cnt: 0 - valLoss: 0.6666334271430969 - trainLoss: 0.6649521589279175\n",
      "cnt: 0 - valLoss: 0.6666332483291626 - trainLoss: 0.6649519801139832\n",
      "cnt: 0 - valLoss: 0.6666330099105835 - trainLoss: 0.664951741695404\n",
      "cnt: 0 - valLoss: 0.6666327714920044 - trainLoss: 0.6649514436721802\n",
      "cnt: 0 - valLoss: 0.6666325926780701 - trainLoss: 0.6649512052536011\n",
      "cnt: 0 - valLoss: 0.6666322946548462 - trainLoss: 0.6649509072303772\n",
      "cnt: 0 - valLoss: 0.6666320562362671 - trainLoss: 0.6649506688117981\n",
      "cnt: 0 - valLoss: 0.666631817817688 - trainLoss: 0.664950430393219\n",
      "cnt: 0 - valLoss: 0.6666315793991089 - trainLoss: 0.6649501919746399\n",
      "cnt: 0 - valLoss: 0.6666314005851746 - trainLoss: 0.664949893951416\n",
      "cnt: 0 - valLoss: 0.6666311025619507 - trainLoss: 0.6649497151374817\n",
      "cnt: 0 - valLoss: 0.6666309237480164 - trainLoss: 0.6649494171142578\n",
      "cnt: 0 - valLoss: 0.6666306853294373 - trainLoss: 0.6649491786956787\n",
      "cnt: 0 - valLoss: 0.6666304469108582 - trainLoss: 0.6649489402770996\n",
      "cnt: 0 - valLoss: 0.666630208492279 - trainLoss: 0.6649486422538757\n",
      "cnt: 0 - valLoss: 0.6666299700737 - trainLoss: 0.6649484038352966\n",
      "cnt: 0 - valLoss: 0.6666297316551208 - trainLoss: 0.6649481654167175\n",
      "cnt: 0 - valLoss: 0.6666295528411865 - trainLoss: 0.6649478673934937\n",
      "cnt: 0 - valLoss: 0.6666292548179626 - trainLoss: 0.6649476885795593\n",
      "cnt: 0 - valLoss: 0.6666290760040283 - trainLoss: 0.6649474501609802\n",
      "cnt: 0 - valLoss: 0.6666288375854492 - trainLoss: 0.6649472117424011\n",
      "cnt: 0 - valLoss: 0.6666285991668701 - trainLoss: 0.664946973323822\n",
      "cnt: 0 - valLoss: 0.666628360748291 - trainLoss: 0.6649466753005981\n",
      "cnt: 0 - valLoss: 0.6666281223297119 - trainLoss: 0.664946436882019\n",
      "cnt: 0 - valLoss: 0.6666278839111328 - trainLoss: 0.6649461984634399\n",
      "cnt: 0 - valLoss: 0.6666277050971985 - trainLoss: 0.6649459600448608\n",
      "cnt: 0 - valLoss: 0.6666274070739746 - trainLoss: 0.6649457216262817\n",
      "cnt: 0 - valLoss: 0.6666272878646851 - trainLoss: 0.6649454236030579\n",
      "cnt: 0 - valLoss: 0.6666269898414612 - trainLoss: 0.6649451851844788\n",
      "cnt: 0 - valLoss: 0.6666267514228821 - trainLoss: 0.6649450063705444\n",
      "cnt: 0 - valLoss: 0.666626513004303 - trainLoss: 0.6649447083473206\n",
      "cnt: 0 - valLoss: 0.6666262745857239 - trainLoss: 0.6649444103240967\n",
      "cnt: 0 - valLoss: 0.6666260361671448 - trainLoss: 0.6649441123008728\n",
      "cnt: 0 - valLoss: 0.6666257977485657 - trainLoss: 0.6649439334869385\n",
      "cnt: 0 - valLoss: 0.6666256189346313 - trainLoss: 0.6649436950683594\n",
      "cnt: 0 - valLoss: 0.6666253805160522 - trainLoss: 0.6649434566497803\n",
      "cnt: 0 - valLoss: 0.6666251420974731 - trainLoss: 0.6649431586265564\n",
      "cnt: 0 - valLoss: 0.666624903678894 - trainLoss: 0.6649429798126221\n",
      "cnt: 0 - valLoss: 0.6666246652603149 - trainLoss: 0.6649426817893982\n",
      "cnt: 0 - valLoss: 0.6666244268417358 - trainLoss: 0.6649424433708191\n",
      "cnt: 0 - valLoss: 0.6666241884231567 - trainLoss: 0.66494220495224\n",
      "cnt: 0 - valLoss: 0.6666239500045776 - trainLoss: 0.6649419665336609\n",
      "cnt: 0 - valLoss: 0.6666237711906433 - trainLoss: 0.6649417281150818\n",
      "cnt: 0 - valLoss: 0.6666235327720642 - trainLoss: 0.6649414300918579\n",
      "cnt: 0 - valLoss: 0.6666232943534851 - trainLoss: 0.6649411916732788\n",
      "cnt: 0 - valLoss: 0.666623055934906 - trainLoss: 0.6649409532546997\n",
      "cnt: 0 - valLoss: 0.6666228175163269 - trainLoss: 0.6649407148361206\n",
      "cnt: 0 - valLoss: 0.6666226387023926 - trainLoss: 0.6649404764175415\n",
      "cnt: 0 - valLoss: 0.6666224002838135 - trainLoss: 0.6649402976036072\n",
      "cnt: 0 - valLoss: 0.6666221618652344 - trainLoss: 0.6649399995803833\n",
      "cnt: 0 - valLoss: 0.6666219234466553 - trainLoss: 0.6649397015571594\n",
      "cnt: 0 - valLoss: 0.6666216850280762 - trainLoss: 0.6649394631385803\n",
      "cnt: 0 - valLoss: 0.6666214466094971 - trainLoss: 0.6649392247200012\n",
      "cnt: 0 - valLoss: 0.666621208190918 - trainLoss: 0.6649389863014221\n",
      "cnt: 0 - valLoss: 0.6666210293769836 - trainLoss: 0.6649388074874878\n",
      "cnt: 0 - valLoss: 0.6666207313537598 - trainLoss: 0.6649384498596191\n",
      "cnt: 0 - valLoss: 0.6666204929351807 - trainLoss: 0.6649382710456848\n",
      "cnt: 0 - valLoss: 0.6666203737258911 - trainLoss: 0.6649380326271057\n",
      "cnt: 0 - valLoss: 0.6666200757026672 - trainLoss: 0.6649377942085266\n",
      "cnt: 0 - valLoss: 0.6666198372840881 - trainLoss: 0.6649374961853027\n",
      "cnt: 0 - valLoss: 0.6666196584701538 - trainLoss: 0.6649372577667236\n",
      "cnt: 0 - valLoss: 0.6666194200515747 - trainLoss: 0.6649370193481445\n",
      "cnt: 0 - valLoss: 0.6666191816329956 - trainLoss: 0.6649367809295654\n",
      "cnt: 0 - valLoss: 0.6666188836097717 - trainLoss: 0.6649364829063416\n",
      "cnt: 0 - valLoss: 0.6666187047958374 - trainLoss: 0.664936363697052\n",
      "cnt: 0 - valLoss: 0.6666184663772583 - trainLoss: 0.6649360656738281\n",
      "cnt: 0 - valLoss: 0.666618287563324 - trainLoss: 0.664935827255249\n",
      "cnt: 0 - valLoss: 0.6666179895401001 - trainLoss: 0.6649355888366699\n",
      "cnt: 0 - valLoss: 0.6666178107261658 - trainLoss: 0.664935290813446\n",
      "cnt: 0 - valLoss: 0.6666175723075867 - trainLoss: 0.6649350523948669\n",
      "cnt: 0 - valLoss: 0.6666173338890076 - trainLoss: 0.6649348139762878\n",
      "cnt: 0 - valLoss: 0.6666170954704285 - trainLoss: 0.664934515953064\n",
      "cnt: 0 - valLoss: 0.6666167974472046 - trainLoss: 0.6649343371391296\n",
      "cnt: 0 - valLoss: 0.6666166186332703 - trainLoss: 0.6649340987205505\n",
      "cnt: 0 - valLoss: 0.6666164398193359 - trainLoss: 0.6649338603019714\n",
      "cnt: 0 - valLoss: 0.6666162014007568 - trainLoss: 0.6649336218833923\n",
      "cnt: 0 - valLoss: 0.6666159629821777 - trainLoss: 0.6649333238601685\n",
      "cnt: 0 - valLoss: 0.6666156649589539 - trainLoss: 0.6649330854415894\n",
      "cnt: 0 - valLoss: 0.6666154861450195 - trainLoss: 0.6649328470230103\n",
      "cnt: 0 - valLoss: 0.6666152477264404 - trainLoss: 0.6649325489997864\n",
      "cnt: 0 - valLoss: 0.6666150689125061 - trainLoss: 0.6649324297904968\n",
      "cnt: 0 - valLoss: 0.666614830493927 - trainLoss: 0.664932131767273\n",
      "cnt: 0 - valLoss: 0.6666145920753479 - trainLoss: 0.6649318933486938\n",
      "cnt: 0 - valLoss: 0.6666143536567688 - trainLoss: 0.6649316549301147\n",
      "cnt: 0 - valLoss: 0.6666140556335449 - trainLoss: 0.6649314165115356\n",
      "cnt: 0 - valLoss: 0.6666138768196106 - trainLoss: 0.6649311780929565\n",
      "cnt: 0 - valLoss: 0.6666136980056763 - trainLoss: 0.6649308800697327\n",
      "cnt: 0 - valLoss: 0.6666134595870972 - trainLoss: 0.6649307012557983\n",
      "cnt: 0 - valLoss: 0.6666132211685181 - trainLoss: 0.6649304032325745\n",
      "cnt: 0 - valLoss: 0.666612982749939 - trainLoss: 0.6649302244186401\n",
      "cnt: 0 - valLoss: 0.6666128039360046 - trainLoss: 0.664929986000061\n",
      "cnt: 0 - valLoss: 0.6666126251220703 - trainLoss: 0.6649296879768372\n",
      "cnt: 0 - valLoss: 0.6666123270988464 - trainLoss: 0.6649295091629028\n",
      "cnt: 0 - valLoss: 0.6666121482849121 - trainLoss: 0.664929211139679\n",
      "cnt: 0 - valLoss: 0.6666118502616882 - trainLoss: 0.6649289727210999\n",
      "cnt: 0 - valLoss: 0.6666116714477539 - trainLoss: 0.6649287343025208\n",
      "cnt: 0 - valLoss: 0.6666114330291748 - trainLoss: 0.6649285554885864\n",
      "cnt: 0 - valLoss: 0.6666112542152405 - trainLoss: 0.6649282574653625\n",
      "cnt: 0 - valLoss: 0.6666109561920166 - trainLoss: 0.6649280190467834\n",
      "cnt: 0 - valLoss: 0.6666107773780823 - trainLoss: 0.6649278402328491\n",
      "cnt: 0 - valLoss: 0.6666105389595032 - trainLoss: 0.66492760181427\n",
      "cnt: 0 - valLoss: 0.6666103005409241 - trainLoss: 0.6649273633956909\n",
      "cnt: 0 - valLoss: 0.6666101217269897 - trainLoss: 0.664927065372467\n",
      "cnt: 0 - valLoss: 0.6666098833084106 - trainLoss: 0.6649268269538879\n",
      "cnt: 0 - valLoss: 0.6666097044944763 - trainLoss: 0.6649266481399536\n",
      "cnt: 0 - valLoss: 0.6666094064712524 - trainLoss: 0.6649263501167297\n",
      "cnt: 0 - valLoss: 0.6666092276573181 - trainLoss: 0.6649261713027954\n",
      "cnt: 0 - valLoss: 0.666608989238739 - trainLoss: 0.6649258732795715\n",
      "cnt: 0 - valLoss: 0.6666088104248047 - trainLoss: 0.6649256348609924\n",
      "cnt: 0 - valLoss: 0.6666085720062256 - trainLoss: 0.6649253964424133\n",
      "cnt: 0 - valLoss: 0.6666083335876465 - trainLoss: 0.6649251580238342\n",
      "cnt: 0 - valLoss: 0.6666080951690674 - trainLoss: 0.6649249792098999\n",
      "cnt: 0 - valLoss: 0.6666079163551331 - trainLoss: 0.664924681186676\n",
      "cnt: 0 - valLoss: 0.6666076183319092 - trainLoss: 0.6649244427680969\n",
      "cnt: 0 - valLoss: 0.6666074395179749 - trainLoss: 0.6649242639541626\n",
      "cnt: 0 - valLoss: 0.6666072010993958 - trainLoss: 0.6649240255355835\n",
      "cnt: 0 - valLoss: 0.6666069626808167 - trainLoss: 0.6649237275123596\n",
      "cnt: 0 - valLoss: 0.6666067838668823 - trainLoss: 0.6649234890937805\n",
      "cnt: 0 - valLoss: 0.6666065454483032 - trainLoss: 0.6649233102798462\n",
      "cnt: 0 - valLoss: 0.6666063666343689 - trainLoss: 0.6649230718612671\n",
      "cnt: 0 - valLoss: 0.6666061282157898 - trainLoss: 0.664922833442688\n",
      "cnt: 0 - valLoss: 0.6666058897972107 - trainLoss: 0.6649225950241089\n",
      "cnt: 0 - valLoss: 0.6666056513786316 - trainLoss: 0.6649223566055298\n",
      "cnt: 0 - valLoss: 0.6666054725646973 - trainLoss: 0.6649220585823059\n",
      "cnt: 0 - valLoss: 0.6666052341461182 - trainLoss: 0.6649218797683716\n",
      "cnt: 0 - valLoss: 0.6666049957275391 - trainLoss: 0.6649216413497925\n",
      "cnt: 0 - valLoss: 0.6666048169136047 - trainLoss: 0.6649213433265686\n",
      "cnt: 0 - valLoss: 0.6666045784950256 - trainLoss: 0.6649211645126343\n",
      "cnt: 0 - valLoss: 0.6666043996810913 - trainLoss: 0.6649209260940552\n",
      "cnt: 0 - valLoss: 0.6666041016578674 - trainLoss: 0.6649206876754761\n",
      "cnt: 0 - valLoss: 0.6666038632392883 - trainLoss: 0.6649203896522522\n",
      "cnt: 0 - valLoss: 0.6666036248207092 - trainLoss: 0.6649202108383179\n",
      "cnt: 0 - valLoss: 0.6666034460067749 - trainLoss: 0.6649199724197388\n",
      "cnt: 0 - valLoss: 0.6666032075881958 - trainLoss: 0.6649197340011597\n",
      "cnt: 0 - valLoss: 0.6666030287742615 - trainLoss: 0.6649194955825806\n",
      "cnt: 0 - valLoss: 0.6666027903556824 - trainLoss: 0.6649192571640015\n",
      "cnt: 0 - valLoss: 0.666602611541748 - trainLoss: 0.6649190783500671\n",
      "cnt: 0 - valLoss: 0.666602373123169 - trainLoss: 0.6649187803268433\n",
      "cnt: 0 - valLoss: 0.6666021347045898 - trainLoss: 0.6649186015129089\n",
      "cnt: 0 - valLoss: 0.6666019558906555 - trainLoss: 0.6649183630943298\n",
      "cnt: 0 - valLoss: 0.6666016578674316 - trainLoss: 0.6649181246757507\n",
      "cnt: 0 - valLoss: 0.6666014790534973 - trainLoss: 0.6649178266525269\n",
      "cnt: 0 - valLoss: 0.666601300239563 - trainLoss: 0.6649175882339478\n",
      "cnt: 0 - valLoss: 0.6666010022163391 - trainLoss: 0.6649174094200134\n",
      "cnt: 0 - valLoss: 0.6666008234024048 - trainLoss: 0.6649171710014343\n",
      "cnt: 0 - valLoss: 0.6666005849838257 - trainLoss: 0.6649169325828552\n",
      "cnt: 0 - valLoss: 0.6666004061698914 - trainLoss: 0.6649167537689209\n",
      "cnt: 0 - valLoss: 0.6666001081466675 - trainLoss: 0.664916455745697\n",
      "cnt: 0 - valLoss: 0.6665999293327332 - trainLoss: 0.6649162173271179\n",
      "cnt: 0 - valLoss: 0.6665996313095093 - trainLoss: 0.6649159789085388\n",
      "cnt: 0 - valLoss: 0.6665995121002197 - trainLoss: 0.6649157404899597\n",
      "cnt: 0 - valLoss: 0.6665992140769958 - trainLoss: 0.6649155020713806\n",
      "cnt: 0 - valLoss: 0.6665990352630615 - trainLoss: 0.6649152636528015\n",
      "cnt: 0 - valLoss: 0.6665987968444824 - trainLoss: 0.6649150252342224\n",
      "cnt: 0 - valLoss: 0.6665985584259033 - trainLoss: 0.6649147868156433\n",
      "cnt: 0 - valLoss: 0.666598379611969 - trainLoss: 0.664914608001709\n",
      "cnt: 0 - valLoss: 0.6665981411933899 - trainLoss: 0.6649143099784851\n",
      "cnt: 0 - valLoss: 0.6665979027748108 - trainLoss: 0.6649141311645508\n",
      "cnt: 0 - valLoss: 0.6665976643562317 - trainLoss: 0.6649138927459717\n",
      "cnt: 0 - valLoss: 0.6665974855422974 - trainLoss: 0.6649136543273926\n",
      "cnt: 0 - valLoss: 0.6665972471237183 - trainLoss: 0.6649134159088135\n",
      "cnt: 0 - valLoss: 0.6665970683097839 - trainLoss: 0.6649132370948792\n",
      "cnt: 0 - valLoss: 0.6665967702865601 - trainLoss: 0.6649129390716553\n",
      "cnt: 0 - valLoss: 0.6665965914726257 - trainLoss: 0.6649127006530762\n",
      "cnt: 0 - valLoss: 0.6665964126586914 - trainLoss: 0.6649125218391418\n",
      "cnt: 0 - valLoss: 0.6665961742401123 - trainLoss: 0.6649122834205627\n",
      "cnt: 0 - valLoss: 0.6665959358215332 - trainLoss: 0.6649120450019836\n",
      "cnt: 0 - valLoss: 0.6665956974029541 - trainLoss: 0.6649118065834045\n",
      "cnt: 0 - valLoss: 0.6665955185890198 - trainLoss: 0.6649115085601807\n",
      "cnt: 0 - valLoss: 0.6665952801704407 - trainLoss: 0.6649113297462463\n",
      "cnt: 0 - valLoss: 0.6665951013565063 - trainLoss: 0.664911150932312\n",
      "cnt: 0 - valLoss: 0.6665948033332825 - trainLoss: 0.6649108529090881\n",
      "cnt: 0 - valLoss: 0.6665946245193481 - trainLoss: 0.6649106740951538\n",
      "cnt: 0 - valLoss: 0.6665944457054138 - trainLoss: 0.6649104356765747\n",
      "cnt: 0 - valLoss: 0.6665942072868347 - trainLoss: 0.6649101972579956\n",
      "cnt: 0 - valLoss: 0.6665939688682556 - trainLoss: 0.6649099588394165\n",
      "cnt: 0 - valLoss: 0.6665937304496765 - trainLoss: 0.6649097204208374\n",
      "cnt: 0 - valLoss: 0.6665934920310974 - trainLoss: 0.6649094820022583\n",
      "cnt: 0 - valLoss: 0.6665933132171631 - trainLoss: 0.664909303188324\n",
      "cnt: 0 - valLoss: 0.666593074798584 - trainLoss: 0.6649090647697449\n",
      "cnt: 0 - valLoss: 0.6665928959846497 - trainLoss: 0.6649088263511658\n",
      "cnt: 0 - valLoss: 0.6665926575660706 - trainLoss: 0.6649085879325867\n",
      "cnt: 0 - valLoss: 0.6665924191474915 - trainLoss: 0.6649083495140076\n",
      "cnt: 0 - valLoss: 0.6665921807289124 - trainLoss: 0.6649081110954285\n",
      "cnt: 0 - valLoss: 0.666592001914978 - trainLoss: 0.6649079322814941\n",
      "cnt: 0 - valLoss: 0.6665917634963989 - trainLoss: 0.6649076342582703\n",
      "cnt: 0 - valLoss: 0.6665915846824646 - trainLoss: 0.6649073958396912\n",
      "cnt: 0 - valLoss: 0.6665913462638855 - trainLoss: 0.6649072170257568\n",
      "cnt: 0 - valLoss: 0.6665911078453064 - trainLoss: 0.6649069786071777\n",
      "cnt: 0 - valLoss: 0.6665909290313721 - trainLoss: 0.6649067401885986\n",
      "cnt: 0 - valLoss: 0.6665906310081482 - trainLoss: 0.6649065613746643\n",
      "cnt: 0 - valLoss: 0.6665905117988586 - trainLoss: 0.6649063229560852\n",
      "cnt: 0 - valLoss: 0.6665902137756348 - trainLoss: 0.6649060845375061\n",
      "cnt: 0 - valLoss: 0.6665900349617004 - trainLoss: 0.664905846118927\n",
      "cnt: 0 - valLoss: 0.6665897965431213 - trainLoss: 0.6649056077003479\n",
      "cnt: 0 - valLoss: 0.6665895581245422 - trainLoss: 0.6649053692817688\n",
      "cnt: 0 - valLoss: 0.6665893793106079 - trainLoss: 0.6649051904678345\n",
      "cnt: 0 - valLoss: 0.6665891408920288 - trainLoss: 0.6649049520492554\n",
      "cnt: 0 - valLoss: 0.6665889620780945 - trainLoss: 0.6649047136306763\n",
      "cnt: 0 - valLoss: 0.6665887236595154 - trainLoss: 0.6649044752120972\n",
      "cnt: 0 - valLoss: 0.6665884852409363 - trainLoss: 0.6649042367935181\n",
      "cnt: 0 - valLoss: 0.6665882468223572 - trainLoss: 0.6649040579795837\n",
      "cnt: 0 - valLoss: 0.6665880680084229 - trainLoss: 0.6649038195610046\n",
      "cnt: 0 - valLoss: 0.6665878295898438 - trainLoss: 0.6649035811424255\n",
      "cnt: 0 - valLoss: 0.6665876507759094 - trainLoss: 0.6649033427238464\n",
      "cnt: 0 - valLoss: 0.6665874123573303 - trainLoss: 0.6649031043052673\n",
      "cnt: 0 - valLoss: 0.6665871739387512 - trainLoss: 0.6649028658866882\n",
      "cnt: 0 - valLoss: 0.6665869951248169 - trainLoss: 0.6649026274681091\n",
      "cnt: 0 - valLoss: 0.666586697101593 - trainLoss: 0.6649024486541748\n",
      "cnt: 0 - valLoss: 0.6665865182876587 - trainLoss: 0.6649022102355957\n",
      "cnt: 0 - valLoss: 0.6665863394737244 - trainLoss: 0.6649020314216614\n",
      "cnt: 0 - valLoss: 0.66658616065979 - trainLoss: 0.6649017930030823\n",
      "cnt: 0 - valLoss: 0.6665859222412109 - trainLoss: 0.6649014949798584\n",
      "cnt: 0 - valLoss: 0.6665856838226318 - trainLoss: 0.6649013161659241\n",
      "cnt: 0 - valLoss: 0.6665854454040527 - trainLoss: 0.664901077747345\n",
      "cnt: 0 - valLoss: 0.6665852665901184 - trainLoss: 0.6649008393287659\n",
      "cnt: 0 - valLoss: 0.6665850281715393 - trainLoss: 0.6649006605148315\n",
      "cnt: 0 - valLoss: 0.666584849357605 - trainLoss: 0.6649003624916077\n",
      "cnt: 0 - valLoss: 0.6665846109390259 - trainLoss: 0.6649002432823181\n",
      "cnt: 0 - valLoss: 0.6665843725204468 - trainLoss: 0.6648999452590942\n",
      "cnt: 0 - valLoss: 0.6665841341018677 - trainLoss: 0.6648997664451599\n",
      "cnt: 0 - valLoss: 0.6665839552879333 - trainLoss: 0.6648995280265808\n",
      "cnt: 0 - valLoss: 0.666583776473999 - trainLoss: 0.6648992896080017\n",
      "cnt: 0 - valLoss: 0.6665835380554199 - trainLoss: 0.6648991107940674\n",
      "cnt: 0 - valLoss: 0.6665833592414856 - trainLoss: 0.6648988723754883\n",
      "cnt: 0 - valLoss: 0.6665831208229065 - trainLoss: 0.664898693561554\n",
      "cnt: 0 - valLoss: 0.6665828824043274 - trainLoss: 0.6648983955383301\n",
      "cnt: 0 - valLoss: 0.6665827035903931 - trainLoss: 0.664898157119751\n",
      "cnt: 0 - valLoss: 0.6665825247764587 - trainLoss: 0.6648979187011719\n",
      "cnt: 0 - valLoss: 0.6665822863578796 - trainLoss: 0.6648977398872375\n",
      "cnt: 0 - valLoss: 0.6665821075439453 - trainLoss: 0.6648975014686584\n",
      "cnt: 0 - valLoss: 0.6665818691253662 - trainLoss: 0.6648973226547241\n",
      "cnt: 0 - valLoss: 0.6665816307067871 - trainLoss: 0.664897084236145\n",
      "cnt: 0 - valLoss: 0.666581392288208 - trainLoss: 0.6648967862129211\n",
      "cnt: 0 - valLoss: 0.6665811538696289 - trainLoss: 0.6648966073989868\n",
      "cnt: 0 - valLoss: 0.6665809750556946 - trainLoss: 0.6648964285850525\n",
      "cnt: 0 - valLoss: 0.6665807962417603 - trainLoss: 0.6648961901664734\n",
      "cnt: 0 - valLoss: 0.6665805578231812 - trainLoss: 0.6648959517478943\n",
      "cnt: 0 - valLoss: 0.6665803790092468 - trainLoss: 0.66489577293396\n",
      "cnt: 0 - valLoss: 0.6665801405906677 - trainLoss: 0.6648955345153809\n",
      "cnt: 0 - valLoss: 0.6665799617767334 - trainLoss: 0.664895236492157\n",
      "cnt: 0 - valLoss: 0.6665797233581543 - trainLoss: 0.6648950576782227\n",
      "cnt: 0 - valLoss: 0.6665794849395752 - trainLoss: 0.6648948192596436\n",
      "cnt: 0 - valLoss: 0.6665793657302856 - trainLoss: 0.6648945808410645\n",
      "cnt: 0 - valLoss: 0.6665790677070618 - trainLoss: 0.6648943424224854\n",
      "cnt: 0 - valLoss: 0.6665788888931274 - trainLoss: 0.664894163608551\n",
      "cnt: 0 - valLoss: 0.6665786504745483 - trainLoss: 0.6648939251899719\n",
      "cnt: 0 - valLoss: 0.666578471660614 - trainLoss: 0.6648936867713928\n",
      "cnt: 0 - valLoss: 0.6665782332420349 - trainLoss: 0.6648935079574585\n",
      "cnt: 0 - valLoss: 0.6665780544281006 - trainLoss: 0.6648932695388794\n",
      "cnt: 0 - valLoss: 0.6665778756141663 - trainLoss: 0.6648930907249451\n",
      "cnt: 0 - valLoss: 0.6665775775909424 - trainLoss: 0.664892852306366\n",
      "cnt: 0 - valLoss: 0.6665773987770081 - trainLoss: 0.6648926138877869\n",
      "cnt: 0 - valLoss: 0.6665772199630737 - trainLoss: 0.6648923754692078\n",
      "cnt: 0 - valLoss: 0.6665770411491394 - trainLoss: 0.6648921966552734\n",
      "cnt: 0 - valLoss: 0.6665767431259155 - trainLoss: 0.6648918986320496\n",
      "cnt: 0 - valLoss: 0.6665765643119812 - trainLoss: 0.6648917198181152\n",
      "cnt: 0 - valLoss: 0.6665763258934021 - trainLoss: 0.6648915410041809\n",
      "cnt: 0 - valLoss: 0.6665761470794678 - trainLoss: 0.6648913025856018\n",
      "cnt: 0 - valLoss: 0.6665759086608887 - trainLoss: 0.6648910641670227\n",
      "cnt: 0 - valLoss: 0.6665756702423096 - trainLoss: 0.6648908257484436\n",
      "cnt: 0 - valLoss: 0.66657555103302 - trainLoss: 0.6648906469345093\n",
      "cnt: 0 - valLoss: 0.6665753126144409 - trainLoss: 0.6648903489112854\n",
      "cnt: 0 - valLoss: 0.6665750741958618 - trainLoss: 0.6648901700973511\n",
      "cnt: 0 - valLoss: 0.6665748953819275 - trainLoss: 0.6648899912834167\n",
      "cnt: 0 - valLoss: 0.6665746569633484 - trainLoss: 0.6648897528648376\n",
      "cnt: 0 - valLoss: 0.6665744781494141 - trainLoss: 0.6648895144462585\n",
      "cnt: 0 - valLoss: 0.6665741801261902 - trainLoss: 0.6648892760276794\n",
      "cnt: 0 - valLoss: 0.6665740609169006 - trainLoss: 0.6648890972137451\n",
      "cnt: 0 - valLoss: 0.6665738224983215 - trainLoss: 0.6648889183998108\n",
      "cnt: 0 - valLoss: 0.6665735840797424 - trainLoss: 0.6648886799812317\n",
      "cnt: 0 - valLoss: 0.6665734052658081 - trainLoss: 0.6648884415626526\n",
      "cnt: 0 - valLoss: 0.666573166847229 - trainLoss: 0.6648882031440735\n",
      "cnt: 0 - valLoss: 0.6665729284286499 - trainLoss: 0.6648879647254944\n",
      "cnt: 0 - valLoss: 0.6665728092193604 - trainLoss: 0.6648877859115601\n",
      "cnt: 0 - valLoss: 0.6665725708007812 - trainLoss: 0.664887547492981\n",
      "cnt: 0 - valLoss: 0.6665723323822021 - trainLoss: 0.6648873686790466\n",
      "cnt: 0 - valLoss: 0.666572093963623 - trainLoss: 0.6648871302604675\n",
      "cnt: 0 - valLoss: 0.6665719151496887 - trainLoss: 0.6648868322372437\n",
      "cnt: 0 - valLoss: 0.6665717363357544 - trainLoss: 0.6648867130279541\n",
      "cnt: 0 - valLoss: 0.6665714979171753 - trainLoss: 0.6648864150047302\n",
      "cnt: 0 - valLoss: 0.6665712594985962 - trainLoss: 0.6648862361907959\n",
      "cnt: 0 - valLoss: 0.6665710806846619 - trainLoss: 0.6648860573768616\n",
      "cnt: 0 - valLoss: 0.6665708422660828 - trainLoss: 0.6648858189582825\n",
      "cnt: 0 - valLoss: 0.6665706634521484 - trainLoss: 0.6648855805397034\n",
      "cnt: 0 - valLoss: 0.6665704250335693 - trainLoss: 0.664885401725769\n",
      "cnt: 0 - valLoss: 0.6665701866149902 - trainLoss: 0.6648851633071899\n",
      "cnt: 0 - valLoss: 0.6665700078010559 - trainLoss: 0.6648849844932556\n",
      "cnt: 0 - valLoss: 0.6665698289871216 - trainLoss: 0.6648847460746765\n",
      "cnt: 0 - valLoss: 0.6665695905685425 - trainLoss: 0.6648844480514526\n",
      "cnt: 0 - valLoss: 0.6665694117546082 - trainLoss: 0.6648842692375183\n",
      "cnt: 0 - valLoss: 0.6665691137313843 - trainLoss: 0.664884090423584\n",
      "cnt: 0 - valLoss: 0.6665689945220947 - trainLoss: 0.6648838520050049\n",
      "cnt: 0 - valLoss: 0.6665687561035156 - trainLoss: 0.6648836135864258\n",
      "cnt: 0 - valLoss: 0.6665685176849365 - trainLoss: 0.6648834347724915\n",
      "cnt: 0 - valLoss: 0.666568398475647 - trainLoss: 0.6648831963539124\n",
      "cnt: 0 - valLoss: 0.6665681600570679 - trainLoss: 0.664883017539978\n",
      "cnt: 0 - valLoss: 0.6665679216384888 - trainLoss: 0.6648827791213989\n",
      "cnt: 0 - valLoss: 0.6665676832199097 - trainLoss: 0.6648825407028198\n",
      "cnt: 0 - valLoss: 0.6665675044059753 - trainLoss: 0.6648823022842407\n",
      "cnt: 0 - valLoss: 0.666567325592041 - trainLoss: 0.6648821234703064\n",
      "cnt: 0 - valLoss: 0.6665670871734619 - trainLoss: 0.6648818850517273\n",
      "cnt: 0 - valLoss: 0.6665668487548828 - trainLoss: 0.664881706237793\n",
      "cnt: 0 - valLoss: 0.6665666699409485 - trainLoss: 0.6648814678192139\n",
      "cnt: 0 - valLoss: 0.6665664315223694 - trainLoss: 0.6648812294006348\n",
      "cnt: 0 - valLoss: 0.6665662527084351 - trainLoss: 0.6648810505867004\n",
      "cnt: 0 - valLoss: 0.6665660738945007 - trainLoss: 0.6648807525634766\n",
      "cnt: 0 - valLoss: 0.6665657758712769 - trainLoss: 0.664880633354187\n",
      "cnt: 0 - valLoss: 0.6665655970573425 - trainLoss: 0.6648803353309631\n",
      "cnt: 0 - valLoss: 0.666565477848053 - trainLoss: 0.6648801565170288\n",
      "cnt: 0 - valLoss: 0.6665651798248291 - trainLoss: 0.6648799180984497\n",
      "cnt: 0 - valLoss: 0.6665650010108948 - trainLoss: 0.6648796796798706\n",
      "cnt: 0 - valLoss: 0.6665647625923157 - trainLoss: 0.6648795008659363\n",
      "cnt: 0 - valLoss: 0.6665645241737366 - trainLoss: 0.664879322052002\n",
      "cnt: 0 - valLoss: 0.6665643453598022 - trainLoss: 0.6648790836334229\n",
      "cnt: 0 - valLoss: 0.6665641665458679 - trainLoss: 0.6648788452148438\n",
      "cnt: 0 - valLoss: 0.6665639281272888 - trainLoss: 0.6648786664009094\n",
      "cnt: 0 - valLoss: 0.6665637493133545 - trainLoss: 0.6648784279823303\n",
      "cnt: 0 - valLoss: 0.6665635108947754 - trainLoss: 0.6648781895637512\n",
      "cnt: 0 - valLoss: 0.6665633320808411 - trainLoss: 0.6648779511451721\n",
      "cnt: 0 - valLoss: 0.666563093662262 - trainLoss: 0.6648777723312378\n",
      "cnt: 0 - valLoss: 0.6665628552436829 - trainLoss: 0.6648775339126587\n",
      "cnt: 0 - valLoss: 0.6665626764297485 - trainLoss: 0.6648773550987244\n",
      "cnt: 0 - valLoss: 0.6665624380111694 - trainLoss: 0.6648771166801453\n",
      "cnt: 0 - valLoss: 0.6665622591972351 - trainLoss: 0.6648769378662109\n",
      "cnt: 0 - valLoss: 0.666562020778656 - trainLoss: 0.6648766994476318\n",
      "cnt: 0 - valLoss: 0.6665618419647217 - trainLoss: 0.6648765206336975\n",
      "cnt: 0 - valLoss: 0.6665616035461426 - trainLoss: 0.6648762822151184\n",
      "cnt: 0 - valLoss: 0.6665614247322083 - trainLoss: 0.6648761034011841\n",
      "cnt: 0 - valLoss: 0.6665611863136292 - trainLoss: 0.6648758053779602\n",
      "cnt: 0 - valLoss: 0.6665610074996948 - trainLoss: 0.6648756265640259\n",
      "cnt: 0 - valLoss: 0.6665607690811157 - trainLoss: 0.6648753881454468\n",
      "cnt: 0 - valLoss: 0.6665605902671814 - trainLoss: 0.6648751497268677\n",
      "cnt: 0 - valLoss: 0.6665604114532471 - trainLoss: 0.6648750305175781\n",
      "cnt: 0 - valLoss: 0.666560173034668 - trainLoss: 0.664874792098999\n",
      "cnt: 0 - valLoss: 0.6665599942207336 - trainLoss: 0.6648745536804199\n",
      "cnt: 0 - valLoss: 0.6665596961975098 - trainLoss: 0.6648743748664856\n",
      "cnt: 0 - valLoss: 0.6665595173835754 - trainLoss: 0.6648741960525513\n",
      "cnt: 0 - valLoss: 0.6665593385696411 - trainLoss: 0.6648739576339722\n",
      "cnt: 0 - valLoss: 0.6665591597557068 - trainLoss: 0.6648736596107483\n",
      "cnt: 0 - valLoss: 0.6665589213371277 - trainLoss: 0.664873480796814\n",
      "cnt: 0 - valLoss: 0.6665586829185486 - trainLoss: 0.6648733019828796\n",
      "cnt: 0 - valLoss: 0.6665585041046143 - trainLoss: 0.6648730635643005\n",
      "cnt: 0 - valLoss: 0.6665583252906799 - trainLoss: 0.6648728847503662\n",
      "cnt: 0 - valLoss: 0.6665581464767456 - trainLoss: 0.6648726463317871\n",
      "cnt: 0 - valLoss: 0.6665579080581665 - trainLoss: 0.664872407913208\n",
      "cnt: 0 - valLoss: 0.6665577292442322 - trainLoss: 0.6648722290992737\n",
      "cnt: 0 - valLoss: 0.6665574908256531 - trainLoss: 0.6648720502853394\n",
      "cnt: 0 - valLoss: 0.6665573120117188 - trainLoss: 0.6648718118667603\n",
      "cnt: 0 - valLoss: 0.6665570735931396 - trainLoss: 0.6648716330528259\n",
      "cnt: 0 - valLoss: 0.6665569543838501 - trainLoss: 0.6648714542388916\n",
      "cnt: 0 - valLoss: 0.6665567755699158 - trainLoss: 0.6648712754249573\n",
      "cnt: 0 - valLoss: 0.6665565967559814 - trainLoss: 0.6648710370063782\n",
      "cnt: 0 - valLoss: 0.6665563583374023 - trainLoss: 0.6648708581924438\n",
      "cnt: 0 - valLoss: 0.666556179523468 - trainLoss: 0.6648706197738647\n",
      "cnt: 0 - valLoss: 0.6665560007095337 - trainLoss: 0.6648704409599304\n",
      "cnt: 0 - valLoss: 0.6665558218955994 - trainLoss: 0.6648702621459961\n",
      "cnt: 0 - valLoss: 0.6665557026863098 - trainLoss: 0.664870023727417\n",
      "cnt: 0 - valLoss: 0.6665554642677307 - trainLoss: 0.6648698449134827\n",
      "cnt: 0 - valLoss: 0.6665553450584412 - trainLoss: 0.6648696660995483\n",
      "cnt: 0 - valLoss: 0.6665551066398621 - trainLoss: 0.664869487285614\n",
      "cnt: 0 - valLoss: 0.6665549278259277 - trainLoss: 0.6648692488670349\n",
      "cnt: 0 - valLoss: 0.6665547490119934 - trainLoss: 0.6648690700531006\n",
      "cnt: 0 - valLoss: 0.6665545701980591 - trainLoss: 0.6648688912391663\n",
      "cnt: 0 - valLoss: 0.66655433177948 - trainLoss: 0.6648686528205872\n",
      "cnt: 0 - valLoss: 0.6665542125701904 - trainLoss: 0.6648684740066528\n",
      "cnt: 0 - valLoss: 0.6665540337562561 - trainLoss: 0.6648683547973633\n",
      "cnt: 0 - valLoss: 0.666553795337677 - trainLoss: 0.6648681163787842\n",
      "cnt: 0 - valLoss: 0.6665536761283875 - trainLoss: 0.6648679375648499\n",
      "cnt: 0 - valLoss: 0.6665534973144531 - trainLoss: 0.6648676991462708\n",
      "cnt: 0 - valLoss: 0.666553258895874 - trainLoss: 0.6648675203323364\n",
      "cnt: 0 - valLoss: 0.6665531396865845 - trainLoss: 0.6648673415184021\n",
      "cnt: 0 - valLoss: 0.6665529608726501 - trainLoss: 0.6648671627044678\n",
      "cnt: 0 - valLoss: 0.6665527820587158 - trainLoss: 0.6648669838905334\n",
      "cnt: 0 - valLoss: 0.6665526032447815 - trainLoss: 0.6648667454719543\n",
      "cnt: 0 - valLoss: 0.6665523648262024 - trainLoss: 0.66486656665802\n",
      "cnt: 0 - valLoss: 0.6665521860122681 - trainLoss: 0.6648663878440857\n",
      "cnt: 0 - valLoss: 0.6665520071983337 - trainLoss: 0.6648662090301514\n",
      "cnt: 0 - valLoss: 0.6665518283843994 - trainLoss: 0.6648659706115723\n",
      "cnt: 0 - valLoss: 0.6665516495704651 - trainLoss: 0.6648658514022827\n",
      "cnt: 0 - valLoss: 0.6665514707565308 - trainLoss: 0.6648656129837036\n",
      "cnt: 0 - valLoss: 0.6665512919425964 - trainLoss: 0.6648654341697693\n",
      "cnt: 0 - valLoss: 0.6665511131286621 - trainLoss: 0.6648651957511902\n",
      "cnt: 0 - valLoss: 0.6665509343147278 - trainLoss: 0.6648650169372559\n",
      "cnt: 0 - valLoss: 0.6665506958961487 - trainLoss: 0.6648648381233215\n",
      "cnt: 0 - valLoss: 0.6665505170822144 - trainLoss: 0.6648646593093872\n",
      "cnt: 0 - valLoss: 0.6665503978729248 - trainLoss: 0.6648644208908081\n",
      "cnt: 0 - valLoss: 0.6665502190589905 - trainLoss: 0.6648642420768738\n",
      "cnt: 0 - valLoss: 0.6665500402450562 - trainLoss: 0.6648640632629395\n",
      "cnt: 0 - valLoss: 0.6665498614311218 - trainLoss: 0.6648639440536499\n",
      "cnt: 0 - valLoss: 0.6665496826171875 - trainLoss: 0.6648637056350708\n",
      "cnt: 0 - valLoss: 0.6665494441986084 - trainLoss: 0.6648635268211365\n",
      "cnt: 0 - valLoss: 0.6665493249893188 - trainLoss: 0.6648633480072021\n",
      "cnt: 0 - valLoss: 0.6665490865707397 - trainLoss: 0.6648631691932678\n",
      "cnt: 0 - valLoss: 0.6665489673614502 - trainLoss: 0.6648629903793335\n",
      "cnt: 0 - valLoss: 0.6665487289428711 - trainLoss: 0.6648627519607544\n",
      "cnt: 0 - valLoss: 0.6665485501289368 - trainLoss: 0.6648625731468201\n",
      "cnt: 0 - valLoss: 0.6665483713150024 - trainLoss: 0.6648623943328857\n",
      "cnt: 0 - valLoss: 0.6665481925010681 - trainLoss: 0.6648621559143066\n",
      "cnt: 0 - valLoss: 0.6665480732917786 - trainLoss: 0.6648619771003723\n",
      "cnt: 0 - valLoss: 0.6665478348731995 - trainLoss: 0.664861798286438\n",
      "cnt: 0 - valLoss: 0.6665475964546204 - trainLoss: 0.6648616194725037\n",
      "cnt: 0 - valLoss: 0.6665474772453308 - trainLoss: 0.6648614406585693\n",
      "cnt: 0 - valLoss: 0.6665472984313965 - trainLoss: 0.6648612022399902\n",
      "cnt: 0 - valLoss: 0.6665471196174622 - trainLoss: 0.6648610234260559\n",
      "cnt: 0 - valLoss: 0.6665469408035278 - trainLoss: 0.6648608446121216\n",
      "cnt: 0 - valLoss: 0.6665467023849487 - trainLoss: 0.6648606657981873\n",
      "cnt: 0 - valLoss: 0.6665465831756592 - trainLoss: 0.6648604273796082\n",
      "cnt: 0 - valLoss: 0.6665464043617249 - trainLoss: 0.6648603081703186\n",
      "cnt: 0 - valLoss: 0.6665462255477905 - trainLoss: 0.6648600697517395\n",
      "cnt: 0 - valLoss: 0.6665459871292114 - trainLoss: 0.6648598909378052\n",
      "cnt: 0 - valLoss: 0.6665459275245667 - trainLoss: 0.6648597121238708\n",
      "cnt: 0 - valLoss: 0.6665456295013428 - trainLoss: 0.6648595333099365\n",
      "cnt: 0 - valLoss: 0.6665455102920532 - trainLoss: 0.6648593544960022\n",
      "cnt: 0 - valLoss: 0.6665453314781189 - trainLoss: 0.6648591160774231\n",
      "cnt: 0 - valLoss: 0.6665451526641846 - trainLoss: 0.6648589372634888\n",
      "cnt: 0 - valLoss: 0.6665449142456055 - trainLoss: 0.6648587584495544\n",
      "cnt: 0 - valLoss: 0.6665447354316711 - trainLoss: 0.6648586392402649\n",
      "cnt: 0 - valLoss: 0.6665445566177368 - trainLoss: 0.6648584008216858\n",
      "cnt: 0 - valLoss: 0.6665444374084473 - trainLoss: 0.6648581624031067\n",
      "cnt: 0 - valLoss: 0.6665442585945129 - trainLoss: 0.6648579835891724\n",
      "cnt: 0 - valLoss: 0.6665440201759338 - trainLoss: 0.664857804775238\n",
      "cnt: 0 - valLoss: 0.6665438413619995 - trainLoss: 0.6648576259613037\n",
      "cnt: 0 - valLoss: 0.6665436625480652 - trainLoss: 0.6648574471473694\n",
      "cnt: 0 - valLoss: 0.6665434837341309 - trainLoss: 0.6648572683334351\n",
      "cnt: 0 - valLoss: 0.6665433645248413 - trainLoss: 0.6648570895195007\n",
      "cnt: 0 - valLoss: 0.666543185710907 - trainLoss: 0.6648568511009216\n",
      "cnt: 0 - valLoss: 0.6665429472923279 - trainLoss: 0.6648567318916321\n",
      "cnt: 0 - valLoss: 0.6665427684783936 - trainLoss: 0.664856493473053\n",
      "cnt: 0 - valLoss: 0.6665425896644592 - trainLoss: 0.6648563146591187\n",
      "cnt: 0 - valLoss: 0.6665424108505249 - trainLoss: 0.6648561954498291\n",
      "cnt: 0 - valLoss: 0.6665422916412354 - trainLoss: 0.66485595703125\n",
      "cnt: 0 - valLoss: 0.6665420532226562 - trainLoss: 0.6648557186126709\n",
      "cnt: 0 - valLoss: 0.6665418744087219 - trainLoss: 0.6648555994033813\n",
      "cnt: 0 - valLoss: 0.6665416955947876 - trainLoss: 0.6648553609848022\n",
      "cnt: 0 - valLoss: 0.6665415167808533 - trainLoss: 0.6648551821708679\n",
      "cnt: 0 - valLoss: 0.666541337966919 - trainLoss: 0.6648550033569336\n",
      "cnt: 0 - valLoss: 0.6665411591529846 - trainLoss: 0.6648548245429993\n",
      "cnt: 0 - valLoss: 0.6665409803390503 - trainLoss: 0.6648546457290649\n",
      "cnt: 0 - valLoss: 0.666540801525116 - trainLoss: 0.6648544073104858\n",
      "cnt: 0 - valLoss: 0.6665406227111816 - trainLoss: 0.6648542284965515\n",
      "cnt: 0 - valLoss: 0.6665404438972473 - trainLoss: 0.6648540496826172\n",
      "cnt: 0 - valLoss: 0.666540265083313 - trainLoss: 0.6648538112640381\n",
      "cnt: 0 - valLoss: 0.6665400862693787 - trainLoss: 0.6648536324501038\n",
      "cnt: 0 - valLoss: 0.6665398478507996 - trainLoss: 0.6648535132408142\n",
      "cnt: 0 - valLoss: 0.66653972864151 - trainLoss: 0.6648533344268799\n",
      "cnt: 0 - valLoss: 0.6665395498275757 - trainLoss: 0.6648531556129456\n",
      "cnt: 0 - valLoss: 0.6665393710136414 - trainLoss: 0.6648529767990112\n",
      "cnt: 0 - valLoss: 0.666539192199707 - trainLoss: 0.6648527383804321\n",
      "cnt: 0 - valLoss: 0.6665390133857727 - trainLoss: 0.6648525595664978\n",
      "cnt: 0 - valLoss: 0.6665388345718384 - trainLoss: 0.6648524403572083\n",
      "cnt: 0 - valLoss: 0.666538655757904 - trainLoss: 0.6648522019386292\n",
      "cnt: 0 - valLoss: 0.6665384769439697 - trainLoss: 0.6648520231246948\n",
      "cnt: 0 - valLoss: 0.6665382981300354 - trainLoss: 0.6648518443107605\n",
      "cnt: 0 - valLoss: 0.6665381193161011 - trainLoss: 0.6648516654968262\n",
      "cnt: 0 - valLoss: 0.6665380001068115 - trainLoss: 0.6648514270782471\n",
      "cnt: 0 - valLoss: 0.6665377616882324 - trainLoss: 0.6648513078689575\n",
      "cnt: 0 - valLoss: 0.6665375828742981 - trainLoss: 0.6648510694503784\n",
      "cnt: 0 - valLoss: 0.6665374040603638 - trainLoss: 0.6648508906364441\n",
      "cnt: 0 - valLoss: 0.6665372252464294 - trainLoss: 0.6648507118225098\n",
      "cnt: 0 - valLoss: 0.6665371060371399 - trainLoss: 0.6648505926132202\n",
      "cnt: 0 - valLoss: 0.6665369272232056 - trainLoss: 0.6648504137992859\n",
      "cnt: 0 - valLoss: 0.6665367484092712 - trainLoss: 0.664850115776062\n",
      "cnt: 0 - valLoss: 0.6665365099906921 - trainLoss: 0.6648499369621277\n",
      "cnt: 0 - valLoss: 0.6665363311767578 - trainLoss: 0.6648498177528381\n",
      "cnt: 0 - valLoss: 0.6665361523628235 - trainLoss: 0.6648496389389038\n",
      "cnt: 0 - valLoss: 0.6665359735488892 - trainLoss: 0.6648494601249695\n",
      "cnt: 0 - valLoss: 0.6665358543395996 - trainLoss: 0.6648492217063904\n",
      "cnt: 0 - valLoss: 0.6665356159210205 - trainLoss: 0.664849042892456\n",
      "cnt: 0 - valLoss: 0.6665354371070862 - trainLoss: 0.6648489236831665\n",
      "cnt: 0 - valLoss: 0.6665352582931519 - trainLoss: 0.6648486852645874\n",
      "cnt: 0 - valLoss: 0.6665351390838623 - trainLoss: 0.6648485064506531\n",
      "cnt: 0 - valLoss: 0.666534960269928 - trainLoss: 0.664848268032074\n",
      "cnt: 0 - valLoss: 0.6665347814559937 - trainLoss: 0.6648481488227844\n",
      "cnt: 0 - valLoss: 0.6665346026420593 - trainLoss: 0.6648479700088501\n",
      "cnt: 0 - valLoss: 0.6665343642234802 - trainLoss: 0.664847731590271\n",
      "cnt: 0 - valLoss: 0.6665342450141907 - trainLoss: 0.6648475527763367\n",
      "cnt: 0 - valLoss: 0.6665340662002563 - trainLoss: 0.6648474335670471\n",
      "cnt: 0 - valLoss: 0.666533887386322 - trainLoss: 0.6648472547531128\n",
      "cnt: 0 - valLoss: 0.6665337085723877 - trainLoss: 0.6648470759391785\n",
      "cnt: 0 - valLoss: 0.6665335297584534 - trainLoss: 0.6648468375205994\n",
      "cnt: 0 - valLoss: 0.6665334105491638 - trainLoss: 0.664846658706665\n",
      "cnt: 0 - valLoss: 0.6665331721305847 - trainLoss: 0.6648464798927307\n",
      "cnt: 0 - valLoss: 0.6665329933166504 - trainLoss: 0.6648463010787964\n",
      "cnt: 0 - valLoss: 0.6665328145027161 - trainLoss: 0.6648461222648621\n",
      "cnt: 0 - valLoss: 0.6665326952934265 - trainLoss: 0.6648459434509277\n",
      "cnt: 0 - valLoss: 0.6665325164794922 - trainLoss: 0.6648457646369934\n",
      "cnt: 0 - valLoss: 0.6665323376655579 - trainLoss: 0.6648455858230591\n",
      "cnt: 0 - valLoss: 0.6665321588516235 - trainLoss: 0.6648454070091248\n",
      "cnt: 0 - valLoss: 0.6665319204330444 - trainLoss: 0.6648452281951904\n",
      "cnt: 0 - valLoss: 0.6665317416191101 - trainLoss: 0.6648450493812561\n",
      "cnt: 0 - valLoss: 0.6665316224098206 - trainLoss: 0.664844810962677\n",
      "cnt: 0 - valLoss: 0.6665314435958862 - trainLoss: 0.6648446917533875\n",
      "cnt: 0 - valLoss: 0.6665312647819519 - trainLoss: 0.6648445129394531\n",
      "cnt: 0 - valLoss: 0.6665310859680176 - trainLoss: 0.6648443341255188\n",
      "cnt: 0 - valLoss: 0.6665309071540833 - trainLoss: 0.6648441553115845\n",
      "cnt: 0 - valLoss: 0.6665306687355042 - trainLoss: 0.6648439168930054\n",
      "cnt: 0 - valLoss: 0.6665304899215698 - trainLoss: 0.664843738079071\n",
      "cnt: 0 - valLoss: 0.6665303707122803 - trainLoss: 0.6648435592651367\n",
      "cnt: 0 - valLoss: 0.666530191898346 - trainLoss: 0.6648433804512024\n",
      "cnt: 0 - valLoss: 0.6665300726890564 - trainLoss: 0.6648432016372681\n",
      "cnt: 0 - valLoss: 0.6665298342704773 - trainLoss: 0.6648430228233337\n",
      "cnt: 0 - valLoss: 0.6665297150611877 - trainLoss: 0.6648428440093994\n",
      "cnt: 0 - valLoss: 0.6665295362472534 - trainLoss: 0.6648426651954651\n",
      "cnt: 0 - valLoss: 0.6665292978286743 - trainLoss: 0.6648424863815308\n",
      "cnt: 0 - valLoss: 0.66652911901474 - trainLoss: 0.6648423671722412\n",
      "cnt: 0 - valLoss: 0.6665289998054504 - trainLoss: 0.6648421287536621\n",
      "cnt: 0 - valLoss: 0.6665287613868713 - trainLoss: 0.6648419499397278\n",
      "cnt: 0 - valLoss: 0.666528582572937 - trainLoss: 0.6648417711257935\n",
      "cnt: 0 - valLoss: 0.6665284037590027 - trainLoss: 0.6648415923118591\n",
      "cnt: 0 - valLoss: 0.6665282845497131 - trainLoss: 0.6648414134979248\n",
      "cnt: 0 - valLoss: 0.6665281057357788 - trainLoss: 0.6648412346839905\n",
      "cnt: 0 - valLoss: 0.6665279269218445 - trainLoss: 0.6648410558700562\n",
      "cnt: 0 - valLoss: 0.6665277481079102 - trainLoss: 0.6648408770561218\n",
      "cnt: 0 - valLoss: 0.6665275692939758 - trainLoss: 0.6648406982421875\n",
      "cnt: 0 - valLoss: 0.6665273904800415 - trainLoss: 0.6648405194282532\n",
      "cnt: 0 - valLoss: 0.6665271520614624 - trainLoss: 0.6648404002189636\n",
      "cnt: 0 - valLoss: 0.6665270328521729 - trainLoss: 0.6648401618003845\n",
      "cnt: 0 - valLoss: 0.6665268540382385 - trainLoss: 0.6648399829864502\n",
      "cnt: 0 - valLoss: 0.666526734828949 - trainLoss: 0.6648398041725159\n",
      "cnt: 0 - valLoss: 0.6665265560150146 - trainLoss: 0.6648396253585815\n",
      "cnt: 0 - valLoss: 0.6665263772010803 - trainLoss: 0.6648394465446472\n",
      "cnt: 0 - valLoss: 0.6665262579917908 - trainLoss: 0.6648392677307129\n",
      "cnt: 0 - valLoss: 0.6665260195732117 - trainLoss: 0.6648390889167786\n",
      "cnt: 0 - valLoss: 0.6665258407592773 - trainLoss: 0.664838969707489\n",
      "cnt: 0 - valLoss: 0.666525661945343 - trainLoss: 0.6648387312889099\n",
      "cnt: 0 - valLoss: 0.6665254831314087 - trainLoss: 0.6648385524749756\n",
      "cnt: 0 - valLoss: 0.6665253043174744 - trainLoss: 0.6648383736610413\n",
      "cnt: 0 - valLoss: 0.66652512550354 - trainLoss: 0.6648381948471069\n",
      "cnt: 0 - valLoss: 0.6665249466896057 - trainLoss: 0.6648379564285278\n",
      "cnt: 0 - valLoss: 0.6665247082710266 - trainLoss: 0.6648378372192383\n",
      "cnt: 0 - valLoss: 0.6665245890617371 - trainLoss: 0.664837658405304\n",
      "cnt: 0 - valLoss: 0.666524350643158 - trainLoss: 0.6648374199867249\n",
      "cnt: 0 - valLoss: 0.6665241718292236 - trainLoss: 0.6648372411727905\n",
      "cnt: 0 - valLoss: 0.6665239930152893 - trainLoss: 0.6648370623588562\n",
      "cnt: 0 - valLoss: 0.666523814201355 - trainLoss: 0.6648368835449219\n",
      "cnt: 0 - valLoss: 0.6665236353874207 - trainLoss: 0.664836585521698\n",
      "cnt: 0 - valLoss: 0.6665234565734863 - trainLoss: 0.6648364663124084\n",
      "cnt: 0 - valLoss: 0.666523277759552 - trainLoss: 0.6648362874984741\n",
      "cnt: 0 - valLoss: 0.6665230393409729 - trainLoss: 0.664836049079895\n",
      "cnt: 0 - valLoss: 0.6665229201316833 - trainLoss: 0.6648358702659607\n",
      "cnt: 0 - valLoss: 0.666522741317749 - trainLoss: 0.6648356914520264\n",
      "cnt: 0 - valLoss: 0.6665225028991699 - trainLoss: 0.664835512638092\n",
      "cnt: 0 - valLoss: 0.6665223240852356 - trainLoss: 0.6648352742195129\n",
      "cnt: 0 - valLoss: 0.6665221452713013 - trainLoss: 0.6648350954055786\n",
      "cnt: 0 - valLoss: 0.6665219068527222 - trainLoss: 0.6648349165916443\n",
      "cnt: 0 - valLoss: 0.6665217280387878 - trainLoss: 0.66483473777771\n",
      "cnt: 0 - valLoss: 0.6665215492248535 - trainLoss: 0.6648345589637756\n",
      "cnt: 0 - valLoss: 0.6665213704109192 - trainLoss: 0.6648343205451965\n",
      "cnt: 0 - valLoss: 0.6665211915969849 - trainLoss: 0.6648341417312622\n",
      "cnt: 0 - valLoss: 0.6665209531784058 - trainLoss: 0.6648339629173279\n",
      "cnt: 0 - valLoss: 0.6665207743644714 - trainLoss: 0.6648337244987488\n",
      "cnt: 0 - valLoss: 0.6665205955505371 - trainLoss: 0.6648335456848145\n",
      "cnt: 0 - valLoss: 0.6665204167366028 - trainLoss: 0.6648333668708801\n",
      "cnt: 0 - valLoss: 0.6665201783180237 - trainLoss: 0.664833128452301\n",
      "cnt: 0 - valLoss: 0.6665199995040894 - trainLoss: 0.6648329496383667\n",
      "cnt: 0 - valLoss: 0.6665197610855103 - trainLoss: 0.6648327708244324\n",
      "cnt: 0 - valLoss: 0.6665195822715759 - trainLoss: 0.6648324728012085\n",
      "cnt: 0 - valLoss: 0.6665194034576416 - trainLoss: 0.664832353591919\n",
      "cnt: 0 - valLoss: 0.6665192246437073 - trainLoss: 0.6648320555686951\n",
      "cnt: 0 - valLoss: 0.6665189862251282 - trainLoss: 0.6648318767547607\n",
      "cnt: 0 - valLoss: 0.6665188074111938 - trainLoss: 0.6648317575454712\n",
      "cnt: 0 - valLoss: 0.6665186285972595 - trainLoss: 0.6648315191268921\n",
      "cnt: 0 - valLoss: 0.6665184497833252 - trainLoss: 0.664831280708313\n",
      "cnt: 0 - valLoss: 0.6665182709693909 - trainLoss: 0.6648311614990234\n",
      "cnt: 0 - valLoss: 0.6665180325508118 - trainLoss: 0.6648309230804443\n",
      "cnt: 0 - valLoss: 0.6665178537368774 - trainLoss: 0.6648306846618652\n",
      "cnt: 0 - valLoss: 0.6665177345275879 - trainLoss: 0.6648305058479309\n",
      "cnt: 0 - valLoss: 0.6665174961090088 - trainLoss: 0.6648303270339966\n",
      "cnt: 0 - valLoss: 0.6665172576904297 - trainLoss: 0.6648300886154175\n",
      "cnt: 0 - valLoss: 0.6665170788764954 - trainLoss: 0.6648299098014832\n",
      "cnt: 0 - valLoss: 0.666516900062561 - trainLoss: 0.6648297905921936\n",
      "cnt: 0 - valLoss: 0.6665166616439819 - trainLoss: 0.6648294925689697\n",
      "cnt: 0 - valLoss: 0.6665164828300476 - trainLoss: 0.6648293137550354\n",
      "cnt: 0 - valLoss: 0.6665163040161133 - trainLoss: 0.6648291349411011\n",
      "cnt: 0 - valLoss: 0.666516125202179 - trainLoss: 0.664828896522522\n",
      "cnt: 0 - valLoss: 0.6665158867835999 - trainLoss: 0.6648287177085876\n",
      "cnt: 0 - valLoss: 0.6665157079696655 - trainLoss: 0.6648285388946533\n",
      "cnt: 0 - valLoss: 0.666515588760376 - trainLoss: 0.6648283004760742\n",
      "cnt: 0 - valLoss: 0.6665152907371521 - trainLoss: 0.6648281216621399\n",
      "cnt: 0 - valLoss: 0.6665151119232178 - trainLoss: 0.6648279428482056\n",
      "cnt: 0 - valLoss: 0.6665149927139282 - trainLoss: 0.6648277640342712\n",
      "cnt: 0 - valLoss: 0.6665148138999939 - trainLoss: 0.6648275256156921\n",
      "cnt: 0 - valLoss: 0.66651451587677 - trainLoss: 0.6648273468017578\n",
      "cnt: 0 - valLoss: 0.6665143370628357 - trainLoss: 0.6648271679878235\n",
      "cnt: 0 - valLoss: 0.6665141582489014 - trainLoss: 0.6648269295692444\n",
      "cnt: 0 - valLoss: 0.6665140390396118 - trainLoss: 0.6648268103599548\n",
      "cnt: 0 - valLoss: 0.6665138006210327 - trainLoss: 0.6648265719413757\n",
      "cnt: 0 - valLoss: 0.6665136218070984 - trainLoss: 0.6648264527320862\n",
      "cnt: 0 - valLoss: 0.6665134429931641 - trainLoss: 0.6648261547088623\n",
      "cnt: 0 - valLoss: 0.6665131449699402 - trainLoss: 0.664825975894928\n",
      "cnt: 0 - valLoss: 0.6665130257606506 - trainLoss: 0.6648257970809937\n",
      "cnt: 0 - valLoss: 0.6665128469467163 - trainLoss: 0.6648256182670593\n",
      "cnt: 0 - valLoss: 0.666512668132782 - trainLoss: 0.6648253798484802\n",
      "cnt: 0 - valLoss: 0.6665124297142029 - trainLoss: 0.6648252010345459\n",
      "cnt: 0 - valLoss: 0.6665122509002686 - trainLoss: 0.6648250222206116\n",
      "cnt: 0 - valLoss: 0.6665120720863342 - trainLoss: 0.6648247838020325\n",
      "cnt: 0 - valLoss: 0.6665118932723999 - trainLoss: 0.6648246049880981\n",
      "cnt: 0 - valLoss: 0.6665117144584656 - trainLoss: 0.6648244261741638\n",
      "cnt: 0 - valLoss: 0.6665114760398865 - trainLoss: 0.6648242473602295\n",
      "cnt: 0 - valLoss: 0.6665112972259521 - trainLoss: 0.6648240685462952\n",
      "cnt: 0 - valLoss: 0.6665111184120178 - trainLoss: 0.6648238301277161\n",
      "cnt: 0 - valLoss: 0.6665108799934387 - trainLoss: 0.6648236513137817\n",
      "cnt: 0 - valLoss: 0.6665107011795044 - trainLoss: 0.6648234724998474\n",
      "cnt: 0 - valLoss: 0.6665105223655701 - trainLoss: 0.6648232340812683\n",
      "cnt: 0 - valLoss: 0.6665103435516357 - trainLoss: 0.6648231148719788\n",
      "cnt: 0 - valLoss: 0.6665101051330566 - trainLoss: 0.6648228764533997\n",
      "cnt: 0 - valLoss: 0.6665099263191223 - trainLoss: 0.6648226976394653\n",
      "cnt: 0 - valLoss: 0.666509747505188 - trainLoss: 0.6648224592208862\n",
      "cnt: 0 - valLoss: 0.6665095686912537 - trainLoss: 0.6648223400115967\n",
      "cnt: 0 - valLoss: 0.6665093302726746 - trainLoss: 0.6648221015930176\n",
      "cnt: 0 - valLoss: 0.6665091514587402 - trainLoss: 0.6648219227790833\n",
      "cnt: 0 - valLoss: 0.6665089130401611 - trainLoss: 0.6648217439651489\n",
      "cnt: 0 - valLoss: 0.6665087938308716 - trainLoss: 0.6648215055465698\n",
      "cnt: 0 - valLoss: 0.6665086150169373 - trainLoss: 0.6648213863372803\n",
      "cnt: 0 - valLoss: 0.6665084362030029 - trainLoss: 0.6648211479187012\n",
      "cnt: 0 - valLoss: 0.6665081977844238 - trainLoss: 0.6648209691047668\n",
      "cnt: 0 - valLoss: 0.6665080189704895 - trainLoss: 0.6648207902908325\n",
      "cnt: 0 - valLoss: 0.6665077805519104 - trainLoss: 0.6648206114768982\n",
      "cnt: 0 - valLoss: 0.6665076017379761 - trainLoss: 0.6648203730583191\n",
      "cnt: 0 - valLoss: 0.6665074229240417 - trainLoss: 0.66482013463974\n",
      "cnt: 0 - valLoss: 0.6665072441101074 - trainLoss: 0.6648200154304504\n",
      "cnt: 0 - valLoss: 0.6665070056915283 - trainLoss: 0.6648197174072266\n",
      "cnt: 0 - valLoss: 0.666506826877594 - trainLoss: 0.664819598197937\n",
      "cnt: 0 - valLoss: 0.6665066480636597 - trainLoss: 0.6648194193840027\n",
      "cnt: 0 - valLoss: 0.6665064692497253 - trainLoss: 0.6648191809654236\n",
      "cnt: 0 - valLoss: 0.666506290435791 - trainLoss: 0.6648190021514893\n",
      "cnt: 0 - valLoss: 0.6665061116218567 - trainLoss: 0.6648188233375549\n",
      "cnt: 0 - valLoss: 0.6665059328079224 - trainLoss: 0.6648186445236206\n",
      "cnt: 0 - valLoss: 0.6665056943893433 - trainLoss: 0.6648184657096863\n",
      "cnt: 0 - valLoss: 0.6665055155754089 - trainLoss: 0.664818286895752\n",
      "cnt: 0 - valLoss: 0.6665053367614746 - trainLoss: 0.6648180484771729\n",
      "cnt: 0 - valLoss: 0.6665051579475403 - trainLoss: 0.6648179292678833\n",
      "cnt: 0 - valLoss: 0.6665049195289612 - trainLoss: 0.6648176908493042\n",
      "cnt: 0 - valLoss: 0.6665047407150269 - trainLoss: 0.6648175120353699\n",
      "cnt: 0 - valLoss: 0.6665046215057373 - trainLoss: 0.6648173332214355\n",
      "cnt: 0 - valLoss: 0.666504442691803 - trainLoss: 0.6648171544075012\n",
      "cnt: 0 - valLoss: 0.6665042042732239 - trainLoss: 0.6648169755935669\n",
      "cnt: 0 - valLoss: 0.6665039658546448 - trainLoss: 0.6648167371749878\n",
      "cnt: 0 - valLoss: 0.6665038466453552 - trainLoss: 0.6648165583610535\n",
      "cnt: 0 - valLoss: 0.6665036082267761 - trainLoss: 0.6648163795471191\n",
      "cnt: 0 - valLoss: 0.666503369808197 - trainLoss: 0.66481614112854\n",
      "cnt: 0 - valLoss: 0.6665031909942627 - trainLoss: 0.6648159623146057\n",
      "cnt: 0 - valLoss: 0.6665030717849731 - trainLoss: 0.6648157835006714\n",
      "cnt: 0 - valLoss: 0.666502833366394 - trainLoss: 0.6648156642913818\n",
      "cnt: 0 - valLoss: 0.6665027141571045 - trainLoss: 0.664815366268158\n",
      "cnt: 0 - valLoss: 0.6665024757385254 - trainLoss: 0.6648151874542236\n",
      "cnt: 0 - valLoss: 0.6665022373199463 - trainLoss: 0.6648150086402893\n",
      "cnt: 0 - valLoss: 0.666502058506012 - trainLoss: 0.6648148894309998\n",
      "cnt: 0 - valLoss: 0.6665018200874329 - trainLoss: 0.6648145914077759\n",
      "cnt: 0 - valLoss: 0.6665017008781433 - trainLoss: 0.6648144125938416\n",
      "cnt: 0 - valLoss: 0.666501522064209 - trainLoss: 0.664814293384552\n",
      "cnt: 0 - valLoss: 0.6665012836456299 - trainLoss: 0.6648139953613281\n",
      "cnt: 0 - valLoss: 0.6665011048316956 - trainLoss: 0.6648138165473938\n",
      "cnt: 0 - valLoss: 0.6665009260177612 - trainLoss: 0.6648136973381042\n",
      "cnt: 0 - valLoss: 0.6665007472038269 - trainLoss: 0.6648134589195251\n",
      "cnt: 0 - valLoss: 0.6665005683898926 - trainLoss: 0.6648132801055908\n",
      "cnt: 0 - valLoss: 0.6665003299713135 - trainLoss: 0.6648131012916565\n",
      "cnt: 0 - valLoss: 0.6665001511573792 - trainLoss: 0.6648129224777222\n",
      "cnt: 0 - valLoss: 0.6664999723434448 - trainLoss: 0.6648127436637878\n",
      "cnt: 0 - valLoss: 0.6664997935295105 - trainLoss: 0.6648125052452087\n",
      "cnt: 0 - valLoss: 0.6664996147155762 - trainLoss: 0.6648123264312744\n",
      "cnt: 0 - valLoss: 0.6664993762969971 - trainLoss: 0.6648121476173401\n",
      "cnt: 0 - valLoss: 0.6664991974830627 - trainLoss: 0.6648119688034058\n",
      "cnt: 0 - valLoss: 0.6664990186691284 - trainLoss: 0.6648117303848267\n",
      "cnt: 0 - valLoss: 0.6664987802505493 - trainLoss: 0.6648115515708923\n",
      "cnt: 0 - valLoss: 0.666498601436615 - trainLoss: 0.664811372756958\n",
      "cnt: 0 - valLoss: 0.6664983630180359 - trainLoss: 0.6648111939430237\n",
      "cnt: 0 - valLoss: 0.6664982438087463 - trainLoss: 0.6648109555244446\n",
      "cnt: 0 - valLoss: 0.6664980053901672 - trainLoss: 0.664810836315155\n",
      "cnt: 0 - valLoss: 0.6664978265762329 - trainLoss: 0.6648106575012207\n",
      "cnt: 0 - valLoss: 0.6664976477622986 - trainLoss: 0.6648104190826416\n",
      "cnt: 0 - valLoss: 0.6664974689483643 - trainLoss: 0.6648101806640625\n",
      "cnt: 0 - valLoss: 0.6664972901344299 - trainLoss: 0.6648100018501282\n",
      "cnt: 0 - valLoss: 0.6664970517158508 - trainLoss: 0.6648098826408386\n",
      "cnt: 0 - valLoss: 0.666496992111206 - trainLoss: 0.6648096442222595\n",
      "cnt: 0 - valLoss: 0.666496753692627 - trainLoss: 0.6648094654083252\n",
      "cnt: 0 - valLoss: 0.6664965152740479 - trainLoss: 0.6648092865943909\n",
      "cnt: 0 - valLoss: 0.6664963364601135 - trainLoss: 0.6648091077804565\n",
      "cnt: 0 - valLoss: 0.666496217250824 - trainLoss: 0.664808988571167\n",
      "cnt: 0 - valLoss: 0.6664959192276001 - trainLoss: 0.6648086905479431\n",
      "cnt: 0 - valLoss: 0.6664958000183105 - trainLoss: 0.6648085713386536\n",
      "cnt: 0 - valLoss: 0.6664955615997314 - trainLoss: 0.6648083329200745\n",
      "cnt: 0 - valLoss: 0.6664954423904419 - trainLoss: 0.6648082137107849\n",
      "cnt: 0 - valLoss: 0.6664952635765076 - trainLoss: 0.6648079752922058\n",
      "cnt: 0 - valLoss: 0.6664950847625732 - trainLoss: 0.6648077964782715\n",
      "cnt: 0 - valLoss: 0.6664948463439941 - trainLoss: 0.6648076772689819\n",
      "cnt: 0 - valLoss: 0.6664946675300598 - trainLoss: 0.6648074388504028\n",
      "cnt: 0 - valLoss: 0.6664944887161255 - trainLoss: 0.6648072600364685\n",
      "cnt: 0 - valLoss: 0.6664942502975464 - trainLoss: 0.6648070812225342\n",
      "cnt: 0 - valLoss: 0.6664941310882568 - trainLoss: 0.6648069024085999\n",
      "cnt: 0 - valLoss: 0.6664938926696777 - trainLoss: 0.6648066639900208\n",
      "cnt: 0 - valLoss: 0.6664937734603882 - trainLoss: 0.6648065447807312\n",
      "cnt: 0 - valLoss: 0.6664935350418091 - trainLoss: 0.6648063659667969\n",
      "cnt: 0 - valLoss: 0.6664933562278748 - trainLoss: 0.6648061275482178\n",
      "cnt: 0 - valLoss: 0.6664931774139404 - trainLoss: 0.6648059487342834\n",
      "cnt: 0 - valLoss: 0.6664929986000061 - trainLoss: 0.6648057699203491\n",
      "cnt: 0 - valLoss: 0.6664928197860718 - trainLoss: 0.6648056507110596\n",
      "cnt: 0 - valLoss: 0.6664926409721375 - trainLoss: 0.6648054718971252\n",
      "cnt: 0 - valLoss: 0.6664924025535583 - trainLoss: 0.6648052930831909\n",
      "cnt: 0 - valLoss: 0.666492223739624 - trainLoss: 0.664804995059967\n",
      "cnt: 0 - valLoss: 0.6664920449256897 - trainLoss: 0.6648048758506775\n",
      "cnt: 0 - valLoss: 0.6664918661117554 - trainLoss: 0.6648046970367432\n",
      "cnt: 0 - valLoss: 0.6664916276931763 - trainLoss: 0.6648045182228088\n",
      "cnt: 0 - valLoss: 0.6664915084838867 - trainLoss: 0.6648042798042297\n",
      "cnt: 0 - valLoss: 0.6664913296699524 - trainLoss: 0.6648041009902954\n",
      "cnt: 0 - valLoss: 0.6664911508560181 - trainLoss: 0.6648039817810059\n",
      "cnt: 0 - valLoss: 0.666490912437439 - trainLoss: 0.6648038029670715\n",
      "cnt: 0 - valLoss: 0.6664907336235046 - trainLoss: 0.6648035645484924\n",
      "cnt: 0 - valLoss: 0.6664905548095703 - trainLoss: 0.6648033857345581\n",
      "cnt: 0 - valLoss: 0.666490375995636 - trainLoss: 0.6648032069206238\n",
      "cnt: 0 - valLoss: 0.6664901971817017 - trainLoss: 0.6648030281066895\n",
      "cnt: 0 - valLoss: 0.6664900183677673 - trainLoss: 0.6648029088973999\n",
      "cnt: 0 - valLoss: 0.6664897799491882 - trainLoss: 0.664802610874176\n",
      "cnt: 0 - valLoss: 0.6664896607398987 - trainLoss: 0.6648024916648865\n",
      "cnt: 0 - valLoss: 0.6664894223213196 - trainLoss: 0.6648023128509521\n",
      "cnt: 0 - valLoss: 0.6664892435073853 - trainLoss: 0.6648021340370178\n",
      "cnt: 0 - valLoss: 0.6664890050888062 - trainLoss: 0.6648019552230835\n",
      "cnt: 0 - valLoss: 0.6664888858795166 - trainLoss: 0.6648017764091492\n",
      "cnt: 0 - valLoss: 0.6664885878562927 - trainLoss: 0.6648015379905701\n",
      "cnt: 0 - valLoss: 0.6664884686470032 - trainLoss: 0.6648013591766357\n",
      "cnt: 0 - valLoss: 0.6664882302284241 - trainLoss: 0.6648011803627014\n",
      "cnt: 0 - valLoss: 0.6664880514144897 - trainLoss: 0.6648010015487671\n",
      "cnt: 0 - valLoss: 0.6664878726005554 - trainLoss: 0.664800763130188\n",
      "cnt: 0 - valLoss: 0.6664876341819763 - trainLoss: 0.6648005843162537\n",
      "cnt: 0 - valLoss: 0.666487455368042 - trainLoss: 0.6648004055023193\n",
      "cnt: 0 - valLoss: 0.6664872169494629 - trainLoss: 0.664800226688385\n",
      "cnt: 0 - valLoss: 0.6664870381355286 - trainLoss: 0.6648000478744507\n",
      "cnt: 0 - valLoss: 0.6664868593215942 - trainLoss: 0.6647998690605164\n",
      "cnt: 0 - valLoss: 0.6664866209030151 - trainLoss: 0.664799690246582\n",
      "cnt: 0 - valLoss: 0.6664864420890808 - trainLoss: 0.6647994518280029\n",
      "cnt: 0 - valLoss: 0.6664862632751465 - trainLoss: 0.6647992730140686\n",
      "cnt: 0 - valLoss: 0.6664860248565674 - trainLoss: 0.6647990942001343\n",
      "cnt: 0 - valLoss: 0.6664858460426331 - trainLoss: 0.6647988557815552\n",
      "cnt: 0 - valLoss: 0.6664856672286987 - trainLoss: 0.6647987365722656\n",
      "cnt: 0 - valLoss: 0.6664854884147644 - trainLoss: 0.6647984981536865\n",
      "cnt: 0 - valLoss: 0.6664851903915405 - trainLoss: 0.6647983193397522\n",
      "cnt: 0 - valLoss: 0.6664850115776062 - trainLoss: 0.6647982001304626\n",
      "cnt: 0 - valLoss: 0.6664848327636719 - trainLoss: 0.6647979617118835\n",
      "cnt: 0 - valLoss: 0.6664845943450928 - trainLoss: 0.6647977232933044\n",
      "cnt: 0 - valLoss: 0.6664844751358032 - trainLoss: 0.6647976040840149\n",
      "cnt: 0 - valLoss: 0.6664842963218689 - trainLoss: 0.6647974252700806\n",
      "cnt: 0 - valLoss: 0.6664840579032898 - trainLoss: 0.6647972464561462\n",
      "cnt: 0 - valLoss: 0.6664838194847107 - trainLoss: 0.6647970080375671\n",
      "cnt: 0 - valLoss: 0.6664836406707764 - trainLoss: 0.6647968292236328\n",
      "cnt: 0 - valLoss: 0.6664834022521973 - trainLoss: 0.6647966504096985\n",
      "cnt: 0 - valLoss: 0.6664832234382629 - trainLoss: 0.6647964715957642\n",
      "cnt: 0 - valLoss: 0.6664830446243286 - trainLoss: 0.6647962331771851\n",
      "cnt: 0 - valLoss: 0.6664828658103943 - trainLoss: 0.6647960543632507\n",
      "cnt: 0 - valLoss: 0.6664826273918152 - trainLoss: 0.6647959351539612\n",
      "cnt: 0 - valLoss: 0.6664824485778809 - trainLoss: 0.6647956967353821\n",
      "cnt: 0 - valLoss: 0.6664823293685913 - trainLoss: 0.6647955179214478\n",
      "cnt: 0 - valLoss: 0.6664820909500122 - trainLoss: 0.6647953391075134\n",
      "cnt: 0 - valLoss: 0.6664818525314331 - trainLoss: 0.6647951602935791\n",
      "cnt: 0 - valLoss: 0.6664816737174988 - trainLoss: 0.6647949814796448\n",
      "cnt: 0 - valLoss: 0.6664815545082092 - trainLoss: 0.6647948026657104\n",
      "cnt: 0 - valLoss: 0.6664812564849854 - trainLoss: 0.6647946238517761\n",
      "cnt: 0 - valLoss: 0.666481077671051 - trainLoss: 0.6647944450378418\n",
      "cnt: 0 - valLoss: 0.6664808988571167 - trainLoss: 0.6647942662239075\n",
      "cnt: 0 - valLoss: 0.6664807796478271 - trainLoss: 0.6647940874099731\n",
      "cnt: 0 - valLoss: 0.666480541229248 - trainLoss: 0.664793848991394\n",
      "cnt: 0 - valLoss: 0.666480302810669 - trainLoss: 0.6647937297821045\n",
      "cnt: 0 - valLoss: 0.6664801836013794 - trainLoss: 0.6647934913635254\n",
      "cnt: 0 - valLoss: 0.6664800047874451 - trainLoss: 0.6647933125495911\n",
      "cnt: 0 - valLoss: 0.6664798259735107 - trainLoss: 0.6647931337356567\n",
      "cnt: 0 - valLoss: 0.6664795875549316 - trainLoss: 0.6647929549217224\n",
      "cnt: 0 - valLoss: 0.6664794087409973 - trainLoss: 0.6647927761077881\n",
      "cnt: 0 - valLoss: 0.666479229927063 - trainLoss: 0.6647925972938538\n",
      "cnt: 0 - valLoss: 0.6664790511131287 - trainLoss: 0.6647924184799194\n",
      "cnt: 0 - valLoss: 0.6664788126945496 - trainLoss: 0.6647922396659851\n",
      "cnt: 0 - valLoss: 0.6664786338806152 - trainLoss: 0.664792001247406\n",
      "cnt: 0 - valLoss: 0.6664784550666809 - trainLoss: 0.6647918224334717\n",
      "cnt: 0 - valLoss: 0.6664782762527466 - trainLoss: 0.6647916436195374\n",
      "cnt: 0 - valLoss: 0.6664780378341675 - trainLoss: 0.664791464805603\n",
      "cnt: 0 - valLoss: 0.6664778590202332 - trainLoss: 0.6647912263870239\n",
      "cnt: 0 - valLoss: 0.6664776802062988 - trainLoss: 0.6647910475730896\n",
      "cnt: 0 - valLoss: 0.6664774417877197 - trainLoss: 0.6647908687591553\n",
      "cnt: 0 - valLoss: 0.6664773225784302 - trainLoss: 0.6647907495498657\n",
      "cnt: 0 - valLoss: 0.6664770841598511 - trainLoss: 0.6647905111312866\n",
      "cnt: 0 - valLoss: 0.6664769649505615 - trainLoss: 0.6647903323173523\n",
      "cnt: 0 - valLoss: 0.6664767265319824 - trainLoss: 0.6647902131080627\n",
      "cnt: 0 - valLoss: 0.6664764881134033 - trainLoss: 0.6647899746894836\n",
      "cnt: 0 - valLoss: 0.6664763689041138 - trainLoss: 0.6647897958755493\n",
      "cnt: 0 - valLoss: 0.6664761304855347 - trainLoss: 0.664789617061615\n",
      "cnt: 0 - valLoss: 0.6664759516716003 - trainLoss: 0.6647894382476807\n",
      "cnt: 0 - valLoss: 0.6664758324623108 - trainLoss: 0.6647892594337463\n",
      "cnt: 0 - valLoss: 0.6664755940437317 - trainLoss: 0.664789080619812\n",
      "cnt: 0 - valLoss: 0.6664754748344421 - trainLoss: 0.6647889018058777\n",
      "cnt: 0 - valLoss: 0.6664752960205078 - trainLoss: 0.6647887229919434\n",
      "cnt: 0 - valLoss: 0.6664751172065735 - trainLoss: 0.664788544178009\n",
      "cnt: 0 - valLoss: 0.6664749383926392 - trainLoss: 0.6647883057594299\n",
      "cnt: 0 - valLoss: 0.6664747595787048 - trainLoss: 0.6647881865501404\n",
      "cnt: 0 - valLoss: 0.6664745211601257 - trainLoss: 0.664788007736206\n",
      "cnt: 0 - valLoss: 0.6664744019508362 - trainLoss: 0.6647878289222717\n",
      "cnt: 0 - valLoss: 0.6664742231369019 - trainLoss: 0.6647875905036926\n",
      "cnt: 0 - valLoss: 0.6664741039276123 - trainLoss: 0.6647874712944031\n",
      "cnt: 0 - valLoss: 0.666473925113678 - trainLoss: 0.664787232875824\n",
      "cnt: 0 - valLoss: 0.6664737462997437 - trainLoss: 0.6647871136665344\n",
      "cnt: 0 - valLoss: 0.6664735674858093 - trainLoss: 0.6647869348526001\n",
      "cnt: 0 - valLoss: 0.666473388671875 - trainLoss: 0.664786696434021\n",
      "cnt: 0 - valLoss: 0.6664731502532959 - trainLoss: 0.6647865176200867\n",
      "cnt: 0 - valLoss: 0.6664730310440063 - trainLoss: 0.6647863388061523\n",
      "cnt: 0 - valLoss: 0.6664729118347168 - trainLoss: 0.664786159992218\n",
      "cnt: 0 - valLoss: 0.6664726734161377 - trainLoss: 0.6647860407829285\n",
      "cnt: 0 - valLoss: 0.6664725542068481 - trainLoss: 0.6647858023643494\n",
      "cnt: 0 - valLoss: 0.666472315788269 - trainLoss: 0.6647856831550598\n",
      "cnt: 0 - valLoss: 0.6664721965789795 - trainLoss: 0.6647854447364807\n",
      "cnt: 0 - valLoss: 0.6664719581604004 - trainLoss: 0.6647852659225464\n",
      "cnt: 0 - valLoss: 0.6664718389511108 - trainLoss: 0.6647850871086121\n",
      "cnt: 0 - valLoss: 0.6664716601371765 - trainLoss: 0.6647849678993225\n",
      "cnt: 0 - valLoss: 0.666471540927887 - trainLoss: 0.6647847294807434\n",
      "cnt: 0 - valLoss: 0.6664713025093079 - trainLoss: 0.6647845506668091\n",
      "cnt: 0 - valLoss: 0.6664711833000183 - trainLoss: 0.6647843718528748\n",
      "cnt: 0 - valLoss: 0.666471004486084 - trainLoss: 0.6647841930389404\n",
      "cnt: 0 - valLoss: 0.6664708852767944 - trainLoss: 0.6647840738296509\n",
      "cnt: 0 - valLoss: 0.6664707064628601 - trainLoss: 0.664783775806427\n",
      "cnt: 0 - valLoss: 0.6664705872535706 - trainLoss: 0.6647836565971375\n",
      "cnt: 0 - valLoss: 0.6664704084396362 - trainLoss: 0.6647834777832031\n",
      "cnt: 0 - valLoss: 0.6664701700210571 - trainLoss: 0.6647832989692688\n",
      "cnt: 0 - valLoss: 0.6664700508117676 - trainLoss: 0.6647831201553345\n",
      "cnt: 0 - valLoss: 0.6664698719978333 - trainLoss: 0.6647829413414001\n",
      "cnt: 0 - valLoss: 0.6664696931838989 - trainLoss: 0.6647827625274658\n",
      "cnt: 0 - valLoss: 0.6664695739746094 - trainLoss: 0.6647825837135315\n",
      "cnt: 0 - valLoss: 0.6664693355560303 - trainLoss: 0.6647824048995972\n",
      "cnt: 0 - valLoss: 0.6664692163467407 - trainLoss: 0.6647822260856628\n",
      "cnt: 0 - valLoss: 0.6664690375328064 - trainLoss: 0.6647821068763733\n",
      "cnt: 0 - valLoss: 0.6664689183235168 - trainLoss: 0.6647818684577942\n",
      "cnt: 0 - valLoss: 0.6664687395095825 - trainLoss: 0.6647816896438599\n",
      "cnt: 0 - valLoss: 0.6664685606956482 - trainLoss: 0.6647815108299255\n",
      "cnt: 0 - valLoss: 0.6664684414863586 - trainLoss: 0.6647813320159912\n",
      "cnt: 0 - valLoss: 0.6664682626724243 - trainLoss: 0.6647811532020569\n",
      "cnt: 0 - valLoss: 0.66646808385849 - trainLoss: 0.6647810339927673\n",
      "cnt: 0 - valLoss: 0.6664679646492004 - trainLoss: 0.6647807955741882\n",
      "cnt: 0 - valLoss: 0.6664677858352661 - trainLoss: 0.6647806167602539\n",
      "cnt: 0 - valLoss: 0.666467547416687 - trainLoss: 0.6647804379463196\n",
      "cnt: 0 - valLoss: 0.6664674282073975 - trainLoss: 0.6647802591323853\n",
      "cnt: 0 - valLoss: 0.6664672493934631 - trainLoss: 0.6647801399230957\n",
      "cnt: 0 - valLoss: 0.6664670705795288 - trainLoss: 0.6647799611091614\n",
      "cnt: 0 - valLoss: 0.6664669513702393 - trainLoss: 0.664779782295227\n",
      "cnt: 0 - valLoss: 0.6664668321609497 - trainLoss: 0.6647796034812927\n",
      "cnt: 0 - valLoss: 0.6664666533470154 - trainLoss: 0.6647794246673584\n",
      "cnt: 0 - valLoss: 0.666466474533081 - trainLoss: 0.6647791862487793\n",
      "cnt: 0 - valLoss: 0.6664662957191467 - trainLoss: 0.6647790670394897\n",
      "cnt: 0 - valLoss: 0.6664661169052124 - trainLoss: 0.6647788882255554\n",
      "cnt: 0 - valLoss: 0.6664659976959229 - trainLoss: 0.6647786498069763\n",
      "cnt: 0 - valLoss: 0.6664658188819885 - trainLoss: 0.6647785902023315\n",
      "cnt: 0 - valLoss: 0.666465699672699 - trainLoss: 0.6647783517837524\n",
      "cnt: 0 - valLoss: 0.6664655208587646 - trainLoss: 0.6647781729698181\n",
      "cnt: 0 - valLoss: 0.6664654016494751 - trainLoss: 0.6647779941558838\n",
      "cnt: 0 - valLoss: 0.6664652228355408 - trainLoss: 0.6647778153419495\n",
      "cnt: 0 - valLoss: 0.6664650440216064 - trainLoss: 0.6647776961326599\n",
      "cnt: 0 - valLoss: 0.6664649248123169 - trainLoss: 0.6647775173187256\n",
      "cnt: 0 - valLoss: 0.6664647459983826 - trainLoss: 0.6647773385047913\n",
      "cnt: 0 - valLoss: 0.6664645671844482 - trainLoss: 0.6647771596908569\n",
      "cnt: 0 - valLoss: 0.6664644479751587 - trainLoss: 0.6647770404815674\n",
      "cnt: 0 - valLoss: 0.6664643287658691 - trainLoss: 0.6647768616676331\n",
      "cnt: 0 - valLoss: 0.6664641499519348 - trainLoss: 0.6647766828536987\n",
      "cnt: 0 - valLoss: 0.6664639711380005 - trainLoss: 0.6647765040397644\n",
      "cnt: 0 - valLoss: 0.6664638519287109 - trainLoss: 0.6647763252258301\n",
      "cnt: 0 - valLoss: 0.6664636731147766 - trainLoss: 0.6647761464118958\n",
      "cnt: 0 - valLoss: 0.6664635539054871 - trainLoss: 0.6647760272026062\n",
      "cnt: 0 - valLoss: 0.6664634346961975 - trainLoss: 0.6647758483886719\n",
      "cnt: 0 - valLoss: 0.6664632558822632 - trainLoss: 0.6647756695747375\n",
      "cnt: 0 - valLoss: 0.6664630770683289 - trainLoss: 0.664775550365448\n",
      "cnt: 0 - valLoss: 0.6664629578590393 - trainLoss: 0.6647753119468689\n",
      "cnt: 0 - valLoss: 0.666462779045105 - trainLoss: 0.6647751927375793\n",
      "cnt: 0 - valLoss: 0.6664626598358154 - trainLoss: 0.664775013923645\n",
      "cnt: 0 - valLoss: 0.6664624810218811 - trainLoss: 0.6647748351097107\n",
      "cnt: 0 - valLoss: 0.6664623022079468 - trainLoss: 0.6647747159004211\n",
      "cnt: 0 - valLoss: 0.6664621829986572 - trainLoss: 0.664774477481842\n",
      "cnt: 0 - valLoss: 0.6664620637893677 - trainLoss: 0.6647743582725525\n",
      "cnt: 0 - valLoss: 0.6664618849754333 - trainLoss: 0.6647741794586182\n",
      "cnt: 0 - valLoss: 0.666461706161499 - trainLoss: 0.6647740602493286\n",
      "cnt: 0 - valLoss: 0.6664615869522095 - trainLoss: 0.6647738814353943\n",
      "cnt: 0 - valLoss: 0.6664614081382751 - trainLoss: 0.66477370262146\n",
      "cnt: 0 - valLoss: 0.6664612293243408 - trainLoss: 0.6647735238075256\n",
      "cnt: 0 - valLoss: 0.6664611101150513 - trainLoss: 0.6647733449935913\n",
      "cnt: 0 - valLoss: 0.6664609313011169 - trainLoss: 0.6647732257843018\n",
      "cnt: 0 - valLoss: 0.6664608120918274 - trainLoss: 0.6647729873657227\n",
      "cnt: 0 - valLoss: 0.6664606332778931 - trainLoss: 0.6647728085517883\n",
      "cnt: 0 - valLoss: 0.6664605140686035 - trainLoss: 0.6647726893424988\n",
      "cnt: 0 - valLoss: 0.6664603352546692 - trainLoss: 0.6647725105285645\n",
      "cnt: 0 - valLoss: 0.6664602160453796 - trainLoss: 0.6647723317146301\n",
      "cnt: 0 - valLoss: 0.6664600372314453 - trainLoss: 0.6647722125053406\n",
      "cnt: 0 - valLoss: 0.666459858417511 - trainLoss: 0.6647720336914062\n",
      "cnt: 0 - valLoss: 0.6664597392082214 - trainLoss: 0.6647719144821167\n",
      "cnt: 0 - valLoss: 0.6664595603942871 - trainLoss: 0.6647717356681824\n",
      "cnt: 0 - valLoss: 0.6664594411849976 - trainLoss: 0.664771556854248\n",
      "cnt: 0 - valLoss: 0.666459321975708 - trainLoss: 0.6647713780403137\n",
      "cnt: 0 - valLoss: 0.6664591431617737 - trainLoss: 0.6647711992263794\n",
      "cnt: 0 - valLoss: 0.6664589643478394 - trainLoss: 0.6647710800170898\n",
      "cnt: 0 - valLoss: 0.6664588451385498 - trainLoss: 0.6647708415985107\n",
      "cnt: 0 - valLoss: 0.6664586663246155 - trainLoss: 0.6647707223892212\n",
      "cnt: 0 - valLoss: 0.6664585471153259 - trainLoss: 0.6647705435752869\n",
      "cnt: 0 - valLoss: 0.6664583683013916 - trainLoss: 0.6647704243659973\n",
      "cnt: 0 - valLoss: 0.666458249092102 - trainLoss: 0.664770245552063\n",
      "cnt: 0 - valLoss: 0.6664580702781677 - trainLoss: 0.6647701263427734\n",
      "cnt: 0 - valLoss: 0.6664579510688782 - trainLoss: 0.6647698879241943\n",
      "cnt: 0 - valLoss: 0.6664577722549438 - trainLoss: 0.66476970911026\n",
      "cnt: 0 - valLoss: 0.6664576530456543 - trainLoss: 0.6647695899009705\n",
      "cnt: 0 - valLoss: 0.66645747423172 - trainLoss: 0.6647694110870361\n",
      "cnt: 0 - valLoss: 0.6664572954177856 - trainLoss: 0.6647692322731018\n",
      "cnt: 0 - valLoss: 0.6664571166038513 - trainLoss: 0.6647690534591675\n",
      "cnt: 0 - valLoss: 0.6664569973945618 - trainLoss: 0.6647689938545227\n",
      "cnt: 0 - valLoss: 0.6664568781852722 - trainLoss: 0.6647687554359436\n",
      "cnt: 0 - valLoss: 0.6664567589759827 - trainLoss: 0.6647685766220093\n",
      "cnt: 0 - valLoss: 0.6664565801620483 - trainLoss: 0.6647684574127197\n",
      "cnt: 0 - valLoss: 0.666456401348114 - trainLoss: 0.6647682785987854\n",
      "cnt: 0 - valLoss: 0.6664563417434692 - trainLoss: 0.6647680997848511\n",
      "cnt: 0 - valLoss: 0.6664561033248901 - trainLoss: 0.6647679805755615\n",
      "cnt: 0 - valLoss: 0.6664559841156006 - trainLoss: 0.6647678017616272\n",
      "cnt: 0 - valLoss: 0.666455864906311 - trainLoss: 0.6647676229476929\n",
      "cnt: 0 - valLoss: 0.6664556860923767 - trainLoss: 0.6647674441337585\n",
      "cnt: 0 - valLoss: 0.6664555668830872 - trainLoss: 0.6647672653198242\n",
      "cnt: 0 - valLoss: 0.6664553880691528 - trainLoss: 0.6647671461105347\n",
      "cnt: 0 - valLoss: 0.6664552688598633 - trainLoss: 0.6647670269012451\n",
      "cnt: 0 - valLoss: 0.6664551496505737 - trainLoss: 0.6647668480873108\n",
      "cnt: 0 - valLoss: 0.6664549708366394 - trainLoss: 0.6647666692733765\n",
      "cnt: 0 - valLoss: 0.6664548516273499 - trainLoss: 0.6647665500640869\n",
      "cnt: 0 - valLoss: 0.6664546728134155 - trainLoss: 0.6647663116455078\n",
      "cnt: 0 - valLoss: 0.666454553604126 - trainLoss: 0.6647661924362183\n",
      "cnt: 0 - valLoss: 0.6664544343948364 - trainLoss: 0.6647660136222839\n",
      "cnt: 0 - valLoss: 0.6664541959762573 - trainLoss: 0.6647658348083496\n",
      "cnt: 0 - valLoss: 0.6664540767669678 - trainLoss: 0.6647657155990601\n",
      "cnt: 0 - valLoss: 0.6664539575576782 - trainLoss: 0.6647655367851257\n",
      "cnt: 0 - valLoss: 0.6664537787437439 - trainLoss: 0.6647654175758362\n",
      "cnt: 0 - valLoss: 0.6664535999298096 - trainLoss: 0.6647652387619019\n",
      "cnt: 0 - valLoss: 0.66645348072052 - trainLoss: 0.6647650599479675\n",
      "cnt: 0 - valLoss: 0.6664533019065857 - trainLoss: 0.6647648811340332\n",
      "cnt: 0 - valLoss: 0.6664532423019409 - trainLoss: 0.6647647023200989\n",
      "cnt: 0 - valLoss: 0.6664530634880066 - trainLoss: 0.6647646427154541\n",
      "cnt: 0 - valLoss: 0.6664528846740723 - trainLoss: 0.664764404296875\n",
      "cnt: 0 - valLoss: 0.6664527654647827 - trainLoss: 0.6647642850875854\n",
      "cnt: 0 - valLoss: 0.6664526462554932 - trainLoss: 0.6647641062736511\n",
      "cnt: 0 - valLoss: 0.6664524674415588 - trainLoss: 0.6647639870643616\n",
      "cnt: 0 - valLoss: 0.6664522886276245 - trainLoss: 0.6647637486457825\n",
      "cnt: 0 - valLoss: 0.6664522290229797 - trainLoss: 0.6647636294364929\n",
      "cnt: 0 - valLoss: 0.6664520502090454 - trainLoss: 0.6647634506225586\n",
      "cnt: 0 - valLoss: 0.6664519309997559 - trainLoss: 0.664763331413269\n",
      "cnt: 0 - valLoss: 0.6664517521858215 - trainLoss: 0.6647631525993347\n",
      "cnt: 0 - valLoss: 0.666451632976532 - trainLoss: 0.6647629737854004\n",
      "cnt: 0 - valLoss: 0.6664514541625977 - trainLoss: 0.6647627949714661\n",
      "cnt: 0 - valLoss: 0.6664512753486633 - trainLoss: 0.6647626757621765\n",
      "cnt: 0 - valLoss: 0.6664512157440186 - trainLoss: 0.6647624969482422\n",
      "cnt: 0 - valLoss: 0.6664510369300842 - trainLoss: 0.6647623777389526\n",
      "cnt: 0 - valLoss: 0.6664508581161499 - trainLoss: 0.6647621393203735\n",
      "cnt: 0 - valLoss: 0.6664506793022156 - trainLoss: 0.6647620797157288\n",
      "cnt: 0 - valLoss: 0.666450560092926 - trainLoss: 0.6647619009017944\n",
      "cnt: 0 - valLoss: 0.6664504408836365 - trainLoss: 0.6647616028785706\n",
      "cnt: 0 - valLoss: 0.6664502620697021 - trainLoss: 0.6647615432739258\n",
      "cnt: 0 - valLoss: 0.6664501428604126 - trainLoss: 0.6647613644599915\n",
      "cnt: 0 - valLoss: 0.6664499640464783 - trainLoss: 0.6647611856460571\n",
      "cnt: 0 - valLoss: 0.6664498448371887 - trainLoss: 0.6647610068321228\n",
      "cnt: 0 - valLoss: 0.6664496660232544 - trainLoss: 0.664760947227478\n",
      "cnt: 0 - valLoss: 0.6664495468139648 - trainLoss: 0.6647607684135437\n",
      "cnt: 0 - valLoss: 0.6664493680000305 - trainLoss: 0.6647605895996094\n",
      "cnt: 0 - valLoss: 0.666449248790741 - trainLoss: 0.664760410785675\n",
      "cnt: 0 - valLoss: 0.6664490699768066 - trainLoss: 0.6647602319717407\n",
      "cnt: 0 - valLoss: 0.6664489507675171 - trainLoss: 0.6647601127624512\n",
      "cnt: 0 - valLoss: 0.6664488315582275 - trainLoss: 0.6647599339485168\n",
      "cnt: 0 - valLoss: 0.6664486527442932 - trainLoss: 0.6647597551345825\n",
      "cnt: 0 - valLoss: 0.6664484739303589 - trainLoss: 0.6647595763206482\n",
      "cnt: 0 - valLoss: 0.6664483547210693 - trainLoss: 0.6647594571113586\n",
      "cnt: 0 - valLoss: 0.666448175907135 - trainLoss: 0.6647593379020691\n",
      "cnt: 0 - valLoss: 0.6664480566978455 - trainLoss: 0.66475909948349\n",
      "cnt: 0 - valLoss: 0.6664479374885559 - trainLoss: 0.6647589802742004\n",
      "cnt: 0 - valLoss: 0.6664477586746216 - trainLoss: 0.6647588610649109\n",
      "cnt: 0 - valLoss: 0.666447639465332 - trainLoss: 0.6647586226463318\n",
      "cnt: 0 - valLoss: 0.6664474606513977 - trainLoss: 0.6647585034370422\n",
      "cnt: 0 - valLoss: 0.6664473414421082 - trainLoss: 0.6647583246231079\n",
      "cnt: 0 - valLoss: 0.6664471626281738 - trainLoss: 0.6647581458091736\n",
      "cnt: 0 - valLoss: 0.6664469838142395 - trainLoss: 0.6647579669952393\n",
      "cnt: 0 - valLoss: 0.6664468050003052 - trainLoss: 0.6647578477859497\n",
      "cnt: 0 - valLoss: 0.6664466857910156 - trainLoss: 0.6647576689720154\n",
      "cnt: 0 - valLoss: 0.6664465665817261 - trainLoss: 0.6647575497627258\n",
      "cnt: 0 - valLoss: 0.6664464473724365 - trainLoss: 0.6647574305534363\n",
      "cnt: 0 - valLoss: 0.666446328163147 - trainLoss: 0.664757251739502\n",
      "cnt: 0 - valLoss: 0.6664462089538574 - trainLoss: 0.6647570729255676\n",
      "cnt: 0 - valLoss: 0.6664461493492126 - trainLoss: 0.6647570133209229\n",
      "cnt: 0 - valLoss: 0.6664459705352783 - trainLoss: 0.6647568941116333\n",
      "cnt: 0 - valLoss: 0.6664458513259888 - trainLoss: 0.6647566556930542\n",
      "cnt: 0 - valLoss: 0.6664456725120544 - trainLoss: 0.6647565364837646\n",
      "cnt: 0 - valLoss: 0.6664455533027649 - trainLoss: 0.6647564172744751\n",
      "cnt: 0 - valLoss: 0.6664454340934753 - trainLoss: 0.664756178855896\n",
      "cnt: 0 - valLoss: 0.666445255279541 - trainLoss: 0.6647560596466064\n",
      "cnt: 0 - valLoss: 0.6664451956748962 - trainLoss: 0.6647558808326721\n",
      "cnt: 0 - valLoss: 0.6664450764656067 - trainLoss: 0.6647557020187378\n",
      "cnt: 0 - valLoss: 0.6664449572563171 - trainLoss: 0.6647555232048035\n",
      "cnt: 0 - valLoss: 0.6664447784423828 - trainLoss: 0.6647553443908691\n",
      "cnt: 0 - valLoss: 0.6664446592330933 - trainLoss: 0.6647552251815796\n",
      "cnt: 0 - valLoss: 0.6664445400238037 - trainLoss: 0.6647550463676453\n",
      "cnt: 0 - valLoss: 0.6664443612098694 - trainLoss: 0.6647548675537109\n",
      "cnt: 0 - valLoss: 0.6664442420005798 - trainLoss: 0.6647546887397766\n",
      "cnt: 0 - valLoss: 0.6664441823959351 - trainLoss: 0.6647545695304871\n",
      "cnt: 0 - valLoss: 0.6664440035820007 - trainLoss: 0.6647543907165527\n",
      "cnt: 0 - valLoss: 0.6664438843727112 - trainLoss: 0.6647542715072632\n",
      "cnt: 0 - valLoss: 0.6664438247680664 - trainLoss: 0.6647540330886841\n",
      "cnt: 0 - valLoss: 0.6664436459541321 - trainLoss: 0.6647539138793945\n",
      "cnt: 0 - valLoss: 0.6664435267448425 - trainLoss: 0.6647536754608154\n",
      "cnt: 0 - valLoss: 0.6664433479309082 - trainLoss: 0.6647535562515259\n",
      "cnt: 0 - valLoss: 0.6664432287216187 - trainLoss: 0.6647533774375916\n",
      "cnt: 0 - valLoss: 0.6664431095123291 - trainLoss: 0.664753258228302\n",
      "cnt: 0 - valLoss: 0.6664429903030396 - trainLoss: 0.6647530794143677\n",
      "cnt: 0 - valLoss: 0.66644287109375 - trainLoss: 0.6647529006004333\n",
      "cnt: 0 - valLoss: 0.6664427518844604 - trainLoss: 0.664752721786499\n",
      "cnt: 0 - valLoss: 0.6664426326751709 - trainLoss: 0.6647525429725647\n",
      "cnt: 0 - valLoss: 0.6664424538612366 - trainLoss: 0.6647523045539856\n",
      "cnt: 0 - valLoss: 0.666442334651947 - trainLoss: 0.664752185344696\n",
      "cnt: 0 - valLoss: 0.6664421558380127 - trainLoss: 0.6647520065307617\n",
      "cnt: 0 - valLoss: 0.6664420366287231 - trainLoss: 0.6647518873214722\n",
      "cnt: 0 - valLoss: 0.6664419174194336 - trainLoss: 0.6647517085075378\n",
      "cnt: 0 - valLoss: 0.666441798210144 - trainLoss: 0.6647515296936035\n",
      "cnt: 0 - valLoss: 0.6664416790008545 - trainLoss: 0.6647513508796692\n",
      "cnt: 0 - valLoss: 0.6664415597915649 - trainLoss: 0.6647512316703796\n",
      "cnt: 0 - valLoss: 0.6664414405822754 - trainLoss: 0.6647510528564453\n",
      "cnt: 0 - valLoss: 0.6664413809776306 - trainLoss: 0.6647509336471558\n",
      "cnt: 0 - valLoss: 0.6664412617683411 - trainLoss: 0.6647508144378662\n",
      "cnt: 0 - valLoss: 0.6664411425590515 - trainLoss: 0.6647506356239319\n",
      "cnt: 0 - valLoss: 0.6664409637451172 - trainLoss: 0.6647505164146423\n",
      "cnt: 0 - valLoss: 0.6664409041404724 - trainLoss: 0.664750337600708\n",
      "cnt: 0 - valLoss: 0.6664407849311829 - trainLoss: 0.6647502183914185\n",
      "cnt: 0 - valLoss: 0.6664407253265381 - trainLoss: 0.6647500395774841\n",
      "cnt: 0 - valLoss: 0.6664406061172485 - trainLoss: 0.6647499203681946\n",
      "cnt: 0 - valLoss: 0.666440486907959 - trainLoss: 0.6647497415542603\n",
      "cnt: 0 - valLoss: 0.6664403676986694 - trainLoss: 0.6647496223449707\n",
      "cnt: 0 - valLoss: 0.6664403080940247 - trainLoss: 0.6647494435310364\n",
      "cnt: 0 - valLoss: 0.6664401888847351 - trainLoss: 0.6647493243217468\n",
      "cnt: 0 - valLoss: 0.6664400696754456 - trainLoss: 0.6647492051124573\n",
      "cnt: 0 - valLoss: 0.6664398908615112 - trainLoss: 0.664749026298523\n",
      "cnt: 0 - valLoss: 0.6664398312568665 - trainLoss: 0.6647489070892334\n",
      "cnt: 0 - valLoss: 0.6664397120475769 - trainLoss: 0.6647487878799438\n",
      "cnt: 0 - valLoss: 0.6664395928382874 - trainLoss: 0.6647485494613647\n",
      "cnt: 0 - valLoss: 0.6664395332336426 - trainLoss: 0.66474848985672\n",
      "cnt: 0 - valLoss: 0.666439414024353 - trainLoss: 0.6647482514381409\n",
      "cnt: 0 - valLoss: 0.6664392948150635 - trainLoss: 0.6647481918334961\n",
      "cnt: 0 - valLoss: 0.6664391160011292 - trainLoss: 0.6647480130195618\n",
      "cnt: 0 - valLoss: 0.6664391160011292 - trainLoss: 0.6647478938102722\n",
      "cnt: 0 - valLoss: 0.6664389967918396 - trainLoss: 0.6647477746009827\n",
      "cnt: 0 - valLoss: 0.6664388179779053 - trainLoss: 0.6647476553916931\n",
      "cnt: 0 - valLoss: 0.6664387583732605 - trainLoss: 0.6647474765777588\n",
      "cnt: 0 - valLoss: 0.666438639163971 - trainLoss: 0.6647472977638245\n",
      "cnt: 0 - valLoss: 0.6664385199546814 - trainLoss: 0.6647472381591797\n",
      "cnt: 0 - valLoss: 0.6664384007453918 - trainLoss: 0.6647470593452454\n",
      "cnt: 0 - valLoss: 0.6664383411407471 - trainLoss: 0.664746880531311\n",
      "cnt: 0 - valLoss: 0.6664381623268127 - trainLoss: 0.6647468209266663\n",
      "cnt: 0 - valLoss: 0.6664380431175232 - trainLoss: 0.6647466421127319\n",
      "cnt: 0 - valLoss: 0.6664379835128784 - trainLoss: 0.6647465229034424\n",
      "cnt: 0 - valLoss: 0.6664378643035889 - trainLoss: 0.6647464036941528\n",
      "cnt: 0 - valLoss: 0.6664377450942993 - trainLoss: 0.6647462844848633\n",
      "cnt: 0 - valLoss: 0.6664376258850098 - trainLoss: 0.664746105670929\n",
      "cnt: 0 - valLoss: 0.6664374470710754 - trainLoss: 0.6647459864616394\n",
      "cnt: 0 - valLoss: 0.6664373874664307 - trainLoss: 0.6647458076477051\n",
      "cnt: 0 - valLoss: 0.6664373278617859 - trainLoss: 0.6647456288337708\n",
      "cnt: 0 - valLoss: 0.6664372086524963 - trainLoss: 0.6647455096244812\n",
      "cnt: 0 - valLoss: 0.6664370894432068 - trainLoss: 0.6647454500198364\n",
      "cnt: 0 - valLoss: 0.6664369702339172 - trainLoss: 0.6647452712059021\n",
      "cnt: 0 - valLoss: 0.6664368510246277 - trainLoss: 0.6647451519966125\n",
      "cnt: 0 - valLoss: 0.6664366722106934 - trainLoss: 0.664745032787323\n",
      "cnt: 0 - valLoss: 0.6664366126060486 - trainLoss: 0.6647448539733887\n",
      "cnt: 0 - valLoss: 0.6664365530014038 - trainLoss: 0.6647447347640991\n",
      "cnt: 0 - valLoss: 0.6664364337921143 - trainLoss: 0.6647446155548096\n",
      "cnt: 0 - valLoss: 0.6664363145828247 - trainLoss: 0.66474449634552\n",
      "cnt: 0 - valLoss: 0.6664362549781799 - trainLoss: 0.6647442579269409\n",
      "cnt: 0 - valLoss: 0.6664360761642456 - trainLoss: 0.6647441387176514\n",
      "cnt: 0 - valLoss: 0.6664360165596008 - trainLoss: 0.6647440791130066\n",
      "cnt: 0 - valLoss: 0.666435956954956 - trainLoss: 0.6647439002990723\n",
      "cnt: 0 - valLoss: 0.6664357781410217 - trainLoss: 0.6647438406944275\n",
      "cnt: 0 - valLoss: 0.6664357781410217 - trainLoss: 0.6647436618804932\n",
      "cnt: 0 - valLoss: 0.6664355993270874 - trainLoss: 0.6647435426712036\n",
      "cnt: 0 - valLoss: 0.6664354801177979 - trainLoss: 0.6647434234619141\n",
      "cnt: 0 - valLoss: 0.6664353609085083 - trainLoss: 0.6647433042526245\n",
      "cnt: 0 - valLoss: 0.6664352416992188 - trainLoss: 0.6647431254386902\n",
      "cnt: 0 - valLoss: 0.666435182094574 - trainLoss: 0.6647430062294006\n",
      "cnt: 0 - valLoss: 0.6664350628852844 - trainLoss: 0.6647428870201111\n",
      "cnt: 0 - valLoss: 0.6664350032806396 - trainLoss: 0.6647427678108215\n",
      "cnt: 0 - valLoss: 0.6664348840713501 - trainLoss: 0.6647425889968872\n",
      "cnt: 0 - valLoss: 0.6664347052574158 - trainLoss: 0.6647424697875977\n",
      "cnt: 0 - valLoss: 0.666434645652771 - trainLoss: 0.6647424101829529\n",
      "cnt: 0 - valLoss: 0.6664345264434814 - trainLoss: 0.6647421717643738\n",
      "cnt: 0 - valLoss: 0.6664344668388367 - trainLoss: 0.664742112159729\n",
      "cnt: 0 - valLoss: 0.6664343476295471 - trainLoss: 0.6647419929504395\n",
      "cnt: 0 - valLoss: 0.6664342284202576 - trainLoss: 0.6647418737411499\n",
      "cnt: 0 - valLoss: 0.666434109210968 - trainLoss: 0.6647416949272156\n",
      "cnt: 0 - valLoss: 0.6664340496063232 - trainLoss: 0.664741575717926\n",
      "cnt: 0 - valLoss: 0.6664339303970337 - trainLoss: 0.6647414565086365\n",
      "cnt: 0 - valLoss: 0.6664338707923889 - trainLoss: 0.6647413372993469\n",
      "cnt: 0 - valLoss: 0.6664336919784546 - trainLoss: 0.6647412180900574\n",
      "cnt: 0 - valLoss: 0.6664336323738098 - trainLoss: 0.6647410988807678\n",
      "cnt: 0 - valLoss: 0.6664335131645203 - trainLoss: 0.6647409200668335\n",
      "cnt: 0 - valLoss: 0.6664334535598755 - trainLoss: 0.664740800857544\n",
      "cnt: 0 - valLoss: 0.6664333343505859 - trainLoss: 0.6647406220436096\n",
      "cnt: 0 - valLoss: 0.6664332151412964 - trainLoss: 0.6647405624389648\n",
      "cnt: 0 - valLoss: 0.6664330959320068 - trainLoss: 0.6647404432296753\n",
      "cnt: 0 - valLoss: 0.6664330363273621 - trainLoss: 0.6647403240203857\n",
      "cnt: 0 - valLoss: 0.6664328575134277 - trainLoss: 0.6647401452064514\n",
      "cnt: 0 - valLoss: 0.6664328575134277 - trainLoss: 0.6647400259971619\n",
      "cnt: 0 - valLoss: 0.6664326786994934 - trainLoss: 0.6647399067878723\n",
      "cnt: 0 - valLoss: 0.6664325594902039 - trainLoss: 0.6647397875785828\n",
      "cnt: 0 - valLoss: 0.6664324998855591 - trainLoss: 0.6647396683692932\n",
      "cnt: 0 - valLoss: 0.6664323806762695 - trainLoss: 0.6647394895553589\n",
      "cnt: 0 - valLoss: 0.6664323210716248 - trainLoss: 0.6647393703460693\n",
      "cnt: 0 - valLoss: 0.6664322018623352 - trainLoss: 0.6647392511367798\n",
      "cnt: 0 - valLoss: 0.6664320826530457 - trainLoss: 0.6647391319274902\n",
      "cnt: 0 - valLoss: 0.6664319634437561 - trainLoss: 0.6647389531135559\n",
      "cnt: 0 - valLoss: 0.6664319038391113 - trainLoss: 0.6647388339042664\n",
      "cnt: 0 - valLoss: 0.6664317846298218 - trainLoss: 0.6647387742996216\n",
      "cnt: 0 - valLoss: 0.6664316654205322 - trainLoss: 0.6647385954856873\n",
      "cnt: 0 - valLoss: 0.6664315462112427 - trainLoss: 0.6647384762763977\n",
      "cnt: 0 - valLoss: 0.6664314866065979 - trainLoss: 0.6647383570671082\n",
      "cnt: 0 - valLoss: 0.6664313673973083 - trainLoss: 0.6647382378578186\n",
      "cnt: 0 - valLoss: 0.6664313077926636 - trainLoss: 0.6647380590438843\n",
      "cnt: 0 - valLoss: 0.666431188583374 - trainLoss: 0.6647379994392395\n",
      "cnt: 0 - valLoss: 0.6664310693740845 - trainLoss: 0.66473788022995\n",
      "cnt: 0 - valLoss: 0.6664308905601501 - trainLoss: 0.6647377014160156\n",
      "cnt: 0 - valLoss: 0.6664308905601501 - trainLoss: 0.6647375822067261\n",
      "cnt: 0 - valLoss: 0.6664307117462158 - trainLoss: 0.6647374033927917\n",
      "cnt: 0 - valLoss: 0.6664307117462158 - trainLoss: 0.6647372841835022\n",
      "cnt: 0 - valLoss: 0.6664305925369263 - trainLoss: 0.6647371649742126\n",
      "cnt: 0 - valLoss: 0.6664304733276367 - trainLoss: 0.6647370457649231\n",
      "cnt: 0 - valLoss: 0.6664303541183472 - trainLoss: 0.6647368669509888\n",
      "cnt: 0 - valLoss: 0.6664302945137024 - trainLoss: 0.6647367477416992\n",
      "cnt: 0 - valLoss: 0.6664301156997681 - trainLoss: 0.6647366285324097\n",
      "cnt: 0 - valLoss: 0.6664300560951233 - trainLoss: 0.6647365093231201\n",
      "cnt: 0 - valLoss: 0.6664299368858337 - trainLoss: 0.6647363901138306\n",
      "cnt: 0 - valLoss: 0.6664298176765442 - trainLoss: 0.664736270904541\n",
      "cnt: 0 - valLoss: 0.6664296984672546 - trainLoss: 0.6647360920906067\n",
      "cnt: 0 - valLoss: 0.6664296388626099 - trainLoss: 0.6647360324859619\n",
      "cnt: 0 - valLoss: 0.6664295196533203 - trainLoss: 0.6647358536720276\n",
      "cnt: 0 - valLoss: 0.6664294004440308 - trainLoss: 0.664735734462738\n",
      "cnt: 0 - valLoss: 0.6664292812347412 - trainLoss: 0.6647356152534485\n",
      "cnt: 0 - valLoss: 0.6664292216300964 - trainLoss: 0.6647354960441589\n",
      "cnt: 0 - valLoss: 0.6664291024208069 - trainLoss: 0.6647353172302246\n",
      "cnt: 0 - valLoss: 0.6664289832115173 - trainLoss: 0.6647351980209351\n",
      "cnt: 0 - valLoss: 0.6664289236068726 - trainLoss: 0.6647351384162903\n",
      "cnt: 0 - valLoss: 0.666428804397583 - trainLoss: 0.664734959602356\n",
      "cnt: 0 - valLoss: 0.6664287447929382 - trainLoss: 0.6647348403930664\n",
      "cnt: 0 - valLoss: 0.6664286255836487 - trainLoss: 0.6647347211837769\n",
      "cnt: 0 - valLoss: 0.6664285063743591 - trainLoss: 0.6647345423698425\n",
      "cnt: 0 - valLoss: 0.6664283871650696 - trainLoss: 0.6647344827651978\n",
      "cnt: 0 - valLoss: 0.6664283275604248 - trainLoss: 0.6647343635559082\n",
      "cnt: 0 - valLoss: 0.6664282083511353 - trainLoss: 0.6647341847419739\n",
      "cnt: 0 - valLoss: 0.6664281487464905 - trainLoss: 0.6647341251373291\n",
      "cnt: 0 - valLoss: 0.6664279699325562 - trainLoss: 0.6647339463233948\n",
      "cnt: 0 - valLoss: 0.6664278507232666 - trainLoss: 0.6647338271141052\n",
      "cnt: 0 - valLoss: 0.6664277911186218 - trainLoss: 0.6647337079048157\n",
      "cnt: 0 - valLoss: 0.6664276719093323 - trainLoss: 0.6647335886955261\n",
      "cnt: 0 - valLoss: 0.6664276123046875 - trainLoss: 0.6647334694862366\n",
      "cnt: 0 - valLoss: 0.6664275527000427 - trainLoss: 0.664733350276947\n",
      "cnt: 0 - valLoss: 0.6664274334907532 - trainLoss: 0.6647332310676575\n",
      "cnt: 0 - valLoss: 0.6664273142814636 - trainLoss: 0.6647330522537231\n",
      "cnt: 0 - valLoss: 0.6664271950721741 - trainLoss: 0.6647329330444336\n",
      "cnt: 0 - valLoss: 0.6664270758628845 - trainLoss: 0.664732813835144\n",
      "cnt: 0 - valLoss: 0.6664270162582397 - trainLoss: 0.6647326946258545\n",
      "cnt: 0 - valLoss: 0.6664268970489502 - trainLoss: 0.6647325754165649\n",
      "cnt: 0 - valLoss: 0.6664268374443054 - trainLoss: 0.6647324562072754\n",
      "cnt: 0 - valLoss: 0.6664267182350159 - trainLoss: 0.6647323369979858\n",
      "cnt: 0 - valLoss: 0.6664266586303711 - trainLoss: 0.6647322177886963\n",
      "cnt: 0 - valLoss: 0.6664264798164368 - trainLoss: 0.664732038974762\n",
      "cnt: 0 - valLoss: 0.6664264798164368 - trainLoss: 0.6647319793701172\n",
      "cnt: 0 - valLoss: 0.6664263010025024 - trainLoss: 0.6647318601608276\n",
      "cnt: 0 - valLoss: 0.6664262413978577 - trainLoss: 0.6647317409515381\n",
      "cnt: 0 - valLoss: 0.6664261221885681 - trainLoss: 0.6647315621376038\n",
      "cnt: 0 - valLoss: 0.6664260029792786 - trainLoss: 0.664731502532959\n",
      "cnt: 0 - valLoss: 0.666425883769989 - trainLoss: 0.6647313237190247\n",
      "cnt: 0 - valLoss: 0.6664258241653442 - trainLoss: 0.6647312045097351\n",
      "cnt: 0 - valLoss: 0.6664257049560547 - trainLoss: 0.6647310853004456\n",
      "cnt: 0 - valLoss: 0.6664256453514099 - trainLoss: 0.6647309064865112\n",
      "cnt: 0 - valLoss: 0.6664255261421204 - trainLoss: 0.6647307872772217\n",
      "cnt: 0 - valLoss: 0.6664254069328308 - trainLoss: 0.6647307276725769\n",
      "cnt: 0 - valLoss: 0.6664254069328308 - trainLoss: 0.6647306084632874\n",
      "cnt: 0 - valLoss: 0.6664252281188965 - trainLoss: 0.6647304892539978\n",
      "cnt: 0 - valLoss: 0.6664251685142517 - trainLoss: 0.6647303104400635\n",
      "cnt: 0 - valLoss: 0.6664250493049622 - trainLoss: 0.6647301912307739\n",
      "cnt: 0 - valLoss: 0.6664249300956726 - trainLoss: 0.6647300720214844\n",
      "cnt: 0 - valLoss: 0.6664248108863831 - trainLoss: 0.6647299528121948\n",
      "cnt: 0 - valLoss: 0.6664248108863831 - trainLoss: 0.6647298336029053\n",
      "cnt: 0 - valLoss: 0.6664246320724487 - trainLoss: 0.6647297143936157\n",
      "cnt: 0 - valLoss: 0.666424572467804 - trainLoss: 0.6647295951843262\n",
      "cnt: 0 - valLoss: 0.6664244532585144 - trainLoss: 0.6647294759750366\n",
      "cnt: 0 - valLoss: 0.6664243340492249 - trainLoss: 0.6647292971611023\n",
      "cnt: 0 - valLoss: 0.6664242148399353 - trainLoss: 0.6647292375564575\n",
      "cnt: 0 - valLoss: 0.6664241552352905 - trainLoss: 0.664729118347168\n",
      "cnt: 0 - valLoss: 0.666424036026001 - trainLoss: 0.6647289395332336\n",
      "cnt: 0 - valLoss: 0.6664239168167114 - trainLoss: 0.6647288203239441\n",
      "cnt: 0 - valLoss: 0.6664238572120667 - trainLoss: 0.6647287607192993\n",
      "cnt: 0 - valLoss: 0.6664237380027771 - trainLoss: 0.664728581905365\n",
      "cnt: 0 - valLoss: 0.6664236187934875 - trainLoss: 0.6647285223007202\n",
      "cnt: 0 - valLoss: 0.6664235591888428 - trainLoss: 0.6647283434867859\n",
      "cnt: 0 - valLoss: 0.6664234399795532 - trainLoss: 0.6647281646728516\n",
      "cnt: 0 - valLoss: 0.6664234399795532 - trainLoss: 0.6647281050682068\n",
      "cnt: 0 - valLoss: 0.6664232611656189 - trainLoss: 0.6647279858589172\n",
      "cnt: 0 - valLoss: 0.6664232015609741 - trainLoss: 0.6647278666496277\n",
      "cnt: 0 - valLoss: 0.6664230823516846 - trainLoss: 0.6647277474403381\n",
      "cnt: 0 - valLoss: 0.666422963142395 - trainLoss: 0.6647276282310486\n",
      "cnt: 0 - valLoss: 0.6664228439331055 - trainLoss: 0.6647274494171143\n",
      "cnt: 0 - valLoss: 0.6664227843284607 - trainLoss: 0.6647273302078247\n",
      "cnt: 0 - valLoss: 0.6664226651191711 - trainLoss: 0.6647272109985352\n",
      "cnt: 0 - valLoss: 0.6664225459098816 - trainLoss: 0.6647271513938904\n",
      "cnt: 0 - valLoss: 0.6664224863052368 - trainLoss: 0.6647270321846008\n",
      "cnt: 0 - valLoss: 0.6664223670959473 - trainLoss: 0.6647268533706665\n",
      "cnt: 0 - valLoss: 0.6664223074913025 - trainLoss: 0.664726734161377\n",
      "cnt: 0 - valLoss: 0.6664221882820129 - trainLoss: 0.6647266149520874\n",
      "cnt: 0 - valLoss: 0.6664220690727234 - trainLoss: 0.6647264957427979\n",
      "cnt: 0 - valLoss: 0.6664220094680786 - trainLoss: 0.6647263765335083\n",
      "cnt: 0 - valLoss: 0.6664218902587891 - trainLoss: 0.6647263169288635\n",
      "cnt: 0 - valLoss: 0.6664217710494995 - trainLoss: 0.664726197719574\n",
      "cnt: 0 - valLoss: 0.6664217114448547 - trainLoss: 0.6647260785102844\n",
      "cnt: 0 - valLoss: 0.6664215922355652 - trainLoss: 0.6647258996963501\n",
      "cnt: 0 - valLoss: 0.6664215326309204 - trainLoss: 0.6647257804870605\n",
      "cnt: 0 - valLoss: 0.6664214134216309 - trainLoss: 0.664725661277771\n",
      "cnt: 0 - valLoss: 0.6664213538169861 - trainLoss: 0.6647256016731262\n",
      "cnt: 0 - valLoss: 0.6664212346076965 - trainLoss: 0.6647254824638367\n",
      "cnt: 0 - valLoss: 0.666421115398407 - trainLoss: 0.6647253036499023\n",
      "cnt: 0 - valLoss: 0.6664209961891174 - trainLoss: 0.6647251844406128\n",
      "cnt: 0 - valLoss: 0.6664208769798279 - trainLoss: 0.6647250652313232\n",
      "cnt: 0 - valLoss: 0.6664208173751831 - trainLoss: 0.6647250056266785\n",
      "cnt: 0 - valLoss: 0.6664207577705383 - trainLoss: 0.6647248268127441\n",
      "cnt: 0 - valLoss: 0.666420578956604 - trainLoss: 0.6647247076034546\n",
      "cnt: 0 - valLoss: 0.6664205193519592 - trainLoss: 0.6647246479988098\n",
      "cnt: 0 - valLoss: 0.6664204597473145 - trainLoss: 0.6647244691848755\n",
      "cnt: 0 - valLoss: 0.6664203405380249 - trainLoss: 0.6647243499755859\n",
      "cnt: 0 - valLoss: 0.6664202213287354 - trainLoss: 0.6647242307662964\n",
      "cnt: 0 - valLoss: 0.6664201021194458 - trainLoss: 0.6647241115570068\n",
      "cnt: 0 - valLoss: 0.666420042514801 - trainLoss: 0.6647239923477173\n",
      "cnt: 0 - valLoss: 0.6664199233055115 - trainLoss: 0.6647238731384277\n",
      "cnt: 0 - valLoss: 0.6664198040962219 - trainLoss: 0.6647237539291382\n",
      "cnt: 0 - valLoss: 0.6664197444915771 - trainLoss: 0.6647236347198486\n",
      "cnt: 0 - valLoss: 0.6664196252822876 - trainLoss: 0.6647235155105591\n",
      "cnt: 0 - valLoss: 0.6664195656776428 - trainLoss: 0.6647233366966248\n",
      "cnt: 0 - valLoss: 0.6664194464683533 - trainLoss: 0.66472327709198\n",
      "cnt: 0 - valLoss: 0.6664193272590637 - trainLoss: 0.6647231578826904\n",
      "cnt: 0 - valLoss: 0.666419267654419 - trainLoss: 0.6647230386734009\n",
      "cnt: 0 - valLoss: 0.6664191484451294 - trainLoss: 0.6647229194641113\n",
      "cnt: 0 - valLoss: 0.6664190888404846 - trainLoss: 0.664722740650177\n",
      "cnt: 0 - valLoss: 0.6664189696311951 - trainLoss: 0.6647226810455322\n",
      "cnt: 0 - valLoss: 0.6664189100265503 - trainLoss: 0.6647225022315979\n",
      "cnt: 0 - valLoss: 0.666418731212616 - trainLoss: 0.6647224426269531\n",
      "cnt: 0 - valLoss: 0.6664186716079712 - trainLoss: 0.6647222638130188\n",
      "cnt: 0 - valLoss: 0.6664186120033264 - trainLoss: 0.664722204208374\n",
      "cnt: 0 - valLoss: 0.6664184331893921 - trainLoss: 0.6647220849990845\n",
      "cnt: 0 - valLoss: 0.6664183735847473 - trainLoss: 0.6647219657897949\n",
      "cnt: 0 - valLoss: 0.6664182543754578 - trainLoss: 0.6647217869758606\n",
      "cnt: 0 - valLoss: 0.666418194770813 - trainLoss: 0.6647217273712158\n",
      "cnt: 0 - valLoss: 0.6664180755615234 - trainLoss: 0.6647216081619263\n",
      "cnt: 0 - valLoss: 0.6664179563522339 - trainLoss: 0.6647214293479919\n",
      "cnt: 0 - valLoss: 0.6664178967475891 - trainLoss: 0.6647213101387024\n",
      "cnt: 0 - valLoss: 0.6664178371429443 - trainLoss: 0.6647212505340576\n",
      "cnt: 0 - valLoss: 0.6664177179336548 - trainLoss: 0.6647210717201233\n",
      "cnt: 0 - valLoss: 0.6664175987243652 - trainLoss: 0.6647210121154785\n",
      "cnt: 0 - valLoss: 0.6664174795150757 - trainLoss: 0.664720892906189\n",
      "cnt: 0 - valLoss: 0.6664174199104309 - trainLoss: 0.6647207736968994\n",
      "cnt: 0 - valLoss: 0.6664173007011414 - trainLoss: 0.6647205948829651\n",
      "cnt: 0 - valLoss: 0.6664172410964966 - trainLoss: 0.6647204756736755\n",
      "cnt: 0 - valLoss: 0.6664170622825623 - trainLoss: 0.6647204756736755\n",
      "cnt: 0 - valLoss: 0.6664170026779175 - trainLoss: 0.6647202968597412\n",
      "cnt: 0 - valLoss: 0.6664169430732727 - trainLoss: 0.6647201180458069\n",
      "cnt: 0 - valLoss: 0.6664168834686279 - trainLoss: 0.6647199988365173\n",
      "cnt: 0 - valLoss: 0.6664167046546936 - trainLoss: 0.6647199392318726\n",
      "cnt: 0 - valLoss: 0.6664166450500488 - trainLoss: 0.664719820022583\n",
      "cnt: 0 - valLoss: 0.6664165258407593 - trainLoss: 0.6647197008132935\n",
      "cnt: 0 - valLoss: 0.6664164066314697 - trainLoss: 0.6647195816040039\n",
      "cnt: 0 - valLoss: 0.666416347026825 - trainLoss: 0.6647194027900696\n",
      "cnt: 0 - valLoss: 0.6664162278175354 - trainLoss: 0.6647193431854248\n",
      "cnt: 0 - valLoss: 0.6664161682128906 - trainLoss: 0.6647191643714905\n",
      "cnt: 0 - valLoss: 0.6664160490036011 - trainLoss: 0.6647191047668457\n",
      "cnt: 0 - valLoss: 0.6664159297943115 - trainLoss: 0.6647189855575562\n",
      "cnt: 0 - valLoss: 0.6664158701896667 - trainLoss: 0.6647188663482666\n",
      "cnt: 0 - valLoss: 0.666415810585022 - trainLoss: 0.664718747138977\n",
      "cnt: 0 - valLoss: 0.6664156317710876 - trainLoss: 0.6647186279296875\n",
      "cnt: 0 - valLoss: 0.6664155125617981 - trainLoss: 0.664718508720398\n",
      "cnt: 0 - valLoss: 0.6664154529571533 - trainLoss: 0.6647183299064636\n",
      "cnt: 0 - valLoss: 0.6664153337478638 - trainLoss: 0.6647183299064636\n",
      "cnt: 0 - valLoss: 0.666415274143219 - trainLoss: 0.6647181510925293\n",
      "cnt: 0 - valLoss: 0.6664151549339294 - trainLoss: 0.6647180318832397\n",
      "cnt: 0 - valLoss: 0.6664150357246399 - trainLoss: 0.6647178530693054\n",
      "cnt: 0 - valLoss: 0.6664149761199951 - trainLoss: 0.6647177934646606\n",
      "cnt: 0 - valLoss: 0.6664149165153503 - trainLoss: 0.6647176742553711\n",
      "cnt: 0 - valLoss: 0.6664147973060608 - trainLoss: 0.6647175550460815\n",
      "cnt: 0 - valLoss: 0.6664146780967712 - trainLoss: 0.664717435836792\n",
      "cnt: 0 - valLoss: 0.6664145588874817 - trainLoss: 0.6647173166275024\n",
      "cnt: 0 - valLoss: 0.6664144992828369 - trainLoss: 0.6647171974182129\n",
      "cnt: 0 - valLoss: 0.6664144396781921 - trainLoss: 0.6647171378135681\n",
      "cnt: 0 - valLoss: 0.6664142608642578 - trainLoss: 0.6647169589996338\n",
      "cnt: 0 - valLoss: 0.6664142608642578 - trainLoss: 0.6647168397903442\n",
      "cnt: 0 - valLoss: 0.6664140820503235 - trainLoss: 0.6647167801856995\n",
      "cnt: 0 - valLoss: 0.6664140224456787 - trainLoss: 0.6647166013717651\n",
      "cnt: 0 - valLoss: 0.6664139032363892 - trainLoss: 0.6647164225578308\n",
      "cnt: 0 - valLoss: 0.6664138436317444 - trainLoss: 0.664716362953186\n",
      "cnt: 0 - valLoss: 0.6664137244224548 - trainLoss: 0.6647161841392517\n",
      "cnt: 0 - valLoss: 0.6664136648178101 - trainLoss: 0.6647161245346069\n",
      "cnt: 0 - valLoss: 0.6664135456085205 - trainLoss: 0.6647160053253174\n",
      "cnt: 0 - valLoss: 0.666413426399231 - trainLoss: 0.6647158861160278\n",
      "cnt: 0 - valLoss: 0.6664133667945862 - trainLoss: 0.6647157669067383\n",
      "cnt: 0 - valLoss: 0.6664131879806519 - trainLoss: 0.6647157073020935\n",
      "cnt: 0 - valLoss: 0.6664131283760071 - trainLoss: 0.6647154688835144\n",
      "cnt: 0 - valLoss: 0.6664130091667175 - trainLoss: 0.6647154688835144\n",
      "cnt: 0 - valLoss: 0.666412889957428 - trainLoss: 0.6647152900695801\n",
      "cnt: 0 - valLoss: 0.6664128303527832 - trainLoss: 0.6647152304649353\n",
      "cnt: 0 - valLoss: 0.6664127111434937 - trainLoss: 0.664715051651001\n",
      "cnt: 0 - valLoss: 0.6664125919342041 - trainLoss: 0.6647149920463562\n",
      "cnt: 0 - valLoss: 0.6664125323295593 - trainLoss: 0.6647148132324219\n",
      "cnt: 0 - valLoss: 0.6664124131202698 - trainLoss: 0.6647146940231323\n",
      "cnt: 0 - valLoss: 0.6664122939109802 - trainLoss: 0.6647146344184875\n",
      "cnt: 0 - valLoss: 0.6664122343063354 - trainLoss: 0.6647144556045532\n",
      "cnt: 0 - valLoss: 0.6664121150970459 - trainLoss: 0.6647143959999084\n",
      "cnt: 0 - valLoss: 0.6664119958877563 - trainLoss: 0.6647142767906189\n",
      "cnt: 0 - valLoss: 0.6664119362831116 - trainLoss: 0.6647140979766846\n",
      "cnt: 0 - valLoss: 0.666411817073822 - trainLoss: 0.6647140383720398\n",
      "cnt: 0 - valLoss: 0.6664116382598877 - trainLoss: 0.6647139191627502\n",
      "cnt: 0 - valLoss: 0.6664115786552429 - trainLoss: 0.6647137403488159\n",
      "cnt: 0 - valLoss: 0.6664115190505981 - trainLoss: 0.6647136807441711\n",
      "cnt: 0 - valLoss: 0.6664113998413086 - trainLoss: 0.6647135615348816\n",
      "cnt: 0 - valLoss: 0.666411280632019 - trainLoss: 0.664713442325592\n",
      "cnt: 0 - valLoss: 0.6664111614227295 - trainLoss: 0.6647133231163025\n",
      "cnt: 0 - valLoss: 0.6664110422134399 - trainLoss: 0.6647131443023682\n",
      "cnt: 0 - valLoss: 0.6664109826087952 - trainLoss: 0.6647130846977234\n",
      "cnt: 0 - valLoss: 0.6664109230041504 - trainLoss: 0.6647130250930786\n",
      "cnt: 0 - valLoss: 0.6664107441902161 - trainLoss: 0.6647128462791443\n",
      "cnt: 0 - valLoss: 0.6664106845855713 - trainLoss: 0.6647127270698547\n",
      "cnt: 0 - valLoss: 0.6664106249809265 - trainLoss: 0.6647126078605652\n",
      "cnt: 0 - valLoss: 0.6664104461669922 - trainLoss: 0.6647124886512756\n",
      "cnt: 0 - valLoss: 0.6664103865623474 - trainLoss: 0.6647124290466309\n",
      "cnt: 0 - valLoss: 0.6664102673530579 - trainLoss: 0.6647122502326965\n",
      "cnt: 0 - valLoss: 0.6664102077484131 - trainLoss: 0.664712131023407\n",
      "cnt: 0 - valLoss: 0.6664100289344788 - trainLoss: 0.6647120714187622\n",
      "cnt: 0 - valLoss: 0.666409969329834 - trainLoss: 0.6647119522094727\n",
      "cnt: 0 - valLoss: 0.6664098501205444 - trainLoss: 0.6647117733955383\n",
      "cnt: 0 - valLoss: 0.6664097905158997 - trainLoss: 0.6647117137908936\n",
      "cnt: 0 - valLoss: 0.6664096713066101 - trainLoss: 0.6647115349769592\n",
      "cnt: 0 - valLoss: 0.6664096117019653 - trainLoss: 0.6647114753723145\n",
      "cnt: 0 - valLoss: 0.666409432888031 - trainLoss: 0.6647113561630249\n",
      "cnt: 0 - valLoss: 0.6664093732833862 - trainLoss: 0.6647111773490906\n",
      "cnt: 0 - valLoss: 0.6664092540740967 - trainLoss: 0.6647111177444458\n",
      "cnt: 0 - valLoss: 0.6664091944694519 - trainLoss: 0.664711058139801\n",
      "cnt: 0 - valLoss: 0.6664090752601624 - trainLoss: 0.6647108793258667\n",
      "cnt: 0 - valLoss: 0.6664089560508728 - trainLoss: 0.6647107601165771\n",
      "cnt: 0 - valLoss: 0.6664088368415833 - trainLoss: 0.6647106409072876\n",
      "cnt: 0 - valLoss: 0.6664087772369385 - trainLoss: 0.664710521697998\n",
      "cnt: 0 - valLoss: 0.6664086580276489 - trainLoss: 0.6647104024887085\n",
      "cnt: 0 - valLoss: 0.6664085388183594 - trainLoss: 0.6647103428840637\n",
      "cnt: 0 - valLoss: 0.6664084792137146 - trainLoss: 0.6647101640701294\n",
      "cnt: 0 - valLoss: 0.6664083003997803 - trainLoss: 0.6647100448608398\n",
      "cnt: 0 - valLoss: 0.6664081811904907 - trainLoss: 0.6647099852561951\n",
      "cnt: 0 - valLoss: 0.6664081811904907 - trainLoss: 0.6647098064422607\n",
      "cnt: 0 - valLoss: 0.6664080619812012 - trainLoss: 0.6647096872329712\n",
      "cnt: 0 - valLoss: 0.6664079427719116 - trainLoss: 0.6647096276283264\n",
      "cnt: 0 - valLoss: 0.6664078235626221 - trainLoss: 0.6647095084190369\n",
      "cnt: 0 - valLoss: 0.6664077639579773 - trainLoss: 0.6647093892097473\n",
      "cnt: 0 - valLoss: 0.6664076447486877 - trainLoss: 0.664709210395813\n",
      "cnt: 0 - valLoss: 0.6664075255393982 - trainLoss: 0.6647091507911682\n",
      "cnt: 0 - valLoss: 0.6664074063301086 - trainLoss: 0.6647090315818787\n",
      "cnt: 0 - valLoss: 0.6664074063301086 - trainLoss: 0.6647089123725891\n",
      "cnt: 0 - valLoss: 0.6664072275161743 - trainLoss: 0.6647087335586548\n",
      "cnt: 0 - valLoss: 0.6664071083068848 - trainLoss: 0.66470867395401\n",
      "cnt: 0 - valLoss: 0.66640704870224 - trainLoss: 0.6647085547447205\n",
      "cnt: 0 - valLoss: 0.6664069294929504 - trainLoss: 0.6647084355354309\n",
      "cnt: 0 - valLoss: 0.6664068102836609 - trainLoss: 0.6647083163261414\n",
      "cnt: 0 - valLoss: 0.6664067506790161 - trainLoss: 0.6647081971168518\n",
      "cnt: 0 - valLoss: 0.6664066314697266 - trainLoss: 0.6647080779075623\n",
      "cnt: 0 - valLoss: 0.666406512260437 - trainLoss: 0.6647079586982727\n",
      "cnt: 0 - valLoss: 0.6664064526557922 - trainLoss: 0.6647078394889832\n",
      "cnt: 0 - valLoss: 0.6664063334465027 - trainLoss: 0.6647077202796936\n",
      "cnt: 0 - valLoss: 0.6664062142372131 - trainLoss: 0.6647076606750488\n",
      "cnt: 0 - valLoss: 0.6664061546325684 - trainLoss: 0.6647075414657593\n",
      "cnt: 0 - valLoss: 0.6664060354232788 - trainLoss: 0.6647074222564697\n",
      "cnt: 0 - valLoss: 0.6664059162139893 - trainLoss: 0.6647072434425354\n",
      "cnt: 0 - valLoss: 0.6664057970046997 - trainLoss: 0.6647071838378906\n",
      "cnt: 0 - valLoss: 0.6664057374000549 - trainLoss: 0.6647070646286011\n",
      "cnt: 0 - valLoss: 0.6664056181907654 - trainLoss: 0.6647069454193115\n",
      "cnt: 0 - valLoss: 0.6664054989814758 - trainLoss: 0.664706826210022\n",
      "cnt: 0 - valLoss: 0.666405439376831 - trainLoss: 0.6647067070007324\n",
      "cnt: 0 - valLoss: 0.6664053201675415 - trainLoss: 0.6647065877914429\n",
      "cnt: 0 - valLoss: 0.666405200958252 - trainLoss: 0.6647064685821533\n",
      "cnt: 0 - valLoss: 0.6664050817489624 - trainLoss: 0.6647063493728638\n",
      "cnt: 0 - valLoss: 0.6664050817489624 - trainLoss: 0.6647062301635742\n",
      "cnt: 0 - valLoss: 0.6664049625396729 - trainLoss: 0.6647061705589294\n",
      "cnt: 0 - valLoss: 0.6664047837257385 - trainLoss: 0.6647060513496399\n",
      "cnt: 0 - valLoss: 0.666404664516449 - trainLoss: 0.6647058725357056\n",
      "cnt: 0 - valLoss: 0.6664046049118042 - trainLoss: 0.664705753326416\n",
      "cnt: 0 - valLoss: 0.6664045453071594 - trainLoss: 0.6647056341171265\n",
      "cnt: 0 - valLoss: 0.6664044260978699 - trainLoss: 0.6647055745124817\n",
      "cnt: 0 - valLoss: 0.6664043068885803 - trainLoss: 0.6647053956985474\n",
      "cnt: 0 - valLoss: 0.6664041876792908 - trainLoss: 0.6647052764892578\n",
      "cnt: 0 - valLoss: 0.666404128074646 - trainLoss: 0.664705216884613\n",
      "cnt: 0 - valLoss: 0.6664040088653564 - trainLoss: 0.6647050380706787\n",
      "cnt: 0 - valLoss: 0.6664039492607117 - trainLoss: 0.6647049784660339\n",
      "cnt: 0 - valLoss: 0.6664038300514221 - trainLoss: 0.6647048592567444\n",
      "cnt: 0 - valLoss: 0.6664037108421326 - trainLoss: 0.6647047400474548\n",
      "cnt: 0 - valLoss: 0.666403591632843 - trainLoss: 0.6647046208381653\n",
      "cnt: 0 - valLoss: 0.6664035320281982 - trainLoss: 0.6647045016288757\n",
      "cnt: 0 - valLoss: 0.6664034128189087 - trainLoss: 0.6647043824195862\n",
      "cnt: 0 - valLoss: 0.6664032936096191 - trainLoss: 0.6647042632102966\n",
      "cnt: 0 - valLoss: 0.6664031744003296 - trainLoss: 0.6647042036056519\n",
      "cnt: 0 - valLoss: 0.6664031147956848 - trainLoss: 0.6647040843963623\n",
      "cnt: 0 - valLoss: 0.6664029955863953 - trainLoss: 0.6647040247917175\n",
      "cnt: 0 - valLoss: 0.6664028763771057 - trainLoss: 0.6647037863731384\n",
      "cnt: 0 - valLoss: 0.6664027571678162 - trainLoss: 0.6647036671638489\n",
      "cnt: 0 - valLoss: 0.6664026379585266 - trainLoss: 0.6647036075592041\n",
      "cnt: 0 - valLoss: 0.6664025783538818 - trainLoss: 0.6647034883499146\n",
      "cnt: 0 - valLoss: 0.6664025187492371 - trainLoss: 0.664703369140625\n",
      "cnt: 0 - valLoss: 0.6664023995399475 - trainLoss: 0.6647032499313354\n",
      "cnt: 0 - valLoss: 0.6664023399353027 - trainLoss: 0.6647031307220459\n",
      "cnt: 0 - valLoss: 0.6664021611213684 - trainLoss: 0.6647030115127563\n",
      "cnt: 0 - valLoss: 0.6664020419120789 - trainLoss: 0.6647029519081116\n",
      "cnt: 0 - valLoss: 0.6664019823074341 - trainLoss: 0.664702832698822\n",
      "cnt: 0 - valLoss: 0.6664019227027893 - trainLoss: 0.6647026538848877\n",
      "cnt: 0 - valLoss: 0.666401743888855 - trainLoss: 0.6647026538848877\n",
      "cnt: 0 - valLoss: 0.6664016842842102 - trainLoss: 0.6647024750709534\n",
      "cnt: 0 - valLoss: 0.6664016246795654 - trainLoss: 0.6647023558616638\n",
      "cnt: 0 - valLoss: 0.6664014458656311 - trainLoss: 0.6647022366523743\n",
      "cnt: 0 - valLoss: 0.6664013266563416 - trainLoss: 0.6647021174430847\n",
      "cnt: 0 - valLoss: 0.6664012670516968 - trainLoss: 0.6647020578384399\n",
      "cnt: 0 - valLoss: 0.6664011478424072 - trainLoss: 0.6647018790245056\n",
      "cnt: 0 - valLoss: 0.6664011478424072 - trainLoss: 0.6647017598152161\n",
      "cnt: 0 - valLoss: 0.6664009690284729 - trainLoss: 0.6647016406059265\n",
      "cnt: 0 - valLoss: 0.6664008498191833 - trainLoss: 0.664701521396637\n",
      "cnt: 0 - valLoss: 0.6664007902145386 - trainLoss: 0.6647014617919922\n",
      "cnt: 0 - valLoss: 0.666400671005249 - trainLoss: 0.6647012829780579\n",
      "cnt: 0 - valLoss: 0.6664005517959595 - trainLoss: 0.6647011637687683\n",
      "cnt: 0 - valLoss: 0.6664004325866699 - trainLoss: 0.6647010445594788\n",
      "cnt: 0 - valLoss: 0.6664003729820251 - trainLoss: 0.664700984954834\n",
      "cnt: 0 - valLoss: 0.6664002537727356 - trainLoss: 0.6647008657455444\n",
      "cnt: 0 - valLoss: 0.666400134563446 - trainLoss: 0.6647006869316101\n",
      "cnt: 0 - valLoss: 0.6664000153541565 - trainLoss: 0.6647006869316101\n",
      "cnt: 0 - valLoss: 0.6663999557495117 - trainLoss: 0.6647005081176758\n",
      "cnt: 0 - valLoss: 0.6663998365402222 - trainLoss: 0.6647003889083862\n",
      "cnt: 0 - valLoss: 0.6663997769355774 - trainLoss: 0.6647002696990967\n",
      "cnt: 0 - valLoss: 0.6663996577262878 - trainLoss: 0.6647001504898071\n",
      "cnt: 0 - valLoss: 0.6663995385169983 - trainLoss: 0.6647000908851624\n",
      "cnt: 0 - valLoss: 0.6663994193077087 - trainLoss: 0.6646999716758728\n",
      "cnt: 0 - valLoss: 0.666399359703064 - trainLoss: 0.6646997928619385\n",
      "cnt: 0 - valLoss: 0.6663993000984192 - trainLoss: 0.6646997332572937\n",
      "cnt: 0 - valLoss: 0.6663991808891296 - trainLoss: 0.6646996140480042\n",
      "cnt: 0 - valLoss: 0.6663990616798401 - trainLoss: 0.6646994948387146\n",
      "cnt: 0 - valLoss: 0.6663989424705505 - trainLoss: 0.6646993160247803\n",
      "cnt: 0 - valLoss: 0.666398823261261 - trainLoss: 0.6646993160247803\n",
      "cnt: 0 - valLoss: 0.6663987040519714 - trainLoss: 0.664699137210846\n",
      "cnt: 0 - valLoss: 0.6663987040519714 - trainLoss: 0.6646990180015564\n",
      "cnt: 0 - valLoss: 0.6663985848426819 - trainLoss: 0.6646988987922668\n",
      "cnt: 0 - valLoss: 0.6663984656333923 - trainLoss: 0.6646988391876221\n",
      "cnt: 0 - valLoss: 0.6663984060287476 - trainLoss: 0.6646987199783325\n",
      "cnt: 0 - valLoss: 0.666398286819458 - trainLoss: 0.664698600769043\n",
      "cnt: 0 - valLoss: 0.6663982272148132 - trainLoss: 0.6646984815597534\n",
      "cnt: 0 - valLoss: 0.6663981080055237 - trainLoss: 0.6646983623504639\n",
      "cnt: 0 - valLoss: 0.6663979291915894 - trainLoss: 0.6646983027458191\n",
      "cnt: 0 - valLoss: 0.6663978695869446 - trainLoss: 0.6646981835365295\n",
      "cnt: 0 - valLoss: 0.6663978099822998 - trainLoss: 0.6646981239318848\n",
      "cnt: 0 - valLoss: 0.666397750377655 - trainLoss: 0.6646979451179504\n",
      "cnt: 0 - valLoss: 0.6663976311683655 - trainLoss: 0.6646978259086609\n",
      "cnt: 0 - valLoss: 0.6663975119590759 - trainLoss: 0.6646976470947266\n",
      "cnt: 0 - valLoss: 0.6663973927497864 - trainLoss: 0.664697527885437\n",
      "cnt: 0 - valLoss: 0.6663973331451416 - trainLoss: 0.664697527885437\n",
      "cnt: 0 - valLoss: 0.6663971543312073 - trainLoss: 0.6646973490715027\n",
      "cnt: 0 - valLoss: 0.6663971543312073 - trainLoss: 0.6646972298622131\n",
      "cnt: 0 - valLoss: 0.6663970351219177 - trainLoss: 0.6646971702575684\n",
      "cnt: 0 - valLoss: 0.6663969159126282 - trainLoss: 0.6646970510482788\n",
      "cnt: 0 - valLoss: 0.6663967967033386 - trainLoss: 0.6646969318389893\n",
      "cnt: 0 - valLoss: 0.6663967370986938 - trainLoss: 0.6646968126296997\n",
      "cnt: 0 - valLoss: 0.6663966774940491 - trainLoss: 0.6646966934204102\n",
      "cnt: 0 - valLoss: 0.6663965582847595 - trainLoss: 0.6646965742111206\n",
      "cnt: 0 - valLoss: 0.66639643907547 - trainLoss: 0.664696455001831\n",
      "cnt: 0 - valLoss: 0.6663963198661804 - trainLoss: 0.6646963953971863\n",
      "cnt: 0 - valLoss: 0.6663962602615356 - trainLoss: 0.6646963357925415\n",
      "cnt: 0 - valLoss: 0.6663961410522461 - trainLoss: 0.6646961569786072\n",
      "cnt: 0 - valLoss: 0.6663960814476013 - trainLoss: 0.6646960377693176\n",
      "cnt: 0 - valLoss: 0.6663959622383118 - trainLoss: 0.6646959185600281\n",
      "cnt: 0 - valLoss: 0.666395902633667 - trainLoss: 0.6646958589553833\n",
      "cnt: 0 - valLoss: 0.6663957834243774 - trainLoss: 0.664695680141449\n",
      "cnt: 0 - valLoss: 0.6663956642150879 - trainLoss: 0.6646956205368042\n",
      "cnt: 0 - valLoss: 0.6663955450057983 - trainLoss: 0.6646955013275146\n",
      "cnt: 0 - valLoss: 0.6663954854011536 - trainLoss: 0.6646953821182251\n",
      "cnt: 0 - valLoss: 0.666395366191864 - trainLoss: 0.6646952629089355\n",
      "cnt: 0 - valLoss: 0.6663953065872192 - trainLoss: 0.664695143699646\n",
      "cnt: 0 - valLoss: 0.6663951873779297 - trainLoss: 0.6646950244903564\n",
      "cnt: 0 - valLoss: 0.6663950681686401 - trainLoss: 0.6646949648857117\n",
      "cnt: 0 - valLoss: 0.6663950085639954 - trainLoss: 0.6646947860717773\n",
      "cnt: 0 - valLoss: 0.6663948893547058 - trainLoss: 0.6646946668624878\n",
      "cnt: 0 - valLoss: 0.6663947701454163 - trainLoss: 0.6646945476531982\n",
      "cnt: 0 - valLoss: 0.6663947105407715 - trainLoss: 0.6646944880485535\n",
      "cnt: 0 - valLoss: 0.6663945913314819 - trainLoss: 0.6646943688392639\n",
      "cnt: 0 - valLoss: 0.6663944721221924 - trainLoss: 0.6646941900253296\n",
      "cnt: 0 - valLoss: 0.6663944125175476 - trainLoss: 0.6646941304206848\n",
      "cnt: 0 - valLoss: 0.6663942933082581 - trainLoss: 0.6646940112113953\n",
      "cnt: 0 - valLoss: 0.6663942337036133 - trainLoss: 0.6646938920021057\n",
      "cnt: 0 - valLoss: 0.6663941144943237 - trainLoss: 0.6646938323974609\n",
      "cnt: 0 - valLoss: 0.6663939952850342 - trainLoss: 0.6646937131881714\n",
      "cnt: 0 - valLoss: 0.6663939356803894 - trainLoss: 0.6646935939788818\n",
      "cnt: 0 - valLoss: 0.6663938164710999 - trainLoss: 0.6646934747695923\n",
      "cnt: 0 - valLoss: 0.6663936972618103 - trainLoss: 0.6646933555603027\n",
      "cnt: 0 - valLoss: 0.6663935780525208 - trainLoss: 0.664693295955658\n",
      "cnt: 0 - valLoss: 0.666393518447876 - trainLoss: 0.6646931171417236\n",
      "cnt: 0 - valLoss: 0.6663934588432312 - trainLoss: 0.6646930575370789\n",
      "cnt: 0 - valLoss: 0.6663933396339417 - trainLoss: 0.6646929383277893\n",
      "cnt: 0 - valLoss: 0.6663932204246521 - trainLoss: 0.6646928191184998\n",
      "cnt: 0 - valLoss: 0.6663931012153625 - trainLoss: 0.6646926403045654\n",
      "cnt: 0 - valLoss: 0.6663930416107178 - trainLoss: 0.6646925806999207\n",
      "cnt: 0 - valLoss: 0.666392982006073 - trainLoss: 0.6646924614906311\n",
      "cnt: 0 - valLoss: 0.6663928031921387 - trainLoss: 0.6646923422813416\n",
      "cnt: 0 - valLoss: 0.6663927435874939 - trainLoss: 0.664692223072052\n",
      "cnt: 0 - valLoss: 0.6663926839828491 - trainLoss: 0.6646921634674072\n",
      "cnt: 0 - valLoss: 0.6663925647735596 - trainLoss: 0.6646920442581177\n",
      "cnt: 0 - valLoss: 0.66639244556427 - trainLoss: 0.6646919250488281\n",
      "cnt: 0 - valLoss: 0.6663923859596252 - trainLoss: 0.6646918058395386\n",
      "cnt: 0 - valLoss: 0.6663922667503357 - trainLoss: 0.664691686630249\n",
      "cnt: 0 - valLoss: 0.6663922071456909 - trainLoss: 0.6646916270256042\n",
      "cnt: 0 - valLoss: 0.6663920879364014 - trainLoss: 0.6646915078163147\n",
      "cnt: 0 - valLoss: 0.6663919687271118 - trainLoss: 0.6646913290023804\n",
      "cnt: 0 - valLoss: 0.666391909122467 - trainLoss: 0.6646912693977356\n",
      "cnt: 0 - valLoss: 0.6663917899131775 - trainLoss: 0.664691150188446\n",
      "cnt: 0 - valLoss: 0.6663916707038879 - trainLoss: 0.6646910309791565\n",
      "cnt: 0 - valLoss: 0.6663916110992432 - trainLoss: 0.6646909117698669\n",
      "cnt: 0 - valLoss: 0.6663915514945984 - trainLoss: 0.6646907925605774\n",
      "cnt: 0 - valLoss: 0.6663914322853088 - trainLoss: 0.6646907329559326\n",
      "cnt: 0 - valLoss: 0.6663913130760193 - trainLoss: 0.6646906137466431\n",
      "cnt: 0 - valLoss: 0.6663911938667297 - trainLoss: 0.6646904945373535\n",
      "cnt: 0 - valLoss: 0.6663910746574402 - trainLoss: 0.664690375328064\n",
      "cnt: 0 - valLoss: 0.6663910150527954 - trainLoss: 0.6646902561187744\n",
      "cnt: 0 - valLoss: 0.6663908958435059 - trainLoss: 0.6646901965141296\n",
      "cnt: 0 - valLoss: 0.6663908958435059 - trainLoss: 0.6646900773048401\n",
      "cnt: 0 - valLoss: 0.6663907766342163 - trainLoss: 0.6646899580955505\n",
      "cnt: 0 - valLoss: 0.6663906574249268 - trainLoss: 0.6646897792816162\n",
      "cnt: 0 - valLoss: 0.6663905382156372 - trainLoss: 0.6646897196769714\n",
      "cnt: 0 - valLoss: 0.6663904190063477 - trainLoss: 0.6646896004676819\n",
      "cnt: 0 - valLoss: 0.6663904190063477 - trainLoss: 0.6646894812583923\n",
      "cnt: 0 - valLoss: 0.6663902401924133 - trainLoss: 0.6646893620491028\n",
      "cnt: 0 - valLoss: 0.6663901805877686 - trainLoss: 0.6646892428398132\n",
      "cnt: 0 - valLoss: 0.666390061378479 - trainLoss: 0.6646891832351685\n",
      "cnt: 0 - valLoss: 0.6663900017738342 - trainLoss: 0.6646890044212341\n",
      "cnt: 0 - valLoss: 0.6663898229598999 - trainLoss: 0.6646889448165894\n",
      "cnt: 0 - valLoss: 0.6663898229598999 - trainLoss: 0.6646888852119446\n",
      "cnt: 0 - valLoss: 0.6663897037506104 - trainLoss: 0.664688766002655\n",
      "cnt: 0 - valLoss: 0.6663895845413208 - trainLoss: 0.6646885871887207\n",
      "cnt: 0 - valLoss: 0.666389524936676 - trainLoss: 0.6646885871887207\n",
      "cnt: 0 - valLoss: 0.6663894057273865 - trainLoss: 0.6646884083747864\n",
      "cnt: 0 - valLoss: 0.6663893461227417 - trainLoss: 0.6646882891654968\n",
      "cnt: 0 - valLoss: 0.6663892269134521 - trainLoss: 0.6646881699562073\n",
      "cnt: 0 - valLoss: 0.6663891077041626 - trainLoss: 0.6646880507469177\n",
      "cnt: 0 - valLoss: 0.6663890480995178 - trainLoss: 0.664687991142273\n",
      "cnt: 0 - valLoss: 0.6663889288902283 - trainLoss: 0.6646878719329834\n",
      "cnt: 0 - valLoss: 0.6663888096809387 - trainLoss: 0.6646877527236938\n",
      "cnt: 0 - valLoss: 0.666388750076294 - trainLoss: 0.6646876335144043\n",
      "cnt: 0 - valLoss: 0.6663886308670044 - trainLoss: 0.6646875143051147\n",
      "cnt: 0 - valLoss: 0.6663885712623596 - trainLoss: 0.6646873950958252\n",
      "cnt: 0 - valLoss: 0.6663884520530701 - trainLoss: 0.6646873354911804\n",
      "cnt: 0 - valLoss: 0.6663883924484253 - trainLoss: 0.6646872162818909\n",
      "cnt: 0 - valLoss: 0.6663882732391357 - trainLoss: 0.6646870374679565\n",
      "cnt: 0 - valLoss: 0.6663881540298462 - trainLoss: 0.6646869778633118\n",
      "cnt: 0 - valLoss: 0.6663880944252014 - trainLoss: 0.6646868586540222\n",
      "cnt: 0 - valLoss: 0.6663880348205566 - trainLoss: 0.6646867394447327\n",
      "cnt: 0 - valLoss: 0.6663878560066223 - trainLoss: 0.6646866798400879\n",
      "cnt: 0 - valLoss: 0.6663877964019775 - trainLoss: 0.6646865606307983\n",
      "cnt: 0 - valLoss: 0.6663877367973328 - trainLoss: 0.6646864414215088\n",
      "cnt: 0 - valLoss: 0.6663876175880432 - trainLoss: 0.6646863222122192\n",
      "cnt: 0 - valLoss: 0.6663874983787537 - trainLoss: 0.6646862626075745\n",
      "cnt: 0 - valLoss: 0.6663874387741089 - trainLoss: 0.6646861433982849\n",
      "cnt: 0 - valLoss: 0.6663873195648193 - trainLoss: 0.6646859645843506\n",
      "cnt: 0 - valLoss: 0.6663872599601746 - trainLoss: 0.664685845375061\n",
      "cnt: 0 - valLoss: 0.6663872003555298 - trainLoss: 0.6646857857704163\n",
      "cnt: 0 - valLoss: 0.6663870811462402 - trainLoss: 0.6646856665611267\n",
      "cnt: 0 - valLoss: 0.6663869619369507 - trainLoss: 0.6646855473518372\n",
      "cnt: 0 - valLoss: 0.6663869023323059 - trainLoss: 0.6646854281425476\n",
      "cnt: 0 - valLoss: 0.6663867831230164 - trainLoss: 0.6646853685379028\n",
      "cnt: 0 - valLoss: 0.6663867235183716 - trainLoss: 0.6646852493286133\n",
      "cnt: 0 - valLoss: 0.6663865447044373 - trainLoss: 0.664685070514679\n",
      "cnt: 0 - valLoss: 0.6663865447044373 - trainLoss: 0.6646850109100342\n",
      "cnt: 0 - valLoss: 0.6663864254951477 - trainLoss: 0.6646849513053894\n",
      "cnt: 0 - valLoss: 0.6663863658905029 - trainLoss: 0.6646847724914551\n",
      "cnt: 0 - valLoss: 0.6663861870765686 - trainLoss: 0.6646846532821655\n",
      "cnt: 0 - valLoss: 0.6663861274719238 - trainLoss: 0.6646845936775208\n",
      "cnt: 0 - valLoss: 0.6663860082626343 - trainLoss: 0.6646844744682312\n",
      "cnt: 0 - valLoss: 0.6663859486579895 - trainLoss: 0.6646842956542969\n",
      "cnt: 0 - valLoss: 0.6663858890533447 - trainLoss: 0.6646842956542969\n",
      "cnt: 0 - valLoss: 0.6663857698440552 - trainLoss: 0.6646841764450073\n",
      "cnt: 0 - valLoss: 0.6663856506347656 - trainLoss: 0.664683997631073\n",
      "cnt: 0 - valLoss: 0.6663855910301208 - trainLoss: 0.6646839380264282\n",
      "cnt: 0 - valLoss: 0.6663854718208313 - trainLoss: 0.6646838784217834\n",
      "cnt: 0 - valLoss: 0.6663853526115417 - trainLoss: 0.6646836996078491\n",
      "cnt: 0 - valLoss: 0.666385293006897 - trainLoss: 0.6646835803985596\n",
      "cnt: 0 - valLoss: 0.6663851737976074 - trainLoss: 0.6646835207939148\n",
      "cnt: 0 - valLoss: 0.6663851141929626 - trainLoss: 0.6646834015846252\n",
      "cnt: 0 - valLoss: 0.6663849949836731 - trainLoss: 0.6646832227706909\n",
      "cnt: 0 - valLoss: 0.6663848757743835 - trainLoss: 0.6646831631660461\n",
      "cnt: 0 - valLoss: 0.6663848161697388 - trainLoss: 0.6646830439567566\n",
      "cnt: 0 - valLoss: 0.6663846969604492 - trainLoss: 0.664682924747467\n",
      "cnt: 0 - valLoss: 0.6663846373558044 - trainLoss: 0.6646828055381775\n",
      "cnt: 0 - valLoss: 0.6663845181465149 - trainLoss: 0.6646827459335327\n",
      "cnt: 0 - valLoss: 0.6663844585418701 - trainLoss: 0.6646826267242432\n",
      "cnt: 0 - valLoss: 0.6663843393325806 - trainLoss: 0.6646825075149536\n",
      "cnt: 0 - valLoss: 0.6663842797279358 - trainLoss: 0.6646823287010193\n",
      "cnt: 0 - valLoss: 0.6663841605186462 - trainLoss: 0.6646822690963745\n",
      "cnt: 0 - valLoss: 0.6663840413093567 - trainLoss: 0.6646822094917297\n",
      "cnt: 0 - valLoss: 0.6663839221000671 - trainLoss: 0.6646820306777954\n",
      "cnt: 0 - valLoss: 0.6663838624954224 - trainLoss: 0.6646819114685059\n",
      "cnt: 0 - valLoss: 0.6663838028907776 - trainLoss: 0.6646817922592163\n",
      "cnt: 0 - valLoss: 0.6663837432861328 - trainLoss: 0.6646817326545715\n",
      "cnt: 0 - valLoss: 0.6663835644721985 - trainLoss: 0.6646815538406372\n",
      "cnt: 0 - valLoss: 0.6663834452629089 - trainLoss: 0.6646814942359924\n",
      "cnt: 0 - valLoss: 0.6663834452629089 - trainLoss: 0.6646814346313477\n",
      "cnt: 0 - valLoss: 0.6663832664489746 - trainLoss: 0.6646812558174133\n",
      "cnt: 0 - valLoss: 0.6663832068443298 - trainLoss: 0.6646812558174133\n",
      "cnt: 0 - valLoss: 0.6663831472396851 - trainLoss: 0.6646810173988342\n",
      "cnt: 0 - valLoss: 0.6663830280303955 - trainLoss: 0.6646809577941895\n",
      "cnt: 0 - valLoss: 0.6663829684257507 - trainLoss: 0.6646808385848999\n",
      "cnt: 0 - valLoss: 0.6663827896118164 - trainLoss: 0.6646807193756104\n",
      "cnt: 0 - valLoss: 0.6663827300071716 - trainLoss: 0.6646806597709656\n",
      "cnt: 0 - valLoss: 0.6663826704025269 - trainLoss: 0.664680540561676\n",
      "cnt: 0 - valLoss: 0.6663825511932373 - trainLoss: 0.6646804213523865\n",
      "cnt: 0 - valLoss: 0.6663824319839478 - trainLoss: 0.6646803021430969\n",
      "cnt: 0 - valLoss: 0.666382372379303 - trainLoss: 0.6646802425384521\n",
      "cnt: 0 - valLoss: 0.6663822531700134 - trainLoss: 0.6646801233291626\n",
      "cnt: 0 - valLoss: 0.6663821935653687 - trainLoss: 0.6646799445152283\n",
      "cnt: 0 - valLoss: 0.6663820743560791 - trainLoss: 0.6646798849105835\n",
      "cnt: 0 - valLoss: 0.6663820147514343 - trainLoss: 0.664679765701294\n",
      "cnt: 0 - valLoss: 0.6663818955421448 - trainLoss: 0.6646796464920044\n",
      "cnt: 0 - valLoss: 0.6663818359375 - trainLoss: 0.6646795272827148\n",
      "cnt: 0 - valLoss: 0.6663817167282104 - trainLoss: 0.6646794080734253\n",
      "cnt: 0 - valLoss: 0.6663815975189209 - trainLoss: 0.6646792888641357\n",
      "cnt: 0 - valLoss: 0.6663815379142761 - trainLoss: 0.664679229259491\n",
      "cnt: 0 - valLoss: 0.6663814187049866 - trainLoss: 0.6646791100502014\n",
      "cnt: 0 - valLoss: 0.666381299495697 - trainLoss: 0.6646789908409119\n",
      "cnt: 0 - valLoss: 0.6663812398910522 - trainLoss: 0.6646788716316223\n",
      "cnt: 0 - valLoss: 0.6663811206817627 - trainLoss: 0.6646788120269775\n",
      "cnt: 0 - valLoss: 0.6663810610771179 - trainLoss: 0.6646786332130432\n",
      "cnt: 0 - valLoss: 0.6663810014724731 - trainLoss: 0.6646785736083984\n",
      "cnt: 0 - valLoss: 0.6663808226585388 - trainLoss: 0.6646783947944641\n",
      "cnt: 0 - valLoss: 0.6663808226585388 - trainLoss: 0.6646783351898193\n",
      "cnt: 0 - valLoss: 0.6663806438446045 - trainLoss: 0.6646782159805298\n",
      "cnt: 0 - valLoss: 0.6663805246353149 - trainLoss: 0.6646780967712402\n",
      "cnt: 0 - valLoss: 0.6663804650306702 - trainLoss: 0.6646780371665955\n",
      "cnt: 0 - valLoss: 0.6663803458213806 - trainLoss: 0.6646779179573059\n",
      "cnt: 0 - valLoss: 0.6663802862167358 - trainLoss: 0.6646777987480164\n",
      "cnt: 0 - valLoss: 0.6663801670074463 - trainLoss: 0.6646777391433716\n",
      "cnt: 0 - valLoss: 0.6663801074028015 - trainLoss: 0.664677619934082\n",
      "cnt: 0 - valLoss: 0.666379988193512 - trainLoss: 0.6646775007247925\n",
      "cnt: 0 - valLoss: 0.6663798689842224 - trainLoss: 0.6646773815155029\n",
      "cnt: 0 - valLoss: 0.6663797497749329 - trainLoss: 0.6646773219108582\n",
      "cnt: 0 - valLoss: 0.6663796901702881 - trainLoss: 0.6646771430969238\n",
      "cnt: 0 - valLoss: 0.6663796305656433 - trainLoss: 0.664677083492279\n",
      "cnt: 0 - valLoss: 0.6663795113563538 - trainLoss: 0.6646769046783447\n",
      "cnt: 0 - valLoss: 0.666379451751709 - trainLoss: 0.6646769046783447\n",
      "cnt: 0 - valLoss: 0.6663793325424194 - trainLoss: 0.6646767258644104\n",
      "cnt: 0 - valLoss: 0.6663792133331299 - trainLoss: 0.6646766066551208\n",
      "cnt: 0 - valLoss: 0.6663791537284851 - trainLoss: 0.6646764874458313\n",
      "cnt: 0 - valLoss: 0.6663790345191956 - trainLoss: 0.6646764278411865\n",
      "cnt: 0 - valLoss: 0.666378915309906 - trainLoss: 0.664676308631897\n",
      "cnt: 0 - valLoss: 0.6663788557052612 - trainLoss: 0.6646761894226074\n",
      "cnt: 0 - valLoss: 0.6663787364959717 - trainLoss: 0.6646761298179626\n",
      "cnt: 0 - valLoss: 0.6663786768913269 - trainLoss: 0.6646759510040283\n",
      "cnt: 0 - valLoss: 0.6663785576820374 - trainLoss: 0.6646758317947388\n",
      "cnt: 0 - valLoss: 0.6663784384727478 - trainLoss: 0.664675772190094\n",
      "cnt: 0 - valLoss: 0.666378378868103 - trainLoss: 0.6646755933761597\n",
      "cnt: 0 - valLoss: 0.6663782596588135 - trainLoss: 0.6646755337715149\n",
      "cnt: 0 - valLoss: 0.6663781404495239 - trainLoss: 0.6646754145622253\n",
      "cnt: 0 - valLoss: 0.6663780808448792 - trainLoss: 0.6646753549575806\n",
      "cnt: 0 - valLoss: 0.6663779616355896 - trainLoss: 0.6646751761436462\n",
      "cnt: 0 - valLoss: 0.6663778424263 - trainLoss: 0.6646750569343567\n",
      "cnt: 0 - valLoss: 0.6663777828216553 - trainLoss: 0.6646749973297119\n",
      "cnt: 0 - valLoss: 0.6663777232170105 - trainLoss: 0.6646748781204224\n",
      "cnt: 0 - valLoss: 0.6663776636123657 - trainLoss: 0.6646748185157776\n",
      "cnt: 0 - valLoss: 0.6663774847984314 - trainLoss: 0.6646746397018433\n",
      "cnt: 0 - valLoss: 0.6663774251937866 - trainLoss: 0.6646745800971985\n",
      "cnt: 0 - valLoss: 0.6663773059844971 - trainLoss: 0.6646744608879089\n",
      "cnt: 0 - valLoss: 0.6663771867752075 - trainLoss: 0.6646743416786194\n",
      "cnt: 0 - valLoss: 0.6663771271705627 - trainLoss: 0.6646741628646851\n",
      "cnt: 0 - valLoss: 0.6663770079612732 - trainLoss: 0.6646741628646851\n",
      "cnt: 0 - valLoss: 0.6663768887519836 - trainLoss: 0.6646739840507507\n",
      "cnt: 0 - valLoss: 0.6663768291473389 - trainLoss: 0.6646738648414612\n",
      "cnt: 0 - valLoss: 0.6663767099380493 - trainLoss: 0.6646738052368164\n",
      "cnt: 0 - valLoss: 0.6663765907287598 - trainLoss: 0.6646736860275269\n",
      "cnt: 0 - valLoss: 0.666376531124115 - trainLoss: 0.6646736264228821\n",
      "cnt: 0 - valLoss: 0.6663764715194702 - trainLoss: 0.6646735072135925\n",
      "cnt: 0 - valLoss: 0.6663763523101807 - trainLoss: 0.664673388004303\n",
      "cnt: 0 - valLoss: 0.6663762927055359 - trainLoss: 0.6646732687950134\n",
      "cnt: 0 - valLoss: 0.6663761734962463 - trainLoss: 0.6646732091903687\n",
      "cnt: 0 - valLoss: 0.6663760542869568 - trainLoss: 0.6646730899810791\n",
      "cnt: 0 - valLoss: 0.666375994682312 - trainLoss: 0.6646729111671448\n",
      "cnt: 0 - valLoss: 0.6663759350776672 - trainLoss: 0.6646728515625\n",
      "cnt: 0 - valLoss: 0.6663758158683777 - trainLoss: 0.6646727323532104\n",
      "cnt: 0 - valLoss: 0.6663757562637329 - trainLoss: 0.6646726131439209\n",
      "cnt: 0 - valLoss: 0.6663756370544434 - trainLoss: 0.6646724939346313\n",
      "cnt: 0 - valLoss: 0.6663755178451538 - trainLoss: 0.6646724343299866\n",
      "cnt: 0 - valLoss: 0.6663753986358643 - trainLoss: 0.664672315120697\n",
      "cnt: 0 - valLoss: 0.6663752794265747 - trainLoss: 0.6646721959114075\n",
      "cnt: 0 - valLoss: 0.6663752198219299 - trainLoss: 0.6646720767021179\n",
      "cnt: 0 - valLoss: 0.6663751602172852 - trainLoss: 0.6646720170974731\n",
      "cnt: 0 - valLoss: 0.6663750410079956 - trainLoss: 0.6646718382835388\n",
      "cnt: 0 - valLoss: 0.666374921798706 - trainLoss: 0.6646717190742493\n",
      "cnt: 0 - valLoss: 0.6663748621940613 - trainLoss: 0.6646716594696045\n",
      "cnt: 0 - valLoss: 0.6663747429847717 - trainLoss: 0.6646715402603149\n",
      "cnt: 0 - valLoss: 0.666374683380127 - trainLoss: 0.6646714210510254\n",
      "cnt: 0 - valLoss: 0.6663745045661926 - trainLoss: 0.6646713614463806\n",
      "cnt: 0 - valLoss: 0.6663744449615479 - trainLoss: 0.6646712422370911\n",
      "cnt: 0 - valLoss: 0.6663743853569031 - trainLoss: 0.6646711230278015\n",
      "cnt: 0 - valLoss: 0.6663742661476135 - trainLoss: 0.6646710634231567\n",
      "cnt: 0 - valLoss: 0.666374146938324 - trainLoss: 0.6646708846092224\n",
      "cnt: 0 - valLoss: 0.6663740873336792 - trainLoss: 0.6646708250045776\n",
      "cnt: 0 - valLoss: 0.6663739681243896 - trainLoss: 0.6646707057952881\n",
      "cnt: 0 - valLoss: 0.6663738489151001 - trainLoss: 0.6646705269813538\n",
      "cnt: 0 - valLoss: 0.6663737893104553 - trainLoss: 0.6646705269813538\n",
      "cnt: 0 - valLoss: 0.6663736701011658 - trainLoss: 0.6646703481674194\n",
      "cnt: 0 - valLoss: 0.666373610496521 - trainLoss: 0.6646702885627747\n",
      "cnt: 0 - valLoss: 0.6663734912872314 - trainLoss: 0.6646701097488403\n",
      "cnt: 0 - valLoss: 0.6663733720779419 - trainLoss: 0.6646700501441956\n",
      "cnt: 0 - valLoss: 0.6663733124732971 - trainLoss: 0.6646699905395508\n",
      "cnt: 0 - valLoss: 0.6663731932640076 - trainLoss: 0.6646698117256165\n",
      "cnt: 0 - valLoss: 0.6663731336593628 - trainLoss: 0.6646697521209717\n",
      "cnt: 0 - valLoss: 0.6663730144500732 - trainLoss: 0.6646695733070374\n",
      "cnt: 0 - valLoss: 0.6663728952407837 - trainLoss: 0.6646695137023926\n",
      "cnt: 0 - valLoss: 0.6663727760314941 - trainLoss: 0.6646694540977478\n",
      "cnt: 0 - valLoss: 0.6663727760314941 - trainLoss: 0.6646692752838135\n",
      "cnt: 0 - valLoss: 0.6663725972175598 - trainLoss: 0.6646692156791687\n",
      "cnt: 0 - valLoss: 0.666372537612915 - trainLoss: 0.6646690964698792\n",
      "cnt: 0 - valLoss: 0.6663724184036255 - trainLoss: 0.6646689176559448\n",
      "cnt: 0 - valLoss: 0.6663722991943359 - trainLoss: 0.6646688580513\n",
      "cnt: 0 - valLoss: 0.6663722395896912 - trainLoss: 0.6646687984466553\n",
      "cnt: 0 - valLoss: 0.6663721203804016 - trainLoss: 0.6646686792373657\n",
      "cnt: 0 - valLoss: 0.6663720607757568 - trainLoss: 0.6646685600280762\n",
      "cnt: 0 - valLoss: 0.6663720011711121 - trainLoss: 0.6646684408187866\n",
      "cnt: 0 - valLoss: 0.6663718223571777 - trainLoss: 0.6646683812141418\n",
      "cnt: 0 - valLoss: 0.666371762752533 - trainLoss: 0.6646682024002075\n",
      "cnt: 0 - valLoss: 0.6663717031478882 - trainLoss: 0.6646681427955627\n",
      "cnt: 0 - valLoss: 0.6663715839385986 - trainLoss: 0.6646680235862732\n",
      "cnt: 0 - valLoss: 0.6663714647293091 - trainLoss: 0.6646679043769836\n",
      "cnt: 0 - valLoss: 0.6663714051246643 - trainLoss: 0.6646677851676941\n",
      "cnt: 0 - valLoss: 0.6663713455200195 - trainLoss: 0.6646677255630493\n",
      "cnt: 0 - valLoss: 0.66637122631073 - trainLoss: 0.6646676063537598\n",
      "cnt: 0 - valLoss: 0.6663711071014404 - trainLoss: 0.6646674871444702\n",
      "cnt: 0 - valLoss: 0.6663709878921509 - trainLoss: 0.6646673679351807\n",
      "cnt: 0 - valLoss: 0.6663709282875061 - trainLoss: 0.6646673083305359\n",
      "cnt: 0 - valLoss: 0.6663707494735718 - trainLoss: 0.6646671295166016\n",
      "cnt: 0 - valLoss: 0.6663707494735718 - trainLoss: 0.664667010307312\n",
      "cnt: 0 - valLoss: 0.6663706302642822 - trainLoss: 0.6646669507026672\n",
      "cnt: 0 - valLoss: 0.6663705706596375 - trainLoss: 0.6646668314933777\n",
      "cnt: 0 - valLoss: 0.6663704514503479 - trainLoss: 0.6646667122840881\n",
      "cnt: 0 - valLoss: 0.6663703322410583 - trainLoss: 0.6646665930747986\n",
      "cnt: 0 - valLoss: 0.6663702130317688 - trainLoss: 0.6646665334701538\n",
      "cnt: 0 - valLoss: 0.666370153427124 - trainLoss: 0.6646663546562195\n",
      "cnt: 0 - valLoss: 0.6663700342178345 - trainLoss: 0.6646662950515747\n",
      "cnt: 0 - valLoss: 0.6663699746131897 - trainLoss: 0.6646661758422852\n",
      "cnt: 0 - valLoss: 0.6663698554039001 - trainLoss: 0.6646661162376404\n",
      "cnt: 0 - valLoss: 0.6663696765899658 - trainLoss: 0.664665937423706\n",
      "cnt: 0 - valLoss: 0.6663696765899658 - trainLoss: 0.6646658182144165\n",
      "cnt: 0 - valLoss: 0.6663695573806763 - trainLoss: 0.664665699005127\n",
      "cnt: 0 - valLoss: 0.6663694977760315 - trainLoss: 0.6646656394004822\n",
      "cnt: 0 - valLoss: 0.6663693785667419 - trainLoss: 0.6646655201911926\n",
      "cnt: 0 - valLoss: 0.6663692593574524 - trainLoss: 0.6646654009819031\n",
      "cnt: 0 - valLoss: 0.6663691401481628 - trainLoss: 0.6646653413772583\n",
      "cnt: 0 - valLoss: 0.6663690805435181 - trainLoss: 0.6646652221679688\n",
      "cnt: 0 - valLoss: 0.6663689613342285 - trainLoss: 0.664665162563324\n",
      "cnt: 0 - valLoss: 0.6663689017295837 - trainLoss: 0.6646650433540344\n",
      "cnt: 0 - valLoss: 0.6663687825202942 - trainLoss: 0.6646648645401001\n",
      "cnt: 0 - valLoss: 0.6663686633110046 - trainLoss: 0.6646648049354553\n",
      "cnt: 0 - valLoss: 0.6663686037063599 - trainLoss: 0.6646647453308105\n",
      "cnt: 0 - valLoss: 0.6663685441017151 - trainLoss: 0.6646645665168762\n",
      "cnt: 0 - valLoss: 0.6663684248924255 - trainLoss: 0.6646644473075867\n",
      "cnt: 0 - valLoss: 0.666368305683136 - trainLoss: 0.6646643280982971\n",
      "cnt: 0 - valLoss: 0.6663682460784912 - trainLoss: 0.6646642684936523\n",
      "cnt: 0 - valLoss: 0.6663681268692017 - trainLoss: 0.6646641492843628\n",
      "cnt: 0 - valLoss: 0.6663680076599121 - trainLoss: 0.6646640300750732\n",
      "cnt: 0 - valLoss: 0.6663679480552673 - trainLoss: 0.6646639704704285\n",
      "cnt: 0 - valLoss: 0.6663678884506226 - trainLoss: 0.6646638512611389\n",
      "cnt: 0 - valLoss: 0.666367769241333 - trainLoss: 0.6646637916564941\n",
      "cnt: 0 - valLoss: 0.6663676500320435 - trainLoss: 0.6646636128425598\n",
      "cnt: 0 - valLoss: 0.6663675904273987 - trainLoss: 0.6646634936332703\n",
      "cnt: 0 - valLoss: 0.6663674116134644 - trainLoss: 0.6646634340286255\n",
      "cnt: 0 - valLoss: 0.6663673520088196 - trainLoss: 0.6646632552146912\n",
      "cnt: 0 - valLoss: 0.6663672924041748 - trainLoss: 0.6646631956100464\n",
      "cnt: 0 - valLoss: 0.6663671731948853 - trainLoss: 0.6646630764007568\n",
      "cnt: 0 - valLoss: 0.6663670539855957 - trainLoss: 0.6646630167961121\n",
      "cnt: 0 - valLoss: 0.6663669943809509 - trainLoss: 0.6646628379821777\n",
      "cnt: 0 - valLoss: 0.6663668751716614 - trainLoss: 0.664662778377533\n",
      "cnt: 0 - valLoss: 0.6663667559623718 - trainLoss: 0.6646626591682434\n",
      "cnt: 0 - valLoss: 0.6663666367530823 - trainLoss: 0.6646625399589539\n",
      "cnt: 0 - valLoss: 0.6663665771484375 - trainLoss: 0.6646624803543091\n",
      "cnt: 0 - valLoss: 0.6663665175437927 - trainLoss: 0.6646623015403748\n",
      "cnt: 0 - valLoss: 0.6663663983345032 - trainLoss: 0.6646621823310852\n",
      "cnt: 0 - valLoss: 0.6663663387298584 - trainLoss: 0.6646620631217957\n",
      "cnt: 0 - valLoss: 0.6663662195205688 - trainLoss: 0.6646620035171509\n",
      "cnt: 0 - valLoss: 0.6663661003112793 - trainLoss: 0.6646619439125061\n",
      "cnt: 0 - valLoss: 0.6663660407066345 - trainLoss: 0.6646617650985718\n",
      "cnt: 0 - valLoss: 0.6663658618927002 - trainLoss: 0.6646616458892822\n",
      "cnt: 0 - valLoss: 0.6663658022880554 - trainLoss: 0.6646615266799927\n",
      "cnt: 0 - valLoss: 0.6663657426834106 - trainLoss: 0.6646614670753479\n",
      "cnt: 0 - valLoss: 0.6663656830787659 - trainLoss: 0.6646614074707031\n",
      "cnt: 0 - valLoss: 0.6663655638694763 - trainLoss: 0.6646612286567688\n",
      "cnt: 0 - valLoss: 0.6663654446601868 - trainLoss: 0.6646611094474792\n",
      "cnt: 0 - valLoss: 0.666365385055542 - trainLoss: 0.6646609902381897\n",
      "cnt: 0 - valLoss: 0.6663652658462524 - trainLoss: 0.6646609306335449\n",
      "cnt: 0 - valLoss: 0.6663651466369629 - trainLoss: 0.6646607518196106\n",
      "cnt: 0 - valLoss: 0.6663650274276733 - trainLoss: 0.6646606922149658\n",
      "cnt: 0 - valLoss: 0.6663649678230286 - trainLoss: 0.664660632610321\n",
      "cnt: 0 - valLoss: 0.666364848613739 - trainLoss: 0.6646605134010315\n",
      "cnt: 0 - valLoss: 0.6663647890090942 - trainLoss: 0.6646603941917419\n",
      "cnt: 0 - valLoss: 0.6663646697998047 - trainLoss: 0.6646603345870972\n",
      "cnt: 0 - valLoss: 0.6663645505905151 - trainLoss: 0.6646601557731628\n",
      "cnt: 0 - valLoss: 0.6663644909858704 - trainLoss: 0.6646600365638733\n",
      "cnt: 0 - valLoss: 0.6663643717765808 - trainLoss: 0.6646599173545837\n",
      "cnt: 0 - valLoss: 0.666364312171936 - trainLoss: 0.664659857749939\n",
      "cnt: 0 - valLoss: 0.6663641929626465 - trainLoss: 0.6646597385406494\n",
      "cnt: 0 - valLoss: 0.6663640737533569 - trainLoss: 0.6646596789360046\n",
      "cnt: 0 - valLoss: 0.6663640141487122 - trainLoss: 0.6646595597267151\n",
      "cnt: 0 - valLoss: 0.6663638949394226 - trainLoss: 0.6646594405174255\n",
      "cnt: 0 - valLoss: 0.6663637757301331 - trainLoss: 0.664659321308136\n",
      "cnt: 0 - valLoss: 0.6663636565208435 - trainLoss: 0.6646592020988464\n",
      "cnt: 0 - valLoss: 0.6663635969161987 - trainLoss: 0.6646590828895569\n",
      "cnt: 0 - valLoss: 0.6663634777069092 - trainLoss: 0.6646589636802673\n",
      "cnt: 0 - valLoss: 0.6663634181022644 - trainLoss: 0.6646589040756226\n",
      "cnt: 0 - valLoss: 0.6663632988929749 - trainLoss: 0.664658784866333\n",
      "cnt: 0 - valLoss: 0.6663631796836853 - trainLoss: 0.6646587252616882\n",
      "cnt: 0 - valLoss: 0.666363000869751 - trainLoss: 0.6646585464477539\n",
      "cnt: 0 - valLoss: 0.6663629412651062 - trainLoss: 0.6646584868431091\n",
      "cnt: 0 - valLoss: 0.6663628220558167 - trainLoss: 0.6646583676338196\n",
      "cnt: 0 - valLoss: 0.6663627624511719 - trainLoss: 0.6646583080291748\n",
      "cnt: 0 - valLoss: 0.6663626432418823 - trainLoss: 0.6646581292152405\n",
      "cnt: 0 - valLoss: 0.6663625240325928 - trainLoss: 0.6646580696105957\n",
      "cnt: 0 - valLoss: 0.6663624048233032 - trainLoss: 0.6646579504013062\n",
      "cnt: 0 - valLoss: 0.6663623452186584 - trainLoss: 0.6646578907966614\n",
      "cnt: 0 - valLoss: 0.6663622260093689 - trainLoss: 0.664657711982727\n",
      "cnt: 0 - valLoss: 0.6663621068000793 - trainLoss: 0.6646576523780823\n",
      "cnt: 0 - valLoss: 0.6663619875907898 - trainLoss: 0.6646575331687927\n",
      "cnt: 0 - valLoss: 0.666361927986145 - trainLoss: 0.6646574139595032\n",
      "cnt: 0 - valLoss: 0.6663617491722107 - trainLoss: 0.6646573543548584\n",
      "cnt: 0 - valLoss: 0.6663616895675659 - trainLoss: 0.6646571755409241\n",
      "cnt: 0 - valLoss: 0.6663616299629211 - trainLoss: 0.6646571159362793\n",
      "cnt: 0 - valLoss: 0.6663614511489868 - trainLoss: 0.6646569967269897\n",
      "cnt: 0 - valLoss: 0.666361391544342 - trainLoss: 0.664656937122345\n",
      "cnt: 0 - valLoss: 0.6663612723350525 - trainLoss: 0.6646567583084106\n",
      "cnt: 0 - valLoss: 0.6663611531257629 - trainLoss: 0.6646566987037659\n",
      "cnt: 0 - valLoss: 0.6663610935211182 - trainLoss: 0.6646565794944763\n",
      "cnt: 0 - valLoss: 0.6663609743118286 - trainLoss: 0.6646565198898315\n",
      "cnt: 0 - valLoss: 0.6663608551025391 - trainLoss: 0.664656400680542\n",
      "cnt: 0 - valLoss: 0.6663607358932495 - trainLoss: 0.6646562218666077\n",
      "cnt: 0 - valLoss: 0.6663606762886047 - trainLoss: 0.6646561622619629\n",
      "cnt: 0 - valLoss: 0.6663605570793152 - trainLoss: 0.6646560430526733\n",
      "cnt: 0 - valLoss: 0.6663603782653809 - trainLoss: 0.6646559238433838\n",
      "cnt: 0 - valLoss: 0.6663603186607361 - trainLoss: 0.6646558046340942\n",
      "cnt: 0 - valLoss: 0.6663602590560913 - trainLoss: 0.6646557450294495\n",
      "cnt: 0 - valLoss: 0.6663601398468018 - trainLoss: 0.6646556258201599\n",
      "cnt: 0 - valLoss: 0.6663600206375122 - trainLoss: 0.6646555066108704\n",
      "cnt: 0 - valLoss: 0.6663599610328674 - trainLoss: 0.6646554470062256\n",
      "cnt: 0 - valLoss: 0.6663597822189331 - trainLoss: 0.664655327796936\n",
      "cnt: 0 - valLoss: 0.6663597226142883 - trainLoss: 0.6646552085876465\n",
      "cnt: 0 - valLoss: 0.6663596630096436 - trainLoss: 0.6646550893783569\n",
      "cnt: 0 - valLoss: 0.6663594841957092 - trainLoss: 0.6646550297737122\n",
      "cnt: 0 - valLoss: 0.6663593649864197 - trainLoss: 0.6646548509597778\n",
      "cnt: 0 - valLoss: 0.6663593053817749 - trainLoss: 0.6646547913551331\n",
      "cnt: 0 - valLoss: 0.6663591861724854 - trainLoss: 0.6646546721458435\n",
      "cnt: 0 - valLoss: 0.6663590669631958 - trainLoss: 0.664654552936554\n",
      "cnt: 0 - valLoss: 0.6663589477539062 - trainLoss: 0.6646544337272644\n",
      "cnt: 0 - valLoss: 0.6663588881492615 - trainLoss: 0.6646543741226196\n",
      "cnt: 0 - valLoss: 0.6663587689399719 - trainLoss: 0.6646542549133301\n",
      "cnt: 0 - valLoss: 0.6663585901260376 - trainLoss: 0.6646541357040405\n",
      "cnt: 0 - valLoss: 0.6663585305213928 - trainLoss: 0.664654016494751\n",
      "cnt: 0 - valLoss: 0.666358470916748 - trainLoss: 0.6646539568901062\n",
      "cnt: 0 - valLoss: 0.6663583517074585 - trainLoss: 0.6646538376808167\n",
      "cnt: 0 - valLoss: 0.666358232498169 - trainLoss: 0.6646536588668823\n",
      "cnt: 0 - valLoss: 0.6663581132888794 - trainLoss: 0.6646535992622375\n",
      "cnt: 0 - valLoss: 0.6663579940795898 - trainLoss: 0.664653480052948\n",
      "cnt: 0 - valLoss: 0.6663579344749451 - trainLoss: 0.6646534204483032\n",
      "cnt: 0 - valLoss: 0.6663578152656555 - trainLoss: 0.6646533608436584\n",
      "cnt: 0 - valLoss: 0.6663576364517212 - trainLoss: 0.6646531820297241\n",
      "cnt: 0 - valLoss: 0.6663575768470764 - trainLoss: 0.6646530628204346\n",
      "cnt: 0 - valLoss: 0.6663575172424316 - trainLoss: 0.6646530032157898\n",
      "cnt: 0 - valLoss: 0.6663573384284973 - trainLoss: 0.6646528840065002\n",
      "cnt: 0 - valLoss: 0.6663572192192078 - trainLoss: 0.6646527647972107\n",
      "cnt: 0 - valLoss: 0.666357159614563 - trainLoss: 0.6646526455879211\n",
      "cnt: 0 - valLoss: 0.6663570404052734 - trainLoss: 0.6646525859832764\n",
      "cnt: 0 - valLoss: 0.6663569808006287 - trainLoss: 0.6646524667739868\n",
      "cnt: 0 - valLoss: 0.6663568615913391 - trainLoss: 0.6646523475646973\n",
      "cnt: 0 - valLoss: 0.6663567423820496 - trainLoss: 0.6646522283554077\n",
      "cnt: 0 - valLoss: 0.66635662317276 - trainLoss: 0.6646521091461182\n",
      "cnt: 0 - valLoss: 0.6663565039634705 - trainLoss: 0.6646520495414734\n",
      "cnt: 0 - valLoss: 0.6663564443588257 - trainLoss: 0.6646519303321838\n",
      "cnt: 0 - valLoss: 0.6663562655448914 - trainLoss: 0.6646518707275391\n",
      "cnt: 0 - valLoss: 0.6663562655448914 - trainLoss: 0.6646516919136047\n",
      "cnt: 0 - valLoss: 0.6663561463356018 - trainLoss: 0.66465163230896\n",
      "cnt: 0 - valLoss: 0.6663559675216675 - trainLoss: 0.6646514534950256\n",
      "cnt: 0 - valLoss: 0.6663559079170227 - trainLoss: 0.6646513938903809\n",
      "cnt: 0 - valLoss: 0.6663558483123779 - trainLoss: 0.6646513342857361\n",
      "cnt: 0 - valLoss: 0.6663556694984436 - trainLoss: 0.6646512150764465\n",
      "cnt: 0 - valLoss: 0.666355550289154 - trainLoss: 0.664651095867157\n",
      "cnt: 0 - valLoss: 0.6663554310798645 - trainLoss: 0.6646510362625122\n",
      "cnt: 0 - valLoss: 0.6663553714752197 - trainLoss: 0.6646508574485779\n",
      "cnt: 0 - valLoss: 0.6663552522659302 - trainLoss: 0.6646507382392883\n",
      "cnt: 0 - valLoss: 0.6663551926612854 - trainLoss: 0.6646506786346436\n",
      "cnt: 0 - valLoss: 0.6663550734519958 - trainLoss: 0.6646506190299988\n",
      "cnt: 0 - valLoss: 0.6663548946380615 - trainLoss: 0.6646504402160645\n",
      "cnt: 0 - valLoss: 0.6663548350334167 - trainLoss: 0.6646503806114197\n",
      "cnt: 0 - valLoss: 0.666354775428772 - trainLoss: 0.6646502614021301\n",
      "cnt: 0 - valLoss: 0.6663546562194824 - trainLoss: 0.6646501421928406\n",
      "cnt: 0 - valLoss: 0.6663545370101929 - trainLoss: 0.664650022983551\n",
      "cnt: 0 - valLoss: 0.6663544178009033 - trainLoss: 0.6646499633789062\n",
      "cnt: 0 - valLoss: 0.6663542985916138 - trainLoss: 0.6646498441696167\n",
      "cnt: 0 - valLoss: 0.666354238986969 - trainLoss: 0.6646497249603271\n",
      "cnt: 0 - valLoss: 0.6663540601730347 - trainLoss: 0.6646496653556824\n",
      "cnt: 0 - valLoss: 0.6663540005683899 - trainLoss: 0.664649486541748\n",
      "cnt: 0 - valLoss: 0.6663538813591003 - trainLoss: 0.6646493673324585\n",
      "cnt: 0 - valLoss: 0.6663538217544556 - trainLoss: 0.6646493077278137\n",
      "cnt: 0 - valLoss: 0.666353702545166 - trainLoss: 0.664649248123169\n",
      "cnt: 0 - valLoss: 0.6663535237312317 - trainLoss: 0.6646490693092346\n",
      "cnt: 0 - valLoss: 0.6663534641265869 - trainLoss: 0.6646489500999451\n",
      "cnt: 0 - valLoss: 0.6663533449172974 - trainLoss: 0.6646488904953003\n",
      "cnt: 0 - valLoss: 0.6663532853126526 - trainLoss: 0.6646487712860107\n",
      "cnt: 0 - valLoss: 0.666353166103363 - trainLoss: 0.664648711681366\n",
      "cnt: 0 - valLoss: 0.6663530468940735 - trainLoss: 0.6646485924720764\n",
      "cnt: 0 - valLoss: 0.6663529276847839 - trainLoss: 0.6646484732627869\n",
      "cnt: 0 - valLoss: 0.6663528680801392 - trainLoss: 0.6646483540534973\n",
      "cnt: 0 - valLoss: 0.6663527488708496 - trainLoss: 0.6646482348442078\n",
      "cnt: 0 - valLoss: 0.6663526296615601 - trainLoss: 0.664648175239563\n",
      "cnt: 0 - valLoss: 0.6663525104522705 - trainLoss: 0.6646480560302734\n",
      "cnt: 0 - valLoss: 0.6663524508476257 - trainLoss: 0.6646478772163391\n",
      "cnt: 0 - valLoss: 0.6663522720336914 - trainLoss: 0.6646478772163391\n",
      "cnt: 0 - valLoss: 0.6663521528244019 - trainLoss: 0.6646476984024048\n",
      "cnt: 0 - valLoss: 0.6663521528244019 - trainLoss: 0.66464763879776\n",
      "cnt: 0 - valLoss: 0.6663520336151123 - trainLoss: 0.6646475791931152\n",
      "cnt: 0 - valLoss: 0.666351854801178 - trainLoss: 0.6646474003791809\n",
      "cnt: 0 - valLoss: 0.6663517355918884 - trainLoss: 0.6646473407745361\n",
      "cnt: 0 - valLoss: 0.6663516759872437 - trainLoss: 0.6646471619606018\n",
      "cnt: 0 - valLoss: 0.6663515567779541 - trainLoss: 0.664647102355957\n",
      "cnt: 0 - valLoss: 0.6663514971733093 - trainLoss: 0.6646469831466675\n",
      "cnt: 0 - valLoss: 0.6663513779640198 - trainLoss: 0.6646469235420227\n",
      "cnt: 0 - valLoss: 0.6663512587547302 - trainLoss: 0.6646468043327332\n",
      "cnt: 0 - valLoss: 0.6663511395454407 - trainLoss: 0.6646467447280884\n",
      "cnt: 0 - valLoss: 0.6663510799407959 - trainLoss: 0.664646565914154\n",
      "cnt: 0 - valLoss: 0.6663509607315063 - trainLoss: 0.6646465063095093\n",
      "cnt: 0 - valLoss: 0.666350781917572 - trainLoss: 0.6646463871002197\n",
      "cnt: 0 - valLoss: 0.6663507223129272 - trainLoss: 0.6646462678909302\n",
      "cnt: 0 - valLoss: 0.6663506031036377 - trainLoss: 0.6646461486816406\n",
      "cnt: 0 - valLoss: 0.6663504838943481 - trainLoss: 0.6646460294723511\n",
      "cnt: 0 - valLoss: 0.6663504242897034 - trainLoss: 0.6646459102630615\n",
      "cnt: 0 - valLoss: 0.6663503050804138 - trainLoss: 0.6646458506584167\n",
      "cnt: 0 - valLoss: 0.6663501858711243 - trainLoss: 0.6646457314491272\n",
      "cnt: 0 - valLoss: 0.6663500070571899 - trainLoss: 0.6646456122398376\n",
      "cnt: 0 - valLoss: 0.6663500070571899 - trainLoss: 0.6646455526351929\n",
      "cnt: 0 - valLoss: 0.6663498878479004 - trainLoss: 0.6646453738212585\n",
      "cnt: 0 - valLoss: 0.6663497090339661 - trainLoss: 0.6646453142166138\n",
      "cnt: 0 - valLoss: 0.6663496494293213 - trainLoss: 0.6646451950073242\n",
      "cnt: 0 - valLoss: 0.6663495898246765 - trainLoss: 0.6646451354026794\n",
      "cnt: 0 - valLoss: 0.6663494110107422 - trainLoss: 0.6646450161933899\n",
      "cnt: 0 - valLoss: 0.6663493514060974 - trainLoss: 0.6646449565887451\n",
      "cnt: 0 - valLoss: 0.6663492321968079 - trainLoss: 0.6646448373794556\n",
      "cnt: 0 - valLoss: 0.6663491129875183 - trainLoss: 0.6646446585655212\n",
      "cnt: 0 - valLoss: 0.6663489937782288 - trainLoss: 0.6646445393562317\n",
      "cnt: 0 - valLoss: 0.666348934173584 - trainLoss: 0.6646444201469421\n",
      "cnt: 0 - valLoss: 0.6663488149642944 - trainLoss: 0.6646443605422974\n",
      "cnt: 0 - valLoss: 0.6663486957550049 - trainLoss: 0.6646442413330078\n",
      "cnt: 0 - valLoss: 0.6663486361503601 - trainLoss: 0.664644181728363\n",
      "cnt: 0 - valLoss: 0.6663484573364258 - trainLoss: 0.6646440625190735\n",
      "cnt: 0 - valLoss: 0.666348397731781 - trainLoss: 0.6646439433097839\n",
      "cnt: 0 - valLoss: 0.6663483381271362 - trainLoss: 0.6646438837051392\n",
      "cnt: 0 - valLoss: 0.6663482189178467 - trainLoss: 0.6646437644958496\n",
      "cnt: 0 - valLoss: 0.6663480401039124 - trainLoss: 0.6646437048912048\n",
      "cnt: 0 - valLoss: 0.6663479804992676 - trainLoss: 0.6646434664726257\n",
      "cnt: 0 - valLoss: 0.666347861289978 - trainLoss: 0.6646434664726257\n",
      "cnt: 0 - valLoss: 0.6663477420806885 - trainLoss: 0.6646433472633362\n",
      "cnt: 0 - valLoss: 0.6663476824760437 - trainLoss: 0.6646432280540466\n",
      "cnt: 0 - valLoss: 0.6663475632667542 - trainLoss: 0.6646431088447571\n",
      "cnt: 0 - valLoss: 0.6663474440574646 - trainLoss: 0.6646429896354675\n",
      "cnt: 0 - valLoss: 0.6663472652435303 - trainLoss: 0.6646429300308228\n",
      "cnt: 0 - valLoss: 0.6663472056388855 - trainLoss: 0.6646428108215332\n",
      "cnt: 0 - valLoss: 0.6663471460342407 - trainLoss: 0.6646426916122437\n",
      "cnt: 0 - valLoss: 0.6663470268249512 - trainLoss: 0.6646426320075989\n",
      "cnt: 0 - valLoss: 0.6663469076156616 - trainLoss: 0.6646425127983093\n",
      "cnt: 0 - valLoss: 0.6663467884063721 - trainLoss: 0.6646423935890198\n",
      "cnt: 0 - valLoss: 0.6663466691970825 - trainLoss: 0.6646422743797302\n",
      "cnt: 0 - valLoss: 0.666346549987793 - trainLoss: 0.6646421551704407\n",
      "cnt: 0 - valLoss: 0.6663464307785034 - trainLoss: 0.6646420359611511\n",
      "cnt: 0 - valLoss: 0.6663463115692139 - trainLoss: 0.6646419763565063\n",
      "cnt: 0 - valLoss: 0.6663462519645691 - trainLoss: 0.664641797542572\n",
      "cnt: 0 - valLoss: 0.6663461327552795 - trainLoss: 0.664641797542572\n",
      "cnt: 0 - valLoss: 0.66634601354599 - trainLoss: 0.6646416783332825\n",
      "cnt: 0 - valLoss: 0.6663458943367004 - trainLoss: 0.6646415591239929\n",
      "cnt: 0 - valLoss: 0.6663458347320557 - trainLoss: 0.6646414399147034\n",
      "cnt: 0 - valLoss: 0.6663457751274109 - trainLoss: 0.6646413207054138\n",
      "cnt: 0 - valLoss: 0.6663455963134766 - trainLoss: 0.664641261100769\n",
      "cnt: 0 - valLoss: 0.666345477104187 - trainLoss: 0.6646411418914795\n",
      "cnt: 0 - valLoss: 0.6663454174995422 - trainLoss: 0.6646410226821899\n",
      "cnt: 0 - valLoss: 0.6663452982902527 - trainLoss: 0.6646409034729004\n",
      "cnt: 0 - valLoss: 0.6663452386856079 - trainLoss: 0.6646407842636108\n",
      "cnt: 0 - valLoss: 0.6663450598716736 - trainLoss: 0.6646407246589661\n",
      "cnt: 0 - valLoss: 0.6663450002670288 - trainLoss: 0.6646406054496765\n",
      "cnt: 0 - valLoss: 0.6663448810577393 - trainLoss: 0.664640486240387\n",
      "cnt: 0 - valLoss: 0.6663448214530945 - trainLoss: 0.6646403670310974\n",
      "cnt: 0 - valLoss: 0.6663447022438049 - trainLoss: 0.6646403074264526\n",
      "cnt: 0 - valLoss: 0.6663445830345154 - trainLoss: 0.6646401882171631\n",
      "cnt: 0 - valLoss: 0.6663444638252258 - trainLoss: 0.6646401286125183\n",
      "cnt: 0 - valLoss: 0.6663442850112915 - trainLoss: 0.6646400094032288\n",
      "cnt: 0 - valLoss: 0.6663442254066467 - trainLoss: 0.6646398305892944\n",
      "cnt: 0 - valLoss: 0.666344165802002 - trainLoss: 0.6646398305892944\n",
      "cnt: 0 - valLoss: 0.6663440465927124 - trainLoss: 0.6646396517753601\n",
      "cnt: 0 - valLoss: 0.6663439273834229 - trainLoss: 0.6646395325660706\n",
      "cnt: 0 - valLoss: 0.6663437485694885 - trainLoss: 0.6646394729614258\n",
      "cnt: 0 - valLoss: 0.6663437485694885 - trainLoss: 0.6646393537521362\n",
      "cnt: 0 - valLoss: 0.666343629360199 - trainLoss: 0.6646392345428467\n",
      "cnt: 0 - valLoss: 0.6663435101509094 - trainLoss: 0.6646391749382019\n",
      "cnt: 0 - valLoss: 0.6663433313369751 - trainLoss: 0.6646390557289124\n",
      "cnt: 0 - valLoss: 0.6663432717323303 - trainLoss: 0.6646389365196228\n",
      "cnt: 0 - valLoss: 0.6663431525230408 - trainLoss: 0.6646388173103333\n",
      "cnt: 0 - valLoss: 0.666343092918396 - trainLoss: 0.6646386981010437\n",
      "cnt: 0 - valLoss: 0.6663429737091064 - trainLoss: 0.6646386384963989\n",
      "cnt: 0 - valLoss: 0.6663428544998169 - trainLoss: 0.6646385192871094\n",
      "cnt: 0 - valLoss: 0.6663427352905273 - trainLoss: 0.6646384596824646\n",
      "cnt: 0 - valLoss: 0.6663426160812378 - trainLoss: 0.6646382808685303\n",
      "cnt: 0 - valLoss: 0.666342556476593 - trainLoss: 0.6646381616592407\n",
      "cnt: 0 - valLoss: 0.6663423776626587 - trainLoss: 0.6646381616592407\n",
      "cnt: 0 - valLoss: 0.6663423180580139 - trainLoss: 0.6646379828453064\n",
      "cnt: 0 - valLoss: 0.6663422584533691 - trainLoss: 0.6646379232406616\n",
      "cnt: 0 - valLoss: 0.6663421392440796 - trainLoss: 0.6646378040313721\n",
      "cnt: 0 - valLoss: 0.6663419604301453 - trainLoss: 0.6646376848220825\n",
      "cnt: 0 - valLoss: 0.6663419008255005 - trainLoss: 0.6646376252174377\n",
      "cnt: 0 - valLoss: 0.6663417220115662 - trainLoss: 0.6646375060081482\n",
      "cnt: 0 - valLoss: 0.6663416624069214 - trainLoss: 0.6646373867988586\n",
      "cnt: 0 - valLoss: 0.6663416028022766 - trainLoss: 0.6646372675895691\n",
      "cnt: 0 - valLoss: 0.6663414835929871 - trainLoss: 0.6646371483802795\n",
      "cnt: 0 - valLoss: 0.6663413643836975 - trainLoss: 0.66463702917099\n",
      "cnt: 0 - valLoss: 0.6663411855697632 - trainLoss: 0.6646369695663452\n",
      "cnt: 0 - valLoss: 0.6663411259651184 - trainLoss: 0.6646367907524109\n",
      "cnt: 0 - valLoss: 0.6663410067558289 - trainLoss: 0.6646367311477661\n",
      "cnt: 0 - valLoss: 0.6663408875465393 - trainLoss: 0.6646366119384766\n",
      "cnt: 0 - valLoss: 0.6663408279418945 - trainLoss: 0.664636492729187\n",
      "cnt: 0 - valLoss: 0.666340708732605 - trainLoss: 0.664636492729187\n",
      "cnt: 0 - valLoss: 0.6663405895233154 - trainLoss: 0.6646363139152527\n",
      "cnt: 0 - valLoss: 0.6663405299186707 - trainLoss: 0.6646361947059631\n",
      "cnt: 0 - valLoss: 0.6663403511047363 - trainLoss: 0.6646361351013184\n",
      "cnt: 0 - valLoss: 0.6663402318954468 - trainLoss: 0.6646360158920288\n",
      "cnt: 0 - valLoss: 0.666340172290802 - trainLoss: 0.6646358966827393\n",
      "cnt: 0 - valLoss: 0.6663401126861572 - trainLoss: 0.6646358370780945\n",
      "cnt: 0 - valLoss: 0.6663399338722229 - trainLoss: 0.6646357178688049\n",
      "cnt: 0 - valLoss: 0.6663398146629333 - trainLoss: 0.6646355986595154\n",
      "cnt: 0 - valLoss: 0.6663397550582886 - trainLoss: 0.6646354794502258\n",
      "cnt: 0 - valLoss: 0.666339635848999 - trainLoss: 0.664635419845581\n",
      "cnt: 0 - valLoss: 0.6663395166397095 - trainLoss: 0.6646353006362915\n",
      "cnt: 0 - valLoss: 0.6663393378257751 - trainLoss: 0.6646351218223572\n",
      "cnt: 0 - valLoss: 0.6663393378257751 - trainLoss: 0.6646351218223572\n",
      "cnt: 0 - valLoss: 0.6663391590118408 - trainLoss: 0.6646350026130676\n",
      "cnt: 0 - valLoss: 0.666339099407196 - trainLoss: 0.6646348237991333\n",
      "cnt: 0 - valLoss: 0.6663389801979065 - trainLoss: 0.6646347641944885\n",
      "cnt: 0 - valLoss: 0.6663388609886169 - trainLoss: 0.664634644985199\n",
      "cnt: 0 - valLoss: 0.6663387417793274 - trainLoss: 0.6646345257759094\n",
      "cnt: 0 - valLoss: 0.6663386225700378 - trainLoss: 0.6646344661712646\n",
      "cnt: 0 - valLoss: 0.6663385629653931 - trainLoss: 0.6646343469619751\n",
      "cnt: 0 - valLoss: 0.6663384437561035 - trainLoss: 0.6646342277526855\n",
      "cnt: 0 - valLoss: 0.666338324546814 - trainLoss: 0.664634108543396\n",
      "cnt: 0 - valLoss: 0.6663382649421692 - trainLoss: 0.6646340489387512\n",
      "cnt: 0 - valLoss: 0.6663381457328796 - trainLoss: 0.6646339297294617\n",
      "cnt: 0 - valLoss: 0.6663379669189453 - trainLoss: 0.6646338701248169\n",
      "cnt: 0 - valLoss: 0.6663379669189453 - trainLoss: 0.6646336913108826\n",
      "cnt: 0 - valLoss: 0.666337788105011 - trainLoss: 0.6646336317062378\n",
      "cnt: 0 - valLoss: 0.6663376688957214 - trainLoss: 0.6646335124969482\n",
      "cnt: 0 - valLoss: 0.6663375496864319 - trainLoss: 0.6646333932876587\n",
      "cnt: 0 - valLoss: 0.6663374900817871 - trainLoss: 0.6646332740783691\n",
      "cnt: 0 - valLoss: 0.6663373708724976 - trainLoss: 0.6646331548690796\n",
      "cnt: 0 - valLoss: 0.666337251663208 - trainLoss: 0.6646331548690796\n",
      "cnt: 0 - valLoss: 0.6663371324539185 - trainLoss: 0.6646329760551453\n",
      "cnt: 0 - valLoss: 0.6663370728492737 - trainLoss: 0.6646328568458557\n",
      "cnt: 0 - valLoss: 0.6663368940353394 - trainLoss: 0.6646327972412109\n",
      "cnt: 0 - valLoss: 0.6663368344306946 - trainLoss: 0.6646326780319214\n",
      "cnt: 0 - valLoss: 0.666336715221405 - trainLoss: 0.6646325588226318\n",
      "cnt: 0 - valLoss: 0.6663366556167603 - trainLoss: 0.6646324396133423\n",
      "cnt: 0 - valLoss: 0.6663364768028259 - trainLoss: 0.6646323204040527\n",
      "cnt: 0 - valLoss: 0.6663364171981812 - trainLoss: 0.664632260799408\n",
      "cnt: 0 - valLoss: 0.6663362979888916 - trainLoss: 0.6646322011947632\n",
      "cnt: 0 - valLoss: 0.6663361191749573 - trainLoss: 0.6646320223808289\n",
      "cnt: 0 - valLoss: 0.6663360595703125 - trainLoss: 0.6646319627761841\n",
      "cnt: 0 - valLoss: 0.6663359999656677 - trainLoss: 0.6646318435668945\n",
      "cnt: 0 - valLoss: 0.6663358807563782 - trainLoss: 0.664631724357605\n",
      "cnt: 0 - valLoss: 0.6663357615470886 - trainLoss: 0.6646316051483154\n",
      "cnt: 0 - valLoss: 0.6663356423377991 - trainLoss: 0.6646316051483154\n",
      "cnt: 0 - valLoss: 0.6663355231285095 - trainLoss: 0.6646314859390259\n",
      "cnt: 0 - valLoss: 0.66633540391922 - trainLoss: 0.6646313071250916\n",
      "cnt: 0 - valLoss: 0.6663352847099304 - trainLoss: 0.664631187915802\n",
      "cnt: 0 - valLoss: 0.6663352251052856 - trainLoss: 0.664631187915802\n",
      "cnt: 0 - valLoss: 0.6663351058959961 - trainLoss: 0.6646310091018677\n",
      "cnt: 0 - valLoss: 0.6663349866867065 - trainLoss: 0.6646308898925781\n",
      "cnt: 0 - valLoss: 0.666334867477417 - trainLoss: 0.6646307706832886\n",
      "cnt: 0 - valLoss: 0.6663348078727722 - trainLoss: 0.6646307110786438\n",
      "cnt: 0 - valLoss: 0.6663346290588379 - trainLoss: 0.6646305918693542\n",
      "cnt: 0 - valLoss: 0.6663345694541931 - trainLoss: 0.6646304726600647\n",
      "cnt: 0 - valLoss: 0.6663344502449036 - trainLoss: 0.6646304130554199\n",
      "cnt: 0 - valLoss: 0.6663343906402588 - trainLoss: 0.6646302938461304\n",
      "cnt: 0 - valLoss: 0.6663342118263245 - trainLoss: 0.6646301746368408\n",
      "cnt: 0 - valLoss: 0.6663341522216797 - trainLoss: 0.664630115032196\n",
      "cnt: 0 - valLoss: 0.6663339734077454 - trainLoss: 0.6646299362182617\n",
      "cnt: 0 - valLoss: 0.6663339138031006 - trainLoss: 0.6646298170089722\n",
      "cnt: 0 - valLoss: 0.6663337349891663 - trainLoss: 0.6646297574043274\n",
      "cnt: 0 - valLoss: 0.6663336753845215 - trainLoss: 0.6646296977996826\n",
      "cnt: 0 - valLoss: 0.6663335561752319 - trainLoss: 0.6646295189857483\n",
      "cnt: 0 - valLoss: 0.6663334965705872 - trainLoss: 0.6646294593811035\n",
      "cnt: 0 - valLoss: 0.6663333773612976 - trainLoss: 0.664629340171814\n",
      "cnt: 0 - valLoss: 0.6663332581520081 - trainLoss: 0.6646292805671692\n",
      "cnt: 0 - valLoss: 0.6663331389427185 - trainLoss: 0.6646292209625244\n",
      "cnt: 0 - valLoss: 0.666333019733429 - trainLoss: 0.6646290421485901\n",
      "cnt: 0 - valLoss: 0.6663329601287842 - trainLoss: 0.6646289229393005\n",
      "cnt: 0 - valLoss: 0.6663327813148499 - trainLoss: 0.664628803730011\n",
      "cnt: 0 - valLoss: 0.6663327813148499 - trainLoss: 0.6646287441253662\n",
      "cnt: 0 - valLoss: 0.6663326025009155 - trainLoss: 0.6646286249160767\n",
      "cnt: 0 - valLoss: 0.666332483291626 - trainLoss: 0.6646285057067871\n",
      "cnt: 0 - valLoss: 0.6663323640823364 - trainLoss: 0.6646284461021423\n",
      "cnt: 0 - valLoss: 0.6663323044776917 - trainLoss: 0.664628267288208\n",
      "cnt: 0 - valLoss: 0.6663321852684021 - trainLoss: 0.6646282076835632\n",
      "cnt: 0 - valLoss: 0.6663320660591125 - trainLoss: 0.6646280884742737\n",
      "cnt: 0 - valLoss: 0.666331946849823 - trainLoss: 0.6646279692649841\n",
      "cnt: 0 - valLoss: 0.6663318872451782 - trainLoss: 0.6646278500556946\n",
      "cnt: 0 - valLoss: 0.6663317680358887 - trainLoss: 0.6646277904510498\n",
      "cnt: 0 - valLoss: 0.6663316488265991 - trainLoss: 0.6646276712417603\n",
      "cnt: 0 - valLoss: 0.6663315296173096 - trainLoss: 0.6646275520324707\n",
      "cnt: 0 - valLoss: 0.66633141040802 - trainLoss: 0.6646274924278259\n",
      "cnt: 0 - valLoss: 0.6663312911987305 - trainLoss: 0.6646273732185364\n",
      "cnt: 0 - valLoss: 0.6663312315940857 - trainLoss: 0.6646273136138916\n",
      "cnt: 0 - valLoss: 0.6663311123847961 - trainLoss: 0.664627194404602\n",
      "cnt: 0 - valLoss: 0.6663309931755066 - trainLoss: 0.6646270751953125\n",
      "cnt: 0 - valLoss: 0.6663309335708618 - trainLoss: 0.664626955986023\n",
      "cnt: 0 - valLoss: 0.6663307547569275 - trainLoss: 0.6646268367767334\n",
      "cnt: 0 - valLoss: 0.6663306355476379 - trainLoss: 0.6646267771720886\n",
      "cnt: 0 - valLoss: 0.6663305759429932 - trainLoss: 0.6646266579627991\n",
      "cnt: 0 - valLoss: 0.6663303971290588 - trainLoss: 0.6646265387535095\n",
      "cnt: 0 - valLoss: 0.6663303375244141 - trainLoss: 0.6646264791488647\n",
      "cnt: 0 - valLoss: 0.6663301587104797 - trainLoss: 0.6646263599395752\n",
      "cnt: 0 - valLoss: 0.6663301587104797 - trainLoss: 0.6646263003349304\n",
      "cnt: 0 - valLoss: 0.6663299798965454 - trainLoss: 0.6646261215209961\n",
      "cnt: 0 - valLoss: 0.6663299202919006 - trainLoss: 0.6646260023117065\n",
      "cnt: 0 - valLoss: 0.6663298010826111 - trainLoss: 0.664625883102417\n",
      "cnt: 0 - valLoss: 0.6663297414779663 - trainLoss: 0.6646257638931274\n",
      "cnt: 0 - valLoss: 0.666329562664032 - trainLoss: 0.6646257042884827\n",
      "cnt: 0 - valLoss: 0.6663294434547424 - trainLoss: 0.6646256446838379\n",
      "cnt: 0 - valLoss: 0.6663292646408081 - trainLoss: 0.6646255850791931\n",
      "cnt: 0 - valLoss: 0.6663292646408081 - trainLoss: 0.6646254062652588\n",
      "cnt: 0 - valLoss: 0.6663291454315186 - trainLoss: 0.6646252870559692\n",
      "cnt: 0 - valLoss: 0.6663289666175842 - trainLoss: 0.6646252274513245\n",
      "cnt: 0 - valLoss: 0.6663289070129395 - trainLoss: 0.6646251082420349\n",
      "cnt: 0 - valLoss: 0.6663287878036499 - trainLoss: 0.6646249890327454\n",
      "cnt: 0 - valLoss: 0.6663287281990051 - trainLoss: 0.664624810218811\n",
      "cnt: 0 - valLoss: 0.6663285493850708 - trainLoss: 0.6646247506141663\n",
      "cnt: 0 - valLoss: 0.6663284301757812 - trainLoss: 0.6646247506141663\n",
      "cnt: 0 - valLoss: 0.6663283705711365 - trainLoss: 0.6646246314048767\n",
      "cnt: 0 - valLoss: 0.6663283109664917 - trainLoss: 0.6646245121955872\n",
      "cnt: 0 - valLoss: 0.6663281917572021 - trainLoss: 0.6646243333816528\n",
      "cnt: 0 - valLoss: 0.6663280129432678 - trainLoss: 0.6646243333816528\n",
      "cnt: 0 - valLoss: 0.6663278937339783 - trainLoss: 0.6646241545677185\n",
      "cnt: 0 - valLoss: 0.6663278341293335 - trainLoss: 0.664624035358429\n",
      "cnt: 0 - valLoss: 0.666327714920044 - trainLoss: 0.6646239757537842\n",
      "cnt: 0 - valLoss: 0.6663275957107544 - trainLoss: 0.6646239161491394\n",
      "cnt: 0 - valLoss: 0.6663274765014648 - trainLoss: 0.6646237969398499\n",
      "cnt: 0 - valLoss: 0.6663273572921753 - trainLoss: 0.6646236181259155\n",
      "cnt: 0 - valLoss: 0.6663272976875305 - trainLoss: 0.6646235585212708\n",
      "cnt: 0 - valLoss: 0.6663271188735962 - trainLoss: 0.6646234393119812\n",
      "cnt: 0 - valLoss: 0.6663270592689514 - trainLoss: 0.6646233797073364\n",
      "cnt: 0 - valLoss: 0.6663269996643066 - trainLoss: 0.6646232008934021\n",
      "cnt: 0 - valLoss: 0.6663268208503723 - trainLoss: 0.6646231412887573\n",
      "cnt: 0 - valLoss: 0.6663267016410828 - trainLoss: 0.6646230220794678\n",
      "cnt: 0 - valLoss: 0.6663265824317932 - trainLoss: 0.664622962474823\n",
      "cnt: 0 - valLoss: 0.6663265228271484 - trainLoss: 0.6646227836608887\n",
      "cnt: 0 - valLoss: 0.6663263440132141 - trainLoss: 0.6646226644515991\n",
      "cnt: 0 - valLoss: 0.6663262844085693 - trainLoss: 0.6646226644515991\n",
      "cnt: 0 - valLoss: 0.6663261651992798 - trainLoss: 0.6646225452423096\n",
      "cnt: 0 - valLoss: 0.6663260459899902 - trainLoss: 0.6646223664283752\n",
      "cnt: 0 - valLoss: 0.6663259267807007 - trainLoss: 0.6646223068237305\n",
      "cnt: 0 - valLoss: 0.6663258671760559 - trainLoss: 0.6646221876144409\n",
      "cnt: 0 - valLoss: 0.6663256883621216 - trainLoss: 0.6646220684051514\n",
      "cnt: 0 - valLoss: 0.6663256287574768 - trainLoss: 0.6646220088005066\n",
      "cnt: 0 - valLoss: 0.6663255095481873 - trainLoss: 0.664621889591217\n",
      "cnt: 0 - valLoss: 0.6663254499435425 - trainLoss: 0.6646218299865723\n",
      "cnt: 0 - valLoss: 0.6663252711296082 - trainLoss: 0.6646217107772827\n",
      "cnt: 0 - valLoss: 0.6663252115249634 - trainLoss: 0.6646215915679932\n",
      "cnt: 0 - valLoss: 0.666325032711029 - trainLoss: 0.6646214723587036\n",
      "cnt: 0 - valLoss: 0.6663249731063843 - trainLoss: 0.6646213531494141\n",
      "cnt: 0 - valLoss: 0.6663248538970947 - trainLoss: 0.6646212339401245\n",
      "cnt: 0 - valLoss: 0.6663247346878052 - trainLoss: 0.6646211743354797\n",
      "cnt: 0 - valLoss: 0.6663246750831604 - trainLoss: 0.6646210551261902\n",
      "cnt: 0 - valLoss: 0.6663244962692261 - trainLoss: 0.6646209955215454\n",
      "cnt: 0 - valLoss: 0.6663243770599365 - trainLoss: 0.6646208763122559\n",
      "cnt: 0 - valLoss: 0.666324257850647 - trainLoss: 0.6646207571029663\n",
      "cnt: 0 - valLoss: 0.666324257850647 - trainLoss: 0.6646206378936768\n",
      "cnt: 0 - valLoss: 0.6663240790367126 - trainLoss: 0.6646205186843872\n",
      "cnt: 0 - valLoss: 0.6663240194320679 - trainLoss: 0.6646204590797424\n",
      "cnt: 0 - valLoss: 0.6663239002227783 - trainLoss: 0.6646203398704529\n",
      "cnt: 0 - valLoss: 0.6663237810134888 - trainLoss: 0.6646202206611633\n",
      "cnt: 0 - valLoss: 0.6663236021995544 - trainLoss: 0.6646201014518738\n",
      "cnt: 0 - valLoss: 0.6663234829902649 - trainLoss: 0.664620041847229\n",
      "cnt: 0 - valLoss: 0.6663233637809753 - trainLoss: 0.6646198630332947\n",
      "cnt: 0 - valLoss: 0.6663233041763306 - trainLoss: 0.6646198034286499\n",
      "cnt: 0 - valLoss: 0.666323184967041 - trainLoss: 0.6646196842193604\n",
      "cnt: 0 - valLoss: 0.6663231253623962 - trainLoss: 0.6646196246147156\n",
      "cnt: 0 - valLoss: 0.6663229465484619 - trainLoss: 0.664619505405426\n",
      "cnt: 0 - valLoss: 0.6663228869438171 - trainLoss: 0.6646193861961365\n",
      "cnt: 0 - valLoss: 0.6663227677345276 - trainLoss: 0.6646192669868469\n",
      "cnt: 0 - valLoss: 0.666322648525238 - trainLoss: 0.6646192073822021\n",
      "cnt: 0 - valLoss: 0.6663225293159485 - trainLoss: 0.6646190881729126\n",
      "cnt: 0 - valLoss: 0.6663224101066589 - trainLoss: 0.6646190285682678\n",
      "cnt: 0 - valLoss: 0.6663223505020142 - trainLoss: 0.6646188497543335\n",
      "cnt: 0 - valLoss: 0.6663221716880798 - trainLoss: 0.6646188497543335\n",
      "cnt: 0 - valLoss: 0.6663221120834351 - trainLoss: 0.6646186709403992\n",
      "cnt: 0 - valLoss: 0.6663219332695007 - trainLoss: 0.6646185517311096\n",
      "cnt: 0 - valLoss: 0.666321873664856 - trainLoss: 0.6646184325218201\n",
      "cnt: 0 - valLoss: 0.6663217544555664 - trainLoss: 0.6646183133125305\n",
      "cnt: 0 - valLoss: 0.6663216352462769 - trainLoss: 0.6646182537078857\n",
      "cnt: 0 - valLoss: 0.6663215160369873 - trainLoss: 0.6646181344985962\n",
      "cnt: 0 - valLoss: 0.6663215160369873 - trainLoss: 0.6646180152893066\n",
      "cnt: 0 - valLoss: 0.666321337223053 - trainLoss: 0.6646178960800171\n",
      "cnt: 0 - valLoss: 0.6663211584091187 - trainLoss: 0.6646178364753723\n",
      "cnt: 0 - valLoss: 0.6663210988044739 - trainLoss: 0.6646177172660828\n",
      "cnt: 0 - valLoss: 0.6663209795951843 - trainLoss: 0.664617657661438\n",
      "cnt: 0 - valLoss: 0.6663208603858948 - trainLoss: 0.6646175384521484\n",
      "cnt: 0 - valLoss: 0.66632080078125 - trainLoss: 0.6646174788475037\n",
      "cnt: 0 - valLoss: 0.6663206815719604 - trainLoss: 0.6646173000335693\n",
      "cnt: 0 - valLoss: 0.6663205623626709 - trainLoss: 0.6646172404289246\n",
      "cnt: 0 - valLoss: 0.6663204431533813 - trainLoss: 0.664617121219635\n",
      "cnt: 0 - valLoss: 0.6663203239440918 - trainLoss: 0.6646170616149902\n",
      "cnt: 0 - valLoss: 0.666320264339447 - trainLoss: 0.6646169424057007\n",
      "cnt: 0 - valLoss: 0.6663200855255127 - trainLoss: 0.6646167635917664\n",
      "cnt: 0 - valLoss: 0.6663199663162231 - trainLoss: 0.6646167039871216\n",
      "cnt: 0 - valLoss: 0.6663199067115784 - trainLoss: 0.664616584777832\n",
      "cnt: 0 - valLoss: 0.6663197875022888 - trainLoss: 0.6646164655685425\n",
      "cnt: 0 - valLoss: 0.6663196682929993 - trainLoss: 0.6646163463592529\n",
      "cnt: 0 - valLoss: 0.6663195490837097 - trainLoss: 0.6646162867546082\n",
      "cnt: 0 - valLoss: 0.6663194298744202 - trainLoss: 0.6646161675453186\n",
      "cnt: 0 - valLoss: 0.6663192510604858 - trainLoss: 0.664616048336029\n",
      "cnt: 0 - valLoss: 0.6663192510604858 - trainLoss: 0.6646159291267395\n",
      "cnt: 0 - valLoss: 0.6663190722465515 - trainLoss: 0.6646158695220947\n",
      "cnt: 0 - valLoss: 0.6663190126419067 - trainLoss: 0.66461580991745\n",
      "cnt: 0 - valLoss: 0.6663188934326172 - trainLoss: 0.6646156907081604\n",
      "cnt: 0 - valLoss: 0.6663187146186829 - trainLoss: 0.6646155714988708\n",
      "cnt: 0 - valLoss: 0.6663186550140381 - trainLoss: 0.6646154522895813\n",
      "cnt: 0 - valLoss: 0.6663185954093933 - trainLoss: 0.6646153926849365\n",
      "cnt: 0 - valLoss: 0.666318416595459 - trainLoss: 0.664615273475647\n",
      "cnt: 0 - valLoss: 0.6663183569908142 - trainLoss: 0.6646151542663574\n",
      "cnt: 0 - valLoss: 0.6663182377815247 - trainLoss: 0.6646150350570679\n",
      "cnt: 0 - valLoss: 0.6663180589675903 - trainLoss: 0.6646149754524231\n",
      "cnt: 0 - valLoss: 0.6663179993629456 - trainLoss: 0.6646148562431335\n",
      "cnt: 0 - valLoss: 0.6663179397583008 - trainLoss: 0.664614737033844\n",
      "cnt: 0 - valLoss: 0.6663178205490112 - trainLoss: 0.6646146178245544\n",
      "cnt: 0 - valLoss: 0.6663177013397217 - trainLoss: 0.6646144986152649\n",
      "cnt: 0 - valLoss: 0.6663175821304321 - trainLoss: 0.6646143794059753\n",
      "cnt: 0 - valLoss: 0.6663174629211426 - trainLoss: 0.6646143198013306\n",
      "cnt: 0 - valLoss: 0.666317343711853 - trainLoss: 0.664614200592041\n",
      "cnt: 0 - valLoss: 0.6663172245025635 - trainLoss: 0.6646140217781067\n",
      "cnt: 0 - valLoss: 0.6663170456886292 - trainLoss: 0.6646140217781067\n",
      "cnt: 0 - valLoss: 0.6663169860839844 - trainLoss: 0.6646138429641724\n",
      "cnt: 0 - valLoss: 0.6663168668746948 - trainLoss: 0.6646138429641724\n",
      "cnt: 0 - valLoss: 0.6663167476654053 - trainLoss: 0.6646137237548828\n",
      "cnt: 0 - valLoss: 0.6663166880607605 - trainLoss: 0.6646135449409485\n",
      "cnt: 0 - valLoss: 0.666316568851471 - trainLoss: 0.6646134853363037\n",
      "cnt: 0 - valLoss: 0.6663164496421814 - trainLoss: 0.6646133661270142\n",
      "cnt: 0 - valLoss: 0.6663162708282471 - trainLoss: 0.6646132469177246\n",
      "cnt: 0 - valLoss: 0.6663162708282471 - trainLoss: 0.6646131873130798\n",
      "cnt: 0 - valLoss: 0.6663161516189575 - trainLoss: 0.6646130681037903\n",
      "cnt: 0 - valLoss: 0.6663159728050232 - trainLoss: 0.6646130084991455\n",
      "cnt: 0 - valLoss: 0.6663159132003784 - trainLoss: 0.664612889289856\n",
      "cnt: 0 - valLoss: 0.6663157939910889 - trainLoss: 0.6646127104759216\n",
      "cnt: 0 - valLoss: 0.6663156747817993 - trainLoss: 0.6646126508712769\n",
      "cnt: 0 - valLoss: 0.6663155555725098 - trainLoss: 0.6646125316619873\n",
      "cnt: 0 - valLoss: 0.6663154363632202 - trainLoss: 0.6646124720573425\n",
      "cnt: 0 - valLoss: 0.6663153171539307 - trainLoss: 0.664612352848053\n",
      "cnt: 0 - valLoss: 0.6663152575492859 - trainLoss: 0.6646122336387634\n",
      "cnt: 0 - valLoss: 0.6663151383399963 - trainLoss: 0.6646121144294739\n",
      "cnt: 0 - valLoss: 0.6663150191307068 - trainLoss: 0.6646119952201843\n",
      "cnt: 0 - valLoss: 0.6663148999214172 - trainLoss: 0.6646119356155396\n",
      "cnt: 0 - valLoss: 0.6663147807121277 - trainLoss: 0.6646118760108948\n",
      "cnt: 0 - valLoss: 0.6663147211074829 - trainLoss: 0.6646117568016052\n",
      "cnt: 0 - valLoss: 0.6663145422935486 - trainLoss: 0.6646116375923157\n",
      "cnt: 0 - valLoss: 0.666314423084259 - trainLoss: 0.6646115779876709\n",
      "cnt: 0 - valLoss: 0.6663143634796143 - trainLoss: 0.6646113991737366\n",
      "cnt: 0 - valLoss: 0.6663142442703247 - trainLoss: 0.664611279964447\n",
      "cnt: 0 - valLoss: 0.6663141846656799 - trainLoss: 0.6646112203598022\n",
      "cnt: 0 - valLoss: 0.6663140058517456 - trainLoss: 0.6646110415458679\n",
      "cnt: 0 - valLoss: 0.666313886642456 - trainLoss: 0.6646109819412231\n",
      "cnt: 0 - valLoss: 0.6663137674331665 - trainLoss: 0.6646108627319336\n",
      "cnt: 0 - valLoss: 0.666313648223877 - trainLoss: 0.6646108031272888\n",
      "cnt: 0 - valLoss: 0.6663135290145874 - trainLoss: 0.6646106839179993\n",
      "cnt: 0 - valLoss: 0.6663134098052979 - trainLoss: 0.6646106243133545\n",
      "cnt: 0 - valLoss: 0.6663133502006531 - trainLoss: 0.6646105051040649\n",
      "cnt: 0 - valLoss: 0.6663132309913635 - trainLoss: 0.6646103858947754\n",
      "cnt: 0 - valLoss: 0.666313111782074 - trainLoss: 0.6646102666854858\n",
      "cnt: 0 - valLoss: 0.6663129329681396 - trainLoss: 0.6646102070808411\n",
      "cnt: 0 - valLoss: 0.6663129329681396 - trainLoss: 0.6646100878715515\n",
      "cnt: 0 - valLoss: 0.6663127541542053 - trainLoss: 0.6646100282669067\n",
      "cnt: 0 - valLoss: 0.6663126349449158 - trainLoss: 0.6646098494529724\n",
      "cnt: 0 - valLoss: 0.666312575340271 - trainLoss: 0.6646097898483276\n",
      "cnt: 0 - valLoss: 0.6663123965263367 - trainLoss: 0.6646096706390381\n",
      "cnt: 0 - valLoss: 0.6663123369216919 - trainLoss: 0.6646094918251038\n",
      "cnt: 0 - valLoss: 0.6663121581077576 - trainLoss: 0.664609432220459\n",
      "cnt: 0 - valLoss: 0.666312038898468 - trainLoss: 0.6646093130111694\n",
      "cnt: 0 - valLoss: 0.666312038898468 - trainLoss: 0.6646092534065247\n",
      "cnt: 0 - valLoss: 0.6663118600845337 - trainLoss: 0.6646091341972351\n",
      "cnt: 0 - valLoss: 0.6663118004798889 - trainLoss: 0.6646090149879456\n",
      "cnt: 0 - valLoss: 0.6663116216659546 - trainLoss: 0.664608895778656\n",
      "cnt: 0 - valLoss: 0.6663115620613098 - trainLoss: 0.6646088361740112\n",
      "cnt: 0 - valLoss: 0.6663114428520203 - trainLoss: 0.6646087169647217\n",
      "cnt: 0 - valLoss: 0.6663113236427307 - trainLoss: 0.6646085977554321\n",
      "cnt: 0 - valLoss: 0.6663112044334412 - trainLoss: 0.6646085381507874\n",
      "cnt: 0 - valLoss: 0.6663110852241516 - trainLoss: 0.6646084189414978\n",
      "cnt: 0 - valLoss: 0.6663109660148621 - trainLoss: 0.6646082997322083\n",
      "cnt: 0 - valLoss: 0.6663108468055725 - trainLoss: 0.6646082401275635\n",
      "cnt: 0 - valLoss: 0.6663107872009277 - trainLoss: 0.6646081209182739\n",
      "cnt: 0 - valLoss: 0.6663106083869934 - trainLoss: 0.6646080017089844\n",
      "cnt: 0 - valLoss: 0.6663105487823486 - trainLoss: 0.6646078824996948\n",
      "cnt: 0 - valLoss: 0.6663104295730591 - trainLoss: 0.6646077632904053\n",
      "cnt: 0 - valLoss: 0.6663103699684143 - trainLoss: 0.6646077036857605\n",
      "cnt: 0 - valLoss: 0.66631019115448 - trainLoss: 0.664607584476471\n",
      "cnt: 0 - valLoss: 0.6663100719451904 - trainLoss: 0.6646074652671814\n",
      "cnt: 0 - valLoss: 0.6663100123405457 - trainLoss: 0.6646074652671814\n",
      "cnt: 0 - valLoss: 0.6663098931312561 - trainLoss: 0.6646072864532471\n",
      "cnt: 0 - valLoss: 0.6663097143173218 - trainLoss: 0.6646071076393127\n",
      "cnt: 0 - valLoss: 0.666309654712677 - trainLoss: 0.664607048034668\n",
      "cnt: 0 - valLoss: 0.6663095355033875 - trainLoss: 0.664607048034668\n",
      "cnt: 0 - valLoss: 0.6663094162940979 - trainLoss: 0.6646068692207336\n",
      "cnt: 0 - valLoss: 0.6663092970848083 - trainLoss: 0.6646067500114441\n",
      "cnt: 0 - valLoss: 0.666309118270874 - trainLoss: 0.6646066308021545\n",
      "cnt: 0 - valLoss: 0.666309118270874 - trainLoss: 0.6646065711975098\n",
      "cnt: 0 - valLoss: 0.6663089990615845 - trainLoss: 0.6646064519882202\n",
      "cnt: 0 - valLoss: 0.6663088798522949 - trainLoss: 0.6646063327789307\n",
      "cnt: 0 - valLoss: 0.6663087606430054 - trainLoss: 0.6646062135696411\n",
      "cnt: 0 - valLoss: 0.6663086414337158 - trainLoss: 0.6646060943603516\n",
      "cnt: 0 - valLoss: 0.6663085222244263 - trainLoss: 0.664605975151062\n",
      "cnt: 0 - valLoss: 0.6663083434104919 - trainLoss: 0.664605975151062\n",
      "cnt: 0 - valLoss: 0.6663082838058472 - trainLoss: 0.6646057963371277\n",
      "cnt: 0 - valLoss: 0.6663081645965576 - trainLoss: 0.6646057367324829\n",
      "cnt: 0 - valLoss: 0.6663080453872681 - trainLoss: 0.6646056175231934\n",
      "cnt: 0 - valLoss: 0.6663079857826233 - trainLoss: 0.6646054983139038\n",
      "cnt: 0 - valLoss: 0.6663078665733337 - trainLoss: 0.6646053791046143\n",
      "cnt: 0 - valLoss: 0.6663077473640442 - trainLoss: 0.6646052598953247\n",
      "cnt: 0 - valLoss: 0.6663076281547546 - trainLoss: 0.6646052002906799\n",
      "cnt: 0 - valLoss: 0.6663075089454651 - trainLoss: 0.6646050810813904\n",
      "cnt: 0 - valLoss: 0.6663074493408203 - trainLoss: 0.6646050214767456\n",
      "cnt: 0 - valLoss: 0.666307270526886 - trainLoss: 0.6646048426628113\n",
      "cnt: 0 - valLoss: 0.6663071513175964 - trainLoss: 0.6646047830581665\n",
      "cnt: 0 - valLoss: 0.6663070917129517 - trainLoss: 0.6646047234535217\n",
      "cnt: 0 - valLoss: 0.6663069128990173 - trainLoss: 0.6646045446395874\n",
      "cnt: 0 - valLoss: 0.6663068532943726 - trainLoss: 0.6646044850349426\n",
      "cnt: 0 - valLoss: 0.666306734085083 - trainLoss: 0.6646043658256531\n",
      "cnt: 0 - valLoss: 0.6663066744804382 - trainLoss: 0.6646042466163635\n",
      "cnt: 0 - valLoss: 0.6663064956665039 - trainLoss: 0.664604127407074\n",
      "cnt: 0 - valLoss: 0.6663063764572144 - trainLoss: 0.6646040081977844\n",
      "cnt: 0 - valLoss: 0.6663062572479248 - trainLoss: 0.6646040081977844\n",
      "cnt: 0 - valLoss: 0.6663061380386353 - trainLoss: 0.6646038293838501\n",
      "cnt: 0 - valLoss: 0.6663060784339905 - trainLoss: 0.6646037697792053\n",
      "cnt: 0 - valLoss: 0.6663058996200562 - trainLoss: 0.6646036505699158\n",
      "cnt: 0 - valLoss: 0.6663057804107666 - trainLoss: 0.6646035313606262\n",
      "cnt: 0 - valLoss: 0.6663057208061218 - trainLoss: 0.6646034121513367\n",
      "cnt: 0 - valLoss: 0.6663056015968323 - trainLoss: 0.6646033525466919\n",
      "cnt: 0 - valLoss: 0.6663054823875427 - trainLoss: 0.6646032333374023\n",
      "cnt: 0 - valLoss: 0.6663053631782532 - trainLoss: 0.6646031737327576\n",
      "cnt: 0 - valLoss: 0.6663053035736084 - trainLoss: 0.6646029949188232\n",
      "cnt: 0 - valLoss: 0.6663051843643188 - trainLoss: 0.6646029353141785\n",
      "cnt: 0 - valLoss: 0.6663050055503845 - trainLoss: 0.6646028161048889\n",
      "cnt: 0 - valLoss: 0.666304886341095 - trainLoss: 0.6646026968955994\n",
      "cnt: 0 - valLoss: 0.6663048267364502 - trainLoss: 0.6646025776863098\n",
      "cnt: 0 - valLoss: 0.6663047075271606 - trainLoss: 0.6646024584770203\n",
      "cnt: 0 - valLoss: 0.6663045883178711 - trainLoss: 0.6646023392677307\n",
      "cnt: 0 - valLoss: 0.6663044095039368 - trainLoss: 0.6646022796630859\n",
      "cnt: 0 - valLoss: 0.6663044095039368 - trainLoss: 0.6646021604537964\n",
      "cnt: 0 - valLoss: 0.6663042306900024 - trainLoss: 0.6646020412445068\n",
      "cnt: 0 - valLoss: 0.6663041114807129 - trainLoss: 0.6646019816398621\n",
      "cnt: 0 - valLoss: 0.6663039922714233 - trainLoss: 0.6646019220352173\n",
      "cnt: 0 - valLoss: 0.6663038730621338 - trainLoss: 0.664601743221283\n",
      "cnt: 0 - valLoss: 0.666303813457489 - trainLoss: 0.6646016836166382\n",
      "cnt: 0 - valLoss: 0.6663037538528442 - trainLoss: 0.6646015644073486\n",
      "cnt: 0 - valLoss: 0.6663035750389099 - trainLoss: 0.6646014451980591\n",
      "cnt: 0 - valLoss: 0.6663034558296204 - trainLoss: 0.6646013855934143\n",
      "cnt: 0 - valLoss: 0.6663033366203308 - trainLoss: 0.6646012663841248\n",
      "cnt: 0 - valLoss: 0.666303277015686 - trainLoss: 0.6646011471748352\n",
      "cnt: 0 - valLoss: 0.6663030982017517 - trainLoss: 0.6646010279655457\n",
      "cnt: 0 - valLoss: 0.6663030385971069 - trainLoss: 0.6646009683609009\n",
      "cnt: 0 - valLoss: 0.6663028597831726 - trainLoss: 0.6646008491516113\n",
      "cnt: 0 - valLoss: 0.6663028001785278 - trainLoss: 0.6646007299423218\n",
      "cnt: 0 - valLoss: 0.6663026809692383 - trainLoss: 0.664600670337677\n",
      "cnt: 0 - valLoss: 0.6663025617599487 - trainLoss: 0.6646004915237427\n",
      "cnt: 0 - valLoss: 0.6663024425506592 - trainLoss: 0.6646004319190979\n",
      "cnt: 0 - valLoss: 0.6663023233413696 - trainLoss: 0.6646003723144531\n",
      "cnt: 0 - valLoss: 0.6663022637367249 - trainLoss: 0.6646001935005188\n",
      "cnt: 0 - valLoss: 0.6663020849227905 - trainLoss: 0.6646000742912292\n",
      "cnt: 0 - valLoss: 0.666301965713501 - trainLoss: 0.6645999550819397\n",
      "cnt: 0 - valLoss: 0.6663018465042114 - trainLoss: 0.6645998954772949\n",
      "cnt: 0 - valLoss: 0.6663017868995667 - trainLoss: 0.6645997166633606\n",
      "cnt: 0 - valLoss: 0.6663016676902771 - trainLoss: 0.6645996570587158\n",
      "cnt: 0 - valLoss: 0.6663014888763428 - trainLoss: 0.664599597454071\n",
      "cnt: 0 - valLoss: 0.666301429271698 - trainLoss: 0.6645994782447815\n",
      "cnt: 0 - valLoss: 0.6663012504577637 - trainLoss: 0.6645994186401367\n",
      "cnt: 0 - valLoss: 0.6663011908531189 - trainLoss: 0.6645992994308472\n",
      "cnt: 0 - valLoss: 0.6663010120391846 - trainLoss: 0.6645991206169128\n",
      "cnt: 0 - valLoss: 0.6663010120391846 - trainLoss: 0.6645990610122681\n",
      "cnt: 0 - valLoss: 0.666300892829895 - trainLoss: 0.6645990014076233\n",
      "cnt: 0 - valLoss: 0.6663007140159607 - trainLoss: 0.6645988821983337\n",
      "cnt: 0 - valLoss: 0.6663006544113159 - trainLoss: 0.664598822593689\n",
      "cnt: 0 - valLoss: 0.6663004755973816 - trainLoss: 0.6645986437797546\n",
      "cnt: 0 - valLoss: 0.6663004159927368 - trainLoss: 0.6645985245704651\n",
      "cnt: 0 - valLoss: 0.6663002967834473 - trainLoss: 0.6645984649658203\n",
      "cnt: 0 - valLoss: 0.6663001775741577 - trainLoss: 0.6645983457565308\n",
      "cnt: 0 - valLoss: 0.6663000583648682 - trainLoss: 0.6645982265472412\n",
      "cnt: 0 - valLoss: 0.6662999391555786 - trainLoss: 0.6645980477333069\n",
      "cnt: 0 - valLoss: 0.6662998199462891 - trainLoss: 0.6645980477333069\n",
      "cnt: 0 - valLoss: 0.6662997007369995 - trainLoss: 0.6645979285240173\n",
      "cnt: 0 - valLoss: 0.66629958152771 - trainLoss: 0.6645978093147278\n",
      "cnt: 0 - valLoss: 0.6662995219230652 - trainLoss: 0.6645976901054382\n",
      "cnt: 0 - valLoss: 0.6662993431091309 - trainLoss: 0.6645976305007935\n",
      "cnt: 0 - valLoss: 0.6662992835044861 - trainLoss: 0.6645975112915039\n",
      "cnt: 0 - valLoss: 0.6662991642951965 - trainLoss: 0.6645973920822144\n",
      "cnt: 0 - valLoss: 0.6662989854812622 - trainLoss: 0.6645973324775696\n",
      "cnt: 0 - valLoss: 0.6662989258766174 - trainLoss: 0.66459721326828\n",
      "cnt: 0 - valLoss: 0.6662988066673279 - trainLoss: 0.6645970940589905\n",
      "cnt: 0 - valLoss: 0.6662986874580383 - trainLoss: 0.6645970344543457\n",
      "cnt: 0 - valLoss: 0.666298508644104 - trainLoss: 0.6645968556404114\n",
      "cnt: 0 - valLoss: 0.6662984490394592 - trainLoss: 0.6645967364311218\n",
      "cnt: 0 - valLoss: 0.6662983298301697 - trainLoss: 0.664596676826477\n",
      "cnt: 0 - valLoss: 0.6662982106208801 - trainLoss: 0.6645965576171875\n",
      "cnt: 0 - valLoss: 0.6662980914115906 - trainLoss: 0.664596438407898\n",
      "cnt: 0 - valLoss: 0.666297972202301 - trainLoss: 0.6645963788032532\n",
      "cnt: 0 - valLoss: 0.6662978529930115 - trainLoss: 0.6645962595939636\n",
      "cnt: 0 - valLoss: 0.6662977933883667 - trainLoss: 0.6645961403846741\n",
      "cnt: 0 - valLoss: 0.6662976741790771 - trainLoss: 0.6645960807800293\n",
      "cnt: 0 - valLoss: 0.6662974953651428 - trainLoss: 0.6645959615707397\n",
      "cnt: 0 - valLoss: 0.6662974953651428 - trainLoss: 0.6645958423614502\n",
      "cnt: 0 - valLoss: 0.6662973165512085 - trainLoss: 0.6645957231521606\n",
      "cnt: 0 - valLoss: 0.666297197341919 - trainLoss: 0.6645956635475159\n",
      "cnt: 0 - valLoss: 0.6662970781326294 - trainLoss: 0.6645955443382263\n",
      "cnt: 0 - valLoss: 0.6662969589233398 - trainLoss: 0.664595365524292\n",
      "cnt: 0 - valLoss: 0.6662968993186951 - trainLoss: 0.6645953059196472\n",
      "cnt: 0 - valLoss: 0.6662967801094055 - trainLoss: 0.6645951867103577\n",
      "cnt: 0 - valLoss: 0.6662966012954712 - trainLoss: 0.6645950675010681\n",
      "cnt: 0 - valLoss: 0.6662964820861816 - trainLoss: 0.6645950078964233\n",
      "cnt: 0 - valLoss: 0.6662964224815369 - trainLoss: 0.6645948886871338\n",
      "cnt: 0 - valLoss: 0.6662963032722473 - trainLoss: 0.664594829082489\n",
      "cnt: 0 - valLoss: 0.6662961840629578 - trainLoss: 0.6645946502685547\n",
      "cnt: 0 - valLoss: 0.6662960052490234 - trainLoss: 0.6645945906639099\n",
      "cnt: 0 - valLoss: 0.6662958860397339 - trainLoss: 0.6645945310592651\n",
      "cnt: 0 - valLoss: 0.6662958264350891 - trainLoss: 0.6645944118499756\n",
      "cnt: 0 - valLoss: 0.6662957072257996 - trainLoss: 0.664594292640686\n",
      "cnt: 0 - valLoss: 0.66629558801651 - trainLoss: 0.6645941734313965\n",
      "cnt: 0 - valLoss: 0.6662955284118652 - trainLoss: 0.6645940542221069\n",
      "cnt: 0 - valLoss: 0.6662953495979309 - trainLoss: 0.6645939946174622\n",
      "cnt: 0 - valLoss: 0.6662952303886414 - trainLoss: 0.6645938158035278\n",
      "cnt: 0 - valLoss: 0.6662951111793518 - trainLoss: 0.6645936965942383\n",
      "cnt: 0 - valLoss: 0.666295051574707 - trainLoss: 0.6645936965942383\n",
      "cnt: 0 - valLoss: 0.6662948727607727 - trainLoss: 0.664593517780304\n",
      "cnt: 0 - valLoss: 0.6662948131561279 - trainLoss: 0.6645933985710144\n",
      "cnt: 0 - valLoss: 0.6662946343421936 - trainLoss: 0.6645933985710144\n",
      "cnt: 0 - valLoss: 0.666294515132904 - trainLoss: 0.6645932197570801\n",
      "cnt: 0 - valLoss: 0.6662944555282593 - trainLoss: 0.6645931005477905\n",
      "cnt: 0 - valLoss: 0.6662943363189697 - trainLoss: 0.6645930409431458\n",
      "cnt: 0 - valLoss: 0.6662942171096802 - trainLoss: 0.6645929217338562\n",
      "cnt: 0 - valLoss: 0.6662940382957458 - trainLoss: 0.6645928025245667\n",
      "cnt: 0 - valLoss: 0.6662940382957458 - trainLoss: 0.6645926833152771\n",
      "cnt: 0 - valLoss: 0.6662938594818115 - trainLoss: 0.6645926237106323\n",
      "cnt: 0 - valLoss: 0.666293740272522 - trainLoss: 0.6645925045013428\n",
      "cnt: 0 - valLoss: 0.6662936210632324 - trainLoss: 0.6645923852920532\n",
      "cnt: 0 - valLoss: 0.6662935018539429 - trainLoss: 0.6645923256874084\n",
      "cnt: 0 - valLoss: 0.6662933826446533 - trainLoss: 0.6645921468734741\n",
      "cnt: 0 - valLoss: 0.6662933230400085 - trainLoss: 0.6645920872688293\n",
      "cnt: 0 - valLoss: 0.666293203830719 - trainLoss: 0.6645919680595398\n",
      "cnt: 0 - valLoss: 0.6662930250167847 - trainLoss: 0.6645918488502502\n",
      "cnt: 0 - valLoss: 0.6662929654121399 - trainLoss: 0.6645917296409607\n",
      "cnt: 0 - valLoss: 0.6662928462028503 - trainLoss: 0.6645916104316711\n",
      "cnt: 0 - valLoss: 0.666292667388916 - trainLoss: 0.6645916104316711\n",
      "cnt: 0 - valLoss: 0.6662926077842712 - trainLoss: 0.6645914316177368\n",
      "cnt: 0 - valLoss: 0.6662924885749817 - trainLoss: 0.664591372013092\n",
      "cnt: 0 - valLoss: 0.6662923693656921 - trainLoss: 0.6645912528038025\n",
      "cnt: 0 - valLoss: 0.6662922501564026 - trainLoss: 0.6645911335945129\n",
      "cnt: 0 - valLoss: 0.6662921905517578 - trainLoss: 0.6645910143852234\n",
      "cnt: 0 - valLoss: 0.6662920713424683 - trainLoss: 0.6645909547805786\n",
      "cnt: 0 - valLoss: 0.6662918925285339 - trainLoss: 0.6645908355712891\n",
      "cnt: 0 - valLoss: 0.6662917733192444 - trainLoss: 0.6645907163619995\n",
      "cnt: 0 - valLoss: 0.6662916541099548 - trainLoss: 0.6645906567573547\n",
      "cnt: 0 - valLoss: 0.6662915945053101 - trainLoss: 0.6645905375480652\n",
      "cnt: 0 - valLoss: 0.6662914156913757 - trainLoss: 0.6645904183387756\n",
      "cnt: 0 - valLoss: 0.6662914156913757 - trainLoss: 0.6645902991294861\n",
      "cnt: 0 - valLoss: 0.6662912368774414 - trainLoss: 0.6645901799201965\n",
      "cnt: 0 - valLoss: 0.6662911176681519 - trainLoss: 0.6645901203155518\n",
      "cnt: 0 - valLoss: 0.6662909984588623 - trainLoss: 0.6645900011062622\n",
      "cnt: 0 - valLoss: 0.6662908792495728 - trainLoss: 0.6645898818969727\n",
      "cnt: 0 - valLoss: 0.666290819644928 - trainLoss: 0.6645897030830383\n",
      "cnt: 0 - valLoss: 0.6662906408309937 - trainLoss: 0.6645897030830383\n",
      "cnt: 0 - valLoss: 0.6662905216217041 - trainLoss: 0.6645895838737488\n",
      "cnt: 0 - valLoss: 0.6662904620170593 - trainLoss: 0.6645894646644592\n",
      "cnt: 0 - valLoss: 0.666290283203125 - trainLoss: 0.6645893454551697\n",
      "cnt: 0 - valLoss: 0.6662901639938354 - trainLoss: 0.6645892858505249\n",
      "cnt: 0 - valLoss: 0.6662901043891907 - trainLoss: 0.6645891666412354\n",
      "cnt: 0 - valLoss: 0.6662899851799011 - trainLoss: 0.6645891070365906\n",
      "cnt: 0 - valLoss: 0.6662898063659668 - trainLoss: 0.664588987827301\n",
      "cnt: 0 - valLoss: 0.666289746761322 - trainLoss: 0.6645888686180115\n",
      "cnt: 0 - valLoss: 0.6662895679473877 - trainLoss: 0.6645887494087219\n",
      "cnt: 0 - valLoss: 0.6662895083427429 - trainLoss: 0.6645886301994324\n",
      "cnt: 0 - valLoss: 0.6662894487380981 - trainLoss: 0.6645885109901428\n",
      "cnt: 0 - valLoss: 0.6662892699241638 - trainLoss: 0.664588451385498\n",
      "cnt: 0 - valLoss: 0.6662891507148743 - trainLoss: 0.6645883321762085\n",
      "cnt: 0 - valLoss: 0.6662890911102295 - trainLoss: 0.664588212966919\n",
      "cnt: 0 - valLoss: 0.6662889122962952 - trainLoss: 0.6645881533622742\n",
      "cnt: 0 - valLoss: 0.6662887930870056 - trainLoss: 0.6645880341529846\n",
      "cnt: 0 - valLoss: 0.6662886738777161 - trainLoss: 0.6645879149436951\n",
      "cnt: 0 - valLoss: 0.6662885546684265 - trainLoss: 0.6645877957344055\n",
      "cnt: 0 - valLoss: 0.6662884950637817 - trainLoss: 0.664587676525116\n",
      "cnt: 0 - valLoss: 0.6662883162498474 - trainLoss: 0.6645876169204712\n",
      "cnt: 0 - valLoss: 0.6662881970405579 - trainLoss: 0.6645874977111816\n",
      "cnt: 0 - valLoss: 0.6662881374359131 - trainLoss: 0.6645874381065369\n",
      "cnt: 0 - valLoss: 0.6662880182266235 - trainLoss: 0.6645873188972473\n",
      "cnt: 0 - valLoss: 0.666287899017334 - trainLoss: 0.664587140083313\n",
      "cnt: 0 - valLoss: 0.6662877798080444 - trainLoss: 0.6645870208740234\n",
      "cnt: 0 - valLoss: 0.6662876009941101 - trainLoss: 0.6645870208740234\n",
      "cnt: 0 - valLoss: 0.6662874817848206 - trainLoss: 0.6645868420600891\n",
      "cnt: 0 - valLoss: 0.666287362575531 - trainLoss: 0.6645867228507996\n",
      "cnt: 0 - valLoss: 0.6662873029708862 - trainLoss: 0.6645866632461548\n",
      "cnt: 0 - valLoss: 0.6662871837615967 - trainLoss: 0.6645865440368652\n",
      "cnt: 0 - valLoss: 0.6662870645523071 - trainLoss: 0.6645864844322205\n",
      "cnt: 0 - valLoss: 0.6662868857383728 - trainLoss: 0.6645863652229309\n",
      "cnt: 0 - valLoss: 0.666286826133728 - trainLoss: 0.6645862460136414\n",
      "cnt: 0 - valLoss: 0.6662867069244385 - trainLoss: 0.6645861864089966\n",
      "cnt: 0 - valLoss: 0.6662865877151489 - trainLoss: 0.6645860075950623\n",
      "cnt: 0 - valLoss: 0.6662864685058594 - trainLoss: 0.6645859479904175\n",
      "cnt: 0 - valLoss: 0.6662863492965698 - trainLoss: 0.6645858287811279\n",
      "cnt: 0 - valLoss: 0.6662862300872803 - trainLoss: 0.6645857095718384\n",
      "cnt: 0 - valLoss: 0.6662861704826355 - trainLoss: 0.6645856499671936\n",
      "cnt: 0 - valLoss: 0.666286051273346 - trainLoss: 0.6645854711532593\n",
      "cnt: 0 - valLoss: 0.6662859320640564 - trainLoss: 0.6645854115486145\n",
      "cnt: 0 - valLoss: 0.6662858128547668 - trainLoss: 0.664585292339325\n",
      "cnt: 0 - valLoss: 0.6662856340408325 - trainLoss: 0.6645851731300354\n",
      "cnt: 0 - valLoss: 0.6662855744361877 - trainLoss: 0.6645851135253906\n",
      "cnt: 0 - valLoss: 0.6662854552268982 - trainLoss: 0.6645849943161011\n",
      "cnt: 0 - valLoss: 0.6662853360176086 - trainLoss: 0.6645848751068115\n",
      "cnt: 0 - valLoss: 0.6662852168083191 - trainLoss: 0.664584755897522\n",
      "cnt: 0 - valLoss: 0.6662850975990295 - trainLoss: 0.6645846962928772\n",
      "cnt: 0 - valLoss: 0.66628497838974 - trainLoss: 0.6645845770835876\n",
      "cnt: 0 - valLoss: 0.6662848591804504 - trainLoss: 0.6645845174789429\n",
      "cnt: 0 - valLoss: 0.6662847399711609 - trainLoss: 0.6645843386650085\n",
      "cnt: 0 - valLoss: 0.6662846207618713 - trainLoss: 0.6645842790603638\n",
      "cnt: 0 - valLoss: 0.6662845611572266 - trainLoss: 0.6645841598510742\n",
      "cnt: 0 - valLoss: 0.666284441947937 - trainLoss: 0.6645840406417847\n",
      "cnt: 0 - valLoss: 0.6662842631340027 - trainLoss: 0.6645839810371399\n",
      "cnt: 0 - valLoss: 0.6662842035293579 - trainLoss: 0.6645838022232056\n",
      "cnt: 0 - valLoss: 0.6662840247154236 - trainLoss: 0.6645838022232056\n",
      "cnt: 0 - valLoss: 0.6662839651107788 - trainLoss: 0.6645836234092712\n",
      "cnt: 0 - valLoss: 0.6662838459014893 - trainLoss: 0.6645835041999817\n",
      "cnt: 0 - valLoss: 0.6662837266921997 - trainLoss: 0.6645833849906921\n",
      "cnt: 0 - valLoss: 0.6662835478782654 - trainLoss: 0.6645833253860474\n",
      "cnt: 0 - valLoss: 0.6662834882736206 - trainLoss: 0.6645832061767578\n",
      "cnt: 0 - valLoss: 0.666283369064331 - trainLoss: 0.6645830869674683\n",
      "cnt: 0 - valLoss: 0.6662832498550415 - trainLoss: 0.6645830273628235\n",
      "cnt: 0 - valLoss: 0.6662830710411072 - trainLoss: 0.6645829081535339\n",
      "cnt: 0 - valLoss: 0.6662830114364624 - trainLoss: 0.6645827889442444\n",
      "cnt: 0 - valLoss: 0.6662828922271729 - trainLoss: 0.6645827293395996\n",
      "cnt: 0 - valLoss: 0.6662827134132385 - trainLoss: 0.6645826101303101\n",
      "cnt: 0 - valLoss: 0.6662826538085938 - trainLoss: 0.6645824909210205\n",
      "cnt: 0 - valLoss: 0.6662825345993042 - trainLoss: 0.664582371711731\n",
      "cnt: 0 - valLoss: 0.6662824153900146 - trainLoss: 0.6645822525024414\n",
      "cnt: 0 - valLoss: 0.6662822961807251 - trainLoss: 0.6645821928977966\n",
      "cnt: 0 - valLoss: 0.6662821769714355 - trainLoss: 0.6645821332931519\n",
      "cnt: 0 - valLoss: 0.6662821173667908 - trainLoss: 0.6645819544792175\n",
      "cnt: 0 - valLoss: 0.6662819385528564 - trainLoss: 0.6645818948745728\n",
      "cnt: 0 - valLoss: 0.6662818789482117 - trainLoss: 0.6645817756652832\n",
      "cnt: 0 - valLoss: 0.6662817001342773 - trainLoss: 0.6645816564559937\n",
      "cnt: 0 - valLoss: 0.6662816405296326 - trainLoss: 0.6645815372467041\n",
      "cnt: 0 - valLoss: 0.666281521320343 - trainLoss: 0.6645814776420593\n",
      "cnt: 0 - valLoss: 0.6662813425064087 - trainLoss: 0.6645813584327698\n",
      "cnt: 0 - valLoss: 0.6662812829017639 - trainLoss: 0.6645812392234802\n",
      "cnt: 0 - valLoss: 0.6662812232971191 - trainLoss: 0.6645811796188354\n",
      "cnt: 0 - valLoss: 0.6662810444831848 - trainLoss: 0.6645810008049011\n",
      "cnt: 0 - valLoss: 0.6662809252738953 - trainLoss: 0.6645809412002563\n",
      "cnt: 0 - valLoss: 0.6662807464599609 - trainLoss: 0.6645808219909668\n",
      "cnt: 0 - valLoss: 0.6662806868553162 - trainLoss: 0.664580762386322\n",
      "cnt: 0 - valLoss: 0.6662805676460266 - trainLoss: 0.6645806431770325\n",
      "cnt: 0 - valLoss: 0.6662804484367371 - trainLoss: 0.6645805239677429\n",
      "cnt: 0 - valLoss: 0.6662803292274475 - trainLoss: 0.6645804047584534\n",
      "cnt: 0 - valLoss: 0.666280210018158 - trainLoss: 0.6645802855491638\n",
      "cnt: 0 - valLoss: 0.6662800908088684 - trainLoss: 0.6645801663398743\n",
      "cnt: 0 - valLoss: 0.6662799715995789 - trainLoss: 0.6645801067352295\n",
      "cnt: 0 - valLoss: 0.6662798523902893 - trainLoss: 0.6645799875259399\n",
      "cnt: 0 - valLoss: 0.6662797331809998 - trainLoss: 0.6645798683166504\n",
      "cnt: 0 - valLoss: 0.6662796139717102 - trainLoss: 0.6645798087120056\n",
      "cnt: 0 - valLoss: 0.6662794947624207 - trainLoss: 0.6645796895027161\n",
      "cnt: 0 - valLoss: 0.6662793755531311 - trainLoss: 0.6645795702934265\n",
      "cnt: 0 - valLoss: 0.6662792563438416 - trainLoss: 0.6645795106887817\n",
      "cnt: 0 - valLoss: 0.666279137134552 - trainLoss: 0.6645793914794922\n",
      "cnt: 0 - valLoss: 0.6662790775299072 - trainLoss: 0.6645792722702026\n",
      "cnt: 0 - valLoss: 0.6662788987159729 - trainLoss: 0.6645790934562683\n",
      "cnt: 0 - valLoss: 0.6662787795066833 - trainLoss: 0.6645790338516235\n",
      "cnt: 0 - valLoss: 0.6662786602973938 - trainLoss: 0.6645789742469788\n",
      "cnt: 0 - valLoss: 0.666278600692749 - trainLoss: 0.6645788550376892\n",
      "cnt: 0 - valLoss: 0.6662784814834595 - trainLoss: 0.6645787954330444\n",
      "cnt: 0 - valLoss: 0.6662783026695251 - trainLoss: 0.6645786166191101\n",
      "cnt: 0 - valLoss: 0.6662781834602356 - trainLoss: 0.6645785570144653\n",
      "cnt: 0 - valLoss: 0.6662781238555908 - trainLoss: 0.6645784378051758\n",
      "cnt: 0 - valLoss: 0.6662779450416565 - trainLoss: 0.6645783185958862\n",
      "cnt: 0 - valLoss: 0.6662778258323669 - trainLoss: 0.6645781993865967\n",
      "cnt: 0 - valLoss: 0.6662777066230774 - trainLoss: 0.6645781397819519\n",
      "cnt: 0 - valLoss: 0.6662775874137878 - trainLoss: 0.6645780205726624\n",
      "cnt: 0 - valLoss: 0.6662775278091431 - trainLoss: 0.6645779013633728\n",
      "cnt: 0 - valLoss: 0.6662774085998535 - trainLoss: 0.6645777821540833\n",
      "cnt: 0 - valLoss: 0.666277289390564 - trainLoss: 0.6645777225494385\n",
      "cnt: 0 - valLoss: 0.6662771701812744 - trainLoss: 0.6645776033401489\n",
      "cnt: 0 - valLoss: 0.6662770509719849 - trainLoss: 0.6645774841308594\n",
      "cnt: 0 - valLoss: 0.6662769317626953 - trainLoss: 0.6645774245262146\n",
      "cnt: 0 - valLoss: 0.6662768125534058 - trainLoss: 0.664577305316925\n",
      "cnt: 0 - valLoss: 0.666276752948761 - trainLoss: 0.6645771861076355\n",
      "cnt: 0 - valLoss: 0.6662765741348267 - trainLoss: 0.664577066898346\n",
      "cnt: 0 - valLoss: 0.6662764549255371 - trainLoss: 0.6645770072937012\n",
      "cnt: 0 - valLoss: 0.6662763357162476 - trainLoss: 0.6645768880844116\n",
      "cnt: 0 - valLoss: 0.666276216506958 - trainLoss: 0.6645768284797668\n",
      "cnt: 0 - valLoss: 0.6662760972976685 - trainLoss: 0.6645766496658325\n",
      "cnt: 0 - valLoss: 0.6662759780883789 - trainLoss: 0.664576530456543\n",
      "cnt: 0 - valLoss: 0.6662758588790894 - trainLoss: 0.6645764708518982\n",
      "cnt: 0 - valLoss: 0.6662757992744446 - trainLoss: 0.6645763516426086\n",
      "cnt: 0 - valLoss: 0.6662756204605103 - trainLoss: 0.6645762920379639\n",
      "cnt: 0 - valLoss: 0.6662755608558655 - trainLoss: 0.6645761728286743\n",
      "cnt: 0 - valLoss: 0.6662754416465759 - trainLoss: 0.6645760536193848\n",
      "cnt: 0 - valLoss: 0.6662752628326416 - trainLoss: 0.66457599401474\n",
      "cnt: 0 - valLoss: 0.6662752032279968 - trainLoss: 0.6645758748054504\n",
      "cnt: 0 - valLoss: 0.6662750244140625 - trainLoss: 0.6645757555961609\n",
      "cnt: 0 - valLoss: 0.6662749648094177 - trainLoss: 0.6645756959915161\n",
      "cnt: 0 - valLoss: 0.6662747859954834 - trainLoss: 0.6645755171775818\n",
      "cnt: 0 - valLoss: 0.6662747263908386 - trainLoss: 0.664575457572937\n",
      "cnt: 0 - valLoss: 0.6662746667861938 - trainLoss: 0.6645753383636475\n",
      "cnt: 0 - valLoss: 0.6662744879722595 - trainLoss: 0.6645752787590027\n",
      "cnt: 0 - valLoss: 0.66627436876297 - trainLoss: 0.6645751595497131\n",
      "cnt: 0 - valLoss: 0.6662741899490356 - trainLoss: 0.6645749807357788\n",
      "cnt: 0 - valLoss: 0.6662741303443909 - trainLoss: 0.664574921131134\n",
      "cnt: 0 - valLoss: 0.6662740111351013 - trainLoss: 0.6645748615264893\n",
      "cnt: 0 - valLoss: 0.6662738919258118 - trainLoss: 0.6645746827125549\n",
      "cnt: 0 - valLoss: 0.6662737131118774 - trainLoss: 0.6645745635032654\n",
      "cnt: 0 - valLoss: 0.6662736535072327 - trainLoss: 0.6645745038986206\n",
      "cnt: 0 - valLoss: 0.6662735342979431 - trainLoss: 0.664574384689331\n",
      "cnt: 0 - valLoss: 0.6662734150886536 - trainLoss: 0.6645743250846863\n",
      "cnt: 0 - valLoss: 0.666273295879364 - trainLoss: 0.6645742654800415\n",
      "cnt: 0 - valLoss: 0.6662731766700745 - trainLoss: 0.6645740866661072\n",
      "cnt: 0 - valLoss: 0.6662731170654297 - trainLoss: 0.6645739674568176\n",
      "cnt: 0 - valLoss: 0.6662729382514954 - trainLoss: 0.6645739078521729\n",
      "cnt: 0 - valLoss: 0.6662728190422058 - trainLoss: 0.6645737886428833\n",
      "cnt: 0 - valLoss: 0.6662726998329163 - trainLoss: 0.6645737290382385\n",
      "cnt: 0 - valLoss: 0.6662725806236267 - trainLoss: 0.6645735502243042\n",
      "cnt: 0 - valLoss: 0.6662724614143372 - trainLoss: 0.6645734906196594\n",
      "cnt: 0 - valLoss: 0.6662723422050476 - trainLoss: 0.6645733118057251\n",
      "cnt: 0 - valLoss: 0.6662722229957581 - trainLoss: 0.6645731925964355\n",
      "cnt: 0 - valLoss: 0.6662721037864685 - trainLoss: 0.6645731925964355\n",
      "cnt: 0 - valLoss: 0.6662720441818237 - trainLoss: 0.6645731329917908\n",
      "cnt: 0 - valLoss: 0.6662719249725342 - trainLoss: 0.6645729541778564\n",
      "cnt: 0 - valLoss: 0.6662717461585999 - trainLoss: 0.6645728945732117\n",
      "cnt: 0 - valLoss: 0.6662716269493103 - trainLoss: 0.6645727157592773\n",
      "cnt: 0 - valLoss: 0.6662715673446655 - trainLoss: 0.6645726561546326\n",
      "cnt: 0 - valLoss: 0.666271448135376 - trainLoss: 0.664572536945343\n",
      "cnt: 0 - valLoss: 0.6662712693214417 - trainLoss: 0.6645724177360535\n",
      "cnt: 0 - valLoss: 0.6662711501121521 - trainLoss: 0.6645723581314087\n",
      "cnt: 0 - valLoss: 0.6662710309028625 - trainLoss: 0.6645722389221191\n",
      "cnt: 0 - valLoss: 0.6662709712982178 - trainLoss: 0.6645721197128296\n",
      "cnt: 0 - valLoss: 0.6662708520889282 - trainLoss: 0.66457200050354\n",
      "cnt: 0 - valLoss: 0.6662707328796387 - trainLoss: 0.6645719408988953\n",
      "cnt: 0 - valLoss: 0.6662705540657043 - trainLoss: 0.6645718216896057\n",
      "cnt: 0 - valLoss: 0.6662704348564148 - trainLoss: 0.6645716428756714\n",
      "cnt: 0 - valLoss: 0.6662703156471252 - trainLoss: 0.6645715832710266\n",
      "cnt: 0 - valLoss: 0.6662701964378357 - trainLoss: 0.6645715236663818\n",
      "cnt: 0 - valLoss: 0.6662700772285461 - trainLoss: 0.6645713448524475\n",
      "cnt: 0 - valLoss: 0.6662700176239014 - trainLoss: 0.6645712852478027\n",
      "cnt: 0 - valLoss: 0.6662698984146118 - trainLoss: 0.664571225643158\n",
      "cnt: 0 - valLoss: 0.6662697792053223 - trainLoss: 0.6645710468292236\n",
      "cnt: 0 - valLoss: 0.6662696003913879 - trainLoss: 0.6645709872245789\n",
      "cnt: 0 - valLoss: 0.6662695407867432 - trainLoss: 0.6645708680152893\n",
      "cnt: 0 - valLoss: 0.6662693619728088 - trainLoss: 0.6645707488059998\n",
      "cnt: 0 - valLoss: 0.6662693023681641 - trainLoss: 0.6645706295967102\n",
      "cnt: 0 - valLoss: 0.6662691831588745 - trainLoss: 0.6645705699920654\n",
      "cnt: 0 - valLoss: 0.666269063949585 - trainLoss: 0.6645704507827759\n",
      "cnt: 0 - valLoss: 0.6662689447402954 - trainLoss: 0.6645703315734863\n",
      "cnt: 0 - valLoss: 0.6662687659263611 - trainLoss: 0.6645702719688416\n",
      "cnt: 0 - valLoss: 0.6662687063217163 - trainLoss: 0.664570152759552\n",
      "cnt: 0 - valLoss: 0.666268527507782 - trainLoss: 0.6645700335502625\n",
      "cnt: 0 - valLoss: 0.6662684679031372 - trainLoss: 0.6645699739456177\n",
      "cnt: 0 - valLoss: 0.6662683486938477 - trainLoss: 0.6645698547363281\n",
      "cnt: 0 - valLoss: 0.6662682294845581 - trainLoss: 0.6645697951316833\n",
      "cnt: 0 - valLoss: 0.6662681102752686 - trainLoss: 0.664569616317749\n",
      "cnt: 0 - valLoss: 0.666267991065979 - trainLoss: 0.6645695567131042\n",
      "cnt: 0 - valLoss: 0.6662678718566895 - trainLoss: 0.6645694375038147\n",
      "cnt: 0 - valLoss: 0.6662677526473999 - trainLoss: 0.6645693182945251\n",
      "cnt: 0 - valLoss: 0.6662676334381104 - trainLoss: 0.6645691990852356\n",
      "cnt: 0 - valLoss: 0.666267454624176 - trainLoss: 0.664569079875946\n",
      "cnt: 0 - valLoss: 0.6662673950195312 - trainLoss: 0.6645690202713013\n",
      "cnt: 0 - valLoss: 0.6662673354148865 - trainLoss: 0.6645689010620117\n",
      "cnt: 0 - valLoss: 0.6662671566009521 - trainLoss: 0.6645687818527222\n",
      "cnt: 0 - valLoss: 0.6662670373916626 - trainLoss: 0.6645687222480774\n",
      "cnt: 0 - valLoss: 0.6662669777870178 - trainLoss: 0.6645686626434326\n",
      "cnt: 0 - valLoss: 0.6662667989730835 - trainLoss: 0.6645684838294983\n",
      "cnt: 0 - valLoss: 0.666266679763794 - trainLoss: 0.6645683646202087\n",
      "cnt: 0 - valLoss: 0.6662665605545044 - trainLoss: 0.664568305015564\n",
      "cnt: 0 - valLoss: 0.6662664413452148 - trainLoss: 0.6645681858062744\n",
      "cnt: 0 - valLoss: 0.6662663221359253 - trainLoss: 0.6645680665969849\n",
      "cnt: 0 - valLoss: 0.6662662029266357 - trainLoss: 0.6645680069923401\n",
      "cnt: 0 - valLoss: 0.6662660837173462 - trainLoss: 0.6645678877830505\n",
      "cnt: 0 - valLoss: 0.6662659645080566 - trainLoss: 0.6645677089691162\n",
      "cnt: 0 - valLoss: 0.6662658452987671 - trainLoss: 0.6645676493644714\n",
      "cnt: 0 - valLoss: 0.6662657260894775 - trainLoss: 0.6645675301551819\n",
      "cnt: 0 - valLoss: 0.6662656664848328 - trainLoss: 0.6645674705505371\n",
      "cnt: 0 - valLoss: 0.6662654876708984 - trainLoss: 0.6645672917366028\n",
      "cnt: 0 - valLoss: 0.6662653684616089 - trainLoss: 0.664567232131958\n",
      "cnt: 0 - valLoss: 0.6662652492523193 - trainLoss: 0.6645671129226685\n",
      "cnt: 0 - valLoss: 0.6662651896476746 - trainLoss: 0.6645671129226685\n",
      "cnt: 0 - valLoss: 0.6662650108337402 - trainLoss: 0.6645668745040894\n",
      "cnt: 0 - valLoss: 0.6662648916244507 - trainLoss: 0.6645668148994446\n",
      "cnt: 0 - valLoss: 0.6662648320198059 - trainLoss: 0.664566695690155\n",
      "cnt: 0 - valLoss: 0.6662646532058716 - trainLoss: 0.6645666360855103\n",
      "cnt: 0 - valLoss: 0.6662645936012268 - trainLoss: 0.6645665168762207\n",
      "cnt: 0 - valLoss: 0.6662644147872925 - trainLoss: 0.6645663976669312\n",
      "cnt: 0 - valLoss: 0.6662642955780029 - trainLoss: 0.6645663380622864\n",
      "cnt: 0 - valLoss: 0.6662642359733582 - trainLoss: 0.6645662188529968\n",
      "cnt: 0 - valLoss: 0.6662640571594238 - trainLoss: 0.6645660996437073\n",
      "cnt: 0 - valLoss: 0.6662639379501343 - trainLoss: 0.6645660400390625\n",
      "cnt: 0 - valLoss: 0.6662638187408447 - trainLoss: 0.664565920829773\n",
      "cnt: 0 - valLoss: 0.6662636995315552 - trainLoss: 0.6645657420158386\n",
      "cnt: 0 - valLoss: 0.6662635803222656 - trainLoss: 0.6645656824111938\n",
      "cnt: 0 - valLoss: 0.6662634611129761 - trainLoss: 0.6645655632019043\n",
      "cnt: 0 - valLoss: 0.6662633419036865 - trainLoss: 0.6645655035972595\n",
      "cnt: 0 - valLoss: 0.6662632822990417 - trainLoss: 0.6645654439926147\n",
      "cnt: 0 - valLoss: 0.6662631034851074 - trainLoss: 0.6645652651786804\n",
      "cnt: 0 - valLoss: 0.6662629842758179 - trainLoss: 0.6645651459693909\n",
      "cnt: 0 - valLoss: 0.6662628650665283 - trainLoss: 0.6645650863647461\n",
      "cnt: 0 - valLoss: 0.6662627458572388 - trainLoss: 0.6645649671554565\n",
      "cnt: 0 - valLoss: 0.6662626266479492 - trainLoss: 0.664564847946167\n",
      "cnt: 0 - valLoss: 0.6662625074386597 - trainLoss: 0.6645647287368774\n",
      "cnt: 0 - valLoss: 0.6662623882293701 - trainLoss: 0.6645646691322327\n",
      "cnt: 0 - valLoss: 0.6662622690200806 - trainLoss: 0.6645645499229431\n",
      "cnt: 0 - valLoss: 0.666262149810791 - trainLoss: 0.6645644307136536\n",
      "cnt: 0 - valLoss: 0.6662620902061462 - trainLoss: 0.6645643711090088\n",
      "cnt: 0 - valLoss: 0.6662619113922119 - trainLoss: 0.6645641922950745\n",
      "cnt: 0 - valLoss: 0.6662617921829224 - trainLoss: 0.6645641326904297\n",
      "cnt: 0 - valLoss: 0.6662616729736328 - trainLoss: 0.6645640730857849\n",
      "cnt: 0 - valLoss: 0.6662615537643433 - trainLoss: 0.6645638942718506\n",
      "cnt: 0 - valLoss: 0.6662613749504089 - trainLoss: 0.6645638346672058\n",
      "cnt: 0 - valLoss: 0.6662613749504089 - trainLoss: 0.6645637154579163\n",
      "cnt: 0 - valLoss: 0.6662611961364746 - trainLoss: 0.6645635962486267\n",
      "cnt: 0 - valLoss: 0.6662610769271851 - trainLoss: 0.6645634770393372\n",
      "cnt: 0 - valLoss: 0.6662609577178955 - trainLoss: 0.6645634174346924\n",
      "cnt: 0 - valLoss: 0.666260838508606 - trainLoss: 0.6645632982254028\n",
      "cnt: 0 - valLoss: 0.6662607192993164 - trainLoss: 0.6645631790161133\n",
      "cnt: 0 - valLoss: 0.6662606000900269 - trainLoss: 0.6645631194114685\n",
      "cnt: 0 - valLoss: 0.6662604808807373 - trainLoss: 0.664563000202179\n",
      "cnt: 0 - valLoss: 0.6662604212760925 - trainLoss: 0.6645628809928894\n",
      "cnt: 0 - valLoss: 0.6662602424621582 - trainLoss: 0.6645627617835999\n",
      "cnt: 0 - valLoss: 0.6662601232528687 - trainLoss: 0.6645627021789551\n",
      "cnt: 0 - valLoss: 0.6662600040435791 - trainLoss: 0.6645625233650208\n",
      "cnt: 0 - valLoss: 0.6662598848342896 - trainLoss: 0.6645625233650208\n",
      "cnt: 0 - valLoss: 0.666259765625 - trainLoss: 0.6645623445510864\n",
      "cnt: 0 - valLoss: 0.6662597060203552 - trainLoss: 0.6645622253417969\n",
      "cnt: 0 - valLoss: 0.6662595272064209 - trainLoss: 0.6645621061325073\n",
      "cnt: 0 - valLoss: 0.6662594676017761 - trainLoss: 0.6645620465278625\n",
      "cnt: 0 - valLoss: 0.666259229183197 - trainLoss: 0.664561927318573\n",
      "cnt: 0 - valLoss: 0.666259229183197 - trainLoss: 0.6645618081092834\n",
      "cnt: 0 - valLoss: 0.6662590503692627 - trainLoss: 0.6645616888999939\n",
      "cnt: 0 - valLoss: 0.6662589311599731 - trainLoss: 0.6645616292953491\n",
      "cnt: 0 - valLoss: 0.6662588119506836 - trainLoss: 0.6645615100860596\n",
      "cnt: 0 - valLoss: 0.666258692741394 - trainLoss: 0.66456139087677\n",
      "cnt: 0 - valLoss: 0.6662585735321045 - trainLoss: 0.6645613312721252\n",
      "cnt: 0 - valLoss: 0.6662585139274597 - trainLoss: 0.6645612120628357\n",
      "cnt: 0 - valLoss: 0.6662583351135254 - trainLoss: 0.6645610332489014\n",
      "cnt: 0 - valLoss: 0.6662582159042358 - trainLoss: 0.6645609736442566\n",
      "cnt: 0 - valLoss: 0.6662580966949463 - trainLoss: 0.664560854434967\n",
      "cnt: 0 - valLoss: 0.6662579774856567 - trainLoss: 0.6645607948303223\n",
      "cnt: 0 - valLoss: 0.6662578582763672 - trainLoss: 0.6645607352256775\n",
      "cnt: 0 - valLoss: 0.6662577390670776 - trainLoss: 0.6645605564117432\n",
      "cnt: 0 - valLoss: 0.6662576198577881 - trainLoss: 0.6645604968070984\n",
      "cnt: 0 - valLoss: 0.6662575602531433 - trainLoss: 0.6645603775978088\n",
      "cnt: 0 - valLoss: 0.666257381439209 - trainLoss: 0.6645603179931641\n",
      "cnt: 0 - valLoss: 0.6662572622299194 - trainLoss: 0.6645601987838745\n",
      "cnt: 0 - valLoss: 0.6662572026252747 - trainLoss: 0.664560079574585\n",
      "cnt: 0 - valLoss: 0.6662570238113403 - trainLoss: 0.6645599603652954\n",
      "cnt: 0 - valLoss: 0.6662569642066956 - trainLoss: 0.6645598411560059\n",
      "cnt: 0 - valLoss: 0.6662567853927612 - trainLoss: 0.6645597815513611\n",
      "cnt: 0 - valLoss: 0.6662566661834717 - trainLoss: 0.6645596623420715\n",
      "cnt: 0 - valLoss: 0.6662565469741821 - trainLoss: 0.664559543132782\n",
      "cnt: 0 - valLoss: 0.6662564873695374 - trainLoss: 0.6645594239234924\n",
      "cnt: 0 - valLoss: 0.666256308555603 - trainLoss: 0.6645593047142029\n",
      "cnt: 0 - valLoss: 0.6662562489509583 - trainLoss: 0.6645592451095581\n",
      "cnt: 0 - valLoss: 0.6662560701370239 - trainLoss: 0.6645591259002686\n",
      "cnt: 0 - valLoss: 0.6662560105323792 - trainLoss: 0.664559006690979\n",
      "cnt: 0 - valLoss: 0.6662558317184448 - trainLoss: 0.6645589470863342\n",
      "cnt: 0 - valLoss: 0.6662557125091553 - trainLoss: 0.6645588278770447\n",
      "cnt: 0 - valLoss: 0.6662555932998657 - trainLoss: 0.6645587682723999\n",
      "cnt: 0 - valLoss: 0.666255533695221 - trainLoss: 0.6645585894584656\n",
      "cnt: 0 - valLoss: 0.6662554144859314 - trainLoss: 0.6645585298538208\n",
      "cnt: 0 - valLoss: 0.6662552356719971 - trainLoss: 0.6645584106445312\n",
      "cnt: 0 - valLoss: 0.6662551164627075 - trainLoss: 0.6645582914352417\n",
      "cnt: 0 - valLoss: 0.666254997253418 - trainLoss: 0.6645581722259521\n",
      "cnt: 0 - valLoss: 0.6662549376487732 - trainLoss: 0.6645581722259521\n",
      "cnt: 0 - valLoss: 0.6662548184394836 - trainLoss: 0.6645579934120178\n",
      "cnt: 0 - valLoss: 0.6662546396255493 - trainLoss: 0.6645578742027283\n",
      "cnt: 0 - valLoss: 0.6662545204162598 - trainLoss: 0.6645578145980835\n",
      "cnt: 0 - valLoss: 0.6662544012069702 - trainLoss: 0.664557695388794\n",
      "cnt: 0 - valLoss: 0.6662542819976807 - trainLoss: 0.6645576357841492\n",
      "cnt: 0 - valLoss: 0.6662542223930359 - trainLoss: 0.6645575165748596\n",
      "cnt: 0 - valLoss: 0.6662541031837463 - trainLoss: 0.6645573973655701\n",
      "cnt: 0 - valLoss: 0.666253924369812 - trainLoss: 0.6645572781562805\n",
      "cnt: 0 - valLoss: 0.6662538051605225 - trainLoss: 0.664557158946991\n",
      "cnt: 0 - valLoss: 0.6662537455558777 - trainLoss: 0.6645570993423462\n",
      "cnt: 0 - valLoss: 0.6662535667419434 - trainLoss: 0.6645569801330566\n",
      "cnt: 0 - valLoss: 0.6662534475326538 - trainLoss: 0.6645568609237671\n",
      "cnt: 0 - valLoss: 0.6662532687187195 - trainLoss: 0.6645567417144775\n",
      "cnt: 0 - valLoss: 0.6662532687187195 - trainLoss: 0.664556622505188\n",
      "cnt: 0 - valLoss: 0.6662531495094299 - trainLoss: 0.6645565032958984\n",
      "cnt: 0 - valLoss: 0.6662529706954956 - trainLoss: 0.6645564436912537\n",
      "cnt: 0 - valLoss: 0.666252851486206 - trainLoss: 0.6645563244819641\n",
      "cnt: 0 - valLoss: 0.6662527322769165 - trainLoss: 0.6645562052726746\n",
      "cnt: 0 - valLoss: 0.666252613067627 - trainLoss: 0.664556086063385\n",
      "cnt: 0 - valLoss: 0.6662524938583374 - trainLoss: 0.6645560264587402\n",
      "cnt: 0 - valLoss: 0.6662523746490479 - trainLoss: 0.6645558476448059\n",
      "cnt: 0 - valLoss: 0.6662522554397583 - trainLoss: 0.6645558476448059\n",
      "cnt: 0 - valLoss: 0.666252076625824 - trainLoss: 0.6645556688308716\n",
      "cnt: 0 - valLoss: 0.6662520170211792 - trainLoss: 0.664555549621582\n",
      "cnt: 0 - valLoss: 0.6662518978118896 - trainLoss: 0.6645554900169373\n",
      "cnt: 0 - valLoss: 0.6662517786026001 - trainLoss: 0.6645554304122925\n",
      "cnt: 0 - valLoss: 0.6662516593933105 - trainLoss: 0.6645552515983582\n",
      "cnt: 0 - valLoss: 0.6662515997886658 - trainLoss: 0.6645551323890686\n",
      "cnt: 0 - valLoss: 0.6662514209747314 - trainLoss: 0.6645551323890686\n",
      "cnt: 0 - valLoss: 0.6662512421607971 - trainLoss: 0.6645549535751343\n",
      "cnt: 0 - valLoss: 0.6662511825561523 - trainLoss: 0.6645548939704895\n",
      "cnt: 0 - valLoss: 0.6662511229515076 - trainLoss: 0.6645547747612\n",
      "cnt: 0 - valLoss: 0.666251003742218 - trainLoss: 0.6645546555519104\n",
      "cnt: 0 - valLoss: 0.6662508249282837 - trainLoss: 0.6645545363426208\n",
      "cnt: 0 - valLoss: 0.6662507057189941 - trainLoss: 0.6645544767379761\n",
      "cnt: 0 - valLoss: 0.6662505865097046 - trainLoss: 0.6645543575286865\n",
      "cnt: 0 - valLoss: 0.6662505269050598 - trainLoss: 0.664554238319397\n",
      "cnt: 0 - valLoss: 0.6662503480911255 - trainLoss: 0.6645541787147522\n",
      "cnt: 0 - valLoss: 0.6662502288818359 - trainLoss: 0.6645540595054626\n",
      "cnt: 0 - valLoss: 0.6662501096725464 - trainLoss: 0.6645539402961731\n",
      "cnt: 0 - valLoss: 0.6662499904632568 - trainLoss: 0.6645537614822388\n",
      "cnt: 0 - valLoss: 0.6662498712539673 - trainLoss: 0.6645537614822388\n",
      "cnt: 0 - valLoss: 0.6662497520446777 - trainLoss: 0.6645535826683044\n",
      "cnt: 0 - valLoss: 0.6662496328353882 - trainLoss: 0.6645535230636597\n",
      "cnt: 0 - valLoss: 0.6662495136260986 - trainLoss: 0.6645534038543701\n",
      "cnt: 0 - valLoss: 0.6662493348121643 - trainLoss: 0.6645533442497253\n",
      "cnt: 0 - valLoss: 0.6662493348121643 - trainLoss: 0.664553165435791\n",
      "cnt: 0 - valLoss: 0.66624915599823 - trainLoss: 0.6645531058311462\n",
      "cnt: 0 - valLoss: 0.6662489771842957 - trainLoss: 0.6645529866218567\n",
      "cnt: 0 - valLoss: 0.6662489771842957 - trainLoss: 0.6645528078079224\n",
      "cnt: 0 - valLoss: 0.6662487983703613 - trainLoss: 0.6645527482032776\n",
      "cnt: 0 - valLoss: 0.6662486791610718 - trainLoss: 0.6645526885986328\n",
      "cnt: 0 - valLoss: 0.6662485599517822 - trainLoss: 0.6645525693893433\n",
      "cnt: 0 - valLoss: 0.6662484407424927 - trainLoss: 0.6645524501800537\n",
      "cnt: 0 - valLoss: 0.6662483215332031 - trainLoss: 0.6645523905754089\n",
      "cnt: 0 - valLoss: 0.6662482023239136 - trainLoss: 0.6645522713661194\n",
      "cnt: 0 - valLoss: 0.666248083114624 - trainLoss: 0.6645521521568298\n",
      "cnt: 0 - valLoss: 0.6662479639053345 - trainLoss: 0.6645520925521851\n",
      "cnt: 0 - valLoss: 0.6662478446960449 - trainLoss: 0.6645519733428955\n",
      "cnt: 0 - valLoss: 0.6662477850914001 - trainLoss: 0.664551854133606\n",
      "cnt: 0 - valLoss: 0.666247546672821 - trainLoss: 0.6645517945289612\n",
      "cnt: 0 - valLoss: 0.6662474870681763 - trainLoss: 0.6645516157150269\n",
      "cnt: 0 - valLoss: 0.6662473082542419 - trainLoss: 0.6645515561103821\n",
      "cnt: 0 - valLoss: 0.6662471890449524 - trainLoss: 0.6645514369010925\n",
      "cnt: 0 - valLoss: 0.6662471294403076 - trainLoss: 0.664551317691803\n",
      "cnt: 0 - valLoss: 0.6662470102310181 - trainLoss: 0.6645511984825134\n",
      "cnt: 0 - valLoss: 0.6662468314170837 - trainLoss: 0.6645511388778687\n",
      "cnt: 0 - valLoss: 0.6662467122077942 - trainLoss: 0.6645510196685791\n",
      "cnt: 0 - valLoss: 0.6662465929985046 - trainLoss: 0.6645509004592896\n",
      "cnt: 0 - valLoss: 0.6662465333938599 - trainLoss: 0.66455078125\n",
      "cnt: 0 - valLoss: 0.6662464141845703 - trainLoss: 0.6645507216453552\n",
      "cnt: 0 - valLoss: 0.666246235370636 - trainLoss: 0.6645506024360657\n",
      "cnt: 0 - valLoss: 0.6662461757659912 - trainLoss: 0.6645505428314209\n",
      "cnt: 0 - valLoss: 0.6662460565567017 - trainLoss: 0.6645504236221313\n",
      "cnt: 0 - valLoss: 0.6662458777427673 - trainLoss: 0.664550244808197\n",
      "cnt: 0 - valLoss: 0.6662457585334778 - trainLoss: 0.6645501852035522\n",
      "cnt: 0 - valLoss: 0.6662456393241882 - trainLoss: 0.6645501255989075\n",
      "cnt: 0 - valLoss: 0.6662455201148987 - trainLoss: 0.6645499467849731\n",
      "cnt: 0 - valLoss: 0.6662454009056091 - trainLoss: 0.6645498275756836\n",
      "cnt: 0 - valLoss: 0.6662452816963196 - trainLoss: 0.6645497679710388\n",
      "cnt: 0 - valLoss: 0.66624516248703 - trainLoss: 0.6645495891571045\n",
      "cnt: 0 - valLoss: 0.6662450432777405 - trainLoss: 0.6645495295524597\n",
      "cnt: 0 - valLoss: 0.6662449240684509 - trainLoss: 0.6645494699478149\n",
      "cnt: 0 - valLoss: 0.6662448048591614 - trainLoss: 0.6645493507385254\n",
      "cnt: 0 - valLoss: 0.666244626045227 - trainLoss: 0.6645492315292358\n",
      "cnt: 0 - valLoss: 0.6662445664405823 - trainLoss: 0.6645491123199463\n",
      "cnt: 0 - valLoss: 0.6662444472312927 - trainLoss: 0.6645489931106567\n",
      "cnt: 0 - valLoss: 0.6662442684173584 - trainLoss: 0.6645488739013672\n",
      "cnt: 0 - valLoss: 0.6662442684173584 - trainLoss: 0.6645488142967224\n",
      "cnt: 0 - valLoss: 0.6662440896034241 - trainLoss: 0.6645487546920776\n",
      "cnt: 0 - valLoss: 0.6662439703941345 - trainLoss: 0.6645485758781433\n",
      "cnt: 0 - valLoss: 0.666243851184845 - trainLoss: 0.6645484566688538\n",
      "cnt: 0 - valLoss: 0.6662437319755554 - trainLoss: 0.664548397064209\n",
      "cnt: 0 - valLoss: 0.6662436723709106 - trainLoss: 0.6645482778549194\n",
      "cnt: 0 - valLoss: 0.6662434935569763 - trainLoss: 0.6645482182502747\n",
      "cnt: 0 - valLoss: 0.666243314743042 - trainLoss: 0.6645480990409851\n",
      "cnt: 0 - valLoss: 0.6662431955337524 - trainLoss: 0.6645479798316956\n",
      "cnt: 0 - valLoss: 0.6662430763244629 - trainLoss: 0.664547860622406\n",
      "cnt: 0 - valLoss: 0.6662429571151733 - trainLoss: 0.6645477414131165\n",
      "cnt: 0 - valLoss: 0.6662428975105286 - trainLoss: 0.6645476818084717\n",
      "cnt: 0 - valLoss: 0.666242778301239 - trainLoss: 0.6645475029945374\n",
      "cnt: 0 - valLoss: 0.6662425994873047 - trainLoss: 0.6645474433898926\n",
      "cnt: 0 - valLoss: 0.6662425398826599 - trainLoss: 0.6645473837852478\n",
      "cnt: 0 - valLoss: 0.6662423610687256 - trainLoss: 0.6645472049713135\n",
      "cnt: 0 - valLoss: 0.6662423014640808 - trainLoss: 0.6645472049713135\n",
      "cnt: 0 - valLoss: 0.6662421226501465 - trainLoss: 0.6645470261573792\n",
      "cnt: 0 - valLoss: 0.6662420034408569 - trainLoss: 0.6645469069480896\n",
      "cnt: 0 - valLoss: 0.6662418246269226 - trainLoss: 0.6645468473434448\n",
      "cnt: 0 - valLoss: 0.6662417650222778 - trainLoss: 0.6645467281341553\n",
      "cnt: 0 - valLoss: 0.6662417054176331 - trainLoss: 0.6645466089248657\n",
      "cnt: 0 - valLoss: 0.6662415266036987 - trainLoss: 0.6645464897155762\n",
      "cnt: 0 - valLoss: 0.6662414073944092 - trainLoss: 0.6645464301109314\n",
      "cnt: 0 - valLoss: 0.6662412285804749 - trainLoss: 0.6645462512969971\n",
      "cnt: 0 - valLoss: 0.6662411689758301 - trainLoss: 0.6645461320877075\n",
      "cnt: 0 - valLoss: 0.6662410497665405 - trainLoss: 0.6645461320877075\n",
      "cnt: 0 - valLoss: 0.666240930557251 - trainLoss: 0.664546012878418\n",
      "cnt: 0 - valLoss: 0.6662408113479614 - trainLoss: 0.6645458936691284\n",
      "cnt: 0 - valLoss: 0.6662406921386719 - trainLoss: 0.6645457744598389\n",
      "cnt: 0 - valLoss: 0.6662405729293823 - trainLoss: 0.6645456552505493\n",
      "cnt: 0 - valLoss: 0.6662404537200928 - trainLoss: 0.6645455360412598\n",
      "cnt: 0 - valLoss: 0.6662403345108032 - trainLoss: 0.6645454168319702\n",
      "cnt: 0 - valLoss: 0.6662402153015137 - trainLoss: 0.6645454168319702\n",
      "cnt: 0 - valLoss: 0.6662400960922241 - trainLoss: 0.6645452380180359\n",
      "cnt: 0 - valLoss: 0.6662399768829346 - trainLoss: 0.6645451188087463\n",
      "cnt: 0 - valLoss: 0.666239857673645 - trainLoss: 0.6645450592041016\n",
      "cnt: 0 - valLoss: 0.6662396788597107 - trainLoss: 0.664544939994812\n",
      "cnt: 0 - valLoss: 0.6662396192550659 - trainLoss: 0.6645447611808777\n",
      "cnt: 0 - valLoss: 0.6662394404411316 - trainLoss: 0.6645447015762329\n",
      "cnt: 0 - valLoss: 0.666239321231842 - trainLoss: 0.6645446419715881\n",
      "cnt: 0 - valLoss: 0.6662392616271973 - trainLoss: 0.6645445227622986\n",
      "cnt: 0 - valLoss: 0.6662390828132629 - trainLoss: 0.664544403553009\n",
      "cnt: 0 - valLoss: 0.6662389636039734 - trainLoss: 0.6645442843437195\n",
      "cnt: 0 - valLoss: 0.6662388443946838 - trainLoss: 0.6645441651344299\n",
      "cnt: 0 - valLoss: 0.6662387251853943 - trainLoss: 0.6645441055297852\n",
      "cnt: 0 - valLoss: 0.6662386059761047 - trainLoss: 0.6645440459251404\n",
      "cnt: 0 - valLoss: 0.6662384867668152 - trainLoss: 0.664543867111206\n",
      "cnt: 0 - valLoss: 0.6662383079528809 - trainLoss: 0.6645437479019165\n",
      "cnt: 0 - valLoss: 0.6662382483482361 - trainLoss: 0.6645436882972717\n",
      "cnt: 0 - valLoss: 0.6662381291389465 - trainLoss: 0.6645435690879822\n",
      "cnt: 0 - valLoss: 0.666238009929657 - trainLoss: 0.6645434498786926\n",
      "cnt: 0 - valLoss: 0.6662378907203674 - trainLoss: 0.6645433902740479\n",
      "cnt: 0 - valLoss: 0.6662377715110779 - trainLoss: 0.6645432710647583\n",
      "cnt: 0 - valLoss: 0.6662376523017883 - trainLoss: 0.6645431518554688\n",
      "cnt: 0 - valLoss: 0.6662375926971436 - trainLoss: 0.664543092250824\n",
      "cnt: 0 - valLoss: 0.6662374138832092 - trainLoss: 0.6645429730415344\n",
      "cnt: 0 - valLoss: 0.6662372350692749 - trainLoss: 0.6645427942276001\n",
      "cnt: 0 - valLoss: 0.6662371158599854 - trainLoss: 0.6645427942276001\n",
      "cnt: 0 - valLoss: 0.6662370562553406 - trainLoss: 0.6645426154136658\n",
      "cnt: 0 - valLoss: 0.6662368774414062 - trainLoss: 0.6645424962043762\n",
      "cnt: 0 - valLoss: 0.6662368178367615 - trainLoss: 0.6645423769950867\n",
      "cnt: 0 - valLoss: 0.6662366390228271 - trainLoss: 0.6645423769950867\n",
      "cnt: 0 - valLoss: 0.6662365198135376 - trainLoss: 0.6645421385765076\n",
      "cnt: 0 - valLoss: 0.6662364602088928 - trainLoss: 0.6645421385765076\n",
      "cnt: 0 - valLoss: 0.6662362813949585 - trainLoss: 0.6645420789718628\n",
      "cnt: 0 - valLoss: 0.666236162185669 - trainLoss: 0.6645419001579285\n",
      "cnt: 0 - valLoss: 0.6662360429763794 - trainLoss: 0.6645418405532837\n",
      "cnt: 0 - valLoss: 0.6662359237670898 - trainLoss: 0.6645417213439941\n",
      "cnt: 0 - valLoss: 0.6662358045578003 - trainLoss: 0.6645416021347046\n",
      "cnt: 0 - valLoss: 0.6662356853485107 - trainLoss: 0.6645414233207703\n",
      "cnt: 0 - valLoss: 0.6662355661392212 - trainLoss: 0.6645413637161255\n",
      "cnt: 0 - valLoss: 0.6662354469299316 - trainLoss: 0.6645412445068359\n",
      "cnt: 0 - valLoss: 0.6662353277206421 - trainLoss: 0.6645411252975464\n",
      "cnt: 0 - valLoss: 0.6662351489067078 - trainLoss: 0.6645410060882568\n",
      "cnt: 0 - valLoss: 0.6662350296974182 - trainLoss: 0.6645410060882568\n",
      "cnt: 0 - valLoss: 0.6662349104881287 - trainLoss: 0.6645408272743225\n",
      "cnt: 0 - valLoss: 0.6662347912788391 - trainLoss: 0.6645407676696777\n",
      "cnt: 0 - valLoss: 0.6662347316741943 - trainLoss: 0.6645406484603882\n",
      "cnt: 0 - valLoss: 0.66623455286026 - trainLoss: 0.6645405292510986\n",
      "cnt: 0 - valLoss: 0.6662344932556152 - trainLoss: 0.6645404100418091\n",
      "cnt: 0 - valLoss: 0.6662343740463257 - trainLoss: 0.6645403504371643\n",
      "cnt: 0 - valLoss: 0.6662341952323914 - trainLoss: 0.6645402312278748\n",
      "cnt: 0 - valLoss: 0.6662340760231018 - trainLoss: 0.6645401120185852\n",
      "cnt: 0 - valLoss: 0.6662339568138123 - trainLoss: 0.6645399928092957\n",
      "cnt: 0 - valLoss: 0.6662338972091675 - trainLoss: 0.6645398736000061\n",
      "cnt: 0 - valLoss: 0.6662337183952332 - trainLoss: 0.6645397543907166\n",
      "cnt: 0 - valLoss: 0.6662335991859436 - trainLoss: 0.6645396947860718\n",
      "cnt: 0 - valLoss: 0.6662334203720093 - trainLoss: 0.6645395755767822\n",
      "cnt: 0 - valLoss: 0.6662333607673645 - trainLoss: 0.6645394563674927\n",
      "cnt: 0 - valLoss: 0.6662333011627197 - trainLoss: 0.6645393371582031\n",
      "cnt: 0 - valLoss: 0.6662331223487854 - trainLoss: 0.6645392179489136\n",
      "cnt: 0 - valLoss: 0.6662330031394958 - trainLoss: 0.6645391583442688\n",
      "cnt: 0 - valLoss: 0.6662328243255615 - trainLoss: 0.664539098739624\n",
      "cnt: 0 - valLoss: 0.6662327647209167 - trainLoss: 0.6645389795303345\n",
      "cnt: 0 - valLoss: 0.6662325859069824 - trainLoss: 0.6645388603210449\n",
      "cnt: 0 - valLoss: 0.6662325263023376 - trainLoss: 0.6645386815071106\n",
      "cnt: 0 - valLoss: 0.6662323474884033 - trainLoss: 0.6645386219024658\n",
      "cnt: 0 - valLoss: 0.6662322282791138 - trainLoss: 0.664538562297821\n",
      "cnt: 0 - valLoss: 0.6662321090698242 - trainLoss: 0.6645384430885315\n",
      "cnt: 0 - valLoss: 0.6662319898605347 - trainLoss: 0.6645382642745972\n",
      "cnt: 0 - valLoss: 0.6662318706512451 - trainLoss: 0.6645382642745972\n",
      "cnt: 0 - valLoss: 0.6662317514419556 - trainLoss: 0.6645380854606628\n",
      "cnt: 0 - valLoss: 0.666231632232666 - trainLoss: 0.6645380258560181\n",
      "cnt: 0 - valLoss: 0.6662315726280212 - trainLoss: 0.6645379066467285\n",
      "cnt: 0 - valLoss: 0.6662314534187317 - trainLoss: 0.664537787437439\n",
      "cnt: 0 - valLoss: 0.6662313342094421 - trainLoss: 0.6645376682281494\n",
      "cnt: 0 - valLoss: 0.6662311553955078 - trainLoss: 0.6645376086235046\n",
      "cnt: 0 - valLoss: 0.6662310361862183 - trainLoss: 0.6645374894142151\n",
      "cnt: 0 - valLoss: 0.6662308573722839 - trainLoss: 0.6645373702049255\n",
      "cnt: 0 - valLoss: 0.6662307977676392 - trainLoss: 0.6645373106002808\n",
      "cnt: 0 - valLoss: 0.6662306189537048 - trainLoss: 0.6645371913909912\n",
      "cnt: 0 - valLoss: 0.6662304997444153 - trainLoss: 0.6645370125770569\n",
      "cnt: 0 - valLoss: 0.6662304401397705 - trainLoss: 0.6645368933677673\n",
      "cnt: 0 - valLoss: 0.6662302613258362 - trainLoss: 0.6645368337631226\n",
      "cnt: 0 - valLoss: 0.6662301421165466 - trainLoss: 0.664536714553833\n",
      "cnt: 0 - valLoss: 0.6662300229072571 - trainLoss: 0.6645366549491882\n",
      "cnt: 0 - valLoss: 0.6662299633026123 - trainLoss: 0.6645365357398987\n",
      "cnt: 0 - valLoss: 0.666229784488678 - trainLoss: 0.6645364165306091\n",
      "cnt: 0 - valLoss: 0.6662296652793884 - trainLoss: 0.6645362973213196\n",
      "cnt: 0 - valLoss: 0.6662294864654541 - trainLoss: 0.6645362377166748\n",
      "cnt: 0 - valLoss: 0.6662294268608093 - trainLoss: 0.6645361185073853\n",
      "cnt: 0 - valLoss: 0.666229248046875 - trainLoss: 0.6645359992980957\n",
      "cnt: 0 - valLoss: 0.6662291884422302 - trainLoss: 0.6645358800888062\n",
      "cnt: 0 - valLoss: 0.6662290096282959 - trainLoss: 0.6645358204841614\n",
      "cnt: 0 - valLoss: 0.6662289500236511 - trainLoss: 0.6645357012748718\n",
      "cnt: 0 - valLoss: 0.6662288308143616 - trainLoss: 0.6645355224609375\n",
      "cnt: 0 - valLoss: 0.666228711605072 - trainLoss: 0.6645354628562927\n",
      "cnt: 0 - valLoss: 0.6662285327911377 - trainLoss: 0.6645353436470032\n",
      "cnt: 0 - valLoss: 0.6662284135818481 - trainLoss: 0.6645352840423584\n",
      "cnt: 0 - valLoss: 0.6662283539772034 - trainLoss: 0.6645351648330688\n",
      "cnt: 0 - valLoss: 0.6662282347679138 - trainLoss: 0.6645350456237793\n",
      "cnt: 0 - valLoss: 0.6662280559539795 - trainLoss: 0.6645349264144897\n",
      "cnt: 0 - valLoss: 0.6662279367446899 - trainLoss: 0.6645348072052002\n",
      "cnt: 0 - valLoss: 0.6662278175354004 - trainLoss: 0.6645347476005554\n",
      "cnt: 0 - valLoss: 0.6662276983261108 - trainLoss: 0.6645346283912659\n",
      "cnt: 0 - valLoss: 0.6662275791168213 - trainLoss: 0.6645345091819763\n",
      "cnt: 0 - valLoss: 0.6662274599075317 - trainLoss: 0.6645344495773315\n",
      "cnt: 0 - valLoss: 0.6662273406982422 - trainLoss: 0.664534330368042\n",
      "cnt: 0 - valLoss: 0.6662272214889526 - trainLoss: 0.6645342111587524\n",
      "cnt: 0 - valLoss: 0.6662271022796631 - trainLoss: 0.6645340919494629\n",
      "cnt: 0 - valLoss: 0.6662269234657288 - trainLoss: 0.6645340323448181\n",
      "cnt: 0 - valLoss: 0.6662268042564392 - trainLoss: 0.6645338535308838\n",
      "cnt: 0 - valLoss: 0.6662266850471497 - trainLoss: 0.6645337343215942\n",
      "cnt: 0 - valLoss: 0.6662265658378601 - trainLoss: 0.6645336747169495\n",
      "cnt: 0 - valLoss: 0.6662264466285706 - trainLoss: 0.6645335555076599\n",
      "cnt: 0 - valLoss: 0.666226327419281 - trainLoss: 0.6645334959030151\n",
      "cnt: 0 - valLoss: 0.6662262082099915 - trainLoss: 0.6645333766937256\n",
      "cnt: 0 - valLoss: 0.6662260890007019 - trainLoss: 0.664533257484436\n",
      "cnt: 0 - valLoss: 0.6662259697914124 - trainLoss: 0.6645331382751465\n",
      "cnt: 0 - valLoss: 0.6662258505821228 - trainLoss: 0.6645330190658569\n",
      "cnt: 0 - valLoss: 0.6662257313728333 - trainLoss: 0.6645329594612122\n",
      "cnt: 0 - valLoss: 0.6662256121635437 - trainLoss: 0.6645327806472778\n",
      "cnt: 0 - valLoss: 0.6662254929542542 - trainLoss: 0.6645327210426331\n",
      "cnt: 0 - valLoss: 0.6662253737449646 - trainLoss: 0.6645326614379883\n",
      "cnt: 0 - valLoss: 0.6662251949310303 - trainLoss: 0.6645325422286987\n",
      "cnt: 0 - valLoss: 0.6662250757217407 - trainLoss: 0.6645324230194092\n",
      "cnt: 0 - valLoss: 0.666225016117096 - trainLoss: 0.6645323038101196\n",
      "cnt: 0 - valLoss: 0.6662248969078064 - trainLoss: 0.6645321846008301\n",
      "cnt: 0 - valLoss: 0.6662247180938721 - trainLoss: 0.6645320653915405\n",
      "cnt: 0 - valLoss: 0.6662245988845825 - trainLoss: 0.6645320057868958\n",
      "cnt: 0 - valLoss: 0.6662244200706482 - trainLoss: 0.6645318865776062\n",
      "cnt: 0 - valLoss: 0.6662243604660034 - trainLoss: 0.6645317673683167\n",
      "cnt: 0 - valLoss: 0.6662242412567139 - trainLoss: 0.6645316481590271\n",
      "cnt: 0 - valLoss: 0.6662241220474243 - trainLoss: 0.6645315289497375\n",
      "cnt: 0 - valLoss: 0.6662240028381348 - trainLoss: 0.664531409740448\n",
      "cnt: 0 - valLoss: 0.6662238240242004 - trainLoss: 0.6645312905311584\n",
      "cnt: 0 - valLoss: 0.6662237048149109 - trainLoss: 0.6645312309265137\n",
      "cnt: 0 - valLoss: 0.6662235856056213 - trainLoss: 0.6645311117172241\n",
      "cnt: 0 - valLoss: 0.6662235260009766 - trainLoss: 0.6645309925079346\n",
      "cnt: 0 - valLoss: 0.6662233471870422 - trainLoss: 0.6645309329032898\n",
      "cnt: 0 - valLoss: 0.6662232279777527 - trainLoss: 0.6645308136940002\n",
      "cnt: 0 - valLoss: 0.6662231087684631 - trainLoss: 0.6645306348800659\n",
      "cnt: 0 - valLoss: 0.6662229895591736 - trainLoss: 0.6645305752754211\n",
      "cnt: 0 - valLoss: 0.666222870349884 - trainLoss: 0.6645305156707764\n",
      "cnt: 0 - valLoss: 0.6662227511405945 - trainLoss: 0.664530336856842\n",
      "cnt: 0 - valLoss: 0.6662226319313049 - trainLoss: 0.6645302772521973\n",
      "cnt: 0 - valLoss: 0.6662225723266602 - trainLoss: 0.6645301580429077\n",
      "cnt: 0 - valLoss: 0.6662223935127258 - trainLoss: 0.6645300388336182\n",
      "cnt: 0 - valLoss: 0.6662222743034363 - trainLoss: 0.6645299792289734\n",
      "cnt: 0 - valLoss: 0.6662221550941467 - trainLoss: 0.6645299196243286\n",
      "cnt: 0 - valLoss: 0.6662219762802124 - trainLoss: 0.6645297408103943\n",
      "cnt: 0 - valLoss: 0.6662218570709229 - trainLoss: 0.6645296216011047\n",
      "cnt: 0 - valLoss: 0.6662217378616333 - trainLoss: 0.6645295023918152\n",
      "cnt: 0 - valLoss: 0.6662216782569885 - trainLoss: 0.6645293831825256\n",
      "cnt: 0 - valLoss: 0.6662214398384094 - trainLoss: 0.6645293235778809\n",
      "cnt: 0 - valLoss: 0.6662213802337646 - trainLoss: 0.6645292043685913\n",
      "cnt: 0 - valLoss: 0.6662212610244751 - trainLoss: 0.6645291447639465\n",
      "cnt: 0 - valLoss: 0.6662210822105408 - trainLoss: 0.6645289659500122\n",
      "cnt: 0 - valLoss: 0.666221022605896 - trainLoss: 0.6645288467407227\n",
      "cnt: 0 - valLoss: 0.6662209033966064 - trainLoss: 0.6645287871360779\n",
      "cnt: 0 - valLoss: 0.6662207841873169 - trainLoss: 0.6645286679267883\n",
      "cnt: 0 - valLoss: 0.6662206053733826 - trainLoss: 0.6645285487174988\n",
      "cnt: 0 - valLoss: 0.666220486164093 - trainLoss: 0.6645285487174988\n",
      "cnt: 0 - valLoss: 0.6662203669548035 - trainLoss: 0.6645283102989197\n",
      "cnt: 0 - valLoss: 0.6662202477455139 - trainLoss: 0.6645282506942749\n",
      "cnt: 0 - valLoss: 0.6662201285362244 - trainLoss: 0.6645281910896301\n",
      "cnt: 0 - valLoss: 0.6662200093269348 - trainLoss: 0.6645280122756958\n",
      "cnt: 0 - valLoss: 0.6662198901176453 - trainLoss: 0.664527952671051\n",
      "cnt: 0 - valLoss: 0.6662197709083557 - trainLoss: 0.6645278334617615\n",
      "cnt: 0 - valLoss: 0.6662195920944214 - trainLoss: 0.6645277738571167\n",
      "cnt: 0 - valLoss: 0.6662194728851318 - trainLoss: 0.6645275950431824\n",
      "cnt: 0 - valLoss: 0.6662194132804871 - trainLoss: 0.6645275354385376\n",
      "cnt: 0 - valLoss: 0.6662192344665527 - trainLoss: 0.6645273566246033\n",
      "cnt: 0 - valLoss: 0.6662191152572632 - trainLoss: 0.6645272970199585\n",
      "cnt: 0 - valLoss: 0.6662189960479736 - trainLoss: 0.664527177810669\n",
      "cnt: 0 - valLoss: 0.6662188768386841 - trainLoss: 0.6645271182060242\n",
      "cnt: 0 - valLoss: 0.6662186980247498 - trainLoss: 0.6645269989967346\n",
      "cnt: 0 - valLoss: 0.666218638420105 - trainLoss: 0.6645268797874451\n",
      "cnt: 0 - valLoss: 0.6662184596061707 - trainLoss: 0.6645267605781555\n",
      "cnt: 0 - valLoss: 0.6662184000015259 - trainLoss: 0.664526641368866\n",
      "cnt: 0 - valLoss: 0.6662182211875916 - trainLoss: 0.6645265817642212\n",
      "cnt: 0 - valLoss: 0.666218101978302 - trainLoss: 0.6645264029502869\n",
      "cnt: 0 - valLoss: 0.6662179827690125 - trainLoss: 0.6645263433456421\n",
      "cnt: 0 - valLoss: 0.6662178635597229 - trainLoss: 0.6645262837409973\n",
      "cnt: 0 - valLoss: 0.6662177443504333 - trainLoss: 0.664526104927063\n",
      "cnt: 0 - valLoss: 0.6662176251411438 - trainLoss: 0.6645259857177734\n",
      "cnt: 0 - valLoss: 0.6662175059318542 - trainLoss: 0.6645258665084839\n",
      "cnt: 0 - valLoss: 0.6662173271179199 - trainLoss: 0.6645258069038391\n",
      "cnt: 0 - valLoss: 0.6662172675132751 - trainLoss: 0.6645256876945496\n",
      "cnt: 0 - valLoss: 0.6662170886993408 - trainLoss: 0.66452556848526\n",
      "cnt: 0 - valLoss: 0.6662169694900513 - trainLoss: 0.6645255088806152\n",
      "cnt: 0 - valLoss: 0.6662168502807617 - trainLoss: 0.6645253300666809\n",
      "cnt: 0 - valLoss: 0.6662167310714722 - trainLoss: 0.6645252704620361\n",
      "cnt: 0 - valLoss: 0.6662166118621826 - trainLoss: 0.6645252108573914\n",
      "cnt: 0 - valLoss: 0.6662164926528931 - trainLoss: 0.664525032043457\n",
      "cnt: 0 - valLoss: 0.6662163138389587 - trainLoss: 0.6645249724388123\n",
      "cnt: 0 - valLoss: 0.6662161946296692 - trainLoss: 0.6645248532295227\n",
      "cnt: 0 - valLoss: 0.6662161350250244 - trainLoss: 0.6645247340202332\n",
      "cnt: 0 - valLoss: 0.6662160158157349 - trainLoss: 0.6645246148109436\n",
      "cnt: 0 - valLoss: 0.6662158966064453 - trainLoss: 0.6645244359970093\n",
      "cnt: 0 - valLoss: 0.666215717792511 - trainLoss: 0.6645244359970093\n",
      "cnt: 0 - valLoss: 0.6662155985832214 - trainLoss: 0.6645243167877197\n",
      "cnt: 0 - valLoss: 0.6662154197692871 - trainLoss: 0.6645241379737854\n",
      "cnt: 0 - valLoss: 0.6662153601646423 - trainLoss: 0.6645240783691406\n",
      "cnt: 0 - valLoss: 0.6662152409553528 - trainLoss: 0.6645239591598511\n",
      "cnt: 0 - valLoss: 0.6662151217460632 - trainLoss: 0.6645238399505615\n",
      "cnt: 0 - valLoss: 0.6662150025367737 - trainLoss: 0.6645237803459167\n",
      "cnt: 0 - valLoss: 0.6662148237228394 - trainLoss: 0.6645236611366272\n",
      "cnt: 0 - valLoss: 0.6662147045135498 - trainLoss: 0.6645235419273376\n",
      "cnt: 0 - valLoss: 0.6662145853042603 - trainLoss: 0.6645234823226929\n",
      "cnt: 0 - valLoss: 0.6662144660949707 - trainLoss: 0.6645233035087585\n",
      "cnt: 0 - valLoss: 0.6662143468856812 - trainLoss: 0.6645232439041138\n",
      "cnt: 0 - valLoss: 0.6662142276763916 - trainLoss: 0.6645231246948242\n",
      "cnt: 0 - valLoss: 0.6662140488624573 - trainLoss: 0.6645230650901794\n",
      "cnt: 0 - valLoss: 0.6662139892578125 - trainLoss: 0.6645229458808899\n",
      "cnt: 0 - valLoss: 0.666213870048523 - trainLoss: 0.6645227670669556\n",
      "cnt: 0 - valLoss: 0.6662137508392334 - trainLoss: 0.6645227074623108\n",
      "cnt: 0 - valLoss: 0.6662135720252991 - trainLoss: 0.6645225882530212\n",
      "cnt: 0 - valLoss: 0.6662134528160095 - trainLoss: 0.6645224690437317\n",
      "cnt: 0 - valLoss: 0.6662132740020752 - trainLoss: 0.6645223498344421\n",
      "cnt: 0 - valLoss: 0.6662131547927856 - trainLoss: 0.6645222902297974\n",
      "cnt: 0 - valLoss: 0.6662130951881409 - trainLoss: 0.6645221710205078\n",
      "cnt: 0 - valLoss: 0.6662129759788513 - trainLoss: 0.6645219922065735\n",
      "cnt: 0 - valLoss: 0.666212797164917 - trainLoss: 0.6645219326019287\n",
      "cnt: 0 - valLoss: 0.6662126779556274 - trainLoss: 0.6645218729972839\n",
      "cnt: 0 - valLoss: 0.6662125587463379 - trainLoss: 0.6645216941833496\n",
      "cnt: 0 - valLoss: 0.6662124991416931 - trainLoss: 0.6645216345787048\n",
      "cnt: 0 - valLoss: 0.6662123203277588 - trainLoss: 0.6645215153694153\n",
      "cnt: 0 - valLoss: 0.6662121415138245 - trainLoss: 0.6645213961601257\n",
      "cnt: 0 - valLoss: 0.6662120819091797 - trainLoss: 0.664521336555481\n",
      "cnt: 0 - valLoss: 0.6662119626998901 - trainLoss: 0.6645212769508362\n",
      "cnt: 0 - valLoss: 0.6662117838859558 - trainLoss: 0.6645210981369019\n",
      "cnt: 0 - valLoss: 0.6662116646766663 - trainLoss: 0.6645209789276123\n",
      "cnt: 0 - valLoss: 0.6662116050720215 - trainLoss: 0.6645208597183228\n",
      "cnt: 0 - valLoss: 0.6662114858627319 - trainLoss: 0.664520800113678\n",
      "cnt: 0 - valLoss: 0.6662113070487976 - trainLoss: 0.6645206212997437\n",
      "cnt: 0 - valLoss: 0.6662111878395081 - trainLoss: 0.6645205616950989\n",
      "cnt: 0 - valLoss: 0.6662110686302185 - trainLoss: 0.6645203828811646\n",
      "cnt: 0 - valLoss: 0.666210949420929 - trainLoss: 0.6645203232765198\n",
      "cnt: 0 - valLoss: 0.6662108302116394 - trainLoss: 0.6645202040672302\n",
      "cnt: 0 - valLoss: 0.6662107110023499 - trainLoss: 0.6645201444625854\n",
      "cnt: 0 - valLoss: 0.6662105321884155 - trainLoss: 0.6645200252532959\n",
      "cnt: 0 - valLoss: 0.6662104725837708 - trainLoss: 0.6645199060440063\n",
      "cnt: 0 - valLoss: 0.6662102937698364 - trainLoss: 0.6645197868347168\n",
      "cnt: 0 - valLoss: 0.6662101745605469 - trainLoss: 0.664519727230072\n",
      "cnt: 0 - valLoss: 0.6662100553512573 - trainLoss: 0.6645196080207825\n",
      "cnt: 0 - valLoss: 0.666209876537323 - trainLoss: 0.6645194888114929\n",
      "cnt: 0 - valLoss: 0.6662097573280334 - trainLoss: 0.6645193696022034\n",
      "cnt: 0 - valLoss: 0.6662096381187439 - trainLoss: 0.6645192503929138\n",
      "cnt: 0 - valLoss: 0.6662095785140991 - trainLoss: 0.664519190788269\n",
      "cnt: 0 - valLoss: 0.6662094593048096 - trainLoss: 0.6645190715789795\n",
      "cnt: 0 - valLoss: 0.6662092804908752 - trainLoss: 0.6645189523696899\n",
      "cnt: 0 - valLoss: 0.6662091612815857 - trainLoss: 0.6645187735557556\n",
      "cnt: 0 - valLoss: 0.6662090420722961 - trainLoss: 0.6645187139511108\n",
      "cnt: 0 - valLoss: 0.6662088632583618 - trainLoss: 0.6645186543464661\n",
      "cnt: 0 - valLoss: 0.6662087440490723 - trainLoss: 0.6645185351371765\n",
      "cnt: 0 - valLoss: 0.6662086248397827 - trainLoss: 0.664518415927887\n",
      "cnt: 0 - valLoss: 0.6662085652351379 - trainLoss: 0.6645182967185974\n",
      "cnt: 0 - valLoss: 0.6662083864212036 - trainLoss: 0.6645182371139526\n",
      "cnt: 0 - valLoss: 0.6662082672119141 - trainLoss: 0.6645180583000183\n",
      "cnt: 0 - valLoss: 0.6662080883979797 - trainLoss: 0.6645179986953735\n",
      "cnt: 0 - valLoss: 0.6662079691886902 - trainLoss: 0.6645178198814392\n",
      "cnt: 0 - valLoss: 0.6662079095840454 - trainLoss: 0.6645177602767944\n",
      "cnt: 0 - valLoss: 0.6662077903747559 - trainLoss: 0.6645176410675049\n",
      "cnt: 0 - valLoss: 0.6662076115608215 - trainLoss: 0.6645175814628601\n",
      "cnt: 0 - valLoss: 0.666207492351532 - trainLoss: 0.6645174622535706\n",
      "cnt: 0 - valLoss: 0.6662073731422424 - trainLoss: 0.664517343044281\n",
      "cnt: 0 - valLoss: 0.6662071943283081 - trainLoss: 0.6645172834396362\n",
      "cnt: 0 - valLoss: 0.6662071347236633 - trainLoss: 0.6645171642303467\n",
      "cnt: 0 - valLoss: 0.6662070155143738 - trainLoss: 0.6645169854164124\n",
      "cnt: 0 - valLoss: 0.6662068963050842 - trainLoss: 0.6645169258117676\n",
      "cnt: 0 - valLoss: 0.6662067174911499 - trainLoss: 0.664516806602478\n",
      "cnt: 0 - valLoss: 0.6662065982818604 - trainLoss: 0.6645166873931885\n",
      "cnt: 0 - valLoss: 0.6662064790725708 - trainLoss: 0.6645166277885437\n",
      "cnt: 0 - valLoss: 0.666206419467926 - trainLoss: 0.6645165085792542\n",
      "cnt: 0 - valLoss: 0.6662062406539917 - trainLoss: 0.6645163893699646\n",
      "cnt: 0 - valLoss: 0.6662061214447021 - trainLoss: 0.664516270160675\n",
      "cnt: 0 - valLoss: 0.6662060022354126 - trainLoss: 0.6645160913467407\n",
      "cnt: 0 - valLoss: 0.6662058234214783 - trainLoss: 0.664516031742096\n",
      "cnt: 0 - valLoss: 0.6662057638168335 - trainLoss: 0.6645159125328064\n",
      "cnt: 0 - valLoss: 0.6662055850028992 - trainLoss: 0.6645158529281616\n",
      "cnt: 0 - valLoss: 0.6662054657936096 - trainLoss: 0.6645156741142273\n",
      "cnt: 0 - valLoss: 0.6662053465843201 - trainLoss: 0.6645156145095825\n",
      "cnt: 0 - valLoss: 0.6662052273750305 - trainLoss: 0.664515495300293\n",
      "cnt: 0 - valLoss: 0.6662050485610962 - trainLoss: 0.6645153760910034\n",
      "cnt: 0 - valLoss: 0.6662050485610962 - trainLoss: 0.6645153164863586\n",
      "cnt: 0 - valLoss: 0.6662048697471619 - trainLoss: 0.6645151972770691\n",
      "cnt: 0 - valLoss: 0.6662046909332275 - trainLoss: 0.6645150184631348\n",
      "cnt: 0 - valLoss: 0.666204571723938 - trainLoss: 0.66451495885849\n",
      "cnt: 0 - valLoss: 0.6662044525146484 - trainLoss: 0.6645148992538452\n",
      "cnt: 0 - valLoss: 0.6662042737007141 - trainLoss: 0.6645147204399109\n",
      "cnt: 0 - valLoss: 0.6662042140960693 - trainLoss: 0.6645146012306213\n",
      "cnt: 0 - valLoss: 0.6662041544914246 - trainLoss: 0.6645145416259766\n",
      "cnt: 0 - valLoss: 0.6662039756774902 - trainLoss: 0.664514422416687\n",
      "cnt: 0 - valLoss: 0.6662038564682007 - trainLoss: 0.6645143032073975\n",
      "cnt: 0 - valLoss: 0.6662036776542664 - trainLoss: 0.6645142436027527\n",
      "cnt: 0 - valLoss: 0.6662035584449768 - trainLoss: 0.6645141243934631\n",
      "cnt: 0 - valLoss: 0.6662034392356873 - trainLoss: 0.6645139455795288\n",
      "cnt: 0 - valLoss: 0.6662032604217529 - trainLoss: 0.664513885974884\n",
      "cnt: 0 - valLoss: 0.6662031412124634 - trainLoss: 0.6645138263702393\n",
      "cnt: 0 - valLoss: 0.6662030220031738 - trainLoss: 0.6645136475563049\n",
      "cnt: 0 - valLoss: 0.6662028431892395 - trainLoss: 0.6645135879516602\n",
      "cnt: 0 - valLoss: 0.6662027835845947 - trainLoss: 0.6645134687423706\n",
      "cnt: 0 - valLoss: 0.6662026643753052 - trainLoss: 0.664513349533081\n",
      "cnt: 0 - valLoss: 0.6662025451660156 - trainLoss: 0.6645132303237915\n",
      "cnt: 0 - valLoss: 0.6662024259567261 - trainLoss: 0.664513111114502\n",
      "cnt: 0 - valLoss: 0.6662022471427917 - trainLoss: 0.6645130515098572\n",
      "cnt: 0 - valLoss: 0.6662021279335022 - trainLoss: 0.6645128726959229\n",
      "cnt: 0 - valLoss: 0.6662020087242126 - trainLoss: 0.6645127534866333\n",
      "cnt: 0 - valLoss: 0.6662018895149231 - trainLoss: 0.6645126938819885\n",
      "cnt: 0 - valLoss: 0.6662017107009888 - trainLoss: 0.664512574672699\n",
      "cnt: 0 - valLoss: 0.666201651096344 - trainLoss: 0.6645124554634094\n",
      "cnt: 0 - valLoss: 0.6662015318870544 - trainLoss: 0.6645123362541199\n",
      "cnt: 0 - valLoss: 0.6662014126777649 - trainLoss: 0.6645122766494751\n",
      "cnt: 0 - valLoss: 0.6662012934684753 - trainLoss: 0.6645121574401855\n",
      "cnt: 0 - valLoss: 0.666201114654541 - trainLoss: 0.664512038230896\n",
      "cnt: 0 - valLoss: 0.6662009954452515 - trainLoss: 0.6645119190216064\n",
      "cnt: 0 - valLoss: 0.6662008762359619 - trainLoss: 0.6645118594169617\n",
      "cnt: 0 - valLoss: 0.6662006974220276 - trainLoss: 0.6645116806030273\n",
      "cnt: 0 - valLoss: 0.6662006378173828 - trainLoss: 0.6645115613937378\n",
      "cnt: 0 - valLoss: 0.6662004590034485 - trainLoss: 0.664511501789093\n",
      "cnt: 0 - valLoss: 0.6662003397941589 - trainLoss: 0.6645113825798035\n",
      "cnt: 0 - valLoss: 0.6662002205848694 - trainLoss: 0.6645113229751587\n",
      "cnt: 0 - valLoss: 0.6662001013755798 - trainLoss: 0.6645112037658691\n",
      "cnt: 0 - valLoss: 0.6661999225616455 - trainLoss: 0.6645110249519348\n",
      "cnt: 0 - valLoss: 0.6661998629570007 - trainLoss: 0.66451096534729\n",
      "cnt: 0 - valLoss: 0.6661996841430664 - trainLoss: 0.6645109057426453\n",
      "cnt: 0 - valLoss: 0.6661995649337769 - trainLoss: 0.6645107865333557\n",
      "cnt: 0 - valLoss: 0.6661994457244873 - trainLoss: 0.6645106077194214\n",
      "cnt: 0 - valLoss: 0.6661993265151978 - trainLoss: 0.6645105481147766\n",
      "cnt: 0 - valLoss: 0.6661992073059082 - trainLoss: 0.6645104289054871\n",
      "cnt: 0 - valLoss: 0.6661990284919739 - trainLoss: 0.6645103096961975\n",
      "cnt: 0 - valLoss: 0.6661989092826843 - trainLoss: 0.664510190486908\n",
      "cnt: 0 - valLoss: 0.6661987900733948 - trainLoss: 0.6645100712776184\n",
      "cnt: 0 - valLoss: 0.6661986708641052 - trainLoss: 0.6645099520683289\n",
      "cnt: 0 - valLoss: 0.6661984920501709 - trainLoss: 0.6645098924636841\n",
      "cnt: 0 - valLoss: 0.6661983728408813 - trainLoss: 0.6645097136497498\n",
      "cnt: 0 - valLoss: 0.6661982536315918 - trainLoss: 0.664509654045105\n",
      "cnt: 0 - valLoss: 0.6661981344223022 - trainLoss: 0.6645095348358154\n",
      "cnt: 0 - valLoss: 0.6661980152130127 - trainLoss: 0.6645094156265259\n",
      "cnt: 0 - valLoss: 0.6661978960037231 - trainLoss: 0.6645093560218811\n",
      "cnt: 0 - valLoss: 0.6661977767944336 - trainLoss: 0.6645092368125916\n",
      "cnt: 0 - valLoss: 0.666197657585144 - trainLoss: 0.664509117603302\n",
      "cnt: 0 - valLoss: 0.6661975383758545 - trainLoss: 0.6645089983940125\n",
      "cnt: 0 - valLoss: 0.6661974191665649 - trainLoss: 0.6645088791847229\n",
      "cnt: 0 - valLoss: 0.6661972999572754 - trainLoss: 0.6645088195800781\n",
      "cnt: 0 - valLoss: 0.6661971211433411 - trainLoss: 0.6645087003707886\n",
      "cnt: 0 - valLoss: 0.6661970019340515 - trainLoss: 0.6645085215568542\n",
      "cnt: 0 - valLoss: 0.6661968231201172 - trainLoss: 0.6645084023475647\n",
      "cnt: 0 - valLoss: 0.6661967635154724 - trainLoss: 0.6645083427429199\n",
      "cnt: 0 - valLoss: 0.6661966443061829 - trainLoss: 0.6645082235336304\n",
      "cnt: 0 - valLoss: 0.6661965250968933 - trainLoss: 0.6645081043243408\n",
      "cnt: 0 - valLoss: 0.666196346282959 - trainLoss: 0.6645079851150513\n",
      "cnt: 0 - valLoss: 0.6661962270736694 - trainLoss: 0.6645078659057617\n",
      "cnt: 0 - valLoss: 0.6661960482597351 - trainLoss: 0.6645078063011169\n",
      "cnt: 0 - valLoss: 0.6661959290504456 - trainLoss: 0.6645077466964722\n",
      "cnt: 0 - valLoss: 0.666195809841156 - trainLoss: 0.6645075678825378\n",
      "cnt: 0 - valLoss: 0.6661957502365112 - trainLoss: 0.6645074486732483\n",
      "cnt: 0 - valLoss: 0.6661956310272217 - trainLoss: 0.6645073890686035\n",
      "cnt: 0 - valLoss: 0.6661954522132874 - trainLoss: 0.664507269859314\n",
      "cnt: 0 - valLoss: 0.666195273399353 - trainLoss: 0.6645071506500244\n",
      "cnt: 0 - valLoss: 0.6661952137947083 - trainLoss: 0.6645070314407349\n",
      "cnt: 0 - valLoss: 0.6661950349807739 - trainLoss: 0.6645069122314453\n",
      "cnt: 0 - valLoss: 0.6661949157714844 - trainLoss: 0.6645067930221558\n",
      "cnt: 0 - valLoss: 0.6661947965621948 - trainLoss: 0.6645066738128662\n",
      "cnt: 0 - valLoss: 0.6661946773529053 - trainLoss: 0.6645066142082214\n",
      "cnt: 0 - valLoss: 0.6661945581436157 - trainLoss: 0.6645064949989319\n",
      "cnt: 0 - valLoss: 0.6661944389343262 - trainLoss: 0.6645063757896423\n",
      "cnt: 0 - valLoss: 0.6661943197250366 - trainLoss: 0.6645062565803528\n",
      "cnt: 0 - valLoss: 0.6661941409111023 - trainLoss: 0.6645061373710632\n",
      "cnt: 0 - valLoss: 0.6661940813064575 - trainLoss: 0.6645060777664185\n",
      "cnt: 0 - valLoss: 0.6661939024925232 - trainLoss: 0.6645058989524841\n",
      "cnt: 0 - valLoss: 0.6661937832832336 - trainLoss: 0.6645058393478394\n",
      "cnt: 0 - valLoss: 0.6661936044692993 - trainLoss: 0.6645057797431946\n",
      "cnt: 0 - valLoss: 0.6661934852600098 - trainLoss: 0.664505660533905\n",
      "cnt: 0 - valLoss: 0.6661933660507202 - trainLoss: 0.6645054817199707\n",
      "cnt: 0 - valLoss: 0.6661931872367859 - trainLoss: 0.6645054221153259\n",
      "cnt: 0 - valLoss: 0.6661930680274963 - trainLoss: 0.6645052433013916\n",
      "cnt: 0 - valLoss: 0.6661930084228516 - trainLoss: 0.6645051836967468\n",
      "cnt: 0 - valLoss: 0.666192889213562 - trainLoss: 0.6645050644874573\n",
      "cnt: 0 - valLoss: 0.6661927103996277 - trainLoss: 0.6645050048828125\n",
      "cnt: 0 - valLoss: 0.6661926507949829 - trainLoss: 0.6645048260688782\n",
      "cnt: 0 - valLoss: 0.6661924719810486 - trainLoss: 0.6645047664642334\n",
      "cnt: 0 - valLoss: 0.6661922931671143 - trainLoss: 0.6645046472549438\n",
      "cnt: 0 - valLoss: 0.6661922335624695 - trainLoss: 0.6645045280456543\n",
      "cnt: 0 - valLoss: 0.6661921143531799 - trainLoss: 0.6645044088363647\n",
      "cnt: 0 - valLoss: 0.6661919355392456 - trainLoss: 0.6645042896270752\n",
      "cnt: 0 - valLoss: 0.666191816329956 - trainLoss: 0.6645042300224304\n",
      "cnt: 0 - valLoss: 0.6661916971206665 - trainLoss: 0.6645041108131409\n",
      "cnt: 0 - valLoss: 0.666191577911377 - trainLoss: 0.6645039319992065\n",
      "cnt: 0 - valLoss: 0.6661914587020874 - trainLoss: 0.664503812789917\n",
      "cnt: 0 - valLoss: 0.6661913394927979 - trainLoss: 0.6645037531852722\n",
      "cnt: 0 - valLoss: 0.6661911606788635 - trainLoss: 0.6645036339759827\n",
      "cnt: 0 - valLoss: 0.666191041469574 - trainLoss: 0.6645035743713379\n",
      "cnt: 0 - valLoss: 0.6661908626556396 - trainLoss: 0.6645033955574036\n",
      "cnt: 0 - valLoss: 0.6661907434463501 - trainLoss: 0.6645033359527588\n",
      "cnt: 0 - valLoss: 0.6661906838417053 - trainLoss: 0.6645031571388245\n",
      "cnt: 0 - valLoss: 0.666190505027771 - trainLoss: 0.6645030975341797\n",
      "cnt: 0 - valLoss: 0.6661903858184814 - trainLoss: 0.6645029783248901\n",
      "cnt: 0 - valLoss: 0.6661902666091919 - trainLoss: 0.6645028591156006\n",
      "cnt: 0 - valLoss: 0.6661900877952576 - trainLoss: 0.664502739906311\n",
      "cnt: 0 - valLoss: 0.666189968585968 - trainLoss: 0.6645026803016663\n",
      "cnt: 0 - valLoss: 0.6661899089813232 - trainLoss: 0.6645025610923767\n",
      "cnt: 0 - valLoss: 0.6661897301673889 - trainLoss: 0.6645024418830872\n",
      "cnt: 0 - valLoss: 0.6661896109580994 - trainLoss: 0.6645023226737976\n",
      "cnt: 0 - valLoss: 0.6661894917488098 - trainLoss: 0.6645022630691528\n",
      "cnt: 0 - valLoss: 0.6661893129348755 - trainLoss: 0.6645021438598633\n",
      "cnt: 0 - valLoss: 0.6661891937255859 - trainLoss: 0.664501965045929\n",
      "cnt: 0 - valLoss: 0.6661891341209412 - trainLoss: 0.6645018458366394\n",
      "cnt: 0 - valLoss: 0.6661889553070068 - trainLoss: 0.6645017862319946\n",
      "cnt: 0 - valLoss: 0.6661888957023621 - trainLoss: 0.6645016670227051\n",
      "cnt: 0 - valLoss: 0.6661887168884277 - trainLoss: 0.6645015478134155\n",
      "cnt: 0 - valLoss: 0.6661885976791382 - trainLoss: 0.664501428604126\n",
      "cnt: 0 - valLoss: 0.6661884188652039 - trainLoss: 0.6645013689994812\n",
      "cnt: 0 - valLoss: 0.6661882996559143 - trainLoss: 0.6645013093948364\n",
      "cnt: 0 - valLoss: 0.6661881804466248 - trainLoss: 0.6645011305809021\n",
      "cnt: 0 - valLoss: 0.6661880016326904 - trainLoss: 0.6645010709762573\n",
      "cnt: 0 - valLoss: 0.6661879420280457 - trainLoss: 0.6645009517669678\n",
      "cnt: 0 - valLoss: 0.6661878228187561 - trainLoss: 0.664500892162323\n",
      "cnt: 0 - valLoss: 0.6661877036094666 - trainLoss: 0.6645007133483887\n",
      "cnt: 0 - valLoss: 0.6661875247955322 - trainLoss: 0.6645005941390991\n",
      "cnt: 0 - valLoss: 0.6661874055862427 - trainLoss: 0.6645004749298096\n",
      "cnt: 0 - valLoss: 0.6661872267723083 - trainLoss: 0.66450035572052\n",
      "cnt: 0 - valLoss: 0.6661871671676636 - trainLoss: 0.6645002961158752\n",
      "cnt: 0 - valLoss: 0.6661869883537292 - trainLoss: 0.6645001173019409\n",
      "cnt: 0 - valLoss: 0.6661869287490845 - trainLoss: 0.6644999980926514\n",
      "cnt: 0 - valLoss: 0.6661867499351501 - trainLoss: 0.6644998788833618\n",
      "cnt: 0 - valLoss: 0.6661865711212158 - trainLoss: 0.664499819278717\n",
      "cnt: 0 - valLoss: 0.6661864519119263 - trainLoss: 0.6644997000694275\n",
      "cnt: 0 - valLoss: 0.6661863327026367 - trainLoss: 0.6644995808601379\n",
      "cnt: 0 - valLoss: 0.6661862134933472 - trainLoss: 0.6644995212554932\n",
      "cnt: 0 - valLoss: 0.6661860942840576 - trainLoss: 0.6644993424415588\n",
      "cnt: 0 - valLoss: 0.6661859750747681 - trainLoss: 0.6644992828369141\n",
      "cnt: 0 - valLoss: 0.6661858558654785 - trainLoss: 0.6644991636276245\n",
      "cnt: 0 - valLoss: 0.6661856770515442 - trainLoss: 0.6644991040229797\n",
      "cnt: 0 - valLoss: 0.6661855578422546 - trainLoss: 0.6644989252090454\n",
      "cnt: 0 - valLoss: 0.6661854386329651 - trainLoss: 0.6644988059997559\n",
      "cnt: 0 - valLoss: 0.6661852598190308 - trainLoss: 0.6644988059997559\n",
      "cnt: 0 - valLoss: 0.666185200214386 - trainLoss: 0.6644986271858215\n",
      "cnt: 0 - valLoss: 0.6661850214004517 - trainLoss: 0.664498507976532\n",
      "cnt: 0 - valLoss: 0.6661849021911621 - trainLoss: 0.6644983887672424\n",
      "cnt: 0 - valLoss: 0.6661847829818726 - trainLoss: 0.6644983291625977\n",
      "cnt: 0 - valLoss: 0.666184663772583 - trainLoss: 0.6644981503486633\n",
      "cnt: 0 - valLoss: 0.6661844849586487 - trainLoss: 0.6644980311393738\n",
      "cnt: 0 - valLoss: 0.6661843657493591 - trainLoss: 0.6644980311393738\n",
      "cnt: 0 - valLoss: 0.6661842465400696 - trainLoss: 0.6644978523254395\n",
      "cnt: 0 - valLoss: 0.6661841869354248 - trainLoss: 0.6644977331161499\n",
      "cnt: 0 - valLoss: 0.6661840081214905 - trainLoss: 0.6644976139068604\n",
      "cnt: 0 - valLoss: 0.6661838889122009 - trainLoss: 0.6644974946975708\n",
      "cnt: 0 - valLoss: 0.6661837100982666 - trainLoss: 0.6644973754882812\n",
      "cnt: 0 - valLoss: 0.6661836504936218 - trainLoss: 0.6644973158836365\n",
      "cnt: 0 - valLoss: 0.6661834716796875 - trainLoss: 0.6644971370697021\n",
      "cnt: 0 - valLoss: 0.666183352470398 - trainLoss: 0.6644971370697021\n",
      "cnt: 0 - valLoss: 0.6661832332611084 - trainLoss: 0.6644969582557678\n",
      "cnt: 0 - valLoss: 0.6661830544471741 - trainLoss: 0.6644968390464783\n",
      "cnt: 0 - valLoss: 0.6661829352378845 - trainLoss: 0.6644967198371887\n",
      "cnt: 0 - valLoss: 0.6661827564239502 - trainLoss: 0.6644966006278992\n",
      "cnt: 0 - valLoss: 0.6661826968193054 - trainLoss: 0.6644964814186096\n",
      "cnt: 0 - valLoss: 0.6661825180053711 - trainLoss: 0.6644964218139648\n",
      "cnt: 0 - valLoss: 0.6661824584007263 - trainLoss: 0.6644963026046753\n",
      "cnt: 0 - valLoss: 0.666182279586792 - trainLoss: 0.6644961833953857\n",
      "cnt: 0 - valLoss: 0.6661821603775024 - trainLoss: 0.6644960641860962\n",
      "cnt: 0 - valLoss: 0.6661820411682129 - trainLoss: 0.6644960045814514\n",
      "cnt: 0 - valLoss: 0.6661818623542786 - trainLoss: 0.6644958257675171\n",
      "cnt: 0 - valLoss: 0.666181743144989 - trainLoss: 0.6644957661628723\n",
      "cnt: 0 - valLoss: 0.6661816239356995 - trainLoss: 0.664495587348938\n",
      "cnt: 0 - valLoss: 0.6661815047264099 - trainLoss: 0.6644955277442932\n",
      "cnt: 0 - valLoss: 0.6661813855171204 - trainLoss: 0.6644954085350037\n",
      "cnt: 0 - valLoss: 0.6661812663078308 - trainLoss: 0.6644952893257141\n",
      "cnt: 0 - valLoss: 0.6661811470985413 - trainLoss: 0.6644952297210693\n",
      "cnt: 0 - valLoss: 0.6661809682846069 - trainLoss: 0.664495050907135\n",
      "cnt: 0 - valLoss: 0.6661808490753174 - trainLoss: 0.6644949913024902\n",
      "cnt: 0 - valLoss: 0.6661807298660278 - trainLoss: 0.6644948720932007\n",
      "cnt: 0 - valLoss: 0.6661806106567383 - trainLoss: 0.6644948124885559\n",
      "cnt: 0 - valLoss: 0.6661804914474487 - trainLoss: 0.6644946336746216\n",
      "cnt: 0 - valLoss: 0.6661803126335144 - trainLoss: 0.664494514465332\n",
      "cnt: 0 - valLoss: 0.6661801934242249 - trainLoss: 0.6644944548606873\n",
      "cnt: 0 - valLoss: 0.6661801338195801 - trainLoss: 0.6644943356513977\n",
      "cnt: 0 - valLoss: 0.6661799550056458 - trainLoss: 0.6644942164421082\n",
      "cnt: 0 - valLoss: 0.6661797761917114 - trainLoss: 0.6644940972328186\n",
      "cnt: 0 - valLoss: 0.6661797165870667 - trainLoss: 0.664493978023529\n",
      "cnt: 0 - valLoss: 0.6661795973777771 - trainLoss: 0.6644939184188843\n",
      "cnt: 0 - valLoss: 0.6661794185638428 - trainLoss: 0.66449373960495\n",
      "cnt: 0 - valLoss: 0.6661792993545532 - trainLoss: 0.6644936203956604\n",
      "cnt: 0 - valLoss: 0.6661791205406189 - trainLoss: 0.6644935607910156\n",
      "cnt: 0 - valLoss: 0.6661790013313293 - trainLoss: 0.6644934415817261\n",
      "cnt: 0 - valLoss: 0.6661789417266846 - trainLoss: 0.6644933223724365\n",
      "cnt: 0 - valLoss: 0.6661787629127502 - trainLoss: 0.6644931435585022\n",
      "cnt: 0 - valLoss: 0.6661786437034607 - trainLoss: 0.6644930839538574\n",
      "cnt: 0 - valLoss: 0.6661785244941711 - trainLoss: 0.6644930243492126\n",
      "cnt: 0 - valLoss: 0.6661783456802368 - trainLoss: 0.6644929051399231\n",
      "cnt: 0 - valLoss: 0.666178286075592 - trainLoss: 0.6644927263259888\n",
      "cnt: 0 - valLoss: 0.6661781668663025 - trainLoss: 0.6644926071166992\n",
      "cnt: 0 - valLoss: 0.6661779880523682 - trainLoss: 0.6644925475120544\n",
      "cnt: 0 - valLoss: 0.6661778092384338 - trainLoss: 0.6644924283027649\n",
      "cnt: 0 - valLoss: 0.6661777496337891 - trainLoss: 0.6644923090934753\n",
      "cnt: 0 - valLoss: 0.6661775708198547 - trainLoss: 0.6644922494888306\n",
      "cnt: 0 - valLoss: 0.6661774516105652 - trainLoss: 0.664492130279541\n",
      "cnt: 0 - valLoss: 0.6661772727966309 - trainLoss: 0.6644919514656067\n",
      "cnt: 0 - valLoss: 0.6661772131919861 - trainLoss: 0.6644918918609619\n",
      "cnt: 0 - valLoss: 0.6661770939826965 - trainLoss: 0.6644917726516724\n",
      "cnt: 0 - valLoss: 0.666176974773407 - trainLoss: 0.6644916534423828\n",
      "cnt: 0 - valLoss: 0.6661768555641174 - trainLoss: 0.664491593837738\n",
      "cnt: 0 - valLoss: 0.6661766767501831 - trainLoss: 0.6644914746284485\n",
      "cnt: 0 - valLoss: 0.6661765575408936 - trainLoss: 0.6644913554191589\n",
      "cnt: 0 - valLoss: 0.666176438331604 - trainLoss: 0.6644911766052246\n",
      "cnt: 0 - valLoss: 0.6661762595176697 - trainLoss: 0.6644911170005798\n",
      "cnt: 0 - valLoss: 0.6661761999130249 - trainLoss: 0.6644909977912903\n",
      "cnt: 0 - valLoss: 0.6661760210990906 - trainLoss: 0.6644908785820007\n",
      "cnt: 0 - valLoss: 0.666175901889801 - trainLoss: 0.6644907593727112\n",
      "cnt: 0 - valLoss: 0.6661757826805115 - trainLoss: 0.6644906997680664\n",
      "cnt: 0 - valLoss: 0.6661756038665771 - trainLoss: 0.6644905805587769\n",
      "cnt: 0 - valLoss: 0.6661754846572876 - trainLoss: 0.6644904017448425\n",
      "cnt: 0 - valLoss: 0.6661753058433533 - trainLoss: 0.664490282535553\n",
      "cnt: 0 - valLoss: 0.6661752462387085 - trainLoss: 0.6644902229309082\n",
      "cnt: 0 - valLoss: 0.666175127029419 - trainLoss: 0.6644901037216187\n",
      "cnt: 0 - valLoss: 0.6661749482154846 - trainLoss: 0.6644899845123291\n",
      "cnt: 0 - valLoss: 0.6661748290061951 - trainLoss: 0.6644898653030396\n",
      "cnt: 0 - valLoss: 0.6661747097969055 - trainLoss: 0.66448974609375\n",
      "cnt: 0 - valLoss: 0.6661746501922607 - trainLoss: 0.6644896864891052\n",
      "cnt: 0 - valLoss: 0.6661744117736816 - trainLoss: 0.6644895672798157\n",
      "cnt: 0 - valLoss: 0.6661743521690369 - trainLoss: 0.6644894480705261\n",
      "cnt: 0 - valLoss: 0.6661742329597473 - trainLoss: 0.6644893288612366\n",
      "cnt: 0 - valLoss: 0.666174054145813 - trainLoss: 0.664489209651947\n",
      "cnt: 0 - valLoss: 0.6661739349365234 - trainLoss: 0.6644891500473022\n",
      "cnt: 0 - valLoss: 0.6661737561225891 - trainLoss: 0.6644889712333679\n",
      "cnt: 0 - valLoss: 0.6661736965179443 - trainLoss: 0.6644889116287231\n",
      "cnt: 0 - valLoss: 0.66617351770401 - trainLoss: 0.6644887924194336\n",
      "cnt: 0 - valLoss: 0.6661733388900757 - trainLoss: 0.664488673210144\n",
      "cnt: 0 - valLoss: 0.6661732196807861 - trainLoss: 0.6644885540008545\n",
      "cnt: 0 - valLoss: 0.6661731600761414 - trainLoss: 0.6644884347915649\n",
      "cnt: 0 - valLoss: 0.6661730408668518 - trainLoss: 0.6644883155822754\n",
      "cnt: 0 - valLoss: 0.6661729216575623 - trainLoss: 0.6644882559776306\n",
      "cnt: 0 - valLoss: 0.6661727428436279 - trainLoss: 0.6644881367683411\n",
      "cnt: 0 - valLoss: 0.6661726236343384 - trainLoss: 0.6644879579544067\n",
      "cnt: 0 - valLoss: 0.6661725044250488 - trainLoss: 0.664487898349762\n",
      "cnt: 0 - valLoss: 0.6661723256111145 - trainLoss: 0.6644877791404724\n",
      "cnt: 0 - valLoss: 0.666172206401825 - trainLoss: 0.6644876599311829\n",
      "cnt: 0 - valLoss: 0.6661720871925354 - trainLoss: 0.6644875407218933\n",
      "cnt: 0 - valLoss: 0.6661719679832458 - trainLoss: 0.6644874215126038\n",
      "cnt: 0 - valLoss: 0.6661717891693115 - trainLoss: 0.664487361907959\n",
      "cnt: 0 - valLoss: 0.666171669960022 - trainLoss: 0.6644871830940247\n",
      "cnt: 0 - valLoss: 0.6661715507507324 - trainLoss: 0.6644871234893799\n",
      "cnt: 0 - valLoss: 0.6661713719367981 - trainLoss: 0.6644870042800903\n",
      "cnt: 0 - valLoss: 0.6661713123321533 - trainLoss: 0.6644868850708008\n",
      "cnt: 0 - valLoss: 0.6661711931228638 - trainLoss: 0.6644867658615112\n",
      "cnt: 0 - valLoss: 0.6661710143089294 - trainLoss: 0.6644866466522217\n",
      "cnt: 0 - valLoss: 0.6661708950996399 - trainLoss: 0.6644865870475769\n",
      "cnt: 0 - valLoss: 0.6661707758903503 - trainLoss: 0.6644864082336426\n",
      "cnt: 0 - valLoss: 0.666170597076416 - trainLoss: 0.6644863486289978\n",
      "cnt: 0 - valLoss: 0.6661705374717712 - trainLoss: 0.6644861698150635\n",
      "cnt: 0 - valLoss: 0.6661703586578369 - trainLoss: 0.6644861698150635\n",
      "cnt: 0 - valLoss: 0.6661702394485474 - trainLoss: 0.6644859910011292\n",
      "cnt: 0 - valLoss: 0.6661701202392578 - trainLoss: 0.6644858717918396\n",
      "cnt: 0 - valLoss: 0.6661699414253235 - trainLoss: 0.6644856929779053\n",
      "cnt: 0 - valLoss: 0.6661698222160339 - trainLoss: 0.6644856929779053\n",
      "cnt: 0 - valLoss: 0.6661697030067444 - trainLoss: 0.664485514163971\n",
      "cnt: 0 - valLoss: 0.6661695837974548 - trainLoss: 0.6644853949546814\n",
      "cnt: 0 - valLoss: 0.6661694645881653 - trainLoss: 0.6644852757453918\n",
      "cnt: 0 - valLoss: 0.6661693453788757 - trainLoss: 0.6644852757453918\n",
      "cnt: 0 - valLoss: 0.6661692261695862 - trainLoss: 0.6644850969314575\n",
      "cnt: 0 - valLoss: 0.6661690473556519 - trainLoss: 0.664484977722168\n",
      "cnt: 0 - valLoss: 0.6661689281463623 - trainLoss: 0.6644848585128784\n",
      "cnt: 0 - valLoss: 0.6661688089370728 - trainLoss: 0.6644847393035889\n",
      "cnt: 0 - valLoss: 0.6661686897277832 - trainLoss: 0.6644846200942993\n",
      "cnt: 0 - valLoss: 0.6661685705184937 - trainLoss: 0.6644845604896545\n",
      "cnt: 0 - valLoss: 0.6661683917045593 - trainLoss: 0.6644843816757202\n",
      "cnt: 0 - valLoss: 0.6661682724952698 - trainLoss: 0.6644842624664307\n",
      "cnt: 0 - valLoss: 0.6661681532859802 - trainLoss: 0.6644842028617859\n",
      "cnt: 0 - valLoss: 0.6661680340766907 - trainLoss: 0.6644840836524963\n",
      "cnt: 0 - valLoss: 0.6661678552627563 - trainLoss: 0.6644840240478516\n",
      "cnt: 0 - valLoss: 0.6661677360534668 - trainLoss: 0.6644838452339172\n",
      "cnt: 0 - valLoss: 0.6661676168441772 - trainLoss: 0.6644837260246277\n",
      "cnt: 0 - valLoss: 0.6661674976348877 - trainLoss: 0.6644836664199829\n",
      "cnt: 0 - valLoss: 0.6661673188209534 - trainLoss: 0.6644835472106934\n",
      "cnt: 0 - valLoss: 0.6661671996116638 - trainLoss: 0.6644834280014038\n",
      "cnt: 0 - valLoss: 0.6661670804023743 - trainLoss: 0.6644833087921143\n",
      "cnt: 0 - valLoss: 0.6661669015884399 - trainLoss: 0.6644832491874695\n",
      "cnt: 0 - valLoss: 0.6661667823791504 - trainLoss: 0.6644831299781799\n",
      "cnt: 0 - valLoss: 0.6661666631698608 - trainLoss: 0.6644829511642456\n",
      "cnt: 0 - valLoss: 0.6661666035652161 - trainLoss: 0.664482831954956\n",
      "cnt: 0 - valLoss: 0.6661664843559265 - trainLoss: 0.6644827127456665\n",
      "cnt: 0 - valLoss: 0.6661663055419922 - trainLoss: 0.6644826531410217\n",
      "cnt: 0 - valLoss: 0.6661661267280579 - trainLoss: 0.6644825339317322\n",
      "cnt: 0 - valLoss: 0.6661660075187683 - trainLoss: 0.6644824147224426\n",
      "cnt: 0 - valLoss: 0.6661658883094788 - trainLoss: 0.6644822955131531\n",
      "cnt: 0 - valLoss: 0.6661657094955444 - trainLoss: 0.6644822359085083\n",
      "cnt: 0 - valLoss: 0.6661656498908997 - trainLoss: 0.6644821166992188\n",
      "cnt: 0 - valLoss: 0.6661655306816101 - trainLoss: 0.6644819378852844\n",
      "cnt: 0 - valLoss: 0.6661653518676758 - trainLoss: 0.6644818782806396\n",
      "cnt: 0 - valLoss: 0.6661652326583862 - trainLoss: 0.6644817590713501\n",
      "cnt: 0 - valLoss: 0.6661651134490967 - trainLoss: 0.6644816398620605\n",
      "cnt: 0 - valLoss: 0.6661649346351624 - trainLoss: 0.6644814610481262\n",
      "cnt: 0 - valLoss: 0.6661648750305176 - trainLoss: 0.6644814014434814\n",
      "cnt: 0 - valLoss: 0.6661646962165833 - trainLoss: 0.6644813418388367\n",
      "cnt: 0 - valLoss: 0.6661646366119385 - trainLoss: 0.6644811630249023\n",
      "cnt: 0 - valLoss: 0.6661644577980042 - trainLoss: 0.6644810438156128\n",
      "cnt: 0 - valLoss: 0.6661643385887146 - trainLoss: 0.664480984210968\n",
      "cnt: 0 - valLoss: 0.6661641597747803 - trainLoss: 0.6644808650016785\n",
      "cnt: 0 - valLoss: 0.6661640405654907 - trainLoss: 0.6644806861877441\n",
      "cnt: 0 - valLoss: 0.6661639213562012 - trainLoss: 0.6644806265830994\n",
      "cnt: 0 - valLoss: 0.6661637425422668 - trainLoss: 0.6644805073738098\n",
      "cnt: 0 - valLoss: 0.6661636829376221 - trainLoss: 0.6644803881645203\n",
      "cnt: 0 - valLoss: 0.6661635637283325 - trainLoss: 0.6644802689552307\n",
      "cnt: 0 - valLoss: 0.6661633849143982 - trainLoss: 0.6644802093505859\n",
      "cnt: 0 - valLoss: 0.6661632657051086 - trainLoss: 0.6644800901412964\n",
      "cnt: 0 - valLoss: 0.6661630868911743 - trainLoss: 0.6644799709320068\n",
      "cnt: 0 - valLoss: 0.6661629676818848 - trainLoss: 0.6644798517227173\n",
      "cnt: 0 - valLoss: 0.6661628484725952 - trainLoss: 0.664479672908783\n",
      "cnt: 0 - valLoss: 0.6661627292633057 - trainLoss: 0.664479672908783\n",
      "cnt: 0 - valLoss: 0.6661626100540161 - trainLoss: 0.6644794940948486\n",
      "cnt: 0 - valLoss: 0.6661624908447266 - trainLoss: 0.6644793748855591\n",
      "cnt: 0 - valLoss: 0.6661623120307922 - trainLoss: 0.6644792556762695\n",
      "cnt: 0 - valLoss: 0.6661621928215027 - trainLoss: 0.66447913646698\n",
      "cnt: 0 - valLoss: 0.6661620736122131 - trainLoss: 0.6644790172576904\n",
      "cnt: 0 - valLoss: 0.6661618947982788 - trainLoss: 0.6644789576530457\n",
      "cnt: 0 - valLoss: 0.6661617755889893 - trainLoss: 0.6644788980484009\n",
      "cnt: 0 - valLoss: 0.6661615967750549 - trainLoss: 0.6644787192344666\n",
      "cnt: 0 - valLoss: 0.6661615371704102 - trainLoss: 0.664478600025177\n",
      "cnt: 0 - valLoss: 0.6661614179611206 - trainLoss: 0.6644784808158875\n",
      "cnt: 0 - valLoss: 0.666161298751831 - trainLoss: 0.6644783616065979\n",
      "cnt: 0 - valLoss: 0.6661611199378967 - trainLoss: 0.6644783020019531\n",
      "cnt: 0 - valLoss: 0.6661610007286072 - trainLoss: 0.6644781827926636\n",
      "cnt: 0 - valLoss: 0.6661608219146729 - trainLoss: 0.6644780039787292\n",
      "cnt: 0 - valLoss: 0.6661607623100281 - trainLoss: 0.6644779443740845\n",
      "cnt: 0 - valLoss: 0.6661605834960938 - trainLoss: 0.6644778251647949\n",
      "cnt: 0 - valLoss: 0.6661604642868042 - trainLoss: 0.6644776463508606\n",
      "cnt: 0 - valLoss: 0.6661603450775146 - trainLoss: 0.6644776463508606\n",
      "cnt: 0 - valLoss: 0.6661602258682251 - trainLoss: 0.664477527141571\n",
      "cnt: 0 - valLoss: 0.6661600470542908 - trainLoss: 0.6644774079322815\n",
      "cnt: 0 - valLoss: 0.666159987449646 - trainLoss: 0.6644772291183472\n",
      "cnt: 0 - valLoss: 0.6661598682403564 - trainLoss: 0.6644771695137024\n",
      "cnt: 0 - valLoss: 0.6661596894264221 - trainLoss: 0.6644770503044128\n",
      "cnt: 0 - valLoss: 0.6661595702171326 - trainLoss: 0.6644769310951233\n",
      "cnt: 0 - valLoss: 0.6661593914031982 - trainLoss: 0.6644768118858337\n",
      "cnt: 0 - valLoss: 0.6661592721939087 - trainLoss: 0.6644766926765442\n",
      "cnt: 0 - valLoss: 0.6661591529846191 - trainLoss: 0.6644765734672546\n",
      "cnt: 0 - valLoss: 0.6661589741706848 - trainLoss: 0.6644764542579651\n",
      "cnt: 0 - valLoss: 0.6661588549613953 - trainLoss: 0.6644763350486755\n",
      "cnt: 0 - valLoss: 0.6661587357521057 - trainLoss: 0.6644763350486755\n",
      "cnt: 0 - valLoss: 0.6661586165428162 - trainLoss: 0.6644761562347412\n",
      "cnt: 0 - valLoss: 0.6661584377288818 - trainLoss: 0.6644759774208069\n",
      "cnt: 0 - valLoss: 0.6661583781242371 - trainLoss: 0.6644759178161621\n",
      "cnt: 0 - valLoss: 0.6661581993103027 - trainLoss: 0.6644757986068726\n",
      "cnt: 0 - valLoss: 0.6661580801010132 - trainLoss: 0.664475679397583\n",
      "cnt: 0 - valLoss: 0.6661579012870789 - trainLoss: 0.6644755601882935\n",
      "cnt: 0 - valLoss: 0.6661578416824341 - trainLoss: 0.6644754409790039\n",
      "cnt: 0 - valLoss: 0.666157603263855 - trainLoss: 0.6644753813743591\n",
      "cnt: 0 - valLoss: 0.6661575436592102 - trainLoss: 0.6644752621650696\n",
      "cnt: 0 - valLoss: 0.6661574244499207 - trainLoss: 0.6644750833511353\n",
      "cnt: 0 - valLoss: 0.6661573052406311 - trainLoss: 0.6644750237464905\n",
      "cnt: 0 - valLoss: 0.6661571264266968 - trainLoss: 0.6644748449325562\n",
      "cnt: 0 - valLoss: 0.6661570072174072 - trainLoss: 0.6644747853279114\n",
      "cnt: 0 - valLoss: 0.6661568880081177 - trainLoss: 0.6644746661186218\n",
      "cnt: 0 - valLoss: 0.6661567091941833 - trainLoss: 0.664474606513977\n",
      "cnt: 0 - valLoss: 0.6661566495895386 - trainLoss: 0.6644744873046875\n",
      "cnt: 0 - valLoss: 0.666156530380249 - trainLoss: 0.6644743084907532\n",
      "cnt: 0 - valLoss: 0.6661563515663147 - trainLoss: 0.6644741892814636\n",
      "cnt: 0 - valLoss: 0.6661562323570251 - trainLoss: 0.6644740700721741\n",
      "cnt: 0 - valLoss: 0.6661560535430908 - trainLoss: 0.6644740104675293\n",
      "cnt: 0 - valLoss: 0.6661559343338013 - trainLoss: 0.6644738912582397\n",
      "cnt: 0 - valLoss: 0.6661558747291565 - trainLoss: 0.6644737124443054\n",
      "cnt: 0 - valLoss: 0.6661556959152222 - trainLoss: 0.6644736528396606\n",
      "cnt: 0 - valLoss: 0.6661555767059326 - trainLoss: 0.6644735336303711\n",
      "cnt: 0 - valLoss: 0.6661554574966431 - trainLoss: 0.6644734144210815\n",
      "cnt: 0 - valLoss: 0.6661553382873535 - trainLoss: 0.664473295211792\n",
      "cnt: 0 - valLoss: 0.6661551594734192 - trainLoss: 0.6644732356071472\n",
      "cnt: 0 - valLoss: 0.6661550402641296 - trainLoss: 0.6644730567932129\n",
      "cnt: 0 - valLoss: 0.6661548614501953 - trainLoss: 0.6644729971885681\n",
      "cnt: 0 - valLoss: 0.6661547422409058 - trainLoss: 0.6644728779792786\n",
      "cnt: 0 - valLoss: 0.6661545634269714 - trainLoss: 0.664472758769989\n",
      "cnt: 0 - valLoss: 0.6661545038223267 - trainLoss: 0.6644726395606995\n",
      "cnt: 0 - valLoss: 0.6661543846130371 - trainLoss: 0.6644725203514099\n",
      "cnt: 0 - valLoss: 0.6661542654037476 - trainLoss: 0.6644723415374756\n",
      "cnt: 0 - valLoss: 0.6661540865898132 - trainLoss: 0.6644722819328308\n",
      "cnt: 0 - valLoss: 0.6661539673805237 - trainLoss: 0.664472222328186\n",
      "cnt: 0 - valLoss: 0.6661537885665894 - trainLoss: 0.6644720435142517\n",
      "cnt: 0 - valLoss: 0.6661536693572998 - trainLoss: 0.6644719243049622\n",
      "cnt: 0 - valLoss: 0.6661534905433655 - trainLoss: 0.6644718050956726\n",
      "cnt: 0 - valLoss: 0.6661534309387207 - trainLoss: 0.6644717454910278\n",
      "cnt: 0 - valLoss: 0.6661533117294312 - trainLoss: 0.6644716262817383\n",
      "cnt: 0 - valLoss: 0.6661531925201416 - trainLoss: 0.6644715070724487\n",
      "cnt: 0 - valLoss: 0.6661530137062073 - trainLoss: 0.664471447467804\n",
      "cnt: 0 - valLoss: 0.6661528944969177 - trainLoss: 0.6644712686538696\n",
      "cnt: 0 - valLoss: 0.6661527752876282 - trainLoss: 0.6644711494445801\n",
      "cnt: 0 - valLoss: 0.6661526560783386 - trainLoss: 0.6644710898399353\n",
      "cnt: 0 - valLoss: 0.6661524772644043 - trainLoss: 0.6644709706306458\n",
      "cnt: 0 - valLoss: 0.6661523580551147 - trainLoss: 0.6644707918167114\n",
      "cnt: 0 - valLoss: 0.6661521792411804 - trainLoss: 0.6644707322120667\n",
      "cnt: 0 - valLoss: 0.6661521196365356 - trainLoss: 0.6644706130027771\n",
      "cnt: 0 - valLoss: 0.6661519408226013 - trainLoss: 0.6644704937934875\n",
      "cnt: 0 - valLoss: 0.6661518216133118 - trainLoss: 0.6644703149795532\n",
      "cnt: 0 - valLoss: 0.6661516427993774 - trainLoss: 0.6644702553749084\n",
      "cnt: 0 - valLoss: 0.6661515831947327 - trainLoss: 0.6644701361656189\n",
      "cnt: 0 - valLoss: 0.6661514043807983 - trainLoss: 0.6644700169563293\n",
      "cnt: 0 - valLoss: 0.6661512851715088 - trainLoss: 0.6644698977470398\n",
      "cnt: 0 - valLoss: 0.6661511659622192 - trainLoss: 0.6644697785377502\n",
      "cnt: 0 - valLoss: 0.6661510467529297 - trainLoss: 0.6644696593284607\n",
      "cnt: 0 - valLoss: 0.6661508679389954 - trainLoss: 0.6644695401191711\n",
      "cnt: 0 - valLoss: 0.6661507487297058 - trainLoss: 0.6644694805145264\n",
      "cnt: 0 - valLoss: 0.6661505699157715 - trainLoss: 0.664469301700592\n",
      "cnt: 0 - valLoss: 0.6661504507064819 - trainLoss: 0.664469301700592\n",
      "cnt: 0 - valLoss: 0.6661503911018372 - trainLoss: 0.6644691228866577\n",
      "cnt: 0 - valLoss: 0.6661502122879028 - trainLoss: 0.6644690036773682\n",
      "cnt: 0 - valLoss: 0.6661500334739685 - trainLoss: 0.6644688844680786\n",
      "cnt: 0 - valLoss: 0.666149914264679 - trainLoss: 0.6644688248634338\n",
      "cnt: 0 - valLoss: 0.6661498546600342 - trainLoss: 0.6644686460494995\n",
      "cnt: 0 - valLoss: 0.6661496758460999 - trainLoss: 0.66446852684021\n",
      "cnt: 0 - valLoss: 0.6661495566368103 - trainLoss: 0.6644684076309204\n",
      "cnt: 0 - valLoss: 0.666149377822876 - trainLoss: 0.6644683480262756\n",
      "cnt: 0 - valLoss: 0.6661492586135864 - trainLoss: 0.6644682884216309\n",
      "cnt: 0 - valLoss: 0.6661491394042969 - trainLoss: 0.6644681096076965\n",
      "cnt: 0 - valLoss: 0.6661490797996521 - trainLoss: 0.664467990398407\n",
      "cnt: 0 - valLoss: 0.6661489009857178 - trainLoss: 0.6644678711891174\n",
      "cnt: 0 - valLoss: 0.6661487817764282 - trainLoss: 0.6644677519798279\n",
      "cnt: 0 - valLoss: 0.6661486029624939 - trainLoss: 0.6644676327705383\n",
      "cnt: 0 - valLoss: 0.6661484837532043 - trainLoss: 0.6644675135612488\n",
      "cnt: 0 - valLoss: 0.66614830493927 - trainLoss: 0.6644673943519592\n",
      "cnt: 0 - valLoss: 0.6661482453346252 - trainLoss: 0.6644672751426697\n",
      "cnt: 0 - valLoss: 0.6661480665206909 - trainLoss: 0.6644672155380249\n",
      "cnt: 0 - valLoss: 0.6661479473114014 - trainLoss: 0.6644670367240906\n",
      "cnt: 0 - valLoss: 0.6661478281021118 - trainLoss: 0.6644669771194458\n",
      "cnt: 0 - valLoss: 0.6661476492881775 - trainLoss: 0.6644668579101562\n",
      "cnt: 0 - valLoss: 0.6661474704742432 - trainLoss: 0.6644667387008667\n",
      "cnt: 0 - valLoss: 0.6661474108695984 - trainLoss: 0.6644666194915771\n",
      "cnt: 0 - valLoss: 0.6661472916603088 - trainLoss: 0.6644665002822876\n",
      "cnt: 0 - valLoss: 0.6661471128463745 - trainLoss: 0.664466381072998\n",
      "cnt: 0 - valLoss: 0.666146993637085 - trainLoss: 0.6644662618637085\n",
      "cnt: 0 - valLoss: 0.6661468148231506 - trainLoss: 0.6644662022590637\n",
      "cnt: 0 - valLoss: 0.6661467552185059 - trainLoss: 0.6644660234451294\n",
      "cnt: 0 - valLoss: 0.6661465764045715 - trainLoss: 0.6644659638404846\n",
      "cnt: 0 - valLoss: 0.666146457195282 - trainLoss: 0.6644658446311951\n",
      "cnt: 0 - valLoss: 0.6661463379859924 - trainLoss: 0.6644656658172607\n",
      "cnt: 0 - valLoss: 0.6661461591720581 - trainLoss: 0.664465606212616\n",
      "cnt: 0 - valLoss: 0.6661460399627686 - trainLoss: 0.6644654870033264\n",
      "cnt: 0 - valLoss: 0.666145920753479 - trainLoss: 0.6644653677940369\n",
      "cnt: 0 - valLoss: 0.6661457419395447 - trainLoss: 0.6644651889801025\n",
      "cnt: 0 - valLoss: 0.6661456823348999 - trainLoss: 0.6644651293754578\n",
      "cnt: 0 - valLoss: 0.6661455035209656 - trainLoss: 0.6644650101661682\n",
      "cnt: 0 - valLoss: 0.6661453247070312 - trainLoss: 0.6644648909568787\n",
      "cnt: 0 - valLoss: 0.6661452054977417 - trainLoss: 0.6644647717475891\n",
      "cnt: 0 - valLoss: 0.6661450862884521 - trainLoss: 0.6644646525382996\n",
      "cnt: 0 - valLoss: 0.6661449670791626 - trainLoss: 0.6644645929336548\n",
      "cnt: 0 - valLoss: 0.6661447882652283 - trainLoss: 0.6644644737243652\n",
      "cnt: 0 - valLoss: 0.6661447286605835 - trainLoss: 0.6644643545150757\n",
      "cnt: 0 - valLoss: 0.6661445498466492 - trainLoss: 0.6644642353057861\n",
      "cnt: 0 - valLoss: 0.6661444306373596 - trainLoss: 0.6644640564918518\n",
      "cnt: 0 - valLoss: 0.6661443114280701 - trainLoss: 0.664463996887207\n",
      "cnt: 0 - valLoss: 0.6661441922187805 - trainLoss: 0.6644638776779175\n",
      "cnt: 0 - valLoss: 0.6661439538002014 - trainLoss: 0.6644638180732727\n",
      "cnt: 0 - valLoss: 0.6661438941955566 - trainLoss: 0.6644636392593384\n",
      "cnt: 0 - valLoss: 0.6661437153816223 - trainLoss: 0.6644635796546936\n",
      "cnt: 0 - valLoss: 0.6661435961723328 - trainLoss: 0.664463460445404\n",
      "cnt: 0 - valLoss: 0.6661434173583984 - trainLoss: 0.6644632816314697\n",
      "cnt: 0 - valLoss: 0.6661432981491089 - trainLoss: 0.6644631624221802\n",
      "cnt: 0 - valLoss: 0.6661431193351746 - trainLoss: 0.6644631028175354\n",
      "cnt: 0 - valLoss: 0.666143000125885 - trainLoss: 0.6644629240036011\n",
      "cnt: 0 - valLoss: 0.6661428213119507 - trainLoss: 0.6644628643989563\n",
      "cnt: 0 - valLoss: 0.6661427617073059 - trainLoss: 0.6644627451896667\n",
      "cnt: 0 - valLoss: 0.6661426424980164 - trainLoss: 0.6644626259803772\n",
      "cnt: 0 - valLoss: 0.6661425232887268 - trainLoss: 0.6644625067710876\n",
      "cnt: 0 - valLoss: 0.6661423444747925 - trainLoss: 0.6644623875617981\n",
      "cnt: 0 - valLoss: 0.6661422252655029 - trainLoss: 0.6644622683525085\n",
      "cnt: 0 - valLoss: 0.6661420464515686 - trainLoss: 0.6644622087478638\n",
      "cnt: 0 - valLoss: 0.666141927242279 - trainLoss: 0.6644620299339294\n",
      "cnt: 0 - valLoss: 0.6661418080329895 - trainLoss: 0.6644619107246399\n",
      "cnt: 0 - valLoss: 0.6661416292190552 - trainLoss: 0.6644618511199951\n",
      "cnt: 0 - valLoss: 0.6661415100097656 - trainLoss: 0.6644617319107056\n",
      "cnt: 0 - valLoss: 0.6661413908004761 - trainLoss: 0.6644615530967712\n",
      "cnt: 0 - valLoss: 0.6661412715911865 - trainLoss: 0.6644614934921265\n",
      "cnt: 0 - valLoss: 0.666141152381897 - trainLoss: 0.6644613742828369\n",
      "cnt: 0 - valLoss: 0.6661409735679626 - trainLoss: 0.6644612550735474\n",
      "cnt: 0 - valLoss: 0.6661408543586731 - trainLoss: 0.6644611358642578\n",
      "cnt: 0 - valLoss: 0.6661406755447388 - trainLoss: 0.6644610166549683\n",
      "cnt: 0 - valLoss: 0.6661404967308044 - trainLoss: 0.6644608974456787\n",
      "cnt: 0 - valLoss: 0.6661404371261597 - trainLoss: 0.6644607782363892\n",
      "cnt: 0 - valLoss: 0.6661402583122253 - trainLoss: 0.6644606590270996\n",
      "cnt: 0 - valLoss: 0.6661401391029358 - trainLoss: 0.6644605398178101\n",
      "cnt: 0 - valLoss: 0.6661400198936462 - trainLoss: 0.6644604206085205\n",
      "cnt: 0 - valLoss: 0.6661398410797119 - trainLoss: 0.664460301399231\n",
      "cnt: 0 - valLoss: 0.6661397814750671 - trainLoss: 0.6644602417945862\n",
      "cnt: 0 - valLoss: 0.6661396026611328 - trainLoss: 0.6644601225852966\n",
      "cnt: 0 - valLoss: 0.6661394238471985 - trainLoss: 0.6644600033760071\n",
      "cnt: 0 - valLoss: 0.6661393046379089 - trainLoss: 0.6644598841667175\n",
      "cnt: 0 - valLoss: 0.6661391258239746 - trainLoss: 0.664459764957428\n",
      "cnt: 0 - valLoss: 0.6661391258239746 - trainLoss: 0.6644596457481384\n",
      "cnt: 0 - valLoss: 0.6661388874053955 - trainLoss: 0.6644595861434937\n",
      "cnt: 0 - valLoss: 0.666138768196106 - trainLoss: 0.6644594073295593\n",
      "cnt: 0 - valLoss: 0.6661386489868164 - trainLoss: 0.6644592881202698\n",
      "cnt: 0 - valLoss: 0.6661384701728821 - trainLoss: 0.6644591689109802\n",
      "cnt: 0 - valLoss: 0.6661383509635925 - trainLoss: 0.6644590497016907\n",
      "cnt: 0 - valLoss: 0.666138231754303 - trainLoss: 0.6644589900970459\n",
      "cnt: 0 - valLoss: 0.6661380529403687 - trainLoss: 0.6644588708877563\n",
      "cnt: 0 - valLoss: 0.6661379337310791 - trainLoss: 0.6644587516784668\n",
      "cnt: 0 - valLoss: 0.6661378145217896 - trainLoss: 0.6644585728645325\n",
      "cnt: 0 - valLoss: 0.6661376357078552 - trainLoss: 0.6644585132598877\n",
      "cnt: 0 - valLoss: 0.6661375164985657 - trainLoss: 0.6644583940505981\n",
      "cnt: 0 - valLoss: 0.6661373972892761 - trainLoss: 0.6644582152366638\n",
      "cnt: 0 - valLoss: 0.6661372780799866 - trainLoss: 0.664458155632019\n",
      "cnt: 0 - valLoss: 0.6661370992660522 - trainLoss: 0.6644580960273743\n",
      "cnt: 0 - valLoss: 0.6661369800567627 - trainLoss: 0.6644579172134399\n",
      "cnt: 0 - valLoss: 0.6661368012428284 - trainLoss: 0.6644577980041504\n",
      "cnt: 0 - valLoss: 0.6661366820335388 - trainLoss: 0.6644577383995056\n",
      "cnt: 0 - valLoss: 0.6661365628242493 - trainLoss: 0.6644576191902161\n",
      "cnt: 0 - valLoss: 0.6661363840103149 - trainLoss: 0.6644574999809265\n",
      "cnt: 0 - valLoss: 0.6661362648010254 - trainLoss: 0.664457380771637\n",
      "cnt: 0 - valLoss: 0.6661361455917358 - trainLoss: 0.6644572615623474\n",
      "cnt: 0 - valLoss: 0.6661360263824463 - trainLoss: 0.6644571423530579\n",
      "cnt: 0 - valLoss: 0.666135847568512 - trainLoss: 0.6644570231437683\n",
      "cnt: 0 - valLoss: 0.6661357283592224 - trainLoss: 0.6644569039344788\n",
      "cnt: 0 - valLoss: 0.6661356091499329 - trainLoss: 0.6644567847251892\n",
      "cnt: 0 - valLoss: 0.6661354899406433 - trainLoss: 0.6644566655158997\n",
      "cnt: 0 - valLoss: 0.666135311126709 - trainLoss: 0.6644565463066101\n",
      "cnt: 0 - valLoss: 0.6661351919174194 - trainLoss: 0.6644564270973206\n",
      "cnt: 0 - valLoss: 0.6661350131034851 - trainLoss: 0.6644562482833862\n",
      "cnt: 0 - valLoss: 0.6661349534988403 - trainLoss: 0.6644561886787415\n",
      "cnt: 0 - valLoss: 0.6661347150802612 - trainLoss: 0.6644561290740967\n",
      "cnt: 0 - valLoss: 0.6661345958709717 - trainLoss: 0.6644559502601624\n",
      "cnt: 0 - valLoss: 0.6661345362663269 - trainLoss: 0.6644558906555176\n",
      "cnt: 0 - valLoss: 0.6661343574523926 - trainLoss: 0.664455771446228\n",
      "cnt: 0 - valLoss: 0.6661341786384583 - trainLoss: 0.6644556522369385\n",
      "cnt: 0 - valLoss: 0.6661341190338135 - trainLoss: 0.6644555330276489\n",
      "cnt: 0 - valLoss: 0.6661338806152344 - trainLoss: 0.6644554138183594\n",
      "cnt: 0 - valLoss: 0.6661338210105896 - trainLoss: 0.664455235004425\n",
      "cnt: 0 - valLoss: 0.6661336421966553 - trainLoss: 0.6644551753997803\n",
      "cnt: 0 - valLoss: 0.6661335229873657 - trainLoss: 0.6644550561904907\n",
      "cnt: 0 - valLoss: 0.6661334037780762 - trainLoss: 0.6644548773765564\n",
      "cnt: 0 - valLoss: 0.6661332249641418 - trainLoss: 0.6644548773765564\n",
      "cnt: 0 - valLoss: 0.6661330461502075 - trainLoss: 0.6644546985626221\n",
      "cnt: 0 - valLoss: 0.6661329865455627 - trainLoss: 0.6644546389579773\n",
      "cnt: 0 - valLoss: 0.6661328673362732 - trainLoss: 0.664454460144043\n",
      "cnt: 0 - valLoss: 0.6661327481269836 - trainLoss: 0.6644543409347534\n",
      "cnt: 0 - valLoss: 0.6661325693130493 - trainLoss: 0.6644542217254639\n",
      "cnt: 0 - valLoss: 0.6661324501037598 - trainLoss: 0.6644541621208191\n",
      "cnt: 0 - valLoss: 0.6661322712898254 - trainLoss: 0.6644540429115295\n",
      "cnt: 0 - valLoss: 0.6661321520805359 - trainLoss: 0.6644538640975952\n",
      "cnt: 0 - valLoss: 0.6661320328712463 - trainLoss: 0.6644538640975952\n",
      "cnt: 0 - valLoss: 0.666131854057312 - trainLoss: 0.6644536852836609\n",
      "cnt: 0 - valLoss: 0.6661316752433777 - trainLoss: 0.6644535064697266\n",
      "cnt: 0 - valLoss: 0.6661316156387329 - trainLoss: 0.664453387260437\n",
      "cnt: 0 - valLoss: 0.6661314368247986 - trainLoss: 0.664453387260437\n",
      "cnt: 0 - valLoss: 0.666131317615509 - trainLoss: 0.6644532084465027\n",
      "cnt: 0 - valLoss: 0.6661311388015747 - trainLoss: 0.6644530892372131\n",
      "cnt: 0 - valLoss: 0.6661310791969299 - trainLoss: 0.6644530296325684\n",
      "cnt: 0 - valLoss: 0.6661309003829956 - trainLoss: 0.6644529104232788\n",
      "cnt: 0 - valLoss: 0.666130781173706 - trainLoss: 0.6644527912139893\n",
      "cnt: 0 - valLoss: 0.6661306023597717 - trainLoss: 0.6644526720046997\n",
      "cnt: 0 - valLoss: 0.6661304235458374 - trainLoss: 0.6644524931907654\n",
      "cnt: 0 - valLoss: 0.6661303043365479 - trainLoss: 0.6644524335861206\n",
      "cnt: 0 - valLoss: 0.6661301255226135 - trainLoss: 0.6644522547721863\n",
      "cnt: 0 - valLoss: 0.6661300659179688 - trainLoss: 0.6644521951675415\n",
      "cnt: 0 - valLoss: 0.6661298871040344 - trainLoss: 0.664452075958252\n",
      "cnt: 0 - valLoss: 0.6661298274993896 - trainLoss: 0.6644518971443176\n",
      "cnt: 0 - valLoss: 0.6661296486854553 - trainLoss: 0.6644518375396729\n",
      "cnt: 0 - valLoss: 0.666129469871521 - trainLoss: 0.6644517183303833\n",
      "cnt: 0 - valLoss: 0.6661292910575867 - trainLoss: 0.6644515991210938\n",
      "cnt: 0 - valLoss: 0.6661292314529419 - trainLoss: 0.6644514799118042\n",
      "cnt: 0 - valLoss: 0.6661291122436523 - trainLoss: 0.6644513010978699\n",
      "cnt: 0 - valLoss: 0.666128933429718 - trainLoss: 0.6644512414932251\n",
      "cnt: 0 - valLoss: 0.6661287546157837 - trainLoss: 0.6644511222839355\n",
      "cnt: 0 - valLoss: 0.6661286950111389 - trainLoss: 0.6644510626792908\n",
      "cnt: 0 - valLoss: 0.6661284565925598 - trainLoss: 0.6644509434700012\n",
      "cnt: 0 - valLoss: 0.666128396987915 - trainLoss: 0.6644508242607117\n",
      "cnt: 0 - valLoss: 0.6661282777786255 - trainLoss: 0.6644506454467773\n",
      "cnt: 0 - valLoss: 0.6661280989646912 - trainLoss: 0.6644505262374878\n",
      "cnt: 0 - valLoss: 0.6661279201507568 - trainLoss: 0.664450466632843\n",
      "cnt: 0 - valLoss: 0.6661278605461121 - trainLoss: 0.6644503474235535\n",
      "cnt: 0 - valLoss: 0.6661276817321777 - trainLoss: 0.6644501686096191\n",
      "cnt: 0 - valLoss: 0.6661275625228882 - trainLoss: 0.6644501090049744\n",
      "cnt: 0 - valLoss: 0.6661273837089539 - trainLoss: 0.6644499897956848\n",
      "cnt: 0 - valLoss: 0.6661272644996643 - trainLoss: 0.6644498705863953\n",
      "cnt: 0 - valLoss: 0.6661271452903748 - trainLoss: 0.6644497513771057\n",
      "cnt: 0 - valLoss: 0.6661269664764404 - trainLoss: 0.6644496321678162\n",
      "cnt: 0 - valLoss: 0.6661268472671509 - trainLoss: 0.6644495129585266\n",
      "cnt: 0 - valLoss: 0.6661267280578613 - trainLoss: 0.6644494533538818\n",
      "cnt: 0 - valLoss: 0.6661266088485718 - trainLoss: 0.6644492745399475\n",
      "cnt: 0 - valLoss: 0.6661264896392822 - trainLoss: 0.664449155330658\n",
      "cnt: 0 - valLoss: 0.6661263108253479 - trainLoss: 0.6644490957260132\n",
      "cnt: 0 - valLoss: 0.6661261916160583 - trainLoss: 0.6644489169120789\n",
      "cnt: 0 - valLoss: 0.666126012802124 - trainLoss: 0.6644488573074341\n",
      "cnt: 0 - valLoss: 0.6661258935928345 - trainLoss: 0.6644487380981445\n",
      "cnt: 0 - valLoss: 0.6661257743835449 - trainLoss: 0.6644485592842102\n",
      "cnt: 0 - valLoss: 0.6661255955696106 - trainLoss: 0.6644484996795654\n",
      "cnt: 0 - valLoss: 0.6661254167556763 - trainLoss: 0.6644483804702759\n",
      "cnt: 0 - valLoss: 0.6661252379417419 - trainLoss: 0.6644482016563416\n",
      "cnt: 0 - valLoss: 0.6661251783370972 - trainLoss: 0.664448082447052\n",
      "cnt: 0 - valLoss: 0.6661250591278076 - trainLoss: 0.6644480228424072\n",
      "cnt: 0 - valLoss: 0.6661249399185181 - trainLoss: 0.6644479036331177\n",
      "cnt: 0 - valLoss: 0.6661247611045837 - trainLoss: 0.6644477844238281\n",
      "cnt: 0 - valLoss: 0.6661246418952942 - trainLoss: 0.6644476056098938\n",
      "cnt: 0 - valLoss: 0.6661245226860046 - trainLoss: 0.664447546005249\n",
      "cnt: 0 - valLoss: 0.6661243438720703 - trainLoss: 0.6644474267959595\n",
      "cnt: 0 - valLoss: 0.6661242246627808 - trainLoss: 0.6644473075866699\n",
      "cnt: 0 - valLoss: 0.6661241054534912 - trainLoss: 0.6644471883773804\n",
      "cnt: 0 - valLoss: 0.6661238670349121 - trainLoss: 0.6644470691680908\n",
      "cnt: 0 - valLoss: 0.6661238074302673 - trainLoss: 0.6644469499588013\n",
      "cnt: 0 - valLoss: 0.666123628616333 - trainLoss: 0.6644468307495117\n",
      "cnt: 0 - valLoss: 0.6661234498023987 - trainLoss: 0.6644467115402222\n",
      "cnt: 0 - valLoss: 0.6661233901977539 - trainLoss: 0.6644465923309326\n",
      "cnt: 0 - valLoss: 0.6661232113838196 - trainLoss: 0.6644465327262878\n",
      "cnt: 0 - valLoss: 0.66612309217453 - trainLoss: 0.6644464135169983\n",
      "cnt: 0 - valLoss: 0.6661229729652405 - trainLoss: 0.664446234703064\n",
      "cnt: 0 - valLoss: 0.6661227941513062 - trainLoss: 0.6644461154937744\n",
      "cnt: 0 - valLoss: 0.6661226749420166 - trainLoss: 0.6644460558891296\n",
      "cnt: 0 - valLoss: 0.6661224961280823 - trainLoss: 0.6644459366798401\n",
      "cnt: 0 - valLoss: 0.6661224365234375 - trainLoss: 0.6644457578659058\n",
      "cnt: 0 - valLoss: 0.6661222577095032 - trainLoss: 0.6644456386566162\n",
      "cnt: 0 - valLoss: 0.6661221385002136 - trainLoss: 0.6644455790519714\n",
      "cnt: 0 - valLoss: 0.6661219596862793 - trainLoss: 0.6644454002380371\n",
      "cnt: 0 - valLoss: 0.6661218404769897 - trainLoss: 0.6644453406333923\n",
      "cnt: 0 - valLoss: 0.6661216616630554 - trainLoss: 0.664445161819458\n",
      "cnt: 0 - valLoss: 0.6661216020584106 - trainLoss: 0.664445161819458\n",
      "cnt: 0 - valLoss: 0.6661214232444763 - trainLoss: 0.6644449830055237\n",
      "cnt: 0 - valLoss: 0.6661213040351868 - trainLoss: 0.6644448637962341\n",
      "cnt: 0 - valLoss: 0.6661211252212524 - trainLoss: 0.6644447445869446\n",
      "cnt: 0 - valLoss: 0.6661210060119629 - trainLoss: 0.664444625377655\n",
      "cnt: 0 - valLoss: 0.6661208271980286 - trainLoss: 0.6644445657730103\n",
      "cnt: 0 - valLoss: 0.666120707988739 - trainLoss: 0.6644443869590759\n",
      "cnt: 0 - valLoss: 0.6661206483840942 - trainLoss: 0.6644442677497864\n",
      "cnt: 0 - valLoss: 0.6661204695701599 - trainLoss: 0.6644441485404968\n",
      "cnt: 0 - valLoss: 0.6661202907562256 - trainLoss: 0.6644440293312073\n",
      "cnt: 0 - valLoss: 0.6661202311515808 - trainLoss: 0.664443850517273\n",
      "cnt: 0 - valLoss: 0.6661199927330017 - trainLoss: 0.6644437909126282\n",
      "cnt: 0 - valLoss: 0.6661199331283569 - trainLoss: 0.6644436717033386\n",
      "cnt: 0 - valLoss: 0.6661197543144226 - trainLoss: 0.6644436120986938\n",
      "cnt: 0 - valLoss: 0.6661196351051331 - trainLoss: 0.6644434928894043\n",
      "cnt: 0 - valLoss: 0.6661194562911987 - trainLoss: 0.6644433736801147\n",
      "cnt: 0 - valLoss: 0.6661193370819092 - trainLoss: 0.6644431948661804\n",
      "cnt: 0 - valLoss: 0.6661192178726196 - trainLoss: 0.6644431948661804\n",
      "cnt: 0 - valLoss: 0.6661190390586853 - trainLoss: 0.6644430160522461\n",
      "cnt: 0 - valLoss: 0.6661189198493958 - trainLoss: 0.6644428968429565\n",
      "cnt: 0 - valLoss: 0.6661188006401062 - trainLoss: 0.664442777633667\n",
      "cnt: 0 - valLoss: 0.6661186218261719 - trainLoss: 0.6644426584243774\n",
      "cnt: 0 - valLoss: 0.6661185026168823 - trainLoss: 0.6644425392150879\n",
      "cnt: 0 - valLoss: 0.666118323802948 - trainLoss: 0.6644424200057983\n",
      "cnt: 0 - valLoss: 0.6661182045936584 - trainLoss: 0.6644423007965088\n",
      "cnt: 0 - valLoss: 0.6661180853843689 - trainLoss: 0.6644421815872192\n",
      "cnt: 0 - valLoss: 0.6661179661750793 - trainLoss: 0.6644420623779297\n",
      "cnt: 0 - valLoss: 0.666117787361145 - trainLoss: 0.6644419431686401\n",
      "cnt: 0 - valLoss: 0.6661176681518555 - trainLoss: 0.6644418239593506\n",
      "cnt: 0 - valLoss: 0.6661175489425659 - trainLoss: 0.664441704750061\n",
      "cnt: 0 - valLoss: 0.6661173701286316 - trainLoss: 0.6644415855407715\n",
      "cnt: 0 - valLoss: 0.6661171913146973 - trainLoss: 0.6644415259361267\n",
      "cnt: 0 - valLoss: 0.6661170721054077 - trainLoss: 0.6644414067268372\n",
      "cnt: 0 - valLoss: 0.6661170125007629 - trainLoss: 0.6644412279129028\n",
      "cnt: 0 - valLoss: 0.6661168336868286 - trainLoss: 0.6644411087036133\n",
      "cnt: 0 - valLoss: 0.6661167144775391 - trainLoss: 0.6644410490989685\n",
      "cnt: 0 - valLoss: 0.6661165356636047 - trainLoss: 0.664440929889679\n",
      "cnt: 0 - valLoss: 0.6661164164543152 - trainLoss: 0.6644408106803894\n",
      "cnt: 0 - valLoss: 0.6661162376403809 - trainLoss: 0.6644406914710999\n",
      "cnt: 0 - valLoss: 0.6661161184310913 - trainLoss: 0.6644405126571655\n",
      "cnt: 0 - valLoss: 0.6661159992218018 - trainLoss: 0.6644404530525208\n",
      "cnt: 0 - valLoss: 0.6661158204078674 - trainLoss: 0.6644403338432312\n",
      "cnt: 0 - valLoss: 0.6661157011985779 - trainLoss: 0.6644401550292969\n",
      "cnt: 0 - valLoss: 0.6661155223846436 - trainLoss: 0.6644400954246521\n",
      "cnt: 0 - valLoss: 0.666115403175354 - trainLoss: 0.6644399762153625\n",
      "cnt: 0 - valLoss: 0.6661152839660645 - trainLoss: 0.664439857006073\n",
      "cnt: 0 - valLoss: 0.6661151647567749 - trainLoss: 0.6644397377967834\n",
      "cnt: 0 - valLoss: 0.6661150455474854 - trainLoss: 0.6644395589828491\n",
      "cnt: 0 - valLoss: 0.666114866733551 - trainLoss: 0.6644394397735596\n",
      "cnt: 0 - valLoss: 0.6661147475242615 - trainLoss: 0.6644394397735596\n",
      "cnt: 0 - valLoss: 0.6661145687103271 - trainLoss: 0.6644392609596252\n",
      "cnt: 0 - valLoss: 0.6661144495010376 - trainLoss: 0.6644391417503357\n",
      "cnt: 0 - valLoss: 0.666114330291748 - trainLoss: 0.6644390225410461\n",
      "cnt: 0 - valLoss: 0.6661142110824585 - trainLoss: 0.6644389033317566\n",
      "cnt: 0 - valLoss: 0.6661140322685242 - trainLoss: 0.664438784122467\n",
      "cnt: 0 - valLoss: 0.6661139130592346 - trainLoss: 0.6644386649131775\n",
      "cnt: 0 - valLoss: 0.6661137938499451 - trainLoss: 0.6644385457038879\n",
      "cnt: 0 - valLoss: 0.666113555431366 - trainLoss: 0.6644384860992432\n",
      "cnt: 0 - valLoss: 0.6661134958267212 - trainLoss: 0.6644383072853088\n",
      "cnt: 0 - valLoss: 0.6661133766174316 - trainLoss: 0.6644381880760193\n",
      "cnt: 0 - valLoss: 0.6661131381988525 - trainLoss: 0.6644380688667297\n",
      "cnt: 0 - valLoss: 0.666113018989563 - trainLoss: 0.6644379496574402\n",
      "cnt: 0 - valLoss: 0.6661128997802734 - trainLoss: 0.6644378900527954\n",
      "cnt: 0 - valLoss: 0.6661127209663391 - trainLoss: 0.6644377708435059\n",
      "cnt: 0 - valLoss: 0.6661126613616943 - trainLoss: 0.6644376516342163\n",
      "cnt: 0 - valLoss: 0.66611248254776 - trainLoss: 0.664437472820282\n",
      "cnt: 0 - valLoss: 0.6661123633384705 - trainLoss: 0.6644374132156372\n",
      "cnt: 0 - valLoss: 0.6661121249198914 - trainLoss: 0.6644372940063477\n",
      "cnt: 0 - valLoss: 0.6661120057106018 - trainLoss: 0.6644371151924133\n",
      "cnt: 0 - valLoss: 0.666111946105957 - trainLoss: 0.6644369959831238\n",
      "cnt: 0 - valLoss: 0.6661117672920227 - trainLoss: 0.6644368767738342\n",
      "cnt: 0 - valLoss: 0.6661116480827332 - trainLoss: 0.6644368171691895\n",
      "cnt: 0 - valLoss: 0.6661115288734436 - trainLoss: 0.6644366979598999\n",
      "cnt: 0 - valLoss: 0.6661113500595093 - trainLoss: 0.6644365787506104\n",
      "cnt: 0 - valLoss: 0.6661112308502197 - trainLoss: 0.664436399936676\n",
      "cnt: 0 - valLoss: 0.6661110520362854 - trainLoss: 0.6644363403320312\n",
      "cnt: 0 - valLoss: 0.6661109328269958 - trainLoss: 0.6644362211227417\n",
      "cnt: 0 - valLoss: 0.6661107540130615 - trainLoss: 0.6644361019134521\n",
      "cnt: 0 - valLoss: 0.666110634803772 - trainLoss: 0.6644359827041626\n",
      "cnt: 0 - valLoss: 0.6661105155944824 - trainLoss: 0.6644359230995178\n",
      "cnt: 0 - valLoss: 0.6661103367805481 - trainLoss: 0.6644357442855835\n",
      "cnt: 0 - valLoss: 0.6661102771759033 - trainLoss: 0.664435625076294\n",
      "cnt: 0 - valLoss: 0.666110098361969 - trainLoss: 0.6644355058670044\n",
      "cnt: 0 - valLoss: 0.6661099791526794 - trainLoss: 0.6644353866577148\n",
      "cnt: 0 - valLoss: 0.6661098599433899 - trainLoss: 0.6644353270530701\n",
      "cnt: 0 - valLoss: 0.6661096811294556 - trainLoss: 0.6644351482391357\n",
      "cnt: 0 - valLoss: 0.666109561920166 - trainLoss: 0.6644349694252014\n",
      "cnt: 0 - valLoss: 0.6661093831062317 - trainLoss: 0.6644349098205566\n",
      "cnt: 0 - valLoss: 0.6661092638969421 - trainLoss: 0.6644347906112671\n",
      "cnt: 0 - valLoss: 0.6661090850830078 - trainLoss: 0.6644347310066223\n",
      "cnt: 0 - valLoss: 0.6661089658737183 - trainLoss: 0.664434552192688\n",
      "cnt: 0 - valLoss: 0.6661087870597839 - trainLoss: 0.6644344329833984\n",
      "cnt: 0 - valLoss: 0.6661086678504944 - trainLoss: 0.6644342541694641\n",
      "cnt: 0 - valLoss: 0.6661085486412048 - trainLoss: 0.6644341945648193\n",
      "cnt: 0 - valLoss: 0.6661083698272705 - trainLoss: 0.6644340753555298\n",
      "cnt: 0 - valLoss: 0.6661083102226257 - trainLoss: 0.664434015750885\n",
      "cnt: 0 - valLoss: 0.6661081314086914 - trainLoss: 0.6644338369369507\n",
      "cnt: 0 - valLoss: 0.6661080121994019 - trainLoss: 0.6644337773323059\n",
      "cnt: 0 - valLoss: 0.6661078929901123 - trainLoss: 0.6644335985183716\n",
      "cnt: 0 - valLoss: 0.666107714176178 - trainLoss: 0.664433479309082\n",
      "cnt: 0 - valLoss: 0.6661075949668884 - trainLoss: 0.6644333600997925\n",
      "cnt: 0 - valLoss: 0.6661074161529541 - trainLoss: 0.6644333004951477\n",
      "cnt: 0 - valLoss: 0.6661072373390198 - trainLoss: 0.6644331812858582\n",
      "cnt: 0 - valLoss: 0.6661071181297302 - trainLoss: 0.6644330620765686\n",
      "cnt: 0 - valLoss: 0.6661069989204407 - trainLoss: 0.6644328832626343\n",
      "cnt: 0 - valLoss: 0.6661069393157959 - trainLoss: 0.6644327640533447\n",
      "cnt: 0 - valLoss: 0.6661067008972168 - trainLoss: 0.6644327044487\n",
      "cnt: 0 - valLoss: 0.666106641292572 - trainLoss: 0.6644325256347656\n",
      "cnt: 0 - valLoss: 0.6661064624786377 - trainLoss: 0.6644324064254761\n",
      "cnt: 0 - valLoss: 0.6661062240600586 - trainLoss: 0.6644322872161865\n",
      "cnt: 0 - valLoss: 0.6661061644554138 - trainLoss: 0.664432168006897\n",
      "cnt: 0 - valLoss: 0.6661059856414795 - trainLoss: 0.6644320487976074\n",
      "cnt: 0 - valLoss: 0.6661058664321899 - trainLoss: 0.6644319891929626\n",
      "cnt: 0 - valLoss: 0.6661057472229004 - trainLoss: 0.6644318103790283\n",
      "cnt: 0 - valLoss: 0.6661055684089661 - trainLoss: 0.6644317507743835\n",
      "cnt: 0 - valLoss: 0.6661053895950317 - trainLoss: 0.664431631565094\n",
      "cnt: 0 - valLoss: 0.6661052703857422 - trainLoss: 0.6644315123558044\n",
      "cnt: 0 - valLoss: 0.6661051511764526 - trainLoss: 0.6644313931465149\n",
      "cnt: 0 - valLoss: 0.6661050319671631 - trainLoss: 0.6644312143325806\n",
      "cnt: 0 - valLoss: 0.6661049127578735 - trainLoss: 0.664431095123291\n",
      "cnt: 0 - valLoss: 0.6661047339439392 - trainLoss: 0.6644310355186462\n",
      "cnt: 0 - valLoss: 0.6661045551300049 - trainLoss: 0.6644308567047119\n",
      "cnt: 0 - valLoss: 0.6661044359207153 - trainLoss: 0.6644307374954224\n",
      "cnt: 0 - valLoss: 0.6661043763160706 - trainLoss: 0.6644306182861328\n",
      "cnt: 0 - valLoss: 0.6661041975021362 - trainLoss: 0.6644304990768433\n",
      "cnt: 0 - valLoss: 0.6661040186882019 - trainLoss: 0.6644304394721985\n",
      "cnt: 0 - valLoss: 0.6661038994789124 - trainLoss: 0.6644303202629089\n",
      "cnt: 0 - valLoss: 0.666103720664978 - trainLoss: 0.6644301414489746\n",
      "cnt: 0 - valLoss: 0.6661036014556885 - trainLoss: 0.6644300818443298\n",
      "cnt: 0 - valLoss: 0.6661034822463989 - trainLoss: 0.6644299030303955\n",
      "cnt: 0 - valLoss: 0.6661033630371094 - trainLoss: 0.6644298434257507\n",
      "cnt: 0 - valLoss: 0.666103184223175 - trainLoss: 0.6644297242164612\n",
      "cnt: 0 - valLoss: 0.6661030650138855 - trainLoss: 0.6644295454025269\n",
      "cnt: 0 - valLoss: 0.666102945804596 - trainLoss: 0.6644294261932373\n",
      "cnt: 0 - valLoss: 0.6661028265953064 - trainLoss: 0.6644293069839478\n",
      "cnt: 0 - valLoss: 0.6661026477813721 - trainLoss: 0.664429247379303\n",
      "cnt: 0 - valLoss: 0.6661024689674377 - trainLoss: 0.6644290685653687\n",
      "cnt: 0 - valLoss: 0.6661023497581482 - trainLoss: 0.6644289493560791\n",
      "cnt: 0 - valLoss: 0.6661022305488586 - trainLoss: 0.6644288897514343\n",
      "cnt: 0 - valLoss: 0.6661020517349243 - trainLoss: 0.6644287705421448\n",
      "cnt: 0 - valLoss: 0.6661019325256348 - trainLoss: 0.6644286513328552\n",
      "cnt: 0 - valLoss: 0.6661017537117004 - trainLoss: 0.6644285321235657\n",
      "cnt: 0 - valLoss: 0.6661016345024109 - trainLoss: 0.6644283533096313\n",
      "cnt: 0 - valLoss: 0.6661014556884766 - trainLoss: 0.6644282341003418\n",
      "cnt: 0 - valLoss: 0.666101336479187 - trainLoss: 0.664428174495697\n",
      "cnt: 0 - valLoss: 0.6661011576652527 - trainLoss: 0.6644280552864075\n",
      "cnt: 0 - valLoss: 0.6661010980606079 - trainLoss: 0.6644278764724731\n",
      "cnt: 0 - valLoss: 0.6661009788513184 - trainLoss: 0.6644278168678284\n",
      "cnt: 0 - valLoss: 0.666100800037384 - trainLoss: 0.6644276976585388\n",
      "cnt: 0 - valLoss: 0.6661006808280945 - trainLoss: 0.6644275784492493\n",
      "cnt: 0 - valLoss: 0.6661005616188049 - trainLoss: 0.6644273996353149\n",
      "cnt: 0 - valLoss: 0.6661003828048706 - trainLoss: 0.6644273400306702\n",
      "cnt: 0 - valLoss: 0.666100263595581 - trainLoss: 0.6644272208213806\n",
      "cnt: 0 - valLoss: 0.6661000847816467 - trainLoss: 0.6644271016120911\n",
      "cnt: 0 - valLoss: 0.6660999059677124 - trainLoss: 0.6644269824028015\n",
      "cnt: 0 - valLoss: 0.6660997867584229 - trainLoss: 0.664426863193512\n",
      "cnt: 0 - valLoss: 0.6660996079444885 - trainLoss: 0.6644267439842224\n",
      "cnt: 0 - valLoss: 0.666099488735199 - trainLoss: 0.6644265651702881\n",
      "cnt: 0 - valLoss: 0.6660993695259094 - trainLoss: 0.6644265055656433\n",
      "cnt: 0 - valLoss: 0.6660991907119751 - trainLoss: 0.6644263863563538\n",
      "cnt: 0 - valLoss: 0.6660990715026855 - trainLoss: 0.6644262075424194\n",
      "cnt: 0 - valLoss: 0.666098952293396 - trainLoss: 0.6644261479377747\n",
      "cnt: 0 - valLoss: 0.6660987734794617 - trainLoss: 0.6644260287284851\n",
      "cnt: 0 - valLoss: 0.6660987138748169 - trainLoss: 0.6644259095191956\n",
      "cnt: 0 - valLoss: 0.6660985350608826 - trainLoss: 0.664425790309906\n",
      "cnt: 0 - valLoss: 0.6660983562469482 - trainLoss: 0.6644256114959717\n",
      "cnt: 0 - valLoss: 0.6660982370376587 - trainLoss: 0.6644255518913269\n",
      "cnt: 0 - valLoss: 0.6660981178283691 - trainLoss: 0.6644254326820374\n",
      "cnt: 0 - valLoss: 0.6660979390144348 - trainLoss: 0.6644253134727478\n",
      "cnt: 0 - valLoss: 0.6660978198051453 - trainLoss: 0.6644251942634583\n",
      "cnt: 0 - valLoss: 0.6660976409912109 - trainLoss: 0.6644251346588135\n",
      "cnt: 0 - valLoss: 0.6660975217819214 - trainLoss: 0.6644249558448792\n",
      "cnt: 0 - valLoss: 0.6660974025726318 - trainLoss: 0.6644248366355896\n",
      "cnt: 0 - valLoss: 0.6660971641540527 - trainLoss: 0.6644247174263\n",
      "cnt: 0 - valLoss: 0.6660971641540527 - trainLoss: 0.6644245982170105\n",
      "cnt: 0 - valLoss: 0.6660969853401184 - trainLoss: 0.664424479007721\n",
      "cnt: 0 - valLoss: 0.6660968065261841 - trainLoss: 0.6644243597984314\n",
      "cnt: 0 - valLoss: 0.6660967469215393 - trainLoss: 0.6644242405891418\n",
      "cnt: 0 - valLoss: 0.666096568107605 - trainLoss: 0.6644240617752075\n",
      "cnt: 0 - valLoss: 0.6660963892936707 - trainLoss: 0.6644240021705627\n",
      "cnt: 0 - valLoss: 0.6660962700843811 - trainLoss: 0.6644238829612732\n",
      "cnt: 0 - valLoss: 0.6660960912704468 - trainLoss: 0.6644237637519836\n",
      "cnt: 0 - valLoss: 0.6660959720611572 - trainLoss: 0.6644236445426941\n",
      "cnt: 0 - valLoss: 0.6660957932472229 - trainLoss: 0.6644235253334045\n",
      "cnt: 0 - valLoss: 0.6660956740379333 - trainLoss: 0.664423406124115\n",
      "cnt: 0 - valLoss: 0.6660955548286438 - trainLoss: 0.6644232869148254\n",
      "cnt: 0 - valLoss: 0.6660953760147095 - trainLoss: 0.6644231677055359\n",
      "cnt: 0 - valLoss: 0.6660952568054199 - trainLoss: 0.6644230484962463\n",
      "cnt: 0 - valLoss: 0.6660951375961304 - trainLoss: 0.664422869682312\n",
      "cnt: 0 - valLoss: 0.6660950183868408 - trainLoss: 0.6644228100776672\n",
      "cnt: 0 - valLoss: 0.6660948395729065 - trainLoss: 0.6644226908683777\n",
      "cnt: 0 - valLoss: 0.6660947203636169 - trainLoss: 0.6644225716590881\n",
      "cnt: 0 - valLoss: 0.6660946011543274 - trainLoss: 0.6644223928451538\n",
      "cnt: 0 - valLoss: 0.6660944223403931 - trainLoss: 0.6644222736358643\n",
      "cnt: 0 - valLoss: 0.6660943031311035 - trainLoss: 0.6644221544265747\n",
      "cnt: 0 - valLoss: 0.6660941243171692 - trainLoss: 0.6644220352172852\n",
      "cnt: 0 - valLoss: 0.6660940051078796 - trainLoss: 0.6644219160079956\n",
      "cnt: 0 - valLoss: 0.6660938262939453 - trainLoss: 0.664421796798706\n",
      "cnt: 0 - valLoss: 0.6660937070846558 - trainLoss: 0.6644217371940613\n",
      "cnt: 0 - valLoss: 0.6660935282707214 - trainLoss: 0.6644216179847717\n",
      "cnt: 0 - valLoss: 0.6660934090614319 - trainLoss: 0.6644214987754822\n",
      "cnt: 0 - valLoss: 0.6660932898521423 - trainLoss: 0.6644213199615479\n",
      "cnt: 0 - valLoss: 0.666093111038208 - trainLoss: 0.6644212007522583\n",
      "cnt: 0 - valLoss: 0.6660929322242737 - trainLoss: 0.6644210815429688\n",
      "cnt: 0 - valLoss: 0.6660928726196289 - trainLoss: 0.6644209623336792\n",
      "cnt: 0 - valLoss: 0.6660926938056946 - trainLoss: 0.6644208431243896\n",
      "cnt: 0 - valLoss: 0.666092574596405 - trainLoss: 0.6644207239151001\n",
      "cnt: 0 - valLoss: 0.6660924553871155 - trainLoss: 0.6644206047058105\n",
      "cnt: 0 - valLoss: 0.6660922765731812 - trainLoss: 0.6644205451011658\n",
      "cnt: 0 - valLoss: 0.6660921573638916 - trainLoss: 0.6644204258918762\n",
      "cnt: 0 - valLoss: 0.6660919785499573 - trainLoss: 0.6644202470779419\n",
      "cnt: 0 - valLoss: 0.6660918593406677 - trainLoss: 0.6644201278686523\n",
      "cnt: 0 - valLoss: 0.6660916805267334 - trainLoss: 0.6644200086593628\n",
      "cnt: 0 - valLoss: 0.6660915613174438 - trainLoss: 0.6644198894500732\n",
      "cnt: 0 - valLoss: 0.6660914421081543 - trainLoss: 0.6644197702407837\n",
      "cnt: 0 - valLoss: 0.66609126329422 - trainLoss: 0.6644196510314941\n",
      "cnt: 0 - valLoss: 0.6660911440849304 - trainLoss: 0.6644195318222046\n",
      "cnt: 0 - valLoss: 0.6660910248756409 - trainLoss: 0.664419412612915\n",
      "cnt: 0 - valLoss: 0.6660909056663513 - trainLoss: 0.6644192934036255\n",
      "cnt: 0 - valLoss: 0.6660907864570618 - trainLoss: 0.6644191741943359\n",
      "cnt: 0 - valLoss: 0.6660906076431274 - trainLoss: 0.6644190549850464\n",
      "cnt: 0 - valLoss: 0.6660904884338379 - trainLoss: 0.6644189357757568\n",
      "cnt: 0 - valLoss: 0.6660903096199036 - trainLoss: 0.6644187569618225\n",
      "cnt: 0 - valLoss: 0.6660901308059692 - trainLoss: 0.664418637752533\n",
      "cnt: 0 - valLoss: 0.6660900115966797 - trainLoss: 0.6644185185432434\n",
      "cnt: 0 - valLoss: 0.6660898327827454 - trainLoss: 0.6644184589385986\n",
      "cnt: 0 - valLoss: 0.6660897135734558 - trainLoss: 0.6644183397293091\n",
      "cnt: 0 - valLoss: 0.6660895943641663 - trainLoss: 0.6644181609153748\n",
      "cnt: 0 - valLoss: 0.6660895347595215 - trainLoss: 0.66441810131073\n",
      "cnt: 0 - valLoss: 0.6660892963409424 - trainLoss: 0.6644179224967957\n",
      "cnt: 0 - valLoss: 0.6660892367362976 - trainLoss: 0.6644178032875061\n",
      "cnt: 0 - valLoss: 0.6660889983177185 - trainLoss: 0.6644176840782166\n",
      "cnt: 0 - valLoss: 0.6660889387130737 - trainLoss: 0.664417564868927\n",
      "cnt: 0 - valLoss: 0.6660888195037842 - trainLoss: 0.6644174456596375\n",
      "cnt: 0 - valLoss: 0.6660886406898499 - trainLoss: 0.6644173264503479\n",
      "cnt: 0 - valLoss: 0.6660884618759155 - trainLoss: 0.6644172668457031\n",
      "cnt: 0 - valLoss: 0.666088342666626 - trainLoss: 0.6644170880317688\n",
      "cnt: 0 - valLoss: 0.6660882234573364 - trainLoss: 0.6644169688224792\n",
      "cnt: 0 - valLoss: 0.6660880446434021 - trainLoss: 0.6644169092178345\n",
      "cnt: 0 - valLoss: 0.6660878658294678 - trainLoss: 0.6644167304039001\n",
      "cnt: 0 - valLoss: 0.6660877466201782 - trainLoss: 0.6644166111946106\n",
      "cnt: 0 - valLoss: 0.6660876274108887 - trainLoss: 0.664416491985321\n",
      "cnt: 0 - valLoss: 0.6660874485969543 - trainLoss: 0.6644163727760315\n",
      "cnt: 0 - valLoss: 0.6660873889923096 - trainLoss: 0.6644161939620972\n",
      "cnt: 0 - valLoss: 0.66608726978302 - trainLoss: 0.6644160747528076\n",
      "cnt: 0 - valLoss: 0.6660870909690857 - trainLoss: 0.6644160151481628\n",
      "cnt: 0 - valLoss: 0.6660869717597961 - trainLoss: 0.6644158959388733\n",
      "cnt: 0 - valLoss: 0.6660867929458618 - trainLoss: 0.664415717124939\n",
      "cnt: 0 - valLoss: 0.6660866737365723 - trainLoss: 0.6644155979156494\n",
      "cnt: 0 - valLoss: 0.6660864949226379 - trainLoss: 0.6644154787063599\n",
      "cnt: 0 - valLoss: 0.6660863757133484 - trainLoss: 0.6644153594970703\n",
      "cnt: 0 - valLoss: 0.6660861968994141 - trainLoss: 0.6644152998924255\n",
      "cnt: 0 - valLoss: 0.6660860776901245 - trainLoss: 0.6644151210784912\n",
      "cnt: 0 - valLoss: 0.666085958480835 - trainLoss: 0.6644149422645569\n",
      "cnt: 0 - valLoss: 0.6660858392715454 - trainLoss: 0.6644148826599121\n",
      "cnt: 0 - valLoss: 0.6660856604576111 - trainLoss: 0.6644147634506226\n",
      "cnt: 0 - valLoss: 0.6660855412483215 - trainLoss: 0.664414644241333\n",
      "cnt: 0 - valLoss: 0.666085422039032 - trainLoss: 0.6644145250320435\n",
      "cnt: 0 - valLoss: 0.6660852432250977 - trainLoss: 0.6644144058227539\n",
      "cnt: 0 - valLoss: 0.6660851240158081 - trainLoss: 0.6644142866134644\n",
      "cnt: 0 - valLoss: 0.6660850048065186 - trainLoss: 0.6644141674041748\n",
      "cnt: 0 - valLoss: 0.6660848259925842 - trainLoss: 0.6644140481948853\n",
      "cnt: 0 - valLoss: 0.6660846471786499 - trainLoss: 0.6644139289855957\n",
      "cnt: 0 - valLoss: 0.6660845279693604 - trainLoss: 0.6644137501716614\n",
      "cnt: 0 - valLoss: 0.6660844087600708 - trainLoss: 0.6644136309623718\n",
      "cnt: 0 - valLoss: 0.6660842895507812 - trainLoss: 0.6644135117530823\n",
      "cnt: 0 - valLoss: 0.6660841107368469 - trainLoss: 0.6644133925437927\n",
      "cnt: 0 - valLoss: 0.6660839319229126 - trainLoss: 0.6644132733345032\n",
      "cnt: 0 - valLoss: 0.666083812713623 - trainLoss: 0.6644131541252136\n",
      "cnt: 0 - valLoss: 0.6660837531089783 - trainLoss: 0.6644130349159241\n",
      "cnt: 0 - valLoss: 0.666083574295044 - trainLoss: 0.6644128561019897\n",
      "cnt: 0 - valLoss: 0.6660833954811096 - trainLoss: 0.664412796497345\n",
      "cnt: 0 - valLoss: 0.6660832762718201 - trainLoss: 0.6644126772880554\n",
      "cnt: 0 - valLoss: 0.6660831570625305 - trainLoss: 0.6644125580787659\n",
      "cnt: 0 - valLoss: 0.6660829782485962 - trainLoss: 0.6644123792648315\n",
      "cnt: 0 - valLoss: 0.6660828590393066 - trainLoss: 0.6644123196601868\n",
      "cnt: 0 - valLoss: 0.6660827398300171 - trainLoss: 0.6644122004508972\n",
      "cnt: 0 - valLoss: 0.6660825610160828 - trainLoss: 0.6644120216369629\n",
      "cnt: 0 - valLoss: 0.6660824418067932 - trainLoss: 0.6644119620323181\n",
      "cnt: 0 - valLoss: 0.6660822629928589 - trainLoss: 0.6644117832183838\n",
      "cnt: 0 - valLoss: 0.6660821437835693 - trainLoss: 0.6644116640090942\n",
      "cnt: 0 - valLoss: 0.6660820245742798 - trainLoss: 0.6644116044044495\n",
      "cnt: 0 - valLoss: 0.6660819053649902 - trainLoss: 0.6644113659858704\n",
      "cnt: 0 - valLoss: 0.6660817265510559 - trainLoss: 0.6644113063812256\n",
      "cnt: 0 - valLoss: 0.6660816073417664 - trainLoss: 0.664411187171936\n",
      "cnt: 0 - valLoss: 0.6660814881324768 - trainLoss: 0.6644110679626465\n",
      "cnt: 0 - valLoss: 0.6660813093185425 - trainLoss: 0.6644109487533569\n",
      "cnt: 0 - valLoss: 0.6660811901092529 - trainLoss: 0.6644108295440674\n",
      "cnt: 0 - valLoss: 0.6660810112953186 - trainLoss: 0.6644106507301331\n",
      "cnt: 0 - valLoss: 0.666080892086029 - trainLoss: 0.6644105911254883\n",
      "cnt: 0 - valLoss: 0.6660808324813843 - trainLoss: 0.664410412311554\n",
      "cnt: 0 - valLoss: 0.6660805940628052 - trainLoss: 0.6644102931022644\n",
      "cnt: 0 - valLoss: 0.6660805344581604 - trainLoss: 0.6644102334976196\n",
      "cnt: 0 - valLoss: 0.6660803556442261 - trainLoss: 0.6644099950790405\n",
      "cnt: 0 - valLoss: 0.6660802364349365 - trainLoss: 0.6644099354743958\n",
      "cnt: 0 - valLoss: 0.666080117225647 - trainLoss: 0.6644097566604614\n",
      "cnt: 0 - valLoss: 0.6660799384117126 - trainLoss: 0.6644096374511719\n",
      "cnt: 0 - valLoss: 0.6660798192024231 - trainLoss: 0.6644095778465271\n",
      "cnt: 0 - valLoss: 0.6660796403884888 - trainLoss: 0.6644093990325928\n",
      "cnt: 0 - valLoss: 0.6660795211791992 - trainLoss: 0.6644092798233032\n",
      "cnt: 0 - valLoss: 0.6660794019699097 - trainLoss: 0.6644092202186584\n",
      "cnt: 0 - valLoss: 0.6660792827606201 - trainLoss: 0.6644090414047241\n",
      "cnt: 0 - valLoss: 0.6660791039466858 - trainLoss: 0.6644089221954346\n",
      "cnt: 0 - valLoss: 0.6660789847373962 - trainLoss: 0.6644087433815002\n",
      "cnt: 0 - valLoss: 0.6660788655281067 - trainLoss: 0.6644086837768555\n",
      "cnt: 0 - valLoss: 0.6660787463188171 - trainLoss: 0.6644085645675659\n",
      "cnt: 0 - valLoss: 0.6660785675048828 - trainLoss: 0.6644084453582764\n",
      "cnt: 0 - valLoss: 0.6660783886909485 - trainLoss: 0.664408266544342\n",
      "cnt: 0 - valLoss: 0.6660782694816589 - trainLoss: 0.6644081473350525\n",
      "cnt: 0 - valLoss: 0.6660782098770142 - trainLoss: 0.6644079685211182\n",
      "cnt: 0 - valLoss: 0.6660780310630798 - trainLoss: 0.6644079089164734\n",
      "cnt: 0 - valLoss: 0.6660777926445007 - trainLoss: 0.6644078493118286\n",
      "cnt: 0 - valLoss: 0.666077733039856 - trainLoss: 0.6644076108932495\n",
      "cnt: 0 - valLoss: 0.6660776138305664 - trainLoss: 0.6644075512886047\n",
      "cnt: 0 - valLoss: 0.6660774350166321 - trainLoss: 0.6644073724746704\n",
      "cnt: 0 - valLoss: 0.6660773158073425 - trainLoss: 0.6644072532653809\n",
      "cnt: 0 - valLoss: 0.6660771369934082 - trainLoss: 0.6644071936607361\n",
      "cnt: 0 - valLoss: 0.6660770177841187 - trainLoss: 0.6644070148468018\n",
      "cnt: 0 - valLoss: 0.6660768985748291 - trainLoss: 0.6644068956375122\n",
      "cnt: 0 - valLoss: 0.6660767197608948 - trainLoss: 0.6644067764282227\n",
      "cnt: 0 - valLoss: 0.6660766005516052 - trainLoss: 0.6644065976142883\n",
      "cnt: 0 - valLoss: 0.6660764813423157 - trainLoss: 0.6644065380096436\n",
      "cnt: 0 - valLoss: 0.6660763025283813 - trainLoss: 0.664406418800354\n",
      "cnt: 0 - valLoss: 0.6660761833190918 - trainLoss: 0.6644062995910645\n",
      "cnt: 0 - valLoss: 0.6660760641098022 - trainLoss: 0.6644061803817749\n",
      "cnt: 0 - valLoss: 0.6660758852958679 - trainLoss: 0.6644060015678406\n",
      "cnt: 0 - valLoss: 0.6660758256912231 - trainLoss: 0.6644059419631958\n",
      "cnt: 0 - valLoss: 0.6660756468772888 - trainLoss: 0.6644058227539062\n",
      "cnt: 0 - valLoss: 0.6660755276679993 - trainLoss: 0.6644056439399719\n",
      "cnt: 0 - valLoss: 0.6660753488540649 - trainLoss: 0.6644055247306824\n",
      "cnt: 0 - valLoss: 0.6660752296447754 - trainLoss: 0.6644054055213928\n",
      "cnt: 0 - valLoss: 0.6660750508308411 - trainLoss: 0.6644052863121033\n",
      "cnt: 0 - valLoss: 0.6660749316215515 - trainLoss: 0.6644051671028137\n",
      "cnt: 0 - valLoss: 0.6660747528076172 - trainLoss: 0.6644049882888794\n",
      "cnt: 0 - valLoss: 0.6660746932029724 - trainLoss: 0.6644048690795898\n",
      "cnt: 0 - valLoss: 0.6660744547843933 - trainLoss: 0.6644047498703003\n",
      "cnt: 0 - valLoss: 0.6660743355751038 - trainLoss: 0.664404571056366\n",
      "cnt: 0 - valLoss: 0.6660742163658142 - trainLoss: 0.6644045114517212\n",
      "cnt: 0 - valLoss: 0.6660741567611694 - trainLoss: 0.6644043922424316\n",
      "cnt: 0 - valLoss: 0.6660739779472351 - trainLoss: 0.6644042730331421\n",
      "cnt: 0 - valLoss: 0.6660738587379456 - trainLoss: 0.6644040942192078\n",
      "cnt: 0 - valLoss: 0.6660736799240112 - trainLoss: 0.664404034614563\n",
      "cnt: 0 - valLoss: 0.6660735011100769 - trainLoss: 0.6644039154052734\n",
      "cnt: 0 - valLoss: 0.6660733819007874 - trainLoss: 0.6644037365913391\n",
      "cnt: 0 - valLoss: 0.666073203086853 - trainLoss: 0.6644036173820496\n",
      "cnt: 0 - valLoss: 0.6660730838775635 - trainLoss: 0.66440349817276\n",
      "cnt: 0 - valLoss: 0.6660729646682739 - trainLoss: 0.6644033789634705\n",
      "cnt: 0 - valLoss: 0.6660728454589844 - trainLoss: 0.6644032597541809\n",
      "cnt: 0 - valLoss: 0.6660727262496948 - trainLoss: 0.6644031405448914\n",
      "cnt: 0 - valLoss: 0.6660725474357605 - trainLoss: 0.664402961730957\n",
      "cnt: 0 - valLoss: 0.6660724878311157 - trainLoss: 0.6644029021263123\n",
      "cnt: 0 - valLoss: 0.6660722494125366 - trainLoss: 0.6644027233123779\n",
      "cnt: 0 - valLoss: 0.6660721302032471 - trainLoss: 0.6644026041030884\n",
      "cnt: 0 - valLoss: 0.6660720109939575 - trainLoss: 0.6644024848937988\n",
      "cnt: 0 - valLoss: 0.6660718321800232 - trainLoss: 0.6644023656845093\n",
      "cnt: 0 - valLoss: 0.6660717129707336 - trainLoss: 0.6644022464752197\n",
      "cnt: 0 - valLoss: 0.6660715341567993 - trainLoss: 0.6644020676612854\n",
      "cnt: 0 - valLoss: 0.6660714149475098 - trainLoss: 0.6644020080566406\n",
      "cnt: 0 - valLoss: 0.6660712361335754 - trainLoss: 0.6644018292427063\n",
      "cnt: 0 - valLoss: 0.6660711169242859 - trainLoss: 0.6644017696380615\n",
      "cnt: 0 - valLoss: 0.6660709977149963 - trainLoss: 0.6644015908241272\n",
      "cnt: 0 - valLoss: 0.6660708785057068 - trainLoss: 0.6644014716148376\n",
      "cnt: 0 - valLoss: 0.6660707592964172 - trainLoss: 0.6644013524055481\n",
      "cnt: 0 - valLoss: 0.6660706400871277 - trainLoss: 0.6644012331962585\n",
      "cnt: 0 - valLoss: 0.6660704612731934 - trainLoss: 0.664401113986969\n",
      "cnt: 0 - valLoss: 0.6660703420639038 - trainLoss: 0.6644009351730347\n",
      "cnt: 0 - valLoss: 0.6660701632499695 - trainLoss: 0.6644008755683899\n",
      "cnt: 0 - valLoss: 0.6660700440406799 - trainLoss: 0.6644006967544556\n",
      "cnt: 0 - valLoss: 0.6660698652267456 - trainLoss: 0.6644005179405212\n",
      "cnt: 0 - valLoss: 0.6660698056221008 - trainLoss: 0.6644004583358765\n",
      "cnt: 0 - valLoss: 0.6660695672035217 - trainLoss: 0.6644003391265869\n",
      "cnt: 0 - valLoss: 0.666069507598877 - trainLoss: 0.6644001603126526\n",
      "cnt: 0 - valLoss: 0.6660693883895874 - trainLoss: 0.6644001007080078\n",
      "cnt: 0 - valLoss: 0.6660692095756531 - trainLoss: 0.6643999218940735\n",
      "cnt: 0 - valLoss: 0.6660690307617188 - trainLoss: 0.6643998026847839\n",
      "cnt: 0 - valLoss: 0.6660689115524292 - trainLoss: 0.6643996238708496\n",
      "cnt: 0 - valLoss: 0.6660687923431396 - trainLoss: 0.6643996238708496\n",
      "cnt: 0 - valLoss: 0.6660686731338501 - trainLoss: 0.6643994450569153\n",
      "cnt: 0 - valLoss: 0.6660684943199158 - trainLoss: 0.664399266242981\n",
      "cnt: 0 - valLoss: 0.6660683155059814 - trainLoss: 0.6643992066383362\n",
      "cnt: 0 - valLoss: 0.6660681962966919 - trainLoss: 0.6643990278244019\n",
      "cnt: 0 - valLoss: 0.6660681366920471 - trainLoss: 0.6643989086151123\n",
      "cnt: 0 - valLoss: 0.6660679578781128 - trainLoss: 0.664398729801178\n",
      "cnt: 0 - valLoss: 0.6660677790641785 - trainLoss: 0.6643986105918884\n",
      "cnt: 0 - valLoss: 0.6660676002502441 - trainLoss: 0.6643984913825989\n",
      "cnt: 0 - valLoss: 0.6660674810409546 - trainLoss: 0.6643984317779541\n",
      "cnt: 0 - valLoss: 0.6660674214363098 - trainLoss: 0.6643982529640198\n",
      "cnt: 0 - valLoss: 0.6660672426223755 - trainLoss: 0.6643981337547302\n",
      "cnt: 0 - valLoss: 0.6660671234130859 - trainLoss: 0.6643979549407959\n",
      "cnt: 0 - valLoss: 0.6660670042037964 - trainLoss: 0.6643978953361511\n",
      "cnt: 0 - valLoss: 0.6660668253898621 - trainLoss: 0.6643977165222168\n",
      "cnt: 0 - valLoss: 0.6660667061805725 - trainLoss: 0.6643975973129272\n",
      "cnt: 0 - valLoss: 0.6660665273666382 - trainLoss: 0.6643974781036377\n",
      "cnt: 0 - valLoss: 0.6660663485527039 - trainLoss: 0.6643973588943481\n",
      "cnt: 0 - valLoss: 0.6660662889480591 - trainLoss: 0.6643971800804138\n",
      "cnt: 0 - valLoss: 0.6660661101341248 - trainLoss: 0.6643970608711243\n",
      "cnt: 0 - valLoss: 0.6660659313201904 - trainLoss: 0.6643969416618347\n",
      "cnt: 0 - valLoss: 0.6660658717155457 - trainLoss: 0.6643968224525452\n",
      "cnt: 0 - valLoss: 0.6660656929016113 - trainLoss: 0.6643967032432556\n",
      "cnt: 0 - valLoss: 0.6660655736923218 - trainLoss: 0.6643965840339661\n",
      "cnt: 0 - valLoss: 0.6660654544830322 - trainLoss: 0.6643964052200317\n",
      "cnt: 0 - valLoss: 0.6660652756690979 - trainLoss: 0.6643962860107422\n",
      "cnt: 0 - valLoss: 0.6660651564598083 - trainLoss: 0.6643961668014526\n",
      "cnt: 0 - valLoss: 0.666064977645874 - trainLoss: 0.6643960475921631\n",
      "cnt: 0 - valLoss: 0.6660648584365845 - trainLoss: 0.6643959283828735\n",
      "cnt: 0 - valLoss: 0.6660646796226501 - trainLoss: 0.664395809173584\n",
      "cnt: 0 - valLoss: 0.6660645604133606 - trainLoss: 0.6643956899642944\n",
      "cnt: 0 - valLoss: 0.6660643815994263 - trainLoss: 0.6643955111503601\n",
      "cnt: 0 - valLoss: 0.6660642623901367 - trainLoss: 0.6643953919410706\n",
      "cnt: 0 - valLoss: 0.6660641431808472 - trainLoss: 0.6643952131271362\n",
      "cnt: 0 - valLoss: 0.6660639643669128 - trainLoss: 0.6643950939178467\n",
      "cnt: 0 - valLoss: 0.6660637855529785 - trainLoss: 0.6643950343132019\n",
      "cnt: 0 - valLoss: 0.666063666343689 - trainLoss: 0.6643949151039124\n",
      "cnt: 0 - valLoss: 0.6660635471343994 - trainLoss: 0.664394736289978\n",
      "cnt: 0 - valLoss: 0.6660633683204651 - trainLoss: 0.6643946170806885\n",
      "cnt: 0 - valLoss: 0.6660632491111755 - trainLoss: 0.6643944978713989\n",
      "cnt: 0 - valLoss: 0.666063129901886 - trainLoss: 0.6643943190574646\n",
      "cnt: 0 - valLoss: 0.6660630106925964 - trainLoss: 0.664394199848175\n",
      "cnt: 0 - valLoss: 0.6660628318786621 - trainLoss: 0.6643941402435303\n",
      "cnt: 0 - valLoss: 0.6660627126693726 - trainLoss: 0.664393961429596\n",
      "cnt: 0 - valLoss: 0.6660625338554382 - trainLoss: 0.6643938422203064\n",
      "cnt: 0 - valLoss: 0.6660624146461487 - trainLoss: 0.6643937230110168\n",
      "cnt: 0 - valLoss: 0.6660622358322144 - trainLoss: 0.6643935441970825\n",
      "cnt: 0 - valLoss: 0.6660621166229248 - trainLoss: 0.664393424987793\n",
      "cnt: 0 - valLoss: 0.6660619378089905 - trainLoss: 0.6643933057785034\n",
      "cnt: 0 - valLoss: 0.6660617589950562 - trainLoss: 0.6643931865692139\n",
      "cnt: 0 - valLoss: 0.6660616397857666 - trainLoss: 0.6643930673599243\n",
      "cnt: 0 - valLoss: 0.666061520576477 - trainLoss: 0.6643929481506348\n",
      "cnt: 0 - valLoss: 0.6660614013671875 - trainLoss: 0.6643927693367004\n",
      "cnt: 0 - valLoss: 0.6660612225532532 - trainLoss: 0.6643926501274109\n",
      "cnt: 0 - valLoss: 0.6660610437393188 - trainLoss: 0.6643924713134766\n",
      "cnt: 0 - valLoss: 0.6660609245300293 - trainLoss: 0.6643924117088318\n",
      "cnt: 0 - valLoss: 0.6660608053207397 - trainLoss: 0.6643922328948975\n",
      "cnt: 0 - valLoss: 0.6660606265068054 - trainLoss: 0.6643921136856079\n",
      "cnt: 0 - valLoss: 0.6660605669021606 - trainLoss: 0.6643919944763184\n",
      "cnt: 0 - valLoss: 0.6660603880882263 - trainLoss: 0.6643918752670288\n",
      "cnt: 0 - valLoss: 0.6660602688789368 - trainLoss: 0.6643917560577393\n",
      "cnt: 0 - valLoss: 0.6660600900650024 - trainLoss: 0.6643915772438049\n",
      "cnt: 0 - valLoss: 0.6660599708557129 - trainLoss: 0.6643914580345154\n",
      "cnt: 0 - valLoss: 0.6660597920417786 - trainLoss: 0.6643913388252258\n",
      "cnt: 0 - valLoss: 0.666059672832489 - trainLoss: 0.6643912196159363\n",
      "cnt: 0 - valLoss: 0.6660594940185547 - trainLoss: 0.6643911004066467\n",
      "cnt: 0 - valLoss: 0.6660593748092651 - trainLoss: 0.6643909215927124\n",
      "cnt: 0 - valLoss: 0.6660591959953308 - trainLoss: 0.6643908023834229\n",
      "cnt: 0 - valLoss: 0.6660590767860413 - trainLoss: 0.6643906831741333\n",
      "cnt: 0 - valLoss: 0.6660588979721069 - trainLoss: 0.664390504360199\n",
      "cnt: 0 - valLoss: 0.6660587787628174 - trainLoss: 0.6643903851509094\n",
      "cnt: 0 - valLoss: 0.6660586595535278 - trainLoss: 0.6643902659416199\n",
      "cnt: 0 - valLoss: 0.6660584807395935 - trainLoss: 0.6643902063369751\n",
      "cnt: 0 - valLoss: 0.6660583019256592 - trainLoss: 0.6643900275230408\n",
      "cnt: 0 - valLoss: 0.6660581827163696 - trainLoss: 0.6643899083137512\n",
      "cnt: 0 - valLoss: 0.6660580635070801 - trainLoss: 0.6643897891044617\n",
      "cnt: 0 - valLoss: 0.6660578846931458 - trainLoss: 0.6643896102905273\n",
      "cnt: 0 - valLoss: 0.6660577654838562 - trainLoss: 0.6643894910812378\n",
      "cnt: 0 - valLoss: 0.6660576462745667 - trainLoss: 0.6643893122673035\n",
      "cnt: 0 - valLoss: 0.6660574674606323 - trainLoss: 0.6643892526626587\n",
      "cnt: 0 - valLoss: 0.6660573482513428 - trainLoss: 0.6643890738487244\n",
      "cnt: 0 - valLoss: 0.6660571694374084 - trainLoss: 0.6643889546394348\n",
      "cnt: 0 - valLoss: 0.6660570502281189 - trainLoss: 0.6643888354301453\n",
      "cnt: 0 - valLoss: 0.6660568714141846 - trainLoss: 0.6643887162208557\n",
      "cnt: 0 - valLoss: 0.666056752204895 - trainLoss: 0.6643885374069214\n",
      "cnt: 0 - valLoss: 0.6660565733909607 - trainLoss: 0.6643884778022766\n",
      "cnt: 0 - valLoss: 0.6660564541816711 - trainLoss: 0.6643883585929871\n",
      "cnt: 0 - valLoss: 0.6660562753677368 - trainLoss: 0.6643882393836975\n",
      "cnt: 0 - valLoss: 0.6660561561584473 - trainLoss: 0.6643880605697632\n",
      "cnt: 0 - valLoss: 0.6660560369491577 - trainLoss: 0.6643879413604736\n",
      "cnt: 0 - valLoss: 0.6660558581352234 - trainLoss: 0.6643878221511841\n",
      "cnt: 0 - valLoss: 0.6660557389259338 - trainLoss: 0.6643877029418945\n",
      "cnt: 0 - valLoss: 0.6660555601119995 - trainLoss: 0.6643875241279602\n",
      "cnt: 0 - valLoss: 0.66605544090271 - trainLoss: 0.6643874049186707\n",
      "cnt: 0 - valLoss: 0.6660553216934204 - trainLoss: 0.6643872857093811\n",
      "cnt: 0 - valLoss: 0.6660551428794861 - trainLoss: 0.6643871068954468\n",
      "cnt: 0 - valLoss: 0.6660550236701965 - trainLoss: 0.6643869876861572\n",
      "cnt: 0 - valLoss: 0.666054904460907 - trainLoss: 0.6643868684768677\n",
      "cnt: 0 - valLoss: 0.6660547256469727 - trainLoss: 0.6643867492675781\n",
      "cnt: 0 - valLoss: 0.6660546064376831 - trainLoss: 0.6643865704536438\n",
      "cnt: 0 - valLoss: 0.6660544872283936 - trainLoss: 0.6643864512443542\n",
      "cnt: 0 - valLoss: 0.6660543084144592 - trainLoss: 0.6643863320350647\n",
      "cnt: 0 - valLoss: 0.6660541892051697 - trainLoss: 0.6643862724304199\n",
      "cnt: 0 - valLoss: 0.6660540103912354 - trainLoss: 0.6643860936164856\n",
      "cnt: 0 - valLoss: 0.666053831577301 - trainLoss: 0.6643859148025513\n",
      "cnt: 0 - valLoss: 0.6660537123680115 - trainLoss: 0.6643857955932617\n",
      "cnt: 0 - valLoss: 0.6660535335540771 - trainLoss: 0.6643856763839722\n",
      "cnt: 0 - valLoss: 0.6660534143447876 - trainLoss: 0.6643854975700378\n",
      "cnt: 0 - valLoss: 0.6660532355308533 - trainLoss: 0.6643853783607483\n",
      "cnt: 0 - valLoss: 0.6660531163215637 - trainLoss: 0.6643853187561035\n",
      "cnt: 0 - valLoss: 0.6660529971122742 - trainLoss: 0.6643851399421692\n",
      "cnt: 0 - valLoss: 0.6660528182983398 - trainLoss: 0.6643850207328796\n",
      "cnt: 0 - valLoss: 0.6660526394844055 - trainLoss: 0.6643849015235901\n",
      "cnt: 0 - valLoss: 0.666052520275116 - trainLoss: 0.6643847227096558\n",
      "cnt: 0 - valLoss: 0.6660524010658264 - trainLoss: 0.6643846035003662\n",
      "cnt: 0 - valLoss: 0.6660522818565369 - trainLoss: 0.6643844246864319\n",
      "cnt: 0 - valLoss: 0.6660521626472473 - trainLoss: 0.6643843054771423\n",
      "cnt: 0 - valLoss: 0.666051983833313 - trainLoss: 0.6643841862678528\n",
      "cnt: 0 - valLoss: 0.6660518646240234 - trainLoss: 0.6643840670585632\n",
      "cnt: 0 - valLoss: 0.6660516858100891 - trainLoss: 0.6643839478492737\n",
      "cnt: 0 - valLoss: 0.6660515666007996 - trainLoss: 0.6643838286399841\n",
      "cnt: 0 - valLoss: 0.6660513877868652 - trainLoss: 0.6643837094306946\n",
      "cnt: 0 - valLoss: 0.6660512089729309 - trainLoss: 0.6643835306167603\n",
      "cnt: 0 - valLoss: 0.6660510897636414 - trainLoss: 0.6643834114074707\n",
      "cnt: 0 - valLoss: 0.6660509705543518 - trainLoss: 0.6643832921981812\n",
      "cnt: 0 - valLoss: 0.6660507917404175 - trainLoss: 0.6643831729888916\n",
      "cnt: 0 - valLoss: 0.6660506725311279 - trainLoss: 0.664383053779602\n",
      "cnt: 0 - valLoss: 0.6660504937171936 - trainLoss: 0.6643829345703125\n",
      "cnt: 0 - valLoss: 0.666050374507904 - trainLoss: 0.6643827557563782\n",
      "cnt: 0 - valLoss: 0.6660502552986145 - trainLoss: 0.6643826365470886\n",
      "cnt: 0 - valLoss: 0.6660500764846802 - trainLoss: 0.6643824577331543\n",
      "cnt: 0 - valLoss: 0.6660498976707458 - trainLoss: 0.6643823981285095\n",
      "cnt: 0 - valLoss: 0.6660497188568115 - trainLoss: 0.6643822193145752\n",
      "cnt: 0 - valLoss: 0.6660496592521667 - trainLoss: 0.6643821001052856\n",
      "cnt: 0 - valLoss: 0.6660494804382324 - trainLoss: 0.6643819808959961\n",
      "cnt: 0 - valLoss: 0.6660493016242981 - trainLoss: 0.6643818616867065\n",
      "cnt: 0 - valLoss: 0.6660491824150085 - trainLoss: 0.664381742477417\n",
      "cnt: 0 - valLoss: 0.666049063205719 - trainLoss: 0.6643815636634827\n",
      "cnt: 0 - valLoss: 0.6660488247871399 - trainLoss: 0.6643814444541931\n",
      "cnt: 0 - valLoss: 0.6660487055778503 - trainLoss: 0.6643812656402588\n",
      "cnt: 0 - valLoss: 0.666048526763916 - trainLoss: 0.6643811464309692\n",
      "cnt: 0 - valLoss: 0.6660484075546265 - trainLoss: 0.6643810272216797\n",
      "cnt: 0 - valLoss: 0.6660482883453369 - trainLoss: 0.6643809676170349\n",
      "cnt: 0 - valLoss: 0.6660481095314026 - trainLoss: 0.6643807888031006\n",
      "cnt: 0 - valLoss: 0.6660479307174683 - trainLoss: 0.664380669593811\n",
      "cnt: 0 - valLoss: 0.6660477519035339 - trainLoss: 0.6643804907798767\n",
      "cnt: 0 - valLoss: 0.6660476922988892 - trainLoss: 0.6643804311752319\n",
      "cnt: 0 - valLoss: 0.6660475134849548 - trainLoss: 0.6643802523612976\n",
      "cnt: 0 - valLoss: 0.6660473942756653 - trainLoss: 0.6643801927566528\n",
      "cnt: 0 - valLoss: 0.6660472750663757 - trainLoss: 0.6643800139427185\n",
      "cnt: 0 - valLoss: 0.6660470366477966 - trainLoss: 0.664379894733429\n",
      "cnt: 0 - valLoss: 0.6660469174385071 - trainLoss: 0.6643797755241394\n",
      "cnt: 0 - valLoss: 0.6660467982292175 - trainLoss: 0.6643795967102051\n",
      "cnt: 0 - valLoss: 0.6660466194152832 - trainLoss: 0.6643794775009155\n",
      "cnt: 0 - valLoss: 0.6660465002059937 - trainLoss: 0.664379358291626\n",
      "cnt: 0 - valLoss: 0.6660462617874146 - trainLoss: 0.6643791794776917\n",
      "cnt: 0 - valLoss: 0.6660462021827698 - trainLoss: 0.6643790602684021\n",
      "cnt: 0 - valLoss: 0.6660460233688354 - trainLoss: 0.6643789410591125\n",
      "cnt: 0 - valLoss: 0.6660459041595459 - trainLoss: 0.664378821849823\n",
      "cnt: 0 - valLoss: 0.6660457253456116 - trainLoss: 0.6643786430358887\n",
      "cnt: 0 - valLoss: 0.6660455465316772 - trainLoss: 0.6643785238265991\n",
      "cnt: 0 - valLoss: 0.6660454273223877 - trainLoss: 0.6643784642219543\n",
      "cnt: 0 - valLoss: 0.6660452485084534 - trainLoss: 0.66437828540802\n",
      "cnt: 0 - valLoss: 0.6660451292991638 - trainLoss: 0.6643781661987305\n",
      "cnt: 0 - valLoss: 0.6660449504852295 - trainLoss: 0.6643780469894409\n",
      "cnt: 0 - valLoss: 0.6660448312759399 - trainLoss: 0.6643779277801514\n",
      "cnt: 0 - valLoss: 0.6660446524620056 - trainLoss: 0.664377748966217\n",
      "cnt: 0 - valLoss: 0.6660445332527161 - trainLoss: 0.6643776297569275\n",
      "cnt: 0 - valLoss: 0.6660443544387817 - trainLoss: 0.6643774509429932\n",
      "cnt: 0 - valLoss: 0.6660441756248474 - trainLoss: 0.6643773317337036\n",
      "cnt: 0 - valLoss: 0.6660440564155579 - trainLoss: 0.6643772721290588\n",
      "cnt: 0 - valLoss: 0.6660439372062683 - trainLoss: 0.6643770933151245\n",
      "cnt: 0 - valLoss: 0.666043758392334 - trainLoss: 0.664376974105835\n",
      "cnt: 0 - valLoss: 0.6660436391830444 - trainLoss: 0.6643768548965454\n",
      "cnt: 0 - valLoss: 0.6660434603691101 - trainLoss: 0.6643767356872559\n",
      "cnt: 0 - valLoss: 0.6660432815551758 - trainLoss: 0.6643765568733215\n",
      "cnt: 0 - valLoss: 0.6660431623458862 - trainLoss: 0.6643763780593872\n",
      "cnt: 0 - valLoss: 0.6660430431365967 - trainLoss: 0.6643763184547424\n",
      "cnt: 0 - valLoss: 0.6660428643226624 - trainLoss: 0.6643761992454529\n",
      "cnt: 0 - valLoss: 0.666042685508728 - trainLoss: 0.6643760800361633\n",
      "cnt: 0 - valLoss: 0.6660425662994385 - trainLoss: 0.6643759608268738\n",
      "cnt: 0 - valLoss: 0.6660423874855042 - trainLoss: 0.6643757820129395\n",
      "cnt: 0 - valLoss: 0.6660422682762146 - trainLoss: 0.6643756628036499\n",
      "cnt: 0 - valLoss: 0.6660420894622803 - trainLoss: 0.6643754839897156\n",
      "cnt: 0 - valLoss: 0.666041910648346 - trainLoss: 0.664375364780426\n",
      "cnt: 0 - valLoss: 0.6660417914390564 - trainLoss: 0.6643752455711365\n",
      "cnt: 0 - valLoss: 0.6660416126251221 - trainLoss: 0.6643751263618469\n",
      "cnt: 0 - valLoss: 0.6660414934158325 - trainLoss: 0.6643750667572021\n",
      "cnt: 0 - valLoss: 0.6660413146018982 - trainLoss: 0.6643748879432678\n",
      "cnt: 0 - valLoss: 0.6660411953926086 - trainLoss: 0.6643747091293335\n",
      "cnt: 0 - valLoss: 0.6660410165786743 - trainLoss: 0.664374589920044\n",
      "cnt: 0 - valLoss: 0.6660408973693848 - trainLoss: 0.6643744111061096\n",
      "cnt: 0 - valLoss: 0.6660407185554504 - trainLoss: 0.6643743515014648\n",
      "cnt: 0 - valLoss: 0.6660405993461609 - trainLoss: 0.6643741726875305\n",
      "cnt: 0 - valLoss: 0.6660404205322266 - trainLoss: 0.6643741130828857\n",
      "cnt: 0 - valLoss: 0.6660402417182922 - trainLoss: 0.6643739342689514\n",
      "cnt: 0 - valLoss: 0.6660401225090027 - trainLoss: 0.6643738150596619\n",
      "cnt: 0 - valLoss: 0.6660399436950684 - trainLoss: 0.6643736362457275\n",
      "cnt: 0 - valLoss: 0.6660398244857788 - trainLoss: 0.664373517036438\n",
      "cnt: 0 - valLoss: 0.6660396456718445 - trainLoss: 0.6643733978271484\n",
      "cnt: 0 - valLoss: 0.6660395264625549 - trainLoss: 0.6643732786178589\n",
      "cnt: 0 - valLoss: 0.6660393476486206 - trainLoss: 0.6643731594085693\n",
      "cnt: 0 - valLoss: 0.666039228439331 - trainLoss: 0.664372980594635\n",
      "cnt: 0 - valLoss: 0.6660390496253967 - trainLoss: 0.6643728017807007\n",
      "cnt: 0 - valLoss: 0.6660388708114624 - trainLoss: 0.6643727421760559\n",
      "cnt: 0 - valLoss: 0.6660387516021729 - trainLoss: 0.6643726229667664\n",
      "cnt: 0 - valLoss: 0.6660385727882385 - trainLoss: 0.664372444152832\n",
      "cnt: 0 - valLoss: 0.666038453578949 - trainLoss: 0.6643723249435425\n",
      "cnt: 0 - valLoss: 0.6660382747650146 - trainLoss: 0.6643721461296082\n",
      "cnt: 0 - valLoss: 0.6660381555557251 - trainLoss: 0.6643720269203186\n",
      "cnt: 0 - valLoss: 0.6660379767417908 - trainLoss: 0.664371907711029\n",
      "cnt: 0 - valLoss: 0.6660378575325012 - trainLoss: 0.6643718481063843\n",
      "cnt: 0 - valLoss: 0.6660376787185669 - trainLoss: 0.6643717288970947\n",
      "cnt: 0 - valLoss: 0.6660374999046326 - trainLoss: 0.6643715500831604\n",
      "cnt: 0 - valLoss: 0.666037380695343 - trainLoss: 0.6643713712692261\n",
      "cnt: 0 - valLoss: 0.6660372018814087 - trainLoss: 0.6643712520599365\n",
      "cnt: 0 - valLoss: 0.6660370826721191 - trainLoss: 0.664371132850647\n",
      "cnt: 0 - valLoss: 0.6660369038581848 - trainLoss: 0.6643710136413574\n",
      "cnt: 0 - valLoss: 0.6660367846488953 - trainLoss: 0.6643708944320679\n",
      "cnt: 0 - valLoss: 0.6660366058349609 - trainLoss: 0.6643707752227783\n",
      "cnt: 0 - valLoss: 0.6660364270210266 - trainLoss: 0.6643706560134888\n",
      "cnt: 0 - valLoss: 0.6660363078117371 - trainLoss: 0.6643704771995544\n",
      "cnt: 0 - valLoss: 0.6660361289978027 - trainLoss: 0.6643703579902649\n",
      "cnt: 0 - valLoss: 0.6660360097885132 - trainLoss: 0.6643701791763306\n",
      "cnt: 0 - valLoss: 0.6660358309745789 - trainLoss: 0.6643701195716858\n",
      "cnt: 0 - valLoss: 0.6660357117652893 - trainLoss: 0.6643699407577515\n",
      "cnt: 0 - valLoss: 0.666035532951355 - trainLoss: 0.6643697619438171\n",
      "cnt: 0 - valLoss: 0.6660354137420654 - trainLoss: 0.6643697023391724\n",
      "cnt: 0 - valLoss: 0.6660352349281311 - trainLoss: 0.6643695831298828\n",
      "cnt: 0 - valLoss: 0.6660350561141968 - trainLoss: 0.6643694043159485\n",
      "cnt: 0 - valLoss: 0.6660349369049072 - trainLoss: 0.6643692851066589\n",
      "cnt: 0 - valLoss: 0.6660347580909729 - trainLoss: 0.6643691062927246\n",
      "cnt: 0 - valLoss: 0.6660346388816833 - trainLoss: 0.6643689870834351\n",
      "cnt: 0 - valLoss: 0.6660345196723938 - trainLoss: 0.6643688678741455\n",
      "cnt: 0 - valLoss: 0.6660343408584595 - trainLoss: 0.6643688082695007\n",
      "cnt: 0 - valLoss: 0.6660341620445251 - trainLoss: 0.6643685698509216\n",
      "cnt: 0 - valLoss: 0.6660339832305908 - trainLoss: 0.6643685102462769\n",
      "cnt: 0 - valLoss: 0.6660338640213013 - trainLoss: 0.6643683314323425\n",
      "cnt: 0 - valLoss: 0.6660336852073669 - trainLoss: 0.664368212223053\n",
      "cnt: 0 - valLoss: 0.6660335659980774 - trainLoss: 0.6643680930137634\n",
      "cnt: 0 - valLoss: 0.6660333871841431 - trainLoss: 0.6643679141998291\n",
      "cnt: 0 - valLoss: 0.6660332679748535 - trainLoss: 0.6643677949905396\n",
      "cnt: 0 - valLoss: 0.6660330891609192 - trainLoss: 0.66436767578125\n",
      "cnt: 0 - valLoss: 0.6660329699516296 - trainLoss: 0.6643674969673157\n",
      "cnt: 0 - valLoss: 0.6660327911376953 - trainLoss: 0.6643673777580261\n",
      "cnt: 0 - valLoss: 0.6660326719284058 - trainLoss: 0.6643673181533813\n",
      "cnt: 0 - valLoss: 0.6660324931144714 - trainLoss: 0.664367139339447\n",
      "cnt: 0 - valLoss: 0.6660323143005371 - trainLoss: 0.6643670201301575\n",
      "cnt: 0 - valLoss: 0.6660321950912476 - trainLoss: 0.6643669009208679\n",
      "cnt: 0 - valLoss: 0.666032075881958 - trainLoss: 0.6643667221069336\n",
      "cnt: 0 - valLoss: 0.6660318970680237 - trainLoss: 0.6643665432929993\n",
      "cnt: 0 - valLoss: 0.6660317182540894 - trainLoss: 0.6643664836883545\n",
      "cnt: 0 - valLoss: 0.6660315990447998 - trainLoss: 0.6643663048744202\n",
      "cnt: 0 - valLoss: 0.6660314202308655 - trainLoss: 0.6643661856651306\n",
      "cnt: 0 - valLoss: 0.6660313010215759 - trainLoss: 0.6643660664558411\n",
      "cnt: 0 - valLoss: 0.6660311222076416 - trainLoss: 0.6643658876419067\n",
      "cnt: 0 - valLoss: 0.6660309433937073 - trainLoss: 0.6643657684326172\n",
      "cnt: 0 - valLoss: 0.6660308241844177 - trainLoss: 0.6643656492233276\n",
      "cnt: 0 - valLoss: 0.6660306453704834 - trainLoss: 0.6643654704093933\n",
      "cnt: 0 - valLoss: 0.6660305261611938 - trainLoss: 0.6643654108047485\n",
      "cnt: 0 - valLoss: 0.6660303473472595 - trainLoss: 0.6643652319908142\n",
      "cnt: 0 - valLoss: 0.66603022813797 - trainLoss: 0.6643651127815247\n",
      "cnt: 0 - valLoss: 0.6660300493240356 - trainLoss: 0.6643649935722351\n",
      "cnt: 0 - valLoss: 0.6660299301147461 - trainLoss: 0.6643648743629456\n",
      "cnt: 0 - valLoss: 0.6660297513008118 - trainLoss: 0.6643646955490112\n",
      "cnt: 0 - valLoss: 0.6660295724868774 - trainLoss: 0.6643645763397217\n",
      "cnt: 0 - valLoss: 0.6660294532775879 - trainLoss: 0.6643643975257874\n",
      "cnt: 0 - valLoss: 0.6660292744636536 - trainLoss: 0.6643642783164978\n",
      "cnt: 0 - valLoss: 0.666029155254364 - trainLoss: 0.6643641591072083\n",
      "cnt: 0 - valLoss: 0.6660289764404297 - trainLoss: 0.6643640398979187\n",
      "cnt: 0 - valLoss: 0.6660287976264954 - trainLoss: 0.6643638610839844\n",
      "cnt: 0 - valLoss: 0.6660286784172058 - trainLoss: 0.6643638014793396\n",
      "cnt: 0 - valLoss: 0.6660284996032715 - trainLoss: 0.6643636226654053\n",
      "cnt: 0 - valLoss: 0.6660283803939819 - trainLoss: 0.6643635034561157\n",
      "cnt: 0 - valLoss: 0.6660282015800476 - trainLoss: 0.6643633842468262\n",
      "cnt: 0 - valLoss: 0.6660280823707581 - trainLoss: 0.6643632650375366\n",
      "cnt: 0 - valLoss: 0.6660279035568237 - trainLoss: 0.6643630862236023\n",
      "cnt: 0 - valLoss: 0.6660277843475342 - trainLoss: 0.6643630266189575\n",
      "cnt: 0 - valLoss: 0.6660276055335999 - trainLoss: 0.6643628478050232\n",
      "cnt: 0 - valLoss: 0.6660274267196655 - trainLoss: 0.6643627285957336\n",
      "cnt: 0 - valLoss: 0.666027307510376 - trainLoss: 0.6643625497817993\n",
      "cnt: 0 - valLoss: 0.6660271286964417 - trainLoss: 0.6643624305725098\n",
      "cnt: 0 - valLoss: 0.6660270094871521 - trainLoss: 0.6643623113632202\n",
      "cnt: 0 - valLoss: 0.6660268306732178 - trainLoss: 0.6643621325492859\n",
      "cnt: 0 - valLoss: 0.6660266518592834 - trainLoss: 0.6643620133399963\n",
      "cnt: 0 - valLoss: 0.6660264730453491 - trainLoss: 0.6643618941307068\n",
      "cnt: 0 - valLoss: 0.6660263538360596 - trainLoss: 0.6643617749214172\n",
      "cnt: 0 - valLoss: 0.66602623462677 - trainLoss: 0.6643615961074829\n",
      "cnt: 0 - valLoss: 0.6660260558128357 - trainLoss: 0.6643615365028381\n",
      "cnt: 0 - valLoss: 0.6660259366035461 - trainLoss: 0.6643613576889038\n",
      "cnt: 0 - valLoss: 0.6660257577896118 - trainLoss: 0.6643612384796143\n",
      "cnt: 0 - valLoss: 0.6660255789756775 - trainLoss: 0.6643610596656799\n",
      "cnt: 0 - valLoss: 0.6660254597663879 - trainLoss: 0.6643609404563904\n",
      "cnt: 0 - valLoss: 0.6660252809524536 - trainLoss: 0.6643608808517456\n",
      "cnt: 0 - valLoss: 0.6660251021385193 - trainLoss: 0.664360761642456\n",
      "cnt: 0 - valLoss: 0.666024923324585 - trainLoss: 0.6643605828285217\n",
      "cnt: 0 - valLoss: 0.6660248041152954 - trainLoss: 0.6643604636192322\n",
      "cnt: 0 - valLoss: 0.6660246253013611 - trainLoss: 0.6643602848052979\n",
      "cnt: 0 - valLoss: 0.6660245060920715 - trainLoss: 0.6643601655960083\n",
      "cnt: 0 - valLoss: 0.6660242676734924 - trainLoss: 0.664359986782074\n",
      "cnt: 0 - valLoss: 0.6660241484642029 - trainLoss: 0.6643598675727844\n",
      "cnt: 0 - valLoss: 0.6660240292549133 - trainLoss: 0.6643597483634949\n",
      "cnt: 0 - valLoss: 0.6660239100456238 - trainLoss: 0.6643596291542053\n",
      "cnt: 0 - valLoss: 0.6660236716270447 - trainLoss: 0.6643595099449158\n",
      "cnt: 0 - valLoss: 0.6660235524177551 - trainLoss: 0.6643593311309814\n",
      "cnt: 0 - valLoss: 0.6660233736038208 - trainLoss: 0.6643592119216919\n",
      "cnt: 0 - valLoss: 0.6660231947898865 - trainLoss: 0.6643590927124023\n",
      "cnt: 0 - valLoss: 0.6660230159759521 - trainLoss: 0.6643589735031128\n",
      "cnt: 0 - valLoss: 0.6660228967666626 - trainLoss: 0.6643587946891785\n",
      "cnt: 0 - valLoss: 0.6660227179527283 - trainLoss: 0.6643587350845337\n",
      "cnt: 0 - valLoss: 0.666022539138794 - trainLoss: 0.6643585562705994\n",
      "cnt: 0 - valLoss: 0.6660224199295044 - trainLoss: 0.664358377456665\n",
      "cnt: 0 - valLoss: 0.6660223007202148 - trainLoss: 0.6643583178520203\n",
      "cnt: 0 - valLoss: 0.6660221219062805 - trainLoss: 0.6643581986427307\n",
      "cnt: 0 - valLoss: 0.6660219430923462 - trainLoss: 0.6643580198287964\n",
      "cnt: 0 - valLoss: 0.6660218238830566 - trainLoss: 0.6643579006195068\n",
      "cnt: 0 - valLoss: 0.6660216450691223 - trainLoss: 0.6643577218055725\n",
      "cnt: 0 - valLoss: 0.6660215258598328 - trainLoss: 0.6643576622009277\n",
      "cnt: 0 - valLoss: 0.6660213470458984 - trainLoss: 0.6643575429916382\n",
      "cnt: 0 - valLoss: 0.6660211682319641 - trainLoss: 0.6643573641777039\n",
      "cnt: 0 - valLoss: 0.6660210490226746 - trainLoss: 0.6643572449684143\n",
      "cnt: 0 - valLoss: 0.666020929813385 - trainLoss: 0.66435706615448\n",
      "cnt: 0 - valLoss: 0.6660206913948059 - trainLoss: 0.6643569469451904\n",
      "cnt: 0 - valLoss: 0.6660205721855164 - trainLoss: 0.6643568277359009\n",
      "cnt: 0 - valLoss: 0.666020393371582 - trainLoss: 0.6643567085266113\n",
      "cnt: 0 - valLoss: 0.6660202741622925 - trainLoss: 0.664356529712677\n",
      "cnt: 0 - valLoss: 0.6660200953483582 - trainLoss: 0.6643564105033875\n",
      "cnt: 0 - valLoss: 0.6660199761390686 - trainLoss: 0.6643562912940979\n",
      "cnt: 0 - valLoss: 0.6660197377204895 - trainLoss: 0.6643561720848083\n",
      "cnt: 0 - valLoss: 0.6660196781158447 - trainLoss: 0.6643560528755188\n",
      "cnt: 0 - valLoss: 0.6660194993019104 - trainLoss: 0.6643558740615845\n",
      "cnt: 0 - valLoss: 0.6660192608833313 - trainLoss: 0.6643557548522949\n",
      "cnt: 0 - valLoss: 0.6660191416740417 - trainLoss: 0.6643555760383606\n",
      "cnt: 0 - valLoss: 0.6660189628601074 - trainLoss: 0.6643555164337158\n",
      "cnt: 0 - valLoss: 0.6660187244415283 - trainLoss: 0.6643553376197815\n",
      "cnt: 0 - valLoss: 0.6660186648368835 - trainLoss: 0.6643552184104919\n",
      "cnt: 0 - valLoss: 0.6660184264183044 - trainLoss: 0.6643550992012024\n",
      "cnt: 0 - valLoss: 0.6660183072090149 - trainLoss: 0.6643549799919128\n",
      "cnt: 0 - valLoss: 0.6660181283950806 - trainLoss: 0.6643548607826233\n",
      "cnt: 0 - valLoss: 0.666018009185791 - trainLoss: 0.664354681968689\n",
      "cnt: 0 - valLoss: 0.6660178303718567 - trainLoss: 0.6643545031547546\n",
      "cnt: 0 - valLoss: 0.6660177111625671 - trainLoss: 0.6643543839454651\n",
      "cnt: 0 - valLoss: 0.666017472743988 - trainLoss: 0.6643542647361755\n",
      "cnt: 0 - valLoss: 0.6660172939300537 - trainLoss: 0.6643540859222412\n",
      "cnt: 0 - valLoss: 0.6660171747207642 - trainLoss: 0.6643539667129517\n",
      "cnt: 0 - valLoss: 0.6660169959068298 - trainLoss: 0.6643537878990173\n",
      "cnt: 0 - valLoss: 0.6660168170928955 - trainLoss: 0.664353609085083\n",
      "cnt: 0 - valLoss: 0.6660166382789612 - trainLoss: 0.6643535494804382\n",
      "cnt: 0 - valLoss: 0.6660164594650269 - trainLoss: 0.6643534302711487\n",
      "cnt: 0 - valLoss: 0.6660163402557373 - trainLoss: 0.6643533110618591\n",
      "cnt: 0 - valLoss: 0.666016161441803 - trainLoss: 0.6643531918525696\n",
      "cnt: 0 - valLoss: 0.6660159826278687 - trainLoss: 0.6643530130386353\n",
      "cnt: 0 - valLoss: 0.6660158634185791 - trainLoss: 0.6643528938293457\n",
      "cnt: 0 - valLoss: 0.6660156846046448 - trainLoss: 0.6643527150154114\n",
      "cnt: 0 - valLoss: 0.6660155653953552 - trainLoss: 0.6643525958061218\n",
      "cnt: 0 - valLoss: 0.6660153865814209 - trainLoss: 0.6643524169921875\n",
      "cnt: 0 - valLoss: 0.6660152077674866 - trainLoss: 0.6643523573875427\n",
      "cnt: 0 - valLoss: 0.6660149693489075 - trainLoss: 0.6643521785736084\n",
      "cnt: 0 - valLoss: 0.6660148501396179 - trainLoss: 0.6643521189689636\n",
      "cnt: 0 - valLoss: 0.6660146713256836 - trainLoss: 0.6643519401550293\n",
      "cnt: 0 - valLoss: 0.666014552116394 - trainLoss: 0.6643518209457397\n",
      "cnt: 0 - valLoss: 0.6660143136978149 - trainLoss: 0.6643516421318054\n",
      "cnt: 0 - valLoss: 0.6660141944885254 - trainLoss: 0.6643515229225159\n",
      "cnt: 0 - valLoss: 0.6660140156745911 - trainLoss: 0.6643514633178711\n",
      "cnt: 0 - valLoss: 0.6660138964653015 - trainLoss: 0.6643513441085815\n",
      "cnt: 0 - valLoss: 0.6660137176513672 - trainLoss: 0.6643510460853577\n",
      "cnt: 0 - valLoss: 0.6660135388374329 - trainLoss: 0.6643509864807129\n",
      "cnt: 0 - valLoss: 0.6660134196281433 - trainLoss: 0.6643509268760681\n",
      "cnt: 0 - valLoss: 0.6660131812095642 - trainLoss: 0.6643507480621338\n",
      "cnt: 0 - valLoss: 0.6660130620002747 - trainLoss: 0.6643506288528442\n",
      "cnt: 0 - valLoss: 0.6660128235816956 - trainLoss: 0.6643504500389099\n",
      "cnt: 0 - valLoss: 0.6660127639770508 - trainLoss: 0.6643503308296204\n",
      "cnt: 0 - valLoss: 0.6660125255584717 - trainLoss: 0.6643502712249756\n",
      "cnt: 0 - valLoss: 0.6660123467445374 - trainLoss: 0.6643500328063965\n",
      "cnt: 0 - valLoss: 0.666012167930603 - trainLoss: 0.6643499732017517\n",
      "cnt: 0 - valLoss: 0.6660120487213135 - trainLoss: 0.6643498539924622\n",
      "cnt: 0 - valLoss: 0.6660118699073792 - trainLoss: 0.6643496751785278\n",
      "cnt: 0 - valLoss: 0.6660117506980896 - trainLoss: 0.6643495559692383\n",
      "cnt: 0 - valLoss: 0.6660115718841553 - trainLoss: 0.664349377155304\n",
      "cnt: 0 - valLoss: 0.6660113334655762 - trainLoss: 0.6643492579460144\n",
      "cnt: 0 - valLoss: 0.6660112738609314 - trainLoss: 0.6643491387367249\n",
      "cnt: 0 - valLoss: 0.6660110950469971 - trainLoss: 0.6643490195274353\n",
      "cnt: 0 - valLoss: 0.6660109162330627 - trainLoss: 0.6643489003181458\n",
      "cnt: 0 - valLoss: 0.6660107374191284 - trainLoss: 0.6643487811088562\n",
      "cnt: 0 - valLoss: 0.6660105586051941 - trainLoss: 0.6643486618995667\n",
      "cnt: 0 - valLoss: 0.6660103797912598 - trainLoss: 0.6643484830856323\n",
      "cnt: 0 - valLoss: 0.6660102605819702 - trainLoss: 0.664348304271698\n",
      "cnt: 0 - valLoss: 0.6660100817680359 - trainLoss: 0.6643482446670532\n",
      "cnt: 0 - valLoss: 0.6660099029541016 - trainLoss: 0.6643481850624084\n",
      "cnt: 0 - valLoss: 0.666009783744812 - trainLoss: 0.6643479466438293\n",
      "cnt: 0 - valLoss: 0.6660096049308777 - trainLoss: 0.6643478274345398\n",
      "cnt: 0 - valLoss: 0.6660094261169434 - trainLoss: 0.6643477082252502\n",
      "cnt: 0 - valLoss: 0.6660093069076538 - trainLoss: 0.6643475890159607\n",
      "cnt: 0 - valLoss: 0.6660091280937195 - trainLoss: 0.6643474698066711\n",
      "cnt: 0 - valLoss: 0.6660089492797852 - trainLoss: 0.6643473505973816\n",
      "cnt: 0 - valLoss: 0.6660087704658508 - trainLoss: 0.6643471717834473\n",
      "cnt: 0 - valLoss: 0.6660085916519165 - trainLoss: 0.6643471121788025\n",
      "cnt: 0 - valLoss: 0.6660084128379822 - trainLoss: 0.6643469333648682\n",
      "cnt: 0 - valLoss: 0.6660082340240479 - trainLoss: 0.6643468141555786\n",
      "cnt: 0 - valLoss: 0.6660081744194031 - trainLoss: 0.6643466353416443\n",
      "cnt: 0 - valLoss: 0.6660079956054688 - trainLoss: 0.6643465161323547\n",
      "cnt: 0 - valLoss: 0.6660078167915344 - trainLoss: 0.66434645652771\n",
      "cnt: 0 - valLoss: 0.6660076379776001 - trainLoss: 0.6643462777137756\n",
      "cnt: 0 - valLoss: 0.6660074591636658 - trainLoss: 0.6643461585044861\n",
      "cnt: 0 - valLoss: 0.6660072803497314 - trainLoss: 0.6643460392951965\n",
      "cnt: 0 - valLoss: 0.6660071611404419 - trainLoss: 0.6643458604812622\n",
      "cnt: 0 - valLoss: 0.6660069823265076 - trainLoss: 0.6643457412719727\n",
      "cnt: 0 - valLoss: 0.6660068035125732 - trainLoss: 0.6643455624580383\n",
      "cnt: 0 - valLoss: 0.6660066843032837 - trainLoss: 0.6643454432487488\n",
      "cnt: 0 - valLoss: 0.6660065054893494 - trainLoss: 0.6643453240394592\n",
      "cnt: 0 - valLoss: 0.666006326675415 - trainLoss: 0.6643452048301697\n",
      "cnt: 0 - valLoss: 0.6660062074661255 - trainLoss: 0.6643450856208801\n",
      "cnt: 0 - valLoss: 0.6660060286521912 - trainLoss: 0.6643449664115906\n",
      "cnt: 0 - valLoss: 0.6660057902336121 - trainLoss: 0.664344847202301\n",
      "cnt: 0 - valLoss: 0.6660056114196777 - trainLoss: 0.6643446683883667\n",
      "cnt: 0 - valLoss: 0.6660054922103882 - trainLoss: 0.6643445491790771\n",
      "cnt: 0 - valLoss: 0.6660053133964539 - trainLoss: 0.6643444299697876\n",
      "cnt: 0 - valLoss: 0.6660051941871643 - trainLoss: 0.664344310760498\n",
      "cnt: 0 - valLoss: 0.66600501537323 - trainLoss: 0.6643441319465637\n",
      "cnt: 0 - valLoss: 0.6660048961639404 - trainLoss: 0.664344072341919\n",
      "cnt: 0 - valLoss: 0.6660047173500061 - trainLoss: 0.6643438935279846\n",
      "cnt: 0 - valLoss: 0.6660045385360718 - trainLoss: 0.6643437743186951\n",
      "cnt: 0 - valLoss: 0.6660043597221375 - trainLoss: 0.6643436551094055\n",
      "cnt: 0 - valLoss: 0.6660041809082031 - trainLoss: 0.664343535900116\n",
      "cnt: 0 - valLoss: 0.6660040020942688 - trainLoss: 0.6643433570861816\n",
      "cnt: 0 - valLoss: 0.6660038232803345 - trainLoss: 0.6643432378768921\n",
      "cnt: 0 - valLoss: 0.6660037040710449 - trainLoss: 0.6643431186676025\n",
      "cnt: 0 - valLoss: 0.6660035252571106 - trainLoss: 0.664342999458313\n",
      "cnt: 0 - valLoss: 0.6660033464431763 - trainLoss: 0.6643428802490234\n",
      "cnt: 0 - valLoss: 0.6660031676292419 - trainLoss: 0.6643427014350891\n",
      "cnt: 0 - valLoss: 0.6660030484199524 - trainLoss: 0.6643425822257996\n",
      "cnt: 0 - valLoss: 0.6660028696060181 - trainLoss: 0.6643425226211548\n",
      "cnt: 0 - valLoss: 0.6660026907920837 - trainLoss: 0.6643423438072205\n",
      "cnt: 0 - valLoss: 0.6660025715827942 - trainLoss: 0.6643421649932861\n",
      "cnt: 0 - valLoss: 0.6660023927688599 - trainLoss: 0.6643420457839966\n",
      "cnt: 0 - valLoss: 0.6660022735595703 - trainLoss: 0.664341926574707\n",
      "cnt: 0 - valLoss: 0.6660020351409912 - trainLoss: 0.6643418073654175\n",
      "cnt: 0 - valLoss: 0.6660018563270569 - trainLoss: 0.6643416285514832\n",
      "cnt: 0 - valLoss: 0.6660017371177673 - trainLoss: 0.6643415689468384\n",
      "cnt: 0 - valLoss: 0.666001558303833 - trainLoss: 0.664341390132904\n",
      "cnt: 0 - valLoss: 0.6660013794898987 - trainLoss: 0.6643412709236145\n",
      "cnt: 0 - valLoss: 0.6660012006759644 - trainLoss: 0.6643410921096802\n",
      "cnt: 0 - valLoss: 0.6660010814666748 - trainLoss: 0.6643410325050354\n",
      "cnt: 0 - valLoss: 0.6660008430480957 - trainLoss: 0.6643408536911011\n",
      "cnt: 0 - valLoss: 0.6660007238388062 - trainLoss: 0.6643407344818115\n",
      "cnt: 0 - valLoss: 0.6660005450248718 - trainLoss: 0.664340615272522\n",
      "cnt: 0 - valLoss: 0.6660004258155823 - trainLoss: 0.6643404364585876\n",
      "cnt: 0 - valLoss: 0.6660001873970032 - trainLoss: 0.6643403768539429\n",
      "cnt: 0 - valLoss: 0.6660000681877136 - trainLoss: 0.6643401384353638\n",
      "cnt: 0 - valLoss: 0.6659998893737793 - trainLoss: 0.664340078830719\n",
      "cnt: 0 - valLoss: 0.665999710559845 - trainLoss: 0.6643399596214294\n",
      "cnt: 0 - valLoss: 0.6659995317459106 - trainLoss: 0.6643398404121399\n",
      "cnt: 0 - valLoss: 0.6659993529319763 - trainLoss: 0.6643396615982056\n",
      "cnt: 0 - valLoss: 0.6659992337226868 - trainLoss: 0.664339542388916\n",
      "cnt: 0 - valLoss: 0.6659990549087524 - trainLoss: 0.6643393635749817\n",
      "cnt: 0 - valLoss: 0.6659989356994629 - trainLoss: 0.6643393039703369\n",
      "cnt: 0 - valLoss: 0.6659987568855286 - trainLoss: 0.6643391847610474\n",
      "cnt: 0 - valLoss: 0.6659985780715942 - trainLoss: 0.664339005947113\n",
      "cnt: 0 - valLoss: 0.6659983992576599 - trainLoss: 0.6643388867378235\n",
      "cnt: 0 - valLoss: 0.6659982204437256 - trainLoss: 0.6643387675285339\n",
      "cnt: 0 - valLoss: 0.665998101234436 - trainLoss: 0.6643385887145996\n",
      "cnt: 0 - valLoss: 0.6659979224205017 - trainLoss: 0.6643384695053101\n",
      "cnt: 0 - valLoss: 0.6659976840019226 - trainLoss: 0.6643382906913757\n",
      "cnt: 0 - valLoss: 0.6659976243972778 - trainLoss: 0.664338231086731\n",
      "cnt: 0 - valLoss: 0.6659973859786987 - trainLoss: 0.6643380522727966\n",
      "cnt: 0 - valLoss: 0.6659972667694092 - trainLoss: 0.6643379926681519\n",
      "cnt: 0 - valLoss: 0.6659970879554749 - trainLoss: 0.6643378734588623\n",
      "cnt: 0 - valLoss: 0.6659969091415405 - trainLoss: 0.664337694644928\n",
      "cnt: 0 - valLoss: 0.665996789932251 - trainLoss: 0.6643375754356384\n",
      "cnt: 0 - valLoss: 0.6659966111183167 - trainLoss: 0.6643373966217041\n",
      "cnt: 0 - valLoss: 0.6659964323043823 - trainLoss: 0.6643372774124146\n",
      "cnt: 0 - valLoss: 0.6659961938858032 - trainLoss: 0.6643372178077698\n",
      "cnt: 0 - valLoss: 0.6659960150718689 - trainLoss: 0.6643370389938354\n",
      "cnt: 0 - valLoss: 0.6659958958625793 - trainLoss: 0.6643368601799011\n",
      "cnt: 0 - valLoss: 0.665995717048645 - trainLoss: 0.6643368005752563\n",
      "cnt: 0 - valLoss: 0.6659955382347107 - trainLoss: 0.664336621761322\n",
      "cnt: 0 - valLoss: 0.6659954190254211 - trainLoss: 0.6643365025520325\n",
      "cnt: 0 - valLoss: 0.6659952402114868 - trainLoss: 0.6643363237380981\n",
      "cnt: 0 - valLoss: 0.6659950613975525 - trainLoss: 0.6643361449241638\n",
      "cnt: 0 - valLoss: 0.6659948825836182 - trainLoss: 0.664336085319519\n",
      "cnt: 0 - valLoss: 0.6659947633743286 - trainLoss: 0.6643359661102295\n",
      "cnt: 0 - valLoss: 0.6659946441650391 - trainLoss: 0.6643357872962952\n",
      "cnt: 0 - valLoss: 0.66599440574646 - trainLoss: 0.6643357276916504\n",
      "cnt: 0 - valLoss: 0.6659942865371704 - trainLoss: 0.6643355488777161\n",
      "cnt: 0 - valLoss: 0.6659941077232361 - trainLoss: 0.6643354296684265\n",
      "cnt: 0 - valLoss: 0.665993869304657 - trainLoss: 0.6643352508544922\n",
      "cnt: 0 - valLoss: 0.6659937500953674 - trainLoss: 0.6643351316452026\n",
      "cnt: 0 - valLoss: 0.6659935712814331 - trainLoss: 0.6643350124359131\n",
      "cnt: 0 - valLoss: 0.6659934520721436 - trainLoss: 0.6643348932266235\n",
      "cnt: 0 - valLoss: 0.6659932732582092 - trainLoss: 0.664334774017334\n",
      "cnt: 0 - valLoss: 0.6659930944442749 - trainLoss: 0.6643346548080444\n",
      "cnt: 0 - valLoss: 0.6659929752349854 - trainLoss: 0.6643344759941101\n",
      "cnt: 0 - valLoss: 0.6659927368164062 - trainLoss: 0.6643343567848206\n",
      "cnt: 0 - valLoss: 0.6659926772117615 - trainLoss: 0.6643341779708862\n",
      "cnt: 0 - valLoss: 0.6659923791885376 - trainLoss: 0.6643341183662415\n",
      "cnt: 0 - valLoss: 0.665992259979248 - trainLoss: 0.6643338799476624\n",
      "cnt: 0 - valLoss: 0.6659921407699585 - trainLoss: 0.6643338799476624\n",
      "cnt: 0 - valLoss: 0.6659919023513794 - trainLoss: 0.6643336415290833\n",
      "cnt: 0 - valLoss: 0.6659917831420898 - trainLoss: 0.6643335819244385\n",
      "cnt: 0 - valLoss: 0.6659916043281555 - trainLoss: 0.6643334627151489\n",
      "cnt: 0 - valLoss: 0.6659914255142212 - trainLoss: 0.6643332839012146\n",
      "cnt: 0 - valLoss: 0.6659912467002869 - trainLoss: 0.664333164691925\n",
      "cnt: 0 - valLoss: 0.6659911274909973 - trainLoss: 0.6643330454826355\n",
      "cnt: 0 - valLoss: 0.665990948677063 - trainLoss: 0.6643328666687012\n",
      "cnt: 0 - valLoss: 0.6659907698631287 - trainLoss: 0.6643327474594116\n",
      "cnt: 0 - valLoss: 0.6659905910491943 - trainLoss: 0.6643326282501221\n",
      "cnt: 0 - valLoss: 0.6659904718399048 - trainLoss: 0.6643325090408325\n",
      "cnt: 0 - valLoss: 0.6659902930259705 - trainLoss: 0.664332389831543\n",
      "cnt: 0 - valLoss: 0.6659900546073914 - trainLoss: 0.6643322110176086\n",
      "cnt: 0 - valLoss: 0.6659899353981018 - trainLoss: 0.6643320918083191\n",
      "cnt: 0 - valLoss: 0.6659897565841675 - trainLoss: 0.6643319129943848\n",
      "cnt: 0 - valLoss: 0.6659896373748779 - trainLoss: 0.6643317937850952\n",
      "cnt: 0 - valLoss: 0.6659894585609436 - trainLoss: 0.6643316745758057\n",
      "cnt: 0 - valLoss: 0.6659892797470093 - trainLoss: 0.6643315553665161\n",
      "cnt: 0 - valLoss: 0.6659891605377197 - trainLoss: 0.6643313765525818\n",
      "cnt: 0 - valLoss: 0.6659889221191406 - trainLoss: 0.664331316947937\n",
      "cnt: 0 - valLoss: 0.6659888029098511 - trainLoss: 0.6643311381340027\n",
      "cnt: 0 - valLoss: 0.6659886240959167 - trainLoss: 0.6643309593200684\n",
      "cnt: 0 - valLoss: 0.6659884452819824 - trainLoss: 0.6643308997154236\n",
      "cnt: 0 - valLoss: 0.6659882664680481 - trainLoss: 0.6643307209014893\n",
      "cnt: 0 - valLoss: 0.6659880876541138 - trainLoss: 0.6643306016921997\n",
      "cnt: 0 - valLoss: 0.6659879088401794 - trainLoss: 0.6643305420875549\n",
      "cnt: 0 - valLoss: 0.6659877896308899 - trainLoss: 0.6643303632736206\n",
      "cnt: 0 - valLoss: 0.6659876108169556 - trainLoss: 0.664330244064331\n",
      "cnt: 0 - valLoss: 0.665987491607666 - trainLoss: 0.6643300652503967\n",
      "cnt: 0 - valLoss: 0.6659873127937317 - trainLoss: 0.6643299460411072\n",
      "cnt: 0 - valLoss: 0.6659871339797974 - trainLoss: 0.6643297672271729\n",
      "cnt: 0 - valLoss: 0.6659870147705078 - trainLoss: 0.6643296480178833\n",
      "cnt: 0 - valLoss: 0.6659867763519287 - trainLoss: 0.6643295884132385\n",
      "cnt: 0 - valLoss: 0.6659866571426392 - trainLoss: 0.6643294095993042\n",
      "cnt: 0 - valLoss: 0.6659864783287048 - trainLoss: 0.6643292307853699\n",
      "cnt: 0 - valLoss: 0.6659862399101257 - trainLoss: 0.6643291115760803\n",
      "cnt: 0 - valLoss: 0.6659861207008362 - trainLoss: 0.6643289923667908\n",
      "cnt: 0 - valLoss: 0.6659859418869019 - trainLoss: 0.6643288135528564\n",
      "cnt: 0 - valLoss: 0.6659858226776123 - trainLoss: 0.6643287539482117\n",
      "cnt: 0 - valLoss: 0.665985643863678 - trainLoss: 0.6643285751342773\n",
      "cnt: 0 - valLoss: 0.6659854650497437 - trainLoss: 0.664328396320343\n",
      "cnt: 0 - valLoss: 0.6659852862358093 - trainLoss: 0.6643283367156982\n",
      "cnt: 0 - valLoss: 0.665985107421875 - trainLoss: 0.6643281579017639\n",
      "cnt: 0 - valLoss: 0.6659849286079407 - trainLoss: 0.6643280386924744\n",
      "cnt: 0 - valLoss: 0.6659848093986511 - trainLoss: 0.6643279194831848\n",
      "cnt: 0 - valLoss: 0.6659846305847168 - trainLoss: 0.6643278002738953\n",
      "cnt: 0 - valLoss: 0.6659845113754272 - trainLoss: 0.6643276214599609\n",
      "cnt: 0 - valLoss: 0.6659843325614929 - trainLoss: 0.6643275022506714\n",
      "cnt: 0 - valLoss: 0.6659840941429138 - trainLoss: 0.6643273830413818\n",
      "cnt: 0 - valLoss: 0.6659839749336243 - trainLoss: 0.6643272042274475\n",
      "cnt: 0 - valLoss: 0.6659837961196899 - trainLoss: 0.664327085018158\n",
      "cnt: 0 - valLoss: 0.6659836173057556 - trainLoss: 0.6643269658088684\n",
      "cnt: 0 - valLoss: 0.6659834980964661 - trainLoss: 0.6643268465995789\n",
      "cnt: 0 - valLoss: 0.6659832000732422 - trainLoss: 0.6643266677856445\n",
      "cnt: 0 - valLoss: 0.6659832000732422 - trainLoss: 0.6643266081809998\n",
      "cnt: 0 - valLoss: 0.6659829616546631 - trainLoss: 0.6643264293670654\n",
      "cnt: 0 - valLoss: 0.6659827828407288 - trainLoss: 0.6643263101577759\n",
      "cnt: 0 - valLoss: 0.6659826040267944 - trainLoss: 0.6643261313438416\n",
      "cnt: 0 - valLoss: 0.6659824252128601 - trainLoss: 0.664326012134552\n",
      "cnt: 0 - valLoss: 0.6659823060035706 - trainLoss: 0.6643258333206177\n",
      "cnt: 0 - valLoss: 0.6659821271896362 - trainLoss: 0.6643257141113281\n",
      "cnt: 0 - valLoss: 0.6659820079803467 - trainLoss: 0.6643255949020386\n",
      "cnt: 0 - valLoss: 0.6659817695617676 - trainLoss: 0.664325475692749\n",
      "cnt: 0 - valLoss: 0.665981650352478 - trainLoss: 0.6643253564834595\n",
      "cnt: 0 - valLoss: 0.6659815311431885 - trainLoss: 0.6643252372741699\n",
      "cnt: 0 - valLoss: 0.6659812927246094 - trainLoss: 0.6643250584602356\n",
      "cnt: 0 - valLoss: 0.665981113910675 - trainLoss: 0.664324939250946\n",
      "cnt: 0 - valLoss: 0.6659809350967407 - trainLoss: 0.6643248200416565\n",
      "cnt: 0 - valLoss: 0.6659808158874512 - trainLoss: 0.6643246412277222\n",
      "cnt: 0 - valLoss: 0.6659805774688721 - trainLoss: 0.6643244624137878\n",
      "cnt: 0 - valLoss: 0.6659804582595825 - trainLoss: 0.6643243432044983\n",
      "cnt: 0 - valLoss: 0.665980339050293 - trainLoss: 0.6643242835998535\n",
      "cnt: 0 - valLoss: 0.6659801602363586 - trainLoss: 0.6643241047859192\n",
      "cnt: 0 - valLoss: 0.6659799814224243 - trainLoss: 0.6643239259719849\n",
      "cnt: 0 - valLoss: 0.66597980260849 - trainLoss: 0.6643238663673401\n",
      "cnt: 0 - valLoss: 0.6659796237945557 - trainLoss: 0.6643237471580505\n",
      "cnt: 0 - valLoss: 0.6659794449806213 - trainLoss: 0.6643235683441162\n",
      "cnt: 0 - valLoss: 0.665979266166687 - trainLoss: 0.6643233895301819\n",
      "cnt: 0 - valLoss: 0.6659790873527527 - trainLoss: 0.6643232703208923\n",
      "cnt: 0 - valLoss: 0.6659789085388184 - trainLoss: 0.6643231511116028\n",
      "cnt: 0 - valLoss: 0.6659787893295288 - trainLoss: 0.6643230319023132\n",
      "cnt: 0 - valLoss: 0.6659786105155945 - trainLoss: 0.6643228530883789\n",
      "cnt: 0 - valLoss: 0.6659784913063049 - trainLoss: 0.6643227338790894\n",
      "cnt: 0 - valLoss: 0.6659782528877258 - trainLoss: 0.6643226146697998\n",
      "cnt: 0 - valLoss: 0.665978193283081 - trainLoss: 0.6643224954605103\n",
      "cnt: 0 - valLoss: 0.6659780144691467 - trainLoss: 0.6643223166465759\n",
      "cnt: 0 - valLoss: 0.6659778356552124 - trainLoss: 0.6643221974372864\n",
      "cnt: 0 - valLoss: 0.6659776568412781 - trainLoss: 0.6643220782279968\n",
      "cnt: 0 - valLoss: 0.6659774780273438 - trainLoss: 0.6643219590187073\n",
      "cnt: 0 - valLoss: 0.6659772992134094 - trainLoss: 0.664321780204773\n",
      "cnt: 0 - valLoss: 0.6659771203994751 - trainLoss: 0.6643216609954834\n",
      "cnt: 0 - valLoss: 0.6659769415855408 - trainLoss: 0.6643215417861938\n",
      "cnt: 0 - valLoss: 0.6659767627716064 - trainLoss: 0.6643214225769043\n",
      "cnt: 0 - valLoss: 0.6659766435623169 - trainLoss: 0.66432124376297\n",
      "cnt: 0 - valLoss: 0.6659764647483826 - trainLoss: 0.6643211245536804\n",
      "cnt: 0 - valLoss: 0.665976345539093 - trainLoss: 0.6643210053443909\n",
      "cnt: 0 - valLoss: 0.6659761071205139 - trainLoss: 0.6643208265304565\n",
      "cnt: 0 - valLoss: 0.6659759283065796 - trainLoss: 0.664320707321167\n",
      "cnt: 0 - valLoss: 0.6659757494926453 - trainLoss: 0.6643205285072327\n",
      "cnt: 0 - valLoss: 0.6659755706787109 - trainLoss: 0.6643204092979431\n",
      "cnt: 0 - valLoss: 0.6659753918647766 - trainLoss: 0.6643203496932983\n",
      "cnt: 0 - valLoss: 0.6659752726554871 - trainLoss: 0.664320170879364\n",
      "cnt: 0 - valLoss: 0.6659750938415527 - trainLoss: 0.6643200516700745\n",
      "cnt: 0 - valLoss: 0.6659749150276184 - trainLoss: 0.6643198728561401\n",
      "cnt: 0 - valLoss: 0.6659747362136841 - trainLoss: 0.6643197536468506\n",
      "cnt: 0 - valLoss: 0.6659745573997498 - trainLoss: 0.664319634437561\n",
      "cnt: 0 - valLoss: 0.6659743785858154 - trainLoss: 0.6643194556236267\n",
      "cnt: 0 - valLoss: 0.6659741997718811 - trainLoss: 0.6643193364143372\n",
      "cnt: 0 - valLoss: 0.6659740209579468 - trainLoss: 0.6643191576004028\n",
      "cnt: 0 - valLoss: 0.6659739017486572 - trainLoss: 0.6643190979957581\n",
      "cnt: 0 - valLoss: 0.6659737229347229 - trainLoss: 0.6643189191818237\n",
      "cnt: 0 - valLoss: 0.6659736037254333 - trainLoss: 0.6643187999725342\n",
      "cnt: 0 - valLoss: 0.665973424911499 - trainLoss: 0.6643186211585999\n",
      "cnt: 0 - valLoss: 0.6659732460975647 - trainLoss: 0.6643185615539551\n",
      "cnt: 0 - valLoss: 0.6659731268882751 - trainLoss: 0.6643183827400208\n",
      "cnt: 0 - valLoss: 0.6659728288650513 - trainLoss: 0.6643182635307312\n",
      "cnt: 0 - valLoss: 0.6659727692604065 - trainLoss: 0.6643180847167969\n",
      "cnt: 0 - valLoss: 0.6659725904464722 - trainLoss: 0.6643179655075073\n",
      "cnt: 0 - valLoss: 0.6659724116325378 - trainLoss: 0.6643178462982178\n",
      "cnt: 0 - valLoss: 0.6659722328186035 - trainLoss: 0.6643176674842834\n",
      "cnt: 0 - valLoss: 0.6659720540046692 - trainLoss: 0.6643175482749939\n",
      "cnt: 0 - valLoss: 0.6659719347953796 - trainLoss: 0.6643174290657043\n",
      "cnt: 0 - valLoss: 0.6659717559814453 - trainLoss: 0.66431725025177\n",
      "cnt: 0 - valLoss: 0.665971577167511 - trainLoss: 0.6643171310424805\n",
      "cnt: 0 - valLoss: 0.6659714579582214 - trainLoss: 0.6643170118331909\n",
      "cnt: 0 - valLoss: 0.6659712791442871 - trainLoss: 0.6643168926239014\n",
      "cnt: 0 - valLoss: 0.665971040725708 - trainLoss: 0.664316713809967\n",
      "cnt: 0 - valLoss: 0.6659709215164185 - trainLoss: 0.6643165946006775\n",
      "cnt: 0 - valLoss: 0.6659707427024841 - trainLoss: 0.6643164157867432\n",
      "cnt: 0 - valLoss: 0.6659705638885498 - trainLoss: 0.6643162965774536\n",
      "cnt: 0 - valLoss: 0.6659703850746155 - trainLoss: 0.6643161177635193\n",
      "cnt: 0 - valLoss: 0.6659702658653259 - trainLoss: 0.6643159985542297\n",
      "cnt: 0 - valLoss: 0.6659700870513916 - trainLoss: 0.6643158197402954\n",
      "cnt: 0 - valLoss: 0.6659699082374573 - trainLoss: 0.6643157601356506\n",
      "cnt: 0 - valLoss: 0.6659697890281677 - trainLoss: 0.6643155813217163\n",
      "cnt: 0 - valLoss: 0.6659696102142334 - trainLoss: 0.6643154621124268\n",
      "cnt: 0 - valLoss: 0.6659694314002991 - trainLoss: 0.6643152832984924\n",
      "cnt: 0 - valLoss: 0.6659693121910095 - trainLoss: 0.6643151640892029\n",
      "cnt: 0 - valLoss: 0.6659690737724304 - trainLoss: 0.6643150448799133\n",
      "cnt: 0 - valLoss: 0.6659689545631409 - trainLoss: 0.6643149256706238\n",
      "cnt: 0 - valLoss: 0.6659687161445618 - trainLoss: 0.6643147468566895\n",
      "cnt: 0 - valLoss: 0.6659685373306274 - trainLoss: 0.6643146276473999\n",
      "cnt: 0 - valLoss: 0.6659684181213379 - trainLoss: 0.6643144488334656\n",
      "cnt: 0 - valLoss: 0.6659682393074036 - trainLoss: 0.664314329624176\n",
      "cnt: 0 - valLoss: 0.665968120098114 - trainLoss: 0.6643141508102417\n",
      "cnt: 0 - valLoss: 0.6659679412841797 - trainLoss: 0.6643140316009521\n",
      "cnt: 0 - valLoss: 0.6659677624702454 - trainLoss: 0.6643139123916626\n",
      "cnt: 0 - valLoss: 0.6659675240516663 - trainLoss: 0.6643137335777283\n",
      "cnt: 0 - valLoss: 0.6659674048423767 - trainLoss: 0.6643136143684387\n",
      "cnt: 0 - valLoss: 0.6659672260284424 - trainLoss: 0.6643134951591492\n",
      "cnt: 0 - valLoss: 0.6659670472145081 - trainLoss: 0.6643133759498596\n",
      "cnt: 0 - valLoss: 0.6659669280052185 - trainLoss: 0.6643132567405701\n",
      "cnt: 0 - valLoss: 0.6659667491912842 - trainLoss: 0.6643130779266357\n",
      "cnt: 0 - valLoss: 0.6659665703773499 - trainLoss: 0.6643129587173462\n",
      "cnt: 0 - valLoss: 0.6659663915634155 - trainLoss: 0.6643128395080566\n",
      "cnt: 0 - valLoss: 0.665966272354126 - trainLoss: 0.6643126606941223\n",
      "cnt: 0 - valLoss: 0.6659660935401917 - trainLoss: 0.6643125414848328\n",
      "cnt: 0 - valLoss: 0.6659659147262573 - trainLoss: 0.6643123626708984\n",
      "cnt: 0 - valLoss: 0.665965735912323 - trainLoss: 0.6643122434616089\n",
      "cnt: 0 - valLoss: 0.6659655570983887 - trainLoss: 0.6643121242523193\n",
      "cnt: 0 - valLoss: 0.6659653782844543 - trainLoss: 0.664311945438385\n",
      "cnt: 0 - valLoss: 0.66596519947052 - trainLoss: 0.6643118858337402\n",
      "cnt: 0 - valLoss: 0.6659650802612305 - trainLoss: 0.6643117070198059\n",
      "cnt: 0 - valLoss: 0.6659649014472961 - trainLoss: 0.6643115878105164\n",
      "cnt: 0 - valLoss: 0.6659647226333618 - trainLoss: 0.664311408996582\n",
      "cnt: 0 - valLoss: 0.6659646034240723 - trainLoss: 0.6643112897872925\n",
      "cnt: 0 - valLoss: 0.6659644246101379 - trainLoss: 0.6643111109733582\n",
      "cnt: 0 - valLoss: 0.6659643054008484 - trainLoss: 0.6643109917640686\n",
      "cnt: 0 - valLoss: 0.6659640669822693 - trainLoss: 0.664310872554779\n",
      "cnt: 0 - valLoss: 0.665963888168335 - trainLoss: 0.6643107533454895\n",
      "cnt: 0 - valLoss: 0.6659637689590454 - trainLoss: 0.6643105745315552\n",
      "cnt: 0 - valLoss: 0.6659635901451111 - trainLoss: 0.6643103957176208\n",
      "cnt: 0 - valLoss: 0.6659634113311768 - trainLoss: 0.6643102765083313\n",
      "cnt: 0 - valLoss: 0.6659632325172424 - trainLoss: 0.6643101572990417\n",
      "cnt: 0 - valLoss: 0.6659630537033081 - trainLoss: 0.6643099784851074\n",
      "cnt: 0 - valLoss: 0.6659628748893738 - trainLoss: 0.6643099188804626\n",
      "cnt: 0 - valLoss: 0.6659627556800842 - trainLoss: 0.6643097400665283\n",
      "cnt: 0 - valLoss: 0.6659625172615051 - trainLoss: 0.6643096208572388\n",
      "cnt: 0 - valLoss: 0.6659623980522156 - trainLoss: 0.6643094420433044\n",
      "cnt: 0 - valLoss: 0.6659622192382812 - trainLoss: 0.6643093228340149\n",
      "cnt: 0 - valLoss: 0.6659620404243469 - trainLoss: 0.6643091440200806\n",
      "cnt: 0 - valLoss: 0.6659618616104126 - trainLoss: 0.664309024810791\n",
      "cnt: 0 - valLoss: 0.6659618020057678 - trainLoss: 0.6643089056015015\n",
      "cnt: 0 - valLoss: 0.665961503982544 - trainLoss: 0.6643087863922119\n",
      "cnt: 0 - valLoss: 0.6659613847732544 - trainLoss: 0.6643086075782776\n",
      "cnt: 0 - valLoss: 0.6659612059593201 - trainLoss: 0.664308488368988\n",
      "cnt: 0 - valLoss: 0.6659610867500305 - trainLoss: 0.6643083095550537\n",
      "cnt: 0 - valLoss: 0.6659609079360962 - trainLoss: 0.6643081903457642\n",
      "cnt: 0 - valLoss: 0.6659606695175171 - trainLoss: 0.6643080711364746\n",
      "cnt: 0 - valLoss: 0.6659606099128723 - trainLoss: 0.6643079519271851\n",
      "cnt: 0 - valLoss: 0.6659603714942932 - trainLoss: 0.6643077731132507\n",
      "cnt: 0 - valLoss: 0.6659602522850037 - trainLoss: 0.6643075942993164\n",
      "cnt: 0 - valLoss: 0.6659600734710693 - trainLoss: 0.6643074750900269\n",
      "cnt: 0 - valLoss: 0.6659598350524902 - trainLoss: 0.6643072962760925\n",
      "cnt: 0 - valLoss: 0.6659597158432007 - trainLoss: 0.664307177066803\n",
      "cnt: 0 - valLoss: 0.6659595370292664 - trainLoss: 0.6643070578575134\n",
      "cnt: 0 - valLoss: 0.6659594178199768 - trainLoss: 0.6643069386482239\n",
      "cnt: 0 - valLoss: 0.6659591794013977 - trainLoss: 0.6643067598342896\n",
      "cnt: 0 - valLoss: 0.6659590601921082 - trainLoss: 0.664306640625\n",
      "cnt: 0 - valLoss: 0.6659588813781738 - trainLoss: 0.6643065214157104\n",
      "cnt: 0 - valLoss: 0.6659587025642395 - trainLoss: 0.6643063426017761\n",
      "cnt: 0 - valLoss: 0.66595858335495 - trainLoss: 0.6643062233924866\n",
      "cnt: 0 - valLoss: 0.6659583449363708 - trainLoss: 0.6643060445785522\n",
      "cnt: 0 - valLoss: 0.6659581661224365 - trainLoss: 0.6643059849739075\n",
      "cnt: 0 - valLoss: 0.665958046913147 - trainLoss: 0.6643058061599731\n",
      "cnt: 0 - valLoss: 0.6659578680992126 - trainLoss: 0.6643056869506836\n",
      "cnt: 0 - valLoss: 0.6659576892852783 - trainLoss: 0.6643055081367493\n",
      "cnt: 0 - valLoss: 0.665957510471344 - trainLoss: 0.6643053889274597\n",
      "cnt: 0 - valLoss: 0.6659573316574097 - trainLoss: 0.6643052101135254\n",
      "cnt: 0 - valLoss: 0.6659571528434753 - trainLoss: 0.6643050909042358\n",
      "cnt: 0 - valLoss: 0.665956974029541 - trainLoss: 0.6643049120903015\n",
      "cnt: 0 - valLoss: 0.6659568548202515 - trainLoss: 0.664304792881012\n",
      "cnt: 0 - valLoss: 0.6659566760063171 - trainLoss: 0.6643046736717224\n",
      "cnt: 0 - valLoss: 0.6659564971923828 - trainLoss: 0.6643045544624329\n",
      "cnt: 0 - valLoss: 0.6659563183784485 - trainLoss: 0.6643043160438538\n",
      "cnt: 0 - valLoss: 0.6659561395645142 - trainLoss: 0.6643043160438538\n",
      "cnt: 0 - valLoss: 0.6659559607505798 - trainLoss: 0.6643041372299194\n",
      "cnt: 0 - valLoss: 0.6659557819366455 - trainLoss: 0.6643039584159851\n",
      "cnt: 0 - valLoss: 0.6659556031227112 - trainLoss: 0.6643038988113403\n",
      "cnt: 0 - valLoss: 0.6659554243087769 - trainLoss: 0.6643036603927612\n",
      "cnt: 0 - valLoss: 0.6659552454948425 - trainLoss: 0.6643036007881165\n",
      "cnt: 0 - valLoss: 0.665955126285553 - trainLoss: 0.6643034219741821\n",
      "cnt: 0 - valLoss: 0.6659549474716187 - trainLoss: 0.6643032431602478\n",
      "cnt: 0 - valLoss: 0.6659548282623291 - trainLoss: 0.6643031239509583\n",
      "cnt: 0 - valLoss: 0.66595458984375 - trainLoss: 0.6643030643463135\n",
      "cnt: 0 - valLoss: 0.6659544110298157 - trainLoss: 0.6643028259277344\n",
      "cnt: 0 - valLoss: 0.6659542322158813 - trainLoss: 0.6643027663230896\n",
      "cnt: 0 - valLoss: 0.665954053401947 - trainLoss: 0.6643025875091553\n",
      "cnt: 0 - valLoss: 0.6659538745880127 - trainLoss: 0.6643024682998657\n",
      "cnt: 0 - valLoss: 0.6659537553787231 - trainLoss: 0.6643022894859314\n",
      "cnt: 0 - valLoss: 0.6659535765647888 - trainLoss: 0.6643021702766418\n",
      "cnt: 0 - valLoss: 0.6659533977508545 - trainLoss: 0.6643019914627075\n",
      "cnt: 0 - valLoss: 0.6659532189369202 - trainLoss: 0.664301872253418\n",
      "cnt: 0 - valLoss: 0.6659529805183411 - trainLoss: 0.6643017530441284\n",
      "cnt: 0 - valLoss: 0.6659528613090515 - trainLoss: 0.6643016338348389\n",
      "cnt: 0 - valLoss: 0.6659526824951172 - trainLoss: 0.6643015146255493\n",
      "cnt: 0 - valLoss: 0.6659525036811829 - trainLoss: 0.664301335811615\n",
      "cnt: 0 - valLoss: 0.6659523248672485 - trainLoss: 0.6643012166023254\n",
      "cnt: 0 - valLoss: 0.665952205657959 - trainLoss: 0.6643010377883911\n",
      "cnt: 0 - valLoss: 0.6659520268440247 - trainLoss: 0.6643009185791016\n",
      "cnt: 0 - valLoss: 0.6659518480300903 - trainLoss: 0.664300799369812\n",
      "cnt: 0 - valLoss: 0.665951669216156 - trainLoss: 0.6643006205558777\n",
      "cnt: 0 - valLoss: 0.6659514904022217 - trainLoss: 0.6643005013465881\n",
      "cnt: 0 - valLoss: 0.6659513115882874 - trainLoss: 0.6643003225326538\n",
      "cnt: 0 - valLoss: 0.665951132774353 - trainLoss: 0.6643002033233643\n",
      "cnt: 0 - valLoss: 0.6659510135650635 - trainLoss: 0.6643001437187195\n",
      "cnt: 0 - valLoss: 0.6659508347511292 - trainLoss: 0.6642999053001404\n",
      "cnt: 0 - valLoss: 0.6659506559371948 - trainLoss: 0.6642997860908508\n",
      "cnt: 0 - valLoss: 0.6659504771232605 - trainLoss: 0.6642996072769165\n",
      "cnt: 0 - valLoss: 0.6659502387046814 - trainLoss: 0.664299488067627\n",
      "cnt: 0 - valLoss: 0.6659500598907471 - trainLoss: 0.6642993688583374\n",
      "cnt: 0 - valLoss: 0.6659499406814575 - trainLoss: 0.6642991900444031\n",
      "cnt: 0 - valLoss: 0.6659497618675232 - trainLoss: 0.6642991304397583\n",
      "cnt: 0 - valLoss: 0.6659495234489441 - trainLoss: 0.664298951625824\n",
      "cnt: 0 - valLoss: 0.6659494638442993 - trainLoss: 0.6642988324165344\n",
      "cnt: 0 - valLoss: 0.665949285030365 - trainLoss: 0.6642986536026001\n",
      "cnt: 0 - valLoss: 0.6659490466117859 - trainLoss: 0.6642985343933105\n",
      "cnt: 0 - valLoss: 0.6659488677978516 - trainLoss: 0.6642983555793762\n",
      "cnt: 0 - valLoss: 0.6659486889839172 - trainLoss: 0.6642982363700867\n",
      "cnt: 0 - valLoss: 0.6659485101699829 - trainLoss: 0.6642980575561523\n",
      "cnt: 0 - valLoss: 0.6659483909606934 - trainLoss: 0.6642979383468628\n",
      "cnt: 0 - valLoss: 0.665948212146759 - trainLoss: 0.6642978191375732\n",
      "cnt: 0 - valLoss: 0.6659480333328247 - trainLoss: 0.6642976999282837\n",
      "cnt: 0 - valLoss: 0.6659479141235352 - trainLoss: 0.6642975807189941\n",
      "cnt: 0 - valLoss: 0.665947675704956 - trainLoss: 0.6642974019050598\n",
      "cnt: 0 - valLoss: 0.6659475564956665 - trainLoss: 0.6642972230911255\n",
      "cnt: 0 - valLoss: 0.6659473776817322 - trainLoss: 0.6642971634864807\n",
      "cnt: 0 - valLoss: 0.6659471988677979 - trainLoss: 0.6642969846725464\n",
      "cnt: 0 - valLoss: 0.6659470200538635 - trainLoss: 0.6642968654632568\n",
      "cnt: 0 - valLoss: 0.6659467816352844 - trainLoss: 0.6642966866493225\n",
      "cnt: 0 - valLoss: 0.6659466624259949 - trainLoss: 0.664296567440033\n",
      "cnt: 0 - valLoss: 0.6659464836120605 - trainLoss: 0.6642963886260986\n",
      "cnt: 0 - valLoss: 0.6659463047981262 - trainLoss: 0.6642962694168091\n",
      "cnt: 0 - valLoss: 0.6659461259841919 - trainLoss: 0.6642960906028748\n",
      "cnt: 0 - valLoss: 0.6659459471702576 - trainLoss: 0.6642959117889404\n",
      "cnt: 0 - valLoss: 0.6659457683563232 - trainLoss: 0.6642958521842957\n",
      "cnt: 0 - valLoss: 0.6659456491470337 - trainLoss: 0.6642957329750061\n",
      "cnt: 0 - valLoss: 0.6659455299377441 - trainLoss: 0.6642956137657166\n",
      "cnt: 0 - valLoss: 0.6659452319145203 - trainLoss: 0.6642954349517822\n",
      "cnt: 0 - valLoss: 0.6659451127052307 - trainLoss: 0.6642953157424927\n",
      "cnt: 0 - valLoss: 0.6659448742866516 - trainLoss: 0.6642951369285583\n",
      "cnt: 0 - valLoss: 0.6659447550773621 - trainLoss: 0.6642950177192688\n",
      "cnt: 0 - valLoss: 0.6659445762634277 - trainLoss: 0.6642948985099792\n",
      "cnt: 0 - valLoss: 0.6659443974494934 - trainLoss: 0.6642947196960449\n",
      "cnt: 0 - valLoss: 0.6659442186355591 - trainLoss: 0.6642946004867554\n",
      "cnt: 0 - valLoss: 0.6659440398216248 - trainLoss: 0.664294421672821\n",
      "cnt: 0 - valLoss: 0.6659438610076904 - trainLoss: 0.6642943024635315\n",
      "cnt: 0 - valLoss: 0.6659436821937561 - trainLoss: 0.6642941832542419\n",
      "cnt: 0 - valLoss: 0.6659435033798218 - trainLoss: 0.6642940044403076\n",
      "cnt: 0 - valLoss: 0.6659433841705322 - trainLoss: 0.6642938256263733\n",
      "cnt: 0 - valLoss: 0.6659432053565979 - trainLoss: 0.6642937064170837\n",
      "cnt: 0 - valLoss: 0.6659429669380188 - trainLoss: 0.664293646812439\n",
      "cnt: 0 - valLoss: 0.6659428477287292 - trainLoss: 0.6642934083938599\n",
      "cnt: 0 - valLoss: 0.6659426689147949 - trainLoss: 0.6642932891845703\n",
      "cnt: 0 - valLoss: 0.6659424304962158 - trainLoss: 0.6642931699752808\n",
      "cnt: 0 - valLoss: 0.6659422516822815 - trainLoss: 0.6642930507659912\n",
      "cnt: 0 - valLoss: 0.6659421324729919 - trainLoss: 0.6642928719520569\n",
      "cnt: 0 - valLoss: 0.6659418940544128 - trainLoss: 0.6642926931381226\n",
      "cnt: 0 - valLoss: 0.6659417152404785 - trainLoss: 0.664292573928833\n",
      "cnt: 0 - valLoss: 0.6659415364265442 - trainLoss: 0.6642924547195435\n",
      "cnt: 0 - valLoss: 0.6659414172172546 - trainLoss: 0.6642922759056091\n",
      "cnt: 0 - valLoss: 0.6659412384033203 - trainLoss: 0.6642921566963196\n",
      "cnt: 0 - valLoss: 0.665941059589386 - trainLoss: 0.6642919778823853\n",
      "cnt: 0 - valLoss: 0.6659408211708069 - trainLoss: 0.6642919182777405\n",
      "cnt: 0 - valLoss: 0.6659407019615173 - trainLoss: 0.6642917394638062\n",
      "cnt: 0 - valLoss: 0.665940523147583 - trainLoss: 0.6642915606498718\n",
      "cnt: 0 - valLoss: 0.6659403443336487 - trainLoss: 0.6642914414405823\n",
      "cnt: 0 - valLoss: 0.6659401655197144 - trainLoss: 0.6642913222312927\n",
      "cnt: 0 - valLoss: 0.6659400463104248 - trainLoss: 0.6642911434173584\n",
      "cnt: 0 - valLoss: 0.6659398078918457 - trainLoss: 0.6642909646034241\n",
      "cnt: 0 - valLoss: 0.6659396290779114 - trainLoss: 0.6642909049987793\n",
      "cnt: 0 - valLoss: 0.665939450263977 - trainLoss: 0.6642907857894897\n",
      "cnt: 0 - valLoss: 0.6659392714500427 - trainLoss: 0.6642906069755554\n",
      "cnt: 0 - valLoss: 0.6659390926361084 - trainLoss: 0.6642904877662659\n",
      "cnt: 0 - valLoss: 0.6659389734268188 - trainLoss: 0.6642903089523315\n",
      "cnt: 0 - valLoss: 0.6659387350082397 - trainLoss: 0.664290189743042\n",
      "cnt: 0 - valLoss: 0.6659385561943054 - trainLoss: 0.6642900109291077\n",
      "cnt: 0 - valLoss: 0.6659383773803711 - trainLoss: 0.6642898917198181\n",
      "cnt: 0 - valLoss: 0.6659381985664368 - trainLoss: 0.6642897129058838\n",
      "cnt: 0 - valLoss: 0.6659380197525024 - trainLoss: 0.6642895936965942\n",
      "cnt: 0 - valLoss: 0.6659379005432129 - trainLoss: 0.6642894148826599\n",
      "cnt: 0 - valLoss: 0.6659376621246338 - trainLoss: 0.6642892956733704\n",
      "cnt: 0 - valLoss: 0.6659375429153442 - trainLoss: 0.6642891764640808\n",
      "cnt: 0 - valLoss: 0.6659373044967651 - trainLoss: 0.6642890572547913\n",
      "cnt: 0 - valLoss: 0.6659371256828308 - trainLoss: 0.6642889380455017\n",
      "cnt: 0 - valLoss: 0.6659369468688965 - trainLoss: 0.6642887592315674\n",
      "cnt: 0 - valLoss: 0.6659367680549622 - trainLoss: 0.6642886400222778\n",
      "cnt: 0 - valLoss: 0.6659365892410278 - trainLoss: 0.6642885208129883\n",
      "cnt: 0 - valLoss: 0.6659364104270935 - trainLoss: 0.664288341999054\n",
      "cnt: 0 - valLoss: 0.665936291217804 - trainLoss: 0.6642882227897644\n",
      "cnt: 0 - valLoss: 0.6659360527992249 - trainLoss: 0.6642880439758301\n",
      "cnt: 0 - valLoss: 0.6659358739852905 - trainLoss: 0.6642878651618958\n",
      "cnt: 0 - valLoss: 0.665935754776001 - trainLoss: 0.6642877459526062\n",
      "cnt: 0 - valLoss: 0.6659355759620667 - trainLoss: 0.6642876267433167\n",
      "cnt: 0 - valLoss: 0.6659353971481323 - trainLoss: 0.6642874479293823\n",
      "cnt: 0 - valLoss: 0.665935218334198 - trainLoss: 0.6642873883247375\n",
      "cnt: 0 - valLoss: 0.6659349799156189 - trainLoss: 0.6642871499061584\n",
      "cnt: 0 - valLoss: 0.6659348607063293 - trainLoss: 0.6642870306968689\n",
      "cnt: 0 - valLoss: 0.665934681892395 - trainLoss: 0.6642869114875793\n",
      "cnt: 0 - valLoss: 0.6659345030784607 - trainLoss: 0.6642867922782898\n",
      "cnt: 0 - valLoss: 0.6659342646598816 - trainLoss: 0.6642866134643555\n",
      "cnt: 0 - valLoss: 0.6659340858459473 - trainLoss: 0.6642864942550659\n",
      "cnt: 0 - valLoss: 0.6659339070320129 - trainLoss: 0.6642863154411316\n",
      "cnt: 0 - valLoss: 0.6659337878227234 - trainLoss: 0.664286196231842\n",
      "cnt: 0 - valLoss: 0.6659336090087891 - trainLoss: 0.6642860770225525\n",
      "cnt: 0 - valLoss: 0.6659334301948547 - trainLoss: 0.6642858982086182\n",
      "cnt: 0 - valLoss: 0.6659332513809204 - trainLoss: 0.6642857789993286\n",
      "cnt: 0 - valLoss: 0.6659330129623413 - trainLoss: 0.6642856597900391\n",
      "cnt: 0 - valLoss: 0.6659328937530518 - trainLoss: 0.6642854809761047\n",
      "cnt: 0 - valLoss: 0.6659327149391174 - trainLoss: 0.6642853021621704\n",
      "cnt: 0 - valLoss: 0.6659325361251831 - trainLoss: 0.6642851829528809\n",
      "cnt: 0 - valLoss: 0.6659323573112488 - trainLoss: 0.6642850637435913\n",
      "cnt: 0 - valLoss: 0.6659321784973145 - trainLoss: 0.664284884929657\n",
      "cnt: 0 - valLoss: 0.6659319400787354 - trainLoss: 0.6642847657203674\n",
      "cnt: 0 - valLoss: 0.665931761264801 - trainLoss: 0.6642846465110779\n",
      "cnt: 0 - valLoss: 0.6659316420555115 - trainLoss: 0.6642844676971436\n",
      "cnt: 0 - valLoss: 0.6659314632415771 - trainLoss: 0.664284348487854\n",
      "cnt: 0 - valLoss: 0.665931224822998 - trainLoss: 0.6642842292785645\n",
      "cnt: 0 - valLoss: 0.6659311056137085 - trainLoss: 0.6642840504646301\n",
      "cnt: 0 - valLoss: 0.6659308671951294 - trainLoss: 0.6642839312553406\n",
      "cnt: 0 - valLoss: 0.6659306883811951 - trainLoss: 0.664283812046051\n",
      "cnt: 0 - valLoss: 0.6659305691719055 - trainLoss: 0.6642836332321167\n",
      "cnt: 0 - valLoss: 0.6659303307533264 - trainLoss: 0.6642835140228271\n",
      "cnt: 0 - valLoss: 0.6659301519393921 - trainLoss: 0.6642833352088928\n",
      "cnt: 0 - valLoss: 0.6659299731254578 - trainLoss: 0.6642831563949585\n",
      "cnt: 0 - valLoss: 0.6659298539161682 - trainLoss: 0.664283037185669\n",
      "cnt: 0 - valLoss: 0.6659296154975891 - trainLoss: 0.6642829179763794\n",
      "cnt: 0 - valLoss: 0.6659294962882996 - trainLoss: 0.6642827391624451\n",
      "cnt: 0 - valLoss: 0.6659293174743652 - trainLoss: 0.6642826199531555\n",
      "cnt: 0 - valLoss: 0.6659291386604309 - trainLoss: 0.664282500743866\n",
      "cnt: 0 - valLoss: 0.6659289598464966 - trainLoss: 0.6642823219299316\n",
      "cnt: 0 - valLoss: 0.6659287810325623 - trainLoss: 0.6642822027206421\n",
      "cnt: 0 - valLoss: 0.6659286022186279 - trainLoss: 0.6642820239067078\n",
      "cnt: 0 - valLoss: 0.6659284234046936 - trainLoss: 0.664281964302063\n",
      "cnt: 0 - valLoss: 0.6659282445907593 - trainLoss: 0.6642817258834839\n",
      "cnt: 0 - valLoss: 0.6659280061721802 - trainLoss: 0.6642816066741943\n",
      "cnt: 0 - valLoss: 0.6659278273582458 - trainLoss: 0.6642814874649048\n",
      "cnt: 0 - valLoss: 0.6659276485443115 - trainLoss: 0.6642813682556152\n",
      "cnt: 0 - valLoss: 0.6659274697303772 - trainLoss: 0.6642811894416809\n",
      "cnt: 0 - valLoss: 0.6659273505210876 - trainLoss: 0.6642810702323914\n",
      "cnt: 0 - valLoss: 0.6659271717071533 - trainLoss: 0.664280891418457\n",
      "cnt: 0 - valLoss: 0.6659269332885742 - trainLoss: 0.6642807722091675\n",
      "cnt: 0 - valLoss: 0.6659268140792847 - trainLoss: 0.6642805933952332\n",
      "cnt: 0 - valLoss: 0.6659266352653503 - trainLoss: 0.6642804741859436\n",
      "cnt: 0 - valLoss: 0.6659263968467712 - trainLoss: 0.6642802953720093\n",
      "cnt: 0 - valLoss: 0.6659262180328369 - trainLoss: 0.6642801761627197\n",
      "cnt: 0 - valLoss: 0.6659260392189026 - trainLoss: 0.6642799973487854\n",
      "cnt: 0 - valLoss: 0.6659258604049683 - trainLoss: 0.6642798185348511\n",
      "cnt: 0 - valLoss: 0.6659256815910339 - trainLoss: 0.6642797589302063\n",
      "cnt: 0 - valLoss: 0.6659255027770996 - trainLoss: 0.6642796397209167\n",
      "cnt: 0 - valLoss: 0.6659253239631653 - trainLoss: 0.6642794013023376\n",
      "cnt: 0 - valLoss: 0.665925145149231 - trainLoss: 0.6642792820930481\n",
      "cnt: 0 - valLoss: 0.6659249067306519 - trainLoss: 0.6642791628837585\n",
      "cnt: 0 - valLoss: 0.6659247875213623 - trainLoss: 0.6642789840698242\n",
      "cnt: 0 - valLoss: 0.665924608707428 - trainLoss: 0.6642788052558899\n",
      "cnt: 0 - valLoss: 0.6659244298934937 - trainLoss: 0.6642787456512451\n",
      "cnt: 0 - valLoss: 0.6659243106842041 - trainLoss: 0.6642785668373108\n",
      "cnt: 0 - valLoss: 0.665924072265625 - trainLoss: 0.6642784476280212\n",
      "cnt: 0 - valLoss: 0.6659238934516907 - trainLoss: 0.6642782688140869\n",
      "cnt: 0 - valLoss: 0.6659237146377563 - trainLoss: 0.6642781496047974\n",
      "cnt: 0 - valLoss: 0.665923535823822 - trainLoss: 0.6642780303955078\n",
      "cnt: 0 - valLoss: 0.6659233570098877 - trainLoss: 0.6642777919769287\n",
      "cnt: 0 - valLoss: 0.6659231781959534 - trainLoss: 0.6642777323722839\n",
      "cnt: 0 - valLoss: 0.665922999382019 - trainLoss: 0.6642775535583496\n",
      "cnt: 0 - valLoss: 0.6659228205680847 - trainLoss: 0.6642774343490601\n",
      "cnt: 0 - valLoss: 0.6659226417541504 - trainLoss: 0.6642772555351257\n",
      "cnt: 0 - valLoss: 0.6659224629402161 - trainLoss: 0.6642771363258362\n",
      "cnt: 0 - valLoss: 0.6659222841262817 - trainLoss: 0.6642769575119019\n",
      "cnt: 0 - valLoss: 0.6659221053123474 - trainLoss: 0.6642768383026123\n",
      "cnt: 0 - valLoss: 0.6659218668937683 - trainLoss: 0.664276659488678\n",
      "cnt: 0 - valLoss: 0.665921688079834 - trainLoss: 0.6642765998840332\n",
      "cnt: 0 - valLoss: 0.6659215688705444 - trainLoss: 0.6642764210700989\n",
      "cnt: 0 - valLoss: 0.6659213900566101 - trainLoss: 0.6642762422561646\n",
      "cnt: 0 - valLoss: 0.6659212112426758 - trainLoss: 0.6642760634422302\n",
      "cnt: 0 - valLoss: 0.6659209728240967 - trainLoss: 0.6642759442329407\n",
      "cnt: 0 - valLoss: 0.6659207940101624 - trainLoss: 0.6642758250236511\n",
      "cnt: 0 - valLoss: 0.665920615196228 - trainLoss: 0.6642757058143616\n",
      "cnt: 0 - valLoss: 0.6659204959869385 - trainLoss: 0.6642755270004272\n",
      "cnt: 0 - valLoss: 0.6659203171730042 - trainLoss: 0.6642753481864929\n",
      "cnt: 0 - valLoss: 0.6659201383590698 - trainLoss: 0.6642752289772034\n",
      "cnt: 0 - valLoss: 0.6659199595451355 - trainLoss: 0.6642751097679138\n",
      "cnt: 0 - valLoss: 0.6659197211265564 - trainLoss: 0.6642749309539795\n",
      "cnt: 0 - valLoss: 0.6659195423126221 - trainLoss: 0.6642748117446899\n",
      "cnt: 0 - valLoss: 0.6659193634986877 - trainLoss: 0.6642746329307556\n",
      "cnt: 0 - valLoss: 0.6659191846847534 - trainLoss: 0.6642745137214661\n",
      "cnt: 0 - valLoss: 0.6659190058708191 - trainLoss: 0.6642743945121765\n",
      "cnt: 0 - valLoss: 0.6659188270568848 - trainLoss: 0.6642742156982422\n",
      "cnt: 0 - valLoss: 0.6659186482429504 - trainLoss: 0.6642740964889526\n",
      "cnt: 0 - valLoss: 0.6659184694290161 - trainLoss: 0.6642739176750183\n",
      "cnt: 0 - valLoss: 0.6659183502197266 - trainLoss: 0.6642737984657288\n",
      "cnt: 0 - valLoss: 0.6659181118011475 - trainLoss: 0.6642736196517944\n",
      "cnt: 0 - valLoss: 0.6659179329872131 - trainLoss: 0.6642734408378601\n",
      "cnt: 0 - valLoss: 0.6659177541732788 - trainLoss: 0.6642733216285706\n",
      "cnt: 0 - valLoss: 0.6659175753593445 - trainLoss: 0.6642731428146362\n",
      "cnt: 0 - valLoss: 0.6659173965454102 - trainLoss: 0.6642730236053467\n",
      "cnt: 0 - valLoss: 0.665917158126831 - trainLoss: 0.6642729043960571\n",
      "cnt: 0 - valLoss: 0.6659169793128967 - trainLoss: 0.6642727255821228\n",
      "cnt: 0 - valLoss: 0.6659168004989624 - trainLoss: 0.6642726063728333\n",
      "cnt: 0 - valLoss: 0.6659166812896729 - trainLoss: 0.6642724871635437\n",
      "cnt: 0 - valLoss: 0.6659165024757385 - trainLoss: 0.6642723083496094\n",
      "cnt: 0 - valLoss: 0.6659163236618042 - trainLoss: 0.6642721891403198\n",
      "cnt: 0 - valLoss: 0.6659160852432251 - trainLoss: 0.6642720699310303\n",
      "cnt: 0 - valLoss: 0.6659159660339355 - trainLoss: 0.664271891117096\n",
      "cnt: 0 - valLoss: 0.6659157276153564 - trainLoss: 0.6642717719078064\n",
      "cnt: 0 - valLoss: 0.6659156084060669 - trainLoss: 0.6642715930938721\n",
      "cnt: 0 - valLoss: 0.6659153699874878 - trainLoss: 0.6642714738845825\n",
      "cnt: 0 - valLoss: 0.6659151315689087 - trainLoss: 0.6642712950706482\n",
      "cnt: 0 - valLoss: 0.6659150123596191 - trainLoss: 0.6642711162567139\n",
      "cnt: 0 - valLoss: 0.6659148335456848 - trainLoss: 0.6642709970474243\n",
      "cnt: 0 - valLoss: 0.6659146547317505 - trainLoss: 0.6642708778381348\n",
      "cnt: 0 - valLoss: 0.6659144759178162 - trainLoss: 0.6642707586288452\n",
      "cnt: 0 - valLoss: 0.6659142971038818 - trainLoss: 0.6642705798149109\n",
      "cnt: 0 - valLoss: 0.6659141182899475 - trainLoss: 0.6642703413963318\n",
      "cnt: 0 - valLoss: 0.6659139394760132 - trainLoss: 0.664270281791687\n",
      "cnt: 0 - valLoss: 0.6659137606620789 - trainLoss: 0.6642701029777527\n",
      "cnt: 0 - valLoss: 0.6659135818481445 - trainLoss: 0.6642699837684631\n",
      "cnt: 0 - valLoss: 0.6659134030342102 - trainLoss: 0.6642698049545288\n",
      "cnt: 0 - valLoss: 0.6659131646156311 - trainLoss: 0.6642696857452393\n",
      "cnt: 0 - valLoss: 0.6659129858016968 - trainLoss: 0.6642695069313049\n",
      "cnt: 0 - valLoss: 0.6659128665924072 - trainLoss: 0.6642693877220154\n",
      "cnt: 0 - valLoss: 0.6659126877784729 - trainLoss: 0.664269208908081\n",
      "cnt: 0 - valLoss: 0.6659124493598938 - trainLoss: 0.6642690896987915\n",
      "cnt: 0 - valLoss: 0.6659122705459595 - trainLoss: 0.6642689108848572\n",
      "cnt: 0 - valLoss: 0.6659120917320251 - trainLoss: 0.6642687916755676\n",
      "cnt: 0 - valLoss: 0.6659119129180908 - trainLoss: 0.6642686128616333\n",
      "cnt: 0 - valLoss: 0.6659117341041565 - trainLoss: 0.6642684936523438\n",
      "cnt: 0 - valLoss: 0.6659114956855774 - trainLoss: 0.6642683744430542\n",
      "cnt: 0 - valLoss: 0.6659113764762878 - trainLoss: 0.6642681956291199\n",
      "cnt: 0 - valLoss: 0.6659111976623535 - trainLoss: 0.6642680764198303\n",
      "cnt: 0 - valLoss: 0.6659110188484192 - trainLoss: 0.6642678380012512\n",
      "cnt: 0 - valLoss: 0.6659107804298401 - trainLoss: 0.6642677783966064\n",
      "cnt: 0 - valLoss: 0.6659106016159058 - trainLoss: 0.6642675995826721\n",
      "cnt: 0 - valLoss: 0.6659104228019714 - trainLoss: 0.6642674207687378\n",
      "cnt: 0 - valLoss: 0.6659102439880371 - trainLoss: 0.6642673015594482\n",
      "cnt: 0 - valLoss: 0.6659101247787476 - trainLoss: 0.6642671823501587\n",
      "cnt: 0 - valLoss: 0.6659098863601685 - trainLoss: 0.6642670035362244\n",
      "cnt: 0 - valLoss: 0.6659097075462341 - trainLoss: 0.66426682472229\n",
      "cnt: 0 - valLoss: 0.6659095287322998 - trainLoss: 0.6642667651176453\n",
      "cnt: 0 - valLoss: 0.6659093499183655 - trainLoss: 0.6642665863037109\n",
      "cnt: 0 - valLoss: 0.6659091711044312 - trainLoss: 0.6642664074897766\n",
      "cnt: 0 - valLoss: 0.665908932685852 - trainLoss: 0.6642662882804871\n",
      "cnt: 0 - valLoss: 0.6659088134765625 - trainLoss: 0.6642661690711975\n",
      "cnt: 0 - valLoss: 0.6659086346626282 - trainLoss: 0.6642659902572632\n",
      "cnt: 0 - valLoss: 0.6659084558486938 - trainLoss: 0.6642658710479736\n",
      "cnt: 0 - valLoss: 0.6659082770347595 - trainLoss: 0.6642657518386841\n",
      "cnt: 0 - valLoss: 0.6659080386161804 - trainLoss: 0.6642655730247498\n",
      "cnt: 0 - valLoss: 0.6659079194068909 - trainLoss: 0.6642653942108154\n",
      "cnt: 0 - valLoss: 0.6659076809883118 - trainLoss: 0.6642652153968811\n",
      "cnt: 0 - valLoss: 0.6659075021743774 - trainLoss: 0.6642650961875916\n",
      "cnt: 0 - valLoss: 0.6659073233604431 - trainLoss: 0.664264976978302\n",
      "cnt: 0 - valLoss: 0.665907084941864 - trainLoss: 0.6642647981643677\n",
      "cnt: 0 - valLoss: 0.6659069657325745 - trainLoss: 0.6642646789550781\n",
      "cnt: 0 - valLoss: 0.6659067869186401 - trainLoss: 0.6642645001411438\n",
      "cnt: 0 - valLoss: 0.6659066081047058 - trainLoss: 0.6642643809318542\n",
      "cnt: 0 - valLoss: 0.6659064292907715 - trainLoss: 0.6642642021179199\n",
      "cnt: 0 - valLoss: 0.6659062504768372 - trainLoss: 0.6642640829086304\n",
      "cnt: 0 - valLoss: 0.6659060716629028 - trainLoss: 0.664263904094696\n",
      "cnt: 0 - valLoss: 0.6659058332443237 - trainLoss: 0.6642637252807617\n",
      "cnt: 0 - valLoss: 0.6659056544303894 - trainLoss: 0.6642636060714722\n",
      "cnt: 0 - valLoss: 0.6659055352210999 - trainLoss: 0.6642634272575378\n",
      "cnt: 0 - valLoss: 0.6659052968025208 - trainLoss: 0.6642633080482483\n",
      "cnt: 0 - valLoss: 0.6659051179885864 - trainLoss: 0.6642631888389587\n",
      "cnt: 0 - valLoss: 0.6659049391746521 - trainLoss: 0.6642630100250244\n",
      "cnt: 0 - valLoss: 0.6659047603607178 - trainLoss: 0.6642628908157349\n",
      "cnt: 0 - valLoss: 0.6659045815467834 - trainLoss: 0.6642627120018005\n",
      "cnt: 0 - valLoss: 0.6659044623374939 - trainLoss: 0.6642625331878662\n",
      "cnt: 0 - valLoss: 0.6659042239189148 - trainLoss: 0.6642624735832214\n",
      "cnt: 0 - valLoss: 0.6659039855003357 - trainLoss: 0.6642622351646423\n",
      "cnt: 0 - valLoss: 0.6659038662910461 - trainLoss: 0.6642621159553528\n",
      "cnt: 0 - valLoss: 0.6659036874771118 - trainLoss: 0.664262056350708\n",
      "cnt: 0 - valLoss: 0.6659034490585327 - trainLoss: 0.6642618179321289\n",
      "cnt: 0 - valLoss: 0.6659032702445984 - trainLoss: 0.6642616987228394\n",
      "cnt: 0 - valLoss: 0.6659030914306641 - trainLoss: 0.664261519908905\n",
      "cnt: 0 - valLoss: 0.6659029126167297 - trainLoss: 0.6642614006996155\n",
      "cnt: 0 - valLoss: 0.6659027338027954 - trainLoss: 0.6642612218856812\n",
      "cnt: 0 - valLoss: 0.6659025549888611 - trainLoss: 0.6642610430717468\n",
      "cnt: 0 - valLoss: 0.6659023761749268 - trainLoss: 0.6642609238624573\n",
      "cnt: 0 - valLoss: 0.6659021973609924 - trainLoss: 0.664260745048523\n",
      "cnt: 0 - valLoss: 0.6659020185470581 - trainLoss: 0.6642606854438782\n",
      "cnt: 0 - valLoss: 0.6659018397331238 - trainLoss: 0.6642605066299438\n",
      "cnt: 0 - valLoss: 0.6659016609191895 - trainLoss: 0.6642603278160095\n",
      "cnt: 0 - valLoss: 0.6659014821052551 - trainLoss: 0.66426020860672\n",
      "cnt: 0 - valLoss: 0.665901243686676 - trainLoss: 0.6642600893974304\n",
      "cnt: 0 - valLoss: 0.6659010648727417 - trainLoss: 0.6642599105834961\n",
      "cnt: 0 - valLoss: 0.6659008860588074 - trainLoss: 0.6642597317695618\n",
      "cnt: 0 - valLoss: 0.6659007668495178 - trainLoss: 0.6642595529556274\n",
      "cnt: 0 - valLoss: 0.6659005284309387 - trainLoss: 0.6642593741416931\n",
      "cnt: 0 - valLoss: 0.6659003496170044 - trainLoss: 0.6642593145370483\n",
      "cnt: 0 - valLoss: 0.6659001708030701 - trainLoss: 0.664259135723114\n",
      "cnt: 0 - valLoss: 0.6658999919891357 - trainLoss: 0.6642590165138245\n",
      "cnt: 0 - valLoss: 0.6658997535705566 - trainLoss: 0.6642588376998901\n",
      "cnt: 0 - valLoss: 0.6658995747566223 - trainLoss: 0.6642587184906006\n",
      "cnt: 0 - valLoss: 0.6658994555473328 - trainLoss: 0.6642584800720215\n",
      "cnt: 0 - valLoss: 0.6658992171287537 - trainLoss: 0.6642584204673767\n",
      "cnt: 0 - valLoss: 0.6658990383148193 - trainLoss: 0.6642582416534424\n",
      "cnt: 0 - valLoss: 0.665898859500885 - trainLoss: 0.6642580628395081\n",
      "cnt: 0 - valLoss: 0.6658986806869507 - trainLoss: 0.6642579436302185\n",
      "cnt: 0 - valLoss: 0.6658985018730164 - trainLoss: 0.664257824420929\n",
      "cnt: 0 - valLoss: 0.6658983826637268 - trainLoss: 0.6642576456069946\n",
      "cnt: 0 - valLoss: 0.6658982038497925 - trainLoss: 0.6642574667930603\n",
      "cnt: 0 - valLoss: 0.6658980250358582 - trainLoss: 0.664257287979126\n",
      "cnt: 0 - valLoss: 0.6658977270126343 - trainLoss: 0.6642571687698364\n",
      "cnt: 0 - valLoss: 0.6658976078033447 - trainLoss: 0.6642570495605469\n",
      "cnt: 0 - valLoss: 0.6658974289894104 - trainLoss: 0.6642568111419678\n",
      "cnt: 0 - valLoss: 0.6658972501754761 - trainLoss: 0.6642566323280334\n",
      "cnt: 0 - valLoss: 0.6658970713615417 - trainLoss: 0.6642565727233887\n",
      "cnt: 0 - valLoss: 0.6658968329429626 - trainLoss: 0.6642564535140991\n",
      "cnt: 0 - valLoss: 0.6658966541290283 - trainLoss: 0.6642562747001648\n",
      "cnt: 0 - valLoss: 0.665896475315094 - trainLoss: 0.6642560958862305\n",
      "cnt: 0 - valLoss: 0.6658962965011597 - trainLoss: 0.6642559170722961\n",
      "cnt: 0 - valLoss: 0.6658961176872253 - trainLoss: 0.6642558574676514\n",
      "cnt: 0 - valLoss: 0.665895938873291 - trainLoss: 0.664255678653717\n",
      "cnt: 0 - valLoss: 0.6658957600593567 - trainLoss: 0.6642554402351379\n",
      "cnt: 0 - valLoss: 0.6658956408500671 - trainLoss: 0.6642553806304932\n",
      "cnt: 0 - valLoss: 0.665895402431488 - trainLoss: 0.6642552018165588\n",
      "cnt: 0 - valLoss: 0.6658951640129089 - trainLoss: 0.6642550230026245\n",
      "cnt: 0 - valLoss: 0.6658949851989746 - trainLoss: 0.664254903793335\n",
      "cnt: 0 - valLoss: 0.6658948659896851 - trainLoss: 0.6642547249794006\n",
      "cnt: 0 - valLoss: 0.6658945679664612 - trainLoss: 0.6642546057701111\n",
      "cnt: 0 - valLoss: 0.6658943891525269 - trainLoss: 0.6642544269561768\n",
      "cnt: 0 - valLoss: 0.6658942699432373 - trainLoss: 0.6642542481422424\n",
      "cnt: 0 - valLoss: 0.665894091129303 - trainLoss: 0.6642541289329529\n",
      "cnt: 0 - valLoss: 0.6658939123153687 - trainLoss: 0.6642539501190186\n",
      "cnt: 0 - valLoss: 0.6658936738967896 - trainLoss: 0.664253830909729\n",
      "cnt: 0 - valLoss: 0.6658935546875 - trainLoss: 0.6642536520957947\n",
      "cnt: 0 - valLoss: 0.6658933162689209 - trainLoss: 0.6642535924911499\n",
      "cnt: 0 - valLoss: 0.6658931374549866 - trainLoss: 0.6642534136772156\n",
      "cnt: 0 - valLoss: 0.6658929586410522 - trainLoss: 0.6642532348632812\n",
      "cnt: 0 - valLoss: 0.6658927798271179 - trainLoss: 0.6642530560493469\n",
      "cnt: 0 - valLoss: 0.6658925414085388 - trainLoss: 0.6642528772354126\n",
      "cnt: 0 - valLoss: 0.6658924221992493 - trainLoss: 0.6642526984214783\n",
      "cnt: 0 - valLoss: 0.6658922433853149 - trainLoss: 0.6642526388168335\n",
      "cnt: 0 - valLoss: 0.6658920645713806 - trainLoss: 0.6642524600028992\n",
      "cnt: 0 - valLoss: 0.6658918857574463 - trainLoss: 0.6642522811889648\n",
      "cnt: 0 - valLoss: 0.6658916473388672 - trainLoss: 0.6642521619796753\n",
      "cnt: 0 - valLoss: 0.6658914685249329 - trainLoss: 0.6642520427703857\n",
      "cnt: 0 - valLoss: 0.6658913493156433 - trainLoss: 0.6642518043518066\n",
      "cnt: 0 - valLoss: 0.6658910512924194 - trainLoss: 0.6642516851425171\n",
      "cnt: 0 - valLoss: 0.6658908724784851 - trainLoss: 0.6642515659332275\n",
      "cnt: 0 - valLoss: 0.6658907532691956 - trainLoss: 0.6642513275146484\n",
      "cnt: 0 - valLoss: 0.6658905744552612 - trainLoss: 0.6642512679100037\n",
      "cnt: 0 - valLoss: 0.6658903360366821 - trainLoss: 0.6642510890960693\n",
      "cnt: 0 - valLoss: 0.6658902168273926 - trainLoss: 0.664250910282135\n",
      "cnt: 0 - valLoss: 0.6658899784088135 - trainLoss: 0.6642507314682007\n",
      "cnt: 0 - valLoss: 0.6658897995948792 - trainLoss: 0.6642506122589111\n",
      "cnt: 0 - valLoss: 0.6658896207809448 - trainLoss: 0.6642504930496216\n",
      "cnt: 0 - valLoss: 0.6658893823623657 - trainLoss: 0.6642503142356873\n",
      "cnt: 0 - valLoss: 0.6658892035484314 - trainLoss: 0.6642501354217529\n",
      "cnt: 0 - valLoss: 0.6658890247344971 - trainLoss: 0.6642500162124634\n",
      "cnt: 0 - valLoss: 0.6658888459205627 - trainLoss: 0.6642498970031738\n",
      "cnt: 0 - valLoss: 0.6658886671066284 - trainLoss: 0.6642497181892395\n",
      "cnt: 0 - valLoss: 0.6658884286880493 - trainLoss: 0.6642495393753052\n",
      "cnt: 0 - valLoss: 0.6658883094787598 - trainLoss: 0.6642493605613708\n",
      "cnt: 0 - valLoss: 0.6658881306648254 - trainLoss: 0.6642493009567261\n",
      "cnt: 0 - valLoss: 0.6658878922462463 - trainLoss: 0.6642491221427917\n",
      "cnt: 0 - valLoss: 0.665887713432312 - trainLoss: 0.6642489433288574\n",
      "cnt: 0 - valLoss: 0.6658875346183777 - trainLoss: 0.6642488241195679\n",
      "cnt: 0 - valLoss: 0.6658873558044434 - trainLoss: 0.6642486453056335\n",
      "cnt: 0 - valLoss: 0.6658871173858643 - trainLoss: 0.664248526096344\n",
      "cnt: 0 - valLoss: 0.6658869385719299 - trainLoss: 0.6642484068870544\n",
      "cnt: 0 - valLoss: 0.6658867597579956 - trainLoss: 0.6642482280731201\n",
      "cnt: 0 - valLoss: 0.665886640548706 - trainLoss: 0.6642480492591858\n",
      "cnt: 0 - valLoss: 0.6658863425254822 - trainLoss: 0.6642479300498962\n",
      "cnt: 0 - valLoss: 0.6658862233161926 - trainLoss: 0.6642477512359619\n",
      "cnt: 0 - valLoss: 0.6658859848976135 - trainLoss: 0.6642476320266724\n",
      "cnt: 0 - valLoss: 0.665885865688324 - trainLoss: 0.6642473936080933\n",
      "cnt: 0 - valLoss: 0.6658856272697449 - trainLoss: 0.6642473340034485\n",
      "cnt: 0 - valLoss: 0.6658855080604553 - trainLoss: 0.6642471551895142\n",
      "cnt: 0 - valLoss: 0.6658852696418762 - trainLoss: 0.6642469763755798\n",
      "cnt: 0 - valLoss: 0.6658850908279419 - trainLoss: 0.6642469167709351\n",
      "cnt: 0 - valLoss: 0.6658849120140076 - trainLoss: 0.664246678352356\n",
      "cnt: 0 - valLoss: 0.6658847332000732 - trainLoss: 0.6642465591430664\n",
      "cnt: 0 - valLoss: 0.6658844947814941 - trainLoss: 0.6642463803291321\n",
      "cnt: 0 - valLoss: 0.6658843159675598 - trainLoss: 0.6642462611198425\n",
      "cnt: 0 - valLoss: 0.6658841967582703 - trainLoss: 0.664246141910553\n",
      "cnt: 0 - valLoss: 0.6658839583396912 - trainLoss: 0.6642459630966187\n",
      "cnt: 0 - valLoss: 0.6658837199211121 - trainLoss: 0.6642458438873291\n",
      "cnt: 0 - valLoss: 0.6658835411071777 - trainLoss: 0.66424560546875\n",
      "cnt: 0 - valLoss: 0.6658833622932434 - trainLoss: 0.6642454862594604\n",
      "cnt: 0 - valLoss: 0.6658831834793091 - trainLoss: 0.6642453670501709\n",
      "cnt: 0 - valLoss: 0.66588294506073 - trainLoss: 0.6642451882362366\n",
      "cnt: 0 - valLoss: 0.6658828258514404 - trainLoss: 0.6642450094223022\n",
      "cnt: 0 - valLoss: 0.6658825874328613 - trainLoss: 0.6642448902130127\n",
      "cnt: 0 - valLoss: 0.665882408618927 - trainLoss: 0.6642447710037231\n",
      "cnt: 0 - valLoss: 0.6658822298049927 - trainLoss: 0.6642445921897888\n",
      "cnt: 0 - valLoss: 0.6658820509910583 - trainLoss: 0.6642444133758545\n",
      "cnt: 0 - valLoss: 0.665881872177124 - trainLoss: 0.6642442941665649\n",
      "cnt: 0 - valLoss: 0.6658816337585449 - trainLoss: 0.6642441749572754\n",
      "cnt: 0 - valLoss: 0.6658814549446106 - trainLoss: 0.6642439961433411\n",
      "cnt: 0 - valLoss: 0.6658812761306763 - trainLoss: 0.6642438769340515\n",
      "cnt: 0 - valLoss: 0.6658810973167419 - trainLoss: 0.6642436981201172\n",
      "cnt: 0 - valLoss: 0.6658809781074524 - trainLoss: 0.6642435193061829\n",
      "cnt: 0 - valLoss: 0.6658806800842285 - trainLoss: 0.6642434000968933\n",
      "cnt: 0 - valLoss: 0.6658805012702942 - trainLoss: 0.664243221282959\n",
      "cnt: 0 - valLoss: 0.6658803820610046 - trainLoss: 0.6642431020736694\n",
      "cnt: 0 - valLoss: 0.6658801436424255 - trainLoss: 0.6642429232597351\n",
      "cnt: 0 - valLoss: 0.6658799052238464 - trainLoss: 0.6642428040504456\n",
      "cnt: 0 - valLoss: 0.6658797264099121 - trainLoss: 0.6642426252365112\n",
      "cnt: 0 - valLoss: 0.6658795475959778 - trainLoss: 0.6642425060272217\n",
      "cnt: 0 - valLoss: 0.6658793091773987 - trainLoss: 0.6642423272132874\n",
      "cnt: 0 - valLoss: 0.6658791899681091 - trainLoss: 0.6642422080039978\n",
      "cnt: 0 - valLoss: 0.6658790111541748 - trainLoss: 0.6642420291900635\n",
      "cnt: 0 - valLoss: 0.6658788323402405 - trainLoss: 0.6642418503761292\n",
      "cnt: 0 - valLoss: 0.6658785939216614 - trainLoss: 0.6642417311668396\n",
      "cnt: 0 - valLoss: 0.6658783555030823 - trainLoss: 0.6642415523529053\n",
      "cnt: 0 - valLoss: 0.6658782362937927 - trainLoss: 0.6642414331436157\n",
      "cnt: 0 - valLoss: 0.6658780574798584 - trainLoss: 0.6642413139343262\n",
      "cnt: 0 - valLoss: 0.6658778190612793 - trainLoss: 0.6642410755157471\n",
      "cnt: 0 - valLoss: 0.6658776998519897 - trainLoss: 0.6642409563064575\n",
      "cnt: 0 - valLoss: 0.6658774614334106 - trainLoss: 0.664240837097168\n",
      "cnt: 0 - valLoss: 0.6658772826194763 - trainLoss: 0.6642406582832336\n",
      "cnt: 0 - valLoss: 0.665877103805542 - trainLoss: 0.6642405390739441\n",
      "cnt: 0 - valLoss: 0.6658769249916077 - trainLoss: 0.6642403602600098\n",
      "cnt: 0 - valLoss: 0.6658767461776733 - trainLoss: 0.6642401814460754\n",
      "cnt: 0 - valLoss: 0.665876567363739 - trainLoss: 0.6642400622367859\n",
      "cnt: 0 - valLoss: 0.6658763885498047 - trainLoss: 0.6642398834228516\n",
      "cnt: 0 - valLoss: 0.6658761501312256 - trainLoss: 0.664239764213562\n",
      "cnt: 0 - valLoss: 0.6658759713172913 - trainLoss: 0.6642395853996277\n",
      "cnt: 0 - valLoss: 0.6658757925033569 - trainLoss: 0.6642394661903381\n",
      "cnt: 0 - valLoss: 0.6658755540847778 - trainLoss: 0.6642392873764038\n",
      "cnt: 0 - valLoss: 0.6658753156661987 - trainLoss: 0.6642391085624695\n",
      "cnt: 0 - valLoss: 0.6658751964569092 - trainLoss: 0.6642389893531799\n",
      "cnt: 0 - valLoss: 0.6658750176429749 - trainLoss: 0.6642388701438904\n",
      "cnt: 0 - valLoss: 0.6658747792243958 - trainLoss: 0.664238691329956\n",
      "cnt: 0 - valLoss: 0.6658746004104614 - trainLoss: 0.6642385125160217\n",
      "cnt: 0 - valLoss: 0.6658744215965271 - trainLoss: 0.6642383933067322\n",
      "cnt: 0 - valLoss: 0.6658742427825928 - trainLoss: 0.6642382144927979\n",
      "cnt: 0 - valLoss: 0.6658740639686584 - trainLoss: 0.6642380356788635\n",
      "cnt: 0 - valLoss: 0.6658738851547241 - trainLoss: 0.6642378568649292\n",
      "cnt: 0 - valLoss: 0.665873646736145 - trainLoss: 0.6642377972602844\n",
      "cnt: 0 - valLoss: 0.6658734679222107 - trainLoss: 0.6642376184463501\n",
      "cnt: 0 - valLoss: 0.6658732295036316 - trainLoss: 0.6642374992370605\n",
      "cnt: 0 - valLoss: 0.6658730506896973 - trainLoss: 0.6642373204231262\n",
      "cnt: 0 - valLoss: 0.6658728718757629 - trainLoss: 0.6642371416091919\n",
      "cnt: 0 - valLoss: 0.6658727526664734 - trainLoss: 0.6642369627952576\n",
      "cnt: 0 - valLoss: 0.6658725738525391 - trainLoss: 0.6642367839813232\n",
      "cnt: 0 - valLoss: 0.6658722758293152 - trainLoss: 0.6642366647720337\n",
      "cnt: 0 - valLoss: 0.6658720970153809 - trainLoss: 0.6642365455627441\n",
      "cnt: 0 - valLoss: 0.6658719778060913 - trainLoss: 0.6642363667488098\n",
      "cnt: 0 - valLoss: 0.665871798992157 - trainLoss: 0.6642361283302307\n",
      "cnt: 0 - valLoss: 0.6658716201782227 - trainLoss: 0.6642360091209412\n",
      "cnt: 0 - valLoss: 0.6658713817596436 - trainLoss: 0.6642358899116516\n",
      "cnt: 0 - valLoss: 0.6658712029457092 - trainLoss: 0.6642357110977173\n",
      "cnt: 0 - valLoss: 0.6658709645271301 - trainLoss: 0.664235532283783\n",
      "cnt: 0 - valLoss: 0.6658708453178406 - trainLoss: 0.6642353534698486\n",
      "cnt: 0 - valLoss: 0.6658706068992615 - trainLoss: 0.6642352342605591\n",
      "cnt: 0 - valLoss: 0.6658704280853271 - trainLoss: 0.6642350554466248\n",
      "cnt: 0 - valLoss: 0.6658702492713928 - trainLoss: 0.6642348766326904\n",
      "cnt: 0 - valLoss: 0.6658700704574585 - trainLoss: 0.6642347574234009\n",
      "cnt: 0 - valLoss: 0.6658698320388794 - trainLoss: 0.6642345786094666\n",
      "cnt: 0 - valLoss: 0.6658696532249451 - trainLoss: 0.6642343997955322\n",
      "cnt: 0 - valLoss: 0.6658694744110107 - trainLoss: 0.6642342209815979\n",
      "cnt: 0 - valLoss: 0.6658692359924316 - trainLoss: 0.6642340421676636\n",
      "cnt: 0 - valLoss: 0.6658690571784973 - trainLoss: 0.6642338633537292\n",
      "cnt: 0 - valLoss: 0.665868878364563 - trainLoss: 0.6642338037490845\n",
      "cnt: 0 - valLoss: 0.6658686995506287 - trainLoss: 0.6642335653305054\n",
      "cnt: 0 - valLoss: 0.6658685207366943 - trainLoss: 0.664233386516571\n",
      "cnt: 0 - valLoss: 0.66586834192276 - trainLoss: 0.6642332673072815\n",
      "cnt: 0 - valLoss: 0.6658681035041809 - trainLoss: 0.6642330884933472\n",
      "cnt: 0 - valLoss: 0.6658678650856018 - trainLoss: 0.6642329096794128\n",
      "cnt: 0 - valLoss: 0.6658676862716675 - trainLoss: 0.6642327308654785\n",
      "cnt: 0 - valLoss: 0.6658675670623779 - trainLoss: 0.6642325520515442\n",
      "cnt: 0 - valLoss: 0.6658673286437988 - trainLoss: 0.6642324924468994\n",
      "cnt: 0 - valLoss: 0.6658671498298645 - trainLoss: 0.6642323136329651\n",
      "cnt: 0 - valLoss: 0.6658669710159302 - trainLoss: 0.6642321348190308\n",
      "cnt: 0 - valLoss: 0.6658667922019958 - trainLoss: 0.6642319560050964\n",
      "cnt: 0 - valLoss: 0.6658666133880615 - trainLoss: 0.6642317771911621\n",
      "cnt: 0 - valLoss: 0.6658663749694824 - trainLoss: 0.6642316579818726\n",
      "cnt: 0 - valLoss: 0.6658661961555481 - trainLoss: 0.6642314195632935\n",
      "cnt: 0 - valLoss: 0.6658660173416138 - trainLoss: 0.6642312407493591\n",
      "cnt: 0 - valLoss: 0.6658658385276794 - trainLoss: 0.6642311215400696\n",
      "cnt: 0 - valLoss: 0.6658656001091003 - trainLoss: 0.6642309427261353\n",
      "cnt: 0 - valLoss: 0.665865421295166 - trainLoss: 0.6642307639122009\n",
      "cnt: 0 - valLoss: 0.6658652424812317 - trainLoss: 0.6642306447029114\n",
      "cnt: 0 - valLoss: 0.6658651232719421 - trainLoss: 0.664230465888977\n",
      "cnt: 0 - valLoss: 0.6658648252487183 - trainLoss: 0.6642302870750427\n",
      "cnt: 0 - valLoss: 0.6658646464347839 - trainLoss: 0.6642301082611084\n",
      "cnt: 0 - valLoss: 0.6658644676208496 - trainLoss: 0.6642299294471741\n",
      "cnt: 0 - valLoss: 0.6658643484115601 - trainLoss: 0.6642297506332397\n",
      "cnt: 0 - valLoss: 0.6658640503883362 - trainLoss: 0.6642296314239502\n",
      "cnt: 0 - valLoss: 0.6658638715744019 - trainLoss: 0.6642294526100159\n",
      "cnt: 0 - valLoss: 0.6658637523651123 - trainLoss: 0.6642292737960815\n",
      "cnt: 0 - valLoss: 0.665863573551178 - trainLoss: 0.6642290949821472\n",
      "cnt: 0 - valLoss: 0.6658632755279541 - trainLoss: 0.6642289757728577\n",
      "cnt: 0 - valLoss: 0.6658631563186646 - trainLoss: 0.6642287373542786\n",
      "cnt: 0 - valLoss: 0.6658629775047302 - trainLoss: 0.6642286777496338\n",
      "cnt: 0 - valLoss: 0.6658627986907959 - trainLoss: 0.6642284393310547\n",
      "cnt: 0 - valLoss: 0.6658625602722168 - trainLoss: 0.6642283201217651\n",
      "cnt: 0 - valLoss: 0.6658623218536377 - trainLoss: 0.664228081703186\n",
      "cnt: 0 - valLoss: 0.6658622026443481 - trainLoss: 0.6642279028892517\n",
      "cnt: 0 - valLoss: 0.6658620238304138 - trainLoss: 0.6642277836799622\n",
      "cnt: 0 - valLoss: 0.6658617854118347 - trainLoss: 0.6642276048660278\n",
      "cnt: 0 - valLoss: 0.6658616065979004 - trainLoss: 0.6642274856567383\n",
      "cnt: 0 - valLoss: 0.6658614277839661 - trainLoss: 0.664227306842804\n",
      "cnt: 0 - valLoss: 0.6658612489700317 - trainLoss: 0.6642271280288696\n",
      "cnt: 0 - valLoss: 0.6658610105514526 - trainLoss: 0.6642269492149353\n",
      "cnt: 0 - valLoss: 0.6658608317375183 - trainLoss: 0.664226770401001\n",
      "cnt: 0 - valLoss: 0.665860652923584 - trainLoss: 0.6642265319824219\n",
      "cnt: 0 - valLoss: 0.6658604741096497 - trainLoss: 0.6642264127731323\n",
      "cnt: 0 - valLoss: 0.6658602952957153 - trainLoss: 0.664226233959198\n",
      "cnt: 0 - valLoss: 0.6658600568771362 - trainLoss: 0.6642261147499084\n",
      "cnt: 0 - valLoss: 0.6658598780632019 - trainLoss: 0.6642259359359741\n",
      "cnt: 0 - valLoss: 0.6658596992492676 - trainLoss: 0.6642258167266846\n",
      "cnt: 0 - valLoss: 0.6658594608306885 - trainLoss: 0.6642255783081055\n",
      "cnt: 0 - valLoss: 0.6658592820167542 - trainLoss: 0.6642253994941711\n",
      "cnt: 0 - valLoss: 0.6658591032028198 - trainLoss: 0.6642252206802368\n",
      "cnt: 0 - valLoss: 0.6658589243888855 - trainLoss: 0.6642250418663025\n",
      "cnt: 0 - valLoss: 0.6658586859703064 - trainLoss: 0.6642248630523682\n",
      "cnt: 0 - valLoss: 0.6658585071563721 - trainLoss: 0.6642247438430786\n",
      "cnt: 0 - valLoss: 0.6658583283424377 - trainLoss: 0.6642245650291443\n",
      "cnt: 0 - valLoss: 0.6658580899238586 - trainLoss: 0.6642244458198547\n",
      "cnt: 0 - valLoss: 0.6658579707145691 - trainLoss: 0.6642242670059204\n",
      "cnt: 0 - valLoss: 0.6658577919006348 - trainLoss: 0.6642240881919861\n",
      "cnt: 0 - valLoss: 0.6658574938774109 - trainLoss: 0.6642239093780518\n",
      "cnt: 0 - valLoss: 0.6658573746681213 - trainLoss: 0.6642237305641174\n",
      "cnt: 0 - valLoss: 0.6658571362495422 - trainLoss: 0.6642234921455383\n",
      "cnt: 0 - valLoss: 0.6658569574356079 - trainLoss: 0.6642232537269592\n",
      "cnt: 0 - valLoss: 0.6658567786216736 - trainLoss: 0.6642230749130249\n",
      "cnt: 0 - valLoss: 0.6658565998077393 - trainLoss: 0.6642228960990906\n",
      "cnt: 0 - valLoss: 0.6658564209938049 - trainLoss: 0.6642226576805115\n",
      "cnt: 0 - valLoss: 0.6658561825752258 - trainLoss: 0.6642224788665771\n",
      "cnt: 0 - valLoss: 0.6658559441566467 - trainLoss: 0.664222240447998\n",
      "cnt: 0 - valLoss: 0.6658558249473572 - trainLoss: 0.6642220616340637\n",
      "cnt: 0 - valLoss: 0.6658555865287781 - trainLoss: 0.6642218232154846\n",
      "cnt: 0 - valLoss: 0.6658554077148438 - trainLoss: 0.6642215847969055\n",
      "cnt: 0 - valLoss: 0.6658551692962646 - trainLoss: 0.6642214059829712\n",
      "cnt: 0 - valLoss: 0.6658549904823303 - trainLoss: 0.6642212271690369\n",
      "cnt: 0 - valLoss: 0.6658546924591064 - trainLoss: 0.664220929145813\n",
      "cnt: 0 - valLoss: 0.6658545732498169 - trainLoss: 0.6642207503318787\n",
      "cnt: 0 - valLoss: 0.665854275226593 - trainLoss: 0.6642206311225891\n",
      "cnt: 0 - valLoss: 0.6658540368080139 - trainLoss: 0.6642203330993652\n",
      "cnt: 0 - valLoss: 0.6658538579940796 - trainLoss: 0.6642201542854309\n",
      "cnt: 0 - valLoss: 0.6658536791801453 - trainLoss: 0.6642200350761414\n",
      "cnt: 0 - valLoss: 0.6658534407615662 - trainLoss: 0.6642197370529175\n",
      "cnt: 0 - valLoss: 0.6658532023429871 - trainLoss: 0.6642195582389832\n",
      "cnt: 0 - valLoss: 0.6658530235290527 - trainLoss: 0.664219319820404\n",
      "cnt: 0 - valLoss: 0.6658527255058289 - trainLoss: 0.664219081401825\n",
      "cnt: 0 - valLoss: 0.6658525466918945 - trainLoss: 0.6642189025878906\n",
      "cnt: 0 - valLoss: 0.6658523082733154 - trainLoss: 0.6642186641693115\n",
      "cnt: 0 - valLoss: 0.6658521294593811 - trainLoss: 0.6642184853553772\n",
      "cnt: 0 - valLoss: 0.665851891040802 - trainLoss: 0.6642182469367981\n",
      "cnt: 0 - valLoss: 0.6658516526222229 - trainLoss: 0.6642181277275085\n",
      "cnt: 0 - valLoss: 0.6658514738082886 - trainLoss: 0.6642178297042847\n",
      "cnt: 0 - valLoss: 0.6658512353897095 - trainLoss: 0.6642175912857056\n",
      "cnt: 0 - valLoss: 0.6658510565757751 - trainLoss: 0.6642174124717712\n",
      "cnt: 0 - valLoss: 0.6658507585525513 - trainLoss: 0.6642172932624817\n",
      "cnt: 0 - valLoss: 0.6658505797386169 - trainLoss: 0.6642169952392578\n",
      "cnt: 0 - valLoss: 0.6658503413200378 - trainLoss: 0.6642168164253235\n",
      "cnt: 0 - valLoss: 0.6658501029014587 - trainLoss: 0.6642166376113892\n",
      "cnt: 0 - valLoss: 0.6658499240875244 - trainLoss: 0.6642163991928101\n",
      "cnt: 0 - valLoss: 0.6658496856689453 - trainLoss: 0.6642162203788757\n",
      "cnt: 0 - valLoss: 0.665849506855011 - trainLoss: 0.6642159819602966\n",
      "cnt: 0 - valLoss: 0.6658493280410767 - trainLoss: 0.6642157435417175\n",
      "cnt: 0 - valLoss: 0.6658490896224976 - trainLoss: 0.6642155647277832\n",
      "cnt: 0 - valLoss: 0.6658489108085632 - trainLoss: 0.6642153263092041\n",
      "cnt: 0 - valLoss: 0.6658486127853394 - trainLoss: 0.6642151474952698\n",
      "cnt: 0 - valLoss: 0.6658484935760498 - trainLoss: 0.6642149090766907\n",
      "cnt: 0 - valLoss: 0.6658481955528259 - trainLoss: 0.6642147302627563\n",
      "cnt: 0 - valLoss: 0.6658480167388916 - trainLoss: 0.664214551448822\n",
      "cnt: 0 - valLoss: 0.6658477783203125 - trainLoss: 0.6642142534255981\n",
      "cnt: 0 - valLoss: 0.6658475995063782 - trainLoss: 0.6642140746116638\n",
      "cnt: 0 - valLoss: 0.6658474206924438 - trainLoss: 0.6642138361930847\n",
      "cnt: 0 - valLoss: 0.6658472418785095 - trainLoss: 0.6642136573791504\n",
      "cnt: 0 - valLoss: 0.6658470034599304 - trainLoss: 0.6642134785652161\n",
      "cnt: 0 - valLoss: 0.6658468246459961 - trainLoss: 0.664213240146637\n",
      "cnt: 0 - valLoss: 0.665846586227417 - trainLoss: 0.6642130613327026\n",
      "cnt: 0 - valLoss: 0.6658464074134827 - trainLoss: 0.6642128229141235\n",
      "cnt: 0 - valLoss: 0.6658461689949036 - trainLoss: 0.6642125844955444\n",
      "cnt: 0 - valLoss: 0.6658458709716797 - trainLoss: 0.6642124056816101\n",
      "cnt: 0 - valLoss: 0.6658456921577454 - trainLoss: 0.664212167263031\n",
      "cnt: 0 - valLoss: 0.6658455729484558 - trainLoss: 0.6642119884490967\n",
      "cnt: 0 - valLoss: 0.6658453345298767 - trainLoss: 0.6642118096351624\n",
      "cnt: 0 - valLoss: 0.6658451557159424 - trainLoss: 0.6642115116119385\n",
      "cnt: 0 - valLoss: 0.6658449769020081 - trainLoss: 0.6642113924026489\n",
      "cnt: 0 - valLoss: 0.665844738483429 - trainLoss: 0.6642111539840698\n",
      "cnt: 0 - valLoss: 0.6658445000648499 - trainLoss: 0.6642109155654907\n",
      "cnt: 0 - valLoss: 0.6658443212509155 - trainLoss: 0.6642106771469116\n",
      "cnt: 0 - valLoss: 0.6658440828323364 - trainLoss: 0.6642104983329773\n",
      "cnt: 0 - valLoss: 0.6658439040184021 - trainLoss: 0.664210319519043\n",
      "cnt: 0 - valLoss: 0.665843665599823 - trainLoss: 0.6642100811004639\n",
      "cnt: 0 - valLoss: 0.6658434271812439 - trainLoss: 0.6642098426818848\n",
      "cnt: 0 - valLoss: 0.6658432483673096 - trainLoss: 0.6642096638679504\n",
      "cnt: 0 - valLoss: 0.6658430099487305 - trainLoss: 0.6642093658447266\n",
      "cnt: 0 - valLoss: 0.6658427715301514 - trainLoss: 0.664209246635437\n",
      "cnt: 0 - valLoss: 0.665842592716217 - trainLoss: 0.6642090082168579\n",
      "cnt: 0 - valLoss: 0.6658424139022827 - trainLoss: 0.6642087697982788\n",
      "cnt: 0 - valLoss: 0.6658422350883484 - trainLoss: 0.6642086505889893\n",
      "cnt: 0 - valLoss: 0.6658419966697693 - trainLoss: 0.6642084121704102\n",
      "cnt: 0 - valLoss: 0.6658417582511902 - trainLoss: 0.664208173751831\n",
      "cnt: 0 - valLoss: 0.6658415198326111 - trainLoss: 0.664207935333252\n",
      "cnt: 0 - valLoss: 0.6658413410186768 - trainLoss: 0.6642077565193176\n",
      "cnt: 0 - valLoss: 0.6658411622047424 - trainLoss: 0.6642075181007385\n",
      "cnt: 0 - valLoss: 0.6658409237861633 - trainLoss: 0.6642073392868042\n",
      "cnt: 0 - valLoss: 0.6658406853675842 - trainLoss: 0.6642071008682251\n",
      "cnt: 0 - valLoss: 0.6658405065536499 - trainLoss: 0.6642069220542908\n",
      "cnt: 0 - valLoss: 0.6658403873443604 - trainLoss: 0.6642066836357117\n",
      "cnt: 0 - valLoss: 0.665840208530426 - trainLoss: 0.6642065048217773\n",
      "cnt: 0 - valLoss: 0.6658400297164917 - trainLoss: 0.6642062067985535\n",
      "cnt: 0 - valLoss: 0.6658397912979126 - trainLoss: 0.6642060279846191\n",
      "cnt: 0 - valLoss: 0.6658396124839783 - trainLoss: 0.6642058491706848\n",
      "cnt: 0 - valLoss: 0.665839433670044 - trainLoss: 0.6642056107521057\n",
      "cnt: 0 - valLoss: 0.6658391952514648 - trainLoss: 0.6642054319381714\n",
      "cnt: 0 - valLoss: 0.6658390760421753 - trainLoss: 0.6642051935195923\n",
      "cnt: 0 - valLoss: 0.6658388376235962 - trainLoss: 0.6642049551010132\n",
      "cnt: 0 - valLoss: 0.6658386588096619 - trainLoss: 0.6642047762870789\n",
      "cnt: 0 - valLoss: 0.6658384799957275 - trainLoss: 0.6642045378684998\n",
      "cnt: 0 - valLoss: 0.6658382415771484 - trainLoss: 0.6642043590545654\n",
      "cnt: 0 - valLoss: 0.6658380627632141 - trainLoss: 0.6642041206359863\n",
      "cnt: 0 - valLoss: 0.6658379435539246 - trainLoss: 0.664203941822052\n",
      "cnt: 0 - valLoss: 0.6658377051353455 - trainLoss: 0.6642037034034729\n",
      "cnt: 0 - valLoss: 0.6658375263214111 - trainLoss: 0.6642034649848938\n",
      "cnt: 0 - valLoss: 0.6658373475074768 - trainLoss: 0.6642032861709595\n",
      "cnt: 0 - valLoss: 0.6658371686935425 - trainLoss: 0.6642030477523804\n",
      "cnt: 0 - valLoss: 0.6658369302749634 - trainLoss: 0.664202868938446\n",
      "cnt: 0 - valLoss: 0.6658368110656738 - trainLoss: 0.6642026305198669\n",
      "cnt: 0 - valLoss: 0.6658365726470947 - trainLoss: 0.6642023921012878\n",
      "cnt: 0 - valLoss: 0.6658363938331604 - trainLoss: 0.6642022132873535\n",
      "cnt: 0 - valLoss: 0.6658361554145813 - trainLoss: 0.6642019748687744\n",
      "cnt: 0 - valLoss: 0.6658360362052917 - trainLoss: 0.6642017960548401\n",
      "cnt: 0 - valLoss: 0.6658357977867126 - trainLoss: 0.6642016172409058\n",
      "cnt: 0 - valLoss: 0.6658356189727783 - trainLoss: 0.6642013192176819\n",
      "cnt: 0 - valLoss: 0.6658353805541992 - trainLoss: 0.6642012000083923\n",
      "cnt: 0 - valLoss: 0.6658352017402649 - trainLoss: 0.6642009019851685\n",
      "cnt: 0 - valLoss: 0.6658350229263306 - trainLoss: 0.6642007231712341\n",
      "cnt: 0 - valLoss: 0.665834903717041 - trainLoss: 0.6642005443572998\n",
      "cnt: 0 - valLoss: 0.6658346652984619 - trainLoss: 0.6642003059387207\n",
      "cnt: 0 - valLoss: 0.6658344864845276 - trainLoss: 0.6642001271247864\n",
      "cnt: 0 - valLoss: 0.6658343076705933 - trainLoss: 0.6641998887062073\n",
      "cnt: 0 - valLoss: 0.6658341288566589 - trainLoss: 0.6641996502876282\n",
      "cnt: 0 - valLoss: 0.6658338904380798 - trainLoss: 0.6641994714736938\n",
      "cnt: 0 - valLoss: 0.6658337116241455 - trainLoss: 0.6641992330551147\n",
      "cnt: 0 - valLoss: 0.6658335328102112 - trainLoss: 0.6641990542411804\n",
      "cnt: 0 - valLoss: 0.6658333539962769 - trainLoss: 0.6641988158226013\n",
      "cnt: 0 - valLoss: 0.6658331751823425 - trainLoss: 0.664198637008667\n",
      "cnt: 0 - valLoss: 0.6658329367637634 - trainLoss: 0.6641983985900879\n",
      "cnt: 0 - valLoss: 0.6658327579498291 - trainLoss: 0.6641981601715088\n",
      "cnt: 0 - valLoss: 0.6658325791358948 - trainLoss: 0.6641979813575745\n",
      "cnt: 0 - valLoss: 0.6658324599266052 - trainLoss: 0.6641977429389954\n",
      "cnt: 0 - valLoss: 0.6658321619033813 - trainLoss: 0.664197564125061\n",
      "cnt: 0 - valLoss: 0.6658319234848022 - trainLoss: 0.6641973257064819\n",
      "cnt: 0 - valLoss: 0.6658317446708679 - trainLoss: 0.6641971468925476\n",
      "cnt: 0 - valLoss: 0.6658315658569336 - trainLoss: 0.6641969680786133\n",
      "cnt: 0 - valLoss: 0.6658313870429993 - trainLoss: 0.664196789264679\n",
      "cnt: 0 - valLoss: 0.6658312082290649 - trainLoss: 0.6641965508460999\n",
      "cnt: 0 - valLoss: 0.6658309698104858 - trainLoss: 0.6641963720321655\n",
      "cnt: 0 - valLoss: 0.6658307313919067 - trainLoss: 0.6641961336135864\n",
      "cnt: 0 - valLoss: 0.6658305525779724 - trainLoss: 0.6641959547996521\n",
      "cnt: 0 - valLoss: 0.6658303141593933 - trainLoss: 0.6641957759857178\n",
      "cnt: 0 - valLoss: 0.665830135345459 - trainLoss: 0.6641955375671387\n",
      "cnt: 0 - valLoss: 0.6658299565315247 - trainLoss: 0.6641953587532043\n",
      "cnt: 0 - valLoss: 0.6658297181129456 - trainLoss: 0.66419517993927\n",
      "cnt: 0 - valLoss: 0.6658295392990112 - trainLoss: 0.6641949415206909\n",
      "cnt: 0 - valLoss: 0.6658293604850769 - trainLoss: 0.6641948223114014\n",
      "cnt: 0 - valLoss: 0.6658291220664978 - trainLoss: 0.6641945838928223\n",
      "cnt: 0 - valLoss: 0.6658289432525635 - trainLoss: 0.6641944050788879\n",
      "cnt: 0 - valLoss: 0.6658287644386292 - trainLoss: 0.6641941666603088\n",
      "cnt: 0 - valLoss: 0.66582852602005 - trainLoss: 0.6641939878463745\n",
      "cnt: 0 - valLoss: 0.6658283472061157 - trainLoss: 0.6641937494277954\n",
      "cnt: 0 - valLoss: 0.6658281683921814 - trainLoss: 0.6641935706138611\n",
      "cnt: 0 - valLoss: 0.6658279895782471 - trainLoss: 0.6641933917999268\n",
      "cnt: 0 - valLoss: 0.6658276915550232 - trainLoss: 0.6641931533813477\n",
      "cnt: 0 - valLoss: 0.6658275723457336 - trainLoss: 0.6641929745674133\n",
      "cnt: 0 - valLoss: 0.6658273935317993 - trainLoss: 0.664192795753479\n",
      "cnt: 0 - valLoss: 0.6658270955085754 - trainLoss: 0.6641925573348999\n",
      "cnt: 0 - valLoss: 0.6658269762992859 - trainLoss: 0.6641923785209656\n",
      "cnt: 0 - valLoss: 0.6658267378807068 - trainLoss: 0.6641921401023865\n",
      "cnt: 0 - valLoss: 0.6658264994621277 - trainLoss: 0.6641919612884521\n",
      "cnt: 0 - valLoss: 0.6658263206481934 - trainLoss: 0.664191722869873\n",
      "cnt: 0 - valLoss: 0.665826141834259 - trainLoss: 0.6641916036605835\n",
      "cnt: 0 - valLoss: 0.6658259034156799 - trainLoss: 0.6641913056373596\n",
      "cnt: 0 - valLoss: 0.6658257246017456 - trainLoss: 0.6641911268234253\n",
      "cnt: 0 - valLoss: 0.6658254861831665 - trainLoss: 0.664190948009491\n",
      "cnt: 0 - valLoss: 0.665825366973877 - trainLoss: 0.6641907095909119\n",
      "cnt: 0 - valLoss: 0.6658251285552979 - trainLoss: 0.6641905307769775\n",
      "cnt: 0 - valLoss: 0.6658249497413635 - trainLoss: 0.6641902923583984\n",
      "cnt: 0 - valLoss: 0.6658247113227844 - trainLoss: 0.6641901135444641\n",
      "cnt: 0 - valLoss: 0.6658245325088501 - trainLoss: 0.6641899347305298\n",
      "cnt: 0 - valLoss: 0.6658243536949158 - trainLoss: 0.6641896963119507\n",
      "cnt: 0 - valLoss: 0.6658241152763367 - trainLoss: 0.6641895174980164\n",
      "cnt: 0 - valLoss: 0.6658239364624023 - trainLoss: 0.6641892790794373\n",
      "cnt: 0 - valLoss: 0.665823757648468 - trainLoss: 0.6641891598701477\n",
      "cnt: 0 - valLoss: 0.6658235192298889 - trainLoss: 0.6641889214515686\n",
      "cnt: 0 - valLoss: 0.6658232808113098 - trainLoss: 0.6641887426376343\n",
      "cnt: 0 - valLoss: 0.6658231616020203 - trainLoss: 0.6641885042190552\n",
      "cnt: 0 - valLoss: 0.6658229231834412 - trainLoss: 0.6641883254051208\n",
      "cnt: 0 - valLoss: 0.6658226847648621 - trainLoss: 0.6641880869865417\n",
      "cnt: 0 - valLoss: 0.6658225059509277 - trainLoss: 0.6641879081726074\n",
      "cnt: 0 - valLoss: 0.6658222675323486 - trainLoss: 0.6641877293586731\n",
      "cnt: 0 - valLoss: 0.6658220887184143 - trainLoss: 0.6641875505447388\n",
      "cnt: 0 - valLoss: 0.66582190990448 - trainLoss: 0.6641873121261597\n",
      "cnt: 0 - valLoss: 0.6658217310905457 - trainLoss: 0.6641870737075806\n",
      "cnt: 0 - valLoss: 0.6658214330673218 - trainLoss: 0.664186954498291\n",
      "cnt: 0 - valLoss: 0.6658213138580322 - trainLoss: 0.6641867160797119\n",
      "cnt: 0 - valLoss: 0.6658210754394531 - trainLoss: 0.6641864776611328\n",
      "cnt: 0 - valLoss: 0.665820837020874 - trainLoss: 0.6641862988471985\n",
      "cnt: 0 - valLoss: 0.6658206582069397 - trainLoss: 0.6641861200332642\n",
      "cnt: 0 - valLoss: 0.6658205389976501 - trainLoss: 0.6641859412193298\n",
      "cnt: 0 - valLoss: 0.6658202409744263 - trainLoss: 0.6641857028007507\n",
      "cnt: 0 - valLoss: 0.6658200025558472 - trainLoss: 0.6641855239868164\n",
      "cnt: 0 - valLoss: 0.6658198237419128 - trainLoss: 0.6641852855682373\n",
      "cnt: 0 - valLoss: 0.6658196449279785 - trainLoss: 0.664185106754303\n",
      "cnt: 0 - valLoss: 0.6658194661140442 - trainLoss: 0.6641849279403687\n",
      "cnt: 0 - valLoss: 0.6658192276954651 - trainLoss: 0.6641847491264343\n",
      "cnt: 0 - valLoss: 0.6658190488815308 - trainLoss: 0.6641845107078552\n",
      "cnt: 0 - valLoss: 0.6658188104629517 - trainLoss: 0.6641843318939209\n",
      "cnt: 0 - valLoss: 0.6658186316490173 - trainLoss: 0.6641840934753418\n",
      "cnt: 0 - valLoss: 0.6658183932304382 - trainLoss: 0.6641839146614075\n",
      "cnt: 0 - valLoss: 0.6658182740211487 - trainLoss: 0.6641837358474731\n",
      "cnt: 0 - valLoss: 0.6658180356025696 - trainLoss: 0.664183497428894\n",
      "cnt: 0 - valLoss: 0.6658177971839905 - trainLoss: 0.6641832590103149\n",
      "cnt: 0 - valLoss: 0.6658176183700562 - trainLoss: 0.6641831398010254\n",
      "cnt: 0 - valLoss: 0.6658173203468323 - trainLoss: 0.6641829013824463\n",
      "cnt: 0 - valLoss: 0.6658172011375427 - trainLoss: 0.6641826629638672\n",
      "cnt: 0 - valLoss: 0.6658170223236084 - trainLoss: 0.6641824841499329\n",
      "cnt: 0 - valLoss: 0.6658168435096741 - trainLoss: 0.6641823649406433\n",
      "cnt: 0 - valLoss: 0.6658165454864502 - trainLoss: 0.6641820669174194\n",
      "cnt: 0 - valLoss: 0.6658164262771606 - trainLoss: 0.6641818881034851\n",
      "cnt: 0 - valLoss: 0.6658161282539368 - trainLoss: 0.664181649684906\n",
      "cnt: 0 - valLoss: 0.6658159494400024 - trainLoss: 0.6641814708709717\n",
      "cnt: 0 - valLoss: 0.6658157706260681 - trainLoss: 0.6641812920570374\n",
      "cnt: 0 - valLoss: 0.665815532207489 - trainLoss: 0.6641810536384583\n",
      "cnt: 0 - valLoss: 0.6658153533935547 - trainLoss: 0.6641808748245239\n",
      "cnt: 0 - valLoss: 0.6658151149749756 - trainLoss: 0.6641806960105896\n",
      "cnt: 0 - valLoss: 0.6658149361610413 - trainLoss: 0.6641804575920105\n",
      "cnt: 0 - valLoss: 0.6658147573471069 - trainLoss: 0.6641802787780762\n",
      "cnt: 0 - valLoss: 0.6658145189285278 - trainLoss: 0.6641800999641418\n",
      "cnt: 0 - valLoss: 0.6658143401145935 - trainLoss: 0.6641799211502075\n",
      "cnt: 0 - valLoss: 0.6658141613006592 - trainLoss: 0.6641796231269836\n",
      "cnt: 0 - valLoss: 0.6658139824867249 - trainLoss: 0.6641794443130493\n",
      "cnt: 0 - valLoss: 0.665813684463501 - trainLoss: 0.664179265499115\n",
      "cnt: 0 - valLoss: 0.6658135056495667 - trainLoss: 0.6641790270805359\n",
      "cnt: 0 - valLoss: 0.6658133268356323 - trainLoss: 0.6641788482666016\n",
      "cnt: 0 - valLoss: 0.6658130884170532 - trainLoss: 0.6641786694526672\n",
      "cnt: 0 - valLoss: 0.6658129096031189 - trainLoss: 0.6641784310340881\n",
      "cnt: 0 - valLoss: 0.6658126711845398 - trainLoss: 0.6641782522201538\n",
      "cnt: 0 - valLoss: 0.6658124923706055 - trainLoss: 0.6641780734062195\n",
      "cnt: 0 - valLoss: 0.6658123135566711 - trainLoss: 0.6641778349876404\n",
      "cnt: 0 - valLoss: 0.665812075138092 - trainLoss: 0.664177656173706\n",
      "cnt: 0 - valLoss: 0.6658118367195129 - trainLoss: 0.6641774773597717\n",
      "cnt: 0 - valLoss: 0.6658117175102234 - trainLoss: 0.6641772389411926\n",
      "cnt: 0 - valLoss: 0.6658114194869995 - trainLoss: 0.6641770005226135\n",
      "cnt: 0 - valLoss: 0.6658112406730652 - trainLoss: 0.6641768217086792\n",
      "cnt: 0 - valLoss: 0.6658110022544861 - trainLoss: 0.6641765832901001\n",
      "cnt: 0 - valLoss: 0.6658108234405518 - trainLoss: 0.6641764640808105\n",
      "cnt: 0 - valLoss: 0.6658106446266174 - trainLoss: 0.6641762256622314\n",
      "cnt: 0 - valLoss: 0.6658104062080383 - trainLoss: 0.6641759872436523\n",
      "cnt: 0 - valLoss: 0.665810227394104 - trainLoss: 0.6641758680343628\n",
      "cnt: 0 - valLoss: 0.6658099889755249 - trainLoss: 0.6641755700111389\n",
      "cnt: 0 - valLoss: 0.6658098101615906 - trainLoss: 0.6641753911972046\n",
      "cnt: 0 - valLoss: 0.6658095717430115 - trainLoss: 0.6641752123832703\n",
      "cnt: 0 - valLoss: 0.6658093929290771 - trainLoss: 0.6641749739646912\n",
      "cnt: 0 - valLoss: 0.665809154510498 - trainLoss: 0.6641747951507568\n",
      "cnt: 0 - valLoss: 0.6658089756965637 - trainLoss: 0.6641746163368225\n",
      "cnt: 0 - valLoss: 0.6658087968826294 - trainLoss: 0.6641743779182434\n",
      "cnt: 0 - valLoss: 0.6658085584640503 - trainLoss: 0.6641741991043091\n",
      "cnt: 0 - valLoss: 0.6658083200454712 - trainLoss: 0.66417396068573\n",
      "cnt: 0 - valLoss: 0.6658080816268921 - trainLoss: 0.6641737818717957\n",
      "cnt: 0 - valLoss: 0.6658079028129578 - trainLoss: 0.6641735434532166\n",
      "cnt: 0 - valLoss: 0.6658077239990234 - trainLoss: 0.6641733646392822\n",
      "cnt: 0 - valLoss: 0.6658074855804443 - trainLoss: 0.6641731858253479\n",
      "cnt: 0 - valLoss: 0.66580730676651 - trainLoss: 0.6641729474067688\n",
      "cnt: 0 - valLoss: 0.6658070683479309 - trainLoss: 0.6641727089881897\n",
      "cnt: 0 - valLoss: 0.6658068895339966 - trainLoss: 0.6641725301742554\n",
      "cnt: 0 - valLoss: 0.6658066511154175 - trainLoss: 0.6641722321510315\n",
      "cnt: 0 - valLoss: 0.6658065319061279 - trainLoss: 0.6641720533370972\n",
      "cnt: 0 - valLoss: 0.665806233882904 - trainLoss: 0.6641718745231628\n",
      "cnt: 0 - valLoss: 0.6658060550689697 - trainLoss: 0.6641716957092285\n",
      "cnt: 0 - valLoss: 0.6658058762550354 - trainLoss: 0.6641714572906494\n",
      "cnt: 0 - valLoss: 0.6658056378364563 - trainLoss: 0.6641712188720703\n",
      "cnt: 0 - valLoss: 0.665805459022522 - trainLoss: 0.6641710996627808\n",
      "cnt: 0 - valLoss: 0.6658051609992981 - trainLoss: 0.6641708016395569\n",
      "cnt: 0 - valLoss: 0.6658049821853638 - trainLoss: 0.6641706228256226\n",
      "cnt: 0 - valLoss: 0.6658047437667847 - trainLoss: 0.6641703844070435\n",
      "cnt: 0 - valLoss: 0.6658045649528503 - trainLoss: 0.6641702055931091\n",
      "cnt: 0 - valLoss: 0.6658043265342712 - trainLoss: 0.66416996717453\n",
      "cnt: 0 - valLoss: 0.6658041477203369 - trainLoss: 0.6641697883605957\n",
      "cnt: 0 - valLoss: 0.6658039689064026 - trainLoss: 0.6641696095466614\n",
      "cnt: 0 - valLoss: 0.6658037304878235 - trainLoss: 0.6641693115234375\n",
      "cnt: 0 - valLoss: 0.6658035516738892 - trainLoss: 0.6641691327095032\n",
      "cnt: 0 - valLoss: 0.6658033728599548 - trainLoss: 0.6641688942909241\n",
      "cnt: 0 - valLoss: 0.6658031344413757 - trainLoss: 0.6641687154769897\n",
      "cnt: 0 - valLoss: 0.6658028960227966 - trainLoss: 0.6641684770584106\n",
      "cnt: 0 - valLoss: 0.6658027172088623 - trainLoss: 0.6641682386398315\n",
      "cnt: 0 - valLoss: 0.6658024787902832 - trainLoss: 0.664168119430542\n",
      "cnt: 0 - valLoss: 0.6658022403717041 - trainLoss: 0.6641678810119629\n",
      "cnt: 0 - valLoss: 0.665802001953125 - trainLoss: 0.6641676425933838\n",
      "cnt: 0 - valLoss: 0.6658017635345459 - trainLoss: 0.6641674637794495\n",
      "cnt: 0 - valLoss: 0.6658015847206116 - trainLoss: 0.6641672849655151\n",
      "cnt: 0 - valLoss: 0.6658013463020325 - trainLoss: 0.664167046546936\n",
      "cnt: 0 - valLoss: 0.6658011674880981 - trainLoss: 0.6641668677330017\n",
      "cnt: 0 - valLoss: 0.665800929069519 - trainLoss: 0.6641666889190674\n",
      "cnt: 0 - valLoss: 0.6658006906509399 - trainLoss: 0.6641664505004883\n",
      "cnt: 0 - valLoss: 0.6658005714416504 - trainLoss: 0.664166271686554\n",
      "cnt: 0 - valLoss: 0.6658002734184265 - trainLoss: 0.6641660928726196\n",
      "cnt: 0 - valLoss: 0.6658000349998474 - trainLoss: 0.6641659140586853\n",
      "cnt: 0 - valLoss: 0.6657998561859131 - trainLoss: 0.6641656756401062\n",
      "cnt: 0 - valLoss: 0.665799617767334 - trainLoss: 0.6641654968261719\n",
      "cnt: 0 - valLoss: 0.6657994389533997 - trainLoss: 0.6641653776168823\n",
      "cnt: 0 - valLoss: 0.6657992601394653 - trainLoss: 0.6641651391983032\n",
      "cnt: 0 - valLoss: 0.6657990217208862 - trainLoss: 0.6641649007797241\n",
      "cnt: 0 - valLoss: 0.6657988429069519 - trainLoss: 0.6641647219657898\n",
      "cnt: 0 - valLoss: 0.665798544883728 - trainLoss: 0.6641645431518555\n",
      "cnt: 0 - valLoss: 0.6657984256744385 - trainLoss: 0.6641643047332764\n",
      "cnt: 0 - valLoss: 0.6657981276512146 - trainLoss: 0.664164125919342\n",
      "cnt: 0 - valLoss: 0.6657979488372803 - trainLoss: 0.6641640067100525\n",
      "cnt: 0 - valLoss: 0.6657977104187012 - trainLoss: 0.6641637086868286\n",
      "cnt: 0 - valLoss: 0.6657975316047668 - trainLoss: 0.6641635298728943\n",
      "cnt: 0 - valLoss: 0.6657972931861877 - trainLoss: 0.66416335105896\n",
      "cnt: 0 - valLoss: 0.6657971143722534 - trainLoss: 0.6641631722450256\n",
      "cnt: 0 - valLoss: 0.6657968759536743 - trainLoss: 0.6641629338264465\n",
      "cnt: 0 - valLoss: 0.66579669713974 - trainLoss: 0.6641627550125122\n",
      "cnt: 0 - valLoss: 0.6657964587211609 - trainLoss: 0.6641625761985779\n",
      "cnt: 0 - valLoss: 0.6657962799072266 - trainLoss: 0.6641623377799988\n",
      "cnt: 0 - valLoss: 0.6657961010932922 - trainLoss: 0.6641621589660645\n",
      "cnt: 0 - valLoss: 0.6657958030700684 - trainLoss: 0.6641619205474854\n",
      "cnt: 0 - valLoss: 0.665795624256134 - trainLoss: 0.6641618013381958\n",
      "cnt: 0 - valLoss: 0.6657953858375549 - trainLoss: 0.6641615629196167\n",
      "cnt: 0 - valLoss: 0.6657952070236206 - trainLoss: 0.6641613841056824\n",
      "cnt: 0 - valLoss: 0.6657949686050415 - trainLoss: 0.6641611456871033\n",
      "cnt: 0 - valLoss: 0.6657947301864624 - trainLoss: 0.6641610264778137\n",
      "cnt: 0 - valLoss: 0.6657945513725281 - trainLoss: 0.6641607880592346\n",
      "cnt: 0 - valLoss: 0.665794312953949 - trainLoss: 0.6641606092453003\n",
      "cnt: 0 - valLoss: 0.6657941341400146 - trainLoss: 0.6641603708267212\n",
      "cnt: 0 - valLoss: 0.6657939553260803 - trainLoss: 0.6641601920127869\n",
      "cnt: 0 - valLoss: 0.6657936573028564 - trainLoss: 0.6641600131988525\n",
      "cnt: 0 - valLoss: 0.6657934784889221 - trainLoss: 0.6641597747802734\n",
      "cnt: 0 - valLoss: 0.665793240070343 - trainLoss: 0.6641595959663391\n",
      "cnt: 0 - valLoss: 0.6657930612564087 - trainLoss: 0.6641594171524048\n",
      "cnt: 0 - valLoss: 0.6657928228378296 - trainLoss: 0.6641591787338257\n",
      "cnt: 0 - valLoss: 0.6657926440238953 - trainLoss: 0.6641589999198914\n",
      "cnt: 0 - valLoss: 0.6657924056053162 - trainLoss: 0.664158821105957\n",
      "cnt: 0 - valLoss: 0.6657921671867371 - trainLoss: 0.6641586422920227\n",
      "cnt: 0 - valLoss: 0.6657919883728027 - trainLoss: 0.6641585230827332\n",
      "cnt: 0 - valLoss: 0.6657917499542236 - trainLoss: 0.6641582250595093\n",
      "cnt: 0 - valLoss: 0.6657915711402893 - trainLoss: 0.6641581058502197\n",
      "cnt: 0 - valLoss: 0.6657913327217102 - trainLoss: 0.6641578674316406\n",
      "cnt: 0 - valLoss: 0.6657910943031311 - trainLoss: 0.6641576886177063\n",
      "cnt: 0 - valLoss: 0.6657909154891968 - trainLoss: 0.6641574501991272\n",
      "cnt: 0 - valLoss: 0.6657906770706177 - trainLoss: 0.6641572713851929\n",
      "cnt: 0 - valLoss: 0.6657904982566833 - trainLoss: 0.6641570925712585\n",
      "cnt: 0 - valLoss: 0.6657902598381042 - trainLoss: 0.6641568541526794\n",
      "cnt: 0 - valLoss: 0.6657900214195251 - trainLoss: 0.6641566753387451\n",
      "cnt: 0 - valLoss: 0.665789783000946 - trainLoss: 0.6641564965248108\n",
      "cnt: 0 - valLoss: 0.6657896041870117 - trainLoss: 0.6641563177108765\n",
      "cnt: 0 - valLoss: 0.6657893657684326 - trainLoss: 0.6641561388969421\n",
      "cnt: 0 - valLoss: 0.6657891869544983 - trainLoss: 0.6641559600830078\n",
      "cnt: 0 - valLoss: 0.6657889485359192 - trainLoss: 0.6641557812690735\n",
      "cnt: 0 - valLoss: 0.6657887101173401 - trainLoss: 0.6641555428504944\n",
      "cnt: 0 - valLoss: 0.6657885313034058 - trainLoss: 0.6641553640365601\n",
      "cnt: 0 - valLoss: 0.6657882928848267 - trainLoss: 0.6641551852226257\n",
      "cnt: 0 - valLoss: 0.6657880544662476 - trainLoss: 0.6641549468040466\n",
      "cnt: 0 - valLoss: 0.6657878756523132 - trainLoss: 0.6641547679901123\n",
      "cnt: 0 - valLoss: 0.6657875776290894 - trainLoss: 0.664154589176178\n",
      "cnt: 0 - valLoss: 0.665787398815155 - trainLoss: 0.6641543507575989\n",
      "cnt: 0 - valLoss: 0.6657871603965759 - trainLoss: 0.6641542315483093\n",
      "cnt: 0 - valLoss: 0.6657869815826416 - trainLoss: 0.6641539931297302\n",
      "cnt: 0 - valLoss: 0.6657867431640625 - trainLoss: 0.6641537547111511\n",
      "cnt: 0 - valLoss: 0.6657865047454834 - trainLoss: 0.6641536355018616\n",
      "cnt: 0 - valLoss: 0.6657863259315491 - trainLoss: 0.6641533970832825\n",
      "cnt: 0 - valLoss: 0.6657861471176147 - trainLoss: 0.6641532182693481\n",
      "cnt: 0 - valLoss: 0.6657859086990356 - trainLoss: 0.6641530394554138\n",
      "cnt: 0 - valLoss: 0.6657857298851013 - trainLoss: 0.6641528606414795\n",
      "cnt: 0 - valLoss: 0.6657854318618774 - trainLoss: 0.6641526222229004\n",
      "cnt: 0 - valLoss: 0.6657852530479431 - trainLoss: 0.6641524434089661\n",
      "cnt: 0 - valLoss: 0.665785014629364 - trainLoss: 0.6641523241996765\n",
      "cnt: 0 - valLoss: 0.6657847762107849 - trainLoss: 0.6641520261764526\n",
      "cnt: 0 - valLoss: 0.6657845973968506 - trainLoss: 0.6641518473625183\n",
      "cnt: 0 - valLoss: 0.6657843589782715 - trainLoss: 0.6641517281532288\n",
      "cnt: 0 - valLoss: 0.6657840609550476 - trainLoss: 0.6641515493392944\n",
      "cnt: 0 - valLoss: 0.6657839417457581 - trainLoss: 0.6641512513160706\n",
      "cnt: 0 - valLoss: 0.665783703327179 - trainLoss: 0.6641510725021362\n",
      "cnt: 0 - valLoss: 0.6657834649085999 - trainLoss: 0.6641508340835571\n",
      "cnt: 0 - valLoss: 0.6657832264900208 - trainLoss: 0.6641506552696228\n",
      "cnt: 0 - valLoss: 0.6657830476760864 - trainLoss: 0.6641504764556885\n",
      "cnt: 0 - valLoss: 0.6657828092575073 - trainLoss: 0.6641502976417542\n",
      "cnt: 0 - valLoss: 0.6657825708389282 - trainLoss: 0.664150059223175\n",
      "cnt: 0 - valLoss: 0.6657823920249939 - trainLoss: 0.6641498804092407\n",
      "cnt: 0 - valLoss: 0.6657822132110596 - trainLoss: 0.6641497015953064\n",
      "cnt: 0 - valLoss: 0.6657819151878357 - trainLoss: 0.6641494631767273\n",
      "cnt: 0 - valLoss: 0.6657816767692566 - trainLoss: 0.664149284362793\n",
      "cnt: 0 - valLoss: 0.6657814979553223 - trainLoss: 0.6641491055488586\n",
      "cnt: 0 - valLoss: 0.6657812595367432 - trainLoss: 0.6641489863395691\n",
      "cnt: 0 - valLoss: 0.6657810807228088 - trainLoss: 0.6641486883163452\n",
      "cnt: 0 - valLoss: 0.6657808423042297 - trainLoss: 0.6641485095024109\n",
      "cnt: 0 - valLoss: 0.6657806634902954 - trainLoss: 0.6641483306884766\n",
      "cnt: 0 - valLoss: 0.6657804250717163 - trainLoss: 0.6641481518745422\n",
      "cnt: 0 - valLoss: 0.665780246257782 - trainLoss: 0.6641479730606079\n",
      "cnt: 0 - valLoss: 0.6657799482345581 - trainLoss: 0.6641477346420288\n",
      "cnt: 0 - valLoss: 0.6657797694206238 - trainLoss: 0.6641474962234497\n",
      "cnt: 0 - valLoss: 0.6657795310020447 - trainLoss: 0.6641473174095154\n",
      "cnt: 0 - valLoss: 0.6657793521881104 - trainLoss: 0.664147138595581\n",
      "cnt: 0 - valLoss: 0.6657790541648865 - trainLoss: 0.6641469597816467\n",
      "cnt: 0 - valLoss: 0.6657788753509521 - trainLoss: 0.6641467213630676\n",
      "cnt: 0 - valLoss: 0.6657786965370178 - trainLoss: 0.6641465425491333\n",
      "cnt: 0 - valLoss: 0.665778398513794 - trainLoss: 0.664146363735199\n",
      "cnt: 0 - valLoss: 0.6657782793045044 - trainLoss: 0.6641461253166199\n",
      "cnt: 0 - valLoss: 0.6657779812812805 - trainLoss: 0.6641459465026855\n",
      "cnt: 0 - valLoss: 0.6657778024673462 - trainLoss: 0.6641457676887512\n",
      "cnt: 0 - valLoss: 0.6657775640487671 - trainLoss: 0.6641455292701721\n",
      "cnt: 0 - valLoss: 0.6657773852348328 - trainLoss: 0.6641453504562378\n",
      "cnt: 0 - valLoss: 0.6657771468162537 - trainLoss: 0.6641451716423035\n",
      "cnt: 0 - valLoss: 0.6657769083976746 - trainLoss: 0.6641449332237244\n",
      "cnt: 0 - valLoss: 0.6657767295837402 - trainLoss: 0.66414475440979\n",
      "cnt: 0 - valLoss: 0.6657764315605164 - trainLoss: 0.6641445755958557\n",
      "cnt: 0 - valLoss: 0.6657763123512268 - trainLoss: 0.6641443967819214\n",
      "cnt: 0 - valLoss: 0.6657760143280029 - trainLoss: 0.6641442179679871\n",
      "cnt: 0 - valLoss: 0.6657758355140686 - trainLoss: 0.664143979549408\n",
      "cnt: 0 - valLoss: 0.6657755970954895 - trainLoss: 0.6641438007354736\n",
      "cnt: 0 - valLoss: 0.6657753586769104 - trainLoss: 0.6641435623168945\n",
      "cnt: 0 - valLoss: 0.6657751798629761 - trainLoss: 0.6641433238983154\n",
      "cnt: 0 - valLoss: 0.665774941444397 - trainLoss: 0.6641432046890259\n",
      "cnt: 0 - valLoss: 0.6657747626304626 - trainLoss: 0.6641429662704468\n",
      "cnt: 0 - valLoss: 0.6657744646072388 - trainLoss: 0.6641427278518677\n",
      "cnt: 0 - valLoss: 0.6657742857933044 - trainLoss: 0.6641426086425781\n",
      "cnt: 0 - valLoss: 0.6657740473747253 - trainLoss: 0.6641423106193542\n",
      "cnt: 0 - valLoss: 0.665773868560791 - trainLoss: 0.6641421318054199\n",
      "cnt: 0 - valLoss: 0.6657736897468567 - trainLoss: 0.6641419529914856\n",
      "cnt: 0 - valLoss: 0.6657734513282776 - trainLoss: 0.6641417741775513\n",
      "cnt: 0 - valLoss: 0.6657732129096985 - trainLoss: 0.6641415953636169\n",
      "cnt: 0 - valLoss: 0.6657729148864746 - trainLoss: 0.6641414165496826\n",
      "cnt: 0 - valLoss: 0.6657727956771851 - trainLoss: 0.6641411781311035\n",
      "cnt: 0 - valLoss: 0.665772557258606 - trainLoss: 0.6641409993171692\n",
      "cnt: 0 - valLoss: 0.6657723188400269 - trainLoss: 0.6641407608985901\n",
      "cnt: 0 - valLoss: 0.6657721400260925 - trainLoss: 0.6641405820846558\n",
      "cnt: 0 - valLoss: 0.6657719016075134 - trainLoss: 0.6641403436660767\n",
      "cnt: 0 - valLoss: 0.6657716631889343 - trainLoss: 0.6641401648521423\n",
      "cnt: 0 - valLoss: 0.6657714247703552 - trainLoss: 0.664139986038208\n",
      "cnt: 0 - valLoss: 0.6657712459564209 - trainLoss: 0.6641397476196289\n",
      "cnt: 0 - valLoss: 0.6657710075378418 - trainLoss: 0.6641395688056946\n",
      "cnt: 0 - valLoss: 0.6657707691192627 - trainLoss: 0.6641393303871155\n",
      "cnt: 0 - valLoss: 0.6657705903053284 - trainLoss: 0.6641391515731812\n",
      "cnt: 0 - valLoss: 0.6657702922821045 - trainLoss: 0.6641389727592468\n",
      "cnt: 0 - valLoss: 0.6657701134681702 - trainLoss: 0.6641387939453125\n",
      "cnt: 0 - valLoss: 0.6657698750495911 - trainLoss: 0.6641384959220886\n",
      "cnt: 0 - valLoss: 0.6657696962356567 - trainLoss: 0.6641383171081543\n",
      "cnt: 0 - valLoss: 0.6657694578170776 - trainLoss: 0.66413813829422\n",
      "cnt: 0 - valLoss: 0.6657692790031433 - trainLoss: 0.6641379594802856\n",
      "cnt: 0 - valLoss: 0.6657690405845642 - trainLoss: 0.6641376614570618\n",
      "cnt: 0 - valLoss: 0.6657688021659851 - trainLoss: 0.6641375422477722\n",
      "cnt: 0 - valLoss: 0.665768563747406 - trainLoss: 0.6641373038291931\n",
      "cnt: 0 - valLoss: 0.6657683849334717 - trainLoss: 0.6641371250152588\n",
      "cnt: 0 - valLoss: 0.6657681465148926 - trainLoss: 0.6641368865966797\n",
      "cnt: 0 - valLoss: 0.6657679080963135 - trainLoss: 0.6641367077827454\n",
      "cnt: 0 - valLoss: 0.6657677292823792 - trainLoss: 0.664136528968811\n",
      "cnt: 0 - valLoss: 0.6657674908638 - trainLoss: 0.6641362905502319\n",
      "cnt: 0 - valLoss: 0.665767252445221 - trainLoss: 0.6641361117362976\n",
      "cnt: 0 - valLoss: 0.6657670736312866 - trainLoss: 0.6641359329223633\n",
      "cnt: 0 - valLoss: 0.6657668352127075 - trainLoss: 0.6641356945037842\n",
      "cnt: 0 - valLoss: 0.6657665967941284 - trainLoss: 0.6641355156898499\n",
      "cnt: 0 - valLoss: 0.6657663583755493 - trainLoss: 0.6641352772712708\n",
      "cnt: 0 - valLoss: 0.665766179561615 - trainLoss: 0.6641350984573364\n",
      "cnt: 0 - valLoss: 0.6657659411430359 - trainLoss: 0.6641349196434021\n",
      "cnt: 0 - valLoss: 0.6657657623291016 - trainLoss: 0.664134681224823\n",
      "cnt: 0 - valLoss: 0.6657654643058777 - trainLoss: 0.6641345024108887\n",
      "cnt: 0 - valLoss: 0.6657652854919434 - trainLoss: 0.6641342639923096\n",
      "cnt: 0 - valLoss: 0.6657650470733643 - trainLoss: 0.6641340851783752\n",
      "cnt: 0 - valLoss: 0.6657648682594299 - trainLoss: 0.6641337871551514\n",
      "cnt: 0 - valLoss: 0.6657646298408508 - trainLoss: 0.6641336679458618\n",
      "cnt: 0 - valLoss: 0.6657643914222717 - trainLoss: 0.6641334891319275\n",
      "cnt: 0 - valLoss: 0.6657642126083374 - trainLoss: 0.6641332507133484\n",
      "cnt: 0 - valLoss: 0.6657639145851135 - trainLoss: 0.6641330718994141\n",
      "cnt: 0 - valLoss: 0.665763795375824 - trainLoss: 0.664132833480835\n",
      "cnt: 0 - valLoss: 0.6657634973526001 - trainLoss: 0.6641325950622559\n",
      "cnt: 0 - valLoss: 0.665763258934021 - trainLoss: 0.6641324162483215\n",
      "cnt: 0 - valLoss: 0.6657630205154419 - trainLoss: 0.6641322374343872\n",
      "cnt: 0 - valLoss: 0.6657628417015076 - trainLoss: 0.6641321182250977\n",
      "cnt: 0 - valLoss: 0.6657626628875732 - trainLoss: 0.6641318202018738\n",
      "cnt: 0 - valLoss: 0.6657624244689941 - trainLoss: 0.6641316413879395\n",
      "cnt: 0 - valLoss: 0.665762186050415 - trainLoss: 0.6641314029693604\n",
      "cnt: 0 - valLoss: 0.6657619476318359 - trainLoss: 0.6641312837600708\n",
      "cnt: 0 - valLoss: 0.6657617688179016 - trainLoss: 0.6641310453414917\n",
      "cnt: 0 - valLoss: 0.6657614707946777 - trainLoss: 0.6641308069229126\n",
      "cnt: 0 - valLoss: 0.6657612919807434 - trainLoss: 0.6641306281089783\n",
      "cnt: 0 - valLoss: 0.6657610535621643 - trainLoss: 0.6641303896903992\n",
      "cnt: 0 - valLoss: 0.66576087474823 - trainLoss: 0.6641302108764648\n",
      "cnt: 0 - valLoss: 0.6657605767250061 - trainLoss: 0.6641300320625305\n",
      "cnt: 0 - valLoss: 0.6657603979110718 - trainLoss: 0.6641297936439514\n",
      "cnt: 0 - valLoss: 0.6657602190971375 - trainLoss: 0.6641296148300171\n",
      "cnt: 0 - valLoss: 0.6657599806785583 - trainLoss: 0.664129376411438\n",
      "cnt: 0 - valLoss: 0.6657596826553345 - trainLoss: 0.6641291975975037\n",
      "cnt: 0 - valLoss: 0.6657595038414001 - trainLoss: 0.6641290187835693\n",
      "cnt: 0 - valLoss: 0.665759265422821 - trainLoss: 0.6641287803649902\n",
      "cnt: 0 - valLoss: 0.6657590270042419 - trainLoss: 0.6641286015510559\n",
      "cnt: 0 - valLoss: 0.6657588481903076 - trainLoss: 0.6641284227371216\n",
      "cnt: 0 - valLoss: 0.6657586097717285 - trainLoss: 0.6641281843185425\n",
      "cnt: 0 - valLoss: 0.6657583713531494 - trainLoss: 0.6641280055046082\n",
      "cnt: 0 - valLoss: 0.6657581329345703 - trainLoss: 0.6641277074813843\n",
      "cnt: 0 - valLoss: 0.665757954120636 - trainLoss: 0.6641275882720947\n",
      "cnt: 0 - valLoss: 0.6657576560974121 - trainLoss: 0.6641273498535156\n",
      "cnt: 0 - valLoss: 0.6657575368881226 - trainLoss: 0.6641271114349365\n",
      "cnt: 0 - valLoss: 0.6657572388648987 - trainLoss: 0.6641269326210022\n",
      "cnt: 0 - valLoss: 0.6657570600509644 - trainLoss: 0.6641267538070679\n",
      "cnt: 0 - valLoss: 0.6657568216323853 - trainLoss: 0.6641265153884888\n",
      "cnt: 0 - valLoss: 0.6657565832138062 - trainLoss: 0.6641263365745544\n",
      "cnt: 0 - valLoss: 0.6657564043998718 - trainLoss: 0.6641260981559753\n",
      "cnt: 0 - valLoss: 0.665756106376648 - trainLoss: 0.6641259789466858\n",
      "cnt: 0 - valLoss: 0.6657559275627136 - trainLoss: 0.6641257405281067\n",
      "cnt: 0 - valLoss: 0.6657556891441345 - trainLoss: 0.6641255617141724\n",
      "cnt: 0 - valLoss: 0.6657555103302002 - trainLoss: 0.6641253232955933\n",
      "cnt: 0 - valLoss: 0.6657552719116211 - trainLoss: 0.6641251444816589\n",
      "cnt: 0 - valLoss: 0.665755033493042 - trainLoss: 0.6641249060630798\n",
      "cnt: 0 - valLoss: 0.6657547950744629 - trainLoss: 0.6641246676445007\n",
      "cnt: 0 - valLoss: 0.6657546162605286 - trainLoss: 0.6641245484352112\n",
      "cnt: 0 - valLoss: 0.6657543182373047 - trainLoss: 0.6641242504119873\n",
      "cnt: 0 - valLoss: 0.6657541394233704 - trainLoss: 0.664124071598053\n",
      "cnt: 0 - valLoss: 0.6657539010047913 - trainLoss: 0.6641238927841187\n",
      "cnt: 0 - valLoss: 0.6657536625862122 - trainLoss: 0.6641237139701843\n",
      "cnt: 0 - valLoss: 0.6657534241676331 - trainLoss: 0.6641234755516052\n",
      "cnt: 0 - valLoss: 0.6657532453536987 - trainLoss: 0.6641232967376709\n",
      "cnt: 0 - valLoss: 0.6657530069351196 - trainLoss: 0.6641231179237366\n",
      "cnt: 0 - valLoss: 0.6657527685165405 - trainLoss: 0.6641228795051575\n",
      "cnt: 0 - valLoss: 0.6657525300979614 - trainLoss: 0.6641227006912231\n",
      "cnt: 0 - valLoss: 0.6657522916793823 - trainLoss: 0.664122462272644\n",
      "cnt: 0 - valLoss: 0.665752112865448 - trainLoss: 0.6641222238540649\n",
      "cnt: 0 - valLoss: 0.6657518744468689 - trainLoss: 0.6641221046447754\n",
      "cnt: 0 - valLoss: 0.6657516956329346 - trainLoss: 0.6641218662261963\n",
      "cnt: 0 - valLoss: 0.6657513976097107 - trainLoss: 0.6641216278076172\n",
      "cnt: 0 - valLoss: 0.6657511591911316 - trainLoss: 0.6641214489936829\n",
      "cnt: 0 - valLoss: 0.6657509803771973 - trainLoss: 0.6641212105751038\n",
      "cnt: 0 - valLoss: 0.6657507419586182 - trainLoss: 0.6641210317611694\n",
      "cnt: 0 - valLoss: 0.6657505035400391 - trainLoss: 0.6641207933425903\n",
      "cnt: 0 - valLoss: 0.6657503247261047 - trainLoss: 0.664120614528656\n",
      "cnt: 0 - valLoss: 0.6657500267028809 - trainLoss: 0.6641203761100769\n",
      "cnt: 0 - valLoss: 0.6657497882843018 - trainLoss: 0.6641201972961426\n",
      "cnt: 0 - valLoss: 0.6657496094703674 - trainLoss: 0.6641199588775635\n",
      "cnt: 0 - valLoss: 0.6657493710517883 - trainLoss: 0.6641197800636292\n",
      "cnt: 0 - valLoss: 0.6657491326332092 - trainLoss: 0.6641196012496948\n",
      "cnt: 0 - valLoss: 0.6657489538192749 - trainLoss: 0.6641193628311157\n",
      "cnt: 0 - valLoss: 0.6657487154006958 - trainLoss: 0.6641191840171814\n",
      "cnt: 0 - valLoss: 0.6657484769821167 - trainLoss: 0.6641188859939575\n",
      "cnt: 0 - valLoss: 0.6657482385635376 - trainLoss: 0.664118766784668\n",
      "cnt: 0 - valLoss: 0.6657480597496033 - trainLoss: 0.6641185283660889\n",
      "cnt: 0 - valLoss: 0.6657477617263794 - trainLoss: 0.6641182899475098\n",
      "cnt: 0 - valLoss: 0.6657475829124451 - trainLoss: 0.6641181111335754\n",
      "cnt: 0 - valLoss: 0.665747344493866 - trainLoss: 0.6641178727149963\n",
      "cnt: 0 - valLoss: 0.6657471656799316 - trainLoss: 0.664117693901062\n",
      "cnt: 0 - valLoss: 0.6657468676567078 - trainLoss: 0.6641175150871277\n",
      "cnt: 0 - valLoss: 0.6657466888427734 - trainLoss: 0.6641172766685486\n",
      "cnt: 0 - valLoss: 0.6657464504241943 - trainLoss: 0.6641170382499695\n",
      "cnt: 0 - valLoss: 0.6657462120056152 - trainLoss: 0.6641168594360352\n",
      "cnt: 0 - valLoss: 0.6657459735870361 - trainLoss: 0.6641166806221008\n",
      "cnt: 0 - valLoss: 0.6657457947731018 - trainLoss: 0.6641165018081665\n",
      "cnt: 0 - valLoss: 0.6657455563545227 - trainLoss: 0.6641162037849426\n",
      "cnt: 0 - valLoss: 0.6657453179359436 - trainLoss: 0.6641160249710083\n",
      "cnt: 0 - valLoss: 0.6657450795173645 - trainLoss: 0.664115846157074\n",
      "cnt: 0 - valLoss: 0.6657448410987854 - trainLoss: 0.6641155481338501\n",
      "cnt: 0 - valLoss: 0.6657446026802063 - trainLoss: 0.6641153693199158\n",
      "cnt: 0 - valLoss: 0.6657443642616272 - trainLoss: 0.6641151905059814\n",
      "cnt: 0 - valLoss: 0.6657441258430481 - trainLoss: 0.6641149520874023\n",
      "cnt: 0 - valLoss: 0.6657439470291138 - trainLoss: 0.6641147136688232\n",
      "cnt: 0 - valLoss: 0.6657436490058899 - trainLoss: 0.6641145944595337\n",
      "cnt: 0 - valLoss: 0.6657434701919556 - trainLoss: 0.6641143560409546\n",
      "cnt: 0 - valLoss: 0.6657432317733765 - trainLoss: 0.6641141176223755\n",
      "cnt: 0 - valLoss: 0.6657429337501526 - trainLoss: 0.6641138792037964\n",
      "cnt: 0 - valLoss: 0.6657426953315735 - trainLoss: 0.6641137599945068\n",
      "cnt: 0 - valLoss: 0.6657424569129944 - trainLoss: 0.6641135215759277\n",
      "cnt: 0 - valLoss: 0.6657422780990601 - trainLoss: 0.6641133427619934\n",
      "cnt: 0 - valLoss: 0.6657419800758362 - trainLoss: 0.6641131043434143\n",
      "cnt: 0 - valLoss: 0.6657417416572571 - trainLoss: 0.6641128659248352\n",
      "cnt: 0 - valLoss: 0.6657415628433228 - trainLoss: 0.6641126871109009\n",
      "cnt: 0 - valLoss: 0.6657412648200989 - trainLoss: 0.6641125082969666\n",
      "cnt: 0 - valLoss: 0.6657410264015198 - trainLoss: 0.6641122698783875\n",
      "cnt: 0 - valLoss: 0.6657407879829407 - trainLoss: 0.6641120910644531\n",
      "cnt: 0 - valLoss: 0.6657406091690063 - trainLoss: 0.6641119122505188\n",
      "cnt: 0 - valLoss: 0.6657403111457825 - trainLoss: 0.6641116142272949\n",
      "cnt: 0 - valLoss: 0.6657400727272034 - trainLoss: 0.6641114354133606\n",
      "cnt: 0 - valLoss: 0.6657398343086243 - trainLoss: 0.6641112565994263\n",
      "cnt: 0 - valLoss: 0.6657396554946899 - trainLoss: 0.6641110181808472\n",
      "cnt: 0 - valLoss: 0.6657393574714661 - trainLoss: 0.6641108393669128\n",
      "cnt: 0 - valLoss: 0.6657390594482422 - trainLoss: 0.6641106009483337\n",
      "cnt: 0 - valLoss: 0.6657388806343079 - trainLoss: 0.6641103625297546\n",
      "cnt: 0 - valLoss: 0.6657386422157288 - trainLoss: 0.6641102433204651\n",
      "cnt: 0 - valLoss: 0.6657384037971497 - trainLoss: 0.6641099452972412\n",
      "cnt: 0 - valLoss: 0.6657382249832153 - trainLoss: 0.6641097664833069\n",
      "cnt: 0 - valLoss: 0.6657379269599915 - trainLoss: 0.6641095876693726\n",
      "cnt: 0 - valLoss: 0.6657376885414124 - trainLoss: 0.6641093492507935\n",
      "cnt: 0 - valLoss: 0.6657374501228333 - trainLoss: 0.6641091704368591\n",
      "cnt: 0 - valLoss: 0.6657372117042542 - trainLoss: 0.6641088724136353\n",
      "cnt: 0 - valLoss: 0.6657369136810303 - trainLoss: 0.6641087532043457\n",
      "cnt: 0 - valLoss: 0.665736734867096 - trainLoss: 0.6641085147857666\n",
      "cnt: 0 - valLoss: 0.6657364964485168 - trainLoss: 0.6641082763671875\n",
      "cnt: 0 - valLoss: 0.665736198425293 - trainLoss: 0.6641080975532532\n",
      "cnt: 0 - valLoss: 0.6657360196113586 - trainLoss: 0.6641078591346741\n",
      "cnt: 0 - valLoss: 0.6657357811927795 - trainLoss: 0.6641076803207397\n",
      "cnt: 0 - valLoss: 0.6657355427742004 - trainLoss: 0.6641075015068054\n",
      "cnt: 0 - valLoss: 0.6657352447509766 - trainLoss: 0.6641072630882263\n",
      "cnt: 0 - valLoss: 0.6657350659370422 - trainLoss: 0.664107084274292\n",
      "cnt: 0 - valLoss: 0.6657348275184631 - trainLoss: 0.6641068458557129\n",
      "cnt: 0 - valLoss: 0.665734589099884 - trainLoss: 0.6641066074371338\n",
      "cnt: 0 - valLoss: 0.6657343506813049 - trainLoss: 0.6641064286231995\n",
      "cnt: 0 - valLoss: 0.6657341122627258 - trainLoss: 0.6641061902046204\n",
      "cnt: 0 - valLoss: 0.6657338738441467 - trainLoss: 0.664106011390686\n",
      "cnt: 0 - valLoss: 0.6657336354255676 - trainLoss: 0.6641058325767517\n",
      "cnt: 0 - valLoss: 0.6657333970069885 - trainLoss: 0.6641055345535278\n",
      "cnt: 0 - valLoss: 0.6657330989837646 - trainLoss: 0.6641054153442383\n",
      "cnt: 0 - valLoss: 0.6657329201698303 - trainLoss: 0.6641051769256592\n",
      "cnt: 0 - valLoss: 0.6657326221466064 - trainLoss: 0.6641049385070801\n",
      "cnt: 0 - valLoss: 0.6657323837280273 - trainLoss: 0.6641047596931458\n",
      "cnt: 0 - valLoss: 0.6657321453094482 - trainLoss: 0.6641045212745667\n",
      "cnt: 0 - valLoss: 0.6657319068908691 - trainLoss: 0.6641043424606323\n",
      "cnt: 0 - valLoss: 0.6657317280769348 - trainLoss: 0.6641041040420532\n",
      "cnt: 0 - valLoss: 0.6657314300537109 - trainLoss: 0.6641038656234741\n",
      "cnt: 0 - valLoss: 0.6657312512397766 - trainLoss: 0.6641037464141846\n",
      "cnt: 0 - valLoss: 0.6657309532165527 - trainLoss: 0.6641035079956055\n",
      "cnt: 0 - valLoss: 0.6657307147979736 - trainLoss: 0.6641032695770264\n",
      "cnt: 0 - valLoss: 0.6657305359840393 - trainLoss: 0.664103090763092\n",
      "cnt: 0 - valLoss: 0.6657302975654602 - trainLoss: 0.6641027927398682\n",
      "cnt: 0 - valLoss: 0.6657300591468811 - trainLoss: 0.6641026735305786\n",
      "cnt: 0 - valLoss: 0.665729820728302 - trainLoss: 0.6641024351119995\n",
      "cnt: 0 - valLoss: 0.6657295823097229 - trainLoss: 0.6641021966934204\n",
      "cnt: 0 - valLoss: 0.6657293438911438 - trainLoss: 0.6641020178794861\n",
      "cnt: 0 - valLoss: 0.6657291054725647 - trainLoss: 0.6641017198562622\n",
      "cnt: 0 - valLoss: 0.6657288670539856 - trainLoss: 0.6641016006469727\n",
      "cnt: 0 - valLoss: 0.6657285690307617 - trainLoss: 0.6641013622283936\n",
      "cnt: 0 - valLoss: 0.6657284498214722 - trainLoss: 0.6641011238098145\n",
      "cnt: 0 - valLoss: 0.6657282114028931 - trainLoss: 0.6641010046005249\n",
      "cnt: 0 - valLoss: 0.6657279133796692 - trainLoss: 0.6641007661819458\n",
      "cnt: 0 - valLoss: 0.6657276749610901 - trainLoss: 0.6641005277633667\n",
      "cnt: 0 - valLoss: 0.665727436542511 - trainLoss: 0.6641003489494324\n",
      "cnt: 0 - valLoss: 0.6657272577285767 - trainLoss: 0.6641000509262085\n",
      "cnt: 0 - valLoss: 0.6657270193099976 - trainLoss: 0.664099931716919\n",
      "cnt: 0 - valLoss: 0.6657267808914185 - trainLoss: 0.6640996932983398\n",
      "cnt: 0 - valLoss: 0.6657265424728394 - trainLoss: 0.6640994548797607\n",
      "cnt: 0 - valLoss: 0.6657263040542603 - trainLoss: 0.6640993356704712\n",
      "cnt: 0 - valLoss: 0.6657260656356812 - trainLoss: 0.6640990376472473\n",
      "cnt: 0 - valLoss: 0.6657258868217468 - trainLoss: 0.664098858833313\n",
      "cnt: 0 - valLoss: 0.6657256484031677 - trainLoss: 0.6640986204147339\n",
      "cnt: 0 - valLoss: 0.6657253503799438 - trainLoss: 0.6640984416007996\n",
      "cnt: 0 - valLoss: 0.6657251119613647 - trainLoss: 0.6640982031822205\n",
      "cnt: 0 - valLoss: 0.6657249331474304 - trainLoss: 0.6640979647636414\n",
      "cnt: 0 - valLoss: 0.6657246351242065 - trainLoss: 0.664097785949707\n",
      "cnt: 0 - valLoss: 0.6657243967056274 - trainLoss: 0.6640975475311279\n",
      "cnt: 0 - valLoss: 0.6657242178916931 - trainLoss: 0.6640973687171936\n",
      "cnt: 0 - valLoss: 0.6657239198684692 - trainLoss: 0.6640971899032593\n",
      "cnt: 0 - valLoss: 0.6657236814498901 - trainLoss: 0.6640968918800354\n",
      "cnt: 0 - valLoss: 0.665723443031311 - trainLoss: 0.6640967130661011\n",
      "cnt: 0 - valLoss: 0.6657232046127319 - trainLoss: 0.6640965342521667\n",
      "cnt: 0 - valLoss: 0.6657229661941528 - trainLoss: 0.6640962958335876\n",
      "cnt: 0 - valLoss: 0.6657227277755737 - trainLoss: 0.6640961170196533\n",
      "cnt: 0 - valLoss: 0.6657225489616394 - trainLoss: 0.6640958786010742\n",
      "cnt: 0 - valLoss: 0.6657222509384155 - trainLoss: 0.6640956401824951\n",
      "cnt: 0 - valLoss: 0.6657220125198364 - trainLoss: 0.6640954613685608\n",
      "cnt: 0 - valLoss: 0.6657218337059021 - trainLoss: 0.6640952229499817\n",
      "cnt: 0 - valLoss: 0.665721595287323 - trainLoss: 0.6640950441360474\n",
      "cnt: 0 - valLoss: 0.6657212972640991 - trainLoss: 0.664094865322113\n",
      "cnt: 0 - valLoss: 0.66572105884552 - trainLoss: 0.6640946269035339\n",
      "cnt: 0 - valLoss: 0.6657207608222961 - trainLoss: 0.6640944480895996\n",
      "cnt: 0 - valLoss: 0.6657205820083618 - trainLoss: 0.6640942096710205\n",
      "cnt: 0 - valLoss: 0.6657203435897827 - trainLoss: 0.6640940308570862\n",
      "cnt: 0 - valLoss: 0.6657201051712036 - trainLoss: 0.6640938520431519\n",
      "cnt: 0 - valLoss: 0.6657198071479797 - trainLoss: 0.6640936136245728\n",
      "cnt: 0 - valLoss: 0.6657196283340454 - trainLoss: 0.6640934348106384\n",
      "cnt: 0 - valLoss: 0.6657193899154663 - trainLoss: 0.6640932559967041\n",
      "cnt: 0 - valLoss: 0.6657190918922424 - trainLoss: 0.6640930771827698\n",
      "cnt: 0 - valLoss: 0.6657189130783081 - trainLoss: 0.6640928387641907\n",
      "cnt: 0 - valLoss: 0.6657186150550842 - trainLoss: 0.6640926599502563\n",
      "cnt: 0 - valLoss: 0.6657183766365051 - trainLoss: 0.664092481136322\n",
      "cnt: 0 - valLoss: 0.665718138217926 - trainLoss: 0.6640923023223877\n",
      "cnt: 0 - valLoss: 0.6657178401947021 - trainLoss: 0.6640920042991638\n",
      "cnt: 0 - valLoss: 0.6657176613807678 - trainLoss: 0.6640918850898743\n",
      "cnt: 0 - valLoss: 0.665717363357544 - trainLoss: 0.6640917062759399\n",
      "cnt: 0 - valLoss: 0.6657171249389648 - trainLoss: 0.6640914678573608\n",
      "cnt: 0 - valLoss: 0.6657168865203857 - trainLoss: 0.664091169834137\n",
      "cnt: 0 - valLoss: 0.6657166481018066 - trainLoss: 0.6640910506248474\n",
      "cnt: 0 - valLoss: 0.6657164096832275 - trainLoss: 0.6640908122062683\n",
      "cnt: 0 - valLoss: 0.6657161712646484 - trainLoss: 0.6640906929969788\n",
      "cnt: 0 - valLoss: 0.6657159328460693 - trainLoss: 0.6640904545783997\n",
      "cnt: 0 - valLoss: 0.6657156944274902 - trainLoss: 0.6640902161598206\n",
      "cnt: 0 - valLoss: 0.6657153964042664 - trainLoss: 0.6640900373458862\n",
      "cnt: 0 - valLoss: 0.665715217590332 - trainLoss: 0.6640897989273071\n",
      "cnt: 0 - valLoss: 0.6657149791717529 - trainLoss: 0.6640896201133728\n",
      "cnt: 0 - valLoss: 0.665714681148529 - trainLoss: 0.6640894412994385\n",
      "cnt: 0 - valLoss: 0.6657145023345947 - trainLoss: 0.6640892624855042\n",
      "cnt: 0 - valLoss: 0.6657142043113708 - trainLoss: 0.664089024066925\n",
      "cnt: 0 - valLoss: 0.6657140254974365 - trainLoss: 0.6640888452529907\n",
      "cnt: 0 - valLoss: 0.6657137274742126 - trainLoss: 0.6640886068344116\n",
      "cnt: 0 - valLoss: 0.6657135486602783 - trainLoss: 0.6640884280204773\n",
      "cnt: 0 - valLoss: 0.6657132506370544 - trainLoss: 0.664088249206543\n",
      "cnt: 0 - valLoss: 0.6657130122184753 - trainLoss: 0.6640879511833191\n",
      "cnt: 0 - valLoss: 0.6657127737998962 - trainLoss: 0.6640877723693848\n",
      "cnt: 0 - valLoss: 0.6657125353813171 - trainLoss: 0.6640875935554504\n",
      "cnt: 0 - valLoss: 0.6657123565673828 - trainLoss: 0.6640873551368713\n",
      "cnt: 0 - valLoss: 0.6657120585441589 - trainLoss: 0.664087176322937\n",
      "cnt: 0 - valLoss: 0.6657117605209351 - trainLoss: 0.6640869975090027\n",
      "cnt: 0 - valLoss: 0.6657115817070007 - trainLoss: 0.6640867590904236\n",
      "cnt: 0 - valLoss: 0.6657112836837769 - trainLoss: 0.6640865802764893\n",
      "cnt: 0 - valLoss: 0.6657110452651978 - trainLoss: 0.6640864014625549\n",
      "cnt: 0 - valLoss: 0.6657108068466187 - trainLoss: 0.6640861630439758\n",
      "cnt: 0 - valLoss: 0.6657105684280396 - trainLoss: 0.6640859842300415\n",
      "cnt: 0 - valLoss: 0.6657103896141052 - trainLoss: 0.6640858054161072\n",
      "cnt: 0 - valLoss: 0.6657100915908813 - trainLoss: 0.6640855669975281\n",
      "cnt: 0 - valLoss: 0.6657098531723022 - trainLoss: 0.664085328578949\n",
      "cnt: 0 - valLoss: 0.6657096147537231 - trainLoss: 0.6640851497650146\n",
      "cnt: 0 - valLoss: 0.665709376335144 - trainLoss: 0.6640849709510803\n",
      "cnt: 0 - valLoss: 0.6657090783119202 - trainLoss: 0.6640847325325012\n",
      "cnt: 0 - valLoss: 0.6657088994979858 - trainLoss: 0.6640844941139221\n",
      "cnt: 0 - valLoss: 0.6657086610794067 - trainLoss: 0.6640843152999878\n",
      "cnt: 0 - valLoss: 0.6657083630561829 - trainLoss: 0.6640841364860535\n",
      "cnt: 0 - valLoss: 0.6657081246376038 - trainLoss: 0.6640838980674744\n",
      "cnt: 0 - valLoss: 0.6657079458236694 - trainLoss: 0.66408371925354\n",
      "cnt: 0 - valLoss: 0.6657076478004456 - trainLoss: 0.6640835404396057\n",
      "cnt: 0 - valLoss: 0.6657074689865112 - trainLoss: 0.6640833616256714\n",
      "cnt: 0 - valLoss: 0.6657071709632874 - trainLoss: 0.6640831232070923\n",
      "cnt: 0 - valLoss: 0.6657069325447083 - trainLoss: 0.6640828847885132\n",
      "cnt: 0 - valLoss: 0.6657066941261292 - trainLoss: 0.6640827059745789\n",
      "cnt: 0 - valLoss: 0.6657063961029053 - trainLoss: 0.6640824675559998\n",
      "cnt: 0 - valLoss: 0.6657061576843262 - trainLoss: 0.6640822887420654\n",
      "cnt: 0 - valLoss: 0.6657059192657471 - trainLoss: 0.6640821099281311\n",
      "cnt: 0 - valLoss: 0.6657057404518127 - trainLoss: 0.664081871509552\n",
      "cnt: 0 - valLoss: 0.6657055020332336 - trainLoss: 0.6640816926956177\n",
      "cnt: 0 - valLoss: 0.6657052040100098 - trainLoss: 0.6640815138816833\n",
      "cnt: 0 - valLoss: 0.6657049655914307 - trainLoss: 0.6640812754631042\n",
      "cnt: 0 - valLoss: 0.6657047271728516 - trainLoss: 0.6640810966491699\n",
      "cnt: 0 - valLoss: 0.6657044291496277 - trainLoss: 0.664080798625946\n",
      "cnt: 0 - valLoss: 0.6657042503356934 - trainLoss: 0.6640806198120117\n",
      "cnt: 0 - valLoss: 0.6657040119171143 - trainLoss: 0.6640804409980774\n",
      "cnt: 0 - valLoss: 0.6657037138938904 - trainLoss: 0.6640802621841431\n",
      "cnt: 0 - valLoss: 0.665703535079956 - trainLoss: 0.664080023765564\n",
      "cnt: 0 - valLoss: 0.6657032370567322 - trainLoss: 0.6640797853469849\n",
      "cnt: 0 - valLoss: 0.6657029986381531 - trainLoss: 0.6640796065330505\n",
      "cnt: 0 - valLoss: 0.665702760219574 - trainLoss: 0.6640794277191162\n",
      "cnt: 0 - valLoss: 0.6657025218009949 - trainLoss: 0.6640791893005371\n",
      "cnt: 0 - valLoss: 0.6657022833824158 - trainLoss: 0.6640790104866028\n",
      "cnt: 0 - valLoss: 0.6657020449638367 - trainLoss: 0.6640788316726685\n",
      "cnt: 0 - valLoss: 0.6657018065452576 - trainLoss: 0.6640785932540894\n",
      "cnt: 0 - valLoss: 0.6657015085220337 - trainLoss: 0.664078414440155\n",
      "cnt: 0 - valLoss: 0.6657012701034546 - trainLoss: 0.6640781760215759\n",
      "cnt: 0 - valLoss: 0.6657010912895203 - trainLoss: 0.6640779972076416\n",
      "cnt: 0 - valLoss: 0.6657007336616516 - trainLoss: 0.6640777587890625\n",
      "cnt: 0 - valLoss: 0.6657006144523621 - trainLoss: 0.6640775799751282\n",
      "cnt: 0 - valLoss: 0.6657003164291382 - trainLoss: 0.6640773415565491\n",
      "cnt: 0 - valLoss: 0.6657001376152039 - trainLoss: 0.6640771627426147\n",
      "cnt: 0 - valLoss: 0.66569983959198 - trainLoss: 0.6640769839286804\n",
      "cnt: 0 - valLoss: 0.6656995415687561 - trainLoss: 0.6640767455101013\n",
      "cnt: 0 - valLoss: 0.6656993627548218 - trainLoss: 0.664076566696167\n",
      "cnt: 0 - valLoss: 0.6656990647315979 - trainLoss: 0.6640762686729431\n",
      "cnt: 0 - valLoss: 0.6656988263130188 - trainLoss: 0.6640761494636536\n",
      "cnt: 0 - valLoss: 0.6656986474990845 - trainLoss: 0.6640759110450745\n",
      "cnt: 0 - valLoss: 0.6656983494758606 - trainLoss: 0.6640757322311401\n",
      "cnt: 0 - valLoss: 0.6656981110572815 - trainLoss: 0.664075493812561\n",
      "cnt: 0 - valLoss: 0.6656978130340576 - trainLoss: 0.6640753149986267\n",
      "cnt: 0 - valLoss: 0.6656976342201233 - trainLoss: 0.6640750765800476\n",
      "cnt: 0 - valLoss: 0.6656973361968994 - trainLoss: 0.6640748977661133\n",
      "cnt: 0 - valLoss: 0.6656970977783203 - trainLoss: 0.664074718952179\n",
      "cnt: 0 - valLoss: 0.6656968593597412 - trainLoss: 0.6640744805335999\n",
      "cnt: 0 - valLoss: 0.6656966209411621 - trainLoss: 0.6640743017196655\n",
      "cnt: 0 - valLoss: 0.665696382522583 - trainLoss: 0.6640740633010864\n",
      "cnt: 0 - valLoss: 0.6656961441040039 - trainLoss: 0.6640738248825073\n",
      "cnt: 0 - valLoss: 0.6656959056854248 - trainLoss: 0.664073646068573\n",
      "cnt: 0 - valLoss: 0.6656956076622009 - trainLoss: 0.6640734672546387\n",
      "cnt: 0 - valLoss: 0.6656954288482666 - trainLoss: 0.6640732288360596\n",
      "cnt: 0 - valLoss: 0.6656951308250427 - trainLoss: 0.6640730500221252\n",
      "cnt: 0 - valLoss: 0.6656948328018188 - trainLoss: 0.6640728116035461\n",
      "cnt: 0 - valLoss: 0.6656946539878845 - trainLoss: 0.664072573184967\n",
      "cnt: 0 - valLoss: 0.6656943559646606 - trainLoss: 0.6640724539756775\n",
      "cnt: 0 - valLoss: 0.6656941771507263 - trainLoss: 0.6640722751617432\n",
      "cnt: 0 - valLoss: 0.6656938791275024 - trainLoss: 0.6640719771385193\n",
      "cnt: 0 - valLoss: 0.6656935811042786 - trainLoss: 0.6640718579292297\n",
      "cnt: 0 - valLoss: 0.665693461894989 - trainLoss: 0.6640715599060059\n",
      "cnt: 0 - valLoss: 0.6656931042671204 - trainLoss: 0.6640713810920715\n",
      "cnt: 0 - valLoss: 0.665692925453186 - trainLoss: 0.6640711426734924\n",
      "cnt: 0 - valLoss: 0.6656926870346069 - trainLoss: 0.6640709638595581\n",
      "cnt: 0 - valLoss: 0.6656924486160278 - trainLoss: 0.664070725440979\n",
      "cnt: 0 - valLoss: 0.6656922101974487 - trainLoss: 0.6640705466270447\n",
      "cnt: 0 - valLoss: 0.6656919121742249 - trainLoss: 0.6640703082084656\n",
      "cnt: 0 - valLoss: 0.6656916737556458 - trainLoss: 0.6640701293945312\n",
      "cnt: 0 - valLoss: 0.6656914353370667 - trainLoss: 0.6640699505805969\n",
      "cnt: 0 - valLoss: 0.6656911373138428 - trainLoss: 0.6640697121620178\n",
      "cnt: 0 - valLoss: 0.6656910181045532 - trainLoss: 0.6640695333480835\n",
      "cnt: 0 - valLoss: 0.6656906604766846 - trainLoss: 0.6640692353248596\n",
      "cnt: 0 - valLoss: 0.6656904220581055 - trainLoss: 0.6640691161155701\n",
      "cnt: 0 - valLoss: 0.6656902432441711 - trainLoss: 0.6640689373016357\n",
      "cnt: 0 - valLoss: 0.6656899452209473 - trainLoss: 0.6640686988830566\n",
      "cnt: 0 - valLoss: 0.6656897068023682 - trainLoss: 0.6640685200691223\n",
      "cnt: 0 - valLoss: 0.6656894683837891 - trainLoss: 0.6640682816505432\n",
      "cnt: 0 - valLoss: 0.66568922996521 - trainLoss: 0.6640681028366089\n",
      "cnt: 0 - valLoss: 0.6656889915466309 - trainLoss: 0.6640678644180298\n",
      "cnt: 0 - valLoss: 0.665688693523407 - trainLoss: 0.6640676856040955\n",
      "cnt: 0 - valLoss: 0.6656884551048279 - trainLoss: 0.6640674471855164\n",
      "cnt: 0 - valLoss: 0.6656882762908936 - trainLoss: 0.664067268371582\n",
      "cnt: 0 - valLoss: 0.6656879186630249 - trainLoss: 0.6640669703483582\n",
      "cnt: 0 - valLoss: 0.6656877398490906 - trainLoss: 0.6640668511390686\n",
      "cnt: 0 - valLoss: 0.6656875014305115 - trainLoss: 0.6640666127204895\n",
      "cnt: 0 - valLoss: 0.6656872034072876 - trainLoss: 0.6640663743019104\n",
      "cnt: 0 - valLoss: 0.6656869649887085 - trainLoss: 0.6640662550926208\n",
      "cnt: 0 - valLoss: 0.6656867265701294 - trainLoss: 0.6640660166740417\n",
      "cnt: 0 - valLoss: 0.6656864881515503 - trainLoss: 0.6640657782554626\n",
      "cnt: 0 - valLoss: 0.6656861901283264 - trainLoss: 0.6640655994415283\n",
      "cnt: 0 - valLoss: 0.6656859517097473 - trainLoss: 0.6640653014183044\n",
      "cnt: 0 - valLoss: 0.665685772895813 - trainLoss: 0.6640651822090149\n",
      "cnt: 0 - valLoss: 0.6656854748725891 - trainLoss: 0.6640650033950806\n",
      "cnt: 0 - valLoss: 0.66568523645401 - trainLoss: 0.6640647053718567\n",
      "cnt: 0 - valLoss: 0.6656849384307861 - trainLoss: 0.6640645265579224\n",
      "cnt: 0 - valLoss: 0.6656847596168518 - trainLoss: 0.6640642881393433\n",
      "cnt: 0 - valLoss: 0.6656844615936279 - trainLoss: 0.6640641093254089\n",
      "cnt: 0 - valLoss: 0.6656842231750488 - trainLoss: 0.6640639305114746\n",
      "cnt: 0 - valLoss: 0.6656839847564697 - trainLoss: 0.6640636920928955\n",
      "cnt: 0 - valLoss: 0.6656836867332458 - trainLoss: 0.6640634536743164\n",
      "cnt: 0 - valLoss: 0.6656835079193115 - trainLoss: 0.6640632748603821\n",
      "cnt: 0 - valLoss: 0.6656832695007324 - trainLoss: 0.664063036441803\n",
      "cnt: 0 - valLoss: 0.6656829714775085 - trainLoss: 0.6640628576278687\n",
      "cnt: 0 - valLoss: 0.6656827330589294 - trainLoss: 0.6640625596046448\n",
      "cnt: 0 - valLoss: 0.6656824946403503 - trainLoss: 0.6640624403953552\n",
      "cnt: 0 - valLoss: 0.6656822562217712 - trainLoss: 0.6640622615814209\n",
      "cnt: 0 - valLoss: 0.6656820178031921 - trainLoss: 0.6640620231628418\n",
      "cnt: 0 - valLoss: 0.6656817197799683 - trainLoss: 0.6640618443489075\n",
      "cnt: 0 - valLoss: 0.6656814813613892 - trainLoss: 0.6640616059303284\n",
      "cnt: 0 - valLoss: 0.6656812429428101 - trainLoss: 0.664061427116394\n",
      "cnt: 0 - valLoss: 0.6656809449195862 - trainLoss: 0.6640611886978149\n",
      "cnt: 0 - valLoss: 0.6656807661056519 - trainLoss: 0.6640609502792358\n",
      "cnt: 0 - valLoss: 0.665680468082428 - trainLoss: 0.6640607714653015\n",
      "cnt: 0 - valLoss: 0.6656802296638489 - trainLoss: 0.6640605926513672\n",
      "cnt: 0 - valLoss: 0.6656799912452698 - trainLoss: 0.6640603542327881\n",
      "cnt: 0 - valLoss: 0.6656796932220459 - trainLoss: 0.6640601754188538\n",
      "cnt: 0 - valLoss: 0.6656795144081116 - trainLoss: 0.6640599370002747\n",
      "cnt: 0 - valLoss: 0.6656792163848877 - trainLoss: 0.6640596985816956\n",
      "cnt: 0 - valLoss: 0.6656789779663086 - trainLoss: 0.6640595197677612\n",
      "cnt: 0 - valLoss: 0.6656787395477295 - trainLoss: 0.6640592813491821\n",
      "cnt: 0 - valLoss: 0.6656785011291504 - trainLoss: 0.6640591025352478\n",
      "cnt: 0 - valLoss: 0.6656782031059265 - trainLoss: 0.6640589237213135\n",
      "cnt: 0 - valLoss: 0.6656779646873474 - trainLoss: 0.6640586853027344\n",
      "cnt: 0 - valLoss: 0.6656777262687683 - trainLoss: 0.6640585064888\n",
      "cnt: 0 - valLoss: 0.6656774878501892 - trainLoss: 0.6640583276748657\n",
      "cnt: 0 - valLoss: 0.6656772494316101 - trainLoss: 0.6640580296516418\n",
      "cnt: 0 - valLoss: 0.6656769514083862 - trainLoss: 0.6640577912330627\n",
      "cnt: 0 - valLoss: 0.6656767725944519 - trainLoss: 0.6640576720237732\n",
      "cnt: 0 - valLoss: 0.665676474571228 - trainLoss: 0.6640574336051941\n",
      "cnt: 0 - valLoss: 0.6656762361526489 - trainLoss: 0.664057195186615\n",
      "cnt: 0 - valLoss: 0.665675938129425 - trainLoss: 0.6640569567680359\n",
      "cnt: 0 - valLoss: 0.6656757593154907 - trainLoss: 0.6640567779541016\n",
      "cnt: 0 - valLoss: 0.6656754612922668 - trainLoss: 0.6640565395355225\n",
      "cnt: 0 - valLoss: 0.6656752228736877 - trainLoss: 0.6640563607215881\n",
      "cnt: 0 - valLoss: 0.6656749844551086 - trainLoss: 0.6640561819076538\n",
      "cnt: 0 - valLoss: 0.6656746864318848 - trainLoss: 0.6640558838844299\n",
      "cnt: 0 - valLoss: 0.6656744480133057 - trainLoss: 0.6640557646751404\n",
      "cnt: 0 - valLoss: 0.6656742095947266 - trainLoss: 0.6640554666519165\n",
      "cnt: 0 - valLoss: 0.6656739115715027 - trainLoss: 0.6640552878379822\n",
      "cnt: 0 - valLoss: 0.6656737327575684 - trainLoss: 0.6640550494194031\n",
      "cnt: 0 - valLoss: 0.6656734347343445 - trainLoss: 0.664054811000824\n",
      "cnt: 0 - valLoss: 0.6656732559204102 - trainLoss: 0.6640546917915344\n",
      "cnt: 0 - valLoss: 0.6656729578971863 - trainLoss: 0.6640543937683105\n",
      "cnt: 0 - valLoss: 0.6656726598739624 - trainLoss: 0.6640542149543762\n",
      "cnt: 0 - valLoss: 0.6656724214553833 - trainLoss: 0.6640539765357971\n",
      "cnt: 0 - valLoss: 0.6656721830368042 - trainLoss: 0.664053738117218\n",
      "cnt: 0 - valLoss: 0.6656719446182251 - trainLoss: 0.6640534996986389\n",
      "cnt: 0 - valLoss: 0.6656715869903564 - trainLoss: 0.6640533208847046\n",
      "cnt: 0 - valLoss: 0.6656714677810669 - trainLoss: 0.6640531420707703\n",
      "cnt: 0 - valLoss: 0.665671169757843 - trainLoss: 0.6640528440475464\n",
      "cnt: 0 - valLoss: 0.6656709313392639 - trainLoss: 0.6640526652336121\n",
      "cnt: 0 - valLoss: 0.6656706929206848 - trainLoss: 0.664052426815033\n",
      "cnt: 0 - valLoss: 0.6656703948974609 - trainLoss: 0.6640522480010986\n",
      "cnt: 0 - valLoss: 0.6656701564788818 - trainLoss: 0.6640520691871643\n",
      "cnt: 0 - valLoss: 0.6656699180603027 - trainLoss: 0.6640517711639404\n",
      "cnt: 0 - valLoss: 0.6656696200370789 - trainLoss: 0.6640515327453613\n",
      "cnt: 0 - valLoss: 0.6656693816184998 - trainLoss: 0.664051353931427\n",
      "cnt: 0 - valLoss: 0.6656691431999207 - trainLoss: 0.6640511751174927\n",
      "cnt: 0 - valLoss: 0.6656688451766968 - trainLoss: 0.6640508770942688\n",
      "cnt: 0 - valLoss: 0.6656686663627625 - trainLoss: 0.6640506982803345\n",
      "cnt: 0 - valLoss: 0.6656683683395386 - trainLoss: 0.6640504598617554\n",
      "cnt: 0 - valLoss: 0.6656681299209595 - trainLoss: 0.664050281047821\n",
      "cnt: 0 - valLoss: 0.6656678915023804 - trainLoss: 0.6640501022338867\n",
      "cnt: 0 - valLoss: 0.6656675934791565 - trainLoss: 0.6640498042106628\n",
      "cnt: 0 - valLoss: 0.6656673550605774 - trainLoss: 0.6640495657920837\n",
      "cnt: 0 - valLoss: 0.6656671166419983 - trainLoss: 0.6640493869781494\n",
      "cnt: 0 - valLoss: 0.6656668782234192 - trainLoss: 0.6640491485595703\n",
      "cnt: 0 - valLoss: 0.6656666398048401 - trainLoss: 0.6640489101409912\n",
      "cnt: 0 - valLoss: 0.665666401386261 - trainLoss: 0.6640487313270569\n",
      "cnt: 0 - valLoss: 0.6656661629676819 - trainLoss: 0.664048433303833\n",
      "cnt: 0 - valLoss: 0.6656659841537476 - trainLoss: 0.6640482544898987\n",
      "cnt: 0 - valLoss: 0.6656656265258789 - trainLoss: 0.6640480756759644\n",
      "cnt: 0 - valLoss: 0.6656653881072998 - trainLoss: 0.6640477776527405\n",
      "cnt: 0 - valLoss: 0.6656652092933655 - trainLoss: 0.6640475392341614\n",
      "cnt: 0 - valLoss: 0.6656649112701416 - trainLoss: 0.664047360420227\n",
      "cnt: 0 - valLoss: 0.6656646132469177 - trainLoss: 0.664047122001648\n",
      "cnt: 0 - valLoss: 0.6656644344329834 - trainLoss: 0.6640469431877136\n",
      "cnt: 0 - valLoss: 0.6656641364097595 - trainLoss: 0.6640466451644897\n",
      "cnt: 0 - valLoss: 0.6656639575958252 - trainLoss: 0.6640464663505554\n",
      "cnt: 0 - valLoss: 0.6656636595726013 - trainLoss: 0.6640461683273315\n",
      "cnt: 0 - valLoss: 0.665663480758667 - trainLoss: 0.6640459895133972\n",
      "cnt: 0 - valLoss: 0.6656632423400879 - trainLoss: 0.6640457510948181\n",
      "cnt: 0 - valLoss: 0.665662944316864 - trainLoss: 0.6640455722808838\n",
      "cnt: 0 - valLoss: 0.6656627058982849 - trainLoss: 0.6640452742576599\n",
      "cnt: 0 - valLoss: 0.6656624674797058 - trainLoss: 0.6640450954437256\n",
      "cnt: 0 - valLoss: 0.6656621694564819 - trainLoss: 0.6640447974205017\n",
      "cnt: 0 - valLoss: 0.6656619310379028 - trainLoss: 0.6640446186065674\n",
      "cnt: 0 - valLoss: 0.6656616926193237 - trainLoss: 0.6640443801879883\n",
      "cnt: 0 - valLoss: 0.6656614542007446 - trainLoss: 0.6640441417694092\n",
      "cnt: 0 - valLoss: 0.6656612157821655 - trainLoss: 0.6640439629554749\n",
      "cnt: 0 - valLoss: 0.6656609177589417 - trainLoss: 0.6640437245368958\n",
      "cnt: 0 - valLoss: 0.6656606793403625 - trainLoss: 0.6640434265136719\n",
      "cnt: 0 - valLoss: 0.6656605005264282 - trainLoss: 0.6640432476997375\n",
      "cnt: 0 - valLoss: 0.6656601428985596 - trainLoss: 0.6640430092811584\n",
      "cnt: 0 - valLoss: 0.66566002368927 - trainLoss: 0.6640428304672241\n",
      "cnt: 0 - valLoss: 0.6656597256660461 - trainLoss: 0.6640425324440002\n",
      "cnt: 0 - valLoss: 0.665659487247467 - trainLoss: 0.6640423536300659\n",
      "cnt: 0 - valLoss: 0.6656592488288879 - trainLoss: 0.6640421152114868\n",
      "cnt: 0 - valLoss: 0.6656589508056641 - trainLoss: 0.6640418767929077\n",
      "cnt: 0 - valLoss: 0.6656586527824402 - trainLoss: 0.6640416979789734\n",
      "cnt: 0 - valLoss: 0.6656584143638611 - trainLoss: 0.6640414595603943\n",
      "cnt: 0 - valLoss: 0.665658175945282 - trainLoss: 0.6640412211418152\n",
      "cnt: 0 - valLoss: 0.6656578779220581 - trainLoss: 0.6640409827232361\n",
      "cnt: 0 - valLoss: 0.6656577587127686 - trainLoss: 0.6640408039093018\n",
      "cnt: 0 - valLoss: 0.6656574606895447 - trainLoss: 0.6640405654907227\n",
      "cnt: 0 - valLoss: 0.665657103061676 - trainLoss: 0.6640403270721436\n",
      "cnt: 0 - valLoss: 0.6656568646430969 - trainLoss: 0.6640400886535645\n",
      "cnt: 0 - valLoss: 0.6656566262245178 - trainLoss: 0.6640399098396301\n",
      "cnt: 0 - valLoss: 0.665656328201294 - trainLoss: 0.664039671421051\n",
      "cnt: 0 - valLoss: 0.6656560897827148 - trainLoss: 0.6640394330024719\n",
      "cnt: 0 - valLoss: 0.6656558513641357 - trainLoss: 0.6640391945838928\n",
      "cnt: 0 - valLoss: 0.6656556129455566 - trainLoss: 0.6640390157699585\n",
      "cnt: 0 - valLoss: 0.6656554341316223 - trainLoss: 0.6640387773513794\n",
      "cnt: 0 - valLoss: 0.6656550765037537 - trainLoss: 0.6640385389328003\n",
      "cnt: 0 - valLoss: 0.6656548976898193 - trainLoss: 0.6640383005142212\n",
      "cnt: 0 - valLoss: 0.6656546592712402 - trainLoss: 0.6640381217002869\n",
      "cnt: 0 - valLoss: 0.6656543016433716 - trainLoss: 0.6640378832817078\n",
      "cnt: 0 - valLoss: 0.6656541228294373 - trainLoss: 0.6640376448631287\n",
      "cnt: 0 - valLoss: 0.6656539440155029 - trainLoss: 0.6640374660491943\n",
      "cnt: 0 - valLoss: 0.6656535863876343 - trainLoss: 0.6640372276306152\n",
      "cnt: 0 - valLoss: 0.6656533479690552 - trainLoss: 0.6640369892120361\n",
      "cnt: 0 - valLoss: 0.6656530499458313 - trainLoss: 0.664036750793457\n",
      "cnt: 0 - valLoss: 0.665652871131897 - trainLoss: 0.6640365123748779\n",
      "cnt: 0 - valLoss: 0.6656525135040283 - trainLoss: 0.6640363335609436\n",
      "cnt: 0 - valLoss: 0.6656522750854492 - trainLoss: 0.6640360951423645\n",
      "cnt: 0 - valLoss: 0.6656520962715149 - trainLoss: 0.6640358567237854\n",
      "cnt: 0 - valLoss: 0.665651798248291 - trainLoss: 0.6640356779098511\n",
      "cnt: 0 - valLoss: 0.6656515598297119 - trainLoss: 0.6640353798866272\n",
      "cnt: 0 - valLoss: 0.6656513214111328 - trainLoss: 0.6640352010726929\n",
      "cnt: 0 - valLoss: 0.6656510233879089 - trainLoss: 0.6640349626541138\n",
      "cnt: 0 - valLoss: 0.6656507849693298 - trainLoss: 0.6640347838401794\n",
      "cnt: 0 - valLoss: 0.665650486946106 - trainLoss: 0.6640345454216003\n",
      "cnt: 0 - valLoss: 0.6656502485275269 - trainLoss: 0.6640343070030212\n",
      "cnt: 0 - valLoss: 0.6656500101089478 - trainLoss: 0.6640340685844421\n",
      "cnt: 0 - valLoss: 0.6656497716903687 - trainLoss: 0.6640338897705078\n",
      "cnt: 0 - valLoss: 0.6656494736671448 - trainLoss: 0.6640336513519287\n",
      "cnt: 0 - valLoss: 0.6656492352485657 - trainLoss: 0.6640334129333496\n",
      "cnt: 0 - valLoss: 0.6656489968299866 - trainLoss: 0.6640332341194153\n",
      "cnt: 0 - valLoss: 0.6656486988067627 - trainLoss: 0.6640329957008362\n",
      "cnt: 0 - valLoss: 0.6656484007835388 - trainLoss: 0.6640326976776123\n",
      "cnt: 0 - valLoss: 0.6656481623649597 - trainLoss: 0.664032518863678\n",
      "cnt: 0 - valLoss: 0.6656479239463806 - trainLoss: 0.6640322804450989\n",
      "cnt: 0 - valLoss: 0.6656476855278015 - trainLoss: 0.6640320420265198\n",
      "cnt: 0 - valLoss: 0.6656473875045776 - trainLoss: 0.6640318632125854\n",
      "cnt: 0 - valLoss: 0.6656471490859985 - trainLoss: 0.6640316247940063\n",
      "cnt: 0 - valLoss: 0.6656469106674194 - trainLoss: 0.6640313267707825\n",
      "cnt: 0 - valLoss: 0.6656466126441956 - trainLoss: 0.6640312075614929\n",
      "cnt: 0 - valLoss: 0.6656464338302612 - trainLoss: 0.664030909538269\n",
      "cnt: 0 - valLoss: 0.6656461358070374 - trainLoss: 0.6640307307243347\n",
      "cnt: 0 - valLoss: 0.6656458973884583 - trainLoss: 0.6640304923057556\n",
      "cnt: 0 - valLoss: 0.6656456589698792 - trainLoss: 0.6640302538871765\n",
      "cnt: 0 - valLoss: 0.6656453609466553 - trainLoss: 0.6640300750732422\n",
      "cnt: 0 - valLoss: 0.6656451225280762 - trainLoss: 0.6640297770500183\n",
      "cnt: 0 - valLoss: 0.6656448841094971 - trainLoss: 0.664029598236084\n",
      "cnt: 0 - valLoss: 0.6656445860862732 - trainLoss: 0.6640293002128601\n",
      "cnt: 0 - valLoss: 0.6656443476676941 - trainLoss: 0.6640291810035706\n",
      "cnt: 0 - valLoss: 0.665644109249115 - trainLoss: 0.6640288829803467\n",
      "cnt: 0 - valLoss: 0.6656438708305359 - trainLoss: 0.6640287041664124\n",
      "cnt: 0 - valLoss: 0.665643572807312 - trainLoss: 0.6640284061431885\n",
      "cnt: 0 - valLoss: 0.6656433343887329 - trainLoss: 0.6640281677246094\n",
      "cnt: 0 - valLoss: 0.6656430959701538 - trainLoss: 0.6640279293060303\n",
      "cnt: 0 - valLoss: 0.6656428575515747 - trainLoss: 0.6640276312828064\n",
      "cnt: 0 - valLoss: 0.6656426191329956 - trainLoss: 0.6640273928642273\n",
      "cnt: 0 - valLoss: 0.6656423807144165 - trainLoss: 0.664027214050293\n",
      "cnt: 0 - valLoss: 0.6656420826911926 - trainLoss: 0.6640269160270691\n",
      "cnt: 0 - valLoss: 0.6656419634819031 - trainLoss: 0.66402667760849\n",
      "cnt: 0 - valLoss: 0.6656416654586792 - trainLoss: 0.6640264391899109\n",
      "cnt: 0 - valLoss: 0.6656414270401001 - trainLoss: 0.6640262007713318\n",
      "cnt: 0 - valLoss: 0.6656411290168762 - trainLoss: 0.6640259623527527\n",
      "cnt: 0 - valLoss: 0.6656408905982971 - trainLoss: 0.6640256643295288\n",
      "cnt: 0 - valLoss: 0.665640652179718 - trainLoss: 0.6640254259109497\n",
      "cnt: 0 - valLoss: 0.6656404137611389 - trainLoss: 0.6640251874923706\n",
      "cnt: 0 - valLoss: 0.6656401753425598 - trainLoss: 0.6640249490737915\n",
      "cnt: 0 - valLoss: 0.6656398773193359 - trainLoss: 0.6640246510505676\n",
      "cnt: 0 - valLoss: 0.6656396985054016 - trainLoss: 0.6640244722366333\n",
      "cnt: 0 - valLoss: 0.6656394600868225 - trainLoss: 0.6640241742134094\n",
      "cnt: 0 - valLoss: 0.6656391620635986 - trainLoss: 0.6640239357948303\n",
      "cnt: 0 - valLoss: 0.6656389236450195 - trainLoss: 0.664023756980896\n",
      "cnt: 0 - valLoss: 0.6656386852264404 - trainLoss: 0.6640234589576721\n",
      "cnt: 0 - valLoss: 0.6656384468078613 - trainLoss: 0.6640232801437378\n",
      "cnt: 0 - valLoss: 0.6656382083892822 - trainLoss: 0.6640229225158691\n",
      "cnt: 0 - valLoss: 0.6656379103660583 - trainLoss: 0.6640228033065796\n",
      "cnt: 0 - valLoss: 0.6656376719474792 - trainLoss: 0.6640225052833557\n",
      "cnt: 0 - valLoss: 0.6656374335289001 - trainLoss: 0.6640222668647766\n",
      "cnt: 0 - valLoss: 0.665637195110321 - trainLoss: 0.6640220284461975\n",
      "cnt: 0 - valLoss: 0.6656369566917419 - trainLoss: 0.6640217304229736\n",
      "cnt: 0 - valLoss: 0.6656367182731628 - trainLoss: 0.6640214920043945\n",
      "cnt: 0 - valLoss: 0.6656364798545837 - trainLoss: 0.6640212535858154\n",
      "cnt: 0 - valLoss: 0.6656361818313599 - trainLoss: 0.6640210747718811\n",
      "cnt: 0 - valLoss: 0.6656359434127808 - trainLoss: 0.6640207767486572\n",
      "cnt: 0 - valLoss: 0.6656357645988464 - trainLoss: 0.6640205383300781\n",
      "cnt: 0 - valLoss: 0.6656354665756226 - trainLoss: 0.6640202403068542\n",
      "cnt: 0 - valLoss: 0.6656352877616882 - trainLoss: 0.6640200614929199\n",
      "cnt: 0 - valLoss: 0.6656349897384644 - trainLoss: 0.664019763469696\n",
      "cnt: 0 - valLoss: 0.6656347513198853 - trainLoss: 0.6640195250511169\n",
      "cnt: 0 - valLoss: 0.6656344532966614 - trainLoss: 0.6640192866325378\n",
      "cnt: 0 - valLoss: 0.6656342148780823 - trainLoss: 0.6640191078186035\n",
      "cnt: 0 - valLoss: 0.665634036064148 - trainLoss: 0.6640188097953796\n",
      "cnt: 0 - valLoss: 0.6656337976455688 - trainLoss: 0.6640185713768005\n",
      "cnt: 0 - valLoss: 0.6656334400177002 - trainLoss: 0.6640182137489319\n",
      "cnt: 0 - valLoss: 0.6656332015991211 - trainLoss: 0.6640180945396423\n",
      "cnt: 0 - valLoss: 0.665632963180542 - trainLoss: 0.6640177965164185\n",
      "cnt: 0 - valLoss: 0.6656327843666077 - trainLoss: 0.6640175580978394\n",
      "cnt: 0 - valLoss: 0.6656324863433838 - trainLoss: 0.6640173196792603\n",
      "cnt: 0 - valLoss: 0.6656322479248047 - trainLoss: 0.6640170812606812\n",
      "cnt: 0 - valLoss: 0.6656320095062256 - trainLoss: 0.664016842842102\n",
      "cnt: 0 - valLoss: 0.6656317114830017 - trainLoss: 0.664016604423523\n",
      "cnt: 0 - valLoss: 0.6656314730644226 - trainLoss: 0.6640163064002991\n",
      "cnt: 0 - valLoss: 0.6656311750411987 - trainLoss: 0.6640161275863647\n",
      "cnt: 0 - valLoss: 0.6656309962272644 - trainLoss: 0.6640158295631409\n",
      "cnt: 0 - valLoss: 0.6656307578086853 - trainLoss: 0.6640155911445618\n",
      "cnt: 0 - valLoss: 0.6656304597854614 - trainLoss: 0.6640153527259827\n",
      "cnt: 0 - valLoss: 0.6656302213668823 - trainLoss: 0.6640151143074036\n",
      "cnt: 0 - valLoss: 0.6656299829483032 - trainLoss: 0.6640148162841797\n",
      "cnt: 0 - valLoss: 0.6656298041343689 - trainLoss: 0.6640145778656006\n",
      "cnt: 0 - valLoss: 0.665629506111145 - trainLoss: 0.6640143990516663\n",
      "cnt: 0 - valLoss: 0.6656292080879211 - trainLoss: 0.6640141010284424\n",
      "cnt: 0 - valLoss: 0.6656290292739868 - trainLoss: 0.6640138626098633\n",
      "cnt: 0 - valLoss: 0.6656287312507629 - trainLoss: 0.6640136241912842\n",
      "cnt: 0 - valLoss: 0.6656284332275391 - trainLoss: 0.6640133857727051\n",
      "cnt: 0 - valLoss: 0.6656282544136047 - trainLoss: 0.6640130877494812\n",
      "cnt: 0 - valLoss: 0.6656279563903809 - trainLoss: 0.6640129089355469\n",
      "cnt: 0 - valLoss: 0.6656277179718018 - trainLoss: 0.6640126705169678\n",
      "cnt: 0 - valLoss: 0.6656274795532227 - trainLoss: 0.6640124320983887\n",
      "cnt: 0 - valLoss: 0.6656272411346436 - trainLoss: 0.6640121936798096\n",
      "cnt: 0 - valLoss: 0.6656270027160645 - trainLoss: 0.6640118956565857\n",
      "cnt: 0 - valLoss: 0.6656267642974854 - trainLoss: 0.6640117168426514\n",
      "cnt: 0 - valLoss: 0.6656264662742615 - trainLoss: 0.6640114188194275\n",
      "cnt: 0 - valLoss: 0.6656262278556824 - trainLoss: 0.6640112400054932\n",
      "cnt: 0 - valLoss: 0.6656259298324585 - trainLoss: 0.6640109419822693\n",
      "cnt: 0 - valLoss: 0.6656256914138794 - trainLoss: 0.6640107035636902\n",
      "cnt: 0 - valLoss: 0.6656254529953003 - trainLoss: 0.6640104651451111\n",
      "cnt: 0 - valLoss: 0.6656252145767212 - trainLoss: 0.664010226726532\n",
      "cnt: 0 - valLoss: 0.6656249165534973 - trainLoss: 0.6640099883079529\n",
      "cnt: 0 - valLoss: 0.665624737739563 - trainLoss: 0.6640097498893738\n",
      "cnt: 0 - valLoss: 0.6656244397163391 - trainLoss: 0.6640095114707947\n",
      "cnt: 0 - valLoss: 0.6656241416931152 - trainLoss: 0.6640092730522156\n",
      "cnt: 0 - valLoss: 0.6656239628791809 - trainLoss: 0.6640090942382812\n",
      "cnt: 0 - valLoss: 0.6656237244606018 - trainLoss: 0.6640087962150574\n",
      "cnt: 0 - valLoss: 0.6656234264373779 - trainLoss: 0.6640085577964783\n",
      "cnt: 0 - valLoss: 0.6656231880187988 - trainLoss: 0.6640083193778992\n",
      "cnt: 0 - valLoss: 0.665622889995575 - trainLoss: 0.6640080809593201\n",
      "cnt: 0 - valLoss: 0.6656227111816406 - trainLoss: 0.6640077829360962\n",
      "cnt: 0 - valLoss: 0.6656224131584167 - trainLoss: 0.6640075445175171\n",
      "cnt: 0 - valLoss: 0.6656221151351929 - trainLoss: 0.6640073657035828\n",
      "cnt: 0 - valLoss: 0.6656218767166138 - trainLoss: 0.6640071272850037\n",
      "cnt: 0 - valLoss: 0.6656216382980347 - trainLoss: 0.6640068292617798\n",
      "cnt: 0 - valLoss: 0.6656213998794556 - trainLoss: 0.6640065908432007\n",
      "cnt: 0 - valLoss: 0.6656211018562317 - trainLoss: 0.6640063524246216\n",
      "cnt: 0 - valLoss: 0.6656208634376526 - trainLoss: 0.6640061736106873\n",
      "cnt: 0 - valLoss: 0.6656206250190735 - trainLoss: 0.6640058159828186\n",
      "cnt: 0 - valLoss: 0.6656203269958496 - trainLoss: 0.6640055775642395\n",
      "cnt: 0 - valLoss: 0.6656200885772705 - trainLoss: 0.6640053987503052\n",
      "cnt: 0 - valLoss: 0.6656198501586914 - trainLoss: 0.6640051603317261\n",
      "cnt: 0 - valLoss: 0.6656196117401123 - trainLoss: 0.664004921913147\n",
      "cnt: 0 - valLoss: 0.6656193137168884 - trainLoss: 0.6640046238899231\n",
      "cnt: 0 - valLoss: 0.6656190752983093 - trainLoss: 0.6640044450759888\n",
      "cnt: 0 - valLoss: 0.6656188368797302 - trainLoss: 0.6640042066574097\n",
      "cnt: 0 - valLoss: 0.6656186580657959 - trainLoss: 0.6640039682388306\n",
      "cnt: 0 - valLoss: 0.6656183004379272 - trainLoss: 0.6640036702156067\n",
      "cnt: 0 - valLoss: 0.6656180620193481 - trainLoss: 0.6640034317970276\n",
      "cnt: 0 - valLoss: 0.665617823600769 - trainLoss: 0.6640031933784485\n",
      "cnt: 0 - valLoss: 0.6656175851821899 - trainLoss: 0.6640029549598694\n",
      "cnt: 0 - valLoss: 0.6656172871589661 - trainLoss: 0.6640027165412903\n",
      "cnt: 0 - valLoss: 0.665617048740387 - trainLoss: 0.6640024781227112\n",
      "cnt: 0 - valLoss: 0.6656168103218079 - trainLoss: 0.6640022397041321\n",
      "cnt: 0 - valLoss: 0.6656165719032288 - trainLoss: 0.664002001285553\n",
      "cnt: 0 - valLoss: 0.6656162142753601 - trainLoss: 0.6640017032623291\n",
      "cnt: 0 - valLoss: 0.6656160354614258 - trainLoss: 0.6640015244483948\n",
      "cnt: 0 - valLoss: 0.6656157970428467 - trainLoss: 0.6640012860298157\n",
      "cnt: 0 - valLoss: 0.6656154990196228 - trainLoss: 0.6640010476112366\n",
      "cnt: 0 - valLoss: 0.6656152606010437 - trainLoss: 0.6640007495880127\n",
      "cnt: 0 - valLoss: 0.6656150221824646 - trainLoss: 0.6640005707740784\n",
      "cnt: 0 - valLoss: 0.6656147241592407 - trainLoss: 0.6640003323554993\n",
      "cnt: 0 - valLoss: 0.6656144857406616 - trainLoss: 0.6640000939369202\n",
      "cnt: 0 - valLoss: 0.6656142473220825 - trainLoss: 0.6639998555183411\n",
      "cnt: 0 - valLoss: 0.6656139492988586 - trainLoss: 0.6639995574951172\n",
      "cnt: 0 - valLoss: 0.6656137704849243 - trainLoss: 0.6639993190765381\n",
      "cnt: 0 - valLoss: 0.6656134724617004 - trainLoss: 0.6639991402626038\n",
      "cnt: 0 - valLoss: 0.6656131744384766 - trainLoss: 0.6639988422393799\n",
      "cnt: 0 - valLoss: 0.6656129956245422 - trainLoss: 0.6639986634254456\n",
      "cnt: 0 - valLoss: 0.6656126379966736 - trainLoss: 0.6639983654022217\n",
      "cnt: 0 - valLoss: 0.6656123995780945 - trainLoss: 0.6639981865882874\n",
      "cnt: 0 - valLoss: 0.6656121611595154 - trainLoss: 0.6639978885650635\n",
      "cnt: 0 - valLoss: 0.6656118631362915 - trainLoss: 0.6639976501464844\n",
      "cnt: 0 - valLoss: 0.6656116247177124 - trainLoss: 0.6639974117279053\n",
      "cnt: 0 - valLoss: 0.6656113266944885 - trainLoss: 0.6639971137046814\n",
      "cnt: 0 - valLoss: 0.6656110882759094 - trainLoss: 0.6639969348907471\n",
      "cnt: 0 - valLoss: 0.6656108498573303 - trainLoss: 0.6639966368675232\n",
      "cnt: 0 - valLoss: 0.6656106114387512 - trainLoss: 0.6639964580535889\n",
      "cnt: 0 - valLoss: 0.6656103134155273 - trainLoss: 0.6639962196350098\n",
      "cnt: 0 - valLoss: 0.6656100749969482 - trainLoss: 0.6639959812164307\n",
      "cnt: 0 - valLoss: 0.6656098365783691 - trainLoss: 0.6639956831932068\n",
      "cnt: 0 - valLoss: 0.6656095385551453 - trainLoss: 0.6639955043792725\n",
      "cnt: 0 - valLoss: 0.6656093001365662 - trainLoss: 0.6639952063560486\n",
      "cnt: 0 - valLoss: 0.6656090617179871 - trainLoss: 0.6639949679374695\n",
      "cnt: 0 - valLoss: 0.6656087636947632 - trainLoss: 0.6639947295188904\n",
      "cnt: 0 - valLoss: 0.6656085252761841 - trainLoss: 0.6639944911003113\n",
      "cnt: 0 - valLoss: 0.665608286857605 - trainLoss: 0.6639942526817322\n",
      "cnt: 0 - valLoss: 0.6656079888343811 - trainLoss: 0.6639940142631531\n",
      "cnt: 0 - valLoss: 0.665607750415802 - trainLoss: 0.663993775844574\n",
      "cnt: 0 - valLoss: 0.6656075119972229 - trainLoss: 0.6639934778213501\n",
      "cnt: 0 - valLoss: 0.665607213973999 - trainLoss: 0.6639932990074158\n",
      "cnt: 0 - valLoss: 0.6656069755554199 - trainLoss: 0.6639930605888367\n",
      "cnt: 0 - valLoss: 0.6656067371368408 - trainLoss: 0.6639928221702576\n",
      "cnt: 0 - valLoss: 0.6656064391136169 - trainLoss: 0.6639925837516785\n",
      "cnt: 0 - valLoss: 0.6656062006950378 - trainLoss: 0.6639922857284546\n",
      "cnt: 0 - valLoss: 0.6656060218811035 - trainLoss: 0.6639921069145203\n",
      "cnt: 0 - valLoss: 0.6656057238578796 - trainLoss: 0.6639918088912964\n",
      "cnt: 0 - valLoss: 0.6656054258346558 - trainLoss: 0.6639915108680725\n",
      "cnt: 0 - valLoss: 0.6656052470207214 - trainLoss: 0.6639913320541382\n",
      "cnt: 0 - valLoss: 0.6656049489974976 - trainLoss: 0.6639910936355591\n",
      "cnt: 0 - valLoss: 0.6656047105789185 - trainLoss: 0.66399085521698\n",
      "cnt: 0 - valLoss: 0.6656044125556946 - trainLoss: 0.6639905571937561\n",
      "cnt: 0 - valLoss: 0.6656041741371155 - trainLoss: 0.663990318775177\n",
      "cnt: 0 - valLoss: 0.6656038761138916 - trainLoss: 0.6639900803565979\n",
      "cnt: 0 - valLoss: 0.6656036972999573 - trainLoss: 0.6639898419380188\n",
      "cnt: 0 - valLoss: 0.6656033992767334 - trainLoss: 0.6639895439147949\n",
      "cnt: 0 - valLoss: 0.6656031012535095 - trainLoss: 0.6639893054962158\n",
      "cnt: 0 - valLoss: 0.6656029224395752 - trainLoss: 0.6639890670776367\n",
      "cnt: 0 - valLoss: 0.6656026244163513 - trainLoss: 0.6639888286590576\n",
      "cnt: 0 - valLoss: 0.6656023859977722 - trainLoss: 0.6639885306358337\n",
      "cnt: 0 - valLoss: 0.6656021475791931 - trainLoss: 0.6639883518218994\n",
      "cnt: 0 - valLoss: 0.665601909160614 - trainLoss: 0.6639881134033203\n",
      "cnt: 0 - valLoss: 0.6656015515327454 - trainLoss: 0.6639878153800964\n",
      "cnt: 0 - valLoss: 0.6656014323234558 - trainLoss: 0.6639875769615173\n",
      "cnt: 0 - valLoss: 0.6656011343002319 - trainLoss: 0.6639873385429382\n",
      "cnt: 0 - valLoss: 0.6656008362770081 - trainLoss: 0.6639871001243591\n",
      "cnt: 0 - valLoss: 0.665600597858429 - trainLoss: 0.6639868021011353\n",
      "cnt: 0 - valLoss: 0.6656003594398499 - trainLoss: 0.6639866232872009\n",
      "cnt: 0 - valLoss: 0.6656001210212708 - trainLoss: 0.663986325263977\n",
      "cnt: 0 - valLoss: 0.6655997633934021 - trainLoss: 0.663986086845398\n",
      "cnt: 0 - valLoss: 0.6655995845794678 - trainLoss: 0.6639857888221741\n",
      "cnt: 0 - valLoss: 0.6655992865562439 - trainLoss: 0.6639856100082397\n",
      "cnt: 0 - valLoss: 0.6655990481376648 - trainLoss: 0.6639853119850159\n",
      "cnt: 0 - valLoss: 0.6655987501144409 - trainLoss: 0.6639850735664368\n",
      "cnt: 0 - valLoss: 0.6655985116958618 - trainLoss: 0.6639848351478577\n",
      "cnt: 0 - valLoss: 0.6655982136726379 - trainLoss: 0.6639845371246338\n",
      "cnt: 0 - valLoss: 0.6655979752540588 - trainLoss: 0.6639843583106995\n",
      "cnt: 0 - valLoss: 0.6655977368354797 - trainLoss: 0.6639840602874756\n",
      "cnt: 0 - valLoss: 0.6655974388122559 - trainLoss: 0.6639838218688965\n",
      "cnt: 0 - valLoss: 0.665597140789032 - trainLoss: 0.6639835834503174\n",
      "cnt: 0 - valLoss: 0.6655969619750977 - trainLoss: 0.6639833450317383\n",
      "cnt: 0 - valLoss: 0.6655967235565186 - trainLoss: 0.663983166217804\n",
      "cnt: 0 - valLoss: 0.6655964255332947 - trainLoss: 0.6639828681945801\n",
      "cnt: 0 - valLoss: 0.6655961871147156 - trainLoss: 0.663982629776001\n",
      "cnt: 0 - valLoss: 0.6655959486961365 - trainLoss: 0.6639823913574219\n",
      "cnt: 0 - valLoss: 0.6655956506729126 - trainLoss: 0.663982093334198\n",
      "cnt: 0 - valLoss: 0.6655954122543335 - trainLoss: 0.6639818549156189\n",
      "cnt: 0 - valLoss: 0.6655951142311096 - trainLoss: 0.6639816761016846\n",
      "cnt: 0 - valLoss: 0.6655948758125305 - trainLoss: 0.6639813780784607\n",
      "cnt: 0 - valLoss: 0.6655946373939514 - trainLoss: 0.6639811396598816\n",
      "cnt: 0 - valLoss: 0.6655943989753723 - trainLoss: 0.6639809012413025\n",
      "cnt: 0 - valLoss: 0.6655941009521484 - trainLoss: 0.6639806032180786\n",
      "cnt: 0 - valLoss: 0.6655938029289246 - trainLoss: 0.6639803647994995\n",
      "cnt: 0 - valLoss: 0.6655936241149902 - trainLoss: 0.6639801263809204\n",
      "cnt: 0 - valLoss: 0.6655932664871216 - trainLoss: 0.6639798879623413\n",
      "cnt: 0 - valLoss: 0.6655930280685425 - trainLoss: 0.6639796495437622\n",
      "cnt: 0 - valLoss: 0.6655927300453186 - trainLoss: 0.6639793515205383\n",
      "cnt: 0 - valLoss: 0.6655924916267395 - trainLoss: 0.663979172706604\n",
      "cnt: 0 - valLoss: 0.6655922532081604 - trainLoss: 0.6639788746833801\n",
      "cnt: 0 - valLoss: 0.6655919551849365 - trainLoss: 0.663978636264801\n",
      "cnt: 0 - valLoss: 0.6655917167663574 - trainLoss: 0.6639783978462219\n",
      "cnt: 0 - valLoss: 0.6655913591384888 - trainLoss: 0.6639781594276428\n",
      "cnt: 0 - valLoss: 0.6655911207199097 - trainLoss: 0.6639779806137085\n",
      "cnt: 0 - valLoss: 0.6655908823013306 - trainLoss: 0.6639776825904846\n",
      "cnt: 0 - valLoss: 0.6655905842781067 - trainLoss: 0.6639773845672607\n",
      "cnt: 0 - valLoss: 0.6655903458595276 - trainLoss: 0.6639772057533264\n",
      "cnt: 0 - valLoss: 0.6655899882316589 - trainLoss: 0.6639769077301025\n",
      "cnt: 0 - valLoss: 0.6655898094177246 - trainLoss: 0.6639767289161682\n",
      "cnt: 0 - valLoss: 0.6655895113945007 - trainLoss: 0.6639764904975891\n",
      "cnt: 0 - valLoss: 0.6655892133712769 - trainLoss: 0.6639761924743652\n",
      "cnt: 0 - valLoss: 0.665588915348053 - trainLoss: 0.6639760136604309\n",
      "cnt: 0 - valLoss: 0.6655887365341187 - trainLoss: 0.663975715637207\n",
      "cnt: 0 - valLoss: 0.66558837890625 - trainLoss: 0.6639754772186279\n",
      "cnt: 0 - valLoss: 0.6655881404876709 - trainLoss: 0.6639752984046936\n",
      "cnt: 0 - valLoss: 0.665587842464447 - trainLoss: 0.6639750003814697\n",
      "cnt: 0 - valLoss: 0.6655876040458679 - trainLoss: 0.6639748215675354\n",
      "cnt: 0 - valLoss: 0.665587306022644 - trainLoss: 0.6639745235443115\n",
      "cnt: 0 - valLoss: 0.6655870079994202 - trainLoss: 0.6639743447303772\n",
      "cnt: 0 - valLoss: 0.6655867695808411 - trainLoss: 0.6639740467071533\n",
      "cnt: 0 - valLoss: 0.6655864715576172 - trainLoss: 0.663973867893219\n",
      "cnt: 0 - valLoss: 0.6655862331390381 - trainLoss: 0.6639736294746399\n",
      "cnt: 0 - valLoss: 0.6655858755111694 - trainLoss: 0.6639733910560608\n",
      "cnt: 0 - valLoss: 0.6655856966972351 - trainLoss: 0.6639731526374817\n",
      "cnt: 0 - valLoss: 0.6655853986740112 - trainLoss: 0.6639729142189026\n",
      "cnt: 0 - valLoss: 0.6655851006507874 - trainLoss: 0.6639726758003235\n",
      "cnt: 0 - valLoss: 0.6655848026275635 - trainLoss: 0.6639724373817444\n",
      "cnt: 0 - valLoss: 0.6655846238136292 - trainLoss: 0.6639721989631653\n",
      "cnt: 0 - valLoss: 0.6655842661857605 - trainLoss: 0.6639719605445862\n",
      "cnt: 0 - valLoss: 0.6655840277671814 - trainLoss: 0.6639717221260071\n",
      "cnt: 0 - valLoss: 0.6655837297439575 - trainLoss: 0.663971483707428\n",
      "cnt: 0 - valLoss: 0.6655834913253784 - trainLoss: 0.6639712452888489\n",
      "cnt: 0 - valLoss: 0.6655832529067993 - trainLoss: 0.6639710068702698\n",
      "cnt: 0 - valLoss: 0.6655829548835754 - trainLoss: 0.6639707088470459\n",
      "cnt: 0 - valLoss: 0.6655826568603516 - trainLoss: 0.6639705300331116\n",
      "cnt: 0 - valLoss: 0.6655824184417725 - trainLoss: 0.6639702916145325\n",
      "cnt: 0 - valLoss: 0.6655821800231934 - trainLoss: 0.6639700531959534\n",
      "cnt: 0 - valLoss: 0.6655818819999695 - trainLoss: 0.6639698147773743\n",
      "cnt: 0 - valLoss: 0.6655816435813904 - trainLoss: 0.6639695763587952\n",
      "cnt: 0 - valLoss: 0.6655813455581665 - trainLoss: 0.6639693379402161\n",
      "cnt: 0 - valLoss: 0.6655811071395874 - trainLoss: 0.663969099521637\n",
      "cnt: 0 - valLoss: 0.6655808091163635 - trainLoss: 0.6639688014984131\n",
      "cnt: 0 - valLoss: 0.6655805706977844 - trainLoss: 0.6639686226844788\n",
      "cnt: 0 - valLoss: 0.6655802726745605 - trainLoss: 0.6639684438705444\n",
      "cnt: 0 - valLoss: 0.6655800342559814 - trainLoss: 0.6639681458473206\n",
      "cnt: 0 - valLoss: 0.6655797362327576 - trainLoss: 0.6639679074287415\n",
      "cnt: 0 - valLoss: 0.6655794978141785 - trainLoss: 0.6639676690101624\n",
      "cnt: 0 - valLoss: 0.6655791997909546 - trainLoss: 0.6639673709869385\n",
      "cnt: 0 - valLoss: 0.6655789613723755 - trainLoss: 0.6639671921730042\n",
      "cnt: 0 - valLoss: 0.6655786633491516 - trainLoss: 0.6639668941497803\n",
      "cnt: 0 - valLoss: 0.6655783653259277 - trainLoss: 0.663966715335846\n",
      "cnt: 0 - valLoss: 0.6655781865119934 - trainLoss: 0.6639664769172668\n",
      "cnt: 0 - valLoss: 0.6655778884887695 - trainLoss: 0.663966178894043\n",
      "cnt: 0 - valLoss: 0.6655775904655457 - trainLoss: 0.6639660000801086\n",
      "cnt: 0 - valLoss: 0.6655772924423218 - trainLoss: 0.6639657616615295\n",
      "cnt: 0 - valLoss: 0.6655770540237427 - trainLoss: 0.6639654636383057\n",
      "cnt: 0 - valLoss: 0.6655768156051636 - trainLoss: 0.6639652252197266\n",
      "cnt: 0 - valLoss: 0.6655765771865845 - trainLoss: 0.6639649868011475\n",
      "cnt: 0 - valLoss: 0.6655762195587158 - trainLoss: 0.6639648079872131\n",
      "cnt: 0 - valLoss: 0.6655760407447815 - trainLoss: 0.6639645099639893\n",
      "cnt: 0 - valLoss: 0.6655756831169128 - trainLoss: 0.6639642715454102\n",
      "cnt: 0 - valLoss: 0.6655755043029785 - trainLoss: 0.663964033126831\n",
      "cnt: 0 - valLoss: 0.6655752062797546 - trainLoss: 0.6639638543128967\n",
      "cnt: 0 - valLoss: 0.6655749082565308 - trainLoss: 0.6639636158943176\n",
      "cnt: 0 - valLoss: 0.6655746698379517 - trainLoss: 0.6639633774757385\n",
      "cnt: 0 - valLoss: 0.6655743718147278 - trainLoss: 0.6639631390571594\n",
      "cnt: 0 - valLoss: 0.6655741333961487 - trainLoss: 0.6639629006385803\n",
      "cnt: 0 - valLoss: 0.6655738353729248 - trainLoss: 0.6639626026153564\n",
      "cnt: 0 - valLoss: 0.6655735373497009 - trainLoss: 0.6639624238014221\n",
      "cnt: 0 - valLoss: 0.665573239326477 - trainLoss: 0.663962185382843\n",
      "cnt: 0 - valLoss: 0.6655730605125427 - trainLoss: 0.6639618873596191\n",
      "cnt: 0 - valLoss: 0.6655727624893188 - trainLoss: 0.66396164894104\n",
      "cnt: 0 - valLoss: 0.6655724048614502 - trainLoss: 0.6639614701271057\n",
      "cnt: 0 - valLoss: 0.6655721664428711 - trainLoss: 0.6639612317085266\n",
      "cnt: 0 - valLoss: 0.665571928024292 - trainLoss: 0.6639609932899475\n",
      "cnt: 0 - valLoss: 0.6655716896057129 - trainLoss: 0.6639606952667236\n",
      "cnt: 0 - valLoss: 0.665571391582489 - trainLoss: 0.6639605164527893\n",
      "cnt: 0 - valLoss: 0.6655710935592651 - trainLoss: 0.6639602184295654\n",
      "cnt: 0 - valLoss: 0.6655707955360413 - trainLoss: 0.6639599800109863\n",
      "cnt: 0 - valLoss: 0.6655705571174622 - trainLoss: 0.663959801197052\n",
      "cnt: 0 - valLoss: 0.6655703186988831 - trainLoss: 0.6639595627784729\n",
      "cnt: 0 - valLoss: 0.6655699610710144 - trainLoss: 0.6639593243598938\n",
      "cnt: 0 - valLoss: 0.6655696630477905 - trainLoss: 0.6639590263366699\n",
      "cnt: 0 - valLoss: 0.6655694246292114 - trainLoss: 0.6639587879180908\n",
      "cnt: 0 - valLoss: 0.6655691862106323 - trainLoss: 0.6639585494995117\n",
      "cnt: 0 - valLoss: 0.6655689477920532 - trainLoss: 0.6639583110809326\n",
      "cnt: 0 - valLoss: 0.6655686497688293 - trainLoss: 0.6639580726623535\n",
      "cnt: 0 - valLoss: 0.6655683517456055 - trainLoss: 0.6639578342437744\n",
      "cnt: 0 - valLoss: 0.6655681133270264 - trainLoss: 0.6639576554298401\n",
      "cnt: 0 - valLoss: 0.6655678153038025 - trainLoss: 0.6639573574066162\n",
      "cnt: 0 - valLoss: 0.6655675768852234 - trainLoss: 0.6639571785926819\n",
      "cnt: 0 - valLoss: 0.6655672788619995 - trainLoss: 0.663956880569458\n",
      "cnt: 0 - valLoss: 0.6655669808387756 - trainLoss: 0.6639567017555237\n",
      "cnt: 0 - valLoss: 0.6655667424201965 - trainLoss: 0.6639564633369446\n",
      "cnt: 0 - valLoss: 0.6655664443969727 - trainLoss: 0.6639561653137207\n",
      "cnt: 0 - valLoss: 0.6655662059783936 - trainLoss: 0.6639559864997864\n",
      "cnt: 0 - valLoss: 0.6655659079551697 - trainLoss: 0.6639556884765625\n",
      "cnt: 0 - valLoss: 0.6655656099319458 - trainLoss: 0.6639553904533386\n",
      "cnt: 0 - valLoss: 0.6655653715133667 - trainLoss: 0.6639552116394043\n",
      "cnt: 0 - valLoss: 0.6655650734901428 - trainLoss: 0.6639549136161804\n",
      "cnt: 0 - valLoss: 0.665564775466919 - trainLoss: 0.6639547348022461\n",
      "cnt: 0 - valLoss: 0.6655645370483398 - trainLoss: 0.663954496383667\n",
      "cnt: 0 - valLoss: 0.6655641794204712 - trainLoss: 0.6639542579650879\n",
      "cnt: 0 - valLoss: 0.6655640602111816 - trainLoss: 0.6639540195465088\n",
      "cnt: 0 - valLoss: 0.665563702583313 - trainLoss: 0.6639537811279297\n",
      "cnt: 0 - valLoss: 0.6655634045600891 - trainLoss: 0.6639535427093506\n",
      "cnt: 0 - valLoss: 0.66556316614151 - trainLoss: 0.6639532446861267\n",
      "cnt: 0 - valLoss: 0.6655628681182861 - trainLoss: 0.6639530658721924\n",
      "cnt: 0 - valLoss: 0.6655625700950623 - trainLoss: 0.6639528274536133\n",
      "cnt: 0 - valLoss: 0.6655623316764832 - trainLoss: 0.6639525890350342\n",
      "cnt: 0 - valLoss: 0.6655620336532593 - trainLoss: 0.6639523506164551\n",
      "cnt: 0 - valLoss: 0.6655617952346802 - trainLoss: 0.6639520525932312\n",
      "cnt: 0 - valLoss: 0.6655615568161011 - trainLoss: 0.6639518141746521\n",
      "cnt: 0 - valLoss: 0.6655611991882324 - trainLoss: 0.663951575756073\n",
      "cnt: 0 - valLoss: 0.6655609011650085 - trainLoss: 0.6639512777328491\n",
      "cnt: 0 - valLoss: 0.6655606627464294 - trainLoss: 0.6639511585235596\n",
      "cnt: 0 - valLoss: 0.6655604243278503 - trainLoss: 0.6639509201049805\n",
      "cnt: 0 - valLoss: 0.6655601263046265 - trainLoss: 0.6639506220817566\n",
      "cnt: 0 - valLoss: 0.6655598282814026 - trainLoss: 0.6639503836631775\n",
      "cnt: 0 - valLoss: 0.6655595898628235 - trainLoss: 0.6639500856399536\n",
      "cnt: 0 - valLoss: 0.6655592918395996 - trainLoss: 0.6639499068260193\n",
      "cnt: 0 - valLoss: 0.6655590534210205 - trainLoss: 0.6639496684074402\n",
      "cnt: 0 - valLoss: 0.6655587553977966 - trainLoss: 0.6639494299888611\n",
      "cnt: 0 - valLoss: 0.6655584573745728 - trainLoss: 0.6639491319656372\n",
      "cnt: 0 - valLoss: 0.6655582189559937 - trainLoss: 0.6639489531517029\n",
      "cnt: 0 - valLoss: 0.6655579209327698 - trainLoss: 0.6639487147331238\n",
      "cnt: 0 - valLoss: 0.6655576229095459 - trainLoss: 0.6639484763145447\n",
      "cnt: 0 - valLoss: 0.665557324886322 - trainLoss: 0.6639482378959656\n",
      "cnt: 0 - valLoss: 0.6655570864677429 - trainLoss: 0.6639479398727417\n",
      "cnt: 0 - valLoss: 0.6655568480491638 - trainLoss: 0.6639477014541626\n",
      "cnt: 0 - valLoss: 0.6655565500259399 - trainLoss: 0.6639474630355835\n",
      "cnt: 0 - valLoss: 0.6655562520027161 - trainLoss: 0.6639472246170044\n",
      "cnt: 0 - valLoss: 0.6655559539794922 - trainLoss: 0.6639470458030701\n",
      "cnt: 0 - valLoss: 0.6655557155609131 - trainLoss: 0.6639467477798462\n",
      "cnt: 0 - valLoss: 0.6655554175376892 - trainLoss: 0.6639465093612671\n",
      "cnt: 0 - valLoss: 0.6655551195144653 - trainLoss: 0.663946270942688\n",
      "cnt: 0 - valLoss: 0.6655548810958862 - trainLoss: 0.6639460325241089\n",
      "cnt: 0 - valLoss: 0.6655545830726624 - trainLoss: 0.6639457941055298\n",
      "cnt: 0 - valLoss: 0.6655542850494385 - trainLoss: 0.6639454960823059\n",
      "cnt: 0 - valLoss: 0.6655539870262146 - trainLoss: 0.6639453172683716\n",
      "cnt: 0 - valLoss: 0.6655537486076355 - trainLoss: 0.6639450788497925\n",
      "cnt: 0 - valLoss: 0.6655534505844116 - trainLoss: 0.6639447808265686\n",
      "cnt: 0 - valLoss: 0.6655532121658325 - trainLoss: 0.6639446020126343\n",
      "cnt: 0 - valLoss: 0.6655529141426086 - trainLoss: 0.6639443039894104\n",
      "cnt: 0 - valLoss: 0.6655526757240295 - trainLoss: 0.6639440655708313\n",
      "cnt: 0 - valLoss: 0.6655523777008057 - trainLoss: 0.6639437675476074\n",
      "cnt: 0 - valLoss: 0.6655520796775818 - trainLoss: 0.6639435887336731\n",
      "cnt: 0 - valLoss: 0.6655518412590027 - trainLoss: 0.663943350315094\n",
      "cnt: 0 - valLoss: 0.6655515432357788 - trainLoss: 0.6639431118965149\n",
      "cnt: 0 - valLoss: 0.6655512452125549 - trainLoss: 0.6639428734779358\n",
      "cnt: 0 - valLoss: 0.665550947189331 - trainLoss: 0.6639426350593567\n",
      "cnt: 0 - valLoss: 0.665550708770752 - trainLoss: 0.6639423370361328\n",
      "cnt: 0 - valLoss: 0.6655504107475281 - trainLoss: 0.6639421582221985\n",
      "cnt: 0 - valLoss: 0.665550172328949 - trainLoss: 0.6639418601989746\n",
      "cnt: 0 - valLoss: 0.6655498743057251 - trainLoss: 0.6639416217803955\n",
      "cnt: 0 - valLoss: 0.6655495762825012 - trainLoss: 0.6639413833618164\n",
      "cnt: 0 - valLoss: 0.6655493378639221 - trainLoss: 0.6639410853385925\n",
      "cnt: 0 - valLoss: 0.665549099445343 - trainLoss: 0.6639409065246582\n",
      "cnt: 0 - valLoss: 0.6655487418174744 - trainLoss: 0.6639406681060791\n",
      "cnt: 0 - valLoss: 0.6655485033988953 - trainLoss: 0.6639403700828552\n",
      "cnt: 0 - valLoss: 0.6655481457710266 - trainLoss: 0.6639401912689209\n",
      "cnt: 0 - valLoss: 0.6655479073524475 - trainLoss: 0.663939893245697\n",
      "cnt: 0 - valLoss: 0.6655476093292236 - trainLoss: 0.6639397144317627\n",
      "cnt: 0 - valLoss: 0.6655473113059998 - trainLoss: 0.6639394164085388\n",
      "cnt: 0 - valLoss: 0.6655470728874207 - trainLoss: 0.6639392375946045\n",
      "cnt: 0 - valLoss: 0.6655467748641968 - trainLoss: 0.6639389991760254\n",
      "cnt: 0 - valLoss: 0.6655464768409729 - trainLoss: 0.6639387011528015\n",
      "cnt: 0 - valLoss: 0.665546178817749 - trainLoss: 0.6639384627342224\n",
      "cnt: 0 - valLoss: 0.6655459403991699 - trainLoss: 0.6639382243156433\n",
      "cnt: 0 - valLoss: 0.6655457019805908 - trainLoss: 0.6639379262924194\n",
      "cnt: 0 - valLoss: 0.6655454039573669 - trainLoss: 0.6639376878738403\n",
      "cnt: 0 - valLoss: 0.6655451059341431 - trainLoss: 0.663937509059906\n",
      "cnt: 0 - valLoss: 0.6655448079109192 - trainLoss: 0.6639372110366821\n",
      "cnt: 0 - valLoss: 0.6655445694923401 - trainLoss: 0.6639370322227478\n",
      "cnt: 0 - valLoss: 0.6655442714691162 - trainLoss: 0.6639367341995239\n",
      "cnt: 0 - valLoss: 0.6655439734458923 - trainLoss: 0.6639364957809448\n",
      "cnt: 0 - valLoss: 0.6655436754226685 - trainLoss: 0.6639362573623657\n",
      "cnt: 0 - valLoss: 0.6655434370040894 - trainLoss: 0.6639360189437866\n",
      "cnt: 0 - valLoss: 0.6655431389808655 - trainLoss: 0.6639357209205627\n",
      "cnt: 0 - valLoss: 0.6655428409576416 - trainLoss: 0.6639355421066284\n",
      "cnt: 0 - valLoss: 0.6655426025390625 - trainLoss: 0.6639352440834045\n",
      "cnt: 0 - valLoss: 0.6655422449111938 - trainLoss: 0.6639350652694702\n",
      "cnt: 0 - valLoss: 0.6655420064926147 - trainLoss: 0.6639347672462463\n",
      "cnt: 0 - valLoss: 0.6655417680740356 - trainLoss: 0.6639345288276672\n",
      "cnt: 0 - valLoss: 0.6655414700508118 - trainLoss: 0.6639342904090881\n",
      "cnt: 0 - valLoss: 0.6655412316322327 - trainLoss: 0.6639341115951538\n",
      "cnt: 0 - valLoss: 0.6655409336090088 - trainLoss: 0.6639338135719299\n",
      "cnt: 0 - valLoss: 0.6655406355857849 - trainLoss: 0.6639335751533508\n",
      "cnt: 0 - valLoss: 0.665540337562561 - trainLoss: 0.6639333367347717\n",
      "cnt: 0 - valLoss: 0.6655400991439819 - trainLoss: 0.6639330387115479\n",
      "cnt: 0 - valLoss: 0.6655398011207581 - trainLoss: 0.6639328598976135\n",
      "cnt: 0 - valLoss: 0.6655395030975342 - trainLoss: 0.6639325618743896\n",
      "cnt: 0 - valLoss: 0.6655392050743103 - trainLoss: 0.6639323830604553\n",
      "cnt: 0 - valLoss: 0.6655389070510864 - trainLoss: 0.6639321446418762\n",
      "cnt: 0 - valLoss: 0.6655386686325073 - trainLoss: 0.6639318466186523\n",
      "cnt: 0 - valLoss: 0.6655384302139282 - trainLoss: 0.6639316082000732\n",
      "cnt: 0 - valLoss: 0.6655380725860596 - trainLoss: 0.6639313697814941\n",
      "cnt: 0 - valLoss: 0.6655377745628357 - trainLoss: 0.6639310717582703\n",
      "cnt: 0 - valLoss: 0.6655375361442566 - trainLoss: 0.6639308333396912\n",
      "cnt: 0 - valLoss: 0.6655371785163879 - trainLoss: 0.6639306545257568\n",
      "cnt: 0 - valLoss: 0.6655369997024536 - trainLoss: 0.6639304161071777\n",
      "cnt: 0 - valLoss: 0.6655367016792297 - trainLoss: 0.6639301776885986\n",
      "cnt: 0 - valLoss: 0.6655364036560059 - trainLoss: 0.6639298796653748\n",
      "cnt: 0 - valLoss: 0.665536105632782 - trainLoss: 0.6639297008514404\n",
      "cnt: 0 - valLoss: 0.6655358076095581 - trainLoss: 0.6639294028282166\n",
      "cnt: 0 - valLoss: 0.665535569190979 - trainLoss: 0.6639291644096375\n",
      "cnt: 0 - valLoss: 0.6655352711677551 - trainLoss: 0.6639289259910583\n",
      "cnt: 0 - valLoss: 0.6655349731445312 - trainLoss: 0.6639286875724792\n",
      "cnt: 0 - valLoss: 0.6655347347259521 - trainLoss: 0.6639284491539001\n",
      "cnt: 0 - valLoss: 0.6655344367027283 - trainLoss: 0.663928210735321\n",
      "cnt: 0 - valLoss: 0.6655341386795044 - trainLoss: 0.6639279127120972\n",
      "cnt: 0 - valLoss: 0.6655338406562805 - trainLoss: 0.6639276742935181\n",
      "cnt: 0 - valLoss: 0.6655336022377014 - trainLoss: 0.663927435874939\n",
      "cnt: 0 - valLoss: 0.6655332446098328 - trainLoss: 0.6639272570610046\n",
      "cnt: 0 - valLoss: 0.6655330061912537 - trainLoss: 0.6639269590377808\n",
      "cnt: 0 - valLoss: 0.6655327081680298 - trainLoss: 0.6639266610145569\n",
      "cnt: 0 - valLoss: 0.6655324697494507 - trainLoss: 0.6639264822006226\n",
      "cnt: 0 - valLoss: 0.6655321717262268 - trainLoss: 0.6639262437820435\n",
      "cnt: 0 - valLoss: 0.6655318737030029 - trainLoss: 0.6639259457588196\n",
      "cnt: 0 - valLoss: 0.6655315160751343 - trainLoss: 0.6639257073402405\n",
      "cnt: 0 - valLoss: 0.6655312776565552 - trainLoss: 0.6639254689216614\n",
      "cnt: 0 - valLoss: 0.6655310392379761 - trainLoss: 0.6639252305030823\n",
      "cnt: 0 - valLoss: 0.665530800819397 - trainLoss: 0.6639249920845032\n",
      "cnt: 0 - valLoss: 0.6655304431915283 - trainLoss: 0.6639247536659241\n",
      "cnt: 0 - valLoss: 0.6655301451683044 - trainLoss: 0.663924515247345\n",
      "cnt: 0 - valLoss: 0.6655299067497253 - trainLoss: 0.6639242768287659\n",
      "cnt: 0 - valLoss: 0.6655295491218567 - trainLoss: 0.663923978805542\n",
      "cnt: 0 - valLoss: 0.6655293107032776 - trainLoss: 0.6639237403869629\n",
      "cnt: 0 - valLoss: 0.6655290722846985 - trainLoss: 0.6639235019683838\n",
      "cnt: 0 - valLoss: 0.6655287742614746 - trainLoss: 0.6639233231544495\n",
      "cnt: 0 - valLoss: 0.6655284762382507 - trainLoss: 0.6639230251312256\n",
      "cnt: 0 - valLoss: 0.6655281782150269 - trainLoss: 0.6639227867126465\n",
      "cnt: 0 - valLoss: 0.665527880191803 - trainLoss: 0.6639225482940674\n",
      "cnt: 0 - valLoss: 0.6655276417732239 - trainLoss: 0.6639223098754883\n",
      "cnt: 0 - valLoss: 0.66552734375 - trainLoss: 0.6639220118522644\n",
      "cnt: 0 - valLoss: 0.6655271053314209 - trainLoss: 0.6639218330383301\n",
      "cnt: 0 - valLoss: 0.665526807308197 - trainLoss: 0.6639215350151062\n",
      "cnt: 0 - valLoss: 0.6655265092849731 - trainLoss: 0.6639212965965271\n",
      "cnt: 0 - valLoss: 0.6655262112617493 - trainLoss: 0.663921058177948\n",
      "cnt: 0 - valLoss: 0.6655259132385254 - trainLoss: 0.6639207601547241\n",
      "cnt: 0 - valLoss: 0.6655256152153015 - trainLoss: 0.663920521736145\n",
      "cnt: 0 - valLoss: 0.6655253171920776 - trainLoss: 0.6639202833175659\n",
      "cnt: 0 - valLoss: 0.6655250787734985 - trainLoss: 0.6639200448989868\n",
      "cnt: 0 - valLoss: 0.6655248403549194 - trainLoss: 0.6639198064804077\n",
      "cnt: 0 - valLoss: 0.6655245423316956 - trainLoss: 0.6639195680618286\n",
      "cnt: 0 - valLoss: 0.6655241847038269 - trainLoss: 0.6639192700386047\n",
      "cnt: 0 - valLoss: 0.6655239462852478 - trainLoss: 0.6639190316200256\n",
      "cnt: 0 - valLoss: 0.6655236482620239 - trainLoss: 0.6639187932014465\n",
      "cnt: 0 - valLoss: 0.6655232906341553 - trainLoss: 0.6639185547828674\n",
      "cnt: 0 - valLoss: 0.6655230522155762 - trainLoss: 0.6639183163642883\n",
      "cnt: 0 - valLoss: 0.6655227541923523 - trainLoss: 0.6639180779457092\n",
      "cnt: 0 - valLoss: 0.6655225157737732 - trainLoss: 0.6639177203178406\n",
      "cnt: 0 - valLoss: 0.6655222177505493 - trainLoss: 0.6639175415039062\n",
      "cnt: 0 - valLoss: 0.6655219197273254 - trainLoss: 0.6639173030853271\n",
      "cnt: 0 - valLoss: 0.6655216217041016 - trainLoss: 0.663917064666748\n",
      "cnt: 0 - valLoss: 0.6655213236808777 - trainLoss: 0.663916826248169\n",
      "cnt: 0 - valLoss: 0.6655210852622986 - trainLoss: 0.6639165282249451\n",
      "cnt: 0 - valLoss: 0.6655207872390747 - trainLoss: 0.663916289806366\n",
      "cnt: 0 - valLoss: 0.6655205488204956 - trainLoss: 0.6639160513877869\n",
      "cnt: 0 - valLoss: 0.6655202507972717 - trainLoss: 0.663915753364563\n",
      "cnt: 0 - valLoss: 0.6655199527740479 - trainLoss: 0.6639155149459839\n",
      "cnt: 0 - valLoss: 0.6655197143554688 - trainLoss: 0.6639152765274048\n",
      "cnt: 0 - valLoss: 0.6655194163322449 - trainLoss: 0.6639149785041809\n",
      "cnt: 0 - valLoss: 0.6655191779136658 - trainLoss: 0.6639147400856018\n",
      "cnt: 0 - valLoss: 0.6655188798904419 - trainLoss: 0.6639145612716675\n",
      "cnt: 0 - valLoss: 0.665518581867218 - trainLoss: 0.6639142632484436\n",
      "cnt: 0 - valLoss: 0.6655182838439941 - trainLoss: 0.6639139652252197\n",
      "cnt: 0 - valLoss: 0.665518045425415 - trainLoss: 0.6639137864112854\n",
      "cnt: 0 - valLoss: 0.6655178070068359 - trainLoss: 0.6639134883880615\n",
      "cnt: 0 - valLoss: 0.6655175089836121 - trainLoss: 0.6639131903648376\n",
      "cnt: 0 - valLoss: 0.6655172109603882 - trainLoss: 0.6639129519462585\n",
      "cnt: 0 - valLoss: 0.6655169725418091 - trainLoss: 0.6639127135276794\n",
      "cnt: 0 - valLoss: 0.66551673412323 - trainLoss: 0.6639124751091003\n",
      "cnt: 0 - valLoss: 0.6655163764953613 - trainLoss: 0.6639122366905212\n",
      "cnt: 0 - valLoss: 0.6655161380767822 - trainLoss: 0.6639119386672974\n",
      "cnt: 0 - valLoss: 0.6655158400535583 - trainLoss: 0.6639116406440735\n",
      "cnt: 0 - valLoss: 0.6655155420303345 - trainLoss: 0.6639115214347839\n",
      "cnt: 0 - valLoss: 0.6655152440071106 - trainLoss: 0.6639112234115601\n",
      "cnt: 0 - valLoss: 0.6655150055885315 - trainLoss: 0.6639109253883362\n",
      "cnt: 0 - valLoss: 0.6655147671699524 - trainLoss: 0.6639106869697571\n",
      "cnt: 0 - valLoss: 0.6655144691467285 - trainLoss: 0.6639103889465332\n",
      "cnt: 0 - valLoss: 0.6655142307281494 - trainLoss: 0.6639101505279541\n",
      "cnt: 0 - valLoss: 0.6655138731002808 - trainLoss: 0.663909912109375\n",
      "cnt: 0 - valLoss: 0.6655136346817017 - trainLoss: 0.6639096736907959\n",
      "cnt: 0 - valLoss: 0.6655133366584778 - trainLoss: 0.663909375667572\n",
      "cnt: 0 - valLoss: 0.6655130386352539 - trainLoss: 0.6639091372489929\n",
      "cnt: 0 - valLoss: 0.6655128002166748 - trainLoss: 0.6639088988304138\n",
      "cnt: 0 - valLoss: 0.6655125021934509 - trainLoss: 0.6639086604118347\n",
      "cnt: 0 - valLoss: 0.6655121445655823 - trainLoss: 0.6639083623886108\n",
      "cnt: 0 - valLoss: 0.6655119061470032 - trainLoss: 0.6639081835746765\n",
      "cnt: 0 - valLoss: 0.6655116677284241 - trainLoss: 0.6639078855514526\n",
      "cnt: 0 - valLoss: 0.6655113697052002 - trainLoss: 0.6639076471328735\n",
      "cnt: 0 - valLoss: 0.6655110716819763 - trainLoss: 0.6639074087142944\n",
      "cnt: 0 - valLoss: 0.6655107736587524 - trainLoss: 0.6639071106910706\n",
      "cnt: 0 - valLoss: 0.6655104756355286 - trainLoss: 0.6639068722724915\n",
      "cnt: 0 - valLoss: 0.6655101776123047 - trainLoss: 0.6639066338539124\n",
      "cnt: 0 - valLoss: 0.6655099391937256 - trainLoss: 0.6639063358306885\n",
      "cnt: 0 - valLoss: 0.6655096411705017 - trainLoss: 0.6639060974121094\n",
      "cnt: 0 - valLoss: 0.6655094027519226 - trainLoss: 0.6639058589935303\n",
      "cnt: 0 - valLoss: 0.6655091047286987 - trainLoss: 0.6639056205749512\n",
      "cnt: 0 - valLoss: 0.6655088067054749 - trainLoss: 0.6639053225517273\n",
      "cnt: 0 - valLoss: 0.665508508682251 - trainLoss: 0.6639050841331482\n",
      "cnt: 0 - valLoss: 0.6655082106590271 - trainLoss: 0.6639048457145691\n",
      "cnt: 0 - valLoss: 0.665507972240448 - trainLoss: 0.6639046669006348\n",
      "cnt: 0 - valLoss: 0.6655077338218689 - trainLoss: 0.6639043688774109\n",
      "cnt: 0 - valLoss: 0.6655073761940002 - trainLoss: 0.6639041304588318\n",
      "cnt: 0 - valLoss: 0.6655071377754211 - trainLoss: 0.6639038920402527\n",
      "cnt: 0 - valLoss: 0.6655068397521973 - trainLoss: 0.6639035940170288\n",
      "cnt: 0 - valLoss: 0.6655065417289734 - trainLoss: 0.6639033555984497\n",
      "cnt: 0 - valLoss: 0.6655063033103943 - trainLoss: 0.6639031171798706\n",
      "cnt: 0 - valLoss: 0.6655060052871704 - trainLoss: 0.6639028191566467\n",
      "cnt: 0 - valLoss: 0.6655057072639465 - trainLoss: 0.6639025807380676\n",
      "cnt: 0 - valLoss: 0.6655054092407227 - trainLoss: 0.6639023423194885\n",
      "cnt: 0 - valLoss: 0.6655051112174988 - trainLoss: 0.6639021039009094\n",
      "cnt: 0 - valLoss: 0.6655048131942749 - trainLoss: 0.6639018058776855\n",
      "cnt: 0 - valLoss: 0.665504515171051 - trainLoss: 0.6639015674591064\n",
      "cnt: 0 - valLoss: 0.6655042767524719 - trainLoss: 0.6639013290405273\n",
      "cnt: 0 - valLoss: 0.665503978729248 - trainLoss: 0.6639010906219482\n",
      "cnt: 0 - valLoss: 0.665503740310669 - trainLoss: 0.6639007925987244\n",
      "cnt: 0 - valLoss: 0.6655033826828003 - trainLoss: 0.6639005541801453\n",
      "cnt: 0 - valLoss: 0.6655031442642212 - trainLoss: 0.6639003157615662\n",
      "cnt: 0 - valLoss: 0.6655028462409973 - trainLoss: 0.6639001369476318\n",
      "cnt: 0 - valLoss: 0.6655025482177734 - trainLoss: 0.6638997793197632\n",
      "cnt: 0 - valLoss: 0.6655023097991943 - trainLoss: 0.6638995409011841\n",
      "cnt: 0 - valLoss: 0.6655020117759705 - trainLoss: 0.6638993620872498\n",
      "cnt: 0 - valLoss: 0.6655017137527466 - trainLoss: 0.6638990640640259\n",
      "cnt: 0 - valLoss: 0.6655014753341675 - trainLoss: 0.6638988256454468\n",
      "cnt: 0 - valLoss: 0.6655011773109436 - trainLoss: 0.6638985276222229\n",
      "cnt: 0 - valLoss: 0.6655008792877197 - trainLoss: 0.6638982892036438\n",
      "cnt: 0 - valLoss: 0.6655005812644958 - trainLoss: 0.6638980507850647\n",
      "cnt: 0 - valLoss: 0.6655003428459167 - trainLoss: 0.6638978123664856\n",
      "cnt: 0 - valLoss: 0.6654999852180481 - trainLoss: 0.6638975143432617\n",
      "cnt: 0 - valLoss: 0.6654998064041138 - trainLoss: 0.6638972759246826\n",
      "cnt: 0 - valLoss: 0.6654993891716003 - trainLoss: 0.6638970971107483\n",
      "cnt: 0 - valLoss: 0.665499210357666 - trainLoss: 0.6638967990875244\n",
      "cnt: 0 - valLoss: 0.6654989123344421 - trainLoss: 0.6638965010643005\n",
      "cnt: 0 - valLoss: 0.6654985547065735 - trainLoss: 0.6638962626457214\n",
      "cnt: 0 - valLoss: 0.6654983162879944 - trainLoss: 0.6638960242271423\n",
      "cnt: 0 - valLoss: 0.6654980182647705 - trainLoss: 0.6638957262039185\n",
      "cnt: 0 - valLoss: 0.6654977798461914 - trainLoss: 0.6638955473899841\n",
      "cnt: 0 - valLoss: 0.6654974818229675 - trainLoss: 0.6638952493667603\n",
      "cnt: 0 - valLoss: 0.6654971837997437 - trainLoss: 0.6638950109481812\n",
      "cnt: 0 - valLoss: 0.6654968857765198 - trainLoss: 0.663894772529602\n",
      "cnt: 0 - valLoss: 0.6654965877532959 - trainLoss: 0.6638944745063782\n",
      "cnt: 0 - valLoss: 0.665496289730072 - trainLoss: 0.6638941764831543\n",
      "cnt: 0 - valLoss: 0.6654959917068481 - trainLoss: 0.66389399766922\n",
      "cnt: 0 - valLoss: 0.6654956936836243 - trainLoss: 0.6638936996459961\n",
      "cnt: 0 - valLoss: 0.6654953956604004 - trainLoss: 0.663893461227417\n",
      "cnt: 0 - valLoss: 0.6654951572418213 - trainLoss: 0.6638932228088379\n",
      "cnt: 0 - valLoss: 0.6654948592185974 - trainLoss: 0.6638929843902588\n",
      "cnt: 0 - valLoss: 0.6654945611953735 - trainLoss: 0.6638926863670349\n",
      "cnt: 0 - valLoss: 0.6654942631721497 - trainLoss: 0.6638925075531006\n",
      "cnt: 0 - valLoss: 0.6654940247535706 - trainLoss: 0.6638922095298767\n",
      "cnt: 0 - valLoss: 0.6654936671257019 - trainLoss: 0.6638919115066528\n",
      "cnt: 0 - valLoss: 0.665493369102478 - trainLoss: 0.6638917326927185\n",
      "cnt: 0 - valLoss: 0.6654930710792542 - trainLoss: 0.6638914942741394\n",
      "cnt: 0 - valLoss: 0.665492832660675 - trainLoss: 0.6638911366462708\n",
      "cnt: 0 - valLoss: 0.6654924750328064 - trainLoss: 0.6638909578323364\n",
      "cnt: 0 - valLoss: 0.6654922366142273 - trainLoss: 0.6638907194137573\n",
      "cnt: 0 - valLoss: 0.6654919385910034 - trainLoss: 0.6638904213905334\n",
      "cnt: 0 - valLoss: 0.6654916405677795 - trainLoss: 0.6638901829719543\n",
      "cnt: 0 - valLoss: 0.6654913425445557 - trainLoss: 0.6638899445533752\n",
      "cnt: 0 - valLoss: 0.6654910445213318 - trainLoss: 0.6638896465301514\n",
      "cnt: 0 - valLoss: 0.6654907464981079 - trainLoss: 0.6638894081115723\n",
      "cnt: 0 - valLoss: 0.6654905080795288 - trainLoss: 0.6638891696929932\n",
      "cnt: 0 - valLoss: 0.6654901504516602 - trainLoss: 0.6638888716697693\n",
      "cnt: 0 - valLoss: 0.665489912033081 - trainLoss: 0.6638886332511902\n",
      "cnt: 0 - valLoss: 0.6654895544052124 - trainLoss: 0.6638884544372559\n",
      "cnt: 0 - valLoss: 0.6654893159866333 - trainLoss: 0.6638880968093872\n",
      "cnt: 0 - valLoss: 0.6654890179634094 - trainLoss: 0.6638878583908081\n",
      "cnt: 0 - valLoss: 0.6654887199401855 - trainLoss: 0.663887619972229\n",
      "cnt: 0 - valLoss: 0.6654884219169617 - trainLoss: 0.6638873815536499\n",
      "cnt: 0 - valLoss: 0.6654881834983826 - trainLoss: 0.663887083530426\n",
      "cnt: 0 - valLoss: 0.6654878258705139 - trainLoss: 0.6638868451118469\n",
      "cnt: 0 - valLoss: 0.6654875874519348 - trainLoss: 0.6638866066932678\n",
      "cnt: 0 - valLoss: 0.6654872894287109 - trainLoss: 0.663886308670044\n",
      "cnt: 0 - valLoss: 0.6654869914054871 - trainLoss: 0.6638860702514648\n",
      "cnt: 0 - valLoss: 0.6654866933822632 - trainLoss: 0.6638858318328857\n",
      "cnt: 0 - valLoss: 0.6654863953590393 - trainLoss: 0.6638855934143066\n",
      "cnt: 0 - valLoss: 0.6654860973358154 - trainLoss: 0.6638852953910828\n",
      "cnt: 0 - valLoss: 0.6654857993125916 - trainLoss: 0.6638850569725037\n",
      "cnt: 0 - valLoss: 0.6654855012893677 - trainLoss: 0.6638848185539246\n",
      "cnt: 0 - valLoss: 0.6654852032661438 - trainLoss: 0.6638845205307007\n",
      "cnt: 0 - valLoss: 0.6654849052429199 - trainLoss: 0.6638842821121216\n",
      "cnt: 0 - valLoss: 0.665484607219696 - trainLoss: 0.6638840436935425\n",
      "cnt: 0 - valLoss: 0.6654843688011169 - trainLoss: 0.6638837456703186\n",
      "cnt: 0 - valLoss: 0.6654840707778931 - trainLoss: 0.6638835072517395\n",
      "cnt: 0 - valLoss: 0.6654837727546692 - trainLoss: 0.6638832688331604\n",
      "cnt: 0 - valLoss: 0.6654834747314453 - trainLoss: 0.6638830304145813\n",
      "cnt: 0 - valLoss: 0.6654831767082214 - trainLoss: 0.6638827919960022\n",
      "cnt: 0 - valLoss: 0.6654828786849976 - trainLoss: 0.6638824939727783\n",
      "cnt: 0 - valLoss: 0.6654825806617737 - trainLoss: 0.6638821959495544\n",
      "cnt: 0 - valLoss: 0.6654822826385498 - trainLoss: 0.6638819575309753\n",
      "cnt: 0 - valLoss: 0.6654819846153259 - trainLoss: 0.6638817191123962\n",
      "cnt: 0 - valLoss: 0.6654816269874573 - trainLoss: 0.6638814210891724\n",
      "cnt: 0 - valLoss: 0.6654813885688782 - trainLoss: 0.6638811826705933\n",
      "cnt: 0 - valLoss: 0.6654811501502991 - trainLoss: 0.6638809442520142\n",
      "cnt: 0 - valLoss: 0.6654807925224304 - trainLoss: 0.6638807058334351\n",
      "cnt: 0 - valLoss: 0.6654805541038513 - trainLoss: 0.6638804078102112\n",
      "cnt: 0 - valLoss: 0.6654802560806274 - trainLoss: 0.6638802289962769\n",
      "cnt: 0 - valLoss: 0.6654798984527588 - trainLoss: 0.663879930973053\n",
      "cnt: 0 - valLoss: 0.6654796004295349 - trainLoss: 0.6638796925544739\n",
      "cnt: 0 - valLoss: 0.6654793620109558 - trainLoss: 0.6638794541358948\n",
      "cnt: 0 - valLoss: 0.6654790639877319 - trainLoss: 0.6638791561126709\n",
      "cnt: 0 - valLoss: 0.6654787659645081 - trainLoss: 0.6638789176940918\n",
      "cnt: 0 - valLoss: 0.6654784679412842 - trainLoss: 0.6638787388801575\n",
      "cnt: 0 - valLoss: 0.6654781699180603 - trainLoss: 0.6638784408569336\n",
      "cnt: 0 - valLoss: 0.6654778718948364 - trainLoss: 0.6638781428337097\n",
      "cnt: 0 - valLoss: 0.6654775142669678 - trainLoss: 0.6638779640197754\n",
      "cnt: 0 - valLoss: 0.6654772758483887 - trainLoss: 0.6638776659965515\n",
      "cnt: 0 - valLoss: 0.6654769778251648 - trainLoss: 0.6638774275779724\n",
      "cnt: 0 - valLoss: 0.6654766798019409 - trainLoss: 0.6638771891593933\n",
      "cnt: 0 - valLoss: 0.6654763221740723 - trainLoss: 0.6638768911361694\n",
      "cnt: 0 - valLoss: 0.6654760837554932 - trainLoss: 0.6638766527175903\n",
      "cnt: 0 - valLoss: 0.6654758453369141 - trainLoss: 0.6638764142990112\n",
      "cnt: 0 - valLoss: 0.6654755473136902 - trainLoss: 0.6638761758804321\n",
      "cnt: 0 - valLoss: 0.6654752492904663 - trainLoss: 0.6638758778572083\n",
      "cnt: 0 - valLoss: 0.6654748916625977 - trainLoss: 0.6638756990432739\n",
      "cnt: 0 - valLoss: 0.6654746532440186 - trainLoss: 0.6638754606246948\n",
      "cnt: 0 - valLoss: 0.6654743552207947 - trainLoss: 0.6638752222061157\n",
      "cnt: 0 - valLoss: 0.665473997592926 - trainLoss: 0.6638749241828918\n",
      "cnt: 0 - valLoss: 0.6654736995697021 - trainLoss: 0.663874626159668\n",
      "cnt: 0 - valLoss: 0.6654734015464783 - trainLoss: 0.6638744473457336\n",
      "cnt: 0 - valLoss: 0.6654731631278992 - trainLoss: 0.6638741493225098\n",
      "cnt: 0 - valLoss: 0.6654728055000305 - trainLoss: 0.6638739109039307\n",
      "cnt: 0 - valLoss: 0.6654725074768066 - trainLoss: 0.6638736128807068\n",
      "cnt: 0 - valLoss: 0.6654722094535828 - trainLoss: 0.6638733744621277\n",
      "cnt: 0 - valLoss: 0.6654719710350037 - trainLoss: 0.6638731360435486\n",
      "cnt: 0 - valLoss: 0.6654716730117798 - trainLoss: 0.6638728976249695\n",
      "cnt: 0 - valLoss: 0.6654713749885559 - trainLoss: 0.6638726592063904\n",
      "cnt: 0 - valLoss: 0.665471076965332 - trainLoss: 0.6638724207878113\n",
      "cnt: 0 - valLoss: 0.6654707789421082 - trainLoss: 0.6638721823692322\n",
      "cnt: 0 - valLoss: 0.6654704213142395 - trainLoss: 0.6638718843460083\n",
      "cnt: 0 - valLoss: 0.6654701828956604 - trainLoss: 0.6638716459274292\n",
      "cnt: 0 - valLoss: 0.6654698252677917 - trainLoss: 0.6638714075088501\n",
      "cnt: 0 - valLoss: 0.6654695868492126 - trainLoss: 0.6638711094856262\n",
      "cnt: 0 - valLoss: 0.665469229221344 - trainLoss: 0.6638708114624023\n",
      "cnt: 0 - valLoss: 0.6654689908027649 - trainLoss: 0.6638705730438232\n",
      "cnt: 0 - valLoss: 0.665468692779541 - trainLoss: 0.6638703942298889\n",
      "cnt: 0 - valLoss: 0.6654683947563171 - trainLoss: 0.6638700366020203\n",
      "cnt: 0 - valLoss: 0.6654680371284485 - trainLoss: 0.6638698577880859\n",
      "cnt: 0 - valLoss: 0.6654677391052246 - trainLoss: 0.6638696193695068\n",
      "cnt: 0 - valLoss: 0.6654674410820007 - trainLoss: 0.6638693809509277\n",
      "cnt: 0 - valLoss: 0.6654671430587769 - trainLoss: 0.6638691425323486\n",
      "cnt: 0 - valLoss: 0.665466845035553 - trainLoss: 0.6638688445091248\n",
      "cnt: 0 - valLoss: 0.6654665470123291 - trainLoss: 0.6638686060905457\n",
      "cnt: 0 - valLoss: 0.6654662489891052 - trainLoss: 0.6638683676719666\n",
      "cnt: 0 - valLoss: 0.6654658913612366 - trainLoss: 0.6638680696487427\n",
      "cnt: 0 - valLoss: 0.6654655933380127 - trainLoss: 0.6638677716255188\n",
      "cnt: 0 - valLoss: 0.6654652953147888 - trainLoss: 0.6638675928115845\n",
      "cnt: 0 - valLoss: 0.6654649972915649 - trainLoss: 0.6638672947883606\n",
      "cnt: 0 - valLoss: 0.6654646992683411 - trainLoss: 0.6638670563697815\n",
      "cnt: 0 - valLoss: 0.6654644012451172 - trainLoss: 0.6638668179512024\n",
      "cnt: 0 - valLoss: 0.6654641032218933 - trainLoss: 0.6638665795326233\n",
      "cnt: 0 - valLoss: 0.6654638051986694 - trainLoss: 0.6638662815093994\n",
      "cnt: 0 - valLoss: 0.6654635071754456 - trainLoss: 0.6638660430908203\n",
      "cnt: 0 - valLoss: 0.6654632091522217 - trainLoss: 0.6638658046722412\n",
      "cnt: 0 - valLoss: 0.665462851524353 - trainLoss: 0.6638656258583069\n",
      "cnt: 0 - valLoss: 0.6654625535011292 - trainLoss: 0.6638652682304382\n",
      "cnt: 0 - valLoss: 0.6654622554779053 - trainLoss: 0.6638650298118591\n",
      "cnt: 0 - valLoss: 0.6654620170593262 - trainLoss: 0.6638647317886353\n",
      "cnt: 0 - valLoss: 0.6654617190361023 - trainLoss: 0.6638645529747009\n",
      "cnt: 0 - valLoss: 0.6654614210128784 - trainLoss: 0.663864254951477\n",
      "cnt: 0 - valLoss: 0.6654610633850098 - trainLoss: 0.663864016532898\n",
      "cnt: 0 - valLoss: 0.6654608845710754 - trainLoss: 0.6638638377189636\n",
      "cnt: 0 - valLoss: 0.6654605269432068 - trainLoss: 0.6638635396957397\n",
      "cnt: 0 - valLoss: 0.6654602289199829 - trainLoss: 0.6638632416725159\n",
      "cnt: 0 - valLoss: 0.665459930896759 - trainLoss: 0.663862943649292\n",
      "cnt: 0 - valLoss: 0.6654595732688904 - trainLoss: 0.6638627648353577\n",
      "cnt: 0 - valLoss: 0.665459394454956 - trainLoss: 0.6638625264167786\n",
      "cnt: 0 - valLoss: 0.6654590368270874 - trainLoss: 0.6638622879981995\n",
      "cnt: 0 - valLoss: 0.6654587388038635 - trainLoss: 0.6638619899749756\n",
      "cnt: 0 - valLoss: 0.6654584407806396 - trainLoss: 0.6638617515563965\n",
      "cnt: 0 - valLoss: 0.6654581427574158 - trainLoss: 0.6638615131378174\n",
      "cnt: 0 - valLoss: 0.6654578447341919 - trainLoss: 0.6638612747192383\n",
      "cnt: 0 - valLoss: 0.665457546710968 - trainLoss: 0.6638609766960144\n",
      "cnt: 0 - valLoss: 0.6654572486877441 - trainLoss: 0.6638607382774353\n",
      "cnt: 0 - valLoss: 0.6654569506645203 - trainLoss: 0.6638604998588562\n",
      "cnt: 0 - valLoss: 0.6654565930366516 - trainLoss: 0.6638602614402771\n",
      "cnt: 0 - valLoss: 0.6654563546180725 - trainLoss: 0.6638599634170532\n",
      "cnt: 0 - valLoss: 0.6654559969902039 - trainLoss: 0.6638597249984741\n",
      "cnt: 0 - valLoss: 0.6654557585716248 - trainLoss: 0.663859486579895\n",
      "cnt: 0 - valLoss: 0.6654554605484009 - trainLoss: 0.6638591885566711\n",
      "cnt: 0 - valLoss: 0.665455162525177 - trainLoss: 0.663858950138092\n",
      "cnt: 0 - valLoss: 0.6654548645019531 - trainLoss: 0.6638586521148682\n",
      "cnt: 0 - valLoss: 0.6654545664787292 - trainLoss: 0.6638584733009338\n",
      "cnt: 0 - valLoss: 0.6654543280601501 - trainLoss: 0.6638582348823547\n",
      "cnt: 0 - valLoss: 0.6654539108276367 - trainLoss: 0.6638579964637756\n",
      "cnt: 0 - valLoss: 0.6654536724090576 - trainLoss: 0.6638577580451965\n",
      "cnt: 0 - valLoss: 0.6654533743858337 - trainLoss: 0.6638574600219727\n",
      "cnt: 0 - valLoss: 0.6654530763626099 - trainLoss: 0.6638572216033936\n",
      "cnt: 0 - valLoss: 0.665452778339386 - trainLoss: 0.6638569831848145\n",
      "cnt: 0 - valLoss: 0.6654524803161621 - trainLoss: 0.6638566851615906\n",
      "cnt: 0 - valLoss: 0.6654521822929382 - trainLoss: 0.6638564467430115\n",
      "cnt: 0 - valLoss: 0.6654518842697144 - trainLoss: 0.6638562083244324\n",
      "cnt: 0 - valLoss: 0.6654515862464905 - trainLoss: 0.6638559103012085\n",
      "cnt: 0 - valLoss: 0.6654512286186218 - trainLoss: 0.6638556718826294\n",
      "cnt: 0 - valLoss: 0.6654509902000427 - trainLoss: 0.6638554334640503\n",
      "cnt: 0 - valLoss: 0.6654506921768188 - trainLoss: 0.6638551950454712\n",
      "cnt: 0 - valLoss: 0.6654503345489502 - trainLoss: 0.6638549566268921\n",
      "cnt: 0 - valLoss: 0.6654500961303711 - trainLoss: 0.663854718208313\n",
      "cnt: 0 - valLoss: 0.6654497981071472 - trainLoss: 0.6638544201850891\n",
      "cnt: 0 - valLoss: 0.6654494404792786 - trainLoss: 0.6638541221618652\n",
      "cnt: 0 - valLoss: 0.6654492616653442 - trainLoss: 0.6638538837432861\n",
      "cnt: 0 - valLoss: 0.6654488444328308 - trainLoss: 0.663853645324707\n",
      "cnt: 0 - valLoss: 0.6654485464096069 - trainLoss: 0.6638533473014832\n",
      "cnt: 0 - valLoss: 0.6654483079910278 - trainLoss: 0.6638531684875488\n",
      "cnt: 0 - valLoss: 0.6654479503631592 - trainLoss: 0.6638529300689697\n",
      "cnt: 0 - valLoss: 0.6654477119445801 - trainLoss: 0.6638525724411011\n",
      "cnt: 0 - valLoss: 0.6654473543167114 - trainLoss: 0.663852334022522\n",
      "cnt: 0 - valLoss: 0.6654471158981323 - trainLoss: 0.6638521552085876\n",
      "cnt: 0 - valLoss: 0.6654467582702637 - trainLoss: 0.6638518571853638\n",
      "cnt: 0 - valLoss: 0.6654465198516846 - trainLoss: 0.6638516187667847\n",
      "cnt: 0 - valLoss: 0.6654462218284607 - trainLoss: 0.6638513803482056\n",
      "cnt: 0 - valLoss: 0.6654459238052368 - trainLoss: 0.6638511419296265\n",
      "cnt: 0 - valLoss: 0.6654456257820129 - trainLoss: 0.6638509035110474\n",
      "cnt: 0 - valLoss: 0.6654453277587891 - trainLoss: 0.6638506054878235\n",
      "cnt: 0 - valLoss: 0.6654450297355652 - trainLoss: 0.6638503670692444\n",
      "cnt: 0 - valLoss: 0.6654447317123413 - trainLoss: 0.6638501286506653\n",
      "cnt: 0 - valLoss: 0.6654444336891174 - trainLoss: 0.6638498902320862\n",
      "cnt: 0 - valLoss: 0.6654441356658936 - trainLoss: 0.6638495326042175\n",
      "cnt: 0 - valLoss: 0.6654437780380249 - trainLoss: 0.6638493537902832\n",
      "cnt: 0 - valLoss: 0.665443480014801 - trainLoss: 0.6638491153717041\n",
      "cnt: 0 - valLoss: 0.6654431819915771 - trainLoss: 0.6638488173484802\n",
      "cnt: 0 - valLoss: 0.6654428839683533 - trainLoss: 0.6638485789299011\n",
      "cnt: 0 - valLoss: 0.6654425859451294 - trainLoss: 0.663848340511322\n",
      "cnt: 0 - valLoss: 0.6654422879219055 - trainLoss: 0.6638480424880981\n",
      "cnt: 0 - valLoss: 0.6654419898986816 - trainLoss: 0.663847804069519\n",
      "cnt: 0 - valLoss: 0.6654416918754578 - trainLoss: 0.6638475060462952\n",
      "cnt: 0 - valLoss: 0.6654413938522339 - trainLoss: 0.6638472676277161\n",
      "cnt: 0 - valLoss: 0.6654410362243652 - trainLoss: 0.663847029209137\n",
      "cnt: 0 - valLoss: 0.6654407382011414 - trainLoss: 0.6638467907905579\n",
      "cnt: 0 - valLoss: 0.6654404401779175 - trainLoss: 0.6638465523719788\n",
      "cnt: 0 - valLoss: 0.6654401421546936 - trainLoss: 0.6638462543487549\n",
      "cnt: 0 - valLoss: 0.6654398441314697 - trainLoss: 0.663845956325531\n",
      "cnt: 0 - valLoss: 0.6654395461082458 - trainLoss: 0.6638457179069519\n",
      "cnt: 0 - valLoss: 0.665439248085022 - trainLoss: 0.6638454794883728\n",
      "cnt: 0 - valLoss: 0.6654389500617981 - trainLoss: 0.6638452410697937\n",
      "cnt: 0 - valLoss: 0.6654385924339294 - trainLoss: 0.6638450026512146\n",
      "cnt: 0 - valLoss: 0.6654383540153503 - trainLoss: 0.6638447046279907\n",
      "cnt: 0 - valLoss: 0.6654379963874817 - trainLoss: 0.6638445258140564\n",
      "cnt: 0 - valLoss: 0.6654377579689026 - trainLoss: 0.6638442277908325\n",
      "cnt: 0 - valLoss: 0.6654374599456787 - trainLoss: 0.6638439297676086\n",
      "cnt: 0 - valLoss: 0.6654372215270996 - trainLoss: 0.6638436913490295\n",
      "cnt: 0 - valLoss: 0.6654368042945862 - trainLoss: 0.6638434529304504\n",
      "cnt: 0 - valLoss: 0.6654365062713623 - trainLoss: 0.6638431549072266\n",
      "cnt: 0 - valLoss: 0.6654362082481384 - trainLoss: 0.6638429164886475\n",
      "cnt: 0 - valLoss: 0.6654359698295593 - trainLoss: 0.6638426780700684\n",
      "cnt: 0 - valLoss: 0.6654356718063354 - trainLoss: 0.6638424396514893\n",
      "cnt: 0 - valLoss: 0.6654353737831116 - trainLoss: 0.6638421416282654\n",
      "cnt: 0 - valLoss: 0.6654350161552429 - trainLoss: 0.6638419032096863\n",
      "cnt: 0 - valLoss: 0.665434718132019 - trainLoss: 0.6638416647911072\n",
      "cnt: 0 - valLoss: 0.6654344201087952 - trainLoss: 0.6638413667678833\n",
      "cnt: 0 - valLoss: 0.6654340624809265 - trainLoss: 0.6638411283493042\n",
      "cnt: 0 - valLoss: 0.6654337644577026 - trainLoss: 0.6638408303260803\n",
      "cnt: 0 - valLoss: 0.6654334664344788 - trainLoss: 0.6638405919075012\n",
      "cnt: 0 - valLoss: 0.6654332280158997 - trainLoss: 0.6638404130935669\n",
      "cnt: 0 - valLoss: 0.6654329299926758 - trainLoss: 0.663840115070343\n",
      "cnt: 0 - valLoss: 0.6654325723648071 - trainLoss: 0.6638398170471191\n",
      "cnt: 0 - valLoss: 0.6654322743415833 - trainLoss: 0.66383957862854\n",
      "cnt: 0 - valLoss: 0.6654319763183594 - trainLoss: 0.6638393402099609\n",
      "cnt: 0 - valLoss: 0.6654316782951355 - trainLoss: 0.6638391017913818\n",
      "cnt: 0 - valLoss: 0.6654314398765564 - trainLoss: 0.663838803768158\n",
      "cnt: 0 - valLoss: 0.6654310822486877 - trainLoss: 0.6638385653495789\n",
      "cnt: 0 - valLoss: 0.6654307842254639 - trainLoss: 0.6638383269309998\n",
      "cnt: 0 - valLoss: 0.66543048620224 - trainLoss: 0.6638380289077759\n",
      "cnt: 0 - valLoss: 0.6654301285743713 - trainLoss: 0.663837730884552\n",
      "cnt: 0 - valLoss: 0.6654298901557922 - trainLoss: 0.6638375520706177\n",
      "cnt: 0 - valLoss: 0.6654295325279236 - trainLoss: 0.6638372540473938\n",
      "cnt: 0 - valLoss: 0.6654292345046997 - trainLoss: 0.6638370156288147\n",
      "cnt: 0 - valLoss: 0.6654289364814758 - trainLoss: 0.6638367772102356\n",
      "cnt: 0 - valLoss: 0.665428638458252 - trainLoss: 0.6638364791870117\n",
      "cnt: 0 - valLoss: 0.6654282808303833 - trainLoss: 0.6638362407684326\n",
      "cnt: 0 - valLoss: 0.6654279828071594 - trainLoss: 0.6638360023498535\n",
      "cnt: 0 - valLoss: 0.6654276847839355 - trainLoss: 0.6638357639312744\n",
      "cnt: 0 - valLoss: 0.6654273867607117 - trainLoss: 0.6638355255126953\n",
      "cnt: 0 - valLoss: 0.6654271483421326 - trainLoss: 0.6638352274894714\n",
      "cnt: 0 - valLoss: 0.6654267907142639 - trainLoss: 0.6638349890708923\n",
      "cnt: 0 - valLoss: 0.6654264330863953 - trainLoss: 0.6638346910476685\n",
      "cnt: 0 - valLoss: 0.6654261350631714 - trainLoss: 0.6638344526290894\n",
      "cnt: 0 - valLoss: 0.6654258370399475 - trainLoss: 0.6638342142105103\n",
      "cnt: 0 - valLoss: 0.6654255986213684 - trainLoss: 0.6638339161872864\n",
      "cnt: 0 - valLoss: 0.6654253602027893 - trainLoss: 0.6638336777687073\n",
      "cnt: 0 - valLoss: 0.6654250025749207 - trainLoss: 0.6638334393501282\n",
      "cnt: 0 - valLoss: 0.665424644947052 - trainLoss: 0.6638331413269043\n",
      "cnt: 0 - valLoss: 0.6654242873191833 - trainLoss: 0.6638329029083252\n",
      "cnt: 0 - valLoss: 0.6654240489006042 - trainLoss: 0.6638326048851013\n",
      "cnt: 0 - valLoss: 0.6654236912727356 - trainLoss: 0.6638323664665222\n",
      "cnt: 0 - valLoss: 0.6654233932495117 - trainLoss: 0.6638321280479431\n",
      "cnt: 0 - valLoss: 0.6654230952262878 - trainLoss: 0.663831889629364\n",
      "cnt: 0 - valLoss: 0.665422797203064 - trainLoss: 0.6638315916061401\n",
      "cnt: 0 - valLoss: 0.6654224395751953 - trainLoss: 0.663831353187561\n",
      "cnt: 0 - valLoss: 0.6654222011566162 - trainLoss: 0.6638310551643372\n",
      "cnt: 0 - valLoss: 0.6654218435287476 - trainLoss: 0.6638308167457581\n",
      "cnt: 0 - valLoss: 0.6654215455055237 - trainLoss: 0.663830578327179\n",
      "cnt: 0 - valLoss: 0.6654212474822998 - trainLoss: 0.6638303399085999\n",
      "cnt: 0 - valLoss: 0.6654209494590759 - trainLoss: 0.663830041885376\n",
      "cnt: 0 - valLoss: 0.6654205918312073 - trainLoss: 0.6638298034667969\n",
      "cnt: 0 - valLoss: 0.6654202938079834 - trainLoss: 0.663829505443573\n",
      "cnt: 0 - valLoss: 0.6654199957847595 - trainLoss: 0.6638292670249939\n",
      "cnt: 0 - valLoss: 0.6654196977615356 - trainLoss: 0.66382896900177\n",
      "cnt: 0 - valLoss: 0.6654193997383118 - trainLoss: 0.6638287305831909\n",
      "cnt: 0 - valLoss: 0.6654191017150879 - trainLoss: 0.663828432559967\n",
      "cnt: 0 - valLoss: 0.665418803691864 - trainLoss: 0.6638281941413879\n",
      "cnt: 0 - valLoss: 0.6654184460639954 - trainLoss: 0.6638279557228088\n",
      "cnt: 0 - valLoss: 0.6654181480407715 - trainLoss: 0.6638277173042297\n",
      "cnt: 0 - valLoss: 0.6654178500175476 - trainLoss: 0.6638274192810059\n",
      "cnt: 0 - valLoss: 0.6654175519943237 - trainLoss: 0.6638271808624268\n",
      "cnt: 0 - valLoss: 0.6654171943664551 - trainLoss: 0.6638269424438477\n",
      "cnt: 0 - valLoss: 0.665416955947876 - trainLoss: 0.6638266444206238\n",
      "cnt: 0 - valLoss: 0.6654165983200073 - trainLoss: 0.6638264060020447\n",
      "cnt: 0 - valLoss: 0.6654163002967834 - trainLoss: 0.6638261675834656\n",
      "cnt: 0 - valLoss: 0.6654160022735596 - trainLoss: 0.6638258695602417\n",
      "cnt: 0 - valLoss: 0.6654157042503357 - trainLoss: 0.6638256311416626\n",
      "cnt: 0 - valLoss: 0.6654154062271118 - trainLoss: 0.6638253331184387\n",
      "cnt: 0 - valLoss: 0.6654150485992432 - trainLoss: 0.6638250946998596\n",
      "cnt: 0 - valLoss: 0.6654147505760193 - trainLoss: 0.6638247966766357\n",
      "cnt: 0 - valLoss: 0.6654144525527954 - trainLoss: 0.6638246178627014\n",
      "cnt: 0 - valLoss: 0.6654140949249268 - trainLoss: 0.6638243794441223\n",
      "cnt: 0 - valLoss: 0.6654137969017029 - trainLoss: 0.6638240814208984\n",
      "cnt: 0 - valLoss: 0.6654134392738342 - trainLoss: 0.6638237833976746\n",
      "cnt: 0 - valLoss: 0.6654132008552551 - trainLoss: 0.6638235449790955\n",
      "cnt: 0 - valLoss: 0.6654128432273865 - trainLoss: 0.6638233065605164\n",
      "cnt: 0 - valLoss: 0.6654125452041626 - trainLoss: 0.6638230681419373\n",
      "cnt: 0 - valLoss: 0.6654122471809387 - trainLoss: 0.6638228297233582\n",
      "cnt: 0 - valLoss: 0.6654119491577148 - trainLoss: 0.6638225317001343\n",
      "cnt: 0 - valLoss: 0.6654115915298462 - trainLoss: 0.6638222336769104\n",
      "cnt: 0 - valLoss: 0.6654112935066223 - trainLoss: 0.6638219356536865\n",
      "cnt: 0 - valLoss: 0.6654109954833984 - trainLoss: 0.6638216972351074\n",
      "cnt: 0 - valLoss: 0.6654106974601746 - trainLoss: 0.6638214588165283\n",
      "cnt: 0 - valLoss: 0.6654103994369507 - trainLoss: 0.6638211607933044\n",
      "cnt: 0 - valLoss: 0.6654101014137268 - trainLoss: 0.6638209223747253\n",
      "cnt: 0 - valLoss: 0.6654098033905029 - trainLoss: 0.6638206839561462\n",
      "cnt: 0 - valLoss: 0.6654094457626343 - trainLoss: 0.6638203859329224\n",
      "cnt: 0 - valLoss: 0.6654091477394104 - trainLoss: 0.6638201475143433\n",
      "cnt: 0 - valLoss: 0.6654087901115417 - trainLoss: 0.6638199687004089\n",
      "cnt: 0 - valLoss: 0.6654085516929626 - trainLoss: 0.6638196706771851\n",
      "cnt: 0 - valLoss: 0.665408194065094 - trainLoss: 0.6638193726539612\n",
      "cnt: 0 - valLoss: 0.6654078960418701 - trainLoss: 0.6638191342353821\n",
      "cnt: 0 - valLoss: 0.6654075980186462 - trainLoss: 0.6638188362121582\n",
      "cnt: 0 - valLoss: 0.6654072999954224 - trainLoss: 0.6638185977935791\n",
      "cnt: 0 - valLoss: 0.6654069423675537 - trainLoss: 0.6638182997703552\n",
      "cnt: 0 - valLoss: 0.6654067039489746 - trainLoss: 0.6638180613517761\n",
      "cnt: 0 - valLoss: 0.6654062867164612 - trainLoss: 0.663817822933197\n",
      "cnt: 0 - valLoss: 0.6654060482978821 - trainLoss: 0.6638175249099731\n",
      "cnt: 0 - valLoss: 0.6654056906700134 - trainLoss: 0.6638172268867493\n",
      "cnt: 0 - valLoss: 0.6654053330421448 - trainLoss: 0.6638169884681702\n",
      "cnt: 0 - valLoss: 0.6654050946235657 - trainLoss: 0.6638167500495911\n",
      "cnt: 0 - valLoss: 0.665404736995697 - trainLoss: 0.6638164520263672\n",
      "cnt: 0 - valLoss: 0.6654044389724731 - trainLoss: 0.6638162136077881\n",
      "cnt: 0 - valLoss: 0.6654041409492493 - trainLoss: 0.663815975189209\n",
      "cnt: 0 - valLoss: 0.6654038429260254 - trainLoss: 0.6638156771659851\n",
      "cnt: 0 - valLoss: 0.6654035449028015 - trainLoss: 0.6638153791427612\n",
      "cnt: 0 - valLoss: 0.6654031872749329 - trainLoss: 0.6638152003288269\n",
      "cnt: 0 - valLoss: 0.665402889251709 - trainLoss: 0.663814902305603\n",
      "cnt: 0 - valLoss: 0.6654025316238403 - trainLoss: 0.6638146638870239\n",
      "cnt: 0 - valLoss: 0.6654022336006165 - trainLoss: 0.6638143658638\n",
      "cnt: 0 - valLoss: 0.6654019355773926 - trainLoss: 0.663814127445221\n",
      "cnt: 0 - valLoss: 0.6654016375541687 - trainLoss: 0.6638138890266418\n",
      "cnt: 0 - valLoss: 0.6654012203216553 - trainLoss: 0.663813591003418\n",
      "cnt: 0 - valLoss: 0.6654009222984314 - trainLoss: 0.6638132929801941\n",
      "cnt: 0 - valLoss: 0.6654006242752075 - trainLoss: 0.663813054561615\n",
      "cnt: 0 - valLoss: 0.6654003262519836 - trainLoss: 0.6638128161430359\n",
      "cnt: 0 - valLoss: 0.6654000282287598 - trainLoss: 0.663812518119812\n",
      "cnt: 0 - valLoss: 0.6653996706008911 - trainLoss: 0.6638122797012329\n",
      "cnt: 0 - valLoss: 0.6653993725776672 - trainLoss: 0.6638120412826538\n",
      "cnt: 0 - valLoss: 0.6653990149497986 - trainLoss: 0.6638117432594299\n",
      "cnt: 0 - valLoss: 0.6653987765312195 - trainLoss: 0.663811445236206\n",
      "cnt: 0 - valLoss: 0.6653984189033508 - trainLoss: 0.6638112664222717\n",
      "cnt: 0 - valLoss: 0.6653980612754822 - trainLoss: 0.6638109683990479\n",
      "cnt: 0 - valLoss: 0.6653977632522583 - trainLoss: 0.663810670375824\n",
      "cnt: 0 - valLoss: 0.6653974056243896 - trainLoss: 0.6638104319572449\n",
      "cnt: 0 - valLoss: 0.6653971076011658 - trainLoss: 0.6638101935386658\n",
      "cnt: 0 - valLoss: 0.6653968095779419 - trainLoss: 0.6638098955154419\n",
      "cnt: 0 - valLoss: 0.6653964519500732 - trainLoss: 0.6638096570968628\n",
      "cnt: 0 - valLoss: 0.6653962135314941 - trainLoss: 0.6638093590736389\n",
      "cnt: 0 - valLoss: 0.6653958559036255 - trainLoss: 0.6638091802597046\n",
      "cnt: 0 - valLoss: 0.6653954982757568 - trainLoss: 0.6638088822364807\n",
      "cnt: 0 - valLoss: 0.665395200252533 - trainLoss: 0.6638085842132568\n",
      "cnt: 0 - valLoss: 0.6653949022293091 - trainLoss: 0.6638083457946777\n",
      "cnt: 0 - valLoss: 0.6653945446014404 - trainLoss: 0.6638081073760986\n",
      "cnt: 0 - valLoss: 0.6653941869735718 - trainLoss: 0.6638078093528748\n",
      "cnt: 0 - valLoss: 0.6653938889503479 - trainLoss: 0.6638075709342957\n",
      "cnt: 0 - valLoss: 0.665393590927124 - trainLoss: 0.6638072729110718\n",
      "cnt: 0 - valLoss: 0.6653932929039001 - trainLoss: 0.6638070344924927\n",
      "cnt: 0 - valLoss: 0.6653929352760315 - trainLoss: 0.6638067960739136\n",
      "cnt: 0 - valLoss: 0.6653926968574524 - trainLoss: 0.6638065576553345\n",
      "cnt: 0 - valLoss: 0.6653923392295837 - trainLoss: 0.6638062596321106\n",
      "cnt: 0 - valLoss: 0.6653919816017151 - trainLoss: 0.6638059616088867\n",
      "cnt: 0 - valLoss: 0.6653916835784912 - trainLoss: 0.6638056635856628\n",
      "cnt: 0 - valLoss: 0.6653913259506226 - trainLoss: 0.6638054251670837\n",
      "cnt: 0 - valLoss: 0.6653910279273987 - trainLoss: 0.6638051867485046\n",
      "cnt: 0 - valLoss: 0.6653907299041748 - trainLoss: 0.6638049483299255\n",
      "cnt: 0 - valLoss: 0.6653903722763062 - trainLoss: 0.6638046503067017\n",
      "cnt: 0 - valLoss: 0.6653900742530823 - trainLoss: 0.6638044118881226\n",
      "cnt: 0 - valLoss: 0.6653897762298584 - trainLoss: 0.6638041734695435\n",
      "cnt: 0 - valLoss: 0.6653894186019897 - trainLoss: 0.6638038754463196\n",
      "cnt: 0 - valLoss: 0.6653890609741211 - trainLoss: 0.6638035774230957\n",
      "cnt: 0 - valLoss: 0.6653887629508972 - trainLoss: 0.6638033986091614\n",
      "cnt: 0 - valLoss: 0.6653884649276733 - trainLoss: 0.6638031005859375\n",
      "cnt: 0 - valLoss: 0.6653881072998047 - trainLoss: 0.6638028621673584\n",
      "cnt: 0 - valLoss: 0.6653878092765808 - trainLoss: 0.6638025641441345\n",
      "cnt: 0 - valLoss: 0.6653875112533569 - trainLoss: 0.6638022661209106\n",
      "cnt: 0 - valLoss: 0.6653872132301331 - trainLoss: 0.6638020277023315\n",
      "cnt: 0 - valLoss: 0.6653868556022644 - trainLoss: 0.6638017296791077\n",
      "cnt: 0 - valLoss: 0.6653864979743958 - trainLoss: 0.6638014912605286\n",
      "cnt: 0 - valLoss: 0.6653861999511719 - trainLoss: 0.6638012528419495\n",
      "cnt: 0 - valLoss: 0.665385901927948 - trainLoss: 0.6638009548187256\n",
      "cnt: 0 - valLoss: 0.6653855443000793 - trainLoss: 0.6638007164001465\n",
      "cnt: 0 - valLoss: 0.6653851866722107 - trainLoss: 0.6638004779815674\n",
      "cnt: 0 - valLoss: 0.6653849482536316 - trainLoss: 0.6638001799583435\n",
      "cnt: 0 - valLoss: 0.6653845906257629 - trainLoss: 0.6637999415397644\n",
      "cnt: 0 - valLoss: 0.6653842329978943 - trainLoss: 0.6637995839118958\n",
      "cnt: 0 - valLoss: 0.6653839349746704 - trainLoss: 0.6637994050979614\n",
      "cnt: 0 - valLoss: 0.6653835773468018 - trainLoss: 0.6637991070747375\n",
      "cnt: 0 - valLoss: 0.6653832793235779 - trainLoss: 0.6637988686561584\n",
      "cnt: 0 - valLoss: 0.6653829216957092 - trainLoss: 0.6637985706329346\n",
      "cnt: 0 - valLoss: 0.6653826236724854 - trainLoss: 0.6637983322143555\n",
      "cnt: 0 - valLoss: 0.6653823256492615 - trainLoss: 0.6637980341911316\n",
      "cnt: 0 - valLoss: 0.6653819680213928 - trainLoss: 0.6637977957725525\n",
      "cnt: 0 - valLoss: 0.665381669998169 - trainLoss: 0.6637974977493286\n",
      "cnt: 0 - valLoss: 0.6653813123703003 - trainLoss: 0.6637971997261047\n",
      "cnt: 0 - valLoss: 0.6653810143470764 - trainLoss: 0.6637969613075256\n",
      "cnt: 0 - valLoss: 0.6653806567192078 - trainLoss: 0.6637967228889465\n",
      "cnt: 0 - valLoss: 0.6653803586959839 - trainLoss: 0.6637964248657227\n",
      "cnt: 0 - valLoss: 0.66538006067276 - trainLoss: 0.6637961864471436\n",
      "cnt: 0 - valLoss: 0.6653797030448914 - trainLoss: 0.6637959480285645\n",
      "cnt: 0 - valLoss: 0.6653794050216675 - trainLoss: 0.6637956500053406\n",
      "cnt: 0 - valLoss: 0.6653790473937988 - trainLoss: 0.6637953519821167\n",
      "cnt: 0 - valLoss: 0.6653786897659302 - trainLoss: 0.6637951135635376\n",
      "cnt: 0 - valLoss: 0.6653784513473511 - trainLoss: 0.6637948155403137\n",
      "cnt: 0 - valLoss: 0.6653780937194824 - trainLoss: 0.6637945771217346\n",
      "cnt: 0 - valLoss: 0.6653777360916138 - trainLoss: 0.6637943387031555\n",
      "cnt: 0 - valLoss: 0.6653774380683899 - trainLoss: 0.6637941002845764\n",
      "cnt: 0 - valLoss: 0.665377140045166 - trainLoss: 0.6637938022613525\n",
      "cnt: 0 - valLoss: 0.6653767228126526 - trainLoss: 0.6637935638427734\n",
      "cnt: 0 - valLoss: 0.6653764843940735 - trainLoss: 0.6637932658195496\n",
      "cnt: 0 - valLoss: 0.6653761267662048 - trainLoss: 0.6637929677963257\n",
      "cnt: 0 - valLoss: 0.6653758883476257 - trainLoss: 0.6637927293777466\n",
      "cnt: 0 - valLoss: 0.6653755307197571 - trainLoss: 0.6637924909591675\n",
      "cnt: 0 - valLoss: 0.6653751134872437 - trainLoss: 0.6637921929359436\n",
      "cnt: 0 - valLoss: 0.6653748154640198 - trainLoss: 0.6637918949127197\n",
      "cnt: 0 - valLoss: 0.6653745174407959 - trainLoss: 0.6637916564941406\n",
      "cnt: 0 - valLoss: 0.665374219417572 - trainLoss: 0.6637914180755615\n",
      "cnt: 0 - valLoss: 0.6653739213943481 - trainLoss: 0.6637911200523376\n",
      "cnt: 0 - valLoss: 0.6653735041618347 - trainLoss: 0.6637908220291138\n",
      "cnt: 0 - valLoss: 0.6653732061386108 - trainLoss: 0.6637905836105347\n",
      "cnt: 0 - valLoss: 0.665372908115387 - trainLoss: 0.6637903451919556\n",
      "cnt: 0 - valLoss: 0.6653725504875183 - trainLoss: 0.6637900471687317\n",
      "cnt: 0 - valLoss: 0.6653722524642944 - trainLoss: 0.6637898087501526\n",
      "cnt: 0 - valLoss: 0.6653718948364258 - trainLoss: 0.6637895107269287\n",
      "cnt: 0 - valLoss: 0.6653715372085571 - trainLoss: 0.6637892723083496\n",
      "cnt: 0 - valLoss: 0.6653712391853333 - trainLoss: 0.6637889742851257\n",
      "cnt: 0 - valLoss: 0.6653708815574646 - trainLoss: 0.6637887358665466\n",
      "cnt: 0 - valLoss: 0.6653705835342407 - trainLoss: 0.6637884974479675\n",
      "cnt: 0 - valLoss: 0.6653702855110168 - trainLoss: 0.6637881994247437\n",
      "cnt: 0 - valLoss: 0.6653699278831482 - trainLoss: 0.6637879610061646\n",
      "cnt: 0 - valLoss: 0.6653696298599243 - trainLoss: 0.6637876033782959\n",
      "cnt: 0 - valLoss: 0.6653693318367004 - trainLoss: 0.6637873649597168\n",
      "cnt: 0 - valLoss: 0.665368914604187 - trainLoss: 0.6637871861457825\n",
      "cnt: 0 - valLoss: 0.6653686165809631 - trainLoss: 0.6637868285179138\n",
      "cnt: 0 - valLoss: 0.6653683185577393 - trainLoss: 0.6637865304946899\n",
      "cnt: 0 - valLoss: 0.6653679609298706 - trainLoss: 0.6637862324714661\n",
      "cnt: 0 - valLoss: 0.6653676629066467 - trainLoss: 0.6637861132621765\n",
      "cnt: 0 - valLoss: 0.6653673052787781 - trainLoss: 0.6637857556343079\n",
      "cnt: 0 - valLoss: 0.6653670072555542 - trainLoss: 0.6637855172157288\n",
      "cnt: 0 - valLoss: 0.6653667092323303 - trainLoss: 0.6637852191925049\n",
      "cnt: 0 - valLoss: 0.6653663516044617 - trainLoss: 0.6637849807739258\n",
      "cnt: 0 - valLoss: 0.665365993976593 - trainLoss: 0.6637847423553467\n",
      "cnt: 0 - valLoss: 0.6653656959533691 - trainLoss: 0.6637844443321228\n",
      "cnt: 0 - valLoss: 0.6653653979301453 - trainLoss: 0.6637842059135437\n",
      "cnt: 0 - valLoss: 0.6653650403022766 - trainLoss: 0.6637839078903198\n",
      "cnt: 0 - valLoss: 0.6653647422790527 - trainLoss: 0.663783609867096\n",
      "cnt: 0 - valLoss: 0.6653643250465393 - trainLoss: 0.6637833714485168\n",
      "cnt: 0 - valLoss: 0.6653640866279602 - trainLoss: 0.663783073425293\n",
      "cnt: 0 - valLoss: 0.6653637290000916 - trainLoss: 0.6637828350067139\n",
      "cnt: 0 - valLoss: 0.6653633713722229 - trainLoss: 0.6637825965881348\n",
      "cnt: 0 - valLoss: 0.665363073348999 - trainLoss: 0.6637822985649109\n",
      "cnt: 0 - valLoss: 0.6653627753257751 - trainLoss: 0.663782000541687\n",
      "cnt: 0 - valLoss: 0.6653624176979065 - trainLoss: 0.6637817025184631\n",
      "cnt: 0 - valLoss: 0.6653620600700378 - trainLoss: 0.663781464099884\n",
      "cnt: 0 - valLoss: 0.6653617024421692 - trainLoss: 0.6637812256813049\n",
      "cnt: 0 - valLoss: 0.6653614640235901 - trainLoss: 0.663780927658081\n",
      "cnt: 0 - valLoss: 0.6653611063957214 - trainLoss: 0.6637806296348572\n",
      "cnt: 0 - valLoss: 0.6653608083724976 - trainLoss: 0.6637804508209229\n",
      "cnt: 0 - valLoss: 0.6653605103492737 - trainLoss: 0.663780152797699\n",
      "cnt: 0 - valLoss: 0.6653600931167603 - trainLoss: 0.6637798547744751\n",
      "cnt: 0 - valLoss: 0.6653597950935364 - trainLoss: 0.6637795567512512\n",
      "cnt: 0 - valLoss: 0.6653594374656677 - trainLoss: 0.6637792587280273\n",
      "cnt: 0 - valLoss: 0.6653591394424438 - trainLoss: 0.6637790203094482\n",
      "cnt: 0 - valLoss: 0.6653587818145752 - trainLoss: 0.6637787818908691\n",
      "cnt: 0 - valLoss: 0.6653584837913513 - trainLoss: 0.6637784838676453\n",
      "cnt: 0 - valLoss: 0.6653581857681274 - trainLoss: 0.6637782454490662\n",
      "cnt: 0 - valLoss: 0.6653578281402588 - trainLoss: 0.6637779474258423\n",
      "cnt: 0 - valLoss: 0.6653574705123901 - trainLoss: 0.6637777090072632\n",
      "cnt: 0 - valLoss: 0.6653571128845215 - trainLoss: 0.6637774705886841\n",
      "cnt: 0 - valLoss: 0.6653568148612976 - trainLoss: 0.6637771725654602\n",
      "cnt: 0 - valLoss: 0.6653565168380737 - trainLoss: 0.6637768149375916\n",
      "cnt: 0 - valLoss: 0.6653562188148499 - trainLoss: 0.6637765765190125\n",
      "cnt: 0 - valLoss: 0.6653558611869812 - trainLoss: 0.6637763977050781\n",
      "cnt: 0 - valLoss: 0.6653555631637573 - trainLoss: 0.6637760996818542\n",
      "cnt: 0 - valLoss: 0.6653551459312439 - trainLoss: 0.6637758016586304\n",
      "cnt: 0 - valLoss: 0.66535484790802 - trainLoss: 0.6637755632400513\n",
      "cnt: 0 - valLoss: 0.6653545498847961 - trainLoss: 0.6637753248214722\n",
      "cnt: 0 - valLoss: 0.6653541922569275 - trainLoss: 0.6637749671936035\n",
      "cnt: 0 - valLoss: 0.6653538942337036 - trainLoss: 0.6637747287750244\n",
      "cnt: 0 - valLoss: 0.6653534770011902 - trainLoss: 0.6637744903564453\n",
      "cnt: 0 - valLoss: 0.6653532385826111 - trainLoss: 0.6637741923332214\n",
      "cnt: 0 - valLoss: 0.6653528809547424 - trainLoss: 0.6637739539146423\n",
      "cnt: 0 - valLoss: 0.6653525829315186 - trainLoss: 0.6637736558914185\n",
      "cnt: 0 - valLoss: 0.6653522253036499 - trainLoss: 0.6637734174728394\n",
      "cnt: 0 - valLoss: 0.6653518676757812 - trainLoss: 0.6637731194496155\n",
      "cnt: 0 - valLoss: 0.6653515100479126 - trainLoss: 0.6637728810310364\n",
      "cnt: 0 - valLoss: 0.6653512120246887 - trainLoss: 0.6637725234031677\n",
      "cnt: 0 - valLoss: 0.6653508543968201 - trainLoss: 0.6637722849845886\n",
      "cnt: 0 - valLoss: 0.6653505563735962 - trainLoss: 0.6637720465660095\n",
      "cnt: 0 - valLoss: 0.6653501391410828 - trainLoss: 0.6637718081474304\n",
      "cnt: 0 - valLoss: 0.6653498411178589 - trainLoss: 0.6637714505195618\n",
      "cnt: 0 - valLoss: 0.665349543094635 - trainLoss: 0.6637712121009827\n",
      "cnt: 0 - valLoss: 0.6653491854667664 - trainLoss: 0.6637709140777588\n",
      "cnt: 0 - valLoss: 0.6653488278388977 - trainLoss: 0.6637706756591797\n",
      "cnt: 0 - valLoss: 0.6653485298156738 - trainLoss: 0.6637704372406006\n",
      "cnt: 0 - valLoss: 0.6653482913970947 - trainLoss: 0.6637701392173767\n",
      "cnt: 0 - valLoss: 0.6653479337692261 - trainLoss: 0.6637699007987976\n",
      "cnt: 0 - valLoss: 0.6653475165367126 - trainLoss: 0.6637696623802185\n",
      "cnt: 0 - valLoss: 0.6653472185134888 - trainLoss: 0.6637693047523499\n",
      "cnt: 0 - valLoss: 0.6653469204902649 - trainLoss: 0.6637690663337708\n",
      "cnt: 0 - valLoss: 0.6653465032577515 - trainLoss: 0.6637687683105469\n",
      "cnt: 0 - valLoss: 0.6653462052345276 - trainLoss: 0.6637685298919678\n",
      "cnt: 0 - valLoss: 0.6653458476066589 - trainLoss: 0.6637682914733887\n",
      "cnt: 0 - valLoss: 0.6653455495834351 - trainLoss: 0.6637679934501648\n",
      "cnt: 0 - valLoss: 0.6653451919555664 - trainLoss: 0.6637676954269409\n",
      "cnt: 0 - valLoss: 0.6653448939323425 - trainLoss: 0.663767397403717\n",
      "cnt: 0 - valLoss: 0.6653444766998291 - trainLoss: 0.6637671589851379\n",
      "cnt: 0 - valLoss: 0.66534423828125 - trainLoss: 0.6637669205665588\n",
      "cnt: 0 - valLoss: 0.6653438806533813 - trainLoss: 0.6637666821479797\n",
      "cnt: 0 - valLoss: 0.6653435230255127 - trainLoss: 0.6637663841247559\n",
      "cnt: 0 - valLoss: 0.6653432250022888 - trainLoss: 0.663766086101532\n",
      "cnt: 0 - valLoss: 0.6653428673744202 - trainLoss: 0.6637657880783081\n",
      "cnt: 0 - valLoss: 0.6653425097465515 - trainLoss: 0.663765549659729\n",
      "cnt: 0 - valLoss: 0.6653421521186829 - trainLoss: 0.6637653112411499\n",
      "cnt: 0 - valLoss: 0.665341854095459 - trainLoss: 0.663765013217926\n",
      "cnt: 0 - valLoss: 0.6653415560722351 - trainLoss: 0.6637647151947021\n",
      "cnt: 0 - valLoss: 0.6653411984443665 - trainLoss: 0.6637644171714783\n",
      "cnt: 0 - valLoss: 0.6653408408164978 - trainLoss: 0.6637641787528992\n",
      "cnt: 0 - valLoss: 0.6653404831886292 - trainLoss: 0.6637639403343201\n",
      "cnt: 0 - valLoss: 0.6653401851654053 - trainLoss: 0.6637636423110962\n",
      "cnt: 0 - valLoss: 0.6653398871421814 - trainLoss: 0.6637634038925171\n",
      "cnt: 0 - valLoss: 0.6653395295143127 - trainLoss: 0.6637630462646484\n",
      "cnt: 0 - valLoss: 0.6653391718864441 - trainLoss: 0.6637628078460693\n",
      "cnt: 0 - valLoss: 0.665338933467865 - trainLoss: 0.6637625694274902\n",
      "cnt: 0 - valLoss: 0.6653385162353516 - trainLoss: 0.6637622714042664\n",
      "cnt: 0 - valLoss: 0.6653382182121277 - trainLoss: 0.6637619733810425\n",
      "cnt: 0 - valLoss: 0.6653379201889038 - trainLoss: 0.6637616753578186\n",
      "cnt: 0 - valLoss: 0.6653376221656799 - trainLoss: 0.6637614369392395\n",
      "cnt: 0 - valLoss: 0.6653372645378113 - trainLoss: 0.6637611985206604\n",
      "cnt: 0 - valLoss: 0.6653369069099426 - trainLoss: 0.6637609004974365\n",
      "cnt: 0 - valLoss: 0.6653366088867188 - trainLoss: 0.6637606024742126\n",
      "cnt: 0 - valLoss: 0.6653362512588501 - trainLoss: 0.6637603640556335\n",
      "cnt: 0 - valLoss: 0.6653358936309814 - trainLoss: 0.6637601256370544\n",
      "cnt: 0 - valLoss: 0.6653355956077576 - trainLoss: 0.6637598276138306\n",
      "cnt: 0 - valLoss: 0.6653352975845337 - trainLoss: 0.6637595295906067\n",
      "cnt: 0 - valLoss: 0.6653349995613098 - trainLoss: 0.6637592911720276\n",
      "cnt: 0 - valLoss: 0.6653345823287964 - trainLoss: 0.6637589335441589\n",
      "cnt: 0 - valLoss: 0.6653342843055725 - trainLoss: 0.6637587547302246\n",
      "cnt: 0 - valLoss: 0.6653339266777039 - trainLoss: 0.6637585163116455\n",
      "cnt: 0 - valLoss: 0.66533362865448 - trainLoss: 0.6637581586837769\n",
      "cnt: 0 - valLoss: 0.6653332710266113 - trainLoss: 0.6637579202651978\n",
      "cnt: 0 - valLoss: 0.6653329730033875 - trainLoss: 0.6637576222419739\n",
      "cnt: 0 - valLoss: 0.6653326749801636 - trainLoss: 0.66375732421875\n",
      "cnt: 0 - valLoss: 0.6653323173522949 - trainLoss: 0.6637570858001709\n",
      "cnt: 0 - valLoss: 0.6653319597244263 - trainLoss: 0.663756787776947\n",
      "cnt: 0 - valLoss: 0.6653316617012024 - trainLoss: 0.6637564897537231\n",
      "cnt: 0 - valLoss: 0.6653313040733337 - trainLoss: 0.663756251335144\n",
      "cnt: 0 - valLoss: 0.6653310060501099 - trainLoss: 0.6637560129165649\n",
      "cnt: 0 - valLoss: 0.6653305888175964 - trainLoss: 0.6637557148933411\n",
      "cnt: 0 - valLoss: 0.6653302907943726 - trainLoss: 0.6637554168701172\n",
      "cnt: 0 - valLoss: 0.6653299927711487 - trainLoss: 0.6637551784515381\n",
      "cnt: 0 - valLoss: 0.66532963514328 - trainLoss: 0.6637548804283142\n",
      "cnt: 0 - valLoss: 0.6653293371200562 - trainLoss: 0.6637545824050903\n",
      "cnt: 0 - valLoss: 0.6653290390968323 - trainLoss: 0.6637543439865112\n",
      "cnt: 0 - valLoss: 0.6653286814689636 - trainLoss: 0.6637540459632874\n",
      "cnt: 0 - valLoss: 0.665328323841095 - trainLoss: 0.6637537479400635\n",
      "cnt: 0 - valLoss: 0.6653280258178711 - trainLoss: 0.6637535095214844\n",
      "cnt: 0 - valLoss: 0.6653276681900024 - trainLoss: 0.6637532711029053\n",
      "cnt: 0 - valLoss: 0.6653273701667786 - trainLoss: 0.6637529730796814\n",
      "cnt: 0 - valLoss: 0.6653270721435547 - trainLoss: 0.6637526750564575\n",
      "cnt: 0 - valLoss: 0.6653266549110413 - trainLoss: 0.6637524366378784\n",
      "cnt: 0 - valLoss: 0.6653263568878174 - trainLoss: 0.6637521982192993\n",
      "cnt: 0 - valLoss: 0.6653259992599487 - trainLoss: 0.6637518405914307\n",
      "cnt: 0 - valLoss: 0.6653256416320801 - trainLoss: 0.6637516021728516\n",
      "cnt: 0 - valLoss: 0.6653253436088562 - trainLoss: 0.6637513041496277\n",
      "cnt: 0 - valLoss: 0.6653249859809875 - trainLoss: 0.6637510061264038\n",
      "cnt: 0 - valLoss: 0.6653246879577637 - trainLoss: 0.6637507081031799\n",
      "cnt: 0 - valLoss: 0.6653243899345398 - trainLoss: 0.6637505292892456\n",
      "cnt: 0 - valLoss: 0.6653240323066711 - trainLoss: 0.6637502312660217\n",
      "cnt: 0 - valLoss: 0.6653237342834473 - trainLoss: 0.6637499332427979\n",
      "cnt: 0 - valLoss: 0.6653233766555786 - trainLoss: 0.663749635219574\n",
      "cnt: 0 - valLoss: 0.66532301902771 - trainLoss: 0.6637493371963501\n",
      "cnt: 0 - valLoss: 0.6653226613998413 - trainLoss: 0.663749098777771\n",
      "cnt: 0 - valLoss: 0.6653223633766174 - trainLoss: 0.6637488603591919\n",
      "cnt: 0 - valLoss: 0.6653220653533936 - trainLoss: 0.6637486219406128\n",
      "cnt: 0 - valLoss: 0.6653217077255249 - trainLoss: 0.6637483239173889\n",
      "cnt: 0 - valLoss: 0.665321409702301 - trainLoss: 0.6637479662895203\n",
      "cnt: 0 - valLoss: 0.6653209924697876 - trainLoss: 0.6637477278709412\n",
      "cnt: 0 - valLoss: 0.6653206944465637 - trainLoss: 0.6637474894523621\n",
      "cnt: 0 - valLoss: 0.6653203368186951 - trainLoss: 0.663747251033783\n",
      "cnt: 0 - valLoss: 0.6653200387954712 - trainLoss: 0.6637469530105591\n",
      "cnt: 0 - valLoss: 0.6653196215629578 - trainLoss: 0.6637465953826904\n",
      "cnt: 0 - valLoss: 0.6653193831443787 - trainLoss: 0.6637463569641113\n",
      "cnt: 0 - valLoss: 0.66531902551651 - trainLoss: 0.6637461185455322\n",
      "cnt: 0 - valLoss: 0.6653186678886414 - trainLoss: 0.6637458205223083\n",
      "cnt: 0 - valLoss: 0.6653183698654175 - trainLoss: 0.6637455224990845\n",
      "cnt: 0 - valLoss: 0.6653180122375488 - trainLoss: 0.6637452244758606\n",
      "cnt: 0 - valLoss: 0.6653177738189697 - trainLoss: 0.6637449264526367\n",
      "cnt: 0 - valLoss: 0.6653173565864563 - trainLoss: 0.6637446880340576\n",
      "cnt: 0 - valLoss: 0.6653170585632324 - trainLoss: 0.6637443900108337\n",
      "cnt: 0 - valLoss: 0.6653167009353638 - trainLoss: 0.6637441515922546\n",
      "cnt: 0 - valLoss: 0.6653164029121399 - trainLoss: 0.6637439131736755\n",
      "cnt: 0 - valLoss: 0.6653159856796265 - trainLoss: 0.6637435555458069\n",
      "cnt: 0 - valLoss: 0.6653156876564026 - trainLoss: 0.6637433171272278\n",
      "cnt: 0 - valLoss: 0.6653153896331787 - trainLoss: 0.6637430191040039\n",
      "cnt: 0 - valLoss: 0.6653150320053101 - trainLoss: 0.6637427806854248\n",
      "cnt: 0 - valLoss: 0.6653146743774414 - trainLoss: 0.6637424826622009\n",
      "cnt: 0 - valLoss: 0.6653143763542175 - trainLoss: 0.6637422442436218\n",
      "cnt: 0 - valLoss: 0.6653140783309937 - trainLoss: 0.6637418866157532\n",
      "cnt: 0 - valLoss: 0.665313720703125 - trainLoss: 0.6637416481971741\n",
      "cnt: 0 - valLoss: 0.6653133630752563 - trainLoss: 0.6637413501739502\n",
      "cnt: 0 - valLoss: 0.6653130054473877 - trainLoss: 0.6637411117553711\n",
      "cnt: 0 - valLoss: 0.6653127074241638 - trainLoss: 0.6637408137321472\n",
      "cnt: 0 - valLoss: 0.6653123497962952 - trainLoss: 0.6637405753135681\n",
      "cnt: 0 - valLoss: 0.6653119921684265 - trainLoss: 0.6637402772903442\n",
      "cnt: 0 - valLoss: 0.6653116941452026 - trainLoss: 0.6637399792671204\n",
      "cnt: 0 - valLoss: 0.665311336517334 - trainLoss: 0.6637396812438965\n",
      "cnt: 0 - valLoss: 0.6653110384941101 - trainLoss: 0.6637394428253174\n",
      "cnt: 0 - valLoss: 0.6653106808662415 - trainLoss: 0.6637391448020935\n",
      "cnt: 0 - valLoss: 0.6653103232383728 - trainLoss: 0.6637388467788696\n",
      "cnt: 0 - valLoss: 0.6653100252151489 - trainLoss: 0.6637385487556458\n",
      "cnt: 0 - valLoss: 0.6653096675872803 - trainLoss: 0.6637383103370667\n",
      "cnt: 0 - valLoss: 0.6653093099594116 - trainLoss: 0.663737952709198\n",
      "cnt: 0 - valLoss: 0.6653090119361877 - trainLoss: 0.6637377142906189\n",
      "cnt: 0 - valLoss: 0.6653086543083191 - trainLoss: 0.6637374758720398\n",
      "cnt: 0 - valLoss: 0.6653082966804504 - trainLoss: 0.6637371778488159\n",
      "cnt: 0 - valLoss: 0.6653079986572266 - trainLoss: 0.663736879825592\n",
      "cnt: 0 - valLoss: 0.6653076410293579 - trainLoss: 0.6637365818023682\n",
      "cnt: 0 - valLoss: 0.665307343006134 - trainLoss: 0.6637363433837891\n",
      "cnt: 0 - valLoss: 0.6653069257736206 - trainLoss: 0.66373610496521\n",
      "cnt: 0 - valLoss: 0.6653066277503967 - trainLoss: 0.6637358069419861\n",
      "cnt: 0 - valLoss: 0.6653062701225281 - trainLoss: 0.6637355089187622\n",
      "cnt: 0 - valLoss: 0.6653059124946594 - trainLoss: 0.6637352108955383\n",
      "cnt: 0 - valLoss: 0.6653055548667908 - trainLoss: 0.6637349724769592\n",
      "cnt: 0 - valLoss: 0.6653052568435669 - trainLoss: 0.6637346744537354\n",
      "cnt: 0 - valLoss: 0.6653048992156982 - trainLoss: 0.6637344360351562\n",
      "cnt: 0 - valLoss: 0.6653045415878296 - trainLoss: 0.6637341380119324\n",
      "cnt: 0 - valLoss: 0.6653042435646057 - trainLoss: 0.6637338399887085\n",
      "cnt: 0 - valLoss: 0.6653038859367371 - trainLoss: 0.6637336015701294\n",
      "cnt: 0 - valLoss: 0.6653035283088684 - trainLoss: 0.6637333631515503\n",
      "cnt: 0 - valLoss: 0.6653032302856445 - trainLoss: 0.6637330651283264\n",
      "cnt: 0 - valLoss: 0.6653028130531311 - trainLoss: 0.6637327671051025\n",
      "cnt: 0 - valLoss: 0.6653025150299072 - trainLoss: 0.6637325286865234\n",
      "cnt: 0 - valLoss: 0.6653021574020386 - trainLoss: 0.6637321710586548\n",
      "cnt: 0 - valLoss: 0.6653017997741699 - trainLoss: 0.6637319326400757\n",
      "cnt: 0 - valLoss: 0.6653014421463013 - trainLoss: 0.6637316346168518\n",
      "cnt: 0 - valLoss: 0.6653010845184326 - trainLoss: 0.6637313961982727\n",
      "cnt: 0 - valLoss: 0.6653007864952087 - trainLoss: 0.6637310981750488\n",
      "cnt: 0 - valLoss: 0.6653004288673401 - trainLoss: 0.663730800151825\n",
      "cnt: 0 - valLoss: 0.6653000712394714 - trainLoss: 0.6637305021286011\n",
      "cnt: 0 - valLoss: 0.6652997732162476 - trainLoss: 0.663730263710022\n",
      "cnt: 0 - valLoss: 0.6652994155883789 - trainLoss: 0.6637299656867981\n",
      "cnt: 0 - valLoss: 0.6652990579605103 - trainLoss: 0.6637296676635742\n",
      "cnt: 0 - valLoss: 0.6652987003326416 - trainLoss: 0.6637294292449951\n",
      "cnt: 0 - valLoss: 0.6652984023094177 - trainLoss: 0.6637291312217712\n",
      "cnt: 0 - valLoss: 0.6652979850769043 - trainLoss: 0.6637288331985474\n",
      "cnt: 0 - valLoss: 0.6652976870536804 - trainLoss: 0.6637285351753235\n",
      "cnt: 0 - valLoss: 0.6652973294258118 - trainLoss: 0.6637282967567444\n",
      "cnt: 0 - valLoss: 0.6652970314025879 - trainLoss: 0.6637279987335205\n",
      "cnt: 0 - valLoss: 0.6652966737747192 - trainLoss: 0.6637277007102966\n",
      "cnt: 0 - valLoss: 0.6652962565422058 - trainLoss: 0.6637274622917175\n",
      "cnt: 0 - valLoss: 0.6652959585189819 - trainLoss: 0.6637271642684937\n",
      "cnt: 0 - valLoss: 0.6652956008911133 - trainLoss: 0.6637269258499146\n",
      "cnt: 0 - valLoss: 0.6652952432632446 - trainLoss: 0.6637266278266907\n",
      "cnt: 0 - valLoss: 0.6652949452400208 - trainLoss: 0.6637263298034668\n",
      "cnt: 0 - valLoss: 0.6652945876121521 - trainLoss: 0.6637260913848877\n",
      "cnt: 0 - valLoss: 0.6652942299842834 - trainLoss: 0.6637257933616638\n",
      "cnt: 0 - valLoss: 0.6652938723564148 - trainLoss: 0.6637254953384399\n",
      "cnt: 0 - valLoss: 0.6652935743331909 - trainLoss: 0.6637251973152161\n",
      "cnt: 0 - valLoss: 0.6652932167053223 - trainLoss: 0.6637248992919922\n",
      "cnt: 0 - valLoss: 0.6652928590774536 - trainLoss: 0.6637246608734131\n",
      "cnt: 0 - valLoss: 0.665292501449585 - trainLoss: 0.6637243628501892\n",
      "cnt: 0 - valLoss: 0.6652921438217163 - trainLoss: 0.6637241244316101\n",
      "cnt: 0 - valLoss: 0.6652917861938477 - trainLoss: 0.6637238264083862\n",
      "cnt: 0 - valLoss: 0.6652914881706238 - trainLoss: 0.6637235283851624\n",
      "cnt: 0 - valLoss: 0.6652911305427551 - trainLoss: 0.6637232899665833\n",
      "cnt: 0 - valLoss: 0.6652907729148865 - trainLoss: 0.6637229919433594\n",
      "cnt: 0 - valLoss: 0.6652904748916626 - trainLoss: 0.6637226343154907\n",
      "cnt: 0 - valLoss: 0.6652900576591492 - trainLoss: 0.6637223958969116\n",
      "cnt: 0 - valLoss: 0.6652897000312805 - trainLoss: 0.6637221574783325\n",
      "cnt: 0 - valLoss: 0.6652894020080566 - trainLoss: 0.6637218594551086\n",
      "cnt: 0 - valLoss: 0.6652891039848328 - trainLoss: 0.6637215614318848\n",
      "cnt: 0 - valLoss: 0.6652886867523193 - trainLoss: 0.6637213230133057\n",
      "cnt: 0 - valLoss: 0.6652883291244507 - trainLoss: 0.6637210249900818\n",
      "cnt: 0 - valLoss: 0.6652880311012268 - trainLoss: 0.6637206673622131\n",
      "cnt: 0 - valLoss: 0.6652876734733582 - trainLoss: 0.663720428943634\n",
      "cnt: 0 - valLoss: 0.6652873158454895 - trainLoss: 0.6637201905250549\n",
      "cnt: 0 - valLoss: 0.6652869582176208 - trainLoss: 0.663719892501831\n",
      "cnt: 0 - valLoss: 0.665286660194397 - trainLoss: 0.6637195944786072\n",
      "cnt: 0 - valLoss: 0.6652863025665283 - trainLoss: 0.6637192964553833\n",
      "cnt: 0 - valLoss: 0.6652859449386597 - trainLoss: 0.6637189984321594\n",
      "cnt: 0 - valLoss: 0.665285587310791 - trainLoss: 0.6637188196182251\n",
      "cnt: 0 - valLoss: 0.6652852892875671 - trainLoss: 0.6637184619903564\n",
      "cnt: 0 - valLoss: 0.6652848720550537 - trainLoss: 0.6637181639671326\n",
      "cnt: 0 - valLoss: 0.6652845740318298 - trainLoss: 0.6637179255485535\n",
      "cnt: 0 - valLoss: 0.6652841567993164 - trainLoss: 0.6637176275253296\n",
      "cnt: 0 - valLoss: 0.6652838587760925 - trainLoss: 0.6637173295021057\n",
      "cnt: 0 - valLoss: 0.6652834415435791 - trainLoss: 0.6637170910835266\n",
      "cnt: 0 - valLoss: 0.6652831435203552 - trainLoss: 0.6637168526649475\n",
      "cnt: 0 - valLoss: 0.6652827858924866 - trainLoss: 0.6637164950370789\n",
      "cnt: 0 - valLoss: 0.6652824282646179 - trainLoss: 0.663716197013855\n",
      "cnt: 0 - valLoss: 0.6652820706367493 - trainLoss: 0.6637159585952759\n",
      "cnt: 0 - valLoss: 0.6652817130088806 - trainLoss: 0.663715660572052\n",
      "cnt: 0 - valLoss: 0.6652814149856567 - trainLoss: 0.6637153625488281\n",
      "cnt: 0 - valLoss: 0.6652810573577881 - trainLoss: 0.663715124130249\n",
      "cnt: 0 - valLoss: 0.6652806997299194 - trainLoss: 0.6637147665023804\n",
      "cnt: 0 - valLoss: 0.6652803421020508 - trainLoss: 0.6637145280838013\n",
      "cnt: 0 - valLoss: 0.6652799844741821 - trainLoss: 0.6637142300605774\n",
      "cnt: 0 - valLoss: 0.6652796268463135 - trainLoss: 0.6637139916419983\n",
      "cnt: 0 - valLoss: 0.6652793288230896 - trainLoss: 0.6637136936187744\n",
      "cnt: 0 - valLoss: 0.6652789115905762 - trainLoss: 0.6637133359909058\n",
      "cnt: 0 - valLoss: 0.6652785539627075 - trainLoss: 0.6637130379676819\n",
      "cnt: 0 - valLoss: 0.6652782559394836 - trainLoss: 0.6637127995491028\n",
      "cnt: 0 - valLoss: 0.665277898311615 - trainLoss: 0.6637124419212341\n",
      "cnt: 0 - valLoss: 0.6652775406837463 - trainLoss: 0.6637123227119446\n",
      "cnt: 0 - valLoss: 0.6652771830558777 - trainLoss: 0.6637120246887207\n",
      "cnt: 0 - valLoss: 0.6652768850326538 - trainLoss: 0.6637117266654968\n",
      "cnt: 0 - valLoss: 0.6652765274047852 - trainLoss: 0.6637113690376282\n",
      "cnt: 0 - valLoss: 0.6652761697769165 - trainLoss: 0.6637110710144043\n",
      "cnt: 0 - valLoss: 0.6652758121490479 - trainLoss: 0.6637108325958252\n",
      "cnt: 0 - valLoss: 0.6652753949165344 - trainLoss: 0.6637105941772461\n",
      "cnt: 0 - valLoss: 0.6652750372886658 - trainLoss: 0.6637102365493774\n",
      "cnt: 0 - valLoss: 0.6652747392654419 - trainLoss: 0.6637099981307983\n",
      "cnt: 0 - valLoss: 0.665274441242218 - trainLoss: 0.6637097001075745\n",
      "cnt: 0 - valLoss: 0.6652740240097046 - trainLoss: 0.6637094020843506\n",
      "cnt: 0 - valLoss: 0.6652736663818359 - trainLoss: 0.6637091040611267\n",
      "cnt: 0 - valLoss: 0.6652733683586121 - trainLoss: 0.6637088656425476\n",
      "cnt: 0 - valLoss: 0.6652730107307434 - trainLoss: 0.6637085676193237\n",
      "cnt: 0 - valLoss: 0.66527259349823 - trainLoss: 0.6637082695960999\n",
      "cnt: 0 - valLoss: 0.6652722954750061 - trainLoss: 0.663707971572876\n",
      "cnt: 0 - valLoss: 0.6652719974517822 - trainLoss: 0.6637076735496521\n",
      "cnt: 0 - valLoss: 0.6652715802192688 - trainLoss: 0.663707435131073\n",
      "cnt: 0 - valLoss: 0.6652712225914001 - trainLoss: 0.6637071371078491\n",
      "cnt: 0 - valLoss: 0.6652708649635315 - trainLoss: 0.6637068390846252\n",
      "cnt: 0 - valLoss: 0.6652705073356628 - trainLoss: 0.6637065410614014\n",
      "cnt: 0 - valLoss: 0.6652701497077942 - trainLoss: 0.6637063026428223\n",
      "cnt: 0 - valLoss: 0.6652698516845703 - trainLoss: 0.6637060046195984\n",
      "cnt: 0 - valLoss: 0.6652694940567017 - trainLoss: 0.6637057662010193\n",
      "cnt: 0 - valLoss: 0.6652690768241882 - trainLoss: 0.6637054681777954\n",
      "cnt: 0 - valLoss: 0.6652687788009644 - trainLoss: 0.6637051701545715\n",
      "cnt: 0 - valLoss: 0.6652684211730957 - trainLoss: 0.6637048721313477\n",
      "cnt: 0 - valLoss: 0.6652680039405823 - trainLoss: 0.6637045741081238\n",
      "cnt: 0 - valLoss: 0.6652677059173584 - trainLoss: 0.6637042760848999\n",
      "cnt: 0 - valLoss: 0.6652673482894897 - trainLoss: 0.6637040376663208\n",
      "cnt: 0 - valLoss: 0.6652669906616211 - trainLoss: 0.6637037396430969\n",
      "cnt: 0 - valLoss: 0.6652666330337524 - trainLoss: 0.663703441619873\n",
      "cnt: 0 - valLoss: 0.6652662754058838 - trainLoss: 0.6637031435966492\n",
      "cnt: 0 - valLoss: 0.6652659773826599 - trainLoss: 0.6637029051780701\n",
      "cnt: 0 - valLoss: 0.6652655601501465 - trainLoss: 0.6637026071548462\n",
      "cnt: 0 - valLoss: 0.6652652621269226 - trainLoss: 0.6637023091316223\n",
      "cnt: 0 - valLoss: 0.665264904499054 - trainLoss: 0.6637020111083984\n",
      "cnt: 0 - valLoss: 0.6652644872665405 - trainLoss: 0.6637017130851746\n",
      "cnt: 0 - valLoss: 0.6652641296386719 - trainLoss: 0.6637014150619507\n",
      "cnt: 0 - valLoss: 0.6652637720108032 - trainLoss: 0.6637011766433716\n",
      "cnt: 0 - valLoss: 0.6652634143829346 - trainLoss: 0.6637008786201477\n",
      "cnt: 0 - valLoss: 0.6652630567550659 - trainLoss: 0.6637005805969238\n",
      "cnt: 0 - valLoss: 0.6652626991271973 - trainLoss: 0.6637003421783447\n",
      "cnt: 0 - valLoss: 0.6652624011039734 - trainLoss: 0.6636999845504761\n",
      "cnt: 0 - valLoss: 0.6652620434761047 - trainLoss: 0.6636996865272522\n",
      "cnt: 0 - valLoss: 0.6652616858482361 - trainLoss: 0.6636993885040283\n",
      "cnt: 0 - valLoss: 0.6652613282203674 - trainLoss: 0.6636991500854492\n",
      "cnt: 0 - valLoss: 0.6652609705924988 - trainLoss: 0.6636988520622253\n",
      "cnt: 0 - valLoss: 0.6652606129646301 - trainLoss: 0.6636985540390015\n",
      "cnt: 0 - valLoss: 0.6652602553367615 - trainLoss: 0.6636983156204224\n",
      "cnt: 0 - valLoss: 0.6652598977088928 - trainLoss: 0.6636980175971985\n",
      "cnt: 0 - valLoss: 0.665259599685669 - trainLoss: 0.6636976599693298\n",
      "cnt: 0 - valLoss: 0.6652591824531555 - trainLoss: 0.6636974215507507\n",
      "cnt: 0 - valLoss: 0.6652588844299316 - trainLoss: 0.6636971235275269\n",
      "cnt: 0 - valLoss: 0.665258526802063 - trainLoss: 0.663696825504303\n",
      "cnt: 0 - valLoss: 0.6652581691741943 - trainLoss: 0.6636965274810791\n",
      "cnt: 0 - valLoss: 0.6652578115463257 - trainLoss: 0.6636962294578552\n",
      "cnt: 0 - valLoss: 0.6652575135231018 - trainLoss: 0.6636959314346313\n",
      "cnt: 0 - valLoss: 0.6652570366859436 - trainLoss: 0.6636956334114075\n",
      "cnt: 0 - valLoss: 0.6652567386627197 - trainLoss: 0.6636953949928284\n",
      "cnt: 0 - valLoss: 0.6652563810348511 - trainLoss: 0.6636950969696045\n",
      "cnt: 0 - valLoss: 0.6652559638023376 - trainLoss: 0.6636947393417358\n",
      "cnt: 0 - valLoss: 0.6652556657791138 - trainLoss: 0.6636945605278015\n",
      "cnt: 0 - valLoss: 0.6652553677558899 - trainLoss: 0.6636942625045776\n",
      "cnt: 0 - valLoss: 0.6652548909187317 - trainLoss: 0.6636939644813538\n",
      "cnt: 0 - valLoss: 0.6652545928955078 - trainLoss: 0.6636936664581299\n",
      "cnt: 0 - valLoss: 0.6652542352676392 - trainLoss: 0.663693368434906\n",
      "cnt: 0 - valLoss: 0.6652538180351257 - trainLoss: 0.6636930108070374\n",
      "cnt: 0 - valLoss: 0.6652535200119019 - trainLoss: 0.6636927127838135\n",
      "cnt: 0 - valLoss: 0.6652531623840332 - trainLoss: 0.6636924743652344\n",
      "cnt: 0 - valLoss: 0.6652527451515198 - trainLoss: 0.6636921167373657\n",
      "cnt: 0 - valLoss: 0.6652524471282959 - trainLoss: 0.6636918783187866\n",
      "cnt: 0 - valLoss: 0.6652519702911377 - trainLoss: 0.6636916399002075\n",
      "cnt: 0 - valLoss: 0.6652516722679138 - trainLoss: 0.6636913418769836\n",
      "cnt: 0 - valLoss: 0.6652512550354004 - trainLoss: 0.663690984249115\n",
      "cnt: 0 - valLoss: 0.6652509570121765 - trainLoss: 0.6636907458305359\n",
      "cnt: 0 - valLoss: 0.6652505397796631 - trainLoss: 0.663690447807312\n",
      "cnt: 0 - valLoss: 0.6652502417564392 - trainLoss: 0.6636901497840881\n",
      "cnt: 0 - valLoss: 0.6652498841285706 - trainLoss: 0.6636898517608643\n",
      "cnt: 0 - valLoss: 0.6652494668960571 - trainLoss: 0.6636895537376404\n",
      "cnt: 0 - valLoss: 0.6652491092681885 - trainLoss: 0.6636892557144165\n",
      "cnt: 0 - valLoss: 0.6652487516403198 - trainLoss: 0.6636888980865479\n",
      "cnt: 0 - valLoss: 0.6652483940124512 - trainLoss: 0.6636887192726135\n",
      "cnt: 0 - valLoss: 0.6652480363845825 - trainLoss: 0.6636883616447449\n",
      "cnt: 0 - valLoss: 0.6652476191520691 - trainLoss: 0.663688063621521\n",
      "cnt: 0 - valLoss: 0.6652472615242004 - trainLoss: 0.6636878252029419\n",
      "cnt: 0 - valLoss: 0.6652469635009766 - trainLoss: 0.663687527179718\n",
      "cnt: 0 - valLoss: 0.6652466058731079 - trainLoss: 0.6636872291564941\n",
      "cnt: 0 - valLoss: 0.6652461886405945 - trainLoss: 0.6636868715286255\n",
      "cnt: 0 - valLoss: 0.6652458310127258 - trainLoss: 0.6636866331100464\n",
      "cnt: 0 - valLoss: 0.6652454733848572 - trainLoss: 0.6636863350868225\n",
      "cnt: 0 - valLoss: 0.6652451157569885 - trainLoss: 0.6636860370635986\n",
      "cnt: 0 - valLoss: 0.6652447581291199 - trainLoss: 0.6636857390403748\n",
      "cnt: 0 - valLoss: 0.6652444005012512 - trainLoss: 0.6636854410171509\n",
      "cnt: 0 - valLoss: 0.6652440428733826 - trainLoss: 0.663685142993927\n",
      "cnt: 0 - valLoss: 0.6652436256408691 - trainLoss: 0.6636848449707031\n",
      "cnt: 0 - valLoss: 0.6652432680130005 - trainLoss: 0.6636845469474792\n",
      "cnt: 0 - valLoss: 0.6652429103851318 - trainLoss: 0.6636842489242554\n",
      "cnt: 0 - valLoss: 0.6652425527572632 - trainLoss: 0.6636839509010315\n",
      "cnt: 0 - valLoss: 0.6652421951293945 - trainLoss: 0.6636837124824524\n",
      "cnt: 0 - valLoss: 0.6652418375015259 - trainLoss: 0.6636834144592285\n",
      "cnt: 0 - valLoss: 0.6652414798736572 - trainLoss: 0.6636831164360046\n",
      "cnt: 0 - valLoss: 0.6652410626411438 - trainLoss: 0.663682758808136\n",
      "cnt: 0 - valLoss: 0.6652407050132751 - trainLoss: 0.6636825203895569\n",
      "cnt: 0 - valLoss: 0.6652404069900513 - trainLoss: 0.663682222366333\n",
      "cnt: 0 - valLoss: 0.6652399897575378 - trainLoss: 0.6636819243431091\n",
      "cnt: 0 - valLoss: 0.6652396321296692 - trainLoss: 0.6636816263198853\n",
      "cnt: 0 - valLoss: 0.6652392148971558 - trainLoss: 0.6636813282966614\n",
      "cnt: 0 - valLoss: 0.6652388572692871 - trainLoss: 0.6636810302734375\n",
      "cnt: 0 - valLoss: 0.6652385592460632 - trainLoss: 0.6636807918548584\n",
      "cnt: 0 - valLoss: 0.6652382016181946 - trainLoss: 0.6636804342269897\n",
      "cnt: 0 - valLoss: 0.6652377843856812 - trainLoss: 0.6636801362037659\n",
      "cnt: 0 - valLoss: 0.6652374267578125 - trainLoss: 0.663679838180542\n",
      "cnt: 0 - valLoss: 0.6652370691299438 - trainLoss: 0.6636795401573181\n",
      "cnt: 0 - valLoss: 0.6652367115020752 - trainLoss: 0.6636791825294495\n",
      "cnt: 0 - valLoss: 0.6652362942695618 - trainLoss: 0.6636790037155151\n",
      "cnt: 0 - valLoss: 0.6652359366416931 - trainLoss: 0.6636786460876465\n",
      "cnt: 0 - valLoss: 0.6652356386184692 - trainLoss: 0.6636783480644226\n",
      "cnt: 0 - valLoss: 0.6652352213859558 - trainLoss: 0.6636780500411987\n",
      "cnt: 0 - valLoss: 0.6652348637580872 - trainLoss: 0.6636777520179749\n",
      "cnt: 0 - valLoss: 0.6652344465255737 - trainLoss: 0.663677453994751\n",
      "cnt: 0 - valLoss: 0.6652341485023499 - trainLoss: 0.6636772155761719\n",
      "cnt: 0 - valLoss: 0.665233850479126 - trainLoss: 0.6636768579483032\n",
      "cnt: 0 - valLoss: 0.6652334332466125 - trainLoss: 0.6636766195297241\n",
      "cnt: 0 - valLoss: 0.6652330756187439 - trainLoss: 0.6636762619018555\n",
      "cnt: 0 - valLoss: 0.6652326583862305 - trainLoss: 0.6636760234832764\n",
      "cnt: 0 - valLoss: 0.6652323007583618 - trainLoss: 0.6636757254600525\n",
      "cnt: 0 - valLoss: 0.6652318835258484 - trainLoss: 0.6636754274368286\n",
      "cnt: 0 - valLoss: 0.6652315855026245 - trainLoss: 0.6636751294136047\n",
      "cnt: 0 - valLoss: 0.6652312278747559 - trainLoss: 0.6636748313903809\n",
      "cnt: 0 - valLoss: 0.6652308106422424 - trainLoss: 0.663674533367157\n",
      "cnt: 0 - valLoss: 0.6652304530143738 - trainLoss: 0.6636741757392883\n",
      "cnt: 0 - valLoss: 0.6652300357818604 - trainLoss: 0.6636739373207092\n",
      "cnt: 0 - valLoss: 0.6652297377586365 - trainLoss: 0.6636735796928406\n",
      "cnt: 0 - valLoss: 0.6652293801307678 - trainLoss: 0.6636733412742615\n",
      "cnt: 0 - valLoss: 0.6652289628982544 - trainLoss: 0.6636729836463928\n",
      "cnt: 0 - valLoss: 0.6652286648750305 - trainLoss: 0.663672685623169\n",
      "cnt: 0 - valLoss: 0.6652281880378723 - trainLoss: 0.6636724472045898\n",
      "cnt: 0 - valLoss: 0.6652278900146484 - trainLoss: 0.6636720895767212\n",
      "cnt: 0 - valLoss: 0.665227472782135 - trainLoss: 0.6636718511581421\n",
      "cnt: 0 - valLoss: 0.6652271151542664 - trainLoss: 0.6636715531349182\n",
      "cnt: 0 - valLoss: 0.6652268171310425 - trainLoss: 0.6636711955070496\n",
      "cnt: 0 - valLoss: 0.6652264595031738 - trainLoss: 0.6636709570884705\n",
      "cnt: 0 - valLoss: 0.6652260422706604 - trainLoss: 0.6636706590652466\n",
      "cnt: 0 - valLoss: 0.6652256846427917 - trainLoss: 0.6636703610420227\n",
      "cnt: 0 - valLoss: 0.6652252674102783 - trainLoss: 0.6636700630187988\n",
      "cnt: 0 - valLoss: 0.6652249097824097 - trainLoss: 0.663669764995575\n",
      "cnt: 0 - valLoss: 0.665224552154541 - trainLoss: 0.6636694669723511\n",
      "cnt: 0 - valLoss: 0.6652242541313171 - trainLoss: 0.6636691689491272\n",
      "cnt: 0 - valLoss: 0.6652237772941589 - trainLoss: 0.6636688709259033\n",
      "cnt: 0 - valLoss: 0.6652234792709351 - trainLoss: 0.6636685132980347\n",
      "cnt: 0 - valLoss: 0.6652230620384216 - trainLoss: 0.6636682748794556\n",
      "cnt: 0 - valLoss: 0.665222704410553 - trainLoss: 0.6636679768562317\n",
      "cnt: 0 - valLoss: 0.6652223467826843 - trainLoss: 0.6636676788330078\n",
      "cnt: 0 - valLoss: 0.6652219891548157 - trainLoss: 0.6636673808097839\n",
      "cnt: 0 - valLoss: 0.665221631526947 - trainLoss: 0.6636670827865601\n",
      "cnt: 0 - valLoss: 0.6652212142944336 - trainLoss: 0.6636667847633362\n",
      "cnt: 0 - valLoss: 0.6652208566665649 - trainLoss: 0.6636664867401123\n",
      "cnt: 0 - valLoss: 0.6652204990386963 - trainLoss: 0.6636661887168884\n",
      "cnt: 0 - valLoss: 0.6652201414108276 - trainLoss: 0.6636658906936646\n",
      "cnt: 0 - valLoss: 0.665219783782959 - trainLoss: 0.6636655330657959\n",
      "cnt: 0 - valLoss: 0.6652193665504456 - trainLoss: 0.6636652946472168\n",
      "cnt: 0 - valLoss: 0.6652190089225769 - trainLoss: 0.6636649370193481\n",
      "cnt: 0 - valLoss: 0.6652186512947083 - trainLoss: 0.6636646389961243\n",
      "cnt: 0 - valLoss: 0.6652182936668396 - trainLoss: 0.6636644005775452\n",
      "cnt: 0 - valLoss: 0.665217936038971 - trainLoss: 0.6636641025543213\n",
      "cnt: 0 - valLoss: 0.6652175188064575 - trainLoss: 0.6636637449264526\n",
      "cnt: 0 - valLoss: 0.6652171611785889 - trainLoss: 0.6636634469032288\n",
      "cnt: 0 - valLoss: 0.6652167439460754 - trainLoss: 0.6636631488800049\n",
      "cnt: 0 - valLoss: 0.6652164459228516 - trainLoss: 0.663662850856781\n",
      "cnt: 0 - valLoss: 0.6652159690856934 - trainLoss: 0.6636625528335571\n",
      "cnt: 0 - valLoss: 0.6652156710624695 - trainLoss: 0.6636621952056885\n",
      "cnt: 0 - valLoss: 0.6652153134346008 - trainLoss: 0.6636619567871094\n",
      "cnt: 0 - valLoss: 0.6652148962020874 - trainLoss: 0.6636616587638855\n",
      "cnt: 0 - valLoss: 0.6652145385742188 - trainLoss: 0.6636613607406616\n",
      "cnt: 0 - valLoss: 0.6652141809463501 - trainLoss: 0.6636610627174377\n",
      "cnt: 0 - valLoss: 0.6652137637138367 - trainLoss: 0.6636607646942139\n",
      "cnt: 0 - valLoss: 0.665213406085968 - trainLoss: 0.6636604070663452\n",
      "cnt: 0 - valLoss: 0.6652131080627441 - trainLoss: 0.6636601686477661\n",
      "cnt: 0 - valLoss: 0.6652126312255859 - trainLoss: 0.6636598110198975\n",
      "cnt: 0 - valLoss: 0.6652123332023621 - trainLoss: 0.6636594533920288\n",
      "cnt: 0 - valLoss: 0.6652119755744934 - trainLoss: 0.6636592149734497\n",
      "cnt: 0 - valLoss: 0.66521155834198 - trainLoss: 0.663658857345581\n",
      "cnt: 0 - valLoss: 0.6652112007141113 - trainLoss: 0.6636586785316467\n",
      "cnt: 0 - valLoss: 0.6652108430862427 - trainLoss: 0.6636583209037781\n",
      "cnt: 0 - valLoss: 0.665210485458374 - trainLoss: 0.6636580228805542\n",
      "cnt: 0 - valLoss: 0.6652100086212158 - trainLoss: 0.6636576652526855\n",
      "cnt: 0 - valLoss: 0.6652097105979919 - trainLoss: 0.6636573672294617\n",
      "cnt: 0 - valLoss: 0.6652093529701233 - trainLoss: 0.6636571884155273\n",
      "cnt: 0 - valLoss: 0.6652089357376099 - trainLoss: 0.6636567711830139\n",
      "cnt: 0 - valLoss: 0.6652085185050964 - trainLoss: 0.6636565327644348\n",
      "cnt: 0 - valLoss: 0.6652082204818726 - trainLoss: 0.6636561155319214\n",
      "cnt: 0 - valLoss: 0.6652077436447144 - trainLoss: 0.6636558771133423\n",
      "cnt: 0 - valLoss: 0.6652073860168457 - trainLoss: 0.6636555194854736\n",
      "cnt: 0 - valLoss: 0.6652069687843323 - trainLoss: 0.6636552810668945\n",
      "cnt: 0 - valLoss: 0.6652066111564636 - trainLoss: 0.6636549830436707\n",
      "cnt: 0 - valLoss: 0.6652061939239502 - trainLoss: 0.663654625415802\n",
      "cnt: 0 - valLoss: 0.6652057766914368 - trainLoss: 0.6636543273925781\n",
      "cnt: 0 - valLoss: 0.6652054190635681 - trainLoss: 0.6636540293693542\n",
      "cnt: 0 - valLoss: 0.6652050018310547 - trainLoss: 0.6636537313461304\n",
      "cnt: 0 - valLoss: 0.665204644203186 - trainLoss: 0.6636533737182617\n",
      "cnt: 0 - valLoss: 0.6652042269706726 - trainLoss: 0.6636531352996826\n",
      "cnt: 0 - valLoss: 0.6652038097381592 - trainLoss: 0.663652777671814\n",
      "cnt: 0 - valLoss: 0.6652033925056458 - trainLoss: 0.6636524796485901\n",
      "cnt: 0 - valLoss: 0.6652029752731323 - trainLoss: 0.6636521816253662\n",
      "cnt: 0 - valLoss: 0.6652025580406189 - trainLoss: 0.6636518836021423\n",
      "cnt: 0 - valLoss: 0.6652021408081055 - trainLoss: 0.6636515855789185\n",
      "cnt: 0 - valLoss: 0.6652017831802368 - trainLoss: 0.6636512875556946\n",
      "cnt: 0 - valLoss: 0.6652013659477234 - trainLoss: 0.6636509895324707\n",
      "cnt: 0 - valLoss: 0.66520094871521 - trainLoss: 0.6636506915092468\n",
      "cnt: 0 - valLoss: 0.6652005910873413 - trainLoss: 0.6636503338813782\n",
      "cnt: 0 - valLoss: 0.6652001142501831 - trainLoss: 0.6636500358581543\n",
      "cnt: 0 - valLoss: 0.6651996970176697 - trainLoss: 0.6636497378349304\n",
      "cnt: 0 - valLoss: 0.665199339389801 - trainLoss: 0.6636494398117065\n",
      "cnt: 0 - valLoss: 0.6651989221572876 - trainLoss: 0.6636491417884827\n",
      "cnt: 0 - valLoss: 0.6651985049247742 - trainLoss: 0.6636488437652588\n",
      "cnt: 0 - valLoss: 0.6651980876922607 - trainLoss: 0.6636485457420349\n",
      "cnt: 0 - valLoss: 0.6651976704597473 - trainLoss: 0.6636481881141663\n",
      "cnt: 0 - valLoss: 0.6651973128318787 - trainLoss: 0.6636479496955872\n",
      "cnt: 0 - valLoss: 0.6651968359947205 - trainLoss: 0.6636475920677185\n",
      "cnt: 0 - valLoss: 0.6651964783668518 - trainLoss: 0.6636472344398499\n",
      "cnt: 0 - valLoss: 0.6651960611343384 - trainLoss: 0.6636469960212708\n",
      "cnt: 0 - valLoss: 0.6651957035064697 - trainLoss: 0.6636466979980469\n",
      "cnt: 0 - valLoss: 0.6651952266693115 - trainLoss: 0.6636462807655334\n",
      "cnt: 0 - valLoss: 0.6651948094367981 - trainLoss: 0.6636460423469543\n",
      "cnt: 0 - valLoss: 0.6651943922042847 - trainLoss: 0.6636458039283752\n",
      "cnt: 0 - valLoss: 0.6651939749717712 - trainLoss: 0.6636453866958618\n",
      "cnt: 0 - valLoss: 0.6651935577392578 - trainLoss: 0.6636450886726379\n",
      "cnt: 0 - valLoss: 0.6651932001113892 - trainLoss: 0.6636448502540588\n",
      "cnt: 0 - valLoss: 0.6651927828788757 - trainLoss: 0.6636444330215454\n",
      "cnt: 0 - valLoss: 0.6651923656463623 - trainLoss: 0.6636441349983215\n",
      "cnt: 0 - valLoss: 0.6651918888092041 - trainLoss: 0.6636438965797424\n",
      "cnt: 0 - valLoss: 0.6651915311813354 - trainLoss: 0.6636435389518738\n",
      "cnt: 0 - valLoss: 0.665191113948822 - trainLoss: 0.6636432409286499\n",
      "cnt: 0 - valLoss: 0.6651906967163086 - trainLoss: 0.663642942905426\n",
      "cnt: 0 - valLoss: 0.6651902794837952 - trainLoss: 0.6636426448822021\n",
      "cnt: 0 - valLoss: 0.6651898622512817 - trainLoss: 0.6636422872543335\n",
      "cnt: 0 - valLoss: 0.6651894450187683 - trainLoss: 0.6636419892311096\n",
      "cnt: 0 - valLoss: 0.6651890873908997 - trainLoss: 0.6636416912078857\n",
      "cnt: 0 - valLoss: 0.6651886701583862 - trainLoss: 0.6636413335800171\n",
      "cnt: 0 - valLoss: 0.665188193321228 - trainLoss: 0.663641095161438\n",
      "cnt: 0 - valLoss: 0.6651878356933594 - trainLoss: 0.6636407375335693\n",
      "cnt: 0 - valLoss: 0.6651873588562012 - trainLoss: 0.6636403799057007\n",
      "cnt: 0 - valLoss: 0.6651870012283325 - trainLoss: 0.6636401414871216\n",
      "cnt: 0 - valLoss: 0.6651865839958191 - trainLoss: 0.6636397242546082\n",
      "cnt: 0 - valLoss: 0.6651861667633057 - trainLoss: 0.6636395454406738\n",
      "cnt: 0 - valLoss: 0.6651857495307922 - trainLoss: 0.6636391878128052\n",
      "cnt: 0 - valLoss: 0.6651853322982788 - trainLoss: 0.6636388897895813\n",
      "cnt: 0 - valLoss: 0.6651848554611206 - trainLoss: 0.6636385321617126\n",
      "cnt: 0 - valLoss: 0.665184497833252 - trainLoss: 0.6636382341384888\n",
      "cnt: 0 - valLoss: 0.6651840806007385 - trainLoss: 0.6636379361152649\n",
      "cnt: 0 - valLoss: 0.6651836633682251 - trainLoss: 0.663637638092041\n",
      "cnt: 0 - valLoss: 0.6651833057403564 - trainLoss: 0.6636373400688171\n",
      "cnt: 0 - valLoss: 0.6651828289031982 - trainLoss: 0.6636369824409485\n",
      "cnt: 0 - valLoss: 0.6651824116706848 - trainLoss: 0.6636366844177246\n",
      "cnt: 0 - valLoss: 0.6651819944381714 - trainLoss: 0.6636363863945007\n",
      "cnt: 0 - valLoss: 0.6651816368103027 - trainLoss: 0.6636360287666321\n",
      "cnt: 0 - valLoss: 0.6651811599731445 - trainLoss: 0.6636357307434082\n",
      "cnt: 0 - valLoss: 0.6651807427406311 - trainLoss: 0.6636353731155396\n",
      "cnt: 0 - valLoss: 0.6651803255081177 - trainLoss: 0.6636351346969604\n",
      "cnt: 0 - valLoss: 0.6651799082756042 - trainLoss: 0.6636347770690918\n",
      "cnt: 0 - valLoss: 0.6651794910430908 - trainLoss: 0.6636344790458679\n",
      "cnt: 0 - valLoss: 0.6651790738105774 - trainLoss: 0.663634181022644\n",
      "cnt: 0 - valLoss: 0.665178656578064 - trainLoss: 0.6636338233947754\n",
      "cnt: 0 - valLoss: 0.6651782989501953 - trainLoss: 0.6636335253715515\n",
      "cnt: 0 - valLoss: 0.6651778221130371 - trainLoss: 0.6636332273483276\n",
      "cnt: 0 - valLoss: 0.6651774048805237 - trainLoss: 0.6636329293251038\n",
      "cnt: 0 - valLoss: 0.665177047252655 - trainLoss: 0.6636325716972351\n",
      "cnt: 0 - valLoss: 0.6651766300201416 - trainLoss: 0.6636322736740112\n",
      "cnt: 0 - valLoss: 0.6651761531829834 - trainLoss: 0.6636319160461426\n",
      "cnt: 0 - valLoss: 0.6651756763458252 - trainLoss: 0.6636316776275635\n",
      "cnt: 0 - valLoss: 0.6651753187179565 - trainLoss: 0.6636313199996948\n",
      "cnt: 0 - valLoss: 0.6651749610900879 - trainLoss: 0.663631021976471\n",
      "cnt: 0 - valLoss: 0.6651744842529297 - trainLoss: 0.6636307239532471\n",
      "cnt: 0 - valLoss: 0.6651740670204163 - trainLoss: 0.6636303663253784\n",
      "cnt: 0 - valLoss: 0.6651736497879028 - trainLoss: 0.6636300086975098\n",
      "cnt: 0 - valLoss: 0.6651732325553894 - trainLoss: 0.6636297106742859\n",
      "cnt: 0 - valLoss: 0.665172815322876 - trainLoss: 0.663629412651062\n",
      "cnt: 0 - valLoss: 0.6651723980903625 - trainLoss: 0.6636291146278381\n",
      "cnt: 0 - valLoss: 0.6651720404624939 - trainLoss: 0.6636287569999695\n",
      "cnt: 0 - valLoss: 0.6651715636253357 - trainLoss: 0.6636285185813904\n",
      "cnt: 0 - valLoss: 0.6651711463928223 - trainLoss: 0.6636281609535217\n",
      "cnt: 0 - valLoss: 0.6651707887649536 - trainLoss: 0.6636278629302979\n",
      "cnt: 0 - valLoss: 0.6651703715324402 - trainLoss: 0.663627564907074\n",
      "cnt: 0 - valLoss: 0.6651699542999268 - trainLoss: 0.6636272072792053\n",
      "cnt: 0 - valLoss: 0.6651694774627686 - trainLoss: 0.6636269092559814\n",
      "cnt: 0 - valLoss: 0.6651691198348999 - trainLoss: 0.6636265516281128\n",
      "cnt: 0 - valLoss: 0.6651686429977417 - trainLoss: 0.6636262536048889\n",
      "cnt: 0 - valLoss: 0.6651682257652283 - trainLoss: 0.6636258959770203\n",
      "cnt: 0 - valLoss: 0.6651678085327148 - trainLoss: 0.6636255979537964\n",
      "cnt: 0 - valLoss: 0.6651673913002014 - trainLoss: 0.6636252999305725\n",
      "cnt: 0 - valLoss: 0.665166974067688 - trainLoss: 0.6636250019073486\n",
      "cnt: 0 - valLoss: 0.6651665568351746 - trainLoss: 0.66362464427948\n",
      "cnt: 0 - valLoss: 0.6651661396026611 - trainLoss: 0.6636243462562561\n",
      "cnt: 0 - valLoss: 0.6651657819747925 - trainLoss: 0.6636239886283875\n",
      "cnt: 0 - valLoss: 0.6651653051376343 - trainLoss: 0.6636236906051636\n",
      "cnt: 0 - valLoss: 0.6651648879051208 - trainLoss: 0.6636233329772949\n",
      "cnt: 0 - valLoss: 0.6651644706726074 - trainLoss: 0.6636230945587158\n",
      "cnt: 0 - valLoss: 0.6651641130447388 - trainLoss: 0.6636227965354919\n",
      "cnt: 0 - valLoss: 0.6651636362075806 - trainLoss: 0.6636224389076233\n",
      "cnt: 0 - valLoss: 0.6651632189750671 - trainLoss: 0.6636221408843994\n",
      "cnt: 0 - valLoss: 0.6651627421379089 - trainLoss: 0.6636218428611755\n",
      "cnt: 0 - valLoss: 0.6651623249053955 - trainLoss: 0.6636214852333069\n",
      "cnt: 0 - valLoss: 0.6651619076728821 - trainLoss: 0.6636211276054382\n",
      "cnt: 0 - valLoss: 0.6651614904403687 - trainLoss: 0.6636208891868591\n",
      "cnt: 0 - valLoss: 0.6651610732078552 - trainLoss: 0.6636205315589905\n",
      "cnt: 0 - valLoss: 0.6651607155799866 - trainLoss: 0.6636201739311218\n",
      "cnt: 0 - valLoss: 0.6651602387428284 - trainLoss: 0.663619875907898\n",
      "cnt: 0 - valLoss: 0.6651598215103149 - trainLoss: 0.6636195182800293\n",
      "cnt: 0 - valLoss: 0.6651594042778015 - trainLoss: 0.6636192202568054\n",
      "cnt: 0 - valLoss: 0.6651589870452881 - trainLoss: 0.6636189222335815\n",
      "cnt: 0 - valLoss: 0.6651585102081299 - trainLoss: 0.6636185646057129\n",
      "cnt: 0 - valLoss: 0.6651581525802612 - trainLoss: 0.6636183261871338\n",
      "cnt: 0 - valLoss: 0.665157675743103 - trainLoss: 0.6636179685592651\n",
      "cnt: 0 - valLoss: 0.6651572585105896 - trainLoss: 0.6636176705360413\n",
      "cnt: 0 - valLoss: 0.665156900882721 - trainLoss: 0.6636172533035278\n",
      "cnt: 0 - valLoss: 0.6651564240455627 - trainLoss: 0.663616955280304\n",
      "cnt: 0 - valLoss: 0.6651560068130493 - trainLoss: 0.6636166572570801\n",
      "cnt: 0 - valLoss: 0.6651555895805359 - trainLoss: 0.6636163592338562\n",
      "cnt: 0 - valLoss: 0.6651552319526672 - trainLoss: 0.6636160016059875\n",
      "cnt: 0 - valLoss: 0.665154755115509 - trainLoss: 0.6636157631874084\n",
      "cnt: 0 - valLoss: 0.6651543378829956 - trainLoss: 0.6636154055595398\n",
      "cnt: 0 - valLoss: 0.6651539206504822 - trainLoss: 0.6636151075363159\n",
      "cnt: 0 - valLoss: 0.665153443813324 - trainLoss: 0.663614809513092\n",
      "cnt: 0 - valLoss: 0.6651530861854553 - trainLoss: 0.6636143922805786\n",
      "cnt: 0 - valLoss: 0.6651526093482971 - trainLoss: 0.6636140942573547\n",
      "cnt: 0 - valLoss: 0.6651521921157837 - trainLoss: 0.6636137962341309\n",
      "cnt: 0 - valLoss: 0.6651517748832703 - trainLoss: 0.6636134386062622\n",
      "cnt: 0 - valLoss: 0.6651513576507568 - trainLoss: 0.6636131405830383\n",
      "cnt: 0 - valLoss: 0.6651509404182434 - trainLoss: 0.6636128425598145\n",
      "cnt: 0 - valLoss: 0.66515052318573 - trainLoss: 0.6636125445365906\n",
      "cnt: 0 - valLoss: 0.6651500463485718 - trainLoss: 0.6636122465133667\n",
      "cnt: 0 - valLoss: 0.6651496291160583 - trainLoss: 0.6636118292808533\n",
      "cnt: 0 - valLoss: 0.6651492118835449 - trainLoss: 0.6636114716529846\n",
      "cnt: 0 - valLoss: 0.6651487946510315 - trainLoss: 0.6636111736297607\n",
      "cnt: 0 - valLoss: 0.6651483774185181 - trainLoss: 0.6636108756065369\n",
      "cnt: 0 - valLoss: 0.6651479601860046 - trainLoss: 0.663610577583313\n",
      "cnt: 0 - valLoss: 0.6651474833488464 - trainLoss: 0.6636102795600891\n",
      "cnt: 0 - valLoss: 0.6651471853256226 - trainLoss: 0.6636099219322205\n",
      "cnt: 0 - valLoss: 0.6651467084884644 - trainLoss: 0.6636096239089966\n",
      "cnt: 0 - valLoss: 0.6651462316513062 - trainLoss: 0.6636092662811279\n",
      "cnt: 0 - valLoss: 0.6651458144187927 - trainLoss: 0.6636089086532593\n",
      "cnt: 0 - valLoss: 0.6651454567909241 - trainLoss: 0.6636086106300354\n",
      "cnt: 0 - valLoss: 0.6651449799537659 - trainLoss: 0.6636083126068115\n",
      "cnt: 0 - valLoss: 0.6651445627212524 - trainLoss: 0.6636079549789429\n",
      "cnt: 0 - valLoss: 0.665144145488739 - trainLoss: 0.663607656955719\n",
      "cnt: 0 - valLoss: 0.6651437282562256 - trainLoss: 0.6636072993278503\n",
      "cnt: 0 - valLoss: 0.6651433110237122 - trainLoss: 0.6636070013046265\n",
      "cnt: 0 - valLoss: 0.6651428937911987 - trainLoss: 0.6636066436767578\n",
      "cnt: 0 - valLoss: 0.6651424169540405 - trainLoss: 0.6636063456535339\n",
      "cnt: 0 - valLoss: 0.6651419997215271 - trainLoss: 0.6636059880256653\n",
      "cnt: 0 - valLoss: 0.6651415228843689 - trainLoss: 0.6636057496070862\n",
      "cnt: 0 - valLoss: 0.6651411652565002 - trainLoss: 0.6636053919792175\n",
      "cnt: 0 - valLoss: 0.665140688419342 - trainLoss: 0.6636050343513489\n",
      "cnt: 0 - valLoss: 0.6651403307914734 - trainLoss: 0.6636046767234802\n",
      "cnt: 0 - valLoss: 0.6651398539543152 - trainLoss: 0.6636043190956116\n",
      "cnt: 0 - valLoss: 0.665139377117157 - trainLoss: 0.6636040806770325\n",
      "cnt: 0 - valLoss: 0.6651389598846436 - trainLoss: 0.6636037230491638\n",
      "cnt: 0 - valLoss: 0.6651385426521301 - trainLoss: 0.6636034250259399\n",
      "cnt: 0 - valLoss: 0.6651381254196167 - trainLoss: 0.6636030673980713\n",
      "cnt: 0 - valLoss: 0.6651377081871033 - trainLoss: 0.6636027693748474\n",
      "cnt: 0 - valLoss: 0.6651372313499451 - trainLoss: 0.6636024117469788\n",
      "cnt: 0 - valLoss: 0.6651368141174316 - trainLoss: 0.6636020541191101\n",
      "cnt: 0 - valLoss: 0.665136456489563 - trainLoss: 0.6636017560958862\n",
      "cnt: 0 - valLoss: 0.6651359796524048 - trainLoss: 0.6636014580726624\n",
      "cnt: 0 - valLoss: 0.6651355028152466 - trainLoss: 0.6636011600494385\n",
      "cnt: 0 - valLoss: 0.6651350855827332 - trainLoss: 0.663600742816925\n",
      "cnt: 0 - valLoss: 0.6651346683502197 - trainLoss: 0.6636003851890564\n",
      "cnt: 0 - valLoss: 0.6651341915130615 - trainLoss: 0.6636001467704773\n",
      "cnt: 0 - valLoss: 0.6651337742805481 - trainLoss: 0.6635997891426086\n",
      "cnt: 0 - valLoss: 0.6651333570480347 - trainLoss: 0.66359943151474\n",
      "cnt: 0 - valLoss: 0.6651329398155212 - trainLoss: 0.6635991334915161\n",
      "cnt: 0 - valLoss: 0.6651325225830078 - trainLoss: 0.6635987758636475\n",
      "cnt: 0 - valLoss: 0.6651320457458496 - trainLoss: 0.6635984182357788\n",
      "cnt: 0 - valLoss: 0.6651316285133362 - trainLoss: 0.6635981202125549\n",
      "cnt: 0 - valLoss: 0.6651312112808228 - trainLoss: 0.6635977625846863\n",
      "cnt: 0 - valLoss: 0.6651307940483093 - trainLoss: 0.6635974049568176\n",
      "cnt: 0 - valLoss: 0.6651303768157959 - trainLoss: 0.663597047328949\n",
      "cnt: 0 - valLoss: 0.6651298999786377 - trainLoss: 0.6635967493057251\n",
      "cnt: 0 - valLoss: 0.6651294827461243 - trainLoss: 0.6635963916778564\n",
      "cnt: 0 - valLoss: 0.6651290655136108 - trainLoss: 0.6635960340499878\n",
      "cnt: 0 - valLoss: 0.6651286482810974 - trainLoss: 0.6635956764221191\n",
      "cnt: 0 - valLoss: 0.665128231048584 - trainLoss: 0.6635953783988953\n",
      "cnt: 0 - valLoss: 0.665127694606781 - trainLoss: 0.6635950207710266\n",
      "cnt: 0 - valLoss: 0.6651273369789124 - trainLoss: 0.663594663143158\n",
      "cnt: 0 - valLoss: 0.6651268601417542 - trainLoss: 0.6635943651199341\n",
      "cnt: 0 - valLoss: 0.6651264429092407 - trainLoss: 0.6635940074920654\n",
      "cnt: 0 - valLoss: 0.6651259660720825 - trainLoss: 0.6635937094688416\n",
      "cnt: 0 - valLoss: 0.6651255488395691 - trainLoss: 0.6635932922363281\n",
      "cnt: 0 - valLoss: 0.6651251316070557 - trainLoss: 0.6635929942131042\n",
      "cnt: 0 - valLoss: 0.6651247143745422 - trainLoss: 0.6635926365852356\n",
      "cnt: 0 - valLoss: 0.665124237537384 - trainLoss: 0.6635923385620117\n",
      "cnt: 0 - valLoss: 0.6651238203048706 - trainLoss: 0.6635919213294983\n",
      "cnt: 0 - valLoss: 0.6651233434677124 - trainLoss: 0.6635916233062744\n",
      "cnt: 0 - valLoss: 0.665122926235199 - trainLoss: 0.6635912656784058\n",
      "cnt: 0 - valLoss: 0.6651224493980408 - trainLoss: 0.6635909676551819\n",
      "cnt: 0 - valLoss: 0.6651220321655273 - trainLoss: 0.6635905504226685\n",
      "cnt: 0 - valLoss: 0.6651216745376587 - trainLoss: 0.6635902523994446\n",
      "cnt: 0 - valLoss: 0.6651211977005005 - trainLoss: 0.6635898947715759\n",
      "cnt: 0 - valLoss: 0.6651207804679871 - trainLoss: 0.663589596748352\n",
      "cnt: 0 - valLoss: 0.6651203036308289 - trainLoss: 0.6635891795158386\n",
      "cnt: 0 - valLoss: 0.6651198863983154 - trainLoss: 0.6635888814926147\n",
      "cnt: 0 - valLoss: 0.665119469165802 - trainLoss: 0.6635885238647461\n",
      "cnt: 0 - valLoss: 0.6651190519332886 - trainLoss: 0.6635882258415222\n",
      "cnt: 0 - valLoss: 0.6651186347007751 - trainLoss: 0.6635878086090088\n",
      "cnt: 0 - valLoss: 0.6651181578636169 - trainLoss: 0.6635875105857849\n",
      "cnt: 0 - valLoss: 0.6651177406311035 - trainLoss: 0.6635871529579163\n",
      "cnt: 0 - valLoss: 0.6651172637939453 - trainLoss: 0.6635868549346924\n",
      "cnt: 0 - valLoss: 0.6651167869567871 - trainLoss: 0.663586437702179\n",
      "cnt: 0 - valLoss: 0.6651163697242737 - trainLoss: 0.6635861396789551\n",
      "cnt: 0 - valLoss: 0.6651158928871155 - trainLoss: 0.6635858416557312\n",
      "cnt: 0 - valLoss: 0.6651155352592468 - trainLoss: 0.6635854244232178\n",
      "cnt: 0 - valLoss: 0.6651150584220886 - trainLoss: 0.6635850667953491\n",
      "cnt: 0 - valLoss: 0.6651146411895752 - trainLoss: 0.6635847687721252\n",
      "cnt: 0 - valLoss: 0.6651142239570618 - trainLoss: 0.6635844111442566\n",
      "cnt: 0 - valLoss: 0.6651137471199036 - trainLoss: 0.6635839939117432\n",
      "cnt: 0 - valLoss: 0.6651133298873901 - trainLoss: 0.6635836958885193\n",
      "cnt: 0 - valLoss: 0.6651128530502319 - trainLoss: 0.6635833978652954\n",
      "cnt: 0 - valLoss: 0.6651124358177185 - trainLoss: 0.6635830402374268\n",
      "cnt: 0 - valLoss: 0.6651120185852051 - trainLoss: 0.6635826230049133\n",
      "cnt: 0 - valLoss: 0.6651116013526917 - trainLoss: 0.6635823249816895\n",
      "cnt: 0 - valLoss: 0.6651111245155334 - trainLoss: 0.6635819673538208\n",
      "cnt: 0 - valLoss: 0.6651106476783752 - trainLoss: 0.6635816097259521\n",
      "cnt: 0 - valLoss: 0.6651102304458618 - trainLoss: 0.6635813117027283\n",
      "cnt: 0 - valLoss: 0.6651098132133484 - trainLoss: 0.6635809540748596\n",
      "cnt: 0 - valLoss: 0.6651093363761902 - trainLoss: 0.6635805368423462\n",
      "cnt: 0 - valLoss: 0.6651089191436768 - trainLoss: 0.6635802388191223\n",
      "cnt: 0 - valLoss: 0.6651085019111633 - trainLoss: 0.6635799407958984\n",
      "cnt: 0 - valLoss: 0.6651080250740051 - trainLoss: 0.6635795831680298\n",
      "cnt: 0 - valLoss: 0.6651076078414917 - trainLoss: 0.6635792255401611\n",
      "cnt: 0 - valLoss: 0.6651071310043335 - trainLoss: 0.6635788679122925\n",
      "cnt: 0 - valLoss: 0.6651067137718201 - trainLoss: 0.6635785102844238\n",
      "cnt: 0 - valLoss: 0.6651062965393066 - trainLoss: 0.6635781526565552\n",
      "cnt: 0 - valLoss: 0.6651058197021484 - trainLoss: 0.6635777950286865\n",
      "cnt: 0 - valLoss: 0.665105402469635 - trainLoss: 0.6635774970054626\n",
      "cnt: 0 - valLoss: 0.6651049852371216 - trainLoss: 0.6635770797729492\n",
      "cnt: 0 - valLoss: 0.6651045083999634 - trainLoss: 0.6635767817497253\n",
      "cnt: 0 - valLoss: 0.6651040315628052 - trainLoss: 0.6635764241218567\n",
      "cnt: 0 - valLoss: 0.6651036143302917 - trainLoss: 0.6635761260986328\n",
      "cnt: 0 - valLoss: 0.6651031970977783 - trainLoss: 0.6635757684707642\n",
      "cnt: 0 - valLoss: 0.6651027202606201 - trainLoss: 0.6635753512382507\n",
      "cnt: 0 - valLoss: 0.6651023030281067 - trainLoss: 0.6635750532150269\n",
      "cnt: 0 - valLoss: 0.6651018261909485 - trainLoss: 0.6635746359825134\n",
      "cnt: 0 - valLoss: 0.6651014089584351 - trainLoss: 0.6635743379592896\n",
      "cnt: 0 - valLoss: 0.6651009321212769 - trainLoss: 0.6635739803314209\n",
      "cnt: 0 - valLoss: 0.6651005148887634 - trainLoss: 0.6635736227035522\n",
      "cnt: 0 - valLoss: 0.66510009765625 - trainLoss: 0.6635732650756836\n",
      "cnt: 0 - valLoss: 0.6650996208190918 - trainLoss: 0.6635729074478149\n",
      "cnt: 0 - valLoss: 0.6650992035865784 - trainLoss: 0.6635726094245911\n",
      "cnt: 0 - valLoss: 0.6650987267494202 - trainLoss: 0.6635722517967224\n",
      "cnt: 0 - valLoss: 0.6650983095169067 - trainLoss: 0.6635718941688538\n",
      "cnt: 0 - valLoss: 0.6650978922843933 - trainLoss: 0.6635715365409851\n",
      "cnt: 0 - valLoss: 0.6650974154472351 - trainLoss: 0.6635711789131165\n",
      "cnt: 0 - valLoss: 0.6650969982147217 - trainLoss: 0.6635708212852478\n",
      "cnt: 0 - valLoss: 0.6650965213775635 - trainLoss: 0.6635704636573792\n",
      "cnt: 0 - valLoss: 0.6650960445404053 - trainLoss: 0.6635701060295105\n",
      "cnt: 0 - valLoss: 0.6650956273078918 - trainLoss: 0.6635697484016418\n",
      "cnt: 0 - valLoss: 0.6650951504707336 - trainLoss: 0.663569450378418\n",
      "cnt: 0 - valLoss: 0.6650947332382202 - trainLoss: 0.6635690927505493\n",
      "cnt: 0 - valLoss: 0.665094256401062 - trainLoss: 0.6635686755180359\n",
      "cnt: 0 - valLoss: 0.6650938391685486 - trainLoss: 0.663568377494812\n",
      "cnt: 0 - valLoss: 0.6650934219360352 - trainLoss: 0.6635680794715881\n",
      "cnt: 0 - valLoss: 0.665092945098877 - trainLoss: 0.6635677218437195\n",
      "cnt: 0 - valLoss: 0.6650925278663635 - trainLoss: 0.663567304611206\n",
      "cnt: 0 - valLoss: 0.6650920510292053 - trainLoss: 0.6635669469833374\n",
      "cnt: 0 - valLoss: 0.6650915741920471 - trainLoss: 0.6635666489601135\n",
      "cnt: 0 - valLoss: 0.6650911569595337 - trainLoss: 0.6635662913322449\n",
      "cnt: 0 - valLoss: 0.6650906801223755 - trainLoss: 0.6635659337043762\n",
      "cnt: 0 - valLoss: 0.6650902628898621 - trainLoss: 0.6635655164718628\n",
      "cnt: 0 - valLoss: 0.6650897860527039 - trainLoss: 0.6635651588439941\n",
      "cnt: 0 - valLoss: 0.6650893688201904 - trainLoss: 0.6635648012161255\n",
      "cnt: 0 - valLoss: 0.6650888919830322 - trainLoss: 0.6635644435882568\n",
      "cnt: 0 - valLoss: 0.665088415145874 - trainLoss: 0.6635640859603882\n",
      "cnt: 0 - valLoss: 0.6650879383087158 - trainLoss: 0.6635636687278748\n",
      "cnt: 0 - valLoss: 0.6650875210762024 - trainLoss: 0.6635633707046509\n",
      "cnt: 0 - valLoss: 0.6650870442390442 - trainLoss: 0.6635630130767822\n",
      "cnt: 0 - valLoss: 0.6650866270065308 - trainLoss: 0.6635625958442688\n",
      "cnt: 0 - valLoss: 0.6650861501693726 - trainLoss: 0.6635622978210449\n",
      "cnt: 0 - valLoss: 0.6650856733322144 - trainLoss: 0.6635619401931763\n",
      "cnt: 0 - valLoss: 0.6650851964950562 - trainLoss: 0.6635615229606628\n",
      "cnt: 0 - valLoss: 0.6650848388671875 - trainLoss: 0.663561224937439\n",
      "cnt: 0 - valLoss: 0.6650843024253845 - trainLoss: 0.6635608077049255\n",
      "cnt: 0 - valLoss: 0.6650838851928711 - trainLoss: 0.6635604500770569\n",
      "cnt: 0 - valLoss: 0.6650834679603577 - trainLoss: 0.663560152053833\n",
      "cnt: 0 - valLoss: 0.6650830507278442 - trainLoss: 0.6635597944259644\n",
      "cnt: 0 - valLoss: 0.6650825142860413 - trainLoss: 0.6635594367980957\n",
      "cnt: 0 - valLoss: 0.6650820970535278 - trainLoss: 0.6635590195655823\n",
      "cnt: 0 - valLoss: 0.6650816202163696 - trainLoss: 0.6635586619377136\n",
      "cnt: 0 - valLoss: 0.6650811433792114 - trainLoss: 0.6635583639144897\n",
      "cnt: 0 - valLoss: 0.6650807857513428 - trainLoss: 0.6635579466819763\n",
      "cnt: 0 - valLoss: 0.6650803089141846 - trainLoss: 0.6635575890541077\n",
      "cnt: 0 - valLoss: 0.6650798320770264 - trainLoss: 0.6635572910308838\n",
      "cnt: 0 - valLoss: 0.6650793552398682 - trainLoss: 0.6635568141937256\n",
      "cnt: 0 - valLoss: 0.6650789380073547 - trainLoss: 0.6635565161705017\n",
      "cnt: 0 - valLoss: 0.6650784611701965 - trainLoss: 0.6635560989379883\n",
      "cnt: 0 - valLoss: 0.6650779843330383 - trainLoss: 0.6635558009147644\n",
      "cnt: 0 - valLoss: 0.6650775671005249 - trainLoss: 0.6635554432868958\n",
      "cnt: 0 - valLoss: 0.6650770902633667 - trainLoss: 0.6635550260543823\n",
      "cnt: 0 - valLoss: 0.6650766134262085 - trainLoss: 0.6635546684265137\n",
      "cnt: 0 - valLoss: 0.6650761365890503 - trainLoss: 0.6635543704032898\n",
      "cnt: 0 - valLoss: 0.6650757789611816 - trainLoss: 0.6635539531707764\n",
      "cnt: 0 - valLoss: 0.6650753021240234 - trainLoss: 0.6635535955429077\n",
      "cnt: 0 - valLoss: 0.6650748252868652 - trainLoss: 0.6635531783103943\n",
      "cnt: 0 - valLoss: 0.665074348449707 - trainLoss: 0.6635528802871704\n",
      "cnt: 0 - valLoss: 0.6650738716125488 - trainLoss: 0.663552463054657\n",
      "cnt: 0 - valLoss: 0.6650733947753906 - trainLoss: 0.6635521054267883\n",
      "cnt: 0 - valLoss: 0.665073037147522 - trainLoss: 0.6635517477989197\n",
      "cnt: 0 - valLoss: 0.665072500705719 - trainLoss: 0.663551390171051\n",
      "cnt: 0 - valLoss: 0.6650720834732056 - trainLoss: 0.6635510325431824\n",
      "cnt: 0 - valLoss: 0.6650716066360474 - trainLoss: 0.663550615310669\n",
      "cnt: 0 - valLoss: 0.6650711297988892 - trainLoss: 0.6635502576828003\n",
      "cnt: 0 - valLoss: 0.6650707125663757 - trainLoss: 0.6635499596595764\n",
      "cnt: 0 - valLoss: 0.6650702953338623 - trainLoss: 0.663549542427063\n",
      "cnt: 0 - valLoss: 0.6650698184967041 - trainLoss: 0.6635492444038391\n",
      "cnt: 0 - valLoss: 0.6650692820549011 - trainLoss: 0.6635488867759705\n",
      "cnt: 0 - valLoss: 0.6650688648223877 - trainLoss: 0.663548469543457\n",
      "cnt: 0 - valLoss: 0.6650683879852295 - trainLoss: 0.6635481119155884\n",
      "cnt: 0 - valLoss: 0.6650679707527161 - trainLoss: 0.6635477542877197\n",
      "cnt: 0 - valLoss: 0.6650674343109131 - trainLoss: 0.6635473966598511\n",
      "cnt: 0 - valLoss: 0.6650670170783997 - trainLoss: 0.6635469794273376\n",
      "cnt: 0 - valLoss: 0.6650665998458862 - trainLoss: 0.6635466814041138\n",
      "cnt: 0 - valLoss: 0.665066123008728 - trainLoss: 0.6635463237762451\n",
      "cnt: 0 - valLoss: 0.6650656461715698 - trainLoss: 0.6635459065437317\n",
      "cnt: 0 - valLoss: 0.6650652289390564 - trainLoss: 0.663545548915863\n",
      "cnt: 0 - valLoss: 0.6650646924972534 - trainLoss: 0.6635451912879944\n",
      "cnt: 0 - valLoss: 0.6650642156600952 - trainLoss: 0.6635448336601257\n",
      "cnt: 0 - valLoss: 0.6650637984275818 - trainLoss: 0.6635444164276123\n",
      "cnt: 0 - valLoss: 0.6650633215904236 - trainLoss: 0.6635440587997437\n",
      "cnt: 0 - valLoss: 0.6650629043579102 - trainLoss: 0.6635437607765198\n",
      "cnt: 0 - valLoss: 0.6650623679161072 - trainLoss: 0.6635433435440063\n",
      "cnt: 0 - valLoss: 0.6650619506835938 - trainLoss: 0.6635429859161377\n",
      "cnt: 0 - valLoss: 0.6650614738464355 - trainLoss: 0.663542628288269\n",
      "cnt: 0 - valLoss: 0.6650610566139221 - trainLoss: 0.6635422706604004\n",
      "cnt: 0 - valLoss: 0.6650605797767639 - trainLoss: 0.663541853427887\n",
      "cnt: 0 - valLoss: 0.6650601625442505 - trainLoss: 0.6635414958000183\n",
      "cnt: 0 - valLoss: 0.6650596857070923 - trainLoss: 0.6635411381721497\n",
      "cnt: 0 - valLoss: 0.6650592088699341 - trainLoss: 0.6635407209396362\n",
      "cnt: 0 - valLoss: 0.6650587320327759 - trainLoss: 0.6635404229164124\n",
      "cnt: 0 - valLoss: 0.6650583148002625 - trainLoss: 0.6635400652885437\n",
      "cnt: 0 - valLoss: 0.6650578379631042 - trainLoss: 0.6635396480560303\n",
      "cnt: 0 - valLoss: 0.6650574207305908 - trainLoss: 0.6635392904281616\n",
      "cnt: 0 - valLoss: 0.6650569438934326 - trainLoss: 0.663538932800293\n",
      "cnt: 0 - valLoss: 0.6650564670562744 - trainLoss: 0.6635385751724243\n",
      "cnt: 0 - valLoss: 0.665056049823761 - trainLoss: 0.6635381579399109\n",
      "cnt: 0 - valLoss: 0.6650556325912476 - trainLoss: 0.663537859916687\n",
      "cnt: 0 - valLoss: 0.6650550961494446 - trainLoss: 0.6635375022888184\n",
      "cnt: 0 - valLoss: 0.6650546789169312 - trainLoss: 0.6635371446609497\n",
      "cnt: 0 - valLoss: 0.6650542616844177 - trainLoss: 0.6635367274284363\n",
      "cnt: 0 - valLoss: 0.6650537848472595 - trainLoss: 0.6635363698005676\n",
      "cnt: 0 - valLoss: 0.6650533080101013 - trainLoss: 0.663536012172699\n",
      "cnt: 0 - valLoss: 0.6650528907775879 - trainLoss: 0.6635355949401855\n",
      "cnt: 0 - valLoss: 0.6650524139404297 - trainLoss: 0.6635352969169617\n",
      "cnt: 0 - valLoss: 0.6650519371032715 - trainLoss: 0.663534939289093\n",
      "cnt: 0 - valLoss: 0.6650515198707581 - trainLoss: 0.6635345220565796\n",
      "cnt: 0 - valLoss: 0.6650510430335999 - trainLoss: 0.6635341644287109\n",
      "cnt: 0 - valLoss: 0.6650505661964417 - trainLoss: 0.6635337471961975\n",
      "cnt: 0 - valLoss: 0.6650501489639282 - trainLoss: 0.6635333895683289\n",
      "cnt: 0 - valLoss: 0.6650496125221252 - trainLoss: 0.6635330319404602\n",
      "cnt: 0 - valLoss: 0.6650491952896118 - trainLoss: 0.6635326743125916\n",
      "cnt: 0 - valLoss: 0.6650487780570984 - trainLoss: 0.6635322570800781\n",
      "cnt: 0 - valLoss: 0.6650482416152954 - trainLoss: 0.6635318994522095\n",
      "cnt: 0 - valLoss: 0.665047824382782 - trainLoss: 0.6635315418243408\n",
      "cnt: 0 - valLoss: 0.6650474071502686 - trainLoss: 0.6635311841964722\n",
      "cnt: 0 - valLoss: 0.6650468707084656 - trainLoss: 0.6635307669639587\n",
      "cnt: 0 - valLoss: 0.6650464534759521 - trainLoss: 0.6635304093360901\n",
      "cnt: 0 - valLoss: 0.665045976638794 - trainLoss: 0.6635300517082214\n",
      "cnt: 0 - valLoss: 0.6650455594062805 - trainLoss: 0.663529634475708\n",
      "cnt: 0 - valLoss: 0.6650450825691223 - trainLoss: 0.6635292768478394\n",
      "cnt: 0 - valLoss: 0.6650446653366089 - trainLoss: 0.6635289192199707\n",
      "cnt: 0 - valLoss: 0.6650441884994507 - trainLoss: 0.6635285019874573\n",
      "cnt: 0 - valLoss: 0.6650437116622925 - trainLoss: 0.6635281443595886\n",
      "cnt: 0 - valLoss: 0.6650432348251343 - trainLoss: 0.6635278463363647\n",
      "cnt: 0 - valLoss: 0.6650428175926208 - trainLoss: 0.6635273694992065\n",
      "cnt: 0 - valLoss: 0.6650423407554626 - trainLoss: 0.6635270714759827\n",
      "cnt: 0 - valLoss: 0.6650419235229492 - trainLoss: 0.6635265946388245\n",
      "cnt: 0 - valLoss: 0.6650415062904358 - trainLoss: 0.6635262966156006\n",
      "cnt: 0 - valLoss: 0.6650410294532776 - trainLoss: 0.6635259389877319\n",
      "cnt: 0 - valLoss: 0.6650406122207642 - trainLoss: 0.6635255813598633\n",
      "cnt: 0 - valLoss: 0.665040135383606 - trainLoss: 0.6635251641273499\n",
      "cnt: 0 - valLoss: 0.6650396585464478 - trainLoss: 0.6635248064994812\n",
      "cnt: 0 - valLoss: 0.6650392413139343 - trainLoss: 0.6635244488716125\n",
      "cnt: 0 - valLoss: 0.6650388240814209 - trainLoss: 0.6635240912437439\n",
      "cnt: 0 - valLoss: 0.6650382876396179 - trainLoss: 0.6635236740112305\n",
      "cnt: 0 - valLoss: 0.6650378704071045 - trainLoss: 0.663523256778717\n",
      "cnt: 0 - valLoss: 0.6650374531745911 - trainLoss: 0.6635229587554932\n",
      "cnt: 0 - valLoss: 0.6650369763374329 - trainLoss: 0.6635225415229797\n",
      "cnt: 0 - valLoss: 0.6650364995002747 - trainLoss: 0.6635221838951111\n",
      "cnt: 0 - valLoss: 0.6650360822677612 - trainLoss: 0.6635217666625977\n",
      "cnt: 0 - valLoss: 0.665035605430603 - trainLoss: 0.6635214686393738\n",
      "cnt: 0 - valLoss: 0.6650351881980896 - trainLoss: 0.6635210514068604\n",
      "cnt: 0 - valLoss: 0.6650347113609314 - trainLoss: 0.6635206937789917\n",
      "cnt: 0 - valLoss: 0.6650342345237732 - trainLoss: 0.663520336151123\n",
      "cnt: 0 - valLoss: 0.6650338172912598 - trainLoss: 0.6635199189186096\n",
      "cnt: 0 - valLoss: 0.6650333404541016 - trainLoss: 0.663519561290741\n",
      "cnt: 0 - valLoss: 0.6650329232215881 - trainLoss: 0.6635192036628723\n",
      "cnt: 0 - valLoss: 0.6650324463844299 - trainLoss: 0.6635188460350037\n",
      "cnt: 0 - valLoss: 0.6650319695472717 - trainLoss: 0.6635184288024902\n",
      "cnt: 0 - valLoss: 0.6650314927101135 - trainLoss: 0.6635180115699768\n",
      "cnt: 0 - valLoss: 0.6650310754776001 - trainLoss: 0.6635176539421082\n",
      "cnt: 0 - valLoss: 0.6650305986404419 - trainLoss: 0.6635172963142395\n",
      "cnt: 0 - valLoss: 0.6650301218032837 - trainLoss: 0.6635168790817261\n",
      "cnt: 0 - valLoss: 0.6650297045707703 - trainLoss: 0.6635165810585022\n",
      "cnt: 0 - valLoss: 0.6650292873382568 - trainLoss: 0.6635161638259888\n",
      "cnt: 0 - valLoss: 0.6650288105010986 - trainLoss: 0.6635158061981201\n",
      "cnt: 0 - valLoss: 0.6650283932685852 - trainLoss: 0.6635153889656067\n",
      "cnt: 0 - valLoss: 0.665027916431427 - trainLoss: 0.663515031337738\n",
      "cnt: 0 - valLoss: 0.6650274395942688 - trainLoss: 0.6635146141052246\n",
      "cnt: 0 - valLoss: 0.6650269627571106 - trainLoss: 0.6635143160820007\n",
      "cnt: 0 - valLoss: 0.6650265455245972 - trainLoss: 0.6635138988494873\n",
      "cnt: 0 - valLoss: 0.665026068687439 - trainLoss: 0.6635134816169739\n",
      "cnt: 0 - valLoss: 0.6650256514549255 - trainLoss: 0.66351318359375\n",
      "cnt: 0 - valLoss: 0.6650251746177673 - trainLoss: 0.6635127663612366\n",
      "cnt: 0 - valLoss: 0.6650247573852539 - trainLoss: 0.6635124087333679\n",
      "cnt: 0 - valLoss: 0.6650242805480957 - trainLoss: 0.6635120511054993\n",
      "cnt: 0 - valLoss: 0.6650238037109375 - trainLoss: 0.6635116338729858\n",
      "cnt: 0 - valLoss: 0.6650233864784241 - trainLoss: 0.6635112166404724\n",
      "cnt: 0 - valLoss: 0.6650229692459106 - trainLoss: 0.6635108590126038\n",
      "cnt: 0 - valLoss: 0.6650224924087524 - trainLoss: 0.6635105013847351\n",
      "cnt: 0 - valLoss: 0.6650219559669495 - trainLoss: 0.6635102033615112\n",
      "cnt: 0 - valLoss: 0.6650215983390808 - trainLoss: 0.6635097861289978\n",
      "cnt: 0 - valLoss: 0.6650211215019226 - trainLoss: 0.6635093688964844\n",
      "cnt: 0 - valLoss: 0.6650206446647644 - trainLoss: 0.6635090112686157\n",
      "cnt: 0 - valLoss: 0.6650201678276062 - trainLoss: 0.6635086536407471\n",
      "cnt: 0 - valLoss: 0.665019690990448 - trainLoss: 0.6635082364082336\n",
      "cnt: 0 - valLoss: 0.665019154548645 - trainLoss: 0.6635079383850098\n",
      "cnt: 0 - valLoss: 0.6650188565254211 - trainLoss: 0.6635074615478516\n",
      "cnt: 0 - valLoss: 0.6650183200836182 - trainLoss: 0.6635071635246277\n",
      "cnt: 0 - valLoss: 0.6650179028511047 - trainLoss: 0.6635067462921143\n",
      "cnt: 0 - valLoss: 0.6650174856185913 - trainLoss: 0.6635063886642456\n",
      "cnt: 0 - valLoss: 0.6650170087814331 - trainLoss: 0.6635059714317322\n",
      "cnt: 0 - valLoss: 0.6650165319442749 - trainLoss: 0.6635056734085083\n",
      "cnt: 0 - valLoss: 0.6650160551071167 - trainLoss: 0.6635052561759949\n",
      "cnt: 0 - valLoss: 0.6650156378746033 - trainLoss: 0.6635048985481262\n",
      "cnt: 0 - valLoss: 0.6650151610374451 - trainLoss: 0.6635044813156128\n",
      "cnt: 0 - valLoss: 0.6650146842002869 - trainLoss: 0.6635041236877441\n",
      "cnt: 0 - valLoss: 0.6650142073631287 - trainLoss: 0.6635037064552307\n",
      "cnt: 0 - valLoss: 0.6650137901306152 - trainLoss: 0.6635033488273621\n",
      "cnt: 0 - valLoss: 0.6650132536888123 - trainLoss: 0.6635029911994934\n",
      "cnt: 0 - valLoss: 0.6650128364562988 - trainLoss: 0.66350257396698\n",
      "cnt: 0 - valLoss: 0.6650123596191406 - trainLoss: 0.6635022163391113\n",
      "cnt: 0 - valLoss: 0.6650118827819824 - trainLoss: 0.6635017991065979\n",
      "cnt: 0 - valLoss: 0.665011465549469 - trainLoss: 0.663501501083374\n",
      "cnt: 0 - valLoss: 0.665010929107666 - trainLoss: 0.6635010838508606\n",
      "cnt: 0 - valLoss: 0.6650105118751526 - trainLoss: 0.6635006666183472\n",
      "cnt: 0 - valLoss: 0.6650099754333496 - trainLoss: 0.6635003685951233\n",
      "cnt: 0 - valLoss: 0.6650095582008362 - trainLoss: 0.6634998917579651\n",
      "cnt: 0 - valLoss: 0.6650092005729675 - trainLoss: 0.6634994745254517\n",
      "cnt: 0 - valLoss: 0.6650086641311646 - trainLoss: 0.663499116897583\n",
      "cnt: 0 - valLoss: 0.6650082468986511 - trainLoss: 0.6634987592697144\n",
      "cnt: 0 - valLoss: 0.6650077700614929 - trainLoss: 0.6634983420372009\n",
      "cnt: 0 - valLoss: 0.6650072336196899 - trainLoss: 0.6634979248046875\n",
      "cnt: 0 - valLoss: 0.6650068163871765 - trainLoss: 0.6634975671768188\n",
      "cnt: 0 - valLoss: 0.6650063395500183 - trainLoss: 0.6634971499443054\n",
      "cnt: 0 - valLoss: 0.6650058627128601 - trainLoss: 0.6634967923164368\n",
      "cnt: 0 - valLoss: 0.6650054454803467 - trainLoss: 0.6634964346885681\n",
      "cnt: 0 - valLoss: 0.6650049686431885 - trainLoss: 0.6634960770606995\n",
      "cnt: 0 - valLoss: 0.6650044918060303 - trainLoss: 0.663495659828186\n",
      "cnt: 0 - valLoss: 0.6650040149688721 - trainLoss: 0.6634952425956726\n",
      "cnt: 0 - valLoss: 0.6650035977363586 - trainLoss: 0.663494884967804\n",
      "cnt: 0 - valLoss: 0.6650031208992004 - trainLoss: 0.6634944677352905\n",
      "cnt: 0 - valLoss: 0.6650026440620422 - trainLoss: 0.6634940505027771\n",
      "cnt: 0 - valLoss: 0.665002167224884 - trainLoss: 0.6634937524795532\n",
      "cnt: 0 - valLoss: 0.6650016903877258 - trainLoss: 0.6634933352470398\n",
      "cnt: 0 - valLoss: 0.6650012731552124 - trainLoss: 0.6634929776191711\n",
      "cnt: 0 - valLoss: 0.6650007963180542 - trainLoss: 0.6634925603866577\n",
      "cnt: 0 - valLoss: 0.6650003790855408 - trainLoss: 0.6634921431541443\n",
      "cnt: 0 - valLoss: 0.6649998426437378 - trainLoss: 0.6634917259216309\n",
      "cnt: 0 - valLoss: 0.6649994254112244 - trainLoss: 0.6634913682937622\n",
      "cnt: 0 - valLoss: 0.6649989485740662 - trainLoss: 0.6634910702705383\n",
      "cnt: 0 - valLoss: 0.664998471736908 - trainLoss: 0.6634906530380249\n",
      "cnt: 0 - valLoss: 0.6649979948997498 - trainLoss: 0.6634902954101562\n",
      "cnt: 0 - valLoss: 0.6649975776672363 - trainLoss: 0.6634898781776428\n",
      "cnt: 0 - valLoss: 0.6649970412254333 - trainLoss: 0.6634894609451294\n",
      "cnt: 0 - valLoss: 0.6649966239929199 - trainLoss: 0.6634891033172607\n",
      "cnt: 0 - valLoss: 0.6649962067604065 - trainLoss: 0.6634886860847473\n",
      "cnt: 0 - valLoss: 0.6649956703186035 - trainLoss: 0.6634883284568787\n",
      "cnt: 0 - valLoss: 0.6649951934814453 - trainLoss: 0.6634879112243652\n",
      "cnt: 0 - valLoss: 0.6649947762489319 - trainLoss: 0.6634876132011414\n",
      "cnt: 0 - valLoss: 0.6649942994117737 - trainLoss: 0.6634871959686279\n",
      "cnt: 0 - valLoss: 0.6649938225746155 - trainLoss: 0.6634868383407593\n",
      "cnt: 0 - valLoss: 0.6649933457374573 - trainLoss: 0.6634864807128906\n",
      "cnt: 0 - valLoss: 0.6649929285049438 - trainLoss: 0.6634860634803772\n",
      "cnt: 0 - valLoss: 0.6649924516677856 - trainLoss: 0.6634857058525085\n",
      "cnt: 0 - valLoss: 0.6649919748306274 - trainLoss: 0.6634852290153503\n",
      "cnt: 0 - valLoss: 0.6649914979934692 - trainLoss: 0.6634848713874817\n",
      "cnt: 0 - valLoss: 0.6649910807609558 - trainLoss: 0.6634844541549683\n",
      "cnt: 0 - valLoss: 0.6649905443191528 - trainLoss: 0.6634841561317444\n",
      "cnt: 0 - valLoss: 0.6649901270866394 - trainLoss: 0.663483738899231\n",
      "cnt: 0 - valLoss: 0.6649896502494812 - trainLoss: 0.6634833812713623\n",
      "cnt: 0 - valLoss: 0.664989173412323 - trainLoss: 0.6634829044342041\n",
      "cnt: 0 - valLoss: 0.6649887561798096 - trainLoss: 0.6634826064109802\n",
      "cnt: 0 - valLoss: 0.6649882197380066 - trainLoss: 0.6634821891784668\n",
      "cnt: 0 - valLoss: 0.6649878025054932 - trainLoss: 0.6634818315505981\n",
      "cnt: 0 - valLoss: 0.6649872660636902 - trainLoss: 0.6634814143180847\n",
      "cnt: 0 - valLoss: 0.6649868488311768 - trainLoss: 0.6634810566902161\n",
      "cnt: 0 - valLoss: 0.6649863719940186 - trainLoss: 0.6634806394577026\n",
      "cnt: 0 - valLoss: 0.6649858951568604 - trainLoss: 0.663480281829834\n",
      "cnt: 0 - valLoss: 0.6649854183197021 - trainLoss: 0.6634798645973206\n",
      "cnt: 0 - valLoss: 0.664984941482544 - trainLoss: 0.6634795069694519\n",
      "cnt: 0 - valLoss: 0.6649845242500305 - trainLoss: 0.6634790897369385\n",
      "cnt: 0 - valLoss: 0.6649840474128723 - trainLoss: 0.6634787917137146\n",
      "cnt: 0 - valLoss: 0.6649835705757141 - trainLoss: 0.6634783744812012\n",
      "cnt: 0 - valLoss: 0.6649830937385559 - trainLoss: 0.6634780168533325\n",
      "cnt: 0 - valLoss: 0.6649826169013977 - trainLoss: 0.6634776592254639\n",
      "cnt: 0 - valLoss: 0.6649821996688843 - trainLoss: 0.6634772419929504\n",
      "cnt: 0 - valLoss: 0.6649816036224365 - trainLoss: 0.663476824760437\n",
      "cnt: 0 - valLoss: 0.6649811863899231 - trainLoss: 0.6634765267372131\n",
      "cnt: 0 - valLoss: 0.6649807095527649 - trainLoss: 0.6634760499000549\n",
      "cnt: 0 - valLoss: 0.6649802327156067 - trainLoss: 0.6634756922721863\n",
      "cnt: 0 - valLoss: 0.6649797558784485 - trainLoss: 0.6634753346443176\n",
      "cnt: 0 - valLoss: 0.6649793386459351 - trainLoss: 0.6634749174118042\n",
      "cnt: 0 - valLoss: 0.6649788618087769 - trainLoss: 0.6634745597839355\n",
      "cnt: 0 - valLoss: 0.6649783253669739 - trainLoss: 0.6634742617607117\n",
      "cnt: 0 - valLoss: 0.6649779081344604 - trainLoss: 0.6634737849235535\n",
      "cnt: 0 - valLoss: 0.6649774312973022 - trainLoss: 0.6634734869003296\n",
      "cnt: 0 - valLoss: 0.6649768948554993 - trainLoss: 0.6634730100631714\n",
      "cnt: 0 - valLoss: 0.6649764180183411 - trainLoss: 0.6634727120399475\n",
      "cnt: 0 - valLoss: 0.6649760007858276 - trainLoss: 0.6634722948074341\n",
      "cnt: 0 - valLoss: 0.6649755239486694 - trainLoss: 0.6634719371795654\n",
      "cnt: 0 - valLoss: 0.6649750471115112 - trainLoss: 0.663471519947052\n",
      "cnt: 0 - valLoss: 0.664974570274353 - trainLoss: 0.6634711623191833\n",
      "cnt: 0 - valLoss: 0.6649740934371948 - trainLoss: 0.6634707450866699\n",
      "cnt: 0 - valLoss: 0.6649736762046814 - trainLoss: 0.6634703874588013\n",
      "cnt: 0 - valLoss: 0.6649731397628784 - trainLoss: 0.6634700298309326\n",
      "cnt: 0 - valLoss: 0.664972722530365 - trainLoss: 0.663469672203064\n",
      "cnt: 0 - valLoss: 0.6649722456932068 - trainLoss: 0.6634692549705505\n",
      "cnt: 0 - valLoss: 0.6649717688560486 - trainLoss: 0.6634688973426819\n",
      "cnt: 0 - valLoss: 0.6649712920188904 - trainLoss: 0.6634684801101685\n",
      "cnt: 0 - valLoss: 0.6649708151817322 - trainLoss: 0.6634681820869446\n",
      "cnt: 0 - valLoss: 0.664970338344574 - trainLoss: 0.6634677052497864\n",
      "cnt: 0 - valLoss: 0.6649698615074158 - trainLoss: 0.6634673476219177\n",
      "cnt: 0 - valLoss: 0.6649693846702576 - trainLoss: 0.6634669303894043\n",
      "cnt: 0 - valLoss: 0.6649689078330994 - trainLoss: 0.6634666323661804\n",
      "cnt: 0 - valLoss: 0.6649683713912964 - trainLoss: 0.663466215133667\n",
      "cnt: 0 - valLoss: 0.6649678945541382 - trainLoss: 0.6634658575057983\n",
      "cnt: 0 - valLoss: 0.6649674773216248 - trainLoss: 0.6634654402732849\n",
      "cnt: 0 - valLoss: 0.6649669408798218 - trainLoss: 0.6634650826454163\n",
      "cnt: 0 - valLoss: 0.6649665236473083 - trainLoss: 0.6634646654129028\n",
      "cnt: 0 - valLoss: 0.6649660468101501 - trainLoss: 0.6634643077850342\n",
      "cnt: 0 - valLoss: 0.6649655699729919 - trainLoss: 0.6634638905525208\n",
      "cnt: 0 - valLoss: 0.6649650931358337 - trainLoss: 0.6634635329246521\n",
      "cnt: 0 - valLoss: 0.6649646759033203 - trainLoss: 0.6634631752967834\n",
      "cnt: 0 - valLoss: 0.6649641394615173 - trainLoss: 0.66346275806427\n",
      "cnt: 0 - valLoss: 0.6649636626243591 - trainLoss: 0.6634624004364014\n",
      "cnt: 0 - valLoss: 0.6649631857872009 - trainLoss: 0.6634620428085327\n",
      "cnt: 0 - valLoss: 0.6649627089500427 - trainLoss: 0.6634616851806641\n",
      "cnt: 0 - valLoss: 0.6649622321128845 - trainLoss: 0.6634612679481506\n",
      "cnt: 0 - valLoss: 0.6649617552757263 - trainLoss: 0.6634608507156372\n",
      "cnt: 0 - valLoss: 0.6649613380432129 - trainLoss: 0.6634604930877686\n",
      "cnt: 0 - valLoss: 0.6649608016014099 - trainLoss: 0.6634601354598999\n",
      "cnt: 0 - valLoss: 0.6649603247642517 - trainLoss: 0.6634597778320312\n",
      "cnt: 0 - valLoss: 0.6649599075317383 - trainLoss: 0.6634593605995178\n",
      "cnt: 0 - valLoss: 0.6649594306945801 - trainLoss: 0.6634590029716492\n",
      "cnt: 0 - valLoss: 0.6649588942527771 - trainLoss: 0.6634585857391357\n",
      "cnt: 0 - valLoss: 0.6649584174156189 - trainLoss: 0.6634582281112671\n",
      "cnt: 0 - valLoss: 0.6649579405784607 - trainLoss: 0.6634578704833984\n",
      "cnt: 0 - valLoss: 0.6649574637413025 - trainLoss: 0.663457453250885\n",
      "cnt: 0 - valLoss: 0.6649569869041443 - trainLoss: 0.6634570956230164\n",
      "cnt: 0 - valLoss: 0.6649565100669861 - trainLoss: 0.6634566783905029\n",
      "cnt: 0 - valLoss: 0.6649560928344727 - trainLoss: 0.6634563207626343\n",
      "cnt: 0 - valLoss: 0.6649556159973145 - trainLoss: 0.6634559035301208\n",
      "cnt: 0 - valLoss: 0.6649550795555115 - trainLoss: 0.6634555459022522\n",
      "cnt: 0 - valLoss: 0.6649546027183533 - trainLoss: 0.6634551286697388\n",
      "cnt: 0 - valLoss: 0.6649541258811951 - trainLoss: 0.6634547710418701\n",
      "cnt: 0 - valLoss: 0.6649536490440369 - trainLoss: 0.6634543538093567\n",
      "cnt: 0 - valLoss: 0.6649532318115234 - trainLoss: 0.663453996181488\n",
      "cnt: 0 - valLoss: 0.6649526357650757 - trainLoss: 0.6634535789489746\n",
      "cnt: 0 - valLoss: 0.6649522185325623 - trainLoss: 0.6634532809257507\n",
      "cnt: 0 - valLoss: 0.6649516820907593 - trainLoss: 0.6634528040885925\n",
      "cnt: 0 - valLoss: 0.6649512648582458 - trainLoss: 0.6634524464607239\n",
      "cnt: 0 - valLoss: 0.6649507880210876 - trainLoss: 0.6634520888328552\n",
      "cnt: 0 - valLoss: 0.6649503111839294 - trainLoss: 0.6634516716003418\n",
      "cnt: 0 - valLoss: 0.6649498343467712 - trainLoss: 0.6634513139724731\n",
      "cnt: 0 - valLoss: 0.6649492979049683 - trainLoss: 0.6634508967399597\n",
      "cnt: 0 - valLoss: 0.6649488806724548 - trainLoss: 0.6634505391120911\n",
      "cnt: 0 - valLoss: 0.6649483442306519 - trainLoss: 0.6634501218795776\n",
      "cnt: 0 - valLoss: 0.6649478673934937 - trainLoss: 0.663449764251709\n",
      "cnt: 0 - valLoss: 0.6649473309516907 - trainLoss: 0.6634493470191956\n",
      "cnt: 0 - valLoss: 0.6649469137191772 - trainLoss: 0.6634489297866821\n",
      "cnt: 0 - valLoss: 0.6649464964866638 - trainLoss: 0.6634485721588135\n",
      "cnt: 0 - valLoss: 0.6649459600448608 - trainLoss: 0.6634482145309448\n",
      "cnt: 0 - valLoss: 0.6649454832077026 - trainLoss: 0.6634477972984314\n",
      "cnt: 0 - valLoss: 0.6649450063705444 - trainLoss: 0.6634474396705627\n",
      "cnt: 0 - valLoss: 0.6649445295333862 - trainLoss: 0.6634470224380493\n",
      "cnt: 0 - valLoss: 0.664944052696228 - trainLoss: 0.6634466648101807\n",
      "cnt: 0 - valLoss: 0.6649435758590698 - trainLoss: 0.6634462475776672\n",
      "cnt: 0 - valLoss: 0.6649430394172668 - trainLoss: 0.6634458899497986\n",
      "cnt: 0 - valLoss: 0.6649426221847534 - trainLoss: 0.6634455323219299\n",
      "cnt: 0 - valLoss: 0.6649420857429504 - trainLoss: 0.6634451150894165\n",
      "cnt: 0 - valLoss: 0.6649416089057922 - trainLoss: 0.6634447574615479\n",
      "cnt: 0 - valLoss: 0.664941132068634 - trainLoss: 0.6634443402290344\n",
      "cnt: 0 - valLoss: 0.6649406552314758 - trainLoss: 0.6634439826011658\n",
      "cnt: 0 - valLoss: 0.6649402379989624 - trainLoss: 0.6634435653686523\n",
      "cnt: 0 - valLoss: 0.6649397015571594 - trainLoss: 0.6634431481361389\n",
      "cnt: 0 - valLoss: 0.6649392247200012 - trainLoss: 0.6634427309036255\n",
      "cnt: 0 - valLoss: 0.664938747882843 - trainLoss: 0.6634423732757568\n",
      "cnt: 0 - valLoss: 0.6649382710456848 - trainLoss: 0.663442075252533\n",
      "cnt: 0 - valLoss: 0.6649377942085266 - trainLoss: 0.6634415984153748\n",
      "cnt: 0 - valLoss: 0.6649373173713684 - trainLoss: 0.6634411811828613\n",
      "cnt: 0 - valLoss: 0.6649368405342102 - trainLoss: 0.6634408235549927\n",
      "cnt: 0 - valLoss: 0.6649363040924072 - trainLoss: 0.663440465927124\n",
      "cnt: 0 - valLoss: 0.664935827255249 - trainLoss: 0.6634400486946106\n",
      "cnt: 0 - valLoss: 0.6649353504180908 - trainLoss: 0.6634396314620972\n",
      "cnt: 0 - valLoss: 0.6649348735809326 - trainLoss: 0.6634393334388733\n",
      "cnt: 0 - valLoss: 0.6649343967437744 - trainLoss: 0.6634388566017151\n",
      "cnt: 0 - valLoss: 0.6649338603019714 - trainLoss: 0.6634385585784912\n",
      "cnt: 0 - valLoss: 0.664933443069458 - trainLoss: 0.663438081741333\n",
      "cnt: 0 - valLoss: 0.6649329662322998 - trainLoss: 0.6634377837181091\n",
      "cnt: 0 - valLoss: 0.6649324297904968 - trainLoss: 0.6634373664855957\n",
      "cnt: 0 - valLoss: 0.6649319529533386 - trainLoss: 0.6634369492530823\n",
      "cnt: 0 - valLoss: 0.6649314761161804 - trainLoss: 0.6634365916252136\n",
      "cnt: 0 - valLoss: 0.6649309396743774 - trainLoss: 0.6634361743927002\n",
      "cnt: 0 - valLoss: 0.664930522441864 - trainLoss: 0.6634358167648315\n",
      "cnt: 0 - valLoss: 0.6649300456047058 - trainLoss: 0.6634353995323181\n",
      "cnt: 0 - valLoss: 0.6649295687675476 - trainLoss: 0.6634349822998047\n",
      "cnt: 0 - valLoss: 0.6649290323257446 - trainLoss: 0.663434624671936\n",
      "cnt: 0 - valLoss: 0.6649285554885864 - trainLoss: 0.6634342670440674\n",
      "cnt: 0 - valLoss: 0.6649280786514282 - trainLoss: 0.663433849811554\n",
      "cnt: 0 - valLoss: 0.6649275422096252 - trainLoss: 0.6634334325790405\n",
      "cnt: 0 - valLoss: 0.6649271249771118 - trainLoss: 0.6634330749511719\n",
      "cnt: 0 - valLoss: 0.6649266481399536 - trainLoss: 0.6634326577186584\n",
      "cnt: 0 - valLoss: 0.6649261713027954 - trainLoss: 0.6634323000907898\n",
      "cnt: 0 - valLoss: 0.6649256348609924 - trainLoss: 0.6634318828582764\n",
      "cnt: 0 - valLoss: 0.664925217628479 - trainLoss: 0.6634314060211182\n",
      "cnt: 0 - valLoss: 0.664924681186676 - trainLoss: 0.6634311079978943\n",
      "cnt: 0 - valLoss: 0.6649242639541626 - trainLoss: 0.6634307503700256\n",
      "cnt: 0 - valLoss: 0.6649236679077148 - trainLoss: 0.6634303331375122\n",
      "cnt: 0 - valLoss: 0.6649232506752014 - trainLoss: 0.6634299755096436\n",
      "cnt: 0 - valLoss: 0.6649227142333984 - trainLoss: 0.6634295582771301\n",
      "cnt: 0 - valLoss: 0.6649222373962402 - trainLoss: 0.6634291410446167\n",
      "cnt: 0 - valLoss: 0.6649218201637268 - trainLoss: 0.663428783416748\n",
      "cnt: 0 - valLoss: 0.6649213433265686 - trainLoss: 0.6634283661842346\n",
      "cnt: 0 - valLoss: 0.6649207472801208 - trainLoss: 0.663428008556366\n",
      "cnt: 0 - valLoss: 0.6649203300476074 - trainLoss: 0.6634276509284973\n",
      "cnt: 0 - valLoss: 0.6649197936058044 - trainLoss: 0.6634272336959839\n",
      "cnt: 0 - valLoss: 0.6649193167686462 - trainLoss: 0.6634268164634705\n",
      "cnt: 0 - valLoss: 0.664918839931488 - trainLoss: 0.6634264588356018\n",
      "cnt: 0 - valLoss: 0.6649183630943298 - trainLoss: 0.6634260416030884\n",
      "cnt: 0 - valLoss: 0.6649178266525269 - trainLoss: 0.6634256839752197\n",
      "cnt: 0 - valLoss: 0.6649173498153687 - trainLoss: 0.6634252667427063\n",
      "cnt: 0 - valLoss: 0.6649169325828552 - trainLoss: 0.6634249091148376\n",
      "cnt: 0 - valLoss: 0.6649163961410522 - trainLoss: 0.6634244918823242\n",
      "cnt: 0 - valLoss: 0.6649158596992493 - trainLoss: 0.6634241342544556\n",
      "cnt: 0 - valLoss: 0.6649153828620911 - trainLoss: 0.6634236574172974\n",
      "cnt: 0 - valLoss: 0.6649149060249329 - trainLoss: 0.6634232997894287\n",
      "cnt: 0 - valLoss: 0.6649144291877747 - trainLoss: 0.6634229421615601\n",
      "cnt: 0 - valLoss: 0.6649138927459717 - trainLoss: 0.6634225249290466\n",
      "cnt: 0 - valLoss: 0.6649134159088135 - trainLoss: 0.663422167301178\n",
      "cnt: 0 - valLoss: 0.6649129390716553 - trainLoss: 0.6634217500686646\n",
      "cnt: 0 - valLoss: 0.6649124622344971 - trainLoss: 0.6634213328361511\n",
      "cnt: 0 - valLoss: 0.6649119257926941 - trainLoss: 0.6634209752082825\n",
      "cnt: 0 - valLoss: 0.6649115085601807 - trainLoss: 0.663420557975769\n",
      "cnt: 0 - valLoss: 0.6649109721183777 - trainLoss: 0.6634202003479004\n",
      "cnt: 0 - valLoss: 0.6649104952812195 - trainLoss: 0.663419783115387\n",
      "cnt: 0 - valLoss: 0.6649100184440613 - trainLoss: 0.6634194254875183\n",
      "cnt: 0 - valLoss: 0.6649095416069031 - trainLoss: 0.6634189486503601\n",
      "cnt: 0 - valLoss: 0.6649090051651001 - trainLoss: 0.6634186506271362\n",
      "cnt: 0 - valLoss: 0.6649085283279419 - trainLoss: 0.6634182333946228\n",
      "cnt: 0 - valLoss: 0.6649080514907837 - trainLoss: 0.6634178161621094\n",
      "cnt: 0 - valLoss: 0.6649075746536255 - trainLoss: 0.6634174585342407\n",
      "cnt: 0 - valLoss: 0.6649070382118225 - trainLoss: 0.6634170413017273\n",
      "cnt: 0 - valLoss: 0.6649065613746643 - trainLoss: 0.6634166240692139\n",
      "cnt: 0 - valLoss: 0.6649060845375061 - trainLoss: 0.6634162664413452\n",
      "cnt: 0 - valLoss: 0.6649056077003479 - trainLoss: 0.6634158492088318\n",
      "cnt: 0 - valLoss: 0.6649051308631897 - trainLoss: 0.6634154915809631\n",
      "cnt: 0 - valLoss: 0.6649045348167419 - trainLoss: 0.6634150743484497\n",
      "cnt: 0 - valLoss: 0.6649041175842285 - trainLoss: 0.663414716720581\n",
      "cnt: 0 - valLoss: 0.6649036407470703 - trainLoss: 0.6634142994880676\n",
      "cnt: 0 - valLoss: 0.6649031639099121 - trainLoss: 0.6634138822555542\n",
      "cnt: 0 - valLoss: 0.6649025678634644 - trainLoss: 0.6634135246276855\n",
      "cnt: 0 - valLoss: 0.6649020910263062 - trainLoss: 0.6634131073951721\n",
      "cnt: 0 - valLoss: 0.6649016737937927 - trainLoss: 0.6634127497673035\n",
      "cnt: 0 - valLoss: 0.6649011373519897 - trainLoss: 0.6634122729301453\n",
      "cnt: 0 - valLoss: 0.6649006009101868 - trainLoss: 0.6634118556976318\n",
      "cnt: 0 - valLoss: 0.6649001240730286 - trainLoss: 0.6634114980697632\n",
      "cnt: 0 - valLoss: 0.6648996472358704 - trainLoss: 0.6634111404418945\n",
      "cnt: 0 - valLoss: 0.6648991703987122 - trainLoss: 0.6634107232093811\n",
      "cnt: 0 - valLoss: 0.6648986339569092 - trainLoss: 0.6634103059768677\n",
      "cnt: 0 - valLoss: 0.664898157119751 - trainLoss: 0.663409948348999\n",
      "cnt: 0 - valLoss: 0.6648976802825928 - trainLoss: 0.6634095311164856\n",
      "cnt: 0 - valLoss: 0.664897084236145 - trainLoss: 0.6634091138839722\n",
      "cnt: 0 - valLoss: 0.6648966670036316 - trainLoss: 0.6634086966514587\n",
      "cnt: 0 - valLoss: 0.6648961901664734 - trainLoss: 0.6634083390235901\n",
      "cnt: 0 - valLoss: 0.6648957133293152 - trainLoss: 0.6634078621864319\n",
      "cnt: 0 - valLoss: 0.664895236492157 - trainLoss: 0.6634074449539185\n",
      "cnt: 0 - valLoss: 0.664894700050354 - trainLoss: 0.6634071469306946\n",
      "cnt: 0 - valLoss: 0.6648942232131958 - trainLoss: 0.6634067296981812\n",
      "cnt: 0 - valLoss: 0.6648937463760376 - trainLoss: 0.663406252861023\n",
      "cnt: 0 - valLoss: 0.6648932695388794 - trainLoss: 0.6634058952331543\n",
      "cnt: 0 - valLoss: 0.6648927330970764 - trainLoss: 0.6634055376052856\n",
      "cnt: 0 - valLoss: 0.6648921966552734 - trainLoss: 0.663405179977417\n",
      "cnt: 0 - valLoss: 0.6648917198181152 - trainLoss: 0.6634047031402588\n",
      "cnt: 0 - valLoss: 0.6648913025856018 - trainLoss: 0.6634043455123901\n",
      "cnt: 0 - valLoss: 0.6648907661437988 - trainLoss: 0.6634039282798767\n",
      "cnt: 0 - valLoss: 0.6648902893066406 - trainLoss: 0.6634035110473633\n",
      "cnt: 0 - valLoss: 0.6648897528648376 - trainLoss: 0.6634030938148499\n",
      "cnt: 0 - valLoss: 0.6648892760276794 - trainLoss: 0.6634027361869812\n",
      "cnt: 0 - valLoss: 0.6648887991905212 - trainLoss: 0.6634023189544678\n",
      "cnt: 0 - valLoss: 0.6648882627487183 - trainLoss: 0.6634019613265991\n",
      "cnt: 0 - valLoss: 0.6648877859115601 - trainLoss: 0.6634014844894409\n",
      "cnt: 0 - valLoss: 0.6648873090744019 - trainLoss: 0.6634011268615723\n",
      "cnt: 0 - valLoss: 0.6648868322372437 - trainLoss: 0.6634007096290588\n",
      "cnt: 0 - valLoss: 0.6648862957954407 - trainLoss: 0.6634002923965454\n",
      "cnt: 0 - valLoss: 0.6648858189582825 - trainLoss: 0.6633999347686768\n",
      "cnt: 0 - valLoss: 0.6648853421211243 - trainLoss: 0.6633995175361633\n",
      "cnt: 0 - valLoss: 0.6648848056793213 - trainLoss: 0.6633991003036499\n",
      "cnt: 0 - valLoss: 0.6648843288421631 - trainLoss: 0.6633987426757812\n",
      "cnt: 0 - valLoss: 0.6648837924003601 - trainLoss: 0.6633983254432678\n",
      "cnt: 0 - valLoss: 0.6648833155632019 - trainLoss: 0.6633978486061096\n",
      "cnt: 0 - valLoss: 0.6648828983306885 - trainLoss: 0.663397490978241\n",
      "cnt: 0 - valLoss: 0.6648823618888855 - trainLoss: 0.6633971333503723\n",
      "cnt: 0 - valLoss: 0.6648818254470825 - trainLoss: 0.6633966565132141\n",
      "cnt: 0 - valLoss: 0.6648813486099243 - trainLoss: 0.6633963584899902\n",
      "cnt: 0 - valLoss: 0.6648809313774109 - trainLoss: 0.663395881652832\n",
      "cnt: 0 - valLoss: 0.6648803353309631 - trainLoss: 0.6633955240249634\n",
      "cnt: 0 - valLoss: 0.6648798584938049 - trainLoss: 0.6633951663970947\n",
      "cnt: 0 - valLoss: 0.6648793816566467 - trainLoss: 0.6633946895599365\n",
      "cnt: 0 - valLoss: 0.6648788452148438 - trainLoss: 0.6633943319320679\n",
      "cnt: 0 - valLoss: 0.6648784279823303 - trainLoss: 0.6633939146995544\n",
      "cnt: 0 - valLoss: 0.6648778319358826 - trainLoss: 0.663393497467041\n",
      "cnt: 0 - valLoss: 0.6648774147033691 - trainLoss: 0.6633930206298828\n",
      "cnt: 0 - valLoss: 0.6648768782615662 - trainLoss: 0.6633926630020142\n",
      "cnt: 0 - valLoss: 0.6648763418197632 - trainLoss: 0.6633923053741455\n",
      "cnt: 0 - valLoss: 0.664875864982605 - trainLoss: 0.6633918881416321\n",
      "cnt: 0 - valLoss: 0.6648753881454468 - trainLoss: 0.6633914709091187\n",
      "cnt: 0 - valLoss: 0.6648748517036438 - trainLoss: 0.66339111328125\n",
      "cnt: 0 - valLoss: 0.6648743748664856 - trainLoss: 0.6633906960487366\n",
      "cnt: 0 - valLoss: 0.6648738980293274 - trainLoss: 0.6633902788162231\n",
      "cnt: 0 - valLoss: 0.6648734211921692 - trainLoss: 0.6633899211883545\n",
      "cnt: 0 - valLoss: 0.6648728847503662 - trainLoss: 0.6633895039558411\n",
      "cnt: 0 - valLoss: 0.6648723483085632 - trainLoss: 0.6633890867233276\n",
      "cnt: 0 - valLoss: 0.6648719310760498 - trainLoss: 0.663388729095459\n",
      "cnt: 0 - valLoss: 0.6648713946342468 - trainLoss: 0.6633883118629456\n",
      "cnt: 0 - valLoss: 0.6648709177970886 - trainLoss: 0.6633878350257874\n",
      "cnt: 0 - valLoss: 0.6648703813552856 - trainLoss: 0.6633874773979187\n",
      "cnt: 0 - valLoss: 0.6648698449134827 - trainLoss: 0.6633870601654053\n",
      "cnt: 0 - valLoss: 0.6648694276809692 - trainLoss: 0.6633866429328918\n",
      "cnt: 0 - valLoss: 0.6648688912391663 - trainLoss: 0.6633862257003784\n",
      "cnt: 0 - valLoss: 0.6648684144020081 - trainLoss: 0.6633858680725098\n",
      "cnt: 0 - valLoss: 0.6648679375648499 - trainLoss: 0.6633854508399963\n",
      "cnt: 0 - valLoss: 0.6648674011230469 - trainLoss: 0.6633850932121277\n",
      "cnt: 0 - valLoss: 0.6648668646812439 - trainLoss: 0.6633846759796143\n",
      "cnt: 0 - valLoss: 0.6648663878440857 - trainLoss: 0.6633842587471008\n",
      "cnt: 0 - valLoss: 0.6648658514022827 - trainLoss: 0.6633838415145874\n",
      "cnt: 0 - valLoss: 0.6648653745651245 - trainLoss: 0.663383424282074\n",
      "cnt: 0 - valLoss: 0.6648648977279663 - trainLoss: 0.6633830666542053\n",
      "cnt: 0 - valLoss: 0.6648643612861633 - trainLoss: 0.6633826494216919\n",
      "cnt: 0 - valLoss: 0.6648638844490051 - trainLoss: 0.6633822321891785\n",
      "cnt: 0 - valLoss: 0.6648633480072021 - trainLoss: 0.663381814956665\n",
      "cnt: 0 - valLoss: 0.664862871170044 - trainLoss: 0.6633813977241516\n",
      "cnt: 0 - valLoss: 0.664862334728241 - trainLoss: 0.6633809804916382\n",
      "cnt: 0 - valLoss: 0.6648618578910828 - trainLoss: 0.6633805632591248\n",
      "cnt: 0 - valLoss: 0.6648613810539246 - trainLoss: 0.6633801460266113\n",
      "cnt: 0 - valLoss: 0.6648607850074768 - trainLoss: 0.6633797287940979\n",
      "cnt: 0 - valLoss: 0.6648603081703186 - trainLoss: 0.6633793711662292\n",
      "cnt: 0 - valLoss: 0.6648598313331604 - trainLoss: 0.663378894329071\n",
      "cnt: 0 - valLoss: 0.6648592948913574 - trainLoss: 0.6633785367012024\n",
      "cnt: 0 - valLoss: 0.6648587584495544 - trainLoss: 0.663378119468689\n",
      "cnt: 0 - valLoss: 0.6648582220077515 - trainLoss: 0.6633777022361755\n",
      "cnt: 0 - valLoss: 0.6648577451705933 - trainLoss: 0.6633772253990173\n",
      "cnt: 0 - valLoss: 0.6648572683334351 - trainLoss: 0.6633768081665039\n",
      "cnt: 0 - valLoss: 0.6648567914962769 - trainLoss: 0.6633764505386353\n",
      "cnt: 0 - valLoss: 0.6648561954498291 - trainLoss: 0.6633760333061218\n",
      "cnt: 0 - valLoss: 0.6648557186126709 - trainLoss: 0.6633756160736084\n",
      "cnt: 0 - valLoss: 0.6648551821708679 - trainLoss: 0.6633752584457397\n",
      "cnt: 0 - valLoss: 0.6648546457290649 - trainLoss: 0.6633747816085815\n",
      "cnt: 0 - valLoss: 0.6648542284965515 - trainLoss: 0.6633743643760681\n",
      "cnt: 0 - valLoss: 0.6648536920547485 - trainLoss: 0.6633739471435547\n",
      "cnt: 0 - valLoss: 0.6648531556129456 - trainLoss: 0.663373589515686\n",
      "cnt: 0 - valLoss: 0.6648526787757874 - trainLoss: 0.6633731126785278\n",
      "cnt: 0 - valLoss: 0.6648521423339844 - trainLoss: 0.6633726954460144\n",
      "cnt: 0 - valLoss: 0.6648516654968262 - trainLoss: 0.6633723378181458\n",
      "cnt: 0 - valLoss: 0.6648511290550232 - trainLoss: 0.6633718609809875\n",
      "cnt: 0 - valLoss: 0.664850652217865 - trainLoss: 0.6633715033531189\n",
      "cnt: 0 - valLoss: 0.664850115776062 - trainLoss: 0.6633710861206055\n",
      "cnt: 0 - valLoss: 0.664849579334259 - trainLoss: 0.663370668888092\n",
      "cnt: 0 - valLoss: 0.6648491024971008 - trainLoss: 0.6633702516555786\n",
      "cnt: 0 - valLoss: 0.6648485660552979 - trainLoss: 0.6633697748184204\n",
      "cnt: 0 - valLoss: 0.6648480892181396 - trainLoss: 0.663369357585907\n",
      "cnt: 0 - valLoss: 0.6648475527763367 - trainLoss: 0.6633689999580383\n",
      "cnt: 0 - valLoss: 0.6648470163345337 - trainLoss: 0.6633685231208801\n",
      "cnt: 0 - valLoss: 0.6648465394973755 - trainLoss: 0.6633681654930115\n",
      "cnt: 0 - valLoss: 0.6648460030555725 - trainLoss: 0.6633676886558533\n",
      "cnt: 0 - valLoss: 0.6648455262184143 - trainLoss: 0.6633673310279846\n",
      "cnt: 0 - valLoss: 0.6648449897766113 - trainLoss: 0.6633669137954712\n",
      "cnt: 0 - valLoss: 0.6648444533348083 - trainLoss: 0.663366436958313\n",
      "cnt: 0 - valLoss: 0.6648439764976501 - trainLoss: 0.6633660197257996\n",
      "cnt: 0 - valLoss: 0.6648434400558472 - trainLoss: 0.6633656620979309\n",
      "cnt: 0 - valLoss: 0.6648429036140442 - trainLoss: 0.6633651852607727\n",
      "cnt: 0 - valLoss: 0.664842426776886 - trainLoss: 0.6633647680282593\n",
      "cnt: 0 - valLoss: 0.6648419499397278 - trainLoss: 0.6633643507957458\n",
      "cnt: 0 - valLoss: 0.6648414134979248 - trainLoss: 0.6633639335632324\n",
      "cnt: 0 - valLoss: 0.6648408770561218 - trainLoss: 0.663363516330719\n",
      "cnt: 0 - valLoss: 0.6648403406143188 - trainLoss: 0.6633630990982056\n",
      "cnt: 0 - valLoss: 0.6648398637771606 - trainLoss: 0.6633626222610474\n",
      "cnt: 0 - valLoss: 0.6648393273353577 - trainLoss: 0.6633622050285339\n",
      "cnt: 0 - valLoss: 0.6648387908935547 - trainLoss: 0.6633618474006653\n",
      "cnt: 0 - valLoss: 0.6648383140563965 - trainLoss: 0.6633613705635071\n",
      "cnt: 0 - valLoss: 0.6648377776145935 - trainLoss: 0.6633609533309937\n",
      "cnt: 0 - valLoss: 0.6648373007774353 - trainLoss: 0.6633605360984802\n",
      "cnt: 0 - valLoss: 0.6648367643356323 - trainLoss: 0.6633601188659668\n",
      "cnt: 0 - valLoss: 0.6648362874984741 - trainLoss: 0.6633597612380981\n",
      "cnt: 0 - valLoss: 0.6648357510566711 - trainLoss: 0.6633592844009399\n",
      "cnt: 0 - valLoss: 0.6648352742195129 - trainLoss: 0.6633588671684265\n",
      "cnt: 0 - valLoss: 0.66483473777771 - trainLoss: 0.6633583903312683\n",
      "cnt: 0 - valLoss: 0.664834201335907 - trainLoss: 0.6633579730987549\n",
      "cnt: 0 - valLoss: 0.664833664894104 - trainLoss: 0.6633576154708862\n",
      "cnt: 0 - valLoss: 0.6648332476615906 - trainLoss: 0.6633571982383728\n",
      "cnt: 0 - valLoss: 0.6648326516151428 - trainLoss: 0.6633567810058594\n",
      "cnt: 0 - valLoss: 0.6648321747779846 - trainLoss: 0.6633563041687012\n",
      "cnt: 0 - valLoss: 0.6648316383361816 - trainLoss: 0.6633559465408325\n",
      "cnt: 0 - valLoss: 0.6648311614990234 - trainLoss: 0.6633555293083191\n",
      "cnt: 0 - valLoss: 0.6648305654525757 - trainLoss: 0.6633550524711609\n",
      "cnt: 0 - valLoss: 0.6648300886154175 - trainLoss: 0.6633545756340027\n",
      "cnt: 0 - valLoss: 0.6648296117782593 - trainLoss: 0.663354218006134\n",
      "cnt: 0 - valLoss: 0.6648290157318115 - trainLoss: 0.6633538007736206\n",
      "cnt: 0 - valLoss: 0.6648285388946533 - trainLoss: 0.6633533835411072\n",
      "cnt: 0 - valLoss: 0.6648280024528503 - trainLoss: 0.663352906703949\n",
      "cnt: 0 - valLoss: 0.6648275256156921 - trainLoss: 0.6633524894714355\n",
      "cnt: 0 - valLoss: 0.6648269891738892 - trainLoss: 0.6633520722389221\n",
      "cnt: 0 - valLoss: 0.6648264527320862 - trainLoss: 0.6633515954017639\n",
      "cnt: 0 - valLoss: 0.664825975894928 - trainLoss: 0.6633512377738953\n",
      "cnt: 0 - valLoss: 0.6648253798484802 - trainLoss: 0.6633508205413818\n",
      "cnt: 0 - valLoss: 0.664824903011322 - trainLoss: 0.6633503437042236\n",
      "cnt: 0 - valLoss: 0.6648244261741638 - trainLoss: 0.6633499264717102\n",
      "cnt: 0 - valLoss: 0.6648238897323608 - trainLoss: 0.6633495688438416\n",
      "cnt: 0 - valLoss: 0.6648233532905579 - trainLoss: 0.6633491516113281\n",
      "cnt: 0 - valLoss: 0.6648228764533997 - trainLoss: 0.6633486747741699\n",
      "cnt: 0 - valLoss: 0.6648223400115967 - trainLoss: 0.6633482575416565\n",
      "cnt: 0 - valLoss: 0.6648218035697937 - trainLoss: 0.6633477807044983\n",
      "cnt: 0 - valLoss: 0.6648212671279907 - trainLoss: 0.6633473634719849\n",
      "cnt: 0 - valLoss: 0.6648207306861877 - trainLoss: 0.6633470058441162\n",
      "cnt: 0 - valLoss: 0.6648202538490295 - trainLoss: 0.663346529006958\n",
      "cnt: 0 - valLoss: 0.6648197174072266 - trainLoss: 0.6633461117744446\n",
      "cnt: 0 - valLoss: 0.6648192405700684 - trainLoss: 0.6633456349372864\n",
      "cnt: 0 - valLoss: 0.6648186445236206 - trainLoss: 0.6633453369140625\n",
      "cnt: 0 - valLoss: 0.6648181676864624 - trainLoss: 0.6633448600769043\n",
      "cnt: 0 - valLoss: 0.6648176312446594 - trainLoss: 0.6633444428443909\n",
      "cnt: 0 - valLoss: 0.6648170948028564 - trainLoss: 0.6633439660072327\n",
      "cnt: 0 - valLoss: 0.6648166179656982 - trainLoss: 0.6633435487747192\n",
      "cnt: 0 - valLoss: 0.6648160815238953 - trainLoss: 0.663343071937561\n",
      "cnt: 0 - valLoss: 0.6648156046867371 - trainLoss: 0.6633427143096924\n",
      "cnt: 0 - valLoss: 0.6648150682449341 - trainLoss: 0.663342297077179\n",
      "cnt: 0 - valLoss: 0.6648145318031311 - trainLoss: 0.6633418798446655\n",
      "cnt: 0 - valLoss: 0.6648139953613281 - trainLoss: 0.6633414030075073\n",
      "cnt: 0 - valLoss: 0.6648134589195251 - trainLoss: 0.6633409261703491\n",
      "cnt: 0 - valLoss: 0.6648129820823669 - trainLoss: 0.6633405089378357\n",
      "cnt: 0 - valLoss: 0.6648123860359192 - trainLoss: 0.663340151309967\n",
      "cnt: 0 - valLoss: 0.664811909198761 - trainLoss: 0.6633397340774536\n",
      "cnt: 0 - valLoss: 0.664811372756958 - trainLoss: 0.6633392572402954\n",
      "cnt: 0 - valLoss: 0.664810836315155 - trainLoss: 0.6633387804031372\n",
      "cnt: 0 - valLoss: 0.664810299873352 - trainLoss: 0.6633384227752686\n",
      "cnt: 0 - valLoss: 0.6648098230361938 - trainLoss: 0.6633379459381104\n",
      "cnt: 0 - valLoss: 0.6648092865943909 - trainLoss: 0.6633375287055969\n",
      "cnt: 0 - valLoss: 0.6648087501525879 - trainLoss: 0.6633371114730835\n",
      "cnt: 0 - valLoss: 0.6648082137107849 - trainLoss: 0.6633366942405701\n",
      "cnt: 0 - valLoss: 0.6648077368736267 - trainLoss: 0.6633362174034119\n",
      "cnt: 0 - valLoss: 0.6648072004318237 - trainLoss: 0.6633358001708984\n",
      "cnt: 0 - valLoss: 0.6648066639900208 - trainLoss: 0.663335382938385\n",
      "cnt: 0 - valLoss: 0.6648061275482178 - trainLoss: 0.6633350253105164\n",
      "cnt: 0 - valLoss: 0.6648055911064148 - trainLoss: 0.6633345484733582\n",
      "cnt: 0 - valLoss: 0.6648051142692566 - trainLoss: 0.6633340716362\n",
      "cnt: 0 - valLoss: 0.6648045182228088 - trainLoss: 0.6633336544036865\n",
      "cnt: 0 - valLoss: 0.6648040413856506 - trainLoss: 0.6633332371711731\n",
      "cnt: 0 - valLoss: 0.6648035049438477 - trainLoss: 0.6633328795433044\n",
      "cnt: 0 - valLoss: 0.6648030281066895 - trainLoss: 0.6633324027061462\n",
      "cnt: 0 - valLoss: 0.6648024916648865 - trainLoss: 0.6633319854736328\n",
      "cnt: 0 - valLoss: 0.6648019552230835 - trainLoss: 0.6633315086364746\n",
      "cnt: 0 - valLoss: 0.6648014187812805 - trainLoss: 0.6633310914039612\n",
      "cnt: 0 - valLoss: 0.6648009419441223 - trainLoss: 0.6633306741714478\n",
      "cnt: 0 - valLoss: 0.6648003458976746 - trainLoss: 0.6633302569389343\n",
      "cnt: 0 - valLoss: 0.6647998094558716 - trainLoss: 0.6633298397064209\n",
      "cnt: 0 - valLoss: 0.6647992730140686 - trainLoss: 0.6633294224739075\n",
      "cnt: 0 - valLoss: 0.6647987961769104 - trainLoss: 0.663329005241394\n",
      "cnt: 0 - valLoss: 0.6647982597351074 - trainLoss: 0.6633285284042358\n",
      "cnt: 0 - valLoss: 0.6647977232933044 - trainLoss: 0.6633281111717224\n",
      "cnt: 0 - valLoss: 0.6647972464561462 - trainLoss: 0.663327693939209\n",
      "cnt: 0 - valLoss: 0.6647966504096985 - trainLoss: 0.6633272767066956\n",
      "cnt: 0 - valLoss: 0.6647961139678955 - trainLoss: 0.6633267998695374\n",
      "cnt: 0 - valLoss: 0.6647956371307373 - trainLoss: 0.6633263826370239\n",
      "cnt: 0 - valLoss: 0.6647951006889343 - trainLoss: 0.6633259654045105\n",
      "cnt: 0 - valLoss: 0.6647945642471313 - trainLoss: 0.6633255481719971\n",
      "cnt: 0 - valLoss: 0.6647940278053284 - trainLoss: 0.6633250713348389\n",
      "cnt: 0 - valLoss: 0.6647934913635254 - trainLoss: 0.6633246541023254\n",
      "cnt: 0 - valLoss: 0.6647930145263672 - trainLoss: 0.663324236869812\n",
      "cnt: 0 - valLoss: 0.6647924184799194 - trainLoss: 0.6633238196372986\n",
      "cnt: 0 - valLoss: 0.6647918820381165 - trainLoss: 0.6633233428001404\n",
      "cnt: 0 - valLoss: 0.6647913455963135 - trainLoss: 0.663322925567627\n",
      "cnt: 0 - valLoss: 0.6647908687591553 - trainLoss: 0.6633225083351135\n",
      "cnt: 0 - valLoss: 0.6647903323173523 - trainLoss: 0.6633220911026001\n",
      "cnt: 0 - valLoss: 0.6647897958755493 - trainLoss: 0.6633216738700867\n",
      "cnt: 0 - valLoss: 0.6647892594337463 - trainLoss: 0.6633211970329285\n",
      "cnt: 0 - valLoss: 0.6647887229919434 - trainLoss: 0.663320779800415\n",
      "cnt: 0 - valLoss: 0.6647881269454956 - trainLoss: 0.6633203625679016\n",
      "cnt: 0 - valLoss: 0.6647876501083374 - trainLoss: 0.6633199453353882\n",
      "cnt: 0 - valLoss: 0.6647871136665344 - trainLoss: 0.66331946849823\n",
      "cnt: 0 - valLoss: 0.6647865772247314 - trainLoss: 0.6633190512657166\n",
      "cnt: 0 - valLoss: 0.6647860407829285 - trainLoss: 0.6633186340332031\n",
      "cnt: 0 - valLoss: 0.6647855639457703 - trainLoss: 0.6633181571960449\n",
      "cnt: 0 - valLoss: 0.6647849678993225 - trainLoss: 0.6633177399635315\n",
      "cnt: 0 - valLoss: 0.6647844910621643 - trainLoss: 0.6633173823356628\n",
      "cnt: 0 - valLoss: 0.6647838950157166 - trainLoss: 0.6633169054985046\n",
      "cnt: 0 - valLoss: 0.6647834181785583 - trainLoss: 0.6633164882659912\n",
      "cnt: 0 - valLoss: 0.6647828817367554 - trainLoss: 0.663316011428833\n",
      "cnt: 0 - valLoss: 0.6647823452949524 - trainLoss: 0.6633155941963196\n",
      "cnt: 0 - valLoss: 0.6647818088531494 - trainLoss: 0.6633151769638062\n",
      "cnt: 0 - valLoss: 0.6647812724113464 - trainLoss: 0.6633147597312927\n",
      "cnt: 0 - valLoss: 0.6647807359695435 - trainLoss: 0.6633143424987793\n",
      "cnt: 0 - valLoss: 0.6647801995277405 - trainLoss: 0.6633138656616211\n",
      "cnt: 0 - valLoss: 0.6647796630859375 - trainLoss: 0.6633134484291077\n",
      "cnt: 0 - valLoss: 0.6647791266441345 - trainLoss: 0.6633130311965942\n",
      "cnt: 0 - valLoss: 0.6647785902023315 - trainLoss: 0.663312554359436\n",
      "cnt: 0 - valLoss: 0.6647780537605286 - trainLoss: 0.6633121371269226\n",
      "cnt: 0 - valLoss: 0.6647775769233704 - trainLoss: 0.663311779499054\n",
      "cnt: 0 - valLoss: 0.6647769808769226 - trainLoss: 0.663311243057251\n",
      "cnt: 0 - valLoss: 0.6647765040397644 - trainLoss: 0.6633108854293823\n",
      "cnt: 0 - valLoss: 0.6647759675979614 - trainLoss: 0.6633104085922241\n",
      "cnt: 0 - valLoss: 0.6647753715515137 - trainLoss: 0.6633099913597107\n",
      "cnt: 0 - valLoss: 0.6647748947143555 - trainLoss: 0.663309633731842\n",
      "cnt: 0 - valLoss: 0.6647743582725525 - trainLoss: 0.6633090972900391\n",
      "cnt: 0 - valLoss: 0.6647738218307495 - trainLoss: 0.6633086800575256\n",
      "cnt: 0 - valLoss: 0.6647732853889465 - trainLoss: 0.6633082628250122\n",
      "cnt: 0 - valLoss: 0.6647727489471436 - trainLoss: 0.6633078455924988\n",
      "cnt: 0 - valLoss: 0.6647722125053406 - trainLoss: 0.6633073687553406\n",
      "cnt: 0 - valLoss: 0.6647716760635376 - trainLoss: 0.6633069515228271\n",
      "cnt: 0 - valLoss: 0.6647711396217346 - trainLoss: 0.6633065342903137\n",
      "cnt: 0 - valLoss: 0.6647706031799316 - trainLoss: 0.6633061170578003\n",
      "cnt: 0 - valLoss: 0.6647700667381287 - trainLoss: 0.6633056998252869\n",
      "cnt: 0 - valLoss: 0.6647695302963257 - trainLoss: 0.6633052825927734\n",
      "cnt: 0 - valLoss: 0.6647690534591675 - trainLoss: 0.6633048057556152\n",
      "cnt: 0 - valLoss: 0.6647685170173645 - trainLoss: 0.6633043885231018\n",
      "cnt: 0 - valLoss: 0.6647679805755615 - trainLoss: 0.6633039712905884\n",
      "cnt: 0 - valLoss: 0.6647675037384033 - trainLoss: 0.663303554058075\n",
      "cnt: 0 - valLoss: 0.6647669076919556 - trainLoss: 0.6633031368255615\n",
      "cnt: 0 - valLoss: 0.6647663712501526 - trainLoss: 0.6633026599884033\n",
      "cnt: 0 - valLoss: 0.6647658348083496 - trainLoss: 0.6633022427558899\n",
      "cnt: 0 - valLoss: 0.6647654175758362 - trainLoss: 0.6633018255233765\n",
      "cnt: 0 - valLoss: 0.6647648215293884 - trainLoss: 0.663301408290863\n",
      "cnt: 0 - valLoss: 0.6647643446922302 - trainLoss: 0.6633009910583496\n",
      "cnt: 0 - valLoss: 0.6647636890411377 - trainLoss: 0.6633005738258362\n",
      "cnt: 0 - valLoss: 0.6647632718086243 - trainLoss: 0.663300096988678\n",
      "cnt: 0 - valLoss: 0.6647627949714661 - trainLoss: 0.6632996797561646\n",
      "cnt: 0 - valLoss: 0.6647621989250183 - trainLoss: 0.6632992625236511\n",
      "cnt: 0 - valLoss: 0.6647616624832153 - trainLoss: 0.6632988452911377\n",
      "cnt: 0 - valLoss: 0.6647611260414124 - trainLoss: 0.6632983684539795\n",
      "cnt: 0 - valLoss: 0.6647606492042542 - trainLoss: 0.6632979512214661\n",
      "cnt: 0 - valLoss: 0.6647600531578064 - trainLoss: 0.6632975339889526\n",
      "cnt: 0 - valLoss: 0.6647595167160034 - trainLoss: 0.6632971167564392\n",
      "cnt: 0 - valLoss: 0.6647589802742004 - trainLoss: 0.663296639919281\n",
      "cnt: 0 - valLoss: 0.6647585034370422 - trainLoss: 0.6632962822914124\n",
      "cnt: 0 - valLoss: 0.6647579669952393 - trainLoss: 0.6632958650588989\n",
      "cnt: 0 - valLoss: 0.664757490158081 - trainLoss: 0.6632953882217407\n",
      "cnt: 0 - valLoss: 0.6647568941116333 - trainLoss: 0.6632949113845825\n",
      "cnt: 0 - valLoss: 0.6647564172744751 - trainLoss: 0.6632944941520691\n",
      "cnt: 0 - valLoss: 0.6647558212280273 - trainLoss: 0.6632941365242004\n",
      "cnt: 0 - valLoss: 0.6647553443908691 - trainLoss: 0.6632936596870422\n",
      "cnt: 0 - valLoss: 0.6647548675537109 - trainLoss: 0.6632932424545288\n",
      "cnt: 0 - valLoss: 0.6647542715072632 - trainLoss: 0.6632927656173706\n",
      "cnt: 0 - valLoss: 0.6647537350654602 - trainLoss: 0.6632923483848572\n",
      "cnt: 0 - valLoss: 0.664753258228302 - trainLoss: 0.6632919311523438\n",
      "cnt: 0 - valLoss: 0.664752721786499 - trainLoss: 0.6632915139198303\n",
      "cnt: 0 - valLoss: 0.664752185344696 - trainLoss: 0.6632910370826721\n",
      "cnt: 0 - valLoss: 0.6647516489028931 - trainLoss: 0.6632906198501587\n",
      "cnt: 0 - valLoss: 0.6647511720657349 - trainLoss: 0.6632902026176453\n",
      "cnt: 0 - valLoss: 0.6647505760192871 - trainLoss: 0.6632897853851318\n",
      "cnt: 0 - valLoss: 0.6647500395774841 - trainLoss: 0.6632893085479736\n",
      "cnt: 0 - valLoss: 0.6647495031356812 - trainLoss: 0.6632888913154602\n",
      "cnt: 0 - valLoss: 0.6647489666938782 - trainLoss: 0.6632884740829468\n",
      "cnt: 0 - valLoss: 0.6647484302520752 - trainLoss: 0.6632879972457886\n",
      "cnt: 0 - valLoss: 0.664747953414917 - trainLoss: 0.6632876396179199\n",
      "cnt: 0 - valLoss: 0.664747416973114 - trainLoss: 0.6632871627807617\n",
      "cnt: 0 - valLoss: 0.664746880531311 - trainLoss: 0.6632867455482483\n",
      "cnt: 0 - valLoss: 0.6647463440895081 - trainLoss: 0.6632863283157349\n",
      "cnt: 0 - valLoss: 0.6647457480430603 - trainLoss: 0.6632859110832214\n",
      "cnt: 0 - valLoss: 0.6647452712059021 - trainLoss: 0.6632854342460632\n",
      "cnt: 0 - valLoss: 0.6647446751594543 - trainLoss: 0.663284957408905\n",
      "cnt: 0 - valLoss: 0.6647442579269409 - trainLoss: 0.6632845997810364\n",
      "cnt: 0 - valLoss: 0.6647437214851379 - trainLoss: 0.663284182548523\n",
      "cnt: 0 - valLoss: 0.6647431254386902 - trainLoss: 0.6632837057113647\n",
      "cnt: 0 - valLoss: 0.6647425889968872 - trainLoss: 0.6632832288742065\n",
      "cnt: 0 - valLoss: 0.664742112159729 - trainLoss: 0.6632828116416931\n",
      "cnt: 0 - valLoss: 0.6647415161132812 - trainLoss: 0.6632824540138245\n",
      "cnt: 0 - valLoss: 0.664741039276123 - trainLoss: 0.6632818579673767\n",
      "cnt: 0 - valLoss: 0.6647405028343201 - trainLoss: 0.6632815003395081\n",
      "cnt: 0 - valLoss: 0.6647399663925171 - trainLoss: 0.6632810831069946\n",
      "cnt: 0 - valLoss: 0.6647394299507141 - trainLoss: 0.6632806658744812\n",
      "cnt: 0 - valLoss: 0.6647388339042664 - trainLoss: 0.6632802486419678\n",
      "cnt: 0 - valLoss: 0.6647383570671082 - trainLoss: 0.6632797718048096\n",
      "cnt: 0 - valLoss: 0.6647377610206604 - trainLoss: 0.6632792949676514\n",
      "cnt: 0 - valLoss: 0.6647372841835022 - trainLoss: 0.6632789373397827\n",
      "cnt: 0 - valLoss: 0.6647366881370544 - trainLoss: 0.6632784605026245\n",
      "cnt: 0 - valLoss: 0.6647362112998962 - trainLoss: 0.6632780432701111\n",
      "cnt: 0 - valLoss: 0.6647356748580933 - trainLoss: 0.6632776260375977\n",
      "cnt: 0 - valLoss: 0.6647351980209351 - trainLoss: 0.6632771492004395\n",
      "cnt: 0 - valLoss: 0.6647346019744873 - trainLoss: 0.663276731967926\n",
      "cnt: 0 - valLoss: 0.6647341251373291 - trainLoss: 0.6632763147354126\n",
      "cnt: 0 - valLoss: 0.6647335290908813 - trainLoss: 0.6632758378982544\n",
      "cnt: 0 - valLoss: 0.6647329926490784 - trainLoss: 0.663275420665741\n",
      "cnt: 0 - valLoss: 0.6647324562072754 - trainLoss: 0.6632750034332275\n",
      "cnt: 0 - valLoss: 0.6647319197654724 - trainLoss: 0.6632745862007141\n",
      "cnt: 0 - valLoss: 0.6647313833236694 - trainLoss: 0.6632741093635559\n",
      "cnt: 0 - valLoss: 0.6647309064865112 - trainLoss: 0.6632736921310425\n",
      "cnt: 0 - valLoss: 0.6647303104400635 - trainLoss: 0.6632732152938843\n",
      "cnt: 0 - valLoss: 0.6647297739982605 - trainLoss: 0.6632727384567261\n",
      "cnt: 0 - valLoss: 0.6647292375564575 - trainLoss: 0.6632723212242126\n",
      "cnt: 0 - valLoss: 0.6647287011146545 - trainLoss: 0.6632719039916992\n",
      "cnt: 0 - valLoss: 0.6647281646728516 - trainLoss: 0.663271427154541\n",
      "cnt: 0 - valLoss: 0.6647276878356934 - trainLoss: 0.6632710099220276\n",
      "cnt: 0 - valLoss: 0.6647271513938904 - trainLoss: 0.6632705926895142\n",
      "cnt: 0 - valLoss: 0.6647265553474426 - trainLoss: 0.6632701754570007\n",
      "cnt: 0 - valLoss: 0.6647260189056396 - trainLoss: 0.6632696986198425\n",
      "cnt: 0 - valLoss: 0.6647254824638367 - trainLoss: 0.6632692813873291\n",
      "cnt: 0 - valLoss: 0.6647249460220337 - trainLoss: 0.6632688045501709\n",
      "cnt: 0 - valLoss: 0.6647244095802307 - trainLoss: 0.6632684469223022\n",
      "cnt: 0 - valLoss: 0.6647238731384277 - trainLoss: 0.6632680296897888\n",
      "cnt: 0 - valLoss: 0.66472327709198 - trainLoss: 0.6632674932479858\n",
      "cnt: 0 - valLoss: 0.6647228002548218 - trainLoss: 0.6632670760154724\n",
      "cnt: 0 - valLoss: 0.664722204208374 - trainLoss: 0.663266658782959\n",
      "cnt: 0 - valLoss: 0.6647217273712158 - trainLoss: 0.6632662415504456\n",
      "cnt: 0 - valLoss: 0.6647211909294128 - trainLoss: 0.6632657647132874\n",
      "cnt: 0 - valLoss: 0.6647205948829651 - trainLoss: 0.6632653474807739\n",
      "cnt: 0 - valLoss: 0.6647200584411621 - trainLoss: 0.6632648706436157\n",
      "cnt: 0 - valLoss: 0.6647195816040039 - trainLoss: 0.6632643938064575\n",
      "cnt: 0 - valLoss: 0.6647189855575562 - trainLoss: 0.6632639765739441\n",
      "cnt: 0 - valLoss: 0.6647184491157532 - trainLoss: 0.6632634997367859\n",
      "cnt: 0 - valLoss: 0.6647178530693054 - trainLoss: 0.6632630825042725\n",
      "cnt: 0 - valLoss: 0.6647172570228577 - trainLoss: 0.663262665271759\n",
      "cnt: 0 - valLoss: 0.6647167205810547 - trainLoss: 0.663262128829956\n",
      "cnt: 0 - valLoss: 0.6647161841392517 - trainLoss: 0.6632617712020874\n",
      "cnt: 0 - valLoss: 0.6647156476974487 - trainLoss: 0.6632612347602844\n",
      "cnt: 0 - valLoss: 0.6647151112556458 - trainLoss: 0.663260817527771\n",
      "cnt: 0 - valLoss: 0.6647145748138428 - trainLoss: 0.6632604002952576\n",
      "cnt: 0 - valLoss: 0.664713978767395 - trainLoss: 0.6632599234580994\n",
      "cnt: 0 - valLoss: 0.664713442325592 - trainLoss: 0.6632594466209412\n",
      "cnt: 0 - valLoss: 0.6647129058837891 - trainLoss: 0.6632590293884277\n",
      "cnt: 0 - valLoss: 0.6647123694419861 - trainLoss: 0.6632585525512695\n",
      "cnt: 0 - valLoss: 0.6647117733955383 - trainLoss: 0.6632581353187561\n",
      "cnt: 0 - valLoss: 0.6647112369537354 - trainLoss: 0.6632575988769531\n",
      "cnt: 0 - valLoss: 0.6647106409072876 - trainLoss: 0.6632572412490845\n",
      "cnt: 0 - valLoss: 0.6647101044654846 - trainLoss: 0.6632567644119263\n",
      "cnt: 0 - valLoss: 0.6647095680236816 - trainLoss: 0.6632562875747681\n",
      "cnt: 0 - valLoss: 0.6647089123725891 - trainLoss: 0.6632559299468994\n",
      "cnt: 0 - valLoss: 0.6647083759307861 - trainLoss: 0.6632553935050964\n",
      "cnt: 0 - valLoss: 0.6647078990936279 - trainLoss: 0.6632549166679382\n",
      "cnt: 0 - valLoss: 0.6647073030471802 - trainLoss: 0.6632544994354248\n",
      "cnt: 0 - valLoss: 0.6647067070007324 - trainLoss: 0.6632540822029114\n",
      "cnt: 0 - valLoss: 0.6647062301635742 - trainLoss: 0.6632536053657532\n",
      "cnt: 0 - valLoss: 0.6647056937217712 - trainLoss: 0.6632530689239502\n",
      "cnt: 0 - valLoss: 0.6647050976753235 - trainLoss: 0.6632527112960815\n",
      "cnt: 0 - valLoss: 0.6647045016288757 - trainLoss: 0.6632521748542786\n",
      "cnt: 0 - valLoss: 0.664703905582428 - trainLoss: 0.6632518172264099\n",
      "cnt: 0 - valLoss: 0.6647034287452698 - trainLoss: 0.6632513403892517\n",
      "cnt: 0 - valLoss: 0.6647027730941772 - trainLoss: 0.6632509231567383\n",
      "cnt: 0 - valLoss: 0.664702296257019 - trainLoss: 0.6632503867149353\n",
      "cnt: 0 - valLoss: 0.6647017598152161 - trainLoss: 0.6632499694824219\n",
      "cnt: 0 - valLoss: 0.6647011637687683 - trainLoss: 0.6632494926452637\n",
      "cnt: 0 - valLoss: 0.6647005677223206 - trainLoss: 0.6632490158081055\n",
      "cnt: 0 - valLoss: 0.6647000312805176 - trainLoss: 0.663248598575592\n",
      "cnt: 0 - valLoss: 0.6646994948387146 - trainLoss: 0.6632480621337891\n",
      "cnt: 0 - valLoss: 0.6646988987922668 - trainLoss: 0.6632477045059204\n",
      "cnt: 0 - valLoss: 0.6646984219551086 - trainLoss: 0.6632472276687622\n",
      "cnt: 0 - valLoss: 0.6646978259086609 - trainLoss: 0.6632468104362488\n",
      "cnt: 0 - valLoss: 0.6646971702575684 - trainLoss: 0.6632462739944458\n",
      "cnt: 0 - valLoss: 0.6646966338157654 - trainLoss: 0.6632458567619324\n",
      "cnt: 0 - valLoss: 0.6646960973739624 - trainLoss: 0.663245439529419\n",
      "cnt: 0 - valLoss: 0.6646955013275146 - trainLoss: 0.663244903087616\n",
      "cnt: 0 - valLoss: 0.6646949648857117 - trainLoss: 0.6632444858551025\n",
      "cnt: 0 - valLoss: 0.6646944284439087 - trainLoss: 0.6632440090179443\n",
      "cnt: 0 - valLoss: 0.6646938323974609 - trainLoss: 0.6632435321807861\n",
      "cnt: 0 - valLoss: 0.6646933555603027 - trainLoss: 0.6632430553436279\n",
      "cnt: 0 - valLoss: 0.664692759513855 - trainLoss: 0.6632425785064697\n",
      "cnt: 0 - valLoss: 0.664692223072052 - trainLoss: 0.6632422208786011\n",
      "cnt: 0 - valLoss: 0.6646916270256042 - trainLoss: 0.6632417440414429\n",
      "cnt: 0 - valLoss: 0.6646909713745117 - trainLoss: 0.6632412672042847\n",
      "cnt: 0 - valLoss: 0.6646904349327087 - trainLoss: 0.6632407903671265\n",
      "cnt: 0 - valLoss: 0.664689838886261 - trainLoss: 0.663240373134613\n",
      "cnt: 0 - valLoss: 0.6646893620491028 - trainLoss: 0.6632398366928101\n",
      "cnt: 0 - valLoss: 0.664688766002655 - trainLoss: 0.6632394194602966\n",
      "cnt: 0 - valLoss: 0.6646881699562073 - trainLoss: 0.6632389426231384\n",
      "cnt: 0 - valLoss: 0.6646876335144043 - trainLoss: 0.6632384657859802\n",
      "cnt: 0 - valLoss: 0.6646870970726013 - trainLoss: 0.6632380485534668\n",
      "cnt: 0 - valLoss: 0.6646865010261536 - trainLoss: 0.6632375717163086\n",
      "cnt: 0 - valLoss: 0.6646859645843506 - trainLoss: 0.6632370948791504\n",
      "cnt: 0 - valLoss: 0.6646853685379028 - trainLoss: 0.6632366180419922\n",
      "cnt: 0 - valLoss: 0.6646847724914551 - trainLoss: 0.6632362008094788\n",
      "cnt: 0 - valLoss: 0.6646842360496521 - trainLoss: 0.6632357239723206\n",
      "cnt: 0 - valLoss: 0.6646836996078491 - trainLoss: 0.6632352471351624\n",
      "cnt: 0 - valLoss: 0.6646831035614014 - trainLoss: 0.6632348299026489\n",
      "cnt: 0 - valLoss: 0.6646825671195984 - trainLoss: 0.6632343530654907\n",
      "cnt: 0 - valLoss: 0.6646819710731506 - trainLoss: 0.6632338762283325\n",
      "cnt: 0 - valLoss: 0.6646814942359924 - trainLoss: 0.6632334589958191\n",
      "cnt: 0 - valLoss: 0.6646808385848999 - trainLoss: 0.6632329821586609\n",
      "cnt: 0 - valLoss: 0.6646802425384521 - trainLoss: 0.6632325649261475\n",
      "cnt: 0 - valLoss: 0.6646797060966492 - trainLoss: 0.6632320880889893\n",
      "cnt: 0 - valLoss: 0.6646791696548462 - trainLoss: 0.663231611251831\n",
      "cnt: 0 - valLoss: 0.6646785736083984 - trainLoss: 0.6632311344146729\n",
      "cnt: 0 - valLoss: 0.6646779775619507 - trainLoss: 0.6632306575775146\n",
      "cnt: 0 - valLoss: 0.6646775007247925 - trainLoss: 0.6632301807403564\n",
      "cnt: 0 - valLoss: 0.6646769046783447 - trainLoss: 0.663229763507843\n",
      "cnt: 0 - valLoss: 0.6646762490272522 - trainLoss: 0.66322922706604\n",
      "cnt: 0 - valLoss: 0.6646756529808044 - trainLoss: 0.6632288694381714\n",
      "cnt: 0 - valLoss: 0.6646751165390015 - trainLoss: 0.6632283329963684\n",
      "cnt: 0 - valLoss: 0.6646745800971985 - trainLoss: 0.663227915763855\n",
      "cnt: 0 - valLoss: 0.6646740436553955 - trainLoss: 0.6632274389266968\n",
      "cnt: 0 - valLoss: 0.6646734476089478 - trainLoss: 0.6632269024848938\n",
      "cnt: 0 - valLoss: 0.6646728515625 - trainLoss: 0.6632264852523804\n",
      "cnt: 0 - valLoss: 0.664672315120697 - trainLoss: 0.6632260084152222\n",
      "cnt: 0 - valLoss: 0.6646717190742493 - trainLoss: 0.663225531578064\n",
      "cnt: 0 - valLoss: 0.6646711230278015 - trainLoss: 0.6632250547409058\n",
      "cnt: 0 - valLoss: 0.6646705865859985 - trainLoss: 0.6632246375083923\n",
      "cnt: 0 - valLoss: 0.6646700501441956 - trainLoss: 0.6632241606712341\n",
      "cnt: 0 - valLoss: 0.6646694540977478 - trainLoss: 0.6632236838340759\n",
      "cnt: 0 - valLoss: 0.6646689176559448 - trainLoss: 0.6632232069969177\n",
      "cnt: 0 - valLoss: 0.6646683216094971 - trainLoss: 0.6632227301597595\n",
      "cnt: 0 - valLoss: 0.6646677255630493 - trainLoss: 0.6632221937179565\n",
      "cnt: 0 - valLoss: 0.6646671295166016 - trainLoss: 0.6632218360900879\n",
      "cnt: 0 - valLoss: 0.6646665334701538 - trainLoss: 0.6632212996482849\n",
      "cnt: 0 - valLoss: 0.6646659970283508 - trainLoss: 0.6632208824157715\n",
      "cnt: 0 - valLoss: 0.6646654605865479 - trainLoss: 0.6632204055786133\n",
      "cnt: 0 - valLoss: 0.6646648645401001 - trainLoss: 0.6632199287414551\n",
      "cnt: 0 - valLoss: 0.6646642684936523 - trainLoss: 0.6632195115089417\n",
      "cnt: 0 - valLoss: 0.6646637320518494 - trainLoss: 0.6632190346717834\n",
      "cnt: 0 - valLoss: 0.6646631360054016 - trainLoss: 0.6632185578346252\n",
      "cnt: 0 - valLoss: 0.6646625399589539 - trainLoss: 0.663218080997467\n",
      "cnt: 0 - valLoss: 0.6646619439125061 - trainLoss: 0.6632176041603088\n",
      "cnt: 0 - valLoss: 0.6646613478660583 - trainLoss: 0.6632171273231506\n",
      "cnt: 0 - valLoss: 0.6646608710289001 - trainLoss: 0.6632167100906372\n",
      "cnt: 0 - valLoss: 0.6646602749824524 - trainLoss: 0.663216233253479\n",
      "cnt: 0 - valLoss: 0.6646596789360046 - trainLoss: 0.663215696811676\n",
      "cnt: 0 - valLoss: 0.6646591424942017 - trainLoss: 0.6632152199745178\n",
      "cnt: 0 - valLoss: 0.6646584868431091 - trainLoss: 0.6632147431373596\n",
      "cnt: 0 - valLoss: 0.6646579504013062 - trainLoss: 0.6632143259048462\n",
      "cnt: 0 - valLoss: 0.6646573543548584 - trainLoss: 0.663213849067688\n",
      "cnt: 0 - valLoss: 0.6646568179130554 - trainLoss: 0.6632133722305298\n",
      "cnt: 0 - valLoss: 0.6646562814712524 - trainLoss: 0.6632129549980164\n",
      "cnt: 0 - valLoss: 0.6646556258201599 - trainLoss: 0.6632124781608582\n",
      "cnt: 0 - valLoss: 0.6646550893783569 - trainLoss: 0.6632120013237\n",
      "cnt: 0 - valLoss: 0.6646544933319092 - trainLoss: 0.6632114052772522\n",
      "cnt: 0 - valLoss: 0.6646538972854614 - trainLoss: 0.6632109880447388\n",
      "cnt: 0 - valLoss: 0.6646533012390137 - trainLoss: 0.6632105112075806\n",
      "cnt: 0 - valLoss: 0.6646527647972107 - trainLoss: 0.6632100343704224\n",
      "cnt: 0 - valLoss: 0.6646521687507629 - trainLoss: 0.6632096171379089\n",
      "cnt: 0 - valLoss: 0.6646515727043152 - trainLoss: 0.6632091403007507\n",
      "cnt: 0 - valLoss: 0.6646510362625122 - trainLoss: 0.6632086634635925\n",
      "cnt: 0 - valLoss: 0.6646504998207092 - trainLoss: 0.6632081866264343\n",
      "cnt: 0 - valLoss: 0.6646499037742615 - trainLoss: 0.6632077693939209\n",
      "cnt: 0 - valLoss: 0.664649248123169 - trainLoss: 0.6632072329521179\n",
      "cnt: 0 - valLoss: 0.664648711681366 - trainLoss: 0.6632067561149597\n",
      "cnt: 0 - valLoss: 0.6646481156349182 - trainLoss: 0.6632062792778015\n",
      "cnt: 0 - valLoss: 0.6646475791931152 - trainLoss: 0.6632058024406433\n",
      "cnt: 0 - valLoss: 0.6646469831466675 - trainLoss: 0.6632053256034851\n",
      "cnt: 0 - valLoss: 0.6646463871002197 - trainLoss: 0.6632048487663269\n",
      "cnt: 0 - valLoss: 0.664645791053772 - trainLoss: 0.6632044315338135\n",
      "cnt: 0 - valLoss: 0.664645254611969 - trainLoss: 0.6632039546966553\n",
      "cnt: 0 - valLoss: 0.6646446585655212 - trainLoss: 0.6632034778594971\n",
      "cnt: 0 - valLoss: 0.6646440625190735 - trainLoss: 0.6632030010223389\n",
      "cnt: 0 - valLoss: 0.6646435260772705 - trainLoss: 0.6632025241851807\n",
      "cnt: 0 - valLoss: 0.6646429300308228 - trainLoss: 0.6632020473480225\n",
      "cnt: 0 - valLoss: 0.6646423935890198 - trainLoss: 0.6632015705108643\n",
      "cnt: 0 - valLoss: 0.6646416783332825 - trainLoss: 0.663201093673706\n",
      "cnt: 0 - valLoss: 0.6646412014961243 - trainLoss: 0.6632006168365479\n",
      "cnt: 0 - valLoss: 0.6646406054496765 - trainLoss: 0.6632001399993896\n",
      "cnt: 0 - valLoss: 0.664639949798584 - trainLoss: 0.6631996035575867\n",
      "cnt: 0 - valLoss: 0.664639413356781 - trainLoss: 0.6631991863250732\n",
      "cnt: 0 - valLoss: 0.664638876914978 - trainLoss: 0.663198709487915\n",
      "cnt: 0 - valLoss: 0.6646382808685303 - trainLoss: 0.6631982326507568\n",
      "cnt: 0 - valLoss: 0.6646376252174377 - trainLoss: 0.6631977558135986\n",
      "cnt: 0 - valLoss: 0.6646370887756348 - trainLoss: 0.6631972193717957\n",
      "cnt: 0 - valLoss: 0.664636492729187 - trainLoss: 0.6631968021392822\n",
      "cnt: 0 - valLoss: 0.6646358966827393 - trainLoss: 0.6631963849067688\n",
      "cnt: 0 - valLoss: 0.6646353006362915 - trainLoss: 0.6631958484649658\n",
      "cnt: 0 - valLoss: 0.6646347641944885 - trainLoss: 0.6631953120231628\n",
      "cnt: 0 - valLoss: 0.6646341681480408 - trainLoss: 0.6631948947906494\n",
      "cnt: 0 - valLoss: 0.664633572101593 - trainLoss: 0.6631944179534912\n",
      "cnt: 0 - valLoss: 0.6646329760551453 - trainLoss: 0.663193941116333\n",
      "cnt: 0 - valLoss: 0.6646323800086975 - trainLoss: 0.6631935238838196\n",
      "cnt: 0 - valLoss: 0.6646317839622498 - trainLoss: 0.6631929874420166\n",
      "cnt: 0 - valLoss: 0.6646312475204468 - trainLoss: 0.6631925702095032\n",
      "cnt: 0 - valLoss: 0.664630651473999 - trainLoss: 0.6631920337677002\n",
      "cnt: 0 - valLoss: 0.6646300554275513 - trainLoss: 0.663191556930542\n",
      "cnt: 0 - valLoss: 0.6646294593811035 - trainLoss: 0.6631910800933838\n",
      "cnt: 0 - valLoss: 0.6646288633346558 - trainLoss: 0.6631906032562256\n",
      "cnt: 0 - valLoss: 0.664628267288208 - trainLoss: 0.6631901860237122\n",
      "cnt: 0 - valLoss: 0.6646277904510498 - trainLoss: 0.663189709186554\n",
      "cnt: 0 - valLoss: 0.6646271347999573 - trainLoss: 0.663189172744751\n",
      "cnt: 0 - valLoss: 0.6646265387535095 - trainLoss: 0.663188636302948\n",
      "cnt: 0 - valLoss: 0.6646260023117065 - trainLoss: 0.6631882190704346\n",
      "cnt: 0 - valLoss: 0.664625346660614 - trainLoss: 0.6631877422332764\n",
      "cnt: 0 - valLoss: 0.6646247506141663 - trainLoss: 0.6631872653961182\n",
      "cnt: 0 - valLoss: 0.6646241545677185 - trainLoss: 0.66318678855896\n",
      "cnt: 0 - valLoss: 0.6646236181259155 - trainLoss: 0.6631863117218018\n",
      "cnt: 0 - valLoss: 0.6646230220794678 - trainLoss: 0.6631858348846436\n",
      "cnt: 0 - valLoss: 0.66462242603302 - trainLoss: 0.6631852984428406\n",
      "cnt: 0 - valLoss: 0.6646218299865723 - trainLoss: 0.6631848812103271\n",
      "cnt: 0 - valLoss: 0.6646212935447693 - trainLoss: 0.663184404373169\n",
      "cnt: 0 - valLoss: 0.6646207571029663 - trainLoss: 0.6631839275360107\n",
      "cnt: 0 - valLoss: 0.6646201014518738 - trainLoss: 0.6631835103034973\n",
      "cnt: 0 - valLoss: 0.664619505405426 - trainLoss: 0.6631830334663391\n",
      "cnt: 0 - valLoss: 0.6646189093589783 - trainLoss: 0.6631825566291809\n",
      "cnt: 0 - valLoss: 0.6646183133125305 - trainLoss: 0.6631820797920227\n",
      "cnt: 0 - valLoss: 0.6646177172660828 - trainLoss: 0.6631815433502197\n",
      "cnt: 0 - valLoss: 0.664617121219635 - trainLoss: 0.6631810665130615\n",
      "cnt: 0 - valLoss: 0.664616584777832 - trainLoss: 0.6631805896759033\n",
      "cnt: 0 - valLoss: 0.6646159887313843 - trainLoss: 0.6631801724433899\n",
      "cnt: 0 - valLoss: 0.6646153926849365 - trainLoss: 0.6631796956062317\n",
      "cnt: 0 - valLoss: 0.6646147966384888 - trainLoss: 0.6631792187690735\n",
      "cnt: 0 - valLoss: 0.6646142601966858 - trainLoss: 0.6631787419319153\n",
      "cnt: 0 - valLoss: 0.6646136045455933 - trainLoss: 0.6631783246994019\n",
      "cnt: 0 - valLoss: 0.6646131277084351 - trainLoss: 0.6631777882575989\n",
      "cnt: 0 - valLoss: 0.6646125316619873 - trainLoss: 0.6631773114204407\n",
      "cnt: 0 - valLoss: 0.6646119356155396 - trainLoss: 0.6631768345832825\n",
      "cnt: 0 - valLoss: 0.6646113395690918 - trainLoss: 0.6631763577461243\n",
      "cnt: 0 - valLoss: 0.664610743522644 - trainLoss: 0.6631758809089661\n",
      "cnt: 0 - valLoss: 0.6646101474761963 - trainLoss: 0.6631754636764526\n",
      "cnt: 0 - valLoss: 0.6646095514297485 - trainLoss: 0.6631749868392944\n",
      "cnt: 0 - valLoss: 0.6646090149879456 - trainLoss: 0.6631745100021362\n",
      "cnt: 0 - valLoss: 0.664608359336853 - trainLoss: 0.663174033164978\n",
      "cnt: 0 - valLoss: 0.6646077632904053 - trainLoss: 0.663173496723175\n",
      "cnt: 0 - valLoss: 0.6646071672439575 - trainLoss: 0.6631730794906616\n",
      "cnt: 0 - valLoss: 0.6646065711975098 - trainLoss: 0.6631725430488586\n",
      "cnt: 0 - valLoss: 0.664605975151062 - trainLoss: 0.6631721258163452\n",
      "cnt: 0 - valLoss: 0.664605438709259 - trainLoss: 0.663171648979187\n",
      "cnt: 0 - valLoss: 0.6646047830581665 - trainLoss: 0.6631711721420288\n",
      "cnt: 0 - valLoss: 0.6646042466163635 - trainLoss: 0.6631706953048706\n",
      "cnt: 0 - valLoss: 0.6646036505699158 - trainLoss: 0.6631701588630676\n",
      "cnt: 0 - valLoss: 0.664603054523468 - trainLoss: 0.6631697416305542\n",
      "cnt: 0 - valLoss: 0.6646024584770203 - trainLoss: 0.6631692051887512\n",
      "cnt: 0 - valLoss: 0.6646018624305725 - trainLoss: 0.6631687879562378\n",
      "cnt: 0 - valLoss: 0.6646012663841248 - trainLoss: 0.6631683111190796\n",
      "cnt: 0 - valLoss: 0.664600670337677 - trainLoss: 0.6631678342819214\n",
      "cnt: 0 - valLoss: 0.664600133895874 - trainLoss: 0.6631673574447632\n",
      "cnt: 0 - valLoss: 0.6645994782447815 - trainLoss: 0.663166880607605\n",
      "cnt: 0 - valLoss: 0.6645989418029785 - trainLoss: 0.6631664037704468\n",
      "cnt: 0 - valLoss: 0.664598286151886 - trainLoss: 0.6631658673286438\n",
      "cnt: 0 - valLoss: 0.664597749710083 - trainLoss: 0.6631653904914856\n",
      "cnt: 0 - valLoss: 0.6645970940589905 - trainLoss: 0.6631649136543274\n",
      "cnt: 0 - valLoss: 0.6645965576171875 - trainLoss: 0.6631644368171692\n",
      "cnt: 0 - valLoss: 0.6645959615707397 - trainLoss: 0.663163959980011\n",
      "cnt: 0 - valLoss: 0.664595365524292 - trainLoss: 0.6631634831428528\n",
      "cnt: 0 - valLoss: 0.6645947694778442 - trainLoss: 0.6631630063056946\n",
      "cnt: 0 - valLoss: 0.6645941734313965 - trainLoss: 0.6631625294685364\n",
      "cnt: 0 - valLoss: 0.6645935773849487 - trainLoss: 0.6631620526313782\n",
      "cnt: 0 - valLoss: 0.664592981338501 - trainLoss: 0.6631615161895752\n",
      "cnt: 0 - valLoss: 0.6645923852920532 - trainLoss: 0.663161039352417\n",
      "cnt: 0 - valLoss: 0.6645917296409607 - trainLoss: 0.6631605625152588\n",
      "cnt: 0 - valLoss: 0.6645911335945129 - trainLoss: 0.6631600856781006\n",
      "cnt: 0 - valLoss: 0.6645905375480652 - trainLoss: 0.6631595492362976\n",
      "cnt: 0 - valLoss: 0.6645899415016174 - trainLoss: 0.6631591320037842\n",
      "cnt: 0 - valLoss: 0.6645894050598145 - trainLoss: 0.6631585955619812\n",
      "cnt: 0 - valLoss: 0.6645888686180115 - trainLoss: 0.663158118724823\n",
      "cnt: 0 - valLoss: 0.664588212966919 - trainLoss: 0.6631577014923096\n",
      "cnt: 0 - valLoss: 0.6645876169204712 - trainLoss: 0.6631571054458618\n",
      "cnt: 0 - valLoss: 0.6645870208740234 - trainLoss: 0.6631566286087036\n",
      "cnt: 0 - valLoss: 0.6645864844322205 - trainLoss: 0.6631561517715454\n",
      "cnt: 0 - valLoss: 0.6645858287811279 - trainLoss: 0.6631556749343872\n",
      "cnt: 0 - valLoss: 0.6645852327346802 - trainLoss: 0.6631550788879395\n",
      "cnt: 0 - valLoss: 0.6645846366882324 - trainLoss: 0.663154661655426\n",
      "cnt: 0 - valLoss: 0.6645839810371399 - trainLoss: 0.6631541848182678\n",
      "cnt: 0 - valLoss: 0.6645833849906921 - trainLoss: 0.6631536483764648\n",
      "cnt: 0 - valLoss: 0.6645827889442444 - trainLoss: 0.6631532311439514\n",
      "cnt: 0 - valLoss: 0.6645822525024414 - trainLoss: 0.6631526947021484\n",
      "cnt: 0 - valLoss: 0.6645816564559937 - trainLoss: 0.6631522178649902\n",
      "cnt: 0 - valLoss: 0.6645810008049011 - trainLoss: 0.6631516814231873\n",
      "cnt: 0 - valLoss: 0.6645804643630981 - trainLoss: 0.663151204586029\n",
      "cnt: 0 - valLoss: 0.6645798087120056 - trainLoss: 0.6631507277488708\n",
      "cnt: 0 - valLoss: 0.6645792126655579 - trainLoss: 0.6631502509117126\n",
      "cnt: 0 - valLoss: 0.6645786166191101 - trainLoss: 0.6631497144699097\n",
      "cnt: 0 - valLoss: 0.6645780205726624 - trainLoss: 0.6631492376327515\n",
      "cnt: 0 - valLoss: 0.6645774245262146 - trainLoss: 0.6631487011909485\n",
      "cnt: 0 - valLoss: 0.6645767688751221 - trainLoss: 0.6631482839584351\n",
      "cnt: 0 - valLoss: 0.6645761728286743 - trainLoss: 0.6631477475166321\n",
      "cnt: 0 - valLoss: 0.6645755767822266 - trainLoss: 0.6631472110748291\n",
      "cnt: 0 - valLoss: 0.6645749807357788 - trainLoss: 0.6631467342376709\n",
      "cnt: 0 - valLoss: 0.664574384689331 - trainLoss: 0.6631462574005127\n",
      "cnt: 0 - valLoss: 0.6645737886428833 - trainLoss: 0.6631457209587097\n",
      "cnt: 0 - valLoss: 0.6645731925964355 - trainLoss: 0.6631452441215515\n",
      "cnt: 0 - valLoss: 0.6645725965499878 - trainLoss: 0.6631447672843933\n",
      "cnt: 0 - valLoss: 0.6645719408988953 - trainLoss: 0.6631442308425903\n",
      "cnt: 0 - valLoss: 0.6645714044570923 - trainLoss: 0.6631438136100769\n",
      "cnt: 0 - valLoss: 0.6645707488059998 - trainLoss: 0.6631432771682739\n",
      "cnt: 0 - valLoss: 0.664570152759552 - trainLoss: 0.6631428003311157\n",
      "cnt: 0 - valLoss: 0.6645695567131042 - trainLoss: 0.6631422638893127\n",
      "cnt: 0 - valLoss: 0.6645689010620117 - trainLoss: 0.6631417274475098\n",
      "cnt: 0 - valLoss: 0.664568305015564 - trainLoss: 0.6631412506103516\n",
      "cnt: 0 - valLoss: 0.6645676493644714 - trainLoss: 0.6631408333778381\n",
      "cnt: 0 - valLoss: 0.6645671725273132 - trainLoss: 0.6631402969360352\n",
      "cnt: 0 - valLoss: 0.6645665764808655 - trainLoss: 0.663139820098877\n",
      "cnt: 0 - valLoss: 0.6645659804344177 - trainLoss: 0.663139283657074\n",
      "cnt: 0 - valLoss: 0.66456538438797 - trainLoss: 0.6631388664245605\n",
      "cnt: 0 - valLoss: 0.6645647883415222 - trainLoss: 0.6631383299827576\n",
      "cnt: 0 - valLoss: 0.6645641922950745 - trainLoss: 0.6631379127502441\n",
      "cnt: 0 - valLoss: 0.6645636558532715 - trainLoss: 0.6631374359130859\n",
      "cnt: 0 - valLoss: 0.6645630598068237 - trainLoss: 0.663136899471283\n",
      "cnt: 0 - valLoss: 0.6645623445510864 - trainLoss: 0.6631364226341248\n",
      "cnt: 0 - valLoss: 0.6645617485046387 - trainLoss: 0.6631359457969666\n",
      "cnt: 0 - valLoss: 0.6645612120628357 - trainLoss: 0.6631354093551636\n",
      "cnt: 0 - valLoss: 0.6645606160163879 - trainLoss: 0.6631348729133606\n",
      "cnt: 0 - valLoss: 0.6645600199699402 - trainLoss: 0.6631344556808472\n",
      "cnt: 0 - valLoss: 0.6645594239234924 - trainLoss: 0.663133978843689\n",
      "cnt: 0 - valLoss: 0.6645588278770447 - trainLoss: 0.6631335020065308\n",
      "cnt: 0 - valLoss: 0.6645582318305969 - trainLoss: 0.6631329655647278\n",
      "cnt: 0 - valLoss: 0.6645576357841492 - trainLoss: 0.6631324887275696\n",
      "cnt: 0 - valLoss: 0.6645570397377014 - trainLoss: 0.6631320118904114\n",
      "cnt: 0 - valLoss: 0.6645564436912537 - trainLoss: 0.6631315350532532\n",
      "cnt: 0 - valLoss: 0.6645559072494507 - trainLoss: 0.663131058216095\n",
      "cnt: 0 - valLoss: 0.6645553112030029 - trainLoss: 0.6631305813789368\n",
      "cnt: 0 - valLoss: 0.6645546555519104 - trainLoss: 0.6631300449371338\n",
      "cnt: 0 - valLoss: 0.6645540595054626 - trainLoss: 0.6631295680999756\n",
      "cnt: 0 - valLoss: 0.6645534038543701 - trainLoss: 0.6631290912628174\n",
      "cnt: 0 - valLoss: 0.6645528078079224 - trainLoss: 0.6631285548210144\n",
      "cnt: 0 - valLoss: 0.6645522117614746 - trainLoss: 0.6631280779838562\n",
      "cnt: 0 - valLoss: 0.6645516157150269 - trainLoss: 0.663127601146698\n",
      "cnt: 0 - valLoss: 0.6645510196685791 - trainLoss: 0.663127064704895\n",
      "cnt: 0 - valLoss: 0.6645504236221313 - trainLoss: 0.6631265878677368\n",
      "cnt: 0 - valLoss: 0.6645497679710388 - trainLoss: 0.6631261110305786\n",
      "cnt: 0 - valLoss: 0.6645491719245911 - trainLoss: 0.6631256341934204\n",
      "cnt: 0 - valLoss: 0.6645485758781433 - trainLoss: 0.6631250381469727\n",
      "cnt: 0 - valLoss: 0.6645479798316956 - trainLoss: 0.6631245613098145\n",
      "cnt: 0 - valLoss: 0.664547324180603 - trainLoss: 0.6631240844726562\n",
      "cnt: 0 - valLoss: 0.6645467281341553 - trainLoss: 0.663123607635498\n",
      "cnt: 0 - valLoss: 0.6645461320877075 - trainLoss: 0.6631230711936951\n",
      "cnt: 0 - valLoss: 0.664545476436615 - trainLoss: 0.6631225943565369\n",
      "cnt: 0 - valLoss: 0.6645448803901672 - trainLoss: 0.6631221175193787\n",
      "cnt: 0 - valLoss: 0.6645442843437195 - trainLoss: 0.6631215214729309\n",
      "cnt: 0 - valLoss: 0.6645436882972717 - trainLoss: 0.6631211042404175\n",
      "cnt: 0 - valLoss: 0.6645429730415344 - trainLoss: 0.6631206274032593\n",
      "cnt: 0 - valLoss: 0.6645423769950867 - trainLoss: 0.6631200909614563\n",
      "cnt: 0 - valLoss: 0.6645417809486389 - trainLoss: 0.6631195545196533\n",
      "cnt: 0 - valLoss: 0.6645411252975464 - trainLoss: 0.6631190180778503\n",
      "cnt: 0 - valLoss: 0.6645404696464539 - trainLoss: 0.6631185412406921\n",
      "cnt: 0 - valLoss: 0.6645398736000061 - trainLoss: 0.6631180644035339\n",
      "cnt: 0 - valLoss: 0.6645392179489136 - trainLoss: 0.663117527961731\n",
      "cnt: 0 - valLoss: 0.6645386219024658 - trainLoss: 0.663116991519928\n",
      "cnt: 0 - valLoss: 0.6645380258560181 - trainLoss: 0.6631165146827698\n",
      "cnt: 0 - valLoss: 0.6645373702049255 - trainLoss: 0.6631159782409668\n",
      "cnt: 0 - valLoss: 0.6645366549491882 - trainLoss: 0.6631154417991638\n",
      "cnt: 0 - valLoss: 0.6645361185073853 - trainLoss: 0.6631149649620056\n",
      "cnt: 0 - valLoss: 0.6645354628562927 - trainLoss: 0.6631144881248474\n",
      "cnt: 0 - valLoss: 0.664534866809845 - trainLoss: 0.6631139516830444\n",
      "cnt: 0 - valLoss: 0.6645341515541077 - trainLoss: 0.6631134748458862\n",
      "cnt: 0 - valLoss: 0.6645336151123047 - trainLoss: 0.6631128787994385\n",
      "cnt: 0 - valLoss: 0.6645329594612122 - trainLoss: 0.6631124019622803\n",
      "cnt: 0 - valLoss: 0.6645323634147644 - trainLoss: 0.6631118655204773\n",
      "cnt: 0 - valLoss: 0.6645316481590271 - trainLoss: 0.6631113886833191\n",
      "cnt: 0 - valLoss: 0.6645310521125793 - trainLoss: 0.6631108522415161\n",
      "cnt: 0 - valLoss: 0.6645303964614868 - trainLoss: 0.6631103754043579\n",
      "cnt: 0 - valLoss: 0.6645298004150391 - trainLoss: 0.6631098389625549\n",
      "cnt: 0 - valLoss: 0.6645292043685913 - trainLoss: 0.663109302520752\n",
      "cnt: 0 - valLoss: 0.6645285487174988 - trainLoss: 0.663108766078949\n",
      "cnt: 0 - valLoss: 0.6645278930664062 - trainLoss: 0.6631082892417908\n",
      "cnt: 0 - valLoss: 0.6645272374153137 - trainLoss: 0.6631078124046326\n",
      "cnt: 0 - valLoss: 0.664526641368866 - trainLoss: 0.6631072759628296\n",
      "cnt: 0 - valLoss: 0.6645260453224182 - trainLoss: 0.6631067395210266\n",
      "cnt: 0 - valLoss: 0.6645253896713257 - trainLoss: 0.6631062030792236\n",
      "cnt: 0 - valLoss: 0.6645247340202332 - trainLoss: 0.6631057262420654\n",
      "cnt: 0 - valLoss: 0.6645241379737854 - trainLoss: 0.6631051898002625\n",
      "cnt: 0 - valLoss: 0.6645234227180481 - trainLoss: 0.6631047129631042\n",
      "cnt: 0 - valLoss: 0.6645228266716003 - trainLoss: 0.6631041765213013\n",
      "cnt: 0 - valLoss: 0.6645221710205078 - trainLoss: 0.6631036400794983\n",
      "cnt: 0 - valLoss: 0.6645215749740601 - trainLoss: 0.6631031632423401\n",
      "cnt: 0 - valLoss: 0.6645209789276123 - trainLoss: 0.6631026268005371\n",
      "cnt: 0 - valLoss: 0.6645203232765198 - trainLoss: 0.6631020903587341\n",
      "cnt: 0 - valLoss: 0.664519727230072 - trainLoss: 0.6631016135215759\n",
      "cnt: 0 - valLoss: 0.6645190119743347 - trainLoss: 0.663101077079773\n",
      "cnt: 0 - valLoss: 0.6645183563232422 - trainLoss: 0.6631006002426147\n",
      "cnt: 0 - valLoss: 0.6645177602767944 - trainLoss: 0.6631000638008118\n",
      "cnt: 0 - valLoss: 0.6645171046257019 - trainLoss: 0.6630995273590088\n",
      "cnt: 0 - valLoss: 0.6645165085792542 - trainLoss: 0.6630990505218506\n",
      "cnt: 0 - valLoss: 0.6645159125328064 - trainLoss: 0.6630985140800476\n",
      "cnt: 0 - valLoss: 0.6645151972770691 - trainLoss: 0.6630980372428894\n",
      "cnt: 0 - valLoss: 0.6645145416259766 - trainLoss: 0.6630975008010864\n",
      "cnt: 0 - valLoss: 0.6645139455795288 - trainLoss: 0.6630969643592834\n",
      "cnt: 0 - valLoss: 0.664513349533081 - trainLoss: 0.6630964875221252\n",
      "cnt: 0 - valLoss: 0.6645126342773438 - trainLoss: 0.6630959510803223\n",
      "cnt: 0 - valLoss: 0.664512038230896 - trainLoss: 0.6630954146385193\n",
      "cnt: 0 - valLoss: 0.6645113825798035 - trainLoss: 0.6630949378013611\n",
      "cnt: 0 - valLoss: 0.6645107269287109 - trainLoss: 0.6630943417549133\n",
      "cnt: 0 - valLoss: 0.6645101308822632 - trainLoss: 0.6630938649177551\n",
      "cnt: 0 - valLoss: 0.6645094156265259 - trainLoss: 0.6630933284759521\n",
      "cnt: 0 - valLoss: 0.6645088791847229 - trainLoss: 0.663092851638794\n",
      "cnt: 0 - valLoss: 0.6645082235336304 - trainLoss: 0.6630922555923462\n",
      "cnt: 0 - valLoss: 0.6645075082778931 - trainLoss: 0.663091778755188\n",
      "cnt: 0 - valLoss: 0.6645069122314453 - trainLoss: 0.663091242313385\n",
      "cnt: 0 - valLoss: 0.6645063161849976 - trainLoss: 0.663090705871582\n",
      "cnt: 0 - valLoss: 0.6645056009292603 - trainLoss: 0.6630902290344238\n",
      "cnt: 0 - valLoss: 0.6645050048828125 - trainLoss: 0.6630896925926208\n",
      "cnt: 0 - valLoss: 0.6645042896270752 - trainLoss: 0.6630892157554626\n",
      "cnt: 0 - valLoss: 0.6645036935806274 - trainLoss: 0.6630886793136597\n",
      "cnt: 0 - valLoss: 0.6645029783248901 - trainLoss: 0.6630881428718567\n",
      "cnt: 0 - valLoss: 0.6645023226737976 - trainLoss: 0.6630876064300537\n",
      "cnt: 0 - valLoss: 0.6645017266273499 - trainLoss: 0.6630870699882507\n",
      "cnt: 0 - valLoss: 0.6645010709762573 - trainLoss: 0.6630865931510925\n",
      "cnt: 0 - valLoss: 0.6645004153251648 - trainLoss: 0.6630859971046448\n",
      "cnt: 0 - valLoss: 0.6644997596740723 - trainLoss: 0.6630855202674866\n",
      "cnt: 0 - valLoss: 0.6644991040229797 - trainLoss: 0.6630849838256836\n",
      "cnt: 0 - valLoss: 0.6644984483718872 - trainLoss: 0.6630845069885254\n",
      "cnt: 0 - valLoss: 0.6644977927207947 - trainLoss: 0.6630839705467224\n",
      "cnt: 0 - valLoss: 0.6644971370697021 - trainLoss: 0.6630834341049194\n",
      "cnt: 0 - valLoss: 0.6644965410232544 - trainLoss: 0.6630829572677612\n",
      "cnt: 0 - valLoss: 0.6644959449768066 - trainLoss: 0.6630823612213135\n",
      "cnt: 0 - valLoss: 0.6644951701164246 - trainLoss: 0.6630818247795105\n",
      "cnt: 0 - valLoss: 0.6644945740699768 - trainLoss: 0.6630813479423523\n",
      "cnt: 0 - valLoss: 0.6644939184188843 - trainLoss: 0.6630808711051941\n",
      "cnt: 0 - valLoss: 0.6644933223724365 - trainLoss: 0.6630802154541016\n",
      "cnt: 0 - valLoss: 0.664492666721344 - trainLoss: 0.6630797982215881\n",
      "cnt: 0 - valLoss: 0.6644919514656067 - trainLoss: 0.6630793213844299\n",
      "cnt: 0 - valLoss: 0.6644912958145142 - trainLoss: 0.6630787253379822\n",
      "cnt: 0 - valLoss: 0.6644906401634216 - trainLoss: 0.6630781888961792\n",
      "cnt: 0 - valLoss: 0.6644899845123291 - trainLoss: 0.6630776524543762\n",
      "cnt: 0 - valLoss: 0.6644893884658813 - trainLoss: 0.6630771160125732\n",
      "cnt: 0 - valLoss: 0.6644887328147888 - trainLoss: 0.6630766987800598\n",
      "cnt: 0 - valLoss: 0.6644880771636963 - trainLoss: 0.6630761027336121\n",
      "cnt: 0 - valLoss: 0.6644874215126038 - trainLoss: 0.6630755662918091\n",
      "cnt: 0 - valLoss: 0.6644867658615112 - trainLoss: 0.6630750298500061\n",
      "cnt: 0 - valLoss: 0.6644860506057739 - trainLoss: 0.6630745530128479\n",
      "cnt: 0 - valLoss: 0.6644853949546814 - trainLoss: 0.6630740165710449\n",
      "cnt: 0 - valLoss: 0.6644847989082336 - trainLoss: 0.6630734205245972\n",
      "cnt: 0 - valLoss: 0.6644841432571411 - trainLoss: 0.663072943687439\n",
      "cnt: 0 - valLoss: 0.6644834876060486 - trainLoss: 0.663072407245636\n",
      "cnt: 0 - valLoss: 0.664482831954956 - trainLoss: 0.663071870803833\n",
      "cnt: 0 - valLoss: 0.6644821763038635 - trainLoss: 0.6630713939666748\n",
      "cnt: 0 - valLoss: 0.664481520652771 - trainLoss: 0.6630708575248718\n",
      "cnt: 0 - valLoss: 0.6644808650016785 - trainLoss: 0.6630703806877136\n",
      "cnt: 0 - valLoss: 0.6644802093505859 - trainLoss: 0.6630698442459106\n",
      "cnt: 0 - valLoss: 0.6644795536994934 - trainLoss: 0.6630693078041077\n",
      "cnt: 0 - valLoss: 0.6644788384437561 - trainLoss: 0.6630687713623047\n",
      "cnt: 0 - valLoss: 0.6644782423973083 - trainLoss: 0.6630682349205017\n",
      "cnt: 0 - valLoss: 0.6644775867462158 - trainLoss: 0.6630676984786987\n",
      "cnt: 0 - valLoss: 0.6644769310951233 - trainLoss: 0.6630671620368958\n",
      "cnt: 0 - valLoss: 0.6644762754440308 - trainLoss: 0.6630667448043823\n",
      "cnt: 0 - valLoss: 0.664475679397583 - trainLoss: 0.6630661487579346\n",
      "cnt: 0 - valLoss: 0.6644749641418457 - trainLoss: 0.6630656719207764\n",
      "cnt: 0 - valLoss: 0.6644743084907532 - trainLoss: 0.6630650758743286\n",
      "cnt: 0 - valLoss: 0.6644736528396606 - trainLoss: 0.6630645990371704\n",
      "cnt: 0 - valLoss: 0.6644729375839233 - trainLoss: 0.6630641222000122\n",
      "cnt: 0 - valLoss: 0.6644723415374756 - trainLoss: 0.6630635261535645\n",
      "cnt: 0 - valLoss: 0.6644716858863831 - trainLoss: 0.6630630493164062\n",
      "cnt: 0 - valLoss: 0.6644710302352905 - trainLoss: 0.6630624532699585\n",
      "cnt: 0 - valLoss: 0.6644704341888428 - trainLoss: 0.6630620360374451\n",
      "cnt: 0 - valLoss: 0.6644696593284607 - trainLoss: 0.6630614399909973\n",
      "cnt: 0 - valLoss: 0.6644690632820129 - trainLoss: 0.6630609631538391\n",
      "cnt: 0 - valLoss: 0.6644684076309204 - trainLoss: 0.6630604267120361\n",
      "cnt: 0 - valLoss: 0.6644678115844727 - trainLoss: 0.6630598902702332\n",
      "cnt: 0 - valLoss: 0.6644670963287354 - trainLoss: 0.663059413433075\n",
      "cnt: 0 - valLoss: 0.6644664406776428 - trainLoss: 0.6630588173866272\n",
      "cnt: 0 - valLoss: 0.6644657254219055 - trainLoss: 0.663058340549469\n",
      "cnt: 0 - valLoss: 0.6644651293754578 - trainLoss: 0.663057804107666\n",
      "cnt: 0 - valLoss: 0.6644644737243652 - trainLoss: 0.663057267665863\n",
      "cnt: 0 - valLoss: 0.6644637584686279 - trainLoss: 0.6630567312240601\n",
      "cnt: 0 - valLoss: 0.6644631624221802 - trainLoss: 0.6630562543869019\n",
      "cnt: 0 - valLoss: 0.6644625067710876 - trainLoss: 0.6630557179450989\n",
      "cnt: 0 - valLoss: 0.6644617915153503 - trainLoss: 0.6630551815032959\n",
      "cnt: 0 - valLoss: 0.6644611954689026 - trainLoss: 0.6630547046661377\n",
      "cnt: 0 - valLoss: 0.6644605398178101 - trainLoss: 0.6630541682243347\n",
      "cnt: 0 - valLoss: 0.6644598245620728 - trainLoss: 0.6630536913871765\n",
      "cnt: 0 - valLoss: 0.6644591689109802 - trainLoss: 0.6630531549453735\n",
      "cnt: 0 - valLoss: 0.6644585132598877 - trainLoss: 0.6630525588989258\n",
      "cnt: 0 - valLoss: 0.6644578576087952 - trainLoss: 0.6630520224571228\n",
      "cnt: 0 - valLoss: 0.6644572019577026 - trainLoss: 0.6630515456199646\n",
      "cnt: 0 - valLoss: 0.6644565463066101 - trainLoss: 0.6630510091781616\n",
      "cnt: 0 - valLoss: 0.6644559502601624 - trainLoss: 0.6630504727363586\n",
      "cnt: 0 - valLoss: 0.664455235004425 - trainLoss: 0.6630499958992004\n",
      "cnt: 0 - valLoss: 0.6644545793533325 - trainLoss: 0.6630494594573975\n",
      "cnt: 0 - valLoss: 0.66445392370224 - trainLoss: 0.6630489230155945\n",
      "cnt: 0 - valLoss: 0.6644532680511475 - trainLoss: 0.6630483865737915\n",
      "cnt: 0 - valLoss: 0.6644526124000549 - trainLoss: 0.6630479097366333\n",
      "cnt: 0 - valLoss: 0.6644519567489624 - trainLoss: 0.6630473732948303\n",
      "cnt: 0 - valLoss: 0.6644512414932251 - trainLoss: 0.6630468368530273\n",
      "cnt: 0 - valLoss: 0.6644506454467773 - trainLoss: 0.6630463004112244\n",
      "cnt: 0 - valLoss: 0.6644499897956848 - trainLoss: 0.6630457639694214\n",
      "cnt: 0 - valLoss: 0.6644492745399475 - trainLoss: 0.6630452871322632\n",
      "cnt: 0 - valLoss: 0.6644486784934998 - trainLoss: 0.6630446910858154\n",
      "cnt: 0 - valLoss: 0.6644479036331177 - trainLoss: 0.663044273853302\n",
      "cnt: 0 - valLoss: 0.6644472479820251 - trainLoss: 0.6630436778068542\n",
      "cnt: 0 - valLoss: 0.6644466519355774 - trainLoss: 0.6630431413650513\n",
      "cnt: 0 - valLoss: 0.6644459366798401 - trainLoss: 0.6630426049232483\n",
      "cnt: 0 - valLoss: 0.6644453406333923 - trainLoss: 0.6630421280860901\n",
      "cnt: 0 - valLoss: 0.664444625377655 - trainLoss: 0.6630415916442871\n",
      "cnt: 0 - valLoss: 0.6644439697265625 - trainLoss: 0.6630410552024841\n",
      "cnt: 0 - valLoss: 0.6644433736801147 - trainLoss: 0.6630404591560364\n",
      "cnt: 0 - valLoss: 0.6644426584243774 - trainLoss: 0.6630399823188782\n",
      "cnt: 0 - valLoss: 0.6644420027732849 - trainLoss: 0.6630393862724304\n",
      "cnt: 0 - valLoss: 0.6644412875175476 - trainLoss: 0.663038969039917\n",
      "cnt: 0 - valLoss: 0.6644406914710999 - trainLoss: 0.6630383729934692\n",
      "cnt: 0 - valLoss: 0.6644399762153625 - trainLoss: 0.663037896156311\n",
      "cnt: 0 - valLoss: 0.66443932056427 - trainLoss: 0.6630373597145081\n",
      "cnt: 0 - valLoss: 0.6644386649131775 - trainLoss: 0.6630368232727051\n",
      "cnt: 0 - valLoss: 0.6644379496574402 - trainLoss: 0.6630362868309021\n",
      "cnt: 0 - valLoss: 0.6644373536109924 - trainLoss: 0.6630357503890991\n",
      "cnt: 0 - valLoss: 0.6644366383552551 - trainLoss: 0.6630352735519409\n",
      "cnt: 0 - valLoss: 0.6644359827041626 - trainLoss: 0.6630346775054932\n",
      "cnt: 0 - valLoss: 0.6644353270530701 - trainLoss: 0.6630341410636902\n",
      "cnt: 0 - valLoss: 0.6644347310066223 - trainLoss: 0.6630336046218872\n",
      "cnt: 0 - valLoss: 0.664434015750885 - trainLoss: 0.6630330681800842\n",
      "cnt: 0 - valLoss: 0.6644333004951477 - trainLoss: 0.663032591342926\n",
      "cnt: 0 - valLoss: 0.6644326448440552 - trainLoss: 0.663032054901123\n",
      "cnt: 0 - valLoss: 0.6644320487976074 - trainLoss: 0.6630315780639648\n",
      "cnt: 0 - valLoss: 0.6644313931465149 - trainLoss: 0.6630309820175171\n",
      "cnt: 0 - valLoss: 0.6644307374954224 - trainLoss: 0.6630304455757141\n",
      "cnt: 0 - valLoss: 0.6644300818443298 - trainLoss: 0.6630299687385559\n",
      "cnt: 0 - valLoss: 0.6644293665885925 - trainLoss: 0.6630294322967529\n",
      "cnt: 0 - valLoss: 0.6644287109375 - trainLoss: 0.66302889585495\n",
      "cnt: 0 - valLoss: 0.6644279956817627 - trainLoss: 0.663028359413147\n",
      "cnt: 0 - valLoss: 0.6644273996353149 - trainLoss: 0.6630278825759888\n",
      "cnt: 0 - valLoss: 0.6644267439842224 - trainLoss: 0.663027286529541\n",
      "cnt: 0 - valLoss: 0.6644260287284851 - trainLoss: 0.6630268096923828\n",
      "cnt: 0 - valLoss: 0.6644253730773926 - trainLoss: 0.6630262732505798\n",
      "cnt: 0 - valLoss: 0.6644247174263 - trainLoss: 0.6630257368087769\n",
      "cnt: 0 - valLoss: 0.6644240617752075 - trainLoss: 0.6630252003669739\n",
      "cnt: 0 - valLoss: 0.664423406124115 - trainLoss: 0.6630246639251709\n",
      "cnt: 0 - valLoss: 0.6644227504730225 - trainLoss: 0.6630241274833679\n",
      "cnt: 0 - valLoss: 0.6644220948219299 - trainLoss: 0.6630235910415649\n",
      "cnt: 0 - valLoss: 0.6644214391708374 - trainLoss: 0.6630229949951172\n",
      "cnt: 0 - valLoss: 0.6644207239151001 - trainLoss: 0.6630225777626038\n",
      "cnt: 0 - valLoss: 0.6644200682640076 - trainLoss: 0.663021981716156\n",
      "cnt: 0 - valLoss: 0.664419412612915 - trainLoss: 0.6630215048789978\n",
      "cnt: 0 - valLoss: 0.6644187569618225 - trainLoss: 0.6630209684371948\n",
      "cnt: 0 - valLoss: 0.66441810131073 - trainLoss: 0.6630204319953918\n",
      "cnt: 0 - valLoss: 0.6644174456596375 - trainLoss: 0.6630198955535889\n",
      "cnt: 0 - valLoss: 0.6644167304039001 - trainLoss: 0.6630192995071411\n",
      "cnt: 0 - valLoss: 0.6644160747528076 - trainLoss: 0.6630188226699829\n",
      "cnt: 0 - valLoss: 0.6644154191017151 - trainLoss: 0.6630182862281799\n",
      "cnt: 0 - valLoss: 0.6644147634506226 - trainLoss: 0.663017749786377\n",
      "cnt: 0 - valLoss: 0.6644140481948853 - trainLoss: 0.663017213344574\n",
      "cnt: 0 - valLoss: 0.6644133925437927 - trainLoss: 0.663016676902771\n",
      "cnt: 0 - valLoss: 0.6644127368927002 - trainLoss: 0.6630160808563232\n",
      "cnt: 0 - valLoss: 0.6644120216369629 - trainLoss: 0.663015604019165\n",
      "cnt: 0 - valLoss: 0.6644113659858704 - trainLoss: 0.6630151271820068\n",
      "cnt: 0 - valLoss: 0.6644106507301331 - trainLoss: 0.6630146503448486\n",
      "cnt: 0 - valLoss: 0.6644099950790405 - trainLoss: 0.6630140542984009\n",
      "cnt: 0 - valLoss: 0.664409339427948 - trainLoss: 0.6630134582519531\n",
      "cnt: 0 - valLoss: 0.6644086241722107 - trainLoss: 0.6630129218101501\n",
      "cnt: 0 - valLoss: 0.6644079685211182 - trainLoss: 0.6630123853683472\n",
      "cnt: 0 - valLoss: 0.6644073128700256 - trainLoss: 0.663011908531189\n",
      "cnt: 0 - valLoss: 0.6644066572189331 - trainLoss: 0.6630113124847412\n",
      "cnt: 0 - valLoss: 0.6644060015678406 - trainLoss: 0.6630107760429382\n",
      "cnt: 0 - valLoss: 0.6644052863121033 - trainLoss: 0.6630102396011353\n",
      "cnt: 0 - valLoss: 0.6644046306610107 - trainLoss: 0.6630097031593323\n",
      "cnt: 0 - valLoss: 0.6644039154052734 - trainLoss: 0.6630091667175293\n",
      "cnt: 0 - valLoss: 0.6644032597541809 - trainLoss: 0.6630087494850159\n",
      "cnt: 0 - valLoss: 0.6644025444984436 - trainLoss: 0.6630081534385681\n",
      "cnt: 0 - valLoss: 0.6644019484519958 - trainLoss: 0.6630075573921204\n",
      "cnt: 0 - valLoss: 0.6644012331962585 - trainLoss: 0.6630070209503174\n",
      "cnt: 0 - valLoss: 0.664400577545166 - trainLoss: 0.6630064845085144\n",
      "cnt: 0 - valLoss: 0.6643999218940735 - trainLoss: 0.6630060076713562\n",
      "cnt: 0 - valLoss: 0.6643992066383362 - trainLoss: 0.6630054116249084\n",
      "cnt: 0 - valLoss: 0.6643985509872437 - trainLoss: 0.6630049347877502\n",
      "cnt: 0 - valLoss: 0.6643978357315063 - trainLoss: 0.6630043983459473\n",
      "cnt: 0 - valLoss: 0.6643971800804138 - trainLoss: 0.6630038619041443\n",
      "cnt: 0 - valLoss: 0.6643965244293213 - trainLoss: 0.6630033254623413\n",
      "cnt: 0 - valLoss: 0.664395809173584 - trainLoss: 0.6630027294158936\n",
      "cnt: 0 - valLoss: 0.6643950939178467 - trainLoss: 0.6630021929740906\n",
      "cnt: 0 - valLoss: 0.6643944382667542 - trainLoss: 0.6630016565322876\n",
      "cnt: 0 - valLoss: 0.6643937826156616 - trainLoss: 0.6630011796951294\n",
      "cnt: 0 - valLoss: 0.6643930673599243 - trainLoss: 0.6630005836486816\n",
      "cnt: 0 - valLoss: 0.6643924713134766 - trainLoss: 0.6630001068115234\n",
      "cnt: 0 - valLoss: 0.6643917560577393 - trainLoss: 0.6629995703697205\n",
      "cnt: 0 - valLoss: 0.6643911004066467 - trainLoss: 0.6629989743232727\n",
      "cnt: 0 - valLoss: 0.6643903851509094 - trainLoss: 0.6629984974861145\n",
      "cnt: 0 - valLoss: 0.6643897294998169 - trainLoss: 0.662997841835022\n",
      "cnt: 0 - valLoss: 0.6643891334533691 - trainLoss: 0.6629973649978638\n",
      "cnt: 0 - valLoss: 0.6643883585929871 - trainLoss: 0.662996768951416\n",
      "cnt: 0 - valLoss: 0.6643877029418945 - trainLoss: 0.6629962921142578\n",
      "cnt: 0 - valLoss: 0.6643869876861572 - trainLoss: 0.6629958152770996\n",
      "cnt: 0 - valLoss: 0.6643863916397095 - trainLoss: 0.6629952192306519\n",
      "cnt: 0 - valLoss: 0.6643856763839722 - trainLoss: 0.6629947423934937\n",
      "cnt: 0 - valLoss: 0.6643850207328796 - trainLoss: 0.6629941463470459\n",
      "cnt: 0 - valLoss: 0.6643843054771423 - trainLoss: 0.6629935503005981\n",
      "cnt: 0 - valLoss: 0.6643836498260498 - trainLoss: 0.6629930138587952\n",
      "cnt: 0 - valLoss: 0.6643829941749573 - trainLoss: 0.6629924774169922\n",
      "cnt: 0 - valLoss: 0.66438227891922 - trainLoss: 0.662992000579834\n",
      "cnt: 0 - valLoss: 0.6643816232681274 - trainLoss: 0.6629914045333862\n",
      "cnt: 0 - valLoss: 0.6643809676170349 - trainLoss: 0.662990927696228\n",
      "cnt: 0 - valLoss: 0.6643802523612976 - trainLoss: 0.6629903316497803\n",
      "cnt: 0 - valLoss: 0.6643795967102051 - trainLoss: 0.6629897952079773\n",
      "cnt: 0 - valLoss: 0.6643788814544678 - trainLoss: 0.6629892587661743\n",
      "cnt: 0 - valLoss: 0.6643781661987305 - trainLoss: 0.6629886627197266\n",
      "cnt: 0 - valLoss: 0.6643775105476379 - trainLoss: 0.6629881858825684\n",
      "cnt: 0 - valLoss: 0.6643768548965454 - trainLoss: 0.6629876494407654\n",
      "cnt: 0 - valLoss: 0.6643761396408081 - trainLoss: 0.6629871726036072\n",
      "cnt: 0 - valLoss: 0.6643755435943604 - trainLoss: 0.6629865765571594\n",
      "cnt: 0 - valLoss: 0.6643748879432678 - trainLoss: 0.6629860401153564\n",
      "cnt: 0 - valLoss: 0.6643741130828857 - trainLoss: 0.6629855036735535\n",
      "cnt: 0 - valLoss: 0.6643733978271484 - trainLoss: 0.6629849076271057\n",
      "cnt: 0 - valLoss: 0.6643728017807007 - trainLoss: 0.6629843711853027\n",
      "cnt: 0 - valLoss: 0.6643720865249634 - trainLoss: 0.6629838347434998\n",
      "cnt: 0 - valLoss: 0.6643714308738708 - trainLoss: 0.662983238697052\n",
      "cnt: 0 - valLoss: 0.6643707156181335 - trainLoss: 0.6629827618598938\n",
      "cnt: 0 - valLoss: 0.6643700003623962 - trainLoss: 0.662982165813446\n",
      "cnt: 0 - valLoss: 0.6643693447113037 - trainLoss: 0.6629816889762878\n",
      "cnt: 0 - valLoss: 0.6643686890602112 - trainLoss: 0.6629810929298401\n",
      "cnt: 0 - valLoss: 0.6643680334091187 - trainLoss: 0.6629804968833923\n",
      "cnt: 0 - valLoss: 0.6643673181533813 - trainLoss: 0.6629799604415894\n",
      "cnt: 0 - valLoss: 0.664366602897644 - trainLoss: 0.6629794239997864\n",
      "cnt: 0 - valLoss: 0.6643659472465515 - trainLoss: 0.6629788875579834\n",
      "cnt: 0 - valLoss: 0.6643651723861694 - trainLoss: 0.6629783511161804\n",
      "cnt: 0 - valLoss: 0.6643645167350769 - trainLoss: 0.6629777550697327\n",
      "cnt: 0 - valLoss: 0.6643638610839844 - trainLoss: 0.6629772782325745\n",
      "cnt: 0 - valLoss: 0.6643631458282471 - trainLoss: 0.6629766821861267\n",
      "cnt: 0 - valLoss: 0.6643625497817993 - trainLoss: 0.6629761457443237\n",
      "cnt: 0 - valLoss: 0.6643617749214172 - trainLoss: 0.6629756093025208\n",
      "cnt: 0 - valLoss: 0.6643610596656799 - trainLoss: 0.6629750728607178\n",
      "cnt: 0 - valLoss: 0.6643604040145874 - trainLoss: 0.6629745364189148\n",
      "cnt: 0 - valLoss: 0.6643596887588501 - trainLoss: 0.662973940372467\n",
      "cnt: 0 - valLoss: 0.6643590331077576 - trainLoss: 0.6629733443260193\n",
      "cnt: 0 - valLoss: 0.6643583178520203 - trainLoss: 0.6629728078842163\n",
      "cnt: 0 - valLoss: 0.6643576622009277 - trainLoss: 0.6629722714424133\n",
      "cnt: 0 - valLoss: 0.6643570065498352 - trainLoss: 0.6629717350006104\n",
      "cnt: 0 - valLoss: 0.6643562912940979 - trainLoss: 0.6629711985588074\n",
      "cnt: 0 - valLoss: 0.6643555760383606 - trainLoss: 0.6629706025123596\n",
      "cnt: 0 - valLoss: 0.6643549203872681 - trainLoss: 0.6629700660705566\n",
      "cnt: 0 - valLoss: 0.6643542051315308 - trainLoss: 0.6629695892333984\n",
      "cnt: 0 - valLoss: 0.6643535494804382 - trainLoss: 0.6629689931869507\n",
      "cnt: 0 - valLoss: 0.6643528342247009 - trainLoss: 0.6629685163497925\n",
      "cnt: 0 - valLoss: 0.6643521785736084 - trainLoss: 0.6629679203033447\n",
      "cnt: 0 - valLoss: 0.6643514633178711 - trainLoss: 0.6629673838615417\n",
      "cnt: 0 - valLoss: 0.6643508076667786 - trainLoss: 0.662966787815094\n",
      "cnt: 0 - valLoss: 0.664350152015686 - trainLoss: 0.662966251373291\n",
      "cnt: 0 - valLoss: 0.6643494367599487 - trainLoss: 0.6629657745361328\n",
      "cnt: 0 - valLoss: 0.6643487215042114 - trainLoss: 0.6629651784896851\n",
      "cnt: 0 - valLoss: 0.6643480658531189 - trainLoss: 0.6629646420478821\n",
      "cnt: 0 - valLoss: 0.6643473505973816 - trainLoss: 0.6629640460014343\n",
      "cnt: 0 - valLoss: 0.6643466949462891 - trainLoss: 0.6629635095596313\n",
      "cnt: 0 - valLoss: 0.6643459796905518 - trainLoss: 0.6629629135131836\n",
      "cnt: 0 - valLoss: 0.6643452644348145 - trainLoss: 0.6629624366760254\n",
      "cnt: 0 - valLoss: 0.6643445491790771 - trainLoss: 0.6629619002342224\n",
      "cnt: 0 - valLoss: 0.6643439531326294 - trainLoss: 0.6629613637924194\n",
      "cnt: 0 - valLoss: 0.6643432378768921 - trainLoss: 0.6629607677459717\n",
      "cnt: 0 - valLoss: 0.6643425226211548 - trainLoss: 0.6629602909088135\n",
      "cnt: 0 - valLoss: 0.6643418669700623 - trainLoss: 0.6629596948623657\n",
      "cnt: 0 - valLoss: 0.6643410921096802 - trainLoss: 0.6629591584205627\n",
      "cnt: 0 - valLoss: 0.6643404364585876 - trainLoss: 0.662958562374115\n",
      "cnt: 0 - valLoss: 0.6643397212028503 - trainLoss: 0.662958025932312\n",
      "cnt: 0 - valLoss: 0.6643390655517578 - trainLoss: 0.6629574298858643\n",
      "cnt: 0 - valLoss: 0.6643383502960205 - trainLoss: 0.6629568934440613\n",
      "cnt: 0 - valLoss: 0.664337694644928 - trainLoss: 0.6629563570022583\n",
      "cnt: 0 - valLoss: 0.6643369793891907 - trainLoss: 0.6629558205604553\n",
      "cnt: 0 - valLoss: 0.6643362045288086 - trainLoss: 0.6629552841186523\n",
      "cnt: 0 - valLoss: 0.6643355488777161 - trainLoss: 0.6629546880722046\n",
      "cnt: 0 - valLoss: 0.6643348336219788 - trainLoss: 0.6629541516304016\n",
      "cnt: 0 - valLoss: 0.664334237575531 - trainLoss: 0.6629536151885986\n",
      "cnt: 0 - valLoss: 0.6643335223197937 - trainLoss: 0.6629530191421509\n",
      "cnt: 0 - valLoss: 0.6643328070640564 - trainLoss: 0.6629524827003479\n",
      "cnt: 0 - valLoss: 0.6643320918083191 - trainLoss: 0.6629519462585449\n",
      "cnt: 0 - valLoss: 0.6643313765525818 - trainLoss: 0.6629513502120972\n",
      "cnt: 0 - valLoss: 0.6643307209014893 - trainLoss: 0.6629508137702942\n",
      "cnt: 0 - valLoss: 0.664330005645752 - trainLoss: 0.6629502773284912\n",
      "cnt: 0 - valLoss: 0.6643293499946594 - trainLoss: 0.6629497408866882\n",
      "cnt: 0 - valLoss: 0.6643285751342773 - trainLoss: 0.6629492044448853\n",
      "cnt: 0 - valLoss: 0.6643279194831848 - trainLoss: 0.6629486083984375\n",
      "cnt: 0 - valLoss: 0.6643272042274475 - trainLoss: 0.6629480123519897\n",
      "cnt: 0 - valLoss: 0.664326548576355 - trainLoss: 0.6629475355148315\n",
      "cnt: 0 - valLoss: 0.6643258333206177 - trainLoss: 0.6629469394683838\n",
      "cnt: 0 - valLoss: 0.6643251180648804 - trainLoss: 0.662946343421936\n",
      "cnt: 0 - valLoss: 0.6643244624137878 - trainLoss: 0.6629458665847778\n",
      "cnt: 0 - valLoss: 0.6643236875534058 - trainLoss: 0.6629452705383301\n",
      "cnt: 0 - valLoss: 0.6643230319023132 - trainLoss: 0.6629447937011719\n",
      "cnt: 0 - valLoss: 0.6643223166465759 - trainLoss: 0.6629441976547241\n",
      "cnt: 0 - valLoss: 0.6643216609954834 - trainLoss: 0.6629436016082764\n",
      "cnt: 0 - valLoss: 0.6643208861351013 - trainLoss: 0.6629430651664734\n",
      "cnt: 0 - valLoss: 0.6643202900886536 - trainLoss: 0.6629424691200256\n",
      "cnt: 0 - valLoss: 0.6643195152282715 - trainLoss: 0.6629419326782227\n",
      "cnt: 0 - valLoss: 0.664318859577179 - trainLoss: 0.6629414558410645\n",
      "cnt: 0 - valLoss: 0.6643181443214417 - trainLoss: 0.6629408597946167\n",
      "cnt: 0 - valLoss: 0.6643174290657043 - trainLoss: 0.662940263748169\n",
      "cnt: 0 - valLoss: 0.6643167734146118 - trainLoss: 0.6629396677017212\n",
      "cnt: 0 - valLoss: 0.6643160581588745 - trainLoss: 0.662939190864563\n",
      "cnt: 0 - valLoss: 0.6643152832984924 - trainLoss: 0.6629385948181152\n",
      "cnt: 0 - valLoss: 0.6643146276473999 - trainLoss: 0.6629380583763123\n",
      "cnt: 0 - valLoss: 0.6643139123916626 - trainLoss: 0.6629374623298645\n",
      "cnt: 0 - valLoss: 0.6643132567405701 - trainLoss: 0.6629369258880615\n",
      "cnt: 0 - valLoss: 0.6643125414848328 - trainLoss: 0.6629363298416138\n",
      "cnt: 0 - valLoss: 0.6643118858337402 - trainLoss: 0.6629357933998108\n",
      "cnt: 0 - valLoss: 0.6643111109733582 - trainLoss: 0.6629352569580078\n",
      "cnt: 0 - valLoss: 0.6643103957176208 - trainLoss: 0.6629346609115601\n",
      "cnt: 0 - valLoss: 0.6643097400665283 - trainLoss: 0.6629341244697571\n",
      "cnt: 0 - valLoss: 0.664309024810791 - trainLoss: 0.6629335880279541\n",
      "cnt: 0 - valLoss: 0.6643083095550537 - trainLoss: 0.6629329919815063\n",
      "cnt: 0 - valLoss: 0.6643075942993164 - trainLoss: 0.6629324555397034\n",
      "cnt: 0 - valLoss: 0.6643068790435791 - trainLoss: 0.6629319190979004\n",
      "cnt: 0 - valLoss: 0.664306104183197 - trainLoss: 0.6629313230514526\n",
      "cnt: 0 - valLoss: 0.6643055081367493 - trainLoss: 0.6629307270050049\n",
      "cnt: 0 - valLoss: 0.6643047332763672 - trainLoss: 0.6629301309585571\n",
      "cnt: 0 - valLoss: 0.6643040776252747 - trainLoss: 0.6629296541213989\n",
      "cnt: 0 - valLoss: 0.6643033623695374 - trainLoss: 0.6629289984703064\n",
      "cnt: 0 - valLoss: 0.6643025875091553 - trainLoss: 0.6629285216331482\n",
      "cnt: 0 - valLoss: 0.6643019318580627 - trainLoss: 0.6629279851913452\n",
      "cnt: 0 - valLoss: 0.6643012166023254 - trainLoss: 0.6629273891448975\n",
      "cnt: 0 - valLoss: 0.6643005609512329 - trainLoss: 0.6629267930984497\n",
      "cnt: 0 - valLoss: 0.6642998456954956 - trainLoss: 0.6629262566566467\n",
      "cnt: 0 - valLoss: 0.6642990708351135 - trainLoss: 0.662925660610199\n",
      "cnt: 0 - valLoss: 0.6642983555793762 - trainLoss: 0.6629251837730408\n",
      "cnt: 0 - valLoss: 0.6642976999282837 - trainLoss: 0.662924587726593\n",
      "cnt: 0 - valLoss: 0.6642969250679016 - trainLoss: 0.6629239916801453\n",
      "cnt: 0 - valLoss: 0.6642962694168091 - trainLoss: 0.6629234552383423\n",
      "cnt: 0 - valLoss: 0.6642955541610718 - trainLoss: 0.6629227995872498\n",
      "cnt: 0 - valLoss: 0.6642948389053345 - trainLoss: 0.6629223227500916\n",
      "cnt: 0 - valLoss: 0.6642941236495972 - trainLoss: 0.6629217267036438\n",
      "cnt: 0 - valLoss: 0.6642934679985046 - trainLoss: 0.662921130657196\n",
      "cnt: 0 - valLoss: 0.6642927527427673 - trainLoss: 0.6629206538200378\n",
      "cnt: 0 - valLoss: 0.66429203748703 - trainLoss: 0.6629200577735901\n",
      "cnt: 0 - valLoss: 0.6642913222312927 - trainLoss: 0.6629194617271423\n",
      "cnt: 0 - valLoss: 0.6642905473709106 - trainLoss: 0.6629189252853394\n",
      "cnt: 0 - valLoss: 0.6642898917198181 - trainLoss: 0.6629183888435364\n",
      "cnt: 0 - valLoss: 0.6642891764640808 - trainLoss: 0.6629177927970886\n",
      "cnt: 0 - valLoss: 0.6642884612083435 - trainLoss: 0.6629171967506409\n",
      "cnt: 0 - valLoss: 0.664287805557251 - trainLoss: 0.6629166603088379\n",
      "cnt: 0 - valLoss: 0.6642870903015137 - trainLoss: 0.6629160642623901\n",
      "cnt: 0 - valLoss: 0.6642863154411316 - trainLoss: 0.6629155278205872\n",
      "cnt: 0 - valLoss: 0.6642856597900391 - trainLoss: 0.6629149913787842\n",
      "cnt: 0 - valLoss: 0.664284884929657 - trainLoss: 0.6629144549369812\n",
      "cnt: 0 - valLoss: 0.6642842292785645 - trainLoss: 0.6629138588905334\n",
      "cnt: 0 - valLoss: 0.6642835140228271 - trainLoss: 0.6629132032394409\n",
      "cnt: 0 - valLoss: 0.6642827987670898 - trainLoss: 0.6629126667976379\n",
      "cnt: 0 - valLoss: 0.6642820239067078 - trainLoss: 0.6629120707511902\n",
      "cnt: 0 - valLoss: 0.6642813086509705 - trainLoss: 0.662911593914032\n",
      "cnt: 0 - valLoss: 0.6642806529998779 - trainLoss: 0.6629109978675842\n",
      "cnt: 0 - valLoss: 0.6642798781394958 - trainLoss: 0.6629104018211365\n",
      "cnt: 0 - valLoss: 0.6642792224884033 - trainLoss: 0.6629098653793335\n",
      "cnt: 0 - valLoss: 0.664278507232666 - trainLoss: 0.6629092693328857\n",
      "cnt: 0 - valLoss: 0.6642777323722839 - trainLoss: 0.662908673286438\n",
      "cnt: 0 - valLoss: 0.6642770171165466 - trainLoss: 0.662908136844635\n",
      "cnt: 0 - valLoss: 0.6642763614654541 - trainLoss: 0.662907600402832\n",
      "cnt: 0 - valLoss: 0.664275586605072 - trainLoss: 0.6629070043563843\n",
      "cnt: 0 - valLoss: 0.6642748713493347 - trainLoss: 0.6629064083099365\n",
      "cnt: 0 - valLoss: 0.6642741560935974 - trainLoss: 0.6629058122634888\n",
      "cnt: 0 - valLoss: 0.6642734408378601 - trainLoss: 0.6629053354263306\n",
      "cnt: 0 - valLoss: 0.6642727255821228 - trainLoss: 0.6629047393798828\n",
      "cnt: 0 - valLoss: 0.6642720103263855 - trainLoss: 0.6629042029380798\n",
      "cnt: 0 - valLoss: 0.6642712950706482 - trainLoss: 0.6629035472869873\n",
      "cnt: 0 - valLoss: 0.6642705798149109 - trainLoss: 0.6629030108451843\n",
      "cnt: 0 - valLoss: 0.6642698645591736 - trainLoss: 0.6629024744033813\n",
      "cnt: 0 - valLoss: 0.6642691493034363 - trainLoss: 0.6629018783569336\n",
      "cnt: 0 - valLoss: 0.664268434047699 - trainLoss: 0.6629012823104858\n",
      "cnt: 0 - valLoss: 0.6642676591873169 - trainLoss: 0.6629007458686829\n",
      "cnt: 0 - valLoss: 0.6642670631408691 - trainLoss: 0.6629001498222351\n",
      "cnt: 0 - valLoss: 0.6642662882804871 - trainLoss: 0.6628995537757874\n",
      "cnt: 0 - valLoss: 0.664265513420105 - trainLoss: 0.6628990173339844\n",
      "cnt: 0 - valLoss: 0.6642647981643677 - trainLoss: 0.6628984808921814\n",
      "cnt: 0 - valLoss: 0.6642641425132751 - trainLoss: 0.6628978848457336\n",
      "cnt: 0 - valLoss: 0.6642634272575378 - trainLoss: 0.6628972887992859\n",
      "cnt: 0 - valLoss: 0.6642627120018005 - trainLoss: 0.6628967523574829\n",
      "cnt: 0 - valLoss: 0.6642619371414185 - trainLoss: 0.6628961563110352\n",
      "cnt: 0 - valLoss: 0.6642612218856812 - trainLoss: 0.6628955602645874\n",
      "cnt: 0 - valLoss: 0.6642605066299438 - trainLoss: 0.6628950238227844\n",
      "cnt: 0 - valLoss: 0.6642597913742065 - trainLoss: 0.6628944277763367\n",
      "cnt: 0 - valLoss: 0.6642590761184692 - trainLoss: 0.6628938317298889\n",
      "cnt: 0 - valLoss: 0.6642583608627319 - trainLoss: 0.6628932356834412\n",
      "cnt: 0 - valLoss: 0.6642575860023499 - trainLoss: 0.6628926992416382\n",
      "cnt: 0 - valLoss: 0.6642569303512573 - trainLoss: 0.6628921031951904\n",
      "cnt: 0 - valLoss: 0.6642561554908752 - trainLoss: 0.6628915667533875\n",
      "cnt: 0 - valLoss: 0.6642554402351379 - trainLoss: 0.6628909707069397\n",
      "cnt: 0 - valLoss: 0.6642546653747559 - trainLoss: 0.6628903746604919\n",
      "cnt: 0 - valLoss: 0.6642540097236633 - trainLoss: 0.662889838218689\n",
      "cnt: 0 - valLoss: 0.664253294467926 - trainLoss: 0.6628892421722412\n",
      "cnt: 0 - valLoss: 0.664252519607544 - trainLoss: 0.6628887057304382\n",
      "cnt: 0 - valLoss: 0.6642518043518066 - trainLoss: 0.6628881096839905\n",
      "cnt: 0 - valLoss: 0.6642510890960693 - trainLoss: 0.6628875136375427\n",
      "cnt: 0 - valLoss: 0.664250373840332 - trainLoss: 0.662886917591095\n",
      "cnt: 0 - valLoss: 0.6642496585845947 - trainLoss: 0.662886381149292\n",
      "cnt: 0 - valLoss: 0.6642488837242126 - trainLoss: 0.6628857851028442\n",
      "cnt: 0 - valLoss: 0.6642481684684753 - trainLoss: 0.6628852486610413\n",
      "cnt: 0 - valLoss: 0.664247453212738 - trainLoss: 0.6628846526145935\n",
      "cnt: 0 - valLoss: 0.6642467379570007 - trainLoss: 0.6628840565681458\n",
      "cnt: 0 - valLoss: 0.6642460227012634 - trainLoss: 0.662883460521698\n",
      "cnt: 0 - valLoss: 0.6642453074455261 - trainLoss: 0.6628828644752502\n",
      "cnt: 0 - valLoss: 0.6642444729804993 - trainLoss: 0.6628823280334473\n",
      "cnt: 0 - valLoss: 0.6642438173294067 - trainLoss: 0.6628817319869995\n",
      "cnt: 0 - valLoss: 0.6642431020736694 - trainLoss: 0.6628811955451965\n",
      "cnt: 0 - valLoss: 0.6642423868179321 - trainLoss: 0.6628805994987488\n",
      "cnt: 0 - valLoss: 0.6642416715621948 - trainLoss: 0.662880003452301\n",
      "cnt: 0 - valLoss: 0.6642409563064575 - trainLoss: 0.662879467010498\n",
      "cnt: 0 - valLoss: 0.6642401814460754 - trainLoss: 0.6628788709640503\n",
      "cnt: 0 - valLoss: 0.6642394065856934 - trainLoss: 0.6628782749176025\n",
      "cnt: 0 - valLoss: 0.6642387509346008 - trainLoss: 0.6628776788711548\n",
      "cnt: 0 - valLoss: 0.6642379760742188 - trainLoss: 0.662877082824707\n",
      "cnt: 0 - valLoss: 0.6642372012138367 - trainLoss: 0.662876546382904\n",
      "cnt: 0 - valLoss: 0.6642365455627441 - trainLoss: 0.6628759503364563\n",
      "cnt: 0 - valLoss: 0.6642357707023621 - trainLoss: 0.6628753542900085\n",
      "cnt: 0 - valLoss: 0.6642350554466248 - trainLoss: 0.6628747582435608\n",
      "cnt: 0 - valLoss: 0.6642342805862427 - trainLoss: 0.6628742218017578\n",
      "cnt: 0 - valLoss: 0.6642336249351501 - trainLoss: 0.6628736257553101\n",
      "cnt: 0 - valLoss: 0.6642329096794128 - trainLoss: 0.6628730297088623\n",
      "cnt: 0 - valLoss: 0.664232075214386 - trainLoss: 0.6628724336624146\n",
      "cnt: 0 - valLoss: 0.6642313599586487 - trainLoss: 0.6628718972206116\n",
      "cnt: 0 - valLoss: 0.6642306447029114 - trainLoss: 0.662871241569519\n",
      "cnt: 0 - valLoss: 0.6642299294471741 - trainLoss: 0.6628707051277161\n",
      "cnt: 0 - valLoss: 0.6642292141914368 - trainLoss: 0.6628701090812683\n",
      "cnt: 0 - valLoss: 0.6642284393310547 - trainLoss: 0.6628695130348206\n",
      "cnt: 0 - valLoss: 0.6642277240753174 - trainLoss: 0.6628689765930176\n",
      "cnt: 0 - valLoss: 0.6642270088195801 - trainLoss: 0.6628684401512146\n",
      "cnt: 0 - valLoss: 0.6642262935638428 - trainLoss: 0.6628677845001221\n",
      "cnt: 0 - valLoss: 0.6642255187034607 - trainLoss: 0.6628672480583191\n",
      "cnt: 0 - valLoss: 0.6642248034477234 - trainLoss: 0.6628665924072266\n",
      "cnt: 0 - valLoss: 0.6642240285873413 - trainLoss: 0.6628659963607788\n",
      "cnt: 0 - valLoss: 0.664223313331604 - trainLoss: 0.6628654599189758\n",
      "cnt: 0 - valLoss: 0.6642225384712219 - trainLoss: 0.6628649234771729\n",
      "cnt: 0 - valLoss: 0.6642218828201294 - trainLoss: 0.6628643274307251\n",
      "cnt: 0 - valLoss: 0.6642211079597473 - trainLoss: 0.6628637313842773\n",
      "cnt: 0 - valLoss: 0.6642203330993652 - trainLoss: 0.6628631353378296\n",
      "cnt: 0 - valLoss: 0.6642196178436279 - trainLoss: 0.6628625392913818\n",
      "cnt: 0 - valLoss: 0.6642188429832458 - trainLoss: 0.6628619432449341\n",
      "cnt: 0 - valLoss: 0.6642181873321533 - trainLoss: 0.6628613471984863\n",
      "cnt: 0 - valLoss: 0.664217472076416 - trainLoss: 0.6628608107566833\n",
      "cnt: 0 - valLoss: 0.6642166972160339 - trainLoss: 0.6628602147102356\n",
      "cnt: 0 - valLoss: 0.6642159223556519 - trainLoss: 0.6628596186637878\n",
      "cnt: 0 - valLoss: 0.6642152070999146 - trainLoss: 0.6628590822219849\n",
      "cnt: 0 - valLoss: 0.6642144918441772 - trainLoss: 0.6628584265708923\n",
      "cnt: 0 - valLoss: 0.6642137169837952 - trainLoss: 0.6628578305244446\n",
      "cnt: 0 - valLoss: 0.6642130017280579 - trainLoss: 0.6628572344779968\n",
      "cnt: 0 - valLoss: 0.6642122864723206 - trainLoss: 0.6628566384315491\n",
      "cnt: 0 - valLoss: 0.6642115116119385 - trainLoss: 0.6628561019897461\n",
      "cnt: 0 - valLoss: 0.6642107367515564 - trainLoss: 0.6628555059432983\n",
      "cnt: 0 - valLoss: 0.6642100214958191 - trainLoss: 0.6628549098968506\n",
      "cnt: 0 - valLoss: 0.6642093658447266 - trainLoss: 0.6628543138504028\n",
      "cnt: 0 - valLoss: 0.6642085909843445 - trainLoss: 0.6628537178039551\n",
      "cnt: 0 - valLoss: 0.6642078161239624 - trainLoss: 0.6628531217575073\n",
      "cnt: 0 - valLoss: 0.6642071008682251 - trainLoss: 0.6628525257110596\n",
      "cnt: 0 - valLoss: 0.664206326007843 - trainLoss: 0.6628519892692566\n",
      "cnt: 0 - valLoss: 0.6642056107521057 - trainLoss: 0.6628513932228088\n",
      "cnt: 0 - valLoss: 0.6642048954963684 - trainLoss: 0.6628507971763611\n",
      "cnt: 0 - valLoss: 0.6642041802406311 - trainLoss: 0.6628502011299133\n",
      "cnt: 0 - valLoss: 0.6642033457756042 - trainLoss: 0.6628496050834656\n",
      "cnt: 0 - valLoss: 0.6642026305198669 - trainLoss: 0.6628490090370178\n",
      "cnt: 0 - valLoss: 0.6642019152641296 - trainLoss: 0.6628484725952148\n",
      "cnt: 0 - valLoss: 0.6642011404037476 - trainLoss: 0.6628478765487671\n",
      "cnt: 0 - valLoss: 0.6642004251480103 - trainLoss: 0.6628472208976746\n",
      "cnt: 0 - valLoss: 0.6641996502876282 - trainLoss: 0.6628466844558716\n",
      "cnt: 0 - valLoss: 0.6641989946365356 - trainLoss: 0.6628460884094238\n",
      "cnt: 0 - valLoss: 0.6641982197761536 - trainLoss: 0.6628454923629761\n",
      "cnt: 0 - valLoss: 0.6641974449157715 - trainLoss: 0.6628448963165283\n",
      "cnt: 0 - valLoss: 0.6641967296600342 - trainLoss: 0.6628443002700806\n",
      "cnt: 0 - valLoss: 0.6641959547996521 - trainLoss: 0.6628437042236328\n",
      "cnt: 0 - valLoss: 0.66419517993927 - trainLoss: 0.6628431677818298\n",
      "cnt: 0 - valLoss: 0.6641944050788879 - trainLoss: 0.6628424525260925\n",
      "cnt: 0 - valLoss: 0.6641936302185059 - trainLoss: 0.6628419756889343\n",
      "cnt: 0 - valLoss: 0.6641929149627686 - trainLoss: 0.6628413796424866\n",
      "cnt: 0 - valLoss: 0.6641921997070312 - trainLoss: 0.6628407835960388\n",
      "cnt: 0 - valLoss: 0.664191484451294 - trainLoss: 0.6628401279449463\n",
      "cnt: 0 - valLoss: 0.6641907095909119 - trainLoss: 0.6628395915031433\n",
      "cnt: 0 - valLoss: 0.6641899943351746 - trainLoss: 0.6628389954566956\n",
      "cnt: 0 - valLoss: 0.6641892194747925 - trainLoss: 0.6628383994102478\n",
      "cnt: 0 - valLoss: 0.6641884446144104 - trainLoss: 0.6628377437591553\n",
      "cnt: 0 - valLoss: 0.6641876697540283 - trainLoss: 0.6628371477127075\n",
      "cnt: 0 - valLoss: 0.664186954498291 - trainLoss: 0.6628366112709045\n",
      "cnt: 0 - valLoss: 0.6641861796379089 - trainLoss: 0.662835955619812\n",
      "cnt: 0 - valLoss: 0.6641854643821716 - trainLoss: 0.662835419178009\n",
      "cnt: 0 - valLoss: 0.6641847491264343 - trainLoss: 0.662834882736206\n",
      "cnt: 0 - valLoss: 0.6641839742660522 - trainLoss: 0.6628342270851135\n",
      "cnt: 0 - valLoss: 0.6641831994056702 - trainLoss: 0.6628336310386658\n",
      "cnt: 0 - valLoss: 0.6641824841499329 - trainLoss: 0.6628330945968628\n",
      "cnt: 0 - valLoss: 0.6641817688941956 - trainLoss: 0.6628324389457703\n",
      "cnt: 0 - valLoss: 0.6641809940338135 - trainLoss: 0.6628318428993225\n",
      "cnt: 0 - valLoss: 0.6641802191734314 - trainLoss: 0.6628312468528748\n",
      "cnt: 0 - valLoss: 0.6641794443130493 - trainLoss: 0.662830650806427\n",
      "cnt: 0 - valLoss: 0.6641786694526672 - trainLoss: 0.6628300547599792\n",
      "cnt: 0 - valLoss: 0.6641779541969299 - trainLoss: 0.6628293991088867\n",
      "cnt: 0 - valLoss: 0.6641771793365479 - trainLoss: 0.6628288626670837\n",
      "cnt: 0 - valLoss: 0.6641764044761658 - trainLoss: 0.662828266620636\n",
      "cnt: 0 - valLoss: 0.6641756296157837 - trainLoss: 0.6628276109695435\n",
      "cnt: 0 - valLoss: 0.6641749739646912 - trainLoss: 0.6628270745277405\n",
      "cnt: 0 - valLoss: 0.6641741991043091 - trainLoss: 0.662826418876648\n",
      "cnt: 0 - valLoss: 0.664173424243927 - trainLoss: 0.6628258228302002\n",
      "cnt: 0 - valLoss: 0.6641727089881897 - trainLoss: 0.6628252863883972\n",
      "cnt: 0 - valLoss: 0.6641719937324524 - trainLoss: 0.6628246903419495\n",
      "cnt: 0 - valLoss: 0.6641712188720703 - trainLoss: 0.6628240346908569\n",
      "cnt: 0 - valLoss: 0.6641703844070435 - trainLoss: 0.6628234386444092\n",
      "cnt: 0 - valLoss: 0.6641696691513062 - trainLoss: 0.6628228425979614\n",
      "cnt: 0 - valLoss: 0.6641689538955688 - trainLoss: 0.6628221869468689\n",
      "cnt: 0 - valLoss: 0.6641681790351868 - trainLoss: 0.6628215909004211\n",
      "cnt: 0 - valLoss: 0.6641673445701599 - trainLoss: 0.6628209948539734\n",
      "cnt: 0 - valLoss: 0.6641666293144226 - trainLoss: 0.6628203988075256\n",
      "cnt: 0 - valLoss: 0.6641657948493958 - trainLoss: 0.6628198027610779\n",
      "cnt: 0 - valLoss: 0.6641651391983032 - trainLoss: 0.6628191471099854\n",
      "cnt: 0 - valLoss: 0.6641643643379211 - trainLoss: 0.6628185510635376\n",
      "cnt: 0 - valLoss: 0.6641635894775391 - trainLoss: 0.6628179550170898\n",
      "cnt: 0 - valLoss: 0.664162814617157 - trainLoss: 0.6628173589706421\n",
      "cnt: 0 - valLoss: 0.6641620397567749 - trainLoss: 0.6628167629241943\n",
      "cnt: 0 - valLoss: 0.6641613245010376 - trainLoss: 0.6628161072731018\n",
      "cnt: 0 - valLoss: 0.6641604900360107 - trainLoss: 0.662815511226654\n",
      "cnt: 0 - valLoss: 0.6641597747802734 - trainLoss: 0.6628149151802063\n",
      "cnt: 0 - valLoss: 0.6641589999198914 - trainLoss: 0.6628143191337585\n",
      "cnt: 0 - valLoss: 0.6641582250595093 - trainLoss: 0.6628136038780212\n",
      "cnt: 0 - valLoss: 0.6641574501991272 - trainLoss: 0.6628130674362183\n",
      "cnt: 0 - valLoss: 0.6641567349433899 - trainLoss: 0.6628124713897705\n",
      "cnt: 0 - valLoss: 0.6641559600830078 - trainLoss: 0.6628118753433228\n",
      "cnt: 0 - valLoss: 0.6641551852226257 - trainLoss: 0.6628112196922302\n",
      "cnt: 0 - valLoss: 0.6641544103622437 - trainLoss: 0.6628106236457825\n",
      "cnt: 0 - valLoss: 0.6641536951065063 - trainLoss: 0.6628100275993347\n",
      "cnt: 0 - valLoss: 0.6641528606414795 - trainLoss: 0.662809431552887\n",
      "cnt: 0 - valLoss: 0.6641520857810974 - trainLoss: 0.6628087759017944\n",
      "cnt: 0 - valLoss: 0.6641513705253601 - trainLoss: 0.6628081798553467\n",
      "cnt: 0 - valLoss: 0.664150595664978 - trainLoss: 0.6628075838088989\n",
      "cnt: 0 - valLoss: 0.664149820804596 - trainLoss: 0.6628069877624512\n",
      "cnt: 0 - valLoss: 0.6641490459442139 - trainLoss: 0.6628063321113586\n",
      "cnt: 0 - valLoss: 0.6641482710838318 - trainLoss: 0.6628056764602661\n",
      "cnt: 0 - valLoss: 0.6641474366188049 - trainLoss: 0.6628051400184631\n",
      "cnt: 0 - valLoss: 0.6641467213630676 - trainLoss: 0.6628045439720154\n",
      "cnt: 0 - valLoss: 0.6641460061073303 - trainLoss: 0.6628038883209229\n",
      "cnt: 0 - valLoss: 0.6641452312469482 - trainLoss: 0.6628032922744751\n",
      "cnt: 0 - valLoss: 0.6641444563865662 - trainLoss: 0.6628026366233826\n",
      "cnt: 0 - valLoss: 0.6641436815261841 - trainLoss: 0.6628021001815796\n",
      "cnt: 0 - valLoss: 0.664142906665802 - trainLoss: 0.6628014445304871\n",
      "cnt: 0 - valLoss: 0.6641421318054199 - trainLoss: 0.6628007888793945\n",
      "cnt: 0 - valLoss: 0.6641413569450378 - trainLoss: 0.6628002524375916\n",
      "cnt: 0 - valLoss: 0.6641405820846558 - trainLoss: 0.662799596786499\n",
      "cnt: 0 - valLoss: 0.6641398072242737 - trainLoss: 0.6627990007400513\n",
      "cnt: 0 - valLoss: 0.6641390323638916 - trainLoss: 0.6627984046936035\n",
      "cnt: 0 - valLoss: 0.6641382575035095 - trainLoss: 0.6627976894378662\n",
      "cnt: 0 - valLoss: 0.6641374826431274 - trainLoss: 0.6627970933914185\n",
      "cnt: 0 - valLoss: 0.6641367077827454 - trainLoss: 0.6627964973449707\n",
      "cnt: 0 - valLoss: 0.6641359925270081 - trainLoss: 0.6627958416938782\n",
      "cnt: 0 - valLoss: 0.664135217666626 - trainLoss: 0.6627952456474304\n",
      "cnt: 0 - valLoss: 0.6641343235969543 - trainLoss: 0.6627946496009827\n",
      "cnt: 0 - valLoss: 0.6641335487365723 - trainLoss: 0.6627940535545349\n",
      "cnt: 0 - valLoss: 0.664132833480835 - trainLoss: 0.6627934575080872\n",
      "cnt: 0 - valLoss: 0.6641320586204529 - trainLoss: 0.6627927422523499\n",
      "cnt: 0 - valLoss: 0.6641312837600708 - trainLoss: 0.6627921462059021\n",
      "cnt: 0 - valLoss: 0.6641305088996887 - trainLoss: 0.6627915501594543\n",
      "cnt: 0 - valLoss: 0.6641297340393066 - trainLoss: 0.6627909541130066\n",
      "cnt: 0 - valLoss: 0.6641289591789246 - trainLoss: 0.6627902984619141\n",
      "cnt: 0 - valLoss: 0.6641282439231873 - trainLoss: 0.6627896428108215\n",
      "cnt: 0 - valLoss: 0.6641274094581604 - trainLoss: 0.6627891063690186\n",
      "cnt: 0 - valLoss: 0.6641266345977783 - trainLoss: 0.662788450717926\n",
      "cnt: 0 - valLoss: 0.6641258597373962 - trainLoss: 0.6627877950668335\n",
      "cnt: 0 - valLoss: 0.6641250848770142 - trainLoss: 0.6627871990203857\n",
      "cnt: 0 - valLoss: 0.6641243100166321 - trainLoss: 0.662786602973938\n",
      "cnt: 0 - valLoss: 0.66412353515625 - trainLoss: 0.6627860069274902\n",
      "cnt: 0 - valLoss: 0.6641227602958679 - trainLoss: 0.6627853512763977\n",
      "cnt: 0 - valLoss: 0.6641219854354858 - trainLoss: 0.6627846956253052\n",
      "cnt: 0 - valLoss: 0.664121150970459 - trainLoss: 0.6627840995788574\n",
      "cnt: 0 - valLoss: 0.6641204357147217 - trainLoss: 0.6627835035324097\n",
      "cnt: 0 - valLoss: 0.6641196012496948 - trainLoss: 0.6627828478813171\n",
      "cnt: 0 - valLoss: 0.6641188263893127 - trainLoss: 0.6627821922302246\n",
      "cnt: 0 - valLoss: 0.6641180515289307 - trainLoss: 0.6627815961837769\n",
      "cnt: 0 - valLoss: 0.6641172766685486 - trainLoss: 0.6627810001373291\n",
      "cnt: 0 - valLoss: 0.6641165018081665 - trainLoss: 0.6627804040908813\n",
      "cnt: 0 - valLoss: 0.6641157269477844 - trainLoss: 0.6627797484397888\n",
      "cnt: 0 - valLoss: 0.6641148924827576 - trainLoss: 0.6627791523933411\n",
      "cnt: 0 - valLoss: 0.6641141772270203 - trainLoss: 0.6627784967422485\n",
      "cnt: 0 - valLoss: 0.6641134023666382 - trainLoss: 0.662777841091156\n",
      "cnt: 0 - valLoss: 0.6641125679016113 - trainLoss: 0.6627772450447083\n",
      "cnt: 0 - valLoss: 0.664111852645874 - trainLoss: 0.6627765893936157\n",
      "cnt: 0 - valLoss: 0.6641110777854919 - trainLoss: 0.662775993347168\n",
      "cnt: 0 - valLoss: 0.6641101837158203 - trainLoss: 0.6627753973007202\n",
      "cnt: 0 - valLoss: 0.664109468460083 - trainLoss: 0.6627747416496277\n",
      "cnt: 0 - valLoss: 0.6641086935997009 - trainLoss: 0.6627740859985352\n",
      "cnt: 0 - valLoss: 0.6641079187393188 - trainLoss: 0.6627734899520874\n",
      "cnt: 0 - valLoss: 0.6641071438789368 - trainLoss: 0.6627728343009949\n",
      "cnt: 0 - valLoss: 0.6641063690185547 - trainLoss: 0.6627721786499023\n",
      "cnt: 0 - valLoss: 0.6641055941581726 - trainLoss: 0.6627715826034546\n",
      "cnt: 0 - valLoss: 0.6641047596931458 - trainLoss: 0.6627709269523621\n",
      "cnt: 0 - valLoss: 0.6641039848327637 - trainLoss: 0.6627703309059143\n",
      "cnt: 0 - valLoss: 0.6641031503677368 - trainLoss: 0.6627697348594666\n",
      "cnt: 0 - valLoss: 0.6641024351119995 - trainLoss: 0.662769079208374\n",
      "cnt: 0 - valLoss: 0.6641016006469727 - trainLoss: 0.6627684235572815\n",
      "cnt: 0 - valLoss: 0.6641008257865906 - trainLoss: 0.6627678275108337\n",
      "cnt: 0 - valLoss: 0.6641000509262085 - trainLoss: 0.6627671718597412\n",
      "cnt: 0 - valLoss: 0.6640992760658264 - trainLoss: 0.6627665758132935\n",
      "cnt: 0 - valLoss: 0.6640984416007996 - trainLoss: 0.6627659797668457\n",
      "cnt: 0 - valLoss: 0.6640976667404175 - trainLoss: 0.6627653241157532\n",
      "cnt: 0 - valLoss: 0.6640968918800354 - trainLoss: 0.6627646684646606\n",
      "cnt: 0 - valLoss: 0.6640961170196533 - trainLoss: 0.6627640724182129\n",
      "cnt: 0 - valLoss: 0.6640953421592712 - trainLoss: 0.6627633571624756\n",
      "cnt: 0 - valLoss: 0.6640945672988892 - trainLoss: 0.6627628207206726\n",
      "cnt: 0 - valLoss: 0.6640937328338623 - trainLoss: 0.6627621650695801\n",
      "cnt: 0 - valLoss: 0.6640929579734802 - trainLoss: 0.6627615094184875\n",
      "cnt: 0 - valLoss: 0.6640921831130981 - trainLoss: 0.6627609133720398\n",
      "cnt: 0 - valLoss: 0.6640914082527161 - trainLoss: 0.662760317325592\n",
      "cnt: 0 - valLoss: 0.664090633392334 - trainLoss: 0.6627596616744995\n",
      "cnt: 0 - valLoss: 0.6640897393226624 - trainLoss: 0.6627589464187622\n",
      "cnt: 0 - valLoss: 0.6640889644622803 - trainLoss: 0.6627583503723145\n",
      "cnt: 0 - valLoss: 0.6640881896018982 - trainLoss: 0.6627576947212219\n",
      "cnt: 0 - valLoss: 0.6640874147415161 - trainLoss: 0.662757158279419\n",
      "cnt: 0 - valLoss: 0.6640866994857788 - trainLoss: 0.6627563834190369\n",
      "cnt: 0 - valLoss: 0.664085865020752 - trainLoss: 0.6627558469772339\n",
      "cnt: 0 - valLoss: 0.6640850305557251 - trainLoss: 0.6627551913261414\n",
      "cnt: 0 - valLoss: 0.664084255695343 - trainLoss: 0.6627545952796936\n",
      "cnt: 0 - valLoss: 0.6640834808349609 - trainLoss: 0.6627539396286011\n",
      "cnt: 0 - valLoss: 0.6640827059745789 - trainLoss: 0.6627532839775085\n",
      "cnt: 0 - valLoss: 0.6640818119049072 - trainLoss: 0.6627526879310608\n",
      "cnt: 0 - valLoss: 0.6640810370445251 - trainLoss: 0.6627519726753235\n",
      "cnt: 0 - valLoss: 0.6640802621841431 - trainLoss: 0.6627513766288757\n",
      "cnt: 0 - valLoss: 0.664079487323761 - trainLoss: 0.662750780582428\n",
      "cnt: 0 - valLoss: 0.6640787124633789 - trainLoss: 0.6627501249313354\n",
      "cnt: 0 - valLoss: 0.6640778183937073 - trainLoss: 0.6627495288848877\n",
      "cnt: 0 - valLoss: 0.6640770435333252 - trainLoss: 0.6627488136291504\n",
      "cnt: 0 - valLoss: 0.6640763282775879 - trainLoss: 0.6627482175827026\n",
      "cnt: 0 - valLoss: 0.6640755534172058 - trainLoss: 0.6627475619316101\n",
      "cnt: 0 - valLoss: 0.6640746593475342 - trainLoss: 0.6627469658851624\n",
      "cnt: 0 - valLoss: 0.6640738844871521 - trainLoss: 0.6627463102340698\n",
      "cnt: 0 - valLoss: 0.66407310962677 - trainLoss: 0.6627456545829773\n",
      "cnt: 0 - valLoss: 0.6640723347663879 - trainLoss: 0.6627449989318848\n",
      "cnt: 0 - valLoss: 0.6640715599060059 - trainLoss: 0.662744402885437\n",
      "cnt: 0 - valLoss: 0.664070725440979 - trainLoss: 0.6627438068389893\n",
      "cnt: 0 - valLoss: 0.6640698909759521 - trainLoss: 0.662743091583252\n",
      "cnt: 0 - valLoss: 0.6640690565109253 - trainLoss: 0.6627424955368042\n",
      "cnt: 0 - valLoss: 0.664068341255188 - trainLoss: 0.6627418398857117\n",
      "cnt: 0 - valLoss: 0.6640675067901611 - trainLoss: 0.6627411842346191\n",
      "cnt: 0 - valLoss: 0.6640666723251343 - trainLoss: 0.6627405285835266\n",
      "cnt: 0 - valLoss: 0.6640658974647522 - trainLoss: 0.6627398729324341\n",
      "cnt: 0 - valLoss: 0.6640651226043701 - trainLoss: 0.6627392172813416\n",
      "cnt: 0 - valLoss: 0.6640642881393433 - trainLoss: 0.6627386212348938\n",
      "cnt: 0 - valLoss: 0.6640635132789612 - trainLoss: 0.6627379655838013\n",
      "cnt: 0 - valLoss: 0.6640626788139343 - trainLoss: 0.6627373695373535\n",
      "cnt: 0 - valLoss: 0.6640619039535522 - trainLoss: 0.662736713886261\n",
      "cnt: 0 - valLoss: 0.6640611290931702 - trainLoss: 0.6627360582351685\n",
      "cnt: 0 - valLoss: 0.6640602946281433 - trainLoss: 0.6627354621887207\n",
      "cnt: 0 - valLoss: 0.6640595197677612 - trainLoss: 0.6627348065376282\n",
      "cnt: 0 - valLoss: 0.6640587449073792 - trainLoss: 0.6627342104911804\n",
      "cnt: 0 - valLoss: 0.6640579104423523 - trainLoss: 0.6627334952354431\n",
      "cnt: 0 - valLoss: 0.6640570759773254 - trainLoss: 0.6627328395843506\n",
      "cnt: 0 - valLoss: 0.6640563011169434 - trainLoss: 0.6627321839332581\n",
      "cnt: 0 - valLoss: 0.6640554666519165 - trainLoss: 0.6627315878868103\n",
      "cnt: 0 - valLoss: 0.6640546917915344 - trainLoss: 0.6627309918403625\n",
      "cnt: 0 - valLoss: 0.6640538573265076 - trainLoss: 0.66273033618927\n",
      "cnt: 0 - valLoss: 0.6640530824661255 - trainLoss: 0.6627296805381775\n",
      "cnt: 0 - valLoss: 0.6640523076057434 - trainLoss: 0.6627290844917297\n",
      "cnt: 0 - valLoss: 0.6640514731407166 - trainLoss: 0.6627283692359924\n",
      "cnt: 0 - valLoss: 0.6640506982803345 - trainLoss: 0.6627277731895447\n",
      "cnt: 0 - valLoss: 0.6640499234199524 - trainLoss: 0.6627271175384521\n",
      "cnt: 0 - valLoss: 0.6640490889549255 - trainLoss: 0.6627264618873596\n",
      "cnt: 0 - valLoss: 0.6640482544898987 - trainLoss: 0.6627258658409119\n",
      "cnt: 0 - valLoss: 0.6640474796295166 - trainLoss: 0.6627252101898193\n",
      "cnt: 0 - valLoss: 0.6640466451644897 - trainLoss: 0.662724494934082\n",
      "cnt: 0 - valLoss: 0.6640458703041077 - trainLoss: 0.6627238988876343\n",
      "cnt: 0 - valLoss: 0.6640450358390808 - trainLoss: 0.6627232432365417\n",
      "cnt: 0 - valLoss: 0.664044201374054 - trainLoss: 0.662722647190094\n",
      "cnt: 0 - valLoss: 0.6640434265136719 - trainLoss: 0.6627219319343567\n",
      "cnt: 0 - valLoss: 0.664042592048645 - trainLoss: 0.6627213358879089\n",
      "cnt: 0 - valLoss: 0.6640418171882629 - trainLoss: 0.6627206802368164\n",
      "cnt: 0 - valLoss: 0.6640409827232361 - trainLoss: 0.6627200245857239\n",
      "cnt: 0 - valLoss: 0.664040207862854 - trainLoss: 0.6627193689346313\n",
      "cnt: 0 - valLoss: 0.6640393733978271 - trainLoss: 0.6627187132835388\n",
      "cnt: 0 - valLoss: 0.6640385985374451 - trainLoss: 0.6627180576324463\n",
      "cnt: 0 - valLoss: 0.6640377044677734 - trainLoss: 0.6627174019813538\n",
      "cnt: 0 - valLoss: 0.6640369296073914 - trainLoss: 0.6627167463302612\n",
      "cnt: 0 - valLoss: 0.6640361547470093 - trainLoss: 0.6627161502838135\n",
      "cnt: 0 - valLoss: 0.6640353202819824 - trainLoss: 0.6627155542373657\n",
      "cnt: 0 - valLoss: 0.6640344858169556 - trainLoss: 0.6627148389816284\n",
      "cnt: 0 - valLoss: 0.6640337109565735 - trainLoss: 0.6627141833305359\n",
      "cnt: 0 - valLoss: 0.6640328764915466 - trainLoss: 0.6627135872840881\n",
      "cnt: 0 - valLoss: 0.6640320420265198 - trainLoss: 0.6627129316329956\n",
      "cnt: 0 - valLoss: 0.6640312671661377 - trainLoss: 0.6627122759819031\n",
      "cnt: 0 - valLoss: 0.6640304327011108 - trainLoss: 0.6627116203308105\n",
      "cnt: 0 - valLoss: 0.664029598236084 - trainLoss: 0.6627110242843628\n",
      "cnt: 0 - valLoss: 0.6640288233757019 - trainLoss: 0.6627103686332703\n",
      "cnt: 0 - valLoss: 0.664027988910675 - trainLoss: 0.662709653377533\n",
      "cnt: 0 - valLoss: 0.6640271544456482 - trainLoss: 0.6627089977264404\n",
      "cnt: 0 - valLoss: 0.6640263795852661 - trainLoss: 0.6627083420753479\n",
      "cnt: 0 - valLoss: 0.6640255451202393 - trainLoss: 0.6627077460289001\n",
      "cnt: 0 - valLoss: 0.6640247106552124 - trainLoss: 0.6627070903778076\n",
      "cnt: 0 - valLoss: 0.6640239357948303 - trainLoss: 0.6627064347267151\n",
      "cnt: 0 - valLoss: 0.6640230417251587 - trainLoss: 0.6627057194709778\n",
      "cnt: 0 - valLoss: 0.6640222668647766 - trainLoss: 0.66270512342453\n",
      "cnt: 0 - valLoss: 0.6640215516090393 - trainLoss: 0.6627044677734375\n",
      "cnt: 0 - valLoss: 0.6640206575393677 - trainLoss: 0.662703812122345\n",
      "cnt: 0 - valLoss: 0.6640198230743408 - trainLoss: 0.6627031564712524\n",
      "cnt: 0 - valLoss: 0.6640190482139587 - trainLoss: 0.6627025008201599\n",
      "cnt: 0 - valLoss: 0.6640181541442871 - trainLoss: 0.6627019047737122\n",
      "cnt: 0 - valLoss: 0.6640174388885498 - trainLoss: 0.6627012491226196\n",
      "cnt: 0 - valLoss: 0.6640165448188782 - trainLoss: 0.6627005338668823\n",
      "cnt: 0 - valLoss: 0.6640157103538513 - trainLoss: 0.6626999378204346\n",
      "cnt: 0 - valLoss: 0.6640149354934692 - trainLoss: 0.6626992225646973\n",
      "cnt: 0 - valLoss: 0.6640140414237976 - trainLoss: 0.6626985669136047\n",
      "cnt: 0 - valLoss: 0.6640132665634155 - trainLoss: 0.662697970867157\n",
      "cnt: 0 - valLoss: 0.6640124917030334 - trainLoss: 0.6626973152160645\n",
      "cnt: 0 - valLoss: 0.6640115976333618 - trainLoss: 0.6626965999603271\n",
      "cnt: 0 - valLoss: 0.6640108227729797 - trainLoss: 0.6626960039138794\n",
      "cnt: 0 - valLoss: 0.6640099883079529 - trainLoss: 0.6626953482627869\n",
      "cnt: 0 - valLoss: 0.664009153842926 - trainLoss: 0.6626946330070496\n",
      "cnt: 0 - valLoss: 0.664008378982544 - trainLoss: 0.662693977355957\n",
      "cnt: 0 - valLoss: 0.6640074849128723 - trainLoss: 0.6626932621002197\n",
      "cnt: 0 - valLoss: 0.6640067100524902 - trainLoss: 0.6626927256584167\n",
      "cnt: 0 - valLoss: 0.6640058159828186 - trainLoss: 0.6626920104026794\n",
      "cnt: 0 - valLoss: 0.6640050411224365 - trainLoss: 0.6626914143562317\n",
      "cnt: 0 - valLoss: 0.6640042066574097 - trainLoss: 0.6626906991004944\n",
      "cnt: 0 - valLoss: 0.6640033721923828 - trainLoss: 0.6626900434494019\n",
      "cnt: 0 - valLoss: 0.6640024781227112 - trainLoss: 0.6626893877983093\n",
      "cnt: 0 - valLoss: 0.6640017032623291 - trainLoss: 0.662688672542572\n",
      "cnt: 0 - valLoss: 0.6640008687973022 - trainLoss: 0.6626880168914795\n",
      "cnt: 0 - valLoss: 0.6640000343322754 - trainLoss: 0.6626874208450317\n",
      "cnt: 0 - valLoss: 0.6639992594718933 - trainLoss: 0.6626867055892944\n",
      "cnt: 0 - valLoss: 0.6639984250068665 - trainLoss: 0.6626860499382019\n",
      "cnt: 0 - valLoss: 0.6639975309371948 - trainLoss: 0.6626853346824646\n",
      "cnt: 0 - valLoss: 0.6639967560768127 - trainLoss: 0.6626847386360168\n",
      "cnt: 0 - valLoss: 0.6639959216117859 - trainLoss: 0.6626839637756348\n",
      "cnt: 0 - valLoss: 0.6639950275421143 - trainLoss: 0.662683367729187\n",
      "cnt: 0 - valLoss: 0.6639941930770874 - trainLoss: 0.6626827120780945\n",
      "cnt: 0 - valLoss: 0.6639933586120605 - trainLoss: 0.6626819968223572\n",
      "cnt: 0 - valLoss: 0.6639925241470337 - trainLoss: 0.6626814007759094\n",
      "cnt: 0 - valLoss: 0.6639916896820068 - trainLoss: 0.6626806259155273\n",
      "cnt: 0 - valLoss: 0.66399085521698 - trainLoss: 0.6626800298690796\n",
      "cnt: 0 - valLoss: 0.6639899611473083 - trainLoss: 0.6626793742179871\n",
      "cnt: 0 - valLoss: 0.6639891862869263 - trainLoss: 0.662678599357605\n",
      "cnt: 0 - valLoss: 0.6639884114265442 - trainLoss: 0.662678062915802\n",
      "cnt: 0 - valLoss: 0.6639875173568726 - trainLoss: 0.6626773476600647\n",
      "cnt: 0 - valLoss: 0.6639866828918457 - trainLoss: 0.6626766920089722\n",
      "cnt: 0 - valLoss: 0.6639858484268188 - trainLoss: 0.6626759767532349\n",
      "cnt: 0 - valLoss: 0.663985013961792 - trainLoss: 0.6626753211021423\n",
      "cnt: 0 - valLoss: 0.6639841198921204 - trainLoss: 0.6626746654510498\n",
      "cnt: 0 - valLoss: 0.6639833450317383 - trainLoss: 0.6626740097999573\n",
      "cnt: 0 - valLoss: 0.6639824509620667 - trainLoss: 0.6626732349395752\n",
      "cnt: 0 - valLoss: 0.6639816761016846 - trainLoss: 0.6626725792884827\n",
      "cnt: 0 - valLoss: 0.6639807820320129 - trainLoss: 0.6626719832420349\n",
      "cnt: 0 - valLoss: 0.6639799475669861 - trainLoss: 0.6626712083816528\n",
      "cnt: 0 - valLoss: 0.6639791131019592 - trainLoss: 0.6626706123352051\n",
      "cnt: 0 - valLoss: 0.6639782786369324 - trainLoss: 0.6626699566841125\n",
      "cnt: 0 - valLoss: 0.6639774441719055 - trainLoss: 0.6626692414283752\n",
      "cnt: 0 - valLoss: 0.6639766097068787 - trainLoss: 0.6626685261726379\n",
      "cnt: 0 - valLoss: 0.6639757752418518 - trainLoss: 0.6626678705215454\n",
      "cnt: 0 - valLoss: 0.6639748811721802 - trainLoss: 0.6626672744750977\n",
      "cnt: 0 - valLoss: 0.6639741063117981 - trainLoss: 0.6626665592193604\n",
      "cnt: 0 - valLoss: 0.6639732122421265 - trainLoss: 0.6626659035682678\n",
      "cnt: 0 - valLoss: 0.6639723777770996 - trainLoss: 0.6626652479171753\n",
      "cnt: 0 - valLoss: 0.6639715433120728 - trainLoss: 0.662664532661438\n",
      "cnt: 0 - valLoss: 0.6639707088470459 - trainLoss: 0.6626638174057007\n",
      "cnt: 0 - valLoss: 0.663969874382019 - trainLoss: 0.6626632213592529\n",
      "cnt: 0 - valLoss: 0.6639690399169922 - trainLoss: 0.6626625657081604\n",
      "cnt: 0 - valLoss: 0.6639682054519653 - trainLoss: 0.6626617908477783\n",
      "cnt: 0 - valLoss: 0.6639673113822937 - trainLoss: 0.6626611948013306\n",
      "cnt: 0 - valLoss: 0.6639664769172668 - trainLoss: 0.6626604795455933\n",
      "cnt: 0 - valLoss: 0.66396564245224 - trainLoss: 0.6626598238945007\n",
      "cnt: 0 - valLoss: 0.6639647483825684 - trainLoss: 0.6626591086387634\n",
      "cnt: 0 - valLoss: 0.6639639139175415 - trainLoss: 0.6626584529876709\n",
      "cnt: 0 - valLoss: 0.6639630794525146 - trainLoss: 0.6626577973365784\n",
      "cnt: 0 - valLoss: 0.663962185382843 - trainLoss: 0.6626570820808411\n",
      "cnt: 0 - valLoss: 0.6639614105224609 - trainLoss: 0.6626563668251038\n",
      "cnt: 0 - valLoss: 0.6639605760574341 - trainLoss: 0.6626557111740112\n",
      "cnt: 0 - valLoss: 0.6639597415924072 - trainLoss: 0.6626551151275635\n",
      "cnt: 0 - valLoss: 0.6639588475227356 - trainLoss: 0.6626543402671814\n",
      "cnt: 0 - valLoss: 0.6639580130577087 - trainLoss: 0.6626536846160889\n",
      "cnt: 0 - valLoss: 0.6639571189880371 - trainLoss: 0.6626530289649963\n",
      "cnt: 0 - valLoss: 0.663956344127655 - trainLoss: 0.662652313709259\n",
      "cnt: 0 - valLoss: 0.6639554500579834 - trainLoss: 0.6626515984535217\n",
      "cnt: 0 - valLoss: 0.6639545559883118 - trainLoss: 0.662651002407074\n",
      "cnt: 0 - valLoss: 0.6639537811279297 - trainLoss: 0.6626502871513367\n",
      "cnt: 0 - valLoss: 0.6639528870582581 - trainLoss: 0.6626496315002441\n",
      "cnt: 0 - valLoss: 0.6639519929885864 - trainLoss: 0.6626489162445068\n",
      "cnt: 0 - valLoss: 0.6639511585235596 - trainLoss: 0.6626483201980591\n",
      "cnt: 0 - valLoss: 0.6639503240585327 - trainLoss: 0.662647545337677\n",
      "cnt: 0 - valLoss: 0.6639494895935059 - trainLoss: 0.6626468300819397\n",
      "cnt: 0 - valLoss: 0.663948655128479 - trainLoss: 0.6626461744308472\n",
      "cnt: 0 - valLoss: 0.6639478206634521 - trainLoss: 0.6626455187797546\n",
      "cnt: 0 - valLoss: 0.6639469265937805 - trainLoss: 0.6626448035240173\n",
      "cnt: 0 - valLoss: 0.6639460921287537 - trainLoss: 0.6626441478729248\n",
      "cnt: 0 - valLoss: 0.6639452576637268 - trainLoss: 0.6626434326171875\n",
      "cnt: 0 - valLoss: 0.6639443635940552 - trainLoss: 0.662642776966095\n",
      "cnt: 0 - valLoss: 0.6639435291290283 - trainLoss: 0.6626420617103577\n",
      "cnt: 0 - valLoss: 0.6639426350593567 - trainLoss: 0.6626414060592651\n",
      "cnt: 0 - valLoss: 0.6639418005943298 - trainLoss: 0.6626406908035278\n",
      "cnt: 0 - valLoss: 0.663940966129303 - trainLoss: 0.6626400351524353\n",
      "cnt: 0 - valLoss: 0.6639400720596313 - trainLoss: 0.662639319896698\n",
      "cnt: 0 - valLoss: 0.6639392375946045 - trainLoss: 0.6626386642456055\n",
      "cnt: 0 - valLoss: 0.6639384031295776 - trainLoss: 0.6626379489898682\n",
      "cnt: 0 - valLoss: 0.663937509059906 - trainLoss: 0.6626372933387756\n",
      "cnt: 0 - valLoss: 0.6639366745948792 - trainLoss: 0.6626365780830383\n",
      "cnt: 0 - valLoss: 0.6639357805252075 - trainLoss: 0.6626359224319458\n",
      "cnt: 0 - valLoss: 0.6639348864555359 - trainLoss: 0.6626352071762085\n",
      "cnt: 0 - valLoss: 0.663934051990509 - trainLoss: 0.662634551525116\n",
      "cnt: 0 - valLoss: 0.6639331579208374 - trainLoss: 0.6626338958740234\n",
      "cnt: 0 - valLoss: 0.6639323234558105 - trainLoss: 0.6626331806182861\n",
      "cnt: 0 - valLoss: 0.6639314293861389 - trainLoss: 0.662632405757904\n",
      "cnt: 0 - valLoss: 0.6639305949211121 - trainLoss: 0.6626317501068115\n",
      "cnt: 0 - valLoss: 0.6639297008514404 - trainLoss: 0.6626310348510742\n",
      "cnt: 0 - valLoss: 0.6639288663864136 - trainLoss: 0.6626303791999817\n",
      "cnt: 0 - valLoss: 0.6639279723167419 - trainLoss: 0.6626296639442444\n",
      "cnt: 0 - valLoss: 0.6639270782470703 - trainLoss: 0.6626290082931519\n",
      "cnt: 0 - valLoss: 0.6639261841773987 - trainLoss: 0.6626282334327698\n",
      "cnt: 0 - valLoss: 0.6639252305030823 - trainLoss: 0.662627637386322\n",
      "cnt: 0 - valLoss: 0.6639243960380554 - trainLoss: 0.6626268625259399\n",
      "cnt: 0 - valLoss: 0.6639235615730286 - trainLoss: 0.6626262664794922\n",
      "cnt: 0 - valLoss: 0.6639226675033569 - trainLoss: 0.6626254916191101\n",
      "cnt: 0 - valLoss: 0.6639217734336853 - trainLoss: 0.6626247763633728\n",
      "cnt: 0 - valLoss: 0.6639209389686584 - trainLoss: 0.6626241207122803\n",
      "cnt: 0 - valLoss: 0.6639200448989868 - trainLoss: 0.662623405456543\n",
      "cnt: 0 - valLoss: 0.6639191508293152 - trainLoss: 0.6626227498054504\n",
      "cnt: 0 - valLoss: 0.6639183163642883 - trainLoss: 0.6626220345497131\n",
      "cnt: 0 - valLoss: 0.6639174818992615 - trainLoss: 0.662621259689331\n",
      "cnt: 0 - valLoss: 0.6639165282249451 - trainLoss: 0.6626206040382385\n",
      "cnt: 0 - valLoss: 0.6639156937599182 - trainLoss: 0.6626200079917908\n",
      "cnt: 0 - valLoss: 0.6639147996902466 - trainLoss: 0.6626192331314087\n",
      "cnt: 0 - valLoss: 0.6639139652252197 - trainLoss: 0.6626185178756714\n",
      "cnt: 0 - valLoss: 0.6639130115509033 - trainLoss: 0.6626178622245789\n",
      "cnt: 0 - valLoss: 0.6639121770858765 - trainLoss: 0.6626171469688416\n",
      "cnt: 0 - valLoss: 0.6639112830162048 - trainLoss: 0.6626164317131042\n",
      "cnt: 0 - valLoss: 0.6639103889465332 - trainLoss: 0.6626157164573669\n",
      "cnt: 0 - valLoss: 0.6639094948768616 - trainLoss: 0.6626150608062744\n",
      "cnt: 0 - valLoss: 0.6639086008071899 - trainLoss: 0.6626143455505371\n",
      "cnt: 0 - valLoss: 0.6639077663421631 - trainLoss: 0.6626136898994446\n",
      "cnt: 0 - valLoss: 0.6639068722724915 - trainLoss: 0.6626129150390625\n",
      "cnt: 0 - valLoss: 0.6639059782028198 - trainLoss: 0.6626121997833252\n",
      "cnt: 0 - valLoss: 0.6639050841331482 - trainLoss: 0.6626115441322327\n",
      "cnt: 0 - valLoss: 0.6639041900634766 - trainLoss: 0.6626108288764954\n",
      "cnt: 0 - valLoss: 0.6639033555984497 - trainLoss: 0.6626101136207581\n",
      "cnt: 0 - valLoss: 0.6639024615287781 - trainLoss: 0.6626093983650208\n",
      "cnt: 0 - valLoss: 0.6639015674591064 - trainLoss: 0.6626086831092834\n",
      "cnt: 0 - valLoss: 0.6639006733894348 - trainLoss: 0.6626080274581909\n",
      "cnt: 0 - valLoss: 0.6638997793197632 - trainLoss: 0.6626073122024536\n",
      "cnt: 0 - valLoss: 0.6638989448547363 - trainLoss: 0.6626065969467163\n",
      "cnt: 0 - valLoss: 0.6638979911804199 - trainLoss: 0.6626059412956238\n",
      "cnt: 0 - valLoss: 0.6638971567153931 - trainLoss: 0.6626051664352417\n",
      "cnt: 0 - valLoss: 0.6638962626457214 - trainLoss: 0.6626044511795044\n",
      "cnt: 0 - valLoss: 0.6638953685760498 - trainLoss: 0.6626037955284119\n",
      "cnt: 0 - valLoss: 0.6638944745063782 - trainLoss: 0.6626030206680298\n",
      "cnt: 0 - valLoss: 0.6638935804367065 - trainLoss: 0.6626023650169373\n",
      "cnt: 0 - valLoss: 0.6638927459716797 - trainLoss: 0.6626017093658447\n",
      "cnt: 0 - valLoss: 0.6638917922973633 - trainLoss: 0.6626009345054626\n",
      "cnt: 0 - valLoss: 0.6638909578323364 - trainLoss: 0.6626002788543701\n",
      "cnt: 0 - valLoss: 0.6638900637626648 - trainLoss: 0.6625995635986328\n",
      "cnt: 0 - valLoss: 0.6638891696929932 - trainLoss: 0.6625988483428955\n",
      "cnt: 0 - valLoss: 0.6638882756233215 - trainLoss: 0.6625980734825134\n",
      "cnt: 0 - valLoss: 0.6638873815536499 - trainLoss: 0.6625974178314209\n",
      "cnt: 0 - valLoss: 0.6638864874839783 - trainLoss: 0.6625967025756836\n",
      "cnt: 0 - valLoss: 0.6638855934143066 - trainLoss: 0.6625960469245911\n",
      "cnt: 0 - valLoss: 0.6638846397399902 - trainLoss: 0.6625953316688538\n",
      "cnt: 0 - valLoss: 0.6638838052749634 - trainLoss: 0.6625946164131165\n",
      "cnt: 0 - valLoss: 0.6638829112052917 - trainLoss: 0.6625939011573792\n",
      "cnt: 0 - valLoss: 0.6638820767402649 - trainLoss: 0.6625931262969971\n",
      "cnt: 0 - valLoss: 0.6638811230659485 - trainLoss: 0.6625924110412598\n",
      "cnt: 0 - valLoss: 0.6638802289962769 - trainLoss: 0.6625917553901672\n",
      "cnt: 0 - valLoss: 0.6638793349266052 - trainLoss: 0.6625909805297852\n",
      "cnt: 0 - valLoss: 0.6638784408569336 - trainLoss: 0.6625903248786926\n",
      "cnt: 0 - valLoss: 0.663877546787262 - trainLoss: 0.6625895500183105\n",
      "cnt: 0 - valLoss: 0.6638766527175903 - trainLoss: 0.6625888347625732\n",
      "cnt: 0 - valLoss: 0.6638758182525635 - trainLoss: 0.6625881791114807\n",
      "cnt: 0 - valLoss: 0.6638748645782471 - trainLoss: 0.6625874638557434\n",
      "cnt: 0 - valLoss: 0.6638739705085754 - trainLoss: 0.6625866889953613\n",
      "cnt: 0 - valLoss: 0.6638730764389038 - trainLoss: 0.662585973739624\n",
      "cnt: 0 - valLoss: 0.6638721823692322 - trainLoss: 0.6625852584838867\n",
      "cnt: 0 - valLoss: 0.6638712286949158 - trainLoss: 0.6625845432281494\n",
      "cnt: 0 - valLoss: 0.6638703346252441 - trainLoss: 0.6625838875770569\n",
      "cnt: 0 - valLoss: 0.6638694405555725 - trainLoss: 0.6625831723213196\n",
      "cnt: 0 - valLoss: 0.6638686060905457 - trainLoss: 0.6625823974609375\n",
      "cnt: 0 - valLoss: 0.663867712020874 - trainLoss: 0.6625816226005554\n",
      "cnt: 0 - valLoss: 0.6638668179512024 - trainLoss: 0.6625810265541077\n",
      "cnt: 0 - valLoss: 0.663865864276886 - trainLoss: 0.6625802516937256\n",
      "cnt: 0 - valLoss: 0.6638649702072144 - trainLoss: 0.6625795960426331\n",
      "cnt: 0 - valLoss: 0.6638640761375427 - trainLoss: 0.662578821182251\n",
      "cnt: 0 - valLoss: 0.6638631820678711 - trainLoss: 0.6625781655311584\n",
      "cnt: 0 - valLoss: 0.6638622879981995 - trainLoss: 0.6625773906707764\n",
      "cnt: 0 - valLoss: 0.6638613343238831 - trainLoss: 0.6625767350196838\n",
      "cnt: 0 - valLoss: 0.6638604402542114 - trainLoss: 0.6625760197639465\n",
      "cnt: 0 - valLoss: 0.6638595461845398 - trainLoss: 0.6625752449035645\n",
      "cnt: 0 - valLoss: 0.6638587117195129 - trainLoss: 0.6625745296478271\n",
      "cnt: 0 - valLoss: 0.6638578176498413 - trainLoss: 0.6625738143920898\n",
      "cnt: 0 - valLoss: 0.6638568639755249 - trainLoss: 0.6625730991363525\n",
      "cnt: 0 - valLoss: 0.6638559699058533 - trainLoss: 0.6625723838806152\n",
      "cnt: 0 - valLoss: 0.6638550758361816 - trainLoss: 0.6625716090202332\n",
      "cnt: 0 - valLoss: 0.6638541221618652 - trainLoss: 0.6625709533691406\n",
      "cnt: 0 - valLoss: 0.6638532280921936 - trainLoss: 0.6625701785087585\n",
      "cnt: 0 - valLoss: 0.663852334022522 - trainLoss: 0.6625694632530212\n",
      "cnt: 0 - valLoss: 0.6638514399528503 - trainLoss: 0.6625688076019287\n",
      "cnt: 0 - valLoss: 0.6638504862785339 - trainLoss: 0.6625680327415466\n",
      "cnt: 0 - valLoss: 0.6638495922088623 - trainLoss: 0.6625673770904541\n",
      "cnt: 0 - valLoss: 0.6638487577438354 - trainLoss: 0.662566602230072\n",
      "cnt: 0 - valLoss: 0.6638478636741638 - trainLoss: 0.6625658869743347\n",
      "cnt: 0 - valLoss: 0.6638469099998474 - trainLoss: 0.6625651717185974\n",
      "cnt: 0 - valLoss: 0.6638460159301758 - trainLoss: 0.6625644564628601\n",
      "cnt: 0 - valLoss: 0.6638451218605042 - trainLoss: 0.662563681602478\n",
      "cnt: 0 - valLoss: 0.6638441681861877 - trainLoss: 0.6625629663467407\n",
      "cnt: 0 - valLoss: 0.6638433337211609 - trainLoss: 0.6625622510910034\n",
      "cnt: 0 - valLoss: 0.6638423800468445 - trainLoss: 0.6625615358352661\n",
      "cnt: 0 - valLoss: 0.6638414859771729 - trainLoss: 0.6625608205795288\n",
      "cnt: 0 - valLoss: 0.6638405919075012 - trainLoss: 0.6625601053237915\n",
      "cnt: 0 - valLoss: 0.6638396382331848 - trainLoss: 0.6625593304634094\n",
      "cnt: 0 - valLoss: 0.6638388633728027 - trainLoss: 0.6625587344169617\n",
      "cnt: 0 - valLoss: 0.6638379693031311 - trainLoss: 0.6625580191612244\n",
      "cnt: 0 - valLoss: 0.663837194442749 - trainLoss: 0.6625574827194214\n",
      "cnt: 0 - valLoss: 0.6638363599777222 - trainLoss: 0.6625568866729736\n",
      "cnt: 0 - valLoss: 0.6638355851173401 - trainLoss: 0.6625562906265259\n",
      "cnt: 0 - valLoss: 0.6638347506523132 - trainLoss: 0.6625556349754333\n",
      "cnt: 0 - valLoss: 0.6638339757919312 - trainLoss: 0.6625550389289856\n",
      "cnt: 0 - valLoss: 0.6638330817222595 - trainLoss: 0.6625545024871826\n",
      "cnt: 0 - valLoss: 0.6638323068618774 - trainLoss: 0.6625539064407349\n",
      "cnt: 0 - valLoss: 0.6638314723968506 - trainLoss: 0.6625533103942871\n",
      "cnt: 0 - valLoss: 0.6638306975364685 - trainLoss: 0.6625527739524841\n",
      "cnt: 0 - valLoss: 0.6638298630714417 - trainLoss: 0.6625521183013916\n",
      "cnt: 0 - valLoss: 0.66382896900177 - trainLoss: 0.6625515222549438\n",
      "cnt: 0 - valLoss: 0.6638281941413879 - trainLoss: 0.6625508666038513\n",
      "cnt: 0 - valLoss: 0.6638273596763611 - trainLoss: 0.6625503301620483\n",
      "cnt: 0 - valLoss: 0.663826584815979 - trainLoss: 0.6625497341156006\n",
      "cnt: 0 - valLoss: 0.6638257503509521 - trainLoss: 0.6625491380691528\n",
      "cnt: 0 - valLoss: 0.6638249754905701 - trainLoss: 0.6625486016273499\n",
      "cnt: 0 - valLoss: 0.6638240814208984 - trainLoss: 0.6625479459762573\n",
      "cnt: 0 - valLoss: 0.6638232469558716 - trainLoss: 0.6625473499298096\n",
      "cnt: 0 - valLoss: 0.6638224720954895 - trainLoss: 0.6625467538833618\n",
      "cnt: 0 - valLoss: 0.6638216376304626 - trainLoss: 0.6625460982322693\n",
      "cnt: 0 - valLoss: 0.6638208031654358 - trainLoss: 0.6625455021858215\n",
      "cnt: 0 - valLoss: 0.6638199687004089 - trainLoss: 0.6625450253486633\n",
      "cnt: 0 - valLoss: 0.6638191938400269 - trainLoss: 0.6625443696975708\n",
      "cnt: 0 - valLoss: 0.663818359375 - trainLoss: 0.662543773651123\n",
      "cnt: 0 - valLoss: 0.6638175249099731 - trainLoss: 0.6625431776046753\n",
      "cnt: 0 - valLoss: 0.6638167500495911 - trainLoss: 0.6625425815582275\n",
      "cnt: 0 - valLoss: 0.6638158559799194 - trainLoss: 0.662541925907135\n",
      "cnt: 0 - valLoss: 0.6638150811195374 - trainLoss: 0.662541389465332\n",
      "cnt: 0 - valLoss: 0.6638143062591553 - trainLoss: 0.6625407934188843\n",
      "cnt: 0 - valLoss: 0.6638134121894836 - trainLoss: 0.6625401973724365\n",
      "cnt: 0 - valLoss: 0.6638126373291016 - trainLoss: 0.6625396013259888\n",
      "cnt: 0 - valLoss: 0.6638118028640747 - trainLoss: 0.6625389456748962\n",
      "cnt: 0 - valLoss: 0.6638109683990479 - trainLoss: 0.6625384092330933\n",
      "cnt: 0 - valLoss: 0.663810133934021 - trainLoss: 0.6625377535820007\n",
      "cnt: 0 - valLoss: 0.6638092994689941 - trainLoss: 0.662537157535553\n",
      "cnt: 0 - valLoss: 0.6638084053993225 - trainLoss: 0.6625365614891052\n",
      "cnt: 0 - valLoss: 0.6638076305389404 - trainLoss: 0.6625359654426575\n",
      "cnt: 0 - valLoss: 0.6638067960739136 - trainLoss: 0.6625353693962097\n",
      "cnt: 0 - valLoss: 0.6638060212135315 - trainLoss: 0.6625347137451172\n",
      "cnt: 0 - valLoss: 0.6638051867485046 - trainLoss: 0.6625341176986694\n",
      "cnt: 0 - valLoss: 0.6638043522834778 - trainLoss: 0.6625336408615112\n",
      "cnt: 0 - valLoss: 0.6638034582138062 - trainLoss: 0.6625329256057739\n",
      "cnt: 0 - valLoss: 0.6638027429580688 - trainLoss: 0.662532389163971\n",
      "cnt: 0 - valLoss: 0.6638018488883972 - trainLoss: 0.6625317931175232\n",
      "cnt: 0 - valLoss: 0.6638010144233704 - trainLoss: 0.6625311374664307\n",
      "cnt: 0 - valLoss: 0.6638001799583435 - trainLoss: 0.6625306010246277\n",
      "cnt: 0 - valLoss: 0.6637993454933167 - trainLoss: 0.6625299453735352\n",
      "cnt: 0 - valLoss: 0.663798451423645 - trainLoss: 0.6625294089317322\n",
      "cnt: 0 - valLoss: 0.6637976765632629 - trainLoss: 0.6625287532806396\n",
      "cnt: 0 - valLoss: 0.6637969017028809 - trainLoss: 0.6625280976295471\n",
      "cnt: 0 - valLoss: 0.6637960076332092 - trainLoss: 0.6625275611877441\n",
      "cnt: 0 - valLoss: 0.6637952327728271 - trainLoss: 0.6625269055366516\n",
      "cnt: 0 - valLoss: 0.6637943387031555 - trainLoss: 0.6625263690948486\n",
      "cnt: 0 - valLoss: 0.6637935042381287 - trainLoss: 0.6625257134437561\n",
      "cnt: 0 - valLoss: 0.6637927889823914 - trainLoss: 0.6625251770019531\n",
      "cnt: 0 - valLoss: 0.6637919545173645 - trainLoss: 0.6625245809555054\n",
      "cnt: 0 - valLoss: 0.6637911796569824 - trainLoss: 0.6625240445137024\n",
      "cnt: 0 - valLoss: 0.6637904047966003 - trainLoss: 0.6625233292579651\n",
      "cnt: 0 - valLoss: 0.6637895107269287 - trainLoss: 0.6625228524208069\n",
      "cnt: 0 - valLoss: 0.6637887358665466 - trainLoss: 0.6625221967697144\n",
      "cnt: 0 - valLoss: 0.6637879014015198 - trainLoss: 0.6625216603279114\n",
      "cnt: 0 - valLoss: 0.6637871265411377 - trainLoss: 0.6625210642814636\n",
      "cnt: 0 - valLoss: 0.6637862920761108 - trainLoss: 0.6625204682350159\n",
      "cnt: 0 - valLoss: 0.663785457611084 - trainLoss: 0.6625199317932129\n",
      "cnt: 0 - valLoss: 0.6637846231460571 - trainLoss: 0.6625193357467651\n",
      "cnt: 0 - valLoss: 0.6637837886810303 - trainLoss: 0.6625187397003174\n",
      "cnt: 0 - valLoss: 0.6637830138206482 - trainLoss: 0.6625180840492249\n",
      "cnt: 0 - valLoss: 0.6637822985649109 - trainLoss: 0.6625175476074219\n",
      "cnt: 0 - valLoss: 0.6637813448905945 - trainLoss: 0.6625169515609741\n",
      "cnt: 0 - valLoss: 0.6637806296348572 - trainLoss: 0.6625164151191711\n",
      "cnt: 0 - valLoss: 0.6637797355651855 - trainLoss: 0.6625157594680786\n",
      "cnt: 0 - valLoss: 0.6637790203094482 - trainLoss: 0.6625152230262756\n",
      "cnt: 0 - valLoss: 0.6637781262397766 - trainLoss: 0.6625146269798279\n",
      "cnt: 0 - valLoss: 0.6637774109840393 - trainLoss: 0.6625140309333801\n",
      "cnt: 0 - valLoss: 0.6637765169143677 - trainLoss: 0.6625134348869324\n",
      "cnt: 0 - valLoss: 0.6637757420539856 - trainLoss: 0.6625128388404846\n",
      "cnt: 0 - valLoss: 0.663774847984314 - trainLoss: 0.6625122427940369\n",
      "cnt: 0 - valLoss: 0.6637740731239319 - trainLoss: 0.6625117063522339\n",
      "cnt: 0 - valLoss: 0.6637733578681946 - trainLoss: 0.6625110507011414\n",
      "cnt: 0 - valLoss: 0.663772463798523 - trainLoss: 0.6625104546546936\n",
      "cnt: 0 - valLoss: 0.6637716889381409 - trainLoss: 0.6625099182128906\n",
      "cnt: 0 - valLoss: 0.663770854473114 - trainLoss: 0.6625093221664429\n",
      "cnt: 0 - valLoss: 0.6637700200080872 - trainLoss: 0.6625087261199951\n",
      "cnt: 0 - valLoss: 0.6637691855430603 - trainLoss: 0.6625081300735474\n",
      "cnt: 0 - valLoss: 0.6637684106826782 - trainLoss: 0.6625075340270996\n",
      "cnt: 0 - valLoss: 0.6637675762176514 - trainLoss: 0.6625069379806519\n",
      "cnt: 0 - valLoss: 0.6637667417526245 - trainLoss: 0.6625063419342041\n",
      "cnt: 0 - valLoss: 0.6637659072875977 - trainLoss: 0.6625057458877563\n",
      "cnt: 0 - valLoss: 0.6637650728225708 - trainLoss: 0.6625050902366638\n",
      "cnt: 0 - valLoss: 0.663764238357544 - trainLoss: 0.6625045537948608\n",
      "cnt: 0 - valLoss: 0.6637634634971619 - trainLoss: 0.6625039577484131\n",
      "cnt: 0 - valLoss: 0.663762629032135 - trainLoss: 0.6625033617019653\n",
      "cnt: 0 - valLoss: 0.6637618541717529 - trainLoss: 0.6625028252601624\n",
      "cnt: 0 - valLoss: 0.6637610197067261 - trainLoss: 0.6625021696090698\n",
      "cnt: 0 - valLoss: 0.6637601852416992 - trainLoss: 0.6625016331672668\n",
      "cnt: 0 - valLoss: 0.6637592911720276 - trainLoss: 0.6625010371208191\n",
      "cnt: 0 - valLoss: 0.6637585163116455 - trainLoss: 0.6625003814697266\n",
      "cnt: 0 - valLoss: 0.6637576818466187 - trainLoss: 0.6624997854232788\n",
      "cnt: 0 - valLoss: 0.6637569069862366 - trainLoss: 0.662499189376831\n",
      "cnt: 0 - valLoss: 0.6637560129165649 - trainLoss: 0.6624985933303833\n",
      "cnt: 0 - valLoss: 0.6637552380561829 - trainLoss: 0.6624979972839355\n",
      "cnt: 0 - valLoss: 0.6637544631958008 - trainLoss: 0.6624974012374878\n",
      "cnt: 0 - valLoss: 0.6637535691261292 - trainLoss: 0.6624968647956848\n",
      "cnt: 0 - valLoss: 0.6637527942657471 - trainLoss: 0.6624962091445923\n",
      "cnt: 0 - valLoss: 0.6637519001960754 - trainLoss: 0.6624956727027893\n",
      "cnt: 0 - valLoss: 0.6637511253356934 - trainLoss: 0.6624950170516968\n",
      "cnt: 0 - valLoss: 0.6637502908706665 - trainLoss: 0.6624944806098938\n",
      "cnt: 0 - valLoss: 0.6637495160102844 - trainLoss: 0.6624938249588013\n",
      "cnt: 0 - valLoss: 0.6637486815452576 - trainLoss: 0.6624932289123535\n",
      "cnt: 0 - valLoss: 0.6637477874755859 - trainLoss: 0.662492573261261\n",
      "cnt: 0 - valLoss: 0.6637470126152039 - trainLoss: 0.662492036819458\n",
      "cnt: 0 - valLoss: 0.663746178150177 - trainLoss: 0.6624913811683655\n",
      "cnt: 0 - valLoss: 0.6637453436851501 - trainLoss: 0.6624908447265625\n",
      "cnt: 0 - valLoss: 0.6637445092201233 - trainLoss: 0.66249018907547\n",
      "cnt: 0 - valLoss: 0.6637436747550964 - trainLoss: 0.662489652633667\n",
      "cnt: 0 - valLoss: 0.6637428998947144 - trainLoss: 0.6624889373779297\n",
      "cnt: 0 - valLoss: 0.6637420058250427 - trainLoss: 0.6624884009361267\n",
      "cnt: 0 - valLoss: 0.6637412309646606 - trainLoss: 0.6624876856803894\n",
      "cnt: 0 - valLoss: 0.663740336894989 - trainLoss: 0.6624871492385864\n",
      "cnt: 0 - valLoss: 0.6637395620346069 - trainLoss: 0.6624864339828491\n",
      "cnt: 0 - valLoss: 0.6637386083602905 - trainLoss: 0.6624858975410461\n",
      "cnt: 0 - valLoss: 0.6637377738952637 - trainLoss: 0.6624852418899536\n",
      "cnt: 0 - valLoss: 0.6637369394302368 - trainLoss: 0.6624846458435059\n",
      "cnt: 0 - valLoss: 0.66373610496521 - trainLoss: 0.6624839901924133\n",
      "cnt: 0 - valLoss: 0.6637353301048279 - trainLoss: 0.6624833941459656\n",
      "cnt: 0 - valLoss: 0.6637344360351562 - trainLoss: 0.6624827980995178\n",
      "cnt: 0 - valLoss: 0.6637336015701294 - trainLoss: 0.6624822020530701\n",
      "cnt: 0 - valLoss: 0.6637328267097473 - trainLoss: 0.6624816060066223\n",
      "cnt: 0 - valLoss: 0.6637319326400757 - trainLoss: 0.6624809503555298\n",
      "cnt: 0 - valLoss: 0.6637310981750488 - trainLoss: 0.6624802947044373\n",
      "cnt: 0 - valLoss: 0.663730263710022 - trainLoss: 0.6624796986579895\n",
      "cnt: 0 - valLoss: 0.6637294292449951 - trainLoss: 0.662479043006897\n",
      "cnt: 0 - valLoss: 0.6637285947799683 - trainLoss: 0.662478506565094\n",
      "cnt: 0 - valLoss: 0.6637277603149414 - trainLoss: 0.6624777913093567\n",
      "cnt: 0 - valLoss: 0.6637268662452698 - trainLoss: 0.6624771952629089\n",
      "cnt: 0 - valLoss: 0.6637260317802429 - trainLoss: 0.6624765992164612\n",
      "cnt: 0 - valLoss: 0.6637252569198608 - trainLoss: 0.6624759435653687\n",
      "cnt: 0 - valLoss: 0.6637243032455444 - trainLoss: 0.6624754071235657\n",
      "cnt: 0 - valLoss: 0.6637235283851624 - trainLoss: 0.6624747514724731\n",
      "cnt: 0 - valLoss: 0.6637226343154907 - trainLoss: 0.6624740958213806\n",
      "cnt: 0 - valLoss: 0.6637218594551086 - trainLoss: 0.6624734997749329\n",
      "cnt: 0 - valLoss: 0.6637209057807922 - trainLoss: 0.6624728441238403\n",
      "cnt: 0 - valLoss: 0.6637201905250549 - trainLoss: 0.6624722480773926\n",
      "cnt: 0 - valLoss: 0.6637192964553833 - trainLoss: 0.6624716520309448\n",
      "cnt: 0 - valLoss: 0.6637184619903564 - trainLoss: 0.6624710559844971\n",
      "cnt: 0 - valLoss: 0.6637175679206848 - trainLoss: 0.6624704003334045\n",
      "cnt: 0 - valLoss: 0.6637167930603027 - trainLoss: 0.662469744682312\n",
      "cnt: 0 - valLoss: 0.6637158989906311 - trainLoss: 0.6624691486358643\n",
      "cnt: 0 - valLoss: 0.6637150049209595 - trainLoss: 0.6624684929847717\n",
      "cnt: 0 - valLoss: 0.6637141704559326 - trainLoss: 0.662467896938324\n",
      "cnt: 0 - valLoss: 0.663713276386261 - trainLoss: 0.6624672412872314\n",
      "cnt: 0 - valLoss: 0.6637124419212341 - trainLoss: 0.6624666452407837\n",
      "cnt: 0 - valLoss: 0.6637116074562073 - trainLoss: 0.6624659299850464\n",
      "cnt: 0 - valLoss: 0.6637107133865356 - trainLoss: 0.6624653339385986\n",
      "cnt: 0 - valLoss: 0.663709819316864 - trainLoss: 0.6624647378921509\n",
      "cnt: 0 - valLoss: 0.6637090444564819 - trainLoss: 0.6624642014503479\n",
      "cnt: 0 - valLoss: 0.6637082099914551 - trainLoss: 0.6624634861946106\n",
      "cnt: 0 - valLoss: 0.6637073755264282 - trainLoss: 0.6624628901481628\n",
      "cnt: 0 - valLoss: 0.6637064814567566 - trainLoss: 0.6624622941017151\n",
      "cnt: 0 - valLoss: 0.6637056469917297 - trainLoss: 0.6624616980552673\n",
      "cnt: 0 - valLoss: 0.6637047529220581 - trainLoss: 0.6624610424041748\n",
      "cnt: 0 - valLoss: 0.6637039184570312 - trainLoss: 0.6624605059623718\n",
      "cnt: 0 - valLoss: 0.6637031435966492 - trainLoss: 0.6624598503112793\n",
      "cnt: 0 - valLoss: 0.6637021899223328 - trainLoss: 0.6624591946601868\n",
      "cnt: 0 - valLoss: 0.6637014150619507 - trainLoss: 0.6624585390090942\n",
      "cnt: 0 - valLoss: 0.663700520992279 - trainLoss: 0.6624579429626465\n",
      "cnt: 0 - valLoss: 0.6636996269226074 - trainLoss: 0.6624573469161987\n",
      "cnt: 0 - valLoss: 0.6636988520622253 - trainLoss: 0.6624566912651062\n",
      "cnt: 0 - valLoss: 0.6636979579925537 - trainLoss: 0.6624560952186584\n",
      "cnt: 0 - valLoss: 0.6636970639228821 - trainLoss: 0.6624554395675659\n",
      "cnt: 0 - valLoss: 0.6636962294578552 - trainLoss: 0.6624548435211182\n",
      "cnt: 0 - valLoss: 0.6636953353881836 - trainLoss: 0.6624541878700256\n",
      "cnt: 0 - valLoss: 0.6636945009231567 - trainLoss: 0.6624535918235779\n",
      "cnt: 0 - valLoss: 0.6636936068534851 - trainLoss: 0.6624529957771301\n",
      "cnt: 0 - valLoss: 0.663692831993103 - trainLoss: 0.6624523401260376\n",
      "cnt: 0 - valLoss: 0.6636919379234314 - trainLoss: 0.6624517440795898\n",
      "cnt: 0 - valLoss: 0.6636911034584045 - trainLoss: 0.6624510884284973\n",
      "cnt: 0 - valLoss: 0.6636902689933777 - trainLoss: 0.6624504327774048\n",
      "cnt: 0 - valLoss: 0.663689374923706 - trainLoss: 0.6624498963356018\n",
      "cnt: 0 - valLoss: 0.6636885404586792 - trainLoss: 0.6624492406845093\n",
      "cnt: 0 - valLoss: 0.6636876463890076 - trainLoss: 0.6624486446380615\n",
      "cnt: 0 - valLoss: 0.6636867523193359 - trainLoss: 0.6624479293823242\n",
      "cnt: 0 - valLoss: 0.6636859178543091 - trainLoss: 0.6624473333358765\n",
      "cnt: 0 - valLoss: 0.6636850833892822 - trainLoss: 0.6624466776847839\n",
      "cnt: 0 - valLoss: 0.6636841297149658 - trainLoss: 0.6624460816383362\n",
      "cnt: 0 - valLoss: 0.6636832356452942 - trainLoss: 0.6624454259872437\n",
      "cnt: 0 - valLoss: 0.6636824011802673 - trainLoss: 0.6624447703361511\n",
      "cnt: 0 - valLoss: 0.6636815667152405 - trainLoss: 0.6624441146850586\n",
      "cnt: 0 - valLoss: 0.6636806726455688 - trainLoss: 0.6624434590339661\n",
      "cnt: 0 - valLoss: 0.6636797189712524 - trainLoss: 0.6624428629875183\n",
      "cnt: 0 - valLoss: 0.6636789441108704 - trainLoss: 0.662442147731781\n",
      "cnt: 0 - valLoss: 0.6636780500411987 - trainLoss: 0.6624414920806885\n",
      "cnt: 0 - valLoss: 0.6636771559715271 - trainLoss: 0.6624408960342407\n",
      "cnt: 0 - valLoss: 0.6636763215065002 - trainLoss: 0.6624402403831482\n",
      "cnt: 0 - valLoss: 0.6636754870414734 - trainLoss: 0.6624395847320557\n",
      "cnt: 0 - valLoss: 0.663674533367157 - trainLoss: 0.6624389886856079\n",
      "cnt: 0 - valLoss: 0.6636736392974854 - trainLoss: 0.6624383330345154\n",
      "cnt: 0 - valLoss: 0.6636728048324585 - trainLoss: 0.6624376773834229\n",
      "cnt: 0 - valLoss: 0.6636719703674316 - trainLoss: 0.6624370217323303\n",
      "cnt: 0 - valLoss: 0.6636710166931152 - trainLoss: 0.6624364256858826\n",
      "cnt: 0 - valLoss: 0.6636701822280884 - trainLoss: 0.6624357104301453\n",
      "cnt: 0 - valLoss: 0.663669228553772 - trainLoss: 0.6624351143836975\n",
      "cnt: 0 - valLoss: 0.6636684536933899 - trainLoss: 0.6624345183372498\n",
      "cnt: 0 - valLoss: 0.6636675000190735 - trainLoss: 0.6624338030815125\n",
      "cnt: 0 - valLoss: 0.6636666655540466 - trainLoss: 0.6624331474304199\n",
      "cnt: 0 - valLoss: 0.663665771484375 - trainLoss: 0.6624325513839722\n",
      "cnt: 0 - valLoss: 0.6636649370193481 - trainLoss: 0.6624318957328796\n",
      "cnt: 0 - valLoss: 0.6636640429496765 - trainLoss: 0.6624312996864319\n",
      "cnt: 0 - valLoss: 0.6636630892753601 - trainLoss: 0.6624305844306946\n",
      "cnt: 0 - valLoss: 0.6636622548103333 - trainLoss: 0.6624299883842468\n",
      "cnt: 0 - valLoss: 0.6636613011360168 - trainLoss: 0.6624292731285095\n",
      "cnt: 0 - valLoss: 0.66366046667099 - trainLoss: 0.6624287366867065\n",
      "cnt: 0 - valLoss: 0.6636595726013184 - trainLoss: 0.6624280214309692\n",
      "cnt: 0 - valLoss: 0.6636586785316467 - trainLoss: 0.6624273657798767\n",
      "cnt: 0 - valLoss: 0.6636577844619751 - trainLoss: 0.6624267101287842\n",
      "cnt: 0 - valLoss: 0.6636568903923035 - trainLoss: 0.6624261140823364\n",
      "cnt: 0 - valLoss: 0.6636559963226318 - trainLoss: 0.6624253988265991\n",
      "cnt: 0 - valLoss: 0.663655161857605 - trainLoss: 0.6624248027801514\n",
      "cnt: 0 - valLoss: 0.6636542677879333 - trainLoss: 0.6624241471290588\n",
      "cnt: 0 - valLoss: 0.6636533141136169 - trainLoss: 0.6624234914779663\n",
      "cnt: 0 - valLoss: 0.6636524796485901 - trainLoss: 0.6624228358268738\n",
      "cnt: 0 - valLoss: 0.6636516451835632 - trainLoss: 0.662422239780426\n",
      "cnt: 0 - valLoss: 0.6636507511138916 - trainLoss: 0.6624215245246887\n",
      "cnt: 0 - valLoss: 0.66364985704422 - trainLoss: 0.662420928478241\n",
      "cnt: 0 - valLoss: 0.6636489629745483 - trainLoss: 0.6624202728271484\n",
      "cnt: 0 - valLoss: 0.6636480689048767 - trainLoss: 0.6624196171760559\n",
      "cnt: 0 - valLoss: 0.6636472344398499 - trainLoss: 0.6624190211296082\n",
      "cnt: 0 - valLoss: 0.6636463403701782 - trainLoss: 0.6624184250831604\n",
      "cnt: 0 - valLoss: 0.6636455655097961 - trainLoss: 0.6624177694320679\n",
      "cnt: 0 - valLoss: 0.6636446118354797 - trainLoss: 0.6624171137809753\n",
      "cnt: 0 - valLoss: 0.6636437177658081 - trainLoss: 0.6624164581298828\n",
      "cnt: 0 - valLoss: 0.6636428236961365 - trainLoss: 0.6624158024787903\n",
      "cnt: 0 - valLoss: 0.6636420488357544 - trainLoss: 0.6624152064323425\n",
      "cnt: 0 - valLoss: 0.6636411547660828 - trainLoss: 0.66241455078125\n",
      "cnt: 0 - valLoss: 0.6636402010917664 - trainLoss: 0.6624139547348022\n",
      "cnt: 0 - valLoss: 0.6636394262313843 - trainLoss: 0.6624132394790649\n",
      "cnt: 0 - valLoss: 0.6636385321617126 - trainLoss: 0.6624126434326172\n",
      "cnt: 0 - valLoss: 0.6636376976966858 - trainLoss: 0.6624120473861694\n",
      "cnt: 0 - valLoss: 0.6636368036270142 - trainLoss: 0.6624113917350769\n",
      "cnt: 0 - valLoss: 0.6636359691619873 - trainLoss: 0.6624107360839844\n",
      "cnt: 0 - valLoss: 0.6636350154876709 - trainLoss: 0.6624100804328918\n",
      "cnt: 0 - valLoss: 0.663634181022644 - trainLoss: 0.6624094843864441\n",
      "cnt: 0 - valLoss: 0.6636333465576172 - trainLoss: 0.6624087691307068\n",
      "cnt: 0 - valLoss: 0.6636324524879456 - trainLoss: 0.6624082326889038\n",
      "cnt: 0 - valLoss: 0.6636316180229187 - trainLoss: 0.6624075174331665\n",
      "cnt: 0 - valLoss: 0.6636306643486023 - trainLoss: 0.6624069213867188\n",
      "cnt: 0 - valLoss: 0.6636298298835754 - trainLoss: 0.6624062657356262\n",
      "cnt: 0 - valLoss: 0.6636290550231934 - trainLoss: 0.6624057292938232\n",
      "cnt: 0 - valLoss: 0.663628101348877 - trainLoss: 0.6624050736427307\n",
      "cnt: 0 - valLoss: 0.6636272668838501 - trainLoss: 0.662404477596283\n",
      "cnt: 0 - valLoss: 0.6636263728141785 - trainLoss: 0.6624037623405457\n",
      "cnt: 0 - valLoss: 0.6636255383491516 - trainLoss: 0.6624032258987427\n",
      "cnt: 0 - valLoss: 0.6636247038841248 - trainLoss: 0.6624025702476501\n",
      "cnt: 0 - valLoss: 0.6636237502098083 - trainLoss: 0.6624019742012024\n",
      "cnt: 0 - valLoss: 0.6636229753494263 - trainLoss: 0.6624013185501099\n",
      "cnt: 0 - valLoss: 0.6636220216751099 - trainLoss: 0.6624006628990173\n",
      "cnt: 0 - valLoss: 0.663621187210083 - trainLoss: 0.6624000668525696\n",
      "cnt: 0 - valLoss: 0.6636203527450562 - trainLoss: 0.6623994708061218\n",
      "cnt: 0 - valLoss: 0.6636194586753845 - trainLoss: 0.6623988747596741\n",
      "cnt: 0 - valLoss: 0.6636186242103577 - trainLoss: 0.6623982191085815\n",
      "cnt: 0 - valLoss: 0.663617730140686 - trainLoss: 0.662397563457489\n",
      "cnt: 0 - valLoss: 0.6636168360710144 - trainLoss: 0.6623969674110413\n",
      "cnt: 0 - valLoss: 0.6636160612106323 - trainLoss: 0.6623963117599487\n",
      "cnt: 0 - valLoss: 0.6636151075363159 - trainLoss: 0.662395715713501\n",
      "cnt: 0 - valLoss: 0.6636142730712891 - trainLoss: 0.6623950600624084\n",
      "cnt: 0 - valLoss: 0.6636133790016174 - trainLoss: 0.6623944044113159\n",
      "cnt: 0 - valLoss: 0.6636125445365906 - trainLoss: 0.6623938083648682\n",
      "cnt: 0 - valLoss: 0.663611650466919 - trainLoss: 0.6623931527137756\n",
      "cnt: 0 - valLoss: 0.6636107563972473 - trainLoss: 0.6623926162719727\n",
      "cnt: 0 - valLoss: 0.6636099815368652 - trainLoss: 0.6623919606208801\n",
      "cnt: 0 - valLoss: 0.6636090278625488 - trainLoss: 0.6623912453651428\n",
      "cnt: 0 - valLoss: 0.663608193397522 - trainLoss: 0.6623906493186951\n",
      "cnt: 0 - valLoss: 0.6636072993278503 - trainLoss: 0.6623899936676025\n",
      "cnt: 0 - valLoss: 0.6636063456535339 - trainLoss: 0.6623894572257996\n",
      "cnt: 0 - valLoss: 0.6636055707931519 - trainLoss: 0.662388801574707\n",
      "cnt: 0 - valLoss: 0.6636046767234802 - trainLoss: 0.6623880863189697\n",
      "cnt: 0 - valLoss: 0.6636037826538086 - trainLoss: 0.662387490272522\n",
      "cnt: 0 - valLoss: 0.6636029481887817 - trainLoss: 0.6623868346214294\n",
      "cnt: 0 - valLoss: 0.6636020541191101 - trainLoss: 0.6623862385749817\n",
      "cnt: 0 - valLoss: 0.6636012196540833 - trainLoss: 0.6623855829238892\n",
      "cnt: 0 - valLoss: 0.6636003255844116 - trainLoss: 0.6623850464820862\n",
      "cnt: 0 - valLoss: 0.6635993719100952 - trainLoss: 0.6623843908309937\n",
      "cnt: 0 - valLoss: 0.6635985374450684 - trainLoss: 0.6623837351799011\n",
      "cnt: 0 - valLoss: 0.6635977029800415 - trainLoss: 0.6623831391334534\n",
      "cnt: 0 - valLoss: 0.6635968089103699 - trainLoss: 0.6623824238777161\n",
      "cnt: 0 - valLoss: 0.6635959148406982 - trainLoss: 0.6623818278312683\n",
      "cnt: 0 - valLoss: 0.6635950803756714 - trainLoss: 0.6623812317848206\n",
      "cnt: 0 - valLoss: 0.6635941863059998 - trainLoss: 0.662380576133728\n",
      "cnt: 0 - valLoss: 0.6635932326316833 - trainLoss: 0.6623799800872803\n",
      "cnt: 0 - valLoss: 0.6635924577713013 - trainLoss: 0.662379264831543\n",
      "cnt: 0 - valLoss: 0.6635915637016296 - trainLoss: 0.6623786091804504\n",
      "cnt: 0 - valLoss: 0.663590669631958 - trainLoss: 0.6623780727386475\n",
      "cnt: 0 - valLoss: 0.6635898351669312 - trainLoss: 0.6623774170875549\n",
      "cnt: 0 - valLoss: 0.6635888814926147 - trainLoss: 0.6623768210411072\n",
      "cnt: 0 - valLoss: 0.6635880470275879 - trainLoss: 0.6623761653900146\n",
      "cnt: 0 - valLoss: 0.6635870933532715 - trainLoss: 0.6623755097389221\n",
      "cnt: 0 - valLoss: 0.6635861992835999 - trainLoss: 0.6623748540878296\n",
      "cnt: 0 - valLoss: 0.6635854244232178 - trainLoss: 0.6623741984367371\n",
      "cnt: 0 - valLoss: 0.6635844707489014 - trainLoss: 0.6623736023902893\n",
      "cnt: 0 - valLoss: 0.6635835766792297 - trainLoss: 0.6623729467391968\n",
      "cnt: 0 - valLoss: 0.6635826826095581 - trainLoss: 0.662372350692749\n",
      "cnt: 0 - valLoss: 0.6635818481445312 - trainLoss: 0.6623716354370117\n",
      "cnt: 0 - valLoss: 0.6635808944702148 - trainLoss: 0.6623709797859192\n",
      "cnt: 0 - valLoss: 0.663580060005188 - trainLoss: 0.6623703837394714\n",
      "cnt: 0 - valLoss: 0.6635792255401611 - trainLoss: 0.6623697876930237\n",
      "cnt: 0 - valLoss: 0.6635783314704895 - trainLoss: 0.6623691320419312\n",
      "cnt: 0 - valLoss: 0.6635774374008179 - trainLoss: 0.6623684763908386\n",
      "cnt: 0 - valLoss: 0.6635764837265015 - trainLoss: 0.6623678207397461\n",
      "cnt: 0 - valLoss: 0.6635756492614746 - trainLoss: 0.6623672842979431\n",
      "cnt: 0 - valLoss: 0.6635748147964478 - trainLoss: 0.6623665690422058\n",
      "cnt: 0 - valLoss: 0.6635738611221313 - trainLoss: 0.6623659729957581\n",
      "cnt: 0 - valLoss: 0.6635730266571045 - trainLoss: 0.6623652577400208\n",
      "cnt: 0 - valLoss: 0.6635720729827881 - trainLoss: 0.662364661693573\n",
      "cnt: 0 - valLoss: 0.6635712385177612 - trainLoss: 0.6623640656471252\n",
      "cnt: 0 - valLoss: 0.6635703444480896 - trainLoss: 0.6623633503913879\n",
      "cnt: 0 - valLoss: 0.6635693907737732 - trainLoss: 0.6623626947402954\n",
      "cnt: 0 - valLoss: 0.6635685563087463 - trainLoss: 0.6623620986938477\n",
      "cnt: 0 - valLoss: 0.6635676026344299 - trainLoss: 0.6623615026473999\n",
      "cnt: 0 - valLoss: 0.6635668277740479 - trainLoss: 0.6623607873916626\n",
      "cnt: 0 - valLoss: 0.6635658740997314 - trainLoss: 0.6623601913452148\n",
      "cnt: 0 - valLoss: 0.6635650396347046 - trainLoss: 0.6623595356941223\n",
      "cnt: 0 - valLoss: 0.663564145565033 - trainLoss: 0.6623588800430298\n",
      "cnt: 0 - valLoss: 0.6635631322860718 - trainLoss: 0.6623582243919373\n",
      "cnt: 0 - valLoss: 0.6635622978210449 - trainLoss: 0.6623576283454895\n",
      "cnt: 0 - valLoss: 0.6635614633560181 - trainLoss: 0.6623569130897522\n",
      "cnt: 0 - valLoss: 0.6635605096817017 - trainLoss: 0.6623563170433044\n",
      "cnt: 0 - valLoss: 0.6635596752166748 - trainLoss: 0.6623556613922119\n",
      "cnt: 0 - valLoss: 0.6635587215423584 - trainLoss: 0.6623550653457642\n",
      "cnt: 0 - valLoss: 0.6635578870773315 - trainLoss: 0.6623543500900269\n",
      "cnt: 0 - valLoss: 0.6635569930076599 - trainLoss: 0.6623536944389343\n",
      "cnt: 0 - valLoss: 0.6635560989379883 - trainLoss: 0.6623530983924866\n",
      "cnt: 0 - valLoss: 0.6635552048683167 - trainLoss: 0.6623525023460388\n",
      "cnt: 0 - valLoss: 0.6635542511940002 - trainLoss: 0.6623518466949463\n",
      "cnt: 0 - valLoss: 0.6635534167289734 - trainLoss: 0.6623511910438538\n",
      "cnt: 0 - valLoss: 0.6635525226593018 - trainLoss: 0.6623504757881165\n",
      "cnt: 0 - valLoss: 0.6635516285896301 - trainLoss: 0.6623498797416687\n",
      "cnt: 0 - valLoss: 0.6635507941246033 - trainLoss: 0.6623492240905762\n",
      "cnt: 0 - valLoss: 0.6635497808456421 - trainLoss: 0.6623485684394836\n",
      "cnt: 0 - valLoss: 0.6635488867759705 - trainLoss: 0.6623479723930359\n",
      "cnt: 0 - valLoss: 0.6635480523109436 - trainLoss: 0.6623472571372986\n",
      "cnt: 0 - valLoss: 0.663547158241272 - trainLoss: 0.6623466610908508\n",
      "cnt: 0 - valLoss: 0.6635462045669556 - trainLoss: 0.6623460054397583\n",
      "cnt: 0 - valLoss: 0.6635453104972839 - trainLoss: 0.6623453497886658\n",
      "cnt: 0 - valLoss: 0.6635444164276123 - trainLoss: 0.6623446941375732\n",
      "cnt: 0 - valLoss: 0.6635434627532959 - trainLoss: 0.6623440384864807\n",
      "cnt: 0 - valLoss: 0.6635425686836243 - trainLoss: 0.662343442440033\n",
      "cnt: 0 - valLoss: 0.6635417342185974 - trainLoss: 0.6623427271842957\n",
      "cnt: 0 - valLoss: 0.663540780544281 - trainLoss: 0.6623420715332031\n",
      "cnt: 0 - valLoss: 0.6635399460792542 - trainLoss: 0.6623414754867554\n",
      "cnt: 0 - valLoss: 0.663538932800293 - trainLoss: 0.6623407602310181\n",
      "cnt: 0 - valLoss: 0.6635380983352661 - trainLoss: 0.6623401641845703\n",
      "cnt: 0 - valLoss: 0.6635370850563049 - trainLoss: 0.662339448928833\n",
      "cnt: 0 - valLoss: 0.6635362505912781 - trainLoss: 0.6623387932777405\n",
      "cnt: 0 - valLoss: 0.6635354161262512 - trainLoss: 0.6623380780220032\n",
      "cnt: 0 - valLoss: 0.66353440284729 - trainLoss: 0.6623374819755554\n",
      "cnt: 0 - valLoss: 0.6635335683822632 - trainLoss: 0.6623368263244629\n",
      "cnt: 0 - valLoss: 0.6635326147079468 - trainLoss: 0.6623361706733704\n",
      "cnt: 0 - valLoss: 0.6635317206382751 - trainLoss: 0.6623355150222778\n",
      "cnt: 0 - valLoss: 0.6635308265686035 - trainLoss: 0.6623348593711853\n",
      "cnt: 0 - valLoss: 0.6635298728942871 - trainLoss: 0.6623342633247375\n",
      "cnt: 0 - valLoss: 0.6635289788246155 - trainLoss: 0.6623335480690002\n",
      "cnt: 0 - valLoss: 0.6635280847549438 - trainLoss: 0.6623328924179077\n",
      "cnt: 0 - valLoss: 0.6635271310806274 - trainLoss: 0.6623321771621704\n",
      "cnt: 0 - valLoss: 0.6635262370109558 - trainLoss: 0.6623315215110779\n",
      "cnt: 0 - valLoss: 0.6635253429412842 - trainLoss: 0.6623309254646301\n",
      "cnt: 0 - valLoss: 0.6635243892669678 - trainLoss: 0.6623302102088928\n",
      "cnt: 0 - valLoss: 0.6635234951972961 - trainLoss: 0.6623295545578003\n",
      "cnt: 0 - valLoss: 0.6635226011276245 - trainLoss: 0.662328839302063\n",
      "cnt: 0 - valLoss: 0.6635216474533081 - trainLoss: 0.6623282432556152\n",
      "cnt: 0 - valLoss: 0.6635207533836365 - trainLoss: 0.6623275876045227\n",
      "cnt: 0 - valLoss: 0.6635197997093201 - trainLoss: 0.6623269319534302\n",
      "cnt: 0 - valLoss: 0.6635189056396484 - trainLoss: 0.6623262763023376\n",
      "cnt: 0 - valLoss: 0.6635180115699768 - trainLoss: 0.6623256206512451\n",
      "cnt: 0 - valLoss: 0.6635171175003052 - trainLoss: 0.6623249053955078\n",
      "cnt: 0 - valLoss: 0.6635162234306335 - trainLoss: 0.6623243093490601\n",
      "cnt: 0 - valLoss: 0.6635152697563171 - trainLoss: 0.6623235940933228\n",
      "cnt: 0 - valLoss: 0.6635143756866455 - trainLoss: 0.6623229384422302\n",
      "cnt: 0 - valLoss: 0.6635134220123291 - trainLoss: 0.6623222827911377\n",
      "cnt: 0 - valLoss: 0.6635124683380127 - trainLoss: 0.6623215675354004\n",
      "cnt: 0 - valLoss: 0.6635115742683411 - trainLoss: 0.6623209118843079\n",
      "cnt: 0 - valLoss: 0.6635106801986694 - trainLoss: 0.6623202562332153\n",
      "cnt: 0 - valLoss: 0.663509726524353 - trainLoss: 0.6623196005821228\n",
      "cnt: 0 - valLoss: 0.6635087132453918 - trainLoss: 0.6623188853263855\n",
      "cnt: 0 - valLoss: 0.6635077595710754 - trainLoss: 0.6623181700706482\n",
      "cnt: 0 - valLoss: 0.663506805896759 - trainLoss: 0.6623174548149109\n",
      "cnt: 0 - valLoss: 0.6635057926177979 - trainLoss: 0.6623167395591736\n",
      "cnt: 0 - valLoss: 0.6635048389434814 - trainLoss: 0.662316083908081\n",
      "cnt: 0 - valLoss: 0.6635037660598755 - trainLoss: 0.662315309047699\n",
      "cnt: 0 - valLoss: 0.6635028719902039 - trainLoss: 0.6623145937919617\n",
      "cnt: 0 - valLoss: 0.6635018587112427 - trainLoss: 0.6623138785362244\n",
      "cnt: 0 - valLoss: 0.6635009050369263 - trainLoss: 0.6623132228851318\n",
      "cnt: 0 - valLoss: 0.6634998321533203 - trainLoss: 0.6623124480247498\n",
      "cnt: 0 - valLoss: 0.6634989380836487 - trainLoss: 0.6623117327690125\n",
      "cnt: 0 - valLoss: 0.6634979248046875 - trainLoss: 0.6623110175132751\n",
      "cnt: 0 - valLoss: 0.6634969115257263 - trainLoss: 0.6623103022575378\n",
      "cnt: 0 - valLoss: 0.6634959578514099 - trainLoss: 0.6623095870018005\n",
      "cnt: 0 - valLoss: 0.6634949445724487 - trainLoss: 0.6623088121414185\n",
      "cnt: 0 - valLoss: 0.6634940505027771 - trainLoss: 0.6623080968856812\n",
      "cnt: 0 - valLoss: 0.6634930372238159 - trainLoss: 0.6623074412345886\n",
      "cnt: 0 - valLoss: 0.6634920239448547 - trainLoss: 0.6623067259788513\n",
      "cnt: 0 - valLoss: 0.6634910702705383 - trainLoss: 0.662306010723114\n",
      "cnt: 0 - valLoss: 0.6634900569915771 - trainLoss: 0.6623052358627319\n",
      "cnt: 0 - valLoss: 0.6634891033172607 - trainLoss: 0.6623045206069946\n",
      "cnt: 0 - valLoss: 0.6634880900382996 - trainLoss: 0.6623038649559021\n",
      "cnt: 0 - valLoss: 0.6634870767593384 - trainLoss: 0.6623031497001648\n",
      "cnt: 0 - valLoss: 0.6634864211082458 - trainLoss: 0.6623023748397827\n",
      "cnt: 0 - valLoss: 0.6634852290153503 - trainLoss: 0.6623017191886902\n",
      "cnt: 0 - valLoss: 0.6634843945503235 - trainLoss: 0.6623010039329529\n",
      "cnt: 0 - valLoss: 0.663483202457428 - trainLoss: 0.6623002886772156\n",
      "cnt: 0 - valLoss: 0.6634824275970459 - trainLoss: 0.6622995734214783\n",
      "cnt: 0 - valLoss: 0.6634815335273743 - trainLoss: 0.6622989177703857\n",
      "cnt: 0 - valLoss: 0.6634806394577026 - trainLoss: 0.6622982025146484\n",
      "cnt: 0 - valLoss: 0.663479745388031 - trainLoss: 0.6622975468635559\n",
      "cnt: 0 - valLoss: 0.6634789109230042 - trainLoss: 0.6622968316078186\n",
      "cnt: 0 - valLoss: 0.6634779572486877 - trainLoss: 0.6622961163520813\n",
      "cnt: 0 - valLoss: 0.6634771227836609 - trainLoss: 0.6622954607009888\n",
      "cnt: 0 - valLoss: 0.6634758710861206 - trainLoss: 0.6622946858406067\n",
      "cnt: 0 - valLoss: 0.663474977016449 - trainLoss: 0.6622940897941589\n",
      "cnt: 0 - valLoss: 0.6634740829467773 - trainLoss: 0.6622933745384216\n",
      "cnt: 0 - valLoss: 0.6634731888771057 - trainLoss: 0.6622927188873291\n",
      "cnt: 0 - valLoss: 0.6634722948074341 - trainLoss: 0.662291944026947\n",
      "cnt: 0 - valLoss: 0.6634714007377625 - trainLoss: 0.6622912883758545\n",
      "cnt: 0 - valLoss: 0.6634705066680908 - trainLoss: 0.6622905731201172\n",
      "cnt: 0 - valLoss: 0.6634696125984192 - trainLoss: 0.6622897982597351\n",
      "cnt: 0 - valLoss: 0.6634687185287476 - trainLoss: 0.6622892022132874\n",
      "cnt: 0 - valLoss: 0.6634678244590759 - trainLoss: 0.66228848695755\n",
      "cnt: 0 - valLoss: 0.6634669899940491 - trainLoss: 0.6622878313064575\n",
      "cnt: 0 - valLoss: 0.6634660959243774 - trainLoss: 0.6622871160507202\n",
      "cnt: 0 - valLoss: 0.6634652018547058 - trainLoss: 0.6622864007949829\n",
      "cnt: 0 - valLoss: 0.6634643077850342 - trainLoss: 0.6622856855392456\n",
      "cnt: 0 - valLoss: 0.6634630560874939 - trainLoss: 0.6622849702835083\n",
      "cnt: 0 - valLoss: 0.6634621024131775 - trainLoss: 0.6622843742370605\n",
      "cnt: 0 - valLoss: 0.6634612083435059 - trainLoss: 0.6622835993766785\n",
      "cnt: 0 - valLoss: 0.6634603142738342 - trainLoss: 0.6622828841209412\n",
      "cnt: 0 - valLoss: 0.6634594202041626 - trainLoss: 0.6622822284698486\n",
      "cnt: 0 - valLoss: 0.6634585857391357 - trainLoss: 0.6622815132141113\n",
      "cnt: 0 - valLoss: 0.6634576320648193 - trainLoss: 0.662280797958374\n",
      "cnt: 0 - valLoss: 0.6634567975997925 - trainLoss: 0.6622800827026367\n",
      "cnt: 0 - valLoss: 0.6634558439254761 - trainLoss: 0.6622793674468994\n",
      "cnt: 0 - valLoss: 0.6634549498558044 - trainLoss: 0.6622786521911621\n",
      "cnt: 0 - valLoss: 0.6634540557861328 - trainLoss: 0.6622779369354248\n",
      "cnt: 0 - valLoss: 0.6634528636932373 - trainLoss: 0.6622772812843323\n",
      "cnt: 0 - valLoss: 0.6634519100189209 - trainLoss: 0.6622765064239502\n",
      "cnt: 0 - valLoss: 0.6634510159492493 - trainLoss: 0.6622758507728577\n",
      "cnt: 0 - valLoss: 0.6634501218795776 - trainLoss: 0.6622750759124756\n",
      "cnt: 0 - valLoss: 0.6634492874145508 - trainLoss: 0.6622743606567383\n",
      "cnt: 0 - valLoss: 0.6634483337402344 - trainLoss: 0.662273645401001\n",
      "cnt: 0 - valLoss: 0.6634474992752075 - trainLoss: 0.6622729301452637\n",
      "cnt: 0 - valLoss: 0.6634464263916016 - trainLoss: 0.6622722148895264\n",
      "cnt: 0 - valLoss: 0.6634455323219299 - trainLoss: 0.6622714996337891\n",
      "cnt: 0 - valLoss: 0.6634445190429688 - trainLoss: 0.6622708439826965\n",
      "cnt: 0 - valLoss: 0.6634436845779419 - trainLoss: 0.6622700691223145\n",
      "cnt: 0 - valLoss: 0.6634425520896912 - trainLoss: 0.6622694730758667\n",
      "cnt: 0 - valLoss: 0.6634417176246643 - trainLoss: 0.6622686982154846\n",
      "cnt: 0 - valLoss: 0.6634406447410583 - trainLoss: 0.6622680425643921\n",
      "cnt: 0 - valLoss: 0.6634398102760315 - trainLoss: 0.6622673273086548\n",
      "cnt: 0 - valLoss: 0.6634387969970703 - trainLoss: 0.6622666120529175\n",
      "cnt: 0 - valLoss: 0.6634379029273987 - trainLoss: 0.662265956401825\n",
      "cnt: 0 - valLoss: 0.6634368300437927 - trainLoss: 0.6622652411460876\n",
      "cnt: 0 - valLoss: 0.6634357571601868 - trainLoss: 0.6622645854949951\n",
      "cnt: 0 - valLoss: 0.6634348630905151 - trainLoss: 0.6622638702392578\n",
      "cnt: 0 - valLoss: 0.6634339094161987 - trainLoss: 0.6622631549835205\n",
      "cnt: 0 - valLoss: 0.6634330153465271 - trainLoss: 0.662262499332428\n",
      "cnt: 0 - valLoss: 0.6634319424629211 - trainLoss: 0.6622617244720459\n",
      "cnt: 0 - valLoss: 0.6634310483932495 - trainLoss: 0.6622610688209534\n",
      "cnt: 0 - valLoss: 0.6634300351142883 - trainLoss: 0.6622603535652161\n",
      "cnt: 0 - valLoss: 0.6634291410446167 - trainLoss: 0.6622596979141235\n",
      "cnt: 0 - valLoss: 0.6634281277656555 - trainLoss: 0.6622589826583862\n",
      "cnt: 0 - valLoss: 0.6634271740913391 - trainLoss: 0.6622582674026489\n",
      "cnt: 0 - valLoss: 0.6634261608123779 - trainLoss: 0.6622575521469116\n",
      "cnt: 0 - valLoss: 0.6634252071380615 - trainLoss: 0.6622568964958191\n",
      "cnt: 0 - valLoss: 0.6634241938591003 - trainLoss: 0.6622561812400818\n",
      "cnt: 0 - valLoss: 0.6634233593940735 - trainLoss: 0.6622555255889893\n",
      "cnt: 0 - valLoss: 0.6634222865104675 - trainLoss: 0.6622547507286072\n",
      "cnt: 0 - valLoss: 0.6634213924407959 - trainLoss: 0.6622541546821594\n",
      "cnt: 0 - valLoss: 0.6634202599525452 - trainLoss: 0.6622534394264221\n",
      "cnt: 0 - valLoss: 0.6634194254875183 - trainLoss: 0.6622527837753296\n",
      "cnt: 0 - valLoss: 0.6634184718132019 - trainLoss: 0.6622520685195923\n",
      "cnt: 0 - valLoss: 0.6634174585342407 - trainLoss: 0.662251353263855\n",
      "cnt: 0 - valLoss: 0.6634165048599243 - trainLoss: 0.6622506976127625\n",
      "cnt: 0 - valLoss: 0.6634154319763184 - trainLoss: 0.6622500419616699\n",
      "cnt: 0 - valLoss: 0.6634145379066467 - trainLoss: 0.6622493267059326\n",
      "cnt: 0 - valLoss: 0.6634136438369751 - trainLoss: 0.6622486710548401\n",
      "cnt: 0 - valLoss: 0.6634125709533691 - trainLoss: 0.6622480154037476\n",
      "cnt: 0 - valLoss: 0.6634116172790527 - trainLoss: 0.6622473001480103\n",
      "cnt: 0 - valLoss: 0.6634105443954468 - trainLoss: 0.662246584892273\n",
      "cnt: 0 - valLoss: 0.6634096503257751 - trainLoss: 0.6622459292411804\n",
      "cnt: 0 - valLoss: 0.6634087562561035 - trainLoss: 0.6622452735900879\n",
      "cnt: 0 - valLoss: 0.6634076833724976 - trainLoss: 0.6622445583343506\n",
      "cnt: 0 - valLoss: 0.6634067296981812 - trainLoss: 0.6622438430786133\n",
      "cnt: 0 - valLoss: 0.6634056568145752 - trainLoss: 0.6622431874275208\n",
      "cnt: 0 - valLoss: 0.6634047627449036 - trainLoss: 0.6622424721717834\n",
      "cnt: 0 - valLoss: 0.6634036898612976 - trainLoss: 0.6622418165206909\n",
      "cnt: 0 - valLoss: 0.663402795791626 - trainLoss: 0.6622411608695984\n",
      "cnt: 0 - valLoss: 0.6634018421173096 - trainLoss: 0.6622404456138611\n",
      "cnt: 0 - valLoss: 0.6634007692337036 - trainLoss: 0.6622397303581238\n",
      "cnt: 0 - valLoss: 0.663399875164032 - trainLoss: 0.6622390747070312\n",
      "cnt: 0 - valLoss: 0.663398802280426 - trainLoss: 0.662238359451294\n",
      "cnt: 0 - valLoss: 0.6633979082107544 - trainLoss: 0.6622377634048462\n",
      "cnt: 0 - valLoss: 0.6633968353271484 - trainLoss: 0.6622369885444641\n",
      "cnt: 0 - valLoss: 0.6633959412574768 - trainLoss: 0.6622363924980164\n",
      "cnt: 0 - valLoss: 0.6633948683738708 - trainLoss: 0.662235677242279\n",
      "cnt: 0 - valLoss: 0.6633939743041992 - trainLoss: 0.6622350215911865\n",
      "cnt: 0 - valLoss: 0.663392961025238 - trainLoss: 0.6622342467308044\n",
      "cnt: 0 - valLoss: 0.6633919477462769 - trainLoss: 0.6622336506843567\n",
      "cnt: 0 - valLoss: 0.6633909344673157 - trainLoss: 0.6622328758239746\n",
      "cnt: 0 - valLoss: 0.663390040397644 - trainLoss: 0.6622322797775269\n",
      "cnt: 0 - valLoss: 0.6633889675140381 - trainLoss: 0.6622315049171448\n",
      "cnt: 0 - valLoss: 0.6633880734443665 - trainLoss: 0.662230908870697\n",
      "cnt: 0 - valLoss: 0.6633870005607605 - trainLoss: 0.6622301340103149\n",
      "cnt: 0 - valLoss: 0.6633860468864441 - trainLoss: 0.6622294783592224\n",
      "cnt: 0 - valLoss: 0.6633850336074829 - trainLoss: 0.6622287631034851\n",
      "cnt: 0 - valLoss: 0.6633841395378113 - trainLoss: 0.6622281074523926\n",
      "cnt: 0 - valLoss: 0.6633831858634949 - trainLoss: 0.6622273921966553\n",
      "cnt: 0 - valLoss: 0.6633821129798889 - trainLoss: 0.6622267365455627\n",
      "cnt: 0 - valLoss: 0.6633811593055725 - trainLoss: 0.6622260212898254\n",
      "cnt: 0 - valLoss: 0.6633801460266113 - trainLoss: 0.6622253060340881\n",
      "cnt: 0 - valLoss: 0.6633792519569397 - trainLoss: 0.6622245907783508\n",
      "cnt: 0 - valLoss: 0.6633781790733337 - trainLoss: 0.6622239351272583\n",
      "cnt: 0 - valLoss: 0.6633772253990173 - trainLoss: 0.662223219871521\n",
      "cnt: 0 - valLoss: 0.6633762121200562 - trainLoss: 0.6622225642204285\n",
      "cnt: 0 - valLoss: 0.6633752584457397 - trainLoss: 0.6622218489646912\n",
      "cnt: 0 - valLoss: 0.6633742451667786 - trainLoss: 0.6622211933135986\n",
      "cnt: 0 - valLoss: 0.6633732914924622 - trainLoss: 0.6622204780578613\n",
      "cnt: 0 - valLoss: 0.6633721590042114 - trainLoss: 0.6622198224067688\n",
      "cnt: 0 - valLoss: 0.6633713245391846 - trainLoss: 0.6622191071510315\n",
      "cnt: 0 - valLoss: 0.6633701920509338 - trainLoss: 0.6622183322906494\n",
      "cnt: 0 - valLoss: 0.663369357585907 - trainLoss: 0.6622177362442017\n",
      "cnt: 0 - valLoss: 0.6633682250976562 - trainLoss: 0.6622169613838196\n",
      "cnt: 0 - valLoss: 0.6633672714233398 - trainLoss: 0.662216305732727\n",
      "cnt: 0 - valLoss: 0.6633662581443787 - trainLoss: 0.6622155904769897\n",
      "cnt: 0 - valLoss: 0.663365364074707 - trainLoss: 0.6622148752212524\n",
      "cnt: 0 - valLoss: 0.6633642315864563 - trainLoss: 0.6622142195701599\n",
      "cnt: 0 - valLoss: 0.6633632779121399 - trainLoss: 0.6622135043144226\n",
      "cnt: 0 - valLoss: 0.6633622050285339 - trainLoss: 0.6622128486633301\n",
      "cnt: 0 - valLoss: 0.663361132144928 - trainLoss: 0.6622121334075928\n",
      "cnt: 0 - valLoss: 0.6633602380752563 - trainLoss: 0.6622114181518555\n",
      "cnt: 0 - valLoss: 0.6633591651916504 - trainLoss: 0.6622107028961182\n",
      "cnt: 0 - valLoss: 0.6633582711219788 - trainLoss: 0.6622100472450256\n",
      "cnt: 0 - valLoss: 0.663357138633728 - trainLoss: 0.6622093319892883\n",
      "cnt: 0 - valLoss: 0.6633562445640564 - trainLoss: 0.662208616733551\n",
      "cnt: 0 - valLoss: 0.6633551716804504 - trainLoss: 0.6622079014778137\n",
      "cnt: 0 - valLoss: 0.6633542776107788 - trainLoss: 0.6622072458267212\n",
      "cnt: 0 - valLoss: 0.6633531451225281 - trainLoss: 0.6622065305709839\n",
      "cnt: 0 - valLoss: 0.6633521914482117 - trainLoss: 0.6622058749198914\n",
      "cnt: 0 - valLoss: 0.6633511781692505 - trainLoss: 0.6622051000595093\n",
      "cnt: 0 - valLoss: 0.6633502840995789 - trainLoss: 0.6622044444084167\n",
      "cnt: 0 - valLoss: 0.6633491516113281 - trainLoss: 0.6622037291526794\n",
      "cnt: 0 - valLoss: 0.6633482575416565 - trainLoss: 0.6622030735015869\n",
      "cnt: 0 - valLoss: 0.6633471250534058 - trainLoss: 0.6622022986412048\n",
      "cnt: 0 - valLoss: 0.6633461713790894 - trainLoss: 0.6622016429901123\n",
      "cnt: 0 - valLoss: 0.6633452773094177 - trainLoss: 0.6622008681297302\n",
      "cnt: 0 - valLoss: 0.663344144821167 - trainLoss: 0.6622002720832825\n",
      "cnt: 0 - valLoss: 0.6633432507514954 - trainLoss: 0.6621994972229004\n",
      "cnt: 0 - valLoss: 0.6633421778678894 - trainLoss: 0.6621988415718079\n",
      "cnt: 0 - valLoss: 0.663341224193573 - trainLoss: 0.6621981263160706\n",
      "cnt: 0 - valLoss: 0.6633402109146118 - trainLoss: 0.6621974110603333\n",
      "cnt: 0 - valLoss: 0.6633392572402954 - trainLoss: 0.662196695804596\n",
      "cnt: 0 - valLoss: 0.6633381843566895 - trainLoss: 0.6621959805488586\n",
      "cnt: 0 - valLoss: 0.663337230682373 - trainLoss: 0.6621952652931213\n",
      "cnt: 0 - valLoss: 0.6633361577987671 - trainLoss: 0.662194550037384\n",
      "cnt: 0 - valLoss: 0.6633352041244507 - trainLoss: 0.6621938943862915\n",
      "cnt: 0 - valLoss: 0.6633340716362 - trainLoss: 0.6621931791305542\n",
      "cnt: 0 - valLoss: 0.6633331775665283 - trainLoss: 0.6621924638748169\n",
      "cnt: 0 - valLoss: 0.6633320450782776 - trainLoss: 0.6621917486190796\n",
      "cnt: 0 - valLoss: 0.6633310914039612 - trainLoss: 0.6621910333633423\n",
      "cnt: 0 - valLoss: 0.6633300185203552 - trainLoss: 0.6621903777122498\n",
      "cnt: 0 - valLoss: 0.6633290648460388 - trainLoss: 0.6621896624565125\n",
      "cnt: 0 - valLoss: 0.6633279919624329 - trainLoss: 0.6621890068054199\n",
      "cnt: 0 - valLoss: 0.6633270382881165 - trainLoss: 0.6621882319450378\n",
      "cnt: 0 - valLoss: 0.6633259057998657 - trainLoss: 0.6621875166893005\n",
      "cnt: 0 - valLoss: 0.6633250117301941 - trainLoss: 0.662186861038208\n",
      "cnt: 0 - valLoss: 0.6633240580558777 - trainLoss: 0.6621861457824707\n",
      "cnt: 0 - valLoss: 0.6633229851722717 - trainLoss: 0.6621854901313782\n",
      "cnt: 0 - valLoss: 0.6633220911026001 - trainLoss: 0.6621847152709961\n",
      "cnt: 0 - valLoss: 0.6633208990097046 - trainLoss: 0.6621840000152588\n",
      "cnt: 0 - valLoss: 0.6633199453353882 - trainLoss: 0.6621833443641663\n",
      "cnt: 0 - valLoss: 0.6633188724517822 - trainLoss: 0.662182629108429\n",
      "cnt: 0 - valLoss: 0.6633179187774658 - trainLoss: 0.6621819138526917\n",
      "cnt: 0 - valLoss: 0.6633168458938599 - trainLoss: 0.6621812582015991\n",
      "cnt: 0 - valLoss: 0.6633158922195435 - trainLoss: 0.662180483341217\n",
      "cnt: 0 - valLoss: 0.6633147597312927 - trainLoss: 0.6621797680854797\n",
      "cnt: 0 - valLoss: 0.6633138060569763 - trainLoss: 0.6621790528297424\n",
      "cnt: 0 - valLoss: 0.6633127331733704 - trainLoss: 0.6621783971786499\n",
      "cnt: 0 - valLoss: 0.6633117198944092 - trainLoss: 0.6621776819229126\n",
      "cnt: 0 - valLoss: 0.6633108258247375 - trainLoss: 0.6621770262718201\n",
      "cnt: 0 - valLoss: 0.6633097529411316 - trainLoss: 0.6621763110160828\n",
      "cnt: 0 - valLoss: 0.6633087992668152 - trainLoss: 0.6621755361557007\n",
      "cnt: 0 - valLoss: 0.6633077263832092 - trainLoss: 0.6621748805046082\n",
      "cnt: 0 - valLoss: 0.6633066534996033 - trainLoss: 0.6621741056442261\n",
      "cnt: 0 - valLoss: 0.6633056402206421 - trainLoss: 0.6621733903884888\n",
      "cnt: 0 - valLoss: 0.6633046865463257 - trainLoss: 0.6621727347373962\n",
      "cnt: 0 - valLoss: 0.663303554058075 - trainLoss: 0.6621720194816589\n",
      "cnt: 0 - valLoss: 0.6633025407791138 - trainLoss: 0.6621713042259216\n",
      "cnt: 0 - valLoss: 0.6633014678955078 - trainLoss: 0.6621705889701843\n",
      "cnt: 0 - valLoss: 0.6633005738258362 - trainLoss: 0.662169873714447\n",
      "cnt: 0 - valLoss: 0.6632993817329407 - trainLoss: 0.6621691584587097\n",
      "cnt: 0 - valLoss: 0.663298487663269 - trainLoss: 0.6621684432029724\n",
      "cnt: 0 - valLoss: 0.6632973551750183 - trainLoss: 0.6621677279472351\n",
      "cnt: 0 - valLoss: 0.6632964015007019 - trainLoss: 0.6621670126914978\n",
      "cnt: 0 - valLoss: 0.663295328617096 - trainLoss: 0.6621662974357605\n",
      "cnt: 0 - valLoss: 0.6632943153381348 - trainLoss: 0.6621655821800232\n",
      "cnt: 0 - valLoss: 0.6632933616638184 - trainLoss: 0.6621648669242859\n",
      "cnt: 0 - valLoss: 0.6632922887802124 - trainLoss: 0.6621642112731934\n",
      "cnt: 0 - valLoss: 0.663291335105896 - trainLoss: 0.662163496017456\n",
      "cnt: 0 - valLoss: 0.6632902026176453 - trainLoss: 0.662162721157074\n",
      "cnt: 0 - valLoss: 0.6632892489433289 - trainLoss: 0.6621620059013367\n",
      "cnt: 0 - valLoss: 0.6632881164550781 - trainLoss: 0.6621613502502441\n",
      "cnt: 0 - valLoss: 0.6632871031761169 - trainLoss: 0.6621605753898621\n",
      "cnt: 0 - valLoss: 0.663286030292511 - trainLoss: 0.6621598601341248\n",
      "cnt: 0 - valLoss: 0.6632851362228394 - trainLoss: 0.6621591448783875\n",
      "cnt: 0 - valLoss: 0.6632840037345886 - trainLoss: 0.6621584296226501\n",
      "cnt: 0 - valLoss: 0.6632830500602722 - trainLoss: 0.6621577143669128\n",
      "cnt: 0 - valLoss: 0.6632819175720215 - trainLoss: 0.6621569991111755\n",
      "cnt: 0 - valLoss: 0.6632810235023499 - trainLoss: 0.6621562838554382\n",
      "cnt: 0 - valLoss: 0.6632798314094543 - trainLoss: 0.6621555685997009\n",
      "cnt: 0 - valLoss: 0.6632788777351379 - trainLoss: 0.6621548533439636\n",
      "cnt: 0 - valLoss: 0.663277804851532 - trainLoss: 0.6621541380882263\n",
      "cnt: 0 - valLoss: 0.6632768511772156 - trainLoss: 0.662153422832489\n",
      "cnt: 0 - valLoss: 0.6632757186889648 - trainLoss: 0.6621527075767517\n",
      "cnt: 0 - valLoss: 0.6632747650146484 - trainLoss: 0.6621519923210144\n",
      "cnt: 0 - valLoss: 0.6632736325263977 - trainLoss: 0.6621512770652771\n",
      "cnt: 0 - valLoss: 0.6632726192474365 - trainLoss: 0.6621505618095398\n",
      "cnt: 0 - valLoss: 0.6632715463638306 - trainLoss: 0.6621498465538025\n",
      "cnt: 0 - valLoss: 0.6632705926895142 - trainLoss: 0.6621490716934204\n",
      "cnt: 0 - valLoss: 0.6632694602012634 - trainLoss: 0.6621484160423279\n",
      "cnt: 0 - valLoss: 0.663268506526947 - trainLoss: 0.6621476411819458\n",
      "cnt: 0 - valLoss: 0.6632674336433411 - trainLoss: 0.6621469259262085\n",
      "cnt: 0 - valLoss: 0.6632664799690247 - trainLoss: 0.6621462106704712\n",
      "cnt: 0 - valLoss: 0.6632653474807739 - trainLoss: 0.6621454954147339\n",
      "cnt: 0 - valLoss: 0.6632643938064575 - trainLoss: 0.6621448397636414\n",
      "cnt: 0 - valLoss: 0.6632634401321411 - trainLoss: 0.6621440052986145\n",
      "cnt: 0 - valLoss: 0.6632622480392456 - trainLoss: 0.6621432900428772\n",
      "cnt: 0 - valLoss: 0.6632612943649292 - trainLoss: 0.6621426343917847\n",
      "cnt: 0 - valLoss: 0.6632601618766785 - trainLoss: 0.6621418595314026\n",
      "cnt: 0 - valLoss: 0.6632592082023621 - trainLoss: 0.6621411442756653\n",
      "cnt: 0 - valLoss: 0.6632580757141113 - trainLoss: 0.662140429019928\n",
      "cnt: 0 - valLoss: 0.6632570624351501 - trainLoss: 0.6621396541595459\n",
      "cnt: 0 - valLoss: 0.6632559895515442 - trainLoss: 0.6621389985084534\n",
      "cnt: 0 - valLoss: 0.663254976272583 - trainLoss: 0.6621382832527161\n",
      "cnt: 0 - valLoss: 0.663253903388977 - trainLoss: 0.6621375679969788\n",
      "cnt: 0 - valLoss: 0.6632529497146606 - trainLoss: 0.6621368527412415\n",
      "cnt: 0 - valLoss: 0.6632518172264099 - trainLoss: 0.6621361374855042\n",
      "cnt: 0 - valLoss: 0.6632508039474487 - trainLoss: 0.6621354222297668\n",
      "cnt: 0 - valLoss: 0.6632498502731323 - trainLoss: 0.6621346473693848\n",
      "cnt: 0 - valLoss: 0.6632487177848816 - trainLoss: 0.6621339917182922\n",
      "cnt: 0 - valLoss: 0.6632477045059204 - trainLoss: 0.6621332764625549\n",
      "cnt: 0 - valLoss: 0.6632465720176697 - trainLoss: 0.6621325016021729\n",
      "cnt: 0 - valLoss: 0.6632456183433533 - trainLoss: 0.6621317863464355\n",
      "cnt: 0 - valLoss: 0.6632444858551025 - trainLoss: 0.662131130695343\n",
      "cnt: 0 - valLoss: 0.6632435321807861 - trainLoss: 0.6621303558349609\n",
      "cnt: 0 - valLoss: 0.6632423996925354 - trainLoss: 0.6621296405792236\n",
      "cnt: 0 - valLoss: 0.6632413864135742 - trainLoss: 0.6621288657188416\n",
      "cnt: 0 - valLoss: 0.6632404327392578 - trainLoss: 0.662128210067749\n",
      "cnt: 0 - valLoss: 0.6632393598556519 - trainLoss: 0.6621274352073669\n",
      "cnt: 0 - valLoss: 0.6632382869720459 - trainLoss: 0.6621267795562744\n",
      "cnt: 0 - valLoss: 0.6632372140884399 - trainLoss: 0.6621260643005371\n",
      "cnt: 0 - valLoss: 0.6632362008094788 - trainLoss: 0.6621254086494446\n",
      "cnt: 0 - valLoss: 0.663235068321228 - trainLoss: 0.6621246337890625\n",
      "cnt: 0 - valLoss: 0.6632341146469116 - trainLoss: 0.6621238589286804\n",
      "cnt: 0 - valLoss: 0.6632329821586609 - trainLoss: 0.6621231436729431\n",
      "cnt: 0 - valLoss: 0.6632320284843445 - trainLoss: 0.6621224880218506\n",
      "cnt: 0 - valLoss: 0.6632310152053833 - trainLoss: 0.6621217727661133\n",
      "cnt: 0 - valLoss: 0.6632298827171326 - trainLoss: 0.6621209979057312\n",
      "cnt: 0 - valLoss: 0.6632289290428162 - trainLoss: 0.6621202230453491\n",
      "cnt: 0 - valLoss: 0.6632277369499207 - trainLoss: 0.6621195673942566\n",
      "cnt: 0 - valLoss: 0.6632267832756042 - trainLoss: 0.6621187925338745\n",
      "cnt: 0 - valLoss: 0.6632256507873535 - trainLoss: 0.6621180772781372\n",
      "cnt: 0 - valLoss: 0.6632246375083923 - trainLoss: 0.6621173620223999\n",
      "cnt: 0 - valLoss: 0.6632235050201416 - trainLoss: 0.6621166467666626\n",
      "cnt: 0 - valLoss: 0.6632225513458252 - trainLoss: 0.6621159315109253\n",
      "cnt: 0 - valLoss: 0.663221538066864 - trainLoss: 0.662115216255188\n",
      "cnt: 0 - valLoss: 0.6632204651832581 - trainLoss: 0.6621145009994507\n",
      "cnt: 0 - valLoss: 0.6632194519042969 - trainLoss: 0.6621137261390686\n",
      "cnt: 0 - valLoss: 0.6632183194160461 - trainLoss: 0.6621129512786865\n",
      "cnt: 0 - valLoss: 0.6632173657417297 - trainLoss: 0.662112295627594\n",
      "cnt: 0 - valLoss: 0.6632161736488342 - trainLoss: 0.6621115803718567\n",
      "cnt: 0 - valLoss: 0.663215160369873 - trainLoss: 0.6621108651161194\n",
      "cnt: 0 - valLoss: 0.6632140278816223 - trainLoss: 0.6621101498603821\n",
      "cnt: 0 - valLoss: 0.6632130742073059 - trainLoss: 0.662109375\n",
      "cnt: 0 - valLoss: 0.6632120609283447 - trainLoss: 0.6621087193489075\n",
      "cnt: 0 - valLoss: 0.663210928440094 - trainLoss: 0.6621079444885254\n",
      "cnt: 0 - valLoss: 0.6632099151611328 - trainLoss: 0.6621072292327881\n",
      "cnt: 0 - valLoss: 0.6632088422775269 - trainLoss: 0.662106454372406\n",
      "cnt: 0 - valLoss: 0.6632078289985657 - trainLoss: 0.6621057987213135\n",
      "cnt: 0 - valLoss: 0.6632066965103149 - trainLoss: 0.6621050238609314\n",
      "cnt: 0 - valLoss: 0.6632056832313538 - trainLoss: 0.6621043086051941\n",
      "cnt: 0 - valLoss: 0.663204550743103 - trainLoss: 0.662103533744812\n",
      "cnt: 0 - valLoss: 0.6632035374641418 - trainLoss: 0.6621028184890747\n",
      "cnt: 0 - valLoss: 0.6632025241851807 - trainLoss: 0.6621021032333374\n",
      "cnt: 0 - valLoss: 0.6632013916969299 - trainLoss: 0.6621013879776001\n",
      "cnt: 0 - valLoss: 0.6632004380226135 - trainLoss: 0.6621006727218628\n",
      "cnt: 0 - valLoss: 0.663199245929718 - trainLoss: 0.6620999574661255\n",
      "cnt: 0 - valLoss: 0.6631982326507568 - trainLoss: 0.6620991826057434\n",
      "cnt: 0 - valLoss: 0.6631971001625061 - trainLoss: 0.6620985269546509\n",
      "cnt: 0 - valLoss: 0.6631960868835449 - trainLoss: 0.6620977520942688\n",
      "cnt: 0 - valLoss: 0.6631951332092285 - trainLoss: 0.6620969772338867\n",
      "cnt: 0 - valLoss: 0.6631940603256226 - trainLoss: 0.6620962619781494\n",
      "cnt: 0 - valLoss: 0.6631929874420166 - trainLoss: 0.6620955467224121\n",
      "cnt: 0 - valLoss: 0.6631917953491211 - trainLoss: 0.6620948314666748\n",
      "cnt: 0 - valLoss: 0.6631908416748047 - trainLoss: 0.6620941162109375\n",
      "cnt: 0 - valLoss: 0.663189709186554 - trainLoss: 0.6620933413505554\n",
      "cnt: 0 - valLoss: 0.6631886959075928 - trainLoss: 0.6620926260948181\n",
      "cnt: 0 - valLoss: 0.6631875038146973 - trainLoss: 0.6620919108390808\n",
      "cnt: 0 - valLoss: 0.6631865501403809 - trainLoss: 0.6620911955833435\n",
      "cnt: 0 - valLoss: 0.6631854772567749 - trainLoss: 0.6620904803276062\n",
      "cnt: 0 - valLoss: 0.6631843447685242 - trainLoss: 0.6620897054672241\n",
      "cnt: 0 - valLoss: 0.6631833910942078 - trainLoss: 0.662088930606842\n",
      "cnt: 0 - valLoss: 0.6631821990013123 - trainLoss: 0.6620882153511047\n",
      "cnt: 0 - valLoss: 0.6631812453269958 - trainLoss: 0.6620875597000122\n",
      "cnt: 0 - valLoss: 0.6631800532341003 - trainLoss: 0.6620867848396301\n",
      "cnt: 0 - valLoss: 0.6631790995597839 - trainLoss: 0.6620860695838928\n",
      "cnt: 0 - valLoss: 0.663178026676178 - trainLoss: 0.6620852947235107\n",
      "cnt: 0 - valLoss: 0.663176953792572 - trainLoss: 0.6620845794677734\n",
      "cnt: 0 - valLoss: 0.6631759405136108 - trainLoss: 0.6620838642120361\n",
      "cnt: 0 - valLoss: 0.6631747484207153 - trainLoss: 0.6620831489562988\n",
      "cnt: 0 - valLoss: 0.6631737947463989 - trainLoss: 0.6620824337005615\n",
      "cnt: 0 - valLoss: 0.6631726026535034 - trainLoss: 0.6620816588401794\n",
      "cnt: 0 - valLoss: 0.6631715893745422 - trainLoss: 0.6620808839797974\n",
      "cnt: 0 - valLoss: 0.6631704568862915 - trainLoss: 0.6620801687240601\n",
      "cnt: 0 - valLoss: 0.6631694436073303 - trainLoss: 0.662079393863678\n",
      "cnt: 0 - valLoss: 0.6631682515144348 - trainLoss: 0.6620787382125854\n",
      "cnt: 0 - valLoss: 0.6631672978401184 - trainLoss: 0.6620779633522034\n",
      "cnt: 0 - valLoss: 0.6631662845611572 - trainLoss: 0.6620771884918213\n",
      "cnt: 0 - valLoss: 0.6631650924682617 - trainLoss: 0.6620764136314392\n",
      "cnt: 0 - valLoss: 0.6631641387939453 - trainLoss: 0.6620757579803467\n",
      "cnt: 0 - valLoss: 0.6631629467010498 - trainLoss: 0.6620749831199646\n",
      "cnt: 0 - valLoss: 0.6631619334220886 - trainLoss: 0.6620742678642273\n",
      "cnt: 0 - valLoss: 0.6631608009338379 - trainLoss: 0.6620734930038452\n",
      "cnt: 0 - valLoss: 0.6631597280502319 - trainLoss: 0.6620727181434631\n",
      "cnt: 0 - valLoss: 0.6631585955619812 - trainLoss: 0.6620720624923706\n",
      "cnt: 0 - valLoss: 0.66315758228302 - trainLoss: 0.6620713472366333\n",
      "cnt: 0 - valLoss: 0.6631565690040588 - trainLoss: 0.6620705723762512\n",
      "cnt: 0 - valLoss: 0.6631554365158081 - trainLoss: 0.6620697975158691\n",
      "cnt: 0 - valLoss: 0.6631544232368469 - trainLoss: 0.6620690822601318\n",
      "cnt: 0 - valLoss: 0.6631532311439514 - trainLoss: 0.6620683670043945\n",
      "cnt: 0 - valLoss: 0.6631522178649902 - trainLoss: 0.6620675325393677\n",
      "cnt: 0 - valLoss: 0.6631510853767395 - trainLoss: 0.6620668172836304\n",
      "cnt: 0 - valLoss: 0.6631500720977783 - trainLoss: 0.6620661616325378\n",
      "cnt: 0 - valLoss: 0.6631489396095276 - trainLoss: 0.6620653867721558\n",
      "cnt: 0 - valLoss: 0.6631479263305664 - trainLoss: 0.6620646119117737\n",
      "cnt: 0 - valLoss: 0.6631467342376709 - trainLoss: 0.6620638966560364\n",
      "cnt: 0 - valLoss: 0.6631457209587097 - trainLoss: 0.6620631217956543\n",
      "cnt: 0 - valLoss: 0.6631445288658142 - trainLoss: 0.662062406539917\n",
      "cnt: 0 - valLoss: 0.663143515586853 - trainLoss: 0.6620616316795349\n",
      "cnt: 0 - valLoss: 0.6631424427032471 - trainLoss: 0.6620609164237976\n",
      "cnt: 0 - valLoss: 0.6631414294242859 - trainLoss: 0.6620601415634155\n",
      "cnt: 0 - valLoss: 0.6631403565406799 - trainLoss: 0.6620594263076782\n",
      "cnt: 0 - valLoss: 0.6631391644477844 - trainLoss: 0.6620585918426514\n",
      "cnt: 0 - valLoss: 0.663138210773468 - trainLoss: 0.6620579361915588\n",
      "cnt: 0 - valLoss: 0.6631370186805725 - trainLoss: 0.6620572209358215\n",
      "cnt: 0 - valLoss: 0.6631360054016113 - trainLoss: 0.6620563864707947\n",
      "cnt: 0 - valLoss: 0.6631348133087158 - trainLoss: 0.6620557308197021\n",
      "cnt: 0 - valLoss: 0.6631338000297546 - trainLoss: 0.6620548963546753\n",
      "cnt: 0 - valLoss: 0.6631326675415039 - trainLoss: 0.662054181098938\n",
      "cnt: 0 - valLoss: 0.6631316542625427 - trainLoss: 0.6620534062385559\n",
      "cnt: 0 - valLoss: 0.663130521774292 - trainLoss: 0.6620526909828186\n",
      "cnt: 0 - valLoss: 0.6631293892860413 - trainLoss: 0.6620519161224365\n",
      "cnt: 0 - valLoss: 0.6631282567977905 - trainLoss: 0.6620511412620544\n",
      "cnt: 0 - valLoss: 0.6631272435188293 - trainLoss: 0.6620503664016724\n",
      "cnt: 0 - valLoss: 0.6631261110305786 - trainLoss: 0.6620496511459351\n",
      "cnt: 0 - valLoss: 0.6631250977516174 - trainLoss: 0.662048876285553\n",
      "cnt: 0 - valLoss: 0.6631239056587219 - trainLoss: 0.6620481014251709\n",
      "cnt: 0 - valLoss: 0.6631229519844055 - trainLoss: 0.6620474457740784\n",
      "cnt: 0 - valLoss: 0.6631217002868652 - trainLoss: 0.6620467305183411\n",
      "cnt: 0 - valLoss: 0.6631206274032593 - trainLoss: 0.662045955657959\n",
      "cnt: 0 - valLoss: 0.6631195545196533 - trainLoss: 0.6620451807975769\n",
      "cnt: 0 - valLoss: 0.6631185412406921 - trainLoss: 0.6620444059371948\n",
      "cnt: 0 - valLoss: 0.6631172895431519 - trainLoss: 0.6620436906814575\n",
      "cnt: 0 - valLoss: 0.6631162762641907 - trainLoss: 0.6620429158210754\n",
      "cnt: 0 - valLoss: 0.6631151437759399 - trainLoss: 0.6620421409606934\n",
      "cnt: 0 - valLoss: 0.6631141304969788 - trainLoss: 0.662041425704956\n",
      "cnt: 0 - valLoss: 0.6631129384040833 - trainLoss: 0.662040650844574\n",
      "cnt: 0 - valLoss: 0.6631119251251221 - trainLoss: 0.6620399355888367\n",
      "cnt: 0 - valLoss: 0.6631107330322266 - trainLoss: 0.6620391011238098\n",
      "cnt: 0 - valLoss: 0.6631097197532654 - trainLoss: 0.6620383858680725\n",
      "cnt: 0 - valLoss: 0.6631085276603699 - trainLoss: 0.6620376110076904\n",
      "cnt: 0 - valLoss: 0.6631075143814087 - trainLoss: 0.6620368957519531\n",
      "cnt: 0 - valLoss: 0.6631063222885132 - trainLoss: 0.662036120891571\n",
      "cnt: 0 - valLoss: 0.6631052494049072 - trainLoss: 0.662035346031189\n",
      "cnt: 0 - valLoss: 0.6631041169166565 - trainLoss: 0.6620346307754517\n",
      "cnt: 0 - valLoss: 0.6631031036376953 - trainLoss: 0.6620338559150696\n",
      "cnt: 0 - valLoss: 0.6631020307540894 - trainLoss: 0.6620330810546875\n",
      "cnt: 0 - valLoss: 0.6631009578704834 - trainLoss: 0.6620323061943054\n",
      "cnt: 0 - valLoss: 0.6630998849868774 - trainLoss: 0.6620315909385681\n",
      "cnt: 0 - valLoss: 0.6630986928939819 - trainLoss: 0.662030816078186\n",
      "cnt: 0 - valLoss: 0.663097620010376 - trainLoss: 0.662030041217804\n",
      "cnt: 0 - valLoss: 0.6630964279174805 - trainLoss: 0.6620292663574219\n",
      "cnt: 0 - valLoss: 0.6630954742431641 - trainLoss: 0.6620285511016846\n",
      "cnt: 0 - valLoss: 0.6630942225456238 - trainLoss: 0.6620277762413025\n",
      "cnt: 0 - valLoss: 0.6630932092666626 - trainLoss: 0.6620270013809204\n",
      "cnt: 0 - valLoss: 0.6630920767784119 - trainLoss: 0.6620262265205383\n",
      "cnt: 0 - valLoss: 0.6630910038948059 - trainLoss: 0.662025511264801\n",
      "cnt: 0 - valLoss: 0.6630898118019104 - trainLoss: 0.662024736404419\n",
      "cnt: 0 - valLoss: 0.6630887985229492 - trainLoss: 0.6620239615440369\n",
      "cnt: 0 - valLoss: 0.6630875468254089 - trainLoss: 0.6620231866836548\n",
      "cnt: 0 - valLoss: 0.6630865931510925 - trainLoss: 0.6620224714279175\n",
      "cnt: 0 - valLoss: 0.663085401058197 - trainLoss: 0.6620216965675354\n",
      "cnt: 0 - valLoss: 0.6630843877792358 - trainLoss: 0.6620209217071533\n",
      "cnt: 0 - valLoss: 0.6630831956863403 - trainLoss: 0.6620201468467712\n",
      "cnt: 0 - valLoss: 0.6630821228027344 - trainLoss: 0.6620194315910339\n",
      "cnt: 0 - valLoss: 0.6630809903144836 - trainLoss: 0.6620186567306519\n",
      "cnt: 0 - valLoss: 0.6630799174308777 - trainLoss: 0.6620178818702698\n",
      "cnt: 0 - valLoss: 0.6630787253379822 - trainLoss: 0.6620171070098877\n",
      "cnt: 0 - valLoss: 0.663077712059021 - trainLoss: 0.6620163917541504\n",
      "cnt: 0 - valLoss: 0.6630765199661255 - trainLoss: 0.6620155572891235\n",
      "cnt: 0 - valLoss: 0.6630755066871643 - trainLoss: 0.6620147824287415\n",
      "cnt: 0 - valLoss: 0.663074254989624 - trainLoss: 0.6620140671730042\n",
      "cnt: 0 - valLoss: 0.6630732417106628 - trainLoss: 0.6620133519172668\n",
      "cnt: 0 - valLoss: 0.6630720496177673 - trainLoss: 0.66201251745224\n",
      "cnt: 0 - valLoss: 0.6630709767341614 - trainLoss: 0.6620118021965027\n",
      "cnt: 0 - valLoss: 0.6630698442459106 - trainLoss: 0.6620110273361206\n",
      "cnt: 0 - valLoss: 0.6630687713623047 - trainLoss: 0.6620102524757385\n",
      "cnt: 0 - valLoss: 0.6630675792694092 - trainLoss: 0.6620094776153564\n",
      "cnt: 0 - valLoss: 0.663066565990448 - trainLoss: 0.6620087027549744\n",
      "cnt: 0 - valLoss: 0.6630653738975525 - trainLoss: 0.6620078682899475\n",
      "cnt: 0 - valLoss: 0.6630643010139465 - trainLoss: 0.6620071530342102\n",
      "cnt: 0 - valLoss: 0.663063108921051 - trainLoss: 0.6620063781738281\n",
      "cnt: 0 - valLoss: 0.6630620956420898 - trainLoss: 0.662005603313446\n",
      "cnt: 0 - valLoss: 0.6630608439445496 - trainLoss: 0.662004828453064\n",
      "cnt: 0 - valLoss: 0.6630598902702332 - trainLoss: 0.6620040535926819\n",
      "cnt: 0 - valLoss: 0.6630586385726929 - trainLoss: 0.6620033383369446\n",
      "cnt: 0 - valLoss: 0.6630575656890869 - trainLoss: 0.6620025038719177\n",
      "cnt: 0 - valLoss: 0.6630563735961914 - trainLoss: 0.6620017886161804\n",
      "cnt: 0 - valLoss: 0.6630553603172302 - trainLoss: 0.6620010137557983\n",
      "cnt: 0 - valLoss: 0.6630541086196899 - trainLoss: 0.6620002388954163\n",
      "cnt: 0 - valLoss: 0.6630530953407288 - trainLoss: 0.6619994640350342\n",
      "cnt: 0 - valLoss: 0.6630519032478333 - trainLoss: 0.6619986891746521\n",
      "cnt: 0 - valLoss: 0.6630507707595825 - trainLoss: 0.66199791431427\n",
      "cnt: 0 - valLoss: 0.663049578666687 - trainLoss: 0.6619971394538879\n",
      "cnt: 0 - valLoss: 0.6630485653877258 - trainLoss: 0.6619963645935059\n",
      "cnt: 0 - valLoss: 0.6630474328994751 - trainLoss: 0.6619956493377686\n",
      "cnt: 0 - valLoss: 0.6630462408065796 - trainLoss: 0.6619949340820312\n",
      "cnt: 0 - valLoss: 0.6630451679229736 - trainLoss: 0.6619940996170044\n",
      "cnt: 0 - valLoss: 0.6630440354347229 - trainLoss: 0.6619933843612671\n",
      "cnt: 0 - valLoss: 0.6630428433418274 - trainLoss: 0.661992609500885\n",
      "cnt: 0 - valLoss: 0.6630417704582214 - trainLoss: 0.6619918942451477\n",
      "cnt: 0 - valLoss: 0.6630407571792603 - trainLoss: 0.6619910597801208\n",
      "cnt: 0 - valLoss: 0.66303950548172 - trainLoss: 0.6619903445243835\n",
      "cnt: 0 - valLoss: 0.663038432598114 - trainLoss: 0.6619896292686462\n",
      "cnt: 0 - valLoss: 0.6630374193191528 - trainLoss: 0.6619888544082642\n",
      "cnt: 0 - valLoss: 0.6630362868309021 - trainLoss: 0.6619880199432373\n",
      "cnt: 0 - valLoss: 0.6630350351333618 - trainLoss: 0.6619873046875\n",
      "cnt: 0 - valLoss: 0.6630340218544006 - trainLoss: 0.6619864702224731\n",
      "cnt: 0 - valLoss: 0.6630328893661499 - trainLoss: 0.6619857549667358\n",
      "cnt: 0 - valLoss: 0.6630316972732544 - trainLoss: 0.6619849801063538\n",
      "cnt: 0 - valLoss: 0.6630306243896484 - trainLoss: 0.6619842648506165\n",
      "cnt: 0 - valLoss: 0.6630295515060425 - trainLoss: 0.6619834303855896\n",
      "cnt: 0 - valLoss: 0.6630282998085022 - trainLoss: 0.6619827151298523\n",
      "cnt: 0 - valLoss: 0.6630272269248962 - trainLoss: 0.6619819402694702\n",
      "cnt: 0 - valLoss: 0.6630261540412903 - trainLoss: 0.6619811654090881\n",
      "cnt: 0 - valLoss: 0.66302490234375 - trainLoss: 0.661980390548706\n",
      "cnt: 0 - valLoss: 0.6630238890647888 - trainLoss: 0.6619796752929688\n",
      "cnt: 0 - valLoss: 0.6630227565765381 - trainLoss: 0.6619789004325867\n",
      "cnt: 0 - valLoss: 0.6630215644836426 - trainLoss: 0.6619781255722046\n",
      "cnt: 0 - valLoss: 0.6630204319953918 - trainLoss: 0.6619773507118225\n",
      "cnt: 0 - valLoss: 0.6630193591117859 - trainLoss: 0.6619765758514404\n",
      "cnt: 0 - valLoss: 0.6630182862281799 - trainLoss: 0.6619758009910583\n",
      "cnt: 0 - valLoss: 0.6630170941352844 - trainLoss: 0.661975085735321\n",
      "cnt: 0 - valLoss: 0.6630159616470337 - trainLoss: 0.6619742512702942\n",
      "cnt: 0 - valLoss: 0.6630148887634277 - trainLoss: 0.6619734764099121\n",
      "cnt: 0 - valLoss: 0.6630136370658875 - trainLoss: 0.6619727611541748\n",
      "cnt: 0 - valLoss: 0.6630125641822815 - trainLoss: 0.6619719862937927\n",
      "cnt: 0 - valLoss: 0.6630114912986755 - trainLoss: 0.6619712710380554\n",
      "cnt: 0 - valLoss: 0.6630102396011353 - trainLoss: 0.6619704365730286\n",
      "cnt: 0 - valLoss: 0.6630091667175293 - trainLoss: 0.6619696617126465\n",
      "cnt: 0 - valLoss: 0.6630080938339233 - trainLoss: 0.6619688868522644\n",
      "cnt: 0 - valLoss: 0.6630068421363831 - trainLoss: 0.6619681119918823\n",
      "cnt: 0 - valLoss: 0.6630057692527771 - trainLoss: 0.6619673371315002\n",
      "cnt: 0 - valLoss: 0.6630046963691711 - trainLoss: 0.6619665622711182\n",
      "cnt: 0 - valLoss: 0.6630034446716309 - trainLoss: 0.6619657874107361\n",
      "cnt: 0 - valLoss: 0.6630023717880249 - trainLoss: 0.6619650721549988\n",
      "cnt: 0 - valLoss: 0.6630012392997742 - trainLoss: 0.6619642972946167\n",
      "cnt: 0 - valLoss: 0.6630000472068787 - trainLoss: 0.6619635224342346\n",
      "cnt: 0 - valLoss: 0.6629989147186279 - trainLoss: 0.6619627475738525\n",
      "cnt: 0 - valLoss: 0.662997841835022 - trainLoss: 0.6619619727134705\n",
      "cnt: 0 - valLoss: 0.6629965901374817 - trainLoss: 0.6619611978530884\n",
      "cnt: 0 - valLoss: 0.6629955172538757 - trainLoss: 0.6619604229927063\n",
      "cnt: 0 - valLoss: 0.6629944443702698 - trainLoss: 0.6619596481323242\n",
      "cnt: 0 - valLoss: 0.6629933714866638 - trainLoss: 0.6619588732719421\n",
      "cnt: 0 - valLoss: 0.6629920601844788 - trainLoss: 0.6619580984115601\n",
      "cnt: 0 - valLoss: 0.6629909873008728 - trainLoss: 0.661957323551178\n",
      "cnt: 0 - valLoss: 0.6629898548126221 - trainLoss: 0.6619565486907959\n",
      "cnt: 0 - valLoss: 0.6629886627197266 - trainLoss: 0.6619557738304138\n",
      "cnt: 0 - valLoss: 0.6629875898361206 - trainLoss: 0.6619549989700317\n",
      "cnt: 0 - valLoss: 0.6629865169525146 - trainLoss: 0.6619542241096497\n",
      "cnt: 0 - valLoss: 0.6629852652549744 - trainLoss: 0.6619535088539124\n",
      "cnt: 0 - valLoss: 0.6629840731620789 - trainLoss: 0.6619526743888855\n",
      "cnt: 0 - valLoss: 0.6629830598831177 - trainLoss: 0.6619518399238586\n",
      "cnt: 0 - valLoss: 0.6629818081855774 - trainLoss: 0.6619511246681213\n",
      "cnt: 0 - valLoss: 0.6629806756973267 - trainLoss: 0.6619503498077393\n",
      "cnt: 0 - valLoss: 0.6629796624183655 - trainLoss: 0.6619495749473572\n",
      "cnt: 0 - valLoss: 0.6629783511161804 - trainLoss: 0.6619488000869751\n",
      "cnt: 0 - valLoss: 0.6629772186279297 - trainLoss: 0.6619480848312378\n",
      "cnt: 0 - valLoss: 0.6629761457443237 - trainLoss: 0.6619472503662109\n",
      "cnt: 0 - valLoss: 0.6629749536514282 - trainLoss: 0.6619464755058289\n",
      "cnt: 0 - valLoss: 0.6629738211631775 - trainLoss: 0.661945641040802\n",
      "cnt: 0 - valLoss: 0.6629726886749268 - trainLoss: 0.6619448661804199\n",
      "cnt: 0 - valLoss: 0.6629714369773865 - trainLoss: 0.6619441509246826\n",
      "cnt: 0 - valLoss: 0.6629703640937805 - trainLoss: 0.6619433164596558\n",
      "cnt: 0 - valLoss: 0.6629692912101746 - trainLoss: 0.6619425415992737\n",
      "cnt: 0 - valLoss: 0.6629679799079895 - trainLoss: 0.6619417667388916\n",
      "cnt: 0 - valLoss: 0.6629669070243835 - trainLoss: 0.6619410514831543\n",
      "cnt: 0 - valLoss: 0.6629657745361328 - trainLoss: 0.6619401574134827\n",
      "cnt: 0 - valLoss: 0.6629645824432373 - trainLoss: 0.6619394421577454\n",
      "cnt: 0 - valLoss: 0.6629633903503418 - trainLoss: 0.6619386672973633\n",
      "cnt: 0 - valLoss: 0.6629624366760254 - trainLoss: 0.6619378328323364\n",
      "cnt: 0 - valLoss: 0.6629611253738403 - trainLoss: 0.6619371175765991\n",
      "cnt: 0 - valLoss: 0.6629599928855896 - trainLoss: 0.6619362831115723\n",
      "cnt: 0 - valLoss: 0.6629589200019836 - trainLoss: 0.6619355082511902\n",
      "cnt: 0 - valLoss: 0.6629576086997986 - trainLoss: 0.6619347333908081\n",
      "cnt: 0 - valLoss: 0.6629565358161926 - trainLoss: 0.661933958530426\n",
      "cnt: 0 - valLoss: 0.6629554033279419 - trainLoss: 0.6619331240653992\n",
      "cnt: 0 - valLoss: 0.6629541516304016 - trainLoss: 0.6619324088096619\n",
      "cnt: 0 - valLoss: 0.6629530787467957 - trainLoss: 0.661931574344635\n",
      "cnt: 0 - valLoss: 0.6629518866539001 - trainLoss: 0.6619307994842529\n",
      "cnt: 0 - valLoss: 0.6629507541656494 - trainLoss: 0.6619300842285156\n",
      "cnt: 0 - valLoss: 0.6629495620727539 - trainLoss: 0.6619293093681335\n",
      "cnt: 0 - valLoss: 0.6629484295845032 - trainLoss: 0.6619285345077515\n",
      "cnt: 0 - valLoss: 0.6629472374916077 - trainLoss: 0.6619277596473694\n",
      "cnt: 0 - valLoss: 0.6629461050033569 - trainLoss: 0.6619269847869873\n",
      "cnt: 0 - valLoss: 0.6629449129104614 - trainLoss: 0.66192626953125\n",
      "cnt: 0 - valLoss: 0.6629437208175659 - trainLoss: 0.6619255542755127\n",
      "cnt: 0 - valLoss: 0.6629425883293152 - trainLoss: 0.6619247794151306\n",
      "cnt: 0 - valLoss: 0.6629413962364197 - trainLoss: 0.6619240045547485\n",
      "cnt: 0 - valLoss: 0.6629402041435242 - trainLoss: 0.6619232892990112\n",
      "cnt: 0 - valLoss: 0.6629390716552734 - trainLoss: 0.6619225144386292\n",
      "cnt: 0 - valLoss: 0.6629379391670227 - trainLoss: 0.6619217395782471\n",
      "cnt: 0 - valLoss: 0.6629366874694824 - trainLoss: 0.6619210243225098\n",
      "cnt: 0 - valLoss: 0.6629355549812317 - trainLoss: 0.6619202494621277\n",
      "cnt: 0 - valLoss: 0.6629343628883362 - trainLoss: 0.6619194746017456\n",
      "cnt: 0 - valLoss: 0.6629331707954407 - trainLoss: 0.6619186997413635\n",
      "cnt: 0 - valLoss: 0.6629319787025452 - trainLoss: 0.6619179248809814\n",
      "cnt: 0 - valLoss: 0.6629307866096497 - trainLoss: 0.6619171500205994\n",
      "cnt: 0 - valLoss: 0.6629295945167542 - trainLoss: 0.6619163155555725\n",
      "cnt: 0 - valLoss: 0.6629284620285034 - trainLoss: 0.66191565990448\n",
      "cnt: 0 - valLoss: 0.6629272699356079 - trainLoss: 0.6619148850440979\n",
      "cnt: 0 - valLoss: 0.6629260778427124 - trainLoss: 0.6619141101837158\n",
      "cnt: 0 - valLoss: 0.6629248857498169 - trainLoss: 0.6619133353233337\n",
      "cnt: 0 - valLoss: 0.6629237532615662 - trainLoss: 0.6619125604629517\n",
      "cnt: 0 - valLoss: 0.6629225015640259 - trainLoss: 0.6619117856025696\n",
      "cnt: 0 - valLoss: 0.6629213690757751 - trainLoss: 0.6619110107421875\n",
      "cnt: 0 - valLoss: 0.6629201769828796 - trainLoss: 0.6619102358818054\n",
      "cnt: 0 - valLoss: 0.6629189848899841 - trainLoss: 0.6619095206260681\n",
      "cnt: 0 - valLoss: 0.6629177331924438 - trainLoss: 0.661908745765686\n",
      "cnt: 0 - valLoss: 0.6629166007041931 - trainLoss: 0.661907970905304\n",
      "cnt: 0 - valLoss: 0.6629154086112976 - trainLoss: 0.6619071960449219\n",
      "cnt: 0 - valLoss: 0.6629142165184021 - trainLoss: 0.6619064807891846\n",
      "cnt: 0 - valLoss: 0.6629130244255066 - trainLoss: 0.6619057059288025\n",
      "cnt: 0 - valLoss: 0.6629118323326111 - trainLoss: 0.6619049310684204\n",
      "cnt: 0 - valLoss: 0.6629106402397156 - trainLoss: 0.6619041562080383\n",
      "cnt: 0 - valLoss: 0.6629094481468201 - trainLoss: 0.6619033813476562\n",
      "cnt: 0 - valLoss: 0.6629082560539246 - trainLoss: 0.661902666091919\n",
      "cnt: 0 - valLoss: 0.662907063961029 - trainLoss: 0.6619018912315369\n",
      "cnt: 0 - valLoss: 0.6629059314727783 - trainLoss: 0.66190105676651\n",
      "cnt: 0 - valLoss: 0.6629047393798828 - trainLoss: 0.6619002819061279\n",
      "cnt: 0 - valLoss: 0.6629034876823425 - trainLoss: 0.6618995070457458\n",
      "cnt: 0 - valLoss: 0.662902295589447 - trainLoss: 0.6618987917900085\n",
      "cnt: 0 - valLoss: 0.6629011034965515 - trainLoss: 0.6618980169296265\n",
      "cnt: 0 - valLoss: 0.662899911403656 - trainLoss: 0.6618972420692444\n",
      "cnt: 0 - valLoss: 0.6628987193107605 - trainLoss: 0.6618964672088623\n",
      "cnt: 0 - valLoss: 0.6628974676132202 - trainLoss: 0.6618956923484802\n",
      "cnt: 0 - valLoss: 0.6628963351249695 - trainLoss: 0.6618949174880981\n",
      "cnt: 0 - valLoss: 0.662895143032074 - trainLoss: 0.6618941426277161\n",
      "cnt: 0 - valLoss: 0.6628938913345337 - trainLoss: 0.6618934273719788\n",
      "cnt: 0 - valLoss: 0.6628926992416382 - trainLoss: 0.6618926525115967\n",
      "cnt: 0 - valLoss: 0.6628915667533875 - trainLoss: 0.6618918776512146\n",
      "cnt: 0 - valLoss: 0.6628903150558472 - trainLoss: 0.6618910431861877\n",
      "cnt: 0 - valLoss: 0.6628891229629517 - trainLoss: 0.6618902087211609\n",
      "cnt: 0 - valLoss: 0.6628878712654114 - trainLoss: 0.6618894934654236\n",
      "cnt: 0 - valLoss: 0.6628867387771606 - trainLoss: 0.6618887186050415\n",
      "cnt: 0 - valLoss: 0.6628854870796204 - trainLoss: 0.6618879437446594\n",
      "cnt: 0 - valLoss: 0.6628842949867249 - trainLoss: 0.6618872284889221\n",
      "cnt: 0 - valLoss: 0.6628830432891846 - trainLoss: 0.6618863940238953\n",
      "cnt: 0 - valLoss: 0.6628818511962891 - trainLoss: 0.6618856191635132\n",
      "cnt: 0 - valLoss: 0.6628807187080383 - trainLoss: 0.6618849039077759\n",
      "cnt: 0 - valLoss: 0.6628794074058533 - trainLoss: 0.6618841290473938\n",
      "cnt: 0 - valLoss: 0.6628782749176025 - trainLoss: 0.6618832945823669\n",
      "cnt: 0 - valLoss: 0.6628770232200623 - trainLoss: 0.6618825197219849\n",
      "cnt: 0 - valLoss: 0.6628758311271667 - trainLoss: 0.6618817448616028\n",
      "cnt: 0 - valLoss: 0.6628746390342712 - trainLoss: 0.6618809700012207\n",
      "cnt: 0 - valLoss: 0.662873387336731 - trainLoss: 0.6618802547454834\n",
      "cnt: 0 - valLoss: 0.6628721952438354 - trainLoss: 0.6618794202804565\n",
      "cnt: 0 - valLoss: 0.6628710031509399 - trainLoss: 0.6618786454200745\n",
      "cnt: 0 - valLoss: 0.6628697514533997 - trainLoss: 0.6618779301643372\n",
      "cnt: 0 - valLoss: 0.6628685593605042 - trainLoss: 0.6618770956993103\n",
      "cnt: 0 - valLoss: 0.6628673672676086 - trainLoss: 0.6618763208389282\n",
      "cnt: 0 - valLoss: 0.6628661155700684 - trainLoss: 0.6618754863739014\n",
      "cnt: 0 - valLoss: 0.6628649234771729 - trainLoss: 0.6618747711181641\n",
      "cnt: 0 - valLoss: 0.6628636717796326 - trainLoss: 0.661873996257782\n",
      "cnt: 0 - valLoss: 0.6628624796867371 - trainLoss: 0.6618732213973999\n",
      "cnt: 0 - valLoss: 0.6628612875938416 - trainLoss: 0.6618724465370178\n",
      "cnt: 0 - valLoss: 0.6628600358963013 - trainLoss: 0.6618716716766357\n",
      "cnt: 0 - valLoss: 0.6628589034080505 - trainLoss: 0.6618708968162537\n",
      "cnt: 0 - valLoss: 0.6628576517105103 - trainLoss: 0.661870002746582\n",
      "cnt: 0 - valLoss: 0.6628564596176147 - trainLoss: 0.6618693470954895\n",
      "cnt: 0 - valLoss: 0.6628552079200745 - trainLoss: 0.6618685126304626\n",
      "cnt: 0 - valLoss: 0.6628539562225342 - trainLoss: 0.6618677377700806\n",
      "cnt: 0 - valLoss: 0.6628527641296387 - trainLoss: 0.6618669033050537\n",
      "cnt: 0 - valLoss: 0.6628515720367432 - trainLoss: 0.6618661284446716\n",
      "cnt: 0 - valLoss: 0.6628503799438477 - trainLoss: 0.6618653535842896\n",
      "cnt: 0 - valLoss: 0.6628490686416626 - trainLoss: 0.6618645787239075\n",
      "cnt: 0 - valLoss: 0.6628478765487671 - trainLoss: 0.6618638038635254\n",
      "cnt: 0 - valLoss: 0.6628466248512268 - trainLoss: 0.6618630886077881\n",
      "cnt: 0 - valLoss: 0.6628454327583313 - trainLoss: 0.6618622541427612\n",
      "cnt: 0 - valLoss: 0.662844181060791 - trainLoss: 0.6618614792823792\n",
      "cnt: 0 - valLoss: 0.6628431081771851 - trainLoss: 0.6618607044219971\n",
      "cnt: 0 - valLoss: 0.6628417372703552 - trainLoss: 0.6618598699569702\n",
      "cnt: 0 - valLoss: 0.6628405451774597 - trainLoss: 0.6618591547012329\n",
      "cnt: 0 - valLoss: 0.6628392934799194 - trainLoss: 0.661858320236206\n",
      "cnt: 0 - valLoss: 0.6628380417823792 - trainLoss: 0.661857545375824\n",
      "cnt: 0 - valLoss: 0.6628368496894836 - trainLoss: 0.6618567705154419\n",
      "cnt: 0 - valLoss: 0.6628356575965881 - trainLoss: 0.6618559956550598\n",
      "cnt: 0 - valLoss: 0.6628344058990479 - trainLoss: 0.6618552207946777\n",
      "cnt: 0 - valLoss: 0.6628331542015076 - trainLoss: 0.6618543863296509\n",
      "cnt: 0 - valLoss: 0.6628319621086121 - trainLoss: 0.6618536710739136\n",
      "cnt: 0 - valLoss: 0.6628307104110718 - trainLoss: 0.6618528366088867\n",
      "cnt: 0 - valLoss: 0.6628295183181763 - trainLoss: 0.6618520617485046\n",
      "cnt: 0 - valLoss: 0.662828266620636 - trainLoss: 0.6618512272834778\n",
      "cnt: 0 - valLoss: 0.6628270745277405 - trainLoss: 0.6618505120277405\n",
      "cnt: 0 - valLoss: 0.6628258228302002 - trainLoss: 0.6618496775627136\n",
      "cnt: 0 - valLoss: 0.6628245711326599 - trainLoss: 0.6618489027023315\n",
      "cnt: 0 - valLoss: 0.6628233194351196 - trainLoss: 0.6618481278419495\n",
      "cnt: 0 - valLoss: 0.6628221273422241 - trainLoss: 0.6618473529815674\n",
      "cnt: 0 - valLoss: 0.6628208756446838 - trainLoss: 0.6618466377258301\n",
      "cnt: 0 - valLoss: 0.6628196835517883 - trainLoss: 0.6618457436561584\n",
      "cnt: 0 - valLoss: 0.662818431854248 - trainLoss: 0.6618449687957764\n",
      "cnt: 0 - valLoss: 0.6628171801567078 - trainLoss: 0.6618441939353943\n",
      "cnt: 0 - valLoss: 0.6628159284591675 - trainLoss: 0.6618434190750122\n",
      "cnt: 0 - valLoss: 0.662814736366272 - trainLoss: 0.6618426442146301\n",
      "cnt: 0 - valLoss: 0.6628134846687317 - trainLoss: 0.6618417501449585\n",
      "cnt: 0 - valLoss: 0.6628122329711914 - trainLoss: 0.6618410348892212\n",
      "cnt: 0 - valLoss: 0.6628109812736511 - trainLoss: 0.6618402600288391\n",
      "cnt: 0 - valLoss: 0.6628097295761108 - trainLoss: 0.661839485168457\n",
      "cnt: 0 - valLoss: 0.6628085374832153 - trainLoss: 0.6618386507034302\n",
      "cnt: 0 - valLoss: 0.6628072261810303 - trainLoss: 0.6618378162384033\n",
      "cnt: 0 - valLoss: 0.6628060340881348 - trainLoss: 0.6618370413780212\n",
      "cnt: 0 - valLoss: 0.6628048419952393 - trainLoss: 0.6618362665176392\n",
      "cnt: 0 - valLoss: 0.662803590297699 - trainLoss: 0.6618355512619019\n",
      "cnt: 0 - valLoss: 0.6628022789955139 - trainLoss: 0.6618346571922302\n",
      "cnt: 0 - valLoss: 0.6628010869026184 - trainLoss: 0.6618339419364929\n",
      "cnt: 0 - valLoss: 0.6627998948097229 - trainLoss: 0.6618331074714661\n",
      "cnt: 0 - valLoss: 0.6627985835075378 - trainLoss: 0.6618322730064392\n",
      "cnt: 0 - valLoss: 0.6627973914146423 - trainLoss: 0.6618315577507019\n",
      "cnt: 0 - valLoss: 0.6627961993217468 - trainLoss: 0.661830723285675\n",
      "cnt: 0 - valLoss: 0.6627949476242065 - trainLoss: 0.661829948425293\n",
      "cnt: 0 - valLoss: 0.6627936959266663 - trainLoss: 0.6618291735649109\n",
      "cnt: 0 - valLoss: 0.662792444229126 - trainLoss: 0.661828339099884\n",
      "cnt: 0 - valLoss: 0.6627911925315857 - trainLoss: 0.661827564239502\n",
      "cnt: 0 - valLoss: 0.6627900004386902 - trainLoss: 0.6618267893791199\n",
      "cnt: 0 - valLoss: 0.6627887487411499 - trainLoss: 0.661825954914093\n",
      "cnt: 0 - valLoss: 0.6627875566482544 - trainLoss: 0.6618251800537109\n",
      "cnt: 0 - valLoss: 0.6627863049507141 - trainLoss: 0.6618244051933289\n",
      "cnt: 0 - valLoss: 0.6627851724624634 - trainLoss: 0.661823570728302\n",
      "cnt: 0 - valLoss: 0.6627839207649231 - trainLoss: 0.6618227958679199\n",
      "cnt: 0 - valLoss: 0.6627826690673828 - trainLoss: 0.6618220210075378\n",
      "cnt: 0 - valLoss: 0.6627814173698425 - trainLoss: 0.6618212461471558\n",
      "cnt: 0 - valLoss: 0.6627801060676575 - trainLoss: 0.6618203520774841\n",
      "cnt: 0 - valLoss: 0.6627788543701172 - trainLoss: 0.661819577217102\n",
      "cnt: 0 - valLoss: 0.6627776622772217 - trainLoss: 0.6618187427520752\n",
      "cnt: 0 - valLoss: 0.6627764105796814 - trainLoss: 0.6618179678916931\n",
      "cnt: 0 - valLoss: 0.6627752184867859 - trainLoss: 0.661817193031311\n",
      "cnt: 0 - valLoss: 0.6627740859985352 - trainLoss: 0.661816418170929\n",
      "cnt: 0 - valLoss: 0.6627727746963501 - trainLoss: 0.6618155837059021\n",
      "cnt: 0 - valLoss: 0.6627715229988098 - trainLoss: 0.6618147492408752\n",
      "cnt: 0 - valLoss: 0.6627702713012695 - trainLoss: 0.6618139743804932\n",
      "cnt: 0 - valLoss: 0.6627689599990845 - trainLoss: 0.6618132591247559\n",
      "cnt: 0 - valLoss: 0.6627678275108337 - trainLoss: 0.661812424659729\n",
      "cnt: 0 - valLoss: 0.6627664566040039 - trainLoss: 0.6618115901947021\n",
      "cnt: 0 - valLoss: 0.6627652645111084 - trainLoss: 0.6618107557296753\n",
      "cnt: 0 - valLoss: 0.6627640128135681 - trainLoss: 0.6618099808692932\n",
      "cnt: 0 - valLoss: 0.6627627611160278 - trainLoss: 0.6618091464042664\n",
      "cnt: 0 - valLoss: 0.6627615690231323 - trainLoss: 0.6618083715438843\n",
      "cnt: 0 - valLoss: 0.662760317325592 - trainLoss: 0.6618075966835022\n",
      "cnt: 0 - valLoss: 0.6627590656280518 - trainLoss: 0.6618067622184753\n",
      "cnt: 0 - valLoss: 0.6627578139305115 - trainLoss: 0.6618059873580933\n",
      "cnt: 0 - valLoss: 0.6627565622329712 - trainLoss: 0.6618052124977112\n",
      "cnt: 0 - valLoss: 0.6627553105354309 - trainLoss: 0.6618043184280396\n",
      "cnt: 0 - valLoss: 0.6627541184425354 - trainLoss: 0.6618036031723022\n",
      "cnt: 0 - valLoss: 0.6627527475357056 - trainLoss: 0.6618027687072754\n",
      "cnt: 0 - valLoss: 0.6627515554428101 - trainLoss: 0.6618019342422485\n",
      "cnt: 0 - valLoss: 0.6627503633499146 - trainLoss: 0.6618011593818665\n",
      "cnt: 0 - valLoss: 0.6627491116523743 - trainLoss: 0.6618003249168396\n",
      "cnt: 0 - valLoss: 0.662747859954834 - trainLoss: 0.6617995500564575\n",
      "cnt: 0 - valLoss: 0.6627465486526489 - trainLoss: 0.6617988348007202\n",
      "cnt: 0 - valLoss: 0.6627452969551086 - trainLoss: 0.6617979407310486\n",
      "cnt: 0 - valLoss: 0.6627440452575684 - trainLoss: 0.6617971658706665\n",
      "cnt: 0 - valLoss: 0.6627427935600281 - trainLoss: 0.6617963314056396\n",
      "cnt: 0 - valLoss: 0.6627415418624878 - trainLoss: 0.6617955565452576\n",
      "cnt: 0 - valLoss: 0.6627404093742371 - trainLoss: 0.6617947220802307\n",
      "cnt: 0 - valLoss: 0.6627391576766968 - trainLoss: 0.6617939472198486\n",
      "cnt: 0 - valLoss: 0.6627379059791565 - trainLoss: 0.6617931127548218\n",
      "cnt: 0 - valLoss: 0.6627365946769714 - trainLoss: 0.6617923378944397\n",
      "cnt: 0 - valLoss: 0.6627353429794312 - trainLoss: 0.6617915034294128\n",
      "cnt: 0 - valLoss: 0.6627340912818909 - trainLoss: 0.6617907881736755\n",
      "cnt: 0 - valLoss: 0.6627327799797058 - trainLoss: 0.6617898344993591\n",
      "cnt: 0 - valLoss: 0.6627315282821655 - trainLoss: 0.6617891192436218\n",
      "cnt: 0 - valLoss: 0.66273033618927 - trainLoss: 0.661788284778595\n",
      "cnt: 0 - valLoss: 0.6627290844917297 - trainLoss: 0.6617875099182129\n",
      "cnt: 0 - valLoss: 0.6627278923988342 - trainLoss: 0.661786675453186\n",
      "cnt: 0 - valLoss: 0.6627265810966492 - trainLoss: 0.661785900592804\n",
      "cnt: 0 - valLoss: 0.6627252697944641 - trainLoss: 0.6617851257324219\n",
      "cnt: 0 - valLoss: 0.6627240777015686 - trainLoss: 0.6617842316627502\n",
      "cnt: 0 - valLoss: 0.6627227067947388 - trainLoss: 0.6617834568023682\n",
      "cnt: 0 - valLoss: 0.6627214550971985 - trainLoss: 0.6617826223373413\n",
      "cnt: 0 - valLoss: 0.662720263004303 - trainLoss: 0.6617817878723145\n",
      "cnt: 0 - valLoss: 0.6627190113067627 - trainLoss: 0.6617810726165771\n",
      "cnt: 0 - valLoss: 0.6627178192138672 - trainLoss: 0.6617801785469055\n",
      "cnt: 0 - valLoss: 0.6627165079116821 - trainLoss: 0.6617794036865234\n",
      "cnt: 0 - valLoss: 0.6627151966094971 - trainLoss: 0.6617786288261414\n",
      "cnt: 0 - valLoss: 0.6627140045166016 - trainLoss: 0.6617777347564697\n",
      "cnt: 0 - valLoss: 0.6627126336097717 - trainLoss: 0.6617769598960876\n",
      "cnt: 0 - valLoss: 0.6627113819122314 - trainLoss: 0.6617761254310608\n",
      "cnt: 0 - valLoss: 0.6627100706100464 - trainLoss: 0.6617753505706787\n",
      "cnt: 0 - valLoss: 0.6627088189125061 - trainLoss: 0.6617745161056519\n",
      "cnt: 0 - valLoss: 0.6627076864242554 - trainLoss: 0.661773681640625\n",
      "cnt: 0 - valLoss: 0.6627063751220703 - trainLoss: 0.6617728471755981\n",
      "cnt: 0 - valLoss: 0.6627051830291748 - trainLoss: 0.6617720723152161\n",
      "cnt: 0 - valLoss: 0.6627038717269897 - trainLoss: 0.6617712378501892\n",
      "cnt: 0 - valLoss: 0.6627025604248047 - trainLoss: 0.6617704033851624\n",
      "cnt: 0 - valLoss: 0.6627013087272644 - trainLoss: 0.6617696285247803\n",
      "cnt: 0 - valLoss: 0.6626999974250793 - trainLoss: 0.6617687940597534\n",
      "cnt: 0 - valLoss: 0.6626987457275391 - trainLoss: 0.6617679595947266\n",
      "cnt: 0 - valLoss: 0.6626975536346436 - trainLoss: 0.6617671847343445\n",
      "cnt: 0 - valLoss: 0.6626963019371033 - trainLoss: 0.6617663502693176\n",
      "cnt: 0 - valLoss: 0.6626949906349182 - trainLoss: 0.6617655754089355\n",
      "cnt: 0 - valLoss: 0.6626937389373779 - trainLoss: 0.6617647409439087\n",
      "cnt: 0 - valLoss: 0.6626924276351929 - trainLoss: 0.6617639660835266\n",
      "cnt: 0 - valLoss: 0.6626911163330078 - trainLoss: 0.6617631316184998\n",
      "cnt: 0 - valLoss: 0.6626898646354675 - trainLoss: 0.6617622375488281\n",
      "cnt: 0 - valLoss: 0.6626886129379272 - trainLoss: 0.661761462688446\n",
      "cnt: 0 - valLoss: 0.6626873016357422 - trainLoss: 0.6617606282234192\n",
      "cnt: 0 - valLoss: 0.6626861095428467 - trainLoss: 0.6617598533630371\n",
      "cnt: 0 - valLoss: 0.6626848578453064 - trainLoss: 0.6617590188980103\n",
      "cnt: 0 - valLoss: 0.6626835465431213 - trainLoss: 0.6617581248283386\n",
      "cnt: 0 - valLoss: 0.662682294845581 - trainLoss: 0.6617573499679565\n",
      "cnt: 0 - valLoss: 0.6626809239387512 - trainLoss: 0.6617565751075745\n",
      "cnt: 0 - valLoss: 0.6626796722412109 - trainLoss: 0.6617557406425476\n",
      "cnt: 0 - valLoss: 0.6626784205436707 - trainLoss: 0.6617549061775208\n",
      "cnt: 0 - valLoss: 0.6626770496368408 - trainLoss: 0.6617540717124939\n",
      "cnt: 0 - valLoss: 0.6626757979393005 - trainLoss: 0.661753237247467\n",
      "cnt: 0 - valLoss: 0.662674605846405 - trainLoss: 0.661752462387085\n",
      "cnt: 0 - valLoss: 0.66267329454422 - trainLoss: 0.6617515683174133\n",
      "cnt: 0 - valLoss: 0.6626720428466797 - trainLoss: 0.6617507934570312\n",
      "cnt: 0 - valLoss: 0.6626706719398499 - trainLoss: 0.6617499589920044\n",
      "cnt: 0 - valLoss: 0.6626694202423096 - trainLoss: 0.6617491841316223\n",
      "cnt: 0 - valLoss: 0.6626682281494141 - trainLoss: 0.6617482900619507\n",
      "cnt: 0 - valLoss: 0.6626668572425842 - trainLoss: 0.6617475152015686\n",
      "cnt: 0 - valLoss: 0.6626655459403992 - trainLoss: 0.6617466807365417\n",
      "cnt: 0 - valLoss: 0.6626642942428589 - trainLoss: 0.6617458462715149\n",
      "cnt: 0 - valLoss: 0.6626631021499634 - trainLoss: 0.6617450714111328\n",
      "cnt: 0 - valLoss: 0.6626617908477783 - trainLoss: 0.6617441773414612\n",
      "cnt: 0 - valLoss: 0.6626604795455933 - trainLoss: 0.6617434024810791\n",
      "cnt: 0 - valLoss: 0.662659227848053 - trainLoss: 0.6617425680160522\n",
      "cnt: 0 - valLoss: 0.6626579165458679 - trainLoss: 0.6617417335510254\n",
      "cnt: 0 - valLoss: 0.6626566052436829 - trainLoss: 0.6617409586906433\n",
      "cnt: 0 - valLoss: 0.6626552939414978 - trainLoss: 0.6617400646209717\n",
      "cnt: 0 - valLoss: 0.6626539826393127 - trainLoss: 0.6617392301559448\n",
      "cnt: 0 - valLoss: 0.6626526713371277 - trainLoss: 0.661738395690918\n",
      "cnt: 0 - valLoss: 0.662651538848877 - trainLoss: 0.6617376208305359\n",
      "cnt: 0 - valLoss: 0.6626502275466919 - trainLoss: 0.661736786365509\n",
      "cnt: 0 - valLoss: 0.6626489162445068 - trainLoss: 0.6617359519004822\n",
      "cnt: 0 - valLoss: 0.6626476049423218 - trainLoss: 0.6617351174354553\n",
      "cnt: 0 - valLoss: 0.6626462340354919 - trainLoss: 0.6617342829704285\n",
      "cnt: 0 - valLoss: 0.6626450419425964 - trainLoss: 0.6617335081100464\n",
      "cnt: 0 - valLoss: 0.6626437306404114 - trainLoss: 0.6617326140403748\n",
      "cnt: 0 - valLoss: 0.6626424193382263 - trainLoss: 0.6617317795753479\n",
      "cnt: 0 - valLoss: 0.6626411080360413 - trainLoss: 0.661730945110321\n",
      "cnt: 0 - valLoss: 0.6626397371292114 - trainLoss: 0.6617301106452942\n",
      "cnt: 0 - valLoss: 0.6626384258270264 - trainLoss: 0.6617292761802673\n",
      "cnt: 0 - valLoss: 0.6626371741294861 - trainLoss: 0.6617284417152405\n",
      "cnt: 0 - valLoss: 0.662635862827301 - trainLoss: 0.6617276072502136\n",
      "cnt: 0 - valLoss: 0.6626346111297607 - trainLoss: 0.661726713180542\n",
      "cnt: 0 - valLoss: 0.6626332998275757 - trainLoss: 0.6617259383201599\n",
      "cnt: 0 - valLoss: 0.6626319289207458 - trainLoss: 0.6617251038551331\n",
      "cnt: 0 - valLoss: 0.6626306176185608 - trainLoss: 0.6617242693901062\n",
      "cnt: 0 - valLoss: 0.6626293063163757 - trainLoss: 0.6617234349250793\n",
      "cnt: 0 - valLoss: 0.6626279354095459 - trainLoss: 0.6617225408554077\n",
      "cnt: 0 - valLoss: 0.6626266241073608 - trainLoss: 0.6617217063903809\n",
      "cnt: 0 - valLoss: 0.6626253724098206 - trainLoss: 0.6617209315299988\n",
      "cnt: 0 - valLoss: 0.6626240015029907 - trainLoss: 0.6617200374603271\n",
      "cnt: 0 - valLoss: 0.6626226902008057 - trainLoss: 0.6617191433906555\n",
      "cnt: 0 - valLoss: 0.6626213192939758 - trainLoss: 0.6617183685302734\n",
      "cnt: 0 - valLoss: 0.6626200079917908 - trainLoss: 0.6617175340652466\n",
      "cnt: 0 - valLoss: 0.6626186370849609 - trainLoss: 0.661716639995575\n",
      "cnt: 0 - valLoss: 0.6626173257827759 - trainLoss: 0.6617158651351929\n",
      "cnt: 0 - valLoss: 0.6626161336898804 - trainLoss: 0.6617149710655212\n",
      "cnt: 0 - valLoss: 0.6626147627830505 - trainLoss: 0.6617140769958496\n",
      "cnt: 0 - valLoss: 0.6626134514808655 - trainLoss: 0.6617133021354675\n",
      "cnt: 0 - valLoss: 0.6626121401786804 - trainLoss: 0.6617124080657959\n",
      "cnt: 0 - valLoss: 0.6626107692718506 - trainLoss: 0.661711573600769\n",
      "cnt: 0 - valLoss: 0.6626094579696655 - trainLoss: 0.6617107391357422\n",
      "cnt: 0 - valLoss: 0.6626080870628357 - trainLoss: 0.6617099642753601\n",
      "cnt: 0 - valLoss: 0.6626067161560059 - trainLoss: 0.6617090702056885\n",
      "cnt: 0 - valLoss: 0.6626054644584656 - trainLoss: 0.6617082357406616\n",
      "cnt: 0 - valLoss: 0.6626041531562805 - trainLoss: 0.6617074012756348\n",
      "cnt: 0 - valLoss: 0.6626027822494507 - trainLoss: 0.6617065072059631\n",
      "cnt: 0 - valLoss: 0.6626014113426208 - trainLoss: 0.6617056727409363\n",
      "cnt: 0 - valLoss: 0.6626001000404358 - trainLoss: 0.6617048382759094\n",
      "cnt: 0 - valLoss: 0.6625987887382507 - trainLoss: 0.6617040038108826\n",
      "cnt: 0 - valLoss: 0.6625974178314209 - trainLoss: 0.6617031097412109\n",
      "cnt: 0 - valLoss: 0.6625961065292358 - trainLoss: 0.6617022752761841\n",
      "cnt: 0 - valLoss: 0.662594735622406 - trainLoss: 0.6617014408111572\n",
      "cnt: 0 - valLoss: 0.6625934839248657 - trainLoss: 0.6617006063461304\n",
      "cnt: 0 - valLoss: 0.6625921726226807 - trainLoss: 0.661699652671814\n",
      "cnt: 0 - valLoss: 0.6625908613204956 - trainLoss: 0.6616988778114319\n",
      "cnt: 0 - valLoss: 0.6625894904136658 - trainLoss: 0.661698043346405\n",
      "cnt: 0 - valLoss: 0.6625881195068359 - trainLoss: 0.6616972088813782\n",
      "cnt: 0 - valLoss: 0.6625868678092957 - trainLoss: 0.6616963148117065\n",
      "cnt: 0 - valLoss: 0.662585437297821 - trainLoss: 0.6616954803466797\n",
      "cnt: 0 - valLoss: 0.6625841856002808 - trainLoss: 0.6616945862770081\n",
      "cnt: 0 - valLoss: 0.6625827550888062 - trainLoss: 0.6616937518119812\n",
      "cnt: 0 - valLoss: 0.6625814437866211 - trainLoss: 0.6616929769515991\n",
      "cnt: 0 - valLoss: 0.662580132484436 - trainLoss: 0.6616920828819275\n",
      "cnt: 0 - valLoss: 0.6625787615776062 - trainLoss: 0.6616912484169006\n",
      "cnt: 0 - valLoss: 0.6625774502754211 - trainLoss: 0.661690354347229\n",
      "cnt: 0 - valLoss: 0.6625760793685913 - trainLoss: 0.6616895198822021\n",
      "cnt: 0 - valLoss: 0.6625747084617615 - trainLoss: 0.6616886258125305\n",
      "cnt: 0 - valLoss: 0.6625733971595764 - trainLoss: 0.6616877913475037\n",
      "cnt: 0 - valLoss: 0.6625720262527466 - trainLoss: 0.661686897277832\n",
      "cnt: 0 - valLoss: 0.6625706553459167 - trainLoss: 0.66168612241745\n",
      "cnt: 0 - valLoss: 0.6625694036483765 - trainLoss: 0.6616852283477783\n",
      "cnt: 0 - valLoss: 0.6625680923461914 - trainLoss: 0.6616844534873962\n",
      "cnt: 0 - valLoss: 0.6625667214393616 - trainLoss: 0.6616835594177246\n",
      "cnt: 0 - valLoss: 0.6625654101371765 - trainLoss: 0.661682665348053\n",
      "cnt: 0 - valLoss: 0.6625640392303467 - trainLoss: 0.6616817712783813\n",
      "cnt: 0 - valLoss: 0.6625626683235168 - trainLoss: 0.6616809964179993\n",
      "cnt: 0 - valLoss: 0.6625613570213318 - trainLoss: 0.6616801619529724\n",
      "cnt: 0 - valLoss: 0.662559986114502 - trainLoss: 0.6616792678833008\n",
      "cnt: 0 - valLoss: 0.6625586748123169 - trainLoss: 0.6616783738136292\n",
      "cnt: 0 - valLoss: 0.6625573039054871 - trainLoss: 0.6616774797439575\n",
      "cnt: 0 - valLoss: 0.662555992603302 - trainLoss: 0.6616767644882202\n",
      "cnt: 0 - valLoss: 0.6625546216964722 - trainLoss: 0.6616758108139038\n",
      "cnt: 0 - valLoss: 0.6625532507896423 - trainLoss: 0.6616750359535217\n",
      "cnt: 0 - valLoss: 0.6625519394874573 - trainLoss: 0.6616741418838501\n",
      "cnt: 0 - valLoss: 0.6625505685806274 - trainLoss: 0.6616732478141785\n",
      "cnt: 0 - valLoss: 0.6625491976737976 - trainLoss: 0.6616724133491516\n",
      "cnt: 0 - valLoss: 0.6625478863716125 - trainLoss: 0.66167151927948\n",
      "cnt: 0 - valLoss: 0.6625465750694275 - trainLoss: 0.6616706848144531\n",
      "cnt: 0 - valLoss: 0.6625452041625977 - trainLoss: 0.6616697907447815\n",
      "cnt: 0 - valLoss: 0.6625438928604126 - trainLoss: 0.6616688966751099\n",
      "cnt: 0 - valLoss: 0.662542462348938 - trainLoss: 0.661668062210083\n",
      "cnt: 0 - valLoss: 0.6625411510467529 - trainLoss: 0.6616671681404114\n",
      "cnt: 0 - valLoss: 0.6625397801399231 - trainLoss: 0.6616663932800293\n",
      "cnt: 0 - valLoss: 0.6625384092330933 - trainLoss: 0.6616654992103577\n",
      "cnt: 0 - valLoss: 0.6625370979309082 - trainLoss: 0.661664605140686\n",
      "cnt: 0 - valLoss: 0.6625357270240784 - trainLoss: 0.6616637110710144\n",
      "cnt: 0 - valLoss: 0.6625344157218933 - trainLoss: 0.6616628766059875\n",
      "cnt: 0 - valLoss: 0.6625329852104187 - trainLoss: 0.6616619825363159\n",
      "cnt: 0 - valLoss: 0.6625316739082336 - trainLoss: 0.6616610884666443\n",
      "cnt: 0 - valLoss: 0.6625303030014038 - trainLoss: 0.6616602540016174\n",
      "cnt: 0 - valLoss: 0.662528932094574 - trainLoss: 0.6616593599319458\n",
      "cnt: 0 - valLoss: 0.6625276207923889 - trainLoss: 0.6616584658622742\n",
      "cnt: 0 - valLoss: 0.6625262498855591 - trainLoss: 0.6616576313972473\n",
      "cnt: 0 - valLoss: 0.6625248789787292 - trainLoss: 0.6616567969322205\n",
      "cnt: 0 - valLoss: 0.6625235676765442 - trainLoss: 0.661655843257904\n",
      "cnt: 0 - valLoss: 0.6625221967697144 - trainLoss: 0.6616550087928772\n",
      "cnt: 0 - valLoss: 0.6625208854675293 - trainLoss: 0.6616541147232056\n",
      "cnt: 0 - valLoss: 0.6625194549560547 - trainLoss: 0.6616532206535339\n",
      "cnt: 0 - valLoss: 0.6625181436538696 - trainLoss: 0.6616523265838623\n",
      "cnt: 0 - valLoss: 0.6625167727470398 - trainLoss: 0.6616514921188354\n",
      "cnt: 0 - valLoss: 0.6625154614448547 - trainLoss: 0.6616505980491638\n",
      "cnt: 0 - valLoss: 0.6625140905380249 - trainLoss: 0.6616497039794922\n",
      "cnt: 0 - valLoss: 0.6625127196311951 - trainLoss: 0.6616488099098206\n",
      "cnt: 0 - valLoss: 0.6625113487243652 - trainLoss: 0.6616479158401489\n",
      "cnt: 0 - valLoss: 0.6625099778175354 - trainLoss: 0.6616470217704773\n",
      "cnt: 0 - valLoss: 0.6625086665153503 - trainLoss: 0.6616461873054504\n",
      "cnt: 0 - valLoss: 0.6625072956085205 - trainLoss: 0.6616452932357788\n",
      "cnt: 0 - valLoss: 0.6625059247016907 - trainLoss: 0.6616443991661072\n",
      "cnt: 0 - valLoss: 0.6625045537948608 - trainLoss: 0.6616435050964355\n",
      "cnt: 0 - valLoss: 0.662503182888031 - trainLoss: 0.6616425514221191\n",
      "cnt: 0 - valLoss: 0.6625018119812012 - trainLoss: 0.6616417169570923\n",
      "cnt: 0 - valLoss: 0.6625005602836609 - trainLoss: 0.6616408228874207\n",
      "cnt: 0 - valLoss: 0.6624991297721863 - trainLoss: 0.6616399884223938\n",
      "cnt: 0 - valLoss: 0.6624977588653564 - trainLoss: 0.6616390943527222\n",
      "cnt: 0 - valLoss: 0.6624963879585266 - trainLoss: 0.6616382002830505\n",
      "cnt: 0 - valLoss: 0.6624950170516968 - trainLoss: 0.6616372466087341\n",
      "cnt: 0 - valLoss: 0.6624936461448669 - trainLoss: 0.6616364121437073\n",
      "cnt: 0 - valLoss: 0.6624922752380371 - trainLoss: 0.6616354584693909\n",
      "cnt: 0 - valLoss: 0.6624909043312073 - trainLoss: 0.6616346836090088\n",
      "cnt: 0 - valLoss: 0.6624895334243774 - trainLoss: 0.6616337895393372\n",
      "cnt: 0 - valLoss: 0.6624881625175476 - trainLoss: 0.6616328358650208\n",
      "cnt: 0 - valLoss: 0.6624867916107178 - trainLoss: 0.6616319417953491\n",
      "cnt: 0 - valLoss: 0.6624854207038879 - trainLoss: 0.6616311073303223\n",
      "cnt: 0 - valLoss: 0.6624840497970581 - trainLoss: 0.6616301536560059\n",
      "cnt: 0 - valLoss: 0.662482738494873 - trainLoss: 0.661629319190979\n",
      "cnt: 0 - valLoss: 0.6624813079833984 - trainLoss: 0.6616284251213074\n",
      "cnt: 0 - valLoss: 0.6624799966812134 - trainLoss: 0.6616275310516357\n",
      "cnt: 0 - valLoss: 0.6624785661697388 - trainLoss: 0.6616266369819641\n",
      "cnt: 0 - valLoss: 0.6624772548675537 - trainLoss: 0.6616257429122925\n",
      "cnt: 0 - valLoss: 0.6624758839607239 - trainLoss: 0.6616247892379761\n",
      "cnt: 0 - valLoss: 0.6624745726585388 - trainLoss: 0.6616239547729492\n",
      "cnt: 0 - valLoss: 0.662473201751709 - trainLoss: 0.6616230607032776\n",
      "cnt: 0 - valLoss: 0.6624718308448792 - trainLoss: 0.6616221070289612\n",
      "cnt: 0 - valLoss: 0.6624704599380493 - trainLoss: 0.6616213321685791\n",
      "cnt: 0 - valLoss: 0.6624690890312195 - trainLoss: 0.6616204380989075\n",
      "cnt: 0 - valLoss: 0.6624677181243896 - trainLoss: 0.6616194844245911\n",
      "cnt: 0 - valLoss: 0.6624663472175598 - trainLoss: 0.6616186499595642\n",
      "cnt: 0 - valLoss: 0.66246497631073 - trainLoss: 0.6616176962852478\n",
      "cnt: 0 - valLoss: 0.6624636054039001 - trainLoss: 0.6616169214248657\n",
      "cnt: 0 - valLoss: 0.6624622344970703 - trainLoss: 0.6616159677505493\n",
      "cnt: 0 - valLoss: 0.6624608635902405 - trainLoss: 0.6616150736808777\n",
      "cnt: 0 - valLoss: 0.6624594926834106 - trainLoss: 0.661614179611206\n",
      "cnt: 0 - valLoss: 0.6624581217765808 - trainLoss: 0.6616132855415344\n",
      "cnt: 0 - valLoss: 0.662456750869751 - trainLoss: 0.6616123914718628\n",
      "cnt: 0 - valLoss: 0.6624553799629211 - trainLoss: 0.6616114974021912\n",
      "cnt: 0 - valLoss: 0.6624540090560913 - trainLoss: 0.6616105437278748\n",
      "cnt: 0 - valLoss: 0.6624526381492615 - trainLoss: 0.6616097092628479\n",
      "cnt: 0 - valLoss: 0.6624512672424316 - trainLoss: 0.6616088151931763\n",
      "cnt: 0 - valLoss: 0.6624498963356018 - trainLoss: 0.6616078615188599\n",
      "cnt: 0 - valLoss: 0.662448525428772 - trainLoss: 0.661607027053833\n",
      "cnt: 0 - valLoss: 0.6624471545219421 - trainLoss: 0.6616061329841614\n",
      "cnt: 0 - valLoss: 0.6624457836151123 - trainLoss: 0.6616052389144897\n",
      "cnt: 0 - valLoss: 0.6624442934989929 - trainLoss: 0.6616043448448181\n",
      "cnt: 0 - valLoss: 0.6624429821968079 - trainLoss: 0.6616033911705017\n",
      "cnt: 0 - valLoss: 0.6624415516853333 - trainLoss: 0.6616024971008301\n",
      "cnt: 0 - valLoss: 0.6624402403831482 - trainLoss: 0.6616016626358032\n",
      "cnt: 0 - valLoss: 0.6624388098716736 - trainLoss: 0.6616007685661316\n",
      "cnt: 0 - valLoss: 0.6624374389648438 - trainLoss: 0.6615998148918152\n",
      "cnt: 0 - valLoss: 0.6624360084533691 - trainLoss: 0.6615989208221436\n",
      "cnt: 0 - valLoss: 0.6624346971511841 - trainLoss: 0.6615980267524719\n",
      "cnt: 0 - valLoss: 0.6624333262443542 - trainLoss: 0.6615971326828003\n",
      "cnt: 0 - valLoss: 0.6624318957328796 - trainLoss: 0.6615962982177734\n",
      "cnt: 0 - valLoss: 0.6624305248260498 - trainLoss: 0.661595344543457\n",
      "cnt: 0 - valLoss: 0.6624290943145752 - trainLoss: 0.6615944504737854\n",
      "cnt: 0 - valLoss: 0.6624277234077454 - trainLoss: 0.6615935564041138\n",
      "cnt: 0 - valLoss: 0.6624263525009155 - trainLoss: 0.6615926623344421\n",
      "cnt: 0 - valLoss: 0.6624249219894409 - trainLoss: 0.6615917086601257\n",
      "cnt: 0 - valLoss: 0.6624234914779663 - trainLoss: 0.6615908145904541\n",
      "cnt: 0 - valLoss: 0.6624221205711365 - trainLoss: 0.6615899205207825\n",
      "cnt: 0 - valLoss: 0.6624207496643066 - trainLoss: 0.6615889668464661\n",
      "cnt: 0 - valLoss: 0.6624193787574768 - trainLoss: 0.6615881323814392\n",
      "cnt: 0 - valLoss: 0.6624179482460022 - trainLoss: 0.6615871787071228\n",
      "cnt: 0 - valLoss: 0.6624165773391724 - trainLoss: 0.6615862846374512\n",
      "cnt: 0 - valLoss: 0.6624151468276978 - trainLoss: 0.6615854501724243\n",
      "cnt: 0 - valLoss: 0.6624137163162231 - trainLoss: 0.6615844964981079\n",
      "cnt: 0 - valLoss: 0.6624123454093933 - trainLoss: 0.6615836024284363\n",
      "cnt: 0 - valLoss: 0.6624109745025635 - trainLoss: 0.6615827679634094\n",
      "cnt: 0 - valLoss: 0.6624095439910889 - trainLoss: 0.661581814289093\n",
      "cnt: 0 - valLoss: 0.6624082326889038 - trainLoss: 0.6615809202194214\n",
      "cnt: 0 - valLoss: 0.6624067425727844 - trainLoss: 0.6615800261497498\n",
      "cnt: 0 - valLoss: 0.6624053120613098 - trainLoss: 0.6615791320800781\n",
      "cnt: 0 - valLoss: 0.6624040007591248 - trainLoss: 0.6615781784057617\n",
      "cnt: 0 - valLoss: 0.6624025702476501 - trainLoss: 0.6615773439407349\n",
      "cnt: 0 - valLoss: 0.6624011397361755 - trainLoss: 0.6615763902664185\n",
      "cnt: 0 - valLoss: 0.6623997688293457 - trainLoss: 0.6615754961967468\n",
      "cnt: 0 - valLoss: 0.6623983383178711 - trainLoss: 0.6615746021270752\n",
      "cnt: 0 - valLoss: 0.6623969078063965 - trainLoss: 0.6615736484527588\n",
      "cnt: 0 - valLoss: 0.6623955368995667 - trainLoss: 0.6615728139877319\n",
      "cnt: 0 - valLoss: 0.6623941659927368 - trainLoss: 0.6615718603134155\n",
      "cnt: 0 - valLoss: 0.6623926758766174 - trainLoss: 0.6615710258483887\n",
      "cnt: 0 - valLoss: 0.6623913645744324 - trainLoss: 0.661570131778717\n",
      "cnt: 0 - valLoss: 0.6623899340629578 - trainLoss: 0.6615692377090454\n",
      "cnt: 0 - valLoss: 0.6623885035514832 - trainLoss: 0.661568284034729\n",
      "cnt: 0 - valLoss: 0.6623871326446533 - trainLoss: 0.6615673899650574\n",
      "cnt: 0 - valLoss: 0.6623856425285339 - trainLoss: 0.6615664958953857\n",
      "cnt: 0 - valLoss: 0.6623842716217041 - trainLoss: 0.6615656018257141\n",
      "cnt: 0 - valLoss: 0.6623829007148743 - trainLoss: 0.6615647077560425\n",
      "cnt: 0 - valLoss: 0.6623814702033997 - trainLoss: 0.6615637540817261\n",
      "cnt: 0 - valLoss: 0.6623800992965698 - trainLoss: 0.6615628600120544\n",
      "cnt: 0 - valLoss: 0.6623786687850952 - trainLoss: 0.6615619659423828\n",
      "cnt: 0 - valLoss: 0.6623772382736206 - trainLoss: 0.6615610718727112\n",
      "cnt: 0 - valLoss: 0.662375807762146 - trainLoss: 0.6615601778030396\n",
      "cnt: 0 - valLoss: 0.6623743772506714 - trainLoss: 0.6615592837333679\n",
      "cnt: 0 - valLoss: 0.6623729467391968 - trainLoss: 0.6615583300590515\n",
      "cnt: 0 - valLoss: 0.6623715758323669 - trainLoss: 0.6615574359893799\n",
      "cnt: 0 - valLoss: 0.6623701453208923 - trainLoss: 0.6615564823150635\n",
      "cnt: 0 - valLoss: 0.6623687148094177 - trainLoss: 0.6615555882453918\n",
      "cnt: 0 - valLoss: 0.6623673439025879 - trainLoss: 0.6615546941757202\n",
      "cnt: 0 - valLoss: 0.6623659133911133 - trainLoss: 0.6615538001060486\n",
      "cnt: 0 - valLoss: 0.6623644828796387 - trainLoss: 0.6615529656410217\n",
      "cnt: 0 - valLoss: 0.6623630523681641 - trainLoss: 0.6615520119667053\n",
      "cnt: 0 - valLoss: 0.6623616814613342 - trainLoss: 0.6615511178970337\n",
      "cnt: 0 - valLoss: 0.6623602509498596 - trainLoss: 0.6615501642227173\n",
      "cnt: 0 - valLoss: 0.662358820438385 - trainLoss: 0.6615492701530457\n",
      "cnt: 0 - valLoss: 0.6623573899269104 - trainLoss: 0.6615483164787292\n",
      "cnt: 0 - valLoss: 0.6623560190200806 - trainLoss: 0.6615474224090576\n",
      "cnt: 0 - valLoss: 0.6623545289039612 - trainLoss: 0.661546528339386\n",
      "cnt: 0 - valLoss: 0.6623531579971313 - trainLoss: 0.6615455746650696\n",
      "cnt: 0 - valLoss: 0.6623517274856567 - trainLoss: 0.661544680595398\n",
      "cnt: 0 - valLoss: 0.6623503565788269 - trainLoss: 0.6615437865257263\n",
      "cnt: 0 - valLoss: 0.6623488664627075 - trainLoss: 0.6615428328514099\n",
      "cnt: 0 - valLoss: 0.6623474955558777 - trainLoss: 0.6615419387817383\n",
      "cnt: 0 - valLoss: 0.6623460054397583 - trainLoss: 0.6615410447120667\n",
      "cnt: 0 - valLoss: 0.6623445749282837 - trainLoss: 0.6615400910377502\n",
      "cnt: 0 - valLoss: 0.6623431444168091 - trainLoss: 0.6615392565727234\n",
      "cnt: 0 - valLoss: 0.6623417139053345 - trainLoss: 0.661538302898407\n",
      "cnt: 0 - valLoss: 0.6623403429985046 - trainLoss: 0.6615374088287354\n",
      "cnt: 0 - valLoss: 0.6623388528823853 - trainLoss: 0.661536455154419\n",
      "cnt: 0 - valLoss: 0.6623374819755554 - trainLoss: 0.6615355610847473\n",
      "cnt: 0 - valLoss: 0.6623360514640808 - trainLoss: 0.6615346074104309\n",
      "cnt: 0 - valLoss: 0.6623346209526062 - trainLoss: 0.6615337133407593\n",
      "cnt: 0 - valLoss: 0.6623331308364868 - trainLoss: 0.6615328192710876\n",
      "cnt: 0 - valLoss: 0.662331759929657 - trainLoss: 0.661531925201416\n",
      "cnt: 0 - valLoss: 0.6623302698135376 - trainLoss: 0.6615309715270996\n",
      "cnt: 0 - valLoss: 0.6623288989067078 - trainLoss: 0.661530077457428\n",
      "cnt: 0 - valLoss: 0.6623274683952332 - trainLoss: 0.6615291833877563\n",
      "cnt: 0 - valLoss: 0.6623260378837585 - trainLoss: 0.6615282297134399\n",
      "cnt: 0 - valLoss: 0.6623246073722839 - trainLoss: 0.6615273356437683\n",
      "cnt: 0 - valLoss: 0.6623231172561646 - trainLoss: 0.6615263819694519\n",
      "cnt: 0 - valLoss: 0.6623217463493347 - trainLoss: 0.6615254878997803\n",
      "cnt: 0 - valLoss: 0.6623203158378601 - trainLoss: 0.6615245342254639\n",
      "cnt: 0 - valLoss: 0.6623188853263855 - trainLoss: 0.6615236401557922\n",
      "cnt: 0 - valLoss: 0.6623174548149109 - trainLoss: 0.6615227460861206\n",
      "cnt: 0 - valLoss: 0.6623159646987915 - trainLoss: 0.6615217924118042\n",
      "cnt: 0 - valLoss: 0.6623145341873169 - trainLoss: 0.6615208387374878\n",
      "cnt: 0 - valLoss: 0.6623129844665527 - trainLoss: 0.6615199446678162\n",
      "cnt: 0 - valLoss: 0.6623116135597229 - trainLoss: 0.6615188717842102\n",
      "cnt: 0 - valLoss: 0.6623101234436035 - trainLoss: 0.6615179181098938\n",
      "cnt: 0 - valLoss: 0.6623085737228394 - trainLoss: 0.6615169048309326\n",
      "cnt: 0 - valLoss: 0.6623070240020752 - trainLoss: 0.6615159511566162\n",
      "cnt: 0 - valLoss: 0.6623055338859558 - trainLoss: 0.661514937877655\n",
      "cnt: 0 - valLoss: 0.6623039841651917 - trainLoss: 0.6615139245986938\n",
      "cnt: 0 - valLoss: 0.6623024940490723 - trainLoss: 0.6615129709243774\n",
      "cnt: 0 - valLoss: 0.6623011231422424 - trainLoss: 0.6615118980407715\n",
      "cnt: 0 - valLoss: 0.6622995734214783 - trainLoss: 0.6615109443664551\n",
      "cnt: 0 - valLoss: 0.6622980833053589 - trainLoss: 0.6615098714828491\n",
      "cnt: 0 - valLoss: 0.6622965335845947 - trainLoss: 0.6615089178085327\n",
      "cnt: 0 - valLoss: 0.6622950434684753 - trainLoss: 0.6615079045295715\n",
      "cnt: 0 - valLoss: 0.6622934937477112 - trainLoss: 0.6615068912506104\n",
      "cnt: 0 - valLoss: 0.6622920036315918 - trainLoss: 0.6615058779716492\n",
      "cnt: 0 - valLoss: 0.6622905731201172 - trainLoss: 0.661504864692688\n",
      "cnt: 0 - valLoss: 0.662289023399353 - trainLoss: 0.6615039110183716\n",
      "cnt: 0 - valLoss: 0.6622875928878784 - trainLoss: 0.6615028977394104\n",
      "cnt: 0 - valLoss: 0.6622859835624695 - trainLoss: 0.6615018844604492\n",
      "cnt: 0 - valLoss: 0.6622844934463501 - trainLoss: 0.6615008115768433\n",
      "cnt: 0 - valLoss: 0.6622831225395203 - trainLoss: 0.6614998579025269\n",
      "cnt: 0 - valLoss: 0.6622815728187561 - trainLoss: 0.6614989042282104\n",
      "cnt: 0 - valLoss: 0.6622800230979919 - trainLoss: 0.6614978313446045\n",
      "cnt: 0 - valLoss: 0.6622785329818726 - trainLoss: 0.6614968776702881\n",
      "cnt: 0 - valLoss: 0.6622769832611084 - trainLoss: 0.6614958643913269\n",
      "cnt: 0 - valLoss: 0.662275493144989 - trainLoss: 0.6614948511123657\n",
      "cnt: 0 - valLoss: 0.6622740626335144 - trainLoss: 0.6614938974380493\n",
      "cnt: 0 - valLoss: 0.662272572517395 - trainLoss: 0.6614928841590881\n",
      "cnt: 0 - valLoss: 0.6622710227966309 - trainLoss: 0.6614918112754822\n",
      "cnt: 0 - valLoss: 0.6622695326805115 - trainLoss: 0.6614908576011658\n",
      "cnt: 0 - valLoss: 0.6622679829597473 - trainLoss: 0.6614898443222046\n",
      "cnt: 0 - valLoss: 0.6622665524482727 - trainLoss: 0.6614888310432434\n",
      "cnt: 0 - valLoss: 0.6622650623321533 - trainLoss: 0.661487877368927\n",
      "cnt: 0 - valLoss: 0.6622635126113892 - trainLoss: 0.661486804485321\n",
      "cnt: 0 - valLoss: 0.662261962890625 - trainLoss: 0.6614858508110046\n",
      "cnt: 0 - valLoss: 0.6622604131698608 - trainLoss: 0.6614848375320435\n",
      "cnt: 0 - valLoss: 0.6622589230537415 - trainLoss: 0.6614837646484375\n",
      "cnt: 0 - valLoss: 0.6622574925422668 - trainLoss: 0.6614828705787659\n",
      "cnt: 0 - valLoss: 0.6622559428215027 - trainLoss: 0.6614817976951599\n",
      "cnt: 0 - valLoss: 0.6622543931007385 - trainLoss: 0.6614807844161987\n",
      "cnt: 0 - valLoss: 0.6622528433799744 - trainLoss: 0.6614797711372375\n",
      "cnt: 0 - valLoss: 0.662251353263855 - trainLoss: 0.6614787578582764\n",
      "cnt: 0 - valLoss: 0.6622498631477356 - trainLoss: 0.66147780418396\n",
      "cnt: 0 - valLoss: 0.6622483134269714 - trainLoss: 0.6614767909049988\n",
      "cnt: 0 - valLoss: 0.662246823310852 - trainLoss: 0.6614757180213928\n",
      "cnt: 0 - valLoss: 0.6622452735900879 - trainLoss: 0.6614747047424316\n",
      "cnt: 0 - valLoss: 0.6622437238693237 - trainLoss: 0.6614736914634705\n",
      "cnt: 0 - valLoss: 0.6622421741485596 - trainLoss: 0.661472737789154\n",
      "cnt: 0 - valLoss: 0.6622408032417297 - trainLoss: 0.6614716649055481\n",
      "cnt: 0 - valLoss: 0.6622393131256104 - trainLoss: 0.6614707112312317\n",
      "cnt: 0 - valLoss: 0.6622377038002014 - trainLoss: 0.6614696979522705\n",
      "cnt: 0 - valLoss: 0.6622361540794373 - trainLoss: 0.6614686846733093\n",
      "cnt: 0 - valLoss: 0.6622346639633179 - trainLoss: 0.6614676713943481\n",
      "cnt: 0 - valLoss: 0.6622334122657776 - trainLoss: 0.6614665985107422\n",
      "cnt: 0 - valLoss: 0.6622320413589478 - trainLoss: 0.6614657044410706\n",
      "cnt: 0 - valLoss: 0.6622306704521179 - trainLoss: 0.6614647507667542\n",
      "cnt: 0 - valLoss: 0.6622292995452881 - trainLoss: 0.661463737487793\n",
      "cnt: 0 - valLoss: 0.6622279286384583 - trainLoss: 0.6614627838134766\n",
      "cnt: 0 - valLoss: 0.6622265577316284 - trainLoss: 0.6614618897438049\n",
      "cnt: 0 - valLoss: 0.6622251868247986 - trainLoss: 0.6614609360694885\n",
      "cnt: 0 - valLoss: 0.6622238159179688 - trainLoss: 0.6614599227905273\n",
      "cnt: 0 - valLoss: 0.6622224450111389 - trainLoss: 0.6614590287208557\n",
      "cnt: 0 - valLoss: 0.6622210741043091 - trainLoss: 0.6614580154418945\n",
      "cnt: 0 - valLoss: 0.6622197031974792 - trainLoss: 0.6614570617675781\n",
      "cnt: 0 - valLoss: 0.6622182726860046 - trainLoss: 0.6614561080932617\n",
      "cnt: 0 - valLoss: 0.6622170209884644 - trainLoss: 0.6614551544189453\n",
      "cnt: 0 - valLoss: 0.6622156500816345 - trainLoss: 0.6614541411399841\n",
      "cnt: 0 - valLoss: 0.6622142791748047 - trainLoss: 0.6614532470703125\n",
      "cnt: 0 - valLoss: 0.6622129082679749 - trainLoss: 0.6614522933959961\n",
      "cnt: 0 - valLoss: 0.662211537361145 - trainLoss: 0.6614512801170349\n",
      "cnt: 0 - valLoss: 0.6622101664543152 - trainLoss: 0.6614503264427185\n",
      "cnt: 0 - valLoss: 0.6622087359428406 - trainLoss: 0.6614493727684021\n",
      "cnt: 0 - valLoss: 0.6622074246406555 - trainLoss: 0.6614483594894409\n",
      "cnt: 0 - valLoss: 0.6622060537338257 - trainLoss: 0.6614474654197693\n",
      "cnt: 0 - valLoss: 0.6622046232223511 - trainLoss: 0.6614464521408081\n",
      "cnt: 0 - valLoss: 0.662203311920166 - trainLoss: 0.6614454984664917\n",
      "cnt: 0 - valLoss: 0.6622019410133362 - trainLoss: 0.6614445447921753\n",
      "cnt: 0 - valLoss: 0.6622005701065063 - trainLoss: 0.6614435911178589\n",
      "cnt: 0 - valLoss: 0.6621991991996765 - trainLoss: 0.6614426374435425\n",
      "cnt: 0 - valLoss: 0.6621978878974915 - trainLoss: 0.6614416241645813\n",
      "cnt: 0 - valLoss: 0.6621964573860168 - trainLoss: 0.6614407300949097\n",
      "cnt: 0 - valLoss: 0.662195086479187 - trainLoss: 0.6614397168159485\n",
      "cnt: 0 - valLoss: 0.6621936559677124 - trainLoss: 0.6614388227462769\n",
      "cnt: 0 - valLoss: 0.6621923446655273 - trainLoss: 0.6614378094673157\n",
      "cnt: 0 - valLoss: 0.6621909141540527 - trainLoss: 0.6614367961883545\n",
      "cnt: 0 - valLoss: 0.6621895432472229 - trainLoss: 0.6614358425140381\n",
      "cnt: 0 - valLoss: 0.6621881723403931 - trainLoss: 0.6614348888397217\n",
      "cnt: 0 - valLoss: 0.6621868014335632 - trainLoss: 0.6614339351654053\n",
      "cnt: 0 - valLoss: 0.6621854305267334 - trainLoss: 0.6614329218864441\n",
      "cnt: 0 - valLoss: 0.662183940410614 - trainLoss: 0.6614319682121277\n",
      "cnt: 0 - valLoss: 0.662182629108429 - trainLoss: 0.6614309549331665\n",
      "cnt: 0 - valLoss: 0.6621813178062439 - trainLoss: 0.6614300012588501\n",
      "cnt: 0 - valLoss: 0.6621798872947693 - trainLoss: 0.6614291071891785\n",
      "cnt: 0 - valLoss: 0.6621785759925842 - trainLoss: 0.6614280939102173\n",
      "cnt: 0 - valLoss: 0.6621771454811096 - trainLoss: 0.6614271402359009\n",
      "cnt: 0 - valLoss: 0.662175714969635 - trainLoss: 0.6614261865615845\n",
      "cnt: 0 - valLoss: 0.66217440366745 - trainLoss: 0.6614251732826233\n",
      "cnt: 0 - valLoss: 0.6621729731559753 - trainLoss: 0.6614241600036621\n",
      "cnt: 0 - valLoss: 0.6621715426445007 - trainLoss: 0.6614232063293457\n",
      "cnt: 0 - valLoss: 0.6621701717376709 - trainLoss: 0.6614222526550293\n",
      "cnt: 0 - valLoss: 0.6621688008308411 - trainLoss: 0.6614212989807129\n",
      "cnt: 0 - valLoss: 0.6621674299240112 - trainLoss: 0.6614202857017517\n",
      "cnt: 0 - valLoss: 0.6621659994125366 - trainLoss: 0.6614193320274353\n",
      "cnt: 0 - valLoss: 0.662164568901062 - trainLoss: 0.6614183187484741\n",
      "cnt: 0 - valLoss: 0.6621631979942322 - trainLoss: 0.6614173650741577\n",
      "cnt: 0 - valLoss: 0.6621619462966919 - trainLoss: 0.6614164113998413\n",
      "cnt: 0 - valLoss: 0.6621605157852173 - trainLoss: 0.6614154577255249\n",
      "cnt: 0 - valLoss: 0.6621591448783875 - trainLoss: 0.6614144444465637\n",
      "cnt: 0 - valLoss: 0.6621578335762024 - trainLoss: 0.6614134907722473\n",
      "cnt: 0 - valLoss: 0.6621565222740173 - trainLoss: 0.6614125370979309\n",
      "cnt: 0 - valLoss: 0.6621552109718323 - trainLoss: 0.6614115238189697\n",
      "cnt: 0 - valLoss: 0.6621538400650024 - trainLoss: 0.6614105701446533\n",
      "cnt: 0 - valLoss: 0.6621525287628174 - trainLoss: 0.6614096760749817\n",
      "cnt: 0 - valLoss: 0.6621512770652771 - trainLoss: 0.6614086031913757\n",
      "cnt: 0 - valLoss: 0.6621499061584473 - trainLoss: 0.6614077091217041\n",
      "cnt: 0 - valLoss: 0.662148654460907 - trainLoss: 0.6614067554473877\n",
      "cnt: 0 - valLoss: 0.6621472835540771 - trainLoss: 0.6614058017730713\n",
      "cnt: 0 - valLoss: 0.6621460318565369 - trainLoss: 0.6614048480987549\n",
      "cnt: 0 - valLoss: 0.6621446013450623 - trainLoss: 0.6614038348197937\n",
      "cnt: 0 - valLoss: 0.6621432900428772 - trainLoss: 0.6614028215408325\n",
      "cnt: 0 - valLoss: 0.6621419787406921 - trainLoss: 0.6614018678665161\n",
      "cnt: 0 - valLoss: 0.6621406078338623 - trainLoss: 0.6614008545875549\n",
      "cnt: 0 - valLoss: 0.6621391773223877 - trainLoss: 0.661399781703949\n",
      "cnt: 0 - valLoss: 0.6621378660202026 - trainLoss: 0.6613987684249878\n",
      "cnt: 0 - valLoss: 0.662136435508728 - trainLoss: 0.6613977551460266\n",
      "cnt: 0 - valLoss: 0.6621350646018982 - trainLoss: 0.6613967418670654\n",
      "cnt: 0 - valLoss: 0.6621337532997131 - trainLoss: 0.6613956093788147\n",
      "cnt: 0 - valLoss: 0.6621323227882385 - trainLoss: 0.6613946557044983\n",
      "cnt: 0 - valLoss: 0.6621309518814087 - trainLoss: 0.6613936424255371\n",
      "cnt: 0 - valLoss: 0.6621295809745789 - trainLoss: 0.6613926291465759\n",
      "cnt: 0 - valLoss: 0.6621283888816833 - trainLoss: 0.6613916158676147\n",
      "cnt: 0 - valLoss: 0.6621269583702087 - trainLoss: 0.6613905429840088\n",
      "cnt: 0 - valLoss: 0.6621257066726685 - trainLoss: 0.6613896489143372\n",
      "cnt: 0 - valLoss: 0.6621243953704834 - trainLoss: 0.661388635635376\n",
      "cnt: 0 - valLoss: 0.6621230840682983 - trainLoss: 0.6613876819610596\n",
      "cnt: 0 - valLoss: 0.6621217727661133 - trainLoss: 0.6613866090774536\n",
      "cnt: 0 - valLoss: 0.6621204614639282 - trainLoss: 0.6613856554031372\n",
      "cnt: 0 - valLoss: 0.6621192097663879 - trainLoss: 0.6613847017288208\n",
      "cnt: 0 - valLoss: 0.6621178388595581 - trainLoss: 0.6613836884498596\n",
      "cnt: 0 - valLoss: 0.6621165871620178 - trainLoss: 0.661382794380188\n",
      "cnt: 0 - valLoss: 0.6621152758598328 - trainLoss: 0.6613817811012268\n",
      "cnt: 0 - valLoss: 0.6621139049530029 - trainLoss: 0.6613808274269104\n",
      "cnt: 0 - valLoss: 0.6621126532554626 - trainLoss: 0.6613797545433044\n",
      "cnt: 0 - valLoss: 0.6621113419532776 - trainLoss: 0.661378800868988\n",
      "cnt: 0 - valLoss: 0.6621100902557373 - trainLoss: 0.6613777875900269\n",
      "cnt: 0 - valLoss: 0.6621086597442627 - trainLoss: 0.6613768935203552\n",
      "cnt: 0 - valLoss: 0.6621074080467224 - trainLoss: 0.661375880241394\n",
      "cnt: 0 - valLoss: 0.6621060967445374 - trainLoss: 0.6613749265670776\n",
      "cnt: 0 - valLoss: 0.6621047258377075 - trainLoss: 0.6613739132881165\n",
      "cnt: 0 - valLoss: 0.6621034741401672 - trainLoss: 0.6613729000091553\n",
      "cnt: 0 - valLoss: 0.6621021628379822 - trainLoss: 0.6613718867301941\n",
      "cnt: 0 - valLoss: 0.6621008515357971 - trainLoss: 0.6613709330558777\n",
      "cnt: 0 - valLoss: 0.6620995402336121 - trainLoss: 0.6613699793815613\n",
      "cnt: 0 - valLoss: 0.662098228931427 - trainLoss: 0.6613689661026001\n",
      "cnt: 0 - valLoss: 0.6620969176292419 - trainLoss: 0.6613679528236389\n",
      "cnt: 0 - valLoss: 0.6620956063270569 - trainLoss: 0.6613669991493225\n",
      "cnt: 0 - valLoss: 0.6620942950248718 - trainLoss: 0.6613659858703613\n",
      "cnt: 0 - valLoss: 0.6620929837226868 - trainLoss: 0.6613649725914001\n",
      "cnt: 0 - valLoss: 0.6620916724205017 - trainLoss: 0.6613640189170837\n",
      "cnt: 0 - valLoss: 0.6620903611183167 - trainLoss: 0.6613630652427673\n",
      "cnt: 0 - valLoss: 0.6620889902114868 - trainLoss: 0.6613620519638062\n",
      "cnt: 0 - valLoss: 0.6620876789093018 - trainLoss: 0.661361038684845\n",
      "cnt: 0 - valLoss: 0.6620863676071167 - trainLoss: 0.6613600850105286\n",
      "cnt: 0 - valLoss: 0.6620850563049316 - trainLoss: 0.6613590717315674\n",
      "cnt: 0 - valLoss: 0.6620837450027466 - trainLoss: 0.6613580584526062\n",
      "cnt: 0 - valLoss: 0.6620824337005615 - trainLoss: 0.6613571047782898\n",
      "cnt: 0 - valLoss: 0.6620810627937317 - trainLoss: 0.6613560914993286\n",
      "cnt: 0 - valLoss: 0.6620797514915466 - trainLoss: 0.6613550782203674\n",
      "cnt: 0 - valLoss: 0.6620784997940063 - trainLoss: 0.661354124546051\n",
      "cnt: 0 - valLoss: 0.6620772480964661 - trainLoss: 0.6613531112670898\n",
      "cnt: 0 - valLoss: 0.6620758771896362 - trainLoss: 0.6613521575927734\n",
      "cnt: 0 - valLoss: 0.662074625492096 - trainLoss: 0.6613511443138123\n",
      "cnt: 0 - valLoss: 0.6620733141899109 - trainLoss: 0.6613501906394958\n",
      "cnt: 0 - valLoss: 0.6620720028877258 - trainLoss: 0.6613491177558899\n",
      "cnt: 0 - valLoss: 0.6620706915855408 - trainLoss: 0.6613481640815735\n",
      "cnt: 0 - valLoss: 0.6620694398880005 - trainLoss: 0.6613472104072571\n",
      "cnt: 0 - valLoss: 0.6620680689811707 - trainLoss: 0.6613461971282959\n",
      "cnt: 0 - valLoss: 0.6620668172836304 - trainLoss: 0.6613451838493347\n",
      "cnt: 0 - valLoss: 0.6620654463768005 - trainLoss: 0.6613441705703735\n",
      "cnt: 0 - valLoss: 0.6620641350746155 - trainLoss: 0.6613431572914124\n",
      "cnt: 0 - valLoss: 0.6620627641677856 - trainLoss: 0.6613421440124512\n",
      "cnt: 0 - valLoss: 0.6620615124702454 - trainLoss: 0.6613410711288452\n",
      "cnt: 0 - valLoss: 0.6620601415634155 - trainLoss: 0.661340057849884\n",
      "cnt: 0 - valLoss: 0.6620588302612305 - trainLoss: 0.6613390445709229\n",
      "cnt: 0 - valLoss: 0.6620575189590454 - trainLoss: 0.6613379716873169\n",
      "cnt: 0 - valLoss: 0.6620562076568604 - trainLoss: 0.6613369584083557\n",
      "cnt: 0 - valLoss: 0.6620548963546753 - trainLoss: 0.6613358855247498\n",
      "cnt: 0 - valLoss: 0.6620535850524902 - trainLoss: 0.6613348126411438\n",
      "cnt: 0 - valLoss: 0.6620522141456604 - trainLoss: 0.6613337993621826\n",
      "cnt: 0 - valLoss: 0.6620509028434753 - trainLoss: 0.6613327860832214\n",
      "cnt: 0 - valLoss: 0.6620495915412903 - trainLoss: 0.6613317131996155\n",
      "cnt: 0 - valLoss: 0.6620482206344604 - trainLoss: 0.6613306403160095\n",
      "cnt: 0 - valLoss: 0.6620469093322754 - trainLoss: 0.6613296866416931\n",
      "cnt: 0 - valLoss: 0.6620455384254456 - trainLoss: 0.6613286137580872\n",
      "cnt: 0 - valLoss: 0.6620442271232605 - trainLoss: 0.6613275408744812\n",
      "cnt: 0 - valLoss: 0.6620429158210754 - trainLoss: 0.66132652759552\n",
      "cnt: 0 - valLoss: 0.6620415449142456 - trainLoss: 0.6613254547119141\n",
      "cnt: 0 - valLoss: 0.6620402336120605 - trainLoss: 0.6613244414329529\n",
      "cnt: 0 - valLoss: 0.6620389223098755 - trainLoss: 0.6613234281539917\n",
      "cnt: 0 - valLoss: 0.6620375514030457 - trainLoss: 0.6613223552703857\n",
      "cnt: 0 - valLoss: 0.6620361804962158 - trainLoss: 0.6613212823867798\n",
      "cnt: 0 - valLoss: 0.662034809589386 - trainLoss: 0.6613202691078186\n",
      "cnt: 0 - valLoss: 0.6620334982872009 - trainLoss: 0.6613191962242126\n",
      "cnt: 0 - valLoss: 0.6620321869850159 - trainLoss: 0.6613181233406067\n",
      "cnt: 0 - valLoss: 0.6620308756828308 - trainLoss: 0.6613171100616455\n",
      "cnt: 0 - valLoss: 0.662029504776001 - trainLoss: 0.6613160371780396\n",
      "cnt: 0 - valLoss: 0.6620281934738159 - trainLoss: 0.6613150238990784\n",
      "cnt: 0 - valLoss: 0.6620268225669861 - trainLoss: 0.6613139510154724\n",
      "cnt: 0 - valLoss: 0.662025511264801 - trainLoss: 0.6613128185272217\n",
      "cnt: 0 - valLoss: 0.6620241403579712 - trainLoss: 0.6613118052482605\n",
      "cnt: 0 - valLoss: 0.6620227694511414 - trainLoss: 0.6613108515739441\n",
      "cnt: 0 - valLoss: 0.6620214581489563 - trainLoss: 0.6613097190856934\n",
      "cnt: 0 - valLoss: 0.6620200872421265 - trainLoss: 0.6613087058067322\n",
      "cnt: 0 - valLoss: 0.6620187759399414 - trainLoss: 0.6613076329231262\n",
      "cnt: 0 - valLoss: 0.6620174050331116 - trainLoss: 0.6613065600395203\n",
      "cnt: 0 - valLoss: 0.6620160341262817 - trainLoss: 0.6613055467605591\n",
      "cnt: 0 - valLoss: 0.6620147228240967 - trainLoss: 0.6613044738769531\n",
      "cnt: 0 - valLoss: 0.6620133519172668 - trainLoss: 0.6613034009933472\n",
      "cnt: 0 - valLoss: 0.662011981010437 - trainLoss: 0.6613023281097412\n",
      "cnt: 0 - valLoss: 0.6620106101036072 - trainLoss: 0.6613012552261353\n",
      "cnt: 0 - valLoss: 0.6620093584060669 - trainLoss: 0.6613002419471741\n",
      "cnt: 0 - valLoss: 0.6620079278945923 - trainLoss: 0.6612991690635681\n",
      "cnt: 0 - valLoss: 0.6620065569877625 - trainLoss: 0.6612980961799622\n",
      "cnt: 0 - valLoss: 0.6620052456855774 - trainLoss: 0.6612970232963562\n",
      "cnt: 0 - valLoss: 0.6620038747787476 - trainLoss: 0.6612959504127502\n",
      "cnt: 0 - valLoss: 0.6620025038719177 - trainLoss: 0.6612949371337891\n",
      "cnt: 0 - valLoss: 0.6620011329650879 - trainLoss: 0.6612938642501831\n",
      "cnt: 0 - valLoss: 0.6619997620582581 - trainLoss: 0.6612928509712219\n",
      "cnt: 0 - valLoss: 0.661998450756073 - trainLoss: 0.661291778087616\n",
      "cnt: 0 - valLoss: 0.6619970798492432 - trainLoss: 0.6612906455993652\n",
      "cnt: 0 - valLoss: 0.6619957089424133 - trainLoss: 0.661289632320404\n",
      "cnt: 0 - valLoss: 0.6619943380355835 - trainLoss: 0.6612885594367981\n",
      "cnt: 0 - valLoss: 0.6619929671287537 - trainLoss: 0.6612874865531921\n",
      "cnt: 0 - valLoss: 0.6619916558265686 - trainLoss: 0.6612864136695862\n",
      "cnt: 0 - valLoss: 0.6619902849197388 - trainLoss: 0.661285400390625\n",
      "cnt: 0 - valLoss: 0.6619889140129089 - trainLoss: 0.661284327507019\n",
      "cnt: 0 - valLoss: 0.6619875431060791 - trainLoss: 0.6612831950187683\n",
      "cnt: 0 - valLoss: 0.661986231803894 - trainLoss: 0.6612821817398071\n",
      "cnt: 0 - valLoss: 0.661984920501709 - trainLoss: 0.661281168460846\n",
      "cnt: 0 - valLoss: 0.6619835495948792 - trainLoss: 0.66128009557724\n",
      "cnt: 0 - valLoss: 0.6619822382926941 - trainLoss: 0.6612790822982788\n",
      "cnt: 0 - valLoss: 0.6619808673858643 - trainLoss: 0.6612779498100281\n",
      "cnt: 0 - valLoss: 0.6619795560836792 - trainLoss: 0.6612769365310669\n",
      "cnt: 0 - valLoss: 0.6619782447814941 - trainLoss: 0.6612758636474609\n",
      "cnt: 0 - valLoss: 0.6619768142700195 - trainLoss: 0.6612748503684998\n",
      "cnt: 0 - valLoss: 0.6619755029678345 - trainLoss: 0.6612737774848938\n",
      "cnt: 0 - valLoss: 0.6619741320610046 - trainLoss: 0.6612727046012878\n",
      "cnt: 0 - valLoss: 0.6619728207588196 - trainLoss: 0.6612716317176819\n",
      "cnt: 0 - valLoss: 0.6619714498519897 - trainLoss: 0.6612706184387207\n",
      "cnt: 0 - valLoss: 0.6619700789451599 - trainLoss: 0.6612695455551147\n",
      "cnt: 0 - valLoss: 0.6619687676429749 - trainLoss: 0.6612684726715088\n",
      "cnt: 0 - valLoss: 0.661967396736145 - trainLoss: 0.6612673997879028\n",
      "cnt: 0 - valLoss: 0.6619660258293152 - trainLoss: 0.6612663269042969\n",
      "cnt: 0 - valLoss: 0.6619647741317749 - trainLoss: 0.6612652540206909\n",
      "cnt: 0 - valLoss: 0.6619632840156555 - trainLoss: 0.6612643003463745\n",
      "cnt: 0 - valLoss: 0.6619619727134705 - trainLoss: 0.6612631678581238\n",
      "cnt: 0 - valLoss: 0.6619605422019958 - trainLoss: 0.6612621545791626\n",
      "cnt: 0 - valLoss: 0.6619592905044556 - trainLoss: 0.6612611413002014\n",
      "cnt: 0 - valLoss: 0.6619579195976257 - trainLoss: 0.661260187625885\n",
      "cnt: 0 - valLoss: 0.6619566082954407 - trainLoss: 0.6612591743469238\n",
      "cnt: 0 - valLoss: 0.6619553565979004 - trainLoss: 0.6612581610679626\n",
      "cnt: 0 - valLoss: 0.6619539856910706 - trainLoss: 0.6612572073936462\n",
      "cnt: 0 - valLoss: 0.6619526743888855 - trainLoss: 0.6612561345100403\n",
      "cnt: 0 - valLoss: 0.6619514226913452 - trainLoss: 0.6612551212310791\n",
      "cnt: 0 - valLoss: 0.6619499921798706 - trainLoss: 0.6612541079521179\n",
      "cnt: 0 - valLoss: 0.6619487404823303 - trainLoss: 0.6612531542778015\n",
      "cnt: 0 - valLoss: 0.6619474291801453 - trainLoss: 0.6612521409988403\n",
      "cnt: 0 - valLoss: 0.6619460582733154 - trainLoss: 0.6612511873245239\n",
      "cnt: 0 - valLoss: 0.6619448065757751 - trainLoss: 0.661250114440918\n",
      "cnt: 0 - valLoss: 0.6619434356689453 - trainLoss: 0.6612491607666016\n",
      "cnt: 0 - valLoss: 0.6619421243667603 - trainLoss: 0.6612481474876404\n",
      "cnt: 0 - valLoss: 0.66194087266922 - trainLoss: 0.6612470746040344\n",
      "cnt: 0 - valLoss: 0.6619394421577454 - trainLoss: 0.661246120929718\n",
      "cnt: 0 - valLoss: 0.6619381904602051 - trainLoss: 0.6612451076507568\n",
      "cnt: 0 - valLoss: 0.66193687915802 - trainLoss: 0.6612440943717957\n",
      "cnt: 0 - valLoss: 0.6619355082511902 - trainLoss: 0.6612430810928345\n",
      "cnt: 0 - valLoss: 0.6619341969490051 - trainLoss: 0.6612420678138733\n",
      "cnt: 0 - valLoss: 0.6619328856468201 - trainLoss: 0.6612411141395569\n",
      "cnt: 0 - valLoss: 0.6619315147399902 - trainLoss: 0.6612401008605957\n",
      "cnt: 0 - valLoss: 0.6619302034378052 - trainLoss: 0.6612390279769897\n",
      "cnt: 0 - valLoss: 0.6619288325309753 - trainLoss: 0.6612380146980286\n",
      "cnt: 0 - valLoss: 0.6619275808334351 - trainLoss: 0.6612370610237122\n",
      "cnt: 0 - valLoss: 0.6619262099266052 - trainLoss: 0.661236047744751\n",
      "cnt: 0 - valLoss: 0.6619248986244202 - trainLoss: 0.6612350344657898\n",
      "cnt: 0 - valLoss: 0.6619235277175903 - trainLoss: 0.6612339615821838\n",
      "cnt: 0 - valLoss: 0.6619222164154053 - trainLoss: 0.6612330079078674\n",
      "cnt: 0 - valLoss: 0.6619208455085754 - trainLoss: 0.6612319350242615\n",
      "cnt: 0 - valLoss: 0.6619194746017456 - trainLoss: 0.6612309217453003\n",
      "cnt: 0 - valLoss: 0.6619182229042053 - trainLoss: 0.6612299084663391\n",
      "cnt: 0 - valLoss: 0.6619168519973755 - trainLoss: 0.6612288951873779\n",
      "cnt: 0 - valLoss: 0.6619155406951904 - trainLoss: 0.6612278819084167\n",
      "cnt: 0 - valLoss: 0.6619141697883606 - trainLoss: 0.6612268686294556\n",
      "cnt: 0 - valLoss: 0.6619127988815308 - trainLoss: 0.6612258553504944\n",
      "cnt: 0 - valLoss: 0.6619115471839905 - trainLoss: 0.6612247824668884\n",
      "cnt: 0 - valLoss: 0.6619101762771606 - trainLoss: 0.661223828792572\n",
      "cnt: 0 - valLoss: 0.6619088053703308 - trainLoss: 0.6612227559089661\n",
      "cnt: 0 - valLoss: 0.6619074940681458 - trainLoss: 0.6612217426300049\n",
      "cnt: 0 - valLoss: 0.6619061231613159 - trainLoss: 0.6612207293510437\n",
      "cnt: 0 - valLoss: 0.6619046926498413 - trainLoss: 0.6612197160720825\n",
      "cnt: 0 - valLoss: 0.661903440952301 - trainLoss: 0.6612186431884766\n",
      "cnt: 0 - valLoss: 0.6619020700454712 - trainLoss: 0.6612176299095154\n",
      "cnt: 0 - valLoss: 0.6619006991386414 - trainLoss: 0.6612166166305542\n",
      "cnt: 0 - valLoss: 0.6618993282318115 - trainLoss: 0.661215603351593\n",
      "cnt: 0 - valLoss: 0.6618979573249817 - trainLoss: 0.6612145900726318\n",
      "cnt: 0 - valLoss: 0.6618966460227966 - trainLoss: 0.6612135171890259\n",
      "cnt: 0 - valLoss: 0.6618953347206116 - trainLoss: 0.6612125635147095\n",
      "cnt: 0 - valLoss: 0.661893904209137 - trainLoss: 0.6612114906311035\n",
      "cnt: 0 - valLoss: 0.6618926525115967 - trainLoss: 0.6612104773521423\n",
      "cnt: 0 - valLoss: 0.6618912220001221 - trainLoss: 0.6612095236778259\n",
      "cnt: 0 - valLoss: 0.6618898510932922 - trainLoss: 0.6612083911895752\n",
      "cnt: 0 - valLoss: 0.6618884801864624 - trainLoss: 0.661207377910614\n",
      "cnt: 0 - valLoss: 0.6618871092796326 - trainLoss: 0.6612063646316528\n",
      "cnt: 0 - valLoss: 0.6618858575820923 - trainLoss: 0.6612053513526917\n",
      "cnt: 0 - valLoss: 0.6618844866752625 - trainLoss: 0.6612042784690857\n",
      "cnt: 0 - valLoss: 0.6618830561637878 - trainLoss: 0.6612032651901245\n",
      "cnt: 0 - valLoss: 0.6618817448616028 - trainLoss: 0.6612022519111633\n",
      "cnt: 0 - valLoss: 0.6618804335594177 - trainLoss: 0.6612012386322021\n",
      "cnt: 0 - valLoss: 0.6618790626525879 - trainLoss: 0.6612001657485962\n",
      "cnt: 0 - valLoss: 0.6618776917457581 - trainLoss: 0.661199152469635\n",
      "cnt: 0 - valLoss: 0.6618763208389282 - trainLoss: 0.6611981391906738\n",
      "cnt: 0 - valLoss: 0.6618749499320984 - trainLoss: 0.6611971259117126\n",
      "cnt: 0 - valLoss: 0.6618735790252686 - trainLoss: 0.6611961126327515\n",
      "cnt: 0 - valLoss: 0.661872148513794 - trainLoss: 0.6611949801445007\n",
      "cnt: 0 - valLoss: 0.6618708968162537 - trainLoss: 0.6611939668655396\n",
      "cnt: 0 - valLoss: 0.6618695259094238 - trainLoss: 0.6611930131912231\n",
      "cnt: 0 - valLoss: 0.6618680953979492 - trainLoss: 0.6611919403076172\n",
      "cnt: 0 - valLoss: 0.6618667840957642 - trainLoss: 0.6611908674240112\n",
      "cnt: 0 - valLoss: 0.6618654131889343 - trainLoss: 0.6611899137496948\n",
      "cnt: 0 - valLoss: 0.6618640422821045 - trainLoss: 0.6611887812614441\n",
      "cnt: 0 - valLoss: 0.6618627309799194 - trainLoss: 0.6611878275871277\n",
      "cnt: 0 - valLoss: 0.6618613004684448 - trainLoss: 0.6611868143081665\n",
      "cnt: 0 - valLoss: 0.661859929561615 - trainLoss: 0.6611857414245605\n",
      "cnt: 0 - valLoss: 0.6618585586547852 - trainLoss: 0.6611846685409546\n",
      "cnt: 0 - valLoss: 0.6618571281433105 - trainLoss: 0.6611836552619934\n",
      "cnt: 0 - valLoss: 0.6618558168411255 - trainLoss: 0.6611825823783875\n",
      "cnt: 0 - valLoss: 0.6618544459342957 - trainLoss: 0.661181628704071\n",
      "cnt: 0 - valLoss: 0.6618530750274658 - trainLoss: 0.6611805558204651\n",
      "cnt: 0 - valLoss: 0.661851704120636 - trainLoss: 0.6611794829368591\n",
      "cnt: 0 - valLoss: 0.6618503332138062 - trainLoss: 0.661178469657898\n",
      "cnt: 0 - valLoss: 0.6618489623069763 - trainLoss: 0.6611774563789368\n",
      "cnt: 0 - valLoss: 0.6618475914001465 - trainLoss: 0.6611764430999756\n",
      "cnt: 0 - valLoss: 0.6618462204933167 - trainLoss: 0.6611753702163696\n",
      "cnt: 0 - valLoss: 0.6618448495864868 - trainLoss: 0.6611742973327637\n",
      "cnt: 0 - valLoss: 0.661843478679657 - trainLoss: 0.6611732840538025\n",
      "cnt: 0 - valLoss: 0.6618420481681824 - trainLoss: 0.6611722111701965\n",
      "cnt: 0 - valLoss: 0.6618407368659973 - trainLoss: 0.6611711978912354\n",
      "cnt: 0 - valLoss: 0.6618393063545227 - trainLoss: 0.6611700654029846\n",
      "cnt: 0 - valLoss: 0.6618379354476929 - trainLoss: 0.6611690521240234\n",
      "cnt: 0 - valLoss: 0.6618365049362183 - trainLoss: 0.6611679792404175\n",
      "cnt: 0 - valLoss: 0.6618350744247437 - trainLoss: 0.6611669063568115\n",
      "cnt: 0 - valLoss: 0.6618337035179138 - trainLoss: 0.6611658334732056\n",
      "cnt: 0 - valLoss: 0.6618322730064392 - trainLoss: 0.6611647605895996\n",
      "cnt: 0 - valLoss: 0.6618308424949646 - trainLoss: 0.6611635684967041\n",
      "cnt: 0 - valLoss: 0.66182941198349 - trainLoss: 0.6611624360084534\n",
      "cnt: 0 - valLoss: 0.6618280410766602 - trainLoss: 0.6611613035202026\n",
      "cnt: 0 - valLoss: 0.6618265509605408 - trainLoss: 0.6611601114273071\n",
      "cnt: 0 - valLoss: 0.6618251204490662 - trainLoss: 0.6611590385437012\n",
      "cnt: 0 - valLoss: 0.6618237495422363 - trainLoss: 0.6611579656600952\n",
      "cnt: 0 - valLoss: 0.6618222594261169 - trainLoss: 0.6611567735671997\n",
      "cnt: 0 - valLoss: 0.6618208885192871 - trainLoss: 0.661155641078949\n",
      "cnt: 0 - valLoss: 0.6618194580078125 - trainLoss: 0.6611545085906982\n",
      "cnt: 0 - valLoss: 0.6618180274963379 - trainLoss: 0.6611533761024475\n",
      "cnt: 0 - valLoss: 0.6618166565895081 - trainLoss: 0.6611522436141968\n",
      "cnt: 0 - valLoss: 0.6618151068687439 - trainLoss: 0.661151111125946\n",
      "cnt: 0 - valLoss: 0.6618137359619141 - trainLoss: 0.6611499190330505\n",
      "cnt: 0 - valLoss: 0.6618122458457947 - trainLoss: 0.6611488461494446\n",
      "cnt: 0 - valLoss: 0.6618108749389648 - trainLoss: 0.6611477136611938\n",
      "cnt: 0 - valLoss: 0.6618094444274902 - trainLoss: 0.6611465811729431\n",
      "cnt: 0 - valLoss: 0.6618080139160156 - trainLoss: 0.6611454486846924\n",
      "cnt: 0 - valLoss: 0.661806583404541 - trainLoss: 0.6611442565917969\n",
      "cnt: 0 - valLoss: 0.6618050932884216 - trainLoss: 0.6611431837081909\n",
      "cnt: 0 - valLoss: 0.6618037223815918 - trainLoss: 0.6611419916152954\n",
      "cnt: 0 - valLoss: 0.6618022918701172 - trainLoss: 0.6611409187316895\n",
      "cnt: 0 - valLoss: 0.6618008613586426 - trainLoss: 0.661139726638794\n",
      "cnt: 0 - valLoss: 0.6617993712425232 - trainLoss: 0.661138653755188\n",
      "cnt: 0 - valLoss: 0.6617980003356934 - trainLoss: 0.6611374616622925\n",
      "cnt: 0 - valLoss: 0.6617965698242188 - trainLoss: 0.6611363291740417\n",
      "cnt: 0 - valLoss: 0.6617950797080994 - trainLoss: 0.661135196685791\n",
      "cnt: 0 - valLoss: 0.6617937684059143 - trainLoss: 0.6611340045928955\n",
      "cnt: 0 - valLoss: 0.6617922186851501 - trainLoss: 0.6611328721046448\n",
      "cnt: 0 - valLoss: 0.6617907881736755 - trainLoss: 0.6611317992210388\n",
      "cnt: 0 - valLoss: 0.6617892980575562 - trainLoss: 0.6611306071281433\n",
      "cnt: 0 - valLoss: 0.6617879271507263 - trainLoss: 0.6611294746398926\n",
      "cnt: 0 - valLoss: 0.6617865562438965 - trainLoss: 0.6611282825469971\n",
      "cnt: 0 - valLoss: 0.6617850065231323 - trainLoss: 0.6611272096633911\n",
      "cnt: 0 - valLoss: 0.6617835760116577 - trainLoss: 0.6611260771751404\n",
      "cnt: 0 - valLoss: 0.6617821455001831 - trainLoss: 0.6611248850822449\n",
      "cnt: 0 - valLoss: 0.661780595779419 - trainLoss: 0.6611236929893494\n",
      "cnt: 0 - valLoss: 0.6617792248725891 - trainLoss: 0.6611225605010986\n",
      "cnt: 0 - valLoss: 0.6617777943611145 - trainLoss: 0.6611214280128479\n",
      "cnt: 0 - valLoss: 0.6617763638496399 - trainLoss: 0.6611202359199524\n",
      "cnt: 0 - valLoss: 0.6617748737335205 - trainLoss: 0.6611191034317017\n",
      "cnt: 0 - valLoss: 0.6617734432220459 - trainLoss: 0.6611180305480957\n",
      "cnt: 0 - valLoss: 0.6617720723152161 - trainLoss: 0.6611167788505554\n",
      "cnt: 0 - valLoss: 0.6617705821990967 - trainLoss: 0.6611157059669495\n",
      "cnt: 0 - valLoss: 0.6617692112922668 - trainLoss: 0.661114513874054\n",
      "cnt: 0 - valLoss: 0.6617677807807922 - trainLoss: 0.6611133813858032\n",
      "cnt: 0 - valLoss: 0.6617664098739624 - trainLoss: 0.661112368106842\n",
      "cnt: 0 - valLoss: 0.6617651581764221 - trainLoss: 0.6611112356185913\n",
      "cnt: 0 - valLoss: 0.6617636680603027 - trainLoss: 0.6611101627349854\n",
      "cnt: 0 - valLoss: 0.6617622971534729 - trainLoss: 0.6611090302467346\n",
      "cnt: 0 - valLoss: 0.6617609858512878 - trainLoss: 0.6611079573631287\n",
      "cnt: 0 - valLoss: 0.6617596745491028 - trainLoss: 0.6611068844795227\n",
      "cnt: 0 - valLoss: 0.6617581844329834 - trainLoss: 0.6611058115959167\n",
      "cnt: 0 - valLoss: 0.6617568135261536 - trainLoss: 0.6611047387123108\n",
      "cnt: 0 - valLoss: 0.6617555022239685 - trainLoss: 0.6611036062240601\n",
      "cnt: 0 - valLoss: 0.6617540717124939 - trainLoss: 0.6611025333404541\n",
      "cnt: 0 - valLoss: 0.6617527008056641 - trainLoss: 0.6611014008522034\n",
      "cnt: 0 - valLoss: 0.661751389503479 - trainLoss: 0.6611002683639526\n",
      "cnt: 0 - valLoss: 0.6617500185966492 - trainLoss: 0.6610992550849915\n",
      "cnt: 0 - valLoss: 0.6617485880851746 - trainLoss: 0.6610981225967407\n",
      "cnt: 0 - valLoss: 0.6617472171783447 - trainLoss: 0.6610970497131348\n",
      "cnt: 0 - valLoss: 0.6617459654808044 - trainLoss: 0.6610959768295288\n",
      "cnt: 0 - valLoss: 0.6617444753646851 - trainLoss: 0.6610947847366333\n",
      "cnt: 0 - valLoss: 0.6617431044578552 - trainLoss: 0.6610937118530273\n",
      "cnt: 0 - valLoss: 0.6617417931556702 - trainLoss: 0.6610926389694214\n",
      "cnt: 0 - valLoss: 0.6617403030395508 - trainLoss: 0.6610915064811707\n",
      "cnt: 0 - valLoss: 0.661738932132721 - trainLoss: 0.6610903739929199\n",
      "cnt: 0 - valLoss: 0.6617376208305359 - trainLoss: 0.661089301109314\n",
      "cnt: 0 - valLoss: 0.6617361307144165 - trainLoss: 0.661088228225708\n",
      "cnt: 0 - valLoss: 0.6617347598075867 - trainLoss: 0.6610870361328125\n",
      "cnt: 0 - valLoss: 0.6617334485054016 - trainLoss: 0.6610859632492065\n",
      "cnt: 0 - valLoss: 0.661732017993927 - trainLoss: 0.6610848903656006\n",
      "cnt: 0 - valLoss: 0.6617305874824524 - trainLoss: 0.6610837578773499\n",
      "cnt: 0 - valLoss: 0.6617293357849121 - trainLoss: 0.6610826253890991\n",
      "cnt: 0 - valLoss: 0.6617278456687927 - trainLoss: 0.6610816121101379\n",
      "cnt: 0 - valLoss: 0.6617264747619629 - trainLoss: 0.6610804200172424\n",
      "cnt: 0 - valLoss: 0.6617251038551331 - trainLoss: 0.6610792875289917\n",
      "cnt: 0 - valLoss: 0.6617237329483032 - trainLoss: 0.6610782742500305\n",
      "cnt: 0 - valLoss: 0.6617223024368286 - trainLoss: 0.6610771417617798\n",
      "cnt: 0 - valLoss: 0.661720871925354 - trainLoss: 0.661076009273529\n",
      "cnt: 0 - valLoss: 0.661719560623169 - trainLoss: 0.6610748767852783\n",
      "cnt: 0 - valLoss: 0.6617181301116943 - trainLoss: 0.6610738039016724\n",
      "cnt: 0 - valLoss: 0.6617166996002197 - trainLoss: 0.6610726714134216\n",
      "cnt: 0 - valLoss: 0.6617153882980347 - trainLoss: 0.6610715985298157\n",
      "cnt: 0 - valLoss: 0.6617138981819153 - trainLoss: 0.6610704660415649\n",
      "cnt: 0 - valLoss: 0.6617125272750854 - trainLoss: 0.6610692739486694\n",
      "cnt: 0 - valLoss: 0.6617112159729004 - trainLoss: 0.6610682606697083\n",
      "cnt: 0 - valLoss: 0.661709725856781 - trainLoss: 0.6610671281814575\n",
      "cnt: 0 - valLoss: 0.6617082953453064 - trainLoss: 0.6610659956932068\n",
      "cnt: 0 - valLoss: 0.6617069840431213 - trainLoss: 0.661064863204956\n",
      "cnt: 0 - valLoss: 0.6617055535316467 - trainLoss: 0.6610637903213501\n",
      "cnt: 0 - valLoss: 0.6617041230201721 - trainLoss: 0.6610626578330994\n",
      "cnt: 0 - valLoss: 0.6617028117179871 - trainLoss: 0.6610615253448486\n",
      "cnt: 0 - valLoss: 0.6617013216018677 - trainLoss: 0.6610604524612427\n",
      "cnt: 0 - valLoss: 0.6616998910903931 - trainLoss: 0.6610592603683472\n",
      "cnt: 0 - valLoss: 0.6616985201835632 - trainLoss: 0.6610581874847412\n",
      "cnt: 0 - valLoss: 0.6616971492767334 - trainLoss: 0.6610570549964905\n",
      "cnt: 0 - valLoss: 0.661695659160614 - trainLoss: 0.6610559225082397\n",
      "cnt: 0 - valLoss: 0.6616942882537842 - trainLoss: 0.6610548496246338\n",
      "cnt: 0 - valLoss: 0.6616929173469543 - trainLoss: 0.6610537171363831\n",
      "cnt: 0 - valLoss: 0.6616914868354797 - trainLoss: 0.6610525846481323\n",
      "cnt: 0 - valLoss: 0.6616901159286499 - trainLoss: 0.6610514521598816\n",
      "cnt: 0 - valLoss: 0.6616886854171753 - trainLoss: 0.6610503196716309\n",
      "cnt: 0 - valLoss: 0.6616872549057007 - trainLoss: 0.6610492467880249\n",
      "cnt: 0 - valLoss: 0.6616858839988708 - trainLoss: 0.6610479950904846\n",
      "cnt: 0 - valLoss: 0.6616844534873962 - trainLoss: 0.6610469818115234\n",
      "cnt: 0 - valLoss: 0.6616830229759216 - trainLoss: 0.6610458493232727\n",
      "cnt: 0 - valLoss: 0.6616815328598022 - trainLoss: 0.6610446572303772\n",
      "cnt: 0 - valLoss: 0.6616802215576172 - trainLoss: 0.6610435247421265\n",
      "cnt: 0 - valLoss: 0.6616787314414978 - trainLoss: 0.6610423922538757\n",
      "cnt: 0 - valLoss: 0.661677360534668 - trainLoss: 0.661041259765625\n",
      "cnt: 0 - valLoss: 0.6616759300231934 - trainLoss: 0.6610401272773743\n",
      "cnt: 0 - valLoss: 0.6616745591163635 - trainLoss: 0.6610390543937683\n",
      "cnt: 0 - valLoss: 0.6616730690002441 - trainLoss: 0.6610379219055176\n",
      "cnt: 0 - valLoss: 0.6616717576980591 - trainLoss: 0.6610367894172668\n",
      "cnt: 0 - valLoss: 0.6616703271865845 - trainLoss: 0.6610357165336609\n",
      "cnt: 0 - valLoss: 0.6616687774658203 - trainLoss: 0.6610345244407654\n",
      "cnt: 0 - valLoss: 0.66166752576828 - trainLoss: 0.6610332727432251\n",
      "cnt: 0 - valLoss: 0.6616659164428711 - trainLoss: 0.6610321998596191\n",
      "cnt: 0 - valLoss: 0.6616644859313965 - trainLoss: 0.6610310673713684\n",
      "cnt: 0 - valLoss: 0.6616631150245667 - trainLoss: 0.6610298752784729\n",
      "cnt: 0 - valLoss: 0.6616616249084473 - trainLoss: 0.6610287427902222\n",
      "cnt: 0 - valLoss: 0.6616602540016174 - trainLoss: 0.6610276103019714\n",
      "cnt: 0 - valLoss: 0.6616588830947876 - trainLoss: 0.6610264182090759\n",
      "cnt: 0 - valLoss: 0.6616573333740234 - trainLoss: 0.66102534532547\n",
      "cnt: 0 - valLoss: 0.6616559028625488 - trainLoss: 0.6610240936279297\n",
      "cnt: 0 - valLoss: 0.661654531955719 - trainLoss: 0.6610230207443237\n",
      "cnt: 0 - valLoss: 0.6616529822349548 - trainLoss: 0.6610217690467834\n",
      "cnt: 0 - valLoss: 0.6616515517234802 - trainLoss: 0.6610206961631775\n",
      "cnt: 0 - valLoss: 0.6616501808166504 - trainLoss: 0.661019504070282\n",
      "cnt: 0 - valLoss: 0.661648690700531 - trainLoss: 0.6610183119773865\n",
      "cnt: 0 - valLoss: 0.6616472601890564 - trainLoss: 0.6610171794891357\n",
      "cnt: 0 - valLoss: 0.6616458892822266 - trainLoss: 0.661016047000885\n",
      "cnt: 0 - valLoss: 0.6616443395614624 - trainLoss: 0.6610148549079895\n",
      "cnt: 0 - valLoss: 0.6616429686546326 - trainLoss: 0.661013662815094\n",
      "cnt: 0 - valLoss: 0.6616414785385132 - trainLoss: 0.6610125303268433\n",
      "cnt: 0 - valLoss: 0.6616400480270386 - trainLoss: 0.6610113978385925\n",
      "cnt: 0 - valLoss: 0.6616387367248535 - trainLoss: 0.661010205745697\n",
      "cnt: 0 - valLoss: 0.6616372466087341 - trainLoss: 0.6610090732574463\n",
      "cnt: 0 - valLoss: 0.6616357564926147 - trainLoss: 0.6610078811645508\n",
      "cnt: 0 - valLoss: 0.6616344451904297 - trainLoss: 0.6610066890716553\n",
      "cnt: 0 - valLoss: 0.6616328954696655 - trainLoss: 0.6610055565834045\n",
      "cnt: 0 - valLoss: 0.6616314053535461 - trainLoss: 0.661004364490509\n",
      "cnt: 0 - valLoss: 0.6616300344467163 - trainLoss: 0.6610031723976135\n",
      "cnt: 0 - valLoss: 0.6616286039352417 - trainLoss: 0.6610020399093628\n",
      "cnt: 0 - valLoss: 0.6616271138191223 - trainLoss: 0.6610008478164673\n",
      "cnt: 0 - valLoss: 0.6616257429122925 - trainLoss: 0.6609997153282166\n",
      "cnt: 0 - valLoss: 0.6616242527961731 - trainLoss: 0.660998523235321\n",
      "cnt: 0 - valLoss: 0.6616227626800537 - trainLoss: 0.6609973907470703\n",
      "cnt: 0 - valLoss: 0.6616213321685791 - trainLoss: 0.66099613904953\n",
      "cnt: 0 - valLoss: 0.6616199612617493 - trainLoss: 0.6609950065612793\n",
      "cnt: 0 - valLoss: 0.6616184115409851 - trainLoss: 0.6609938144683838\n",
      "cnt: 0 - valLoss: 0.6616169810295105 - trainLoss: 0.6609926223754883\n",
      "cnt: 0 - valLoss: 0.6616155505180359 - trainLoss: 0.660991370677948\n",
      "cnt: 0 - valLoss: 0.6616140007972717 - trainLoss: 0.6609901785850525\n",
      "cnt: 0 - valLoss: 0.6616125106811523 - trainLoss: 0.6609889268875122\n",
      "cnt: 0 - valLoss: 0.6616110801696777 - trainLoss: 0.6609876751899719\n",
      "cnt: 0 - valLoss: 0.6616097092628479 - trainLoss: 0.6609865427017212\n",
      "cnt: 0 - valLoss: 0.6616082191467285 - trainLoss: 0.6609854698181152\n",
      "cnt: 0 - valLoss: 0.6616069078445435 - trainLoss: 0.6609843969345093\n",
      "cnt: 0 - valLoss: 0.6616054773330688 - trainLoss: 0.6609833836555481\n",
      "cnt: 0 - valLoss: 0.661604106426239 - trainLoss: 0.6609823107719421\n",
      "cnt: 0 - valLoss: 0.6616026759147644 - trainLoss: 0.6609812378883362\n",
      "cnt: 0 - valLoss: 0.6616013050079346 - trainLoss: 0.6609801650047302\n",
      "cnt: 0 - valLoss: 0.66159987449646 - trainLoss: 0.6609790921211243\n",
      "cnt: 0 - valLoss: 0.6615985035896301 - trainLoss: 0.6609780192375183\n",
      "cnt: 0 - valLoss: 0.6615970730781555 - trainLoss: 0.6609769463539124\n",
      "cnt: 0 - valLoss: 0.6615957021713257 - trainLoss: 0.6609758734703064\n",
      "cnt: 0 - valLoss: 0.6615942716598511 - trainLoss: 0.6609748601913452\n",
      "cnt: 0 - valLoss: 0.6615929007530212 - trainLoss: 0.6609737873077393\n",
      "cnt: 0 - valLoss: 0.6615914106369019 - trainLoss: 0.6609727144241333\n",
      "cnt: 0 - valLoss: 0.661590039730072 - trainLoss: 0.6609716415405273\n",
      "cnt: 0 - valLoss: 0.6615886688232422 - trainLoss: 0.6609705090522766\n",
      "cnt: 0 - valLoss: 0.6615872979164124 - trainLoss: 0.6609694957733154\n",
      "cnt: 0 - valLoss: 0.661585807800293 - trainLoss: 0.6609683632850647\n",
      "cnt: 0 - valLoss: 0.6615844368934631 - trainLoss: 0.6609672904014587\n",
      "cnt: 0 - valLoss: 0.6615830063819885 - trainLoss: 0.6609662175178528\n",
      "cnt: 0 - valLoss: 0.6615815758705139 - trainLoss: 0.6609652042388916\n",
      "cnt: 0 - valLoss: 0.6615802049636841 - trainLoss: 0.6609640717506409\n",
      "cnt: 0 - valLoss: 0.6615787744522095 - trainLoss: 0.6609629988670349\n",
      "cnt: 0 - valLoss: 0.6615774035453796 - trainLoss: 0.660961925983429\n",
      "cnt: 0 - valLoss: 0.6615759134292603 - trainLoss: 0.6609609127044678\n",
      "cnt: 0 - valLoss: 0.6615745425224304 - trainLoss: 0.660959780216217\n",
      "cnt: 0 - valLoss: 0.6615731120109558 - trainLoss: 0.6609587669372559\n",
      "cnt: 0 - valLoss: 0.661571741104126 - trainLoss: 0.6609576344490051\n",
      "cnt: 0 - valLoss: 0.6615702509880066 - trainLoss: 0.6609565615653992\n",
      "cnt: 0 - valLoss: 0.6615689396858215 - trainLoss: 0.6609554886817932\n",
      "cnt: 0 - valLoss: 0.6615674495697021 - trainLoss: 0.6609544157981873\n",
      "cnt: 0 - valLoss: 0.6615660786628723 - trainLoss: 0.6609532833099365\n",
      "cnt: 0 - valLoss: 0.6615645885467529 - trainLoss: 0.6609522700309753\n",
      "cnt: 0 - valLoss: 0.6615632176399231 - trainLoss: 0.6609511375427246\n",
      "cnt: 0 - valLoss: 0.6615617871284485 - trainLoss: 0.6609500646591187\n",
      "cnt: 0 - valLoss: 0.6615604162216187 - trainLoss: 0.6609490513801575\n",
      "cnt: 0 - valLoss: 0.6615589261054993 - trainLoss: 0.6609479188919067\n",
      "cnt: 0 - valLoss: 0.6615575551986694 - trainLoss: 0.6609469056129456\n",
      "cnt: 0 - valLoss: 0.6615561842918396 - trainLoss: 0.66094571352005\n",
      "cnt: 0 - valLoss: 0.6615546345710754 - trainLoss: 0.6609447002410889\n",
      "cnt: 0 - valLoss: 0.6615533232688904 - trainLoss: 0.6609435677528381\n",
      "cnt: 0 - valLoss: 0.661551833152771 - trainLoss: 0.6609424948692322\n",
      "cnt: 0 - valLoss: 0.6615504622459412 - trainLoss: 0.6609414219856262\n",
      "cnt: 0 - valLoss: 0.6615489721298218 - trainLoss: 0.6609402894973755\n",
      "cnt: 0 - valLoss: 0.6615476012229919 - trainLoss: 0.6609392166137695\n",
      "cnt: 0 - valLoss: 0.6615461707115173 - trainLoss: 0.6609381437301636\n",
      "cnt: 0 - valLoss: 0.6615447998046875 - trainLoss: 0.6609369516372681\n",
      "cnt: 0 - valLoss: 0.6615433096885681 - trainLoss: 0.6609359383583069\n",
      "cnt: 0 - valLoss: 0.6615419387817383 - trainLoss: 0.6609348654747009\n",
      "cnt: 0 - valLoss: 0.6615404486656189 - trainLoss: 0.6609337329864502\n",
      "cnt: 0 - valLoss: 0.6615390181541443 - trainLoss: 0.6609326004981995\n",
      "cnt: 0 - valLoss: 0.6615375876426697 - trainLoss: 0.6609315872192383\n",
      "cnt: 0 - valLoss: 0.6615361571311951 - trainLoss: 0.6609303951263428\n",
      "cnt: 0 - valLoss: 0.6615346670150757 - trainLoss: 0.6609293222427368\n",
      "cnt: 0 - valLoss: 0.6615332961082458 - trainLoss: 0.6609282493591309\n",
      "cnt: 0 - valLoss: 0.6615318655967712 - trainLoss: 0.6609271764755249\n",
      "cnt: 0 - valLoss: 0.6615303754806519 - trainLoss: 0.660926103591919\n",
      "cnt: 0 - valLoss: 0.661529004573822 - trainLoss: 0.6609249711036682\n",
      "cnt: 0 - valLoss: 0.6615275740623474 - trainLoss: 0.6609238386154175\n",
      "cnt: 0 - valLoss: 0.661526083946228 - trainLoss: 0.6609227657318115\n",
      "cnt: 0 - valLoss: 0.6615246534347534 - trainLoss: 0.6609216928482056\n",
      "cnt: 0 - valLoss: 0.6615233421325684 - trainLoss: 0.6609205007553101\n",
      "cnt: 0 - valLoss: 0.661521852016449 - trainLoss: 0.6609194874763489\n",
      "cnt: 0 - valLoss: 0.6615204215049744 - trainLoss: 0.6609183549880981\n",
      "cnt: 0 - valLoss: 0.661518931388855 - trainLoss: 0.6609172821044922\n",
      "cnt: 0 - valLoss: 0.6615175604820251 - trainLoss: 0.6609161496162415\n",
      "cnt: 0 - valLoss: 0.6615160703659058 - trainLoss: 0.6609150767326355\n",
      "cnt: 0 - valLoss: 0.6615146398544312 - trainLoss: 0.6609139442443848\n",
      "cnt: 0 - valLoss: 0.6615132093429565 - trainLoss: 0.6609128713607788\n",
      "cnt: 0 - valLoss: 0.6615117788314819 - trainLoss: 0.6609117984771729\n",
      "cnt: 0 - valLoss: 0.6615103483200073 - trainLoss: 0.6609106659889221\n",
      "cnt: 0 - valLoss: 0.6615088582038879 - trainLoss: 0.6609095931053162\n",
      "cnt: 0 - valLoss: 0.6615074872970581 - trainLoss: 0.6609084606170654\n",
      "cnt: 0 - valLoss: 0.661505937576294 - trainLoss: 0.6609074473381042\n",
      "cnt: 0 - valLoss: 0.6615045666694641 - trainLoss: 0.6609062552452087\n",
      "cnt: 0 - valLoss: 0.6615030765533447 - trainLoss: 0.6609051823616028\n",
      "cnt: 0 - valLoss: 0.6615017056465149 - trainLoss: 0.6609041094779968\n",
      "cnt: 0 - valLoss: 0.6615002751350403 - trainLoss: 0.6609030365943909\n",
      "cnt: 0 - valLoss: 0.6614988446235657 - trainLoss: 0.6609018445014954\n",
      "cnt: 0 - valLoss: 0.6614972949028015 - trainLoss: 0.6609007716178894\n",
      "cnt: 0 - valLoss: 0.6614959239959717 - trainLoss: 0.6608996391296387\n",
      "cnt: 0 - valLoss: 0.6614944338798523 - trainLoss: 0.6608985662460327\n",
      "cnt: 0 - valLoss: 0.6614930033683777 - trainLoss: 0.660897433757782\n",
      "cnt: 0 - valLoss: 0.6614915728569031 - trainLoss: 0.660896360874176\n",
      "cnt: 0 - valLoss: 0.661490261554718 - trainLoss: 0.6608952879905701\n",
      "cnt: 0 - valLoss: 0.6614887118339539 - trainLoss: 0.6608941555023193\n",
      "cnt: 0 - valLoss: 0.6614874601364136 - trainLoss: 0.6608930826187134\n",
      "cnt: 0 - valLoss: 0.661486029624939 - trainLoss: 0.6608919501304626\n",
      "cnt: 0 - valLoss: 0.6614845991134644 - trainLoss: 0.6608908772468567\n",
      "cnt: 0 - valLoss: 0.6614832878112793 - trainLoss: 0.6608898639678955\n",
      "cnt: 0 - valLoss: 0.6614819169044495 - trainLoss: 0.6608887314796448\n",
      "cnt: 0 - valLoss: 0.6614806056022644 - trainLoss: 0.6608877182006836\n",
      "cnt: 0 - valLoss: 0.6614792346954346 - trainLoss: 0.6608866453170776\n",
      "cnt: 0 - valLoss: 0.6614778637886047 - trainLoss: 0.6608855128288269\n",
      "cnt: 0 - valLoss: 0.6614764928817749 - trainLoss: 0.6608844995498657\n",
      "cnt: 0 - valLoss: 0.6614751219749451 - trainLoss: 0.6608834266662598\n",
      "cnt: 0 - valLoss: 0.66147381067276 - trainLoss: 0.6608823537826538\n",
      "cnt: 0 - valLoss: 0.6614724397659302 - trainLoss: 0.6608812808990479\n",
      "cnt: 0 - valLoss: 0.6614711284637451 - trainLoss: 0.6608802080154419\n",
      "cnt: 0 - valLoss: 0.6614698171615601 - trainLoss: 0.6608791351318359\n",
      "cnt: 0 - valLoss: 0.661468505859375 - trainLoss: 0.66087806224823\n",
      "cnt: 0 - valLoss: 0.6614670753479004 - trainLoss: 0.6608769297599792\n",
      "cnt: 0 - valLoss: 0.6614657044410706 - trainLoss: 0.6608759164810181\n",
      "cnt: 0 - valLoss: 0.6614643335342407 - trainLoss: 0.6608747839927673\n",
      "cnt: 0 - valLoss: 0.6614630222320557 - trainLoss: 0.6608737707138062\n",
      "cnt: 0 - valLoss: 0.6614616513252258 - trainLoss: 0.6608726382255554\n",
      "cnt: 0 - valLoss: 0.6614603400230408 - trainLoss: 0.6608715653419495\n",
      "cnt: 0 - valLoss: 0.6614589691162109 - trainLoss: 0.6608705520629883\n",
      "cnt: 0 - valLoss: 0.6614575982093811 - trainLoss: 0.6608694791793823\n",
      "cnt: 0 - valLoss: 0.6614562273025513 - trainLoss: 0.6608684062957764\n",
      "cnt: 0 - valLoss: 0.661454975605011 - trainLoss: 0.6608672738075256\n",
      "cnt: 0 - valLoss: 0.6614536046981812 - trainLoss: 0.6608662605285645\n",
      "cnt: 0 - valLoss: 0.6614522337913513 - trainLoss: 0.6608651280403137\n",
      "cnt: 0 - valLoss: 0.6614508628845215 - trainLoss: 0.6608640551567078\n",
      "cnt: 0 - valLoss: 0.6614495515823364 - trainLoss: 0.660862922668457\n",
      "cnt: 0 - valLoss: 0.6614481806755066 - trainLoss: 0.6608619093894958\n",
      "cnt: 0 - valLoss: 0.6614468693733215 - trainLoss: 0.6608608365058899\n",
      "cnt: 0 - valLoss: 0.6614454388618469 - trainLoss: 0.6608597636222839\n",
      "cnt: 0 - valLoss: 0.6614441871643066 - trainLoss: 0.660858690738678\n",
      "cnt: 0 - valLoss: 0.6614428162574768 - trainLoss: 0.660857617855072\n",
      "cnt: 0 - valLoss: 0.6614415049552917 - trainLoss: 0.6608564853668213\n",
      "cnt: 0 - valLoss: 0.6614401340484619 - trainLoss: 0.6608554720878601\n",
      "cnt: 0 - valLoss: 0.6614387631416321 - trainLoss: 0.6608543992042542\n",
      "cnt: 0 - valLoss: 0.6614373922348022 - trainLoss: 0.6608532667160034\n",
      "cnt: 0 - valLoss: 0.6614360809326172 - trainLoss: 0.6608521938323975\n",
      "cnt: 0 - valLoss: 0.6614347100257874 - trainLoss: 0.6608511805534363\n",
      "cnt: 0 - valLoss: 0.6614333987236023 - trainLoss: 0.6608500480651855\n",
      "cnt: 0 - valLoss: 0.6614320874214172 - trainLoss: 0.6608489751815796\n",
      "cnt: 0 - valLoss: 0.6614307165145874 - trainLoss: 0.6608479022979736\n",
      "cnt: 0 - valLoss: 0.6614293456077576 - trainLoss: 0.6608468294143677\n",
      "cnt: 0 - valLoss: 0.6614279747009277 - trainLoss: 0.6608457565307617\n",
      "cnt: 0 - valLoss: 0.6614266037940979 - trainLoss: 0.6608446836471558\n",
      "cnt: 0 - valLoss: 0.6614252924919128 - trainLoss: 0.6608436703681946\n",
      "cnt: 0 - valLoss: 0.6614239811897278 - trainLoss: 0.6608425378799438\n",
      "cnt: 0 - valLoss: 0.6614225506782532 - trainLoss: 0.6608415246009827\n",
      "cnt: 0 - valLoss: 0.6614212393760681 - trainLoss: 0.6608404517173767\n",
      "cnt: 0 - valLoss: 0.6614199280738831 - trainLoss: 0.6608393788337708\n",
      "cnt: 0 - valLoss: 0.6614185571670532 - trainLoss: 0.6608383655548096\n",
      "cnt: 0 - valLoss: 0.6614171862602234 - trainLoss: 0.6608372926712036\n",
      "cnt: 0 - valLoss: 0.6614158153533936 - trainLoss: 0.6608362793922424\n",
      "cnt: 0 - valLoss: 0.6614145040512085 - trainLoss: 0.6608351469039917\n",
      "cnt: 0 - valLoss: 0.6614131927490234 - trainLoss: 0.6608341336250305\n",
      "cnt: 0 - valLoss: 0.6614118218421936 - trainLoss: 0.6608331203460693\n",
      "cnt: 0 - valLoss: 0.661410391330719 - trainLoss: 0.6608321070671082\n",
      "cnt: 0 - valLoss: 0.6614091396331787 - trainLoss: 0.6608310341835022\n",
      "cnt: 0 - valLoss: 0.6614077687263489 - trainLoss: 0.6608299612998962\n",
      "cnt: 0 - valLoss: 0.6614063382148743 - trainLoss: 0.6608289480209351\n",
      "cnt: 0 - valLoss: 0.6614049673080444 - trainLoss: 0.6608278751373291\n",
      "cnt: 0 - valLoss: 0.6614037156105042 - trainLoss: 0.6608268022537231\n",
      "cnt: 0 - valLoss: 0.6614023447036743 - trainLoss: 0.660825788974762\n",
      "cnt: 0 - valLoss: 0.6614009141921997 - trainLoss: 0.6608246564865112\n",
      "cnt: 0 - valLoss: 0.6613994836807251 - trainLoss: 0.66082364320755\n",
      "cnt: 0 - valLoss: 0.6613981127738953 - trainLoss: 0.6608226895332336\n",
      "cnt: 0 - valLoss: 0.661396861076355 - trainLoss: 0.6608215570449829\n",
      "cnt: 0 - valLoss: 0.6613954901695251 - trainLoss: 0.6608205437660217\n",
      "cnt: 0 - valLoss: 0.6613941192626953 - trainLoss: 0.6608195304870605\n",
      "cnt: 0 - valLoss: 0.6613927483558655 - trainLoss: 0.6608184576034546\n",
      "cnt: 0 - valLoss: 0.6613913774490356 - trainLoss: 0.6608173847198486\n",
      "cnt: 0 - valLoss: 0.6613900065422058 - trainLoss: 0.6608163118362427\n",
      "cnt: 0 - valLoss: 0.661388635635376 - trainLoss: 0.6608152389526367\n",
      "cnt: 0 - valLoss: 0.6613872647285461 - trainLoss: 0.6608142256736755\n",
      "cnt: 0 - valLoss: 0.6613858938217163 - trainLoss: 0.6608131527900696\n",
      "cnt: 0 - valLoss: 0.6613845229148865 - trainLoss: 0.6608120799064636\n",
      "cnt: 0 - valLoss: 0.6613831520080566 - trainLoss: 0.6608110070228577\n",
      "cnt: 0 - valLoss: 0.6613817811012268 - trainLoss: 0.6608099341392517\n",
      "cnt: 0 - valLoss: 0.661380410194397 - trainLoss: 0.6608089208602905\n",
      "cnt: 0 - valLoss: 0.6613790392875671 - trainLoss: 0.6608079075813293\n",
      "cnt: 0 - valLoss: 0.6613776683807373 - trainLoss: 0.6608068346977234\n",
      "cnt: 0 - valLoss: 0.6613762378692627 - trainLoss: 0.6608057022094727\n",
      "cnt: 0 - valLoss: 0.6613748669624329 - trainLoss: 0.6608046889305115\n",
      "cnt: 0 - valLoss: 0.6613735556602478 - trainLoss: 0.6608036160469055\n",
      "cnt: 0 - valLoss: 0.6613721251487732 - trainLoss: 0.6608026027679443\n",
      "cnt: 0 - valLoss: 0.6613707542419434 - trainLoss: 0.6608015298843384\n",
      "cnt: 0 - valLoss: 0.6613694429397583 - trainLoss: 0.6608004570007324\n",
      "cnt: 0 - valLoss: 0.6613680124282837 - trainLoss: 0.6607993841171265\n",
      "cnt: 0 - valLoss: 0.6613667011260986 - trainLoss: 0.6607983112335205\n",
      "cnt: 0 - valLoss: 0.661365270614624 - trainLoss: 0.6607972383499146\n",
      "cnt: 0 - valLoss: 0.6613638997077942 - trainLoss: 0.6607962250709534\n",
      "cnt: 0 - valLoss: 0.6613625288009644 - trainLoss: 0.6607950925827026\n",
      "cnt: 0 - valLoss: 0.6613611578941345 - trainLoss: 0.6607940793037415\n",
      "cnt: 0 - valLoss: 0.6613597869873047 - trainLoss: 0.6607930660247803\n",
      "cnt: 0 - valLoss: 0.6613584160804749 - trainLoss: 0.6607919335365295\n",
      "cnt: 0 - valLoss: 0.661357045173645 - trainLoss: 0.6607908606529236\n",
      "cnt: 0 - valLoss: 0.6613556742668152 - trainLoss: 0.6607898473739624\n",
      "cnt: 0 - valLoss: 0.6613543033599854 - trainLoss: 0.6607887148857117\n",
      "cnt: 0 - valLoss: 0.6613528728485107 - trainLoss: 0.6607877016067505\n",
      "cnt: 0 - valLoss: 0.6613515019416809 - trainLoss: 0.6607866287231445\n",
      "cnt: 0 - valLoss: 0.6613500118255615 - trainLoss: 0.6607855558395386\n",
      "cnt: 0 - valLoss: 0.6613487601280212 - trainLoss: 0.6607844829559326\n",
      "cnt: 0 - valLoss: 0.6613472700119019 - trainLoss: 0.6607834100723267\n",
      "cnt: 0 - valLoss: 0.661345899105072 - trainLoss: 0.6607823371887207\n",
      "cnt: 0 - valLoss: 0.6613445281982422 - trainLoss: 0.6607812643051147\n",
      "cnt: 0 - valLoss: 0.6613432765007019 - trainLoss: 0.6607801914215088\n",
      "cnt: 0 - valLoss: 0.6613417863845825 - trainLoss: 0.6607791185379028\n",
      "cnt: 0 - valLoss: 0.6613404154777527 - trainLoss: 0.6607780456542969\n",
      "cnt: 0 - valLoss: 0.6613390445709229 - trainLoss: 0.6607769727706909\n",
      "cnt: 0 - valLoss: 0.6613376140594482 - trainLoss: 0.6607759594917297\n",
      "cnt: 0 - valLoss: 0.6613363027572632 - trainLoss: 0.6607748866081238\n",
      "cnt: 0 - valLoss: 0.6613348722457886 - trainLoss: 0.6607738137245178\n",
      "cnt: 0 - valLoss: 0.6613333821296692 - trainLoss: 0.6607727408409119\n",
      "cnt: 0 - valLoss: 0.6613320112228394 - trainLoss: 0.6607716679573059\n",
      "cnt: 0 - valLoss: 0.6613306403160095 - trainLoss: 0.6607706546783447\n",
      "cnt: 0 - valLoss: 0.6613292694091797 - trainLoss: 0.660769522190094\n",
      "cnt: 0 - valLoss: 0.6613278985023499 - trainLoss: 0.6607683897018433\n",
      "cnt: 0 - valLoss: 0.6613264679908752 - trainLoss: 0.6607673764228821\n",
      "cnt: 0 - valLoss: 0.6613250374794006 - trainLoss: 0.6607662439346313\n",
      "cnt: 0 - valLoss: 0.661323606967926 - trainLoss: 0.6607651710510254\n",
      "cnt: 0 - valLoss: 0.6613222360610962 - trainLoss: 0.6607640385627747\n",
      "cnt: 0 - valLoss: 0.6613208055496216 - trainLoss: 0.6607629656791687\n",
      "cnt: 0 - valLoss: 0.6613194346427917 - trainLoss: 0.6607618927955627\n",
      "cnt: 0 - valLoss: 0.6613180637359619 - trainLoss: 0.6607608199119568\n",
      "cnt: 0 - valLoss: 0.6613166332244873 - trainLoss: 0.660759687423706\n",
      "cnt: 0 - valLoss: 0.6613152027130127 - trainLoss: 0.6607586145401001\n",
      "cnt: 0 - valLoss: 0.6613138318061829 - trainLoss: 0.6607575416564941\n",
      "cnt: 0 - valLoss: 0.661312460899353 - trainLoss: 0.660756528377533\n",
      "cnt: 0 - valLoss: 0.6613110303878784 - trainLoss: 0.6607553958892822\n",
      "cnt: 0 - valLoss: 0.6613096594810486 - trainLoss: 0.660754382610321\n",
      "cnt: 0 - valLoss: 0.6613082885742188 - trainLoss: 0.6607532501220703\n",
      "cnt: 0 - valLoss: 0.6613068580627441 - trainLoss: 0.6607521772384644\n",
      "cnt: 0 - valLoss: 0.6613054275512695 - trainLoss: 0.6607511043548584\n",
      "cnt: 0 - valLoss: 0.6613040566444397 - trainLoss: 0.6607499718666077\n",
      "cnt: 0 - valLoss: 0.6613027453422546 - trainLoss: 0.6607488989830017\n",
      "cnt: 0 - valLoss: 0.6613012552261353 - trainLoss: 0.6607478260993958\n",
      "cnt: 0 - valLoss: 0.6612998247146606 - trainLoss: 0.6607468128204346\n",
      "cnt: 0 - valLoss: 0.6612984538078308 - trainLoss: 0.6607457399368286\n",
      "cnt: 0 - valLoss: 0.661297082901001 - trainLoss: 0.6607446670532227\n",
      "cnt: 0 - valLoss: 0.6612957119941711 - trainLoss: 0.6607435941696167\n",
      "cnt: 0 - valLoss: 0.6612942814826965 - trainLoss: 0.660742461681366\n",
      "cnt: 0 - valLoss: 0.6612927913665771 - trainLoss: 0.6607413291931152\n",
      "cnt: 0 - valLoss: 0.6612913608551025 - trainLoss: 0.6607402563095093\n",
      "cnt: 0 - valLoss: 0.6612900495529175 - trainLoss: 0.6607391834259033\n",
      "cnt: 0 - valLoss: 0.6612885594367981 - trainLoss: 0.6607381105422974\n",
      "cnt: 0 - valLoss: 0.6612871885299683 - trainLoss: 0.6607370376586914\n",
      "cnt: 0 - valLoss: 0.6612858176231384 - trainLoss: 0.6607359051704407\n",
      "cnt: 0 - valLoss: 0.6612843871116638 - trainLoss: 0.6607348322868347\n",
      "cnt: 0 - valLoss: 0.661283016204834 - trainLoss: 0.6607337594032288\n",
      "cnt: 0 - valLoss: 0.6612815856933594 - trainLoss: 0.6607326865196228\n",
      "cnt: 0 - valLoss: 0.6612801551818848 - trainLoss: 0.6607316136360168\n",
      "cnt: 0 - valLoss: 0.6612787842750549 - trainLoss: 0.6607304811477661\n",
      "cnt: 0 - valLoss: 0.6612773537635803 - trainLoss: 0.6607293486595154\n",
      "cnt: 0 - valLoss: 0.6612758636474609 - trainLoss: 0.6607283353805542\n",
      "cnt: 0 - valLoss: 0.6612744927406311 - trainLoss: 0.6607272028923035\n",
      "cnt: 0 - valLoss: 0.6612730622291565 - trainLoss: 0.6607261300086975\n",
      "cnt: 0 - valLoss: 0.6612717509269714 - trainLoss: 0.6607250571250916\n",
      "cnt: 0 - valLoss: 0.661270260810852 - trainLoss: 0.6607239842414856\n",
      "cnt: 0 - valLoss: 0.6612688302993774 - trainLoss: 0.6607228517532349\n",
      "cnt: 0 - valLoss: 0.6612674593925476 - trainLoss: 0.6607217192649841\n",
      "cnt: 0 - valLoss: 0.6612659692764282 - trainLoss: 0.6607206463813782\n",
      "cnt: 0 - valLoss: 0.6612645983695984 - trainLoss: 0.660719633102417\n",
      "cnt: 0 - valLoss: 0.6612632274627686 - trainLoss: 0.6607185006141663\n",
      "cnt: 0 - valLoss: 0.661261796951294 - trainLoss: 0.6607174277305603\n",
      "cnt: 0 - valLoss: 0.6612603664398193 - trainLoss: 0.6607162952423096\n",
      "cnt: 0 - valLoss: 0.6612589359283447 - trainLoss: 0.6607152223587036\n",
      "cnt: 0 - valLoss: 0.6612575650215149 - trainLoss: 0.6607141494750977\n",
      "cnt: 0 - valLoss: 0.6612560749053955 - trainLoss: 0.6607130765914917\n",
      "cnt: 0 - valLoss: 0.6612547039985657 - trainLoss: 0.660711944103241\n",
      "cnt: 0 - valLoss: 0.6612532734870911 - trainLoss: 0.660710871219635\n",
      "cnt: 0 - valLoss: 0.6612517833709717 - trainLoss: 0.660709798336029\n",
      "cnt: 0 - valLoss: 0.6612503528594971 - trainLoss: 0.6607087254524231\n",
      "cnt: 0 - valLoss: 0.661249041557312 - trainLoss: 0.6607075929641724\n",
      "cnt: 0 - valLoss: 0.6612475514411926 - trainLoss: 0.6607065200805664\n",
      "cnt: 0 - valLoss: 0.6612461805343628 - trainLoss: 0.6607053875923157\n",
      "cnt: 0 - valLoss: 0.6612447500228882 - trainLoss: 0.6607043147087097\n",
      "cnt: 0 - valLoss: 0.6612433791160583 - trainLoss: 0.6607032418251038\n",
      "cnt: 0 - valLoss: 0.6612418293952942 - trainLoss: 0.660702109336853\n",
      "cnt: 0 - valLoss: 0.6612404584884644 - trainLoss: 0.6607010364532471\n",
      "cnt: 0 - valLoss: 0.6612390279769897 - trainLoss: 0.6606999635696411\n",
      "cnt: 0 - valLoss: 0.6612375974655151 - trainLoss: 0.6606988310813904\n",
      "cnt: 0 - valLoss: 0.6612361669540405 - trainLoss: 0.6606977581977844\n",
      "cnt: 0 - valLoss: 0.6612347364425659 - trainLoss: 0.6606966853141785\n",
      "cnt: 0 - valLoss: 0.6612333655357361 - trainLoss: 0.6606955528259277\n",
      "cnt: 0 - valLoss: 0.6612319350242615 - trainLoss: 0.660694420337677\n",
      "cnt: 0 - valLoss: 0.6612304449081421 - trainLoss: 0.660693347454071\n",
      "cnt: 0 - valLoss: 0.6612290143966675 - trainLoss: 0.6606922149658203\n",
      "cnt: 0 - valLoss: 0.6612276434898376 - trainLoss: 0.6606911420822144\n",
      "cnt: 0 - valLoss: 0.6612261533737183 - trainLoss: 0.6606900691986084\n",
      "cnt: 0 - valLoss: 0.6612247228622437 - trainLoss: 0.6606889367103577\n",
      "cnt: 0 - valLoss: 0.6612233519554138 - trainLoss: 0.6606878638267517\n",
      "cnt: 0 - valLoss: 0.6612219214439392 - trainLoss: 0.6606866717338562\n",
      "cnt: 0 - valLoss: 0.6612204909324646 - trainLoss: 0.6606855988502502\n",
      "cnt: 0 - valLoss: 0.6612190008163452 - trainLoss: 0.6606845259666443\n",
      "cnt: 0 - valLoss: 0.6612175703048706 - trainLoss: 0.6606834530830383\n",
      "cnt: 0 - valLoss: 0.6612161993980408 - trainLoss: 0.6606822609901428\n",
      "cnt: 0 - valLoss: 0.6612146496772766 - trainLoss: 0.6606812477111816\n",
      "cnt: 0 - valLoss: 0.6612132787704468 - trainLoss: 0.6606801152229309\n",
      "cnt: 0 - valLoss: 0.6612118482589722 - trainLoss: 0.6606789827346802\n",
      "cnt: 0 - valLoss: 0.6612104177474976 - trainLoss: 0.6606779098510742\n",
      "cnt: 0 - valLoss: 0.661208987236023 - trainLoss: 0.6606767773628235\n",
      "cnt: 0 - valLoss: 0.6612074971199036 - trainLoss: 0.6606757044792175\n",
      "cnt: 0 - valLoss: 0.661206066608429 - trainLoss: 0.6606745719909668\n",
      "cnt: 0 - valLoss: 0.6612046360969543 - trainLoss: 0.6606734395027161\n",
      "cnt: 0 - valLoss: 0.6612032055854797 - trainLoss: 0.6606723666191101\n",
      "cnt: 0 - valLoss: 0.6612017154693604 - trainLoss: 0.6606712937355042\n",
      "cnt: 0 - valLoss: 0.6612003445625305 - trainLoss: 0.6606701612472534\n",
      "cnt: 0 - valLoss: 0.6611989140510559 - trainLoss: 0.6606690287590027\n",
      "cnt: 0 - valLoss: 0.6611974239349365 - trainLoss: 0.660667896270752\n",
      "cnt: 0 - valLoss: 0.6611959934234619 - trainLoss: 0.6606667637825012\n",
      "cnt: 0 - valLoss: 0.6611945629119873 - trainLoss: 0.6606656908988953\n",
      "cnt: 0 - valLoss: 0.6611931324005127 - trainLoss: 0.6606646180152893\n",
      "cnt: 0 - valLoss: 0.6611916422843933 - trainLoss: 0.6606634855270386\n",
      "cnt: 0 - valLoss: 0.6611902713775635 - trainLoss: 0.6606623530387878\n",
      "cnt: 0 - valLoss: 0.6611889004707336 - trainLoss: 0.6606612801551819\n",
      "cnt: 0 - valLoss: 0.6611873507499695 - trainLoss: 0.6606600880622864\n",
      "cnt: 0 - valLoss: 0.6611859202384949 - trainLoss: 0.6606590151786804\n",
      "cnt: 0 - valLoss: 0.6611844897270203 - trainLoss: 0.6606579422950745\n",
      "cnt: 0 - valLoss: 0.6611830592155457 - trainLoss: 0.660656750202179\n",
      "cnt: 0 - valLoss: 0.6611816883087158 - trainLoss: 0.660655677318573\n",
      "cnt: 0 - valLoss: 0.6611801385879517 - trainLoss: 0.660654604434967\n",
      "cnt: 0 - valLoss: 0.6611786484718323 - trainLoss: 0.6606534719467163\n",
      "cnt: 0 - valLoss: 0.6611772775650024 - trainLoss: 0.6606523394584656\n",
      "cnt: 0 - valLoss: 0.6611757874488831 - trainLoss: 0.6606512069702148\n",
      "cnt: 0 - valLoss: 0.6611743569374084 - trainLoss: 0.6606500744819641\n",
      "cnt: 0 - valLoss: 0.6611729264259338 - trainLoss: 0.6606490015983582\n",
      "cnt: 0 - valLoss: 0.661171555519104 - trainLoss: 0.6606478095054626\n",
      "cnt: 0 - valLoss: 0.6611700057983398 - trainLoss: 0.6606467366218567\n",
      "cnt: 0 - valLoss: 0.6611685752868652 - trainLoss: 0.6606456637382507\n",
      "cnt: 0 - valLoss: 0.6611670851707458 - trainLoss: 0.66064453125\n",
      "cnt: 0 - valLoss: 0.661165714263916 - trainLoss: 0.6606433987617493\n",
      "cnt: 0 - valLoss: 0.6611642241477966 - trainLoss: 0.6606423258781433\n",
      "cnt: 0 - valLoss: 0.661162793636322 - trainLoss: 0.6606411933898926\n",
      "cnt: 0 - valLoss: 0.6611613631248474 - trainLoss: 0.6606400609016418\n",
      "cnt: 0 - valLoss: 0.6611598134040833 - trainLoss: 0.6606388688087463\n",
      "cnt: 0 - valLoss: 0.6611584424972534 - trainLoss: 0.6606377959251404\n",
      "cnt: 0 - valLoss: 0.6611570119857788 - trainLoss: 0.6606367230415344\n",
      "cnt: 0 - valLoss: 0.6611555218696594 - trainLoss: 0.6606355309486389\n",
      "cnt: 0 - valLoss: 0.6611540913581848 - trainLoss: 0.6606343984603882\n",
      "cnt: 0 - valLoss: 0.6611526012420654 - trainLoss: 0.6606333255767822\n",
      "cnt: 0 - valLoss: 0.6611511707305908 - trainLoss: 0.6606321334838867\n",
      "cnt: 0 - valLoss: 0.6611496806144714 - trainLoss: 0.6606310606002808\n",
      "cnt: 0 - valLoss: 0.6611482501029968 - trainLoss: 0.6606298685073853\n",
      "cnt: 0 - valLoss: 0.6611468195915222 - trainLoss: 0.6606287956237793\n",
      "cnt: 0 - valLoss: 0.6611453294754028 - trainLoss: 0.6606276035308838\n",
      "cnt: 0 - valLoss: 0.6611438989639282 - trainLoss: 0.6606265306472778\n",
      "cnt: 0 - valLoss: 0.6611424088478088 - trainLoss: 0.6606253385543823\n",
      "cnt: 0 - valLoss: 0.6611409783363342 - trainLoss: 0.6606242656707764\n",
      "cnt: 0 - valLoss: 0.6611395478248596 - trainLoss: 0.6606231331825256\n",
      "cnt: 0 - valLoss: 0.6611380577087402 - trainLoss: 0.6606220006942749\n",
      "cnt: 0 - valLoss: 0.6611365675926208 - trainLoss: 0.6606208086013794\n",
      "cnt: 0 - valLoss: 0.6611350774765015 - trainLoss: 0.6606197357177734\n",
      "cnt: 0 - valLoss: 0.6611336469650269 - trainLoss: 0.6606186032295227\n",
      "cnt: 0 - valLoss: 0.6611322164535522 - trainLoss: 0.6606174111366272\n",
      "cnt: 0 - valLoss: 0.6611307263374329 - trainLoss: 0.6606163382530212\n",
      "cnt: 0 - valLoss: 0.6611292362213135 - trainLoss: 0.6606152057647705\n",
      "cnt: 0 - valLoss: 0.6611278653144836 - trainLoss: 0.6606140732765198\n",
      "cnt: 0 - valLoss: 0.6611263155937195 - trainLoss: 0.660612940788269\n",
      "cnt: 0 - valLoss: 0.6611248254776001 - trainLoss: 0.6606118083000183\n",
      "cnt: 0 - valLoss: 0.6611233353614807 - trainLoss: 0.6606106162071228\n",
      "cnt: 0 - valLoss: 0.6611219048500061 - trainLoss: 0.6606095433235168\n",
      "cnt: 0 - valLoss: 0.6611204743385315 - trainLoss: 0.6606084108352661\n",
      "cnt: 0 - valLoss: 0.6611189842224121 - trainLoss: 0.6606072783470154\n",
      "cnt: 0 - valLoss: 0.6611174941062927 - trainLoss: 0.6606062054634094\n",
      "cnt: 0 - valLoss: 0.6611160039901733 - trainLoss: 0.6606050133705139\n",
      "cnt: 0 - valLoss: 0.6611145734786987 - trainLoss: 0.6606038808822632\n",
      "cnt: 0 - valLoss: 0.6611131429672241 - trainLoss: 0.6606026887893677\n",
      "cnt: 0 - valLoss: 0.6611116528511047 - trainLoss: 0.6606015563011169\n",
      "cnt: 0 - valLoss: 0.6611101627349854 - trainLoss: 0.6606004238128662\n",
      "cnt: 0 - valLoss: 0.6611087322235107 - trainLoss: 0.6605992913246155\n",
      "cnt: 0 - valLoss: 0.6611072421073914 - trainLoss: 0.6605982184410095\n",
      "cnt: 0 - valLoss: 0.661105751991272 - trainLoss: 0.660597026348114\n",
      "cnt: 0 - valLoss: 0.6611042022705078 - trainLoss: 0.6605958938598633\n",
      "cnt: 0 - valLoss: 0.6611027717590332 - trainLoss: 0.6605947017669678\n",
      "cnt: 0 - valLoss: 0.6611012816429138 - trainLoss: 0.6605936288833618\n",
      "cnt: 0 - valLoss: 0.6610998511314392 - trainLoss: 0.6605924367904663\n",
      "cnt: 0 - valLoss: 0.661098301410675 - trainLoss: 0.6605913043022156\n",
      "cnt: 0 - valLoss: 0.6610968708992004 - trainLoss: 0.6605901718139648\n",
      "cnt: 0 - valLoss: 0.661095380783081 - trainLoss: 0.6605889797210693\n",
      "cnt: 0 - valLoss: 0.6610939502716064 - trainLoss: 0.6605878472328186\n",
      "cnt: 0 - valLoss: 0.6610924601554871 - trainLoss: 0.6605867743492126\n",
      "cnt: 0 - valLoss: 0.6610909700393677 - trainLoss: 0.6605855226516724\n",
      "cnt: 0 - valLoss: 0.6610894203186035 - trainLoss: 0.6605844497680664\n",
      "cnt: 0 - valLoss: 0.6610879898071289 - trainLoss: 0.6605832576751709\n",
      "cnt: 0 - valLoss: 0.6610864996910095 - trainLoss: 0.6605821251869202\n",
      "cnt: 0 - valLoss: 0.6610849499702454 - trainLoss: 0.6605809926986694\n",
      "cnt: 0 - valLoss: 0.6610835194587708 - trainLoss: 0.6605798006057739\n",
      "cnt: 0 - valLoss: 0.6610819697380066 - trainLoss: 0.660578727722168\n",
      "cnt: 0 - valLoss: 0.661080539226532 - trainLoss: 0.6605775356292725\n",
      "cnt: 0 - valLoss: 0.6610790491104126 - trainLoss: 0.6605764031410217\n",
      "cnt: 0 - valLoss: 0.6610775589942932 - trainLoss: 0.6605752110481262\n",
      "cnt: 0 - valLoss: 0.6610760688781738 - trainLoss: 0.6605741381645203\n",
      "cnt: 0 - valLoss: 0.6610745787620544 - trainLoss: 0.6605729460716248\n",
      "cnt: 0 - valLoss: 0.6610731482505798 - trainLoss: 0.6605717539787292\n",
      "cnt: 0 - valLoss: 0.6610715985298157 - trainLoss: 0.6605706810951233\n",
      "cnt: 0 - valLoss: 0.6610700488090515 - trainLoss: 0.660569429397583\n",
      "cnt: 0 - valLoss: 0.6610686182975769 - trainLoss: 0.660568356513977\n",
      "cnt: 0 - valLoss: 0.6610671281814575 - trainLoss: 0.6605671644210815\n",
      "cnt: 0 - valLoss: 0.6610656976699829 - trainLoss: 0.6605660319328308\n",
      "cnt: 0 - valLoss: 0.6610641479492188 - trainLoss: 0.6605648994445801\n",
      "cnt: 0 - valLoss: 0.6610626578330994 - trainLoss: 0.6605637073516846\n",
      "cnt: 0 - valLoss: 0.6610611081123352 - trainLoss: 0.6605625152587891\n",
      "cnt: 0 - valLoss: 0.6610596179962158 - trainLoss: 0.6605613827705383\n",
      "cnt: 0 - valLoss: 0.6610581278800964 - trainLoss: 0.6605602502822876\n",
      "cnt: 0 - valLoss: 0.6610566973686218 - trainLoss: 0.6605591177940369\n",
      "cnt: 0 - valLoss: 0.6610551476478577 - trainLoss: 0.6605579257011414\n",
      "cnt: 0 - valLoss: 0.6610536575317383 - trainLoss: 0.6605567336082458\n",
      "cnt: 0 - valLoss: 0.6610522270202637 - trainLoss: 0.6605556011199951\n",
      "cnt: 0 - valLoss: 0.6610506772994995 - trainLoss: 0.6605544090270996\n",
      "cnt: 0 - valLoss: 0.6610491871833801 - trainLoss: 0.6605532765388489\n",
      "cnt: 0 - valLoss: 0.6610477566719055 - trainLoss: 0.6605521440505981\n",
      "cnt: 0 - valLoss: 0.6610461473464966 - trainLoss: 0.6605509519577026\n",
      "cnt: 0 - valLoss: 0.661044716835022 - trainLoss: 0.6605498194694519\n",
      "cnt: 0 - valLoss: 0.6610432267189026 - trainLoss: 0.6605486869812012\n",
      "cnt: 0 - valLoss: 0.6610416769981384 - trainLoss: 0.6605474948883057\n",
      "cnt: 0 - valLoss: 0.6610402464866638 - trainLoss: 0.6605463624000549\n",
      "cnt: 0 - valLoss: 0.6610386967658997 - trainLoss: 0.6605451703071594\n",
      "cnt: 0 - valLoss: 0.6610372066497803 - trainLoss: 0.6605439186096191\n",
      "cnt: 0 - valLoss: 0.6610357761383057 - trainLoss: 0.6605428457260132\n",
      "cnt: 0 - valLoss: 0.6610342264175415 - trainLoss: 0.6605416536331177\n",
      "cnt: 0 - valLoss: 0.6610326766967773 - trainLoss: 0.6605404615402222\n",
      "cnt: 0 - valLoss: 0.6610312461853027 - trainLoss: 0.6605393290519714\n",
      "cnt: 0 - valLoss: 0.6610296964645386 - trainLoss: 0.6605381965637207\n",
      "cnt: 0 - valLoss: 0.6610282063484192 - trainLoss: 0.6605370044708252\n",
      "cnt: 0 - valLoss: 0.6610267162322998 - trainLoss: 0.6605358123779297\n",
      "cnt: 0 - valLoss: 0.6610252261161804 - trainLoss: 0.660534679889679\n",
      "cnt: 0 - valLoss: 0.6610236167907715 - trainLoss: 0.6605334877967834\n",
      "cnt: 0 - valLoss: 0.6610221862792969 - trainLoss: 0.6605322957038879\n",
      "cnt: 0 - valLoss: 0.6610206961631775 - trainLoss: 0.6605311632156372\n",
      "cnt: 0 - valLoss: 0.6610192060470581 - trainLoss: 0.6605299711227417\n",
      "cnt: 0 - valLoss: 0.661017656326294 - trainLoss: 0.6605287790298462\n",
      "cnt: 0 - valLoss: 0.6610161662101746 - trainLoss: 0.6605277061462402\n",
      "cnt: 0 - valLoss: 0.6610146760940552 - trainLoss: 0.6605265140533447\n",
      "cnt: 0 - valLoss: 0.661013126373291 - trainLoss: 0.6605253219604492\n",
      "cnt: 0 - valLoss: 0.6610116958618164 - trainLoss: 0.6605241298675537\n",
      "cnt: 0 - valLoss: 0.6610101461410522 - trainLoss: 0.6605228781700134\n",
      "cnt: 0 - valLoss: 0.6610085964202881 - trainLoss: 0.6605218052864075\n",
      "cnt: 0 - valLoss: 0.6610071063041687 - trainLoss: 0.6605205535888672\n",
      "cnt: 0 - valLoss: 0.6610055565834045 - trainLoss: 0.6605194807052612\n",
      "cnt: 0 - valLoss: 0.6610040664672852 - trainLoss: 0.6605182886123657\n",
      "cnt: 0 - valLoss: 0.6610025763511658 - trainLoss: 0.6605170965194702\n",
      "cnt: 0 - valLoss: 0.6610010266304016 - trainLoss: 0.6605159640312195\n",
      "cnt: 0 - valLoss: 0.6609995365142822 - trainLoss: 0.6605147123336792\n",
      "cnt: 0 - valLoss: 0.6609979867935181 - trainLoss: 0.6605135202407837\n",
      "cnt: 0 - valLoss: 0.6609964966773987 - trainLoss: 0.660512387752533\n",
      "cnt: 0 - valLoss: 0.6609950065612793 - trainLoss: 0.6605111360549927\n",
      "cnt: 0 - valLoss: 0.6609934568405151 - trainLoss: 0.6605099439620972\n",
      "cnt: 0 - valLoss: 0.660991907119751 - trainLoss: 0.6605088114738464\n",
      "cnt: 0 - valLoss: 0.6609903573989868 - trainLoss: 0.6605076193809509\n",
      "cnt: 0 - valLoss: 0.6609888672828674 - trainLoss: 0.6605064272880554\n",
      "cnt: 0 - valLoss: 0.660987377166748 - trainLoss: 0.6605052351951599\n",
      "cnt: 0 - valLoss: 0.6609858870506287 - trainLoss: 0.660504162311554\n",
      "cnt: 0 - valLoss: 0.6609842777252197 - trainLoss: 0.6605029106140137\n",
      "cnt: 0 - valLoss: 0.6609827876091003 - trainLoss: 0.6605017185211182\n",
      "cnt: 0 - valLoss: 0.6609812378883362 - trainLoss: 0.6605005264282227\n",
      "cnt: 0 - valLoss: 0.660979688167572 - trainLoss: 0.6604993939399719\n",
      "cnt: 0 - valLoss: 0.6609782576560974 - trainLoss: 0.6604982018470764\n",
      "cnt: 0 - valLoss: 0.6609767079353333 - trainLoss: 0.6604969501495361\n",
      "cnt: 0 - valLoss: 0.6609751582145691 - trainLoss: 0.6604958176612854\n",
      "cnt: 0 - valLoss: 0.6609736084938049 - trainLoss: 0.6604946255683899\n",
      "cnt: 0 - valLoss: 0.6609720587730408 - trainLoss: 0.6604933738708496\n",
      "cnt: 0 - valLoss: 0.6609705090522766 - trainLoss: 0.6604921817779541\n",
      "cnt: 0 - valLoss: 0.6609689593315125 - trainLoss: 0.6604909896850586\n",
      "cnt: 0 - valLoss: 0.6609674692153931 - trainLoss: 0.6604897975921631\n",
      "cnt: 0 - valLoss: 0.6609658598899841 - trainLoss: 0.6604885458946228\n",
      "cnt: 0 - valLoss: 0.66096431016922 - trainLoss: 0.6604873538017273\n",
      "cnt: 0 - valLoss: 0.6609627604484558 - trainLoss: 0.6604861617088318\n",
      "cnt: 0 - valLoss: 0.6609612703323364 - trainLoss: 0.6604849696159363\n",
      "cnt: 0 - valLoss: 0.6609597206115723 - trainLoss: 0.6604837775230408\n",
      "cnt: 0 - valLoss: 0.6609581708908081 - trainLoss: 0.6604825258255005\n",
      "cnt: 0 - valLoss: 0.660956621170044 - trainLoss: 0.6604813933372498\n",
      "cnt: 0 - valLoss: 0.660955011844635 - trainLoss: 0.6604801416397095\n",
      "cnt: 0 - valLoss: 0.6609534621238708 - trainLoss: 0.660478949546814\n",
      "cnt: 0 - valLoss: 0.6609519720077515 - trainLoss: 0.6604777574539185\n",
      "cnt: 0 - valLoss: 0.6609504222869873 - trainLoss: 0.6604765057563782\n",
      "cnt: 0 - valLoss: 0.6609488129615784 - trainLoss: 0.6604753732681274\n",
      "cnt: 0 - valLoss: 0.6609472632408142 - trainLoss: 0.6604741215705872\n",
      "cnt: 0 - valLoss: 0.66094571352005 - trainLoss: 0.6604728698730469\n",
      "cnt: 0 - valLoss: 0.6609441637992859 - trainLoss: 0.6604716777801514\n",
      "cnt: 0 - valLoss: 0.6609426140785217 - trainLoss: 0.6604704856872559\n",
      "cnt: 0 - valLoss: 0.6609410643577576 - trainLoss: 0.6604692935943604\n",
      "cnt: 0 - valLoss: 0.6609395146369934 - trainLoss: 0.6604681015014648\n",
      "cnt: 0 - valLoss: 0.6609379053115845 - trainLoss: 0.6604668498039246\n",
      "cnt: 0 - valLoss: 0.6609363555908203 - trainLoss: 0.660465657711029\n",
      "cnt: 0 - valLoss: 0.6609348058700562 - trainLoss: 0.6604645252227783\n",
      "cnt: 0 - valLoss: 0.660933256149292 - trainLoss: 0.6604632139205933\n",
      "cnt: 0 - valLoss: 0.6609317064285278 - trainLoss: 0.660461962223053\n",
      "cnt: 0 - valLoss: 0.6609301567077637 - trainLoss: 0.6604607701301575\n",
      "cnt: 0 - valLoss: 0.6609285473823547 - trainLoss: 0.6604596376419067\n",
      "cnt: 0 - valLoss: 0.6609269976615906 - trainLoss: 0.6604583859443665\n",
      "cnt: 0 - valLoss: 0.6609254479408264 - trainLoss: 0.6604572534561157\n",
      "cnt: 0 - valLoss: 0.6609238982200623 - trainLoss: 0.6604559421539307\n",
      "cnt: 0 - valLoss: 0.6609222888946533 - trainLoss: 0.6604546904563904\n",
      "cnt: 0 - valLoss: 0.6609207391738892 - trainLoss: 0.6604534983634949\n",
      "cnt: 0 - valLoss: 0.660919189453125 - trainLoss: 0.6604522466659546\n",
      "cnt: 0 - valLoss: 0.6609175801277161 - trainLoss: 0.6604510545730591\n",
      "cnt: 0 - valLoss: 0.6609160900115967 - trainLoss: 0.6604498624801636\n",
      "cnt: 0 - valLoss: 0.6609144806861877 - trainLoss: 0.6604486703872681\n",
      "cnt: 0 - valLoss: 0.6609128713607788 - trainLoss: 0.6604474186897278\n",
      "cnt: 0 - valLoss: 0.6609113216400146 - trainLoss: 0.6604461669921875\n",
      "cnt: 0 - valLoss: 0.6609097719192505 - trainLoss: 0.660444974899292\n",
      "cnt: 0 - valLoss: 0.6609082221984863 - trainLoss: 0.6604437232017517\n",
      "cnt: 0 - valLoss: 0.6609066724777222 - trainLoss: 0.6604425311088562\n",
      "cnt: 0 - valLoss: 0.6609050631523132 - trainLoss: 0.6604412794113159\n",
      "cnt: 0 - valLoss: 0.6609035134315491 - trainLoss: 0.6604400873184204\n",
      "cnt: 0 - valLoss: 0.6609019041061401 - trainLoss: 0.6604388952255249\n",
      "cnt: 0 - valLoss: 0.660900354385376 - trainLoss: 0.6604376435279846\n",
      "cnt: 0 - valLoss: 0.6608988046646118 - trainLoss: 0.6604364514350891\n",
      "cnt: 0 - valLoss: 0.6608972549438477 - trainLoss: 0.6604352593421936\n",
      "cnt: 0 - valLoss: 0.6608956456184387 - trainLoss: 0.6604340076446533\n",
      "cnt: 0 - valLoss: 0.6608940958976746 - trainLoss: 0.6604328155517578\n",
      "cnt: 0 - valLoss: 0.6608925461769104 - trainLoss: 0.6604315638542175\n",
      "cnt: 0 - valLoss: 0.6608909368515015 - trainLoss: 0.660430371761322\n",
      "cnt: 0 - valLoss: 0.6608893871307373 - trainLoss: 0.660429060459137\n",
      "cnt: 0 - valLoss: 0.6608878374099731 - trainLoss: 0.6604278683662415\n",
      "cnt: 0 - valLoss: 0.6608862280845642 - trainLoss: 0.660426676273346\n",
      "cnt: 0 - valLoss: 0.6608846187591553 - trainLoss: 0.6604254245758057\n",
      "cnt: 0 - valLoss: 0.6608831286430359 - trainLoss: 0.6604242324829102\n",
      "cnt: 0 - valLoss: 0.6608814597129822 - trainLoss: 0.6604229807853699\n",
      "cnt: 0 - valLoss: 0.660879909992218 - trainLoss: 0.6604217290878296\n",
      "cnt: 0 - valLoss: 0.6608783006668091 - trainLoss: 0.6604205369949341\n",
      "cnt: 0 - valLoss: 0.6608767509460449 - trainLoss: 0.6604192852973938\n",
      "cnt: 0 - valLoss: 0.660875141620636 - trainLoss: 0.6604180335998535\n",
      "cnt: 0 - valLoss: 0.660873532295227 - trainLoss: 0.660416841506958\n",
      "cnt: 0 - valLoss: 0.6608719825744629 - trainLoss: 0.6604155898094177\n",
      "cnt: 0 - valLoss: 0.6608704328536987 - trainLoss: 0.6604143977165222\n",
      "cnt: 0 - valLoss: 0.6608688235282898 - trainLoss: 0.6604132056236267\n",
      "cnt: 0 - valLoss: 0.6608672738075256 - trainLoss: 0.6604118347167969\n",
      "cnt: 0 - valLoss: 0.6608657240867615 - trainLoss: 0.6604106426239014\n",
      "cnt: 0 - valLoss: 0.6608641743659973 - trainLoss: 0.6604094505310059\n",
      "cnt: 0 - valLoss: 0.6608625054359436 - trainLoss: 0.6604081988334656\n",
      "cnt: 0 - valLoss: 0.6608609557151794 - trainLoss: 0.6604070067405701\n",
      "cnt: 0 - valLoss: 0.6608593463897705 - trainLoss: 0.660405695438385\n",
      "cnt: 0 - valLoss: 0.6608577966690063 - trainLoss: 0.6604045033454895\n",
      "cnt: 0 - valLoss: 0.6608561873435974 - trainLoss: 0.660403311252594\n",
      "cnt: 0 - valLoss: 0.6608545780181885 - trainLoss: 0.6604019999504089\n",
      "cnt: 0 - valLoss: 0.6608530282974243 - trainLoss: 0.6604008078575134\n",
      "cnt: 0 - valLoss: 0.6608514785766602 - trainLoss: 0.6603995561599731\n",
      "cnt: 0 - valLoss: 0.6608498096466064 - trainLoss: 0.6603983640670776\n",
      "cnt: 0 - valLoss: 0.6608482003211975 - trainLoss: 0.6603971123695374\n",
      "cnt: 0 - valLoss: 0.6608466506004333 - trainLoss: 0.6603958606719971\n",
      "cnt: 0 - valLoss: 0.6608450412750244 - trainLoss: 0.6603946089744568\n",
      "cnt: 0 - valLoss: 0.6608434319496155 - trainLoss: 0.6603933572769165\n",
      "cnt: 0 - valLoss: 0.6608418822288513 - trainLoss: 0.6603921055793762\n",
      "cnt: 0 - valLoss: 0.6608402132987976 - trainLoss: 0.6603909134864807\n",
      "cnt: 0 - valLoss: 0.6608387231826782 - trainLoss: 0.6603896617889404\n",
      "cnt: 0 - valLoss: 0.6608370542526245 - trainLoss: 0.6603884100914001\n",
      "cnt: 0 - valLoss: 0.6608355045318604 - trainLoss: 0.6603872179985046\n",
      "cnt: 0 - valLoss: 0.6608338356018066 - trainLoss: 0.6603859066963196\n",
      "cnt: 0 - valLoss: 0.6608322858810425 - trainLoss: 0.6603846549987793\n",
      "cnt: 0 - valLoss: 0.6608307361602783 - trainLoss: 0.6603834629058838\n",
      "cnt: 0 - valLoss: 0.6608290672302246 - trainLoss: 0.6603822708129883\n",
      "cnt: 0 - valLoss: 0.6608274579048157 - trainLoss: 0.6603809595108032\n",
      "cnt: 0 - valLoss: 0.6608258485794067 - trainLoss: 0.6603797078132629\n",
      "cnt: 0 - valLoss: 0.6608242988586426 - trainLoss: 0.6603785157203674\n",
      "cnt: 0 - valLoss: 0.6608226895332336 - trainLoss: 0.6603772640228271\n",
      "cnt: 0 - valLoss: 0.6608210206031799 - trainLoss: 0.6603760123252869\n",
      "cnt: 0 - valLoss: 0.660819411277771 - trainLoss: 0.6603747606277466\n",
      "cnt: 0 - valLoss: 0.6608178615570068 - trainLoss: 0.6603735089302063\n",
      "cnt: 0 - valLoss: 0.6608162522315979 - trainLoss: 0.6603723168373108\n",
      "cnt: 0 - valLoss: 0.6608145833015442 - trainLoss: 0.6603710055351257\n",
      "cnt: 0 - valLoss: 0.66081303358078 - trainLoss: 0.6603698134422302\n",
      "cnt: 0 - valLoss: 0.6608113646507263 - trainLoss: 0.6603685617446899\n",
      "cnt: 0 - valLoss: 0.6608098149299622 - trainLoss: 0.6603673696517944\n",
      "cnt: 0 - valLoss: 0.6608082056045532 - trainLoss: 0.6603660583496094\n",
      "cnt: 0 - valLoss: 0.6608065962791443 - trainLoss: 0.6603648662567139\n",
      "cnt: 0 - valLoss: 0.6608049869537354 - trainLoss: 0.6603636741638184\n",
      "cnt: 0 - valLoss: 0.6608033776283264 - trainLoss: 0.6603624224662781\n",
      "cnt: 0 - valLoss: 0.6608017683029175 - trainLoss: 0.6603611707687378\n",
      "cnt: 0 - valLoss: 0.6608001589775085 - trainLoss: 0.6603599190711975\n",
      "cnt: 0 - valLoss: 0.6607984900474548 - trainLoss: 0.660358726978302\n",
      "cnt: 0 - valLoss: 0.6607968807220459 - trainLoss: 0.6603574752807617\n",
      "cnt: 0 - valLoss: 0.6607953310012817 - trainLoss: 0.6603562235832214\n",
      "cnt: 0 - valLoss: 0.660793662071228 - trainLoss: 0.6603549718856812\n",
      "cnt: 0 - valLoss: 0.6607920527458191 - trainLoss: 0.6603537201881409\n",
      "cnt: 0 - valLoss: 0.6607903838157654 - trainLoss: 0.6603524684906006\n",
      "cnt: 0 - valLoss: 0.6607887744903564 - trainLoss: 0.6603512763977051\n",
      "cnt: 0 - valLoss: 0.6607871651649475 - trainLoss: 0.6603500247001648\n",
      "cnt: 0 - valLoss: 0.6607855558395386 - trainLoss: 0.6603487730026245\n",
      "cnt: 0 - valLoss: 0.6607839465141296 - trainLoss: 0.6603474617004395\n",
      "cnt: 0 - valLoss: 0.6607823371887207 - trainLoss: 0.660346269607544\n",
      "cnt: 0 - valLoss: 0.660780668258667 - trainLoss: 0.6603450179100037\n",
      "cnt: 0 - valLoss: 0.6607790589332581 - trainLoss: 0.6603437066078186\n",
      "cnt: 0 - valLoss: 0.6607774496078491 - trainLoss: 0.6603425145149231\n",
      "cnt: 0 - valLoss: 0.6607758402824402 - trainLoss: 0.6603412628173828\n",
      "cnt: 0 - valLoss: 0.6607741713523865 - trainLoss: 0.6603400111198425\n",
      "cnt: 0 - valLoss: 0.6607725620269775 - trainLoss: 0.6603387594223022\n",
      "cnt: 0 - valLoss: 0.6607708930969238 - trainLoss: 0.6603375673294067\n",
      "cnt: 0 - valLoss: 0.6607692837715149 - trainLoss: 0.6603362560272217\n",
      "cnt: 0 - valLoss: 0.6607676148414612 - trainLoss: 0.6603350043296814\n",
      "cnt: 0 - valLoss: 0.660766065120697 - trainLoss: 0.6603337526321411\n",
      "cnt: 0 - valLoss: 0.6607643961906433 - trainLoss: 0.6603325009346008\n",
      "cnt: 0 - valLoss: 0.6607627272605896 - trainLoss: 0.6603312492370605\n",
      "cnt: 0 - valLoss: 0.6607611179351807 - trainLoss: 0.6603299975395203\n",
      "cnt: 0 - valLoss: 0.6607595086097717 - trainLoss: 0.66032874584198\n",
      "cnt: 0 - valLoss: 0.660757839679718 - trainLoss: 0.6603274941444397\n",
      "cnt: 0 - valLoss: 0.6607562303543091 - trainLoss: 0.6603262424468994\n",
      "cnt: 0 - valLoss: 0.6607546210289001 - trainLoss: 0.6603249907493591\n",
      "cnt: 0 - valLoss: 0.6607529520988464 - trainLoss: 0.6603236794471741\n",
      "cnt: 0 - valLoss: 0.6607513427734375 - trainLoss: 0.6603224873542786\n",
      "cnt: 0 - valLoss: 0.660749614238739 - trainLoss: 0.6603212356567383\n",
      "cnt: 0 - valLoss: 0.6607480645179749 - trainLoss: 0.660319983959198\n",
      "cnt: 0 - valLoss: 0.6607463955879211 - trainLoss: 0.6603187322616577\n",
      "cnt: 0 - valLoss: 0.6607447266578674 - trainLoss: 0.6603174805641174\n",
      "cnt: 0 - valLoss: 0.6607431173324585 - trainLoss: 0.6603161692619324\n",
      "cnt: 0 - valLoss: 0.6607415080070496 - trainLoss: 0.6603148579597473\n",
      "cnt: 0 - valLoss: 0.6607398390769958 - trainLoss: 0.6603136658668518\n",
      "cnt: 0 - valLoss: 0.6607381701469421 - trainLoss: 0.6603124141693115\n",
      "cnt: 0 - valLoss: 0.6607365012168884 - trainLoss: 0.6603111624717712\n",
      "cnt: 0 - valLoss: 0.6607349514961243 - trainLoss: 0.6603098511695862\n",
      "cnt: 0 - valLoss: 0.6607332825660706 - trainLoss: 0.6603085994720459\n",
      "cnt: 0 - valLoss: 0.6607316136360168 - trainLoss: 0.6603073477745056\n",
      "cnt: 0 - valLoss: 0.6607299447059631 - trainLoss: 0.6603060364723206\n",
      "cnt: 0 - valLoss: 0.6607283353805542 - trainLoss: 0.6603047847747803\n",
      "cnt: 0 - valLoss: 0.6607267260551453 - trainLoss: 0.6603035926818848\n",
      "cnt: 0 - valLoss: 0.6607249975204468 - trainLoss: 0.6603022813796997\n",
      "cnt: 0 - valLoss: 0.6607233285903931 - trainLoss: 0.6603010296821594\n",
      "cnt: 0 - valLoss: 0.6607217192649841 - trainLoss: 0.6602997779846191\n",
      "cnt: 0 - valLoss: 0.6607200503349304 - trainLoss: 0.6602985262870789\n",
      "cnt: 0 - valLoss: 0.6607184410095215 - trainLoss: 0.6602972149848938\n",
      "cnt: 0 - valLoss: 0.6607167720794678 - trainLoss: 0.6602959632873535\n",
      "cnt: 0 - valLoss: 0.6607151031494141 - trainLoss: 0.6602946519851685\n",
      "cnt: 0 - valLoss: 0.6607134342193604 - trainLoss: 0.6602934002876282\n",
      "cnt: 0 - valLoss: 0.6607117652893066 - trainLoss: 0.6602921485900879\n",
      "cnt: 0 - valLoss: 0.6607101559638977 - trainLoss: 0.6602908372879028\n",
      "cnt: 0 - valLoss: 0.660708487033844 - trainLoss: 0.6602895855903625\n",
      "cnt: 0 - valLoss: 0.6607068181037903 - trainLoss: 0.6602883338928223\n",
      "cnt: 0 - valLoss: 0.6607052087783813 - trainLoss: 0.6602870225906372\n",
      "cnt: 0 - valLoss: 0.6607034802436829 - trainLoss: 0.6602857708930969\n",
      "cnt: 0 - valLoss: 0.6607018709182739 - trainLoss: 0.6602844595909119\n",
      "cnt: 0 - valLoss: 0.6607003211975098 - trainLoss: 0.6602832674980164\n",
      "cnt: 0 - valLoss: 0.660698652267456 - trainLoss: 0.6602820754051208\n",
      "cnt: 0 - valLoss: 0.6606971025466919 - trainLoss: 0.6602808833122253\n",
      "cnt: 0 - valLoss: 0.6606955528259277 - trainLoss: 0.6602797508239746\n",
      "cnt: 0 - valLoss: 0.660693883895874 - trainLoss: 0.6602784991264343\n",
      "cnt: 0 - valLoss: 0.6606923341751099 - trainLoss: 0.6602773070335388\n",
      "cnt: 0 - valLoss: 0.6606906652450562 - trainLoss: 0.6602761745452881\n",
      "cnt: 0 - valLoss: 0.660689115524292 - trainLoss: 0.6602749228477478\n",
      "cnt: 0 - valLoss: 0.6606875061988831 - trainLoss: 0.6602737903594971\n",
      "cnt: 0 - valLoss: 0.6606858372688293 - trainLoss: 0.6602725982666016\n",
      "cnt: 0 - valLoss: 0.6606842875480652 - trainLoss: 0.660271406173706\n",
      "cnt: 0 - valLoss: 0.6606826186180115 - trainLoss: 0.6602702140808105\n",
      "cnt: 0 - valLoss: 0.6606810688972473 - trainLoss: 0.6602689623832703\n",
      "cnt: 0 - valLoss: 0.6606793999671936 - trainLoss: 0.6602677702903748\n",
      "cnt: 0 - valLoss: 0.6606777906417847 - trainLoss: 0.660266637802124\n",
      "cnt: 0 - valLoss: 0.6606762409210205 - trainLoss: 0.6602654457092285\n",
      "cnt: 0 - valLoss: 0.6606746315956116 - trainLoss: 0.660264253616333\n",
      "cnt: 0 - valLoss: 0.6606729626655579 - trainLoss: 0.6602630615234375\n",
      "cnt: 0 - valLoss: 0.6606714129447937 - trainLoss: 0.660261869430542\n",
      "cnt: 0 - valLoss: 0.66066974401474 - trainLoss: 0.6602606773376465\n",
      "cnt: 0 - valLoss: 0.6606681942939758 - trainLoss: 0.660259485244751\n",
      "cnt: 0 - valLoss: 0.6606665849685669 - trainLoss: 0.6602582931518555\n",
      "cnt: 0 - valLoss: 0.6606649160385132 - trainLoss: 0.66025710105896\n",
      "cnt: 0 - valLoss: 0.6606633067131042 - trainLoss: 0.6602559089660645\n",
      "cnt: 0 - valLoss: 0.6606616973876953 - trainLoss: 0.660254716873169\n",
      "cnt: 0 - valLoss: 0.6606601476669312 - trainLoss: 0.6602535247802734\n",
      "cnt: 0 - valLoss: 0.6606584787368774 - trainLoss: 0.6602523922920227\n",
      "cnt: 0 - valLoss: 0.6606568694114685 - trainLoss: 0.6602512001991272\n",
      "cnt: 0 - valLoss: 0.6606552600860596 - trainLoss: 0.6602500081062317\n",
      "cnt: 0 - valLoss: 0.6606535911560059 - trainLoss: 0.6602488160133362\n",
      "cnt: 0 - valLoss: 0.6606519222259521 - trainLoss: 0.6602476239204407\n",
      "cnt: 0 - valLoss: 0.660650372505188 - trainLoss: 0.6602464318275452\n",
      "cnt: 0 - valLoss: 0.6606487035751343 - trainLoss: 0.6602452993392944\n",
      "cnt: 0 - valLoss: 0.6606471538543701 - trainLoss: 0.6602441072463989\n",
      "cnt: 0 - valLoss: 0.6606455445289612 - trainLoss: 0.6602428555488586\n",
      "cnt: 0 - valLoss: 0.6606438755989075 - trainLoss: 0.6602416634559631\n",
      "cnt: 0 - valLoss: 0.6606422066688538 - trainLoss: 0.6602404713630676\n",
      "cnt: 0 - valLoss: 0.6606406569480896 - trainLoss: 0.6602392792701721\n",
      "cnt: 0 - valLoss: 0.6606389880180359 - trainLoss: 0.6602380871772766\n",
      "cnt: 0 - valLoss: 0.6606373190879822 - trainLoss: 0.6602368950843811\n",
      "cnt: 0 - valLoss: 0.660635769367218 - trainLoss: 0.6602357029914856\n",
      "cnt: 0 - valLoss: 0.6606341004371643 - trainLoss: 0.6602345108985901\n",
      "cnt: 0 - valLoss: 0.6606324315071106 - trainLoss: 0.6602333188056946\n",
      "cnt: 0 - valLoss: 0.6606308221817017 - trainLoss: 0.6602321863174438\n",
      "cnt: 0 - valLoss: 0.6606292128562927 - trainLoss: 0.6602309346199036\n",
      "cnt: 0 - valLoss: 0.660627543926239 - trainLoss: 0.6602297425270081\n",
      "cnt: 0 - valLoss: 0.6606259346008301 - trainLoss: 0.6602286100387573\n",
      "cnt: 0 - valLoss: 0.6606243252754211 - trainLoss: 0.660227358341217\n",
      "cnt: 0 - valLoss: 0.6606227159500122 - trainLoss: 0.6602261662483215\n",
      "cnt: 0 - valLoss: 0.6606211066246033 - trainLoss: 0.6602250933647156\n",
      "cnt: 0 - valLoss: 0.6606194972991943 - trainLoss: 0.6602239012718201\n",
      "cnt: 0 - valLoss: 0.6606178879737854 - trainLoss: 0.6602227091789246\n",
      "cnt: 0 - valLoss: 0.6606162786483765 - trainLoss: 0.6602215766906738\n",
      "cnt: 0 - valLoss: 0.6606147289276123 - trainLoss: 0.6602203845977783\n",
      "cnt: 0 - valLoss: 0.6606130599975586 - trainLoss: 0.6602191925048828\n",
      "cnt: 0 - valLoss: 0.6606115102767944 - trainLoss: 0.6602180600166321\n",
      "cnt: 0 - valLoss: 0.6606098413467407 - trainLoss: 0.6602168679237366\n",
      "cnt: 0 - valLoss: 0.6606082916259766 - trainLoss: 0.6602156758308411\n",
      "cnt: 0 - valLoss: 0.6606066823005676 - trainLoss: 0.6602144837379456\n",
      "cnt: 0 - valLoss: 0.6606050133705139 - trainLoss: 0.6602133512496948\n",
      "cnt: 0 - valLoss: 0.660603404045105 - trainLoss: 0.6602121591567993\n",
      "cnt: 0 - valLoss: 0.660601794719696 - trainLoss: 0.6602109670639038\n",
      "cnt: 0 - valLoss: 0.6606001853942871 - trainLoss: 0.6602098345756531\n",
      "cnt: 0 - valLoss: 0.6605985760688782 - trainLoss: 0.6602086424827576\n",
      "cnt: 0 - valLoss: 0.6605969667434692 - trainLoss: 0.6602075099945068\n",
      "cnt: 0 - valLoss: 0.6605953574180603 - trainLoss: 0.6602063179016113\n",
      "cnt: 0 - valLoss: 0.6605936884880066 - trainLoss: 0.6602051258087158\n",
      "cnt: 0 - valLoss: 0.6605921387672424 - trainLoss: 0.6602039337158203\n",
      "cnt: 0 - valLoss: 0.6605905294418335 - trainLoss: 0.6602027416229248\n",
      "cnt: 0 - valLoss: 0.6605889201164246 - trainLoss: 0.6602015495300293\n",
      "cnt: 0 - valLoss: 0.6605872511863708 - trainLoss: 0.6602004170417786\n",
      "cnt: 0 - valLoss: 0.6605857014656067 - trainLoss: 0.6601991653442383\n",
      "cnt: 0 - valLoss: 0.660584032535553 - trainLoss: 0.6601980924606323\n",
      "cnt: 0 - valLoss: 0.6605824828147888 - trainLoss: 0.660196840763092\n",
      "cnt: 0 - valLoss: 0.6605808138847351 - trainLoss: 0.6601956486701965\n",
      "cnt: 0 - valLoss: 0.6605791449546814 - trainLoss: 0.660194456577301\n",
      "cnt: 0 - valLoss: 0.6605775356292725 - trainLoss: 0.6601933240890503\n",
      "cnt: 0 - valLoss: 0.6605759263038635 - trainLoss: 0.6601921319961548\n",
      "cnt: 0 - valLoss: 0.6605743169784546 - trainLoss: 0.6601909399032593\n",
      "cnt: 0 - valLoss: 0.6605727076530457 - trainLoss: 0.6601897478103638\n",
      "cnt: 0 - valLoss: 0.6605710983276367 - trainLoss: 0.6601885557174683\n",
      "cnt: 0 - valLoss: 0.660569429397583 - trainLoss: 0.6601874232292175\n",
      "cnt: 0 - valLoss: 0.6605678200721741 - trainLoss: 0.6601861715316772\n",
      "cnt: 0 - valLoss: 0.6605661511421204 - trainLoss: 0.6601850390434265\n",
      "cnt: 0 - valLoss: 0.6605645418167114 - trainLoss: 0.6601837873458862\n",
      "cnt: 0 - valLoss: 0.6605629324913025 - trainLoss: 0.6601827144622803\n",
      "cnt: 0 - valLoss: 0.6605613231658936 - trainLoss: 0.66018146276474\n",
      "cnt: 0 - valLoss: 0.6605596542358398 - trainLoss: 0.6601802706718445\n",
      "cnt: 0 - valLoss: 0.6605580449104309 - trainLoss: 0.660179078578949\n",
      "cnt: 0 - valLoss: 0.660556435585022 - trainLoss: 0.6601778864860535\n",
      "cnt: 0 - valLoss: 0.6605547666549683 - trainLoss: 0.660176694393158\n",
      "cnt: 0 - valLoss: 0.6605532169342041 - trainLoss: 0.6601755023002625\n",
      "cnt: 0 - valLoss: 0.6605515480041504 - trainLoss: 0.6601743102073669\n",
      "cnt: 0 - valLoss: 0.6605499386787415 - trainLoss: 0.6601731181144714\n",
      "cnt: 0 - valLoss: 0.6605482697486877 - trainLoss: 0.6601719856262207\n",
      "cnt: 0 - valLoss: 0.6605466604232788 - trainLoss: 0.6601707339286804\n",
      "cnt: 0 - valLoss: 0.6605449914932251 - trainLoss: 0.6601696014404297\n",
      "cnt: 0 - valLoss: 0.6605433821678162 - trainLoss: 0.6601684093475342\n",
      "cnt: 0 - valLoss: 0.6605417728424072 - trainLoss: 0.6601672172546387\n",
      "cnt: 0 - valLoss: 0.6605401039123535 - trainLoss: 0.6601660251617432\n",
      "cnt: 0 - valLoss: 0.6605384945869446 - trainLoss: 0.6601648926734924\n",
      "cnt: 0 - valLoss: 0.6605368852615356 - trainLoss: 0.6601636409759521\n",
      "cnt: 0 - valLoss: 0.6605352163314819 - trainLoss: 0.6601625084877014\n",
      "cnt: 0 - valLoss: 0.660533607006073 - trainLoss: 0.6601613163948059\n",
      "cnt: 0 - valLoss: 0.6605319976806641 - trainLoss: 0.6601601243019104\n",
      "cnt: 0 - valLoss: 0.6605303287506104 - trainLoss: 0.6601589322090149\n",
      "cnt: 0 - valLoss: 0.6605286598205566 - trainLoss: 0.6601577401161194\n",
      "cnt: 0 - valLoss: 0.6605270504951477 - trainLoss: 0.6601566076278687\n",
      "cnt: 0 - valLoss: 0.6605254411697388 - trainLoss: 0.6601553559303284\n",
      "cnt: 0 - valLoss: 0.6605237722396851 - trainLoss: 0.6601542234420776\n",
      "cnt: 0 - valLoss: 0.6605221033096313 - trainLoss: 0.6601529717445374\n",
      "cnt: 0 - valLoss: 0.6605204939842224 - trainLoss: 0.6601517796516418\n",
      "cnt: 0 - valLoss: 0.6605188846588135 - trainLoss: 0.6601505875587463\n",
      "cnt: 0 - valLoss: 0.660517156124115 - trainLoss: 0.6601494550704956\n",
      "cnt: 0 - valLoss: 0.660515546798706 - trainLoss: 0.6601482629776001\n",
      "cnt: 0 - valLoss: 0.6605138778686523 - trainLoss: 0.6601470112800598\n",
      "cnt: 0 - valLoss: 0.6605122685432434 - trainLoss: 0.6601458787918091\n",
      "cnt: 0 - valLoss: 0.6605105996131897 - trainLoss: 0.6601446270942688\n",
      "cnt: 0 - valLoss: 0.660508930683136 - trainLoss: 0.6601434350013733\n",
      "cnt: 0 - valLoss: 0.6605072617530823 - trainLoss: 0.6601423025131226\n",
      "cnt: 0 - valLoss: 0.6605055928230286 - trainLoss: 0.660141110420227\n",
      "cnt: 0 - valLoss: 0.6605039834976196 - trainLoss: 0.6601399183273315\n",
      "cnt: 0 - valLoss: 0.6605023741722107 - trainLoss: 0.6601386666297913\n",
      "cnt: 0 - valLoss: 0.660500705242157 - trainLoss: 0.6601374745368958\n",
      "cnt: 0 - valLoss: 0.6604990363121033 - trainLoss: 0.6601362824440002\n",
      "cnt: 0 - valLoss: 0.6604974269866943 - trainLoss: 0.6601350903511047\n",
      "cnt: 0 - valLoss: 0.6604957580566406 - trainLoss: 0.6601338982582092\n",
      "cnt: 0 - valLoss: 0.6604940891265869 - trainLoss: 0.6601327061653137\n",
      "cnt: 0 - valLoss: 0.660492479801178 - trainLoss: 0.6601315140724182\n",
      "cnt: 0 - valLoss: 0.6604908108711243 - trainLoss: 0.6601303219795227\n",
      "cnt: 0 - valLoss: 0.6604890823364258 - trainLoss: 0.6601291298866272\n",
      "cnt: 0 - valLoss: 0.6604874134063721 - trainLoss: 0.6601279377937317\n",
      "cnt: 0 - valLoss: 0.6604857444763184 - trainLoss: 0.6601267457008362\n",
      "cnt: 0 - valLoss: 0.6604840755462646 - trainLoss: 0.6601254940032959\n",
      "cnt: 0 - valLoss: 0.6604823470115662 - trainLoss: 0.6601243019104004\n",
      "cnt: 0 - valLoss: 0.6604806780815125 - trainLoss: 0.6601230502128601\n",
      "cnt: 0 - valLoss: 0.6604790091514587 - trainLoss: 0.6601218581199646\n",
      "cnt: 0 - valLoss: 0.660477340221405 - trainLoss: 0.6601206660270691\n",
      "cnt: 0 - valLoss: 0.6604756116867065 - trainLoss: 0.6601194143295288\n",
      "cnt: 0 - valLoss: 0.6604738831520081 - trainLoss: 0.6601181030273438\n",
      "cnt: 0 - valLoss: 0.6604721546173096 - trainLoss: 0.660116970539093\n",
      "cnt: 0 - valLoss: 0.6604704260826111 - trainLoss: 0.660115659236908\n",
      "cnt: 0 - valLoss: 0.6604687571525574 - trainLoss: 0.6601144075393677\n",
      "cnt: 0 - valLoss: 0.6604670286178589 - trainLoss: 0.6601131558418274\n",
      "cnt: 0 - valLoss: 0.6604653000831604 - trainLoss: 0.6601119041442871\n",
      "cnt: 0 - valLoss: 0.6604636311531067 - trainLoss: 0.6601106524467468\n",
      "cnt: 0 - valLoss: 0.660461962223053 - trainLoss: 0.6601094007492065\n",
      "cnt: 0 - valLoss: 0.6604601740837097 - trainLoss: 0.6601081490516663\n",
      "cnt: 0 - valLoss: 0.6604584455490112 - trainLoss: 0.6601069569587708\n",
      "cnt: 0 - valLoss: 0.6604567170143127 - trainLoss: 0.6601056456565857\n",
      "cnt: 0 - valLoss: 0.6604549884796143 - trainLoss: 0.6601044535636902\n",
      "cnt: 0 - valLoss: 0.6604533791542053 - trainLoss: 0.6601032018661499\n",
      "cnt: 0 - valLoss: 0.6604515910148621 - trainLoss: 0.6601019501686096\n",
      "cnt: 0 - valLoss: 0.6604499220848083 - trainLoss: 0.6601006984710693\n",
      "cnt: 0 - valLoss: 0.6604481935501099 - trainLoss: 0.6600995063781738\n",
      "cnt: 0 - valLoss: 0.6604465246200562 - trainLoss: 0.6600982546806335\n",
      "cnt: 0 - valLoss: 0.6604447364807129 - trainLoss: 0.6600969433784485\n",
      "cnt: 0 - valLoss: 0.6604430675506592 - trainLoss: 0.660095751285553\n",
      "cnt: 0 - valLoss: 0.6604413390159607 - trainLoss: 0.6600944995880127\n",
      "cnt: 0 - valLoss: 0.6604395508766174 - trainLoss: 0.6600932478904724\n",
      "cnt: 0 - valLoss: 0.660437822341919 - trainLoss: 0.6600920557975769\n",
      "cnt: 0 - valLoss: 0.6604361534118652 - trainLoss: 0.6600907444953918\n",
      "cnt: 0 - valLoss: 0.6604344248771667 - trainLoss: 0.6600894927978516\n",
      "cnt: 0 - valLoss: 0.660432755947113 - trainLoss: 0.660088300704956\n",
      "cnt: 0 - valLoss: 0.6604309678077698 - trainLoss: 0.660086989402771\n",
      "cnt: 0 - valLoss: 0.6604292988777161 - trainLoss: 0.6600857973098755\n",
      "cnt: 0 - valLoss: 0.6604276299476624 - trainLoss: 0.6600844860076904\n",
      "cnt: 0 - valLoss: 0.6604259014129639 - trainLoss: 0.6600832939147949\n",
      "cnt: 0 - valLoss: 0.6604241728782654 - trainLoss: 0.6600819826126099\n",
      "cnt: 0 - valLoss: 0.6604224443435669 - trainLoss: 0.6600807309150696\n",
      "cnt: 0 - valLoss: 0.6604207158088684 - trainLoss: 0.6600794792175293\n",
      "cnt: 0 - valLoss: 0.6604190468788147 - trainLoss: 0.6600782871246338\n",
      "cnt: 0 - valLoss: 0.660417377948761 - trainLoss: 0.6600770950317383\n",
      "cnt: 0 - valLoss: 0.6604157090187073 - trainLoss: 0.660075843334198\n",
      "cnt: 0 - valLoss: 0.6604140400886536 - trainLoss: 0.6600746512413025\n",
      "cnt: 0 - valLoss: 0.6604123115539551 - trainLoss: 0.6600733995437622\n",
      "cnt: 0 - valLoss: 0.6604105830192566 - trainLoss: 0.6600722074508667\n",
      "cnt: 0 - valLoss: 0.6604089736938477 - trainLoss: 0.6600710153579712\n",
      "cnt: 0 - valLoss: 0.6604072451591492 - trainLoss: 0.6600698232650757\n",
      "cnt: 0 - valLoss: 0.6604055166244507 - trainLoss: 0.6600685715675354\n",
      "cnt: 0 - valLoss: 0.660403847694397 - trainLoss: 0.6600673794746399\n",
      "cnt: 0 - valLoss: 0.6604021787643433 - trainLoss: 0.6600661873817444\n",
      "cnt: 0 - valLoss: 0.6604004502296448 - trainLoss: 0.6600649356842041\n",
      "cnt: 0 - valLoss: 0.6603987216949463 - trainLoss: 0.6600637435913086\n",
      "cnt: 0 - valLoss: 0.6603971123695374 - trainLoss: 0.6600625514984131\n",
      "cnt: 0 - valLoss: 0.6603954434394836 - trainLoss: 0.6600612998008728\n",
      "cnt: 0 - valLoss: 0.6603937149047852 - trainLoss: 0.6600600481033325\n",
      "cnt: 0 - valLoss: 0.6603919863700867 - trainLoss: 0.660058856010437\n",
      "cnt: 0 - valLoss: 0.6603902578353882 - trainLoss: 0.6600576639175415\n",
      "cnt: 0 - valLoss: 0.6603885889053345 - trainLoss: 0.660056471824646\n",
      "cnt: 0 - valLoss: 0.660386860370636 - trainLoss: 0.6600552201271057\n",
      "cnt: 0 - valLoss: 0.6603851318359375 - trainLoss: 0.6600540280342102\n",
      "cnt: 0 - valLoss: 0.6603835225105286 - trainLoss: 0.6600528359413147\n",
      "cnt: 0 - valLoss: 0.6603817939758301 - trainLoss: 0.6600515842437744\n",
      "cnt: 0 - valLoss: 0.6603800654411316 - trainLoss: 0.6600503325462341\n",
      "cnt: 0 - valLoss: 0.6603783369064331 - trainLoss: 0.6600491404533386\n",
      "cnt: 0 - valLoss: 0.6603766679763794 - trainLoss: 0.6600479483604431\n",
      "cnt: 0 - valLoss: 0.6603749990463257 - trainLoss: 0.6600466966629028\n",
      "cnt: 0 - valLoss: 0.660373330116272 - trainLoss: 0.6600455641746521\n",
      "cnt: 0 - valLoss: 0.6603716611862183 - trainLoss: 0.6600444316864014\n",
      "cnt: 0 - valLoss: 0.6603700518608093 - trainLoss: 0.6600432395935059\n",
      "cnt: 0 - valLoss: 0.6603683829307556 - trainLoss: 0.6600421667098999\n",
      "cnt: 0 - valLoss: 0.6603667140007019 - trainLoss: 0.6600409150123596\n",
      "cnt: 0 - valLoss: 0.660365104675293 - trainLoss: 0.6600398421287537\n",
      "cnt: 0 - valLoss: 0.660363495349884 - trainLoss: 0.6600387096405029\n",
      "cnt: 0 - valLoss: 0.6603618264198303 - trainLoss: 0.6600375771522522\n",
      "cnt: 0 - valLoss: 0.6603601574897766 - trainLoss: 0.6600363850593567\n",
      "cnt: 0 - valLoss: 0.6603585481643677 - trainLoss: 0.6600353121757507\n",
      "cnt: 0 - valLoss: 0.660356879234314 - trainLoss: 0.6600341200828552\n",
      "cnt: 0 - valLoss: 0.6603552103042603 - trainLoss: 0.6600330471992493\n",
      "cnt: 0 - valLoss: 0.6603536009788513 - trainLoss: 0.6600318551063538\n",
      "cnt: 0 - valLoss: 0.6603519320487976 - trainLoss: 0.660030722618103\n",
      "cnt: 0 - valLoss: 0.6603502631187439 - trainLoss: 0.6600295901298523\n",
      "cnt: 0 - valLoss: 0.6603485941886902 - trainLoss: 0.6600283980369568\n",
      "cnt: 0 - valLoss: 0.6603469848632812 - trainLoss: 0.660027265548706\n",
      "cnt: 0 - valLoss: 0.6603452563285828 - trainLoss: 0.6600261330604553\n",
      "cnt: 0 - valLoss: 0.6603436470031738 - trainLoss: 0.6600250005722046\n",
      "cnt: 0 - valLoss: 0.6603420376777649 - trainLoss: 0.6600238680839539\n",
      "cnt: 0 - valLoss: 0.6603403687477112 - trainLoss: 0.6600227355957031\n",
      "cnt: 0 - valLoss: 0.6603386402130127 - trainLoss: 0.6600216031074524\n",
      "cnt: 0 - valLoss: 0.6603370308876038 - trainLoss: 0.6600204110145569\n",
      "cnt: 0 - valLoss: 0.6603354215621948 - trainLoss: 0.6600192785263062\n",
      "cnt: 0 - valLoss: 0.6603337526321411 - trainLoss: 0.6600181460380554\n",
      "cnt: 0 - valLoss: 0.6603320837020874 - trainLoss: 0.6600170135498047\n",
      "cnt: 0 - valLoss: 0.6603304147720337 - trainLoss: 0.660015881061554\n",
      "cnt: 0 - valLoss: 0.66032874584198 - trainLoss: 0.6600147485733032\n",
      "cnt: 0 - valLoss: 0.6603270769119263 - trainLoss: 0.6600135564804077\n",
      "cnt: 0 - valLoss: 0.6603254079818726 - trainLoss: 0.660012423992157\n",
      "cnt: 0 - valLoss: 0.6603238582611084 - trainLoss: 0.6600112318992615\n",
      "cnt: 0 - valLoss: 0.6603221297264099 - trainLoss: 0.660010039806366\n",
      "cnt: 0 - valLoss: 0.6603204011917114 - trainLoss: 0.6600089073181152\n",
      "cnt: 0 - valLoss: 0.6603187918663025 - trainLoss: 0.6600077152252197\n",
      "cnt: 0 - valLoss: 0.6603171229362488 - trainLoss: 0.6600065231323242\n",
      "cnt: 0 - valLoss: 0.6603154540061951 - trainLoss: 0.6600053906440735\n",
      "cnt: 0 - valLoss: 0.6603137850761414 - trainLoss: 0.660004198551178\n",
      "cnt: 0 - valLoss: 0.6603121161460876 - trainLoss: 0.6600030064582825\n",
      "cnt: 0 - valLoss: 0.6603104472160339 - trainLoss: 0.6600019335746765\n",
      "cnt: 0 - valLoss: 0.6603087782859802 - trainLoss: 0.660000741481781\n",
      "cnt: 0 - valLoss: 0.6603071093559265 - trainLoss: 0.6599994897842407\n",
      "cnt: 0 - valLoss: 0.660305380821228 - trainLoss: 0.65999835729599\n",
      "cnt: 0 - valLoss: 0.6603037118911743 - trainLoss: 0.6599972248077393\n",
      "cnt: 0 - valLoss: 0.6603020429611206 - trainLoss: 0.659995973110199\n",
      "cnt: 0 - valLoss: 0.6603004336357117 - trainLoss: 0.6599948406219482\n",
      "cnt: 0 - valLoss: 0.6602987051010132 - trainLoss: 0.6599936485290527\n",
      "cnt: 0 - valLoss: 0.6602970361709595 - trainLoss: 0.6599924564361572\n",
      "cnt: 0 - valLoss: 0.6602953672409058 - trainLoss: 0.6599912643432617\n",
      "cnt: 0 - valLoss: 0.6602936387062073 - trainLoss: 0.6599900722503662\n",
      "cnt: 0 - valLoss: 0.6602920293807983 - trainLoss: 0.6599889397621155\n",
      "cnt: 0 - valLoss: 0.6602903008460999 - trainLoss: 0.6599876880645752\n",
      "cnt: 0 - valLoss: 0.6602886319160461 - trainLoss: 0.6599865555763245\n",
      "cnt: 0 - valLoss: 0.6602869629859924 - trainLoss: 0.6599853038787842\n",
      "cnt: 0 - valLoss: 0.660285234451294 - trainLoss: 0.6599841117858887\n",
      "cnt: 0 - valLoss: 0.6602835655212402 - trainLoss: 0.6599829196929932\n",
      "cnt: 0 - valLoss: 0.6602818965911865 - trainLoss: 0.6599817276000977\n",
      "cnt: 0 - valLoss: 0.6602802276611328 - trainLoss: 0.6599805355072021\n",
      "cnt: 0 - valLoss: 0.6602785587310791 - trainLoss: 0.6599794030189514\n",
      "cnt: 0 - valLoss: 0.6602767705917358 - trainLoss: 0.6599782109260559\n",
      "cnt: 0 - valLoss: 0.6602751612663269 - trainLoss: 0.6599770188331604\n",
      "cnt: 0 - valLoss: 0.6602734327316284 - trainLoss: 0.6599758267402649\n",
      "cnt: 0 - valLoss: 0.6602717041969299 - trainLoss: 0.6599745750427246\n",
      "cnt: 0 - valLoss: 0.6602700352668762 - trainLoss: 0.6599734425544739\n",
      "cnt: 0 - valLoss: 0.6602683663368225 - trainLoss: 0.6599722504615784\n",
      "cnt: 0 - valLoss: 0.6602666974067688 - trainLoss: 0.6599709987640381\n",
      "cnt: 0 - valLoss: 0.6602649688720703 - trainLoss: 0.6599698066711426\n",
      "cnt: 0 - valLoss: 0.6602632999420166 - trainLoss: 0.6599686741828918\n",
      "cnt: 0 - valLoss: 0.6602616310119629 - trainLoss: 0.6599674820899963\n",
      "cnt: 0 - valLoss: 0.6602599024772644 - trainLoss: 0.659966230392456\n",
      "cnt: 0 - valLoss: 0.6602581739425659 - trainLoss: 0.6599650979042053\n",
      "cnt: 0 - valLoss: 0.660256564617157 - trainLoss: 0.659963846206665\n",
      "cnt: 0 - valLoss: 0.6602547764778137 - trainLoss: 0.6599626541137695\n",
      "cnt: 0 - valLoss: 0.6602530479431152 - trainLoss: 0.659961462020874\n",
      "cnt: 0 - valLoss: 0.6602513790130615 - trainLoss: 0.6599602699279785\n",
      "cnt: 0 - valLoss: 0.6602497100830078 - trainLoss: 0.659959077835083\n",
      "cnt: 0 - valLoss: 0.6602480411529541 - trainLoss: 0.6599578857421875\n",
      "cnt: 0 - valLoss: 0.6602463126182556 - trainLoss: 0.659956693649292\n",
      "cnt: 0 - valLoss: 0.6602446436882019 - trainLoss: 0.6599555015563965\n",
      "cnt: 0 - valLoss: 0.6602429151535034 - trainLoss: 0.6599542498588562\n",
      "cnt: 0 - valLoss: 0.6602411866188049 - trainLoss: 0.6599531173706055\n",
      "cnt: 0 - valLoss: 0.6602394580841064 - trainLoss: 0.65995192527771\n",
      "cnt: 0 - valLoss: 0.6602377891540527 - trainLoss: 0.6599507331848145\n",
      "cnt: 0 - valLoss: 0.6602360606193542 - trainLoss: 0.659949541091919\n",
      "cnt: 0 - valLoss: 0.6602343916893005 - trainLoss: 0.6599483489990234\n",
      "cnt: 0 - valLoss: 0.6602326035499573 - trainLoss: 0.6599470973014832\n",
      "cnt: 0 - valLoss: 0.6602309346199036 - trainLoss: 0.6599459052085876\n",
      "cnt: 0 - valLoss: 0.6602292656898499 - trainLoss: 0.6599447131156921\n",
      "cnt: 0 - valLoss: 0.6602274775505066 - trainLoss: 0.6599435806274414\n",
      "cnt: 0 - valLoss: 0.6602258086204529 - trainLoss: 0.6599423289299011\n",
      "cnt: 0 - valLoss: 0.6602240800857544 - trainLoss: 0.6599411368370056\n",
      "cnt: 0 - valLoss: 0.6602223515510559 - trainLoss: 0.6599399447441101\n",
      "cnt: 0 - valLoss: 0.6602206826210022 - trainLoss: 0.6599387526512146\n",
      "cnt: 0 - valLoss: 0.6602190136909485 - trainLoss: 0.6599375605583191\n",
      "cnt: 0 - valLoss: 0.6602172255516052 - trainLoss: 0.6599363088607788\n",
      "cnt: 0 - valLoss: 0.6602155566215515 - trainLoss: 0.6599351167678833\n",
      "cnt: 0 - valLoss: 0.660213828086853 - trainLoss: 0.6599339246749878\n",
      "cnt: 0 - valLoss: 0.6602120995521545 - trainLoss: 0.6599327325820923\n",
      "cnt: 0 - valLoss: 0.660210371017456 - trainLoss: 0.6599315404891968\n",
      "cnt: 0 - valLoss: 0.6602086424827576 - trainLoss: 0.6599303483963013\n",
      "cnt: 0 - valLoss: 0.6602069139480591 - trainLoss: 0.659929096698761\n",
      "cnt: 0 - valLoss: 0.6602051854133606 - trainLoss: 0.6599279046058655\n",
      "cnt: 0 - valLoss: 0.6602035164833069 - trainLoss: 0.6599267721176147\n",
      "cnt: 0 - valLoss: 0.6602017879486084 - trainLoss: 0.6599254608154297\n",
      "cnt: 0 - valLoss: 0.6602001190185547 - trainLoss: 0.659924328327179\n",
      "cnt: 0 - valLoss: 0.6601983904838562 - trainLoss: 0.6599231362342834\n",
      "cnt: 0 - valLoss: 0.6601966023445129 - trainLoss: 0.6599218845367432\n",
      "cnt: 0 - valLoss: 0.6601949334144592 - trainLoss: 0.6599207520484924\n",
      "cnt: 0 - valLoss: 0.6601932048797607 - trainLoss: 0.6599195003509521\n",
      "cnt: 0 - valLoss: 0.6601914763450623 - trainLoss: 0.6599182486534119\n",
      "cnt: 0 - valLoss: 0.6601897478103638 - trainLoss: 0.6599170565605164\n",
      "cnt: 0 - valLoss: 0.6601880192756653 - trainLoss: 0.6599158644676208\n",
      "cnt: 0 - valLoss: 0.6601862907409668 - trainLoss: 0.6599146127700806\n",
      "cnt: 0 - valLoss: 0.6601846218109131 - trainLoss: 0.6599134206771851\n",
      "cnt: 0 - valLoss: 0.6601828336715698 - trainLoss: 0.6599121689796448\n",
      "cnt: 0 - valLoss: 0.6601811051368713 - trainLoss: 0.6599109768867493\n",
      "cnt: 0 - valLoss: 0.6601793766021729 - trainLoss: 0.659909725189209\n",
      "cnt: 0 - valLoss: 0.6601776480674744 - trainLoss: 0.6599084734916687\n",
      "cnt: 0 - valLoss: 0.6601759195327759 - trainLoss: 0.6599072813987732\n",
      "cnt: 0 - valLoss: 0.6601741909980774 - trainLoss: 0.6599060297012329\n",
      "cnt: 0 - valLoss: 0.6601725220680237 - trainLoss: 0.6599047183990479\n",
      "cnt: 0 - valLoss: 0.6601707339286804 - trainLoss: 0.6599035263061523\n",
      "cnt: 0 - valLoss: 0.6601690053939819 - trainLoss: 0.6599023342132568\n",
      "cnt: 0 - valLoss: 0.6601673364639282 - trainLoss: 0.6599010825157166\n",
      "cnt: 0 - valLoss: 0.6601656079292297 - trainLoss: 0.6598998308181763\n",
      "cnt: 0 - valLoss: 0.6601638197898865 - trainLoss: 0.659898579120636\n",
      "cnt: 0 - valLoss: 0.660162091255188 - trainLoss: 0.6598973870277405\n",
      "cnt: 0 - valLoss: 0.6601603627204895 - trainLoss: 0.6598961353302002\n",
      "cnt: 0 - valLoss: 0.660158634185791 - trainLoss: 0.6598948836326599\n",
      "cnt: 0 - valLoss: 0.6601569056510925 - trainLoss: 0.6598936915397644\n",
      "cnt: 0 - valLoss: 0.6601551175117493 - trainLoss: 0.6598924398422241\n",
      "cnt: 0 - valLoss: 0.6601534485816956 - trainLoss: 0.6598911881446838\n",
      "cnt: 0 - valLoss: 0.6601517200469971 - trainLoss: 0.6598899364471436\n",
      "cnt: 0 - valLoss: 0.6601499319076538 - trainLoss: 0.659888744354248\n",
      "cnt: 0 - valLoss: 0.6601482033729553 - trainLoss: 0.6598874926567078\n",
      "cnt: 0 - valLoss: 0.6601464748382568 - trainLoss: 0.6598862409591675\n",
      "cnt: 0 - valLoss: 0.6601447463035583 - trainLoss: 0.6598849892616272\n",
      "cnt: 0 - valLoss: 0.6601429581642151 - trainLoss: 0.6598837971687317\n",
      "cnt: 0 - valLoss: 0.6601411700248718 - trainLoss: 0.6598824858665466\n",
      "cnt: 0 - valLoss: 0.6601394414901733 - trainLoss: 0.6598812341690063\n",
      "cnt: 0 - valLoss: 0.6601376533508301 - trainLoss: 0.6598799228668213\n",
      "cnt: 0 - valLoss: 0.6601359248161316 - trainLoss: 0.659878671169281\n",
      "cnt: 0 - valLoss: 0.6601341962814331 - trainLoss: 0.659877359867096\n",
      "cnt: 0 - valLoss: 0.6601324081420898 - trainLoss: 0.6598761081695557\n",
      "cnt: 0 - valLoss: 0.6601306200027466 - trainLoss: 0.6598748564720154\n",
      "cnt: 0 - valLoss: 0.6601288914680481 - trainLoss: 0.6598735451698303\n",
      "cnt: 0 - valLoss: 0.6601271629333496 - trainLoss: 0.6598722338676453\n",
      "cnt: 0 - valLoss: 0.6601253747940063 - trainLoss: 0.6598710417747498\n",
      "cnt: 0 - valLoss: 0.6601235866546631 - trainLoss: 0.6598696708679199\n",
      "cnt: 0 - valLoss: 0.6601218581199646 - trainLoss: 0.6598684191703796\n",
      "cnt: 0 - valLoss: 0.6601200699806213 - trainLoss: 0.6598671078681946\n",
      "cnt: 0 - valLoss: 0.6601182818412781 - trainLoss: 0.6598658561706543\n",
      "cnt: 0 - valLoss: 0.6601165533065796 - trainLoss: 0.6598644852638245\n",
      "cnt: 0 - valLoss: 0.6601147651672363 - trainLoss: 0.6598631739616394\n",
      "cnt: 0 - valLoss: 0.6601129770278931 - trainLoss: 0.6598619222640991\n",
      "cnt: 0 - valLoss: 0.6601111888885498 - trainLoss: 0.6598605513572693\n",
      "cnt: 0 - valLoss: 0.6601094603538513 - trainLoss: 0.659859299659729\n",
      "cnt: 0 - valLoss: 0.6601076722145081 - trainLoss: 0.659857988357544\n",
      "cnt: 0 - valLoss: 0.6601058840751648 - trainLoss: 0.6598566770553589\n",
      "cnt: 0 - valLoss: 0.6601040959358215 - trainLoss: 0.6598554253578186\n",
      "cnt: 0 - valLoss: 0.660102367401123 - trainLoss: 0.6598540544509888\n",
      "cnt: 0 - valLoss: 0.6601006388664246 - trainLoss: 0.6598528027534485\n",
      "cnt: 0 - valLoss: 0.6600987911224365 - trainLoss: 0.6598514318466187\n",
      "cnt: 0 - valLoss: 0.6600970029830933 - trainLoss: 0.6598501205444336\n",
      "cnt: 0 - valLoss: 0.66009521484375 - trainLoss: 0.6598488688468933\n",
      "cnt: 0 - valLoss: 0.6600934267044067 - trainLoss: 0.6598475575447083\n",
      "cnt: 0 - valLoss: 0.6600916385650635 - trainLoss: 0.6598461866378784\n",
      "cnt: 0 - valLoss: 0.660089910030365 - trainLoss: 0.6598449349403381\n",
      "cnt: 0 - valLoss: 0.6600881218910217 - trainLoss: 0.6598435640335083\n",
      "cnt: 0 - valLoss: 0.6600862741470337 - trainLoss: 0.659842312335968\n",
      "cnt: 0 - valLoss: 0.6600845456123352 - trainLoss: 0.659841001033783\n",
      "cnt: 0 - valLoss: 0.6600827574729919 - trainLoss: 0.6598396301269531\n",
      "cnt: 0 - valLoss: 0.6600809693336487 - trainLoss: 0.6598383784294128\n",
      "cnt: 0 - valLoss: 0.6600791811943054 - trainLoss: 0.6598370671272278\n",
      "cnt: 0 - valLoss: 0.6600773930549622 - trainLoss: 0.6598357558250427\n",
      "cnt: 0 - valLoss: 0.6600756049156189 - trainLoss: 0.6598344445228577\n",
      "cnt: 0 - valLoss: 0.6600738167762756 - trainLoss: 0.6598330736160278\n",
      "cnt: 0 - valLoss: 0.6600720286369324 - trainLoss: 0.659831702709198\n",
      "cnt: 0 - valLoss: 0.6600702404975891 - trainLoss: 0.6598305106163025\n",
      "cnt: 0 - valLoss: 0.6600684523582458 - trainLoss: 0.6598291397094727\n",
      "cnt: 0 - valLoss: 0.6600666642189026 - trainLoss: 0.6598278284072876\n",
      "cnt: 0 - valLoss: 0.6600648760795593 - trainLoss: 0.6598264575004578\n",
      "cnt: 0 - valLoss: 0.6600630879402161 - trainLoss: 0.6598252058029175\n",
      "cnt: 0 - valLoss: 0.6600612998008728 - trainLoss: 0.6598238348960876\n",
      "cnt: 0 - valLoss: 0.6600595116615295 - trainLoss: 0.6598225831985474\n",
      "cnt: 0 - valLoss: 0.6600577235221863 - trainLoss: 0.6598212122917175\n",
      "cnt: 0 - valLoss: 0.660055935382843 - trainLoss: 0.6598199009895325\n",
      "cnt: 0 - valLoss: 0.660054087638855 - trainLoss: 0.6598185300827026\n",
      "cnt: 0 - valLoss: 0.6600522994995117 - trainLoss: 0.6598172783851624\n",
      "cnt: 0 - valLoss: 0.6600505709648132 - trainLoss: 0.6598159074783325\n",
      "cnt: 0 - valLoss: 0.6600486636161804 - trainLoss: 0.6598145961761475\n",
      "cnt: 0 - valLoss: 0.6600469350814819 - trainLoss: 0.6598132848739624\n",
      "cnt: 0 - valLoss: 0.6600450873374939 - trainLoss: 0.6598119735717773\n",
      "cnt: 0 - valLoss: 0.6600432991981506 - trainLoss: 0.6598106026649475\n",
      "cnt: 0 - valLoss: 0.6600415706634521 - trainLoss: 0.6598092317581177\n",
      "cnt: 0 - valLoss: 0.6600396633148193 - trainLoss: 0.6598079204559326\n",
      "cnt: 0 - valLoss: 0.6600378751754761 - trainLoss: 0.6598066687583923\n",
      "cnt: 0 - valLoss: 0.6600360870361328 - trainLoss: 0.6598052978515625\n",
      "cnt: 0 - valLoss: 0.6600342392921448 - trainLoss: 0.6598039269447327\n",
      "cnt: 0 - valLoss: 0.6600324511528015 - trainLoss: 0.6598027348518372\n",
      "cnt: 0 - valLoss: 0.6600306630134583 - trainLoss: 0.6598013639450073\n",
      "cnt: 0 - valLoss: 0.660028874874115 - trainLoss: 0.6597999930381775\n",
      "cnt: 0 - valLoss: 0.660027027130127 - trainLoss: 0.6597986221313477\n",
      "cnt: 0 - valLoss: 0.6600252389907837 - trainLoss: 0.6597973108291626\n",
      "cnt: 0 - valLoss: 0.6600234508514404 - trainLoss: 0.6597960591316223\n",
      "cnt: 0 - valLoss: 0.6600216031074524 - trainLoss: 0.6597946882247925\n",
      "cnt: 0 - valLoss: 0.6600197553634644 - trainLoss: 0.6597933173179626\n",
      "cnt: 0 - valLoss: 0.6600180268287659 - trainLoss: 0.6597920060157776\n",
      "cnt: 0 - valLoss: 0.6600161790847778 - trainLoss: 0.6597906351089478\n",
      "cnt: 0 - valLoss: 0.6600143313407898 - trainLoss: 0.6597893238067627\n",
      "cnt: 0 - valLoss: 0.6600125432014465 - trainLoss: 0.6597880125045776\n",
      "cnt: 0 - valLoss: 0.6600107550621033 - trainLoss: 0.6597866415977478\n",
      "cnt: 0 - valLoss: 0.6600089073181152 - trainLoss: 0.6597853302955627\n",
      "cnt: 0 - valLoss: 0.660007119178772 - trainLoss: 0.6597839593887329\n",
      "cnt: 0 - valLoss: 0.6600052714347839 - trainLoss: 0.6597826480865479\n",
      "cnt: 0 - valLoss: 0.6600034236907959 - trainLoss: 0.6597812175750732\n",
      "cnt: 0 - valLoss: 0.6600015759468079 - trainLoss: 0.6597799062728882\n",
      "cnt: 0 - valLoss: 0.6599997878074646 - trainLoss: 0.6597785949707031\n",
      "cnt: 0 - valLoss: 0.6599979400634766 - trainLoss: 0.6597772240638733\n",
      "cnt: 0 - valLoss: 0.6599960923194885 - trainLoss: 0.6597758531570435\n",
      "cnt: 0 - valLoss: 0.6599943041801453 - trainLoss: 0.6597745418548584\n",
      "cnt: 0 - valLoss: 0.6599924564361572 - trainLoss: 0.6597731709480286\n",
      "cnt: 0 - valLoss: 0.6599906086921692 - trainLoss: 0.6597719192504883\n",
      "cnt: 0 - valLoss: 0.6599888205528259 - trainLoss: 0.6597704887390137\n",
      "cnt: 0 - valLoss: 0.6599869728088379 - trainLoss: 0.6597691774368286\n",
      "cnt: 0 - valLoss: 0.6599851846694946 - trainLoss: 0.6597678065299988\n",
      "cnt: 0 - valLoss: 0.6599834561347961 - trainLoss: 0.659766435623169\n",
      "cnt: 0 - valLoss: 0.6599816083908081 - trainLoss: 0.6597651243209839\n",
      "cnt: 0 - valLoss: 0.6599797606468201 - trainLoss: 0.659763753414154\n",
      "cnt: 0 - valLoss: 0.6599779725074768 - trainLoss: 0.6597623825073242\n",
      "cnt: 0 - valLoss: 0.6599761843681335 - trainLoss: 0.6597610712051392\n",
      "cnt: 0 - valLoss: 0.6599743366241455 - trainLoss: 0.6597597599029541\n",
      "cnt: 0 - valLoss: 0.6599725484848022 - trainLoss: 0.6597583889961243\n",
      "cnt: 0 - valLoss: 0.659970760345459 - trainLoss: 0.6597570180892944\n",
      "cnt: 0 - valLoss: 0.659968912601471 - trainLoss: 0.6597557067871094\n",
      "cnt: 0 - valLoss: 0.6599671244621277 - trainLoss: 0.6597543358802795\n",
      "cnt: 0 - valLoss: 0.6599652767181396 - trainLoss: 0.6597529649734497\n",
      "cnt: 0 - valLoss: 0.6599634885787964 - trainLoss: 0.6597515940666199\n",
      "cnt: 0 - valLoss: 0.6599617004394531 - trainLoss: 0.65975022315979\n",
      "cnt: 0 - valLoss: 0.6599598526954651 - trainLoss: 0.6597489714622498\n",
      "cnt: 0 - valLoss: 0.6599580645561218 - trainLoss: 0.6597475409507751\n",
      "cnt: 0 - valLoss: 0.6599562764167786 - trainLoss: 0.6597462296485901\n",
      "cnt: 0 - valLoss: 0.6599544286727905 - trainLoss: 0.6597448587417603\n",
      "cnt: 0 - valLoss: 0.6599526405334473 - trainLoss: 0.6597434878349304\n",
      "cnt: 0 - valLoss: 0.6599507927894592 - trainLoss: 0.6597421169281006\n",
      "cnt: 0 - valLoss: 0.6599489450454712 - trainLoss: 0.6597407460212708\n",
      "cnt: 0 - valLoss: 0.6599471569061279 - trainLoss: 0.6597393751144409\n",
      "cnt: 0 - valLoss: 0.6599453091621399 - trainLoss: 0.6597380042076111\n",
      "cnt: 0 - valLoss: 0.6599435210227966 - trainLoss: 0.659736692905426\n",
      "cnt: 0 - valLoss: 0.6599417328834534 - trainLoss: 0.6597353219985962\n",
      "cnt: 0 - valLoss: 0.6599398851394653 - trainLoss: 0.6597339510917664\n",
      "cnt: 0 - valLoss: 0.6599380970001221 - trainLoss: 0.6597326397895813\n",
      "cnt: 0 - valLoss: 0.659936249256134 - trainLoss: 0.6597313284873962\n",
      "cnt: 0 - valLoss: 0.6599344611167908 - trainLoss: 0.6597299575805664\n",
      "cnt: 0 - valLoss: 0.659932553768158 - trainLoss: 0.6597285866737366\n",
      "cnt: 0 - valLoss: 0.6599308252334595 - trainLoss: 0.6597272157669067\n",
      "cnt: 0 - valLoss: 0.6599289178848267 - trainLoss: 0.6597259044647217\n",
      "cnt: 0 - valLoss: 0.6599271297454834 - trainLoss: 0.6597245335578918\n",
      "cnt: 0 - valLoss: 0.6599252820014954 - trainLoss: 0.659723162651062\n",
      "cnt: 0 - valLoss: 0.6599234938621521 - trainLoss: 0.6597217917442322\n",
      "cnt: 0 - valLoss: 0.6599216461181641 - trainLoss: 0.6597205400466919\n",
      "cnt: 0 - valLoss: 0.659919798374176 - trainLoss: 0.6597190499305725\n",
      "cnt: 0 - valLoss: 0.659917950630188 - trainLoss: 0.6597177386283875\n",
      "cnt: 0 - valLoss: 0.6599161624908447 - trainLoss: 0.6597164273262024\n",
      "cnt: 0 - valLoss: 0.6599142551422119 - trainLoss: 0.6597150564193726\n",
      "cnt: 0 - valLoss: 0.6599124670028687 - trainLoss: 0.6597136855125427\n",
      "cnt: 0 - valLoss: 0.6599106192588806 - trainLoss: 0.6597123742103577\n",
      "cnt: 0 - valLoss: 0.6599088311195374 - trainLoss: 0.6597109436988831\n",
      "cnt: 0 - valLoss: 0.6599069833755493 - trainLoss: 0.6597095727920532\n",
      "cnt: 0 - valLoss: 0.6599051356315613 - trainLoss: 0.6597082614898682\n",
      "cnt: 0 - valLoss: 0.6599032878875732 - trainLoss: 0.6597068905830383\n",
      "cnt: 0 - valLoss: 0.6599013805389404 - trainLoss: 0.6597055196762085\n",
      "cnt: 0 - valLoss: 0.6598995327949524 - trainLoss: 0.6597042083740234\n",
      "cnt: 0 - valLoss: 0.6598978042602539 - trainLoss: 0.6597027778625488\n",
      "cnt: 0 - valLoss: 0.6598958969116211 - trainLoss: 0.6597014665603638\n",
      "cnt: 0 - valLoss: 0.6598940491676331 - trainLoss: 0.6597000956535339\n",
      "cnt: 0 - valLoss: 0.6598922610282898 - trainLoss: 0.6596987247467041\n",
      "cnt: 0 - valLoss: 0.659890353679657 - trainLoss: 0.6596973538398743\n",
      "cnt: 0 - valLoss: 0.659888505935669 - trainLoss: 0.6596959829330444\n",
      "cnt: 0 - valLoss: 0.6598867177963257 - trainLoss: 0.6596946120262146\n",
      "cnt: 0 - valLoss: 0.6598848104476929 - trainLoss: 0.6596932411193848\n",
      "cnt: 0 - valLoss: 0.6598829627037048 - trainLoss: 0.6596918702125549\n",
      "cnt: 0 - valLoss: 0.6598811745643616 - trainLoss: 0.6596905589103699\n",
      "cnt: 0 - valLoss: 0.6598792672157288 - trainLoss: 0.6596891283988953\n",
      "cnt: 0 - valLoss: 0.6598774194717407 - trainLoss: 0.6596877574920654\n",
      "cnt: 0 - valLoss: 0.6598755717277527 - trainLoss: 0.6596863865852356\n",
      "cnt: 0 - valLoss: 0.6598737239837646 - trainLoss: 0.6596850156784058\n",
      "cnt: 0 - valLoss: 0.6598718762397766 - trainLoss: 0.6596836447715759\n",
      "cnt: 0 - valLoss: 0.6598699688911438 - trainLoss: 0.6596822738647461\n",
      "cnt: 0 - valLoss: 0.6598681211471558 - trainLoss: 0.6596809029579163\n",
      "cnt: 0 - valLoss: 0.6598662734031677 - trainLoss: 0.6596795320510864\n",
      "cnt: 0 - valLoss: 0.6598644256591797 - trainLoss: 0.6596781015396118\n",
      "cnt: 0 - valLoss: 0.6598625183105469 - trainLoss: 0.659676730632782\n",
      "cnt: 0 - valLoss: 0.6598606705665588 - trainLoss: 0.6596753597259521\n",
      "cnt: 0 - valLoss: 0.659858763217926 - trainLoss: 0.6596739888191223\n",
      "cnt: 0 - valLoss: 0.6598569750785828 - trainLoss: 0.6596726179122925\n",
      "cnt: 0 - valLoss: 0.6598551273345947 - trainLoss: 0.6596712470054626\n",
      "cnt: 0 - valLoss: 0.6598532795906067 - trainLoss: 0.6596698760986328\n",
      "cnt: 0 - valLoss: 0.6598513722419739 - trainLoss: 0.659668505191803\n",
      "cnt: 0 - valLoss: 0.6598494648933411 - trainLoss: 0.6596671342849731\n",
      "cnt: 0 - valLoss: 0.659847617149353 - trainLoss: 0.6596656441688538\n",
      "cnt: 0 - valLoss: 0.659845769405365 - trainLoss: 0.6596642732620239\n",
      "cnt: 0 - valLoss: 0.6598438620567322 - trainLoss: 0.6596629023551941\n",
      "cnt: 0 - valLoss: 0.6598420143127441 - trainLoss: 0.6596615314483643\n",
      "cnt: 0 - valLoss: 0.6598401665687561 - trainLoss: 0.6596601605415344\n",
      "cnt: 0 - valLoss: 0.6598382592201233 - trainLoss: 0.6596587300300598\n",
      "cnt: 0 - valLoss: 0.6598364114761353 - trainLoss: 0.65965735912323\n",
      "cnt: 0 - valLoss: 0.6598345041275024 - trainLoss: 0.6596560478210449\n",
      "cnt: 0 - valLoss: 0.6598326563835144 - trainLoss: 0.6596545577049255\n",
      "cnt: 0 - valLoss: 0.6598308086395264 - trainLoss: 0.6596531867980957\n",
      "cnt: 0 - valLoss: 0.6598289012908936 - trainLoss: 0.6596518158912659\n",
      "cnt: 0 - valLoss: 0.6598270535469055 - trainLoss: 0.659650444984436\n",
      "cnt: 0 - valLoss: 0.6598250865936279 - trainLoss: 0.6596490740776062\n",
      "cnt: 0 - valLoss: 0.6598232388496399 - trainLoss: 0.6596477031707764\n",
      "cnt: 0 - valLoss: 0.6598213911056519 - trainLoss: 0.6596463322639465\n",
      "cnt: 0 - valLoss: 0.659819483757019 - trainLoss: 0.6596448421478271\n",
      "cnt: 0 - valLoss: 0.659817636013031 - trainLoss: 0.6596435308456421\n",
      "cnt: 0 - valLoss: 0.6598157286643982 - trainLoss: 0.6596421003341675\n",
      "cnt: 0 - valLoss: 0.6598138809204102 - trainLoss: 0.6596407294273376\n",
      "cnt: 0 - valLoss: 0.6598119139671326 - trainLoss: 0.659639298915863\n",
      "cnt: 0 - valLoss: 0.6598101258277893 - trainLoss: 0.6596379280090332\n",
      "cnt: 0 - valLoss: 0.6598081588745117 - trainLoss: 0.6596365571022034\n",
      "cnt: 0 - valLoss: 0.6598063111305237 - trainLoss: 0.6596351265907288\n",
      "cnt: 0 - valLoss: 0.6598044037818909 - trainLoss: 0.6596337556838989\n",
      "cnt: 0 - valLoss: 0.6598024964332581 - trainLoss: 0.6596323847770691\n",
      "cnt: 0 - valLoss: 0.65980064868927 - trainLoss: 0.6596310138702393\n",
      "cnt: 0 - valLoss: 0.6597987413406372 - trainLoss: 0.6596296429634094\n",
      "cnt: 0 - valLoss: 0.6597968339920044 - trainLoss: 0.65962815284729\n",
      "cnt: 0 - valLoss: 0.6597949266433716 - trainLoss: 0.6596267819404602\n",
      "cnt: 0 - valLoss: 0.6597930192947388 - trainLoss: 0.6596253514289856\n",
      "cnt: 0 - valLoss: 0.6597910523414612 - trainLoss: 0.6596240401268005\n",
      "cnt: 0 - valLoss: 0.6597892045974731 - trainLoss: 0.6596226692199707\n",
      "cnt: 0 - valLoss: 0.6597872972488403 - trainLoss: 0.6596212387084961\n",
      "cnt: 0 - valLoss: 0.6597852110862732 - trainLoss: 0.6596198678016663\n",
      "cnt: 0 - valLoss: 0.6597833633422852 - trainLoss: 0.6596184372901917\n",
      "cnt: 0 - valLoss: 0.6597813963890076 - trainLoss: 0.659617006778717\n",
      "cnt: 0 - valLoss: 0.65977942943573 - trainLoss: 0.6596156358718872\n",
      "cnt: 0 - valLoss: 0.6597775220870972 - trainLoss: 0.6596142053604126\n",
      "cnt: 0 - valLoss: 0.6597754955291748 - trainLoss: 0.6596128344535828\n",
      "cnt: 0 - valLoss: 0.659773588180542 - trainLoss: 0.6596114635467529\n",
      "cnt: 0 - valLoss: 0.6597716808319092 - trainLoss: 0.6596100330352783\n",
      "cnt: 0 - valLoss: 0.6597696542739868 - trainLoss: 0.6596086621284485\n",
      "cnt: 0 - valLoss: 0.659767746925354 - trainLoss: 0.6596072912216187\n",
      "cnt: 0 - valLoss: 0.6597658395767212 - trainLoss: 0.6596058011054993\n",
      "cnt: 0 - valLoss: 0.6597638726234436 - trainLoss: 0.6596044301986694\n",
      "cnt: 0 - valLoss: 0.6597619652748108 - trainLoss: 0.6596030592918396\n",
      "cnt: 0 - valLoss: 0.6597598791122437 - trainLoss: 0.659601628780365\n",
      "cnt: 0 - valLoss: 0.6597579717636108 - trainLoss: 0.6596001982688904\n",
      "cnt: 0 - valLoss: 0.659756064414978 - trainLoss: 0.6595988273620605\n",
      "cnt: 0 - valLoss: 0.6597540974617004 - trainLoss: 0.6595974564552307\n",
      "cnt: 0 - valLoss: 0.6597521305084229 - trainLoss: 0.6595959663391113\n",
      "cnt: 0 - valLoss: 0.6597501635551453 - trainLoss: 0.6595945954322815\n",
      "cnt: 0 - valLoss: 0.6597483158111572 - trainLoss: 0.6595932245254517\n",
      "cnt: 0 - valLoss: 0.6597464084625244 - trainLoss: 0.659591794013977\n",
      "cnt: 0 - valLoss: 0.6597446203231812 - trainLoss: 0.6595904231071472\n",
      "cnt: 0 - valLoss: 0.6597428321838379 - trainLoss: 0.6595891118049622\n",
      "cnt: 0 - valLoss: 0.6597409248352051 - trainLoss: 0.6595877408981323\n",
      "cnt: 0 - valLoss: 0.6597391366958618 - trainLoss: 0.6595863103866577\n",
      "cnt: 0 - valLoss: 0.6597373485565186 - trainLoss: 0.6595849990844727\n",
      "cnt: 0 - valLoss: 0.6597355008125305 - trainLoss: 0.6595836281776428\n",
      "cnt: 0 - valLoss: 0.6597335934638977 - trainLoss: 0.659582257270813\n",
      "cnt: 0 - valLoss: 0.6597318053245544 - trainLoss: 0.6595808863639832\n",
      "cnt: 0 - valLoss: 0.6597299575805664 - trainLoss: 0.6595795154571533\n",
      "cnt: 0 - valLoss: 0.6597281694412231 - trainLoss: 0.6595781445503235\n",
      "cnt: 0 - valLoss: 0.6597262620925903 - trainLoss: 0.6595768332481384\n",
      "cnt: 0 - valLoss: 0.6597244739532471 - trainLoss: 0.6595754623413086\n",
      "cnt: 0 - valLoss: 0.659722626209259 - trainLoss: 0.6595740914344788\n",
      "cnt: 0 - valLoss: 0.6597208380699158 - trainLoss: 0.6595727801322937\n",
      "cnt: 0 - valLoss: 0.6597189903259277 - trainLoss: 0.6595714688301086\n",
      "cnt: 0 - valLoss: 0.6597172021865845 - trainLoss: 0.6595700979232788\n",
      "cnt: 0 - valLoss: 0.6597153544425964 - trainLoss: 0.659568727016449\n",
      "cnt: 0 - valLoss: 0.6597135663032532 - trainLoss: 0.6595674157142639\n",
      "cnt: 0 - valLoss: 0.6597117185592651 - trainLoss: 0.6595661044120789\n",
      "cnt: 0 - valLoss: 0.6597098708152771 - trainLoss: 0.6595647931098938\n",
      "cnt: 0 - valLoss: 0.6597080826759338 - trainLoss: 0.659563422203064\n",
      "cnt: 0 - valLoss: 0.6597062945365906 - trainLoss: 0.6595620512962341\n",
      "cnt: 0 - valLoss: 0.6597045063972473 - trainLoss: 0.6595607995986938\n",
      "cnt: 0 - valLoss: 0.6597026586532593 - trainLoss: 0.659559428691864\n",
      "cnt: 0 - valLoss: 0.659700870513916 - trainLoss: 0.659558117389679\n",
      "cnt: 0 - valLoss: 0.659699022769928 - trainLoss: 0.6595567464828491\n",
      "cnt: 0 - valLoss: 0.6596971750259399 - trainLoss: 0.6595554947853088\n",
      "cnt: 0 - valLoss: 0.6596953868865967 - trainLoss: 0.659554123878479\n",
      "cnt: 0 - valLoss: 0.6596935391426086 - trainLoss: 0.6595527529716492\n",
      "cnt: 0 - valLoss: 0.6596918106079102 - trainLoss: 0.6595514416694641\n",
      "cnt: 0 - valLoss: 0.6596900224685669 - trainLoss: 0.6595501899719238\n",
      "cnt: 0 - valLoss: 0.6596881151199341 - trainLoss: 0.6595488786697388\n",
      "cnt: 0 - valLoss: 0.6596863865852356 - trainLoss: 0.6595475077629089\n",
      "cnt: 0 - valLoss: 0.6596845984458923 - trainLoss: 0.6595462560653687\n",
      "cnt: 0 - valLoss: 0.6596828103065491 - trainLoss: 0.6595449447631836\n",
      "cnt: 0 - valLoss: 0.6596809029579163 - trainLoss: 0.6595436334609985\n",
      "cnt: 0 - valLoss: 0.6596791744232178 - trainLoss: 0.6595423221588135\n",
      "cnt: 0 - valLoss: 0.6596773862838745 - trainLoss: 0.6595410108566284\n",
      "cnt: 0 - valLoss: 0.6596755385398865 - trainLoss: 0.6595396995544434\n",
      "cnt: 0 - valLoss: 0.6596736907958984 - trainLoss: 0.6595383882522583\n",
      "cnt: 0 - valLoss: 0.6596719026565552 - trainLoss: 0.6595370173454285\n",
      "cnt: 0 - valLoss: 0.6596700549125671 - trainLoss: 0.6595357060432434\n",
      "cnt: 0 - valLoss: 0.6596683263778687 - trainLoss: 0.6595343351364136\n",
      "cnt: 0 - valLoss: 0.6596664190292358 - trainLoss: 0.6595330834388733\n",
      "cnt: 0 - valLoss: 0.6596646308898926 - trainLoss: 0.6595317125320435\n",
      "cnt: 0 - valLoss: 0.6596627235412598 - trainLoss: 0.6595304012298584\n",
      "cnt: 0 - valLoss: 0.6596608757972717 - trainLoss: 0.6595290303230286\n",
      "cnt: 0 - valLoss: 0.6596590876579285 - trainLoss: 0.6595277190208435\n",
      "cnt: 0 - valLoss: 0.6596572399139404 - trainLoss: 0.6595264077186584\n",
      "cnt: 0 - valLoss: 0.6596553921699524 - trainLoss: 0.6595250368118286\n",
      "cnt: 0 - valLoss: 0.6596535444259644 - trainLoss: 0.6595237255096436\n",
      "cnt: 0 - valLoss: 0.6596517562866211 - trainLoss: 0.6595224142074585\n",
      "cnt: 0 - valLoss: 0.6596499085426331 - trainLoss: 0.6595211029052734\n",
      "cnt: 0 - valLoss: 0.659648060798645 - trainLoss: 0.6595197319984436\n",
      "cnt: 0 - valLoss: 0.6596462726593018 - trainLoss: 0.6595184206962585\n",
      "cnt: 0 - valLoss: 0.659644365310669 - trainLoss: 0.6595170497894287\n",
      "cnt: 0 - valLoss: 0.6596425771713257 - trainLoss: 0.6595157384872437\n",
      "cnt: 0 - valLoss: 0.6596407890319824 - trainLoss: 0.6595144271850586\n",
      "cnt: 0 - valLoss: 0.6596388816833496 - trainLoss: 0.6595131158828735\n",
      "cnt: 0 - valLoss: 0.6596370935440063 - trainLoss: 0.6595118045806885\n",
      "cnt: 0 - valLoss: 0.6596352458000183 - trainLoss: 0.6595104932785034\n",
      "cnt: 0 - valLoss: 0.6596333980560303 - trainLoss: 0.6595091223716736\n",
      "cnt: 0 - valLoss: 0.659631609916687 - trainLoss: 0.6595078110694885\n",
      "cnt: 0 - valLoss: 0.6596297025680542 - trainLoss: 0.6595064997673035\n",
      "cnt: 0 - valLoss: 0.6596279144287109 - trainLoss: 0.6595051288604736\n",
      "cnt: 0 - valLoss: 0.6596260666847229 - trainLoss: 0.6595038771629333\n",
      "cnt: 0 - valLoss: 0.6596242785453796 - trainLoss: 0.6595025658607483\n",
      "cnt: 0 - valLoss: 0.6596223711967468 - trainLoss: 0.6595011949539185\n",
      "cnt: 0 - valLoss: 0.6596205830574036 - trainLoss: 0.6594998836517334\n",
      "cnt: 0 - valLoss: 0.6596186757087708 - trainLoss: 0.6594985723495483\n",
      "cnt: 0 - valLoss: 0.6596168875694275 - trainLoss: 0.6594972610473633\n",
      "cnt: 0 - valLoss: 0.6596149802207947 - trainLoss: 0.6594958901405334\n",
      "cnt: 0 - valLoss: 0.6596131324768066 - trainLoss: 0.6594945192337036\n",
      "cnt: 0 - valLoss: 0.6596114039421082 - trainLoss: 0.6594932079315186\n",
      "cnt: 0 - valLoss: 0.6596096158027649 - trainLoss: 0.6594919562339783\n",
      "cnt: 0 - valLoss: 0.6596078872680664 - trainLoss: 0.659490704536438\n",
      "cnt: 0 - valLoss: 0.6596061587333679 - trainLoss: 0.6594894528388977\n",
      "cnt: 0 - valLoss: 0.6596043705940247 - trainLoss: 0.6594881415367126\n",
      "cnt: 0 - valLoss: 0.6596025824546814 - trainLoss: 0.6594868898391724\n",
      "cnt: 0 - valLoss: 0.6596009135246277 - trainLoss: 0.6594856381416321\n",
      "cnt: 0 - valLoss: 0.6595991253852844 - trainLoss: 0.6594844460487366\n",
      "cnt: 0 - valLoss: 0.6595973968505859 - trainLoss: 0.6594831943511963\n",
      "cnt: 0 - valLoss: 0.6595956087112427 - trainLoss: 0.659481942653656\n",
      "cnt: 0 - valLoss: 0.659593939781189 - trainLoss: 0.6594806909561157\n",
      "cnt: 0 - valLoss: 0.6595922112464905 - trainLoss: 0.6594794988632202\n",
      "cnt: 0 - valLoss: 0.6595904231071472 - trainLoss: 0.6594782471656799\n",
      "cnt: 0 - valLoss: 0.6595886945724487 - trainLoss: 0.6594769954681396\n",
      "cnt: 0 - valLoss: 0.6595869064331055 - trainLoss: 0.6594758033752441\n",
      "cnt: 0 - valLoss: 0.659585177898407 - trainLoss: 0.6594744920730591\n",
      "cnt: 0 - valLoss: 0.6595834493637085 - trainLoss: 0.6594732403755188\n",
      "cnt: 0 - valLoss: 0.65958172082901 - trainLoss: 0.6594720482826233\n",
      "cnt: 0 - valLoss: 0.6595799922943115 - trainLoss: 0.6594708561897278\n",
      "cnt: 0 - valLoss: 0.659578263759613 - trainLoss: 0.6594696044921875\n",
      "cnt: 0 - valLoss: 0.6595764756202698 - trainLoss: 0.6594683527946472\n",
      "cnt: 0 - valLoss: 0.6595747470855713 - trainLoss: 0.6594671607017517\n",
      "cnt: 0 - valLoss: 0.659572958946228 - trainLoss: 0.6594658493995667\n",
      "cnt: 0 - valLoss: 0.6595712304115295 - trainLoss: 0.6594645977020264\n",
      "cnt: 0 - valLoss: 0.659569501876831 - trainLoss: 0.6594634056091309\n",
      "cnt: 0 - valLoss: 0.6595677733421326 - trainLoss: 0.6594621539115906\n",
      "cnt: 0 - valLoss: 0.6595659852027893 - trainLoss: 0.6594609022140503\n",
      "cnt: 0 - valLoss: 0.6595642566680908 - trainLoss: 0.6594597101211548\n",
      "cnt: 0 - valLoss: 0.6595624685287476 - trainLoss: 0.6594584584236145\n",
      "cnt: 0 - valLoss: 0.6595607399940491 - trainLoss: 0.6594572067260742\n",
      "cnt: 0 - valLoss: 0.6595590114593506 - trainLoss: 0.6594559550285339\n",
      "cnt: 0 - valLoss: 0.6595572233200073 - trainLoss: 0.6594547033309937\n",
      "cnt: 0 - valLoss: 0.6595554351806641 - trainLoss: 0.6594535112380981\n",
      "cnt: 0 - valLoss: 0.6595537066459656 - trainLoss: 0.6594522595405579\n",
      "cnt: 0 - valLoss: 0.6595519185066223 - trainLoss: 0.6594509482383728\n",
      "cnt: 0 - valLoss: 0.6595502495765686 - trainLoss: 0.6594497561454773\n",
      "cnt: 0 - valLoss: 0.6595484018325806 - trainLoss: 0.6594485640525818\n",
      "cnt: 0 - valLoss: 0.6595466732978821 - trainLoss: 0.6594473123550415\n",
      "cnt: 0 - valLoss: 0.6595448851585388 - trainLoss: 0.6594460010528564\n",
      "cnt: 0 - valLoss: 0.6595431566238403 - trainLoss: 0.6594447493553162\n",
      "cnt: 0 - valLoss: 0.6595413684844971 - trainLoss: 0.6594434976577759\n",
      "cnt: 0 - valLoss: 0.6595396399497986 - trainLoss: 0.6594423055648804\n",
      "cnt: 0 - valLoss: 0.6595379114151001 - trainLoss: 0.6594410538673401\n",
      "cnt: 0 - valLoss: 0.6595361232757568 - trainLoss: 0.6594398617744446\n",
      "cnt: 0 - valLoss: 0.6595343947410583 - trainLoss: 0.6594385504722595\n",
      "cnt: 0 - valLoss: 0.6595326066017151 - trainLoss: 0.6594372987747192\n",
      "cnt: 0 - valLoss: 0.6595308780670166 - trainLoss: 0.659436047077179\n",
      "cnt: 0 - valLoss: 0.6595290899276733 - trainLoss: 0.6594348549842834\n",
      "cnt: 0 - valLoss: 0.6595273017883301 - trainLoss: 0.6594336032867432\n",
      "cnt: 0 - valLoss: 0.6595255136489868 - trainLoss: 0.6594323515892029\n",
      "cnt: 0 - valLoss: 0.6595237255096436 - trainLoss: 0.6594310402870178\n",
      "cnt: 0 - valLoss: 0.6595219969749451 - trainLoss: 0.6594298481941223\n",
      "cnt: 0 - valLoss: 0.6595202088356018 - trainLoss: 0.659428596496582\n",
      "cnt: 0 - valLoss: 0.6595184206962585 - trainLoss: 0.6594273447990417\n",
      "cnt: 0 - valLoss: 0.6595166921615601 - trainLoss: 0.6594260334968567\n",
      "cnt: 0 - valLoss: 0.659514844417572 - trainLoss: 0.6594247817993164\n",
      "cnt: 0 - valLoss: 0.6595131754875183 - trainLoss: 0.6594235301017761\n",
      "cnt: 0 - valLoss: 0.6595114469528198 - trainLoss: 0.6594223380088806\n",
      "cnt: 0 - valLoss: 0.6595096588134766 - trainLoss: 0.6594210267066956\n",
      "cnt: 0 - valLoss: 0.6595078706741333 - trainLoss: 0.6594197750091553\n",
      "cnt: 0 - valLoss: 0.6595061421394348 - trainLoss: 0.6594185829162598\n",
      "cnt: 0 - valLoss: 0.6595044136047363 - trainLoss: 0.6594173908233643\n",
      "cnt: 0 - valLoss: 0.6595026254653931 - trainLoss: 0.659416139125824\n",
      "cnt: 0 - valLoss: 0.6595008969306946 - trainLoss: 0.6594148874282837\n",
      "cnt: 0 - valLoss: 0.6594991087913513 - trainLoss: 0.659413754940033\n",
      "cnt: 0 - valLoss: 0.6594974398612976 - trainLoss: 0.6594124436378479\n",
      "cnt: 0 - valLoss: 0.6594956517219543 - trainLoss: 0.6594112515449524\n",
      "cnt: 0 - valLoss: 0.6594939231872559 - trainLoss: 0.6594100594520569\n",
      "cnt: 0 - valLoss: 0.6594922542572021 - trainLoss: 0.6594088673591614\n",
      "cnt: 0 - valLoss: 0.6594905257225037 - trainLoss: 0.6594076752662659\n",
      "cnt: 0 - valLoss: 0.6594887971878052 - trainLoss: 0.6594064235687256\n",
      "cnt: 0 - valLoss: 0.6594870686531067 - trainLoss: 0.6594052314758301\n",
      "cnt: 0 - valLoss: 0.659485399723053 - trainLoss: 0.6594040393829346\n",
      "cnt: 0 - valLoss: 0.6594835519790649 - trainLoss: 0.6594028472900391\n",
      "cnt: 0 - valLoss: 0.6594818830490112 - trainLoss: 0.6594016551971436\n",
      "cnt: 0 - valLoss: 0.6594801545143127 - trainLoss: 0.659400463104248\n",
      "cnt: 0 - valLoss: 0.6594784259796143 - trainLoss: 0.6593993306159973\n",
      "cnt: 0 - valLoss: 0.6594766974449158 - trainLoss: 0.659398078918457\n",
      "cnt: 0 - valLoss: 0.6594749689102173 - trainLoss: 0.6593968868255615\n",
      "cnt: 0 - valLoss: 0.6594732999801636 - trainLoss: 0.6593957543373108\n",
      "cnt: 0 - valLoss: 0.6594715118408203 - trainLoss: 0.6593944430351257\n",
      "cnt: 0 - valLoss: 0.6594697833061218 - trainLoss: 0.6593932509422302\n",
      "cnt: 0 - valLoss: 0.6594681143760681 - trainLoss: 0.6593921184539795\n",
      "cnt: 0 - valLoss: 0.6594663262367249 - trainLoss: 0.6593908667564392\n",
      "cnt: 0 - valLoss: 0.6594646573066711 - trainLoss: 0.6593896746635437\n",
      "cnt: 0 - valLoss: 0.6594628691673279 - trainLoss: 0.659388542175293\n",
      "cnt: 0 - valLoss: 0.6594611406326294 - trainLoss: 0.6593872904777527\n",
      "cnt: 0 - valLoss: 0.6594593524932861 - trainLoss: 0.6593860983848572\n",
      "cnt: 0 - valLoss: 0.6594576239585876 - trainLoss: 0.6593848466873169\n",
      "cnt: 0 - valLoss: 0.6594558954238892 - trainLoss: 0.6593836545944214\n",
      "cnt: 0 - valLoss: 0.6594541668891907 - trainLoss: 0.6593824625015259\n",
      "cnt: 0 - valLoss: 0.6594524383544922 - trainLoss: 0.6593812704086304\n",
      "cnt: 0 - valLoss: 0.6594506502151489 - trainLoss: 0.6593800187110901\n",
      "cnt: 0 - valLoss: 0.6594489216804504 - trainLoss: 0.6593788266181946\n",
      "cnt: 0 - valLoss: 0.6594472527503967 - trainLoss: 0.6593776345252991\n",
      "cnt: 0 - valLoss: 0.6594454646110535 - trainLoss: 0.6593763828277588\n",
      "cnt: 0 - valLoss: 0.659443736076355 - trainLoss: 0.6593753099441528\n",
      "cnt: 0 - valLoss: 0.6594419479370117 - trainLoss: 0.6593739986419678\n",
      "cnt: 0 - valLoss: 0.659440279006958 - trainLoss: 0.6593728065490723\n",
      "cnt: 0 - valLoss: 0.65943843126297 - trainLoss: 0.6593716144561768\n",
      "cnt: 0 - valLoss: 0.6594367027282715 - trainLoss: 0.6593703627586365\n",
      "cnt: 0 - valLoss: 0.6594350337982178 - trainLoss: 0.659369170665741\n",
      "cnt: 0 - valLoss: 0.6594332456588745 - trainLoss: 0.6593679785728455\n",
      "cnt: 0 - valLoss: 0.6594314575195312 - trainLoss: 0.6593667268753052\n",
      "cnt: 0 - valLoss: 0.6594297289848328 - trainLoss: 0.6593655347824097\n",
      "cnt: 0 - valLoss: 0.6594280004501343 - trainLoss: 0.6593642830848694\n",
      "cnt: 0 - valLoss: 0.659426212310791 - trainLoss: 0.6593630909919739\n",
      "cnt: 0 - valLoss: 0.6594244241714478 - trainLoss: 0.6593618988990784\n",
      "cnt: 0 - valLoss: 0.6594226956367493 - trainLoss: 0.6593606472015381\n",
      "cnt: 0 - valLoss: 0.6594209671020508 - trainLoss: 0.6593594551086426\n",
      "cnt: 0 - valLoss: 0.6594191789627075 - trainLoss: 0.6593582630157471\n",
      "cnt: 0 - valLoss: 0.6594175100326538 - trainLoss: 0.6593570709228516\n",
      "cnt: 0 - valLoss: 0.6594157218933105 - trainLoss: 0.6593557596206665\n",
      "cnt: 0 - valLoss: 0.6594139933586121 - trainLoss: 0.659354567527771\n",
      "cnt: 0 - valLoss: 0.6594122648239136 - trainLoss: 0.6593533754348755\n",
      "cnt: 0 - valLoss: 0.6594104766845703 - trainLoss: 0.6593521237373352\n",
      "cnt: 0 - valLoss: 0.6594086289405823 - trainLoss: 0.6593509912490845\n",
      "cnt: 0 - valLoss: 0.6594069600105286 - trainLoss: 0.6593496799468994\n",
      "cnt: 0 - valLoss: 0.6594051718711853 - trainLoss: 0.6593484878540039\n",
      "cnt: 0 - valLoss: 0.6594034433364868 - trainLoss: 0.6593472361564636\n",
      "cnt: 0 - valLoss: 0.6594016551971436 - trainLoss: 0.6593460440635681\n",
      "cnt: 0 - valLoss: 0.6593998670578003 - trainLoss: 0.6593447923660278\n",
      "cnt: 0 - valLoss: 0.6593981385231018 - trainLoss: 0.6593435406684875\n",
      "cnt: 0 - valLoss: 0.6593963503837585 - trainLoss: 0.659342348575592\n",
      "cnt: 0 - valLoss: 0.6593946218490601 - trainLoss: 0.6593411564826965\n",
      "cnt: 0 - valLoss: 0.659392774105072 - trainLoss: 0.6593398451805115\n",
      "cnt: 0 - valLoss: 0.6593910455703735 - trainLoss: 0.6593385934829712\n",
      "cnt: 0 - valLoss: 0.6593892574310303 - trainLoss: 0.6593373417854309\n",
      "cnt: 0 - valLoss: 0.659387469291687 - trainLoss: 0.6593361496925354\n",
      "cnt: 0 - valLoss: 0.6593857407569885 - trainLoss: 0.6593348979949951\n",
      "cnt: 0 - valLoss: 0.6593839526176453 - trainLoss: 0.6593337059020996\n",
      "cnt: 0 - valLoss: 0.659382164478302 - trainLoss: 0.6593323945999146\n",
      "cnt: 0 - valLoss: 0.6593803763389587 - trainLoss: 0.659331202507019\n",
      "cnt: 0 - valLoss: 0.6593785881996155 - trainLoss: 0.6593299508094788\n",
      "cnt: 0 - valLoss: 0.659376859664917 - trainLoss: 0.6593286991119385\n",
      "cnt: 0 - valLoss: 0.6593751311302185 - trainLoss: 0.659327507019043\n",
      "cnt: 0 - valLoss: 0.6593733429908752 - trainLoss: 0.6593262553215027\n",
      "cnt: 0 - valLoss: 0.6593716740608215 - trainLoss: 0.659325122833252\n",
      "cnt: 0 - valLoss: 0.6593698859214783 - trainLoss: 0.6593238711357117\n",
      "cnt: 0 - valLoss: 0.659368097782135 - trainLoss: 0.6593227386474609\n",
      "cnt: 0 - valLoss: 0.6593664288520813 - trainLoss: 0.6593214869499207\n",
      "cnt: 0 - valLoss: 0.6593645811080933 - trainLoss: 0.6593202352523804\n",
      "cnt: 0 - valLoss: 0.6593629121780396 - trainLoss: 0.6593191027641296\n",
      "cnt: 0 - valLoss: 0.6593611836433411 - trainLoss: 0.6593178510665894\n",
      "cnt: 0 - valLoss: 0.6593594551086426 - trainLoss: 0.6593166589736938\n",
      "cnt: 0 - valLoss: 0.6593576669692993 - trainLoss: 0.6593155264854431\n",
      "cnt: 0 - valLoss: 0.659355878829956 - trainLoss: 0.6593142747879028\n",
      "cnt: 0 - valLoss: 0.6593541502952576 - trainLoss: 0.6593130826950073\n",
      "cnt: 0 - valLoss: 0.6593524217605591 - trainLoss: 0.659311830997467\n",
      "cnt: 0 - valLoss: 0.6593506336212158 - trainLoss: 0.6593106389045715\n",
      "cnt: 0 - valLoss: 0.6593489050865173 - trainLoss: 0.659309446811676\n",
      "cnt: 0 - valLoss: 0.6593471169471741 - trainLoss: 0.6593082547187805\n",
      "cnt: 0 - valLoss: 0.6593453884124756 - trainLoss: 0.6593070030212402\n",
      "cnt: 0 - valLoss: 0.6593436002731323 - trainLoss: 0.6593058109283447\n",
      "cnt: 0 - valLoss: 0.6593418717384338 - trainLoss: 0.6593046188354492\n",
      "cnt: 0 - valLoss: 0.6593400835990906 - trainLoss: 0.6593033671379089\n",
      "cnt: 0 - valLoss: 0.6593383550643921 - trainLoss: 0.6593021154403687\n",
      "cnt: 0 - valLoss: 0.6593366265296936 - trainLoss: 0.6593009829521179\n",
      "cnt: 0 - valLoss: 0.6593348383903503 - trainLoss: 0.6592997312545776\n",
      "cnt: 0 - valLoss: 0.6593330502510071 - trainLoss: 0.6592984795570374\n",
      "cnt: 0 - valLoss: 0.6593313217163086 - trainLoss: 0.6592972874641418\n",
      "cnt: 0 - valLoss: 0.6593295335769653 - trainLoss: 0.6592960357666016\n",
      "cnt: 0 - valLoss: 0.6593277454376221 - trainLoss: 0.659294843673706\n",
      "cnt: 0 - valLoss: 0.6593260169029236 - trainLoss: 0.6592936515808105\n",
      "cnt: 0 - valLoss: 0.6593242883682251 - trainLoss: 0.6592923998832703\n",
      "cnt: 0 - valLoss: 0.6593224406242371 - trainLoss: 0.6592912077903748\n",
      "cnt: 0 - valLoss: 0.6593207120895386 - trainLoss: 0.6592900156974792\n",
      "cnt: 0 - valLoss: 0.6593189835548401 - trainLoss: 0.6592888236045837\n",
      "cnt: 0 - valLoss: 0.6593171954154968 - trainLoss: 0.6592875719070435\n",
      "cnt: 0 - valLoss: 0.6593154072761536 - trainLoss: 0.6592863202095032\n",
      "cnt: 0 - valLoss: 0.6593136787414551 - trainLoss: 0.6592851281166077\n",
      "cnt: 0 - valLoss: 0.6593120694160461 - trainLoss: 0.6592839360237122\n",
      "cnt: 0 - valLoss: 0.6593103408813477 - trainLoss: 0.6592827439308167\n",
      "cnt: 0 - valLoss: 0.659308671951294 - trainLoss: 0.6592815518379211\n",
      "cnt: 0 - valLoss: 0.6593070030212402 - trainLoss: 0.6592804193496704\n",
      "cnt: 0 - valLoss: 0.6593053340911865 - trainLoss: 0.6592791676521301\n",
      "cnt: 0 - valLoss: 0.6593036651611328 - trainLoss: 0.6592779755592346\n",
      "cnt: 0 - valLoss: 0.6593019962310791 - trainLoss: 0.6592767834663391\n",
      "cnt: 0 - valLoss: 0.6593003273010254 - trainLoss: 0.6592755913734436\n",
      "cnt: 0 - valLoss: 0.6592986583709717 - trainLoss: 0.6592744588851929\n",
      "cnt: 0 - valLoss: 0.6592969298362732 - trainLoss: 0.6592732667922974\n",
      "cnt: 0 - valLoss: 0.6592953205108643 - trainLoss: 0.6592720746994019\n",
      "cnt: 0 - valLoss: 0.6592935919761658 - trainLoss: 0.6592708826065063\n",
      "cnt: 0 - valLoss: 0.6592919826507568 - trainLoss: 0.6592696309089661\n",
      "cnt: 0 - valLoss: 0.6592902541160583 - trainLoss: 0.6592684388160706\n",
      "cnt: 0 - valLoss: 0.6592885851860046 - trainLoss: 0.6592673063278198\n",
      "cnt: 0 - valLoss: 0.6592869162559509 - trainLoss: 0.6592660546302795\n",
      "cnt: 0 - valLoss: 0.6592851877212524 - trainLoss: 0.6592649221420288\n",
      "cnt: 0 - valLoss: 0.6592835783958435 - trainLoss: 0.6592637300491333\n",
      "cnt: 0 - valLoss: 0.659281849861145 - trainLoss: 0.6592625379562378\n",
      "cnt: 0 - valLoss: 0.6592801809310913 - trainLoss: 0.6592612862586975\n",
      "cnt: 0 - valLoss: 0.6592785120010376 - trainLoss: 0.6592601537704468\n",
      "cnt: 0 - valLoss: 0.6592767834663391 - trainLoss: 0.6592589616775513\n",
      "cnt: 0 - valLoss: 0.6592751741409302 - trainLoss: 0.6592577695846558\n",
      "cnt: 0 - valLoss: 0.6592735648155212 - trainLoss: 0.6592565774917603\n",
      "cnt: 0 - valLoss: 0.6592718362808228 - trainLoss: 0.6592554450035095\n",
      "cnt: 0 - valLoss: 0.6592702269554138 - trainLoss: 0.659254252910614\n",
      "cnt: 0 - valLoss: 0.6592684984207153 - trainLoss: 0.6592530608177185\n",
      "cnt: 0 - valLoss: 0.6592668890953064 - trainLoss: 0.659251868724823\n",
      "cnt: 0 - valLoss: 0.6592651605606079 - trainLoss: 0.6592506766319275\n",
      "cnt: 0 - valLoss: 0.659263551235199 - trainLoss: 0.6592494249343872\n",
      "cnt: 0 - valLoss: 0.6592618227005005 - trainLoss: 0.6592482328414917\n",
      "cnt: 0 - valLoss: 0.659260094165802 - trainLoss: 0.659247100353241\n",
      "cnt: 0 - valLoss: 0.6592584848403931 - trainLoss: 0.6592459678649902\n",
      "cnt: 0 - valLoss: 0.6592567563056946 - trainLoss: 0.65924471616745\n",
      "cnt: 0 - valLoss: 0.6592550873756409 - trainLoss: 0.6592435240745544\n",
      "cnt: 0 - valLoss: 0.6592534780502319 - trainLoss: 0.6592423319816589\n",
      "cnt: 0 - valLoss: 0.6592517495155334 - trainLoss: 0.6592411398887634\n",
      "cnt: 0 - valLoss: 0.6592500805854797 - trainLoss: 0.6592399477958679\n",
      "cnt: 0 - valLoss: 0.6592483520507812 - trainLoss: 0.6592387557029724\n",
      "cnt: 0 - valLoss: 0.6592466831207275 - trainLoss: 0.6592375636100769\n",
      "cnt: 0 - valLoss: 0.6592450141906738 - trainLoss: 0.6592363715171814\n",
      "cnt: 0 - valLoss: 0.6592433452606201 - trainLoss: 0.6592351794242859\n",
      "cnt: 0 - valLoss: 0.6592416763305664 - trainLoss: 0.6592339873313904\n",
      "cnt: 0 - valLoss: 0.6592400074005127 - trainLoss: 0.6592327356338501\n",
      "cnt: 0 - valLoss: 0.6592382788658142 - trainLoss: 0.6592316627502441\n",
      "cnt: 0 - valLoss: 0.6592365503311157 - trainLoss: 0.6592303514480591\n",
      "cnt: 0 - valLoss: 0.659234881401062 - trainLoss: 0.6592292189598083\n",
      "cnt: 0 - valLoss: 0.6592332720756531 - trainLoss: 0.6592280268669128\n",
      "cnt: 0 - valLoss: 0.6592315435409546 - trainLoss: 0.6592268347740173\n",
      "cnt: 0 - valLoss: 0.6592298150062561 - trainLoss: 0.6592256426811218\n",
      "cnt: 0 - valLoss: 0.6592281460762024 - trainLoss: 0.6592243909835815\n",
      "cnt: 0 - valLoss: 0.6592264771461487 - trainLoss: 0.6592232584953308\n",
      "cnt: 0 - valLoss: 0.6592247486114502 - trainLoss: 0.6592219471931458\n",
      "cnt: 0 - valLoss: 0.6592230796813965 - trainLoss: 0.659220814704895\n",
      "cnt: 0 - valLoss: 0.659221351146698 - trainLoss: 0.6592196226119995\n",
      "cnt: 0 - valLoss: 0.6592197418212891 - trainLoss: 0.659218430519104\n",
      "cnt: 0 - valLoss: 0.6592180132865906 - trainLoss: 0.6592172384262085\n",
      "cnt: 0 - valLoss: 0.6592163443565369 - trainLoss: 0.659216046333313\n",
      "cnt: 0 - valLoss: 0.6592146158218384 - trainLoss: 0.6592147946357727\n",
      "cnt: 0 - valLoss: 0.6592128872871399 - trainLoss: 0.6592136025428772\n",
      "cnt: 0 - valLoss: 0.6592112183570862 - trainLoss: 0.6592124104499817\n",
      "cnt: 0 - valLoss: 0.6592094898223877 - trainLoss: 0.6592112183570862\n",
      "cnt: 0 - valLoss: 0.6592078804969788 - trainLoss: 0.6592099666595459\n",
      "cnt: 0 - valLoss: 0.6592061519622803 - trainLoss: 0.6592087745666504\n",
      "cnt: 0 - valLoss: 0.6592044830322266 - trainLoss: 0.6592075824737549\n",
      "cnt: 0 - valLoss: 0.6592028141021729 - trainLoss: 0.6592063903808594\n",
      "cnt: 0 - valLoss: 0.6592010259628296 - trainLoss: 0.6592051386833191\n",
      "cnt: 0 - valLoss: 0.6591992974281311 - trainLoss: 0.6592040061950684\n",
      "cnt: 0 - valLoss: 0.6591976284980774 - trainLoss: 0.6592028141021729\n",
      "cnt: 0 - valLoss: 0.6591959595680237 - trainLoss: 0.6592016220092773\n",
      "cnt: 0 - valLoss: 0.6591942310333252 - trainLoss: 0.6592004299163818\n",
      "cnt: 0 - valLoss: 0.6591925621032715 - trainLoss: 0.6591991782188416\n",
      "cnt: 0 - valLoss: 0.659190833568573 - trainLoss: 0.659197986125946\n",
      "cnt: 0 - valLoss: 0.6591891050338745 - trainLoss: 0.6591967344284058\n",
      "cnt: 0 - valLoss: 0.6591874361038208 - trainLoss: 0.659195601940155\n",
      "cnt: 0 - valLoss: 0.6591857075691223 - trainLoss: 0.6591943502426147\n",
      "cnt: 0 - valLoss: 0.6591840386390686 - trainLoss: 0.6591931581497192\n",
      "cnt: 0 - valLoss: 0.6591823101043701 - trainLoss: 0.659191906452179\n",
      "cnt: 0 - valLoss: 0.6591806411743164 - trainLoss: 0.6591907739639282\n",
      "cnt: 0 - valLoss: 0.6591789126396179 - trainLoss: 0.6591894626617432\n",
      "cnt: 0 - valLoss: 0.6591772437095642 - trainLoss: 0.6591883301734924\n",
      "cnt: 0 - valLoss: 0.6591755151748657 - trainLoss: 0.6591870784759521\n",
      "cnt: 0 - valLoss: 0.6591737866401672 - trainLoss: 0.6591859459877014\n",
      "cnt: 0 - valLoss: 0.6591721177101135 - trainLoss: 0.6591847538948059\n",
      "cnt: 0 - valLoss: 0.6591704487800598 - trainLoss: 0.6591835618019104\n",
      "cnt: 0 - valLoss: 0.6591687798500061 - trainLoss: 0.6591823697090149\n",
      "cnt: 0 - valLoss: 0.6591670513153076 - trainLoss: 0.6591810584068298\n",
      "cnt: 0 - valLoss: 0.6591653823852539 - trainLoss: 0.6591799259185791\n",
      "cnt: 0 - valLoss: 0.6591636538505554 - trainLoss: 0.6591786742210388\n",
      "cnt: 0 - valLoss: 0.6591619253158569 - trainLoss: 0.6591774821281433\n",
      "cnt: 0 - valLoss: 0.6591602563858032 - trainLoss: 0.6591762900352478\n",
      "cnt: 0 - valLoss: 0.6591585278511047 - trainLoss: 0.6591750383377075\n",
      "cnt: 0 - valLoss: 0.659156858921051 - trainLoss: 0.659173846244812\n",
      "cnt: 0 - valLoss: 0.6591551899909973 - trainLoss: 0.6591726541519165\n",
      "cnt: 0 - valLoss: 0.6591535210609436 - trainLoss: 0.659171462059021\n",
      "cnt: 0 - valLoss: 0.6591517925262451 - trainLoss: 0.6591702699661255\n",
      "cnt: 0 - valLoss: 0.6591500639915466 - trainLoss: 0.65916907787323\n",
      "cnt: 0 - valLoss: 0.6591483950614929 - trainLoss: 0.6591678857803345\n",
      "cnt: 0 - valLoss: 0.6591466665267944 - trainLoss: 0.659166693687439\n",
      "cnt: 0 - valLoss: 0.6591449975967407 - trainLoss: 0.6591655015945435\n",
      "cnt: 0 - valLoss: 0.6591432690620422 - trainLoss: 0.6591642498970032\n",
      "cnt: 0 - valLoss: 0.6591416001319885 - trainLoss: 0.6591630578041077\n",
      "cnt: 0 - valLoss: 0.6591399312019348 - trainLoss: 0.6591618657112122\n",
      "cnt: 0 - valLoss: 0.6591382026672363 - trainLoss: 0.6591606140136719\n",
      "cnt: 0 - valLoss: 0.6591365337371826 - trainLoss: 0.6591594815254211\n",
      "cnt: 0 - valLoss: 0.6591347455978394 - trainLoss: 0.6591582298278809\n",
      "cnt: 0 - valLoss: 0.6591331362724304 - trainLoss: 0.6591570973396301\n",
      "cnt: 0 - valLoss: 0.6591314077377319 - trainLoss: 0.6591559052467346\n",
      "cnt: 0 - valLoss: 0.6591297388076782 - trainLoss: 0.6591545939445496\n",
      "cnt: 0 - valLoss: 0.659127950668335 - trainLoss: 0.659153401851654\n",
      "cnt: 0 - valLoss: 0.659126341342926 - trainLoss: 0.6591522097587585\n",
      "cnt: 0 - valLoss: 0.6591245532035828 - trainLoss: 0.659151017665863\n",
      "cnt: 0 - valLoss: 0.659122884273529 - trainLoss: 0.6591498255729675\n",
      "cnt: 0 - valLoss: 0.6591212153434753 - trainLoss: 0.659148633480072\n",
      "cnt: 0 - valLoss: 0.6591194868087769 - trainLoss: 0.6591474413871765\n",
      "cnt: 0 - valLoss: 0.6591178178787231 - trainLoss: 0.6591461896896362\n",
      "cnt: 0 - valLoss: 0.6591160893440247 - trainLoss: 0.6591449975967407\n",
      "cnt: 0 - valLoss: 0.6591143608093262 - trainLoss: 0.6591438055038452\n",
      "cnt: 0 - valLoss: 0.6591126918792725 - trainLoss: 0.6591425538063049\n",
      "cnt: 0 - valLoss: 0.659110963344574 - trainLoss: 0.6591413617134094\n",
      "cnt: 0 - valLoss: 0.6591092944145203 - trainLoss: 0.6591401100158691\n",
      "cnt: 0 - valLoss: 0.6591075658798218 - trainLoss: 0.6591390371322632\n",
      "cnt: 0 - valLoss: 0.6591058373451233 - trainLoss: 0.6591377258300781\n",
      "cnt: 0 - valLoss: 0.6591041088104248 - trainLoss: 0.6591365337371826\n",
      "cnt: 0 - valLoss: 0.6591023802757263 - trainLoss: 0.6591353416442871\n",
      "cnt: 0 - valLoss: 0.6591007113456726 - trainLoss: 0.6591340899467468\n",
      "cnt: 0 - valLoss: 0.6590990424156189 - trainLoss: 0.6591328978538513\n",
      "cnt: 0 - valLoss: 0.6590973138809204 - trainLoss: 0.659131646156311\n",
      "cnt: 0 - valLoss: 0.6590955853462219 - trainLoss: 0.6591304540634155\n",
      "cnt: 0 - valLoss: 0.6590938568115234 - trainLoss: 0.65912926197052\n",
      "cnt: 0 - valLoss: 0.6590921878814697 - trainLoss: 0.6591280102729797\n",
      "cnt: 0 - valLoss: 0.6590904593467712 - trainLoss: 0.659126877784729\n",
      "cnt: 0 - valLoss: 0.6590887308120728 - trainLoss: 0.6591256856918335\n",
      "cnt: 0 - valLoss: 0.6590870022773743 - trainLoss: 0.659124493598938\n",
      "cnt: 0 - valLoss: 0.6590853333473206 - trainLoss: 0.6591231822967529\n",
      "cnt: 0 - valLoss: 0.6590836048126221 - trainLoss: 0.6591219902038574\n",
      "cnt: 0 - valLoss: 0.6590818762779236 - trainLoss: 0.6591207981109619\n",
      "cnt: 0 - valLoss: 0.6590801477432251 - trainLoss: 0.6591196060180664\n",
      "cnt: 0 - valLoss: 0.6590785384178162 - trainLoss: 0.6591183543205261\n",
      "cnt: 0 - valLoss: 0.6590767502784729 - trainLoss: 0.6591171026229858\n",
      "cnt: 0 - valLoss: 0.6590750813484192 - trainLoss: 0.6591159105300903\n",
      "cnt: 0 - valLoss: 0.6590734124183655 - trainLoss: 0.6591147780418396\n",
      "cnt: 0 - valLoss: 0.6590717434883118 - trainLoss: 0.6591135859489441\n",
      "cnt: 0 - valLoss: 0.6590700745582581 - trainLoss: 0.6591124534606934\n",
      "cnt: 0 - valLoss: 0.6590684056282043 - trainLoss: 0.6591112613677979\n",
      "cnt: 0 - valLoss: 0.6590667366981506 - trainLoss: 0.6591101884841919\n",
      "cnt: 0 - valLoss: 0.6590650677680969 - trainLoss: 0.6591089963912964\n",
      "cnt: 0 - valLoss: 0.6590633988380432 - trainLoss: 0.6591078042984009\n",
      "cnt: 0 - valLoss: 0.6590617895126343 - trainLoss: 0.6591067314147949\n",
      "cnt: 0 - valLoss: 0.6590601205825806 - trainLoss: 0.6591055393218994\n",
      "cnt: 0 - valLoss: 0.6590584516525269 - trainLoss: 0.6591044068336487\n",
      "cnt: 0 - valLoss: 0.6590568423271179 - trainLoss: 0.659103274345398\n",
      "cnt: 0 - valLoss: 0.6590551137924194 - trainLoss: 0.6591021418571472\n",
      "cnt: 0 - valLoss: 0.6590534448623657 - trainLoss: 0.6591009497642517\n",
      "cnt: 0 - valLoss: 0.6590518355369568 - trainLoss: 0.659099817276001\n",
      "cnt: 0 - valLoss: 0.6590501666069031 - trainLoss: 0.6590986847877502\n",
      "cnt: 0 - valLoss: 0.6590484976768494 - trainLoss: 0.6590974926948547\n",
      "cnt: 0 - valLoss: 0.6590468287467957 - trainLoss: 0.659096360206604\n",
      "cnt: 0 - valLoss: 0.6590451598167419 - trainLoss: 0.659095287322998\n",
      "cnt: 0 - valLoss: 0.6590434908866882 - trainLoss: 0.6590940356254578\n",
      "cnt: 0 - valLoss: 0.6590418219566345 - trainLoss: 0.659092903137207\n",
      "cnt: 0 - valLoss: 0.6590402126312256 - trainLoss: 0.6590917706489563\n",
      "cnt: 0 - valLoss: 0.6590384840965271 - trainLoss: 0.6590906381607056\n",
      "cnt: 0 - valLoss: 0.6590368151664734 - trainLoss: 0.6590894460678101\n",
      "cnt: 0 - valLoss: 0.6590352058410645 - trainLoss: 0.6590883135795593\n",
      "cnt: 0 - valLoss: 0.6590335369110107 - trainLoss: 0.6590871214866638\n",
      "cnt: 0 - valLoss: 0.659031867980957 - trainLoss: 0.6590860486030579\n",
      "cnt: 0 - valLoss: 0.6590301990509033 - trainLoss: 0.6590848565101624\n",
      "cnt: 0 - valLoss: 0.6590285301208496 - trainLoss: 0.6590837240219116\n",
      "cnt: 0 - valLoss: 0.6590268611907959 - trainLoss: 0.6590825319290161\n",
      "cnt: 0 - valLoss: 0.6590251922607422 - trainLoss: 0.6590813994407654\n",
      "cnt: 0 - valLoss: 0.6590235233306885 - trainLoss: 0.6590802073478699\n",
      "cnt: 0 - valLoss: 0.65902179479599 - trainLoss: 0.6590790748596191\n",
      "cnt: 0 - valLoss: 0.659020185470581 - trainLoss: 0.6590778827667236\n",
      "cnt: 0 - valLoss: 0.6590184569358826 - trainLoss: 0.6590768098831177\n",
      "cnt: 0 - valLoss: 0.6590167880058289 - trainLoss: 0.6590756177902222\n",
      "cnt: 0 - valLoss: 0.6590151190757751 - trainLoss: 0.6590744256973267\n",
      "cnt: 0 - valLoss: 0.6590134501457214 - trainLoss: 0.6590733528137207\n",
      "cnt: 0 - valLoss: 0.6590117812156677 - trainLoss: 0.6590721011161804\n",
      "cnt: 0 - valLoss: 0.659010112285614 - trainLoss: 0.6590710282325745\n",
      "cnt: 0 - valLoss: 0.6590083837509155 - trainLoss: 0.659069836139679\n",
      "cnt: 0 - valLoss: 0.6590067148208618 - trainLoss: 0.6590686440467834\n",
      "cnt: 0 - valLoss: 0.6590050458908081 - trainLoss: 0.6590675711631775\n",
      "cnt: 0 - valLoss: 0.6590033769607544 - trainLoss: 0.659066379070282\n",
      "cnt: 0 - valLoss: 0.6590017080307007 - trainLoss: 0.659065306186676\n",
      "cnt: 0 - valLoss: 0.6590000987052917 - trainLoss: 0.6590640544891357\n",
      "cnt: 0 - valLoss: 0.6589984893798828 - trainLoss: 0.6590629816055298\n",
      "cnt: 0 - valLoss: 0.6589968204498291 - trainLoss: 0.6590617895126343\n",
      "cnt: 0 - valLoss: 0.6589951515197754 - trainLoss: 0.6590607166290283\n",
      "cnt: 0 - valLoss: 0.6589935421943665 - trainLoss: 0.6590595245361328\n",
      "cnt: 0 - valLoss: 0.6589918732643127 - trainLoss: 0.6590583324432373\n",
      "cnt: 0 - valLoss: 0.6589902639389038 - trainLoss: 0.6590572595596313\n",
      "cnt: 0 - valLoss: 0.6589885950088501 - trainLoss: 0.6590560674667358\n",
      "cnt: 0 - valLoss: 0.6589869856834412 - trainLoss: 0.6590548753738403\n",
      "cnt: 0 - valLoss: 0.6589852571487427 - trainLoss: 0.6590537428855896\n",
      "cnt: 0 - valLoss: 0.6589837074279785 - trainLoss: 0.6590526700019836\n",
      "cnt: 0 - valLoss: 0.6589820384979248 - trainLoss: 0.6590515375137329\n",
      "cnt: 0 - valLoss: 0.6589804291725159 - trainLoss: 0.6590503454208374\n",
      "cnt: 0 - valLoss: 0.6589788198471069 - trainLoss: 0.6590492129325867\n",
      "cnt: 0 - valLoss: 0.6589771509170532 - trainLoss: 0.6590480804443359\n",
      "cnt: 0 - valLoss: 0.6589754819869995 - trainLoss: 0.6590468883514404\n",
      "cnt: 0 - valLoss: 0.6589738130569458 - trainLoss: 0.6590457558631897\n",
      "cnt: 0 - valLoss: 0.6589722633361816 - trainLoss: 0.659044623374939\n",
      "cnt: 0 - valLoss: 0.6589705944061279 - trainLoss: 0.6590434908866882\n",
      "cnt: 0 - valLoss: 0.658968985080719 - trainLoss: 0.6590422987937927\n",
      "cnt: 0 - valLoss: 0.6589673161506653 - trainLoss: 0.659041166305542\n",
      "cnt: 0 - valLoss: 0.6589657068252563 - trainLoss: 0.6590400338172913\n",
      "cnt: 0 - valLoss: 0.6589639782905579 - trainLoss: 0.6590388417243958\n",
      "cnt: 0 - valLoss: 0.6589623689651489 - trainLoss: 0.6590377688407898\n",
      "cnt: 0 - valLoss: 0.65896075963974 - trainLoss: 0.6590365767478943\n",
      "cnt: 0 - valLoss: 0.6589592099189758 - trainLoss: 0.6590355038642883\n",
      "cnt: 0 - valLoss: 0.6589576601982117 - trainLoss: 0.6590344309806824\n",
      "cnt: 0 - valLoss: 0.6589561104774475 - trainLoss: 0.6590333580970764\n",
      "cnt: 0 - valLoss: 0.6589545607566833 - trainLoss: 0.6590322852134705\n",
      "cnt: 0 - valLoss: 0.6589529514312744 - trainLoss: 0.6590312123298645\n",
      "cnt: 0 - valLoss: 0.6589513421058655 - trainLoss: 0.6590300798416138\n",
      "cnt: 0 - valLoss: 0.6589497923851013 - trainLoss: 0.6590290069580078\n",
      "cnt: 0 - valLoss: 0.6589481830596924 - trainLoss: 0.6590279340744019\n",
      "cnt: 0 - valLoss: 0.6589466333389282 - trainLoss: 0.6590268611907959\n",
      "cnt: 0 - valLoss: 0.6589450836181641 - trainLoss: 0.6590257883071899\n",
      "cnt: 0 - valLoss: 0.6589434146881104 - trainLoss: 0.659024715423584\n",
      "cnt: 0 - valLoss: 0.6589418649673462 - trainLoss: 0.659023642539978\n",
      "cnt: 0 - valLoss: 0.658940315246582 - trainLoss: 0.6590224504470825\n",
      "cnt: 0 - valLoss: 0.6589387059211731 - trainLoss: 0.6590214371681213\n",
      "cnt: 0 - valLoss: 0.6589370965957642 - trainLoss: 0.6590203642845154\n",
      "cnt: 0 - valLoss: 0.6589354872703552 - trainLoss: 0.6590192914009094\n",
      "cnt: 0 - valLoss: 0.6589339375495911 - trainLoss: 0.6590181589126587\n",
      "cnt: 0 - valLoss: 0.6589323282241821 - trainLoss: 0.659017026424408\n",
      "cnt: 0 - valLoss: 0.6589307188987732 - trainLoss: 0.659015953540802\n",
      "cnt: 0 - valLoss: 0.6589291095733643 - trainLoss: 0.659014880657196\n",
      "cnt: 0 - valLoss: 0.6589275002479553 - trainLoss: 0.6590137481689453\n",
      "cnt: 0 - valLoss: 0.6589258909225464 - trainLoss: 0.6590126752853394\n",
      "cnt: 0 - valLoss: 0.6589242815971375 - trainLoss: 0.6590115427970886\n",
      "cnt: 0 - valLoss: 0.6589226722717285 - trainLoss: 0.6590104103088379\n",
      "cnt: 0 - valLoss: 0.6589210629463196 - trainLoss: 0.6590093374252319\n",
      "cnt: 0 - valLoss: 0.6589194536209106 - trainLoss: 0.6590082049369812\n",
      "cnt: 0 - valLoss: 0.6589177846908569 - trainLoss: 0.6590071320533752\n",
      "cnt: 0 - valLoss: 0.658916175365448 - trainLoss: 0.6590060591697693\n",
      "cnt: 0 - valLoss: 0.6589146256446838 - trainLoss: 0.6590048670768738\n",
      "cnt: 0 - valLoss: 0.6589128971099854 - trainLoss: 0.6590037941932678\n",
      "cnt: 0 - valLoss: 0.6589113473892212 - trainLoss: 0.6590027213096619\n",
      "cnt: 0 - valLoss: 0.6589097380638123 - trainLoss: 0.6590016484260559\n",
      "cnt: 0 - valLoss: 0.6589081287384033 - trainLoss: 0.6590005159378052\n",
      "cnt: 0 - valLoss: 0.6589064598083496 - trainLoss: 0.6589993834495544\n",
      "cnt: 0 - valLoss: 0.6589049100875854 - trainLoss: 0.6589982509613037\n",
      "cnt: 0 - valLoss: 0.6589032411575317 - trainLoss: 0.6589971780776978\n",
      "cnt: 0 - valLoss: 0.6589016318321228 - trainLoss: 0.6589959859848022\n",
      "cnt: 0 - valLoss: 0.6589000821113586 - trainLoss: 0.6589949727058411\n",
      "cnt: 0 - valLoss: 0.6588984131813049 - trainLoss: 0.6589938402175903\n",
      "cnt: 0 - valLoss: 0.658896803855896 - trainLoss: 0.6589927077293396\n",
      "cnt: 0 - valLoss: 0.6588951945304871 - trainLoss: 0.6589916348457336\n",
      "cnt: 0 - valLoss: 0.6588935256004333 - trainLoss: 0.6589905023574829\n",
      "cnt: 0 - valLoss: 0.6588918566703796 - trainLoss: 0.6589893698692322\n",
      "cnt: 0 - valLoss: 0.6588902473449707 - trainLoss: 0.6589882969856262\n",
      "cnt: 0 - valLoss: 0.6588886380195618 - trainLoss: 0.6589871644973755\n",
      "cnt: 0 - valLoss: 0.6588869690895081 - trainLoss: 0.6589860320091248\n",
      "cnt: 0 - valLoss: 0.6588854193687439 - trainLoss: 0.6589849591255188\n",
      "cnt: 0 - valLoss: 0.6588837504386902 - trainLoss: 0.6589838266372681\n",
      "cnt: 0 - valLoss: 0.6588821411132812 - trainLoss: 0.6589826941490173\n",
      "cnt: 0 - valLoss: 0.6588805317878723 - trainLoss: 0.6589816212654114\n",
      "cnt: 0 - valLoss: 0.6588788628578186 - trainLoss: 0.6589804887771606\n",
      "cnt: 0 - valLoss: 0.6588772535324097 - trainLoss: 0.6589793562889099\n",
      "cnt: 0 - valLoss: 0.6588756442070007 - trainLoss: 0.658978283405304\n",
      "cnt: 0 - valLoss: 0.658873975276947 - trainLoss: 0.6589771509170532\n",
      "cnt: 0 - valLoss: 0.6588723659515381 - trainLoss: 0.6589760780334473\n",
      "cnt: 0 - valLoss: 0.6588707566261292 - trainLoss: 0.6589749455451965\n",
      "cnt: 0 - valLoss: 0.6588690876960754 - trainLoss: 0.6589738726615906\n",
      "cnt: 0 - valLoss: 0.6588674783706665 - trainLoss: 0.6589726805686951\n",
      "cnt: 0 - valLoss: 0.6588658690452576 - trainLoss: 0.6589716076850891\n",
      "cnt: 0 - valLoss: 0.6588642597198486 - trainLoss: 0.6589704751968384\n",
      "cnt: 0 - valLoss: 0.6588625907897949 - trainLoss: 0.6589693427085876\n",
      "cnt: 0 - valLoss: 0.658860981464386 - trainLoss: 0.6589682698249817\n",
      "cnt: 0 - valLoss: 0.658859372138977 - trainLoss: 0.658967137336731\n",
      "cnt: 0 - valLoss: 0.6588577032089233 - trainLoss: 0.6589660048484802\n",
      "cnt: 0 - valLoss: 0.6588560938835144 - trainLoss: 0.6589649319648743\n",
      "cnt: 0 - valLoss: 0.6588544249534607 - trainLoss: 0.6589637994766235\n",
      "cnt: 0 - valLoss: 0.6588528156280518 - trainLoss: 0.6589626669883728\n",
      "cnt: 0 - valLoss: 0.6588512063026428 - trainLoss: 0.6589615941047668\n",
      "cnt: 0 - valLoss: 0.6588495373725891 - trainLoss: 0.6589604020118713\n",
      "cnt: 0 - valLoss: 0.6588479280471802 - trainLoss: 0.6589593291282654\n",
      "cnt: 0 - valLoss: 0.6588462591171265 - trainLoss: 0.6589581966400146\n",
      "cnt: 0 - valLoss: 0.6588446497917175 - trainLoss: 0.6589570641517639\n",
      "cnt: 0 - valLoss: 0.6588429808616638 - trainLoss: 0.6589559316635132\n",
      "cnt: 0 - valLoss: 0.6588413119316101 - trainLoss: 0.6589547991752625\n",
      "cnt: 0 - valLoss: 0.6588397026062012 - trainLoss: 0.6589536666870117\n",
      "cnt: 0 - valLoss: 0.6588380932807922 - trainLoss: 0.6589525938034058\n",
      "cnt: 0 - valLoss: 0.6588364243507385 - trainLoss: 0.658951461315155\n",
      "cnt: 0 - valLoss: 0.6588348150253296 - trainLoss: 0.6589503288269043\n",
      "cnt: 0 - valLoss: 0.6588331460952759 - trainLoss: 0.6589491963386536\n",
      "cnt: 0 - valLoss: 0.6588314771652222 - trainLoss: 0.6589481234550476\n",
      "cnt: 0 - valLoss: 0.6588298678398132 - trainLoss: 0.6589469909667969\n",
      "cnt: 0 - valLoss: 0.6588281989097595 - trainLoss: 0.6589457988739014\n",
      "cnt: 0 - valLoss: 0.6588265895843506 - trainLoss: 0.6589447259902954\n",
      "cnt: 0 - valLoss: 0.6588248610496521 - trainLoss: 0.6589436531066895\n",
      "cnt: 0 - valLoss: 0.6588232517242432 - trainLoss: 0.658942461013794\n",
      "cnt: 0 - valLoss: 0.6588216423988342 - trainLoss: 0.658941388130188\n",
      "cnt: 0 - valLoss: 0.6588199734687805 - trainLoss: 0.6589402556419373\n",
      "cnt: 0 - valLoss: 0.6588183641433716 - trainLoss: 0.6589391231536865\n",
      "cnt: 0 - valLoss: 0.6588166952133179 - trainLoss: 0.6589380502700806\n",
      "cnt: 0 - valLoss: 0.6588150858879089 - trainLoss: 0.6589368581771851\n",
      "cnt: 0 - valLoss: 0.6588134169578552 - trainLoss: 0.6589357852935791\n",
      "cnt: 0 - valLoss: 0.6588117480278015 - trainLoss: 0.6589345932006836\n",
      "cnt: 0 - valLoss: 0.6588101387023926 - trainLoss: 0.6589335203170776\n",
      "cnt: 0 - valLoss: 0.6588084697723389 - trainLoss: 0.6589323878288269\n",
      "cnt: 0 - valLoss: 0.6588068008422852 - trainLoss: 0.6589312553405762\n",
      "cnt: 0 - valLoss: 0.6588051915168762 - trainLoss: 0.6589301228523254\n",
      "cnt: 0 - valLoss: 0.6588034629821777 - trainLoss: 0.6589289307594299\n",
      "cnt: 0 - valLoss: 0.658801794052124 - trainLoss: 0.6589277982711792\n",
      "cnt: 0 - valLoss: 0.6588001847267151 - trainLoss: 0.6589267253875732\n",
      "cnt: 0 - valLoss: 0.6587984561920166 - trainLoss: 0.6589255928993225\n",
      "cnt: 0 - valLoss: 0.6587968468666077 - trainLoss: 0.6589244604110718\n",
      "cnt: 0 - valLoss: 0.6587952375411987 - trainLoss: 0.658923327922821\n",
      "cnt: 0 - valLoss: 0.658793568611145 - trainLoss: 0.6589221954345703\n",
      "cnt: 0 - valLoss: 0.6587918996810913 - trainLoss: 0.6589210629463196\n",
      "cnt: 0 - valLoss: 0.6587902307510376 - trainLoss: 0.6589199304580688\n",
      "cnt: 0 - valLoss: 0.6587885618209839 - trainLoss: 0.6589188575744629\n",
      "cnt: 0 - valLoss: 0.6587868928909302 - trainLoss: 0.6589176654815674\n",
      "cnt: 0 - valLoss: 0.6587852239608765 - trainLoss: 0.6589165925979614\n",
      "cnt: 0 - valLoss: 0.6587835550308228 - trainLoss: 0.6589154005050659\n",
      "cnt: 0 - valLoss: 0.658781886100769 - trainLoss: 0.65891432762146\n",
      "cnt: 0 - valLoss: 0.6587802767753601 - trainLoss: 0.6589131951332092\n",
      "cnt: 0 - valLoss: 0.6587786674499512 - trainLoss: 0.6589120626449585\n",
      "cnt: 0 - valLoss: 0.6587769389152527 - trainLoss: 0.6589109301567078\n",
      "cnt: 0 - valLoss: 0.6587753295898438 - trainLoss: 0.658909797668457\n",
      "cnt: 0 - valLoss: 0.65877366065979 - trainLoss: 0.6589086651802063\n",
      "cnt: 0 - valLoss: 0.6587720513343811 - trainLoss: 0.6589075326919556\n",
      "cnt: 0 - valLoss: 0.6587703824043274 - trainLoss: 0.6589064002037048\n",
      "cnt: 0 - valLoss: 0.6587687134742737 - trainLoss: 0.6589052677154541\n",
      "cnt: 0 - valLoss: 0.6587669849395752 - trainLoss: 0.6589041352272034\n",
      "cnt: 0 - valLoss: 0.6587653160095215 - trainLoss: 0.6589030027389526\n",
      "cnt: 0 - valLoss: 0.6587636470794678 - trainLoss: 0.6589019298553467\n",
      "cnt: 0 - valLoss: 0.6587620377540588 - trainLoss: 0.658900797367096\n",
      "cnt: 0 - valLoss: 0.6587603092193604 - trainLoss: 0.6588996648788452\n",
      "cnt: 0 - valLoss: 0.6587586402893066 - trainLoss: 0.6588984727859497\n",
      "cnt: 0 - valLoss: 0.6587569713592529 - trainLoss: 0.6588973999023438\n",
      "cnt: 0 - valLoss: 0.6587553024291992 - trainLoss: 0.658896267414093\n",
      "cnt: 0 - valLoss: 0.6587536334991455 - trainLoss: 0.6588951349258423\n",
      "cnt: 0 - valLoss: 0.6587519645690918 - trainLoss: 0.6588939428329468\n",
      "cnt: 0 - valLoss: 0.6587502956390381 - trainLoss: 0.658892810344696\n",
      "cnt: 0 - valLoss: 0.6587486863136292 - trainLoss: 0.6588917374610901\n",
      "cnt: 0 - valLoss: 0.6587470173835754 - trainLoss: 0.6588906049728394\n",
      "cnt: 0 - valLoss: 0.658745288848877 - trainLoss: 0.6588894724845886\n",
      "cnt: 0 - valLoss: 0.6587436199188232 - trainLoss: 0.6588882803916931\n",
      "cnt: 0 - valLoss: 0.6587419509887695 - trainLoss: 0.6588872075080872\n",
      "cnt: 0 - valLoss: 0.658740222454071 - trainLoss: 0.6588860750198364\n",
      "cnt: 0 - valLoss: 0.6587386131286621 - trainLoss: 0.6588849425315857\n",
      "cnt: 0 - valLoss: 0.6587368845939636 - trainLoss: 0.658883810043335\n",
      "cnt: 0 - valLoss: 0.6587352156639099 - trainLoss: 0.6588826179504395\n",
      "cnt: 0 - valLoss: 0.6587335467338562 - trainLoss: 0.6588815450668335\n",
      "cnt: 0 - valLoss: 0.6587318778038025 - trainLoss: 0.658880352973938\n",
      "cnt: 0 - valLoss: 0.6587302684783936 - trainLoss: 0.658879280090332\n",
      "cnt: 0 - valLoss: 0.6587285399436951 - trainLoss: 0.6588780879974365\n",
      "cnt: 0 - valLoss: 0.6587268710136414 - trainLoss: 0.6588770151138306\n",
      "cnt: 0 - valLoss: 0.6587252020835876 - trainLoss: 0.6588758826255798\n",
      "cnt: 0 - valLoss: 0.6587234735488892 - trainLoss: 0.6588747501373291\n",
      "cnt: 0 - valLoss: 0.6587218046188354 - trainLoss: 0.6588735580444336\n",
      "cnt: 0 - valLoss: 0.6587201356887817 - trainLoss: 0.6588724851608276\n",
      "cnt: 0 - valLoss: 0.658718466758728 - trainLoss: 0.6588712334632874\n",
      "cnt: 0 - valLoss: 0.6587167382240295 - trainLoss: 0.6588701605796814\n",
      "cnt: 0 - valLoss: 0.6587150692939758 - trainLoss: 0.6588690280914307\n",
      "cnt: 0 - valLoss: 0.6587134003639221 - trainLoss: 0.6588678956031799\n",
      "cnt: 0 - valLoss: 0.6587117314338684 - trainLoss: 0.6588667631149292\n",
      "cnt: 0 - valLoss: 0.6587100028991699 - trainLoss: 0.6588656306266785\n",
      "cnt: 0 - valLoss: 0.6587083339691162 - trainLoss: 0.6588644981384277\n",
      "cnt: 0 - valLoss: 0.6587066650390625 - trainLoss: 0.6588633060455322\n",
      "cnt: 0 - valLoss: 0.6587049961090088 - trainLoss: 0.6588622331619263\n",
      "cnt: 0 - valLoss: 0.6587032079696655 - trainLoss: 0.6588610410690308\n",
      "cnt: 0 - valLoss: 0.6587015986442566 - trainLoss: 0.6588599681854248\n",
      "cnt: 0 - valLoss: 0.6586998701095581 - trainLoss: 0.6588587760925293\n",
      "cnt: 0 - valLoss: 0.6586982011795044 - trainLoss: 0.6588575839996338\n",
      "cnt: 0 - valLoss: 0.6586964726448059 - trainLoss: 0.6588563919067383\n",
      "cnt: 0 - valLoss: 0.6586947441101074 - trainLoss: 0.6588552594184875\n",
      "cnt: 0 - valLoss: 0.6586930155754089 - trainLoss: 0.658854067325592\n",
      "cnt: 0 - valLoss: 0.6586913466453552 - trainLoss: 0.6588528752326965\n",
      "cnt: 0 - valLoss: 0.658689558506012 - trainLoss: 0.6588517427444458\n",
      "cnt: 0 - valLoss: 0.6586878299713135 - trainLoss: 0.6588505506515503\n",
      "cnt: 0 - valLoss: 0.6586861610412598 - trainLoss: 0.6588493585586548\n",
      "cnt: 0 - valLoss: 0.658684492111206 - trainLoss: 0.6588481664657593\n",
      "cnt: 0 - valLoss: 0.6586827635765076 - trainLoss: 0.6588469743728638\n",
      "cnt: 0 - valLoss: 0.6586810350418091 - trainLoss: 0.658845841884613\n",
      "cnt: 0 - valLoss: 0.6586793661117554 - trainLoss: 0.6588446497917175\n",
      "cnt: 0 - valLoss: 0.6586776375770569 - trainLoss: 0.658843457698822\n",
      "cnt: 0 - valLoss: 0.6586759090423584 - trainLoss: 0.6588422656059265\n",
      "cnt: 0 - valLoss: 0.6586741805076599 - trainLoss: 0.6588411331176758\n",
      "cnt: 0 - valLoss: 0.658672571182251 - trainLoss: 0.6588399410247803\n",
      "cnt: 0 - valLoss: 0.6586707830429077 - trainLoss: 0.6588387489318848\n",
      "cnt: 0 - valLoss: 0.6586690545082092 - trainLoss: 0.6588375568389893\n",
      "cnt: 0 - valLoss: 0.6586674451828003 - trainLoss: 0.6588363647460938\n",
      "cnt: 0 - valLoss: 0.6586657166481018 - trainLoss: 0.658835232257843\n",
      "cnt: 0 - valLoss: 0.6586640477180481 - trainLoss: 0.6588340401649475\n",
      "cnt: 0 - valLoss: 0.6586623191833496 - trainLoss: 0.658832848072052\n",
      "cnt: 0 - valLoss: 0.6586606502532959 - trainLoss: 0.6588315963745117\n",
      "cnt: 0 - valLoss: 0.6586589813232422 - trainLoss: 0.6588304042816162\n",
      "cnt: 0 - valLoss: 0.6586573123931885 - trainLoss: 0.6588292717933655\n",
      "cnt: 0 - valLoss: 0.6586556434631348 - trainLoss: 0.6588280200958252\n",
      "cnt: 0 - valLoss: 0.658653974533081 - trainLoss: 0.6588268876075745\n",
      "cnt: 0 - valLoss: 0.6586522459983826 - trainLoss: 0.658825695514679\n",
      "cnt: 0 - valLoss: 0.6586505770683289 - trainLoss: 0.6588245630264282\n",
      "cnt: 0 - valLoss: 0.6586488485336304 - trainLoss: 0.6588233709335327\n",
      "cnt: 0 - valLoss: 0.6586471796035767 - trainLoss: 0.6588221192359924\n",
      "cnt: 0 - valLoss: 0.6586454510688782 - trainLoss: 0.6588209867477417\n",
      "cnt: 0 - valLoss: 0.6586438417434692 - trainLoss: 0.6588197946548462\n",
      "cnt: 0 - valLoss: 0.6586421132087708 - trainLoss: 0.6588186025619507\n",
      "cnt: 0 - valLoss: 0.658640444278717 - trainLoss: 0.6588174104690552\n",
      "cnt: 0 - valLoss: 0.6586387157440186 - trainLoss: 0.6588162183761597\n",
      "cnt: 0 - valLoss: 0.6586370468139648 - trainLoss: 0.6588150262832642\n",
      "cnt: 0 - valLoss: 0.6586353182792664 - trainLoss: 0.6588138341903687\n",
      "cnt: 0 - valLoss: 0.6586336493492126 - trainLoss: 0.6588126420974731\n",
      "cnt: 0 - valLoss: 0.6586319804191589 - trainLoss: 0.6588115096092224\n",
      "cnt: 0 - valLoss: 0.6586303114891052 - trainLoss: 0.6588103175163269\n",
      "cnt: 0 - valLoss: 0.6586285829544067 - trainLoss: 0.6588091254234314\n",
      "cnt: 0 - valLoss: 0.6586268544197083 - trainLoss: 0.6588079333305359\n",
      "cnt: 0 - valLoss: 0.6586251854896545 - trainLoss: 0.6588067412376404\n",
      "cnt: 0 - valLoss: 0.6586235165596008 - trainLoss: 0.6588056087493896\n",
      "cnt: 0 - valLoss: 0.6586217880249023 - trainLoss: 0.6588044166564941\n",
      "cnt: 0 - valLoss: 0.6586201190948486 - trainLoss: 0.6588031649589539\n",
      "cnt: 0 - valLoss: 0.6586183905601501 - trainLoss: 0.6588020324707031\n",
      "cnt: 0 - valLoss: 0.6586167216300964 - trainLoss: 0.6588007807731628\n",
      "cnt: 0 - valLoss: 0.658614993095398 - trainLoss: 0.6587995886802673\n",
      "cnt: 0 - valLoss: 0.6586132645606995 - trainLoss: 0.6587984561920166\n",
      "cnt: 0 - valLoss: 0.6586115956306458 - trainLoss: 0.6587972044944763\n",
      "cnt: 0 - valLoss: 0.6586098670959473 - trainLoss: 0.6587960720062256\n",
      "cnt: 0 - valLoss: 0.6586081981658936 - trainLoss: 0.6587948203086853\n",
      "cnt: 0 - valLoss: 0.6586064696311951 - trainLoss: 0.6587936878204346\n",
      "cnt: 0 - valLoss: 0.6586047410964966 - trainLoss: 0.6587924361228943\n",
      "cnt: 0 - valLoss: 0.6586030721664429 - trainLoss: 0.6587913036346436\n",
      "cnt: 0 - valLoss: 0.6586013436317444 - trainLoss: 0.6587899923324585\n",
      "cnt: 0 - valLoss: 0.6585996150970459 - trainLoss: 0.6587889194488525\n",
      "cnt: 0 - valLoss: 0.658598005771637 - trainLoss: 0.6587876677513123\n",
      "cnt: 0 - valLoss: 0.6585962772369385 - trainLoss: 0.6587864756584167\n",
      "cnt: 0 - valLoss: 0.6585946083068848 - trainLoss: 0.6587852835655212\n",
      "cnt: 0 - valLoss: 0.658592939376831 - trainLoss: 0.6587840914726257\n",
      "cnt: 0 - valLoss: 0.6585912108421326 - trainLoss: 0.6587828993797302\n",
      "cnt: 0 - valLoss: 0.6585895419120789 - trainLoss: 0.6587816476821899\n",
      "cnt: 0 - valLoss: 0.6585878729820251 - trainLoss: 0.6587804555892944\n",
      "cnt: 0 - valLoss: 0.6585862040519714 - trainLoss: 0.6587792634963989\n",
      "cnt: 0 - valLoss: 0.658584475517273 - trainLoss: 0.6587780117988586\n",
      "cnt: 0 - valLoss: 0.6585828065872192 - trainLoss: 0.6587768197059631\n",
      "cnt: 0 - valLoss: 0.6585811376571655 - trainLoss: 0.6587756276130676\n",
      "cnt: 0 - valLoss: 0.658579409122467 - trainLoss: 0.6587744355201721\n",
      "cnt: 0 - valLoss: 0.6585776805877686 - trainLoss: 0.6587732434272766\n",
      "cnt: 0 - valLoss: 0.6585760116577148 - trainLoss: 0.6587719917297363\n",
      "cnt: 0 - valLoss: 0.6585743427276611 - trainLoss: 0.6587707996368408\n",
      "cnt: 0 - valLoss: 0.6585726737976074 - trainLoss: 0.6587696671485901\n",
      "cnt: 0 - valLoss: 0.6585709452629089 - trainLoss: 0.6587684154510498\n",
      "cnt: 0 - valLoss: 0.6585692763328552 - trainLoss: 0.6587672233581543\n",
      "cnt: 0 - valLoss: 0.6585676074028015 - trainLoss: 0.6587660312652588\n",
      "cnt: 0 - valLoss: 0.658565878868103 - trainLoss: 0.6587648391723633\n",
      "cnt: 0 - valLoss: 0.6585642099380493 - trainLoss: 0.658763587474823\n",
      "cnt: 0 - valLoss: 0.6585624814033508 - trainLoss: 0.6587623953819275\n",
      "cnt: 0 - valLoss: 0.6585608720779419 - trainLoss: 0.6587611436843872\n",
      "cnt: 0 - valLoss: 0.6585591435432434 - trainLoss: 0.6587599515914917\n",
      "cnt: 0 - valLoss: 0.6585574150085449 - trainLoss: 0.6587587594985962\n",
      "cnt: 0 - valLoss: 0.6585556864738464 - trainLoss: 0.6587575674057007\n",
      "cnt: 0 - valLoss: 0.6585540175437927 - trainLoss: 0.6587563157081604\n",
      "cnt: 0 - valLoss: 0.658552348613739 - trainLoss: 0.6587551236152649\n",
      "cnt: 0 - valLoss: 0.6585506200790405 - trainLoss: 0.6587539315223694\n",
      "cnt: 0 - valLoss: 0.6585489511489868 - trainLoss: 0.6587527394294739\n",
      "cnt: 0 - valLoss: 0.6585472226142883 - trainLoss: 0.6587514877319336\n",
      "cnt: 0 - valLoss: 0.6585455536842346 - trainLoss: 0.6587502956390381\n",
      "cnt: 0 - valLoss: 0.6585438251495361 - trainLoss: 0.6587491035461426\n",
      "cnt: 0 - valLoss: 0.6585420966148376 - trainLoss: 0.6587479114532471\n",
      "cnt: 0 - valLoss: 0.6585404276847839 - trainLoss: 0.6587466597557068\n",
      "cnt: 0 - valLoss: 0.6585386991500854 - trainLoss: 0.6587454676628113\n",
      "cnt: 0 - valLoss: 0.6585370302200317 - trainLoss: 0.658744215965271\n",
      "cnt: 0 - valLoss: 0.6585353016853333 - trainLoss: 0.6587430238723755\n",
      "cnt: 0 - valLoss: 0.6585336327552795 - trainLoss: 0.65874183177948\n",
      "cnt: 0 - valLoss: 0.658531904220581 - trainLoss: 0.6587405800819397\n",
      "cnt: 0 - valLoss: 0.6585302352905273 - trainLoss: 0.6587393283843994\n",
      "cnt: 0 - valLoss: 0.6585285067558289 - trainLoss: 0.6587381958961487\n",
      "cnt: 0 - valLoss: 0.6585268378257751 - trainLoss: 0.6587369441986084\n",
      "cnt: 0 - valLoss: 0.6585251092910767 - trainLoss: 0.6587357521057129\n",
      "cnt: 0 - valLoss: 0.6585233211517334 - trainLoss: 0.6587345004081726\n",
      "cnt: 0 - valLoss: 0.6585215926170349 - trainLoss: 0.6587332487106323\n",
      "cnt: 0 - valLoss: 0.6585198640823364 - trainLoss: 0.658731997013092\n",
      "cnt: 0 - valLoss: 0.6585181355476379 - trainLoss: 0.6587308049201965\n",
      "cnt: 0 - valLoss: 0.6585164666175842 - trainLoss: 0.658729612827301\n",
      "cnt: 0 - valLoss: 0.6585147380828857 - trainLoss: 0.658728301525116\n",
      "cnt: 0 - valLoss: 0.6585130095481873 - trainLoss: 0.6587271094322205\n",
      "cnt: 0 - valLoss: 0.6585114002227783 - trainLoss: 0.658725917339325\n",
      "cnt: 0 - valLoss: 0.6585096120834351 - trainLoss: 0.6587246060371399\n",
      "cnt: 0 - valLoss: 0.6585078835487366 - trainLoss: 0.6587234139442444\n",
      "cnt: 0 - valLoss: 0.6585061550140381 - trainLoss: 0.6587221622467041\n",
      "cnt: 0 - valLoss: 0.6585044264793396 - trainLoss: 0.6587209105491638\n",
      "cnt: 0 - valLoss: 0.6585026979446411 - trainLoss: 0.6587197184562683\n",
      "cnt: 0 - valLoss: 0.6585010290145874 - trainLoss: 0.658718466758728\n",
      "cnt: 0 - valLoss: 0.6584992408752441 - trainLoss: 0.6587172150611877\n",
      "cnt: 0 - valLoss: 0.6584975719451904 - trainLoss: 0.6587159633636475\n",
      "cnt: 0 - valLoss: 0.6584957242012024 - trainLoss: 0.6587147116661072\n",
      "cnt: 0 - valLoss: 0.6584940552711487 - trainLoss: 0.6587135195732117\n",
      "cnt: 0 - valLoss: 0.6584923267364502 - trainLoss: 0.6587122678756714\n",
      "cnt: 0 - valLoss: 0.6584905385971069 - trainLoss: 0.6587110161781311\n",
      "cnt: 0 - valLoss: 0.6584888696670532 - trainLoss: 0.6587098240852356\n",
      "cnt: 0 - valLoss: 0.6584870219230652 - trainLoss: 0.6587085127830505\n",
      "cnt: 0 - valLoss: 0.6584852933883667 - trainLoss: 0.658707320690155\n",
      "cnt: 0 - valLoss: 0.6584835052490234 - trainLoss: 0.6587060689926147\n",
      "cnt: 0 - valLoss: 0.658481776714325 - trainLoss: 0.6587048768997192\n",
      "cnt: 0 - valLoss: 0.6584799885749817 - trainLoss: 0.658703625202179\n",
      "cnt: 0 - valLoss: 0.6584782600402832 - trainLoss: 0.6587024927139282\n",
      "cnt: 0 - valLoss: 0.6584764719009399 - trainLoss: 0.6587011814117432\n",
      "cnt: 0 - valLoss: 0.6584746837615967 - trainLoss: 0.6586999893188477\n",
      "cnt: 0 - valLoss: 0.6584729552268982 - trainLoss: 0.6586987972259521\n",
      "cnt: 0 - valLoss: 0.6584711670875549 - trainLoss: 0.6586975455284119\n",
      "cnt: 0 - valLoss: 0.6584694385528564 - trainLoss: 0.6586963534355164\n",
      "cnt: 0 - valLoss: 0.658467710018158 - trainLoss: 0.6586951017379761\n",
      "cnt: 0 - valLoss: 0.6584659814834595 - trainLoss: 0.6586938500404358\n",
      "cnt: 0 - valLoss: 0.6584641933441162 - trainLoss: 0.6586925983428955\n",
      "cnt: 0 - valLoss: 0.658462405204773 - trainLoss: 0.6586913466453552\n",
      "cnt: 0 - valLoss: 0.6584606170654297 - trainLoss: 0.6586901545524597\n",
      "cnt: 0 - valLoss: 0.6584588885307312 - trainLoss: 0.6586889028549194\n",
      "cnt: 0 - valLoss: 0.6584571003913879 - trainLoss: 0.6586877107620239\n",
      "cnt: 0 - valLoss: 0.6584553122520447 - trainLoss: 0.6586864590644836\n",
      "cnt: 0 - valLoss: 0.6584535837173462 - trainLoss: 0.6586852669715881\n",
      "cnt: 0 - valLoss: 0.6584517955780029 - trainLoss: 0.6586840152740479\n",
      "cnt: 0 - valLoss: 0.6584500670433044 - trainLoss: 0.6586827039718628\n",
      "cnt: 0 - valLoss: 0.6584482789039612 - trainLoss: 0.6586814522743225\n",
      "cnt: 0 - valLoss: 0.6584465503692627 - trainLoss: 0.658680260181427\n",
      "cnt: 0 - valLoss: 0.6584447026252747 - trainLoss: 0.6586790680885315\n",
      "cnt: 0 - valLoss: 0.6584429740905762 - trainLoss: 0.6586778163909912\n",
      "cnt: 0 - valLoss: 0.6584411859512329 - trainLoss: 0.6586765646934509\n",
      "cnt: 0 - valLoss: 0.6584394574165344 - trainLoss: 0.6586753726005554\n",
      "cnt: 0 - valLoss: 0.6584376692771912 - trainLoss: 0.6586741209030151\n",
      "cnt: 0 - valLoss: 0.6584358811378479 - trainLoss: 0.6586728096008301\n",
      "cnt: 0 - valLoss: 0.6584340929985046 - trainLoss: 0.6586716175079346\n",
      "cnt: 0 - valLoss: 0.6584323048591614 - trainLoss: 0.6586703658103943\n",
      "cnt: 0 - valLoss: 0.6584305167198181 - trainLoss: 0.6586691737174988\n",
      "cnt: 0 - valLoss: 0.6584287285804749 - trainLoss: 0.6586678624153137\n",
      "cnt: 0 - valLoss: 0.6584270596504211 - trainLoss: 0.6586666703224182\n",
      "cnt: 0 - valLoss: 0.6584252119064331 - trainLoss: 0.6586654782295227\n",
      "cnt: 0 - valLoss: 0.6584234833717346 - trainLoss: 0.6586641669273376\n",
      "cnt: 0 - valLoss: 0.6584216952323914 - trainLoss: 0.6586629748344421\n",
      "cnt: 0 - valLoss: 0.6584199070930481 - trainLoss: 0.6586617231369019\n",
      "cnt: 0 - valLoss: 0.6584181189537048 - trainLoss: 0.6586605310440063\n",
      "cnt: 0 - valLoss: 0.6584163308143616 - trainLoss: 0.6586592197418213\n",
      "cnt: 0 - valLoss: 0.6584145426750183 - trainLoss: 0.6586579084396362\n",
      "cnt: 0 - valLoss: 0.658412754535675 - trainLoss: 0.658656656742096\n",
      "cnt: 0 - valLoss: 0.6584109663963318 - trainLoss: 0.6586554646492004\n",
      "cnt: 0 - valLoss: 0.6584091782569885 - trainLoss: 0.6586541533470154\n",
      "cnt: 0 - valLoss: 0.6584073901176453 - trainLoss: 0.6586529612541199\n",
      "cnt: 0 - valLoss: 0.6584055423736572 - trainLoss: 0.6586517095565796\n",
      "cnt: 0 - valLoss: 0.658403754234314 - trainLoss: 0.6586504578590393\n",
      "cnt: 0 - valLoss: 0.6584020256996155 - trainLoss: 0.6586491465568542\n",
      "cnt: 0 - valLoss: 0.6584001183509827 - trainLoss: 0.658647894859314\n",
      "cnt: 0 - valLoss: 0.6583983302116394 - trainLoss: 0.6586467027664185\n",
      "cnt: 0 - valLoss: 0.6583965420722961 - trainLoss: 0.6586454510688782\n",
      "cnt: 0 - valLoss: 0.6583947539329529 - trainLoss: 0.6586441397666931\n",
      "cnt: 0 - valLoss: 0.6583929657936096 - trainLoss: 0.6586428880691528\n",
      "cnt: 0 - valLoss: 0.6583911776542664 - trainLoss: 0.6586416959762573\n",
      "cnt: 0 - valLoss: 0.6583893299102783 - trainLoss: 0.6586403846740723\n",
      "cnt: 0 - valLoss: 0.6583875417709351 - trainLoss: 0.6586391925811768\n",
      "cnt: 0 - valLoss: 0.658385694026947 - trainLoss: 0.6586378812789917\n",
      "cnt: 0 - valLoss: 0.6583839654922485 - trainLoss: 0.6586366295814514\n",
      "cnt: 0 - valLoss: 0.6583821773529053 - trainLoss: 0.6586353778839111\n",
      "cnt: 0 - valLoss: 0.6583803296089172 - trainLoss: 0.6586341261863708\n",
      "cnt: 0 - valLoss: 0.6583786010742188 - trainLoss: 0.6586328744888306\n",
      "cnt: 0 - valLoss: 0.6583767533302307 - trainLoss: 0.6586316823959351\n",
      "cnt: 0 - valLoss: 0.6583749651908875 - trainLoss: 0.6586304306983948\n",
      "cnt: 0 - valLoss: 0.6583731770515442 - trainLoss: 0.6586291790008545\n",
      "cnt: 0 - valLoss: 0.6583713889122009 - trainLoss: 0.6586279273033142\n",
      "cnt: 0 - valLoss: 0.6583696007728577 - trainLoss: 0.6586266756057739\n",
      "cnt: 0 - valLoss: 0.6583678126335144 - trainLoss: 0.6586254239082336\n",
      "cnt: 0 - valLoss: 0.6583660244941711 - trainLoss: 0.6586242318153381\n",
      "cnt: 0 - valLoss: 0.6583641767501831 - trainLoss: 0.6586229801177979\n",
      "cnt: 0 - valLoss: 0.6583623886108398 - trainLoss: 0.6586217284202576\n",
      "cnt: 0 - valLoss: 0.6583606004714966 - trainLoss: 0.6586204171180725\n",
      "cnt: 0 - valLoss: 0.6583588123321533 - trainLoss: 0.658619225025177\n",
      "cnt: 0 - valLoss: 0.6583569645881653 - trainLoss: 0.6586179733276367\n",
      "cnt: 0 - valLoss: 0.658355176448822 - trainLoss: 0.6586167216300964\n",
      "cnt: 0 - valLoss: 0.6583533883094788 - trainLoss: 0.6586154699325562\n",
      "cnt: 0 - valLoss: 0.6583515405654907 - trainLoss: 0.6586142182350159\n",
      "cnt: 0 - valLoss: 0.6583498120307922 - trainLoss: 0.6586130261421204\n",
      "cnt: 0 - valLoss: 0.6583479642868042 - trainLoss: 0.6586117744445801\n",
      "cnt: 0 - valLoss: 0.6583461761474609 - trainLoss: 0.6586105227470398\n",
      "cnt: 0 - valLoss: 0.6583443284034729 - trainLoss: 0.6586092710494995\n",
      "cnt: 0 - valLoss: 0.6583425402641296 - trainLoss: 0.658608078956604\n",
      "cnt: 0 - valLoss: 0.6583406925201416 - trainLoss: 0.6586068272590637\n",
      "cnt: 0 - valLoss: 0.6583390235900879 - trainLoss: 0.6586055755615234\n",
      "cnt: 0 - valLoss: 0.6583371758460999 - trainLoss: 0.6586043238639832\n",
      "cnt: 0 - valLoss: 0.6583353281021118 - trainLoss: 0.6586031317710876\n",
      "cnt: 0 - valLoss: 0.6583335399627686 - trainLoss: 0.6586018204689026\n",
      "cnt: 0 - valLoss: 0.6583317518234253 - trainLoss: 0.6586005687713623\n",
      "cnt: 0 - valLoss: 0.6583299040794373 - trainLoss: 0.658599317073822\n",
      "cnt: 0 - valLoss: 0.658328115940094 - trainLoss: 0.6585981249809265\n",
      "cnt: 0 - valLoss: 0.6583263278007507 - trainLoss: 0.6585968732833862\n",
      "cnt: 0 - valLoss: 0.6583244800567627 - trainLoss: 0.6585956811904907\n",
      "cnt: 0 - valLoss: 0.6583226323127747 - trainLoss: 0.6585944294929504\n",
      "cnt: 0 - valLoss: 0.6583208441734314 - trainLoss: 0.6585931777954102\n",
      "cnt: 0 - valLoss: 0.6583190560340881 - trainLoss: 0.6585919260978699\n",
      "cnt: 0 - valLoss: 0.6583172082901001 - trainLoss: 0.6585906744003296\n",
      "cnt: 0 - valLoss: 0.6583154201507568 - trainLoss: 0.6585894823074341\n",
      "cnt: 0 - valLoss: 0.658313512802124 - trainLoss: 0.658588171005249\n",
      "cnt: 0 - valLoss: 0.6583117246627808 - trainLoss: 0.6585869193077087\n",
      "cnt: 0 - valLoss: 0.6583099365234375 - trainLoss: 0.6585856676101685\n",
      "cnt: 0 - valLoss: 0.6583081483840942 - trainLoss: 0.6585844159126282\n",
      "cnt: 0 - valLoss: 0.658306360244751 - trainLoss: 0.6585831642150879\n",
      "cnt: 0 - valLoss: 0.6583045125007629 - trainLoss: 0.6585819125175476\n",
      "cnt: 0 - valLoss: 0.6583026647567749 - trainLoss: 0.6585806608200073\n",
      "cnt: 0 - valLoss: 0.6583008766174316 - trainLoss: 0.658579409122467\n",
      "cnt: 0 - valLoss: 0.6582990288734436 - trainLoss: 0.6585781574249268\n",
      "cnt: 0 - valLoss: 0.6582971811294556 - trainLoss: 0.6585769057273865\n",
      "cnt: 0 - valLoss: 0.6582954525947571 - trainLoss: 0.6585756540298462\n",
      "cnt: 0 - valLoss: 0.6582935452461243 - trainLoss: 0.6585744619369507\n",
      "cnt: 0 - valLoss: 0.658291757106781 - trainLoss: 0.6585732102394104\n",
      "cnt: 0 - valLoss: 0.658289909362793 - trainLoss: 0.6585718989372253\n",
      "cnt: 0 - valLoss: 0.6582880616188049 - trainLoss: 0.6585706472396851\n",
      "cnt: 0 - valLoss: 0.6582862734794617 - trainLoss: 0.6585693955421448\n",
      "cnt: 0 - valLoss: 0.6582843661308289 - trainLoss: 0.6585681438446045\n",
      "cnt: 0 - valLoss: 0.6582826375961304 - trainLoss: 0.6585668921470642\n",
      "cnt: 0 - valLoss: 0.6582807898521423 - trainLoss: 0.6585656404495239\n",
      "cnt: 0 - valLoss: 0.6582789421081543 - trainLoss: 0.6585643887519836\n",
      "cnt: 0 - valLoss: 0.6582770943641663 - trainLoss: 0.6585631966590881\n",
      "cnt: 0 - valLoss: 0.658275306224823 - trainLoss: 0.6585618853569031\n",
      "cnt: 0 - valLoss: 0.6582733988761902 - trainLoss: 0.6585606336593628\n",
      "cnt: 0 - valLoss: 0.6582716107368469 - trainLoss: 0.6585593819618225\n",
      "cnt: 0 - valLoss: 0.6582698225975037 - trainLoss: 0.6585581302642822\n",
      "cnt: 0 - valLoss: 0.6582679748535156 - trainLoss: 0.6585568189620972\n",
      "cnt: 0 - valLoss: 0.6582661271095276 - trainLoss: 0.6585555672645569\n",
      "cnt: 0 - valLoss: 0.6582642197608948 - trainLoss: 0.6585543155670166\n",
      "cnt: 0 - valLoss: 0.6582624316215515 - trainLoss: 0.6585530638694763\n",
      "cnt: 0 - valLoss: 0.6582605838775635 - trainLoss: 0.658551812171936\n",
      "cnt: 0 - valLoss: 0.6582587361335754 - trainLoss: 0.658550500869751\n",
      "cnt: 0 - valLoss: 0.6582570672035217 - trainLoss: 0.6585491895675659\n",
      "cnt: 0 - valLoss: 0.658255398273468 - trainLoss: 0.6585479974746704\n",
      "cnt: 0 - valLoss: 0.65825355052948 - trainLoss: 0.6585467457771301\n",
      "cnt: 0 - valLoss: 0.658251941204071 - trainLoss: 0.6585454344749451\n",
      "cnt: 0 - valLoss: 0.6582503318786621 - trainLoss: 0.6585441827774048\n",
      "cnt: 0 - valLoss: 0.6582484245300293 - trainLoss: 0.6585428714752197\n",
      "cnt: 0 - valLoss: 0.6582467555999756 - trainLoss: 0.6585416793823242\n",
      "cnt: 0 - valLoss: 0.6582450866699219 - trainLoss: 0.6585404276847839\n",
      "cnt: 0 - valLoss: 0.6582432389259338 - trainLoss: 0.6585391759872437\n",
      "cnt: 0 - valLoss: 0.6582416296005249 - trainLoss: 0.6585378646850586\n",
      "cnt: 0 - valLoss: 0.6582399606704712 - trainLoss: 0.6585366129875183\n",
      "cnt: 0 - valLoss: 0.6582381129264832 - trainLoss: 0.658535361289978\n",
      "cnt: 0 - valLoss: 0.6582364439964294 - trainLoss: 0.658534049987793\n",
      "cnt: 0 - valLoss: 0.6582347750663757 - trainLoss: 0.6585327982902527\n",
      "cnt: 0 - valLoss: 0.6582328677177429 - trainLoss: 0.6585315465927124\n",
      "cnt: 0 - valLoss: 0.6582311391830444 - trainLoss: 0.6585302948951721\n",
      "cnt: 0 - valLoss: 0.6582294702529907 - trainLoss: 0.6585290431976318\n",
      "cnt: 0 - valLoss: 0.658227801322937 - trainLoss: 0.6585278511047363\n",
      "cnt: 0 - valLoss: 0.658225953578949 - trainLoss: 0.658526599407196\n",
      "cnt: 0 - valLoss: 0.6582242846488953 - trainLoss: 0.6585253477096558\n",
      "cnt: 0 - valLoss: 0.658222496509552 - trainLoss: 0.6585240364074707\n",
      "cnt: 0 - valLoss: 0.6582208871841431 - trainLoss: 0.6585227847099304\n",
      "cnt: 0 - valLoss: 0.6582191586494446 - trainLoss: 0.6585215926170349\n",
      "cnt: 0 - valLoss: 0.6582173109054565 - trainLoss: 0.6585203409194946\n",
      "cnt: 0 - valLoss: 0.6582155823707581 - trainLoss: 0.6585190296173096\n",
      "cnt: 0 - valLoss: 0.6582138538360596 - trainLoss: 0.6585177779197693\n",
      "cnt: 0 - valLoss: 0.6582120656967163 - trainLoss: 0.658516526222229\n",
      "cnt: 0 - valLoss: 0.6582103967666626 - trainLoss: 0.6585152745246887\n",
      "cnt: 0 - valLoss: 0.6582087278366089 - trainLoss: 0.6585140228271484\n",
      "cnt: 0 - valLoss: 0.6582070589065552 - trainLoss: 0.6585127711296082\n",
      "cnt: 0 - valLoss: 0.6582052111625671 - trainLoss: 0.6585115790367126\n",
      "cnt: 0 - valLoss: 0.6582035422325134 - trainLoss: 0.6585102677345276\n",
      "cnt: 0 - valLoss: 0.6582018733024597 - trainLoss: 0.6585089564323425\n",
      "cnt: 0 - valLoss: 0.6582002639770508 - trainLoss: 0.6585077047348022\n",
      "cnt: 0 - valLoss: 0.6581982970237732 - trainLoss: 0.658506453037262\n",
      "cnt: 0 - valLoss: 0.6581966876983643 - trainLoss: 0.6585052013397217\n",
      "cnt: 0 - valLoss: 0.6581950783729553 - trainLoss: 0.6585039496421814\n",
      "cnt: 0 - valLoss: 0.6581933498382568 - trainLoss: 0.6585026383399963\n",
      "cnt: 0 - valLoss: 0.658191442489624 - trainLoss: 0.658501386642456\n",
      "cnt: 0 - valLoss: 0.6581897735595703 - trainLoss: 0.6585001349449158\n",
      "cnt: 0 - valLoss: 0.6581881046295166 - trainLoss: 0.6584989428520203\n",
      "cnt: 0 - valLoss: 0.6581864356994629 - trainLoss: 0.6584975719451904\n",
      "cnt: 0 - valLoss: 0.6581845283508301 - trainLoss: 0.6584963798522949\n",
      "cnt: 0 - valLoss: 0.6581829190254211 - trainLoss: 0.6584950685501099\n",
      "cnt: 0 - valLoss: 0.6581812500953674 - trainLoss: 0.6584938168525696\n",
      "cnt: 0 - valLoss: 0.658179521560669 - trainLoss: 0.6584925651550293\n",
      "cnt: 0 - valLoss: 0.6581776738166809 - trainLoss: 0.6584912538528442\n",
      "cnt: 0 - valLoss: 0.6581759452819824 - trainLoss: 0.658490002155304\n",
      "cnt: 0 - valLoss: 0.6581743359565735 - trainLoss: 0.6584888100624084\n",
      "cnt: 0 - valLoss: 0.658172607421875 - trainLoss: 0.6584874987602234\n",
      "cnt: 0 - valLoss: 0.6581707000732422 - trainLoss: 0.6584861874580383\n",
      "cnt: 0 - valLoss: 0.6581690311431885 - trainLoss: 0.658484935760498\n",
      "cnt: 0 - valLoss: 0.65816730260849 - trainLoss: 0.6584837436676025\n",
      "cnt: 0 - valLoss: 0.6581655740737915 - trainLoss: 0.658482551574707\n",
      "cnt: 0 - valLoss: 0.658163845539093 - trainLoss: 0.6584812998771667\n",
      "cnt: 0 - valLoss: 0.6581621766090393 - trainLoss: 0.6584800481796265\n",
      "cnt: 0 - valLoss: 0.6581604480743408 - trainLoss: 0.6584787964820862\n",
      "cnt: 0 - valLoss: 0.6581587791442871 - trainLoss: 0.6584775447845459\n",
      "cnt: 0 - valLoss: 0.6581570506095886 - trainLoss: 0.6584762930870056\n",
      "cnt: 0 - valLoss: 0.6581552624702454 - trainLoss: 0.6584751009941101\n",
      "cnt: 0 - valLoss: 0.6581535935401917 - trainLoss: 0.658473789691925\n",
      "cnt: 0 - valLoss: 0.6581519246101379 - trainLoss: 0.6584725975990295\n",
      "cnt: 0 - valLoss: 0.6581501960754395 - trainLoss: 0.6584713459014893\n",
      "cnt: 0 - valLoss: 0.658148467540741 - trainLoss: 0.658470094203949\n",
      "cnt: 0 - valLoss: 0.6581467390060425 - trainLoss: 0.6584689021110535\n",
      "cnt: 0 - valLoss: 0.6581450700759888 - trainLoss: 0.6584676504135132\n",
      "cnt: 0 - valLoss: 0.6581434011459351 - trainLoss: 0.6584663987159729\n",
      "cnt: 0 - valLoss: 0.6581416130065918 - trainLoss: 0.6584651470184326\n",
      "cnt: 0 - valLoss: 0.6581398844718933 - trainLoss: 0.6584639549255371\n",
      "cnt: 0 - valLoss: 0.6581381559371948 - trainLoss: 0.6584627032279968\n",
      "cnt: 0 - valLoss: 0.6581364274024963 - trainLoss: 0.6584614515304565\n",
      "cnt: 0 - valLoss: 0.6581346988677979 - trainLoss: 0.658460259437561\n",
      "cnt: 0 - valLoss: 0.6581329107284546 - trainLoss: 0.6584590077400208\n",
      "cnt: 0 - valLoss: 0.6581311225891113 - trainLoss: 0.6584578156471252\n",
      "cnt: 0 - valLoss: 0.6581293940544128 - trainLoss: 0.658456563949585\n",
      "cnt: 0 - valLoss: 0.6581276059150696 - trainLoss: 0.6584553718566895\n",
      "cnt: 0 - valLoss: 0.6581258177757263 - trainLoss: 0.658454179763794\n",
      "cnt: 0 - valLoss: 0.6581240296363831 - trainLoss: 0.6584528684616089\n",
      "cnt: 0 - valLoss: 0.6581223011016846 - trainLoss: 0.6584516763687134\n",
      "cnt: 0 - valLoss: 0.6581205129623413 - trainLoss: 0.6584504842758179\n",
      "cnt: 0 - valLoss: 0.6581187844276428 - trainLoss: 0.6584491729736328\n",
      "cnt: 0 - valLoss: 0.6581169962882996 - trainLoss: 0.6584479808807373\n",
      "cnt: 0 - valLoss: 0.6581152081489563 - trainLoss: 0.6584467887878418\n",
      "cnt: 0 - valLoss: 0.658113420009613 - trainLoss: 0.6584455370903015\n",
      "cnt: 0 - valLoss: 0.6581116318702698 - trainLoss: 0.6584442853927612\n",
      "cnt: 0 - valLoss: 0.6581098437309265 - trainLoss: 0.6584430932998657\n",
      "cnt: 0 - valLoss: 0.658108115196228 - trainLoss: 0.6584419012069702\n",
      "cnt: 0 - valLoss: 0.6581063270568848 - trainLoss: 0.6584406495094299\n",
      "cnt: 0 - valLoss: 0.6581044793128967 - trainLoss: 0.6584393978118896\n",
      "cnt: 0 - valLoss: 0.6581026315689087 - trainLoss: 0.6584381461143494\n",
      "cnt: 0 - valLoss: 0.6581008434295654 - trainLoss: 0.6584368944168091\n",
      "cnt: 0 - valLoss: 0.6580989956855774 - trainLoss: 0.6584356427192688\n",
      "cnt: 0 - valLoss: 0.6580971479415894 - trainLoss: 0.6584343314170837\n",
      "cnt: 0 - valLoss: 0.6580953001976013 - trainLoss: 0.6584330797195435\n",
      "cnt: 0 - valLoss: 0.6580935120582581 - trainLoss: 0.658431887626648\n",
      "cnt: 0 - valLoss: 0.65809166431427 - trainLoss: 0.6584306359291077\n",
      "cnt: 0 - valLoss: 0.658089816570282 - trainLoss: 0.6584293842315674\n",
      "cnt: 0 - valLoss: 0.658087968826294 - trainLoss: 0.6584280729293823\n",
      "cnt: 0 - valLoss: 0.6580861210823059 - trainLoss: 0.6584268808364868\n",
      "cnt: 0 - valLoss: 0.6580843329429626 - trainLoss: 0.6584256291389465\n",
      "cnt: 0 - valLoss: 0.6580824255943298 - trainLoss: 0.658424437046051\n",
      "cnt: 0 - valLoss: 0.6580806374549866 - trainLoss: 0.658423125743866\n",
      "cnt: 0 - valLoss: 0.6580787897109985 - trainLoss: 0.6584218740463257\n",
      "cnt: 0 - valLoss: 0.6580769419670105 - trainLoss: 0.6584206223487854\n",
      "cnt: 0 - valLoss: 0.6580751538276672 - trainLoss: 0.6584193706512451\n",
      "cnt: 0 - valLoss: 0.6580732464790344 - trainLoss: 0.6584181189537048\n",
      "cnt: 0 - valLoss: 0.6580714583396912 - trainLoss: 0.6584168672561646\n",
      "cnt: 0 - valLoss: 0.6580695509910583 - trainLoss: 0.6584156155586243\n",
      "cnt: 0 - valLoss: 0.6580677032470703 - trainLoss: 0.6584144234657288\n",
      "cnt: 0 - valLoss: 0.6580658555030823 - trainLoss: 0.6584131717681885\n",
      "cnt: 0 - valLoss: 0.6580640077590942 - trainLoss: 0.6584118008613586\n",
      "cnt: 0 - valLoss: 0.658062219619751 - trainLoss: 0.6584106087684631\n",
      "cnt: 0 - valLoss: 0.6580603122711182 - trainLoss: 0.6584093570709229\n",
      "cnt: 0 - valLoss: 0.6580584645271301 - trainLoss: 0.6584081649780273\n",
      "cnt: 0 - valLoss: 0.6580566167831421 - trainLoss: 0.6584068536758423\n",
      "cnt: 0 - valLoss: 0.6580547094345093 - trainLoss: 0.658405601978302\n",
      "cnt: 0 - valLoss: 0.658052921295166 - trainLoss: 0.6584042906761169\n",
      "cnt: 0 - valLoss: 0.658051073551178 - trainLoss: 0.6584030389785767\n",
      "cnt: 0 - valLoss: 0.6580492258071899 - trainLoss: 0.6584017872810364\n",
      "cnt: 0 - valLoss: 0.6580473780632019 - trainLoss: 0.6584005355834961\n",
      "cnt: 0 - valLoss: 0.6580454707145691 - trainLoss: 0.658399224281311\n",
      "cnt: 0 - valLoss: 0.658043622970581 - trainLoss: 0.6583979725837708\n",
      "cnt: 0 - valLoss: 0.6580418348312378 - trainLoss: 0.6583967804908752\n",
      "cnt: 0 - valLoss: 0.658039927482605 - trainLoss: 0.658395528793335\n",
      "cnt: 0 - valLoss: 0.6580380797386169 - trainLoss: 0.6583942174911499\n",
      "cnt: 0 - valLoss: 0.6580362915992737 - trainLoss: 0.6583929657936096\n",
      "cnt: 0 - valLoss: 0.6580344438552856 - trainLoss: 0.6583917140960693\n",
      "cnt: 0 - valLoss: 0.6580325961112976 - trainLoss: 0.6583904027938843\n",
      "cnt: 0 - valLoss: 0.6580307483673096 - trainLoss: 0.658389151096344\n",
      "cnt: 0 - valLoss: 0.6580289602279663 - trainLoss: 0.6583878397941589\n",
      "cnt: 0 - valLoss: 0.6580270528793335 - trainLoss: 0.6583866477012634\n",
      "cnt: 0 - valLoss: 0.6580252051353455 - trainLoss: 0.6583853960037231\n",
      "cnt: 0 - valLoss: 0.6580233573913574 - trainLoss: 0.6583840847015381\n",
      "cnt: 0 - valLoss: 0.6580214500427246 - trainLoss: 0.6583828330039978\n",
      "cnt: 0 - valLoss: 0.6580196022987366 - trainLoss: 0.6583815813064575\n",
      "cnt: 0 - valLoss: 0.6580177545547485 - trainLoss: 0.6583803296089172\n",
      "cnt: 0 - valLoss: 0.6580159068107605 - trainLoss: 0.6583790183067322\n",
      "cnt: 0 - valLoss: 0.6580139994621277 - trainLoss: 0.6583777666091919\n",
      "cnt: 0 - valLoss: 0.6580121517181396 - trainLoss: 0.6583764553070068\n",
      "cnt: 0 - valLoss: 0.6580103039741516 - trainLoss: 0.6583752036094666\n",
      "cnt: 0 - valLoss: 0.6580086350440979 - trainLoss: 0.658374011516571\n",
      "cnt: 0 - valLoss: 0.6580068469047546 - trainLoss: 0.658372700214386\n",
      "cnt: 0 - valLoss: 0.6580051779747009 - trainLoss: 0.6583714485168457\n",
      "cnt: 0 - valLoss: 0.6580033898353577 - trainLoss: 0.6583701968193054\n",
      "cnt: 0 - valLoss: 0.6580016613006592 - trainLoss: 0.6583689451217651\n",
      "cnt: 0 - valLoss: 0.6579999327659607 - trainLoss: 0.6583676934242249\n",
      "cnt: 0 - valLoss: 0.6579982042312622 - trainLoss: 0.6583664417266846\n",
      "cnt: 0 - valLoss: 0.6579964756965637 - trainLoss: 0.6583651900291443\n",
      "cnt: 0 - valLoss: 0.6579947471618652 - trainLoss: 0.658363938331604\n",
      "cnt: 0 - valLoss: 0.6579930782318115 - trainLoss: 0.6583626866340637\n",
      "cnt: 0 - valLoss: 0.657991349697113 - trainLoss: 0.6583614349365234\n",
      "cnt: 0 - valLoss: 0.6579895615577698 - trainLoss: 0.6583601832389832\n",
      "cnt: 0 - valLoss: 0.6579878926277161 - trainLoss: 0.6583589315414429\n",
      "cnt: 0 - valLoss: 0.6579861640930176 - trainLoss: 0.6583576798439026\n",
      "cnt: 0 - valLoss: 0.6579843759536743 - trainLoss: 0.6583563685417175\n",
      "cnt: 0 - valLoss: 0.657982587814331 - trainLoss: 0.658355176448822\n",
      "cnt: 0 - valLoss: 0.6579808592796326 - trainLoss: 0.658353865146637\n",
      "cnt: 0 - valLoss: 0.6579791307449341 - trainLoss: 0.6583526134490967\n",
      "cnt: 0 - valLoss: 0.6579774022102356 - trainLoss: 0.6583513617515564\n",
      "cnt: 0 - valLoss: 0.6579756736755371 - trainLoss: 0.6583501100540161\n",
      "cnt: 0 - valLoss: 0.6579740047454834 - trainLoss: 0.6583489179611206\n",
      "cnt: 0 - valLoss: 0.6579721570014954 - trainLoss: 0.6583476066589355\n",
      "cnt: 0 - valLoss: 0.6579704284667969 - trainLoss: 0.6583462953567505\n",
      "cnt: 0 - valLoss: 0.6579686999320984 - trainLoss: 0.6583451628684998\n",
      "cnt: 0 - valLoss: 0.6579669713973999 - trainLoss: 0.6583437919616699\n",
      "cnt: 0 - valLoss: 0.6579652428627014 - trainLoss: 0.6583425998687744\n",
      "cnt: 0 - valLoss: 0.6579634547233582 - trainLoss: 0.6583413481712341\n",
      "cnt: 0 - valLoss: 0.6579617261886597 - trainLoss: 0.6583400368690491\n",
      "cnt: 0 - valLoss: 0.6579599976539612 - trainLoss: 0.6583387851715088\n",
      "cnt: 0 - valLoss: 0.6579582095146179 - trainLoss: 0.6583375334739685\n",
      "cnt: 0 - valLoss: 0.6579564809799194 - trainLoss: 0.6583362817764282\n",
      "cnt: 0 - valLoss: 0.6579548120498657 - trainLoss: 0.6583349704742432\n",
      "cnt: 0 - valLoss: 0.6579530239105225 - trainLoss: 0.6583337783813477\n",
      "cnt: 0 - valLoss: 0.657951295375824 - trainLoss: 0.6583324670791626\n",
      "cnt: 0 - valLoss: 0.6579495668411255 - trainLoss: 0.6583312749862671\n",
      "cnt: 0 - valLoss: 0.657947838306427 - trainLoss: 0.6583299040794373\n",
      "cnt: 0 - valLoss: 0.657945990562439 - trainLoss: 0.6583287119865417\n",
      "cnt: 0 - valLoss: 0.6579442620277405 - trainLoss: 0.6583274602890015\n",
      "cnt: 0 - valLoss: 0.657942533493042 - trainLoss: 0.6583261489868164\n",
      "cnt: 0 - valLoss: 0.6579408049583435 - trainLoss: 0.6583249568939209\n",
      "cnt: 0 - valLoss: 0.6579390168190002 - trainLoss: 0.6583236455917358\n",
      "cnt: 0 - valLoss: 0.6579372882843018 - trainLoss: 0.6583224534988403\n",
      "cnt: 0 - valLoss: 0.6579355001449585 - trainLoss: 0.6583211421966553\n",
      "cnt: 0 - valLoss: 0.6579337120056152 - trainLoss: 0.658319890499115\n",
      "cnt: 0 - valLoss: 0.6579319834709167 - trainLoss: 0.6583186388015747\n",
      "cnt: 0 - valLoss: 0.6579302549362183 - trainLoss: 0.6583173871040344\n",
      "cnt: 0 - valLoss: 0.657928466796875 - trainLoss: 0.6583161354064941\n",
      "cnt: 0 - valLoss: 0.6579267382621765 - trainLoss: 0.6583148837089539\n",
      "cnt: 0 - valLoss: 0.657925009727478 - trainLoss: 0.6583136320114136\n",
      "cnt: 0 - valLoss: 0.6579232215881348 - trainLoss: 0.6583123803138733\n",
      "cnt: 0 - valLoss: 0.657921552658081 - trainLoss: 0.658311128616333\n",
      "cnt: 0 - valLoss: 0.657919704914093 - trainLoss: 0.6583098769187927\n",
      "cnt: 0 - valLoss: 0.6579180359840393 - trainLoss: 0.6583085656166077\n",
      "cnt: 0 - valLoss: 0.6579161286354065 - trainLoss: 0.6583073735237122\n",
      "cnt: 0 - valLoss: 0.657914400100708 - trainLoss: 0.6583060622215271\n",
      "cnt: 0 - valLoss: 0.6579126119613647 - trainLoss: 0.6583048105239868\n",
      "cnt: 0 - valLoss: 0.6579108238220215 - trainLoss: 0.6583036184310913\n",
      "cnt: 0 - valLoss: 0.657909095287323 - trainLoss: 0.658302366733551\n",
      "cnt: 0 - valLoss: 0.6579073071479797 - trainLoss: 0.6583011150360107\n",
      "cnt: 0 - valLoss: 0.6579055190086365 - trainLoss: 0.6582998633384705\n",
      "cnt: 0 - valLoss: 0.6579036712646484 - trainLoss: 0.6582985520362854\n",
      "cnt: 0 - valLoss: 0.6579020023345947 - trainLoss: 0.6582973599433899\n",
      "cnt: 0 - valLoss: 0.6579001545906067 - trainLoss: 0.6582961082458496\n",
      "cnt: 0 - valLoss: 0.657898485660553 - trainLoss: 0.6582949161529541\n",
      "cnt: 0 - valLoss: 0.6578966379165649 - trainLoss: 0.6582936644554138\n",
      "cnt: 0 - valLoss: 0.6578948497772217 - trainLoss: 0.6582924127578735\n",
      "cnt: 0 - valLoss: 0.6578930020332336 - trainLoss: 0.6582911014556885\n",
      "cnt: 0 - valLoss: 0.6578912734985352 - trainLoss: 0.6582899689674377\n",
      "cnt: 0 - valLoss: 0.6578894853591919 - trainLoss: 0.6582886576652527\n",
      "cnt: 0 - valLoss: 0.6578876972198486 - trainLoss: 0.6582874655723572\n",
      "cnt: 0 - valLoss: 0.6578859686851501 - trainLoss: 0.6582861542701721\n",
      "cnt: 0 - valLoss: 0.6578841209411621 - trainLoss: 0.6582849621772766\n",
      "cnt: 0 - valLoss: 0.6578823924064636 - trainLoss: 0.6582837700843811\n",
      "cnt: 0 - valLoss: 0.6578806042671204 - trainLoss: 0.658282458782196\n",
      "cnt: 0 - valLoss: 0.6578788161277771 - trainLoss: 0.6582812666893005\n",
      "cnt: 0 - valLoss: 0.6578770875930786 - trainLoss: 0.6582800149917603\n",
      "cnt: 0 - valLoss: 0.6578752994537354 - trainLoss: 0.6582788228988647\n",
      "cnt: 0 - valLoss: 0.6578735113143921 - trainLoss: 0.6582775712013245\n",
      "cnt: 0 - valLoss: 0.657871663570404 - trainLoss: 0.6582763195037842\n",
      "cnt: 0 - valLoss: 0.6578699350357056 - trainLoss: 0.6582750678062439\n",
      "cnt: 0 - valLoss: 0.6578681468963623 - trainLoss: 0.6582738161087036\n",
      "cnt: 0 - valLoss: 0.6578664183616638 - trainLoss: 0.6582725644111633\n",
      "cnt: 0 - valLoss: 0.6578646898269653 - trainLoss: 0.6582713723182678\n",
      "cnt: 0 - valLoss: 0.6578627824783325 - trainLoss: 0.6582701206207275\n",
      "cnt: 0 - valLoss: 0.657861053943634 - trainLoss: 0.6582688689231873\n",
      "cnt: 0 - valLoss: 0.6578592658042908 - trainLoss: 0.6582675576210022\n",
      "cnt: 0 - valLoss: 0.6578574776649475 - trainLoss: 0.6582663655281067\n",
      "cnt: 0 - valLoss: 0.6578556895256042 - trainLoss: 0.6582651138305664\n",
      "cnt: 0 - valLoss: 0.657853901386261 - trainLoss: 0.6582638621330261\n",
      "cnt: 0 - valLoss: 0.6578521728515625 - trainLoss: 0.6582626104354858\n",
      "cnt: 0 - valLoss: 0.6578503847122192 - trainLoss: 0.6582614183425903\n",
      "cnt: 0 - valLoss: 0.6578485369682312 - trainLoss: 0.65826016664505\n",
      "cnt: 0 - valLoss: 0.6578468084335327 - trainLoss: 0.6582589149475098\n",
      "cnt: 0 - valLoss: 0.6578449606895447 - trainLoss: 0.6582576632499695\n",
      "cnt: 0 - valLoss: 0.6578432321548462 - trainLoss: 0.658256471157074\n",
      "cnt: 0 - valLoss: 0.6578414440155029 - trainLoss: 0.6582551002502441\n",
      "cnt: 0 - valLoss: 0.6578395962715149 - trainLoss: 0.6582539677619934\n",
      "cnt: 0 - valLoss: 0.6578379273414612 - trainLoss: 0.6582527160644531\n",
      "cnt: 0 - valLoss: 0.6578361392021179 - trainLoss: 0.6582514643669128\n",
      "cnt: 0 - valLoss: 0.6578344106674194 - trainLoss: 0.6582502722740173\n",
      "cnt: 0 - valLoss: 0.6578326225280762 - trainLoss: 0.6582490801811218\n",
      "cnt: 0 - valLoss: 0.6578308939933777 - trainLoss: 0.6582478284835815\n",
      "cnt: 0 - valLoss: 0.6578291058540344 - trainLoss: 0.658246636390686\n",
      "cnt: 0 - valLoss: 0.6578273773193359 - trainLoss: 0.6582453846931458\n",
      "cnt: 0 - valLoss: 0.6578257083892822 - trainLoss: 0.6582441926002502\n",
      "cnt: 0 - valLoss: 0.6578239798545837 - trainLoss: 0.6582430005073547\n",
      "cnt: 0 - valLoss: 0.6578221917152405 - trainLoss: 0.6582418084144592\n",
      "cnt: 0 - valLoss: 0.657820463180542 - trainLoss: 0.658240556716919\n",
      "cnt: 0 - valLoss: 0.6578186750411987 - trainLoss: 0.6582393646240234\n",
      "cnt: 0 - valLoss: 0.6578169465065002 - trainLoss: 0.6582381725311279\n",
      "cnt: 0 - valLoss: 0.657815158367157 - trainLoss: 0.6582369208335876\n",
      "cnt: 0 - valLoss: 0.6578134298324585 - trainLoss: 0.6582357287406921\n",
      "cnt: 0 - valLoss: 0.65781170129776 - trainLoss: 0.6582344770431519\n",
      "cnt: 0 - valLoss: 0.6578099131584167 - trainLoss: 0.6582332849502563\n",
      "cnt: 0 - valLoss: 0.6578081846237183 - trainLoss: 0.6582320332527161\n",
      "cnt: 0 - valLoss: 0.6578063368797302 - trainLoss: 0.6582308411598206\n",
      "cnt: 0 - valLoss: 0.657804548740387 - trainLoss: 0.6582295894622803\n",
      "cnt: 0 - valLoss: 0.6578028202056885 - trainLoss: 0.6582283973693848\n",
      "cnt: 0 - valLoss: 0.6578011512756348 - trainLoss: 0.6582272052764893\n",
      "cnt: 0 - valLoss: 0.6577993035316467 - trainLoss: 0.658225953578949\n",
      "cnt: 0 - valLoss: 0.6577975749969482 - trainLoss: 0.6582247614860535\n",
      "cnt: 0 - valLoss: 0.657795786857605 - trainLoss: 0.658223569393158\n",
      "cnt: 0 - valLoss: 0.6577940583229065 - trainLoss: 0.6582223176956177\n",
      "cnt: 0 - valLoss: 0.6577922701835632 - trainLoss: 0.6582211256027222\n",
      "cnt: 0 - valLoss: 0.6577904224395752 - trainLoss: 0.6582198739051819\n",
      "cnt: 0 - valLoss: 0.6577887535095215 - trainLoss: 0.6582186818122864\n",
      "cnt: 0 - valLoss: 0.657787024974823 - trainLoss: 0.6582174897193909\n",
      "cnt: 0 - valLoss: 0.6577852368354797 - trainLoss: 0.6582162976264954\n",
      "cnt: 0 - valLoss: 0.6577835083007812 - trainLoss: 0.6582150459289551\n",
      "cnt: 0 - valLoss: 0.657781720161438 - trainLoss: 0.6582138538360596\n",
      "cnt: 0 - valLoss: 0.6577799320220947 - trainLoss: 0.6582125425338745\n",
      "cnt: 0 - valLoss: 0.6577782034873962 - trainLoss: 0.658211350440979\n",
      "cnt: 0 - valLoss: 0.657776415348053 - trainLoss: 0.6582101583480835\n",
      "cnt: 0 - valLoss: 0.6577746868133545 - trainLoss: 0.658208966255188\n",
      "cnt: 0 - valLoss: 0.6577728986740112 - trainLoss: 0.6582077145576477\n",
      "cnt: 0 - valLoss: 0.6577710509300232 - trainLoss: 0.6582064628601074\n",
      "cnt: 0 - valLoss: 0.6577693819999695 - trainLoss: 0.6582052111625671\n",
      "cnt: 0 - valLoss: 0.6577675342559814 - trainLoss: 0.6582040190696716\n",
      "cnt: 0 - valLoss: 0.6577657461166382 - trainLoss: 0.6582027673721313\n",
      "cnt: 0 - valLoss: 0.6577640175819397 - trainLoss: 0.6582015752792358\n",
      "cnt: 0 - valLoss: 0.6577622294425964 - trainLoss: 0.6582003235816956\n",
      "cnt: 0 - valLoss: 0.6577604413032532 - trainLoss: 0.6581990718841553\n",
      "cnt: 0 - valLoss: 0.6577587127685547 - trainLoss: 0.6581978797912598\n",
      "cnt: 0 - valLoss: 0.6577569246292114 - trainLoss: 0.6581966280937195\n",
      "cnt: 0 - valLoss: 0.6577551364898682 - trainLoss: 0.658195436000824\n",
      "cnt: 0 - valLoss: 0.6577533483505249 - trainLoss: 0.6581941843032837\n",
      "cnt: 0 - valLoss: 0.6577515602111816 - trainLoss: 0.6581929922103882\n",
      "cnt: 0 - valLoss: 0.6577498316764832 - trainLoss: 0.6581916809082031\n",
      "cnt: 0 - valLoss: 0.6577480435371399 - trainLoss: 0.6581904292106628\n",
      "cnt: 0 - valLoss: 0.6577463150024414 - trainLoss: 0.6581892371177673\n",
      "cnt: 0 - valLoss: 0.6577444076538086 - trainLoss: 0.6581880450248718\n",
      "cnt: 0 - valLoss: 0.6577426791191101 - trainLoss: 0.6581867933273315\n",
      "cnt: 0 - valLoss: 0.6577408909797668 - trainLoss: 0.658185601234436\n",
      "cnt: 0 - valLoss: 0.6577391624450684 - trainLoss: 0.658184289932251\n",
      "cnt: 0 - valLoss: 0.6577373743057251 - trainLoss: 0.6581830978393555\n",
      "cnt: 0 - valLoss: 0.6577355861663818 - trainLoss: 0.6581818461418152\n",
      "cnt: 0 - valLoss: 0.6577337980270386 - trainLoss: 0.6581805944442749\n",
      "cnt: 0 - valLoss: 0.6577320098876953 - trainLoss: 0.6581793427467346\n",
      "cnt: 0 - valLoss: 0.6577302813529968 - trainLoss: 0.6581781506538391\n",
      "cnt: 0 - valLoss: 0.657728374004364 - trainLoss: 0.6581769585609436\n",
      "cnt: 0 - valLoss: 0.6577266454696655 - trainLoss: 0.6581756472587585\n",
      "cnt: 0 - valLoss: 0.6577248573303223 - trainLoss: 0.658174455165863\n",
      "cnt: 0 - valLoss: 0.657723069190979 - trainLoss: 0.6581732034683228\n",
      "cnt: 0 - valLoss: 0.6577213406562805 - trainLoss: 0.6581719517707825\n",
      "cnt: 0 - valLoss: 0.6577195525169373 - trainLoss: 0.658170759677887\n",
      "cnt: 0 - valLoss: 0.6577178239822388 - trainLoss: 0.6581695675849915\n",
      "cnt: 0 - valLoss: 0.6577159762382507 - trainLoss: 0.6581682562828064\n",
      "cnt: 0 - valLoss: 0.6577142477035522 - trainLoss: 0.6581671237945557\n",
      "cnt: 0 - valLoss: 0.657712459564209 - trainLoss: 0.6581658720970154\n",
      "cnt: 0 - valLoss: 0.6577106714248657 - trainLoss: 0.6581646800041199\n",
      "cnt: 0 - valLoss: 0.6577089428901672 - trainLoss: 0.6581634283065796\n",
      "cnt: 0 - valLoss: 0.657707154750824 - trainLoss: 0.6581622362136841\n",
      "cnt: 0 - valLoss: 0.6577053070068359 - trainLoss: 0.6581609845161438\n",
      "cnt: 0 - valLoss: 0.6577036380767822 - trainLoss: 0.6581597924232483\n",
      "cnt: 0 - valLoss: 0.6577019095420837 - trainLoss: 0.658158540725708\n",
      "cnt: 0 - valLoss: 0.65770024061203 - trainLoss: 0.6581573486328125\n",
      "cnt: 0 - valLoss: 0.6576985716819763 - trainLoss: 0.658156156539917\n",
      "cnt: 0 - valLoss: 0.6576967835426331 - trainLoss: 0.6581548452377319\n",
      "cnt: 0 - valLoss: 0.6576951146125793 - trainLoss: 0.6581537127494812\n",
      "cnt: 0 - valLoss: 0.6576934456825256 - trainLoss: 0.6581524610519409\n",
      "cnt: 0 - valLoss: 0.6576917171478271 - trainLoss: 0.6581512689590454\n",
      "cnt: 0 - valLoss: 0.6576900482177734 - trainLoss: 0.6581500172615051\n",
      "cnt: 0 - valLoss: 0.6576883792877197 - trainLoss: 0.6581487655639648\n",
      "cnt: 0 - valLoss: 0.6576866507530212 - trainLoss: 0.6581475734710693\n",
      "cnt: 0 - valLoss: 0.6576849222183228 - trainLoss: 0.6581463813781738\n",
      "cnt: 0 - valLoss: 0.6576831936836243 - trainLoss: 0.6581450700759888\n",
      "cnt: 0 - valLoss: 0.6576815843582153 - trainLoss: 0.6581438779830933\n",
      "cnt: 0 - valLoss: 0.6576797962188721 - trainLoss: 0.658142626285553\n",
      "cnt: 0 - valLoss: 0.6576781272888184 - trainLoss: 0.6581414341926575\n",
      "cnt: 0 - valLoss: 0.6576764583587646 - trainLoss: 0.658140242099762\n",
      "cnt: 0 - valLoss: 0.6576746702194214 - trainLoss: 0.6581389904022217\n",
      "cnt: 0 - valLoss: 0.6576730012893677 - trainLoss: 0.6581377387046814\n",
      "cnt: 0 - valLoss: 0.657671332359314 - trainLoss: 0.6581365466117859\n",
      "cnt: 0 - valLoss: 0.6576696038246155 - trainLoss: 0.6581352949142456\n",
      "cnt: 0 - valLoss: 0.6576679348945618 - trainLoss: 0.6581341028213501\n",
      "cnt: 0 - valLoss: 0.6576661467552185 - trainLoss: 0.6581328511238098\n",
      "cnt: 0 - valLoss: 0.6576645374298096 - trainLoss: 0.6581315398216248\n",
      "cnt: 0 - valLoss: 0.6576627492904663 - trainLoss: 0.6581303477287292\n",
      "cnt: 0 - valLoss: 0.6576611399650574 - trainLoss: 0.6581291556358337\n",
      "cnt: 0 - valLoss: 0.6576594710350037 - trainLoss: 0.6581279635429382\n",
      "cnt: 0 - valLoss: 0.6576576828956604 - trainLoss: 0.6581266522407532\n",
      "cnt: 0 - valLoss: 0.6576560139656067 - trainLoss: 0.6581254601478577\n",
      "cnt: 0 - valLoss: 0.6576542854309082 - trainLoss: 0.6581242680549622\n",
      "cnt: 0 - valLoss: 0.6576526165008545 - trainLoss: 0.6581230163574219\n",
      "cnt: 0 - valLoss: 0.657650887966156 - trainLoss: 0.6581218242645264\n",
      "cnt: 0 - valLoss: 0.6576492190361023 - trainLoss: 0.6581205129623413\n",
      "cnt: 0 - valLoss: 0.6576474905014038 - trainLoss: 0.658119261264801\n",
      "cnt: 0 - valLoss: 0.6576457619667053 - trainLoss: 0.6581180691719055\n",
      "cnt: 0 - valLoss: 0.6576440930366516 - trainLoss: 0.6581168174743652\n",
      "cnt: 0 - valLoss: 0.6576423645019531 - trainLoss: 0.6581156253814697\n",
      "cnt: 0 - valLoss: 0.6576406359672546 - trainLoss: 0.6581143736839294\n",
      "cnt: 0 - valLoss: 0.6576389074325562 - trainLoss: 0.6581131219863892\n",
      "cnt: 0 - valLoss: 0.6576372385025024 - trainLoss: 0.6581118702888489\n",
      "cnt: 0 - valLoss: 0.657635509967804 - trainLoss: 0.6581106781959534\n",
      "cnt: 0 - valLoss: 0.6576338410377502 - trainLoss: 0.6581094861030579\n",
      "cnt: 0 - valLoss: 0.6576321125030518 - trainLoss: 0.6581082344055176\n",
      "cnt: 0 - valLoss: 0.6576303839683533 - trainLoss: 0.6581069231033325\n",
      "cnt: 0 - valLoss: 0.6576287150382996 - trainLoss: 0.658105731010437\n",
      "cnt: 0 - valLoss: 0.6576269865036011 - trainLoss: 0.6581045389175415\n",
      "cnt: 0 - valLoss: 0.6576253175735474 - trainLoss: 0.6581032276153564\n",
      "cnt: 0 - valLoss: 0.6576235294342041 - trainLoss: 0.6581019759178162\n",
      "cnt: 0 - valLoss: 0.6576218605041504 - trainLoss: 0.6581007838249207\n",
      "cnt: 0 - valLoss: 0.6576201915740967 - trainLoss: 0.6580995321273804\n",
      "cnt: 0 - valLoss: 0.6576184034347534 - trainLoss: 0.6580982804298401\n",
      "cnt: 0 - valLoss: 0.6576166749000549 - trainLoss: 0.6580970883369446\n",
      "cnt: 0 - valLoss: 0.6576150059700012 - trainLoss: 0.6580958366394043\n",
      "cnt: 0 - valLoss: 0.6576133370399475 - trainLoss: 0.658094584941864\n",
      "cnt: 0 - valLoss: 0.6576114892959595 - trainLoss: 0.658093273639679\n",
      "cnt: 0 - valLoss: 0.6576098203659058 - trainLoss: 0.6580920815467834\n",
      "cnt: 0 - valLoss: 0.6576080918312073 - trainLoss: 0.6580908298492432\n",
      "cnt: 0 - valLoss: 0.6576063632965088 - trainLoss: 0.6580896377563477\n",
      "cnt: 0 - valLoss: 0.6576045751571655 - trainLoss: 0.6580883860588074\n",
      "cnt: 0 - valLoss: 0.6576029062271118 - trainLoss: 0.6580871343612671\n",
      "cnt: 0 - valLoss: 0.6576012372970581 - trainLoss: 0.658085823059082\n",
      "cnt: 0 - valLoss: 0.6575995087623596 - trainLoss: 0.6580846309661865\n",
      "cnt: 0 - valLoss: 0.6575977802276611 - trainLoss: 0.6580833792686462\n",
      "cnt: 0 - valLoss: 0.6575960516929626 - trainLoss: 0.658082127571106\n",
      "cnt: 0 - valLoss: 0.6575943231582642 - trainLoss: 0.6580809354782104\n",
      "cnt: 0 - valLoss: 0.6575925946235657 - trainLoss: 0.6580796837806702\n",
      "cnt: 0 - valLoss: 0.6575908660888672 - trainLoss: 0.6580783724784851\n",
      "cnt: 0 - valLoss: 0.6575891375541687 - trainLoss: 0.6580771803855896\n",
      "cnt: 0 - valLoss: 0.657587468624115 - trainLoss: 0.6580759286880493\n",
      "cnt: 0 - valLoss: 0.6575856804847717 - trainLoss: 0.6580747365951538\n",
      "cnt: 0 - valLoss: 0.657584011554718 - trainLoss: 0.6580734252929688\n",
      "cnt: 0 - valLoss: 0.6575822234153748 - trainLoss: 0.6580722332000732\n",
      "cnt: 0 - valLoss: 0.6575804948806763 - trainLoss: 0.658070981502533\n",
      "cnt: 0 - valLoss: 0.6575788259506226 - trainLoss: 0.6580697298049927\n",
      "cnt: 0 - valLoss: 0.6575770378112793 - trainLoss: 0.6580683588981628\n",
      "cnt: 0 - valLoss: 0.6575753092765808 - trainLoss: 0.6580672264099121\n",
      "cnt: 0 - valLoss: 0.6575736403465271 - trainLoss: 0.6580659747123718\n",
      "cnt: 0 - valLoss: 0.6575719118118286 - trainLoss: 0.6580647230148315\n",
      "cnt: 0 - valLoss: 0.6575701236724854 - trainLoss: 0.6580634713172913\n",
      "cnt: 0 - valLoss: 0.6575683951377869 - trainLoss: 0.6580621600151062\n",
      "cnt: 0 - valLoss: 0.6575666069984436 - trainLoss: 0.6580609083175659\n",
      "cnt: 0 - valLoss: 0.6575649380683899 - trainLoss: 0.6580596566200256\n",
      "cnt: 0 - valLoss: 0.6575631499290466 - trainLoss: 0.6580584049224854\n",
      "cnt: 0 - valLoss: 0.6575614213943481 - trainLoss: 0.6580571532249451\n",
      "cnt: 0 - valLoss: 0.6575596928596497 - trainLoss: 0.6580559015274048\n",
      "cnt: 0 - valLoss: 0.6575579047203064 - trainLoss: 0.6580546498298645\n",
      "cnt: 0 - valLoss: 0.6575561761856079 - trainLoss: 0.6580533981323242\n",
      "cnt: 0 - valLoss: 0.6575544476509094 - trainLoss: 0.6580521464347839\n",
      "cnt: 0 - valLoss: 0.6575526595115662 - trainLoss: 0.6580508947372437\n",
      "cnt: 0 - valLoss: 0.6575509309768677 - trainLoss: 0.6580496430397034\n",
      "cnt: 0 - valLoss: 0.6575492024421692 - trainLoss: 0.6580483317375183\n",
      "cnt: 0 - valLoss: 0.6575474739074707 - trainLoss: 0.6580471396446228\n",
      "cnt: 0 - valLoss: 0.6575456857681274 - trainLoss: 0.6580458879470825\n",
      "cnt: 0 - valLoss: 0.657543957233429 - trainLoss: 0.658044695854187\n",
      "cnt: 0 - valLoss: 0.6575421690940857 - trainLoss: 0.6580433249473572\n",
      "cnt: 0 - valLoss: 0.6575404405593872 - trainLoss: 0.6580421328544617\n",
      "cnt: 0 - valLoss: 0.6575387120246887 - trainLoss: 0.6580408215522766\n",
      "cnt: 0 - valLoss: 0.6575369238853455 - trainLoss: 0.6580395698547363\n",
      "cnt: 0 - valLoss: 0.657535195350647 - trainLoss: 0.658038318157196\n",
      "cnt: 0 - valLoss: 0.6575334072113037 - trainLoss: 0.6580370664596558\n",
      "cnt: 0 - valLoss: 0.6575316786766052 - trainLoss: 0.6580357551574707\n",
      "cnt: 0 - valLoss: 0.657529890537262 - trainLoss: 0.6580345034599304\n",
      "cnt: 0 - valLoss: 0.6575281620025635 - trainLoss: 0.6580333113670349\n",
      "cnt: 0 - valLoss: 0.657526433467865 - trainLoss: 0.6580320000648499\n",
      "cnt: 0 - valLoss: 0.6575246453285217 - trainLoss: 0.6580307483673096\n",
      "cnt: 0 - valLoss: 0.6575228571891785 - trainLoss: 0.6580294966697693\n",
      "cnt: 0 - valLoss: 0.65752112865448 - trainLoss: 0.658028244972229\n",
      "cnt: 0 - valLoss: 0.6575194001197815 - trainLoss: 0.658026933670044\n",
      "cnt: 0 - valLoss: 0.657517671585083 - trainLoss: 0.6580256819725037\n",
      "cnt: 0 - valLoss: 0.6575159430503845 - trainLoss: 0.6580244898796082\n",
      "cnt: 0 - valLoss: 0.657514214515686 - trainLoss: 0.6580232381820679\n",
      "cnt: 0 - valLoss: 0.6575124263763428 - trainLoss: 0.6580220460891724\n",
      "cnt: 0 - valLoss: 0.6575106978416443 - trainLoss: 0.6580207943916321\n",
      "cnt: 0 - valLoss: 0.6575089693069458 - trainLoss: 0.6580195426940918\n",
      "cnt: 0 - valLoss: 0.6575072407722473 - trainLoss: 0.6580183506011963\n",
      "cnt: 0 - valLoss: 0.6575055122375488 - trainLoss: 0.658017098903656\n",
      "cnt: 0 - valLoss: 0.6575037837028503 - trainLoss: 0.6580159068107605\n",
      "cnt: 0 - valLoss: 0.6575020551681519 - trainLoss: 0.6580146551132202\n",
      "cnt: 0 - valLoss: 0.6575003862380981 - trainLoss: 0.6580134034156799\n",
      "cnt: 0 - valLoss: 0.6574985384941101 - trainLoss: 0.6580122113227844\n",
      "cnt: 0 - valLoss: 0.6574968099594116 - trainLoss: 0.6580109596252441\n",
      "cnt: 0 - valLoss: 0.6574950218200684 - trainLoss: 0.6580097675323486\n",
      "cnt: 0 - valLoss: 0.6574933528900146 - trainLoss: 0.6580084562301636\n",
      "cnt: 0 - valLoss: 0.6574915647506714 - trainLoss: 0.6580072641372681\n",
      "cnt: 0 - valLoss: 0.6574898362159729 - trainLoss: 0.6580060720443726\n",
      "cnt: 0 - valLoss: 0.6574881076812744 - trainLoss: 0.658004879951477\n",
      "cnt: 0 - valLoss: 0.6574863195419312 - trainLoss: 0.658003568649292\n",
      "cnt: 0 - valLoss: 0.6574845910072327 - trainLoss: 0.6580023169517517\n",
      "cnt: 0 - valLoss: 0.6574828624725342 - trainLoss: 0.6580011248588562\n",
      "cnt: 0 - valLoss: 0.6574810743331909 - trainLoss: 0.6579998731613159\n",
      "cnt: 0 - valLoss: 0.6574793457984924 - trainLoss: 0.6579986810684204\n",
      "cnt: 0 - valLoss: 0.657477617263794 - trainLoss: 0.6579974293708801\n",
      "cnt: 0 - valLoss: 0.6574758887290955 - trainLoss: 0.6579961776733398\n",
      "cnt: 0 - valLoss: 0.6574741005897522 - trainLoss: 0.6579949259757996\n",
      "cnt: 0 - valLoss: 0.6574723720550537 - trainLoss: 0.657993733882904\n",
      "cnt: 0 - valLoss: 0.6574706435203552 - trainLoss: 0.6579924821853638\n",
      "cnt: 0 - valLoss: 0.6574689149856567 - trainLoss: 0.6579912304878235\n",
      "cnt: 0 - valLoss: 0.6574671268463135 - trainLoss: 0.6579899787902832\n",
      "cnt: 0 - valLoss: 0.657465398311615 - trainLoss: 0.6579887270927429\n",
      "cnt: 0 - valLoss: 0.6574636101722717 - trainLoss: 0.6579875946044922\n",
      "cnt: 0 - valLoss: 0.657461941242218 - trainLoss: 0.6579862833023071\n",
      "cnt: 0 - valLoss: 0.65746009349823 - trainLoss: 0.6579850912094116\n",
      "cnt: 0 - valLoss: 0.6574583649635315 - trainLoss: 0.6579838395118713\n",
      "cnt: 0 - valLoss: 0.6574565768241882 - trainLoss: 0.657982587814331\n",
      "cnt: 0 - valLoss: 0.6574549078941345 - trainLoss: 0.6579813957214355\n",
      "cnt: 0 - valLoss: 0.6574531197547913 - trainLoss: 0.6579800844192505\n",
      "cnt: 0 - valLoss: 0.6574513912200928 - trainLoss: 0.6579788327217102\n",
      "cnt: 0 - valLoss: 0.6574495434761047 - trainLoss: 0.6579776406288147\n",
      "cnt: 0 - valLoss: 0.6574478149414062 - trainLoss: 0.6579763889312744\n",
      "cnt: 0 - valLoss: 0.6574460864067078 - trainLoss: 0.6579751372337341\n",
      "cnt: 0 - valLoss: 0.6574442982673645 - trainLoss: 0.6579738855361938\n",
      "cnt: 0 - valLoss: 0.657442569732666 - trainLoss: 0.6579726338386536\n",
      "cnt: 0 - valLoss: 0.6574407815933228 - trainLoss: 0.6579713821411133\n",
      "cnt: 0 - valLoss: 0.6574390530586243 - trainLoss: 0.6579701900482178\n",
      "cnt: 0 - valLoss: 0.657437264919281 - trainLoss: 0.6579689383506775\n",
      "cnt: 0 - valLoss: 0.6574354767799377 - trainLoss: 0.6579676866531372\n",
      "cnt: 0 - valLoss: 0.6574337482452393 - trainLoss: 0.6579664349555969\n",
      "cnt: 0 - valLoss: 0.6574320197105408 - trainLoss: 0.6579652428627014\n",
      "cnt: 0 - valLoss: 0.6574302315711975 - trainLoss: 0.6579639911651611\n",
      "cnt: 0 - valLoss: 0.6574284434318542 - trainLoss: 0.6579626798629761\n",
      "cnt: 0 - valLoss: 0.657426655292511 - trainLoss: 0.6579614877700806\n",
      "cnt: 0 - valLoss: 0.6574249863624573 - trainLoss: 0.6579602360725403\n",
      "cnt: 0 - valLoss: 0.6574231386184692 - trainLoss: 0.657958984375\n",
      "cnt: 0 - valLoss: 0.6574214696884155 - trainLoss: 0.6579577922821045\n",
      "cnt: 0 - valLoss: 0.6574196219444275 - trainLoss: 0.6579564809799194\n",
      "cnt: 0 - valLoss: 0.6574178338050842 - trainLoss: 0.6579552888870239\n",
      "cnt: 0 - valLoss: 0.6574161052703857 - trainLoss: 0.6579540371894836\n",
      "cnt: 0 - valLoss: 0.6574143171310425 - trainLoss: 0.6579527258872986\n",
      "cnt: 0 - valLoss: 0.657412588596344 - trainLoss: 0.6579515337944031\n",
      "cnt: 0 - valLoss: 0.6574108004570007 - trainLoss: 0.6579502820968628\n",
      "cnt: 0 - valLoss: 0.6574089527130127 - trainLoss: 0.6579490303993225\n",
      "cnt: 0 - valLoss: 0.657407283782959 - trainLoss: 0.6579477787017822\n",
      "cnt: 0 - valLoss: 0.6574054956436157 - trainLoss: 0.6579465270042419\n",
      "cnt: 0 - valLoss: 0.6574037075042725 - trainLoss: 0.6579452753067017\n",
      "cnt: 0 - valLoss: 0.657401978969574 - trainLoss: 0.6579440236091614\n",
      "cnt: 0 - valLoss: 0.6574001312255859 - trainLoss: 0.6579427123069763\n",
      "cnt: 0 - valLoss: 0.6573984622955322 - trainLoss: 0.6579415202140808\n",
      "cnt: 0 - valLoss: 0.6573966145515442 - trainLoss: 0.6579402685165405\n",
      "cnt: 0 - valLoss: 0.6573949456214905 - trainLoss: 0.6579390168190002\n",
      "cnt: 0 - valLoss: 0.6573930978775024 - trainLoss: 0.65793776512146\n",
      "cnt: 0 - valLoss: 0.6573913097381592 - trainLoss: 0.6579364538192749\n",
      "cnt: 0 - valLoss: 0.6573895812034607 - trainLoss: 0.6579352617263794\n",
      "cnt: 0 - valLoss: 0.6573877930641174 - trainLoss: 0.6579340100288391\n",
      "cnt: 0 - valLoss: 0.6573860049247742 - trainLoss: 0.6579328179359436\n",
      "cnt: 0 - valLoss: 0.6573842167854309 - trainLoss: 0.6579315066337585\n",
      "cnt: 0 - valLoss: 0.6573824286460876 - trainLoss: 0.6579302549362183\n",
      "cnt: 0 - valLoss: 0.6573806405067444 - trainLoss: 0.657929003238678\n",
      "cnt: 0 - valLoss: 0.6573789119720459 - trainLoss: 0.6579277515411377\n",
      "cnt: 0 - valLoss: 0.6573771238327026 - trainLoss: 0.6579265594482422\n",
      "cnt: 0 - valLoss: 0.6573753952980042 - trainLoss: 0.6579253077507019\n",
      "cnt: 0 - valLoss: 0.6573736071586609 - trainLoss: 0.6579239964485168\n",
      "cnt: 0 - valLoss: 0.6573717594146729 - trainLoss: 0.6579227447509766\n",
      "cnt: 0 - valLoss: 0.6573699712753296 - trainLoss: 0.657921552658081\n",
      "cnt: 0 - valLoss: 0.6573681831359863 - trainLoss: 0.6579203009605408\n",
      "cnt: 0 - valLoss: 0.6573663949966431 - trainLoss: 0.6579189896583557\n",
      "cnt: 0 - valLoss: 0.6573646068572998 - trainLoss: 0.6579177975654602\n",
      "cnt: 0 - valLoss: 0.6573628783226013 - trainLoss: 0.6579164862632751\n",
      "cnt: 0 - valLoss: 0.6573610901832581 - trainLoss: 0.6579152941703796\n",
      "cnt: 0 - valLoss: 0.6573593616485596 - trainLoss: 0.6579140424728394\n",
      "cnt: 0 - valLoss: 0.6573575735092163 - trainLoss: 0.6579127907752991\n",
      "cnt: 0 - valLoss: 0.657355785369873 - trainLoss: 0.657911479473114\n",
      "cnt: 0 - valLoss: 0.6573539972305298 - trainLoss: 0.6579102873802185\n",
      "cnt: 0 - valLoss: 0.6573522090911865 - trainLoss: 0.6579089760780334\n",
      "cnt: 0 - valLoss: 0.6573504209518433 - trainLoss: 0.6579077243804932\n",
      "cnt: 0 - valLoss: 0.6573486328125 - trainLoss: 0.6579064726829529\n",
      "cnt: 0 - valLoss: 0.6573469042778015 - trainLoss: 0.6579052805900574\n",
      "cnt: 0 - valLoss: 0.6573450565338135 - trainLoss: 0.6579039692878723\n",
      "cnt: 0 - valLoss: 0.657343327999115 - trainLoss: 0.657902717590332\n",
      "cnt: 0 - valLoss: 0.6573415398597717 - trainLoss: 0.6579014658927917\n",
      "cnt: 0 - valLoss: 0.6573396921157837 - trainLoss: 0.6579001545906067\n",
      "cnt: 0 - valLoss: 0.6573379039764404 - trainLoss: 0.6578989624977112\n",
      "cnt: 0 - valLoss: 0.6573361754417419 - trainLoss: 0.6578977108001709\n",
      "cnt: 0 - valLoss: 0.6573343873023987 - trainLoss: 0.6578965187072754\n",
      "cnt: 0 - valLoss: 0.6573325395584106 - trainLoss: 0.6578952074050903\n",
      "cnt: 0 - valLoss: 0.6573307514190674 - trainLoss: 0.6578938961029053\n",
      "cnt: 0 - valLoss: 0.6573290228843689 - trainLoss: 0.6578927040100098\n",
      "cnt: 0 - valLoss: 0.6573271751403809 - trainLoss: 0.6578914523124695\n",
      "cnt: 0 - valLoss: 0.6573254466056824 - trainLoss: 0.6578901410102844\n",
      "cnt: 0 - valLoss: 0.6573235988616943 - trainLoss: 0.6578888893127441\n",
      "cnt: 0 - valLoss: 0.6573218107223511 - trainLoss: 0.6578876376152039\n",
      "cnt: 0 - valLoss: 0.6573200225830078 - trainLoss: 0.6578863859176636\n",
      "cnt: 0 - valLoss: 0.6573181748390198 - trainLoss: 0.6578851342201233\n",
      "cnt: 0 - valLoss: 0.6573164463043213 - trainLoss: 0.6578838229179382\n",
      "cnt: 0 - valLoss: 0.657314658164978 - trainLoss: 0.657882571220398\n",
      "cnt: 0 - valLoss: 0.6573128700256348 - trainLoss: 0.6578813195228577\n",
      "cnt: 0 - valLoss: 0.6573110222816467 - trainLoss: 0.6578800678253174\n",
      "cnt: 0 - valLoss: 0.6573092937469482 - trainLoss: 0.6578787565231323\n",
      "cnt: 0 - valLoss: 0.6573073863983154 - trainLoss: 0.657877504825592\n",
      "cnt: 0 - valLoss: 0.6573056578636169 - trainLoss: 0.6578763127326965\n",
      "cnt: 0 - valLoss: 0.6573038697242737 - trainLoss: 0.6578750014305115\n",
      "cnt: 0 - valLoss: 0.6573020219802856 - trainLoss: 0.6578737497329712\n",
      "cnt: 0 - valLoss: 0.6573002338409424 - trainLoss: 0.6578724980354309\n",
      "cnt: 0 - valLoss: 0.6572983860969543 - trainLoss: 0.6578712463378906\n",
      "cnt: 0 - valLoss: 0.6572966575622559 - trainLoss: 0.6578699350357056\n",
      "cnt: 0 - valLoss: 0.6572948694229126 - trainLoss: 0.6578686833381653\n",
      "cnt: 0 - valLoss: 0.6572930216789246 - trainLoss: 0.6578673720359802\n",
      "cnt: 0 - valLoss: 0.6572912335395813 - trainLoss: 0.6578661203384399\n",
      "cnt: 0 - valLoss: 0.6572895050048828 - trainLoss: 0.6578649282455444\n",
      "cnt: 0 - valLoss: 0.6572876572608948 - trainLoss: 0.6578636169433594\n",
      "cnt: 0 - valLoss: 0.6572858691215515 - trainLoss: 0.6578623652458191\n",
      "cnt: 0 - valLoss: 0.6572840213775635 - trainLoss: 0.6578611135482788\n",
      "cnt: 0 - valLoss: 0.6572822332382202 - trainLoss: 0.6578598618507385\n",
      "cnt: 0 - valLoss: 0.657280445098877 - trainLoss: 0.6578585505485535\n",
      "cnt: 0 - valLoss: 0.6572785973548889 - trainLoss: 0.6578572988510132\n",
      "cnt: 0 - valLoss: 0.6572768688201904 - trainLoss: 0.6578559875488281\n",
      "cnt: 0 - valLoss: 0.6572750210762024 - trainLoss: 0.6578547954559326\n",
      "cnt: 0 - valLoss: 0.6572731733322144 - trainLoss: 0.6578535437583923\n",
      "cnt: 0 - valLoss: 0.6572713255882263 - trainLoss: 0.6578521728515625\n",
      "cnt: 0 - valLoss: 0.6572695970535278 - trainLoss: 0.657850980758667\n",
      "cnt: 0 - valLoss: 0.6572678089141846 - trainLoss: 0.6578496694564819\n",
      "cnt: 0 - valLoss: 0.6572659611701965 - trainLoss: 0.6578483581542969\n",
      "cnt: 0 - valLoss: 0.6572641730308533 - trainLoss: 0.6578471660614014\n",
      "cnt: 0 - valLoss: 0.6572623252868652 - trainLoss: 0.6578457951545715\n",
      "cnt: 0 - valLoss: 0.657260537147522 - trainLoss: 0.657844603061676\n",
      "cnt: 0 - valLoss: 0.6572586894035339 - trainLoss: 0.657843291759491\n",
      "cnt: 0 - valLoss: 0.6572568416595459 - trainLoss: 0.6578419804573059\n",
      "cnt: 0 - valLoss: 0.6572551131248474 - trainLoss: 0.6578407883644104\n",
      "cnt: 0 - valLoss: 0.6572532653808594 - trainLoss: 0.6578394174575806\n",
      "cnt: 0 - valLoss: 0.6572515368461609 - trainLoss: 0.6578382253646851\n",
      "cnt: 0 - valLoss: 0.6572496891021729 - trainLoss: 0.6578369140625\n",
      "cnt: 0 - valLoss: 0.6572478413581848 - trainLoss: 0.6578356027603149\n",
      "cnt: 0 - valLoss: 0.6572459936141968 - trainLoss: 0.6578344106674194\n",
      "cnt: 0 - valLoss: 0.6572442054748535 - trainLoss: 0.6578330397605896\n",
      "cnt: 0 - valLoss: 0.6572423577308655 - trainLoss: 0.6578318476676941\n",
      "cnt: 0 - valLoss: 0.6572406888008118 - trainLoss: 0.657830536365509\n",
      "cnt: 0 - valLoss: 0.6572390198707581 - trainLoss: 0.6578292846679688\n",
      "cnt: 0 - valLoss: 0.6572372913360596 - trainLoss: 0.6578279137611389\n",
      "cnt: 0 - valLoss: 0.6572356820106506 - trainLoss: 0.6578267216682434\n",
      "cnt: 0 - valLoss: 0.6572340130805969 - trainLoss: 0.6578254103660583\n",
      "cnt: 0 - valLoss: 0.6572323441505432 - trainLoss: 0.6578241586685181\n",
      "cnt: 0 - valLoss: 0.6572306752204895 - trainLoss: 0.657822847366333\n",
      "cnt: 0 - valLoss: 0.6572290062904358 - trainLoss: 0.6578214764595032\n",
      "cnt: 0 - valLoss: 0.6572273373603821 - trainLoss: 0.6578202843666077\n",
      "cnt: 0 - valLoss: 0.6572256088256836 - trainLoss: 0.6578190326690674\n",
      "cnt: 0 - valLoss: 0.6572239398956299 - trainLoss: 0.6578177213668823\n",
      "cnt: 0 - valLoss: 0.6572222709655762 - trainLoss: 0.657816469669342\n",
      "cnt: 0 - valLoss: 0.6572206616401672 - trainLoss: 0.657815158367157\n",
      "cnt: 0 - valLoss: 0.6572189331054688 - trainLoss: 0.6578139066696167\n",
      "cnt: 0 - valLoss: 0.657217264175415 - trainLoss: 0.6578125953674316\n",
      "cnt: 0 - valLoss: 0.6572155952453613 - trainLoss: 0.6578113436698914\n",
      "cnt: 0 - valLoss: 0.6572139859199524 - trainLoss: 0.6578100919723511\n",
      "cnt: 0 - valLoss: 0.6572122573852539 - trainLoss: 0.6578087210655212\n",
      "cnt: 0 - valLoss: 0.6572105288505554 - trainLoss: 0.657807469367981\n",
      "cnt: 0 - valLoss: 0.6572088599205017 - trainLoss: 0.6578061580657959\n",
      "cnt: 0 - valLoss: 0.657207190990448 - trainLoss: 0.6578049659729004\n",
      "cnt: 0 - valLoss: 0.6572054624557495 - trainLoss: 0.6578035950660706\n",
      "cnt: 0 - valLoss: 0.6572037935256958 - trainLoss: 0.6578023433685303\n",
      "cnt: 0 - valLoss: 0.6572021245956421 - trainLoss: 0.6578010320663452\n",
      "cnt: 0 - valLoss: 0.6572003960609436 - trainLoss: 0.6577997803688049\n",
      "cnt: 0 - valLoss: 0.6571987271308899 - trainLoss: 0.6577984690666199\n",
      "cnt: 0 - valLoss: 0.6571970582008362 - trainLoss: 0.6577972173690796\n",
      "cnt: 0 - valLoss: 0.6571953296661377 - trainLoss: 0.6577959060668945\n",
      "cnt: 0 - valLoss: 0.657193660736084 - trainLoss: 0.6577946543693542\n",
      "cnt: 0 - valLoss: 0.6571919918060303 - trainLoss: 0.6577933430671692\n",
      "cnt: 0 - valLoss: 0.6571903228759766 - trainLoss: 0.6577920317649841\n",
      "cnt: 0 - valLoss: 0.6571886539459229 - trainLoss: 0.6577907204627991\n",
      "cnt: 0 - valLoss: 0.6571869850158691 - trainLoss: 0.6577894687652588\n",
      "cnt: 0 - valLoss: 0.6571853160858154 - trainLoss: 0.6577881574630737\n",
      "cnt: 0 - valLoss: 0.6571835875511169 - trainLoss: 0.6577869057655334\n",
      "cnt: 0 - valLoss: 0.6571817994117737 - trainLoss: 0.6577855944633484\n",
      "cnt: 0 - valLoss: 0.6571801900863647 - trainLoss: 0.6577842831611633\n",
      "cnt: 0 - valLoss: 0.657178521156311 - trainLoss: 0.657783031463623\n",
      "cnt: 0 - valLoss: 0.6571767926216125 - trainLoss: 0.657781720161438\n",
      "cnt: 0 - valLoss: 0.6571750640869141 - trainLoss: 0.6577804088592529\n",
      "cnt: 0 - valLoss: 0.6571733951568604 - trainLoss: 0.6577791571617126\n",
      "cnt: 0 - valLoss: 0.6571717262268066 - trainLoss: 0.6577778458595276\n",
      "cnt: 0 - valLoss: 0.6571700572967529 - trainLoss: 0.6577765345573425\n",
      "cnt: 0 - valLoss: 0.6571683287620544 - trainLoss: 0.6577752232551575\n",
      "cnt: 0 - valLoss: 0.6571666598320007 - trainLoss: 0.6577739119529724\n",
      "cnt: 0 - valLoss: 0.6571649312973022 - trainLoss: 0.6577726006507874\n",
      "cnt: 0 - valLoss: 0.6571632027626038 - trainLoss: 0.6577713489532471\n",
      "cnt: 0 - valLoss: 0.6571615934371948 - trainLoss: 0.657770037651062\n",
      "cnt: 0 - valLoss: 0.6571598052978516 - trainLoss: 0.6577687859535217\n",
      "cnt: 0 - valLoss: 0.6571581363677979 - trainLoss: 0.6577674746513367\n",
      "cnt: 0 - valLoss: 0.6571564674377441 - trainLoss: 0.6577661633491516\n",
      "cnt: 0 - valLoss: 0.6571547985076904 - trainLoss: 0.6577648520469666\n",
      "cnt: 0 - valLoss: 0.6571530699729919 - trainLoss: 0.6577635407447815\n",
      "cnt: 0 - valLoss: 0.6571514010429382 - trainLoss: 0.6577622890472412\n",
      "cnt: 0 - valLoss: 0.6571496725082397 - trainLoss: 0.6577609777450562\n",
      "cnt: 0 - valLoss: 0.6571479439735413 - trainLoss: 0.6577597260475159\n",
      "cnt: 0 - valLoss: 0.6571462154388428 - trainLoss: 0.657758355140686\n",
      "cnt: 0 - valLoss: 0.6571445465087891 - trainLoss: 0.657757043838501\n",
      "cnt: 0 - valLoss: 0.6571428775787354 - trainLoss: 0.6577557325363159\n",
      "cnt: 0 - valLoss: 0.6571411490440369 - trainLoss: 0.6577544808387756\n",
      "cnt: 0 - valLoss: 0.6571394205093384 - trainLoss: 0.6577531695365906\n",
      "cnt: 0 - valLoss: 0.6571377515792847 - trainLoss: 0.6577519178390503\n",
      "cnt: 0 - valLoss: 0.6571360230445862 - trainLoss: 0.6577505469322205\n",
      "cnt: 0 - valLoss: 0.6571342945098877 - trainLoss: 0.6577492356300354\n",
      "cnt: 0 - valLoss: 0.657132625579834 - trainLoss: 0.6577479243278503\n",
      "cnt: 0 - valLoss: 0.6571308970451355 - trainLoss: 0.6577465534210205\n",
      "cnt: 0 - valLoss: 0.657129168510437 - trainLoss: 0.6577453017234802\n",
      "cnt: 0 - valLoss: 0.6571274399757385 - trainLoss: 0.6577439904212952\n",
      "cnt: 0 - valLoss: 0.6571257710456848 - trainLoss: 0.6577426791191101\n",
      "cnt: 0 - valLoss: 0.6571241021156311 - trainLoss: 0.6577413082122803\n",
      "cnt: 0 - valLoss: 0.6571223735809326 - trainLoss: 0.65774005651474\n",
      "cnt: 0 - valLoss: 0.6571205854415894 - trainLoss: 0.6577387452125549\n",
      "cnt: 0 - valLoss: 0.6571189165115356 - trainLoss: 0.6577373743057251\n",
      "cnt: 0 - valLoss: 0.6571171879768372 - trainLoss: 0.6577361226081848\n",
      "cnt: 0 - valLoss: 0.6571154594421387 - trainLoss: 0.6577348113059998\n",
      "cnt: 0 - valLoss: 0.6571137309074402 - trainLoss: 0.6577334403991699\n",
      "cnt: 0 - valLoss: 0.6571120619773865 - trainLoss: 0.6577321290969849\n",
      "cnt: 0 - valLoss: 0.657110333442688 - trainLoss: 0.657730758190155\n",
      "cnt: 0 - valLoss: 0.6571085453033447 - trainLoss: 0.6577295064926147\n",
      "cnt: 0 - valLoss: 0.657106876373291 - trainLoss: 0.6577281951904297\n",
      "cnt: 0 - valLoss: 0.6571052074432373 - trainLoss: 0.6577268242835999\n",
      "cnt: 0 - valLoss: 0.657103419303894 - trainLoss: 0.6577255725860596\n",
      "cnt: 0 - valLoss: 0.6571016907691956 - trainLoss: 0.6577242016792297\n",
      "cnt: 0 - valLoss: 0.6570999622344971 - trainLoss: 0.6577228903770447\n",
      "cnt: 0 - valLoss: 0.6570982336997986 - trainLoss: 0.6577215790748596\n",
      "cnt: 0 - valLoss: 0.6570965051651001 - trainLoss: 0.6577202081680298\n",
      "cnt: 0 - valLoss: 0.6570948958396912 - trainLoss: 0.6577189564704895\n",
      "cnt: 0 - valLoss: 0.6570931077003479 - trainLoss: 0.6577176451683044\n",
      "cnt: 0 - valLoss: 0.6570913791656494 - trainLoss: 0.6577162742614746\n",
      "cnt: 0 - valLoss: 0.6570896506309509 - trainLoss: 0.6577149629592896\n",
      "cnt: 0 - valLoss: 0.6570879220962524 - trainLoss: 0.6577136516571045\n",
      "cnt: 0 - valLoss: 0.657086193561554 - trainLoss: 0.6577123403549194\n",
      "cnt: 0 - valLoss: 0.6570844054222107 - trainLoss: 0.6577109694480896\n",
      "cnt: 0 - valLoss: 0.6570827960968018 - trainLoss: 0.6577095985412598\n",
      "cnt: 0 - valLoss: 0.6570810079574585 - trainLoss: 0.6577083468437195\n",
      "cnt: 0 - valLoss: 0.6570792198181152 - trainLoss: 0.6577070355415344\n",
      "cnt: 0 - valLoss: 0.6570775508880615 - trainLoss: 0.6577056646347046\n",
      "cnt: 0 - valLoss: 0.657075822353363 - trainLoss: 0.6577043533325195\n",
      "cnt: 0 - valLoss: 0.6570740342140198 - trainLoss: 0.6577030420303345\n",
      "cnt: 0 - valLoss: 0.6570723652839661 - trainLoss: 0.6577017307281494\n",
      "cnt: 0 - valLoss: 0.6570706367492676 - trainLoss: 0.6577003598213196\n",
      "cnt: 0 - valLoss: 0.6570688486099243 - trainLoss: 0.6576990485191345\n",
      "cnt: 0 - valLoss: 0.6570671796798706 - trainLoss: 0.6576977372169495\n",
      "cnt: 0 - valLoss: 0.6570653915405273 - trainLoss: 0.6576964259147644\n",
      "cnt: 0 - valLoss: 0.6570636630058289 - trainLoss: 0.6576950550079346\n",
      "cnt: 0 - valLoss: 0.6570619940757751 - trainLoss: 0.6576937437057495\n",
      "cnt: 0 - valLoss: 0.6570602655410767 - trainLoss: 0.6576924324035645\n",
      "cnt: 0 - valLoss: 0.6570585370063782 - trainLoss: 0.6576911211013794\n",
      "cnt: 0 - valLoss: 0.6570568084716797 - trainLoss: 0.6576897501945496\n",
      "cnt: 0 - valLoss: 0.6570550203323364 - trainLoss: 0.6576883792877197\n",
      "cnt: 0 - valLoss: 0.6570532917976379 - trainLoss: 0.6576870083808899\n",
      "cnt: 0 - valLoss: 0.6570515036582947 - trainLoss: 0.6576857566833496\n",
      "cnt: 0 - valLoss: 0.6570497751235962 - trainLoss: 0.6576843857765198\n",
      "cnt: 0 - valLoss: 0.6570479869842529 - trainLoss: 0.6576830148696899\n",
      "cnt: 0 - valLoss: 0.6570462584495544 - trainLoss: 0.6576815247535706\n",
      "cnt: 0 - valLoss: 0.6570444703102112 - trainLoss: 0.6576802730560303\n",
      "cnt: 0 - valLoss: 0.6570426821708679 - trainLoss: 0.6576789021492004\n",
      "cnt: 0 - valLoss: 0.6570409536361694 - trainLoss: 0.6576775312423706\n",
      "cnt: 0 - valLoss: 0.6570391654968262 - trainLoss: 0.6576761603355408\n",
      "cnt: 0 - valLoss: 0.6570374369621277 - trainLoss: 0.6576747894287109\n",
      "cnt: 0 - valLoss: 0.6570357084274292 - trainLoss: 0.6576734185218811\n",
      "cnt: 0 - valLoss: 0.6570339202880859 - trainLoss: 0.657672107219696\n",
      "cnt: 0 - valLoss: 0.6570321321487427 - trainLoss: 0.6576707363128662\n",
      "cnt: 0 - valLoss: 0.6570304036140442 - trainLoss: 0.6576693654060364\n",
      "cnt: 0 - valLoss: 0.6570286750793457 - trainLoss: 0.6576679944992065\n",
      "cnt: 0 - valLoss: 0.6570268869400024 - trainLoss: 0.6576666235923767\n",
      "cnt: 0 - valLoss: 0.6570250988006592 - trainLoss: 0.6576652526855469\n",
      "cnt: 0 - valLoss: 0.6570233702659607 - trainLoss: 0.657663881778717\n",
      "cnt: 0 - valLoss: 0.6570215821266174 - trainLoss: 0.6576625108718872\n",
      "cnt: 0 - valLoss: 0.6570197939872742 - trainLoss: 0.6576611399650574\n",
      "cnt: 0 - valLoss: 0.6570180654525757 - trainLoss: 0.6576597690582275\n",
      "cnt: 0 - valLoss: 0.6570162773132324 - trainLoss: 0.6576583981513977\n",
      "cnt: 0 - valLoss: 0.6570144891738892 - trainLoss: 0.6576570868492126\n",
      "cnt: 0 - valLoss: 0.6570127606391907 - trainLoss: 0.657655656337738\n",
      "cnt: 0 - valLoss: 0.6570109724998474 - trainLoss: 0.6576542854309082\n",
      "cnt: 0 - valLoss: 0.6570091843605042 - trainLoss: 0.6576529741287231\n",
      "cnt: 0 - valLoss: 0.6570075154304504 - trainLoss: 0.6576516032218933\n",
      "cnt: 0 - valLoss: 0.6570057272911072 - trainLoss: 0.6576502323150635\n",
      "cnt: 0 - valLoss: 0.6570039987564087 - trainLoss: 0.6576489210128784\n",
      "cnt: 0 - valLoss: 0.657002329826355 - trainLoss: 0.6576475501060486\n",
      "cnt: 0 - valLoss: 0.6570005416870117 - trainLoss: 0.6576462388038635\n",
      "cnt: 0 - valLoss: 0.6569988131523132 - trainLoss: 0.6576449275016785\n",
      "cnt: 0 - valLoss: 0.6569971442222595 - trainLoss: 0.6576435565948486\n",
      "cnt: 0 - valLoss: 0.6569952964782715 - trainLoss: 0.6576421856880188\n",
      "cnt: 0 - valLoss: 0.656993567943573 - trainLoss: 0.657640814781189\n",
      "cnt: 0 - valLoss: 0.6569917798042297 - trainLoss: 0.6576394438743591\n",
      "cnt: 0 - valLoss: 0.656990110874176 - trainLoss: 0.6576381325721741\n",
      "cnt: 0 - valLoss: 0.6569883227348328 - trainLoss: 0.6576367616653442\n",
      "cnt: 0 - valLoss: 0.6569865942001343 - trainLoss: 0.657635509967804\n",
      "cnt: 0 - valLoss: 0.6569849252700806 - trainLoss: 0.6576340794563293\n",
      "cnt: 0 - valLoss: 0.6569832563400269 - trainLoss: 0.6576327681541443\n",
      "cnt: 0 - valLoss: 0.6569816470146179 - trainLoss: 0.6576313972473145\n",
      "cnt: 0 - valLoss: 0.6569799184799194 - trainLoss: 0.6576300859451294\n",
      "cnt: 0 - valLoss: 0.6569782495498657 - trainLoss: 0.6576287150382996\n",
      "cnt: 0 - valLoss: 0.6569766402244568 - trainLoss: 0.6576274633407593\n",
      "cnt: 0 - valLoss: 0.6569749712944031 - trainLoss: 0.6576260924339294\n",
      "cnt: 0 - valLoss: 0.6569733619689941 - trainLoss: 0.6576247215270996\n",
      "cnt: 0 - valLoss: 0.6569716930389404 - trainLoss: 0.6576234698295593\n",
      "cnt: 0 - valLoss: 0.6569700241088867 - trainLoss: 0.6576221585273743\n",
      "cnt: 0 - valLoss: 0.6569684743881226 - trainLoss: 0.6576207876205444\n",
      "cnt: 0 - valLoss: 0.6569669246673584 - trainLoss: 0.6576194763183594\n",
      "cnt: 0 - valLoss: 0.6569652557373047 - trainLoss: 0.6576182246208191\n",
      "cnt: 0 - valLoss: 0.6569637060165405 - trainLoss: 0.657616913318634\n",
      "cnt: 0 - valLoss: 0.6569620966911316 - trainLoss: 0.6576155424118042\n",
      "cnt: 0 - valLoss: 0.6569605469703674 - trainLoss: 0.6576141715049744\n",
      "cnt: 0 - valLoss: 0.6569589972496033 - trainLoss: 0.6576129794120789\n",
      "cnt: 0 - valLoss: 0.6569573283195496 - trainLoss: 0.657611608505249\n",
      "cnt: 0 - valLoss: 0.6569557785987854 - trainLoss: 0.657610297203064\n",
      "cnt: 0 - valLoss: 0.6569542288780212 - trainLoss: 0.6576089262962341\n",
      "cnt: 0 - valLoss: 0.6569526195526123 - trainLoss: 0.6576076149940491\n",
      "cnt: 0 - valLoss: 0.6569510698318481 - trainLoss: 0.657606303691864\n",
      "cnt: 0 - valLoss: 0.6569494009017944 - trainLoss: 0.657604992389679\n",
      "cnt: 0 - valLoss: 0.6569478511810303 - trainLoss: 0.6576036810874939\n",
      "cnt: 0 - valLoss: 0.6569463014602661 - trainLoss: 0.6576023697853088\n",
      "cnt: 0 - valLoss: 0.6569446921348572 - trainLoss: 0.657600998878479\n",
      "cnt: 0 - valLoss: 0.6569430828094482 - trainLoss: 0.657599687576294\n",
      "cnt: 0 - valLoss: 0.6569415330886841 - trainLoss: 0.6575983166694641\n",
      "cnt: 0 - valLoss: 0.6569399237632751 - trainLoss: 0.6575970649719238\n",
      "cnt: 0 - valLoss: 0.656938374042511 - trainLoss: 0.6575957536697388\n",
      "cnt: 0 - valLoss: 0.6569367051124573 - trainLoss: 0.6575943827629089\n",
      "cnt: 0 - valLoss: 0.6569350957870483 - trainLoss: 0.6575930118560791\n",
      "cnt: 0 - valLoss: 0.6569335460662842 - trainLoss: 0.6575916409492493\n",
      "cnt: 0 - valLoss: 0.6569319367408752 - trainLoss: 0.6575904488563538\n",
      "cnt: 0 - valLoss: 0.6569303870201111 - trainLoss: 0.6575890779495239\n",
      "cnt: 0 - valLoss: 0.6569286584854126 - trainLoss: 0.6575877070426941\n",
      "cnt: 0 - valLoss: 0.6569271683692932 - trainLoss: 0.657586395740509\n",
      "cnt: 0 - valLoss: 0.6569254994392395 - trainLoss: 0.657585084438324\n",
      "cnt: 0 - valLoss: 0.6569238901138306 - trainLoss: 0.6575837135314941\n",
      "cnt: 0 - valLoss: 0.6569223403930664 - trainLoss: 0.6575824022293091\n",
      "cnt: 0 - valLoss: 0.6569206714630127 - trainLoss: 0.6575810313224792\n",
      "cnt: 0 - valLoss: 0.6569191217422485 - trainLoss: 0.6575796604156494\n",
      "cnt: 0 - valLoss: 0.6569175124168396 - trainLoss: 0.6575784087181091\n",
      "cnt: 0 - valLoss: 0.6569158434867859 - trainLoss: 0.6575770378112793\n",
      "cnt: 0 - valLoss: 0.6569142937660217 - trainLoss: 0.6575756669044495\n",
      "cnt: 0 - valLoss: 0.6569127440452576 - trainLoss: 0.6575743556022644\n",
      "cnt: 0 - valLoss: 0.6569111347198486 - trainLoss: 0.6575731039047241\n",
      "cnt: 0 - valLoss: 0.6569095253944397 - trainLoss: 0.6575717329978943\n",
      "cnt: 0 - valLoss: 0.656907856464386 - trainLoss: 0.6575703620910645\n",
      "cnt: 0 - valLoss: 0.6569063067436218 - trainLoss: 0.6575690507888794\n",
      "cnt: 0 - valLoss: 0.6569046378135681 - trainLoss: 0.6575676798820496\n",
      "cnt: 0 - valLoss: 0.6569031476974487 - trainLoss: 0.6575664281845093\n",
      "cnt: 0 - valLoss: 0.6569014191627502 - trainLoss: 0.6575650572776794\n",
      "cnt: 0 - valLoss: 0.6568998098373413 - trainLoss: 0.6575636863708496\n",
      "cnt: 0 - valLoss: 0.6568982601165771 - trainLoss: 0.6575623750686646\n",
      "cnt: 0 - valLoss: 0.6568965911865234 - trainLoss: 0.6575610041618347\n",
      "cnt: 0 - valLoss: 0.6568949818611145 - trainLoss: 0.6575596332550049\n",
      "cnt: 0 - valLoss: 0.6568933725357056 - trainLoss: 0.6575583815574646\n",
      "cnt: 0 - valLoss: 0.6568918228149414 - trainLoss: 0.6575570106506348\n",
      "cnt: 0 - valLoss: 0.6568901538848877 - trainLoss: 0.6575555801391602\n",
      "cnt: 0 - valLoss: 0.6568885445594788 - trainLoss: 0.6575542688369751\n",
      "cnt: 0 - valLoss: 0.6568869948387146 - trainLoss: 0.6575528979301453\n",
      "cnt: 0 - valLoss: 0.6568853259086609 - trainLoss: 0.6575515866279602\n",
      "cnt: 0 - valLoss: 0.6568837761878967 - trainLoss: 0.6575502753257751\n",
      "cnt: 0 - valLoss: 0.6568821668624878 - trainLoss: 0.6575489640235901\n",
      "cnt: 0 - valLoss: 0.6568805575370789 - trainLoss: 0.6575475931167603\n",
      "cnt: 0 - valLoss: 0.6568789482116699 - trainLoss: 0.6575462818145752\n",
      "cnt: 0 - valLoss: 0.656877338886261 - trainLoss: 0.6575449109077454\n",
      "cnt: 0 - valLoss: 0.6568756699562073 - trainLoss: 0.6575435996055603\n",
      "cnt: 0 - valLoss: 0.6568740606307983 - trainLoss: 0.6575422286987305\n",
      "cnt: 0 - valLoss: 0.6568724513053894 - trainLoss: 0.6575409173965454\n",
      "cnt: 0 - valLoss: 0.6568708419799805 - trainLoss: 0.6575395464897156\n",
      "cnt: 0 - valLoss: 0.6568692326545715 - trainLoss: 0.6575382351875305\n",
      "cnt: 0 - valLoss: 0.6568676233291626 - trainLoss: 0.6575368642807007\n",
      "cnt: 0 - valLoss: 0.6568658947944641 - trainLoss: 0.6575354933738708\n",
      "cnt: 0 - valLoss: 0.6568643450737 - trainLoss: 0.657534122467041\n",
      "cnt: 0 - valLoss: 0.656862735748291 - trainLoss: 0.657532811164856\n",
      "cnt: 0 - valLoss: 0.6568611264228821 - trainLoss: 0.6575314998626709\n",
      "cnt: 0 - valLoss: 0.6568594574928284 - trainLoss: 0.6575301885604858\n",
      "cnt: 0 - valLoss: 0.6568578481674194 - trainLoss: 0.6575288772583008\n",
      "cnt: 0 - valLoss: 0.6568562388420105 - trainLoss: 0.6575275659561157\n",
      "cnt: 0 - valLoss: 0.6568547487258911 - trainLoss: 0.6575261950492859\n",
      "cnt: 0 - valLoss: 0.6568530797958374 - trainLoss: 0.6575249433517456\n",
      "cnt: 0 - valLoss: 0.6568515300750732 - trainLoss: 0.6575236320495605\n",
      "cnt: 0 - valLoss: 0.6568499207496643 - trainLoss: 0.6575222611427307\n",
      "cnt: 0 - valLoss: 0.6568483114242554 - trainLoss: 0.6575210094451904\n",
      "cnt: 0 - valLoss: 0.6568467020988464 - trainLoss: 0.6575196385383606\n",
      "cnt: 0 - valLoss: 0.6568450927734375 - trainLoss: 0.6575183272361755\n",
      "cnt: 0 - valLoss: 0.6568434834480286 - trainLoss: 0.6575170755386353\n",
      "cnt: 0 - valLoss: 0.6568419337272644 - trainLoss: 0.6575157046318054\n",
      "cnt: 0 - valLoss: 0.6568402647972107 - trainLoss: 0.6575144529342651\n",
      "cnt: 0 - valLoss: 0.656838595867157 - trainLoss: 0.6575131416320801\n",
      "cnt: 0 - valLoss: 0.656836986541748 - trainLoss: 0.6575117707252502\n",
      "cnt: 0 - valLoss: 0.6568353772163391 - trainLoss: 0.6575103998184204\n",
      "cnt: 0 - valLoss: 0.656833827495575 - trainLoss: 0.6575092077255249\n",
      "cnt: 0 - valLoss: 0.656832218170166 - trainLoss: 0.6575078368186951\n",
      "cnt: 0 - valLoss: 0.6568306684494019 - trainLoss: 0.6575065851211548\n",
      "cnt: 0 - valLoss: 0.6568290591239929 - trainLoss: 0.6575053334236145\n",
      "cnt: 0 - valLoss: 0.6568275094032288 - trainLoss: 0.6575040221214294\n",
      "cnt: 0 - valLoss: 0.6568259596824646 - trainLoss: 0.6575027704238892\n",
      "cnt: 0 - valLoss: 0.6568243503570557 - trainLoss: 0.6575015187263489\n",
      "cnt: 0 - valLoss: 0.6568227410316467 - trainLoss: 0.657500147819519\n",
      "cnt: 0 - valLoss: 0.6568211913108826 - trainLoss: 0.6574988961219788\n",
      "cnt: 0 - valLoss: 0.6568195819854736 - trainLoss: 0.6574975848197937\n",
      "cnt: 0 - valLoss: 0.6568180322647095 - trainLoss: 0.6574962735176086\n",
      "cnt: 0 - valLoss: 0.6568164825439453 - trainLoss: 0.6574950218200684\n",
      "cnt: 0 - valLoss: 0.6568148732185364 - trainLoss: 0.6574937105178833\n",
      "cnt: 0 - valLoss: 0.6568132638931274 - trainLoss: 0.657492458820343\n",
      "cnt: 0 - valLoss: 0.6568116545677185 - trainLoss: 0.657491147518158\n",
      "cnt: 0 - valLoss: 0.6568100452423096 - trainLoss: 0.6574898958206177\n",
      "cnt: 0 - valLoss: 0.6568085551261902 - trainLoss: 0.6574885845184326\n",
      "cnt: 0 - valLoss: 0.6568069458007812 - trainLoss: 0.6574873328208923\n",
      "cnt: 0 - valLoss: 0.6568053364753723 - trainLoss: 0.657486081123352\n",
      "cnt: 0 - valLoss: 0.6568037867546082 - trainLoss: 0.657484769821167\n",
      "cnt: 0 - valLoss: 0.6568021178245544 - trainLoss: 0.6574833989143372\n",
      "cnt: 0 - valLoss: 0.6568005681037903 - trainLoss: 0.6574821472167969\n",
      "cnt: 0 - valLoss: 0.6567990183830261 - trainLoss: 0.657480776309967\n",
      "cnt: 0 - valLoss: 0.6567974090576172 - trainLoss: 0.6574795246124268\n",
      "cnt: 0 - valLoss: 0.656795859336853 - trainLoss: 0.6574782729148865\n",
      "cnt: 0 - valLoss: 0.6567942500114441 - trainLoss: 0.6574769616127014\n",
      "cnt: 0 - valLoss: 0.6567927002906799 - trainLoss: 0.6574756503105164\n",
      "cnt: 0 - valLoss: 0.6567911505699158 - trainLoss: 0.6574743986129761\n",
      "cnt: 0 - valLoss: 0.6567895412445068 - trainLoss: 0.657473087310791\n",
      "cnt: 0 - valLoss: 0.6567879319190979 - trainLoss: 0.657471776008606\n",
      "cnt: 0 - valLoss: 0.6567863821983337 - trainLoss: 0.6574704647064209\n",
      "cnt: 0 - valLoss: 0.6567848324775696 - trainLoss: 0.6574691534042358\n",
      "cnt: 0 - valLoss: 0.6567832231521606 - trainLoss: 0.6574678421020508\n",
      "cnt: 0 - valLoss: 0.6567816138267517 - trainLoss: 0.6574665904045105\n",
      "cnt: 0 - valLoss: 0.6567801237106323 - trainLoss: 0.6574653387069702\n",
      "cnt: 0 - valLoss: 0.6567785143852234 - trainLoss: 0.6574639678001404\n",
      "cnt: 0 - valLoss: 0.6567768454551697 - trainLoss: 0.6574626564979553\n",
      "cnt: 0 - valLoss: 0.6567752957344055 - trainLoss: 0.6574613451957703\n",
      "cnt: 0 - valLoss: 0.6567737460136414 - trainLoss: 0.65746009349823\n",
      "cnt: 0 - valLoss: 0.6567721366882324 - trainLoss: 0.6574587821960449\n",
      "cnt: 0 - valLoss: 0.6567705273628235 - trainLoss: 0.6574575304985046\n",
      "cnt: 0 - valLoss: 0.6567689776420593 - trainLoss: 0.6574561595916748\n",
      "cnt: 0 - valLoss: 0.6567673683166504 - trainLoss: 0.6574548482894897\n",
      "cnt: 0 - valLoss: 0.6567658185958862 - trainLoss: 0.6574535965919495\n",
      "cnt: 0 - valLoss: 0.6567642688751221 - trainLoss: 0.6574522256851196\n",
      "cnt: 0 - valLoss: 0.6567626595497131 - trainLoss: 0.6574509739875793\n",
      "cnt: 0 - valLoss: 0.6567610502243042 - trainLoss: 0.6574496626853943\n",
      "cnt: 0 - valLoss: 0.65675950050354 - trainLoss: 0.6574483513832092\n",
      "cnt: 0 - valLoss: 0.6567578911781311 - trainLoss: 0.657447099685669\n",
      "cnt: 0 - valLoss: 0.6567562818527222 - trainLoss: 0.6574457883834839\n",
      "cnt: 0 - valLoss: 0.656754732131958 - trainLoss: 0.6574444770812988\n",
      "cnt: 0 - valLoss: 0.6567531228065491 - trainLoss: 0.6574431657791138\n",
      "cnt: 0 - valLoss: 0.6567515134811401 - trainLoss: 0.6574418544769287\n",
      "cnt: 0 - valLoss: 0.6567499041557312 - trainLoss: 0.6574405431747437\n",
      "cnt: 0 - valLoss: 0.656748354434967 - trainLoss: 0.6574392318725586\n",
      "cnt: 0 - valLoss: 0.6567467451095581 - trainLoss: 0.6574378609657288\n",
      "cnt: 0 - valLoss: 0.6567451357841492 - trainLoss: 0.6574366092681885\n",
      "cnt: 0 - valLoss: 0.656743586063385 - trainLoss: 0.6574352979660034\n",
      "cnt: 0 - valLoss: 0.6567419767379761 - trainLoss: 0.6574340462684631\n",
      "cnt: 0 - valLoss: 0.6567404270172119 - trainLoss: 0.6574326753616333\n",
      "cnt: 0 - valLoss: 0.656738817691803 - trainLoss: 0.6574313640594482\n",
      "cnt: 0 - valLoss: 0.656737208366394 - trainLoss: 0.6574300527572632\n",
      "cnt: 0 - valLoss: 0.6567355990409851 - trainLoss: 0.6574287414550781\n",
      "cnt: 0 - valLoss: 0.656734049320221 - trainLoss: 0.6574274897575378\n",
      "cnt: 0 - valLoss: 0.6567325592041016 - trainLoss: 0.657426118850708\n",
      "cnt: 0 - valLoss: 0.6567308902740479 - trainLoss: 0.6574248671531677\n",
      "cnt: 0 - valLoss: 0.6567292809486389 - trainLoss: 0.6574235558509827\n",
      "cnt: 0 - valLoss: 0.65672767162323 - trainLoss: 0.6574221849441528\n",
      "cnt: 0 - valLoss: 0.6567261219024658 - trainLoss: 0.6574209332466125\n",
      "cnt: 0 - valLoss: 0.6567245721817017 - trainLoss: 0.6574196219444275\n",
      "cnt: 0 - valLoss: 0.6567229628562927 - trainLoss: 0.6574182510375977\n",
      "cnt: 0 - valLoss: 0.6567213535308838 - trainLoss: 0.6574169397354126\n",
      "cnt: 0 - valLoss: 0.6567197442054749 - trainLoss: 0.6574156880378723\n",
      "cnt: 0 - valLoss: 0.6567181348800659 - trainLoss: 0.6574143767356873\n",
      "cnt: 0 - valLoss: 0.656716525554657 - trainLoss: 0.6574130654335022\n",
      "cnt: 0 - valLoss: 0.6567149758338928 - trainLoss: 0.6574116945266724\n",
      "cnt: 0 - valLoss: 0.6567133069038391 - trainLoss: 0.6574104428291321\n",
      "cnt: 0 - valLoss: 0.6567118167877197 - trainLoss: 0.657409131526947\n",
      "cnt: 0 - valLoss: 0.6567102670669556 - trainLoss: 0.6574077606201172\n",
      "cnt: 0 - valLoss: 0.6567087769508362 - trainLoss: 0.6574065089225769\n",
      "cnt: 0 - valLoss: 0.6567074060440063 - trainLoss: 0.6574051976203918\n",
      "cnt: 0 - valLoss: 0.6567059755325317 - trainLoss: 0.6574038863182068\n",
      "cnt: 0 - valLoss: 0.6567044854164124 - trainLoss: 0.6574026346206665\n",
      "cnt: 0 - valLoss: 0.6567030549049377 - trainLoss: 0.6574013829231262\n",
      "cnt: 0 - valLoss: 0.6567015647888184 - trainLoss: 0.6574000716209412\n",
      "cnt: 0 - valLoss: 0.6567001938819885 - trainLoss: 0.6573988199234009\n",
      "cnt: 0 - valLoss: 0.6566987037658691 - trainLoss: 0.6573975682258606\n",
      "cnt: 0 - valLoss: 0.6566973328590393 - trainLoss: 0.6573963165283203\n",
      "cnt: 0 - valLoss: 0.6566958427429199 - trainLoss: 0.65739506483078\n",
      "cnt: 0 - valLoss: 0.6566944122314453 - trainLoss: 0.6573938131332397\n",
      "cnt: 0 - valLoss: 0.6566929817199707 - trainLoss: 0.6573925614356995\n",
      "cnt: 0 - valLoss: 0.6566915512084961 - trainLoss: 0.657391369342804\n",
      "cnt: 0 - valLoss: 0.6566901206970215 - trainLoss: 0.6573900580406189\n",
      "cnt: 0 - valLoss: 0.6566886305809021 - trainLoss: 0.6573888659477234\n",
      "cnt: 0 - valLoss: 0.6566872596740723 - trainLoss: 0.6573876142501831\n",
      "cnt: 0 - valLoss: 0.6566857695579529 - trainLoss: 0.6573863625526428\n",
      "cnt: 0 - valLoss: 0.6566843390464783 - trainLoss: 0.6573851108551025\n",
      "cnt: 0 - valLoss: 0.6566829085350037 - trainLoss: 0.6573837995529175\n",
      "cnt: 0 - valLoss: 0.656681478023529 - trainLoss: 0.657382607460022\n",
      "cnt: 0 - valLoss: 0.6566800475120544 - trainLoss: 0.6573812961578369\n",
      "cnt: 0 - valLoss: 0.6566786170005798 - trainLoss: 0.6573800444602966\n",
      "cnt: 0 - valLoss: 0.6566771864891052 - trainLoss: 0.6573788523674011\n",
      "cnt: 0 - valLoss: 0.6566758155822754 - trainLoss: 0.6573775410652161\n",
      "cnt: 0 - valLoss: 0.656674325466156 - trainLoss: 0.6573763489723206\n",
      "cnt: 0 - valLoss: 0.6566728949546814 - trainLoss: 0.6573750972747803\n",
      "cnt: 0 - valLoss: 0.6566714644432068 - trainLoss: 0.6573737859725952\n",
      "cnt: 0 - valLoss: 0.6566699743270874 - trainLoss: 0.6573725938796997\n",
      "cnt: 0 - valLoss: 0.656668484210968 - trainLoss: 0.6573713421821594\n",
      "cnt: 0 - valLoss: 0.6566671133041382 - trainLoss: 0.6573700308799744\n",
      "cnt: 0 - valLoss: 0.6566656827926636 - trainLoss: 0.6573687791824341\n",
      "cnt: 0 - valLoss: 0.6566641926765442 - trainLoss: 0.6573675274848938\n",
      "cnt: 0 - valLoss: 0.6566627621650696 - trainLoss: 0.6573662757873535\n",
      "cnt: 0 - valLoss: 0.6566612720489502 - trainLoss: 0.6573650240898132\n",
      "cnt: 0 - valLoss: 0.6566599011421204 - trainLoss: 0.6573637127876282\n",
      "cnt: 0 - valLoss: 0.656658411026001 - trainLoss: 0.6573625206947327\n",
      "cnt: 0 - valLoss: 0.6566569805145264 - trainLoss: 0.6573612689971924\n",
      "cnt: 0 - valLoss: 0.6566555500030518 - trainLoss: 0.6573600172996521\n",
      "cnt: 0 - valLoss: 0.6566540598869324 - trainLoss: 0.657358705997467\n",
      "cnt: 0 - valLoss: 0.6566526293754578 - trainLoss: 0.6573574542999268\n",
      "cnt: 0 - valLoss: 0.6566512584686279 - trainLoss: 0.6573562026023865\n",
      "cnt: 0 - valLoss: 0.6566497683525085 - trainLoss: 0.657355010509491\n",
      "cnt: 0 - valLoss: 0.6566483378410339 - trainLoss: 0.6573536992073059\n",
      "cnt: 0 - valLoss: 0.6566469073295593 - trainLoss: 0.6573525071144104\n",
      "cnt: 0 - valLoss: 0.6566454172134399 - trainLoss: 0.6573511958122253\n",
      "cnt: 0 - valLoss: 0.6566440463066101 - trainLoss: 0.6573499441146851\n",
      "cnt: 0 - valLoss: 0.6566425561904907 - trainLoss: 0.6573486924171448\n",
      "cnt: 0 - valLoss: 0.6566411852836609 - trainLoss: 0.6573474407196045\n",
      "cnt: 0 - valLoss: 0.6566397547721863 - trainLoss: 0.6573461294174194\n",
      "cnt: 0 - valLoss: 0.6566382646560669 - trainLoss: 0.6573449373245239\n",
      "cnt: 0 - valLoss: 0.6566368341445923 - trainLoss: 0.6573435664176941\n",
      "cnt: 0 - valLoss: 0.6566353440284729 - trainLoss: 0.6573423743247986\n",
      "cnt: 0 - valLoss: 0.6566339731216431 - trainLoss: 0.6573411226272583\n",
      "cnt: 0 - valLoss: 0.6566324830055237 - trainLoss: 0.6573398113250732\n",
      "cnt: 0 - valLoss: 0.6566310524940491 - trainLoss: 0.657338559627533\n",
      "cnt: 0 - valLoss: 0.6566296219825745 - trainLoss: 0.6573373079299927\n",
      "cnt: 0 - valLoss: 0.6566281914710999 - trainLoss: 0.6573359370231628\n",
      "cnt: 0 - valLoss: 0.6566267609596252 - trainLoss: 0.6573347449302673\n",
      "cnt: 0 - valLoss: 0.6566252708435059 - trainLoss: 0.657333493232727\n",
      "cnt: 0 - valLoss: 0.6566238403320312 - trainLoss: 0.657332181930542\n",
      "cnt: 0 - valLoss: 0.6566224694252014 - trainLoss: 0.6573309302330017\n",
      "cnt: 0 - valLoss: 0.6566210389137268 - trainLoss: 0.6573296189308167\n",
      "cnt: 0 - valLoss: 0.6566196084022522 - trainLoss: 0.6573283672332764\n",
      "cnt: 0 - valLoss: 0.6566181182861328 - trainLoss: 0.6573271155357361\n",
      "cnt: 0 - valLoss: 0.656616747379303 - trainLoss: 0.657325804233551\n",
      "cnt: 0 - valLoss: 0.6566151976585388 - trainLoss: 0.6573245525360107\n",
      "cnt: 0 - valLoss: 0.656613826751709 - trainLoss: 0.6573233008384705\n",
      "cnt: 0 - valLoss: 0.6566123962402344 - trainLoss: 0.6573219895362854\n",
      "cnt: 0 - valLoss: 0.6566109657287598 - trainLoss: 0.6573207378387451\n",
      "cnt: 0 - valLoss: 0.6566094756126404 - trainLoss: 0.6573194265365601\n",
      "cnt: 0 - valLoss: 0.6566080451011658 - trainLoss: 0.6573181748390198\n",
      "cnt: 0 - valLoss: 0.6566066145896912 - trainLoss: 0.6573168635368347\n",
      "cnt: 0 - valLoss: 0.6566051244735718 - trainLoss: 0.6573156118392944\n",
      "cnt: 0 - valLoss: 0.6566037535667419 - trainLoss: 0.6573143005371094\n",
      "cnt: 0 - valLoss: 0.6566022634506226 - trainLoss: 0.6573130488395691\n",
      "cnt: 0 - valLoss: 0.6566007733345032 - trainLoss: 0.6573117971420288\n",
      "cnt: 0 - valLoss: 0.6565993428230286 - trainLoss: 0.6573104858398438\n",
      "cnt: 0 - valLoss: 0.6565979719161987 - trainLoss: 0.6573092341423035\n",
      "cnt: 0 - valLoss: 0.6565965414047241 - trainLoss: 0.6573079228401184\n",
      "cnt: 0 - valLoss: 0.6565950512886047 - trainLoss: 0.6573066711425781\n",
      "cnt: 0 - valLoss: 0.6565936207771301 - trainLoss: 0.6573054194450378\n",
      "cnt: 0 - valLoss: 0.6565921902656555 - trainLoss: 0.6573041081428528\n",
      "cnt: 0 - valLoss: 0.6565907597541809 - trainLoss: 0.6573028564453125\n",
      "cnt: 0 - valLoss: 0.6565892696380615 - trainLoss: 0.6573016047477722\n",
      "cnt: 0 - valLoss: 0.6565878391265869 - trainLoss: 0.6573002934455872\n",
      "cnt: 0 - valLoss: 0.6565864086151123 - trainLoss: 0.6572990417480469\n",
      "cnt: 0 - valLoss: 0.6565849184989929 - trainLoss: 0.6572977304458618\n",
      "cnt: 0 - valLoss: 0.6565834879875183 - trainLoss: 0.6572964191436768\n",
      "cnt: 0 - valLoss: 0.6565821170806885 - trainLoss: 0.6572952270507812\n",
      "cnt: 0 - valLoss: 0.6565805673599243 - trainLoss: 0.6572939157485962\n",
      "cnt: 0 - valLoss: 0.6565791964530945 - trainLoss: 0.6572926044464111\n",
      "cnt: 0 - valLoss: 0.6565777063369751 - trainLoss: 0.6572913527488708\n",
      "cnt: 0 - valLoss: 0.6565762758255005 - trainLoss: 0.6572900414466858\n",
      "cnt: 0 - valLoss: 0.6565747857093811 - trainLoss: 0.6572887897491455\n",
      "cnt: 0 - valLoss: 0.6565733551979065 - trainLoss: 0.6572875380516052\n",
      "cnt: 0 - valLoss: 0.6565718650817871 - trainLoss: 0.6572861671447754\n",
      "cnt: 0 - valLoss: 0.6565704941749573 - trainLoss: 0.6572849154472351\n",
      "cnt: 0 - valLoss: 0.6565690040588379 - trainLoss: 0.6572836637496948\n",
      "cnt: 0 - valLoss: 0.6565676331520081 - trainLoss: 0.6572823524475098\n",
      "cnt: 0 - valLoss: 0.6565660834312439 - trainLoss: 0.6572811007499695\n",
      "cnt: 0 - valLoss: 0.6565647125244141 - trainLoss: 0.6572797894477844\n",
      "cnt: 0 - valLoss: 0.6565632820129395 - trainLoss: 0.6572784781455994\n",
      "cnt: 0 - valLoss: 0.6565617918968201 - trainLoss: 0.6572772264480591\n",
      "cnt: 0 - valLoss: 0.6565603017807007 - trainLoss: 0.6572759747505188\n",
      "cnt: 0 - valLoss: 0.6565589308738708 - trainLoss: 0.6572746634483337\n",
      "cnt: 0 - valLoss: 0.6565574407577515 - trainLoss: 0.6572734117507935\n",
      "cnt: 0 - valLoss: 0.6565560102462769 - trainLoss: 0.6572721004486084\n",
      "cnt: 0 - valLoss: 0.6565545201301575 - trainLoss: 0.6572708487510681\n",
      "cnt: 0 - valLoss: 0.6565530896186829 - trainLoss: 0.6572695374488831\n",
      "cnt: 0 - valLoss: 0.6565515995025635 - trainLoss: 0.657268226146698\n",
      "cnt: 0 - valLoss: 0.6565502285957336 - trainLoss: 0.6572669744491577\n",
      "cnt: 0 - valLoss: 0.6565486788749695 - trainLoss: 0.6572656631469727\n",
      "cnt: 0 - valLoss: 0.6565473079681396 - trainLoss: 0.6572644114494324\n",
      "cnt: 0 - valLoss: 0.6565458178520203 - trainLoss: 0.6572631001472473\n",
      "cnt: 0 - valLoss: 0.6565443873405457 - trainLoss: 0.657261848449707\n",
      "cnt: 0 - valLoss: 0.6565428972244263 - trainLoss: 0.6572604775428772\n",
      "cnt: 0 - valLoss: 0.6565415263175964 - trainLoss: 0.6572592854499817\n",
      "cnt: 0 - valLoss: 0.656540036201477 - trainLoss: 0.6572579741477966\n",
      "cnt: 0 - valLoss: 0.6565386056900024 - trainLoss: 0.6572567224502563\n",
      "cnt: 0 - valLoss: 0.6565371751785278 - trainLoss: 0.6572553515434265\n",
      "cnt: 0 - valLoss: 0.6565357446670532 - trainLoss: 0.6572540998458862\n",
      "cnt: 0 - valLoss: 0.6565343141555786 - trainLoss: 0.657252848148346\n",
      "cnt: 0 - valLoss: 0.6565328240394592 - trainLoss: 0.6572515368461609\n",
      "cnt: 0 - valLoss: 0.6565314531326294 - trainLoss: 0.6572502255439758\n",
      "cnt: 0 - valLoss: 0.65652996301651 - trainLoss: 0.6572489738464355\n",
      "cnt: 0 - valLoss: 0.6565285325050354 - trainLoss: 0.6572477221488953\n",
      "cnt: 0 - valLoss: 0.6565271019935608 - trainLoss: 0.6572463512420654\n",
      "cnt: 0 - valLoss: 0.6565256714820862 - trainLoss: 0.6572450399398804\n",
      "cnt: 0 - valLoss: 0.6565242409706116 - trainLoss: 0.6572437882423401\n",
      "cnt: 0 - valLoss: 0.6565227508544922 - trainLoss: 0.6572425365447998\n",
      "cnt: 0 - valLoss: 0.6565213203430176 - trainLoss: 0.6572412252426147\n",
      "cnt: 0 - valLoss: 0.656519889831543 - trainLoss: 0.6572399139404297\n",
      "cnt: 0 - valLoss: 0.6565184593200684 - trainLoss: 0.6572386026382446\n",
      "cnt: 0 - valLoss: 0.6565170884132385 - trainLoss: 0.6572372913360596\n",
      "cnt: 0 - valLoss: 0.6565155982971191 - trainLoss: 0.6572359800338745\n",
      "cnt: 0 - valLoss: 0.6565142273902893 - trainLoss: 0.6572347283363342\n",
      "cnt: 0 - valLoss: 0.6565127968788147 - trainLoss: 0.6572334170341492\n",
      "cnt: 0 - valLoss: 0.6565113663673401 - trainLoss: 0.6572321653366089\n",
      "cnt: 0 - valLoss: 0.6565098762512207 - trainLoss: 0.6572308540344238\n",
      "cnt: 0 - valLoss: 0.6565084457397461 - trainLoss: 0.6572295427322388\n",
      "cnt: 0 - valLoss: 0.6565070152282715 - trainLoss: 0.6572282314300537\n",
      "cnt: 0 - valLoss: 0.6565055251121521 - trainLoss: 0.6572269201278687\n",
      "cnt: 0 - valLoss: 0.6565041542053223 - trainLoss: 0.6572256088256836\n",
      "cnt: 0 - valLoss: 0.6565027236938477 - trainLoss: 0.6572242975234985\n",
      "cnt: 0 - valLoss: 0.6565012335777283 - trainLoss: 0.6572229862213135\n",
      "cnt: 0 - valLoss: 0.6564998030662537 - trainLoss: 0.657221794128418\n",
      "cnt: 0 - valLoss: 0.6564983129501343 - trainLoss: 0.6572204232215881\n",
      "cnt: 0 - valLoss: 0.6564969420433044 - trainLoss: 0.6572191119194031\n",
      "cnt: 0 - valLoss: 0.6564955115318298 - trainLoss: 0.657217800617218\n",
      "cnt: 0 - valLoss: 0.6564940810203552 - trainLoss: 0.657216489315033\n",
      "cnt: 0 - valLoss: 0.6564925312995911 - trainLoss: 0.6572152376174927\n",
      "cnt: 0 - valLoss: 0.6564911603927612 - trainLoss: 0.6572138667106628\n",
      "cnt: 0 - valLoss: 0.6564897298812866 - trainLoss: 0.657212495803833\n",
      "cnt: 0 - valLoss: 0.656488299369812 - trainLoss: 0.6572113037109375\n",
      "cnt: 0 - valLoss: 0.6564868688583374 - trainLoss: 0.6572099924087524\n",
      "cnt: 0 - valLoss: 0.656485378742218 - trainLoss: 0.6572086811065674\n",
      "cnt: 0 - valLoss: 0.6564840078353882 - trainLoss: 0.6572073698043823\n",
      "cnt: 0 - valLoss: 0.6564825773239136 - trainLoss: 0.657206118106842\n",
      "cnt: 0 - valLoss: 0.6564810872077942 - trainLoss: 0.6572047472000122\n",
      "cnt: 0 - valLoss: 0.6564797163009644 - trainLoss: 0.6572034955024719\n",
      "cnt: 0 - valLoss: 0.6564782857894897 - trainLoss: 0.6572021245956421\n",
      "cnt: 0 - valLoss: 0.6564767956733704 - trainLoss: 0.657200813293457\n",
      "cnt: 0 - valLoss: 0.6564753651618958 - trainLoss: 0.657199501991272\n",
      "cnt: 0 - valLoss: 0.6564739942550659 - trainLoss: 0.6571982502937317\n",
      "cnt: 0 - valLoss: 0.6564725637435913 - trainLoss: 0.6571968793869019\n",
      "cnt: 0 - valLoss: 0.6564711928367615 - trainLoss: 0.6571956276893616\n",
      "cnt: 0 - valLoss: 0.6564697027206421 - trainLoss: 0.6571943163871765\n",
      "cnt: 0 - valLoss: 0.6564682722091675 - trainLoss: 0.6571929454803467\n",
      "cnt: 0 - valLoss: 0.6564667820930481 - trainLoss: 0.6571916937828064\n",
      "cnt: 0 - valLoss: 0.6564653515815735 - trainLoss: 0.6571903228759766\n",
      "cnt: 0 - valLoss: 0.6564639210700989 - trainLoss: 0.6571890711784363\n",
      "cnt: 0 - valLoss: 0.6564624905586243 - trainLoss: 0.6571877598762512\n",
      "cnt: 0 - valLoss: 0.6564610600471497 - trainLoss: 0.6571864485740662\n",
      "cnt: 0 - valLoss: 0.6564596891403198 - trainLoss: 0.6571851372718811\n",
      "cnt: 0 - valLoss: 0.6564581990242004 - trainLoss: 0.657183825969696\n",
      "cnt: 0 - valLoss: 0.6564567685127258 - trainLoss: 0.6571825742721558\n",
      "cnt: 0 - valLoss: 0.6564552783966064 - trainLoss: 0.6571812033653259\n",
      "cnt: 0 - valLoss: 0.6564538478851318 - trainLoss: 0.6571798920631409\n",
      "cnt: 0 - valLoss: 0.6564524173736572 - trainLoss: 0.657178521156311\n",
      "cnt: 0 - valLoss: 0.6564509868621826 - trainLoss: 0.6571772694587708\n",
      "cnt: 0 - valLoss: 0.6564494967460632 - trainLoss: 0.6571759581565857\n",
      "cnt: 0 - valLoss: 0.6564481258392334 - trainLoss: 0.6571745872497559\n",
      "cnt: 0 - valLoss: 0.656446635723114 - trainLoss: 0.6571733355522156\n",
      "cnt: 0 - valLoss: 0.6564452052116394 - trainLoss: 0.6571720242500305\n",
      "cnt: 0 - valLoss: 0.65644371509552 - trainLoss: 0.6571706533432007\n",
      "cnt: 0 - valLoss: 0.6564422845840454 - trainLoss: 0.6571694016456604\n",
      "cnt: 0 - valLoss: 0.6564408540725708 - trainLoss: 0.6571680903434753\n",
      "cnt: 0 - valLoss: 0.6564394235610962 - trainLoss: 0.6571667790412903\n",
      "cnt: 0 - valLoss: 0.6564379334449768 - trainLoss: 0.6571654677391052\n",
      "cnt: 0 - valLoss: 0.656436562538147 - trainLoss: 0.6571641564369202\n",
      "cnt: 0 - valLoss: 0.6564350724220276 - trainLoss: 0.6571628451347351\n",
      "cnt: 0 - valLoss: 0.656433641910553 - trainLoss: 0.6571615934371948\n",
      "cnt: 0 - valLoss: 0.6564321517944336 - trainLoss: 0.6571602821350098\n",
      "cnt: 0 - valLoss: 0.656430721282959 - trainLoss: 0.6571590900421143\n",
      "cnt: 0 - valLoss: 0.6564292311668396 - trainLoss: 0.6571577191352844\n",
      "cnt: 0 - valLoss: 0.6564277410507202 - trainLoss: 0.6571564674377441\n",
      "cnt: 0 - valLoss: 0.6564262509346008 - trainLoss: 0.6571551561355591\n",
      "cnt: 0 - valLoss: 0.6564248204231262 - trainLoss: 0.657153844833374\n",
      "cnt: 0 - valLoss: 0.6564232707023621 - trainLoss: 0.6571525931358337\n",
      "cnt: 0 - valLoss: 0.6564217805862427 - trainLoss: 0.6571513414382935\n",
      "cnt: 0 - valLoss: 0.6564203500747681 - trainLoss: 0.6571500301361084\n",
      "cnt: 0 - valLoss: 0.6564188599586487 - trainLoss: 0.6571487188339233\n",
      "cnt: 0 - valLoss: 0.6564174294471741 - trainLoss: 0.6571474671363831\n",
      "cnt: 0 - valLoss: 0.6564159393310547 - trainLoss: 0.657146155834198\n",
      "cnt: 0 - valLoss: 0.6564145088195801 - trainLoss: 0.6571448445320129\n",
      "cnt: 0 - valLoss: 0.6564129590988159 - trainLoss: 0.6571435928344727\n",
      "cnt: 0 - valLoss: 0.6564115285873413 - trainLoss: 0.6571423411369324\n",
      "cnt: 0 - valLoss: 0.6564100384712219 - trainLoss: 0.6571410298347473\n",
      "cnt: 0 - valLoss: 0.6564086079597473 - trainLoss: 0.6571397185325623\n",
      "cnt: 0 - valLoss: 0.6564071774482727 - trainLoss: 0.6571384072303772\n",
      "cnt: 0 - valLoss: 0.6564056873321533 - trainLoss: 0.6571370959281921\n",
      "cnt: 0 - valLoss: 0.6564041972160339 - trainLoss: 0.6571358442306519\n",
      "cnt: 0 - valLoss: 0.6564026474952698 - trainLoss: 0.6571345329284668\n",
      "cnt: 0 - valLoss: 0.6564012765884399 - trainLoss: 0.6571332216262817\n",
      "cnt: 0 - valLoss: 0.6563997268676758 - trainLoss: 0.6571319699287415\n",
      "cnt: 0 - valLoss: 0.6563982367515564 - trainLoss: 0.6571305990219116\n",
      "cnt: 0 - valLoss: 0.6563968658447266 - trainLoss: 0.6571292877197266\n",
      "cnt: 0 - valLoss: 0.6563953161239624 - trainLoss: 0.657128095626831\n",
      "cnt: 0 - valLoss: 0.656393826007843 - trainLoss: 0.6571267247200012\n",
      "cnt: 0 - valLoss: 0.6563924551010132 - trainLoss: 0.6571254134178162\n",
      "cnt: 0 - valLoss: 0.656390905380249 - trainLoss: 0.6571241617202759\n",
      "cnt: 0 - valLoss: 0.6563894152641296 - trainLoss: 0.6571229100227356\n",
      "cnt: 0 - valLoss: 0.6563880443572998 - trainLoss: 0.6571215987205505\n",
      "cnt: 0 - valLoss: 0.6563864946365356 - trainLoss: 0.6571202874183655\n",
      "cnt: 0 - valLoss: 0.6563849449157715 - trainLoss: 0.6571189761161804\n",
      "cnt: 0 - valLoss: 0.6563835740089417 - trainLoss: 0.6571177244186401\n",
      "cnt: 0 - valLoss: 0.6563820838928223 - trainLoss: 0.6571164131164551\n",
      "cnt: 0 - valLoss: 0.6563805937767029 - trainLoss: 0.65711510181427\n",
      "cnt: 0 - valLoss: 0.6563791632652283 - trainLoss: 0.6571138501167297\n",
      "cnt: 0 - valLoss: 0.6563776731491089 - trainLoss: 0.6571124792098999\n",
      "cnt: 0 - valLoss: 0.6563762426376343 - trainLoss: 0.6571111679077148\n",
      "cnt: 0 - valLoss: 0.6563746929168701 - trainLoss: 0.6571098566055298\n",
      "cnt: 0 - valLoss: 0.6563732028007507 - trainLoss: 0.6571085453033447\n",
      "cnt: 0 - valLoss: 0.6563717126846313 - trainLoss: 0.6571072936058044\n",
      "cnt: 0 - valLoss: 0.6563702821731567 - trainLoss: 0.6571059226989746\n",
      "cnt: 0 - valLoss: 0.6563687920570374 - trainLoss: 0.6571046113967896\n",
      "cnt: 0 - valLoss: 0.6563672423362732 - trainLoss: 0.6571033596992493\n",
      "cnt: 0 - valLoss: 0.6563657522201538 - trainLoss: 0.6571019887924194\n",
      "cnt: 0 - valLoss: 0.656364381313324 - trainLoss: 0.6571007370948792\n",
      "cnt: 0 - valLoss: 0.6563628315925598 - trainLoss: 0.6570994257926941\n",
      "cnt: 0 - valLoss: 0.6563613414764404 - trainLoss: 0.657098114490509\n",
      "cnt: 0 - valLoss: 0.656359851360321 - trainLoss: 0.657096803188324\n",
      "cnt: 0 - valLoss: 0.6563584208488464 - trainLoss: 0.6570954918861389\n",
      "cnt: 0 - valLoss: 0.6563568711280823 - trainLoss: 0.6570941209793091\n",
      "cnt: 0 - valLoss: 0.6563553810119629 - trainLoss: 0.6570928692817688\n",
      "cnt: 0 - valLoss: 0.6563539505004883 - trainLoss: 0.6570915579795837\n",
      "cnt: 0 - valLoss: 0.6563525199890137 - trainLoss: 0.6570903062820435\n",
      "cnt: 0 - valLoss: 0.6563509702682495 - trainLoss: 0.6570889353752136\n",
      "cnt: 0 - valLoss: 0.6563495397567749 - trainLoss: 0.6570875644683838\n",
      "cnt: 0 - valLoss: 0.6563479900360107 - trainLoss: 0.6570863127708435\n",
      "cnt: 0 - valLoss: 0.6563464999198914 - trainLoss: 0.6570850014686584\n",
      "cnt: 0 - valLoss: 0.6563450694084167 - trainLoss: 0.6570836901664734\n",
      "cnt: 0 - valLoss: 0.6563435196876526 - trainLoss: 0.6570823788642883\n",
      "cnt: 0 - valLoss: 0.656342089176178 - trainLoss: 0.6570810079574585\n",
      "cnt: 0 - valLoss: 0.656340479850769 - trainLoss: 0.6570797562599182\n",
      "cnt: 0 - valLoss: 0.6563391089439392 - trainLoss: 0.6570784449577332\n",
      "cnt: 0 - valLoss: 0.6563376188278198 - trainLoss: 0.6570770740509033\n",
      "cnt: 0 - valLoss: 0.6563360691070557 - trainLoss: 0.6570757627487183\n",
      "cnt: 0 - valLoss: 0.6563345789909363 - trainLoss: 0.657074511051178\n",
      "cnt: 0 - valLoss: 0.6563331484794617 - trainLoss: 0.6570731401443481\n",
      "cnt: 0 - valLoss: 0.6563315987586975 - trainLoss: 0.6570718884468079\n",
      "cnt: 0 - valLoss: 0.6563301682472229 - trainLoss: 0.657070517539978\n",
      "cnt: 0 - valLoss: 0.6563286185264587 - trainLoss: 0.657069206237793\n",
      "cnt: 0 - valLoss: 0.6563271284103394 - trainLoss: 0.6570679545402527\n",
      "cnt: 0 - valLoss: 0.6563256978988647 - trainLoss: 0.6570665836334229\n",
      "cnt: 0 - valLoss: 0.6563242077827454 - trainLoss: 0.6570652723312378\n",
      "cnt: 0 - valLoss: 0.656322717666626 - trainLoss: 0.657063901424408\n",
      "cnt: 0 - valLoss: 0.6563212275505066 - trainLoss: 0.6570626497268677\n",
      "cnt: 0 - valLoss: 0.6563197374343872 - trainLoss: 0.6570613384246826\n",
      "cnt: 0 - valLoss: 0.6563182473182678 - trainLoss: 0.6570599675178528\n",
      "cnt: 0 - valLoss: 0.6563167572021484 - trainLoss: 0.6570587158203125\n",
      "cnt: 0 - valLoss: 0.6563152074813843 - trainLoss: 0.6570573449134827\n",
      "cnt: 0 - valLoss: 0.6563137173652649 - trainLoss: 0.6570560336112976\n",
      "cnt: 0 - valLoss: 0.6563122868537903 - trainLoss: 0.6570547819137573\n",
      "cnt: 0 - valLoss: 0.6563107967376709 - trainLoss: 0.6570534110069275\n",
      "cnt: 0 - valLoss: 0.6563092470169067 - trainLoss: 0.6570520997047424\n",
      "cnt: 0 - valLoss: 0.6563077569007874 - trainLoss: 0.6570507884025574\n",
      "cnt: 0 - valLoss: 0.6563062071800232 - trainLoss: 0.6570494771003723\n",
      "cnt: 0 - valLoss: 0.6563047766685486 - trainLoss: 0.6570481657981873\n",
      "cnt: 0 - valLoss: 0.656303346157074 - trainLoss: 0.6570467948913574\n",
      "cnt: 0 - valLoss: 0.6563017964363098 - trainLoss: 0.6570454835891724\n",
      "cnt: 0 - valLoss: 0.6563003063201904 - trainLoss: 0.6570442318916321\n",
      "cnt: 0 - valLoss: 0.6562988758087158 - trainLoss: 0.657042920589447\n",
      "cnt: 0 - valLoss: 0.6562973260879517 - trainLoss: 0.6570415496826172\n",
      "cnt: 0 - valLoss: 0.6562958359718323 - trainLoss: 0.6570402383804321\n",
      "cnt: 0 - valLoss: 0.6562942862510681 - trainLoss: 0.6570389270782471\n",
      "cnt: 0 - valLoss: 0.6562928557395935 - trainLoss: 0.657037615776062\n",
      "cnt: 0 - valLoss: 0.6562913060188293 - trainLoss: 0.6570362448692322\n",
      "cnt: 0 - valLoss: 0.6562897562980652 - trainLoss: 0.6570349931716919\n",
      "cnt: 0 - valLoss: 0.6562883853912354 - trainLoss: 0.6570336222648621\n",
      "cnt: 0 - valLoss: 0.6562868356704712 - trainLoss: 0.657032310962677\n",
      "cnt: 0 - valLoss: 0.6562853455543518 - trainLoss: 0.6570309996604919\n",
      "cnt: 0 - valLoss: 0.6562838554382324 - trainLoss: 0.6570296883583069\n",
      "cnt: 0 - valLoss: 0.656282365322113 - trainLoss: 0.657028317451477\n",
      "cnt: 0 - valLoss: 0.6562807559967041 - trainLoss: 0.6570269465446472\n",
      "cnt: 0 - valLoss: 0.6562792658805847 - trainLoss: 0.6570256352424622\n",
      "cnt: 0 - valLoss: 0.6562778353691101 - trainLoss: 0.6570242643356323\n",
      "cnt: 0 - valLoss: 0.6562763452529907 - trainLoss: 0.6570229530334473\n",
      "cnt: 0 - valLoss: 0.6562748551368713 - trainLoss: 0.6570216417312622\n",
      "cnt: 0 - valLoss: 0.656273365020752 - trainLoss: 0.6570202708244324\n",
      "cnt: 0 - valLoss: 0.6562718152999878 - trainLoss: 0.6570189595222473\n",
      "cnt: 0 - valLoss: 0.6562703251838684 - trainLoss: 0.6570175886154175\n",
      "cnt: 0 - valLoss: 0.656268835067749 - trainLoss: 0.6570162177085876\n",
      "cnt: 0 - valLoss: 0.6562673449516296 - trainLoss: 0.6570148468017578\n",
      "cnt: 0 - valLoss: 0.6562657952308655 - trainLoss: 0.657013475894928\n",
      "cnt: 0 - valLoss: 0.6562642455101013 - trainLoss: 0.6570121645927429\n",
      "cnt: 0 - valLoss: 0.6562627553939819 - trainLoss: 0.6570107936859131\n",
      "cnt: 0 - valLoss: 0.6562612652778625 - trainLoss: 0.6570093631744385\n",
      "cnt: 0 - valLoss: 0.6562597155570984 - trainLoss: 0.6570081114768982\n",
      "cnt: 0 - valLoss: 0.656258225440979 - trainLoss: 0.6570067405700684\n",
      "cnt: 0 - valLoss: 0.6562566161155701 - trainLoss: 0.6570053696632385\n",
      "cnt: 0 - valLoss: 0.6562551259994507 - trainLoss: 0.6570040583610535\n",
      "cnt: 0 - valLoss: 0.6562536358833313 - trainLoss: 0.6570026874542236\n",
      "cnt: 0 - valLoss: 0.6562521457672119 - trainLoss: 0.6570013165473938\n",
      "cnt: 0 - valLoss: 0.6562505960464478 - trainLoss: 0.656999945640564\n",
      "cnt: 0 - valLoss: 0.6562491059303284 - trainLoss: 0.6569985747337341\n",
      "cnt: 0 - valLoss: 0.656247615814209 - trainLoss: 0.6569972634315491\n",
      "cnt: 0 - valLoss: 0.6562461256980896 - trainLoss: 0.6569958925247192\n",
      "cnt: 0 - valLoss: 0.6562445759773254 - trainLoss: 0.6569945216178894\n",
      "cnt: 0 - valLoss: 0.656243085861206 - trainLoss: 0.6569932103157043\n",
      "cnt: 0 - valLoss: 0.6562415361404419 - trainLoss: 0.6569918394088745\n",
      "cnt: 0 - valLoss: 0.6562399864196777 - trainLoss: 0.6569904685020447\n",
      "cnt: 0 - valLoss: 0.6562384963035583 - trainLoss: 0.6569891571998596\n",
      "cnt: 0 - valLoss: 0.6562369465827942 - trainLoss: 0.6569877862930298\n",
      "cnt: 0 - valLoss: 0.6562354564666748 - trainLoss: 0.6569864749908447\n",
      "cnt: 0 - valLoss: 0.6562339663505554 - trainLoss: 0.6569851040840149\n",
      "cnt: 0 - valLoss: 0.6562324166297913 - trainLoss: 0.6569837331771851\n",
      "cnt: 0 - valLoss: 0.6562308073043823 - trainLoss: 0.6569823622703552\n",
      "cnt: 0 - valLoss: 0.6562293171882629 - trainLoss: 0.6569809913635254\n",
      "cnt: 0 - valLoss: 0.6562278270721436 - trainLoss: 0.6569796204566956\n",
      "cnt: 0 - valLoss: 0.6562263369560242 - trainLoss: 0.6569782495498657\n",
      "cnt: 0 - valLoss: 0.6562247276306152 - trainLoss: 0.6569769382476807\n",
      "cnt: 0 - valLoss: 0.6562232971191406 - trainLoss: 0.656975507736206\n",
      "cnt: 0 - valLoss: 0.6562218070030212 - trainLoss: 0.6569741368293762\n",
      "cnt: 0 - valLoss: 0.6562201976776123 - trainLoss: 0.6569728255271912\n",
      "cnt: 0 - valLoss: 0.6562186479568481 - trainLoss: 0.6569715142250061\n",
      "cnt: 0 - valLoss: 0.6562172174453735 - trainLoss: 0.6569700837135315\n",
      "cnt: 0 - valLoss: 0.6562156081199646 - trainLoss: 0.6569687724113464\n",
      "cnt: 0 - valLoss: 0.6562140583992004 - trainLoss: 0.6569674611091614\n",
      "cnt: 0 - valLoss: 0.6562126278877258 - trainLoss: 0.6569660902023315\n",
      "cnt: 0 - valLoss: 0.6562110781669617 - trainLoss: 0.6569647192955017\n",
      "cnt: 0 - valLoss: 0.6562095284461975 - trainLoss: 0.6569632887840271\n",
      "cnt: 0 - valLoss: 0.6562079787254333 - trainLoss: 0.656961977481842\n",
      "cnt: 0 - valLoss: 0.6562064290046692 - trainLoss: 0.6569606065750122\n",
      "cnt: 0 - valLoss: 0.6562049388885498 - trainLoss: 0.6569592356681824\n",
      "cnt: 0 - valLoss: 0.6562033891677856 - trainLoss: 0.6569578647613525\n",
      "cnt: 0 - valLoss: 0.6562018990516663 - trainLoss: 0.6569564938545227\n",
      "cnt: 0 - valLoss: 0.6562003493309021 - trainLoss: 0.6569551229476929\n",
      "cnt: 0 - valLoss: 0.6561987996101379 - trainLoss: 0.656953752040863\n",
      "cnt: 0 - valLoss: 0.6561973094940186 - trainLoss: 0.656952440738678\n",
      "cnt: 0 - valLoss: 0.6561958193778992 - trainLoss: 0.6569510102272034\n",
      "cnt: 0 - valLoss: 0.6561942100524902 - trainLoss: 0.6569496989250183\n",
      "cnt: 0 - valLoss: 0.6561927199363708 - trainLoss: 0.6569483280181885\n",
      "cnt: 0 - valLoss: 0.6561911702156067 - trainLoss: 0.6569469571113586\n",
      "cnt: 0 - valLoss: 0.6561896800994873 - trainLoss: 0.6569455862045288\n",
      "cnt: 0 - valLoss: 0.656188428401947 - trainLoss: 0.656944215297699\n",
      "cnt: 0 - valLoss: 0.6561868786811829 - trainLoss: 0.6569428443908691\n",
      "cnt: 0 - valLoss: 0.6561856865882874 - trainLoss: 0.6569415926933289\n",
      "cnt: 0 - valLoss: 0.6561844944953918 - trainLoss: 0.656940221786499\n",
      "cnt: 0 - valLoss: 0.6561828851699829 - trainLoss: 0.656938910484314\n",
      "cnt: 0 - valLoss: 0.6561816930770874 - trainLoss: 0.6569375395774841\n",
      "cnt: 0 - valLoss: 0.6561801433563232 - trainLoss: 0.6569362878799438\n",
      "cnt: 0 - valLoss: 0.6561789512634277 - trainLoss: 0.6569349765777588\n",
      "cnt: 0 - valLoss: 0.6561777591705322 - trainLoss: 0.656933605670929\n",
      "cnt: 0 - valLoss: 0.6561765074729919 - trainLoss: 0.6569322347640991\n",
      "cnt: 0 - valLoss: 0.6561750173568726 - trainLoss: 0.6569309830665588\n",
      "cnt: 0 - valLoss: 0.6561737656593323 - trainLoss: 0.6569296717643738\n",
      "cnt: 0 - valLoss: 0.6561726927757263 - trainLoss: 0.656928300857544\n",
      "cnt: 0 - valLoss: 0.6561711430549622 - trainLoss: 0.6569269895553589\n",
      "cnt: 0 - valLoss: 0.6561698913574219 - trainLoss: 0.6569256782531738\n",
      "cnt: 0 - valLoss: 0.6561686992645264 - trainLoss: 0.6569243669509888\n",
      "cnt: 0 - valLoss: 0.656167209148407 - trainLoss: 0.6569229960441589\n",
      "cnt: 0 - valLoss: 0.6561659574508667 - trainLoss: 0.6569217443466187\n",
      "cnt: 0 - valLoss: 0.656164824962616 - trainLoss: 0.6569203734397888\n",
      "cnt: 0 - valLoss: 0.6561632752418518 - trainLoss: 0.6569190621376038\n",
      "cnt: 0 - valLoss: 0.6561621427536011 - trainLoss: 0.6569176912307739\n",
      "cnt: 0 - valLoss: 0.6561610698699951 - trainLoss: 0.6569163203239441\n",
      "cnt: 0 - valLoss: 0.6561594605445862 - trainLoss: 0.656915009021759\n",
      "cnt: 0 - valLoss: 0.6561583280563354 - trainLoss: 0.6569135785102844\n",
      "cnt: 0 - valLoss: 0.6561571359634399 - trainLoss: 0.6569122672080994\n",
      "cnt: 0 - valLoss: 0.6561558842658997 - trainLoss: 0.6569109559059143\n",
      "cnt: 0 - valLoss: 0.6561546325683594 - trainLoss: 0.6569095849990845\n",
      "cnt: 0 - valLoss: 0.6561533808708191 - trainLoss: 0.6569082736968994\n",
      "cnt: 0 - valLoss: 0.6561521291732788 - trainLoss: 0.6569069623947144\n",
      "cnt: 0 - valLoss: 0.6561508774757385 - trainLoss: 0.6569056510925293\n",
      "cnt: 0 - valLoss: 0.6561492681503296 - trainLoss: 0.6569042801856995\n",
      "cnt: 0 - valLoss: 0.6561480760574341 - trainLoss: 0.6569029688835144\n",
      "cnt: 0 - valLoss: 0.6561468839645386 - trainLoss: 0.6569016575813293\n",
      "cnt: 0 - valLoss: 0.6561455726623535 - trainLoss: 0.6569003462791443\n",
      "cnt: 0 - valLoss: 0.6561443209648132 - trainLoss: 0.6568990349769592\n",
      "cnt: 0 - valLoss: 0.6561431288719177 - trainLoss: 0.6568976640701294\n",
      "cnt: 0 - valLoss: 0.6561419367790222 - trainLoss: 0.6568962931632996\n",
      "cnt: 0 - valLoss: 0.6561406850814819 - trainLoss: 0.6568949818611145\n",
      "cnt: 0 - valLoss: 0.6561393141746521 - trainLoss: 0.6568936705589294\n",
      "cnt: 0 - valLoss: 0.656138002872467 - trainLoss: 0.6568923592567444\n",
      "cnt: 0 - valLoss: 0.656136691570282 - trainLoss: 0.6568910479545593\n",
      "cnt: 0 - valLoss: 0.6561353206634521 - trainLoss: 0.6568897366523743\n",
      "cnt: 0 - valLoss: 0.6561340093612671 - trainLoss: 0.6568884253501892\n",
      "cnt: 0 - valLoss: 0.6561326384544373 - trainLoss: 0.6568870544433594\n",
      "cnt: 0 - valLoss: 0.6561313271522522 - trainLoss: 0.6568858027458191\n",
      "cnt: 0 - valLoss: 0.6561300158500671 - trainLoss: 0.6568844318389893\n",
      "cnt: 0 - valLoss: 0.6561287045478821 - trainLoss: 0.6568831205368042\n",
      "cnt: 0 - valLoss: 0.656127393245697 - trainLoss: 0.6568818092346191\n",
      "cnt: 0 - valLoss: 0.6561260223388672 - trainLoss: 0.6568804979324341\n",
      "cnt: 0 - valLoss: 0.6561247110366821 - trainLoss: 0.656879186630249\n",
      "cnt: 0 - valLoss: 0.6561233997344971 - trainLoss: 0.656877875328064\n",
      "cnt: 0 - valLoss: 0.6561220288276672 - trainLoss: 0.6568765044212341\n",
      "cnt: 0 - valLoss: 0.6561207175254822 - trainLoss: 0.6568751931190491\n",
      "cnt: 0 - valLoss: 0.6561194658279419 - trainLoss: 0.656873881816864\n",
      "cnt: 0 - valLoss: 0.6561180949211121 - trainLoss: 0.656872570514679\n",
      "cnt: 0 - valLoss: 0.6561167240142822 - trainLoss: 0.6568711996078491\n",
      "cnt: 0 - valLoss: 0.6561154127120972 - trainLoss: 0.6568698883056641\n",
      "cnt: 0 - valLoss: 0.6561141014099121 - trainLoss: 0.656868577003479\n",
      "cnt: 0 - valLoss: 0.6561128497123718 - trainLoss: 0.656867265701294\n",
      "cnt: 0 - valLoss: 0.6561115384101868 - trainLoss: 0.6568658947944641\n",
      "cnt: 0 - valLoss: 0.6561102867126465 - trainLoss: 0.6568646430969238\n",
      "cnt: 0 - valLoss: 0.656109094619751 - trainLoss: 0.6568633317947388\n",
      "cnt: 0 - valLoss: 0.6561077833175659 - trainLoss: 0.6568620800971985\n",
      "cnt: 0 - valLoss: 0.6561064720153809 - trainLoss: 0.656860888004303\n",
      "cnt: 0 - valLoss: 0.6561052799224854 - trainLoss: 0.6568595170974731\n",
      "cnt: 0 - valLoss: 0.6561040282249451 - trainLoss: 0.6568583250045776\n",
      "cnt: 0 - valLoss: 0.6561027765274048 - trainLoss: 0.6568570137023926\n",
      "cnt: 0 - valLoss: 0.6561014652252197 - trainLoss: 0.6568557620048523\n",
      "cnt: 0 - valLoss: 0.6561002135276794 - trainLoss: 0.656854510307312\n",
      "cnt: 0 - valLoss: 0.6560989618301392 - trainLoss: 0.656853199005127\n",
      "cnt: 0 - valLoss: 0.6560976505279541 - trainLoss: 0.6568519473075867\n",
      "cnt: 0 - valLoss: 0.6560963988304138 - trainLoss: 0.6568506956100464\n",
      "cnt: 0 - valLoss: 0.6560952067375183 - trainLoss: 0.6568493843078613\n",
      "cnt: 0 - valLoss: 0.6560938358306885 - trainLoss: 0.656848132610321\n",
      "cnt: 0 - valLoss: 0.6560925245285034 - trainLoss: 0.6568468809127808\n",
      "cnt: 0 - valLoss: 0.6560913324356079 - trainLoss: 0.6568456292152405\n",
      "cnt: 0 - valLoss: 0.6560900211334229 - trainLoss: 0.6568443179130554\n",
      "cnt: 0 - valLoss: 0.6560887694358826 - trainLoss: 0.6568430662155151\n",
      "cnt: 0 - valLoss: 0.6560875177383423 - trainLoss: 0.6568417549133301\n",
      "cnt: 0 - valLoss: 0.6560861468315125 - trainLoss: 0.6568405628204346\n",
      "cnt: 0 - valLoss: 0.6560848355293274 - trainLoss: 0.6568393111228943\n",
      "cnt: 0 - valLoss: 0.6560835242271423 - trainLoss: 0.6568379998207092\n",
      "cnt: 0 - valLoss: 0.6560822129249573 - trainLoss: 0.656836748123169\n",
      "cnt: 0 - valLoss: 0.656080961227417 - trainLoss: 0.6568354964256287\n",
      "cnt: 0 - valLoss: 0.6560796499252319 - trainLoss: 0.6568341851234436\n",
      "cnt: 0 - valLoss: 0.6560782194137573 - trainLoss: 0.6568329334259033\n",
      "cnt: 0 - valLoss: 0.6560769081115723 - trainLoss: 0.6568317413330078\n",
      "cnt: 0 - valLoss: 0.6560754776000977 - trainLoss: 0.6568305492401123\n",
      "cnt: 0 - valLoss: 0.6560741066932678 - trainLoss: 0.656829297542572\n",
      "cnt: 0 - valLoss: 0.6560727953910828 - trainLoss: 0.6568281054496765\n",
      "cnt: 0 - valLoss: 0.6560714244842529 - trainLoss: 0.6568267941474915\n",
      "cnt: 0 - valLoss: 0.6560700535774231 - trainLoss: 0.656825602054596\n",
      "cnt: 0 - valLoss: 0.6560688018798828 - trainLoss: 0.6568244695663452\n",
      "cnt: 0 - valLoss: 0.656067430973053 - trainLoss: 0.6568232178688049\n",
      "cnt: 0 - valLoss: 0.6560661196708679 - trainLoss: 0.6568220257759094\n",
      "cnt: 0 - valLoss: 0.6560647487640381 - trainLoss: 0.6568207740783691\n",
      "cnt: 0 - valLoss: 0.656063437461853 - trainLoss: 0.6568195223808289\n",
      "cnt: 0 - valLoss: 0.6560620665550232 - trainLoss: 0.6568183898925781\n",
      "cnt: 0 - valLoss: 0.6560607552528381 - trainLoss: 0.6568171381950378\n",
      "cnt: 0 - valLoss: 0.6560594439506531 - trainLoss: 0.6568159461021423\n",
      "cnt: 0 - valLoss: 0.6560580730438232 - trainLoss: 0.6568147540092468\n",
      "cnt: 0 - valLoss: 0.6560567617416382 - trainLoss: 0.6568135023117065\n",
      "cnt: 0 - valLoss: 0.6560555100440979 - trainLoss: 0.656812310218811\n",
      "cnt: 0 - valLoss: 0.6560541391372681 - trainLoss: 0.6568110585212708\n",
      "cnt: 0 - valLoss: 0.6560527682304382 - trainLoss: 0.6568098664283752\n",
      "cnt: 0 - valLoss: 0.6560514569282532 - trainLoss: 0.6568086743354797\n",
      "cnt: 0 - valLoss: 0.6560502052307129 - trainLoss: 0.6568074226379395\n",
      "cnt: 0 - valLoss: 0.6560488343238831 - trainLoss: 0.656806230545044\n",
      "cnt: 0 - valLoss: 0.6560474634170532 - trainLoss: 0.6568050384521484\n",
      "cnt: 0 - valLoss: 0.6560461521148682 - trainLoss: 0.6568037867546082\n",
      "cnt: 0 - valLoss: 0.6560447812080383 - trainLoss: 0.6568025946617126\n",
      "cnt: 0 - valLoss: 0.6560434699058533 - trainLoss: 0.6568014025688171\n",
      "cnt: 0 - valLoss: 0.6560421586036682 - trainLoss: 0.6568001508712769\n",
      "cnt: 0 - valLoss: 0.6560408473014832 - trainLoss: 0.6567988991737366\n",
      "cnt: 0 - valLoss: 0.6560394763946533 - trainLoss: 0.6567977070808411\n",
      "cnt: 0 - valLoss: 0.656038224697113 - trainLoss: 0.6567965149879456\n",
      "cnt: 0 - valLoss: 0.656036913394928 - trainLoss: 0.6567952632904053\n",
      "cnt: 0 - valLoss: 0.6560355424880981 - trainLoss: 0.656794011592865\n",
      "cnt: 0 - valLoss: 0.6560342311859131 - trainLoss: 0.6567928194999695\n",
      "cnt: 0 - valLoss: 0.656032919883728 - trainLoss: 0.656791627407074\n",
      "cnt: 0 - valLoss: 0.656031608581543 - trainLoss: 0.6567903757095337\n",
      "cnt: 0 - valLoss: 0.6560303568840027 - trainLoss: 0.6567891836166382\n",
      "cnt: 0 - valLoss: 0.6560289859771729 - trainLoss: 0.6567879915237427\n",
      "cnt: 0 - valLoss: 0.656027615070343 - trainLoss: 0.6567867398262024\n",
      "cnt: 0 - valLoss: 0.6560262441635132 - trainLoss: 0.6567855477333069\n",
      "cnt: 0 - valLoss: 0.6560248732566833 - trainLoss: 0.6567843556404114\n",
      "cnt: 0 - valLoss: 0.6560235023498535 - trainLoss: 0.6567831635475159\n",
      "cnt: 0 - valLoss: 0.6560221314430237 - trainLoss: 0.6567819118499756\n",
      "cnt: 0 - valLoss: 0.6560207009315491 - trainLoss: 0.6567807197570801\n",
      "cnt: 0 - valLoss: 0.6560193300247192 - trainLoss: 0.6567795276641846\n",
      "cnt: 0 - valLoss: 0.6560179591178894 - trainLoss: 0.6567782759666443\n",
      "cnt: 0 - valLoss: 0.6560165882110596 - trainLoss: 0.6567770838737488\n",
      "cnt: 0 - valLoss: 0.6560152173042297 - trainLoss: 0.6567758321762085\n",
      "cnt: 0 - valLoss: 0.6560138463973999 - trainLoss: 0.6567746996879578\n",
      "cnt: 0 - valLoss: 0.6560124754905701 - trainLoss: 0.6567734479904175\n",
      "cnt: 0 - valLoss: 0.6560111045837402 - trainLoss: 0.656772255897522\n",
      "cnt: 0 - valLoss: 0.6560096740722656 - trainLoss: 0.6567710638046265\n",
      "cnt: 0 - valLoss: 0.6560083627700806 - trainLoss: 0.6567698121070862\n",
      "cnt: 0 - valLoss: 0.656006932258606 - trainLoss: 0.6567686796188354\n",
      "cnt: 0 - valLoss: 0.6560056209564209 - trainLoss: 0.6567674279212952\n",
      "cnt: 0 - valLoss: 0.6560042500495911 - trainLoss: 0.6567662954330444\n",
      "cnt: 0 - valLoss: 0.6560028791427612 - trainLoss: 0.6567650437355042\n",
      "cnt: 0 - valLoss: 0.6560015082359314 - trainLoss: 0.6567638516426086\n",
      "cnt: 0 - valLoss: 0.6560001373291016 - trainLoss: 0.6567626595497131\n",
      "cnt: 0 - valLoss: 0.6559987664222717 - trainLoss: 0.6567614078521729\n",
      "cnt: 0 - valLoss: 0.6559973359107971 - trainLoss: 0.6567602157592773\n",
      "cnt: 0 - valLoss: 0.6559960246086121 - trainLoss: 0.6567590236663818\n",
      "cnt: 0 - valLoss: 0.6559946537017822 - trainLoss: 0.6567578315734863\n",
      "cnt: 0 - valLoss: 0.6559932827949524 - trainLoss: 0.6567566394805908\n",
      "cnt: 0 - valLoss: 0.6559918522834778 - trainLoss: 0.6567553877830505\n",
      "cnt: 0 - valLoss: 0.6559905409812927 - trainLoss: 0.656754195690155\n",
      "cnt: 0 - valLoss: 0.6559891700744629 - trainLoss: 0.6567529439926147\n",
      "cnt: 0 - valLoss: 0.6559877991676331 - trainLoss: 0.656751811504364\n",
      "cnt: 0 - valLoss: 0.6559864282608032 - trainLoss: 0.656750500202179\n",
      "cnt: 0 - valLoss: 0.6559851169586182 - trainLoss: 0.6567493677139282\n",
      "cnt: 0 - valLoss: 0.6559837460517883 - trainLoss: 0.6567481160163879\n",
      "cnt: 0 - valLoss: 0.6559824347496033 - trainLoss: 0.6567469239234924\n",
      "cnt: 0 - valLoss: 0.6559810638427734 - trainLoss: 0.6567457318305969\n",
      "cnt: 0 - valLoss: 0.6559798121452332 - trainLoss: 0.6567445397377014\n",
      "cnt: 0 - valLoss: 0.6559784412384033 - trainLoss: 0.6567433476448059\n",
      "cnt: 0 - valLoss: 0.6559771299362183 - trainLoss: 0.6567421555519104\n",
      "cnt: 0 - valLoss: 0.6559757590293884 - trainLoss: 0.6567409634590149\n",
      "cnt: 0 - valLoss: 0.6559745073318481 - trainLoss: 0.6567397117614746\n",
      "cnt: 0 - valLoss: 0.6559731364250183 - trainLoss: 0.6567384600639343\n",
      "cnt: 0 - valLoss: 0.6559717655181885 - trainLoss: 0.6567372679710388\n",
      "cnt: 0 - valLoss: 0.6559704542160034 - trainLoss: 0.6567360758781433\n",
      "cnt: 0 - valLoss: 0.6559691429138184 - trainLoss: 0.6567348837852478\n",
      "cnt: 0 - valLoss: 0.6559678912162781 - trainLoss: 0.6567336320877075\n",
      "cnt: 0 - valLoss: 0.655966579914093 - trainLoss: 0.656732439994812\n",
      "cnt: 0 - valLoss: 0.6559652090072632 - trainLoss: 0.6567312479019165\n",
      "cnt: 0 - valLoss: 0.6559639573097229 - trainLoss: 0.6567299962043762\n",
      "cnt: 0 - valLoss: 0.6559625864028931 - trainLoss: 0.6567288041114807\n",
      "cnt: 0 - valLoss: 0.6559613347053528 - trainLoss: 0.6567275524139404\n",
      "cnt: 0 - valLoss: 0.6559600234031677 - trainLoss: 0.6567263603210449\n",
      "cnt: 0 - valLoss: 0.6559587121009827 - trainLoss: 0.6567251682281494\n",
      "cnt: 0 - valLoss: 0.6559574007987976 - trainLoss: 0.6567239761352539\n",
      "cnt: 0 - valLoss: 0.6559561491012573 - trainLoss: 0.6567227244377136\n",
      "cnt: 0 - valLoss: 0.6559548377990723 - trainLoss: 0.6567215323448181\n",
      "cnt: 0 - valLoss: 0.6559535264968872 - trainLoss: 0.6567202806472778\n",
      "cnt: 0 - valLoss: 0.6559522151947021 - trainLoss: 0.6567190885543823\n",
      "cnt: 0 - valLoss: 0.6559509038925171 - trainLoss: 0.656717836856842\n",
      "cnt: 0 - valLoss: 0.655949592590332 - trainLoss: 0.6567167043685913\n",
      "cnt: 0 - valLoss: 0.655948281288147 - trainLoss: 0.656715452671051\n",
      "cnt: 0 - valLoss: 0.6559470295906067 - trainLoss: 0.6567142009735107\n",
      "cnt: 0 - valLoss: 0.6559456586837769 - trainLoss: 0.6567130088806152\n",
      "cnt: 0 - valLoss: 0.6559444069862366 - trainLoss: 0.656711757183075\n",
      "cnt: 0 - valLoss: 0.6559430956840515 - trainLoss: 0.6567105650901794\n",
      "cnt: 0 - valLoss: 0.6559417843818665 - trainLoss: 0.6567093729972839\n",
      "cnt: 0 - valLoss: 0.6559404730796814 - trainLoss: 0.6567081212997437\n",
      "cnt: 0 - valLoss: 0.6559392809867859 - trainLoss: 0.6567069292068481\n",
      "cnt: 0 - valLoss: 0.655937910079956 - trainLoss: 0.6567057371139526\n",
      "cnt: 0 - valLoss: 0.655936598777771 - trainLoss: 0.6567044854164124\n",
      "cnt: 0 - valLoss: 0.6559352874755859 - trainLoss: 0.6567032933235168\n",
      "cnt: 0 - valLoss: 0.6559339761734009 - trainLoss: 0.6567020416259766\n",
      "cnt: 0 - valLoss: 0.6559327244758606 - trainLoss: 0.656700849533081\n",
      "cnt: 0 - valLoss: 0.6559314727783203 - trainLoss: 0.6566996574401855\n",
      "cnt: 0 - valLoss: 0.6559301018714905 - trainLoss: 0.6566984057426453\n",
      "cnt: 0 - valLoss: 0.6559288501739502 - trainLoss: 0.6566972136497498\n",
      "cnt: 0 - valLoss: 0.6559275388717651 - trainLoss: 0.6566959619522095\n",
      "cnt: 0 - valLoss: 0.6559262871742249 - trainLoss: 0.656694769859314\n",
      "cnt: 0 - valLoss: 0.655924916267395 - trainLoss: 0.6566935181617737\n",
      "cnt: 0 - valLoss: 0.6559236645698547 - trainLoss: 0.6566923260688782\n",
      "cnt: 0 - valLoss: 0.6559222936630249 - trainLoss: 0.6566910147666931\n",
      "cnt: 0 - valLoss: 0.6559209823608398 - trainLoss: 0.6566898226737976\n",
      "cnt: 0 - valLoss: 0.6559197306632996 - trainLoss: 0.6566886305809021\n",
      "cnt: 0 - valLoss: 0.6559184193611145 - trainLoss: 0.6566874384880066\n",
      "cnt: 0 - valLoss: 0.6559171080589294 - trainLoss: 0.6566861867904663\n",
      "cnt: 0 - valLoss: 0.6559158563613892 - trainLoss: 0.656684935092926\n",
      "cnt: 0 - valLoss: 0.6559145450592041 - trainLoss: 0.6566837430000305\n",
      "cnt: 0 - valLoss: 0.6559131741523743 - trainLoss: 0.656682550907135\n",
      "cnt: 0 - valLoss: 0.655911922454834 - trainLoss: 0.6566812992095947\n",
      "cnt: 0 - valLoss: 0.6559106111526489 - trainLoss: 0.6566801071166992\n",
      "cnt: 0 - valLoss: 0.6559093594551086 - trainLoss: 0.6566788554191589\n",
      "cnt: 0 - valLoss: 0.6559081077575684 - trainLoss: 0.6566776633262634\n",
      "cnt: 0 - valLoss: 0.6559067368507385 - trainLoss: 0.6566764116287231\n",
      "cnt: 0 - valLoss: 0.6559054255485535 - trainLoss: 0.6566752195358276\n",
      "cnt: 0 - valLoss: 0.6559041142463684 - trainLoss: 0.6566739678382874\n",
      "cnt: 0 - valLoss: 0.6559028029441833 - trainLoss: 0.6566727757453918\n",
      "cnt: 0 - valLoss: 0.6559015512466431 - trainLoss: 0.6566715240478516\n",
      "cnt: 0 - valLoss: 0.6559001803398132 - trainLoss: 0.656670331954956\n",
      "cnt: 0 - valLoss: 0.6558988690376282 - trainLoss: 0.6566690802574158\n",
      "cnt: 0 - valLoss: 0.6558975577354431 - trainLoss: 0.6566678285598755\n",
      "cnt: 0 - valLoss: 0.6558962464332581 - trainLoss: 0.65666663646698\n",
      "cnt: 0 - valLoss: 0.6558948755264282 - trainLoss: 0.6566653847694397\n",
      "cnt: 0 - valLoss: 0.6558936238288879 - trainLoss: 0.6566641330718994\n",
      "cnt: 0 - valLoss: 0.6558922529220581 - trainLoss: 0.6566629409790039\n",
      "cnt: 0 - valLoss: 0.6558910012245178 - trainLoss: 0.6566616892814636\n",
      "cnt: 0 - valLoss: 0.6558896899223328 - trainLoss: 0.6566604971885681\n",
      "cnt: 0 - valLoss: 0.6558883190155029 - trainLoss: 0.6566592454910278\n",
      "cnt: 0 - valLoss: 0.6558870673179626 - trainLoss: 0.6566580533981323\n",
      "cnt: 0 - valLoss: 0.6558857560157776 - trainLoss: 0.656656801700592\n",
      "cnt: 0 - valLoss: 0.6558845043182373 - trainLoss: 0.6566556096076965\n",
      "cnt: 0 - valLoss: 0.6558831334114075 - trainLoss: 0.656654417514801\n",
      "cnt: 0 - valLoss: 0.6558818221092224 - trainLoss: 0.656653106212616\n",
      "cnt: 0 - valLoss: 0.6558805108070374 - trainLoss: 0.6566519141197205\n",
      "cnt: 0 - valLoss: 0.6558791995048523 - trainLoss: 0.656650722026825\n",
      "cnt: 0 - valLoss: 0.6558778882026672 - trainLoss: 0.6566494107246399\n",
      "cnt: 0 - valLoss: 0.6558765172958374 - trainLoss: 0.6566481590270996\n",
      "cnt: 0 - valLoss: 0.6558752059936523 - trainLoss: 0.6566469669342041\n",
      "cnt: 0 - valLoss: 0.6558739542961121 - trainLoss: 0.6566457152366638\n",
      "cnt: 0 - valLoss: 0.655872642993927 - trainLoss: 0.6566445231437683\n",
      "cnt: 0 - valLoss: 0.6558713912963867 - trainLoss: 0.6566433310508728\n",
      "cnt: 0 - valLoss: 0.6558701395988464 - trainLoss: 0.6566421389579773\n",
      "cnt: 0 - valLoss: 0.6558688879013062 - trainLoss: 0.656640887260437\n",
      "cnt: 0 - valLoss: 0.6558675765991211 - trainLoss: 0.6566396951675415\n",
      "cnt: 0 - valLoss: 0.6558663249015808 - trainLoss: 0.656638503074646\n",
      "cnt: 0 - valLoss: 0.6558650732040405 - trainLoss: 0.6566373109817505\n",
      "cnt: 0 - valLoss: 0.655863881111145 - trainLoss: 0.6566362380981445\n",
      "cnt: 0 - valLoss: 0.6558626294136047 - trainLoss: 0.6566351056098938\n",
      "cnt: 0 - valLoss: 0.6558614373207092 - trainLoss: 0.6566339731216431\n",
      "cnt: 0 - valLoss: 0.655860185623169 - trainLoss: 0.6566328406333923\n",
      "cnt: 0 - valLoss: 0.6558589339256287 - trainLoss: 0.6566316485404968\n",
      "cnt: 0 - valLoss: 0.6558576226234436 - trainLoss: 0.6566305756568909\n",
      "cnt: 0 - valLoss: 0.6558564305305481 - trainLoss: 0.6566293835639954\n",
      "cnt: 0 - valLoss: 0.6558551788330078 - trainLoss: 0.6566282510757446\n",
      "cnt: 0 - valLoss: 0.6558539271354675 - trainLoss: 0.6566270589828491\n",
      "cnt: 0 - valLoss: 0.6558526754379272 - trainLoss: 0.6566259860992432\n",
      "cnt: 0 - valLoss: 0.6558513641357422 - trainLoss: 0.6566248536109924\n",
      "cnt: 0 - valLoss: 0.6558501720428467 - trainLoss: 0.6566236615180969\n",
      "cnt: 0 - valLoss: 0.6558489203453064 - trainLoss: 0.6566225290298462\n",
      "cnt: 0 - valLoss: 0.6558476686477661 - trainLoss: 0.6566214561462402\n",
      "cnt: 0 - valLoss: 0.6558464169502258 - trainLoss: 0.6566202640533447\n",
      "cnt: 0 - valLoss: 0.6558451056480408 - trainLoss: 0.656619131565094\n",
      "cnt: 0 - valLoss: 0.6558439135551453 - trainLoss: 0.6566179990768433\n",
      "cnt: 0 - valLoss: 0.6558426022529602 - trainLoss: 0.6566168665885925\n",
      "cnt: 0 - valLoss: 0.6558413505554199 - trainLoss: 0.656615674495697\n",
      "cnt: 0 - valLoss: 0.6558400988578796 - trainLoss: 0.6566145420074463\n",
      "cnt: 0 - valLoss: 0.6558388471603394 - trainLoss: 0.6566134095191956\n",
      "cnt: 0 - valLoss: 0.6558375358581543 - trainLoss: 0.6566122770309448\n",
      "cnt: 0 - valLoss: 0.655836284160614 - trainLoss: 0.6566111445426941\n",
      "cnt: 0 - valLoss: 0.6558350324630737 - trainLoss: 0.6566100120544434\n",
      "cnt: 0 - valLoss: 0.6558337211608887 - trainLoss: 0.6566088199615479\n",
      "cnt: 0 - valLoss: 0.6558324694633484 - trainLoss: 0.6566077470779419\n",
      "cnt: 0 - valLoss: 0.6558312177658081 - trainLoss: 0.6566066145896912\n",
      "cnt: 0 - valLoss: 0.655829906463623 - trainLoss: 0.6566054224967957\n",
      "cnt: 0 - valLoss: 0.6558287143707275 - trainLoss: 0.6566042304039001\n",
      "cnt: 0 - valLoss: 0.6558274030685425 - trainLoss: 0.656603217124939\n",
      "cnt: 0 - valLoss: 0.6558261513710022 - trainLoss: 0.6566020250320435\n",
      "cnt: 0 - valLoss: 0.6558248400688171 - trainLoss: 0.6566008925437927\n",
      "cnt: 0 - valLoss: 0.6558235883712769 - trainLoss: 0.656599760055542\n",
      "cnt: 0 - valLoss: 0.6558223366737366 - trainLoss: 0.6565985679626465\n",
      "cnt: 0 - valLoss: 0.6558210253715515 - trainLoss: 0.6565974354743958\n",
      "cnt: 0 - valLoss: 0.6558197736740112 - trainLoss: 0.656596302986145\n",
      "cnt: 0 - valLoss: 0.6558184623718262 - trainLoss: 0.6565951108932495\n",
      "cnt: 0 - valLoss: 0.6558172702789307 - trainLoss: 0.6565939784049988\n",
      "cnt: 0 - valLoss: 0.6558159589767456 - trainLoss: 0.656592845916748\n",
      "cnt: 0 - valLoss: 0.6558147668838501 - trainLoss: 0.6565917134284973\n",
      "cnt: 0 - valLoss: 0.655813455581665 - trainLoss: 0.6565905213356018\n",
      "cnt: 0 - valLoss: 0.6558122038841248 - trainLoss: 0.6565893888473511\n",
      "cnt: 0 - valLoss: 0.6558109521865845 - trainLoss: 0.6565881967544556\n",
      "cnt: 0 - valLoss: 0.6558097004890442 - trainLoss: 0.6565871238708496\n",
      "cnt: 0 - valLoss: 0.6558083891868591 - trainLoss: 0.6565859317779541\n",
      "cnt: 0 - valLoss: 0.6558071374893188 - trainLoss: 0.6565847992897034\n",
      "cnt: 0 - valLoss: 0.6558058261871338 - trainLoss: 0.6565836668014526\n",
      "cnt: 0 - valLoss: 0.6558045744895935 - trainLoss: 0.6565824747085571\n",
      "cnt: 0 - valLoss: 0.6558033227920532 - trainLoss: 0.6565813422203064\n",
      "cnt: 0 - valLoss: 0.6558020114898682 - trainLoss: 0.6565801501274109\n",
      "cnt: 0 - valLoss: 0.6558007001876831 - trainLoss: 0.6565790176391602\n",
      "cnt: 0 - valLoss: 0.6557995080947876 - trainLoss: 0.6565778851509094\n",
      "cnt: 0 - valLoss: 0.6557982563972473 - trainLoss: 0.6565766334533691\n",
      "cnt: 0 - valLoss: 0.6557969450950623 - trainLoss: 0.6565754413604736\n",
      "cnt: 0 - valLoss: 0.655795693397522 - trainLoss: 0.6565743684768677\n",
      "cnt: 0 - valLoss: 0.6557944416999817 - trainLoss: 0.6565731763839722\n",
      "cnt: 0 - valLoss: 0.6557932496070862 - trainLoss: 0.6565720438957214\n",
      "cnt: 0 - valLoss: 0.6557918787002563 - trainLoss: 0.6565708518028259\n",
      "cnt: 0 - valLoss: 0.6557906270027161 - trainLoss: 0.6565696597099304\n",
      "cnt: 0 - valLoss: 0.6557894349098206 - trainLoss: 0.6565684676170349\n",
      "cnt: 0 - valLoss: 0.6557881236076355 - trainLoss: 0.6565672755241394\n",
      "cnt: 0 - valLoss: 0.6557868123054504 - trainLoss: 0.6565660238265991\n",
      "cnt: 0 - valLoss: 0.6557856202125549 - trainLoss: 0.6565648317337036\n",
      "cnt: 0 - valLoss: 0.6557842493057251 - trainLoss: 0.6565637588500977\n",
      "cnt: 0 - valLoss: 0.6557829976081848 - trainLoss: 0.6565625667572021\n",
      "cnt: 0 - valLoss: 0.6557817459106445 - trainLoss: 0.6565613746643066\n",
      "cnt: 0 - valLoss: 0.6557804942131042 - trainLoss: 0.6565601229667664\n",
      "cnt: 0 - valLoss: 0.6557791829109192 - trainLoss: 0.6565589904785156\n",
      "cnt: 0 - valLoss: 0.6557779312133789 - trainLoss: 0.6565577983856201\n",
      "cnt: 0 - valLoss: 0.6557766199111938 - trainLoss: 0.6565566658973694\n",
      "cnt: 0 - valLoss: 0.6557753682136536 - trainLoss: 0.6565554141998291\n",
      "cnt: 0 - valLoss: 0.6557740569114685 - trainLoss: 0.6565542817115784\n",
      "cnt: 0 - valLoss: 0.6557728052139282 - trainLoss: 0.6565530896186829\n",
      "cnt: 0 - valLoss: 0.6557715535163879 - trainLoss: 0.6565518975257874\n",
      "cnt: 0 - valLoss: 0.6557703614234924 - trainLoss: 0.6565507054328918\n",
      "cnt: 0 - valLoss: 0.6557691097259521 - trainLoss: 0.6565495133399963\n",
      "cnt: 0 - valLoss: 0.6557677984237671 - trainLoss: 0.6565483808517456\n",
      "cnt: 0 - valLoss: 0.6557665467262268 - trainLoss: 0.6565471291542053\n",
      "cnt: 0 - valLoss: 0.6557652354240417 - trainLoss: 0.6565459966659546\n",
      "cnt: 0 - valLoss: 0.6557640433311462 - trainLoss: 0.6565448045730591\n",
      "cnt: 0 - valLoss: 0.6557627320289612 - trainLoss: 0.6565435528755188\n",
      "cnt: 0 - valLoss: 0.6557614207267761 - trainLoss: 0.6565424203872681\n",
      "cnt: 0 - valLoss: 0.6557601690292358 - trainLoss: 0.6565412282943726\n",
      "cnt: 0 - valLoss: 0.6557589173316956 - trainLoss: 0.656540036201477\n",
      "cnt: 0 - valLoss: 0.6557576060295105 - trainLoss: 0.6565388441085815\n",
      "cnt: 0 - valLoss: 0.6557563543319702 - trainLoss: 0.656537652015686\n",
      "cnt: 0 - valLoss: 0.6557550430297852 - trainLoss: 0.6565364003181458\n",
      "cnt: 0 - valLoss: 0.6557537317276001 - trainLoss: 0.656535267829895\n",
      "cnt: 0 - valLoss: 0.6557524800300598 - trainLoss: 0.6565341353416443\n",
      "cnt: 0 - valLoss: 0.6557512283325195 - trainLoss: 0.6565329432487488\n",
      "cnt: 0 - valLoss: 0.655750036239624 - trainLoss: 0.6565316915512085\n",
      "cnt: 0 - valLoss: 0.655748724937439 - trainLoss: 0.656530499458313\n",
      "cnt: 0 - valLoss: 0.6557474732398987 - trainLoss: 0.6565293073654175\n",
      "cnt: 0 - valLoss: 0.6557461619377136 - trainLoss: 0.6565282344818115\n",
      "cnt: 0 - valLoss: 0.6557448506355286 - trainLoss: 0.6565269827842712\n",
      "cnt: 0 - valLoss: 0.6557435989379883 - trainLoss: 0.6565257906913757\n",
      "cnt: 0 - valLoss: 0.6557422876358032 - trainLoss: 0.6565245985984802\n",
      "cnt: 0 - valLoss: 0.6557410359382629 - trainLoss: 0.6565234661102295\n",
      "cnt: 0 - valLoss: 0.6557397246360779 - trainLoss: 0.656522274017334\n",
      "cnt: 0 - valLoss: 0.6557384729385376 - trainLoss: 0.6565210223197937\n",
      "cnt: 0 - valLoss: 0.6557371616363525 - trainLoss: 0.656519889831543\n",
      "cnt: 0 - valLoss: 0.6557358503341675 - trainLoss: 0.6565186977386475\n",
      "cnt: 0 - valLoss: 0.655734658241272 - trainLoss: 0.6565174460411072\n",
      "cnt: 0 - valLoss: 0.6557332873344421 - trainLoss: 0.6565163135528564\n",
      "cnt: 0 - valLoss: 0.6557320356369019 - trainLoss: 0.6565151214599609\n",
      "cnt: 0 - valLoss: 0.6557307839393616 - trainLoss: 0.6565139293670654\n",
      "cnt: 0 - valLoss: 0.6557294726371765 - trainLoss: 0.6565127372741699\n",
      "cnt: 0 - valLoss: 0.6557282209396362 - trainLoss: 0.6565115451812744\n",
      "cnt: 0 - valLoss: 0.655726969242096 - trainLoss: 0.6565102934837341\n",
      "cnt: 0 - valLoss: 0.6557256579399109 - trainLoss: 0.6565092206001282\n",
      "cnt: 0 - valLoss: 0.6557244062423706 - trainLoss: 0.6565080285072327\n",
      "cnt: 0 - valLoss: 0.6557230353355408 - trainLoss: 0.6565067768096924\n",
      "cnt: 0 - valLoss: 0.6557218432426453 - trainLoss: 0.6565055251121521\n",
      "cnt: 0 - valLoss: 0.6557205319404602 - trainLoss: 0.6565043926239014\n",
      "cnt: 0 - valLoss: 0.6557192206382751 - trainLoss: 0.6565032005310059\n",
      "cnt: 0 - valLoss: 0.6557179689407349 - trainLoss: 0.6565020084381104\n",
      "cnt: 0 - valLoss: 0.6557166576385498 - trainLoss: 0.6565007567405701\n",
      "cnt: 0 - valLoss: 0.6557153463363647 - trainLoss: 0.6564996242523193\n",
      "cnt: 0 - valLoss: 0.6557140350341797 - trainLoss: 0.6564984321594238\n",
      "cnt: 0 - valLoss: 0.6557127833366394 - trainLoss: 0.6564972400665283\n",
      "cnt: 0 - valLoss: 0.6557114124298096 - trainLoss: 0.6564960479736328\n",
      "cnt: 0 - valLoss: 0.6557101607322693 - trainLoss: 0.6564947962760925\n",
      "cnt: 0 - valLoss: 0.655708909034729 - trainLoss: 0.656493604183197\n",
      "cnt: 0 - valLoss: 0.655707597732544 - trainLoss: 0.6564924120903015\n",
      "cnt: 0 - valLoss: 0.6557063460350037 - trainLoss: 0.6564912796020508\n",
      "cnt: 0 - valLoss: 0.6557050347328186 - trainLoss: 0.6564900875091553\n",
      "cnt: 0 - valLoss: 0.6557037830352783 - trainLoss: 0.6564888954162598\n",
      "cnt: 0 - valLoss: 0.6557024121284485 - trainLoss: 0.6564877033233643\n",
      "cnt: 0 - valLoss: 0.655701220035553 - trainLoss: 0.6564865112304688\n",
      "cnt: 0 - valLoss: 0.6556999087333679 - trainLoss: 0.6564853191375732\n",
      "cnt: 0 - valLoss: 0.6556985974311829 - trainLoss: 0.6564841270446777\n",
      "cnt: 0 - valLoss: 0.6556972861289978 - trainLoss: 0.656482994556427\n",
      "cnt: 0 - valLoss: 0.6556960344314575 - trainLoss: 0.6564817428588867\n",
      "cnt: 0 - valLoss: 0.6556947827339172 - trainLoss: 0.6564805507659912\n",
      "cnt: 0 - valLoss: 0.6556934118270874 - trainLoss: 0.6564793586730957\n",
      "cnt: 0 - valLoss: 0.6556920409202576 - trainLoss: 0.6564781069755554\n",
      "cnt: 0 - valLoss: 0.6556908488273621 - trainLoss: 0.6564768552780151\n",
      "cnt: 0 - valLoss: 0.655689537525177 - trainLoss: 0.6564756631851196\n",
      "cnt: 0 - valLoss: 0.6556881666183472 - trainLoss: 0.6564744710922241\n",
      "cnt: 0 - valLoss: 0.6556869745254517 - trainLoss: 0.6564732789993286\n",
      "cnt: 0 - valLoss: 0.6556856632232666 - trainLoss: 0.6564719676971436\n",
      "cnt: 0 - valLoss: 0.6556844115257263 - trainLoss: 0.6564708352088928\n",
      "cnt: 0 - valLoss: 0.655683159828186 - trainLoss: 0.6564695835113525\n",
      "cnt: 0 - valLoss: 0.655681848526001 - trainLoss: 0.656468391418457\n",
      "cnt: 0 - valLoss: 0.6556805372238159 - trainLoss: 0.6564671993255615\n",
      "cnt: 0 - valLoss: 0.6556792259216309 - trainLoss: 0.6564659476280212\n",
      "cnt: 0 - valLoss: 0.6556779146194458 - trainLoss: 0.6564647555351257\n",
      "cnt: 0 - valLoss: 0.6556766629219055 - trainLoss: 0.6564635634422302\n",
      "cnt: 0 - valLoss: 0.6556752920150757 - trainLoss: 0.6564623117446899\n",
      "cnt: 0 - valLoss: 0.6556740403175354 - trainLoss: 0.6564611196517944\n",
      "cnt: 0 - valLoss: 0.6556727290153503 - trainLoss: 0.6564598679542542\n",
      "cnt: 0 - valLoss: 0.6556714773178101 - trainLoss: 0.6564586162567139\n",
      "cnt: 0 - valLoss: 0.655670166015625 - trainLoss: 0.6564574241638184\n",
      "cnt: 0 - valLoss: 0.6556688547134399 - trainLoss: 0.6564561724662781\n",
      "cnt: 0 - valLoss: 0.6556675434112549 - trainLoss: 0.6564549207687378\n",
      "cnt: 0 - valLoss: 0.6556662917137146 - trainLoss: 0.6564537882804871\n",
      "cnt: 0 - valLoss: 0.6556649804115295 - trainLoss: 0.6564525365829468\n",
      "cnt: 0 - valLoss: 0.6556637287139893 - trainLoss: 0.6564513444900513\n",
      "cnt: 0 - valLoss: 0.655662477016449 - trainLoss: 0.6564501523971558\n",
      "cnt: 0 - valLoss: 0.6556612253189087 - trainLoss: 0.6564489006996155\n",
      "cnt: 0 - valLoss: 0.6556599736213684 - trainLoss: 0.6564476490020752\n",
      "cnt: 0 - valLoss: 0.6556587815284729 - trainLoss: 0.6564464569091797\n",
      "cnt: 0 - valLoss: 0.6556574702262878 - trainLoss: 0.6564452648162842\n",
      "cnt: 0 - valLoss: 0.6556562781333923 - trainLoss: 0.6564439535140991\n",
      "cnt: 0 - valLoss: 0.6556550860404968 - trainLoss: 0.6564427614212036\n",
      "cnt: 0 - valLoss: 0.6556538343429565 - trainLoss: 0.6564415693283081\n",
      "cnt: 0 - valLoss: 0.6556525826454163 - trainLoss: 0.6564403772354126\n",
      "cnt: 0 - valLoss: 0.655651330947876 - trainLoss: 0.6564391255378723\n",
      "cnt: 0 - valLoss: 0.6556500792503357 - trainLoss: 0.656437873840332\n",
      "cnt: 0 - valLoss: 0.6556488871574402 - trainLoss: 0.6564366817474365\n",
      "cnt: 0 - valLoss: 0.6556476354598999 - trainLoss: 0.6564354300498962\n",
      "cnt: 0 - valLoss: 0.6556464433670044 - trainLoss: 0.6564342379570007\n",
      "cnt: 0 - valLoss: 0.6556451320648193 - trainLoss: 0.6564330458641052\n",
      "cnt: 0 - valLoss: 0.6556439995765686 - trainLoss: 0.6564317941665649\n",
      "cnt: 0 - valLoss: 0.6556428074836731 - trainLoss: 0.6564306020736694\n",
      "cnt: 0 - valLoss: 0.6556415557861328 - trainLoss: 0.6564294099807739\n",
      "cnt: 0 - valLoss: 0.6556403636932373 - trainLoss: 0.6564281582832336\n",
      "cnt: 0 - valLoss: 0.6556390523910522 - trainLoss: 0.6564269661903381\n",
      "cnt: 0 - valLoss: 0.6556378602981567 - trainLoss: 0.6564257144927979\n",
      "cnt: 0 - valLoss: 0.6556366682052612 - trainLoss: 0.6564245223999023\n",
      "cnt: 0 - valLoss: 0.655635416507721 - trainLoss: 0.6564232707023621\n",
      "cnt: 0 - valLoss: 0.6556341648101807 - trainLoss: 0.6564220190048218\n",
      "cnt: 0 - valLoss: 0.6556328535079956 - trainLoss: 0.6564208269119263\n",
      "cnt: 0 - valLoss: 0.6556316614151001 - trainLoss: 0.6564196348190308\n",
      "cnt: 0 - valLoss: 0.6556304097175598 - trainLoss: 0.6564183831214905\n",
      "cnt: 0 - valLoss: 0.6556292176246643 - trainLoss: 0.656417191028595\n",
      "cnt: 0 - valLoss: 0.655627965927124 - trainLoss: 0.6564159989356995\n",
      "cnt: 0 - valLoss: 0.6556267142295837 - trainLoss: 0.6564146876335144\n",
      "cnt: 0 - valLoss: 0.6556255221366882 - trainLoss: 0.6564134359359741\n",
      "cnt: 0 - valLoss: 0.6556242108345032 - trainLoss: 0.6564122438430786\n",
      "cnt: 0 - valLoss: 0.6556230783462524 - trainLoss: 0.6564110517501831\n",
      "cnt: 0 - valLoss: 0.6556218266487122 - trainLoss: 0.6564098000526428\n",
      "cnt: 0 - valLoss: 0.6556206345558167 - trainLoss: 0.6564086079597473\n",
      "cnt: 0 - valLoss: 0.6556193232536316 - trainLoss: 0.6564074158668518\n",
      "cnt: 0 - valLoss: 0.6556181311607361 - trainLoss: 0.6564061641693115\n",
      "cnt: 0 - valLoss: 0.6556169390678406 - trainLoss: 0.6564049124717712\n",
      "cnt: 0 - valLoss: 0.6556156277656555 - trainLoss: 0.6564037203788757\n",
      "cnt: 0 - valLoss: 0.6556143760681152 - trainLoss: 0.6564025282859802\n",
      "cnt: 0 - valLoss: 0.6556131839752197 - trainLoss: 0.6564012765884399\n",
      "cnt: 0 - valLoss: 0.6556118726730347 - trainLoss: 0.6564000844955444\n",
      "cnt: 0 - valLoss: 0.6556106805801392 - trainLoss: 0.6563987731933594\n",
      "cnt: 0 - valLoss: 0.6556094884872437 - trainLoss: 0.6563976407051086\n",
      "cnt: 0 - valLoss: 0.6556082963943481 - trainLoss: 0.6563964486122131\n",
      "cnt: 0 - valLoss: 0.6556069850921631 - trainLoss: 0.6563951969146729\n",
      "cnt: 0 - valLoss: 0.655605673789978 - trainLoss: 0.6563938856124878\n",
      "cnt: 0 - valLoss: 0.6556044816970825 - trainLoss: 0.6563927531242371\n",
      "cnt: 0 - valLoss: 0.6556032299995422 - trainLoss: 0.6563915014266968\n",
      "cnt: 0 - valLoss: 0.6556020379066467 - trainLoss: 0.6563903093338013\n",
      "cnt: 0 - valLoss: 0.6556008458137512 - trainLoss: 0.6563891172409058\n",
      "cnt: 0 - valLoss: 0.6555995941162109 - trainLoss: 0.6563878655433655\n",
      "cnt: 0 - valLoss: 0.6555984020233154 - trainLoss: 0.6563866138458252\n",
      "cnt: 0 - valLoss: 0.6555971503257751 - trainLoss: 0.6563854217529297\n",
      "cnt: 0 - valLoss: 0.6555958390235901 - trainLoss: 0.6563841700553894\n",
      "cnt: 0 - valLoss: 0.6555946469306946 - trainLoss: 0.6563829779624939\n",
      "cnt: 0 - valLoss: 0.6555933952331543 - trainLoss: 0.6563817262649536\n",
      "cnt: 0 - valLoss: 0.655592143535614 - trainLoss: 0.6563804745674133\n",
      "cnt: 0 - valLoss: 0.6555909514427185 - trainLoss: 0.6563792824745178\n",
      "cnt: 0 - valLoss: 0.6555896997451782 - trainLoss: 0.6563780903816223\n",
      "cnt: 0 - valLoss: 0.6555884480476379 - trainLoss: 0.656376838684082\n",
      "cnt: 0 - valLoss: 0.6555871963500977 - trainLoss: 0.6563756465911865\n",
      "cnt: 0 - valLoss: 0.6555859446525574 - trainLoss: 0.6563743948936462\n",
      "cnt: 0 - valLoss: 0.6555846929550171 - trainLoss: 0.6563730835914612\n",
      "cnt: 0 - valLoss: 0.6555834412574768 - trainLoss: 0.6563718914985657\n",
      "cnt: 0 - valLoss: 0.6555823087692261 - trainLoss: 0.6563706994056702\n",
      "cnt: 0 - valLoss: 0.6555810570716858 - trainLoss: 0.6563695073127747\n",
      "cnt: 0 - valLoss: 0.6555798053741455 - trainLoss: 0.6563682556152344\n",
      "cnt: 0 - valLoss: 0.6555785536766052 - trainLoss: 0.6563670039176941\n",
      "cnt: 0 - valLoss: 0.6555773019790649 - trainLoss: 0.6563657522201538\n",
      "cnt: 0 - valLoss: 0.6555760502815247 - trainLoss: 0.6563645601272583\n",
      "cnt: 0 - valLoss: 0.6555747985839844 - trainLoss: 0.6563633680343628\n",
      "cnt: 0 - valLoss: 0.6555735468864441 - trainLoss: 0.6563621163368225\n",
      "cnt: 0 - valLoss: 0.6555724143981934 - trainLoss: 0.6563608646392822\n",
      "cnt: 0 - valLoss: 0.6555711627006531 - trainLoss: 0.6563596725463867\n",
      "cnt: 0 - valLoss: 0.6555700302124023 - trainLoss: 0.6563584804534912\n",
      "cnt: 0 - valLoss: 0.6555687785148621 - trainLoss: 0.6563572883605957\n",
      "cnt: 0 - valLoss: 0.6555675864219666 - trainLoss: 0.6563560366630554\n",
      "cnt: 0 - valLoss: 0.6555663347244263 - trainLoss: 0.6563548445701599\n",
      "cnt: 0 - valLoss: 0.6555652618408203 - trainLoss: 0.6563536524772644\n",
      "cnt: 0 - valLoss: 0.6555640697479248 - trainLoss: 0.6563524007797241\n",
      "cnt: 0 - valLoss: 0.6555629372596741 - trainLoss: 0.6563512086868286\n",
      "cnt: 0 - valLoss: 0.6555617451667786 - trainLoss: 0.6563499569892883\n",
      "cnt: 0 - valLoss: 0.6555605530738831 - trainLoss: 0.6563487648963928\n",
      "cnt: 0 - valLoss: 0.6555593013763428 - trainLoss: 0.6563475728034973\n",
      "cnt: 0 - valLoss: 0.6555581092834473 - trainLoss: 0.656346321105957\n",
      "cnt: 0 - valLoss: 0.6555569171905518 - trainLoss: 0.6563451886177063\n",
      "cnt: 0 - valLoss: 0.6555557250976562 - trainLoss: 0.6563438773155212\n",
      "cnt: 0 - valLoss: 0.655554473400116 - trainLoss: 0.6563427448272705\n",
      "cnt: 0 - valLoss: 0.6555532813072205 - trainLoss: 0.656341552734375\n",
      "cnt: 0 - valLoss: 0.6555521488189697 - trainLoss: 0.6563403010368347\n",
      "cnt: 0 - valLoss: 0.6555508971214294 - trainLoss: 0.6563391089439392\n",
      "cnt: 0 - valLoss: 0.6555497050285339 - trainLoss: 0.6563379168510437\n",
      "cnt: 0 - valLoss: 0.6555485725402832 - trainLoss: 0.6563366651535034\n",
      "cnt: 0 - valLoss: 0.6555473804473877 - trainLoss: 0.6563354730606079\n",
      "cnt: 0 - valLoss: 0.6555460691452026 - trainLoss: 0.6563342213630676\n",
      "cnt: 0 - valLoss: 0.6555449366569519 - trainLoss: 0.6563330292701721\n",
      "cnt: 0 - valLoss: 0.6555438041687012 - trainLoss: 0.6563318371772766\n",
      "cnt: 0 - valLoss: 0.6555426120758057 - trainLoss: 0.6563306450843811\n",
      "cnt: 0 - valLoss: 0.6555414199829102 - trainLoss: 0.6563293933868408\n",
      "cnt: 0 - valLoss: 0.6555401682853699 - trainLoss: 0.6563282012939453\n",
      "cnt: 0 - valLoss: 0.6555390357971191 - trainLoss: 0.6563270092010498\n",
      "cnt: 0 - valLoss: 0.6555377840995789 - trainLoss: 0.6563257575035095\n",
      "cnt: 0 - valLoss: 0.6555365920066833 - trainLoss: 0.656324565410614\n",
      "cnt: 0 - valLoss: 0.6555353403091431 - trainLoss: 0.6563233137130737\n",
      "cnt: 0 - valLoss: 0.6555342078208923 - trainLoss: 0.6563220620155334\n",
      "cnt: 0 - valLoss: 0.655532956123352 - trainLoss: 0.6563208699226379\n",
      "cnt: 0 - valLoss: 0.6555317640304565 - trainLoss: 0.6563196778297424\n",
      "cnt: 0 - valLoss: 0.655530571937561 - trainLoss: 0.6563184857368469\n",
      "cnt: 0 - valLoss: 0.6555293798446655 - trainLoss: 0.6563171744346619\n",
      "cnt: 0 - valLoss: 0.65552818775177 - trainLoss: 0.6563159823417664\n",
      "cnt: 0 - valLoss: 0.6555269956588745 - trainLoss: 0.6563147902488708\n",
      "cnt: 0 - valLoss: 0.655525803565979 - trainLoss: 0.6563135981559753\n",
      "cnt: 0 - valLoss: 0.6555246710777283 - trainLoss: 0.6563123464584351\n",
      "cnt: 0 - valLoss: 0.6555234789848328 - trainLoss: 0.6563111543655396\n",
      "cnt: 0 - valLoss: 0.6555222272872925 - trainLoss: 0.656309962272644\n",
      "cnt: 0 - valLoss: 0.655521035194397 - trainLoss: 0.6563087105751038\n",
      "cnt: 0 - valLoss: 0.6555197834968567 - trainLoss: 0.6563075184822083\n",
      "cnt: 0 - valLoss: 0.6555187106132507 - trainLoss: 0.6563063263893127\n",
      "cnt: 0 - valLoss: 0.6555173993110657 - trainLoss: 0.6563051342964172\n",
      "cnt: 0 - valLoss: 0.6555162668228149 - trainLoss: 0.6563038229942322\n",
      "cnt: 0 - valLoss: 0.6555150747299194 - trainLoss: 0.6563026309013367\n",
      "cnt: 0 - valLoss: 0.6555138230323792 - trainLoss: 0.6563013792037964\n",
      "cnt: 0 - valLoss: 0.6555126309394836 - trainLoss: 0.6563001871109009\n",
      "cnt: 0 - valLoss: 0.6555113792419434 - trainLoss: 0.6562989354133606\n",
      "cnt: 0 - valLoss: 0.6555101871490479 - trainLoss: 0.6562977433204651\n",
      "cnt: 0 - valLoss: 0.6555089950561523 - trainLoss: 0.6562965512275696\n",
      "cnt: 0 - valLoss: 0.6555078029632568 - trainLoss: 0.6562952995300293\n",
      "cnt: 0 - valLoss: 0.6555065512657166 - trainLoss: 0.6562941074371338\n",
      "cnt: 0 - valLoss: 0.6555054187774658 - trainLoss: 0.6562929153442383\n",
      "cnt: 0 - valLoss: 0.6555042862892151 - trainLoss: 0.6562916040420532\n",
      "cnt: 0 - valLoss: 0.6555030941963196 - trainLoss: 0.6562902927398682\n",
      "cnt: 0 - valLoss: 0.6555019021034241 - trainLoss: 0.6562891006469727\n",
      "cnt: 0 - valLoss: 0.6555007100105286 - trainLoss: 0.6562877893447876\n",
      "cnt: 0 - valLoss: 0.6554995179176331 - trainLoss: 0.6562865376472473\n",
      "cnt: 0 - valLoss: 0.6554982662200928 - trainLoss: 0.656285285949707\n",
      "cnt: 0 - valLoss: 0.6554970741271973 - trainLoss: 0.656283974647522\n",
      "cnt: 0 - valLoss: 0.6554959416389465 - trainLoss: 0.6562827229499817\n",
      "cnt: 0 - valLoss: 0.6554946899414062 - trainLoss: 0.6562814712524414\n",
      "cnt: 0 - valLoss: 0.6554934978485107 - trainLoss: 0.6562801599502563\n",
      "cnt: 0 - valLoss: 0.6554923057556152 - trainLoss: 0.6562789082527161\n",
      "cnt: 0 - valLoss: 0.6554911136627197 - trainLoss: 0.6562776565551758\n",
      "cnt: 0 - valLoss: 0.6554898619651794 - trainLoss: 0.6562764048576355\n",
      "cnt: 0 - valLoss: 0.6554886698722839 - trainLoss: 0.6562752723693848\n",
      "cnt: 0 - valLoss: 0.6554874777793884 - trainLoss: 0.6562740206718445\n",
      "cnt: 0 - valLoss: 0.6554863452911377 - trainLoss: 0.656272828578949\n",
      "cnt: 0 - valLoss: 0.6554851531982422 - trainLoss: 0.6562715768814087\n",
      "cnt: 0 - valLoss: 0.6554839015007019 - trainLoss: 0.6562703847885132\n",
      "cnt: 0 - valLoss: 0.6554827094078064 - trainLoss: 0.6562691926956177\n",
      "cnt: 0 - valLoss: 0.6554815173149109 - trainLoss: 0.6562680006027222\n",
      "cnt: 0 - valLoss: 0.6554802656173706 - trainLoss: 0.6562667489051819\n",
      "cnt: 0 - valLoss: 0.6554790735244751 - trainLoss: 0.6562655568122864\n",
      "cnt: 0 - valLoss: 0.6554778814315796 - trainLoss: 0.6562643647193909\n",
      "cnt: 0 - valLoss: 0.6554766893386841 - trainLoss: 0.6562631130218506\n",
      "cnt: 0 - valLoss: 0.6554754376411438 - trainLoss: 0.6562618017196655\n",
      "cnt: 0 - valLoss: 0.6554743051528931 - trainLoss: 0.65626060962677\n",
      "cnt: 0 - valLoss: 0.6554730534553528 - trainLoss: 0.6562594175338745\n",
      "cnt: 0 - valLoss: 0.6554718613624573 - trainLoss: 0.6562581658363342\n",
      "cnt: 0 - valLoss: 0.655470609664917 - trainLoss: 0.6562569737434387\n",
      "cnt: 0 - valLoss: 0.6554694175720215 - trainLoss: 0.6562557220458984\n",
      "cnt: 0 - valLoss: 0.655468225479126 - trainLoss: 0.6562545299530029\n",
      "cnt: 0 - valLoss: 0.6554670929908752 - trainLoss: 0.6562533378601074\n",
      "cnt: 0 - valLoss: 0.6554657816886902 - trainLoss: 0.6562520861625671\n",
      "cnt: 0 - valLoss: 0.6554646492004395 - trainLoss: 0.6562508344650269\n",
      "cnt: 0 - valLoss: 0.6554633975028992 - trainLoss: 0.6562496423721313\n",
      "cnt: 0 - valLoss: 0.6554622054100037 - trainLoss: 0.6562483906745911\n",
      "cnt: 0 - valLoss: 0.6554610133171082 - trainLoss: 0.6562471985816956\n",
      "cnt: 0 - valLoss: 0.6554598212242126 - trainLoss: 0.6562459468841553\n",
      "cnt: 0 - valLoss: 0.6554585099220276 - trainLoss: 0.6562447547912598\n",
      "cnt: 0 - valLoss: 0.6554573774337769 - trainLoss: 0.6562435030937195\n",
      "cnt: 0 - valLoss: 0.6554561853408813 - trainLoss: 0.656242311000824\n",
      "cnt: 0 - valLoss: 0.6554549336433411 - trainLoss: 0.6562410593032837\n",
      "cnt: 0 - valLoss: 0.6554537415504456 - trainLoss: 0.6562398672103882\n",
      "cnt: 0 - valLoss: 0.6554524898529053 - trainLoss: 0.6562386155128479\n",
      "cnt: 0 - valLoss: 0.6554512977600098 - trainLoss: 0.6562374234199524\n",
      "cnt: 0 - valLoss: 0.6554500460624695 - trainLoss: 0.6562362313270569\n",
      "cnt: 0 - valLoss: 0.6554489135742188 - trainLoss: 0.6562349200248718\n",
      "cnt: 0 - valLoss: 0.6554477214813232 - trainLoss: 0.6562336683273315\n",
      "cnt: 0 - valLoss: 0.6554465293884277 - trainLoss: 0.6562325358390808\n",
      "cnt: 0 - valLoss: 0.6554453372955322 - trainLoss: 0.6562312841415405\n",
      "cnt: 0 - valLoss: 0.6554440855979919 - trainLoss: 0.6562300324440002\n",
      "cnt: 0 - valLoss: 0.6554428935050964 - trainLoss: 0.6562288403511047\n",
      "cnt: 0 - valLoss: 0.6554416418075562 - trainLoss: 0.6562275886535645\n",
      "cnt: 0 - valLoss: 0.6554405093193054 - trainLoss: 0.6562263369560242\n",
      "cnt: 0 - valLoss: 0.6554392576217651 - trainLoss: 0.6562251448631287\n",
      "cnt: 0 - valLoss: 0.6554380059242249 - trainLoss: 0.6562238931655884\n",
      "cnt: 0 - valLoss: 0.6554367542266846 - trainLoss: 0.6562226414680481\n",
      "cnt: 0 - valLoss: 0.6554356813430786 - trainLoss: 0.6562215089797974\n",
      "cnt: 0 - valLoss: 0.6554343700408936 - trainLoss: 0.6562201976776123\n",
      "cnt: 0 - valLoss: 0.655433177947998 - trainLoss: 0.6562190055847168\n",
      "cnt: 0 - valLoss: 0.6554319262504578 - trainLoss: 0.6562177538871765\n",
      "cnt: 0 - valLoss: 0.655430793762207 - trainLoss: 0.6562165021896362\n",
      "cnt: 0 - valLoss: 0.655429482460022 - trainLoss: 0.6562153100967407\n",
      "cnt: 0 - valLoss: 0.655428409576416 - trainLoss: 0.6562140583992004\n",
      "cnt: 0 - valLoss: 0.6554271578788757 - trainLoss: 0.6562128663063049\n",
      "cnt: 0 - valLoss: 0.6554259657859802 - trainLoss: 0.6562116742134094\n",
      "cnt: 0 - valLoss: 0.6554248332977295 - trainLoss: 0.6562104225158691\n",
      "cnt: 0 - valLoss: 0.6554235219955444 - trainLoss: 0.6562091708183289\n",
      "cnt: 0 - valLoss: 0.6554223299026489 - trainLoss: 0.6562078595161438\n",
      "cnt: 0 - valLoss: 0.6554210782051086 - trainLoss: 0.6562066674232483\n",
      "cnt: 0 - valLoss: 0.6554198861122131 - trainLoss: 0.6562054753303528\n",
      "cnt: 0 - valLoss: 0.6554186344146729 - trainLoss: 0.6562042236328125\n",
      "cnt: 0 - valLoss: 0.6554174423217773 - trainLoss: 0.656203031539917\n",
      "cnt: 0 - valLoss: 0.6554161906242371 - trainLoss: 0.6562017798423767\n",
      "cnt: 0 - valLoss: 0.6554149985313416 - trainLoss: 0.6562005281448364\n",
      "cnt: 0 - valLoss: 0.655413806438446 - trainLoss: 0.6561992764472961\n",
      "cnt: 0 - valLoss: 0.6554125547409058 - trainLoss: 0.6561980843544006\n",
      "cnt: 0 - valLoss: 0.6554113030433655 - trainLoss: 0.6561968326568604\n",
      "cnt: 0 - valLoss: 0.65541011095047 - trainLoss: 0.6561955809593201\n",
      "cnt: 0 - valLoss: 0.6554089784622192 - trainLoss: 0.6561943292617798\n",
      "cnt: 0 - valLoss: 0.6554077863693237 - trainLoss: 0.6561931371688843\n",
      "cnt: 0 - valLoss: 0.655406653881073 - trainLoss: 0.656191885471344\n",
      "cnt: 0 - valLoss: 0.6554052829742432 - trainLoss: 0.6561906933784485\n",
      "cnt: 0 - valLoss: 0.6554041504859924 - trainLoss: 0.6561894416809082\n",
      "cnt: 0 - valLoss: 0.6554028987884521 - trainLoss: 0.6561881899833679\n",
      "cnt: 0 - valLoss: 0.6554017066955566 - trainLoss: 0.6561869978904724\n",
      "cnt: 0 - valLoss: 0.6554004549980164 - trainLoss: 0.6561856865882874\n",
      "cnt: 0 - valLoss: 0.6553992629051208 - trainLoss: 0.6561844944953918\n",
      "cnt: 0 - valLoss: 0.6553980112075806 - trainLoss: 0.6561832427978516\n",
      "cnt: 0 - valLoss: 0.6553968191146851 - trainLoss: 0.656182050704956\n",
      "cnt: 0 - valLoss: 0.6553955674171448 - trainLoss: 0.6561808586120605\n",
      "cnt: 0 - valLoss: 0.6553943157196045 - trainLoss: 0.6561796069145203\n",
      "cnt: 0 - valLoss: 0.655393123626709 - trainLoss: 0.65617835521698\n",
      "cnt: 0 - valLoss: 0.6553919315338135 - trainLoss: 0.6561771631240845\n",
      "cnt: 0 - valLoss: 0.6553906798362732 - trainLoss: 0.6561759114265442\n",
      "cnt: 0 - valLoss: 0.6553894877433777 - trainLoss: 0.6561747193336487\n",
      "cnt: 0 - valLoss: 0.6553881764411926 - trainLoss: 0.6561735272407532\n",
      "cnt: 0 - valLoss: 0.6553869843482971 - trainLoss: 0.6561723351478577\n",
      "cnt: 0 - valLoss: 0.6553857922554016 - trainLoss: 0.6561710238456726\n",
      "cnt: 0 - valLoss: 0.6553845405578613 - trainLoss: 0.6561698317527771\n",
      "cnt: 0 - valLoss: 0.6553832292556763 - trainLoss: 0.6561686396598816\n",
      "cnt: 0 - valLoss: 0.6553821563720703 - trainLoss: 0.6561673879623413\n",
      "cnt: 0 - valLoss: 0.6553808450698853 - trainLoss: 0.656166136264801\n",
      "cnt: 0 - valLoss: 0.655379593372345 - trainLoss: 0.6561649441719055\n",
      "cnt: 0 - valLoss: 0.6553783416748047 - trainLoss: 0.6561636924743652\n",
      "cnt: 0 - valLoss: 0.6553771495819092 - trainLoss: 0.6561625003814697\n",
      "cnt: 0 - valLoss: 0.6553758978843689 - trainLoss: 0.6561613082885742\n",
      "cnt: 0 - valLoss: 0.6553747057914734 - trainLoss: 0.6561600565910339\n",
      "cnt: 0 - valLoss: 0.6553734540939331 - trainLoss: 0.6561588048934937\n",
      "cnt: 0 - valLoss: 0.6553722620010376 - trainLoss: 0.6561576128005981\n",
      "cnt: 0 - valLoss: 0.6553710699081421 - trainLoss: 0.6561563611030579\n",
      "cnt: 0 - valLoss: 0.6553699970245361 - trainLoss: 0.6561551094055176\n",
      "cnt: 0 - valLoss: 0.6553687453269958 - trainLoss: 0.6561539769172668\n",
      "cnt: 0 - valLoss: 0.6553676128387451 - trainLoss: 0.6561527252197266\n",
      "cnt: 0 - valLoss: 0.6553664207458496 - trainLoss: 0.6561514735221863\n",
      "cnt: 0 - valLoss: 0.6553652286529541 - trainLoss: 0.6561502814292908\n",
      "cnt: 0 - valLoss: 0.6553639769554138 - trainLoss: 0.6561489701271057\n",
      "cnt: 0 - valLoss: 0.6553627848625183 - trainLoss: 0.6561477184295654\n",
      "cnt: 0 - valLoss: 0.6553615927696228 - trainLoss: 0.6561465263366699\n",
      "cnt: 0 - valLoss: 0.6553604602813721 - trainLoss: 0.6561453342437744\n",
      "cnt: 0 - valLoss: 0.6553592681884766 - trainLoss: 0.6561440825462341\n",
      "cnt: 0 - valLoss: 0.655358076095581 - trainLoss: 0.6561428904533386\n",
      "cnt: 0 - valLoss: 0.6553568243980408 - trainLoss: 0.6561415791511536\n",
      "cnt: 0 - valLoss: 0.6553556323051453 - trainLoss: 0.6561403870582581\n",
      "cnt: 0 - valLoss: 0.6553544402122498 - trainLoss: 0.6561391353607178\n",
      "cnt: 0 - valLoss: 0.6553532481193542 - trainLoss: 0.6561379432678223\n",
      "cnt: 0 - valLoss: 0.6553521156311035 - trainLoss: 0.656136691570282\n",
      "cnt: 0 - valLoss: 0.655350923538208 - trainLoss: 0.6561354398727417\n",
      "cnt: 0 - valLoss: 0.6553497314453125 - trainLoss: 0.6561342477798462\n",
      "cnt: 0 - valLoss: 0.6553485989570618 - trainLoss: 0.6561329960823059\n",
      "cnt: 0 - valLoss: 0.6553473472595215 - trainLoss: 0.6561318039894104\n",
      "cnt: 0 - valLoss: 0.655346155166626 - trainLoss: 0.6561305522918701\n",
      "cnt: 0 - valLoss: 0.6553449630737305 - trainLoss: 0.6561293005943298\n",
      "cnt: 0 - valLoss: 0.655343770980835 - trainLoss: 0.6561281085014343\n",
      "cnt: 0 - valLoss: 0.6553426384925842 - trainLoss: 0.656126856803894\n",
      "cnt: 0 - valLoss: 0.655341386795044 - trainLoss: 0.6561256051063538\n",
      "cnt: 0 - valLoss: 0.6553401947021484 - trainLoss: 0.6561244130134583\n",
      "cnt: 0 - valLoss: 0.6553390026092529 - trainLoss: 0.656123161315918\n",
      "cnt: 0 - valLoss: 0.6553378105163574 - trainLoss: 0.6561219096183777\n",
      "cnt: 0 - valLoss: 0.6553366184234619 - trainLoss: 0.6561206579208374\n",
      "cnt: 0 - valLoss: 0.6553354263305664 - trainLoss: 0.6561194062232971\n",
      "cnt: 0 - valLoss: 0.6553342342376709 - trainLoss: 0.6561181545257568\n",
      "cnt: 0 - valLoss: 0.6553331613540649 - trainLoss: 0.6561169624328613\n",
      "cnt: 0 - valLoss: 0.6553319096565247 - trainLoss: 0.656115710735321\n",
      "cnt: 0 - valLoss: 0.6553307175636292 - trainLoss: 0.6561145186424255\n",
      "cnt: 0 - valLoss: 0.6553295254707336 - trainLoss: 0.6561132669448853\n",
      "cnt: 0 - valLoss: 0.6553282737731934 - trainLoss: 0.656112015247345\n",
      "cnt: 0 - valLoss: 0.6553271412849426 - trainLoss: 0.6561108231544495\n",
      "cnt: 0 - valLoss: 0.6553258895874023 - trainLoss: 0.6561095714569092\n",
      "cnt: 0 - valLoss: 0.6553246974945068 - trainLoss: 0.6561084389686584\n",
      "cnt: 0 - valLoss: 0.6553235054016113 - trainLoss: 0.6561071872711182\n",
      "cnt: 0 - valLoss: 0.6553223133087158 - trainLoss: 0.6561059355735779\n",
      "cnt: 0 - valLoss: 0.6553211212158203 - trainLoss: 0.6561046838760376\n",
      "cnt: 0 - valLoss: 0.65531986951828 - trainLoss: 0.6561034917831421\n",
      "cnt: 0 - valLoss: 0.6553186774253845 - trainLoss: 0.6561022400856018\n",
      "cnt: 0 - valLoss: 0.655317485332489 - trainLoss: 0.6561010479927063\n",
      "cnt: 0 - valLoss: 0.6553162336349487 - trainLoss: 0.6560998558998108\n",
      "cnt: 0 - valLoss: 0.655315101146698 - trainLoss: 0.6560986042022705\n",
      "cnt: 0 - valLoss: 0.6553139686584473 - trainLoss: 0.656097412109375\n",
      "cnt: 0 - valLoss: 0.655312716960907 - trainLoss: 0.6560962200164795\n",
      "cnt: 0 - valLoss: 0.6553115248680115 - trainLoss: 0.6560949683189392\n",
      "cnt: 0 - valLoss: 0.6553102731704712 - trainLoss: 0.6560937166213989\n",
      "cnt: 0 - valLoss: 0.6553090810775757 - trainLoss: 0.6560924649238586\n",
      "cnt: 0 - valLoss: 0.6553078293800354 - trainLoss: 0.6560912132263184\n",
      "cnt: 0 - valLoss: 0.6553066372871399 - trainLoss: 0.6560900807380676\n",
      "cnt: 0 - valLoss: 0.6553053855895996 - trainLoss: 0.6560888290405273\n",
      "cnt: 0 - valLoss: 0.6553042531013489 - trainLoss: 0.6560876369476318\n",
      "cnt: 0 - valLoss: 0.6553029417991638 - trainLoss: 0.6560863256454468\n",
      "cnt: 0 - valLoss: 0.6553018093109131 - trainLoss: 0.6560851335525513\n",
      "cnt: 0 - valLoss: 0.6553005576133728 - trainLoss: 0.656083881855011\n",
      "cnt: 0 - valLoss: 0.6552994251251221 - trainLoss: 0.6560826301574707\n",
      "cnt: 0 - valLoss: 0.6552981734275818 - trainLoss: 0.6560814380645752\n",
      "cnt: 0 - valLoss: 0.6552969217300415 - trainLoss: 0.6560801863670349\n",
      "cnt: 0 - valLoss: 0.6552958488464355 - trainLoss: 0.6560789942741394\n",
      "cnt: 0 - valLoss: 0.65529465675354 - trainLoss: 0.6560777425765991\n",
      "cnt: 0 - valLoss: 0.6552934050559998 - trainLoss: 0.6560764908790588\n",
      "cnt: 0 - valLoss: 0.6552921533584595 - trainLoss: 0.6560752987861633\n",
      "cnt: 0 - valLoss: 0.655290961265564 - trainLoss: 0.656074047088623\n",
      "cnt: 0 - valLoss: 0.6552898287773132 - trainLoss: 0.6560729146003723\n",
      "cnt: 0 - valLoss: 0.6552885174751282 - trainLoss: 0.6560716032981873\n",
      "cnt: 0 - valLoss: 0.6552873849868774 - trainLoss: 0.6560704112052917\n",
      "cnt: 0 - valLoss: 0.6552861332893372 - trainLoss: 0.6560691595077515\n",
      "cnt: 0 - valLoss: 0.6552849411964417 - trainLoss: 0.6560679078102112\n",
      "cnt: 0 - valLoss: 0.6552836298942566 - trainLoss: 0.6560666561126709\n",
      "cnt: 0 - valLoss: 0.6552824974060059 - trainLoss: 0.6560654640197754\n",
      "cnt: 0 - valLoss: 0.6552813053131104 - trainLoss: 0.6560642123222351\n",
      "cnt: 0 - valLoss: 0.6552800536155701 - trainLoss: 0.6560630202293396\n",
      "cnt: 0 - valLoss: 0.6552788615226746 - trainLoss: 0.6560617685317993\n",
      "cnt: 0 - valLoss: 0.655277669429779 - trainLoss: 0.6560605764389038\n",
      "cnt: 0 - valLoss: 0.6552765369415283 - trainLoss: 0.6560593247413635\n",
      "cnt: 0 - valLoss: 0.6552753448486328 - trainLoss: 0.6560580730438232\n",
      "cnt: 0 - valLoss: 0.6552741527557373 - trainLoss: 0.656056821346283\n",
      "cnt: 0 - valLoss: 0.655272901058197 - trainLoss: 0.6560556292533875\n",
      "cnt: 0 - valLoss: 0.6552717685699463 - trainLoss: 0.6560543775558472\n",
      "cnt: 0 - valLoss: 0.655270516872406 - trainLoss: 0.6560531258583069\n",
      "cnt: 0 - valLoss: 0.6552693843841553 - trainLoss: 0.6560519337654114\n",
      "cnt: 0 - valLoss: 0.655268132686615 - trainLoss: 0.6560507416725159\n",
      "cnt: 0 - valLoss: 0.6552668809890747 - trainLoss: 0.6560494303703308\n",
      "cnt: 0 - valLoss: 0.655265748500824 - trainLoss: 0.6560481786727905\n",
      "cnt: 0 - valLoss: 0.6552644968032837 - trainLoss: 0.6560469269752502\n",
      "cnt: 0 - valLoss: 0.6552633047103882 - trainLoss: 0.6560457944869995\n",
      "cnt: 0 - valLoss: 0.6552621126174927 - trainLoss: 0.6560445427894592\n",
      "cnt: 0 - valLoss: 0.6552608609199524 - trainLoss: 0.656043291091919\n",
      "cnt: 0 - valLoss: 0.6552597284317017 - trainLoss: 0.6560419797897339\n",
      "cnt: 0 - valLoss: 0.6552585363388062 - trainLoss: 0.6560407876968384\n",
      "cnt: 0 - valLoss: 0.6552574038505554 - trainLoss: 0.6560395956039429\n",
      "cnt: 0 - valLoss: 0.6552560925483704 - trainLoss: 0.6560383439064026\n",
      "cnt: 0 - valLoss: 0.6552549600601196 - trainLoss: 0.6560370922088623\n",
      "cnt: 0 - valLoss: 0.6552538275718689 - trainLoss: 0.6560359001159668\n",
      "cnt: 0 - valLoss: 0.6552525758743286 - trainLoss: 0.6560346484184265\n",
      "cnt: 0 - valLoss: 0.6552513837814331 - trainLoss: 0.6560333967208862\n",
      "cnt: 0 - valLoss: 0.6552501320838928 - trainLoss: 0.6560322046279907\n",
      "cnt: 0 - valLoss: 0.6552489399909973 - trainLoss: 0.6560309529304504\n",
      "cnt: 0 - valLoss: 0.6552477478981018 - trainLoss: 0.6560297608375549\n",
      "cnt: 0 - valLoss: 0.6552464962005615 - trainLoss: 0.6560285091400146\n",
      "cnt: 0 - valLoss: 0.655245304107666 - trainLoss: 0.6560272574424744\n",
      "cnt: 0 - valLoss: 0.6552440524101257 - trainLoss: 0.6560260057449341\n",
      "cnt: 0 - valLoss: 0.6552428603172302 - trainLoss: 0.6560247540473938\n",
      "cnt: 0 - valLoss: 0.6552416086196899 - trainLoss: 0.6560235023498535\n",
      "cnt: 0 - valLoss: 0.6552404165267944 - trainLoss: 0.656022310256958\n",
      "cnt: 0 - valLoss: 0.6552393436431885 - trainLoss: 0.6560210585594177\n",
      "cnt: 0 - valLoss: 0.6552380919456482 - trainLoss: 0.6560198068618774\n",
      "cnt: 0 - valLoss: 0.6552368998527527 - trainLoss: 0.6560186147689819\n",
      "cnt: 0 - valLoss: 0.6552356481552124 - trainLoss: 0.6560173034667969\n",
      "cnt: 0 - valLoss: 0.6552344560623169 - trainLoss: 0.6560161113739014\n",
      "cnt: 0 - valLoss: 0.6552333235740662 - trainLoss: 0.6560148596763611\n",
      "cnt: 0 - valLoss: 0.6552320122718811 - trainLoss: 0.6560136675834656\n",
      "cnt: 0 - valLoss: 0.6552308201789856 - trainLoss: 0.6560123562812805\n",
      "cnt: 0 - valLoss: 0.6552296876907349 - trainLoss: 0.6560111045837402\n",
      "cnt: 0 - valLoss: 0.6552284359931946 - trainLoss: 0.6560099124908447\n",
      "cnt: 0 - valLoss: 0.6552272439002991 - trainLoss: 0.6560086607933044\n",
      "cnt: 0 - valLoss: 0.6552259922027588 - trainLoss: 0.6560074687004089\n",
      "cnt: 0 - valLoss: 0.6552248597145081 - trainLoss: 0.6560062170028687\n",
      "cnt: 0 - valLoss: 0.6552236080169678 - trainLoss: 0.6560049653053284\n",
      "cnt: 0 - valLoss: 0.6552224159240723 - trainLoss: 0.6560037136077881\n",
      "cnt: 0 - valLoss: 0.6552212238311768 - trainLoss: 0.6560024619102478\n",
      "cnt: 0 - valLoss: 0.6552200317382812 - trainLoss: 0.6560012102127075\n",
      "cnt: 0 - valLoss: 0.6552188396453857 - trainLoss: 0.656000018119812\n",
      "cnt: 0 - valLoss: 0.6552176475524902 - trainLoss: 0.6559987664222717\n",
      "cnt: 0 - valLoss: 0.6552164554595947 - trainLoss: 0.6559975147247314\n",
      "cnt: 0 - valLoss: 0.6552152037620544 - trainLoss: 0.6559962034225464\n",
      "cnt: 0 - valLoss: 0.6552140116691589 - trainLoss: 0.6559950113296509\n",
      "cnt: 0 - valLoss: 0.6552127599716187 - trainLoss: 0.6559937596321106\n",
      "cnt: 0 - valLoss: 0.6552115678787231 - trainLoss: 0.6559925079345703\n",
      "cnt: 0 - valLoss: 0.6552103161811829 - trainLoss: 0.6559911966323853\n",
      "cnt: 0 - valLoss: 0.6552091240882874 - trainLoss: 0.6559900045394897\n",
      "cnt: 0 - valLoss: 0.6552079319953918 - trainLoss: 0.6559887528419495\n",
      "cnt: 0 - valLoss: 0.6552066802978516 - trainLoss: 0.6559874415397644\n",
      "cnt: 0 - valLoss: 0.6552055478096008 - trainLoss: 0.6559861898422241\n",
      "cnt: 0 - valLoss: 0.6552042961120605 - trainLoss: 0.6559848785400391\n",
      "cnt: 0 - valLoss: 0.6552031636238098 - trainLoss: 0.6559836268424988\n",
      "cnt: 0 - valLoss: 0.6552019715309143 - trainLoss: 0.6559823751449585\n",
      "cnt: 0 - valLoss: 0.655200719833374 - trainLoss: 0.655981183052063\n",
      "cnt: 0 - valLoss: 0.6551995277404785 - trainLoss: 0.6559799313545227\n",
      "cnt: 0 - valLoss: 0.6551982760429382 - trainLoss: 0.6559786796569824\n",
      "cnt: 0 - valLoss: 0.6551971435546875 - trainLoss: 0.6559774279594421\n",
      "cnt: 0 - valLoss: 0.6551958918571472 - trainLoss: 0.6559761762619019\n",
      "cnt: 0 - valLoss: 0.6551946401596069 - trainLoss: 0.6559749245643616\n",
      "cnt: 0 - valLoss: 0.6551934480667114 - trainLoss: 0.6559737324714661\n",
      "cnt: 0 - valLoss: 0.6551922559738159 - trainLoss: 0.6559723615646362\n",
      "cnt: 0 - valLoss: 0.6551910042762756 - trainLoss: 0.6559711694717407\n",
      "cnt: 0 - valLoss: 0.6551897525787354 - trainLoss: 0.6559699177742004\n",
      "cnt: 0 - valLoss: 0.6551883816719055 - trainLoss: 0.6559687256813049\n",
      "cnt: 0 - valLoss: 0.6551870107650757 - trainLoss: 0.6559673547744751\n",
      "cnt: 0 - valLoss: 0.6551857590675354 - trainLoss: 0.6559661030769348\n",
      "cnt: 0 - valLoss: 0.6551843881607056 - trainLoss: 0.6559648513793945\n",
      "cnt: 0 - valLoss: 0.6551831960678101 - trainLoss: 0.6559635400772095\n",
      "cnt: 0 - valLoss: 0.6551818251609802 - trainLoss: 0.6559622883796692\n",
      "cnt: 0 - valLoss: 0.6551805734634399 - trainLoss: 0.6559609770774841\n",
      "cnt: 0 - valLoss: 0.6551792025566101 - trainLoss: 0.6559596061706543\n",
      "cnt: 0 - valLoss: 0.655177891254425 - trainLoss: 0.655958354473114\n",
      "cnt: 0 - valLoss: 0.65517657995224 - trainLoss: 0.655957043170929\n",
      "cnt: 0 - valLoss: 0.6551752686500549 - trainLoss: 0.6559557914733887\n",
      "cnt: 0 - valLoss: 0.6551739573478699 - trainLoss: 0.6559544801712036\n",
      "cnt: 0 - valLoss: 0.6551726460456848 - trainLoss: 0.6559532284736633\n",
      "cnt: 0 - valLoss: 0.655171275138855 - trainLoss: 0.6559519171714783\n",
      "cnt: 0 - valLoss: 0.6551699638366699 - trainLoss: 0.6559506058692932\n",
      "cnt: 0 - valLoss: 0.6551687121391296 - trainLoss: 0.6559493541717529\n",
      "cnt: 0 - valLoss: 0.6551673412322998 - trainLoss: 0.6559479832649231\n",
      "cnt: 0 - valLoss: 0.6551660299301147 - trainLoss: 0.6559467315673828\n",
      "cnt: 0 - valLoss: 0.6551647186279297 - trainLoss: 0.6559454202651978\n",
      "cnt: 0 - valLoss: 0.6551634669303894 - trainLoss: 0.6559441089630127\n",
      "cnt: 0 - valLoss: 0.6551620960235596 - trainLoss: 0.6559427976608276\n",
      "cnt: 0 - valLoss: 0.6551607847213745 - trainLoss: 0.6559415459632874\n",
      "cnt: 0 - valLoss: 0.6551594734191895 - trainLoss: 0.6559401750564575\n",
      "cnt: 0 - valLoss: 0.6551581621170044 - trainLoss: 0.6559389233589172\n",
      "cnt: 0 - valLoss: 0.6551568508148193 - trainLoss: 0.6559376120567322\n",
      "cnt: 0 - valLoss: 0.6551555395126343 - trainLoss: 0.6559362411499023\n",
      "cnt: 0 - valLoss: 0.6551541686058044 - trainLoss: 0.6559349894523621\n",
      "cnt: 0 - valLoss: 0.6551529169082642 - trainLoss: 0.655933678150177\n",
      "cnt: 0 - valLoss: 0.6551516056060791 - trainLoss: 0.6559324264526367\n",
      "cnt: 0 - valLoss: 0.6551502346992493 - trainLoss: 0.6559310555458069\n",
      "cnt: 0 - valLoss: 0.6551489233970642 - trainLoss: 0.6559297442436218\n",
      "cnt: 0 - valLoss: 0.6551476120948792 - trainLoss: 0.6559284925460815\n",
      "cnt: 0 - valLoss: 0.6551463007926941 - trainLoss: 0.6559271216392517\n",
      "cnt: 0 - valLoss: 0.6551449298858643 - trainLoss: 0.6559258699417114\n",
      "cnt: 0 - valLoss: 0.6551437377929688 - trainLoss: 0.6559245586395264\n",
      "cnt: 0 - valLoss: 0.6551423668861389 - trainLoss: 0.6559232473373413\n",
      "cnt: 0 - valLoss: 0.6551410555839539 - trainLoss: 0.655921995639801\n",
      "cnt: 0 - valLoss: 0.6551398038864136 - trainLoss: 0.6559206247329712\n",
      "cnt: 0 - valLoss: 0.6551384329795837 - trainLoss: 0.6559193730354309\n",
      "cnt: 0 - valLoss: 0.6551371216773987 - trainLoss: 0.6559180617332458\n",
      "cnt: 0 - valLoss: 0.6551357507705688 - trainLoss: 0.6559167504310608\n",
      "cnt: 0 - valLoss: 0.655134379863739 - trainLoss: 0.6559154391288757\n",
      "cnt: 0 - valLoss: 0.6551331281661987 - trainLoss: 0.6559141278266907\n",
      "cnt: 0 - valLoss: 0.6551317572593689 - trainLoss: 0.6559128761291504\n",
      "cnt: 0 - valLoss: 0.6551305055618286 - trainLoss: 0.6559115052223206\n",
      "cnt: 0 - valLoss: 0.6551291942596436 - trainLoss: 0.6559102535247803\n",
      "cnt: 0 - valLoss: 0.655127763748169 - trainLoss: 0.6559089422225952\n",
      "cnt: 0 - valLoss: 0.6551265120506287 - trainLoss: 0.6559076905250549\n",
      "cnt: 0 - valLoss: 0.6551252007484436 - trainLoss: 0.6559063196182251\n",
      "cnt: 0 - valLoss: 0.6551239490509033 - trainLoss: 0.6559051275253296\n",
      "cnt: 0 - valLoss: 0.6551225781440735 - trainLoss: 0.6559037566184998\n",
      "cnt: 0 - valLoss: 0.6551212668418884 - trainLoss: 0.6559025049209595\n",
      "cnt: 0 - valLoss: 0.6551198959350586 - trainLoss: 0.6559011936187744\n",
      "cnt: 0 - valLoss: 0.6551186442375183 - trainLoss: 0.6558998823165894\n",
      "cnt: 0 - valLoss: 0.6551172733306885 - trainLoss: 0.6558985710144043\n",
      "cnt: 0 - valLoss: 0.6551159620285034 - trainLoss: 0.6558972597122192\n",
      "cnt: 0 - valLoss: 0.6551146507263184 - trainLoss: 0.6558959484100342\n",
      "cnt: 0 - valLoss: 0.6551132798194885 - trainLoss: 0.6558946371078491\n",
      "cnt: 0 - valLoss: 0.6551119685173035 - trainLoss: 0.6558933854103088\n",
      "cnt: 0 - valLoss: 0.6551107168197632 - trainLoss: 0.6558920741081238\n",
      "cnt: 0 - valLoss: 0.6551093459129333 - trainLoss: 0.6558907628059387\n",
      "cnt: 0 - valLoss: 0.6551079750061035 - trainLoss: 0.6558894515037537\n",
      "cnt: 0 - valLoss: 0.6551066637039185 - trainLoss: 0.6558881402015686\n",
      "cnt: 0 - valLoss: 0.6551053524017334 - trainLoss: 0.6558868885040283\n",
      "cnt: 0 - valLoss: 0.6551041603088379 - trainLoss: 0.6558855175971985\n",
      "cnt: 0 - valLoss: 0.6551027894020081 - trainLoss: 0.6558842062950134\n",
      "cnt: 0 - valLoss: 0.655101478099823 - trainLoss: 0.6558829545974731\n",
      "cnt: 0 - valLoss: 0.6551001667976379 - trainLoss: 0.6558816432952881\n",
      "cnt: 0 - valLoss: 0.6550987958908081 - trainLoss: 0.6558803915977478\n",
      "cnt: 0 - valLoss: 0.655097484588623 - trainLoss: 0.6558790802955627\n",
      "cnt: 0 - valLoss: 0.655096173286438 - trainLoss: 0.6558777689933777\n",
      "cnt: 0 - valLoss: 0.6550948619842529 - trainLoss: 0.6558765172958374\n",
      "cnt: 0 - valLoss: 0.6550934314727783 - trainLoss: 0.6558751463890076\n",
      "cnt: 0 - valLoss: 0.6550921201705933 - trainLoss: 0.6558738350868225\n",
      "cnt: 0 - valLoss: 0.6550908088684082 - trainLoss: 0.6558724641799927\n",
      "cnt: 0 - valLoss: 0.6550894975662231 - trainLoss: 0.6558712124824524\n",
      "cnt: 0 - valLoss: 0.6550881266593933 - trainLoss: 0.6558699607849121\n",
      "cnt: 0 - valLoss: 0.6550868153572083 - trainLoss: 0.655868649482727\n",
      "cnt: 0 - valLoss: 0.6550856232643127 - trainLoss: 0.6558672785758972\n",
      "cnt: 0 - valLoss: 0.6550842523574829 - trainLoss: 0.6558660864830017\n",
      "cnt: 0 - valLoss: 0.6550829410552979 - trainLoss: 0.6558647155761719\n",
      "cnt: 0 - valLoss: 0.655081570148468 - trainLoss: 0.6558634042739868\n",
      "cnt: 0 - valLoss: 0.6550803184509277 - trainLoss: 0.6558620929718018\n",
      "cnt: 0 - valLoss: 0.6550789475440979 - trainLoss: 0.6558608412742615\n",
      "cnt: 0 - valLoss: 0.6550776362419128 - trainLoss: 0.6558595299720764\n",
      "cnt: 0 - valLoss: 0.6550763249397278 - trainLoss: 0.6558582186698914\n",
      "cnt: 0 - valLoss: 0.6550750136375427 - trainLoss: 0.6558569073677063\n",
      "cnt: 0 - valLoss: 0.6550737023353577 - trainLoss: 0.6558555960655212\n",
      "cnt: 0 - valLoss: 0.6550723910331726 - trainLoss: 0.655854344367981\n",
      "cnt: 0 - valLoss: 0.6550710201263428 - trainLoss: 0.6558530330657959\n",
      "cnt: 0 - valLoss: 0.6550697684288025 - trainLoss: 0.6558517813682556\n",
      "cnt: 0 - valLoss: 0.6550683975219727 - trainLoss: 0.6558504700660706\n",
      "cnt: 0 - valLoss: 0.6550671458244324 - trainLoss: 0.6558492183685303\n",
      "cnt: 0 - valLoss: 0.6550658345222473 - trainLoss: 0.6558478474617004\n",
      "cnt: 0 - valLoss: 0.655064582824707 - trainLoss: 0.6558466553688049\n",
      "cnt: 0 - valLoss: 0.6550632119178772 - trainLoss: 0.6558453440666199\n",
      "cnt: 0 - valLoss: 0.6550619006156921 - trainLoss: 0.6558440923690796\n",
      "cnt: 0 - valLoss: 0.6550605893135071 - trainLoss: 0.6558427810668945\n",
      "cnt: 0 - valLoss: 0.655059278011322 - trainLoss: 0.6558415293693542\n",
      "cnt: 0 - valLoss: 0.6550579071044922 - trainLoss: 0.6558401584625244\n",
      "cnt: 0 - valLoss: 0.6550565958023071 - trainLoss: 0.6558389067649841\n",
      "cnt: 0 - valLoss: 0.6550552845001221 - trainLoss: 0.6558375358581543\n",
      "cnt: 0 - valLoss: 0.655053973197937 - trainLoss: 0.6558363437652588\n",
      "cnt: 0 - valLoss: 0.6550526022911072 - trainLoss: 0.6558350324630737\n",
      "cnt: 0 - valLoss: 0.6550513505935669 - trainLoss: 0.6558337211608887\n",
      "cnt: 0 - valLoss: 0.6550500392913818 - trainLoss: 0.6558324694633484\n",
      "cnt: 0 - valLoss: 0.655048668384552 - trainLoss: 0.6558311581611633\n",
      "cnt: 0 - valLoss: 0.6550473570823669 - trainLoss: 0.655829906463623\n",
      "cnt: 0 - valLoss: 0.6550461053848267 - trainLoss: 0.655828595161438\n",
      "cnt: 0 - valLoss: 0.6550447940826416 - trainLoss: 0.6558272838592529\n",
      "cnt: 0 - valLoss: 0.6550434827804565 - trainLoss: 0.6558260321617126\n",
      "cnt: 0 - valLoss: 0.6550421714782715 - trainLoss: 0.6558247208595276\n",
      "cnt: 0 - valLoss: 0.6550408005714417 - trainLoss: 0.6558234095573425\n",
      "cnt: 0 - valLoss: 0.6550394296646118 - trainLoss: 0.6558220982551575\n",
      "cnt: 0 - valLoss: 0.6550381779670715 - trainLoss: 0.6558208465576172\n",
      "cnt: 0 - valLoss: 0.6550368070602417 - trainLoss: 0.6558195948600769\n",
      "cnt: 0 - valLoss: 0.6550354957580566 - trainLoss: 0.6558182835578918\n",
      "cnt: 0 - valLoss: 0.6550341844558716 - trainLoss: 0.6558169722557068\n",
      "cnt: 0 - valLoss: 0.6550328731536865 - trainLoss: 0.6558156609535217\n",
      "cnt: 0 - valLoss: 0.6550315618515015 - trainLoss: 0.6558143496513367\n",
      "cnt: 0 - valLoss: 0.6550301909446716 - trainLoss: 0.6558130979537964\n",
      "cnt: 0 - valLoss: 0.6550289392471313 - trainLoss: 0.6558117866516113\n",
      "cnt: 0 - valLoss: 0.6550276875495911 - trainLoss: 0.6558104753494263\n",
      "cnt: 0 - valLoss: 0.6550263166427612 - trainLoss: 0.655809223651886\n",
      "cnt: 0 - valLoss: 0.6550249457359314 - trainLoss: 0.6558079123497009\n",
      "cnt: 0 - valLoss: 0.6550236344337463 - trainLoss: 0.6558066010475159\n",
      "cnt: 0 - valLoss: 0.6550223231315613 - trainLoss: 0.6558052897453308\n",
      "cnt: 0 - valLoss: 0.6550209522247314 - trainLoss: 0.6558039784431458\n",
      "cnt: 0 - valLoss: 0.6550196409225464 - trainLoss: 0.6558027267456055\n",
      "cnt: 0 - valLoss: 0.6550183296203613 - trainLoss: 0.6558014154434204\n",
      "cnt: 0 - valLoss: 0.6550170183181763 - trainLoss: 0.6558001041412354\n",
      "cnt: 0 - valLoss: 0.6550156474113464 - trainLoss: 0.6557988524436951\n",
      "cnt: 0 - valLoss: 0.6550143957138062 - trainLoss: 0.65579754114151\n",
      "cnt: 0 - valLoss: 0.6550130248069763 - trainLoss: 0.655796229839325\n",
      "cnt: 0 - valLoss: 0.6550116539001465 - trainLoss: 0.6557949185371399\n",
      "cnt: 0 - valLoss: 0.6550102829933167 - trainLoss: 0.6557936668395996\n",
      "cnt: 0 - valLoss: 0.6550090909004211 - trainLoss: 0.6557923555374146\n",
      "cnt: 0 - valLoss: 0.6550077199935913 - trainLoss: 0.6557910442352295\n",
      "cnt: 0 - valLoss: 0.6550064086914062 - trainLoss: 0.6557897925376892\n",
      "cnt: 0 - valLoss: 0.6550050973892212 - trainLoss: 0.6557884216308594\n",
      "cnt: 0 - valLoss: 0.6550037264823914 - trainLoss: 0.6557871699333191\n",
      "cnt: 0 - valLoss: 0.6550024747848511 - trainLoss: 0.655785858631134\n",
      "cnt: 0 - valLoss: 0.655001163482666 - trainLoss: 0.655784547328949\n",
      "cnt: 0 - valLoss: 0.6549997925758362 - trainLoss: 0.6557832360267639\n",
      "cnt: 0 - valLoss: 0.6549984216690063 - trainLoss: 0.6557819843292236\n",
      "cnt: 0 - valLoss: 0.6549971699714661 - trainLoss: 0.6557806134223938\n",
      "cnt: 0 - valLoss: 0.6549957990646362 - trainLoss: 0.6557793617248535\n",
      "cnt: 0 - valLoss: 0.6549944877624512 - trainLoss: 0.6557780504226685\n",
      "cnt: 0 - valLoss: 0.6549931168556213 - trainLoss: 0.6557767391204834\n",
      "cnt: 0 - valLoss: 0.654991865158081 - trainLoss: 0.6557754278182983\n",
      "cnt: 0 - valLoss: 0.654990553855896 - trainLoss: 0.6557741761207581\n",
      "cnt: 0 - valLoss: 0.6549892425537109 - trainLoss: 0.655772864818573\n",
      "cnt: 0 - valLoss: 0.6549878716468811 - trainLoss: 0.6557716131210327\n",
      "cnt: 0 - valLoss: 0.654986560344696 - trainLoss: 0.6557703018188477\n",
      "cnt: 0 - valLoss: 0.654985249042511 - trainLoss: 0.6557689905166626\n",
      "cnt: 0 - valLoss: 0.6549839377403259 - trainLoss: 0.6557676792144775\n",
      "cnt: 0 - valLoss: 0.6549826264381409 - trainLoss: 0.6557663679122925\n",
      "cnt: 0 - valLoss: 0.654981255531311 - trainLoss: 0.6557650566101074\n",
      "cnt: 0 - valLoss: 0.654979944229126 - trainLoss: 0.6557637453079224\n",
      "cnt: 0 - valLoss: 0.6549786329269409 - trainLoss: 0.6557624340057373\n",
      "cnt: 0 - valLoss: 0.6549772620201111 - trainLoss: 0.6557611227035522\n",
      "cnt: 0 - valLoss: 0.654975950717926 - trainLoss: 0.655759871006012\n",
      "cnt: 0 - valLoss: 0.6549745798110962 - trainLoss: 0.6557585000991821\n",
      "cnt: 0 - valLoss: 0.6549733281135559 - trainLoss: 0.6557571887969971\n",
      "cnt: 0 - valLoss: 0.6549719572067261 - trainLoss: 0.6557559370994568\n",
      "cnt: 0 - valLoss: 0.6549707055091858 - trainLoss: 0.6557546257972717\n",
      "cnt: 0 - valLoss: 0.6549693942070007 - trainLoss: 0.6557533144950867\n",
      "cnt: 0 - valLoss: 0.6549680233001709 - trainLoss: 0.6557519435882568\n",
      "cnt: 0 - valLoss: 0.6549667119979858 - trainLoss: 0.6557506918907166\n",
      "cnt: 0 - valLoss: 0.654965341091156 - trainLoss: 0.6557494401931763\n",
      "cnt: 0 - valLoss: 0.6549640893936157 - trainLoss: 0.6557481288909912\n",
      "cnt: 0 - valLoss: 0.6549627184867859 - trainLoss: 0.6557467579841614\n",
      "cnt: 0 - valLoss: 0.6549614071846008 - trainLoss: 0.6557455062866211\n",
      "cnt: 0 - valLoss: 0.654960036277771 - trainLoss: 0.655744194984436\n",
      "cnt: 0 - valLoss: 0.6549587249755859 - trainLoss: 0.655742883682251\n",
      "cnt: 0 - valLoss: 0.6549573540687561 - trainLoss: 0.6557415723800659\n",
      "cnt: 0 - valLoss: 0.6549561023712158 - trainLoss: 0.6557402610778809\n",
      "cnt: 0 - valLoss: 0.654954731464386 - trainLoss: 0.6557389497756958\n",
      "cnt: 0 - valLoss: 0.6549533605575562 - trainLoss: 0.6557376384735107\n",
      "cnt: 0 - valLoss: 0.6549521684646606 - trainLoss: 0.6557363271713257\n",
      "cnt: 0 - valLoss: 0.6549507975578308 - trainLoss: 0.6557350754737854\n",
      "cnt: 0 - valLoss: 0.6549494862556458 - trainLoss: 0.6557337045669556\n",
      "cnt: 0 - valLoss: 0.6549481153488159 - trainLoss: 0.6557323932647705\n",
      "cnt: 0 - valLoss: 0.6549468040466309 - trainLoss: 0.6557310819625854\n",
      "cnt: 0 - valLoss: 0.654945433139801 - trainLoss: 0.6557297706604004\n",
      "cnt: 0 - valLoss: 0.654944121837616 - trainLoss: 0.6557285189628601\n",
      "cnt: 0 - valLoss: 0.6549428105354309 - trainLoss: 0.6557271480560303\n",
      "cnt: 0 - valLoss: 0.6549414396286011 - trainLoss: 0.65572589635849\n",
      "cnt: 0 - valLoss: 0.654940128326416 - trainLoss: 0.6557245254516602\n",
      "cnt: 0 - valLoss: 0.6549387574195862 - trainLoss: 0.6557232737541199\n",
      "cnt: 0 - valLoss: 0.6549375057220459 - trainLoss: 0.65572190284729\n",
      "cnt: 0 - valLoss: 0.6549360752105713 - trainLoss: 0.655720591545105\n",
      "cnt: 0 - valLoss: 0.6549347639083862 - trainLoss: 0.6557192802429199\n",
      "cnt: 0 - valLoss: 0.6549333930015564 - trainLoss: 0.6557179093360901\n",
      "cnt: 0 - valLoss: 0.6549321413040161 - trainLoss: 0.6557166576385498\n",
      "cnt: 0 - valLoss: 0.654930830001831 - trainLoss: 0.6557153463363647\n",
      "cnt: 0 - valLoss: 0.6549293994903564 - trainLoss: 0.6557140350341797\n",
      "cnt: 0 - valLoss: 0.6549280285835266 - trainLoss: 0.6557127237319946\n",
      "cnt: 0 - valLoss: 0.6549266576766968 - trainLoss: 0.6557114124298096\n",
      "cnt: 0 - valLoss: 0.6549252867698669 - trainLoss: 0.6557100415229797\n",
      "cnt: 0 - valLoss: 0.6549239158630371 - trainLoss: 0.6557087302207947\n",
      "cnt: 0 - valLoss: 0.654922604560852 - trainLoss: 0.6557074189186096\n",
      "cnt: 0 - valLoss: 0.6549212336540222 - trainLoss: 0.6557061076164246\n",
      "cnt: 0 - valLoss: 0.6549198627471924 - trainLoss: 0.6557047963142395\n",
      "cnt: 0 - valLoss: 0.6549185514450073 - trainLoss: 0.6557034850120544\n",
      "cnt: 0 - valLoss: 0.6549172401428223 - trainLoss: 0.6557021141052246\n",
      "cnt: 0 - valLoss: 0.6549158692359924 - trainLoss: 0.6557008028030396\n",
      "cnt: 0 - valLoss: 0.6549144983291626 - trainLoss: 0.6556995511054993\n",
      "cnt: 0 - valLoss: 0.6549132466316223 - trainLoss: 0.6556981801986694\n",
      "cnt: 0 - valLoss: 0.6549117565155029 - trainLoss: 0.6556969285011292\n",
      "cnt: 0 - valLoss: 0.6549105048179626 - trainLoss: 0.6556956171989441\n",
      "cnt: 0 - valLoss: 0.6549091339111328 - trainLoss: 0.6556942462921143\n",
      "cnt: 0 - valLoss: 0.654907763004303 - trainLoss: 0.655692994594574\n",
      "cnt: 0 - valLoss: 0.6549063920974731 - trainLoss: 0.6556916236877441\n",
      "cnt: 0 - valLoss: 0.6549050211906433 - trainLoss: 0.6556903719902039\n",
      "cnt: 0 - valLoss: 0.6549037098884583 - trainLoss: 0.655689001083374\n",
      "cnt: 0 - valLoss: 0.6549023389816284 - trainLoss: 0.655687689781189\n",
      "cnt: 0 - valLoss: 0.6549009084701538 - trainLoss: 0.6556863784790039\n",
      "cnt: 0 - valLoss: 0.6548995971679688 - trainLoss: 0.6556850075721741\n",
      "cnt: 0 - valLoss: 0.6548982262611389 - trainLoss: 0.655683696269989\n",
      "cnt: 0 - valLoss: 0.6548967957496643 - trainLoss: 0.655682384967804\n",
      "cnt: 0 - valLoss: 0.654895544052124 - trainLoss: 0.6556810736656189\n",
      "cnt: 0 - valLoss: 0.6548941731452942 - trainLoss: 0.6556797623634338\n",
      "cnt: 0 - valLoss: 0.6548928022384644 - trainLoss: 0.6556784510612488\n",
      "cnt: 0 - valLoss: 0.6548915505409241 - trainLoss: 0.6556771397590637\n",
      "cnt: 0 - valLoss: 0.6548900604248047 - trainLoss: 0.6556758880615234\n",
      "cnt: 0 - valLoss: 0.6548888087272644 - trainLoss: 0.6556745171546936\n",
      "cnt: 0 - valLoss: 0.6548873782157898 - trainLoss: 0.6556732058525085\n",
      "cnt: 0 - valLoss: 0.6548860669136047 - trainLoss: 0.6556718945503235\n",
      "cnt: 0 - valLoss: 0.6548846960067749 - trainLoss: 0.6556705236434937\n",
      "cnt: 0 - valLoss: 0.6548833250999451 - trainLoss: 0.6556691527366638\n",
      "cnt: 0 - valLoss: 0.6548819541931152 - trainLoss: 0.6556678414344788\n",
      "cnt: 0 - valLoss: 0.6548805832862854 - trainLoss: 0.6556665301322937\n",
      "cnt: 0 - valLoss: 0.6548792123794556 - trainLoss: 0.6556652188301086\n",
      "cnt: 0 - valLoss: 0.6548778414726257 - trainLoss: 0.6556638479232788\n",
      "cnt: 0 - valLoss: 0.6548764705657959 - trainLoss: 0.6556625962257385\n",
      "cnt: 0 - valLoss: 0.6548751592636108 - trainLoss: 0.6556612253189087\n",
      "cnt: 0 - valLoss: 0.6548739075660706 - trainLoss: 0.6556599140167236\n",
      "cnt: 0 - valLoss: 0.6548724174499512 - trainLoss: 0.6556586623191833\n",
      "cnt: 0 - valLoss: 0.6548711061477661 - trainLoss: 0.6556572914123535\n",
      "cnt: 0 - valLoss: 0.654869794845581 - trainLoss: 0.6556559801101685\n",
      "cnt: 0 - valLoss: 0.6548683047294617 - trainLoss: 0.6556546092033386\n",
      "cnt: 0 - valLoss: 0.6548669934272766 - trainLoss: 0.6556533575057983\n",
      "cnt: 0 - valLoss: 0.6548656225204468 - trainLoss: 0.6556519865989685\n",
      "cnt: 0 - valLoss: 0.6548642516136169 - trainLoss: 0.6556506752967834\n",
      "cnt: 0 - valLoss: 0.6548628807067871 - trainLoss: 0.6556493043899536\n",
      "cnt: 0 - valLoss: 0.6548615097999573 - trainLoss: 0.6556479930877686\n",
      "cnt: 0 - valLoss: 0.6548601388931274 - trainLoss: 0.6556466817855835\n",
      "cnt: 0 - valLoss: 0.6548588275909424 - trainLoss: 0.6556453108787537\n",
      "cnt: 0 - valLoss: 0.6548573970794678 - trainLoss: 0.6556439995765686\n",
      "cnt: 0 - valLoss: 0.6548560857772827 - trainLoss: 0.6556427478790283\n",
      "cnt: 0 - valLoss: 0.6548547744750977 - trainLoss: 0.6556414365768433\n",
      "cnt: 0 - valLoss: 0.6548534035682678 - trainLoss: 0.655640184879303\n",
      "cnt: 0 - valLoss: 0.6548520922660828 - trainLoss: 0.6556388735771179\n",
      "cnt: 0 - valLoss: 0.6548506617546082 - trainLoss: 0.6556375026702881\n",
      "cnt: 0 - valLoss: 0.6548492908477783 - trainLoss: 0.6556362509727478\n",
      "cnt: 0 - valLoss: 0.6548479199409485 - trainLoss: 0.655634880065918\n",
      "cnt: 0 - valLoss: 0.6548465490341187 - trainLoss: 0.6556335687637329\n",
      "cnt: 0 - valLoss: 0.6548452377319336 - trainLoss: 0.6556322574615479\n",
      "cnt: 0 - valLoss: 0.654843807220459 - trainLoss: 0.6556310057640076\n",
      "cnt: 0 - valLoss: 0.6548424363136292 - trainLoss: 0.6556296348571777\n",
      "cnt: 0 - valLoss: 0.6548410654067993 - trainLoss: 0.6556283235549927\n",
      "cnt: 0 - valLoss: 0.6548396944999695 - trainLoss: 0.6556270122528076\n",
      "cnt: 0 - valLoss: 0.6548383235931396 - trainLoss: 0.6556257009506226\n",
      "cnt: 0 - valLoss: 0.6548370122909546 - trainLoss: 0.6556243896484375\n",
      "cnt: 0 - valLoss: 0.65483558177948 - trainLoss: 0.6556230783462524\n",
      "cnt: 0 - valLoss: 0.6548343300819397 - trainLoss: 0.6556217670440674\n",
      "cnt: 0 - valLoss: 0.6548328995704651 - trainLoss: 0.6556203365325928\n",
      "cnt: 0 - valLoss: 0.6548316478729248 - trainLoss: 0.6556189060211182\n",
      "cnt: 0 - valLoss: 0.6548302173614502 - trainLoss: 0.6556175351142883\n",
      "cnt: 0 - valLoss: 0.6548288464546204 - trainLoss: 0.6556161642074585\n",
      "cnt: 0 - valLoss: 0.6548274755477905 - trainLoss: 0.6556148529052734\n",
      "cnt: 0 - valLoss: 0.6548261046409607 - trainLoss: 0.6556134223937988\n",
      "cnt: 0 - valLoss: 0.6548247337341309 - trainLoss: 0.655612051486969\n",
      "cnt: 0 - valLoss: 0.654823362827301 - trainLoss: 0.6556106805801392\n",
      "cnt: 0 - valLoss: 0.6548219919204712 - trainLoss: 0.6556092500686646\n",
      "cnt: 0 - valLoss: 0.6548206806182861 - trainLoss: 0.6556079387664795\n",
      "cnt: 0 - valLoss: 0.6548193097114563 - trainLoss: 0.6556065082550049\n",
      "cnt: 0 - valLoss: 0.6548179984092712 - trainLoss: 0.655605137348175\n",
      "cnt: 0 - valLoss: 0.6548166275024414 - trainLoss: 0.6556037664413452\n",
      "cnt: 0 - valLoss: 0.6548153162002563 - trainLoss: 0.6556023955345154\n",
      "cnt: 0 - valLoss: 0.6548138856887817 - trainLoss: 0.6556010246276855\n",
      "cnt: 0 - valLoss: 0.6548125147819519 - trainLoss: 0.6555996537208557\n",
      "cnt: 0 - valLoss: 0.6548111438751221 - trainLoss: 0.6555982828140259\n",
      "cnt: 0 - valLoss: 0.654809832572937 - trainLoss: 0.6555968523025513\n",
      "cnt: 0 - valLoss: 0.6548084020614624 - trainLoss: 0.6555954813957214\n",
      "cnt: 0 - valLoss: 0.6548070311546326 - trainLoss: 0.6555940508842468\n",
      "cnt: 0 - valLoss: 0.6548056602478027 - trainLoss: 0.6555927395820618\n",
      "cnt: 0 - valLoss: 0.6548042893409729 - trainLoss: 0.6555913090705872\n",
      "cnt: 0 - valLoss: 0.6548029184341431 - trainLoss: 0.6555899381637573\n",
      "cnt: 0 - valLoss: 0.6548015475273132 - trainLoss: 0.6555885672569275\n",
      "cnt: 0 - valLoss: 0.654800295829773 - trainLoss: 0.6555870771408081\n",
      "cnt: 0 - valLoss: 0.6547989249229431 - trainLoss: 0.6555858254432678\n",
      "cnt: 0 - valLoss: 0.6547974944114685 - trainLoss: 0.6555843949317932\n",
      "cnt: 0 - valLoss: 0.6547961235046387 - trainLoss: 0.6555830240249634\n",
      "cnt: 0 - valLoss: 0.6547948122024536 - trainLoss: 0.6555815935134888\n",
      "cnt: 0 - valLoss: 0.6547934412956238 - trainLoss: 0.6555802226066589\n",
      "cnt: 0 - valLoss: 0.654792070388794 - trainLoss: 0.6555788516998291\n",
      "cnt: 0 - valLoss: 0.6547906994819641 - trainLoss: 0.6555774211883545\n",
      "cnt: 0 - valLoss: 0.6547892689704895 - trainLoss: 0.6555760502815247\n",
      "cnt: 0 - valLoss: 0.6547878384590149 - trainLoss: 0.6555746793746948\n",
      "cnt: 0 - valLoss: 0.6547865867614746 - trainLoss: 0.6555732488632202\n",
      "cnt: 0 - valLoss: 0.65478515625 - trainLoss: 0.6555718779563904\n",
      "cnt: 0 - valLoss: 0.6547837853431702 - trainLoss: 0.6555705070495605\n",
      "cnt: 0 - valLoss: 0.6547824144363403 - trainLoss: 0.6555691361427307\n",
      "cnt: 0 - valLoss: 0.6547811031341553 - trainLoss: 0.6555676460266113\n",
      "cnt: 0 - valLoss: 0.6547797322273254 - trainLoss: 0.6555662751197815\n",
      "cnt: 0 - valLoss: 0.6547783613204956 - trainLoss: 0.6555649042129517\n",
      "cnt: 0 - valLoss: 0.6547769904136658 - trainLoss: 0.6555635333061218\n",
      "cnt: 0 - valLoss: 0.6547756195068359 - trainLoss: 0.655562162399292\n",
      "cnt: 0 - valLoss: 0.6547742486000061 - trainLoss: 0.6555607914924622\n",
      "cnt: 0 - valLoss: 0.6547728776931763 - trainLoss: 0.6555593609809875\n",
      "cnt: 0 - valLoss: 0.6547715067863464 - trainLoss: 0.6555579304695129\n",
      "cnt: 0 - valLoss: 0.6547701358795166 - trainLoss: 0.6555565595626831\n",
      "cnt: 0 - valLoss: 0.654768705368042 - trainLoss: 0.6555551290512085\n",
      "cnt: 0 - valLoss: 0.6547673940658569 - trainLoss: 0.6555538177490234\n",
      "cnt: 0 - valLoss: 0.6547659635543823 - trainLoss: 0.6555524468421936\n",
      "cnt: 0 - valLoss: 0.6547646522521973 - trainLoss: 0.6555509567260742\n",
      "cnt: 0 - valLoss: 0.6547632813453674 - trainLoss: 0.6555495858192444\n",
      "cnt: 0 - valLoss: 0.6547619700431824 - trainLoss: 0.6555482149124146\n",
      "cnt: 0 - valLoss: 0.6547605991363525 - trainLoss: 0.6555467844009399\n",
      "cnt: 0 - valLoss: 0.6547591686248779 - trainLoss: 0.6555454134941101\n",
      "cnt: 0 - valLoss: 0.6547577977180481 - trainLoss: 0.6555440425872803\n",
      "cnt: 0 - valLoss: 0.6547564268112183 - trainLoss: 0.6555426716804504\n",
      "cnt: 0 - valLoss: 0.6547550559043884 - trainLoss: 0.6555413007736206\n",
      "cnt: 0 - valLoss: 0.6547536253929138 - trainLoss: 0.6555398106575012\n",
      "cnt: 0 - valLoss: 0.6547523140907288 - trainLoss: 0.6555384397506714\n",
      "cnt: 0 - valLoss: 0.6547508835792542 - trainLoss: 0.6555370092391968\n",
      "cnt: 0 - valLoss: 0.6547495126724243 - trainLoss: 0.6555356383323669\n",
      "cnt: 0 - valLoss: 0.6547481417655945 - trainLoss: 0.6555342674255371\n",
      "cnt: 0 - valLoss: 0.6547467112541199 - trainLoss: 0.6555328369140625\n",
      "cnt: 0 - valLoss: 0.6547453999519348 - trainLoss: 0.6555314064025879\n",
      "cnt: 0 - valLoss: 0.654744029045105 - trainLoss: 0.6555300354957581\n",
      "cnt: 0 - valLoss: 0.6547427177429199 - trainLoss: 0.6555286645889282\n",
      "cnt: 0 - valLoss: 0.6547412872314453 - trainLoss: 0.6555272340774536\n",
      "cnt: 0 - valLoss: 0.6547399163246155 - trainLoss: 0.6555258631706238\n",
      "cnt: 0 - valLoss: 0.6547385454177856 - trainLoss: 0.6555244326591492\n",
      "cnt: 0 - valLoss: 0.654737114906311 - trainLoss: 0.6555230617523193\n",
      "cnt: 0 - valLoss: 0.654735803604126 - trainLoss: 0.6555216312408447\n",
      "cnt: 0 - valLoss: 0.6547343730926514 - trainLoss: 0.6555202603340149\n",
      "cnt: 0 - valLoss: 0.6547330617904663 - trainLoss: 0.6555188894271851\n",
      "cnt: 0 - valLoss: 0.6547316908836365 - trainLoss: 0.6555173993110657\n",
      "cnt: 0 - valLoss: 0.6547302603721619 - trainLoss: 0.6555160284042358\n",
      "cnt: 0 - valLoss: 0.654728889465332 - trainLoss: 0.6555145978927612\n",
      "cnt: 0 - valLoss: 0.6547274589538574 - trainLoss: 0.6555131673812866\n",
      "cnt: 0 - valLoss: 0.6547262072563171 - trainLoss: 0.6555117964744568\n",
      "cnt: 0 - valLoss: 0.6547248363494873 - trainLoss: 0.655510425567627\n",
      "cnt: 0 - valLoss: 0.6547234654426575 - trainLoss: 0.6555089950561523\n",
      "cnt: 0 - valLoss: 0.6547220349311829 - trainLoss: 0.6555076241493225\n",
      "cnt: 0 - valLoss: 0.654720664024353 - trainLoss: 0.6555062532424927\n",
      "cnt: 0 - valLoss: 0.6547192335128784 - trainLoss: 0.6555047631263733\n",
      "cnt: 0 - valLoss: 0.6547178626060486 - trainLoss: 0.6555033922195435\n",
      "cnt: 0 - valLoss: 0.6547164916992188 - trainLoss: 0.6555020213127136\n",
      "cnt: 0 - valLoss: 0.6547150611877441 - trainLoss: 0.6555006504058838\n",
      "cnt: 0 - valLoss: 0.6547136902809143 - trainLoss: 0.6554991602897644\n",
      "cnt: 0 - valLoss: 0.6547123193740845 - trainLoss: 0.6554977297782898\n",
      "cnt: 0 - valLoss: 0.6547109484672546 - trainLoss: 0.65549635887146\n",
      "cnt: 0 - valLoss: 0.6547095775604248 - trainLoss: 0.6554948687553406\n",
      "cnt: 0 - valLoss: 0.654708206653595 - trainLoss: 0.6554934978485107\n",
      "cnt: 0 - valLoss: 0.6547068953514099 - trainLoss: 0.6554921269416809\n",
      "cnt: 0 - valLoss: 0.6547054648399353 - trainLoss: 0.6554906964302063\n",
      "cnt: 0 - valLoss: 0.6547040343284607 - trainLoss: 0.6554893255233765\n",
      "cnt: 0 - valLoss: 0.6547026634216309 - trainLoss: 0.6554878950119019\n",
      "cnt: 0 - valLoss: 0.654701292514801 - trainLoss: 0.655486524105072\n",
      "cnt: 0 - valLoss: 0.6546999216079712 - trainLoss: 0.6554850935935974\n",
      "cnt: 0 - valLoss: 0.6546984910964966 - trainLoss: 0.6554836630821228\n",
      "cnt: 0 - valLoss: 0.6546971201896667 - trainLoss: 0.655482292175293\n",
      "cnt: 0 - valLoss: 0.6546957492828369 - trainLoss: 0.6554808020591736\n",
      "cnt: 0 - valLoss: 0.6546943783760071 - trainLoss: 0.655479371547699\n",
      "cnt: 0 - valLoss: 0.6546929478645325 - trainLoss: 0.6554780006408691\n",
      "cnt: 0 - valLoss: 0.6546915173530579 - trainLoss: 0.6554766297340393\n",
      "cnt: 0 - valLoss: 0.6546902656555176 - trainLoss: 0.6554751992225647\n",
      "cnt: 0 - valLoss: 0.6546887755393982 - trainLoss: 0.6554738283157349\n",
      "cnt: 0 - valLoss: 0.6546874046325684 - trainLoss: 0.6554723978042603\n",
      "cnt: 0 - valLoss: 0.6546860337257385 - trainLoss: 0.6554709672927856\n",
      "cnt: 0 - valLoss: 0.6546846628189087 - trainLoss: 0.655469536781311\n",
      "cnt: 0 - valLoss: 0.6546832919120789 - trainLoss: 0.6554681658744812\n",
      "cnt: 0 - valLoss: 0.6546818614006042 - trainLoss: 0.6554667353630066\n",
      "cnt: 0 - valLoss: 0.6546804904937744 - trainLoss: 0.6554652452468872\n",
      "cnt: 0 - valLoss: 0.6546791195869446 - trainLoss: 0.6554638743400574\n",
      "cnt: 0 - valLoss: 0.6546777486801147 - trainLoss: 0.6554624438285828\n",
      "cnt: 0 - valLoss: 0.6546762585639954 - trainLoss: 0.6554610133171082\n",
      "cnt: 0 - valLoss: 0.6546748876571655 - trainLoss: 0.6554595828056335\n",
      "cnt: 0 - valLoss: 0.6546735167503357 - trainLoss: 0.6554581522941589\n",
      "cnt: 0 - valLoss: 0.6546722054481506 - trainLoss: 0.6554567813873291\n",
      "cnt: 0 - valLoss: 0.6546708345413208 - trainLoss: 0.6554554104804993\n",
      "cnt: 0 - valLoss: 0.6546694040298462 - trainLoss: 0.6554539203643799\n",
      "cnt: 0 - valLoss: 0.6546680331230164 - trainLoss: 0.65545254945755\n",
      "cnt: 0 - valLoss: 0.6546666622161865 - trainLoss: 0.6554511189460754\n",
      "cnt: 0 - valLoss: 0.6546652317047119 - trainLoss: 0.6554497480392456\n",
      "cnt: 0 - valLoss: 0.6546638011932373 - trainLoss: 0.6554482579231262\n",
      "cnt: 0 - valLoss: 0.6546624898910522 - trainLoss: 0.6554467678070068\n",
      "cnt: 0 - valLoss: 0.6546610593795776 - trainLoss: 0.655445396900177\n",
      "cnt: 0 - valLoss: 0.654659628868103 - trainLoss: 0.6554439067840576\n",
      "cnt: 0 - valLoss: 0.6546582579612732 - trainLoss: 0.655442476272583\n",
      "cnt: 0 - valLoss: 0.6546568870544434 - trainLoss: 0.6554410457611084\n",
      "cnt: 0 - valLoss: 0.6546555161476135 - trainLoss: 0.6554396152496338\n",
      "cnt: 0 - valLoss: 0.6546540856361389 - trainLoss: 0.6554381251335144\n",
      "cnt: 0 - valLoss: 0.6546527743339539 - trainLoss: 0.6554366946220398\n",
      "cnt: 0 - valLoss: 0.6546513438224792 - trainLoss: 0.6554352045059204\n",
      "cnt: 0 - valLoss: 0.6546499729156494 - trainLoss: 0.6554337739944458\n",
      "cnt: 0 - valLoss: 0.6546486020088196 - trainLoss: 0.6554323434829712\n",
      "cnt: 0 - valLoss: 0.654647171497345 - trainLoss: 0.6554308533668518\n",
      "cnt: 0 - valLoss: 0.6546458005905151 - trainLoss: 0.6554294228553772\n",
      "cnt: 0 - valLoss: 0.6546443700790405 - trainLoss: 0.6554279327392578\n",
      "cnt: 0 - valLoss: 0.6546429991722107 - trainLoss: 0.6554265022277832\n",
      "cnt: 0 - valLoss: 0.6546416282653809 - trainLoss: 0.6554250121116638\n",
      "cnt: 0 - valLoss: 0.654640257358551 - trainLoss: 0.6554235816001892\n",
      "cnt: 0 - valLoss: 0.6546387672424316 - trainLoss: 0.6554221510887146\n",
      "cnt: 0 - valLoss: 0.6546373963356018 - trainLoss: 0.6554206609725952\n",
      "cnt: 0 - valLoss: 0.6546360850334167 - trainLoss: 0.6554192304611206\n",
      "cnt: 0 - valLoss: 0.6546346545219421 - trainLoss: 0.6554177403450012\n",
      "cnt: 0 - valLoss: 0.6546332836151123 - trainLoss: 0.6554163694381714\n",
      "cnt: 0 - valLoss: 0.6546318531036377 - trainLoss: 0.655414879322052\n",
      "cnt: 0 - valLoss: 0.6546304821968079 - trainLoss: 0.6554133892059326\n",
      "cnt: 0 - valLoss: 0.654629111289978 - trainLoss: 0.655411958694458\n",
      "cnt: 0 - valLoss: 0.6546276807785034 - trainLoss: 0.6554104685783386\n",
      "cnt: 0 - valLoss: 0.6546263694763184 - trainLoss: 0.6554090976715088\n",
      "cnt: 0 - valLoss: 0.654624879360199 - trainLoss: 0.6554076075553894\n",
      "cnt: 0 - valLoss: 0.6546235084533691 - trainLoss: 0.65540611743927\n",
      "cnt: 0 - valLoss: 0.6546221375465393 - trainLoss: 0.6554046869277954\n",
      "cnt: 0 - valLoss: 0.6546207070350647 - trainLoss: 0.655403196811676\n",
      "cnt: 0 - valLoss: 0.6546192765235901 - trainLoss: 0.6554017663002014\n",
      "cnt: 0 - valLoss: 0.6546180248260498 - trainLoss: 0.655400276184082\n",
      "cnt: 0 - valLoss: 0.6546165347099304 - trainLoss: 0.6553987860679626\n",
      "cnt: 0 - valLoss: 0.6546151041984558 - trainLoss: 0.6553974151611328\n",
      "cnt: 0 - valLoss: 0.654613733291626 - trainLoss: 0.6553959250450134\n",
      "cnt: 0 - valLoss: 0.6546123623847961 - trainLoss: 0.6553944945335388\n",
      "cnt: 0 - valLoss: 0.6546109318733215 - trainLoss: 0.6553930044174194\n",
      "cnt: 0 - valLoss: 0.6546096205711365 - trainLoss: 0.6553915143013\n",
      "cnt: 0 - valLoss: 0.6546081304550171 - trainLoss: 0.6553900837898254\n",
      "cnt: 0 - valLoss: 0.6546067595481873 - trainLoss: 0.655388593673706\n",
      "cnt: 0 - valLoss: 0.6546053290367126 - trainLoss: 0.6553871631622314\n",
      "cnt: 0 - valLoss: 0.6546039581298828 - trainLoss: 0.6553856730461121\n",
      "cnt: 0 - valLoss: 0.654602587223053 - trainLoss: 0.6553842425346375\n",
      "cnt: 0 - valLoss: 0.6546010971069336 - trainLoss: 0.6553828120231628\n",
      "cnt: 0 - valLoss: 0.6545997858047485 - trainLoss: 0.6553813219070435\n",
      "cnt: 0 - valLoss: 0.6545984148979187 - trainLoss: 0.6553798913955688\n",
      "cnt: 0 - valLoss: 0.6545969843864441 - trainLoss: 0.6553784012794495\n",
      "cnt: 0 - valLoss: 0.6545956134796143 - trainLoss: 0.6553769111633301\n",
      "cnt: 0 - valLoss: 0.6545941829681396 - trainLoss: 0.6553755402565002\n",
      "cnt: 0 - valLoss: 0.654592752456665 - trainLoss: 0.6553740501403809\n",
      "cnt: 0 - valLoss: 0.6545913815498352 - trainLoss: 0.6553725600242615\n",
      "cnt: 0 - valLoss: 0.6545899510383606 - trainLoss: 0.6553711295127869\n",
      "cnt: 0 - valLoss: 0.654588520526886 - trainLoss: 0.6553696990013123\n",
      "cnt: 0 - valLoss: 0.6545870900154114 - trainLoss: 0.6553682684898376\n",
      "cnt: 0 - valLoss: 0.6545857191085815 - trainLoss: 0.6553667187690735\n",
      "cnt: 0 - valLoss: 0.6545842885971069 - trainLoss: 0.6553652882575989\n",
      "cnt: 0 - valLoss: 0.6545829176902771 - trainLoss: 0.6553638577461243\n",
      "cnt: 0 - valLoss: 0.6545815467834473 - trainLoss: 0.6553623676300049\n",
      "cnt: 0 - valLoss: 0.6545801758766174 - trainLoss: 0.6553609371185303\n",
      "cnt: 0 - valLoss: 0.6545788049697876 - trainLoss: 0.6553594470024109\n",
      "cnt: 0 - valLoss: 0.6545773148536682 - trainLoss: 0.6553580164909363\n",
      "cnt: 0 - valLoss: 0.6545758843421936 - trainLoss: 0.6553565263748169\n",
      "cnt: 0 - valLoss: 0.6545745134353638 - trainLoss: 0.6553550958633423\n",
      "cnt: 0 - valLoss: 0.6545730829238892 - trainLoss: 0.6553536653518677\n",
      "cnt: 0 - valLoss: 0.6545717716217041 - trainLoss: 0.6553521752357483\n",
      "cnt: 0 - valLoss: 0.6545702815055847 - trainLoss: 0.6553506255149841\n",
      "cnt: 0 - valLoss: 0.6545688509941101 - trainLoss: 0.6553492546081543\n",
      "cnt: 0 - valLoss: 0.6545674800872803 - trainLoss: 0.6553477644920349\n",
      "cnt: 0 - valLoss: 0.6545660495758057 - trainLoss: 0.6553463339805603\n",
      "cnt: 0 - valLoss: 0.6545646786689758 - trainLoss: 0.6553448438644409\n",
      "cnt: 0 - valLoss: 0.654563307762146 - trainLoss: 0.6553433537483215\n",
      "cnt: 0 - valLoss: 0.6545619368553162 - trainLoss: 0.6553419232368469\n",
      "cnt: 0 - valLoss: 0.6545605659484863 - trainLoss: 0.6553404927253723\n",
      "cnt: 0 - valLoss: 0.6545590758323669 - trainLoss: 0.6553389430046082\n",
      "cnt: 0 - valLoss: 0.6545576453208923 - trainLoss: 0.6553375124931335\n",
      "cnt: 0 - valLoss: 0.6545562148094177 - trainLoss: 0.6553360819816589\n",
      "cnt: 0 - valLoss: 0.6545548439025879 - trainLoss: 0.6553345918655396\n",
      "cnt: 0 - valLoss: 0.6545534729957581 - trainLoss: 0.6553331017494202\n",
      "cnt: 0 - valLoss: 0.6545519828796387 - trainLoss: 0.6553316712379456\n",
      "cnt: 0 - valLoss: 0.6545506119728088 - trainLoss: 0.6553301811218262\n",
      "cnt: 0 - valLoss: 0.6545491814613342 - trainLoss: 0.6553286910057068\n",
      "cnt: 0 - valLoss: 0.6545478105545044 - trainLoss: 0.6553272604942322\n",
      "cnt: 0 - valLoss: 0.6545464396476746 - trainLoss: 0.6553258299827576\n",
      "cnt: 0 - valLoss: 0.6545450687408447 - trainLoss: 0.6553243398666382\n",
      "cnt: 0 - valLoss: 0.6545435786247253 - trainLoss: 0.6553229093551636\n",
      "cnt: 0 - valLoss: 0.6545421481132507 - trainLoss: 0.6553213596343994\n",
      "cnt: 0 - valLoss: 0.6545406579971313 - trainLoss: 0.6553198099136353\n",
      "cnt: 0 - valLoss: 0.6545392274856567 - trainLoss: 0.6553183197975159\n",
      "cnt: 0 - valLoss: 0.6545377373695374 - trainLoss: 0.6553167700767517\n",
      "cnt: 0 - valLoss: 0.6545361876487732 - trainLoss: 0.655315101146698\n",
      "cnt: 0 - valLoss: 0.6545347571372986 - trainLoss: 0.6553135514259338\n",
      "cnt: 0 - valLoss: 0.6545332074165344 - trainLoss: 0.6553120017051697\n",
      "cnt: 0 - valLoss: 0.6545317769050598 - trainLoss: 0.6553103923797607\n",
      "cnt: 0 - valLoss: 0.6545303463935852 - trainLoss: 0.6553089022636414\n",
      "cnt: 0 - valLoss: 0.6545288562774658 - trainLoss: 0.6553073525428772\n",
      "cnt: 0 - valLoss: 0.6545273661613464 - trainLoss: 0.6553056836128235\n",
      "cnt: 0 - valLoss: 0.6545259356498718 - trainLoss: 0.6553041934967041\n",
      "cnt: 0 - valLoss: 0.6545244455337524 - trainLoss: 0.6553026437759399\n",
      "cnt: 0 - valLoss: 0.6545229554176331 - trainLoss: 0.6553010940551758\n",
      "cnt: 0 - valLoss: 0.6545214653015137 - trainLoss: 0.6552994847297668\n",
      "cnt: 0 - valLoss: 0.6545200347900391 - trainLoss: 0.6552979350090027\n",
      "cnt: 0 - valLoss: 0.6545184850692749 - trainLoss: 0.6552963852882385\n",
      "cnt: 0 - valLoss: 0.6545169949531555 - trainLoss: 0.6552947759628296\n",
      "cnt: 0 - valLoss: 0.6545155644416809 - trainLoss: 0.6552932262420654\n",
      "cnt: 0 - valLoss: 0.6545140743255615 - trainLoss: 0.6552916765213013\n",
      "cnt: 0 - valLoss: 0.6545125842094421 - trainLoss: 0.6552901268005371\n",
      "cnt: 0 - valLoss: 0.654511034488678 - trainLoss: 0.6552885174751282\n",
      "cnt: 0 - valLoss: 0.6545096635818481 - trainLoss: 0.6552870273590088\n",
      "cnt: 0 - valLoss: 0.6545081734657288 - trainLoss: 0.6552854180335999\n",
      "cnt: 0 - valLoss: 0.6545067429542542 - trainLoss: 0.6552839279174805\n",
      "cnt: 0 - valLoss: 0.6545052528381348 - trainLoss: 0.6552823185920715\n",
      "cnt: 0 - valLoss: 0.6545037627220154 - trainLoss: 0.6552807688713074\n",
      "cnt: 0 - valLoss: 0.654502272605896 - trainLoss: 0.6552791595458984\n",
      "cnt: 0 - valLoss: 0.6545007228851318 - trainLoss: 0.6552776098251343\n",
      "cnt: 0 - valLoss: 0.6544992923736572 - trainLoss: 0.6552760601043701\n",
      "cnt: 0 - valLoss: 0.6544978022575378 - trainLoss: 0.6552744507789612\n",
      "cnt: 0 - valLoss: 0.6544963121414185 - trainLoss: 0.655272901058197\n",
      "cnt: 0 - valLoss: 0.6544948220252991 - trainLoss: 0.6552714109420776\n",
      "cnt: 0 - valLoss: 0.6544933319091797 - trainLoss: 0.6552698016166687\n",
      "cnt: 0 - valLoss: 0.6544919013977051 - trainLoss: 0.6552681922912598\n",
      "cnt: 0 - valLoss: 0.6544904112815857 - trainLoss: 0.6552667021751404\n",
      "cnt: 0 - valLoss: 0.6544889807701111 - trainLoss: 0.6552650332450867\n",
      "cnt: 0 - valLoss: 0.6544875502586365 - trainLoss: 0.6552634835243225\n",
      "cnt: 0 - valLoss: 0.6544860005378723 - trainLoss: 0.6552619934082031\n",
      "cnt: 0 - valLoss: 0.6544844508171082 - trainLoss: 0.655260443687439\n",
      "cnt: 0 - valLoss: 0.6544829607009888 - trainLoss: 0.6552587747573853\n",
      "cnt: 0 - valLoss: 0.6544815301895142 - trainLoss: 0.6552572846412659\n",
      "cnt: 0 - valLoss: 0.6544800400733948 - trainLoss: 0.6552556753158569\n",
      "cnt: 0 - valLoss: 0.6544785499572754 - trainLoss: 0.655254065990448\n",
      "cnt: 0 - valLoss: 0.6544770002365112 - trainLoss: 0.6552524566650391\n",
      "cnt: 0 - valLoss: 0.6544755101203918 - trainLoss: 0.6552509069442749\n",
      "cnt: 0 - valLoss: 0.6544740796089172 - trainLoss: 0.6552493572235107\n",
      "cnt: 0 - valLoss: 0.6544727087020874 - trainLoss: 0.6552478671073914\n",
      "cnt: 0 - valLoss: 0.6544710993766785 - trainLoss: 0.6552462577819824\n",
      "cnt: 0 - valLoss: 0.6544696688652039 - trainLoss: 0.6552447080612183\n",
      "cnt: 0 - valLoss: 0.6544681191444397 - trainLoss: 0.6552431583404541\n",
      "cnt: 0 - valLoss: 0.6544666290283203 - trainLoss: 0.6552416086196899\n",
      "cnt: 0 - valLoss: 0.6544651389122009 - trainLoss: 0.6552399396896362\n",
      "cnt: 0 - valLoss: 0.6544637084007263 - trainLoss: 0.6552384495735168\n",
      "cnt: 0 - valLoss: 0.6544621586799622 - trainLoss: 0.6552367806434631\n",
      "cnt: 0 - valLoss: 0.6544606685638428 - trainLoss: 0.655235230922699\n",
      "cnt: 0 - valLoss: 0.6544591784477234 - trainLoss: 0.6552336812019348\n",
      "cnt: 0 - valLoss: 0.654457688331604 - trainLoss: 0.6552321314811707\n",
      "cnt: 0 - valLoss: 0.6544563174247742 - trainLoss: 0.6552305221557617\n",
      "cnt: 0 - valLoss: 0.6544548869132996 - trainLoss: 0.6552289724349976\n",
      "cnt: 0 - valLoss: 0.6544533371925354 - trainLoss: 0.6552274227142334\n",
      "cnt: 0 - valLoss: 0.654451847076416 - trainLoss: 0.655225932598114\n",
      "cnt: 0 - valLoss: 0.6544504761695862 - trainLoss: 0.6552243232727051\n",
      "cnt: 0 - valLoss: 0.6544490456581116 - trainLoss: 0.6552227735519409\n",
      "cnt: 0 - valLoss: 0.6544475555419922 - trainLoss: 0.6552212834358215\n",
      "cnt: 0 - valLoss: 0.654446005821228 - trainLoss: 0.6552196741104126\n",
      "cnt: 0 - valLoss: 0.6544446349143982 - trainLoss: 0.6552181243896484\n",
      "cnt: 0 - valLoss: 0.6544431447982788 - trainLoss: 0.6552165746688843\n",
      "cnt: 0 - valLoss: 0.6544417142868042 - trainLoss: 0.6552150845527649\n",
      "cnt: 0 - valLoss: 0.65444016456604 - trainLoss: 0.6552135348320007\n",
      "cnt: 0 - valLoss: 0.6544387936592102 - trainLoss: 0.6552119851112366\n",
      "cnt: 0 - valLoss: 0.6544373035430908 - trainLoss: 0.6552103757858276\n",
      "cnt: 0 - valLoss: 0.654435932636261 - trainLoss: 0.6552088260650635\n",
      "cnt: 0 - valLoss: 0.6544344425201416 - trainLoss: 0.6552072763442993\n",
      "cnt: 0 - valLoss: 0.6544329524040222 - trainLoss: 0.6552056670188904\n",
      "cnt: 0 - valLoss: 0.6544315218925476 - trainLoss: 0.655204176902771\n",
      "cnt: 0 - valLoss: 0.6544300317764282 - trainLoss: 0.6552026271820068\n",
      "cnt: 0 - valLoss: 0.6544284820556641 - trainLoss: 0.6552010774612427\n",
      "cnt: 0 - valLoss: 0.6544271111488342 - trainLoss: 0.6551995873451233\n",
      "cnt: 0 - valLoss: 0.6544256210327148 - trainLoss: 0.6551980376243591\n",
      "cnt: 0 - valLoss: 0.6544241309165955 - trainLoss: 0.6551964282989502\n",
      "cnt: 0 - valLoss: 0.6544227004051208 - trainLoss: 0.655194878578186\n",
      "cnt: 0 - valLoss: 0.654421329498291 - trainLoss: 0.6551933288574219\n",
      "cnt: 0 - valLoss: 0.6544198989868164 - trainLoss: 0.6551918387413025\n",
      "cnt: 0 - valLoss: 0.654418408870697 - trainLoss: 0.6551902294158936\n",
      "cnt: 0 - valLoss: 0.6544169187545776 - trainLoss: 0.6551886200904846\n",
      "cnt: 0 - valLoss: 0.654415488243103 - trainLoss: 0.6551871299743652\n",
      "cnt: 0 - valLoss: 0.6544140577316284 - trainLoss: 0.6551855802536011\n",
      "cnt: 0 - valLoss: 0.6544126272201538 - trainLoss: 0.6551840305328369\n",
      "cnt: 0 - valLoss: 0.6544110774993896 - trainLoss: 0.655182421207428\n",
      "cnt: 0 - valLoss: 0.6544097065925598 - trainLoss: 0.6551808714866638\n",
      "cnt: 0 - valLoss: 0.6544082164764404 - trainLoss: 0.6551792621612549\n",
      "cnt: 0 - valLoss: 0.6544066667556763 - trainLoss: 0.6551777124404907\n",
      "cnt: 0 - valLoss: 0.6544052958488464 - trainLoss: 0.6551761627197266\n",
      "cnt: 0 - valLoss: 0.654403805732727 - trainLoss: 0.6551745533943176\n",
      "cnt: 0 - valLoss: 0.6544023752212524 - trainLoss: 0.6551730036735535\n",
      "cnt: 0 - valLoss: 0.6544009447097778 - trainLoss: 0.6551715135574341\n",
      "cnt: 0 - valLoss: 0.6543995141983032 - trainLoss: 0.6551699638366699\n",
      "cnt: 0 - valLoss: 0.6543980240821838 - trainLoss: 0.6551684141159058\n",
      "cnt: 0 - valLoss: 0.6543965935707092 - trainLoss: 0.6551668047904968\n",
      "cnt: 0 - valLoss: 0.6543950438499451 - trainLoss: 0.6551652550697327\n",
      "cnt: 0 - valLoss: 0.6543936729431152 - trainLoss: 0.6551636457443237\n",
      "cnt: 0 - valLoss: 0.6543921828269958 - trainLoss: 0.6551620960235596\n",
      "cnt: 0 - valLoss: 0.6543907523155212 - trainLoss: 0.6551605463027954\n",
      "cnt: 0 - valLoss: 0.6543892621994019 - trainLoss: 0.6551589369773865\n",
      "cnt: 0 - valLoss: 0.6543878316879272 - trainLoss: 0.6551574468612671\n",
      "cnt: 0 - valLoss: 0.6543862819671631 - trainLoss: 0.6551558375358582\n",
      "cnt: 0 - valLoss: 0.6543848514556885 - trainLoss: 0.6551542282104492\n",
      "cnt: 0 - valLoss: 0.6543834209442139 - trainLoss: 0.6551526784896851\n",
      "cnt: 0 - valLoss: 0.654382050037384 - trainLoss: 0.6551511287689209\n",
      "cnt: 0 - valLoss: 0.6543805599212646 - trainLoss: 0.6551494598388672\n",
      "cnt: 0 - valLoss: 0.6543790698051453 - trainLoss: 0.655147910118103\n",
      "cnt: 0 - valLoss: 0.6543776392936707 - trainLoss: 0.6551463603973389\n",
      "cnt: 0 - valLoss: 0.6543762683868408 - trainLoss: 0.6551447510719299\n",
      "cnt: 0 - valLoss: 0.6543747782707214 - trainLoss: 0.655143141746521\n",
      "cnt: 0 - valLoss: 0.6543733477592468 - trainLoss: 0.6551415324211121\n",
      "cnt: 0 - valLoss: 0.6543718576431274 - trainLoss: 0.6551399827003479\n",
      "cnt: 0 - valLoss: 0.6543704271316528 - trainLoss: 0.655138373374939\n",
      "cnt: 0 - valLoss: 0.654369056224823 - trainLoss: 0.6551367044448853\n",
      "cnt: 0 - valLoss: 0.6543676257133484 - trainLoss: 0.6551352143287659\n",
      "cnt: 0 - valLoss: 0.6543661952018738 - trainLoss: 0.6551335453987122\n",
      "cnt: 0 - valLoss: 0.6543647646903992 - trainLoss: 0.655131995677948\n",
      "cnt: 0 - valLoss: 0.6543633341789246 - trainLoss: 0.6551303863525391\n",
      "cnt: 0 - valLoss: 0.6543618440628052 - trainLoss: 0.6551288366317749\n",
      "cnt: 0 - valLoss: 0.6543604135513306 - trainLoss: 0.655127227306366\n",
      "cnt: 0 - valLoss: 0.654358983039856 - trainLoss: 0.655125617980957\n",
      "cnt: 0 - valLoss: 0.6543575525283813 - trainLoss: 0.6551240682601929\n",
      "cnt: 0 - valLoss: 0.654356062412262 - trainLoss: 0.6551224589347839\n",
      "cnt: 0 - valLoss: 0.6543546319007874 - trainLoss: 0.655120849609375\n",
      "cnt: 0 - valLoss: 0.6543532013893127 - trainLoss: 0.6551192402839661\n",
      "cnt: 0 - valLoss: 0.6543517112731934 - trainLoss: 0.6551176309585571\n",
      "cnt: 0 - valLoss: 0.6543503403663635 - trainLoss: 0.655116081237793\n",
      "cnt: 0 - valLoss: 0.6543489098548889 - trainLoss: 0.655114471912384\n",
      "cnt: 0 - valLoss: 0.6543474793434143 - trainLoss: 0.6551128625869751\n",
      "cnt: 0 - valLoss: 0.6543460488319397 - trainLoss: 0.6551113128662109\n",
      "cnt: 0 - valLoss: 0.6543446183204651 - trainLoss: 0.655109703540802\n",
      "cnt: 0 - valLoss: 0.6543431282043457 - trainLoss: 0.6551080346107483\n",
      "cnt: 0 - valLoss: 0.6543416976928711 - trainLoss: 0.6551064252853394\n",
      "cnt: 0 - valLoss: 0.6543402671813965 - trainLoss: 0.6551048755645752\n",
      "cnt: 0 - valLoss: 0.6543387770652771 - trainLoss: 0.6551032066345215\n",
      "cnt: 0 - valLoss: 0.6543373465538025 - trainLoss: 0.6551016569137573\n",
      "cnt: 0 - valLoss: 0.6543358564376831 - trainLoss: 0.6551000475883484\n",
      "cnt: 0 - valLoss: 0.6543344259262085 - trainLoss: 0.6550984382629395\n",
      "cnt: 0 - valLoss: 0.6543329358100891 - trainLoss: 0.6550968289375305\n",
      "cnt: 0 - valLoss: 0.654331624507904 - trainLoss: 0.6550952792167664\n",
      "cnt: 0 - valLoss: 0.6543301939964294 - trainLoss: 0.6550937294960022\n",
      "cnt: 0 - valLoss: 0.6543287038803101 - trainLoss: 0.6550921201705933\n",
      "cnt: 0 - valLoss: 0.6543272137641907 - trainLoss: 0.6550905108451843\n",
      "cnt: 0 - valLoss: 0.6543257832527161 - trainLoss: 0.6550889015197754\n",
      "cnt: 0 - valLoss: 0.6543243527412415 - trainLoss: 0.6550872921943665\n",
      "cnt: 0 - valLoss: 0.6543229222297668 - trainLoss: 0.6550856828689575\n",
      "cnt: 0 - valLoss: 0.6543213725090027 - trainLoss: 0.6550841331481934\n",
      "cnt: 0 - valLoss: 0.6543200016021729 - trainLoss: 0.6550825238227844\n",
      "cnt: 0 - valLoss: 0.6543185710906982 - trainLoss: 0.6550808548927307\n",
      "cnt: 0 - valLoss: 0.6543170809745789 - trainLoss: 0.6550793051719666\n",
      "cnt: 0 - valLoss: 0.6543156504631042 - trainLoss: 0.6550776362419128\n",
      "cnt: 0 - valLoss: 0.6543142795562744 - trainLoss: 0.6550760865211487\n",
      "cnt: 0 - valLoss: 0.6543128490447998 - trainLoss: 0.6550745368003845\n",
      "cnt: 0 - valLoss: 0.6543113589286804 - trainLoss: 0.6550729274749756\n",
      "cnt: 0 - valLoss: 0.6543099284172058 - trainLoss: 0.6550713181495667\n",
      "cnt: 0 - valLoss: 0.6543084383010864 - trainLoss: 0.6550697088241577\n",
      "cnt: 0 - valLoss: 0.6543070077896118 - trainLoss: 0.6550680994987488\n",
      "cnt: 0 - valLoss: 0.6543055772781372 - trainLoss: 0.6550665497779846\n",
      "cnt: 0 - valLoss: 0.6543041467666626 - trainLoss: 0.6550649404525757\n",
      "cnt: 0 - valLoss: 0.6543025970458984 - trainLoss: 0.6550633907318115\n",
      "cnt: 0 - valLoss: 0.6543011665344238 - trainLoss: 0.6550617218017578\n",
      "cnt: 0 - valLoss: 0.6542997360229492 - trainLoss: 0.6550601720809937\n",
      "cnt: 0 - valLoss: 0.6542982459068298 - trainLoss: 0.6550585627555847\n",
      "cnt: 0 - valLoss: 0.6542968153953552 - trainLoss: 0.6550570130348206\n",
      "cnt: 0 - valLoss: 0.6542954444885254 - trainLoss: 0.6550554037094116\n",
      "cnt: 0 - valLoss: 0.654293954372406 - trainLoss: 0.6550537943840027\n",
      "cnt: 0 - valLoss: 0.6542925238609314 - trainLoss: 0.6550522446632385\n",
      "cnt: 0 - valLoss: 0.654291033744812 - trainLoss: 0.6550506353378296\n",
      "cnt: 0 - valLoss: 0.6542894840240479 - trainLoss: 0.6550490260124207\n",
      "cnt: 0 - valLoss: 0.6542880535125732 - trainLoss: 0.6550475358963013\n",
      "cnt: 0 - valLoss: 0.6542865633964539 - trainLoss: 0.6550458669662476\n",
      "cnt: 0 - valLoss: 0.6542851328849792 - trainLoss: 0.6550443172454834\n",
      "cnt: 0 - valLoss: 0.6542836427688599 - trainLoss: 0.6550427079200745\n",
      "cnt: 0 - valLoss: 0.6542821526527405 - trainLoss: 0.6550411581993103\n",
      "cnt: 0 - valLoss: 0.6542806625366211 - trainLoss: 0.6550395488739014\n",
      "cnt: 0 - valLoss: 0.6542791128158569 - trainLoss: 0.6550379395484924\n",
      "cnt: 0 - valLoss: 0.6542777419090271 - trainLoss: 0.6550363898277283\n",
      "cnt: 0 - valLoss: 0.6542763710021973 - trainLoss: 0.6550348401069641\n",
      "cnt: 0 - valLoss: 0.6542748212814331 - trainLoss: 0.6550333499908447\n",
      "cnt: 0 - valLoss: 0.654273271560669 - trainLoss: 0.655031681060791\n",
      "cnt: 0 - valLoss: 0.6542719006538391 - trainLoss: 0.6550300717353821\n",
      "cnt: 0 - valLoss: 0.6542704105377197 - trainLoss: 0.6550284624099731\n",
      "cnt: 0 - valLoss: 0.6542688608169556 - trainLoss: 0.655026912689209\n",
      "cnt: 0 - valLoss: 0.6542673707008362 - trainLoss: 0.6550253033638\n",
      "cnt: 0 - valLoss: 0.6542659997940063 - trainLoss: 0.6550237536430359\n",
      "cnt: 0 - valLoss: 0.6542644500732422 - trainLoss: 0.655022144317627\n",
      "cnt: 0 - valLoss: 0.6542629599571228 - trainLoss: 0.6550205945968628\n",
      "cnt: 0 - valLoss: 0.6542615294456482 - trainLoss: 0.6550190448760986\n",
      "cnt: 0 - valLoss: 0.6542600393295288 - trainLoss: 0.6550173759460449\n",
      "cnt: 0 - valLoss: 0.6542586088180542 - trainLoss: 0.6550158858299255\n",
      "cnt: 0 - valLoss: 0.6542571187019348 - trainLoss: 0.6550142168998718\n",
      "cnt: 0 - valLoss: 0.654255747795105 - trainLoss: 0.6550126671791077\n",
      "cnt: 0 - valLoss: 0.6542543768882751 - trainLoss: 0.6550111770629883\n",
      "cnt: 0 - valLoss: 0.6542528867721558 - trainLoss: 0.6550096273422241\n",
      "cnt: 0 - valLoss: 0.6542514562606812 - trainLoss: 0.6550080180168152\n",
      "cnt: 0 - valLoss: 0.6542500853538513 - trainLoss: 0.655006468296051\n",
      "cnt: 0 - valLoss: 0.6542485952377319 - trainLoss: 0.6550049185752869\n",
      "cnt: 0 - valLoss: 0.6542471647262573 - trainLoss: 0.6550033092498779\n",
      "cnt: 0 - valLoss: 0.6542457938194275 - trainLoss: 0.6550017595291138\n",
      "cnt: 0 - valLoss: 0.6542443037033081 - trainLoss: 0.6550002098083496\n",
      "cnt: 0 - valLoss: 0.6542429327964783 - trainLoss: 0.6549986600875854\n",
      "cnt: 0 - valLoss: 0.6542415022850037 - trainLoss: 0.6549970507621765\n",
      "cnt: 0 - valLoss: 0.654240071773529 - trainLoss: 0.6549955010414124\n",
      "cnt: 0 - valLoss: 0.6542386412620544 - trainLoss: 0.6549939513206482\n",
      "cnt: 0 - valLoss: 0.6542372107505798 - trainLoss: 0.6549924612045288\n",
      "cnt: 0 - valLoss: 0.6542357802391052 - trainLoss: 0.6549909114837646\n",
      "cnt: 0 - valLoss: 0.6542344093322754 - trainLoss: 0.6549893617630005\n",
      "cnt: 0 - valLoss: 0.6542328596115112 - trainLoss: 0.6549877524375916\n",
      "cnt: 0 - valLoss: 0.6542314887046814 - trainLoss: 0.6549862623214722\n",
      "cnt: 0 - valLoss: 0.6542300581932068 - trainLoss: 0.654984712600708\n",
      "cnt: 0 - valLoss: 0.6542285680770874 - trainLoss: 0.6549831628799438\n",
      "cnt: 0 - valLoss: 0.6542271971702576 - trainLoss: 0.6549816727638245\n",
      "cnt: 0 - valLoss: 0.654225766658783 - trainLoss: 0.6549801230430603\n",
      "cnt: 0 - valLoss: 0.6542243361473083 - trainLoss: 0.6549786329269409\n",
      "cnt: 0 - valLoss: 0.6542229652404785 - trainLoss: 0.6549769639968872\n",
      "cnt: 0 - valLoss: 0.6542214751243591 - trainLoss: 0.6549755334854126\n",
      "cnt: 0 - valLoss: 0.6542200446128845 - trainLoss: 0.6549739837646484\n",
      "cnt: 0 - valLoss: 0.6542186141014099 - trainLoss: 0.6549724340438843\n",
      "cnt: 0 - valLoss: 0.6542172431945801 - trainLoss: 0.6549708843231201\n",
      "cnt: 0 - valLoss: 0.6542157530784607 - trainLoss: 0.654969334602356\n",
      "cnt: 0 - valLoss: 0.6542143225669861 - trainLoss: 0.6549678444862366\n",
      "cnt: 0 - valLoss: 0.6542128920555115 - trainLoss: 0.6549662947654724\n",
      "cnt: 0 - valLoss: 0.6542115211486816 - trainLoss: 0.6549646854400635\n",
      "cnt: 0 - valLoss: 0.6542099714279175 - trainLoss: 0.6549631953239441\n",
      "cnt: 0 - valLoss: 0.6542086005210876 - trainLoss: 0.6549616456031799\n",
      "cnt: 0 - valLoss: 0.6542072296142578 - trainLoss: 0.6549600958824158\n",
      "cnt: 0 - valLoss: 0.6542057394981384 - trainLoss: 0.6549585461616516\n",
      "cnt: 0 - valLoss: 0.6542043089866638 - trainLoss: 0.6549570560455322\n",
      "cnt: 0 - valLoss: 0.654202938079834 - trainLoss: 0.6549554467201233\n",
      "cnt: 0 - valLoss: 0.6542014479637146 - trainLoss: 0.6549539566040039\n",
      "cnt: 0 - valLoss: 0.65420001745224 - trainLoss: 0.654952347278595\n",
      "cnt: 0 - valLoss: 0.6541985869407654 - trainLoss: 0.6549508571624756\n",
      "cnt: 0 - valLoss: 0.6541971564292908 - trainLoss: 0.6549492478370667\n",
      "cnt: 0 - valLoss: 0.6541957259178162 - trainLoss: 0.6549476981163025\n",
      "cnt: 0 - valLoss: 0.6541942358016968 - trainLoss: 0.6549462080001831\n",
      "cnt: 0 - valLoss: 0.6541927456855774 - trainLoss: 0.654944658279419\n",
      "cnt: 0 - valLoss: 0.6541913747787476 - trainLoss: 0.6549431085586548\n",
      "cnt: 0 - valLoss: 0.654189944267273 - trainLoss: 0.6549415588378906\n",
      "cnt: 0 - valLoss: 0.6541885137557983 - trainLoss: 0.6549400687217712\n",
      "cnt: 0 - valLoss: 0.6541870832443237 - trainLoss: 0.6549384593963623\n",
      "cnt: 0 - valLoss: 0.6541856527328491 - trainLoss: 0.6549369096755981\n",
      "cnt: 0 - valLoss: 0.6541842222213745 - trainLoss: 0.6549354195594788\n",
      "cnt: 0 - valLoss: 0.6541827917098999 - trainLoss: 0.6549338102340698\n",
      "cnt: 0 - valLoss: 0.6541813611984253 - trainLoss: 0.6549323201179504\n",
      "cnt: 0 - valLoss: 0.6541798710823059 - trainLoss: 0.6549307107925415\n",
      "cnt: 0 - valLoss: 0.6541783809661865 - trainLoss: 0.6549291610717773\n",
      "cnt: 0 - valLoss: 0.6541770100593567 - trainLoss: 0.654927670955658\n",
      "cnt: 0 - valLoss: 0.6541755199432373 - trainLoss: 0.654926061630249\n",
      "cnt: 0 - valLoss: 0.6541740298271179 - trainLoss: 0.6549245119094849\n",
      "cnt: 0 - valLoss: 0.6541725993156433 - trainLoss: 0.6549229621887207\n",
      "cnt: 0 - valLoss: 0.6541711688041687 - trainLoss: 0.6549214124679565\n",
      "cnt: 0 - valLoss: 0.6541697382926941 - trainLoss: 0.6549198627471924\n",
      "cnt: 0 - valLoss: 0.6541683673858643 - trainLoss: 0.654918372631073\n",
      "cnt: 0 - valLoss: 0.6541668176651001 - trainLoss: 0.6549168229103088\n",
      "cnt: 0 - valLoss: 0.6541654467582703 - trainLoss: 0.6549152135848999\n",
      "cnt: 0 - valLoss: 0.6541640162467957 - trainLoss: 0.6549136638641357\n",
      "cnt: 0 - valLoss: 0.6541625261306763 - trainLoss: 0.6549121737480164\n",
      "cnt: 0 - valLoss: 0.6541610956192017 - trainLoss: 0.6549105644226074\n",
      "cnt: 0 - valLoss: 0.6541596055030823 - trainLoss: 0.6549090147018433\n",
      "cnt: 0 - valLoss: 0.6541582345962524 - trainLoss: 0.6549074649810791\n",
      "cnt: 0 - valLoss: 0.6541567444801331 - trainLoss: 0.6549058556556702\n",
      "cnt: 0 - valLoss: 0.6541553139686584 - trainLoss: 0.654904305934906\n",
      "cnt: 0 - valLoss: 0.6541538238525391 - trainLoss: 0.6549027562141418\n",
      "cnt: 0 - valLoss: 0.6541523337364197 - trainLoss: 0.6549012064933777\n",
      "cnt: 0 - valLoss: 0.6541510224342346 - trainLoss: 0.6548997163772583\n",
      "cnt: 0 - valLoss: 0.65414959192276 - trainLoss: 0.6548980474472046\n",
      "cnt: 0 - valLoss: 0.6541481614112854 - trainLoss: 0.6548964977264404\n",
      "cnt: 0 - valLoss: 0.654146671295166 - trainLoss: 0.654895007610321\n",
      "cnt: 0 - valLoss: 0.6541451811790466 - trainLoss: 0.6548934578895569\n",
      "cnt: 0 - valLoss: 0.654143750667572 - trainLoss: 0.6548917889595032\n",
      "cnt: 0 - valLoss: 0.6541423201560974 - trainLoss: 0.6548902988433838\n",
      "cnt: 0 - valLoss: 0.654140830039978 - trainLoss: 0.6548886895179749\n",
      "cnt: 0 - valLoss: 0.6541393995285034 - trainLoss: 0.6548870801925659\n",
      "cnt: 0 - valLoss: 0.6541379690170288 - trainLoss: 0.6548855900764465\n",
      "cnt: 0 - valLoss: 0.6541365385055542 - trainLoss: 0.6548840403556824\n",
      "cnt: 0 - valLoss: 0.6541350483894348 - trainLoss: 0.6548824310302734\n",
      "cnt: 0 - valLoss: 0.6541335582733154 - trainLoss: 0.6548808813095093\n",
      "cnt: 0 - valLoss: 0.6541323065757751 - trainLoss: 0.6548792719841003\n",
      "cnt: 0 - valLoss: 0.654130756855011 - trainLoss: 0.654877781867981\n",
      "cnt: 0 - valLoss: 0.6541292667388916 - trainLoss: 0.654876172542572\n",
      "cnt: 0 - valLoss: 0.6541278958320618 - trainLoss: 0.6548746228218079\n",
      "cnt: 0 - valLoss: 0.6541263461112976 - trainLoss: 0.6548730134963989\n",
      "cnt: 0 - valLoss: 0.6541249752044678 - trainLoss: 0.6548714637756348\n",
      "cnt: 0 - valLoss: 0.6541234850883484 - trainLoss: 0.6548699140548706\n",
      "cnt: 0 - valLoss: 0.6541220545768738 - trainLoss: 0.6548683643341064\n",
      "cnt: 0 - valLoss: 0.6541206240653992 - trainLoss: 0.6548668146133423\n",
      "cnt: 0 - valLoss: 0.6541191339492798 - trainLoss: 0.6548652648925781\n",
      "cnt: 0 - valLoss: 0.65411776304245 - trainLoss: 0.654863715171814\n",
      "cnt: 0 - valLoss: 0.6541162729263306 - trainLoss: 0.654862105846405\n",
      "cnt: 0 - valLoss: 0.6541147828102112 - trainLoss: 0.6548605561256409\n",
      "cnt: 0 - valLoss: 0.6541134715080261 - trainLoss: 0.6548590064048767\n",
      "cnt: 0 - valLoss: 0.6541119813919067 - trainLoss: 0.6548574566841125\n",
      "cnt: 0 - valLoss: 0.6541106104850769 - trainLoss: 0.6548559069633484\n",
      "cnt: 0 - valLoss: 0.6541090607643127 - trainLoss: 0.6548543572425842\n",
      "cnt: 0 - valLoss: 0.6541076302528381 - trainLoss: 0.6548527479171753\n",
      "cnt: 0 - valLoss: 0.6541061997413635 - trainLoss: 0.6548512578010559\n",
      "cnt: 0 - valLoss: 0.6541047692298889 - trainLoss: 0.654849648475647\n",
      "cnt: 0 - valLoss: 0.6541033387184143 - trainLoss: 0.6548480987548828\n",
      "cnt: 0 - valLoss: 0.6541019082069397 - trainLoss: 0.6548465490341187\n",
      "cnt: 0 - valLoss: 0.6541004180908203 - trainLoss: 0.6548449397087097\n",
      "cnt: 0 - valLoss: 0.6540990471839905 - trainLoss: 0.6548433899879456\n",
      "cnt: 0 - valLoss: 0.6540975570678711 - trainLoss: 0.6548417806625366\n",
      "cnt: 0 - valLoss: 0.6540961265563965 - trainLoss: 0.6548401713371277\n",
      "cnt: 0 - valLoss: 0.6540947556495667 - trainLoss: 0.6548386812210083\n",
      "cnt: 0 - valLoss: 0.6540932655334473 - trainLoss: 0.6548371315002441\n",
      "cnt: 0 - valLoss: 0.6540917754173279 - trainLoss: 0.6548355221748352\n",
      "cnt: 0 - valLoss: 0.6540903449058533 - trainLoss: 0.654833972454071\n",
      "cnt: 0 - valLoss: 0.6540889143943787 - trainLoss: 0.6548323631286621\n",
      "cnt: 0 - valLoss: 0.654087483882904 - trainLoss: 0.654830813407898\n",
      "cnt: 0 - valLoss: 0.6540859937667847 - trainLoss: 0.654829204082489\n",
      "cnt: 0 - valLoss: 0.6540845632553101 - trainLoss: 0.6548275947570801\n",
      "cnt: 0 - valLoss: 0.6540831923484802 - trainLoss: 0.6548260450363159\n",
      "cnt: 0 - valLoss: 0.6540816426277161 - trainLoss: 0.6548243761062622\n",
      "cnt: 0 - valLoss: 0.6540802121162415 - trainLoss: 0.6548228859901428\n",
      "cnt: 0 - valLoss: 0.6540788412094116 - trainLoss: 0.6548212170600891\n",
      "cnt: 0 - valLoss: 0.654077410697937 - trainLoss: 0.6548196077346802\n",
      "cnt: 0 - valLoss: 0.6540759801864624 - trainLoss: 0.654818058013916\n",
      "cnt: 0 - valLoss: 0.6540745496749878 - trainLoss: 0.6548165082931519\n",
      "cnt: 0 - valLoss: 0.6540730595588684 - trainLoss: 0.6548148393630981\n",
      "cnt: 0 - valLoss: 0.6540716290473938 - trainLoss: 0.654813289642334\n",
      "cnt: 0 - valLoss: 0.6540701985359192 - trainLoss: 0.654811680316925\n",
      "cnt: 0 - valLoss: 0.6540687084197998 - trainLoss: 0.6548100709915161\n",
      "cnt: 0 - valLoss: 0.6540672779083252 - trainLoss: 0.6548084616661072\n",
      "cnt: 0 - valLoss: 0.6540657877922058 - trainLoss: 0.6548068523406982\n",
      "cnt: 0 - valLoss: 0.654064416885376 - trainLoss: 0.6548053026199341\n",
      "cnt: 0 - valLoss: 0.6540631055831909 - trainLoss: 0.6548036932945251\n",
      "cnt: 0 - valLoss: 0.6540616750717163 - trainLoss: 0.6548020839691162\n",
      "cnt: 0 - valLoss: 0.6540603041648865 - trainLoss: 0.6548004746437073\n",
      "cnt: 0 - valLoss: 0.6540589928627014 - trainLoss: 0.6547989249229431\n",
      "cnt: 0 - valLoss: 0.6540576815605164 - trainLoss: 0.654797375202179\n",
      "cnt: 0 - valLoss: 0.6540562510490417 - trainLoss: 0.65479576587677\n",
      "cnt: 0 - valLoss: 0.6540548205375671 - trainLoss: 0.6547941565513611\n",
      "cnt: 0 - valLoss: 0.6540534496307373 - trainLoss: 0.6547926068305969\n",
      "cnt: 0 - valLoss: 0.6540520787239075 - trainLoss: 0.654790997505188\n",
      "cnt: 0 - valLoss: 0.6540506482124329 - trainLoss: 0.654789388179779\n",
      "cnt: 0 - valLoss: 0.654049277305603 - trainLoss: 0.6547878384590149\n",
      "cnt: 0 - valLoss: 0.6540479063987732 - trainLoss: 0.654786229133606\n",
      "cnt: 0 - valLoss: 0.6540465354919434 - trainLoss: 0.654784619808197\n",
      "cnt: 0 - valLoss: 0.6540451645851135 - trainLoss: 0.6547830700874329\n",
      "cnt: 0 - valLoss: 0.6540437340736389 - trainLoss: 0.6547814607620239\n",
      "cnt: 0 - valLoss: 0.6540423631668091 - trainLoss: 0.654779851436615\n",
      "cnt: 0 - valLoss: 0.6540411114692688 - trainLoss: 0.654778242111206\n",
      "cnt: 0 - valLoss: 0.6540396213531494 - trainLoss: 0.6547766923904419\n",
      "cnt: 0 - valLoss: 0.6540383100509644 - trainLoss: 0.654775083065033\n",
      "cnt: 0 - valLoss: 0.6540369391441345 - trainLoss: 0.654773473739624\n",
      "cnt: 0 - valLoss: 0.6540356278419495 - trainLoss: 0.6547718644142151\n",
      "cnt: 0 - valLoss: 0.6540340781211853 - trainLoss: 0.6547703146934509\n",
      "cnt: 0 - valLoss: 0.6540327668190002 - trainLoss: 0.654768705368042\n",
      "cnt: 0 - valLoss: 0.6540313959121704 - trainLoss: 0.6547670364379883\n",
      "cnt: 0 - valLoss: 0.654029905796051 - trainLoss: 0.6547654867172241\n",
      "cnt: 0 - valLoss: 0.6540286540985107 - trainLoss: 0.6547638773918152\n",
      "cnt: 0 - valLoss: 0.6540272831916809 - trainLoss: 0.6547622084617615\n",
      "cnt: 0 - valLoss: 0.6540257930755615 - trainLoss: 0.6547606587409973\n",
      "cnt: 0 - valLoss: 0.6540244221687317 - trainLoss: 0.6547589898109436\n",
      "cnt: 0 - valLoss: 0.6540231704711914 - trainLoss: 0.6547574400901794\n",
      "cnt: 0 - valLoss: 0.654021680355072 - trainLoss: 0.6547558307647705\n",
      "cnt: 0 - valLoss: 0.654020369052887 - trainLoss: 0.6547542214393616\n",
      "cnt: 0 - valLoss: 0.6540191173553467 - trainLoss: 0.6547526121139526\n",
      "cnt: 0 - valLoss: 0.6540178060531616 - trainLoss: 0.6547510027885437\n",
      "cnt: 0 - valLoss: 0.6540164351463318 - trainLoss: 0.6547493934631348\n",
      "cnt: 0 - valLoss: 0.654015064239502 - trainLoss: 0.6547477841377258\n",
      "cnt: 0 - valLoss: 0.6540137529373169 - trainLoss: 0.6547462344169617\n",
      "cnt: 0 - valLoss: 0.6540123820304871 - trainLoss: 0.654744565486908\n",
      "cnt: 0 - valLoss: 0.6540110111236572 - trainLoss: 0.654742956161499\n",
      "cnt: 0 - valLoss: 0.6540097594261169 - trainLoss: 0.6547413468360901\n",
      "cnt: 0 - valLoss: 0.6540082693099976 - trainLoss: 0.6547397375106812\n",
      "cnt: 0 - valLoss: 0.6540070176124573 - trainLoss: 0.6547380685806274\n",
      "cnt: 0 - valLoss: 0.6540058255195618 - trainLoss: 0.6547365188598633\n",
      "cnt: 0 - valLoss: 0.6540043354034424 - trainLoss: 0.6547349095344543\n",
      "cnt: 0 - valLoss: 0.6540030837059021 - trainLoss: 0.6547333002090454\n",
      "cnt: 0 - valLoss: 0.6540017127990723 - trainLoss: 0.6547316312789917\n",
      "cnt: 0 - valLoss: 0.654000461101532 - trainLoss: 0.6547300219535828\n",
      "cnt: 0 - valLoss: 0.6539990305900574 - trainLoss: 0.6547284722328186\n",
      "cnt: 0 - valLoss: 0.6539977192878723 - trainLoss: 0.6547268629074097\n",
      "cnt: 0 - valLoss: 0.6539964079856873 - trainLoss: 0.654725193977356\n",
      "cnt: 0 - valLoss: 0.6539949774742126 - trainLoss: 0.654723584651947\n",
      "cnt: 0 - valLoss: 0.6539937257766724 - trainLoss: 0.6547219753265381\n",
      "cnt: 0 - valLoss: 0.6539924144744873 - trainLoss: 0.6547203660011292\n",
      "cnt: 0 - valLoss: 0.6539909243583679 - trainLoss: 0.6547187566757202\n",
      "cnt: 0 - valLoss: 0.6539896130561829 - trainLoss: 0.6547170877456665\n",
      "cnt: 0 - valLoss: 0.6539884209632874 - trainLoss: 0.6547155380249023\n",
      "cnt: 0 - valLoss: 0.6539870500564575 - trainLoss: 0.6547138690948486\n",
      "cnt: 0 - valLoss: 0.6539856791496277 - trainLoss: 0.6547123193740845\n",
      "cnt: 0 - valLoss: 0.6539843678474426 - trainLoss: 0.6547107100486755\n",
      "cnt: 0 - valLoss: 0.6539830565452576 - trainLoss: 0.6547090411186218\n",
      "cnt: 0 - valLoss: 0.6539817452430725 - trainLoss: 0.6547074913978577\n",
      "cnt: 0 - valLoss: 0.6539801955223083 - trainLoss: 0.654705822467804\n",
      "cnt: 0 - valLoss: 0.6539790034294128 - trainLoss: 0.6547042727470398\n",
      "cnt: 0 - valLoss: 0.6539775729179382 - trainLoss: 0.6547026634216309\n",
      "cnt: 0 - valLoss: 0.6539763808250427 - trainLoss: 0.6547009944915771\n",
      "cnt: 0 - valLoss: 0.6539748907089233 - trainLoss: 0.654699444770813\n",
      "cnt: 0 - valLoss: 0.6539735794067383 - trainLoss: 0.654697835445404\n",
      "cnt: 0 - valLoss: 0.6539722084999084 - trainLoss: 0.6546961665153503\n",
      "cnt: 0 - valLoss: 0.6539710164070129 - trainLoss: 0.6546945571899414\n",
      "cnt: 0 - valLoss: 0.6539695858955383 - trainLoss: 0.6546929478645325\n",
      "cnt: 0 - valLoss: 0.6539682745933533 - trainLoss: 0.6546913385391235\n",
      "cnt: 0 - valLoss: 0.6539669632911682 - trainLoss: 0.6546897292137146\n",
      "cnt: 0 - valLoss: 0.6539655923843384 - trainLoss: 0.6546881794929504\n",
      "cnt: 0 - valLoss: 0.6539641618728638 - trainLoss: 0.6546865105628967\n",
      "cnt: 0 - valLoss: 0.6539628505706787 - trainLoss: 0.6546849012374878\n",
      "cnt: 0 - valLoss: 0.6539615392684937 - trainLoss: 0.6546832919120789\n",
      "cnt: 0 - valLoss: 0.6539601683616638 - trainLoss: 0.6546816825866699\n",
      "cnt: 0 - valLoss: 0.653958797454834 - trainLoss: 0.6546800136566162\n",
      "cnt: 0 - valLoss: 0.6539574861526489 - trainLoss: 0.6546784043312073\n",
      "cnt: 0 - valLoss: 0.6539561748504639 - trainLoss: 0.6546767950057983\n",
      "cnt: 0 - valLoss: 0.6539548635482788 - trainLoss: 0.6546751856803894\n",
      "cnt: 0 - valLoss: 0.6539535522460938 - trainLoss: 0.6546735763549805\n",
      "cnt: 0 - valLoss: 0.6539521217346191 - trainLoss: 0.6546719670295715\n",
      "cnt: 0 - valLoss: 0.6539508104324341 - trainLoss: 0.6546703577041626\n",
      "cnt: 0 - valLoss: 0.6539494395256042 - trainLoss: 0.6546688079833984\n",
      "cnt: 0 - valLoss: 0.6539480686187744 - trainLoss: 0.6546671986579895\n",
      "cnt: 0 - valLoss: 0.6539466381072998 - trainLoss: 0.6546655893325806\n",
      "cnt: 0 - valLoss: 0.6539453268051147 - trainLoss: 0.6546640396118164\n",
      "cnt: 0 - valLoss: 0.6539438366889954 - trainLoss: 0.6546624898910522\n",
      "cnt: 0 - valLoss: 0.6539425253868103 - trainLoss: 0.6546608209609985\n",
      "cnt: 0 - valLoss: 0.6539410948753357 - trainLoss: 0.6546592116355896\n",
      "cnt: 0 - valLoss: 0.6539397239685059 - trainLoss: 0.6546576023101807\n",
      "cnt: 0 - valLoss: 0.653938353061676 - trainLoss: 0.6546559929847717\n",
      "cnt: 0 - valLoss: 0.653937041759491 - trainLoss: 0.6546544432640076\n",
      "cnt: 0 - valLoss: 0.6539356708526611 - trainLoss: 0.6546528339385986\n",
      "cnt: 0 - valLoss: 0.6539343595504761 - trainLoss: 0.6546512842178345\n",
      "cnt: 0 - valLoss: 0.6539329886436462 - trainLoss: 0.6546496748924255\n",
      "cnt: 0 - valLoss: 0.6539316177368164 - trainLoss: 0.6546481251716614\n",
      "cnt: 0 - valLoss: 0.6539301872253418 - trainLoss: 0.6546464562416077\n",
      "cnt: 0 - valLoss: 0.6539288759231567 - trainLoss: 0.6546448469161987\n",
      "cnt: 0 - valLoss: 0.6539275050163269 - trainLoss: 0.6546432971954346\n",
      "cnt: 0 - valLoss: 0.6539260149002075 - trainLoss: 0.6546416878700256\n",
      "cnt: 0 - valLoss: 0.6539247035980225 - trainLoss: 0.6546400785446167\n",
      "cnt: 0 - valLoss: 0.6539232730865479 - trainLoss: 0.6546384692192078\n",
      "cnt: 0 - valLoss: 0.653921902179718 - trainLoss: 0.6546369194984436\n",
      "cnt: 0 - valLoss: 0.6539205312728882 - trainLoss: 0.6546352505683899\n",
      "cnt: 0 - valLoss: 0.6539191603660583 - trainLoss: 0.654633641242981\n",
      "cnt: 0 - valLoss: 0.6539178490638733 - trainLoss: 0.6546320915222168\n",
      "cnt: 0 - valLoss: 0.6539165377616882 - trainLoss: 0.6546305418014526\n",
      "cnt: 0 - valLoss: 0.6539151668548584 - trainLoss: 0.6546289324760437\n",
      "cnt: 0 - valLoss: 0.6539137959480286 - trainLoss: 0.6546273231506348\n",
      "cnt: 0 - valLoss: 0.6539123058319092 - trainLoss: 0.654625654220581\n",
      "cnt: 0 - valLoss: 0.6539109945297241 - trainLoss: 0.6546241044998169\n",
      "cnt: 0 - valLoss: 0.6539095640182495 - trainLoss: 0.654622495174408\n",
      "cnt: 0 - valLoss: 0.6539082527160645 - trainLoss: 0.654620885848999\n",
      "cnt: 0 - valLoss: 0.6539068222045898 - trainLoss: 0.6546192765235901\n",
      "cnt: 0 - valLoss: 0.65390545129776 - trainLoss: 0.6546176075935364\n",
      "cnt: 0 - valLoss: 0.6539040207862854 - trainLoss: 0.6546160578727722\n",
      "cnt: 0 - valLoss: 0.6539027094841003 - trainLoss: 0.6546144485473633\n",
      "cnt: 0 - valLoss: 0.6539012789726257 - trainLoss: 0.6546128988265991\n",
      "cnt: 0 - valLoss: 0.6539000272750854 - trainLoss: 0.6546112298965454\n",
      "cnt: 0 - valLoss: 0.6538985967636108 - trainLoss: 0.6546096801757812\n",
      "cnt: 0 - valLoss: 0.653897225856781 - trainLoss: 0.6546080708503723\n",
      "cnt: 0 - valLoss: 0.6538958549499512 - trainLoss: 0.6546064019203186\n",
      "cnt: 0 - valLoss: 0.6538944840431213 - trainLoss: 0.6546048521995544\n",
      "cnt: 0 - valLoss: 0.6538930535316467 - trainLoss: 0.6546032428741455\n",
      "cnt: 0 - valLoss: 0.6538917422294617 - trainLoss: 0.6546016335487366\n",
      "cnt: 0 - valLoss: 0.6538903117179871 - trainLoss: 0.6546000242233276\n",
      "cnt: 0 - valLoss: 0.653889000415802 - trainLoss: 0.6545984148979187\n",
      "cnt: 0 - valLoss: 0.6538875699043274 - trainLoss: 0.6545968055725098\n",
      "cnt: 0 - valLoss: 0.6538861989974976 - trainLoss: 0.654595136642456\n",
      "cnt: 0 - valLoss: 0.653884768486023 - trainLoss: 0.6545935869216919\n",
      "cnt: 0 - valLoss: 0.6538833975791931 - trainLoss: 0.654591977596283\n",
      "cnt: 0 - valLoss: 0.6538820862770081 - trainLoss: 0.6545903086662292\n",
      "cnt: 0 - valLoss: 0.6538807153701782 - trainLoss: 0.6545887589454651\n",
      "cnt: 0 - valLoss: 0.6538793444633484 - trainLoss: 0.6545870900154114\n",
      "cnt: 0 - valLoss: 0.6538779735565186 - trainLoss: 0.6545854806900024\n",
      "cnt: 0 - valLoss: 0.653876543045044 - trainLoss: 0.6545839309692383\n",
      "cnt: 0 - valLoss: 0.6538751721382141 - trainLoss: 0.6545822620391846\n",
      "cnt: 0 - valLoss: 0.6538738012313843 - trainLoss: 0.6545806527137756\n",
      "cnt: 0 - valLoss: 0.6538724303245544 - trainLoss: 0.6545790433883667\n",
      "cnt: 0 - valLoss: 0.6538709998130798 - trainLoss: 0.654577374458313\n",
      "cnt: 0 - valLoss: 0.6538696885108948 - trainLoss: 0.6545758247375488\n",
      "cnt: 0 - valLoss: 0.6538682579994202 - trainLoss: 0.6545742154121399\n",
      "cnt: 0 - valLoss: 0.6538668870925903 - trainLoss: 0.654572606086731\n",
      "cnt: 0 - valLoss: 0.6538654565811157 - trainLoss: 0.654570996761322\n",
      "cnt: 0 - valLoss: 0.6538641452789307 - trainLoss: 0.6545693278312683\n",
      "cnt: 0 - valLoss: 0.6538627743721008 - trainLoss: 0.6545677781105042\n",
      "cnt: 0 - valLoss: 0.653861403465271 - trainLoss: 0.6545661687850952\n",
      "cnt: 0 - valLoss: 0.6538600325584412 - trainLoss: 0.6545645594596863\n",
      "cnt: 0 - valLoss: 0.6538585424423218 - trainLoss: 0.6545629501342773\n",
      "cnt: 0 - valLoss: 0.6538571715354919 - trainLoss: 0.6545612812042236\n",
      "cnt: 0 - valLoss: 0.6538558006286621 - trainLoss: 0.6545596718788147\n",
      "cnt: 0 - valLoss: 0.6538544297218323 - trainLoss: 0.6545580625534058\n",
      "cnt: 0 - valLoss: 0.6538530588150024 - trainLoss: 0.654556393623352\n",
      "cnt: 0 - valLoss: 0.6538516283035278 - trainLoss: 0.6545547842979431\n",
      "cnt: 0 - valLoss: 0.653850257396698 - trainLoss: 0.6545531749725342\n",
      "cnt: 0 - valLoss: 0.6538488268852234 - trainLoss: 0.6545515656471252\n",
      "cnt: 0 - valLoss: 0.6538474559783936 - trainLoss: 0.6545498967170715\n",
      "cnt: 0 - valLoss: 0.6538462042808533 - trainLoss: 0.6545482873916626\n",
      "cnt: 0 - valLoss: 0.6538448333740234 - trainLoss: 0.6545467376708984\n",
      "cnt: 0 - valLoss: 0.6538434624671936 - trainLoss: 0.6545451283454895\n",
      "cnt: 0 - valLoss: 0.653842031955719 - trainLoss: 0.654543399810791\n",
      "cnt: 0 - valLoss: 0.6538407206535339 - trainLoss: 0.6545418500900269\n",
      "cnt: 0 - valLoss: 0.6538392901420593 - trainLoss: 0.6545401811599731\n",
      "cnt: 0 - valLoss: 0.6538379192352295 - trainLoss: 0.654538631439209\n",
      "cnt: 0 - valLoss: 0.6538365483283997 - trainLoss: 0.6545369625091553\n",
      "cnt: 0 - valLoss: 0.6538352370262146 - trainLoss: 0.6545353531837463\n",
      "cnt: 0 - valLoss: 0.65383380651474 - trainLoss: 0.6545337438583374\n",
      "cnt: 0 - valLoss: 0.6538324356079102 - trainLoss: 0.6545321345329285\n",
      "cnt: 0 - valLoss: 0.6538309454917908 - trainLoss: 0.6545304656028748\n",
      "cnt: 0 - valLoss: 0.6538295745849609 - trainLoss: 0.6545289158821106\n",
      "cnt: 0 - valLoss: 0.6538282632827759 - trainLoss: 0.6545273065567017\n",
      "cnt: 0 - valLoss: 0.6538268327713013 - trainLoss: 0.6545257568359375\n",
      "cnt: 0 - valLoss: 0.6538254022598267 - trainLoss: 0.6545241475105286\n",
      "cnt: 0 - valLoss: 0.6538239121437073 - trainLoss: 0.6545225977897644\n",
      "cnt: 0 - valLoss: 0.6538224816322327 - trainLoss: 0.6545209288597107\n",
      "cnt: 0 - valLoss: 0.6538211107254028 - trainLoss: 0.6545193791389465\n",
      "cnt: 0 - valLoss: 0.6538196802139282 - trainLoss: 0.6545178294181824\n",
      "cnt: 0 - valLoss: 0.6538181900978088 - trainLoss: 0.6545162200927734\n",
      "cnt: 0 - valLoss: 0.653816819190979 - trainLoss: 0.6545146703720093\n",
      "cnt: 0 - valLoss: 0.6538153290748596 - trainLoss: 0.6545130014419556\n",
      "cnt: 0 - valLoss: 0.6538139581680298 - trainLoss: 0.6545113921165466\n",
      "cnt: 0 - valLoss: 0.6538124680519104 - trainLoss: 0.6545098423957825\n",
      "cnt: 0 - valLoss: 0.6538111567497253 - trainLoss: 0.6545082330703735\n",
      "cnt: 0 - valLoss: 0.6538097858428955 - trainLoss: 0.6545066237449646\n",
      "cnt: 0 - valLoss: 0.6538084149360657 - trainLoss: 0.6545050740242004\n",
      "cnt: 0 - valLoss: 0.6538069844245911 - trainLoss: 0.6545034646987915\n",
      "cnt: 0 - valLoss: 0.6538056135177612 - trainLoss: 0.6545019149780273\n",
      "cnt: 0 - valLoss: 0.6538041830062866 - trainLoss: 0.6545002460479736\n",
      "cnt: 0 - valLoss: 0.653802752494812 - trainLoss: 0.6544986963272095\n",
      "cnt: 0 - valLoss: 0.6538013815879822 - trainLoss: 0.6544970870018005\n",
      "cnt: 0 - valLoss: 0.6537999510765076 - trainLoss: 0.6544954776763916\n",
      "cnt: 0 - valLoss: 0.6537985801696777 - trainLoss: 0.6544938683509827\n",
      "cnt: 0 - valLoss: 0.6537970900535583 - trainLoss: 0.6544922590255737\n",
      "cnt: 0 - valLoss: 0.6537957191467285 - trainLoss: 0.6544907093048096\n",
      "cnt: 0 - valLoss: 0.6537942886352539 - trainLoss: 0.6544890999794006\n",
      "cnt: 0 - valLoss: 0.6537929177284241 - trainLoss: 0.6544874906539917\n",
      "cnt: 0 - valLoss: 0.653791606426239 - trainLoss: 0.6544858813285828\n",
      "cnt: 0 - valLoss: 0.6537901759147644 - trainLoss: 0.6544843316078186\n",
      "cnt: 0 - valLoss: 0.6537888050079346 - trainLoss: 0.6544827222824097\n",
      "cnt: 0 - valLoss: 0.6537873148918152 - trainLoss: 0.6544811129570007\n",
      "cnt: 0 - valLoss: 0.6537859439849854 - trainLoss: 0.654479444026947\n",
      "cnt: 0 - valLoss: 0.6537845730781555 - trainLoss: 0.6544779539108276\n",
      "cnt: 0 - valLoss: 0.6537831425666809 - trainLoss: 0.6544762849807739\n",
      "cnt: 0 - valLoss: 0.6537818312644958 - trainLoss: 0.6544747352600098\n",
      "cnt: 0 - valLoss: 0.6537802815437317 - trainLoss: 0.654473066329956\n",
      "cnt: 0 - valLoss: 0.6537789106369019 - trainLoss: 0.6544715166091919\n",
      "cnt: 0 - valLoss: 0.653777539730072 - trainLoss: 0.6544699668884277\n",
      "cnt: 0 - valLoss: 0.6537761092185974 - trainLoss: 0.6544683575630188\n",
      "cnt: 0 - valLoss: 0.6537746787071228 - trainLoss: 0.6544666886329651\n",
      "cnt: 0 - valLoss: 0.6537734270095825 - trainLoss: 0.6544650793075562\n",
      "cnt: 0 - valLoss: 0.6537719368934631 - trainLoss: 0.654463529586792\n",
      "cnt: 0 - valLoss: 0.6537705063819885 - trainLoss: 0.6544619202613831\n",
      "cnt: 0 - valLoss: 0.6537691354751587 - trainLoss: 0.6544603109359741\n",
      "cnt: 0 - valLoss: 0.6537677049636841 - trainLoss: 0.65445876121521\n",
      "cnt: 0 - valLoss: 0.6537663340568542 - trainLoss: 0.6544570922851562\n",
      "cnt: 0 - valLoss: 0.6537649035453796 - trainLoss: 0.6544554829597473\n",
      "cnt: 0 - valLoss: 0.6537635326385498 - trainLoss: 0.6544538736343384\n",
      "cnt: 0 - valLoss: 0.6537621021270752 - trainLoss: 0.6544523239135742\n",
      "cnt: 0 - valLoss: 0.6537606716156006 - trainLoss: 0.6544507145881653\n",
      "cnt: 0 - valLoss: 0.653759241104126 - trainLoss: 0.6544491052627563\n",
      "cnt: 0 - valLoss: 0.6537578701972961 - trainLoss: 0.6544474959373474\n",
      "cnt: 0 - valLoss: 0.6537564992904663 - trainLoss: 0.6544458866119385\n",
      "cnt: 0 - valLoss: 0.6537551879882812 - trainLoss: 0.6544442772865295\n",
      "cnt: 0 - valLoss: 0.6537537574768066 - trainLoss: 0.6544426679611206\n",
      "cnt: 0 - valLoss: 0.6537523865699768 - trainLoss: 0.6544410586357117\n",
      "cnt: 0 - valLoss: 0.6537509560585022 - trainLoss: 0.6544394493103027\n",
      "cnt: 0 - valLoss: 0.6537495255470276 - trainLoss: 0.6544378399848938\n",
      "cnt: 0 - valLoss: 0.6537481546401978 - trainLoss: 0.6544362306594849\n",
      "cnt: 0 - valLoss: 0.6537467241287231 - trainLoss: 0.6544346213340759\n",
      "cnt: 0 - valLoss: 0.6537453532218933 - trainLoss: 0.654433012008667\n",
      "cnt: 0 - valLoss: 0.6537438631057739 - trainLoss: 0.6544314026832581\n",
      "cnt: 0 - valLoss: 0.6537424921989441 - trainLoss: 0.6544297337532043\n",
      "cnt: 0 - valLoss: 0.6537411212921143 - trainLoss: 0.6544281244277954\n",
      "cnt: 0 - valLoss: 0.6537396907806396 - trainLoss: 0.6544265747070312\n",
      "cnt: 0 - valLoss: 0.6537383198738098 - trainLoss: 0.6544249653816223\n",
      "cnt: 0 - valLoss: 0.65373694896698 - trainLoss: 0.6544232964515686\n",
      "cnt: 0 - valLoss: 0.6537355780601501 - trainLoss: 0.6544217467308044\n",
      "cnt: 0 - valLoss: 0.6537340879440308 - trainLoss: 0.6544201374053955\n",
      "cnt: 0 - valLoss: 0.6537328362464905 - trainLoss: 0.6544185280799866\n",
      "cnt: 0 - valLoss: 0.6537313461303711 - trainLoss: 0.6544169187545776\n",
      "cnt: 0 - valLoss: 0.6537299752235413 - trainLoss: 0.6544152498245239\n",
      "cnt: 0 - valLoss: 0.6537285447120667 - trainLoss: 0.654413640499115\n",
      "cnt: 0 - valLoss: 0.6537271738052368 - trainLoss: 0.654412031173706\n",
      "cnt: 0 - valLoss: 0.6537256836891174 - trainLoss: 0.6544103622436523\n",
      "cnt: 0 - valLoss: 0.6537243127822876 - trainLoss: 0.6544087529182434\n",
      "cnt: 0 - valLoss: 0.6537229418754578 - trainLoss: 0.6544071435928345\n",
      "cnt: 0 - valLoss: 0.6537215113639832 - trainLoss: 0.6544055342674255\n",
      "cnt: 0 - valLoss: 0.6537202596664429 - trainLoss: 0.6544038653373718\n",
      "cnt: 0 - valLoss: 0.6537188291549683 - trainLoss: 0.6544023156166077\n",
      "cnt: 0 - valLoss: 0.6537174582481384 - trainLoss: 0.654400646686554\n",
      "cnt: 0 - valLoss: 0.6537160277366638 - trainLoss: 0.654399037361145\n",
      "cnt: 0 - valLoss: 0.653714656829834 - trainLoss: 0.6543974876403809\n",
      "cnt: 0 - valLoss: 0.6537132263183594 - trainLoss: 0.6543958187103271\n",
      "cnt: 0 - valLoss: 0.6537117958068848 - trainLoss: 0.6543942093849182\n",
      "cnt: 0 - valLoss: 0.6537104249000549 - trainLoss: 0.6543926000595093\n",
      "cnt: 0 - valLoss: 0.6537090539932251 - trainLoss: 0.6543909311294556\n",
      "cnt: 0 - valLoss: 0.6537076830863953 - trainLoss: 0.6543892621994019\n",
      "cnt: 0 - valLoss: 0.6537061929702759 - trainLoss: 0.6543877124786377\n",
      "cnt: 0 - valLoss: 0.6537047624588013 - trainLoss: 0.6543861031532288\n",
      "cnt: 0 - valLoss: 0.6537033915519714 - trainLoss: 0.654384434223175\n",
      "cnt: 0 - valLoss: 0.6537020206451416 - trainLoss: 0.6543828248977661\n",
      "cnt: 0 - valLoss: 0.6537006497383118 - trainLoss: 0.6543812155723572\n",
      "cnt: 0 - valLoss: 0.6536992192268372 - trainLoss: 0.6543796062469482\n",
      "cnt: 0 - valLoss: 0.6536978483200073 - trainLoss: 0.6543779969215393\n",
      "cnt: 0 - valLoss: 0.6536964178085327 - trainLoss: 0.6543763279914856\n",
      "cnt: 0 - valLoss: 0.6536949872970581 - trainLoss: 0.6543747782707214\n",
      "cnt: 0 - valLoss: 0.6536936163902283 - trainLoss: 0.654373049736023\n",
      "cnt: 0 - valLoss: 0.6536922454833984 - trainLoss: 0.6543715000152588\n",
      "cnt: 0 - valLoss: 0.6536908149719238 - trainLoss: 0.6543698310852051\n",
      "cnt: 0 - valLoss: 0.6536893844604492 - trainLoss: 0.6543682217597961\n",
      "cnt: 0 - valLoss: 0.6536880135536194 - trainLoss: 0.6543665528297424\n",
      "cnt: 0 - valLoss: 0.6536865830421448 - trainLoss: 0.6543649435043335\n",
      "cnt: 0 - valLoss: 0.6536852121353149 - trainLoss: 0.6543633937835693\n",
      "cnt: 0 - valLoss: 0.6536838412284851 - trainLoss: 0.6543617248535156\n",
      "cnt: 0 - valLoss: 0.6536824107170105 - trainLoss: 0.6543600559234619\n",
      "cnt: 0 - valLoss: 0.6536810398101807 - trainLoss: 0.654358446598053\n",
      "cnt: 0 - valLoss: 0.653679609298706 - trainLoss: 0.654356837272644\n",
      "cnt: 0 - valLoss: 0.6536781787872314 - trainLoss: 0.6543551683425903\n",
      "cnt: 0 - valLoss: 0.6536767482757568 - trainLoss: 0.6543535590171814\n",
      "cnt: 0 - valLoss: 0.653675377368927 - trainLoss: 0.6543518900871277\n",
      "cnt: 0 - valLoss: 0.6536739468574524 - trainLoss: 0.654350221157074\n",
      "cnt: 0 - valLoss: 0.653672456741333 - trainLoss: 0.6543486714363098\n",
      "cnt: 0 - valLoss: 0.653671145439148 - trainLoss: 0.6543470025062561\n",
      "cnt: 0 - valLoss: 0.6536697149276733 - trainLoss: 0.6543453931808472\n",
      "cnt: 0 - valLoss: 0.6536682844161987 - trainLoss: 0.6543437838554382\n",
      "cnt: 0 - valLoss: 0.6536669135093689 - trainLoss: 0.6543420553207397\n",
      "cnt: 0 - valLoss: 0.6536655426025391 - trainLoss: 0.6543405055999756\n",
      "cnt: 0 - valLoss: 0.6536641716957092 - trainLoss: 0.6543388366699219\n",
      "cnt: 0 - valLoss: 0.6536626815795898 - trainLoss: 0.6543371677398682\n",
      "cnt: 0 - valLoss: 0.65366131067276 - trainLoss: 0.654335618019104\n",
      "cnt: 0 - valLoss: 0.6536598801612854 - trainLoss: 0.6543339490890503\n",
      "cnt: 0 - valLoss: 0.6536585092544556 - trainLoss: 0.6543323397636414\n",
      "cnt: 0 - valLoss: 0.6536570191383362 - trainLoss: 0.6543306708335876\n",
      "cnt: 0 - valLoss: 0.6536556482315063 - trainLoss: 0.6543290615081787\n",
      "cnt: 0 - valLoss: 0.6536542773246765 - trainLoss: 0.654327392578125\n",
      "cnt: 0 - valLoss: 0.6536527276039124 - trainLoss: 0.6543257832527161\n",
      "cnt: 0 - valLoss: 0.6536513566970825 - trainLoss: 0.6543241143226624\n",
      "cnt: 0 - valLoss: 0.6536499857902527 - trainLoss: 0.6543224453926086\n",
      "cnt: 0 - valLoss: 0.6536486148834229 - trainLoss: 0.6543208360671997\n",
      "cnt: 0 - valLoss: 0.6536473035812378 - trainLoss: 0.6543192863464355\n",
      "cnt: 0 - valLoss: 0.6536457538604736 - trainLoss: 0.6543176174163818\n",
      "cnt: 0 - valLoss: 0.653644323348999 - trainLoss: 0.6543159484863281\n",
      "cnt: 0 - valLoss: 0.6536428928375244 - trainLoss: 0.6543143391609192\n",
      "cnt: 0 - valLoss: 0.6536414623260498 - trainLoss: 0.6543127298355103\n",
      "cnt: 0 - valLoss: 0.6536400318145752 - trainLoss: 0.6543110609054565\n",
      "cnt: 0 - valLoss: 0.6536385416984558 - trainLoss: 0.6543094515800476\n",
      "cnt: 0 - valLoss: 0.6536371111869812 - trainLoss: 0.6543078422546387\n",
      "cnt: 0 - valLoss: 0.6536356806755066 - trainLoss: 0.654306173324585\n",
      "cnt: 0 - valLoss: 0.653634250164032 - trainLoss: 0.654304563999176\n",
      "cnt: 0 - valLoss: 0.6536327600479126 - trainLoss: 0.6543029546737671\n",
      "cnt: 0 - valLoss: 0.6536313891410828 - trainLoss: 0.6543012857437134\n",
      "cnt: 0 - valLoss: 0.6536299586296082 - trainLoss: 0.6542996764183044\n",
      "cnt: 0 - valLoss: 0.6536285281181335 - trainLoss: 0.6542980670928955\n",
      "cnt: 0 - valLoss: 0.6536270976066589 - trainLoss: 0.654296338558197\n",
      "cnt: 0 - valLoss: 0.6536255478858948 - trainLoss: 0.6542947292327881\n",
      "cnt: 0 - valLoss: 0.6536241769790649 - trainLoss: 0.6542931199073792\n",
      "cnt: 0 - valLoss: 0.6536226868629456 - trainLoss: 0.6542914509773254\n",
      "cnt: 0 - valLoss: 0.6536213159561157 - trainLoss: 0.6542898416519165\n",
      "cnt: 0 - valLoss: 0.6536197662353516 - trainLoss: 0.6542881727218628\n",
      "cnt: 0 - valLoss: 0.6536182761192322 - trainLoss: 0.6542865633964539\n",
      "cnt: 0 - valLoss: 0.6536168456077576 - trainLoss: 0.6542849540710449\n",
      "cnt: 0 - valLoss: 0.653615415096283 - trainLoss: 0.6542832851409912\n",
      "cnt: 0 - valLoss: 0.6536139845848083 - trainLoss: 0.6542816162109375\n",
      "cnt: 0 - valLoss: 0.6536126136779785 - trainLoss: 0.6542800068855286\n",
      "cnt: 0 - valLoss: 0.6536111235618591 - trainLoss: 0.6542783975601196\n",
      "cnt: 0 - valLoss: 0.6536096930503845 - trainLoss: 0.6542767286300659\n",
      "cnt: 0 - valLoss: 0.6536082625389099 - trainLoss: 0.6542750597000122\n",
      "cnt: 0 - valLoss: 0.6536067724227905 - trainLoss: 0.654273509979248\n",
      "cnt: 0 - valLoss: 0.6536053419113159 - trainLoss: 0.6542717814445496\n",
      "cnt: 0 - valLoss: 0.6536038517951965 - trainLoss: 0.6542702317237854\n",
      "cnt: 0 - valLoss: 0.6536024212837219 - trainLoss: 0.6542685627937317\n",
      "cnt: 0 - valLoss: 0.6536009907722473 - trainLoss: 0.654266893863678\n",
      "cnt: 0 - valLoss: 0.6535995006561279 - trainLoss: 0.654265284538269\n",
      "cnt: 0 - valLoss: 0.6535979509353638 - trainLoss: 0.6542635560035706\n",
      "cnt: 0 - valLoss: 0.6535965800285339 - trainLoss: 0.6542618870735168\n",
      "cnt: 0 - valLoss: 0.6535950899124146 - trainLoss: 0.6542603373527527\n",
      "cnt: 0 - valLoss: 0.6535937190055847 - trainLoss: 0.6542587280273438\n",
      "cnt: 0 - valLoss: 0.6535922884941101 - trainLoss: 0.65425705909729\n",
      "cnt: 0 - valLoss: 0.6535907983779907 - trainLoss: 0.6542554497718811\n",
      "cnt: 0 - valLoss: 0.6535893082618713 - trainLoss: 0.6542537808418274\n",
      "cnt: 0 - valLoss: 0.6535878777503967 - trainLoss: 0.6542521119117737\n",
      "cnt: 0 - valLoss: 0.6535863876342773 - trainLoss: 0.6542505025863647\n",
      "cnt: 0 - valLoss: 0.6535849571228027 - trainLoss: 0.654248833656311\n",
      "cnt: 0 - valLoss: 0.6535834670066833 - trainLoss: 0.6542471647262573\n",
      "cnt: 0 - valLoss: 0.6535820364952087 - trainLoss: 0.6542455554008484\n",
      "cnt: 0 - valLoss: 0.6535806059837341 - trainLoss: 0.6542438864707947\n",
      "cnt: 0 - valLoss: 0.6535791158676147 - trainLoss: 0.6542422771453857\n",
      "cnt: 0 - valLoss: 0.6535776257514954 - trainLoss: 0.654240608215332\n",
      "cnt: 0 - valLoss: 0.6535762548446655 - trainLoss: 0.6542389392852783\n",
      "cnt: 0 - valLoss: 0.6535748243331909 - trainLoss: 0.6542373895645142\n",
      "cnt: 0 - valLoss: 0.6535733938217163 - trainLoss: 0.6542356610298157\n",
      "cnt: 0 - valLoss: 0.6535719037055969 - trainLoss: 0.6542340517044067\n",
      "cnt: 0 - valLoss: 0.6535704731941223 - trainLoss: 0.6542323231697083\n",
      "cnt: 0 - valLoss: 0.6535690426826477 - trainLoss: 0.6542307734489441\n",
      "cnt: 0 - valLoss: 0.6535674929618835 - trainLoss: 0.6542291045188904\n",
      "cnt: 0 - valLoss: 0.6535660624504089 - trainLoss: 0.6542274355888367\n",
      "cnt: 0 - valLoss: 0.6535646915435791 - trainLoss: 0.654225766658783\n",
      "cnt: 0 - valLoss: 0.6535632610321045 - trainLoss: 0.6542240977287292\n",
      "cnt: 0 - valLoss: 0.6535617709159851 - trainLoss: 0.6542224884033203\n",
      "cnt: 0 - valLoss: 0.6535604000091553 - trainLoss: 0.6542208194732666\n",
      "cnt: 0 - valLoss: 0.6535590887069702 - trainLoss: 0.6542192101478577\n",
      "cnt: 0 - valLoss: 0.6535577774047852 - trainLoss: 0.654217541217804\n",
      "cnt: 0 - valLoss: 0.6535562872886658 - trainLoss: 0.6542158126831055\n",
      "cnt: 0 - valLoss: 0.6535549163818359 - trainLoss: 0.6542142033576965\n",
      "cnt: 0 - valLoss: 0.6535535454750061 - trainLoss: 0.6542125344276428\n",
      "cnt: 0 - valLoss: 0.6535521149635315 - trainLoss: 0.6542108654975891\n",
      "cnt: 0 - valLoss: 0.6535506844520569 - trainLoss: 0.6542092561721802\n",
      "cnt: 0 - valLoss: 0.6535492539405823 - trainLoss: 0.6542075276374817\n",
      "cnt: 0 - valLoss: 0.6535478830337524 - trainLoss: 0.654205858707428\n",
      "cnt: 0 - valLoss: 0.6535465121269226 - trainLoss: 0.6542041897773743\n",
      "cnt: 0 - valLoss: 0.653545081615448 - trainLoss: 0.6542025208473206\n",
      "cnt: 0 - valLoss: 0.6535437107086182 - trainLoss: 0.6542008519172668\n",
      "cnt: 0 - valLoss: 0.6535422801971436 - trainLoss: 0.6541991829872131\n",
      "cnt: 0 - valLoss: 0.6535409688949585 - trainLoss: 0.6541975140571594\n",
      "cnt: 0 - valLoss: 0.6535395383834839 - trainLoss: 0.6541959047317505\n",
      "cnt: 0 - valLoss: 0.6535381078720093 - trainLoss: 0.6541942358016968\n",
      "cnt: 0 - valLoss: 0.6535367369651794 - trainLoss: 0.6541926264762878\n",
      "cnt: 0 - valLoss: 0.6535353660583496 - trainLoss: 0.6541908979415894\n",
      "cnt: 0 - valLoss: 0.653533935546875 - trainLoss: 0.6541892886161804\n",
      "cnt: 0 - valLoss: 0.6535325646400452 - trainLoss: 0.6541875600814819\n",
      "cnt: 0 - valLoss: 0.6535311341285706 - trainLoss: 0.6541858911514282\n",
      "cnt: 0 - valLoss: 0.6535297632217407 - trainLoss: 0.6541842818260193\n",
      "cnt: 0 - valLoss: 0.6535283923149109 - trainLoss: 0.6541826128959656\n",
      "cnt: 0 - valLoss: 0.6535269021987915 - trainLoss: 0.6541808843612671\n",
      "cnt: 0 - valLoss: 0.6535254716873169 - trainLoss: 0.6541792154312134\n",
      "cnt: 0 - valLoss: 0.6535241007804871 - trainLoss: 0.6541775465011597\n",
      "cnt: 0 - valLoss: 0.653522789478302 - trainLoss: 0.6541759371757507\n",
      "cnt: 0 - valLoss: 0.6535214185714722 - trainLoss: 0.654174268245697\n",
      "cnt: 0 - valLoss: 0.6535199284553528 - trainLoss: 0.6541725993156433\n",
      "cnt: 0 - valLoss: 0.6535184979438782 - trainLoss: 0.6541709303855896\n",
      "cnt: 0 - valLoss: 0.6535170674324036 - trainLoss: 0.6541692614555359\n",
      "cnt: 0 - valLoss: 0.6535157561302185 - trainLoss: 0.6541675925254822\n",
      "cnt: 0 - valLoss: 0.6535143256187439 - trainLoss: 0.6541659235954285\n",
      "cnt: 0 - valLoss: 0.6535128951072693 - trainLoss: 0.6541642546653748\n",
      "cnt: 0 - valLoss: 0.6535115242004395 - trainLoss: 0.654162585735321\n",
      "cnt: 0 - valLoss: 0.6535100340843201 - trainLoss: 0.6541609168052673\n",
      "cnt: 0 - valLoss: 0.6535086631774902 - trainLoss: 0.6541592478752136\n",
      "cnt: 0 - valLoss: 0.6535072922706604 - trainLoss: 0.6541575789451599\n",
      "cnt: 0 - valLoss: 0.6535058617591858 - trainLoss: 0.6541559100151062\n",
      "cnt: 0 - valLoss: 0.653504490852356 - trainLoss: 0.6541543006896973\n",
      "cnt: 0 - valLoss: 0.6535030603408813 - trainLoss: 0.6541525721549988\n",
      "cnt: 0 - valLoss: 0.6535016894340515 - trainLoss: 0.6541509032249451\n",
      "cnt: 0 - valLoss: 0.6535002589225769 - trainLoss: 0.6541492342948914\n",
      "cnt: 0 - valLoss: 0.6534988880157471 - trainLoss: 0.6541475653648376\n",
      "cnt: 0 - valLoss: 0.6534973978996277 - trainLoss: 0.6541458964347839\n",
      "cnt: 0 - valLoss: 0.6534960269927979 - trainLoss: 0.6541442275047302\n",
      "cnt: 0 - valLoss: 0.6534945964813232 - trainLoss: 0.6541425585746765\n",
      "cnt: 0 - valLoss: 0.6534932255744934 - trainLoss: 0.654140830039978\n",
      "cnt: 0 - valLoss: 0.653491735458374 - trainLoss: 0.6541392207145691\n",
      "cnt: 0 - valLoss: 0.6534903645515442 - trainLoss: 0.6541375517845154\n",
      "cnt: 0 - valLoss: 0.6534889936447144 - trainLoss: 0.6541358828544617\n",
      "cnt: 0 - valLoss: 0.6534876227378845 - trainLoss: 0.654134213924408\n",
      "cnt: 0 - valLoss: 0.6534862518310547 - trainLoss: 0.6541325449943542\n",
      "cnt: 0 - valLoss: 0.6534848213195801 - trainLoss: 0.6541308164596558\n",
      "cnt: 0 - valLoss: 0.6534833312034607 - trainLoss: 0.6541292071342468\n",
      "cnt: 0 - valLoss: 0.6534820199012756 - trainLoss: 0.6541274785995483\n",
      "cnt: 0 - valLoss: 0.6534805297851562 - trainLoss: 0.6541257500648499\n",
      "cnt: 0 - valLoss: 0.6534790992736816 - trainLoss: 0.6541240811347961\n",
      "cnt: 0 - valLoss: 0.653477668762207 - trainLoss: 0.6541224122047424\n",
      "cnt: 0 - valLoss: 0.6534762978553772 - trainLoss: 0.6541207432746887\n",
      "cnt: 0 - valLoss: 0.6534749865531921 - trainLoss: 0.6541190147399902\n",
      "cnt: 0 - valLoss: 0.653473436832428 - trainLoss: 0.6541173458099365\n",
      "cnt: 0 - valLoss: 0.6534721255302429 - trainLoss: 0.6541156768798828\n",
      "cnt: 0 - valLoss: 0.6534706950187683 - trainLoss: 0.6541140079498291\n",
      "cnt: 0 - valLoss: 0.6534693241119385 - trainLoss: 0.6541123390197754\n",
      "cnt: 0 - valLoss: 0.6534679532051086 - trainLoss: 0.6541106700897217\n",
      "cnt: 0 - valLoss: 0.6534664630889893 - trainLoss: 0.654109001159668\n",
      "cnt: 0 - valLoss: 0.6534650325775146 - trainLoss: 0.6541073322296143\n",
      "cnt: 0 - valLoss: 0.65346360206604 - trainLoss: 0.6541056632995605\n",
      "cnt: 0 - valLoss: 0.6534621715545654 - trainLoss: 0.6541039347648621\n",
      "cnt: 0 - valLoss: 0.6534608006477356 - trainLoss: 0.6541022658348083\n",
      "cnt: 0 - valLoss: 0.653459370136261 - trainLoss: 0.6541005373001099\n",
      "cnt: 0 - valLoss: 0.6534579396247864 - trainLoss: 0.6540988683700562\n",
      "cnt: 0 - valLoss: 0.6534565091133118 - trainLoss: 0.6540971994400024\n",
      "cnt: 0 - valLoss: 0.6534551382064819 - trainLoss: 0.6540955305099487\n",
      "cnt: 0 - valLoss: 0.6534537076950073 - trainLoss: 0.654093861579895\n",
      "cnt: 0 - valLoss: 0.6534522771835327 - trainLoss: 0.6540921926498413\n",
      "cnt: 0 - valLoss: 0.6534508466720581 - trainLoss: 0.6540904641151428\n",
      "cnt: 0 - valLoss: 0.6534494757652283 - trainLoss: 0.6540888547897339\n",
      "cnt: 0 - valLoss: 0.6534479856491089 - trainLoss: 0.6540871858596802\n",
      "cnt: 0 - valLoss: 0.6534465551376343 - trainLoss: 0.6540854573249817\n",
      "cnt: 0 - valLoss: 0.6534451842308044 - trainLoss: 0.654083788394928\n",
      "cnt: 0 - valLoss: 0.6534436941146851 - trainLoss: 0.6540821194648743\n",
      "cnt: 0 - valLoss: 0.6534423232078552 - trainLoss: 0.6540804505348206\n",
      "cnt: 0 - valLoss: 0.6534408330917358 - trainLoss: 0.6540787816047668\n",
      "cnt: 0 - valLoss: 0.6534394025802612 - trainLoss: 0.6540770530700684\n",
      "cnt: 0 - valLoss: 0.6534380316734314 - trainLoss: 0.6540754437446594\n",
      "cnt: 0 - valLoss: 0.653436541557312 - trainLoss: 0.6540737152099609\n",
      "cnt: 0 - valLoss: 0.653435230255127 - trainLoss: 0.6540719866752625\n",
      "cnt: 0 - valLoss: 0.6534337401390076 - trainLoss: 0.6540703773498535\n",
      "cnt: 0 - valLoss: 0.653432309627533 - trainLoss: 0.6540687084197998\n",
      "cnt: 0 - valLoss: 0.6534308791160583 - trainLoss: 0.6540670394897461\n",
      "cnt: 0 - valLoss: 0.6534295082092285 - trainLoss: 0.6540653109550476\n",
      "cnt: 0 - valLoss: 0.6534279584884644 - trainLoss: 0.6540635824203491\n",
      "cnt: 0 - valLoss: 0.6534265875816345 - trainLoss: 0.6540619730949402\n",
      "cnt: 0 - valLoss: 0.6534251570701599 - trainLoss: 0.6540602445602417\n",
      "cnt: 0 - valLoss: 0.6534236669540405 - trainLoss: 0.654058575630188\n",
      "cnt: 0 - valLoss: 0.6534222960472107 - trainLoss: 0.6540569067001343\n",
      "cnt: 0 - valLoss: 0.6534208655357361 - trainLoss: 0.6540552377700806\n",
      "cnt: 0 - valLoss: 0.6534194350242615 - trainLoss: 0.6540535092353821\n",
      "cnt: 0 - valLoss: 0.6534180641174316 - trainLoss: 0.6540517807006836\n",
      "cnt: 0 - valLoss: 0.653416633605957 - trainLoss: 0.6540501713752747\n",
      "cnt: 0 - valLoss: 0.6534152030944824 - trainLoss: 0.6540485620498657\n",
      "cnt: 0 - valLoss: 0.6534137725830078 - trainLoss: 0.6540468335151672\n",
      "cnt: 0 - valLoss: 0.6534122824668884 - trainLoss: 0.6540451049804688\n",
      "cnt: 0 - valLoss: 0.6534108519554138 - trainLoss: 0.654043436050415\n",
      "cnt: 0 - valLoss: 0.6534094214439392 - trainLoss: 0.6540417075157166\n",
      "cnt: 0 - valLoss: 0.6534079909324646 - trainLoss: 0.6540400385856628\n",
      "cnt: 0 - valLoss: 0.6534065008163452 - trainLoss: 0.6540383696556091\n",
      "cnt: 0 - valLoss: 0.6534050703048706 - trainLoss: 0.6540367007255554\n",
      "cnt: 0 - valLoss: 0.6534036993980408 - trainLoss: 0.6540350317955017\n",
      "cnt: 0 - valLoss: 0.6534021496772766 - trainLoss: 0.6540333032608032\n",
      "cnt: 0 - valLoss: 0.6534008383750916 - trainLoss: 0.6540315747261047\n",
      "cnt: 0 - valLoss: 0.6533994078636169 - trainLoss: 0.6540299654006958\n",
      "cnt: 0 - valLoss: 0.6533979773521423 - trainLoss: 0.6540282368659973\n",
      "cnt: 0 - valLoss: 0.6533965468406677 - trainLoss: 0.6540265679359436\n",
      "cnt: 0 - valLoss: 0.6533950567245483 - trainLoss: 0.6540248990058899\n",
      "cnt: 0 - valLoss: 0.6533936858177185 - trainLoss: 0.6540231704711914\n",
      "cnt: 0 - valLoss: 0.6533922553062439 - trainLoss: 0.6540215015411377\n",
      "cnt: 0 - valLoss: 0.6533907055854797 - trainLoss: 0.654019832611084\n",
      "cnt: 0 - valLoss: 0.6533893346786499 - trainLoss: 0.6540181636810303\n",
      "cnt: 0 - valLoss: 0.6533878445625305 - trainLoss: 0.6540164947509766\n",
      "cnt: 0 - valLoss: 0.6533864140510559 - trainLoss: 0.6540148854255676\n",
      "cnt: 0 - valLoss: 0.6533849239349365 - trainLoss: 0.6540131568908691\n",
      "cnt: 0 - valLoss: 0.6533835530281067 - trainLoss: 0.654011607170105\n",
      "cnt: 0 - valLoss: 0.6533821821212769 - trainLoss: 0.654009997844696\n",
      "cnt: 0 - valLoss: 0.6533807516098022 - trainLoss: 0.6540083289146423\n",
      "cnt: 0 - valLoss: 0.6533792614936829 - trainLoss: 0.6540066599845886\n",
      "cnt: 0 - valLoss: 0.6533778309822083 - trainLoss: 0.6540049910545349\n",
      "cnt: 0 - valLoss: 0.6533764004707336 - trainLoss: 0.6540033221244812\n",
      "cnt: 0 - valLoss: 0.6533749103546143 - trainLoss: 0.6540016531944275\n",
      "cnt: 0 - valLoss: 0.6533734202384949 - trainLoss: 0.6540000438690186\n",
      "cnt: 0 - valLoss: 0.6533719897270203 - trainLoss: 0.6539983749389648\n",
      "cnt: 0 - valLoss: 0.6533705592155457 - trainLoss: 0.6539967060089111\n",
      "cnt: 0 - valLoss: 0.653369128704071 - trainLoss: 0.6539950370788574\n",
      "cnt: 0 - valLoss: 0.6533676385879517 - trainLoss: 0.6539934277534485\n",
      "cnt: 0 - valLoss: 0.6533661484718323 - trainLoss: 0.6539917588233948\n",
      "cnt: 0 - valLoss: 0.6533647775650024 - trainLoss: 0.6539901494979858\n",
      "cnt: 0 - valLoss: 0.6533633470535278 - trainLoss: 0.6539885401725769\n",
      "cnt: 0 - valLoss: 0.6533619165420532 - trainLoss: 0.6539868712425232\n",
      "cnt: 0 - valLoss: 0.6533604264259338 - trainLoss: 0.6539852619171143\n",
      "cnt: 0 - valLoss: 0.6533589959144592 - trainLoss: 0.6539835929870605\n",
      "cnt: 0 - valLoss: 0.6533574461936951 - trainLoss: 0.6539819240570068\n",
      "cnt: 0 - valLoss: 0.6533560752868652 - trainLoss: 0.6539802551269531\n",
      "cnt: 0 - valLoss: 0.6533545255661011 - trainLoss: 0.6539785861968994\n",
      "cnt: 0 - valLoss: 0.6533530354499817 - trainLoss: 0.6539770364761353\n",
      "cnt: 0 - valLoss: 0.6533516049385071 - trainLoss: 0.6539753675460815\n",
      "cnt: 0 - valLoss: 0.6533501148223877 - trainLoss: 0.6539736986160278\n",
      "cnt: 0 - valLoss: 0.6533486843109131 - trainLoss: 0.6539720892906189\n",
      "cnt: 0 - valLoss: 0.6533472537994385 - trainLoss: 0.6539703607559204\n",
      "cnt: 0 - valLoss: 0.6533457636833191 - trainLoss: 0.6539688110351562\n",
      "cnt: 0 - valLoss: 0.6533442735671997 - trainLoss: 0.6539671421051025\n",
      "cnt: 0 - valLoss: 0.6533427238464355 - trainLoss: 0.6539655327796936\n",
      "cnt: 0 - valLoss: 0.6533412337303162 - trainLoss: 0.6539638638496399\n",
      "cnt: 0 - valLoss: 0.653339684009552 - trainLoss: 0.653962254524231\n",
      "cnt: 0 - valLoss: 0.6533382534980774 - trainLoss: 0.653960645198822\n",
      "cnt: 0 - valLoss: 0.6533367037773132 - trainLoss: 0.6539589762687683\n",
      "cnt: 0 - valLoss: 0.6533352136611938 - trainLoss: 0.6539572477340698\n",
      "cnt: 0 - valLoss: 0.6533336639404297 - trainLoss: 0.6539556384086609\n",
      "cnt: 0 - valLoss: 0.6533321142196655 - trainLoss: 0.653954029083252\n",
      "cnt: 0 - valLoss: 0.6533306241035461 - trainLoss: 0.653952419757843\n",
      "cnt: 0 - valLoss: 0.6533291935920715 - trainLoss: 0.6539506912231445\n",
      "cnt: 0 - valLoss: 0.6533277034759521 - trainLoss: 0.6539490818977356\n",
      "cnt: 0 - valLoss: 0.6533262133598328 - trainLoss: 0.6539474129676819\n",
      "cnt: 0 - valLoss: 0.6533247232437134 - trainLoss: 0.653945803642273\n",
      "cnt: 0 - valLoss: 0.6533231735229492 - trainLoss: 0.653944194316864\n",
      "cnt: 0 - valLoss: 0.6533216834068298 - trainLoss: 0.6539425253868103\n",
      "cnt: 0 - valLoss: 0.6533202528953552 - trainLoss: 0.6539408564567566\n",
      "cnt: 0 - valLoss: 0.6533187031745911 - trainLoss: 0.6539392471313477\n",
      "cnt: 0 - valLoss: 0.6533172130584717 - trainLoss: 0.653937578201294\n",
      "cnt: 0 - valLoss: 0.6533156037330627 - trainLoss: 0.6539359092712402\n",
      "cnt: 0 - valLoss: 0.6533141136169434 - trainLoss: 0.6539342403411865\n",
      "cnt: 0 - valLoss: 0.653312623500824 - trainLoss: 0.6539326310157776\n",
      "cnt: 0 - valLoss: 0.6533111333847046 - trainLoss: 0.6539309620857239\n",
      "cnt: 0 - valLoss: 0.6533096432685852 - trainLoss: 0.6539293527603149\n",
      "cnt: 0 - valLoss: 0.6533081531524658 - trainLoss: 0.6539276838302612\n",
      "cnt: 0 - valLoss: 0.6533066630363464 - trainLoss: 0.6539260149002075\n",
      "cnt: 0 - valLoss: 0.6533051133155823 - trainLoss: 0.6539244055747986\n",
      "cnt: 0 - valLoss: 0.6533036231994629 - trainLoss: 0.6539226770401001\n",
      "cnt: 0 - valLoss: 0.6533020734786987 - trainLoss: 0.6539211273193359\n",
      "cnt: 0 - valLoss: 0.6533005237579346 - trainLoss: 0.6539194583892822\n",
      "cnt: 0 - valLoss: 0.6532990336418152 - trainLoss: 0.6539178490638733\n",
      "cnt: 0 - valLoss: 0.6532974243164062 - trainLoss: 0.6539161801338196\n",
      "cnt: 0 - valLoss: 0.6532959938049316 - trainLoss: 0.6539145112037659\n",
      "cnt: 0 - valLoss: 0.6532943844795227 - trainLoss: 0.6539129614830017\n",
      "cnt: 0 - valLoss: 0.6532928943634033 - trainLoss: 0.653911292552948\n",
      "cnt: 0 - valLoss: 0.6532914042472839 - trainLoss: 0.6539096832275391\n",
      "cnt: 0 - valLoss: 0.6532899737358093 - trainLoss: 0.6539080739021301\n",
      "cnt: 0 - valLoss: 0.6532883644104004 - trainLoss: 0.6539064049720764\n",
      "cnt: 0 - valLoss: 0.6532868146896362 - trainLoss: 0.6539047956466675\n",
      "cnt: 0 - valLoss: 0.6532852649688721 - trainLoss: 0.6539031267166138\n",
      "cnt: 0 - valLoss: 0.6532837748527527 - trainLoss: 0.6539014577865601\n",
      "cnt: 0 - valLoss: 0.6532822251319885 - trainLoss: 0.6538998484611511\n",
      "cnt: 0 - valLoss: 0.6532807350158691 - trainLoss: 0.6538982391357422\n",
      "cnt: 0 - valLoss: 0.653279185295105 - trainLoss: 0.6538965702056885\n",
      "cnt: 0 - valLoss: 0.6532776355743408 - trainLoss: 0.6538949608802795\n",
      "cnt: 0 - valLoss: 0.6532760858535767 - trainLoss: 0.6538932919502258\n",
      "cnt: 0 - valLoss: 0.6532745957374573 - trainLoss: 0.6538916230201721\n",
      "cnt: 0 - valLoss: 0.6532731056213379 - trainLoss: 0.653890073299408\n",
      "cnt: 0 - valLoss: 0.6532715559005737 - trainLoss: 0.6538884043693542\n",
      "cnt: 0 - valLoss: 0.6532700061798096 - trainLoss: 0.6538867354393005\n",
      "cnt: 0 - valLoss: 0.6532685160636902 - trainLoss: 0.6538851261138916\n",
      "cnt: 0 - valLoss: 0.653266966342926 - trainLoss: 0.6538835167884827\n",
      "cnt: 0 - valLoss: 0.6532654762268066 - trainLoss: 0.6538817882537842\n",
      "cnt: 0 - valLoss: 0.6532639265060425 - trainLoss: 0.6538801789283752\n",
      "cnt: 0 - valLoss: 0.6532623767852783 - trainLoss: 0.6538784503936768\n",
      "cnt: 0 - valLoss: 0.6532608866691589 - trainLoss: 0.653876781463623\n",
      "cnt: 0 - valLoss: 0.6532593369483948 - trainLoss: 0.6538751721382141\n",
      "cnt: 0 - valLoss: 0.6532578468322754 - trainLoss: 0.6538733839988708\n",
      "cnt: 0 - valLoss: 0.6532562971115112 - trainLoss: 0.6538717746734619\n",
      "cnt: 0 - valLoss: 0.6532547473907471 - trainLoss: 0.653870165348053\n",
      "cnt: 0 - valLoss: 0.6532532572746277 - trainLoss: 0.6538684964179993\n",
      "cnt: 0 - valLoss: 0.6532517075538635 - trainLoss: 0.6538668274879456\n",
      "cnt: 0 - valLoss: 0.6532502174377441 - trainLoss: 0.6538651585578918\n",
      "cnt: 0 - valLoss: 0.65324866771698 - trainLoss: 0.6538634896278381\n",
      "cnt: 0 - valLoss: 0.6532471179962158 - trainLoss: 0.6538618206977844\n",
      "cnt: 0 - valLoss: 0.6532455682754517 - trainLoss: 0.6538601517677307\n",
      "cnt: 0 - valLoss: 0.6532440185546875 - trainLoss: 0.6538584232330322\n",
      "cnt: 0 - valLoss: 0.6532425284385681 - trainLoss: 0.6538568139076233\n",
      "cnt: 0 - valLoss: 0.6532409191131592 - trainLoss: 0.6538551449775696\n",
      "cnt: 0 - valLoss: 0.653239369392395 - trainLoss: 0.6538534760475159\n",
      "cnt: 0 - valLoss: 0.6532379388809204 - trainLoss: 0.6538517475128174\n",
      "cnt: 0 - valLoss: 0.6532363891601562 - trainLoss: 0.6538501381874084\n",
      "cnt: 0 - valLoss: 0.6532348990440369 - trainLoss: 0.6538484692573547\n",
      "cnt: 0 - valLoss: 0.6532332897186279 - trainLoss: 0.6538467407226562\n",
      "cnt: 0 - valLoss: 0.6532317399978638 - trainLoss: 0.6538450717926025\n",
      "cnt: 0 - valLoss: 0.6532301902770996 - trainLoss: 0.653843343257904\n",
      "cnt: 0 - valLoss: 0.6532286405563354 - trainLoss: 0.6538416743278503\n",
      "cnt: 0 - valLoss: 0.6532271504402161 - trainLoss: 0.6538399457931519\n",
      "cnt: 0 - valLoss: 0.6532255411148071 - trainLoss: 0.6538382768630981\n",
      "cnt: 0 - valLoss: 0.6532240509986877 - trainLoss: 0.6538366079330444\n",
      "cnt: 0 - valLoss: 0.6532225608825684 - trainLoss: 0.6538349390029907\n",
      "cnt: 0 - valLoss: 0.6532208919525146 - trainLoss: 0.6538332104682922\n",
      "cnt: 0 - valLoss: 0.6532194018363953 - trainLoss: 0.6538315415382385\n",
      "cnt: 0 - valLoss: 0.6532179713249207 - trainLoss: 0.65382981300354\n",
      "cnt: 0 - valLoss: 0.6532163619995117 - trainLoss: 0.6538282036781311\n",
      "cnt: 0 - valLoss: 0.6532148718833923 - trainLoss: 0.6538264751434326\n",
      "cnt: 0 - valLoss: 0.6532133221626282 - trainLoss: 0.6538247466087341\n",
      "cnt: 0 - valLoss: 0.653211772441864 - trainLoss: 0.6538230776786804\n",
      "cnt: 0 - valLoss: 0.6532102227210999 - trainLoss: 0.6538214087486267\n",
      "cnt: 0 - valLoss: 0.6532086730003357 - trainLoss: 0.6538196802139282\n",
      "cnt: 0 - valLoss: 0.6532071232795715 - trainLoss: 0.6538179516792297\n",
      "cnt: 0 - valLoss: 0.6532055139541626 - trainLoss: 0.653816282749176\n",
      "cnt: 0 - valLoss: 0.6532040238380432 - trainLoss: 0.6538146138191223\n",
      "cnt: 0 - valLoss: 0.653202474117279 - trainLoss: 0.6538129448890686\n",
      "cnt: 0 - valLoss: 0.6532009840011597 - trainLoss: 0.6538112163543701\n",
      "cnt: 0 - valLoss: 0.6531994938850403 - trainLoss: 0.6538096070289612\n",
      "cnt: 0 - valLoss: 0.6531979441642761 - trainLoss: 0.6538078188896179\n",
      "cnt: 0 - valLoss: 0.6531963348388672 - trainLoss: 0.6538061499595642\n",
      "cnt: 0 - valLoss: 0.653194785118103 - trainLoss: 0.6538044214248657\n",
      "cnt: 0 - valLoss: 0.6531932950019836 - trainLoss: 0.6538026928901672\n",
      "cnt: 0 - valLoss: 0.6531916856765747 - trainLoss: 0.6538010239601135\n",
      "cnt: 0 - valLoss: 0.6531900763511658 - trainLoss: 0.6537993550300598\n",
      "cnt: 0 - valLoss: 0.6531885266304016 - trainLoss: 0.6537976264953613\n",
      "cnt: 0 - valLoss: 0.6531870365142822 - trainLoss: 0.6537958979606628\n",
      "cnt: 0 - valLoss: 0.6531854867935181 - trainLoss: 0.6537942290306091\n",
      "cnt: 0 - valLoss: 0.6531839966773987 - trainLoss: 0.6537925004959106\n",
      "cnt: 0 - valLoss: 0.6531824469566345 - trainLoss: 0.6537908911705017\n",
      "cnt: 0 - valLoss: 0.6531808972358704 - trainLoss: 0.6537891626358032\n",
      "cnt: 0 - valLoss: 0.653179407119751 - trainLoss: 0.6537874341011047\n",
      "cnt: 0 - valLoss: 0.6531778573989868 - trainLoss: 0.653785765171051\n",
      "cnt: 0 - valLoss: 0.6531761884689331 - trainLoss: 0.6537840962409973\n",
      "cnt: 0 - valLoss: 0.6531746983528137 - trainLoss: 0.653782308101654\n",
      "cnt: 0 - valLoss: 0.6531732082366943 - trainLoss: 0.6537805795669556\n",
      "cnt: 0 - valLoss: 0.6531715393066406 - trainLoss: 0.6537789106369019\n",
      "cnt: 0 - valLoss: 0.6531700491905212 - trainLoss: 0.6537771821022034\n",
      "cnt: 0 - valLoss: 0.6531684994697571 - trainLoss: 0.6537755131721497\n",
      "cnt: 0 - valLoss: 0.6531668901443481 - trainLoss: 0.653773844242096\n",
      "cnt: 0 - valLoss: 0.6531654596328735 - trainLoss: 0.6537720561027527\n",
      "cnt: 0 - valLoss: 0.6531639099121094 - trainLoss: 0.653770387172699\n",
      "cnt: 0 - valLoss: 0.6531623601913452 - trainLoss: 0.6537687182426453\n",
      "cnt: 0 - valLoss: 0.653160810470581 - trainLoss: 0.6537670493125916\n",
      "cnt: 0 - valLoss: 0.6531592607498169 - trainLoss: 0.6537652611732483\n",
      "cnt: 0 - valLoss: 0.6531577110290527 - trainLoss: 0.6537635922431946\n",
      "cnt: 0 - valLoss: 0.6531561017036438 - trainLoss: 0.6537618637084961\n",
      "cnt: 0 - valLoss: 0.6531545519828796 - trainLoss: 0.6537601947784424\n",
      "cnt: 0 - valLoss: 0.6531530618667603 - trainLoss: 0.6537584662437439\n",
      "cnt: 0 - valLoss: 0.6531514525413513 - trainLoss: 0.6537567377090454\n",
      "cnt: 0 - valLoss: 0.6531498432159424 - trainLoss: 0.6537550687789917\n",
      "cnt: 0 - valLoss: 0.653148353099823 - trainLoss: 0.6537532806396484\n",
      "cnt: 0 - valLoss: 0.6531468629837036 - trainLoss: 0.6537516117095947\n",
      "cnt: 0 - valLoss: 0.6531453132629395 - trainLoss: 0.6537498831748962\n",
      "cnt: 0 - valLoss: 0.6531437039375305 - trainLoss: 0.6537482142448425\n",
      "cnt: 0 - valLoss: 0.6531421542167664 - trainLoss: 0.653746485710144\n",
      "cnt: 0 - valLoss: 0.653140664100647 - trainLoss: 0.6537447571754456\n",
      "cnt: 0 - valLoss: 0.6531391143798828 - trainLoss: 0.6537430286407471\n",
      "cnt: 0 - valLoss: 0.6531374454498291 - trainLoss: 0.6537413001060486\n",
      "cnt: 0 - valLoss: 0.6531358957290649 - trainLoss: 0.6537395715713501\n",
      "cnt: 0 - valLoss: 0.6531344056129456 - trainLoss: 0.6537379026412964\n",
      "cnt: 0 - valLoss: 0.6531328558921814 - trainLoss: 0.6537362337112427\n",
      "cnt: 0 - valLoss: 0.6531312465667725 - trainLoss: 0.6537344455718994\n",
      "cnt: 0 - valLoss: 0.6531298160552979 - trainLoss: 0.6537327766418457\n",
      "cnt: 0 - valLoss: 0.6531282663345337 - trainLoss: 0.653731107711792\n",
      "cnt: 0 - valLoss: 0.65312659740448 - trainLoss: 0.6537294387817383\n",
      "cnt: 0 - valLoss: 0.6531251072883606 - trainLoss: 0.6537275910377502\n",
      "cnt: 0 - valLoss: 0.6531235575675964 - trainLoss: 0.6537259221076965\n",
      "cnt: 0 - valLoss: 0.6531219482421875 - trainLoss: 0.6537242531776428\n",
      "cnt: 0 - valLoss: 0.6531203985214233 - trainLoss: 0.6537225842475891\n",
      "cnt: 0 - valLoss: 0.6531188488006592 - trainLoss: 0.6537207365036011\n",
      "cnt: 0 - valLoss: 0.653117299079895 - trainLoss: 0.6537190675735474\n",
      "cnt: 0 - valLoss: 0.6531156897544861 - trainLoss: 0.6537173390388489\n",
      "cnt: 0 - valLoss: 0.6531140804290771 - trainLoss: 0.6537156701087952\n",
      "cnt: 0 - valLoss: 0.6531125903129578 - trainLoss: 0.6537139415740967\n",
      "cnt: 0 - valLoss: 0.6531110405921936 - trainLoss: 0.6537122130393982\n",
      "cnt: 0 - valLoss: 0.6531094908714294 - trainLoss: 0.6537106037139893\n",
      "cnt: 0 - valLoss: 0.6531078815460205 - trainLoss: 0.653708815574646\n",
      "cnt: 0 - valLoss: 0.6531063318252563 - trainLoss: 0.6537070870399475\n",
      "cnt: 0 - valLoss: 0.6531047821044922 - trainLoss: 0.6537054181098938\n",
      "cnt: 0 - valLoss: 0.6531031727790833 - trainLoss: 0.6537037491798401\n",
      "cnt: 0 - valLoss: 0.6531016230583191 - trainLoss: 0.6537019610404968\n",
      "cnt: 0 - valLoss: 0.6531000137329102 - trainLoss: 0.6537002921104431\n",
      "cnt: 0 - valLoss: 0.653098464012146 - trainLoss: 0.6536986231803894\n",
      "cnt: 0 - valLoss: 0.6530967950820923 - trainLoss: 0.6536968350410461\n",
      "cnt: 0 - valLoss: 0.6530952453613281 - trainLoss: 0.6536951661109924\n",
      "cnt: 0 - valLoss: 0.6530937552452087 - trainLoss: 0.653693437576294\n",
      "cnt: 0 - valLoss: 0.6530922055244446 - trainLoss: 0.6536917686462402\n",
      "cnt: 0 - valLoss: 0.6530905961990356 - trainLoss: 0.6536900401115417\n",
      "cnt: 0 - valLoss: 0.6530889272689819 - trainLoss: 0.6536883115768433\n",
      "cnt: 0 - valLoss: 0.6530873775482178 - trainLoss: 0.6536865830421448\n",
      "cnt: 0 - valLoss: 0.6530858278274536 - trainLoss: 0.6536847949028015\n",
      "cnt: 0 - valLoss: 0.6530842185020447 - trainLoss: 0.6536831855773926\n",
      "cnt: 0 - valLoss: 0.6530824899673462 - trainLoss: 0.6536814570426941\n",
      "cnt: 0 - valLoss: 0.6530809998512268 - trainLoss: 0.6536797881126404\n",
      "cnt: 0 - valLoss: 0.6530794501304626 - trainLoss: 0.6536779403686523\n",
      "cnt: 0 - valLoss: 0.6530777812004089 - trainLoss: 0.6536762714385986\n",
      "cnt: 0 - valLoss: 0.6530762314796448 - trainLoss: 0.6536746025085449\n",
      "cnt: 0 - valLoss: 0.6530747413635254 - trainLoss: 0.6536728143692017\n",
      "cnt: 0 - valLoss: 0.6530731320381165 - trainLoss: 0.653671145439148\n",
      "cnt: 0 - valLoss: 0.6530714631080627 - trainLoss: 0.6536694765090942\n",
      "cnt: 0 - valLoss: 0.6530698537826538 - trainLoss: 0.6536677479743958\n",
      "cnt: 0 - valLoss: 0.6530683636665344 - trainLoss: 0.6536660194396973\n",
      "cnt: 0 - valLoss: 0.6530666947364807 - trainLoss: 0.6536642909049988\n",
      "cnt: 0 - valLoss: 0.6530650854110718 - trainLoss: 0.6536625623703003\n",
      "cnt: 0 - valLoss: 0.6530635356903076 - trainLoss: 0.6536608338356018\n",
      "cnt: 0 - valLoss: 0.6530618667602539 - trainLoss: 0.6536591053009033\n",
      "cnt: 0 - valLoss: 0.6530603170394897 - trainLoss: 0.6536574363708496\n",
      "cnt: 0 - valLoss: 0.6530587077140808 - trainLoss: 0.6536556482315063\n",
      "cnt: 0 - valLoss: 0.6530572175979614 - trainLoss: 0.6536539793014526\n",
      "cnt: 0 - valLoss: 0.6530555486679077 - trainLoss: 0.6536523103713989\n",
      "cnt: 0 - valLoss: 0.6530539989471436 - trainLoss: 0.6536505222320557\n",
      "cnt: 0 - valLoss: 0.6530523896217346 - trainLoss: 0.6536487936973572\n",
      "cnt: 0 - valLoss: 0.6530508399009705 - trainLoss: 0.6536470651626587\n",
      "cnt: 0 - valLoss: 0.6530491709709167 - trainLoss: 0.6536453366279602\n",
      "cnt: 0 - valLoss: 0.6530475616455078 - trainLoss: 0.6536436080932617\n",
      "cnt: 0 - valLoss: 0.6530460119247437 - trainLoss: 0.653641939163208\n",
      "cnt: 0 - valLoss: 0.6530443429946899 - trainLoss: 0.6536401510238647\n",
      "cnt: 0 - valLoss: 0.653042733669281 - trainLoss: 0.653638482093811\n",
      "cnt: 0 - valLoss: 0.6530411839485168 - trainLoss: 0.6536366939544678\n",
      "cnt: 0 - valLoss: 0.6530396342277527 - trainLoss: 0.6536349654197693\n",
      "cnt: 0 - valLoss: 0.6530380845069885 - trainLoss: 0.6536332964897156\n",
      "cnt: 0 - valLoss: 0.6530364751815796 - trainLoss: 0.6536315679550171\n",
      "cnt: 0 - valLoss: 0.6530348062515259 - trainLoss: 0.6536298394203186\n",
      "cnt: 0 - valLoss: 0.6530333161354065 - trainLoss: 0.6536280512809753\n",
      "cnt: 0 - valLoss: 0.6530317068099976 - trainLoss: 0.6536263823509216\n",
      "cnt: 0 - valLoss: 0.6530300378799438 - trainLoss: 0.6536246538162231\n",
      "cnt: 0 - valLoss: 0.6530284285545349 - trainLoss: 0.6536229252815247\n",
      "cnt: 0 - valLoss: 0.6530267000198364 - trainLoss: 0.6536211967468262\n",
      "cnt: 0 - valLoss: 0.6530250906944275 - trainLoss: 0.6536194682121277\n",
      "cnt: 0 - valLoss: 0.6530234813690186 - trainLoss: 0.6536177396774292\n",
      "cnt: 0 - valLoss: 0.6530217528343201 - trainLoss: 0.6536160707473755\n",
      "cnt: 0 - valLoss: 0.6530202031135559 - trainLoss: 0.653614342212677\n",
      "cnt: 0 - valLoss: 0.653018593788147 - trainLoss: 0.6536126732826233\n",
      "cnt: 0 - valLoss: 0.6530168652534485 - trainLoss: 0.6536109447479248\n",
      "cnt: 0 - valLoss: 0.6530152559280396 - trainLoss: 0.6536091566085815\n",
      "cnt: 0 - valLoss: 0.6530135273933411 - trainLoss: 0.6536074876785278\n",
      "cnt: 0 - valLoss: 0.6530118584632874 - trainLoss: 0.6536056995391846\n",
      "cnt: 0 - valLoss: 0.6530101895332336 - trainLoss: 0.6536040306091309\n",
      "cnt: 0 - valLoss: 0.6530085802078247 - trainLoss: 0.6536023020744324\n",
      "cnt: 0 - valLoss: 0.653006911277771 - trainLoss: 0.6536005139350891\n",
      "cnt: 0 - valLoss: 0.6530053019523621 - trainLoss: 0.6535988450050354\n",
      "cnt: 0 - valLoss: 0.6530036330223083 - trainLoss: 0.6535971164703369\n",
      "cnt: 0 - valLoss: 0.6530020236968994 - trainLoss: 0.6535954475402832\n",
      "cnt: 0 - valLoss: 0.6530004143714905 - trainLoss: 0.6535937190055847\n",
      "cnt: 0 - valLoss: 0.6529987454414368 - trainLoss: 0.6535919904708862\n",
      "cnt: 0 - valLoss: 0.6529970765113831 - trainLoss: 0.653590202331543\n",
      "cnt: 0 - valLoss: 0.6529953479766846 - trainLoss: 0.6535885334014893\n",
      "cnt: 0 - valLoss: 0.6529936790466309 - trainLoss: 0.653586745262146\n",
      "cnt: 0 - valLoss: 0.6529920101165771 - trainLoss: 0.6535850167274475\n",
      "cnt: 0 - valLoss: 0.6529902815818787 - trainLoss: 0.6535833477973938\n",
      "cnt: 0 - valLoss: 0.652988612651825 - trainLoss: 0.6535815596580505\n",
      "cnt: 0 - valLoss: 0.6529869437217712 - trainLoss: 0.6535798907279968\n",
      "cnt: 0 - valLoss: 0.6529852151870728 - trainLoss: 0.6535781621932983\n",
      "cnt: 0 - valLoss: 0.6529836058616638 - trainLoss: 0.6535763740539551\n",
      "cnt: 0 - valLoss: 0.6529819369316101 - trainLoss: 0.6535747051239014\n",
      "cnt: 0 - valLoss: 0.6529802680015564 - trainLoss: 0.6535730361938477\n",
      "cnt: 0 - valLoss: 0.6529785394668579 - trainLoss: 0.653571367263794\n",
      "cnt: 0 - valLoss: 0.6529768109321594 - trainLoss: 0.6535696387290955\n",
      "cnt: 0 - valLoss: 0.6529752016067505 - trainLoss: 0.6535678505897522\n",
      "cnt: 0 - valLoss: 0.652973473072052 - trainLoss: 0.6535661220550537\n",
      "cnt: 0 - valLoss: 0.6529717445373535 - trainLoss: 0.6535643935203552\n",
      "cnt: 0 - valLoss: 0.6529700756072998 - trainLoss: 0.6535626649856567\n",
      "cnt: 0 - valLoss: 0.6529683470726013 - trainLoss: 0.6535609364509583\n",
      "cnt: 0 - valLoss: 0.6529666781425476 - trainLoss: 0.6535592079162598\n",
      "cnt: 0 - valLoss: 0.6529650688171387 - trainLoss: 0.653557538986206\n",
      "cnt: 0 - valLoss: 0.652963399887085 - trainLoss: 0.6535558104515076\n",
      "cnt: 0 - valLoss: 0.6529617309570312 - trainLoss: 0.6535540819168091\n",
      "cnt: 0 - valLoss: 0.6529600024223328 - trainLoss: 0.6535523533821106\n",
      "cnt: 0 - valLoss: 0.652958333492279 - trainLoss: 0.6535506844520569\n",
      "cnt: 0 - valLoss: 0.6529566645622253 - trainLoss: 0.6535488963127136\n",
      "cnt: 0 - valLoss: 0.6529549360275269 - trainLoss: 0.6535471677780151\n",
      "cnt: 0 - valLoss: 0.6529532670974731 - trainLoss: 0.6535454392433167\n",
      "cnt: 0 - valLoss: 0.6529515981674194 - trainLoss: 0.6535437107086182\n",
      "cnt: 0 - valLoss: 0.6529498100280762 - trainLoss: 0.6535419225692749\n",
      "cnt: 0 - valLoss: 0.6529482007026672 - trainLoss: 0.6535402536392212\n",
      "cnt: 0 - valLoss: 0.6529465317726135 - trainLoss: 0.6535385251045227\n",
      "cnt: 0 - valLoss: 0.6529448628425598 - trainLoss: 0.6535367369651794\n",
      "cnt: 0 - valLoss: 0.6529431939125061 - trainLoss: 0.6535351276397705\n",
      "cnt: 0 - valLoss: 0.6529414653778076 - trainLoss: 0.653533399105072\n",
      "cnt: 0 - valLoss: 0.6529397964477539 - trainLoss: 0.6535316109657288\n",
      "cnt: 0 - valLoss: 0.6529381275177002 - trainLoss: 0.6535298824310303\n",
      "cnt: 0 - valLoss: 0.6529363989830017 - trainLoss: 0.6535281538963318\n",
      "cnt: 0 - valLoss: 0.6529346704483032 - trainLoss: 0.6535264253616333\n",
      "cnt: 0 - valLoss: 0.6529329419136047 - trainLoss: 0.6535246968269348\n",
      "cnt: 0 - valLoss: 0.652931272983551 - trainLoss: 0.6535229086875916\n",
      "cnt: 0 - valLoss: 0.6529296040534973 - trainLoss: 0.6535212397575378\n",
      "cnt: 0 - valLoss: 0.6529279351234436 - trainLoss: 0.6535194516181946\n",
      "cnt: 0 - valLoss: 0.6529262661933899 - trainLoss: 0.6535177230834961\n",
      "cnt: 0 - valLoss: 0.6529245972633362 - trainLoss: 0.6535160541534424\n",
      "cnt: 0 - valLoss: 0.6529228687286377 - trainLoss: 0.6535142660140991\n",
      "cnt: 0 - valLoss: 0.652921199798584 - trainLoss: 0.6535125970840454\n",
      "cnt: 0 - valLoss: 0.6529194712638855 - trainLoss: 0.6535108089447021\n",
      "cnt: 0 - valLoss: 0.6529178023338318 - trainLoss: 0.6535090804100037\n",
      "cnt: 0 - valLoss: 0.6529160737991333 - trainLoss: 0.6535073518753052\n",
      "cnt: 0 - valLoss: 0.6529144048690796 - trainLoss: 0.6535056233406067\n",
      "cnt: 0 - valLoss: 0.6529127359390259 - trainLoss: 0.6535038948059082\n",
      "cnt: 0 - valLoss: 0.6529110074043274 - trainLoss: 0.6535021066665649\n",
      "cnt: 0 - valLoss: 0.6529092788696289 - trainLoss: 0.6535004377365112\n",
      "cnt: 0 - valLoss: 0.6529077291488647 - trainLoss: 0.653498649597168\n",
      "cnt: 0 - valLoss: 0.6529060006141663 - trainLoss: 0.6534969806671143\n",
      "cnt: 0 - valLoss: 0.6529042720794678 - trainLoss: 0.653495192527771\n",
      "cnt: 0 - valLoss: 0.6529025435447693 - trainLoss: 0.6534934639930725\n",
      "cnt: 0 - valLoss: 0.6529008746147156 - trainLoss: 0.653491735458374\n",
      "cnt: 0 - valLoss: 0.6528991460800171 - trainLoss: 0.6534900069236755\n",
      "cnt: 0 - valLoss: 0.6528974175453186 - trainLoss: 0.6534881591796875\n",
      "cnt: 0 - valLoss: 0.6528957486152649 - trainLoss: 0.6534865498542786\n",
      "cnt: 0 - valLoss: 0.652894139289856 - trainLoss: 0.6534847617149353\n",
      "cnt: 0 - valLoss: 0.6528924107551575 - trainLoss: 0.653482973575592\n",
      "cnt: 0 - valLoss: 0.652890682220459 - trainLoss: 0.6534812450408936\n",
      "cnt: 0 - valLoss: 0.6528891324996948 - trainLoss: 0.6534795165061951\n",
      "cnt: 0 - valLoss: 0.6528873443603516 - trainLoss: 0.6534777879714966\n",
      "cnt: 0 - valLoss: 0.6528856158256531 - trainLoss: 0.6534759998321533\n",
      "cnt: 0 - valLoss: 0.6528838872909546 - trainLoss: 0.6534743309020996\n",
      "cnt: 0 - valLoss: 0.6528822183609009 - trainLoss: 0.6534725427627563\n",
      "cnt: 0 - valLoss: 0.6528805494308472 - trainLoss: 0.6534708738327026\n",
      "cnt: 0 - valLoss: 0.6528788208961487 - trainLoss: 0.6534690260887146\n",
      "cnt: 0 - valLoss: 0.6528770923614502 - trainLoss: 0.6534672975540161\n",
      "cnt: 0 - valLoss: 0.6528753638267517 - trainLoss: 0.6534655690193176\n",
      "cnt: 0 - valLoss: 0.652873694896698 - trainLoss: 0.6534638404846191\n",
      "cnt: 0 - valLoss: 0.6528719663619995 - trainLoss: 0.6534620523452759\n",
      "cnt: 0 - valLoss: 0.6528702974319458 - trainLoss: 0.6534602642059326\n",
      "cnt: 0 - valLoss: 0.6528686881065369 - trainLoss: 0.6534585952758789\n",
      "cnt: 0 - valLoss: 0.6528670191764832 - trainLoss: 0.6534568667411804\n",
      "cnt: 0 - valLoss: 0.6528652906417847 - trainLoss: 0.6534551382064819\n",
      "cnt: 0 - valLoss: 0.6528635621070862 - trainLoss: 0.6534533500671387\n",
      "cnt: 0 - valLoss: 0.6528618335723877 - trainLoss: 0.6534516215324402\n",
      "cnt: 0 - valLoss: 0.652860164642334 - trainLoss: 0.6534498333930969\n",
      "cnt: 0 - valLoss: 0.6528584361076355 - trainLoss: 0.6534481048583984\n",
      "cnt: 0 - valLoss: 0.6528566479682922 - trainLoss: 0.6534463167190552\n",
      "cnt: 0 - valLoss: 0.6528549790382385 - trainLoss: 0.6534445881843567\n",
      "cnt: 0 - valLoss: 0.6528533101081848 - trainLoss: 0.6534428596496582\n",
      "cnt: 0 - valLoss: 0.6528516411781311 - trainLoss: 0.6534410715103149\n",
      "cnt: 0 - valLoss: 0.6528499722480774 - trainLoss: 0.6534394025802612\n",
      "cnt: 0 - valLoss: 0.6528482437133789 - trainLoss: 0.653437614440918\n",
      "cnt: 0 - valLoss: 0.6528465151786804 - trainLoss: 0.6534358859062195\n",
      "cnt: 0 - valLoss: 0.6528448462486267 - trainLoss: 0.6534340977668762\n",
      "cnt: 0 - valLoss: 0.652843177318573 - trainLoss: 0.653432309627533\n",
      "cnt: 0 - valLoss: 0.6528413891792297 - trainLoss: 0.6534305810928345\n",
      "cnt: 0 - valLoss: 0.6528396606445312 - trainLoss: 0.6534287929534912\n",
      "cnt: 0 - valLoss: 0.6528379321098328 - trainLoss: 0.6534270644187927\n",
      "cnt: 0 - valLoss: 0.652836263179779 - trainLoss: 0.6534252762794495\n",
      "cnt: 0 - valLoss: 0.6528345346450806 - trainLoss: 0.653423547744751\n",
      "cnt: 0 - valLoss: 0.6528328657150269 - trainLoss: 0.6534217596054077\n",
      "cnt: 0 - valLoss: 0.6528311967849731 - trainLoss: 0.6534200310707092\n",
      "cnt: 0 - valLoss: 0.6528295278549194 - trainLoss: 0.6534183025360107\n",
      "cnt: 0 - valLoss: 0.6528278589248657 - trainLoss: 0.6534165143966675\n",
      "cnt: 0 - valLoss: 0.6528261303901672 - trainLoss: 0.653414785861969\n",
      "cnt: 0 - valLoss: 0.652824342250824 - trainLoss: 0.6534129977226257\n",
      "cnt: 0 - valLoss: 0.6528226137161255 - trainLoss: 0.6534112691879272\n",
      "cnt: 0 - valLoss: 0.6528209447860718 - trainLoss: 0.6534095406532288\n",
      "cnt: 0 - valLoss: 0.6528192162513733 - trainLoss: 0.6534076929092407\n",
      "cnt: 0 - valLoss: 0.6528174877166748 - trainLoss: 0.6534059643745422\n",
      "cnt: 0 - valLoss: 0.6528157591819763 - trainLoss: 0.653404176235199\n",
      "cnt: 0 - valLoss: 0.6528141498565674 - trainLoss: 0.6534024477005005\n",
      "cnt: 0 - valLoss: 0.6528124809265137 - trainLoss: 0.653400719165802\n",
      "cnt: 0 - valLoss: 0.6528107523918152 - trainLoss: 0.6533989310264587\n",
      "cnt: 0 - valLoss: 0.6528090238571167 - trainLoss: 0.6533971428871155\n",
      "cnt: 0 - valLoss: 0.6528072357177734 - trainLoss: 0.653395414352417\n",
      "cnt: 0 - valLoss: 0.6528055667877197 - trainLoss: 0.6533936262130737\n",
      "cnt: 0 - valLoss: 0.652803897857666 - trainLoss: 0.6533918976783752\n",
      "cnt: 0 - valLoss: 0.6528021097183228 - trainLoss: 0.653390109539032\n",
      "cnt: 0 - valLoss: 0.652800440788269 - trainLoss: 0.6533883810043335\n",
      "cnt: 0 - valLoss: 0.6527987122535706 - trainLoss: 0.6533865928649902\n",
      "cnt: 0 - valLoss: 0.6527969837188721 - trainLoss: 0.6533847451210022\n",
      "cnt: 0 - valLoss: 0.6527953147888184 - trainLoss: 0.6533830165863037\n",
      "cnt: 0 - valLoss: 0.6527935862541199 - trainLoss: 0.6533812880516052\n",
      "cnt: 0 - valLoss: 0.6527919173240662 - trainLoss: 0.6533795595169067\n",
      "cnt: 0 - valLoss: 0.6527901887893677 - trainLoss: 0.6533777713775635\n",
      "cnt: 0 - valLoss: 0.6527884602546692 - trainLoss: 0.653376042842865\n",
      "cnt: 0 - valLoss: 0.6527867317199707 - trainLoss: 0.6533742547035217\n",
      "cnt: 0 - valLoss: 0.652785062789917 - trainLoss: 0.6533724665641785\n",
      "cnt: 0 - valLoss: 0.6527832746505737 - trainLoss: 0.65337073802948\n",
      "cnt: 0 - valLoss: 0.65278160572052 - trainLoss: 0.6533689498901367\n",
      "cnt: 0 - valLoss: 0.6527798771858215 - trainLoss: 0.6533671617507935\n",
      "cnt: 0 - valLoss: 0.6527780890464783 - trainLoss: 0.6533653140068054\n",
      "cnt: 0 - valLoss: 0.6527764797210693 - trainLoss: 0.6533635854721069\n",
      "cnt: 0 - valLoss: 0.6527747511863708 - trainLoss: 0.6533618569374084\n",
      "cnt: 0 - valLoss: 0.6527730822563171 - trainLoss: 0.6533600091934204\n",
      "cnt: 0 - valLoss: 0.6527713537216187 - trainLoss: 0.6533582806587219\n",
      "cnt: 0 - valLoss: 0.6527696251869202 - trainLoss: 0.6533565521240234\n",
      "cnt: 0 - valLoss: 0.6527678966522217 - trainLoss: 0.6533547043800354\n",
      "cnt: 0 - valLoss: 0.6527661681175232 - trainLoss: 0.6533529162406921\n",
      "cnt: 0 - valLoss: 0.6527644991874695 - trainLoss: 0.6533511877059937\n",
      "cnt: 0 - valLoss: 0.6527626514434814 - trainLoss: 0.6533493995666504\n",
      "cnt: 0 - valLoss: 0.6527609825134277 - trainLoss: 0.6533476710319519\n",
      "cnt: 0 - valLoss: 0.652759313583374 - trainLoss: 0.6533458232879639\n",
      "cnt: 0 - valLoss: 0.6527576446533203 - trainLoss: 0.6533440947532654\n",
      "cnt: 0 - valLoss: 0.652755856513977 - trainLoss: 0.6533423066139221\n",
      "cnt: 0 - valLoss: 0.6527541875839233 - trainLoss: 0.6533405184745789\n",
      "cnt: 0 - valLoss: 0.6527524590492249 - trainLoss: 0.6533387899398804\n",
      "cnt: 0 - valLoss: 0.6527506709098816 - trainLoss: 0.6533370018005371\n",
      "cnt: 0 - valLoss: 0.6527490615844727 - trainLoss: 0.6533352136611938\n",
      "cnt: 0 - valLoss: 0.6527472734451294 - trainLoss: 0.6533334255218506\n",
      "cnt: 0 - valLoss: 0.6527455449104309 - trainLoss: 0.6533316373825073\n",
      "cnt: 0 - valLoss: 0.6527437567710876 - trainLoss: 0.6533299088478088\n",
      "cnt: 0 - valLoss: 0.6527420878410339 - trainLoss: 0.6533280611038208\n",
      "cnt: 0 - valLoss: 0.6527404189109802 - trainLoss: 0.6533263325691223\n",
      "cnt: 0 - valLoss: 0.6527387499809265 - trainLoss: 0.6533244848251343\n",
      "cnt: 0 - valLoss: 0.652737021446228 - trainLoss: 0.6533227562904358\n",
      "cnt: 0 - valLoss: 0.6527352333068848 - trainLoss: 0.6533209681510925\n",
      "cnt: 0 - valLoss: 0.652733564376831 - trainLoss: 0.6533191800117493\n",
      "cnt: 0 - valLoss: 0.652731716632843 - trainLoss: 0.653317391872406\n",
      "cnt: 0 - valLoss: 0.6527300477027893 - trainLoss: 0.6533156037330627\n",
      "cnt: 0 - valLoss: 0.6527283787727356 - trainLoss: 0.6533138751983643\n",
      "cnt: 0 - valLoss: 0.6527266502380371 - trainLoss: 0.653312087059021\n",
      "cnt: 0 - valLoss: 0.6527249217033386 - trainLoss: 0.6533102989196777\n",
      "cnt: 0 - valLoss: 0.6527231335639954 - trainLoss: 0.6533085107803345\n",
      "cnt: 0 - valLoss: 0.6527214646339417 - trainLoss: 0.6533067226409912\n",
      "cnt: 0 - valLoss: 0.6527197957038879 - trainLoss: 0.6533048748970032\n",
      "cnt: 0 - valLoss: 0.6527181267738342 - trainLoss: 0.6533031463623047\n",
      "cnt: 0 - valLoss: 0.6527162790298462 - trainLoss: 0.6533013582229614\n",
      "cnt: 0 - valLoss: 0.6527146100997925 - trainLoss: 0.6532995700836182\n",
      "cnt: 0 - valLoss: 0.6527128219604492 - trainLoss: 0.6532977819442749\n",
      "cnt: 0 - valLoss: 0.6527110934257507 - trainLoss: 0.6532959938049316\n",
      "cnt: 0 - valLoss: 0.6527093648910522 - trainLoss: 0.6532942056655884\n",
      "cnt: 0 - valLoss: 0.6527076363563538 - trainLoss: 0.6532924175262451\n",
      "cnt: 0 - valLoss: 0.6527059078216553 - trainLoss: 0.6532906293869019\n",
      "cnt: 0 - valLoss: 0.6527042984962463 - trainLoss: 0.6532887816429138\n",
      "cnt: 0 - valLoss: 0.6527024507522583 - trainLoss: 0.6532869935035706\n",
      "cnt: 0 - valLoss: 0.6527008414268494 - trainLoss: 0.6532853245735168\n",
      "cnt: 0 - valLoss: 0.6526990532875061 - trainLoss: 0.6532834768295288\n",
      "cnt: 0 - valLoss: 0.6526973843574524 - trainLoss: 0.6532816886901855\n",
      "cnt: 0 - valLoss: 0.6526955962181091 - trainLoss: 0.6532799005508423\n",
      "cnt: 0 - valLoss: 0.6526938676834106 - trainLoss: 0.653278112411499\n",
      "cnt: 0 - valLoss: 0.6526921987533569 - trainLoss: 0.6532763242721558\n",
      "cnt: 0 - valLoss: 0.6526904106140137 - trainLoss: 0.6532745361328125\n",
      "cnt: 0 - valLoss: 0.6526886820793152 - trainLoss: 0.6532726883888245\n",
      "cnt: 0 - valLoss: 0.6526868939399719 - trainLoss: 0.6532709002494812\n",
      "cnt: 0 - valLoss: 0.6526851654052734 - trainLoss: 0.6532691121101379\n",
      "cnt: 0 - valLoss: 0.6526834964752197 - trainLoss: 0.6532673835754395\n",
      "cnt: 0 - valLoss: 0.652681827545166 - trainLoss: 0.6532655954360962\n",
      "cnt: 0 - valLoss: 0.6526800990104675 - trainLoss: 0.6532637476921082\n",
      "cnt: 0 - valLoss: 0.6526783108711243 - trainLoss: 0.6532619595527649\n",
      "cnt: 0 - valLoss: 0.6526766419410706 - trainLoss: 0.6532601714134216\n",
      "cnt: 0 - valLoss: 0.6526749134063721 - trainLoss: 0.6532583236694336\n",
      "cnt: 0 - valLoss: 0.6526731252670288 - trainLoss: 0.6532565355300903\n",
      "cnt: 0 - valLoss: 0.6526713967323303 - trainLoss: 0.6532548069953918\n",
      "cnt: 0 - valLoss: 0.6526696085929871 - trainLoss: 0.6532530188560486\n",
      "cnt: 0 - valLoss: 0.6526679396629333 - trainLoss: 0.6532511711120605\n",
      "cnt: 0 - valLoss: 0.6526661515235901 - trainLoss: 0.6532493829727173\n",
      "cnt: 0 - valLoss: 0.6526645421981812 - trainLoss: 0.6532476544380188\n",
      "cnt: 0 - valLoss: 0.6526627540588379 - trainLoss: 0.6532459259033203\n",
      "cnt: 0 - valLoss: 0.6526610255241394 - trainLoss: 0.6532441973686218\n",
      "cnt: 0 - valLoss: 0.6526592373847961 - trainLoss: 0.6532423496246338\n",
      "cnt: 0 - valLoss: 0.6526575684547424 - trainLoss: 0.6532406210899353\n",
      "cnt: 0 - valLoss: 0.652655839920044 - trainLoss: 0.653238832950592\n",
      "cnt: 0 - valLoss: 0.6526540517807007 - trainLoss: 0.6532371044158936\n",
      "cnt: 0 - valLoss: 0.6526523232460022 - trainLoss: 0.6532353162765503\n",
      "cnt: 0 - valLoss: 0.6526505947113037 - trainLoss: 0.653233528137207\n",
      "cnt: 0 - valLoss: 0.6526488065719604 - trainLoss: 0.6532317996025085\n",
      "cnt: 0 - valLoss: 0.6526471376419067 - trainLoss: 0.6532300114631653\n",
      "cnt: 0 - valLoss: 0.652645468711853 - trainLoss: 0.653228223323822\n",
      "cnt: 0 - valLoss: 0.6526437997817993 - trainLoss: 0.6532264947891235\n",
      "cnt: 0 - valLoss: 0.652642011642456 - trainLoss: 0.6532247066497803\n",
      "cnt: 0 - valLoss: 0.6526402235031128 - trainLoss: 0.6532229781150818\n",
      "cnt: 0 - valLoss: 0.6526384949684143 - trainLoss: 0.6532211899757385\n",
      "cnt: 0 - valLoss: 0.6526367664337158 - trainLoss: 0.65321946144104\n",
      "cnt: 0 - valLoss: 0.6526350378990173 - trainLoss: 0.653217613697052\n",
      "cnt: 0 - valLoss: 0.6526333093643188 - trainLoss: 0.6532158851623535\n",
      "cnt: 0 - valLoss: 0.6526315808296204 - trainLoss: 0.6532140970230103\n",
      "cnt: 0 - valLoss: 0.6526298522949219 - trainLoss: 0.653212308883667\n",
      "cnt: 0 - valLoss: 0.6526281237602234 - trainLoss: 0.6532105803489685\n",
      "cnt: 0 - valLoss: 0.6526263952255249 - trainLoss: 0.6532087922096252\n",
      "cnt: 0 - valLoss: 0.6526247262954712 - trainLoss: 0.6532070636749268\n",
      "cnt: 0 - valLoss: 0.6526229381561279 - trainLoss: 0.6532052755355835\n",
      "cnt: 0 - valLoss: 0.6526212096214294 - trainLoss: 0.653203547000885\n",
      "cnt: 0 - valLoss: 0.6526195406913757 - trainLoss: 0.6532017588615417\n",
      "cnt: 0 - valLoss: 0.6526177525520325 - trainLoss: 0.6531999707221985\n",
      "cnt: 0 - valLoss: 0.652616024017334 - trainLoss: 0.6531981825828552\n",
      "cnt: 0 - valLoss: 0.6526142358779907 - trainLoss: 0.6531964540481567\n",
      "cnt: 0 - valLoss: 0.6526125073432922 - trainLoss: 0.6531946659088135\n",
      "cnt: 0 - valLoss: 0.652610719203949 - trainLoss: 0.6531928777694702\n",
      "cnt: 0 - valLoss: 0.6526089906692505 - trainLoss: 0.6531910300254822\n",
      "cnt: 0 - valLoss: 0.6526073217391968 - trainLoss: 0.6531893014907837\n",
      "cnt: 0 - valLoss: 0.6526056528091431 - trainLoss: 0.6531875729560852\n",
      "cnt: 0 - valLoss: 0.6526038646697998 - trainLoss: 0.6531858444213867\n",
      "cnt: 0 - valLoss: 0.6526021361351013 - trainLoss: 0.6531840562820435\n",
      "cnt: 0 - valLoss: 0.6526004076004028 - trainLoss: 0.6531822085380554\n",
      "cnt: 0 - valLoss: 0.6525986194610596 - trainLoss: 0.6531804203987122\n",
      "cnt: 0 - valLoss: 0.6525968909263611 - trainLoss: 0.6531786322593689\n",
      "cnt: 0 - valLoss: 0.6525951623916626 - trainLoss: 0.6531769037246704\n",
      "cnt: 0 - valLoss: 0.6525933742523193 - trainLoss: 0.6531751155853271\n",
      "cnt: 0 - valLoss: 0.6525916457176208 - trainLoss: 0.6531733274459839\n",
      "cnt: 0 - valLoss: 0.6525899171829224 - trainLoss: 0.6531715989112854\n",
      "cnt: 0 - valLoss: 0.6525882482528687 - trainLoss: 0.6531697511672974\n",
      "cnt: 0 - valLoss: 0.6525865197181702 - trainLoss: 0.6531680226325989\n",
      "cnt: 0 - valLoss: 0.6525847911834717 - trainLoss: 0.6531662344932556\n",
      "cnt: 0 - valLoss: 0.6525830626487732 - trainLoss: 0.6531645059585571\n",
      "cnt: 0 - valLoss: 0.6525812745094299 - trainLoss: 0.6531627178192139\n",
      "cnt: 0 - valLoss: 0.6525794863700867 - trainLoss: 0.6531608700752258\n",
      "cnt: 0 - valLoss: 0.6525777578353882 - trainLoss: 0.6531591415405273\n",
      "cnt: 0 - valLoss: 0.6525760293006897 - trainLoss: 0.6531573534011841\n",
      "cnt: 0 - valLoss: 0.6525742411613464 - trainLoss: 0.6531555652618408\n",
      "cnt: 0 - valLoss: 0.6525725722312927 - trainLoss: 0.6531537771224976\n",
      "cnt: 0 - valLoss: 0.652570903301239 - trainLoss: 0.6531520485877991\n",
      "cnt: 0 - valLoss: 0.6525693535804749 - trainLoss: 0.6531502604484558\n",
      "cnt: 0 - valLoss: 0.6525676846504211 - trainLoss: 0.6531485319137573\n",
      "cnt: 0 - valLoss: 0.6525659561157227 - trainLoss: 0.6531468629837036\n",
      "cnt: 0 - valLoss: 0.652564287185669 - trainLoss: 0.6531450748443604\n",
      "cnt: 0 - valLoss: 0.6525626182556152 - trainLoss: 0.6531433463096619\n",
      "cnt: 0 - valLoss: 0.6525608897209167 - trainLoss: 0.6531416177749634\n",
      "cnt: 0 - valLoss: 0.652559220790863 - trainLoss: 0.6531398892402649\n",
      "cnt: 0 - valLoss: 0.6525575518608093 - trainLoss: 0.6531381607055664\n",
      "cnt: 0 - valLoss: 0.6525558233261108 - trainLoss: 0.6531363725662231\n",
      "cnt: 0 - valLoss: 0.6525542140007019 - trainLoss: 0.6531346440315247\n",
      "cnt: 0 - valLoss: 0.6525524854660034 - trainLoss: 0.6531328558921814\n",
      "cnt: 0 - valLoss: 0.6525508761405945 - trainLoss: 0.6531311273574829\n",
      "cnt: 0 - valLoss: 0.6525492072105408 - trainLoss: 0.6531293988227844\n",
      "cnt: 0 - valLoss: 0.6525475382804871 - trainLoss: 0.6531276702880859\n",
      "cnt: 0 - valLoss: 0.6525458693504333 - trainLoss: 0.6531258821487427\n",
      "cnt: 0 - valLoss: 0.6525442004203796 - trainLoss: 0.6531240940093994\n",
      "cnt: 0 - valLoss: 0.6525424718856812 - trainLoss: 0.6531224250793457\n",
      "cnt: 0 - valLoss: 0.6525408029556274 - trainLoss: 0.6531206965446472\n",
      "cnt: 0 - valLoss: 0.6525391340255737 - trainLoss: 0.653118908405304\n",
      "cnt: 0 - valLoss: 0.6525374054908752 - trainLoss: 0.6531170606613159\n",
      "cnt: 0 - valLoss: 0.6525357365608215 - trainLoss: 0.6531153917312622\n",
      "cnt: 0 - valLoss: 0.6525340676307678 - trainLoss: 0.6531136631965637\n",
      "cnt: 0 - valLoss: 0.6525323987007141 - trainLoss: 0.6531118750572205\n",
      "cnt: 0 - valLoss: 0.6525307893753052 - trainLoss: 0.653110146522522\n",
      "cnt: 0 - valLoss: 0.6525290608406067 - trainLoss: 0.6531084179878235\n",
      "cnt: 0 - valLoss: 0.652527391910553 - trainLoss: 0.653106689453125\n",
      "cnt: 0 - valLoss: 0.6525257229804993 - trainLoss: 0.6531049609184265\n",
      "cnt: 0 - valLoss: 0.6525240540504456 - trainLoss: 0.6531031131744385\n",
      "cnt: 0 - valLoss: 0.6525223255157471 - trainLoss: 0.6531014442443848\n",
      "cnt: 0 - valLoss: 0.6525206565856934 - trainLoss: 0.6530996561050415\n",
      "cnt: 0 - valLoss: 0.6525190472602844 - trainLoss: 0.653097927570343\n",
      "cnt: 0 - valLoss: 0.6525173187255859 - trainLoss: 0.6530961990356445\n",
      "cnt: 0 - valLoss: 0.6525156497955322 - trainLoss: 0.6530944108963013\n",
      "cnt: 0 - valLoss: 0.6525139808654785 - trainLoss: 0.653092622756958\n",
      "cnt: 0 - valLoss: 0.6525123715400696 - trainLoss: 0.6530908942222595\n",
      "cnt: 0 - valLoss: 0.6525106430053711 - trainLoss: 0.653089165687561\n",
      "cnt: 0 - valLoss: 0.6525089144706726 - trainLoss: 0.6530873775482178\n",
      "cnt: 0 - valLoss: 0.6525073051452637 - trainLoss: 0.6530856490135193\n",
      "cnt: 0 - valLoss: 0.6525055766105652 - trainLoss: 0.653083860874176\n",
      "cnt: 0 - valLoss: 0.6525039076805115 - trainLoss: 0.6530821919441223\n",
      "cnt: 0 - valLoss: 0.652502179145813 - trainLoss: 0.6530803442001343\n",
      "cnt: 0 - valLoss: 0.6525005102157593 - trainLoss: 0.6530786156654358\n",
      "cnt: 0 - valLoss: 0.6524988412857056 - trainLoss: 0.6530768275260925\n",
      "cnt: 0 - valLoss: 0.6524971127510071 - trainLoss: 0.653075098991394\n",
      "cnt: 0 - valLoss: 0.6524954438209534 - trainLoss: 0.6530733704566956\n",
      "cnt: 0 - valLoss: 0.6524938344955444 - trainLoss: 0.6530715823173523\n",
      "cnt: 0 - valLoss: 0.6524921655654907 - trainLoss: 0.6530698537826538\n",
      "cnt: 0 - valLoss: 0.652490496635437 - trainLoss: 0.6530680656433105\n",
      "cnt: 0 - valLoss: 0.6524887681007385 - trainLoss: 0.6530663967132568\n",
      "cnt: 0 - valLoss: 0.6524870991706848 - trainLoss: 0.6530645489692688\n",
      "cnt: 0 - valLoss: 0.6524853706359863 - trainLoss: 0.6530628204345703\n",
      "cnt: 0 - valLoss: 0.6524836421012878 - trainLoss: 0.6530610918998718\n",
      "cnt: 0 - valLoss: 0.6524820327758789 - trainLoss: 0.6530592441558838\n",
      "cnt: 0 - valLoss: 0.6524802446365356 - trainLoss: 0.6530575156211853\n",
      "cnt: 0 - valLoss: 0.6524785757064819 - trainLoss: 0.653055727481842\n",
      "cnt: 0 - valLoss: 0.6524769067764282 - trainLoss: 0.6530539989471436\n",
      "cnt: 0 - valLoss: 0.6524753570556641 - trainLoss: 0.6530522704124451\n",
      "cnt: 0 - valLoss: 0.6524735689163208 - trainLoss: 0.6530504822731018\n",
      "cnt: 0 - valLoss: 0.6524718999862671 - trainLoss: 0.6530486941337585\n",
      "cnt: 0 - valLoss: 0.6524702310562134 - trainLoss: 0.6530469655990601\n",
      "cnt: 0 - valLoss: 0.6524685025215149 - trainLoss: 0.6530451774597168\n",
      "cnt: 0 - valLoss: 0.6524668335914612 - trainLoss: 0.6530433893203735\n",
      "cnt: 0 - valLoss: 0.6524651050567627 - trainLoss: 0.6530417203903198\n",
      "cnt: 0 - valLoss: 0.652463436126709 - trainLoss: 0.6530398726463318\n",
      "cnt: 0 - valLoss: 0.6524617671966553 - trainLoss: 0.6530381441116333\n",
      "cnt: 0 - valLoss: 0.6524600982666016 - trainLoss: 0.65303635597229\n",
      "cnt: 0 - valLoss: 0.6524584293365479 - trainLoss: 0.6530346274375916\n",
      "cnt: 0 - valLoss: 0.6524567604064941 - trainLoss: 0.6530327796936035\n",
      "cnt: 0 - valLoss: 0.6524550914764404 - trainLoss: 0.653031051158905\n",
      "cnt: 0 - valLoss: 0.6524533629417419 - trainLoss: 0.6530293226242065\n",
      "cnt: 0 - valLoss: 0.6524516344070435 - trainLoss: 0.6530275344848633\n",
      "cnt: 0 - valLoss: 0.6524499654769897 - trainLoss: 0.65302574634552\n",
      "cnt: 0 - valLoss: 0.6524482369422913 - trainLoss: 0.6530240178108215\n",
      "cnt: 0 - valLoss: 0.6524465084075928 - trainLoss: 0.6530222296714783\n",
      "cnt: 0 - valLoss: 0.6524448394775391 - trainLoss: 0.6530205011367798\n",
      "cnt: 0 - valLoss: 0.6524431705474854 - trainLoss: 0.6530187129974365\n",
      "cnt: 0 - valLoss: 0.6524414420127869 - trainLoss: 0.6530170440673828\n",
      "cnt: 0 - valLoss: 0.6524397730827332 - trainLoss: 0.6530151963233948\n",
      "cnt: 0 - valLoss: 0.6524381041526794 - trainLoss: 0.6530135273933411\n",
      "cnt: 0 - valLoss: 0.6524364352226257 - trainLoss: 0.6530118584632874\n",
      "cnt: 0 - valLoss: 0.6524347066879272 - trainLoss: 0.6530100703239441\n",
      "cnt: 0 - valLoss: 0.6524329781532288 - trainLoss: 0.6530084013938904\n",
      "cnt: 0 - valLoss: 0.6524312496185303 - trainLoss: 0.6530065536499023\n",
      "cnt: 0 - valLoss: 0.6524295806884766 - trainLoss: 0.6530048251152039\n",
      "cnt: 0 - valLoss: 0.6524278521537781 - trainLoss: 0.6530030369758606\n",
      "cnt: 0 - valLoss: 0.6524261832237244 - trainLoss: 0.6530013084411621\n",
      "cnt: 0 - valLoss: 0.6524244546890259 - trainLoss: 0.6529995799064636\n",
      "cnt: 0 - valLoss: 0.6524227261543274 - trainLoss: 0.6529978513717651\n",
      "cnt: 0 - valLoss: 0.6524210572242737 - trainLoss: 0.6529960632324219\n",
      "cnt: 0 - valLoss: 0.6524194478988647 - trainLoss: 0.6529943346977234\n",
      "cnt: 0 - valLoss: 0.6524176597595215 - trainLoss: 0.6529926061630249\n",
      "cnt: 0 - valLoss: 0.6524159908294678 - trainLoss: 0.6529909372329712\n",
      "cnt: 0 - valLoss: 0.6524142026901245 - trainLoss: 0.6529892086982727\n",
      "cnt: 0 - valLoss: 0.6524125337600708 - trainLoss: 0.6529874801635742\n",
      "cnt: 0 - valLoss: 0.6524108052253723 - trainLoss: 0.6529857516288757\n",
      "cnt: 0 - valLoss: 0.6524090766906738 - trainLoss: 0.652984082698822\n",
      "cnt: 0 - valLoss: 0.6524073481559753 - trainLoss: 0.6529823541641235\n",
      "cnt: 0 - valLoss: 0.6524056196212769 - trainLoss: 0.6529805660247803\n",
      "cnt: 0 - valLoss: 0.6524038314819336 - trainLoss: 0.6529788970947266\n",
      "cnt: 0 - valLoss: 0.6524021029472351 - trainLoss: 0.6529771685600281\n",
      "cnt: 0 - valLoss: 0.6524004340171814 - trainLoss: 0.6529754996299744\n",
      "cnt: 0 - valLoss: 0.6523987650871277 - trainLoss: 0.6529737710952759\n",
      "cnt: 0 - valLoss: 0.652397096157074 - trainLoss: 0.6529720425605774\n",
      "cnt: 0 - valLoss: 0.6523953080177307 - trainLoss: 0.6529703736305237\n",
      "cnt: 0 - valLoss: 0.6523935794830322 - trainLoss: 0.6529685854911804\n",
      "cnt: 0 - valLoss: 0.6523918509483337 - trainLoss: 0.6529669165611267\n",
      "cnt: 0 - valLoss: 0.6523901224136353 - trainLoss: 0.6529651880264282\n",
      "cnt: 0 - valLoss: 0.652388334274292 - trainLoss: 0.6529635190963745\n",
      "cnt: 0 - valLoss: 0.6523866653442383 - trainLoss: 0.6529617309570312\n",
      "cnt: 0 - valLoss: 0.6523849368095398 - trainLoss: 0.6529600620269775\n",
      "cnt: 0 - valLoss: 0.6523831486701965 - trainLoss: 0.6529582738876343\n",
      "cnt: 0 - valLoss: 0.6523814797401428 - trainLoss: 0.6529566049575806\n",
      "cnt: 0 - valLoss: 0.6523797512054443 - trainLoss: 0.6529548764228821\n",
      "cnt: 0 - valLoss: 0.6523779630661011 - trainLoss: 0.6529531478881836\n",
      "cnt: 0 - valLoss: 0.6523760557174683 - trainLoss: 0.6529515981674194\n",
      "cnt: 0 - valLoss: 0.6523740887641907 - trainLoss: 0.6529500484466553\n",
      "cnt: 0 - valLoss: 0.6523721814155579 - trainLoss: 0.6529483795166016\n",
      "cnt: 0 - valLoss: 0.6523703336715698 - trainLoss: 0.6529468297958374\n",
      "cnt: 0 - valLoss: 0.652368426322937 - trainLoss: 0.6529452204704285\n",
      "cnt: 0 - valLoss: 0.652366578578949 - trainLoss: 0.6529436707496643\n",
      "cnt: 0 - valLoss: 0.6523646712303162 - trainLoss: 0.6529420018196106\n",
      "cnt: 0 - valLoss: 0.6523627042770386 - trainLoss: 0.6529403924942017\n",
      "cnt: 0 - valLoss: 0.6523607969284058 - trainLoss: 0.6529387831687927\n",
      "cnt: 0 - valLoss: 0.6523589491844177 - trainLoss: 0.6529371738433838\n",
      "cnt: 0 - valLoss: 0.6523569226264954 - trainLoss: 0.6529356241226196\n",
      "cnt: 0 - valLoss: 0.6523551940917969 - trainLoss: 0.6529340147972107\n",
      "cnt: 0 - valLoss: 0.6523533463478088 - trainLoss: 0.6529325246810913\n",
      "cnt: 0 - valLoss: 0.652351438999176 - trainLoss: 0.6529309153556824\n",
      "cnt: 0 - valLoss: 0.652349591255188 - trainLoss: 0.6529293060302734\n",
      "cnt: 0 - valLoss: 0.6523476839065552 - trainLoss: 0.6529277563095093\n",
      "cnt: 0 - valLoss: 0.6523458957672119 - trainLoss: 0.6529260873794556\n",
      "cnt: 0 - valLoss: 0.6523441076278687 - trainLoss: 0.6529245376586914\n",
      "cnt: 0 - valLoss: 0.6523423194885254 - trainLoss: 0.6529229283332825\n",
      "cnt: 0 - valLoss: 0.6523404717445374 - trainLoss: 0.6529213786125183\n",
      "cnt: 0 - valLoss: 0.6523386240005493 - trainLoss: 0.6529197692871094\n",
      "cnt: 0 - valLoss: 0.6523368954658508 - trainLoss: 0.6529182195663452\n",
      "cnt: 0 - valLoss: 0.6523350477218628 - trainLoss: 0.652916669845581\n",
      "cnt: 0 - valLoss: 0.6523332595825195 - trainLoss: 0.6529150605201721\n",
      "cnt: 0 - valLoss: 0.6523315906524658 - trainLoss: 0.652913510799408\n",
      "cnt: 0 - valLoss: 0.6523298025131226 - trainLoss: 0.6529119610786438\n",
      "cnt: 0 - valLoss: 0.6523279547691345 - trainLoss: 0.6529104113578796\n",
      "cnt: 0 - valLoss: 0.652326226234436 - trainLoss: 0.6529089212417603\n",
      "cnt: 0 - valLoss: 0.6523244380950928 - trainLoss: 0.6529072523117065\n",
      "cnt: 0 - valLoss: 0.6523225903511047 - trainLoss: 0.6529057621955872\n",
      "cnt: 0 - valLoss: 0.652320921421051 - trainLoss: 0.6529041528701782\n",
      "cnt: 0 - valLoss: 0.652319073677063 - trainLoss: 0.6529025435447693\n",
      "cnt: 0 - valLoss: 0.6523172855377197 - trainLoss: 0.6529010534286499\n",
      "cnt: 0 - valLoss: 0.6523154377937317 - trainLoss: 0.652899444103241\n",
      "cnt: 0 - valLoss: 0.6523137092590332 - trainLoss: 0.6528978943824768\n",
      "cnt: 0 - valLoss: 0.6523119211196899 - trainLoss: 0.6528963446617126\n",
      "cnt: 0 - valLoss: 0.6523100733757019 - trainLoss: 0.6528946757316589\n",
      "cnt: 0 - valLoss: 0.6523084044456482 - trainLoss: 0.6528931260108948\n",
      "cnt: 0 - valLoss: 0.6523066759109497 - trainLoss: 0.6528916358947754\n",
      "cnt: 0 - valLoss: 0.6523048877716064 - trainLoss: 0.6528900861740112\n",
      "cnt: 0 - valLoss: 0.6523030996322632 - trainLoss: 0.6528884768486023\n",
      "cnt: 0 - valLoss: 0.6523012518882751 - trainLoss: 0.6528869271278381\n",
      "cnt: 0 - valLoss: 0.6522995233535767 - trainLoss: 0.652885377407074\n",
      "cnt: 0 - valLoss: 0.6522977352142334 - trainLoss: 0.652883768081665\n",
      "cnt: 0 - valLoss: 0.6522958874702454 - trainLoss: 0.6528822183609009\n",
      "cnt: 0 - valLoss: 0.6522940993309021 - trainLoss: 0.6528806090354919\n",
      "cnt: 0 - valLoss: 0.6522922515869141 - trainLoss: 0.6528791189193726\n",
      "cnt: 0 - valLoss: 0.6522905230522156 - trainLoss: 0.6528775095939636\n",
      "cnt: 0 - valLoss: 0.6522887349128723 - trainLoss: 0.6528759598731995\n",
      "cnt: 0 - valLoss: 0.652286946773529 - trainLoss: 0.6528743505477905\n",
      "cnt: 0 - valLoss: 0.6522852778434753 - trainLoss: 0.6528728008270264\n",
      "cnt: 0 - valLoss: 0.6522834897041321 - trainLoss: 0.6528712511062622\n",
      "cnt: 0 - valLoss: 0.652281641960144 - trainLoss: 0.652869701385498\n",
      "cnt: 0 - valLoss: 0.6522799134254456 - trainLoss: 0.6528680920600891\n",
      "cnt: 0 - valLoss: 0.6522780656814575 - trainLoss: 0.652866542339325\n",
      "cnt: 0 - valLoss: 0.6522762775421143 - trainLoss: 0.652864933013916\n",
      "cnt: 0 - valLoss: 0.6522745490074158 - trainLoss: 0.6528633832931519\n",
      "cnt: 0 - valLoss: 0.6522727012634277 - trainLoss: 0.6528618335723877\n",
      "cnt: 0 - valLoss: 0.6522709131240845 - trainLoss: 0.6528602242469788\n",
      "cnt: 0 - valLoss: 0.6522690653800964 - trainLoss: 0.6528586745262146\n",
      "cnt: 0 - valLoss: 0.6522672772407532 - trainLoss: 0.6528570652008057\n",
      "cnt: 0 - valLoss: 0.6522654891014099 - trainLoss: 0.6528554558753967\n",
      "cnt: 0 - valLoss: 0.6522637009620667 - trainLoss: 0.6528539061546326\n",
      "cnt: 0 - valLoss: 0.6522620320320129 - trainLoss: 0.6528522968292236\n",
      "cnt: 0 - valLoss: 0.6522602438926697 - trainLoss: 0.6528508067131042\n",
      "cnt: 0 - valLoss: 0.6522584557533264 - trainLoss: 0.6528492569923401\n",
      "cnt: 0 - valLoss: 0.6522566080093384 - trainLoss: 0.6528476476669312\n",
      "cnt: 0 - valLoss: 0.6522548794746399 - trainLoss: 0.652846097946167\n",
      "cnt: 0 - valLoss: 0.6522530317306519 - trainLoss: 0.6528445482254028\n",
      "cnt: 0 - valLoss: 0.6522512435913086 - trainLoss: 0.6528428792953491\n",
      "cnt: 0 - valLoss: 0.6522494554519653 - trainLoss: 0.6528413891792297\n",
      "cnt: 0 - valLoss: 0.6522476077079773 - trainLoss: 0.6528397798538208\n",
      "cnt: 0 - valLoss: 0.6522458791732788 - trainLoss: 0.6528381705284119\n",
      "cnt: 0 - valLoss: 0.6522440314292908 - trainLoss: 0.6528366208076477\n",
      "cnt: 0 - valLoss: 0.6522422432899475 - trainLoss: 0.6528350114822388\n",
      "cnt: 0 - valLoss: 0.6522404551506042 - trainLoss: 0.6528334617614746\n",
      "cnt: 0 - valLoss: 0.6522387862205505 - trainLoss: 0.6528318524360657\n",
      "cnt: 0 - valLoss: 0.6522369980812073 - trainLoss: 0.6528303027153015\n",
      "cnt: 0 - valLoss: 0.652235209941864 - trainLoss: 0.6528287529945374\n",
      "cnt: 0 - valLoss: 0.652233362197876 - trainLoss: 0.6528271436691284\n",
      "cnt: 0 - valLoss: 0.6522315740585327 - trainLoss: 0.6528255939483643\n",
      "cnt: 0 - valLoss: 0.6522297859191895 - trainLoss: 0.6528239846229553\n",
      "cnt: 0 - valLoss: 0.6522279977798462 - trainLoss: 0.6528224349021912\n",
      "cnt: 0 - valLoss: 0.6522262096405029 - trainLoss: 0.652820885181427\n",
      "cnt: 0 - valLoss: 0.6522243618965149 - trainLoss: 0.6528192758560181\n",
      "cnt: 0 - valLoss: 0.6522225141525269 - trainLoss: 0.6528176665306091\n",
      "cnt: 0 - valLoss: 0.6522207260131836 - trainLoss: 0.6528160572052002\n",
      "cnt: 0 - valLoss: 0.6522189378738403 - trainLoss: 0.652814507484436\n",
      "cnt: 0 - valLoss: 0.6522171497344971 - trainLoss: 0.6528129577636719\n",
      "cnt: 0 - valLoss: 0.6522154211997986 - trainLoss: 0.6528114080429077\n",
      "cnt: 0 - valLoss: 0.6522136926651001 - trainLoss: 0.6528098583221436\n",
      "cnt: 0 - valLoss: 0.6522118449211121 - trainLoss: 0.6528082489967346\n",
      "cnt: 0 - valLoss: 0.652209997177124 - trainLoss: 0.6528066992759705\n",
      "cnt: 0 - valLoss: 0.6522082090377808 - trainLoss: 0.6528050899505615\n",
      "cnt: 0 - valLoss: 0.6522064805030823 - trainLoss: 0.6528034806251526\n",
      "cnt: 0 - valLoss: 0.6522046327590942 - trainLoss: 0.6528018712997437\n",
      "cnt: 0 - valLoss: 0.652202844619751 - trainLoss: 0.6528002619743347\n",
      "cnt: 0 - valLoss: 0.6522009968757629 - trainLoss: 0.6527987122535706\n",
      "cnt: 0 - valLoss: 0.6521991491317749 - trainLoss: 0.6527971625328064\n",
      "cnt: 0 - valLoss: 0.6521974205970764 - trainLoss: 0.6527955532073975\n",
      "cnt: 0 - valLoss: 0.6521956324577332 - trainLoss: 0.6527939438819885\n",
      "cnt: 0 - valLoss: 0.6521939635276794 - trainLoss: 0.6527923941612244\n",
      "cnt: 0 - valLoss: 0.6521921753883362 - trainLoss: 0.6527908444404602\n",
      "cnt: 0 - valLoss: 0.6521903872489929 - trainLoss: 0.6527892351150513\n",
      "cnt: 0 - valLoss: 0.6521884799003601 - trainLoss: 0.6527876853942871\n",
      "cnt: 0 - valLoss: 0.6521866917610168 - trainLoss: 0.6527860760688782\n",
      "cnt: 0 - valLoss: 0.6521849036216736 - trainLoss: 0.652784526348114\n",
      "cnt: 0 - valLoss: 0.6521831750869751 - trainLoss: 0.6527829170227051\n",
      "cnt: 0 - valLoss: 0.6521813273429871 - trainLoss: 0.6527813673019409\n",
      "cnt: 0 - valLoss: 0.6521795392036438 - trainLoss: 0.6527796983718872\n",
      "cnt: 0 - valLoss: 0.6521776914596558 - trainLoss: 0.6527780890464783\n",
      "cnt: 0 - valLoss: 0.6521759033203125 - trainLoss: 0.6527765393257141\n",
      "cnt: 0 - valLoss: 0.6521741151809692 - trainLoss: 0.6527750492095947\n",
      "cnt: 0 - valLoss: 0.652172327041626 - trainLoss: 0.6527733206748962\n",
      "cnt: 0 - valLoss: 0.6521705985069275 - trainLoss: 0.6527718305587769\n",
      "cnt: 0 - valLoss: 0.6521688103675842 - trainLoss: 0.6527702808380127\n",
      "cnt: 0 - valLoss: 0.6521669626235962 - trainLoss: 0.6527686715126038\n",
      "cnt: 0 - valLoss: 0.6521651744842529 - trainLoss: 0.6527671217918396\n",
      "cnt: 0 - valLoss: 0.6521633863449097 - trainLoss: 0.6527655124664307\n",
      "cnt: 0 - valLoss: 0.6521615386009216 - trainLoss: 0.6527639031410217\n",
      "cnt: 0 - valLoss: 0.6521598100662231 - trainLoss: 0.6527622938156128\n",
      "cnt: 0 - valLoss: 0.6521579623222351 - trainLoss: 0.6527607440948486\n",
      "cnt: 0 - valLoss: 0.6521561741828918 - trainLoss: 0.6527591347694397\n",
      "cnt: 0 - valLoss: 0.6521543264389038 - trainLoss: 0.6527575850486755\n",
      "cnt: 0 - valLoss: 0.6521524786949158 - trainLoss: 0.6527559161186218\n",
      "cnt: 0 - valLoss: 0.6521508097648621 - trainLoss: 0.6527543663978577\n",
      "cnt: 0 - valLoss: 0.6521490812301636 - trainLoss: 0.6527527570724487\n",
      "cnt: 0 - valLoss: 0.6521472334861755 - trainLoss: 0.6527512073516846\n",
      "cnt: 0 - valLoss: 0.6521454453468323 - trainLoss: 0.6527496576309204\n",
      "cnt: 0 - valLoss: 0.652143657207489 - trainLoss: 0.6527480483055115\n",
      "cnt: 0 - valLoss: 0.652141809463501 - trainLoss: 0.6527464985847473\n",
      "cnt: 0 - valLoss: 0.6521399617195129 - trainLoss: 0.6527448892593384\n",
      "cnt: 0 - valLoss: 0.6521381735801697 - trainLoss: 0.6527432799339294\n",
      "cnt: 0 - valLoss: 0.6521363258361816 - trainLoss: 0.6527416706085205\n",
      "cnt: 0 - valLoss: 0.6521345376968384 - trainLoss: 0.6527401208877563\n",
      "cnt: 0 - valLoss: 0.6521326899528503 - trainLoss: 0.6527384519577026\n",
      "cnt: 0 - valLoss: 0.6521309614181519 - trainLoss: 0.6527369022369385\n",
      "cnt: 0 - valLoss: 0.6521291136741638 - trainLoss: 0.6527352929115295\n",
      "cnt: 0 - valLoss: 0.6521275043487549 - trainLoss: 0.6527337431907654\n",
      "cnt: 0 - valLoss: 0.6521256566047668 - trainLoss: 0.6527321934700012\n",
      "cnt: 0 - valLoss: 0.6521238684654236 - trainLoss: 0.6527305245399475\n",
      "cnt: 0 - valLoss: 0.6521220803260803 - trainLoss: 0.6527289748191833\n",
      "cnt: 0 - valLoss: 0.6521202921867371 - trainLoss: 0.6527273654937744\n",
      "cnt: 0 - valLoss: 0.652118444442749 - trainLoss: 0.6527258157730103\n",
      "cnt: 0 - valLoss: 0.6521166563034058 - trainLoss: 0.6527242064476013\n",
      "cnt: 0 - valLoss: 0.6521148681640625 - trainLoss: 0.6527225971221924\n",
      "cnt: 0 - valLoss: 0.652113139629364 - trainLoss: 0.6527209877967834\n",
      "cnt: 0 - valLoss: 0.652111291885376 - trainLoss: 0.6527193784713745\n",
      "cnt: 0 - valLoss: 0.6521095037460327 - trainLoss: 0.6527177691459656\n",
      "cnt: 0 - valLoss: 0.6521076560020447 - trainLoss: 0.6527162194252014\n",
      "cnt: 0 - valLoss: 0.652105987071991 - trainLoss: 0.6527146100997925\n",
      "cnt: 0 - valLoss: 0.6521042585372925 - trainLoss: 0.6527130603790283\n",
      "cnt: 0 - valLoss: 0.6521023511886597 - trainLoss: 0.6527114510536194\n",
      "cnt: 0 - valLoss: 0.6521006226539612 - trainLoss: 0.6527099013328552\n",
      "cnt: 0 - valLoss: 0.6520988345146179 - trainLoss: 0.6527083516120911\n",
      "cnt: 0 - valLoss: 0.6520970463752747 - trainLoss: 0.6527066826820374\n",
      "cnt: 0 - valLoss: 0.6520952582359314 - trainLoss: 0.6527050733566284\n",
      "cnt: 0 - valLoss: 0.6520934104919434 - trainLoss: 0.6527035236358643\n",
      "cnt: 0 - valLoss: 0.6520916223526001 - trainLoss: 0.6527018547058105\n",
      "cnt: 0 - valLoss: 0.6520897746086121 - trainLoss: 0.6527002453804016\n",
      "cnt: 0 - valLoss: 0.652087926864624 - trainLoss: 0.6526986360549927\n",
      "cnt: 0 - valLoss: 0.6520861983299255 - trainLoss: 0.6526970863342285\n",
      "cnt: 0 - valLoss: 0.6520844101905823 - trainLoss: 0.6526954770088196\n",
      "cnt: 0 - valLoss: 0.6520827412605286 - trainLoss: 0.6526939272880554\n",
      "cnt: 0 - valLoss: 0.6520809531211853 - trainLoss: 0.6526923179626465\n",
      "cnt: 0 - valLoss: 0.6520791053771973 - trainLoss: 0.6526907086372375\n",
      "cnt: 0 - valLoss: 0.6520772576332092 - trainLoss: 0.6526892185211182\n",
      "cnt: 0 - valLoss: 0.6520755290985107 - trainLoss: 0.6526875495910645\n",
      "cnt: 0 - valLoss: 0.6520737409591675 - trainLoss: 0.6526859402656555\n",
      "cnt: 0 - valLoss: 0.6520718932151794 - trainLoss: 0.6526843309402466\n",
      "cnt: 0 - valLoss: 0.6520701050758362 - trainLoss: 0.6526827216148376\n",
      "cnt: 0 - valLoss: 0.6520682573318481 - trainLoss: 0.6526811122894287\n",
      "cnt: 0 - valLoss: 0.6520664691925049 - trainLoss: 0.6526795029640198\n",
      "cnt: 0 - valLoss: 0.6520646214485168 - trainLoss: 0.6526779532432556\n",
      "cnt: 0 - valLoss: 0.6520628929138184 - trainLoss: 0.6526764035224915\n",
      "cnt: 0 - valLoss: 0.6520611643791199 - trainLoss: 0.6526747941970825\n",
      "cnt: 0 - valLoss: 0.6520593166351318 - trainLoss: 0.6526733040809631\n",
      "cnt: 0 - valLoss: 0.6520575284957886 - trainLoss: 0.6526716947555542\n",
      "cnt: 0 - valLoss: 0.6520556807518005 - trainLoss: 0.6526700854301453\n",
      "cnt: 0 - valLoss: 0.6520538330078125 - trainLoss: 0.6526685357093811\n",
      "cnt: 0 - valLoss: 0.6520520448684692 - trainLoss: 0.6526669859886169\n",
      "cnt: 0 - valLoss: 0.6520501971244812 - trainLoss: 0.652665376663208\n",
      "cnt: 0 - valLoss: 0.6520484089851379 - trainLoss: 0.6526638269424438\n",
      "cnt: 0 - valLoss: 0.6520465612411499 - trainLoss: 0.6526622176170349\n",
      "cnt: 0 - valLoss: 0.6520447731018066 - trainLoss: 0.652660608291626\n",
      "cnt: 0 - valLoss: 0.6520429253578186 - trainLoss: 0.652658998966217\n",
      "cnt: 0 - valLoss: 0.6520410776138306 - trainLoss: 0.6526575088500977\n",
      "cnt: 0 - valLoss: 0.6520394086837769 - trainLoss: 0.6526558995246887\n",
      "cnt: 0 - valLoss: 0.6520375609397888 - trainLoss: 0.6526543498039246\n",
      "cnt: 0 - valLoss: 0.6520357728004456 - trainLoss: 0.6526528000831604\n",
      "cnt: 0 - valLoss: 0.6520339250564575 - trainLoss: 0.6526511907577515\n",
      "cnt: 0 - valLoss: 0.6520320773124695 - trainLoss: 0.6526495814323425\n",
      "cnt: 0 - valLoss: 0.6520302295684814 - trainLoss: 0.6526480317115784\n",
      "cnt: 0 - valLoss: 0.6520283818244934 - trainLoss: 0.6526464223861694\n",
      "cnt: 0 - valLoss: 0.6520265936851501 - trainLoss: 0.6526448726654053\n",
      "cnt: 0 - valLoss: 0.6520247459411621 - trainLoss: 0.6526432633399963\n",
      "cnt: 0 - valLoss: 0.6520228981971741 - trainLoss: 0.6526417136192322\n",
      "cnt: 0 - valLoss: 0.6520211100578308 - trainLoss: 0.6526400446891785\n",
      "cnt: 0 - valLoss: 0.652019202709198 - trainLoss: 0.6526384949684143\n",
      "cnt: 0 - valLoss: 0.6520174741744995 - trainLoss: 0.6526368856430054\n",
      "cnt: 0 - valLoss: 0.652015745639801 - trainLoss: 0.652635395526886\n",
      "cnt: 0 - valLoss: 0.6520139575004578 - trainLoss: 0.6526338458061218\n",
      "cnt: 0 - valLoss: 0.6520121097564697 - trainLoss: 0.6526322364807129\n",
      "cnt: 0 - valLoss: 0.6520102620124817 - trainLoss: 0.652630627155304\n",
      "cnt: 0 - valLoss: 0.6520084142684937 - trainLoss: 0.6526290774345398\n",
      "cnt: 0 - valLoss: 0.6520066261291504 - trainLoss: 0.6526274681091309\n",
      "cnt: 0 - valLoss: 0.6520047783851624 - trainLoss: 0.6526259183883667\n",
      "cnt: 0 - valLoss: 0.6520029306411743 - trainLoss: 0.652624249458313\n",
      "cnt: 0 - valLoss: 0.652001142501831 - trainLoss: 0.6526226997375488\n",
      "cnt: 0 - valLoss: 0.6519992351531982 - trainLoss: 0.6526210904121399\n",
      "cnt: 0 - valLoss: 0.651997447013855 - trainLoss: 0.6526195406913757\n",
      "cnt: 0 - valLoss: 0.6519955992698669 - trainLoss: 0.6526179313659668\n",
      "cnt: 0 - valLoss: 0.6519939303398132 - trainLoss: 0.6526163816452026\n",
      "cnt: 0 - valLoss: 0.6519920825958252 - trainLoss: 0.6526148319244385\n",
      "cnt: 0 - valLoss: 0.6519902944564819 - trainLoss: 0.6526132225990295\n",
      "cnt: 0 - valLoss: 0.6519885063171387 - trainLoss: 0.6526116728782654\n",
      "cnt: 0 - valLoss: 0.6519867181777954 - trainLoss: 0.6526101231575012\n",
      "cnt: 0 - valLoss: 0.6519848704338074 - trainLoss: 0.6526084542274475\n",
      "cnt: 0 - valLoss: 0.6519830822944641 - trainLoss: 0.6526069045066833\n",
      "cnt: 0 - valLoss: 0.6519812941551208 - trainLoss: 0.6526052951812744\n",
      "cnt: 0 - valLoss: 0.6519795060157776 - trainLoss: 0.6526037454605103\n",
      "cnt: 0 - valLoss: 0.6519775986671448 - trainLoss: 0.6526021361351013\n",
      "cnt: 0 - valLoss: 0.6519758105278015 - trainLoss: 0.6526005268096924\n",
      "cnt: 0 - valLoss: 0.6519740223884583 - trainLoss: 0.6525989174842834\n",
      "cnt: 0 - valLoss: 0.6519723534584045 - trainLoss: 0.6525973677635193\n",
      "cnt: 0 - valLoss: 0.651970624923706 - trainLoss: 0.6525958180427551\n",
      "cnt: 0 - valLoss: 0.651968777179718 - trainLoss: 0.6525942087173462\n",
      "cnt: 0 - valLoss: 0.65196692943573 - trainLoss: 0.6525925993919373\n",
      "cnt: 0 - valLoss: 0.6519651412963867 - trainLoss: 0.6525910496711731\n",
      "cnt: 0 - valLoss: 0.6519633531570435 - trainLoss: 0.6525894403457642\n",
      "cnt: 0 - valLoss: 0.6519615650177002 - trainLoss: 0.652587890625\n",
      "cnt: 0 - valLoss: 0.6519597768783569 - trainLoss: 0.6525862812995911\n",
      "cnt: 0 - valLoss: 0.6519579291343689 - trainLoss: 0.6525846123695374\n",
      "cnt: 0 - valLoss: 0.6519560813903809 - trainLoss: 0.652583122253418\n",
      "cnt: 0 - valLoss: 0.6519542932510376 - trainLoss: 0.6525814533233643\n",
      "cnt: 0 - valLoss: 0.6519525051116943 - trainLoss: 0.6525798439979553\n",
      "cnt: 0 - valLoss: 0.6519508361816406 - trainLoss: 0.6525783538818359\n",
      "cnt: 0 - valLoss: 0.6519491076469421 - trainLoss: 0.652576744556427\n",
      "cnt: 0 - valLoss: 0.6519472599029541 - trainLoss: 0.6525751948356628\n",
      "cnt: 0 - valLoss: 0.6519454717636108 - trainLoss: 0.6525735855102539\n",
      "cnt: 0 - valLoss: 0.651943564414978 - trainLoss: 0.6525720357894897\n",
      "cnt: 0 - valLoss: 0.6519418954849243 - trainLoss: 0.652570366859436\n",
      "cnt: 0 - valLoss: 0.6519399881362915 - trainLoss: 0.6525688171386719\n",
      "cnt: 0 - valLoss: 0.6519381403923035 - trainLoss: 0.6525672078132629\n",
      "cnt: 0 - valLoss: 0.6519363522529602 - trainLoss: 0.6525656580924988\n",
      "cnt: 0 - valLoss: 0.6519345641136169 - trainLoss: 0.6525640487670898\n",
      "cnt: 0 - valLoss: 0.6519327759742737 - trainLoss: 0.6525624394416809\n",
      "cnt: 0 - valLoss: 0.6519309282302856 - trainLoss: 0.652560830116272\n",
      "cnt: 0 - valLoss: 0.6519292593002319 - trainLoss: 0.652559220790863\n",
      "cnt: 0 - valLoss: 0.6519274711608887 - trainLoss: 0.6525577306747437\n",
      "cnt: 0 - valLoss: 0.6519256830215454 - trainLoss: 0.6525561213493347\n",
      "cnt: 0 - valLoss: 0.6519238948822021 - trainLoss: 0.6525545716285706\n",
      "cnt: 0 - valLoss: 0.6519219875335693 - trainLoss: 0.6525529623031616\n",
      "cnt: 0 - valLoss: 0.6519201993942261 - trainLoss: 0.6525513529777527\n",
      "cnt: 0 - valLoss: 0.6519184112548828 - trainLoss: 0.6525497436523438\n",
      "cnt: 0 - valLoss: 0.6519166231155396 - trainLoss: 0.6525481939315796\n",
      "cnt: 0 - valLoss: 0.6519147753715515 - trainLoss: 0.6525465846061707\n",
      "cnt: 0 - valLoss: 0.6519129872322083 - trainLoss: 0.6525449752807617\n",
      "cnt: 0 - valLoss: 0.651911199092865 - trainLoss: 0.6525433659553528\n",
      "cnt: 0 - valLoss: 0.6519094109535217 - trainLoss: 0.6525418162345886\n",
      "cnt: 0 - valLoss: 0.651907742023468 - trainLoss: 0.6525401473045349\n",
      "cnt: 0 - valLoss: 0.65190589427948 - trainLoss: 0.6525386571884155\n",
      "cnt: 0 - valLoss: 0.6519040465354919 - trainLoss: 0.6525371074676514\n",
      "cnt: 0 - valLoss: 0.6519022583961487 - trainLoss: 0.6525354385375977\n",
      "cnt: 0 - valLoss: 0.6519004106521606 - trainLoss: 0.6525338888168335\n",
      "cnt: 0 - valLoss: 0.6518986225128174 - trainLoss: 0.6525322198867798\n",
      "cnt: 0 - valLoss: 0.6518967747688293 - trainLoss: 0.6525307297706604\n",
      "cnt: 0 - valLoss: 0.6518949270248413 - trainLoss: 0.6525290608406067\n",
      "cnt: 0 - valLoss: 0.6518931984901428 - trainLoss: 0.6525274515151978\n",
      "cnt: 0 - valLoss: 0.6518914103507996 - trainLoss: 0.6525259017944336\n",
      "cnt: 0 - valLoss: 0.6518895030021667 - trainLoss: 0.6525242924690247\n",
      "cnt: 0 - valLoss: 0.6518877148628235 - trainLoss: 0.6525226831436157\n",
      "cnt: 0 - valLoss: 0.6518860459327698 - trainLoss: 0.6525211334228516\n",
      "cnt: 0 - valLoss: 0.6518842577934265 - trainLoss: 0.6525195837020874\n",
      "cnt: 0 - valLoss: 0.6518824100494385 - trainLoss: 0.6525179743766785\n",
      "cnt: 0 - valLoss: 0.6518806219100952 - trainLoss: 0.6525163054466248\n",
      "cnt: 0 - valLoss: 0.6518788933753967 - trainLoss: 0.6525147557258606\n",
      "cnt: 0 - valLoss: 0.6518770456314087 - trainLoss: 0.6525132060050964\n",
      "cnt: 0 - valLoss: 0.6518751978874207 - trainLoss: 0.6525115966796875\n",
      "cnt: 0 - valLoss: 0.6518734097480774 - trainLoss: 0.6525099873542786\n",
      "cnt: 0 - valLoss: 0.6518715620040894 - trainLoss: 0.6525083780288696\n",
      "cnt: 0 - valLoss: 0.6518697142601013 - trainLoss: 0.6525067687034607\n",
      "cnt: 0 - valLoss: 0.6518679261207581 - trainLoss: 0.6525052189826965\n",
      "cnt: 0 - valLoss: 0.6518661975860596 - trainLoss: 0.6525035500526428\n",
      "cnt: 0 - valLoss: 0.6518644690513611 - trainLoss: 0.6525019407272339\n",
      "cnt: 0 - valLoss: 0.6518626809120178 - trainLoss: 0.6525003910064697\n",
      "cnt: 0 - valLoss: 0.6518608927726746 - trainLoss: 0.6524988412857056\n",
      "cnt: 0 - valLoss: 0.6518590450286865 - trainLoss: 0.6524972319602966\n",
      "cnt: 0 - valLoss: 0.6518571972846985 - trainLoss: 0.6524956226348877\n",
      "cnt: 0 - valLoss: 0.6518554091453552 - trainLoss: 0.6524940133094788\n",
      "cnt: 0 - valLoss: 0.651853621006012 - trainLoss: 0.6524924635887146\n",
      "cnt: 0 - valLoss: 0.6518517732620239 - trainLoss: 0.6524907946586609\n",
      "cnt: 0 - valLoss: 0.6518499255180359 - trainLoss: 0.6524892449378967\n",
      "cnt: 0 - valLoss: 0.6518481373786926 - trainLoss: 0.6524876356124878\n",
      "cnt: 0 - valLoss: 0.6518463492393494 - trainLoss: 0.6524860262870789\n",
      "cnt: 0 - valLoss: 0.6518445611000061 - trainLoss: 0.6524844169616699\n",
      "cnt: 0 - valLoss: 0.6518428325653076 - trainLoss: 0.6524828672409058\n",
      "cnt: 0 - valLoss: 0.6518410444259644 - trainLoss: 0.6524812579154968\n",
      "cnt: 0 - valLoss: 0.6518391966819763 - trainLoss: 0.6524796485900879\n",
      "cnt: 0 - valLoss: 0.6518374085426331 - trainLoss: 0.652478039264679\n",
      "cnt: 0 - valLoss: 0.651835560798645 - trainLoss: 0.65247642993927\n",
      "cnt: 0 - valLoss: 0.651833713054657 - trainLoss: 0.6524748802185059\n",
      "cnt: 0 - valLoss: 0.6518319249153137 - trainLoss: 0.6524732708930969\n",
      "cnt: 0 - valLoss: 0.6518300771713257 - trainLoss: 0.652471661567688\n",
      "cnt: 0 - valLoss: 0.6518282890319824 - trainLoss: 0.652470052242279\n",
      "cnt: 0 - valLoss: 0.6518265008926392 - trainLoss: 0.6524684429168701\n",
      "cnt: 0 - valLoss: 0.6518247127532959 - trainLoss: 0.6524668335914612\n",
      "cnt: 0 - valLoss: 0.6518229246139526 - trainLoss: 0.6524652242660522\n",
      "cnt: 0 - valLoss: 0.6518211960792542 - trainLoss: 0.6524636745452881\n",
      "cnt: 0 - valLoss: 0.6518194079399109 - trainLoss: 0.6524621248245239\n",
      "cnt: 0 - valLoss: 0.6518175601959229 - trainLoss: 0.6524604558944702\n",
      "cnt: 0 - valLoss: 0.6518157124519348 - trainLoss: 0.652458906173706\n",
      "cnt: 0 - valLoss: 0.6518138647079468 - trainLoss: 0.6524572372436523\n",
      "cnt: 0 - valLoss: 0.6518120765686035 - trainLoss: 0.6524556279182434\n",
      "cnt: 0 - valLoss: 0.6518102884292603 - trainLoss: 0.6524540781974792\n",
      "cnt: 0 - valLoss: 0.6518083810806274 - trainLoss: 0.6524524092674255\n",
      "cnt: 0 - valLoss: 0.651806652545929 - trainLoss: 0.6524508595466614\n",
      "cnt: 0 - valLoss: 0.6518048048019409 - trainLoss: 0.6524491906166077\n",
      "cnt: 0 - valLoss: 0.6518030762672424 - trainLoss: 0.6524475812911987\n",
      "cnt: 0 - valLoss: 0.651801347732544 - trainLoss: 0.6524460315704346\n",
      "cnt: 0 - valLoss: 0.6517994999885559 - trainLoss: 0.6524444818496704\n",
      "cnt: 0 - valLoss: 0.6517977118492126 - trainLoss: 0.6524428129196167\n",
      "cnt: 0 - valLoss: 0.6517958045005798 - trainLoss: 0.6524412631988525\n",
      "cnt: 0 - valLoss: 0.6517940759658813 - trainLoss: 0.6524396538734436\n",
      "cnt: 0 - valLoss: 0.6517922282218933 - trainLoss: 0.6524379849433899\n",
      "cnt: 0 - valLoss: 0.6517903804779053 - trainLoss: 0.652436375617981\n",
      "cnt: 0 - valLoss: 0.6517885327339172 - trainLoss: 0.6524348258972168\n",
      "cnt: 0 - valLoss: 0.651786744594574 - trainLoss: 0.6524331569671631\n",
      "cnt: 0 - valLoss: 0.6517848968505859 - trainLoss: 0.6524316072463989\n",
      "cnt: 0 - valLoss: 0.6517831087112427 - trainLoss: 0.6524299383163452\n",
      "cnt: 0 - valLoss: 0.6517813801765442 - trainLoss: 0.652428388595581\n",
      "cnt: 0 - valLoss: 0.6517795920372009 - trainLoss: 0.6524267196655273\n",
      "cnt: 0 - valLoss: 0.6517777442932129 - trainLoss: 0.6524251699447632\n",
      "cnt: 0 - valLoss: 0.6517760157585144 - trainLoss: 0.6524235606193542\n",
      "cnt: 0 - valLoss: 0.6517741084098816 - trainLoss: 0.6524218916893005\n",
      "cnt: 0 - valLoss: 0.6517722606658936 - trainLoss: 0.6524203419685364\n",
      "cnt: 0 - valLoss: 0.6517704725265503 - trainLoss: 0.6524187326431274\n",
      "cnt: 0 - valLoss: 0.6517686247825623 - trainLoss: 0.6524171233177185\n",
      "cnt: 0 - valLoss: 0.651766836643219 - trainLoss: 0.6524155139923096\n",
      "cnt: 0 - valLoss: 0.6517650485038757 - trainLoss: 0.6524138450622559\n",
      "cnt: 0 - valLoss: 0.6517632007598877 - trainLoss: 0.6524122953414917\n",
      "cnt: 0 - valLoss: 0.6517614126205444 - trainLoss: 0.652410626411438\n",
      "cnt: 0 - valLoss: 0.651759684085846 - trainLoss: 0.6524090766906738\n",
      "cnt: 0 - valLoss: 0.6517578363418579 - trainLoss: 0.6524074673652649\n",
      "cnt: 0 - valLoss: 0.6517559885978699 - trainLoss: 0.652405858039856\n",
      "cnt: 0 - valLoss: 0.6517541408538818 - trainLoss: 0.652404248714447\n",
      "cnt: 0 - valLoss: 0.6517524123191833 - trainLoss: 0.6524026393890381\n",
      "cnt: 0 - valLoss: 0.6517505645751953 - trainLoss: 0.6524010300636292\n",
      "cnt: 0 - valLoss: 0.6517486572265625 - trainLoss: 0.6523994207382202\n",
      "cnt: 0 - valLoss: 0.6517468690872192 - trainLoss: 0.6523977518081665\n",
      "cnt: 0 - valLoss: 0.651745080947876 - trainLoss: 0.6523961424827576\n",
      "cnt: 0 - valLoss: 0.6517432332038879 - trainLoss: 0.6523945331573486\n",
      "cnt: 0 - valLoss: 0.6517413854598999 - trainLoss: 0.6523929834365845\n",
      "cnt: 0 - valLoss: 0.6517396569252014 - trainLoss: 0.6523913741111755\n",
      "cnt: 0 - valLoss: 0.6517378687858582 - trainLoss: 0.6523897647857666\n",
      "cnt: 0 - valLoss: 0.6517360806465149 - trainLoss: 0.6523881554603577\n",
      "cnt: 0 - valLoss: 0.6517342329025269 - trainLoss: 0.6523865461349487\n",
      "cnt: 0 - valLoss: 0.6517324447631836 - trainLoss: 0.6523849368095398\n",
      "cnt: 0 - valLoss: 0.6517305970191956 - trainLoss: 0.6523833274841309\n",
      "cnt: 0 - valLoss: 0.6517287492752075 - trainLoss: 0.6523816585540771\n",
      "cnt: 0 - valLoss: 0.6517269015312195 - trainLoss: 0.6523800492286682\n",
      "cnt: 0 - valLoss: 0.6517250537872314 - trainLoss: 0.6523784399032593\n",
      "cnt: 0 - valLoss: 0.6517232656478882 - trainLoss: 0.6523767709732056\n",
      "cnt: 0 - valLoss: 0.6517214775085449 - trainLoss: 0.6523751616477966\n",
      "cnt: 0 - valLoss: 0.6517196893692017 - trainLoss: 0.6523735523223877\n",
      "cnt: 0 - valLoss: 0.6517179012298584 - trainLoss: 0.6523720026016235\n",
      "cnt: 0 - valLoss: 0.6517160534858704 - trainLoss: 0.6523703932762146\n",
      "cnt: 0 - valLoss: 0.6517142653465271 - trainLoss: 0.6523687243461609\n",
      "cnt: 0 - valLoss: 0.6517124176025391 - trainLoss: 0.6523672342300415\n",
      "cnt: 0 - valLoss: 0.651710569858551 - trainLoss: 0.6523655652999878\n",
      "cnt: 0 - valLoss: 0.6517087817192078 - trainLoss: 0.6523638963699341\n",
      "cnt: 0 - valLoss: 0.6517069339752197 - trainLoss: 0.6523623466491699\n",
      "cnt: 0 - valLoss: 0.6517050862312317 - trainLoss: 0.6523606777191162\n",
      "cnt: 0 - valLoss: 0.6517032980918884 - trainLoss: 0.6523590087890625\n",
      "cnt: 0 - valLoss: 0.6517014503479004 - trainLoss: 0.6523573994636536\n",
      "cnt: 0 - valLoss: 0.6516996622085571 - trainLoss: 0.6523558497428894\n",
      "cnt: 0 - valLoss: 0.6516979336738586 - trainLoss: 0.6523542404174805\n",
      "cnt: 0 - valLoss: 0.6516960859298706 - trainLoss: 0.6523526310920715\n",
      "cnt: 0 - valLoss: 0.6516942381858826 - trainLoss: 0.6523510217666626\n",
      "cnt: 0 - valLoss: 0.6516924500465393 - trainLoss: 0.6523494124412537\n",
      "cnt: 0 - valLoss: 0.6516906023025513 - trainLoss: 0.6523478031158447\n",
      "cnt: 0 - valLoss: 0.651688814163208 - trainLoss: 0.652346134185791\n",
      "cnt: 0 - valLoss: 0.65168696641922 - trainLoss: 0.6523445248603821\n",
      "cnt: 0 - valLoss: 0.6516851186752319 - trainLoss: 0.6523429155349731\n",
      "cnt: 0 - valLoss: 0.6516833305358887 - trainLoss: 0.6523412466049194\n",
      "cnt: 0 - valLoss: 0.6516814827919006 - trainLoss: 0.6523396372795105\n",
      "cnt: 0 - valLoss: 0.6516797542572021 - trainLoss: 0.6523380279541016\n",
      "cnt: 0 - valLoss: 0.6516780853271484 - trainLoss: 0.6523363590240479\n",
      "cnt: 0 - valLoss: 0.6516762375831604 - trainLoss: 0.6523348689079285\n",
      "cnt: 0 - valLoss: 0.6516743898391724 - trainLoss: 0.6523331999778748\n",
      "cnt: 0 - valLoss: 0.6516725420951843 - trainLoss: 0.6523316502571106\n",
      "cnt: 0 - valLoss: 0.6516707539558411 - trainLoss: 0.6523299813270569\n",
      "cnt: 0 - valLoss: 0.651668906211853 - trainLoss: 0.652328372001648\n",
      "cnt: 0 - valLoss: 0.6516671180725098 - trainLoss: 0.6523267030715942\n",
      "cnt: 0 - valLoss: 0.6516653299331665 - trainLoss: 0.6523251533508301\n",
      "cnt: 0 - valLoss: 0.6516634225845337 - trainLoss: 0.6523234844207764\n",
      "cnt: 0 - valLoss: 0.6516616344451904 - trainLoss: 0.6523218750953674\n",
      "cnt: 0 - valLoss: 0.6516597867012024 - trainLoss: 0.6523202657699585\n",
      "cnt: 0 - valLoss: 0.6516581177711487 - trainLoss: 0.6523186564445496\n",
      "cnt: 0 - valLoss: 0.6516563892364502 - trainLoss: 0.6523170471191406\n",
      "cnt: 0 - valLoss: 0.6516546010971069 - trainLoss: 0.6523153781890869\n",
      "cnt: 0 - valLoss: 0.6516527533531189 - trainLoss: 0.6523138284683228\n",
      "cnt: 0 - valLoss: 0.6516509056091309 - trainLoss: 0.652312159538269\n",
      "cnt: 0 - valLoss: 0.6516491770744324 - trainLoss: 0.6523104310035706\n",
      "cnt: 0 - valLoss: 0.6516473889350891 - trainLoss: 0.6523088812828064\n",
      "cnt: 0 - valLoss: 0.6516455411911011 - trainLoss: 0.6523072123527527\n",
      "cnt: 0 - valLoss: 0.651643693447113 - trainLoss: 0.652305543422699\n",
      "cnt: 0 - valLoss: 0.651641845703125 - trainLoss: 0.65230393409729\n",
      "cnt: 0 - valLoss: 0.6516401171684265 - trainLoss: 0.6523023247718811\n",
      "cnt: 0 - valLoss: 0.651638388633728 - trainLoss: 0.6523007154464722\n",
      "cnt: 0 - valLoss: 0.6516366004943848 - trainLoss: 0.652299165725708\n",
      "cnt: 0 - valLoss: 0.6516347527503967 - trainLoss: 0.6522974967956543\n",
      "cnt: 0 - valLoss: 0.6516329050064087 - trainLoss: 0.6522958874702454\n",
      "cnt: 0 - valLoss: 0.651631236076355 - trainLoss: 0.6522941589355469\n",
      "cnt: 0 - valLoss: 0.6516293287277222 - trainLoss: 0.6522925496101379\n",
      "cnt: 0 - valLoss: 0.6516275405883789 - trainLoss: 0.652290940284729\n",
      "cnt: 0 - valLoss: 0.6516257524490356 - trainLoss: 0.6522893309593201\n",
      "cnt: 0 - valLoss: 0.6516239047050476 - trainLoss: 0.6522876620292664\n",
      "cnt: 0 - valLoss: 0.6516220569610596 - trainLoss: 0.6522860527038574\n",
      "cnt: 0 - valLoss: 0.6516203284263611 - trainLoss: 0.6522844433784485\n",
      "cnt: 0 - valLoss: 0.6516185998916626 - trainLoss: 0.6522827744483948\n",
      "cnt: 0 - valLoss: 0.6516168117523193 - trainLoss: 0.6522812247276306\n",
      "cnt: 0 - valLoss: 0.6516149640083313 - trainLoss: 0.6522795557975769\n",
      "cnt: 0 - valLoss: 0.651613175868988 - trainLoss: 0.652277946472168\n",
      "cnt: 0 - valLoss: 0.651611328125 - trainLoss: 0.6522762775421143\n",
      "cnt: 0 - valLoss: 0.651609480381012 - trainLoss: 0.6522746682167053\n",
      "cnt: 0 - valLoss: 0.6516077518463135 - trainLoss: 0.6522729992866516\n",
      "cnt: 0 - valLoss: 0.6516059041023254 - trainLoss: 0.6522713899612427\n",
      "cnt: 0 - valLoss: 0.6516040563583374 - trainLoss: 0.652269721031189\n",
      "cnt: 0 - valLoss: 0.6516022086143494 - trainLoss: 0.6522680521011353\n",
      "cnt: 0 - valLoss: 0.6516004204750061 - trainLoss: 0.6522664427757263\n",
      "cnt: 0 - valLoss: 0.6515988111495972 - trainLoss: 0.6522648334503174\n",
      "cnt: 0 - valLoss: 0.6515969038009644 - trainLoss: 0.6522632837295532\n",
      "cnt: 0 - valLoss: 0.6515951156616211 - trainLoss: 0.6522616147994995\n",
      "cnt: 0 - valLoss: 0.6515932679176331 - trainLoss: 0.6522599458694458\n",
      "cnt: 0 - valLoss: 0.6515915393829346 - trainLoss: 0.6522583365440369\n",
      "cnt: 0 - valLoss: 0.6515896320343018 - trainLoss: 0.6522566676139832\n",
      "cnt: 0 - valLoss: 0.6515878438949585 - trainLoss: 0.6522549986839294\n",
      "cnt: 0 - valLoss: 0.6515860557556152 - trainLoss: 0.6522533297538757\n",
      "cnt: 0 - valLoss: 0.6515841484069824 - trainLoss: 0.6522517204284668\n",
      "cnt: 0 - valLoss: 0.6515824198722839 - trainLoss: 0.6522501111030579\n",
      "cnt: 0 - valLoss: 0.6515806317329407 - trainLoss: 0.6522485017776489\n",
      "cnt: 0 - valLoss: 0.6515789031982422 - trainLoss: 0.6522468328475952\n",
      "cnt: 0 - valLoss: 0.6515770554542542 - trainLoss: 0.652245283126831\n",
      "cnt: 0 - valLoss: 0.6515752673149109 - trainLoss: 0.6522436141967773\n",
      "cnt: 0 - valLoss: 0.6515734195709229 - trainLoss: 0.6522418856620789\n",
      "cnt: 0 - valLoss: 0.6515716314315796 - trainLoss: 0.6522402763366699\n",
      "cnt: 0 - valLoss: 0.6515698432922363 - trainLoss: 0.6522387266159058\n",
      "cnt: 0 - valLoss: 0.6515679359436035 - trainLoss: 0.6522369980812073\n",
      "cnt: 0 - valLoss: 0.6515661478042603 - trainLoss: 0.6522353887557983\n",
      "cnt: 0 - valLoss: 0.6515643000602722 - trainLoss: 0.6522337198257446\n",
      "cnt: 0 - valLoss: 0.651562511920929 - trainLoss: 0.6522320508956909\n",
      "cnt: 0 - valLoss: 0.6515607237815857 - trainLoss: 0.6522303819656372\n",
      "cnt: 0 - valLoss: 0.651559054851532 - trainLoss: 0.652228832244873\n",
      "cnt: 0 - valLoss: 0.651557207107544 - trainLoss: 0.6522272229194641\n",
      "cnt: 0 - valLoss: 0.6515553593635559 - trainLoss: 0.6522255539894104\n",
      "cnt: 0 - valLoss: 0.6515535712242126 - trainLoss: 0.6522239446640015\n",
      "cnt: 0 - valLoss: 0.6515517234802246 - trainLoss: 0.6522222757339478\n",
      "cnt: 0 - valLoss: 0.6515498161315918 - trainLoss: 0.6522206664085388\n",
      "cnt: 0 - valLoss: 0.651547908782959 - trainLoss: 0.6522189974784851\n",
      "cnt: 0 - valLoss: 0.6515461802482605 - trainLoss: 0.6522173285484314\n",
      "cnt: 0 - valLoss: 0.6515443325042725 - trainLoss: 0.6522157192230225\n",
      "cnt: 0 - valLoss: 0.6515424847602844 - trainLoss: 0.6522140502929688\n",
      "cnt: 0 - valLoss: 0.6515407562255859 - trainLoss: 0.6522124409675598\n",
      "cnt: 0 - valLoss: 0.6515389680862427 - trainLoss: 0.6522108316421509\n",
      "cnt: 0 - valLoss: 0.6515371203422546 - trainLoss: 0.6522092223167419\n",
      "cnt: 0 - valLoss: 0.6515352725982666 - trainLoss: 0.652207612991333\n",
      "cnt: 0 - valLoss: 0.6515334248542786 - trainLoss: 0.6522059440612793\n",
      "cnt: 0 - valLoss: 0.6515315175056458 - trainLoss: 0.6522043347358704\n",
      "cnt: 0 - valLoss: 0.6515296697616577 - trainLoss: 0.6522027254104614\n",
      "cnt: 0 - valLoss: 0.6515277624130249 - trainLoss: 0.6522011160850525\n",
      "cnt: 0 - valLoss: 0.6515259742736816 - trainLoss: 0.6521994471549988\n",
      "cnt: 0 - valLoss: 0.6515240669250488 - trainLoss: 0.6521977782249451\n",
      "cnt: 0 - valLoss: 0.6515222787857056 - trainLoss: 0.6521961688995361\n",
      "cnt: 0 - valLoss: 0.6515205502510071 - trainLoss: 0.6521945595741272\n",
      "cnt: 0 - valLoss: 0.6515186429023743 - trainLoss: 0.652193009853363\n",
      "cnt: 0 - valLoss: 0.6515167951583862 - trainLoss: 0.6521913409233093\n",
      "cnt: 0 - valLoss: 0.6515149474143982 - trainLoss: 0.6521897315979004\n",
      "cnt: 0 - valLoss: 0.6515130996704102 - trainLoss: 0.6521881222724915\n",
      "cnt: 0 - valLoss: 0.6515111923217773 - trainLoss: 0.6521864533424377\n",
      "cnt: 0 - valLoss: 0.6515093445777893 - trainLoss: 0.6521848440170288\n",
      "cnt: 0 - valLoss: 0.6515074968338013 - trainLoss: 0.6521832346916199\n",
      "cnt: 0 - valLoss: 0.6515056490898132 - trainLoss: 0.6521816253662109\n",
      "cnt: 0 - valLoss: 0.6515037417411804 - trainLoss: 0.6521799564361572\n",
      "cnt: 0 - valLoss: 0.6515019536018372 - trainLoss: 0.6521782875061035\n",
      "cnt: 0 - valLoss: 0.6515001654624939 - trainLoss: 0.6521767377853394\n",
      "cnt: 0 - valLoss: 0.6514983177185059 - trainLoss: 0.6521751284599304\n",
      "cnt: 0 - valLoss: 0.6514964699745178 - trainLoss: 0.6521734595298767\n",
      "cnt: 0 - valLoss: 0.6514946222305298 - trainLoss: 0.652171790599823\n",
      "cnt: 0 - valLoss: 0.651492714881897 - trainLoss: 0.6521702408790588\n",
      "cnt: 0 - valLoss: 0.6514908671379089 - trainLoss: 0.6521685719490051\n",
      "cnt: 0 - valLoss: 0.6514890193939209 - trainLoss: 0.6521669626235962\n",
      "cnt: 0 - valLoss: 0.6514871120452881 - trainLoss: 0.6521652936935425\n",
      "cnt: 0 - valLoss: 0.6514853239059448 - trainLoss: 0.6521636843681335\n",
      "cnt: 0 - valLoss: 0.6514833569526672 - trainLoss: 0.6521620750427246\n",
      "cnt: 0 - valLoss: 0.6514816284179688 - trainLoss: 0.6521604061126709\n",
      "cnt: 0 - valLoss: 0.6514798998832703 - trainLoss: 0.6521588563919067\n",
      "cnt: 0 - valLoss: 0.6514779329299927 - trainLoss: 0.6521572470664978\n",
      "cnt: 0 - valLoss: 0.6514761447906494 - trainLoss: 0.6521555185317993\n",
      "cnt: 0 - valLoss: 0.6514742374420166 - trainLoss: 0.6521539092063904\n",
      "cnt: 0 - valLoss: 0.6514723896980286 - trainLoss: 0.6521522998809814\n",
      "cnt: 0 - valLoss: 0.6514705419540405 - trainLoss: 0.6521506905555725\n",
      "cnt: 0 - valLoss: 0.6514686942100525 - trainLoss: 0.6521490216255188\n",
      "cnt: 0 - valLoss: 0.6514667868614197 - trainLoss: 0.6521473526954651\n",
      "cnt: 0 - valLoss: 0.6514648795127869 - trainLoss: 0.6521457433700562\n",
      "cnt: 0 - valLoss: 0.6514630913734436 - trainLoss: 0.6521441340446472\n",
      "cnt: 0 - valLoss: 0.6514613628387451 - trainLoss: 0.6521425247192383\n",
      "cnt: 0 - valLoss: 0.6514594554901123 - trainLoss: 0.6521409153938293\n",
      "cnt: 0 - valLoss: 0.6514576077461243 - trainLoss: 0.6521393060684204\n",
      "cnt: 0 - valLoss: 0.6514557600021362 - trainLoss: 0.6521376371383667\n",
      "cnt: 0 - valLoss: 0.6514538526535034 - trainLoss: 0.6521360278129578\n",
      "cnt: 0 - valLoss: 0.6514520049095154 - trainLoss: 0.652134358882904\n",
      "cnt: 0 - valLoss: 0.6514500975608826 - trainLoss: 0.6521326899528503\n",
      "cnt: 0 - valLoss: 0.6514483094215393 - trainLoss: 0.6521310806274414\n",
      "cnt: 0 - valLoss: 0.6514464020729065 - trainLoss: 0.6521294713020325\n",
      "cnt: 0 - valLoss: 0.6514444947242737 - trainLoss: 0.6521278023719788\n",
      "cnt: 0 - valLoss: 0.6514427065849304 - trainLoss: 0.652126133441925\n",
      "cnt: 0 - valLoss: 0.6514409184455872 - trainLoss: 0.6521245241165161\n",
      "cnt: 0 - valLoss: 0.6514390707015991 - trainLoss: 0.652122974395752\n",
      "cnt: 0 - valLoss: 0.6514371633529663 - trainLoss: 0.652121365070343\n",
      "cnt: 0 - valLoss: 0.6514353156089783 - trainLoss: 0.6521196961402893\n",
      "cnt: 0 - valLoss: 0.6514334678649902 - trainLoss: 0.6521180272102356\n",
      "cnt: 0 - valLoss: 0.6514316201210022 - trainLoss: 0.6521164178848267\n",
      "cnt: 0 - valLoss: 0.6514297127723694 - trainLoss: 0.652114748954773\n",
      "cnt: 0 - valLoss: 0.6514278650283813 - trainLoss: 0.6521130800247192\n",
      "cnt: 0 - valLoss: 0.6514260172843933 - trainLoss: 0.6521115303039551\n",
      "cnt: 0 - valLoss: 0.6514240503311157 - trainLoss: 0.6521098613739014\n",
      "cnt: 0 - valLoss: 0.651422381401062 - trainLoss: 0.6521081924438477\n",
      "cnt: 0 - valLoss: 0.651420533657074 - trainLoss: 0.6521065831184387\n",
      "cnt: 0 - valLoss: 0.6514186263084412 - trainLoss: 0.652104914188385\n",
      "cnt: 0 - valLoss: 0.6514167785644531 - trainLoss: 0.6521033048629761\n",
      "cnt: 0 - valLoss: 0.6514149308204651 - trainLoss: 0.6521016359329224\n",
      "cnt: 0 - valLoss: 0.6514130234718323 - trainLoss: 0.6521000266075134\n",
      "cnt: 0 - valLoss: 0.6514111161231995 - trainLoss: 0.6520984172821045\n",
      "cnt: 0 - valLoss: 0.6514092683792114 - trainLoss: 0.6520967483520508\n",
      "cnt: 0 - valLoss: 0.6514074206352234 - trainLoss: 0.6520950794219971\n",
      "cnt: 0 - valLoss: 0.6514055132865906 - trainLoss: 0.6520934700965881\n",
      "cnt: 0 - valLoss: 0.6514037251472473 - trainLoss: 0.6520918011665344\n",
      "cnt: 0 - valLoss: 0.6514018774032593 - trainLoss: 0.6520901918411255\n",
      "cnt: 0 - valLoss: 0.6514000296592712 - trainLoss: 0.6520885825157166\n",
      "cnt: 0 - valLoss: 0.6513981819152832 - trainLoss: 0.6520869135856628\n",
      "cnt: 0 - valLoss: 0.6513963341712952 - trainLoss: 0.6520853042602539\n",
      "cnt: 0 - valLoss: 0.6513944268226624 - trainLoss: 0.6520836353302002\n",
      "cnt: 0 - valLoss: 0.6513925790786743 - trainLoss: 0.6520819664001465\n",
      "cnt: 0 - valLoss: 0.6513906717300415 - trainLoss: 0.6520803570747375\n",
      "cnt: 0 - valLoss: 0.6513887643814087 - trainLoss: 0.6520787477493286\n",
      "cnt: 0 - valLoss: 0.6513868570327759 - trainLoss: 0.6520770192146301\n",
      "cnt: 0 - valLoss: 0.6513850092887878 - trainLoss: 0.6520754098892212\n",
      "cnt: 0 - valLoss: 0.6513833403587341 - trainLoss: 0.6520737409591675\n",
      "cnt: 0 - valLoss: 0.6513814330101013 - trainLoss: 0.6520721912384033\n",
      "cnt: 0 - valLoss: 0.6513796448707581 - trainLoss: 0.6520705223083496\n",
      "cnt: 0 - valLoss: 0.6513776779174805 - trainLoss: 0.6520688533782959\n",
      "cnt: 0 - valLoss: 0.6513758301734924 - trainLoss: 0.652067244052887\n",
      "cnt: 0 - valLoss: 0.6513739228248596 - trainLoss: 0.6520655751228333\n",
      "cnt: 0 - valLoss: 0.6513720750808716 - trainLoss: 0.6520639657974243\n",
      "cnt: 0 - valLoss: 0.6513701677322388 - trainLoss: 0.6520622968673706\n",
      "cnt: 0 - valLoss: 0.6513683199882507 - trainLoss: 0.6520606875419617\n",
      "cnt: 0 - valLoss: 0.6513664126396179 - trainLoss: 0.652059018611908\n",
      "cnt: 0 - valLoss: 0.6513646245002747 - trainLoss: 0.6520573496818542\n",
      "cnt: 0 - valLoss: 0.6513628363609314 - trainLoss: 0.6520557403564453\n",
      "cnt: 0 - valLoss: 0.6513609290122986 - trainLoss: 0.6520540714263916\n",
      "cnt: 0 - valLoss: 0.6513590812683105 - trainLoss: 0.6520524621009827\n",
      "cnt: 0 - valLoss: 0.6513571739196777 - trainLoss: 0.652050793170929\n",
      "cnt: 0 - valLoss: 0.6513553857803345 - trainLoss: 0.6520491242408752\n",
      "cnt: 0 - valLoss: 0.6513533592224121 - trainLoss: 0.6520474553108215\n",
      "cnt: 0 - valLoss: 0.6513515114784241 - trainLoss: 0.6520459055900574\n",
      "cnt: 0 - valLoss: 0.651349663734436 - trainLoss: 0.6520441174507141\n",
      "cnt: 0 - valLoss: 0.651347815990448 - trainLoss: 0.6520425081253052\n",
      "cnt: 0 - valLoss: 0.6513459086418152 - trainLoss: 0.6520408987998962\n",
      "cnt: 0 - valLoss: 0.6513442397117615 - trainLoss: 0.6520392298698425\n",
      "cnt: 0 - valLoss: 0.6513422727584839 - trainLoss: 0.6520376205444336\n",
      "cnt: 0 - valLoss: 0.6513404250144958 - trainLoss: 0.6520360112190247\n",
      "cnt: 0 - valLoss: 0.6513385772705078 - trainLoss: 0.652034342288971\n",
      "cnt: 0 - valLoss: 0.651336669921875 - trainLoss: 0.652032732963562\n",
      "cnt: 0 - valLoss: 0.6513347625732422 - trainLoss: 0.6520310640335083\n",
      "cnt: 0 - valLoss: 0.6513329148292542 - trainLoss: 0.6520293951034546\n",
      "cnt: 0 - valLoss: 0.6513310074806213 - trainLoss: 0.6520277261734009\n",
      "cnt: 0 - valLoss: 0.6513291001319885 - trainLoss: 0.6520260572433472\n",
      "cnt: 0 - valLoss: 0.6513271927833557 - trainLoss: 0.6520243883132935\n",
      "cnt: 0 - valLoss: 0.6513254642486572 - trainLoss: 0.6520227789878845\n",
      "cnt: 0 - valLoss: 0.6513236165046692 - trainLoss: 0.6520211696624756\n",
      "cnt: 0 - valLoss: 0.6513217091560364 - trainLoss: 0.6520195007324219\n",
      "cnt: 0 - valLoss: 0.6513198614120483 - trainLoss: 0.6520178318023682\n",
      "cnt: 0 - valLoss: 0.6513179540634155 - trainLoss: 0.6520161628723145\n",
      "cnt: 0 - valLoss: 0.6513160467147827 - trainLoss: 0.6520145535469055\n",
      "cnt: 0 - valLoss: 0.6513141989707947 - trainLoss: 0.6520128846168518\n",
      "cnt: 0 - valLoss: 0.6513123512268066 - trainLoss: 0.6520112156867981\n",
      "cnt: 0 - valLoss: 0.6513104438781738 - trainLoss: 0.6520096659660339\n",
      "cnt: 0 - valLoss: 0.651308536529541 - trainLoss: 0.6520079374313354\n",
      "cnt: 0 - valLoss: 0.651306688785553 - trainLoss: 0.6520062685012817\n",
      "cnt: 0 - valLoss: 0.6513049602508545 - trainLoss: 0.6520046591758728\n",
      "cnt: 0 - valLoss: 0.6513030529022217 - trainLoss: 0.6520030498504639\n",
      "cnt: 0 - valLoss: 0.6513011455535889 - trainLoss: 0.6520013213157654\n",
      "cnt: 0 - valLoss: 0.6512992978096008 - trainLoss: 0.6519997119903564\n",
      "cnt: 0 - valLoss: 0.651297390460968 - trainLoss: 0.6519981026649475\n",
      "cnt: 0 - valLoss: 0.6512954831123352 - trainLoss: 0.6519963145256042\n",
      "cnt: 0 - valLoss: 0.6512935757637024 - trainLoss: 0.6519947648048401\n",
      "cnt: 0 - valLoss: 0.6512916684150696 - trainLoss: 0.6519930958747864\n",
      "cnt: 0 - valLoss: 0.6512898206710815 - trainLoss: 0.6519914269447327\n",
      "cnt: 0 - valLoss: 0.6512879729270935 - trainLoss: 0.651989758014679\n",
      "cnt: 0 - valLoss: 0.651286244392395 - trainLoss: 0.65198814868927\n",
      "cnt: 0 - valLoss: 0.6512842774391174 - trainLoss: 0.6519865393638611\n",
      "cnt: 0 - valLoss: 0.6512824296951294 - trainLoss: 0.6519848108291626\n",
      "cnt: 0 - valLoss: 0.6512805223464966 - trainLoss: 0.6519832015037537\n",
      "cnt: 0 - valLoss: 0.6512786149978638 - trainLoss: 0.6519814729690552\n",
      "cnt: 0 - valLoss: 0.6512767672538757 - trainLoss: 0.6519798636436462\n",
      "cnt: 0 - valLoss: 0.6512749195098877 - trainLoss: 0.6519781947135925\n",
      "cnt: 0 - valLoss: 0.6512729525566101 - trainLoss: 0.6519765257835388\n",
      "cnt: 0 - valLoss: 0.6512711048126221 - trainLoss: 0.6519748568534851\n",
      "cnt: 0 - valLoss: 0.6512691974639893 - trainLoss: 0.6519731879234314\n",
      "cnt: 0 - valLoss: 0.651267409324646 - trainLoss: 0.6519715189933777\n",
      "cnt: 0 - valLoss: 0.651265561580658 - trainLoss: 0.6519699096679688\n",
      "cnt: 0 - valLoss: 0.6512636542320251 - trainLoss: 0.6519683003425598\n",
      "cnt: 0 - valLoss: 0.6512618064880371 - trainLoss: 0.6519665718078613\n",
      "cnt: 0 - valLoss: 0.6512598395347595 - trainLoss: 0.6519649624824524\n",
      "cnt: 0 - valLoss: 0.6512579917907715 - trainLoss: 0.6519633531570435\n",
      "cnt: 0 - valLoss: 0.6512561440467834 - trainLoss: 0.651961624622345\n",
      "cnt: 0 - valLoss: 0.6512541770935059 - trainLoss: 0.6519598960876465\n",
      "cnt: 0 - valLoss: 0.6512523293495178 - trainLoss: 0.6519582867622375\n",
      "cnt: 0 - valLoss: 0.6512503623962402 - trainLoss: 0.6519565582275391\n",
      "cnt: 0 - valLoss: 0.6512486934661865 - trainLoss: 0.6519549489021301\n",
      "cnt: 0 - valLoss: 0.6512467861175537 - trainLoss: 0.6519533395767212\n",
      "cnt: 0 - valLoss: 0.6512448787689209 - trainLoss: 0.6519516706466675\n",
      "cnt: 0 - valLoss: 0.6512430310249329 - trainLoss: 0.6519500613212585\n",
      "cnt: 0 - valLoss: 0.6512410640716553 - trainLoss: 0.6519483327865601\n",
      "cnt: 0 - valLoss: 0.6512392163276672 - trainLoss: 0.6519466638565063\n",
      "cnt: 0 - valLoss: 0.6512372493743896 - trainLoss: 0.6519449949264526\n",
      "cnt: 0 - valLoss: 0.6512354016304016 - trainLoss: 0.6519433259963989\n",
      "cnt: 0 - valLoss: 0.6512335538864136 - trainLoss: 0.6519416570663452\n",
      "cnt: 0 - valLoss: 0.651231586933136 - trainLoss: 0.6519399881362915\n",
      "cnt: 0 - valLoss: 0.6512298583984375 - trainLoss: 0.6519383192062378\n",
      "cnt: 0 - valLoss: 0.6512280106544495 - trainLoss: 0.6519367694854736\n",
      "cnt: 0 - valLoss: 0.6512261033058167 - trainLoss: 0.6519351005554199\n",
      "cnt: 0 - valLoss: 0.6512242555618286 - trainLoss: 0.6519334316253662\n",
      "cnt: 0 - valLoss: 0.651222288608551 - trainLoss: 0.6519317626953125\n",
      "cnt: 0 - valLoss: 0.6512205004692078 - trainLoss: 0.6519300937652588\n",
      "cnt: 0 - valLoss: 0.651218593120575 - trainLoss: 0.6519284248352051\n",
      "cnt: 0 - valLoss: 0.6512166857719421 - trainLoss: 0.6519267559051514\n",
      "cnt: 0 - valLoss: 0.6512147784233093 - trainLoss: 0.6519250869750977\n",
      "cnt: 0 - valLoss: 0.6512128710746765 - trainLoss: 0.651923418045044\n",
      "cnt: 0 - valLoss: 0.651211142539978 - trainLoss: 0.6519216895103455\n",
      "cnt: 0 - valLoss: 0.65120929479599 - trainLoss: 0.6519201397895813\n",
      "cnt: 0 - valLoss: 0.6512073874473572 - trainLoss: 0.6519184708595276\n",
      "cnt: 0 - valLoss: 0.6512055397033691 - trainLoss: 0.6519168615341187\n",
      "cnt: 0 - valLoss: 0.6512036323547363 - trainLoss: 0.6519151329994202\n",
      "cnt: 0 - valLoss: 0.6512017250061035 - trainLoss: 0.6519134640693665\n",
      "cnt: 0 - valLoss: 0.6511998176574707 - trainLoss: 0.6519117951393127\n",
      "cnt: 0 - valLoss: 0.6511979103088379 - trainLoss: 0.651910126209259\n",
      "cnt: 0 - valLoss: 0.6511960029602051 - trainLoss: 0.6519083976745605\n",
      "cnt: 0 - valLoss: 0.651194155216217 - trainLoss: 0.6519067287445068\n",
      "cnt: 0 - valLoss: 0.6511923670768738 - trainLoss: 0.6519051194190979\n",
      "cnt: 0 - valLoss: 0.6511905193328857 - trainLoss: 0.651903510093689\n",
      "cnt: 0 - valLoss: 0.6511886119842529 - trainLoss: 0.6519018411636353\n",
      "cnt: 0 - valLoss: 0.6511867642402649 - trainLoss: 0.6519001722335815\n",
      "cnt: 0 - valLoss: 0.6511848568916321 - trainLoss: 0.6518985033035278\n",
      "cnt: 0 - valLoss: 0.6511829495429993 - trainLoss: 0.6518968343734741\n",
      "cnt: 0 - valLoss: 0.6511810421943665 - trainLoss: 0.6518951654434204\n",
      "cnt: 0 - valLoss: 0.6511791348457336 - trainLoss: 0.6518934369087219\n",
      "cnt: 0 - valLoss: 0.6511772871017456 - trainLoss: 0.6518917679786682\n",
      "cnt: 0 - valLoss: 0.6511753797531128 - trainLoss: 0.6518900990486145\n",
      "cnt: 0 - valLoss: 0.6511735320091248 - trainLoss: 0.6518884301185608\n",
      "cnt: 0 - valLoss: 0.6511717438697815 - trainLoss: 0.6518868207931519\n",
      "cnt: 0 - valLoss: 0.6511698365211487 - trainLoss: 0.6518851518630981\n",
      "cnt: 0 - valLoss: 0.6511679291725159 - trainLoss: 0.6518834829330444\n",
      "cnt: 0 - valLoss: 0.6511660814285278 - trainLoss: 0.6518818140029907\n",
      "cnt: 0 - valLoss: 0.6511641144752502 - trainLoss: 0.651880145072937\n",
      "cnt: 0 - valLoss: 0.6511622071266174 - trainLoss: 0.6518784761428833\n",
      "cnt: 0 - valLoss: 0.6511603593826294 - trainLoss: 0.6518768072128296\n",
      "cnt: 0 - valLoss: 0.6511585116386414 - trainLoss: 0.6518751382827759\n",
      "cnt: 0 - valLoss: 0.6511565446853638 - trainLoss: 0.6518734097480774\n",
      "cnt: 0 - valLoss: 0.6511547565460205 - trainLoss: 0.6518717408180237\n",
      "cnt: 0 - valLoss: 0.6511529088020325 - trainLoss: 0.6518701314926147\n",
      "cnt: 0 - valLoss: 0.6511510014533997 - trainLoss: 0.651868462562561\n",
      "cnt: 0 - valLoss: 0.6511491537094116 - trainLoss: 0.6518667936325073\n",
      "cnt: 0 - valLoss: 0.6511472463607788 - trainLoss: 0.6518651247024536\n",
      "cnt: 0 - valLoss: 0.6511452198028564 - trainLoss: 0.6518634557723999\n",
      "cnt: 0 - valLoss: 0.6511433720588684 - trainLoss: 0.6518617272377014\n",
      "cnt: 0 - valLoss: 0.6511414647102356 - trainLoss: 0.6518600583076477\n",
      "cnt: 0 - valLoss: 0.6511396169662476 - trainLoss: 0.651858389377594\n",
      "cnt: 0 - valLoss: 0.6511377096176147 - trainLoss: 0.6518566608428955\n",
      "cnt: 0 - valLoss: 0.6511359214782715 - trainLoss: 0.6518550515174866\n",
      "cnt: 0 - valLoss: 0.6511341333389282 - trainLoss: 0.6518534421920776\n",
      "cnt: 0 - valLoss: 0.6511321067810059 - trainLoss: 0.6518517732620239\n",
      "cnt: 0 - valLoss: 0.6511303186416626 - trainLoss: 0.6518500447273254\n",
      "cnt: 0 - valLoss: 0.651128351688385 - trainLoss: 0.6518483757972717\n",
      "cnt: 0 - valLoss: 0.651126503944397 - trainLoss: 0.651846706867218\n",
      "cnt: 0 - valLoss: 0.6511244773864746 - trainLoss: 0.6518450379371643\n",
      "cnt: 0 - valLoss: 0.6511226296424866 - trainLoss: 0.6518433094024658\n",
      "cnt: 0 - valLoss: 0.651120662689209 - trainLoss: 0.6518416404724121\n",
      "cnt: 0 - valLoss: 0.651118814945221 - trainLoss: 0.6518399715423584\n",
      "cnt: 0 - valLoss: 0.6511170268058777 - trainLoss: 0.6518383026123047\n",
      "cnt: 0 - valLoss: 0.6511150598526001 - trainLoss: 0.651836633682251\n",
      "cnt: 0 - valLoss: 0.6511132121086121 - trainLoss: 0.651835024356842\n",
      "cnt: 0 - valLoss: 0.6511112451553345 - trainLoss: 0.6518332958221436\n",
      "cnt: 0 - valLoss: 0.6511093378067017 - trainLoss: 0.6518316864967346\n",
      "cnt: 0 - valLoss: 0.6511074304580688 - trainLoss: 0.6518299579620361\n",
      "cnt: 0 - valLoss: 0.6511054635047913 - trainLoss: 0.6518282890319824\n",
      "cnt: 0 - valLoss: 0.6511034965515137 - trainLoss: 0.6518266201019287\n",
      "cnt: 0 - valLoss: 0.6511015892028809 - trainLoss: 0.6518248915672302\n",
      "cnt: 0 - valLoss: 0.6510997414588928 - trainLoss: 0.6518232226371765\n",
      "cnt: 0 - valLoss: 0.6510979533195496 - trainLoss: 0.6518215537071228\n",
      "cnt: 0 - valLoss: 0.6510960459709167 - trainLoss: 0.6518199443817139\n",
      "cnt: 0 - valLoss: 0.6510940790176392 - trainLoss: 0.6518182754516602\n",
      "cnt: 0 - valLoss: 0.6510921716690063 - trainLoss: 0.6518166065216064\n",
      "cnt: 0 - valLoss: 0.6510902047157288 - trainLoss: 0.6518149375915527\n",
      "cnt: 0 - valLoss: 0.6510883569717407 - trainLoss: 0.6518132090568542\n",
      "cnt: 0 - valLoss: 0.6510863900184631 - trainLoss: 0.6518115401268005\n",
      "cnt: 0 - valLoss: 0.6510844826698303 - trainLoss: 0.6518098711967468\n",
      "cnt: 0 - valLoss: 0.6510825157165527 - trainLoss: 0.6518082022666931\n",
      "cnt: 0 - valLoss: 0.6510806679725647 - trainLoss: 0.6518064737319946\n",
      "cnt: 0 - valLoss: 0.6510788202285767 - trainLoss: 0.6518048644065857\n",
      "cnt: 0 - valLoss: 0.6510769128799438 - trainLoss: 0.6518032550811768\n",
      "cnt: 0 - valLoss: 0.6510749459266663 - trainLoss: 0.6518015265464783\n",
      "cnt: 0 - valLoss: 0.6510729789733887 - trainLoss: 0.6517997980117798\n",
      "cnt: 0 - valLoss: 0.6510710716247559 - trainLoss: 0.6517981290817261\n",
      "cnt: 0 - valLoss: 0.6510691046714783 - trainLoss: 0.6517964601516724\n",
      "cnt: 0 - valLoss: 0.6510672569274902 - trainLoss: 0.6517947912216187\n",
      "cnt: 0 - valLoss: 0.6510652899742126 - trainLoss: 0.6517930626869202\n",
      "cnt: 0 - valLoss: 0.6510633826255798 - trainLoss: 0.6517914533615112\n",
      "cnt: 0 - valLoss: 0.651061475276947 - trainLoss: 0.6517897248268127\n",
      "cnt: 0 - valLoss: 0.6510596871376038 - trainLoss: 0.6517881155014038\n",
      "cnt: 0 - valLoss: 0.651057779788971 - trainLoss: 0.6517864465713501\n",
      "cnt: 0 - valLoss: 0.6510558128356934 - trainLoss: 0.6517847776412964\n",
      "cnt: 0 - valLoss: 0.6510539054870605 - trainLoss: 0.6517830491065979\n",
      "cnt: 0 - valLoss: 0.6510519981384277 - trainLoss: 0.6517813801765442\n",
      "cnt: 0 - valLoss: 0.6510500311851501 - trainLoss: 0.6517796516418457\n",
      "cnt: 0 - valLoss: 0.6510480642318726 - trainLoss: 0.651777982711792\n",
      "cnt: 0 - valLoss: 0.6510461568832397 - trainLoss: 0.6517763137817383\n",
      "cnt: 0 - valLoss: 0.6510442495346069 - trainLoss: 0.6517746448516846\n",
      "cnt: 0 - valLoss: 0.6510424017906189 - trainLoss: 0.6517729163169861\n",
      "cnt: 0 - valLoss: 0.6510405540466309 - trainLoss: 0.6517713665962219\n",
      "cnt: 0 - valLoss: 0.6510385870933533 - trainLoss: 0.6517696976661682\n",
      "cnt: 0 - valLoss: 0.6510366201400757 - trainLoss: 0.6517679691314697\n",
      "cnt: 0 - valLoss: 0.6510347723960876 - trainLoss: 0.6517662405967712\n",
      "cnt: 0 - valLoss: 0.6510328054428101 - trainLoss: 0.6517645716667175\n",
      "cnt: 0 - valLoss: 0.6510308980941772 - trainLoss: 0.6517629027366638\n",
      "cnt: 0 - valLoss: 0.6510289311408997 - trainLoss: 0.6517612338066101\n",
      "cnt: 0 - valLoss: 0.6510269641876221 - trainLoss: 0.6517595052719116\n",
      "cnt: 0 - valLoss: 0.651025116443634 - trainLoss: 0.6517577767372131\n",
      "cnt: 0 - valLoss: 0.6510233283042908 - trainLoss: 0.6517561674118042\n",
      "cnt: 0 - valLoss: 0.6510213613510132 - trainLoss: 0.6517545580863953\n",
      "cnt: 0 - valLoss: 0.6510193943977356 - trainLoss: 0.6517528295516968\n",
      "cnt: 0 - valLoss: 0.6510174870491028 - trainLoss: 0.6517511606216431\n",
      "cnt: 0 - valLoss: 0.6510155200958252 - trainLoss: 0.6517494320869446\n",
      "cnt: 0 - valLoss: 0.6510136723518372 - trainLoss: 0.6517477035522461\n",
      "cnt: 0 - valLoss: 0.6510117053985596 - trainLoss: 0.6517460942268372\n",
      "cnt: 0 - valLoss: 0.651009738445282 - trainLoss: 0.6517443656921387\n",
      "cnt: 0 - valLoss: 0.6510077714920044 - trainLoss: 0.6517426371574402\n",
      "cnt: 0 - valLoss: 0.6510059237480164 - trainLoss: 0.6517410278320312\n",
      "cnt: 0 - valLoss: 0.6510040760040283 - trainLoss: 0.6517392992973328\n",
      "cnt: 0 - valLoss: 0.6510021686553955 - trainLoss: 0.651737630367279\n",
      "cnt: 0 - valLoss: 0.6510002017021179 - trainLoss: 0.6517359614372253\n",
      "cnt: 0 - valLoss: 0.6509982943534851 - trainLoss: 0.6517342329025269\n",
      "cnt: 0 - valLoss: 0.6509963274002075 - trainLoss: 0.6517325639724731\n",
      "cnt: 0 - valLoss: 0.6509943604469299 - trainLoss: 0.6517308950424194\n",
      "cnt: 0 - valLoss: 0.6509923934936523 - trainLoss: 0.651729166507721\n",
      "cnt: 0 - valLoss: 0.6509905457496643 - trainLoss: 0.6517274975776672\n",
      "cnt: 0 - valLoss: 0.6509885787963867 - trainLoss: 0.6517258286476135\n",
      "cnt: 0 - valLoss: 0.6509868502616882 - trainLoss: 0.6517241597175598\n",
      "cnt: 0 - valLoss: 0.6509848833084106 - trainLoss: 0.6517224907875061\n",
      "cnt: 0 - valLoss: 0.6509829163551331 - trainLoss: 0.6517207026481628\n",
      "cnt: 0 - valLoss: 0.6509810090065002 - trainLoss: 0.6517190337181091\n",
      "cnt: 0 - valLoss: 0.6509790420532227 - trainLoss: 0.6517173647880554\n",
      "cnt: 0 - valLoss: 0.6509770750999451 - trainLoss: 0.6517156958580017\n",
      "cnt: 0 - valLoss: 0.6509751677513123 - trainLoss: 0.651714026927948\n",
      "cnt: 0 - valLoss: 0.6509732007980347 - trainLoss: 0.6517122983932495\n",
      "cnt: 0 - valLoss: 0.6509712934494019 - trainLoss: 0.651710569858551\n",
      "cnt: 0 - valLoss: 0.6509694457054138 - trainLoss: 0.6517089009284973\n",
      "cnt: 0 - valLoss: 0.6509675979614258 - trainLoss: 0.6517072916030884\n",
      "cnt: 0 - valLoss: 0.6509656310081482 - trainLoss: 0.6517055630683899\n",
      "cnt: 0 - valLoss: 0.6509636640548706 - trainLoss: 0.6517038941383362\n",
      "cnt: 0 - valLoss: 0.6509617567062378 - trainLoss: 0.6517022252082825\n",
      "cnt: 0 - valLoss: 0.6509597897529602 - trainLoss: 0.651700496673584\n",
      "cnt: 0 - valLoss: 0.6509578824043274 - trainLoss: 0.6516988277435303\n",
      "cnt: 0 - valLoss: 0.6509559154510498 - trainLoss: 0.651697039604187\n",
      "cnt: 0 - valLoss: 0.6509539484977722 - trainLoss: 0.6516953706741333\n",
      "cnt: 0 - valLoss: 0.6509521007537842 - trainLoss: 0.6516937017440796\n",
      "cnt: 0 - valLoss: 0.6509502530097961 - trainLoss: 0.6516920328140259\n",
      "cnt: 0 - valLoss: 0.6509482860565186 - trainLoss: 0.6516903638839722\n",
      "cnt: 0 - valLoss: 0.6509463787078857 - trainLoss: 0.6516886353492737\n",
      "cnt: 0 - valLoss: 0.6509444713592529 - trainLoss: 0.6516869068145752\n",
      "cnt: 0 - valLoss: 0.6509424448013306 - trainLoss: 0.6516852378845215\n",
      "cnt: 0 - valLoss: 0.6509405374526978 - trainLoss: 0.6516835689544678\n",
      "cnt: 0 - valLoss: 0.6509385704994202 - trainLoss: 0.6516818404197693\n",
      "cnt: 0 - valLoss: 0.6509366631507874 - trainLoss: 0.6516801714897156\n",
      "cnt: 0 - valLoss: 0.6509346961975098 - trainLoss: 0.6516784429550171\n",
      "cnt: 0 - valLoss: 0.6509328484535217 - trainLoss: 0.6516767144203186\n",
      "cnt: 0 - valLoss: 0.6509309411048889 - trainLoss: 0.6516751646995544\n",
      "cnt: 0 - valLoss: 0.6509290337562561 - trainLoss: 0.651673436164856\n",
      "cnt: 0 - valLoss: 0.6509270668029785 - trainLoss: 0.6516717076301575\n",
      "cnt: 0 - valLoss: 0.6509250998497009 - trainLoss: 0.6516700387001038\n",
      "cnt: 0 - valLoss: 0.6509231328964233 - trainLoss: 0.6516683101654053\n",
      "cnt: 0 - valLoss: 0.6509212255477905 - trainLoss: 0.6516666412353516\n",
      "cnt: 0 - valLoss: 0.6509192585945129 - trainLoss: 0.6516649127006531\n",
      "cnt: 0 - valLoss: 0.6509172916412354 - trainLoss: 0.6516631841659546\n",
      "cnt: 0 - valLoss: 0.6509154438972473 - trainLoss: 0.6516615152359009\n",
      "cnt: 0 - valLoss: 0.6509135961532593 - trainLoss: 0.6516598463058472\n",
      "cnt: 0 - valLoss: 0.6509116291999817 - trainLoss: 0.6516581773757935\n",
      "cnt: 0 - valLoss: 0.6509096622467041 - trainLoss: 0.651656448841095\n",
      "cnt: 0 - valLoss: 0.6509076952934265 - trainLoss: 0.6516547203063965\n",
      "cnt: 0 - valLoss: 0.6509057879447937 - trainLoss: 0.6516530513763428\n",
      "cnt: 0 - valLoss: 0.6509038805961609 - trainLoss: 0.6516513228416443\n",
      "cnt: 0 - valLoss: 0.6509019136428833 - trainLoss: 0.6516495943069458\n",
      "cnt: 0 - valLoss: 0.6509000062942505 - trainLoss: 0.6516479253768921\n",
      "cnt: 0 - valLoss: 0.6508979797363281 - trainLoss: 0.6516461968421936\n",
      "cnt: 0 - valLoss: 0.6508961319923401 - trainLoss: 0.6516445279121399\n",
      "cnt: 0 - valLoss: 0.650894284248352 - trainLoss: 0.6516428589820862\n",
      "cnt: 0 - valLoss: 0.6508923768997192 - trainLoss: 0.6516411900520325\n",
      "cnt: 0 - valLoss: 0.6508904099464417 - trainLoss: 0.651639461517334\n",
      "cnt: 0 - valLoss: 0.6508884429931641 - trainLoss: 0.6516377329826355\n",
      "cnt: 0 - valLoss: 0.6508864760398865 - trainLoss: 0.651636004447937\n",
      "cnt: 0 - valLoss: 0.6508845090866089 - trainLoss: 0.6516343355178833\n",
      "cnt: 0 - valLoss: 0.6508825421333313 - trainLoss: 0.6516326069831848\n",
      "cnt: 0 - valLoss: 0.6508806347846985 - trainLoss: 0.6516309380531311\n",
      "cnt: 0 - valLoss: 0.6508787274360657 - trainLoss: 0.6516291499137878\n",
      "cnt: 0 - valLoss: 0.6508768796920776 - trainLoss: 0.6516274809837341\n",
      "cnt: 0 - valLoss: 0.6508749723434448 - trainLoss: 0.6516258120536804\n",
      "cnt: 0 - valLoss: 0.6508730053901672 - trainLoss: 0.6516241431236267\n",
      "cnt: 0 - valLoss: 0.6508710384368896 - trainLoss: 0.651622474193573\n",
      "cnt: 0 - valLoss: 0.6508691310882568 - trainLoss: 0.6516206860542297\n",
      "cnt: 0 - valLoss: 0.6508671045303345 - trainLoss: 0.651619017124176\n",
      "cnt: 0 - valLoss: 0.6508652567863464 - trainLoss: 0.6516173481941223\n",
      "cnt: 0 - valLoss: 0.6508632302284241 - trainLoss: 0.651615560054779\n",
      "cnt: 0 - valLoss: 0.6508613228797913 - trainLoss: 0.6516138911247253\n",
      "cnt: 0 - valLoss: 0.650859534740448 - trainLoss: 0.6516121625900269\n",
      "cnt: 0 - valLoss: 0.6508576273918152 - trainLoss: 0.6516105532646179\n",
      "cnt: 0 - valLoss: 0.6508556008338928 - trainLoss: 0.6516088247299194\n",
      "cnt: 0 - valLoss: 0.6508536338806152 - trainLoss: 0.6516071557998657\n",
      "cnt: 0 - valLoss: 0.6508517265319824 - trainLoss: 0.6516053676605225\n",
      "cnt: 0 - valLoss: 0.6508497595787048 - trainLoss: 0.651603639125824\n",
      "cnt: 0 - valLoss: 0.650847852230072 - trainLoss: 0.6516019701957703\n",
      "cnt: 0 - valLoss: 0.6508458256721497 - trainLoss: 0.651600182056427\n",
      "cnt: 0 - valLoss: 0.6508439183235168 - trainLoss: 0.6515985131263733\n",
      "cnt: 0 - valLoss: 0.6508420705795288 - trainLoss: 0.6515967845916748\n",
      "cnt: 0 - valLoss: 0.6508402228355408 - trainLoss: 0.6515951752662659\n",
      "cnt: 0 - valLoss: 0.6508381962776184 - trainLoss: 0.6515935063362122\n",
      "cnt: 0 - valLoss: 0.6508362293243408 - trainLoss: 0.6515917778015137\n",
      "cnt: 0 - valLoss: 0.650834321975708 - trainLoss: 0.6515900492668152\n",
      "cnt: 0 - valLoss: 0.6508323550224304 - trainLoss: 0.6515883207321167\n",
      "cnt: 0 - valLoss: 0.6508303880691528 - trainLoss: 0.6515865921974182\n",
      "cnt: 0 - valLoss: 0.6508284211158752 - trainLoss: 0.6515848636627197\n",
      "cnt: 0 - valLoss: 0.6508264541625977 - trainLoss: 0.6515831351280212\n",
      "cnt: 0 - valLoss: 0.6508246064186096 - trainLoss: 0.6515814661979675\n",
      "cnt: 0 - valLoss: 0.6508227586746216 - trainLoss: 0.651579737663269\n",
      "cnt: 0 - valLoss: 0.650820791721344 - trainLoss: 0.6515781283378601\n",
      "cnt: 0 - valLoss: 0.6508188247680664 - trainLoss: 0.6515763998031616\n",
      "cnt: 0 - valLoss: 0.650816798210144 - trainLoss: 0.6515746712684631\n",
      "cnt: 0 - valLoss: 0.6508148908615112 - trainLoss: 0.6515729427337646\n",
      "cnt: 0 - valLoss: 0.6508129239082336 - trainLoss: 0.6515712738037109\n",
      "cnt: 0 - valLoss: 0.650810956954956 - trainLoss: 0.6515694856643677\n",
      "cnt: 0 - valLoss: 0.6508090496063232 - trainLoss: 0.6515677571296692\n",
      "cnt: 0 - valLoss: 0.6508071422576904 - trainLoss: 0.6515660881996155\n",
      "cnt: 0 - valLoss: 0.6508052945137024 - trainLoss: 0.651564359664917\n",
      "cnt: 0 - valLoss: 0.6508033275604248 - trainLoss: 0.6515627503395081\n",
      "cnt: 0 - valLoss: 0.6508013010025024 - trainLoss: 0.6515610218048096\n",
      "cnt: 0 - valLoss: 0.6507995128631592 - trainLoss: 0.6515592932701111\n",
      "cnt: 0 - valLoss: 0.6507974863052368 - trainLoss: 0.6515575647354126\n",
      "cnt: 0 - valLoss: 0.6507955193519592 - trainLoss: 0.6515558362007141\n",
      "cnt: 0 - valLoss: 0.6507936120033264 - trainLoss: 0.6515541076660156\n",
      "cnt: 0 - valLoss: 0.650791585445404 - trainLoss: 0.6515524387359619\n",
      "cnt: 0 - valLoss: 0.6507896780967712 - trainLoss: 0.6515507102012634\n",
      "cnt: 0 - valLoss: 0.650787889957428 - trainLoss: 0.6515489816665649\n",
      "cnt: 0 - valLoss: 0.6507859230041504 - trainLoss: 0.6515473127365112\n",
      "cnt: 0 - valLoss: 0.6507839560508728 - trainLoss: 0.6515455842018127\n",
      "cnt: 0 - valLoss: 0.65078204870224 - trainLoss: 0.651543915271759\n",
      "cnt: 0 - valLoss: 0.6507800817489624 - trainLoss: 0.6515421867370605\n",
      "cnt: 0 - valLoss: 0.6507781147956848 - trainLoss: 0.6515404582023621\n",
      "cnt: 0 - valLoss: 0.6507761478424072 - trainLoss: 0.6515387296676636\n",
      "cnt: 0 - valLoss: 0.6507741808891296 - trainLoss: 0.6515370607376099\n",
      "cnt: 0 - valLoss: 0.6507722735404968 - trainLoss: 0.6515353322029114\n",
      "cnt: 0 - valLoss: 0.6507704854011536 - trainLoss: 0.6515336036682129\n",
      "cnt: 0 - valLoss: 0.6507685780525208 - trainLoss: 0.651531994342804\n",
      "cnt: 0 - valLoss: 0.6507666110992432 - trainLoss: 0.6515303254127502\n",
      "cnt: 0 - valLoss: 0.6507646441459656 - trainLoss: 0.651528537273407\n",
      "cnt: 0 - valLoss: 0.6507627367973328 - trainLoss: 0.6515268683433533\n",
      "cnt: 0 - valLoss: 0.6507607102394104 - trainLoss: 0.6515251398086548\n",
      "cnt: 0 - valLoss: 0.6507588028907776 - trainLoss: 0.6515234112739563\n",
      "cnt: 0 - valLoss: 0.6507567763328552 - trainLoss: 0.6515216827392578\n",
      "cnt: 0 - valLoss: 0.6507548689842224 - trainLoss: 0.6515200138092041\n",
      "cnt: 0 - valLoss: 0.6507530808448792 - trainLoss: 0.6515182852745056\n",
      "cnt: 0 - valLoss: 0.6507511138916016 - trainLoss: 0.6515166163444519\n",
      "cnt: 0 - valLoss: 0.6507492065429688 - trainLoss: 0.6515149474143982\n",
      "cnt: 0 - valLoss: 0.6507472395896912 - trainLoss: 0.6515132784843445\n",
      "cnt: 0 - valLoss: 0.6507452726364136 - trainLoss: 0.6515114903450012\n",
      "cnt: 0 - valLoss: 0.650743305683136 - trainLoss: 0.6515097618103027\n",
      "cnt: 0 - valLoss: 0.6507413983345032 - trainLoss: 0.6515081524848938\n",
      "cnt: 0 - valLoss: 0.6507393717765808 - trainLoss: 0.6515064239501953\n",
      "cnt: 0 - valLoss: 0.6507374048233032 - trainLoss: 0.6515046954154968\n",
      "cnt: 0 - valLoss: 0.6507355570793152 - trainLoss: 0.6515029072761536\n",
      "cnt: 0 - valLoss: 0.6507337093353271 - trainLoss: 0.6515012383460999\n",
      "cnt: 0 - valLoss: 0.6507317423820496 - trainLoss: 0.6514996290206909\n",
      "cnt: 0 - valLoss: 0.6507298350334167 - trainLoss: 0.6514979004859924\n",
      "cnt: 0 - valLoss: 0.6507278680801392 - trainLoss: 0.651496171951294\n",
      "cnt: 0 - valLoss: 0.6507259011268616 - trainLoss: 0.6514944434165955\n",
      "cnt: 0 - valLoss: 0.650723934173584 - trainLoss: 0.6514927744865417\n",
      "cnt: 0 - valLoss: 0.6507219672203064 - trainLoss: 0.6514909863471985\n",
      "cnt: 0 - valLoss: 0.6507200598716736 - trainLoss: 0.6514893174171448\n",
      "cnt: 0 - valLoss: 0.6507180333137512 - trainLoss: 0.6514875888824463\n",
      "cnt: 0 - valLoss: 0.6507163047790527 - trainLoss: 0.6514858603477478\n",
      "cnt: 0 - valLoss: 0.6507143378257751 - trainLoss: 0.6514841914176941\n",
      "cnt: 0 - valLoss: 0.6507123708724976 - trainLoss: 0.6514825224876404\n",
      "cnt: 0 - valLoss: 0.65071040391922 - trainLoss: 0.6514807939529419\n",
      "cnt: 0 - valLoss: 0.6507083773612976 - trainLoss: 0.6514790654182434\n",
      "cnt: 0 - valLoss: 0.6507064700126648 - trainLoss: 0.6514773368835449\n",
      "cnt: 0 - valLoss: 0.6507045030593872 - trainLoss: 0.6514756679534912\n",
      "cnt: 0 - valLoss: 0.6507025957107544 - trainLoss: 0.6514739394187927\n",
      "cnt: 0 - valLoss: 0.6507006287574768 - trainLoss: 0.6514722108840942\n",
      "cnt: 0 - valLoss: 0.6506987810134888 - trainLoss: 0.6514704823493958\n",
      "cnt: 0 - valLoss: 0.650696873664856 - trainLoss: 0.6514688730239868\n",
      "cnt: 0 - valLoss: 0.6506949663162231 - trainLoss: 0.6514671444892883\n",
      "cnt: 0 - valLoss: 0.6506929993629456 - trainLoss: 0.6514654755592346\n",
      "cnt: 0 - valLoss: 0.6506909728050232 - trainLoss: 0.6514636874198914\n",
      "cnt: 0 - valLoss: 0.6506890058517456 - trainLoss: 0.6514619588851929\n",
      "cnt: 0 - valLoss: 0.6506870985031128 - trainLoss: 0.6514602303504944\n",
      "cnt: 0 - valLoss: 0.6506850719451904 - trainLoss: 0.6514585018157959\n",
      "cnt: 0 - valLoss: 0.6506832242012024 - trainLoss: 0.6514567732810974\n",
      "cnt: 0 - valLoss: 0.6506813168525696 - trainLoss: 0.6514551043510437\n",
      "cnt: 0 - valLoss: 0.6506794095039368 - trainLoss: 0.65145343542099\n",
      "cnt: 0 - valLoss: 0.650677502155304 - trainLoss: 0.6514517068862915\n",
      "cnt: 0 - valLoss: 0.6506754755973816 - trainLoss: 0.6514500379562378\n",
      "cnt: 0 - valLoss: 0.6506736278533936 - trainLoss: 0.6514483094215393\n",
      "cnt: 0 - valLoss: 0.6506716012954712 - trainLoss: 0.651446521282196\n",
      "cnt: 0 - valLoss: 0.6506696343421936 - trainLoss: 0.6514448523521423\n",
      "cnt: 0 - valLoss: 0.650667667388916 - trainLoss: 0.6514430642127991\n",
      "cnt: 0 - valLoss: 0.6506657004356384 - trainLoss: 0.6514413952827454\n",
      "cnt: 0 - valLoss: 0.6506638526916504 - trainLoss: 0.6514396667480469\n",
      "cnt: 0 - valLoss: 0.6506620049476624 - trainLoss: 0.6514379978179932\n",
      "cnt: 0 - valLoss: 0.6506600379943848 - trainLoss: 0.6514363288879395\n",
      "cnt: 0 - valLoss: 0.6506580710411072 - trainLoss: 0.651434600353241\n",
      "cnt: 0 - valLoss: 0.6506561636924744 - trainLoss: 0.6514328718185425\n",
      "cnt: 0 - valLoss: 0.6506541967391968 - trainLoss: 0.651431143283844\n",
      "cnt: 0 - valLoss: 0.6506522297859192 - trainLoss: 0.6514293551445007\n",
      "cnt: 0 - valLoss: 0.6506502628326416 - trainLoss: 0.651427686214447\n",
      "cnt: 0 - valLoss: 0.650648295879364 - trainLoss: 0.6514258980751038\n",
      "cnt: 0 - valLoss: 0.6506463885307312 - trainLoss: 0.6514241695404053\n",
      "cnt: 0 - valLoss: 0.6506445407867432 - trainLoss: 0.6514225006103516\n",
      "cnt: 0 - valLoss: 0.6506425738334656 - trainLoss: 0.6514208316802979\n",
      "cnt: 0 - valLoss: 0.6506406664848328 - trainLoss: 0.6514191627502441\n",
      "cnt: 0 - valLoss: 0.6506386995315552 - trainLoss: 0.6514174342155457\n",
      "cnt: 0 - valLoss: 0.6506366729736328 - trainLoss: 0.6514157056808472\n",
      "cnt: 0 - valLoss: 0.6506347060203552 - trainLoss: 0.6514139771461487\n",
      "cnt: 0 - valLoss: 0.6506327390670776 - trainLoss: 0.6514122486114502\n",
      "cnt: 0 - valLoss: 0.6506307721138 - trainLoss: 0.6514105200767517\n",
      "cnt: 0 - valLoss: 0.6506288647651672 - trainLoss: 0.6514087915420532\n",
      "cnt: 0 - valLoss: 0.650627076625824 - trainLoss: 0.6514070630073547\n",
      "cnt: 0 - valLoss: 0.6506251692771912 - trainLoss: 0.651405394077301\n",
      "cnt: 0 - valLoss: 0.650623083114624 - trainLoss: 0.6514037251472473\n",
      "cnt: 0 - valLoss: 0.6506211161613464 - trainLoss: 0.651401937007904\n",
      "cnt: 0 - valLoss: 0.6506192088127136 - trainLoss: 0.6514002084732056\n",
      "cnt: 0 - valLoss: 0.6506171822547913 - trainLoss: 0.6513985395431519\n",
      "cnt: 0 - valLoss: 0.6506152153015137 - trainLoss: 0.6513966917991638\n",
      "cnt: 0 - valLoss: 0.6506132483482361 - trainLoss: 0.6513950228691101\n",
      "cnt: 0 - valLoss: 0.6506113409996033 - trainLoss: 0.6513932943344116\n",
      "cnt: 0 - valLoss: 0.6506094932556152 - trainLoss: 0.6513915657997131\n",
      "cnt: 0 - valLoss: 0.6506075263023376 - trainLoss: 0.6513898968696594\n",
      "cnt: 0 - valLoss: 0.6506055593490601 - trainLoss: 0.6513882279396057\n",
      "cnt: 0 - valLoss: 0.6506035327911377 - trainLoss: 0.6513864994049072\n",
      "cnt: 0 - valLoss: 0.6506015658378601 - trainLoss: 0.651384711265564\n",
      "cnt: 0 - valLoss: 0.6505995988845825 - trainLoss: 0.6513830423355103\n",
      "cnt: 0 - valLoss: 0.6505976319313049 - trainLoss: 0.651381254196167\n",
      "cnt: 0 - valLoss: 0.6505956649780273 - trainLoss: 0.6513795256614685\n",
      "cnt: 0 - valLoss: 0.6505936980247498 - trainLoss: 0.65137779712677\n",
      "cnt: 0 - valLoss: 0.6505919694900513 - trainLoss: 0.6513760685920715\n",
      "cnt: 0 - valLoss: 0.6505899429321289 - trainLoss: 0.6513743996620178\n",
      "cnt: 0 - valLoss: 0.6505879759788513 - trainLoss: 0.6513727307319641\n",
      "cnt: 0 - valLoss: 0.6505860090255737 - trainLoss: 0.6513709425926208\n",
      "cnt: 0 - valLoss: 0.6505839824676514 - trainLoss: 0.6513692140579224\n",
      "cnt: 0 - valLoss: 0.6505820751190186 - trainLoss: 0.6513675451278687\n",
      "cnt: 0 - valLoss: 0.6505800485610962 - trainLoss: 0.6513657569885254\n",
      "cnt: 0 - valLoss: 0.6505780816078186 - trainLoss: 0.6513640284538269\n",
      "cnt: 0 - valLoss: 0.650576114654541 - trainLoss: 0.6513623595237732\n",
      "cnt: 0 - valLoss: 0.650574266910553 - trainLoss: 0.6513605713844299\n",
      "cnt: 0 - valLoss: 0.6505722999572754 - trainLoss: 0.651358962059021\n",
      "cnt: 0 - valLoss: 0.6505703330039978 - trainLoss: 0.6513571739196777\n",
      "cnt: 0 - valLoss: 0.6505683660507202 - trainLoss: 0.6513554453849792\n",
      "cnt: 0 - valLoss: 0.6505663394927979 - trainLoss: 0.6513537168502808\n",
      "cnt: 0 - valLoss: 0.6505643725395203 - trainLoss: 0.6513519883155823\n",
      "cnt: 0 - valLoss: 0.6505624055862427 - trainLoss: 0.6513502597808838\n",
      "cnt: 0 - valLoss: 0.6505604386329651 - trainLoss: 0.6513485312461853\n",
      "cnt: 0 - valLoss: 0.6505584716796875 - trainLoss: 0.651346743106842\n",
      "cnt: 0 - valLoss: 0.6505566835403442 - trainLoss: 0.6513450741767883\n",
      "cnt: 0 - valLoss: 0.6505547165870667 - trainLoss: 0.6513434052467346\n",
      "cnt: 0 - valLoss: 0.6505527496337891 - trainLoss: 0.6513416767120361\n",
      "cnt: 0 - valLoss: 0.6505507826805115 - trainLoss: 0.6513399481773376\n",
      "cnt: 0 - valLoss: 0.6505487561225891 - trainLoss: 0.6513381600379944\n",
      "cnt: 0 - valLoss: 0.6505467891693115 - trainLoss: 0.6513364315032959\n",
      "cnt: 0 - valLoss: 0.6505448222160339 - trainLoss: 0.6513347625732422\n",
      "cnt: 0 - valLoss: 0.6505428552627563 - trainLoss: 0.6513329744338989\n",
      "cnt: 0 - valLoss: 0.6505408883094788 - trainLoss: 0.6513312458992004\n",
      "cnt: 0 - valLoss: 0.6505390405654907 - trainLoss: 0.651329517364502\n",
      "cnt: 0 - valLoss: 0.6505370140075684 - trainLoss: 0.6513277888298035\n",
      "cnt: 0 - valLoss: 0.6505350470542908 - trainLoss: 0.6513261198997498\n",
      "cnt: 0 - valLoss: 0.650533139705658 - trainLoss: 0.6513243913650513\n",
      "cnt: 0 - valLoss: 0.6505311131477356 - trainLoss: 0.651322603225708\n",
      "cnt: 0 - valLoss: 0.6505290865898132 - trainLoss: 0.6513208746910095\n",
      "cnt: 0 - valLoss: 0.6505271196365356 - trainLoss: 0.651319146156311\n",
      "cnt: 0 - valLoss: 0.6505251526832581 - trainLoss: 0.6513173580169678\n",
      "cnt: 0 - valLoss: 0.6505232453346252 - trainLoss: 0.6513156294822693\n",
      "cnt: 0 - valLoss: 0.6505213975906372 - trainLoss: 0.6513139605522156\n",
      "cnt: 0 - valLoss: 0.6505193710327148 - trainLoss: 0.6513122320175171\n",
      "cnt: 0 - valLoss: 0.6505174040794373 - trainLoss: 0.6513105034828186\n",
      "cnt: 0 - valLoss: 0.6505154371261597 - trainLoss: 0.6513088345527649\n",
      "cnt: 0 - valLoss: 0.6505134105682373 - trainLoss: 0.6513069868087769\n",
      "cnt: 0 - valLoss: 0.6505115032196045 - trainLoss: 0.6513053178787231\n",
      "cnt: 0 - valLoss: 0.6505094170570374 - trainLoss: 0.6513035297393799\n",
      "cnt: 0 - valLoss: 0.6505074501037598 - trainLoss: 0.6513018012046814\n",
      "cnt: 0 - valLoss: 0.6505056023597717 - trainLoss: 0.6513000130653381\n",
      "cnt: 0 - valLoss: 0.6505036950111389 - trainLoss: 0.6512983441352844\n",
      "cnt: 0 - valLoss: 0.6505016684532166 - trainLoss: 0.6512966752052307\n",
      "cnt: 0 - valLoss: 0.650499701499939 - trainLoss: 0.6512948870658875\n",
      "cnt: 0 - valLoss: 0.6504976749420166 - trainLoss: 0.651293158531189\n",
      "cnt: 0 - valLoss: 0.6504957675933838 - trainLoss: 0.6512914299964905\n",
      "cnt: 0 - valLoss: 0.6504937410354614 - trainLoss: 0.651289701461792\n",
      "cnt: 0 - valLoss: 0.6504917740821838 - trainLoss: 0.6512879133224487\n",
      "cnt: 0 - valLoss: 0.6504897475242615 - trainLoss: 0.6512861847877502\n",
      "cnt: 0 - valLoss: 0.6504878997802734 - trainLoss: 0.6512844562530518\n",
      "cnt: 0 - valLoss: 0.6504860520362854 - trainLoss: 0.651282787322998\n",
      "cnt: 0 - valLoss: 0.6504839658737183 - trainLoss: 0.6512810587882996\n",
      "cnt: 0 - valLoss: 0.6504819989204407 - trainLoss: 0.6512792706489563\n",
      "cnt: 0 - valLoss: 0.6504800319671631 - trainLoss: 0.6512775421142578\n",
      "cnt: 0 - valLoss: 0.6504780650138855 - trainLoss: 0.6512757539749146\n",
      "cnt: 0 - valLoss: 0.6504760980606079 - trainLoss: 0.6512740254402161\n",
      "cnt: 0 - valLoss: 0.6504741311073303 - trainLoss: 0.6512722373008728\n",
      "cnt: 0 - valLoss: 0.6504720449447632 - trainLoss: 0.6512705683708191\n",
      "cnt: 0 - valLoss: 0.6504701972007751 - trainLoss: 0.651268720626831\n",
      "cnt: 0 - valLoss: 0.6504682898521423 - trainLoss: 0.6512671113014221\n",
      "cnt: 0 - valLoss: 0.6504662036895752 - trainLoss: 0.6512653827667236\n",
      "cnt: 0 - valLoss: 0.6504642963409424 - trainLoss: 0.6512636542320251\n",
      "cnt: 0 - valLoss: 0.65046226978302 - trainLoss: 0.6512618660926819\n",
      "cnt: 0 - valLoss: 0.6504603028297424 - trainLoss: 0.6512601971626282\n",
      "cnt: 0 - valLoss: 0.6504582762718201 - trainLoss: 0.6512584090232849\n",
      "cnt: 0 - valLoss: 0.6504563093185425 - trainLoss: 0.6512566804885864\n",
      "cnt: 0 - valLoss: 0.6504543423652649 - trainLoss: 0.6512548923492432\n",
      "cnt: 0 - valLoss: 0.6504524946212769 - trainLoss: 0.6512531638145447\n",
      "cnt: 0 - valLoss: 0.6504505276679993 - trainLoss: 0.6512514352798462\n",
      "cnt: 0 - valLoss: 0.6504485607147217 - trainLoss: 0.6512497663497925\n",
      "cnt: 0 - valLoss: 0.6504465341567993 - trainLoss: 0.6512479782104492\n",
      "cnt: 0 - valLoss: 0.6504445672035217 - trainLoss: 0.6512462496757507\n",
      "cnt: 0 - valLoss: 0.6504426002502441 - trainLoss: 0.6512444615364075\n",
      "cnt: 0 - valLoss: 0.650440514087677 - trainLoss: 0.651242733001709\n",
      "cnt: 0 - valLoss: 0.6504385471343994 - trainLoss: 0.6512409448623657\n",
      "cnt: 0 - valLoss: 0.6504365801811218 - trainLoss: 0.6512392163276672\n",
      "cnt: 0 - valLoss: 0.6504347920417786 - trainLoss: 0.651237428188324\n",
      "cnt: 0 - valLoss: 0.6504327654838562 - trainLoss: 0.651235818862915\n",
      "cnt: 0 - valLoss: 0.6504307389259338 - trainLoss: 0.6512340903282166\n",
      "cnt: 0 - valLoss: 0.650428831577301 - trainLoss: 0.6512323021888733\n",
      "cnt: 0 - valLoss: 0.6504268050193787 - trainLoss: 0.6512305736541748\n",
      "cnt: 0 - valLoss: 0.6504248380661011 - trainLoss: 0.6512287855148315\n",
      "cnt: 0 - valLoss: 0.6504227519035339 - trainLoss: 0.6512270569801331\n",
      "cnt: 0 - valLoss: 0.6504207849502563 - trainLoss: 0.6512252688407898\n",
      "cnt: 0 - valLoss: 0.6504189372062683 - trainLoss: 0.6512235403060913\n",
      "cnt: 0 - valLoss: 0.6504169702529907 - trainLoss: 0.651221752166748\n",
      "cnt: 0 - valLoss: 0.6504150629043579 - trainLoss: 0.6512201428413391\n",
      "cnt: 0 - valLoss: 0.6504130363464355 - trainLoss: 0.6512183547019958\n",
      "cnt: 0 - valLoss: 0.6504110097885132 - trainLoss: 0.6512166261672974\n",
      "cnt: 0 - valLoss: 0.6504089832305908 - trainLoss: 0.6512148380279541\n",
      "cnt: 0 - valLoss: 0.6504070162773132 - trainLoss: 0.6512131094932556\n",
      "cnt: 0 - valLoss: 0.6504049301147461 - trainLoss: 0.6512113213539124\n",
      "cnt: 0 - valLoss: 0.6504030823707581 - trainLoss: 0.6512095928192139\n",
      "cnt: 0 - valLoss: 0.6504011154174805 - trainLoss: 0.6512078046798706\n",
      "cnt: 0 - valLoss: 0.6503992676734924 - trainLoss: 0.6512061357498169\n",
      "cnt: 0 - valLoss: 0.6503972411155701 - trainLoss: 0.6512044072151184\n",
      "cnt: 0 - valLoss: 0.6503952145576477 - trainLoss: 0.6512026786804199\n",
      "cnt: 0 - valLoss: 0.6503931879997253 - trainLoss: 0.6512008309364319\n",
      "cnt: 0 - valLoss: 0.6503912210464478 - trainLoss: 0.6511991620063782\n",
      "cnt: 0 - valLoss: 0.6503891944885254 - trainLoss: 0.6511973738670349\n",
      "cnt: 0 - valLoss: 0.650387167930603 - trainLoss: 0.6511956453323364\n",
      "cnt: 0 - valLoss: 0.6503852605819702 - trainLoss: 0.6511938571929932\n",
      "cnt: 0 - valLoss: 0.6503834128379822 - trainLoss: 0.6511921286582947\n",
      "cnt: 0 - valLoss: 0.6503813862800598 - trainLoss: 0.6511904001235962\n",
      "cnt: 0 - valLoss: 0.650379478931427 - trainLoss: 0.6511886119842529\n",
      "cnt: 0 - valLoss: 0.6503774523735046 - trainLoss: 0.6511868834495544\n",
      "cnt: 0 - valLoss: 0.6503753662109375 - trainLoss: 0.651185154914856\n",
      "cnt: 0 - valLoss: 0.6503732800483704 - trainLoss: 0.6511834263801575\n",
      "cnt: 0 - valLoss: 0.6503713130950928 - trainLoss: 0.651181697845459\n",
      "cnt: 0 - valLoss: 0.6503692865371704 - trainLoss: 0.6511799097061157\n",
      "cnt: 0 - valLoss: 0.6503673791885376 - trainLoss: 0.6511781811714172\n",
      "cnt: 0 - valLoss: 0.6503654718399048 - trainLoss: 0.6511764526367188\n",
      "cnt: 0 - valLoss: 0.6503634452819824 - trainLoss: 0.6511747241020203\n",
      "cnt: 0 - valLoss: 0.6503614187240601 - trainLoss: 0.6511729955673218\n",
      "cnt: 0 - valLoss: 0.6503593921661377 - trainLoss: 0.6511712670326233\n",
      "cnt: 0 - valLoss: 0.6503573060035706 - trainLoss: 0.6511695384979248\n",
      "cnt: 0 - valLoss: 0.6503552794456482 - trainLoss: 0.6511677503585815\n",
      "cnt: 0 - valLoss: 0.6503532528877258 - trainLoss: 0.6511660218238831\n",
      "cnt: 0 - valLoss: 0.6503512859344482 - trainLoss: 0.651164174079895\n",
      "cnt: 0 - valLoss: 0.6503493189811707 - trainLoss: 0.6511625051498413\n",
      "cnt: 0 - valLoss: 0.6503474116325378 - trainLoss: 0.6511607766151428\n",
      "cnt: 0 - valLoss: 0.6503453850746155 - trainLoss: 0.6511591076850891\n",
      "cnt: 0 - valLoss: 0.6503433585166931 - trainLoss: 0.6511573195457458\n",
      "cnt: 0 - valLoss: 0.650341272354126 - trainLoss: 0.6511555910110474\n",
      "cnt: 0 - valLoss: 0.6503392457962036 - trainLoss: 0.6511538028717041\n",
      "cnt: 0 - valLoss: 0.6503372192382812 - trainLoss: 0.6511520743370056\n",
      "cnt: 0 - valLoss: 0.6503351926803589 - trainLoss: 0.6511502861976624\n",
      "cnt: 0 - valLoss: 0.6503331661224365 - trainLoss: 0.6511484980583191\n",
      "cnt: 0 - valLoss: 0.6503313183784485 - trainLoss: 0.6511467695236206\n",
      "cnt: 0 - valLoss: 0.6503293514251709 - trainLoss: 0.6511451601982117\n",
      "cnt: 0 - valLoss: 0.6503272652626038 - trainLoss: 0.6511433124542236\n",
      "cnt: 0 - valLoss: 0.6503252387046814 - trainLoss: 0.6511416435241699\n",
      "cnt: 0 - valLoss: 0.6503232717514038 - trainLoss: 0.6511397957801819\n",
      "cnt: 0 - valLoss: 0.6503211855888367 - trainLoss: 0.6511380672454834\n",
      "cnt: 0 - valLoss: 0.6503191590309143 - trainLoss: 0.6511363387107849\n",
      "cnt: 0 - valLoss: 0.6503171324729919 - trainLoss: 0.6511346101760864\n",
      "cnt: 0 - valLoss: 0.6503150463104248 - trainLoss: 0.6511328220367432\n",
      "cnt: 0 - valLoss: 0.6503133177757263 - trainLoss: 0.6511310935020447\n",
      "cnt: 0 - valLoss: 0.6503112316131592 - trainLoss: 0.651129424571991\n",
      "cnt: 0 - valLoss: 0.6503092050552368 - trainLoss: 0.6511276960372925\n",
      "cnt: 0 - valLoss: 0.6503071188926697 - trainLoss: 0.6511259078979492\n",
      "cnt: 0 - valLoss: 0.6503051519393921 - trainLoss: 0.651124119758606\n",
      "cnt: 0 - valLoss: 0.6503031253814697 - trainLoss: 0.6511224508285522\n",
      "cnt: 0 - valLoss: 0.6503010392189026 - trainLoss: 0.651120662689209\n",
      "cnt: 0 - valLoss: 0.6502990126609802 - trainLoss: 0.6511188745498657\n",
      "cnt: 0 - valLoss: 0.6502970457077026 - trainLoss: 0.6511170864105225\n",
      "cnt: 0 - valLoss: 0.6502951979637146 - trainLoss: 0.651115357875824\n",
      "cnt: 0 - valLoss: 0.6502931714057922 - trainLoss: 0.6511136889457703\n",
      "cnt: 0 - valLoss: 0.6502910852432251 - trainLoss: 0.651111900806427\n",
      "cnt: 0 - valLoss: 0.6502890586853027 - trainLoss: 0.6511101126670837\n",
      "cnt: 0 - valLoss: 0.6502870917320251 - trainLoss: 0.6511083841323853\n",
      "cnt: 0 - valLoss: 0.650285005569458 - trainLoss: 0.651106595993042\n",
      "cnt: 0 - valLoss: 0.6502829790115356 - trainLoss: 0.6511049270629883\n",
      "cnt: 0 - valLoss: 0.6502808928489685 - trainLoss: 0.6511030793190002\n",
      "cnt: 0 - valLoss: 0.6502789855003357 - trainLoss: 0.651101291179657\n",
      "cnt: 0 - valLoss: 0.6502770185470581 - trainLoss: 0.6510996222496033\n",
      "cnt: 0 - valLoss: 0.6502749919891357 - trainLoss: 0.6510979533195496\n",
      "cnt: 0 - valLoss: 0.6502730250358582 - trainLoss: 0.6510961651802063\n",
      "cnt: 0 - valLoss: 0.650270938873291 - trainLoss: 0.6510944366455078\n",
      "cnt: 0 - valLoss: 0.6502689123153687 - trainLoss: 0.6510926485061646\n",
      "cnt: 0 - valLoss: 0.6502668261528015 - trainLoss: 0.6510909199714661\n",
      "cnt: 0 - valLoss: 0.6502648591995239 - trainLoss: 0.6510891318321228\n",
      "cnt: 0 - valLoss: 0.6502628326416016 - trainLoss: 0.6510873436927795\n",
      "cnt: 0 - valLoss: 0.6502609252929688 - trainLoss: 0.6510855555534363\n",
      "cnt: 0 - valLoss: 0.6502588391304016 - trainLoss: 0.6510838866233826\n",
      "cnt: 0 - valLoss: 0.650256872177124 - trainLoss: 0.6510821580886841\n",
      "cnt: 0 - valLoss: 0.6502548456192017 - trainLoss: 0.6510804295539856\n",
      "cnt: 0 - valLoss: 0.6502527594566345 - trainLoss: 0.6510786414146423\n",
      "cnt: 0 - valLoss: 0.6502507925033569 - trainLoss: 0.6510768532752991\n",
      "cnt: 0 - valLoss: 0.6502487659454346 - trainLoss: 0.6510751247406006\n",
      "cnt: 0 - valLoss: 0.6502466797828674 - trainLoss: 0.6510733366012573\n",
      "cnt: 0 - valLoss: 0.6502446532249451 - trainLoss: 0.6510714888572693\n",
      "cnt: 0 - valLoss: 0.650242805480957 - trainLoss: 0.6510698795318604\n",
      "cnt: 0 - valLoss: 0.6502407789230347 - trainLoss: 0.6510681509971619\n",
      "cnt: 0 - valLoss: 0.6502387523651123 - trainLoss: 0.6510663628578186\n",
      "cnt: 0 - valLoss: 0.6502367258071899 - trainLoss: 0.6510646343231201\n",
      "cnt: 0 - valLoss: 0.6502346992492676 - trainLoss: 0.6510628461837769\n",
      "cnt: 0 - valLoss: 0.6502326130867004 - trainLoss: 0.6510610580444336\n",
      "cnt: 0 - valLoss: 0.6502305865287781 - trainLoss: 0.6510593295097351\n",
      "cnt: 0 - valLoss: 0.6502285003662109 - trainLoss: 0.6510575413703918\n",
      "cnt: 0 - valLoss: 0.6502266526222229 - trainLoss: 0.6510557532310486\n",
      "cnt: 0 - valLoss: 0.6502246856689453 - trainLoss: 0.6510540246963501\n",
      "cnt: 0 - valLoss: 0.650222659111023 - trainLoss: 0.6510523557662964\n",
      "cnt: 0 - valLoss: 0.6502206325531006 - trainLoss: 0.6510505676269531\n",
      "cnt: 0 - valLoss: 0.6502184867858887 - trainLoss: 0.6510488390922546\n",
      "cnt: 0 - valLoss: 0.6502165198326111 - trainLoss: 0.6510470509529114\n",
      "cnt: 0 - valLoss: 0.650214433670044 - trainLoss: 0.6510452628135681\n",
      "cnt: 0 - valLoss: 0.6502124071121216 - trainLoss: 0.6510435342788696\n",
      "cnt: 0 - valLoss: 0.6502103209495544 - trainLoss: 0.6510417461395264\n",
      "cnt: 0 - valLoss: 0.6502085328102112 - trainLoss: 0.6510398983955383\n",
      "cnt: 0 - valLoss: 0.6502065062522888 - trainLoss: 0.6510382890701294\n",
      "cnt: 0 - valLoss: 0.6502044200897217 - trainLoss: 0.6510365009307861\n",
      "cnt: 0 - valLoss: 0.6502023935317993 - trainLoss: 0.6510347127914429\n",
      "cnt: 0 - valLoss: 0.6502003073692322 - trainLoss: 0.6510329246520996\n",
      "cnt: 0 - valLoss: 0.6501982808113098 - trainLoss: 0.6510311365127563\n",
      "cnt: 0 - valLoss: 0.6501961946487427 - trainLoss: 0.6510294079780579\n",
      "cnt: 0 - valLoss: 0.6501941680908203 - trainLoss: 0.6510276794433594\n",
      "cnt: 0 - valLoss: 0.6501922607421875 - trainLoss: 0.6510258913040161\n",
      "cnt: 0 - valLoss: 0.6501902937889099 - trainLoss: 0.6510241031646729\n",
      "cnt: 0 - valLoss: 0.6501882076263428 - trainLoss: 0.6510224342346191\n",
      "cnt: 0 - valLoss: 0.6501861810684204 - trainLoss: 0.6510206460952759\n",
      "cnt: 0 - valLoss: 0.6501842141151428 - trainLoss: 0.6510188579559326\n",
      "cnt: 0 - valLoss: 0.6501821279525757 - trainLoss: 0.6510171294212341\n",
      "cnt: 0 - valLoss: 0.6501801013946533 - trainLoss: 0.6510153412818909\n",
      "cnt: 0 - valLoss: 0.6501780152320862 - trainLoss: 0.6510135531425476\n",
      "cnt: 0 - valLoss: 0.6501759886741638 - trainLoss: 0.6510117650032043\n",
      "cnt: 0 - valLoss: 0.6501741409301758 - trainLoss: 0.6510100364685059\n",
      "cnt: 0 - valLoss: 0.6501720547676086 - trainLoss: 0.6510083675384521\n",
      "cnt: 0 - valLoss: 0.650170087814331 - trainLoss: 0.6510065197944641\n",
      "cnt: 0 - valLoss: 0.6501679420471191 - trainLoss: 0.6510047912597656\n",
      "cnt: 0 - valLoss: 0.6501659154891968 - trainLoss: 0.6510030031204224\n",
      "cnt: 0 - valLoss: 0.6501638889312744 - trainLoss: 0.6510012149810791\n",
      "cnt: 0 - valLoss: 0.650161862373352 - trainLoss: 0.6509994268417358\n",
      "cnt: 0 - valLoss: 0.6501597762107849 - trainLoss: 0.6509976387023926\n",
      "cnt: 0 - valLoss: 0.6501578688621521 - trainLoss: 0.6509959101676941\n",
      "cnt: 0 - valLoss: 0.6501559019088745 - trainLoss: 0.6509941816329956\n",
      "cnt: 0 - valLoss: 0.6501538157463074 - trainLoss: 0.6509923934936523\n",
      "cnt: 0 - valLoss: 0.650151789188385 - trainLoss: 0.6509906053543091\n",
      "cnt: 0 - valLoss: 0.6501497626304626 - trainLoss: 0.6509888768196106\n",
      "cnt: 0 - valLoss: 0.6501476764678955 - trainLoss: 0.6509870886802673\n",
      "cnt: 0 - valLoss: 0.6501456499099731 - trainLoss: 0.6509853005409241\n",
      "cnt: 0 - valLoss: 0.650143563747406 - trainLoss: 0.6509835720062256\n",
      "cnt: 0 - valLoss: 0.6501415371894836 - trainLoss: 0.6509817242622375\n",
      "cnt: 0 - valLoss: 0.6501396894454956 - trainLoss: 0.6509799361228943\n",
      "cnt: 0 - valLoss: 0.6501376032829285 - trainLoss: 0.6509782671928406\n",
      "cnt: 0 - valLoss: 0.6501355767250061 - trainLoss: 0.6509764790534973\n",
      "cnt: 0 - valLoss: 0.6501335501670837 - trainLoss: 0.6509747505187988\n",
      "cnt: 0 - valLoss: 0.6501314640045166 - trainLoss: 0.6509729623794556\n",
      "cnt: 0 - valLoss: 0.6501294374465942 - trainLoss: 0.6509711742401123\n",
      "cnt: 0 - valLoss: 0.6501273512840271 - trainLoss: 0.6509694457054138\n",
      "cnt: 0 - valLoss: 0.6501253247261047 - trainLoss: 0.6509676575660706\n",
      "cnt: 0 - valLoss: 0.6501233577728271 - trainLoss: 0.6509658098220825\n",
      "cnt: 0 - valLoss: 0.6501214504241943 - trainLoss: 0.6509641408920288\n",
      "cnt: 0 - valLoss: 0.6501193642616272 - trainLoss: 0.6509623527526855\n",
      "cnt: 0 - valLoss: 0.6501172780990601 - trainLoss: 0.6509606242179871\n",
      "cnt: 0 - valLoss: 0.6501152515411377 - trainLoss: 0.6509588360786438\n",
      "cnt: 0 - valLoss: 0.6501132249832153 - trainLoss: 0.6509570479393005\n",
      "cnt: 0 - valLoss: 0.6501111388206482 - trainLoss: 0.6509552597999573\n",
      "cnt: 0 - valLoss: 0.6501091122627258 - trainLoss: 0.650953471660614\n",
      "cnt: 0 - valLoss: 0.6501070261001587 - trainLoss: 0.6509516835212708\n",
      "cnt: 0 - valLoss: 0.6501051783561707 - trainLoss: 0.6509498953819275\n",
      "cnt: 0 - valLoss: 0.6501030921936035 - trainLoss: 0.6509482264518738\n",
      "cnt: 0 - valLoss: 0.6501010656356812 - trainLoss: 0.6509464383125305\n",
      "cnt: 0 - valLoss: 0.650098979473114 - trainLoss: 0.6509446501731873\n",
      "cnt: 0 - valLoss: 0.6500968933105469 - trainLoss: 0.6509429216384888\n",
      "cnt: 0 - valLoss: 0.6500948667526245 - trainLoss: 0.6509410738945007\n",
      "cnt: 0 - valLoss: 0.6500928401947021 - trainLoss: 0.6509392857551575\n",
      "cnt: 0 - valLoss: 0.650090754032135 - trainLoss: 0.6509374976158142\n",
      "cnt: 0 - valLoss: 0.6500887870788574 - trainLoss: 0.6509357690811157\n",
      "cnt: 0 - valLoss: 0.6500868797302246 - trainLoss: 0.6509340405464172\n",
      "cnt: 0 - valLoss: 0.6500848531723022 - trainLoss: 0.650932252407074\n",
      "cnt: 0 - valLoss: 0.6500827670097351 - trainLoss: 0.6509305238723755\n",
      "cnt: 0 - valLoss: 0.6500806212425232 - trainLoss: 0.6509286761283875\n",
      "cnt: 0 - valLoss: 0.6500786542892456 - trainLoss: 0.6509268879890442\n",
      "cnt: 0 - valLoss: 0.6500765681266785 - trainLoss: 0.6509251594543457\n",
      "cnt: 0 - valLoss: 0.6500744819641113 - trainLoss: 0.6509233117103577\n",
      "cnt: 0 - valLoss: 0.6500725150108337 - trainLoss: 0.6509215235710144\n",
      "cnt: 0 - valLoss: 0.6500706076622009 - trainLoss: 0.6509197354316711\n",
      "cnt: 0 - valLoss: 0.6500685214996338 - trainLoss: 0.6509180665016174\n",
      "cnt: 0 - valLoss: 0.6500664949417114 - trainLoss: 0.6509162187576294\n",
      "cnt: 0 - valLoss: 0.6500644683837891 - trainLoss: 0.6509144306182861\n",
      "cnt: 0 - valLoss: 0.6500623822212219 - trainLoss: 0.6509126424789429\n",
      "cnt: 0 - valLoss: 0.65006023645401 - trainLoss: 0.6509108543395996\n",
      "cnt: 0 - valLoss: 0.6500582098960876 - trainLoss: 0.6509091258049011\n",
      "cnt: 0 - valLoss: 0.6500561237335205 - trainLoss: 0.6509073376655579\n",
      "cnt: 0 - valLoss: 0.6500542163848877 - trainLoss: 0.6509055495262146\n",
      "cnt: 0 - valLoss: 0.6500522494316101 - trainLoss: 0.6509038209915161\n",
      "cnt: 0 - valLoss: 0.6500501036643982 - trainLoss: 0.6509020328521729\n",
      "cnt: 0 - valLoss: 0.6500481367111206 - trainLoss: 0.6509002447128296\n",
      "cnt: 0 - valLoss: 0.6500460505485535 - trainLoss: 0.6508983969688416\n",
      "cnt: 0 - valLoss: 0.6500439643859863 - trainLoss: 0.6508966684341431\n",
      "cnt: 0 - valLoss: 0.6500418782234192 - trainLoss: 0.6508949398994446\n",
      "cnt: 0 - valLoss: 0.6500398516654968 - trainLoss: 0.6508930921554565\n",
      "cnt: 0 - valLoss: 0.6500378847122192 - trainLoss: 0.6508913040161133\n",
      "cnt: 0 - valLoss: 0.6500359177589417 - trainLoss: 0.65088951587677\n",
      "cnt: 0 - valLoss: 0.6500338912010193 - trainLoss: 0.6508877873420715\n",
      "cnt: 0 - valLoss: 0.6500318050384521 - trainLoss: 0.650886058807373\n",
      "cnt: 0 - valLoss: 0.650029718875885 - trainLoss: 0.650884211063385\n",
      "cnt: 0 - valLoss: 0.6500276923179626 - trainLoss: 0.6508823037147522\n",
      "cnt: 0 - valLoss: 0.6500256061553955 - trainLoss: 0.6508806347846985\n",
      "cnt: 0 - valLoss: 0.6500235795974731 - trainLoss: 0.6508788466453552\n",
      "cnt: 0 - valLoss: 0.6500215530395508 - trainLoss: 0.6508769989013672\n",
      "cnt: 0 - valLoss: 0.6500195860862732 - trainLoss: 0.6508753299713135\n",
      "cnt: 0 - valLoss: 0.6500175595283508 - trainLoss: 0.650873601436615\n",
      "cnt: 0 - valLoss: 0.6500154733657837 - trainLoss: 0.650871753692627\n",
      "cnt: 0 - valLoss: 0.6500134468078613 - trainLoss: 0.6508699655532837\n",
      "cnt: 0 - valLoss: 0.6500113606452942 - trainLoss: 0.6508681774139404\n",
      "cnt: 0 - valLoss: 0.650009274482727 - trainLoss: 0.6508663892745972\n",
      "cnt: 0 - valLoss: 0.6500071883201599 - trainLoss: 0.6508645415306091\n",
      "cnt: 0 - valLoss: 0.6500051617622375 - trainLoss: 0.6508627533912659\n",
      "cnt: 0 - valLoss: 0.6500032544136047 - trainLoss: 0.6508609652519226\n",
      "cnt: 0 - valLoss: 0.6500012278556824 - trainLoss: 0.6508592963218689\n",
      "cnt: 0 - valLoss: 0.6499991416931152 - trainLoss: 0.6508575081825256\n",
      "cnt: 0 - valLoss: 0.6499970555305481 - trainLoss: 0.6508557200431824\n",
      "cnt: 0 - valLoss: 0.6499950289726257 - trainLoss: 0.6508539319038391\n",
      "cnt: 0 - valLoss: 0.6499928832054138 - trainLoss: 0.6508520841598511\n",
      "cnt: 0 - valLoss: 0.6499909162521362 - trainLoss: 0.6508502960205078\n",
      "cnt: 0 - valLoss: 0.6499888300895691 - trainLoss: 0.6508485078811646\n",
      "cnt: 0 - valLoss: 0.6499868631362915 - trainLoss: 0.6508466601371765\n",
      "cnt: 0 - valLoss: 0.6499848961830139 - trainLoss: 0.6508449912071228\n",
      "cnt: 0 - valLoss: 0.649982750415802 - trainLoss: 0.6508432030677795\n",
      "cnt: 0 - valLoss: 0.6499807834625244 - trainLoss: 0.6508414149284363\n",
      "cnt: 0 - valLoss: 0.6499786376953125 - trainLoss: 0.650839626789093\n",
      "cnt: 0 - valLoss: 0.6499765515327454 - trainLoss: 0.6508378386497498\n",
      "cnt: 0 - valLoss: 0.6499744653701782 - trainLoss: 0.6508359909057617\n",
      "cnt: 0 - valLoss: 0.6499724388122559 - trainLoss: 0.6508342027664185\n",
      "cnt: 0 - valLoss: 0.6499704718589783 - trainLoss: 0.6508324146270752\n",
      "cnt: 0 - valLoss: 0.6499685049057007 - trainLoss: 0.6508306860923767\n",
      "cnt: 0 - valLoss: 0.6499664783477783 - trainLoss: 0.6508288979530334\n",
      "cnt: 0 - valLoss: 0.6499643921852112 - trainLoss: 0.6508271098136902\n",
      "cnt: 0 - valLoss: 0.6499622464179993 - trainLoss: 0.6508252620697021\n",
      "cnt: 0 - valLoss: 0.6499602198600769 - trainLoss: 0.6508234739303589\n",
      "cnt: 0 - valLoss: 0.6499581336975098 - trainLoss: 0.6508217453956604\n",
      "cnt: 0 - valLoss: 0.6499561071395874 - trainLoss: 0.6508198976516724\n",
      "cnt: 0 - valLoss: 0.6499540209770203 - trainLoss: 0.6508181691169739\n",
      "cnt: 0 - valLoss: 0.6499521136283875 - trainLoss: 0.6508162617683411\n",
      "cnt: 0 - valLoss: 0.6499500870704651 - trainLoss: 0.6508145928382874\n",
      "cnt: 0 - valLoss: 0.649948000907898 - trainLoss: 0.6508128046989441\n",
      "cnt: 0 - valLoss: 0.6499459147453308 - trainLoss: 0.650810956954956\n",
      "cnt: 0 - valLoss: 0.6499438285827637 - trainLoss: 0.6508092284202576\n",
      "cnt: 0 - valLoss: 0.6499418020248413 - trainLoss: 0.6508073806762695\n",
      "cnt: 0 - valLoss: 0.6499396562576294 - trainLoss: 0.6508055925369263\n",
      "cnt: 0 - valLoss: 0.6499376893043518 - trainLoss: 0.650803804397583\n",
      "cnt: 0 - valLoss: 0.6499356627464294 - trainLoss: 0.6508020162582397\n",
      "cnt: 0 - valLoss: 0.6499336957931519 - trainLoss: 0.6508002877235413\n",
      "cnt: 0 - valLoss: 0.6499315500259399 - trainLoss: 0.6507984399795532\n",
      "cnt: 0 - valLoss: 0.6499295234680176 - trainLoss: 0.65079665184021\n",
      "cnt: 0 - valLoss: 0.6499274373054504 - trainLoss: 0.6507948637008667\n",
      "cnt: 0 - valLoss: 0.6499253511428833 - trainLoss: 0.6507930755615234\n",
      "cnt: 0 - valLoss: 0.6499232649803162 - trainLoss: 0.6507912278175354\n",
      "cnt: 0 - valLoss: 0.649921178817749 - trainLoss: 0.6507894396781921\n",
      "cnt: 0 - valLoss: 0.649919331073761 - trainLoss: 0.6507875919342041\n",
      "cnt: 0 - valLoss: 0.6499172449111938 - trainLoss: 0.6507859230041504\n",
      "cnt: 0 - valLoss: 0.6499152183532715 - trainLoss: 0.6507841348648071\n",
      "cnt: 0 - valLoss: 0.6499130725860596 - trainLoss: 0.6507823467254639\n",
      "cnt: 0 - valLoss: 0.6499109864234924 - trainLoss: 0.650780439376831\n",
      "cnt: 0 - valLoss: 0.6499089598655701 - trainLoss: 0.6507786512374878\n",
      "cnt: 0 - valLoss: 0.6499068737030029 - trainLoss: 0.6507768630981445\n",
      "cnt: 0 - valLoss: 0.6499047875404358 - trainLoss: 0.6507750749588013\n",
      "cnt: 0 - valLoss: 0.649902880191803 - trainLoss: 0.6507732272148132\n",
      "cnt: 0 - valLoss: 0.6499008536338806 - trainLoss: 0.6507714986801147\n",
      "cnt: 0 - valLoss: 0.6498987674713135 - trainLoss: 0.6507697105407715\n",
      "cnt: 0 - valLoss: 0.6498966813087463 - trainLoss: 0.6507678627967834\n",
      "cnt: 0 - valLoss: 0.6498945951461792 - trainLoss: 0.6507661938667297\n",
      "cnt: 0 - valLoss: 0.6498925089836121 - trainLoss: 0.6507642865180969\n",
      "cnt: 0 - valLoss: 0.6498904228210449 - trainLoss: 0.6507624983787537\n",
      "cnt: 0 - valLoss: 0.6498883366584778 - trainLoss: 0.6507607102394104\n",
      "cnt: 0 - valLoss: 0.6498863697052002 - trainLoss: 0.6507588624954224\n",
      "cnt: 0 - valLoss: 0.6498844027519226 - trainLoss: 0.6507571339607239\n",
      "cnt: 0 - valLoss: 0.6498822569847107 - trainLoss: 0.6507553458213806\n",
      "cnt: 0 - valLoss: 0.6498802304267883 - trainLoss: 0.6507535576820374\n",
      "cnt: 0 - valLoss: 0.6498781442642212 - trainLoss: 0.6507517099380493\n",
      "cnt: 0 - valLoss: 0.6498759984970093 - trainLoss: 0.650749921798706\n",
      "cnt: 0 - valLoss: 0.6498740315437317 - trainLoss: 0.6507481336593628\n",
      "cnt: 0 - valLoss: 0.6498718857765198 - trainLoss: 0.65074622631073\n",
      "cnt: 0 - valLoss: 0.6498699188232422 - trainLoss: 0.6507444381713867\n",
      "cnt: 0 - valLoss: 0.6498679518699646 - trainLoss: 0.6507427096366882\n",
      "cnt: 0 - valLoss: 0.6498658657073975 - trainLoss: 0.650740921497345\n",
      "cnt: 0 - valLoss: 0.6498637795448303 - trainLoss: 0.6507391333580017\n",
      "cnt: 0 - valLoss: 0.6498616933822632 - trainLoss: 0.6507373452186584\n",
      "cnt: 0 - valLoss: 0.649859607219696 - trainLoss: 0.6507354974746704\n",
      "cnt: 0 - valLoss: 0.6498574614524841 - trainLoss: 0.6507337093353271\n",
      "cnt: 0 - valLoss: 0.6498553156852722 - trainLoss: 0.6507319211959839\n",
      "cnt: 0 - valLoss: 0.6498532891273499 - trainLoss: 0.6507300734519958\n",
      "cnt: 0 - valLoss: 0.6498513221740723 - trainLoss: 0.6507283449172974\n",
      "cnt: 0 - valLoss: 0.6498491764068604 - trainLoss: 0.6507265567779541\n",
      "cnt: 0 - valLoss: 0.6498470306396484 - trainLoss: 0.6507247686386108\n",
      "cnt: 0 - valLoss: 0.6498448848724365 - trainLoss: 0.6507229804992676\n",
      "cnt: 0 - valLoss: 0.6498427987098694 - trainLoss: 0.6507211923599243\n",
      "cnt: 0 - valLoss: 0.6498406529426575 - trainLoss: 0.650719404220581\n",
      "cnt: 0 - valLoss: 0.6498385071754456 - trainLoss: 0.650717556476593\n",
      "cnt: 0 - valLoss: 0.6498363614082336 - trainLoss: 0.6507157683372498\n",
      "cnt: 0 - valLoss: 0.6498345136642456 - trainLoss: 0.6507139801979065\n",
      "cnt: 0 - valLoss: 0.6498323678970337 - trainLoss: 0.6507123112678528\n",
      "cnt: 0 - valLoss: 0.6498302221298218 - trainLoss: 0.6507105231285095\n",
      "cnt: 0 - valLoss: 0.6498281359672546 - trainLoss: 0.6507087349891663\n",
      "cnt: 0 - valLoss: 0.6498259902000427 - trainLoss: 0.650706946849823\n",
      "cnt: 0 - valLoss: 0.6498238444328308 - trainLoss: 0.6507051587104797\n",
      "cnt: 0 - valLoss: 0.6498216986656189 - trainLoss: 0.6507033109664917\n",
      "cnt: 0 - valLoss: 0.649819552898407 - trainLoss: 0.6507014632225037\n",
      "cnt: 0 - valLoss: 0.6498175859451294 - trainLoss: 0.6506996750831604\n",
      "cnt: 0 - valLoss: 0.6498154997825623 - trainLoss: 0.6506979465484619\n",
      "cnt: 0 - valLoss: 0.6498133540153503 - trainLoss: 0.6506962180137634\n",
      "cnt: 0 - valLoss: 0.6498112678527832 - trainLoss: 0.6506943702697754\n",
      "cnt: 0 - valLoss: 0.6498090624809265 - trainLoss: 0.6506925225257874\n",
      "cnt: 0 - valLoss: 0.6498070359230042 - trainLoss: 0.6506907343864441\n",
      "cnt: 0 - valLoss: 0.6498047709465027 - trainLoss: 0.6506889462471008\n",
      "cnt: 0 - valLoss: 0.6498026251792908 - trainLoss: 0.6506870985031128\n",
      "cnt: 0 - valLoss: 0.6498006582260132 - trainLoss: 0.6506853699684143\n",
      "cnt: 0 - valLoss: 0.6497986316680908 - trainLoss: 0.650683581829071\n",
      "cnt: 0 - valLoss: 0.6497965455055237 - trainLoss: 0.6506818532943726\n",
      "cnt: 0 - valLoss: 0.6497943997383118 - trainLoss: 0.6506800651550293\n",
      "cnt: 0 - valLoss: 0.6497922539710999 - trainLoss: 0.650678277015686\n",
      "cnt: 0 - valLoss: 0.6497901082038879 - trainLoss: 0.650676429271698\n",
      "cnt: 0 - valLoss: 0.6497879028320312 - trainLoss: 0.6506746411323547\n",
      "cnt: 0 - valLoss: 0.6497857570648193 - trainLoss: 0.6506727933883667\n",
      "cnt: 0 - valLoss: 0.6497836709022522 - trainLoss: 0.6506710052490234\n",
      "cnt: 0 - valLoss: 0.6497817635536194 - trainLoss: 0.6506691575050354\n",
      "cnt: 0 - valLoss: 0.6497796773910522 - trainLoss: 0.6506674885749817\n",
      "cnt: 0 - valLoss: 0.6497774720191956 - trainLoss: 0.6506657004356384\n",
      "cnt: 0 - valLoss: 0.6497753262519836 - trainLoss: 0.6506638526916504\n",
      "cnt: 0 - valLoss: 0.6497732400894165 - trainLoss: 0.6506620645523071\n",
      "cnt: 0 - valLoss: 0.6497710943222046 - trainLoss: 0.6506602764129639\n",
      "cnt: 0 - valLoss: 0.6497688889503479 - trainLoss: 0.6506584286689758\n",
      "cnt: 0 - valLoss: 0.649766743183136 - trainLoss: 0.6506565809249878\n",
      "cnt: 0 - valLoss: 0.6497647762298584 - trainLoss: 0.6506547927856445\n",
      "cnt: 0 - valLoss: 0.6497626900672913 - trainLoss: 0.650653064250946\n",
      "cnt: 0 - valLoss: 0.6497605443000793 - trainLoss: 0.6506513357162476\n",
      "cnt: 0 - valLoss: 0.6497583985328674 - trainLoss: 0.6506494879722595\n",
      "cnt: 0 - valLoss: 0.6497562527656555 - trainLoss: 0.6506476998329163\n",
      "cnt: 0 - valLoss: 0.6497541069984436 - trainLoss: 0.650645911693573\n",
      "cnt: 0 - valLoss: 0.6497519612312317 - trainLoss: 0.6506440043449402\n",
      "cnt: 0 - valLoss: 0.6497498154640198 - trainLoss: 0.6506422758102417\n",
      "cnt: 0 - valLoss: 0.6497477889060974 - trainLoss: 0.6506404280662537\n",
      "cnt: 0 - valLoss: 0.6497458219528198 - trainLoss: 0.6506386399269104\n",
      "cnt: 0 - valLoss: 0.6497436761856079 - trainLoss: 0.6506369113922119\n",
      "cnt: 0 - valLoss: 0.649741530418396 - trainLoss: 0.6506351232528687\n",
      "cnt: 0 - valLoss: 0.6497393846511841 - trainLoss: 0.6506332755088806\n",
      "cnt: 0 - valLoss: 0.6497372388839722 - trainLoss: 0.6506314873695374\n",
      "cnt: 0 - valLoss: 0.6497350931167603 - trainLoss: 0.6506296992301941\n",
      "cnt: 0 - valLoss: 0.6497328877449036 - trainLoss: 0.650627851486206\n",
      "cnt: 0 - valLoss: 0.6497308611869812 - trainLoss: 0.6506260633468628\n",
      "cnt: 0 - valLoss: 0.6497288942337036 - trainLoss: 0.6506242752075195\n",
      "cnt: 0 - valLoss: 0.6497266888618469 - trainLoss: 0.650622546672821\n",
      "cnt: 0 - valLoss: 0.6497246026992798 - trainLoss: 0.650620698928833\n",
      "cnt: 0 - valLoss: 0.6497224569320679 - trainLoss: 0.650618851184845\n",
      "cnt: 0 - valLoss: 0.649720311164856 - trainLoss: 0.6506170630455017\n",
      "cnt: 0 - valLoss: 0.6497181057929993 - trainLoss: 0.6506152749061584\n",
      "cnt: 0 - valLoss: 0.6497159600257874 - trainLoss: 0.6506134271621704\n",
      "cnt: 0 - valLoss: 0.6497138142585754 - trainLoss: 0.6506115794181824\n",
      "cnt: 0 - valLoss: 0.6497119665145874 - trainLoss: 0.6506098508834839\n",
      "cnt: 0 - valLoss: 0.6497097611427307 - trainLoss: 0.6506081223487854\n",
      "cnt: 0 - valLoss: 0.6497076153755188 - trainLoss: 0.6506062746047974\n",
      "cnt: 0 - valLoss: 0.6497054696083069 - trainLoss: 0.6506044864654541\n",
      "cnt: 0 - valLoss: 0.649703323841095 - trainLoss: 0.6506026387214661\n",
      "cnt: 0 - valLoss: 0.6497012376785278 - trainLoss: 0.6506008505821228\n",
      "cnt: 0 - valLoss: 0.6496990919113159 - trainLoss: 0.6505990028381348\n",
      "cnt: 0 - valLoss: 0.6496968865394592 - trainLoss: 0.6505972146987915\n",
      "cnt: 0 - valLoss: 0.6496949195861816 - trainLoss: 0.6505953669548035\n",
      "cnt: 0 - valLoss: 0.6496928334236145 - trainLoss: 0.650593638420105\n",
      "cnt: 0 - valLoss: 0.6496906876564026 - trainLoss: 0.6505918502807617\n",
      "cnt: 0 - valLoss: 0.6496885418891907 - trainLoss: 0.6505900621414185\n",
      "cnt: 0 - valLoss: 0.6496863961219788 - trainLoss: 0.6505882143974304\n",
      "cnt: 0 - valLoss: 0.6496842503547668 - trainLoss: 0.6505864262580872\n",
      "cnt: 0 - valLoss: 0.6496821045875549 - trainLoss: 0.6505845785140991\n",
      "cnt: 0 - valLoss: 0.649679958820343 - trainLoss: 0.6505827307701111\n",
      "cnt: 0 - valLoss: 0.6496779322624207 - trainLoss: 0.6505809426307678\n",
      "cnt: 0 - valLoss: 0.6496758460998535 - trainLoss: 0.6505791544914246\n",
      "cnt: 0 - valLoss: 0.6496737003326416 - trainLoss: 0.6505774259567261\n",
      "cnt: 0 - valLoss: 0.6496715545654297 - trainLoss: 0.6505756378173828\n",
      "cnt: 0 - valLoss: 0.6496694087982178 - trainLoss: 0.6505737900733948\n",
      "cnt: 0 - valLoss: 0.6496672630310059 - trainLoss: 0.6505719423294067\n",
      "cnt: 0 - valLoss: 0.6496650576591492 - trainLoss: 0.6505700945854187\n",
      "cnt: 0 - valLoss: 0.6496629118919373 - trainLoss: 0.6505682468414307\n",
      "cnt: 0 - valLoss: 0.6496608853340149 - trainLoss: 0.6505664587020874\n",
      "cnt: 0 - valLoss: 0.6496587991714478 - trainLoss: 0.6505646705627441\n",
      "cnt: 0 - valLoss: 0.6496565937995911 - trainLoss: 0.6505630016326904\n",
      "cnt: 0 - valLoss: 0.6496545076370239 - trainLoss: 0.6505611538887024\n",
      "cnt: 0 - valLoss: 0.649652361869812 - trainLoss: 0.6505593061447144\n",
      "cnt: 0 - valLoss: 0.6496501564979553 - trainLoss: 0.6505574584007263\n",
      "cnt: 0 - valLoss: 0.6496480107307434 - trainLoss: 0.6505556702613831\n",
      "cnt: 0 - valLoss: 0.6496458649635315 - trainLoss: 0.650553822517395\n",
      "cnt: 0 - valLoss: 0.6496437788009644 - trainLoss: 0.650551974773407\n",
      "cnt: 0 - valLoss: 0.6496416926383972 - trainLoss: 0.6505502462387085\n",
      "cnt: 0 - valLoss: 0.6496395468711853 - trainLoss: 0.6505484580993652\n",
      "cnt: 0 - valLoss: 0.6496374011039734 - trainLoss: 0.650546669960022\n",
      "cnt: 0 - valLoss: 0.6496352553367615 - trainLoss: 0.6505448222160339\n",
      "cnt: 0 - valLoss: 0.6496331095695496 - trainLoss: 0.6505429744720459\n",
      "cnt: 0 - valLoss: 0.6496309041976929 - trainLoss: 0.6505411863327026\n",
      "cnt: 0 - valLoss: 0.649628758430481 - trainLoss: 0.6505393385887146\n",
      "cnt: 0 - valLoss: 0.6496266722679138 - trainLoss: 0.6505374908447266\n",
      "cnt: 0 - valLoss: 0.6496246457099915 - trainLoss: 0.6505357027053833\n",
      "cnt: 0 - valLoss: 0.6496224999427795 - trainLoss: 0.6505340337753296\n",
      "cnt: 0 - valLoss: 0.6496202945709229 - trainLoss: 0.6505321860313416\n",
      "cnt: 0 - valLoss: 0.6496181488037109 - trainLoss: 0.6505303978919983\n",
      "cnt: 0 - valLoss: 0.6496159434318542 - trainLoss: 0.6505284905433655\n",
      "cnt: 0 - valLoss: 0.6496137976646423 - trainLoss: 0.6505267024040222\n",
      "cnt: 0 - valLoss: 0.6496116518974304 - trainLoss: 0.6505248546600342\n",
      "cnt: 0 - valLoss: 0.6496095061302185 - trainLoss: 0.6505230665206909\n",
      "cnt: 0 - valLoss: 0.6496074795722961 - trainLoss: 0.6505212187767029\n",
      "cnt: 0 - valLoss: 0.6496053338050842 - trainLoss: 0.6505194902420044\n",
      "cnt: 0 - valLoss: 0.6496031880378723 - trainLoss: 0.6505176424980164\n",
      "cnt: 0 - valLoss: 0.6496010422706604 - trainLoss: 0.6505158543586731\n",
      "cnt: 0 - valLoss: 0.6495988965034485 - trainLoss: 0.6505140066146851\n",
      "cnt: 0 - valLoss: 0.6495967507362366 - trainLoss: 0.650512158870697\n",
      "cnt: 0 - valLoss: 0.6495946049690247 - trainLoss: 0.6505103707313538\n",
      "cnt: 0 - valLoss: 0.6495925188064575 - trainLoss: 0.6505085825920105\n",
      "cnt: 0 - valLoss: 0.6495905518531799 - trainLoss: 0.6505067348480225\n",
      "cnt: 0 - valLoss: 0.649588406085968 - trainLoss: 0.650505006313324\n",
      "cnt: 0 - valLoss: 0.6495862603187561 - trainLoss: 0.6505032181739807\n",
      "cnt: 0 - valLoss: 0.6495841145515442 - trainLoss: 0.6505013704299927\n",
      "cnt: 0 - valLoss: 0.6495819687843323 - trainLoss: 0.6504995822906494\n",
      "cnt: 0 - valLoss: 0.6495797634124756 - trainLoss: 0.6504976749420166\n",
      "cnt: 0 - valLoss: 0.6495776772499084 - trainLoss: 0.6504958271980286\n",
      "cnt: 0 - valLoss: 0.6495754718780518 - trainLoss: 0.6504940390586853\n",
      "cnt: 0 - valLoss: 0.649573564529419 - trainLoss: 0.650492250919342\n",
      "cnt: 0 - valLoss: 0.6495714783668518 - trainLoss: 0.6504904627799988\n",
      "cnt: 0 - valLoss: 0.6495692133903503 - trainLoss: 0.6504886746406555\n",
      "cnt: 0 - valLoss: 0.6495671272277832 - trainLoss: 0.6504868268966675\n",
      "cnt: 0 - valLoss: 0.6495649218559265 - trainLoss: 0.6504849791526794\n",
      "cnt: 0 - valLoss: 0.6495627760887146 - trainLoss: 0.6504831910133362\n",
      "cnt: 0 - valLoss: 0.6495606303215027 - trainLoss: 0.6504813432693481\n",
      "cnt: 0 - valLoss: 0.6495586037635803 - trainLoss: 0.6504794955253601\n",
      "cnt: 0 - valLoss: 0.6495566368103027 - trainLoss: 0.6504777073860168\n",
      "cnt: 0 - valLoss: 0.649554431438446 - trainLoss: 0.6504759788513184\n",
      "cnt: 0 - valLoss: 0.6495522260665894 - trainLoss: 0.6504741907119751\n",
      "cnt: 0 - valLoss: 0.6495500802993774 - trainLoss: 0.6504722833633423\n",
      "cnt: 0 - valLoss: 0.6495479345321655 - trainLoss: 0.650470495223999\n",
      "cnt: 0 - valLoss: 0.6495458483695984 - trainLoss: 0.6504687070846558\n",
      "cnt: 0 - valLoss: 0.6495435833930969 - trainLoss: 0.650466799736023\n",
      "cnt: 0 - valLoss: 0.649541437625885 - trainLoss: 0.6504649519920349\n",
      "cnt: 0 - valLoss: 0.6495394706726074 - trainLoss: 0.6504632234573364\n",
      "cnt: 0 - valLoss: 0.6495373249053955 - trainLoss: 0.6504614949226379\n",
      "cnt: 0 - valLoss: 0.6495351195335388 - trainLoss: 0.6504596471786499\n",
      "cnt: 0 - valLoss: 0.6495329141616821 - trainLoss: 0.6504577994346619\n",
      "cnt: 0 - valLoss: 0.6495307683944702 - trainLoss: 0.6504560112953186\n",
      "cnt: 0 - valLoss: 0.6495285630226135 - trainLoss: 0.6504541039466858\n",
      "cnt: 0 - valLoss: 0.6495263576507568 - trainLoss: 0.6504522562026978\n",
      "cnt: 0 - valLoss: 0.6495243310928345 - trainLoss: 0.6504504680633545\n",
      "cnt: 0 - valLoss: 0.6495223045349121 - trainLoss: 0.6504486799240112\n",
      "cnt: 0 - valLoss: 0.6495201587677002 - trainLoss: 0.6504469513893127\n",
      "cnt: 0 - valLoss: 0.6495179533958435 - trainLoss: 0.6504451036453247\n",
      "cnt: 0 - valLoss: 0.6495157480239868 - trainLoss: 0.6504432559013367\n",
      "cnt: 0 - valLoss: 0.6495135426521301 - trainLoss: 0.6504414081573486\n",
      "cnt: 0 - valLoss: 0.6495113372802734 - trainLoss: 0.6504395604133606\n",
      "cnt: 0 - valLoss: 0.6495091915130615 - trainLoss: 0.6504377126693726\n",
      "cnt: 0 - valLoss: 0.6495070457458496 - trainLoss: 0.6504359245300293\n",
      "cnt: 0 - valLoss: 0.649505078792572 - trainLoss: 0.650434136390686\n",
      "cnt: 0 - valLoss: 0.6495029330253601 - trainLoss: 0.6504324078559875\n",
      "cnt: 0 - valLoss: 0.6495007276535034 - trainLoss: 0.6504305601119995\n",
      "cnt: 0 - valLoss: 0.6494985818862915 - trainLoss: 0.6504287123680115\n",
      "cnt: 0 - valLoss: 0.64949631690979 - trainLoss: 0.6504268646240234\n",
      "cnt: 0 - valLoss: 0.6494941711425781 - trainLoss: 0.6504250168800354\n",
      "cnt: 0 - valLoss: 0.6494920253753662 - trainLoss: 0.6504231691360474\n",
      "cnt: 0 - valLoss: 0.6494898200035095 - trainLoss: 0.6504213809967041\n",
      "cnt: 0 - valLoss: 0.6494879126548767 - trainLoss: 0.6504195332527161\n",
      "cnt: 0 - valLoss: 0.64948570728302 - trainLoss: 0.6504178643226624\n",
      "cnt: 0 - valLoss: 0.6494835615158081 - trainLoss: 0.6504160761833191\n",
      "cnt: 0 - valLoss: 0.6494812965393066 - trainLoss: 0.6504141688346863\n",
      "cnt: 0 - valLoss: 0.6494791507720947 - trainLoss: 0.6504123210906982\n",
      "cnt: 0 - valLoss: 0.6494770050048828 - trainLoss: 0.650410532951355\n",
      "cnt: 0 - valLoss: 0.6494747400283813 - trainLoss: 0.6504086852073669\n",
      "cnt: 0 - valLoss: 0.6494726538658142 - trainLoss: 0.6504068374633789\n",
      "cnt: 0 - valLoss: 0.6494706273078918 - trainLoss: 0.6504049897193909\n",
      "cnt: 0 - valLoss: 0.6494684815406799 - trainLoss: 0.6504033207893372\n",
      "cnt: 0 - valLoss: 0.6494662761688232 - trainLoss: 0.6504014134407043\n",
      "cnt: 0 - valLoss: 0.6494641304016113 - trainLoss: 0.6503996253013611\n",
      "cnt: 0 - valLoss: 0.6494619250297546 - trainLoss: 0.6503977179527283\n",
      "cnt: 0 - valLoss: 0.649459719657898 - trainLoss: 0.6503958702087402\n",
      "cnt: 0 - valLoss: 0.649457573890686 - trainLoss: 0.6503940224647522\n",
      "cnt: 0 - valLoss: 0.6494553685188293 - trainLoss: 0.6503922343254089\n",
      "cnt: 0 - valLoss: 0.6494534015655518 - trainLoss: 0.6503904461860657\n",
      "cnt: 0 - valLoss: 0.6494511961936951 - trainLoss: 0.6503887176513672\n",
      "cnt: 0 - valLoss: 0.6494490504264832 - trainLoss: 0.6503868699073792\n",
      "cnt: 0 - valLoss: 0.6494469046592712 - trainLoss: 0.6503849625587463\n",
      "cnt: 0 - valLoss: 0.6494446396827698 - trainLoss: 0.6503831744194031\n",
      "cnt: 0 - valLoss: 0.6494424939155579 - trainLoss: 0.650381326675415\n",
      "cnt: 0 - valLoss: 0.6494402885437012 - trainLoss: 0.650379478931427\n",
      "cnt: 0 - valLoss: 0.6494380831718445 - trainLoss: 0.6503776907920837\n",
      "cnt: 0 - valLoss: 0.6494361162185669 - trainLoss: 0.6503758430480957\n",
      "cnt: 0 - valLoss: 0.649433970451355 - trainLoss: 0.6503741145133972\n",
      "cnt: 0 - valLoss: 0.6494317650794983 - trainLoss: 0.650372326374054\n",
      "cnt: 0 - valLoss: 0.6494295597076416 - trainLoss: 0.6503703594207764\n",
      "cnt: 0 - valLoss: 0.6494274139404297 - trainLoss: 0.6503685116767883\n",
      "cnt: 0 - valLoss: 0.649425208568573 - trainLoss: 0.6503667235374451\n",
      "cnt: 0 - valLoss: 0.6494230031967163 - trainLoss: 0.650364875793457\n",
      "cnt: 0 - valLoss: 0.6494208574295044 - trainLoss: 0.6503630876541138\n",
      "cnt: 0 - valLoss: 0.6494188904762268 - trainLoss: 0.650361180305481\n",
      "cnt: 0 - valLoss: 0.6494166851043701 - trainLoss: 0.6503594517707825\n",
      "cnt: 0 - valLoss: 0.6494144797325134 - trainLoss: 0.6503576636314392\n",
      "cnt: 0 - valLoss: 0.6494122743606567 - trainLoss: 0.6503557562828064\n",
      "cnt: 0 - valLoss: 0.6494101285934448 - trainLoss: 0.6503539681434631\n",
      "cnt: 0 - valLoss: 0.6494078636169434 - trainLoss: 0.6503521203994751\n",
      "cnt: 0 - valLoss: 0.6494057178497314 - trainLoss: 0.6503502726554871\n",
      "cnt: 0 - valLoss: 0.6494035124778748 - trainLoss: 0.6503483653068542\n",
      "cnt: 0 - valLoss: 0.6494015455245972 - trainLoss: 0.6503465175628662\n",
      "cnt: 0 - valLoss: 0.6493993997573853 - trainLoss: 0.6503448486328125\n",
      "cnt: 0 - valLoss: 0.6493971943855286 - trainLoss: 0.6503430008888245\n",
      "cnt: 0 - valLoss: 0.6493949890136719 - trainLoss: 0.6503411531448364\n",
      "cnt: 0 - valLoss: 0.6493927836418152 - trainLoss: 0.6503392457962036\n",
      "cnt: 0 - valLoss: 0.6493905782699585 - trainLoss: 0.6503374576568604\n",
      "cnt: 0 - valLoss: 0.6493883728981018 - trainLoss: 0.6503356099128723\n",
      "cnt: 0 - valLoss: 0.6493862271308899 - trainLoss: 0.6503337621688843\n",
      "cnt: 0 - valLoss: 0.6493842601776123 - trainLoss: 0.6503319144248962\n",
      "cnt: 0 - valLoss: 0.6493821144104004 - trainLoss: 0.6503302454948425\n",
      "cnt: 0 - valLoss: 0.6493798494338989 - trainLoss: 0.6503283381462097\n",
      "cnt: 0 - valLoss: 0.649377703666687 - trainLoss: 0.6503264904022217\n",
      "cnt: 0 - valLoss: 0.6493754982948303 - trainLoss: 0.6503246426582336\n",
      "cnt: 0 - valLoss: 0.6493732333183289 - trainLoss: 0.6503227949142456\n",
      "cnt: 0 - valLoss: 0.6493711471557617 - trainLoss: 0.6503209471702576\n",
      "cnt: 0 - valLoss: 0.649368941783905 - trainLoss: 0.6503190994262695\n",
      "cnt: 0 - valLoss: 0.6493669152259827 - trainLoss: 0.6503172516822815\n",
      "cnt: 0 - valLoss: 0.649364709854126 - trainLoss: 0.650315523147583\n",
      "cnt: 0 - valLoss: 0.6493625640869141 - trainLoss: 0.6503137350082397\n",
      "cnt: 0 - valLoss: 0.6493604183197021 - trainLoss: 0.6503118872642517\n",
      "cnt: 0 - valLoss: 0.6493581533432007 - trainLoss: 0.6503099799156189\n",
      "cnt: 0 - valLoss: 0.649355947971344 - trainLoss: 0.6503081321716309\n",
      "cnt: 0 - valLoss: 0.6493537425994873 - trainLoss: 0.650306224822998\n",
      "cnt: 0 - valLoss: 0.6493515968322754 - trainLoss: 0.6503044366836548\n",
      "cnt: 0 - valLoss: 0.649349570274353 - trainLoss: 0.6503026485443115\n",
      "cnt: 0 - valLoss: 0.6493473649024963 - trainLoss: 0.6503008604049683\n",
      "cnt: 0 - valLoss: 0.6493452191352844 - trainLoss: 0.6502990126609802\n",
      "cnt: 0 - valLoss: 0.649342954158783 - trainLoss: 0.6502971649169922\n",
      "cnt: 0 - valLoss: 0.6493407487869263 - trainLoss: 0.6502952575683594\n",
      "cnt: 0 - valLoss: 0.6493385434150696 - trainLoss: 0.6502934098243713\n",
      "cnt: 0 - valLoss: 0.6493363976478577 - trainLoss: 0.6502915620803833\n",
      "cnt: 0 - valLoss: 0.6493343114852905 - trainLoss: 0.6502897143363953\n",
      "cnt: 0 - valLoss: 0.6493322253227234 - trainLoss: 0.650287926197052\n",
      "cnt: 0 - valLoss: 0.6493300795555115 - trainLoss: 0.6502861380577087\n",
      "cnt: 0 - valLoss: 0.6493278741836548 - trainLoss: 0.6502842903137207\n",
      "cnt: 0 - valLoss: 0.6493256092071533 - trainLoss: 0.6502824425697327\n",
      "cnt: 0 - valLoss: 0.6493233442306519 - trainLoss: 0.6502806544303894\n",
      "cnt: 0 - valLoss: 0.6493211984634399 - trainLoss: 0.6502787470817566\n",
      "cnt: 0 - valLoss: 0.6493189930915833 - trainLoss: 0.6502768397331238\n",
      "cnt: 0 - valLoss: 0.6493169665336609 - trainLoss: 0.6502749919891357\n",
      "cnt: 0 - valLoss: 0.6493148803710938 - trainLoss: 0.6502732634544373\n",
      "cnt: 0 - valLoss: 0.6493126749992371 - trainLoss: 0.6502714157104492\n",
      "cnt: 0 - valLoss: 0.6493104696273804 - trainLoss: 0.6502695679664612\n",
      "cnt: 0 - valLoss: 0.6493082642555237 - trainLoss: 0.6502677798271179\n",
      "cnt: 0 - valLoss: 0.649306058883667 - trainLoss: 0.6502658128738403\n",
      "cnt: 0 - valLoss: 0.6493037939071655 - trainLoss: 0.6502639651298523\n",
      "cnt: 0 - valLoss: 0.6493016481399536 - trainLoss: 0.6502621173858643\n",
      "cnt: 0 - valLoss: 0.6492995619773865 - trainLoss: 0.650260329246521\n",
      "cnt: 0 - valLoss: 0.6492974758148193 - trainLoss: 0.6502585411071777\n",
      "cnt: 0 - valLoss: 0.6492952704429626 - trainLoss: 0.6502567529678345\n",
      "cnt: 0 - valLoss: 0.6492930054664612 - trainLoss: 0.6502548456192017\n",
      "cnt: 0 - valLoss: 0.6492908596992493 - trainLoss: 0.6502529978752136\n",
      "cnt: 0 - valLoss: 0.6492886543273926 - trainLoss: 0.6502511501312256\n",
      "cnt: 0 - valLoss: 0.6492864489555359 - trainLoss: 0.6502492427825928\n",
      "cnt: 0 - valLoss: 0.6492842435836792 - trainLoss: 0.6502474546432495\n",
      "cnt: 0 - valLoss: 0.6492822170257568 - trainLoss: 0.6502455472946167\n",
      "cnt: 0 - valLoss: 0.6492800712585449 - trainLoss: 0.650243878364563\n",
      "cnt: 0 - valLoss: 0.6492778658866882 - trainLoss: 0.6502419710159302\n",
      "cnt: 0 - valLoss: 0.6492756009101868 - trainLoss: 0.6502401232719421\n",
      "cnt: 0 - valLoss: 0.6492734551429749 - trainLoss: 0.6502382755279541\n",
      "cnt: 0 - valLoss: 0.6492711901664734 - trainLoss: 0.6502363681793213\n",
      "cnt: 0 - valLoss: 0.6492690443992615 - trainLoss: 0.650234580039978\n",
      "cnt: 0 - valLoss: 0.6492668390274048 - trainLoss: 0.6502326726913452\n",
      "cnt: 0 - valLoss: 0.6492648124694824 - trainLoss: 0.6502308249473572\n",
      "cnt: 0 - valLoss: 0.6492626070976257 - trainLoss: 0.6502290964126587\n",
      "cnt: 0 - valLoss: 0.6492604613304138 - trainLoss: 0.6502271890640259\n",
      "cnt: 0 - valLoss: 0.6492581963539124 - trainLoss: 0.6502253413200378\n",
      "cnt: 0 - valLoss: 0.6492559909820557 - trainLoss: 0.6502235531806946\n",
      "cnt: 0 - valLoss: 0.649253785610199 - trainLoss: 0.6502216458320618\n",
      "cnt: 0 - valLoss: 0.6492515802383423 - trainLoss: 0.6502197980880737\n",
      "cnt: 0 - valLoss: 0.6492494940757751 - trainLoss: 0.6502178907394409\n",
      "cnt: 0 - valLoss: 0.6492473483085632 - trainLoss: 0.6502161026000977\n",
      "cnt: 0 - valLoss: 0.6492452025413513 - trainLoss: 0.6502143144607544\n",
      "cnt: 0 - valLoss: 0.6492429375648499 - trainLoss: 0.6502124667167664\n",
      "cnt: 0 - valLoss: 0.6492407321929932 - trainLoss: 0.6502106189727783\n",
      "cnt: 0 - valLoss: 0.6492385268211365 - trainLoss: 0.6502087712287903\n",
      "cnt: 0 - valLoss: 0.6492363810539246 - trainLoss: 0.6502068638801575\n",
      "cnt: 0 - valLoss: 0.6492341160774231 - trainLoss: 0.6502050161361694\n",
      "cnt: 0 - valLoss: 0.6492320895195007 - trainLoss: 0.6502031683921814\n",
      "cnt: 0 - valLoss: 0.6492299437522888 - trainLoss: 0.6502013802528381\n",
      "cnt: 0 - valLoss: 0.6492277979850769 - trainLoss: 0.6501995325088501\n",
      "cnt: 0 - valLoss: 0.6492255330085754 - trainLoss: 0.6501976847648621\n",
      "cnt: 0 - valLoss: 0.649223268032074 - trainLoss: 0.6501957774162292\n",
      "cnt: 0 - valLoss: 0.6492211222648621 - trainLoss: 0.6501939296722412\n",
      "cnt: 0 - valLoss: 0.6492188572883606 - trainLoss: 0.6501920819282532\n",
      "cnt: 0 - valLoss: 0.6492167115211487 - trainLoss: 0.6501901745796204\n",
      "cnt: 0 - valLoss: 0.6492146849632263 - trainLoss: 0.6501883268356323\n",
      "cnt: 0 - valLoss: 0.6492124199867249 - trainLoss: 0.6501865983009338\n",
      "cnt: 0 - valLoss: 0.6492102742195129 - trainLoss: 0.6501847505569458\n",
      "cnt: 0 - valLoss: 0.6492080688476562 - trainLoss: 0.650182843208313\n",
      "cnt: 0 - valLoss: 0.6492058038711548 - trainLoss: 0.650180995464325\n",
      "cnt: 0 - valLoss: 0.6492035984992981 - trainLoss: 0.6501792073249817\n",
      "cnt: 0 - valLoss: 0.6492013931274414 - trainLoss: 0.6501772403717041\n",
      "cnt: 0 - valLoss: 0.6491992473602295 - trainLoss: 0.6501754522323608\n",
      "cnt: 0 - valLoss: 0.6491972208023071 - trainLoss: 0.650173544883728\n",
      "cnt: 0 - valLoss: 0.6491949558258057 - trainLoss: 0.6501718163490295\n",
      "cnt: 0 - valLoss: 0.649192750453949 - trainLoss: 0.6501699686050415\n",
      "cnt: 0 - valLoss: 0.6491906046867371 - trainLoss: 0.6501680612564087\n",
      "cnt: 0 - valLoss: 0.6491882801055908 - trainLoss: 0.6501661539077759\n",
      "cnt: 0 - valLoss: 0.6491861343383789 - trainLoss: 0.6501643061637878\n",
      "cnt: 0 - valLoss: 0.6491838693618774 - trainLoss: 0.650162398815155\n",
      "cnt: 0 - valLoss: 0.6491819024085999 - trainLoss: 0.650160551071167\n",
      "cnt: 0 - valLoss: 0.6491797566413879 - trainLoss: 0.6501588225364685\n",
      "cnt: 0 - valLoss: 0.6491774916648865 - trainLoss: 0.6501570343971252\n",
      "cnt: 0 - valLoss: 0.6491753458976746 - trainLoss: 0.6501550674438477\n",
      "cnt: 0 - valLoss: 0.6491730213165283 - trainLoss: 0.6501532196998596\n",
      "cnt: 0 - valLoss: 0.6491708159446716 - trainLoss: 0.6501513719558716\n",
      "cnt: 0 - valLoss: 0.6491686105728149 - trainLoss: 0.6501494646072388\n",
      "cnt: 0 - valLoss: 0.6491664052009583 - trainLoss: 0.6501476168632507\n",
      "cnt: 0 - valLoss: 0.6491644382476807 - trainLoss: 0.6501457095146179\n",
      "cnt: 0 - valLoss: 0.649162232875824 - trainLoss: 0.6501439809799194\n",
      "cnt: 0 - valLoss: 0.6491600275039673 - trainLoss: 0.6501421332359314\n",
      "cnt: 0 - valLoss: 0.6491577625274658 - trainLoss: 0.6501402258872986\n",
      "cnt: 0 - valLoss: 0.6491555571556091 - trainLoss: 0.6501383781433105\n",
      "cnt: 0 - valLoss: 0.6491532921791077 - trainLoss: 0.6501364707946777\n",
      "cnt: 0 - valLoss: 0.6491511464118958 - trainLoss: 0.6501346230506897\n",
      "cnt: 0 - valLoss: 0.6491489410400391 - trainLoss: 0.6501327157020569\n",
      "cnt: 0 - valLoss: 0.6491468548774719 - trainLoss: 0.6501309275627136\n",
      "cnt: 0 - valLoss: 0.6491446495056152 - trainLoss: 0.6501290798187256\n",
      "cnt: 0 - valLoss: 0.6491423845291138 - trainLoss: 0.6501272916793823\n",
      "cnt: 0 - valLoss: 0.6491402387619019 - trainLoss: 0.6501254439353943\n",
      "cnt: 0 - valLoss: 0.6491379737854004 - trainLoss: 0.6501235365867615\n",
      "cnt: 0 - valLoss: 0.6491357088088989 - trainLoss: 0.6501216292381287\n",
      "cnt: 0 - valLoss: 0.6491335034370422 - trainLoss: 0.6501197218894958\n",
      "cnt: 0 - valLoss: 0.6491314172744751 - trainLoss: 0.6501178741455078\n",
      "cnt: 0 - valLoss: 0.649129331111908 - trainLoss: 0.6501160860061646\n",
      "cnt: 0 - valLoss: 0.6491270661354065 - trainLoss: 0.6501142382621765\n",
      "cnt: 0 - valLoss: 0.6491248607635498 - trainLoss: 0.6501123905181885\n",
      "cnt: 0 - valLoss: 0.6491227149963379 - trainLoss: 0.6501104831695557\n",
      "cnt: 0 - valLoss: 0.6491203904151917 - trainLoss: 0.6501085758209229\n",
      "cnt: 0 - valLoss: 0.6491182446479797 - trainLoss: 0.6501067280769348\n",
      "cnt: 0 - valLoss: 0.6491159796714783 - trainLoss: 0.6501048803329468\n",
      "cnt: 0 - valLoss: 0.6491139531135559 - trainLoss: 0.650102972984314\n",
      "cnt: 0 - valLoss: 0.6491117477416992 - trainLoss: 0.6501012444496155\n",
      "cnt: 0 - valLoss: 0.6491094827651978 - trainLoss: 0.6500993371009827\n",
      "cnt: 0 - valLoss: 0.6491072773933411 - trainLoss: 0.6500974893569946\n",
      "cnt: 0 - valLoss: 0.6491050720214844 - trainLoss: 0.650095522403717\n",
      "cnt: 0 - valLoss: 0.6491027474403381 - trainLoss: 0.6500937342643738\n",
      "cnt: 0 - valLoss: 0.649100661277771 - trainLoss: 0.6500917673110962\n",
      "cnt: 0 - valLoss: 0.6490985155105591 - trainLoss: 0.6500899195671082\n",
      "cnt: 0 - valLoss: 0.6490963697433472 - trainLoss: 0.6500881910324097\n",
      "cnt: 0 - valLoss: 0.6490941643714905 - trainLoss: 0.6500863432884216\n",
      "cnt: 0 - valLoss: 0.6490919589996338 - trainLoss: 0.6500844359397888\n",
      "cnt: 0 - valLoss: 0.6490896940231323 - trainLoss: 0.650082528591156\n",
      "cnt: 0 - valLoss: 0.6490874886512756 - trainLoss: 0.650080680847168\n",
      "cnt: 0 - valLoss: 0.6490852236747742 - trainLoss: 0.6500787734985352\n",
      "cnt: 0 - valLoss: 0.6490830183029175 - trainLoss: 0.6500768661499023\n",
      "cnt: 0 - valLoss: 0.6490810513496399 - trainLoss: 0.6500750184059143\n",
      "cnt: 0 - valLoss: 0.6490788459777832 - trainLoss: 0.6500732898712158\n",
      "cnt: 0 - valLoss: 0.6490765810012817 - trainLoss: 0.650071382522583\n",
      "cnt: 0 - valLoss: 0.6490743160247803 - trainLoss: 0.650069534778595\n",
      "cnt: 0 - valLoss: 0.6490721702575684 - trainLoss: 0.6500676274299622\n",
      "cnt: 0 - valLoss: 0.6490698456764221 - trainLoss: 0.6500657796859741\n",
      "cnt: 0 - valLoss: 0.6490676403045654 - trainLoss: 0.6500638127326965\n",
      "cnt: 0 - valLoss: 0.6490654945373535 - trainLoss: 0.6500619649887085\n",
      "cnt: 0 - valLoss: 0.6490634083747864 - trainLoss: 0.6500601768493652\n",
      "cnt: 0 - valLoss: 0.6490612030029297 - trainLoss: 0.6500583291053772\n",
      "cnt: 0 - valLoss: 0.6490589380264282 - trainLoss: 0.6500564813613892\n",
      "cnt: 0 - valLoss: 0.6490567326545715 - trainLoss: 0.6500546336174011\n",
      "cnt: 0 - valLoss: 0.6490544676780701 - trainLoss: 0.6500526666641235\n",
      "cnt: 0 - valLoss: 0.6490522623062134 - trainLoss: 0.6500508785247803\n",
      "cnt: 0 - valLoss: 0.6490499973297119 - trainLoss: 0.6500489115715027\n",
      "cnt: 0 - valLoss: 0.6490480303764343 - trainLoss: 0.6500470042228699\n",
      "cnt: 0 - valLoss: 0.6490457653999329 - trainLoss: 0.6500452756881714\n",
      "cnt: 0 - valLoss: 0.6490435004234314 - trainLoss: 0.6500434875488281\n",
      "cnt: 0 - valLoss: 0.6490412354469299 - trainLoss: 0.6500415205955505\n",
      "cnt: 0 - valLoss: 0.6490389108657837 - trainLoss: 0.6500396728515625\n",
      "cnt: 0 - valLoss: 0.649036705493927 - trainLoss: 0.6500377655029297\n",
      "cnt: 0 - valLoss: 0.6490344405174255 - trainLoss: 0.6500358581542969\n",
      "cnt: 0 - valLoss: 0.6490322947502136 - trainLoss: 0.6500340104103088\n",
      "cnt: 0 - valLoss: 0.6490302085876465 - trainLoss: 0.6500322222709656\n",
      "cnt: 0 - valLoss: 0.6490278840065002 - trainLoss: 0.6500304341316223\n",
      "cnt: 0 - valLoss: 0.6490256786346436 - trainLoss: 0.6500285267829895\n",
      "cnt: 0 - valLoss: 0.6490233540534973 - trainLoss: 0.6500266194343567\n",
      "cnt: 0 - valLoss: 0.6490210890769958 - trainLoss: 0.6500247120857239\n",
      "cnt: 0 - valLoss: 0.6490188241004944 - trainLoss: 0.6500228643417358\n",
      "cnt: 0 - valLoss: 0.6490164995193481 - trainLoss: 0.6500210165977478\n",
      "cnt: 0 - valLoss: 0.6490144729614258 - trainLoss: 0.6500190496444702\n",
      "cnt: 0 - valLoss: 0.6490122675895691 - trainLoss: 0.6500173807144165\n",
      "cnt: 0 - valLoss: 0.6490100026130676 - trainLoss: 0.6500154733657837\n",
      "cnt: 0 - valLoss: 0.6490077376365662 - trainLoss: 0.6500135660171509\n",
      "cnt: 0 - valLoss: 0.6490054130554199 - trainLoss: 0.6500117182731628\n",
      "cnt: 0 - valLoss: 0.6490032076835632 - trainLoss: 0.65000981092453\n",
      "cnt: 0 - valLoss: 0.6490009427070618 - trainLoss: 0.650007963180542\n",
      "cnt: 0 - valLoss: 0.6489987373352051 - trainLoss: 0.6500060558319092\n",
      "cnt: 0 - valLoss: 0.6489966511726379 - trainLoss: 0.6500042080879211\n",
      "cnt: 0 - valLoss: 0.6489943861961365 - trainLoss: 0.6500024199485779\n",
      "cnt: 0 - valLoss: 0.648992121219635 - trainLoss: 0.6500005125999451\n",
      "cnt: 0 - valLoss: 0.6489898562431335 - trainLoss: 0.649998664855957\n",
      "cnt: 0 - valLoss: 0.6489875912666321 - trainLoss: 0.6499967575073242\n",
      "cnt: 0 - valLoss: 0.6489852666854858 - trainLoss: 0.6499949097633362\n",
      "cnt: 0 - valLoss: 0.6489830613136292 - trainLoss: 0.6499930024147034\n",
      "cnt: 0 - valLoss: 0.648980975151062 - trainLoss: 0.6499910950660706\n",
      "cnt: 0 - valLoss: 0.6489787101745605 - trainLoss: 0.6499893069267273\n",
      "cnt: 0 - valLoss: 0.6489764451980591 - trainLoss: 0.649987518787384\n",
      "cnt: 0 - valLoss: 0.6489741206169128 - trainLoss: 0.6499856114387512\n",
      "cnt: 0 - valLoss: 0.6489719152450562 - trainLoss: 0.6499836444854736\n",
      "cnt: 0 - valLoss: 0.6489696502685547 - trainLoss: 0.6499817967414856\n",
      "cnt: 0 - valLoss: 0.6489673852920532 - trainLoss: 0.6499799489974976\n",
      "cnt: 0 - valLoss: 0.6489652395248413 - trainLoss: 0.6499780416488647\n",
      "cnt: 0 - valLoss: 0.6489630937576294 - trainLoss: 0.6499761939048767\n",
      "cnt: 0 - valLoss: 0.6489608287811279 - trainLoss: 0.6499744057655334\n",
      "cnt: 0 - valLoss: 0.6489585041999817 - trainLoss: 0.6499725580215454\n",
      "cnt: 0 - valLoss: 0.6489562392234802 - trainLoss: 0.6499705910682678\n",
      "cnt: 0 - valLoss: 0.6489539742469788 - trainLoss: 0.6499687433242798\n",
      "cnt: 0 - valLoss: 0.6489517092704773 - trainLoss: 0.6499667763710022\n",
      "cnt: 0 - valLoss: 0.6489494442939758 - trainLoss: 0.6499649286270142\n",
      "cnt: 0 - valLoss: 0.6489473581314087 - trainLoss: 0.6499630212783813\n",
      "cnt: 0 - valLoss: 0.648945152759552 - trainLoss: 0.6499613523483276\n",
      "cnt: 0 - valLoss: 0.6489428281784058 - trainLoss: 0.6499594449996948\n",
      "cnt: 0 - valLoss: 0.6489405632019043 - trainLoss: 0.649957537651062\n",
      "cnt: 0 - valLoss: 0.6489382982254028 - trainLoss: 0.649955689907074\n",
      "cnt: 0 - valLoss: 0.6489360332489014 - trainLoss: 0.6499537825584412\n",
      "cnt: 0 - valLoss: 0.6489337682723999 - trainLoss: 0.6499518752098083\n",
      "cnt: 0 - valLoss: 0.6489316821098328 - trainLoss: 0.6499499678611755\n",
      "cnt: 0 - valLoss: 0.6489294767379761 - trainLoss: 0.6499481797218323\n",
      "cnt: 0 - valLoss: 0.6489271521568298 - trainLoss: 0.649946391582489\n",
      "cnt: 0 - valLoss: 0.6489249467849731 - trainLoss: 0.6499444246292114\n",
      "cnt: 0 - valLoss: 0.6489226222038269 - trainLoss: 0.6499425172805786\n",
      "cnt: 0 - valLoss: 0.6489203572273254 - trainLoss: 0.6499406695365906\n",
      "cnt: 0 - valLoss: 0.648918092250824 - trainLoss: 0.649938702583313\n",
      "cnt: 0 - valLoss: 0.6489158272743225 - trainLoss: 0.649936854839325\n",
      "cnt: 0 - valLoss: 0.6489138007164001 - trainLoss: 0.6499350070953369\n",
      "cnt: 0 - valLoss: 0.6489114761352539 - trainLoss: 0.6499332189559937\n",
      "cnt: 0 - valLoss: 0.6489092111587524 - trainLoss: 0.6499312520027161\n",
      "cnt: 0 - valLoss: 0.6489068865776062 - trainLoss: 0.649929404258728\n",
      "cnt: 0 - valLoss: 0.6489046812057495 - trainLoss: 0.6499274969100952\n",
      "cnt: 0 - valLoss: 0.6489023566246033 - trainLoss: 0.6499256491661072\n",
      "cnt: 0 - valLoss: 0.6489000916481018 - trainLoss: 0.6499236822128296\n",
      "cnt: 0 - valLoss: 0.6488980054855347 - trainLoss: 0.6499218344688416\n",
      "cnt: 0 - valLoss: 0.6488957405090332 - trainLoss: 0.6499200463294983\n",
      "cnt: 0 - valLoss: 0.6488934755325317 - trainLoss: 0.6499181985855103\n",
      "cnt: 0 - valLoss: 0.6488912105560303 - trainLoss: 0.6499162912368774\n",
      "cnt: 0 - valLoss: 0.6488889455795288 - trainLoss: 0.6499144434928894\n",
      "cnt: 0 - valLoss: 0.6488866209983826 - trainLoss: 0.6499125361442566\n",
      "cnt: 0 - valLoss: 0.6488843560218811 - trainLoss: 0.6499106287956238\n",
      "cnt: 0 - valLoss: 0.6488822102546692 - trainLoss: 0.649908721446991\n",
      "cnt: 0 - valLoss: 0.6488800048828125 - trainLoss: 0.6499068737030029\n",
      "cnt: 0 - valLoss: 0.6488777995109558 - trainLoss: 0.6499050855636597\n",
      "cnt: 0 - valLoss: 0.6488754749298096 - trainLoss: 0.6499031782150269\n",
      "cnt: 0 - valLoss: 0.6488732099533081 - trainLoss: 0.6499012112617493\n",
      "cnt: 0 - valLoss: 0.6488709449768066 - trainLoss: 0.6498993635177612\n",
      "cnt: 0 - valLoss: 0.6488686203956604 - trainLoss: 0.6498974561691284\n",
      "cnt: 0 - valLoss: 0.6488664746284485 - trainLoss: 0.6498956084251404\n",
      "cnt: 0 - valLoss: 0.6488642692565918 - trainLoss: 0.6498937010765076\n",
      "cnt: 0 - valLoss: 0.6488620638847351 - trainLoss: 0.6498919129371643\n",
      "cnt: 0 - valLoss: 0.6488596796989441 - trainLoss: 0.6498900055885315\n",
      "cnt: 0 - valLoss: 0.6488574147224426 - trainLoss: 0.6498881578445435\n",
      "cnt: 0 - valLoss: 0.6488552093505859 - trainLoss: 0.6498861908912659\n",
      "cnt: 0 - valLoss: 0.6488529443740845 - trainLoss: 0.6498842835426331\n",
      "cnt: 0 - valLoss: 0.6488506197929382 - trainLoss: 0.6498823761940002\n",
      "cnt: 0 - valLoss: 0.6488485336303711 - trainLoss: 0.6498805284500122\n",
      "cnt: 0 - valLoss: 0.6488462686538696 - trainLoss: 0.6498787999153137\n",
      "cnt: 0 - valLoss: 0.6488439440727234 - trainLoss: 0.6498768925666809\n",
      "cnt: 0 - valLoss: 0.6488416790962219 - trainLoss: 0.6498749256134033\n",
      "cnt: 0 - valLoss: 0.6488394141197205 - trainLoss: 0.6498730182647705\n",
      "cnt: 0 - valLoss: 0.648837149143219 - trainLoss: 0.6498711705207825\n",
      "cnt: 0 - valLoss: 0.648834764957428 - trainLoss: 0.6498692035675049\n",
      "cnt: 0 - valLoss: 0.6488327980041504 - trainLoss: 0.6498673558235168\n",
      "cnt: 0 - valLoss: 0.6488304734230042 - trainLoss: 0.6498655080795288\n",
      "cnt: 0 - valLoss: 0.6488282084465027 - trainLoss: 0.6498636603355408\n",
      "cnt: 0 - valLoss: 0.6488258838653564 - trainLoss: 0.649861752986908\n",
      "cnt: 0 - valLoss: 0.648823618888855 - trainLoss: 0.6498599052429199\n",
      "cnt: 0 - valLoss: 0.6488213539123535 - trainLoss: 0.6498579382896423\n",
      "cnt: 0 - valLoss: 0.648819088935852 - trainLoss: 0.6498560309410095\n",
      "cnt: 0 - valLoss: 0.6488168835639954 - trainLoss: 0.6498541235923767\n",
      "cnt: 0 - valLoss: 0.6488147377967834 - trainLoss: 0.6498523354530334\n",
      "cnt: 0 - valLoss: 0.648812472820282 - trainLoss: 0.6498504877090454\n",
      "cnt: 0 - valLoss: 0.6488101482391357 - trainLoss: 0.6498485803604126\n",
      "cnt: 0 - valLoss: 0.6488078832626343 - trainLoss: 0.649846613407135\n",
      "cnt: 0 - valLoss: 0.6488056182861328 - trainLoss: 0.6498447060585022\n",
      "cnt: 0 - valLoss: 0.6488032937049866 - trainLoss: 0.6498429179191589\n",
      "cnt: 0 - valLoss: 0.6488010287284851 - trainLoss: 0.6498409509658813\n",
      "cnt: 0 - valLoss: 0.6487990021705627 - trainLoss: 0.6498391032218933\n",
      "cnt: 0 - valLoss: 0.6487966775894165 - trainLoss: 0.6498372554779053\n",
      "cnt: 0 - valLoss: 0.6487943530082703 - trainLoss: 0.6498354077339172\n",
      "cnt: 0 - valLoss: 0.6487921476364136 - trainLoss: 0.6498335003852844\n",
      "cnt: 0 - valLoss: 0.6487898826599121 - trainLoss: 0.6498315334320068\n",
      "cnt: 0 - valLoss: 0.6487876176834106 - trainLoss: 0.6498296856880188\n",
      "cnt: 0 - valLoss: 0.6487853527069092 - trainLoss: 0.649827778339386\n",
      "cnt: 0 - valLoss: 0.6487833857536316 - trainLoss: 0.6498258709907532\n",
      "cnt: 0 - valLoss: 0.6487811803817749 - trainLoss: 0.6498240828514099\n",
      "cnt: 0 - valLoss: 0.6487789154052734 - trainLoss: 0.6498222351074219\n",
      "cnt: 0 - valLoss: 0.648776650428772 - trainLoss: 0.6498202681541443\n",
      "cnt: 0 - valLoss: 0.6487743854522705 - trainLoss: 0.6498183608055115\n",
      "cnt: 0 - valLoss: 0.648772120475769 - trainLoss: 0.6498165130615234\n",
      "cnt: 0 - valLoss: 0.6487698554992676 - trainLoss: 0.6498145461082458\n",
      "cnt: 0 - valLoss: 0.6487678289413452 - trainLoss: 0.6498126983642578\n",
      "cnt: 0 - valLoss: 0.6487656235694885 - trainLoss: 0.6498108506202698\n",
      "cnt: 0 - valLoss: 0.6487633585929871 - trainLoss: 0.6498090028762817\n",
      "cnt: 0 - valLoss: 0.6487610936164856 - trainLoss: 0.6498070955276489\n",
      "cnt: 0 - valLoss: 0.6487588882446289 - trainLoss: 0.6498051881790161\n",
      "cnt: 0 - valLoss: 0.6487566232681274 - trainLoss: 0.6498032808303833\n",
      "cnt: 0 - valLoss: 0.648754358291626 - trainLoss: 0.6498013138771057\n",
      "cnt: 0 - valLoss: 0.6487523317337036 - trainLoss: 0.6497994661331177\n",
      "cnt: 0 - valLoss: 0.6487501263618469 - trainLoss: 0.6497976779937744\n",
      "cnt: 0 - valLoss: 0.6487479209899902 - trainLoss: 0.6497957706451416\n",
      "cnt: 0 - valLoss: 0.6487456560134888 - trainLoss: 0.6497938632965088\n",
      "cnt: 0 - valLoss: 0.6487433910369873 - trainLoss: 0.649791955947876\n",
      "cnt: 0 - valLoss: 0.6487411260604858 - trainLoss: 0.6497900485992432\n",
      "cnt: 0 - valLoss: 0.6487388014793396 - trainLoss: 0.6497882008552551\n",
      "cnt: 0 - valLoss: 0.6487367749214172 - trainLoss: 0.6497862935066223\n",
      "cnt: 0 - valLoss: 0.6487345695495605 - trainLoss: 0.6497844457626343\n",
      "cnt: 0 - valLoss: 0.6487323641777039 - trainLoss: 0.6497825980186462\n",
      "cnt: 0 - valLoss: 0.6487300395965576 - trainLoss: 0.6497806310653687\n",
      "cnt: 0 - valLoss: 0.6487278342247009 - trainLoss: 0.6497787237167358\n",
      "cnt: 0 - valLoss: 0.6487255096435547 - trainLoss: 0.6497768759727478\n",
      "cnt: 0 - valLoss: 0.6487233638763428 - trainLoss: 0.6497749090194702\n",
      "cnt: 0 - valLoss: 0.6487210988998413 - trainLoss: 0.6497730016708374\n",
      "cnt: 0 - valLoss: 0.648719072341919 - trainLoss: 0.6497712135314941\n",
      "cnt: 0 - valLoss: 0.6487168073654175 - trainLoss: 0.6497693061828613\n",
      "cnt: 0 - valLoss: 0.648714542388916 - trainLoss: 0.6497674584388733\n",
      "cnt: 0 - valLoss: 0.6487123370170593 - trainLoss: 0.6497654914855957\n",
      "cnt: 0 - valLoss: 0.6487100720405579 - trainLoss: 0.6497635841369629\n",
      "cnt: 0 - valLoss: 0.6487077474594116 - trainLoss: 0.6497616767883301\n",
      "cnt: 0 - valLoss: 0.6487055420875549 - trainLoss: 0.6497597098350525\n",
      "cnt: 0 - valLoss: 0.6487035155296326 - trainLoss: 0.6497578620910645\n",
      "cnt: 0 - valLoss: 0.6487012505531311 - trainLoss: 0.6497560739517212\n",
      "cnt: 0 - valLoss: 0.6486989855766296 - trainLoss: 0.6497541666030884\n",
      "cnt: 0 - valLoss: 0.6486967206001282 - trainLoss: 0.6497522592544556\n",
      "cnt: 0 - valLoss: 0.6486945152282715 - trainLoss: 0.6497503519058228\n",
      "cnt: 0 - valLoss: 0.64869225025177 - trainLoss: 0.6497484445571899\n",
      "cnt: 0 - valLoss: 0.6486899852752686 - trainLoss: 0.6497464776039124\n",
      "cnt: 0 - valLoss: 0.6486879587173462 - trainLoss: 0.6497445702552795\n",
      "cnt: 0 - valLoss: 0.6486856937408447 - trainLoss: 0.649742841720581\n",
      "cnt: 0 - valLoss: 0.6486834287643433 - trainLoss: 0.6497408747673035\n",
      "cnt: 0 - valLoss: 0.6486811637878418 - trainLoss: 0.6497390270233154\n",
      "cnt: 0 - valLoss: 0.6486788988113403 - trainLoss: 0.6497370600700378\n",
      "cnt: 0 - valLoss: 0.6486766934394836 - trainLoss: 0.649735152721405\n",
      "cnt: 0 - valLoss: 0.6486743092536926 - trainLoss: 0.649733304977417\n",
      "cnt: 0 - valLoss: 0.6486722826957703 - trainLoss: 0.6497313380241394\n",
      "cnt: 0 - valLoss: 0.6486700177192688 - trainLoss: 0.6497296094894409\n",
      "cnt: 0 - valLoss: 0.6486677527427673 - trainLoss: 0.6497277021408081\n",
      "cnt: 0 - valLoss: 0.6486654281616211 - trainLoss: 0.6497258543968201\n",
      "cnt: 0 - valLoss: 0.6486632227897644 - trainLoss: 0.6497238874435425\n",
      "cnt: 0 - valLoss: 0.6486608982086182 - trainLoss: 0.6497219800949097\n",
      "cnt: 0 - valLoss: 0.6486586928367615 - trainLoss: 0.6497200727462769\n",
      "cnt: 0 - valLoss: 0.6486565470695496 - trainLoss: 0.649718165397644\n",
      "cnt: 0 - valLoss: 0.6486542820930481 - trainLoss: 0.6497164368629456\n",
      "cnt: 0 - valLoss: 0.6486520171165466 - trainLoss: 0.6497145295143127\n",
      "cnt: 0 - valLoss: 0.6486497521400452 - trainLoss: 0.6497125625610352\n",
      "cnt: 0 - valLoss: 0.6486474275588989 - trainLoss: 0.6497106552124023\n",
      "cnt: 0 - valLoss: 0.6486451625823975 - trainLoss: 0.6497087478637695\n",
      "cnt: 0 - valLoss: 0.648642897605896 - trainLoss: 0.6497068405151367\n",
      "cnt: 0 - valLoss: 0.6486408114433289 - trainLoss: 0.6497048735618591\n",
      "cnt: 0 - valLoss: 0.6486386656761169 - trainLoss: 0.6497031450271606\n",
      "cnt: 0 - valLoss: 0.6486363410949707 - trainLoss: 0.6497012376785278\n",
      "cnt: 0 - valLoss: 0.6486340761184692 - trainLoss: 0.6496993899345398\n",
      "cnt: 0 - valLoss: 0.648631751537323 - trainLoss: 0.6496974229812622\n",
      "cnt: 0 - valLoss: 0.6486294269561768 - trainLoss: 0.6496955156326294\n",
      "cnt: 0 - valLoss: 0.6486272215843201 - trainLoss: 0.6496936082839966\n",
      "cnt: 0 - valLoss: 0.6486250162124634 - trainLoss: 0.6496917009353638\n",
      "cnt: 0 - valLoss: 0.6486228704452515 - trainLoss: 0.6496898531913757\n",
      "cnt: 0 - valLoss: 0.64862060546875 - trainLoss: 0.6496880650520325\n",
      "cnt: 0 - valLoss: 0.6486182808876038 - trainLoss: 0.6496860980987549\n",
      "cnt: 0 - valLoss: 0.6486160159111023 - trainLoss: 0.6496842503547668\n",
      "cnt: 0 - valLoss: 0.6486137509346008 - trainLoss: 0.6496822834014893\n",
      "cnt: 0 - valLoss: 0.6486113667488098 - trainLoss: 0.6496803164482117\n",
      "cnt: 0 - valLoss: 0.6486092805862427 - trainLoss: 0.6496784090995789\n",
      "cnt: 0 - valLoss: 0.6486071348190308 - trainLoss: 0.6496766209602356\n",
      "cnt: 0 - valLoss: 0.6486048102378845 - trainLoss: 0.6496747732162476\n",
      "cnt: 0 - valLoss: 0.6486025452613831 - trainLoss: 0.6496728658676147\n",
      "cnt: 0 - valLoss: 0.6486002802848816 - trainLoss: 0.6496708989143372\n",
      "cnt: 0 - valLoss: 0.6485979557037354 - trainLoss: 0.6496690511703491\n",
      "cnt: 0 - valLoss: 0.6485956311225891 - trainLoss: 0.6496670842170715\n",
      "cnt: 0 - valLoss: 0.648593544960022 - trainLoss: 0.6496651768684387\n",
      "cnt: 0 - valLoss: 0.6485912799835205 - trainLoss: 0.6496633887290955\n",
      "cnt: 0 - valLoss: 0.6485889554023743 - trainLoss: 0.6496615409851074\n",
      "cnt: 0 - valLoss: 0.6485866904258728 - trainLoss: 0.6496595740318298\n",
      "cnt: 0 - valLoss: 0.6485843658447266 - trainLoss: 0.649657666683197\n",
      "cnt: 0 - valLoss: 0.6485821008682251 - trainLoss: 0.6496556997299194\n",
      "cnt: 0 - valLoss: 0.6485797762870789 - trainLoss: 0.6496538519859314\n",
      "cnt: 0 - valLoss: 0.6485775709152222 - trainLoss: 0.6496518850326538\n",
      "cnt: 0 - valLoss: 0.6485755443572998 - trainLoss: 0.6496500968933105\n",
      "cnt: 0 - valLoss: 0.6485732197761536 - trainLoss: 0.6496482491493225\n",
      "cnt: 0 - valLoss: 0.6485708951950073 - trainLoss: 0.6496464014053345\n",
      "cnt: 0 - valLoss: 0.6485685706138611 - trainLoss: 0.6496444344520569\n",
      "cnt: 0 - valLoss: 0.6485662460327148 - trainLoss: 0.6496425271034241\n",
      "cnt: 0 - valLoss: 0.6485639214515686 - trainLoss: 0.6496405601501465\n",
      "cnt: 0 - valLoss: 0.6485617756843567 - trainLoss: 0.6496386528015137\n",
      "cnt: 0 - valLoss: 0.6485596299171448 - trainLoss: 0.6496368050575256\n",
      "cnt: 0 - valLoss: 0.6485573649406433 - trainLoss: 0.6496349573135376\n",
      "cnt: 0 - valLoss: 0.6485550403594971 - trainLoss: 0.6496330499649048\n",
      "cnt: 0 - valLoss: 0.6485527157783508 - trainLoss: 0.649631142616272\n",
      "cnt: 0 - valLoss: 0.6485504508018494 - trainLoss: 0.6496291756629944\n",
      "cnt: 0 - valLoss: 0.6485480666160583 - trainLoss: 0.6496272087097168\n",
      "cnt: 0 - valLoss: 0.6485459208488464 - trainLoss: 0.6496253609657288\n",
      "cnt: 0 - valLoss: 0.6485437750816345 - trainLoss: 0.649623453617096\n",
      "cnt: 0 - valLoss: 0.6485414505004883 - trainLoss: 0.6496216654777527\n",
      "cnt: 0 - valLoss: 0.6485391855239868 - trainLoss: 0.6496197581291199\n",
      "cnt: 0 - valLoss: 0.6485368609428406 - trainLoss: 0.6496179103851318\n",
      "cnt: 0 - valLoss: 0.6485345363616943 - trainLoss: 0.6496159434318542\n",
      "cnt: 0 - valLoss: 0.6485322117805481 - trainLoss: 0.6496139764785767\n",
      "cnt: 0 - valLoss: 0.6485300660133362 - trainLoss: 0.6496120691299438\n",
      "cnt: 0 - valLoss: 0.6485279202461243 - trainLoss: 0.649610161781311\n",
      "cnt: 0 - valLoss: 0.648525595664978 - trainLoss: 0.6496083736419678\n",
      "cnt: 0 - valLoss: 0.6485233306884766 - trainLoss: 0.649606466293335\n",
      "cnt: 0 - valLoss: 0.6485210061073303 - trainLoss: 0.6496045589447021\n",
      "cnt: 0 - valLoss: 0.6485186815261841 - trainLoss: 0.6496026515960693\n",
      "cnt: 0 - valLoss: 0.6485163569450378 - trainLoss: 0.6496006846427917\n",
      "cnt: 0 - valLoss: 0.6485141515731812 - trainLoss: 0.6495987772941589\n",
      "cnt: 0 - valLoss: 0.6485120058059692 - trainLoss: 0.6495969295501709\n",
      "cnt: 0 - valLoss: 0.6485097408294678 - trainLoss: 0.6495950222015381\n",
      "cnt: 0 - valLoss: 0.6485074162483215 - trainLoss: 0.6495931148529053\n",
      "cnt: 0 - valLoss: 0.6485051512718201 - trainLoss: 0.6495912671089172\n",
      "cnt: 0 - valLoss: 0.648502767086029 - trainLoss: 0.6495893001556396\n",
      "cnt: 0 - valLoss: 0.6485004425048828 - trainLoss: 0.6495873928070068\n",
      "cnt: 0 - valLoss: 0.6484982967376709 - trainLoss: 0.649585485458374\n",
      "cnt: 0 - valLoss: 0.648496150970459 - trainLoss: 0.6495835781097412\n",
      "cnt: 0 - valLoss: 0.6484938263893127 - trainLoss: 0.6495817303657532\n",
      "cnt: 0 - valLoss: 0.6484915018081665 - trainLoss: 0.6495798230171204\n",
      "cnt: 0 - valLoss: 0.648489236831665 - trainLoss: 0.6495778560638428\n",
      "cnt: 0 - valLoss: 0.6484869122505188 - trainLoss: 0.6495760083198547\n",
      "cnt: 0 - valLoss: 0.6484845876693726 - trainLoss: 0.6495741009712219\n",
      "cnt: 0 - valLoss: 0.6484824419021606 - trainLoss: 0.6495720744132996\n",
      "cnt: 0 - valLoss: 0.6484802961349487 - trainLoss: 0.6495702266693115\n",
      "cnt: 0 - valLoss: 0.6484779715538025 - trainLoss: 0.6495684385299683\n",
      "cnt: 0 - valLoss: 0.6484756469726562 - trainLoss: 0.6495665311813354\n",
      "cnt: 0 - valLoss: 0.64847332239151 - trainLoss: 0.6495645642280579\n",
      "cnt: 0 - valLoss: 0.6484709978103638 - trainLoss: 0.6495625972747803\n",
      "cnt: 0 - valLoss: 0.6484687328338623 - trainLoss: 0.6495607495307922\n",
      "cnt: 0 - valLoss: 0.6484664678573608 - trainLoss: 0.6495588421821594\n",
      "cnt: 0 - valLoss: 0.6484643220901489 - trainLoss: 0.6495569348335266\n",
      "cnt: 0 - valLoss: 0.6484619975090027 - trainLoss: 0.6495550870895386\n",
      "cnt: 0 - valLoss: 0.6484597325325012 - trainLoss: 0.6495531797409058\n",
      "cnt: 0 - valLoss: 0.648457407951355 - trainLoss: 0.6495512127876282\n",
      "cnt: 0 - valLoss: 0.6484550833702087 - trainLoss: 0.6495493054389954\n",
      "cnt: 0 - valLoss: 0.6484527587890625 - trainLoss: 0.6495473384857178\n",
      "cnt: 0 - valLoss: 0.6484506130218506 - trainLoss: 0.6495454907417297\n",
      "cnt: 0 - valLoss: 0.6484484076499939 - trainLoss: 0.6495435833930969\n",
      "cnt: 0 - valLoss: 0.6484460830688477 - trainLoss: 0.6495417356491089\n",
      "cnt: 0 - valLoss: 0.6484438180923462 - trainLoss: 0.6495398283004761\n",
      "cnt: 0 - valLoss: 0.6484414339065552 - trainLoss: 0.6495378613471985\n",
      "cnt: 0 - valLoss: 0.6484391093254089 - trainLoss: 0.6495359539985657\n",
      "cnt: 0 - valLoss: 0.6484367847442627 - trainLoss: 0.6495339870452881\n",
      "cnt: 0 - valLoss: 0.6484346985816956 - trainLoss: 0.6495320796966553\n",
      "cnt: 0 - valLoss: 0.6484324336051941 - trainLoss: 0.6495302319526672\n",
      "cnt: 0 - valLoss: 0.6484301090240479 - trainLoss: 0.6495283246040344\n",
      "cnt: 0 - valLoss: 0.6484277844429016 - trainLoss: 0.6495264768600464\n",
      "cnt: 0 - valLoss: 0.6484255194664001 - trainLoss: 0.6495245099067688\n",
      "cnt: 0 - valLoss: 0.6484231948852539 - trainLoss: 0.6495225429534912\n",
      "cnt: 0 - valLoss: 0.6484208703041077 - trainLoss: 0.6495206356048584\n",
      "cnt: 0 - valLoss: 0.6484187841415405 - trainLoss: 0.6495187282562256\n",
      "cnt: 0 - valLoss: 0.6484165191650391 - trainLoss: 0.6495168805122375\n",
      "cnt: 0 - valLoss: 0.6484141945838928 - trainLoss: 0.6495149731636047\n",
      "cnt: 0 - valLoss: 0.6484118103981018 - trainLoss: 0.6495130658149719\n",
      "cnt: 0 - valLoss: 0.6484095454216003 - trainLoss: 0.6495110988616943\n",
      "cnt: 0 - valLoss: 0.6484072208404541 - trainLoss: 0.6495091915130615\n",
      "cnt: 0 - valLoss: 0.6484048962593079 - trainLoss: 0.6495072245597839\n",
      "cnt: 0 - valLoss: 0.6484028697013855 - trainLoss: 0.6495053172111511\n",
      "cnt: 0 - valLoss: 0.6484005451202393 - trainLoss: 0.6495035290718079\n",
      "cnt: 0 - valLoss: 0.648398220539093 - trainLoss: 0.6495015621185303\n",
      "cnt: 0 - valLoss: 0.6483958959579468 - trainLoss: 0.6494996547698975\n",
      "cnt: 0 - valLoss: 0.6483935713768005 - trainLoss: 0.6494976878166199\n",
      "cnt: 0 - valLoss: 0.6483913064002991 - trainLoss: 0.6494957208633423\n",
      "cnt: 0 - valLoss: 0.6483889818191528 - trainLoss: 0.6494938135147095\n",
      "cnt: 0 - valLoss: 0.6483868956565857 - trainLoss: 0.6494919657707214\n",
      "cnt: 0 - valLoss: 0.6483845114707947 - trainLoss: 0.6494900584220886\n",
      "cnt: 0 - valLoss: 0.6483821868896484 - trainLoss: 0.6494882106781006\n",
      "cnt: 0 - valLoss: 0.648379921913147 - trainLoss: 0.649486243724823\n",
      "cnt: 0 - valLoss: 0.6483775973320007 - trainLoss: 0.6494842767715454\n",
      "cnt: 0 - valLoss: 0.6483752727508545 - trainLoss: 0.6494823098182678\n",
      "cnt: 0 - valLoss: 0.648373007774353 - trainLoss: 0.649480402469635\n",
      "cnt: 0 - valLoss: 0.6483709216117859 - trainLoss: 0.6494784951210022\n",
      "cnt: 0 - valLoss: 0.6483685970306396 - trainLoss: 0.6494767069816589\n",
      "cnt: 0 - valLoss: 0.6483662724494934 - trainLoss: 0.6494747400283813\n",
      "cnt: 0 - valLoss: 0.6483639478683472 - trainLoss: 0.6494728922843933\n",
      "cnt: 0 - valLoss: 0.6483616232872009 - trainLoss: 0.649470865726471\n",
      "cnt: 0 - valLoss: 0.6483592987060547 - trainLoss: 0.6494689583778381\n",
      "cnt: 0 - valLoss: 0.648357093334198 - trainLoss: 0.6494670510292053\n",
      "cnt: 0 - valLoss: 0.6483549475669861 - trainLoss: 0.6494651436805725\n",
      "cnt: 0 - valLoss: 0.6483525633811951 - trainLoss: 0.6494632959365845\n",
      "cnt: 0 - valLoss: 0.6483502388000488 - trainLoss: 0.6494613289833069\n",
      "cnt: 0 - valLoss: 0.6483479738235474 - trainLoss: 0.6494593620300293\n",
      "cnt: 0 - valLoss: 0.6483455896377563 - trainLoss: 0.6494574546813965\n",
      "cnt: 0 - valLoss: 0.6483432650566101 - trainLoss: 0.6494554877281189\n",
      "cnt: 0 - valLoss: 0.6483411192893982 - trainLoss: 0.6494535803794861\n",
      "cnt: 0 - valLoss: 0.6483389139175415 - trainLoss: 0.649451732635498\n",
      "cnt: 0 - valLoss: 0.6483365893363953 - trainLoss: 0.6494498252868652\n",
      "cnt: 0 - valLoss: 0.648334264755249 - trainLoss: 0.6494479179382324\n",
      "cnt: 0 - valLoss: 0.6483319401741028 - trainLoss: 0.6494459509849548\n",
      "cnt: 0 - valLoss: 0.6483295559883118 - trainLoss: 0.649444043636322\n",
      "cnt: 0 - valLoss: 0.6483272314071655 - trainLoss: 0.6494420766830444\n",
      "cnt: 0 - valLoss: 0.6483251452445984 - trainLoss: 0.6494401097297668\n",
      "cnt: 0 - valLoss: 0.6483228802680969 - trainLoss: 0.6494382619857788\n",
      "cnt: 0 - valLoss: 0.6483205556869507 - trainLoss: 0.6494364142417908\n",
      "cnt: 0 - valLoss: 0.6483182311058044 - trainLoss: 0.6494344472885132\n",
      "cnt: 0 - valLoss: 0.6483159065246582 - trainLoss: 0.6494324803352356\n",
      "cnt: 0 - valLoss: 0.6483135223388672 - trainLoss: 0.6494305729866028\n",
      "cnt: 0 - valLoss: 0.6483112573623657 - trainLoss: 0.6494285464286804\n",
      "cnt: 0 - valLoss: 0.6483091711997986 - trainLoss: 0.6494266390800476\n",
      "cnt: 0 - valLoss: 0.6483068466186523 - trainLoss: 0.6494248509407043\n",
      "cnt: 0 - valLoss: 0.6483045220375061 - trainLoss: 0.6494229435920715\n",
      "cnt: 0 - valLoss: 0.6483021974563599 - trainLoss: 0.649420976638794\n",
      "cnt: 0 - valLoss: 0.6482998132705688 - trainLoss: 0.6494190096855164\n",
      "cnt: 0 - valLoss: 0.6482975482940674 - trainLoss: 0.6494171023368835\n",
      "cnt: 0 - valLoss: 0.6482952833175659 - trainLoss: 0.6494151949882507\n",
      "cnt: 0 - valLoss: 0.6482931971549988 - trainLoss: 0.6494132280349731\n",
      "cnt: 0 - valLoss: 0.6482908129692078 - trainLoss: 0.6494113802909851\n",
      "cnt: 0 - valLoss: 0.6482884883880615 - trainLoss: 0.6494094133377075\n",
      "cnt: 0 - valLoss: 0.6482861638069153 - trainLoss: 0.6494074463844299\n",
      "cnt: 0 - valLoss: 0.6482837796211243 - trainLoss: 0.6494055390357971\n",
      "cnt: 0 - valLoss: 0.648281455039978 - trainLoss: 0.6494036316871643\n",
      "cnt: 0 - valLoss: 0.6482793092727661 - trainLoss: 0.6494016647338867\n",
      "cnt: 0 - valLoss: 0.6482771039009094 - trainLoss: 0.6493997573852539\n",
      "cnt: 0 - valLoss: 0.6482747793197632 - trainLoss: 0.6493979096412659\n",
      "cnt: 0 - valLoss: 0.6482723951339722 - trainLoss: 0.6493960022926331\n",
      "cnt: 0 - valLoss: 0.6482700705528259 - trainLoss: 0.6493939757347107\n",
      "cnt: 0 - valLoss: 0.6482678055763245 - trainLoss: 0.6493920683860779\n",
      "cnt: 0 - valLoss: 0.6482654809951782 - trainLoss: 0.6493901014328003\n",
      "cnt: 0 - valLoss: 0.6482632756233215 - trainLoss: 0.6493881344795227\n",
      "cnt: 0 - valLoss: 0.6482610106468201 - trainLoss: 0.6493862867355347\n",
      "cnt: 0 - valLoss: 0.6482586860656738 - trainLoss: 0.6493843793869019\n",
      "cnt: 0 - valLoss: 0.6482563018798828 - trainLoss: 0.649382472038269\n",
      "cnt: 0 - valLoss: 0.6482540369033813 - trainLoss: 0.6493805050849915\n",
      "cnt: 0 - valLoss: 0.6482516527175903 - trainLoss: 0.6493785381317139\n",
      "cnt: 0 - valLoss: 0.6482493281364441 - trainLoss: 0.649376630783081\n",
      "cnt: 0 - valLoss: 0.6482473015785217 - trainLoss: 0.6493746042251587\n",
      "cnt: 0 - valLoss: 0.6482449173927307 - trainLoss: 0.6493728756904602\n",
      "cnt: 0 - valLoss: 0.6482425928115845 - trainLoss: 0.6493708491325378\n",
      "cnt: 0 - valLoss: 0.6482402682304382 - trainLoss: 0.6493688821792603\n",
      "cnt: 0 - valLoss: 0.648237943649292 - trainLoss: 0.6493669748306274\n",
      "cnt: 0 - valLoss: 0.648235559463501 - trainLoss: 0.6493650674819946\n",
      "cnt: 0 - valLoss: 0.6482332348823547 - trainLoss: 0.649363100528717\n",
      "cnt: 0 - valLoss: 0.6482311487197876 - trainLoss: 0.6493611335754395\n",
      "cnt: 0 - valLoss: 0.6482288241386414 - trainLoss: 0.6493592858314514\n",
      "cnt: 0 - valLoss: 0.6482264399528503 - trainLoss: 0.6493573784828186\n",
      "cnt: 0 - valLoss: 0.6482241153717041 - trainLoss: 0.649355411529541\n",
      "cnt: 0 - valLoss: 0.6482217311859131 - trainLoss: 0.6493534445762634\n",
      "cnt: 0 - valLoss: 0.6482194662094116 - trainLoss: 0.6493514776229858\n",
      "cnt: 0 - valLoss: 0.6482172012329102 - trainLoss: 0.6493495106697083\n",
      "cnt: 0 - valLoss: 0.6482149958610535 - trainLoss: 0.6493476629257202\n",
      "cnt: 0 - valLoss: 0.6482126712799072 - trainLoss: 0.6493458151817322\n",
      "cnt: 0 - valLoss: 0.6482102870941162 - trainLoss: 0.6493437886238098\n",
      "cnt: 0 - valLoss: 0.6482079029083252 - trainLoss: 0.649341881275177\n",
      "cnt: 0 - valLoss: 0.6482056379318237 - trainLoss: 0.6493399143218994\n",
      "cnt: 0 - valLoss: 0.6482033133506775 - trainLoss: 0.6493379473686218\n",
      "cnt: 0 - valLoss: 0.6482012271881104 - trainLoss: 0.6493359804153442\n",
      "cnt: 0 - valLoss: 0.6481989026069641 - trainLoss: 0.649334192276001\n",
      "cnt: 0 - valLoss: 0.6481965184211731 - trainLoss: 0.6493322253227234\n",
      "cnt: 0 - valLoss: 0.6481941938400269 - trainLoss: 0.6493302583694458\n",
      "cnt: 0 - valLoss: 0.6481918692588806 - trainLoss: 0.649328351020813\n",
      "cnt: 0 - valLoss: 0.6481894850730896 - trainLoss: 0.6493263840675354\n",
      "cnt: 0 - valLoss: 0.6481871604919434 - trainLoss: 0.6493244171142578\n",
      "cnt: 0 - valLoss: 0.6481850147247314 - trainLoss: 0.6493224501609802\n",
      "cnt: 0 - valLoss: 0.64818274974823 - trainLoss: 0.6493206024169922\n",
      "cnt: 0 - valLoss: 0.6481803059577942 - trainLoss: 0.6493186354637146\n",
      "cnt: 0 - valLoss: 0.6481780409812927 - trainLoss: 0.649316668510437\n",
      "cnt: 0 - valLoss: 0.6481757164001465 - trainLoss: 0.6493147611618042\n",
      "cnt: 0 - valLoss: 0.6481732726097107 - trainLoss: 0.6493127346038818\n",
      "cnt: 0 - valLoss: 0.6481711864471436 - trainLoss: 0.649310827255249\n",
      "cnt: 0 - valLoss: 0.6481689214706421 - trainLoss: 0.649308979511261\n",
      "cnt: 0 - valLoss: 0.6481665372848511 - trainLoss: 0.6493070721626282\n",
      "cnt: 0 - valLoss: 0.6481642127037048 - trainLoss: 0.6493051052093506\n",
      "cnt: 0 - valLoss: 0.6481618285179138 - trainLoss: 0.6493031978607178\n",
      "cnt: 0 - valLoss: 0.6481595039367676 - trainLoss: 0.6493011713027954\n",
      "cnt: 0 - valLoss: 0.6481571197509766 - trainLoss: 0.6492992043495178\n",
      "cnt: 0 - valLoss: 0.6481550335884094 - trainLoss: 0.6492972373962402\n",
      "cnt: 0 - valLoss: 0.6481527090072632 - trainLoss: 0.649295449256897\n",
      "cnt: 0 - valLoss: 0.6481503844261169 - trainLoss: 0.6492935419082642\n",
      "cnt: 0 - valLoss: 0.6481480598449707 - trainLoss: 0.6492915153503418\n",
      "cnt: 0 - valLoss: 0.6481456756591797 - trainLoss: 0.6492895483970642\n",
      "cnt: 0 - valLoss: 0.6481433510780334 - trainLoss: 0.6492875814437866\n",
      "cnt: 0 - valLoss: 0.6481410264968872 - trainLoss: 0.6492856740951538\n",
      "cnt: 0 - valLoss: 0.6481388211250305 - trainLoss: 0.6492837071418762\n",
      "cnt: 0 - valLoss: 0.648136556148529 - trainLoss: 0.6492818593978882\n",
      "cnt: 0 - valLoss: 0.6481342315673828 - trainLoss: 0.6492798924446106\n",
      "cnt: 0 - valLoss: 0.6481318473815918 - trainLoss: 0.649277925491333\n",
      "cnt: 0 - valLoss: 0.6481295228004456 - trainLoss: 0.6492759585380554\n",
      "cnt: 0 - valLoss: 0.6481270790100098 - trainLoss: 0.6492739915847778\n",
      "cnt: 0 - valLoss: 0.6481249332427979 - trainLoss: 0.6492720246315002\n",
      "cnt: 0 - valLoss: 0.6481226682662964 - trainLoss: 0.6492701768875122\n",
      "cnt: 0 - valLoss: 0.6481203436851501 - trainLoss: 0.6492682695388794\n",
      "cnt: 0 - valLoss: 0.6481179594993591 - trainLoss: 0.649266242980957\n",
      "cnt: 0 - valLoss: 0.6481156349182129 - trainLoss: 0.6492643356323242\n",
      "cnt: 0 - valLoss: 0.6481133103370667 - trainLoss: 0.6492623090744019\n",
      "cnt: 0 - valLoss: 0.6481108665466309 - trainLoss: 0.649260401725769\n",
      "cnt: 0 - valLoss: 0.6481087803840637 - trainLoss: 0.6492584347724915\n",
      "cnt: 0 - valLoss: 0.6481064558029175 - trainLoss: 0.6492565274238586\n",
      "cnt: 0 - valLoss: 0.6481041312217712 - trainLoss: 0.6492546796798706\n",
      "cnt: 0 - valLoss: 0.6481017470359802 - trainLoss: 0.6492525935173035\n",
      "cnt: 0 - valLoss: 0.648099422454834 - trainLoss: 0.6492507457733154\n",
      "cnt: 0 - valLoss: 0.648097038269043 - trainLoss: 0.6492487788200378\n",
      "cnt: 0 - valLoss: 0.6480947732925415 - trainLoss: 0.6492468118667603\n",
      "cnt: 0 - valLoss: 0.6480925679206848 - trainLoss: 0.6492448449134827\n",
      "cnt: 0 - valLoss: 0.6480902433395386 - trainLoss: 0.6492429971694946\n",
      "cnt: 0 - valLoss: 0.6480879187583923 - trainLoss: 0.6492409706115723\n",
      "cnt: 0 - valLoss: 0.6480855345726013 - trainLoss: 0.6492390632629395\n",
      "cnt: 0 - valLoss: 0.6480832099914551 - trainLoss: 0.6492370367050171\n",
      "cnt: 0 - valLoss: 0.6480808258056641 - trainLoss: 0.6492350697517395\n",
      "cnt: 0 - valLoss: 0.6480786800384521 - trainLoss: 0.6492331027984619\n",
      "cnt: 0 - valLoss: 0.6480763554573059 - trainLoss: 0.6492312550544739\n",
      "cnt: 0 - valLoss: 0.6480739712715149 - trainLoss: 0.6492293477058411\n",
      "cnt: 0 - valLoss: 0.6480716466903687 - trainLoss: 0.6492273807525635\n",
      "cnt: 0 - valLoss: 0.6480693221092224 - trainLoss: 0.6492254137992859\n",
      "cnt: 0 - valLoss: 0.6480669379234314 - trainLoss: 0.6492234468460083\n",
      "cnt: 0 - valLoss: 0.6480646729469299 - trainLoss: 0.6492214798927307\n",
      "cnt: 0 - valLoss: 0.6480624675750732 - trainLoss: 0.6492195129394531\n",
      "cnt: 0 - valLoss: 0.648060142993927 - trainLoss: 0.6492176651954651\n",
      "cnt: 0 - valLoss: 0.648057758808136 - trainLoss: 0.6492156982421875\n",
      "cnt: 0 - valLoss: 0.6480554938316345 - trainLoss: 0.6492137312889099\n",
      "cnt: 0 - valLoss: 0.6480530500411987 - trainLoss: 0.6492117047309875\n",
      "cnt: 0 - valLoss: 0.6480506658554077 - trainLoss: 0.6492097973823547\n",
      "cnt: 0 - valLoss: 0.648048460483551 - trainLoss: 0.6492078304290771\n",
      "cnt: 0 - valLoss: 0.6480462551116943 - trainLoss: 0.6492059826850891\n",
      "cnt: 0 - valLoss: 0.6480438709259033 - trainLoss: 0.6492040157318115\n",
      "cnt: 0 - valLoss: 0.6480414867401123 - trainLoss: 0.6492020487785339\n",
      "cnt: 0 - valLoss: 0.6480391621589661 - trainLoss: 0.6492000818252563\n",
      "cnt: 0 - valLoss: 0.6480367183685303 - trainLoss: 0.6491981148719788\n",
      "cnt: 0 - valLoss: 0.6480344533920288 - trainLoss: 0.6491960883140564\n",
      "cnt: 0 - valLoss: 0.6480323076248169 - trainLoss: 0.6491941213607788\n",
      "cnt: 0 - valLoss: 0.6480299830436707 - trainLoss: 0.6491923332214355\n",
      "cnt: 0 - valLoss: 0.6480275988578796 - trainLoss: 0.649190366268158\n",
      "cnt: 0 - valLoss: 0.6480252146720886 - trainLoss: 0.6491883397102356\n",
      "cnt: 0 - valLoss: 0.6480228304862976 - trainLoss: 0.649186372756958\n",
      "cnt: 0 - valLoss: 0.6480205059051514 - trainLoss: 0.6491843461990356\n",
      "cnt: 0 - valLoss: 0.6480182409286499 - trainLoss: 0.6491824388504028\n",
      "cnt: 0 - valLoss: 0.6480159759521484 - trainLoss: 0.6491804718971252\n",
      "cnt: 0 - valLoss: 0.648013710975647 - trainLoss: 0.6491786241531372\n",
      "cnt: 0 - valLoss: 0.6480112671852112 - trainLoss: 0.6491766571998596\n",
      "cnt: 0 - valLoss: 0.6480089426040649 - trainLoss: 0.6491746306419373\n",
      "cnt: 0 - valLoss: 0.6480064988136292 - trainLoss: 0.6491726636886597\n",
      "cnt: 0 - valLoss: 0.6480041742324829 - trainLoss: 0.6491706967353821\n",
      "cnt: 0 - valLoss: 0.6480020880699158 - trainLoss: 0.6491687297821045\n",
      "cnt: 0 - valLoss: 0.6479997038841248 - trainLoss: 0.6491669416427612\n",
      "cnt: 0 - valLoss: 0.6479973793029785 - trainLoss: 0.6491649150848389\n",
      "cnt: 0 - valLoss: 0.6479949951171875 - trainLoss: 0.6491629481315613\n",
      "cnt: 0 - valLoss: 0.6479926109313965 - trainLoss: 0.6491609811782837\n",
      "cnt: 0 - valLoss: 0.6479902863502502 - trainLoss: 0.6491589546203613\n",
      "cnt: 0 - valLoss: 0.6479880213737488 - trainLoss: 0.6491569876670837\n",
      "cnt: 0 - valLoss: 0.6479858160018921 - trainLoss: 0.6491551399230957\n",
      "cnt: 0 - valLoss: 0.6479834318161011 - trainLoss: 0.6491531729698181\n",
      "cnt: 0 - valLoss: 0.6479810476303101 - trainLoss: 0.6491511464118958\n",
      "cnt: 0 - valLoss: 0.6479787230491638 - trainLoss: 0.6491491794586182\n",
      "cnt: 0 - valLoss: 0.647976279258728 - trainLoss: 0.6491472125053406\n",
      "cnt: 0 - valLoss: 0.6479739546775818 - trainLoss: 0.649145245552063\n",
      "cnt: 0 - valLoss: 0.6479718089103699 - trainLoss: 0.6491432785987854\n",
      "cnt: 0 - valLoss: 0.6479694843292236 - trainLoss: 0.6491414308547974\n",
      "cnt: 0 - valLoss: 0.6479671001434326 - trainLoss: 0.6491394639015198\n",
      "cnt: 0 - valLoss: 0.6479647159576416 - trainLoss: 0.6491374969482422\n",
      "cnt: 0 - valLoss: 0.6479623913764954 - trainLoss: 0.6491355299949646\n",
      "cnt: 0 - valLoss: 0.6479599475860596 - trainLoss: 0.6491335034370422\n",
      "cnt: 0 - valLoss: 0.6479576826095581 - trainLoss: 0.6491315364837646\n",
      "cnt: 0 - valLoss: 0.6479555368423462 - trainLoss: 0.6491296291351318\n",
      "cnt: 0 - valLoss: 0.6479531526565552 - trainLoss: 0.649127721786499\n",
      "cnt: 0 - valLoss: 0.6479507684707642 - trainLoss: 0.6491257548332214\n",
      "cnt: 0 - valLoss: 0.6479484438896179 - trainLoss: 0.6491237282752991\n",
      "cnt: 0 - valLoss: 0.6479460597038269 - trainLoss: 0.6491217613220215\n",
      "cnt: 0 - valLoss: 0.6479436755180359 - trainLoss: 0.6491197347640991\n",
      "cnt: 0 - valLoss: 0.647941529750824 - trainLoss: 0.6491177678108215\n",
      "cnt: 0 - valLoss: 0.647939145565033 - trainLoss: 0.6491159200668335\n",
      "cnt: 0 - valLoss: 0.6479368209838867 - trainLoss: 0.6491139531135559\n",
      "cnt: 0 - valLoss: 0.6479344367980957 - trainLoss: 0.6491119861602783\n",
      "cnt: 0 - valLoss: 0.6479320526123047 - trainLoss: 0.6491100192070007\n",
      "cnt: 0 - valLoss: 0.6479296684265137 - trainLoss: 0.6491079926490784\n",
      "cnt: 0 - valLoss: 0.6479273438453674 - trainLoss: 0.6491060256958008\n",
      "cnt: 0 - valLoss: 0.6479251980781555 - trainLoss: 0.649104118347168\n",
      "cnt: 0 - valLoss: 0.6479228138923645 - trainLoss: 0.6491022109985352\n",
      "cnt: 0 - valLoss: 0.6479204297065735 - trainLoss: 0.6491001844406128\n",
      "cnt: 0 - valLoss: 0.6479181051254272 - trainLoss: 0.6490982174873352\n",
      "cnt: 0 - valLoss: 0.6479157209396362 - trainLoss: 0.6490961909294128\n",
      "cnt: 0 - valLoss: 0.6479132771492004 - trainLoss: 0.6490942239761353\n",
      "cnt: 0 - valLoss: 0.6479111313819885 - trainLoss: 0.6490922570228577\n",
      "cnt: 0 - valLoss: 0.6479088664054871 - trainLoss: 0.6490904092788696\n",
      "cnt: 0 - valLoss: 0.647906482219696 - trainLoss: 0.649088442325592\n",
      "cnt: 0 - valLoss: 0.647904098033905 - trainLoss: 0.6490864753723145\n",
      "cnt: 0 - valLoss: 0.647901713848114 - trainLoss: 0.6490843892097473\n",
      "cnt: 0 - valLoss: 0.647899329662323 - trainLoss: 0.6490824222564697\n",
      "cnt: 0 - valLoss: 0.6478970050811768 - trainLoss: 0.6490804553031921\n",
      "cnt: 0 - valLoss: 0.6478948593139648 - trainLoss: 0.6490785479545593\n",
      "cnt: 0 - valLoss: 0.647892415523529 - trainLoss: 0.6490766406059265\n",
      "cnt: 0 - valLoss: 0.6478900909423828 - trainLoss: 0.6490746736526489\n",
      "cnt: 0 - valLoss: 0.647887647151947 - trainLoss: 0.6490726470947266\n",
      "cnt: 0 - valLoss: 0.6478853225708008 - trainLoss: 0.6490706205368042\n",
      "cnt: 0 - valLoss: 0.6478829383850098 - trainLoss: 0.6490686535835266\n",
      "cnt: 0 - valLoss: 0.6478807926177979 - trainLoss: 0.6490666270256042\n",
      "cnt: 0 - valLoss: 0.6478784680366516 - trainLoss: 0.6490647792816162\n",
      "cnt: 0 - valLoss: 0.6478760838508606 - trainLoss: 0.6490628123283386\n",
      "cnt: 0 - valLoss: 0.6478736400604248 - trainLoss: 0.649060845375061\n",
      "cnt: 0 - valLoss: 0.6478713154792786 - trainLoss: 0.6490588784217834\n",
      "cnt: 0 - valLoss: 0.6478689312934875 - trainLoss: 0.6490568518638611\n",
      "cnt: 0 - valLoss: 0.6478666663169861 - trainLoss: 0.6490548849105835\n",
      "cnt: 0 - valLoss: 0.6478644013404846 - trainLoss: 0.6490529775619507\n",
      "cnt: 0 - valLoss: 0.6478620171546936 - trainLoss: 0.6490510702133179\n",
      "cnt: 0 - valLoss: 0.6478596329689026 - trainLoss: 0.6490490436553955\n",
      "cnt: 0 - valLoss: 0.6478573083877563 - trainLoss: 0.6490470170974731\n",
      "cnt: 0 - valLoss: 0.6478548645973206 - trainLoss: 0.6490450501441956\n",
      "cnt: 0 - valLoss: 0.6478525400161743 - trainLoss: 0.649043083190918\n",
      "cnt: 0 - valLoss: 0.6478503942489624 - trainLoss: 0.6490410566329956\n",
      "cnt: 0 - valLoss: 0.6478479504585266 - trainLoss: 0.6490392088890076\n",
      "cnt: 0 - valLoss: 0.6478456854820251 - trainLoss: 0.64903724193573\n",
      "cnt: 0 - valLoss: 0.6478432416915894 - trainLoss: 0.6490351557731628\n",
      "cnt: 0 - valLoss: 0.6478409171104431 - trainLoss: 0.6490331888198853\n",
      "cnt: 0 - valLoss: 0.6478385329246521 - trainLoss: 0.6490311622619629\n",
      "cnt: 0 - valLoss: 0.6478362083435059 - trainLoss: 0.6490291953086853\n",
      "cnt: 0 - valLoss: 0.6478340029716492 - trainLoss: 0.6490273475646973\n",
      "cnt: 0 - valLoss: 0.6478316187858582 - trainLoss: 0.6490253806114197\n",
      "cnt: 0 - valLoss: 0.6478292346000671 - trainLoss: 0.6490234136581421\n",
      "cnt: 0 - valLoss: 0.6478268504142761 - trainLoss: 0.6490213871002197\n",
      "cnt: 0 - valLoss: 0.6478244066238403 - trainLoss: 0.6490193605422974\n",
      "cnt: 0 - valLoss: 0.6478221416473389 - trainLoss: 0.6490173935890198\n",
      "cnt: 0 - valLoss: 0.6478199362754822 - trainLoss: 0.6490154266357422\n",
      "cnt: 0 - valLoss: 0.6478175520896912 - trainLoss: 0.6490135192871094\n",
      "cnt: 0 - valLoss: 0.6478151679039001 - trainLoss: 0.6490115523338318\n",
      "cnt: 0 - valLoss: 0.6478127837181091 - trainLoss: 0.6490095853805542\n",
      "cnt: 0 - valLoss: 0.6478104591369629 - trainLoss: 0.6490075588226318\n",
      "cnt: 0 - valLoss: 0.6478080153465271 - trainLoss: 0.6490055918693542\n",
      "cnt: 0 - valLoss: 0.6478058695793152 - trainLoss: 0.6490035653114319\n",
      "cnt: 0 - valLoss: 0.6478034853935242 - trainLoss: 0.6490017175674438\n",
      "cnt: 0 - valLoss: 0.6478011012077332 - trainLoss: 0.6489997506141663\n",
      "cnt: 0 - valLoss: 0.6477985978126526 - trainLoss: 0.6489977240562439\n",
      "cnt: 0 - valLoss: 0.6477962732315063 - trainLoss: 0.6489956974983215\n",
      "cnt: 0 - valLoss: 0.6477938294410706 - trainLoss: 0.648993730545044\n",
      "cnt: 0 - valLoss: 0.6477916240692139 - trainLoss: 0.6489917039871216\n",
      "cnt: 0 - valLoss: 0.6477892398834229 - trainLoss: 0.648989737033844\n",
      "cnt: 0 - valLoss: 0.6477868556976318 - trainLoss: 0.6489878296852112\n",
      "cnt: 0 - valLoss: 0.6477844715118408 - trainLoss: 0.6489858031272888\n",
      "cnt: 0 - valLoss: 0.6477820873260498 - trainLoss: 0.6489837765693665\n",
      "cnt: 0 - valLoss: 0.647779643535614 - trainLoss: 0.6489818096160889\n",
      "cnt: 0 - valLoss: 0.647777259349823 - trainLoss: 0.6489797830581665\n",
      "cnt: 0 - valLoss: 0.6477750539779663 - trainLoss: 0.6489778161048889\n",
      "cnt: 0 - valLoss: 0.6477727293968201 - trainLoss: 0.6489759683609009\n",
      "cnt: 0 - valLoss: 0.6477702260017395 - trainLoss: 0.6489739418029785\n",
      "cnt: 0 - valLoss: 0.6477678418159485 - trainLoss: 0.6489719152450562\n",
      "cnt: 0 - valLoss: 0.6477653980255127 - trainLoss: 0.6489699482917786\n",
      "cnt: 0 - valLoss: 0.6477630138397217 - trainLoss: 0.6489679217338562\n",
      "cnt: 0 - valLoss: 0.647760808467865 - trainLoss: 0.6489658355712891\n",
      "cnt: 0 - valLoss: 0.6477584838867188 - trainLoss: 0.648963987827301\n",
      "cnt: 0 - valLoss: 0.6477560997009277 - trainLoss: 0.6489620208740234\n",
      "cnt: 0 - valLoss: 0.6477536559104919 - trainLoss: 0.6489599943161011\n",
      "cnt: 0 - valLoss: 0.6477512121200562 - trainLoss: 0.6489580273628235\n",
      "cnt: 0 - valLoss: 0.6477487683296204 - trainLoss: 0.6489560008049011\n",
      "cnt: 0 - valLoss: 0.6477465033531189 - trainLoss: 0.6489539742469788\n",
      "cnt: 0 - valLoss: 0.6477442383766174 - trainLoss: 0.6489520072937012\n",
      "cnt: 0 - valLoss: 0.6477418541908264 - trainLoss: 0.6489501595497131\n",
      "cnt: 0 - valLoss: 0.6477394104003906 - trainLoss: 0.6489481329917908\n",
      "cnt: 0 - valLoss: 0.6477369666099548 - trainLoss: 0.6489460468292236\n",
      "cnt: 0 - valLoss: 0.6477344632148743 - trainLoss: 0.648944079875946\n",
      "cnt: 0 - valLoss: 0.6477320194244385 - trainLoss: 0.6489421129226685\n",
      "cnt: 0 - valLoss: 0.6477298736572266 - trainLoss: 0.6489400863647461\n",
      "cnt: 0 - valLoss: 0.6477274298667908 - trainLoss: 0.6489382386207581\n",
      "cnt: 0 - valLoss: 0.6477250456809998 - trainLoss: 0.6489362120628357\n",
      "cnt: 0 - valLoss: 0.647722601890564 - trainLoss: 0.6489341855049133\n",
      "cnt: 0 - valLoss: 0.6477201581001282 - trainLoss: 0.6489322185516357\n",
      "cnt: 0 - valLoss: 0.6477176547050476 - trainLoss: 0.6489301323890686\n",
      "cnt: 0 - valLoss: 0.6477153897285461 - trainLoss: 0.6489281058311462\n",
      "cnt: 0 - valLoss: 0.6477131247520447 - trainLoss: 0.6489261984825134\n",
      "cnt: 0 - valLoss: 0.6477107405662537 - trainLoss: 0.6489243507385254\n",
      "cnt: 0 - valLoss: 0.6477082371711731 - trainLoss: 0.648922324180603\n",
      "cnt: 0 - valLoss: 0.6477057933807373 - trainLoss: 0.6489202380180359\n",
      "cnt: 0 - valLoss: 0.6477034091949463 - trainLoss: 0.6489182710647583\n",
      "cnt: 0 - valLoss: 0.6477009057998657 - trainLoss: 0.6489162445068359\n",
      "cnt: 0 - valLoss: 0.6476987600326538 - trainLoss: 0.6489142179489136\n",
      "cnt: 0 - valLoss: 0.647696316242218 - trainLoss: 0.6489123702049255\n",
      "cnt: 0 - valLoss: 0.6476938724517822 - trainLoss: 0.6489103436470032\n",
      "cnt: 0 - valLoss: 0.6476914286613464 - trainLoss: 0.6489083766937256\n",
      "cnt: 0 - valLoss: 0.6476889848709106 - trainLoss: 0.6489062905311584\n",
      "cnt: 0 - valLoss: 0.6476865410804749 - trainLoss: 0.6489043235778809\n",
      "cnt: 0 - valLoss: 0.6476843357086182 - trainLoss: 0.6489022970199585\n",
      "cnt: 0 - valLoss: 0.6476820111274719 - trainLoss: 0.6489003300666809\n",
      "cnt: 0 - valLoss: 0.6476795077323914 - trainLoss: 0.6488984823226929\n",
      "cnt: 0 - valLoss: 0.6476770639419556 - trainLoss: 0.6488963961601257\n",
      "cnt: 0 - valLoss: 0.6476746201515198 - trainLoss: 0.6488944292068481\n",
      "cnt: 0 - valLoss: 0.6476721167564392 - trainLoss: 0.6488924026489258\n",
      "cnt: 0 - valLoss: 0.6476697325706482 - trainLoss: 0.6488903164863586\n",
      "cnt: 0 - valLoss: 0.6476675868034363 - trainLoss: 0.6488884091377258\n",
      "cnt: 0 - valLoss: 0.6476651430130005 - trainLoss: 0.648886501789093\n",
      "cnt: 0 - valLoss: 0.6476626992225647 - trainLoss: 0.6488844752311707\n",
      "cnt: 0 - valLoss: 0.6476601958274841 - trainLoss: 0.6488824486732483\n",
      "cnt: 0 - valLoss: 0.6476577520370483 - trainLoss: 0.6488804221153259\n",
      "cnt: 0 - valLoss: 0.6476553678512573 - trainLoss: 0.6488783955574036\n",
      "cnt: 0 - valLoss: 0.6476531028747559 - trainLoss: 0.6488763689994812\n",
      "cnt: 0 - valLoss: 0.6476508378982544 - trainLoss: 0.6488745212554932\n",
      "cnt: 0 - valLoss: 0.6476483345031738 - trainLoss: 0.6488724946975708\n",
      "cnt: 0 - valLoss: 0.647645890712738 - trainLoss: 0.6488704681396484\n",
      "cnt: 0 - valLoss: 0.6476434469223022 - trainLoss: 0.6488685011863708\n",
      "cnt: 0 - valLoss: 0.6476410627365112 - trainLoss: 0.6488664746284485\n",
      "cnt: 0 - valLoss: 0.647638738155365 - trainLoss: 0.6488643884658813\n",
      "cnt: 0 - valLoss: 0.6476364731788635 - trainLoss: 0.6488624811172485\n",
      "cnt: 0 - valLoss: 0.6476340889930725 - trainLoss: 0.6488605737686157\n",
      "cnt: 0 - valLoss: 0.6476315855979919 - trainLoss: 0.6488585472106934\n",
      "cnt: 0 - valLoss: 0.6476292014122009 - trainLoss: 0.648856520652771\n",
      "cnt: 0 - valLoss: 0.6476267576217651 - trainLoss: 0.6488544940948486\n",
      "cnt: 0 - valLoss: 0.6476244330406189 - trainLoss: 0.648852527141571\n",
      "cnt: 0 - valLoss: 0.6476221680641174 - trainLoss: 0.6488504409790039\n",
      "cnt: 0 - valLoss: 0.6476197838783264 - trainLoss: 0.6488486528396606\n",
      "cnt: 0 - valLoss: 0.6476172804832458 - trainLoss: 0.6488466262817383\n",
      "cnt: 0 - valLoss: 0.6476149559020996 - trainLoss: 0.6488445997238159\n",
      "cnt: 0 - valLoss: 0.647612452507019 - trainLoss: 0.6488425135612488\n",
      "cnt: 0 - valLoss: 0.647610068321228 - trainLoss: 0.6488405466079712\n",
      "cnt: 0 - valLoss: 0.6476078033447266 - trainLoss: 0.6488385200500488\n",
      "cnt: 0 - valLoss: 0.6476055383682251 - trainLoss: 0.648836612701416\n",
      "cnt: 0 - valLoss: 0.6476030945777893 - trainLoss: 0.6488346457481384\n",
      "cnt: 0 - valLoss: 0.6476005911827087 - trainLoss: 0.6488326191902161\n",
      "cnt: 0 - valLoss: 0.6475982069969177 - trainLoss: 0.6488306522369385\n",
      "cnt: 0 - valLoss: 0.6475957632064819 - trainLoss: 0.6488285660743713\n",
      "cnt: 0 - valLoss: 0.6475934386253357 - trainLoss: 0.648826539516449\n",
      "cnt: 0 - valLoss: 0.647591233253479 - trainLoss: 0.6488245725631714\n",
      "cnt: 0 - valLoss: 0.6475887298583984 - trainLoss: 0.6488226056098938\n",
      "cnt: 0 - valLoss: 0.6475862860679626 - trainLoss: 0.6488206386566162\n",
      "cnt: 0 - valLoss: 0.6475839614868164 - trainLoss: 0.6488186120986938\n",
      "cnt: 0 - valLoss: 0.6475814580917358 - trainLoss: 0.6488165259361267\n",
      "cnt: 0 - valLoss: 0.6475789546966553 - trainLoss: 0.6488145589828491\n",
      "cnt: 0 - valLoss: 0.6475768089294434 - trainLoss: 0.6488125324249268\n",
      "cnt: 0 - valLoss: 0.6475744247436523 - trainLoss: 0.6488107442855835\n",
      "cnt: 0 - valLoss: 0.6475720405578613 - trainLoss: 0.6488086581230164\n",
      "cnt: 0 - valLoss: 0.6475695371627808 - trainLoss: 0.6488066911697388\n",
      "cnt: 0 - valLoss: 0.6475671529769897 - trainLoss: 0.6488045454025269\n",
      "cnt: 0 - valLoss: 0.647564709186554 - trainLoss: 0.6488025784492493\n",
      "cnt: 0 - valLoss: 0.6475625038146973 - trainLoss: 0.6488005518913269\n",
      "cnt: 0 - valLoss: 0.6475601196289062 - trainLoss: 0.6487986445426941\n",
      "cnt: 0 - valLoss: 0.6475576758384705 - trainLoss: 0.6487966775894165\n",
      "cnt: 0 - valLoss: 0.6475552320480347 - trainLoss: 0.6487946510314941\n",
      "cnt: 0 - valLoss: 0.6475527286529541 - trainLoss: 0.648792564868927\n",
      "cnt: 0 - valLoss: 0.6475503444671631 - trainLoss: 0.6487905383110046\n",
      "cnt: 0 - valLoss: 0.6475480198860168 - trainLoss: 0.6487885117530823\n",
      "cnt: 0 - valLoss: 0.6475458145141602 - trainLoss: 0.6487865447998047\n",
      "cnt: 0 - valLoss: 0.6475433707237244 - trainLoss: 0.6487846970558167\n",
      "cnt: 0 - valLoss: 0.6475409269332886 - trainLoss: 0.6487826108932495\n",
      "cnt: 0 - valLoss: 0.647538423538208 - trainLoss: 0.6487805843353271\n",
      "cnt: 0 - valLoss: 0.647536039352417 - trainLoss: 0.6487785577774048\n",
      "cnt: 0 - valLoss: 0.6475335955619812 - trainLoss: 0.6487765312194824\n",
      "cnt: 0 - valLoss: 0.6475315093994141 - trainLoss: 0.6487745046615601\n",
      "cnt: 0 - valLoss: 0.6475290060043335 - trainLoss: 0.6487725973129272\n",
      "cnt: 0 - valLoss: 0.6475265622138977 - trainLoss: 0.6487706303596497\n",
      "cnt: 0 - valLoss: 0.6475241184234619 - trainLoss: 0.6487685441970825\n",
      "cnt: 0 - valLoss: 0.6475216150283813 - trainLoss: 0.6487665176391602\n",
      "cnt: 0 - valLoss: 0.6475192308425903 - trainLoss: 0.6487644910812378\n",
      "cnt: 0 - valLoss: 0.6475169658660889 - trainLoss: 0.6487624645233154\n",
      "cnt: 0 - valLoss: 0.6475145816802979 - trainLoss: 0.6487605571746826\n",
      "cnt: 0 - valLoss: 0.6475121974945068 - trainLoss: 0.6487585306167603\n",
      "cnt: 0 - valLoss: 0.6475096940994263 - trainLoss: 0.6487565636634827\n",
      "cnt: 0 - valLoss: 0.6475073099136353 - trainLoss: 0.6487545371055603\n",
      "cnt: 0 - valLoss: 0.6475049257278442 - trainLoss: 0.6487524509429932\n",
      "cnt: 0 - valLoss: 0.6475025415420532 - trainLoss: 0.6487504243850708\n",
      "cnt: 0 - valLoss: 0.6475002765655518 - trainLoss: 0.6487484574317932\n",
      "cnt: 0 - valLoss: 0.6474977731704712 - trainLoss: 0.6487465500831604\n",
      "cnt: 0 - valLoss: 0.6474953293800354 - trainLoss: 0.6487444639205933\n",
      "cnt: 0 - valLoss: 0.6474928855895996 - trainLoss: 0.6487424373626709\n",
      "cnt: 0 - valLoss: 0.6474905014038086 - trainLoss: 0.6487404704093933\n",
      "cnt: 0 - valLoss: 0.647487998008728 - trainLoss: 0.6487383246421814\n",
      "cnt: 0 - valLoss: 0.6474858522415161 - trainLoss: 0.6487363576889038\n",
      "cnt: 0 - valLoss: 0.6474834680557251 - trainLoss: 0.6487345695495605\n",
      "cnt: 0 - valLoss: 0.6474810242652893 - trainLoss: 0.6487324833869934\n",
      "cnt: 0 - valLoss: 0.6474785208702087 - trainLoss: 0.6487303972244263\n",
      "cnt: 0 - valLoss: 0.647476077079773 - trainLoss: 0.6487283706665039\n",
      "cnt: 0 - valLoss: 0.6474735736846924 - trainLoss: 0.6487264037132263\n",
      "cnt: 0 - valLoss: 0.6474713683128357 - trainLoss: 0.648724377155304\n",
      "cnt: 0 - valLoss: 0.6474689245223999 - trainLoss: 0.6487224102020264\n",
      "cnt: 0 - valLoss: 0.6474665999412537 - trainLoss: 0.6487204432487488\n",
      "cnt: 0 - valLoss: 0.6474640965461731 - trainLoss: 0.6487183570861816\n",
      "cnt: 0 - valLoss: 0.6474616527557373 - trainLoss: 0.6487163305282593\n",
      "cnt: 0 - valLoss: 0.6474591493606567 - trainLoss: 0.6487143039703369\n",
      "cnt: 0 - valLoss: 0.6474568843841553 - trainLoss: 0.6487122178077698\n",
      "cnt: 0 - valLoss: 0.6474546194076538 - trainLoss: 0.6487103700637817\n",
      "cnt: 0 - valLoss: 0.6474521160125732 - trainLoss: 0.6487084031105042\n",
      "cnt: 0 - valLoss: 0.6474496722221375 - trainLoss: 0.648706316947937\n",
      "cnt: 0 - valLoss: 0.6474472284317017 - trainLoss: 0.6487042903900146\n",
      "cnt: 0 - valLoss: 0.6474448442459106 - trainLoss: 0.6487022042274475\n",
      "cnt: 0 - valLoss: 0.6474424004554749 - trainLoss: 0.6487002372741699\n",
      "cnt: 0 - valLoss: 0.6474401354789734 - trainLoss: 0.6486982703208923\n",
      "cnt: 0 - valLoss: 0.6474376916885376 - trainLoss: 0.6486963033676147\n",
      "cnt: 0 - valLoss: 0.6474352478981018 - trainLoss: 0.6486942172050476\n",
      "cnt: 0 - valLoss: 0.647432804107666 - trainLoss: 0.64869225025177\n",
      "cnt: 0 - valLoss: 0.6474302411079407 - trainLoss: 0.6486902236938477\n",
      "cnt: 0 - valLoss: 0.6474278569221497 - trainLoss: 0.6486881375312805\n",
      "cnt: 0 - valLoss: 0.6474255919456482 - trainLoss: 0.6486861109733582\n",
      "cnt: 0 - valLoss: 0.6474232077598572 - trainLoss: 0.6486842632293701\n",
      "cnt: 0 - valLoss: 0.6474207043647766 - trainLoss: 0.648682177066803\n",
      "cnt: 0 - valLoss: 0.6474183201789856 - trainLoss: 0.6486800909042358\n",
      "cnt: 0 - valLoss: 0.6474158763885498 - trainLoss: 0.6486781239509583\n",
      "cnt: 0 - valLoss: 0.647413432598114 - trainLoss: 0.6486760973930359\n",
      "cnt: 0 - valLoss: 0.6474112272262573 - trainLoss: 0.6486740112304688\n",
      "cnt: 0 - valLoss: 0.6474087834358215 - trainLoss: 0.6486721634864807\n",
      "cnt: 0 - valLoss: 0.647406280040741 - trainLoss: 0.6486701369285583\n",
      "cnt: 0 - valLoss: 0.6474037766456604 - trainLoss: 0.6486680507659912\n",
      "cnt: 0 - valLoss: 0.6474013328552246 - trainLoss: 0.6486660242080688\n",
      "cnt: 0 - valLoss: 0.6473988890647888 - trainLoss: 0.6486639380455017\n",
      "cnt: 0 - valLoss: 0.6473966240882874 - trainLoss: 0.6486619114875793\n",
      "cnt: 0 - valLoss: 0.6473943591117859 - trainLoss: 0.6486600041389465\n",
      "cnt: 0 - valLoss: 0.6473918557167053 - trainLoss: 0.648658037185669\n",
      "cnt: 0 - valLoss: 0.64738929271698 - trainLoss: 0.6486559510231018\n",
      "cnt: 0 - valLoss: 0.6473868489265442 - trainLoss: 0.6486539244651794\n",
      "cnt: 0 - valLoss: 0.6473844051361084 - trainLoss: 0.6486518383026123\n",
      "cnt: 0 - valLoss: 0.6473820805549622 - trainLoss: 0.6486497521400452\n",
      "cnt: 0 - valLoss: 0.6473798155784607 - trainLoss: 0.6486478447914124\n",
      "cnt: 0 - valLoss: 0.6473773717880249 - trainLoss: 0.6486459374427795\n",
      "cnt: 0 - valLoss: 0.6473748087882996 - trainLoss: 0.6486438512802124\n",
      "cnt: 0 - valLoss: 0.6473723649978638 - trainLoss: 0.6486417651176453\n",
      "cnt: 0 - valLoss: 0.647369921207428 - trainLoss: 0.6486397385597229\n",
      "cnt: 0 - valLoss: 0.647367537021637 - trainLoss: 0.6486376523971558\n",
      "cnt: 0 - valLoss: 0.6473653316497803 - trainLoss: 0.6486356854438782\n",
      "cnt: 0 - valLoss: 0.6473628282546997 - trainLoss: 0.6486338376998901\n",
      "cnt: 0 - valLoss: 0.6473603844642639 - trainLoss: 0.648631751537323\n",
      "cnt: 0 - valLoss: 0.6473578810691833 - trainLoss: 0.6486296653747559\n",
      "cnt: 0 - valLoss: 0.6473554372787476 - trainLoss: 0.6486276388168335\n",
      "cnt: 0 - valLoss: 0.6473529934883118 - trainLoss: 0.6486255526542664\n",
      "cnt: 0 - valLoss: 0.6473507881164551 - trainLoss: 0.6486235857009888\n",
      "cnt: 0 - valLoss: 0.6473482847213745 - trainLoss: 0.6486216187477112\n",
      "cnt: 0 - valLoss: 0.647345781326294 - trainLoss: 0.6486195921897888\n",
      "cnt: 0 - valLoss: 0.6473433971405029 - trainLoss: 0.6486175060272217\n",
      "cnt: 0 - valLoss: 0.6473409533500671 - trainLoss: 0.6486154794692993\n",
      "cnt: 0 - valLoss: 0.6473384499549866 - trainLoss: 0.6486133933067322\n",
      "cnt: 0 - valLoss: 0.6473361849784851 - trainLoss: 0.6486113667488098\n",
      "cnt: 0 - valLoss: 0.6473337411880493 - trainLoss: 0.6486095190048218\n",
      "cnt: 0 - valLoss: 0.6473312973976135 - trainLoss: 0.6486074924468994\n",
      "cnt: 0 - valLoss: 0.6473288536071777 - trainLoss: 0.648605465888977\n",
      "cnt: 0 - valLoss: 0.6473264098167419 - trainLoss: 0.6486033797264099\n",
      "cnt: 0 - valLoss: 0.6473239660263062 - trainLoss: 0.6486013531684875\n",
      "cnt: 0 - valLoss: 0.6473216414451599 - trainLoss: 0.6485992670059204\n",
      "cnt: 0 - valLoss: 0.6473192572593689 - trainLoss: 0.6485973000526428\n",
      "cnt: 0 - valLoss: 0.6473168134689331 - trainLoss: 0.6485953330993652\n",
      "cnt: 0 - valLoss: 0.6473142504692078 - trainLoss: 0.6485932469367981\n",
      "cnt: 0 - valLoss: 0.6473118662834167 - trainLoss: 0.6485912203788757\n",
      "cnt: 0 - valLoss: 0.6473093628883362 - trainLoss: 0.6485891342163086\n",
      "cnt: 0 - valLoss: 0.6473070383071899 - trainLoss: 0.6485871076583862\n",
      "cnt: 0 - valLoss: 0.6473047733306885 - trainLoss: 0.6485851407051086\n",
      "cnt: 0 - valLoss: 0.6473021507263184 - trainLoss: 0.648583173751831\n",
      "cnt: 0 - valLoss: 0.6472997069358826 - trainLoss: 0.6485810875892639\n",
      "cnt: 0 - valLoss: 0.6472972631454468 - trainLoss: 0.6485790014266968\n",
      "cnt: 0 - valLoss: 0.647294819355011 - trainLoss: 0.6485769748687744\n",
      "cnt: 0 - valLoss: 0.6472923755645752 - trainLoss: 0.648574948310852\n",
      "cnt: 0 - valLoss: 0.6472901701927185 - trainLoss: 0.6485729217529297\n",
      "cnt: 0 - valLoss: 0.6472876667976379 - trainLoss: 0.6485710144042969\n",
      "cnt: 0 - valLoss: 0.6472851634025574 - trainLoss: 0.6485689282417297\n",
      "cnt: 0 - valLoss: 0.6472827792167664 - trainLoss: 0.6485668420791626\n",
      "cnt: 0 - valLoss: 0.6472802758216858 - trainLoss: 0.6485648155212402\n",
      "cnt: 0 - valLoss: 0.6472778916358948 - trainLoss: 0.6485627293586731\n",
      "cnt: 0 - valLoss: 0.6472756266593933 - trainLoss: 0.6485607028007507\n",
      "cnt: 0 - valLoss: 0.6472731828689575 - trainLoss: 0.6485588550567627\n",
      "cnt: 0 - valLoss: 0.6472707390785217 - trainLoss: 0.6485567688941956\n",
      "cnt: 0 - valLoss: 0.6472681760787964 - trainLoss: 0.6485546827316284\n",
      "cnt: 0 - valLoss: 0.6472656726837158 - trainLoss: 0.648552656173706\n",
      "cnt: 0 - valLoss: 0.6472632884979248 - trainLoss: 0.6485505700111389\n",
      "cnt: 0 - valLoss: 0.6472610831260681 - trainLoss: 0.6485485434532166\n",
      "cnt: 0 - valLoss: 0.6472585797309875 - trainLoss: 0.6485466957092285\n",
      "cnt: 0 - valLoss: 0.6472561359405518 - trainLoss: 0.6485446095466614\n",
      "cnt: 0 - valLoss: 0.6472536325454712 - trainLoss: 0.6485425233840942\n",
      "cnt: 0 - valLoss: 0.6472511887550354 - trainLoss: 0.6485404968261719\n",
      "cnt: 0 - valLoss: 0.6472487449645996 - trainLoss: 0.6485384106636047\n",
      "cnt: 0 - valLoss: 0.6472465991973877 - trainLoss: 0.6485363245010376\n",
      "cnt: 0 - valLoss: 0.6472440958023071 - trainLoss: 0.6485344767570496\n",
      "cnt: 0 - valLoss: 0.6472416520118713 - trainLoss: 0.6485323309898376\n",
      "cnt: 0 - valLoss: 0.6472392082214355 - trainLoss: 0.6485303640365601\n",
      "cnt: 0 - valLoss: 0.6472367644309998 - trainLoss: 0.6485283374786377\n",
      "cnt: 0 - valLoss: 0.6472342014312744 - trainLoss: 0.6485261917114258\n",
      "cnt: 0 - valLoss: 0.6472321152687073 - trainLoss: 0.6485241055488586\n",
      "cnt: 0 - valLoss: 0.6472296714782715 - trainLoss: 0.6485222578048706\n",
      "cnt: 0 - valLoss: 0.6472271680831909 - trainLoss: 0.6485201716423035\n",
      "cnt: 0 - valLoss: 0.6472247242927551 - trainLoss: 0.6485181450843811\n",
      "cnt: 0 - valLoss: 0.6472222805023193 - trainLoss: 0.648516058921814\n",
      "cnt: 0 - valLoss: 0.6472197771072388 - trainLoss: 0.6485139727592468\n",
      "cnt: 0 - valLoss: 0.6472175717353821 - trainLoss: 0.6485118865966797\n",
      "cnt: 0 - valLoss: 0.6472151875495911 - trainLoss: 0.6485099792480469\n",
      "cnt: 0 - valLoss: 0.6472126841545105 - trainLoss: 0.6485079526901245\n",
      "cnt: 0 - valLoss: 0.6472102403640747 - trainLoss: 0.6485059261322021\n",
      "cnt: 0 - valLoss: 0.6472077369689941 - trainLoss: 0.6485037803649902\n",
      "cnt: 0 - valLoss: 0.6472052335739136 - trainLoss: 0.6485017538070679\n",
      "cnt: 0 - valLoss: 0.6472030878067017 - trainLoss: 0.6484996676445007\n",
      "cnt: 0 - valLoss: 0.6472005844116211 - trainLoss: 0.6484977006912231\n",
      "cnt: 0 - valLoss: 0.6471981406211853 - trainLoss: 0.6484957337379456\n",
      "cnt: 0 - valLoss: 0.6471956968307495 - trainLoss: 0.6484935879707336\n",
      "cnt: 0 - valLoss: 0.647193193435669 - trainLoss: 0.6484915614128113\n",
      "cnt: 0 - valLoss: 0.6471907496452332 - trainLoss: 0.6484894752502441\n",
      "cnt: 0 - valLoss: 0.6471885442733765 - trainLoss: 0.648487389087677\n",
      "cnt: 0 - valLoss: 0.6471861600875854 - trainLoss: 0.6484854817390442\n",
      "cnt: 0 - valLoss: 0.6471835970878601 - trainLoss: 0.6484835147857666\n",
      "cnt: 0 - valLoss: 0.6471811532974243 - trainLoss: 0.6484814286231995\n",
      "cnt: 0 - valLoss: 0.6471787095069885 - trainLoss: 0.6484792828559875\n",
      "cnt: 0 - valLoss: 0.647176206111908 - trainLoss: 0.6484771966934204\n",
      "cnt: 0 - valLoss: 0.6471740007400513 - trainLoss: 0.648475170135498\n",
      "cnt: 0 - valLoss: 0.6471715569496155 - trainLoss: 0.6484732031822205\n",
      "cnt: 0 - valLoss: 0.6471691131591797 - trainLoss: 0.6484711766242981\n",
      "cnt: 0 - valLoss: 0.6471666693687439 - trainLoss: 0.6484691500663757\n",
      "cnt: 0 - valLoss: 0.6471641063690186 - trainLoss: 0.6484670042991638\n",
      "cnt: 0 - valLoss: 0.6471616625785828 - trainLoss: 0.6484649181365967\n",
      "cnt: 0 - valLoss: 0.6471595168113708 - trainLoss: 0.6484628915786743\n",
      "cnt: 0 - valLoss: 0.6471570730209351 - trainLoss: 0.6484609842300415\n",
      "cnt: 0 - valLoss: 0.6471545696258545 - trainLoss: 0.6484588980674744\n",
      "cnt: 0 - valLoss: 0.6471521258354187 - trainLoss: 0.648456871509552\n",
      "cnt: 0 - valLoss: 0.6471496224403381 - trainLoss: 0.6484547853469849\n",
      "cnt: 0 - valLoss: 0.6471471786499023 - trainLoss: 0.6484527587890625\n",
      "cnt: 0 - valLoss: 0.6471448540687561 - trainLoss: 0.6484506130218506\n",
      "cnt: 0 - valLoss: 0.6471424698829651 - trainLoss: 0.648448646068573\n",
      "cnt: 0 - valLoss: 0.6471400856971741 - trainLoss: 0.6484466791152954\n",
      "cnt: 0 - valLoss: 0.6471375823020935 - trainLoss: 0.6484445333480835\n",
      "cnt: 0 - valLoss: 0.6471350789070129 - trainLoss: 0.6484425663948059\n",
      "cnt: 0 - valLoss: 0.6471326351165771 - trainLoss: 0.6484403610229492\n",
      "cnt: 0 - valLoss: 0.6471303701400757 - trainLoss: 0.6484383344650269\n",
      "cnt: 0 - valLoss: 0.6471279859542847 - trainLoss: 0.6484363675117493\n",
      "cnt: 0 - valLoss: 0.6471254825592041 - trainLoss: 0.6484343409538269\n",
      "cnt: 0 - valLoss: 0.6471229791641235 - trainLoss: 0.6484322547912598\n",
      "cnt: 0 - valLoss: 0.6471205353736877 - trainLoss: 0.6484301686286926\n",
      "cnt: 0 - valLoss: 0.6471181511878967 - trainLoss: 0.6484281420707703\n",
      "cnt: 0 - valLoss: 0.6471158266067505 - trainLoss: 0.6484261155128479\n",
      "cnt: 0 - valLoss: 0.6471134424209595 - trainLoss: 0.6484240889549255\n",
      "cnt: 0 - valLoss: 0.6471109390258789 - trainLoss: 0.6484220623970032\n",
      "cnt: 0 - valLoss: 0.6471084356307983 - trainLoss: 0.648419976234436\n",
      "cnt: 0 - valLoss: 0.6471059322357178 - trainLoss: 0.6484179496765137\n",
      "cnt: 0 - valLoss: 0.6471034288406372 - trainLoss: 0.6484158635139465\n",
      "cnt: 0 - valLoss: 0.6471012234687805 - trainLoss: 0.6484137177467346\n",
      "cnt: 0 - valLoss: 0.6470988392829895 - trainLoss: 0.6484118103981018\n",
      "cnt: 0 - valLoss: 0.6470963358879089 - trainLoss: 0.6484097838401794\n",
      "cnt: 0 - valLoss: 0.6470938920974731 - trainLoss: 0.6484076380729675\n",
      "cnt: 0 - valLoss: 0.6470914483070374 - trainLoss: 0.6484055519104004\n",
      "cnt: 0 - valLoss: 0.647088885307312 - trainLoss: 0.648403525352478\n",
      "cnt: 0 - valLoss: 0.6470866799354553 - trainLoss: 0.6484014391899109\n",
      "cnt: 0 - valLoss: 0.6470842957496643 - trainLoss: 0.6483994722366333\n",
      "cnt: 0 - valLoss: 0.647081732749939 - trainLoss: 0.6483974456787109\n",
      "cnt: 0 - valLoss: 0.6470792889595032 - trainLoss: 0.6483953595161438\n",
      "cnt: 0 - valLoss: 0.6470767855644226 - trainLoss: 0.6483932733535767\n",
      "cnt: 0 - valLoss: 0.6470743417739868 - trainLoss: 0.6483911871910095\n",
      "cnt: 0 - valLoss: 0.6470720767974854 - trainLoss: 0.6483891010284424\n",
      "cnt: 0 - valLoss: 0.6470696926116943 - trainLoss: 0.6483871936798096\n",
      "cnt: 0 - valLoss: 0.6470671892166138 - trainLoss: 0.6483851075172424\n",
      "cnt: 0 - valLoss: 0.647064745426178 - trainLoss: 0.6483830809593201\n",
      "cnt: 0 - valLoss: 0.6470622420310974 - trainLoss: 0.6483809351921082\n",
      "cnt: 0 - valLoss: 0.6470596790313721 - trainLoss: 0.6483789086341858\n",
      "cnt: 0 - valLoss: 0.6470574736595154 - trainLoss: 0.6483768224716187\n",
      "cnt: 0 - valLoss: 0.6470550894737244 - trainLoss: 0.6483748555183411\n",
      "cnt: 0 - valLoss: 0.6470525860786438 - trainLoss: 0.6483727693557739\n",
      "cnt: 0 - valLoss: 0.6470500826835632 - trainLoss: 0.6483707427978516\n",
      "cnt: 0 - valLoss: 0.6470475792884827 - trainLoss: 0.6483686566352844\n",
      "cnt: 0 - valLoss: 0.6470450758934021 - trainLoss: 0.6483665108680725\n",
      "cnt: 0 - valLoss: 0.6470429301261902 - trainLoss: 0.6483644247055054\n",
      "cnt: 0 - valLoss: 0.6470404863357544 - trainLoss: 0.6483625173568726\n",
      "cnt: 0 - valLoss: 0.6470379829406738 - trainLoss: 0.6483604311943054\n",
      "cnt: 0 - valLoss: 0.6470354795455933 - trainLoss: 0.6483584046363831\n",
      "cnt: 0 - valLoss: 0.6470329761505127 - trainLoss: 0.6483562588691711\n",
      "cnt: 0 - valLoss: 0.6470305323600769 - trainLoss: 0.6483542323112488\n",
      "cnt: 0 - valLoss: 0.6470283269882202 - trainLoss: 0.6483520865440369\n",
      "cnt: 0 - valLoss: 0.6470258235931396 - trainLoss: 0.648350179195404\n",
      "cnt: 0 - valLoss: 0.6470233798027039 - trainLoss: 0.6483481526374817\n",
      "cnt: 0 - valLoss: 0.6470208168029785 - trainLoss: 0.6483460068702698\n",
      "cnt: 0 - valLoss: 0.6470183730125427 - trainLoss: 0.6483439207077026\n",
      "cnt: 0 - valLoss: 0.6470158696174622 - trainLoss: 0.6483417749404907\n",
      "cnt: 0 - valLoss: 0.6470137238502502 - trainLoss: 0.6483397483825684\n",
      "cnt: 0 - valLoss: 0.6470112204551697 - trainLoss: 0.6483378410339355\n",
      "cnt: 0 - valLoss: 0.6470087170600891 - trainLoss: 0.6483357548713684\n",
      "cnt: 0 - valLoss: 0.6470062136650085 - trainLoss: 0.6483336687088013\n",
      "cnt: 0 - valLoss: 0.647003710269928 - trainLoss: 0.6483315825462341\n",
      "cnt: 0 - valLoss: 0.6470012664794922 - trainLoss: 0.6483294367790222\n",
      "cnt: 0 - valLoss: 0.6469991207122803 - trainLoss: 0.6483273506164551\n",
      "cnt: 0 - valLoss: 0.6469965577125549 - trainLoss: 0.6483254432678223\n",
      "cnt: 0 - valLoss: 0.6469941139221191 - trainLoss: 0.6483234167098999\n",
      "cnt: 0 - valLoss: 0.6469916105270386 - trainLoss: 0.6483213305473328\n",
      "cnt: 0 - valLoss: 0.6469890475273132 - trainLoss: 0.6483191847801208\n",
      "cnt: 0 - valLoss: 0.6469866633415222 - trainLoss: 0.6483170390129089\n",
      "cnt: 0 - valLoss: 0.6469844579696655 - trainLoss: 0.6483150124549866\n",
      "cnt: 0 - valLoss: 0.6469818949699402 - trainLoss: 0.6483131051063538\n",
      "cnt: 0 - valLoss: 0.6469794511795044 - trainLoss: 0.6483109593391418\n",
      "cnt: 0 - valLoss: 0.6469769477844238 - trainLoss: 0.6483088731765747\n",
      "cnt: 0 - valLoss: 0.6469744443893433 - trainLoss: 0.6483067870140076\n",
      "cnt: 0 - valLoss: 0.6469720005989075 - trainLoss: 0.6483047008514404\n",
      "cnt: 0 - valLoss: 0.646969735622406 - trainLoss: 0.6483026742935181\n",
      "cnt: 0 - valLoss: 0.6469672322273254 - trainLoss: 0.6483007073402405\n",
      "cnt: 0 - valLoss: 0.6469647288322449 - trainLoss: 0.6482985615730286\n",
      "cnt: 0 - valLoss: 0.6469622850418091 - trainLoss: 0.6482965350151062\n",
      "cnt: 0 - valLoss: 0.6469597816467285 - trainLoss: 0.6482944488525391\n",
      "cnt: 0 - valLoss: 0.6469573974609375 - trainLoss: 0.6482923030853271\n",
      "cnt: 0 - valLoss: 0.646955132484436 - trainLoss: 0.6482902765274048\n",
      "cnt: 0 - valLoss: 0.6469525694847107 - trainLoss: 0.6482883095741272\n",
      "cnt: 0 - valLoss: 0.6469501256942749 - trainLoss: 0.6482862234115601\n",
      "cnt: 0 - valLoss: 0.6469476222991943 - trainLoss: 0.6482840776443481\n",
      "cnt: 0 - valLoss: 0.646945059299469 - trainLoss: 0.648281991481781\n",
      "cnt: 0 - valLoss: 0.6469427943229675 - trainLoss: 0.6482799053192139\n",
      "cnt: 0 - valLoss: 0.6469404101371765 - trainLoss: 0.6482778787612915\n",
      "cnt: 0 - valLoss: 0.6469379663467407 - trainLoss: 0.6482758522033691\n",
      "cnt: 0 - valLoss: 0.6469354033470154 - trainLoss: 0.648273766040802\n",
      "cnt: 0 - valLoss: 0.6469329595565796 - trainLoss: 0.6482716202735901\n",
      "cnt: 0 - valLoss: 0.646930456161499 - trainLoss: 0.648269534111023\n",
      "cnt: 0 - valLoss: 0.6469281911849976 - trainLoss: 0.6482674479484558\n",
      "cnt: 0 - valLoss: 0.6469257473945618 - trainLoss: 0.6482654213905334\n",
      "cnt: 0 - valLoss: 0.6469232439994812 - trainLoss: 0.6482634544372559\n",
      "cnt: 0 - valLoss: 0.6469207406044006 - trainLoss: 0.648261308670044\n",
      "cnt: 0 - valLoss: 0.6469182372093201 - trainLoss: 0.6482592821121216\n",
      "cnt: 0 - valLoss: 0.6469157338142395 - trainLoss: 0.6482570767402649\n",
      "cnt: 0 - valLoss: 0.646913468837738 - trainLoss: 0.6482550501823425\n",
      "cnt: 0 - valLoss: 0.646911084651947 - trainLoss: 0.6482530832290649\n",
      "cnt: 0 - valLoss: 0.6469085216522217 - trainLoss: 0.6482510566711426\n",
      "cnt: 0 - valLoss: 0.6469060778617859 - trainLoss: 0.6482489109039307\n",
      "cnt: 0 - valLoss: 0.6469035148620605 - trainLoss: 0.6482467651367188\n",
      "cnt: 0 - valLoss: 0.64690101146698 - trainLoss: 0.6482447385787964\n",
      "cnt: 0 - valLoss: 0.6468988060951233 - trainLoss: 0.6482425928115845\n",
      "cnt: 0 - valLoss: 0.6468963623046875 - trainLoss: 0.6482406854629517\n",
      "cnt: 0 - valLoss: 0.6468938589096069 - trainLoss: 0.6482385396957397\n",
      "cnt: 0 - valLoss: 0.6468914151191711 - trainLoss: 0.6482364535331726\n",
      "cnt: 0 - valLoss: 0.646888792514801 - trainLoss: 0.6482344269752502\n",
      "cnt: 0 - valLoss: 0.6468863487243652 - trainLoss: 0.6482322216033936\n",
      "cnt: 0 - valLoss: 0.6468841433525085 - trainLoss: 0.6482301354408264\n",
      "cnt: 0 - valLoss: 0.646881639957428 - trainLoss: 0.6482282280921936\n",
      "cnt: 0 - valLoss: 0.6468791365623474 - trainLoss: 0.6482261419296265\n",
      "cnt: 0 - valLoss: 0.6468766331672668 - trainLoss: 0.6482239961624146\n",
      "cnt: 0 - valLoss: 0.6468740701675415 - trainLoss: 0.6482219696044922\n",
      "cnt: 0 - valLoss: 0.6468716859817505 - trainLoss: 0.6482198238372803\n",
      "cnt: 0 - valLoss: 0.646869421005249 - trainLoss: 0.6482176780700684\n",
      "cnt: 0 - valLoss: 0.6468669176101685 - trainLoss: 0.6482157707214355\n",
      "cnt: 0 - valLoss: 0.6468644142150879 - trainLoss: 0.6482136249542236\n",
      "cnt: 0 - valLoss: 0.6468619108200073 - trainLoss: 0.6482115983963013\n",
      "cnt: 0 - valLoss: 0.6468594074249268 - trainLoss: 0.6482094526290894\n",
      "cnt: 0 - valLoss: 0.646856963634491 - trainLoss: 0.6482073068618774\n",
      "cnt: 0 - valLoss: 0.6468546390533447 - trainLoss: 0.6482052803039551\n",
      "cnt: 0 - valLoss: 0.6468521356582642 - trainLoss: 0.6482033133506775\n",
      "cnt: 0 - valLoss: 0.6468496322631836 - trainLoss: 0.6482012271881104\n",
      "cnt: 0 - valLoss: 0.646847128868103 - trainLoss: 0.6481990814208984\n",
      "cnt: 0 - valLoss: 0.6468446850776672 - trainLoss: 0.6481969952583313\n",
      "cnt: 0 - valLoss: 0.6468422412872314 - trainLoss: 0.6481948494911194\n",
      "cnt: 0 - valLoss: 0.64683997631073 - trainLoss: 0.6481928825378418\n",
      "cnt: 0 - valLoss: 0.6468374729156494 - trainLoss: 0.6481908559799194\n",
      "cnt: 0 - valLoss: 0.6468349099159241 - trainLoss: 0.6481887102127075\n",
      "cnt: 0 - valLoss: 0.6468324065208435 - trainLoss: 0.6481866240501404\n",
      "cnt: 0 - valLoss: 0.6468299031257629 - trainLoss: 0.6481844782829285\n",
      "cnt: 0 - valLoss: 0.6468275785446167 - trainLoss: 0.6481823921203613\n",
      "cnt: 0 - valLoss: 0.6468251943588257 - trainLoss: 0.648180365562439\n",
      "cnt: 0 - valLoss: 0.6468226909637451 - trainLoss: 0.6481783986091614\n",
      "cnt: 0 - valLoss: 0.6468202471733093 - trainLoss: 0.6481762528419495\n",
      "cnt: 0 - valLoss: 0.646817684173584 - trainLoss: 0.6481741666793823\n",
      "cnt: 0 - valLoss: 0.6468151211738586 - trainLoss: 0.6481720209121704\n",
      "cnt: 0 - valLoss: 0.646812915802002 - trainLoss: 0.6481698751449585\n",
      "cnt: 0 - valLoss: 0.6468104124069214 - trainLoss: 0.6481679677963257\n",
      "cnt: 0 - valLoss: 0.6468079090118408 - trainLoss: 0.6481658220291138\n",
      "cnt: 0 - valLoss: 0.6468054056167603 - trainLoss: 0.6481637954711914\n",
      "cnt: 0 - valLoss: 0.6468028426170349 - trainLoss: 0.6481616497039795\n",
      "cnt: 0 - valLoss: 0.6468004584312439 - trainLoss: 0.6481595039367676\n",
      "cnt: 0 - valLoss: 0.6467981338500977 - trainLoss: 0.6481574177742004\n",
      "cnt: 0 - valLoss: 0.6467956304550171 - trainLoss: 0.6481554508209229\n",
      "cnt: 0 - valLoss: 0.6467931270599365 - trainLoss: 0.6481533646583557\n",
      "cnt: 0 - valLoss: 0.6467906832695007 - trainLoss: 0.6481512188911438\n",
      "cnt: 0 - valLoss: 0.6467881202697754 - trainLoss: 0.6481490731239319\n",
      "cnt: 0 - valLoss: 0.6467856764793396 - trainLoss: 0.6481469869613647\n",
      "cnt: 0 - valLoss: 0.6467834115028381 - trainLoss: 0.6481449007987976\n",
      "cnt: 0 - valLoss: 0.6467809081077576 - trainLoss: 0.64814293384552\n",
      "cnt: 0 - valLoss: 0.646778404712677 - trainLoss: 0.6481408476829529\n",
      "cnt: 0 - valLoss: 0.6467759013175964 - trainLoss: 0.648138701915741\n",
      "cnt: 0 - valLoss: 0.6467733383178711 - trainLoss: 0.6481366157531738\n",
      "cnt: 0 - valLoss: 0.6467709541320801 - trainLoss: 0.6481344699859619\n",
      "cnt: 0 - valLoss: 0.6467686295509338 - trainLoss: 0.6481323838233948\n",
      "cnt: 0 - valLoss: 0.6467661261558533 - trainLoss: 0.6481304168701172\n",
      "cnt: 0 - valLoss: 0.6467635631561279 - trainLoss: 0.6481282711029053\n",
      "cnt: 0 - valLoss: 0.6467611193656921 - trainLoss: 0.6481261849403381\n",
      "cnt: 0 - valLoss: 0.646758496761322 - trainLoss: 0.648124098777771\n",
      "cnt: 0 - valLoss: 0.6467562317848206 - trainLoss: 0.6481219530105591\n",
      "cnt: 0 - valLoss: 0.6467537879943848 - trainLoss: 0.6481199264526367\n",
      "cnt: 0 - valLoss: 0.646751344203949 - trainLoss: 0.6481179594993591\n",
      "cnt: 0 - valLoss: 0.6467487812042236 - trainLoss: 0.6481158137321472\n",
      "cnt: 0 - valLoss: 0.6467462778091431 - trainLoss: 0.6481136679649353\n",
      "cnt: 0 - valLoss: 0.6467437744140625 - trainLoss: 0.6481115221977234\n",
      "cnt: 0 - valLoss: 0.6467413902282715 - trainLoss: 0.648109495639801\n",
      "cnt: 0 - valLoss: 0.6467389464378357 - trainLoss: 0.6481075286865234\n",
      "cnt: 0 - valLoss: 0.6467363834381104 - trainLoss: 0.6481053829193115\n",
      "cnt: 0 - valLoss: 0.6467339396476746 - trainLoss: 0.6481032371520996\n",
      "cnt: 0 - valLoss: 0.646731436252594 - trainLoss: 0.6481011509895325\n",
      "cnt: 0 - valLoss: 0.6467288732528687 - trainLoss: 0.6480990052223206\n",
      "cnt: 0 - valLoss: 0.646726667881012 - trainLoss: 0.6480968594551086\n",
      "cnt: 0 - valLoss: 0.6467241048812866 - trainLoss: 0.6480950117111206\n",
      "cnt: 0 - valLoss: 0.646721601486206 - trainLoss: 0.6480928063392639\n",
      "cnt: 0 - valLoss: 0.6467190980911255 - trainLoss: 0.6480907201766968\n",
      "cnt: 0 - valLoss: 0.6467165350914001 - trainLoss: 0.6480886340141296\n",
      "cnt: 0 - valLoss: 0.6467140913009644 - trainLoss: 0.648086428642273\n",
      "cnt: 0 - valLoss: 0.6467117667198181 - trainLoss: 0.6480843424797058\n",
      "cnt: 0 - valLoss: 0.6467092633247375 - trainLoss: 0.648082435131073\n",
      "cnt: 0 - valLoss: 0.6467067003250122 - trainLoss: 0.6480802893638611\n",
      "cnt: 0 - valLoss: 0.6467041969299316 - trainLoss: 0.6480781435966492\n",
      "cnt: 0 - valLoss: 0.6467015743255615 - trainLoss: 0.648076057434082\n",
      "cnt: 0 - valLoss: 0.6466992497444153 - trainLoss: 0.6480739116668701\n",
      "cnt: 0 - valLoss: 0.646696925163269 - trainLoss: 0.6480718851089478\n",
      "cnt: 0 - valLoss: 0.6466943621635437 - trainLoss: 0.6480698585510254\n",
      "cnt: 0 - valLoss: 0.6466918587684631 - trainLoss: 0.6480677723884583\n",
      "cnt: 0 - valLoss: 0.6466893553733826 - trainLoss: 0.6480656266212463\n",
      "cnt: 0 - valLoss: 0.6466867923736572 - trainLoss: 0.6480634808540344\n",
      "cnt: 0 - valLoss: 0.646684467792511 - trainLoss: 0.6480613350868225\n",
      "cnt: 0 - valLoss: 0.6466820240020752 - trainLoss: 0.6480593681335449\n",
      "cnt: 0 - valLoss: 0.6466794610023499 - trainLoss: 0.6480572819709778\n",
      "cnt: 0 - valLoss: 0.6466769576072693 - trainLoss: 0.6480551958084106\n",
      "cnt: 0 - valLoss: 0.646674394607544 - trainLoss: 0.6480530500411987\n",
      "cnt: 0 - valLoss: 0.6466719508171082 - trainLoss: 0.648050844669342\n",
      "cnt: 0 - valLoss: 0.6466696858406067 - trainLoss: 0.6480487585067749\n",
      "cnt: 0 - valLoss: 0.6466671228408813 - trainLoss: 0.6480468511581421\n",
      "cnt: 0 - valLoss: 0.646664559841156 - trainLoss: 0.6480447053909302\n",
      "cnt: 0 - valLoss: 0.6466620564460754 - trainLoss: 0.648042619228363\n",
      "cnt: 0 - valLoss: 0.6466594934463501 - trainLoss: 0.6480404734611511\n",
      "cnt: 0 - valLoss: 0.6466570496559143 - trainLoss: 0.6480383276939392\n",
      "cnt: 0 - valLoss: 0.6466547250747681 - trainLoss: 0.6480361819267273\n",
      "cnt: 0 - valLoss: 0.6466522812843323 - trainLoss: 0.6480342149734497\n",
      "cnt: 0 - valLoss: 0.6466497182846069 - trainLoss: 0.6480321288108826\n",
      "cnt: 0 - valLoss: 0.6466470956802368 - trainLoss: 0.6480299830436707\n",
      "cnt: 0 - valLoss: 0.6466445922851562 - trainLoss: 0.6480278968811035\n",
      "cnt: 0 - valLoss: 0.6466422080993652 - trainLoss: 0.6480257511138916\n",
      "cnt: 0 - valLoss: 0.6466398239135742 - trainLoss: 0.6480236649513245\n",
      "cnt: 0 - valLoss: 0.6466373205184937 - trainLoss: 0.6480216979980469\n",
      "cnt: 0 - valLoss: 0.6466347575187683 - trainLoss: 0.648019552230835\n",
      "cnt: 0 - valLoss: 0.646632194519043 - trainLoss: 0.648017406463623\n",
      "cnt: 0 - valLoss: 0.6466296911239624 - trainLoss: 0.6480152606964111\n",
      "cnt: 0 - valLoss: 0.6466274261474609 - trainLoss: 0.6480131149291992\n",
      "cnt: 0 - valLoss: 0.6466249823570251 - trainLoss: 0.6480110883712769\n",
      "cnt: 0 - valLoss: 0.646622359752655 - trainLoss: 0.6480090618133545\n",
      "cnt: 0 - valLoss: 0.6466197967529297 - trainLoss: 0.6480069160461426\n",
      "cnt: 0 - valLoss: 0.6466173529624939 - trainLoss: 0.6480047702789307\n",
      "cnt: 0 - valLoss: 0.6466147303581238 - trainLoss: 0.6480026841163635\n",
      "cnt: 0 - valLoss: 0.6466125249862671 - trainLoss: 0.6480004787445068\n",
      "cnt: 0 - valLoss: 0.6466099619865417 - trainLoss: 0.6479986310005188\n",
      "cnt: 0 - valLoss: 0.6466073989868164 - trainLoss: 0.6479963660240173\n",
      "cnt: 0 - valLoss: 0.6466048955917358 - trainLoss: 0.647994339466095\n",
      "cnt: 0 - valLoss: 0.6466023921966553 - trainLoss: 0.6479921340942383\n",
      "cnt: 0 - valLoss: 0.6465999484062195 - trainLoss: 0.6479899883270264\n",
      "cnt: 0 - valLoss: 0.6465976238250732 - trainLoss: 0.647987961769104\n",
      "cnt: 0 - valLoss: 0.6465950608253479 - trainLoss: 0.6479859352111816\n",
      "cnt: 0 - valLoss: 0.6465924978256226 - trainLoss: 0.6479837894439697\n",
      "cnt: 0 - valLoss: 0.6465900540351868 - trainLoss: 0.6479817032814026\n",
      "cnt: 0 - valLoss: 0.6465873718261719 - trainLoss: 0.6479794979095459\n",
      "cnt: 0 - valLoss: 0.6465851068496704 - trainLoss: 0.6479774117469788\n",
      "cnt: 0 - valLoss: 0.6465826630592346 - trainLoss: 0.6479753851890564\n",
      "cnt: 0 - valLoss: 0.6465801000595093 - trainLoss: 0.6479732990264893\n",
      "cnt: 0 - valLoss: 0.6465775370597839 - trainLoss: 0.6479711532592773\n",
      "cnt: 0 - valLoss: 0.6465750336647034 - trainLoss: 0.6479690074920654\n",
      "cnt: 0 - valLoss: 0.6465724110603333 - trainLoss: 0.6479669213294983\n",
      "cnt: 0 - valLoss: 0.6465702056884766 - trainLoss: 0.6479647159576416\n",
      "cnt: 0 - valLoss: 0.646567702293396 - trainLoss: 0.6479628086090088\n",
      "cnt: 0 - valLoss: 0.6465651392936707 - trainLoss: 0.6479606628417969\n",
      "cnt: 0 - valLoss: 0.6465625762939453 - trainLoss: 0.647958517074585\n",
      "cnt: 0 - valLoss: 0.6465600728988647 - trainLoss: 0.6479564309120178\n",
      "cnt: 0 - valLoss: 0.6465575695037842 - trainLoss: 0.6479542255401611\n",
      "cnt: 0 - valLoss: 0.6465552449226379 - trainLoss: 0.647952139377594\n",
      "cnt: 0 - valLoss: 0.6465527415275574 - trainLoss: 0.6479501724243164\n",
      "cnt: 0 - valLoss: 0.6465502381324768 - trainLoss: 0.6479480266571045\n",
      "cnt: 0 - valLoss: 0.6465476751327515 - trainLoss: 0.6479458808898926\n",
      "cnt: 0 - valLoss: 0.6465450525283813 - trainLoss: 0.6479437351226807\n",
      "cnt: 0 - valLoss: 0.6465427279472351 - trainLoss: 0.647941529750824\n",
      "cnt: 0 - valLoss: 0.6465402841567993 - trainLoss: 0.6479395627975464\n",
      "cnt: 0 - valLoss: 0.6465377807617188 - trainLoss: 0.6479374766349792\n",
      "cnt: 0 - valLoss: 0.6465352177619934 - trainLoss: 0.6479353308677673\n",
      "cnt: 0 - valLoss: 0.6465326547622681 - trainLoss: 0.6479331851005554\n",
      "cnt: 0 - valLoss: 0.6465300917625427 - trainLoss: 0.6479310989379883\n",
      "cnt: 0 - valLoss: 0.6465278267860413 - trainLoss: 0.6479289531707764\n",
      "cnt: 0 - valLoss: 0.6465253233909607 - trainLoss: 0.647926926612854\n",
      "cnt: 0 - valLoss: 0.6465227603912354 - trainLoss: 0.6479248404502869\n",
      "cnt: 0 - valLoss: 0.6465202569961548 - trainLoss: 0.6479227542877197\n",
      "cnt: 0 - valLoss: 0.6465176343917847 - trainLoss: 0.647920548915863\n",
      "cnt: 0 - valLoss: 0.6465151906013489 - trainLoss: 0.6479184031486511\n",
      "cnt: 0 - valLoss: 0.6465129256248474 - trainLoss: 0.6479162573814392\n",
      "cnt: 0 - valLoss: 0.6465103626251221 - trainLoss: 0.6479142904281616\n",
      "cnt: 0 - valLoss: 0.6465077996253967 - trainLoss: 0.6479122042655945\n",
      "cnt: 0 - valLoss: 0.6465051770210266 - trainLoss: 0.6479099988937378\n",
      "cnt: 0 - valLoss: 0.646502673625946 - trainLoss: 0.6479078531265259\n",
      "cnt: 0 - valLoss: 0.646500289440155 - trainLoss: 0.647905707359314\n",
      "cnt: 0 - valLoss: 0.646497905254364 - trainLoss: 0.6479036808013916\n",
      "cnt: 0 - valLoss: 0.6464952826499939 - trainLoss: 0.6479015946388245\n",
      "cnt: 0 - valLoss: 0.6464927792549133 - trainLoss: 0.6478994488716125\n",
      "cnt: 0 - valLoss: 0.6464902758598328 - trainLoss: 0.6478973031044006\n",
      "cnt: 0 - valLoss: 0.6464876532554626 - trainLoss: 0.647895097732544\n",
      "cnt: 0 - valLoss: 0.6464853882789612 - trainLoss: 0.647892951965332\n",
      "cnt: 0 - valLoss: 0.6464828848838806 - trainLoss: 0.6478909850120544\n",
      "cnt: 0 - valLoss: 0.6464803218841553 - trainLoss: 0.6478888392448425\n",
      "cnt: 0 - valLoss: 0.6464777588844299 - trainLoss: 0.6478867530822754\n",
      "cnt: 0 - valLoss: 0.6464751362800598 - trainLoss: 0.6478846073150635\n",
      "cnt: 0 - valLoss: 0.646472692489624 - trainLoss: 0.6478824615478516\n",
      "cnt: 0 - valLoss: 0.6464704275131226 - trainLoss: 0.6478803157806396\n",
      "cnt: 0 - valLoss: 0.6464678645133972 - trainLoss: 0.6478783488273621\n",
      "cnt: 0 - valLoss: 0.6464653015136719 - trainLoss: 0.6478762030601501\n",
      "cnt: 0 - valLoss: 0.6464627385139465 - trainLoss: 0.6478740572929382\n",
      "cnt: 0 - valLoss: 0.6464601159095764 - trainLoss: 0.6478719115257263\n",
      "cnt: 0 - valLoss: 0.6464577913284302 - trainLoss: 0.6478697061538696\n",
      "cnt: 0 - valLoss: 0.6464554071426392 - trainLoss: 0.6478676795959473\n",
      "cnt: 0 - valLoss: 0.646452784538269 - trainLoss: 0.6478656530380249\n",
      "cnt: 0 - valLoss: 0.6464502215385437 - trainLoss: 0.647863507270813\n",
      "cnt: 0 - valLoss: 0.6464477181434631 - trainLoss: 0.6478613615036011\n",
      "cnt: 0 - valLoss: 0.6464451551437378 - trainLoss: 0.6478591561317444\n",
      "cnt: 0 - valLoss: 0.6464428901672363 - trainLoss: 0.6478570103645325\n",
      "cnt: 0 - valLoss: 0.646440327167511 - trainLoss: 0.6478550434112549\n",
      "cnt: 0 - valLoss: 0.6464377641677856 - trainLoss: 0.647852897644043\n",
      "cnt: 0 - valLoss: 0.6464352011680603 - trainLoss: 0.647850751876831\n",
      "cnt: 0 - valLoss: 0.6464326977729797 - trainLoss: 0.6478486061096191\n",
      "cnt: 0 - valLoss: 0.6464301347732544 - trainLoss: 0.6478464007377625\n",
      "cnt: 0 - valLoss: 0.6464278697967529 - trainLoss: 0.6478443741798401\n",
      "cnt: 0 - valLoss: 0.6464253067970276 - trainLoss: 0.647842288017273\n",
      "cnt: 0 - valLoss: 0.6464226841926575 - trainLoss: 0.647840142250061\n",
      "cnt: 0 - valLoss: 0.6464200615882874 - trainLoss: 0.6478379964828491\n",
      "cnt: 0 - valLoss: 0.6464176177978516 - trainLoss: 0.6478358507156372\n",
      "cnt: 0 - valLoss: 0.6464152932167053 - trainLoss: 0.6478337049484253\n",
      "cnt: 0 - valLoss: 0.6464127898216248 - trainLoss: 0.6478316783905029\n",
      "cnt: 0 - valLoss: 0.6464102864265442 - trainLoss: 0.6478295922279358\n",
      "cnt: 0 - valLoss: 0.6464076638221741 - trainLoss: 0.6478273868560791\n",
      "cnt: 0 - valLoss: 0.6464051008224487 - trainLoss: 0.6478252410888672\n",
      "cnt: 0 - valLoss: 0.6464025378227234 - trainLoss: 0.6478230953216553\n",
      "cnt: 0 - valLoss: 0.6464003324508667 - trainLoss: 0.6478210091590881\n",
      "cnt: 0 - valLoss: 0.6463977098464966 - trainLoss: 0.6478190422058105\n",
      "cnt: 0 - valLoss: 0.6463951468467712 - trainLoss: 0.6478167772293091\n",
      "cnt: 0 - valLoss: 0.6463925838470459 - trainLoss: 0.6478146910667419\n",
      "cnt: 0 - valLoss: 0.6463900208473206 - trainLoss: 0.6478124856948853\n",
      "cnt: 0 - valLoss: 0.6463875770568848 - trainLoss: 0.6478103399276733\n",
      "cnt: 0 - valLoss: 0.6463851928710938 - trainLoss: 0.6478082537651062\n",
      "cnt: 0 - valLoss: 0.6463826298713684 - trainLoss: 0.6478061676025391\n",
      "cnt: 0 - valLoss: 0.6463800668716431 - trainLoss: 0.6478040814399719\n",
      "cnt: 0 - valLoss: 0.6463775634765625 - trainLoss: 0.6478018760681152\n",
      "cnt: 0 - valLoss: 0.6463748812675476 - trainLoss: 0.6477997303009033\n",
      "cnt: 0 - valLoss: 0.6463726758956909 - trainLoss: 0.6477975249290466\n",
      "cnt: 0 - valLoss: 0.6463701725006104 - trainLoss: 0.647795557975769\n",
      "cnt: 0 - valLoss: 0.6463675498962402 - trainLoss: 0.6477934122085571\n",
      "cnt: 0 - valLoss: 0.6463649868965149 - trainLoss: 0.6477912664413452\n",
      "cnt: 0 - valLoss: 0.6463623642921448 - trainLoss: 0.6477890610694885\n",
      "cnt: 0 - valLoss: 0.646359920501709 - trainLoss: 0.6477869749069214\n",
      "cnt: 0 - valLoss: 0.6463575959205627 - trainLoss: 0.6477848291397095\n",
      "cnt: 0 - valLoss: 0.6463550329208374 - trainLoss: 0.6477828025817871\n",
      "cnt: 0 - valLoss: 0.6463524699211121 - trainLoss: 0.6477806568145752\n",
      "cnt: 0 - valLoss: 0.6463498473167419 - trainLoss: 0.6477785110473633\n",
      "cnt: 0 - valLoss: 0.6463472843170166 - trainLoss: 0.6477763056755066\n",
      "cnt: 0 - valLoss: 0.6463449597358704 - trainLoss: 0.6477741599082947\n",
      "cnt: 0 - valLoss: 0.6463424563407898 - trainLoss: 0.6477721333503723\n",
      "cnt: 0 - valLoss: 0.6463399529457092 - trainLoss: 0.6477700471878052\n",
      "cnt: 0 - valLoss: 0.6463373303413391 - trainLoss: 0.6477678418159485\n",
      "cnt: 0 - valLoss: 0.6463347673416138 - trainLoss: 0.6477656960487366\n",
      "cnt: 0 - valLoss: 0.6463322043418884 - trainLoss: 0.6477634310722351\n",
      "cnt: 0 - valLoss: 0.6463299989700317 - trainLoss: 0.647761344909668\n",
      "cnt: 0 - valLoss: 0.6463273763656616 - trainLoss: 0.6477594375610352\n",
      "cnt: 0 - valLoss: 0.646324872970581 - trainLoss: 0.6477572321891785\n",
      "cnt: 0 - valLoss: 0.6463221907615662 - trainLoss: 0.6477550864219666\n",
      "cnt: 0 - valLoss: 0.6463196277618408 - trainLoss: 0.6477528214454651\n",
      "cnt: 0 - valLoss: 0.6463172435760498 - trainLoss: 0.6477506756782532\n",
      "cnt: 0 - valLoss: 0.646314799785614 - trainLoss: 0.6477486491203308\n",
      "cnt: 0 - valLoss: 0.6463122367858887 - trainLoss: 0.6477466225624084\n",
      "cnt: 0 - valLoss: 0.6463096737861633 - trainLoss: 0.647744357585907\n",
      "cnt: 0 - valLoss: 0.646307110786438 - trainLoss: 0.6477422118186951\n",
      "cnt: 0 - valLoss: 0.6463045477867126 - trainLoss: 0.6477400660514832\n",
      "cnt: 0 - valLoss: 0.6463022232055664 - trainLoss: 0.6477378606796265\n",
      "cnt: 0 - valLoss: 0.6462996602058411 - trainLoss: 0.6477358937263489\n",
      "cnt: 0 - valLoss: 0.6462970972061157 - trainLoss: 0.6477338075637817\n",
      "cnt: 0 - valLoss: 0.6462944746017456 - trainLoss: 0.647731602191925\n",
      "cnt: 0 - valLoss: 0.646291971206665 - trainLoss: 0.6477293968200684\n",
      "cnt: 0 - valLoss: 0.6462895274162292 - trainLoss: 0.6477272510528564\n",
      "cnt: 0 - valLoss: 0.6462871432304382 - trainLoss: 0.6477252244949341\n",
      "cnt: 0 - valLoss: 0.6462845206260681 - trainLoss: 0.6477231383323669\n",
      "cnt: 0 - valLoss: 0.6462819576263428 - trainLoss: 0.6477209329605103\n",
      "cnt: 0 - valLoss: 0.6462793946266174 - trainLoss: 0.6477187871932983\n",
      "cnt: 0 - valLoss: 0.6462767720222473 - trainLoss: 0.6477165818214417\n",
      "cnt: 0 - valLoss: 0.6462744474411011 - trainLoss: 0.6477144360542297\n",
      "cnt: 0 - valLoss: 0.6462719440460205 - trainLoss: 0.6477124691009521\n",
      "cnt: 0 - valLoss: 0.6462693214416504 - trainLoss: 0.6477102637290955\n",
      "cnt: 0 - valLoss: 0.6462668180465698 - trainLoss: 0.6477081179618835\n",
      "cnt: 0 - valLoss: 0.6462641954421997 - trainLoss: 0.6477059125900269\n",
      "cnt: 0 - valLoss: 0.6462616920471191 - trainLoss: 0.6477037668228149\n",
      "cnt: 0 - valLoss: 0.6462593674659729 - trainLoss: 0.6477016806602478\n",
      "cnt: 0 - valLoss: 0.6462568044662476 - trainLoss: 0.6476995944976807\n",
      "cnt: 0 - valLoss: 0.6462541818618774 - trainLoss: 0.6476974487304688\n",
      "cnt: 0 - valLoss: 0.6462515592575073 - trainLoss: 0.6476952433586121\n",
      "cnt: 0 - valLoss: 0.646248996257782 - trainLoss: 0.6476930975914001\n",
      "cnt: 0 - valLoss: 0.6462466716766357 - trainLoss: 0.6476908922195435\n",
      "cnt: 0 - valLoss: 0.6462441682815552 - trainLoss: 0.6476889252662659\n",
      "cnt: 0 - valLoss: 0.6462415456771851 - trainLoss: 0.647686779499054\n",
      "cnt: 0 - valLoss: 0.6462389230728149 - trainLoss: 0.6476845741271973\n",
      "cnt: 0 - valLoss: 0.6462364196777344 - trainLoss: 0.6476824283599854\n",
      "cnt: 0 - valLoss: 0.6462339162826538 - trainLoss: 0.6476802825927734\n",
      "cnt: 0 - valLoss: 0.6462315917015076 - trainLoss: 0.6476780772209167\n",
      "cnt: 0 - valLoss: 0.6462290287017822 - trainLoss: 0.6476761102676392\n",
      "cnt: 0 - valLoss: 0.6462264060974121 - trainLoss: 0.6476739048957825\n",
      "cnt: 0 - valLoss: 0.6462238430976868 - trainLoss: 0.6476716995239258\n",
      "cnt: 0 - valLoss: 0.6462212204933167 - trainLoss: 0.6476695537567139\n",
      "cnt: 0 - valLoss: 0.6462188959121704 - trainLoss: 0.647667407989502\n",
      "cnt: 0 - valLoss: 0.6462163925170898 - trainLoss: 0.6476653814315796\n",
      "cnt: 0 - valLoss: 0.6462138295173645 - trainLoss: 0.6476632356643677\n",
      "cnt: 0 - valLoss: 0.6462112069129944 - trainLoss: 0.6476610898971558\n",
      "cnt: 0 - valLoss: 0.6462085843086243 - trainLoss: 0.6476588249206543\n",
      "cnt: 0 - valLoss: 0.6462060809135437 - trainLoss: 0.6476566791534424\n",
      "cnt: 0 - valLoss: 0.6462037563323975 - trainLoss: 0.6476544737815857\n",
      "cnt: 0 - valLoss: 0.6462011337280273 - trainLoss: 0.6476525068283081\n",
      "cnt: 0 - valLoss: 0.6461986303329468 - trainLoss: 0.6476503610610962\n",
      "cnt: 0 - valLoss: 0.6461959481239319 - trainLoss: 0.6476481556892395\n",
      "cnt: 0 - valLoss: 0.6461933851242065 - trainLoss: 0.6476459503173828\n",
      "cnt: 0 - valLoss: 0.6461910009384155 - trainLoss: 0.6476438045501709\n",
      "cnt: 0 - valLoss: 0.6461885571479797 - trainLoss: 0.6476417183876038\n",
      "cnt: 0 - valLoss: 0.6461859941482544 - trainLoss: 0.6476396918296814\n",
      "cnt: 0 - valLoss: 0.6461833119392395 - trainLoss: 0.6476374268531799\n",
      "cnt: 0 - valLoss: 0.6461807489395142 - trainLoss: 0.6476353406906128\n",
      "cnt: 0 - valLoss: 0.6461781859397888 - trainLoss: 0.6476330161094666\n",
      "cnt: 0 - valLoss: 0.6461759209632874 - trainLoss: 0.6476309299468994\n",
      "cnt: 0 - valLoss: 0.6461732983589172 - trainLoss: 0.6476289629936218\n",
      "cnt: 0 - valLoss: 0.6461707353591919 - trainLoss: 0.6476267576217651\n",
      "cnt: 0 - valLoss: 0.6461681723594666 - trainLoss: 0.6476245522499084\n",
      "cnt: 0 - valLoss: 0.6461655497550964 - trainLoss: 0.6476223468780518\n",
      "cnt: 0 - valLoss: 0.6461631655693054 - trainLoss: 0.6476201415061951\n",
      "cnt: 0 - valLoss: 0.6461607217788696 - trainLoss: 0.6476181745529175\n",
      "cnt: 0 - valLoss: 0.6461580991744995 - trainLoss: 0.6476160287857056\n",
      "cnt: 0 - valLoss: 0.6461554765701294 - trainLoss: 0.6476137638092041\n",
      "cnt: 0 - valLoss: 0.646152913570404 - trainLoss: 0.6476116180419922\n",
      "cnt: 0 - valLoss: 0.6461502909660339 - trainLoss: 0.6476094722747803\n",
      "cnt: 0 - valLoss: 0.6461479663848877 - trainLoss: 0.6476072669029236\n",
      "cnt: 0 - valLoss: 0.6461454629898071 - trainLoss: 0.647605299949646\n",
      "cnt: 0 - valLoss: 0.646142840385437 - trainLoss: 0.6476031541824341\n",
      "cnt: 0 - valLoss: 0.6461402773857117 - trainLoss: 0.6476009488105774\n",
      "cnt: 0 - valLoss: 0.6461375951766968 - trainLoss: 0.6475986838340759\n",
      "cnt: 0 - valLoss: 0.6461353302001953 - trainLoss: 0.647596538066864\n",
      "cnt: 0 - valLoss: 0.6461328268051147 - trainLoss: 0.6475944519042969\n",
      "cnt: 0 - valLoss: 0.6461302042007446 - trainLoss: 0.6475923657417297\n",
      "cnt: 0 - valLoss: 0.6461275815963745 - trainLoss: 0.647590160369873\n",
      "cnt: 0 - valLoss: 0.6461249589920044 - trainLoss: 0.6475879549980164\n",
      "cnt: 0 - valLoss: 0.6461224555969238 - trainLoss: 0.6475857496261597\n",
      "cnt: 0 - valLoss: 0.6461201310157776 - trainLoss: 0.6475836038589478\n",
      "cnt: 0 - valLoss: 0.6461175084114075 - trainLoss: 0.6475816369056702\n",
      "cnt: 0 - valLoss: 0.6461148858070374 - trainLoss: 0.6475794315338135\n",
      "cnt: 0 - valLoss: 0.646112322807312 - trainLoss: 0.6475772261619568\n",
      "cnt: 0 - valLoss: 0.6461097002029419 - trainLoss: 0.6475750803947449\n",
      "cnt: 0 - valLoss: 0.6461073756217957 - trainLoss: 0.6475728154182434\n",
      "cnt: 0 - valLoss: 0.6461048126220703 - trainLoss: 0.647570788860321\n",
      "cnt: 0 - valLoss: 0.646102249622345 - trainLoss: 0.6475687026977539\n",
      "cnt: 0 - valLoss: 0.6460996270179749 - trainLoss: 0.6475664973258972\n",
      "cnt: 0 - valLoss: 0.6460970044136047 - trainLoss: 0.6475642323493958\n",
      "cnt: 0 - valLoss: 0.6460945010185242 - trainLoss: 0.6475621461868286\n",
      "cnt: 0 - valLoss: 0.6460921764373779 - trainLoss: 0.6475599408149719\n",
      "cnt: 0 - valLoss: 0.6460895538330078 - trainLoss: 0.6475579142570496\n",
      "cnt: 0 - valLoss: 0.6460869908332825 - trainLoss: 0.6475557088851929\n",
      "cnt: 0 - valLoss: 0.6460843682289124 - trainLoss: 0.647553563117981\n",
      "cnt: 0 - valLoss: 0.6460817456245422 - trainLoss: 0.6475513577461243\n",
      "cnt: 0 - valLoss: 0.646079421043396 - trainLoss: 0.6475490927696228\n",
      "cnt: 0 - valLoss: 0.6460769176483154 - trainLoss: 0.6475470662117004\n",
      "cnt: 0 - valLoss: 0.6460742950439453 - trainLoss: 0.6475449800491333\n",
      "cnt: 0 - valLoss: 0.6460716724395752 - trainLoss: 0.6475427746772766\n",
      "cnt: 0 - valLoss: 0.6460690498352051 - trainLoss: 0.6475405693054199\n",
      "cnt: 0 - valLoss: 0.6460665464401245 - trainLoss: 0.6475383043289185\n",
      "cnt: 0 - valLoss: 0.6460642218589783 - trainLoss: 0.6475362181663513\n",
      "cnt: 0 - valLoss: 0.6460616588592529 - trainLoss: 0.647534191608429\n",
      "cnt: 0 - valLoss: 0.6460590362548828 - trainLoss: 0.6475319862365723\n",
      "cnt: 0 - valLoss: 0.6460564136505127 - trainLoss: 0.6475297808647156\n",
      "cnt: 0 - valLoss: 0.6460537910461426 - trainLoss: 0.6475275754928589\n",
      "cnt: 0 - valLoss: 0.6460514664649963 - trainLoss: 0.6475253701210022\n",
      "cnt: 0 - valLoss: 0.646048903465271 - trainLoss: 0.6475234031677246\n",
      "cnt: 0 - valLoss: 0.6460462808609009 - trainLoss: 0.6475211977958679\n",
      "cnt: 0 - valLoss: 0.6460436582565308 - trainLoss: 0.6475189924240112\n",
      "cnt: 0 - valLoss: 0.6460410356521606 - trainLoss: 0.6475168466567993\n",
      "cnt: 0 - valLoss: 0.6460385918617249 - trainLoss: 0.6475145816802979\n",
      "cnt: 0 - valLoss: 0.6460362076759338 - trainLoss: 0.6475124955177307\n",
      "cnt: 0 - valLoss: 0.646033525466919 - trainLoss: 0.6475103497505188\n",
      "cnt: 0 - valLoss: 0.6460310220718384 - trainLoss: 0.6475082039833069\n",
      "cnt: 0 - valLoss: 0.6460283398628235 - trainLoss: 0.647506058216095\n",
      "cnt: 0 - valLoss: 0.6460257768630981 - trainLoss: 0.6475037932395935\n",
      "cnt: 0 - valLoss: 0.6460235118865967 - trainLoss: 0.6475015878677368\n",
      "cnt: 0 - valLoss: 0.6460208892822266 - trainLoss: 0.6474995613098145\n",
      "cnt: 0 - valLoss: 0.6460182666778564 - trainLoss: 0.6474974155426025\n",
      "cnt: 0 - valLoss: 0.6460156440734863 - trainLoss: 0.6474951505661011\n",
      "cnt: 0 - valLoss: 0.646013081073761 - trainLoss: 0.6474929451942444\n",
      "cnt: 0 - valLoss: 0.6460105776786804 - trainLoss: 0.6474907994270325\n",
      "cnt: 0 - valLoss: 0.6460081934928894 - trainLoss: 0.6474886536598206\n",
      "cnt: 0 - valLoss: 0.6460055708885193 - trainLoss: 0.6474865674972534\n",
      "cnt: 0 - valLoss: 0.6460029482841492 - trainLoss: 0.6474844217300415\n",
      "cnt: 0 - valLoss: 0.6460003852844238 - trainLoss: 0.64748215675354\n",
      "cnt: 0 - valLoss: 0.6459977626800537 - trainLoss: 0.6474799513816833\n",
      "cnt: 0 - valLoss: 0.6459954977035522 - trainLoss: 0.6474778056144714\n",
      "cnt: 0 - valLoss: 0.6459928154945374 - trainLoss: 0.6474757790565491\n",
      "cnt: 0 - valLoss: 0.645990252494812 - trainLoss: 0.6474735736846924\n",
      "cnt: 0 - valLoss: 0.6459876298904419 - trainLoss: 0.6474713087081909\n",
      "cnt: 0 - valLoss: 0.6459850072860718 - trainLoss: 0.647469162940979\n",
      "cnt: 0 - valLoss: 0.6459826231002808 - trainLoss: 0.6474668979644775\n",
      "cnt: 0 - valLoss: 0.6459801197052002 - trainLoss: 0.6474648714065552\n",
      "cnt: 0 - valLoss: 0.6459774971008301 - trainLoss: 0.6474627256393433\n",
      "cnt: 0 - valLoss: 0.6459749341011047 - trainLoss: 0.6474605202674866\n",
      "cnt: 0 - valLoss: 0.6459722518920898 - trainLoss: 0.6474583148956299\n",
      "cnt: 0 - valLoss: 0.6459697484970093 - trainLoss: 0.6474561095237732\n",
      "cnt: 0 - valLoss: 0.645967423915863 - trainLoss: 0.6474539041519165\n",
      "cnt: 0 - valLoss: 0.6459648013114929 - trainLoss: 0.6474518775939941\n",
      "cnt: 0 - valLoss: 0.645962119102478 - trainLoss: 0.6474497318267822\n",
      "cnt: 0 - valLoss: 0.6459595561027527 - trainLoss: 0.6474475264549255\n",
      "cnt: 0 - valLoss: 0.6459569334983826 - trainLoss: 0.6474452614784241\n",
      "cnt: 0 - valLoss: 0.6459546685218811 - trainLoss: 0.6474430561065674\n",
      "cnt: 0 - valLoss: 0.645952045917511 - trainLoss: 0.647441029548645\n",
      "cnt: 0 - valLoss: 0.6459494829177856 - trainLoss: 0.6474388241767883\n",
      "cnt: 0 - valLoss: 0.6459468007087708 - trainLoss: 0.6474366784095764\n",
      "cnt: 0 - valLoss: 0.6459442377090454 - trainLoss: 0.6474344730377197\n",
      "cnt: 0 - valLoss: 0.6459417343139648 - trainLoss: 0.6474322080612183\n",
      "cnt: 0 - valLoss: 0.645939290523529 - trainLoss: 0.6474301218986511\n",
      "cnt: 0 - valLoss: 0.6459366679191589 - trainLoss: 0.647428035736084\n",
      "cnt: 0 - valLoss: 0.6459340453147888 - trainLoss: 0.6474257707595825\n",
      "cnt: 0 - valLoss: 0.6459314823150635 - trainLoss: 0.647423505783081\n",
      "cnt: 0 - valLoss: 0.6459288001060486 - trainLoss: 0.6474213004112244\n",
      "cnt: 0 - valLoss: 0.6459265947341919 - trainLoss: 0.6474190950393677\n",
      "cnt: 0 - valLoss: 0.645923912525177 - trainLoss: 0.6474171280860901\n",
      "cnt: 0 - valLoss: 0.6459213495254517 - trainLoss: 0.6474149823188782\n",
      "cnt: 0 - valLoss: 0.6459186673164368 - trainLoss: 0.6474127173423767\n",
      "cnt: 0 - valLoss: 0.6459160447120667 - trainLoss: 0.6474104523658752\n",
      "cnt: 0 - valLoss: 0.6459137201309204 - trainLoss: 0.6474082469940186\n",
      "cnt: 0 - valLoss: 0.6459111571311951 - trainLoss: 0.6474062204360962\n",
      "cnt: 0 - valLoss: 0.645908534526825 - trainLoss: 0.6474040746688843\n",
      "cnt: 0 - valLoss: 0.6459059119224548 - trainLoss: 0.6474018692970276\n",
      "cnt: 0 - valLoss: 0.6459033489227295 - trainLoss: 0.6473996043205261\n",
      "cnt: 0 - valLoss: 0.6459007263183594 - trainLoss: 0.6473973989486694\n",
      "cnt: 0 - valLoss: 0.6458984017372131 - trainLoss: 0.6473951935768127\n",
      "cnt: 0 - valLoss: 0.645895779132843 - trainLoss: 0.6473932266235352\n",
      "cnt: 0 - valLoss: 0.6458931565284729 - trainLoss: 0.6473909616470337\n",
      "cnt: 0 - valLoss: 0.6458905935287476 - trainLoss: 0.647388756275177\n",
      "cnt: 0 - valLoss: 0.6458878517150879 - trainLoss: 0.6473865509033203\n",
      "cnt: 0 - valLoss: 0.6458855867385864 - trainLoss: 0.6473842859268188\n",
      "cnt: 0 - valLoss: 0.6458829045295715 - trainLoss: 0.6473823189735413\n",
      "cnt: 0 - valLoss: 0.6458802819252014 - trainLoss: 0.6473801136016846\n",
      "cnt: 0 - valLoss: 0.6458775401115417 - trainLoss: 0.6473779082298279\n",
      "cnt: 0 - valLoss: 0.6458747982978821 - trainLoss: 0.6473757028579712\n",
      "cnt: 0 - valLoss: 0.6458721160888672 - trainLoss: 0.6473734378814697\n",
      "cnt: 0 - valLoss: 0.645869791507721 - trainLoss: 0.6473712921142578\n",
      "cnt: 0 - valLoss: 0.6458670496940613 - trainLoss: 0.6473692059516907\n",
      "cnt: 0 - valLoss: 0.6458643078804016 - trainLoss: 0.6473670601844788\n",
      "cnt: 0 - valLoss: 0.6458615660667419 - trainLoss: 0.6473648548126221\n",
      "cnt: 0 - valLoss: 0.6458589434623718 - trainLoss: 0.6473626494407654\n",
      "cnt: 0 - valLoss: 0.645856499671936 - trainLoss: 0.6473604440689087\n",
      "cnt: 0 - valLoss: 0.6458538174629211 - trainLoss: 0.6473584175109863\n",
      "cnt: 0 - valLoss: 0.6458511352539062 - trainLoss: 0.6473562121391296\n",
      "cnt: 0 - valLoss: 0.6458484530448914 - trainLoss: 0.6473539471626282\n",
      "cnt: 0 - valLoss: 0.6458457112312317 - trainLoss: 0.6473517417907715\n",
      "cnt: 0 - valLoss: 0.6458430290222168 - trainLoss: 0.6473495364189148\n",
      "cnt: 0 - valLoss: 0.645840585231781 - trainLoss: 0.6473473310470581\n",
      "cnt: 0 - valLoss: 0.6458379626274109 - trainLoss: 0.6473453640937805\n",
      "cnt: 0 - valLoss: 0.6458352208137512 - trainLoss: 0.6473431587219238\n",
      "cnt: 0 - valLoss: 0.6458324790000916 - trainLoss: 0.6473408937454224\n",
      "cnt: 0 - valLoss: 0.6458297967910767 - trainLoss: 0.6473386883735657\n",
      "cnt: 0 - valLoss: 0.6458272337913513 - trainLoss: 0.647336483001709\n",
      "cnt: 0 - valLoss: 0.6458246111869812 - trainLoss: 0.6473344564437866\n",
      "cnt: 0 - valLoss: 0.6458219885826111 - trainLoss: 0.6473322510719299\n",
      "cnt: 0 - valLoss: 0.6458192467689514 - trainLoss: 0.6473300457000732\n",
      "cnt: 0 - valLoss: 0.6458165049552917 - trainLoss: 0.6473278403282166\n",
      "cnt: 0 - valLoss: 0.6458139419555664 - trainLoss: 0.6473255753517151\n",
      "cnt: 0 - valLoss: 0.6458114981651306 - trainLoss: 0.6473234295845032\n",
      "cnt: 0 - valLoss: 0.645808756351471 - trainLoss: 0.6473214030265808\n",
      "cnt: 0 - valLoss: 0.6458059549331665 - trainLoss: 0.6473191976547241\n",
      "cnt: 0 - valLoss: 0.6458031535148621 - trainLoss: 0.6473169326782227\n",
      "cnt: 0 - valLoss: 0.6458005905151367 - trainLoss: 0.647314727306366\n",
      "cnt: 0 - valLoss: 0.6457980871200562 - trainLoss: 0.6473125219345093\n",
      "cnt: 0 - valLoss: 0.6457955241203308 - trainLoss: 0.6473104357719421\n",
      "cnt: 0 - valLoss: 0.6457927823066711 - trainLoss: 0.6473082900047302\n",
      "cnt: 0 - valLoss: 0.6457901000976562 - trainLoss: 0.6473060846328735\n",
      "cnt: 0 - valLoss: 0.6457873582839966 - trainLoss: 0.6473038792610168\n",
      "cnt: 0 - valLoss: 0.6457846760749817 - trainLoss: 0.6473017334938049\n",
      "cnt: 0 - valLoss: 0.6457822322845459 - trainLoss: 0.6472994685173035\n",
      "cnt: 0 - valLoss: 0.6457794904708862 - trainLoss: 0.6472974419593811\n",
      "cnt: 0 - valLoss: 0.6457768082618713 - trainLoss: 0.6472952365875244\n",
      "cnt: 0 - valLoss: 0.6457740664482117 - trainLoss: 0.6472929120063782\n",
      "cnt: 0 - valLoss: 0.645771324634552 - trainLoss: 0.6472907662391663\n",
      "cnt: 0 - valLoss: 0.6457688212394714 - trainLoss: 0.6472885012626648\n",
      "cnt: 0 - valLoss: 0.6457662582397461 - trainLoss: 0.6472864151000977\n",
      "cnt: 0 - valLoss: 0.6457635760307312 - trainLoss: 0.6472843289375305\n",
      "cnt: 0 - valLoss: 0.6457607746124268 - trainLoss: 0.6472821235656738\n",
      "cnt: 0 - valLoss: 0.6457581520080566 - trainLoss: 0.6472798585891724\n",
      "cnt: 0 - valLoss: 0.6457553505897522 - trainLoss: 0.6472776532173157\n",
      "cnt: 0 - valLoss: 0.6457529067993164 - trainLoss: 0.647275447845459\n",
      "cnt: 0 - valLoss: 0.6457502841949463 - trainLoss: 0.6472734212875366\n",
      "cnt: 0 - valLoss: 0.6457475423812866 - trainLoss: 0.6472711563110352\n",
      "cnt: 0 - valLoss: 0.6457447409629822 - trainLoss: 0.6472689509391785\n",
      "cnt: 0 - valLoss: 0.6457419991493225 - trainLoss: 0.647266685962677\n",
      "cnt: 0 - valLoss: 0.6457396149635315 - trainLoss: 0.6472645401954651\n",
      "cnt: 0 - valLoss: 0.6457369327545166 - trainLoss: 0.6472623944282532\n",
      "cnt: 0 - valLoss: 0.6457341909408569 - trainLoss: 0.647260308265686\n",
      "cnt: 0 - valLoss: 0.6457314491271973 - trainLoss: 0.6472580432891846\n",
      "cnt: 0 - valLoss: 0.6457287073135376 - trainLoss: 0.6472558379173279\n",
      "cnt: 0 - valLoss: 0.6457261443138123 - trainLoss: 0.6472536325454712\n",
      "cnt: 0 - valLoss: 0.6457237005233765 - trainLoss: 0.6472513675689697\n",
      "cnt: 0 - valLoss: 0.6457209587097168 - trainLoss: 0.6472493410110474\n",
      "cnt: 0 - valLoss: 0.6457181572914124 - trainLoss: 0.6472470760345459\n",
      "cnt: 0 - valLoss: 0.6457154154777527 - trainLoss: 0.647244930267334\n",
      "cnt: 0 - valLoss: 0.6457127928733826 - trainLoss: 0.6472426652908325\n",
      "cnt: 0 - valLoss: 0.6457102298736572 - trainLoss: 0.6472404599189758\n",
      "cnt: 0 - valLoss: 0.6457076072692871 - trainLoss: 0.6472383141517639\n",
      "cnt: 0 - valLoss: 0.6457048654556274 - trainLoss: 0.6472362279891968\n",
      "cnt: 0 - valLoss: 0.6457021236419678 - trainLoss: 0.6472340226173401\n",
      "cnt: 0 - valLoss: 0.6456995010375977 - trainLoss: 0.6472317576408386\n",
      "cnt: 0 - valLoss: 0.6456968188285828 - trainLoss: 0.6472294926643372\n",
      "cnt: 0 - valLoss: 0.645694375038147 - trainLoss: 0.6472272872924805\n",
      "cnt: 0 - valLoss: 0.6456915736198425 - trainLoss: 0.6472252607345581\n",
      "cnt: 0 - valLoss: 0.6456888318061829 - trainLoss: 0.6472230553627014\n",
      "cnt: 0 - valLoss: 0.645686149597168 - trainLoss: 0.6472208499908447\n",
      "cnt: 0 - valLoss: 0.6456834673881531 - trainLoss: 0.6472185850143433\n",
      "cnt: 0 - valLoss: 0.6456809043884277 - trainLoss: 0.647216260433197\n",
      "cnt: 0 - valLoss: 0.6456783413887024 - trainLoss: 0.6472142338752747\n",
      "cnt: 0 - valLoss: 0.6456754803657532 - trainLoss: 0.6472121477127075\n",
      "cnt: 0 - valLoss: 0.6456728577613831 - trainLoss: 0.647209882736206\n",
      "cnt: 0 - valLoss: 0.6456701159477234 - trainLoss: 0.6472076177597046\n",
      "cnt: 0 - valLoss: 0.6456673741340637 - trainLoss: 0.6472054123878479\n",
      "cnt: 0 - valLoss: 0.6456649303436279 - trainLoss: 0.6472032070159912\n",
      "cnt: 0 - valLoss: 0.6456621885299683 - trainLoss: 0.6472011804580688\n",
      "cnt: 0 - valLoss: 0.6456595659255981 - trainLoss: 0.6471989750862122\n",
      "cnt: 0 - valLoss: 0.6456568241119385 - trainLoss: 0.6471966505050659\n",
      "cnt: 0 - valLoss: 0.6456540822982788 - trainLoss: 0.6471944451332092\n",
      "cnt: 0 - valLoss: 0.6456514596939087 - trainLoss: 0.6471922397613525\n",
      "cnt: 0 - valLoss: 0.6456488966941833 - trainLoss: 0.6471901535987854\n",
      "cnt: 0 - valLoss: 0.6456460952758789 - trainLoss: 0.6471880078315735\n",
      "cnt: 0 - valLoss: 0.645643413066864 - trainLoss: 0.647185742855072\n",
      "cnt: 0 - valLoss: 0.6456406712532043 - trainLoss: 0.6471835374832153\n",
      "cnt: 0 - valLoss: 0.6456379890441895 - trainLoss: 0.6471812725067139\n",
      "cnt: 0 - valLoss: 0.6456354856491089 - trainLoss: 0.6471790671348572\n",
      "cnt: 0 - valLoss: 0.6456327438354492 - trainLoss: 0.6471770405769348\n",
      "cnt: 0 - valLoss: 0.6456301212310791 - trainLoss: 0.6471747756004333\n",
      "cnt: 0 - valLoss: 0.6456273198127747 - trainLoss: 0.6471725106239319\n",
      "cnt: 0 - valLoss: 0.6456246376037598 - trainLoss: 0.6471703052520752\n",
      "cnt: 0 - valLoss: 0.6456221342086792 - trainLoss: 0.6471680998802185\n",
      "cnt: 0 - valLoss: 0.6456194519996643 - trainLoss: 0.6471660137176514\n",
      "cnt: 0 - valLoss: 0.6456166505813599 - trainLoss: 0.6471638679504395\n",
      "cnt: 0 - valLoss: 0.645613968372345 - trainLoss: 0.647161602973938\n",
      "cnt: 0 - valLoss: 0.6456112265586853 - trainLoss: 0.6471593379974365\n",
      "cnt: 0 - valLoss: 0.6456086039543152 - trainLoss: 0.6471571326255798\n",
      "cnt: 0 - valLoss: 0.6456061005592346 - trainLoss: 0.6471549272537231\n",
      "cnt: 0 - valLoss: 0.6456032991409302 - trainLoss: 0.647152841091156\n",
      "cnt: 0 - valLoss: 0.6456004977226257 - trainLoss: 0.6471506357192993\n",
      "cnt: 0 - valLoss: 0.6455978751182556 - trainLoss: 0.6471483707427979\n",
      "cnt: 0 - valLoss: 0.645595133304596 - trainLoss: 0.6471461057662964\n",
      "cnt: 0 - valLoss: 0.6455926895141602 - trainLoss: 0.6471438407897949\n",
      "cnt: 0 - valLoss: 0.6455899477005005 - trainLoss: 0.6471418142318726\n",
      "cnt: 0 - valLoss: 0.6455872058868408 - trainLoss: 0.6471396088600159\n",
      "cnt: 0 - valLoss: 0.6455845236778259 - trainLoss: 0.6471374034881592\n",
      "cnt: 0 - valLoss: 0.6455817818641663 - trainLoss: 0.6471351981163025\n",
      "cnt: 0 - valLoss: 0.6455792188644409 - trainLoss: 0.647132933139801\n",
      "cnt: 0 - valLoss: 0.645576536655426 - trainLoss: 0.6471307873725891\n",
      "cnt: 0 - valLoss: 0.6455737948417664 - trainLoss: 0.6471286416053772\n",
      "cnt: 0 - valLoss: 0.6455710530281067 - trainLoss: 0.6471263766288757\n",
      "cnt: 0 - valLoss: 0.6455683708190918 - trainLoss: 0.647124171257019\n",
      "cnt: 0 - valLoss: 0.6455656290054321 - trainLoss: 0.6471219658851624\n",
      "cnt: 0 - valLoss: 0.6455631852149963 - trainLoss: 0.6471197009086609\n",
      "cnt: 0 - valLoss: 0.6455605030059814 - trainLoss: 0.6471176743507385\n",
      "cnt: 0 - valLoss: 0.645557701587677 - trainLoss: 0.6471154093742371\n",
      "cnt: 0 - valLoss: 0.6455549001693726 - trainLoss: 0.6471132040023804\n",
      "cnt: 0 - valLoss: 0.6455522179603577 - trainLoss: 0.6471108794212341\n",
      "cnt: 0 - valLoss: 0.6455497145652771 - trainLoss: 0.6471086740493774\n",
      "cnt: 0 - valLoss: 0.645547091960907 - trainLoss: 0.6471065878868103\n",
      "cnt: 0 - valLoss: 0.6455442309379578 - trainLoss: 0.6471043825149536\n",
      "cnt: 0 - valLoss: 0.6455414891242981 - trainLoss: 0.6471021771430969\n",
      "cnt: 0 - valLoss: 0.6455387473106384 - trainLoss: 0.6470998525619507\n",
      "cnt: 0 - valLoss: 0.6455361843109131 - trainLoss: 0.6470977067947388\n",
      "cnt: 0 - valLoss: 0.6455336809158325 - trainLoss: 0.6470954418182373\n",
      "cnt: 0 - valLoss: 0.6455309391021729 - trainLoss: 0.6470934152603149\n",
      "cnt: 0 - valLoss: 0.6455281376838684 - trainLoss: 0.6470911502838135\n",
      "cnt: 0 - valLoss: 0.645525336265564 - trainLoss: 0.647088885307312\n",
      "cnt: 0 - valLoss: 0.6455225944519043 - trainLoss: 0.6470866203308105\n",
      "cnt: 0 - valLoss: 0.6455200910568237 - trainLoss: 0.6470843553543091\n",
      "cnt: 0 - valLoss: 0.6455173492431641 - trainLoss: 0.6470823287963867\n",
      "cnt: 0 - valLoss: 0.645514726638794 - trainLoss: 0.6470800638198853\n",
      "cnt: 0 - valLoss: 0.6455119252204895 - trainLoss: 0.6470779180526733\n",
      "cnt: 0 - valLoss: 0.6455091834068298 - trainLoss: 0.6470756530761719\n",
      "cnt: 0 - valLoss: 0.6455065608024597 - trainLoss: 0.6470733284950256\n",
      "cnt: 0 - valLoss: 0.6455039978027344 - trainLoss: 0.6470712423324585\n",
      "cnt: 0 - valLoss: 0.6455013155937195 - trainLoss: 0.6470691561698914\n",
      "cnt: 0 - valLoss: 0.645498514175415 - trainLoss: 0.6470668911933899\n",
      "cnt: 0 - valLoss: 0.6454957723617554 - trainLoss: 0.6470645666122437\n",
      "cnt: 0 - valLoss: 0.6454930901527405 - trainLoss: 0.647062361240387\n",
      "cnt: 0 - valLoss: 0.6454905867576599 - trainLoss: 0.6470601558685303\n",
      "cnt: 0 - valLoss: 0.6454878449440002 - trainLoss: 0.6470580697059631\n",
      "cnt: 0 - valLoss: 0.6454851031303406 - trainLoss: 0.6470558643341064\n",
      "cnt: 0 - valLoss: 0.6454823613166809 - trainLoss: 0.6470536589622498\n",
      "cnt: 0 - valLoss: 0.6454795598983765 - trainLoss: 0.6470513343811035\n",
      "cnt: 0 - valLoss: 0.6454770565032959 - trainLoss: 0.647049069404602\n",
      "cnt: 0 - valLoss: 0.645474374294281 - trainLoss: 0.6470469832420349\n",
      "cnt: 0 - valLoss: 0.6454715728759766 - trainLoss: 0.6470447778701782\n",
      "cnt: 0 - valLoss: 0.6454688310623169 - trainLoss: 0.6470425724983215\n",
      "cnt: 0 - valLoss: 0.645466148853302 - trainLoss: 0.6470403075218201\n",
      "cnt: 0 - valLoss: 0.6454634666442871 - trainLoss: 0.6470380425453186\n",
      "cnt: 0 - valLoss: 0.6454609036445618 - trainLoss: 0.6470358967781067\n",
      "cnt: 0 - valLoss: 0.6454581618309021 - trainLoss: 0.6470337510108948\n",
      "cnt: 0 - valLoss: 0.6454554200172424 - trainLoss: 0.6470314860343933\n",
      "cnt: 0 - valLoss: 0.6454525589942932 - trainLoss: 0.6470292210578918\n",
      "cnt: 0 - valLoss: 0.6454498171806335 - trainLoss: 0.6470269560813904\n",
      "cnt: 0 - valLoss: 0.6454475522041321 - trainLoss: 0.6470246911048889\n",
      "cnt: 0 - valLoss: 0.6454446911811829 - trainLoss: 0.6470227241516113\n",
      "cnt: 0 - valLoss: 0.6454419493675232 - trainLoss: 0.6470203995704651\n",
      "cnt: 0 - valLoss: 0.6454391479492188 - trainLoss: 0.6470181941986084\n",
      "cnt: 0 - valLoss: 0.6454364061355591 - trainLoss: 0.6470159888267517\n",
      "cnt: 0 - valLoss: 0.6454339027404785 - trainLoss: 0.6470136642456055\n",
      "cnt: 0 - valLoss: 0.6454311609268188 - trainLoss: 0.6470115780830383\n",
      "cnt: 0 - valLoss: 0.6454285383224487 - trainLoss: 0.6470093727111816\n",
      "cnt: 0 - valLoss: 0.6454257369041443 - trainLoss: 0.6470071077346802\n",
      "cnt: 0 - valLoss: 0.6454229950904846 - trainLoss: 0.6470048427581787\n",
      "cnt: 0 - valLoss: 0.6454203128814697 - trainLoss: 0.647002637386322\n",
      "cnt: 0 - valLoss: 0.6454176902770996 - trainLoss: 0.6470003724098206\n",
      "cnt: 0 - valLoss: 0.6454149484634399 - trainLoss: 0.6469983458518982\n",
      "cnt: 0 - valLoss: 0.6454122066497803 - trainLoss: 0.646996021270752\n",
      "cnt: 0 - valLoss: 0.6454094052314758 - trainLoss: 0.6469938158988953\n",
      "cnt: 0 - valLoss: 0.6454067230224609 - trainLoss: 0.6469915509223938\n",
      "cnt: 0 - valLoss: 0.6454042792320251 - trainLoss: 0.6469892859458923\n",
      "cnt: 0 - valLoss: 0.6454015374183655 - trainLoss: 0.6469871997833252\n",
      "cnt: 0 - valLoss: 0.6453987956047058 - trainLoss: 0.6469849944114685\n",
      "cnt: 0 - valLoss: 0.6453958749771118 - trainLoss: 0.646982729434967\n",
      "cnt: 0 - valLoss: 0.6453931331634521 - trainLoss: 0.6469804048538208\n",
      "cnt: 0 - valLoss: 0.6453906297683716 - trainLoss: 0.6469781994819641\n",
      "cnt: 0 - valLoss: 0.6453879475593567 - trainLoss: 0.6469760537147522\n",
      "cnt: 0 - valLoss: 0.6453853249549866 - trainLoss: 0.6469739675521851\n",
      "cnt: 0 - valLoss: 0.6453824639320374 - trainLoss: 0.6469716429710388\n",
      "cnt: 0 - valLoss: 0.6453797221183777 - trainLoss: 0.6469693779945374\n",
      "cnt: 0 - valLoss: 0.645376980304718 - trainLoss: 0.6469671130180359\n",
      "cnt: 0 - valLoss: 0.6453745365142822 - trainLoss: 0.6469649076461792\n",
      "cnt: 0 - valLoss: 0.645371675491333 - trainLoss: 0.6469628214836121\n",
      "cnt: 0 - valLoss: 0.6453689336776733 - trainLoss: 0.6469605565071106\n",
      "cnt: 0 - valLoss: 0.6453661322593689 - trainLoss: 0.6469582915306091\n",
      "cnt: 0 - valLoss: 0.645363450050354 - trainLoss: 0.6469560265541077\n",
      "cnt: 0 - valLoss: 0.6453610062599182 - trainLoss: 0.6469537615776062\n",
      "cnt: 0 - valLoss: 0.6453582048416138 - trainLoss: 0.6469517946243286\n",
      "cnt: 0 - valLoss: 0.6453554630279541 - trainLoss: 0.6469494700431824\n",
      "cnt: 0 - valLoss: 0.6453526616096497 - trainLoss: 0.6469472050666809\n",
      "cnt: 0 - valLoss: 0.6453498601913452 - trainLoss: 0.6469449400901794\n",
      "cnt: 0 - valLoss: 0.6453473567962646 - trainLoss: 0.646942675113678\n",
      "cnt: 0 - valLoss: 0.6453446745872498 - trainLoss: 0.6469405293464661\n",
      "cnt: 0 - valLoss: 0.6453418731689453 - trainLoss: 0.6469383835792542\n",
      "cnt: 0 - valLoss: 0.6453391313552856 - trainLoss: 0.6469361186027527\n",
      "cnt: 0 - valLoss: 0.645336389541626 - trainLoss: 0.6469337940216064\n",
      "cnt: 0 - valLoss: 0.6453337669372559 - trainLoss: 0.6469315886497498\n",
      "cnt: 0 - valLoss: 0.6453311443328857 - trainLoss: 0.6469293832778931\n",
      "cnt: 0 - valLoss: 0.6453283429145813 - trainLoss: 0.6469272971153259\n",
      "cnt: 0 - valLoss: 0.6453255414962769 - trainLoss: 0.6469249725341797\n",
      "cnt: 0 - valLoss: 0.6453227996826172 - trainLoss: 0.6469227075576782\n",
      "cnt: 0 - valLoss: 0.6453199982643127 - trainLoss: 0.6469204425811768\n",
      "cnt: 0 - valLoss: 0.6453176140785217 - trainLoss: 0.6469181776046753\n",
      "cnt: 0 - valLoss: 0.6453148722648621 - trainLoss: 0.6469160914421082\n",
      "cnt: 0 - valLoss: 0.6453120112419128 - trainLoss: 0.6469138264656067\n",
      "cnt: 0 - valLoss: 0.6453092694282532 - trainLoss: 0.6469115614891052\n",
      "cnt: 0 - valLoss: 0.6453064680099487 - trainLoss: 0.6469092965126038\n",
      "cnt: 0 - valLoss: 0.6453039646148682 - trainLoss: 0.6469070315361023\n",
      "cnt: 0 - valLoss: 0.6453012228012085 - trainLoss: 0.6469050049781799\n",
      "cnt: 0 - valLoss: 0.6452984809875488 - trainLoss: 0.6469027400016785\n",
      "cnt: 0 - valLoss: 0.6452956199645996 - trainLoss: 0.646900475025177\n",
      "cnt: 0 - valLoss: 0.6452928781509399 - trainLoss: 0.6468982100486755\n",
      "cnt: 0 - valLoss: 0.6452904343605042 - trainLoss: 0.6468958854675293\n",
      "cnt: 0 - valLoss: 0.6452876925468445 - trainLoss: 0.6468937993049622\n",
      "cnt: 0 - valLoss: 0.6452849507331848 - trainLoss: 0.6468916535377502\n",
      "cnt: 0 - valLoss: 0.6452821493148804 - trainLoss: 0.6468893885612488\n",
      "cnt: 0 - valLoss: 0.6452792882919312 - trainLoss: 0.6468870639801025\n",
      "cnt: 0 - valLoss: 0.6452767252922058 - trainLoss: 0.6468847393989563\n",
      "cnt: 0 - valLoss: 0.6452740430831909 - trainLoss: 0.6468825936317444\n",
      "cnt: 0 - valLoss: 0.6452713012695312 - trainLoss: 0.6468804478645325\n",
      "cnt: 0 - valLoss: 0.6452684998512268 - trainLoss: 0.646878182888031\n",
      "cnt: 0 - valLoss: 0.6452657580375671 - trainLoss: 0.6468759179115295\n",
      "cnt: 0 - valLoss: 0.6452630758285522 - trainLoss: 0.6468735933303833\n",
      "cnt: 0 - valLoss: 0.6452605724334717 - trainLoss: 0.6468713879585266\n",
      "cnt: 0 - valLoss: 0.6452577710151672 - trainLoss: 0.6468693614006042\n",
      "cnt: 0 - valLoss: 0.645254909992218 - trainLoss: 0.646867036819458\n",
      "cnt: 0 - valLoss: 0.6452521681785583 - trainLoss: 0.6468647718429565\n",
      "cnt: 0 - valLoss: 0.6452493667602539 - trainLoss: 0.6468625068664551\n",
      "cnt: 0 - valLoss: 0.6452468633651733 - trainLoss: 0.6468601822853088\n",
      "cnt: 0 - valLoss: 0.6452441215515137 - trainLoss: 0.6468580961227417\n",
      "cnt: 0 - valLoss: 0.6452413201332092 - trainLoss: 0.646855890750885\n",
      "cnt: 0 - valLoss: 0.6452386379241943 - trainLoss: 0.6468536257743835\n",
      "cnt: 0 - valLoss: 0.6452358365058899 - trainLoss: 0.6468514204025269\n",
      "cnt: 0 - valLoss: 0.6452332735061646 - trainLoss: 0.6468490362167358\n",
      "cnt: 0 - valLoss: 0.6452305912971497 - trainLoss: 0.6468469500541687\n",
      "cnt: 0 - valLoss: 0.6452277898788452 - trainLoss: 0.646844744682312\n",
      "cnt: 0 - valLoss: 0.6452249884605408 - trainLoss: 0.6468424797058105\n",
      "cnt: 0 - valLoss: 0.6452221870422363 - trainLoss: 0.6468402147293091\n",
      "cnt: 0 - valLoss: 0.6452195048332214 - trainLoss: 0.6468378901481628\n",
      "cnt: 0 - valLoss: 0.6452169418334961 - trainLoss: 0.6468356847763062\n",
      "cnt: 0 - valLoss: 0.6452141404151917 - trainLoss: 0.646833598613739\n",
      "cnt: 0 - valLoss: 0.6452113389968872 - trainLoss: 0.6468313336372375\n",
      "cnt: 0 - valLoss: 0.6452085375785828 - trainLoss: 0.6468290686607361\n",
      "cnt: 0 - valLoss: 0.6452058553695679 - trainLoss: 0.6468266844749451\n",
      "cnt: 0 - valLoss: 0.6452034115791321 - trainLoss: 0.6468244791030884\n",
      "cnt: 0 - valLoss: 0.6452005505561829 - trainLoss: 0.6468223929405212\n",
      "cnt: 0 - valLoss: 0.6451976895332336 - trainLoss: 0.646820068359375\n",
      "cnt: 0 - valLoss: 0.6451950073242188 - trainLoss: 0.6468178033828735\n",
      "cnt: 0 - valLoss: 0.6451921463012695 - trainLoss: 0.6468155384063721\n",
      "cnt: 0 - valLoss: 0.6451897025108337 - trainLoss: 0.6468132734298706\n",
      "cnt: 0 - valLoss: 0.6451869010925293 - trainLoss: 0.6468111872673035\n",
      "cnt: 0 - valLoss: 0.6451840996742249 - trainLoss: 0.6468089818954468\n",
      "cnt: 0 - valLoss: 0.6451812386512756 - trainLoss: 0.6468066573143005\n",
      "cnt: 0 - valLoss: 0.6451784372329712 - trainLoss: 0.6468043327331543\n",
      "cnt: 0 - valLoss: 0.6451760530471802 - trainLoss: 0.6468020677566528\n",
      "cnt: 0 - valLoss: 0.6451733112335205 - trainLoss: 0.6467999219894409\n",
      "cnt: 0 - valLoss: 0.6451705098152161 - trainLoss: 0.646797776222229\n",
      "cnt: 0 - valLoss: 0.6451676487922668 - trainLoss: 0.6467954516410828\n",
      "cnt: 0 - valLoss: 0.6451648473739624 - trainLoss: 0.6467931270599365\n",
      "cnt: 0 - valLoss: 0.6451622843742371 - trainLoss: 0.6467909216880798\n",
      "cnt: 0 - valLoss: 0.6451596617698669 - trainLoss: 0.6467887163162231\n",
      "cnt: 0 - valLoss: 0.6451568007469177 - trainLoss: 0.6467865705490112\n",
      "cnt: 0 - valLoss: 0.6451540589332581 - trainLoss: 0.646784245967865\n",
      "cnt: 0 - valLoss: 0.6451511383056641 - trainLoss: 0.6467819809913635\n",
      "cnt: 0 - valLoss: 0.645148515701294 - trainLoss: 0.6467796564102173\n",
      "cnt: 0 - valLoss: 0.6451458930969238 - trainLoss: 0.6467774510383606\n",
      "cnt: 0 - valLoss: 0.6451432108879089 - trainLoss: 0.6467753648757935\n",
      "cnt: 0 - valLoss: 0.6451404094696045 - trainLoss: 0.646773099899292\n",
      "cnt: 0 - valLoss: 0.6451375484466553 - trainLoss: 0.6467707753181458\n",
      "cnt: 0 - valLoss: 0.6451348066329956 - trainLoss: 0.6467684507369995\n",
      "cnt: 0 - valLoss: 0.6451323628425598 - trainLoss: 0.646766185760498\n",
      "cnt: 0 - valLoss: 0.6451294422149658 - trainLoss: 0.6467640995979309\n",
      "cnt: 0 - valLoss: 0.6451267004013062 - trainLoss: 0.6467618346214294\n",
      "cnt: 0 - valLoss: 0.6451238989830017 - trainLoss: 0.646759569644928\n",
      "cnt: 0 - valLoss: 0.6451210975646973 - trainLoss: 0.6467573046684265\n",
      "cnt: 0 - valLoss: 0.6451186537742615 - trainLoss: 0.6467549800872803\n",
      "cnt: 0 - valLoss: 0.6451159119606018 - trainLoss: 0.6467528939247131\n",
      "cnt: 0 - valLoss: 0.6451130509376526 - trainLoss: 0.6467506289482117\n",
      "cnt: 0 - valLoss: 0.6451101899147034 - trainLoss: 0.6467483043670654\n",
      "cnt: 0 - valLoss: 0.6451073884963989 - trainLoss: 0.646746039390564\n",
      "cnt: 0 - valLoss: 0.6451048254966736 - trainLoss: 0.6467437744140625\n",
      "cnt: 0 - valLoss: 0.6451021432876587 - trainLoss: 0.6467416286468506\n",
      "cnt: 0 - valLoss: 0.645099401473999 - trainLoss: 0.6467393636703491\n",
      "cnt: 0 - valLoss: 0.6450966000556946 - trainLoss: 0.6467370986938477\n",
      "cnt: 0 - valLoss: 0.6450937390327454 - trainLoss: 0.6467348337173462\n",
      "cnt: 0 - valLoss: 0.6450911164283752 - trainLoss: 0.6467325687408447\n",
      "cnt: 0 - valLoss: 0.6450884938240051 - trainLoss: 0.646730363368988\n",
      "cnt: 0 - valLoss: 0.6450856924057007 - trainLoss: 0.6467281579971313\n",
      "cnt: 0 - valLoss: 0.6450827717781067 - trainLoss: 0.6467258334159851\n",
      "cnt: 0 - valLoss: 0.6450799703598022 - trainLoss: 0.6467235684394836\n",
      "cnt: 0 - valLoss: 0.6450774073600769 - trainLoss: 0.6467213034629822\n",
      "cnt: 0 - valLoss: 0.6450748443603516 - trainLoss: 0.6467190980911255\n",
      "cnt: 0 - valLoss: 0.6450719237327576 - trainLoss: 0.6467169523239136\n",
      "cnt: 0 - valLoss: 0.6450691819190979 - trainLoss: 0.6467146277427673\n",
      "cnt: 0 - valLoss: 0.6450663208961487 - trainLoss: 0.6467123627662659\n",
      "cnt: 0 - valLoss: 0.645063579082489 - trainLoss: 0.6467100381851196\n",
      "cnt: 0 - valLoss: 0.6450611352920532 - trainLoss: 0.6467077732086182\n",
      "cnt: 0 - valLoss: 0.645058274269104 - trainLoss: 0.646705687046051\n",
      "cnt: 0 - valLoss: 0.6450554728507996 - trainLoss: 0.6467033624649048\n",
      "cnt: 0 - valLoss: 0.6450526118278503 - trainLoss: 0.6467010974884033\n",
      "cnt: 0 - valLoss: 0.6450498104095459 - trainLoss: 0.6466988325119019\n",
      "cnt: 0 - valLoss: 0.6450473666191101 - trainLoss: 0.6466964483261108\n",
      "cnt: 0 - valLoss: 0.6450445652008057 - trainLoss: 0.6466944217681885\n",
      "cnt: 0 - valLoss: 0.6450417041778564 - trainLoss: 0.646692156791687\n",
      "cnt: 0 - valLoss: 0.6450389623641968 - trainLoss: 0.6466898322105408\n",
      "cnt: 0 - valLoss: 0.6450361609458923 - trainLoss: 0.6466876268386841\n",
      "cnt: 0 - valLoss: 0.6450336575508118 - trainLoss: 0.6466852426528931\n",
      "cnt: 0 - valLoss: 0.6450308561325073 - trainLoss: 0.6466831564903259\n",
      "cnt: 0 - valLoss: 0.6450280547142029 - trainLoss: 0.6466808915138245\n",
      "cnt: 0 - valLoss: 0.6450251936912537 - trainLoss: 0.646678626537323\n",
      "cnt: 0 - valLoss: 0.6450223922729492 - trainLoss: 0.6466763019561768\n",
      "cnt: 0 - valLoss: 0.6450198292732239 - trainLoss: 0.6466739773750305\n",
      "cnt: 0 - valLoss: 0.645017147064209 - trainLoss: 0.6466718316078186\n",
      "cnt: 0 - valLoss: 0.6450142860412598 - trainLoss: 0.6466695666313171\n",
      "cnt: 0 - valLoss: 0.6450114846229553 - trainLoss: 0.6466673016548157\n",
      "cnt: 0 - valLoss: 0.6450086832046509 - trainLoss: 0.6466650366783142\n",
      "cnt: 0 - valLoss: 0.6450060606002808 - trainLoss: 0.646662712097168\n",
      "cnt: 0 - valLoss: 0.6450034379959106 - trainLoss: 0.646660566329956\n",
      "cnt: 0 - valLoss: 0.645000696182251 - trainLoss: 0.6466583013534546\n",
      "cnt: 0 - valLoss: 0.644997775554657 - trainLoss: 0.6466560363769531\n",
      "cnt: 0 - valLoss: 0.6449949741363525 - trainLoss: 0.6466537117958069\n",
      "cnt: 0 - valLoss: 0.6449922919273376 - trainLoss: 0.6466513872146606\n",
      "cnt: 0 - valLoss: 0.6449897289276123 - trainLoss: 0.646649181842804\n",
      "cnt: 0 - valLoss: 0.6449868679046631 - trainLoss: 0.646647036075592\n",
      "cnt: 0 - valLoss: 0.6449840664863586 - trainLoss: 0.6466447710990906\n",
      "cnt: 0 - valLoss: 0.6449812054634094 - trainLoss: 0.6466425061225891\n",
      "cnt: 0 - valLoss: 0.6449784636497498 - trainLoss: 0.6466401815414429\n",
      "cnt: 0 - valLoss: 0.6449759602546692 - trainLoss: 0.6466379761695862\n",
      "cnt: 0 - valLoss: 0.6449731588363647 - trainLoss: 0.6466358304023743\n",
      "cnt: 0 - valLoss: 0.6449702978134155 - trainLoss: 0.6466335654258728\n",
      "cnt: 0 - valLoss: 0.6449674963951111 - trainLoss: 0.6466312408447266\n",
      "cnt: 0 - valLoss: 0.6449647545814514 - trainLoss: 0.6466288566589355\n",
      "cnt: 0 - valLoss: 0.6449622511863708 - trainLoss: 0.6466265916824341\n",
      "cnt: 0 - valLoss: 0.6449594497680664 - trainLoss: 0.6466245651245117\n",
      "cnt: 0 - valLoss: 0.6449565887451172 - trainLoss: 0.6466222405433655\n",
      "cnt: 0 - valLoss: 0.6449537873268127 - trainLoss: 0.6466198563575745\n",
      "cnt: 0 - valLoss: 0.6449509263038635 - trainLoss: 0.646617591381073\n",
      "cnt: 0 - valLoss: 0.6449484825134277 - trainLoss: 0.6466152667999268\n",
      "cnt: 0 - valLoss: 0.6449456810951233 - trainLoss: 0.6466132998466492\n",
      "cnt: 0 - valLoss: 0.6449428796768188 - trainLoss: 0.6466109156608582\n",
      "cnt: 0 - valLoss: 0.6449400186538696 - trainLoss: 0.6466085910797119\n",
      "cnt: 0 - valLoss: 0.6449371576309204 - trainLoss: 0.6466062664985657\n",
      "cnt: 0 - valLoss: 0.6449347734451294 - trainLoss: 0.6466039419174194\n",
      "cnt: 0 - valLoss: 0.6449319124221802 - trainLoss: 0.6466019153594971\n",
      "cnt: 0 - valLoss: 0.6449291110038757 - trainLoss: 0.6465996503829956\n",
      "cnt: 0 - valLoss: 0.6449262499809265 - trainLoss: 0.6465972661972046\n",
      "cnt: 0 - valLoss: 0.6449234485626221 - trainLoss: 0.6465950012207031\n",
      "cnt: 0 - valLoss: 0.6449208855628967 - trainLoss: 0.6465926766395569\n",
      "cnt: 0 - valLoss: 0.6449181437492371 - trainLoss: 0.6465905904769897\n",
      "cnt: 0 - valLoss: 0.6449153423309326 - trainLoss: 0.6465883255004883\n",
      "cnt: 0 - valLoss: 0.6449124813079834 - trainLoss: 0.646586000919342\n",
      "cnt: 0 - valLoss: 0.6449096202850342 - trainLoss: 0.6465836763381958\n",
      "cnt: 0 - valLoss: 0.6449071168899536 - trainLoss: 0.6465813517570496\n",
      "cnt: 0 - valLoss: 0.644904375076294 - trainLoss: 0.6465792059898376\n",
      "cnt: 0 - valLoss: 0.6449015140533447 - trainLoss: 0.646577000617981\n",
      "cnt: 0 - valLoss: 0.6448987722396851 - trainLoss: 0.6465746760368347\n",
      "cnt: 0 - valLoss: 0.6448959112167358 - trainLoss: 0.6465723514556885\n",
      "cnt: 0 - valLoss: 0.6448932886123657 - trainLoss: 0.6465700268745422\n",
      "cnt: 0 - valLoss: 0.6448906064033508 - trainLoss: 0.6465678215026855\n",
      "cnt: 0 - valLoss: 0.6448878049850464 - trainLoss: 0.6465656757354736\n",
      "cnt: 0 - valLoss: 0.6448849439620972 - trainLoss: 0.6465634107589722\n",
      "cnt: 0 - valLoss: 0.6448821425437927 - trainLoss: 0.6465610265731812\n",
      "cnt: 0 - valLoss: 0.6448794603347778 - trainLoss: 0.6465587019920349\n",
      "cnt: 0 - valLoss: 0.6448768973350525 - trainLoss: 0.646556556224823\n",
      "cnt: 0 - valLoss: 0.6448739767074585 - trainLoss: 0.6465544104576111\n",
      "cnt: 0 - valLoss: 0.644871175289154 - trainLoss: 0.6465520262718201\n",
      "cnt: 0 - valLoss: 0.6448683738708496 - trainLoss: 0.6465497016906738\n",
      "cnt: 0 - valLoss: 0.6448656320571899 - trainLoss: 0.6465474367141724\n",
      "cnt: 0 - valLoss: 0.6448630690574646 - trainLoss: 0.6465451717376709\n",
      "cnt: 0 - valLoss: 0.6448602676391602 - trainLoss: 0.646543025970459\n",
      "cnt: 0 - valLoss: 0.6448574066162109 - trainLoss: 0.6465407013893127\n",
      "cnt: 0 - valLoss: 0.6448545455932617 - trainLoss: 0.6465383768081665\n",
      "cnt: 0 - valLoss: 0.6448517441749573 - trainLoss: 0.6465360522270203\n",
      "cnt: 0 - valLoss: 0.6448493003845215 - trainLoss: 0.6465337872505188\n",
      "cnt: 0 - valLoss: 0.6448464393615723 - trainLoss: 0.6465316414833069\n",
      "cnt: 0 - valLoss: 0.6448436379432678 - trainLoss: 0.6465293765068054\n",
      "cnt: 0 - valLoss: 0.6448407769203186 - trainLoss: 0.6465270519256592\n",
      "cnt: 0 - valLoss: 0.6448379755020142 - trainLoss: 0.6465246677398682\n",
      "cnt: 0 - valLoss: 0.6448354721069336 - trainLoss: 0.6465224027633667\n",
      "cnt: 0 - valLoss: 0.6448326110839844 - trainLoss: 0.6465203166007996\n",
      "cnt: 0 - valLoss: 0.6448297500610352 - trainLoss: 0.6465179920196533\n",
      "cnt: 0 - valLoss: 0.6448269486427307 - trainLoss: 0.6465156674385071\n",
      "cnt: 0 - valLoss: 0.6448241472244263 - trainLoss: 0.6465132832527161\n",
      "cnt: 0 - valLoss: 0.6448217034339905 - trainLoss: 0.6465110778808594\n",
      "cnt: 0 - valLoss: 0.6448188424110413 - trainLoss: 0.6465089321136475\n",
      "cnt: 0 - valLoss: 0.644815981388092 - trainLoss: 0.646506667137146\n",
      "cnt: 0 - valLoss: 0.6448131799697876 - trainLoss: 0.6465043425559998\n",
      "cnt: 0 - valLoss: 0.6448102593421936 - trainLoss: 0.6465020179748535\n",
      "cnt: 0 - valLoss: 0.6448078751564026 - trainLoss: 0.6464996933937073\n",
      "cnt: 0 - valLoss: 0.6448050141334534 - trainLoss: 0.6464976668357849\n",
      "cnt: 0 - valLoss: 0.6448021531105042 - trainLoss: 0.6464952826499939\n",
      "cnt: 0 - valLoss: 0.6447992920875549 - trainLoss: 0.6464929580688477\n",
      "cnt: 0 - valLoss: 0.6447964906692505 - trainLoss: 0.6464906334877014\n",
      "cnt: 0 - valLoss: 0.6447941064834595 - trainLoss: 0.6464883089065552\n",
      "cnt: 0 - valLoss: 0.6447911858558655 - trainLoss: 0.6464862823486328\n",
      "cnt: 0 - valLoss: 0.6447883248329163 - trainLoss: 0.6464838981628418\n",
      "cnt: 0 - valLoss: 0.6447855234146118 - trainLoss: 0.6464815735816956\n",
      "cnt: 0 - valLoss: 0.6447826623916626 - trainLoss: 0.6464792490005493\n",
      "cnt: 0 - valLoss: 0.6447802186012268 - trainLoss: 0.6464769244194031\n",
      "cnt: 0 - valLoss: 0.6447773575782776 - trainLoss: 0.6464747786521912\n",
      "cnt: 0 - valLoss: 0.6447744965553284 - trainLoss: 0.6464725136756897\n",
      "cnt: 0 - valLoss: 0.6447716355323792 - trainLoss: 0.6464702486991882\n",
      "cnt: 0 - valLoss: 0.6447687745094299 - trainLoss: 0.6464678049087524\n",
      "cnt: 0 - valLoss: 0.6447663307189941 - trainLoss: 0.6464655995368958\n",
      "cnt: 0 - valLoss: 0.6447635889053345 - trainLoss: 0.6464635133743286\n",
      "cnt: 0 - valLoss: 0.6447606682777405 - trainLoss: 0.6464611291885376\n",
      "cnt: 0 - valLoss: 0.6447578072547913 - trainLoss: 0.6464588046073914\n",
      "cnt: 0 - valLoss: 0.6447550058364868 - trainLoss: 0.6464564800262451\n",
      "cnt: 0 - valLoss: 0.6447525024414062 - trainLoss: 0.6464540958404541\n",
      "cnt: 0 - valLoss: 0.6447497010231018 - trainLoss: 0.646452009677887\n",
      "cnt: 0 - valLoss: 0.6447468400001526 - trainLoss: 0.6464498043060303\n",
      "cnt: 0 - valLoss: 0.6447439193725586 - trainLoss: 0.6464474201202393\n",
      "cnt: 0 - valLoss: 0.6447411179542542 - trainLoss: 0.646445095539093\n",
      "cnt: 0 - valLoss: 0.6447385549545288 - trainLoss: 0.6464427709579468\n",
      "cnt: 0 - valLoss: 0.6447358131408691 - trainLoss: 0.6464406847953796\n",
      "cnt: 0 - valLoss: 0.6447328925132751 - trainLoss: 0.6464383602142334\n",
      "cnt: 0 - valLoss: 0.6447300314903259 - trainLoss: 0.6464360356330872\n",
      "cnt: 0 - valLoss: 0.6447272300720215 - trainLoss: 0.6464337110519409\n",
      "cnt: 0 - valLoss: 0.6447247266769409 - trainLoss: 0.6464313268661499\n",
      "cnt: 0 - valLoss: 0.6447218656539917 - trainLoss: 0.646429181098938\n",
      "cnt: 0 - valLoss: 0.6447190046310425 - trainLoss: 0.6464269161224365\n",
      "cnt: 0 - valLoss: 0.6447161436080933 - trainLoss: 0.6464246511459351\n",
      "cnt: 0 - valLoss: 0.644713282585144 - trainLoss: 0.6464222073554993\n",
      "cnt: 0 - valLoss: 0.6447107791900635 - trainLoss: 0.6464199423789978\n",
      "cnt: 0 - valLoss: 0.6447080373764038 - trainLoss: 0.6464177966117859\n",
      "cnt: 0 - valLoss: 0.6447051167488098 - trainLoss: 0.6464155316352844\n",
      "cnt: 0 - valLoss: 0.6447022557258606 - trainLoss: 0.6464131474494934\n",
      "cnt: 0 - valLoss: 0.6446994543075562 - trainLoss: 0.6464108228683472\n",
      "cnt: 0 - valLoss: 0.6446968913078308 - trainLoss: 0.6464085578918457\n",
      "cnt: 0 - valLoss: 0.6446941494941711 - trainLoss: 0.6464064121246338\n",
      "cnt: 0 - valLoss: 0.6446912288665771 - trainLoss: 0.6464041471481323\n",
      "cnt: 0 - valLoss: 0.6446883678436279 - trainLoss: 0.6464017629623413\n",
      "cnt: 0 - valLoss: 0.6446855068206787 - trainLoss: 0.6463994383811951\n",
      "cnt: 0 - valLoss: 0.6446830034255981 - trainLoss: 0.6463971138000488\n",
      "cnt: 0 - valLoss: 0.6446802020072937 - trainLoss: 0.6463949084281921\n",
      "cnt: 0 - valLoss: 0.6446773409843445 - trainLoss: 0.6463927626609802\n",
      "cnt: 0 - valLoss: 0.6446744799613953 - trainLoss: 0.6463903188705444\n",
      "cnt: 0 - valLoss: 0.6446715593338013 - trainLoss: 0.6463879942893982\n",
      "cnt: 0 - valLoss: 0.6446689963340759 - trainLoss: 0.646385669708252\n",
      "cnt: 0 - valLoss: 0.6446661949157715 - trainLoss: 0.64638352394104\n",
      "cnt: 0 - valLoss: 0.6446632742881775 - trainLoss: 0.6463812589645386\n",
      "cnt: 0 - valLoss: 0.6446604132652283 - trainLoss: 0.6463789343833923\n",
      "cnt: 0 - valLoss: 0.6446574926376343 - trainLoss: 0.6463765501976013\n",
      "cnt: 0 - valLoss: 0.6446549296379089 - trainLoss: 0.6463742256164551\n",
      "cnt: 0 - valLoss: 0.6446521282196045 - trainLoss: 0.6463720798492432\n",
      "cnt: 0 - valLoss: 0.6446492671966553 - trainLoss: 0.6463698148727417\n",
      "cnt: 0 - valLoss: 0.644646406173706 - trainLoss: 0.6463674902915955\n",
      "cnt: 0 - valLoss: 0.6446434855461121 - trainLoss: 0.6463651061058044\n",
      "cnt: 0 - valLoss: 0.6446408629417419 - trainLoss: 0.6463627219200134\n",
      "cnt: 0 - valLoss: 0.6446380615234375 - trainLoss: 0.6463605761528015\n",
      "cnt: 0 - valLoss: 0.6446351408958435 - trainLoss: 0.6463583707809448\n",
      "cnt: 0 - valLoss: 0.6446322798728943 - trainLoss: 0.6463560461997986\n",
      "cnt: 0 - valLoss: 0.6446294188499451 - trainLoss: 0.6463536620140076\n",
      "cnt: 0 - valLoss: 0.644626796245575 - trainLoss: 0.6463512778282166\n",
      "cnt: 0 - valLoss: 0.6446239948272705 - trainLoss: 0.6463491916656494\n",
      "cnt: 0 - valLoss: 0.6446211338043213 - trainLoss: 0.6463468074798584\n",
      "cnt: 0 - valLoss: 0.6446182131767273 - trainLoss: 0.6463444828987122\n",
      "cnt: 0 - valLoss: 0.6446153521537781 - trainLoss: 0.6463421583175659\n",
      "cnt: 0 - valLoss: 0.644612729549408 - trainLoss: 0.6463397741317749\n",
      "cnt: 0 - valLoss: 0.6446099281311035 - trainLoss: 0.646337628364563\n",
      "cnt: 0 - valLoss: 0.6446070075035095 - trainLoss: 0.6463353633880615\n",
      "cnt: 0 - valLoss: 0.6446040868759155 - trainLoss: 0.6463330388069153\n",
      "cnt: 0 - valLoss: 0.6446012258529663 - trainLoss: 0.6463306546211243\n",
      "cnt: 0 - valLoss: 0.6445986032485962 - trainLoss: 0.646328330039978\n",
      "cnt: 0 - valLoss: 0.6445958018302917 - trainLoss: 0.6463261842727661\n",
      "cnt: 0 - valLoss: 0.6445928812026978 - trainLoss: 0.6463238596916199\n",
      "cnt: 0 - valLoss: 0.6445899605751038 - trainLoss: 0.6463215351104736\n",
      "cnt: 0 - valLoss: 0.6445871591567993 - trainLoss: 0.6463191509246826\n",
      "cnt: 0 - valLoss: 0.6445845365524292 - trainLoss: 0.6463168263435364\n",
      "cnt: 0 - valLoss: 0.64458167552948 - trainLoss: 0.6463146805763245\n",
      "cnt: 0 - valLoss: 0.6445788145065308 - trainLoss: 0.646312415599823\n",
      "cnt: 0 - valLoss: 0.6445758938789368 - trainLoss: 0.646310031414032\n",
      "cnt: 0 - valLoss: 0.6445729732513428 - trainLoss: 0.6463077068328857\n",
      "cnt: 0 - valLoss: 0.6445704698562622 - trainLoss: 0.6463053226470947\n",
      "cnt: 0 - valLoss: 0.644567608833313 - trainLoss: 0.6463032364845276\n",
      "cnt: 0 - valLoss: 0.6445647478103638 - trainLoss: 0.6463008522987366\n",
      "cnt: 0 - valLoss: 0.6445618271827698 - trainLoss: 0.6462985277175903\n",
      "cnt: 0 - valLoss: 0.644558846950531 - trainLoss: 0.6462962031364441\n",
      "cnt: 0 - valLoss: 0.6445564031600952 - trainLoss: 0.6462938189506531\n",
      "cnt: 0 - valLoss: 0.6445534825325012 - trainLoss: 0.6462916731834412\n",
      "cnt: 0 - valLoss: 0.6445505619049072 - trainLoss: 0.6462893486022949\n",
      "cnt: 0 - valLoss: 0.6445476412773132 - trainLoss: 0.6462869644165039\n",
      "cnt: 0 - valLoss: 0.644544780254364 - trainLoss: 0.6462846398353577\n",
      "cnt: 0 - valLoss: 0.6445423364639282 - trainLoss: 0.6462822556495667\n",
      "cnt: 0 - valLoss: 0.6445394158363342 - trainLoss: 0.6462802290916443\n",
      "cnt: 0 - valLoss: 0.6445364952087402 - trainLoss: 0.6462777853012085\n",
      "cnt: 0 - valLoss: 0.6445335745811462 - trainLoss: 0.646275520324707\n",
      "cnt: 0 - valLoss: 0.644530713558197 - trainLoss: 0.6462731957435608\n",
      "cnt: 0 - valLoss: 0.6445281505584717 - trainLoss: 0.6462708115577698\n",
      "cnt: 0 - valLoss: 0.6445252895355225 - trainLoss: 0.6462687253952026\n",
      "cnt: 0 - valLoss: 0.6445223689079285 - trainLoss: 0.6462663412094116\n",
      "cnt: 0 - valLoss: 0.6445194482803345 - trainLoss: 0.6462639570236206\n",
      "cnt: 0 - valLoss: 0.6445165276527405 - trainLoss: 0.6462616324424744\n",
      "cnt: 0 - valLoss: 0.6445140242576599 - trainLoss: 0.6462593078613281\n",
      "cnt: 0 - valLoss: 0.6445111632347107 - trainLoss: 0.6462571620941162\n",
      "cnt: 0 - valLoss: 0.6445082426071167 - trainLoss: 0.6462547779083252\n",
      "cnt: 0 - valLoss: 0.6445053219795227 - trainLoss: 0.6462523937225342\n",
      "cnt: 0 - valLoss: 0.6445024609565735 - trainLoss: 0.6462500691413879\n",
      "cnt: 0 - valLoss: 0.6444998979568481 - trainLoss: 0.6462477445602417\n",
      "cnt: 0 - valLoss: 0.6444969773292542 - trainLoss: 0.6462455987930298\n",
      "cnt: 0 - valLoss: 0.6444941163063049 - trainLoss: 0.6462432146072388\n",
      "cnt: 0 - valLoss: 0.6444911360740662 - trainLoss: 0.6462408900260925\n",
      "cnt: 0 - valLoss: 0.6444883942604065 - trainLoss: 0.6462385058403015\n",
      "cnt: 0 - valLoss: 0.6444857120513916 - trainLoss: 0.6462362408638\n",
      "cnt: 0 - valLoss: 0.6444828510284424 - trainLoss: 0.6462340950965881\n",
      "cnt: 0 - valLoss: 0.6444799304008484 - trainLoss: 0.6462317109107971\n",
      "cnt: 0 - valLoss: 0.6444770097732544 - trainLoss: 0.6462293267250061\n",
      "cnt: 0 - valLoss: 0.6444742679595947 - trainLoss: 0.6462270021438599\n",
      "cnt: 0 - valLoss: 0.6444716453552246 - trainLoss: 0.6462246775627136\n",
      "cnt: 0 - valLoss: 0.6444686651229858 - trainLoss: 0.6462225317955017\n",
      "cnt: 0 - valLoss: 0.6444657444953918 - trainLoss: 0.6462201476097107\n",
      "cnt: 0 - valLoss: 0.6444628834724426 - trainLoss: 0.6462177634239197\n",
      "cnt: 0 - valLoss: 0.6444600820541382 - trainLoss: 0.6462154388427734\n",
      "cnt: 0 - valLoss: 0.6444574594497681 - trainLoss: 0.646213173866272\n",
      "cnt: 0 - valLoss: 0.6444545388221741 - trainLoss: 0.6462109684944153\n",
      "cnt: 0 - valLoss: 0.6444516181945801 - trainLoss: 0.646208643913269\n",
      "cnt: 0 - valLoss: 0.6444486975669861 - trainLoss: 0.646206259727478\n",
      "cnt: 0 - valLoss: 0.6444460153579712 - trainLoss: 0.646203875541687\n",
      "cnt: 0 - valLoss: 0.6444432735443115 - trainLoss: 0.6462016105651855\n",
      "cnt: 0 - valLoss: 0.6444404125213623 - trainLoss: 0.6461994051933289\n",
      "cnt: 0 - valLoss: 0.6444374918937683 - trainLoss: 0.6461970210075378\n",
      "cnt: 0 - valLoss: 0.6444345712661743 - trainLoss: 0.6461946964263916\n",
      "cnt: 0 - valLoss: 0.6444318890571594 - trainLoss: 0.6461923122406006\n",
      "cnt: 0 - valLoss: 0.644429087638855 - trainLoss: 0.6461900472640991\n",
      "cnt: 0 - valLoss: 0.6444262266159058 - trainLoss: 0.6461878418922424\n",
      "cnt: 0 - valLoss: 0.6444233059883118 - trainLoss: 0.6461854577064514\n",
      "cnt: 0 - valLoss: 0.6444203853607178 - trainLoss: 0.6461830735206604\n",
      "cnt: 0 - valLoss: 0.6444177627563477 - trainLoss: 0.6461806893348694\n",
      "cnt: 0 - valLoss: 0.6444149613380432 - trainLoss: 0.6461785435676575\n",
      "cnt: 0 - valLoss: 0.6444119811058044 - trainLoss: 0.646176278591156\n",
      "cnt: 0 - valLoss: 0.6444091200828552 - trainLoss: 0.646173894405365\n",
      "cnt: 0 - valLoss: 0.6444061398506165 - trainLoss: 0.646171510219574\n",
      "cnt: 0 - valLoss: 0.6444035172462463 - trainLoss: 0.646169126033783\n",
      "cnt: 0 - valLoss: 0.6444007158279419 - trainLoss: 0.646166980266571\n",
      "cnt: 0 - valLoss: 0.6443978548049927 - trainLoss: 0.6461646556854248\n",
      "cnt: 0 - valLoss: 0.6443949341773987 - trainLoss: 0.6461622714996338\n",
      "cnt: 0 - valLoss: 0.6443919539451599 - trainLoss: 0.6461599469184875\n",
      "cnt: 0 - valLoss: 0.6443894505500793 - trainLoss: 0.6461575627326965\n",
      "cnt: 0 - valLoss: 0.6443865895271301 - trainLoss: 0.6461554169654846\n",
      "cnt: 0 - valLoss: 0.6443836092948914 - trainLoss: 0.6461530923843384\n",
      "cnt: 0 - valLoss: 0.6443807482719421 - trainLoss: 0.6461507081985474\n",
      "cnt: 0 - valLoss: 0.6443778276443481 - trainLoss: 0.6461483240127563\n",
      "cnt: 0 - valLoss: 0.6443753242492676 - trainLoss: 0.6461459398269653\n",
      "cnt: 0 - valLoss: 0.6443723440170288 - trainLoss: 0.6461438536643982\n",
      "cnt: 0 - valLoss: 0.6443694233894348 - trainLoss: 0.6461414694786072\n",
      "cnt: 0 - valLoss: 0.6443665623664856 - trainLoss: 0.6461391448974609\n",
      "cnt: 0 - valLoss: 0.6443636417388916 - trainLoss: 0.6461367607116699\n",
      "cnt: 0 - valLoss: 0.6443610787391663 - trainLoss: 0.6461343765258789\n",
      "cnt: 0 - valLoss: 0.6443581581115723 - trainLoss: 0.6461322903633118\n",
      "cnt: 0 - valLoss: 0.6443552374839783 - trainLoss: 0.6461299061775208\n",
      "cnt: 0 - valLoss: 0.6443523168563843 - trainLoss: 0.6461275219917297\n",
      "cnt: 0 - valLoss: 0.6443495154380798 - trainLoss: 0.6461251378059387\n",
      "cnt: 0 - valLoss: 0.6443468332290649 - trainLoss: 0.6461228132247925\n",
      "cnt: 0 - valLoss: 0.6443439722061157 - trainLoss: 0.6461206674575806\n",
      "cnt: 0 - valLoss: 0.6443410515785217 - trainLoss: 0.6461182832717896\n",
      "cnt: 0 - valLoss: 0.6443381309509277 - trainLoss: 0.6461158990859985\n",
      "cnt: 0 - valLoss: 0.6443353891372681 - trainLoss: 0.6461134552955627\n",
      "cnt: 0 - valLoss: 0.6443326473236084 - trainLoss: 0.646111249923706\n",
      "cnt: 0 - valLoss: 0.6443297863006592 - trainLoss: 0.6461090445518494\n",
      "cnt: 0 - valLoss: 0.6443268060684204 - trainLoss: 0.6461066603660583\n",
      "cnt: 0 - valLoss: 0.6443238854408264 - trainLoss: 0.6461042761802673\n",
      "cnt: 0 - valLoss: 0.6443211436271667 - trainLoss: 0.6461018919944763\n",
      "cnt: 0 - valLoss: 0.6443184614181519 - trainLoss: 0.6460996270179749\n",
      "cnt: 0 - valLoss: 0.6443155407905579 - trainLoss: 0.6460974216461182\n",
      "cnt: 0 - valLoss: 0.6443125605583191 - trainLoss: 0.6460950374603271\n",
      "cnt: 0 - valLoss: 0.6443096399307251 - trainLoss: 0.6460926532745361\n",
      "cnt: 0 - valLoss: 0.644307017326355 - trainLoss: 0.6460902094841003\n",
      "cnt: 0 - valLoss: 0.6443041563034058 - trainLoss: 0.6460880041122437\n",
      "cnt: 0 - valLoss: 0.6443012356758118 - trainLoss: 0.6460857391357422\n",
      "cnt: 0 - valLoss: 0.6442983746528625 - trainLoss: 0.646083414554596\n",
      "cnt: 0 - valLoss: 0.644295334815979 - trainLoss: 0.6460810303688049\n",
      "cnt: 0 - valLoss: 0.6442928314208984 - trainLoss: 0.6460785269737244\n",
      "cnt: 0 - valLoss: 0.6442899107933044 - trainLoss: 0.646076500415802\n",
      "cnt: 0 - valLoss: 0.6442870497703552 - trainLoss: 0.646074116230011\n",
      "cnt: 0 - valLoss: 0.6442840695381165 - trainLoss: 0.6460717916488647\n",
      "cnt: 0 - valLoss: 0.6442812085151672 - trainLoss: 0.646069347858429\n",
      "cnt: 0 - valLoss: 0.6442786455154419 - trainLoss: 0.6460669636726379\n",
      "cnt: 0 - valLoss: 0.6442756652832031 - trainLoss: 0.646064817905426\n",
      "cnt: 0 - valLoss: 0.6442728042602539 - trainLoss: 0.6460624933242798\n",
      "cnt: 0 - valLoss: 0.6442698240280151 - trainLoss: 0.6460601091384888\n",
      "cnt: 0 - valLoss: 0.6442669034004211 - trainLoss: 0.646057665348053\n",
      "cnt: 0 - valLoss: 0.6442643404006958 - trainLoss: 0.6460553407669067\n",
      "cnt: 0 - valLoss: 0.6442614793777466 - trainLoss: 0.6460531949996948\n",
      "cnt: 0 - valLoss: 0.6442584991455078 - trainLoss: 0.6460508704185486\n",
      "cnt: 0 - valLoss: 0.6442555785179138 - trainLoss: 0.6460484266281128\n",
      "cnt: 0 - valLoss: 0.6442527770996094 - trainLoss: 0.646045982837677\n",
      "cnt: 0 - valLoss: 0.6442500948905945 - trainLoss: 0.6460437178611755\n",
      "cnt: 0 - valLoss: 0.6442471742630005 - trainLoss: 0.6460415720939636\n",
      "cnt: 0 - valLoss: 0.6442442536354065 - trainLoss: 0.6460391283035278\n",
      "cnt: 0 - valLoss: 0.6442413330078125 - trainLoss: 0.646036684513092\n",
      "cnt: 0 - valLoss: 0.6442385911941528 - trainLoss: 0.6460344195365906\n",
      "cnt: 0 - valLoss: 0.6442358493804932 - trainLoss: 0.6460320949554443\n",
      "cnt: 0 - valLoss: 0.644232988357544 - trainLoss: 0.6460298895835876\n",
      "cnt: 0 - valLoss: 0.6442300081253052 - trainLoss: 0.6460275053977966\n",
      "cnt: 0 - valLoss: 0.6442270874977112 - trainLoss: 0.6460251212120056\n",
      "cnt: 0 - valLoss: 0.6442244052886963 - trainLoss: 0.6460227370262146\n",
      "cnt: 0 - valLoss: 0.6442216038703918 - trainLoss: 0.6460204720497131\n",
      "cnt: 0 - valLoss: 0.6442186832427979 - trainLoss: 0.6460182070732117\n",
      "cnt: 0 - valLoss: 0.6442157626152039 - trainLoss: 0.6460157632827759\n",
      "cnt: 0 - valLoss: 0.6442128419876099 - trainLoss: 0.6460133790969849\n",
      "cnt: 0 - valLoss: 0.6442102789878845 - trainLoss: 0.6460110545158386\n",
      "cnt: 0 - valLoss: 0.6442073583602905 - trainLoss: 0.6460089087486267\n",
      "cnt: 0 - valLoss: 0.6442044377326965 - trainLoss: 0.6460065245628357\n",
      "cnt: 0 - valLoss: 0.6442014575004578 - trainLoss: 0.6460040807723999\n",
      "cnt: 0 - valLoss: 0.6441985368728638 - trainLoss: 0.6460016965866089\n",
      "cnt: 0 - valLoss: 0.644196093082428 - trainLoss: 0.6459993720054626\n",
      "cnt: 0 - valLoss: 0.6441930532455444 - trainLoss: 0.6459972262382507\n",
      "cnt: 0 - valLoss: 0.6441901326179504 - trainLoss: 0.6459947824478149\n",
      "cnt: 0 - valLoss: 0.6441871523857117 - trainLoss: 0.6459923982620239\n",
      "cnt: 0 - valLoss: 0.6441843509674072 - trainLoss: 0.6459900140762329\n",
      "cnt: 0 - valLoss: 0.6441816687583923 - trainLoss: 0.6459876894950867\n",
      "cnt: 0 - valLoss: 0.6441786885261536 - trainLoss: 0.64598548412323\n",
      "cnt: 0 - valLoss: 0.6441757082939148 - trainLoss: 0.645983099937439\n",
      "cnt: 0 - valLoss: 0.6441727876663208 - trainLoss: 0.6459806561470032\n",
      "cnt: 0 - valLoss: 0.6441699862480164 - trainLoss: 0.6459783315658569\n",
      "cnt: 0 - valLoss: 0.6441672444343567 - trainLoss: 0.6459759473800659\n",
      "cnt: 0 - valLoss: 0.6441642642021179 - trainLoss: 0.6459737420082092\n",
      "cnt: 0 - valLoss: 0.6441612839698792 - trainLoss: 0.645971417427063\n",
      "cnt: 0 - valLoss: 0.6441583633422852 - trainLoss: 0.6459689736366272\n",
      "cnt: 0 - valLoss: 0.6441556811332703 - trainLoss: 0.6459665894508362\n",
      "cnt: 0 - valLoss: 0.644152820110321 - trainLoss: 0.6459643244743347\n",
      "cnt: 0 - valLoss: 0.644149899482727 - trainLoss: 0.6459619998931885\n",
      "cnt: 0 - valLoss: 0.6441469788551331 - trainLoss: 0.6459596157073975\n",
      "cnt: 0 - valLoss: 0.6441439390182495 - trainLoss: 0.6459572911262512\n",
      "cnt: 0 - valLoss: 0.6441413164138794 - trainLoss: 0.6459548473358154\n",
      "cnt: 0 - valLoss: 0.6441383957862854 - trainLoss: 0.6459527015686035\n",
      "cnt: 0 - valLoss: 0.6441355347633362 - trainLoss: 0.6459503173828125\n",
      "cnt: 0 - valLoss: 0.6441324949264526 - trainLoss: 0.6459478735923767\n",
      "cnt: 0 - valLoss: 0.6441295146942139 - trainLoss: 0.6459454894065857\n",
      "cnt: 0 - valLoss: 0.6441270112991333 - trainLoss: 0.6459431052207947\n",
      "cnt: 0 - valLoss: 0.6441240906715393 - trainLoss: 0.6459409594535828\n",
      "cnt: 0 - valLoss: 0.644120991230011 - trainLoss: 0.6459385752677917\n",
      "cnt: 0 - valLoss: 0.644118070602417 - trainLoss: 0.6459361910820007\n",
      "cnt: 0 - valLoss: 0.6441152095794678 - trainLoss: 0.6459337472915649\n",
      "cnt: 0 - valLoss: 0.6441125273704529 - trainLoss: 0.6459313631057739\n",
      "cnt: 0 - valLoss: 0.6441095471382141 - trainLoss: 0.645929217338562\n",
      "cnt: 0 - valLoss: 0.6441066861152649 - trainLoss: 0.645926833152771\n",
      "cnt: 0 - valLoss: 0.6441036462783813 - trainLoss: 0.6459243893623352\n",
      "cnt: 0 - valLoss: 0.6441008448600769 - trainLoss: 0.6459220051765442\n",
      "cnt: 0 - valLoss: 0.6440981030464172 - trainLoss: 0.6459197402000427\n",
      "cnt: 0 - valLoss: 0.6440951824188232 - trainLoss: 0.6459174752235413\n",
      "cnt: 0 - valLoss: 0.6440922021865845 - trainLoss: 0.6459150314331055\n",
      "cnt: 0 - valLoss: 0.6440891623497009 - trainLoss: 0.6459125876426697\n",
      "cnt: 0 - valLoss: 0.6440865397453308 - trainLoss: 0.6459102034568787\n",
      "cnt: 0 - valLoss: 0.6440836787223816 - trainLoss: 0.645907998085022\n",
      "cnt: 0 - valLoss: 0.6440807580947876 - trainLoss: 0.6459056735038757\n",
      "cnt: 0 - valLoss: 0.6440777778625488 - trainLoss: 0.6459032297134399\n",
      "cnt: 0 - valLoss: 0.6440747976303101 - trainLoss: 0.6459008455276489\n",
      "cnt: 0 - valLoss: 0.6440721750259399 - trainLoss: 0.6458984613418579\n",
      "cnt: 0 - valLoss: 0.6440693140029907 - trainLoss: 0.6458962559700012\n",
      "cnt: 0 - valLoss: 0.6440662741661072 - trainLoss: 0.645893931388855\n",
      "cnt: 0 - valLoss: 0.6440633535385132 - trainLoss: 0.6458914875984192\n",
      "cnt: 0 - valLoss: 0.6440603733062744 - trainLoss: 0.6458891034126282\n",
      "cnt: 0 - valLoss: 0.6440576910972595 - trainLoss: 0.6458867192268372\n",
      "cnt: 0 - valLoss: 0.6440548300743103 - trainLoss: 0.6458845734596252\n",
      "cnt: 0 - valLoss: 0.6440517902374268 - trainLoss: 0.6458821296691895\n",
      "cnt: 0 - valLoss: 0.6440488696098328 - trainLoss: 0.6458797454833984\n",
      "cnt: 0 - valLoss: 0.6440460681915283 - trainLoss: 0.6458773016929626\n",
      "cnt: 0 - valLoss: 0.6440433263778687 - trainLoss: 0.6458749771118164\n",
      "cnt: 0 - valLoss: 0.6440403461456299 - trainLoss: 0.6458727717399597\n",
      "cnt: 0 - valLoss: 0.6440374255180359 - trainLoss: 0.6458703279495239\n",
      "cnt: 0 - valLoss: 0.6440343856811523 - trainLoss: 0.6458679437637329\n",
      "cnt: 0 - valLoss: 0.6440317034721375 - trainLoss: 0.6458654403686523\n",
      "cnt: 0 - valLoss: 0.6440288424491882 - trainLoss: 0.6458632946014404\n",
      "cnt: 0 - valLoss: 0.6440259218215942 - trainLoss: 0.6458609700202942\n",
      "cnt: 0 - valLoss: 0.6440228819847107 - trainLoss: 0.6458585858345032\n",
      "cnt: 0 - valLoss: 0.6440199017524719 - trainLoss: 0.6458561420440674\n",
      "cnt: 0 - valLoss: 0.6440173387527466 - trainLoss: 0.6458537578582764\n",
      "cnt: 0 - valLoss: 0.6440144181251526 - trainLoss: 0.6458516120910645\n",
      "cnt: 0 - valLoss: 0.6440114378929138 - trainLoss: 0.6458491086959839\n",
      "cnt: 0 - valLoss: 0.6440083980560303 - trainLoss: 0.6458467841148376\n",
      "cnt: 0 - valLoss: 0.644005537033081 - trainLoss: 0.6458443403244019\n",
      "cnt: 0 - valLoss: 0.6440028548240662 - trainLoss: 0.6458418965339661\n",
      "cnt: 0 - valLoss: 0.6439999341964722 - trainLoss: 0.6458397507667542\n",
      "cnt: 0 - valLoss: 0.6439968943595886 - trainLoss: 0.6458373069763184\n",
      "cnt: 0 - valLoss: 0.6439939141273499 - trainLoss: 0.6458349227905273\n",
      "cnt: 0 - valLoss: 0.6439911723136902 - trainLoss: 0.6458324790000916\n",
      "cnt: 0 - valLoss: 0.6439883708953857 - trainLoss: 0.6458302140235901\n",
      "cnt: 0 - valLoss: 0.643985390663147 - trainLoss: 0.6458279490470886\n",
      "cnt: 0 - valLoss: 0.643982470035553 - trainLoss: 0.6458255052566528\n",
      "cnt: 0 - valLoss: 0.6439794301986694 - trainLoss: 0.645823061466217\n",
      "cnt: 0 - valLoss: 0.6439768075942993 - trainLoss: 0.645820677280426\n",
      "cnt: 0 - valLoss: 0.6439738869667053 - trainLoss: 0.6458184719085693\n",
      "cnt: 0 - valLoss: 0.6439709067344666 - trainLoss: 0.6458161473274231\n",
      "cnt: 0 - valLoss: 0.6439679265022278 - trainLoss: 0.6458137035369873\n",
      "cnt: 0 - valLoss: 0.643964946269989 - trainLoss: 0.6458112597465515\n",
      "cnt: 0 - valLoss: 0.6439623832702637 - trainLoss: 0.6458088159561157\n",
      "cnt: 0 - valLoss: 0.6439594030380249 - trainLoss: 0.6458067297935486\n",
      "cnt: 0 - valLoss: 0.6439563632011414 - trainLoss: 0.6458042860031128\n",
      "cnt: 0 - valLoss: 0.6439533829689026 - trainLoss: 0.645801842212677\n",
      "cnt: 0 - valLoss: 0.6439505815505981 - trainLoss: 0.645799458026886\n",
      "cnt: 0 - valLoss: 0.6439478993415833 - trainLoss: 0.6457971334457397\n",
      "cnt: 0 - valLoss: 0.6439449191093445 - trainLoss: 0.6457948684692383\n",
      "cnt: 0 - valLoss: 0.6439418792724609 - trainLoss: 0.6457924246788025\n",
      "cnt: 0 - valLoss: 0.6439389586448669 - trainLoss: 0.6457900404930115\n",
      "cnt: 0 - valLoss: 0.6439361572265625 - trainLoss: 0.6457876563072205\n",
      "cnt: 0 - valLoss: 0.6439333558082581 - trainLoss: 0.6457853317260742\n",
      "cnt: 0 - valLoss: 0.6439303159713745 - trainLoss: 0.645783007144928\n",
      "cnt: 0 - valLoss: 0.6439273953437805 - trainLoss: 0.645780622959137\n",
      "cnt: 0 - valLoss: 0.6439244151115417 - trainLoss: 0.6457781791687012\n",
      "cnt: 0 - valLoss: 0.6439217329025269 - trainLoss: 0.6457757353782654\n",
      "cnt: 0 - valLoss: 0.6439188122749329 - trainLoss: 0.6457735896110535\n",
      "cnt: 0 - valLoss: 0.6439158320426941 - trainLoss: 0.6457712054252625\n",
      "cnt: 0 - valLoss: 0.6439128518104553 - trainLoss: 0.6457687616348267\n",
      "cnt: 0 - valLoss: 0.6439099311828613 - trainLoss: 0.6457663178443909\n",
      "cnt: 0 - valLoss: 0.6439072489738464 - trainLoss: 0.6457639336585999\n",
      "cnt: 0 - valLoss: 0.6439043283462524 - trainLoss: 0.6457617878913879\n",
      "cnt: 0 - valLoss: 0.6439012885093689 - trainLoss: 0.6457594037055969\n",
      "cnt: 0 - valLoss: 0.6438983678817749 - trainLoss: 0.6457569003105164\n",
      "cnt: 0 - valLoss: 0.6438955068588257 - trainLoss: 0.6457545161247253\n",
      "cnt: 0 - valLoss: 0.643892765045166 - trainLoss: 0.6457521915435791\n",
      "cnt: 0 - valLoss: 0.6438897848129272 - trainLoss: 0.6457499265670776\n",
      "cnt: 0 - valLoss: 0.6438868045806885 - trainLoss: 0.6457474827766418\n",
      "cnt: 0 - valLoss: 0.6438837647438049 - trainLoss: 0.645745038986206\n",
      "cnt: 0 - valLoss: 0.6438811421394348 - trainLoss: 0.6457425951957703\n",
      "cnt: 0 - valLoss: 0.6438782215118408 - trainLoss: 0.6457403898239136\n",
      "cnt: 0 - valLoss: 0.6438751816749573 - trainLoss: 0.6457380652427673\n",
      "cnt: 0 - valLoss: 0.6438722610473633 - trainLoss: 0.6457356810569763\n",
      "cnt: 0 - valLoss: 0.6438692808151245 - trainLoss: 0.6457331776618958\n",
      "cnt: 0 - valLoss: 0.6438666582107544 - trainLoss: 0.6457307934761047\n",
      "cnt: 0 - valLoss: 0.6438637375831604 - trainLoss: 0.645728588104248\n",
      "cnt: 0 - valLoss: 0.6438606977462769 - trainLoss: 0.645726203918457\n",
      "cnt: 0 - valLoss: 0.6438576579093933 - trainLoss: 0.6457237005233765\n",
      "cnt: 0 - valLoss: 0.6438547968864441 - trainLoss: 0.6457213163375854\n",
      "cnt: 0 - valLoss: 0.643852174282074 - trainLoss: 0.6457189321517944\n",
      "cnt: 0 - valLoss: 0.6438491344451904 - trainLoss: 0.6457167267799377\n",
      "cnt: 0 - valLoss: 0.6438461542129517 - trainLoss: 0.645714282989502\n",
      "cnt: 0 - valLoss: 0.6438431739807129 - trainLoss: 0.6457118391990662\n",
      "cnt: 0 - valLoss: 0.6438404321670532 - trainLoss: 0.6457093954086304\n",
      "cnt: 0 - valLoss: 0.643837571144104 - trainLoss: 0.6457071900367737\n",
      "cnt: 0 - valLoss: 0.6438345909118652 - trainLoss: 0.6457048654556274\n",
      "cnt: 0 - valLoss: 0.6438315510749817 - trainLoss: 0.6457023620605469\n",
      "cnt: 0 - valLoss: 0.6438285708427429 - trainLoss: 0.6456999778747559\n",
      "cnt: 0 - valLoss: 0.6438260078430176 - trainLoss: 0.6456975340843201\n",
      "cnt: 0 - valLoss: 0.6438230276107788 - trainLoss: 0.6456953883171082\n",
      "cnt: 0 - valLoss: 0.6438199877738953 - trainLoss: 0.6456929445266724\n",
      "cnt: 0 - valLoss: 0.6438169479370117 - trainLoss: 0.6456905007362366\n",
      "cnt: 0 - valLoss: 0.6438141465187073 - trainLoss: 0.645687997341156\n",
      "cnt: 0 - valLoss: 0.6438114047050476 - trainLoss: 0.6456856727600098\n",
      "cnt: 0 - valLoss: 0.6438083648681641 - trainLoss: 0.6456834673881531\n",
      "cnt: 0 - valLoss: 0.6438054442405701 - trainLoss: 0.6456810235977173\n",
      "cnt: 0 - valLoss: 0.6438024044036865 - trainLoss: 0.6456785798072815\n",
      "cnt: 0 - valLoss: 0.6437996625900269 - trainLoss: 0.6456761360168457\n",
      "cnt: 0 - valLoss: 0.6437967419624329 - trainLoss: 0.6456738710403442\n",
      "cnt: 0 - valLoss: 0.6437938809394836 - trainLoss: 0.645671546459198\n",
      "cnt: 0 - valLoss: 0.6437908411026001 - trainLoss: 0.6456691026687622\n",
      "cnt: 0 - valLoss: 0.6437878012657166 - trainLoss: 0.6456667184829712\n",
      "cnt: 0 - valLoss: 0.643785297870636 - trainLoss: 0.6456642746925354\n",
      "cnt: 0 - valLoss: 0.6437822580337524 - trainLoss: 0.6456620693206787\n",
      "cnt: 0 - valLoss: 0.6437792181968689 - trainLoss: 0.6456596255302429\n",
      "cnt: 0 - valLoss: 0.6437761783599854 - trainLoss: 0.6456571817398071\n",
      "cnt: 0 - valLoss: 0.6437733769416809 - trainLoss: 0.6456547379493713\n",
      "cnt: 0 - valLoss: 0.643770694732666 - trainLoss: 0.6456524133682251\n",
      "cnt: 0 - valLoss: 0.6437675952911377 - trainLoss: 0.6456501483917236\n",
      "cnt: 0 - valLoss: 0.6437646150588989 - trainLoss: 0.6456477046012878\n",
      "cnt: 0 - valLoss: 0.6437616348266602 - trainLoss: 0.6456453204154968\n",
      "cnt: 0 - valLoss: 0.6437589526176453 - trainLoss: 0.6456428170204163\n",
      "cnt: 0 - valLoss: 0.6437560319900513 - trainLoss: 0.6456406116485596\n",
      "cnt: 0 - valLoss: 0.6437529921531677 - trainLoss: 0.6456382870674133\n",
      "cnt: 0 - valLoss: 0.643750011920929 - trainLoss: 0.645635724067688\n",
      "cnt: 0 - valLoss: 0.6437470316886902 - trainLoss: 0.645633339881897\n",
      "cnt: 0 - valLoss: 0.6437444090843201 - trainLoss: 0.6456308960914612\n",
      "cnt: 0 - valLoss: 0.6437413692474365 - trainLoss: 0.6456287503242493\n",
      "cnt: 0 - valLoss: 0.6437383890151978 - trainLoss: 0.6456263065338135\n",
      "cnt: 0 - valLoss: 0.643735408782959 - trainLoss: 0.6456238627433777\n",
      "cnt: 0 - valLoss: 0.6437325477600098 - trainLoss: 0.6456214189529419\n",
      "cnt: 0 - valLoss: 0.6437298059463501 - trainLoss: 0.6456190347671509\n",
      "cnt: 0 - valLoss: 0.6437268257141113 - trainLoss: 0.6456168293952942\n",
      "cnt: 0 - valLoss: 0.6437238454818726 - trainLoss: 0.6456143260002136\n",
      "cnt: 0 - valLoss: 0.643720805644989 - trainLoss: 0.6456118822097778\n",
      "cnt: 0 - valLoss: 0.6437181830406189 - trainLoss: 0.645609438419342\n",
      "cnt: 0 - valLoss: 0.6437151432037354 - trainLoss: 0.6456071734428406\n",
      "cnt: 0 - valLoss: 0.6437122225761414 - trainLoss: 0.6456048488616943\n",
      "cnt: 0 - valLoss: 0.6437091827392578 - trainLoss: 0.6456024050712585\n",
      "cnt: 0 - valLoss: 0.6437062621116638 - trainLoss: 0.6455999612808228\n",
      "cnt: 0 - valLoss: 0.6437035799026489 - trainLoss: 0.6455975770950317\n",
      "cnt: 0 - valLoss: 0.6437005400657654 - trainLoss: 0.645595371723175\n",
      "cnt: 0 - valLoss: 0.6436975598335266 - trainLoss: 0.6455929279327393\n",
      "cnt: 0 - valLoss: 0.6436945199966431 - trainLoss: 0.6455904245376587\n",
      "cnt: 0 - valLoss: 0.6436917781829834 - trainLoss: 0.6455880403518677\n",
      "cnt: 0 - valLoss: 0.6436889171600342 - trainLoss: 0.6455856561660767\n",
      "cnt: 0 - valLoss: 0.6436859369277954 - trainLoss: 0.6455833315849304\n",
      "cnt: 0 - valLoss: 0.6436829566955566 - trainLoss: 0.6455809473991394\n",
      "cnt: 0 - valLoss: 0.6436799168586731 - trainLoss: 0.6455784440040588\n",
      "cnt: 0 - valLoss: 0.6436773538589478 - trainLoss: 0.645576000213623\n",
      "cnt: 0 - valLoss: 0.643674373626709 - trainLoss: 0.6455738544464111\n",
      "cnt: 0 - valLoss: 0.6436712741851807 - trainLoss: 0.6455713510513306\n",
      "cnt: 0 - valLoss: 0.6436682939529419 - trainLoss: 0.6455689072608948\n",
      "cnt: 0 - valLoss: 0.6436653733253479 - trainLoss: 0.645566463470459\n",
      "cnt: 0 - valLoss: 0.643662691116333 - trainLoss: 0.6455641388893127\n",
      "cnt: 0 - valLoss: 0.6436597108840942 - trainLoss: 0.645561933517456\n",
      "cnt: 0 - valLoss: 0.6436566710472107 - trainLoss: 0.6455594897270203\n",
      "cnt: 0 - valLoss: 0.6436536908149719 - trainLoss: 0.6455569863319397\n",
      "cnt: 0 - valLoss: 0.6436508893966675 - trainLoss: 0.6455545425415039\n",
      "cnt: 0 - valLoss: 0.6436480283737183 - trainLoss: 0.6455522775650024\n",
      "cnt: 0 - valLoss: 0.6436450481414795 - trainLoss: 0.6455499529838562\n",
      "cnt: 0 - valLoss: 0.6436420679092407 - trainLoss: 0.6455475091934204\n",
      "cnt: 0 - valLoss: 0.6436390280723572 - trainLoss: 0.6455450654029846\n",
      "cnt: 0 - valLoss: 0.6436364054679871 - trainLoss: 0.645542562007904\n",
      "cnt: 0 - valLoss: 0.6436333656311035 - trainLoss: 0.6455403566360474\n",
      "cnt: 0 - valLoss: 0.6436303853988647 - trainLoss: 0.6455379724502563\n",
      "cnt: 0 - valLoss: 0.643627405166626 - trainLoss: 0.6455354690551758\n",
      "cnt: 0 - valLoss: 0.6436245441436768 - trainLoss: 0.64553302526474\n",
      "cnt: 0 - valLoss: 0.6436217427253723 - trainLoss: 0.645530641078949\n",
      "cnt: 0 - valLoss: 0.6436187028884888 - trainLoss: 0.6455283761024475\n",
      "cnt: 0 - valLoss: 0.64361572265625 - trainLoss: 0.6455259323120117\n",
      "cnt: 0 - valLoss: 0.6436127424240112 - trainLoss: 0.6455234885215759\n",
      "cnt: 0 - valLoss: 0.6436100602149963 - trainLoss: 0.6455209851264954\n",
      "cnt: 0 - valLoss: 0.6436070799827576 - trainLoss: 0.6455187797546387\n",
      "cnt: 0 - valLoss: 0.643604040145874 - trainLoss: 0.6455163955688477\n",
      "cnt: 0 - valLoss: 0.6436010003089905 - trainLoss: 0.6455139517784119\n",
      "cnt: 0 - valLoss: 0.6435981392860413 - trainLoss: 0.6455114483833313\n",
      "cnt: 0 - valLoss: 0.6435953974723816 - trainLoss: 0.6455091238021851\n",
      "cnt: 0 - valLoss: 0.6435924768447876 - trainLoss: 0.6455068588256836\n",
      "cnt: 0 - valLoss: 0.6435893774032593 - trainLoss: 0.6455044150352478\n",
      "cnt: 0 - valLoss: 0.6435863375663757 - trainLoss: 0.6455019116401672\n",
      "cnt: 0 - valLoss: 0.6435836553573608 - trainLoss: 0.6454994082450867\n",
      "cnt: 0 - valLoss: 0.6435807347297668 - trainLoss: 0.64549720287323\n",
      "cnt: 0 - valLoss: 0.6435777544975281 - trainLoss: 0.645494818687439\n",
      "cnt: 0 - valLoss: 0.6435747742652893 - trainLoss: 0.6454923748970032\n",
      "cnt: 0 - valLoss: 0.6435717344284058 - trainLoss: 0.6454899311065674\n",
      "cnt: 0 - valLoss: 0.6435691118240356 - trainLoss: 0.6454874873161316\n",
      "cnt: 0 - valLoss: 0.6435660719871521 - trainLoss: 0.6454852223396301\n",
      "cnt: 0 - valLoss: 0.6435630321502686 - trainLoss: 0.6454827785491943\n",
      "cnt: 0 - valLoss: 0.643559992313385 - trainLoss: 0.6454803347587585\n",
      "cnt: 0 - valLoss: 0.6435571908950806 - trainLoss: 0.6454778909683228\n",
      "cnt: 0 - valLoss: 0.6435544490814209 - trainLoss: 0.6454755663871765\n",
      "cnt: 0 - valLoss: 0.6435514092445374 - trainLoss: 0.6454732418060303\n",
      "cnt: 0 - valLoss: 0.6435483694076538 - trainLoss: 0.6454707980155945\n",
      "cnt: 0 - valLoss: 0.6435453295707703 - trainLoss: 0.6454682946205139\n",
      "cnt: 0 - valLoss: 0.6435427069664001 - trainLoss: 0.6454658508300781\n",
      "cnt: 0 - valLoss: 0.6435396671295166 - trainLoss: 0.6454637050628662\n",
      "cnt: 0 - valLoss: 0.6435366868972778 - trainLoss: 0.6454612612724304\n",
      "cnt: 0 - valLoss: 0.6435335874557495 - trainLoss: 0.6454587578773499\n",
      "cnt: 0 - valLoss: 0.6435307264328003 - trainLoss: 0.6454562544822693\n",
      "cnt: 0 - valLoss: 0.6435279846191406 - trainLoss: 0.6454538702964783\n",
      "cnt: 0 - valLoss: 0.6435250043869019 - trainLoss: 0.6454516649246216\n",
      "cnt: 0 - valLoss: 0.6435219645500183 - trainLoss: 0.645449161529541\n",
      "cnt: 0 - valLoss: 0.6435189247131348 - trainLoss: 0.6454467177391052\n",
      "cnt: 0 - valLoss: 0.6435161828994751 - trainLoss: 0.6454442143440247\n",
      "cnt: 0 - valLoss: 0.6435132622718811 - trainLoss: 0.645442008972168\n",
      "cnt: 0 - valLoss: 0.6435102820396423 - trainLoss: 0.6454395651817322\n",
      "cnt: 0 - valLoss: 0.643507182598114 - trainLoss: 0.6454371213912964\n",
      "cnt: 0 - valLoss: 0.64350426197052 - trainLoss: 0.6454346776008606\n",
      "cnt: 0 - valLoss: 0.6435016393661499 - trainLoss: 0.6454322338104248\n",
      "cnt: 0 - valLoss: 0.6434985399246216 - trainLoss: 0.6454300284385681\n",
      "cnt: 0 - valLoss: 0.6434955596923828 - trainLoss: 0.6454275250434875\n",
      "cnt: 0 - valLoss: 0.6434924602508545 - trainLoss: 0.6454250812530518\n",
      "cnt: 0 - valLoss: 0.6434897780418396 - trainLoss: 0.6454225778579712\n",
      "cnt: 0 - valLoss: 0.6434868574142456 - trainLoss: 0.6454203128814697\n",
      "cnt: 0 - valLoss: 0.6434838771820068 - trainLoss: 0.6454179883003235\n",
      "cnt: 0 - valLoss: 0.6434807777404785 - trainLoss: 0.6454154849052429\n",
      "cnt: 0 - valLoss: 0.6434778571128845 - trainLoss: 0.6454129815101624\n",
      "cnt: 0 - valLoss: 0.6434752345085144 - trainLoss: 0.6454105377197266\n",
      "cnt: 0 - valLoss: 0.6434721350669861 - trainLoss: 0.6454083323478699\n",
      "cnt: 0 - valLoss: 0.6434690952301025 - trainLoss: 0.6454058885574341\n",
      "cnt: 0 - valLoss: 0.6434661149978638 - trainLoss: 0.6454034447669983\n",
      "cnt: 0 - valLoss: 0.6434633135795593 - trainLoss: 0.645400881767273\n",
      "cnt: 0 - valLoss: 0.6434604525566101 - trainLoss: 0.6453985571861267\n",
      "cnt: 0 - valLoss: 0.6434574127197266 - trainLoss: 0.6453962922096252\n",
      "cnt: 0 - valLoss: 0.643454372882843 - trainLoss: 0.6453937888145447\n",
      "cnt: 0 - valLoss: 0.6434513926506042 - trainLoss: 0.6453912854194641\n",
      "cnt: 0 - valLoss: 0.6434487700462341 - trainLoss: 0.6453888416290283\n",
      "cnt: 0 - valLoss: 0.6434457302093506 - trainLoss: 0.6453866958618164\n",
      "cnt: 0 - valLoss: 0.643442690372467 - trainLoss: 0.6453841328620911\n",
      "cnt: 0 - valLoss: 0.6434396505355835 - trainLoss: 0.6453816890716553\n",
      "cnt: 0 - valLoss: 0.6434367895126343 - trainLoss: 0.6453792452812195\n",
      "cnt: 0 - valLoss: 0.6434339880943298 - trainLoss: 0.6453769207000732\n",
      "cnt: 0 - valLoss: 0.6434309482574463 - trainLoss: 0.645374596118927\n",
      "cnt: 0 - valLoss: 0.6434279680252075 - trainLoss: 0.6453721523284912\n",
      "cnt: 0 - valLoss: 0.643424928188324 - trainLoss: 0.6453696489334106\n",
      "cnt: 0 - valLoss: 0.6434223055839539 - trainLoss: 0.6453671455383301\n",
      "cnt: 0 - valLoss: 0.6434192657470703 - trainLoss: 0.6453649997711182\n",
      "cnt: 0 - valLoss: 0.6434162259101868 - trainLoss: 0.6453625559806824\n",
      "cnt: 0 - valLoss: 0.6434131860733032 - trainLoss: 0.645359992980957\n",
      "cnt: 0 - valLoss: 0.6434102654457092 - trainLoss: 0.6453575491905212\n",
      "cnt: 0 - valLoss: 0.6434075236320496 - trainLoss: 0.6453551650047302\n",
      "cnt: 0 - valLoss: 0.643404483795166 - trainLoss: 0.6453529000282288\n",
      "cnt: 0 - valLoss: 0.6434015035629272 - trainLoss: 0.6453503966331482\n",
      "cnt: 0 - valLoss: 0.6433984041213989 - trainLoss: 0.6453479528427124\n",
      "cnt: 0 - valLoss: 0.6433957815170288 - trainLoss: 0.6453454494476318\n",
      "cnt: 0 - valLoss: 0.6433927416801453 - trainLoss: 0.6453432440757751\n",
      "cnt: 0 - valLoss: 0.6433897614479065 - trainLoss: 0.6453407406806946\n",
      "cnt: 0 - valLoss: 0.643386721611023 - trainLoss: 0.6453382968902588\n",
      "cnt: 0 - valLoss: 0.643383800983429 - trainLoss: 0.6453357934951782\n",
      "cnt: 0 - valLoss: 0.6433810591697693 - trainLoss: 0.6453333497047424\n",
      "cnt: 0 - valLoss: 0.6433780193328857 - trainLoss: 0.6453311443328857\n",
      "cnt: 0 - valLoss: 0.6433749794960022 - trainLoss: 0.6453286409378052\n",
      "cnt: 0 - valLoss: 0.6433719396591187 - trainLoss: 0.6453261971473694\n",
      "cnt: 0 - valLoss: 0.6433692574501038 - trainLoss: 0.6453236937522888\n",
      "cnt: 0 - valLoss: 0.643366277217865 - trainLoss: 0.6453214287757874\n",
      "cnt: 0 - valLoss: 0.6433632373809814 - trainLoss: 0.6453189849853516\n",
      "cnt: 0 - valLoss: 0.6433601975440979 - trainLoss: 0.6453165411949158\n",
      "cnt: 0 - valLoss: 0.6433572769165039 - trainLoss: 0.6453140377998352\n",
      "cnt: 0 - valLoss: 0.6433545351028442 - trainLoss: 0.6453115940093994\n",
      "cnt: 0 - valLoss: 0.6433514952659607 - trainLoss: 0.6453093886375427\n",
      "cnt: 0 - valLoss: 0.6433484554290771 - trainLoss: 0.6453069448471069\n",
      "cnt: 0 - valLoss: 0.6433454155921936 - trainLoss: 0.6453043818473816\n",
      "cnt: 0 - valLoss: 0.6433427333831787 - trainLoss: 0.645301878452301\n",
      "cnt: 0 - valLoss: 0.6433396935462952 - trainLoss: 0.6452996730804443\n",
      "cnt: 0 - valLoss: 0.6433367133140564 - trainLoss: 0.6452972292900085\n",
      "cnt: 0 - valLoss: 0.6433336734771729 - trainLoss: 0.645294725894928\n",
      "cnt: 0 - valLoss: 0.6433307528495789 - trainLoss: 0.6452922821044922\n",
      "cnt: 0 - valLoss: 0.6433280110359192 - trainLoss: 0.6452898383140564\n",
      "cnt: 0 - valLoss: 0.6433249711990356 - trainLoss: 0.6452875733375549\n",
      "cnt: 0 - valLoss: 0.6433219313621521 - trainLoss: 0.6452851295471191\n",
      "cnt: 0 - valLoss: 0.6433188915252686 - trainLoss: 0.6452826261520386\n",
      "cnt: 0 - valLoss: 0.6433161497116089 - trainLoss: 0.645280122756958\n",
      "cnt: 0 - valLoss: 0.6433132290840149 - trainLoss: 0.6452778577804565\n",
      "cnt: 0 - valLoss: 0.6433101296424866 - trainLoss: 0.6452754139900208\n",
      "cnt: 0 - valLoss: 0.643307089805603 - trainLoss: 0.645272970199585\n",
      "cnt: 0 - valLoss: 0.6433041095733643 - trainLoss: 0.6452704668045044\n",
      "cnt: 0 - valLoss: 0.6433014869689941 - trainLoss: 0.6452680230140686\n",
      "cnt: 0 - valLoss: 0.643298327922821 - trainLoss: 0.6452657580375671\n",
      "cnt: 0 - valLoss: 0.6432952880859375 - trainLoss: 0.6452633142471313\n",
      "cnt: 0 - valLoss: 0.6432923078536987 - trainLoss: 0.645260751247406\n",
      "cnt: 0 - valLoss: 0.6432895660400391 - trainLoss: 0.6452583074569702\n",
      "cnt: 0 - valLoss: 0.6432866454124451 - trainLoss: 0.6452560424804688\n",
      "cnt: 0 - valLoss: 0.6432835459709167 - trainLoss: 0.6452536582946777\n",
      "cnt: 0 - valLoss: 0.643280565738678 - trainLoss: 0.6452510952949524\n",
      "cnt: 0 - valLoss: 0.6432775855064392 - trainLoss: 0.6452485918998718\n",
      "cnt: 0 - valLoss: 0.6432748436927795 - trainLoss: 0.6452462077140808\n",
      "cnt: 0 - valLoss: 0.643271803855896 - trainLoss: 0.6452439427375793\n",
      "cnt: 0 - valLoss: 0.6432687640190125 - trainLoss: 0.6452414393424988\n",
      "cnt: 0 - valLoss: 0.6432657241821289 - trainLoss: 0.6452389359474182\n",
      "cnt: 0 - valLoss: 0.643263041973114 - trainLoss: 0.6452364325523376\n",
      "cnt: 0 - valLoss: 0.6432600021362305 - trainLoss: 0.645234227180481\n",
      "cnt: 0 - valLoss: 0.6432569622993469 - trainLoss: 0.6452317833900452\n",
      "cnt: 0 - valLoss: 0.6432539224624634 - trainLoss: 0.6452292799949646\n",
      "cnt: 0 - valLoss: 0.6432510018348694 - trainLoss: 0.645226776599884\n",
      "cnt: 0 - valLoss: 0.6432482004165649 - trainLoss: 0.645224392414093\n",
      "cnt: 0 - valLoss: 0.6432451605796814 - trainLoss: 0.6452220678329468\n",
      "cnt: 0 - valLoss: 0.6432421207427979 - trainLoss: 0.6452195644378662\n",
      "cnt: 0 - valLoss: 0.6432390809059143 - trainLoss: 0.6452170610427856\n",
      "cnt: 0 - valLoss: 0.6432363986968994 - trainLoss: 0.6452145576477051\n",
      "cnt: 0 - valLoss: 0.6432334184646606 - trainLoss: 0.6452122926712036\n",
      "cnt: 0 - valLoss: 0.6432303190231323 - trainLoss: 0.6452099084854126\n",
      "cnt: 0 - valLoss: 0.6432272791862488 - trainLoss: 0.645207405090332\n",
      "cnt: 0 - valLoss: 0.6432244181632996 - trainLoss: 0.6452049016952515\n",
      "cnt: 0 - valLoss: 0.6432216167449951 - trainLoss: 0.6452025175094604\n",
      "cnt: 0 - valLoss: 0.6432185769081116 - trainLoss: 0.6452001929283142\n",
      "cnt: 0 - valLoss: 0.6432154774665833 - trainLoss: 0.6451976895332336\n",
      "cnt: 0 - valLoss: 0.6432123780250549 - trainLoss: 0.6451951861381531\n",
      "cnt: 0 - valLoss: 0.6432097554206848 - trainLoss: 0.6451926827430725\n",
      "cnt: 0 - valLoss: 0.6432067155838013 - trainLoss: 0.6451904773712158\n",
      "cnt: 0 - valLoss: 0.6432036757469177 - trainLoss: 0.6451879739761353\n",
      "cnt: 0 - valLoss: 0.6432006359100342 - trainLoss: 0.6451855301856995\n",
      "cnt: 0 - valLoss: 0.6431977152824402 - trainLoss: 0.6451829671859741\n",
      "cnt: 0 - valLoss: 0.6431949138641357 - trainLoss: 0.6451805830001831\n",
      "cnt: 0 - valLoss: 0.6431918740272522 - trainLoss: 0.6451782584190369\n",
      "cnt: 0 - valLoss: 0.6431888341903687 - trainLoss: 0.6451757550239563\n",
      "cnt: 0 - valLoss: 0.6431857943534851 - trainLoss: 0.6451732516288757\n",
      "cnt: 0 - valLoss: 0.6431831121444702 - trainLoss: 0.6451708078384399\n",
      "cnt: 0 - valLoss: 0.6431801319122314 - trainLoss: 0.6451686024665833\n",
      "cnt: 0 - valLoss: 0.6431769728660583 - trainLoss: 0.6451660990715027\n",
      "cnt: 0 - valLoss: 0.6431739330291748 - trainLoss: 0.6451635360717773\n",
      "cnt: 0 - valLoss: 0.6431710720062256 - trainLoss: 0.6451610922813416\n",
      "cnt: 0 - valLoss: 0.6431682705879211 - trainLoss: 0.6451587080955505\n",
      "cnt: 0 - valLoss: 0.6431651711463928 - trainLoss: 0.6451563239097595\n",
      "cnt: 0 - valLoss: 0.6431621313095093 - trainLoss: 0.645153820514679\n",
      "cnt: 0 - valLoss: 0.643159031867981 - trainLoss: 0.6451513767242432\n",
      "cnt: 0 - valLoss: 0.6431564688682556 - trainLoss: 0.6451488137245178\n",
      "cnt: 0 - valLoss: 0.6431533098220825 - trainLoss: 0.6451466679573059\n",
      "cnt: 0 - valLoss: 0.643150269985199 - trainLoss: 0.6451441049575806\n",
      "cnt: 0 - valLoss: 0.6431472301483154 - trainLoss: 0.6451416611671448\n",
      "cnt: 0 - valLoss: 0.643144428730011 - trainLoss: 0.6451390981674194\n",
      "cnt: 0 - valLoss: 0.6431415677070618 - trainLoss: 0.645136833190918\n",
      "cnt: 0 - valLoss: 0.6431385278701782 - trainLoss: 0.6451343894004822\n",
      "cnt: 0 - valLoss: 0.6431354284286499 - trainLoss: 0.6451319456100464\n",
      "cnt: 0 - valLoss: 0.6431323885917664 - trainLoss: 0.6451294422149658\n",
      "cnt: 0 - valLoss: 0.6431297063827515 - trainLoss: 0.6451269388198853\n",
      "cnt: 0 - valLoss: 0.6431266665458679 - trainLoss: 0.6451246738433838\n",
      "cnt: 0 - valLoss: 0.6431235671043396 - trainLoss: 0.6451221704483032\n",
      "cnt: 0 - valLoss: 0.643120527267456 - trainLoss: 0.6451196670532227\n",
      "cnt: 0 - valLoss: 0.6431177854537964 - trainLoss: 0.6451171636581421\n",
      "cnt: 0 - valLoss: 0.6431148052215576 - trainLoss: 0.6451148390769958\n",
      "cnt: 0 - valLoss: 0.6431117653846741 - trainLoss: 0.6451123952865601\n",
      "cnt: 0 - valLoss: 0.6431086659431458 - trainLoss: 0.6451099514961243\n",
      "cnt: 0 - valLoss: 0.643105685710907 - trainLoss: 0.6451073884963989\n",
      "cnt: 0 - valLoss: 0.6431029438972473 - trainLoss: 0.6451049447059631\n",
      "cnt: 0 - valLoss: 0.6430999040603638 - trainLoss: 0.6451026797294617\n",
      "cnt: 0 - valLoss: 0.6430968046188354 - trainLoss: 0.6451002359390259\n",
      "cnt: 0 - valLoss: 0.6430937647819519 - trainLoss: 0.6450976729393005\n",
      "cnt: 0 - valLoss: 0.6430910229682922 - trainLoss: 0.6450952291488647\n",
      "cnt: 0 - valLoss: 0.6430880427360535 - trainLoss: 0.6450929045677185\n",
      "cnt: 0 - valLoss: 0.6430850028991699 - trainLoss: 0.6450904607772827\n",
      "cnt: 0 - valLoss: 0.6430819034576416 - trainLoss: 0.6450879573822021\n",
      "cnt: 0 - valLoss: 0.6430789828300476 - trainLoss: 0.6450853943824768\n",
      "cnt: 0 - valLoss: 0.6430761814117432 - trainLoss: 0.6450830101966858\n",
      "cnt: 0 - valLoss: 0.6430731415748596 - trainLoss: 0.6450806260108948\n",
      "cnt: 0 - valLoss: 0.6430701017379761 - trainLoss: 0.645078182220459\n",
      "cnt: 0 - valLoss: 0.643066942691803 - trainLoss: 0.6450756788253784\n",
      "cnt: 0 - valLoss: 0.6430643200874329 - trainLoss: 0.6450731754302979\n",
      "cnt: 0 - valLoss: 0.6430612802505493 - trainLoss: 0.6450709104537964\n",
      "cnt: 0 - valLoss: 0.643058180809021 - trainLoss: 0.6450684070587158\n",
      "cnt: 0 - valLoss: 0.6430551409721375 - trainLoss: 0.6450659036636353\n",
      "cnt: 0 - valLoss: 0.6430522799491882 - trainLoss: 0.6450633406639099\n",
      "cnt: 0 - valLoss: 0.643049418926239 - trainLoss: 0.6450610756874084\n",
      "cnt: 0 - valLoss: 0.6430463194847107 - trainLoss: 0.6450586318969727\n",
      "cnt: 0 - valLoss: 0.6430432200431824 - trainLoss: 0.6450561285018921\n",
      "cnt: 0 - valLoss: 0.6430401802062988 - trainLoss: 0.6450536251068115\n",
      "cnt: 0 - valLoss: 0.6430374979972839 - trainLoss: 0.645051121711731\n",
      "cnt: 0 - valLoss: 0.6430344581604004 - trainLoss: 0.6450489163398743\n",
      "cnt: 0 - valLoss: 0.6430313587188721 - trainLoss: 0.6450463533401489\n",
      "cnt: 0 - valLoss: 0.6430283188819885 - trainLoss: 0.6450438499450684\n",
      "cnt: 0 - valLoss: 0.6430255174636841 - trainLoss: 0.6450413465499878\n",
      "cnt: 0 - valLoss: 0.6430225372314453 - trainLoss: 0.6450390219688416\n",
      "cnt: 0 - valLoss: 0.6430194973945618 - trainLoss: 0.6450365781784058\n",
      "cnt: 0 - valLoss: 0.6430164575576782 - trainLoss: 0.6450340747833252\n",
      "cnt: 0 - valLoss: 0.6430134177207947 - trainLoss: 0.6450315117835999\n",
      "cnt: 0 - valLoss: 0.6430107355117798 - trainLoss: 0.6450291275978088\n",
      "cnt: 0 - valLoss: 0.6430076360702515 - trainLoss: 0.6450268626213074\n",
      "cnt: 0 - valLoss: 0.6430045366287231 - trainLoss: 0.645024299621582\n",
      "cnt: 0 - valLoss: 0.6430014371871948 - trainLoss: 0.6450217366218567\n",
      "cnt: 0 - valLoss: 0.6429987549781799 - trainLoss: 0.6450191736221313\n",
      "cnt: 0 - valLoss: 0.6429957151412964 - trainLoss: 0.6450169682502747\n",
      "cnt: 0 - valLoss: 0.6429926753044128 - trainLoss: 0.6450144648551941\n",
      "cnt: 0 - valLoss: 0.6429895758628845 - trainLoss: 0.6450120210647583\n",
      "cnt: 0 - valLoss: 0.6429866552352905 - trainLoss: 0.645009458065033\n",
      "cnt: 0 - valLoss: 0.6429838538169861 - trainLoss: 0.6450070738792419\n",
      "cnt: 0 - valLoss: 0.6429807543754578 - trainLoss: 0.6450046896934509\n",
      "cnt: 0 - valLoss: 0.6429777145385742 - trainLoss: 0.6450021266937256\n",
      "cnt: 0 - valLoss: 0.6429746150970459 - trainLoss: 0.6449996829032898\n",
      "cnt: 0 - valLoss: 0.6429719924926758 - trainLoss: 0.6449971199035645\n",
      "cnt: 0 - valLoss: 0.6429688334465027 - trainLoss: 0.6449949145317078\n",
      "cnt: 0 - valLoss: 0.6429657936096191 - trainLoss: 0.6449924111366272\n",
      "cnt: 0 - valLoss: 0.6429627537727356 - trainLoss: 0.6449898481369019\n",
      "cnt: 0 - valLoss: 0.6429599523544312 - trainLoss: 0.6449873447418213\n",
      "cnt: 0 - valLoss: 0.6429569721221924 - trainLoss: 0.644985020160675\n",
      "cnt: 0 - valLoss: 0.6429539322853088 - trainLoss: 0.6449825763702393\n",
      "cnt: 0 - valLoss: 0.6429508328437805 - trainLoss: 0.6449800729751587\n",
      "cnt: 0 - valLoss: 0.6429478526115417 - trainLoss: 0.6449775695800781\n",
      "cnt: 0 - valLoss: 0.6429451107978821 - trainLoss: 0.6449751257896423\n",
      "cnt: 0 - valLoss: 0.6429420113563538 - trainLoss: 0.6449728012084961\n",
      "cnt: 0 - valLoss: 0.6429389119148254 - trainLoss: 0.6449702978134155\n",
      "cnt: 0 - valLoss: 0.6429358720779419 - trainLoss: 0.6449677348136902\n",
      "cnt: 0 - valLoss: 0.6429331302642822 - trainLoss: 0.6449652314186096\n",
      "cnt: 0 - valLoss: 0.6429300904273987 - trainLoss: 0.6449629664421082\n",
      "cnt: 0 - valLoss: 0.6429269909858704 - trainLoss: 0.6449604630470276\n",
      "cnt: 0 - valLoss: 0.6429239511489868 - trainLoss: 0.6449579000473022\n",
      "cnt: 0 - valLoss: 0.6429210901260376 - trainLoss: 0.6449553966522217\n",
      "cnt: 0 - valLoss: 0.6429181694984436 - trainLoss: 0.6449530124664307\n",
      "cnt: 0 - valLoss: 0.6429151296615601 - trainLoss: 0.6449506878852844\n",
      "cnt: 0 - valLoss: 0.6429119110107422 - trainLoss: 0.6449481248855591\n",
      "cnt: 0 - valLoss: 0.6429089307785034 - trainLoss: 0.6449455618858337\n",
      "cnt: 0 - valLoss: 0.6429062485694885 - trainLoss: 0.6449430584907532\n",
      "cnt: 0 - valLoss: 0.6429031491279602 - trainLoss: 0.6449407935142517\n",
      "cnt: 0 - valLoss: 0.6429000496864319 - trainLoss: 0.6449382901191711\n",
      "cnt: 0 - valLoss: 0.6428969502449036 - trainLoss: 0.6449357271194458\n",
      "cnt: 0 - valLoss: 0.6428942084312439 - trainLoss: 0.6449332237243652\n",
      "cnt: 0 - valLoss: 0.6428912878036499 - trainLoss: 0.644930899143219\n",
      "cnt: 0 - valLoss: 0.6428881287574768 - trainLoss: 0.6449284553527832\n",
      "cnt: 0 - valLoss: 0.6428850293159485 - trainLoss: 0.6449259519577026\n",
      "cnt: 0 - valLoss: 0.6428821682929993 - trainLoss: 0.6449233889579773\n",
      "cnt: 0 - valLoss: 0.6428792476654053 - trainLoss: 0.6449209451675415\n",
      "cnt: 0 - valLoss: 0.6428762674331665 - trainLoss: 0.6449186205863953\n",
      "cnt: 0 - valLoss: 0.6428731083869934 - trainLoss: 0.6449161171913147\n",
      "cnt: 0 - valLoss: 0.6428700089454651 - trainLoss: 0.6449135541915894\n",
      "cnt: 0 - valLoss: 0.6428674459457397 - trainLoss: 0.6449110507965088\n",
      "cnt: 0 - valLoss: 0.6428643465042114 - trainLoss: 0.6449088454246521\n",
      "cnt: 0 - valLoss: 0.6428611874580383 - trainLoss: 0.6449062824249268\n",
      "cnt: 0 - valLoss: 0.64285808801651 - trainLoss: 0.6449037194252014\n",
      "cnt: 0 - valLoss: 0.6428552865982056 - trainLoss: 0.6449011564254761\n",
      "cnt: 0 - valLoss: 0.6428523659706116 - trainLoss: 0.6448987722396851\n",
      "cnt: 0 - valLoss: 0.6428492069244385 - trainLoss: 0.6448964476585388\n",
      "cnt: 0 - valLoss: 0.6428461670875549 - trainLoss: 0.6448938846588135\n",
      "cnt: 0 - valLoss: 0.6428431868553162 - trainLoss: 0.6448913216590881\n",
      "cnt: 0 - valLoss: 0.6428403854370117 - trainLoss: 0.6448888778686523\n",
      "cnt: 0 - valLoss: 0.6428372859954834 - trainLoss: 0.6448866128921509\n",
      "cnt: 0 - valLoss: 0.6428341865539551 - trainLoss: 0.6448840498924255\n",
      "cnt: 0 - valLoss: 0.6428310871124268 - trainLoss: 0.644881546497345\n",
      "cnt: 0 - valLoss: 0.6428284645080566 - trainLoss: 0.6448789834976196\n",
      "cnt: 0 - valLoss: 0.6428253054618835 - trainLoss: 0.6448767185211182\n",
      "cnt: 0 - valLoss: 0.642822265625 - trainLoss: 0.6448741555213928\n",
      "cnt: 0 - valLoss: 0.6428191661834717 - trainLoss: 0.6448716521263123\n",
      "cnt: 0 - valLoss: 0.6428162455558777 - trainLoss: 0.6448690295219421\n",
      "cnt: 0 - valLoss: 0.6428133845329285 - trainLoss: 0.6448667049407959\n",
      "cnt: 0 - valLoss: 0.6428102850914001 - trainLoss: 0.6448643207550049\n",
      "cnt: 0 - valLoss: 0.6428071856498718 - trainLoss: 0.6448618173599243\n",
      "cnt: 0 - valLoss: 0.6428042054176331 - trainLoss: 0.644859254360199\n",
      "cnt: 0 - valLoss: 0.6428014636039734 - trainLoss: 0.6448567509651184\n",
      "cnt: 0 - valLoss: 0.6427983641624451 - trainLoss: 0.6448544859886169\n",
      "cnt: 0 - valLoss: 0.6427952647209167 - trainLoss: 0.6448519229888916\n",
      "cnt: 0 - valLoss: 0.6427921056747437 - trainLoss: 0.6448493599891663\n",
      "cnt: 0 - valLoss: 0.6427894234657288 - trainLoss: 0.6448467969894409\n",
      "cnt: 0 - valLoss: 0.6427863240242004 - trainLoss: 0.6448445320129395\n",
      "cnt: 0 - valLoss: 0.6427832841873169 - trainLoss: 0.6448420286178589\n",
      "cnt: 0 - valLoss: 0.6427801251411438 - trainLoss: 0.6448395252227783\n",
      "cnt: 0 - valLoss: 0.6427772641181946 - trainLoss: 0.644836962223053\n",
      "cnt: 0 - valLoss: 0.6427743434906006 - trainLoss: 0.644834578037262\n",
      "cnt: 0 - valLoss: 0.6427712440490723 - trainLoss: 0.6448321342468262\n",
      "cnt: 0 - valLoss: 0.642768144607544 - trainLoss: 0.6448296904563904\n",
      "cnt: 0 - valLoss: 0.6427651047706604 - trainLoss: 0.6448270678520203\n",
      "cnt: 0 - valLoss: 0.6427623629570007 - trainLoss: 0.6448245048522949\n",
      "cnt: 0 - valLoss: 0.6427592635154724 - trainLoss: 0.6448222994804382\n",
      "cnt: 0 - valLoss: 0.6427561640739441 - trainLoss: 0.6448197960853577\n",
      "cnt: 0 - valLoss: 0.6427530646324158 - trainLoss: 0.6448172330856323\n",
      "cnt: 0 - valLoss: 0.6427503228187561 - trainLoss: 0.644814670085907\n",
      "cnt: 0 - valLoss: 0.6427472829818726 - trainLoss: 0.6448123455047607\n",
      "cnt: 0 - valLoss: 0.6427441835403442 - trainLoss: 0.644809901714325\n",
      "cnt: 0 - valLoss: 0.6427410840988159 - trainLoss: 0.6448073387145996\n",
      "cnt: 0 - valLoss: 0.6427381634712219 - trainLoss: 0.6448047757148743\n",
      "cnt: 0 - valLoss: 0.6427353024482727 - trainLoss: 0.6448023319244385\n",
      "cnt: 0 - valLoss: 0.6427322030067444 - trainLoss: 0.6447999477386475\n",
      "cnt: 0 - valLoss: 0.6427291035652161 - trainLoss: 0.6447973847389221\n",
      "cnt: 0 - valLoss: 0.6427260637283325 - trainLoss: 0.6447948813438416\n",
      "cnt: 0 - valLoss: 0.6427233219146729 - trainLoss: 0.644792377948761\n",
      "cnt: 0 - valLoss: 0.6427201628684998 - trainLoss: 0.6447901129722595\n",
      "cnt: 0 - valLoss: 0.6427170634269714 - trainLoss: 0.6447875499725342\n",
      "cnt: 0 - valLoss: 0.6427140235900879 - trainLoss: 0.6447849869728088\n",
      "cnt: 0 - valLoss: 0.6427112817764282 - trainLoss: 0.6447824239730835\n",
      "cnt: 0 - valLoss: 0.6427082419395447 - trainLoss: 0.644780158996582\n",
      "cnt: 0 - valLoss: 0.6427051424980164 - trainLoss: 0.6447776556015015\n",
      "cnt: 0 - valLoss: 0.6427019834518433 - trainLoss: 0.6447750926017761\n",
      "cnt: 0 - valLoss: 0.6426990628242493 - trainLoss: 0.6447725892066956\n",
      "cnt: 0 - valLoss: 0.6426962018013 - trainLoss: 0.6447701454162598\n",
      "cnt: 0 - valLoss: 0.6426931023597717 - trainLoss: 0.644767701625824\n",
      "cnt: 0 - valLoss: 0.6426900029182434 - trainLoss: 0.6447651982307434\n",
      "cnt: 0 - valLoss: 0.6426869630813599 - trainLoss: 0.6447626352310181\n",
      "cnt: 0 - valLoss: 0.6426842212677002 - trainLoss: 0.6447601318359375\n",
      "cnt: 0 - valLoss: 0.6426810622215271 - trainLoss: 0.6447578072547913\n",
      "cnt: 0 - valLoss: 0.6426780223846436 - trainLoss: 0.6447552442550659\n",
      "cnt: 0 - valLoss: 0.6426749229431152 - trainLoss: 0.6447526812553406\n",
      "cnt: 0 - valLoss: 0.6426721811294556 - trainLoss: 0.6447501182556152\n",
      "cnt: 0 - valLoss: 0.6426690816879272 - trainLoss: 0.6447478532791138\n",
      "cnt: 0 - valLoss: 0.6426659822463989 - trainLoss: 0.6447453498840332\n",
      "cnt: 0 - valLoss: 0.6426628828048706 - trainLoss: 0.6447428464889526\n",
      "cnt: 0 - valLoss: 0.6426599621772766 - trainLoss: 0.6447402238845825\n",
      "cnt: 0 - valLoss: 0.6426571011543274 - trainLoss: 0.6447378396987915\n",
      "cnt: 0 - valLoss: 0.6426539421081543 - trainLoss: 0.6447354555130005\n",
      "cnt: 0 - valLoss: 0.642650842666626 - trainLoss: 0.6447329521179199\n",
      "cnt: 0 - valLoss: 0.6426477432250977 - trainLoss: 0.6447303295135498\n",
      "cnt: 0 - valLoss: 0.6426450610160828 - trainLoss: 0.6447278261184692\n",
      "cnt: 0 - valLoss: 0.6426419019699097 - trainLoss: 0.644725501537323\n",
      "cnt: 0 - valLoss: 0.6426388025283813 - trainLoss: 0.6447229981422424\n",
      "cnt: 0 - valLoss: 0.642635703086853 - trainLoss: 0.6447203755378723\n",
      "cnt: 0 - valLoss: 0.6426329612731934 - trainLoss: 0.644717812538147\n",
      "cnt: 0 - valLoss: 0.6426299214363098 - trainLoss: 0.6447155475616455\n",
      "cnt: 0 - valLoss: 0.6426268219947815 - trainLoss: 0.6447130441665649\n",
      "cnt: 0 - valLoss: 0.6426236629486084 - trainLoss: 0.6447104811668396\n",
      "cnt: 0 - valLoss: 0.6426208019256592 - trainLoss: 0.6447079181671143\n",
      "cnt: 0 - valLoss: 0.6426178812980652 - trainLoss: 0.6447055339813232\n",
      "cnt: 0 - valLoss: 0.6426147222518921 - trainLoss: 0.6447030901908875\n",
      "cnt: 0 - valLoss: 0.642611563205719 - trainLoss: 0.6447005271911621\n",
      "cnt: 0 - valLoss: 0.6426085829734802 - trainLoss: 0.6446979641914368\n",
      "cnt: 0 - valLoss: 0.642605721950531 - trainLoss: 0.644695520401001\n",
      "cnt: 0 - valLoss: 0.6426027417182922 - trainLoss: 0.64469313621521\n",
      "cnt: 0 - valLoss: 0.6425995826721191 - trainLoss: 0.6446905732154846\n",
      "cnt: 0 - valLoss: 0.6425965428352356 - trainLoss: 0.6446880102157593\n",
      "cnt: 0 - valLoss: 0.6425939202308655 - trainLoss: 0.6446854472160339\n",
      "cnt: 0 - valLoss: 0.6425907611846924 - trainLoss: 0.6446831822395325\n",
      "cnt: 0 - valLoss: 0.6425876617431641 - trainLoss: 0.6446806192398071\n",
      "cnt: 0 - valLoss: 0.6425845623016357 - trainLoss: 0.6446780562400818\n",
      "cnt: 0 - valLoss: 0.6425818204879761 - trainLoss: 0.6446755528450012\n",
      "cnt: 0 - valLoss: 0.6425788402557373 - trainLoss: 0.6446731686592102\n",
      "cnt: 0 - valLoss: 0.642575740814209 - trainLoss: 0.6446707248687744\n",
      "cnt: 0 - valLoss: 0.6425725817680359 - trainLoss: 0.6446681618690491\n",
      "cnt: 0 - valLoss: 0.6425696611404419 - trainLoss: 0.644665539264679\n",
      "cnt: 0 - valLoss: 0.6425668001174927 - trainLoss: 0.6446630954742432\n",
      "cnt: 0 - valLoss: 0.6425637602806091 - trainLoss: 0.6446607708930969\n",
      "cnt: 0 - valLoss: 0.6425606608390808 - trainLoss: 0.6446582674980164\n",
      "cnt: 0 - valLoss: 0.6425575613975525 - trainLoss: 0.6446556448936462\n",
      "cnt: 0 - valLoss: 0.6425548791885376 - trainLoss: 0.6446530818939209\n",
      "cnt: 0 - valLoss: 0.6425517797470093 - trainLoss: 0.6446508169174194\n",
      "cnt: 0 - valLoss: 0.6425487399101257 - trainLoss: 0.6446482539176941\n",
      "cnt: 0 - valLoss: 0.6425455212593079 - trainLoss: 0.6446456909179688\n",
      "cnt: 0 - valLoss: 0.6425428986549377 - trainLoss: 0.6446431279182434\n",
      "cnt: 0 - valLoss: 0.6425397992134094 - trainLoss: 0.6446408033370972\n",
      "cnt: 0 - valLoss: 0.6425366997718811 - trainLoss: 0.6446383595466614\n",
      "cnt: 0 - valLoss: 0.6425336599349976 - trainLoss: 0.644635796546936\n",
      "cnt: 0 - valLoss: 0.6425307393074036 - trainLoss: 0.6446331739425659\n",
      "cnt: 0 - valLoss: 0.6425278186798096 - trainLoss: 0.6446307301521301\n",
      "cnt: 0 - valLoss: 0.642524778842926 - trainLoss: 0.6446283459663391\n",
      "cnt: 0 - valLoss: 0.6425216197967529 - trainLoss: 0.644625723361969\n",
      "cnt: 0 - valLoss: 0.6425186395645142 - trainLoss: 0.6446232199668884\n",
      "cnt: 0 - valLoss: 0.6425158381462097 - trainLoss: 0.6446207165718079\n",
      "cnt: 0 - valLoss: 0.6425127387046814 - trainLoss: 0.6446183919906616\n",
      "cnt: 0 - valLoss: 0.6425096392631531 - trainLoss: 0.6446157693862915\n",
      "cnt: 0 - valLoss: 0.64250648021698 - trainLoss: 0.6446131467819214\n",
      "cnt: 0 - valLoss: 0.6425039172172546 - trainLoss: 0.644610583782196\n",
      "cnt: 0 - valLoss: 0.6425007581710815 - trainLoss: 0.6446084380149841\n",
      "cnt: 0 - valLoss: 0.6424976587295532 - trainLoss: 0.6446058750152588\n",
      "cnt: 0 - valLoss: 0.6424945592880249 - trainLoss: 0.6446031928062439\n",
      "cnt: 0 - valLoss: 0.6424917578697205 - trainLoss: 0.6446006894111633\n",
      "cnt: 0 - valLoss: 0.6424887776374817 - trainLoss: 0.6445983052253723\n",
      "cnt: 0 - valLoss: 0.6424857378005981 - trainLoss: 0.6445958614349365\n",
      "cnt: 0 - valLoss: 0.6424825191497803 - trainLoss: 0.6445932984352112\n",
      "cnt: 0 - valLoss: 0.642479658126831 - trainLoss: 0.6445906758308411\n",
      "cnt: 0 - valLoss: 0.6424768567085266 - trainLoss: 0.6445882320404053\n",
      "cnt: 0 - valLoss: 0.6424736976623535 - trainLoss: 0.6445858478546143\n",
      "cnt: 0 - valLoss: 0.6424705386161804 - trainLoss: 0.6445832848548889\n",
      "cnt: 0 - valLoss: 0.6424674987792969 - trainLoss: 0.6445806622505188\n",
      "cnt: 0 - valLoss: 0.6424647569656372 - trainLoss: 0.6445781588554382\n",
      "cnt: 0 - valLoss: 0.6424617171287537 - trainLoss: 0.6445758938789368\n",
      "cnt: 0 - valLoss: 0.6424585580825806 - trainLoss: 0.6445732712745667\n",
      "cnt: 0 - valLoss: 0.6424554586410522 - trainLoss: 0.6445707082748413\n",
      "cnt: 0 - valLoss: 0.6424527764320374 - trainLoss: 0.6445680856704712\n",
      "cnt: 0 - valLoss: 0.6424497365951538 - trainLoss: 0.6445658206939697\n",
      "cnt: 0 - valLoss: 0.6424465775489807 - trainLoss: 0.6445633172988892\n",
      "cnt: 0 - valLoss: 0.6424434781074524 - trainLoss: 0.6445607542991638\n",
      "cnt: 0 - valLoss: 0.6424406170845032 - trainLoss: 0.6445580720901489\n",
      "cnt: 0 - valLoss: 0.6424376368522644 - trainLoss: 0.6445557475090027\n",
      "cnt: 0 - valLoss: 0.6424345374107361 - trainLoss: 0.6445532441139221\n",
      "cnt: 0 - valLoss: 0.642431378364563 - trainLoss: 0.6445506811141968\n",
      "cnt: 0 - valLoss: 0.642428457736969 - trainLoss: 0.6445481181144714\n",
      "cnt: 0 - valLoss: 0.6424256563186646 - trainLoss: 0.6445456147193909\n",
      "cnt: 0 - valLoss: 0.6424225568771362 - trainLoss: 0.6445433497428894\n",
      "cnt: 0 - valLoss: 0.6424193978309631 - trainLoss: 0.6445407271385193\n",
      "cnt: 0 - valLoss: 0.6424163579940796 - trainLoss: 0.644538164138794\n",
      "cnt: 0 - valLoss: 0.6424136161804199 - trainLoss: 0.6445356011390686\n",
      "cnt: 0 - valLoss: 0.6424105167388916 - trainLoss: 0.6445332765579224\n",
      "cnt: 0 - valLoss: 0.6424073576927185 - trainLoss: 0.6445306539535522\n",
      "cnt: 0 - valLoss: 0.6424042582511902 - trainLoss: 0.6445280909538269\n",
      "cnt: 0 - valLoss: 0.6424016356468201 - trainLoss: 0.6445255279541016\n",
      "cnt: 0 - valLoss: 0.642398476600647 - trainLoss: 0.6445232629776001\n",
      "cnt: 0 - valLoss: 0.6423953771591187 - trainLoss: 0.6445206999778748\n",
      "cnt: 0 - valLoss: 0.6423922181129456 - trainLoss: 0.6445181369781494\n",
      "cnt: 0 - valLoss: 0.6423894762992859 - trainLoss: 0.6445155143737793\n",
      "cnt: 0 - valLoss: 0.6423864364624023 - trainLoss: 0.6445131301879883\n",
      "cnt: 0 - valLoss: 0.642383337020874 - trainLoss: 0.6445106863975525\n",
      "cnt: 0 - valLoss: 0.6423801779747009 - trainLoss: 0.6445080637931824\n",
      "cnt: 0 - valLoss: 0.6423772573471069 - trainLoss: 0.644505500793457\n",
      "cnt: 0 - valLoss: 0.6423744559288025 - trainLoss: 0.6445030570030212\n",
      "cnt: 0 - valLoss: 0.6423712968826294 - trainLoss: 0.6445006728172302\n",
      "cnt: 0 - valLoss: 0.6423681974411011 - trainLoss: 0.6444980502128601\n",
      "cnt: 0 - valLoss: 0.6423651576042175 - trainLoss: 0.6444954872131348\n",
      "cnt: 0 - valLoss: 0.6423624157905579 - trainLoss: 0.6444929242134094\n",
      "cnt: 0 - valLoss: 0.6423592567443848 - trainLoss: 0.644490659236908\n",
      "cnt: 0 - valLoss: 0.6423561573028564 - trainLoss: 0.6444880366325378\n",
      "cnt: 0 - valLoss: 0.6423529982566833 - trainLoss: 0.6444854140281677\n",
      "cnt: 0 - valLoss: 0.6423503756523132 - trainLoss: 0.6444828510284424\n",
      "cnt: 0 - valLoss: 0.6423472166061401 - trainLoss: 0.6444805264472961\n",
      "cnt: 0 - valLoss: 0.642344057559967 - trainLoss: 0.6444780230522156\n",
      "cnt: 0 - valLoss: 0.642340898513794 - trainLoss: 0.6444754004478455\n",
      "cnt: 0 - valLoss: 0.6423381567001343 - trainLoss: 0.6444727778434753\n",
      "cnt: 0 - valLoss: 0.6423351168632507 - trainLoss: 0.6444704532623291\n",
      "cnt: 0 - valLoss: 0.6423320174217224 - trainLoss: 0.6444679498672485\n",
      "cnt: 0 - valLoss: 0.6423289179801941 - trainLoss: 0.6444653868675232\n",
      "cnt: 0 - valLoss: 0.6423259973526001 - trainLoss: 0.6444627642631531\n",
      "cnt: 0 - valLoss: 0.6423230767250061 - trainLoss: 0.6444603204727173\n",
      "cnt: 0 - valLoss: 0.6423200368881226 - trainLoss: 0.6444578766822815\n",
      "cnt: 0 - valLoss: 0.6423168778419495 - trainLoss: 0.6444553136825562\n",
      "cnt: 0 - valLoss: 0.6423138380050659 - trainLoss: 0.644452691078186\n",
      "cnt: 0 - valLoss: 0.6423110365867615 - trainLoss: 0.6444501876831055\n",
      "cnt: 0 - valLoss: 0.6423079371452332 - trainLoss: 0.6444478631019592\n",
      "cnt: 0 - valLoss: 0.6423047780990601 - trainLoss: 0.6444452404975891\n",
      "cnt: 0 - valLoss: 0.642301619052887 - trainLoss: 0.6444426774978638\n",
      "cnt: 0 - valLoss: 0.6422990560531616 - trainLoss: 0.6444400548934937\n",
      "cnt: 0 - valLoss: 0.642295777797699 - trainLoss: 0.644437849521637\n",
      "cnt: 0 - valLoss: 0.6422926783561707 - trainLoss: 0.6444351673126221\n",
      "cnt: 0 - valLoss: 0.6422895789146423 - trainLoss: 0.644432544708252\n",
      "cnt: 0 - valLoss: 0.6422868371009827 - trainLoss: 0.6444300413131714\n",
      "cnt: 0 - valLoss: 0.6422838568687439 - trainLoss: 0.6444276571273804\n",
      "cnt: 0 - valLoss: 0.642280638217926 - trainLoss: 0.644425094127655\n",
      "cnt: 0 - valLoss: 0.6422775387763977 - trainLoss: 0.6444225311279297\n",
      "cnt: 0 - valLoss: 0.6422746777534485 - trainLoss: 0.6444199085235596\n",
      "cnt: 0 - valLoss: 0.6422716975212097 - trainLoss: 0.6444175839424133\n",
      "cnt: 0 - valLoss: 0.6422685980796814 - trainLoss: 0.644415020942688\n",
      "cnt: 0 - valLoss: 0.6422653794288635 - trainLoss: 0.6444124579429626\n",
      "cnt: 0 - valLoss: 0.6422625184059143 - trainLoss: 0.6444098353385925\n",
      "cnt: 0 - valLoss: 0.6422596573829651 - trainLoss: 0.6444073915481567\n",
      "cnt: 0 - valLoss: 0.6422565579414368 - trainLoss: 0.6444050073623657\n",
      "cnt: 0 - valLoss: 0.6422533392906189 - trainLoss: 0.6444023847579956\n",
      "cnt: 0 - valLoss: 0.6422502994537354 - trainLoss: 0.6443997621536255\n",
      "cnt: 0 - valLoss: 0.6422475576400757 - trainLoss: 0.6443971991539001\n",
      "cnt: 0 - valLoss: 0.6422443985939026 - trainLoss: 0.6443949341773987\n",
      "cnt: 0 - valLoss: 0.6422412395477295 - trainLoss: 0.6443922519683838\n",
      "cnt: 0 - valLoss: 0.6422381401062012 - trainLoss: 0.6443896889686584\n",
      "cnt: 0 - valLoss: 0.6422354578971863 - trainLoss: 0.6443870663642883\n",
      "cnt: 0 - valLoss: 0.6422322988510132 - trainLoss: 0.6443848013877869\n",
      "cnt: 0 - valLoss: 0.6422291994094849 - trainLoss: 0.6443822383880615\n",
      "cnt: 0 - valLoss: 0.6422260403633118 - trainLoss: 0.6443796753883362\n",
      "cnt: 0 - valLoss: 0.6422232985496521 - trainLoss: 0.6443769931793213\n",
      "cnt: 0 - valLoss: 0.6422202587127686 - trainLoss: 0.644374668598175\n",
      "cnt: 0 - valLoss: 0.6422170400619507 - trainLoss: 0.6443721652030945\n",
      "cnt: 0 - valLoss: 0.6422139406204224 - trainLoss: 0.6443695425987244\n",
      "cnt: 0 - valLoss: 0.6422110795974731 - trainLoss: 0.6443669199943542\n",
      "cnt: 0 - valLoss: 0.6422081589698792 - trainLoss: 0.6443645358085632\n",
      "cnt: 0 - valLoss: 0.642204999923706 - trainLoss: 0.6443620324134827\n",
      "cnt: 0 - valLoss: 0.642201840877533 - trainLoss: 0.6443594694137573\n",
      "cnt: 0 - valLoss: 0.6421988606452942 - trainLoss: 0.6443567872047424\n",
      "cnt: 0 - valLoss: 0.6421960592269897 - trainLoss: 0.6443543434143066\n",
      "cnt: 0 - valLoss: 0.6421929001808167 - trainLoss: 0.6443520188331604\n",
      "cnt: 0 - valLoss: 0.6421898007392883 - trainLoss: 0.6443493962287903\n",
      "cnt: 0 - valLoss: 0.6421866416931152 - trainLoss: 0.6443467736244202\n",
      "cnt: 0 - valLoss: 0.6421839594841003 - trainLoss: 0.64434415102005\n",
      "cnt: 0 - valLoss: 0.6421808004379272 - trainLoss: 0.6443418860435486\n",
      "cnt: 0 - valLoss: 0.6421776413917542 - trainLoss: 0.6443392634391785\n",
      "cnt: 0 - valLoss: 0.642174482345581 - trainLoss: 0.6443367004394531\n",
      "cnt: 0 - valLoss: 0.6421718001365662 - trainLoss: 0.644334077835083\n",
      "cnt: 0 - valLoss: 0.6421686410903931 - trainLoss: 0.6443317532539368\n",
      "cnt: 0 - valLoss: 0.6421654224395752 - trainLoss: 0.6443291306495667\n",
      "cnt: 0 - valLoss: 0.6421623229980469 - trainLoss: 0.6443265676498413\n",
      "cnt: 0 - valLoss: 0.6421595215797424 - trainLoss: 0.6443239450454712\n",
      "cnt: 0 - valLoss: 0.6421564221382141 - trainLoss: 0.6443215608596802\n",
      "cnt: 0 - valLoss: 0.642153263092041 - trainLoss: 0.6443190574645996\n",
      "cnt: 0 - valLoss: 0.6421501040458679 - trainLoss: 0.6443164348602295\n",
      "cnt: 0 - valLoss: 0.6421471834182739 - trainLoss: 0.6443138122558594\n",
      "cnt: 0 - valLoss: 0.6421442031860352 - trainLoss: 0.6443113088607788\n",
      "cnt: 0 - valLoss: 0.6421410441398621 - trainLoss: 0.644308865070343\n",
      "cnt: 0 - valLoss: 0.6421378254890442 - trainLoss: 0.6443063020706177\n",
      "cnt: 0 - valLoss: 0.6421348452568054 - trainLoss: 0.6443036198616028\n",
      "cnt: 0 - valLoss: 0.6421319246292114 - trainLoss: 0.644301176071167\n",
      "cnt: 0 - valLoss: 0.6421287655830383 - trainLoss: 0.6442987322807312\n",
      "cnt: 0 - valLoss: 0.6421256065368652 - trainLoss: 0.6442961096763611\n",
      "cnt: 0 - valLoss: 0.6421225070953369 - trainLoss: 0.644293487071991\n",
      "cnt: 0 - valLoss: 0.6421197056770325 - trainLoss: 0.6442909240722656\n",
      "cnt: 0 - valLoss: 0.6421164870262146 - trainLoss: 0.6442885994911194\n",
      "cnt: 0 - valLoss: 0.6421133279800415 - trainLoss: 0.6442859172821045\n",
      "cnt: 0 - valLoss: 0.6421101689338684 - trainLoss: 0.6442833542823792\n",
      "cnt: 0 - valLoss: 0.6421074867248535 - trainLoss: 0.6442806720733643\n",
      "cnt: 0 - valLoss: 0.6421042680740356 - trainLoss: 0.6442784070968628\n",
      "cnt: 0 - valLoss: 0.6421010494232178 - trainLoss: 0.6442757844924927\n",
      "cnt: 0 - valLoss: 0.6420979499816895 - trainLoss: 0.6442731618881226\n",
      "cnt: 0 - valLoss: 0.6420952081680298 - trainLoss: 0.6442705392837524\n",
      "cnt: 0 - valLoss: 0.6420920491218567 - trainLoss: 0.6442682147026062\n",
      "cnt: 0 - valLoss: 0.6420888304710388 - trainLoss: 0.6442656517028809\n",
      "cnt: 0 - valLoss: 0.6420856714248657 - trainLoss: 0.6442630290985107\n",
      "cnt: 0 - valLoss: 0.6420828104019165 - trainLoss: 0.6442604064941406\n",
      "cnt: 0 - valLoss: 0.6420797109603882 - trainLoss: 0.6442579627037048\n",
      "cnt: 0 - valLoss: 0.6420766115188599 - trainLoss: 0.644255518913269\n",
      "cnt: 0 - valLoss: 0.642073392868042 - trainLoss: 0.6442528963088989\n",
      "cnt: 0 - valLoss: 0.6420704126358032 - trainLoss: 0.644250214099884\n",
      "cnt: 0 - valLoss: 0.6420674920082092 - trainLoss: 0.6442477703094482\n",
      "cnt: 0 - valLoss: 0.6420642733573914 - trainLoss: 0.6442452669143677\n",
      "cnt: 0 - valLoss: 0.6420611143112183 - trainLoss: 0.6442427039146423\n",
      "cnt: 0 - valLoss: 0.6420581340789795 - trainLoss: 0.6442400813102722\n",
      "cnt: 0 - valLoss: 0.6420551538467407 - trainLoss: 0.6442375183105469\n",
      "cnt: 0 - valLoss: 0.6420520544052124 - trainLoss: 0.6442351341247559\n",
      "cnt: 0 - valLoss: 0.6420488953590393 - trainLoss: 0.6442325115203857\n",
      "cnt: 0 - valLoss: 0.6420456767082214 - trainLoss: 0.6442298889160156\n",
      "cnt: 0 - valLoss: 0.6420429348945618 - trainLoss: 0.6442273259162903\n",
      "cnt: 0 - valLoss: 0.6420397162437439 - trainLoss: 0.6442249417304993\n",
      "cnt: 0 - valLoss: 0.6420365571975708 - trainLoss: 0.6442223191261292\n",
      "cnt: 0 - valLoss: 0.6420333385467529 - trainLoss: 0.644219696521759\n",
      "cnt: 0 - valLoss: 0.642030656337738 - trainLoss: 0.6442170739173889\n",
      "cnt: 0 - valLoss: 0.6420274972915649 - trainLoss: 0.6442147493362427\n",
      "cnt: 0 - valLoss: 0.6420242786407471 - trainLoss: 0.6442121863365173\n",
      "cnt: 0 - valLoss: 0.6420210599899292 - trainLoss: 0.6442095041275024\n",
      "cnt: 0 - valLoss: 0.6420183777809143 - trainLoss: 0.6442068815231323\n",
      "cnt: 0 - valLoss: 0.6420151591300964 - trainLoss: 0.6442044973373413\n",
      "cnt: 0 - valLoss: 0.6420120000839233 - trainLoss: 0.644201934337616\n",
      "cnt: 0 - valLoss: 0.6420087814331055 - trainLoss: 0.6441993117332458\n",
      "cnt: 0 - valLoss: 0.642005980014801 - trainLoss: 0.6441966891288757\n",
      "cnt: 0 - valLoss: 0.6420028805732727 - trainLoss: 0.6441942453384399\n",
      "cnt: 0 - valLoss: 0.6419997215270996 - trainLoss: 0.6441917419433594\n",
      "cnt: 0 - valLoss: 0.6419965624809265 - trainLoss: 0.6441891193389893\n",
      "cnt: 0 - valLoss: 0.6419935822486877 - trainLoss: 0.6441864967346191\n",
      "cnt: 0 - valLoss: 0.641990602016449 - trainLoss: 0.6441840529441833\n",
      "cnt: 0 - valLoss: 0.6419873833656311 - trainLoss: 0.6441815495491028\n",
      "cnt: 0 - valLoss: 0.641984224319458 - trainLoss: 0.6441789269447327\n",
      "cnt: 0 - valLoss: 0.6419812440872192 - trainLoss: 0.6441763043403625\n",
      "cnt: 0 - valLoss: 0.6419782042503357 - trainLoss: 0.6441737413406372\n",
      "cnt: 0 - valLoss: 0.6419751644134521 - trainLoss: 0.6441713571548462\n",
      "cnt: 0 - valLoss: 0.6419719457626343 - trainLoss: 0.6441687345504761\n",
      "cnt: 0 - valLoss: 0.6419689059257507 - trainLoss: 0.6441660523414612\n",
      "cnt: 0 - valLoss: 0.6419659852981567 - trainLoss: 0.6441634893417358\n",
      "cnt: 0 - valLoss: 0.6419628262519836 - trainLoss: 0.6441611647605896\n",
      "cnt: 0 - valLoss: 0.6419596076011658 - trainLoss: 0.6441585421562195\n",
      "cnt: 0 - valLoss: 0.6419565081596375 - trainLoss: 0.6441558599472046\n",
      "cnt: 0 - valLoss: 0.6419536471366882 - trainLoss: 0.6441532373428345\n",
      "cnt: 0 - valLoss: 0.6419504880905151 - trainLoss: 0.6441509127616882\n",
      "cnt: 0 - valLoss: 0.641947329044342 - trainLoss: 0.6441482901573181\n",
      "cnt: 0 - valLoss: 0.6419440507888794 - trainLoss: 0.644145667552948\n",
      "cnt: 0 - valLoss: 0.6419414281845093 - trainLoss: 0.6441430449485779\n",
      "cnt: 0 - valLoss: 0.6419382095336914 - trainLoss: 0.6441406607627869\n",
      "cnt: 0 - valLoss: 0.6419349908828735 - trainLoss: 0.6441380381584167\n",
      "cnt: 0 - valLoss: 0.6419318318367004 - trainLoss: 0.6441354155540466\n",
      "cnt: 0 - valLoss: 0.641929030418396 - trainLoss: 0.6441327929496765\n",
      "cnt: 0 - valLoss: 0.6419258713722229 - trainLoss: 0.6441304087638855\n",
      "cnt: 0 - valLoss: 0.6419227123260498 - trainLoss: 0.6441277861595154\n",
      "cnt: 0 - valLoss: 0.6419194340705872 - trainLoss: 0.6441251635551453\n",
      "cnt: 0 - valLoss: 0.6419167518615723 - trainLoss: 0.6441226005554199\n",
      "cnt: 0 - valLoss: 0.641913652420044 - trainLoss: 0.6441201567649841\n",
      "cnt: 0 - valLoss: 0.6419104933738708 - trainLoss: 0.6441175937652588\n",
      "cnt: 0 - valLoss: 0.6419073343276978 - trainLoss: 0.6441149711608887\n",
      "cnt: 0 - valLoss: 0.6419044733047485 - trainLoss: 0.6441123485565186\n",
      "cnt: 0 - valLoss: 0.6419014930725098 - trainLoss: 0.6441099047660828\n",
      "cnt: 0 - valLoss: 0.6418983340263367 - trainLoss: 0.6441074013710022\n",
      "cnt: 0 - valLoss: 0.6418951749801636 - trainLoss: 0.6441047787666321\n",
      "cnt: 0 - valLoss: 0.6418922543525696 - trainLoss: 0.6441020965576172\n",
      "cnt: 0 - valLoss: 0.6418892741203308 - trainLoss: 0.6440996527671814\n",
      "cnt: 0 - valLoss: 0.6418861150741577 - trainLoss: 0.6440972089767456\n",
      "cnt: 0 - valLoss: 0.6418830156326294 - trainLoss: 0.6440945267677307\n",
      "cnt: 0 - valLoss: 0.6418799757957458 - trainLoss: 0.6440919041633606\n",
      "cnt: 0 - valLoss: 0.6418771743774414 - trainLoss: 0.64408940076828\n",
      "cnt: 0 - valLoss: 0.6418740153312683 - trainLoss: 0.644087016582489\n",
      "cnt: 0 - valLoss: 0.6418707966804504 - trainLoss: 0.6440843343734741\n",
      "cnt: 0 - valLoss: 0.6418677568435669 - trainLoss: 0.644081711769104\n",
      "cnt: 0 - valLoss: 0.6418650150299072 - trainLoss: 0.6440791487693787\n",
      "cnt: 0 - valLoss: 0.6418617963790894 - trainLoss: 0.6440767645835876\n",
      "cnt: 0 - valLoss: 0.6418586373329163 - trainLoss: 0.6440741419792175\n",
      "cnt: 0 - valLoss: 0.6418555378913879 - trainLoss: 0.6440715193748474\n",
      "cnt: 0 - valLoss: 0.6418527960777283 - trainLoss: 0.6440688967704773\n",
      "cnt: 0 - valLoss: 0.6418495774269104 - trainLoss: 0.644066572189331\n",
      "cnt: 0 - valLoss: 0.6418464779853821 - trainLoss: 0.6440638899803162\n",
      "cnt: 0 - valLoss: 0.641843318939209 - trainLoss: 0.644061267375946\n",
      "cnt: 0 - valLoss: 0.6418405771255493 - trainLoss: 0.6440586447715759\n",
      "cnt: 0 - valLoss: 0.641837477684021 - trainLoss: 0.6440563797950745\n",
      "cnt: 0 - valLoss: 0.6418342590332031 - trainLoss: 0.6440536975860596\n",
      "cnt: 0 - valLoss: 0.6418311595916748 - trainLoss: 0.6440510153770447\n",
      "cnt: 0 - valLoss: 0.6418284177780151 - trainLoss: 0.6440483927726746\n",
      "cnt: 0 - valLoss: 0.6418251991271973 - trainLoss: 0.6440460681915283\n",
      "cnt: 0 - valLoss: 0.6418220400810242 - trainLoss: 0.644043505191803\n",
      "cnt: 0 - valLoss: 0.6418189406394958 - trainLoss: 0.6440408229827881\n",
      "cnt: 0 - valLoss: 0.6418161392211914 - trainLoss: 0.644038200378418\n",
      "cnt: 0 - valLoss: 0.6418130397796631 - trainLoss: 0.644035816192627\n",
      "cnt: 0 - valLoss: 0.6418098211288452 - trainLoss: 0.6440331935882568\n",
      "cnt: 0 - valLoss: 0.6418067216873169 - trainLoss: 0.6440305709838867\n",
      "cnt: 0 - valLoss: 0.6418039202690125 - trainLoss: 0.6440279483795166\n",
      "cnt: 0 - valLoss: 0.6418008208274841 - trainLoss: 0.6440255641937256\n",
      "cnt: 0 - valLoss: 0.6417976021766663 - trainLoss: 0.6440230011940002\n",
      "cnt: 0 - valLoss: 0.6417945027351379 - trainLoss: 0.6440203785896301\n",
      "cnt: 0 - valLoss: 0.641791582107544 - trainLoss: 0.6440176963806152\n",
      "cnt: 0 - valLoss: 0.64178866147995 - trainLoss: 0.6440152525901794\n",
      "cnt: 0 - valLoss: 0.6417855024337769 - trainLoss: 0.6440127491950989\n",
      "cnt: 0 - valLoss: 0.641782283782959 - trainLoss: 0.6440101265907288\n",
      "cnt: 0 - valLoss: 0.641779363155365 - trainLoss: 0.6440074443817139\n",
      "cnt: 0 - valLoss: 0.6417763829231262 - trainLoss: 0.6440049409866333\n",
      "cnt: 0 - valLoss: 0.6417732834815979 - trainLoss: 0.6440024971961975\n",
      "cnt: 0 - valLoss: 0.64177006483078 - trainLoss: 0.6439998149871826\n",
      "cnt: 0 - valLoss: 0.6417670845985413 - trainLoss: 0.6439971923828125\n",
      "cnt: 0 - valLoss: 0.641764223575592 - trainLoss: 0.6439946889877319\n",
      "cnt: 0 - valLoss: 0.641761064529419 - trainLoss: 0.6439921855926514\n",
      "cnt: 0 - valLoss: 0.6417578458786011 - trainLoss: 0.643989622592926\n",
      "cnt: 0 - valLoss: 0.6417548656463623 - trainLoss: 0.6439869403839111\n",
      "cnt: 0 - valLoss: 0.6417519450187683 - trainLoss: 0.6439843773841858\n",
      "cnt: 0 - valLoss: 0.64174884557724 - trainLoss: 0.6439819931983948\n",
      "cnt: 0 - valLoss: 0.6417456269264221 - trainLoss: 0.6439793109893799\n",
      "cnt: 0 - valLoss: 0.6417425870895386 - trainLoss: 0.6439766883850098\n",
      "cnt: 0 - valLoss: 0.6417397856712341 - trainLoss: 0.6439740657806396\n",
      "cnt: 0 - valLoss: 0.641736626625061 - trainLoss: 0.6439716815948486\n",
      "cnt: 0 - valLoss: 0.6417334675788879 - trainLoss: 0.6439691185951233\n",
      "cnt: 0 - valLoss: 0.6417303085327148 - trainLoss: 0.6439664363861084\n",
      "cnt: 0 - valLoss: 0.6417275071144104 - trainLoss: 0.6439638137817383\n",
      "cnt: 0 - valLoss: 0.6417243480682373 - trainLoss: 0.643961489200592\n",
      "cnt: 0 - valLoss: 0.6417211890220642 - trainLoss: 0.6439588069915771\n",
      "cnt: 0 - valLoss: 0.6417180895805359 - trainLoss: 0.643956184387207\n",
      "cnt: 0 - valLoss: 0.6417153477668762 - trainLoss: 0.6439534425735474\n",
      "cnt: 0 - valLoss: 0.6417121291160583 - trainLoss: 0.6439511775970459\n",
      "cnt: 0 - valLoss: 0.6417089104652405 - trainLoss: 0.643948495388031\n",
      "cnt: 0 - valLoss: 0.6417057514190674 - trainLoss: 0.6439458131790161\n",
      "cnt: 0 - valLoss: 0.6417031288146973 - trainLoss: 0.643943190574646\n",
      "cnt: 0 - valLoss: 0.6416999101638794 - trainLoss: 0.6439408659934998\n",
      "cnt: 0 - valLoss: 0.6416966915130615 - trainLoss: 0.6439383029937744\n",
      "cnt: 0 - valLoss: 0.6416935324668884 - trainLoss: 0.6439355611801147\n",
      "cnt: 0 - valLoss: 0.6416907906532288 - trainLoss: 0.6439329385757446\n",
      "cnt: 0 - valLoss: 0.6416876316070557 - trainLoss: 0.6439305543899536\n",
      "cnt: 0 - valLoss: 0.6416845321655273 - trainLoss: 0.6439279317855835\n",
      "cnt: 0 - valLoss: 0.6416813135147095 - trainLoss: 0.6439253091812134\n",
      "cnt: 0 - valLoss: 0.6416785717010498 - trainLoss: 0.6439226269721985\n",
      "cnt: 0 - valLoss: 0.6416754126548767 - trainLoss: 0.6439202427864075\n",
      "cnt: 0 - valLoss: 0.6416721940040588 - trainLoss: 0.6439176201820374\n",
      "cnt: 0 - valLoss: 0.6416690945625305 - trainLoss: 0.6439149975776672\n",
      "cnt: 0 - valLoss: 0.6416661739349365 - trainLoss: 0.6439123153686523\n",
      "cnt: 0 - valLoss: 0.641663134098053 - trainLoss: 0.6439099311828613\n",
      "cnt: 0 - valLoss: 0.6416599750518799 - trainLoss: 0.643907368183136\n",
      "cnt: 0 - valLoss: 0.641656756401062 - trainLoss: 0.6439046859741211\n",
      "cnt: 0 - valLoss: 0.6416538953781128 - trainLoss: 0.643902063369751\n",
      "cnt: 0 - valLoss: 0.6416508555412292 - trainLoss: 0.64389967918396\n",
      "cnt: 0 - valLoss: 0.6416476964950562 - trainLoss: 0.6438971161842346\n",
      "cnt: 0 - valLoss: 0.6416445374488831 - trainLoss: 0.6438944339752197\n",
      "cnt: 0 - valLoss: 0.6416416168212891 - trainLoss: 0.6438916921615601\n",
      "cnt: 0 - valLoss: 0.6416386961936951 - trainLoss: 0.6438892483711243\n",
      "cnt: 0 - valLoss: 0.6416354179382324 - trainLoss: 0.6438867449760437\n",
      "cnt: 0 - valLoss: 0.6416322588920593 - trainLoss: 0.6438840627670288\n",
      "cnt: 0 - valLoss: 0.6416293382644653 - trainLoss: 0.6438814401626587\n",
      "cnt: 0 - valLoss: 0.6416263580322266 - trainLoss: 0.6438789367675781\n",
      "cnt: 0 - valLoss: 0.6416231393814087 - trainLoss: 0.6438764929771423\n",
      "cnt: 0 - valLoss: 0.6416199803352356 - trainLoss: 0.6438738107681274\n",
      "cnt: 0 - valLoss: 0.6416170001029968 - trainLoss: 0.6438711881637573\n",
      "cnt: 0 - valLoss: 0.6416140794754028 - trainLoss: 0.6438685655593872\n",
      "cnt: 0 - valLoss: 0.6416109204292297 - trainLoss: 0.6438661813735962\n",
      "cnt: 0 - valLoss: 0.6416076421737671 - trainLoss: 0.6438634395599365\n",
      "cnt: 0 - valLoss: 0.6416046619415283 - trainLoss: 0.6438608169555664\n",
      "cnt: 0 - valLoss: 0.6416018009185791 - trainLoss: 0.6438582539558411\n",
      "cnt: 0 - valLoss: 0.6415985822677612 - trainLoss: 0.6438558101654053\n",
      "cnt: 0 - valLoss: 0.6415954232215881 - trainLoss: 0.6438531875610352\n",
      "cnt: 0 - valLoss: 0.6415923833847046 - trainLoss: 0.6438504457473755\n",
      "cnt: 0 - valLoss: 0.6415895223617554 - trainLoss: 0.6438478827476501\n",
      "cnt: 0 - valLoss: 0.6415863633155823 - trainLoss: 0.6438454985618591\n",
      "cnt: 0 - valLoss: 0.6415831446647644 - trainLoss: 0.6438428163528442\n",
      "cnt: 0 - valLoss: 0.6415800452232361 - trainLoss: 0.6438401341438293\n",
      "cnt: 0 - valLoss: 0.6415772438049316 - trainLoss: 0.6438375115394592\n",
      "cnt: 0 - valLoss: 0.6415740251541138 - trainLoss: 0.6438351273536682\n",
      "cnt: 0 - valLoss: 0.6415708661079407 - trainLoss: 0.6438324451446533\n",
      "cnt: 0 - valLoss: 0.6415677666664124 - trainLoss: 0.6438298225402832\n",
      "cnt: 0 - valLoss: 0.6415649056434631 - trainLoss: 0.6438271403312683\n",
      "cnt: 0 - valLoss: 0.6415616869926453 - trainLoss: 0.6438248753547668\n",
      "cnt: 0 - valLoss: 0.6415583491325378 - trainLoss: 0.6438221335411072\n",
      "cnt: 0 - valLoss: 0.6415551900863647 - trainLoss: 0.6438193917274475\n",
      "cnt: 0 - valLoss: 0.6415523290634155 - trainLoss: 0.6438167691230774\n",
      "cnt: 0 - valLoss: 0.6415490508079529 - trainLoss: 0.6438143849372864\n",
      "cnt: 0 - valLoss: 0.6415457725524902 - trainLoss: 0.6438117027282715\n",
      "cnt: 0 - valLoss: 0.6415426135063171 - trainLoss: 0.6438090205192566\n",
      "cnt: 0 - valLoss: 0.6415397524833679 - trainLoss: 0.6438063383102417\n",
      "cnt: 0 - valLoss: 0.6415364742279053 - trainLoss: 0.6438040137290955\n",
      "cnt: 0 - valLoss: 0.6415332555770874 - trainLoss: 0.6438013315200806\n",
      "cnt: 0 - valLoss: 0.6415299773216248 - trainLoss: 0.6437985897064209\n",
      "cnt: 0 - valLoss: 0.6415271759033203 - trainLoss: 0.643795907497406\n",
      "cnt: 0 - valLoss: 0.6415239572525024 - trainLoss: 0.6437935829162598\n",
      "cnt: 0 - valLoss: 0.641520619392395 - trainLoss: 0.6437909603118896\n",
      "cnt: 0 - valLoss: 0.6415174007415771 - trainLoss: 0.64378821849823\n",
      "cnt: 0 - valLoss: 0.6415146589279175 - trainLoss: 0.6437854766845703\n",
      "cnt: 0 - valLoss: 0.6415113210678101 - trainLoss: 0.6437831521034241\n",
      "cnt: 0 - valLoss: 0.6415081024169922 - trainLoss: 0.643780529499054\n",
      "cnt: 0 - valLoss: 0.6415048241615295 - trainLoss: 0.6437777876853943\n",
      "cnt: 0 - valLoss: 0.6415020227432251 - trainLoss: 0.6437751054763794\n",
      "cnt: 0 - valLoss: 0.6414988040924072 - trainLoss: 0.6437727212905884\n",
      "cnt: 0 - valLoss: 0.6414954662322998 - trainLoss: 0.6437700390815735\n",
      "cnt: 0 - valLoss: 0.6414922475814819 - trainLoss: 0.6437673568725586\n",
      "cnt: 0 - valLoss: 0.6414894461631775 - trainLoss: 0.6437646746635437\n",
      "cnt: 0 - valLoss: 0.6414861679077148 - trainLoss: 0.6437622904777527\n",
      "cnt: 0 - valLoss: 0.6414828300476074 - trainLoss: 0.6437596678733826\n",
      "cnt: 0 - valLoss: 0.6414796113967896 - trainLoss: 0.6437569260597229\n",
      "cnt: 0 - valLoss: 0.6414768099784851 - trainLoss: 0.643754243850708\n",
      "cnt: 0 - valLoss: 0.6414735913276672 - trainLoss: 0.643751859664917\n",
      "cnt: 0 - valLoss: 0.6414703130722046 - trainLoss: 0.6437492370605469\n",
      "cnt: 0 - valLoss: 0.6414670348167419 - trainLoss: 0.6437464952468872\n",
      "cnt: 0 - valLoss: 0.6414641737937927 - trainLoss: 0.6437438130378723\n",
      "cnt: 0 - valLoss: 0.6414609551429749 - trainLoss: 0.6437414288520813\n",
      "cnt: 0 - valLoss: 0.641457736492157 - trainLoss: 0.6437388062477112\n",
      "cnt: 0 - valLoss: 0.6414543986320496 - trainLoss: 0.6437361240386963\n",
      "cnt: 0 - valLoss: 0.6414515972137451 - trainLoss: 0.6437334418296814\n",
      "cnt: 0 - valLoss: 0.6414483785629272 - trainLoss: 0.6437309384346008\n",
      "cnt: 0 - valLoss: 0.6414451003074646 - trainLoss: 0.6437283158302307\n",
      "cnt: 0 - valLoss: 0.6414418816566467 - trainLoss: 0.6437256336212158\n",
      "cnt: 0 - valLoss: 0.6414390206336975 - trainLoss: 0.6437230110168457\n",
      "cnt: 0 - valLoss: 0.6414358019828796 - trainLoss: 0.6437205672264099\n",
      "cnt: 0 - valLoss: 0.6414324641227722 - trainLoss: 0.6437179446220398\n",
      "cnt: 0 - valLoss: 0.6414291858673096 - trainLoss: 0.6437152028083801\n",
      "cnt: 0 - valLoss: 0.6414262652397156 - trainLoss: 0.6437125205993652\n",
      "cnt: 0 - valLoss: 0.6414231061935425 - trainLoss: 0.6437100768089294\n",
      "cnt: 0 - valLoss: 0.6414197683334351 - trainLoss: 0.6437073945999146\n",
      "cnt: 0 - valLoss: 0.6414164304733276 - trainLoss: 0.6437047719955444\n",
      "cnt: 0 - valLoss: 0.6414135694503784 - trainLoss: 0.6437020301818848\n",
      "cnt: 0 - valLoss: 0.6414103507995605 - trainLoss: 0.643699586391449\n",
      "cnt: 0 - valLoss: 0.6414070725440979 - trainLoss: 0.6436969637870789\n",
      "cnt: 0 - valLoss: 0.6414037942886353 - trainLoss: 0.643694281578064\n",
      "cnt: 0 - valLoss: 0.6414008140563965 - trainLoss: 0.6436915397644043\n",
      "cnt: 0 - valLoss: 0.6413976550102234 - trainLoss: 0.6436891555786133\n",
      "cnt: 0 - valLoss: 0.6413943767547607 - trainLoss: 0.6436865329742432\n",
      "cnt: 0 - valLoss: 0.6413911581039429 - trainLoss: 0.6436837911605835\n",
      "cnt: 0 - valLoss: 0.641388475894928 - trainLoss: 0.6436811089515686\n",
      "cnt: 0 - valLoss: 0.6413852572441101 - trainLoss: 0.6436786651611328\n",
      "cnt: 0 - valLoss: 0.6413820385932922 - trainLoss: 0.6436761021614075\n",
      "cnt: 0 - valLoss: 0.6413789391517639 - trainLoss: 0.6436733603477478\n",
      "cnt: 0 - valLoss: 0.6413761377334595 - trainLoss: 0.6436706781387329\n",
      "cnt: 0 - valLoss: 0.6413730978965759 - trainLoss: 0.6436682343482971\n",
      "cnt: 0 - valLoss: 0.6413698792457581 - trainLoss: 0.643665611743927\n",
      "cnt: 0 - valLoss: 0.6413666009902954 - trainLoss: 0.6436629891395569\n",
      "cnt: 0 - valLoss: 0.6413638591766357 - trainLoss: 0.6436602473258972\n",
      "cnt: 0 - valLoss: 0.641360878944397 - trainLoss: 0.6436577439308167\n",
      "cnt: 0 - valLoss: 0.6413576602935791 - trainLoss: 0.6436552405357361\n",
      "cnt: 0 - valLoss: 0.6413543224334717 - trainLoss: 0.6436525583267212\n",
      "cnt: 0 - valLoss: 0.641351580619812 - trainLoss: 0.6436498165130615\n",
      "cnt: 0 - valLoss: 0.6413484811782837 - trainLoss: 0.6436473727226257\n",
      "cnt: 0 - valLoss: 0.6413453221321106 - trainLoss: 0.6436447501182556\n",
      "cnt: 0 - valLoss: 0.6413421034812927 - trainLoss: 0.6436420679092407\n",
      "cnt: 0 - valLoss: 0.6413393020629883 - trainLoss: 0.643639326095581\n",
      "cnt: 0 - valLoss: 0.6413362622261047 - trainLoss: 0.6436368823051453\n",
      "cnt: 0 - valLoss: 0.6413331031799316 - trainLoss: 0.6436343193054199\n",
      "cnt: 0 - valLoss: 0.6413298845291138 - trainLoss: 0.643631637096405\n",
      "cnt: 0 - valLoss: 0.6413269639015198 - trainLoss: 0.6436289548873901\n",
      "cnt: 0 - valLoss: 0.6413239240646362 - trainLoss: 0.6436264514923096\n",
      "cnt: 0 - valLoss: 0.6413207054138184 - trainLoss: 0.6436238884925842\n",
      "cnt: 0 - valLoss: 0.6413176655769348 - trainLoss: 0.6436211466789246\n",
      "cnt: 0 - valLoss: 0.6413147449493408 - trainLoss: 0.6436184644699097\n",
      "cnt: 0 - valLoss: 0.6413116455078125 - trainLoss: 0.6436159610748291\n",
      "cnt: 0 - valLoss: 0.6413085460662842 - trainLoss: 0.6436134576797485\n",
      "cnt: 0 - valLoss: 0.6413053274154663 - trainLoss: 0.6436107158660889\n",
      "cnt: 0 - valLoss: 0.6413025856018066 - trainLoss: 0.643608033657074\n",
      "cnt: 0 - valLoss: 0.641299307346344 - trainLoss: 0.6436055898666382\n",
      "cnt: 0 - valLoss: 0.6412962079048157 - trainLoss: 0.6436029672622681\n",
      "cnt: 0 - valLoss: 0.6412929892539978 - trainLoss: 0.6436002850532532\n",
      "cnt: 0 - valLoss: 0.6412901878356934 - trainLoss: 0.6435976028442383\n",
      "cnt: 0 - valLoss: 0.6412872076034546 - trainLoss: 0.6435950994491577\n",
      "cnt: 0 - valLoss: 0.6412838697433472 - trainLoss: 0.6435925364494324\n",
      "cnt: 0 - valLoss: 0.6412807106971741 - trainLoss: 0.6435897946357727\n",
      "cnt: 0 - valLoss: 0.6412779092788696 - trainLoss: 0.6435871124267578\n",
      "cnt: 0 - valLoss: 0.6412749290466309 - trainLoss: 0.643584668636322\n",
      "cnt: 0 - valLoss: 0.6412715911865234 - trainLoss: 0.6435820460319519\n",
      "cnt: 0 - valLoss: 0.6412684321403503 - trainLoss: 0.643579363822937\n",
      "cnt: 0 - valLoss: 0.6412656307220459 - trainLoss: 0.6435766220092773\n",
      "cnt: 0 - valLoss: 0.6412625908851624 - trainLoss: 0.6435741782188416\n",
      "cnt: 0 - valLoss: 0.6412593722343445 - trainLoss: 0.6435715556144714\n",
      "cnt: 0 - valLoss: 0.6412560939788818 - trainLoss: 0.6435688734054565\n",
      "cnt: 0 - valLoss: 0.6412532925605774 - trainLoss: 0.6435661315917969\n",
      "cnt: 0 - valLoss: 0.6412502527236938 - trainLoss: 0.6435637474060059\n",
      "cnt: 0 - valLoss: 0.6412470936775208 - trainLoss: 0.643561065196991\n",
      "cnt: 0 - valLoss: 0.6412437558174133 - trainLoss: 0.6435583829879761\n",
      "cnt: 0 - valLoss: 0.6412408947944641 - trainLoss: 0.6435557007789612\n",
      "cnt: 0 - valLoss: 0.6412379145622253 - trainLoss: 0.6435531973838806\n",
      "cnt: 0 - valLoss: 0.6412347555160522 - trainLoss: 0.6435506343841553\n",
      "cnt: 0 - valLoss: 0.6412314772605896 - trainLoss: 0.6435479521751404\n",
      "cnt: 0 - valLoss: 0.6412286758422852 - trainLoss: 0.6435452103614807\n",
      "cnt: 0 - valLoss: 0.6412256360054016 - trainLoss: 0.6435427665710449\n",
      "cnt: 0 - valLoss: 0.6412222981452942 - trainLoss: 0.6435401439666748\n",
      "cnt: 0 - valLoss: 0.6412191390991211 - trainLoss: 0.6435374617576599\n",
      "cnt: 0 - valLoss: 0.6412163376808167 - trainLoss: 0.6435346603393555\n",
      "cnt: 0 - valLoss: 0.6412132978439331 - trainLoss: 0.6435322761535645\n",
      "cnt: 0 - valLoss: 0.6412099599838257 - trainLoss: 0.6435295939445496\n",
      "cnt: 0 - valLoss: 0.6412068009376526 - trainLoss: 0.6435269117355347\n",
      "cnt: 0 - valLoss: 0.6412039995193481 - trainLoss: 0.6435241103172302\n",
      "cnt: 0 - valLoss: 0.6412007808685303 - trainLoss: 0.6435217261314392\n",
      "cnt: 0 - valLoss: 0.6411976218223572 - trainLoss: 0.6435191035270691\n",
      "cnt: 0 - valLoss: 0.6411944031715393 - trainLoss: 0.6435163617134094\n",
      "cnt: 0 - valLoss: 0.6411916017532349 - trainLoss: 0.6435136795043945\n",
      "cnt: 0 - valLoss: 0.6411885619163513 - trainLoss: 0.6435112357139587\n",
      "cnt: 0 - valLoss: 0.6411852240562439 - trainLoss: 0.6435085535049438\n",
      "cnt: 0 - valLoss: 0.641182005405426 - trainLoss: 0.643505871295929\n",
      "cnt: 0 - valLoss: 0.6411792635917664 - trainLoss: 0.6435031294822693\n",
      "cnt: 0 - valLoss: 0.6411761045455933 - trainLoss: 0.6435006856918335\n",
      "cnt: 0 - valLoss: 0.6411729454994202 - trainLoss: 0.6434980630874634\n",
      "cnt: 0 - valLoss: 0.6411696672439575 - trainLoss: 0.6434953808784485\n",
      "cnt: 0 - valLoss: 0.6411669254302979 - trainLoss: 0.6434926390647888\n",
      "cnt: 0 - valLoss: 0.6411637663841248 - trainLoss: 0.643490195274353\n",
      "cnt: 0 - valLoss: 0.6411604285240173 - trainLoss: 0.6434875130653381\n",
      "cnt: 0 - valLoss: 0.6411572694778442 - trainLoss: 0.6434848308563232\n",
      "cnt: 0 - valLoss: 0.6411545872688293 - trainLoss: 0.6434820294380188\n",
      "cnt: 0 - valLoss: 0.6411513090133667 - trainLoss: 0.6434796452522278\n",
      "cnt: 0 - valLoss: 0.6411480903625488 - trainLoss: 0.6434770226478577\n",
      "cnt: 0 - valLoss: 0.6411449313163757 - trainLoss: 0.6434742212295532\n",
      "cnt: 0 - valLoss: 0.6411420702934265 - trainLoss: 0.6434715390205383\n",
      "cnt: 0 - valLoss: 0.6411389708518982 - trainLoss: 0.6434691548347473\n",
      "cnt: 0 - valLoss: 0.6411357522010803 - trainLoss: 0.6434664130210876\n",
      "cnt: 0 - valLoss: 0.6411324739456177 - trainLoss: 0.643463671207428\n",
      "cnt: 0 - valLoss: 0.6411297917366028 - trainLoss: 0.6434609889984131\n",
      "cnt: 0 - valLoss: 0.6411265730857849 - trainLoss: 0.6434585452079773\n",
      "cnt: 0 - valLoss: 0.6411234140396118 - trainLoss: 0.6434559226036072\n",
      "cnt: 0 - valLoss: 0.6411200761795044 - trainLoss: 0.6434531807899475\n",
      "cnt: 0 - valLoss: 0.6411174535751343 - trainLoss: 0.6434504389762878\n",
      "cnt: 0 - valLoss: 0.6411142349243164 - trainLoss: 0.6434480547904968\n",
      "cnt: 0 - valLoss: 0.641110897064209 - trainLoss: 0.6434453725814819\n",
      "cnt: 0 - valLoss: 0.6411077380180359 - trainLoss: 0.6434426307678223\n",
      "cnt: 0 - valLoss: 0.6411051154136658 - trainLoss: 0.6434398889541626\n",
      "cnt: 0 - valLoss: 0.6411017179489136 - trainLoss: 0.6434375643730164\n",
      "cnt: 0 - valLoss: 0.6410985589027405 - trainLoss: 0.6434347629547119\n",
      "cnt: 0 - valLoss: 0.6410953402519226 - trainLoss: 0.643432080745697\n",
      "cnt: 0 - valLoss: 0.6410925388336182 - trainLoss: 0.6434293389320374\n",
      "cnt: 0 - valLoss: 0.6410893201828003 - trainLoss: 0.6434269547462463\n",
      "cnt: 0 - valLoss: 0.6410861611366272 - trainLoss: 0.6434242129325867\n",
      "cnt: 0 - valLoss: 0.6410828232765198 - trainLoss: 0.643421471118927\n",
      "cnt: 0 - valLoss: 0.6410802006721497 - trainLoss: 0.6434188485145569\n",
      "cnt: 0 - valLoss: 0.6410769820213318 - trainLoss: 0.6434164643287659\n",
      "cnt: 0 - valLoss: 0.6410737633705139 - trainLoss: 0.6434137225151062\n",
      "cnt: 0 - valLoss: 0.6410704851150513 - trainLoss: 0.6434109807014465\n",
      "cnt: 0 - valLoss: 0.6410677433013916 - trainLoss: 0.6434082388877869\n",
      "cnt: 0 - valLoss: 0.6410645842552185 - trainLoss: 0.6434057950973511\n",
      "cnt: 0 - valLoss: 0.6410612463951111 - trainLoss: 0.6434031128883362\n",
      "cnt: 0 - valLoss: 0.6410581469535828 - trainLoss: 0.6434003710746765\n",
      "cnt: 0 - valLoss: 0.6410553455352783 - trainLoss: 0.6433976888656616\n",
      "cnt: 0 - valLoss: 0.6410520076751709 - trainLoss: 0.6433953046798706\n",
      "cnt: 0 - valLoss: 0.6410488486289978 - trainLoss: 0.6433925032615662\n",
      "cnt: 0 - valLoss: 0.6410457491874695 - trainLoss: 0.6433897614479065\n",
      "cnt: 0 - valLoss: 0.6410428285598755 - trainLoss: 0.6433871388435364\n",
      "cnt: 0 - valLoss: 0.6410396695137024 - trainLoss: 0.6433846950531006\n",
      "cnt: 0 - valLoss: 0.6410364508628845 - trainLoss: 0.6433819532394409\n",
      "cnt: 0 - valLoss: 0.6410332322120667 - trainLoss: 0.6433792114257812\n",
      "cnt: 0 - valLoss: 0.641030490398407 - trainLoss: 0.6433765292167664\n",
      "cnt: 0 - valLoss: 0.6410272121429443 - trainLoss: 0.6433740854263306\n",
      "cnt: 0 - valLoss: 0.6410239338874817 - trainLoss: 0.6433713436126709\n",
      "cnt: 0 - valLoss: 0.6410208940505981 - trainLoss: 0.6433686017990112\n",
      "cnt: 0 - valLoss: 0.6410180330276489 - trainLoss: 0.6433659195899963\n",
      "cnt: 0 - valLoss: 0.6410146951675415 - trainLoss: 0.6433635354042053\n",
      "cnt: 0 - valLoss: 0.6410115361213684 - trainLoss: 0.6433608531951904\n",
      "cnt: 0 - valLoss: 0.6410085558891296 - trainLoss: 0.6433579921722412\n",
      "cnt: 0 - valLoss: 0.6410056352615356 - trainLoss: 0.6433554291725159\n",
      "cnt: 0 - valLoss: 0.6410022974014282 - trainLoss: 0.6433529257774353\n",
      "cnt: 0 - valLoss: 0.6409990787506104 - trainLoss: 0.6433501243591309\n",
      "cnt: 0 - valLoss: 0.6409961581230164 - trainLoss: 0.643347442150116\n",
      "cnt: 0 - valLoss: 0.6409931182861328 - trainLoss: 0.6433448195457458\n",
      "cnt: 0 - valLoss: 0.6409898996353149 - trainLoss: 0.6433423161506653\n",
      "cnt: 0 - valLoss: 0.6409866809844971 - trainLoss: 0.6433396339416504\n",
      "cnt: 0 - valLoss: 0.6409836411476135 - trainLoss: 0.643336832523346\n",
      "cnt: 0 - valLoss: 0.6409806609153748 - trainLoss: 0.6433343291282654\n",
      "cnt: 0 - valLoss: 0.6409775018692017 - trainLoss: 0.6433317065238953\n",
      "cnt: 0 - valLoss: 0.640974223613739 - trainLoss: 0.6433289647102356\n",
      "cnt: 0 - valLoss: 0.6409712433815002 - trainLoss: 0.6433262228965759\n",
      "cnt: 0 - valLoss: 0.6409682631492615 - trainLoss: 0.6433236598968506\n",
      "cnt: 0 - valLoss: 0.640964925289154 - trainLoss: 0.6433210968971252\n",
      "cnt: 0 - valLoss: 0.6409617066383362 - trainLoss: 0.6433183550834656\n",
      "cnt: 0 - valLoss: 0.640958845615387 - trainLoss: 0.6433156132698059\n",
      "cnt: 0 - valLoss: 0.6409556865692139 - trainLoss: 0.6433131098747253\n",
      "cnt: 0 - valLoss: 0.640952467918396 - trainLoss: 0.6433104872703552\n",
      "cnt: 0 - valLoss: 0.6409493088722229 - trainLoss: 0.6433077454566956\n",
      "cnt: 0 - valLoss: 0.6409462690353394 - trainLoss: 0.6433050036430359\n",
      "cnt: 0 - valLoss: 0.6409432291984558 - trainLoss: 0.6433025002479553\n",
      "cnt: 0 - valLoss: 0.6409399509429932 - trainLoss: 0.6432998776435852\n",
      "cnt: 0 - valLoss: 0.6409366726875305 - trainLoss: 0.6432970762252808\n",
      "cnt: 0 - valLoss: 0.6409337520599365 - trainLoss: 0.6432943940162659\n",
      "cnt: 0 - valLoss: 0.6409305930137634 - trainLoss: 0.6432918906211853\n",
      "cnt: 0 - valLoss: 0.6409273743629456 - trainLoss: 0.6432892084121704\n",
      "cnt: 0 - valLoss: 0.6409239768981934 - trainLoss: 0.6432864665985107\n",
      "cnt: 0 - valLoss: 0.6409211754798889 - trainLoss: 0.6432836651802063\n",
      "cnt: 0 - valLoss: 0.6409178972244263 - trainLoss: 0.6432812213897705\n",
      "cnt: 0 - valLoss: 0.6409146785736084 - trainLoss: 0.6432785391807556\n",
      "cnt: 0 - valLoss: 0.6409114599227905 - trainLoss: 0.643275797367096\n",
      "cnt: 0 - valLoss: 0.6409085988998413 - trainLoss: 0.6432729959487915\n",
      "cnt: 0 - valLoss: 0.6409053802490234 - trainLoss: 0.6432706117630005\n",
      "cnt: 0 - valLoss: 0.640902042388916 - trainLoss: 0.6432678699493408\n",
      "cnt: 0 - valLoss: 0.6408988237380981 - trainLoss: 0.6432650685310364\n",
      "cnt: 0 - valLoss: 0.6408959031105042 - trainLoss: 0.6432623267173767\n",
      "cnt: 0 - valLoss: 0.640892744064331 - trainLoss: 0.6432599425315857\n",
      "cnt: 0 - valLoss: 0.6408893465995789 - trainLoss: 0.6432571411132812\n",
      "cnt: 0 - valLoss: 0.640886127948761 - trainLoss: 0.6432543396949768\n",
      "cnt: 0 - valLoss: 0.6408833265304565 - trainLoss: 0.6432516574859619\n",
      "cnt: 0 - valLoss: 0.6408800482749939 - trainLoss: 0.6432492733001709\n",
      "cnt: 0 - valLoss: 0.6408767104148865 - trainLoss: 0.6432464718818665\n",
      "cnt: 0 - valLoss: 0.6408734917640686 - trainLoss: 0.643243670463562\n",
      "cnt: 0 - valLoss: 0.6408705711364746 - trainLoss: 0.6432410478591919\n",
      "cnt: 0 - valLoss: 0.6408673524856567 - trainLoss: 0.6432384848594666\n",
      "cnt: 0 - valLoss: 0.6408640146255493 - trainLoss: 0.6432357430458069\n",
      "cnt: 0 - valLoss: 0.640860915184021 - trainLoss: 0.6432329416275024\n",
      "cnt: 0 - valLoss: 0.640857994556427 - trainLoss: 0.6432303190231323\n",
      "cnt: 0 - valLoss: 0.6408546566963196 - trainLoss: 0.6432278156280518\n",
      "cnt: 0 - valLoss: 0.6408513784408569 - trainLoss: 0.6432250142097473\n",
      "cnt: 0 - valLoss: 0.6408483386039734 - trainLoss: 0.6432222723960876\n",
      "cnt: 0 - valLoss: 0.6408452391624451 - trainLoss: 0.6432196497917175\n",
      "cnt: 0 - valLoss: 0.6408419609069824 - trainLoss: 0.6432170867919922\n",
      "cnt: 0 - valLoss: 0.640838623046875 - trainLoss: 0.6432142853736877\n",
      "cnt: 0 - valLoss: 0.640835702419281 - trainLoss: 0.6432114839553833\n",
      "cnt: 0 - valLoss: 0.6408325433731079 - trainLoss: 0.6432089805603027\n",
      "cnt: 0 - valLoss: 0.64082932472229 - trainLoss: 0.6432062983512878\n",
      "cnt: 0 - valLoss: 0.6408259272575378 - trainLoss: 0.643203616142273\n",
      "cnt: 0 - valLoss: 0.6408231258392334 - trainLoss: 0.6432008147239685\n",
      "cnt: 0 - valLoss: 0.6408198475837708 - trainLoss: 0.6431983113288879\n",
      "cnt: 0 - valLoss: 0.6408165693283081 - trainLoss: 0.6431956887245178\n",
      "cnt: 0 - valLoss: 0.6408132314682007 - trainLoss: 0.6431928277015686\n",
      "cnt: 0 - valLoss: 0.6408104300498962 - trainLoss: 0.6431900858879089\n",
      "cnt: 0 - valLoss: 0.6408071517944336 - trainLoss: 0.6431876420974731\n",
      "cnt: 0 - valLoss: 0.6408038139343262 - trainLoss: 0.6431849002838135\n",
      "cnt: 0 - valLoss: 0.6408005356788635 - trainLoss: 0.6431821584701538\n",
      "cnt: 0 - valLoss: 0.6407977342605591 - trainLoss: 0.6431792974472046\n",
      "cnt: 0 - valLoss: 0.6407945156097412 - trainLoss: 0.6431769132614136\n",
      "cnt: 0 - valLoss: 0.640791118144989 - trainLoss: 0.6431741714477539\n",
      "cnt: 0 - valLoss: 0.6407879590988159 - trainLoss: 0.6431713700294495\n",
      "cnt: 0 - valLoss: 0.6407850384712219 - trainLoss: 0.6431686282157898\n",
      "cnt: 0 - valLoss: 0.6407817602157593 - trainLoss: 0.6431662440299988\n",
      "cnt: 0 - valLoss: 0.6407784819602966 - trainLoss: 0.6431634426116943\n",
      "cnt: 0 - valLoss: 0.640775203704834 - trainLoss: 0.6431606411933899\n",
      "cnt: 0 - valLoss: 0.6407723426818848 - trainLoss: 0.6431578993797302\n",
      "cnt: 0 - valLoss: 0.6407690048217773 - trainLoss: 0.6431554555892944\n",
      "cnt: 0 - valLoss: 0.6407657265663147 - trainLoss: 0.64315265417099\n",
      "cnt: 0 - valLoss: 0.6407625675201416 - trainLoss: 0.6431498527526855\n",
      "cnt: 0 - valLoss: 0.6407596468925476 - trainLoss: 0.6431472301483154\n",
      "cnt: 0 - valLoss: 0.6407562494277954 - trainLoss: 0.6431446671485901\n",
      "cnt: 0 - valLoss: 0.6407530307769775 - trainLoss: 0.6431419253349304\n",
      "cnt: 0 - valLoss: 0.640749990940094 - trainLoss: 0.643139123916626\n",
      "cnt: 0 - valLoss: 0.6407468914985657 - trainLoss: 0.6431365013122559\n",
      "cnt: 0 - valLoss: 0.6407435536384583 - trainLoss: 0.6431338787078857\n",
      "cnt: 0 - valLoss: 0.6407402157783508 - trainLoss: 0.6431311368942261\n",
      "cnt: 0 - valLoss: 0.6407372355461121 - trainLoss: 0.6431283354759216\n",
      "cnt: 0 - valLoss: 0.640734076499939 - trainLoss: 0.6431257724761963\n",
      "cnt: 0 - valLoss: 0.6407307982444763 - trainLoss: 0.6431231498718262\n",
      "cnt: 0 - valLoss: 0.6407274603843689 - trainLoss: 0.6431204080581665\n",
      "cnt: 0 - valLoss: 0.6407245993614197 - trainLoss: 0.6431176066398621\n",
      "cnt: 0 - valLoss: 0.6407213807106018 - trainLoss: 0.6431151032447815\n",
      "cnt: 0 - valLoss: 0.6407181024551392 - trainLoss: 0.6431123614311218\n",
      "cnt: 0 - valLoss: 0.6407146453857422 - trainLoss: 0.6431096196174622\n",
      "cnt: 0 - valLoss: 0.6407118439674377 - trainLoss: 0.6431068181991577\n",
      "cnt: 0 - valLoss: 0.6407086253166199 - trainLoss: 0.6431043148040771\n",
      "cnt: 0 - valLoss: 0.6407053470611572 - trainLoss: 0.6431016325950623\n",
      "cnt: 0 - valLoss: 0.640701949596405 - trainLoss: 0.6430988311767578\n",
      "cnt: 0 - valLoss: 0.6406991481781006 - trainLoss: 0.6430960297584534\n",
      "cnt: 0 - valLoss: 0.6406958103179932 - trainLoss: 0.6430936455726624\n",
      "cnt: 0 - valLoss: 0.6406924724578857 - trainLoss: 0.6430908441543579\n",
      "cnt: 0 - valLoss: 0.6406891345977783 - trainLoss: 0.6430880427360535\n",
      "cnt: 0 - valLoss: 0.6406862735748291 - trainLoss: 0.6430853009223938\n",
      "cnt: 0 - valLoss: 0.6406829357147217 - trainLoss: 0.643082857131958\n",
      "cnt: 0 - valLoss: 0.6406795978546143 - trainLoss: 0.6430800557136536\n",
      "cnt: 0 - valLoss: 0.6406763195991516 - trainLoss: 0.6430772542953491\n",
      "cnt: 0 - valLoss: 0.6406733393669128 - trainLoss: 0.6430745720863342\n",
      "cnt: 0 - valLoss: 0.6406700015068054 - trainLoss: 0.6430720686912537\n",
      "cnt: 0 - valLoss: 0.6406666040420532 - trainLoss: 0.643069326877594\n",
      "cnt: 0 - valLoss: 0.6406635046005249 - trainLoss: 0.6430664658546448\n",
      "cnt: 0 - valLoss: 0.6406604051589966 - trainLoss: 0.6430638432502747\n",
      "cnt: 0 - valLoss: 0.6406570672988892 - trainLoss: 0.6430613398551941\n",
      "cnt: 0 - valLoss: 0.640653669834137 - trainLoss: 0.6430584788322449\n",
      "cnt: 0 - valLoss: 0.6406506299972534 - trainLoss: 0.6430556774139404\n",
      "cnt: 0 - valLoss: 0.6406475305557251 - trainLoss: 0.6430531144142151\n",
      "cnt: 0 - valLoss: 0.6406441330909729 - trainLoss: 0.643050491809845\n",
      "cnt: 0 - valLoss: 0.6406407952308655 - trainLoss: 0.6430476903915405\n",
      "cnt: 0 - valLoss: 0.6406377553939819 - trainLoss: 0.6430449485778809\n",
      "cnt: 0 - valLoss: 0.6406345963478088 - trainLoss: 0.6430423259735107\n",
      "cnt: 0 - valLoss: 0.6406312584877014 - trainLoss: 0.6430397033691406\n",
      "cnt: 0 - valLoss: 0.6406278014183044 - trainLoss: 0.6430369019508362\n",
      "cnt: 0 - valLoss: 0.6406249403953552 - trainLoss: 0.6430341601371765\n",
      "cnt: 0 - valLoss: 0.6406216025352478 - trainLoss: 0.6430315375328064\n",
      "cnt: 0 - valLoss: 0.6406183242797852 - trainLoss: 0.6430288553237915\n",
      "cnt: 0 - valLoss: 0.640614926815033 - trainLoss: 0.6430260539054871\n",
      "cnt: 0 - valLoss: 0.6406120657920837 - trainLoss: 0.6430232524871826\n",
      "cnt: 0 - valLoss: 0.6406087279319763 - trainLoss: 0.6430208086967468\n",
      "cnt: 0 - valLoss: 0.6406053900718689 - trainLoss: 0.6430180668830872\n",
      "cnt: 0 - valLoss: 0.6406019926071167 - trainLoss: 0.6430152654647827\n",
      "cnt: 0 - valLoss: 0.6405991315841675 - trainLoss: 0.643012523651123\n",
      "cnt: 0 - valLoss: 0.6405957341194153 - trainLoss: 0.6430100798606873\n",
      "cnt: 0 - valLoss: 0.6405923962593079 - trainLoss: 0.6430072784423828\n",
      "cnt: 0 - valLoss: 0.6405891180038452 - trainLoss: 0.6430044770240784\n",
      "cnt: 0 - valLoss: 0.6405861377716064 - trainLoss: 0.6430017352104187\n",
      "cnt: 0 - valLoss: 0.640582799911499 - trainLoss: 0.6429992318153381\n",
      "cnt: 0 - valLoss: 0.6405794024467468 - trainLoss: 0.6429964900016785\n",
      "cnt: 0 - valLoss: 0.6405763626098633 - trainLoss: 0.642993688583374\n",
      "cnt: 0 - valLoss: 0.6405732035636902 - trainLoss: 0.6429910063743591\n",
      "cnt: 0 - valLoss: 0.6405698657035828 - trainLoss: 0.6429884433746338\n",
      "cnt: 0 - valLoss: 0.6405664682388306 - trainLoss: 0.6429856419563293\n",
      "cnt: 0 - valLoss: 0.640563428401947 - trainLoss: 0.6429827809333801\n",
      "cnt: 0 - valLoss: 0.6405602693557739 - trainLoss: 0.6429802179336548\n",
      "cnt: 0 - valLoss: 0.6405569314956665 - trainLoss: 0.6429775953292847\n",
      "cnt: 0 - valLoss: 0.6405535340309143 - trainLoss: 0.6429747939109802\n",
      "cnt: 0 - valLoss: 0.6405504941940308 - trainLoss: 0.6429719924926758\n",
      "cnt: 0 - valLoss: 0.6405472755432129 - trainLoss: 0.6429694890975952\n",
      "cnt: 0 - valLoss: 0.6405439972877502 - trainLoss: 0.6429667472839355\n",
      "cnt: 0 - valLoss: 0.6405405402183533 - trainLoss: 0.6429639458656311\n",
      "cnt: 0 - valLoss: 0.6405376195907593 - trainLoss: 0.6429610848426819\n",
      "cnt: 0 - valLoss: 0.6405344009399414 - trainLoss: 0.6429586410522461\n",
      "cnt: 0 - valLoss: 0.6405309438705444 - trainLoss: 0.6429558992385864\n",
      "cnt: 0 - valLoss: 0.6405277252197266 - trainLoss: 0.6429531574249268\n",
      "cnt: 0 - valLoss: 0.640524685382843 - trainLoss: 0.6429503560066223\n",
      "cnt: 0 - valLoss: 0.6405213475227356 - trainLoss: 0.6429479122161865\n",
      "cnt: 0 - valLoss: 0.6405180096626282 - trainLoss: 0.6429450511932373\n",
      "cnt: 0 - valLoss: 0.6405147314071655 - trainLoss: 0.6429423093795776\n",
      "cnt: 0 - valLoss: 0.6405117511749268 - trainLoss: 0.642939567565918\n",
      "cnt: 0 - valLoss: 0.6405084133148193 - trainLoss: 0.6429370045661926\n",
      "cnt: 0 - valLoss: 0.6405050158500671 - trainLoss: 0.642934262752533\n",
      "cnt: 0 - valLoss: 0.640501856803894 - trainLoss: 0.6429314017295837\n",
      "cnt: 0 - valLoss: 0.6404988169670105 - trainLoss: 0.6429287791252136\n",
      "cnt: 0 - valLoss: 0.6404953598976135 - trainLoss: 0.6429262161254883\n",
      "cnt: 0 - valLoss: 0.6404920220375061 - trainLoss: 0.6429234147071838\n",
      "cnt: 0 - valLoss: 0.640488862991333 - trainLoss: 0.6429205536842346\n",
      "cnt: 0 - valLoss: 0.6404858231544495 - trainLoss: 0.6429179906845093\n",
      "cnt: 0 - valLoss: 0.6404824256896973 - trainLoss: 0.6429153084754944\n",
      "cnt: 0 - valLoss: 0.6404790282249451 - trainLoss: 0.6429125666618347\n",
      "cnt: 0 - valLoss: 0.6404759883880615 - trainLoss: 0.6429097056388855\n",
      "cnt: 0 - valLoss: 0.6404727697372437 - trainLoss: 0.6429072022438049\n",
      "cnt: 0 - valLoss: 0.6404693722724915 - trainLoss: 0.64290452003479\n",
      "cnt: 0 - valLoss: 0.640466034412384 - trainLoss: 0.6429017186164856\n",
      "cnt: 0 - valLoss: 0.6404630541801453 - trainLoss: 0.6428988575935364\n",
      "cnt: 0 - valLoss: 0.6404597759246826 - trainLoss: 0.6428963541984558\n",
      "cnt: 0 - valLoss: 0.6404563784599304 - trainLoss: 0.6428936719894409\n",
      "cnt: 0 - valLoss: 0.640453040599823 - trainLoss: 0.6428908109664917\n",
      "cnt: 0 - valLoss: 0.6404501795768738 - trainLoss: 0.6428880095481873\n",
      "cnt: 0 - valLoss: 0.6404467225074768 - trainLoss: 0.6428855657577515\n",
      "cnt: 0 - valLoss: 0.6404433846473694 - trainLoss: 0.642882764339447\n",
      "cnt: 0 - valLoss: 0.6404401063919067 - trainLoss: 0.6428799033164978\n",
      "cnt: 0 - valLoss: 0.6404371857643127 - trainLoss: 0.6428771615028381\n",
      "cnt: 0 - valLoss: 0.6404337286949158 - trainLoss: 0.6428746581077576\n",
      "cnt: 0 - valLoss: 0.6404303908348083 - trainLoss: 0.6428718566894531\n",
      "cnt: 0 - valLoss: 0.6404271721839905 - trainLoss: 0.6428690552711487\n",
      "cnt: 0 - valLoss: 0.6404241323471069 - trainLoss: 0.6428663730621338\n",
      "cnt: 0 - valLoss: 0.6404206156730652 - trainLoss: 0.6428638100624084\n",
      "cnt: 0 - valLoss: 0.6404172778129578 - trainLoss: 0.6428609490394592\n",
      "cnt: 0 - valLoss: 0.6404142379760742 - trainLoss: 0.6428581476211548\n",
      "cnt: 0 - valLoss: 0.6404111385345459 - trainLoss: 0.6428555250167847\n",
      "cnt: 0 - valLoss: 0.6404076814651489 - trainLoss: 0.6428529024124146\n",
      "cnt: 0 - valLoss: 0.6404042840003967 - trainLoss: 0.6428500413894653\n",
      "cnt: 0 - valLoss: 0.6404013633728027 - trainLoss: 0.6428472399711609\n",
      "cnt: 0 - valLoss: 0.6403980255126953 - trainLoss: 0.6428447365760803\n",
      "cnt: 0 - valLoss: 0.6403946876525879 - trainLoss: 0.6428419947624207\n",
      "cnt: 0 - valLoss: 0.6403912305831909 - trainLoss: 0.6428391933441162\n",
      "cnt: 0 - valLoss: 0.6403883695602417 - trainLoss: 0.642836332321167\n",
      "cnt: 0 - valLoss: 0.6403849124908447 - trainLoss: 0.6428338885307312\n",
      "cnt: 0 - valLoss: 0.6403815746307373 - trainLoss: 0.6428310871124268\n",
      "cnt: 0 - valLoss: 0.6403781771659851 - trainLoss: 0.6428282856941223\n",
      "cnt: 0 - valLoss: 0.6403753161430359 - trainLoss: 0.6428254842758179\n",
      "cnt: 0 - valLoss: 0.6403718590736389 - trainLoss: 0.6428230404853821\n",
      "cnt: 0 - valLoss: 0.6403685808181763 - trainLoss: 0.6428202390670776\n",
      "cnt: 0 - valLoss: 0.6403653621673584 - trainLoss: 0.6428173780441284\n",
      "cnt: 0 - valLoss: 0.6403622627258301 - trainLoss: 0.6428146362304688\n",
      "cnt: 0 - valLoss: 0.6403589248657227 - trainLoss: 0.6428120732307434\n",
      "cnt: 0 - valLoss: 0.6403554677963257 - trainLoss: 0.642809271812439\n",
      "cnt: 0 - valLoss: 0.6403523087501526 - trainLoss: 0.6428064107894897\n",
      "cnt: 0 - valLoss: 0.6403492093086243 - trainLoss: 0.6428037881851196\n",
      "cnt: 0 - valLoss: 0.6403458118438721 - trainLoss: 0.6428012251853943\n",
      "cnt: 0 - valLoss: 0.6403423547744751 - trainLoss: 0.6427983641624451\n",
      "cnt: 0 - valLoss: 0.6403393745422363 - trainLoss: 0.6427955627441406\n",
      "cnt: 0 - valLoss: 0.6403361558914185 - trainLoss: 0.6427929997444153\n",
      "cnt: 0 - valLoss: 0.6403327584266663 - trainLoss: 0.6427903175354004\n",
      "cnt: 0 - valLoss: 0.6403293609619141 - trainLoss: 0.6427873969078064\n",
      "cnt: 0 - valLoss: 0.6403264403343201 - trainLoss: 0.6427846550941467\n",
      "cnt: 0 - valLoss: 0.6403229832649231 - trainLoss: 0.6427820920944214\n",
      "cnt: 0 - valLoss: 0.6403197646141052 - trainLoss: 0.6427792906761169\n",
      "cnt: 0 - valLoss: 0.6403162479400635 - trainLoss: 0.6427764892578125\n",
      "cnt: 0 - valLoss: 0.6403133869171143 - trainLoss: 0.6427737474441528\n",
      "cnt: 0 - valLoss: 0.6403100490570068 - trainLoss: 0.642771303653717\n",
      "cnt: 0 - valLoss: 0.6403066515922546 - trainLoss: 0.6427684426307678\n",
      "cnt: 0 - valLoss: 0.6403033137321472 - trainLoss: 0.6427655816078186\n",
      "cnt: 0 - valLoss: 0.6403002738952637 - trainLoss: 0.6427628397941589\n",
      "cnt: 0 - valLoss: 0.6402968764305115 - trainLoss: 0.6427603363990784\n",
      "cnt: 0 - valLoss: 0.6402934789657593 - trainLoss: 0.6427574753761292\n",
      "cnt: 0 - valLoss: 0.6402903199195862 - trainLoss: 0.6427546143531799\n",
      "cnt: 0 - valLoss: 0.6402872204780579 - trainLoss: 0.6427520513534546\n",
      "cnt: 0 - valLoss: 0.6402838230133057 - trainLoss: 0.6427494287490845\n",
      "cnt: 0 - valLoss: 0.6402803659439087 - trainLoss: 0.6427465677261353\n",
      "cnt: 0 - valLoss: 0.6402773857116699 - trainLoss: 0.642743706703186\n",
      "cnt: 0 - valLoss: 0.6402741074562073 - trainLoss: 0.6427410840988159\n",
      "cnt: 0 - valLoss: 0.6402707695960999 - trainLoss: 0.6427384614944458\n",
      "cnt: 0 - valLoss: 0.6402673125267029 - trainLoss: 0.6427356004714966\n",
      "cnt: 0 - valLoss: 0.6402643322944641 - trainLoss: 0.6427327394485474\n",
      "cnt: 0 - valLoss: 0.6402609944343567 - trainLoss: 0.6427302360534668\n",
      "cnt: 0 - valLoss: 0.6402576565742493 - trainLoss: 0.6427274942398071\n",
      "cnt: 0 - valLoss: 0.6402542591094971 - trainLoss: 0.6427246928215027\n",
      "cnt: 0 - valLoss: 0.6402513384819031 - trainLoss: 0.6427218317985535\n",
      "cnt: 0 - valLoss: 0.6402479410171509 - trainLoss: 0.6427193880081177\n",
      "cnt: 0 - valLoss: 0.6402445435523987 - trainLoss: 0.6427165269851685\n",
      "cnt: 0 - valLoss: 0.6402412056922913 - trainLoss: 0.6427136659622192\n",
      "cnt: 0 - valLoss: 0.6402381658554077 - trainLoss: 0.6427109837532043\n",
      "cnt: 0 - valLoss: 0.6402347683906555 - trainLoss: 0.642708420753479\n",
      "cnt: 0 - valLoss: 0.6402313709259033 - trainLoss: 0.6427055597305298\n",
      "cnt: 0 - valLoss: 0.6402282118797302 - trainLoss: 0.6427027583122253\n",
      "cnt: 0 - valLoss: 0.6402250528335571 - trainLoss: 0.6427000761032104\n",
      "cnt: 0 - valLoss: 0.6402216553688049 - trainLoss: 0.6426974534988403\n",
      "cnt: 0 - valLoss: 0.6402182579040527 - trainLoss: 0.6426945924758911\n",
      "cnt: 0 - valLoss: 0.6402152180671692 - trainLoss: 0.6426917910575867\n",
      "cnt: 0 - valLoss: 0.6402119398117065 - trainLoss: 0.6426891684532166\n",
      "cnt: 0 - valLoss: 0.6402086019515991 - trainLoss: 0.6426864862442017\n",
      "cnt: 0 - valLoss: 0.6402051448822021 - trainLoss: 0.6426836848258972\n",
      "cnt: 0 - valLoss: 0.6402021646499634 - trainLoss: 0.6426807641983032\n",
      "cnt: 0 - valLoss: 0.6401987671852112 - trainLoss: 0.6426782608032227\n",
      "cnt: 0 - valLoss: 0.6401954293251038 - trainLoss: 0.642675518989563\n",
      "cnt: 0 - valLoss: 0.6401919722557068 - trainLoss: 0.6426726579666138\n",
      "cnt: 0 - valLoss: 0.6401891112327576 - trainLoss: 0.6426697969436646\n",
      "cnt: 0 - valLoss: 0.6401857733726501 - trainLoss: 0.6426673531532288\n",
      "cnt: 0 - valLoss: 0.6401822566986084 - trainLoss: 0.6426644921302795\n",
      "cnt: 0 - valLoss: 0.6401790380477905 - trainLoss: 0.6426616907119751\n",
      "cnt: 0 - valLoss: 0.640175998210907 - trainLoss: 0.6426589488983154\n",
      "cnt: 0 - valLoss: 0.6401726603507996 - trainLoss: 0.6426563858985901\n",
      "cnt: 0 - valLoss: 0.6401691436767578 - trainLoss: 0.6426535248756409\n",
      "cnt: 0 - valLoss: 0.6401659846305847 - trainLoss: 0.6426506638526917\n",
      "cnt: 0 - valLoss: 0.6401628255844116 - trainLoss: 0.6426479816436768\n",
      "cnt: 0 - valLoss: 0.6401594281196594 - trainLoss: 0.6426452994346619\n",
      "cnt: 0 - valLoss: 0.6401559710502625 - trainLoss: 0.6426424980163574\n",
      "cnt: 0 - valLoss: 0.6401529312133789 - trainLoss: 0.642639696598053\n",
      "cnt: 0 - valLoss: 0.6401496529579163 - trainLoss: 0.6426371335983276\n",
      "cnt: 0 - valLoss: 0.6401462554931641 - trainLoss: 0.642634391784668\n",
      "cnt: 0 - valLoss: 0.6401427984237671 - trainLoss: 0.6426315307617188\n",
      "cnt: 0 - valLoss: 0.6401398777961731 - trainLoss: 0.6426286697387695\n",
      "cnt: 0 - valLoss: 0.6401364803314209 - trainLoss: 0.642626166343689\n",
      "cnt: 0 - valLoss: 0.6401330828666687 - trainLoss: 0.6426233649253845\n",
      "cnt: 0 - valLoss: 0.6401296854019165 - trainLoss: 0.6426205039024353\n",
      "cnt: 0 - valLoss: 0.6401268243789673 - trainLoss: 0.6426177024841309\n",
      "cnt: 0 - valLoss: 0.6401233077049255 - trainLoss: 0.6426152586936951\n",
      "cnt: 0 - valLoss: 0.6401199102401733 - trainLoss: 0.6426123380661011\n",
      "cnt: 0 - valLoss: 0.6401166319847107 - trainLoss: 0.6426094770431519\n",
      "cnt: 0 - valLoss: 0.6401135921478271 - trainLoss: 0.642606794834137\n",
      "cnt: 0 - valLoss: 0.640110194683075 - trainLoss: 0.6426042318344116\n",
      "cnt: 0 - valLoss: 0.640106737613678 - trainLoss: 0.6426013112068176\n",
      "cnt: 0 - valLoss: 0.6401036381721497 - trainLoss: 0.6425984501838684\n",
      "cnt: 0 - valLoss: 0.6401004195213318 - trainLoss: 0.6425958275794983\n",
      "cnt: 0 - valLoss: 0.6400970220565796 - trainLoss: 0.6425932049751282\n",
      "cnt: 0 - valLoss: 0.6400936245918274 - trainLoss: 0.642590343952179\n",
      "cnt: 0 - valLoss: 0.6400905847549438 - trainLoss: 0.6425875425338745\n",
      "cnt: 0 - valLoss: 0.6400871872901917 - trainLoss: 0.6425848603248596\n",
      "cnt: 0 - valLoss: 0.6400838494300842 - trainLoss: 0.6425821781158447\n",
      "cnt: 0 - valLoss: 0.6400803327560425 - trainLoss: 0.6425793170928955\n",
      "cnt: 0 - valLoss: 0.6400774717330933 - trainLoss: 0.6425764560699463\n",
      "cnt: 0 - valLoss: 0.6400740742683411 - trainLoss: 0.6425740122795105\n",
      "cnt: 0 - valLoss: 0.6400706171989441 - trainLoss: 0.6425711512565613\n",
      "cnt: 0 - valLoss: 0.6400672793388367 - trainLoss: 0.6425682902336121\n",
      "cnt: 0 - valLoss: 0.6400642991065979 - trainLoss: 0.6425655484199524\n",
      "cnt: 0 - valLoss: 0.6400607824325562 - trainLoss: 0.642562985420227\n",
      "cnt: 0 - valLoss: 0.640057384967804 - trainLoss: 0.6425601243972778\n",
      "cnt: 0 - valLoss: 0.6400542259216309 - trainLoss: 0.6425572633743286\n",
      "cnt: 0 - valLoss: 0.6400510668754578 - trainLoss: 0.6425545811653137\n",
      "cnt: 0 - valLoss: 0.6400476694107056 - trainLoss: 0.6425519585609436\n",
      "cnt: 0 - valLoss: 0.6400441527366638 - trainLoss: 0.6425490975379944\n",
      "cnt: 0 - valLoss: 0.640041172504425 - trainLoss: 0.6425462365150452\n",
      "cnt: 0 - valLoss: 0.6400378942489624 - trainLoss: 0.642543613910675\n",
      "cnt: 0 - valLoss: 0.6400344371795654 - trainLoss: 0.6425408720970154\n",
      "cnt: 0 - valLoss: 0.6400310397148132 - trainLoss: 0.6425380110740662\n",
      "cnt: 0 - valLoss: 0.6400279998779297 - trainLoss: 0.6425352096557617\n",
      "cnt: 0 - valLoss: 0.6400246620178223 - trainLoss: 0.6425326466560364\n",
      "cnt: 0 - valLoss: 0.6400212049484253 - trainLoss: 0.6425298452377319\n",
      "cnt: 0 - valLoss: 0.6400178670883179 - trainLoss: 0.6425270438194275\n",
      "cnt: 0 - valLoss: 0.6400148868560791 - trainLoss: 0.6425241827964783\n",
      "cnt: 0 - valLoss: 0.6400113701820374 - trainLoss: 0.6425216794013977\n",
      "cnt: 0 - valLoss: 0.6400080323219299 - trainLoss: 0.6425188183784485\n",
      "cnt: 0 - valLoss: 0.6400046944618225 - trainLoss: 0.6425159573554993\n",
      "cnt: 0 - valLoss: 0.640001654624939 - trainLoss: 0.6425132155418396\n",
      "cnt: 0 - valLoss: 0.6399982571601868 - trainLoss: 0.6425106525421143\n",
      "cnt: 0 - valLoss: 0.6399948000907898 - trainLoss: 0.6425077319145203\n",
      "cnt: 0 - valLoss: 0.6399917006492615 - trainLoss: 0.6425049304962158\n",
      "cnt: 0 - valLoss: 0.639988362789154 - trainLoss: 0.6425023078918457\n",
      "cnt: 0 - valLoss: 0.6399849653244019 - trainLoss: 0.642499566078186\n",
      "cnt: 0 - valLoss: 0.6399815678596497 - trainLoss: 0.6424967050552368\n",
      "cnt: 0 - valLoss: 0.6399785876274109 - trainLoss: 0.6424938440322876\n",
      "cnt: 0 - valLoss: 0.6399751901626587 - trainLoss: 0.642491340637207\n",
      "cnt: 0 - valLoss: 0.6399717330932617 - trainLoss: 0.6424885392189026\n",
      "cnt: 0 - valLoss: 0.6399683356285095 - trainLoss: 0.6424856781959534\n",
      "cnt: 0 - valLoss: 0.6399654150009155 - trainLoss: 0.6424827575683594\n",
      "cnt: 0 - valLoss: 0.6399619579315186 - trainLoss: 0.6424803137779236\n",
      "cnt: 0 - valLoss: 0.6399584412574768 - trainLoss: 0.6424774527549744\n",
      "cnt: 0 - valLoss: 0.6399552226066589 - trainLoss: 0.6424745321273804\n",
      "cnt: 0 - valLoss: 0.6399522423744202 - trainLoss: 0.6424717903137207\n",
      "cnt: 0 - valLoss: 0.6399487257003784 - trainLoss: 0.6424692273139954\n",
      "cnt: 0 - valLoss: 0.6399452686309814 - trainLoss: 0.6424663066864014\n",
      "cnt: 0 - valLoss: 0.6399421095848083 - trainLoss: 0.6424635052680969\n",
      "cnt: 0 - valLoss: 0.6399388909339905 - trainLoss: 0.642460823059082\n",
      "cnt: 0 - valLoss: 0.6399355530738831 - trainLoss: 0.6424581408500671\n",
      "cnt: 0 - valLoss: 0.6399320363998413 - trainLoss: 0.6424552798271179\n",
      "cnt: 0 - valLoss: 0.6399289965629578 - trainLoss: 0.6424524188041687\n",
      "cnt: 0 - valLoss: 0.6399256587028503 - trainLoss: 0.6424498558044434\n",
      "cnt: 0 - valLoss: 0.6399222612380981 - trainLoss: 0.6424470543861389\n",
      "cnt: 0 - valLoss: 0.6399187445640564 - trainLoss: 0.6424442529678345\n",
      "cnt: 0 - valLoss: 0.6399158239364624 - trainLoss: 0.6424413323402405\n",
      "cnt: 0 - valLoss: 0.6399124264717102 - trainLoss: 0.6424388289451599\n",
      "cnt: 0 - valLoss: 0.6399089694023132 - trainLoss: 0.6424360275268555\n",
      "cnt: 0 - valLoss: 0.6399056911468506 - trainLoss: 0.6424331665039062\n",
      "cnt: 0 - valLoss: 0.6399025917053223 - trainLoss: 0.6424303650856018\n",
      "cnt: 0 - valLoss: 0.6398991942405701 - trainLoss: 0.6424277424812317\n",
      "cnt: 0 - valLoss: 0.6398956179618835 - trainLoss: 0.6424248814582825\n",
      "cnt: 0 - valLoss: 0.6398925185203552 - trainLoss: 0.642422080039978\n",
      "cnt: 0 - valLoss: 0.6398892998695374 - trainLoss: 0.6424193978309631\n",
      "cnt: 0 - valLoss: 0.6398859620094299 - trainLoss: 0.6424166560173035\n",
      "cnt: 0 - valLoss: 0.639882504940033 - trainLoss: 0.642413854598999\n",
      "cnt: 0 - valLoss: 0.6398794054985046 - trainLoss: 0.642410933971405\n",
      "cnt: 0 - valLoss: 0.639876127243042 - trainLoss: 0.6424083709716797\n",
      "cnt: 0 - valLoss: 0.6398727297782898 - trainLoss: 0.64240562915802\n",
      "cnt: 0 - valLoss: 0.6398693323135376 - trainLoss: 0.6424028277397156\n",
      "cnt: 0 - valLoss: 0.6398662328720093 - trainLoss: 0.6423999667167664\n",
      "cnt: 0 - valLoss: 0.6398628950119019 - trainLoss: 0.642397403717041\n",
      "cnt: 0 - valLoss: 0.6398594975471497 - trainLoss: 0.6423946022987366\n",
      "cnt: 0 - valLoss: 0.639856219291687 - trainLoss: 0.6423916816711426\n",
      "cnt: 0 - valLoss: 0.6398530602455139 - trainLoss: 0.6423889398574829\n",
      "cnt: 0 - valLoss: 0.6398496031761169 - trainLoss: 0.6423863768577576\n",
      "cnt: 0 - valLoss: 0.6398462653160095 - trainLoss: 0.6423835754394531\n",
      "cnt: 0 - valLoss: 0.6398431062698364 - trainLoss: 0.6423806548118591\n",
      "cnt: 0 - valLoss: 0.6398398876190186 - trainLoss: 0.6423779726028442\n",
      "cnt: 0 - valLoss: 0.6398364305496216 - trainLoss: 0.6423753499984741\n",
      "cnt: 0 - valLoss: 0.6398329734802246 - trainLoss: 0.6423724293708801\n",
      "cnt: 0 - valLoss: 0.6398301124572754 - trainLoss: 0.6423695683479309\n",
      "cnt: 0 - valLoss: 0.6398265957832336 - trainLoss: 0.6423670053482056\n",
      "cnt: 0 - valLoss: 0.6398231983184814 - trainLoss: 0.6423642635345459\n",
      "cnt: 0 - valLoss: 0.6398198008537292 - trainLoss: 0.6423614025115967\n",
      "cnt: 0 - valLoss: 0.6398168802261353 - trainLoss: 0.6423585414886475\n",
      "cnt: 0 - valLoss: 0.6398133635520935 - trainLoss: 0.6423560380935669\n",
      "cnt: 0 - valLoss: 0.6398099660873413 - trainLoss: 0.6423531770706177\n",
      "cnt: 0 - valLoss: 0.6398066878318787 - trainLoss: 0.6423503160476685\n",
      "cnt: 0 - valLoss: 0.6398036479949951 - trainLoss: 0.642347514629364\n",
      "cnt: 0 - valLoss: 0.6398001909255981 - trainLoss: 0.6423449516296387\n",
      "cnt: 0 - valLoss: 0.6397966742515564 - trainLoss: 0.6423420906066895\n",
      "cnt: 0 - valLoss: 0.6397936344146729 - trainLoss: 0.6423392295837402\n",
      "cnt: 0 - valLoss: 0.639790415763855 - trainLoss: 0.6423365473747253\n",
      "cnt: 0 - valLoss: 0.6397868990898132 - trainLoss: 0.6423338055610657\n",
      "cnt: 0 - valLoss: 0.6397834420204163 - trainLoss: 0.6423310041427612\n",
      "cnt: 0 - valLoss: 0.6397805213928223 - trainLoss: 0.6423280835151672\n",
      "cnt: 0 - valLoss: 0.6397771239280701 - trainLoss: 0.6423255801200867\n",
      "cnt: 0 - valLoss: 0.6397736072540283 - trainLoss: 0.6423227190971375\n",
      "cnt: 0 - valLoss: 0.6397702693939209 - trainLoss: 0.6423198580741882\n",
      "cnt: 0 - valLoss: 0.6397672295570374 - trainLoss: 0.6423170566558838\n",
      "cnt: 0 - valLoss: 0.6397638320922852 - trainLoss: 0.6423145532608032\n",
      "cnt: 0 - valLoss: 0.6397603154182434 - trainLoss: 0.6423116326332092\n",
      "cnt: 0 - valLoss: 0.6397571563720703 - trainLoss: 0.64230877161026\n",
      "cnt: 0 - valLoss: 0.6397539973258972 - trainLoss: 0.6423060297966003\n",
      "cnt: 0 - valLoss: 0.639750599861145 - trainLoss: 0.6423034071922302\n",
      "cnt: 0 - valLoss: 0.6397470831871033 - trainLoss: 0.642300546169281\n",
      "cnt: 0 - valLoss: 0.639743983745575 - trainLoss: 0.642297625541687\n",
      "cnt: 0 - valLoss: 0.6397407054901123 - trainLoss: 0.6422950625419617\n",
      "cnt: 0 - valLoss: 0.6397373080253601 - trainLoss: 0.642292320728302\n",
      "cnt: 0 - valLoss: 0.6397338509559631 - trainLoss: 0.642289400100708\n",
      "cnt: 0 - valLoss: 0.6397308111190796 - trainLoss: 0.6422865390777588\n",
      "cnt: 0 - valLoss: 0.6397274136543274 - trainLoss: 0.642284095287323\n",
      "cnt: 0 - valLoss: 0.6397239565849304 - trainLoss: 0.642281174659729\n",
      "cnt: 0 - valLoss: 0.6397206783294678 - trainLoss: 0.642278254032135\n",
      "cnt: 0 - valLoss: 0.6397176384925842 - trainLoss: 0.6422755122184753\n",
      "cnt: 0 - valLoss: 0.6397141218185425 - trainLoss: 0.64227294921875\n",
      "cnt: 0 - valLoss: 0.6397107243537903 - trainLoss: 0.642270028591156\n",
      "cnt: 0 - valLoss: 0.6397075653076172 - trainLoss: 0.6422671675682068\n",
      "cnt: 0 - valLoss: 0.6397043466567993 - trainLoss: 0.6422644853591919\n",
      "cnt: 0 - valLoss: 0.6397008299827576 - trainLoss: 0.642261803150177\n",
      "cnt: 0 - valLoss: 0.6396974325180054 - trainLoss: 0.6422589421272278\n",
      "cnt: 0 - valLoss: 0.639694333076477 - trainLoss: 0.6422560214996338\n",
      "cnt: 0 - valLoss: 0.6396909952163696 - trainLoss: 0.6422534584999084\n",
      "cnt: 0 - valLoss: 0.6396875977516174 - trainLoss: 0.6422507166862488\n",
      "cnt: 0 - valLoss: 0.6396841406822205 - trainLoss: 0.6422477960586548\n",
      "cnt: 0 - valLoss: 0.6396811604499817 - trainLoss: 0.6422449350357056\n",
      "cnt: 0 - valLoss: 0.6396777033805847 - trainLoss: 0.6422423720359802\n",
      "cnt: 0 - valLoss: 0.6396742463111877 - trainLoss: 0.642239511013031\n",
      "cnt: 0 - valLoss: 0.6396709680557251 - trainLoss: 0.6422366499900818\n",
      "cnt: 0 - valLoss: 0.6396677494049072 - trainLoss: 0.6422338485717773\n",
      "cnt: 0 - valLoss: 0.639664351940155 - trainLoss: 0.642231285572052\n",
      "cnt: 0 - valLoss: 0.6396609544754028 - trainLoss: 0.642228364944458\n",
      "cnt: 0 - valLoss: 0.6396577954292297 - trainLoss: 0.6422255039215088\n",
      "cnt: 0 - valLoss: 0.6396545171737671 - trainLoss: 0.6422228217124939\n",
      "cnt: 0 - valLoss: 0.6396511197090149 - trainLoss: 0.642220139503479\n",
      "cnt: 0 - valLoss: 0.6396476030349731 - trainLoss: 0.642217218875885\n",
      "cnt: 0 - valLoss: 0.6396446824073792 - trainLoss: 0.6422143578529358\n",
      "cnt: 0 - valLoss: 0.6396411657333374 - trainLoss: 0.6422118544578552\n",
      "cnt: 0 - valLoss: 0.6396377682685852 - trainLoss: 0.642208993434906\n",
      "cnt: 0 - valLoss: 0.6396343111991882 - trainLoss: 0.642206072807312\n",
      "cnt: 0 - valLoss: 0.6396313905715942 - trainLoss: 0.6422032713890076\n",
      "cnt: 0 - valLoss: 0.6396278142929077 - trainLoss: 0.6422007083892822\n",
      "cnt: 0 - valLoss: 0.6396244168281555 - trainLoss: 0.6421977877616882\n",
      "cnt: 0 - valLoss: 0.6396211981773376 - trainLoss: 0.642194926738739\n",
      "cnt: 0 - valLoss: 0.6396180391311646 - trainLoss: 0.6421922445297241\n",
      "cnt: 0 - valLoss: 0.639614462852478 - trainLoss: 0.6421895623207092\n",
      "cnt: 0 - valLoss: 0.6396111249923706 - trainLoss: 0.6421866416931152\n",
      "cnt: 0 - valLoss: 0.6396080851554871 - trainLoss: 0.6421837210655212\n",
      "cnt: 0 - valLoss: 0.6396047472953796 - trainLoss: 0.6421811580657959\n",
      "cnt: 0 - valLoss: 0.6396011710166931 - trainLoss: 0.6421783566474915\n",
      "cnt: 0 - valLoss: 0.6395978331565857 - trainLoss: 0.6421754360198975\n",
      "cnt: 0 - valLoss: 0.6395947933197021 - trainLoss: 0.642172634601593\n",
      "cnt: 0 - valLoss: 0.63959139585495 - trainLoss: 0.6421701312065125\n",
      "cnt: 0 - valLoss: 0.6395878195762634 - trainLoss: 0.6421672105789185\n",
      "cnt: 0 - valLoss: 0.6395846009254456 - trainLoss: 0.6421642899513245\n",
      "cnt: 0 - valLoss: 0.6395815014839172 - trainLoss: 0.6421615481376648\n",
      "cnt: 0 - valLoss: 0.6395780444145203 - trainLoss: 0.6421589851379395\n",
      "cnt: 0 - valLoss: 0.6395745277404785 - trainLoss: 0.6421560645103455\n",
      "cnt: 0 - valLoss: 0.6395713686943054 - trainLoss: 0.6421530842781067\n",
      "cnt: 0 - valLoss: 0.6395681500434875 - trainLoss: 0.6421505212783813\n",
      "cnt: 0 - valLoss: 0.6395646929740906 - trainLoss: 0.6421477198600769\n",
      "cnt: 0 - valLoss: 0.639561116695404 - trainLoss: 0.6421448588371277\n",
      "cnt: 0 - valLoss: 0.6395581960678101 - trainLoss: 0.6421419382095337\n",
      "cnt: 0 - valLoss: 0.6395547986030579 - trainLoss: 0.6421394348144531\n",
      "cnt: 0 - valLoss: 0.6395512223243713 - trainLoss: 0.6421365141868591\n",
      "cnt: 0 - valLoss: 0.6395479440689087 - trainLoss: 0.6421335935592651\n",
      "cnt: 0 - valLoss: 0.6395448446273804 - trainLoss: 0.6421308517456055\n",
      "cnt: 0 - valLoss: 0.6395413875579834 - trainLoss: 0.6421282291412354\n",
      "cnt: 0 - valLoss: 0.6395378708839417 - trainLoss: 0.6421253085136414\n",
      "cnt: 0 - valLoss: 0.6395347714424133 - trainLoss: 0.6421224474906921\n",
      "cnt: 0 - valLoss: 0.6395314931869507 - trainLoss: 0.6421197056770325\n",
      "cnt: 0 - valLoss: 0.6395279765129089 - trainLoss: 0.6421170830726624\n",
      "cnt: 0 - valLoss: 0.639524519443512 - trainLoss: 0.6421141624450684\n",
      "cnt: 0 - valLoss: 0.6395214796066284 - trainLoss: 0.6421112418174744\n",
      "cnt: 0 - valLoss: 0.639518141746521 - trainLoss: 0.642108678817749\n",
      "cnt: 0 - valLoss: 0.6395146250724792 - trainLoss: 0.6421058177947998\n",
      "cnt: 0 - valLoss: 0.6395111680030823 - trainLoss: 0.6421029567718506\n",
      "cnt: 0 - valLoss: 0.6395082473754883 - trainLoss: 0.6421000957489014\n",
      "cnt: 0 - valLoss: 0.6395047307014465 - trainLoss: 0.642097532749176\n",
      "cnt: 0 - valLoss: 0.6395012736320496 - trainLoss: 0.642094612121582\n",
      "cnt: 0 - valLoss: 0.6394979357719421 - trainLoss: 0.6420917510986328\n",
      "cnt: 0 - valLoss: 0.6394947171211243 - trainLoss: 0.6420890092849731\n",
      "cnt: 0 - valLoss: 0.6394913196563721 - trainLoss: 0.6420862674713135\n",
      "cnt: 0 - valLoss: 0.6394878625869751 - trainLoss: 0.6420834064483643\n",
      "cnt: 0 - valLoss: 0.6394847631454468 - trainLoss: 0.6420804858207703\n",
      "cnt: 0 - valLoss: 0.6394814252853394 - trainLoss: 0.6420778632164001\n",
      "cnt: 0 - valLoss: 0.6394779086112976 - trainLoss: 0.6420751214027405\n",
      "cnt: 0 - valLoss: 0.6394743919372559 - trainLoss: 0.6420722007751465\n",
      "cnt: 0 - valLoss: 0.6394714713096619 - trainLoss: 0.6420692801475525\n",
      "cnt: 0 - valLoss: 0.6394680142402649 - trainLoss: 0.6420667171478271\n",
      "cnt: 0 - valLoss: 0.6394645571708679 - trainLoss: 0.6420638561248779\n",
      "cnt: 0 - valLoss: 0.6394612193107605 - trainLoss: 0.6420609951019287\n",
      "cnt: 0 - valLoss: 0.6394580602645874 - trainLoss: 0.6420581936836243\n",
      "cnt: 0 - valLoss: 0.6394546031951904 - trainLoss: 0.6420555710792542\n",
      "cnt: 0 - valLoss: 0.6394512057304382 - trainLoss: 0.6420527100563049\n",
      "cnt: 0 - valLoss: 0.6394479274749756 - trainLoss: 0.6420497298240662\n",
      "cnt: 0 - valLoss: 0.6394446492195129 - trainLoss: 0.6420470476150513\n",
      "cnt: 0 - valLoss: 0.6394412517547607 - trainLoss: 0.6420443654060364\n",
      "cnt: 0 - valLoss: 0.639437735080719 - trainLoss: 0.6420414447784424\n",
      "cnt: 0 - valLoss: 0.6394347548484802 - trainLoss: 0.6420385241508484\n",
      "cnt: 0 - valLoss: 0.6394312381744385 - trainLoss: 0.6420360207557678\n",
      "cnt: 0 - valLoss: 0.6394277811050415 - trainLoss: 0.6420331001281738\n",
      "cnt: 0 - valLoss: 0.6394244432449341 - trainLoss: 0.6420301795005798\n",
      "cnt: 0 - valLoss: 0.639421284198761 - trainLoss: 0.6420274376869202\n",
      "cnt: 0 - valLoss: 0.639417827129364 - trainLoss: 0.6420247554779053\n",
      "cnt: 0 - valLoss: 0.639414370059967 - trainLoss: 0.6420218348503113\n",
      "cnt: 0 - valLoss: 0.6394111514091492 - trainLoss: 0.6420188546180725\n",
      "cnt: 0 - valLoss: 0.6394078731536865 - trainLoss: 0.6420162916183472\n",
      "cnt: 0 - valLoss: 0.6394043564796448 - trainLoss: 0.6420135498046875\n",
      "cnt: 0 - valLoss: 0.6394009590148926 - trainLoss: 0.6420105695724487\n",
      "cnt: 0 - valLoss: 0.639397919178009 - trainLoss: 0.6420076489448547\n",
      "cnt: 0 - valLoss: 0.6393944621086121 - trainLoss: 0.6420051455497742\n",
      "cnt: 0 - valLoss: 0.6393909454345703 - trainLoss: 0.6420022249221802\n",
      "cnt: 0 - valLoss: 0.6393876671791077 - trainLoss: 0.641999363899231\n",
      "cnt: 0 - valLoss: 0.6393845081329346 - trainLoss: 0.6419965624809265\n",
      "cnt: 0 - valLoss: 0.6393809914588928 - trainLoss: 0.6419938802719116\n",
      "cnt: 0 - valLoss: 0.6393775343894958 - trainLoss: 0.6419909000396729\n",
      "cnt: 0 - valLoss: 0.639374315738678 - trainLoss: 0.6419880390167236\n",
      "cnt: 0 - valLoss: 0.6393710374832153 - trainLoss: 0.6419854760169983\n",
      "cnt: 0 - valLoss: 0.6393675804138184 - trainLoss: 0.6419826149940491\n",
      "cnt: 0 - valLoss: 0.6393640637397766 - trainLoss: 0.6419796943664551\n",
      "cnt: 0 - valLoss: 0.6393610239028931 - trainLoss: 0.6419768333435059\n",
      "cnt: 0 - valLoss: 0.6393575668334961 - trainLoss: 0.6419742703437805\n",
      "cnt: 0 - valLoss: 0.6393541097640991 - trainLoss: 0.6419714093208313\n",
      "cnt: 0 - valLoss: 0.6393507719039917 - trainLoss: 0.6419684290885925\n",
      "cnt: 0 - valLoss: 0.6393476128578186 - trainLoss: 0.6419656276702881\n",
      "cnt: 0 - valLoss: 0.6393440961837769 - trainLoss: 0.641963005065918\n",
      "cnt: 0 - valLoss: 0.6393406391143799 - trainLoss: 0.641960084438324\n",
      "cnt: 0 - valLoss: 0.639337420463562 - trainLoss: 0.64195716381073\n",
      "cnt: 0 - valLoss: 0.6393342018127441 - trainLoss: 0.6419544816017151\n",
      "cnt: 0 - valLoss: 0.6393306851387024 - trainLoss: 0.6419516801834106\n",
      "cnt: 0 - valLoss: 0.6393272280693054 - trainLoss: 0.6419487595558167\n",
      "cnt: 0 - valLoss: 0.6393241882324219 - trainLoss: 0.6419458985328674\n",
      "cnt: 0 - valLoss: 0.6393207311630249 - trainLoss: 0.6419433951377869\n",
      "cnt: 0 - valLoss: 0.6393172144889832 - trainLoss: 0.6419404745101929\n",
      "cnt: 0 - valLoss: 0.6393138766288757 - trainLoss: 0.6419375538825989\n",
      "cnt: 0 - valLoss: 0.6393106579780579 - trainLoss: 0.6419346928596497\n",
      "cnt: 0 - valLoss: 0.6393072605133057 - trainLoss: 0.6419320702552795\n",
      "cnt: 0 - valLoss: 0.6393038034439087 - trainLoss: 0.6419291496276855\n",
      "cnt: 0 - valLoss: 0.6393004655838013 - trainLoss: 0.6419262290000916\n",
      "cnt: 0 - valLoss: 0.6392972469329834 - trainLoss: 0.6419234871864319\n",
      "cnt: 0 - valLoss: 0.6392937302589417 - trainLoss: 0.641920804977417\n",
      "cnt: 0 - valLoss: 0.6392902731895447 - trainLoss: 0.641917884349823\n",
      "cnt: 0 - valLoss: 0.6392871737480164 - trainLoss: 0.6419149041175842\n",
      "cnt: 0 - valLoss: 0.6392837762832642 - trainLoss: 0.6419124007225037\n",
      "cnt: 0 - valLoss: 0.6392803192138672 - trainLoss: 0.6419094800949097\n",
      "cnt: 0 - valLoss: 0.639276921749115 - trainLoss: 0.6419065594673157\n",
      "cnt: 0 - valLoss: 0.6392737627029419 - trainLoss: 0.6419036984443665\n",
      "cnt: 0 - valLoss: 0.6392703056335449 - trainLoss: 0.6419010758399963\n",
      "cnt: 0 - valLoss: 0.6392667889595032 - trainLoss: 0.6418981552124023\n",
      "cnt: 0 - valLoss: 0.6392635107040405 - trainLoss: 0.6418952345848083\n",
      "cnt: 0 - valLoss: 0.6392602920532227 - trainLoss: 0.6418926119804382\n",
      "cnt: 0 - valLoss: 0.6392568349838257 - trainLoss: 0.6418898105621338\n",
      "cnt: 0 - valLoss: 0.6392533183097839 - trainLoss: 0.6418868899345398\n",
      "cnt: 0 - valLoss: 0.6392502784729004 - trainLoss: 0.6418839693069458\n",
      "cnt: 0 - valLoss: 0.6392467617988586 - trainLoss: 0.6418814063072205\n",
      "cnt: 0 - valLoss: 0.6392433047294617 - trainLoss: 0.6418784856796265\n",
      "cnt: 0 - valLoss: 0.6392399072647095 - trainLoss: 0.6418756246566772\n",
      "cnt: 0 - valLoss: 0.6392368078231812 - trainLoss: 0.641872763633728\n",
      "cnt: 0 - valLoss: 0.6392332911491394 - trainLoss: 0.6418700814247131\n",
      "cnt: 0 - valLoss: 0.6392298340797424 - trainLoss: 0.6418671607971191\n",
      "cnt: 0 - valLoss: 0.6392265558242798 - trainLoss: 0.6418642997741699\n",
      "cnt: 0 - valLoss: 0.6392232775688171 - trainLoss: 0.6418615579605103\n",
      "cnt: 0 - valLoss: 0.6392198204994202 - trainLoss: 0.6418588161468506\n",
      "cnt: 0 - valLoss: 0.6392163038253784 - trainLoss: 0.641855776309967\n",
      "cnt: 0 - valLoss: 0.6392132043838501 - trainLoss: 0.6418529152870178\n",
      "cnt: 0 - valLoss: 0.6392098069190979 - trainLoss: 0.6418503522872925\n",
      "cnt: 0 - valLoss: 0.6392062902450562 - trainLoss: 0.6418474316596985\n",
      "cnt: 0 - valLoss: 0.6392028331756592 - trainLoss: 0.6418445706367493\n",
      "cnt: 0 - valLoss: 0.6391997337341309 - trainLoss: 0.6418416500091553\n",
      "cnt: 0 - valLoss: 0.6391962170600891 - trainLoss: 0.6418390274047852\n",
      "cnt: 0 - valLoss: 0.6391927599906921 - trainLoss: 0.6418361067771912\n",
      "cnt: 0 - valLoss: 0.6391896605491638 - trainLoss: 0.6418331265449524\n",
      "cnt: 0 - valLoss: 0.6391862630844116 - trainLoss: 0.6418304443359375\n",
      "cnt: 0 - valLoss: 0.6391826868057251 - trainLoss: 0.6418277025222778\n",
      "cnt: 0 - valLoss: 0.6391792893409729 - trainLoss: 0.6418247818946838\n",
      "cnt: 0 - valLoss: 0.6391763687133789 - trainLoss: 0.6418218016624451\n",
      "cnt: 0 - valLoss: 0.6391726732254028 - trainLoss: 0.6418193578720093\n",
      "cnt: 0 - valLoss: 0.6391692161560059 - trainLoss: 0.6418163180351257\n",
      "cnt: 0 - valLoss: 0.6391659379005432 - trainLoss: 0.6418134570121765\n",
      "cnt: 0 - valLoss: 0.6391628384590149 - trainLoss: 0.6418106555938721\n",
      "cnt: 0 - valLoss: 0.6391591429710388 - trainLoss: 0.6418079733848572\n",
      "cnt: 0 - valLoss: 0.6391556262969971 - trainLoss: 0.6418050527572632\n",
      "cnt: 0 - valLoss: 0.6391525864601135 - trainLoss: 0.6418020725250244\n",
      "cnt: 0 - valLoss: 0.6391491889953613 - trainLoss: 0.6417994499206543\n",
      "cnt: 0 - valLoss: 0.6391457319259644 - trainLoss: 0.6417965888977051\n",
      "cnt: 0 - valLoss: 0.6391421556472778 - trainLoss: 0.6417936682701111\n",
      "cnt: 0 - valLoss: 0.6391391158103943 - trainLoss: 0.6417907476425171\n",
      "cnt: 0 - valLoss: 0.6391356587409973 - trainLoss: 0.6417881846427917\n",
      "cnt: 0 - valLoss: 0.6391322016716003 - trainLoss: 0.6417852640151978\n",
      "cnt: 0 - valLoss: 0.6391288042068481 - trainLoss: 0.6417823433876038\n",
      "cnt: 0 - valLoss: 0.639125645160675 - trainLoss: 0.6417795419692993\n",
      "cnt: 0 - valLoss: 0.6391220688819885 - trainLoss: 0.6417768597602844\n",
      "cnt: 0 - valLoss: 0.6391186118125916 - trainLoss: 0.6417738795280457\n",
      "cnt: 0 - valLoss: 0.639115571975708 - trainLoss: 0.6417709589004517\n",
      "cnt: 0 - valLoss: 0.6391120553016663 - trainLoss: 0.6417683362960815\n",
      "cnt: 0 - valLoss: 0.6391085386276245 - trainLoss: 0.6417654752731323\n",
      "cnt: 0 - valLoss: 0.6391050815582275 - trainLoss: 0.6417624950408936\n",
      "cnt: 0 - valLoss: 0.6391021013259888 - trainLoss: 0.6417596340179443\n",
      "cnt: 0 - valLoss: 0.6390985250473022 - trainLoss: 0.641757071018219\n",
      "cnt: 0 - valLoss: 0.6390950083732605 - trainLoss: 0.6417540907859802\n",
      "cnt: 0 - valLoss: 0.6390917897224426 - trainLoss: 0.6417511701583862\n",
      "cnt: 0 - valLoss: 0.63908851146698 - trainLoss: 0.6417484283447266\n",
      "cnt: 0 - valLoss: 0.6390848755836487 - trainLoss: 0.6417457461357117\n",
      "cnt: 0 - valLoss: 0.6390814185142517 - trainLoss: 0.6417427659034729\n",
      "cnt: 0 - valLoss: 0.6390783786773682 - trainLoss: 0.6417397856712341\n",
      "cnt: 0 - valLoss: 0.6390749216079712 - trainLoss: 0.6417372226715088\n",
      "cnt: 0 - valLoss: 0.6390714645385742 - trainLoss: 0.6417343616485596\n",
      "cnt: 0 - valLoss: 0.6390680074691772 - trainLoss: 0.641731321811676\n",
      "cnt: 0 - valLoss: 0.6390649080276489 - trainLoss: 0.6417285799980164\n",
      "cnt: 0 - valLoss: 0.6390613317489624 - trainLoss: 0.6417258381843567\n",
      "cnt: 0 - valLoss: 0.6390578746795654 - trainLoss: 0.6417229175567627\n",
      "cnt: 0 - valLoss: 0.639054536819458 - trainLoss: 0.6417199373245239\n",
      "cnt: 0 - valLoss: 0.6390512585639954 - trainLoss: 0.641717255115509\n",
      "cnt: 0 - valLoss: 0.6390478014945984 - trainLoss: 0.6417144536972046\n",
      "cnt: 0 - valLoss: 0.6390441060066223 - trainLoss: 0.6417114734649658\n",
      "cnt: 0 - valLoss: 0.6390411853790283 - trainLoss: 0.6417086124420166\n",
      "cnt: 0 - valLoss: 0.6390376687049866 - trainLoss: 0.6417059302330017\n",
      "cnt: 0 - valLoss: 0.6390340328216553 - trainLoss: 0.6417030692100525\n",
      "cnt: 0 - valLoss: 0.6390307545661926 - trainLoss: 0.641700029373169\n",
      "cnt: 0 - valLoss: 0.6390276551246643 - trainLoss: 0.6416972875595093\n",
      "cnt: 0 - valLoss: 0.6390239596366882 - trainLoss: 0.6416946053504944\n",
      "cnt: 0 - valLoss: 0.6390205025672913 - trainLoss: 0.6416916847229004\n",
      "cnt: 0 - valLoss: 0.6390173435211182 - trainLoss: 0.6416886448860168\n",
      "cnt: 0 - valLoss: 0.6390138864517212 - trainLoss: 0.641685962677002\n",
      "cnt: 0 - valLoss: 0.6390103697776794 - trainLoss: 0.6416831612586975\n",
      "cnt: 0 - valLoss: 0.6390069127082825 - trainLoss: 0.6416801810264587\n",
      "cnt: 0 - valLoss: 0.6390038132667542 - trainLoss: 0.6416772603988647\n",
      "cnt: 0 - valLoss: 0.6390002369880676 - trainLoss: 0.6416746973991394\n",
      "cnt: 0 - valLoss: 0.6389967799186707 - trainLoss: 0.6416717171669006\n",
      "cnt: 0 - valLoss: 0.6389934420585632 - trainLoss: 0.6416687369346619\n",
      "cnt: 0 - valLoss: 0.6389901638031006 - trainLoss: 0.6416659951210022\n",
      "cnt: 0 - valLoss: 0.6389867067337036 - trainLoss: 0.6416631937026978\n",
      "cnt: 0 - valLoss: 0.6389830112457275 - trainLoss: 0.6416602730751038\n",
      "cnt: 0 - valLoss: 0.638979971408844 - trainLoss: 0.641657292842865\n",
      "cnt: 0 - valLoss: 0.638976514339447 - trainLoss: 0.6416546106338501\n",
      "cnt: 0 - valLoss: 0.6389729976654053 - trainLoss: 0.6416518092155457\n",
      "cnt: 0 - valLoss: 0.6389694809913635 - trainLoss: 0.6416488289833069\n",
      "cnt: 0 - valLoss: 0.6389663219451904 - trainLoss: 0.6416459679603577\n",
      "cnt: 0 - valLoss: 0.6389628052711487 - trainLoss: 0.6416432857513428\n",
      "cnt: 0 - valLoss: 0.6389592885971069 - trainLoss: 0.6416403651237488\n",
      "cnt: 0 - valLoss: 0.6389560103416443 - trainLoss: 0.64163738489151\n",
      "cnt: 0 - valLoss: 0.6389527320861816 - trainLoss: 0.6416345834732056\n",
      "cnt: 0 - valLoss: 0.6389492154121399 - trainLoss: 0.6416318416595459\n",
      "cnt: 0 - valLoss: 0.6389455795288086 - trainLoss: 0.6416289210319519\n",
      "cnt: 0 - valLoss: 0.6389426589012146 - trainLoss: 0.6416259407997131\n",
      "cnt: 0 - valLoss: 0.6389390826225281 - trainLoss: 0.6416233777999878\n",
      "cnt: 0 - valLoss: 0.638935387134552 - trainLoss: 0.6416204571723938\n",
      "cnt: 0 - valLoss: 0.6389321088790894 - trainLoss: 0.6416174173355103\n",
      "cnt: 0 - valLoss: 0.638929009437561 - trainLoss: 0.6416146755218506\n",
      "cnt: 0 - valLoss: 0.6389253735542297 - trainLoss: 0.6416119337081909\n",
      "cnt: 0 - valLoss: 0.6389217972755432 - trainLoss: 0.6416089534759521\n",
      "cnt: 0 - valLoss: 0.6389187574386597 - trainLoss: 0.6416059732437134\n",
      "cnt: 0 - valLoss: 0.6389151811599731 - trainLoss: 0.6416032910346985\n",
      "cnt: 0 - valLoss: 0.6389117240905762 - trainLoss: 0.6416004300117493\n",
      "cnt: 0 - valLoss: 0.6389081478118896 - trainLoss: 0.6415974497795105\n",
      "cnt: 0 - valLoss: 0.6389051079750061 - trainLoss: 0.6415945291519165\n",
      "cnt: 0 - valLoss: 0.6389015913009644 - trainLoss: 0.6415919065475464\n",
      "cnt: 0 - valLoss: 0.6388979554176331 - trainLoss: 0.6415889859199524\n",
      "cnt: 0 - valLoss: 0.6388947367668152 - trainLoss: 0.6415860056877136\n",
      "cnt: 0 - valLoss: 0.6388915181159973 - trainLoss: 0.641583263874054\n",
      "cnt: 0 - valLoss: 0.6388877630233765 - trainLoss: 0.6415804028511047\n",
      "cnt: 0 - valLoss: 0.6388843059539795 - trainLoss: 0.6415774822235107\n",
      "cnt: 0 - valLoss: 0.6388812065124512 - trainLoss: 0.6415744423866272\n",
      "cnt: 0 - valLoss: 0.6388776898384094 - trainLoss: 0.6415718793869019\n",
      "cnt: 0 - valLoss: 0.6388741731643677 - trainLoss: 0.6415689587593079\n",
      "cnt: 0 - valLoss: 0.6388707756996155 - trainLoss: 0.6415659785270691\n",
      "cnt: 0 - valLoss: 0.6388675570487976 - trainLoss: 0.6415631175041199\n",
      "cnt: 0 - valLoss: 0.6388639807701111 - trainLoss: 0.641560435295105\n",
      "cnt: 0 - valLoss: 0.6388603448867798 - trainLoss: 0.6415574550628662\n",
      "cnt: 0 - valLoss: 0.6388572454452515 - trainLoss: 0.6415544748306274\n",
      "cnt: 0 - valLoss: 0.6388538479804993 - trainLoss: 0.6415517926216125\n",
      "cnt: 0 - valLoss: 0.638850212097168 - trainLoss: 0.6415489315986633\n",
      "cnt: 0 - valLoss: 0.6388466954231262 - trainLoss: 0.6415458917617798\n",
      "cnt: 0 - valLoss: 0.6388437151908875 - trainLoss: 0.6415430307388306\n",
      "cnt: 0 - valLoss: 0.6388400793075562 - trainLoss: 0.6415404677391052\n",
      "cnt: 0 - valLoss: 0.6388365626335144 - trainLoss: 0.6415374279022217\n",
      "cnt: 0 - valLoss: 0.6388332843780518 - trainLoss: 0.6415344476699829\n",
      "cnt: 0 - valLoss: 0.6388299465179443 - trainLoss: 0.6415317058563232\n",
      "cnt: 0 - valLoss: 0.6388263702392578 - trainLoss: 0.6415289044380188\n",
      "cnt: 0 - valLoss: 0.6388227343559265 - trainLoss: 0.6415259838104248\n",
      "cnt: 0 - valLoss: 0.6388197541236877 - trainLoss: 0.6415229439735413\n",
      "cnt: 0 - valLoss: 0.638816237449646 - trainLoss: 0.6415203213691711\n",
      "cnt: 0 - valLoss: 0.6388126015663147 - trainLoss: 0.6415174007415771\n",
      "cnt: 0 - valLoss: 0.6388092041015625 - trainLoss: 0.6415144801139832\n",
      "cnt: 0 - valLoss: 0.6388061046600342 - trainLoss: 0.6415115594863892\n",
      "cnt: 0 - valLoss: 0.6388024687767029 - trainLoss: 0.6415088772773743\n",
      "cnt: 0 - valLoss: 0.6387988924980164 - trainLoss: 0.6415059566497803\n",
      "cnt: 0 - valLoss: 0.6387957334518433 - trainLoss: 0.6415029168128967\n",
      "cnt: 0 - valLoss: 0.6387922763824463 - trainLoss: 0.6415001749992371\n",
      "cnt: 0 - valLoss: 0.6387887597084045 - trainLoss: 0.6414973735809326\n",
      "cnt: 0 - valLoss: 0.6387852430343628 - trainLoss: 0.6414944529533386\n",
      "cnt: 0 - valLoss: 0.6387821435928345 - trainLoss: 0.6414914131164551\n",
      "cnt: 0 - valLoss: 0.6387786269187927 - trainLoss: 0.6414888501167297\n",
      "cnt: 0 - valLoss: 0.6387749314308167 - trainLoss: 0.6414858102798462\n",
      "cnt: 0 - valLoss: 0.638771653175354 - trainLoss: 0.6414828896522522\n",
      "cnt: 0 - valLoss: 0.6387683749198914 - trainLoss: 0.641480028629303\n",
      "cnt: 0 - valLoss: 0.6387647390365601 - trainLoss: 0.6414772868156433\n",
      "cnt: 0 - valLoss: 0.6387612223625183 - trainLoss: 0.6414742469787598\n",
      "cnt: 0 - valLoss: 0.63875812292099 - trainLoss: 0.6414713859558105\n",
      "cnt: 0 - valLoss: 0.6387546062469482 - trainLoss: 0.6414687037467957\n",
      "cnt: 0 - valLoss: 0.6387510299682617 - trainLoss: 0.6414657235145569\n",
      "cnt: 0 - valLoss: 0.6387476325035095 - trainLoss: 0.6414628028869629\n",
      "cnt: 0 - valLoss: 0.6387443542480469 - trainLoss: 0.6414598822593689\n",
      "cnt: 0 - valLoss: 0.6387408375740051 - trainLoss: 0.641457200050354\n",
      "cnt: 0 - valLoss: 0.6387372016906738 - trainLoss: 0.6414542198181152\n",
      "cnt: 0 - valLoss: 0.6387340426445007 - trainLoss: 0.6414512395858765\n",
      "cnt: 0 - valLoss: 0.6387307047843933 - trainLoss: 0.6414484977722168\n",
      "cnt: 0 - valLoss: 0.6387270092964172 - trainLoss: 0.6414456963539124\n",
      "cnt: 0 - valLoss: 0.6387235522270203 - trainLoss: 0.6414426565170288\n",
      "cnt: 0 - valLoss: 0.6387204527854919 - trainLoss: 0.6414397358894348\n",
      "cnt: 0 - valLoss: 0.6387168169021606 - trainLoss: 0.6414371132850647\n",
      "cnt: 0 - valLoss: 0.6387133002281189 - trainLoss: 0.6414341330528259\n",
      "cnt: 0 - valLoss: 0.6387100219726562 - trainLoss: 0.6414311528205872\n",
      "cnt: 0 - valLoss: 0.6387066841125488 - trainLoss: 0.6414283514022827\n",
      "cnt: 0 - valLoss: 0.6387031078338623 - trainLoss: 0.6414255499839783\n",
      "cnt: 0 - valLoss: 0.638699471950531 - trainLoss: 0.6414225697517395\n",
      "cnt: 0 - valLoss: 0.6386964321136475 - trainLoss: 0.6414195895195007\n",
      "cnt: 0 - valLoss: 0.6386929154396057 - trainLoss: 0.6414170265197754\n",
      "cnt: 0 - valLoss: 0.6386892199516296 - trainLoss: 0.6414139866828918\n",
      "cnt: 0 - valLoss: 0.6386858820915222 - trainLoss: 0.6414109468460083\n",
      "cnt: 0 - valLoss: 0.6386826634407043 - trainLoss: 0.6414081454277039\n",
      "cnt: 0 - valLoss: 0.6386790871620178 - trainLoss: 0.6414054036140442\n",
      "cnt: 0 - valLoss: 0.6386754512786865 - trainLoss: 0.6414024233818054\n",
      "cnt: 0 - valLoss: 0.6386722326278687 - trainLoss: 0.6413994431495667\n",
      "cnt: 0 - valLoss: 0.6386688947677612 - trainLoss: 0.6413967609405518\n",
      "cnt: 0 - valLoss: 0.6386653184890747 - trainLoss: 0.6413938999176025\n",
      "cnt: 0 - valLoss: 0.6386617422103882 - trainLoss: 0.641390860080719\n",
      "cnt: 0 - valLoss: 0.6386587023735046 - trainLoss: 0.6413878798484802\n",
      "cnt: 0 - valLoss: 0.6386550664901733 - trainLoss: 0.6413852572441101\n",
      "cnt: 0 - valLoss: 0.6386513710021973 - trainLoss: 0.6413822770118713\n",
      "cnt: 0 - valLoss: 0.6386482119560242 - trainLoss: 0.6413792371749878\n",
      "cnt: 0 - valLoss: 0.6386449337005615 - trainLoss: 0.6413764953613281\n",
      "cnt: 0 - valLoss: 0.6386412978172302 - trainLoss: 0.6413736939430237\n",
      "cnt: 0 - valLoss: 0.6386376619338989 - trainLoss: 0.6413707137107849\n",
      "cnt: 0 - valLoss: 0.6386345624923706 - trainLoss: 0.6413676738739014\n",
      "cnt: 0 - valLoss: 0.6386309862136841 - trainLoss: 0.641365110874176\n",
      "cnt: 0 - valLoss: 0.6386274695396423 - trainLoss: 0.6413620710372925\n",
      "cnt: 0 - valLoss: 0.6386240124702454 - trainLoss: 0.6413591504096985\n",
      "cnt: 0 - valLoss: 0.6386207938194275 - trainLoss: 0.6413562893867493\n",
      "cnt: 0 - valLoss: 0.6386172771453857 - trainLoss: 0.6413535475730896\n",
      "cnt: 0 - valLoss: 0.6386135816574097 - trainLoss: 0.641350507736206\n",
      "cnt: 0 - valLoss: 0.6386104822158813 - trainLoss: 0.6413474678993225\n",
      "cnt: 0 - valLoss: 0.6386069655418396 - trainLoss: 0.6413448452949524\n",
      "cnt: 0 - valLoss: 0.6386033892631531 - trainLoss: 0.6413419842720032\n",
      "cnt: 0 - valLoss: 0.6385999321937561 - trainLoss: 0.6413389444351196\n",
      "cnt: 0 - valLoss: 0.6385967135429382 - trainLoss: 0.6413360238075256\n",
      "cnt: 0 - valLoss: 0.6385930776596069 - trainLoss: 0.6413333415985107\n",
      "cnt: 0 - valLoss: 0.6385895609855652 - trainLoss: 0.6413303017616272\n",
      "cnt: 0 - valLoss: 0.6385863423347473 - trainLoss: 0.6413273215293884\n",
      "cnt: 0 - valLoss: 0.6385828852653503 - trainLoss: 0.6413245797157288\n",
      "cnt: 0 - valLoss: 0.6385791897773743 - trainLoss: 0.6413216590881348\n",
      "cnt: 0 - valLoss: 0.6385756731033325 - trainLoss: 0.6413187384605408\n",
      "cnt: 0 - valLoss: 0.6385726928710938 - trainLoss: 0.641315758228302\n",
      "cnt: 0 - valLoss: 0.6385689973831177 - trainLoss: 0.6413131356239319\n",
      "cnt: 0 - valLoss: 0.6385654211044312 - trainLoss: 0.6413100957870483\n",
      "cnt: 0 - valLoss: 0.6385621428489685 - trainLoss: 0.6413071155548096\n",
      "cnt: 0 - valLoss: 0.6385587453842163 - trainLoss: 0.6413043141365051\n",
      "cnt: 0 - valLoss: 0.6385552287101746 - trainLoss: 0.6413014531135559\n",
      "cnt: 0 - valLoss: 0.6385515332221985 - trainLoss: 0.6412985324859619\n",
      "cnt: 0 - valLoss: 0.6385484933853149 - trainLoss: 0.6412955522537231\n",
      "cnt: 0 - valLoss: 0.6385449171066284 - trainLoss: 0.6412928700447083\n",
      "cnt: 0 - valLoss: 0.6385412216186523 - trainLoss: 0.6412898898124695\n",
      "cnt: 0 - valLoss: 0.6385378837585449 - trainLoss: 0.6412869095802307\n",
      "cnt: 0 - valLoss: 0.6385347247123718 - trainLoss: 0.6412840485572815\n",
      "cnt: 0 - valLoss: 0.6385310292243958 - trainLoss: 0.6412813067436218\n",
      "cnt: 0 - valLoss: 0.6385274529457092 - trainLoss: 0.6412782669067383\n",
      "cnt: 0 - valLoss: 0.6385242342948914 - trainLoss: 0.6412752866744995\n",
      "cnt: 0 - valLoss: 0.6385207176208496 - trainLoss: 0.6412726044654846\n",
      "cnt: 0 - valLoss: 0.6385171413421631 - trainLoss: 0.6412696242332458\n",
      "cnt: 0 - valLoss: 0.6385136246681213 - trainLoss: 0.6412665843963623\n",
      "cnt: 0 - valLoss: 0.6385104656219482 - trainLoss: 0.6412637233734131\n",
      "cnt: 0 - valLoss: 0.6385068893432617 - trainLoss: 0.6412610411643982\n",
      "cnt: 0 - valLoss: 0.6385032534599304 - trainLoss: 0.6412580609321594\n",
      "cnt: 0 - valLoss: 0.6385000944137573 - trainLoss: 0.6412550210952759\n",
      "cnt: 0 - valLoss: 0.6384965777397156 - trainLoss: 0.6412522792816162\n",
      "cnt: 0 - valLoss: 0.638493001461029 - trainLoss: 0.641249418258667\n",
      "cnt: 0 - valLoss: 0.6384894847869873 - trainLoss: 0.6412463784217834\n",
      "cnt: 0 - valLoss: 0.6384862661361694 - trainLoss: 0.6412433981895447\n",
      "cnt: 0 - valLoss: 0.6384825706481934 - trainLoss: 0.6412407159805298\n",
      "cnt: 0 - valLoss: 0.6384790539741516 - trainLoss: 0.641237735748291\n",
      "cnt: 0 - valLoss: 0.6384757161140442 - trainLoss: 0.6412346959114075\n",
      "cnt: 0 - valLoss: 0.638472318649292 - trainLoss: 0.6412319540977478\n",
      "cnt: 0 - valLoss: 0.6384686827659607 - trainLoss: 0.6412291526794434\n",
      "cnt: 0 - valLoss: 0.6384650468826294 - trainLoss: 0.6412261128425598\n",
      "cnt: 0 - valLoss: 0.6384620070457458 - trainLoss: 0.6412230730056763\n",
      "cnt: 0 - valLoss: 0.6384583115577698 - trainLoss: 0.6412204504013062\n",
      "cnt: 0 - valLoss: 0.6384547352790833 - trainLoss: 0.6412174701690674\n",
      "cnt: 0 - valLoss: 0.6384514570236206 - trainLoss: 0.6412144303321838\n",
      "cnt: 0 - valLoss: 0.6384480595588684 - trainLoss: 0.6412116289138794\n",
      "cnt: 0 - valLoss: 0.6384444236755371 - trainLoss: 0.641208827495575\n",
      "cnt: 0 - valLoss: 0.6384407877922058 - trainLoss: 0.6412057876586914\n",
      "cnt: 0 - valLoss: 0.6384377479553223 - trainLoss: 0.6412027478218079\n",
      "cnt: 0 - valLoss: 0.638434112071991 - trainLoss: 0.6412001252174377\n",
      "cnt: 0 - valLoss: 0.6384304165840149 - trainLoss: 0.641197144985199\n",
      "cnt: 0 - valLoss: 0.6384270191192627 - trainLoss: 0.6411941051483154\n",
      "cnt: 0 - valLoss: 0.6384238004684448 - trainLoss: 0.6411912441253662\n",
      "cnt: 0 - valLoss: 0.638420045375824 - trainLoss: 0.6411885023117065\n",
      "cnt: 0 - valLoss: 0.6384164690971375 - trainLoss: 0.641185462474823\n",
      "cnt: 0 - valLoss: 0.6384133100509644 - trainLoss: 0.6411824822425842\n",
      "cnt: 0 - valLoss: 0.6384097337722778 - trainLoss: 0.6411796808242798\n",
      "cnt: 0 - valLoss: 0.6384061574935913 - trainLoss: 0.6411768198013306\n",
      "cnt: 0 - valLoss: 0.6384026408195496 - trainLoss: 0.641173779964447\n",
      "cnt: 0 - valLoss: 0.6383993625640869 - trainLoss: 0.641170859336853\n",
      "cnt: 0 - valLoss: 0.6383957862854004 - trainLoss: 0.6411681771278381\n",
      "cnt: 0 - valLoss: 0.6383921504020691 - trainLoss: 0.6411651372909546\n",
      "cnt: 0 - valLoss: 0.6383889317512512 - trainLoss: 0.641162097454071\n",
      "cnt: 0 - valLoss: 0.6383854150772095 - trainLoss: 0.6411593556404114\n",
      "cnt: 0 - valLoss: 0.6383817195892334 - trainLoss: 0.6411564946174622\n",
      "cnt: 0 - valLoss: 0.6383782625198364 - trainLoss: 0.6411533951759338\n",
      "cnt: 0 - valLoss: 0.6383750438690186 - trainLoss: 0.6411504745483398\n",
      "cnt: 0 - valLoss: 0.6383714079856873 - trainLoss: 0.641147792339325\n",
      "cnt: 0 - valLoss: 0.6383677124977112 - trainLoss: 0.6411448121070862\n",
      "cnt: 0 - valLoss: 0.6383644938468933 - trainLoss: 0.6411417722702026\n",
      "cnt: 0 - valLoss: 0.6383611559867859 - trainLoss: 0.641139030456543\n",
      "cnt: 0 - valLoss: 0.6383573412895203 - trainLoss: 0.641136109828949\n",
      "cnt: 0 - valLoss: 0.6383538246154785 - trainLoss: 0.6411330699920654\n",
      "cnt: 0 - valLoss: 0.6383507251739502 - trainLoss: 0.6411300301551819\n",
      "cnt: 0 - valLoss: 0.6383470892906189 - trainLoss: 0.6411274671554565\n",
      "cnt: 0 - valLoss: 0.6383434534072876 - trainLoss: 0.641124427318573\n",
      "cnt: 0 - valLoss: 0.6383400559425354 - trainLoss: 0.6411213874816895\n",
      "cnt: 0 - valLoss: 0.638336718082428 - trainLoss: 0.6411185264587402\n",
      "cnt: 0 - valLoss: 0.6383330821990967 - trainLoss: 0.6411157846450806\n",
      "cnt: 0 - valLoss: 0.6383293867111206 - trainLoss: 0.6411126852035522\n",
      "cnt: 0 - valLoss: 0.6383264064788818 - trainLoss: 0.6411097049713135\n",
      "cnt: 0 - valLoss: 0.638322651386261 - trainLoss: 0.6411070227622986\n",
      "cnt: 0 - valLoss: 0.6383190155029297 - trainLoss: 0.6411040425300598\n",
      "cnt: 0 - valLoss: 0.638315737247467 - trainLoss: 0.6411010026931763\n",
      "cnt: 0 - valLoss: 0.6383122801780701 - trainLoss: 0.6410982012748718\n",
      "cnt: 0 - valLoss: 0.6383086442947388 - trainLoss: 0.6410952806472778\n",
      "cnt: 0 - valLoss: 0.638305127620697 - trainLoss: 0.6410923600196838\n",
      "cnt: 0 - valLoss: 0.6383019089698792 - trainLoss: 0.6410892605781555\n",
      "cnt: 0 - valLoss: 0.6382982730865479 - trainLoss: 0.6410866379737854\n",
      "cnt: 0 - valLoss: 0.6382946372032166 - trainLoss: 0.6410836577415466\n",
      "cnt: 0 - valLoss: 0.6382911801338196 - trainLoss: 0.6410805583000183\n",
      "cnt: 0 - valLoss: 0.6382879614830017 - trainLoss: 0.6410776972770691\n",
      "cnt: 0 - valLoss: 0.6382842063903809 - trainLoss: 0.6410749554634094\n",
      "cnt: 0 - valLoss: 0.6382806301116943 - trainLoss: 0.6410718560218811\n",
      "cnt: 0 - valLoss: 0.6382773518562317 - trainLoss: 0.6410688757896423\n",
      "cnt: 0 - valLoss: 0.6382738947868347 - trainLoss: 0.6410661339759827\n",
      "cnt: 0 - valLoss: 0.6382702589035034 - trainLoss: 0.6410632133483887\n",
      "cnt: 0 - valLoss: 0.6382667422294617 - trainLoss: 0.6410602331161499\n",
      "cnt: 0 - valLoss: 0.638263463973999 - trainLoss: 0.6410572528839111\n",
      "cnt: 0 - valLoss: 0.6382598876953125 - trainLoss: 0.6410545110702515\n",
      "cnt: 0 - valLoss: 0.6382561326026917 - trainLoss: 0.6410514712333679\n",
      "cnt: 0 - valLoss: 0.6382530331611633 - trainLoss: 0.6410484313964844\n",
      "cnt: 0 - valLoss: 0.638249397277832 - trainLoss: 0.6410457491874695\n",
      "cnt: 0 - valLoss: 0.6382457613945007 - trainLoss: 0.6410427689552307\n",
      "cnt: 0 - valLoss: 0.6382423043251038 - trainLoss: 0.6410397291183472\n",
      "cnt: 0 - valLoss: 0.6382389664649963 - trainLoss: 0.6410368084907532\n",
      "cnt: 0 - valLoss: 0.6382354497909546 - trainLoss: 0.6410341262817383\n",
      "cnt: 0 - valLoss: 0.6382316946983337 - trainLoss: 0.64103102684021\n",
      "cnt: 0 - valLoss: 0.6382285356521606 - trainLoss: 0.6410279870033264\n",
      "cnt: 0 - valLoss: 0.6382250189781189 - trainLoss: 0.641025185585022\n",
      "cnt: 0 - valLoss: 0.638221263885498 - trainLoss: 0.6410223245620728\n",
      "cnt: 0 - valLoss: 0.6382177472114563 - trainLoss: 0.6410192251205444\n",
      "cnt: 0 - valLoss: 0.6382144689559937 - trainLoss: 0.6410163044929504\n",
      "cnt: 0 - valLoss: 0.6382108926773071 - trainLoss: 0.6410136222839355\n",
      "cnt: 0 - valLoss: 0.6382072567939758 - trainLoss: 0.641010582447052\n",
      "cnt: 0 - valLoss: 0.6382039189338684 - trainLoss: 0.6410074830055237\n",
      "cnt: 0 - valLoss: 0.6382004618644714 - trainLoss: 0.641004741191864\n",
      "cnt: 0 - valLoss: 0.6381968259811401 - trainLoss: 0.64100182056427\n",
      "cnt: 0 - valLoss: 0.6381931900978088 - trainLoss: 0.6409987807273865\n",
      "cnt: 0 - valLoss: 0.6381900906562805 - trainLoss: 0.6409958004951477\n",
      "cnt: 0 - valLoss: 0.6381863355636597 - trainLoss: 0.6409931182861328\n",
      "cnt: 0 - valLoss: 0.6381827592849731 - trainLoss: 0.6409900784492493\n",
      "cnt: 0 - valLoss: 0.6381794810295105 - trainLoss: 0.6409870386123657\n",
      "cnt: 0 - valLoss: 0.6381759643554688 - trainLoss: 0.6409841775894165\n",
      "cnt: 0 - valLoss: 0.6381723880767822 - trainLoss: 0.6409813761711121\n",
      "cnt: 0 - valLoss: 0.6381686925888062 - trainLoss: 0.6409782767295837\n",
      "cnt: 0 - valLoss: 0.6381655335426331 - trainLoss: 0.640975296497345\n",
      "cnt: 0 - valLoss: 0.6381618976593018 - trainLoss: 0.6409726142883301\n",
      "cnt: 0 - valLoss: 0.6381582021713257 - trainLoss: 0.6409695744514465\n",
      "cnt: 0 - valLoss: 0.6381548643112183 - trainLoss: 0.640966534614563\n",
      "cnt: 0 - valLoss: 0.6381514668464661 - trainLoss: 0.6409636735916138\n",
      "cnt: 0 - valLoss: 0.6381478309631348 - trainLoss: 0.6409608125686646\n",
      "cnt: 0 - valLoss: 0.6381441354751587 - trainLoss: 0.6409577131271362\n",
      "cnt: 0 - valLoss: 0.6381409764289856 - trainLoss: 0.6409547328948975\n",
      "cnt: 0 - valLoss: 0.6381374001502991 - trainLoss: 0.6409521102905273\n",
      "cnt: 0 - valLoss: 0.6381336450576782 - trainLoss: 0.6409490704536438\n",
      "cnt: 0 - valLoss: 0.6381303668022156 - trainLoss: 0.6409460306167603\n",
      "cnt: 0 - valLoss: 0.6381269693374634 - trainLoss: 0.640943169593811\n",
      "cnt: 0 - valLoss: 0.6381232738494873 - trainLoss: 0.6409403681755066\n",
      "cnt: 0 - valLoss: 0.6381195783615112 - trainLoss: 0.6409372091293335\n",
      "cnt: 0 - valLoss: 0.6381163597106934 - trainLoss: 0.6409342288970947\n",
      "cnt: 0 - valLoss: 0.6381128430366516 - trainLoss: 0.6409316062927246\n",
      "cnt: 0 - valLoss: 0.6381092071533203 - trainLoss: 0.6409285664558411\n",
      "cnt: 0 - valLoss: 0.6381056904792786 - trainLoss: 0.6409254670143127\n",
      "cnt: 0 - valLoss: 0.6381023526191711 - trainLoss: 0.6409226655960083\n",
      "cnt: 0 - valLoss: 0.6380986571311951 - trainLoss: 0.6409197449684143\n",
      "cnt: 0 - valLoss: 0.638094961643219 - trainLoss: 0.6409167647361755\n",
      "cnt: 0 - valLoss: 0.6380919814109802 - trainLoss: 0.6409136652946472\n",
      "cnt: 0 - valLoss: 0.6380881667137146 - trainLoss: 0.6409111022949219\n",
      "cnt: 0 - valLoss: 0.6380845904350281 - trainLoss: 0.6409079432487488\n",
      "cnt: 0 - valLoss: 0.6380811929702759 - trainLoss: 0.6409049034118652\n",
      "cnt: 0 - valLoss: 0.6380777955055237 - trainLoss: 0.640902042388916\n",
      "cnt: 0 - valLoss: 0.6380741000175476 - trainLoss: 0.6408991813659668\n",
      "cnt: 0 - valLoss: 0.6380703449249268 - trainLoss: 0.6408961415290833\n",
      "cnt: 0 - valLoss: 0.638067364692688 - trainLoss: 0.6408930420875549\n",
      "cnt: 0 - valLoss: 0.6380636096000671 - trainLoss: 0.6408904790878296\n",
      "cnt: 0 - valLoss: 0.6380599141120911 - trainLoss: 0.6408873796463013\n",
      "cnt: 0 - valLoss: 0.6380566358566284 - trainLoss: 0.6408843398094177\n",
      "cnt: 0 - valLoss: 0.6380531191825867 - trainLoss: 0.6408814191818237\n",
      "cnt: 0 - valLoss: 0.6380495429039001 - trainLoss: 0.6408786177635193\n",
      "cnt: 0 - valLoss: 0.6380459070205688 - trainLoss: 0.6408755779266357\n",
      "cnt: 0 - valLoss: 0.638042688369751 - trainLoss: 0.6408725380897522\n",
      "cnt: 0 - valLoss: 0.6380390524864197 - trainLoss: 0.6408697962760925\n",
      "cnt: 0 - valLoss: 0.6380352973937988 - trainLoss: 0.6408668160438538\n",
      "cnt: 0 - valLoss: 0.6380319595336914 - trainLoss: 0.6408637762069702\n",
      "cnt: 0 - valLoss: 0.638028621673584 - trainLoss: 0.6408608555793762\n",
      "cnt: 0 - valLoss: 0.6380248069763184 - trainLoss: 0.6408580541610718\n",
      "cnt: 0 - valLoss: 0.6380212306976318 - trainLoss: 0.6408549547195435\n",
      "cnt: 0 - valLoss: 0.638018012046814 - trainLoss: 0.6408519148826599\n",
      "cnt: 0 - valLoss: 0.6380143761634827 - trainLoss: 0.6408491730690002\n",
      "cnt: 0 - valLoss: 0.6380107402801514 - trainLoss: 0.6408461928367615\n",
      "cnt: 0 - valLoss: 0.6380072832107544 - trainLoss: 0.6408430933952332\n",
      "cnt: 0 - valLoss: 0.638003945350647 - trainLoss: 0.6408402323722839\n",
      "cnt: 0 - valLoss: 0.6380001902580261 - trainLoss: 0.6408374309539795\n",
      "cnt: 0 - valLoss: 0.6379965543746948 - trainLoss: 0.6408343315124512\n",
      "cnt: 0 - valLoss: 0.6379934549331665 - trainLoss: 0.6408312916755676\n",
      "cnt: 0 - valLoss: 0.6379897594451904 - trainLoss: 0.6408286094665527\n",
      "cnt: 0 - valLoss: 0.6379860639572144 - trainLoss: 0.6408255696296692\n",
      "cnt: 0 - valLoss: 0.6379825472831726 - trainLoss: 0.6408224701881409\n",
      "cnt: 0 - valLoss: 0.6379793286323547 - trainLoss: 0.6408196687698364\n",
      "cnt: 0 - valLoss: 0.6379756331443787 - trainLoss: 0.6408168077468872\n",
      "cnt: 0 - valLoss: 0.6379718780517578 - trainLoss: 0.6408137083053589\n",
      "cnt: 0 - valLoss: 0.6379687786102295 - trainLoss: 0.6408106684684753\n",
      "cnt: 0 - valLoss: 0.6379650235176086 - trainLoss: 0.6408079266548157\n",
      "cnt: 0 - valLoss: 0.6379613280296326 - trainLoss: 0.6408049464225769\n",
      "cnt: 0 - valLoss: 0.6379579901695251 - trainLoss: 0.6408018469810486\n",
      "cnt: 0 - valLoss: 0.6379545331001282 - trainLoss: 0.6407989859580994\n",
      "cnt: 0 - valLoss: 0.6379508376121521 - trainLoss: 0.6407961249351501\n",
      "cnt: 0 - valLoss: 0.637947142124176 - trainLoss: 0.6407930850982666\n",
      "cnt: 0 - valLoss: 0.6379441022872925 - trainLoss: 0.6407899856567383\n",
      "cnt: 0 - valLoss: 0.6379404664039612 - trainLoss: 0.6407873034477234\n",
      "cnt: 0 - valLoss: 0.6379367113113403 - trainLoss: 0.6407842636108398\n",
      "cnt: 0 - valLoss: 0.6379332542419434 - trainLoss: 0.6407811641693115\n",
      "cnt: 0 - valLoss: 0.6379297375679016 - trainLoss: 0.6407783031463623\n",
      "cnt: 0 - valLoss: 0.6379261016845703 - trainLoss: 0.6407754421234131\n",
      "cnt: 0 - valLoss: 0.6379225254058838 - trainLoss: 0.6407724022865295\n",
      "cnt: 0 - valLoss: 0.6379193067550659 - trainLoss: 0.640769362449646\n",
      "cnt: 0 - valLoss: 0.6379156112670898 - trainLoss: 0.6407666802406311\n",
      "cnt: 0 - valLoss: 0.6379119157791138 - trainLoss: 0.6407635807991028\n",
      "cnt: 0 - valLoss: 0.6379085183143616 - trainLoss: 0.6407604813575745\n",
      "cnt: 0 - valLoss: 0.6379050612449646 - trainLoss: 0.6407576203346252\n",
      "cnt: 0 - valLoss: 0.6379014253616333 - trainLoss: 0.640754759311676\n",
      "cnt: 0 - valLoss: 0.6378976702690125 - trainLoss: 0.6407517194747925\n",
      "cnt: 0 - valLoss: 0.6378945708274841 - trainLoss: 0.6407486796379089\n",
      "cnt: 0 - valLoss: 0.6378909349441528 - trainLoss: 0.6407459378242493\n",
      "cnt: 0 - valLoss: 0.6378871202468872 - trainLoss: 0.6407428979873657\n",
      "cnt: 0 - valLoss: 0.6378837823867798 - trainLoss: 0.6407398581504822\n",
      "cnt: 0 - valLoss: 0.6378804445266724 - trainLoss: 0.640736997127533\n",
      "cnt: 0 - valLoss: 0.6378766298294067 - trainLoss: 0.640734076499939\n",
      "cnt: 0 - valLoss: 0.6378729939460754 - trainLoss: 0.6407310366630554\n",
      "cnt: 0 - valLoss: 0.6378699541091919 - trainLoss: 0.6407279372215271\n",
      "cnt: 0 - valLoss: 0.6378660798072815 - trainLoss: 0.6407252550125122\n",
      "cnt: 0 - valLoss: 0.6378624439239502 - trainLoss: 0.6407222747802734\n",
      "cnt: 0 - valLoss: 0.6378591060638428 - trainLoss: 0.6407190561294556\n",
      "cnt: 0 - valLoss: 0.637855589389801 - trainLoss: 0.6407162547111511\n",
      "cnt: 0 - valLoss: 0.6378519535064697 - trainLoss: 0.6407134532928467\n",
      "cnt: 0 - valLoss: 0.6378482580184937 - trainLoss: 0.6407102942466736\n",
      "cnt: 0 - valLoss: 0.6378450989723206 - trainLoss: 0.64070725440979\n",
      "cnt: 0 - valLoss: 0.6378414034843445 - trainLoss: 0.6407045125961304\n",
      "cnt: 0 - valLoss: 0.6378377079963684 - trainLoss: 0.6407014727592468\n",
      "cnt: 0 - valLoss: 0.6378342509269714 - trainLoss: 0.6406984329223633\n",
      "cnt: 0 - valLoss: 0.6378308534622192 - trainLoss: 0.6406955122947693\n",
      "cnt: 0 - valLoss: 0.6378270983695984 - trainLoss: 0.6406926512718201\n",
      "cnt: 0 - valLoss: 0.6378234624862671 - trainLoss: 0.6406895518302917\n",
      "cnt: 0 - valLoss: 0.6378203630447388 - trainLoss: 0.640686571598053\n",
      "cnt: 0 - valLoss: 0.6378166079521179 - trainLoss: 0.6406838297843933\n",
      "cnt: 0 - valLoss: 0.6378128528594971 - trainLoss: 0.640680730342865\n",
      "cnt: 0 - valLoss: 0.6378095746040344 - trainLoss: 0.6406777501106262\n",
      "cnt: 0 - valLoss: 0.6378061175346375 - trainLoss: 0.640674889087677\n",
      "cnt: 0 - valLoss: 0.6378023028373718 - trainLoss: 0.6406719088554382\n",
      "cnt: 0 - valLoss: 0.6377986669540405 - trainLoss: 0.6406688094139099\n",
      "cnt: 0 - valLoss: 0.6377954483032227 - trainLoss: 0.6406658291816711\n",
      "cnt: 0 - valLoss: 0.6377918124198914 - trainLoss: 0.6406631469726562\n",
      "cnt: 0 - valLoss: 0.6377880573272705 - trainLoss: 0.6406599879264832\n",
      "cnt: 0 - valLoss: 0.6377847194671631 - trainLoss: 0.6406569480895996\n",
      "cnt: 0 - valLoss: 0.6377812027931213 - trainLoss: 0.6406540870666504\n",
      "cnt: 0 - valLoss: 0.63777756690979 - trainLoss: 0.6406511664390564\n",
      "cnt: 0 - valLoss: 0.6377739310264587 - trainLoss: 0.6406480669975281\n",
      "cnt: 0 - valLoss: 0.6377706527709961 - trainLoss: 0.6406450867652893\n",
      "cnt: 0 - valLoss: 0.63776695728302 - trainLoss: 0.6406423449516296\n",
      "cnt: 0 - valLoss: 0.6377632021903992 - trainLoss: 0.6406392455101013\n",
      "cnt: 0 - valLoss: 0.6377599239349365 - trainLoss: 0.640636146068573\n",
      "cnt: 0 - valLoss: 0.6377564072608948 - trainLoss: 0.6406333446502686\n",
      "cnt: 0 - valLoss: 0.6377526521682739 - trainLoss: 0.6406303644180298\n",
      "cnt: 0 - valLoss: 0.6377490758895874 - trainLoss: 0.6406273245811462\n",
      "cnt: 0 - valLoss: 0.6377459168434143 - trainLoss: 0.6406243443489075\n",
      "cnt: 0 - valLoss: 0.6377421021461487 - trainLoss: 0.6406216025352478\n",
      "cnt: 0 - valLoss: 0.6377384066581726 - trainLoss: 0.6406185030937195\n",
      "cnt: 0 - valLoss: 0.6377351880073547 - trainLoss: 0.6406154036521912\n",
      "cnt: 0 - valLoss: 0.6377314925193787 - trainLoss: 0.6406126022338867\n",
      "cnt: 0 - valLoss: 0.6377278566360474 - trainLoss: 0.6406096816062927\n",
      "cnt: 0 - valLoss: 0.6377242803573608 - trainLoss: 0.6406065225601196\n",
      "cnt: 0 - valLoss: 0.6377209424972534 - trainLoss: 0.6406035423278809\n",
      "cnt: 0 - valLoss: 0.6377172470092773 - trainLoss: 0.6406008005142212\n",
      "cnt: 0 - valLoss: 0.6377135515213013 - trainLoss: 0.6405976414680481\n",
      "cnt: 0 - valLoss: 0.6377103328704834 - trainLoss: 0.6405945420265198\n",
      "cnt: 0 - valLoss: 0.6377066373825073 - trainLoss: 0.6405918002128601\n",
      "cnt: 0 - valLoss: 0.6377028822898865 - trainLoss: 0.6405887603759766\n",
      "cnt: 0 - valLoss: 0.6376994252204895 - trainLoss: 0.6405856609344482\n",
      "cnt: 0 - valLoss: 0.6376960873603821 - trainLoss: 0.6405827403068542\n",
      "cnt: 0 - valLoss: 0.6376923322677612 - trainLoss: 0.6405799984931946\n",
      "cnt: 0 - valLoss: 0.6376886367797852 - trainLoss: 0.6405768394470215\n",
      "cnt: 0 - valLoss: 0.6376854777336121 - trainLoss: 0.6405737400054932\n",
      "cnt: 0 - valLoss: 0.6376817226409912 - trainLoss: 0.6405710577964783\n",
      "cnt: 0 - valLoss: 0.6376779675483704 - trainLoss: 0.64056795835495\n",
      "cnt: 0 - valLoss: 0.6376746296882629 - trainLoss: 0.6405648589134216\n",
      "cnt: 0 - valLoss: 0.6376711130142212 - trainLoss: 0.6405619382858276\n",
      "cnt: 0 - valLoss: 0.6376674175262451 - trainLoss: 0.6405591368675232\n",
      "cnt: 0 - valLoss: 0.6376637816429138 - trainLoss: 0.6405559778213501\n",
      "cnt: 0 - valLoss: 0.6376605033874512 - trainLoss: 0.6405528783798218\n",
      "cnt: 0 - valLoss: 0.6376568078994751 - trainLoss: 0.6405501961708069\n",
      "cnt: 0 - valLoss: 0.637653112411499 - trainLoss: 0.6405470967292786\n",
      "cnt: 0 - valLoss: 0.6376495957374573 - trainLoss: 0.6405439972877502\n",
      "cnt: 0 - valLoss: 0.6376462578773499 - trainLoss: 0.640541136264801\n",
      "cnt: 0 - valLoss: 0.6376426219940186 - trainLoss: 0.640538215637207\n",
      "cnt: 0 - valLoss: 0.6376388072967529 - trainLoss: 0.6405351758003235\n",
      "cnt: 0 - valLoss: 0.6376356482505798 - trainLoss: 0.6405320167541504\n",
      "cnt: 0 - valLoss: 0.6376318335533142 - trainLoss: 0.6405293941497803\n",
      "cnt: 0 - valLoss: 0.6376281380653381 - trainLoss: 0.640526294708252\n",
      "cnt: 0 - valLoss: 0.6376248598098755 - trainLoss: 0.6405231356620789\n",
      "cnt: 0 - valLoss: 0.6376212239265442 - trainLoss: 0.6405203342437744\n",
      "cnt: 0 - valLoss: 0.6376175880432129 - trainLoss: 0.6405174136161804\n",
      "cnt: 0 - valLoss: 0.6376138925552368 - trainLoss: 0.6405142545700073\n",
      "cnt: 0 - valLoss: 0.637610673904419 - trainLoss: 0.6405112147331238\n",
      "cnt: 0 - valLoss: 0.6376069188117981 - trainLoss: 0.6405084729194641\n",
      "cnt: 0 - valLoss: 0.637603223323822 - trainLoss: 0.6405053734779358\n",
      "cnt: 0 - valLoss: 0.6375998854637146 - trainLoss: 0.6405022740364075\n",
      "cnt: 0 - valLoss: 0.6375963687896729 - trainLoss: 0.6404994130134583\n",
      "cnt: 0 - valLoss: 0.6375926733016968 - trainLoss: 0.6404964923858643\n",
      "cnt: 0 - valLoss: 0.6375889182090759 - trainLoss: 0.6404933929443359\n",
      "cnt: 0 - valLoss: 0.6375857591629028 - trainLoss: 0.6404903531074524\n",
      "cnt: 0 - valLoss: 0.6375818848609924 - trainLoss: 0.6404876112937927\n",
      "cnt: 0 - valLoss: 0.6375781893730164 - trainLoss: 0.6404845118522644\n",
      "cnt: 0 - valLoss: 0.6375749707221985 - trainLoss: 0.6404813528060913\n",
      "cnt: 0 - valLoss: 0.6375712752342224 - trainLoss: 0.6404786109924316\n",
      "cnt: 0 - valLoss: 0.6375675797462463 - trainLoss: 0.6404756307601929\n",
      "cnt: 0 - valLoss: 0.6375640630722046 - trainLoss: 0.6404724717140198\n",
      "cnt: 0 - valLoss: 0.6375606656074524 - trainLoss: 0.640469491481781\n",
      "cnt: 0 - valLoss: 0.6375569701194763 - trainLoss: 0.6404666900634766\n",
      "cnt: 0 - valLoss: 0.6375532746315002 - trainLoss: 0.6404635906219482\n",
      "cnt: 0 - valLoss: 0.6375499963760376 - trainLoss: 0.6404604911804199\n",
      "cnt: 0 - valLoss: 0.6375463008880615 - trainLoss: 0.6404576897621155\n",
      "cnt: 0 - valLoss: 0.6375426650047302 - trainLoss: 0.6404546499252319\n",
      "cnt: 0 - valLoss: 0.6375391483306885 - trainLoss: 0.6404515504837036\n",
      "cnt: 0 - valLoss: 0.6375356912612915 - trainLoss: 0.6404486298561096\n",
      "cnt: 0 - valLoss: 0.6375319361686707 - trainLoss: 0.6404457688331604\n",
      "cnt: 0 - valLoss: 0.6375282406806946 - trainLoss: 0.6404426693916321\n",
      "cnt: 0 - valLoss: 0.6375250816345215 - trainLoss: 0.640439510345459\n",
      "cnt: 0 - valLoss: 0.6375212669372559 - trainLoss: 0.6404368281364441\n",
      "cnt: 0 - valLoss: 0.6375175714492798 - trainLoss: 0.6404337286949158\n",
      "cnt: 0 - valLoss: 0.6375141739845276 - trainLoss: 0.6404305696487427\n",
      "cnt: 0 - valLoss: 0.6375105977058411 - trainLoss: 0.6404277086257935\n",
      "cnt: 0 - valLoss: 0.637506902217865 - trainLoss: 0.6404247879981995\n",
      "cnt: 0 - valLoss: 0.6375032663345337 - trainLoss: 0.6404216885566711\n",
      "cnt: 0 - valLoss: 0.637499988079071 - trainLoss: 0.6404185891151428\n",
      "cnt: 0 - valLoss: 0.6374962329864502 - trainLoss: 0.6404158473014832\n",
      "cnt: 0 - valLoss: 0.6374925971031189 - trainLoss: 0.6404127478599548\n",
      "cnt: 0 - valLoss: 0.6374891400337219 - trainLoss: 0.6404096484184265\n",
      "cnt: 0 - valLoss: 0.6374856233596802 - trainLoss: 0.6404068470001221\n",
      "cnt: 0 - valLoss: 0.6374818682670593 - trainLoss: 0.6404038667678833\n",
      "cnt: 0 - valLoss: 0.637478232383728 - trainLoss: 0.6404007077217102\n",
      "cnt: 0 - valLoss: 0.6374750137329102 - trainLoss: 0.6403976678848267\n",
      "cnt: 0 - valLoss: 0.6374711394309998 - trainLoss: 0.640394926071167\n",
      "cnt: 0 - valLoss: 0.6374674439430237 - trainLoss: 0.6403917670249939\n",
      "cnt: 0 - valLoss: 0.6374642252922058 - trainLoss: 0.6403886675834656\n",
      "cnt: 0 - valLoss: 0.6374605298042297 - trainLoss: 0.6403858661651611\n",
      "cnt: 0 - valLoss: 0.6374567747116089 - trainLoss: 0.6403828263282776\n",
      "cnt: 0 - valLoss: 0.6374532580375671 - trainLoss: 0.6403797268867493\n",
      "cnt: 0 - valLoss: 0.6374498605728149 - trainLoss: 0.6403767466545105\n",
      "cnt: 0 - valLoss: 0.6374461054801941 - trainLoss: 0.640373945236206\n",
      "cnt: 0 - valLoss: 0.6374422907829285 - trainLoss: 0.640370786190033\n",
      "cnt: 0 - valLoss: 0.6374391317367554 - trainLoss: 0.6403676867485046\n",
      "cnt: 0 - valLoss: 0.6374354362487793 - trainLoss: 0.640364944934845\n",
      "cnt: 0 - valLoss: 0.6374316215515137 - trainLoss: 0.6403617858886719\n",
      "cnt: 0 - valLoss: 0.6374282240867615 - trainLoss: 0.6403587460517883\n",
      "cnt: 0 - valLoss: 0.6374247670173645 - trainLoss: 0.6403557658195496\n",
      "cnt: 0 - valLoss: 0.6374210119247437 - trainLoss: 0.6403529047966003\n",
      "cnt: 0 - valLoss: 0.6374172568321228 - trainLoss: 0.6403497457504272\n",
      "cnt: 0 - valLoss: 0.6374141573905945 - trainLoss: 0.6403466463088989\n",
      "cnt: 0 - valLoss: 0.6374103426933289 - trainLoss: 0.640343964099884\n",
      "cnt: 0 - valLoss: 0.637406587600708 - trainLoss: 0.6403408646583557\n",
      "cnt: 0 - valLoss: 0.6374031901359558 - trainLoss: 0.6403376460075378\n",
      "cnt: 0 - valLoss: 0.6373996138572693 - trainLoss: 0.6403348445892334\n",
      "cnt: 0 - valLoss: 0.6373959183692932 - trainLoss: 0.6403319239616394\n",
      "cnt: 0 - valLoss: 0.6373922228813171 - trainLoss: 0.6403287053108215\n",
      "cnt: 0 - valLoss: 0.6373889446258545 - trainLoss: 0.6403257250785828\n",
      "cnt: 0 - valLoss: 0.6373852491378784 - trainLoss: 0.6403229236602783\n",
      "cnt: 0 - valLoss: 0.637381374835968 - trainLoss: 0.6403197646141052\n",
      "cnt: 0 - valLoss: 0.6373781561851501 - trainLoss: 0.6403167247772217\n",
      "cnt: 0 - valLoss: 0.6373745799064636 - trainLoss: 0.6403138041496277\n",
      "cnt: 0 - valLoss: 0.6373707056045532 - trainLoss: 0.6403108239173889\n",
      "cnt: 0 - valLoss: 0.6373671889305115 - trainLoss: 0.6403076648712158\n",
      "cnt: 0 - valLoss: 0.6373637318611145 - trainLoss: 0.640304684638977\n",
      "cnt: 0 - valLoss: 0.6373600363731384 - trainLoss: 0.6403018236160278\n",
      "cnt: 0 - valLoss: 0.6373563408851624 - trainLoss: 0.6402987837791443\n",
      "cnt: 0 - valLoss: 0.6373530030250549 - trainLoss: 0.6402956247329712\n",
      "cnt: 0 - valLoss: 0.6373493075370789 - trainLoss: 0.6402928829193115\n",
      "cnt: 0 - valLoss: 0.6373456120491028 - trainLoss: 0.6402897834777832\n",
      "cnt: 0 - valLoss: 0.6373420357704163 - trainLoss: 0.6402866244316101\n",
      "cnt: 0 - valLoss: 0.6373386383056641 - trainLoss: 0.6402837038040161\n",
      "cnt: 0 - valLoss: 0.6373348236083984 - trainLoss: 0.6402808427810669\n",
      "cnt: 0 - valLoss: 0.6373310685157776 - trainLoss: 0.6402776837348938\n",
      "cnt: 0 - valLoss: 0.6373279094696045 - trainLoss: 0.6402745246887207\n",
      "cnt: 0 - valLoss: 0.6373241543769836 - trainLoss: 0.640271782875061\n",
      "cnt: 0 - valLoss: 0.637320339679718 - trainLoss: 0.6402686834335327\n",
      "cnt: 0 - valLoss: 0.6373170614242554 - trainLoss: 0.6402655243873596\n",
      "cnt: 0 - valLoss: 0.6373134255409241 - trainLoss: 0.6402626633644104\n",
      "cnt: 0 - valLoss: 0.6373096704483032 - trainLoss: 0.6402597427368164\n",
      "cnt: 0 - valLoss: 0.6373060345649719 - trainLoss: 0.6402565836906433\n",
      "cnt: 0 - valLoss: 0.6373026371002197 - trainLoss: 0.640253484249115\n",
      "cnt: 0 - valLoss: 0.6372989416122437 - trainLoss: 0.6402507424354553\n",
      "cnt: 0 - valLoss: 0.6372952461242676 - trainLoss: 0.640247642993927\n",
      "cnt: 0 - valLoss: 0.6372918486595154 - trainLoss: 0.6402444243431091\n",
      "cnt: 0 - valLoss: 0.6372882723808289 - trainLoss: 0.6402416229248047\n",
      "cnt: 0 - valLoss: 0.6372844576835632 - trainLoss: 0.6402385830879211\n",
      "cnt: 0 - valLoss: 0.6372809410095215 - trainLoss: 0.6402354836463928\n",
      "cnt: 0 - valLoss: 0.6372776031494141 - trainLoss: 0.6402324438095093\n",
      "cnt: 0 - valLoss: 0.6372736692428589 - trainLoss: 0.6402295827865601\n",
      "cnt: 0 - valLoss: 0.6372699737548828 - trainLoss: 0.6402264833450317\n",
      "cnt: 0 - valLoss: 0.6372668147087097 - trainLoss: 0.6402233242988586\n",
      "cnt: 0 - valLoss: 0.6372630000114441 - trainLoss: 0.640220582485199\n",
      "cnt: 0 - valLoss: 0.6372592449188232 - trainLoss: 0.6402174234390259\n",
      "cnt: 0 - valLoss: 0.6372557282447815 - trainLoss: 0.6402143836021423\n",
      "cnt: 0 - valLoss: 0.6372522711753845 - trainLoss: 0.6402114629745483\n",
      "cnt: 0 - valLoss: 0.6372485160827637 - trainLoss: 0.6402084827423096\n",
      "cnt: 0 - valLoss: 0.6372447609901428 - trainLoss: 0.6402053833007812\n",
      "cnt: 0 - valLoss: 0.6372416019439697 - trainLoss: 0.6402022838592529\n",
      "cnt: 0 - valLoss: 0.6372377872467041 - trainLoss: 0.6401995420455933\n",
      "cnt: 0 - valLoss: 0.6372339725494385 - trainLoss: 0.6401963829994202\n",
      "cnt: 0 - valLoss: 0.637230634689331 - trainLoss: 0.6401932239532471\n",
      "cnt: 0 - valLoss: 0.637226939201355 - trainLoss: 0.6401903629302979\n",
      "cnt: 0 - valLoss: 0.6372232437133789 - trainLoss: 0.6401873230934143\n",
      "cnt: 0 - valLoss: 0.6372196078300476 - trainLoss: 0.6401841640472412\n",
      "cnt: 0 - valLoss: 0.6372162103652954 - trainLoss: 0.6401811242103577\n",
      "cnt: 0 - valLoss: 0.6372124552726746 - trainLoss: 0.6401783227920532\n",
      "cnt: 0 - valLoss: 0.6372087597846985 - trainLoss: 0.6401751637458801\n",
      "cnt: 0 - valLoss: 0.6372054815292358 - trainLoss: 0.640172004699707\n",
      "cnt: 0 - valLoss: 0.6372017860412598 - trainLoss: 0.6401693224906921\n",
      "cnt: 0 - valLoss: 0.6371979713439941 - trainLoss: 0.640166163444519\n",
      "cnt: 0 - valLoss: 0.6371944546699524 - trainLoss: 0.6401630640029907\n",
      "cnt: 0 - valLoss: 0.6371910572052002 - trainLoss: 0.640160083770752\n",
      "cnt: 0 - valLoss: 0.6371872425079346 - trainLoss: 0.6401571035385132\n",
      "cnt: 0 - valLoss: 0.6371835470199585 - trainLoss: 0.6401539444923401\n",
      "cnt: 0 - valLoss: 0.6371802687644958 - trainLoss: 0.6401508450508118\n",
      "cnt: 0 - valLoss: 0.637176513671875 - trainLoss: 0.6401480436325073\n",
      "cnt: 0 - valLoss: 0.6371726393699646 - trainLoss: 0.6401448845863342\n",
      "cnt: 0 - valLoss: 0.6371694207191467 - trainLoss: 0.6401417255401611\n",
      "cnt: 0 - valLoss: 0.6371657848358154 - trainLoss: 0.6401389241218567\n",
      "cnt: 0 - valLoss: 0.6371619701385498 - trainLoss: 0.6401358246803284\n",
      "cnt: 0 - valLoss: 0.6371584534645081 - trainLoss: 0.6401326656341553\n",
      "cnt: 0 - valLoss: 0.6371549963951111 - trainLoss: 0.6401296854019165\n",
      "cnt: 0 - valLoss: 0.637151300907135 - trainLoss: 0.6401267647743225\n",
      "cnt: 0 - valLoss: 0.6371474266052246 - trainLoss: 0.6401236057281494\n",
      "cnt: 0 - valLoss: 0.6371442675590515 - trainLoss: 0.6401203870773315\n",
      "cnt: 0 - valLoss: 0.6371405720710754 - trainLoss: 0.6401177048683167\n",
      "cnt: 0 - valLoss: 0.637136697769165 - trainLoss: 0.6401144862174988\n",
      "cnt: 0 - valLoss: 0.6371333599090576 - trainLoss: 0.6401113271713257\n",
      "cnt: 0 - valLoss: 0.6371297240257263 - trainLoss: 0.6401084661483765\n",
      "cnt: 0 - valLoss: 0.6371259689331055 - trainLoss: 0.6401054263114929\n",
      "cnt: 0 - valLoss: 0.6371223330497742 - trainLoss: 0.6401022672653198\n",
      "cnt: 0 - valLoss: 0.6371189951896667 - trainLoss: 0.6400992274284363\n",
      "cnt: 0 - valLoss: 0.6371151208877563 - trainLoss: 0.6400963664054871\n",
      "cnt: 0 - valLoss: 0.6371114253997803 - trainLoss: 0.640093207359314\n",
      "cnt: 0 - valLoss: 0.6371082663536072 - trainLoss: 0.6400900483131409\n",
      "cnt: 0 - valLoss: 0.6371043920516968 - trainLoss: 0.6400872468948364\n",
      "cnt: 0 - valLoss: 0.6371006369590759 - trainLoss: 0.6400840878486633\n",
      "cnt: 0 - valLoss: 0.6370972394943237 - trainLoss: 0.6400809288024902\n",
      "cnt: 0 - valLoss: 0.637093722820282 - trainLoss: 0.6400780081748962\n",
      "cnt: 0 - valLoss: 0.6370898485183716 - trainLoss: 0.6400750279426575\n",
      "cnt: 0 - valLoss: 0.6370862722396851 - trainLoss: 0.6400717496871948\n",
      "cnt: 0 - valLoss: 0.6370828151702881 - trainLoss: 0.640068769454956\n",
      "cnt: 0 - valLoss: 0.6370790600776672 - trainLoss: 0.6400659084320068\n",
      "cnt: 0 - valLoss: 0.6370754241943359 - trainLoss: 0.640062689781189\n",
      "cnt: 0 - valLoss: 0.6370720863342285 - trainLoss: 0.6400595307350159\n",
      "cnt: 0 - valLoss: 0.6370683312416077 - trainLoss: 0.6400567889213562\n",
      "cnt: 0 - valLoss: 0.637064516544342 - trainLoss: 0.6400535702705383\n",
      "cnt: 0 - valLoss: 0.6370611190795898 - trainLoss: 0.6400504112243652\n",
      "cnt: 0 - valLoss: 0.6370575428009033 - trainLoss: 0.640047550201416\n",
      "cnt: 0 - valLoss: 0.6370537281036377 - trainLoss: 0.6400445103645325\n",
      "cnt: 0 - valLoss: 0.6370500922203064 - trainLoss: 0.6400413513183594\n",
      "cnt: 0 - valLoss: 0.637046754360199 - trainLoss: 0.640038251876831\n",
      "cnt: 0 - valLoss: 0.6370429992675781 - trainLoss: 0.6400353908538818\n",
      "cnt: 0 - valLoss: 0.6370391845703125 - trainLoss: 0.6400322318077087\n",
      "cnt: 0 - valLoss: 0.6370360255241394 - trainLoss: 0.6400290727615356\n",
      "cnt: 0 - valLoss: 0.637032151222229 - trainLoss: 0.6400262713432312\n",
      "cnt: 0 - valLoss: 0.6370284557342529 - trainLoss: 0.6400231122970581\n",
      "cnt: 0 - valLoss: 0.6370249390602112 - trainLoss: 0.6400198936462402\n",
      "cnt: 0 - valLoss: 0.6370214223861694 - trainLoss: 0.6400169730186462\n",
      "cnt: 0 - valLoss: 0.6370176076889038 - trainLoss: 0.6400139927864075\n",
      "cnt: 0 - valLoss: 0.6370139718055725 - trainLoss: 0.6400107741355896\n",
      "cnt: 0 - valLoss: 0.6370106339454651 - trainLoss: 0.640007734298706\n",
      "cnt: 0 - valLoss: 0.6370067596435547 - trainLoss: 0.6400048732757568\n",
      "cnt: 0 - valLoss: 0.6370031237602234 - trainLoss: 0.6400017142295837\n",
      "cnt: 0 - valLoss: 0.6369997262954712 - trainLoss: 0.6399984955787659\n",
      "cnt: 0 - valLoss: 0.6369960904121399 - trainLoss: 0.6399956941604614\n",
      "cnt: 0 - valLoss: 0.6369922161102295 - trainLoss: 0.6399925947189331\n",
      "cnt: 0 - valLoss: 0.6369888186454773 - trainLoss: 0.63998943567276\n",
      "cnt: 0 - valLoss: 0.636985182762146 - trainLoss: 0.6399863958358765\n",
      "cnt: 0 - valLoss: 0.6369814276695251 - trainLoss: 0.6399834752082825\n",
      "cnt: 0 - valLoss: 0.6369777917861938 - trainLoss: 0.6399802565574646\n",
      "cnt: 0 - valLoss: 0.6369743943214417 - trainLoss: 0.6399771571159363\n",
      "cnt: 0 - valLoss: 0.6369706988334656 - trainLoss: 0.6399744153022766\n",
      "cnt: 0 - valLoss: 0.6369667649269104 - trainLoss: 0.639971137046814\n",
      "cnt: 0 - valLoss: 0.6369636058807373 - trainLoss: 0.6399679780006409\n",
      "cnt: 0 - valLoss: 0.6369597911834717 - trainLoss: 0.6399651765823364\n",
      "cnt: 0 - valLoss: 0.6369560360908508 - trainLoss: 0.6399620175361633\n",
      "cnt: 0 - valLoss: 0.6369524598121643 - trainLoss: 0.6399588584899902\n",
      "cnt: 0 - valLoss: 0.6369489431381226 - trainLoss: 0.6399558782577515\n",
      "cnt: 0 - valLoss: 0.6369452476501465 - trainLoss: 0.6399528384208679\n",
      "cnt: 0 - valLoss: 0.6369414925575256 - trainLoss: 0.6399496793746948\n",
      "cnt: 0 - valLoss: 0.636938214302063 - trainLoss: 0.6399465799331665\n",
      "cnt: 0 - valLoss: 0.6369343400001526 - trainLoss: 0.6399437785148621\n",
      "cnt: 0 - valLoss: 0.6369306445121765 - trainLoss: 0.6399405598640442\n",
      "cnt: 0 - valLoss: 0.6369273066520691 - trainLoss: 0.6399374008178711\n",
      "cnt: 0 - valLoss: 0.636923611164093 - trainLoss: 0.6399345397949219\n",
      "cnt: 0 - valLoss: 0.6369197368621826 - trainLoss: 0.6399313807487488\n",
      "cnt: 0 - valLoss: 0.6369162797927856 - trainLoss: 0.6399281620979309\n",
      "cnt: 0 - valLoss: 0.6369127631187439 - trainLoss: 0.6399252414703369\n",
      "cnt: 0 - valLoss: 0.6369088888168335 - trainLoss: 0.6399222612380981\n",
      "cnt: 0 - valLoss: 0.636905312538147 - trainLoss: 0.639919102191925\n",
      "cnt: 0 - valLoss: 0.63690185546875 - trainLoss: 0.6399160027503967\n",
      "cnt: 0 - valLoss: 0.6368981599807739 - trainLoss: 0.6399131417274475\n",
      "cnt: 0 - valLoss: 0.6368942260742188 - trainLoss: 0.6399099230766296\n",
      "cnt: 0 - valLoss: 0.6368910074234009 - trainLoss: 0.6399067044258118\n",
      "cnt: 0 - valLoss: 0.6368873119354248 - trainLoss: 0.6399039626121521\n",
      "cnt: 0 - valLoss: 0.6368833184242249 - trainLoss: 0.6399008631706238\n",
      "cnt: 0 - valLoss: 0.6368798017501831 - trainLoss: 0.6398976445198059\n",
      "cnt: 0 - valLoss: 0.6368762850761414 - trainLoss: 0.6398946642875671\n",
      "cnt: 0 - valLoss: 0.6368725895881653 - trainLoss: 0.6398918032646179\n",
      "cnt: 0 - valLoss: 0.6368685960769653 - trainLoss: 0.6398886442184448\n",
      "cnt: 0 - valLoss: 0.6368654370307922 - trainLoss: 0.639885425567627\n",
      "cnt: 0 - valLoss: 0.6368616223335266 - trainLoss: 0.6398827433586121\n",
      "cnt: 0 - valLoss: 0.6368576884269714 - trainLoss: 0.639879584312439\n",
      "cnt: 0 - valLoss: 0.6368541717529297 - trainLoss: 0.6398764252662659\n",
      "cnt: 0 - valLoss: 0.6368507146835327 - trainLoss: 0.6398735046386719\n",
      "cnt: 0 - valLoss: 0.6368467807769775 - trainLoss: 0.6398704648017883\n",
      "cnt: 0 - valLoss: 0.6368429660797119 - trainLoss: 0.6398673057556152\n",
      "cnt: 0 - valLoss: 0.6368398070335388 - trainLoss: 0.6398642063140869\n",
      "cnt: 0 - valLoss: 0.6368359327316284 - trainLoss: 0.6398614645004272\n",
      "cnt: 0 - valLoss: 0.636832058429718 - trainLoss: 0.6398583054542542\n",
      "cnt: 0 - valLoss: 0.6368286609649658 - trainLoss: 0.6398550868034363\n",
      "cnt: 0 - valLoss: 0.6368250250816345 - trainLoss: 0.6398522257804871\n",
      "cnt: 0 - valLoss: 0.6368212103843689 - trainLoss: 0.6398491859436035\n",
      "cnt: 0 - valLoss: 0.6368173956871033 - trainLoss: 0.6398460268974304\n",
      "cnt: 0 - valLoss: 0.6368141770362854 - trainLoss: 0.6398429870605469\n",
      "cnt: 0 - valLoss: 0.636810302734375 - trainLoss: 0.6398401260375977\n",
      "cnt: 0 - valLoss: 0.6368064284324646 - trainLoss: 0.6398369669914246\n",
      "cnt: 0 - valLoss: 0.6368030309677124 - trainLoss: 0.6398338079452515\n",
      "cnt: 0 - valLoss: 0.6367993354797363 - trainLoss: 0.6398309469223022\n",
      "cnt: 0 - valLoss: 0.6367954015731812 - trainLoss: 0.6398279070854187\n",
      "cnt: 0 - valLoss: 0.6367917656898499 - trainLoss: 0.6398247480392456\n",
      "cnt: 0 - valLoss: 0.6367884278297424 - trainLoss: 0.6398216485977173\n",
      "cnt: 0 - valLoss: 0.6367846131324768 - trainLoss: 0.6398187875747681\n",
      "cnt: 0 - valLoss: 0.6367807388305664 - trainLoss: 0.6398156881332397\n",
      "cnt: 0 - valLoss: 0.636777400970459 - trainLoss: 0.6398124694824219\n",
      "cnt: 0 - valLoss: 0.6367737054824829 - trainLoss: 0.6398096084594727\n",
      "cnt: 0 - valLoss: 0.6367697715759277 - trainLoss: 0.6398065686225891\n",
      "cnt: 0 - valLoss: 0.6367661952972412 - trainLoss: 0.6398033499717712\n",
      "cnt: 0 - valLoss: 0.6367627382278442 - trainLoss: 0.6398003697395325\n",
      "cnt: 0 - valLoss: 0.6367589235305786 - trainLoss: 0.6397974491119385\n",
      "cnt: 0 - valLoss: 0.6367549896240234 - trainLoss: 0.6397942900657654\n",
      "cnt: 0 - valLoss: 0.6367517709732056 - trainLoss: 0.6397911310195923\n",
      "cnt: 0 - valLoss: 0.6367479562759399 - trainLoss: 0.6397883892059326\n",
      "cnt: 0 - valLoss: 0.6367440223693848 - trainLoss: 0.6397852301597595\n",
      "cnt: 0 - valLoss: 0.636740505695343 - trainLoss: 0.6397819519042969\n",
      "cnt: 0 - valLoss: 0.6367369890213013 - trainLoss: 0.6397790908813477\n",
      "cnt: 0 - valLoss: 0.6367331743240356 - trainLoss: 0.6397761702537537\n",
      "cnt: 0 - valLoss: 0.6367293000221252 - trainLoss: 0.6397729516029358\n",
      "cnt: 0 - valLoss: 0.6367260813713074 - trainLoss: 0.6397697925567627\n",
      "cnt: 0 - valLoss: 0.636722207069397 - trainLoss: 0.6397671103477478\n",
      "cnt: 0 - valLoss: 0.6367183327674866 - trainLoss: 0.6397638916969299\n",
      "cnt: 0 - valLoss: 0.6367148756980896 - trainLoss: 0.6397606730461121\n",
      "cnt: 0 - valLoss: 0.6367112994194031 - trainLoss: 0.6397577524185181\n",
      "cnt: 0 - valLoss: 0.6367073655128479 - trainLoss: 0.6397547721862793\n",
      "cnt: 0 - valLoss: 0.636703610420227 - trainLoss: 0.6397516131401062\n",
      "cnt: 0 - valLoss: 0.6367003321647644 - trainLoss: 0.6397485136985779\n",
      "cnt: 0 - valLoss: 0.636696457862854 - trainLoss: 0.6397457122802734\n",
      "cnt: 0 - valLoss: 0.6366925835609436 - trainLoss: 0.6397424936294556\n",
      "cnt: 0 - valLoss: 0.6366891860961914 - trainLoss: 0.6397392749786377\n",
      "cnt: 0 - valLoss: 0.6366854906082153 - trainLoss: 0.6397364139556885\n",
      "cnt: 0 - valLoss: 0.6366816163063049 - trainLoss: 0.6397333741188049\n",
      "cnt: 0 - valLoss: 0.6366779208183289 - trainLoss: 0.6397302150726318\n",
      "cnt: 0 - valLoss: 0.6366745829582214 - trainLoss: 0.6397271156311035\n",
      "cnt: 0 - valLoss: 0.6366706490516663 - trainLoss: 0.6397242546081543\n",
      "cnt: 0 - valLoss: 0.6366667747497559 - trainLoss: 0.6397210955619812\n",
      "cnt: 0 - valLoss: 0.636663556098938 - trainLoss: 0.6397178769111633\n",
      "cnt: 0 - valLoss: 0.6366596817970276 - trainLoss: 0.6397151350975037\n",
      "cnt: 0 - valLoss: 0.6366558074951172 - trainLoss: 0.6397119760513306\n",
      "cnt: 0 - valLoss: 0.6366522908210754 - trainLoss: 0.6397088170051575\n",
      "cnt: 0 - valLoss: 0.6366487741470337 - trainLoss: 0.6397058367729187\n",
      "cnt: 0 - valLoss: 0.6366448402404785 - trainLoss: 0.6397028565406799\n",
      "cnt: 0 - valLoss: 0.6366410255432129 - trainLoss: 0.6396996974945068\n",
      "cnt: 0 - valLoss: 0.636637806892395 - trainLoss: 0.639696478843689\n",
      "cnt: 0 - valLoss: 0.6366338729858398 - trainLoss: 0.6396937370300293\n",
      "cnt: 0 - valLoss: 0.6366299986839294 - trainLoss: 0.6396905779838562\n",
      "cnt: 0 - valLoss: 0.636626660823822 - trainLoss: 0.6396873593330383\n",
      "cnt: 0 - valLoss: 0.6366228461265564 - trainLoss: 0.6396844387054443\n",
      "cnt: 0 - valLoss: 0.6366190314292908 - trainLoss: 0.6396813988685608\n",
      "cnt: 0 - valLoss: 0.6366153955459595 - trainLoss: 0.6396782994270325\n",
      "cnt: 0 - valLoss: 0.6366119980812073 - trainLoss: 0.6396751403808594\n",
      "cnt: 0 - valLoss: 0.6366080641746521 - trainLoss: 0.6396722793579102\n",
      "cnt: 0 - valLoss: 0.6366042494773865 - trainLoss: 0.6396691203117371\n",
      "cnt: 0 - valLoss: 0.6366008520126343 - trainLoss: 0.6396659016609192\n",
      "cnt: 0 - valLoss: 0.6365970373153687 - trainLoss: 0.6396631002426147\n",
      "cnt: 0 - valLoss: 0.636593222618103 - trainLoss: 0.6396600008010864\n",
      "cnt: 0 - valLoss: 0.6365895867347717 - trainLoss: 0.6396568417549133\n",
      "cnt: 0 - valLoss: 0.63658607006073 - trainLoss: 0.639653742313385\n",
      "cnt: 0 - valLoss: 0.6365821957588196 - trainLoss: 0.639650821685791\n",
      "cnt: 0 - valLoss: 0.636578381061554 - trainLoss: 0.6396476626396179\n",
      "cnt: 0 - valLoss: 0.6365751624107361 - trainLoss: 0.6396443843841553\n",
      "cnt: 0 - valLoss: 0.6365712285041809 - trainLoss: 0.6396417021751404\n",
      "cnt: 0 - valLoss: 0.6365674138069153 - trainLoss: 0.6396386027336121\n",
      "cnt: 0 - valLoss: 0.6365638375282288 - trainLoss: 0.6396353840827942\n",
      "cnt: 0 - valLoss: 0.6365601420402527 - trainLoss: 0.6396323442459106\n",
      "cnt: 0 - valLoss: 0.6365563273429871 - trainLoss: 0.6396293640136719\n",
      "cnt: 0 - valLoss: 0.636552631855011 - trainLoss: 0.639626145362854\n",
      "cnt: 0 - valLoss: 0.636549174785614 - trainLoss: 0.6396230459213257\n",
      "cnt: 0 - valLoss: 0.6365453600883484 - trainLoss: 0.6396202445030212\n",
      "cnt: 0 - valLoss: 0.6365415453910828 - trainLoss: 0.6396170854568481\n",
      "cnt: 0 - valLoss: 0.6365381479263306 - trainLoss: 0.6396138072013855\n",
      "cnt: 0 - valLoss: 0.6365343332290649 - trainLoss: 0.6396109461784363\n",
      "cnt: 0 - valLoss: 0.6365305185317993 - trainLoss: 0.6396079063415527\n",
      "cnt: 0 - valLoss: 0.636526882648468 - trainLoss: 0.6396046876907349\n",
      "cnt: 0 - valLoss: 0.6365233659744263 - trainLoss: 0.6396015882492065\n",
      "cnt: 0 - valLoss: 0.6365194916725159 - trainLoss: 0.6395987868309021\n",
      "cnt: 0 - valLoss: 0.6365156769752502 - trainLoss: 0.6395955681800842\n",
      "cnt: 0 - valLoss: 0.6365123987197876 - trainLoss: 0.6395923495292664\n",
      "cnt: 0 - valLoss: 0.6365084648132324 - trainLoss: 0.6395895481109619\n",
      "cnt: 0 - valLoss: 0.6365046501159668 - trainLoss: 0.6395863890647888\n",
      "cnt: 0 - valLoss: 0.6365010738372803 - trainLoss: 0.6395832300186157\n",
      "cnt: 0 - valLoss: 0.6364975571632385 - trainLoss: 0.6395801901817322\n",
      "cnt: 0 - valLoss: 0.6364936232566833 - trainLoss: 0.639577329158783\n",
      "cnt: 0 - valLoss: 0.6364898085594177 - trainLoss: 0.6395740509033203\n",
      "cnt: 0 - valLoss: 0.6364865899085999 - trainLoss: 0.6395708918571472\n",
      "cnt: 0 - valLoss: 0.6364827752113342 - trainLoss: 0.6395682096481323\n",
      "cnt: 0 - valLoss: 0.6364789009094238 - trainLoss: 0.6395649313926697\n",
      "cnt: 0 - valLoss: 0.6364753246307373 - trainLoss: 0.6395617723464966\n",
      "cnt: 0 - valLoss: 0.6364717483520508 - trainLoss: 0.6395589113235474\n",
      "cnt: 0 - valLoss: 0.6364678740501404 - trainLoss: 0.639555811882019\n",
      "cnt: 0 - valLoss: 0.6364641785621643 - trainLoss: 0.639552652835846\n",
      "cnt: 0 - valLoss: 0.6364607810974121 - trainLoss: 0.6395494937896729\n",
      "cnt: 0 - valLoss: 0.6364568471908569 - trainLoss: 0.6395466327667236\n",
      "cnt: 0 - valLoss: 0.6364530324935913 - trainLoss: 0.6395434737205505\n",
      "cnt: 0 - valLoss: 0.6364496946334839 - trainLoss: 0.6395402550697327\n",
      "cnt: 0 - valLoss: 0.6364458799362183 - trainLoss: 0.639537513256073\n",
      "cnt: 0 - valLoss: 0.6364420056343079 - trainLoss: 0.6395343542098999\n",
      "cnt: 0 - valLoss: 0.6364383697509766 - trainLoss: 0.639531135559082\n",
      "cnt: 0 - valLoss: 0.6364348530769348 - trainLoss: 0.6395280957221985\n",
      "cnt: 0 - valLoss: 0.6364310383796692 - trainLoss: 0.6395251750946045\n",
      "cnt: 0 - valLoss: 0.6364272236824036 - trainLoss: 0.6395220160484314\n",
      "cnt: 0 - valLoss: 0.6364239454269409 - trainLoss: 0.6395188570022583\n",
      "cnt: 0 - valLoss: 0.6364200711250305 - trainLoss: 0.6395159959793091\n",
      "cnt: 0 - valLoss: 0.6364161372184753 - trainLoss: 0.639512836933136\n",
      "cnt: 0 - valLoss: 0.6364127397537231 - trainLoss: 0.6395096182823181\n",
      "cnt: 0 - valLoss: 0.6364090442657471 - trainLoss: 0.6395067572593689\n",
      "cnt: 0 - valLoss: 0.6364052295684814 - trainLoss: 0.6395036578178406\n",
      "cnt: 0 - valLoss: 0.6364015340805054 - trainLoss: 0.6395004987716675\n",
      "cnt: 0 - valLoss: 0.6363980174064636 - trainLoss: 0.6394974589347839\n",
      "cnt: 0 - valLoss: 0.636394202709198 - trainLoss: 0.6394945383071899\n",
      "cnt: 0 - valLoss: 0.6363903284072876 - trainLoss: 0.6394913196563721\n",
      "cnt: 0 - valLoss: 0.636387050151825 - trainLoss: 0.6394881010055542\n",
      "cnt: 0 - valLoss: 0.6363832354545593 - trainLoss: 0.6394853591918945\n",
      "cnt: 0 - valLoss: 0.6363793015480042 - trainLoss: 0.6394821405410767\n",
      "cnt: 0 - valLoss: 0.6363757848739624 - trainLoss: 0.6394789218902588\n",
      "cnt: 0 - valLoss: 0.6363721489906311 - trainLoss: 0.6394760608673096\n",
      "cnt: 0 - valLoss: 0.6363683342933655 - trainLoss: 0.639473021030426\n",
      "cnt: 0 - valLoss: 0.6363645792007446 - trainLoss: 0.6394698023796082\n",
      "cnt: 0 - valLoss: 0.6363611817359924 - trainLoss: 0.6394666433334351\n",
      "cnt: 0 - valLoss: 0.636357307434082 - trainLoss: 0.6394638419151306\n",
      "cnt: 0 - valLoss: 0.6363534331321716 - trainLoss: 0.6394606232643127\n",
      "cnt: 0 - valLoss: 0.6363500356674194 - trainLoss: 0.6394574642181396\n",
      "cnt: 0 - valLoss: 0.6363463401794434 - trainLoss: 0.6394545435905457\n",
      "cnt: 0 - valLoss: 0.6363424062728882 - trainLoss: 0.6394515037536621\n",
      "cnt: 0 - valLoss: 0.6363388299942017 - trainLoss: 0.6394482851028442\n",
      "cnt: 0 - valLoss: 0.6363353133201599 - trainLoss: 0.6394451856613159\n",
      "cnt: 0 - valLoss: 0.6363314986228943 - trainLoss: 0.6394423246383667\n",
      "cnt: 0 - valLoss: 0.6363275647163391 - trainLoss: 0.6394391059875488\n",
      "cnt: 0 - valLoss: 0.636324405670166 - trainLoss: 0.639435887336731\n",
      "cnt: 0 - valLoss: 0.6363204121589661 - trainLoss: 0.6394331455230713\n",
      "cnt: 0 - valLoss: 0.6363165974617004 - trainLoss: 0.6394299268722534\n",
      "cnt: 0 - valLoss: 0.6363131403923035 - trainLoss: 0.6394267082214355\n",
      "cnt: 0 - valLoss: 0.6363094449043274 - trainLoss: 0.6394237875938416\n",
      "cnt: 0 - valLoss: 0.6363056302070618 - trainLoss: 0.639420747756958\n",
      "cnt: 0 - valLoss: 0.6363019347190857 - trainLoss: 0.6394175887107849\n",
      "cnt: 0 - valLoss: 0.6362983584403992 - trainLoss: 0.6394144296646118\n",
      "cnt: 0 - valLoss: 0.6362945437431335 - trainLoss: 0.6394115686416626\n",
      "cnt: 0 - valLoss: 0.6362907290458679 - trainLoss: 0.6394083499908447\n",
      "cnt: 0 - valLoss: 0.6362873911857605 - trainLoss: 0.6394051313400269\n",
      "cnt: 0 - valLoss: 0.6362836360931396 - trainLoss: 0.6394023299217224\n",
      "cnt: 0 - valLoss: 0.6362797617912292 - trainLoss: 0.6393991708755493\n",
      "cnt: 0 - valLoss: 0.6362760663032532 - trainLoss: 0.6393958926200867\n",
      "cnt: 0 - valLoss: 0.6362725496292114 - trainLoss: 0.6393929719924927\n",
      "cnt: 0 - valLoss: 0.636268675327301 - trainLoss: 0.6393900513648987\n",
      "cnt: 0 - valLoss: 0.6362649202346802 - trainLoss: 0.6393868327140808\n",
      "cnt: 0 - valLoss: 0.6362615823745728 - trainLoss: 0.6393836140632629\n",
      "cnt: 0 - valLoss: 0.6362577676773071 - trainLoss: 0.6393808126449585\n",
      "cnt: 0 - valLoss: 0.6362537741661072 - trainLoss: 0.6393775939941406\n",
      "cnt: 0 - valLoss: 0.636250376701355 - trainLoss: 0.6393744349479675\n",
      "cnt: 0 - valLoss: 0.6362466812133789 - trainLoss: 0.6393714547157288\n",
      "cnt: 0 - valLoss: 0.6362427473068237 - trainLoss: 0.6393684148788452\n",
      "cnt: 0 - valLoss: 0.6362391114234924 - trainLoss: 0.6393651962280273\n",
      "cnt: 0 - valLoss: 0.6362357139587402 - trainLoss: 0.639362096786499\n",
      "cnt: 0 - valLoss: 0.6362316608428955 - trainLoss: 0.6393592953681946\n",
      "cnt: 0 - valLoss: 0.6362277865409851 - trainLoss: 0.6393559575080872\n",
      "cnt: 0 - valLoss: 0.6362245678901672 - trainLoss: 0.6393527388572693\n",
      "cnt: 0 - valLoss: 0.6362207531929016 - trainLoss: 0.6393499970436096\n",
      "cnt: 0 - valLoss: 0.6362168192863464 - trainLoss: 0.6393467783927917\n",
      "cnt: 0 - valLoss: 0.6362133622169495 - trainLoss: 0.6393435597419739\n",
      "cnt: 0 - valLoss: 0.6362096667289734 - trainLoss: 0.6393405795097351\n",
      "cnt: 0 - valLoss: 0.636205792427063 - trainLoss: 0.6393375992774963\n",
      "cnt: 0 - valLoss: 0.6362020969390869 - trainLoss: 0.6393344402313232\n",
      "cnt: 0 - valLoss: 0.6361986398696899 - trainLoss: 0.6393312811851501\n",
      "cnt: 0 - valLoss: 0.6361948251724243 - trainLoss: 0.6393283605575562\n",
      "cnt: 0 - valLoss: 0.6361910104751587 - trainLoss: 0.6393251419067383\n",
      "cnt: 0 - valLoss: 0.6361874938011169 - trainLoss: 0.6393219232559204\n",
      "cnt: 0 - valLoss: 0.6361836791038513 - trainLoss: 0.6393191814422607\n",
      "cnt: 0 - valLoss: 0.6361798048019409 - trainLoss: 0.6393160223960876\n",
      "cnt: 0 - valLoss: 0.6361762881278992 - trainLoss: 0.6393126845359802\n",
      "cnt: 0 - valLoss: 0.6361727118492126 - trainLoss: 0.6393097043037415\n",
      "cnt: 0 - valLoss: 0.6361687183380127 - trainLoss: 0.6393067240715027\n",
      "cnt: 0 - valLoss: 0.6361649632453918 - trainLoss: 0.6393035054206848\n",
      "cnt: 0 - valLoss: 0.6361616253852844 - trainLoss: 0.6393003463745117\n",
      "cnt: 0 - valLoss: 0.636157751083374 - trainLoss: 0.6392975449562073\n",
      "cnt: 0 - valLoss: 0.6361538767814636 - trainLoss: 0.6392943263053894\n",
      "cnt: 0 - valLoss: 0.6361505389213562 - trainLoss: 0.6392911076545715\n",
      "cnt: 0 - valLoss: 0.6361466646194458 - trainLoss: 0.6392882466316223\n",
      "cnt: 0 - valLoss: 0.6361427903175354 - trainLoss: 0.6392850875854492\n",
      "cnt: 0 - valLoss: 0.6361392140388489 - trainLoss: 0.6392818689346313\n",
      "cnt: 0 - valLoss: 0.6361356377601624 - trainLoss: 0.6392788290977478\n",
      "cnt: 0 - valLoss: 0.6361318230628967 - trainLoss: 0.639275848865509\n",
      "cnt: 0 - valLoss: 0.6361280083656311 - trainLoss: 0.6392726302146912\n",
      "cnt: 0 - valLoss: 0.6361245512962341 - trainLoss: 0.6392694115638733\n",
      "cnt: 0 - valLoss: 0.6361206769943237 - trainLoss: 0.6392666101455688\n",
      "cnt: 0 - valLoss: 0.6361168026924133 - trainLoss: 0.6392633318901062\n",
      "cnt: 0 - valLoss: 0.6361134052276611 - trainLoss: 0.6392601728439331\n",
      "cnt: 0 - valLoss: 0.6361096501350403 - trainLoss: 0.6392573118209839\n",
      "cnt: 0 - valLoss: 0.6361057162284851 - trainLoss: 0.6392541527748108\n",
      "cnt: 0 - valLoss: 0.6361020803451538 - trainLoss: 0.6392508745193481\n",
      "cnt: 0 - valLoss: 0.6360985636711121 - trainLoss: 0.6392478942871094\n",
      "cnt: 0 - valLoss: 0.6360946893692017 - trainLoss: 0.6392449140548706\n",
      "cnt: 0 - valLoss: 0.6360908150672913 - trainLoss: 0.6392416954040527\n",
      "cnt: 0 - valLoss: 0.6360874772071838 - trainLoss: 0.6392384767532349\n",
      "cnt: 0 - valLoss: 0.6360836029052734 - trainLoss: 0.6392356753349304\n",
      "cnt: 0 - valLoss: 0.6360796689987183 - trainLoss: 0.6392325162887573\n",
      "cnt: 0 - valLoss: 0.6360762715339661 - trainLoss: 0.6392292976379395\n",
      "cnt: 0 - valLoss: 0.63607257604599 - trainLoss: 0.6392262578010559\n",
      "cnt: 0 - valLoss: 0.6360686421394348 - trainLoss: 0.6392232179641724\n",
      "cnt: 0 - valLoss: 0.6360650062561035 - trainLoss: 0.6392199993133545\n",
      "cnt: 0 - valLoss: 0.636061429977417 - trainLoss: 0.639216959476471\n",
      "cnt: 0 - valLoss: 0.6360575556755066 - trainLoss: 0.6392139792442322\n",
      "cnt: 0 - valLoss: 0.6360536813735962 - trainLoss: 0.6392107605934143\n",
      "cnt: 0 - valLoss: 0.6360504031181335 - trainLoss: 0.6392074823379517\n",
      "cnt: 0 - valLoss: 0.6360465288162231 - trainLoss: 0.6392048001289368\n",
      "cnt: 0 - valLoss: 0.6360425353050232 - trainLoss: 0.6392015218734741\n",
      "cnt: 0 - valLoss: 0.6360390186309814 - trainLoss: 0.6391982436180115\n",
      "cnt: 0 - valLoss: 0.6360353827476501 - trainLoss: 0.6391953229904175\n",
      "cnt: 0 - valLoss: 0.636031448841095 - trainLoss: 0.6391922235488892\n",
      "cnt: 0 - valLoss: 0.6360278129577637 - trainLoss: 0.6391890048980713\n",
      "cnt: 0 - valLoss: 0.6360242366790771 - trainLoss: 0.639185905456543\n",
      "cnt: 0 - valLoss: 0.6360203623771667 - trainLoss: 0.639182984828949\n",
      "cnt: 0 - valLoss: 0.6360164880752563 - trainLoss: 0.6391797065734863\n",
      "cnt: 0 - valLoss: 0.6360132098197937 - trainLoss: 0.6391765475273132\n",
      "cnt: 0 - valLoss: 0.6360092759132385 - trainLoss: 0.6391737461090088\n",
      "cnt: 0 - valLoss: 0.6360053420066833 - trainLoss: 0.6391705274581909\n",
      "cnt: 0 - valLoss: 0.6360018253326416 - trainLoss: 0.6391672492027283\n",
      "cnt: 0 - valLoss: 0.6359981298446655 - trainLoss: 0.6391643285751343\n",
      "cnt: 0 - valLoss: 0.6359942555427551 - trainLoss: 0.6391612887382507\n",
      "cnt: 0 - valLoss: 0.6359906196594238 - trainLoss: 0.6391580104827881\n",
      "cnt: 0 - valLoss: 0.6359871625900269 - trainLoss: 0.6391549110412598\n",
      "cnt: 0 - valLoss: 0.6359830498695374 - trainLoss: 0.6391519904136658\n",
      "cnt: 0 - valLoss: 0.6359792351722717 - trainLoss: 0.6391487121582031\n",
      "cnt: 0 - valLoss: 0.6359760165214539 - trainLoss: 0.6391454339027405\n",
      "cnt: 0 - valLoss: 0.6359720826148987 - trainLoss: 0.639142632484436\n",
      "cnt: 0 - valLoss: 0.6359681487083435 - trainLoss: 0.6391394138336182\n",
      "cnt: 0 - valLoss: 0.635964572429657 - trainLoss: 0.6391361951828003\n",
      "cnt: 0 - valLoss: 0.6359609365463257 - trainLoss: 0.6391332745552063\n",
      "cnt: 0 - valLoss: 0.6359570622444153 - trainLoss: 0.639130175113678\n",
      "cnt: 0 - valLoss: 0.6359533071517944 - trainLoss: 0.6391269564628601\n",
      "cnt: 0 - valLoss: 0.6359499096870422 - trainLoss: 0.639123797416687\n",
      "cnt: 0 - valLoss: 0.6359459161758423 - trainLoss: 0.6391209363937378\n",
      "cnt: 0 - valLoss: 0.6359419822692871 - trainLoss: 0.6391176581382751\n",
      "cnt: 0 - valLoss: 0.6359387636184692 - trainLoss: 0.6391144394874573\n",
      "cnt: 0 - valLoss: 0.6359348297119141 - trainLoss: 0.6391116380691528\n",
      "cnt: 0 - valLoss: 0.6359309554100037 - trainLoss: 0.6391083598136902\n",
      "cnt: 0 - valLoss: 0.6359273791313171 - trainLoss: 0.6391051411628723\n",
      "cnt: 0 - valLoss: 0.6359236836433411 - trainLoss: 0.6391022205352783\n",
      "cnt: 0 - valLoss: 0.6359197497367859 - trainLoss: 0.63909912109375\n",
      "cnt: 0 - valLoss: 0.6359160542488098 - trainLoss: 0.6390958428382874\n",
      "cnt: 0 - valLoss: 0.6359125971794128 - trainLoss: 0.6390928030014038\n",
      "cnt: 0 - valLoss: 0.6359087228775024 - trainLoss: 0.639089822769165\n",
      "cnt: 0 - valLoss: 0.6359047293663025 - trainLoss: 0.6390866041183472\n",
      "cnt: 0 - valLoss: 0.6359013915061951 - trainLoss: 0.6390833258628845\n",
      "cnt: 0 - valLoss: 0.6358975172042847 - trainLoss: 0.6390805244445801\n",
      "cnt: 0 - valLoss: 0.6358936429023743 - trainLoss: 0.6390773057937622\n",
      "cnt: 0 - valLoss: 0.6358901858329773 - trainLoss: 0.6390740275382996\n",
      "cnt: 0 - valLoss: 0.6358863711357117 - trainLoss: 0.6390711069107056\n",
      "cnt: 0 - valLoss: 0.6358824968338013 - trainLoss: 0.6390680074691772\n",
      "cnt: 0 - valLoss: 0.6358788013458252 - trainLoss: 0.6390647292137146\n",
      "cnt: 0 - valLoss: 0.6358752846717834 - trainLoss: 0.6390616297721863\n",
      "cnt: 0 - valLoss: 0.6358713507652283 - trainLoss: 0.6390587091445923\n",
      "cnt: 0 - valLoss: 0.6358674168586731 - trainLoss: 0.6390554308891296\n",
      "cnt: 0 - valLoss: 0.6358641386032104 - trainLoss: 0.6390522122383118\n",
      "cnt: 0 - valLoss: 0.6358602046966553 - trainLoss: 0.6390494108200073\n",
      "cnt: 0 - valLoss: 0.6358562707901001 - trainLoss: 0.6390461325645447\n",
      "cnt: 0 - valLoss: 0.6358528733253479 - trainLoss: 0.639042854309082\n",
      "cnt: 0 - valLoss: 0.6358490586280823 - trainLoss: 0.6390399932861328\n",
      "cnt: 0 - valLoss: 0.6358451247215271 - trainLoss: 0.6390368342399597\n",
      "cnt: 0 - valLoss: 0.635841429233551 - trainLoss: 0.6390336155891418\n",
      "cnt: 0 - valLoss: 0.6358379125595093 - trainLoss: 0.6390304565429688\n",
      "cnt: 0 - valLoss: 0.6358340382575989 - trainLoss: 0.6390275359153748\n",
      "cnt: 0 - valLoss: 0.6358300447463989 - trainLoss: 0.6390243172645569\n",
      "cnt: 0 - valLoss: 0.6358267068862915 - trainLoss: 0.6390210390090942\n",
      "cnt: 0 - valLoss: 0.6358227729797363 - trainLoss: 0.6390182375907898\n",
      "cnt: 0 - valLoss: 0.6358188986778259 - trainLoss: 0.6390149593353271\n",
      "cnt: 0 - valLoss: 0.6358155608177185 - trainLoss: 0.6390117406845093\n",
      "cnt: 0 - valLoss: 0.6358116269111633 - trainLoss: 0.6390088200569153\n",
      "cnt: 0 - valLoss: 0.6358076930046082 - trainLoss: 0.6390056610107422\n",
      "cnt: 0 - valLoss: 0.6358041167259216 - trainLoss: 0.6390023827552795\n",
      "cnt: 0 - valLoss: 0.6358005404472351 - trainLoss: 0.638999342918396\n",
      "cnt: 0 - valLoss: 0.6357966065406799 - trainLoss: 0.6389963030815125\n",
      "cnt: 0 - valLoss: 0.6357927322387695 - trainLoss: 0.6389930844306946\n",
      "cnt: 0 - valLoss: 0.6357892751693726 - trainLoss: 0.6389898657798767\n",
      "cnt: 0 - valLoss: 0.6357854008674622 - trainLoss: 0.6389870047569275\n",
      "cnt: 0 - valLoss: 0.635781466960907 - trainLoss: 0.6389837861061096\n",
      "cnt: 0 - valLoss: 0.6357781291007996 - trainLoss: 0.6389804482460022\n",
      "cnt: 0 - valLoss: 0.6357741355895996 - trainLoss: 0.6389776468276978\n",
      "cnt: 0 - valLoss: 0.635770320892334 - trainLoss: 0.6389743685722351\n",
      "cnt: 0 - valLoss: 0.6357666850090027 - trainLoss: 0.6389711499214172\n",
      "cnt: 0 - valLoss: 0.6357631087303162 - trainLoss: 0.6389681100845337\n",
      "cnt: 0 - valLoss: 0.635759174823761 - trainLoss: 0.6389650702476501\n",
      "cnt: 0 - valLoss: 0.6357552409172058 - trainLoss: 0.6389617919921875\n",
      "cnt: 0 - valLoss: 0.6357519030570984 - trainLoss: 0.6389586329460144\n",
      "cnt: 0 - valLoss: 0.6357479095458984 - trainLoss: 0.6389557719230652\n",
      "cnt: 0 - valLoss: 0.635744035243988 - trainLoss: 0.6389524936676025\n",
      "cnt: 0 - valLoss: 0.6357405781745911 - trainLoss: 0.6389492154121399\n",
      "cnt: 0 - valLoss: 0.6357367634773254 - trainLoss: 0.6389463543891907\n",
      "cnt: 0 - valLoss: 0.6357328295707703 - trainLoss: 0.6389431357383728\n",
      "cnt: 0 - valLoss: 0.6357293128967285 - trainLoss: 0.6389399170875549\n",
      "cnt: 0 - valLoss: 0.6357256174087524 - trainLoss: 0.6389368772506714\n",
      "cnt: 0 - valLoss: 0.6357215046882629 - trainLoss: 0.6389337778091431\n",
      "cnt: 0 - valLoss: 0.6357178688049316 - trainLoss: 0.6389304995536804\n",
      "cnt: 0 - valLoss: 0.6357143521308899 - trainLoss: 0.6389274597167969\n",
      "cnt: 0 - valLoss: 0.6357104778289795 - trainLoss: 0.6389244198799133\n",
      "cnt: 0 - valLoss: 0.6357065439224243 - trainLoss: 0.6389212012290955\n",
      "cnt: 0 - valLoss: 0.6357030868530273 - trainLoss: 0.6389179229736328\n",
      "cnt: 0 - valLoss: 0.6356992125511169 - trainLoss: 0.6389151811599731\n",
      "cnt: 0 - valLoss: 0.6356953382492065 - trainLoss: 0.6389118432998657\n",
      "cnt: 0 - valLoss: 0.63569176197052 - trainLoss: 0.6389085650444031\n",
      "cnt: 0 - valLoss: 0.635688066482544 - trainLoss: 0.6389056444168091\n",
      "cnt: 0 - valLoss: 0.635684072971344 - trainLoss: 0.638902485370636\n",
      "cnt: 0 - valLoss: 0.6356804370880127 - trainLoss: 0.6388992667198181\n",
      "cnt: 0 - valLoss: 0.6356768608093262 - trainLoss: 0.6388961672782898\n",
      "cnt: 0 - valLoss: 0.635672926902771 - trainLoss: 0.638893187046051\n",
      "cnt: 0 - valLoss: 0.6356690526008606 - trainLoss: 0.6388899087905884\n",
      "cnt: 0 - valLoss: 0.6356656551361084 - trainLoss: 0.6388866305351257\n",
      "cnt: 0 - valLoss: 0.6356617212295532 - trainLoss: 0.6388838291168213\n",
      "cnt: 0 - valLoss: 0.6356577277183533 - trainLoss: 0.6388805508613586\n",
      "cnt: 0 - valLoss: 0.6356543898582458 - trainLoss: 0.6388772130012512\n",
      "cnt: 0 - valLoss: 0.6356504559516907 - trainLoss: 0.638874351978302\n",
      "cnt: 0 - valLoss: 0.6356465220451355 - trainLoss: 0.6388711929321289\n",
      "cnt: 0 - valLoss: 0.635642945766449 - trainLoss: 0.6388678550720215\n",
      "cnt: 0 - valLoss: 0.6356393098831177 - trainLoss: 0.6388648748397827\n",
      "cnt: 0 - valLoss: 0.6356353759765625 - trainLoss: 0.6388617753982544\n",
      "cnt: 0 - valLoss: 0.6356315016746521 - trainLoss: 0.6388585567474365\n",
      "cnt: 0 - valLoss: 0.6356280446052551 - trainLoss: 0.6388553380966187\n",
      "cnt: 0 - valLoss: 0.6356241703033447 - trainLoss: 0.6388524770736694\n",
      "cnt: 0 - valLoss: 0.6356201767921448 - trainLoss: 0.6388491988182068\n",
      "cnt: 0 - valLoss: 0.6356168985366821 - trainLoss: 0.6388459205627441\n",
      "cnt: 0 - valLoss: 0.6356128454208374 - trainLoss: 0.6388430595397949\n",
      "cnt: 0 - valLoss: 0.6356089115142822 - trainLoss: 0.638839840888977\n",
      "cnt: 0 - valLoss: 0.6356054544448853 - trainLoss: 0.6388365626335144\n",
      "cnt: 0 - valLoss: 0.6356017589569092 - trainLoss: 0.6388335227966309\n",
      "cnt: 0 - valLoss: 0.6355976462364197 - trainLoss: 0.6388304829597473\n",
      "cnt: 0 - valLoss: 0.6355939507484436 - trainLoss: 0.6388271450996399\n",
      "cnt: 0 - valLoss: 0.6355904340744019 - trainLoss: 0.6388240456581116\n",
      "cnt: 0 - valLoss: 0.6355865001678467 - trainLoss: 0.6388210654258728\n",
      "cnt: 0 - valLoss: 0.635582685470581 - trainLoss: 0.6388177871704102\n",
      "cnt: 0 - valLoss: 0.6355791687965393 - trainLoss: 0.6388145089149475\n",
      "cnt: 0 - valLoss: 0.6355752348899841 - trainLoss: 0.6388117074966431\n",
      "cnt: 0 - valLoss: 0.635571300983429 - trainLoss: 0.6388084292411804\n",
      "cnt: 0 - valLoss: 0.6355679035186768 - trainLoss: 0.638805091381073\n",
      "cnt: 0 - valLoss: 0.6355640292167664 - trainLoss: 0.6388022303581238\n",
      "cnt: 0 - valLoss: 0.6355600953102112 - trainLoss: 0.6387990117073059\n",
      "cnt: 0 - valLoss: 0.6355565190315247 - trainLoss: 0.6387957334518433\n",
      "cnt: 0 - valLoss: 0.6355528235435486 - trainLoss: 0.6387926936149597\n",
      "cnt: 0 - valLoss: 0.6355488896369934 - trainLoss: 0.6387895941734314\n",
      "cnt: 0 - valLoss: 0.635545015335083 - trainLoss: 0.6387863755226135\n",
      "cnt: 0 - valLoss: 0.635541558265686 - trainLoss: 0.6387831568717957\n",
      "cnt: 0 - valLoss: 0.6355376243591309 - trainLoss: 0.6387802362442017\n",
      "cnt: 0 - valLoss: 0.6355336904525757 - trainLoss: 0.6387768983840942\n",
      "cnt: 0 - valLoss: 0.6355302929878235 - trainLoss: 0.6387736797332764\n",
      "cnt: 0 - valLoss: 0.6355263590812683 - trainLoss: 0.6387708187103271\n",
      "cnt: 0 - valLoss: 0.6355223655700684 - trainLoss: 0.6387674808502197\n",
      "cnt: 0 - valLoss: 0.6355188488960266 - trainLoss: 0.6387642621994019\n",
      "cnt: 0 - valLoss: 0.6355151534080505 - trainLoss: 0.6387613415718079\n",
      "cnt: 0 - valLoss: 0.6355111002922058 - trainLoss: 0.6387581825256348\n",
      "cnt: 0 - valLoss: 0.6355074048042297 - trainLoss: 0.6387548446655273\n",
      "cnt: 0 - valLoss: 0.6355037689208984 - trainLoss: 0.6387518048286438\n",
      "cnt: 0 - valLoss: 0.6354999542236328 - trainLoss: 0.6387487649917603\n",
      "cnt: 0 - valLoss: 0.6354960799217224 - trainLoss: 0.6387454271316528\n",
      "cnt: 0 - valLoss: 0.6354925632476807 - trainLoss: 0.6387422680854797\n",
      "cnt: 0 - valLoss: 0.6354886293411255 - trainLoss: 0.6387393474578857\n",
      "cnt: 0 - valLoss: 0.6354846358299255 - trainLoss: 0.6387360692024231\n",
      "cnt: 0 - valLoss: 0.6354813575744629 - trainLoss: 0.6387327909469604\n",
      "cnt: 0 - valLoss: 0.6354773044586182 - trainLoss: 0.6387299299240112\n",
      "cnt: 0 - valLoss: 0.635473370552063 - trainLoss: 0.6387266516685486\n",
      "cnt: 0 - valLoss: 0.6354697942733765 - trainLoss: 0.6387233734130859\n",
      "cnt: 0 - valLoss: 0.6354660987854004 - trainLoss: 0.6387203335762024\n",
      "cnt: 0 - valLoss: 0.6354621052742004 - trainLoss: 0.6387172341346741\n",
      "cnt: 0 - valLoss: 0.6354582905769348 - trainLoss: 0.6387139558792114\n",
      "cnt: 0 - valLoss: 0.6354547739028931 - trainLoss: 0.6387107968330383\n",
      "cnt: 0 - valLoss: 0.6354507207870483 - trainLoss: 0.6387078166007996\n",
      "cnt: 0 - valLoss: 0.6354468464851379 - trainLoss: 0.6387045383453369\n",
      "cnt: 0 - valLoss: 0.6354433298110962 - trainLoss: 0.6387012600898743\n",
      "cnt: 0 - valLoss: 0.635439395904541 - trainLoss: 0.638698399066925\n",
      "cnt: 0 - valLoss: 0.6354354619979858 - trainLoss: 0.6386951208114624\n",
      "cnt: 0 - valLoss: 0.6354320049285889 - trainLoss: 0.6386918425559998\n",
      "cnt: 0 - valLoss: 0.6354281902313232 - trainLoss: 0.6386889219284058\n",
      "cnt: 0 - valLoss: 0.6354241371154785 - trainLoss: 0.6386857628822327\n",
      "cnt: 0 - valLoss: 0.6354204416275024 - trainLoss: 0.6386824250221252\n",
      "cnt: 0 - valLoss: 0.6354168057441711 - trainLoss: 0.6386793851852417\n",
      "cnt: 0 - valLoss: 0.6354128122329712 - trainLoss: 0.6386762857437134\n",
      "cnt: 0 - valLoss: 0.635408878326416 - trainLoss: 0.6386730074882507\n",
      "cnt: 0 - valLoss: 0.635405421257019 - trainLoss: 0.6386697888374329\n",
      "cnt: 0 - valLoss: 0.6354014277458191 - trainLoss: 0.6386668682098389\n",
      "cnt: 0 - valLoss: 0.6353974938392639 - trainLoss: 0.6386634707450867\n",
      "cnt: 0 - valLoss: 0.6353941559791565 - trainLoss: 0.6386602520942688\n",
      "cnt: 0 - valLoss: 0.6353901028633118 - trainLoss: 0.6386573910713196\n",
      "cnt: 0 - valLoss: 0.6353861093521118 - trainLoss: 0.6386541128158569\n",
      "cnt: 0 - valLoss: 0.6353825926780701 - trainLoss: 0.6386508345603943\n",
      "cnt: 0 - valLoss: 0.6353787779808044 - trainLoss: 0.6386479139328003\n",
      "cnt: 0 - valLoss: 0.6353748440742493 - trainLoss: 0.6386446356773376\n",
      "cnt: 0 - valLoss: 0.6353710889816284 - trainLoss: 0.6386414170265198\n",
      "cnt: 0 - valLoss: 0.6353673934936523 - trainLoss: 0.6386383175849915\n",
      "cnt: 0 - valLoss: 0.6353634595870972 - trainLoss: 0.6386352181434631\n",
      "cnt: 0 - valLoss: 0.6353595852851868 - trainLoss: 0.6386319398880005\n",
      "cnt: 0 - valLoss: 0.635356068611145 - trainLoss: 0.6386287212371826\n",
      "cnt: 0 - valLoss: 0.6353519558906555 - trainLoss: 0.6386258006095886\n",
      "cnt: 0 - valLoss: 0.6353480219841003 - trainLoss: 0.6386224627494812\n",
      "cnt: 0 - valLoss: 0.6353446841239929 - trainLoss: 0.6386191844940186\n",
      "cnt: 0 - valLoss: 0.6353407502174377 - trainLoss: 0.6386163234710693\n",
      "cnt: 0 - valLoss: 0.6353366374969482 - trainLoss: 0.6386130452156067\n",
      "cnt: 0 - valLoss: 0.6353331804275513 - trainLoss: 0.638609766960144\n",
      "cnt: 0 - valLoss: 0.6353293657302856 - trainLoss: 0.6386067867279053\n",
      "cnt: 0 - valLoss: 0.6353253722190857 - trainLoss: 0.6386035680770874\n",
      "cnt: 0 - valLoss: 0.6353217363357544 - trainLoss: 0.63860023021698\n",
      "cnt: 0 - valLoss: 0.6353179812431335 - trainLoss: 0.6385972499847412\n",
      "cnt: 0 - valLoss: 0.6353139281272888 - trainLoss: 0.6385942101478577\n",
      "cnt: 0 - valLoss: 0.6353101134300232 - trainLoss: 0.6385908126831055\n",
      "cnt: 0 - valLoss: 0.6353066563606262 - trainLoss: 0.6385876536369324\n",
      "cnt: 0 - valLoss: 0.6353026628494263 - trainLoss: 0.6385847330093384\n",
      "cnt: 0 - valLoss: 0.6352986097335815 - trainLoss: 0.638581395149231\n",
      "cnt: 0 - valLoss: 0.6352952122688293 - trainLoss: 0.6385780572891235\n",
      "cnt: 0 - valLoss: 0.6352912783622742 - trainLoss: 0.6385751962661743\n",
      "cnt: 0 - valLoss: 0.6352872252464294 - trainLoss: 0.6385719180107117\n",
      "cnt: 0 - valLoss: 0.6352836489677429 - trainLoss: 0.6385685801506042\n",
      "cnt: 0 - valLoss: 0.6352798938751221 - trainLoss: 0.6385656595230103\n",
      "cnt: 0 - valLoss: 0.6352758407592773 - trainLoss: 0.6385625004768372\n",
      "cnt: 0 - valLoss: 0.635272204875946 - trainLoss: 0.6385591626167297\n",
      "cnt: 0 - valLoss: 0.6352685689926147 - trainLoss: 0.6385561227798462\n",
      "cnt: 0 - valLoss: 0.63526451587677 - trainLoss: 0.6385530233383179\n",
      "cnt: 0 - valLoss: 0.6352606415748596 - trainLoss: 0.6385496854782104\n",
      "cnt: 0 - valLoss: 0.6352571845054626 - trainLoss: 0.6385464668273926\n",
      "cnt: 0 - valLoss: 0.6352531909942627 - trainLoss: 0.6385435461997986\n",
      "cnt: 0 - valLoss: 0.6352491974830627 - trainLoss: 0.6385402679443359\n",
      "cnt: 0 - valLoss: 0.6352457404136658 - trainLoss: 0.6385368704795837\n",
      "cnt: 0 - valLoss: 0.635241687297821 - trainLoss: 0.6385340690612793\n",
      "cnt: 0 - valLoss: 0.6352378129959106 - trainLoss: 0.6385307908058167\n",
      "cnt: 0 - valLoss: 0.6352342963218689 - trainLoss: 0.6385274529457092\n",
      "cnt: 0 - valLoss: 0.635230302810669 - trainLoss: 0.6385244727134705\n",
      "cnt: 0 - valLoss: 0.6352263689041138 - trainLoss: 0.6385213136672974\n",
      "cnt: 0 - valLoss: 0.6352226734161377 - trainLoss: 0.6385179758071899\n",
      "cnt: 0 - valLoss: 0.6352190375328064 - trainLoss: 0.6385148763656616\n",
      "cnt: 0 - valLoss: 0.6352149248123169 - trainLoss: 0.6385117769241333\n",
      "cnt: 0 - valLoss: 0.6352111101150513 - trainLoss: 0.6385084390640259\n",
      "cnt: 0 - valLoss: 0.6352075934410095 - trainLoss: 0.6385053396224976\n",
      "cnt: 0 - valLoss: 0.6352035999298096 - trainLoss: 0.638502299785614\n",
      "cnt: 0 - valLoss: 0.6351995468139648 - trainLoss: 0.6384989619255066\n",
      "cnt: 0 - valLoss: 0.6351961493492126 - trainLoss: 0.638495683670044\n",
      "cnt: 0 - valLoss: 0.6351921558380127 - trainLoss: 0.6384928822517395\n",
      "cnt: 0 - valLoss: 0.6351882219314575 - trainLoss: 0.6384895443916321\n",
      "cnt: 0 - valLoss: 0.635184645652771 - trainLoss: 0.6384862065315247\n",
      "cnt: 0 - valLoss: 0.6351807117462158 - trainLoss: 0.6384832262992859\n",
      "cnt: 0 - valLoss: 0.6351767778396606 - trainLoss: 0.638480007648468\n",
      "cnt: 0 - valLoss: 0.6351730823516846 - trainLoss: 0.6384766697883606\n",
      "cnt: 0 - valLoss: 0.635169267654419 - trainLoss: 0.638473629951477\n",
      "cnt: 0 - valLoss: 0.635165274143219 - trainLoss: 0.6384705305099487\n",
      "cnt: 0 - valLoss: 0.6351615190505981 - trainLoss: 0.6384672522544861\n",
      "cnt: 0 - valLoss: 0.6351578831672668 - trainLoss: 0.6384640336036682\n",
      "cnt: 0 - valLoss: 0.6351538896560669 - trainLoss: 0.6384609937667847\n",
      "cnt: 0 - valLoss: 0.6351498961448669 - trainLoss: 0.638457715511322\n",
      "cnt: 0 - valLoss: 0.6351464986801147 - trainLoss: 0.6384543776512146\n",
      "cnt: 0 - valLoss: 0.6351425051689148 - trainLoss: 0.6384515166282654\n",
      "cnt: 0 - valLoss: 0.6351383924484253 - trainLoss: 0.6384482383728027\n",
      "cnt: 0 - valLoss: 0.6351349353790283 - trainLoss: 0.6384448409080505\n",
      "cnt: 0 - valLoss: 0.6351310610771179 - trainLoss: 0.6384419202804565\n",
      "cnt: 0 - valLoss: 0.6351270079612732 - trainLoss: 0.6384386420249939\n",
      "cnt: 0 - valLoss: 0.6351233124732971 - trainLoss: 0.6384353637695312\n",
      "cnt: 0 - valLoss: 0.6351195573806763 - trainLoss: 0.6384323239326477\n",
      "cnt: 0 - valLoss: 0.6351155638694763 - trainLoss: 0.6384291648864746\n",
      "cnt: 0 - valLoss: 0.6351118087768555 - trainLoss: 0.6384258270263672\n",
      "cnt: 0 - valLoss: 0.6351081132888794 - trainLoss: 0.6384226679801941\n",
      "cnt: 0 - valLoss: 0.6351040601730347 - trainLoss: 0.6384196877479553\n",
      "cnt: 0 - valLoss: 0.635100245475769 - trainLoss: 0.6384163498878479\n",
      "cnt: 0 - valLoss: 0.6350966691970825 - trainLoss: 0.6384130716323853\n",
      "cnt: 0 - valLoss: 0.6350926756858826 - trainLoss: 0.6384101510047913\n",
      "cnt: 0 - valLoss: 0.6350886225700378 - trainLoss: 0.6384068131446838\n",
      "cnt: 0 - valLoss: 0.6350852251052856 - trainLoss: 0.6384035348892212\n",
      "cnt: 0 - valLoss: 0.6350811719894409 - trainLoss: 0.6384006142616272\n",
      "cnt: 0 - valLoss: 0.6350771188735962 - trainLoss: 0.6383972764015198\n",
      "cnt: 0 - valLoss: 0.6350736021995544 - trainLoss: 0.6383939385414124\n",
      "cnt: 0 - valLoss: 0.6350697875022888 - trainLoss: 0.6383909583091736\n",
      "cnt: 0 - valLoss: 0.6350656747817993 - trainLoss: 0.6383877396583557\n",
      "cnt: 0 - valLoss: 0.6350619792938232 - trainLoss: 0.6383844017982483\n",
      "cnt: 0 - valLoss: 0.6350583434104919 - trainLoss: 0.6383813619613647\n",
      "cnt: 0 - valLoss: 0.6350542306900024 - trainLoss: 0.6383782029151917\n",
      "cnt: 0 - valLoss: 0.6350504159927368 - trainLoss: 0.638374924659729\n",
      "cnt: 0 - valLoss: 0.6350468993186951 - trainLoss: 0.6383716464042664\n",
      "cnt: 0 - valLoss: 0.6350428462028503 - trainLoss: 0.6383686661720276\n",
      "cnt: 0 - valLoss: 0.6350387334823608 - trainLoss: 0.6383653283119202\n",
      "cnt: 0 - valLoss: 0.6350353956222534 - trainLoss: 0.6383620500564575\n",
      "cnt: 0 - valLoss: 0.6350313425064087 - trainLoss: 0.6383591890335083\n",
      "cnt: 0 - valLoss: 0.6350272297859192 - trainLoss: 0.6383558511734009\n",
      "cnt: 0 - valLoss: 0.6350237727165222 - trainLoss: 0.6383524537086487\n",
      "cnt: 0 - valLoss: 0.6350198984146118 - trainLoss: 0.6383495330810547\n",
      "cnt: 0 - valLoss: 0.6350157260894775 - trainLoss: 0.638346254825592\n",
      "cnt: 0 - valLoss: 0.635012149810791 - trainLoss: 0.6383429169654846\n",
      "cnt: 0 - valLoss: 0.6350083947181702 - trainLoss: 0.6383398771286011\n",
      "cnt: 0 - valLoss: 0.6350043416023254 - trainLoss: 0.638336718082428\n",
      "cnt: 0 - valLoss: 0.6350005269050598 - trainLoss: 0.6383333802223206\n",
      "cnt: 0 - valLoss: 0.6349968910217285 - trainLoss: 0.6383302211761475\n",
      "cnt: 0 - valLoss: 0.6349928975105286 - trainLoss: 0.6383271217346191\n",
      "cnt: 0 - valLoss: 0.6349888443946838 - trainLoss: 0.6383237838745117\n",
      "cnt: 0 - valLoss: 0.6349853873252869 - trainLoss: 0.6383205652236938\n",
      "cnt: 0 - valLoss: 0.6349813342094421 - trainLoss: 0.6383176445960999\n",
      "cnt: 0 - valLoss: 0.6349773406982422 - trainLoss: 0.6383142471313477\n",
      "cnt: 0 - valLoss: 0.6349738836288452 - trainLoss: 0.638310968875885\n",
      "cnt: 0 - valLoss: 0.6349698901176453 - trainLoss: 0.638308048248291\n",
      "cnt: 0 - valLoss: 0.6349658966064453 - trainLoss: 0.6383047103881836\n",
      "cnt: 0 - valLoss: 0.634962260723114 - trainLoss: 0.6383013725280762\n",
      "cnt: 0 - valLoss: 0.6349583268165588 - trainLoss: 0.6382983922958374\n",
      "cnt: 0 - valLoss: 0.6349543929100037 - trainLoss: 0.6382951140403748\n",
      "cnt: 0 - valLoss: 0.634950578212738 - trainLoss: 0.6382917761802673\n",
      "cnt: 0 - valLoss: 0.6349468231201172 - trainLoss: 0.6382887363433838\n",
      "cnt: 0 - valLoss: 0.6349428296089172 - trainLoss: 0.6382855772972107\n",
      "cnt: 0 - valLoss: 0.6349390149116516 - trainLoss: 0.6382822394371033\n",
      "cnt: 0 - valLoss: 0.6349353790283203 - trainLoss: 0.6382790207862854\n",
      "cnt: 0 - valLoss: 0.6349313259124756 - trainLoss: 0.6382760405540466\n",
      "cnt: 0 - valLoss: 0.6349273920059204 - trainLoss: 0.6382726430892944\n",
      "cnt: 0 - valLoss: 0.6349238157272339 - trainLoss: 0.6382693648338318\n",
      "cnt: 0 - valLoss: 0.6349198222160339 - trainLoss: 0.6382664442062378\n",
      "cnt: 0 - valLoss: 0.6349157691001892 - trainLoss: 0.6382631063461304\n",
      "cnt: 0 - valLoss: 0.6349123120307922 - trainLoss: 0.6382597088813782\n",
      "cnt: 0 - valLoss: 0.6349082589149475 - trainLoss: 0.638256847858429\n",
      "cnt: 0 - valLoss: 0.6349042654037476 - trainLoss: 0.6382535099983215\n",
      "cnt: 0 - valLoss: 0.6349007487297058 - trainLoss: 0.6382501721382141\n",
      "cnt: 0 - valLoss: 0.6348967552185059 - trainLoss: 0.6382471323013306\n",
      "cnt: 0 - valLoss: 0.6348927617073059 - trainLoss: 0.6382439732551575\n",
      "cnt: 0 - valLoss: 0.6348890066146851 - trainLoss: 0.6382405757904053\n",
      "cnt: 0 - valLoss: 0.6348851919174194 - trainLoss: 0.6382375359535217\n",
      "cnt: 0 - valLoss: 0.6348811984062195 - trainLoss: 0.6382343769073486\n",
      "cnt: 0 - valLoss: 0.6348773837089539 - trainLoss: 0.6382310390472412\n",
      "cnt: 0 - valLoss: 0.6348737478256226 - trainLoss: 0.6382278800010681\n",
      "cnt: 0 - valLoss: 0.6348696351051331 - trainLoss: 0.6382248401641846\n",
      "cnt: 0 - valLoss: 0.6348657011985779 - trainLoss: 0.6382214426994324\n",
      "cnt: 0 - valLoss: 0.6348623037338257 - trainLoss: 0.638218104839325\n",
      "cnt: 0 - valLoss: 0.6348581910133362 - trainLoss: 0.638215184211731\n",
      "cnt: 0 - valLoss: 0.6348540782928467 - trainLoss: 0.6382119059562683\n",
      "cnt: 0 - valLoss: 0.6348506808280945 - trainLoss: 0.6382083892822266\n",
      "cnt: 0 - valLoss: 0.6348466277122498 - trainLoss: 0.6382055282592773\n",
      "cnt: 0 - valLoss: 0.6348425149917603 - trainLoss: 0.6382022500038147\n",
      "cnt: 0 - valLoss: 0.6348389983177185 - trainLoss: 0.6381989121437073\n",
      "cnt: 0 - valLoss: 0.6348350644111633 - trainLoss: 0.638195812702179\n",
      "cnt: 0 - valLoss: 0.6348310112953186 - trainLoss: 0.6381926536560059\n",
      "cnt: 0 - valLoss: 0.6348273158073425 - trainLoss: 0.6381893157958984\n",
      "cnt: 0 - valLoss: 0.6348236799240112 - trainLoss: 0.6381861567497253\n",
      "cnt: 0 - valLoss: 0.6348194479942322 - trainLoss: 0.638183057308197\n",
      "cnt: 0 - valLoss: 0.6348156332969666 - trainLoss: 0.6381797194480896\n",
      "cnt: 0 - valLoss: 0.6348121166229248 - trainLoss: 0.6381765007972717\n",
      "cnt: 0 - valLoss: 0.6348079442977905 - trainLoss: 0.638173520565033\n",
      "cnt: 0 - valLoss: 0.6348040103912354 - trainLoss: 0.638170063495636\n",
      "cnt: 0 - valLoss: 0.6348004937171936 - trainLoss: 0.6381668448448181\n",
      "cnt: 0 - valLoss: 0.6347963809967041 - trainLoss: 0.6381639242172241\n",
      "cnt: 0 - valLoss: 0.6347923874855042 - trainLoss: 0.6381605267524719\n",
      "cnt: 0 - valLoss: 0.6347889304161072 - trainLoss: 0.6381571292877197\n",
      "cnt: 0 - valLoss: 0.6347848176956177 - trainLoss: 0.6381542086601257\n",
      "cnt: 0 - valLoss: 0.634780764579773 - trainLoss: 0.6381508708000183\n",
      "cnt: 0 - valLoss: 0.634777307510376 - trainLoss: 0.6381475329399109\n",
      "cnt: 0 - valLoss: 0.6347733736038208 - trainLoss: 0.6381444931030273\n",
      "cnt: 0 - valLoss: 0.6347692012786865 - trainLoss: 0.6381413340568542\n",
      "cnt: 0 - valLoss: 0.6347655653953552 - trainLoss: 0.6381378769874573\n",
      "cnt: 0 - valLoss: 0.6347616910934448 - trainLoss: 0.6381348371505737\n",
      "cnt: 0 - valLoss: 0.6347576379776001 - trainLoss: 0.6381316184997559\n",
      "cnt: 0 - valLoss: 0.6347538828849792 - trainLoss: 0.6381282806396484\n",
      "cnt: 0 - valLoss: 0.6347501277923584 - trainLoss: 0.6381250619888306\n",
      "cnt: 0 - valLoss: 0.6347460746765137 - trainLoss: 0.638122022151947\n",
      "cnt: 0 - valLoss: 0.6347421407699585 - trainLoss: 0.6381186246871948\n",
      "cnt: 0 - valLoss: 0.634738564491272 - trainLoss: 0.6381153464317322\n",
      "cnt: 0 - valLoss: 0.6347345113754272 - trainLoss: 0.6381124258041382\n",
      "cnt: 0 - valLoss: 0.6347303986549377 - trainLoss: 0.6381089687347412\n",
      "cnt: 0 - valLoss: 0.6347270011901855 - trainLoss: 0.6381056308746338\n",
      "cnt: 0 - valLoss: 0.6347229480743408 - trainLoss: 0.6381027102470398\n",
      "cnt: 0 - valLoss: 0.6347187757492065 - trainLoss: 0.6380993723869324\n",
      "cnt: 0 - valLoss: 0.6347152590751648 - trainLoss: 0.6380959153175354\n",
      "cnt: 0 - valLoss: 0.6347112655639648 - trainLoss: 0.6380930542945862\n",
      "cnt: 0 - valLoss: 0.6347072124481201 - trainLoss: 0.6380897164344788\n",
      "cnt: 0 - valLoss: 0.6347035765647888 - trainLoss: 0.6380862593650818\n",
      "cnt: 0 - valLoss: 0.6346996426582336 - trainLoss: 0.6380832195281982\n",
      "cnt: 0 - valLoss: 0.6346955895423889 - trainLoss: 0.6380801200866699\n",
      "cnt: 0 - valLoss: 0.6346917748451233 - trainLoss: 0.6380767226219177\n",
      "cnt: 0 - valLoss: 0.6346880197525024 - trainLoss: 0.6380735039710999\n",
      "cnt: 0 - valLoss: 0.6346840262413025 - trainLoss: 0.6380704045295715\n",
      "cnt: 0 - valLoss: 0.6346800327301025 - trainLoss: 0.6380670070648193\n",
      "cnt: 0 - valLoss: 0.634676456451416 - trainLoss: 0.6380637288093567\n",
      "cnt: 0 - valLoss: 0.6346722841262817 - trainLoss: 0.6380607485771179\n",
      "cnt: 0 - valLoss: 0.6346683502197266 - trainLoss: 0.6380573511123657\n",
      "cnt: 0 - valLoss: 0.6346648335456848 - trainLoss: 0.6380540132522583\n",
      "cnt: 0 - valLoss: 0.6346607208251953 - trainLoss: 0.6380510330200195\n",
      "cnt: 0 - valLoss: 0.6346566677093506 - trainLoss: 0.6380476951599121\n",
      "cnt: 0 - valLoss: 0.6346532702445984 - trainLoss: 0.6380442976951599\n",
      "cnt: 0 - valLoss: 0.6346490979194641 - trainLoss: 0.6380413770675659\n",
      "cnt: 0 - valLoss: 0.6346450448036194 - trainLoss: 0.6380380392074585\n",
      "cnt: 0 - valLoss: 0.6346414089202881 - trainLoss: 0.6380346417427063\n",
      "cnt: 0 - valLoss: 0.6346375346183777 - trainLoss: 0.6380316615104675\n",
      "cnt: 0 - valLoss: 0.6346334218978882 - trainLoss: 0.6380283236503601\n",
      "cnt: 0 - valLoss: 0.6346296668052673 - trainLoss: 0.6380249857902527\n",
      "cnt: 0 - valLoss: 0.6346259117126465 - trainLoss: 0.6380218863487244\n",
      "cnt: 0 - valLoss: 0.6346217393875122 - trainLoss: 0.6380186676979065\n",
      "cnt: 0 - valLoss: 0.6346179246902466 - trainLoss: 0.6380153298377991\n",
      "cnt: 0 - valLoss: 0.6346142292022705 - trainLoss: 0.6380121111869812\n",
      "cnt: 0 - valLoss: 0.6346100568771362 - trainLoss: 0.6380090713500977\n",
      "cnt: 0 - valLoss: 0.6346062421798706 - trainLoss: 0.6380056142807007\n",
      "cnt: 0 - valLoss: 0.6346025466918945 - trainLoss: 0.6380023956298828\n",
      "cnt: 0 - valLoss: 0.6345984935760498 - trainLoss: 0.6379993557929993\n",
      "cnt: 0 - valLoss: 0.6345944404602051 - trainLoss: 0.6379959583282471\n",
      "cnt: 0 - valLoss: 0.6345909833908081 - trainLoss: 0.6379926204681396\n",
      "cnt: 0 - valLoss: 0.6345868706703186 - trainLoss: 0.6379896402359009\n",
      "cnt: 0 - valLoss: 0.6345826983451843 - trainLoss: 0.6379863023757935\n",
      "cnt: 0 - valLoss: 0.6345792412757874 - trainLoss: 0.6379829049110413\n",
      "cnt: 0 - valLoss: 0.6345752477645874 - trainLoss: 0.6379799246788025\n",
      "cnt: 0 - valLoss: 0.6345710754394531 - trainLoss: 0.6379765868186951\n",
      "cnt: 0 - valLoss: 0.6345674991607666 - trainLoss: 0.6379731893539429\n",
      "cnt: 0 - valLoss: 0.6345635056495667 - trainLoss: 0.6379702091217041\n",
      "cnt: 0 - valLoss: 0.6345594525337219 - trainLoss: 0.6379669308662415\n",
      "cnt: 0 - valLoss: 0.6345557570457458 - trainLoss: 0.6379634737968445\n",
      "cnt: 0 - valLoss: 0.6345518231391907 - trainLoss: 0.6379603743553162\n",
      "cnt: 0 - valLoss: 0.6345478296279907 - trainLoss: 0.6379571557044983\n",
      "cnt: 0 - valLoss: 0.6345439553260803 - trainLoss: 0.6379538178443909\n",
      "cnt: 0 - valLoss: 0.6345402002334595 - trainLoss: 0.637950599193573\n",
      "cnt: 0 - valLoss: 0.6345361471176147 - trainLoss: 0.6379474997520447\n",
      "cnt: 0 - valLoss: 0.6345321536064148 - trainLoss: 0.6379441022872925\n",
      "cnt: 0 - valLoss: 0.6345285773277283 - trainLoss: 0.6379408240318298\n",
      "cnt: 0 - valLoss: 0.6345245838165283 - trainLoss: 0.6379378437995911\n",
      "cnt: 0 - valLoss: 0.634520411491394 - trainLoss: 0.6379343867301941\n",
      "cnt: 0 - valLoss: 0.6345169544219971 - trainLoss: 0.6379310488700867\n",
      "cnt: 0 - valLoss: 0.634512722492218 - trainLoss: 0.6379281282424927\n",
      "cnt: 0 - valLoss: 0.6345087289810181 - trainLoss: 0.6379247307777405\n",
      "cnt: 0 - valLoss: 0.6345052123069763 - trainLoss: 0.6379213333129883\n",
      "cnt: 0 - valLoss: 0.6345010995864868 - trainLoss: 0.6379184126853943\n",
      "cnt: 0 - valLoss: 0.6344970464706421 - trainLoss: 0.6379149556159973\n",
      "cnt: 0 - valLoss: 0.634493350982666 - trainLoss: 0.6379115581512451\n",
      "cnt: 0 - valLoss: 0.6344894766807556 - trainLoss: 0.6379085779190063\n",
      "cnt: 0 - valLoss: 0.6344854235649109 - trainLoss: 0.6379052996635437\n",
      "cnt: 0 - valLoss: 0.6344816088676453 - trainLoss: 0.6379019021987915\n",
      "cnt: 0 - valLoss: 0.6344777941703796 - trainLoss: 0.6378988027572632\n",
      "cnt: 0 - valLoss: 0.6344735622406006 - trainLoss: 0.6378955841064453\n",
      "cnt: 0 - valLoss: 0.6344698667526245 - trainLoss: 0.6378921866416931\n",
      "cnt: 0 - valLoss: 0.6344661116600037 - trainLoss: 0.63788902759552\n",
      "cnt: 0 - valLoss: 0.6344618797302246 - trainLoss: 0.6378858685493469\n",
      "cnt: 0 - valLoss: 0.634458065032959 - trainLoss: 0.6378825306892395\n",
      "cnt: 0 - valLoss: 0.6344544291496277 - trainLoss: 0.6378792524337769\n",
      "cnt: 0 - valLoss: 0.6344501972198486 - trainLoss: 0.6378762125968933\n",
      "cnt: 0 - valLoss: 0.6344462633132935 - trainLoss: 0.6378726959228516\n",
      "cnt: 0 - valLoss: 0.6344426274299622 - trainLoss: 0.6378693580627441\n",
      "cnt: 0 - valLoss: 0.6344385743141174 - trainLoss: 0.6378664374351501\n",
      "cnt: 0 - valLoss: 0.6344345211982727 - trainLoss: 0.6378629803657532\n",
      "cnt: 0 - valLoss: 0.6344309449195862 - trainLoss: 0.6378596425056458\n",
      "cnt: 0 - valLoss: 0.6344269514083862 - trainLoss: 0.6378567218780518\n",
      "cnt: 0 - valLoss: 0.6344227194786072 - trainLoss: 0.6378532648086548\n",
      "cnt: 0 - valLoss: 0.6344191431999207 - trainLoss: 0.6378499269485474\n",
      "cnt: 0 - valLoss: 0.6344152092933655 - trainLoss: 0.6378468871116638\n",
      "cnt: 0 - valLoss: 0.6344110369682312 - trainLoss: 0.6378436088562012\n",
      "cnt: 0 - valLoss: 0.6344074010848999 - trainLoss: 0.6378401517868042\n",
      "cnt: 0 - valLoss: 0.6344034671783447 - trainLoss: 0.6378370523452759\n",
      "cnt: 0 - valLoss: 0.6343993544578552 - trainLoss: 0.6378338932991028\n",
      "cnt: 0 - valLoss: 0.6343956589698792 - trainLoss: 0.637830376625061\n",
      "cnt: 0 - valLoss: 0.634391725063324 - trainLoss: 0.6378272175788879\n",
      "cnt: 0 - valLoss: 0.6343876123428345 - trainLoss: 0.6378241181373596\n",
      "cnt: 0 - valLoss: 0.6343836784362793 - trainLoss: 0.6378206610679626\n",
      "cnt: 0 - valLoss: 0.634380042552948 - trainLoss: 0.6378174424171448\n",
      "cnt: 0 - valLoss: 0.6343759894371033 - trainLoss: 0.6378144025802612\n",
      "cnt: 0 - valLoss: 0.6343718767166138 - trainLoss: 0.637811005115509\n",
      "cnt: 0 - valLoss: 0.634368360042572 - trainLoss: 0.6378076076507568\n",
      "cnt: 0 - valLoss: 0.6343640685081482 - trainLoss: 0.6378046870231628\n",
      "cnt: 0 - valLoss: 0.634360134601593 - trainLoss: 0.6378012299537659\n",
      "cnt: 0 - valLoss: 0.6343566179275513 - trainLoss: 0.6377978920936584\n",
      "cnt: 0 - valLoss: 0.634352445602417 - trainLoss: 0.6377949118614197\n",
      "cnt: 0 - valLoss: 0.6343483924865723 - trainLoss: 0.6377915143966675\n",
      "cnt: 0 - valLoss: 0.6343448758125305 - trainLoss: 0.6377880573272705\n",
      "cnt: 0 - valLoss: 0.6343407034873962 - trainLoss: 0.6377851366996765\n",
      "cnt: 0 - valLoss: 0.6343366503715515 - trainLoss: 0.6377817988395691\n",
      "cnt: 0 - valLoss: 0.6343329548835754 - trainLoss: 0.6377783417701721\n",
      "cnt: 0 - valLoss: 0.6343290209770203 - trainLoss: 0.6377753019332886\n",
      "cnt: 0 - valLoss: 0.6343249082565308 - trainLoss: 0.6377720236778259\n",
      "cnt: 0 - valLoss: 0.6343212127685547 - trainLoss: 0.637768566608429\n",
      "cnt: 0 - valLoss: 0.6343172788619995 - trainLoss: 0.6377655267715454\n",
      "cnt: 0 - valLoss: 0.6343131065368652 - trainLoss: 0.637762188911438\n",
      "cnt: 0 - valLoss: 0.6343093514442444 - trainLoss: 0.6377588510513306\n",
      "cnt: 0 - valLoss: 0.6343055963516235 - trainLoss: 0.6377556920051575\n",
      "cnt: 0 - valLoss: 0.6343013644218445 - trainLoss: 0.6377524733543396\n",
      "cnt: 0 - valLoss: 0.6342975497245789 - trainLoss: 0.6377490758895874\n",
      "cnt: 0 - valLoss: 0.6342937350273132 - trainLoss: 0.6377458572387695\n",
      "cnt: 0 - valLoss: 0.6342896819114685 - trainLoss: 0.6377427577972412\n",
      "cnt: 0 - valLoss: 0.6342856884002686 - trainLoss: 0.6377393007278442\n",
      "cnt: 0 - valLoss: 0.6342820525169373 - trainLoss: 0.6377359628677368\n",
      "cnt: 0 - valLoss: 0.6342779397964478 - trainLoss: 0.637732982635498\n",
      "cnt: 0 - valLoss: 0.6342738270759583 - trainLoss: 0.6377295255661011\n",
      "cnt: 0 - valLoss: 0.6342703104019165 - trainLoss: 0.6377261877059937\n",
      "cnt: 0 - valLoss: 0.6342662572860718 - trainLoss: 0.6377231478691101\n",
      "cnt: 0 - valLoss: 0.6342620253562927 - trainLoss: 0.6377198100090027\n",
      "cnt: 0 - valLoss: 0.6342585682868958 - trainLoss: 0.6377163529396057\n",
      "cnt: 0 - valLoss: 0.6342543363571167 - trainLoss: 0.6377134323120117\n",
      "cnt: 0 - valLoss: 0.634250283241272 - trainLoss: 0.6377099752426147\n",
      "cnt: 0 - valLoss: 0.6342467665672302 - trainLoss: 0.6377065181732178\n",
      "cnt: 0 - valLoss: 0.6342426538467407 - trainLoss: 0.6377035975456238\n",
      "cnt: 0 - valLoss: 0.6342385411262512 - trainLoss: 0.6377002000808716\n",
      "cnt: 0 - valLoss: 0.6342347264289856 - trainLoss: 0.6376967430114746\n",
      "cnt: 0 - valLoss: 0.6342308521270752 - trainLoss: 0.6376937031745911\n",
      "cnt: 0 - valLoss: 0.6342267394065857 - trainLoss: 0.6376904249191284\n",
      "cnt: 0 - valLoss: 0.6342229843139648 - trainLoss: 0.6376870274543762\n",
      "cnt: 0 - valLoss: 0.6342191100120544 - trainLoss: 0.6376838684082031\n",
      "cnt: 0 - valLoss: 0.6342148780822754 - trainLoss: 0.6376805901527405\n",
      "cnt: 0 - valLoss: 0.6342110633850098 - trainLoss: 0.6376771926879883\n",
      "cnt: 0 - valLoss: 0.6342073678970337 - trainLoss: 0.6376739740371704\n",
      "cnt: 0 - valLoss: 0.6342031359672546 - trainLoss: 0.6376708149909973\n",
      "cnt: 0 - valLoss: 0.6341992020606995 - trainLoss: 0.6376674175262451\n",
      "cnt: 0 - valLoss: 0.6341955065727234 - trainLoss: 0.6376641392707825\n",
      "cnt: 0 - valLoss: 0.6341913342475891 - trainLoss: 0.6376610994338989\n",
      "cnt: 0 - valLoss: 0.6341874003410339 - trainLoss: 0.6376575827598572\n",
      "cnt: 0 - valLoss: 0.6341837644577026 - trainLoss: 0.6376543045043945\n",
      "cnt: 0 - valLoss: 0.6341795921325684 - trainLoss: 0.637651264667511\n",
      "cnt: 0 - valLoss: 0.6341754794120789 - trainLoss: 0.637647807598114\n",
      "cnt: 0 - valLoss: 0.6341720223426819 - trainLoss: 0.6376444697380066\n",
      "cnt: 0 - valLoss: 0.6341678500175476 - trainLoss: 0.6376414895057678\n",
      "cnt: 0 - valLoss: 0.6341636776924133 - trainLoss: 0.6376380324363708\n",
      "cnt: 0 - valLoss: 0.6341602206230164 - trainLoss: 0.6376346349716187\n",
      "cnt: 0 - valLoss: 0.6341559886932373 - trainLoss: 0.6376315951347351\n",
      "cnt: 0 - valLoss: 0.6341518759727478 - trainLoss: 0.6376282572746277\n",
      "cnt: 0 - valLoss: 0.634148359298706 - trainLoss: 0.6376248002052307\n",
      "cnt: 0 - valLoss: 0.6341441869735718 - trainLoss: 0.6376217603683472\n",
      "cnt: 0 - valLoss: 0.6341400742530823 - trainLoss: 0.6376184225082397\n",
      "cnt: 0 - valLoss: 0.634136438369751 - trainLoss: 0.6376150250434875\n",
      "cnt: 0 - valLoss: 0.634132444858551 - trainLoss: 0.6376118659973145\n",
      "cnt: 0 - valLoss: 0.6341283321380615 - trainLoss: 0.6376087069511414\n",
      "cnt: 0 - valLoss: 0.6341245770454407 - trainLoss: 0.6376052498817444\n",
      "cnt: 0 - valLoss: 0.6341206431388855 - trainLoss: 0.6376020312309265\n",
      "cnt: 0 - valLoss: 0.634116530418396 - trainLoss: 0.6375988125801086\n",
      "cnt: 0 - valLoss: 0.6341127157211304 - trainLoss: 0.6375953555107117\n",
      "cnt: 0 - valLoss: 0.63410884141922 - trainLoss: 0.6375921368598938\n",
      "cnt: 0 - valLoss: 0.6341047286987305 - trainLoss: 0.6375890970230103\n",
      "cnt: 0 - valLoss: 0.6341008543968201 - trainLoss: 0.6375856399536133\n",
      "cnt: 0 - valLoss: 0.6340970396995544 - trainLoss: 0.6375823020935059\n",
      "cnt: 0 - valLoss: 0.6340929269790649 - trainLoss: 0.6375792622566223\n",
      "cnt: 0 - valLoss: 0.6340888142585754 - trainLoss: 0.6375758051872253\n",
      "cnt: 0 - valLoss: 0.6340852975845337 - trainLoss: 0.6375724673271179\n",
      "cnt: 0 - valLoss: 0.6340811252593994 - trainLoss: 0.6375694274902344\n",
      "cnt: 0 - valLoss: 0.6340769529342651 - trainLoss: 0.6375659704208374\n",
      "cnt: 0 - valLoss: 0.6340734362602234 - trainLoss: 0.63756263256073\n",
      "cnt: 0 - valLoss: 0.6340693235397339 - trainLoss: 0.6375596523284912\n",
      "cnt: 0 - valLoss: 0.6340651512145996 - trainLoss: 0.6375561952590942\n",
      "cnt: 0 - valLoss: 0.6340616345405579 - trainLoss: 0.6375527381896973\n",
      "cnt: 0 - valLoss: 0.6340575218200684 - trainLoss: 0.6375497579574585\n",
      "cnt: 0 - valLoss: 0.6340532898902893 - trainLoss: 0.6375463604927063\n",
      "cnt: 0 - valLoss: 0.6340497732162476 - trainLoss: 0.6375429034233093\n",
      "cnt: 0 - valLoss: 0.6340457201004028 - trainLoss: 0.6375399231910706\n",
      "cnt: 0 - valLoss: 0.6340414881706238 - trainLoss: 0.6375365257263184\n",
      "cnt: 0 - valLoss: 0.6340379118919373 - trainLoss: 0.6375330686569214\n",
      "cnt: 0 - valLoss: 0.6340338587760925 - trainLoss: 0.6375300288200378\n",
      "cnt: 0 - valLoss: 0.6340296268463135 - trainLoss: 0.6375266909599304\n",
      "cnt: 0 - valLoss: 0.6340259909629822 - trainLoss: 0.6375232934951782\n",
      "cnt: 0 - valLoss: 0.634022057056427 - trainLoss: 0.6375201344490051\n",
      "cnt: 0 - valLoss: 0.634017825126648 - trainLoss: 0.6375168561935425\n",
      "cnt: 0 - valLoss: 0.6340140700340271 - trainLoss: 0.6375133991241455\n",
      "cnt: 0 - valLoss: 0.6340101361274719 - trainLoss: 0.6375102400779724\n",
      "cnt: 0 - valLoss: 0.6340059638023376 - trainLoss: 0.6375070810317993\n",
      "cnt: 0 - valLoss: 0.634002149105072 - trainLoss: 0.6375035643577576\n",
      "cnt: 0 - valLoss: 0.6339982748031616 - trainLoss: 0.6375002861022949\n",
      "cnt: 0 - valLoss: 0.6339941620826721 - trainLoss: 0.6374972462654114\n",
      "cnt: 0 - valLoss: 0.6339902281761169 - trainLoss: 0.6374937891960144\n",
      "cnt: 0 - valLoss: 0.6339864134788513 - trainLoss: 0.637490451335907\n",
      "cnt: 0 - valLoss: 0.6339823007583618 - trainLoss: 0.6374873518943787\n",
      "cnt: 0 - valLoss: 0.6339783668518066 - trainLoss: 0.6374839544296265\n",
      "cnt: 0 - valLoss: 0.6339746117591858 - trainLoss: 0.6374805569648743\n",
      "cnt: 0 - valLoss: 0.6339704394340515 - trainLoss: 0.6374775171279907\n",
      "cnt: 0 - valLoss: 0.6339664459228516 - trainLoss: 0.6374740600585938\n",
      "cnt: 0 - valLoss: 0.6339628100395203 - trainLoss: 0.6374706625938416\n",
      "cnt: 0 - valLoss: 0.633958637714386 - trainLoss: 0.637467622756958\n",
      "cnt: 0 - valLoss: 0.6339545249938965 - trainLoss: 0.637464165687561\n",
      "cnt: 0 - valLoss: 0.6339510083198547 - trainLoss: 0.6374607682228088\n",
      "cnt: 0 - valLoss: 0.6339467763900757 - trainLoss: 0.6374577879905701\n",
      "cnt: 0 - valLoss: 0.6339426040649414 - trainLoss: 0.6374543309211731\n",
      "cnt: 0 - valLoss: 0.6339389681816101 - trainLoss: 0.6374509334564209\n",
      "cnt: 0 - valLoss: 0.6339349746704102 - trainLoss: 0.6374478936195374\n",
      "cnt: 0 - valLoss: 0.6339306831359863 - trainLoss: 0.6374444961547852\n",
      "cnt: 0 - valLoss: 0.6339271068572998 - trainLoss: 0.6374410390853882\n",
      "cnt: 0 - valLoss: 0.6339231133460999 - trainLoss: 0.6374379992485046\n",
      "cnt: 0 - valLoss: 0.633918821811676 - trainLoss: 0.6374346613883972\n",
      "cnt: 0 - valLoss: 0.6339151859283447 - trainLoss: 0.6374312043190002\n",
      "cnt: 0 - valLoss: 0.6339113116264343 - trainLoss: 0.6374281048774719\n",
      "cnt: 0 - valLoss: 0.6339070200920105 - trainLoss: 0.6374248266220093\n",
      "cnt: 0 - valLoss: 0.6339032649993896 - trainLoss: 0.6374213099479675\n",
      "cnt: 0 - valLoss: 0.6338993906974792 - trainLoss: 0.6374181509017944\n",
      "cnt: 0 - valLoss: 0.6338951587677002 - trainLoss: 0.6374149322509766\n",
      "cnt: 0 - valLoss: 0.6338913440704346 - trainLoss: 0.6374114751815796\n",
      "cnt: 0 - valLoss: 0.633887529373169 - trainLoss: 0.6374082565307617\n",
      "cnt: 0 - valLoss: 0.6338832974433899 - trainLoss: 0.6374050378799438\n",
      "cnt: 0 - valLoss: 0.6338794231414795 - trainLoss: 0.6374015808105469\n",
      "cnt: 0 - valLoss: 0.6338755488395691 - trainLoss: 0.637398362159729\n",
      "cnt: 0 - valLoss: 0.6338713765144348 - trainLoss: 0.6373952031135559\n",
      "cnt: 0 - valLoss: 0.6338675022125244 - trainLoss: 0.6373917460441589\n",
      "cnt: 0 - valLoss: 0.6338637471199036 - trainLoss: 0.6373884081840515\n",
      "cnt: 0 - valLoss: 0.6338595747947693 - trainLoss: 0.6373853087425232\n",
      "cnt: 0 - valLoss: 0.6338555812835693 - trainLoss: 0.6373818516731262\n",
      "cnt: 0 - valLoss: 0.6338517665863037 - trainLoss: 0.6373785138130188\n",
      "cnt: 0 - valLoss: 0.6338476538658142 - trainLoss: 0.6373754143714905\n",
      "cnt: 0 - valLoss: 0.6338436007499695 - trainLoss: 0.6373719573020935\n",
      "cnt: 0 - valLoss: 0.6338399648666382 - trainLoss: 0.6373685598373413\n",
      "cnt: 0 - valLoss: 0.6338357925415039 - trainLoss: 0.6373655796051025\n",
      "cnt: 0 - valLoss: 0.6338315010070801 - trainLoss: 0.6373620629310608\n",
      "cnt: 0 - valLoss: 0.6338280439376831 - trainLoss: 0.6373586058616638\n",
      "cnt: 0 - valLoss: 0.6338236927986145 - trainLoss: 0.6373555660247803\n",
      "cnt: 0 - valLoss: 0.6338195204734802 - trainLoss: 0.6373521089553833\n",
      "cnt: 0 - valLoss: 0.6338158845901489 - trainLoss: 0.6373485922813416\n",
      "cnt: 0 - valLoss: 0.6338117122650146 - trainLoss: 0.637345552444458\n",
      "cnt: 0 - valLoss: 0.6338074803352356 - trainLoss: 0.637342095375061\n",
      "cnt: 0 - valLoss: 0.6338037252426147 - trainLoss: 0.6373386383056641\n",
      "cnt: 0 - valLoss: 0.63379967212677 - trainLoss: 0.6373355984687805\n",
      "cnt: 0 - valLoss: 0.6337953805923462 - trainLoss: 0.6373320817947388\n",
      "cnt: 0 - valLoss: 0.6337916851043701 - trainLoss: 0.6373286843299866\n",
      "cnt: 0 - valLoss: 0.6337875127792358 - trainLoss: 0.6373255848884583\n",
      "cnt: 0 - valLoss: 0.6337834000587463 - trainLoss: 0.637322187423706\n",
      "cnt: 0 - valLoss: 0.6337795257568359 - trainLoss: 0.6373187303543091\n",
      "cnt: 0 - valLoss: 0.6337754130363464 - trainLoss: 0.6373155117034912\n",
      "cnt: 0 - valLoss: 0.6337713003158569 - trainLoss: 0.6373121738433838\n",
      "cnt: 0 - valLoss: 0.6337673664093018 - trainLoss: 0.637308657169342\n",
      "cnt: 0 - valLoss: 0.633763313293457 - trainLoss: 0.637305498123169\n",
      "cnt: 0 - valLoss: 0.6337591409683228 - trainLoss: 0.6373021602630615\n",
      "cnt: 0 - valLoss: 0.633755087852478 - trainLoss: 0.6372987031936646\n",
      "cnt: 0 - valLoss: 0.6337511539459229 - trainLoss: 0.6372954249382019\n",
      "cnt: 0 - valLoss: 0.6337469816207886 - trainLoss: 0.6372921466827393\n",
      "cnt: 0 - valLoss: 0.6337429881095886 - trainLoss: 0.6372886300086975\n",
      "cnt: 0 - valLoss: 0.6337390542030334 - trainLoss: 0.6372853517532349\n",
      "cnt: 0 - valLoss: 0.6337348222732544 - trainLoss: 0.637282133102417\n",
      "cnt: 0 - valLoss: 0.6337307691574097 - trainLoss: 0.6372786164283752\n",
      "cnt: 0 - valLoss: 0.6337268948554993 - trainLoss: 0.6372752785682678\n",
      "cnt: 0 - valLoss: 0.6337226629257202 - trainLoss: 0.6372721195220947\n",
      "cnt: 0 - valLoss: 0.6337185502052307 - trainLoss: 0.637268602848053\n",
      "cnt: 0 - valLoss: 0.6337146759033203 - trainLoss: 0.6372652053833008\n",
      "cnt: 0 - valLoss: 0.6337103843688965 - trainLoss: 0.6372620463371277\n",
      "cnt: 0 - valLoss: 0.6337063312530518 - trainLoss: 0.6372584700584412\n",
      "cnt: 0 - valLoss: 0.6337025165557861 - trainLoss: 0.637255072593689\n",
      "cnt: 0 - valLoss: 0.6336982250213623 - trainLoss: 0.6372520327568054\n",
      "cnt: 0 - valLoss: 0.6336941123008728 - trainLoss: 0.6372483968734741\n",
      "cnt: 0 - valLoss: 0.6336904168128967 - trainLoss: 0.6372450590133667\n",
      "cnt: 0 - valLoss: 0.6336861252784729 - trainLoss: 0.6372419595718384\n",
      "cnt: 0 - valLoss: 0.6336818933486938 - trainLoss: 0.6372384428977966\n",
      "cnt: 0 - valLoss: 0.6336781978607178 - trainLoss: 0.6372349262237549\n",
      "cnt: 0 - valLoss: 0.633673906326294 - trainLoss: 0.6372318863868713\n",
      "cnt: 0 - valLoss: 0.6336696743965149 - trainLoss: 0.6372283697128296\n",
      "cnt: 0 - valLoss: 0.6336660385131836 - trainLoss: 0.6372248530387878\n",
      "cnt: 0 - valLoss: 0.6336617469787598 - trainLoss: 0.6372218132019043\n",
      "cnt: 0 - valLoss: 0.6336574554443359 - trainLoss: 0.6372183561325073\n",
      "cnt: 0 - valLoss: 0.6336538195610046 - trainLoss: 0.6372148394584656\n",
      "cnt: 0 - valLoss: 0.6336495280265808 - trainLoss: 0.6372117400169373\n",
      "cnt: 0 - valLoss: 0.633645236492157 - trainLoss: 0.6372082829475403\n",
      "cnt: 0 - valLoss: 0.6336416006088257 - trainLoss: 0.6372047066688538\n",
      "cnt: 0 - valLoss: 0.6336373686790466 - trainLoss: 0.6372016668319702\n",
      "cnt: 0 - valLoss: 0.633633017539978 - trainLoss: 0.6371982097625732\n",
      "cnt: 0 - valLoss: 0.6336293816566467 - trainLoss: 0.6371946334838867\n",
      "cnt: 0 - valLoss: 0.6336251497268677 - trainLoss: 0.6371915340423584\n",
      "cnt: 0 - valLoss: 0.6336209177970886 - trainLoss: 0.6371881365776062\n",
      "cnt: 0 - valLoss: 0.6336170434951782 - trainLoss: 0.6371846199035645\n",
      "cnt: 0 - valLoss: 0.6336129903793335 - trainLoss: 0.6371814608573914\n",
      "cnt: 0 - valLoss: 0.6336086988449097 - trainLoss: 0.6371780633926392\n",
      "cnt: 0 - valLoss: 0.6336048245429993 - trainLoss: 0.6371745467185974\n",
      "cnt: 0 - valLoss: 0.6336008310317993 - trainLoss: 0.6371713280677795\n",
      "cnt: 0 - valLoss: 0.6335964798927307 - trainLoss: 0.6371679306030273\n",
      "cnt: 0 - valLoss: 0.6335925459861755 - trainLoss: 0.6371644139289856\n",
      "cnt: 0 - valLoss: 0.6335886120796204 - trainLoss: 0.6371611952781677\n",
      "cnt: 0 - valLoss: 0.6335842609405518 - trainLoss: 0.6371579170227051\n",
      "cnt: 0 - valLoss: 0.6335803270339966 - trainLoss: 0.6371544003486633\n",
      "cnt: 0 - valLoss: 0.6335762739181519 - trainLoss: 0.6371510624885559\n",
      "cnt: 0 - valLoss: 0.6335721015930176 - trainLoss: 0.637147843837738\n",
      "cnt: 0 - valLoss: 0.6335679888725281 - trainLoss: 0.6371442675590515\n",
      "cnt: 0 - valLoss: 0.6335641145706177 - trainLoss: 0.6371409296989441\n",
      "cnt: 0 - valLoss: 0.6335598230361938 - trainLoss: 0.6371377110481262\n",
      "cnt: 0 - valLoss: 0.6335558295249939 - trainLoss: 0.6371342539787292\n",
      "cnt: 0 - valLoss: 0.6335518956184387 - trainLoss: 0.637130856513977\n",
      "cnt: 0 - valLoss: 0.6335475444793701 - trainLoss: 0.6371276378631592\n",
      "cnt: 0 - valLoss: 0.6335434913635254 - trainLoss: 0.6371240615844727\n",
      "cnt: 0 - valLoss: 0.6335396766662598 - trainLoss: 0.6371207237243652\n",
      "cnt: 0 - valLoss: 0.6335353255271912 - trainLoss: 0.6371175646781921\n",
      "cnt: 0 - valLoss: 0.6335312724113464 - trainLoss: 0.6371139883995056\n",
      "cnt: 0 - valLoss: 0.6335274577140808 - trainLoss: 0.6371105313301086\n",
      "cnt: 0 - valLoss: 0.633523166179657 - trainLoss: 0.6371074914932251\n",
      "cnt: 0 - valLoss: 0.6335189938545227 - trainLoss: 0.6371039748191833\n",
      "cnt: 0 - valLoss: 0.6335152387619019 - trainLoss: 0.6371004581451416\n",
      "cnt: 0 - valLoss: 0.6335108876228333 - trainLoss: 0.6370973587036133\n",
      "cnt: 0 - valLoss: 0.6335065960884094 - trainLoss: 0.6370938420295715\n",
      "cnt: 0 - valLoss: 0.6335030198097229 - trainLoss: 0.6370903253555298\n",
      "cnt: 0 - valLoss: 0.6334986090660095 - trainLoss: 0.6370872259140015\n",
      "cnt: 0 - valLoss: 0.6334943175315857 - trainLoss: 0.6370837092399597\n",
      "cnt: 0 - valLoss: 0.6334907412528992 - trainLoss: 0.637080192565918\n",
      "cnt: 0 - valLoss: 0.6334863901138306 - trainLoss: 0.6370771527290344\n",
      "cnt: 0 - valLoss: 0.6334820985794067 - trainLoss: 0.6370735764503479\n",
      "cnt: 0 - valLoss: 0.6334784626960754 - trainLoss: 0.6370700001716614\n",
      "cnt: 0 - valLoss: 0.6334741711616516 - trainLoss: 0.6370669603347778\n",
      "cnt: 0 - valLoss: 0.633469820022583 - trainLoss: 0.6370635032653809\n",
      "cnt: 0 - valLoss: 0.6334661245346069 - trainLoss: 0.6370599865913391\n",
      "cnt: 0 - valLoss: 0.6334619522094727 - trainLoss: 0.6370568871498108\n",
      "cnt: 0 - valLoss: 0.6334575414657593 - trainLoss: 0.637053370475769\n",
      "cnt: 0 - valLoss: 0.633453905582428 - trainLoss: 0.6370497941970825\n",
      "cnt: 0 - valLoss: 0.6334496736526489 - trainLoss: 0.637046754360199\n",
      "cnt: 0 - valLoss: 0.6334453225135803 - trainLoss: 0.637043297290802\n",
      "cnt: 0 - valLoss: 0.6334415674209595 - trainLoss: 0.6370396614074707\n",
      "cnt: 0 - valLoss: 0.6334373354911804 - trainLoss: 0.6370366215705872\n",
      "cnt: 0 - valLoss: 0.6334331631660461 - trainLoss: 0.6370331645011902\n",
      "cnt: 0 - valLoss: 0.6334293484687805 - trainLoss: 0.6370295882225037\n",
      "cnt: 0 - valLoss: 0.6334251165390015 - trainLoss: 0.6370264887809753\n",
      "cnt: 0 - valLoss: 0.6334208250045776 - trainLoss: 0.6370230317115784\n",
      "cnt: 0 - valLoss: 0.633417010307312 - trainLoss: 0.6370194554328918\n",
      "cnt: 0 - valLoss: 0.6334128379821777 - trainLoss: 0.637016236782074\n",
      "cnt: 0 - valLoss: 0.6334086060523987 - trainLoss: 0.6370128989219666\n",
      "cnt: 0 - valLoss: 0.6334047317504883 - trainLoss: 0.63700932264328\n",
      "cnt: 0 - valLoss: 0.633400559425354 - trainLoss: 0.6370061039924622\n",
      "cnt: 0 - valLoss: 0.6333962082862854 - trainLoss: 0.6370027661323547\n",
      "cnt: 0 - valLoss: 0.6333923935890198 - trainLoss: 0.6369991898536682\n",
      "cnt: 0 - valLoss: 0.6333882808685303 - trainLoss: 0.6369959712028503\n",
      "cnt: 0 - valLoss: 0.6333839893341064 - trainLoss: 0.6369925737380981\n",
      "cnt: 0 - valLoss: 0.633380115032196 - trainLoss: 0.6369890570640564\n",
      "cnt: 0 - valLoss: 0.6333760023117065 - trainLoss: 0.6369858384132385\n",
      "cnt: 0 - valLoss: 0.6333716511726379 - trainLoss: 0.6369825005531311\n",
      "cnt: 0 - valLoss: 0.6333677172660828 - trainLoss: 0.6369789242744446\n",
      "cnt: 0 - valLoss: 0.6333637237548828 - trainLoss: 0.6369756460189819\n",
      "cnt: 0 - valLoss: 0.6333593726158142 - trainLoss: 0.6369723677635193\n",
      "cnt: 0 - valLoss: 0.633355438709259 - trainLoss: 0.6369687914848328\n",
      "cnt: 0 - valLoss: 0.6333515048027039 - trainLoss: 0.6369654536247253\n",
      "cnt: 0 - valLoss: 0.6333470940589905 - trainLoss: 0.6369622349739075\n",
      "cnt: 0 - valLoss: 0.6333431005477905 - trainLoss: 0.636958658695221\n",
      "cnt: 0 - valLoss: 0.6333391666412354 - trainLoss: 0.6369553208351135\n",
      "cnt: 0 - valLoss: 0.6333348751068115 - trainLoss: 0.6369521021842957\n",
      "cnt: 0 - valLoss: 0.6333309412002563 - trainLoss: 0.6369485259056091\n",
      "cnt: 0 - valLoss: 0.6333271265029907 - trainLoss: 0.6369451880455017\n",
      "cnt: 0 - valLoss: 0.6333227157592773 - trainLoss: 0.6369419693946838\n",
      "cnt: 0 - valLoss: 0.6333186030387878 - trainLoss: 0.6369383931159973\n",
      "cnt: 0 - valLoss: 0.6333149075508118 - trainLoss: 0.6369350552558899\n",
      "cnt: 0 - valLoss: 0.6333105564117432 - trainLoss: 0.636931836605072\n",
      "cnt: 0 - valLoss: 0.6333064436912537 - trainLoss: 0.6369283199310303\n",
      "cnt: 0 - valLoss: 0.6333026885986328 - trainLoss: 0.6369248628616333\n",
      "cnt: 0 - valLoss: 0.633298397064209 - trainLoss: 0.636921763420105\n",
      "cnt: 0 - valLoss: 0.6332942247390747 - trainLoss: 0.6369181871414185\n",
      "cnt: 0 - valLoss: 0.6332904696464539 - trainLoss: 0.6369147896766663\n",
      "cnt: 0 - valLoss: 0.63328617811203 - trainLoss: 0.6369115710258484\n",
      "cnt: 0 - valLoss: 0.6332821846008301 - trainLoss: 0.6369079947471619\n",
      "cnt: 0 - valLoss: 0.6332782506942749 - trainLoss: 0.6369046568870544\n",
      "cnt: 0 - valLoss: 0.6332739591598511 - trainLoss: 0.6369014978408813\n",
      "cnt: 0 - valLoss: 0.6332699060440063 - trainLoss: 0.6368979811668396\n",
      "cnt: 0 - valLoss: 0.6332660913467407 - trainLoss: 0.6368944644927979\n",
      "cnt: 0 - valLoss: 0.6332617402076721 - trainLoss: 0.6368913054466248\n",
      "cnt: 0 - valLoss: 0.6332576870918274 - trainLoss: 0.636887788772583\n",
      "cnt: 0 - valLoss: 0.6332538723945618 - trainLoss: 0.636884331703186\n",
      "cnt: 0 - valLoss: 0.6332495212554932 - trainLoss: 0.6368812322616577\n",
      "cnt: 0 - valLoss: 0.6332454085350037 - trainLoss: 0.6368776559829712\n",
      "cnt: 0 - valLoss: 0.6332416534423828 - trainLoss: 0.6368741393089294\n",
      "cnt: 0 - valLoss: 0.6332374215126038 - trainLoss: 0.6368710994720459\n",
      "cnt: 0 - valLoss: 0.6332331895828247 - trainLoss: 0.6368674635887146\n",
      "cnt: 0 - valLoss: 0.6332294344902039 - trainLoss: 0.6368640065193176\n",
      "cnt: 0 - valLoss: 0.6332252621650696 - trainLoss: 0.6368609666824341\n",
      "cnt: 0 - valLoss: 0.6332209706306458 - trainLoss: 0.6368573904037476\n",
      "cnt: 0 - valLoss: 0.6332172155380249 - trainLoss: 0.6368538737297058\n",
      "cnt: 0 - valLoss: 0.6332130432128906 - trainLoss: 0.636850893497467\n",
      "cnt: 0 - valLoss: 0.6332087516784668 - trainLoss: 0.6368472576141357\n",
      "cnt: 0 - valLoss: 0.6332051753997803 - trainLoss: 0.6368436813354492\n",
      "cnt: 0 - valLoss: 0.6332007646560669 - trainLoss: 0.6368407011032104\n",
      "cnt: 0 - valLoss: 0.6331964731216431 - trainLoss: 0.6368371248245239\n",
      "cnt: 0 - valLoss: 0.6331929564476013 - trainLoss: 0.6368335485458374\n",
      "cnt: 0 - valLoss: 0.6331886053085327 - trainLoss: 0.6368305683135986\n",
      "cnt: 0 - valLoss: 0.6331842541694641 - trainLoss: 0.6368269324302673\n",
      "cnt: 0 - valLoss: 0.6331807374954224 - trainLoss: 0.6368234157562256\n",
      "cnt: 0 - valLoss: 0.633176326751709 - trainLoss: 0.636820375919342\n",
      "cnt: 0 - valLoss: 0.6331720352172852 - trainLoss: 0.6368167996406555\n",
      "cnt: 0 - valLoss: 0.6331684589385986 - trainLoss: 0.6368132829666138\n",
      "cnt: 0 - valLoss: 0.6331641674041748 - trainLoss: 0.6368102431297302\n",
      "cnt: 0 - valLoss: 0.633159875869751 - trainLoss: 0.6368067264556885\n",
      "cnt: 0 - valLoss: 0.6331561803817749 - trainLoss: 0.636803150177002\n",
      "cnt: 0 - valLoss: 0.6331519484519958 - trainLoss: 0.6368000507354736\n",
      "cnt: 0 - valLoss: 0.633147656917572 - trainLoss: 0.6367965340614319\n",
      "cnt: 0 - valLoss: 0.6331440210342407 - trainLoss: 0.6367930173873901\n",
      "cnt: 0 - valLoss: 0.6331397891044617 - trainLoss: 0.6367899775505066\n",
      "cnt: 0 - valLoss: 0.6331354379653931 - trainLoss: 0.6367864012718201\n",
      "cnt: 0 - valLoss: 0.6331318020820618 - trainLoss: 0.6367828845977783\n",
      "cnt: 0 - valLoss: 0.6331275701522827 - trainLoss: 0.63677978515625\n",
      "cnt: 0 - valLoss: 0.6331233382225037 - trainLoss: 0.6367762684822083\n",
      "cnt: 0 - valLoss: 0.6331195831298828 - trainLoss: 0.6367727518081665\n",
      "cnt: 0 - valLoss: 0.633115291595459 - trainLoss: 0.636769711971283\n",
      "cnt: 0 - valLoss: 0.6331110596656799 - trainLoss: 0.6367661952972412\n",
      "cnt: 0 - valLoss: 0.6331073045730591 - trainLoss: 0.6367625594139099\n",
      "cnt: 0 - valLoss: 0.6331031322479248 - trainLoss: 0.6367595195770264\n",
      "cnt: 0 - valLoss: 0.633098840713501 - trainLoss: 0.6367560029029846\n",
      "cnt: 0 - valLoss: 0.6330952048301697 - trainLoss: 0.6367524862289429\n",
      "cnt: 0 - valLoss: 0.6330909729003906 - trainLoss: 0.6367493271827698\n",
      "cnt: 0 - valLoss: 0.633086621761322 - trainLoss: 0.6367458701133728\n",
      "cnt: 0 - valLoss: 0.6330829858779907 - trainLoss: 0.636742353439331\n",
      "cnt: 0 - valLoss: 0.6330786943435669 - trainLoss: 0.6367392539978027\n",
      "cnt: 0 - valLoss: 0.6330744028091431 - trainLoss: 0.6367357969284058\n",
      "cnt: 0 - valLoss: 0.6330706477165222 - trainLoss: 0.6367321610450745\n",
      "cnt: 0 - valLoss: 0.6330665946006775 - trainLoss: 0.6367290616035461\n",
      "cnt: 0 - valLoss: 0.6330621838569641 - trainLoss: 0.636725664138794\n",
      "cnt: 0 - valLoss: 0.6330585479736328 - trainLoss: 0.6367220878601074\n",
      "cnt: 0 - valLoss: 0.633054256439209 - trainLoss: 0.6367189288139343\n",
      "cnt: 0 - valLoss: 0.6330499053001404 - trainLoss: 0.6367154717445374\n",
      "cnt: 0 - valLoss: 0.6330462098121643 - trainLoss: 0.6367118954658508\n",
      "cnt: 0 - valLoss: 0.6330419778823853 - trainLoss: 0.6367087364196777\n",
      "cnt: 0 - valLoss: 0.633037805557251 - trainLoss: 0.6367053389549255\n",
      "cnt: 0 - valLoss: 0.6330339312553406 - trainLoss: 0.636701762676239\n",
      "cnt: 0 - valLoss: 0.6330298781394958 - trainLoss: 0.6366986036300659\n",
      "cnt: 0 - valLoss: 0.6330255270004272 - trainLoss: 0.636695146560669\n",
      "cnt: 0 - valLoss: 0.6330216526985168 - trainLoss: 0.6366916298866272\n",
      "cnt: 0 - valLoss: 0.6330175995826721 - trainLoss: 0.6366884112358093\n",
      "cnt: 0 - valLoss: 0.6330132484436035 - trainLoss: 0.6366849541664124\n",
      "cnt: 0 - valLoss: 0.6330095529556274 - trainLoss: 0.6366814970970154\n",
      "cnt: 0 - valLoss: 0.6330053210258484 - trainLoss: 0.6366782784461975\n",
      "cnt: 0 - valLoss: 0.6330010890960693 - trainLoss: 0.6366748213768005\n",
      "cnt: 0 - valLoss: 0.6329972147941589 - trainLoss: 0.6366713047027588\n",
      "cnt: 0 - valLoss: 0.6329929828643799 - trainLoss: 0.6366680860519409\n",
      "cnt: 0 - valLoss: 0.6329888105392456 - trainLoss: 0.6366646885871887\n",
      "cnt: 0 - valLoss: 0.6329849362373352 - trainLoss: 0.6366611123085022\n",
      "cnt: 0 - valLoss: 0.6329808831214905 - trainLoss: 0.6366579532623291\n",
      "cnt: 0 - valLoss: 0.6329765319824219 - trainLoss: 0.6366545557975769\n",
      "cnt: 0 - valLoss: 0.632972776889801 - trainLoss: 0.6366509199142456\n",
      "cnt: 0 - valLoss: 0.6329686045646667 - trainLoss: 0.6366477608680725\n",
      "cnt: 0 - valLoss: 0.6329642534255981 - trainLoss: 0.6366443634033203\n",
      "cnt: 0 - valLoss: 0.6329604387283325 - trainLoss: 0.6366407871246338\n",
      "cnt: 0 - valLoss: 0.6329562664031982 - trainLoss: 0.6366375684738159\n",
      "cnt: 0 - valLoss: 0.6329520344734192 - trainLoss: 0.6366341710090637\n",
      "cnt: 0 - valLoss: 0.6329481601715088 - trainLoss: 0.6366305947303772\n",
      "cnt: 0 - valLoss: 0.6329441666603088 - trainLoss: 0.6366273760795593\n",
      "cnt: 0 - valLoss: 0.6329397559165955 - trainLoss: 0.6366239190101624\n",
      "cnt: 0 - valLoss: 0.6329358816146851 - trainLoss: 0.6366204023361206\n",
      "cnt: 0 - valLoss: 0.6329318881034851 - trainLoss: 0.6366172432899475\n",
      "cnt: 0 - valLoss: 0.6329274773597717 - trainLoss: 0.6366137862205505\n",
      "cnt: 0 - valLoss: 0.6329236626625061 - trainLoss: 0.6366102695465088\n",
      "cnt: 0 - valLoss: 0.6329195499420166 - trainLoss: 0.6366070508956909\n",
      "cnt: 0 - valLoss: 0.6329152584075928 - trainLoss: 0.636603593826294\n",
      "cnt: 0 - valLoss: 0.6329114437103271 - trainLoss: 0.6366000771522522\n",
      "cnt: 0 - valLoss: 0.6329073309898376 - trainLoss: 0.6365968585014343\n",
      "cnt: 0 - valLoss: 0.632902979850769 - trainLoss: 0.6365934014320374\n",
      "cnt: 0 - valLoss: 0.6328990459442139 - trainLoss: 0.6365898847579956\n",
      "cnt: 0 - valLoss: 0.6328950524330139 - trainLoss: 0.6365866661071777\n",
      "cnt: 0 - valLoss: 0.6328907012939453 - trainLoss: 0.6365832686424255\n",
      "cnt: 0 - valLoss: 0.6328868865966797 - trainLoss: 0.6365796327590942\n",
      "cnt: 0 - valLoss: 0.6328827738761902 - trainLoss: 0.6365764737129211\n",
      "cnt: 0 - valLoss: 0.6328783631324768 - trainLoss: 0.636573076248169\n",
      "cnt: 0 - valLoss: 0.6328745484352112 - trainLoss: 0.6365694403648376\n",
      "cnt: 0 - valLoss: 0.6328704357147217 - trainLoss: 0.6365662217140198\n",
      "cnt: 0 - valLoss: 0.6328662633895874 - trainLoss: 0.6365628838539124\n",
      "cnt: 0 - valLoss: 0.6328622698783875 - trainLoss: 0.6365593075752258\n",
      "cnt: 0 - valLoss: 0.6328582167625427 - trainLoss: 0.6365560293197632\n",
      "cnt: 0 - valLoss: 0.6328538656234741 - trainLoss: 0.6365526914596558\n",
      "cnt: 0 - valLoss: 0.632849931716919 - trainLoss: 0.6365490555763245\n",
      "cnt: 0 - valLoss: 0.632845938205719 - trainLoss: 0.6365458369255066\n",
      "cnt: 0 - valLoss: 0.6328415274620056 - trainLoss: 0.6365423798561096\n",
      "cnt: 0 - valLoss: 0.6328375935554504 - trainLoss: 0.6365388035774231\n",
      "cnt: 0 - valLoss: 0.6328334808349609 - trainLoss: 0.6365355849266052\n",
      "cnt: 0 - valLoss: 0.6328290700912476 - trainLoss: 0.636532187461853\n",
      "cnt: 0 - valLoss: 0.6328251957893372 - trainLoss: 0.6365285515785217\n",
      "cnt: 0 - valLoss: 0.6328210830688477 - trainLoss: 0.6365252733230591\n",
      "cnt: 0 - valLoss: 0.6328166723251343 - trainLoss: 0.6365218758583069\n",
      "cnt: 0 - valLoss: 0.6328127980232239 - trainLoss: 0.6365182399749756\n",
      "cnt: 0 - valLoss: 0.6328086256980896 - trainLoss: 0.6365150213241577\n",
      "cnt: 0 - valLoss: 0.6328043341636658 - trainLoss: 0.6365116834640503\n",
      "cnt: 0 - valLoss: 0.6328003406524658 - trainLoss: 0.6365079879760742\n",
      "cnt: 0 - valLoss: 0.6327962279319763 - trainLoss: 0.6365047693252563\n",
      "cnt: 0 - valLoss: 0.6327918171882629 - trainLoss: 0.6365013122558594\n",
      "cnt: 0 - valLoss: 0.6327880024909973 - trainLoss: 0.6364976763725281\n",
      "cnt: 0 - valLoss: 0.632783830165863 - trainLoss: 0.636494517326355\n",
      "cnt: 0 - valLoss: 0.6327794790267944 - trainLoss: 0.636491060256958\n",
      "cnt: 0 - valLoss: 0.632775604724884 - trainLoss: 0.6364874243736267\n",
      "cnt: 0 - valLoss: 0.6327714323997498 - trainLoss: 0.6364841461181641\n",
      "cnt: 0 - valLoss: 0.6327670216560364 - trainLoss: 0.6364807486534119\n",
      "cnt: 0 - valLoss: 0.632763147354126 - trainLoss: 0.6364771127700806\n",
      "cnt: 0 - valLoss: 0.6327590346336365 - trainLoss: 0.6364739537239075\n",
      "cnt: 0 - valLoss: 0.6327546238899231 - trainLoss: 0.6364704370498657\n",
      "cnt: 0 - valLoss: 0.6327507495880127 - trainLoss: 0.6364668607711792\n",
      "cnt: 0 - valLoss: 0.6327465772628784 - trainLoss: 0.6364636421203613\n",
      "cnt: 0 - valLoss: 0.6327422261238098 - trainLoss: 0.6364601254463196\n",
      "cnt: 0 - valLoss: 0.6327382922172546 - trainLoss: 0.6364565491676331\n",
      "cnt: 0 - valLoss: 0.6327341198921204 - trainLoss: 0.6364533305168152\n",
      "cnt: 0 - valLoss: 0.6327297687530518 - trainLoss: 0.6364498734474182\n",
      "cnt: 0 - valLoss: 0.6327258944511414 - trainLoss: 0.6364462971687317\n",
      "cnt: 0 - valLoss: 0.6327217221260071 - trainLoss: 0.6364430785179138\n",
      "cnt: 0 - valLoss: 0.6327173113822937 - trainLoss: 0.6364395618438721\n",
      "cnt: 0 - valLoss: 0.6327133774757385 - trainLoss: 0.6364359855651855\n",
      "cnt: 0 - valLoss: 0.6327092051506042 - trainLoss: 0.6364327669143677\n",
      "cnt: 0 - valLoss: 0.6327048540115356 - trainLoss: 0.6364292502403259\n",
      "cnt: 0 - valLoss: 0.63270103931427 - trainLoss: 0.6364256739616394\n",
      "cnt: 0 - valLoss: 0.6326967477798462 - trainLoss: 0.6364224553108215\n",
      "cnt: 0 - valLoss: 0.6326923966407776 - trainLoss: 0.6364189386367798\n",
      "cnt: 0 - valLoss: 0.6326885223388672 - trainLoss: 0.6364153623580933\n",
      "cnt: 0 - valLoss: 0.6326842904090881 - trainLoss: 0.6364122033119202\n",
      "cnt: 0 - valLoss: 0.6326798796653748 - trainLoss: 0.6364086866378784\n",
      "cnt: 0 - valLoss: 0.6326761245727539 - trainLoss: 0.6364049911499023\n",
      "cnt: 0 - valLoss: 0.6326718330383301 - trainLoss: 0.6364018321037292\n",
      "cnt: 0 - valLoss: 0.6326674818992615 - trainLoss: 0.6363983154296875\n",
      "cnt: 0 - valLoss: 0.6326636075973511 - trainLoss: 0.6363946795463562\n",
      "cnt: 0 - valLoss: 0.632659375667572 - trainLoss: 0.6363914608955383\n",
      "cnt: 0 - valLoss: 0.6326549649238586 - trainLoss: 0.6363880038261414\n",
      "cnt: 0 - valLoss: 0.632651150226593 - trainLoss: 0.6363843679428101\n",
      "cnt: 0 - valLoss: 0.6326468586921692 - trainLoss: 0.636381208896637\n",
      "cnt: 0 - valLoss: 0.6326425075531006 - trainLoss: 0.6363776922225952\n",
      "cnt: 0 - valLoss: 0.6326387524604797 - trainLoss: 0.6363740563392639\n",
      "cnt: 0 - valLoss: 0.6326344609260559 - trainLoss: 0.6363709568977356\n",
      "cnt: 0 - valLoss: 0.6326300501823425 - trainLoss: 0.6363673806190491\n",
      "cnt: 0 - valLoss: 0.6326262354850769 - trainLoss: 0.6363637447357178\n",
      "cnt: 0 - valLoss: 0.6326219439506531 - trainLoss: 0.6363605856895447\n",
      "cnt: 0 - valLoss: 0.6326175928115845 - trainLoss: 0.6363570690155029\n",
      "cnt: 0 - valLoss: 0.6326137781143188 - trainLoss: 0.6363533735275269\n",
      "cnt: 0 - valLoss: 0.632609486579895 - trainLoss: 0.6363503336906433\n",
      "cnt: 0 - valLoss: 0.6326050758361816 - trainLoss: 0.636346697807312\n",
      "cnt: 0 - valLoss: 0.632601261138916 - trainLoss: 0.6363431215286255\n",
      "cnt: 0 - valLoss: 0.6325969696044922 - trainLoss: 0.6363399624824524\n",
      "cnt: 0 - valLoss: 0.6325925588607788 - trainLoss: 0.6363363862037659\n",
      "cnt: 0 - valLoss: 0.6325888633728027 - trainLoss: 0.6363327503204346\n",
      "cnt: 0 - valLoss: 0.6325844526290894 - trainLoss: 0.6363295912742615\n",
      "cnt: 0 - valLoss: 0.632580041885376 - trainLoss: 0.636326014995575\n",
      "cnt: 0 - valLoss: 0.6325763463973999 - trainLoss: 0.6363223791122437\n",
      "cnt: 0 - valLoss: 0.6325719356536865 - trainLoss: 0.6363192200660706\n",
      "cnt: 0 - valLoss: 0.6325676441192627 - trainLoss: 0.6363155841827393\n",
      "cnt: 0 - valLoss: 0.6325638294219971 - trainLoss: 0.6363120079040527\n",
      "cnt: 0 - valLoss: 0.6325594186782837 - trainLoss: 0.6363087892532349\n",
      "cnt: 0 - valLoss: 0.6325551867485046 - trainLoss: 0.6363052129745483\n",
      "cnt: 0 - valLoss: 0.632551372051239 - trainLoss: 0.6363016366958618\n",
      "cnt: 0 - valLoss: 0.6325469017028809 - trainLoss: 0.636298418045044\n",
      "cnt: 0 - valLoss: 0.6325427293777466 - trainLoss: 0.6362947821617126\n",
      "cnt: 0 - valLoss: 0.6325388550758362 - trainLoss: 0.6362912654876709\n",
      "cnt: 0 - valLoss: 0.632534384727478 - trainLoss: 0.6362879872322083\n",
      "cnt: 0 - valLoss: 0.6325302720069885 - trainLoss: 0.6362844109535217\n",
      "cnt: 0 - valLoss: 0.6325262784957886 - trainLoss: 0.63628089427948\n",
      "cnt: 0 - valLoss: 0.63252192735672 - trainLoss: 0.6362776160240173\n",
      "cnt: 0 - valLoss: 0.6325177550315857 - trainLoss: 0.636273980140686\n",
      "cnt: 0 - valLoss: 0.6325138211250305 - trainLoss: 0.6362705826759338\n",
      "cnt: 0 - valLoss: 0.6325094103813171 - trainLoss: 0.6362671852111816\n",
      "cnt: 0 - valLoss: 0.6325054168701172 - trainLoss: 0.6362635493278503\n",
      "cnt: 0 - valLoss: 0.6325013041496277 - trainLoss: 0.6362601518630981\n",
      "cnt: 0 - valLoss: 0.6324968934059143 - trainLoss: 0.636256754398346\n",
      "cnt: 0 - valLoss: 0.6324928998947144 - trainLoss: 0.6362531781196594\n",
      "cnt: 0 - valLoss: 0.6324887871742249 - trainLoss: 0.6362497806549072\n",
      "cnt: 0 - valLoss: 0.6324843764305115 - trainLoss: 0.636246383190155\n",
      "cnt: 0 - valLoss: 0.6324803829193115 - trainLoss: 0.636242687702179\n",
      "cnt: 0 - valLoss: 0.6324762105941772 - trainLoss: 0.6362394094467163\n",
      "cnt: 0 - valLoss: 0.6324718594551086 - trainLoss: 0.6362359523773193\n",
      "cnt: 0 - valLoss: 0.6324679255485535 - trainLoss: 0.6362322568893433\n",
      "cnt: 0 - valLoss: 0.6324636936187744 - trainLoss: 0.6362290382385254\n",
      "cnt: 0 - valLoss: 0.6324593424797058 - trainLoss: 0.6362255215644836\n",
      "cnt: 0 - valLoss: 0.6324554681777954 - trainLoss: 0.6362218856811523\n",
      "cnt: 0 - valLoss: 0.6324511170387268 - trainLoss: 0.6362186670303345\n",
      "cnt: 0 - valLoss: 0.6324467062950134 - trainLoss: 0.636215090751648\n",
      "cnt: 0 - valLoss: 0.6324430108070374 - trainLoss: 0.6362113952636719\n",
      "cnt: 0 - valLoss: 0.632438600063324 - trainLoss: 0.6362082362174988\n",
      "cnt: 0 - valLoss: 0.6324341893196106 - trainLoss: 0.6362046599388123\n",
      "cnt: 0 - valLoss: 0.6324304938316345 - trainLoss: 0.6362009644508362\n",
      "cnt: 0 - valLoss: 0.6324260234832764 - trainLoss: 0.6361978650093079\n",
      "cnt: 0 - valLoss: 0.6324216723442078 - trainLoss: 0.6361941695213318\n",
      "cnt: 0 - valLoss: 0.6324179768562317 - trainLoss: 0.6361905932426453\n",
      "cnt: 0 - valLoss: 0.6324134469032288 - trainLoss: 0.6361873149871826\n",
      "cnt: 0 - valLoss: 0.6324091553688049 - trainLoss: 0.6361836194992065\n",
      "cnt: 0 - valLoss: 0.6324054002761841 - trainLoss: 0.6361801624298096\n",
      "cnt: 0 - valLoss: 0.6324009299278259 - trainLoss: 0.6361768841743469\n",
      "cnt: 0 - valLoss: 0.6323967576026917 - trainLoss: 0.6361732482910156\n",
      "cnt: 0 - valLoss: 0.6323928236961365 - trainLoss: 0.6361696720123291\n",
      "cnt: 0 - valLoss: 0.6323883533477783 - trainLoss: 0.6361664533615112\n",
      "cnt: 0 - valLoss: 0.6323841214179993 - trainLoss: 0.6361626982688904\n",
      "cnt: 0 - valLoss: 0.6323801875114441 - trainLoss: 0.6361593008041382\n",
      "cnt: 0 - valLoss: 0.6323757767677307 - trainLoss: 0.6361559629440308\n",
      "cnt: 0 - valLoss: 0.632371723651886 - trainLoss: 0.6361522674560547\n",
      "cnt: 0 - valLoss: 0.6323676705360413 - trainLoss: 0.6361488103866577\n",
      "cnt: 0 - valLoss: 0.6323632001876831 - trainLoss: 0.6361454725265503\n",
      "cnt: 0 - valLoss: 0.6323592066764832 - trainLoss: 0.6361417770385742\n",
      "cnt: 0 - valLoss: 0.6323550939559937 - trainLoss: 0.636138379573822\n",
      "cnt: 0 - valLoss: 0.6323506236076355 - trainLoss: 0.6361349821090698\n",
      "cnt: 0 - valLoss: 0.6323466300964355 - trainLoss: 0.636131227016449\n",
      "cnt: 0 - valLoss: 0.6323424577713013 - trainLoss: 0.6361278891563416\n",
      "cnt: 0 - valLoss: 0.6323381066322327 - trainLoss: 0.6361244916915894\n",
      "cnt: 0 - valLoss: 0.6323341727256775 - trainLoss: 0.6361207962036133\n",
      "cnt: 0 - valLoss: 0.6323299407958984 - trainLoss: 0.6361175775527954\n",
      "cnt: 0 - valLoss: 0.6323255300521851 - trainLoss: 0.6361139416694641\n",
      "cnt: 0 - valLoss: 0.6323216557502747 - trainLoss: 0.6361103057861328\n",
      "cnt: 0 - valLoss: 0.632317304611206 - trainLoss: 0.6361070871353149\n",
      "cnt: 0 - valLoss: 0.6323128938674927 - trainLoss: 0.6361034512519836\n",
      "cnt: 0 - valLoss: 0.632309079170227 - trainLoss: 0.6360998153686523\n",
      "cnt: 0 - valLoss: 0.6323047280311584 - trainLoss: 0.6360965967178345\n",
      "cnt: 0 - valLoss: 0.6323003172874451 - trainLoss: 0.6360929608345032\n",
      "cnt: 0 - valLoss: 0.6322965621948242 - trainLoss: 0.6360893249511719\n",
      "cnt: 0 - valLoss: 0.6322921514511108 - trainLoss: 0.6360861659049988\n",
      "cnt: 0 - valLoss: 0.6322878003120422 - trainLoss: 0.6360824704170227\n",
      "cnt: 0 - valLoss: 0.6322839856147766 - trainLoss: 0.6360788941383362\n",
      "cnt: 0 - valLoss: 0.6322795748710632 - trainLoss: 0.6360756754875183\n",
      "cnt: 0 - valLoss: 0.6322751641273499 - trainLoss: 0.636072039604187\n",
      "cnt: 0 - valLoss: 0.6322713494300842 - trainLoss: 0.6360683441162109\n",
      "cnt: 0 - valLoss: 0.6322669386863708 - trainLoss: 0.6360651254653931\n",
      "cnt: 0 - valLoss: 0.6322627067565918 - trainLoss: 0.636061429977417\n",
      "cnt: 0 - valLoss: 0.6322587132453918 - trainLoss: 0.6360579133033752\n",
      "cnt: 0 - valLoss: 0.6322543621063232 - trainLoss: 0.6360546350479126\n",
      "cnt: 0 - valLoss: 0.6322501301765442 - trainLoss: 0.6360509395599365\n",
      "cnt: 0 - valLoss: 0.6322461366653442 - trainLoss: 0.6360474228858948\n",
      "cnt: 0 - valLoss: 0.6322417259216309 - trainLoss: 0.6360440850257874\n",
      "cnt: 0 - valLoss: 0.6322376132011414 - trainLoss: 0.636040449142456\n",
      "cnt: 0 - valLoss: 0.6322335004806519 - trainLoss: 0.6360369920730591\n",
      "cnt: 0 - valLoss: 0.6322290897369385 - trainLoss: 0.6360335946083069\n",
      "cnt: 0 - valLoss: 0.6322250366210938 - trainLoss: 0.6360298991203308\n",
      "cnt: 0 - valLoss: 0.6322208642959595 - trainLoss: 0.6360265016555786\n",
      "cnt: 0 - valLoss: 0.6322163939476013 - trainLoss: 0.6360230445861816\n",
      "cnt: 0 - valLoss: 0.6322124600410461 - trainLoss: 0.6360194087028503\n",
      "cnt: 0 - valLoss: 0.6322083473205566 - trainLoss: 0.6360160112380981\n",
      "cnt: 0 - valLoss: 0.6322038769721985 - trainLoss: 0.6360124945640564\n",
      "cnt: 0 - valLoss: 0.6321999430656433 - trainLoss: 0.6360088586807251\n",
      "cnt: 0 - valLoss: 0.6321956515312195 - trainLoss: 0.6360055208206177\n",
      "cnt: 0 - valLoss: 0.6321912407875061 - trainLoss: 0.6360020041465759\n",
      "cnt: 0 - valLoss: 0.6321873664855957 - trainLoss: 0.6359982490539551\n",
      "cnt: 0 - valLoss: 0.6321830749511719 - trainLoss: 0.6359950304031372\n",
      "cnt: 0 - valLoss: 0.632178544998169 - trainLoss: 0.6359913945198059\n",
      "cnt: 0 - valLoss: 0.6321747899055481 - trainLoss: 0.6359876990318298\n",
      "cnt: 0 - valLoss: 0.6321703791618347 - trainLoss: 0.6359845399856567\n",
      "cnt: 0 - valLoss: 0.6321659684181213 - trainLoss: 0.6359808444976807\n",
      "cnt: 0 - valLoss: 0.6321622729301453 - trainLoss: 0.6359772086143494\n",
      "cnt: 0 - valLoss: 0.6321577429771423 - trainLoss: 0.6359739899635315\n",
      "cnt: 0 - valLoss: 0.6321533918380737 - trainLoss: 0.6359702944755554\n",
      "cnt: 0 - valLoss: 0.6321495771408081 - trainLoss: 0.6359666585922241\n",
      "cnt: 0 - valLoss: 0.6321451663970947 - trainLoss: 0.635963499546051\n",
      "cnt: 0 - valLoss: 0.6321406364440918 - trainLoss: 0.635959804058075\n",
      "cnt: 0 - valLoss: 0.6321367621421814 - trainLoss: 0.6359561681747437\n",
      "cnt: 0 - valLoss: 0.632132351398468 - trainLoss: 0.635952889919281\n",
      "cnt: 0 - valLoss: 0.6321280598640442 - trainLoss: 0.6359491944313049\n",
      "cnt: 0 - valLoss: 0.632124125957489 - trainLoss: 0.6359456777572632\n",
      "cnt: 0 - valLoss: 0.6321196556091309 - trainLoss: 0.6359423398971558\n",
      "cnt: 0 - valLoss: 0.6321155428886414 - trainLoss: 0.6359386444091797\n",
      "cnt: 0 - valLoss: 0.6321114301681519 - trainLoss: 0.6359351277351379\n",
      "cnt: 0 - valLoss: 0.6321070194244385 - trainLoss: 0.6359317898750305\n",
      "cnt: 0 - valLoss: 0.6321029663085938 - trainLoss: 0.6359280943870544\n",
      "cnt: 0 - valLoss: 0.6320986747741699 - trainLoss: 0.6359246373176575\n",
      "cnt: 0 - valLoss: 0.6320942044258118 - trainLoss: 0.6359212398529053\n",
      "cnt: 0 - valLoss: 0.6320902109146118 - trainLoss: 0.6359174847602844\n",
      "cnt: 0 - valLoss: 0.6320860385894775 - trainLoss: 0.635914146900177\n",
      "cnt: 0 - valLoss: 0.6320815086364746 - trainLoss: 0.6359106302261353\n",
      "cnt: 0 - valLoss: 0.6320776343345642 - trainLoss: 0.6359069347381592\n",
      "cnt: 0 - valLoss: 0.6320733428001404 - trainLoss: 0.6359035968780518\n",
      "cnt: 0 - valLoss: 0.6320688724517822 - trainLoss: 0.63590008020401\n",
      "cnt: 0 - valLoss: 0.6320650577545166 - trainLoss: 0.6358963847160339\n",
      "cnt: 0 - valLoss: 0.6320606470108032 - trainLoss: 0.6358931064605713\n",
      "cnt: 0 - valLoss: 0.6320561766624451 - trainLoss: 0.6358895301818848\n",
      "cnt: 0 - valLoss: 0.6320523023605347 - trainLoss: 0.6358857750892639\n",
      "cnt: 0 - valLoss: 0.6320478320121765 - trainLoss: 0.635882556438446\n",
      "cnt: 0 - valLoss: 0.6320434212684631 - trainLoss: 0.6358789205551147\n",
      "cnt: 0 - valLoss: 0.6320396065711975 - trainLoss: 0.6358751654624939\n",
      "cnt: 0 - valLoss: 0.6320351362228394 - trainLoss: 0.6358720660209656\n",
      "cnt: 0 - valLoss: 0.6320308446884155 - trainLoss: 0.6358683109283447\n",
      "cnt: 0 - valLoss: 0.6320269107818604 - trainLoss: 0.6358646750450134\n",
      "cnt: 0 - valLoss: 0.632022500038147 - trainLoss: 0.6358614563941956\n",
      "cnt: 0 - valLoss: 0.6320182085037231 - trainLoss: 0.6358576416969299\n",
      "cnt: 0 - valLoss: 0.6320142149925232 - trainLoss: 0.635854184627533\n",
      "cnt: 0 - valLoss: 0.6320096850395203 - trainLoss: 0.6358508467674255\n",
      "cnt: 0 - valLoss: 0.632005512714386 - trainLoss: 0.6358470916748047\n",
      "cnt: 0 - valLoss: 0.6320014595985413 - trainLoss: 0.6358436942100525\n",
      "cnt: 0 - valLoss: 0.6319969892501831 - trainLoss: 0.6358402371406555\n",
      "cnt: 0 - valLoss: 0.6319928765296936 - trainLoss: 0.6358365416526794\n",
      "cnt: 0 - valLoss: 0.6319887638092041 - trainLoss: 0.6358330845832825\n",
      "cnt: 0 - valLoss: 0.6319842338562012 - trainLoss: 0.6358296275138855\n",
      "cnt: 0 - valLoss: 0.6319801807403564 - trainLoss: 0.6358259320259094\n",
      "cnt: 0 - valLoss: 0.6319760680198669 - trainLoss: 0.635822594165802\n",
      "cnt: 0 - valLoss: 0.631971538066864 - trainLoss: 0.6358190774917603\n",
      "cnt: 0 - valLoss: 0.6319674849510193 - trainLoss: 0.6358153223991394\n",
      "cnt: 0 - valLoss: 0.6319631934165955 - trainLoss: 0.6358120441436768\n",
      "cnt: 0 - valLoss: 0.6319587230682373 - trainLoss: 0.6358084678649902\n",
      "cnt: 0 - valLoss: 0.6319549083709717 - trainLoss: 0.6358046531677246\n",
      "cnt: 0 - valLoss: 0.6319504380226135 - trainLoss: 0.6358014941215515\n",
      "cnt: 0 - valLoss: 0.6319460272789001 - trainLoss: 0.6357978582382202\n",
      "cnt: 0 - valLoss: 0.6319422125816345 - trainLoss: 0.6357941031455994\n",
      "cnt: 0 - valLoss: 0.6319378018379211 - trainLoss: 0.6357908844947815\n",
      "cnt: 0 - valLoss: 0.6319333910942078 - trainLoss: 0.6357871890068054\n",
      "cnt: 0 - valLoss: 0.6319295167922974 - trainLoss: 0.6357834935188293\n",
      "cnt: 0 - valLoss: 0.6319250464439392 - trainLoss: 0.6357802748680115\n",
      "cnt: 0 - valLoss: 0.631920576095581 - trainLoss: 0.6357765793800354\n",
      "cnt: 0 - valLoss: 0.6319167017936707 - trainLoss: 0.6357729434967041\n",
      "cnt: 0 - valLoss: 0.6319121718406677 - trainLoss: 0.6357696652412415\n",
      "cnt: 0 - valLoss: 0.6319079399108887 - trainLoss: 0.6357659697532654\n",
      "cnt: 0 - valLoss: 0.631903886795044 - trainLoss: 0.6357623934745789\n",
      "cnt: 0 - valLoss: 0.6318994760513306 - trainLoss: 0.6357589960098267\n",
      "cnt: 0 - valLoss: 0.6318952441215515 - trainLoss: 0.6357553005218506\n",
      "cnt: 0 - valLoss: 0.6318911910057068 - trainLoss: 0.6357518434524536\n",
      "cnt: 0 - valLoss: 0.6318867206573486 - trainLoss: 0.6357484459877014\n",
      "cnt: 0 - valLoss: 0.6318826675415039 - trainLoss: 0.6357446908950806\n",
      "cnt: 0 - valLoss: 0.6318783760070801 - trainLoss: 0.6357412338256836\n",
      "cnt: 0 - valLoss: 0.6318738460540771 - trainLoss: 0.6357377171516418\n",
      "cnt: 0 - valLoss: 0.6318698525428772 - trainLoss: 0.6357340216636658\n",
      "cnt: 0 - valLoss: 0.6318656206130981 - trainLoss: 0.6357306838035583\n",
      "cnt: 0 - valLoss: 0.63186115026474 - trainLoss: 0.6357271075248718\n",
      "cnt: 0 - valLoss: 0.6318572163581848 - trainLoss: 0.635723352432251\n",
      "cnt: 0 - valLoss: 0.6318528652191162 - trainLoss: 0.6357200741767883\n",
      "cnt: 0 - valLoss: 0.6318483948707581 - trainLoss: 0.635716438293457\n",
      "cnt: 0 - valLoss: 0.6318445801734924 - trainLoss: 0.635712742805481\n",
      "cnt: 0 - valLoss: 0.631840169429779 - trainLoss: 0.6357094645500183\n",
      "cnt: 0 - valLoss: 0.6318355798721313 - trainLoss: 0.635705828666687\n",
      "cnt: 0 - valLoss: 0.6318317651748657 - trainLoss: 0.6357020735740662\n",
      "cnt: 0 - valLoss: 0.6318272948265076 - trainLoss: 0.6356989741325378\n",
      "cnt: 0 - valLoss: 0.6318228840827942 - trainLoss: 0.635695219039917\n",
      "cnt: 0 - valLoss: 0.6318190693855286 - trainLoss: 0.6356915831565857\n",
      "cnt: 0 - valLoss: 0.6318145394325256 - trainLoss: 0.6356882452964783\n",
      "cnt: 0 - valLoss: 0.6318103075027466 - trainLoss: 0.6356844902038574\n",
      "cnt: 0 - valLoss: 0.6318063139915466 - trainLoss: 0.6356809139251709\n",
      "cnt: 0 - valLoss: 0.6318018436431885 - trainLoss: 0.6356775760650635\n",
      "cnt: 0 - valLoss: 0.6317976713180542 - trainLoss: 0.6356738805770874\n",
      "cnt: 0 - valLoss: 0.6317934989929199 - trainLoss: 0.6356704235076904\n",
      "cnt: 0 - valLoss: 0.631788969039917 - trainLoss: 0.6356669664382935\n",
      "cnt: 0 - valLoss: 0.6317848563194275 - trainLoss: 0.6356632113456726\n",
      "cnt: 0 - valLoss: 0.631780743598938 - trainLoss: 0.6356597542762756\n",
      "cnt: 0 - valLoss: 0.6317762732505798 - trainLoss: 0.6356563568115234\n",
      "cnt: 0 - valLoss: 0.6317722201347351 - trainLoss: 0.6356525421142578\n",
      "cnt: 0 - valLoss: 0.631767988204956 - trainLoss: 0.6356491446495056\n",
      "cnt: 0 - valLoss: 0.6317635178565979 - trainLoss: 0.6356456279754639\n",
      "cnt: 0 - valLoss: 0.6317595839500427 - trainLoss: 0.635641872882843\n",
      "cnt: 0 - valLoss: 0.6317552328109741 - trainLoss: 0.6356385350227356\n",
      "cnt: 0 - valLoss: 0.6317506432533264 - trainLoss: 0.6356348991394043\n",
      "cnt: 0 - valLoss: 0.6317468285560608 - trainLoss: 0.635631263256073\n",
      "cnt: 0 - valLoss: 0.6317424178123474 - trainLoss: 0.6356279850006104\n",
      "cnt: 0 - valLoss: 0.6317378878593445 - trainLoss: 0.6356242895126343\n",
      "cnt: 0 - valLoss: 0.6317340731620789 - trainLoss: 0.6356205344200134\n",
      "cnt: 0 - valLoss: 0.6317296624183655 - trainLoss: 0.6356173753738403\n",
      "cnt: 0 - valLoss: 0.6317252516746521 - trainLoss: 0.6356136202812195\n",
      "cnt: 0 - valLoss: 0.6317213773727417 - trainLoss: 0.635610044002533\n",
      "cnt: 0 - valLoss: 0.6317168474197388 - trainLoss: 0.6356066465377808\n",
      "cnt: 0 - valLoss: 0.6317125558853149 - trainLoss: 0.6356028914451599\n",
      "cnt: 0 - valLoss: 0.6317084431648254 - trainLoss: 0.6355993747711182\n",
      "cnt: 0 - valLoss: 0.6317039728164673 - trainLoss: 0.6355960369110107\n",
      "cnt: 0 - valLoss: 0.631699800491333 - trainLoss: 0.6355922818183899\n",
      "cnt: 0 - valLoss: 0.6316956877708435 - trainLoss: 0.6355887651443481\n",
      "cnt: 0 - valLoss: 0.6316912174224854 - trainLoss: 0.6355852484703064\n",
      "cnt: 0 - valLoss: 0.6316871047019958 - trainLoss: 0.6355815529823303\n",
      "cnt: 0 - valLoss: 0.6316829919815063 - trainLoss: 0.6355781555175781\n",
      "cnt: 0 - valLoss: 0.6316784620285034 - trainLoss: 0.6355745792388916\n",
      "cnt: 0 - valLoss: 0.6316742897033691 - trainLoss: 0.6355708241462708\n",
      "cnt: 0 - valLoss: 0.6316699981689453 - trainLoss: 0.6355674862861633\n",
      "cnt: 0 - valLoss: 0.6316655874252319 - trainLoss: 0.6355639100074768\n",
      "cnt: 0 - valLoss: 0.6316616535186768 - trainLoss: 0.635560154914856\n",
      "cnt: 0 - valLoss: 0.6316573023796082 - trainLoss: 0.6355568766593933\n",
      "cnt: 0 - valLoss: 0.63165283203125 - trainLoss: 0.6355531811714172\n",
      "cnt: 0 - valLoss: 0.6316489577293396 - trainLoss: 0.6355494260787964\n",
      "cnt: 0 - valLoss: 0.6316444873809814 - trainLoss: 0.6355462074279785\n",
      "cnt: 0 - valLoss: 0.6316398978233337 - trainLoss: 0.6355424523353577\n",
      "cnt: 0 - valLoss: 0.6316360235214233 - trainLoss: 0.6355388164520264\n",
      "cnt: 0 - valLoss: 0.6316315531730652 - trainLoss: 0.635535478591919\n",
      "cnt: 0 - valLoss: 0.6316272020339966 - trainLoss: 0.6355317831039429\n",
      "cnt: 0 - valLoss: 0.6316232681274414 - trainLoss: 0.6355280876159668\n",
      "cnt: 0 - valLoss: 0.6316187977790833 - trainLoss: 0.6355247497558594\n",
      "cnt: 0 - valLoss: 0.6316145062446594 - trainLoss: 0.6355210542678833\n",
      "cnt: 0 - valLoss: 0.6316105127334595 - trainLoss: 0.6355174779891968\n",
      "cnt: 0 - valLoss: 0.6316058039665222 - trainLoss: 0.6355140805244446\n",
      "cnt: 0 - valLoss: 0.6316016912460327 - trainLoss: 0.6355103254318237\n",
      "cnt: 0 - valLoss: 0.6315975189208984 - trainLoss: 0.635506808757782\n",
      "cnt: 0 - valLoss: 0.6315930485725403 - trainLoss: 0.6355032920837402\n",
      "cnt: 0 - valLoss: 0.6315890550613403 - trainLoss: 0.6354995965957642\n",
      "cnt: 0 - valLoss: 0.6315847635269165 - trainLoss: 0.6354961395263672\n",
      "cnt: 0 - valLoss: 0.6315802335739136 - trainLoss: 0.6354925632476807\n",
      "cnt: 0 - valLoss: 0.6315761804580688 - trainLoss: 0.6354888081550598\n",
      "cnt: 0 - valLoss: 0.6315717697143555 - trainLoss: 0.6354855298995972\n",
      "cnt: 0 - valLoss: 0.6315672993659973 - trainLoss: 0.6354818940162659\n",
      "cnt: 0 - valLoss: 0.6315634846687317 - trainLoss: 0.6354780793190002\n",
      "cnt: 0 - valLoss: 0.6315590143203735 - trainLoss: 0.6354748606681824\n",
      "cnt: 0 - valLoss: 0.6315544843673706 - trainLoss: 0.6354711055755615\n",
      "cnt: 0 - valLoss: 0.631550669670105 - trainLoss: 0.6354674100875854\n",
      "cnt: 0 - valLoss: 0.6315461993217468 - trainLoss: 0.6354641318321228\n",
      "cnt: 0 - valLoss: 0.6315416693687439 - trainLoss: 0.635460376739502\n",
      "cnt: 0 - valLoss: 0.6315377354621887 - trainLoss: 0.6354566812515259\n",
      "cnt: 0 - valLoss: 0.6315332055091858 - trainLoss: 0.6354533433914185\n",
      "cnt: 0 - valLoss: 0.6315289735794067 - trainLoss: 0.6354495882987976\n",
      "cnt: 0 - valLoss: 0.6315248608589172 - trainLoss: 0.6354460716247559\n",
      "cnt: 0 - valLoss: 0.6315203905105591 - trainLoss: 0.6354426145553589\n",
      "cnt: 0 - valLoss: 0.6315162181854248 - trainLoss: 0.635438859462738\n",
      "cnt: 0 - valLoss: 0.631511926651001 - trainLoss: 0.6354353427886963\n",
      "cnt: 0 - valLoss: 0.6315074563026428 - trainLoss: 0.6354318857192993\n",
      "cnt: 0 - valLoss: 0.6315034627914429 - trainLoss: 0.6354281306266785\n",
      "cnt: 0 - valLoss: 0.6314991116523743 - trainLoss: 0.6354246735572815\n",
      "cnt: 0 - valLoss: 0.6314947009086609 - trainLoss: 0.635421097278595\n",
      "cnt: 0 - valLoss: 0.6314907073974609 - trainLoss: 0.6354173421859741\n",
      "cnt: 0 - valLoss: 0.6314862966537476 - trainLoss: 0.6354140043258667\n",
      "cnt: 0 - valLoss: 0.6314817667007446 - trainLoss: 0.6354103088378906\n",
      "cnt: 0 - valLoss: 0.6314778327941895 - trainLoss: 0.6354065537452698\n",
      "cnt: 0 - valLoss: 0.6314733028411865 - trainLoss: 0.6354032754898071\n",
      "cnt: 0 - valLoss: 0.6314688920974731 - trainLoss: 0.635399580001831\n",
      "cnt: 0 - valLoss: 0.6314650177955627 - trainLoss: 0.6353958249092102\n",
      "cnt: 0 - valLoss: 0.6314604878425598 - trainLoss: 0.6353926062583923\n",
      "cnt: 0 - valLoss: 0.6314560770988464 - trainLoss: 0.6353887915611267\n",
      "cnt: 0 - valLoss: 0.6314521431922913 - trainLoss: 0.6353852152824402\n",
      "cnt: 0 - valLoss: 0.6314475536346436 - trainLoss: 0.635381817817688\n",
      "cnt: 0 - valLoss: 0.6314432621002197 - trainLoss: 0.6353780627250671\n",
      "cnt: 0 - valLoss: 0.6314391493797302 - trainLoss: 0.6353744864463806\n",
      "cnt: 0 - valLoss: 0.6314346194267273 - trainLoss: 0.6353710293769836\n",
      "cnt: 0 - valLoss: 0.6314305067062378 - trainLoss: 0.6353672742843628\n",
      "cnt: 0 - valLoss: 0.6314263343811035 - trainLoss: 0.635363757610321\n",
      "cnt: 0 - valLoss: 0.6314218044281006 - trainLoss: 0.6353602409362793\n",
      "cnt: 0 - valLoss: 0.6314177513122559 - trainLoss: 0.6353564858436584\n",
      "cnt: 0 - valLoss: 0.6314132809638977 - trainLoss: 0.635353147983551\n",
      "cnt: 0 - valLoss: 0.6314087510108948 - trainLoss: 0.6353495121002197\n",
      "cnt: 0 - valLoss: 0.6314048767089844 - trainLoss: 0.6353457570075989\n",
      "cnt: 0 - valLoss: 0.631400465965271 - trainLoss: 0.6353423595428467\n",
      "cnt: 0 - valLoss: 0.6313959360122681 - trainLoss: 0.6353386640548706\n",
      "cnt: 0 - valLoss: 0.6313921809196472 - trainLoss: 0.6353349089622498\n",
      "cnt: 0 - valLoss: 0.6313875913619995 - trainLoss: 0.6353316903114319\n",
      "cnt: 0 - valLoss: 0.6313830614089966 - trainLoss: 0.635327935218811\n",
      "cnt: 0 - valLoss: 0.6313791275024414 - trainLoss: 0.635324239730835\n",
      "cnt: 0 - valLoss: 0.6313745975494385 - trainLoss: 0.6353209614753723\n",
      "cnt: 0 - valLoss: 0.6313703060150146 - trainLoss: 0.6353171467781067\n",
      "cnt: 0 - valLoss: 0.6313662528991699 - trainLoss: 0.6353135704994202\n",
      "cnt: 0 - valLoss: 0.631361722946167 - trainLoss: 0.635310173034668\n",
      "cnt: 0 - valLoss: 0.6313574910163879 - trainLoss: 0.6353063583374023\n",
      "cnt: 0 - valLoss: 0.6313533782958984 - trainLoss: 0.6353028416633606\n",
      "cnt: 0 - valLoss: 0.6313488483428955 - trainLoss: 0.6352993845939636\n",
      "cnt: 0 - valLoss: 0.6313447952270508 - trainLoss: 0.635295569896698\n",
      "cnt: 0 - valLoss: 0.6313403844833374 - trainLoss: 0.6352921724319458\n",
      "cnt: 0 - valLoss: 0.6313357949256897 - trainLoss: 0.6352885961532593\n",
      "cnt: 0 - valLoss: 0.6313318610191345 - trainLoss: 0.6352847814559937\n",
      "cnt: 0 - valLoss: 0.6313275098800659 - trainLoss: 0.6352814435958862\n",
      "cnt: 0 - valLoss: 0.631322979927063 - trainLoss: 0.6352778077125549\n",
      "cnt: 0 - valLoss: 0.6313191056251526 - trainLoss: 0.6352739930152893\n",
      "cnt: 0 - valLoss: 0.6313146352767944 - trainLoss: 0.6352707147598267\n",
      "cnt: 0 - valLoss: 0.6313101053237915 - trainLoss: 0.6352670192718506\n",
      "cnt: 0 - valLoss: 0.6313062310218811 - trainLoss: 0.6352632641792297\n",
      "cnt: 0 - valLoss: 0.6313016414642334 - trainLoss: 0.6352599859237671\n",
      "cnt: 0 - valLoss: 0.63129723072052 - trainLoss: 0.6352562308311462\n",
      "cnt: 0 - valLoss: 0.6312932372093201 - trainLoss: 0.6352525949478149\n",
      "cnt: 0 - valLoss: 0.6312887072563171 - trainLoss: 0.6352491974830627\n",
      "cnt: 0 - valLoss: 0.6312844753265381 - trainLoss: 0.6352454423904419\n",
      "cnt: 0 - valLoss: 0.6312803626060486 - trainLoss: 0.6352418065071106\n",
      "cnt: 0 - valLoss: 0.6312758326530457 - trainLoss: 0.6352384090423584\n",
      "cnt: 0 - valLoss: 0.6312717199325562 - trainLoss: 0.635234534740448\n",
      "cnt: 0 - valLoss: 0.6312674880027771 - trainLoss: 0.6352311372756958\n",
      "cnt: 0 - valLoss: 0.6312627792358398 - trainLoss: 0.635227620601654\n",
      "cnt: 0 - valLoss: 0.6312587857246399 - trainLoss: 0.6352238059043884\n",
      "cnt: 0 - valLoss: 0.6312543749809265 - trainLoss: 0.6352204084396362\n",
      "cnt: 0 - valLoss: 0.6312499046325684 - trainLoss: 0.6352168321609497\n",
      "cnt: 0 - valLoss: 0.6312459707260132 - trainLoss: 0.6352130174636841\n",
      "cnt: 0 - valLoss: 0.6312414407730103 - trainLoss: 0.6352096796035767\n",
      "cnt: 0 - valLoss: 0.6312369704246521 - trainLoss: 0.6352059841156006\n",
      "cnt: 0 - valLoss: 0.6312331557273865 - trainLoss: 0.6352022290229797\n",
      "cnt: 0 - valLoss: 0.6312285661697388 - trainLoss: 0.6351989507675171\n",
      "cnt: 0 - valLoss: 0.6312241554260254 - trainLoss: 0.6351951956748962\n",
      "cnt: 0 - valLoss: 0.6312201023101807 - trainLoss: 0.6351914405822754\n",
      "cnt: 0 - valLoss: 0.631215512752533 - trainLoss: 0.635188102722168\n",
      "cnt: 0 - valLoss: 0.6312112212181091 - trainLoss: 0.6351843476295471\n",
      "cnt: 0 - valLoss: 0.6312071681022644 - trainLoss: 0.6351807117462158\n",
      "cnt: 0 - valLoss: 0.6312026381492615 - trainLoss: 0.6351773142814636\n",
      "cnt: 0 - valLoss: 0.6311984658241272 - trainLoss: 0.635173499584198\n",
      "cnt: 0 - valLoss: 0.6311942338943481 - trainLoss: 0.6351699829101562\n",
      "cnt: 0 - valLoss: 0.6311896443367004 - trainLoss: 0.6351665258407593\n",
      "cnt: 0 - valLoss: 0.6311856508255005 - trainLoss: 0.6351626515388489\n",
      "cnt: 0 - valLoss: 0.6311811804771423 - trainLoss: 0.6351592540740967\n",
      "cnt: 0 - valLoss: 0.6311765909194946 - trainLoss: 0.6351556181907654\n",
      "cnt: 0 - valLoss: 0.6311727166175842 - trainLoss: 0.6351518630981445\n",
      "cnt: 0 - valLoss: 0.6311682462692261 - trainLoss: 0.6351485252380371\n",
      "cnt: 0 - valLoss: 0.6311637163162231 - trainLoss: 0.635144829750061\n",
      "cnt: 0 - valLoss: 0.6311598420143127 - trainLoss: 0.6351410150527954\n",
      "cnt: 0 - valLoss: 0.6311553120613098 - trainLoss: 0.6351377964019775\n",
      "cnt: 0 - valLoss: 0.6311509013175964 - trainLoss: 0.6351339221000671\n",
      "cnt: 0 - valLoss: 0.6311467289924622 - trainLoss: 0.6351302266120911\n",
      "cnt: 0 - valLoss: 0.6311421990394592 - trainLoss: 0.6351269483566284\n",
      "cnt: 0 - valLoss: 0.6311379075050354 - trainLoss: 0.6351231336593628\n",
      "cnt: 0 - valLoss: 0.6311337947845459 - trainLoss: 0.6351194977760315\n",
      "cnt: 0 - valLoss: 0.631129264831543 - trainLoss: 0.6351161003112793\n",
      "cnt: 0 - valLoss: 0.6311248540878296 - trainLoss: 0.6351123452186584\n",
      "cnt: 0 - valLoss: 0.6311206817626953 - trainLoss: 0.6351088285446167\n",
      "cnt: 0 - valLoss: 0.6311160922050476 - trainLoss: 0.635105311870575\n",
      "cnt: 0 - valLoss: 0.6311120390892029 - trainLoss: 0.6351014375686646\n",
      "cnt: 0 - valLoss: 0.631107747554779 - trainLoss: 0.6350980401039124\n",
      "cnt: 0 - valLoss: 0.631102979183197 - trainLoss: 0.635094404220581\n",
      "cnt: 0 - valLoss: 0.6310990452766418 - trainLoss: 0.6350906491279602\n",
      "cnt: 0 - valLoss: 0.6310945749282837 - trainLoss: 0.6350873112678528\n",
      "cnt: 0 - valLoss: 0.631089985370636 - trainLoss: 0.6350835561752319\n",
      "cnt: 0 - valLoss: 0.6310861706733704 - trainLoss: 0.6350798606872559\n",
      "cnt: 0 - valLoss: 0.6310814023017883 - trainLoss: 0.635076642036438\n",
      "cnt: 0 - valLoss: 0.6310770511627197 - trainLoss: 0.6350727677345276\n",
      "cnt: 0 - valLoss: 0.631072998046875 - trainLoss: 0.6350690722465515\n",
      "cnt: 0 - valLoss: 0.6310684680938721 - trainLoss: 0.6350657343864441\n",
      "cnt: 0 - valLoss: 0.631064236164093 - trainLoss: 0.6350619196891785\n",
      "cnt: 0 - valLoss: 0.6310598254203796 - trainLoss: 0.6350584030151367\n",
      "cnt: 0 - valLoss: 0.6310552358627319 - trainLoss: 0.635054886341095\n",
      "cnt: 0 - valLoss: 0.6310510635375977 - trainLoss: 0.6350510120391846\n",
      "cnt: 0 - valLoss: 0.6310465335845947 - trainLoss: 0.6350474953651428\n",
      "cnt: 0 - valLoss: 0.631041944026947 - trainLoss: 0.6350439786911011\n",
      "cnt: 0 - valLoss: 0.6310378909111023 - trainLoss: 0.6350401639938354\n",
      "cnt: 0 - valLoss: 0.6310335397720337 - trainLoss: 0.635036826133728\n",
      "cnt: 0 - valLoss: 0.6310286521911621 - trainLoss: 0.6350330710411072\n",
      "cnt: 0 - valLoss: 0.6310248970985413 - trainLoss: 0.6350293755531311\n",
      "cnt: 0 - valLoss: 0.631020188331604 - trainLoss: 0.6350259184837341\n",
      "cnt: 0 - valLoss: 0.6310160160064697 - trainLoss: 0.6350222229957581\n",
      "cnt: 0 - valLoss: 0.6310118436813354 - trainLoss: 0.635018527507782\n",
      "cnt: 0 - valLoss: 0.6310071349143982 - trainLoss: 0.6350151300430298\n",
      "cnt: 0 - valLoss: 0.6310030817985535 - trainLoss: 0.6350113749504089\n",
      "cnt: 0 - valLoss: 0.6309986710548401 - trainLoss: 0.6350077986717224\n",
      "cnt: 0 - valLoss: 0.6309940218925476 - trainLoss: 0.6350043416023254\n",
      "cnt: 0 - valLoss: 0.6309900879859924 - trainLoss: 0.6350005269050598\n",
      "cnt: 0 - valLoss: 0.6309856176376343 - trainLoss: 0.6349970698356628\n",
      "cnt: 0 - valLoss: 0.6309812068939209 - trainLoss: 0.6349934339523315\n",
      "cnt: 0 - valLoss: 0.630977213382721 - trainLoss: 0.6349896788597107\n",
      "cnt: 0 - valLoss: 0.6309725046157837 - trainLoss: 0.634986400604248\n",
      "cnt: 0 - valLoss: 0.6309682726860046 - trainLoss: 0.6349825859069824\n",
      "cnt: 0 - valLoss: 0.6309641003608704 - trainLoss: 0.6349788904190063\n",
      "cnt: 0 - valLoss: 0.6309593915939331 - trainLoss: 0.6349755525588989\n",
      "cnt: 0 - valLoss: 0.6309552788734436 - trainLoss: 0.6349717378616333\n",
      "cnt: 0 - valLoss: 0.630950927734375 - trainLoss: 0.6349681615829468\n",
      "cnt: 0 - valLoss: 0.6309462785720825 - trainLoss: 0.634964644908905\n",
      "cnt: 0 - valLoss: 0.6309422850608826 - trainLoss: 0.6349608898162842\n",
      "cnt: 0 - valLoss: 0.6309378743171692 - trainLoss: 0.6349574327468872\n",
      "cnt: 0 - valLoss: 0.6309331655502319 - trainLoss: 0.6349537968635559\n",
      "cnt: 0 - valLoss: 0.6309294104576111 - trainLoss: 0.6349500417709351\n",
      "cnt: 0 - valLoss: 0.6309247016906738 - trainLoss: 0.6349466443061829\n",
      "cnt: 0 - valLoss: 0.6309200525283813 - trainLoss: 0.634942889213562\n",
      "cnt: 0 - valLoss: 0.6309163570404053 - trainLoss: 0.6349391341209412\n",
      "cnt: 0 - valLoss: 0.630911648273468 - trainLoss: 0.6349358558654785\n",
      "cnt: 0 - valLoss: 0.6309071779251099 - trainLoss: 0.6349320411682129\n",
      "cnt: 0 - valLoss: 0.6309032440185547 - trainLoss: 0.6349283456802368\n",
      "cnt: 0 - valLoss: 0.6308985352516174 - trainLoss: 0.6349249482154846\n",
      "cnt: 0 - valLoss: 0.6308945417404175 - trainLoss: 0.6349211931228638\n",
      "cnt: 0 - valLoss: 0.6308900713920593 - trainLoss: 0.634917676448822\n",
      "cnt: 0 - valLoss: 0.6308853626251221 - trainLoss: 0.6349141597747803\n",
      "cnt: 0 - valLoss: 0.6308815479278564 - trainLoss: 0.6349102854728699\n",
      "cnt: 0 - valLoss: 0.6308770179748535 - trainLoss: 0.6349068880081177\n",
      "cnt: 0 - valLoss: 0.6308722496032715 - trainLoss: 0.6349031925201416\n",
      "cnt: 0 - valLoss: 0.6308685541152954 - trainLoss: 0.6348994374275208\n",
      "cnt: 0 - valLoss: 0.6308637261390686 - trainLoss: 0.6348961591720581\n",
      "cnt: 0 - valLoss: 0.6308592557907104 - trainLoss: 0.6348923444747925\n",
      "cnt: 0 - valLoss: 0.6308553218841553 - trainLoss: 0.6348887085914612\n",
      "cnt: 0 - valLoss: 0.630850613117218 - trainLoss: 0.634885311126709\n",
      "cnt: 0 - valLoss: 0.6308465600013733 - trainLoss: 0.6348814368247986\n",
      "cnt: 0 - valLoss: 0.6308421492576599 - trainLoss: 0.6348779201507568\n",
      "cnt: 0 - valLoss: 0.6308375000953674 - trainLoss: 0.6348744034767151\n",
      "cnt: 0 - valLoss: 0.6308335065841675 - trainLoss: 0.6348705887794495\n",
      "cnt: 0 - valLoss: 0.6308290362358093 - trainLoss: 0.6348671913146973\n",
      "cnt: 0 - valLoss: 0.6308243274688721 - trainLoss: 0.6348634958267212\n",
      "cnt: 0 - valLoss: 0.6308205127716064 - trainLoss: 0.6348596811294556\n",
      "cnt: 0 - valLoss: 0.6308158040046692 - trainLoss: 0.6348564028739929\n",
      "cnt: 0 - valLoss: 0.6308115124702454 - trainLoss: 0.6348525881767273\n",
      "cnt: 0 - valLoss: 0.6308073997497559 - trainLoss: 0.6348488926887512\n",
      "cnt: 0 - valLoss: 0.6308026909828186 - trainLoss: 0.6348455548286438\n",
      "cnt: 0 - valLoss: 0.6307985782623291 - trainLoss: 0.6348417401313782\n",
      "cnt: 0 - valLoss: 0.6307942271232605 - trainLoss: 0.6348381638526917\n",
      "cnt: 0 - valLoss: 0.6307895183563232 - trainLoss: 0.6348346471786499\n",
      "cnt: 0 - valLoss: 0.6307855844497681 - trainLoss: 0.6348308324813843\n",
      "cnt: 0 - valLoss: 0.6307810544967651 - trainLoss: 0.6348273158073425\n",
      "cnt: 0 - valLoss: 0.6307763457298279 - trainLoss: 0.634823739528656\n",
      "cnt: 0 - valLoss: 0.630772590637207 - trainLoss: 0.6348199248313904\n",
      "cnt: 0 - valLoss: 0.6307678818702698 - trainLoss: 0.634816586971283\n",
      "cnt: 0 - valLoss: 0.6307632923126221 - trainLoss: 0.6348127722740173\n",
      "cnt: 0 - valLoss: 0.6307594776153564 - trainLoss: 0.6348090767860413\n",
      "cnt: 0 - valLoss: 0.6307547688484192 - trainLoss: 0.6348057985305786\n",
      "cnt: 0 - valLoss: 0.6307502388954163 - trainLoss: 0.6348018646240234\n",
      "cnt: 0 - valLoss: 0.6307463049888611 - trainLoss: 0.6347983479499817\n",
      "cnt: 0 - valLoss: 0.6307413578033447 - trainLoss: 0.6347948312759399\n",
      "cnt: 0 - valLoss: 0.6307373642921448 - trainLoss: 0.6347909569740295\n",
      "cnt: 0 - valLoss: 0.6307328343391418 - trainLoss: 0.6347875595092773\n",
      "cnt: 0 - valLoss: 0.6307281255722046 - trainLoss: 0.6347838044166565\n",
      "cnt: 0 - valLoss: 0.6307243704795837 - trainLoss: 0.6347800493240356\n",
      "cnt: 0 - valLoss: 0.6307197213172913 - trainLoss: 0.6347766518592834\n",
      "cnt: 0 - valLoss: 0.6307153701782227 - trainLoss: 0.6347728967666626\n",
      "cnt: 0 - valLoss: 0.6307110786437988 - trainLoss: 0.6347692012786865\n",
      "cnt: 0 - valLoss: 0.6307066082954407 - trainLoss: 0.6347658634185791\n",
      "cnt: 0 - valLoss: 0.6307018399238586 - trainLoss: 0.6347620487213135\n",
      "cnt: 0 - valLoss: 0.6306979060173035 - trainLoss: 0.6347583532333374\n",
      "cnt: 0 - valLoss: 0.6306931376457214 - trainLoss: 0.6347548961639404\n",
      "cnt: 0 - valLoss: 0.6306888461112976 - trainLoss: 0.6347510814666748\n",
      "cnt: 0 - valLoss: 0.6306847333908081 - trainLoss: 0.6347475051879883\n",
      "cnt: 0 - valLoss: 0.6306799650192261 - trainLoss: 0.6347439289093018\n",
      "cnt: 0 - valLoss: 0.6306757926940918 - trainLoss: 0.6347400546073914\n",
      "cnt: 0 - valLoss: 0.6306715607643127 - trainLoss: 0.6347367167472839\n",
      "cnt: 0 - valLoss: 0.6306666135787964 - trainLoss: 0.6347330212593079\n",
      "cnt: 0 - valLoss: 0.6306628584861755 - trainLoss: 0.6347291469573975\n",
      "cnt: 0 - valLoss: 0.6306580305099487 - trainLoss: 0.6347259283065796\n",
      "cnt: 0 - valLoss: 0.6306536197662354 - trainLoss: 0.6347219944000244\n",
      "cnt: 0 - valLoss: 0.6306496262550354 - trainLoss: 0.6347184181213379\n",
      "cnt: 0 - valLoss: 0.6306448578834534 - trainLoss: 0.6347149014472961\n",
      "cnt: 0 - valLoss: 0.6306405663490295 - trainLoss: 0.6347110271453857\n",
      "cnt: 0 - valLoss: 0.6306363940238953 - trainLoss: 0.6347075700759888\n",
      "cnt: 0 - valLoss: 0.6306314468383789 - trainLoss: 0.6347039341926575\n",
      "cnt: 0 - valLoss: 0.6306275725364685 - trainLoss: 0.6347001194953918\n",
      "cnt: 0 - valLoss: 0.630622923374176 - trainLoss: 0.6346967220306396\n",
      "cnt: 0 - valLoss: 0.6306183338165283 - trainLoss: 0.6346929669380188\n",
      "cnt: 0 - valLoss: 0.6306144595146179 - trainLoss: 0.634689211845398\n",
      "cnt: 0 - valLoss: 0.6306096911430359 - trainLoss: 0.6346858739852905\n",
      "cnt: 0 - valLoss: 0.6306055784225464 - trainLoss: 0.6346820592880249\n",
      "cnt: 0 - valLoss: 0.6306010484695435 - trainLoss: 0.6346784234046936\n",
      "cnt: 0 - valLoss: 0.6305966377258301 - trainLoss: 0.6346749067306519\n",
      "cnt: 0 - valLoss: 0.6305920481681824 - trainLoss: 0.6346710920333862\n",
      "cnt: 0 - valLoss: 0.6305879354476929 - trainLoss: 0.6346675157546997\n",
      "cnt: 0 - valLoss: 0.6305831670761108 - trainLoss: 0.6346639394760132\n",
      "cnt: 0 - valLoss: 0.6305790543556213 - trainLoss: 0.6346600651741028\n",
      "cnt: 0 - valLoss: 0.6305747032165527 - trainLoss: 0.6346567273139954\n",
      "cnt: 0 - valLoss: 0.6305698156356812 - trainLoss: 0.6346530318260193\n",
      "cnt: 0 - valLoss: 0.6305660009384155 - trainLoss: 0.6346492171287537\n",
      "cnt: 0 - valLoss: 0.6305612325668335 - trainLoss: 0.6346458196640015\n",
      "cnt: 0 - valLoss: 0.6305567026138306 - trainLoss: 0.6346419453620911\n",
      "cnt: 0 - valLoss: 0.6305528879165649 - trainLoss: 0.6346383690834045\n",
      "cnt: 0 - valLoss: 0.6305478811264038 - trainLoss: 0.6346348524093628\n",
      "cnt: 0 - valLoss: 0.6305435299873352 - trainLoss: 0.6346309781074524\n",
      "cnt: 0 - valLoss: 0.6305394172668457 - trainLoss: 0.6346275210380554\n",
      "cnt: 0 - valLoss: 0.6305347084999084 - trainLoss: 0.6346238851547241\n",
      "cnt: 0 - valLoss: 0.630530595779419 - trainLoss: 0.6346200704574585\n",
      "cnt: 0 - valLoss: 0.6305258870124817 - trainLoss: 0.6346166729927063\n",
      "cnt: 0 - valLoss: 0.6305215954780579 - trainLoss: 0.6346129179000854\n",
      "cnt: 0 - valLoss: 0.6305174827575684 - trainLoss: 0.6346091628074646\n",
      "cnt: 0 - valLoss: 0.6305127739906311 - trainLoss: 0.6346057653427124\n",
      "cnt: 0 - valLoss: 0.6305083632469177 - trainLoss: 0.634601891040802\n",
      "cnt: 0 - valLoss: 0.6305040717124939 - trainLoss: 0.6345983147621155\n",
      "cnt: 0 - valLoss: 0.6304996609687805 - trainLoss: 0.634594738483429\n",
      "cnt: 0 - valLoss: 0.6304953694343567 - trainLoss: 0.6345909237861633\n",
      "cnt: 0 - valLoss: 0.6304909586906433 - trainLoss: 0.6345874667167664\n",
      "cnt: 0 - valLoss: 0.630486249923706 - trainLoss: 0.6345838308334351\n",
      "cnt: 0 - valLoss: 0.6304821372032166 - trainLoss: 0.6345799565315247\n",
      "cnt: 0 - valLoss: 0.6304774880409241 - trainLoss: 0.6345765590667725\n",
      "cnt: 0 - valLoss: 0.6304728984832764 - trainLoss: 0.6345727443695068\n",
      "cnt: 0 - valLoss: 0.6304690837860107 - trainLoss: 0.634568989276886\n",
      "cnt: 0 - valLoss: 0.6304643750190735 - trainLoss: 0.6345655918121338\n",
      "cnt: 0 - valLoss: 0.6304599046707153 - trainLoss: 0.6345617771148682\n",
      "cnt: 0 - valLoss: 0.6304556131362915 - trainLoss: 0.6345581412315369\n",
      "cnt: 0 - valLoss: 0.6304509043693542 - trainLoss: 0.6345546245574951\n",
      "cnt: 0 - valLoss: 0.6304466724395752 - trainLoss: 0.6345507502555847\n",
      "cnt: 0 - valLoss: 0.6304425001144409 - trainLoss: 0.634547233581543\n",
      "cnt: 0 - valLoss: 0.6304377913475037 - trainLoss: 0.6345435976982117\n",
      "cnt: 0 - valLoss: 0.6304337382316589 - trainLoss: 0.6345397233963013\n",
      "cnt: 0 - valLoss: 0.6304290294647217 - trainLoss: 0.6345363855361938\n",
      "cnt: 0 - valLoss: 0.630424439907074 - trainLoss: 0.6345325112342834\n",
      "cnt: 0 - valLoss: 0.6304203271865845 - trainLoss: 0.6345287561416626\n",
      "cnt: 0 - valLoss: 0.6304155588150024 - trainLoss: 0.6345253586769104\n",
      "cnt: 0 - valLoss: 0.6304112076759338 - trainLoss: 0.6345214247703552\n",
      "cnt: 0 - valLoss: 0.6304071545600891 - trainLoss: 0.6345178484916687\n",
      "cnt: 0 - valLoss: 0.6304024457931519 - trainLoss: 0.6345142722129822\n",
      "cnt: 0 - valLoss: 0.6303983330726624 - trainLoss: 0.6345103979110718\n",
      "cnt: 0 - valLoss: 0.6303936839103699 - trainLoss: 0.6345070004463196\n",
      "cnt: 0 - valLoss: 0.6303890347480774 - trainLoss: 0.634503185749054\n",
      "cnt: 0 - valLoss: 0.6303849816322327 - trainLoss: 0.6344993114471436\n",
      "cnt: 0 - valLoss: 0.6303802728652954 - trainLoss: 0.6344960331916809\n",
      "cnt: 0 - valLoss: 0.6303756833076477 - trainLoss: 0.6344922184944153\n",
      "cnt: 0 - valLoss: 0.6303718090057373 - trainLoss: 0.6344884037971497\n",
      "cnt: 0 - valLoss: 0.6303671002388 - trainLoss: 0.6344849467277527\n",
      "cnt: 0 - valLoss: 0.6303627490997314 - trainLoss: 0.6344810724258423\n",
      "cnt: 0 - valLoss: 0.6303583383560181 - trainLoss: 0.6344775557518005\n",
      "cnt: 0 - valLoss: 0.6303536295890808 - trainLoss: 0.6344739198684692\n",
      "cnt: 0 - valLoss: 0.6303494572639465 - trainLoss: 0.6344700455665588\n",
      "cnt: 0 - valLoss: 0.6303448677062988 - trainLoss: 0.6344665884971619\n",
      "cnt: 0 - valLoss: 0.6303405165672302 - trainLoss: 0.6344628930091858\n",
      "cnt: 0 - valLoss: 0.6303364634513855 - trainLoss: 0.6344590187072754\n",
      "cnt: 0 - valLoss: 0.6303317546844482 - trainLoss: 0.6344556212425232\n",
      "cnt: 0 - valLoss: 0.6303272843360901 - trainLoss: 0.6344518065452576\n",
      "cnt: 0 - valLoss: 0.6303229331970215 - trainLoss: 0.6344481110572815\n",
      "cnt: 0 - valLoss: 0.6303182244300842 - trainLoss: 0.6344445943832397\n",
      "cnt: 0 - valLoss: 0.6303139328956604 - trainLoss: 0.6344406604766846\n",
      "cnt: 0 - valLoss: 0.6303098201751709 - trainLoss: 0.6344371438026428\n",
      "cnt: 0 - valLoss: 0.6303051114082336 - trainLoss: 0.6344335675239563\n",
      "cnt: 0 - valLoss: 0.6303009390830994 - trainLoss: 0.6344296932220459\n",
      "cnt: 0 - valLoss: 0.6302962899208069 - trainLoss: 0.6344262361526489\n",
      "cnt: 0 - valLoss: 0.6302917003631592 - trainLoss: 0.6344224214553833\n",
      "cnt: 0 - valLoss: 0.6302875280380249 - trainLoss: 0.6344186663627625\n",
      "cnt: 0 - valLoss: 0.6302828192710876 - trainLoss: 0.6344152688980103\n",
      "cnt: 0 - valLoss: 0.6302784085273743 - trainLoss: 0.6344113349914551\n",
      "cnt: 0 - valLoss: 0.6302743554115295 - trainLoss: 0.6344076991081238\n",
      "cnt: 0 - valLoss: 0.6302697062492371 - trainLoss: 0.634404182434082\n",
      "cnt: 0 - valLoss: 0.6302654147148132 - trainLoss: 0.6344002485275269\n",
      "cnt: 0 - valLoss: 0.6302608847618103 - trainLoss: 0.6343967318534851\n",
      "cnt: 0 - valLoss: 0.6302561163902283 - trainLoss: 0.6343930959701538\n",
      "cnt: 0 - valLoss: 0.6302521228790283 - trainLoss: 0.6343892216682434\n",
      "cnt: 0 - valLoss: 0.6302473545074463 - trainLoss: 0.6343858242034912\n",
      "cnt: 0 - valLoss: 0.6302428245544434 - trainLoss: 0.6343819499015808\n",
      "cnt: 0 - valLoss: 0.6302390098571777 - trainLoss: 0.6343782544136047\n",
      "cnt: 0 - valLoss: 0.6302342414855957 - trainLoss: 0.634374737739563\n",
      "cnt: 0 - valLoss: 0.6302298903465271 - trainLoss: 0.6343708634376526\n",
      "cnt: 0 - valLoss: 0.630225419998169 - trainLoss: 0.6343672871589661\n",
      "cnt: 0 - valLoss: 0.6302207112312317 - trainLoss: 0.6343636512756348\n",
      "cnt: 0 - valLoss: 0.6302165389060974 - trainLoss: 0.6343597173690796\n",
      "cnt: 0 - valLoss: 0.6302118897438049 - trainLoss: 0.6343563795089722\n",
      "cnt: 0 - valLoss: 0.6302071809768677 - trainLoss: 0.6343525052070618\n",
      "cnt: 0 - valLoss: 0.6302034854888916 - trainLoss: 0.6343487501144409\n",
      "cnt: 0 - valLoss: 0.6301987171173096 - trainLoss: 0.634345293045044\n",
      "cnt: 0 - valLoss: 0.6301942467689514 - trainLoss: 0.6343414783477783\n",
      "cnt: 0 - valLoss: 0.6301899552345276 - trainLoss: 0.6343377232551575\n",
      "cnt: 0 - valLoss: 0.6301852464675903 - trainLoss: 0.6343342065811157\n",
      "cnt: 0 - valLoss: 0.6301809549331665 - trainLoss: 0.6343302726745605\n",
      "cnt: 0 - valLoss: 0.6301764249801636 - trainLoss: 0.6343267560005188\n",
      "cnt: 0 - valLoss: 0.6301717162132263 - trainLoss: 0.6343230605125427\n",
      "cnt: 0 - valLoss: 0.6301679015159607 - trainLoss: 0.6343191862106323\n",
      "cnt: 0 - valLoss: 0.6301631927490234 - trainLoss: 0.6343158483505249\n",
      "cnt: 0 - valLoss: 0.6301586031913757 - trainLoss: 0.6343119144439697\n",
      "cnt: 0 - valLoss: 0.6301543712615967 - trainLoss: 0.6343080997467041\n",
      "cnt: 0 - valLoss: 0.6301496624946594 - trainLoss: 0.6343047022819519\n",
      "cnt: 0 - valLoss: 0.6301453113555908 - trainLoss: 0.6343008279800415\n",
      "cnt: 0 - valLoss: 0.6301409006118774 - trainLoss: 0.6342971920967102\n",
      "cnt: 0 - valLoss: 0.6301361918449402 - trainLoss: 0.6342936158180237\n",
      "cnt: 0 - valLoss: 0.6301323175430298 - trainLoss: 0.6342897415161133\n",
      "cnt: 0 - valLoss: 0.6301277279853821 - trainLoss: 0.6342862248420715\n",
      "cnt: 0 - valLoss: 0.6301230192184448 - trainLoss: 0.6342824697494507\n",
      "cnt: 0 - valLoss: 0.6301189064979553 - trainLoss: 0.6342785954475403\n",
      "cnt: 0 - valLoss: 0.6301141381263733 - trainLoss: 0.6342751979827881\n",
      "cnt: 0 - valLoss: 0.6301096677780151 - trainLoss: 0.6342713236808777\n",
      "cnt: 0 - valLoss: 0.6301053762435913 - trainLoss: 0.6342675685882568\n",
      "cnt: 0 - valLoss: 0.6301006078720093 - trainLoss: 0.6342640519142151\n",
      "cnt: 0 - valLoss: 0.6300966739654541 - trainLoss: 0.6342601180076599\n",
      "cnt: 0 - valLoss: 0.6300921440124512 - trainLoss: 0.6342566013336182\n",
      "cnt: 0 - valLoss: 0.6300873756408691 - trainLoss: 0.6342529654502869\n",
      "cnt: 0 - valLoss: 0.6300833225250244 - trainLoss: 0.6342490315437317\n",
      "cnt: 0 - valLoss: 0.6300784945487976 - trainLoss: 0.6342456340789795\n",
      "cnt: 0 - valLoss: 0.6300739645957947 - trainLoss: 0.6342417597770691\n",
      "cnt: 0 - valLoss: 0.6300697922706604 - trainLoss: 0.6342379450798035\n",
      "cnt: 0 - valLoss: 0.6300650238990784 - trainLoss: 0.6342345476150513\n",
      "cnt: 0 - valLoss: 0.6300609707832336 - trainLoss: 0.6342306733131409\n",
      "cnt: 0 - valLoss: 0.6300565600395203 - trainLoss: 0.6342270374298096\n",
      "cnt: 0 - valLoss: 0.6300517916679382 - trainLoss: 0.6342233419418335\n",
      "cnt: 0 - valLoss: 0.6300476789474487 - trainLoss: 0.6342194676399231\n",
      "cnt: 0 - valLoss: 0.6300429701805115 - trainLoss: 0.6342160701751709\n",
      "cnt: 0 - valLoss: 0.630038321018219 - trainLoss: 0.6342121958732605\n",
      "cnt: 0 - valLoss: 0.6300342082977295 - trainLoss: 0.6342083215713501\n",
      "cnt: 0 - valLoss: 0.6300293803215027 - trainLoss: 0.6342049241065979\n",
      "cnt: 0 - valLoss: 0.6300249695777893 - trainLoss: 0.6342010498046875\n",
      "cnt: 0 - valLoss: 0.6300209164619446 - trainLoss: 0.6341973543167114\n",
      "cnt: 0 - valLoss: 0.6300162076950073 - trainLoss: 0.6341937780380249\n",
      "cnt: 0 - valLoss: 0.6300119161605835 - trainLoss: 0.6341899037361145\n",
      "cnt: 0 - valLoss: 0.6300073266029358 - trainLoss: 0.634186327457428\n",
      "cnt: 0 - valLoss: 0.6300026178359985 - trainLoss: 0.6341826319694519\n",
      "cnt: 0 - valLoss: 0.6299985647201538 - trainLoss: 0.6341787576675415\n",
      "cnt: 0 - valLoss: 0.6299937963485718 - trainLoss: 0.6341753005981445\n",
      "cnt: 0 - valLoss: 0.6299892067909241 - trainLoss: 0.6341714262962341\n",
      "cnt: 0 - valLoss: 0.629984974861145 - trainLoss: 0.6341676115989685\n",
      "cnt: 0 - valLoss: 0.6299805641174316 - trainLoss: 0.6341641545295715\n",
      "cnt: 0 - valLoss: 0.6299761533737183 - trainLoss: 0.6341602206230164\n",
      "cnt: 0 - valLoss: 0.6299718022346497 - trainLoss: 0.6341566443443298\n",
      "cnt: 0 - valLoss: 0.6299670338630676 - trainLoss: 0.6341529488563538\n",
      "cnt: 0 - valLoss: 0.6299628615379333 - trainLoss: 0.6341489553451538\n",
      "cnt: 0 - valLoss: 0.6299581527709961 - trainLoss: 0.6341456174850464\n",
      "cnt: 0 - valLoss: 0.6299535632133484 - trainLoss: 0.6341416835784912\n",
      "cnt: 0 - valLoss: 0.6299493908882141 - trainLoss: 0.6341378688812256\n",
      "cnt: 0 - valLoss: 0.6299445629119873 - trainLoss: 0.6341344714164734\n",
      "cnt: 0 - valLoss: 0.6299401521682739 - trainLoss: 0.6341304779052734\n",
      "cnt: 0 - valLoss: 0.6299358010292053 - trainLoss: 0.6341267824172974\n",
      "cnt: 0 - valLoss: 0.6299309730529785 - trainLoss: 0.6341231465339661\n",
      "cnt: 0 - valLoss: 0.6299271583557129 - trainLoss: 0.6341192722320557\n",
      "cnt: 0 - valLoss: 0.6299225687980652 - trainLoss: 0.6341157555580139\n",
      "cnt: 0 - valLoss: 0.6299178004264832 - trainLoss: 0.6341120004653931\n",
      "cnt: 0 - valLoss: 0.6299137473106384 - trainLoss: 0.6341080665588379\n",
      "cnt: 0 - valLoss: 0.6299089789390564 - trainLoss: 0.6341046690940857\n",
      "cnt: 0 - valLoss: 0.6299043893814087 - trainLoss: 0.6341007351875305\n",
      "cnt: 0 - valLoss: 0.6299001574516296 - trainLoss: 0.6340969800949097\n",
      "cnt: 0 - valLoss: 0.6298953890800476 - trainLoss: 0.6340934038162231\n",
      "cnt: 0 - valLoss: 0.6298910975456238 - trainLoss: 0.634089469909668\n",
      "cnt: 0 - valLoss: 0.6298865079879761 - trainLoss: 0.6340859532356262\n",
      "cnt: 0 - valLoss: 0.629881739616394 - trainLoss: 0.6340821385383606\n",
      "cnt: 0 - valLoss: 0.6298781037330627 - trainLoss: 0.6340782046318054\n",
      "cnt: 0 - valLoss: 0.6298732757568359 - trainLoss: 0.634074866771698\n",
      "cnt: 0 - valLoss: 0.6298686861991882 - trainLoss: 0.6340709328651428\n",
      "cnt: 0 - valLoss: 0.6298644542694092 - trainLoss: 0.6340671181678772\n",
      "cnt: 0 - valLoss: 0.6298596858978271 - trainLoss: 0.6340636610984802\n",
      "cnt: 0 - valLoss: 0.6298553347587585 - trainLoss: 0.634059727191925\n",
      "cnt: 0 - valLoss: 0.6298508644104004 - trainLoss: 0.6340560913085938\n",
      "cnt: 0 - valLoss: 0.6298460960388184 - trainLoss: 0.6340523958206177\n",
      "cnt: 0 - valLoss: 0.6298418641090393 - trainLoss: 0.6340484619140625\n",
      "cnt: 0 - valLoss: 0.6298372149467468 - trainLoss: 0.6340450048446655\n",
      "cnt: 0 - valLoss: 0.6298325657844543 - trainLoss: 0.6340411305427551\n",
      "cnt: 0 - valLoss: 0.6298283934593201 - trainLoss: 0.6340372562408447\n",
      "cnt: 0 - valLoss: 0.6298239827156067 - trainLoss: 0.6340338587760925\n",
      "cnt: 0 - valLoss: 0.6298195123672485 - trainLoss: 0.6340298652648926\n",
      "cnt: 0 - valLoss: 0.6298151016235352 - trainLoss: 0.6340261697769165\n",
      "cnt: 0 - valLoss: 0.6298102736473083 - trainLoss: 0.6340225338935852\n",
      "cnt: 0 - valLoss: 0.6298061013221741 - trainLoss: 0.63401859998703\n",
      "cnt: 0 - valLoss: 0.6298014521598816 - trainLoss: 0.6340151429176331\n",
      "cnt: 0 - valLoss: 0.6297967433929443 - trainLoss: 0.6340112686157227\n",
      "cnt: 0 - valLoss: 0.6297926306724548 - trainLoss: 0.634007453918457\n",
      "cnt: 0 - valLoss: 0.6297878623008728 - trainLoss: 0.6340039968490601\n",
      "cnt: 0 - valLoss: 0.6297833919525146 - trainLoss: 0.6340000629425049\n",
      "cnt: 0 - valLoss: 0.629779040813446 - trainLoss: 0.633996307849884\n",
      "cnt: 0 - valLoss: 0.6297742128372192 - trainLoss: 0.6339927315711975\n",
      "cnt: 0 - valLoss: 0.6297702789306641 - trainLoss: 0.6339887380599976\n",
      "cnt: 0 - valLoss: 0.6297657489776611 - trainLoss: 0.6339852213859558\n",
      "cnt: 0 - valLoss: 0.6297609210014343 - trainLoss: 0.6339814066886902\n",
      "cnt: 0 - valLoss: 0.6297568678855896 - trainLoss: 0.633977472782135\n",
      "cnt: 0 - valLoss: 0.6297520995140076 - trainLoss: 0.6339741349220276\n",
      "cnt: 0 - valLoss: 0.6297475695610046 - trainLoss: 0.6339701414108276\n",
      "cnt: 0 - valLoss: 0.6297432780265808 - trainLoss: 0.6339663863182068\n",
      "cnt: 0 - valLoss: 0.6297383904457092 - trainLoss: 0.6339628100395203\n",
      "cnt: 0 - valLoss: 0.6297340989112854 - trainLoss: 0.6339588761329651\n",
      "cnt: 0 - valLoss: 0.6297296285629272 - trainLoss: 0.6339552402496338\n",
      "cnt: 0 - valLoss: 0.6297248005867004 - trainLoss: 0.6339514851570129\n",
      "cnt: 0 - valLoss: 0.6297207474708557 - trainLoss: 0.6339475512504578\n",
      "cnt: 0 - valLoss: 0.6297159790992737 - trainLoss: 0.6339441537857056\n",
      "cnt: 0 - valLoss: 0.629711389541626 - trainLoss: 0.6339402198791504\n",
      "cnt: 0 - valLoss: 0.6297074556350708 - trainLoss: 0.6339364647865295\n",
      "cnt: 0 - valLoss: 0.6297026872634888 - trainLoss: 0.633932888507843\n",
      "cnt: 0 - valLoss: 0.6296983361244202 - trainLoss: 0.6339288949966431\n",
      "cnt: 0 - valLoss: 0.6296938061714172 - trainLoss: 0.6339252591133118\n",
      "cnt: 0 - valLoss: 0.6296890377998352 - trainLoss: 0.6339215636253357\n",
      "cnt: 0 - valLoss: 0.6296849250793457 - trainLoss: 0.6339176893234253\n",
      "cnt: 0 - valLoss: 0.6296801567077637 - trainLoss: 0.6339142322540283\n",
      "cnt: 0 - valLoss: 0.6296755075454712 - trainLoss: 0.6339102983474731\n",
      "cnt: 0 - valLoss: 0.6296712756156921 - trainLoss: 0.6339064240455627\n",
      "cnt: 0 - valLoss: 0.6296665072441101 - trainLoss: 0.633902907371521\n",
      "cnt: 0 - valLoss: 0.6296620965003967 - trainLoss: 0.6338989734649658\n",
      "cnt: 0 - valLoss: 0.6296576857566833 - trainLoss: 0.6338952779769897\n",
      "cnt: 0 - valLoss: 0.6296528577804565 - trainLoss: 0.6338916420936584\n",
      "cnt: 0 - valLoss: 0.6296490430831909 - trainLoss: 0.6338876485824585\n",
      "cnt: 0 - valLoss: 0.6296443939208984 - trainLoss: 0.6338841915130615\n",
      "cnt: 0 - valLoss: 0.6296396851539612 - trainLoss: 0.6338802576065063\n",
      "cnt: 0 - valLoss: 0.6296355128288269 - trainLoss: 0.633876383304596\n",
      "cnt: 0 - valLoss: 0.6296306848526001 - trainLoss: 0.633872926235199\n",
      "cnt: 0 - valLoss: 0.6296262145042419 - trainLoss: 0.633868932723999\n",
      "cnt: 0 - valLoss: 0.6296218037605286 - trainLoss: 0.633865237236023\n",
      "cnt: 0 - valLoss: 0.6296169757843018 - trainLoss: 0.6338615417480469\n",
      "cnt: 0 - valLoss: 0.6296128034591675 - trainLoss: 0.6338576078414917\n",
      "cnt: 0 - valLoss: 0.6296080946922302 - trainLoss: 0.6338541507720947\n",
      "cnt: 0 - valLoss: 0.629603385925293 - trainLoss: 0.6338502764701843\n",
      "cnt: 0 - valLoss: 0.6295992136001587 - trainLoss: 0.6338464021682739\n",
      "cnt: 0 - valLoss: 0.6295944452285767 - trainLoss: 0.6338428854942322\n",
      "cnt: 0 - valLoss: 0.6295899748802185 - trainLoss: 0.6338388919830322\n",
      "cnt: 0 - valLoss: 0.629585862159729 - trainLoss: 0.6338351964950562\n",
      "cnt: 0 - valLoss: 0.629581093788147 - trainLoss: 0.6338315010070801\n",
      "cnt: 0 - valLoss: 0.6295768618583679 - trainLoss: 0.6338275671005249\n",
      "cnt: 0 - valLoss: 0.6295722126960754 - trainLoss: 0.6338240504264832\n",
      "cnt: 0 - valLoss: 0.6295675039291382 - trainLoss: 0.6338202357292175\n",
      "cnt: 0 - valLoss: 0.6295633912086487 - trainLoss: 0.6338163018226624\n",
      "cnt: 0 - valLoss: 0.6295585036277771 - trainLoss: 0.6338128447532654\n",
      "cnt: 0 - valLoss: 0.6295539736747742 - trainLoss: 0.6338088512420654\n",
      "cnt: 0 - valLoss: 0.6295496225357056 - trainLoss: 0.6338050961494446\n",
      "cnt: 0 - valLoss: 0.6295448541641235 - trainLoss: 0.6338015198707581\n",
      "cnt: 0 - valLoss: 0.6295405626296997 - trainLoss: 0.6337975263595581\n",
      "cnt: 0 - valLoss: 0.629535973072052 - trainLoss: 0.6337940096855164\n",
      "cnt: 0 - valLoss: 0.6295311450958252 - trainLoss: 0.6337901949882507\n",
      "cnt: 0 - valLoss: 0.6295270323753357 - trainLoss: 0.6337862014770508\n",
      "cnt: 0 - valLoss: 0.6295221447944641 - trainLoss: 0.6337828040122986\n",
      "cnt: 0 - valLoss: 0.6295180320739746 - trainLoss: 0.6337788105010986\n",
      "cnt: 0 - valLoss: 0.6295136213302612 - trainLoss: 0.633774995803833\n",
      "cnt: 0 - valLoss: 0.629508912563324 - trainLoss: 0.6337713599205017\n",
      "cnt: 0 - valLoss: 0.6295046210289001 - trainLoss: 0.6337674260139465\n",
      "cnt: 0 - valLoss: 0.6294999718666077 - trainLoss: 0.63376384973526\n",
      "cnt: 0 - valLoss: 0.6294951438903809 - trainLoss: 0.6337600350379944\n",
      "cnt: 0 - valLoss: 0.6294910907745361 - trainLoss: 0.6337560415267944\n",
      "cnt: 0 - valLoss: 0.6294862627983093 - trainLoss: 0.633752703666687\n",
      "cnt: 0 - valLoss: 0.6294817328453064 - trainLoss: 0.6337487101554871\n",
      "cnt: 0 - valLoss: 0.629477322101593 - trainLoss: 0.6337448954582214\n",
      "cnt: 0 - valLoss: 0.629472553730011 - trainLoss: 0.6337413191795349\n",
      "cnt: 0 - valLoss: 0.6294682621955872 - trainLoss: 0.6337373852729797\n",
      "cnt: 0 - valLoss: 0.6294636130332947 - trainLoss: 0.6337337493896484\n",
      "cnt: 0 - valLoss: 0.6294588446617126 - trainLoss: 0.6337299346923828\n",
      "cnt: 0 - valLoss: 0.6294550895690918 - trainLoss: 0.6337259411811829\n",
      "cnt: 0 - valLoss: 0.6294503211975098 - trainLoss: 0.6337225437164307\n",
      "cnt: 0 - valLoss: 0.6294456720352173 - trainLoss: 0.6337185502052307\n",
      "cnt: 0 - valLoss: 0.6294413208961487 - trainLoss: 0.6337147355079651\n",
      "cnt: 0 - valLoss: 0.6294365525245667 - trainLoss: 0.6337111592292786\n",
      "cnt: 0 - valLoss: 0.629432201385498 - trainLoss: 0.6337071657180786\n",
      "cnt: 0 - valLoss: 0.6294276118278503 - trainLoss: 0.6337035894393921\n",
      "cnt: 0 - valLoss: 0.6294227838516235 - trainLoss: 0.6336997747421265\n",
      "cnt: 0 - valLoss: 0.6294187307357788 - trainLoss: 0.6336957812309265\n",
      "cnt: 0 - valLoss: 0.629413902759552 - trainLoss: 0.6336924433708191\n",
      "cnt: 0 - valLoss: 0.6294093132019043 - trainLoss: 0.6336883902549744\n",
      "cnt: 0 - valLoss: 0.6294049620628357 - trainLoss: 0.6336846351623535\n",
      "cnt: 0 - valLoss: 0.6294000744819641 - trainLoss: 0.633681058883667\n",
      "cnt: 0 - valLoss: 0.6293961405754089 - trainLoss: 0.6336770057678223\n",
      "cnt: 0 - valLoss: 0.6293915510177612 - trainLoss: 0.6336734294891357\n",
      "cnt: 0 - valLoss: 0.6293867230415344 - trainLoss: 0.6336696743965149\n",
      "cnt: 0 - valLoss: 0.6293826699256897 - trainLoss: 0.6336656212806702\n",
      "cnt: 0 - valLoss: 0.6293778419494629 - trainLoss: 0.633662223815918\n",
      "cnt: 0 - valLoss: 0.6293732523918152 - trainLoss: 0.6336581707000732\n",
      "cnt: 0 - valLoss: 0.6293689012527466 - trainLoss: 0.6336544156074524\n",
      "cnt: 0 - valLoss: 0.6293640732765198 - trainLoss: 0.6336508393287659\n",
      "cnt: 0 - valLoss: 0.6293597221374512 - trainLoss: 0.6336467862129211\n",
      "cnt: 0 - valLoss: 0.6293551325798035 - trainLoss: 0.6336432099342346\n",
      "cnt: 0 - valLoss: 0.6293502449989319 - trainLoss: 0.6336394548416138\n",
      "cnt: 0 - valLoss: 0.6293461918830872 - trainLoss: 0.633635401725769\n",
      "cnt: 0 - valLoss: 0.6293413043022156 - trainLoss: 0.6336320042610168\n",
      "cnt: 0 - valLoss: 0.6293367147445679 - trainLoss: 0.6336280107498169\n",
      "cnt: 0 - valLoss: 0.6293327212333679 - trainLoss: 0.6336241960525513\n",
      "cnt: 0 - valLoss: 0.6293279528617859 - trainLoss: 0.6336206197738647\n",
      "cnt: 0 - valLoss: 0.6293236017227173 - trainLoss: 0.63361656665802\n",
      "cnt: 0 - valLoss: 0.6293190121650696 - trainLoss: 0.6336129903793335\n",
      "cnt: 0 - valLoss: 0.6293141841888428 - trainLoss: 0.6336092352867126\n",
      "cnt: 0 - valLoss: 0.6293100714683533 - trainLoss: 0.6336051821708679\n",
      "cnt: 0 - valLoss: 0.6293051838874817 - trainLoss: 0.633601725101471\n",
      "cnt: 0 - valLoss: 0.629300594329834 - trainLoss: 0.633597731590271\n",
      "cnt: 0 - valLoss: 0.6292962431907654 - trainLoss: 0.6335939764976501\n",
      "cnt: 0 - valLoss: 0.6292914152145386 - trainLoss: 0.6335904002189636\n",
      "cnt: 0 - valLoss: 0.6292871236801147 - trainLoss: 0.6335862874984741\n",
      "cnt: 0 - valLoss: 0.6292824745178223 - trainLoss: 0.6335827708244324\n",
      "cnt: 0 - valLoss: 0.6292776465415955 - trainLoss: 0.633578896522522\n",
      "cnt: 0 - valLoss: 0.6292735934257507 - trainLoss: 0.6335749626159668\n",
      "cnt: 0 - valLoss: 0.629269003868103 - trainLoss: 0.6335715651512146\n",
      "cnt: 0 - valLoss: 0.6292644739151001 - trainLoss: 0.6335675120353699\n",
      "cnt: 0 - valLoss: 0.6292601227760315 - trainLoss: 0.6335636973381042\n",
      "cnt: 0 - valLoss: 0.6292552351951599 - trainLoss: 0.633560061454773\n",
      "cnt: 0 - valLoss: 0.6292509436607361 - trainLoss: 0.633556067943573\n",
      "cnt: 0 - valLoss: 0.6292462944984436 - trainLoss: 0.6335524320602417\n",
      "cnt: 0 - valLoss: 0.6292415857315063 - trainLoss: 0.6335486173629761\n",
      "cnt: 0 - valLoss: 0.6292373538017273 - trainLoss: 0.6335446834564209\n",
      "cnt: 0 - valLoss: 0.6292325258255005 - trainLoss: 0.6335412263870239\n",
      "cnt: 0 - valLoss: 0.6292279958724976 - trainLoss: 0.633537232875824\n",
      "cnt: 0 - valLoss: 0.6292235851287842 - trainLoss: 0.6335334777832031\n",
      "cnt: 0 - valLoss: 0.6292186975479126 - trainLoss: 0.633529782295227\n",
      "cnt: 0 - valLoss: 0.6292144060134888 - trainLoss: 0.6335257887840271\n",
      "cnt: 0 - valLoss: 0.6292097568511963 - trainLoss: 0.6335222125053406\n",
      "cnt: 0 - valLoss: 0.6292054057121277 - trainLoss: 0.633518397808075\n",
      "cnt: 0 - valLoss: 0.6292011141777039 - trainLoss: 0.6335144639015198\n",
      "cnt: 0 - valLoss: 0.629196286201477 - trainLoss: 0.633510947227478\n",
      "cnt: 0 - valLoss: 0.6291918158531189 - trainLoss: 0.6335068941116333\n",
      "cnt: 0 - valLoss: 0.6291873455047607 - trainLoss: 0.6335031986236572\n",
      "cnt: 0 - valLoss: 0.6291825175285339 - trainLoss: 0.6334994435310364\n",
      "cnt: 0 - valLoss: 0.6291782259941101 - trainLoss: 0.6334954500198364\n",
      "cnt: 0 - valLoss: 0.6291735768318176 - trainLoss: 0.6334919333457947\n",
      "cnt: 0 - valLoss: 0.6291688084602356 - trainLoss: 0.6334880590438843\n",
      "cnt: 0 - valLoss: 0.6291646361351013 - trainLoss: 0.6334840655326843\n",
      "cnt: 0 - valLoss: 0.6291597485542297 - trainLoss: 0.6334806084632874\n",
      "cnt: 0 - valLoss: 0.6291552186012268 - trainLoss: 0.6334765553474426\n",
      "cnt: 0 - valLoss: 0.6291507482528687 - trainLoss: 0.6334728598594666\n",
      "cnt: 0 - valLoss: 0.6291458606719971 - trainLoss: 0.6334691047668457\n",
      "cnt: 0 - valLoss: 0.6291417479515076 - trainLoss: 0.6334651112556458\n",
      "cnt: 0 - valLoss: 0.6291373372077942 - trainLoss: 0.6334616541862488\n",
      "cnt: 0 - valLoss: 0.6291326284408569 - trainLoss: 0.6334577202796936\n",
      "cnt: 0 - valLoss: 0.6291283369064331 - trainLoss: 0.6334537863731384\n",
      "cnt: 0 - valLoss: 0.6291234493255615 - trainLoss: 0.6334502696990967\n",
      "cnt: 0 - valLoss: 0.6291190385818481 - trainLoss: 0.633446216583252\n",
      "cnt: 0 - valLoss: 0.6291144490242004 - trainLoss: 0.6334425210952759\n",
      "cnt: 0 - valLoss: 0.6291096210479736 - trainLoss: 0.6334387063980103\n",
      "cnt: 0 - valLoss: 0.6291054487228394 - trainLoss: 0.6334347128868103\n",
      "cnt: 0 - valLoss: 0.6291006207466125 - trainLoss: 0.6334312558174133\n",
      "cnt: 0 - valLoss: 0.6290959715843201 - trainLoss: 0.6334272623062134\n",
      "cnt: 0 - valLoss: 0.6290916800498962 - trainLoss: 0.633423388004303\n",
      "cnt: 0 - valLoss: 0.6290867924690247 - trainLoss: 0.6334198713302612\n",
      "cnt: 0 - valLoss: 0.629082441329956 - trainLoss: 0.6334158182144165\n",
      "cnt: 0 - valLoss: 0.6290777921676636 - trainLoss: 0.6334121227264404\n",
      "cnt: 0 - valLoss: 0.6290733218193054 - trainLoss: 0.6334083676338196\n",
      "cnt: 0 - valLoss: 0.6290692090988159 - trainLoss: 0.6334043145179749\n",
      "cnt: 0 - valLoss: 0.6290643215179443 - trainLoss: 0.6334009170532227\n",
      "cnt: 0 - valLoss: 0.6290597319602966 - trainLoss: 0.6333968043327332\n",
      "cnt: 0 - valLoss: 0.629055380821228 - trainLoss: 0.6333930492401123\n",
      "cnt: 0 - valLoss: 0.6290504932403564 - trainLoss: 0.633389413356781\n",
      "cnt: 0 - valLoss: 0.6290461421012878 - trainLoss: 0.6333853602409363\n",
      "cnt: 0 - valLoss: 0.6290414929389954 - trainLoss: 0.6333817839622498\n",
      "cnt: 0 - valLoss: 0.6290366649627686 - trainLoss: 0.6333779096603394\n",
      "cnt: 0 - valLoss: 0.6290324926376343 - trainLoss: 0.6333739161491394\n",
      "cnt: 0 - valLoss: 0.6290275454521179 - trainLoss: 0.6333704590797424\n",
      "cnt: 0 - valLoss: 0.629023015499115 - trainLoss: 0.6333664059638977\n",
      "cnt: 0 - valLoss: 0.6290186047554016 - trainLoss: 0.6333625912666321\n",
      "cnt: 0 - valLoss: 0.6290137767791748 - trainLoss: 0.6333590149879456\n",
      "cnt: 0 - valLoss: 0.6290097832679749 - trainLoss: 0.6333549618721008\n",
      "cnt: 0 - valLoss: 0.6290051341056824 - trainLoss: 0.6333513855934143\n",
      "cnt: 0 - valLoss: 0.6290002465248108 - trainLoss: 0.6333475112915039\n",
      "cnt: 0 - valLoss: 0.6289960741996765 - trainLoss: 0.633343517780304\n",
      "cnt: 0 - valLoss: 0.6289911866188049 - trainLoss: 0.6333400011062622\n",
      "cnt: 0 - valLoss: 0.6289867162704468 - trainLoss: 0.6333360075950623\n",
      "cnt: 0 - valLoss: 0.6289821863174438 - trainLoss: 0.6333321928977966\n",
      "cnt: 0 - valLoss: 0.6289772987365723 - trainLoss: 0.6333284974098206\n",
      "cnt: 0 - valLoss: 0.6289730072021484 - trainLoss: 0.6333245038986206\n",
      "cnt: 0 - valLoss: 0.6289682984352112 - trainLoss: 0.6333209276199341\n",
      "cnt: 0 - valLoss: 0.6289635896682739 - trainLoss: 0.6333170533180237\n",
      "cnt: 0 - valLoss: 0.6289592981338501 - trainLoss: 0.6333131790161133\n",
      "cnt: 0 - valLoss: 0.6289547681808472 - trainLoss: 0.6333096027374268\n",
      "cnt: 0 - valLoss: 0.628950297832489 - trainLoss: 0.633305549621582\n",
      "cnt: 0 - valLoss: 0.6289457082748413 - trainLoss: 0.633301854133606\n",
      "cnt: 0 - valLoss: 0.6289408206939697 - trainLoss: 0.6332980990409851\n",
      "cnt: 0 - valLoss: 0.6289366483688354 - trainLoss: 0.6332940459251404\n",
      "cnt: 0 - valLoss: 0.6289318203926086 - trainLoss: 0.6332905292510986\n",
      "cnt: 0 - valLoss: 0.6289271116256714 - trainLoss: 0.6332865953445435\n",
      "cnt: 0 - valLoss: 0.6289228200912476 - trainLoss: 0.6332826614379883\n",
      "cnt: 0 - valLoss: 0.6289179921150208 - trainLoss: 0.633279025554657\n",
      "cnt: 0 - valLoss: 0.6289136409759521 - trainLoss: 0.6332749724388123\n",
      "cnt: 0 - valLoss: 0.6289089918136597 - trainLoss: 0.633271336555481\n",
      "cnt: 0 - valLoss: 0.6289041042327881 - trainLoss: 0.6332674622535706\n",
      "cnt: 0 - valLoss: 0.6288999915122986 - trainLoss: 0.633263349533081\n",
      "cnt: 0 - valLoss: 0.6288951635360718 - trainLoss: 0.6332599520683289\n",
      "cnt: 0 - valLoss: 0.6288905739784241 - trainLoss: 0.6332558393478394\n",
      "cnt: 0 - valLoss: 0.6288861036300659 - trainLoss: 0.6332520246505737\n",
      "cnt: 0 - valLoss: 0.6288812160491943 - trainLoss: 0.6332483291625977\n",
      "cnt: 0 - valLoss: 0.6288769245147705 - trainLoss: 0.6332442760467529\n",
      "cnt: 0 - valLoss: 0.628872275352478 - trainLoss: 0.6332406401634216\n",
      "cnt: 0 - valLoss: 0.6288673877716064 - trainLoss: 0.6332367658615112\n",
      "cnt: 0 - valLoss: 0.6288632750511169 - trainLoss: 0.6332327127456665\n",
      "cnt: 0 - valLoss: 0.6288583874702454 - trainLoss: 0.6332291960716248\n",
      "cnt: 0 - valLoss: 0.6288538575172424 - trainLoss: 0.63322514295578\n",
      "cnt: 0 - valLoss: 0.6288493871688843 - trainLoss: 0.6332213282585144\n",
      "cnt: 0 - valLoss: 0.6288444995880127 - trainLoss: 0.6332176327705383\n",
      "cnt: 0 - valLoss: 0.6288402676582336 - trainLoss: 0.6332135796546936\n",
      "cnt: 0 - valLoss: 0.6288354992866516 - trainLoss: 0.6332099437713623\n",
      "cnt: 0 - valLoss: 0.6288307905197144 - trainLoss: 0.6332059502601624\n",
      "cnt: 0 - valLoss: 0.6288264989852905 - trainLoss: 0.6332020163536072\n",
      "cnt: 0 - valLoss: 0.628821611404419 - trainLoss: 0.6331984996795654\n",
      "cnt: 0 - valLoss: 0.6288171410560608 - trainLoss: 0.6331944465637207\n",
      "cnt: 0 - valLoss: 0.6288126111030579 - trainLoss: 0.6331906318664551\n",
      "cnt: 0 - valLoss: 0.6288077235221863 - trainLoss: 0.6331868767738342\n",
      "cnt: 0 - valLoss: 0.628803551197052 - trainLoss: 0.6331828236579895\n",
      "cnt: 0 - valLoss: 0.6287987232208252 - trainLoss: 0.633179247379303\n",
      "cnt: 0 - valLoss: 0.6287944912910461 - trainLoss: 0.6331751942634583\n",
      "cnt: 0 - valLoss: 0.6287897825241089 - trainLoss: 0.6331712603569031\n",
      "cnt: 0 - valLoss: 0.6287849545478821 - trainLoss: 0.6331676244735718\n",
      "cnt: 0 - valLoss: 0.6287806034088135 - trainLoss: 0.6331636309623718\n",
      "cnt: 0 - valLoss: 0.6287758946418762 - trainLoss: 0.6331599354743958\n",
      "cnt: 0 - valLoss: 0.6287710666656494 - trainLoss: 0.6331560015678406\n",
      "cnt: 0 - valLoss: 0.6287668943405151 - trainLoss: 0.6331519484519958\n",
      "cnt: 0 - valLoss: 0.6287620067596436 - trainLoss: 0.6331484913825989\n",
      "cnt: 0 - valLoss: 0.6287574172019958 - trainLoss: 0.6331443786621094\n",
      "cnt: 0 - valLoss: 0.6287529468536377 - trainLoss: 0.6331405639648438\n",
      "cnt: 0 - valLoss: 0.6287481188774109 - trainLoss: 0.6331368684768677\n",
      "cnt: 0 - valLoss: 0.6287438273429871 - trainLoss: 0.6331327557563782\n",
      "cnt: 0 - valLoss: 0.6287391185760498 - trainLoss: 0.6331291198730469\n",
      "cnt: 0 - valLoss: 0.6287343502044678 - trainLoss: 0.6331251859664917\n",
      "cnt: 0 - valLoss: 0.628730058670044 - trainLoss: 0.6331212520599365\n",
      "cnt: 0 - valLoss: 0.6287252306938171 - trainLoss: 0.6331176161766052\n",
      "cnt: 0 - valLoss: 0.628720760345459 - trainLoss: 0.6331135034561157\n",
      "cnt: 0 - valLoss: 0.6287161111831665 - trainLoss: 0.6331098079681396\n",
      "cnt: 0 - valLoss: 0.6287113428115845 - trainLoss: 0.633105993270874\n",
      "cnt: 0 - valLoss: 0.6287071108818054 - trainLoss: 0.6331018805503845\n",
      "cnt: 0 - valLoss: 0.6287022233009338 - trainLoss: 0.6330983638763428\n",
      "cnt: 0 - valLoss: 0.6286975741386414 - trainLoss: 0.6330943703651428\n",
      "cnt: 0 - valLoss: 0.6286932229995728 - trainLoss: 0.6330904364585876\n",
      "cnt: 0 - valLoss: 0.6286882758140564 - trainLoss: 0.6330868005752563\n",
      "cnt: 0 - valLoss: 0.6286839246749878 - trainLoss: 0.6330826878547668\n",
      "cnt: 0 - valLoss: 0.6286792755126953 - trainLoss: 0.6330790519714355\n",
      "cnt: 0 - valLoss: 0.6286744475364685 - trainLoss: 0.6330750584602356\n",
      "cnt: 0 - valLoss: 0.6286702752113342 - trainLoss: 0.6330710053443909\n",
      "cnt: 0 - valLoss: 0.6286653876304626 - trainLoss: 0.6330674886703491\n",
      "cnt: 0 - valLoss: 0.6286607980728149 - trainLoss: 0.6330634951591492\n",
      "cnt: 0 - valLoss: 0.6286563277244568 - trainLoss: 0.6330596208572388\n",
      "cnt: 0 - valLoss: 0.6286512017250061 - trainLoss: 0.6330558657646179\n",
      "cnt: 0 - valLoss: 0.6286472082138062 - trainLoss: 0.633051872253418\n",
      "cnt: 0 - valLoss: 0.6286421418190002 - trainLoss: 0.6330481767654419\n",
      "cnt: 0 - valLoss: 0.628637433052063 - trainLoss: 0.6330442428588867\n",
      "cnt: 0 - valLoss: 0.6286331415176392 - trainLoss: 0.633040189743042\n",
      "cnt: 0 - valLoss: 0.6286282539367676 - trainLoss: 0.6330366134643555\n",
      "cnt: 0 - valLoss: 0.6286237239837646 - trainLoss: 0.6330325603485107\n",
      "cnt: 0 - valLoss: 0.6286191940307617 - trainLoss: 0.6330288052558899\n",
      "cnt: 0 - valLoss: 0.6286143064498901 - trainLoss: 0.6330249905586243\n",
      "cnt: 0 - valLoss: 0.6286101341247559 - trainLoss: 0.6330208778381348\n",
      "cnt: 0 - valLoss: 0.6286052465438843 - trainLoss: 0.6330172419548035\n",
      "cnt: 0 - valLoss: 0.628600537776947 - trainLoss: 0.6330133080482483\n",
      "cnt: 0 - valLoss: 0.6285961866378784 - trainLoss: 0.6330093741416931\n",
      "cnt: 0 - valLoss: 0.6285912990570068 - trainLoss: 0.633005678653717\n",
      "cnt: 0 - valLoss: 0.6285869479179382 - trainLoss: 0.6330016255378723\n",
      "cnt: 0 - valLoss: 0.628582239151001 - trainLoss: 0.6329979300498962\n",
      "cnt: 0 - valLoss: 0.6285774111747742 - trainLoss: 0.6329939961433411\n",
      "cnt: 0 - valLoss: 0.6285735368728638 - trainLoss: 0.6329898834228516\n",
      "cnt: 0 - valLoss: 0.6285682916641235 - trainLoss: 0.6329864859580994\n",
      "cnt: 0 - valLoss: 0.6285635232925415 - trainLoss: 0.6329823136329651\n",
      "cnt: 0 - valLoss: 0.6285589933395386 - trainLoss: 0.6329784989356995\n",
      "cnt: 0 - valLoss: 0.628554105758667 - trainLoss: 0.6329747438430786\n",
      "cnt: 0 - valLoss: 0.6285498142242432 - trainLoss: 0.6329706311225891\n",
      "cnt: 0 - valLoss: 0.6285450458526611 - trainLoss: 0.6329669952392578\n",
      "cnt: 0 - valLoss: 0.6285406351089478 - trainLoss: 0.6329630613327026\n",
      "cnt: 0 - valLoss: 0.6285359859466553 - trainLoss: 0.6329591274261475\n",
      "cnt: 0 - valLoss: 0.6285310983657837 - trainLoss: 0.6329554319381714\n",
      "cnt: 0 - valLoss: 0.6285266876220703 - trainLoss: 0.6329513192176819\n",
      "cnt: 0 - valLoss: 0.6285220384597778 - trainLoss: 0.632947564125061\n",
      "cnt: 0 - valLoss: 0.6285171508789062 - trainLoss: 0.6329436898231506\n",
      "cnt: 0 - valLoss: 0.6285130381584167 - trainLoss: 0.6329395771026611\n",
      "cnt: 0 - valLoss: 0.6285080909729004 - trainLoss: 0.6329361200332642\n",
      "cnt: 0 - valLoss: 0.6285035014152527 - trainLoss: 0.6329320073127747\n",
      "cnt: 0 - valLoss: 0.6284989714622498 - trainLoss: 0.6329281330108643\n",
      "cnt: 0 - valLoss: 0.6284940838813782 - trainLoss: 0.6329244375228882\n",
      "cnt: 0 - valLoss: 0.6284895539283752 - trainLoss: 0.6329202651977539\n",
      "cnt: 0 - valLoss: 0.6284850239753723 - trainLoss: 0.6329166889190674\n",
      "cnt: 0 - valLoss: 0.628480076789856 - trainLoss: 0.6329126358032227\n",
      "cnt: 0 - valLoss: 0.6284756660461426 - trainLoss: 0.6329087018966675\n",
      "cnt: 0 - valLoss: 0.628470778465271 - trainLoss: 0.6329050660133362\n",
      "cnt: 0 - valLoss: 0.6284663677215576 - trainLoss: 0.6329009532928467\n",
      "cnt: 0 - valLoss: 0.6284617185592651 - trainLoss: 0.632897138595581\n",
      "cnt: 0 - valLoss: 0.6284568309783936 - trainLoss: 0.6328933238983154\n",
      "cnt: 0 - valLoss: 0.6284526586532593 - trainLoss: 0.6328892111778259\n",
      "cnt: 0 - valLoss: 0.6284477114677429 - trainLoss: 0.6328856945037842\n",
      "cnt: 0 - valLoss: 0.6284430623054504 - trainLoss: 0.6328815221786499\n",
      "cnt: 0 - valLoss: 0.6284386515617371 - trainLoss: 0.6328777074813843\n",
      "cnt: 0 - valLoss: 0.6284337043762207 - trainLoss: 0.6328739523887634\n",
      "cnt: 0 - valLoss: 0.6284293532371521 - trainLoss: 0.6328698396682739\n",
      "cnt: 0 - valLoss: 0.6284245848655701 - trainLoss: 0.6328661441802979\n",
      "cnt: 0 - valLoss: 0.628419816493988 - trainLoss: 0.6328622102737427\n",
      "cnt: 0 - valLoss: 0.6284155249595642 - trainLoss: 0.6328582167625427\n",
      "cnt: 0 - valLoss: 0.6284105777740479 - trainLoss: 0.6328546404838562\n",
      "cnt: 0 - valLoss: 0.6284058690071106 - trainLoss: 0.6328504681587219\n",
      "cnt: 0 - valLoss: 0.6284015774726868 - trainLoss: 0.6328467130661011\n",
      "cnt: 0 - valLoss: 0.6283963918685913 - trainLoss: 0.6328428983688354\n",
      "cnt: 0 - valLoss: 0.6283922791481018 - trainLoss: 0.632838785648346\n",
      "cnt: 0 - valLoss: 0.6283873915672302 - trainLoss: 0.6328352093696594\n",
      "cnt: 0 - valLoss: 0.628382682800293 - trainLoss: 0.6328310966491699\n",
      "cnt: 0 - valLoss: 0.6283782124519348 - trainLoss: 0.6328272223472595\n",
      "cnt: 0 - valLoss: 0.6283736824989319 - trainLoss: 0.6328235268592834\n",
      "cnt: 0 - valLoss: 0.6283690929412842 - trainLoss: 0.632819414138794\n",
      "cnt: 0 - valLoss: 0.6283643245697021 - trainLoss: 0.6328157782554626\n",
      "cnt: 0 - valLoss: 0.6283594965934753 - trainLoss: 0.6328118443489075\n",
      "cnt: 0 - valLoss: 0.6283552646636963 - trainLoss: 0.632807731628418\n",
      "cnt: 0 - valLoss: 0.6283503174781799 - trainLoss: 0.6328042149543762\n",
      "cnt: 0 - valLoss: 0.6283458471298218 - trainLoss: 0.6328000426292419\n",
      "cnt: 0 - valLoss: 0.6283412575721741 - trainLoss: 0.6327962875366211\n",
      "cnt: 0 - valLoss: 0.6283362507820129 - trainLoss: 0.6327924132347107\n",
      "cnt: 0 - valLoss: 0.6283320784568787 - trainLoss: 0.6327883005142212\n",
      "cnt: 0 - valLoss: 0.6283272504806519 - trainLoss: 0.6327847838401794\n",
      "cnt: 0 - valLoss: 0.6283225417137146 - trainLoss: 0.6327806711196899\n",
      "cnt: 0 - valLoss: 0.6283180713653564 - trainLoss: 0.6327767372131348\n",
      "cnt: 0 - valLoss: 0.6283131241798401 - trainLoss: 0.6327731013298035\n",
      "cnt: 0 - valLoss: 0.6283087730407715 - trainLoss: 0.6327689290046692\n",
      "cnt: 0 - valLoss: 0.6283044219017029 - trainLoss: 0.6327652335166931\n",
      "cnt: 0 - valLoss: 0.6282992959022522 - trainLoss: 0.6327613592147827\n",
      "cnt: 0 - valLoss: 0.6282947659492493 - trainLoss: 0.6327572464942932\n",
      "cnt: 0 - valLoss: 0.6282898187637329 - trainLoss: 0.6327536702156067\n",
      "cnt: 0 - valLoss: 0.62828528881073 - trainLoss: 0.6327494382858276\n",
      "cnt: 0 - valLoss: 0.6282806396484375 - trainLoss: 0.6327456831932068\n",
      "cnt: 0 - valLoss: 0.6282757520675659 - trainLoss: 0.6327418088912964\n",
      "cnt: 0 - valLoss: 0.6282715797424316 - trainLoss: 0.6327376961708069\n",
      "cnt: 0 - valLoss: 0.6282665729522705 - trainLoss: 0.6327340602874756\n",
      "cnt: 0 - valLoss: 0.628261923789978 - trainLoss: 0.6327300071716309\n",
      "cnt: 0 - valLoss: 0.6282573938369751 - trainLoss: 0.6327260732650757\n",
      "cnt: 0 - valLoss: 0.6282525062561035 - trainLoss: 0.6327222585678101\n",
      "cnt: 0 - valLoss: 0.6282479166984558 - trainLoss: 0.6327182054519653\n",
      "cnt: 0 - valLoss: 0.6282435059547424 - trainLoss: 0.632714569568634\n",
      "cnt: 0 - valLoss: 0.628238320350647 - trainLoss: 0.6327105760574341\n",
      "cnt: 0 - valLoss: 0.6282340288162231 - trainLoss: 0.6327065229415894\n",
      "cnt: 0 - valLoss: 0.6282290816307068 - trainLoss: 0.6327028870582581\n",
      "cnt: 0 - valLoss: 0.6282245516777039 - trainLoss: 0.6326987147331238\n",
      "cnt: 0 - valLoss: 0.6282199621200562 - trainLoss: 0.6326949000358582\n",
      "cnt: 0 - valLoss: 0.6282150149345398 - trainLoss: 0.6326910853385925\n",
      "cnt: 0 - valLoss: 0.6282107830047607 - trainLoss: 0.6326869130134583\n",
      "cnt: 0 - valLoss: 0.6282058954238892 - trainLoss: 0.6326833963394165\n",
      "cnt: 0 - valLoss: 0.6282011866569519 - trainLoss: 0.632679283618927\n",
      "cnt: 0 - valLoss: 0.6281967163085938 - trainLoss: 0.6326753497123718\n",
      "cnt: 0 - valLoss: 0.6281917691230774 - trainLoss: 0.632671594619751\n",
      "cnt: 0 - valLoss: 0.6281871795654297 - trainLoss: 0.6326674222946167\n",
      "cnt: 0 - valLoss: 0.6281827092170715 - trainLoss: 0.6326638460159302\n",
      "cnt: 0 - valLoss: 0.6281775832176208 - trainLoss: 0.6326598525047302\n",
      "cnt: 0 - valLoss: 0.628173291683197 - trainLoss: 0.6326557397842407\n",
      "cnt: 0 - valLoss: 0.6281683444976807 - trainLoss: 0.6326521635055542\n",
      "cnt: 0 - valLoss: 0.6281638145446777 - trainLoss: 0.6326479911804199\n",
      "cnt: 0 - valLoss: 0.6281591653823853 - trainLoss: 0.6326441764831543\n",
      "cnt: 0 - valLoss: 0.6281542181968689 - trainLoss: 0.6326403617858887\n",
      "cnt: 0 - valLoss: 0.6281500458717346 - trainLoss: 0.6326361298561096\n",
      "cnt: 0 - valLoss: 0.6281450390815735 - trainLoss: 0.6326326131820679\n",
      "cnt: 0 - valLoss: 0.6281404495239258 - trainLoss: 0.6326284408569336\n",
      "cnt: 0 - valLoss: 0.6281359195709229 - trainLoss: 0.632624626159668\n",
      "cnt: 0 - valLoss: 0.6281309127807617 - trainLoss: 0.6326208114624023\n",
      "cnt: 0 - valLoss: 0.6281263828277588 - trainLoss: 0.6326166987419128\n",
      "cnt: 0 - valLoss: 0.6281218528747559 - trainLoss: 0.6326130628585815\n",
      "cnt: 0 - valLoss: 0.6281168460845947 - trainLoss: 0.6326090097427368\n",
      "cnt: 0 - valLoss: 0.6281124353408813 - trainLoss: 0.6326049566268921\n",
      "cnt: 0 - valLoss: 0.628107488155365 - trainLoss: 0.6326013207435608\n",
      "cnt: 0 - valLoss: 0.6281029582023621 - trainLoss: 0.6325971484184265\n",
      "cnt: 0 - valLoss: 0.6280983090400696 - trainLoss: 0.6325934529304504\n",
      "cnt: 0 - valLoss: 0.6280933618545532 - trainLoss: 0.6325894594192505\n",
      "cnt: 0 - valLoss: 0.6280891299247742 - trainLoss: 0.6325854063034058\n",
      "cnt: 0 - valLoss: 0.6280844807624817 - trainLoss: 0.6325817704200745\n",
      "cnt: 0 - valLoss: 0.6280796527862549 - trainLoss: 0.6325777173042297\n",
      "cnt: 0 - valLoss: 0.6280747652053833 - trainLoss: 0.6325738430023193\n",
      "cnt: 0 - valLoss: 0.6280697584152222 - trainLoss: 0.6325699687004089\n",
      "cnt: 0 - valLoss: 0.6280654668807983 - trainLoss: 0.6325658559799194\n",
      "cnt: 0 - valLoss: 0.6280605792999268 - trainLoss: 0.6325622200965881\n",
      "cnt: 0 - valLoss: 0.6280562877655029 - trainLoss: 0.6325581073760986\n",
      "cnt: 0 - valLoss: 0.6280515193939209 - trainLoss: 0.6325542330741882\n",
      "cnt: 0 - valLoss: 0.6280465722084045 - trainLoss: 0.6325504779815674\n",
      "cnt: 0 - valLoss: 0.6280421614646912 - trainLoss: 0.6325463652610779\n",
      "cnt: 0 - valLoss: 0.6280373334884644 - trainLoss: 0.632542610168457\n",
      "cnt: 0 - valLoss: 0.6280325055122375 - trainLoss: 0.6325386762619019\n",
      "cnt: 0 - valLoss: 0.628028154373169 - trainLoss: 0.6325345635414124\n",
      "cnt: 0 - valLoss: 0.6280232071876526 - trainLoss: 0.632530927658081\n",
      "cnt: 0 - valLoss: 0.6280186772346497 - trainLoss: 0.6325268149375916\n",
      "cnt: 0 - valLoss: 0.6280140280723572 - trainLoss: 0.6325230598449707\n",
      "cnt: 0 - valLoss: 0.628009021282196 - trainLoss: 0.6325191259384155\n",
      "cnt: 0 - valLoss: 0.628004789352417 - trainLoss: 0.632515013217926\n",
      "cnt: 0 - valLoss: 0.6279997825622559 - trainLoss: 0.6325114369392395\n",
      "cnt: 0 - valLoss: 0.6279950737953186 - trainLoss: 0.63250732421875\n",
      "cnt: 0 - valLoss: 0.6279906630516052 - trainLoss: 0.6325033903121948\n",
      "cnt: 0 - valLoss: 0.6279855966567993 - trainLoss: 0.632499635219574\n",
      "cnt: 0 - valLoss: 0.6279817223548889 - trainLoss: 0.6324954628944397\n",
      "cnt: 0 - valLoss: 0.6279764771461487 - trainLoss: 0.6324918866157532\n",
      "cnt: 0 - valLoss: 0.6279714107513428 - trainLoss: 0.6324878334999084\n",
      "cnt: 0 - valLoss: 0.6279670000076294 - trainLoss: 0.632483720779419\n",
      "cnt: 0 - valLoss: 0.6279624104499817 - trainLoss: 0.6324800848960876\n",
      "cnt: 0 - valLoss: 0.6279576420783997 - trainLoss: 0.6324759721755981\n",
      "cnt: 0 - valLoss: 0.6279529333114624 - trainLoss: 0.6324721574783325\n",
      "cnt: 0 - valLoss: 0.6279479265213013 - trainLoss: 0.6324682235717773\n",
      "cnt: 0 - valLoss: 0.6279436945915222 - trainLoss: 0.6324641704559326\n",
      "cnt: 0 - valLoss: 0.6279386878013611 - trainLoss: 0.6324605345726013\n",
      "cnt: 0 - valLoss: 0.6279341578483582 - trainLoss: 0.632456362247467\n",
      "cnt: 0 - valLoss: 0.6279295086860657 - trainLoss: 0.6324525475502014\n",
      "cnt: 0 - valLoss: 0.6279245615005493 - trainLoss: 0.632448673248291\n",
      "cnt: 0 - valLoss: 0.6279202699661255 - trainLoss: 0.6324445605278015\n",
      "cnt: 0 - valLoss: 0.6279153823852539 - trainLoss: 0.6324409246444702\n",
      "cnt: 0 - valLoss: 0.6279106140136719 - trainLoss: 0.6324368119239807\n",
      "cnt: 0 - valLoss: 0.6279062032699585 - trainLoss: 0.6324328780174255\n",
      "cnt: 0 - valLoss: 0.6279011964797974 - trainLoss: 0.6324291229248047\n",
      "cnt: 0 - valLoss: 0.627896785736084 - trainLoss: 0.6324249505996704\n",
      "cnt: 0 - valLoss: 0.627892017364502 - trainLoss: 0.6324212551116943\n",
      "cnt: 0 - valLoss: 0.6278871893882751 - trainLoss: 0.6324172616004944\n",
      "cnt: 0 - valLoss: 0.6278831958770752 - trainLoss: 0.6324131488800049\n",
      "cnt: 0 - valLoss: 0.627877950668335 - trainLoss: 0.6324095726013184\n",
      "cnt: 0 - valLoss: 0.6278731822967529 - trainLoss: 0.6324054002761841\n",
      "cnt: 0 - valLoss: 0.6278685331344604 - trainLoss: 0.6324015259742737\n",
      "cnt: 0 - valLoss: 0.6278635859489441 - trainLoss: 0.6323975920677185\n",
      "cnt: 0 - valLoss: 0.627859354019165 - trainLoss: 0.6323933601379395\n",
      "cnt: 0 - valLoss: 0.6278541684150696 - trainLoss: 0.6323897242546082\n",
      "cnt: 0 - valLoss: 0.6278495192527771 - trainLoss: 0.6323854923248291\n",
      "cnt: 0 - valLoss: 0.6278449892997742 - trainLoss: 0.6323816180229187\n",
      "cnt: 0 - valLoss: 0.6278400421142578 - trainLoss: 0.6323777437210083\n",
      "cnt: 0 - valLoss: 0.627835750579834 - trainLoss: 0.6323735117912292\n",
      "cnt: 0 - valLoss: 0.6278306245803833 - trainLoss: 0.6323698163032532\n",
      "cnt: 0 - valLoss: 0.6278263330459595 - trainLoss: 0.6323656439781189\n",
      "cnt: 0 - valLoss: 0.6278215050697327 - trainLoss: 0.6323617100715637\n",
      "cnt: 0 - valLoss: 0.6278165578842163 - trainLoss: 0.6323578357696533\n",
      "cnt: 0 - valLoss: 0.6278119087219238 - trainLoss: 0.632353663444519\n",
      "cnt: 0 - valLoss: 0.6278071403503418 - trainLoss: 0.632349967956543\n",
      "cnt: 0 - valLoss: 0.6278023719787598 - trainLoss: 0.6323458552360535\n",
      "cnt: 0 - valLoss: 0.6277980208396912 - trainLoss: 0.6323416829109192\n",
      "cnt: 0 - valLoss: 0.6277930736541748 - trainLoss: 0.6323379874229431\n",
      "cnt: 0 - valLoss: 0.627788245677948 - trainLoss: 0.6323338150978088\n",
      "cnt: 0 - valLoss: 0.6277835965156555 - trainLoss: 0.6323299407958984\n",
      "cnt: 0 - valLoss: 0.6277786493301392 - trainLoss: 0.6323259472846985\n",
      "cnt: 0 - valLoss: 0.6277744174003601 - trainLoss: 0.632321834564209\n",
      "cnt: 0 - valLoss: 0.6277695298194885 - trainLoss: 0.6323181390762329\n",
      "cnt: 0 - valLoss: 0.6277645826339722 - trainLoss: 0.6323139071464539\n",
      "cnt: 0 - valLoss: 0.6277600526809692 - trainLoss: 0.6323100328445435\n",
      "cnt: 0 - valLoss: 0.6277550458908081 - trainLoss: 0.6323060989379883\n",
      "cnt: 0 - valLoss: 0.627750813961029 - trainLoss: 0.6323018670082092\n",
      "cnt: 0 - valLoss: 0.6277458667755127 - trainLoss: 0.6322982311248779\n",
      "cnt: 0 - valLoss: 0.6277409195899963 - trainLoss: 0.6322940587997437\n",
      "cnt: 0 - valLoss: 0.6277364492416382 - trainLoss: 0.6322900652885437\n",
      "cnt: 0 - valLoss: 0.6277315616607666 - trainLoss: 0.6322861909866333\n",
      "cnt: 0 - valLoss: 0.6277271509170532 - trainLoss: 0.632282018661499\n",
      "cnt: 0 - valLoss: 0.6277223229408264 - trainLoss: 0.6322782635688782\n",
      "cnt: 0 - valLoss: 0.6277172565460205 - trainLoss: 0.6322742104530334\n",
      "cnt: 0 - valLoss: 0.6277128458023071 - trainLoss: 0.632270097732544\n",
      "cnt: 0 - valLoss: 0.6277078986167908 - trainLoss: 0.6322663426399231\n",
      "cnt: 0 - valLoss: 0.6277034282684326 - trainLoss: 0.632262110710144\n",
      "cnt: 0 - valLoss: 0.6276987195014954 - trainLoss: 0.6322582960128784\n",
      "cnt: 0 - valLoss: 0.6276935338973999 - trainLoss: 0.6322542428970337\n",
      "cnt: 0 - valLoss: 0.6276892423629761 - trainLoss: 0.6322500705718994\n",
      "cnt: 0 - valLoss: 0.6276842951774597 - trainLoss: 0.6322464346885681\n",
      "cnt: 0 - valLoss: 0.627679705619812 - trainLoss: 0.6322422623634338\n",
      "cnt: 0 - valLoss: 0.6276751160621643 - trainLoss: 0.6322383284568787\n",
      "cnt: 0 - valLoss: 0.6276700496673584 - trainLoss: 0.6322343945503235\n",
      "cnt: 0 - valLoss: 0.6276659369468689 - trainLoss: 0.6322302222251892\n",
      "cnt: 0 - valLoss: 0.6276606917381287 - trainLoss: 0.6322265267372131\n",
      "cnt: 0 - valLoss: 0.6276560425758362 - trainLoss: 0.6322222948074341\n",
      "cnt: 0 - valLoss: 0.6276515126228333 - trainLoss: 0.6322182416915894\n",
      "cnt: 0 - valLoss: 0.627646267414093 - trainLoss: 0.6322143077850342\n",
      "cnt: 0 - valLoss: 0.6276419758796692 - trainLoss: 0.6322101354598999\n",
      "cnt: 0 - valLoss: 0.6276372075080872 - trainLoss: 0.6322063207626343\n",
      "cnt: 0 - valLoss: 0.6276321411132812 - trainLoss: 0.6322021484375\n",
      "cnt: 0 - valLoss: 0.6276277899742126 - trainLoss: 0.6321980953216553\n",
      "cnt: 0 - valLoss: 0.6276227831840515 - trainLoss: 0.6321942210197449\n",
      "cnt: 0 - valLoss: 0.6276180744171143 - trainLoss: 0.632189929485321\n",
      "cnt: 0 - valLoss: 0.6276134252548218 - trainLoss: 0.6321861147880554\n",
      "cnt: 0 - valLoss: 0.6276085376739502 - trainLoss: 0.6321820020675659\n",
      "cnt: 0 - valLoss: 0.6276039481163025 - trainLoss: 0.6321778297424316\n",
      "cnt: 0 - valLoss: 0.6275989413261414 - trainLoss: 0.6321740746498108\n",
      "cnt: 0 - valLoss: 0.627594530582428 - trainLoss: 0.632169783115387\n",
      "cnt: 0 - valLoss: 0.6275895833969116 - trainLoss: 0.6321659088134766\n",
      "cnt: 0 - valLoss: 0.6275846362113953 - trainLoss: 0.6321619153022766\n",
      "cnt: 0 - valLoss: 0.6275804042816162 - trainLoss: 0.632157564163208\n",
      "cnt: 0 - valLoss: 0.627575159072876 - trainLoss: 0.6321539282798767\n",
      "cnt: 0 - valLoss: 0.6275705695152283 - trainLoss: 0.6321496367454529\n",
      "cnt: 0 - valLoss: 0.6275656819343567 - trainLoss: 0.6321456432342529\n",
      "cnt: 0 - valLoss: 0.6275607943534851 - trainLoss: 0.632141649723053\n",
      "cnt: 0 - valLoss: 0.627556562423706 - trainLoss: 0.6321374177932739\n",
      "cnt: 0 - valLoss: 0.6275513172149658 - trainLoss: 0.6321337223052979\n",
      "cnt: 0 - valLoss: 0.6275466680526733 - trainLoss: 0.632129430770874\n",
      "cnt: 0 - valLoss: 0.6275421977043152 - trainLoss: 0.6321253776550293\n",
      "cnt: 0 - valLoss: 0.6275368928909302 - trainLoss: 0.6321215033531189\n",
      "cnt: 0 - valLoss: 0.6275326013565063 - trainLoss: 0.6321172118186951\n",
      "cnt: 0 - valLoss: 0.6275277137756348 - trainLoss: 0.6321134567260742\n",
      "cnt: 0 - valLoss: 0.6275227069854736 - trainLoss: 0.6321092844009399\n",
      "cnt: 0 - valLoss: 0.6275182962417603 - trainLoss: 0.6321051716804504\n",
      "cnt: 0 - valLoss: 0.6275133490562439 - trainLoss: 0.63210129737854\n",
      "cnt: 0 - valLoss: 0.6275086402893066 - trainLoss: 0.632097065448761\n",
      "cnt: 0 - valLoss: 0.6275038719177246 - trainLoss: 0.6320931911468506\n",
      "cnt: 0 - valLoss: 0.6274991035461426 - trainLoss: 0.6320890784263611\n",
      "cnt: 0 - valLoss: 0.6274944543838501 - trainLoss: 0.632084846496582\n",
      "cnt: 0 - valLoss: 0.6274895071983337 - trainLoss: 0.6320810914039612\n",
      "cnt: 0 - valLoss: 0.6274849772453308 - trainLoss: 0.6320767998695374\n",
      "cnt: 0 - valLoss: 0.6274799704551697 - trainLoss: 0.632072925567627\n",
      "cnt: 0 - valLoss: 0.6274750828742981 - trainLoss: 0.6320688724517822\n",
      "cnt: 0 - valLoss: 0.627470850944519 - trainLoss: 0.6320646405220032\n",
      "cnt: 0 - valLoss: 0.6274656057357788 - trainLoss: 0.6320608854293823\n",
      "cnt: 0 - valLoss: 0.6274610161781311 - trainLoss: 0.6320565938949585\n",
      "cnt: 0 - valLoss: 0.6274564266204834 - trainLoss: 0.6320526003837585\n",
      "cnt: 0 - valLoss: 0.6274511218070984 - trainLoss: 0.6320486664772034\n",
      "cnt: 0 - valLoss: 0.6274468898773193 - trainLoss: 0.6320443153381348\n",
      "cnt: 0 - valLoss: 0.6274416446685791 - trainLoss: 0.6320406198501587\n",
      "cnt: 0 - valLoss: 0.6274370551109314 - trainLoss: 0.6320363879203796\n",
      "cnt: 0 - valLoss: 0.6274324655532837 - trainLoss: 0.6320323348045349\n",
      "cnt: 0 - valLoss: 0.6274272203445435 - trainLoss: 0.6320284008979797\n",
      "cnt: 0 - valLoss: 0.6274229288101196 - trainLoss: 0.6320241093635559\n",
      "cnt: 0 - valLoss: 0.6274179816246033 - trainLoss: 0.6320203542709351\n",
      "cnt: 0 - valLoss: 0.6274130344390869 - trainLoss: 0.6320160627365112\n",
      "cnt: 0 - valLoss: 0.6274085640907288 - trainLoss: 0.6320120096206665\n",
      "cnt: 0 - valLoss: 0.6274036169052124 - trainLoss: 0.6320080757141113\n",
      "cnt: 0 - valLoss: 0.6273989081382751 - trainLoss: 0.6320038437843323\n",
      "cnt: 0 - valLoss: 0.6273940801620483 - trainLoss: 0.6320000290870667\n",
      "cnt: 0 - valLoss: 0.6273893713951111 - trainLoss: 0.6319958567619324\n",
      "cnt: 0 - valLoss: 0.6273846626281738 - trainLoss: 0.6319916248321533\n",
      "cnt: 0 - valLoss: 0.6273796558380127 - trainLoss: 0.6319878697395325\n",
      "cnt: 0 - valLoss: 0.6273752450942993 - trainLoss: 0.6319835782051086\n",
      "cnt: 0 - valLoss: 0.6273702383041382 - trainLoss: 0.6319796442985535\n",
      "cnt: 0 - valLoss: 0.6273652911186218 - trainLoss: 0.6319755911827087\n",
      "cnt: 0 - valLoss: 0.627360999584198 - trainLoss: 0.6319712996482849\n",
      "cnt: 0 - valLoss: 0.6273557543754578 - trainLoss: 0.6319676041603088\n",
      "cnt: 0 - valLoss: 0.6273512840270996 - trainLoss: 0.631963312625885\n",
      "cnt: 0 - valLoss: 0.6273462772369385 - trainLoss: 0.6319593787193298\n",
      "cnt: 0 - valLoss: 0.6273413896560669 - trainLoss: 0.6319553256034851\n",
      "cnt: 0 - valLoss: 0.6273372769355774 - trainLoss: 0.6319510340690613\n",
      "cnt: 0 - valLoss: 0.6273319721221924 - trainLoss: 0.6319472789764404\n",
      "cnt: 0 - valLoss: 0.6273275017738342 - trainLoss: 0.6319429874420166\n",
      "cnt: 0 - valLoss: 0.6273229122161865 - trainLoss: 0.6319389939308167\n",
      "cnt: 0 - valLoss: 0.6273176074028015 - trainLoss: 0.6319350004196167\n",
      "cnt: 0 - valLoss: 0.6273134350776672 - trainLoss: 0.6319307684898376\n",
      "cnt: 0 - valLoss: 0.6273084878921509 - trainLoss: 0.6319270730018616\n",
      "cnt: 0 - valLoss: 0.6273038983345032 - trainLoss: 0.6319228410720825\n",
      "cnt: 0 - valLoss: 0.6272991299629211 - trainLoss: 0.6319187879562378\n",
      "cnt: 0 - valLoss: 0.62729412317276 - trainLoss: 0.6319148540496826\n",
      "cnt: 0 - valLoss: 0.6272898316383362 - trainLoss: 0.6319105625152588\n",
      "cnt: 0 - valLoss: 0.6272847056388855 - trainLoss: 0.6319067478179932\n",
      "cnt: 0 - valLoss: 0.6272799968719482 - trainLoss: 0.6319026350975037\n",
      "cnt: 0 - valLoss: 0.6272755265235901 - trainLoss: 0.6318984627723694\n",
      "cnt: 0 - valLoss: 0.6272705793380737 - trainLoss: 0.6318946480751038\n",
      "cnt: 0 - valLoss: 0.6272659301757812 - trainLoss: 0.6318903565406799\n",
      "cnt: 0 - valLoss: 0.6272611618041992 - trainLoss: 0.6318865418434143\n",
      "cnt: 0 - valLoss: 0.6272563934326172 - trainLoss: 0.6318824291229248\n",
      "cnt: 0 - valLoss: 0.6272516846656799 - trainLoss: 0.6318782567977905\n",
      "cnt: 0 - valLoss: 0.627246618270874 - trainLoss: 0.6318744421005249\n",
      "cnt: 0 - valLoss: 0.6272423267364502 - trainLoss: 0.6318700909614563\n",
      "cnt: 0 - valLoss: 0.6272376179695129 - trainLoss: 0.6318663358688354\n",
      "cnt: 0 - valLoss: 0.6272324323654175 - trainLoss: 0.631862223148346\n",
      "cnt: 0 - valLoss: 0.6272281408309937 - trainLoss: 0.6318579912185669\n",
      "cnt: 0 - valLoss: 0.6272232532501221 - trainLoss: 0.6318541765213013\n",
      "cnt: 0 - valLoss: 0.6272184252738953 - trainLoss: 0.6318499445915222\n",
      "cnt: 0 - valLoss: 0.627213716506958 - trainLoss: 0.631846010684967\n",
      "cnt: 0 - valLoss: 0.6272088289260864 - trainLoss: 0.6318419575691223\n",
      "cnt: 0 - valLoss: 0.6272042393684387 - trainLoss: 0.6318376660346985\n",
      "cnt: 0 - valLoss: 0.6271993517875671 - trainLoss: 0.6318339705467224\n",
      "cnt: 0 - valLoss: 0.627194881439209 - trainLoss: 0.6318296194076538\n",
      "cnt: 0 - valLoss: 0.6271898746490479 - trainLoss: 0.6318256258964539\n",
      "cnt: 0 - valLoss: 0.6271849870681763 - trainLoss: 0.6318215727806091\n",
      "cnt: 0 - valLoss: 0.6271807551383972 - trainLoss: 0.6318172812461853\n",
      "cnt: 0 - valLoss: 0.627175509929657 - trainLoss: 0.6318135857582092\n",
      "cnt: 0 - valLoss: 0.627170979976654 - trainLoss: 0.6318092942237854\n",
      "cnt: 0 - valLoss: 0.6271663904190063 - trainLoss: 0.6318051815032959\n",
      "cnt: 0 - valLoss: 0.6271612048149109 - trainLoss: 0.6318012475967407\n",
      "cnt: 0 - valLoss: 0.6271570324897766 - trainLoss: 0.6317968368530273\n",
      "cnt: 0 - valLoss: 0.6271520256996155 - trainLoss: 0.631793200969696\n",
      "cnt: 0 - valLoss: 0.6271473169326782 - trainLoss: 0.6317889094352722\n",
      "cnt: 0 - valLoss: 0.6271428465843201 - trainLoss: 0.6317848563194275\n",
      "cnt: 0 - valLoss: 0.6271376609802246 - trainLoss: 0.6317808628082275\n",
      "cnt: 0 - valLoss: 0.6271334290504456 - trainLoss: 0.6317765116691589\n",
      "cnt: 0 - valLoss: 0.6271284222602844 - trainLoss: 0.6317728161811829\n",
      "cnt: 0 - valLoss: 0.6271237134933472 - trainLoss: 0.631768524646759\n",
      "cnt: 0 - valLoss: 0.6271190643310547 - trainLoss: 0.6317644715309143\n",
      "cnt: 0 - valLoss: 0.6271140575408936 - trainLoss: 0.6317605376243591\n",
      "cnt: 0 - valLoss: 0.6271096467971802 - trainLoss: 0.6317561864852905\n",
      "cnt: 0 - valLoss: 0.6271047592163086 - trainLoss: 0.6317524909973145\n",
      "cnt: 0 - valLoss: 0.6271000504493713 - trainLoss: 0.6317482590675354\n",
      "cnt: 0 - valLoss: 0.6270954608917236 - trainLoss: 0.6317441463470459\n",
      "cnt: 0 - valLoss: 0.6270905137062073 - trainLoss: 0.6317402720451355\n",
      "cnt: 0 - valLoss: 0.6270859837532043 - trainLoss: 0.6317359805107117\n",
      "cnt: 0 - valLoss: 0.6270811557769775 - trainLoss: 0.631732165813446\n",
      "cnt: 0 - valLoss: 0.6270763874053955 - trainLoss: 0.6317279934883118\n",
      "cnt: 0 - valLoss: 0.6270718574523926 - trainLoss: 0.6317238211631775\n",
      "cnt: 0 - valLoss: 0.6270668506622314 - trainLoss: 0.6317199468612671\n",
      "cnt: 0 - valLoss: 0.6270623207092285 - trainLoss: 0.6317156553268433\n",
      "cnt: 0 - valLoss: 0.6270575523376465 - trainLoss: 0.6317118406295776\n",
      "cnt: 0 - valLoss: 0.6270527243614197 - trainLoss: 0.6317077279090881\n",
      "cnt: 0 - valLoss: 0.6270482540130615 - trainLoss: 0.6317034959793091\n",
      "cnt: 0 - valLoss: 0.6270432472229004 - trainLoss: 0.6316996812820435\n",
      "cnt: 0 - valLoss: 0.6270386576652527 - trainLoss: 0.6316953897476196\n",
      "cnt: 0 - valLoss: 0.6270339488983154 - trainLoss: 0.6316914558410645\n",
      "cnt: 0 - valLoss: 0.6270290017127991 - trainLoss: 0.6316874027252197\n",
      "cnt: 0 - valLoss: 0.6270245313644409 - trainLoss: 0.6316831707954407\n",
      "cnt: 0 - valLoss: 0.6270195841789246 - trainLoss: 0.6316794157028198\n",
      "cnt: 0 - valLoss: 0.6270149350166321 - trainLoss: 0.6316750645637512\n",
      "cnt: 0 - valLoss: 0.62701016664505 - trainLoss: 0.631671130657196\n",
      "cnt: 0 - valLoss: 0.6270052194595337 - trainLoss: 0.6316670179367065\n",
      "cnt: 0 - valLoss: 0.6270008683204651 - trainLoss: 0.6316627860069275\n",
      "cnt: 0 - valLoss: 0.626995861530304 - trainLoss: 0.6316590905189514\n",
      "cnt: 0 - valLoss: 0.6269912123680115 - trainLoss: 0.6316547393798828\n",
      "cnt: 0 - valLoss: 0.626986563205719 - trainLoss: 0.6316508054733276\n",
      "cnt: 0 - valLoss: 0.6269814968109131 - trainLoss: 0.6316466927528381\n",
      "cnt: 0 - valLoss: 0.6269772052764893 - trainLoss: 0.6316424608230591\n",
      "cnt: 0 - valLoss: 0.6269721388816833 - trainLoss: 0.6316387057304382\n",
      "cnt: 0 - valLoss: 0.6269675493240356 - trainLoss: 0.6316344141960144\n",
      "cnt: 0 - valLoss: 0.6269628405570984 - trainLoss: 0.6316304206848145\n",
      "cnt: 0 - valLoss: 0.6269577741622925 - trainLoss: 0.6316263675689697\n",
      "cnt: 0 - valLoss: 0.6269535422325134 - trainLoss: 0.6316220760345459\n",
      "cnt: 0 - valLoss: 0.6269484758377075 - trainLoss: 0.631618320941925\n",
      "cnt: 0 - valLoss: 0.6269437670707703 - trainLoss: 0.6316140294075012\n",
      "cnt: 0 - valLoss: 0.6269391179084778 - trainLoss: 0.631610095500946\n",
      "cnt: 0 - valLoss: 0.6269339919090271 - trainLoss: 0.6316061019897461\n",
      "cnt: 0 - valLoss: 0.6269297003746033 - trainLoss: 0.6316017508506775\n",
      "cnt: 0 - valLoss: 0.6269246935844421 - trainLoss: 0.6315981149673462\n",
      "cnt: 0 - valLoss: 0.6269199252128601 - trainLoss: 0.6315937638282776\n",
      "cnt: 0 - valLoss: 0.6269152760505676 - trainLoss: 0.6315897703170776\n",
      "cnt: 0 - valLoss: 0.6269102692604065 - trainLoss: 0.6315857768058777\n",
      "cnt: 0 - valLoss: 0.6269059181213379 - trainLoss: 0.6315814256668091\n",
      "cnt: 0 - valLoss: 0.626900851726532 - trainLoss: 0.631577730178833\n",
      "cnt: 0 - valLoss: 0.6268961429595947 - trainLoss: 0.631573498249054\n",
      "cnt: 0 - valLoss: 0.6268914341926575 - trainLoss: 0.6315693855285645\n",
      "cnt: 0 - valLoss: 0.6268864274024963 - trainLoss: 0.631565511226654\n",
      "cnt: 0 - valLoss: 0.6268820762634277 - trainLoss: 0.6315611600875854\n",
      "cnt: 0 - valLoss: 0.6268770694732666 - trainLoss: 0.6315574049949646\n",
      "cnt: 0 - valLoss: 0.6268723011016846 - trainLoss: 0.6315531730651855\n",
      "cnt: 0 - valLoss: 0.6268676519393921 - trainLoss: 0.631549060344696\n",
      "cnt: 0 - valLoss: 0.6268625855445862 - trainLoss: 0.6315451264381409\n",
      "cnt: 0 - valLoss: 0.6268581748008728 - trainLoss: 0.631540834903717\n",
      "cnt: 0 - valLoss: 0.6268532276153564 - trainLoss: 0.6315370798110962\n",
      "cnt: 0 - valLoss: 0.6268483996391296 - trainLoss: 0.6315327882766724\n",
      "cnt: 0 - valLoss: 0.6268438100814819 - trainLoss: 0.6315287351608276\n",
      "cnt: 0 - valLoss: 0.626838743686676 - trainLoss: 0.6315248012542725\n",
      "cnt: 0 - valLoss: 0.6268343329429626 - trainLoss: 0.6315205097198486\n",
      "cnt: 0 - valLoss: 0.6268293857574463 - trainLoss: 0.631516695022583\n",
      "cnt: 0 - valLoss: 0.6268245577812195 - trainLoss: 0.6315125226974487\n",
      "cnt: 0 - valLoss: 0.6268199682235718 - trainLoss: 0.6315083503723145\n",
      "cnt: 0 - valLoss: 0.6268149018287659 - trainLoss: 0.631504476070404\n",
      "cnt: 0 - valLoss: 0.6268104910850525 - trainLoss: 0.6315001845359802\n",
      "cnt: 0 - valLoss: 0.6268054842948914 - trainLoss: 0.6314963102340698\n",
      "cnt: 0 - valLoss: 0.6268005967140198 - trainLoss: 0.6314921379089355\n",
      "cnt: 0 - valLoss: 0.6267961263656616 - trainLoss: 0.6314879655838013\n",
      "cnt: 0 - valLoss: 0.6267910599708557 - trainLoss: 0.6314840912818909\n",
      "cnt: 0 - valLoss: 0.6267865896224976 - trainLoss: 0.631479799747467\n",
      "cnt: 0 - valLoss: 0.626781702041626 - trainLoss: 0.6314759254455566\n",
      "cnt: 0 - valLoss: 0.6267768144607544 - trainLoss: 0.6314718127250671\n",
      "cnt: 0 - valLoss: 0.626772403717041 - trainLoss: 0.6314676403999329\n",
      "cnt: 0 - valLoss: 0.6267672181129456 - trainLoss: 0.6314638257026672\n",
      "cnt: 0 - valLoss: 0.6267627477645874 - trainLoss: 0.6314594745635986\n",
      "cnt: 0 - valLoss: 0.6267578601837158 - trainLoss: 0.6314556002616882\n",
      "cnt: 0 - valLoss: 0.626753032207489 - trainLoss: 0.6314514875411987\n",
      "cnt: 0 - valLoss: 0.6267485022544861 - trainLoss: 0.6314472556114197\n",
      "cnt: 0 - valLoss: 0.6267434358596802 - trainLoss: 0.6314435005187988\n",
      "cnt: 0 - valLoss: 0.626738965511322 - trainLoss: 0.6314391493797302\n",
      "cnt: 0 - valLoss: 0.6267340779304504 - trainLoss: 0.6314352750778198\n",
      "cnt: 0 - valLoss: 0.6267291903495789 - trainLoss: 0.6314311027526855\n",
      "cnt: 0 - valLoss: 0.626724898815155 - trainLoss: 0.6314269304275513\n",
      "cnt: 0 - valLoss: 0.6267197728157043 - trainLoss: 0.6314231157302856\n",
      "cnt: 0 - valLoss: 0.626715362071991 - trainLoss: 0.631418764591217\n",
      "cnt: 0 - valLoss: 0.6267104148864746 - trainLoss: 0.6314149498939514\n",
      "cnt: 0 - valLoss: 0.626705527305603 - trainLoss: 0.6314108371734619\n",
      "cnt: 0 - valLoss: 0.6267009973526001 - trainLoss: 0.6314065456390381\n",
      "cnt: 0 - valLoss: 0.626695990562439 - trainLoss: 0.6314027309417725\n",
      "cnt: 0 - valLoss: 0.6266915202140808 - trainLoss: 0.6313984394073486\n",
      "cnt: 0 - valLoss: 0.6266866326332092 - trainLoss: 0.6313945651054382\n",
      "cnt: 0 - valLoss: 0.6266816258430481 - trainLoss: 0.631390392780304\n",
      "cnt: 0 - valLoss: 0.6266772150993347 - trainLoss: 0.6313861608505249\n",
      "cnt: 0 - valLoss: 0.6266721487045288 - trainLoss: 0.6313823461532593\n",
      "cnt: 0 - valLoss: 0.6266675591468811 - trainLoss: 0.6313781142234802\n",
      "cnt: 0 - valLoss: 0.6266627907752991 - trainLoss: 0.631374180316925\n",
      "cnt: 0 - valLoss: 0.6266577839851379 - trainLoss: 0.6313700675964355\n",
      "cnt: 0 - valLoss: 0.6266533732414246 - trainLoss: 0.6313657760620117\n",
      "cnt: 0 - valLoss: 0.6266483068466187 - trainLoss: 0.6313619613647461\n",
      "cnt: 0 - valLoss: 0.6266437768936157 - trainLoss: 0.6313576102256775\n",
      "cnt: 0 - valLoss: 0.6266388893127441 - trainLoss: 0.6313537359237671\n",
      "cnt: 0 - valLoss: 0.6266339421272278 - trainLoss: 0.6313496232032776\n",
      "cnt: 0 - valLoss: 0.6266294121742249 - trainLoss: 0.6313454508781433\n",
      "cnt: 0 - valLoss: 0.626624345779419 - trainLoss: 0.6313415765762329\n",
      "cnt: 0 - valLoss: 0.6266199350357056 - trainLoss: 0.6313372850418091\n",
      "cnt: 0 - valLoss: 0.6266151666641235 - trainLoss: 0.6313333511352539\n",
      "cnt: 0 - valLoss: 0.6266102194786072 - trainLoss: 0.6313292384147644\n",
      "cnt: 0 - valLoss: 0.626605749130249 - trainLoss: 0.6313249468803406\n",
      "cnt: 0 - valLoss: 0.6266006827354431 - trainLoss: 0.6313211917877197\n",
      "cnt: 0 - valLoss: 0.6265961527824402 - trainLoss: 0.6313167810440063\n",
      "cnt: 0 - valLoss: 0.6265912652015686 - trainLoss: 0.631312906742096\n",
      "cnt: 0 - valLoss: 0.6265863180160522 - trainLoss: 0.6313087940216064\n",
      "cnt: 0 - valLoss: 0.6265818476676941 - trainLoss: 0.6313045620918274\n",
      "cnt: 0 - valLoss: 0.6265767812728882 - trainLoss: 0.6313007473945618\n",
      "cnt: 0 - valLoss: 0.6265721917152405 - trainLoss: 0.6312963962554932\n",
      "cnt: 0 - valLoss: 0.6265671849250793 - trainLoss: 0.631292462348938\n",
      "cnt: 0 - valLoss: 0.626562237739563 - trainLoss: 0.6312882304191589\n",
      "cnt: 0 - valLoss: 0.6265575289726257 - trainLoss: 0.6312839388847351\n",
      "cnt: 0 - valLoss: 0.6265522837638855 - trainLoss: 0.6312799453735352\n",
      "cnt: 0 - valLoss: 0.6265477538108826 - trainLoss: 0.631275475025177\n",
      "cnt: 0 - valLoss: 0.6265425086021423 - trainLoss: 0.6312716007232666\n",
      "cnt: 0 - valLoss: 0.6265376806259155 - trainLoss: 0.631267249584198\n",
      "cnt: 0 - valLoss: 0.626532793045044 - trainLoss: 0.6312631368637085\n",
      "cnt: 0 - valLoss: 0.6265276670455933 - trainLoss: 0.631259024143219\n",
      "cnt: 0 - valLoss: 0.6265230774879456 - trainLoss: 0.6312546730041504\n",
      "cnt: 0 - valLoss: 0.6265178918838501 - trainLoss: 0.63125079870224\n",
      "cnt: 0 - valLoss: 0.6265131831169128 - trainLoss: 0.6312463879585266\n",
      "cnt: 0 - valLoss: 0.6265081167221069 - trainLoss: 0.6312424540519714\n",
      "cnt: 0 - valLoss: 0.6265032291412354 - trainLoss: 0.6312381625175476\n",
      "cnt: 0 - valLoss: 0.6264983415603638 - trainLoss: 0.6312339901924133\n",
      "cnt: 0 - valLoss: 0.6264931559562683 - trainLoss: 0.6312299370765686\n",
      "cnt: 0 - valLoss: 0.6264886856079102 - trainLoss: 0.6312255263328552\n",
      "cnt: 0 - valLoss: 0.6264833807945251 - trainLoss: 0.6312217116355896\n",
      "cnt: 0 - valLoss: 0.6264786720275879 - trainLoss: 0.6312173008918762\n",
      "cnt: 0 - valLoss: 0.6264736652374268 - trainLoss: 0.6312131881713867\n",
      "cnt: 0 - valLoss: 0.6264687180519104 - trainLoss: 0.6312090158462524\n",
      "cnt: 0 - valLoss: 0.6264639496803284 - trainLoss: 0.6312047839164734\n",
      "cnt: 0 - valLoss: 0.6264586448669434 - trainLoss: 0.6312007904052734\n",
      "cnt: 0 - valLoss: 0.6264541149139404 - trainLoss: 0.6311963796615601\n",
      "cnt: 0 - valLoss: 0.6264488101005554 - trainLoss: 0.6311924457550049\n",
      "cnt: 0 - valLoss: 0.6264439821243286 - trainLoss: 0.6311880946159363\n",
      "cnt: 0 - valLoss: 0.6264390349388123 - trainLoss: 0.6311839818954468\n",
      "cnt: 0 - valLoss: 0.6264339685440063 - trainLoss: 0.6311798691749573\n",
      "cnt: 0 - valLoss: 0.6264292597770691 - trainLoss: 0.6311755180358887\n",
      "cnt: 0 - valLoss: 0.6264240145683289 - trainLoss: 0.6311715841293335\n",
      "cnt: 0 - valLoss: 0.6264194250106812 - trainLoss: 0.6311672329902649\n",
      "cnt: 0 - valLoss: 0.6264142394065857 - trainLoss: 0.6311632394790649\n",
      "cnt: 0 - valLoss: 0.6264093518257141 - trainLoss: 0.6311588883399963\n",
      "cnt: 0 - valLoss: 0.626404345035553 - trainLoss: 0.6311547756195068\n",
      "cnt: 0 - valLoss: 0.6263991594314575 - trainLoss: 0.6311506628990173\n",
      "cnt: 0 - valLoss: 0.6263946294784546 - trainLoss: 0.631146252155304\n",
      "cnt: 0 - valLoss: 0.6263893842697144 - trainLoss: 0.6311424374580383\n",
      "cnt: 0 - valLoss: 0.6263846158981323 - trainLoss: 0.631138026714325\n",
      "cnt: 0 - valLoss: 0.6263796091079712 - trainLoss: 0.6311339735984802\n",
      "cnt: 0 - valLoss: 0.6263746023178101 - trainLoss: 0.6311297416687012\n",
      "cnt: 0 - valLoss: 0.626369833946228 - trainLoss: 0.6311255097389221\n",
      "cnt: 0 - valLoss: 0.6263646483421326 - trainLoss: 0.6311215162277222\n",
      "cnt: 0 - valLoss: 0.6263599395751953 - trainLoss: 0.631117045879364\n",
      "cnt: 0 - valLoss: 0.6263546943664551 - trainLoss: 0.6311131715774536\n",
      "cnt: 0 - valLoss: 0.6263498663902283 - trainLoss: 0.6311087608337402\n",
      "cnt: 0 - valLoss: 0.6263448596000671 - trainLoss: 0.6311046481132507\n",
      "cnt: 0 - valLoss: 0.6263397932052612 - trainLoss: 0.6311005353927612\n",
      "cnt: 0 - valLoss: 0.6263351440429688 - trainLoss: 0.6310961842536926\n",
      "cnt: 0 - valLoss: 0.6263298988342285 - trainLoss: 0.6310921907424927\n",
      "cnt: 0 - valLoss: 0.6263251900672913 - trainLoss: 0.6310877799987793\n",
      "cnt: 0 - valLoss: 0.626319944858551 - trainLoss: 0.6310838460922241\n",
      "cnt: 0 - valLoss: 0.6263149976730347 - trainLoss: 0.6310795545578003\n",
      "cnt: 0 - valLoss: 0.6263101696968079 - trainLoss: 0.6310754418373108\n",
      "cnt: 0 - valLoss: 0.6263049840927124 - trainLoss: 0.6310712695121765\n",
      "cnt: 0 - valLoss: 0.6263004541397095 - trainLoss: 0.6310667991638184\n",
      "cnt: 0 - valLoss: 0.626295268535614 - trainLoss: 0.631062924861908\n",
      "cnt: 0 - valLoss: 0.6262905597686768 - trainLoss: 0.6310585141181946\n",
      "cnt: 0 - valLoss: 0.6262853741645813 - trainLoss: 0.6310544013977051\n",
      "cnt: 0 - valLoss: 0.6262804269790649 - trainLoss: 0.631050169467926\n",
      "cnt: 0 - valLoss: 0.6262757182121277 - trainLoss: 0.6310458779335022\n",
      "cnt: 0 - valLoss: 0.6262704730033875 - trainLoss: 0.6310418844223022\n",
      "cnt: 0 - valLoss: 0.6262659430503845 - trainLoss: 0.6310374140739441\n",
      "cnt: 0 - valLoss: 0.6262608170509338 - trainLoss: 0.6310335397720337\n",
      "cnt: 0 - valLoss: 0.6262558698654175 - trainLoss: 0.6310290694236755\n",
      "cnt: 0 - valLoss: 0.6262509822845459 - trainLoss: 0.6310250163078308\n",
      "cnt: 0 - valLoss: 0.6262458562850952 - trainLoss: 0.6310208439826965\n",
      "cnt: 0 - valLoss: 0.6262412071228027 - trainLoss: 0.6310164332389832\n",
      "cnt: 0 - valLoss: 0.6262359619140625 - trainLoss: 0.631012499332428\n",
      "cnt: 0 - valLoss: 0.6262314319610596 - trainLoss: 0.6310080289840698\n",
      "cnt: 0 - valLoss: 0.6262263059616089 - trainLoss: 0.6310039758682251\n",
      "cnt: 0 - valLoss: 0.626221239566803 - trainLoss: 0.6309996843338013\n",
      "cnt: 0 - valLoss: 0.6262164115905762 - trainLoss: 0.6309955716133118\n",
      "cnt: 0 - valLoss: 0.6262112259864807 - trainLoss: 0.6309913992881775\n",
      "cnt: 0 - valLoss: 0.6262066960334778 - trainLoss: 0.6309869885444641\n",
      "cnt: 0 - valLoss: 0.6262014508247375 - trainLoss: 0.6309830546379089\n",
      "cnt: 0 - valLoss: 0.6261968016624451 - trainLoss: 0.6309785842895508\n",
      "cnt: 0 - valLoss: 0.6261916160583496 - trainLoss: 0.630974531173706\n",
      "cnt: 0 - valLoss: 0.6261866092681885 - trainLoss: 0.630970299243927\n",
      "cnt: 0 - valLoss: 0.6261818408966064 - trainLoss: 0.6309659481048584\n",
      "cnt: 0 - valLoss: 0.626176655292511 - trainLoss: 0.6309618949890137\n",
      "cnt: 0 - valLoss: 0.6261721253395081 - trainLoss: 0.6309573650360107\n",
      "cnt: 0 - valLoss: 0.6261667609214783 - trainLoss: 0.6309535503387451\n",
      "cnt: 0 - valLoss: 0.626162052154541 - trainLoss: 0.6309490203857422\n",
      "cnt: 0 - valLoss: 0.6261570453643799 - trainLoss: 0.6309449076652527\n",
      "cnt: 0 - valLoss: 0.626151978969574 - trainLoss: 0.6309406757354736\n",
      "cnt: 0 - valLoss: 0.6261473894119263 - trainLoss: 0.630936324596405\n",
      "cnt: 0 - valLoss: 0.6261420249938965 - trainLoss: 0.6309323310852051\n",
      "cnt: 0 - valLoss: 0.626137375831604 - trainLoss: 0.6309278607368469\n",
      "cnt: 0 - valLoss: 0.6261322498321533 - trainLoss: 0.630923867225647\n",
      "cnt: 0 - valLoss: 0.6261273622512817 - trainLoss: 0.6309194564819336\n",
      "cnt: 0 - valLoss: 0.6261225342750549 - trainLoss: 0.6309152841567993\n",
      "cnt: 0 - valLoss: 0.6261172294616699 - trainLoss: 0.630911111831665\n",
      "cnt: 0 - valLoss: 0.626112699508667 - trainLoss: 0.6309066414833069\n",
      "cnt: 0 - valLoss: 0.6261074542999268 - trainLoss: 0.6309027671813965\n",
      "cnt: 0 - valLoss: 0.6261026859283447 - trainLoss: 0.6308982372283936\n",
      "cnt: 0 - valLoss: 0.6260977387428284 - trainLoss: 0.6308941841125488\n",
      "cnt: 0 - valLoss: 0.6260925531387329 - trainLoss: 0.630889892578125\n",
      "cnt: 0 - valLoss: 0.6260877251625061 - trainLoss: 0.6308855414390564\n",
      "cnt: 0 - valLoss: 0.6260824203491211 - trainLoss: 0.6308814883232117\n",
      "cnt: 0 - valLoss: 0.6260777711868286 - trainLoss: 0.630876898765564\n",
      "cnt: 0 - valLoss: 0.6260724663734436 - trainLoss: 0.6308729648590088\n",
      "cnt: 0 - valLoss: 0.6260678172111511 - trainLoss: 0.6308684349060059\n",
      "cnt: 0 - valLoss: 0.6260626912117004 - trainLoss: 0.6308642625808716\n",
      "cnt: 0 - valLoss: 0.6260575652122498 - trainLoss: 0.630859911441803\n",
      "cnt: 0 - valLoss: 0.6260526776313782 - trainLoss: 0.6308556199073792\n",
      "cnt: 0 - valLoss: 0.6260474920272827 - trainLoss: 0.6308513879776001\n",
      "cnt: 0 - valLoss: 0.6260429620742798 - trainLoss: 0.6308468580245972\n",
      "cnt: 0 - valLoss: 0.6260375380516052 - trainLoss: 0.630842924118042\n",
      "cnt: 0 - valLoss: 0.626032829284668 - trainLoss: 0.6308383941650391\n",
      "cnt: 0 - valLoss: 0.6260277628898621 - trainLoss: 0.6308342814445496\n",
      "cnt: 0 - valLoss: 0.6260225772857666 - trainLoss: 0.6308298707008362\n",
      "cnt: 0 - valLoss: 0.6260177493095398 - trainLoss: 0.6308255791664124\n",
      "cnt: 0 - valLoss: 0.6260125041007996 - trainLoss: 0.6308214068412781\n",
      "cnt: 0 - valLoss: 0.6260080337524414 - trainLoss: 0.6308168172836304\n",
      "cnt: 0 - valLoss: 0.6260025501251221 - trainLoss: 0.63081294298172\n",
      "cnt: 0 - valLoss: 0.6259978413581848 - trainLoss: 0.6308083534240723\n",
      "cnt: 0 - valLoss: 0.6259927749633789 - trainLoss: 0.6308042407035828\n",
      "cnt: 0 - valLoss: 0.6259877681732178 - trainLoss: 0.6307998895645142\n",
      "cnt: 0 - valLoss: 0.6259828209877014 - trainLoss: 0.6307955980300903\n",
      "cnt: 0 - valLoss: 0.6259775757789612 - trainLoss: 0.630791425704956\n",
      "cnt: 0 - valLoss: 0.6259729862213135 - trainLoss: 0.6307868957519531\n",
      "cnt: 0 - valLoss: 0.6259677410125732 - trainLoss: 0.6307829022407532\n",
      "cnt: 0 - valLoss: 0.6259627938270569 - trainLoss: 0.6307783722877502\n",
      "cnt: 0 - valLoss: 0.625957727432251 - trainLoss: 0.6307742595672607\n",
      "cnt: 0 - valLoss: 0.6259527802467346 - trainLoss: 0.6307698488235474\n",
      "cnt: 0 - valLoss: 0.6259479522705078 - trainLoss: 0.6307656168937683\n",
      "cnt: 0 - valLoss: 0.6259427070617676 - trainLoss: 0.6307613849639893\n",
      "cnt: 0 - valLoss: 0.6259378790855408 - trainLoss: 0.6307568550109863\n",
      "cnt: 0 - valLoss: 0.6259326934814453 - trainLoss: 0.6307529211044312\n",
      "cnt: 0 - valLoss: 0.6259279847145081 - trainLoss: 0.6307483911514282\n",
      "cnt: 0 - valLoss: 0.6259228587150574 - trainLoss: 0.630744218826294\n",
      "cnt: 0 - valLoss: 0.6259177327156067 - trainLoss: 0.6307398676872253\n",
      "cnt: 0 - valLoss: 0.6259128451347351 - trainLoss: 0.6307355165481567\n",
      "cnt: 0 - valLoss: 0.6259076595306396 - trainLoss: 0.6307313442230225\n",
      "cnt: 0 - valLoss: 0.6259030699729919 - trainLoss: 0.6307268738746643\n",
      "cnt: 0 - valLoss: 0.6258977651596069 - trainLoss: 0.6307228207588196\n",
      "cnt: 0 - valLoss: 0.6258928775787354 - trainLoss: 0.6307182908058167\n",
      "cnt: 0 - valLoss: 0.6258877515792847 - trainLoss: 0.6307141780853271\n",
      "cnt: 0 - valLoss: 0.6258827447891235 - trainLoss: 0.6307097673416138\n",
      "cnt: 0 - valLoss: 0.6258779168128967 - trainLoss: 0.6307054758071899\n",
      "cnt: 0 - valLoss: 0.6258727312088013 - trainLoss: 0.6307012438774109\n",
      "cnt: 0 - valLoss: 0.6258678436279297 - trainLoss: 0.6306967735290527\n",
      "cnt: 0 - valLoss: 0.6258626580238342 - trainLoss: 0.6306927800178528\n",
      "cnt: 0 - valLoss: 0.625857949256897 - trainLoss: 0.6306881904602051\n",
      "cnt: 0 - valLoss: 0.6258528232574463 - trainLoss: 0.6306841373443604\n",
      "cnt: 0 - valLoss: 0.6258476376533508 - trainLoss: 0.6306796669960022\n",
      "cnt: 0 - valLoss: 0.6258427500724792 - trainLoss: 0.6306754350662231\n",
      "cnt: 0 - valLoss: 0.625837504863739 - trainLoss: 0.6306712031364441\n",
      "cnt: 0 - valLoss: 0.6258329153060913 - trainLoss: 0.6306666731834412\n",
      "cnt: 0 - valLoss: 0.6258276700973511 - trainLoss: 0.6306626796722412\n",
      "cnt: 0 - valLoss: 0.6258227825164795 - trainLoss: 0.6306580901145935\n",
      "cnt: 0 - valLoss: 0.6258176565170288 - trainLoss: 0.6306540369987488\n",
      "cnt: 0 - valLoss: 0.6258126497268677 - trainLoss: 0.6306495666503906\n",
      "cnt: 0 - valLoss: 0.6258077621459961 - trainLoss: 0.630645215511322\n",
      "cnt: 0 - valLoss: 0.6258025169372559 - trainLoss: 0.6306410431861877\n",
      "cnt: 0 - valLoss: 0.6257977485656738 - trainLoss: 0.6306365728378296\n",
      "cnt: 0 - valLoss: 0.6257924437522888 - trainLoss: 0.6306325197219849\n",
      "cnt: 0 - valLoss: 0.6257877349853516 - trainLoss: 0.6306279897689819\n",
      "cnt: 0 - valLoss: 0.6257825493812561 - trainLoss: 0.6306238770484924\n",
      "cnt: 0 - valLoss: 0.6257776618003845 - trainLoss: 0.6306194067001343\n",
      "cnt: 0 - valLoss: 0.6257725358009338 - trainLoss: 0.6306151747703552\n",
      "cnt: 0 - valLoss: 0.6257674098014832 - trainLoss: 0.6306109428405762\n",
      "cnt: 0 - valLoss: 0.6257627010345459 - trainLoss: 0.6306064128875732\n",
      "cnt: 0 - valLoss: 0.6257573962211609 - trainLoss: 0.6306024193763733\n",
      "cnt: 0 - valLoss: 0.6257527470588684 - trainLoss: 0.6305977702140808\n",
      "cnt: 0 - valLoss: 0.6257474422454834 - trainLoss: 0.6305937170982361\n",
      "cnt: 0 - valLoss: 0.6257424354553223 - trainLoss: 0.6305893063545227\n",
      "cnt: 0 - valLoss: 0.6257375478744507 - trainLoss: 0.6305849552154541\n",
      "cnt: 0 - valLoss: 0.6257323622703552 - trainLoss: 0.630580723285675\n",
      "cnt: 0 - valLoss: 0.625727653503418 - trainLoss: 0.6305761933326721\n",
      "cnt: 0 - valLoss: 0.6257221698760986 - trainLoss: 0.6305721998214722\n",
      "cnt: 0 - valLoss: 0.6257175207138062 - trainLoss: 0.6305676102638245\n",
      "cnt: 0 - valLoss: 0.6257122755050659 - trainLoss: 0.6305635571479797\n",
      "cnt: 0 - valLoss: 0.6257073879241943 - trainLoss: 0.6305590867996216\n",
      "cnt: 0 - valLoss: 0.625702440738678 - trainLoss: 0.6305547952651978\n",
      "cnt: 0 - valLoss: 0.6256970763206482 - trainLoss: 0.6305505633354187\n",
      "cnt: 0 - valLoss: 0.6256924271583557 - trainLoss: 0.630545973777771\n",
      "cnt: 0 - valLoss: 0.6256871819496155 - trainLoss: 0.630541980266571\n",
      "cnt: 0 - valLoss: 0.6256824731826782 - trainLoss: 0.6305373907089233\n",
      "cnt: 0 - valLoss: 0.6256771087646484 - trainLoss: 0.6305333375930786\n",
      "cnt: 0 - valLoss: 0.6256721615791321 - trainLoss: 0.6305288672447205\n",
      "cnt: 0 - valLoss: 0.6256672143936157 - trainLoss: 0.6305245161056519\n",
      "cnt: 0 - valLoss: 0.6256620287895203 - trainLoss: 0.630520224571228\n",
      "cnt: 0 - valLoss: 0.6256572008132935 - trainLoss: 0.6305156946182251\n",
      "cnt: 0 - valLoss: 0.6256519556045532 - trainLoss: 0.6305116415023804\n",
      "cnt: 0 - valLoss: 0.6256471872329712 - trainLoss: 0.6305070519447327\n",
      "cnt: 0 - valLoss: 0.6256420016288757 - trainLoss: 0.6305028796195984\n",
      "cnt: 0 - valLoss: 0.6256369352340698 - trainLoss: 0.630498468875885\n",
      "cnt: 0 - valLoss: 0.6256319284439087 - trainLoss: 0.6304941177368164\n",
      "cnt: 0 - valLoss: 0.625626802444458 - trainLoss: 0.6304898262023926\n",
      "cnt: 0 - valLoss: 0.6256218552589417 - trainLoss: 0.6304853558540344\n",
      "cnt: 0 - valLoss: 0.6256166100502014 - trainLoss: 0.6304811835289001\n",
      "cnt: 0 - valLoss: 0.6256119012832642 - trainLoss: 0.6304765343666077\n",
      "cnt: 0 - valLoss: 0.6256067752838135 - trainLoss: 0.6304724812507629\n",
      "cnt: 0 - valLoss: 0.625601589679718 - trainLoss: 0.6304680109024048\n",
      "cnt: 0 - valLoss: 0.6255965828895569 - trainLoss: 0.6304636597633362\n",
      "cnt: 0 - valLoss: 0.6255913972854614 - trainLoss: 0.6304593682289124\n",
      "cnt: 0 - valLoss: 0.6255864500999451 - trainLoss: 0.6304548978805542\n",
      "cnt: 0 - valLoss: 0.6255811452865601 - trainLoss: 0.6304507851600647\n",
      "cnt: 0 - valLoss: 0.6255764961242676 - trainLoss: 0.630446195602417\n",
      "cnt: 0 - valLoss: 0.6255712509155273 - trainLoss: 0.6304420828819275\n",
      "cnt: 0 - valLoss: 0.6255661249160767 - trainLoss: 0.6304375529289246\n",
      "cnt: 0 - valLoss: 0.6255611181259155 - trainLoss: 0.6304332613945007\n",
      "cnt: 0 - valLoss: 0.6255559325218201 - trainLoss: 0.6304289698600769\n",
      "cnt: 0 - valLoss: 0.6255512237548828 - trainLoss: 0.630424439907074\n",
      "cnt: 0 - valLoss: 0.6255457401275635 - trainLoss: 0.6304203867912292\n",
      "cnt: 0 - valLoss: 0.6255409717559814 - trainLoss: 0.6304157376289368\n",
      "cnt: 0 - valLoss: 0.625535786151886 - trainLoss: 0.6304116249084473\n",
      "cnt: 0 - valLoss: 0.6255308985710144 - trainLoss: 0.6304070949554443\n",
      "cnt: 0 - valLoss: 0.625525712966919 - trainLoss: 0.6304028630256653\n",
      "cnt: 0 - valLoss: 0.6255204677581787 - trainLoss: 0.6303985118865967\n",
      "cnt: 0 - valLoss: 0.6255157589912415 - trainLoss: 0.6303939819335938\n",
      "cnt: 0 - valLoss: 0.6255104541778564 - trainLoss: 0.6303898692131042\n",
      "cnt: 0 - valLoss: 0.6255055069923401 - trainLoss: 0.6303852796554565\n",
      "cnt: 0 - valLoss: 0.6255003213882446 - trainLoss: 0.630381166934967\n",
      "cnt: 0 - valLoss: 0.6254953742027283 - trainLoss: 0.6303766369819641\n",
      "cnt: 0 - valLoss: 0.6254903674125671 - trainLoss: 0.6303722858428955\n",
      "cnt: 0 - valLoss: 0.6254850029945374 - trainLoss: 0.6303679943084717\n",
      "cnt: 0 - valLoss: 0.6254802942276001 - trainLoss: 0.6303635239601135\n",
      "cnt: 0 - valLoss: 0.6254749298095703 - trainLoss: 0.6303593516349792\n",
      "cnt: 0 - valLoss: 0.6254702210426331 - trainLoss: 0.6303547620773315\n",
      "cnt: 0 - valLoss: 0.6254650354385376 - trainLoss: 0.630350649356842\n",
      "cnt: 0 - valLoss: 0.6254598498344421 - trainLoss: 0.6303461790084839\n",
      "cnt: 0 - valLoss: 0.625454843044281 - trainLoss: 0.6303418874740601\n",
      "cnt: 0 - valLoss: 0.6254496574401855 - trainLoss: 0.6303375363349915\n",
      "cnt: 0 - valLoss: 0.6254448890686035 - trainLoss: 0.6303330063819885\n",
      "cnt: 0 - valLoss: 0.6254394054412842 - trainLoss: 0.630328893661499\n",
      "cnt: 0 - valLoss: 0.6254346370697021 - trainLoss: 0.6303242444992065\n",
      "cnt: 0 - valLoss: 0.6254294514656067 - trainLoss: 0.630320131778717\n",
      "cnt: 0 - valLoss: 0.6254245042800903 - trainLoss: 0.6303156018257141\n",
      "cnt: 0 - valLoss: 0.6254193186759949 - trainLoss: 0.6303113698959351\n",
      "cnt: 0 - valLoss: 0.6254140734672546 - trainLoss: 0.6303070187568665\n",
      "cnt: 0 - valLoss: 0.6254093647003174 - trainLoss: 0.6303024888038635\n",
      "cnt: 0 - valLoss: 0.6254040002822876 - trainLoss: 0.630298376083374\n",
      "cnt: 0 - valLoss: 0.6253991723060608 - trainLoss: 0.6302937269210815\n",
      "cnt: 0 - valLoss: 0.6253939270973206 - trainLoss: 0.630289614200592\n",
      "cnt: 0 - valLoss: 0.6253889203071594 - trainLoss: 0.6302850842475891\n",
      "cnt: 0 - valLoss: 0.6253838539123535 - trainLoss: 0.6302807927131653\n",
      "cnt: 0 - valLoss: 0.6253784894943237 - trainLoss: 0.6302764415740967\n",
      "cnt: 0 - valLoss: 0.6253737807273865 - trainLoss: 0.6302719116210938\n",
      "cnt: 0 - valLoss: 0.6253684163093567 - trainLoss: 0.6302677989006042\n",
      "cnt: 0 - valLoss: 0.6253637075424194 - trainLoss: 0.6302631497383118\n",
      "cnt: 0 - valLoss: 0.6253582239151001 - trainLoss: 0.6302590370178223\n",
      "cnt: 0 - valLoss: 0.6253532767295837 - trainLoss: 0.6302545070648193\n",
      "cnt: 0 - valLoss: 0.6253482699394226 - trainLoss: 0.6302501559257507\n",
      "cnt: 0 - valLoss: 0.6253430247306824 - trainLoss: 0.6302458047866821\n",
      "cnt: 0 - valLoss: 0.6253383159637451 - trainLoss: 0.630241334438324\n",
      "cnt: 0 - valLoss: 0.625332772731781 - trainLoss: 0.6302372217178345\n",
      "cnt: 0 - valLoss: 0.6253280639648438 - trainLoss: 0.630232572555542\n",
      "cnt: 0 - valLoss: 0.6253228187561035 - trainLoss: 0.6302284598350525\n",
      "cnt: 0 - valLoss: 0.6253177523612976 - trainLoss: 0.6302238702774048\n",
      "cnt: 0 - valLoss: 0.6253126263618469 - trainLoss: 0.630219578742981\n",
      "cnt: 0 - valLoss: 0.6253073811531067 - trainLoss: 0.6302152276039124\n",
      "cnt: 0 - valLoss: 0.6253026127815247 - trainLoss: 0.6302106976509094\n",
      "cnt: 0 - valLoss: 0.6252973079681396 - trainLoss: 0.6302065849304199\n",
      "cnt: 0 - valLoss: 0.6252924203872681 - trainLoss: 0.6302019357681274\n",
      "cnt: 0 - valLoss: 0.6252871751785278 - trainLoss: 0.6301977634429932\n",
      "cnt: 0 - valLoss: 0.6252821683883667 - trainLoss: 0.6301932334899902\n",
      "cnt: 0 - valLoss: 0.6252771615982056 - trainLoss: 0.6301888227462769\n",
      "cnt: 0 - valLoss: 0.6252717971801758 - trainLoss: 0.630184531211853\n",
      "cnt: 0 - valLoss: 0.6252670288085938 - trainLoss: 0.6301800012588501\n",
      "cnt: 0 - valLoss: 0.6252616047859192 - trainLoss: 0.6301758885383606\n",
      "cnt: 0 - valLoss: 0.6252568960189819 - trainLoss: 0.6301712393760681\n",
      "cnt: 0 - valLoss: 0.6252517104148865 - trainLoss: 0.6301671266555786\n",
      "cnt: 0 - valLoss: 0.625246524810791 - trainLoss: 0.6301626563072205\n",
      "cnt: 0 - valLoss: 0.6252415180206299 - trainLoss: 0.6301583647727966\n",
      "cnt: 0 - valLoss: 0.6252363324165344 - trainLoss: 0.6301539540290833\n",
      "cnt: 0 - valLoss: 0.6252315640449524 - trainLoss: 0.6301494836807251\n",
      "cnt: 0 - valLoss: 0.6252261996269226 - trainLoss: 0.6301453113555908\n",
      "cnt: 0 - valLoss: 0.6252214312553406 - trainLoss: 0.6301407217979431\n",
      "cnt: 0 - valLoss: 0.6252161264419556 - trainLoss: 0.6301366686820984\n",
      "cnt: 0 - valLoss: 0.625211238861084 - trainLoss: 0.6301320195198059\n",
      "cnt: 0 - valLoss: 0.6252062320709229 - trainLoss: 0.6301277875900269\n",
      "cnt: 0 - valLoss: 0.6252010464668274 - trainLoss: 0.6301233768463135\n",
      "cnt: 0 - valLoss: 0.6251961588859558 - trainLoss: 0.6301189064979553\n",
      "cnt: 0 - valLoss: 0.625190794467926 - trainLoss: 0.630114734172821\n",
      "cnt: 0 - valLoss: 0.6251861453056335 - trainLoss: 0.6301101446151733\n",
      "cnt: 0 - valLoss: 0.6251809000968933 - trainLoss: 0.6301059722900391\n",
      "cnt: 0 - valLoss: 0.6251760125160217 - trainLoss: 0.6301014423370361\n",
      "cnt: 0 - valLoss: 0.6251709461212158 - trainLoss: 0.6300971508026123\n",
      "cnt: 0 - valLoss: 0.625165581703186 - trainLoss: 0.6300927996635437\n",
      "cnt: 0 - valLoss: 0.625160813331604 - trainLoss: 0.6300883293151855\n",
      "cnt: 0 - valLoss: 0.6251554489135742 - trainLoss: 0.6300841569900513\n",
      "cnt: 0 - valLoss: 0.6251509189605713 - trainLoss: 0.630079448223114\n",
      "cnt: 0 - valLoss: 0.6251456141471863 - trainLoss: 0.6300753951072693\n",
      "cnt: 0 - valLoss: 0.6251405477523804 - trainLoss: 0.6300708055496216\n",
      "cnt: 0 - valLoss: 0.6251354813575745 - trainLoss: 0.6300665736198425\n",
      "cnt: 0 - valLoss: 0.6251303553581238 - trainLoss: 0.6300621032714844\n",
      "cnt: 0 - valLoss: 0.6251255869865417 - trainLoss: 0.6300576329231262\n",
      "cnt: 0 - valLoss: 0.6251202821731567 - trainLoss: 0.6300534605979919\n",
      "cnt: 0 - valLoss: 0.6251153945922852 - trainLoss: 0.6300488114356995\n",
      "cnt: 0 - valLoss: 0.6251100301742554 - trainLoss: 0.63004469871521\n",
      "cnt: 0 - valLoss: 0.6251052021980286 - trainLoss: 0.630040168762207\n",
      "cnt: 0 - valLoss: 0.6251000165939331 - trainLoss: 0.6300357580184937\n",
      "cnt: 0 - valLoss: 0.6250946521759033 - trainLoss: 0.6300312876701355\n",
      "cnt: 0 - valLoss: 0.6250898838043213 - trainLoss: 0.6300268173217773\n",
      "cnt: 0 - valLoss: 0.6250844597816467 - trainLoss: 0.6300225853919983\n",
      "cnt: 0 - valLoss: 0.6250796318054199 - trainLoss: 0.630017876625061\n",
      "cnt: 0 - valLoss: 0.6250742077827454 - trainLoss: 0.6300137639045715\n",
      "cnt: 0 - valLoss: 0.6250693202018738 - trainLoss: 0.630009114742279\n",
      "cnt: 0 - valLoss: 0.6250640153884888 - trainLoss: 0.6300047636032104\n",
      "cnt: 0 - valLoss: 0.6250588893890381 - trainLoss: 0.6300003528594971\n",
      "cnt: 0 - valLoss: 0.6250540018081665 - trainLoss: 0.6299958229064941\n",
      "cnt: 0 - valLoss: 0.6250485181808472 - trainLoss: 0.6299915313720703\n",
      "cnt: 0 - valLoss: 0.6250438094139099 - trainLoss: 0.6299868822097778\n",
      "cnt: 0 - valLoss: 0.6250385046005249 - trainLoss: 0.6299826502799988\n",
      "cnt: 0 - valLoss: 0.6250333786010742 - trainLoss: 0.6299780011177063\n",
      "cnt: 0 - valLoss: 0.6250283122062683 - trainLoss: 0.6299737095832825\n",
      "cnt: 0 - valLoss: 0.6250231266021729 - trainLoss: 0.6299692392349243\n",
      "cnt: 0 - valLoss: 0.6250180602073669 - trainLoss: 0.6299646496772766\n",
      "cnt: 0 - valLoss: 0.6250126957893372 - trainLoss: 0.6299603581428528\n",
      "cnt: 0 - valLoss: 0.6250079870223999 - trainLoss: 0.6299557685852051\n",
      "cnt: 0 - valLoss: 0.6250025629997253 - trainLoss: 0.6299516558647156\n",
      "cnt: 0 - valLoss: 0.624997615814209 - trainLoss: 0.6299470663070679\n",
      "cnt: 0 - valLoss: 0.624992311000824 - trainLoss: 0.6299427151679993\n",
      "cnt: 0 - valLoss: 0.6249871253967285 - trainLoss: 0.6299383640289307\n",
      "cnt: 0 - valLoss: 0.6249821782112122 - trainLoss: 0.6299338936805725\n",
      "cnt: 0 - valLoss: 0.6249767541885376 - trainLoss: 0.6299296617507935\n",
      "cnt: 0 - valLoss: 0.6249720454216003 - trainLoss: 0.629925012588501\n",
      "cnt: 0 - valLoss: 0.6249666810035706 - trainLoss: 0.6299208998680115\n",
      "cnt: 0 - valLoss: 0.6249616742134094 - trainLoss: 0.6299163103103638\n",
      "cnt: 0 - valLoss: 0.6249565482139587 - trainLoss: 0.6299120187759399\n",
      "cnt: 0 - valLoss: 0.6249513030052185 - trainLoss: 0.6299076080322266\n",
      "cnt: 0 - valLoss: 0.6249462366104126 - trainLoss: 0.6299031376838684\n",
      "cnt: 0 - valLoss: 0.624940812587738 - trainLoss: 0.6298989057540894\n",
      "cnt: 0 - valLoss: 0.6249361634254456 - trainLoss: 0.6298942565917969\n",
      "cnt: 0 - valLoss: 0.6249306201934814 - trainLoss: 0.6298902034759521\n",
      "cnt: 0 - valLoss: 0.6249256730079651 - trainLoss: 0.6298856139183044\n",
      "cnt: 0 - valLoss: 0.6249205470085144 - trainLoss: 0.6298813223838806\n",
      "cnt: 0 - valLoss: 0.6249153017997742 - trainLoss: 0.629876971244812\n",
      "cnt: 0 - valLoss: 0.6249103546142578 - trainLoss: 0.6298724412918091\n",
      "cnt: 0 - valLoss: 0.6249049305915833 - trainLoss: 0.6298683285713196\n",
      "cnt: 0 - valLoss: 0.6249002814292908 - trainLoss: 0.6298636198043823\n",
      "cnt: 0 - valLoss: 0.6248947978019714 - trainLoss: 0.6298595666885376\n",
      "cnt: 0 - valLoss: 0.6248898506164551 - trainLoss: 0.6298549175262451\n",
      "cnt: 0 - valLoss: 0.6248846650123596 - trainLoss: 0.6298506855964661\n",
      "cnt: 0 - valLoss: 0.6248794198036194 - trainLoss: 0.6298462748527527\n",
      "cnt: 0 - valLoss: 0.624874472618103 - trainLoss: 0.6298418045043945\n",
      "cnt: 0 - valLoss: 0.6248690485954285 - trainLoss: 0.6298375725746155\n",
      "cnt: 0 - valLoss: 0.6248642802238464 - trainLoss: 0.629832923412323\n",
      "cnt: 0 - valLoss: 0.6248587965965271 - trainLoss: 0.6298288106918335\n",
      "cnt: 0 - valLoss: 0.6248539090156555 - trainLoss: 0.6298242807388306\n",
      "cnt: 0 - valLoss: 0.6248487234115601 - trainLoss: 0.6298199892044067\n",
      "cnt: 0 - valLoss: 0.6248434782028198 - trainLoss: 0.6298155188560486\n",
      "cnt: 0 - valLoss: 0.6248385310173035 - trainLoss: 0.6298110485076904\n",
      "cnt: 0 - valLoss: 0.6248331069946289 - trainLoss: 0.6298068761825562\n",
      "cnt: 0 - valLoss: 0.6248282194137573 - trainLoss: 0.6298022866249084\n",
      "cnt: 0 - valLoss: 0.624822735786438 - trainLoss: 0.629798173904419\n",
      "cnt: 0 - valLoss: 0.6248177289962769 - trainLoss: 0.6297935247421265\n",
      "cnt: 0 - valLoss: 0.6248125433921814 - trainLoss: 0.6297892928123474\n",
      "cnt: 0 - valLoss: 0.6248073577880859 - trainLoss: 0.629784882068634\n",
      "cnt: 0 - valLoss: 0.6248024106025696 - trainLoss: 0.6297804117202759\n",
      "cnt: 0 - valLoss: 0.6247969269752502 - trainLoss: 0.6297761797904968\n",
      "cnt: 0 - valLoss: 0.624792218208313 - trainLoss: 0.6297714710235596\n",
      "cnt: 0 - valLoss: 0.6247867345809937 - trainLoss: 0.6297674179077148\n",
      "cnt: 0 - valLoss: 0.6247817277908325 - trainLoss: 0.6297627687454224\n",
      "cnt: 0 - valLoss: 0.6247766017913818 - trainLoss: 0.6297585368156433\n",
      "cnt: 0 - valLoss: 0.6247713565826416 - trainLoss: 0.6297541260719299\n",
      "cnt: 0 - valLoss: 0.6247663497924805 - trainLoss: 0.629749596118927\n",
      "cnt: 0 - valLoss: 0.6247609257698059 - trainLoss: 0.6297454237937927\n",
      "cnt: 0 - valLoss: 0.6247562170028687 - trainLoss: 0.6297407150268555\n",
      "cnt: 0 - valLoss: 0.6247507333755493 - trainLoss: 0.6297366619110107\n",
      "cnt: 0 - valLoss: 0.6247457265853882 - trainLoss: 0.629732072353363\n",
      "cnt: 0 - valLoss: 0.6247405409812927 - trainLoss: 0.6297277212142944\n",
      "cnt: 0 - valLoss: 0.6247353553771973 - trainLoss: 0.629723310470581\n",
      "cnt: 0 - valLoss: 0.6247303485870361 - trainLoss: 0.6297188997268677\n",
      "cnt: 0 - valLoss: 0.6247249245643616 - trainLoss: 0.6297146081924438\n",
      "cnt: 0 - valLoss: 0.6247202157974243 - trainLoss: 0.6297099590301514\n",
      "cnt: 0 - valLoss: 0.624714732170105 - trainLoss: 0.6297058463096619\n",
      "cnt: 0 - valLoss: 0.6247097253799438 - trainLoss: 0.6297012567520142\n",
      "cnt: 0 - valLoss: 0.6247044801712036 - trainLoss: 0.6296969652175903\n",
      "cnt: 0 - valLoss: 0.6246992945671082 - trainLoss: 0.6296924948692322\n",
      "cnt: 0 - valLoss: 0.624694287776947 - trainLoss: 0.6296880841255188\n",
      "cnt: 0 - valLoss: 0.6246888041496277 - trainLoss: 0.6296838521957397\n",
      "cnt: 0 - valLoss: 0.6246840357780457 - trainLoss: 0.6296791434288025\n",
      "cnt: 0 - valLoss: 0.6246785521507263 - trainLoss: 0.629675030708313\n",
      "cnt: 0 - valLoss: 0.6246736645698547 - trainLoss: 0.6296704411506653\n",
      "cnt: 0 - valLoss: 0.6246683597564697 - trainLoss: 0.6296661496162415\n",
      "cnt: 0 - valLoss: 0.6246631145477295 - trainLoss: 0.6296616792678833\n",
      "cnt: 0 - valLoss: 0.6246581077575684 - trainLoss: 0.6296572685241699\n",
      "cnt: 0 - valLoss: 0.624652624130249 - trainLoss: 0.6296529769897461\n",
      "cnt: 0 - valLoss: 0.6246479153633118 - trainLoss: 0.6296483278274536\n",
      "cnt: 0 - valLoss: 0.6246424317359924 - trainLoss: 0.6296442151069641\n",
      "cnt: 0 - valLoss: 0.6246374845504761 - trainLoss: 0.6296396255493164\n",
      "cnt: 0 - valLoss: 0.6246321797370911 - trainLoss: 0.6296353340148926\n",
      "cnt: 0 - valLoss: 0.6246269941329956 - trainLoss: 0.6296308636665344\n",
      "cnt: 0 - valLoss: 0.6246219873428345 - trainLoss: 0.629626452922821\n",
      "cnt: 0 - valLoss: 0.6246164441108704 - trainLoss: 0.6296221613883972\n",
      "cnt: 0 - valLoss: 0.6246117353439331 - trainLoss: 0.6296175122261047\n",
      "cnt: 0 - valLoss: 0.6246062517166138 - trainLoss: 0.62961345911026\n",
      "cnt: 0 - valLoss: 0.6246013045310974 - trainLoss: 0.6296088099479675\n",
      "cnt: 0 - valLoss: 0.6245959997177124 - trainLoss: 0.6296044588088989\n",
      "cnt: 0 - valLoss: 0.6245908141136169 - trainLoss: 0.6296000480651855\n",
      "cnt: 0 - valLoss: 0.6245858073234558 - trainLoss: 0.6295955777168274\n",
      "cnt: 0 - valLoss: 0.6245803236961365 - trainLoss: 0.6295912861824036\n",
      "cnt: 0 - valLoss: 0.6245755553245544 - trainLoss: 0.6295866370201111\n",
      "cnt: 0 - valLoss: 0.6245700120925903 - trainLoss: 0.6295825242996216\n",
      "cnt: 0 - valLoss: 0.624565064907074 - trainLoss: 0.6295778751373291\n",
      "cnt: 0 - valLoss: 0.6245598196983337 - trainLoss: 0.6295735836029053\n",
      "cnt: 0 - valLoss: 0.6245545744895935 - trainLoss: 0.6295691728591919\n",
      "cnt: 0 - valLoss: 0.6245495080947876 - trainLoss: 0.629564642906189\n",
      "cnt: 0 - valLoss: 0.6245440244674683 - trainLoss: 0.6295604109764099\n",
      "cnt: 0 - valLoss: 0.624539315700531 - trainLoss: 0.6295557022094727\n",
      "cnt: 0 - valLoss: 0.6245337724685669 - trainLoss: 0.6295517086982727\n",
      "cnt: 0 - valLoss: 0.6245288252830505 - trainLoss: 0.6295469403266907\n",
      "cnt: 0 - valLoss: 0.6245235204696655 - trainLoss: 0.6295427680015564\n",
      "cnt: 0 - valLoss: 0.6245183348655701 - trainLoss: 0.6295381784439087\n",
      "cnt: 0 - valLoss: 0.6245132684707642 - trainLoss: 0.6295337677001953\n",
      "cnt: 0 - valLoss: 0.6245077848434448 - trainLoss: 0.6295294165611267\n",
      "cnt: 0 - valLoss: 0.624502956867218 - trainLoss: 0.629524827003479\n",
      "cnt: 0 - valLoss: 0.6244974732398987 - trainLoss: 0.6295206546783447\n",
      "cnt: 0 - valLoss: 0.6244925856590271 - trainLoss: 0.6295160055160522\n",
      "cnt: 0 - valLoss: 0.6244872212409973 - trainLoss: 0.629511833190918\n",
      "cnt: 0 - valLoss: 0.6244820356369019 - trainLoss: 0.629507303237915\n",
      "cnt: 0 - valLoss: 0.6244769096374512 - trainLoss: 0.6295028328895569\n",
      "cnt: 0 - valLoss: 0.6244714856147766 - trainLoss: 0.6294984817504883\n",
      "cnt: 0 - valLoss: 0.6244666576385498 - trainLoss: 0.6294938325881958\n",
      "cnt: 0 - valLoss: 0.6244612336158752 - trainLoss: 0.6294897794723511\n",
      "cnt: 0 - valLoss: 0.6244562268257141 - trainLoss: 0.6294850707054138\n",
      "cnt: 0 - valLoss: 0.6244509220123291 - trainLoss: 0.6294808387756348\n",
      "cnt: 0 - valLoss: 0.6244457364082336 - trainLoss: 0.6294763088226318\n",
      "cnt: 0 - valLoss: 0.6244406700134277 - trainLoss: 0.6294718980789185\n",
      "cnt: 0 - valLoss: 0.6244351863861084 - trainLoss: 0.6294675469398499\n",
      "cnt: 0 - valLoss: 0.6244304180145264 - trainLoss: 0.6294629573822021\n",
      "cnt: 0 - valLoss: 0.6244248151779175 - trainLoss: 0.6294588446617126\n",
      "cnt: 0 - valLoss: 0.6244199275970459 - trainLoss: 0.6294540762901306\n",
      "cnt: 0 - valLoss: 0.6244146227836609 - trainLoss: 0.6294498443603516\n",
      "cnt: 0 - valLoss: 0.6244093775749207 - trainLoss: 0.6294453144073486\n",
      "cnt: 0 - valLoss: 0.6244043111801147 - trainLoss: 0.6294409036636353\n",
      "cnt: 0 - valLoss: 0.6243988275527954 - trainLoss: 0.6294365525245667\n",
      "cnt: 0 - valLoss: 0.6243939995765686 - trainLoss: 0.6294319033622742\n",
      "cnt: 0 - valLoss: 0.6243885159492493 - trainLoss: 0.6294277906417847\n",
      "cnt: 0 - valLoss: 0.6243834495544434 - trainLoss: 0.6294230818748474\n",
      "cnt: 0 - valLoss: 0.6243781447410583 - trainLoss: 0.6294189095497131\n",
      "cnt: 0 - valLoss: 0.6243728995323181 - trainLoss: 0.6294143795967102\n",
      "cnt: 0 - valLoss: 0.6243677735328674 - trainLoss: 0.6294099688529968\n",
      "cnt: 0 - valLoss: 0.6243622303009033 - trainLoss: 0.629405677318573\n",
      "cnt: 0 - valLoss: 0.6243573427200317 - trainLoss: 0.6294010877609253\n",
      "cnt: 0 - valLoss: 0.6243517398834229 - trainLoss: 0.6293969750404358\n",
      "cnt: 0 - valLoss: 0.6243468523025513 - trainLoss: 0.6293923258781433\n",
      "cnt: 0 - valLoss: 0.6243413686752319 - trainLoss: 0.6293880939483643\n",
      "cnt: 0 - valLoss: 0.6243361830711365 - trainLoss: 0.6293835639953613\n",
      "cnt: 0 - valLoss: 0.624330997467041 - trainLoss: 0.6293792128562927\n",
      "cnt: 0 - valLoss: 0.6243255138397217 - trainLoss: 0.6293748021125793\n",
      "cnt: 0 - valLoss: 0.6243206262588501 - trainLoss: 0.6293702721595764\n",
      "cnt: 0 - valLoss: 0.624315083026886 - trainLoss: 0.6293660402297974\n",
      "cnt: 0 - valLoss: 0.6243100762367249 - trainLoss: 0.6293613910675049\n",
      "cnt: 0 - valLoss: 0.6243047118186951 - trainLoss: 0.6293572783470154\n",
      "cnt: 0 - valLoss: 0.6242994666099548 - trainLoss: 0.6293526887893677\n",
      "cnt: 0 - valLoss: 0.6242942214012146 - trainLoss: 0.6293482780456543\n",
      "cnt: 0 - valLoss: 0.6242887377738953 - trainLoss: 0.6293439269065857\n",
      "cnt: 0 - valLoss: 0.6242838501930237 - trainLoss: 0.629339337348938\n",
      "cnt: 0 - valLoss: 0.6242783069610596 - trainLoss: 0.6293352246284485\n",
      "cnt: 0 - valLoss: 0.6242733001708984 - trainLoss: 0.6293305158615112\n",
      "cnt: 0 - valLoss: 0.6242678165435791 - trainLoss: 0.6293264031410217\n",
      "cnt: 0 - valLoss: 0.6242626905441284 - trainLoss: 0.6293217539787292\n",
      "cnt: 0 - valLoss: 0.6242574453353882 - trainLoss: 0.6293174624443054\n",
      "cnt: 0 - valLoss: 0.6242519617080688 - trainLoss: 0.629313051700592\n",
      "cnt: 0 - valLoss: 0.6242470741271973 - trainLoss: 0.6293085217475891\n",
      "cnt: 0 - valLoss: 0.6242414712905884 - trainLoss: 0.6293042898178101\n",
      "cnt: 0 - valLoss: 0.624236524105072 - trainLoss: 0.6292996406555176\n",
      "cnt: 0 - valLoss: 0.6242310404777527 - trainLoss: 0.6292954087257385\n",
      "cnt: 0 - valLoss: 0.624225914478302 - trainLoss: 0.6292908787727356\n",
      "cnt: 0 - valLoss: 0.624220609664917 - trainLoss: 0.629286527633667\n",
      "cnt: 0 - valLoss: 0.6242151856422424 - trainLoss: 0.6292821168899536\n",
      "cnt: 0 - valLoss: 0.6242101788520813 - trainLoss: 0.6292775273323059\n",
      "cnt: 0 - valLoss: 0.6242046356201172 - trainLoss: 0.6292734146118164\n",
      "cnt: 0 - valLoss: 0.6241996884346008 - trainLoss: 0.6292687058448792\n",
      "cnt: 0 - valLoss: 0.6241942048072815 - trainLoss: 0.6292644739151001\n",
      "cnt: 0 - valLoss: 0.624189019203186 - trainLoss: 0.6292599439620972\n",
      "cnt: 0 - valLoss: 0.624183714389801 - trainLoss: 0.6292555332183838\n",
      "cnt: 0 - valLoss: 0.6241783499717712 - trainLoss: 0.62925124168396\n",
      "cnt: 0 - valLoss: 0.6241733431816101 - trainLoss: 0.6292465925216675\n",
      "cnt: 0 - valLoss: 0.6241677403450012 - trainLoss: 0.629242479801178\n",
      "cnt: 0 - valLoss: 0.6241628527641296 - trainLoss: 0.629237711429596\n",
      "cnt: 0 - valLoss: 0.6241573095321655 - trainLoss: 0.6292335987091064\n",
      "cnt: 0 - valLoss: 0.6241521835327148 - trainLoss: 0.6292290091514587\n",
      "cnt: 0 - valLoss: 0.6241468787193298 - trainLoss: 0.6292246580123901\n",
      "cnt: 0 - valLoss: 0.6241414546966553 - trainLoss: 0.629220187664032\n",
      "cnt: 0 - valLoss: 0.6241364479064941 - trainLoss: 0.6292157173156738\n",
      "cnt: 0 - valLoss: 0.6241308450698853 - trainLoss: 0.6292114853858948\n",
      "cnt: 0 - valLoss: 0.6241259574890137 - trainLoss: 0.6292067170143127\n",
      "cnt: 0 - valLoss: 0.6241204142570496 - trainLoss: 0.6292025446891785\n",
      "cnt: 0 - valLoss: 0.6241152286529541 - trainLoss: 0.6291980147361755\n",
      "cnt: 0 - valLoss: 0.6241099238395691 - trainLoss: 0.6291936039924622\n",
      "cnt: 0 - valLoss: 0.6241046190261841 - trainLoss: 0.6291892528533936\n",
      "cnt: 0 - valLoss: 0.6240994930267334 - trainLoss: 0.6291846632957458\n",
      "cnt: 0 - valLoss: 0.6240938901901245 - trainLoss: 0.6291804313659668\n",
      "cnt: 0 - valLoss: 0.6240890622138977 - trainLoss: 0.6291757822036743\n",
      "cnt: 0 - valLoss: 0.6240834593772888 - trainLoss: 0.62917160987854\n",
      "cnt: 0 - valLoss: 0.6240783333778381 - trainLoss: 0.6291670203208923\n",
      "cnt: 0 - valLoss: 0.6240729689598083 - trainLoss: 0.629162609577179\n",
      "cnt: 0 - valLoss: 0.6240676641464233 - trainLoss: 0.6291581988334656\n",
      "cnt: 0 - valLoss: 0.6240625977516174 - trainLoss: 0.6291536688804626\n",
      "cnt: 0 - valLoss: 0.6240569353103638 - trainLoss: 0.6291493773460388\n",
      "cnt: 0 - valLoss: 0.6240521669387817 - trainLoss: 0.6291446685791016\n",
      "cnt: 0 - valLoss: 0.6240465044975281 - trainLoss: 0.6291406154632568\n",
      "cnt: 0 - valLoss: 0.6240413784980774 - trainLoss: 0.6291359066963196\n",
      "cnt: 0 - valLoss: 0.6240360140800476 - trainLoss: 0.6291316151618958\n",
      "cnt: 0 - valLoss: 0.6240307092666626 - trainLoss: 0.6291271448135376\n",
      "cnt: 0 - valLoss: 0.6240255832672119 - trainLoss: 0.6291226744651794\n",
      "cnt: 0 - valLoss: 0.6240199208259583 - trainLoss: 0.6291183829307556\n",
      "cnt: 0 - valLoss: 0.6240150928497314 - trainLoss: 0.6291137337684631\n",
      "cnt: 0 - valLoss: 0.6240094900131226 - trainLoss: 0.6291095614433289\n",
      "cnt: 0 - valLoss: 0.6240044236183167 - trainLoss: 0.6291048526763916\n",
      "cnt: 0 - valLoss: 0.6239989399909973 - trainLoss: 0.6291006207466125\n",
      "cnt: 0 - valLoss: 0.6239936351776123 - trainLoss: 0.6290960907936096\n",
      "cnt: 0 - valLoss: 0.6239884495735168 - trainLoss: 0.6290916204452515\n",
      "cnt: 0 - valLoss: 0.6239829063415527 - trainLoss: 0.6290873289108276\n",
      "cnt: 0 - valLoss: 0.6239779591560364 - trainLoss: 0.6290826797485352\n",
      "cnt: 0 - valLoss: 0.6239722967147827 - trainLoss: 0.6290785670280457\n",
      "cnt: 0 - valLoss: 0.6239672303199768 - trainLoss: 0.6290737986564636\n",
      "cnt: 0 - valLoss: 0.6239616870880127 - trainLoss: 0.6290695667266846\n",
      "cnt: 0 - valLoss: 0.6239563226699829 - trainLoss: 0.6290649771690369\n",
      "cnt: 0 - valLoss: 0.6239510774612427 - trainLoss: 0.6290605664253235\n",
      "cnt: 0 - valLoss: 0.6239454746246338 - trainLoss: 0.6290561556816101\n",
      "cnt: 0 - valLoss: 0.6239404678344727 - trainLoss: 0.6290515065193176\n",
      "cnt: 0 - valLoss: 0.623934805393219 - trainLoss: 0.6290473937988281\n",
      "cnt: 0 - valLoss: 0.6239296793937683 - trainLoss: 0.6290426850318909\n",
      "cnt: 0 - valLoss: 0.623924195766449 - trainLoss: 0.629038393497467\n",
      "cnt: 0 - valLoss: 0.6239188313484192 - trainLoss: 0.6290338635444641\n",
      "cnt: 0 - valLoss: 0.6239135265350342 - trainLoss: 0.6290293335914612\n",
      "cnt: 0 - valLoss: 0.6239079833030701 - trainLoss: 0.6290250420570374\n",
      "cnt: 0 - valLoss: 0.6239029169082642 - trainLoss: 0.6290203928947449\n",
      "cnt: 0 - valLoss: 0.6238971948623657 - trainLoss: 0.6290161609649658\n",
      "cnt: 0 - valLoss: 0.6238921880722046 - trainLoss: 0.6290113925933838\n",
      "cnt: 0 - valLoss: 0.6238865256309509 - trainLoss: 0.6290071606636047\n",
      "cnt: 0 - valLoss: 0.6238812804222107 - trainLoss: 0.629002571105957\n",
      "cnt: 0 - valLoss: 0.6238759756088257 - trainLoss: 0.6289981603622437\n",
      "cnt: 0 - valLoss: 0.6238704323768616 - trainLoss: 0.6289937496185303\n",
      "cnt: 0 - valLoss: 0.6238653659820557 - trainLoss: 0.6289891600608826\n",
      "cnt: 0 - valLoss: 0.6238596439361572 - trainLoss: 0.6289849281311035\n",
      "cnt: 0 - valLoss: 0.6238546371459961 - trainLoss: 0.6289802193641663\n",
      "cnt: 0 - valLoss: 0.6238489747047424 - trainLoss: 0.6289759874343872\n",
      "cnt: 0 - valLoss: 0.623843789100647 - trainLoss: 0.6289713382720947\n",
      "cnt: 0 - valLoss: 0.6238383650779724 - trainLoss: 0.6289669871330261\n",
      "cnt: 0 - valLoss: 0.6238328814506531 - trainLoss: 0.628962516784668\n",
      "cnt: 0 - valLoss: 0.6238277554512024 - trainLoss: 0.6289579272270203\n",
      "cnt: 0 - valLoss: 0.623822033405304 - trainLoss: 0.6289536952972412\n",
      "cnt: 0 - valLoss: 0.6238170862197876 - trainLoss: 0.6289489269256592\n",
      "cnt: 0 - valLoss: 0.6238113641738892 - trainLoss: 0.6289447546005249\n",
      "cnt: 0 - valLoss: 0.6238062381744385 - trainLoss: 0.6289401054382324\n",
      "cnt: 0 - valLoss: 0.6238007545471191 - trainLoss: 0.6289357542991638\n",
      "cnt: 0 - valLoss: 0.6237953305244446 - trainLoss: 0.6289312243461609\n",
      "cnt: 0 - valLoss: 0.6237900853157043 - trainLoss: 0.628926694393158\n",
      "cnt: 0 - valLoss: 0.6237844228744507 - trainLoss: 0.6289224028587341\n",
      "cnt: 0 - valLoss: 0.6237794756889343 - trainLoss: 0.6289176344871521\n",
      "cnt: 0 - valLoss: 0.6237737536430359 - trainLoss: 0.6289134621620178\n",
      "cnt: 0 - valLoss: 0.62376868724823 - trainLoss: 0.6289087533950806\n",
      "cnt: 0 - valLoss: 0.6237630844116211 - trainLoss: 0.6289045214653015\n",
      "cnt: 0 - valLoss: 0.6237577199935913 - trainLoss: 0.6288999319076538\n",
      "cnt: 0 - valLoss: 0.6237524151802063 - trainLoss: 0.6288954019546509\n",
      "cnt: 0 - valLoss: 0.6237468123435974 - trainLoss: 0.6288909912109375\n",
      "cnt: 0 - valLoss: 0.6237417459487915 - trainLoss: 0.6288864016532898\n",
      "cnt: 0 - valLoss: 0.6237360835075378 - trainLoss: 0.6288821697235107\n",
      "cnt: 0 - valLoss: 0.6237309575080872 - trainLoss: 0.6288774609565735\n",
      "cnt: 0 - valLoss: 0.6237252950668335 - trainLoss: 0.6288731098175049\n",
      "cnt: 0 - valLoss: 0.6237198710441589 - trainLoss: 0.6288685202598572\n",
      "cnt: 0 - valLoss: 0.6237145066261292 - trainLoss: 0.6288639903068542\n",
      "cnt: 0 - valLoss: 0.6237089037895203 - trainLoss: 0.6288595795631409\n",
      "cnt: 0 - valLoss: 0.6237037181854248 - trainLoss: 0.6288548707962036\n",
      "cnt: 0 - valLoss: 0.6236979365348816 - trainLoss: 0.6288505792617798\n",
      "cnt: 0 - valLoss: 0.6236928701400757 - trainLoss: 0.6288458108901978\n",
      "cnt: 0 - valLoss: 0.6236871480941772 - trainLoss: 0.6288415789604187\n",
      "cnt: 0 - valLoss: 0.6236818432807922 - trainLoss: 0.6288369297981262\n",
      "cnt: 0 - valLoss: 0.6236763596534729 - trainLoss: 0.6288324594497681\n",
      "cnt: 0 - valLoss: 0.6236708164215088 - trainLoss: 0.6288278698921204\n",
      "cnt: 0 - valLoss: 0.6236655712127686 - trainLoss: 0.6288232803344727\n",
      "cnt: 0 - valLoss: 0.6236597895622253 - trainLoss: 0.6288190484046936\n",
      "cnt: 0 - valLoss: 0.6236547231674194 - trainLoss: 0.628814160823822\n",
      "cnt: 0 - valLoss: 0.623649001121521 - trainLoss: 0.628809928894043\n",
      "cnt: 0 - valLoss: 0.6236437559127808 - trainLoss: 0.6288052201271057\n",
      "cnt: 0 - valLoss: 0.6236382126808167 - trainLoss: 0.6288008689880371\n",
      "cnt: 0 - valLoss: 0.6236327290534973 - trainLoss: 0.6287962794303894\n",
      "cnt: 0 - valLoss: 0.6236274242401123 - trainLoss: 0.6287916898727417\n",
      "cnt: 0 - valLoss: 0.6236216425895691 - trainLoss: 0.6287873387336731\n",
      "cnt: 0 - valLoss: 0.6236165761947632 - trainLoss: 0.6287825703620911\n",
      "cnt: 0 - valLoss: 0.6236108541488647 - trainLoss: 0.628778338432312\n",
      "cnt: 0 - valLoss: 0.6236056685447693 - trainLoss: 0.62877357006073\n",
      "cnt: 0 - valLoss: 0.6236000061035156 - trainLoss: 0.6287692189216614\n",
      "cnt: 0 - valLoss: 0.6235945820808411 - trainLoss: 0.6287645697593689\n",
      "cnt: 0 - valLoss: 0.6235892176628113 - trainLoss: 0.628760039806366\n",
      "cnt: 0 - valLoss: 0.6235835552215576 - trainLoss: 0.6287555694580078\n",
      "cnt: 0 - valLoss: 0.6235784292221069 - trainLoss: 0.6287509202957153\n",
      "cnt: 0 - valLoss: 0.623572587966919 - trainLoss: 0.6287466287612915\n",
      "cnt: 0 - valLoss: 0.6235675811767578 - trainLoss: 0.6287418603897095\n",
      "cnt: 0 - valLoss: 0.6235617995262146 - trainLoss: 0.6287376284599304\n",
      "cnt: 0 - valLoss: 0.6235564947128296 - trainLoss: 0.6287329196929932\n",
      "cnt: 0 - valLoss: 0.6235510110855103 - trainLoss: 0.628728449344635\n",
      "cnt: 0 - valLoss: 0.6235454678535461 - trainLoss: 0.6287238597869873\n",
      "cnt: 0 - valLoss: 0.6235402822494507 - trainLoss: 0.6287192702293396\n",
      "cnt: 0 - valLoss: 0.6235344409942627 - trainLoss: 0.6287149786949158\n",
      "cnt: 0 - valLoss: 0.6235294342041016 - trainLoss: 0.628710150718689\n",
      "cnt: 0 - valLoss: 0.6235235929489136 - trainLoss: 0.6287059187889099\n",
      "cnt: 0 - valLoss: 0.6235184073448181 - trainLoss: 0.6287011504173279\n",
      "cnt: 0 - valLoss: 0.6235128045082092 - trainLoss: 0.6286967992782593\n",
      "cnt: 0 - valLoss: 0.6235073804855347 - trainLoss: 0.6286922097206116\n",
      "cnt: 0 - valLoss: 0.6235019564628601 - trainLoss: 0.6286876797676086\n",
      "cnt: 0 - valLoss: 0.6234962940216064 - trainLoss: 0.6286831498146057\n",
      "cnt: 0 - valLoss: 0.6234911680221558 - trainLoss: 0.6286785006523132\n",
      "cnt: 0 - valLoss: 0.6234853267669678 - trainLoss: 0.6286742091178894\n",
      "cnt: 0 - valLoss: 0.6234802007675171 - trainLoss: 0.6286693811416626\n",
      "cnt: 0 - valLoss: 0.6234745383262634 - trainLoss: 0.6286650896072388\n",
      "cnt: 0 - valLoss: 0.6234692335128784 - trainLoss: 0.6286603808403015\n",
      "cnt: 0 - valLoss: 0.6234637498855591 - trainLoss: 0.6286559700965881\n",
      "cnt: 0 - valLoss: 0.6234581470489502 - trainLoss: 0.6286514401435852\n",
      "cnt: 0 - valLoss: 0.6234528422355652 - trainLoss: 0.6286467909812927\n",
      "cnt: 0 - valLoss: 0.6234471201896667 - trainLoss: 0.6286424398422241\n",
      "cnt: 0 - valLoss: 0.6234420537948608 - trainLoss: 0.6286376118659973\n",
      "cnt: 0 - valLoss: 0.6234363317489624 - trainLoss: 0.628633439540863\n",
      "cnt: 0 - valLoss: 0.6234310269355774 - trainLoss: 0.6286286115646362\n",
      "cnt: 0 - valLoss: 0.6234254240989685 - trainLoss: 0.6286242008209229\n",
      "cnt: 0 - valLoss: 0.623420000076294 - trainLoss: 0.6286196708679199\n",
      "cnt: 0 - valLoss: 0.6234145164489746 - trainLoss: 0.6286150217056274\n",
      "cnt: 0 - valLoss: 0.6234089136123657 - trainLoss: 0.6286105513572693\n",
      "cnt: 0 - valLoss: 0.6234037280082703 - trainLoss: 0.628605842590332\n",
      "cnt: 0 - valLoss: 0.6233980059623718 - trainLoss: 0.628601610660553\n",
      "cnt: 0 - valLoss: 0.6233928799629211 - trainLoss: 0.6285967826843262\n",
      "cnt: 0 - valLoss: 0.6233870983123779 - trainLoss: 0.6285924911499023\n",
      "cnt: 0 - valLoss: 0.6233817934989929 - trainLoss: 0.6285877823829651\n",
      "cnt: 0 - valLoss: 0.6233762502670288 - trainLoss: 0.6285833120346069\n",
      "cnt: 0 - valLoss: 0.6233706474304199 - trainLoss: 0.6285788416862488\n",
      "cnt: 0 - valLoss: 0.6233654022216797 - trainLoss: 0.6285741329193115\n",
      "cnt: 0 - valLoss: 0.6233596205711365 - trainLoss: 0.6285697817802429\n",
      "cnt: 0 - valLoss: 0.6233546137809753 - trainLoss: 0.6285649538040161\n",
      "cnt: 0 - valLoss: 0.6233487725257874 - trainLoss: 0.6285607814788818\n",
      "cnt: 0 - valLoss: 0.6233435869216919 - trainLoss: 0.628555953502655\n",
      "cnt: 0 - valLoss: 0.6233379244804382 - trainLoss: 0.6285516023635864\n",
      "cnt: 0 - valLoss: 0.6233325004577637 - trainLoss: 0.6285468935966492\n",
      "cnt: 0 - valLoss: 0.6233270764350891 - trainLoss: 0.6285423636436462\n",
      "cnt: 0 - valLoss: 0.6233214139938354 - trainLoss: 0.6285379528999329\n",
      "cnt: 0 - valLoss: 0.6233161687850952 - trainLoss: 0.6285331845283508\n",
      "cnt: 0 - valLoss: 0.623310387134552 - trainLoss: 0.6285288333892822\n",
      "cnt: 0 - valLoss: 0.6233052611351013 - trainLoss: 0.6285240650177002\n",
      "cnt: 0 - valLoss: 0.6232995390892029 - trainLoss: 0.6285197734832764\n",
      "cnt: 0 - valLoss: 0.6232941746711731 - trainLoss: 0.6285150647163391\n",
      "cnt: 0 - valLoss: 0.623288631439209 - trainLoss: 0.628510594367981\n",
      "cnt: 0 - valLoss: 0.6232830882072449 - trainLoss: 0.6285060048103333\n",
      "cnt: 0 - valLoss: 0.6232777833938599 - trainLoss: 0.6285013556480408\n",
      "cnt: 0 - valLoss: 0.6232720017433167 - trainLoss: 0.6284970045089722\n",
      "cnt: 0 - valLoss: 0.623266875743866 - trainLoss: 0.6284921765327454\n",
      "cnt: 0 - valLoss: 0.6232610940933228 - trainLoss: 0.6284878849983215\n",
      "cnt: 0 - valLoss: 0.6232558488845825 - trainLoss: 0.6284831762313843\n",
      "cnt: 0 - valLoss: 0.6232501864433289 - trainLoss: 0.6284787654876709\n",
      "cnt: 0 - valLoss: 0.6232447624206543 - trainLoss: 0.6284740567207336\n",
      "cnt: 0 - valLoss: 0.6232393383979797 - trainLoss: 0.6284695267677307\n",
      "cnt: 0 - valLoss: 0.6232336759567261 - trainLoss: 0.6284649968147278\n",
      "cnt: 0 - valLoss: 0.6232284307479858 - trainLoss: 0.6284603476524353\n",
      "cnt: 0 - valLoss: 0.6232225894927979 - trainLoss: 0.6284559965133667\n",
      "cnt: 0 - valLoss: 0.6232175230979919 - trainLoss: 0.6284511685371399\n",
      "cnt: 0 - valLoss: 0.6232117414474487 - trainLoss: 0.6284468770027161\n",
      "cnt: 0 - valLoss: 0.6232064366340637 - trainLoss: 0.628442108631134\n",
      "cnt: 0 - valLoss: 0.6232008337974548 - trainLoss: 0.6284377574920654\n",
      "cnt: 0 - valLoss: 0.6231953501701355 - trainLoss: 0.6284330487251282\n",
      "cnt: 0 - valLoss: 0.6231899261474609 - trainLoss: 0.6284284591674805\n",
      "cnt: 0 - valLoss: 0.6231842041015625 - trainLoss: 0.6284239888191223\n",
      "cnt: 0 - valLoss: 0.623179018497467 - trainLoss: 0.6284192800521851\n",
      "cnt: 0 - valLoss: 0.623173177242279 - trainLoss: 0.6284149289131165\n",
      "cnt: 0 - valLoss: 0.6231681108474731 - trainLoss: 0.6284101605415344\n",
      "cnt: 0 - valLoss: 0.6231622695922852 - trainLoss: 0.6284058094024658\n",
      "cnt: 0 - valLoss: 0.6231569647789001 - trainLoss: 0.6284010410308838\n",
      "cnt: 0 - valLoss: 0.6231513619422913 - trainLoss: 0.6283965706825256\n",
      "cnt: 0 - valLoss: 0.6231458187103271 - trainLoss: 0.6283920407295227\n",
      "cnt: 0 - valLoss: 0.6231404542922974 - trainLoss: 0.6283873915672302\n",
      "cnt: 0 - valLoss: 0.6231347322463989 - trainLoss: 0.6283829212188721\n",
      "cnt: 0 - valLoss: 0.6231295466423035 - trainLoss: 0.62837815284729\n",
      "cnt: 0 - valLoss: 0.6231237053871155 - trainLoss: 0.6283738613128662\n",
      "cnt: 0 - valLoss: 0.62311851978302 - trainLoss: 0.6283690333366394\n",
      "cnt: 0 - valLoss: 0.6231127977371216 - trainLoss: 0.628364622592926\n",
      "cnt: 0 - valLoss: 0.623107373714447 - trainLoss: 0.6283599734306335\n",
      "cnt: 0 - valLoss: 0.6231018900871277 - trainLoss: 0.6283555030822754\n",
      "cnt: 0 - valLoss: 0.623096227645874 - trainLoss: 0.6283508539199829\n",
      "cnt: 0 - valLoss: 0.623090922832489 - trainLoss: 0.6283462047576904\n",
      "cnt: 0 - valLoss: 0.623085081577301 - trainLoss: 0.6283418536186218\n",
      "cnt: 0 - valLoss: 0.6230800151824951 - trainLoss: 0.628337025642395\n",
      "cnt: 0 - valLoss: 0.6230741143226624 - trainLoss: 0.628332793712616\n",
      "cnt: 0 - valLoss: 0.6230689287185669 - trainLoss: 0.6283279061317444\n",
      "cnt: 0 - valLoss: 0.6230631470680237 - trainLoss: 0.6283235549926758\n",
      "cnt: 0 - valLoss: 0.6230577826499939 - trainLoss: 0.6283188462257385\n",
      "cnt: 0 - valLoss: 0.6230522990226746 - trainLoss: 0.6283143162727356\n",
      "cnt: 0 - valLoss: 0.6230466365814209 - trainLoss: 0.6283098459243774\n",
      "cnt: 0 - valLoss: 0.6230413317680359 - trainLoss: 0.6283050775527954\n",
      "cnt: 0 - valLoss: 0.6230354905128479 - trainLoss: 0.628300666809082\n",
      "cnt: 0 - valLoss: 0.623030424118042 - trainLoss: 0.6282958984375\n",
      "cnt: 0 - valLoss: 0.6230245232582092 - trainLoss: 0.6282916069030762\n",
      "cnt: 0 - valLoss: 0.623019278049469 - trainLoss: 0.6282867789268494\n",
      "cnt: 0 - valLoss: 0.6230135560035706 - trainLoss: 0.6282824277877808\n",
      "cnt: 0 - valLoss: 0.6230080127716064 - trainLoss: 0.6282776594161987\n",
      "cnt: 0 - valLoss: 0.6230026483535767 - trainLoss: 0.6282731890678406\n",
      "cnt: 0 - valLoss: 0.6229969263076782 - trainLoss: 0.6282686591148376\n",
      "cnt: 0 - valLoss: 0.6229916214942932 - trainLoss: 0.6282639503479004\n",
      "cnt: 0 - valLoss: 0.6229857802391052 - trainLoss: 0.628259539604187\n",
      "cnt: 0 - valLoss: 0.6229807138442993 - trainLoss: 0.6282547116279602\n",
      "cnt: 0 - valLoss: 0.6229748129844666 - trainLoss: 0.6282504200935364\n",
      "cnt: 0 - valLoss: 0.6229695677757263 - trainLoss: 0.6282456517219543\n",
      "cnt: 0 - valLoss: 0.6229638457298279 - trainLoss: 0.6282411813735962\n",
      "cnt: 0 - valLoss: 0.6229583621025085 - trainLoss: 0.6282364726066589\n",
      "cnt: 0 - valLoss: 0.6229528784751892 - trainLoss: 0.628231942653656\n",
      "cnt: 0 - valLoss: 0.6229471564292908 - trainLoss: 0.6282274127006531\n",
      "cnt: 0 - valLoss: 0.6229419708251953 - trainLoss: 0.6282227039337158\n",
      "cnt: 0 - valLoss: 0.6229360103607178 - trainLoss: 0.6282183527946472\n",
      "cnt: 0 - valLoss: 0.6229310035705566 - trainLoss: 0.6282135248184204\n",
      "cnt: 0 - valLoss: 0.6229251027107239 - trainLoss: 0.6282091736793518\n",
      "cnt: 0 - valLoss: 0.6229198575019836 - trainLoss: 0.6282044053077698\n",
      "cnt: 0 - valLoss: 0.62291419506073 - trainLoss: 0.6281999349594116\n",
      "cnt: 0 - valLoss: 0.6229086518287659 - trainLoss: 0.6281952857971191\n",
      "cnt: 0 - valLoss: 0.6229034662246704 - trainLoss: 0.6281907558441162\n",
      "cnt: 0 - valLoss: 0.6228978633880615 - trainLoss: 0.6281863451004028\n",
      "cnt: 0 - valLoss: 0.6228929162025452 - trainLoss: 0.6281816363334656\n",
      "cnt: 0 - valLoss: 0.6228872537612915 - trainLoss: 0.6281775236129761\n",
      "cnt: 0 - valLoss: 0.6228821873664856 - trainLoss: 0.628172755241394\n",
      "cnt: 0 - valLoss: 0.6228767037391663 - trainLoss: 0.6281684637069702\n",
      "cnt: 0 - valLoss: 0.6228713989257812 - trainLoss: 0.6281638145446777\n",
      "cnt: 0 - valLoss: 0.6228662133216858 - trainLoss: 0.6281593441963196\n",
      "cnt: 0 - valLoss: 0.6228606104850769 - trainLoss: 0.6281550526618958\n",
      "cnt: 0 - valLoss: 0.6228556632995605 - trainLoss: 0.6281503438949585\n",
      "cnt: 0 - valLoss: 0.6228499412536621 - trainLoss: 0.6281461715698242\n",
      "cnt: 0 - valLoss: 0.622844934463501 - trainLoss: 0.6281414031982422\n",
      "cnt: 0 - valLoss: 0.6228394508361816 - trainLoss: 0.6281371712684631\n",
      "cnt: 0 - valLoss: 0.6228340864181519 - trainLoss: 0.6281325817108154\n",
      "cnt: 0 - valLoss: 0.6228289008140564 - trainLoss: 0.6281280517578125\n",
      "cnt: 0 - valLoss: 0.6228232979774475 - trainLoss: 0.6281236410140991\n",
      "cnt: 0 - valLoss: 0.6228184103965759 - trainLoss: 0.6281189322471619\n",
      "cnt: 0 - valLoss: 0.6228127479553223 - trainLoss: 0.6281148195266724\n",
      "cnt: 0 - valLoss: 0.6228076219558716 - trainLoss: 0.6281101107597351\n",
      "cnt: 0 - valLoss: 0.622802197933197 - trainLoss: 0.6281057596206665\n",
      "cnt: 0 - valLoss: 0.6227968335151672 - trainLoss: 0.6281011700630188\n",
      "cnt: 0 - valLoss: 0.6227916479110718 - trainLoss: 0.6280966997146606\n",
      "cnt: 0 - valLoss: 0.6227860450744629 - trainLoss: 0.628092348575592\n",
      "cnt: 0 - valLoss: 0.6227810978889465 - trainLoss: 0.6280876398086548\n",
      "cnt: 0 - valLoss: 0.6227754354476929 - trainLoss: 0.6280834674835205\n",
      "cnt: 0 - valLoss: 0.622770369052887 - trainLoss: 0.6280786991119385\n",
      "cnt: 0 - valLoss: 0.6227649450302124 - trainLoss: 0.6280744671821594\n",
      "cnt: 0 - valLoss: 0.6227595210075378 - trainLoss: 0.6280698180198669\n",
      "cnt: 0 - valLoss: 0.6227543354034424 - trainLoss: 0.6280653476715088\n",
      "cnt: 0 - valLoss: 0.6227487325668335 - trainLoss: 0.6280609369277954\n",
      "cnt: 0 - valLoss: 0.6227438449859619 - trainLoss: 0.6280562281608582\n",
      "cnt: 0 - valLoss: 0.6227381825447083 - trainLoss: 0.6280520558357239\n",
      "cnt: 0 - valLoss: 0.6227330565452576 - trainLoss: 0.6280473470687866\n",
      "cnt: 0 - valLoss: 0.6227275729179382 - trainLoss: 0.6280430555343628\n",
      "cnt: 0 - valLoss: 0.6227222681045532 - trainLoss: 0.6280384063720703\n",
      "cnt: 0 - valLoss: 0.6227170825004578 - trainLoss: 0.6280339360237122\n",
      "cnt: 0 - valLoss: 0.6227114200592041 - trainLoss: 0.6280295252799988\n",
      "cnt: 0 - valLoss: 0.6227065324783325 - trainLoss: 0.6280248165130615\n",
      "cnt: 0 - valLoss: 0.6227008104324341 - trainLoss: 0.6280205845832825\n",
      "cnt: 0 - valLoss: 0.6226957440376282 - trainLoss: 0.6280158758163452\n",
      "cnt: 0 - valLoss: 0.6226902604103088 - trainLoss: 0.6280115842819214\n",
      "cnt: 0 - valLoss: 0.6226849555969238 - trainLoss: 0.6280069947242737\n",
      "cnt: 0 - valLoss: 0.6226797103881836 - trainLoss: 0.6280025243759155\n",
      "cnt: 0 - valLoss: 0.6226741075515747 - trainLoss: 0.6279981136322021\n",
      "cnt: 0 - valLoss: 0.6226691603660583 - trainLoss: 0.6279934644699097\n",
      "cnt: 0 - valLoss: 0.6226634383201599 - trainLoss: 0.6279892921447754\n",
      "cnt: 0 - valLoss: 0.6226584315299988 - trainLoss: 0.6279844045639038\n",
      "cnt: 0 - valLoss: 0.6226528882980347 - trainLoss: 0.6279801726341248\n",
      "cnt: 0 - valLoss: 0.6226476430892944 - trainLoss: 0.627975583076477\n",
      "cnt: 0 - valLoss: 0.6226422786712646 - trainLoss: 0.6279711127281189\n",
      "cnt: 0 - valLoss: 0.6226367354393005 - trainLoss: 0.6279666423797607\n",
      "cnt: 0 - valLoss: 0.6226317286491394 - trainLoss: 0.6279619932174683\n",
      "cnt: 0 - valLoss: 0.6226260662078857 - trainLoss: 0.6279577612876892\n",
      "cnt: 0 - valLoss: 0.6226210594177246 - trainLoss: 0.627953052520752\n",
      "cnt: 0 - valLoss: 0.6226155161857605 - trainLoss: 0.6279488801956177\n",
      "cnt: 0 - valLoss: 0.6226103901863098 - trainLoss: 0.6279441714286804\n",
      "cnt: 0 - valLoss: 0.62260502576828 - trainLoss: 0.6279398798942566\n",
      "cnt: 0 - valLoss: 0.6225994825363159 - trainLoss: 0.6279354095458984\n",
      "cnt: 0 - valLoss: 0.6225944757461548 - trainLoss: 0.6279308199882507\n",
      "cnt: 0 - valLoss: 0.6225888729095459 - trainLoss: 0.6279265880584717\n",
      "cnt: 0 - valLoss: 0.6225839257240295 - trainLoss: 0.6279218196868896\n",
      "cnt: 0 - valLoss: 0.6225783228874207 - trainLoss: 0.6279177069664001\n",
      "cnt: 0 - valLoss: 0.6225730776786804 - trainLoss: 0.6279130578041077\n",
      "cnt: 0 - valLoss: 0.6225677728652954 - trainLoss: 0.6279086470603943\n",
      "cnt: 0 - valLoss: 0.6225623488426208 - trainLoss: 0.6279042363166809\n",
      "cnt: 0 - valLoss: 0.6225572824478149 - trainLoss: 0.6278996467590332\n",
      "cnt: 0 - valLoss: 0.6225515604019165 - trainLoss: 0.6278953552246094\n",
      "cnt: 0 - valLoss: 0.6225466728210449 - trainLoss: 0.6278905868530273\n",
      "cnt: 0 - valLoss: 0.622541069984436 - trainLoss: 0.6278864741325378\n",
      "cnt: 0 - valLoss: 0.6225358843803406 - trainLoss: 0.6278818249702454\n",
      "cnt: 0 - valLoss: 0.622530460357666 - trainLoss: 0.6278774738311768\n",
      "cnt: 0 - valLoss: 0.622525155544281 - trainLoss: 0.6278729438781738\n",
      "cnt: 0 - valLoss: 0.6225199103355408 - trainLoss: 0.6278684735298157\n",
      "cnt: 0 - valLoss: 0.6225143671035767 - trainLoss: 0.6278641223907471\n",
      "cnt: 0 - valLoss: 0.6225093603134155 - trainLoss: 0.6278594136238098\n",
      "cnt: 0 - valLoss: 0.6225037574768066 - trainLoss: 0.6278552412986755\n",
      "cnt: 0 - valLoss: 0.6224986910820007 - trainLoss: 0.6278505325317383\n",
      "cnt: 0 - valLoss: 0.6224932074546814 - trainLoss: 0.6278462409973145\n",
      "cnt: 0 - valLoss: 0.6224878430366516 - trainLoss: 0.6278417110443115\n",
      "cnt: 0 - valLoss: 0.6224826574325562 - trainLoss: 0.6278371810913086\n",
      "cnt: 0 - valLoss: 0.6224770545959473 - trainLoss: 0.62783282995224\n",
      "cnt: 0 - valLoss: 0.6224721074104309 - trainLoss: 0.6278281807899475\n",
      "cnt: 0 - valLoss: 0.6224663853645325 - trainLoss: 0.6278240084648132\n",
      "cnt: 0 - valLoss: 0.6224614381790161 - trainLoss: 0.6278192400932312\n",
      "cnt: 0 - valLoss: 0.622455894947052 - trainLoss: 0.6278150677680969\n",
      "cnt: 0 - valLoss: 0.622450590133667 - trainLoss: 0.6278103590011597\n",
      "cnt: 0 - valLoss: 0.622445285320282 - trainLoss: 0.6278060078620911\n",
      "cnt: 0 - valLoss: 0.6224398016929626 - trainLoss: 0.6278015375137329\n",
      "cnt: 0 - valLoss: 0.6224347949028015 - trainLoss: 0.6277968883514404\n",
      "cnt: 0 - valLoss: 0.6224290728569031 - trainLoss: 0.6277927160263062\n",
      "cnt: 0 - valLoss: 0.6224241256713867 - trainLoss: 0.6277878880500793\n",
      "cnt: 0 - valLoss: 0.6224185228347778 - trainLoss: 0.6277837753295898\n",
      "cnt: 0 - valLoss: 0.6224132776260376 - trainLoss: 0.6277790665626526\n",
      "cnt: 0 - valLoss: 0.6224079728126526 - trainLoss: 0.6277746558189392\n",
      "cnt: 0 - valLoss: 0.622402548789978 - trainLoss: 0.627770185470581\n",
      "cnt: 0 - valLoss: 0.6223974227905273 - trainLoss: 0.6277656555175781\n",
      "cnt: 0 - valLoss: 0.6223917603492737 - trainLoss: 0.6277613639831543\n",
      "cnt: 0 - valLoss: 0.6223868727684021 - trainLoss: 0.6277565360069275\n",
      "cnt: 0 - valLoss: 0.6223811507225037 - trainLoss: 0.6277524828910828\n",
      "cnt: 0 - valLoss: 0.622376024723053 - trainLoss: 0.6277477145195007\n",
      "cnt: 0 - valLoss: 0.6223706007003784 - trainLoss: 0.6277434229850769\n",
      "cnt: 0 - valLoss: 0.6223651766777039 - trainLoss: 0.6277388334274292\n",
      "cnt: 0 - valLoss: 0.6223599910736084 - trainLoss: 0.627734363079071\n",
      "cnt: 0 - valLoss: 0.6223543882369995 - trainLoss: 0.6277299523353577\n",
      "cnt: 0 - valLoss: 0.6223494410514832 - trainLoss: 0.6277253031730652\n",
      "cnt: 0 - valLoss: 0.6223437190055847 - trainLoss: 0.6277211308479309\n",
      "cnt: 0 - valLoss: 0.6223386526107788 - trainLoss: 0.6277163624763489\n",
      "cnt: 0 - valLoss: 0.6223331689834595 - trainLoss: 0.6277121305465698\n",
      "cnt: 0 - valLoss: 0.6223278641700745 - trainLoss: 0.6277074813842773\n",
      "cnt: 0 - valLoss: 0.6223224997520447 - trainLoss: 0.627703070640564\n",
      "cnt: 0 - valLoss: 0.6223170161247253 - trainLoss: 0.6276986002922058\n",
      "cnt: 0 - valLoss: 0.6223119497299194 - trainLoss: 0.6276939511299133\n",
      "cnt: 0 - valLoss: 0.622306227684021 - trainLoss: 0.6276897192001343\n",
      "cnt: 0 - valLoss: 0.6223013401031494 - trainLoss: 0.6276849508285522\n",
      "cnt: 0 - valLoss: 0.6222957372665405 - trainLoss: 0.627680778503418\n",
      "cnt: 0 - valLoss: 0.6222905516624451 - trainLoss: 0.6276761293411255\n",
      "cnt: 0 - valLoss: 0.6222850680351257 - trainLoss: 0.6276717782020569\n",
      "cnt: 0 - valLoss: 0.6222796440124512 - trainLoss: 0.6276671886444092\n",
      "cnt: 0 - valLoss: 0.6222745180130005 - trainLoss: 0.6276625990867615\n",
      "cnt: 0 - valLoss: 0.6222687363624573 - trainLoss: 0.6276583671569824\n",
      "cnt: 0 - valLoss: 0.6222639083862305 - trainLoss: 0.6276535391807556\n",
      "cnt: 0 - valLoss: 0.622258186340332 - trainLoss: 0.6276493668556213\n",
      "cnt: 0 - valLoss: 0.6222530603408813 - trainLoss: 0.6276446580886841\n",
      "cnt: 0 - valLoss: 0.622247576713562 - trainLoss: 0.6276403069496155\n",
      "cnt: 0 - valLoss: 0.6222422122955322 - trainLoss: 0.6276357173919678\n",
      "cnt: 0 - valLoss: 0.6222370266914368 - trainLoss: 0.6276311874389648\n",
      "cnt: 0 - valLoss: 0.6222313642501831 - trainLoss: 0.6276268362998962\n",
      "cnt: 0 - valLoss: 0.6222264170646667 - trainLoss: 0.627622127532959\n",
      "cnt: 0 - valLoss: 0.6222206950187683 - trainLoss: 0.6276178956031799\n",
      "cnt: 0 - valLoss: 0.6222156286239624 - trainLoss: 0.6276130676269531\n",
      "cnt: 0 - valLoss: 0.6222100257873535 - trainLoss: 0.6276087760925293\n",
      "cnt: 0 - valLoss: 0.6222047209739685 - trainLoss: 0.6276042461395264\n",
      "cnt: 0 - valLoss: 0.6221994161605835 - trainLoss: 0.6275997161865234\n",
      "cnt: 0 - valLoss: 0.6221938133239746 - trainLoss: 0.6275951862335205\n",
      "cnt: 0 - valLoss: 0.6221888065338135 - trainLoss: 0.6275905966758728\n",
      "cnt: 0 - valLoss: 0.6221830248832703 - trainLoss: 0.627586305141449\n",
      "cnt: 0 - valLoss: 0.6221781373023987 - trainLoss: 0.6275814771652222\n",
      "cnt: 0 - valLoss: 0.622172474861145 - trainLoss: 0.6275773048400879\n",
      "cnt: 0 - valLoss: 0.6221672296524048 - trainLoss: 0.6275725960731506\n",
      "cnt: 0 - valLoss: 0.6221618056297302 - trainLoss: 0.6275681853294373\n",
      "cnt: 0 - valLoss: 0.6221563816070557 - trainLoss: 0.6275636553764343\n",
      "cnt: 0 - valLoss: 0.6221511363983154 - trainLoss: 0.6275590658187866\n",
      "cnt: 0 - valLoss: 0.6221454739570618 - trainLoss: 0.6275546550750732\n",
      "cnt: 0 - valLoss: 0.6221405267715454 - trainLoss: 0.627549946308136\n",
      "cnt: 0 - valLoss: 0.622134804725647 - trainLoss: 0.6275457143783569\n",
      "cnt: 0 - valLoss: 0.6221297383308411 - trainLoss: 0.6275409460067749\n",
      "cnt: 0 - valLoss: 0.6221241354942322 - trainLoss: 0.6275366544723511\n",
      "cnt: 0 - valLoss: 0.6221188306808472 - trainLoss: 0.6275319457054138\n",
      "cnt: 0 - valLoss: 0.6221135258674622 - trainLoss: 0.6275274753570557\n",
      "cnt: 0 - valLoss: 0.6221079230308533 - trainLoss: 0.6275230050086975\n",
      "cnt: 0 - valLoss: 0.6221028566360474 - trainLoss: 0.6275182962417603\n",
      "cnt: 0 - valLoss: 0.6220970153808594 - trainLoss: 0.6275140047073364\n",
      "cnt: 0 - valLoss: 0.622092068195343 - trainLoss: 0.6275091767311096\n",
      "cnt: 0 - valLoss: 0.6220864057540894 - trainLoss: 0.6275048851966858\n",
      "cnt: 0 - valLoss: 0.6220811605453491 - trainLoss: 0.6275001168251038\n",
      "cnt: 0 - valLoss: 0.6220756769180298 - trainLoss: 0.6274957060813904\n",
      "cnt: 0 - valLoss: 0.6220702528953552 - trainLoss: 0.6274910569190979\n",
      "cnt: 0 - valLoss: 0.622065007686615 - trainLoss: 0.6274864077568054\n",
      "cnt: 0 - valLoss: 0.6220592260360718 - trainLoss: 0.627481997013092\n",
      "cnt: 0 - valLoss: 0.6220542788505554 - trainLoss: 0.6274771690368652\n",
      "cnt: 0 - valLoss: 0.622048556804657 - trainLoss: 0.627472996711731\n",
      "cnt: 0 - valLoss: 0.6220433115959167 - trainLoss: 0.6274680495262146\n",
      "cnt: 0 - valLoss: 0.6220377683639526 - trainLoss: 0.627463698387146\n",
      "cnt: 0 - valLoss: 0.6220324635505676 - trainLoss: 0.627458930015564\n",
      "cnt: 0 - valLoss: 0.6220270395278931 - trainLoss: 0.6274544596672058\n",
      "cnt: 0 - valLoss: 0.6220214366912842 - trainLoss: 0.6274498701095581\n",
      "cnt: 0 - valLoss: 0.6220163702964783 - trainLoss: 0.6274451613426208\n",
      "cnt: 0 - valLoss: 0.6220105886459351 - trainLoss: 0.6274408102035522\n",
      "cnt: 0 - valLoss: 0.6220055222511292 - trainLoss: 0.6274359226226807\n",
      "cnt: 0 - valLoss: 0.6219998598098755 - trainLoss: 0.6274316310882568\n",
      "cnt: 0 - valLoss: 0.6219945549964905 - trainLoss: 0.6274267435073853\n",
      "cnt: 0 - valLoss: 0.6219890713691711 - trainLoss: 0.6274222135543823\n",
      "cnt: 0 - valLoss: 0.621983528137207 - trainLoss: 0.6274175643920898\n",
      "cnt: 0 - valLoss: 0.6219783425331116 - trainLoss: 0.6274129152297974\n",
      "cnt: 0 - valLoss: 0.6219725608825684 - trainLoss: 0.627408504486084\n",
      "cnt: 0 - valLoss: 0.6219675540924072 - trainLoss: 0.6274034976959229\n",
      "cnt: 0 - valLoss: 0.621961772441864 - trainLoss: 0.6273992657661438\n",
      "cnt: 0 - valLoss: 0.6219565868377686 - trainLoss: 0.6273943185806274\n",
      "cnt: 0 - valLoss: 0.6219509840011597 - trainLoss: 0.6273899674415588\n",
      "cnt: 0 - valLoss: 0.6219456195831299 - trainLoss: 0.6273852586746216\n",
      "cnt: 0 - valLoss: 0.6219402551651001 - trainLoss: 0.6273806095123291\n",
      "cnt: 0 - valLoss: 0.6219345927238464 - trainLoss: 0.6273760199546814\n",
      "cnt: 0 - valLoss: 0.6219295263290405 - trainLoss: 0.6273712515830994\n",
      "cnt: 0 - valLoss: 0.6219236254692078 - trainLoss: 0.6273669004440308\n",
      "cnt: 0 - valLoss: 0.6219186782836914 - trainLoss: 0.6273619532585144\n",
      "cnt: 0 - valLoss: 0.6219128370285034 - trainLoss: 0.6273576617240906\n",
      "cnt: 0 - valLoss: 0.6219075918197632 - trainLoss: 0.627352774143219\n",
      "cnt: 0 - valLoss: 0.6219021081924438 - trainLoss: 0.6273482441902161\n",
      "cnt: 0 - valLoss: 0.6218966245651245 - trainLoss: 0.6273436546325684\n",
      "cnt: 0 - valLoss: 0.6218912601470947 - trainLoss: 0.6273389458656311\n",
      "cnt: 0 - valLoss: 0.6218855977058411 - trainLoss: 0.6273343563079834\n",
      "cnt: 0 - valLoss: 0.6218805909156799 - trainLoss: 0.6273295879364014\n",
      "cnt: 0 - valLoss: 0.6218747496604919 - trainLoss: 0.6273252367973328\n",
      "cnt: 0 - valLoss: 0.6218695640563965 - trainLoss: 0.6273203492164612\n",
      "cnt: 0 - valLoss: 0.6218639016151428 - trainLoss: 0.6273159384727478\n",
      "cnt: 0 - valLoss: 0.6218587160110474 - trainLoss: 0.627311110496521\n",
      "cnt: 0 - valLoss: 0.6218531727790833 - trainLoss: 0.6273065805435181\n",
      "cnt: 0 - valLoss: 0.6218476295471191 - trainLoss: 0.6273019313812256\n",
      "cnt: 0 - valLoss: 0.6218423247337341 - trainLoss: 0.6272972226142883\n",
      "cnt: 0 - valLoss: 0.6218365430831909 - trainLoss: 0.6272927522659302\n",
      "cnt: 0 - valLoss: 0.6218315958976746 - trainLoss: 0.6272878646850586\n",
      "cnt: 0 - valLoss: 0.6218257546424866 - trainLoss: 0.6272835731506348\n",
      "cnt: 0 - valLoss: 0.6218205690383911 - trainLoss: 0.6272785663604736\n",
      "cnt: 0 - valLoss: 0.6218149662017822 - trainLoss: 0.6272741556167603\n",
      "cnt: 0 - valLoss: 0.6218096017837524 - trainLoss: 0.627269446849823\n",
      "cnt: 0 - valLoss: 0.6218041181564331 - trainLoss: 0.6272648572921753\n",
      "cnt: 0 - valLoss: 0.6217985153198242 - trainLoss: 0.6272602081298828\n",
      "cnt: 0 - valLoss: 0.621793270111084 - trainLoss: 0.6272554993629456\n",
      "cnt: 0 - valLoss: 0.6217876672744751 - trainLoss: 0.6272510886192322\n",
      "cnt: 0 - valLoss: 0.6217825412750244 - trainLoss: 0.6272461414337158\n",
      "cnt: 0 - valLoss: 0.621776819229126 - trainLoss: 0.6272417902946472\n",
      "cnt: 0 - valLoss: 0.6217714548110962 - trainLoss: 0.6272367238998413\n",
      "cnt: 0 - valLoss: 0.6217658519744873 - trainLoss: 0.6272321343421936\n",
      "cnt: 0 - valLoss: 0.6217603087425232 - trainLoss: 0.6272273063659668\n",
      "cnt: 0 - valLoss: 0.6217550039291382 - trainLoss: 0.6272225379943848\n",
      "cnt: 0 - valLoss: 0.621749222278595 - trainLoss: 0.6272178888320923\n",
      "cnt: 0 - valLoss: 0.6217440962791443 - trainLoss: 0.6272130012512207\n",
      "cnt: 0 - valLoss: 0.6217381954193115 - trainLoss: 0.6272085309028625\n",
      "cnt: 0 - valLoss: 0.6217331886291504 - trainLoss: 0.6272034049034119\n",
      "cnt: 0 - valLoss: 0.6217273473739624 - trainLoss: 0.6271989941596985\n",
      "cnt: 0 - valLoss: 0.6217220425605774 - trainLoss: 0.6271940469741821\n",
      "cnt: 0 - valLoss: 0.6217164993286133 - trainLoss: 0.6271893978118896\n",
      "cnt: 0 - valLoss: 0.6217108964920044 - trainLoss: 0.6271845698356628\n",
      "cnt: 0 - valLoss: 0.6217055320739746 - trainLoss: 0.627179741859436\n",
      "cnt: 0 - valLoss: 0.6216997504234314 - trainLoss: 0.6271751523017883\n",
      "cnt: 0 - valLoss: 0.6216946840286255 - trainLoss: 0.627170205116272\n",
      "cnt: 0 - valLoss: 0.6216887831687927 - trainLoss: 0.6271657347679138\n",
      "cnt: 0 - valLoss: 0.6216835975646973 - trainLoss: 0.6271607279777527\n",
      "cnt: 0 - valLoss: 0.6216778755187988 - trainLoss: 0.6271561980247498\n",
      "cnt: 0 - valLoss: 0.621672511100769 - trainLoss: 0.6271512508392334\n",
      "cnt: 0 - valLoss: 0.6216669678688049 - trainLoss: 0.6271466016769409\n",
      "cnt: 0 - valLoss: 0.621661365032196 - trainLoss: 0.6271417737007141\n",
      "cnt: 0 - valLoss: 0.6216561198234558 - trainLoss: 0.6271370053291321\n",
      "cnt: 0 - valLoss: 0.6216502785682678 - trainLoss: 0.6271323561668396\n",
      "cnt: 0 - valLoss: 0.6216451525688171 - trainLoss: 0.6271273493766785\n",
      "cnt: 0 - valLoss: 0.6216393113136292 - trainLoss: 0.6271229386329651\n",
      "cnt: 0 - valLoss: 0.6216340661048889 - trainLoss: 0.627117931842804\n",
      "cnt: 0 - valLoss: 0.6216283440589905 - trainLoss: 0.627113401889801\n",
      "cnt: 0 - valLoss: 0.6216229796409607 - trainLoss: 0.6271083950996399\n",
      "cnt: 0 - valLoss: 0.6216174364089966 - trainLoss: 0.6271037459373474\n",
      "cnt: 0 - valLoss: 0.6216118335723877 - trainLoss: 0.6270989775657654\n",
      "cnt: 0 - valLoss: 0.6216065287590027 - trainLoss: 0.6270940899848938\n",
      "cnt: 0 - valLoss: 0.6216006875038147 - trainLoss: 0.6270895600318909\n",
      "cnt: 0 - valLoss: 0.6215956211090088 - trainLoss: 0.6270844340324402\n",
      "cnt: 0 - valLoss: 0.621589720249176 - trainLoss: 0.6270800828933716\n",
      "cnt: 0 - valLoss: 0.6215845346450806 - trainLoss: 0.6270749568939209\n",
      "cnt: 0 - valLoss: 0.6215787529945374 - trainLoss: 0.627070426940918\n",
      "cnt: 0 - valLoss: 0.6215733289718628 - trainLoss: 0.6270654797554016\n",
      "cnt: 0 - valLoss: 0.6215677857398987 - trainLoss: 0.6270607709884644\n",
      "cnt: 0 - valLoss: 0.6215622425079346 - trainLoss: 0.6270560026168823\n",
      "cnt: 0 - valLoss: 0.6215569376945496 - trainLoss: 0.6270511150360107\n",
      "cnt: 0 - valLoss: 0.6215510964393616 - trainLoss: 0.6270465850830078\n",
      "cnt: 0 - valLoss: 0.6215459704399109 - trainLoss: 0.6270414590835571\n",
      "cnt: 0 - valLoss: 0.6215401291847229 - trainLoss: 0.6270371079444885\n",
      "cnt: 0 - valLoss: 0.6215348839759827 - trainLoss: 0.6270319819450378\n",
      "cnt: 0 - valLoss: 0.6215291619300842 - trainLoss: 0.6270274519920349\n",
      "cnt: 0 - valLoss: 0.6215236783027649 - trainLoss: 0.6270225644111633\n",
      "cnt: 0 - valLoss: 0.6215182542800903 - trainLoss: 0.6270177960395813\n",
      "cnt: 0 - valLoss: 0.6215125322341919 - trainLoss: 0.6270131468772888\n",
      "cnt: 0 - valLoss: 0.6215072870254517 - trainLoss: 0.6270081400871277\n",
      "cnt: 0 - valLoss: 0.6215013861656189 - trainLoss: 0.62700355052948\n",
      "cnt: 0 - valLoss: 0.621496319770813 - trainLoss: 0.6269984841346741\n",
      "cnt: 0 - valLoss: 0.6214904189109802 - trainLoss: 0.6269940733909607\n",
      "cnt: 0 - valLoss: 0.6214852333068848 - trainLoss: 0.62698894739151\n",
      "cnt: 0 - valLoss: 0.6214795112609863 - trainLoss: 0.6269844770431519\n",
      "cnt: 0 - valLoss: 0.621474027633667 - trainLoss: 0.6269794702529907\n",
      "cnt: 0 - valLoss: 0.6214685440063477 - trainLoss: 0.6269747614860535\n",
      "cnt: 0 - valLoss: 0.621462881565094 - trainLoss: 0.6269699931144714\n",
      "cnt: 0 - valLoss: 0.621457576751709 - trainLoss: 0.6269651651382446\n",
      "cnt: 0 - valLoss: 0.6214516758918762 - trainLoss: 0.6269605755805969\n",
      "cnt: 0 - valLoss: 0.6214466094970703 - trainLoss: 0.626955509185791\n",
      "cnt: 0 - valLoss: 0.6214407682418823 - trainLoss: 0.6269510984420776\n",
      "cnt: 0 - valLoss: 0.6214355230331421 - trainLoss: 0.626945972442627\n",
      "cnt: 0 - valLoss: 0.6214298009872437 - trainLoss: 0.6269415020942688\n",
      "cnt: 0 - valLoss: 0.6214244365692139 - trainLoss: 0.626936674118042\n",
      "cnt: 0 - valLoss: 0.6214190125465393 - trainLoss: 0.6269320845603943\n",
      "cnt: 0 - valLoss: 0.6214133501052856 - trainLoss: 0.626927375793457\n",
      "cnt: 0 - valLoss: 0.6214081048965454 - trainLoss: 0.6269226670265198\n",
      "cnt: 0 - valLoss: 0.6214023232460022 - trainLoss: 0.6269181370735168\n",
      "cnt: 0 - valLoss: 0.6213971972465515 - trainLoss: 0.6269131898880005\n",
      "cnt: 0 - valLoss: 0.6213913559913635 - trainLoss: 0.6269088387489319\n",
      "cnt: 0 - valLoss: 0.6213862299919128 - trainLoss: 0.6269039511680603\n",
      "cnt: 0 - valLoss: 0.6213805079460144 - trainLoss: 0.6268996000289917\n",
      "cnt: 0 - valLoss: 0.6213752031326294 - trainLoss: 0.6268946528434753\n",
      "cnt: 0 - valLoss: 0.6213696599006653 - trainLoss: 0.6268901824951172\n",
      "cnt: 0 - valLoss: 0.6213642954826355 - trainLoss: 0.6268854737281799\n",
      "cnt: 0 - valLoss: 0.6213589906692505 - trainLoss: 0.6268808245658875\n",
      "cnt: 0 - valLoss: 0.6213532090187073 - trainLoss: 0.6268762350082397\n",
      "cnt: 0 - valLoss: 0.6213480830192566 - trainLoss: 0.6268712878227234\n",
      "cnt: 0 - valLoss: 0.6213422417640686 - trainLoss: 0.6268669366836548\n",
      "cnt: 0 - valLoss: 0.621337354183197 - trainLoss: 0.6268619894981384\n",
      "cnt: 0 - valLoss: 0.6213315725326538 - trainLoss: 0.626857578754425\n",
      "cnt: 0 - valLoss: 0.621326208114624 - trainLoss: 0.6268526911735535\n",
      "cnt: 0 - valLoss: 0.6213207244873047 - trainLoss: 0.6268481016159058\n",
      "cnt: 0 - valLoss: 0.6213153600692749 - trainLoss: 0.6268433332443237\n",
      "cnt: 0 - valLoss: 0.6213100552558899 - trainLoss: 0.6268386840820312\n",
      "cnt: 0 - valLoss: 0.6213042736053467 - trainLoss: 0.6268340349197388\n",
      "cnt: 0 - valLoss: 0.621299147605896 - trainLoss: 0.626829206943512\n",
      "cnt: 0 - valLoss: 0.621293306350708 - trainLoss: 0.6268247365951538\n",
      "cnt: 0 - valLoss: 0.6212884187698364 - trainLoss: 0.6268197894096375\n",
      "cnt: 0 - valLoss: 0.6212825775146484 - trainLoss: 0.6268154382705688\n",
      "cnt: 0 - valLoss: 0.621277391910553 - trainLoss: 0.6268104910850525\n",
      "cnt: 0 - valLoss: 0.6212717294692993 - trainLoss: 0.6268059611320496\n",
      "cnt: 0 - valLoss: 0.6212664246559143 - trainLoss: 0.6268011331558228\n",
      "cnt: 0 - valLoss: 0.6212610602378845 - trainLoss: 0.6267964839935303\n",
      "cnt: 0 - valLoss: 0.6212553381919861 - trainLoss: 0.6267918348312378\n",
      "cnt: 0 - valLoss: 0.6212500929832458 - trainLoss: 0.626787006855011\n",
      "cnt: 0 - valLoss: 0.6212443113327026 - trainLoss: 0.6267825365066528\n",
      "cnt: 0 - valLoss: 0.6212394833564758 - trainLoss: 0.6267776489257812\n",
      "cnt: 0 - valLoss: 0.6212335824966431 - trainLoss: 0.6267732381820679\n",
      "cnt: 0 - valLoss: 0.6212283372879028 - trainLoss: 0.6267682313919067\n",
      "cnt: 0 - valLoss: 0.621222734451294 - trainLoss: 0.6267637610435486\n",
      "cnt: 0 - valLoss: 0.6212174892425537 - trainLoss: 0.6267589330673218\n",
      "cnt: 0 - valLoss: 0.6212119460105896 - trainLoss: 0.6267542839050293\n",
      "cnt: 0 - valLoss: 0.6212064027786255 - trainLoss: 0.626749575138092\n",
      "cnt: 0 - valLoss: 0.6212010979652405 - trainLoss: 0.6267447471618652\n",
      "cnt: 0 - valLoss: 0.6211953163146973 - trainLoss: 0.6267402172088623\n",
      "cnt: 0 - valLoss: 0.6211904287338257 - trainLoss: 0.626735270023346\n",
      "cnt: 0 - valLoss: 0.6211844682693481 - trainLoss: 0.6267309188842773\n",
      "cnt: 0 - valLoss: 0.6211792826652527 - trainLoss: 0.6267258524894714\n",
      "cnt: 0 - valLoss: 0.621173620223999 - trainLoss: 0.6267213821411133\n",
      "cnt: 0 - valLoss: 0.6211684346199036 - trainLoss: 0.6267164945602417\n",
      "cnt: 0 - valLoss: 0.6211628913879395 - trainLoss: 0.626711905002594\n",
      "cnt: 0 - valLoss: 0.6211572885513306 - trainLoss: 0.626707136631012\n",
      "cnt: 0 - valLoss: 0.6211519837379456 - trainLoss: 0.6267023086547852\n",
      "cnt: 0 - valLoss: 0.6211462616920471 - trainLoss: 0.6266977190971375\n",
      "cnt: 0 - valLoss: 0.621141254901886 - trainLoss: 0.6266928315162659\n",
      "cnt: 0 - valLoss: 0.6211353540420532 - trainLoss: 0.6266883611679077\n",
      "cnt: 0 - valLoss: 0.6211302876472473 - trainLoss: 0.6266834139823914\n",
      "cnt: 0 - valLoss: 0.6211244463920593 - trainLoss: 0.6266788840293884\n",
      "cnt: 0 - valLoss: 0.6211193203926086 - trainLoss: 0.6266739368438721\n",
      "cnt: 0 - valLoss: 0.6211137175559998 - trainLoss: 0.6266694068908691\n",
      "cnt: 0 - valLoss: 0.6211081743240356 - trainLoss: 0.6266645789146423\n",
      "cnt: 0 - valLoss: 0.6211028099060059 - trainLoss: 0.6266598701477051\n",
      "cnt: 0 - valLoss: 0.6210971474647522 - trainLoss: 0.6266551613807678\n",
      "cnt: 0 - valLoss: 0.6210920214653015 - trainLoss: 0.626650333404541\n",
      "cnt: 0 - valLoss: 0.6210861802101135 - trainLoss: 0.6266458034515381\n",
      "cnt: 0 - valLoss: 0.6210811734199524 - trainLoss: 0.6266407370567322\n",
      "cnt: 0 - valLoss: 0.6210752129554749 - trainLoss: 0.6266364455223083\n",
      "cnt: 0 - valLoss: 0.6210700273513794 - trainLoss: 0.6266313791275024\n",
      "cnt: 0 - valLoss: 0.6210645437240601 - trainLoss: 0.6266269087791443\n",
      "cnt: 0 - valLoss: 0.6210591197013855 - trainLoss: 0.6266220211982727\n",
      "cnt: 0 - valLoss: 0.6210535168647766 - trainLoss: 0.6266173720359802\n",
      "cnt: 0 - valLoss: 0.6210479140281677 - trainLoss: 0.6266126036643982\n",
      "cnt: 0 - valLoss: 0.6210428476333618 - trainLoss: 0.6266077756881714\n",
      "cnt: 0 - valLoss: 0.6210370063781738 - trainLoss: 0.6266032457351685\n",
      "cnt: 0 - valLoss: 0.6210318207740784 - trainLoss: 0.6265982389450073\n",
      "cnt: 0 - valLoss: 0.6210258603096008 - trainLoss: 0.6265937685966492\n",
      "cnt: 0 - valLoss: 0.6210207343101501 - trainLoss: 0.6265886425971985\n",
      "cnt: 0 - valLoss: 0.6210147738456726 - trainLoss: 0.6265841126441956\n",
      "cnt: 0 - valLoss: 0.6210094094276428 - trainLoss: 0.6265790462493896\n",
      "cnt: 0 - valLoss: 0.6210036277770996 - trainLoss: 0.6265742778778076\n",
      "cnt: 0 - valLoss: 0.6209980845451355 - trainLoss: 0.626569390296936\n",
      "cnt: 0 - valLoss: 0.6209924817085266 - trainLoss: 0.6265645027160645\n",
      "cnt: 0 - valLoss: 0.6209867596626282 - trainLoss: 0.6265596747398376\n",
      "cnt: 0 - valLoss: 0.6209813952445984 - trainLoss: 0.626554548740387\n",
      "cnt: 0 - valLoss: 0.6209753751754761 - trainLoss: 0.6265498995780945\n",
      "cnt: 0 - valLoss: 0.6209703087806702 - trainLoss: 0.6265446543693542\n",
      "cnt: 0 - valLoss: 0.6209642887115479 - trainLoss: 0.6265401244163513\n",
      "cnt: 0 - valLoss: 0.6209589242935181 - trainLoss: 0.6265348792076111\n",
      "cnt: 0 - valLoss: 0.6209530830383301 - trainLoss: 0.6265301704406738\n",
      "cnt: 0 - valLoss: 0.6209475994110107 - trainLoss: 0.6265251040458679\n",
      "cnt: 0 - valLoss: 0.6209419369697571 - trainLoss: 0.6265202760696411\n",
      "cnt: 0 - valLoss: 0.6209362149238586 - trainLoss: 0.6265153288841248\n",
      "cnt: 0 - valLoss: 0.6209307909011841 - trainLoss: 0.6265103816986084\n",
      "cnt: 0 - valLoss: 0.6209248900413513 - trainLoss: 0.6265056133270264\n",
      "cnt: 0 - valLoss: 0.6209196448326111 - trainLoss: 0.6265004277229309\n",
      "cnt: 0 - valLoss: 0.6209136247634888 - trainLoss: 0.6264958381652832\n",
      "cnt: 0 - valLoss: 0.6209084391593933 - trainLoss: 0.626490592956543\n",
      "cnt: 0 - valLoss: 0.620902419090271 - trainLoss: 0.6264859437942505\n",
      "cnt: 0 - valLoss: 0.6208970546722412 - trainLoss: 0.626480758190155\n",
      "cnt: 0 - valLoss: 0.620891273021698 - trainLoss: 0.626475989818573\n",
      "cnt: 0 - valLoss: 0.6208856701850891 - trainLoss: 0.6264709830284119\n",
      "cnt: 0 - valLoss: 0.620880126953125 - trainLoss: 0.6264661550521851\n",
      "cnt: 0 - valLoss: 0.620874285697937 - trainLoss: 0.6264612078666687\n",
      "cnt: 0 - valLoss: 0.620868980884552 - trainLoss: 0.6264561414718628\n",
      "cnt: 0 - valLoss: 0.6208629608154297 - trainLoss: 0.6264514923095703\n",
      "cnt: 0 - valLoss: 0.6208577752113342 - trainLoss: 0.6264462471008301\n",
      "cnt: 0 - valLoss: 0.6208517551422119 - trainLoss: 0.6264416575431824\n",
      "cnt: 0 - valLoss: 0.6208465099334717 - trainLoss: 0.6264363527297974\n",
      "cnt: 0 - valLoss: 0.6208405494689941 - trainLoss: 0.6264317035675049\n",
      "cnt: 0 - valLoss: 0.6208351254463196 - trainLoss: 0.6264265179634094\n",
      "cnt: 0 - valLoss: 0.6208294034004211 - trainLoss: 0.6264216303825378\n",
      "cnt: 0 - valLoss: 0.6208237409591675 - trainLoss: 0.6264165043830872\n",
      "cnt: 0 - valLoss: 0.6208181381225586 - trainLoss: 0.626411497592926\n",
      "cnt: 0 - valLoss: 0.6208123564720154 - trainLoss: 0.6264065504074097\n",
      "cnt: 0 - valLoss: 0.6208069324493408 - trainLoss: 0.6264013648033142\n",
      "cnt: 0 - valLoss: 0.6208009719848633 - trainLoss: 0.6263965964317322\n",
      "cnt: 0 - valLoss: 0.6207957863807678 - trainLoss: 0.6263912916183472\n",
      "cnt: 0 - valLoss: 0.6207897067070007 - trainLoss: 0.6263866424560547\n",
      "cnt: 0 - valLoss: 0.6207844614982605 - trainLoss: 0.6263812780380249\n",
      "cnt: 0 - valLoss: 0.6207785606384277 - trainLoss: 0.6263765692710876\n",
      "cnt: 0 - valLoss: 0.6207730770111084 - trainLoss: 0.6263713240623474\n",
      "cnt: 0 - valLoss: 0.6207672953605652 - trainLoss: 0.626366376876831\n",
      "cnt: 0 - valLoss: 0.6207616329193115 - trainLoss: 0.6263613700866699\n",
      "cnt: 0 - valLoss: 0.6207561492919922 - trainLoss: 0.626356303691864\n",
      "cnt: 0 - valLoss: 0.6207503080368042 - trainLoss: 0.6263513565063477\n",
      "cnt: 0 - valLoss: 0.6207448840141296 - trainLoss: 0.6263461709022522\n",
      "cnt: 0 - valLoss: 0.6207388639450073 - trainLoss: 0.6263414025306702\n",
      "cnt: 0 - valLoss: 0.6207337379455566 - trainLoss: 0.6263360977172852\n",
      "cnt: 0 - valLoss: 0.6207275986671448 - trainLoss: 0.6263314485549927\n",
      "cnt: 0 - valLoss: 0.6207223534584045 - trainLoss: 0.6263261437416077\n",
      "cnt: 0 - valLoss: 0.620716392993927 - trainLoss: 0.6263213753700256\n",
      "cnt: 0 - valLoss: 0.6207109689712524 - trainLoss: 0.6263161301612854\n",
      "cnt: 0 - valLoss: 0.6207051873207092 - trainLoss: 0.6263112425804138\n",
      "cnt: 0 - valLoss: 0.6206995248794556 - trainLoss: 0.6263061165809631\n",
      "cnt: 0 - valLoss: 0.6206939220428467 - trainLoss: 0.626301109790802\n",
      "cnt: 0 - valLoss: 0.6206880807876587 - trainLoss: 0.6262961030006409\n",
      "cnt: 0 - valLoss: 0.6206827759742737 - trainLoss: 0.6262909770011902\n",
      "cnt: 0 - valLoss: 0.6206766963005066 - trainLoss: 0.6262861490249634\n",
      "cnt: 0 - valLoss: 0.6206714510917664 - trainLoss: 0.6262808442115784\n",
      "cnt: 0 - valLoss: 0.6206653714179993 - trainLoss: 0.6262761950492859\n",
      "cnt: 0 - valLoss: 0.620660126209259 - trainLoss: 0.6262708306312561\n",
      "cnt: 0 - valLoss: 0.6206542253494263 - trainLoss: 0.6262660026550293\n",
      "cnt: 0 - valLoss: 0.6206487417221069 - trainLoss: 0.6262608766555786\n",
      "cnt: 0 - valLoss: 0.620642900466919 - trainLoss: 0.6262559294700623\n",
      "cnt: 0 - valLoss: 0.6206372976303101 - trainLoss: 0.6262508630752563\n",
      "cnt: 0 - valLoss: 0.6206316351890564 - trainLoss: 0.6262458562850952\n",
      "cnt: 0 - valLoss: 0.6206258535385132 - trainLoss: 0.6262408494949341\n",
      "cnt: 0 - valLoss: 0.6206204295158386 - trainLoss: 0.6262356638908386\n",
      "cnt: 0 - valLoss: 0.6206144690513611 - trainLoss: 0.6262308955192566\n",
      "cnt: 0 - valLoss: 0.6206091046333313 - trainLoss: 0.6262255907058716\n",
      "cnt: 0 - valLoss: 0.620603084564209 - trainLoss: 0.6262208223342896\n",
      "cnt: 0 - valLoss: 0.6205978393554688 - trainLoss: 0.6262154579162598\n",
      "cnt: 0 - valLoss: 0.6205918192863464 - trainLoss: 0.6262108087539673\n",
      "cnt: 0 - valLoss: 0.6205864548683167 - trainLoss: 0.6262054443359375\n",
      "cnt: 0 - valLoss: 0.6205804944038391 - trainLoss: 0.6262006759643555\n",
      "cnt: 0 - valLoss: 0.620574951171875 - trainLoss: 0.6261954307556152\n",
      "cnt: 0 - valLoss: 0.6205692887306213 - trainLoss: 0.6261904835700989\n",
      "cnt: 0 - valLoss: 0.6205635666847229 - trainLoss: 0.626185417175293\n",
      "cnt: 0 - valLoss: 0.6205580830574036 - trainLoss: 0.6261803507804871\n",
      "cnt: 0 - valLoss: 0.620552122592926 - trainLoss: 0.6261754035949707\n",
      "cnt: 0 - valLoss: 0.6205467581748962 - trainLoss: 0.6261702179908752\n",
      "cnt: 0 - valLoss: 0.6205406785011292 - trainLoss: 0.6261654496192932\n",
      "cnt: 0 - valLoss: 0.6205354332923889 - trainLoss: 0.6261600852012634\n",
      "cnt: 0 - valLoss: 0.6205294132232666 - trainLoss: 0.6261553764343262\n",
      "cnt: 0 - valLoss: 0.6205240488052368 - trainLoss: 0.6261500716209412\n",
      "cnt: 0 - valLoss: 0.6205180883407593 - trainLoss: 0.6261453032493591\n",
      "cnt: 0 - valLoss: 0.6205126047134399 - trainLoss: 0.6261399984359741\n",
      "cnt: 0 - valLoss: 0.6205068230628967 - trainLoss: 0.6261351108551025\n",
      "cnt: 0 - valLoss: 0.6205011606216431 - trainLoss: 0.6261299848556519\n",
      "cnt: 0 - valLoss: 0.6204955577850342 - trainLoss: 0.6261247992515564\n",
      "cnt: 0 - valLoss: 0.6204897165298462 - trainLoss: 0.6261196136474609\n",
      "cnt: 0 - valLoss: 0.6204842329025269 - trainLoss: 0.6261143684387207\n",
      "cnt: 0 - valLoss: 0.6204782724380493 - trainLoss: 0.6261093020439148\n",
      "cnt: 0 - valLoss: 0.6204729080200195 - trainLoss: 0.6261038780212402\n",
      "cnt: 0 - valLoss: 0.6204668283462524 - trainLoss: 0.6260989904403687\n",
      "cnt: 0 - valLoss: 0.620461642742157 - trainLoss: 0.6260935068130493\n",
      "cnt: 0 - valLoss: 0.6204555630683899 - trainLoss: 0.6260886788368225\n",
      "cnt: 0 - valLoss: 0.6204501986503601 - trainLoss: 0.6260831356048584\n",
      "cnt: 0 - valLoss: 0.6204442381858826 - trainLoss: 0.626078188419342\n",
      "cnt: 0 - valLoss: 0.6204387545585632 - trainLoss: 0.6260728240013123\n",
      "cnt: 0 - valLoss: 0.6204328536987305 - trainLoss: 0.6260677576065063\n",
      "cnt: 0 - valLoss: 0.6204271912574768 - trainLoss: 0.6260625123977661\n",
      "cnt: 0 - valLoss: 0.6204214096069336 - trainLoss: 0.6260572671890259\n",
      "cnt: 0 - valLoss: 0.6204155683517456 - trainLoss: 0.6260520815849304\n",
      "cnt: 0 - valLoss: 0.6204099059104919 - trainLoss: 0.6260467171669006\n",
      "cnt: 0 - valLoss: 0.6204040050506592 - trainLoss: 0.6260416507720947\n",
      "cnt: 0 - valLoss: 0.6203984618186951 - trainLoss: 0.6260361671447754\n",
      "cnt: 0 - valLoss: 0.620392382144928 - trainLoss: 0.6260311007499695\n",
      "cnt: 0 - valLoss: 0.6203870177268982 - trainLoss: 0.6260256171226501\n",
      "cnt: 0 - valLoss: 0.6203808784484863 - trainLoss: 0.6260206699371338\n",
      "cnt: 0 - valLoss: 0.6203755140304565 - trainLoss: 0.6260150671005249\n",
      "cnt: 0 - valLoss: 0.6203693747520447 - trainLoss: 0.6260101199150085\n",
      "cnt: 0 - valLoss: 0.6203638911247253 - trainLoss: 0.6260046362876892\n",
      "cnt: 0 - valLoss: 0.6203579306602478 - trainLoss: 0.6259995698928833\n",
      "cnt: 0 - valLoss: 0.6203523278236389 - trainLoss: 0.6259941458702087\n",
      "cnt: 0 - valLoss: 0.6203464865684509 - trainLoss: 0.6259890794754028\n",
      "cnt: 0 - valLoss: 0.6203407645225525 - trainLoss: 0.6259837746620178\n",
      "cnt: 0 - valLoss: 0.6203349828720093 - trainLoss: 0.6259784698486328\n",
      "cnt: 0 - valLoss: 0.6203290820121765 - trainLoss: 0.6259732842445374\n",
      "cnt: 0 - valLoss: 0.6203235387802124 - trainLoss: 0.6259679198265076\n",
      "cnt: 0 - valLoss: 0.6203174591064453 - trainLoss: 0.6259627938270569\n",
      "cnt: 0 - valLoss: 0.6203120946884155 - trainLoss: 0.6259573101997375\n",
      "cnt: 0 - valLoss: 0.6203058958053589 - trainLoss: 0.6259523630142212\n",
      "cnt: 0 - valLoss: 0.6203005313873291 - trainLoss: 0.6259467601776123\n",
      "cnt: 0 - valLoss: 0.6202943921089172 - trainLoss: 0.6259418725967407\n",
      "cnt: 0 - valLoss: 0.6202889680862427 - trainLoss: 0.6259362697601318\n",
      "cnt: 0 - valLoss: 0.6202829480171204 - trainLoss: 0.6259313225746155\n",
      "cnt: 0 - valLoss: 0.620277464389801 - trainLoss: 0.6259258389472961\n",
      "cnt: 0 - valLoss: 0.6202715635299683 - trainLoss: 0.6259207725524902\n",
      "cnt: 0 - valLoss: 0.6202659010887146 - trainLoss: 0.6259154081344604\n",
      "cnt: 0 - valLoss: 0.6202601790428162 - trainLoss: 0.6259101629257202\n",
      "cnt: 0 - valLoss: 0.6202543377876282 - trainLoss: 0.62590491771698\n",
      "cnt: 0 - valLoss: 0.6202486753463745 - trainLoss: 0.6258996725082397\n",
      "cnt: 0 - valLoss: 0.6202427744865417 - trainLoss: 0.6258944869041443\n",
      "cnt: 0 - valLoss: 0.6202372312545776 - trainLoss: 0.625889003276825\n",
      "cnt: 0 - valLoss: 0.6202312111854553 - trainLoss: 0.625883936882019\n",
      "cnt: 0 - valLoss: 0.6202257871627808 - trainLoss: 0.6258783936500549\n",
      "cnt: 0 - valLoss: 0.6202196478843689 - trainLoss: 0.6258733868598938\n",
      "cnt: 0 - valLoss: 0.6202144026756287 - trainLoss: 0.6258677840232849\n",
      "cnt: 0 - valLoss: 0.620208203792572 - trainLoss: 0.6258628964424133\n",
      "cnt: 0 - valLoss: 0.6202027201652527 - trainLoss: 0.6258572936058044\n",
      "cnt: 0 - valLoss: 0.6201967597007751 - trainLoss: 0.6258523464202881\n",
      "cnt: 0 - valLoss: 0.620191216468811 - trainLoss: 0.6258468627929688\n",
      "cnt: 0 - valLoss: 0.6201852560043335 - trainLoss: 0.6258417963981628\n",
      "cnt: 0 - valLoss: 0.6201796531677246 - trainLoss: 0.6258364319801331\n",
      "cnt: 0 - valLoss: 0.6201738715171814 - trainLoss: 0.6258313059806824\n",
      "cnt: 0 - valLoss: 0.6201680898666382 - trainLoss: 0.6258260011672974\n",
      "cnt: 0 - valLoss: 0.6201624274253845 - trainLoss: 0.6258207559585571\n",
      "cnt: 0 - valLoss: 0.6201565265655518 - trainLoss: 0.6258155703544617\n",
      "cnt: 0 - valLoss: 0.6201509833335876 - trainLoss: 0.6258102059364319\n",
      "cnt: 0 - valLoss: 0.6201449632644653 - trainLoss: 0.6258050799369812\n",
      "cnt: 0 - valLoss: 0.620139479637146 - trainLoss: 0.6257996559143066\n",
      "cnt: 0 - valLoss: 0.6201333999633789 - trainLoss: 0.6257946491241455\n",
      "cnt: 0 - valLoss: 0.6201280951499939 - trainLoss: 0.6257891058921814\n",
      "cnt: 0 - valLoss: 0.6201218962669373 - trainLoss: 0.6257842183113098\n",
      "cnt: 0 - valLoss: 0.6201165318489075 - trainLoss: 0.6257786750793457\n",
      "cnt: 0 - valLoss: 0.6201103329658508 - trainLoss: 0.6257737278938293\n",
      "cnt: 0 - valLoss: 0.6201048493385315 - trainLoss: 0.6257681250572205\n",
      "cnt: 0 - valLoss: 0.6200986504554749 - trainLoss: 0.6257630586624146\n",
      "cnt: 0 - valLoss: 0.6200931072235107 - trainLoss: 0.6257575750350952\n",
      "cnt: 0 - valLoss: 0.6200870275497437 - trainLoss: 0.6257524490356445\n",
      "cnt: 0 - valLoss: 0.6200814247131348 - trainLoss: 0.6257469654083252\n",
      "cnt: 0 - valLoss: 0.6200753450393677 - trainLoss: 0.6257417798042297\n",
      "cnt: 0 - valLoss: 0.620069682598114 - trainLoss: 0.6257363557815552\n",
      "cnt: 0 - valLoss: 0.6200637221336365 - trainLoss: 0.6257312297821045\n",
      "cnt: 0 - valLoss: 0.6200579404830933 - trainLoss: 0.6257258057594299\n",
      "cnt: 0 - valLoss: 0.6200520992279053 - trainLoss: 0.6257205009460449\n",
      "cnt: 0 - valLoss: 0.6200462579727173 - trainLoss: 0.6257151961326599\n",
      "cnt: 0 - valLoss: 0.6200403571128845 - trainLoss: 0.6257098913192749\n",
      "cnt: 0 - valLoss: 0.6200344562530518 - trainLoss: 0.6257046461105347\n",
      "cnt: 0 - valLoss: 0.6200286149978638 - trainLoss: 0.6256992816925049\n",
      "cnt: 0 - valLoss: 0.6200226545333862 - trainLoss: 0.6256940960884094\n",
      "cnt: 0 - valLoss: 0.620016872882843 - trainLoss: 0.6256886124610901\n",
      "cnt: 0 - valLoss: 0.6200107932090759 - trainLoss: 0.6256833076477051\n",
      "cnt: 0 - valLoss: 0.6200051307678223 - trainLoss: 0.6256777048110962\n",
      "cnt: 0 - valLoss: 0.6199989914894104 - trainLoss: 0.625672459602356\n",
      "cnt: 0 - valLoss: 0.6199933290481567 - trainLoss: 0.6256667375564575\n",
      "cnt: 0 - valLoss: 0.6199870705604553 - trainLoss: 0.6256616115570068\n",
      "cnt: 0 - valLoss: 0.6199814677238464 - trainLoss: 0.6256558299064636\n",
      "cnt: 0 - valLoss: 0.619975209236145 - trainLoss: 0.6256506443023682\n",
      "cnt: 0 - valLoss: 0.6199696660041809 - trainLoss: 0.6256449222564697\n",
      "cnt: 0 - valLoss: 0.6199634075164795 - trainLoss: 0.625639796257019\n",
      "cnt: 0 - valLoss: 0.6199579238891602 - trainLoss: 0.625633955001831\n",
      "cnt: 0 - valLoss: 0.6199515461921692 - trainLoss: 0.6256289482116699\n",
      "cnt: 0 - valLoss: 0.6199460625648499 - trainLoss: 0.6256230473518372\n",
      "cnt: 0 - valLoss: 0.6199397444725037 - trainLoss: 0.6256178021430969\n",
      "cnt: 0 - valLoss: 0.6199341416358948 - trainLoss: 0.6256119608879089\n",
      "cnt: 0 - valLoss: 0.6199278831481934 - trainLoss: 0.6256066560745239\n",
      "cnt: 0 - valLoss: 0.6199222207069397 - trainLoss: 0.6256008148193359\n",
      "cnt: 0 - valLoss: 0.6199160218238831 - trainLoss: 0.6255953907966614\n",
      "cnt: 0 - valLoss: 0.6199103593826294 - trainLoss: 0.6255896091461182\n",
      "cnt: 0 - valLoss: 0.6199041604995728 - trainLoss: 0.6255842447280884\n",
      "cnt: 0 - valLoss: 0.6198984384536743 - trainLoss: 0.6255785226821899\n",
      "cnt: 0 - valLoss: 0.6198923587799072 - trainLoss: 0.6255729794502258\n",
      "cnt: 0 - valLoss: 0.6198865175247192 - trainLoss: 0.6255671977996826\n",
      "cnt: 0 - valLoss: 0.6198804378509521 - trainLoss: 0.6255617141723633\n",
      "cnt: 0 - valLoss: 0.6198745965957642 - trainLoss: 0.6255559921264648\n",
      "cnt: 0 - valLoss: 0.6198685169219971 - trainLoss: 0.6255504488945007\n",
      "cnt: 0 - valLoss: 0.6198626160621643 - trainLoss: 0.6255447268486023\n",
      "cnt: 0 - valLoss: 0.6198566555976868 - trainLoss: 0.6255391240119934\n",
      "cnt: 0 - valLoss: 0.6198506951332092 - trainLoss: 0.6255334615707397\n",
      "cnt: 0 - valLoss: 0.6198447942733765 - trainLoss: 0.6255278587341309\n",
      "cnt: 0 - valLoss: 0.6198387742042542 - trainLoss: 0.6255221962928772\n",
      "cnt: 0 - valLoss: 0.6198329329490662 - trainLoss: 0.6255165338516235\n",
      "cnt: 0 - valLoss: 0.6198267340660095 - trainLoss: 0.6255109906196594\n",
      "cnt: 0 - valLoss: 0.6198208332061768 - trainLoss: 0.6255053281784058\n",
      "cnt: 0 - valLoss: 0.6198148131370544 - trainLoss: 0.6255002617835999\n",
      "cnt: 0 - valLoss: 0.619809091091156 - trainLoss: 0.6254948973655701\n",
      "cnt: 0 - valLoss: 0.6198030114173889 - trainLoss: 0.6254897713661194\n",
      "cnt: 0 - valLoss: 0.6197972893714905 - trainLoss: 0.6254844069480896\n",
      "cnt: 0 - valLoss: 0.6197911500930786 - trainLoss: 0.6254794001579285\n",
      "cnt: 0 - valLoss: 0.6197855472564697 - trainLoss: 0.6254739165306091\n",
      "cnt: 0 - valLoss: 0.6197793483734131 - trainLoss: 0.625468909740448\n",
      "cnt: 0 - valLoss: 0.6197737455368042 - trainLoss: 0.6254634857177734\n",
      "cnt: 0 - valLoss: 0.6197675466537476 - trainLoss: 0.6254584789276123\n",
      "cnt: 0 - valLoss: 0.6197619438171387 - trainLoss: 0.625452995300293\n",
      "cnt: 0 - valLoss: 0.6197556853294373 - trainLoss: 0.6254479885101318\n",
      "cnt: 0 - valLoss: 0.6197502017021179 - trainLoss: 0.6254424452781677\n",
      "cnt: 0 - valLoss: 0.6197438836097717 - trainLoss: 0.6254374980926514\n",
      "cnt: 0 - valLoss: 0.6197383999824524 - trainLoss: 0.6254319548606873\n",
      "cnt: 0 - valLoss: 0.6197320222854614 - trainLoss: 0.6254270076751709\n",
      "cnt: 0 - valLoss: 0.6197266578674316 - trainLoss: 0.625421404838562\n",
      "cnt: 0 - valLoss: 0.6197202205657959 - trainLoss: 0.6254165172576904\n",
      "cnt: 0 - valLoss: 0.6197147369384766 - trainLoss: 0.6254109740257263\n",
      "cnt: 0 - valLoss: 0.6197084188461304 - trainLoss: 0.62540602684021\n",
      "cnt: 0 - valLoss: 0.6197028756141663 - trainLoss: 0.6254004240036011\n",
      "cnt: 0 - valLoss: 0.6196966171264648 - trainLoss: 0.6253955364227295\n",
      "cnt: 0 - valLoss: 0.6196910738945007 - trainLoss: 0.6253899931907654\n",
      "cnt: 0 - valLoss: 0.6196848154067993 - trainLoss: 0.6253849864006042\n",
      "cnt: 0 - valLoss: 0.6196792125701904 - trainLoss: 0.6253795027732849\n",
      "cnt: 0 - valLoss: 0.619672954082489 - trainLoss: 0.625374436378479\n",
      "cnt: 0 - valLoss: 0.6196672916412354 - trainLoss: 0.6253689527511597\n",
      "cnt: 0 - valLoss: 0.6196610331535339 - trainLoss: 0.6253638863563538\n",
      "cnt: 0 - valLoss: 0.6196552515029907 - trainLoss: 0.6253584623336792\n",
      "cnt: 0 - valLoss: 0.6196490526199341 - trainLoss: 0.6253533959388733\n",
      "cnt: 0 - valLoss: 0.6196432113647461 - trainLoss: 0.6253479719161987\n",
      "cnt: 0 - valLoss: 0.6196369528770447 - trainLoss: 0.6253429055213928\n",
      "cnt: 0 - valLoss: 0.6196311116218567 - trainLoss: 0.625337541103363\n",
      "cnt: 0 - valLoss: 0.6196248531341553 - trainLoss: 0.6253324151039124\n",
      "cnt: 0 - valLoss: 0.6196190118789673 - trainLoss: 0.6253269910812378\n",
      "cnt: 0 - valLoss: 0.6196128129959106 - trainLoss: 0.6253218650817871\n",
      "cnt: 0 - valLoss: 0.6196069121360779 - trainLoss: 0.6253165602684021\n",
      "cnt: 0 - valLoss: 0.619600772857666 - trainLoss: 0.6253114342689514\n",
      "cnt: 0 - valLoss: 0.6195948123931885 - trainLoss: 0.6253060698509216\n",
      "cnt: 0 - valLoss: 0.6195886731147766 - trainLoss: 0.6253008842468262\n",
      "cnt: 0 - valLoss: 0.6195819973945618 - trainLoss: 0.6252954006195068\n",
      "cnt: 0 - valLoss: 0.6195752620697021 - trainLoss: 0.625289261341095\n",
      "cnt: 0 - valLoss: 0.61956787109375 - trainLoss: 0.6252830624580383\n",
      "cnt: 0 - valLoss: 0.6195607781410217 - trainLoss: 0.6252766847610474\n",
      "cnt: 0 - valLoss: 0.6195539832115173 - trainLoss: 0.6252703070640564\n",
      "cnt: 0 - valLoss: 0.6195471882820129 - trainLoss: 0.6252636909484863\n",
      "cnt: 0 - valLoss: 0.6195403337478638 - trainLoss: 0.625257134437561\n",
      "cnt: 0 - valLoss: 0.6195335984230042 - trainLoss: 0.6252504587173462\n",
      "cnt: 0 - valLoss: 0.619526743888855 - trainLoss: 0.6252439022064209\n",
      "cnt: 0 - valLoss: 0.6195200085639954 - trainLoss: 0.625237226486206\n",
      "cnt: 0 - valLoss: 0.6195131540298462 - trainLoss: 0.6252306699752808\n",
      "cnt: 0 - valLoss: 0.6195064783096313 - trainLoss: 0.6252239346504211\n",
      "cnt: 0 - valLoss: 0.619499683380127 - trainLoss: 0.6252174973487854\n",
      "cnt: 0 - valLoss: 0.6194930076599121 - trainLoss: 0.6252110600471497\n",
      "cnt: 0 - valLoss: 0.6194862127304077 - trainLoss: 0.6252047419548035\n",
      "cnt: 0 - valLoss: 0.6194796562194824 - trainLoss: 0.6251983046531677\n",
      "cnt: 0 - valLoss: 0.6194728016853333 - trainLoss: 0.6251919865608215\n",
      "cnt: 0 - valLoss: 0.6194661855697632 - trainLoss: 0.625185489654541\n",
      "cnt: 0 - valLoss: 0.619459331035614 - trainLoss: 0.6251791715621948\n",
      "cnt: 0 - valLoss: 0.619452714920044 - trainLoss: 0.6251726746559143\n",
      "cnt: 0 - valLoss: 0.6194459199905396 - trainLoss: 0.6251663565635681\n",
      "cnt: 0 - valLoss: 0.6194393634796143 - trainLoss: 0.6251598000526428\n",
      "cnt: 0 - valLoss: 0.6194324493408203 - trainLoss: 0.6251536011695862\n",
      "cnt: 0 - valLoss: 0.6194258332252502 - trainLoss: 0.6251470446586609\n",
      "cnt: 0 - valLoss: 0.6194187998771667 - trainLoss: 0.6251407265663147\n",
      "cnt: 0 - valLoss: 0.6194120049476624 - trainLoss: 0.6251339912414551\n",
      "cnt: 0 - valLoss: 0.6194049119949341 - trainLoss: 0.6251275539398193\n",
      "cnt: 0 - valLoss: 0.6193981170654297 - trainLoss: 0.6251206994056702\n",
      "cnt: 0 - valLoss: 0.6193910241127014 - trainLoss: 0.6251142024993896\n",
      "cnt: 0 - valLoss: 0.619384229183197 - trainLoss: 0.6251074075698853\n",
      "cnt: 0 - valLoss: 0.6193771958351135 - trainLoss: 0.62510085105896\n",
      "cnt: 0 - valLoss: 0.6193703413009644 - trainLoss: 0.6250940561294556\n",
      "cnt: 0 - valLoss: 0.6193633079528809 - trainLoss: 0.6250874996185303\n",
      "cnt: 0 - valLoss: 0.6193564534187317 - trainLoss: 0.6250807046890259\n",
      "cnt: 0 - valLoss: 0.6193494200706482 - trainLoss: 0.6250741481781006\n",
      "cnt: 0 - valLoss: 0.619342565536499 - trainLoss: 0.6250673532485962\n",
      "cnt: 0 - valLoss: 0.6193357110023499 - trainLoss: 0.6250609755516052\n",
      "cnt: 0 - valLoss: 0.6193291544914246 - trainLoss: 0.6250545978546143\n",
      "cnt: 0 - valLoss: 0.6193223595619202 - trainLoss: 0.6250485777854919\n",
      "cnt: 0 - valLoss: 0.6193157434463501 - trainLoss: 0.6250423192977905\n",
      "cnt: 0 - valLoss: 0.6193090081214905 - trainLoss: 0.6250362396240234\n",
      "cnt: 0 - valLoss: 0.6193023324012756 - trainLoss: 0.625029981136322\n",
      "cnt: 0 - valLoss: 0.619295597076416 - trainLoss: 0.6250238418579102\n",
      "cnt: 0 - valLoss: 0.619288980960846 - trainLoss: 0.625017523765564\n",
      "cnt: 0 - valLoss: 0.6192821264266968 - trainLoss: 0.6250114440917969\n",
      "cnt: 0 - valLoss: 0.6192755103111267 - trainLoss: 0.6250051259994507\n",
      "cnt: 0 - valLoss: 0.6192689538002014 - trainLoss: 0.6249989867210388\n",
      "cnt: 0 - valLoss: 0.6192624568939209 - trainLoss: 0.6249927878379822\n",
      "cnt: 0 - valLoss: 0.6192560195922852 - trainLoss: 0.6249865889549255\n",
      "cnt: 0 - valLoss: 0.6192494630813599 - trainLoss: 0.6249800324440002\n",
      "cnt: 0 - valLoss: 0.6192429661750793 - trainLoss: 0.6249735355377197\n",
      "cnt: 0 - valLoss: 0.6192362904548645 - trainLoss: 0.6249670386314392\n",
      "cnt: 0 - valLoss: 0.6192296743392944 - trainLoss: 0.6249606013298035\n",
      "cnt: 0 - valLoss: 0.6192228198051453 - trainLoss: 0.6249540448188782\n",
      "cnt: 0 - valLoss: 0.6192162036895752 - trainLoss: 0.6249474287033081\n",
      "cnt: 0 - valLoss: 0.6192094087600708 - trainLoss: 0.6249409317970276\n",
      "cnt: 0 - valLoss: 0.6192027926445007 - trainLoss: 0.6249342560768127\n",
      "cnt: 0 - valLoss: 0.6191959381103516 - trainLoss: 0.6249277591705322\n",
      "cnt: 0 - valLoss: 0.6191893815994263 - trainLoss: 0.6249211430549622\n",
      "cnt: 0 - valLoss: 0.6191824674606323 - trainLoss: 0.6249146461486816\n",
      "cnt: 0 - valLoss: 0.6191759705543518 - trainLoss: 0.6249079704284668\n",
      "cnt: 0 - valLoss: 0.6191689372062683 - trainLoss: 0.624901533126831\n",
      "cnt: 0 - valLoss: 0.619162380695343 - trainLoss: 0.6248947381973267\n",
      "cnt: 0 - valLoss: 0.6191553473472595 - trainLoss: 0.6248883008956909\n",
      "cnt: 0 - valLoss: 0.6191489696502686 - trainLoss: 0.6248815059661865\n",
      "cnt: 0 - valLoss: 0.6191418170928955 - trainLoss: 0.6248753070831299\n",
      "cnt: 0 - valLoss: 0.6191355586051941 - trainLoss: 0.624868631362915\n",
      "cnt: 0 - valLoss: 0.6191283464431763 - trainLoss: 0.6248624920845032\n",
      "cnt: 0 - valLoss: 0.6191220879554749 - trainLoss: 0.6248558163642883\n",
      "cnt: 0 - valLoss: 0.6191149950027466 - trainLoss: 0.6248496770858765\n",
      "cnt: 0 - valLoss: 0.6191086173057556 - trainLoss: 0.6248429417610168\n",
      "cnt: 0 - valLoss: 0.6191016435623169 - trainLoss: 0.6248368620872498\n",
      "cnt: 0 - valLoss: 0.6190950870513916 - trainLoss: 0.6248301863670349\n",
      "cnt: 0 - valLoss: 0.6190884709358215 - trainLoss: 0.6248239874839783\n",
      "cnt: 0 - valLoss: 0.6190823316574097 - trainLoss: 0.624817967414856\n",
      "cnt: 0 - valLoss: 0.6190760135650635 - trainLoss: 0.6248123049736023\n",
      "cnt: 0 - valLoss: 0.6190698146820068 - trainLoss: 0.6248063445091248\n",
      "cnt: 0 - valLoss: 0.6190634369850159 - trainLoss: 0.6248006224632263\n",
      "cnt: 0 - valLoss: 0.6190571784973145 - trainLoss: 0.6247947216033936\n",
      "cnt: 0 - valLoss: 0.6190509796142578 - trainLoss: 0.6247888803482056\n",
      "cnt: 0 - valLoss: 0.6190444231033325 - trainLoss: 0.6247831583023071\n",
      "cnt: 0 - valLoss: 0.6190382838249207 - trainLoss: 0.6247773766517639\n",
      "cnt: 0 - valLoss: 0.6190318465232849 - trainLoss: 0.6247715950012207\n",
      "cnt: 0 - valLoss: 0.6190257668495178 - trainLoss: 0.6247656941413879\n",
      "cnt: 0 - valLoss: 0.6190192103385925 - trainLoss: 0.6247600317001343\n",
      "cnt: 0 - valLoss: 0.6190131902694702 - trainLoss: 0.6247540712356567\n",
      "cnt: 0 - valLoss: 0.6190065741539001 - trainLoss: 0.6247483491897583\n",
      "cnt: 0 - valLoss: 0.6190007925033569 - trainLoss: 0.624742329120636\n",
      "cnt: 0 - valLoss: 0.6189938187599182 - trainLoss: 0.6247367858886719\n",
      "cnt: 0 - valLoss: 0.618988037109375 - trainLoss: 0.6247307658195496\n",
      "cnt: 0 - valLoss: 0.6189812421798706 - trainLoss: 0.6247252821922302\n",
      "cnt: 0 - valLoss: 0.6189753413200378 - trainLoss: 0.6247191429138184\n",
      "cnt: 0 - valLoss: 0.6189687252044678 - trainLoss: 0.6247135996818542\n",
      "cnt: 0 - valLoss: 0.6189627647399902 - trainLoss: 0.6247075200080872\n",
      "cnt: 0 - valLoss: 0.6189562082290649 - trainLoss: 0.6247020363807678\n",
      "cnt: 0 - valLoss: 0.6189501285552979 - trainLoss: 0.6246961355209351\n",
      "cnt: 0 - valLoss: 0.6189436912536621 - trainLoss: 0.6246904730796814\n",
      "cnt: 0 - valLoss: 0.6189375519752502 - trainLoss: 0.6246846318244934\n",
      "cnt: 0 - valLoss: 0.6189309358596802 - trainLoss: 0.6246790885925293\n",
      "cnt: 0 - valLoss: 0.6189246773719788 - trainLoss: 0.6246732473373413\n",
      "cnt: 0 - valLoss: 0.6189184188842773 - trainLoss: 0.6246675848960876\n",
      "cnt: 0 - valLoss: 0.6189121603965759 - trainLoss: 0.6246618032455444\n",
      "cnt: 0 - valLoss: 0.6189060807228088 - trainLoss: 0.6246560215950012\n",
      "cnt: 0 - valLoss: 0.6188997030258179 - trainLoss: 0.6246502995491028\n",
      "cnt: 0 - valLoss: 0.6188936829566956 - trainLoss: 0.6246445775032043\n",
      "cnt: 0 - valLoss: 0.6188872456550598 - trainLoss: 0.6246388554573059\n",
      "cnt: 0 - valLoss: 0.6188812255859375 - trainLoss: 0.6246330142021179\n",
      "cnt: 0 - valLoss: 0.618874728679657 - trainLoss: 0.6246272921562195\n",
      "cnt: 0 - valLoss: 0.6188687682151794 - trainLoss: 0.6246213316917419\n",
      "cnt: 0 - valLoss: 0.6188622713088989 - trainLoss: 0.6246156692504883\n",
      "cnt: 0 - valLoss: 0.6188563704490662 - trainLoss: 0.6246095895767212\n",
      "cnt: 0 - valLoss: 0.618849515914917 - trainLoss: 0.6246041059494019\n",
      "cnt: 0 - valLoss: 0.6188437342643738 - trainLoss: 0.62459796667099\n",
      "cnt: 0 - valLoss: 0.6188369989395142 - trainLoss: 0.6245924234390259\n",
      "cnt: 0 - valLoss: 0.6188311576843262 - trainLoss: 0.624586284160614\n",
      "cnt: 0 - valLoss: 0.6188245415687561 - trainLoss: 0.6245807409286499\n",
      "cnt: 0 - valLoss: 0.6188186407089233 - trainLoss: 0.624574601650238\n",
      "cnt: 0 - valLoss: 0.6188121438026428 - trainLoss: 0.6245690584182739\n",
      "cnt: 0 - valLoss: 0.6188061237335205 - trainLoss: 0.6245629787445068\n",
      "cnt: 0 - valLoss: 0.61879962682724 - trainLoss: 0.6245574951171875\n",
      "cnt: 0 - valLoss: 0.6187935471534729 - trainLoss: 0.62455153465271\n",
      "cnt: 0 - valLoss: 0.6187871098518372 - trainLoss: 0.6245459318161011\n",
      "cnt: 0 - valLoss: 0.6187809109687805 - trainLoss: 0.6245400309562683\n",
      "cnt: 0 - valLoss: 0.6187745928764343 - trainLoss: 0.624534547328949\n",
      "cnt: 0 - valLoss: 0.6187680959701538 - trainLoss: 0.6245286464691162\n",
      "cnt: 0 - valLoss: 0.6187618374824524 - trainLoss: 0.6245230436325073\n",
      "cnt: 0 - valLoss: 0.618755578994751 - trainLoss: 0.6245172619819641\n",
      "cnt: 0 - valLoss: 0.6187494397163391 - trainLoss: 0.6245115399360657\n",
      "cnt: 0 - valLoss: 0.6187431216239929 - trainLoss: 0.6245058178901672\n",
      "cnt: 0 - valLoss: 0.6187370419502258 - trainLoss: 0.624500036239624\n",
      "cnt: 0 - valLoss: 0.6187307238578796 - trainLoss: 0.6244943141937256\n",
      "cnt: 0 - valLoss: 0.6187247037887573 - trainLoss: 0.6244884729385376\n",
      "cnt: 0 - valLoss: 0.6187183260917664 - trainLoss: 0.6244828701019287\n",
      "cnt: 0 - valLoss: 0.6187124252319336 - trainLoss: 0.6244770288467407\n",
      "cnt: 0 - valLoss: 0.6187059879302979 - trainLoss: 0.6244715452194214\n",
      "cnt: 0 - valLoss: 0.6187002062797546 - trainLoss: 0.6244656443595886\n",
      "cnt: 0 - valLoss: 0.6186935901641846 - trainLoss: 0.6244603395462036\n",
      "cnt: 0 - valLoss: 0.6186878085136414 - trainLoss: 0.6244543790817261\n",
      "cnt: 0 - valLoss: 0.6186812520027161 - trainLoss: 0.6244490146636963\n",
      "cnt: 0 - valLoss: 0.6186755895614624 - trainLoss: 0.6244430541992188\n",
      "cnt: 0 - valLoss: 0.6186689734458923 - trainLoss: 0.624437689781189\n",
      "cnt: 0 - valLoss: 0.6186632513999939 - trainLoss: 0.6244317889213562\n",
      "cnt: 0 - valLoss: 0.6186566948890686 - trainLoss: 0.6244264245033264\n",
      "cnt: 0 - valLoss: 0.6186507344245911 - trainLoss: 0.6244203448295593\n",
      "cnt: 0 - valLoss: 0.6186443567276001 - trainLoss: 0.62441486120224\n",
      "cnt: 0 - valLoss: 0.6186383962631226 - trainLoss: 0.624409019947052\n",
      "cnt: 0 - valLoss: 0.6186323165893555 - trainLoss: 0.6244035959243774\n",
      "cnt: 0 - valLoss: 0.6186265349388123 - trainLoss: 0.6243977546691895\n",
      "cnt: 0 - valLoss: 0.6186205148696899 - trainLoss: 0.6243923902511597\n",
      "cnt: 0 - valLoss: 0.6186147332191467 - trainLoss: 0.6243865489959717\n",
      "cnt: 0 - valLoss: 0.6186087727546692 - trainLoss: 0.6243810653686523\n",
      "cnt: 0 - valLoss: 0.6186028718948364 - trainLoss: 0.6243752837181091\n",
      "cnt: 0 - valLoss: 0.6185969710350037 - trainLoss: 0.6243696808815002\n",
      "cnt: 0 - valLoss: 0.6185910701751709 - trainLoss: 0.6243639588356018\n",
      "cnt: 0 - valLoss: 0.6185851693153381 - trainLoss: 0.6243582963943481\n",
      "cnt: 0 - valLoss: 0.6185791492462158 - trainLoss: 0.6243526339530945\n",
      "cnt: 0 - valLoss: 0.6185733079910278 - trainLoss: 0.624346911907196\n",
      "cnt: 0 - valLoss: 0.6185668110847473 - trainLoss: 0.6243413090705872\n",
      "cnt: 0 - valLoss: 0.6185610294342041 - trainLoss: 0.6243356466293335\n",
      "cnt: 0 - valLoss: 0.6185547709465027 - trainLoss: 0.6243300437927246\n",
      "cnt: 0 - valLoss: 0.6185490489006042 - trainLoss: 0.6243242025375366\n",
      "cnt: 0 - valLoss: 0.6185427904129028 - trainLoss: 0.6243186593055725\n",
      "cnt: 0 - valLoss: 0.6185371279716492 - trainLoss: 0.6243127584457397\n",
      "cnt: 0 - valLoss: 0.618530809879303 - trainLoss: 0.62430739402771\n",
      "cnt: 0 - valLoss: 0.6185251474380493 - trainLoss: 0.6243014931678772\n",
      "cnt: 0 - valLoss: 0.6185187697410583 - trainLoss: 0.6242960095405579\n",
      "cnt: 0 - valLoss: 0.6185131072998047 - trainLoss: 0.6242899894714355\n",
      "cnt: 0 - valLoss: 0.6185066103935242 - trainLoss: 0.6242845058441162\n",
      "cnt: 0 - valLoss: 0.6185011267662048 - trainLoss: 0.6242784261703491\n",
      "cnt: 0 - valLoss: 0.6184946298599243 - trainLoss: 0.6242729425430298\n",
      "cnt: 0 - valLoss: 0.6184890866279602 - trainLoss: 0.6242669224739075\n",
      "cnt: 0 - valLoss: 0.6184825897216797 - trainLoss: 0.6242614984512329\n",
      "cnt: 0 - valLoss: 0.618476927280426 - trainLoss: 0.624255359172821\n",
      "cnt: 0 - valLoss: 0.6184705495834351 - trainLoss: 0.6242498755455017\n",
      "cnt: 0 - valLoss: 0.6184647679328918 - trainLoss: 0.6242437958717346\n",
      "cnt: 0 - valLoss: 0.6184583306312561 - trainLoss: 0.6242383718490601\n",
      "cnt: 0 - valLoss: 0.6184526085853577 - trainLoss: 0.624232292175293\n",
      "cnt: 0 - valLoss: 0.6184462904930115 - trainLoss: 0.6242268085479736\n",
      "cnt: 0 - valLoss: 0.6184403896331787 - trainLoss: 0.6242208480834961\n",
      "cnt: 0 - valLoss: 0.6184341311454773 - trainLoss: 0.6242151856422424\n",
      "cnt: 0 - valLoss: 0.6184282302856445 - trainLoss: 0.6242092847824097\n",
      "cnt: 0 - valLoss: 0.6184220314025879 - trainLoss: 0.624203622341156\n",
      "cnt: 0 - valLoss: 0.6184160113334656 - trainLoss: 0.6241976618766785\n",
      "cnt: 0 - valLoss: 0.6184098124504089 - trainLoss: 0.6241919994354248\n",
      "cnt: 0 - valLoss: 0.6184038519859314 - trainLoss: 0.624186098575592\n",
      "cnt: 0 - valLoss: 0.6183976531028748 - trainLoss: 0.6241803765296936\n",
      "cnt: 0 - valLoss: 0.6183916330337524 - trainLoss: 0.6241745352745056\n",
      "cnt: 0 - valLoss: 0.6183855533599854 - trainLoss: 0.6241687536239624\n",
      "cnt: 0 - valLoss: 0.6183794140815735 - trainLoss: 0.6241629123687744\n",
      "cnt: 0 - valLoss: 0.6183733344078064 - trainLoss: 0.6241571307182312\n",
      "cnt: 0 - valLoss: 0.6183672547340393 - trainLoss: 0.624151349067688\n",
      "cnt: 0 - valLoss: 0.6183614134788513 - trainLoss: 0.6241455078125\n",
      "cnt: 0 - valLoss: 0.6183552742004395 - trainLoss: 0.6241397857666016\n",
      "cnt: 0 - valLoss: 0.618349552154541 - trainLoss: 0.6241340637207031\n",
      "cnt: 0 - valLoss: 0.6183434128761292 - trainLoss: 0.6241284012794495\n",
      "cnt: 0 - valLoss: 0.6183376312255859 - trainLoss: 0.6241226196289062\n",
      "cnt: 0 - valLoss: 0.6183314919471741 - trainLoss: 0.6241170763969421\n",
      "cnt: 0 - valLoss: 0.6183257699012756 - trainLoss: 0.6241111755371094\n",
      "cnt: 0 - valLoss: 0.6183195114135742 - trainLoss: 0.62410569190979\n",
      "cnt: 0 - valLoss: 0.6183139085769653 - trainLoss: 0.6240997910499573\n",
      "cnt: 0 - valLoss: 0.6183075904846191 - trainLoss: 0.6240943074226379\n",
      "cnt: 0 - valLoss: 0.6183019876480103 - trainLoss: 0.6240883469581604\n",
      "cnt: 0 - valLoss: 0.6182956695556641 - trainLoss: 0.6240829229354858\n",
      "cnt: 0 - valLoss: 0.6182901859283447 - trainLoss: 0.6240769624710083\n",
      "cnt: 0 - valLoss: 0.618283748626709 - trainLoss: 0.624071478843689\n",
      "cnt: 0 - valLoss: 0.6182782649993896 - trainLoss: 0.6240655779838562\n",
      "cnt: 0 - valLoss: 0.6182717084884644 - trainLoss: 0.6240600943565369\n",
      "cnt: 0 - valLoss: 0.6182663440704346 - trainLoss: 0.6240540742874146\n",
      "cnt: 0 - valLoss: 0.6182597875595093 - trainLoss: 0.6240487098693848\n",
      "cnt: 0 - valLoss: 0.6182543039321899 - trainLoss: 0.6240426301956177\n",
      "cnt: 0 - valLoss: 0.6182479858398438 - trainLoss: 0.6240373253822327\n",
      "cnt: 0 - valLoss: 0.6182424426078796 - trainLoss: 0.6240313053131104\n",
      "cnt: 0 - valLoss: 0.6182361245155334 - trainLoss: 0.6240260004997253\n",
      "cnt: 0 - valLoss: 0.6182305812835693 - trainLoss: 0.6240200400352478\n",
      "cnt: 0 - valLoss: 0.6182243227958679 - trainLoss: 0.6240147948265076\n",
      "cnt: 0 - valLoss: 0.6182187795639038 - trainLoss: 0.6240088939666748\n",
      "cnt: 0 - valLoss: 0.6182124018669128 - trainLoss: 0.6240036487579346\n",
      "cnt: 0 - valLoss: 0.6182067394256592 - trainLoss: 0.623997688293457\n",
      "cnt: 0 - valLoss: 0.6182004809379578 - trainLoss: 0.6239924430847168\n",
      "cnt: 0 - valLoss: 0.6181948184967041 - trainLoss: 0.6239866018295288\n",
      "cnt: 0 - valLoss: 0.6181885600090027 - trainLoss: 0.6239812970161438\n",
      "cnt: 0 - valLoss: 0.6181829571723938 - trainLoss: 0.6239754557609558\n",
      "cnt: 0 - valLoss: 0.6181766390800476 - trainLoss: 0.6239701509475708\n",
      "cnt: 0 - valLoss: 0.6181710958480835 - trainLoss: 0.6239643096923828\n",
      "cnt: 0 - valLoss: 0.6181648969650269 - trainLoss: 0.6239591240882874\n",
      "cnt: 0 - valLoss: 0.6181594133377075 - trainLoss: 0.6239535808563232\n",
      "cnt: 0 - valLoss: 0.6181532144546509 - trainLoss: 0.6239485144615173\n",
      "cnt: 0 - valLoss: 0.6181476712226868 - trainLoss: 0.6239429712295532\n",
      "cnt: 0 - valLoss: 0.6181415319442749 - trainLoss: 0.6239379644393921\n",
      "cnt: 0 - valLoss: 0.6181360483169556 - trainLoss: 0.6239323616027832\n",
      "cnt: 0 - valLoss: 0.6181299090385437 - trainLoss: 0.6239274144172668\n",
      "cnt: 0 - valLoss: 0.6181243658065796 - trainLoss: 0.623921811580658\n",
      "cnt: 0 - valLoss: 0.6181182265281677 - trainLoss: 0.6239168047904968\n",
      "cnt: 0 - valLoss: 0.6181126236915588 - trainLoss: 0.6239112019538879\n",
      "cnt: 0 - valLoss: 0.618106484413147 - trainLoss: 0.623906135559082\n",
      "cnt: 0 - valLoss: 0.6181009411811829 - trainLoss: 0.6239006519317627\n",
      "cnt: 0 - valLoss: 0.618094801902771 - trainLoss: 0.6238956451416016\n",
      "cnt: 0 - valLoss: 0.6180891990661621 - trainLoss: 0.6238899827003479\n",
      "cnt: 0 - valLoss: 0.6180830597877502 - trainLoss: 0.6238849759101868\n",
      "cnt: 0 - valLoss: 0.6180775761604309 - trainLoss: 0.6238794326782227\n",
      "cnt: 0 - valLoss: 0.6180713772773743 - trainLoss: 0.6238744258880615\n",
      "cnt: 0 - valLoss: 0.6180658340454102 - trainLoss: 0.6238688230514526\n",
      "cnt: 0 - valLoss: 0.6180596351623535 - trainLoss: 0.6238637566566467\n",
      "cnt: 0 - valLoss: 0.6180540919303894 - trainLoss: 0.6238581538200378\n",
      "cnt: 0 - valLoss: 0.6180480122566223 - trainLoss: 0.6238531470298767\n",
      "cnt: 0 - valLoss: 0.6180423498153687 - trainLoss: 0.6238476037979126\n",
      "cnt: 0 - valLoss: 0.6180362701416016 - trainLoss: 0.6238424777984619\n",
      "cnt: 0 - valLoss: 0.6180306673049927 - trainLoss: 0.6238369345664978\n",
      "cnt: 0 - valLoss: 0.6180245280265808 - trainLoss: 0.6238318085670471\n",
      "cnt: 0 - valLoss: 0.6180189251899719 - trainLoss: 0.623826265335083\n",
      "cnt: 0 - valLoss: 0.6180127859115601 - trainLoss: 0.6238212585449219\n",
      "cnt: 0 - valLoss: 0.6180071830749512 - trainLoss: 0.623815655708313\n",
      "cnt: 0 - valLoss: 0.6180010437965393 - trainLoss: 0.6238105893135071\n",
      "cnt: 0 - valLoss: 0.6179953813552856 - trainLoss: 0.623805046081543\n",
      "cnt: 0 - valLoss: 0.6179893016815186 - trainLoss: 0.6237999200820923\n",
      "cnt: 0 - valLoss: 0.6179836392402649 - trainLoss: 0.6237943768501282\n",
      "cnt: 0 - valLoss: 0.617977499961853 - trainLoss: 0.6237892508506775\n",
      "cnt: 0 - valLoss: 0.6179718375205994 - trainLoss: 0.6237836480140686\n",
      "cnt: 0 - valLoss: 0.6179656386375427 - trainLoss: 0.6237785816192627\n",
      "cnt: 0 - valLoss: 0.6179599761962891 - trainLoss: 0.6237729787826538\n",
      "cnt: 0 - valLoss: 0.617953896522522 - trainLoss: 0.6237678527832031\n",
      "cnt: 0 - valLoss: 0.6179481744766235 - trainLoss: 0.6237622499465942\n",
      "cnt: 0 - valLoss: 0.6179420351982117 - trainLoss: 0.6237571239471436\n",
      "cnt: 0 - valLoss: 0.6179363131523132 - trainLoss: 0.6237515807151794\n",
      "cnt: 0 - valLoss: 0.6179302334785461 - trainLoss: 0.6237464547157288\n",
      "cnt: 0 - valLoss: 0.6179245114326477 - trainLoss: 0.6237409710884094\n",
      "cnt: 0 - valLoss: 0.6179183721542358 - trainLoss: 0.623735785484314\n",
      "cnt: 0 - valLoss: 0.6179127097129822 - trainLoss: 0.6237303614616394\n",
      "cnt: 0 - valLoss: 0.6179065704345703 - trainLoss: 0.623725175857544\n",
      "cnt: 0 - valLoss: 0.6179009079933167 - trainLoss: 0.6237196326255798\n",
      "cnt: 0 - valLoss: 0.61789470911026 - trainLoss: 0.6237145662307739\n",
      "cnt: 0 - valLoss: 0.6178890466690063 - trainLoss: 0.6237090229988098\n",
      "cnt: 0 - valLoss: 0.6178829073905945 - trainLoss: 0.6237038969993591\n",
      "cnt: 0 - valLoss: 0.6178772449493408 - trainLoss: 0.6236984133720398\n",
      "cnt: 0 - valLoss: 0.6178711652755737 - trainLoss: 0.6236932873725891\n",
      "cnt: 0 - valLoss: 0.6178653836250305 - trainLoss: 0.6236876845359802\n",
      "cnt: 0 - valLoss: 0.6178592443466187 - trainLoss: 0.6236826181411743\n",
      "cnt: 0 - valLoss: 0.617853581905365 - trainLoss: 0.6236770749092102\n",
      "cnt: 0 - valLoss: 0.6178473830223083 - trainLoss: 0.6236718893051147\n",
      "cnt: 0 - valLoss: 0.6178417801856995 - trainLoss: 0.6236664056777954\n",
      "cnt: 0 - valLoss: 0.6178356409072876 - trainLoss: 0.6236613392829895\n",
      "cnt: 0 - valLoss: 0.6178300380706787 - trainLoss: 0.6236557960510254\n",
      "cnt: 0 - valLoss: 0.6178238987922668 - trainLoss: 0.6236507892608643\n",
      "cnt: 0 - valLoss: 0.6178183555603027 - trainLoss: 0.6236453056335449\n",
      "cnt: 0 - valLoss: 0.6178120970726013 - trainLoss: 0.6236402988433838\n",
      "cnt: 0 - valLoss: 0.617806613445282 - trainLoss: 0.6236346960067749\n",
      "cnt: 0 - valLoss: 0.6178003549575806 - trainLoss: 0.6236297488212585\n",
      "cnt: 0 - valLoss: 0.617794930934906 - trainLoss: 0.6236242055892944\n",
      "cnt: 0 - valLoss: 0.6177886128425598 - trainLoss: 0.6236191987991333\n",
      "cnt: 0 - valLoss: 0.6177831888198853 - trainLoss: 0.6236135959625244\n",
      "cnt: 0 - valLoss: 0.6177768707275391 - trainLoss: 0.6236087083816528\n",
      "cnt: 0 - valLoss: 0.6177715063095093 - trainLoss: 0.6236030459403992\n",
      "cnt: 0 - valLoss: 0.6177650094032288 - trainLoss: 0.6235980987548828\n",
      "cnt: 0 - valLoss: 0.617759644985199 - trainLoss: 0.6235923767089844\n",
      "cnt: 0 - valLoss: 0.6177532076835632 - trainLoss: 0.6235873699188232\n",
      "cnt: 0 - valLoss: 0.6177477240562439 - trainLoss: 0.6235817074775696\n",
      "cnt: 0 - valLoss: 0.6177414059638977 - trainLoss: 0.6235767006874084\n",
      "cnt: 0 - valLoss: 0.6177358627319336 - trainLoss: 0.6235710382461548\n",
      "cnt: 0 - valLoss: 0.6177295446395874 - trainLoss: 0.6235659718513489\n",
      "cnt: 0 - valLoss: 0.6177239418029785 - trainLoss: 0.6235603094100952\n",
      "cnt: 0 - valLoss: 0.6177176833152771 - trainLoss: 0.6235553026199341\n",
      "cnt: 0 - valLoss: 0.617712140083313 - trainLoss: 0.6235496997833252\n",
      "cnt: 0 - valLoss: 0.6177058219909668 - trainLoss: 0.6235446333885193\n",
      "cnt: 0 - valLoss: 0.6177002787590027 - trainLoss: 0.6235390901565552\n",
      "cnt: 0 - valLoss: 0.6176940202713013 - trainLoss: 0.6235338449478149\n",
      "cnt: 0 - valLoss: 0.6176883578300476 - trainLoss: 0.6235284209251404\n",
      "cnt: 0 - valLoss: 0.6176820993423462 - trainLoss: 0.6235231757164001\n",
      "cnt: 0 - valLoss: 0.6176764965057373 - trainLoss: 0.6235176920890808\n",
      "cnt: 0 - valLoss: 0.6176702380180359 - trainLoss: 0.6235125064849854\n",
      "cnt: 0 - valLoss: 0.6176645755767822 - trainLoss: 0.6235069632530212\n",
      "cnt: 0 - valLoss: 0.6176583766937256 - trainLoss: 0.6235018372535706\n",
      "cnt: 0 - valLoss: 0.6176526546478271 - trainLoss: 0.6234962940216064\n",
      "cnt: 0 - valLoss: 0.6176465749740601 - trainLoss: 0.6234910488128662\n",
      "cnt: 0 - valLoss: 0.6176407337188721 - trainLoss: 0.6234856247901917\n",
      "cnt: 0 - valLoss: 0.617634654045105 - trainLoss: 0.6234803795814514\n",
      "cnt: 0 - valLoss: 0.617628812789917 - trainLoss: 0.6234749555587769\n",
      "cnt: 0 - valLoss: 0.6176227927207947 - trainLoss: 0.6234695911407471\n",
      "cnt: 0 - valLoss: 0.6176168918609619 - trainLoss: 0.6234642267227173\n",
      "cnt: 0 - valLoss: 0.6176108717918396 - trainLoss: 0.6234588623046875\n",
      "cnt: 0 - valLoss: 0.6176049709320068 - trainLoss: 0.6234535574913025\n",
      "cnt: 0 - valLoss: 0.6175990700721741 - trainLoss: 0.6234481930732727\n",
      "cnt: 0 - valLoss: 0.617592990398407 - trainLoss: 0.6234428286552429\n",
      "cnt: 0 - valLoss: 0.6175869703292847 - trainLoss: 0.6234372854232788\n",
      "cnt: 0 - valLoss: 0.6175809502601624 - trainLoss: 0.623431921005249\n",
      "cnt: 0 - valLoss: 0.6175749897956848 - trainLoss: 0.6234265565872192\n",
      "cnt: 0 - valLoss: 0.6175689697265625 - trainLoss: 0.6234211921691895\n",
      "cnt: 0 - valLoss: 0.6175630688667297 - trainLoss: 0.6234157681465149\n",
      "cnt: 0 - valLoss: 0.6175569891929626 - trainLoss: 0.6234104037284851\n",
      "cnt: 0 - valLoss: 0.6175510287284851 - trainLoss: 0.6234049797058105\n",
      "cnt: 0 - valLoss: 0.617544949054718 - trainLoss: 0.6233996748924255\n",
      "cnt: 0 - valLoss: 0.6175390481948853 - trainLoss: 0.6233941316604614\n",
      "cnt: 0 - valLoss: 0.6175329089164734 - trainLoss: 0.6233888864517212\n",
      "cnt: 0 - valLoss: 0.6175270676612854 - trainLoss: 0.6233832836151123\n",
      "cnt: 0 - valLoss: 0.6175208687782288 - trainLoss: 0.6233780980110168\n",
      "cnt: 0 - valLoss: 0.6175150275230408 - trainLoss: 0.623372495174408\n",
      "cnt: 0 - valLoss: 0.6175088286399841 - trainLoss: 0.623367190361023\n",
      "cnt: 0 - valLoss: 0.6175030469894409 - trainLoss: 0.6233617067337036\n",
      "cnt: 0 - valLoss: 0.6174967885017395 - trainLoss: 0.6233565211296082\n",
      "cnt: 0 - valLoss: 0.6174910664558411 - trainLoss: 0.6233508586883545\n",
      "cnt: 0 - valLoss: 0.6174848079681396 - trainLoss: 0.623345673084259\n",
      "cnt: 0 - valLoss: 0.6174790263175964 - trainLoss: 0.6233400702476501\n",
      "cnt: 0 - valLoss: 0.6174727082252502 - trainLoss: 0.6233348846435547\n",
      "cnt: 0 - valLoss: 0.6174669861793518 - trainLoss: 0.6233292818069458\n",
      "cnt: 0 - valLoss: 0.6174606680870056 - trainLoss: 0.6233240365982056\n",
      "cnt: 0 - valLoss: 0.617455005645752 - trainLoss: 0.6233184337615967\n",
      "cnt: 0 - valLoss: 0.617448627948761 - trainLoss: 0.6233132481575012\n",
      "cnt: 0 - valLoss: 0.6174429655075073 - trainLoss: 0.6233075261116028\n",
      "cnt: 0 - valLoss: 0.6174365878105164 - trainLoss: 0.6233024597167969\n",
      "cnt: 0 - valLoss: 0.6174309253692627 - trainLoss: 0.6232967376708984\n",
      "cnt: 0 - valLoss: 0.6174245476722717 - trainLoss: 0.6232916116714478\n",
      "cnt: 0 - valLoss: 0.6174188852310181 - trainLoss: 0.6232858896255493\n",
      "cnt: 0 - valLoss: 0.6174125075340271 - trainLoss: 0.6232808232307434\n",
      "cnt: 0 - valLoss: 0.6174068450927734 - trainLoss: 0.623275101184845\n",
      "cnt: 0 - valLoss: 0.6174004077911377 - trainLoss: 0.6232699751853943\n",
      "cnt: 0 - valLoss: 0.6173948645591736 - trainLoss: 0.6232641935348511\n",
      "cnt: 0 - valLoss: 0.6173884868621826 - trainLoss: 0.6232591867446899\n",
      "cnt: 0 - valLoss: 0.6173830032348633 - trainLoss: 0.6232534646987915\n",
      "cnt: 0 - valLoss: 0.617376446723938 - trainLoss: 0.6232483983039856\n",
      "cnt: 0 - valLoss: 0.6173709630966187 - trainLoss: 0.6232425570487976\n",
      "cnt: 0 - valLoss: 0.6173644065856934 - trainLoss: 0.6232376098632812\n",
      "cnt: 0 - valLoss: 0.617358922958374 - trainLoss: 0.623231828212738\n",
      "cnt: 0 - valLoss: 0.6173524260520935 - trainLoss: 0.6232267022132874\n",
      "cnt: 0 - valLoss: 0.6173469424247742 - trainLoss: 0.6232209801673889\n",
      "cnt: 0 - valLoss: 0.6173404455184937 - trainLoss: 0.6232158541679382\n",
      "cnt: 0 - valLoss: 0.6173349618911743 - trainLoss: 0.6232101321220398\n",
      "cnt: 0 - valLoss: 0.617328405380249 - trainLoss: 0.6232050061225891\n",
      "cnt: 0 - valLoss: 0.6173228621482849 - trainLoss: 0.6231992840766907\n",
      "cnt: 0 - valLoss: 0.617316484451294 - trainLoss: 0.6231942176818848\n",
      "cnt: 0 - valLoss: 0.6173108220100403 - trainLoss: 0.6231884360313416\n",
      "cnt: 0 - valLoss: 0.6173044443130493 - trainLoss: 0.6231833100318909\n",
      "cnt: 0 - valLoss: 0.6172988414764404 - trainLoss: 0.6231776475906372\n",
      "cnt: 0 - valLoss: 0.6172924041748047 - trainLoss: 0.6231724619865417\n",
      "cnt: 0 - valLoss: 0.617286741733551 - trainLoss: 0.6231667995452881\n",
      "cnt: 0 - valLoss: 0.6172803640365601 - trainLoss: 0.6231615543365479\n",
      "cnt: 0 - valLoss: 0.6172747015953064 - trainLoss: 0.6231558918952942\n",
      "cnt: 0 - valLoss: 0.6172683238983154 - trainLoss: 0.623150646686554\n",
      "cnt: 0 - valLoss: 0.617262601852417 - trainLoss: 0.6231449842453003\n",
      "cnt: 0 - valLoss: 0.6172563433647156 - trainLoss: 0.6231397986412048\n",
      "cnt: 0 - valLoss: 0.6172506213188171 - trainLoss: 0.6231340765953064\n",
      "cnt: 0 - valLoss: 0.617244303226471 - trainLoss: 0.6231288909912109\n",
      "cnt: 0 - valLoss: 0.6172387003898621 - trainLoss: 0.6231232285499573\n",
      "cnt: 0 - valLoss: 0.6172323822975159 - trainLoss: 0.6231179237365723\n",
      "cnt: 0 - valLoss: 0.6172266602516174 - trainLoss: 0.6231123208999634\n",
      "cnt: 0 - valLoss: 0.6172204613685608 - trainLoss: 0.6231070160865784\n",
      "cnt: 0 - valLoss: 0.6172147393226624 - trainLoss: 0.6231014132499695\n",
      "cnt: 0 - valLoss: 0.6172085404396057 - trainLoss: 0.6230961084365845\n",
      "cnt: 0 - valLoss: 0.6172028183937073 - trainLoss: 0.6230905055999756\n",
      "cnt: 0 - valLoss: 0.6171966791152954 - trainLoss: 0.6230852007865906\n",
      "cnt: 0 - valLoss: 0.6171910166740417 - trainLoss: 0.6230795979499817\n",
      "cnt: 0 - valLoss: 0.6171848177909851 - trainLoss: 0.6230742931365967\n",
      "cnt: 0 - valLoss: 0.6171790361404419 - trainLoss: 0.6230687499046326\n",
      "cnt: 0 - valLoss: 0.6171729564666748 - trainLoss: 0.6230633854866028\n",
      "cnt: 0 - valLoss: 0.6171672344207764 - trainLoss: 0.6230579018592834\n",
      "cnt: 0 - valLoss: 0.6171612739562988 - trainLoss: 0.6230525374412537\n",
      "cnt: 0 - valLoss: 0.6171556115150452 - trainLoss: 0.6230469346046448\n",
      "cnt: 0 - valLoss: 0.6171495914459229 - trainLoss: 0.6230416297912598\n",
      "cnt: 0 - valLoss: 0.6171438694000244 - trainLoss: 0.6230360865592957\n",
      "cnt: 0 - valLoss: 0.6171379685401917 - trainLoss: 0.6230307221412659\n",
      "cnt: 0 - valLoss: 0.617132306098938 - trainLoss: 0.6230252385139465\n",
      "cnt: 0 - valLoss: 0.6171262264251709 - trainLoss: 0.623019814491272\n",
      "cnt: 0 - valLoss: 0.6171205639839172 - trainLoss: 0.6230143308639526\n",
      "cnt: 0 - valLoss: 0.6171147227287292 - trainLoss: 0.6230089664459229\n",
      "cnt: 0 - valLoss: 0.6171088814735413 - trainLoss: 0.6230034232139587\n",
      "cnt: 0 - valLoss: 0.6171030402183533 - trainLoss: 0.6229979991912842\n",
      "cnt: 0 - valLoss: 0.6170972585678101 - trainLoss: 0.6229925751686096\n",
      "cnt: 0 - valLoss: 0.6170912981033325 - trainLoss: 0.6229870915412903\n",
      "cnt: 0 - valLoss: 0.6170856356620789 - trainLoss: 0.6229816675186157\n",
      "cnt: 0 - valLoss: 0.6170797348022461 - trainLoss: 0.6229762434959412\n",
      "cnt: 0 - valLoss: 0.6170738935470581 - trainLoss: 0.6229707598686218\n",
      "cnt: 0 - valLoss: 0.6170681118965149 - trainLoss: 0.6229652762413025\n",
      "cnt: 0 - valLoss: 0.6170621514320374 - trainLoss: 0.6229599118232727\n",
      "cnt: 0 - valLoss: 0.6170563697814941 - trainLoss: 0.6229544281959534\n",
      "cnt: 0 - valLoss: 0.6170505881309509 - trainLoss: 0.622948944568634\n",
      "cnt: 0 - valLoss: 0.6170446872711182 - trainLoss: 0.6229434609413147\n",
      "cnt: 0 - valLoss: 0.6170387864112854 - trainLoss: 0.6229380965232849\n",
      "cnt: 0 - valLoss: 0.6170331239700317 - trainLoss: 0.6229326128959656\n",
      "cnt: 0 - valLoss: 0.6170271039009094 - trainLoss: 0.6229271292686462\n",
      "cnt: 0 - valLoss: 0.6170212626457214 - trainLoss: 0.6229216456413269\n",
      "cnt: 0 - valLoss: 0.6170154809951782 - trainLoss: 0.6229162216186523\n",
      "cnt: 0 - valLoss: 0.6170096397399902 - trainLoss: 0.622910737991333\n",
      "cnt: 0 - valLoss: 0.6170037388801575 - trainLoss: 0.6229053735733032\n",
      "cnt: 0 - valLoss: 0.6169980764389038 - trainLoss: 0.6228998303413391\n",
      "cnt: 0 - valLoss: 0.6169919967651367 - trainLoss: 0.6228944659233093\n",
      "cnt: 0 - valLoss: 0.6169863939285278 - trainLoss: 0.6228888630867004\n",
      "cnt: 0 - valLoss: 0.6169803142547607 - trainLoss: 0.6228835582733154\n",
      "cnt: 0 - valLoss: 0.6169746518135071 - trainLoss: 0.6228779554367065\n",
      "cnt: 0 - valLoss: 0.6169686913490295 - trainLoss: 0.6228725910186768\n",
      "cnt: 0 - valLoss: 0.6169629693031311 - trainLoss: 0.6228670477867126\n",
      "cnt: 0 - valLoss: 0.6169569492340088 - trainLoss: 0.6228617429733276\n",
      "cnt: 0 - valLoss: 0.6169513463973999 - trainLoss: 0.622856080532074\n",
      "cnt: 0 - valLoss: 0.6169452667236328 - trainLoss: 0.622850775718689\n",
      "cnt: 0 - valLoss: 0.6169396042823792 - trainLoss: 0.6228451132774353\n",
      "cnt: 0 - valLoss: 0.6169335246086121 - trainLoss: 0.6228398084640503\n",
      "cnt: 0 - valLoss: 0.6169278621673584 - trainLoss: 0.6228341460227966\n",
      "cnt: 0 - valLoss: 0.6169219017028809 - trainLoss: 0.6228289008140564\n",
      "cnt: 0 - valLoss: 0.616916298866272 - trainLoss: 0.6228232383728027\n",
      "cnt: 0 - valLoss: 0.6169100999832153 - trainLoss: 0.622817873954773\n",
      "cnt: 0 - valLoss: 0.6169044971466064 - trainLoss: 0.6228122115135193\n",
      "cnt: 0 - valLoss: 0.6168983578681946 - trainLoss: 0.6228070259094238\n",
      "cnt: 0 - valLoss: 0.6168927550315857 - trainLoss: 0.6228013038635254\n",
      "cnt: 0 - valLoss: 0.6168867945671082 - trainLoss: 0.6227959990501404\n",
      "cnt: 0 - valLoss: 0.6168810725212097 - trainLoss: 0.6227903366088867\n",
      "cnt: 0 - valLoss: 0.6168749332427979 - trainLoss: 0.6227850914001465\n",
      "cnt: 0 - valLoss: 0.6168694496154785 - trainLoss: 0.6227794289588928\n",
      "cnt: 0 - valLoss: 0.6168633103370667 - trainLoss: 0.6227742433547974\n",
      "cnt: 0 - valLoss: 0.6168577075004578 - trainLoss: 0.6227685213088989\n",
      "cnt: 0 - valLoss: 0.6168516278266907 - trainLoss: 0.6227632761001587\n",
      "cnt: 0 - valLoss: 0.6168460249900818 - trainLoss: 0.6227575540542603\n",
      "cnt: 0 - valLoss: 0.6168397665023804 - trainLoss: 0.62275230884552\n",
      "cnt: 0 - valLoss: 0.6168343424797058 - trainLoss: 0.6227465867996216\n",
      "cnt: 0 - valLoss: 0.6168280839920044 - trainLoss: 0.6227413415908813\n",
      "cnt: 0 - valLoss: 0.6168226003646851 - trainLoss: 0.6227356195449829\n",
      "cnt: 0 - valLoss: 0.6168164014816284 - trainLoss: 0.6227303743362427\n",
      "cnt: 0 - valLoss: 0.6168108582496643 - trainLoss: 0.6227246522903442\n",
      "cnt: 0 - valLoss: 0.6168045997619629 - trainLoss: 0.6227194666862488\n",
      "cnt: 0 - valLoss: 0.6167992353439331 - trainLoss: 0.6227136850357056\n",
      "cnt: 0 - valLoss: 0.6167928576469421 - trainLoss: 0.6227085590362549\n",
      "cnt: 0 - valLoss: 0.6167874336242676 - trainLoss: 0.6227026581764221\n",
      "cnt: 0 - valLoss: 0.6167811751365662 - trainLoss: 0.6226975917816162\n",
      "cnt: 0 - valLoss: 0.6167756915092468 - trainLoss: 0.6226916909217834\n",
      "cnt: 0 - valLoss: 0.6167694926261902 - trainLoss: 0.6226866245269775\n",
      "cnt: 0 - valLoss: 0.6167640089988708 - trainLoss: 0.6226807236671448\n",
      "cnt: 0 - valLoss: 0.6167576313018799 - trainLoss: 0.6226755976676941\n",
      "cnt: 0 - valLoss: 0.6167523860931396 - trainLoss: 0.6226697564125061\n",
      "cnt: 0 - valLoss: 0.6167458891868591 - trainLoss: 0.6226646304130554\n",
      "cnt: 0 - valLoss: 0.6167405843734741 - trainLoss: 0.6226587295532227\n",
      "cnt: 0 - valLoss: 0.6167342662811279 - trainLoss: 0.622653603553772\n",
      "cnt: 0 - valLoss: 0.6167288422584534 - trainLoss: 0.6226477026939392\n",
      "cnt: 0 - valLoss: 0.6167225241661072 - trainLoss: 0.6226425766944885\n",
      "cnt: 0 - valLoss: 0.6167170405387878 - trainLoss: 0.6226367354393005\n",
      "cnt: 0 - valLoss: 0.6167107224464417 - trainLoss: 0.6226315498352051\n",
      "cnt: 0 - valLoss: 0.6167054176330566 - trainLoss: 0.6226257085800171\n",
      "cnt: 0 - valLoss: 0.6166989803314209 - trainLoss: 0.6226205229759216\n",
      "cnt: 0 - valLoss: 0.6166935563087463 - trainLoss: 0.6226146817207336\n",
      "cnt: 0 - valLoss: 0.6166872978210449 - trainLoss: 0.622609555721283\n",
      "cnt: 0 - valLoss: 0.6166818141937256 - trainLoss: 0.6226036548614502\n",
      "cnt: 0 - valLoss: 0.6166755557060242 - trainLoss: 0.6225984692573547\n",
      "cnt: 0 - valLoss: 0.6166700124740601 - trainLoss: 0.6225926876068115\n",
      "cnt: 0 - valLoss: 0.6166638135910034 - trainLoss: 0.6225873827934265\n",
      "cnt: 0 - valLoss: 0.6166583299636841 - trainLoss: 0.6225816011428833\n",
      "cnt: 0 - valLoss: 0.6166520714759827 - trainLoss: 0.6225764155387878\n",
      "cnt: 0 - valLoss: 0.6166466474533081 - trainLoss: 0.6225706338882446\n",
      "cnt: 0 - valLoss: 0.6166403889656067 - trainLoss: 0.6225654482841492\n",
      "cnt: 0 - valLoss: 0.6166348457336426 - trainLoss: 0.6225597858428955\n",
      "cnt: 0 - valLoss: 0.6166286468505859 - trainLoss: 0.6225545406341553\n",
      "cnt: 0 - valLoss: 0.6166232824325562 - trainLoss: 0.6225489377975464\n",
      "cnt: 0 - valLoss: 0.6166169047355652 - trainLoss: 0.6225436925888062\n",
      "cnt: 0 - valLoss: 0.6166114211082458 - trainLoss: 0.6225380301475525\n",
      "cnt: 0 - valLoss: 0.6166053414344788 - trainLoss: 0.622532844543457\n",
      "cnt: 0 - valLoss: 0.6165996789932251 - trainLoss: 0.6225271821022034\n",
      "cnt: 0 - valLoss: 0.6165935397148132 - trainLoss: 0.6225218772888184\n",
      "cnt: 0 - valLoss: 0.6165880560874939 - trainLoss: 0.6225162744522095\n",
      "cnt: 0 - valLoss: 0.6165817975997925 - trainLoss: 0.6225109696388245\n",
      "cnt: 0 - valLoss: 0.6165761947631836 - trainLoss: 0.6225053668022156\n",
      "cnt: 0 - valLoss: 0.616570234298706 - trainLoss: 0.6225001215934753\n",
      "cnt: 0 - valLoss: 0.6165645718574524 - trainLoss: 0.6224945783615112\n",
      "cnt: 0 - valLoss: 0.6165583729743958 - trainLoss: 0.6224892735481262\n",
      "cnt: 0 - valLoss: 0.6165528893470764 - trainLoss: 0.6224836111068726\n",
      "cnt: 0 - valLoss: 0.6165466904640198 - trainLoss: 0.6224783658981323\n",
      "cnt: 0 - valLoss: 0.6165410280227661 - trainLoss: 0.6224727630615234\n",
      "cnt: 0 - valLoss: 0.6165350675582886 - trainLoss: 0.6224673986434937\n",
      "cnt: 0 - valLoss: 0.6165293455123901 - trainLoss: 0.6224619150161743\n",
      "cnt: 0 - valLoss: 0.616523265838623 - trainLoss: 0.6224566698074341\n",
      "cnt: 0 - valLoss: 0.6165179014205933 - trainLoss: 0.6224510073661804\n",
      "cnt: 0 - valLoss: 0.6165116429328918 - trainLoss: 0.6224458813667297\n",
      "cnt: 0 - valLoss: 0.616506040096283 - trainLoss: 0.6224402785301208\n",
      "cnt: 0 - valLoss: 0.6165000200271606 - trainLoss: 0.6224349737167358\n",
      "cnt: 0 - valLoss: 0.616494357585907 - trainLoss: 0.6224294900894165\n",
      "cnt: 0 - valLoss: 0.6164882779121399 - trainLoss: 0.6224241256713867\n",
      "cnt: 0 - valLoss: 0.6164827942848206 - trainLoss: 0.6224185228347778\n",
      "cnt: 0 - valLoss: 0.6164765357971191 - trainLoss: 0.6224132180213928\n",
      "cnt: 0 - valLoss: 0.6164708733558655 - trainLoss: 0.6224076747894287\n",
      "cnt: 0 - valLoss: 0.6164649128913879 - trainLoss: 0.6224023103713989\n",
      "cnt: 0 - valLoss: 0.6164591908454895 - trainLoss: 0.6223968267440796\n",
      "cnt: 0 - valLoss: 0.6164531111717224 - trainLoss: 0.622391402721405\n",
      "cnt: 0 - valLoss: 0.6164474487304688 - trainLoss: 0.6223858594894409\n",
      "cnt: 0 - valLoss: 0.6164412498474121 - trainLoss: 0.6223804950714111\n",
      "cnt: 0 - valLoss: 0.616435706615448 - trainLoss: 0.6223748922348022\n",
      "cnt: 0 - valLoss: 0.6164295077323914 - trainLoss: 0.622369647026062\n",
      "cnt: 0 - valLoss: 0.616424024105072 - trainLoss: 0.6223640441894531\n",
      "cnt: 0 - valLoss: 0.6164178848266602 - trainLoss: 0.6223587393760681\n",
      "cnt: 0 - valLoss: 0.6164122819900513 - trainLoss: 0.6223531365394592\n",
      "cnt: 0 - valLoss: 0.6164060235023499 - trainLoss: 0.6223478317260742\n",
      "cnt: 0 - valLoss: 0.6164004802703857 - trainLoss: 0.6223421692848206\n",
      "cnt: 0 - valLoss: 0.6163941621780396 - trainLoss: 0.6223369240760803\n",
      "cnt: 0 - valLoss: 0.616388738155365 - trainLoss: 0.6223312020301819\n",
      "cnt: 0 - valLoss: 0.6163825392723083 - trainLoss: 0.6223260164260864\n",
      "cnt: 0 - valLoss: 0.616377055644989 - trainLoss: 0.622320294380188\n",
      "cnt: 0 - valLoss: 0.6163707375526428 - trainLoss: 0.6223151087760925\n",
      "cnt: 0 - valLoss: 0.6163652539253235 - trainLoss: 0.6223093867301941\n",
      "cnt: 0 - valLoss: 0.6163588762283325 - trainLoss: 0.6223042011260986\n",
      "cnt: 0 - valLoss: 0.616353452205658 - trainLoss: 0.6222984194755554\n",
      "cnt: 0 - valLoss: 0.6163472533226013 - trainLoss: 0.6222932934761047\n",
      "cnt: 0 - valLoss: 0.616341769695282 - trainLoss: 0.6222875118255615\n",
      "cnt: 0 - valLoss: 0.616335391998291 - trainLoss: 0.6222823262214661\n",
      "cnt: 0 - valLoss: 0.6163300275802612 - trainLoss: 0.6222765445709229\n",
      "cnt: 0 - valLoss: 0.6163236498832703 - trainLoss: 0.6222714185714722\n",
      "cnt: 0 - valLoss: 0.6163182854652405 - trainLoss: 0.622265636920929\n",
      "cnt: 0 - valLoss: 0.61631178855896 - trainLoss: 0.6222604513168335\n",
      "cnt: 0 - valLoss: 0.6163065433502197 - trainLoss: 0.6222547292709351\n",
      "cnt: 0 - valLoss: 0.616300106048584 - trainLoss: 0.6222495436668396\n",
      "cnt: 0 - valLoss: 0.6162946224212646 - trainLoss: 0.6222437620162964\n",
      "cnt: 0 - valLoss: 0.6162882447242737 - trainLoss: 0.6222386360168457\n",
      "cnt: 0 - valLoss: 0.6162827014923096 - trainLoss: 0.6222327947616577\n",
      "cnt: 0 - valLoss: 0.6162764430046082 - trainLoss: 0.622227668762207\n",
      "cnt: 0 - valLoss: 0.616270899772644 - trainLoss: 0.622221827507019\n",
      "cnt: 0 - valLoss: 0.6162646412849426 - trainLoss: 0.6222166419029236\n",
      "cnt: 0 - valLoss: 0.6162592172622681 - trainLoss: 0.6222108602523804\n",
      "cnt: 0 - valLoss: 0.6162529587745667 - trainLoss: 0.6222057342529297\n",
      "cnt: 0 - valLoss: 0.6162473559379578 - trainLoss: 0.6221998929977417\n",
      "cnt: 0 - valLoss: 0.6162411570549011 - trainLoss: 0.6221947073936462\n",
      "cnt: 0 - valLoss: 0.6162355542182922 - trainLoss: 0.6221889853477478\n",
      "cnt: 0 - valLoss: 0.6162292957305908 - trainLoss: 0.6221837401390076\n",
      "cnt: 0 - valLoss: 0.6162236928939819 - trainLoss: 0.6221780180931091\n",
      "cnt: 0 - valLoss: 0.6162174344062805 - trainLoss: 0.6221727132797241\n",
      "cnt: 0 - valLoss: 0.616212010383606 - trainLoss: 0.6221669912338257\n",
      "cnt: 0 - valLoss: 0.6162058711051941 - trainLoss: 0.6221618056297302\n",
      "cnt: 0 - valLoss: 0.6162001490592957 - trainLoss: 0.6221560835838318\n",
      "cnt: 0 - valLoss: 0.6161940097808838 - trainLoss: 0.6221507787704468\n",
      "cnt: 0 - valLoss: 0.6161882281303406 - trainLoss: 0.6221451163291931\n",
      "cnt: 0 - valLoss: 0.6161821484565735 - trainLoss: 0.6221397519111633\n",
      "cnt: 0 - valLoss: 0.6161763668060303 - trainLoss: 0.6221340894699097\n",
      "cnt: 0 - valLoss: 0.616170346736908 - trainLoss: 0.6221287846565247\n",
      "cnt: 0 - valLoss: 0.6161645650863647 - trainLoss: 0.622123122215271\n",
      "cnt: 0 - valLoss: 0.616158664226532 - trainLoss: 0.6221177577972412\n",
      "cnt: 0 - valLoss: 0.616152822971344 - trainLoss: 0.6221121549606323\n",
      "cnt: 0 - valLoss: 0.6161468029022217 - trainLoss: 0.6221067905426025\n",
      "cnt: 0 - valLoss: 0.6161409616470337 - trainLoss: 0.6221011877059937\n",
      "cnt: 0 - valLoss: 0.6161350011825562 - trainLoss: 0.6220957636833191\n",
      "cnt: 0 - valLoss: 0.6161291003227234 - trainLoss: 0.6220901608467102\n",
      "cnt: 0 - valLoss: 0.6161231398582458 - trainLoss: 0.6220847964286804\n",
      "cnt: 0 - valLoss: 0.6161171793937683 - trainLoss: 0.6220791935920715\n",
      "cnt: 0 - valLoss: 0.6161112785339355 - trainLoss: 0.6220737099647522\n",
      "cnt: 0 - valLoss: 0.6161054968833923 - trainLoss: 0.6220681667327881\n",
      "cnt: 0 - valLoss: 0.6160995960235596 - trainLoss: 0.6220626831054688\n",
      "cnt: 0 - valLoss: 0.616093635559082 - trainLoss: 0.6220571994781494\n",
      "cnt: 0 - valLoss: 0.6160877346992493 - trainLoss: 0.6220517158508301\n",
      "cnt: 0 - valLoss: 0.616081714630127 - trainLoss: 0.622046172618866\n",
      "cnt: 0 - valLoss: 0.616075873374939 - trainLoss: 0.6220406293869019\n",
      "cnt: 0 - valLoss: 0.6160698533058167 - trainLoss: 0.6220352053642273\n",
      "cnt: 0 - valLoss: 0.6160640120506287 - trainLoss: 0.6220296025276184\n",
      "cnt: 0 - valLoss: 0.6160581111907959 - trainLoss: 0.6220241785049438\n",
      "cnt: 0 - valLoss: 0.6160523295402527 - trainLoss: 0.6220186352729797\n",
      "cnt: 0 - valLoss: 0.6160461902618408 - trainLoss: 0.6220131516456604\n",
      "cnt: 0 - valLoss: 0.6160404682159424 - trainLoss: 0.6220075488090515\n",
      "cnt: 0 - valLoss: 0.6160342693328857 - trainLoss: 0.6220021843910217\n",
      "cnt: 0 - valLoss: 0.6160286068916321 - trainLoss: 0.6219965219497681\n",
      "cnt: 0 - valLoss: 0.6160224080085754 - trainLoss: 0.6219911575317383\n",
      "cnt: 0 - valLoss: 0.6160167455673218 - trainLoss: 0.6219854950904846\n",
      "cnt: 0 - valLoss: 0.6160106658935547 - trainLoss: 0.6219800710678101\n",
      "cnt: 0 - valLoss: 0.616005003452301 - trainLoss: 0.6219744086265564\n",
      "cnt: 0 - valLoss: 0.6159987449645996 - trainLoss: 0.6219690442085266\n",
      "cnt: 0 - valLoss: 0.6159931421279907 - trainLoss: 0.621963381767273\n",
      "cnt: 0 - valLoss: 0.6159868836402893 - trainLoss: 0.6219580173492432\n",
      "cnt: 0 - valLoss: 0.6159812211990356 - trainLoss: 0.6219522953033447\n",
      "cnt: 0 - valLoss: 0.6159749627113342 - trainLoss: 0.6219469904899597\n",
      "cnt: 0 - valLoss: 0.6159693598747253 - trainLoss: 0.6219412088394165\n",
      "cnt: 0 - valLoss: 0.6159632205963135 - trainLoss: 0.621936023235321\n",
      "cnt: 0 - valLoss: 0.6159576773643494 - trainLoss: 0.6219302415847778\n",
      "cnt: 0 - valLoss: 0.6159512996673584 - trainLoss: 0.6219249367713928\n",
      "cnt: 0 - valLoss: 0.6159458160400391 - trainLoss: 0.6219191551208496\n",
      "cnt: 0 - valLoss: 0.6159393787384033 - trainLoss: 0.6219139099121094\n",
      "cnt: 0 - valLoss: 0.615933895111084 - trainLoss: 0.6219080686569214\n",
      "cnt: 0 - valLoss: 0.6159273386001587 - trainLoss: 0.6219028830528259\n",
      "cnt: 0 - valLoss: 0.6159220337867737 - trainLoss: 0.6218969821929932\n",
      "cnt: 0 - valLoss: 0.6159156560897827 - trainLoss: 0.6218918561935425\n",
      "cnt: 0 - valLoss: 0.6159101128578186 - trainLoss: 0.6218859553337097\n",
      "cnt: 0 - valLoss: 0.6159037351608276 - trainLoss: 0.6218807697296143\n",
      "cnt: 0 - valLoss: 0.6158981323242188 - trainLoss: 0.6218749284744263\n",
      "cnt: 0 - valLoss: 0.6158917546272278 - trainLoss: 0.6218697428703308\n",
      "cnt: 0 - valLoss: 0.6158862113952637 - trainLoss: 0.621863842010498\n",
      "cnt: 0 - valLoss: 0.615880012512207 - trainLoss: 0.6218585968017578\n",
      "cnt: 0 - valLoss: 0.6158742904663086 - trainLoss: 0.6218528747558594\n",
      "cnt: 0 - valLoss: 0.6158680319786072 - trainLoss: 0.6218475699424744\n",
      "cnt: 0 - valLoss: 0.6158623099327087 - trainLoss: 0.6218417882919312\n",
      "cnt: 0 - valLoss: 0.6158561110496521 - trainLoss: 0.6218365430831909\n",
      "cnt: 0 - valLoss: 0.6158503890037537 - trainLoss: 0.6218307614326477\n",
      "cnt: 0 - valLoss: 0.6158443689346313 - trainLoss: 0.6218255162239075\n",
      "cnt: 0 - valLoss: 0.6158385276794434 - trainLoss: 0.6218197345733643\n",
      "cnt: 0 - valLoss: 0.6158324480056763 - trainLoss: 0.6218144297599792\n",
      "cnt: 0 - valLoss: 0.6158266067504883 - trainLoss: 0.6218087077140808\n",
      "cnt: 0 - valLoss: 0.6158205270767212 - trainLoss: 0.621803343296051\n",
      "cnt: 0 - valLoss: 0.6158146858215332 - trainLoss: 0.6217976808547974\n",
      "cnt: 0 - valLoss: 0.6158086061477661 - trainLoss: 0.6217922568321228\n",
      "cnt: 0 - valLoss: 0.6158027052879333 - trainLoss: 0.6217866539955139\n",
      "cnt: 0 - valLoss: 0.6157968640327454 - trainLoss: 0.6217812299728394\n",
      "cnt: 0 - valLoss: 0.6157909631729126 - trainLoss: 0.6217756867408752\n",
      "cnt: 0 - valLoss: 0.6157850027084351 - trainLoss: 0.6217702031135559\n",
      "cnt: 0 - valLoss: 0.6157790422439575 - trainLoss: 0.621764600276947\n",
      "cnt: 0 - valLoss: 0.61577308177948 - trainLoss: 0.6217591166496277\n",
      "cnt: 0 - valLoss: 0.6157670617103577 - trainLoss: 0.6217535734176636\n",
      "cnt: 0 - valLoss: 0.6157611012458801 - trainLoss: 0.6217480301856995\n",
      "cnt: 0 - valLoss: 0.6157551407814026 - trainLoss: 0.6217424273490906\n",
      "cnt: 0 - valLoss: 0.6157492995262146 - trainLoss: 0.6217369437217712\n",
      "cnt: 0 - valLoss: 0.6157432198524475 - trainLoss: 0.6217313408851624\n",
      "cnt: 0 - valLoss: 0.6157373189926147 - trainLoss: 0.6217257380485535\n",
      "cnt: 0 - valLoss: 0.6157312989234924 - trainLoss: 0.6217202544212341\n",
      "cnt: 0 - valLoss: 0.6157254576683044 - trainLoss: 0.6217146515846252\n",
      "cnt: 0 - valLoss: 0.6157193183898926 - trainLoss: 0.6217091083526611\n",
      "cnt: 0 - valLoss: 0.6157136559486389 - trainLoss: 0.6217035055160522\n",
      "cnt: 0 - valLoss: 0.6157073974609375 - trainLoss: 0.6216980218887329\n",
      "cnt: 0 - valLoss: 0.6157016754150391 - trainLoss: 0.6216923594474792\n",
      "cnt: 0 - valLoss: 0.615695595741272 - trainLoss: 0.6216869354248047\n",
      "cnt: 0 - valLoss: 0.615689754486084 - trainLoss: 0.621681272983551\n",
      "cnt: 0 - valLoss: 0.6156833171844482 - trainLoss: 0.6216757893562317\n",
      "cnt: 0 - valLoss: 0.6156776547431946 - trainLoss: 0.6216699481010437\n",
      "cnt: 0 - valLoss: 0.6156713366508484 - trainLoss: 0.6216645240783691\n",
      "cnt: 0 - valLoss: 0.6156656742095947 - trainLoss: 0.6216586828231812\n",
      "cnt: 0 - valLoss: 0.6156590580940247 - trainLoss: 0.6216533184051514\n",
      "cnt: 0 - valLoss: 0.6156535744667053 - trainLoss: 0.6216474175453186\n",
      "cnt: 0 - valLoss: 0.6156470775604248 - trainLoss: 0.6216421127319336\n",
      "cnt: 0 - valLoss: 0.6156412959098816 - trainLoss: 0.621636152267456\n",
      "cnt: 0 - valLoss: 0.6156348586082458 - trainLoss: 0.621630847454071\n",
      "cnt: 0 - valLoss: 0.6156290769577026 - trainLoss: 0.6216249465942383\n",
      "cnt: 0 - valLoss: 0.6156229376792908 - trainLoss: 0.6216195225715637\n",
      "cnt: 0 - valLoss: 0.615617036819458 - trainLoss: 0.6216136813163757\n",
      "cnt: 0 - valLoss: 0.6156107187271118 - trainLoss: 0.6216082572937012\n",
      "cnt: 0 - valLoss: 0.6156047582626343 - trainLoss: 0.621602475643158\n",
      "cnt: 0 - valLoss: 0.6155986189842224 - trainLoss: 0.6215969324111938\n",
      "cnt: 0 - valLoss: 0.6155927181243896 - trainLoss: 0.6215912103652954\n",
      "cnt: 0 - valLoss: 0.6155866384506226 - trainLoss: 0.6215856671333313\n",
      "cnt: 0 - valLoss: 0.6155804395675659 - trainLoss: 0.6215800046920776\n",
      "cnt: 0 - valLoss: 0.6155744194984436 - trainLoss: 0.621574342250824\n",
      "cnt: 0 - valLoss: 0.6155683398246765 - trainLoss: 0.6215687394142151\n",
      "cnt: 0 - valLoss: 0.6155624389648438 - trainLoss: 0.6215630173683167\n",
      "cnt: 0 - valLoss: 0.6155561208724976 - trainLoss: 0.6215574741363525\n",
      "cnt: 0 - valLoss: 0.6155502796173096 - trainLoss: 0.6215517520904541\n",
      "cnt: 0 - valLoss: 0.6155440211296082 - trainLoss: 0.62154620885849\n",
      "cnt: 0 - valLoss: 0.6155382394790649 - trainLoss: 0.6215404272079468\n",
      "cnt: 0 - valLoss: 0.6155317425727844 - trainLoss: 0.6215349435806274\n",
      "cnt: 0 - valLoss: 0.615526020526886 - trainLoss: 0.6215291023254395\n",
      "cnt: 0 - valLoss: 0.615519642829895 - trainLoss: 0.6215236783027649\n",
      "cnt: 0 - valLoss: 0.6155140995979309 - trainLoss: 0.6215177774429321\n",
      "cnt: 0 - valLoss: 0.6155074238777161 - trainLoss: 0.6215124130249023\n",
      "cnt: 0 - valLoss: 0.6155017614364624 - trainLoss: 0.6215064525604248\n",
      "cnt: 0 - valLoss: 0.6154953837394714 - trainLoss: 0.6215011477470398\n",
      "cnt: 0 - valLoss: 0.6154896020889282 - trainLoss: 0.621495246887207\n",
      "cnt: 0 - valLoss: 0.6154832243919373 - trainLoss: 0.6214898228645325\n",
      "cnt: 0 - valLoss: 0.6154773831367493 - trainLoss: 0.6214838624000549\n",
      "cnt: 0 - valLoss: 0.6154711842536926 - trainLoss: 0.6214785575866699\n",
      "cnt: 0 - valLoss: 0.6154652237892151 - trainLoss: 0.6214726567268372\n",
      "cnt: 0 - valLoss: 0.6154589653015137 - trainLoss: 0.6214671730995178\n",
      "cnt: 0 - valLoss: 0.6154531240463257 - trainLoss: 0.6214613318443298\n",
      "cnt: 0 - valLoss: 0.6154469847679138 - trainLoss: 0.6214558482170105\n",
      "cnt: 0 - valLoss: 0.6154407858848572 - trainLoss: 0.6214501261711121\n",
      "cnt: 0 - valLoss: 0.6154347062110901 - trainLoss: 0.6214444637298584\n",
      "cnt: 0 - valLoss: 0.6154286861419678 - trainLoss: 0.6214388012886047\n",
      "cnt: 0 - valLoss: 0.6154227256774902 - trainLoss: 0.6214331388473511\n",
      "cnt: 0 - valLoss: 0.6154163479804993 - trainLoss: 0.6214275360107422\n",
      "cnt: 0 - valLoss: 0.6154106259346008 - trainLoss: 0.621421754360199\n",
      "cnt: 0 - valLoss: 0.6154043078422546 - trainLoss: 0.6214162111282349\n",
      "cnt: 0 - valLoss: 0.6153984069824219 - trainLoss: 0.6214104294776917\n",
      "cnt: 0 - valLoss: 0.6153920292854309 - trainLoss: 0.6214049458503723\n",
      "cnt: 0 - valLoss: 0.6153863072395325 - trainLoss: 0.6213990449905396\n",
      "cnt: 0 - valLoss: 0.6153797507286072 - trainLoss: 0.621393620967865\n",
      "cnt: 0 - valLoss: 0.6153740882873535 - trainLoss: 0.6213876008987427\n",
      "cnt: 0 - valLoss: 0.615367591381073 - trainLoss: 0.6213821768760681\n",
      "cnt: 0 - valLoss: 0.6153619289398193 - trainLoss: 0.6213762164115906\n",
      "cnt: 0 - valLoss: 0.6153552532196045 - trainLoss: 0.6213708519935608\n",
      "cnt: 0 - valLoss: 0.6153494715690613 - trainLoss: 0.6213648319244385\n",
      "cnt: 0 - valLoss: 0.6153431534767151 - trainLoss: 0.6213594675064087\n",
      "cnt: 0 - valLoss: 0.6153372526168823 - trainLoss: 0.6213534474372864\n",
      "cnt: 0 - valLoss: 0.6153308749198914 - trainLoss: 0.621347963809967\n",
      "cnt: 0 - valLoss: 0.6153250932693481 - trainLoss: 0.6213420629501343\n",
      "cnt: 0 - valLoss: 0.6153187155723572 - trainLoss: 0.6213365197181702\n",
      "cnt: 0 - valLoss: 0.6153126358985901 - trainLoss: 0.6213306784629822\n",
      "cnt: 0 - valLoss: 0.6153064370155334 - trainLoss: 0.6213250756263733\n",
      "cnt: 0 - valLoss: 0.6153003573417664 - trainLoss: 0.6213192343711853\n",
      "cnt: 0 - valLoss: 0.6152942180633545 - trainLoss: 0.6213135123252869\n",
      "cnt: 0 - valLoss: 0.6152878403663635 - trainLoss: 0.6213076710700989\n",
      "cnt: 0 - valLoss: 0.6152819395065308 - trainLoss: 0.6213019490242004\n",
      "cnt: 0 - valLoss: 0.615275502204895 - trainLoss: 0.621296226978302\n",
      "cnt: 0 - valLoss: 0.6152695417404175 - trainLoss: 0.621290385723114\n",
      "cnt: 0 - valLoss: 0.6152631044387817 - trainLoss: 0.6212846636772156\n",
      "cnt: 0 - valLoss: 0.6152572631835938 - trainLoss: 0.6212788224220276\n",
      "cnt: 0 - valLoss: 0.6152507662773132 - trainLoss: 0.6212732195854187\n",
      "cnt: 0 - valLoss: 0.61524498462677 - trainLoss: 0.6212672591209412\n",
      "cnt: 0 - valLoss: 0.6152385473251343 - trainLoss: 0.621261715888977\n",
      "cnt: 0 - valLoss: 0.6152327656745911 - trainLoss: 0.6212556958198547\n",
      "cnt: 0 - valLoss: 0.615226149559021 - trainLoss: 0.6212502121925354\n",
      "cnt: 0 - valLoss: 0.6152206659317017 - trainLoss: 0.6212440729141235\n",
      "cnt: 0 - valLoss: 0.6152139902114868 - trainLoss: 0.6212387084960938\n",
      "cnt: 0 - valLoss: 0.6152081489562988 - trainLoss: 0.6212325692176819\n",
      "cnt: 0 - valLoss: 0.6152017116546631 - trainLoss: 0.6212270855903625\n",
      "cnt: 0 - valLoss: 0.6151958703994751 - trainLoss: 0.621221125125885\n",
      "cnt: 0 - valLoss: 0.6151894330978394 - trainLoss: 0.6212155818939209\n",
      "cnt: 0 - valLoss: 0.6151834726333618 - trainLoss: 0.6212095618247986\n",
      "cnt: 0 - valLoss: 0.6151773929595947 - trainLoss: 0.6212039589881897\n",
      "cnt: 0 - valLoss: 0.6151711940765381 - trainLoss: 0.6211981177330017\n",
      "cnt: 0 - valLoss: 0.6151649951934814 - trainLoss: 0.6211923360824585\n",
      "cnt: 0 - valLoss: 0.6151589155197144 - trainLoss: 0.6211864948272705\n",
      "cnt: 0 - valLoss: 0.6151529550552368 - trainLoss: 0.6211808323860168\n",
      "cnt: 0 - valLoss: 0.6151466965675354 - trainLoss: 0.6211751103401184\n",
      "cnt: 0 - valLoss: 0.6151407361030579 - trainLoss: 0.6211692690849304\n",
      "cnt: 0 - valLoss: 0.6151345372200012 - trainLoss: 0.6211636066436768\n",
      "cnt: 0 - valLoss: 0.6151286959648132 - trainLoss: 0.6211577653884888\n",
      "cnt: 0 - valLoss: 0.6151221990585327 - trainLoss: 0.6211521029472351\n",
      "cnt: 0 - valLoss: 0.6151165962219238 - trainLoss: 0.6211461424827576\n",
      "cnt: 0 - valLoss: 0.6151100993156433 - trainLoss: 0.6211405992507935\n",
      "cnt: 0 - valLoss: 0.6151044368743896 - trainLoss: 0.6211345195770264\n",
      "cnt: 0 - valLoss: 0.6150977611541748 - trainLoss: 0.621129035949707\n",
      "cnt: 0 - valLoss: 0.6150923371315002 - trainLoss: 0.6211229562759399\n",
      "cnt: 0 - valLoss: 0.6150856614112854 - trainLoss: 0.6211175322532654\n",
      "cnt: 0 - valLoss: 0.6150798797607422 - trainLoss: 0.6211113929748535\n",
      "cnt: 0 - valLoss: 0.615073561668396 - trainLoss: 0.6211059093475342\n",
      "cnt: 0 - valLoss: 0.615067720413208 - trainLoss: 0.6210998892784119\n",
      "cnt: 0 - valLoss: 0.6150612831115723 - trainLoss: 0.621094286441803\n",
      "cnt: 0 - valLoss: 0.6150553822517395 - trainLoss: 0.6210882663726807\n",
      "cnt: 0 - valLoss: 0.6150493025779724 - trainLoss: 0.6210827231407166\n",
      "cnt: 0 - valLoss: 0.6150431036949158 - trainLoss: 0.6210768222808838\n",
      "cnt: 0 - valLoss: 0.6150370240211487 - trainLoss: 0.6210710406303406\n",
      "cnt: 0 - valLoss: 0.6150310039520264 - trainLoss: 0.6210651397705078\n",
      "cnt: 0 - valLoss: 0.6150249242782593 - trainLoss: 0.6210594177246094\n",
      "cnt: 0 - valLoss: 0.6150186657905579 - trainLoss: 0.6210535764694214\n",
      "cnt: 0 - valLoss: 0.6150128841400146 - trainLoss: 0.6210477948188782\n",
      "cnt: 0 - valLoss: 0.6150065064430237 - trainLoss: 0.6210420727729797\n",
      "cnt: 0 - valLoss: 0.6150005459785461 - trainLoss: 0.6210361123085022\n",
      "cnt: 0 - valLoss: 0.6149941086769104 - trainLoss: 0.6210304498672485\n",
      "cnt: 0 - valLoss: 0.6149885058403015 - trainLoss: 0.621024489402771\n",
      "cnt: 0 - valLoss: 0.6149818897247314 - trainLoss: 0.6210189461708069\n",
      "cnt: 0 - valLoss: 0.6149762868881226 - trainLoss: 0.6210128664970398\n",
      "cnt: 0 - valLoss: 0.6149696707725525 - trainLoss: 0.6210072636604309\n",
      "cnt: 0 - valLoss: 0.6149640083312988 - trainLoss: 0.6210011839866638\n",
      "cnt: 0 - valLoss: 0.6149575114250183 - trainLoss: 0.6209957003593445\n",
      "cnt: 0 - valLoss: 0.6149516701698303 - trainLoss: 0.6209895610809326\n",
      "cnt: 0 - valLoss: 0.6149452924728394 - trainLoss: 0.6209839582443237\n",
      "cnt: 0 - valLoss: 0.6149395108222961 - trainLoss: 0.6209779977798462\n",
      "cnt: 0 - valLoss: 0.6149331331253052 - trainLoss: 0.6209723353385925\n",
      "cnt: 0 - valLoss: 0.6149271130561829 - trainLoss: 0.620966374874115\n",
      "cnt: 0 - valLoss: 0.614920973777771 - trainLoss: 0.6209606528282166\n",
      "cnt: 0 - valLoss: 0.6149149537086487 - trainLoss: 0.6209547519683838\n",
      "cnt: 0 - valLoss: 0.6149088144302368 - trainLoss: 0.6209489703178406\n",
      "cnt: 0 - valLoss: 0.6149025559425354 - trainLoss: 0.6209431290626526\n",
      "cnt: 0 - valLoss: 0.6148965954780579 - trainLoss: 0.6209372282028198\n",
      "cnt: 0 - valLoss: 0.6148903369903564 - trainLoss: 0.6209315061569214\n",
      "cnt: 0 - valLoss: 0.6148844361305237 - trainLoss: 0.6209255456924438\n",
      "cnt: 0 - valLoss: 0.6148779988288879 - trainLoss: 0.6209198236465454\n",
      "cnt: 0 - valLoss: 0.6148722171783447 - trainLoss: 0.6209137439727783\n",
      "cnt: 0 - valLoss: 0.6148658394813538 - trainLoss: 0.6209081411361694\n",
      "cnt: 0 - valLoss: 0.6148601174354553 - trainLoss: 0.6209020614624023\n",
      "cnt: 0 - valLoss: 0.6148534417152405 - trainLoss: 0.6208964586257935\n",
      "cnt: 0 - valLoss: 0.6148478388786316 - trainLoss: 0.6208902597427368\n",
      "cnt: 0 - valLoss: 0.6148413419723511 - trainLoss: 0.6208847761154175\n",
      "cnt: 0 - valLoss: 0.6148355007171631 - trainLoss: 0.6208786964416504\n",
      "cnt: 0 - valLoss: 0.6148290634155273 - trainLoss: 0.6208731532096863\n",
      "cnt: 0 - valLoss: 0.6148231625556946 - trainLoss: 0.620867133140564\n",
      "cnt: 0 - valLoss: 0.6148168444633484 - trainLoss: 0.6208615303039551\n",
      "cnt: 0 - valLoss: 0.6148110032081604 - trainLoss: 0.6208556294441223\n",
      "cnt: 0 - valLoss: 0.614804744720459 - trainLoss: 0.6208499670028687\n",
      "cnt: 0 - valLoss: 0.6147986054420471 - trainLoss: 0.6208440661430359\n",
      "cnt: 0 - valLoss: 0.6147924661636353 - trainLoss: 0.6208382844924927\n",
      "cnt: 0 - valLoss: 0.6147863268852234 - trainLoss: 0.6208324432373047\n",
      "cnt: 0 - valLoss: 0.6147804856300354 - trainLoss: 0.6208266615867615\n",
      "cnt: 0 - valLoss: 0.6147741079330444 - trainLoss: 0.620820939540863\n",
      "cnt: 0 - valLoss: 0.6147682070732117 - trainLoss: 0.6208150386810303\n",
      "cnt: 0 - valLoss: 0.6147617101669312 - trainLoss: 0.6208093762397766\n",
      "cnt: 0 - valLoss: 0.6147560477256775 - trainLoss: 0.6208033561706543\n",
      "cnt: 0 - valLoss: 0.6147494316101074 - trainLoss: 0.6207978129386902\n",
      "cnt: 0 - valLoss: 0.6147439479827881 - trainLoss: 0.6207917332649231\n",
      "cnt: 0 - valLoss: 0.6147372126579285 - trainLoss: 0.6207862496376038\n",
      "cnt: 0 - valLoss: 0.6147314310073853 - trainLoss: 0.6207801103591919\n",
      "cnt: 0 - valLoss: 0.6147249341011047 - trainLoss: 0.6207746267318726\n",
      "cnt: 0 - valLoss: 0.6147191524505615 - trainLoss: 0.6207685470581055\n",
      "cnt: 0 - valLoss: 0.6147128939628601 - trainLoss: 0.6207630038261414\n",
      "cnt: 0 - valLoss: 0.6147069334983826 - trainLoss: 0.620756983757019\n",
      "cnt: 0 - valLoss: 0.6147006154060364 - trainLoss: 0.6207513809204102\n",
      "cnt: 0 - valLoss: 0.6146945357322693 - trainLoss: 0.6207453608512878\n",
      "cnt: 0 - valLoss: 0.6146883964538574 - trainLoss: 0.6207396984100342\n",
      "cnt: 0 - valLoss: 0.6146822571754456 - trainLoss: 0.6207337975502014\n",
      "cnt: 0 - valLoss: 0.614676296710968 - trainLoss: 0.6207279562950134\n",
      "cnt: 0 - valLoss: 0.6146700382232666 - trainLoss: 0.620722234249115\n",
      "cnt: 0 - valLoss: 0.6146640777587891 - trainLoss: 0.620716392993927\n",
      "cnt: 0 - valLoss: 0.6146576404571533 - trainLoss: 0.6207106113433838\n",
      "cnt: 0 - valLoss: 0.6146517992019653 - trainLoss: 0.6207046508789062\n",
      "cnt: 0 - valLoss: 0.6146453022956848 - trainLoss: 0.6206989288330078\n",
      "cnt: 0 - valLoss: 0.6146396994590759 - trainLoss: 0.620693027973175\n",
      "cnt: 0 - valLoss: 0.6146330237388611 - trainLoss: 0.6206874847412109\n",
      "cnt: 0 - valLoss: 0.6146274209022522 - trainLoss: 0.6206812858581543\n",
      "cnt: 0 - valLoss: 0.6146206855773926 - trainLoss: 0.6206758618354797\n",
      "cnt: 0 - valLoss: 0.6146149635314941 - trainLoss: 0.6206696629524231\n",
      "cnt: 0 - valLoss: 0.6146087050437927 - trainLoss: 0.6206641793251038\n",
      "cnt: 0 - valLoss: 0.6146026849746704 - trainLoss: 0.6206580996513367\n",
      "cnt: 0 - valLoss: 0.6145963072776794 - trainLoss: 0.620652437210083\n",
      "cnt: 0 - valLoss: 0.6145902872085571 - trainLoss: 0.6206464171409607\n",
      "cnt: 0 - valLoss: 0.6145840883255005 - trainLoss: 0.620640754699707\n",
      "cnt: 0 - valLoss: 0.6145779490470886 - trainLoss: 0.6206347942352295\n",
      "cnt: 0 - valLoss: 0.6145719289779663 - trainLoss: 0.6206290125846863\n",
      "cnt: 0 - valLoss: 0.6145657896995544 - trainLoss: 0.6206231713294983\n",
      "cnt: 0 - valLoss: 0.6145597100257874 - trainLoss: 0.6206173896789551\n",
      "cnt: 0 - valLoss: 0.6145533323287964 - trainLoss: 0.6206115484237671\n",
      "cnt: 0 - valLoss: 0.6145473718643188 - trainLoss: 0.6206055879592896\n",
      "cnt: 0 - valLoss: 0.6145408749580383 - trainLoss: 0.6205999255180359\n",
      "cnt: 0 - valLoss: 0.6145352721214294 - trainLoss: 0.6205938458442688\n",
      "cnt: 0 - valLoss: 0.6145287156105042 - trainLoss: 0.6205883026123047\n",
      "cnt: 0 - valLoss: 0.6145229339599609 - trainLoss: 0.6205821633338928\n",
      "cnt: 0 - valLoss: 0.6145162582397461 - trainLoss: 0.6205765604972839\n",
      "cnt: 0 - valLoss: 0.6145104169845581 - trainLoss: 0.6205703616142273\n",
      "cnt: 0 - valLoss: 0.6145040392875671 - trainLoss: 0.620564877986908\n",
      "cnt: 0 - valLoss: 0.6144982576370239 - trainLoss: 0.6205587387084961\n",
      "cnt: 0 - valLoss: 0.614491879940033 - trainLoss: 0.6205531358718872\n",
      "cnt: 0 - valLoss: 0.6144857406616211 - trainLoss: 0.6205471158027649\n",
      "cnt: 0 - valLoss: 0.6144796013832092 - trainLoss: 0.6205413341522217\n",
      "cnt: 0 - valLoss: 0.6144734025001526 - trainLoss: 0.6205353736877441\n",
      "cnt: 0 - valLoss: 0.6144673228263855 - trainLoss: 0.6205295324325562\n",
      "cnt: 0 - valLoss: 0.6144610643386841 - trainLoss: 0.6205236315727234\n",
      "cnt: 0 - valLoss: 0.6144552230834961 - trainLoss: 0.6205177903175354\n",
      "cnt: 0 - valLoss: 0.6144487857818604 - trainLoss: 0.6205120086669922\n",
      "cnt: 0 - valLoss: 0.6144428849220276 - trainLoss: 0.6205059885978699\n",
      "cnt: 0 - valLoss: 0.6144363284111023 - trainLoss: 0.6205003261566162\n",
      "cnt: 0 - valLoss: 0.6144305467605591 - trainLoss: 0.6204941868782043\n",
      "cnt: 0 - valLoss: 0.6144241094589233 - trainLoss: 0.6204885840415955\n",
      "cnt: 0 - valLoss: 0.6144183874130249 - trainLoss: 0.6204824447631836\n",
      "cnt: 0 - valLoss: 0.6144117116928101 - trainLoss: 0.6204769015312195\n",
      "cnt: 0 - valLoss: 0.6144058704376221 - trainLoss: 0.6204707026481628\n",
      "cnt: 0 - valLoss: 0.6143993139266968 - trainLoss: 0.6204651594161987\n",
      "cnt: 0 - valLoss: 0.614393413066864 - trainLoss: 0.6204590201377869\n",
      "cnt: 0 - valLoss: 0.6143871545791626 - trainLoss: 0.6204533576965332\n",
      "cnt: 0 - valLoss: 0.6143811345100403 - trainLoss: 0.6204472780227661\n",
      "cnt: 0 - valLoss: 0.6143748760223389 - trainLoss: 0.6204415559768677\n",
      "cnt: 0 - valLoss: 0.6143686771392822 - trainLoss: 0.6204355955123901\n",
      "cnt: 0 - valLoss: 0.6143625378608704 - trainLoss: 0.6204297542572021\n",
      "cnt: 0 - valLoss: 0.614356279373169 - trainLoss: 0.6204237937927246\n",
      "cnt: 0 - valLoss: 0.6143504977226257 - trainLoss: 0.6204178929328918\n",
      "cnt: 0 - valLoss: 0.6143440008163452 - trainLoss: 0.6204121112823486\n",
      "cnt: 0 - valLoss: 0.6143380999565125 - trainLoss: 0.6204060912132263\n",
      "cnt: 0 - valLoss: 0.6143314242362976 - trainLoss: 0.6204004287719727\n",
      "cnt: 0 - valLoss: 0.614325761795044 - trainLoss: 0.620394229888916\n",
      "cnt: 0 - valLoss: 0.6143190860748291 - trainLoss: 0.6203886270523071\n",
      "cnt: 0 - valLoss: 0.6143136024475098 - trainLoss: 0.6203824877738953\n",
      "cnt: 0 - valLoss: 0.6143068671226501 - trainLoss: 0.6203768849372864\n",
      "cnt: 0 - valLoss: 0.6143009662628174 - trainLoss: 0.6203706860542297\n",
      "cnt: 0 - valLoss: 0.6142944693565369 - trainLoss: 0.6203650832176208\n",
      "cnt: 0 - valLoss: 0.6142885088920593 - trainLoss: 0.620358943939209\n",
      "cnt: 0 - valLoss: 0.6142823100090027 - trainLoss: 0.6203532218933105\n",
      "cnt: 0 - valLoss: 0.6142761707305908 - trainLoss: 0.6203472018241882\n",
      "cnt: 0 - valLoss: 0.6142699718475342 - trainLoss: 0.6203413605690002\n",
      "cnt: 0 - valLoss: 0.614263653755188 - trainLoss: 0.6203354597091675\n",
      "cnt: 0 - valLoss: 0.6142575740814209 - trainLoss: 0.6203294992446899\n",
      "cnt: 0 - valLoss: 0.6142511963844299 - trainLoss: 0.620323657989502\n",
      "cnt: 0 - valLoss: 0.6142453551292419 - trainLoss: 0.6203176379203796\n",
      "cnt: 0 - valLoss: 0.614238977432251 - trainLoss: 0.6203119158744812\n",
      "cnt: 0 - valLoss: 0.6142330169677734 - trainLoss: 0.6203058362007141\n",
      "cnt: 0 - valLoss: 0.6142264604568481 - trainLoss: 0.6203001737594604\n",
      "cnt: 0 - valLoss: 0.6142206788063049 - trainLoss: 0.6202939748764038\n",
      "cnt: 0 - valLoss: 0.6142140030860901 - trainLoss: 0.6202883720397949\n",
      "cnt: 0 - valLoss: 0.6142083406448364 - trainLoss: 0.6202821731567383\n",
      "cnt: 0 - valLoss: 0.6142018437385559 - trainLoss: 0.6202765703201294\n",
      "cnt: 0 - valLoss: 0.6141959428787231 - trainLoss: 0.6202704310417175\n",
      "cnt: 0 - valLoss: 0.6141895651817322 - trainLoss: 0.6202647686004639\n",
      "cnt: 0 - valLoss: 0.6141835451126099 - trainLoss: 0.6202586889266968\n",
      "cnt: 0 - valLoss: 0.6141773462295532 - trainLoss: 0.6202529668807983\n",
      "cnt: 0 - valLoss: 0.6141712665557861 - trainLoss: 0.620246946811676\n",
      "cnt: 0 - valLoss: 0.614165186882019 - trainLoss: 0.6202411651611328\n",
      "cnt: 0 - valLoss: 0.6141588687896729 - trainLoss: 0.6202352046966553\n",
      "cnt: 0 - valLoss: 0.6141529083251953 - trainLoss: 0.6202293038368225\n",
      "cnt: 0 - valLoss: 0.6141464710235596 - trainLoss: 0.6202235221862793\n",
      "cnt: 0 - valLoss: 0.6141406893730164 - trainLoss: 0.6202174425125122\n",
      "cnt: 0 - valLoss: 0.6141343116760254 - trainLoss: 0.6202117204666138\n",
      "cnt: 0 - valLoss: 0.6141286492347717 - trainLoss: 0.6202056407928467\n",
      "cnt: 0 - valLoss: 0.6141219139099121 - trainLoss: 0.6202000379562378\n",
      "cnt: 0 - valLoss: 0.6141160726547241 - trainLoss: 0.6201937794685364\n",
      "cnt: 0 - valLoss: 0.6141096353530884 - trainLoss: 0.6201882362365723\n",
      "cnt: 0 - valLoss: 0.6141036748886108 - trainLoss: 0.6201819777488708\n",
      "cnt: 0 - valLoss: 0.6140975952148438 - trainLoss: 0.6201763153076172\n",
      "cnt: 0 - valLoss: 0.6140914559364319 - trainLoss: 0.6201703548431396\n",
      "cnt: 0 - valLoss: 0.6140852570533752 - trainLoss: 0.6201646327972412\n",
      "cnt: 0 - valLoss: 0.6140789985656738 - trainLoss: 0.6201586127281189\n",
      "cnt: 0 - valLoss: 0.6140729188919067 - trainLoss: 0.6201527714729309\n",
      "cnt: 0 - valLoss: 0.6140667200088501 - trainLoss: 0.6201468706130981\n",
      "cnt: 0 - valLoss: 0.6140607595443726 - trainLoss: 0.6201409101486206\n",
      "cnt: 0 - valLoss: 0.614054262638092 - trainLoss: 0.6201351284980774\n",
      "cnt: 0 - valLoss: 0.6140483617782593 - trainLoss: 0.6201290488243103\n",
      "cnt: 0 - valLoss: 0.6140419840812683 - trainLoss: 0.6201233267784119\n",
      "cnt: 0 - valLoss: 0.6140362620353699 - trainLoss: 0.6201171875\n",
      "cnt: 0 - valLoss: 0.6140295267105103 - trainLoss: 0.6201115250587463\n",
      "cnt: 0 - valLoss: 0.6140236854553223 - trainLoss: 0.6201052665710449\n",
      "cnt: 0 - valLoss: 0.6140172481536865 - trainLoss: 0.620099663734436\n",
      "cnt: 0 - valLoss: 0.6140114068984985 - trainLoss: 0.6200934052467346\n",
      "cnt: 0 - valLoss: 0.6140049695968628 - trainLoss: 0.6200876832008362\n",
      "cnt: 0 - valLoss: 0.6139988303184509 - trainLoss: 0.6200815439224243\n",
      "cnt: 0 - valLoss: 0.6139925718307495 - trainLoss: 0.6200755834579468\n",
      "cnt: 0 - valLoss: 0.613986611366272 - trainLoss: 0.6200695633888245\n",
      "cnt: 0 - valLoss: 0.6139804124832153 - trainLoss: 0.6200636625289917\n",
      "cnt: 0 - valLoss: 0.6139739751815796 - trainLoss: 0.6200577020645142\n",
      "cnt: 0 - valLoss: 0.6139679551124573 - trainLoss: 0.6200516223907471\n",
      "cnt: 0 - valLoss: 0.6139616370201111 - trainLoss: 0.6200457215309143\n",
      "cnt: 0 - valLoss: 0.6139557957649231 - trainLoss: 0.6200396418571472\n",
      "cnt: 0 - valLoss: 0.6139491200447083 - trainLoss: 0.6200338006019592\n",
      "cnt: 0 - valLoss: 0.6139433979988098 - trainLoss: 0.6200275421142578\n",
      "cnt: 0 - valLoss: 0.6139366626739502 - trainLoss: 0.6200218200683594\n",
      "cnt: 0 - valLoss: 0.6139309406280518 - trainLoss: 0.620015561580658\n",
      "cnt: 0 - valLoss: 0.6139244437217712 - trainLoss: 0.6200098395347595\n",
      "cnt: 0 - valLoss: 0.6139184236526489 - trainLoss: 0.6200035214424133\n",
      "cnt: 0 - valLoss: 0.6139121055603027 - trainLoss: 0.6199976801872253\n",
      "cnt: 0 - valLoss: 0.6139059066772461 - trainLoss: 0.6199916005134583\n",
      "cnt: 0 - valLoss: 0.6138998866081238 - trainLoss: 0.6199856996536255\n",
      "cnt: 0 - valLoss: 0.6138935089111328 - trainLoss: 0.6199796199798584\n",
      "cnt: 0 - valLoss: 0.6138874888420105 - trainLoss: 0.6199735999107361\n",
      "cnt: 0 - valLoss: 0.61388099193573 - trainLoss: 0.6199676394462585\n",
      "cnt: 0 - valLoss: 0.613875150680542 - trainLoss: 0.6199615001678467\n",
      "cnt: 0 - valLoss: 0.6138687133789062 - trainLoss: 0.6199556589126587\n",
      "cnt: 0 - valLoss: 0.6138628125190735 - trainLoss: 0.6199494004249573\n",
      "cnt: 0 - valLoss: 0.6138561367988586 - trainLoss: 0.6199435591697693\n",
      "cnt: 0 - valLoss: 0.613850474357605 - trainLoss: 0.6199372410774231\n",
      "cnt: 0 - valLoss: 0.6138439178466797 - trainLoss: 0.6199315190315247\n",
      "cnt: 0 - valLoss: 0.6138381361961365 - trainLoss: 0.6199252605438232\n",
      "cnt: 0 - valLoss: 0.6138319373130798 - trainLoss: 0.6199194192886353\n",
      "cnt: 0 - valLoss: 0.6138261556625366 - trainLoss: 0.6199132204055786\n",
      "cnt: 0 - valLoss: 0.6138201951980591 - trainLoss: 0.6199072599411011\n",
      "cnt: 0 - valLoss: 0.6138138175010681 - trainLoss: 0.6199011206626892\n",
      "cnt: 0 - valLoss: 0.6138080358505249 - trainLoss: 0.6198951601982117\n",
      "cnt: 0 - valLoss: 0.6138020157814026 - trainLoss: 0.6198891401290894\n",
      "cnt: 0 - valLoss: 0.6137963533401489 - trainLoss: 0.6198830008506775\n",
      "cnt: 0 - valLoss: 0.6137902140617371 - trainLoss: 0.6198770403862\n",
      "cnt: 0 - valLoss: 0.6137841939926147 - trainLoss: 0.6198708415031433\n",
      "cnt: 0 - valLoss: 0.6137778759002686 - trainLoss: 0.6198650002479553\n",
      "cnt: 0 - valLoss: 0.6137725114822388 - trainLoss: 0.6198586821556091\n",
      "cnt: 0 - valLoss: 0.613766074180603 - trainLoss: 0.6198529005050659\n",
      "cnt: 0 - valLoss: 0.6137605905532837 - trainLoss: 0.6198465824127197\n",
      "cnt: 0 - valLoss: 0.6137539148330688 - trainLoss: 0.6198408603668213\n",
      "cnt: 0 - valLoss: 0.6137481927871704 - trainLoss: 0.6198345422744751\n",
      "cnt: 0 - valLoss: 0.6137421727180481 - trainLoss: 0.6198286414146423\n",
      "cnt: 0 - valLoss: 0.6137363910675049 - trainLoss: 0.6198224425315857\n",
      "cnt: 0 - valLoss: 0.6137300133705139 - trainLoss: 0.6198164820671082\n",
      "cnt: 0 - valLoss: 0.6137239933013916 - trainLoss: 0.6198104023933411\n",
      "cnt: 0 - valLoss: 0.6137183308601379 - trainLoss: 0.6198042035102844\n",
      "cnt: 0 - valLoss: 0.6137121915817261 - trainLoss: 0.6197982430458069\n",
      "cnt: 0 - valLoss: 0.6137067079544067 - trainLoss: 0.6197919845581055\n",
      "cnt: 0 - valLoss: 0.6136997938156128 - trainLoss: 0.6197861433029175\n",
      "cnt: 0 - valLoss: 0.6136943697929382 - trainLoss: 0.6197798252105713\n",
      "cnt: 0 - valLoss: 0.6136879920959473 - trainLoss: 0.6197740435600281\n",
      "cnt: 0 - valLoss: 0.6136825084686279 - trainLoss: 0.6197676062583923\n",
      "cnt: 0 - valLoss: 0.6136757731437683 - trainLoss: 0.6197618246078491\n",
      "cnt: 0 - valLoss: 0.6136701107025146 - trainLoss: 0.6197555065155029\n",
      "cnt: 0 - valLoss: 0.6136640906333923 - trainLoss: 0.6197496652603149\n",
      "cnt: 0 - valLoss: 0.6136582493782043 - trainLoss: 0.6197434663772583\n",
      "cnt: 0 - valLoss: 0.6136524081230164 - trainLoss: 0.6197375059127808\n",
      "cnt: 0 - valLoss: 0.6136458516120911 - trainLoss: 0.6197314262390137\n",
      "cnt: 0 - valLoss: 0.6136401295661926 - trainLoss: 0.6197253465652466\n",
      "cnt: 0 - valLoss: 0.6136339902877808 - trainLoss: 0.6197193264961243\n",
      "cnt: 0 - valLoss: 0.6136284470558167 - trainLoss: 0.6197130680084229\n",
      "cnt: 0 - valLoss: 0.6136215925216675 - trainLoss: 0.6197072267532349\n",
      "cnt: 0 - valLoss: 0.6136161088943481 - trainLoss: 0.6197009682655334\n",
      "cnt: 0 - valLoss: 0.6136097311973572 - trainLoss: 0.6196951866149902\n",
      "cnt: 0 - valLoss: 0.6136041879653931 - trainLoss: 0.6196888089179993\n",
      "cnt: 0 - valLoss: 0.6135980486869812 - trainLoss: 0.619683027267456\n",
      "cnt: 0 - valLoss: 0.6135916709899902 - trainLoss: 0.6196767687797546\n",
      "cnt: 0 - valLoss: 0.6135857105255127 - trainLoss: 0.6196709275245667\n",
      "cnt: 0 - valLoss: 0.6135799288749695 - trainLoss: 0.61966472864151\n",
      "cnt: 0 - valLoss: 0.6135739684104919 - trainLoss: 0.6196587681770325\n",
      "cnt: 0 - valLoss: 0.6135680675506592 - trainLoss: 0.6196526885032654\n",
      "cnt: 0 - valLoss: 0.6135616898536682 - trainLoss: 0.6196466088294983\n",
      "cnt: 0 - valLoss: 0.6135555505752563 - trainLoss: 0.6196406483650208\n",
      "cnt: 0 - valLoss: 0.6135499477386475 - trainLoss: 0.6196345090866089\n",
      "cnt: 0 - valLoss: 0.6135436296463013 - trainLoss: 0.6196285486221313\n",
      "cnt: 0 - valLoss: 0.6135376691818237 - trainLoss: 0.6196223497390747\n",
      "cnt: 0 - valLoss: 0.613531231880188 - trainLoss: 0.6196166276931763\n",
      "cnt: 0 - valLoss: 0.6135257482528687 - trainLoss: 0.6196102499961853\n",
      "cnt: 0 - valLoss: 0.6135194897651672 - trainLoss: 0.6196045279502869\n",
      "cnt: 0 - valLoss: 0.6135138869285583 - trainLoss: 0.6195981502532959\n",
      "cnt: 0 - valLoss: 0.6135072112083435 - trainLoss: 0.6195923686027527\n",
      "cnt: 0 - valLoss: 0.6135013699531555 - trainLoss: 0.619586169719696\n",
      "cnt: 0 - valLoss: 0.613495409488678 - trainLoss: 0.6195802688598633\n",
      "cnt: 0 - valLoss: 0.6134894490242004 - trainLoss: 0.6195740699768066\n",
      "cnt: 0 - valLoss: 0.6134836673736572 - trainLoss: 0.6195680499076843\n",
      "cnt: 0 - valLoss: 0.6134769916534424 - trainLoss: 0.619562029838562\n",
      "cnt: 0 - valLoss: 0.6134713292121887 - trainLoss: 0.6195558905601501\n",
      "cnt: 0 - valLoss: 0.6134650707244873 - trainLoss: 0.6195499897003174\n",
      "cnt: 0 - valLoss: 0.6134595274925232 - trainLoss: 0.6195436716079712\n",
      "cnt: 0 - valLoss: 0.6134531497955322 - trainLoss: 0.6195378303527832\n",
      "cnt: 0 - valLoss: 0.6134471297264099 - trainLoss: 0.619531512260437\n",
      "cnt: 0 - valLoss: 0.6134408712387085 - trainLoss: 0.6195257306098938\n",
      "cnt: 0 - valLoss: 0.6134351491928101 - trainLoss: 0.6195194125175476\n",
      "cnt: 0 - valLoss: 0.6134290099143982 - trainLoss: 0.6195135116577148\n",
      "cnt: 0 - valLoss: 0.6134232878684998 - trainLoss: 0.6195071935653687\n",
      "cnt: 0 - valLoss: 0.6134167313575745 - trainLoss: 0.6195012927055359\n",
      "cnt: 0 - valLoss: 0.6134107708930969 - trainLoss: 0.6194952130317688\n",
      "cnt: 0 - valLoss: 0.6134049892425537 - trainLoss: 0.6194890141487122\n",
      "cnt: 0 - valLoss: 0.6133987903594971 - trainLoss: 0.6194830536842346\n",
      "cnt: 0 - valLoss: 0.6133931875228882 - trainLoss: 0.6194767951965332\n",
      "cnt: 0 - valLoss: 0.613386332988739 - trainLoss: 0.6194708943367004\n",
      "cnt: 0 - valLoss: 0.6133808493614197 - trainLoss: 0.619464635848999\n",
      "cnt: 0 - valLoss: 0.6133744120597839 - trainLoss: 0.619458794593811\n",
      "cnt: 0 - valLoss: 0.6133688688278198 - trainLoss: 0.6194522976875305\n",
      "cnt: 0 - valLoss: 0.6133626699447632 - trainLoss: 0.6194465756416321\n",
      "cnt: 0 - valLoss: 0.6133564114570618 - trainLoss: 0.6194402575492859\n",
      "cnt: 0 - valLoss: 0.6133503317832947 - trainLoss: 0.6194343566894531\n",
      "cnt: 0 - valLoss: 0.6133444905281067 - trainLoss: 0.6194280982017517\n",
      "cnt: 0 - valLoss: 0.6133385896682739 - trainLoss: 0.6194220781326294\n",
      "cnt: 0 - valLoss: 0.6133325099945068 - trainLoss: 0.6194159388542175\n",
      "cnt: 0 - valLoss: 0.6133261919021606 - trainLoss: 0.6194097995758057\n",
      "cnt: 0 - valLoss: 0.613319993019104 - trainLoss: 0.6194038391113281\n",
      "cnt: 0 - valLoss: 0.6133144497871399 - trainLoss: 0.6193975806236267\n",
      "cnt: 0 - valLoss: 0.6133080720901489 - trainLoss: 0.619391679763794\n",
      "cnt: 0 - valLoss: 0.6133027076721191 - trainLoss: 0.619385302066803\n",
      "cnt: 0 - valLoss: 0.6132956743240356 - trainLoss: 0.619379460811615\n",
      "cnt: 0 - valLoss: 0.613290011882782 - trainLoss: 0.6193731427192688\n",
      "cnt: 0 - valLoss: 0.6132838129997253 - trainLoss: 0.6193673014640808\n",
      "cnt: 0 - valLoss: 0.6132780909538269 - trainLoss: 0.6193609833717346\n",
      "cnt: 0 - valLoss: 0.6132720708847046 - trainLoss: 0.6193549633026123\n",
      "cnt: 0 - valLoss: 0.6132655739784241 - trainLoss: 0.6193488240242004\n",
      "cnt: 0 - valLoss: 0.6132597327232361 - trainLoss: 0.6193427443504333\n",
      "cnt: 0 - valLoss: 0.613253653049469 - trainLoss: 0.6193365454673767\n",
      "cnt: 0 - valLoss: 0.6132479310035706 - trainLoss: 0.6193304061889648\n",
      "cnt: 0 - valLoss: 0.6132416725158691 - trainLoss: 0.6193243861198425\n",
      "cnt: 0 - valLoss: 0.6132355332374573 - trainLoss: 0.6193181276321411\n",
      "cnt: 0 - valLoss: 0.6132290959358215 - trainLoss: 0.6193122863769531\n",
      "cnt: 0 - valLoss: 0.6132237315177917 - trainLoss: 0.6193058490753174\n",
      "cnt: 0 - valLoss: 0.6132172346115112 - trainLoss: 0.6193000078201294\n",
      "cnt: 0 - valLoss: 0.6132116317749023 - trainLoss: 0.6192935705184937\n",
      "cnt: 0 - valLoss: 0.6132049560546875 - trainLoss: 0.6192877292633057\n",
      "cnt: 0 - valLoss: 0.6131991147994995 - trainLoss: 0.619281530380249\n",
      "cnt: 0 - valLoss: 0.613193154335022 - trainLoss: 0.6192754507064819\n",
      "cnt: 0 - valLoss: 0.6131871342658997 - trainLoss: 0.6192693114280701\n",
      "cnt: 0 - valLoss: 0.6131813526153564 - trainLoss: 0.6192631125450134\n",
      "cnt: 0 - valLoss: 0.613174557685852 - trainLoss: 0.6192570924758911\n",
      "cnt: 0 - valLoss: 0.6131688952445984 - trainLoss: 0.6192508935928345\n",
      "cnt: 0 - valLoss: 0.613162636756897 - trainLoss: 0.6192448735237122\n",
      "cnt: 0 - valLoss: 0.6131570935249329 - trainLoss: 0.6192384958267212\n",
      "cnt: 0 - valLoss: 0.6131507158279419 - trainLoss: 0.6192326545715332\n",
      "cnt: 0 - valLoss: 0.6131444573402405 - trainLoss: 0.6192262172698975\n",
      "cnt: 0 - valLoss: 0.6131382584571838 - trainLoss: 0.6192203760147095\n",
      "cnt: 0 - valLoss: 0.6131325364112854 - trainLoss: 0.6192139983177185\n",
      "cnt: 0 - valLoss: 0.6131264567375183 - trainLoss: 0.6192079782485962\n",
      "cnt: 0 - valLoss: 0.6131205558776855 - trainLoss: 0.6192017197608948\n",
      "cnt: 0 - valLoss: 0.6131141185760498 - trainLoss: 0.6191956400871277\n",
      "cnt: 0 - valLoss: 0.6131079792976379 - trainLoss: 0.6191895604133606\n",
      "cnt: 0 - valLoss: 0.6131022572517395 - trainLoss: 0.619183361530304\n",
      "cnt: 0 - valLoss: 0.6130959391593933 - trainLoss: 0.6191772818565369\n",
      "cnt: 0 - valLoss: 0.613090455532074 - trainLoss: 0.6191709041595459\n",
      "cnt: 0 - valLoss: 0.6130834817886353 - trainLoss: 0.6191651225090027\n",
      "cnt: 0 - valLoss: 0.6130779981613159 - trainLoss: 0.6191586852073669\n",
      "cnt: 0 - valLoss: 0.6130717992782593 - trainLoss: 0.619152843952179\n",
      "cnt: 0 - valLoss: 0.6130661964416504 - trainLoss: 0.619146466255188\n",
      "cnt: 0 - valLoss: 0.6130596399307251 - trainLoss: 0.6191405653953552\n",
      "cnt: 0 - valLoss: 0.6130537986755371 - trainLoss: 0.6191343069076538\n",
      "cnt: 0 - valLoss: 0.6130479574203491 - trainLoss: 0.6191282272338867\n",
      "cnt: 0 - valLoss: 0.6130419969558716 - trainLoss: 0.6191220879554749\n",
      "cnt: 0 - valLoss: 0.6130363345146179 - trainLoss: 0.6191158890724182\n",
      "cnt: 0 - valLoss: 0.6130295991897583 - trainLoss: 0.6191098690032959\n",
      "cnt: 0 - valLoss: 0.613024115562439 - trainLoss: 0.6191036701202393\n",
      "cnt: 0 - valLoss: 0.613017737865448 - trainLoss: 0.6190977096557617\n",
      "cnt: 0 - valLoss: 0.6130124926567078 - trainLoss: 0.6190912127494812\n",
      "cnt: 0 - valLoss: 0.6130061745643616 - trainLoss: 0.619085431098938\n",
      "cnt: 0 - valLoss: 0.6130005121231079 - trainLoss: 0.6190789937973022\n",
      "cnt: 0 - valLoss: 0.6129938960075378 - trainLoss: 0.6190731525421143\n",
      "cnt: 0 - valLoss: 0.6129881143569946 - trainLoss: 0.6190668344497681\n",
      "cnt: 0 - valLoss: 0.6129822731018066 - trainLoss: 0.6190608143806458\n",
      "cnt: 0 - valLoss: 0.6129763126373291 - trainLoss: 0.6190546154975891\n",
      "cnt: 0 - valLoss: 0.6129706501960754 - trainLoss: 0.6190484762191772\n",
      "cnt: 0 - valLoss: 0.6129639148712158 - trainLoss: 0.6190423369407654\n",
      "cnt: 0 - valLoss: 0.6129584312438965 - trainLoss: 0.6190361976623535\n",
      "cnt: 0 - valLoss: 0.6129521131515503 - trainLoss: 0.6190301775932312\n",
      "cnt: 0 - valLoss: 0.6129465103149414 - trainLoss: 0.6190237998962402\n",
      "cnt: 0 - valLoss: 0.6129399538040161 - trainLoss: 0.6190179586410522\n",
      "cnt: 0 - valLoss: 0.6129341721534729 - trainLoss: 0.6190115809440613\n",
      "cnt: 0 - valLoss: 0.6129279136657715 - trainLoss: 0.6190057396888733\n",
      "cnt: 0 - valLoss: 0.6129218935966492 - trainLoss: 0.6189993619918823\n",
      "cnt: 0 - valLoss: 0.6129158139228821 - trainLoss: 0.6189934015274048\n",
      "cnt: 0 - valLoss: 0.6129096746444702 - trainLoss: 0.6189872026443481\n",
      "cnt: 0 - valLoss: 0.6129037141799927 - trainLoss: 0.618981122970581\n",
      "cnt: 0 - valLoss: 0.6128973960876465 - trainLoss: 0.6189749836921692\n",
      "cnt: 0 - valLoss: 0.6128916144371033 - trainLoss: 0.6189687848091125\n",
      "cnt: 0 - valLoss: 0.6128851771354675 - trainLoss: 0.618962824344635\n",
      "cnt: 0 - valLoss: 0.6128795742988586 - trainLoss: 0.6189565062522888\n",
      "cnt: 0 - valLoss: 0.6128730177879333 - trainLoss: 0.618950605392456\n",
      "cnt: 0 - valLoss: 0.6128671765327454 - trainLoss: 0.6189441680908203\n",
      "cnt: 0 - valLoss: 0.6128608584403992 - trainLoss: 0.6189383268356323\n",
      "cnt: 0 - valLoss: 0.6128549575805664 - trainLoss: 0.6189320087432861\n",
      "cnt: 0 - valLoss: 0.6128488183021545 - trainLoss: 0.6189259886741638\n",
      "cnt: 0 - valLoss: 0.6128427386283875 - trainLoss: 0.6189197897911072\n",
      "cnt: 0 - valLoss: 0.6128367781639099 - trainLoss: 0.6189137101173401\n",
      "cnt: 0 - valLoss: 0.612830400466919 - trainLoss: 0.6189075708389282\n",
      "cnt: 0 - valLoss: 0.6128246188163757 - trainLoss: 0.618901252746582\n",
      "cnt: 0 - valLoss: 0.61281818151474 - trainLoss: 0.6188953518867493\n",
      "cnt: 0 - valLoss: 0.6128125190734863 - trainLoss: 0.6188889741897583\n",
      "cnt: 0 - valLoss: 0.6128060221672058 - trainLoss: 0.6188830733299255\n",
      "cnt: 0 - valLoss: 0.6128002405166626 - trainLoss: 0.6188766956329346\n",
      "cnt: 0 - valLoss: 0.6127939224243164 - trainLoss: 0.6188707947731018\n",
      "cnt: 0 - valLoss: 0.6127879023551941 - trainLoss: 0.6188644170761108\n",
      "cnt: 0 - valLoss: 0.6127817630767822 - trainLoss: 0.6188584566116333\n",
      "cnt: 0 - valLoss: 0.6127756237983704 - trainLoss: 0.6188521981239319\n",
      "cnt: 0 - valLoss: 0.6127697229385376 - trainLoss: 0.6188461184501648\n",
      "cnt: 0 - valLoss: 0.6127634644508362 - trainLoss: 0.6188399195671082\n",
      "cnt: 0 - valLoss: 0.6127576231956482 - trainLoss: 0.6188337206840515\n",
      "cnt: 0 - valLoss: 0.6127511262893677 - trainLoss: 0.618827760219574\n",
      "cnt: 0 - valLoss: 0.612745463848114 - trainLoss: 0.618821382522583\n",
      "cnt: 0 - valLoss: 0.6127389669418335 - trainLoss: 0.6188154816627502\n",
      "cnt: 0 - valLoss: 0.6127332448959351 - trainLoss: 0.6188090443611145\n",
      "cnt: 0 - valLoss: 0.6127268075942993 - trainLoss: 0.6188032031059265\n",
      "cnt: 0 - valLoss: 0.6127209067344666 - trainLoss: 0.6187968254089355\n",
      "cnt: 0 - valLoss: 0.6127147674560547 - trainLoss: 0.6187908053398132\n",
      "cnt: 0 - valLoss: 0.6127086281776428 - trainLoss: 0.618784487247467\n",
      "cnt: 0 - valLoss: 0.6127026677131653 - trainLoss: 0.6187784671783447\n",
      "cnt: 0 - valLoss: 0.6126963496208191 - trainLoss: 0.6187722682952881\n",
      "cnt: 0 - valLoss: 0.6126905679702759 - trainLoss: 0.6187660098075867\n",
      "cnt: 0 - valLoss: 0.6126840710639954 - trainLoss: 0.6187599897384644\n",
      "cnt: 0 - valLoss: 0.6126784086227417 - trainLoss: 0.6187536120414734\n",
      "cnt: 0 - valLoss: 0.6126718521118164 - trainLoss: 0.6187477111816406\n",
      "cnt: 0 - valLoss: 0.6126660108566284 - trainLoss: 0.6187412142753601\n",
      "cnt: 0 - valLoss: 0.6126596331596375 - trainLoss: 0.6187353134155273\n",
      "cnt: 0 - valLoss: 0.6126537322998047 - trainLoss: 0.6187289357185364\n",
      "cnt: 0 - valLoss: 0.6126474738121033 - trainLoss: 0.6187228560447693\n",
      "cnt: 0 - valLoss: 0.6126413345336914 - trainLoss: 0.6187165975570679\n",
      "cnt: 0 - valLoss: 0.6126352548599243 - trainLoss: 0.6187105774879456\n",
      "cnt: 0 - valLoss: 0.6126288175582886 - trainLoss: 0.6187044978141785\n",
      "cnt: 0 - valLoss: 0.6126229166984558 - trainLoss: 0.6186982989311218\n",
      "cnt: 0 - valLoss: 0.6126163601875305 - trainLoss: 0.6186923384666443\n",
      "cnt: 0 - valLoss: 0.6126106381416321 - trainLoss: 0.6186859607696533\n",
      "cnt: 0 - valLoss: 0.612604022026062 - trainLoss: 0.6186801791191101\n",
      "cnt: 0 - valLoss: 0.6125981211662292 - trainLoss: 0.6186738014221191\n",
      "cnt: 0 - valLoss: 0.6125916838645935 - trainLoss: 0.6186680197715759\n",
      "cnt: 0 - valLoss: 0.6125856041908264 - trainLoss: 0.6186617016792297\n",
      "cnt: 0 - valLoss: 0.6125794053077698 - trainLoss: 0.6186556816101074\n",
      "cnt: 0 - valLoss: 0.6125731468200684 - trainLoss: 0.6186495423316956\n",
      "cnt: 0 - valLoss: 0.6125670075416565 - trainLoss: 0.6186434626579285\n",
      "cnt: 0 - valLoss: 0.6125605702400208 - trainLoss: 0.6186373829841614\n",
      "cnt: 0 - valLoss: 0.6125546097755432 - trainLoss: 0.6186311841011047\n",
      "cnt: 0 - valLoss: 0.6125479936599731 - trainLoss: 0.6186253428459167\n",
      "cnt: 0 - valLoss: 0.6125421524047852 - trainLoss: 0.6186190247535706\n",
      "cnt: 0 - valLoss: 0.6125354170799255 - trainLoss: 0.6186133027076721\n",
      "cnt: 0 - valLoss: 0.6125293970108032 - trainLoss: 0.6186069846153259\n",
      "cnt: 0 - valLoss: 0.6125229001045227 - trainLoss: 0.6186012029647827\n",
      "cnt: 0 - valLoss: 0.6125167012214661 - trainLoss: 0.6185950040817261\n",
      "cnt: 0 - valLoss: 0.6125104427337646 - trainLoss: 0.6185889840126038\n",
      "cnt: 0 - valLoss: 0.6125040650367737 - trainLoss: 0.6185829043388367\n",
      "cnt: 0 - valLoss: 0.6124979257583618 - trainLoss: 0.6185769438743591\n",
      "cnt: 0 - valLoss: 0.6124914884567261 - trainLoss: 0.6185709238052368\n",
      "cnt: 0 - valLoss: 0.6124855279922485 - trainLoss: 0.618564784526825\n",
      "cnt: 0 - valLoss: 0.6124788522720337 - trainLoss: 0.6185588836669922\n",
      "cnt: 0 - valLoss: 0.6124730706214905 - trainLoss: 0.6185526251792908\n",
      "cnt: 0 - valLoss: 0.6124663949012756 - trainLoss: 0.6185469031333923\n",
      "cnt: 0 - valLoss: 0.6124603748321533 - trainLoss: 0.6185405254364014\n",
      "cnt: 0 - valLoss: 0.6124539375305176 - trainLoss: 0.6185347437858582\n",
      "cnt: 0 - valLoss: 0.6124477386474609 - trainLoss: 0.6185285449028015\n",
      "cnt: 0 - valLoss: 0.6124414801597595 - trainLoss: 0.618522584438324\n",
      "cnt: 0 - valLoss: 0.6124351620674133 - trainLoss: 0.6185164451599121\n",
      "cnt: 0 - valLoss: 0.612429141998291 - trainLoss: 0.6185104250907898\n",
      "cnt: 0 - valLoss: 0.6124225854873657 - trainLoss: 0.6185044646263123\n",
      "cnt: 0 - valLoss: 0.6124167442321777 - trainLoss: 0.6184982061386108\n",
      "cnt: 0 - valLoss: 0.6124099493026733 - trainLoss: 0.6184924244880676\n",
      "cnt: 0 - valLoss: 0.6124042868614197 - trainLoss: 0.6184860467910767\n",
      "cnt: 0 - valLoss: 0.6123975515365601 - trainLoss: 0.6184803247451782\n",
      "cnt: 0 - valLoss: 0.6123915910720825 - trainLoss: 0.6184739470481873\n",
      "cnt: 0 - valLoss: 0.6123851537704468 - trainLoss: 0.618468165397644\n",
      "cnt: 0 - valLoss: 0.6123790144920349 - trainLoss: 0.6184618473052979\n",
      "cnt: 0 - valLoss: 0.6123728156089783 - trainLoss: 0.6184560060501099\n",
      "cnt: 0 - valLoss: 0.6123664379119873 - trainLoss: 0.618449866771698\n",
      "cnt: 0 - valLoss: 0.612360417842865 - trainLoss: 0.6184437870979309\n",
      "cnt: 0 - valLoss: 0.6123539209365845 - trainLoss: 0.6184378266334534\n",
      "cnt: 0 - valLoss: 0.6123480796813965 - trainLoss: 0.6184316277503967\n",
      "cnt: 0 - valLoss: 0.6123414039611816 - trainLoss: 0.618425726890564\n",
      "cnt: 0 - valLoss: 0.612335741519928 - trainLoss: 0.6184194087982178\n",
      "cnt: 0 - valLoss: 0.6123290061950684 - trainLoss: 0.6184136867523193\n",
      "cnt: 0 - valLoss: 0.6123230457305908 - trainLoss: 0.6184073686599731\n",
      "cnt: 0 - valLoss: 0.6123166680335999 - trainLoss: 0.6184014678001404\n",
      "cnt: 0 - valLoss: 0.6123104691505432 - trainLoss: 0.6183952689170837\n",
      "cnt: 0 - valLoss: 0.6123043298721313 - trainLoss: 0.6183893084526062\n",
      "cnt: 0 - valLoss: 0.6122980117797852 - trainLoss: 0.6183832287788391\n",
      "cnt: 0 - valLoss: 0.6122919321060181 - trainLoss: 0.618377149105072\n",
      "cnt: 0 - valLoss: 0.6122854351997375 - trainLoss: 0.6183711290359497\n",
      "cnt: 0 - valLoss: 0.6122795939445496 - trainLoss: 0.6183648705482483\n",
      "cnt: 0 - valLoss: 0.6122728586196899 - trainLoss: 0.6183590888977051\n",
      "cnt: 0 - valLoss: 0.6122670769691467 - trainLoss: 0.6183527112007141\n",
      "cnt: 0 - valLoss: 0.6122605204582214 - trainLoss: 0.6183469295501709\n",
      "cnt: 0 - valLoss: 0.6122545599937439 - trainLoss: 0.6183406114578247\n",
      "cnt: 0 - valLoss: 0.6122481822967529 - trainLoss: 0.6183347702026367\n",
      "cnt: 0 - valLoss: 0.6122419238090515 - trainLoss: 0.6183285117149353\n",
      "cnt: 0 - valLoss: 0.6122358441352844 - trainLoss: 0.618322491645813\n",
      "cnt: 0 - valLoss: 0.6122294068336487 - trainLoss: 0.6183164119720459\n",
      "cnt: 0 - valLoss: 0.6122233271598816 - trainLoss: 0.6183103322982788\n",
      "cnt: 0 - valLoss: 0.6122167706489563 - trainLoss: 0.6183043718338013\n",
      "cnt: 0 - valLoss: 0.6122109293937683 - trainLoss: 0.6182981729507446\n",
      "cnt: 0 - valLoss: 0.6122041344642639 - trainLoss: 0.6182923913002014\n",
      "cnt: 0 - valLoss: 0.6121982336044312 - trainLoss: 0.6182860732078552\n",
      "cnt: 0 - valLoss: 0.6121916770935059 - trainLoss: 0.618280291557312\n",
      "cnt: 0 - valLoss: 0.6121855974197388 - trainLoss: 0.6182740330696106\n",
      "cnt: 0 - valLoss: 0.612179160118103 - trainLoss: 0.6182682514190674\n",
      "cnt: 0 - valLoss: 0.6121729016304016 - trainLoss: 0.6182620525360107\n",
      "cnt: 0 - valLoss: 0.612166702747345 - trainLoss: 0.6182560920715332\n",
      "cnt: 0 - valLoss: 0.6121602654457092 - trainLoss: 0.6182500720024109\n",
      "cnt: 0 - valLoss: 0.6121541857719421 - trainLoss: 0.618243932723999\n",
      "cnt: 0 - valLoss: 0.6121475696563721 - trainLoss: 0.6182380318641663\n",
      "cnt: 0 - valLoss: 0.6121417284011841 - trainLoss: 0.6182317137718201\n",
      "cnt: 0 - valLoss: 0.6121349334716797 - trainLoss: 0.6182259321212769\n",
      "cnt: 0 - valLoss: 0.6121290326118469 - trainLoss: 0.6182196140289307\n",
      "cnt: 0 - valLoss: 0.6121225357055664 - trainLoss: 0.6182138919830322\n",
      "cnt: 0 - valLoss: 0.6121164560317993 - trainLoss: 0.618207573890686\n",
      "cnt: 0 - valLoss: 0.6121100187301636 - trainLoss: 0.618201732635498\n",
      "cnt: 0 - valLoss: 0.6121037602424622 - trainLoss: 0.6181955933570862\n",
      "cnt: 0 - valLoss: 0.6120975017547607 - trainLoss: 0.6181895732879639\n",
      "cnt: 0 - valLoss: 0.612091064453125 - trainLoss: 0.6181834936141968\n",
      "cnt: 0 - valLoss: 0.6120850443840027 - trainLoss: 0.6181774139404297\n",
      "cnt: 0 - valLoss: 0.6120783686637878 - trainLoss: 0.6181715130805969\n",
      "cnt: 0 - valLoss: 0.6120725274085999 - trainLoss: 0.6181651949882507\n",
      "cnt: 0 - valLoss: 0.6120657920837402 - trainLoss: 0.6181594133377075\n",
      "cnt: 0 - valLoss: 0.6120598316192627 - trainLoss: 0.6181530952453613\n",
      "cnt: 0 - valLoss: 0.6120533347129822 - trainLoss: 0.6181473135948181\n",
      "cnt: 0 - valLoss: 0.6120471954345703 - trainLoss: 0.6181410551071167\n",
      "cnt: 0 - valLoss: 0.6120408773422241 - trainLoss: 0.6181351542472839\n",
      "cnt: 0 - valLoss: 0.6120345592498779 - trainLoss: 0.6181290745735168\n",
      "cnt: 0 - valLoss: 0.6120283603668213 - trainLoss: 0.618122935295105\n",
      "cnt: 0 - valLoss: 0.6120218634605408 - trainLoss: 0.6181169152259827\n",
      "cnt: 0 - valLoss: 0.6120159029960632 - trainLoss: 0.618110716342926\n",
      "cnt: 0 - valLoss: 0.6120092272758484 - trainLoss: 0.618104875087738\n",
      "cnt: 0 - valLoss: 0.6120033860206604 - trainLoss: 0.6180985569953918\n",
      "cnt: 0 - valLoss: 0.6119967103004456 - trainLoss: 0.6180927753448486\n",
      "cnt: 0 - valLoss: 0.6119906902313232 - trainLoss: 0.6180864572525024\n",
      "cnt: 0 - valLoss: 0.6119841933250427 - trainLoss: 0.6180806159973145\n",
      "cnt: 0 - valLoss: 0.6119779944419861 - trainLoss: 0.6180744171142578\n",
      "cnt: 0 - valLoss: 0.6119716763496399 - trainLoss: 0.6180684566497803\n",
      "cnt: 0 - valLoss: 0.6119652986526489 - trainLoss: 0.6180622577667236\n",
      "cnt: 0 - valLoss: 0.6119591593742371 - trainLoss: 0.6180561780929565\n",
      "cnt: 0 - valLoss: 0.6119526028633118 - trainLoss: 0.618050217628479\n",
      "cnt: 0 - valLoss: 0.6119466423988342 - trainLoss: 0.6180440187454224\n",
      "cnt: 0 - valLoss: 0.6119399070739746 - trainLoss: 0.6180380582809448\n",
      "cnt: 0 - valLoss: 0.6119341254234314 - trainLoss: 0.6180317401885986\n",
      "cnt: 0 - valLoss: 0.611927330493927 - trainLoss: 0.6180259585380554\n",
      "cnt: 0 - valLoss: 0.6119213104248047 - trainLoss: 0.6180196404457092\n",
      "cnt: 0 - valLoss: 0.6119148135185242 - trainLoss: 0.6180137991905212\n",
      "cnt: 0 - valLoss: 0.6119086146354675 - trainLoss: 0.618007481098175\n",
      "cnt: 0 - valLoss: 0.6119022965431213 - trainLoss: 0.6180015206336975\n",
      "cnt: 0 - valLoss: 0.6118958592414856 - trainLoss: 0.6179954409599304\n",
      "cnt: 0 - valLoss: 0.6118897795677185 - trainLoss: 0.6179893016815186\n",
      "cnt: 0 - valLoss: 0.6118831634521484 - trainLoss: 0.6179832816123962\n",
      "cnt: 0 - valLoss: 0.6118772625923157 - trainLoss: 0.6179770231246948\n",
      "cnt: 0 - valLoss: 0.611870527267456 - trainLoss: 0.6179711818695068\n",
      "cnt: 0 - valLoss: 0.6118647456169128 - trainLoss: 0.6179648041725159\n",
      "cnt: 0 - valLoss: 0.6118580102920532 - trainLoss: 0.6179590225219727\n",
      "cnt: 0 - valLoss: 0.6118518710136414 - trainLoss: 0.6179526448249817\n",
      "cnt: 0 - valLoss: 0.6118454337120056 - trainLoss: 0.6179468035697937\n",
      "cnt: 0 - valLoss: 0.6118391156196594 - trainLoss: 0.6179405450820923\n",
      "cnt: 0 - valLoss: 0.6118329167366028 - trainLoss: 0.61793452501297\n",
      "cnt: 0 - valLoss: 0.6118264198303223 - trainLoss: 0.6179283857345581\n",
      "cnt: 0 - valLoss: 0.6118203401565552 - trainLoss: 0.6179222464561462\n",
      "cnt: 0 - valLoss: 0.6118137240409851 - trainLoss: 0.6179162859916687\n",
      "cnt: 0 - valLoss: 0.6118078827857971 - trainLoss: 0.6179099678993225\n",
      "cnt: 0 - valLoss: 0.6118010878562927 - trainLoss: 0.6179040670394897\n",
      "cnt: 0 - valLoss: 0.6117951273918152 - trainLoss: 0.6178976893424988\n",
      "cnt: 0 - valLoss: 0.6117885708808899 - trainLoss: 0.6178919076919556\n",
      "cnt: 0 - valLoss: 0.611782431602478 - trainLoss: 0.6178855895996094\n",
      "cnt: 0 - valLoss: 0.6117760539054871 - trainLoss: 0.6178796887397766\n",
      "cnt: 0 - valLoss: 0.6117697358131409 - trainLoss: 0.6178733706474304\n",
      "cnt: 0 - valLoss: 0.6117635369300842 - trainLoss: 0.6178673505783081\n",
      "cnt: 0 - valLoss: 0.6117569804191589 - trainLoss: 0.617861270904541\n",
      "cnt: 0 - valLoss: 0.6117509007453918 - trainLoss: 0.6178551316261292\n",
      "cnt: 0 - valLoss: 0.6117441654205322 - trainLoss: 0.6178492307662964\n",
      "cnt: 0 - valLoss: 0.6117383241653442 - trainLoss: 0.6178428530693054\n",
      "cnt: 0 - valLoss: 0.6117314696311951 - trainLoss: 0.617837131023407\n",
      "cnt: 0 - valLoss: 0.6117255687713623 - trainLoss: 0.6178306937217712\n",
      "cnt: 0 - valLoss: 0.6117188334465027 - trainLoss: 0.6178249716758728\n",
      "cnt: 0 - valLoss: 0.6117126941680908 - trainLoss: 0.6178185939788818\n",
      "cnt: 0 - valLoss: 0.6117063164710999 - trainLoss: 0.6178127527236938\n",
      "cnt: 0 - valLoss: 0.6116999387741089 - trainLoss: 0.6178064942359924\n",
      "cnt: 0 - valLoss: 0.6116936802864075 - trainLoss: 0.6178004741668701\n",
      "cnt: 0 - valLoss: 0.6116871237754822 - trainLoss: 0.6177944540977478\n",
      "cnt: 0 - valLoss: 0.6116811633110046 - trainLoss: 0.6177881956100464\n",
      "cnt: 0 - valLoss: 0.611674427986145 - trainLoss: 0.6177822947502136\n",
      "cnt: 0 - valLoss: 0.6116685271263123 - trainLoss: 0.6177759766578674\n",
      "cnt: 0 - valLoss: 0.6116616725921631 - trainLoss: 0.6177701354026794\n",
      "cnt: 0 - valLoss: 0.6116556525230408 - trainLoss: 0.6177637577056885\n",
      "cnt: 0 - valLoss: 0.6116491556167603 - trainLoss: 0.6177579760551453\n",
      "cnt: 0 - valLoss: 0.6116428971290588 - trainLoss: 0.6177516579627991\n",
      "cnt: 0 - valLoss: 0.6116365790367126 - trainLoss: 0.6177456974983215\n",
      "cnt: 0 - valLoss: 0.6116301417350769 - trainLoss: 0.6177395582199097\n",
      "cnt: 0 - valLoss: 0.6116239428520203 - trainLoss: 0.6177334189414978\n",
      "cnt: 0 - valLoss: 0.6116173267364502 - trainLoss: 0.6177273988723755\n",
      "cnt: 0 - valLoss: 0.6116113662719727 - trainLoss: 0.6177211999893188\n",
      "cnt: 0 - valLoss: 0.6116045713424683 - trainLoss: 0.6177152991294861\n",
      "cnt: 0 - valLoss: 0.6115987300872803 - trainLoss: 0.6177089214324951\n",
      "cnt: 0 - valLoss: 0.6115919351577759 - trainLoss: 0.6177031397819519\n",
      "cnt: 0 - valLoss: 0.6115858554840088 - trainLoss: 0.6176967024803162\n",
      "cnt: 0 - valLoss: 0.6115793585777283 - trainLoss: 0.617690920829773\n",
      "cnt: 0 - valLoss: 0.6115731000900269 - trainLoss: 0.6176846027374268\n",
      "cnt: 0 - valLoss: 0.6115667223930359 - trainLoss: 0.6176785826683044\n",
      "cnt: 0 - valLoss: 0.6115602850914001 - trainLoss: 0.6176725029945374\n",
      "cnt: 0 - valLoss: 0.6115542054176331 - trainLoss: 0.6176663041114807\n",
      "cnt: 0 - valLoss: 0.6115474700927734 - trainLoss: 0.6176602840423584\n",
      "cnt: 0 - valLoss: 0.6115415096282959 - trainLoss: 0.617654025554657\n",
      "cnt: 0 - valLoss: 0.6115347146987915 - trainLoss: 0.6176481246948242\n",
      "cnt: 0 - valLoss: 0.6115289926528931 - trainLoss: 0.6176416873931885\n",
      "cnt: 0 - valLoss: 0.6115221381187439 - trainLoss: 0.6176359057426453\n",
      "cnt: 0 - valLoss: 0.6115160584449768 - trainLoss: 0.6176295280456543\n",
      "cnt: 0 - valLoss: 0.6115095019340515 - trainLoss: 0.6176236271858215\n",
      "cnt: 0 - valLoss: 0.6115032434463501 - trainLoss: 0.6176173686981201\n",
      "cnt: 0 - valLoss: 0.6114969849586487 - trainLoss: 0.6176113486289978\n",
      "cnt: 0 - valLoss: 0.6114900708198547 - trainLoss: 0.6176051497459412\n",
      "cnt: 0 - valLoss: 0.611483633518219 - trainLoss: 0.6175988912582397\n",
      "cnt: 0 - valLoss: 0.6114765405654907 - trainLoss: 0.6175928115844727\n",
      "cnt: 0 - valLoss: 0.6114703416824341 - trainLoss: 0.6175863146781921\n",
      "cnt: 0 - valLoss: 0.6114631295204163 - trainLoss: 0.6175803542137146\n",
      "cnt: 0 - valLoss: 0.6114566326141357 - trainLoss: 0.6175738573074341\n",
      "cnt: 0 - valLoss: 0.6114498376846313 - trainLoss: 0.6175678372383118\n",
      "cnt: 0 - valLoss: 0.6114431023597717 - trainLoss: 0.6175615191459656\n",
      "cnt: 0 - valLoss: 0.6114364862442017 - trainLoss: 0.6175553202629089\n",
      "cnt: 0 - valLoss: 0.6114290952682495 - trainLoss: 0.617548942565918\n",
      "cnt: 0 - valLoss: 0.6114224791526794 - trainLoss: 0.6175422072410583\n",
      "cnt: 0 - valLoss: 0.6114150881767273 - trainLoss: 0.6175356507301331\n",
      "cnt: 0 - valLoss: 0.6114081144332886 - trainLoss: 0.6175287961959839\n",
      "cnt: 0 - valLoss: 0.6114011406898499 - trainLoss: 0.6175222992897034\n",
      "cnt: 0 - valLoss: 0.6113938093185425 - trainLoss: 0.617515504360199\n",
      "cnt: 0 - valLoss: 0.6113871932029724 - trainLoss: 0.6175087690353394\n",
      "cnt: 0 - valLoss: 0.6113798022270203 - trainLoss: 0.6175023317337036\n",
      "cnt: 0 - valLoss: 0.6113728880882263 - trainLoss: 0.6174954175949097\n",
      "cnt: 0 - valLoss: 0.6113658547401428 - trainLoss: 0.6174889206886292\n",
      "cnt: 0 - valLoss: 0.6113588213920593 - trainLoss: 0.6174821853637695\n",
      "cnt: 0 - valLoss: 0.6113523840904236 - trainLoss: 0.6174755096435547\n",
      "cnt: 0 - valLoss: 0.611345112323761 - trainLoss: 0.617469072341919\n",
      "cnt: 0 - valLoss: 0.6113384962081909 - trainLoss: 0.6174622178077698\n",
      "cnt: 0 - valLoss: 0.6113316416740417 - trainLoss: 0.6174558401107788\n",
      "cnt: 0 - valLoss: 0.611324667930603 - trainLoss: 0.6174491047859192\n",
      "cnt: 0 - valLoss: 0.6113181114196777 - trainLoss: 0.6174424290657043\n",
      "cnt: 0 - valLoss: 0.6113109588623047 - trainLoss: 0.6174359321594238\n",
      "cnt: 0 - valLoss: 0.6113044023513794 - trainLoss: 0.6174290776252747\n",
      "cnt: 0 - valLoss: 0.6112974286079407 - trainLoss: 0.6174227595329285\n",
      "cnt: 0 - valLoss: 0.6112905740737915 - trainLoss: 0.6174159049987793\n",
      "cnt: 0 - valLoss: 0.6112839579582214 - trainLoss: 0.6174094080924988\n",
      "cnt: 0 - valLoss: 0.6112768054008484 - trainLoss: 0.6174027919769287\n",
      "cnt: 0 - valLoss: 0.6112704873085022 - trainLoss: 0.6173959970474243\n",
      "cnt: 0 - valLoss: 0.6112632155418396 - trainLoss: 0.6173896193504333\n",
      "cnt: 0 - valLoss: 0.61125648021698 - trainLoss: 0.617382824420929\n",
      "cnt: 0 - valLoss: 0.6112497448921204 - trainLoss: 0.6173762679100037\n",
      "cnt: 0 - valLoss: 0.6112427115440369 - trainLoss: 0.6173696517944336\n",
      "cnt: 0 - valLoss: 0.6112363338470459 - trainLoss: 0.617362916469574\n",
      "cnt: 0 - valLoss: 0.6112291216850281 - trainLoss: 0.6173564791679382\n",
      "cnt: 0 - valLoss: 0.6112224459648132 - trainLoss: 0.6173496842384338\n",
      "cnt: 0 - valLoss: 0.6112156510353088 - trainLoss: 0.6173431873321533\n",
      "cnt: 0 - valLoss: 0.6112086176872253 - trainLoss: 0.6173365116119385\n",
      "cnt: 0 - valLoss: 0.6112021803855896 - trainLoss: 0.6173298358917236\n",
      "cnt: 0 - valLoss: 0.6111949682235718 - trainLoss: 0.6173233389854431\n",
      "cnt: 0 - valLoss: 0.6111884117126465 - trainLoss: 0.617316484451294\n",
      "cnt: 0 - valLoss: 0.6111814975738525 - trainLoss: 0.617310106754303\n",
      "cnt: 0 - valLoss: 0.6111745834350586 - trainLoss: 0.6173033714294434\n",
      "cnt: 0 - valLoss: 0.6111680865287781 - trainLoss: 0.6172967553138733\n",
      "cnt: 0 - valLoss: 0.6111608743667603 - trainLoss: 0.6172901391983032\n",
      "cnt: 0 - valLoss: 0.6111543774604797 - trainLoss: 0.6172833442687988\n",
      "cnt: 0 - valLoss: 0.611147403717041 - trainLoss: 0.6172770261764526\n",
      "cnt: 0 - valLoss: 0.6111406683921814 - trainLoss: 0.6172701716423035\n",
      "cnt: 0 - valLoss: 0.6111339330673218 - trainLoss: 0.6172636151313782\n",
      "cnt: 0 - valLoss: 0.6111268997192383 - trainLoss: 0.6172569394111633\n",
      "cnt: 0 - valLoss: 0.6111205220222473 - trainLoss: 0.6172502040863037\n",
      "cnt: 0 - valLoss: 0.6111133098602295 - trainLoss: 0.617243766784668\n",
      "cnt: 0 - valLoss: 0.6111066937446594 - trainLoss: 0.6172369718551636\n",
      "cnt: 0 - valLoss: 0.6110999584197998 - trainLoss: 0.6172304749488831\n",
      "cnt: 0 - valLoss: 0.6110929250717163 - trainLoss: 0.6172237396240234\n",
      "cnt: 0 - valLoss: 0.6110864877700806 - trainLoss: 0.6172170042991638\n",
      "cnt: 0 - valLoss: 0.6110792756080627 - trainLoss: 0.6172106266021729\n",
      "cnt: 0 - valLoss: 0.6110727787017822 - trainLoss: 0.6172036528587341\n",
      "cnt: 0 - valLoss: 0.6110659241676331 - trainLoss: 0.6171972751617432\n",
      "cnt: 0 - valLoss: 0.6110591292381287 - trainLoss: 0.6171905994415283\n",
      "cnt: 0 - valLoss: 0.6110525727272034 - trainLoss: 0.6171839237213135\n",
      "cnt: 0 - valLoss: 0.6110454797744751 - trainLoss: 0.617177426815033\n",
      "cnt: 0 - valLoss: 0.6110392212867737 - trainLoss: 0.6171705722808838\n",
      "cnt: 0 - valLoss: 0.6110321283340454 - trainLoss: 0.6171641945838928\n",
      "cnt: 0 - valLoss: 0.6110254526138306 - trainLoss: 0.6171573400497437\n",
      "cnt: 0 - valLoss: 0.6110187768936157 - trainLoss: 0.6171508431434631\n",
      "cnt: 0 - valLoss: 0.6110117435455322 - trainLoss: 0.6171441674232483\n",
      "cnt: 0 - valLoss: 0.611005425453186 - trainLoss: 0.6171374320983887\n",
      "cnt: 0 - valLoss: 0.610998272895813 - trainLoss: 0.6171309351921082\n",
      "cnt: 0 - valLoss: 0.6109917163848877 - trainLoss: 0.6171241402626038\n",
      "cnt: 0 - valLoss: 0.6109849810600281 - trainLoss: 0.6171176433563232\n",
      "cnt: 0 - valLoss: 0.6109781265258789 - trainLoss: 0.6171109080314636\n",
      "cnt: 0 - valLoss: 0.6109716296195984 - trainLoss: 0.6171042919158936\n",
      "cnt: 0 - valLoss: 0.6109644770622253 - trainLoss: 0.6170977354049683\n",
      "cnt: 0 - valLoss: 0.6109580397605896 - trainLoss: 0.6170908808708191\n",
      "cnt: 0 - valLoss: 0.6109493970870972 - trainLoss: 0.6170842051506042\n",
      "cnt: 0 - valLoss: 0.6109408140182495 - trainLoss: 0.6170752048492432\n",
      "cnt: 0 - valLoss: 0.6109325885772705 - trainLoss: 0.6170663833618164\n",
      "cnt: 0 - valLoss: 0.6109237670898438 - trainLoss: 0.617057740688324\n",
      "cnt: 0 - valLoss: 0.6109153032302856 - trainLoss: 0.6170489192008972\n",
      "cnt: 0 - valLoss: 0.6109071373939514 - trainLoss: 0.6170402765274048\n",
      "cnt: 0 - valLoss: 0.6108983755111694 - trainLoss: 0.6170316338539124\n",
      "cnt: 0 - valLoss: 0.6108899116516113 - trainLoss: 0.6170228123664856\n",
      "cnt: 0 - valLoss: 0.6108817458152771 - trainLoss: 0.6170142292976379\n",
      "cnt: 0 - valLoss: 0.6108730435371399 - trainLoss: 0.6170055866241455\n",
      "cnt: 0 - valLoss: 0.6108647584915161 - trainLoss: 0.6169968843460083\n",
      "cnt: 0 - valLoss: 0.6108567714691162 - trainLoss: 0.6169886589050293\n",
      "cnt: 0 - valLoss: 0.6108484864234924 - trainLoss: 0.6169806122779846\n",
      "cnt: 0 - valLoss: 0.6108420491218567 - trainLoss: 0.6169724464416504\n",
      "cnt: 0 - valLoss: 0.6108350157737732 - trainLoss: 0.6169646978378296\n",
      "cnt: 0 - valLoss: 0.6108286380767822 - trainLoss: 0.6169565320014954\n",
      "cnt: 0 - valLoss: 0.6108226180076599 - trainLoss: 0.6169486045837402\n",
      "cnt: 0 - valLoss: 0.6108159422874451 - trainLoss: 0.6169406771659851\n",
      "cnt: 0 - valLoss: 0.610809862613678 - trainLoss: 0.6169325709342957\n",
      "cnt: 0 - valLoss: 0.6108034253120422 - trainLoss: 0.6169247627258301\n",
      "cnt: 0 - valLoss: 0.6107970476150513 - trainLoss: 0.6169167160987854\n",
      "cnt: 0 - valLoss: 0.6107910871505737 - trainLoss: 0.6169086694717407\n",
      "cnt: 0 - valLoss: 0.6107843518257141 - trainLoss: 0.6169008016586304\n",
      "cnt: 0 - valLoss: 0.610778272151947 - trainLoss: 0.6168926954269409\n",
      "cnt: 0 - valLoss: 0.6107720136642456 - trainLoss: 0.6168850064277649\n",
      "cnt: 0 - valLoss: 0.6107655167579651 - trainLoss: 0.6168769598007202\n",
      "cnt: 0 - valLoss: 0.6107596158981323 - trainLoss: 0.6168690323829651\n",
      "cnt: 0 - valLoss: 0.6107529401779175 - trainLoss: 0.6168612241744995\n",
      "cnt: 0 - valLoss: 0.6107468605041504 - trainLoss: 0.6168531179428101\n",
      "cnt: 0 - valLoss: 0.610740602016449 - trainLoss: 0.616845428943634\n",
      "cnt: 0 - valLoss: 0.6107341051101685 - trainLoss: 0.6168374419212341\n",
      "cnt: 0 - valLoss: 0.6107282042503357 - trainLoss: 0.616829514503479\n",
      "cnt: 0 - valLoss: 0.6107214689254761 - trainLoss: 0.616821825504303\n",
      "cnt: 0 - valLoss: 0.6107154488563538 - trainLoss: 0.6168137788772583\n",
      "cnt: 0 - valLoss: 0.610709011554718 - trainLoss: 0.6168060898780823\n",
      "cnt: 0 - valLoss: 0.610702395439148 - trainLoss: 0.6167981028556824\n",
      "cnt: 0 - valLoss: 0.6106961965560913 - trainLoss: 0.6167901158332825\n",
      "cnt: 0 - valLoss: 0.6106893420219421 - trainLoss: 0.6167822480201721\n",
      "cnt: 0 - valLoss: 0.6106830835342407 - trainLoss: 0.6167741417884827\n",
      "cnt: 0 - valLoss: 0.6106765270233154 - trainLoss: 0.6167663931846619\n",
      "cnt: 0 - valLoss: 0.6106699109077454 - trainLoss: 0.6167582869529724\n",
      "cnt: 0 - valLoss: 0.610663652420044 - trainLoss: 0.6167504787445068\n",
      "cnt: 0 - valLoss: 0.6106568574905396 - trainLoss: 0.6167425513267517\n",
      "cnt: 0 - valLoss: 0.610650897026062 - trainLoss: 0.6167346835136414\n",
      "cnt: 0 - valLoss: 0.6106441020965576 - trainLoss: 0.6167272329330444\n",
      "cnt: 0 - valLoss: 0.6106377840042114 - trainLoss: 0.6167193651199341\n",
      "cnt: 0 - valLoss: 0.6106314063072205 - trainLoss: 0.6167119145393372\n",
      "cnt: 0 - valLoss: 0.6106247901916504 - trainLoss: 0.6167041659355164\n",
      "cnt: 0 - valLoss: 0.6106186509132385 - trainLoss: 0.6166965365409851\n",
      "cnt: 0 - valLoss: 0.6106120944023132 - trainLoss: 0.6166890859603882\n",
      "cnt: 0 - valLoss: 0.6106057167053223 - trainLoss: 0.6166815161705017\n",
      "cnt: 0 - valLoss: 0.6105984449386597 - trainLoss: 0.6166746616363525\n",
      "cnt: 0 - valLoss: 0.6105917692184448 - trainLoss: 0.6166673898696899\n",
      "cnt: 0 - valLoss: 0.6105847954750061 - trainLoss: 0.616660475730896\n",
      "cnt: 0 - valLoss: 0.6105778813362122 - trainLoss: 0.616653323173523\n",
      "cnt: 0 - valLoss: 0.6105712652206421 - trainLoss: 0.6166463494300842\n",
      "cnt: 0 - valLoss: 0.6105641722679138 - trainLoss: 0.6166393756866455\n",
      "cnt: 0 - valLoss: 0.6105578541755676 - trainLoss: 0.6166322231292725\n",
      "cnt: 0 - valLoss: 0.6105506420135498 - trainLoss: 0.6166254878044128\n",
      "cnt: 0 - valLoss: 0.6105451583862305 - trainLoss: 0.6166184544563293\n",
      "cnt: 0 - valLoss: 0.6105381846427917 - trainLoss: 0.616611897945404\n",
      "cnt: 0 - valLoss: 0.6105315089225769 - trainLoss: 0.6166048645973206\n",
      "cnt: 0 - valLoss: 0.6105246543884277 - trainLoss: 0.6165981292724609\n",
      "cnt: 0 - valLoss: 0.6105177998542786 - trainLoss: 0.616591215133667\n",
      "cnt: 0 - valLoss: 0.6105111241340637 - trainLoss: 0.6165843605995178\n",
      "cnt: 0 - valLoss: 0.6105040907859802 - trainLoss: 0.6165775060653687\n",
      "cnt: 0 - valLoss: 0.6104971170425415 - trainLoss: 0.6165706515312195\n",
      "cnt: 0 - valLoss: 0.6104913353919983 - trainLoss: 0.6165640354156494\n",
      "cnt: 0 - valLoss: 0.6104843616485596 - trainLoss: 0.6165573000907898\n",
      "cnt: 0 - valLoss: 0.610476553440094 - trainLoss: 0.6165506839752197\n",
      "cnt: 0 - valLoss: 0.6104705333709717 - trainLoss: 0.6165437698364258\n",
      "cnt: 0 - valLoss: 0.6104632019996643 - trainLoss: 0.6165372729301453\n",
      "cnt: 0 - valLoss: 0.6104569435119629 - trainLoss: 0.6165304780006409\n",
      "cnt: 0 - valLoss: 0.6104497313499451 - trainLoss: 0.6165238618850708\n",
      "cnt: 0 - valLoss: 0.6104432940483093 - trainLoss: 0.6165171265602112\n",
      "cnt: 0 - valLoss: 0.6104363203048706 - trainLoss: 0.6165103912353516\n",
      "cnt: 0 - valLoss: 0.6104297637939453 - trainLoss: 0.616503894329071\n",
      "cnt: 0 - valLoss: 0.6104230284690857 - trainLoss: 0.6164969801902771\n",
      "cnt: 0 - valLoss: 0.6104163527488708 - trainLoss: 0.6164906620979309\n",
      "cnt: 0 - valLoss: 0.6104095578193665 - trainLoss: 0.6164836883544922\n",
      "cnt: 0 - valLoss: 0.6104032397270203 - trainLoss: 0.6164773106575012\n",
      "cnt: 0 - valLoss: 0.6103962063789368 - trainLoss: 0.6164705157279968\n",
      "cnt: 0 - valLoss: 0.6103901267051697 - trainLoss: 0.6164639592170715\n",
      "cnt: 0 - valLoss: 0.6103828549385071 - trainLoss: 0.6164572834968567\n",
      "cnt: 0 - valLoss: 0.6103770136833191 - trainLoss: 0.6164506077766418\n",
      "cnt: 0 - valLoss: 0.6103695034980774 - trainLoss: 0.6164440512657166\n",
      "cnt: 0 - valLoss: 0.6103627681732178 - trainLoss: 0.6164371967315674\n",
      "cnt: 0 - valLoss: 0.6103561520576477 - trainLoss: 0.6164308786392212\n",
      "cnt: 0 - valLoss: 0.6103495359420776 - trainLoss: 0.6164239048957825\n",
      "cnt: 0 - valLoss: 0.6103429794311523 - trainLoss: 0.6164175271987915\n",
      "cnt: 0 - valLoss: 0.6103360056877136 - trainLoss: 0.6164106726646423\n",
      "cnt: 0 - valLoss: 0.6103298664093018 - trainLoss: 0.616404116153717\n",
      "cnt: 0 - valLoss: 0.6103225946426392 - trainLoss: 0.6163974404335022\n",
      "cnt: 0 - valLoss: 0.6103168725967407 - trainLoss: 0.6163907647132874\n",
      "cnt: 0 - valLoss: 0.6103099584579468 - trainLoss: 0.6163842082023621\n",
      "cnt: 0 - valLoss: 0.6103035807609558 - trainLoss: 0.6163773536682129\n",
      "cnt: 0 - valLoss: 0.6102964878082275 - trainLoss: 0.6163704991340637\n",
      "cnt: 0 - valLoss: 0.6102922558784485 - trainLoss: 0.6163635849952698\n",
      "cnt: 0 - valLoss: 0.6102848649024963 - trainLoss: 0.6163569688796997\n",
      "cnt: 0 - valLoss: 0.6102782487869263 - trainLoss: 0.6163498759269714\n",
      "cnt: 0 - valLoss: 0.6102713942527771 - trainLoss: 0.6163432002067566\n",
      "cnt: 0 - valLoss: 0.6102645993232727 - trainLoss: 0.6163361072540283\n",
      "cnt: 0 - valLoss: 0.6102599501609802 - trainLoss: 0.6163294315338135\n",
      "cnt: 0 - valLoss: 0.6102527976036072 - trainLoss: 0.6163225769996643\n",
      "cnt: 0 - valLoss: 0.6102463603019714 - trainLoss: 0.6163155436515808\n",
      "cnt: 0 - valLoss: 0.6102390289306641 - trainLoss: 0.6163088083267212\n",
      "cnt: 0 - valLoss: 0.6102326512336731 - trainLoss: 0.6163016557693481\n",
      "cnt: 0 - valLoss: 0.6102275848388672 - trainLoss: 0.6162950992584229\n",
      "cnt: 0 - valLoss: 0.6102208495140076 - trainLoss: 0.6162880659103394\n",
      "cnt: 0 - valLoss: 0.6102139949798584 - trainLoss: 0.6162813305854797\n",
      "cnt: 0 - valLoss: 0.6102070212364197 - trainLoss: 0.6162742972373962\n",
      "cnt: 0 - valLoss: 0.6102024912834167 - trainLoss: 0.6162673830986023\n",
      "cnt: 0 - valLoss: 0.6101952195167542 - trainLoss: 0.6162606477737427\n",
      "cnt: 0 - valLoss: 0.6101888418197632 - trainLoss: 0.6162535548210144\n",
      "cnt: 0 - valLoss: 0.6101815104484558 - trainLoss: 0.6162468194961548\n",
      "cnt: 0 - valLoss: 0.6101749539375305 - trainLoss: 0.6162397265434265\n",
      "cnt: 0 - valLoss: 0.6101700067520142 - trainLoss: 0.6162331104278564\n",
      "cnt: 0 - valLoss: 0.6101630330085754 - trainLoss: 0.6162260174751282\n",
      "cnt: 0 - valLoss: 0.6101563572883606 - trainLoss: 0.6162192225456238\n",
      "cnt: 0 - valLoss: 0.6101493239402771 - trainLoss: 0.6162123084068298\n",
      "cnt: 0 - valLoss: 0.6101447939872742 - trainLoss: 0.6162053942680359\n",
      "cnt: 0 - valLoss: 0.6101374626159668 - trainLoss: 0.6161985993385315\n",
      "cnt: 0 - valLoss: 0.6101311445236206 - trainLoss: 0.6161915063858032\n",
      "cnt: 0 - valLoss: 0.6101236939430237 - trainLoss: 0.6161848306655884\n",
      "cnt: 0 - valLoss: 0.6101171374320984 - trainLoss: 0.6161776185035706\n",
      "cnt: 0 - valLoss: 0.6101120710372925 - trainLoss: 0.6161710619926453\n",
      "cnt: 0 - valLoss: 0.6101050972938538 - trainLoss: 0.6161640286445618\n",
      "cnt: 0 - valLoss: 0.6100983619689941 - trainLoss: 0.6161571145057678\n",
      "cnt: 0 - valLoss: 0.6100912690162659 - trainLoss: 0.6161501407623291\n",
      "cnt: 0 - valLoss: 0.6100866794586182 - trainLoss: 0.6161432266235352\n",
      "cnt: 0 - valLoss: 0.6100792288780212 - trainLoss: 0.6161364912986755\n",
      "cnt: 0 - valLoss: 0.6100726127624512 - trainLoss: 0.6161293387413025\n",
      "cnt: 0 - valLoss: 0.6100656390190125 - trainLoss: 0.6161226630210876\n",
      "cnt: 0 - valLoss: 0.6100606322288513 - trainLoss: 0.6161156296730042\n",
      "cnt: 0 - valLoss: 0.6100537776947021 - trainLoss: 0.6161088347434998\n",
      "cnt: 0 - valLoss: 0.6100466847419739 - trainLoss: 0.6161017417907715\n",
      "cnt: 0 - valLoss: 0.6100420355796814 - trainLoss: 0.6160948276519775\n",
      "cnt: 0 - valLoss: 0.6100346446037292 - trainLoss: 0.6160880327224731\n",
      "cnt: 0 - valLoss: 0.6100281476974487 - trainLoss: 0.6160809993743896\n",
      "cnt: 0 - valLoss: 0.6100207567214966 - trainLoss: 0.6160741448402405\n",
      "cnt: 0 - valLoss: 0.6100160479545593 - trainLoss: 0.6160670518875122\n",
      "cnt: 0 - valLoss: 0.610008955001831 - trainLoss: 0.6160604357719421\n",
      "cnt: 0 - valLoss: 0.6100019812583923 - trainLoss: 0.6160532832145691\n",
      "cnt: 0 - valLoss: 0.6099952459335327 - trainLoss: 0.6160464286804199\n",
      "cnt: 0 - valLoss: 0.6099900007247925 - trainLoss: 0.6160395741462708\n",
      "cnt: 0 - valLoss: 0.6099832653999329 - trainLoss: 0.6160325407981873\n",
      "cnt: 0 - valLoss: 0.6099759340286255 - trainLoss: 0.6160256862640381\n",
      "cnt: 0 - valLoss: 0.6099715232849121 - trainLoss: 0.6160185933113098\n",
      "cnt: 0 - valLoss: 0.6099640727043152 - trainLoss: 0.6160119771957397\n",
      "cnt: 0 - valLoss: 0.609957218170166 - trainLoss: 0.6160048246383667\n",
      "cnt: 0 - valLoss: 0.6099501848220825 - trainLoss: 0.6159979701042175\n",
      "cnt: 0 - valLoss: 0.6099451780319214 - trainLoss: 0.615990936756134\n",
      "cnt: 0 - valLoss: 0.6099382638931274 - trainLoss: 0.6159841418266296\n",
      "cnt: 0 - valLoss: 0.6099309921264648 - trainLoss: 0.6159771680831909\n",
      "cnt: 0 - valLoss: 0.6099264621734619 - trainLoss: 0.6159700751304626\n",
      "cnt: 0 - valLoss: 0.609919011592865 - trainLoss: 0.6159633994102478\n",
      "cnt: 0 - valLoss: 0.6099125146865845 - trainLoss: 0.6159562468528748\n",
      "cnt: 0 - valLoss: 0.6099051237106323 - trainLoss: 0.6159494519233704\n",
      "cnt: 0 - valLoss: 0.6099002361297607 - trainLoss: 0.6159423589706421\n",
      "cnt: 0 - valLoss: 0.6098930835723877 - trainLoss: 0.6159356236457825\n",
      "cnt: 0 - valLoss: 0.6098859906196594 - trainLoss: 0.6159285306930542\n",
      "cnt: 0 - valLoss: 0.6098812818527222 - trainLoss: 0.6159214973449707\n",
      "cnt: 0 - valLoss: 0.6098739504814148 - trainLoss: 0.6159147620201111\n",
      "cnt: 0 - valLoss: 0.6098673343658447 - trainLoss: 0.6159076690673828\n",
      "cnt: 0 - valLoss: 0.609859824180603 - trainLoss: 0.6159008145332336\n",
      "cnt: 0 - valLoss: 0.6098554134368896 - trainLoss: 0.6158936619758606\n",
      "cnt: 0 - valLoss: 0.609847903251648 - trainLoss: 0.6158869862556458\n",
      "cnt: 0 - valLoss: 0.6098409295082092 - trainLoss: 0.6158798336982727\n",
      "cnt: 0 - valLoss: 0.6098359227180481 - trainLoss: 0.6158730387687683\n",
      "cnt: 0 - valLoss: 0.609828770160675 - trainLoss: 0.6158660650253296\n",
      "cnt: 0 - valLoss: 0.6098219156265259 - trainLoss: 0.6158590912818909\n",
      "cnt: 0 - valLoss: 0.6098147034645081 - trainLoss: 0.6158521175384521\n",
      "cnt: 0 - valLoss: 0.6098100543022156 - trainLoss: 0.6158450841903687\n",
      "cnt: 0 - valLoss: 0.6098025441169739 - trainLoss: 0.6158382892608643\n",
      "cnt: 0 - valLoss: 0.6097957491874695 - trainLoss: 0.6158309578895569\n",
      "cnt: 0 - valLoss: 0.6097908616065979 - trainLoss: 0.6158240437507629\n",
      "cnt: 0 - valLoss: 0.6097838878631592 - trainLoss: 0.6158168911933899\n",
      "cnt: 0 - valLoss: 0.6097769141197205 - trainLoss: 0.6158098578453064\n",
      "cnt: 0 - valLoss: 0.6097696423530579 - trainLoss: 0.6158027052879333\n",
      "cnt: 0 - valLoss: 0.6097632050514221 - trainLoss: 0.6157954931259155\n",
      "cnt: 0 - valLoss: 0.6097577810287476 - trainLoss: 0.6157885789871216\n",
      "cnt: 0 - valLoss: 0.6097509860992432 - trainLoss: 0.615781307220459\n",
      "cnt: 0 - valLoss: 0.6097438931465149 - trainLoss: 0.6157743334770203\n",
      "cnt: 0 - valLoss: 0.6097368597984314 - trainLoss: 0.6157670617103577\n",
      "cnt: 0 - valLoss: 0.6097321510314941 - trainLoss: 0.615760087966919\n",
      "cnt: 0 - valLoss: 0.6097248196601868 - trainLoss: 0.6157529354095459\n",
      "cnt: 0 - valLoss: 0.6097181439399719 - trainLoss: 0.6157457828521729\n",
      "cnt: 0 - valLoss: 0.6097107529640198 - trainLoss: 0.6157387495040894\n",
      "cnt: 0 - valLoss: 0.6097060441970825 - trainLoss: 0.6157314777374268\n",
      "cnt: 0 - valLoss: 0.6096988916397095 - trainLoss: 0.6157245635986328\n",
      "cnt: 0 - valLoss: 0.609691858291626 - trainLoss: 0.6157172918319702\n",
      "cnt: 0 - valLoss: 0.6096850037574768 - trainLoss: 0.6157102584838867\n",
      "cnt: 0 - valLoss: 0.6096798181533813 - trainLoss: 0.6157031059265137\n",
      "cnt: 0 - valLoss: 0.6096731424331665 - trainLoss: 0.6156960129737854\n",
      "cnt: 0 - valLoss: 0.6096656322479248 - trainLoss: 0.6156889200210571\n",
      "cnt: 0 - valLoss: 0.6096588969230652 - trainLoss: 0.6156815886497498\n",
      "cnt: 0 - valLoss: 0.6096537709236145 - trainLoss: 0.6156747341156006\n",
      "cnt: 0 - valLoss: 0.6096466779708862 - trainLoss: 0.615667462348938\n",
      "cnt: 0 - valLoss: 0.6096396446228027 - trainLoss: 0.6156604886054993\n",
      "cnt: 0 - valLoss: 0.6096324920654297 - trainLoss: 0.6156532168388367\n",
      "cnt: 0 - valLoss: 0.6096278429031372 - trainLoss: 0.6156461238861084\n",
      "cnt: 0 - valLoss: 0.6096202731132507 - trainLoss: 0.6156390905380249\n",
      "cnt: 0 - valLoss: 0.6096136569976807 - trainLoss: 0.6156317591667175\n",
      "cnt: 0 - valLoss: 0.6096062660217285 - trainLoss: 0.6156247854232788\n",
      "cnt: 0 - valLoss: 0.6096011996269226 - trainLoss: 0.6156176328659058\n",
      "cnt: 0 - valLoss: 0.6095941066741943 - trainLoss: 0.6156105399131775\n",
      "cnt: 0 - valLoss: 0.6095868349075317 - trainLoss: 0.6156033277511597\n",
      "cnt: 0 - valLoss: 0.6095820665359497 - trainLoss: 0.6155962347984314\n",
      "cnt: 0 - valLoss: 0.6095744967460632 - trainLoss: 0.6155892014503479\n",
      "cnt: 0 - valLoss: 0.6095679402351379 - trainLoss: 0.6155818700790405\n",
      "cnt: 0 - valLoss: 0.6095604300498962 - trainLoss: 0.6155748963356018\n",
      "cnt: 0 - valLoss: 0.6095554232597351 - trainLoss: 0.615567684173584\n",
      "cnt: 0 - valLoss: 0.6095482707023621 - trainLoss: 0.6155607104301453\n",
      "cnt: 0 - valLoss: 0.6095409989356995 - trainLoss: 0.6155534386634827\n",
      "cnt: 0 - valLoss: 0.6095361709594727 - trainLoss: 0.6155462861061096\n",
      "cnt: 0 - valLoss: 0.6095286011695862 - trainLoss: 0.6155392527580261\n",
      "cnt: 0 - valLoss: 0.6095219850540161 - trainLoss: 0.6155319213867188\n",
      "cnt: 0 - valLoss: 0.6095165014266968 - trainLoss: 0.61552494764328\n",
      "cnt: 0 - valLoss: 0.609505832195282 - trainLoss: 0.615517795085907\n",
      "cnt: 0 - valLoss: 0.6095003485679626 - trainLoss: 0.6155111193656921\n",
      "cnt: 0 - valLoss: 0.6094951033592224 - trainLoss: 0.6155036091804504\n",
      "cnt: 0 - valLoss: 0.6094881296157837 - trainLoss: 0.6154964566230774\n",
      "cnt: 0 - valLoss: 0.6094806790351868 - trainLoss: 0.6154892444610596\n",
      "cnt: 0 - valLoss: 0.6094760894775391 - trainLoss: 0.6154820322990417\n",
      "cnt: 0 - valLoss: 0.6094683408737183 - trainLoss: 0.6154751181602478\n",
      "cnt: 0 - valLoss: 0.6094614267349243 - trainLoss: 0.6154676675796509\n",
      "cnt: 0 - valLoss: 0.6094562411308289 - trainLoss: 0.6154607534408569\n",
      "cnt: 0 - valLoss: 0.6094489693641663 - trainLoss: 0.6154535412788391\n",
      "cnt: 0 - valLoss: 0.609441876411438 - trainLoss: 0.6154463887214661\n",
      "cnt: 0 - valLoss: 0.6094346046447754 - trainLoss: 0.6154391169548035\n",
      "cnt: 0 - valLoss: 0.6094298362731934 - trainLoss: 0.6154320240020752\n",
      "cnt: 0 - valLoss: 0.6094186902046204 - trainLoss: 0.615425169467926\n",
      "cnt: 0 - valLoss: 0.6094135046005249 - trainLoss: 0.6154181957244873\n",
      "cnt: 0 - valLoss: 0.6094080209732056 - trainLoss: 0.6154109835624695\n",
      "cnt: 0 - valLoss: 0.6094008088111877 - trainLoss: 0.615403413772583\n",
      "cnt: 0 - valLoss: 0.6093958020210266 - trainLoss: 0.6153964400291443\n",
      "cnt: 0 - valLoss: 0.6093882918357849 - trainLoss: 0.6153892278671265\n",
      "cnt: 0 - valLoss: 0.6093814969062805 - trainLoss: 0.6153819561004639\n",
      "cnt: 0 - valLoss: 0.6093758940696716 - trainLoss: 0.6153748631477356\n",
      "cnt: 0 - valLoss: 0.6093656420707703 - trainLoss: 0.6153676509857178\n",
      "cnt: 0 - valLoss: 0.6093595623970032 - trainLoss: 0.6153610944747925\n",
      "cnt: 0 - valLoss: 0.6093543171882629 - trainLoss: 0.6153533458709717\n",
      "cnt: 0 - valLoss: 0.6093471050262451 - trainLoss: 0.6153461933135986\n",
      "cnt: 0 - valLoss: 0.6093418598175049 - trainLoss: 0.615338921546936\n",
      "cnt: 0 - valLoss: 0.6093347668647766 - trainLoss: 0.615331768989563\n",
      "cnt: 0 - valLoss: 0.6093270778656006 - trainLoss: 0.6153246164321899\n",
      "cnt: 0 - valLoss: 0.6093226075172424 - trainLoss: 0.6153172254562378\n",
      "cnt: 0 - valLoss: 0.6093147397041321 - trainLoss: 0.6153103709220886\n",
      "cnt: 0 - valLoss: 0.6093075275421143 - trainLoss: 0.6153029203414917\n",
      "cnt: 0 - valLoss: 0.609300434589386 - trainLoss: 0.6152957677841187\n",
      "cnt: 0 - valLoss: 0.6092950105667114 - trainLoss: 0.6152886748313904\n",
      "cnt: 0 - valLoss: 0.6092879176139832 - trainLoss: 0.615281343460083\n",
      "cnt: 0 - valLoss: 0.6092803478240967 - trainLoss: 0.6152741312980652\n",
      "cnt: 0 - valLoss: 0.6092755794525146 - trainLoss: 0.6152669191360474\n",
      "cnt: 0 - valLoss: 0.6092643141746521 - trainLoss: 0.6152599453926086\n",
      "cnt: 0 - valLoss: 0.6092589497566223 - trainLoss: 0.6152530312538147\n",
      "cnt: 0 - valLoss: 0.6092533469200134 - trainLoss: 0.6152457594871521\n",
      "cnt: 0 - valLoss: 0.6092461347579956 - trainLoss: 0.6152381300926208\n",
      "cnt: 0 - valLoss: 0.6092410683631897 - trainLoss: 0.6152310371398926\n",
      "cnt: 0 - valLoss: 0.6092333793640137 - trainLoss: 0.6152238249778748\n",
      "cnt: 0 - valLoss: 0.6092265844345093 - trainLoss: 0.6152164936065674\n",
      "cnt: 0 - valLoss: 0.6092208027839661 - trainLoss: 0.6152094602584839\n",
      "cnt: 0 - valLoss: 0.6092137694358826 - trainLoss: 0.6152020692825317\n",
      "cnt: 0 - valLoss: 0.6092063784599304 - trainLoss: 0.6151949763298035\n",
      "cnt: 0 - valLoss: 0.6092010736465454 - trainLoss: 0.6151877045631409\n",
      "cnt: 0 - valLoss: 0.6091938614845276 - trainLoss: 0.6151805520057678\n",
      "cnt: 0 - valLoss: 0.6091864109039307 - trainLoss: 0.6151732802391052\n",
      "cnt: 0 - valLoss: 0.6091815233230591 - trainLoss: 0.6151660680770874\n",
      "cnt: 0 - valLoss: 0.609170138835907 - trainLoss: 0.6151590347290039\n",
      "cnt: 0 - valLoss: 0.6091651320457458 - trainLoss: 0.6151520013809204\n",
      "cnt: 0 - valLoss: 0.6091591715812683 - trainLoss: 0.6151447892189026\n",
      "cnt: 0 - valLoss: 0.6091520190238953 - trainLoss: 0.6151371002197266\n",
      "cnt: 0 - valLoss: 0.6091468334197998 - trainLoss: 0.6151301264762878\n",
      "cnt: 0 - valLoss: 0.6091392636299133 - trainLoss: 0.6151228547096252\n",
      "cnt: 0 - valLoss: 0.6091324090957642 - trainLoss: 0.6151154637336731\n",
      "cnt: 0 - valLoss: 0.6091266870498657 - trainLoss: 0.6151084303855896\n",
      "cnt: 0 - valLoss: 0.6091198325157166 - trainLoss: 0.6151009798049927\n",
      "cnt: 0 - valLoss: 0.6091142892837524 - trainLoss: 0.6150938868522644\n",
      "cnt: 0 - valLoss: 0.6091033816337585 - trainLoss: 0.6150866746902466\n",
      "cnt: 0 - valLoss: 0.6090977787971497 - trainLoss: 0.615079939365387\n",
      "cnt: 0 - valLoss: 0.6090922355651855 - trainLoss: 0.6150723099708557\n",
      "cnt: 0 - valLoss: 0.6090851426124573 - trainLoss: 0.6150649785995483\n",
      "cnt: 0 - valLoss: 0.6090796589851379 - trainLoss: 0.6150577664375305\n",
      "cnt: 0 - valLoss: 0.609072744846344 - trainLoss: 0.6150504946708679\n",
      "cnt: 0 - valLoss: 0.6090649366378784 - trainLoss: 0.6150434017181396\n",
      "cnt: 0 - valLoss: 0.6090599298477173 - trainLoss: 0.6150360107421875\n",
      "cnt: 0 - valLoss: 0.6090524792671204 - trainLoss: 0.6150290369987488\n",
      "cnt: 0 - valLoss: 0.6090452075004578 - trainLoss: 0.6150215268135071\n",
      "cnt: 0 - valLoss: 0.6090400815010071 - trainLoss: 0.6150144338607788\n",
      "cnt: 0 - valLoss: 0.6090324521064758 - trainLoss: 0.615007221698761\n",
      "cnt: 0 - valLoss: 0.6090255379676819 - trainLoss: 0.6149998307228088\n",
      "cnt: 0 - valLoss: 0.6090197563171387 - trainLoss: 0.6149928569793701\n",
      "cnt: 0 - valLoss: 0.6090126037597656 - trainLoss: 0.6149853467941284\n",
      "cnt: 0 - valLoss: 0.6090052723884583 - trainLoss: 0.6149783134460449\n",
      "cnt: 0 - valLoss: 0.6089999675750732 - trainLoss: 0.6149710416793823\n",
      "cnt: 0 - valLoss: 0.6089926958084106 - trainLoss: 0.6149638891220093\n",
      "cnt: 0 - valLoss: 0.6089874505996704 - trainLoss: 0.6149564981460571\n",
      "cnt: 0 - valLoss: 0.6089765429496765 - trainLoss: 0.6149497032165527\n",
      "cnt: 0 - valLoss: 0.608970582485199 - trainLoss: 0.6149426102638245\n",
      "cnt: 0 - valLoss: 0.6089655756950378 - trainLoss: 0.6149349212646484\n",
      "cnt: 0 - valLoss: 0.6089578866958618 - trainLoss: 0.6149277687072754\n",
      "cnt: 0 - valLoss: 0.6089527010917664 - trainLoss: 0.6149203181266785\n",
      "cnt: 0 - valLoss: 0.6089451909065247 - trainLoss: 0.614913284778595\n",
      "cnt: 0 - valLoss: 0.6089377999305725 - trainLoss: 0.6149058938026428\n",
      "cnt: 0 - valLoss: 0.6089327335357666 - trainLoss: 0.6148987412452698\n",
      "cnt: 0 - valLoss: 0.608924925327301 - trainLoss: 0.6148914694786072\n",
      "cnt: 0 - valLoss: 0.6089202165603638 - trainLoss: 0.6148840188980103\n",
      "cnt: 0 - valLoss: 0.6089085936546326 - trainLoss: 0.6148771643638611\n",
      "cnt: 0 - valLoss: 0.608903169631958 - trainLoss: 0.6148699522018433\n",
      "cnt: 0 - valLoss: 0.6088976263999939 - trainLoss: 0.6148626804351807\n",
      "cnt: 0 - valLoss: 0.6088902354240417 - trainLoss: 0.6148550510406494\n",
      "cnt: 0 - valLoss: 0.6088851094245911 - trainLoss: 0.6148479580879211\n",
      "cnt: 0 - valLoss: 0.6088773608207703 - trainLoss: 0.6148406863212585\n",
      "cnt: 0 - valLoss: 0.6088705062866211 - trainLoss: 0.6148332953453064\n",
      "cnt: 0 - valLoss: 0.6088646054267883 - trainLoss: 0.6148262023925781\n",
      "cnt: 0 - valLoss: 0.6088574528694153 - trainLoss: 0.6148187518119812\n",
      "cnt: 0 - valLoss: 0.6088520884513855 - trainLoss: 0.6148116588592529\n",
      "cnt: 0 - valLoss: 0.6088446378707886 - trainLoss: 0.6148043274879456\n",
      "cnt: 0 - valLoss: 0.6088372468948364 - trainLoss: 0.614797055721283\n",
      "cnt: 0 - valLoss: 0.6088317632675171 - trainLoss: 0.6147897839546204\n",
      "cnt: 0 - valLoss: 0.6088245511054993 - trainLoss: 0.614782452583313\n",
      "cnt: 0 - valLoss: 0.6088168621063232 - trainLoss: 0.6147751808166504\n",
      "cnt: 0 - valLoss: 0.6088119745254517 - trainLoss: 0.6147679090499878\n",
      "cnt: 0 - valLoss: 0.6088039875030518 - trainLoss: 0.6147608160972595\n",
      "cnt: 0 - valLoss: 0.6087989211082458 - trainLoss: 0.6147533059120178\n",
      "cnt: 0 - valLoss: 0.6087876558303833 - trainLoss: 0.6147462725639343\n",
      "cnt: 0 - valLoss: 0.6087819933891296 - trainLoss: 0.6147392392158508\n",
      "cnt: 0 - valLoss: 0.6087765693664551 - trainLoss: 0.6147317886352539\n",
      "cnt: 0 - valLoss: 0.6087689995765686 - trainLoss: 0.6147242784500122\n",
      "cnt: 0 - valLoss: 0.6087639927864075 - trainLoss: 0.6147170662879944\n",
      "cnt: 0 - valLoss: 0.6087560057640076 - trainLoss: 0.6147098541259766\n",
      "cnt: 0 - valLoss: 0.6087509989738464 - trainLoss: 0.6147023439407349\n",
      "cnt: 0 - valLoss: 0.6087433099746704 - trainLoss: 0.6146953105926514\n",
      "cnt: 0 - valLoss: 0.608735978603363 - trainLoss: 0.6146878004074097\n",
      "cnt: 0 - valLoss: 0.6087306141853333 - trainLoss: 0.6146807074546814\n",
      "cnt: 0 - valLoss: 0.6087229251861572 - trainLoss: 0.614673376083374\n",
      "cnt: 0 - valLoss: 0.6087180376052856 - trainLoss: 0.6146659851074219\n",
      "cnt: 0 - valLoss: 0.6087099313735962 - trainLoss: 0.6146588921546936\n",
      "cnt: 0 - valLoss: 0.6087032556533813 - trainLoss: 0.6146512627601624\n",
      "cnt: 0 - valLoss: 0.6086976528167725 - trainLoss: 0.6146444082260132\n",
      "cnt: 0 - valLoss: 0.6086904406547546 - trainLoss: 0.614637017250061\n",
      "cnt: 0 - valLoss: 0.6086853742599487 - trainLoss: 0.6146300435066223\n",
      "cnt: 0 - valLoss: 0.6086779236793518 - trainLoss: 0.6146228313446045\n",
      "cnt: 0 - valLoss: 0.6086730360984802 - trainLoss: 0.6146157383918762\n",
      "cnt: 0 - valLoss: 0.6086615920066833 - trainLoss: 0.6146085858345032\n",
      "cnt: 0 - valLoss: 0.608656644821167 - trainLoss: 0.6146019101142883\n",
      "cnt: 0 - valLoss: 0.6086509227752686 - trainLoss: 0.6145946979522705\n",
      "cnt: 0 - valLoss: 0.6086462736129761 - trainLoss: 0.6145871877670288\n",
      "cnt: 0 - valLoss: 0.6086384057998657 - trainLoss: 0.6145802140235901\n",
      "cnt: 0 - valLoss: 0.6086335182189941 - trainLoss: 0.6145729422569275\n",
      "cnt: 0 - valLoss: 0.6086260080337524 - trainLoss: 0.6145659685134888\n",
      "cnt: 0 - valLoss: 0.6086210012435913 - trainLoss: 0.6145586967468262\n",
      "cnt: 0 - valLoss: 0.6086136102676392 - trainLoss: 0.6145516633987427\n",
      "cnt: 0 - valLoss: 0.6086084842681885 - trainLoss: 0.6145443916320801\n",
      "cnt: 0 - valLoss: 0.6086012125015259 - trainLoss: 0.6145372986793518\n",
      "cnt: 0 - valLoss: 0.6085958480834961 - trainLoss: 0.6145301461219788\n",
      "cnt: 0 - valLoss: 0.6085888147354126 - trainLoss: 0.6145229339599609\n",
      "cnt: 0 - valLoss: 0.6085828542709351 - trainLoss: 0.6145159006118774\n",
      "cnt: 0 - valLoss: 0.6085756421089172 - trainLoss: 0.6145085096359253\n",
      "cnt: 0 - valLoss: 0.6085676550865173 - trainLoss: 0.6145009398460388\n",
      "cnt: 0 - valLoss: 0.608562171459198 - trainLoss: 0.6144932508468628\n",
      "cnt: 0 - valLoss: 0.6085509061813354 - trainLoss: 0.6144858598709106\n",
      "cnt: 0 - valLoss: 0.6085445880889893 - trainLoss: 0.614478588104248\n",
      "cnt: 0 - valLoss: 0.608539342880249 - trainLoss: 0.6144704222679138\n",
      "cnt: 0 - valLoss: 0.6085312366485596 - trainLoss: 0.6144629120826721\n",
      "cnt: 0 - valLoss: 0.6085237860679626 - trainLoss: 0.6144551038742065\n",
      "cnt: 0 - valLoss: 0.6085181832313538 - trainLoss: 0.6144476532936096\n",
      "cnt: 0 - valLoss: 0.6085102558135986 - trainLoss: 0.6144399642944336\n",
      "cnt: 0 - valLoss: 0.6085029244422913 - trainLoss: 0.6144322752952576\n",
      "cnt: 0 - valLoss: 0.6084970235824585 - trainLoss: 0.6144247651100159\n",
      "cnt: 0 - valLoss: 0.608489453792572 - trainLoss: 0.6144170165061951\n",
      "cnt: 0 - valLoss: 0.6084816455841064 - trainLoss: 0.6144095063209534\n",
      "cnt: 0 - valLoss: 0.6084739565849304 - trainLoss: 0.614401638507843\n",
      "cnt: 0 - valLoss: 0.6084685325622559 - trainLoss: 0.6143941879272461\n",
      "cnt: 0 - valLoss: 0.6084604263305664 - trainLoss: 0.6143865585327148\n",
      "cnt: 0 - valLoss: 0.6084529757499695 - trainLoss: 0.6143786907196045\n",
      "cnt: 0 - valLoss: 0.6084473133087158 - trainLoss: 0.6143712997436523\n",
      "cnt: 0 - valLoss: 0.6084393858909607 - trainLoss: 0.6143635511398315\n",
      "cnt: 0 - valLoss: 0.608431875705719 - trainLoss: 0.6143558621406555\n",
      "cnt: 0 - valLoss: 0.6084259748458862 - trainLoss: 0.6143482327461243\n",
      "cnt: 0 - valLoss: 0.6084185838699341 - trainLoss: 0.6143406629562378\n",
      "cnt: 0 - valLoss: 0.6084104180335999 - trainLoss: 0.6143333315849304\n",
      "cnt: 0 - valLoss: 0.6084047555923462 - trainLoss: 0.6143255829811096\n",
      "cnt: 0 - valLoss: 0.6083968877792358 - trainLoss: 0.6143183708190918\n",
      "cnt: 0 - valLoss: 0.6083890795707703 - trainLoss: 0.6143107414245605\n",
      "cnt: 0 - valLoss: 0.6083835363388062 - trainLoss: 0.6143032908439636\n",
      "cnt: 0 - valLoss: 0.6083751916885376 - trainLoss: 0.6142958998680115\n",
      "cnt: 0 - valLoss: 0.6083701252937317 - trainLoss: 0.6142882108688354\n",
      "cnt: 0 - valLoss: 0.6083617210388184 - trainLoss: 0.6142811179161072\n",
      "cnt: 0 - valLoss: 0.6083539724349976 - trainLoss: 0.6142733097076416\n",
      "cnt: 0 - valLoss: 0.6083483099937439 - trainLoss: 0.6142659187316895\n",
      "cnt: 0 - valLoss: 0.6083402037620544 - trainLoss: 0.6142584681510925\n",
      "cnt: 0 - valLoss: 0.6083328127861023 - trainLoss: 0.614250898361206\n",
      "cnt: 0 - valLoss: 0.6083264946937561 - trainLoss: 0.6142436265945435\n",
      "cnt: 0 - valLoss: 0.6083189249038696 - trainLoss: 0.6142358779907227\n",
      "cnt: 0 - valLoss: 0.6083130836486816 - trainLoss: 0.6142286062240601\n",
      "cnt: 0 - valLoss: 0.6083051562309265 - trainLoss: 0.6142209768295288\n",
      "cnt: 0 - valLoss: 0.6082978248596191 - trainLoss: 0.6142134666442871\n",
      "cnt: 0 - valLoss: 0.6082921624183655 - trainLoss: 0.614206075668335\n",
      "cnt: 0 - valLoss: 0.6082848310470581 - trainLoss: 0.6141985058784485\n",
      "cnt: 0 - valLoss: 0.6082791090011597 - trainLoss: 0.6141910552978516\n",
      "cnt: 0 - valLoss: 0.6082717180252075 - trainLoss: 0.6141834855079651\n",
      "cnt: 0 - valLoss: 0.6082640290260315 - trainLoss: 0.6141760349273682\n",
      "cnt: 0 - valLoss: 0.6082586646080017 - trainLoss: 0.6141684651374817\n",
      "cnt: 0 - valLoss: 0.6082510948181152 - trainLoss: 0.6141610741615295\n",
      "cnt: 0 - valLoss: 0.6082456111907959 - trainLoss: 0.6141534447669983\n",
      "cnt: 0 - valLoss: 0.6082382202148438 - trainLoss: 0.6141459941864014\n",
      "cnt: 0 - valLoss: 0.6082302331924438 - trainLoss: 0.6141385436058044\n",
      "cnt: 0 - valLoss: 0.6082251071929932 - trainLoss: 0.6141308546066284\n",
      "cnt: 0 - valLoss: 0.6082172989845276 - trainLoss: 0.614123523235321\n",
      "cnt: 0 - valLoss: 0.6082119941711426 - trainLoss: 0.6141157746315002\n",
      "cnt: 0 - valLoss: 0.6082004308700562 - trainLoss: 0.6141085028648376\n",
      "cnt: 0 - valLoss: 0.6081945896148682 - trainLoss: 0.6141013503074646\n",
      "cnt: 0 - valLoss: 0.6081893444061279 - trainLoss: 0.614093542098999\n",
      "cnt: 0 - valLoss: 0.6081836223602295 - trainLoss: 0.614085853099823\n",
      "cnt: 0 - valLoss: 0.608176052570343 - trainLoss: 0.6140782833099365\n",
      "cnt: 0 - valLoss: 0.608168363571167 - trainLoss: 0.6140708327293396\n",
      "cnt: 0 - valLoss: 0.6081629395484924 - trainLoss: 0.6140632629394531\n",
      "cnt: 0 - valLoss: 0.6081554293632507 - trainLoss: 0.6140558123588562\n",
      "cnt: 0 - valLoss: 0.6081497669219971 - trainLoss: 0.6140482425689697\n",
      "cnt: 0 - valLoss: 0.6081424951553345 - trainLoss: 0.614040732383728\n",
      "cnt: 0 - valLoss: 0.608134388923645 - trainLoss: 0.6140331625938416\n",
      "cnt: 0 - valLoss: 0.6081291437149048 - trainLoss: 0.6140255928039551\n",
      "cnt: 0 - valLoss: 0.608121395111084 - trainLoss: 0.6140182018280029\n",
      "cnt: 0 - valLoss: 0.6081159710884094 - trainLoss: 0.6140105128288269\n",
      "cnt: 0 - valLoss: 0.6081082820892334 - trainLoss: 0.6140031218528748\n",
      "cnt: 0 - valLoss: 0.6081025004386902 - trainLoss: 0.6139954924583435\n",
      "cnt: 0 - valLoss: 0.6080949902534485 - trainLoss: 0.6139879822731018\n",
      "cnt: 0 - valLoss: 0.6080868244171143 - trainLoss: 0.6139804720878601\n",
      "cnt: 0 - valLoss: 0.6080813407897949 - trainLoss: 0.6139729022979736\n",
      "cnt: 0 - valLoss: 0.6080735325813293 - trainLoss: 0.6139654517173767\n",
      "cnt: 0 - valLoss: 0.6080678701400757 - trainLoss: 0.6139578819274902\n",
      "cnt: 0 - valLoss: 0.6080602407455444 - trainLoss: 0.6139503717422485\n",
      "cnt: 0 - valLoss: 0.6080543398857117 - trainLoss: 0.6139428615570068\n",
      "cnt: 0 - valLoss: 0.6080469489097595 - trainLoss: 0.6139352321624756\n",
      "cnt: 0 - valLoss: 0.6080408692359924 - trainLoss: 0.6139278411865234\n",
      "cnt: 0 - valLoss: 0.6080331802368164 - trainLoss: 0.6139201521873474\n",
      "cnt: 0 - valLoss: 0.6080275774002075 - trainLoss: 0.6139127612113953\n",
      "cnt: 0 - valLoss: 0.6080195903778076 - trainLoss: 0.613905131816864\n",
      "cnt: 0 - valLoss: 0.6080141663551331 - trainLoss: 0.6138975620269775\n",
      "cnt: 0 - valLoss: 0.6080058217048645 - trainLoss: 0.6138902902603149\n",
      "cnt: 0 - valLoss: 0.6080005764961243 - trainLoss: 0.6138827204704285\n",
      "cnt: 0 - valLoss: 0.6079919934272766 - trainLoss: 0.6138755679130554\n",
      "cnt: 0 - valLoss: 0.607986569404602 - trainLoss: 0.613867998123169\n",
      "cnt: 0 - valLoss: 0.6079784631729126 - trainLoss: 0.6138608455657959\n",
      "cnt: 0 - valLoss: 0.6079727411270142 - trainLoss: 0.6138533353805542\n",
      "cnt: 0 - valLoss: 0.6079649329185486 - trainLoss: 0.6138460040092468\n",
      "cnt: 0 - valLoss: 0.6079589128494263 - trainLoss: 0.6138386130332947\n",
      "cnt: 0 - valLoss: 0.6079513430595398 - trainLoss: 0.6138311624526978\n",
      "cnt: 0 - valLoss: 0.6079450249671936 - trainLoss: 0.6138239502906799\n",
      "cnt: 0 - valLoss: 0.6079398393630981 - trainLoss: 0.6138163805007935\n",
      "cnt: 0 - valLoss: 0.6079313158988953 - trainLoss: 0.6138091683387756\n",
      "cnt: 0 - valLoss: 0.6079258322715759 - trainLoss: 0.6138015389442444\n",
      "cnt: 0 - valLoss: 0.6079177260398865 - trainLoss: 0.6137944459915161\n",
      "cnt: 0 - valLoss: 0.6079119443893433 - trainLoss: 0.6137868165969849\n",
      "cnt: 0 - valLoss: 0.6079040765762329 - trainLoss: 0.6137794852256775\n",
      "cnt: 0 - valLoss: 0.6078980565071106 - trainLoss: 0.6137721538543701\n",
      "cnt: 0 - valLoss: 0.6078904271125793 - trainLoss: 0.6137646436691284\n",
      "cnt: 0 - valLoss: 0.6078841686248779 - trainLoss: 0.6137574315071106\n",
      "cnt: 0 - valLoss: 0.6078789830207825 - trainLoss: 0.6137498021125793\n",
      "cnt: 0 - valLoss: 0.60787034034729 - trainLoss: 0.6137427091598511\n",
      "cnt: 0 - valLoss: 0.6078647971153259 - trainLoss: 0.6137349605560303\n",
      "cnt: 0 - valLoss: 0.6078566908836365 - trainLoss: 0.613727867603302\n",
      "cnt: 0 - valLoss: 0.6078508496284485 - trainLoss: 0.6137202978134155\n",
      "cnt: 0 - valLoss: 0.6078430414199829 - trainLoss: 0.6137129664421082\n",
      "cnt: 0 - valLoss: 0.6078369617462158 - trainLoss: 0.6137055158615112\n",
      "cnt: 0 - valLoss: 0.6078293919563293 - trainLoss: 0.6136980056762695\n",
      "cnt: 0 - valLoss: 0.6078230738639832 - trainLoss: 0.6136907935142517\n",
      "cnt: 0 - valLoss: 0.6078177690505981 - trainLoss: 0.6136831641197205\n",
      "cnt: 0 - valLoss: 0.6078091859817505 - trainLoss: 0.6136759519577026\n",
      "cnt: 0 - valLoss: 0.6078037023544312 - trainLoss: 0.6136683225631714\n",
      "cnt: 0 - valLoss: 0.6077955365180969 - trainLoss: 0.6136611700057983\n",
      "cnt: 0 - valLoss: 0.6077896356582642 - trainLoss: 0.6136536002159119\n",
      "cnt: 0 - valLoss: 0.6077818274497986 - trainLoss: 0.6136462092399597\n",
      "cnt: 0 - valLoss: 0.6077757477760315 - trainLoss: 0.6136388778686523\n",
      "cnt: 0 - valLoss: 0.6077702641487122 - trainLoss: 0.6136313080787659\n",
      "cnt: 0 - valLoss: 0.6077617406845093 - trainLoss: 0.613624095916748\n",
      "cnt: 0 - valLoss: 0.6077565550804138 - trainLoss: 0.6136164665222168\n",
      "cnt: 0 - valLoss: 0.6077479720115662 - trainLoss: 0.613609254360199\n",
      "cnt: 0 - valLoss: 0.6077423691749573 - trainLoss: 0.6136016845703125\n",
      "cnt: 0 - valLoss: 0.6077343225479126 - trainLoss: 0.6135944128036499\n",
      "cnt: 0 - valLoss: 0.6077284812927246 - trainLoss: 0.6135868430137634\n",
      "cnt: 0 - valLoss: 0.607720673084259 - trainLoss: 0.6135794520378113\n",
      "cnt: 0 - valLoss: 0.6077145338058472 - trainLoss: 0.6135721206665039\n",
      "cnt: 0 - valLoss: 0.6077092289924622 - trainLoss: 0.6135644912719727\n",
      "cnt: 0 - valLoss: 0.6077006459236145 - trainLoss: 0.6135573387145996\n",
      "cnt: 0 - valLoss: 0.6076955199241638 - trainLoss: 0.6135495901107788\n",
      "cnt: 0 - valLoss: 0.6076868176460266 - trainLoss: 0.6135424971580505\n",
      "cnt: 0 - valLoss: 0.6076813340187073 - trainLoss: 0.6135348081588745\n",
      "cnt: 0 - valLoss: 0.6076731085777283 - trainLoss: 0.6135275959968567\n",
      "cnt: 0 - valLoss: 0.6076673269271851 - trainLoss: 0.6135200262069702\n",
      "cnt: 0 - valLoss: 0.6076594591140747 - trainLoss: 0.6135126352310181\n",
      "cnt: 0 - valLoss: 0.6076533198356628 - trainLoss: 0.6135052442550659\n",
      "cnt: 0 - valLoss: 0.6076456904411316 - trainLoss: 0.6134977340698242\n",
      "cnt: 0 - valLoss: 0.6076391935348511 - trainLoss: 0.6134904623031616\n",
      "cnt: 0 - valLoss: 0.6076322197914124 - trainLoss: 0.6134828329086304\n",
      "cnt: 0 - valLoss: 0.6076244115829468 - trainLoss: 0.6134757995605469\n",
      "cnt: 0 - valLoss: 0.6076173782348633 - trainLoss: 0.6134682893753052\n",
      "cnt: 0 - valLoss: 0.6076098084449768 - trainLoss: 0.6134611964225769\n",
      "cnt: 0 - valLoss: 0.607602596282959 - trainLoss: 0.6134536266326904\n",
      "cnt: 0 - valLoss: 0.6075950264930725 - trainLoss: 0.6134465932846069\n",
      "cnt: 0 - valLoss: 0.6075878143310547 - trainLoss: 0.6134390830993652\n",
      "cnt: 0 - valLoss: 0.6075803637504578 - trainLoss: 0.6134319305419922\n",
      "cnt: 0 - valLoss: 0.6075730323791504 - trainLoss: 0.6134244799613953\n",
      "cnt: 0 - valLoss: 0.607565701007843 - trainLoss: 0.6134172081947327\n",
      "cnt: 0 - valLoss: 0.6075582504272461 - trainLoss: 0.6134098172187805\n",
      "cnt: 0 - valLoss: 0.6075510382652283 - trainLoss: 0.6134025454521179\n",
      "cnt: 0 - valLoss: 0.607543408870697 - trainLoss: 0.6133952140808105\n",
      "cnt: 0 - valLoss: 0.6075363159179688 - trainLoss: 0.6133878231048584\n",
      "cnt: 0 - valLoss: 0.6075286865234375 - trainLoss: 0.6133806109428406\n",
      "cnt: 0 - valLoss: 0.607521653175354 - trainLoss: 0.6133731603622437\n",
      "cnt: 0 - valLoss: 0.6075138449668884 - trainLoss: 0.6133660078048706\n",
      "cnt: 0 - valLoss: 0.6075069904327393 - trainLoss: 0.6133584380149841\n",
      "cnt: 0 - valLoss: 0.6074990630149841 - trainLoss: 0.6133513450622559\n",
      "cnt: 0 - valLoss: 0.6074919700622559 - trainLoss: 0.6133437156677246\n",
      "cnt: 0 - valLoss: 0.6074844002723694 - trainLoss: 0.6133366227149963\n",
      "cnt: 0 - valLoss: 0.6074772477149963 - trainLoss: 0.6133289933204651\n",
      "cnt: 0 - valLoss: 0.6074697375297546 - trainLoss: 0.6133216619491577\n",
      "cnt: 0 - valLoss: 0.6074624061584473 - trainLoss: 0.6133140325546265\n",
      "cnt: 0 - valLoss: 0.6074551343917847 - trainLoss: 0.6133067011833191\n",
      "cnt: 0 - valLoss: 0.6074475646018982 - trainLoss: 0.6132991313934326\n",
      "cnt: 0 - valLoss: 0.6074404716491699 - trainLoss: 0.6132916212081909\n",
      "cnt: 0 - valLoss: 0.6074327826499939 - trainLoss: 0.613284170627594\n",
      "cnt: 0 - valLoss: 0.6074257493019104 - trainLoss: 0.6132766008377075\n",
      "cnt: 0 - valLoss: 0.6074180006980896 - trainLoss: 0.6132692098617554\n",
      "cnt: 0 - valLoss: 0.6074111461639404 - trainLoss: 0.6132615804672241\n",
      "cnt: 0 - valLoss: 0.6074032187461853 - trainLoss: 0.6132543087005615\n",
      "cnt: 0 - valLoss: 0.6073960661888123 - trainLoss: 0.6132465600967407\n",
      "cnt: 0 - valLoss: 0.6073886156082153 - trainLoss: 0.6132392883300781\n",
      "cnt: 0 - valLoss: 0.6073812246322632 - trainLoss: 0.6132315993309021\n",
      "cnt: 0 - valLoss: 0.6073739528656006 - trainLoss: 0.6132242679595947\n",
      "cnt: 0 - valLoss: 0.6073663830757141 - trainLoss: 0.6132166385650635\n",
      "cnt: 0 - valLoss: 0.6073592305183411 - trainLoss: 0.6132091283798218\n",
      "cnt: 0 - valLoss: 0.6073516607284546 - trainLoss: 0.6132017374038696\n",
      "cnt: 0 - valLoss: 0.6073445677757263 - trainLoss: 0.6131941080093384\n",
      "cnt: 0 - valLoss: 0.6073367595672607 - trainLoss: 0.6131867170333862\n",
      "cnt: 0 - valLoss: 0.6073299050331116 - trainLoss: 0.6131789684295654\n",
      "cnt: 0 - valLoss: 0.6073220372200012 - trainLoss: 0.6131717562675476\n",
      "cnt: 0 - valLoss: 0.6073148250579834 - trainLoss: 0.613163948059082\n",
      "cnt: 0 - valLoss: 0.6073073744773865 - trainLoss: 0.6131566166877747\n",
      "cnt: 0 - valLoss: 0.6072999835014343 - trainLoss: 0.6131489872932434\n",
      "cnt: 0 - valLoss: 0.6072935461997986 - trainLoss: 0.6131415367126465\n",
      "cnt: 0 - valLoss: 0.6072859168052673 - trainLoss: 0.6131340861320496\n",
      "cnt: 0 - valLoss: 0.6072789430618286 - trainLoss: 0.6131265163421631\n",
      "cnt: 0 - valLoss: 0.6072719693183899 - trainLoss: 0.6131191849708557\n",
      "cnt: 0 - valLoss: 0.6072651147842407 - trainLoss: 0.6131115555763245\n",
      "cnt: 0 - valLoss: 0.6072579026222229 - trainLoss: 0.6131042838096619\n",
      "cnt: 0 - valLoss: 0.6072509288787842 - trainLoss: 0.6130965352058411\n",
      "cnt: 0 - valLoss: 0.6072440147399902 - trainLoss: 0.6130893230438232\n",
      "cnt: 0 - valLoss: 0.6072368025779724 - trainLoss: 0.6130816340446472\n",
      "cnt: 0 - valLoss: 0.6072301268577576 - trainLoss: 0.6130743622779846\n",
      "cnt: 0 - valLoss: 0.6072227358818054 - trainLoss: 0.6130667924880981\n",
      "cnt: 0 - valLoss: 0.6072162389755249 - trainLoss: 0.6130593419075012\n",
      "cnt: 0 - valLoss: 0.6072086095809937 - trainLoss: 0.6130518317222595\n",
      "cnt: 0 - valLoss: 0.6072023510932922 - trainLoss: 0.6130443215370178\n",
      "cnt: 0 - valLoss: 0.6071945428848267 - trainLoss: 0.6130369305610657\n",
      "cnt: 0 - valLoss: 0.6071884036064148 - trainLoss: 0.6130293011665344\n",
      "cnt: 0 - valLoss: 0.6071804761886597 - trainLoss: 0.6130220293998718\n",
      "cnt: 0 - valLoss: 0.6071741580963135 - trainLoss: 0.613014280796051\n",
      "cnt: 0 - valLoss: 0.6071665287017822 - trainLoss: 0.6130070686340332\n",
      "cnt: 0 - valLoss: 0.607160210609436 - trainLoss: 0.6129993200302124\n",
      "cnt: 0 - valLoss: 0.6071517467498779 - trainLoss: 0.6129921674728394\n",
      "cnt: 0 - valLoss: 0.6071449518203735 - trainLoss: 0.6129846572875977\n",
      "cnt: 0 - valLoss: 0.6071383953094482 - trainLoss: 0.6129770874977112\n",
      "cnt: 0 - valLoss: 0.6071316003799438 - trainLoss: 0.6129696369171143\n",
      "cnt: 0 - valLoss: 0.6071241497993469 - trainLoss: 0.6129621267318726\n",
      "cnt: 0 - valLoss: 0.6071170568466187 - trainLoss: 0.6129547357559204\n",
      "cnt: 0 - valLoss: 0.6071099638938904 - trainLoss: 0.6129470467567444\n",
      "cnt: 0 - valLoss: 0.6071028113365173 - trainLoss: 0.6129400134086609\n",
      "cnt: 0 - valLoss: 0.6070963144302368 - trainLoss: 0.6129321455955505\n",
      "cnt: 0 - valLoss: 0.6070885062217712 - trainLoss: 0.6129249930381775\n",
      "cnt: 0 - valLoss: 0.6070818901062012 - trainLoss: 0.6129173636436462\n",
      "cnt: 0 - valLoss: 0.6070754528045654 - trainLoss: 0.6129099130630493\n",
      "cnt: 0 - valLoss: 0.6070668697357178 - trainLoss: 0.6129024624824524\n",
      "cnt: 0 - valLoss: 0.6070603728294373 - trainLoss: 0.6128950715065002\n",
      "cnt: 0 - valLoss: 0.6070534586906433 - trainLoss: 0.6128875613212585\n",
      "cnt: 0 - valLoss: 0.6070473194122314 - trainLoss: 0.6128798723220825\n",
      "cnt: 0 - valLoss: 0.6070390939712524 - trainLoss: 0.6128726601600647\n",
      "cnt: 0 - valLoss: 0.6070330142974854 - trainLoss: 0.6128649115562439\n",
      "cnt: 0 - valLoss: 0.6070247888565063 - trainLoss: 0.6128576993942261\n",
      "cnt: 0 - valLoss: 0.6070182919502258 - trainLoss: 0.6128500699996948\n",
      "cnt: 0 - valLoss: 0.6070116758346558 - trainLoss: 0.6128426194190979\n",
      "cnt: 0 - valLoss: 0.6070038676261902 - trainLoss: 0.612835168838501\n",
      "cnt: 0 - valLoss: 0.6069973111152649 - trainLoss: 0.6128276586532593\n",
      "cnt: 0 - valLoss: 0.6069895029067993 - trainLoss: 0.6128201484680176\n",
      "cnt: 0 - valLoss: 0.6069830060005188 - trainLoss: 0.6128126382827759\n",
      "cnt: 0 - valLoss: 0.6069760918617249 - trainLoss: 0.612805187702179\n",
      "cnt: 0 - valLoss: 0.6069687604904175 - trainLoss: 0.6127976179122925\n",
      "cnt: 0 - valLoss: 0.6069615483283997 - trainLoss: 0.6127902865409851\n",
      "cnt: 0 - valLoss: 0.6069543361663818 - trainLoss: 0.6127824187278748\n",
      "cnt: 0 - valLoss: 0.6069473028182983 - trainLoss: 0.612775444984436\n",
      "cnt: 0 - valLoss: 0.6069408655166626 - trainLoss: 0.6127675771713257\n",
      "cnt: 0 - valLoss: 0.606933057308197 - trainLoss: 0.6127602458000183\n",
      "cnt: 0 - valLoss: 0.6069262623786926 - trainLoss: 0.6127526760101318\n",
      "cnt: 0 - valLoss: 0.6069198846817017 - trainLoss: 0.6127451062202454\n",
      "cnt: 0 - valLoss: 0.606911838054657 - trainLoss: 0.6127377152442932\n",
      "cnt: 0 - valLoss: 0.606905460357666 - trainLoss: 0.612730085849762\n",
      "cnt: 0 - valLoss: 0.6068974137306213 - trainLoss: 0.6127226948738098\n",
      "cnt: 0 - valLoss: 0.6068911552429199 - trainLoss: 0.6127150654792786\n",
      "cnt: 0 - valLoss: 0.6068840622901917 - trainLoss: 0.6127076745033264\n",
      "cnt: 0 - valLoss: 0.60687655210495 - trainLoss: 0.6126999258995056\n",
      "cnt: 0 - valLoss: 0.606869637966156 - trainLoss: 0.6126927137374878\n",
      "cnt: 0 - valLoss: 0.606863260269165 - trainLoss: 0.6126849055290222\n",
      "cnt: 0 - valLoss: 0.6068553924560547 - trainLoss: 0.6126776337623596\n",
      "cnt: 0 - valLoss: 0.6068485975265503 - trainLoss: 0.6126700043678284\n",
      "cnt: 0 - valLoss: 0.6068410277366638 - trainLoss: 0.6126624345779419\n",
      "cnt: 0 - valLoss: 0.6068340539932251 - trainLoss: 0.6126550436019897\n",
      "cnt: 0 - valLoss: 0.6068277955055237 - trainLoss: 0.6126473546028137\n",
      "cnt: 0 - valLoss: 0.6068195700645447 - trainLoss: 0.6126400232315063\n",
      "cnt: 0 - valLoss: 0.606813371181488 - trainLoss: 0.6126322746276855\n",
      "cnt: 0 - valLoss: 0.6068063974380493 - trainLoss: 0.6126249432563782\n",
      "cnt: 0 - valLoss: 0.6067986488342285 - trainLoss: 0.6126173138618469\n",
      "cnt: 0 - valLoss: 0.6067917943000793 - trainLoss: 0.61260986328125\n",
      "cnt: 0 - valLoss: 0.6067841649055481 - trainLoss: 0.612602174282074\n",
      "cnt: 0 - valLoss: 0.6067774891853333 - trainLoss: 0.6125947833061218\n",
      "cnt: 0 - valLoss: 0.6067706942558289 - trainLoss: 0.6125871539115906\n",
      "cnt: 0 - valLoss: 0.6067631840705872 - trainLoss: 0.6125795841217041\n",
      "cnt: 0 - valLoss: 0.6067560911178589 - trainLoss: 0.6125723719596863\n",
      "cnt: 0 - valLoss: 0.6067497730255127 - trainLoss: 0.612564742565155\n",
      "cnt: 0 - valLoss: 0.6067416667938232 - trainLoss: 0.6125574707984924\n",
      "cnt: 0 - valLoss: 0.606735348701477 - trainLoss: 0.6125499606132507\n",
      "cnt: 0 - valLoss: 0.6067280769348145 - trainLoss: 0.6125427484512329\n",
      "cnt: 0 - valLoss: 0.606721818447113 - trainLoss: 0.6125350594520569\n",
      "cnt: 0 - valLoss: 0.6067137122154236 - trainLoss: 0.6125280857086182\n",
      "cnt: 0 - valLoss: 0.6067070960998535 - trainLoss: 0.6125203967094421\n",
      "cnt: 0 - valLoss: 0.6067003607749939 - trainLoss: 0.6125132441520691\n",
      "cnt: 0 - valLoss: 0.6066926717758179 - trainLoss: 0.6125057935714722\n",
      "cnt: 0 - valLoss: 0.6066858768463135 - trainLoss: 0.6124987602233887\n",
      "cnt: 0 - valLoss: 0.6066790819168091 - trainLoss: 0.6124913692474365\n",
      "cnt: 0 - valLoss: 0.6066724061965942 - trainLoss: 0.6124841570854187\n",
      "cnt: 0 - valLoss: 0.6066657304763794 - trainLoss: 0.6124768257141113\n",
      "cnt: 0 - valLoss: 0.6066579818725586 - trainLoss: 0.6124696731567383\n",
      "cnt: 0 - valLoss: 0.6066510677337646 - trainLoss: 0.6124623417854309\n",
      "cnt: 0 - valLoss: 0.6066444516181946 - trainLoss: 0.6124550104141235\n",
      "cnt: 0 - valLoss: 0.6066374778747559 - trainLoss: 0.6124477982521057\n",
      "cnt: 0 - valLoss: 0.6066300868988037 - trainLoss: 0.6124404072761536\n",
      "cnt: 0 - valLoss: 0.6066229939460754 - trainLoss: 0.6124333739280701\n",
      "cnt: 0 - valLoss: 0.6066166162490845 - trainLoss: 0.6124259233474731\n",
      "cnt: 0 - valLoss: 0.6066094040870667 - trainLoss: 0.6124187707901001\n",
      "cnt: 0 - valLoss: 0.60660320520401 - trainLoss: 0.6124112606048584\n",
      "cnt: 0 - valLoss: 0.6065949201583862 - trainLoss: 0.6124043464660645\n",
      "cnt: 0 - valLoss: 0.6065886616706848 - trainLoss: 0.612396776676178\n",
      "cnt: 0 - valLoss: 0.6065813302993774 - trainLoss: 0.6123898029327393\n",
      "cnt: 0 - valLoss: 0.6065748929977417 - trainLoss: 0.6123822331428528\n",
      "cnt: 0 - valLoss: 0.6065670251846313 - trainLoss: 0.6123751997947693\n",
      "cnt: 0 - valLoss: 0.6065604090690613 - trainLoss: 0.6123678088188171\n",
      "cnt: 0 - valLoss: 0.6065534949302673 - trainLoss: 0.6123607754707336\n",
      "cnt: 0 - valLoss: 0.6065468192100525 - trainLoss: 0.6123532652854919\n",
      "cnt: 0 - valLoss: 0.6065400242805481 - trainLoss: 0.6123461127281189\n",
      "cnt: 0 - valLoss: 0.6065323352813721 - trainLoss: 0.6123387813568115\n",
      "cnt: 0 - valLoss: 0.6065255403518677 - trainLoss: 0.6123316287994385\n",
      "cnt: 0 - valLoss: 0.6065187454223633 - trainLoss: 0.6123242974281311\n",
      "cnt: 0 - valLoss: 0.6065120100975037 - trainLoss: 0.6123170256614685\n",
      "cnt: 0 - valLoss: 0.6065052151679993 - trainLoss: 0.6123097538948059\n",
      "cnt: 0 - valLoss: 0.6064976453781128 - trainLoss: 0.6123025417327881\n",
      "cnt: 0 - valLoss: 0.6064905524253845 - trainLoss: 0.6122953295707703\n",
      "cnt: 0 - valLoss: 0.6064841151237488 - trainLoss: 0.6122879385948181\n",
      "cnt: 0 - valLoss: 0.6064770817756653 - trainLoss: 0.6122806668281555\n",
      "cnt: 0 - valLoss: 0.6064696311950684 - trainLoss: 0.6122732758522034\n",
      "cnt: 0 - valLoss: 0.6064624786376953 - trainLoss: 0.6122662425041199\n",
      "cnt: 0 - valLoss: 0.6064561605453491 - trainLoss: 0.6122586727142334\n",
      "cnt: 0 - valLoss: 0.6064488291740417 - trainLoss: 0.6122516393661499\n",
      "cnt: 0 - valLoss: 0.6064427495002747 - trainLoss: 0.6122440099716187\n",
      "cnt: 0 - valLoss: 0.6064343452453613 - trainLoss: 0.6122370362281799\n",
      "cnt: 0 - valLoss: 0.6064279079437256 - trainLoss: 0.6122294664382935\n",
      "cnt: 0 - valLoss: 0.6064208149909973 - trainLoss: 0.6122223734855652\n",
      "cnt: 0 - valLoss: 0.606414258480072 - trainLoss: 0.6122147440910339\n",
      "cnt: 0 - valLoss: 0.6064063906669617 - trainLoss: 0.6122077107429504\n",
      "cnt: 0 - valLoss: 0.6063997149467468 - trainLoss: 0.6122003197669983\n",
      "cnt: 0 - valLoss: 0.6063928604125977 - trainLoss: 0.6121931076049805\n",
      "cnt: 0 - valLoss: 0.6063860654830933 - trainLoss: 0.6121855974197388\n",
      "cnt: 0 - valLoss: 0.6063794493675232 - trainLoss: 0.6121783256530762\n",
      "cnt: 0 - valLoss: 0.6063715219497681 - trainLoss: 0.6121711134910583\n",
      "cnt: 0 - valLoss: 0.6063648462295532 - trainLoss: 0.612163782119751\n",
      "cnt: 0 - valLoss: 0.6063578724861145 - trainLoss: 0.6121564507484436\n",
      "cnt: 0 - valLoss: 0.6063512563705444 - trainLoss: 0.6121490597724915\n",
      "cnt: 0 - valLoss: 0.6063433289527893 - trainLoss: 0.6121417284011841\n",
      "cnt: 0 - valLoss: 0.606336772441864 - trainLoss: 0.6121343970298767\n",
      "cnt: 0 - valLoss: 0.6063296794891357 - trainLoss: 0.6121271252632141\n",
      "cnt: 0 - valLoss: 0.6063231825828552 - trainLoss: 0.6121196746826172\n",
      "cnt: 0 - valLoss: 0.606316089630127 - trainLoss: 0.6121125221252441\n",
      "cnt: 0 - valLoss: 0.6063086986541748 - trainLoss: 0.6121050119400024\n",
      "cnt: 0 - valLoss: 0.6063013672828674 - trainLoss: 0.612097978591919\n",
      "cnt: 0 - valLoss: 0.6062949299812317 - trainLoss: 0.6120902895927429\n",
      "cnt: 0 - valLoss: 0.6062878370285034 - trainLoss: 0.6120832562446594\n",
      "cnt: 0 - valLoss: 0.6062803864479065 - trainLoss: 0.6120756268501282\n",
      "cnt: 0 - valLoss: 0.6062732934951782 - trainLoss: 0.6120685935020447\n",
      "cnt: 0 - valLoss: 0.6062667369842529 - trainLoss: 0.612061083316803\n",
      "cnt: 0 - valLoss: 0.6062597036361694 - trainLoss: 0.6120538115501404\n",
      "cnt: 0 - valLoss: 0.6062530279159546 - trainLoss: 0.6120463013648987\n",
      "cnt: 0 - valLoss: 0.606245219707489 - trainLoss: 0.6120390892028809\n",
      "cnt: 0 - valLoss: 0.6062383651733398 - trainLoss: 0.6120316982269287\n",
      "cnt: 0 - valLoss: 0.6062316298484802 - trainLoss: 0.6120243668556213\n",
      "cnt: 0 - valLoss: 0.6062246561050415 - trainLoss: 0.6120169758796692\n",
      "cnt: 0 - valLoss: 0.606218159198761 - trainLoss: 0.6120095252990723\n",
      "cnt: 0 - valLoss: 0.6062101125717163 - trainLoss: 0.6120021939277649\n",
      "cnt: 0 - valLoss: 0.6062034964561462 - trainLoss: 0.6119946241378784\n",
      "cnt: 0 - valLoss: 0.6061963438987732 - trainLoss: 0.6119871735572815\n",
      "cnt: 0 - valLoss: 0.6061898469924927 - trainLoss: 0.6119795441627502\n",
      "cnt: 0 - valLoss: 0.6061827540397644 - trainLoss: 0.6119721531867981\n",
      "cnt: 0 - valLoss: 0.6061753630638123 - trainLoss: 0.6119645833969116\n",
      "cnt: 0 - valLoss: 0.6061679720878601 - trainLoss: 0.611957311630249\n",
      "cnt: 0 - valLoss: 0.6061617136001587 - trainLoss: 0.6119495034217834\n",
      "cnt: 0 - valLoss: 0.6061543822288513 - trainLoss: 0.6119422912597656\n",
      "cnt: 0 - valLoss: 0.6061479449272156 - trainLoss: 0.6119344234466553\n",
      "cnt: 0 - valLoss: 0.6061398983001709 - trainLoss: 0.611927330493927\n",
      "cnt: 0 - valLoss: 0.606133222579956 - trainLoss: 0.6119195818901062\n",
      "cnt: 0 - valLoss: 0.6061262488365173 - trainLoss: 0.6119122505187988\n",
      "cnt: 0 - valLoss: 0.6061194539070129 - trainLoss: 0.6119045615196228\n",
      "cnt: 0 - valLoss: 0.6061127185821533 - trainLoss: 0.6118971109390259\n",
      "cnt: 0 - valLoss: 0.6061047911643982 - trainLoss: 0.6118896007537842\n",
      "cnt: 0 - valLoss: 0.6060979962348938 - trainLoss: 0.6118821501731873\n",
      "cnt: 0 - valLoss: 0.6060909032821655 - trainLoss: 0.6118746399879456\n",
      "cnt: 0 - valLoss: 0.6060842275619507 - trainLoss: 0.6118670105934143\n",
      "cnt: 0 - valLoss: 0.6060771942138672 - trainLoss: 0.6118595004081726\n",
      "cnt: 0 - valLoss: 0.6060695648193359 - trainLoss: 0.6118519306182861\n",
      "cnt: 0 - valLoss: 0.6060624122619629 - trainLoss: 0.611844539642334\n",
      "cnt: 0 - valLoss: 0.6060558557510376 - trainLoss: 0.611836850643158\n",
      "cnt: 0 - valLoss: 0.6060485243797302 - trainLoss: 0.611829400062561\n",
      "cnt: 0 - valLoss: 0.6060421466827393 - trainLoss: 0.6118216514587402\n",
      "cnt: 0 - valLoss: 0.6060338616371155 - trainLoss: 0.6118144392967224\n",
      "cnt: 0 - valLoss: 0.6060272455215454 - trainLoss: 0.6118066310882568\n",
      "cnt: 0 - valLoss: 0.6060200333595276 - trainLoss: 0.6117993593215942\n",
      "cnt: 0 - valLoss: 0.6060133576393127 - trainLoss: 0.6117916107177734\n",
      "cnt: 0 - valLoss: 0.6060063242912292 - trainLoss: 0.6117842197418213\n",
      "cnt: 0 - valLoss: 0.6059986352920532 - trainLoss: 0.6117765307426453\n",
      "cnt: 0 - valLoss: 0.6059916615486145 - trainLoss: 0.6117691993713379\n",
      "cnt: 0 - valLoss: 0.6059847474098206 - trainLoss: 0.6117615103721619\n",
      "cnt: 0 - valLoss: 0.6059778332710266 - trainLoss: 0.6117540001869202\n",
      "cnt: 0 - valLoss: 0.6059708595275879 - trainLoss: 0.6117463707923889\n",
      "cnt: 0 - valLoss: 0.6059632301330566 - trainLoss: 0.6117388606071472\n",
      "cnt: 0 - valLoss: 0.6059560179710388 - trainLoss: 0.6117314100265503\n",
      "cnt: 0 - valLoss: 0.6059494018554688 - trainLoss: 0.6117237210273743\n",
      "cnt: 0 - valLoss: 0.6059421896934509 - trainLoss: 0.6117162704467773\n",
      "cnt: 0 - valLoss: 0.6059356331825256 - trainLoss: 0.6117085218429565\n",
      "cnt: 0 - valLoss: 0.6059284210205078 - trainLoss: 0.6117010712623596\n",
      "cnt: 0 - valLoss: 0.6059209704399109 - trainLoss: 0.6116934418678284\n",
      "cnt: 0 - valLoss: 0.605913519859314 - trainLoss: 0.611686110496521\n",
      "cnt: 0 - valLoss: 0.6059069037437439 - trainLoss: 0.6116782426834106\n",
      "cnt: 0 - valLoss: 0.6058997511863708 - trainLoss: 0.6116709113121033\n",
      "cnt: 0 - valLoss: 0.6058931946754456 - trainLoss: 0.6116631031036377\n",
      "cnt: 0 - valLoss: 0.6058850884437561 - trainLoss: 0.6116558313369751\n",
      "cnt: 0 - valLoss: 0.6058782339096069 - trainLoss: 0.6116480827331543\n",
      "cnt: 0 - valLoss: 0.6058712601661682 - trainLoss: 0.6116406917572021\n",
      "cnt: 0 - valLoss: 0.6058643460273743 - trainLoss: 0.6116329431533813\n",
      "cnt: 0 - valLoss: 0.6058576107025146 - trainLoss: 0.6116254329681396\n",
      "cnt: 0 - valLoss: 0.6058495044708252 - trainLoss: 0.6116178631782532\n",
      "cnt: 0 - valLoss: 0.6058428287506104 - trainLoss: 0.6116104125976562\n",
      "cnt: 0 - valLoss: 0.6058356761932373 - trainLoss: 0.6116028428077698\n",
      "cnt: 0 - valLoss: 0.6058289408683777 - trainLoss: 0.6115952134132385\n",
      "cnt: 0 - valLoss: 0.6058216691017151 - trainLoss: 0.6115877032279968\n",
      "cnt: 0 - valLoss: 0.6058144569396973 - trainLoss: 0.611579954624176\n",
      "cnt: 0 - valLoss: 0.6058072447776794 - trainLoss: 0.6115726828575134\n",
      "cnt: 0 - valLoss: 0.6058014631271362 - trainLoss: 0.611564576625824\n",
      "cnt: 0 - valLoss: 0.6057930588722229 - trainLoss: 0.6115573048591614\n",
      "cnt: 0 - valLoss: 0.6057868003845215 - trainLoss: 0.6115493178367615\n",
      "cnt: 0 - valLoss: 0.6057788729667664 - trainLoss: 0.6115418672561646\n",
      "cnt: 0 - valLoss: 0.6057723164558411 - trainLoss: 0.6115341186523438\n",
      "cnt: 0 - valLoss: 0.6057646870613098 - trainLoss: 0.6115264892578125\n",
      "cnt: 0 - valLoss: 0.6057578325271606 - trainLoss: 0.611518919467926\n",
      "cnt: 0 - valLoss: 0.6057504415512085 - trainLoss: 0.6115110516548157\n",
      "cnt: 0 - valLoss: 0.605743408203125 - trainLoss: 0.6115036010742188\n",
      "cnt: 0 - valLoss: 0.6057373881340027 - trainLoss: 0.6114956736564636\n",
      "cnt: 0 - valLoss: 0.6057290434837341 - trainLoss: 0.6114882826805115\n",
      "cnt: 0 - valLoss: 0.6057230234146118 - trainLoss: 0.6114803552627563\n",
      "cnt: 0 - valLoss: 0.6057146787643433 - trainLoss: 0.6114729642868042\n",
      "cnt: 0 - valLoss: 0.6057082414627075 - trainLoss: 0.6114649772644043\n",
      "cnt: 0 - valLoss: 0.6057004332542419 - trainLoss: 0.6114574670791626\n",
      "cnt: 0 - valLoss: 0.6056937575340271 - trainLoss: 0.6114498376846313\n",
      "cnt: 0 - valLoss: 0.6056873202323914 - trainLoss: 0.6114420890808105\n",
      "cnt: 0 - valLoss: 0.6056792736053467 - trainLoss: 0.6114344596862793\n",
      "cnt: 0 - valLoss: 0.6056728363037109 - trainLoss: 0.6114267110824585\n",
      "cnt: 0 - valLoss: 0.605664849281311 - trainLoss: 0.6114190816879272\n",
      "cnt: 0 - valLoss: 0.6056584715843201 - trainLoss: 0.6114113926887512\n",
      "cnt: 0 - valLoss: 0.6056516766548157 - trainLoss: 0.6114037036895752\n",
      "cnt: 0 - valLoss: 0.6056442260742188 - trainLoss: 0.6113960146903992\n",
      "cnt: 0 - valLoss: 0.60563725233078 - trainLoss: 0.6113883852958679\n",
      "cnt: 0 - valLoss: 0.605629563331604 - trainLoss: 0.6113805770874023\n",
      "cnt: 0 - valLoss: 0.6056227684020996 - trainLoss: 0.6113730669021606\n",
      "cnt: 0 - valLoss: 0.6056151390075684 - trainLoss: 0.6113651990890503\n",
      "cnt: 0 - valLoss: 0.6056084036827087 - trainLoss: 0.6113576292991638\n",
      "cnt: 0 - valLoss: 0.6056020259857178 - trainLoss: 0.6113497614860535\n",
      "cnt: 0 - valLoss: 0.6055941581726074 - trainLoss: 0.6113423705101013\n",
      "cnt: 0 - valLoss: 0.6055874824523926 - trainLoss: 0.6113346219062805\n",
      "cnt: 0 - valLoss: 0.6055799126625061 - trainLoss: 0.6113269925117493\n",
      "cnt: 0 - valLoss: 0.6055729389190674 - trainLoss: 0.6113194227218628\n",
      "cnt: 0 - valLoss: 0.6055657267570496 - trainLoss: 0.6113116145133972\n",
      "cnt: 0 - valLoss: 0.6055585145950317 - trainLoss: 0.6113043427467346\n",
      "cnt: 0 - valLoss: 0.6055511236190796 - trainLoss: 0.6112962961196899\n",
      "cnt: 0 - valLoss: 0.6055442690849304 - trainLoss: 0.6112890839576721\n",
      "cnt: 0 - valLoss: 0.6055378913879395 - trainLoss: 0.6112810969352722\n",
      "cnt: 0 - valLoss: 0.6055299639701843 - trainLoss: 0.6112737059593201\n",
      "cnt: 0 - valLoss: 0.6055232882499695 - trainLoss: 0.6112659573554993\n",
      "cnt: 0 - valLoss: 0.605515718460083 - trainLoss: 0.6112582087516785\n",
      "cnt: 0 - valLoss: 0.6055088043212891 - trainLoss: 0.6112506985664368\n",
      "cnt: 0 - valLoss: 0.605502724647522 - trainLoss: 0.6112428903579712\n",
      "cnt: 0 - valLoss: 0.6054943203926086 - trainLoss: 0.611235499382019\n",
      "cnt: 0 - valLoss: 0.6054883003234863 - trainLoss: 0.6112275123596191\n",
      "cnt: 0 - valLoss: 0.6054799556732178 - trainLoss: 0.6112201809883118\n",
      "cnt: 0 - valLoss: 0.6054735779762268 - trainLoss: 0.6112121939659119\n",
      "cnt: 0 - valLoss: 0.6054656505584717 - trainLoss: 0.6112047433853149\n",
      "cnt: 0 - valLoss: 0.6054590344429016 - trainLoss: 0.6111970543861389\n",
      "cnt: 0 - valLoss: 0.6054526567459106 - trainLoss: 0.6111893653869629\n",
      "cnt: 0 - valLoss: 0.6054445505142212 - trainLoss: 0.6111817359924316\n",
      "cnt: 0 - valLoss: 0.6054381728172302 - trainLoss: 0.6111740469932556\n",
      "cnt: 0 - valLoss: 0.6054300665855408 - trainLoss: 0.6111664175987244\n",
      "cnt: 0 - valLoss: 0.6054236888885498 - trainLoss: 0.6111587285995483\n",
      "cnt: 0 - valLoss: 0.6054155826568604 - trainLoss: 0.6111510396003723\n",
      "cnt: 0 - valLoss: 0.6054093241691589 - trainLoss: 0.6111433506011963\n",
      "cnt: 0 - valLoss: 0.605402410030365 - trainLoss: 0.6111357808113098\n",
      "cnt: 0 - valLoss: 0.6053947806358337 - trainLoss: 0.6111279726028442\n",
      "cnt: 0 - valLoss: 0.6053879261016846 - trainLoss: 0.6111205220222473\n",
      "cnt: 0 - valLoss: 0.6053801774978638 - trainLoss: 0.6111125946044922\n",
      "cnt: 0 - valLoss: 0.6053734421730042 - trainLoss: 0.6111051440238953\n",
      "cnt: 0 - valLoss: 0.6053669452667236 - trainLoss: 0.6110972762107849\n",
      "cnt: 0 - valLoss: 0.6053584814071655 - trainLoss: 0.6110897064208984\n",
      "cnt: 0 - valLoss: 0.6053515076637268 - trainLoss: 0.6110822558403015\n",
      "cnt: 0 - valLoss: 0.6053451299667358 - trainLoss: 0.6110742688179016\n",
      "cnt: 0 - valLoss: 0.6053368449211121 - trainLoss: 0.6110666990280151\n",
      "cnt: 0 - valLoss: 0.6053305864334106 - trainLoss: 0.6110588312149048\n",
      "cnt: 0 - valLoss: 0.6053236126899719 - trainLoss: 0.6110512614250183\n",
      "cnt: 0 - valLoss: 0.605315089225769 - trainLoss: 0.611043393611908\n",
      "cnt: 0 - valLoss: 0.6053081750869751 - trainLoss: 0.6110360026359558\n",
      "cnt: 0 - valLoss: 0.6053014993667603 - trainLoss: 0.6110280156135559\n",
      "cnt: 0 - valLoss: 0.6052936911582947 - trainLoss: 0.6110203266143799\n",
      "cnt: 0 - valLoss: 0.6052868366241455 - trainLoss: 0.6110126972198486\n",
      "cnt: 0 - valLoss: 0.6052805185317993 - trainLoss: 0.6110048294067383\n",
      "cnt: 0 - valLoss: 0.6052715182304382 - trainLoss: 0.6109973192214966\n",
      "cnt: 0 - valLoss: 0.6052650213241577 - trainLoss: 0.6109895706176758\n",
      "cnt: 0 - valLoss: 0.6052578687667847 - trainLoss: 0.610981822013855\n",
      "cnt: 0 - valLoss: 0.6052505970001221 - trainLoss: 0.6109736561775208\n",
      "cnt: 0 - valLoss: 0.6052433848381042 - trainLoss: 0.6109662055969238\n",
      "cnt: 0 - valLoss: 0.6052356958389282 - trainLoss: 0.6109579801559448\n",
      "cnt: 0 - valLoss: 0.6052289009094238 - trainLoss: 0.6109504699707031\n",
      "cnt: 0 - valLoss: 0.6052225232124329 - trainLoss: 0.6109422445297241\n",
      "cnt: 0 - valLoss: 0.6052137017250061 - trainLoss: 0.6109346747398376\n",
      "cnt: 0 - valLoss: 0.6052066683769226 - trainLoss: 0.6109268069267273\n",
      "cnt: 0 - valLoss: 0.6052005887031555 - trainLoss: 0.6109187006950378\n",
      "cnt: 0 - valLoss: 0.6051912903785706 - trainLoss: 0.6109111309051514\n",
      "cnt: 0 - valLoss: 0.6051847338676453 - trainLoss: 0.6109030842781067\n",
      "cnt: 0 - valLoss: 0.6051781177520752 - trainLoss: 0.6108953356742859\n",
      "cnt: 0 - valLoss: 0.6051700711250305 - trainLoss: 0.6108872890472412\n",
      "cnt: 0 - valLoss: 0.6051635146141052 - trainLoss: 0.6108795404434204\n",
      "cnt: 0 - valLoss: 0.6051554083824158 - trainLoss: 0.6108716130256653\n",
      "cnt: 0 - valLoss: 0.6051489114761353 - trainLoss: 0.6108637452125549\n",
      "cnt: 0 - valLoss: 0.6051406264305115 - trainLoss: 0.6108558773994446\n",
      "cnt: 0 - valLoss: 0.6051343679428101 - trainLoss: 0.6108479499816895\n",
      "cnt: 0 - valLoss: 0.6051260232925415 - trainLoss: 0.6108402013778687\n",
      "cnt: 0 - valLoss: 0.6051194071769714 - trainLoss: 0.6108322143554688\n",
      "cnt: 0 - valLoss: 0.6051129698753357 - trainLoss: 0.6108244061470032\n",
      "cnt: 0 - valLoss: 0.6051039099693298 - trainLoss: 0.6108166575431824\n",
      "cnt: 0 - valLoss: 0.605097234249115 - trainLoss: 0.610808789730072\n",
      "cnt: 0 - valLoss: 0.6050904393196106 - trainLoss: 0.6108007431030273\n",
      "cnt: 0 - valLoss: 0.6050819754600525 - trainLoss: 0.610792875289917\n",
      "cnt: 0 - valLoss: 0.6050746440887451 - trainLoss: 0.6107852458953857\n",
      "cnt: 0 - valLoss: 0.6050685048103333 - trainLoss: 0.610776960849762\n",
      "cnt: 0 - valLoss: 0.6050600409507751 - trainLoss: 0.6107693314552307\n",
      "cnt: 0 - valLoss: 0.6050534248352051 - trainLoss: 0.610761284828186\n",
      "cnt: 0 - valLoss: 0.60504549741745 - trainLoss: 0.6107534766197205\n",
      "cnt: 0 - valLoss: 0.6050385236740112 - trainLoss: 0.6107456088066101\n",
      "cnt: 0 - valLoss: 0.60503089427948 - trainLoss: 0.6107375621795654\n",
      "cnt: 0 - valLoss: 0.6050237417221069 - trainLoss: 0.6107299327850342\n",
      "cnt: 0 - valLoss: 0.6050177812576294 - trainLoss: 0.6107217073440552\n",
      "cnt: 0 - valLoss: 0.6050083041191101 - trainLoss: 0.6107141971588135\n",
      "cnt: 0 - valLoss: 0.60500168800354 - trainLoss: 0.610706090927124\n",
      "cnt: 0 - valLoss: 0.6049948930740356 - trainLoss: 0.6106982827186584\n",
      "cnt: 0 - valLoss: 0.6049867868423462 - trainLoss: 0.6106902360916138\n",
      "cnt: 0 - valLoss: 0.6049802303314209 - trainLoss: 0.6106824278831482\n",
      "cnt: 0 - valLoss: 0.6049720644950867 - trainLoss: 0.6106744408607483\n",
      "cnt: 0 - valLoss: 0.6049654483795166 - trainLoss: 0.6106665730476379\n",
      "cnt: 0 - valLoss: 0.604958713054657 - trainLoss: 0.6106585264205933\n",
      "cnt: 0 - valLoss: 0.6049501299858093 - trainLoss: 0.6106508374214172\n",
      "cnt: 0 - valLoss: 0.6049427390098572 - trainLoss: 0.6106429696083069\n",
      "cnt: 0 - valLoss: 0.6049363017082214 - trainLoss: 0.6106346249580383\n",
      "cnt: 0 - valLoss: 0.6049273610115051 - trainLoss: 0.6106270551681519\n",
      "cnt: 0 - valLoss: 0.6049204468727112 - trainLoss: 0.6106191277503967\n",
      "cnt: 0 - valLoss: 0.6049138903617859 - trainLoss: 0.610611081123352\n",
      "cnt: 0 - valLoss: 0.6049055457115173 - trainLoss: 0.6106031537055969\n",
      "cnt: 0 - valLoss: 0.6048990488052368 - trainLoss: 0.610595166683197\n",
      "cnt: 0 - valLoss: 0.6048907041549683 - trainLoss: 0.6105872392654419\n",
      "cnt: 0 - valLoss: 0.6048840880393982 - trainLoss: 0.610579252243042\n",
      "cnt: 0 - valLoss: 0.6048772931098938 - trainLoss: 0.6105714440345764\n",
      "cnt: 0 - valLoss: 0.6048684120178223 - trainLoss: 0.6105634570121765\n",
      "cnt: 0 - valLoss: 0.6048615574836731 - trainLoss: 0.6105556488037109\n",
      "cnt: 0 - valLoss: 0.6048546433448792 - trainLoss: 0.6105474829673767\n",
      "cnt: 0 - valLoss: 0.6048468351364136 - trainLoss: 0.6105395555496216\n",
      "cnt: 0 - valLoss: 0.604839563369751 - trainLoss: 0.6105316877365112\n",
      "cnt: 0 - valLoss: 0.6048320531845093 - trainLoss: 0.6105234622955322\n",
      "cnt: 0 - valLoss: 0.6048247814178467 - trainLoss: 0.6105158925056458\n",
      "cnt: 0 - valLoss: 0.6048186421394348 - trainLoss: 0.610507607460022\n",
      "cnt: 0 - valLoss: 0.6048104763031006 - trainLoss: 0.6105000972747803\n",
      "cnt: 0 - valLoss: 0.6048039197921753 - trainLoss: 0.6104921102523804\n",
      "cnt: 0 - valLoss: 0.6047977209091187 - trainLoss: 0.61048424243927\n",
      "cnt: 0 - valLoss: 0.604788601398468 - trainLoss: 0.6104766130447388\n",
      "cnt: 0 - valLoss: 0.6047822833061218 - trainLoss: 0.6104688048362732\n",
      "cnt: 0 - valLoss: 0.6047753095626831 - trainLoss: 0.6104609370231628\n",
      "cnt: 0 - valLoss: 0.6047694087028503 - trainLoss: 0.6104528903961182\n",
      "cnt: 0 - valLoss: 0.6047608256340027 - trainLoss: 0.6104453206062317\n",
      "cnt: 0 - valLoss: 0.6047549247741699 - trainLoss: 0.610437273979187\n",
      "cnt: 0 - valLoss: 0.6047480702400208 - trainLoss: 0.6104296445846558\n",
      "cnt: 0 - valLoss: 0.604739248752594 - trainLoss: 0.6104216575622559\n",
      "cnt: 0 - valLoss: 0.6047325730323792 - trainLoss: 0.6104140281677246\n",
      "cnt: 0 - valLoss: 0.6047260165214539 - trainLoss: 0.6104059815406799\n",
      "cnt: 0 - valLoss: 0.6047197580337524 - trainLoss: 0.6103981137275696\n",
      "cnt: 0 - valLoss: 0.6047114729881287 - trainLoss: 0.6103903651237488\n",
      "cnt: 0 - valLoss: 0.6047052145004272 - trainLoss: 0.6103824377059937\n",
      "cnt: 0 - valLoss: 0.6046984791755676 - trainLoss: 0.6103746294975281\n",
      "cnt: 0 - valLoss: 0.6046908497810364 - trainLoss: 0.6103668212890625\n",
      "cnt: 0 - valLoss: 0.6046836972236633 - trainLoss: 0.6103590726852417\n",
      "cnt: 0 - valLoss: 0.6046777963638306 - trainLoss: 0.6103509068489075\n",
      "cnt: 0 - valLoss: 0.6046684980392456 - trainLoss: 0.6103435158729553\n",
      "cnt: 0 - valLoss: 0.6046621203422546 - trainLoss: 0.6103354692459106\n",
      "cnt: 0 - valLoss: 0.6046554446220398 - trainLoss: 0.6103276610374451\n",
      "cnt: 0 - valLoss: 0.604649007320404 - trainLoss: 0.6103196144104004\n",
      "cnt: 0 - valLoss: 0.6046410799026489 - trainLoss: 0.6103119254112244\n",
      "cnt: 0 - valLoss: 0.6046342253684998 - trainLoss: 0.6103041172027588\n",
      "cnt: 0 - valLoss: 0.604628324508667 - trainLoss: 0.6102960705757141\n",
      "cnt: 0 - valLoss: 0.6046189069747925 - trainLoss: 0.6102885007858276\n",
      "cnt: 0 - valLoss: 0.6046130657196045 - trainLoss: 0.6102805733680725\n",
      "cnt: 0 - valLoss: 0.6046060919761658 - trainLoss: 0.610273003578186\n",
      "cnt: 0 - valLoss: 0.6046003103256226 - trainLoss: 0.6102649569511414\n",
      "cnt: 0 - valLoss: 0.6045913100242615 - trainLoss: 0.6102576851844788\n",
      "cnt: 0 - valLoss: 0.6045849919319153 - trainLoss: 0.6102498769760132\n",
      "cnt: 0 - valLoss: 0.6045786738395691 - trainLoss: 0.6102421879768372\n",
      "cnt: 0 - valLoss: 0.6045724153518677 - trainLoss: 0.6102343201637268\n",
      "cnt: 0 - valLoss: 0.60456383228302 - trainLoss: 0.6102268099784851\n",
      "cnt: 0 - valLoss: 0.6045570969581604 - trainLoss: 0.6102192401885986\n",
      "cnt: 0 - valLoss: 0.6045510768890381 - trainLoss: 0.6102113127708435\n",
      "cnt: 0 - valLoss: 0.6045445799827576 - trainLoss: 0.6102036237716675\n",
      "cnt: 0 - valLoss: 0.6045362949371338 - trainLoss: 0.6101959347724915\n",
      "cnt: 0 - valLoss: 0.6045293807983398 - trainLoss: 0.6101885437965393\n",
      "cnt: 0 - valLoss: 0.6045231223106384 - trainLoss: 0.6101805567741394\n",
      "cnt: 0 - valLoss: 0.6045158505439758 - trainLoss: 0.6101728677749634\n",
      "cnt: 0 - valLoss: 0.6045095920562744 - trainLoss: 0.6101650595664978\n",
      "cnt: 0 - valLoss: 0.6045011281967163 - trainLoss: 0.6101574301719666\n",
      "cnt: 0 - valLoss: 0.6044945120811462 - trainLoss: 0.6101499199867249\n",
      "cnt: 0 - valLoss: 0.6044884324073792 - trainLoss: 0.6101420521736145\n",
      "cnt: 0 - valLoss: 0.6044817566871643 - trainLoss: 0.6101343035697937\n",
      "cnt: 0 - valLoss: 0.6044737100601196 - trainLoss: 0.6101264953613281\n",
      "cnt: 0 - valLoss: 0.6044667363166809 - trainLoss: 0.6101191639900208\n",
      "cnt: 0 - valLoss: 0.6044606566429138 - trainLoss: 0.6101111173629761\n",
      "cnt: 0 - valLoss: 0.6044542193412781 - trainLoss: 0.6101035475730896\n",
      "cnt: 0 - valLoss: 0.6044469475746155 - trainLoss: 0.6100956201553345\n",
      "cnt: 0 - valLoss: 0.6044409871101379 - trainLoss: 0.6100879907608032\n",
      "cnt: 0 - valLoss: 0.6044319868087769 - trainLoss: 0.610080361366272\n",
      "cnt: 0 - valLoss: 0.604425847530365 - trainLoss: 0.610072672367096\n",
      "cnt: 0 - valLoss: 0.6044191718101501 - trainLoss: 0.6100648641586304\n",
      "cnt: 0 - valLoss: 0.604412317276001 - trainLoss: 0.6100569367408752\n",
      "cnt: 0 - valLoss: 0.6044054627418518 - trainLoss: 0.6100494265556335\n",
      "cnt: 0 - valLoss: 0.6043987274169922 - trainLoss: 0.6100414395332336\n",
      "cnt: 0 - valLoss: 0.6043919920921326 - trainLoss: 0.6100338697433472\n",
      "cnt: 0 - valLoss: 0.6043831706047058 - trainLoss: 0.610025942325592\n",
      "cnt: 0 - valLoss: 0.6043765544891357 - trainLoss: 0.6100183129310608\n",
      "cnt: 0 - valLoss: 0.6043700575828552 - trainLoss: 0.6100101470947266\n",
      "cnt: 0 - valLoss: 0.6043635010719299 - trainLoss: 0.6100022792816162\n",
      "cnt: 0 - valLoss: 0.6043544411659241 - trainLoss: 0.6099942326545715\n",
      "cnt: 0 - valLoss: 0.6043481230735779 - trainLoss: 0.6099864840507507\n",
      "cnt: 0 - valLoss: 0.6043411493301392 - trainLoss: 0.6099784970283508\n",
      "cnt: 0 - valLoss: 0.6043348908424377 - trainLoss: 0.6099702715873718\n",
      "cnt: 0 - valLoss: 0.604327917098999 - trainLoss: 0.6099624037742615\n",
      "cnt: 0 - valLoss: 0.604319155216217 - trainLoss: 0.6099542379379272\n",
      "cnt: 0 - valLoss: 0.6043123602867126 - trainLoss: 0.6099465489387512\n",
      "cnt: 0 - valLoss: 0.6043057441711426 - trainLoss: 0.6099382638931274\n",
      "cnt: 0 - valLoss: 0.6042990684509277 - trainLoss: 0.6099302172660828\n",
      "cnt: 0 - valLoss: 0.6042909622192383 - trainLoss: 0.6099220514297485\n",
      "cnt: 0 - valLoss: 0.6042844653129578 - trainLoss: 0.6099141836166382\n",
      "cnt: 0 - valLoss: 0.6042775511741638 - trainLoss: 0.6099061369895935\n",
      "cnt: 0 - valLoss: 0.6042714715003967 - trainLoss: 0.6098978519439697\n",
      "cnt: 0 - valLoss: 0.6042618751525879 - trainLoss: 0.6098901033401489\n",
      "cnt: 0 - valLoss: 0.6042557954788208 - trainLoss: 0.6098819375038147\n",
      "cnt: 0 - valLoss: 0.604248583316803 - trainLoss: 0.6098740696907043\n",
      "cnt: 0 - valLoss: 0.6042421460151672 - trainLoss: 0.609865665435791\n",
      "cnt: 0 - valLoss: 0.6042340397834778 - trainLoss: 0.6098577976226807\n",
      "cnt: 0 - valLoss: 0.6042274236679077 - trainLoss: 0.609849750995636\n",
      "cnt: 0 - valLoss: 0.604220986366272 - trainLoss: 0.6098417043685913\n",
      "cnt: 0 - valLoss: 0.604212760925293 - trainLoss: 0.6098336577415466\n",
      "cnt: 0 - valLoss: 0.6042065024375916 - trainLoss: 0.6098257303237915\n",
      "cnt: 0 - valLoss: 0.6041994690895081 - trainLoss: 0.6098177433013916\n",
      "cnt: 0 - valLoss: 0.6041932702064514 - trainLoss: 0.6098094582557678\n",
      "cnt: 0 - valLoss: 0.6041842699050903 - trainLoss: 0.6098014712333679\n",
      "cnt: 0 - valLoss: 0.6041774749755859 - trainLoss: 0.6097928285598755\n",
      "cnt: 0 - valLoss: 0.604170560836792 - trainLoss: 0.6097846031188965\n",
      "cnt: 0 - valLoss: 0.6041621565818787 - trainLoss: 0.6097761988639832\n",
      "cnt: 0 - valLoss: 0.6041552424430847 - trainLoss: 0.6097679734230042\n",
      "cnt: 0 - valLoss: 0.6041481494903564 - trainLoss: 0.6097594499588013\n",
      "cnt: 0 - valLoss: 0.6041401028633118 - trainLoss: 0.6097509264945984\n",
      "cnt: 0 - valLoss: 0.6041328310966492 - trainLoss: 0.6097426414489746\n",
      "cnt: 0 - valLoss: 0.6041262745857239 - trainLoss: 0.6097339987754822\n",
      "cnt: 0 - valLoss: 0.6041189432144165 - trainLoss: 0.6097256541252136\n",
      "cnt: 0 - valLoss: 0.6041107177734375 - trainLoss: 0.6097170114517212\n",
      "cnt: 0 - valLoss: 0.6041036248207092 - trainLoss: 0.609708845615387\n",
      "cnt: 0 - valLoss: 0.6040967702865601 - trainLoss: 0.6097000241279602\n",
      "cnt: 0 - valLoss: 0.6040884256362915 - trainLoss: 0.6096916794776917\n",
      "cnt: 0 - valLoss: 0.6040813326835632 - trainLoss: 0.6096832752227783\n",
      "cnt: 0 - valLoss: 0.6040745973587036 - trainLoss: 0.6096747517585754\n",
      "cnt: 0 - valLoss: 0.604067325592041 - trainLoss: 0.6096662878990173\n",
      "cnt: 0 - valLoss: 0.6040592789649963 - trainLoss: 0.6096577048301697\n",
      "cnt: 0 - valLoss: 0.6040517091751099 - trainLoss: 0.6096494197845459\n",
      "cnt: 0 - valLoss: 0.6040449738502502 - trainLoss: 0.6096405982971191\n",
      "cnt: 0 - valLoss: 0.6040363907814026 - trainLoss: 0.6096323132514954\n",
      "cnt: 0 - valLoss: 0.6040294170379639 - trainLoss: 0.6096237301826477\n",
      "cnt: 0 - valLoss: 0.6040224432945251 - trainLoss: 0.6096153259277344\n",
      "cnt: 0 - valLoss: 0.6040154695510864 - trainLoss: 0.6096066832542419\n",
      "cnt: 0 - valLoss: 0.6040071249008179 - trainLoss: 0.6095983386039734\n",
      "cnt: 0 - valLoss: 0.6039998531341553 - trainLoss: 0.6095898747444153\n",
      "cnt: 0 - valLoss: 0.60399329662323 - trainLoss: 0.6095812320709229\n",
      "cnt: 0 - valLoss: 0.6039844155311584 - trainLoss: 0.6095728874206543\n",
      "cnt: 0 - valLoss: 0.6039779186248779 - trainLoss: 0.6095643043518066\n",
      "cnt: 0 - valLoss: 0.6039702892303467 - trainLoss: 0.6095560789108276\n",
      "cnt: 0 - valLoss: 0.6039634943008423 - trainLoss: 0.6095472574234009\n",
      "cnt: 0 - valLoss: 0.6039546728134155 - trainLoss: 0.6095390319824219\n",
      "cnt: 0 - valLoss: 0.6039475202560425 - trainLoss: 0.6095303893089294\n",
      "cnt: 0 - valLoss: 0.6039406657218933 - trainLoss: 0.6095218062400818\n",
      "cnt: 0 - valLoss: 0.6039316654205322 - trainLoss: 0.6095133423805237\n",
      "cnt: 0 - valLoss: 0.6039248704910278 - trainLoss: 0.609504759311676\n",
      "cnt: 0 - valLoss: 0.6039171814918518 - trainLoss: 0.6094962954521179\n",
      "cnt: 0 - valLoss: 0.6039091944694519 - trainLoss: 0.6094874143600464\n",
      "cnt: 0 - valLoss: 0.6039014458656311 - trainLoss: 0.6094793081283569\n",
      "cnt: 0 - valLoss: 0.6038944125175476 - trainLoss: 0.6094704866409302\n",
      "cnt: 0 - valLoss: 0.6038873195648193 - trainLoss: 0.6094619631767273\n",
      "cnt: 0 - valLoss: 0.6038784980773926 - trainLoss: 0.6094534397125244\n",
      "cnt: 0 - valLoss: 0.6038714647293091 - trainLoss: 0.6094449162483215\n",
      "cnt: 0 - valLoss: 0.6038640141487122 - trainLoss: 0.6094362735748291\n",
      "cnt: 0 - valLoss: 0.6038557887077332 - trainLoss: 0.6094276309013367\n",
      "cnt: 0 - valLoss: 0.6038480401039124 - trainLoss: 0.6094192862510681\n",
      "cnt: 0 - valLoss: 0.6038414835929871 - trainLoss: 0.6094104647636414\n",
      "cnt: 0 - valLoss: 0.6038339138031006 - trainLoss: 0.609402060508728\n",
      "cnt: 0 - valLoss: 0.6038252115249634 - trainLoss: 0.6093934178352356\n",
      "cnt: 0 - valLoss: 0.6038179993629456 - trainLoss: 0.6093849539756775\n",
      "cnt: 0 - valLoss: 0.6038106679916382 - trainLoss: 0.6093761920928955\n",
      "cnt: 0 - valLoss: 0.6038021445274353 - trainLoss: 0.6093676090240479\n",
      "cnt: 0 - valLoss: 0.6037945747375488 - trainLoss: 0.6093591451644897\n",
      "cnt: 0 - valLoss: 0.6037877202033997 - trainLoss: 0.6093504428863525\n",
      "cnt: 0 - valLoss: 0.6037799119949341 - trainLoss: 0.609342098236084\n",
      "cnt: 0 - valLoss: 0.6037717461585999 - trainLoss: 0.6093333959579468\n",
      "cnt: 0 - valLoss: 0.6037638783454895 - trainLoss: 0.6093252897262573\n",
      "cnt: 0 - valLoss: 0.6037567257881165 - trainLoss: 0.6093164682388306\n",
      "cnt: 0 - valLoss: 0.6037492156028748 - trainLoss: 0.609308123588562\n",
      "cnt: 0 - valLoss: 0.6037420034408569 - trainLoss: 0.6092994213104248\n",
      "cnt: 0 - valLoss: 0.603733241558075 - trainLoss: 0.6092910170555115\n",
      "cnt: 0 - valLoss: 0.6037257313728333 - trainLoss: 0.6092824339866638\n",
      "cnt: 0 - valLoss: 0.603718638420105 - trainLoss: 0.6092739105224609\n",
      "cnt: 0 - valLoss: 0.603710949420929 - trainLoss: 0.6092654466629028\n",
      "cnt: 0 - valLoss: 0.6037026047706604 - trainLoss: 0.6092568039894104\n",
      "cnt: 0 - valLoss: 0.60369473695755 - trainLoss: 0.6092484593391418\n",
      "cnt: 0 - valLoss: 0.6036880612373352 - trainLoss: 0.6092397570610046\n",
      "cnt: 0 - valLoss: 0.6036801934242249 - trainLoss: 0.6092315316200256\n",
      "cnt: 0 - valLoss: 0.6036731600761414 - trainLoss: 0.6092227697372437\n",
      "cnt: 0 - valLoss: 0.6036643385887146 - trainLoss: 0.6092146039009094\n",
      "cnt: 0 - valLoss: 0.6036570072174072 - trainLoss: 0.6092060804367065\n",
      "cnt: 0 - valLoss: 0.6036496758460999 - trainLoss: 0.6091976165771484\n",
      "cnt: 0 - valLoss: 0.6036422252655029 - trainLoss: 0.6091890931129456\n",
      "cnt: 0 - valLoss: 0.6036353707313538 - trainLoss: 0.6091805696487427\n",
      "cnt: 0 - valLoss: 0.6036261916160583 - trainLoss: 0.6091722249984741\n",
      "cnt: 0 - valLoss: 0.6036192774772644 - trainLoss: 0.6091636419296265\n",
      "cnt: 0 - valLoss: 0.603611409664154 - trainLoss: 0.6091552972793579\n",
      "cnt: 0 - valLoss: 0.603604793548584 - trainLoss: 0.6091465950012207\n",
      "cnt: 0 - valLoss: 0.6035954356193542 - trainLoss: 0.6091384887695312\n",
      "cnt: 0 - valLoss: 0.603588342666626 - trainLoss: 0.6091297268867493\n",
      "cnt: 0 - valLoss: 0.6035811901092529 - trainLoss: 0.6091214418411255\n",
      "cnt: 0 - valLoss: 0.6035741567611694 - trainLoss: 0.6091127991676331\n",
      "cnt: 0 - valLoss: 0.6035674214363098 - trainLoss: 0.6091043949127197\n",
      "cnt: 0 - valLoss: 0.603558361530304 - trainLoss: 0.6090959310531616\n",
      "cnt: 0 - valLoss: 0.6035516858100891 - trainLoss: 0.6090874671936035\n",
      "cnt: 0 - valLoss: 0.6035441756248474 - trainLoss: 0.6090789437294006\n",
      "cnt: 0 - valLoss: 0.6035376787185669 - trainLoss: 0.6090702414512634\n",
      "cnt: 0 - valLoss: 0.6035283803939819 - trainLoss: 0.6090619564056396\n",
      "cnt: 0 - valLoss: 0.6035217046737671 - trainLoss: 0.6090533137321472\n",
      "cnt: 0 - valLoss: 0.6035144329071045 - trainLoss: 0.6090450286865234\n",
      "cnt: 0 - valLoss: 0.6035074591636658 - trainLoss: 0.6090362071990967\n",
      "cnt: 0 - valLoss: 0.6035004258155823 - trainLoss: 0.6090278029441833\n",
      "cnt: 0 - valLoss: 0.6034916043281555 - trainLoss: 0.6090192198753357\n",
      "cnt: 0 - valLoss: 0.6034847497940063 - trainLoss: 0.6090108156204224\n",
      "cnt: 0 - valLoss: 0.6034772396087646 - trainLoss: 0.6090022325515747\n",
      "cnt: 0 - valLoss: 0.6034704446792603 - trainLoss: 0.6089933514595032\n",
      "cnt: 0 - valLoss: 0.603462815284729 - trainLoss: 0.6089847683906555\n",
      "cnt: 0 - valLoss: 0.6034544110298157 - trainLoss: 0.6089759469032288\n",
      "cnt: 0 - valLoss: 0.6034466028213501 - trainLoss: 0.6089675426483154\n",
      "cnt: 0 - valLoss: 0.6034395694732666 - trainLoss: 0.6089585423469543\n",
      "cnt: 0 - valLoss: 0.6034322381019592 - trainLoss: 0.6089499592781067\n",
      "cnt: 0 - valLoss: 0.6034252047538757 - trainLoss: 0.6089410781860352\n",
      "cnt: 0 - valLoss: 0.6034162640571594 - trainLoss: 0.6089325547218323\n",
      "cnt: 0 - valLoss: 0.6034088134765625 - trainLoss: 0.6089237928390503\n",
      "cnt: 0 - valLoss: 0.6034018993377686 - trainLoss: 0.6089150309562683\n",
      "cnt: 0 - valLoss: 0.6033942699432373 - trainLoss: 0.6089063286781311\n",
      "cnt: 0 - valLoss: 0.6033859252929688 - trainLoss: 0.6088974475860596\n",
      "cnt: 0 - valLoss: 0.6033781170845032 - trainLoss: 0.6088890433311462\n",
      "cnt: 0 - valLoss: 0.6033711433410645 - trainLoss: 0.6088799834251404\n",
      "cnt: 0 - valLoss: 0.6033637523651123 - trainLoss: 0.6088714599609375\n",
      "cnt: 0 - valLoss: 0.6033567190170288 - trainLoss: 0.6088625192642212\n",
      "cnt: 0 - valLoss: 0.6033477783203125 - trainLoss: 0.6088539958000183\n",
      "cnt: 0 - valLoss: 0.6033403873443604 - trainLoss: 0.6088452339172363\n",
      "cnt: 0 - valLoss: 0.6033333539962769 - trainLoss: 0.6088364720344543\n",
      "cnt: 0 - valLoss: 0.6033258438110352 - trainLoss: 0.6088276505470276\n",
      "cnt: 0 - valLoss: 0.6033191084861755 - trainLoss: 0.608818769454956\n",
      "cnt: 0 - valLoss: 0.6033095717430115 - trainLoss: 0.6088101863861084\n",
      "cnt: 0 - valLoss: 0.6033029556274414 - trainLoss: 0.6088013052940369\n",
      "cnt: 0 - valLoss: 0.603295087814331 - trainLoss: 0.6087926626205444\n",
      "cnt: 0 - valLoss: 0.6032880544662476 - trainLoss: 0.6087836623191833\n",
      "cnt: 0 - valLoss: 0.6032808423042297 - trainLoss: 0.6087750196456909\n",
      "cnt: 0 - valLoss: 0.6032716631889343 - trainLoss: 0.6087661981582642\n",
      "cnt: 0 - valLoss: 0.6032643914222717 - trainLoss: 0.6087574362754822\n",
      "cnt: 0 - valLoss: 0.6032567024230957 - trainLoss: 0.6087484955787659\n",
      "cnt: 0 - valLoss: 0.6032496094703674 - trainLoss: 0.6087394952774048\n",
      "cnt: 0 - valLoss: 0.6032418012619019 - trainLoss: 0.6087305545806885\n",
      "cnt: 0 - valLoss: 0.603233277797699 - trainLoss: 0.6087216138839722\n",
      "cnt: 0 - valLoss: 0.6032252311706543 - trainLoss: 0.6087129712104797\n",
      "cnt: 0 - valLoss: 0.6032179594039917 - trainLoss: 0.6087036728858948\n",
      "cnt: 0 - valLoss: 0.6032105088233948 - trainLoss: 0.6086949706077576\n",
      "cnt: 0 - valLoss: 0.6032031774520874 - trainLoss: 0.6086858510971069\n",
      "cnt: 0 - valLoss: 0.6031940579414368 - trainLoss: 0.6086771488189697\n",
      "cnt: 0 - valLoss: 0.6031863689422607 - trainLoss: 0.6086681485176086\n",
      "cnt: 0 - valLoss: 0.6031793355941772 - trainLoss: 0.6086592078208923\n",
      "cnt: 0 - valLoss: 0.6031714081764221 - trainLoss: 0.6086503863334656\n",
      "cnt: 0 - valLoss: 0.6031647324562073 - trainLoss: 0.6086412668228149\n",
      "cnt: 0 - valLoss: 0.6031549572944641 - trainLoss: 0.6086326837539673\n",
      "cnt: 0 - valLoss: 0.6031476855278015 - trainLoss: 0.6086235642433167\n",
      "cnt: 0 - valLoss: 0.6031402349472046 - trainLoss: 0.6086148619651794\n",
      "cnt: 0 - valLoss: 0.6031327843666077 - trainLoss: 0.6086058020591736\n",
      "cnt: 0 - valLoss: 0.6031255722045898 - trainLoss: 0.6085969805717468\n",
      "cnt: 0 - valLoss: 0.6031160950660706 - trainLoss: 0.6085880398750305\n",
      "cnt: 0 - valLoss: 0.6031091213226318 - trainLoss: 0.608579158782959\n",
      "cnt: 0 - valLoss: 0.6031011939048767 - trainLoss: 0.6085703372955322\n",
      "cnt: 0 - valLoss: 0.6030943989753723 - trainLoss: 0.6085611581802368\n",
      "cnt: 0 - valLoss: 0.6030864715576172 - trainLoss: 0.6085525155067444\n",
      "cnt: 0 - valLoss: 0.6030774116516113 - trainLoss: 0.608543336391449\n",
      "cnt: 0 - valLoss: 0.6030700206756592 - trainLoss: 0.6085346937179565\n",
      "cnt: 0 - valLoss: 0.6030623912811279 - trainLoss: 0.6085256338119507\n",
      "cnt: 0 - valLoss: 0.6030552983283997 - trainLoss: 0.6085167527198792\n",
      "cnt: 0 - valLoss: 0.6030474901199341 - trainLoss: 0.6085078120231628\n",
      "cnt: 0 - valLoss: 0.6030388474464417 - trainLoss: 0.6084986925125122\n",
      "cnt: 0 - valLoss: 0.603030800819397 - trainLoss: 0.6084900498390198\n",
      "cnt: 0 - valLoss: 0.6030240654945374 - trainLoss: 0.6084808111190796\n",
      "cnt: 0 - valLoss: 0.6030159592628479 - trainLoss: 0.6084720492362976\n",
      "cnt: 0 - valLoss: 0.6030086278915405 - trainLoss: 0.6084628701210022\n",
      "cnt: 0 - valLoss: 0.6030012965202332 - trainLoss: 0.6084539890289307\n",
      "cnt: 0 - valLoss: 0.6029918789863586 - trainLoss: 0.6084449887275696\n",
      "cnt: 0 - valLoss: 0.6029846668243408 - trainLoss: 0.6084359288215637\n",
      "cnt: 0 - valLoss: 0.6029767394065857 - trainLoss: 0.6084270477294922\n",
      "cnt: 0 - valLoss: 0.6029698848724365 - trainLoss: 0.6084178686141968\n",
      "cnt: 0 - valLoss: 0.6029618978500366 - trainLoss: 0.6084089875221252\n",
      "cnt: 0 - valLoss: 0.6029528975486755 - trainLoss: 0.6083998680114746\n",
      "cnt: 0 - valLoss: 0.6029453277587891 - trainLoss: 0.6083910465240479\n",
      "cnt: 0 - valLoss: 0.6029378771781921 - trainLoss: 0.6083818674087524\n",
      "cnt: 0 - valLoss: 0.6029305458068848 - trainLoss: 0.6083729267120361\n",
      "cnt: 0 - valLoss: 0.6029229760169983 - trainLoss: 0.6083638668060303\n",
      "cnt: 0 - valLoss: 0.6029142141342163 - trainLoss: 0.6083548665046692\n",
      "cnt: 0 - valLoss: 0.6029062867164612 - trainLoss: 0.6083459854125977\n",
      "cnt: 0 - valLoss: 0.6028995513916016 - trainLoss: 0.6083367466926575\n",
      "cnt: 0 - valLoss: 0.6028915643692017 - trainLoss: 0.6083279252052307\n",
      "cnt: 0 - valLoss: 0.6028843522071838 - trainLoss: 0.6083186268806458\n",
      "cnt: 0 - valLoss: 0.6028751730918884 - trainLoss: 0.6083098649978638\n",
      "cnt: 0 - valLoss: 0.6028677225112915 - trainLoss: 0.6083007454872131\n",
      "cnt: 0 - valLoss: 0.6028605103492737 - trainLoss: 0.6082916855812073\n",
      "cnt: 0 - valLoss: 0.6028527021408081 - trainLoss: 0.6082826256752014\n",
      "cnt: 0 - valLoss: 0.6028459072113037 - trainLoss: 0.608273446559906\n",
      "cnt: 0 - valLoss: 0.6028361916542053 - trainLoss: 0.6082646250724792\n",
      "cnt: 0 - valLoss: 0.6028290390968323 - trainLoss: 0.6082554459571838\n",
      "cnt: 0 - valLoss: 0.6028215885162354 - trainLoss: 0.6082466244697571\n",
      "cnt: 0 - valLoss: 0.6028141379356384 - trainLoss: 0.6082373261451721\n",
      "cnt: 0 - valLoss: 0.6028069853782654 - trainLoss: 0.608228325843811\n",
      "cnt: 0 - valLoss: 0.6027976274490356 - trainLoss: 0.60821932554245\n",
      "cnt: 0 - valLoss: 0.6027905941009521 - trainLoss: 0.6082102656364441\n",
      "cnt: 0 - valLoss: 0.602782666683197 - trainLoss: 0.608201265335083\n",
      "cnt: 0 - valLoss: 0.6027759909629822 - trainLoss: 0.6081920266151428\n",
      "cnt: 0 - valLoss: 0.6027681231498718 - trainLoss: 0.6081831455230713\n",
      "cnt: 0 - valLoss: 0.6027590036392212 - trainLoss: 0.6081739664077759\n",
      "cnt: 0 - valLoss: 0.6027516722679138 - trainLoss: 0.6081650853157043\n",
      "cnt: 0 - valLoss: 0.6027441024780273 - trainLoss: 0.6081558465957642\n",
      "cnt: 0 - valLoss: 0.6027370691299438 - trainLoss: 0.6081467866897583\n",
      "cnt: 0 - valLoss: 0.6027275323867798 - trainLoss: 0.6081377863883972\n",
      "cnt: 0 - valLoss: 0.6027207374572754 - trainLoss: 0.6081286072731018\n",
      "cnt: 0 - valLoss: 0.6027127504348755 - trainLoss: 0.6081197261810303\n",
      "cnt: 0 - valLoss: 0.6027055382728577 - trainLoss: 0.6081104278564453\n",
      "cnt: 0 - valLoss: 0.6026981472969055 - trainLoss: 0.6081015467643738\n",
      "cnt: 0 - valLoss: 0.6026889085769653 - trainLoss: 0.6080923080444336\n",
      "cnt: 0 - valLoss: 0.6026816964149475 - trainLoss: 0.6080833077430725\n",
      "cnt: 0 - valLoss: 0.6026740074157715 - trainLoss: 0.6080741882324219\n",
      "cnt: 0 - valLoss: 0.6026670336723328 - trainLoss: 0.6080649495124817\n",
      "cnt: 0 - valLoss: 0.6026591658592224 - trainLoss: 0.6080559492111206\n",
      "cnt: 0 - valLoss: 0.6026502251625061 - trainLoss: 0.6080467104911804\n",
      "cnt: 0 - valLoss: 0.6026427149772644 - trainLoss: 0.6080378293991089\n",
      "cnt: 0 - valLoss: 0.6026352643966675 - trainLoss: 0.6080284714698792\n",
      "cnt: 0 - valLoss: 0.6026279330253601 - trainLoss: 0.6080194115638733\n",
      "cnt: 0 - valLoss: 0.6026204824447632 - trainLoss: 0.6080101728439331\n",
      "cnt: 0 - valLoss: 0.6026116013526917 - trainLoss: 0.6080010533332825\n",
      "cnt: 0 - valLoss: 0.6026036143302917 - trainLoss: 0.6079920530319214\n",
      "cnt: 0 - valLoss: 0.6025965213775635 - trainLoss: 0.6079825758934021\n",
      "cnt: 0 - valLoss: 0.602588951587677 - trainLoss: 0.6079736948013306\n",
      "cnt: 0 - valLoss: 0.6025797724723816 - trainLoss: 0.6079643368721008\n",
      "cnt: 0 - valLoss: 0.6025725603103638 - trainLoss: 0.6079553961753845\n",
      "cnt: 0 - valLoss: 0.602564811706543 - trainLoss: 0.6079462170600891\n",
      "cnt: 0 - valLoss: 0.6025578379631042 - trainLoss: 0.6079369187355042\n",
      "cnt: 0 - valLoss: 0.6025499701499939 - trainLoss: 0.6079277992248535\n",
      "cnt: 0 - valLoss: 0.6025410294532776 - trainLoss: 0.6079184412956238\n",
      "cnt: 0 - valLoss: 0.6025334596633911 - trainLoss: 0.6079097390174866\n",
      "cnt: 0 - valLoss: 0.6025260090827942 - trainLoss: 0.6079002022743225\n",
      "cnt: 0 - valLoss: 0.6025187373161316 - trainLoss: 0.6078911423683167\n",
      "cnt: 0 - valLoss: 0.6025111079216003 - trainLoss: 0.6078819036483765\n",
      "cnt: 0 - valLoss: 0.6025022864341736 - trainLoss: 0.607872724533081\n",
      "cnt: 0 - valLoss: 0.6024943590164185 - trainLoss: 0.60786372423172\n",
      "cnt: 0 - valLoss: 0.6024876236915588 - trainLoss: 0.6078543066978455\n",
      "cnt: 0 - valLoss: 0.6024796366691589 - trainLoss: 0.6078454256057739\n",
      "cnt: 0 - valLoss: 0.6024705767631531 - trainLoss: 0.6078359484672546\n",
      "cnt: 0 - valLoss: 0.6024633049964905 - trainLoss: 0.6078270077705383\n",
      "cnt: 0 - valLoss: 0.6024556756019592 - trainLoss: 0.6078177094459534\n",
      "cnt: 0 - valLoss: 0.6024486422538757 - trainLoss: 0.6078084707260132\n",
      "cnt: 0 - valLoss: 0.6024408340454102 - trainLoss: 0.6077992916107178\n",
      "cnt: 0 - valLoss: 0.6024320125579834 - trainLoss: 0.6077899932861328\n",
      "cnt: 0 - valLoss: 0.6024243235588074 - trainLoss: 0.607781171798706\n",
      "cnt: 0 - valLoss: 0.6024169325828552 - trainLoss: 0.607771635055542\n",
      "cnt: 0 - valLoss: 0.6024096012115479 - trainLoss: 0.6077625155448914\n",
      "cnt: 0 - valLoss: 0.6024003028869629 - trainLoss: 0.6077532172203064\n",
      "cnt: 0 - valLoss: 0.6023932695388794 - trainLoss: 0.6077439785003662\n",
      "cnt: 0 - valLoss: 0.6023852825164795 - trainLoss: 0.6077348589897156\n",
      "cnt: 0 - valLoss: 0.6023780703544617 - trainLoss: 0.6077253222465515\n",
      "cnt: 0 - valLoss: 0.6023706793785095 - trainLoss: 0.6077163219451904\n",
      "cnt: 0 - valLoss: 0.6023613810539246 - trainLoss: 0.6077069044113159\n",
      "cnt: 0 - valLoss: 0.602354109287262 - trainLoss: 0.6076978445053101\n",
      "cnt: 0 - valLoss: 0.6023463606834412 - trainLoss: 0.6076885461807251\n",
      "cnt: 0 - valLoss: 0.6023393869400024 - trainLoss: 0.6076791286468506\n",
      "cnt: 0 - valLoss: 0.6023297309875488 - trainLoss: 0.6076700091362\n",
      "cnt: 0 - valLoss: 0.6023225784301758 - trainLoss: 0.6076606512069702\n",
      "cnt: 0 - valLoss: 0.6023150682449341 - trainLoss: 0.6076515913009644\n",
      "cnt: 0 - valLoss: 0.6023074984550476 - trainLoss: 0.6076421141624451\n",
      "cnt: 0 - valLoss: 0.6023004055023193 - trainLoss: 0.6076328158378601\n",
      "cnt: 0 - valLoss: 0.6022908091545105 - trainLoss: 0.6076236963272095\n",
      "cnt: 0 - valLoss: 0.6022839546203613 - trainLoss: 0.6076143383979797\n",
      "cnt: 0 - valLoss: 0.6022759675979614 - trainLoss: 0.6076052188873291\n",
      "cnt: 0 - valLoss: 0.6022686958312988 - trainLoss: 0.6075957417488098\n",
      "cnt: 0 - valLoss: 0.6022595763206482 - trainLoss: 0.607586681842804\n",
      "cnt: 0 - valLoss: 0.6022520661354065 - trainLoss: 0.6075774431228638\n",
      "cnt: 0 - valLoss: 0.6022449731826782 - trainLoss: 0.6075681447982788\n",
      "cnt: 0 - valLoss: 0.6022370457649231 - trainLoss: 0.6075589060783386\n",
      "cnt: 0 - valLoss: 0.602230429649353 - trainLoss: 0.6075494885444641\n",
      "cnt: 0 - valLoss: 0.6022200584411621 - trainLoss: 0.6075406074523926\n",
      "cnt: 0 - valLoss: 0.6022127866744995 - trainLoss: 0.6075311899185181\n",
      "cnt: 0 - valLoss: 0.6022054553031921 - trainLoss: 0.607522189617157\n",
      "cnt: 0 - valLoss: 0.6021974682807922 - trainLoss: 0.607512891292572\n",
      "cnt: 0 - valLoss: 0.602189838886261 - trainLoss: 0.6075037121772766\n",
      "cnt: 0 - valLoss: 0.602181613445282 - trainLoss: 0.6074945330619812\n",
      "cnt: 0 - valLoss: 0.6021742820739746 - trainLoss: 0.6074852347373962\n",
      "cnt: 0 - valLoss: 0.6021658182144165 - trainLoss: 0.6074762344360352\n",
      "cnt: 0 - valLoss: 0.6021581888198853 - trainLoss: 0.6074668169021606\n",
      "cnt: 0 - valLoss: 0.6021502614021301 - trainLoss: 0.6074578762054443\n",
      "cnt: 0 - valLoss: 0.6021423935890198 - trainLoss: 0.6074485182762146\n",
      "cnt: 0 - valLoss: 0.602134644985199 - trainLoss: 0.607439398765564\n",
      "cnt: 0 - valLoss: 0.6021264791488647 - trainLoss: 0.6074301600456238\n",
      "cnt: 0 - valLoss: 0.6021190881729126 - trainLoss: 0.6074208617210388\n",
      "cnt: 0 - valLoss: 0.6021106839179993 - trainLoss: 0.6074118614196777\n",
      "cnt: 0 - valLoss: 0.6021031737327576 - trainLoss: 0.6074024438858032\n",
      "cnt: 0 - valLoss: 0.6020951271057129 - trainLoss: 0.6073935627937317\n",
      "cnt: 0 - valLoss: 0.6020873188972473 - trainLoss: 0.6073840856552124\n",
      "cnt: 0 - valLoss: 0.6020795702934265 - trainLoss: 0.6073749661445618\n",
      "cnt: 0 - valLoss: 0.6020714640617371 - trainLoss: 0.6073657274246216\n",
      "cnt: 0 - valLoss: 0.6020638942718506 - trainLoss: 0.6073564887046814\n",
      "cnt: 0 - valLoss: 0.6020556092262268 - trainLoss: 0.607347309589386\n",
      "cnt: 0 - valLoss: 0.6020483374595642 - trainLoss: 0.6073378324508667\n",
      "cnt: 0 - valLoss: 0.6020398736000061 - trainLoss: 0.6073288917541504\n",
      "cnt: 0 - valLoss: 0.6020320653915405 - trainLoss: 0.6073193550109863\n",
      "cnt: 0 - valLoss: 0.6020243167877197 - trainLoss: 0.6073102355003357\n",
      "cnt: 0 - valLoss: 0.6020162105560303 - trainLoss: 0.6073009371757507\n",
      "cnt: 0 - valLoss: 0.6020087003707886 - trainLoss: 0.607291579246521\n",
      "cnt: 0 - valLoss: 0.6020002961158752 - trainLoss: 0.6072824001312256\n",
      "cnt: 0 - valLoss: 0.6019930839538574 - trainLoss: 0.6072729229927063\n",
      "cnt: 0 - valLoss: 0.6019846200942993 - trainLoss: 0.6072639226913452\n",
      "cnt: 0 - valLoss: 0.6019770503044128 - trainLoss: 0.6072545051574707\n",
      "cnt: 0 - valLoss: 0.6019694805145264 - trainLoss: 0.6072458028793335\n",
      "cnt: 0 - valLoss: 0.6019615530967712 - trainLoss: 0.6072367429733276\n",
      "cnt: 0 - valLoss: 0.6019542813301086 - trainLoss: 0.6072277426719666\n",
      "cnt: 0 - valLoss: 0.6019460558891296 - trainLoss: 0.607218861579895\n",
      "cnt: 0 - valLoss: 0.6019390225410461 - trainLoss: 0.6072097420692444\n",
      "cnt: 0 - valLoss: 0.6019307971000671 - trainLoss: 0.6072010397911072\n",
      "cnt: 0 - valLoss: 0.6019232273101807 - trainLoss: 0.6071918606758118\n",
      "cnt: 0 - valLoss: 0.6019155979156494 - trainLoss: 0.6071830987930298\n",
      "cnt: 0 - valLoss: 0.6019077301025391 - trainLoss: 0.6071740388870239\n",
      "cnt: 0 - valLoss: 0.6019003987312317 - trainLoss: 0.6071650981903076\n",
      "cnt: 0 - valLoss: 0.6018921732902527 - trainLoss: 0.6071562170982361\n",
      "cnt: 0 - valLoss: 0.6018851399421692 - trainLoss: 0.6071470975875854\n",
      "cnt: 0 - valLoss: 0.6018770337104797 - trainLoss: 0.6071383357048035\n",
      "cnt: 0 - valLoss: 0.6018694043159485 - trainLoss: 0.6071292757987976\n",
      "cnt: 0 - valLoss: 0.601861834526062 - trainLoss: 0.6071204543113708\n",
      "cnt: 0 - valLoss: 0.6018539667129517 - trainLoss: 0.6071114540100098\n",
      "cnt: 0 - valLoss: 0.6018466949462891 - trainLoss: 0.6071025133132935\n",
      "cnt: 0 - valLoss: 0.6018385291099548 - trainLoss: 0.6070936918258667\n",
      "cnt: 0 - valLoss: 0.6018315553665161 - trainLoss: 0.6070845723152161\n",
      "cnt: 0 - valLoss: 0.6018233299255371 - trainLoss: 0.6070759296417236\n",
      "cnt: 0 - valLoss: 0.6018157005310059 - trainLoss: 0.607066810131073\n",
      "cnt: 0 - valLoss: 0.6018081903457642 - trainLoss: 0.6070579886436462\n",
      "cnt: 0 - valLoss: 0.6018003225326538 - trainLoss: 0.6070489883422852\n",
      "cnt: 0 - valLoss: 0.6017930507659912 - trainLoss: 0.6070399880409241\n",
      "cnt: 0 - valLoss: 0.601784884929657 - trainLoss: 0.6070311665534973\n",
      "cnt: 0 - valLoss: 0.6017774939537048 - trainLoss: 0.6070220470428467\n",
      "cnt: 0 - valLoss: 0.601769745349884 - trainLoss: 0.6070134043693542\n",
      "cnt: 0 - valLoss: 0.601762056350708 - trainLoss: 0.6070042848587036\n",
      "cnt: 0 - valLoss: 0.6017545461654663 - trainLoss: 0.6069954037666321\n",
      "cnt: 0 - valLoss: 0.6017464399337769 - trainLoss: 0.6069864630699158\n",
      "cnt: 0 - valLoss: 0.6017393469810486 - trainLoss: 0.6069774031639099\n",
      "cnt: 0 - valLoss: 0.6017311215400696 - trainLoss: 0.6069686412811279\n",
      "cnt: 0 - valLoss: 0.6017236709594727 - trainLoss: 0.6069594621658325\n",
      "cnt: 0 - valLoss: 0.6017159223556519 - trainLoss: 0.6069508194923401\n",
      "cnt: 0 - valLoss: 0.601708173751831 - trainLoss: 0.6069416999816895\n",
      "cnt: 0 - valLoss: 0.6017007827758789 - trainLoss: 0.6069328188896179\n",
      "cnt: 0 - valLoss: 0.6016926169395447 - trainLoss: 0.6069239377975464\n",
      "cnt: 0 - valLoss: 0.6016855835914612 - trainLoss: 0.6069148182868958\n",
      "cnt: 0 - valLoss: 0.6016772985458374 - trainLoss: 0.6069060564041138\n",
      "cnt: 0 - valLoss: 0.6016697287559509 - trainLoss: 0.6068968772888184\n",
      "cnt: 0 - valLoss: 0.6016621589660645 - trainLoss: 0.6068881154060364\n",
      "cnt: 0 - valLoss: 0.6016542911529541 - trainLoss: 0.6068790555000305\n",
      "cnt: 0 - valLoss: 0.6016469597816467 - trainLoss: 0.6068701148033142\n",
      "cnt: 0 - valLoss: 0.6016387343406677 - trainLoss: 0.6068612337112427\n",
      "cnt: 0 - valLoss: 0.601631760597229 - trainLoss: 0.606852114200592\n",
      "cnt: 0 - valLoss: 0.6016235947608948 - trainLoss: 0.6068434119224548\n",
      "cnt: 0 - valLoss: 0.601615846157074 - trainLoss: 0.6068342328071594\n",
      "cnt: 0 - valLoss: 0.6016083359718323 - trainLoss: 0.6068254113197327\n",
      "cnt: 0 - valLoss: 0.6016003489494324 - trainLoss: 0.6068164110183716\n",
      "cnt: 0 - valLoss: 0.6015931963920593 - trainLoss: 0.6068073511123657\n",
      "cnt: 0 - valLoss: 0.6015849113464355 - trainLoss: 0.606798529624939\n",
      "cnt: 0 - valLoss: 0.6015775203704834 - trainLoss: 0.6067893505096436\n",
      "cnt: 0 - valLoss: 0.6015697717666626 - trainLoss: 0.6067805886268616\n",
      "cnt: 0 - valLoss: 0.601561963558197 - trainLoss: 0.6067715287208557\n",
      "cnt: 0 - valLoss: 0.6015546321868896 - trainLoss: 0.6067625880241394\n",
      "cnt: 0 - valLoss: 0.6015464067459106 - trainLoss: 0.6067536473274231\n",
      "cnt: 0 - valLoss: 0.6015393733978271 - trainLoss: 0.6067444682121277\n",
      "cnt: 0 - valLoss: 0.6015311479568481 - trainLoss: 0.6067357659339905\n",
      "cnt: 0 - valLoss: 0.6015235185623169 - trainLoss: 0.6067265868186951\n",
      "cnt: 0 - valLoss: 0.6015159487724304 - trainLoss: 0.6067177057266235\n",
      "cnt: 0 - valLoss: 0.6015080213546753 - trainLoss: 0.6067087054252625\n",
      "cnt: 0 - valLoss: 0.6015007495880127 - trainLoss: 0.6066996455192566\n",
      "cnt: 0 - valLoss: 0.6014925241470337 - trainLoss: 0.6066907644271851\n",
      "cnt: 0 - valLoss: 0.6014851331710815 - trainLoss: 0.6066815853118896\n",
      "cnt: 0 - valLoss: 0.6014773845672607 - trainLoss: 0.6066728234291077\n",
      "cnt: 0 - valLoss: 0.6014695763587952 - trainLoss: 0.606663703918457\n",
      "cnt: 0 - valLoss: 0.601462185382843 - trainLoss: 0.6066547632217407\n",
      "cnt: 0 - valLoss: 0.6014540791511536 - trainLoss: 0.6066458225250244\n",
      "cnt: 0 - valLoss: 0.6014469265937805 - trainLoss: 0.606636643409729\n",
      "cnt: 0 - valLoss: 0.6014387011528015 - trainLoss: 0.606627881526947\n",
      "cnt: 0 - valLoss: 0.6014310717582703 - trainLoss: 0.6066187024116516\n",
      "cnt: 0 - valLoss: 0.6014220714569092 - trainLoss: 0.6066098213195801\n",
      "cnt: 0 - valLoss: 0.6014140248298645 - trainLoss: 0.6066009402275085\n",
      "cnt: 0 - valLoss: 0.6014068126678467 - trainLoss: 0.6065918803215027\n",
      "cnt: 0 - valLoss: 0.6013985276222229 - trainLoss: 0.6065829992294312\n",
      "cnt: 0 - valLoss: 0.6013911962509155 - trainLoss: 0.606573760509491\n",
      "cnt: 0 - valLoss: 0.6013818979263306 - trainLoss: 0.6065650582313538\n",
      "cnt: 0 - valLoss: 0.6013740301132202 - trainLoss: 0.6065559387207031\n",
      "cnt: 0 - valLoss: 0.6013666987419128 - trainLoss: 0.606546938419342\n",
      "cnt: 0 - valLoss: 0.6013585329055786 - trainLoss: 0.6065379977226257\n",
      "cnt: 0 - valLoss: 0.6013500094413757 - trainLoss: 0.6065287590026855\n",
      "cnt: 0 - valLoss: 0.6013418436050415 - trainLoss: 0.6065201759338379\n",
      "cnt: 0 - valLoss: 0.6013341546058655 - trainLoss: 0.6065109372138977\n",
      "cnt: 0 - valLoss: 0.6013266444206238 - trainLoss: 0.6065019965171814\n",
      "cnt: 0 - valLoss: 0.6013187170028687 - trainLoss: 0.6064929366111755\n",
      "cnt: 0 - valLoss: 0.6013100147247314 - trainLoss: 0.6064839363098145\n",
      "cnt: 0 - valLoss: 0.6013017892837524 - trainLoss: 0.6064751148223877\n",
      "cnt: 0 - valLoss: 0.601294219493866 - trainLoss: 0.6064658761024475\n",
      "cnt: 0 - valLoss: 0.6012868285179138 - trainLoss: 0.606456995010376\n",
      "cnt: 0 - valLoss: 0.6012772917747498 - trainLoss: 0.6064480543136597\n",
      "cnt: 0 - valLoss: 0.6012700200080872 - trainLoss: 0.6064389944076538\n",
      "cnt: 0 - valLoss: 0.6012617945671082 - trainLoss: 0.6064299941062927\n",
      "cnt: 0 - valLoss: 0.6012550592422485 - trainLoss: 0.6064208149909973\n",
      "cnt: 0 - valLoss: 0.6012451648712158 - trainLoss: 0.6064121127128601\n",
      "cnt: 0 - valLoss: 0.6012375950813293 - trainLoss: 0.6064028739929199\n",
      "cnt: 0 - valLoss: 0.6012299656867981 - trainLoss: 0.6063939929008484\n",
      "cnt: 0 - valLoss: 0.6012222766876221 - trainLoss: 0.6063848733901978\n",
      "cnt: 0 - valLoss: 0.6012136340141296 - trainLoss: 0.6063758134841919\n",
      "cnt: 0 - valLoss: 0.6012055277824402 - trainLoss: 0.6063669919967651\n",
      "cnt: 0 - valLoss: 0.601198136806488 - trainLoss: 0.606357753276825\n",
      "cnt: 0 - valLoss: 0.6011907458305359 - trainLoss: 0.6063488125801086\n",
      "cnt: 0 - valLoss: 0.6011813879013062 - trainLoss: 0.606339693069458\n",
      "cnt: 0 - valLoss: 0.6011741161346436 - trainLoss: 0.6063306331634521\n",
      "cnt: 0 - valLoss: 0.6011660099029541 - trainLoss: 0.6063215136528015\n",
      "cnt: 0 - valLoss: 0.601159393787384 - trainLoss: 0.6063122153282166\n",
      "cnt: 0 - valLoss: 0.6011494994163513 - trainLoss: 0.6063035726547241\n",
      "cnt: 0 - valLoss: 0.6011420488357544 - trainLoss: 0.6062941551208496\n",
      "cnt: 0 - valLoss: 0.6011345386505127 - trainLoss: 0.6062852144241333\n",
      "cnt: 0 - valLoss: 0.6011252999305725 - trainLoss: 0.6062760353088379\n",
      "cnt: 0 - valLoss: 0.6011180877685547 - trainLoss: 0.606266975402832\n",
      "cnt: 0 - valLoss: 0.60111004114151 - trainLoss: 0.6062579154968262\n",
      "cnt: 0 - valLoss: 0.6011028289794922 - trainLoss: 0.6062486171722412\n",
      "cnt: 0 - valLoss: 0.6010937094688416 - trainLoss: 0.6062397956848145\n",
      "cnt: 0 - valLoss: 0.6010860204696655 - trainLoss: 0.6062305569648743\n",
      "cnt: 0 - valLoss: 0.6010788083076477 - trainLoss: 0.6062214374542236\n",
      "cnt: 0 - valLoss: 0.6010693907737732 - trainLoss: 0.6062123775482178\n",
      "cnt: 0 - valLoss: 0.6010624766349792 - trainLoss: 0.6062032580375671\n",
      "cnt: 0 - valLoss: 0.601054310798645 - trainLoss: 0.6061943173408508\n",
      "cnt: 0 - valLoss: 0.6010469794273376 - trainLoss: 0.6061850190162659\n",
      "cnt: 0 - valLoss: 0.6010381579399109 - trainLoss: 0.60617595911026\n",
      "cnt: 0 - valLoss: 0.601030170917511 - trainLoss: 0.6061669588088989\n",
      "cnt: 0 - valLoss: 0.6010233163833618 - trainLoss: 0.606157660484314\n",
      "cnt: 0 - valLoss: 0.6010136008262634 - trainLoss: 0.6061487197875977\n",
      "cnt: 0 - valLoss: 0.6010063290596008 - trainLoss: 0.6061395406723022\n",
      "cnt: 0 - valLoss: 0.6009988784790039 - trainLoss: 0.6061305403709412\n",
      "cnt: 0 - valLoss: 0.6009912490844727 - trainLoss: 0.606121301651001\n",
      "cnt: 0 - valLoss: 0.600982666015625 - trainLoss: 0.6061121225357056\n",
      "cnt: 0 - valLoss: 0.6009746193885803 - trainLoss: 0.606103241443634\n",
      "cnt: 0 - valLoss: 0.600967288017273 - trainLoss: 0.6060938835144043\n",
      "cnt: 0 - valLoss: 0.6009583473205566 - trainLoss: 0.606084942817688\n",
      "cnt: 0 - valLoss: 0.6009505391120911 - trainLoss: 0.6060758233070374\n",
      "cnt: 0 - valLoss: 0.6009436845779419 - trainLoss: 0.6060665249824524\n",
      "cnt: 0 - valLoss: 0.6009340882301331 - trainLoss: 0.6060576438903809\n",
      "cnt: 0 - valLoss: 0.6009268164634705 - trainLoss: 0.6060483455657959\n",
      "cnt: 0 - valLoss: 0.6009193062782288 - trainLoss: 0.6060394644737244\n",
      "cnt: 0 - valLoss: 0.6009113192558289 - trainLoss: 0.6060301661491394\n",
      "cnt: 0 - valLoss: 0.6009042263031006 - trainLoss: 0.6060211062431335\n",
      "cnt: 0 - valLoss: 0.6008962392807007 - trainLoss: 0.6060119867324829\n",
      "cnt: 0 - valLoss: 0.6008893847465515 - trainLoss: 0.6060026288032532\n",
      "cnt: 0 - valLoss: 0.6008812785148621 - trainLoss: 0.6059938669204712\n",
      "cnt: 0 - valLoss: 0.6008736491203308 - trainLoss: 0.6059845685958862\n",
      "cnt: 0 - valLoss: 0.6008668541908264 - trainLoss: 0.605975329875946\n",
      "cnt: 0 - valLoss: 0.600858211517334 - trainLoss: 0.6059664487838745\n",
      "cnt: 0 - valLoss: 0.6008515357971191 - trainLoss: 0.6059570908546448\n",
      "cnt: 0 - valLoss: 0.6008431315422058 - trainLoss: 0.6059481501579285\n",
      "cnt: 0 - valLoss: 0.6008357405662537 - trainLoss: 0.6059389114379883\n",
      "cnt: 0 - valLoss: 0.6008284687995911 - trainLoss: 0.6059298515319824\n",
      "cnt: 0 - valLoss: 0.600820779800415 - trainLoss: 0.605920672416687\n",
      "cnt: 0 - valLoss: 0.6008134484291077 - trainLoss: 0.6059114336967468\n",
      "cnt: 0 - valLoss: 0.600805401802063 - trainLoss: 0.6059024930000305\n",
      "cnt: 0 - valLoss: 0.6007977724075317 - trainLoss: 0.6058931946754456\n",
      "cnt: 0 - valLoss: 0.6007906198501587 - trainLoss: 0.6058840155601501\n",
      "cnt: 0 - valLoss: 0.6007817983627319 - trainLoss: 0.6058749556541443\n",
      "cnt: 0 - valLoss: 0.6007751822471619 - trainLoss: 0.6058655977249146\n",
      "cnt: 0 - valLoss: 0.6007671356201172 - trainLoss: 0.6058566570281982\n",
      "cnt: 0 - valLoss: 0.6007589101791382 - trainLoss: 0.605847179889679\n",
      "cnt: 0 - valLoss: 0.6007517576217651 - trainLoss: 0.6058383584022522\n",
      "cnt: 0 - valLoss: 0.6007442474365234 - trainLoss: 0.6058289408683777\n",
      "cnt: 0 - valLoss: 0.6007366180419922 - trainLoss: 0.605819821357727\n",
      "cnt: 0 - valLoss: 0.6007289290428162 - trainLoss: 0.6058108806610107\n",
      "cnt: 0 - valLoss: 0.6007207036018372 - trainLoss: 0.6058014631271362\n",
      "cnt: 0 - valLoss: 0.600713849067688 - trainLoss: 0.6057924032211304\n",
      "cnt: 0 - valLoss: 0.6007052063941956 - trainLoss: 0.6057832837104797\n",
      "cnt: 0 - valLoss: 0.6006989479064941 - trainLoss: 0.6057739853858948\n",
      "cnt: 0 - valLoss: 0.6006901264190674 - trainLoss: 0.6057652235031128\n",
      "cnt: 0 - valLoss: 0.6006830334663391 - trainLoss: 0.6057558059692383\n",
      "cnt: 0 - valLoss: 0.6006752252578735 - trainLoss: 0.6057468056678772\n",
      "cnt: 0 - valLoss: 0.6006665825843811 - trainLoss: 0.6057376861572266\n",
      "cnt: 0 - valLoss: 0.6006597280502319 - trainLoss: 0.6057284474372864\n",
      "cnt: 0 - valLoss: 0.6006516218185425 - trainLoss: 0.6057195663452148\n",
      "cnt: 0 - valLoss: 0.6006442904472351 - trainLoss: 0.6057102680206299\n",
      "cnt: 0 - valLoss: 0.6006367206573486 - trainLoss: 0.6057011485099792\n",
      "cnt: 0 - valLoss: 0.600628137588501 - trainLoss: 0.6056921482086182\n",
      "cnt: 0 - valLoss: 0.6006210446357727 - trainLoss: 0.605682909488678\n",
      "cnt: 0 - valLoss: 0.6006131768226624 - trainLoss: 0.6056739091873169\n",
      "cnt: 0 - valLoss: 0.6006057262420654 - trainLoss: 0.6056647300720215\n",
      "cnt: 0 - valLoss: 0.6005977392196655 - trainLoss: 0.6056555509567261\n",
      "cnt: 0 - valLoss: 0.6005898118019104 - trainLoss: 0.605646550655365\n",
      "cnt: 0 - valLoss: 0.6005825400352478 - trainLoss: 0.6056373715400696\n",
      "cnt: 0 - valLoss: 0.6005749702453613 - trainLoss: 0.6056281328201294\n",
      "cnt: 0 - valLoss: 0.6005676984786987 - trainLoss: 0.6056191325187683\n",
      "cnt: 0 - valLoss: 0.6005595326423645 - trainLoss: 0.6056099534034729\n",
      "cnt: 0 - valLoss: 0.6005517840385437 - trainLoss: 0.605600893497467\n",
      "cnt: 0 - valLoss: 0.6005443334579468 - trainLoss: 0.6055917739868164\n",
      "cnt: 0 - valLoss: 0.6005370020866394 - trainLoss: 0.6055824160575867\n",
      "cnt: 0 - valLoss: 0.6005297303199768 - trainLoss: 0.6055735349655151\n",
      "cnt: 0 - valLoss: 0.6005213856697083 - trainLoss: 0.6055643558502197\n",
      "cnt: 0 - valLoss: 0.6005138158798218 - trainLoss: 0.6055551171302795\n",
      "cnt: 0 - valLoss: 0.6005064845085144 - trainLoss: 0.6055461764335632\n",
      "cnt: 0 - valLoss: 0.6004984378814697 - trainLoss: 0.6055368781089783\n",
      "cnt: 0 - valLoss: 0.6004905104637146 - trainLoss: 0.6055277585983276\n",
      "cnt: 0 - valLoss: 0.6004828810691833 - trainLoss: 0.6055184602737427\n",
      "cnt: 0 - valLoss: 0.6004753708839417 - trainLoss: 0.6055089831352234\n",
      "cnt: 0 - valLoss: 0.6004678010940552 - trainLoss: 0.6054999232292175\n",
      "cnt: 0 - valLoss: 0.6004592776298523 - trainLoss: 0.6054905652999878\n",
      "cnt: 0 - valLoss: 0.6004515886306763 - trainLoss: 0.6054810881614685\n",
      "cnt: 0 - valLoss: 0.600443959236145 - trainLoss: 0.6054720282554626\n",
      "cnt: 0 - valLoss: 0.6004357933998108 - trainLoss: 0.6054624915122986\n",
      "cnt: 0 - valLoss: 0.6004292368888855 - trainLoss: 0.6054533123970032\n",
      "cnt: 0 - valLoss: 0.6004205346107483 - trainLoss: 0.605444073677063\n",
      "cnt: 0 - valLoss: 0.6004130840301514 - trainLoss: 0.6054346561431885\n",
      "cnt: 0 - valLoss: 0.6004045605659485 - trainLoss: 0.6054255962371826\n",
      "cnt: 0 - valLoss: 0.6003967523574829 - trainLoss: 0.6054163575172424\n",
      "cnt: 0 - valLoss: 0.6003903746604919 - trainLoss: 0.6054074764251709\n",
      "cnt: 0 - valLoss: 0.6003820300102234 - trainLoss: 0.6053983569145203\n",
      "cnt: 0 - valLoss: 0.6003747582435608 - trainLoss: 0.6053891777992249\n",
      "cnt: 0 - valLoss: 0.6003662347793579 - trainLoss: 0.6053802967071533\n",
      "cnt: 0 - valLoss: 0.6003583669662476 - trainLoss: 0.6053710579872131\n",
      "cnt: 0 - valLoss: 0.6003506779670715 - trainLoss: 0.6053621172904968\n",
      "cnt: 0 - valLoss: 0.6003423929214478 - trainLoss: 0.605353057384491\n",
      "cnt: 0 - valLoss: 0.6003352999687195 - trainLoss: 0.6053439974784851\n",
      "cnt: 0 - valLoss: 0.6003268361091614 - trainLoss: 0.6053353548049927\n",
      "cnt: 0 - valLoss: 0.600318968296051 - trainLoss: 0.6053261160850525\n",
      "cnt: 0 - valLoss: 0.6003127694129944 - trainLoss: 0.6053172945976257\n",
      "cnt: 0 - valLoss: 0.6003044843673706 - trainLoss: 0.6053083539009094\n",
      "cnt: 0 - valLoss: 0.6002974510192871 - trainLoss: 0.605299174785614\n",
      "cnt: 0 - valLoss: 0.6002891063690186 - trainLoss: 0.605290412902832\n",
      "cnt: 0 - valLoss: 0.600281298160553 - trainLoss: 0.6052812933921814\n",
      "cnt: 0 - valLoss: 0.6002739071846008 - trainLoss: 0.6052723526954651\n",
      "cnt: 0 - valLoss: 0.6002654433250427 - trainLoss: 0.6052634716033936\n",
      "cnt: 0 - valLoss: 0.6002582907676697 - trainLoss: 0.6052544116973877\n",
      "cnt: 0 - valLoss: 0.6002498269081116 - trainLoss: 0.6052459478378296\n",
      "cnt: 0 - valLoss: 0.6002419590950012 - trainLoss: 0.6052368879318237\n",
      "cnt: 0 - valLoss: 0.6002343893051147 - trainLoss: 0.6052281856536865\n",
      "cnt: 0 - valLoss: 0.6002262234687805 - trainLoss: 0.6052194833755493\n",
      "cnt: 0 - valLoss: 0.6002192497253418 - trainLoss: 0.6052107214927673\n",
      "cnt: 0 - valLoss: 0.6002110242843628 - trainLoss: 0.6052024364471436\n",
      "cnt: 0 - valLoss: 0.6002034544944763 - trainLoss: 0.6051936149597168\n",
      "cnt: 0 - valLoss: 0.6001960039138794 - trainLoss: 0.6051852107048035\n",
      "cnt: 0 - valLoss: 0.6001880168914795 - trainLoss: 0.6051766872406006\n",
      "cnt: 0 - valLoss: 0.6001810431480408 - trainLoss: 0.6051680445671082\n",
      "cnt: 0 - valLoss: 0.6001728773117065 - trainLoss: 0.6051597595214844\n",
      "cnt: 0 - valLoss: 0.6001653671264648 - trainLoss: 0.6051510572433472\n",
      "cnt: 0 - valLoss: 0.6001579165458679 - trainLoss: 0.6051427125930786\n",
      "cnt: 0 - valLoss: 0.600149929523468 - trainLoss: 0.605134129524231\n",
      "cnt: 0 - valLoss: 0.6001430153846741 - trainLoss: 0.6051254868507385\n",
      "cnt: 0 - valLoss: 0.6001348495483398 - trainLoss: 0.6051172018051147\n",
      "cnt: 0 - valLoss: 0.6001273989677429 - trainLoss: 0.6051084399223328\n",
      "cnt: 0 - valLoss: 0.600119948387146 - trainLoss: 0.605100154876709\n",
      "cnt: 0 - valLoss: 0.6001120209693909 - trainLoss: 0.6050915122032166\n",
      "cnt: 0 - valLoss: 0.6001050472259521 - trainLoss: 0.6050829291343689\n",
      "cnt: 0 - valLoss: 0.6000968813896179 - trainLoss: 0.6050746440887451\n",
      "cnt: 0 - valLoss: 0.600089430809021 - trainLoss: 0.6050658822059631\n",
      "cnt: 0 - valLoss: 0.6000819802284241 - trainLoss: 0.6050575971603394\n",
      "cnt: 0 - valLoss: 0.600074052810669 - trainLoss: 0.6050489544868469\n",
      "cnt: 0 - valLoss: 0.6000670790672302 - trainLoss: 0.6050403714179993\n",
      "cnt: 0 - valLoss: 0.6000589728355408 - trainLoss: 0.6050320863723755\n",
      "cnt: 0 - valLoss: 0.6000514626502991 - trainLoss: 0.6050233840942383\n",
      "cnt: 0 - valLoss: 0.6000440716743469 - trainLoss: 0.6050150394439697\n",
      "cnt: 0 - valLoss: 0.600036084651947 - trainLoss: 0.6050065755844116\n",
      "cnt: 0 - valLoss: 0.6000291109085083 - trainLoss: 0.6049979329109192\n",
      "cnt: 0 - valLoss: 0.6000209450721741 - trainLoss: 0.6049897074699402\n",
      "cnt: 0 - valLoss: 0.6000134944915771 - trainLoss: 0.604981005191803\n",
      "cnt: 0 - valLoss: 0.6000060439109802 - trainLoss: 0.6049726009368896\n",
      "cnt: 0 - valLoss: 0.5999980568885803 - trainLoss: 0.604964017868042\n",
      "cnt: 0 - valLoss: 0.5999910831451416 - trainLoss: 0.6049554944038391\n",
      "cnt: 0 - valLoss: 0.5999829173088074 - trainLoss: 0.6049471497535706\n",
      "cnt: 0 - valLoss: 0.5999754071235657 - trainLoss: 0.6049384474754333\n",
      "cnt: 0 - valLoss: 0.5999680161476135 - trainLoss: 0.60493004322052\n",
      "cnt: 0 - valLoss: 0.5999599695205688 - trainLoss: 0.6049215793609619\n",
      "cnt: 0 - valLoss: 0.5999531149864197 - trainLoss: 0.6049128770828247\n",
      "cnt: 0 - valLoss: 0.5999450087547302 - trainLoss: 0.6049046516418457\n",
      "cnt: 0 - valLoss: 0.599937379360199 - trainLoss: 0.6048958897590637\n",
      "cnt: 0 - valLoss: 0.5999300479888916 - trainLoss: 0.6048874855041504\n",
      "cnt: 0 - valLoss: 0.5999220013618469 - trainLoss: 0.6048789620399475\n",
      "cnt: 0 - valLoss: 0.5999152064323425 - trainLoss: 0.6048703193664551\n",
      "cnt: 0 - valLoss: 0.5999070405960083 - trainLoss: 0.6048620939254761\n",
      "cnt: 0 - valLoss: 0.5998993515968323 - trainLoss: 0.6048533320426941\n",
      "cnt: 0 - valLoss: 0.599892258644104 - trainLoss: 0.6048448085784912\n",
      "cnt: 0 - valLoss: 0.5998840928077698 - trainLoss: 0.6048364639282227\n",
      "cnt: 0 - valLoss: 0.599876880645752 - trainLoss: 0.6048276424407959\n",
      "cnt: 0 - valLoss: 0.5998693108558655 - trainLoss: 0.6048194766044617\n",
      "cnt: 0 - valLoss: 0.5998615622520447 - trainLoss: 0.6048107743263245\n",
      "cnt: 0 - valLoss: 0.5998545289039612 - trainLoss: 0.6048022508621216\n",
      "cnt: 0 - valLoss: 0.5998464226722717 - trainLoss: 0.6047939658164978\n",
      "cnt: 0 - valLoss: 0.5998390913009644 - trainLoss: 0.6047852039337158\n",
      "cnt: 0 - valLoss: 0.5998316407203674 - trainLoss: 0.604776918888092\n",
      "cnt: 0 - valLoss: 0.5998237729072571 - trainLoss: 0.6047682762145996\n",
      "cnt: 0 - valLoss: 0.5998168587684631 - trainLoss: 0.604759693145752\n",
      "cnt: 0 - valLoss: 0.5998087525367737 - trainLoss: 0.6047514081001282\n",
      "cnt: 0 - valLoss: 0.5998013615608215 - trainLoss: 0.604742705821991\n",
      "cnt: 0 - valLoss: 0.5997940897941589 - trainLoss: 0.6047343611717224\n",
      "cnt: 0 - valLoss: 0.5997860431671143 - trainLoss: 0.6047257781028748\n",
      "cnt: 0 - valLoss: 0.5997793078422546 - trainLoss: 0.6047171354293823\n",
      "cnt: 0 - valLoss: 0.59977126121521 - trainLoss: 0.6047089099884033\n",
      "cnt: 0 - valLoss: 0.5997636318206787 - trainLoss: 0.6047002077102661\n",
      "cnt: 0 - valLoss: 0.5997564792633057 - trainLoss: 0.604691743850708\n",
      "cnt: 0 - valLoss: 0.599748432636261 - trainLoss: 0.6046832799911499\n",
      "cnt: 0 - valLoss: 0.5997412204742432 - trainLoss: 0.6046745181083679\n",
      "cnt: 0 - valLoss: 0.5997337102890015 - trainLoss: 0.6046662926673889\n",
      "cnt: 0 - valLoss: 0.5997259616851807 - trainLoss: 0.6046575903892517\n",
      "cnt: 0 - valLoss: 0.5997189879417419 - trainLoss: 0.6046491265296936\n",
      "cnt: 0 - valLoss: 0.5997109413146973 - trainLoss: 0.6046406626701355\n",
      "cnt: 0 - valLoss: 0.5997036099433899 - trainLoss: 0.6046319603919983\n",
      "cnt: 0 - valLoss: 0.5996962785720825 - trainLoss: 0.6046236157417297\n",
      "cnt: 0 - valLoss: 0.5996883511543274 - trainLoss: 0.6046150326728821\n",
      "cnt: 0 - valLoss: 0.599681556224823 - trainLoss: 0.6046063899993896\n",
      "cnt: 0 - valLoss: 0.5996735095977783 - trainLoss: 0.6045981049537659\n",
      "cnt: 0 - valLoss: 0.5996659994125366 - trainLoss: 0.6045894026756287\n",
      "cnt: 0 - valLoss: 0.5996589064598083 - trainLoss: 0.6045809388160706\n",
      "cnt: 0 - valLoss: 0.5996508598327637 - trainLoss: 0.6045724749565125\n",
      "cnt: 0 - valLoss: 0.5996437072753906 - trainLoss: 0.6045637726783752\n",
      "cnt: 0 - valLoss: 0.5996361970901489 - trainLoss: 0.6045555472373962\n",
      "cnt: 0 - valLoss: 0.5996283888816833 - trainLoss: 0.6045469045639038\n",
      "cnt: 0 - valLoss: 0.5996215343475342 - trainLoss: 0.6045383214950562\n",
      "cnt: 0 - valLoss: 0.5996134877204895 - trainLoss: 0.6045299172401428\n",
      "cnt: 0 - valLoss: 0.5996060967445374 - trainLoss: 0.6045212149620056\n",
      "cnt: 0 - valLoss: 0.5995989441871643 - trainLoss: 0.6045128107070923\n",
      "cnt: 0 - valLoss: 0.5995909571647644 - trainLoss: 0.6045042872428894\n",
      "cnt: 0 - valLoss: 0.5995842814445496 - trainLoss: 0.6044955849647522\n",
      "cnt: 0 - valLoss: 0.5995762944221497 - trainLoss: 0.6044873595237732\n",
      "cnt: 0 - valLoss: 0.5995687246322632 - trainLoss: 0.6044785976409912\n",
      "cnt: 0 - valLoss: 0.5995616912841797 - trainLoss: 0.6044701337814331\n",
      "cnt: 0 - valLoss: 0.5995537638664246 - trainLoss: 0.6044617295265198\n",
      "cnt: 0 - valLoss: 0.5995464324951172 - trainLoss: 0.6044529676437378\n",
      "cnt: 0 - valLoss: 0.5995391607284546 - trainLoss: 0.604444682598114\n",
      "cnt: 0 - valLoss: 0.599531352519989 - trainLoss: 0.6044360995292664\n",
      "cnt: 0 - valLoss: 0.5995246171951294 - trainLoss: 0.6044274568557739\n",
      "cnt: 0 - valLoss: 0.5995166301727295 - trainLoss: 0.6044191718101501\n",
      "cnt: 0 - valLoss: 0.5995091199874878 - trainLoss: 0.6044104695320129\n",
      "cnt: 0 - valLoss: 0.5995020866394043 - trainLoss: 0.6044020652770996\n",
      "cnt: 0 - valLoss: 0.5994940996170044 - trainLoss: 0.6043936014175415\n",
      "cnt: 0 - valLoss: 0.5994869470596313 - trainLoss: 0.6043848395347595\n",
      "cnt: 0 - valLoss: 0.599479615688324 - trainLoss: 0.6043766140937805\n",
      "cnt: 0 - valLoss: 0.5994718074798584 - trainLoss: 0.6043679118156433\n",
      "cnt: 0 - valLoss: 0.599465012550354 - trainLoss: 0.6043592691421509\n",
      "cnt: 0 - valLoss: 0.5994570851325989 - trainLoss: 0.6043510437011719\n",
      "cnt: 0 - valLoss: 0.5994496941566467 - trainLoss: 0.6043422818183899\n",
      "cnt: 0 - valLoss: 0.5994425415992737 - trainLoss: 0.6043338775634766\n",
      "cnt: 0 - valLoss: 0.5994346141815186 - trainLoss: 0.6043253540992737\n",
      "cnt: 0 - valLoss: 0.5994274616241455 - trainLoss: 0.6043165922164917\n",
      "cnt: 0 - valLoss: 0.5994200706481934 - trainLoss: 0.6043083667755127\n",
      "cnt: 0 - valLoss: 0.5994123816490173 - trainLoss: 0.6042996644973755\n",
      "cnt: 0 - valLoss: 0.5994055867195129 - trainLoss: 0.6042910814285278\n",
      "cnt: 0 - valLoss: 0.5993976593017578 - trainLoss: 0.6042827367782593\n",
      "cnt: 0 - valLoss: 0.5993902087211609 - trainLoss: 0.6042739748954773\n",
      "cnt: 0 - valLoss: 0.5993831157684326 - trainLoss: 0.6042655110359192\n",
      "cnt: 0 - valLoss: 0.5993751287460327 - trainLoss: 0.6042571067810059\n",
      "cnt: 0 - valLoss: 0.5993680357933044 - trainLoss: 0.6042482852935791\n",
      "cnt: 0 - valLoss: 0.5993606448173523 - trainLoss: 0.6042400598526001\n",
      "cnt: 0 - valLoss: 0.5993528962135315 - trainLoss: 0.6042313575744629\n",
      "cnt: 0 - valLoss: 0.5993461012840271 - trainLoss: 0.6042227149009705\n",
      "cnt: 0 - valLoss: 0.5993382334709167 - trainLoss: 0.6042143702507019\n",
      "cnt: 0 - valLoss: 0.5993307828903198 - trainLoss: 0.6042056083679199\n",
      "cnt: 0 - valLoss: 0.5993237495422363 - trainLoss: 0.604197084903717\n",
      "cnt: 0 - valLoss: 0.5993158221244812 - trainLoss: 0.6041886210441589\n",
      "cnt: 0 - valLoss: 0.5993085503578186 - trainLoss: 0.6041799187660217\n",
      "cnt: 0 - valLoss: 0.5993013381958008 - trainLoss: 0.6041715741157532\n",
      "cnt: 0 - valLoss: 0.5992934703826904 - trainLoss: 0.6041629314422607\n",
      "cnt: 0 - valLoss: 0.5992868542671204 - trainLoss: 0.6041542291641235\n",
      "cnt: 0 - valLoss: 0.5992789268493652 - trainLoss: 0.6041459441184998\n",
      "cnt: 0 - valLoss: 0.5992713570594788 - trainLoss: 0.6041371822357178\n",
      "cnt: 0 - valLoss: 0.59926438331604 - trainLoss: 0.6041286587715149\n",
      "cnt: 0 - valLoss: 0.5992565155029297 - trainLoss: 0.604120135307312\n",
      "cnt: 0 - valLoss: 0.5992492437362671 - trainLoss: 0.6041114330291748\n",
      "cnt: 0 - valLoss: 0.5992419719696045 - trainLoss: 0.6041030883789062\n",
      "cnt: 0 - valLoss: 0.5992341637611389 - trainLoss: 0.604094386100769\n",
      "cnt: 0 - valLoss: 0.5992275476455688 - trainLoss: 0.6040856838226318\n",
      "cnt: 0 - valLoss: 0.599219560623169 - trainLoss: 0.6040774583816528\n",
      "cnt: 0 - valLoss: 0.5992119312286377 - trainLoss: 0.6040686368942261\n",
      "cnt: 0 - valLoss: 0.5992051362991333 - trainLoss: 0.6040600538253784\n",
      "cnt: 0 - valLoss: 0.5991972088813782 - trainLoss: 0.6040515899658203\n",
      "cnt: 0 - valLoss: 0.599189817905426 - trainLoss: 0.6040428280830383\n",
      "cnt: 0 - valLoss: 0.5991827249526978 - trainLoss: 0.6040343642234802\n",
      "cnt: 0 - valLoss: 0.5991747379302979 - trainLoss: 0.6040258407592773\n",
      "cnt: 0 - valLoss: 0.5991677045822144 - trainLoss: 0.6040170192718506\n",
      "cnt: 0 - valLoss: 0.599160373210907 - trainLoss: 0.604008674621582\n",
      "cnt: 0 - valLoss: 0.5991526246070862 - trainLoss: 0.6039998531341553\n",
      "cnt: 0 - valLoss: 0.5991458892822266 - trainLoss: 0.6039912104606628\n",
      "cnt: 0 - valLoss: 0.5991379022598267 - trainLoss: 0.6039828658103943\n",
      "cnt: 0 - valLoss: 0.5991304516792297 - trainLoss: 0.6039740443229675\n",
      "cnt: 0 - valLoss: 0.599123477935791 - trainLoss: 0.6039655804634094\n",
      "cnt: 0 - valLoss: 0.5991155505180359 - trainLoss: 0.6039570569992065\n",
      "cnt: 0 - valLoss: 0.5991083383560181 - trainLoss: 0.6039482355117798\n",
      "cnt: 0 - valLoss: 0.5991010665893555 - trainLoss: 0.6039398908615112\n",
      "cnt: 0 - valLoss: 0.5990932583808899 - trainLoss: 0.6039311289787292\n",
      "cnt: 0 - valLoss: 0.5990867018699646 - trainLoss: 0.603922426700592\n",
      "cnt: 0 - valLoss: 0.5990787744522095 - trainLoss: 0.6039140224456787\n",
      "cnt: 0 - valLoss: 0.5990710854530334 - trainLoss: 0.6039052605628967\n",
      "cnt: 0 - valLoss: 0.5990642309188843 - trainLoss: 0.6038966178894043\n",
      "cnt: 0 - valLoss: 0.5990563631057739 - trainLoss: 0.6038881540298462\n",
      "cnt: 0 - valLoss: 0.5990489721298218 - trainLoss: 0.6038793325424194\n",
      "cnt: 0 - valLoss: 0.5990419983863831 - trainLoss: 0.6038708686828613\n",
      "cnt: 0 - valLoss: 0.5990341305732727 - trainLoss: 0.6038623452186584\n",
      "cnt: 0 - valLoss: 0.5990269780158997 - trainLoss: 0.6038535237312317\n",
      "cnt: 0 - valLoss: 0.5990197658538818 - trainLoss: 0.6038451194763184\n",
      "cnt: 0 - valLoss: 0.5990118980407715 - trainLoss: 0.6038363575935364\n",
      "cnt: 0 - valLoss: 0.5990053415298462 - trainLoss: 0.6038275361061096\n",
      "cnt: 0 - valLoss: 0.5989975929260254 - trainLoss: 0.6038192510604858\n",
      "cnt: 0 - valLoss: 0.5989899635314941 - trainLoss: 0.6038104891777039\n",
      "cnt: 0 - valLoss: 0.5989832282066345 - trainLoss: 0.6038018465042114\n",
      "cnt: 0 - valLoss: 0.5989753603935242 - trainLoss: 0.6037933230400085\n",
      "cnt: 0 - valLoss: 0.598967969417572 - trainLoss: 0.6037845611572266\n",
      "cnt: 0 - valLoss: 0.5989610552787781 - trainLoss: 0.6037759780883789\n",
      "cnt: 0 - valLoss: 0.5989531874656677 - trainLoss: 0.6037673950195312\n",
      "cnt: 0 - valLoss: 0.5989460349082947 - trainLoss: 0.6037585735321045\n",
      "cnt: 0 - valLoss: 0.5989388227462769 - trainLoss: 0.6037501692771912\n",
      "cnt: 0 - valLoss: 0.5989310145378113 - trainLoss: 0.603741466999054\n",
      "cnt: 0 - valLoss: 0.5989245176315308 - trainLoss: 0.6037326455116272\n",
      "cnt: 0 - valLoss: 0.5989167094230652 - trainLoss: 0.6037242412567139\n",
      "cnt: 0 - valLoss: 0.5989091396331787 - trainLoss: 0.6037154197692871\n",
      "cnt: 0 - valLoss: 0.5989024639129639 - trainLoss: 0.6037067174911499\n",
      "cnt: 0 - valLoss: 0.5988946557044983 - trainLoss: 0.6036983132362366\n",
      "cnt: 0 - valLoss: 0.5988873243331909 - trainLoss: 0.6036893725395203\n",
      "cnt: 0 - valLoss: 0.598880410194397 - trainLoss: 0.6036808490753174\n",
      "cnt: 0 - valLoss: 0.5988726615905762 - trainLoss: 0.6036722660064697\n",
      "cnt: 0 - valLoss: 0.5988654494285583 - trainLoss: 0.603663444519043\n",
      "cnt: 0 - valLoss: 0.5988583564758301 - trainLoss: 0.6036549210548401\n",
      "cnt: 0 - valLoss: 0.5988506078720093 - trainLoss: 0.6036462783813477\n",
      "cnt: 0 - valLoss: 0.5988436341285706 - trainLoss: 0.6036373376846313\n",
      "cnt: 0 - valLoss: 0.5988364219665527 - trainLoss: 0.6036289930343628\n",
      "cnt: 0 - valLoss: 0.5988287329673767 - trainLoss: 0.603620171546936\n",
      "cnt: 0 - valLoss: 0.5988222360610962 - trainLoss: 0.603611409664154\n",
      "cnt: 0 - valLoss: 0.5988144874572754 - trainLoss: 0.603602945804596\n",
      "cnt: 0 - valLoss: 0.5988070964813232 - trainLoss: 0.6035940647125244\n",
      "cnt: 0 - valLoss: 0.5988003611564636 - trainLoss: 0.6035853624343872\n",
      "cnt: 0 - valLoss: 0.5987926125526428 - trainLoss: 0.6035767197608948\n",
      "cnt: 0 - valLoss: 0.5987853407859802 - trainLoss: 0.6035678386688232\n",
      "cnt: 0 - valLoss: 0.598778486251831 - trainLoss: 0.6035592555999756\n",
      "cnt: 0 - valLoss: 0.5987707376480103 - trainLoss: 0.6035506129264832\n",
      "cnt: 0 - valLoss: 0.598763644695282 - trainLoss: 0.6035416722297668\n",
      "cnt: 0 - valLoss: 0.5987566113471985 - trainLoss: 0.6035332083702087\n",
      "cnt: 0 - valLoss: 0.5987488627433777 - trainLoss: 0.6035244464874268\n",
      "cnt: 0 - valLoss: 0.5987419486045837 - trainLoss: 0.6035154461860657\n",
      "cnt: 0 - valLoss: 0.5987346768379211 - trainLoss: 0.6035071015357971\n",
      "cnt: 0 - valLoss: 0.5987271070480347 - trainLoss: 0.6034981608390808\n",
      "cnt: 0 - valLoss: 0.5987204909324646 - trainLoss: 0.603489339351654\n",
      "cnt: 0 - valLoss: 0.5987127423286438 - trainLoss: 0.603480875492096\n",
      "cnt: 0 - valLoss: 0.5987053513526917 - trainLoss: 0.6034719347953796\n",
      "cnt: 0 - valLoss: 0.598698616027832 - trainLoss: 0.6034632325172424\n",
      "cnt: 0 - valLoss: 0.5986908674240112 - trainLoss: 0.60345458984375\n",
      "cnt: 0 - valLoss: 0.5986835956573486 - trainLoss: 0.6034457087516785\n",
      "cnt: 0 - valLoss: 0.5986762642860413 - trainLoss: 0.6034371256828308\n",
      "cnt: 0 - valLoss: 0.5986680388450623 - trainLoss: 0.6034284234046936\n",
      "cnt: 0 - valLoss: 0.5986610054969788 - trainLoss: 0.6034195423126221\n",
      "cnt: 0 - valLoss: 0.5986534953117371 - trainLoss: 0.603411078453064\n",
      "cnt: 0 - valLoss: 0.5986453890800476 - trainLoss: 0.6034022569656372\n",
      "cnt: 0 - valLoss: 0.5986384749412537 - trainLoss: 0.6033934354782104\n",
      "cnt: 0 - valLoss: 0.5986302495002747 - trainLoss: 0.6033849716186523\n",
      "cnt: 0 - valLoss: 0.5986224412918091 - trainLoss: 0.6033760905265808\n",
      "cnt: 0 - valLoss: 0.5986156463623047 - trainLoss: 0.6033674478530884\n",
      "cnt: 0 - valLoss: 0.5986075401306152 - trainLoss: 0.6033588647842407\n",
      "cnt: 0 - valLoss: 0.5985999703407288 - trainLoss: 0.6033499240875244\n",
      "cnt: 0 - valLoss: 0.5985925793647766 - trainLoss: 0.6033414006233215\n",
      "cnt: 0 - valLoss: 0.5985843539237976 - trainLoss: 0.6033326983451843\n",
      "cnt: 0 - valLoss: 0.5985774397850037 - trainLoss: 0.6033238172531128\n",
      "cnt: 0 - valLoss: 0.5985698103904724 - trainLoss: 0.6033153533935547\n",
      "cnt: 0 - valLoss: 0.5985617637634277 - trainLoss: 0.6033065319061279\n",
      "cnt: 0 - valLoss: 0.598554790019989 - trainLoss: 0.603297770023346\n",
      "cnt: 0 - valLoss: 0.59854656457901 - trainLoss: 0.6032892465591431\n",
      "cnt: 0 - valLoss: 0.5985391736030579 - trainLoss: 0.6032803654670715\n",
      "cnt: 0 - valLoss: 0.5985320806503296 - trainLoss: 0.6032716631889343\n",
      "cnt: 0 - valLoss: 0.5985239148139954 - trainLoss: 0.6032630801200867\n",
      "cnt: 0 - valLoss: 0.5985164046287537 - trainLoss: 0.6032541990280151\n",
      "cnt: 0 - valLoss: 0.5985103845596313 - trainLoss: 0.603245735168457\n",
      "cnt: 0 - valLoss: 0.5985022783279419 - trainLoss: 0.6032370924949646\n",
      "cnt: 0 - valLoss: 0.5984949469566345 - trainLoss: 0.6032281517982483\n",
      "cnt: 0 - valLoss: 0.5984883904457092 - trainLoss: 0.6032198667526245\n",
      "cnt: 0 - valLoss: 0.5984802842140198 - trainLoss: 0.6032111048698425\n",
      "cnt: 0 - valLoss: 0.598474383354187 - trainLoss: 0.6032022833824158\n",
      "cnt: 0 - valLoss: 0.5984662771224976 - trainLoss: 0.603193998336792\n",
      "cnt: 0 - valLoss: 0.5984585881233215 - trainLoss: 0.6031850576400757\n",
      "cnt: 0 - valLoss: 0.5984523296356201 - trainLoss: 0.6031765341758728\n",
      "cnt: 0 - valLoss: 0.5984442830085754 - trainLoss: 0.6031678915023804\n",
      "cnt: 0 - valLoss: 0.5984376072883606 - trainLoss: 0.6031590700149536\n",
      "cnt: 0 - valLoss: 0.5984302759170532 - trainLoss: 0.6031506657600403\n",
      "cnt: 0 - valLoss: 0.5984231233596802 - trainLoss: 0.6031419634819031\n",
      "cnt: 0 - valLoss: 0.598415732383728 - trainLoss: 0.6031331419944763\n",
      "cnt: 0 - valLoss: 0.5984091758728027 - trainLoss: 0.603124737739563\n",
      "cnt: 0 - valLoss: 0.5984011292457581 - trainLoss: 0.6031160354614258\n",
      "cnt: 0 - valLoss: 0.5983943939208984 - trainLoss: 0.6031071543693542\n",
      "cnt: 0 - valLoss: 0.5983871221542358 - trainLoss: 0.6030988693237305\n",
      "cnt: 0 - valLoss: 0.5983794331550598 - trainLoss: 0.6030899286270142\n",
      "cnt: 0 - valLoss: 0.5983736515045166 - trainLoss: 0.6030813455581665\n",
      "cnt: 0 - valLoss: 0.5983623266220093 - trainLoss: 0.6030738949775696\n",
      "cnt: 0 - valLoss: 0.5983561873435974 - trainLoss: 0.6030652523040771\n",
      "cnt: 0 - valLoss: 0.5983489155769348 - trainLoss: 0.6030565500259399\n",
      "cnt: 0 - valLoss: 0.5983419418334961 - trainLoss: 0.6030476093292236\n",
      "cnt: 0 - valLoss: 0.5983356237411499 - trainLoss: 0.6030387878417969\n",
      "cnt: 0 - valLoss: 0.5983286499977112 - trainLoss: 0.6030301451683044\n",
      "cnt: 0 - valLoss: 0.5983191728591919 - trainLoss: 0.603022038936615\n",
      "cnt: 0 - valLoss: 0.5983120799064636 - trainLoss: 0.6030139923095703\n",
      "cnt: 0 - valLoss: 0.5983051657676697 - trainLoss: 0.6030048131942749\n",
      "cnt: 0 - valLoss: 0.5982987880706787 - trainLoss: 0.6029959917068481\n",
      "cnt: 0 - valLoss: 0.5982918739318848 - trainLoss: 0.6029872894287109\n",
      "cnt: 0 - valLoss: 0.5982823371887207 - trainLoss: 0.602979302406311\n",
      "cnt: 0 - valLoss: 0.5982752442359924 - trainLoss: 0.6029711365699768\n",
      "cnt: 0 - valLoss: 0.5982683300971985 - trainLoss: 0.6029620170593262\n",
      "cnt: 0 - valLoss: 0.5982620120048523 - trainLoss: 0.6029531955718994\n",
      "cnt: 0 - valLoss: 0.5982551574707031 - trainLoss: 0.602944552898407\n",
      "cnt: 0 - valLoss: 0.5982456207275391 - trainLoss: 0.6029366254806519\n",
      "cnt: 0 - valLoss: 0.598238468170166 - trainLoss: 0.6029284000396729\n",
      "cnt: 0 - valLoss: 0.5982315540313721 - trainLoss: 0.6029192209243774\n",
      "cnt: 0 - valLoss: 0.5982253551483154 - trainLoss: 0.6029103994369507\n",
      "cnt: 0 - valLoss: 0.5982152819633484 - trainLoss: 0.6029018759727478\n",
      "cnt: 0 - valLoss: 0.5982091426849365 - trainLoss: 0.6028939485549927\n",
      "cnt: 0 - valLoss: 0.5982018113136292 - trainLoss: 0.6028853058815002\n",
      "cnt: 0 - valLoss: 0.59819495677948 - trainLoss: 0.6028763055801392\n",
      "cnt: 0 - valLoss: 0.5981888771057129 - trainLoss: 0.6028675436973572\n",
      "cnt: 0 - valLoss: 0.5981785655021667 - trainLoss: 0.6028593182563782\n",
      "cnt: 0 - valLoss: 0.5981724262237549 - trainLoss: 0.6028510928153992\n",
      "cnt: 0 - valLoss: 0.5981652140617371 - trainLoss: 0.6028424501419067\n",
      "cnt: 0 - valLoss: 0.5981582403182983 - trainLoss: 0.6028333902359009\n",
      "cnt: 0 - valLoss: 0.5981524586677551 - trainLoss: 0.6028245687484741\n",
      "cnt: 0 - valLoss: 0.5981419086456299 - trainLoss: 0.6028168201446533\n",
      "cnt: 0 - valLoss: 0.5981358885765076 - trainLoss: 0.6028081178665161\n",
      "cnt: 0 - valLoss: 0.598128616809845 - trainLoss: 0.6027994751930237\n",
      "cnt: 0 - valLoss: 0.5981216430664062 - trainLoss: 0.6027904152870178\n",
      "cnt: 0 - valLoss: 0.5981127023696899 - trainLoss: 0.6027816534042358\n",
      "cnt: 0 - valLoss: 0.5981055498123169 - trainLoss: 0.6027742028236389\n",
      "cnt: 0 - valLoss: 0.5980986952781677 - trainLoss: 0.6027651429176331\n",
      "cnt: 0 - valLoss: 0.5980923175811768 - trainLoss: 0.6027563214302063\n",
      "cnt: 0 - valLoss: 0.5980854630470276 - trainLoss: 0.6027475595474243\n",
      "cnt: 0 - valLoss: 0.5980759859085083 - trainLoss: 0.6027392745018005\n",
      "cnt: 0 - valLoss: 0.5980690121650696 - trainLoss: 0.6027313470840454\n",
      "cnt: 0 - valLoss: 0.5980620384216309 - trainLoss: 0.60272216796875\n",
      "cnt: 0 - valLoss: 0.5980559587478638 - trainLoss: 0.6027132868766785\n",
      "cnt: 0 - valLoss: 0.5980457663536072 - trainLoss: 0.6027047634124756\n",
      "cnt: 0 - valLoss: 0.5980398058891296 - trainLoss: 0.6026967167854309\n",
      "cnt: 0 - valLoss: 0.5980325937271118 - trainLoss: 0.6026880741119385\n",
      "cnt: 0 - valLoss: 0.5980255603790283 - trainLoss: 0.6026790738105774\n",
      "cnt: 0 - valLoss: 0.5980199575424194 - trainLoss: 0.6026701331138611\n",
      "cnt: 0 - valLoss: 0.5980092883110046 - trainLoss: 0.6026625037193298\n",
      "cnt: 0 - valLoss: 0.5980028510093689 - trainLoss: 0.6026536226272583\n",
      "cnt: 0 - valLoss: 0.597996175289154 - trainLoss: 0.6026450395584106\n",
      "cnt: 0 - valLoss: 0.5979891419410706 - trainLoss: 0.6026359796524048\n",
      "cnt: 0 - valLoss: 0.5979804396629333 - trainLoss: 0.6026272177696228\n",
      "cnt: 0 - valLoss: 0.5979732275009155 - trainLoss: 0.6026197075843811\n",
      "cnt: 0 - valLoss: 0.5979661345481873 - trainLoss: 0.6026106476783752\n",
      "cnt: 0 - valLoss: 0.5979600548744202 - trainLoss: 0.6026016473770142\n",
      "cnt: 0 - valLoss: 0.5979534387588501 - trainLoss: 0.6025928854942322\n",
      "cnt: 0 - valLoss: 0.5979434251785278 - trainLoss: 0.6025851368904114\n",
      "cnt: 0 - valLoss: 0.597936749458313 - trainLoss: 0.6025765538215637\n",
      "cnt: 0 - valLoss: 0.5979295372962952 - trainLoss: 0.6025674939155579\n",
      "cnt: 0 - valLoss: 0.597923994064331 - trainLoss: 0.6025583148002625\n",
      "cnt: 0 - valLoss: 0.5979134440422058 - trainLoss: 0.6025506854057312\n",
      "cnt: 0 - valLoss: 0.5979068279266357 - trainLoss: 0.602541983127594\n",
      "cnt: 0 - valLoss: 0.5979003310203552 - trainLoss: 0.602533221244812\n",
      "cnt: 0 - valLoss: 0.597893238067627 - trainLoss: 0.6025242805480957\n",
      "cnt: 0 - valLoss: 0.5978841185569763 - trainLoss: 0.602515459060669\n",
      "cnt: 0 - valLoss: 0.5978770852088928 - trainLoss: 0.6025080680847168\n",
      "cnt: 0 - valLoss: 0.5978702306747437 - trainLoss: 0.6024987697601318\n",
      "cnt: 0 - valLoss: 0.5978639721870422 - trainLoss: 0.6024898290634155\n",
      "cnt: 0 - valLoss: 0.5978540182113647 - trainLoss: 0.6024811267852783\n",
      "cnt: 0 - valLoss: 0.5978479385375977 - trainLoss: 0.6024733185768127\n",
      "cnt: 0 - valLoss: 0.5978407263755798 - trainLoss: 0.6024646162986755\n",
      "cnt: 0 - valLoss: 0.5978337526321411 - trainLoss: 0.6024554967880249\n",
      "cnt: 0 - valLoss: 0.597828209400177 - trainLoss: 0.602446436882019\n",
      "cnt: 0 - valLoss: 0.5978174805641174 - trainLoss: 0.6024391055107117\n",
      "cnt: 0 - valLoss: 0.5978110432624817 - trainLoss: 0.602429986000061\n",
      "cnt: 0 - valLoss: 0.5978043675422668 - trainLoss: 0.6024212837219238\n",
      "cnt: 0 - valLoss: 0.5977975130081177 - trainLoss: 0.6024121046066284\n",
      "cnt: 0 - valLoss: 0.5977884531021118 - trainLoss: 0.6024038791656494\n",
      "cnt: 0 - valLoss: 0.5977811217308044 - trainLoss: 0.6023960709571838\n",
      "cnt: 0 - valLoss: 0.5977745652198792 - trainLoss: 0.6023865938186646\n",
      "cnt: 0 - valLoss: 0.5977684855461121 - trainLoss: 0.6023777723312378\n",
      "cnt: 0 - valLoss: 0.5977583527565002 - trainLoss: 0.6023695468902588\n",
      "cnt: 0 - valLoss: 0.5977521538734436 - trainLoss: 0.602361261844635\n",
      "cnt: 0 - valLoss: 0.5977450013160706 - trainLoss: 0.6023523807525635\n",
      "cnt: 0 - valLoss: 0.5977382659912109 - trainLoss: 0.6023432016372681\n",
      "cnt: 0 - valLoss: 0.5977293252944946 - trainLoss: 0.6023346781730652\n",
      "cnt: 0 - valLoss: 0.5977221131324768 - trainLoss: 0.6023269295692444\n",
      "cnt: 0 - valLoss: 0.5977151989936829 - trainLoss: 0.602317750453949\n",
      "cnt: 0 - valLoss: 0.5977092385292053 - trainLoss: 0.6023087501525879\n",
      "cnt: 0 - valLoss: 0.5976990461349487 - trainLoss: 0.6023004055023193\n",
      "cnt: 0 - valLoss: 0.597693145275116 - trainLoss: 0.6022921800613403\n",
      "cnt: 0 - valLoss: 0.5976859331130981 - trainLoss: 0.6022834777832031\n",
      "cnt: 0 - valLoss: 0.5976788997650146 - trainLoss: 0.6022742986679077\n",
      "cnt: 0 - valLoss: 0.5976703763008118 - trainLoss: 0.6022654175758362\n",
      "cnt: 0 - valLoss: 0.597663164138794 - trainLoss: 0.6022579669952393\n",
      "cnt: 0 - valLoss: 0.5976560115814209 - trainLoss: 0.6022487878799438\n",
      "cnt: 0 - valLoss: 0.5976503491401672 - trainLoss: 0.6022396087646484\n",
      "cnt: 0 - valLoss: 0.5976401567459106 - trainLoss: 0.6022313237190247\n",
      "cnt: 0 - valLoss: 0.5976334810256958 - trainLoss: 0.6022230982780457\n",
      "cnt: 0 - valLoss: 0.5976271033287048 - trainLoss: 0.6022141575813293\n",
      "cnt: 0 - valLoss: 0.5976201891899109 - trainLoss: 0.6022050976753235\n",
      "cnt: 0 - valLoss: 0.5976110100746155 - trainLoss: 0.602196455001831\n",
      "cnt: 0 - valLoss: 0.5976041555404663 - trainLoss: 0.602188766002655\n",
      "cnt: 0 - valLoss: 0.5975971817970276 - trainLoss: 0.6021794080734253\n",
      "cnt: 0 - valLoss: 0.5975915193557739 - trainLoss: 0.6021702885627747\n",
      "cnt: 0 - valLoss: 0.597581148147583 - trainLoss: 0.6021623611450195\n",
      "cnt: 0 - valLoss: 0.5975747108459473 - trainLoss: 0.6021536588668823\n",
      "cnt: 0 - valLoss: 0.597568154335022 - trainLoss: 0.6021449565887451\n",
      "cnt: 0 - valLoss: 0.5975614190101624 - trainLoss: 0.6021357178688049\n",
      "cnt: 0 - valLoss: 0.5975522398948669 - trainLoss: 0.6021274924278259\n",
      "cnt: 0 - valLoss: 0.5975452065467834 - trainLoss: 0.602119505405426\n",
      "cnt: 0 - valLoss: 0.5975383520126343 - trainLoss: 0.6021099090576172\n",
      "cnt: 0 - valLoss: 0.5975328683853149 - trainLoss: 0.6021008491516113\n",
      "cnt: 0 - valLoss: 0.5975223183631897 - trainLoss: 0.6020933985710144\n",
      "cnt: 0 - valLoss: 0.5975164175033569 - trainLoss: 0.602084219455719\n",
      "cnt: 0 - valLoss: 0.5975093245506287 - trainLoss: 0.6020753979682922\n",
      "cnt: 0 - valLoss: 0.5974998474121094 - trainLoss: 0.6020662784576416\n",
      "cnt: 0 - valLoss: 0.5974935293197632 - trainLoss: 0.6020587682723999\n",
      "cnt: 0 - valLoss: 0.5974863171577454 - trainLoss: 0.6020496487617493\n",
      "cnt: 0 - valLoss: 0.5974799394607544 - trainLoss: 0.6020403504371643\n",
      "cnt: 0 - valLoss: 0.5974708795547485 - trainLoss: 0.6020317673683167\n",
      "cnt: 0 - valLoss: 0.5974637866020203 - trainLoss: 0.6020239591598511\n",
      "cnt: 0 - valLoss: 0.5974570512771606 - trainLoss: 0.6020146608352661\n",
      "cnt: 0 - valLoss: 0.5974512696266174 - trainLoss: 0.6020056009292603\n",
      "cnt: 0 - valLoss: 0.5974410176277161 - trainLoss: 0.6019976735115051\n",
      "cnt: 0 - valLoss: 0.5974350571632385 - trainLoss: 0.6019889116287231\n",
      "cnt: 0 - valLoss: 0.5974279642105103 - trainLoss: 0.6019799709320068\n",
      "cnt: 0 - valLoss: 0.5974215269088745 - trainLoss: 0.6019706726074219\n",
      "cnt: 0 - valLoss: 0.5974124073982239 - trainLoss: 0.6019630432128906\n",
      "cnt: 0 - valLoss: 0.5974053144454956 - trainLoss: 0.6019543409347534\n",
      "cnt: 0 - valLoss: 0.597398042678833 - trainLoss: 0.6019451022148132\n",
      "cnt: 0 - valLoss: 0.5973897576332092 - trainLoss: 0.6019359230995178\n",
      "cnt: 0 - valLoss: 0.5973824858665466 - trainLoss: 0.6019288897514343\n",
      "cnt: 0 - valLoss: 0.5973753333091736 - trainLoss: 0.6019196510314941\n",
      "cnt: 0 - valLoss: 0.5973694324493408 - trainLoss: 0.6019105315208435\n",
      "cnt: 0 - valLoss: 0.5973594188690186 - trainLoss: 0.6019020080566406\n",
      "cnt: 0 - valLoss: 0.5973527431488037 - trainLoss: 0.6018941402435303\n",
      "cnt: 0 - valLoss: 0.5973462462425232 - trainLoss: 0.6018853187561035\n",
      "cnt: 0 - valLoss: 0.5973390340805054 - trainLoss: 0.6018762588500977\n",
      "cnt: 0 - valLoss: 0.5973330140113831 - trainLoss: 0.601867139339447\n",
      "cnt: 0 - valLoss: 0.597323477268219 - trainLoss: 0.6018597483634949\n",
      "cnt: 0 - valLoss: 0.5973162651062012 - trainLoss: 0.6018508076667786\n",
      "cnt: 0 - valLoss: 0.5973096489906311 - trainLoss: 0.6018416285514832\n",
      "cnt: 0 - valLoss: 0.5973038673400879 - trainLoss: 0.6018328070640564\n",
      "cnt: 0 - valLoss: 0.5972935557365417 - trainLoss: 0.6018254160881042\n",
      "cnt: 0 - valLoss: 0.5972872972488403 - trainLoss: 0.6018162965774536\n",
      "cnt: 0 - valLoss: 0.5972800850868225 - trainLoss: 0.6018073558807373\n",
      "cnt: 0 - valLoss: 0.5972738265991211 - trainLoss: 0.6017981767654419\n",
      "cnt: 0 - valLoss: 0.5972645282745361 - trainLoss: 0.6017906069755554\n",
      "cnt: 0 - valLoss: 0.5972573757171631 - trainLoss: 0.601781964302063\n",
      "cnt: 0 - valLoss: 0.5972504615783691 - trainLoss: 0.6017727255821228\n",
      "cnt: 0 - valLoss: 0.5972449779510498 - trainLoss: 0.6017637252807617\n",
      "cnt: 0 - valLoss: 0.5972343683242798 - trainLoss: 0.6017564535140991\n",
      "cnt: 0 - valLoss: 0.5972284078598022 - trainLoss: 0.6017471551895142\n",
      "cnt: 0 - valLoss: 0.597221314907074 - trainLoss: 0.6017383933067322\n",
      "cnt: 0 - valLoss: 0.5972118377685547 - trainLoss: 0.6017292737960815\n",
      "cnt: 0 - valLoss: 0.5972054600715637 - trainLoss: 0.6017218232154846\n",
      "cnt: 0 - valLoss: 0.5971983075141907 - trainLoss: 0.6017127633094788\n",
      "cnt: 0 - valLoss: 0.5971916317939758 - trainLoss: 0.6017035841941833\n",
      "cnt: 0 - valLoss: 0.5971828103065491 - trainLoss: 0.6016948223114014\n",
      "cnt: 0 - valLoss: 0.5971755981445312 - trainLoss: 0.6016872525215149\n",
      "cnt: 0 - valLoss: 0.5971688628196716 - trainLoss: 0.6016779541969299\n",
      "cnt: 0 - valLoss: 0.5971628427505493 - trainLoss: 0.6016690135002136\n",
      "cnt: 0 - valLoss: 0.597152829170227 - trainLoss: 0.6016606092453003\n",
      "cnt: 0 - valLoss: 0.5971468091011047 - trainLoss: 0.6016524434089661\n",
      "cnt: 0 - valLoss: 0.5971396565437317 - trainLoss: 0.6016436219215393\n",
      "cnt: 0 - valLoss: 0.5971328020095825 - trainLoss: 0.6016343832015991\n",
      "cnt: 0 - valLoss: 0.5971242189407349 - trainLoss: 0.601625919342041\n",
      "cnt: 0 - valLoss: 0.597117006778717 - trainLoss: 0.6016180515289307\n",
      "cnt: 0 - valLoss: 0.597109854221344 - trainLoss: 0.6016088128089905\n",
      "cnt: 0 - valLoss: 0.5971044301986694 - trainLoss: 0.6015995740890503\n",
      "cnt: 0 - valLoss: 0.5970941185951233 - trainLoss: 0.6015920042991638\n",
      "cnt: 0 - valLoss: 0.5970874428749084 - trainLoss: 0.6015831232070923\n",
      "cnt: 0 - valLoss: 0.5970810651779175 - trainLoss: 0.6015742421150208\n",
      "cnt: 0 - valLoss: 0.5970745086669922 - trainLoss: 0.6015650629997253\n",
      "cnt: 0 - valLoss: 0.5970650315284729 - trainLoss: 0.6015572547912598\n",
      "cnt: 0 - valLoss: 0.5970582365989685 - trainLoss: 0.6015487313270569\n",
      "cnt: 0 - valLoss: 0.5970511436462402 - trainLoss: 0.6015393733978271\n",
      "cnt: 0 - valLoss: 0.5970427989959717 - trainLoss: 0.6015302538871765\n",
      "cnt: 0 - valLoss: 0.5970357060432434 - trainLoss: 0.6015231013298035\n",
      "cnt: 0 - valLoss: 0.5970285534858704 - trainLoss: 0.6015138030052185\n",
      "cnt: 0 - valLoss: 0.5970230102539062 - trainLoss: 0.6015045642852783\n",
      "cnt: 0 - valLoss: 0.5970128774642944 - trainLoss: 0.6014965176582336\n",
      "cnt: 0 - valLoss: 0.5970062017440796 - trainLoss: 0.6014881134033203\n",
      "cnt: 0 - valLoss: 0.5969998240470886 - trainLoss: 0.6014791131019592\n",
      "cnt: 0 - valLoss: 0.5969932675361633 - trainLoss: 0.601469874382019\n",
      "cnt: 0 - valLoss: 0.5969839096069336 - trainLoss: 0.6014620065689087\n",
      "cnt: 0 - valLoss: 0.5969771146774292 - trainLoss: 0.6014536619186401\n",
      "cnt: 0 - valLoss: 0.5969701409339905 - trainLoss: 0.6014442443847656\n",
      "cnt: 0 - valLoss: 0.5969619154930115 - trainLoss: 0.6014351844787598\n",
      "cnt: 0 - valLoss: 0.5969547629356384 - trainLoss: 0.6014279127120972\n",
      "cnt: 0 - valLoss: 0.5969475507736206 - trainLoss: 0.6014185547828674\n",
      "cnt: 0 - valLoss: 0.5969418287277222 - trainLoss: 0.6014091372489929\n",
      "cnt: 0 - valLoss: 0.5969324111938477 - trainLoss: 0.601401686668396\n",
      "cnt: 0 - valLoss: 0.5969251990318298 - trainLoss: 0.6013928055763245\n",
      "cnt: 0 - valLoss: 0.5969190001487732 - trainLoss: 0.6013835072517395\n",
      "cnt: 0 - valLoss: 0.5969100594520569 - trainLoss: 0.601375162601471\n",
      "cnt: 0 - valLoss: 0.5969029068946838 - trainLoss: 0.601367175579071\n",
      "cnt: 0 - valLoss: 0.5968962907791138 - trainLoss: 0.6013577580451965\n",
      "cnt: 0 - valLoss: 0.5968906879425049 - trainLoss: 0.6013486981391907\n",
      "cnt: 0 - valLoss: 0.5968804359436035 - trainLoss: 0.6013413667678833\n",
      "cnt: 0 - valLoss: 0.596874475479126 - trainLoss: 0.6013320684432983\n",
      "cnt: 0 - valLoss: 0.5968677401542664 - trainLoss: 0.6013230681419373\n",
      "cnt: 0 - valLoss: 0.5968583226203918 - trainLoss: 0.6013147234916687\n",
      "cnt: 0 - valLoss: 0.5968518853187561 - trainLoss: 0.6013064384460449\n",
      "cnt: 0 - valLoss: 0.5968448519706726 - trainLoss: 0.6012971997261047\n",
      "cnt: 0 - valLoss: 0.5968361496925354 - trainLoss: 0.6012880802154541\n",
      "cnt: 0 - valLoss: 0.5968292951583862 - trainLoss: 0.6012808680534363\n",
      "cnt: 0 - valLoss: 0.5968222618103027 - trainLoss: 0.6012713313102722\n",
      "cnt: 0 - valLoss: 0.5968170762062073 - trainLoss: 0.6012619733810425\n",
      "cnt: 0 - valLoss: 0.5968068242073059 - trainLoss: 0.6012547612190247\n",
      "cnt: 0 - valLoss: 0.5968001484870911 - trainLoss: 0.6012454628944397\n",
      "cnt: 0 - valLoss: 0.5967941880226135 - trainLoss: 0.6012363433837891\n",
      "cnt: 0 - valLoss: 0.5967843532562256 - trainLoss: 0.6012281179428101\n",
      "cnt: 0 - valLoss: 0.5967784523963928 - trainLoss: 0.601219654083252\n",
      "cnt: 0 - valLoss: 0.5967714786529541 - trainLoss: 0.6012106537818909\n",
      "cnt: 0 - valLoss: 0.5967622399330139 - trainLoss: 0.6012015342712402\n",
      "cnt: 0 - valLoss: 0.5967559218406677 - trainLoss: 0.6011939644813538\n",
      "cnt: 0 - valLoss: 0.5967487692832947 - trainLoss: 0.6011847257614136\n",
      "cnt: 0 - valLoss: 0.5967426896095276 - trainLoss: 0.6011753082275391\n",
      "cnt: 0 - valLoss: 0.5967336893081665 - trainLoss: 0.6011676788330078\n",
      "cnt: 0 - valLoss: 0.5967265963554382 - trainLoss: 0.6011589169502258\n",
      "cnt: 0 - valLoss: 0.5967200398445129 - trainLoss: 0.6011494398117065\n",
      "cnt: 0 - valLoss: 0.5967115163803101 - trainLoss: 0.6011411547660828\n",
      "cnt: 0 - valLoss: 0.596704363822937 - trainLoss: 0.6011330485343933\n",
      "cnt: 0 - valLoss: 0.5966974496841431 - trainLoss: 0.6011236310005188\n",
      "cnt: 0 - valLoss: 0.5966890454292297 - trainLoss: 0.6011146903038025\n",
      "cnt: 0 - valLoss: 0.5966818928718567 - trainLoss: 0.6011074185371399\n",
      "cnt: 0 - valLoss: 0.5966751575469971 - trainLoss: 0.6010976433753967\n",
      "cnt: 0 - valLoss: 0.5966699123382568 - trainLoss: 0.6010884046554565\n",
      "cnt: 0 - valLoss: 0.5966594815254211 - trainLoss: 0.6010814309120178\n",
      "cnt: 0 - valLoss: 0.5966530442237854 - trainLoss: 0.6010717153549194\n",
      "cnt: 0 - valLoss: 0.5966470837593079 - trainLoss: 0.6010626554489136\n",
      "cnt: 0 - valLoss: 0.5966371893882751 - trainLoss: 0.601054847240448\n",
      "cnt: 0 - valLoss: 0.5966311097145081 - trainLoss: 0.6010459661483765\n",
      "cnt: 0 - valLoss: 0.5966243147850037 - trainLoss: 0.601036787033081\n",
      "cnt: 0 - valLoss: 0.5966150760650635 - trainLoss: 0.6010282635688782\n",
      "cnt: 0 - valLoss: 0.596608579158783 - trainLoss: 0.6010202169418335\n",
      "cnt: 0 - valLoss: 0.5966016054153442 - trainLoss: 0.601010799407959\n",
      "cnt: 0 - valLoss: 0.5965929627418518 - trainLoss: 0.6010017395019531\n",
      "cnt: 0 - valLoss: 0.5965860486030579 - trainLoss: 0.6009944081306458\n",
      "cnt: 0 - valLoss: 0.5965788960456848 - trainLoss: 0.6009848713874817\n",
      "cnt: 0 - valLoss: 0.5965732932090759 - trainLoss: 0.6009753346443176\n",
      "cnt: 0 - valLoss: 0.5965638756752014 - trainLoss: 0.6009682416915894\n",
      "cnt: 0 - valLoss: 0.5965566635131836 - trainLoss: 0.6009588837623596\n",
      "cnt: 0 - valLoss: 0.5965506434440613 - trainLoss: 0.6009494662284851\n",
      "cnt: 0 - valLoss: 0.5965416431427002 - trainLoss: 0.6009417772293091\n",
      "cnt: 0 - valLoss: 0.5965344905853271 - trainLoss: 0.6009330749511719\n",
      "cnt: 0 - valLoss: 0.5965279340744019 - trainLoss: 0.6009235978126526\n",
      "cnt: 0 - valLoss: 0.5965194702148438 - trainLoss: 0.6009151935577393\n",
      "cnt: 0 - valLoss: 0.5965122580528259 - trainLoss: 0.6009072065353394\n",
      "cnt: 0 - valLoss: 0.5965052247047424 - trainLoss: 0.6008977890014648\n",
      "cnt: 0 - valLoss: 0.5964971780776978 - trainLoss: 0.6008886098861694\n",
      "cnt: 0 - valLoss: 0.5964900851249695 - trainLoss: 0.6008813381195068\n",
      "cnt: 0 - valLoss: 0.5964828729629517 - trainLoss: 0.6008719205856323\n",
      "cnt: 0 - valLoss: 0.5964770913124084 - trainLoss: 0.6008625030517578\n",
      "cnt: 0 - valLoss: 0.5964678525924683 - trainLoss: 0.6008552312850952\n",
      "cnt: 0 - valLoss: 0.5964607000350952 - trainLoss: 0.6008461117744446\n",
      "cnt: 0 - valLoss: 0.596454381942749 - trainLoss: 0.6008366942405701\n",
      "cnt: 0 - valLoss: 0.5964456796646118 - trainLoss: 0.6008286476135254\n",
      "cnt: 0 - valLoss: 0.5964385271072388 - trainLoss: 0.6008202433586121\n",
      "cnt: 0 - valLoss: 0.5964317917823792 - trainLoss: 0.6008108258247375\n",
      "cnt: 0 - valLoss: 0.5964235067367554 - trainLoss: 0.6008021831512451\n",
      "cnt: 0 - valLoss: 0.5964164137840271 - trainLoss: 0.6007943749427795\n",
      "cnt: 0 - valLoss: 0.596409261226654 - trainLoss: 0.6007848978042603\n",
      "cnt: 0 - valLoss: 0.5964009761810303 - trainLoss: 0.6007756590843201\n",
      "cnt: 0 - valLoss: 0.5963939428329468 - trainLoss: 0.6007686853408813\n",
      "cnt: 0 - valLoss: 0.5963869690895081 - trainLoss: 0.600758969783783\n",
      "cnt: 0 - valLoss: 0.5963817238807678 - trainLoss: 0.6007496118545532\n",
      "cnt: 0 - valLoss: 0.5963715314865112 - trainLoss: 0.600742518901825\n",
      "cnt: 0 - valLoss: 0.5963647961616516 - trainLoss: 0.6007329821586609\n",
      "cnt: 0 - valLoss: 0.5963590741157532 - trainLoss: 0.6007238626480103\n",
      "cnt: 0 - valLoss: 0.5963491201400757 - trainLoss: 0.6007159948348999\n",
      "cnt: 0 - valLoss: 0.5963426828384399 - trainLoss: 0.6007070541381836\n",
      "cnt: 0 - valLoss: 0.5963364243507385 - trainLoss: 0.600697934627533\n",
      "cnt: 0 - valLoss: 0.5963269472122192 - trainLoss: 0.6006894111633301\n",
      "cnt: 0 - valLoss: 0.5963208675384521 - trainLoss: 0.6006811857223511\n",
      "cnt: 0 - valLoss: 0.5963138937950134 - trainLoss: 0.6006720066070557\n",
      "cnt: 0 - valLoss: 0.5963048338890076 - trainLoss: 0.6006630063056946\n",
      "cnt: 0 - valLoss: 0.5962985157966614 - trainLoss: 0.6006553173065186\n",
      "cnt: 0 - valLoss: 0.5962913632392883 - trainLoss: 0.6006460189819336\n",
      "cnt: 0 - valLoss: 0.5962827801704407 - trainLoss: 0.6006366014480591\n",
      "cnt: 0 - valLoss: 0.5962761044502258 - trainLoss: 0.6006295084953308\n",
      "cnt: 0 - valLoss: 0.5962690114974976 - trainLoss: 0.6006199717521667\n",
      "cnt: 0 - valLoss: 0.59626305103302 - trainLoss: 0.6006104946136475\n",
      "cnt: 0 - valLoss: 0.5962540507316589 - trainLoss: 0.600602924823761\n",
      "cnt: 0 - valLoss: 0.5962468981742859 - trainLoss: 0.6005940437316895\n",
      "cnt: 0 - valLoss: 0.5962404608726501 - trainLoss: 0.6005845069885254\n",
      "cnt: 0 - valLoss: 0.5962320566177368 - trainLoss: 0.6005764603614807\n",
      "cnt: 0 - valLoss: 0.5962249040603638 - trainLoss: 0.6005681157112122\n",
      "cnt: 0 - valLoss: 0.5962181687355042 - trainLoss: 0.6005585193634033\n",
      "cnt: 0 - valLoss: 0.5962095260620117 - trainLoss: 0.6005500555038452\n",
      "cnt: 0 - valLoss: 0.5962024927139282 - trainLoss: 0.6005421876907349\n",
      "cnt: 0 - valLoss: 0.5961956977844238 - trainLoss: 0.6005324125289917\n",
      "cnt: 0 - valLoss: 0.5961876511573792 - trainLoss: 0.6005235910415649\n",
      "cnt: 0 - valLoss: 0.5961804986000061 - trainLoss: 0.6005159616470337\n",
      "cnt: 0 - valLoss: 0.5961735248565674 - trainLoss: 0.6005063652992249\n",
      "cnt: 0 - valLoss: 0.5961650609970093 - trainLoss: 0.6004973649978638\n",
      "cnt: 0 - valLoss: 0.5961582064628601 - trainLoss: 0.6004900336265564\n",
      "cnt: 0 - valLoss: 0.5961511135101318 - trainLoss: 0.6004802584648132\n",
      "cnt: 0 - valLoss: 0.5961429476737976 - trainLoss: 0.6004709005355835\n",
      "cnt: 0 - valLoss: 0.5961359143257141 - trainLoss: 0.6004639267921448\n",
      "cnt: 0 - valLoss: 0.5961288809776306 - trainLoss: 0.6004540920257568\n",
      "cnt: 0 - valLoss: 0.5961210131645203 - trainLoss: 0.6004446148872375\n",
      "cnt: 0 - valLoss: 0.596113920211792 - trainLoss: 0.600437581539154\n",
      "cnt: 0 - valLoss: 0.596106767654419 - trainLoss: 0.6004279851913452\n",
      "cnt: 0 - valLoss: 0.5961010456085205 - trainLoss: 0.6004183888435364\n",
      "cnt: 0 - valLoss: 0.5960919260978699 - trainLoss: 0.6004111766815186\n",
      "cnt: 0 - valLoss: 0.5960847735404968 - trainLoss: 0.6004018783569336\n",
      "cnt: 0 - valLoss: 0.5960785746574402 - trainLoss: 0.6003922820091248\n",
      "cnt: 0 - valLoss: 0.596069872379303 - trainLoss: 0.6003847122192383\n",
      "cnt: 0 - valLoss: 0.5960627198219299 - trainLoss: 0.6003757119178772\n",
      "cnt: 0 - valLoss: 0.596056342124939 - trainLoss: 0.6003661751747131\n",
      "cnt: 0 - valLoss: 0.5960474610328674 - trainLoss: 0.6003583073616028\n",
      "cnt: 0 - valLoss: 0.5960405468940735 - trainLoss: 0.6003497838973999\n",
      "cnt: 0 - valLoss: 0.5960339903831482 - trainLoss: 0.600339949131012\n",
      "cnt: 0 - valLoss: 0.5960257053375244 - trainLoss: 0.6003319025039673\n",
      "cnt: 0 - valLoss: 0.5960186123847961 - trainLoss: 0.6003233790397644\n",
      "cnt: 0 - valLoss: 0.5960120558738708 - trainLoss: 0.6003137826919556\n",
      "cnt: 0 - valLoss: 0.5960030555725098 - trainLoss: 0.6003056764602661\n",
      "cnt: 0 - valLoss: 0.5959963202476501 - trainLoss: 0.6002972722053528\n",
      "cnt: 0 - valLoss: 0.5959898233413696 - trainLoss: 0.6002874970436096\n",
      "cnt: 0 - valLoss: 0.5959810614585876 - trainLoss: 0.6002793312072754\n",
      "cnt: 0 - valLoss: 0.5959740877151489 - trainLoss: 0.6002711057662964\n",
      "cnt: 0 - valLoss: 0.5959675908088684 - trainLoss: 0.6002612709999084\n",
      "cnt: 0 - valLoss: 0.5959590077400208 - trainLoss: 0.6002529263496399\n",
      "cnt: 0 - valLoss: 0.5959519147872925 - trainLoss: 0.6002448797225952\n",
      "cnt: 0 - valLoss: 0.5959454774856567 - trainLoss: 0.6002349257469177\n",
      "cnt: 0 - valLoss: 0.5959371328353882 - trainLoss: 0.600226640701294\n",
      "cnt: 0 - valLoss: 0.5959299206733704 - trainLoss: 0.6002183556556702\n",
      "cnt: 0 - valLoss: 0.5959234237670898 - trainLoss: 0.6002086997032166\n",
      "cnt: 0 - valLoss: 0.5959146022796631 - trainLoss: 0.600200355052948\n",
      "cnt: 0 - valLoss: 0.5959077477455139 - trainLoss: 0.6001921892166138\n",
      "cnt: 0 - valLoss: 0.5959012508392334 - trainLoss: 0.6001823544502258\n",
      "cnt: 0 - valLoss: 0.5958925485610962 - trainLoss: 0.6001741290092468\n",
      "cnt: 0 - valLoss: 0.5958856344223022 - trainLoss: 0.6001659035682678\n",
      "cnt: 0 - valLoss: 0.5958791375160217 - trainLoss: 0.6001560091972351\n",
      "cnt: 0 - valLoss: 0.5958705544471741 - trainLoss: 0.6001476645469666\n",
      "cnt: 0 - valLoss: 0.5958634614944458 - trainLoss: 0.6001396775245667\n",
      "cnt: 0 - valLoss: 0.5958570241928101 - trainLoss: 0.6001296639442444\n",
      "cnt: 0 - valLoss: 0.5958486199378967 - trainLoss: 0.6001214385032654\n",
      "cnt: 0 - valLoss: 0.5958415865898132 - trainLoss: 0.6001130938529968\n",
      "cnt: 0 - valLoss: 0.5958350300788879 - trainLoss: 0.6001033782958984\n",
      "cnt: 0 - valLoss: 0.5958261489868164 - trainLoss: 0.6000951528549194\n",
      "cnt: 0 - valLoss: 0.595819354057312 - trainLoss: 0.6000868082046509\n",
      "cnt: 0 - valLoss: 0.5958129167556763 - trainLoss: 0.6000769138336182\n",
      "cnt: 0 - valLoss: 0.5958041548728943 - trainLoss: 0.6000688076019287\n",
      "cnt: 0 - valLoss: 0.5957972407341003 - trainLoss: 0.6000605225563049\n",
      "cnt: 0 - valLoss: 0.5957908034324646 - trainLoss: 0.6000505685806274\n",
      "cnt: 0 - valLoss: 0.5957821011543274 - trainLoss: 0.600042462348938\n",
      "cnt: 0 - valLoss: 0.5957751274108887 - trainLoss: 0.6000341176986694\n",
      "cnt: 0 - valLoss: 0.5957686901092529 - trainLoss: 0.6000241637229919\n",
      "cnt: 0 - valLoss: 0.5957604050636292 - trainLoss: 0.6000161170959473\n",
      "cnt: 0 - valLoss: 0.5957532525062561 - trainLoss: 0.6000075340270996\n",
      "cnt: 0 - valLoss: 0.5957469344139099 - trainLoss: 0.5999978184700012\n",
      "cnt: 0 - valLoss: 0.5957377552986145 - trainLoss: 0.5999899506568909\n",
      "cnt: 0 - valLoss: 0.5957311391830444 - trainLoss: 0.5999812483787537\n",
      "cnt: 0 - valLoss: 0.595724880695343 - trainLoss: 0.5999714732170105\n",
      "cnt: 0 - valLoss: 0.5957157015800476 - trainLoss: 0.5999636054039001\n",
      "cnt: 0 - valLoss: 0.5957090258598328 - trainLoss: 0.5999548435211182\n",
      "cnt: 0 - valLoss: 0.5957028269767761 - trainLoss: 0.5999450087547302\n",
      "cnt: 0 - valLoss: 0.5956937670707703 - trainLoss: 0.5999373197555542\n",
      "cnt: 0 - valLoss: 0.5956869125366211 - trainLoss: 0.5999284386634827\n",
      "cnt: 0 - valLoss: 0.5956807732582092 - trainLoss: 0.5999184846878052\n",
      "cnt: 0 - valLoss: 0.5956717729568481 - trainLoss: 0.5999110341072083\n",
      "cnt: 0 - valLoss: 0.5956648588180542 - trainLoss: 0.5999019742012024\n",
      "cnt: 0 - valLoss: 0.5956560373306274 - trainLoss: 0.5998921394348145\n",
      "cnt: 0 - valLoss: 0.5956498980522156 - trainLoss: 0.5998848676681519\n",
      "cnt: 0 - valLoss: 0.5956429243087769 - trainLoss: 0.599875271320343\n",
      "cnt: 0 - valLoss: 0.5956339240074158 - trainLoss: 0.5998659133911133\n",
      "cnt: 0 - valLoss: 0.5956278443336487 - trainLoss: 0.5998584032058716\n",
      "cnt: 0 - valLoss: 0.5956209897994995 - trainLoss: 0.599848747253418\n",
      "cnt: 0 - valLoss: 0.5956119298934937 - trainLoss: 0.5998396873474121\n",
      "cnt: 0 - valLoss: 0.5956058502197266 - trainLoss: 0.5998318791389465\n",
      "cnt: 0 - valLoss: 0.5955991148948669 - trainLoss: 0.5998222827911377\n",
      "cnt: 0 - valLoss: 0.595589816570282 - trainLoss: 0.5998134613037109\n",
      "cnt: 0 - valLoss: 0.5955837368965149 - trainLoss: 0.5998054146766663\n",
      "cnt: 0 - valLoss: 0.5955772399902344 - trainLoss: 0.5997956395149231\n",
      "cnt: 0 - valLoss: 0.5955677628517151 - trainLoss: 0.599787175655365\n",
      "cnt: 0 - valLoss: 0.595561683177948 - trainLoss: 0.5997788906097412\n",
      "cnt: 0 - valLoss: 0.5955553650856018 - trainLoss: 0.599769115447998\n",
      "cnt: 0 - valLoss: 0.5955457091331482 - trainLoss: 0.5997610092163086\n",
      "cnt: 0 - valLoss: 0.5955396294593811 - trainLoss: 0.5997523069381714\n",
      "cnt: 0 - valLoss: 0.595533549785614 - trainLoss: 0.5997425317764282\n",
      "cnt: 0 - valLoss: 0.5955236554145813 - trainLoss: 0.5997347831726074\n",
      "cnt: 0 - valLoss: 0.5955175161361694 - trainLoss: 0.5997257232666016\n",
      "cnt: 0 - valLoss: 0.5955076813697815 - trainLoss: 0.5997159481048584\n",
      "cnt: 0 - valLoss: 0.5955021977424622 - trainLoss: 0.5997084975242615\n",
      "cnt: 0 - valLoss: 0.5954957008361816 - trainLoss: 0.599699079990387\n",
      "cnt: 0 - valLoss: 0.5954856872558594 - trainLoss: 0.599689781665802\n",
      "cnt: 0 - valLoss: 0.5954801440238953 - trainLoss: 0.5996818542480469\n",
      "cnt: 0 - valLoss: 0.5954738855361938 - trainLoss: 0.5996723771095276\n",
      "cnt: 0 - valLoss: 0.5954636931419373 - trainLoss: 0.5996636152267456\n",
      "cnt: 0 - valLoss: 0.5954581499099731 - trainLoss: 0.599655270576477\n",
      "cnt: 0 - valLoss: 0.5954521298408508 - trainLoss: 0.599645733833313\n",
      "cnt: 0 - valLoss: 0.5954416394233704 - trainLoss: 0.5996373891830444\n",
      "cnt: 0 - valLoss: 0.5954360961914062 - trainLoss: 0.5996285676956177\n",
      "cnt: 0 - valLoss: 0.5954303741455078 - trainLoss: 0.5996190309524536\n",
      "cnt: 0 - valLoss: 0.5954196453094482 - trainLoss: 0.5996112823486328\n",
      "cnt: 0 - valLoss: 0.5954141616821289 - trainLoss: 0.5996018648147583\n",
      "cnt: 0 - valLoss: 0.5954043865203857 - trainLoss: 0.5995924472808838\n",
      "cnt: 0 - valLoss: 0.5953977108001709 - trainLoss: 0.5995849370956421\n",
      "cnt: 0 - valLoss: 0.5953925251960754 - trainLoss: 0.5995751619338989\n",
      "cnt: 0 - valLoss: 0.5953823924064636 - trainLoss: 0.5995663404464722\n",
      "cnt: 0 - valLoss: 0.595375657081604 - trainLoss: 0.5995581746101379\n",
      "cnt: 0 - valLoss: 0.5953708291053772 - trainLoss: 0.59954833984375\n",
      "cnt: 0 - valLoss: 0.5953604578971863 - trainLoss: 0.5995402336120605\n",
      "cnt: 0 - valLoss: 0.595353364944458 - trainLoss: 0.5995314121246338\n",
      "cnt: 0 - valLoss: 0.5953482985496521 - trainLoss: 0.5995218753814697\n",
      "cnt: 0 - valLoss: 0.5953380465507507 - trainLoss: 0.5995134115219116\n",
      "cnt: 0 - valLoss: 0.595331072807312 - trainLoss: 0.5995047688484192\n",
      "cnt: 0 - valLoss: 0.5953250527381897 - trainLoss: 0.5994954109191895\n",
      "cnt: 0 - valLoss: 0.5953193306922913 - trainLoss: 0.5994861721992493\n",
      "cnt: 0 - valLoss: 0.5953087210655212 - trainLoss: 0.599478542804718\n",
      "cnt: 0 - valLoss: 0.5953027009963989 - trainLoss: 0.5994687080383301\n",
      "cnt: 0 - valLoss: 0.5952960252761841 - trainLoss: 0.5994596481323242\n",
      "cnt: 0 - valLoss: 0.5952860116958618 - trainLoss: 0.5994510054588318\n",
      "cnt: 0 - valLoss: 0.5952799916267395 - trainLoss: 0.599442183971405\n",
      "cnt: 0 - valLoss: 0.5952728986740112 - trainLoss: 0.5994331240653992\n",
      "cnt: 0 - valLoss: 0.5952663421630859 - trainLoss: 0.5994236469268799\n",
      "cnt: 0 - valLoss: 0.5952564477920532 - trainLoss: 0.5994154810905457\n",
      "cnt: 0 - valLoss: 0.5952497124671936 - trainLoss: 0.5994068384170532\n",
      "cnt: 0 - valLoss: 0.5952426791191101 - trainLoss: 0.5993973016738892\n",
      "cnt: 0 - valLoss: 0.5952361822128296 - trainLoss: 0.5993878245353699\n",
      "cnt: 0 - valLoss: 0.5952269434928894 - trainLoss: 0.599379301071167\n",
      "cnt: 0 - valLoss: 0.5952197909355164 - trainLoss: 0.599371075630188\n",
      "cnt: 0 - valLoss: 0.5952126979827881 - trainLoss: 0.5993615388870239\n",
      "cnt: 0 - valLoss: 0.5952062606811523 - trainLoss: 0.5993520617485046\n",
      "cnt: 0 - valLoss: 0.5951970219612122 - trainLoss: 0.5993431806564331\n",
      "cnt: 0 - valLoss: 0.5951898694038391 - trainLoss: 0.5993351936340332\n",
      "cnt: 0 - valLoss: 0.5951828956604004 - trainLoss: 0.5993257761001587\n",
      "cnt: 0 - valLoss: 0.5951769351959229 - trainLoss: 0.5993163585662842\n",
      "cnt: 0 - valLoss: 0.5951668620109558 - trainLoss: 0.5993072986602783\n",
      "cnt: 0 - valLoss: 0.5951598882675171 - trainLoss: 0.5992993116378784\n",
      "cnt: 0 - valLoss: 0.5951538681983948 - trainLoss: 0.5992898941040039\n",
      "cnt: 0 - valLoss: 0.5951468348503113 - trainLoss: 0.5992807149887085\n",
      "cnt: 0 - valLoss: 0.595140278339386 - trainLoss: 0.5992712378501892\n",
      "cnt: 0 - valLoss: 0.5951305031776428 - trainLoss: 0.599263072013855\n",
      "cnt: 0 - valLoss: 0.595123827457428 - trainLoss: 0.5992544889450073\n",
      "cnt: 0 - valLoss: 0.5951166749000549 - trainLoss: 0.5992448329925537\n",
      "cnt: 0 - valLoss: 0.5951103568077087 - trainLoss: 0.5992352962493896\n",
      "cnt: 0 - valLoss: 0.595100998878479 - trainLoss: 0.5992269515991211\n",
      "cnt: 0 - valLoss: 0.5950939655303955 - trainLoss: 0.5992184281349182\n",
      "cnt: 0 - valLoss: 0.5950868725776672 - trainLoss: 0.5992089509963989\n",
      "cnt: 0 - valLoss: 0.595080554485321 - trainLoss: 0.5991994142532349\n",
      "cnt: 0 - valLoss: 0.5950711965560913 - trainLoss: 0.5991908311843872\n",
      "cnt: 0 - valLoss: 0.5950641632080078 - trainLoss: 0.5991825461387634\n",
      "cnt: 0 - valLoss: 0.5950570106506348 - trainLoss: 0.5991730690002441\n",
      "cnt: 0 - valLoss: 0.5950508713722229 - trainLoss: 0.5991635322570801\n",
      "cnt: 0 - valLoss: 0.5950414538383484 - trainLoss: 0.5991548299789429\n",
      "cnt: 0 - valLoss: 0.5950344204902649 - trainLoss: 0.5991466045379639\n",
      "cnt: 0 - valLoss: 0.5950273871421814 - trainLoss: 0.5991371273994446\n",
      "cnt: 0 - valLoss: 0.5950217247009277 - trainLoss: 0.5991275906562805\n",
      "cnt: 0 - valLoss: 0.5950114130973816 - trainLoss: 0.5991190671920776\n",
      "cnt: 0 - valLoss: 0.5950044989585876 - trainLoss: 0.5991105437278748\n",
      "cnt: 0 - valLoss: 0.5949985384941101 - trainLoss: 0.5991010665893555\n",
      "cnt: 0 - valLoss: 0.5949916243553162 - trainLoss: 0.5990918874740601\n",
      "cnt: 0 - valLoss: 0.5949814319610596 - trainLoss: 0.5990827679634094\n",
      "cnt: 0 - valLoss: 0.5949756503105164 - trainLoss: 0.5990745425224304\n",
      "cnt: 0 - valLoss: 0.5949685573577881 - trainLoss: 0.5990653038024902\n",
      "cnt: 0 - valLoss: 0.5949616432189941 - trainLoss: 0.5990557670593262\n",
      "cnt: 0 - valLoss: 0.5949521660804749 - trainLoss: 0.5990464687347412\n",
      "cnt: 0 - valLoss: 0.5949456691741943 - trainLoss: 0.59903883934021\n",
      "cnt: 0 - valLoss: 0.5949385762214661 - trainLoss: 0.5990292429924011\n",
      "cnt: 0 - valLoss: 0.5949317812919617 - trainLoss: 0.5990196466445923\n",
      "cnt: 0 - valLoss: 0.5949230194091797 - trainLoss: 0.5990104079246521\n",
      "cnt: 0 - valLoss: 0.5949159860610962 - trainLoss: 0.5990027785301208\n",
      "cnt: 0 - valLoss: 0.5949089527130127 - trainLoss: 0.598993182182312\n",
      "cnt: 0 - valLoss: 0.5949022769927979 - trainLoss: 0.5989835858345032\n",
      "cnt: 0 - valLoss: 0.5948933959007263 - trainLoss: 0.5989744663238525\n",
      "cnt: 0 - valLoss: 0.5948863625526428 - trainLoss: 0.598966658115387\n",
      "cnt: 0 - valLoss: 0.5948792099952698 - trainLoss: 0.5989571213722229\n",
      "cnt: 0 - valLoss: 0.5948726534843445 - trainLoss: 0.5989475250244141\n",
      "cnt: 0 - valLoss: 0.5948637127876282 - trainLoss: 0.598938524723053\n",
      "cnt: 0 - valLoss: 0.5948566794395447 - trainLoss: 0.5989305973052979\n",
      "cnt: 0 - valLoss: 0.5948496460914612 - trainLoss: 0.5989210605621338\n",
      "cnt: 0 - valLoss: 0.5948431491851807 - trainLoss: 0.5989114046096802\n",
      "cnt: 0 - valLoss: 0.5948340892791748 - trainLoss: 0.5989025235176086\n",
      "cnt: 0 - valLoss: 0.5948270559310913 - trainLoss: 0.598894476890564\n",
      "cnt: 0 - valLoss: 0.5948200225830078 - trainLoss: 0.5988849401473999\n",
      "cnt: 0 - valLoss: 0.5948136448860168 - trainLoss: 0.5988753437995911\n",
      "cnt: 0 - valLoss: 0.5948044657707214 - trainLoss: 0.5988665819168091\n",
      "cnt: 0 - valLoss: 0.5947975516319275 - trainLoss: 0.5988583564758301\n",
      "cnt: 0 - valLoss: 0.5947903990745544 - trainLoss: 0.5988487601280212\n",
      "cnt: 0 - valLoss: 0.5947842001914978 - trainLoss: 0.5988391041755676\n",
      "cnt: 0 - valLoss: 0.5947749614715576 - trainLoss: 0.5988306403160095\n",
      "cnt: 0 - valLoss: 0.5947679281234741 - trainLoss: 0.5988221764564514\n",
      "cnt: 0 - valLoss: 0.5947609543800354 - trainLoss: 0.5988125801086426\n",
      "cnt: 0 - valLoss: 0.5947548151016235 - trainLoss: 0.598802924156189\n",
      "cnt: 0 - valLoss: 0.594745397567749 - trainLoss: 0.59879469871521\n",
      "cnt: 0 - valLoss: 0.5947383642196655 - trainLoss: 0.5987859964370728\n",
      "cnt: 0 - valLoss: 0.5947313904762268 - trainLoss: 0.5987763404846191\n",
      "cnt: 0 - valLoss: 0.5947254300117493 - trainLoss: 0.5987667441368103\n",
      "cnt: 0 - valLoss: 0.5947158336639404 - trainLoss: 0.5987587571144104\n",
      "cnt: 0 - valLoss: 0.5947088599205017 - trainLoss: 0.5987497568130493\n",
      "cnt: 0 - valLoss: 0.5947018265724182 - trainLoss: 0.5987401604652405\n",
      "cnt: 0 - valLoss: 0.594696044921875 - trainLoss: 0.5987305045127869\n",
      "cnt: 0 - valLoss: 0.5946863293647766 - trainLoss: 0.5987228155136108\n",
      "cnt: 0 - valLoss: 0.5946793556213379 - trainLoss: 0.5987135171890259\n",
      "cnt: 0 - valLoss: 0.5946723222732544 - trainLoss: 0.5987038612365723\n",
      "cnt: 0 - valLoss: 0.5946635007858276 - trainLoss: 0.598694384098053\n",
      "cnt: 0 - valLoss: 0.5946565866470337 - trainLoss: 0.5986870527267456\n",
      "cnt: 0 - valLoss: 0.5946496725082397 - trainLoss: 0.5986771583557129\n",
      "cnt: 0 - valLoss: 0.594643235206604 - trainLoss: 0.5986675024032593\n",
      "cnt: 0 - valLoss: 0.5946341753005981 - trainLoss: 0.598658561706543\n",
      "cnt: 0 - valLoss: 0.5946271419525146 - trainLoss: 0.5986505150794983\n",
      "cnt: 0 - valLoss: 0.5946201086044312 - trainLoss: 0.5986407995223999\n",
      "cnt: 0 - valLoss: 0.5946139693260193 - trainLoss: 0.5986312031745911\n",
      "cnt: 0 - valLoss: 0.5946047306060791 - trainLoss: 0.598622739315033\n",
      "cnt: 0 - valLoss: 0.5945976972579956 - trainLoss: 0.5986141562461853\n",
      "cnt: 0 - valLoss: 0.5945906639099121 - trainLoss: 0.5986045002937317\n",
      "cnt: 0 - valLoss: 0.5945847034454346 - trainLoss: 0.5985947847366333\n",
      "cnt: 0 - valLoss: 0.5945752859115601 - trainLoss: 0.5985868573188782\n",
      "cnt: 0 - valLoss: 0.5945683121681213 - trainLoss: 0.5985778570175171\n",
      "cnt: 0 - valLoss: 0.5945612788200378 - trainLoss: 0.5985682010650635\n",
      "cnt: 0 - valLoss: 0.5945554375648499 - trainLoss: 0.5985584855079651\n",
      "cnt: 0 - valLoss: 0.594545841217041 - trainLoss: 0.5985509157180786\n",
      "cnt: 0 - valLoss: 0.5945388078689575 - trainLoss: 0.5985414981842041\n",
      "cnt: 0 - valLoss: 0.5945320725440979 - trainLoss: 0.5985317826271057\n",
      "cnt: 0 - valLoss: 0.5945228338241577 - trainLoss: 0.5985226035118103\n",
      "cnt: 0 - valLoss: 0.5945162773132324 - trainLoss: 0.5985147953033447\n",
      "cnt: 0 - valLoss: 0.5945092439651489 - trainLoss: 0.5985050201416016\n",
      "cnt: 0 - valLoss: 0.5945028066635132 - trainLoss: 0.5984952449798584\n",
      "cnt: 0 - valLoss: 0.5944938063621521 - trainLoss: 0.5984867215156555\n",
      "cnt: 0 - valLoss: 0.5944868922233582 - trainLoss: 0.5984781980514526\n",
      "cnt: 0 - valLoss: 0.5944798588752747 - trainLoss: 0.598468542098999\n",
      "cnt: 0 - valLoss: 0.594473659992218 - trainLoss: 0.5984587669372559\n",
      "cnt: 0 - valLoss: 0.5944644808769226 - trainLoss: 0.5984508395195007\n",
      "cnt: 0 - valLoss: 0.5944575071334839 - trainLoss: 0.5984417200088501\n",
      "cnt: 0 - valLoss: 0.5944507122039795 - trainLoss: 0.5984320044517517\n",
      "cnt: 0 - valLoss: 0.5944412350654602 - trainLoss: 0.5984227657318115\n",
      "cnt: 0 - valLoss: 0.594434916973114 - trainLoss: 0.5984148383140564\n",
      "cnt: 0 - valLoss: 0.5944279432296753 - trainLoss: 0.598405122756958\n",
      "cnt: 0 - valLoss: 0.5944214463233948 - trainLoss: 0.5983954071998596\n",
      "cnt: 0 - valLoss: 0.5944121479988098 - trainLoss: 0.5983868837356567\n",
      "cnt: 0 - valLoss: 0.5944054126739502 - trainLoss: 0.5983784794807434\n",
      "cnt: 0 - valLoss: 0.5943984389305115 - trainLoss: 0.5983685255050659\n",
      "cnt: 0 - valLoss: 0.5943924784660339 - trainLoss: 0.5983588099479675\n",
      "cnt: 0 - valLoss: 0.5943830609321594 - trainLoss: 0.5983511209487915\n",
      "cnt: 0 - valLoss: 0.5943760871887207 - trainLoss: 0.598341703414917\n",
      "cnt: 0 - valLoss: 0.5943694114685059 - trainLoss: 0.5983319282531738\n",
      "cnt: 0 - valLoss: 0.5943599939346313 - trainLoss: 0.5983229875564575\n",
      "cnt: 0 - valLoss: 0.5943535566329956 - trainLoss: 0.5983148217201233\n",
      "cnt: 0 - valLoss: 0.5943465828895569 - trainLoss: 0.5983050465583801\n",
      "cnt: 0 - valLoss: 0.5943403244018555 - trainLoss: 0.598295271396637\n",
      "cnt: 0 - valLoss: 0.5943309664726257 - trainLoss: 0.5982871055603027\n",
      "cnt: 0 - valLoss: 0.5943241119384766 - trainLoss: 0.5982783436775208\n",
      "cnt: 0 - valLoss: 0.5943173170089722 - trainLoss: 0.5982682704925537\n",
      "cnt: 0 - valLoss: 0.5943082571029663 - trainLoss: 0.5982590317726135\n",
      "cnt: 0 - valLoss: 0.5943015217781067 - trainLoss: 0.5982512831687927\n",
      "cnt: 0 - valLoss: 0.5942946672439575 - trainLoss: 0.5982413291931152\n",
      "cnt: 0 - valLoss: 0.5942884087562561 - trainLoss: 0.5982314944267273\n",
      "cnt: 0 - valLoss: 0.5942793488502502 - trainLoss: 0.5982232689857483\n",
      "cnt: 0 - valLoss: 0.5942723751068115 - trainLoss: 0.5982143878936768\n",
      "cnt: 0 - valLoss: 0.5942656397819519 - trainLoss: 0.5982046127319336\n",
      "cnt: 0 - valLoss: 0.5942562222480774 - trainLoss: 0.5981953144073486\n",
      "cnt: 0 - valLoss: 0.594249963760376 - trainLoss: 0.5981873869895935\n",
      "cnt: 0 - valLoss: 0.5942429900169373 - trainLoss: 0.5981775522232056\n",
      "cnt: 0 - valLoss: 0.5942366719245911 - trainLoss: 0.5981677770614624\n",
      "cnt: 0 - valLoss: 0.5942271947860718 - trainLoss: 0.5981594920158386\n",
      "cnt: 0 - valLoss: 0.5942205190658569 - trainLoss: 0.5981507897377014\n",
      "cnt: 0 - valLoss: 0.5942137837409973 - trainLoss: 0.5981407761573792\n",
      "cnt: 0 - valLoss: 0.5942045450210571 - trainLoss: 0.5981314778327942\n",
      "cnt: 0 - valLoss: 0.5941980481147766 - trainLoss: 0.5981236100196838\n",
      "cnt: 0 - valLoss: 0.5941911935806274 - trainLoss: 0.5981136560440063\n",
      "cnt: 0 - valLoss: 0.594184935092926 - trainLoss: 0.5981038808822632\n",
      "cnt: 0 - valLoss: 0.5941755175590515 - trainLoss: 0.598095715045929\n",
      "cnt: 0 - valLoss: 0.5941687822341919 - trainLoss: 0.5980868935585022\n",
      "cnt: 0 - valLoss: 0.594162106513977 - trainLoss: 0.5980768203735352\n",
      "cnt: 0 - valLoss: 0.5941529273986816 - trainLoss: 0.5980677008628845\n",
      "cnt: 0 - valLoss: 0.5941463708877563 - trainLoss: 0.5980595946311951\n",
      "cnt: 0 - valLoss: 0.5941394567489624 - trainLoss: 0.5980497002601624\n",
      "cnt: 0 - valLoss: 0.5941303372383118 - trainLoss: 0.5980398058891296\n",
      "cnt: 0 - valLoss: 0.5941240787506104 - trainLoss: 0.5980323553085327\n",
      "cnt: 0 - valLoss: 0.5941171646118164 - trainLoss: 0.5980224609375\n",
      "cnt: 0 - valLoss: 0.5941107869148254 - trainLoss: 0.5980126261711121\n",
      "cnt: 0 - valLoss: 0.5941013693809509 - trainLoss: 0.598004162311554\n",
      "cnt: 0 - valLoss: 0.5940948128700256 - trainLoss: 0.5979955196380615\n",
      "cnt: 0 - valLoss: 0.5940881967544556 - trainLoss: 0.5979854464530945\n",
      "cnt: 0 - valLoss: 0.5940788388252258 - trainLoss: 0.5979762673377991\n",
      "cnt: 0 - valLoss: 0.5940725207328796 - trainLoss: 0.5979681611061096\n",
      "cnt: 0 - valLoss: 0.5940657258033752 - trainLoss: 0.5979582667350769\n",
      "cnt: 0 - valLoss: 0.5940563082695007 - trainLoss: 0.5979484915733337\n",
      "cnt: 0 - valLoss: 0.5940502882003784 - trainLoss: 0.5979408621788025\n",
      "cnt: 0 - valLoss: 0.5940434336662292 - trainLoss: 0.5979310274124146\n",
      "cnt: 0 - valLoss: 0.5940373539924622 - trainLoss: 0.5979211330413818\n",
      "cnt: 0 - valLoss: 0.5940274596214294 - trainLoss: 0.5979130864143372\n",
      "cnt: 0 - valLoss: 0.5940211415290833 - trainLoss: 0.5979037880897522\n",
      "cnt: 0 - valLoss: 0.5940148234367371 - trainLoss: 0.5978938937187195\n",
      "cnt: 0 - valLoss: 0.5940049886703491 - trainLoss: 0.5978853106498718\n",
      "cnt: 0 - valLoss: 0.5939989686012268 - trainLoss: 0.5978764295578003\n",
      "cnt: 0 - valLoss: 0.5939923524856567 - trainLoss: 0.5978665947914124\n",
      "cnt: 0 - valLoss: 0.593982458114624 - trainLoss: 0.5978574752807617\n",
      "cnt: 0 - valLoss: 0.5939767360687256 - trainLoss: 0.5978490114212036\n",
      "cnt: 0 - valLoss: 0.593970000743866 - trainLoss: 0.59783935546875\n",
      "cnt: 0 - valLoss: 0.5939603447914124 - trainLoss: 0.5978298783302307\n",
      "cnt: 0 - valLoss: 0.5939536094665527 - trainLoss: 0.5978217124938965\n",
      "cnt: 0 - valLoss: 0.59394770860672 - trainLoss: 0.5978118181228638\n",
      "cnt: 0 - valLoss: 0.5939381718635559 - trainLoss: 0.5978022217750549\n",
      "cnt: 0 - valLoss: 0.5939311981201172 - trainLoss: 0.5977943539619446\n",
      "cnt: 0 - valLoss: 0.593924880027771 - trainLoss: 0.5977843403816223\n",
      "cnt: 0 - valLoss: 0.5939195156097412 - trainLoss: 0.5977746248245239\n",
      "cnt: 0 - valLoss: 0.5939090847969055 - trainLoss: 0.5977668762207031\n",
      "cnt: 0 - valLoss: 0.5939023494720459 - trainLoss: 0.5977569818496704\n",
      "cnt: 0 - valLoss: 0.5938971638679504 - trainLoss: 0.5977470874786377\n",
      "cnt: 0 - valLoss: 0.5938869714736938 - trainLoss: 0.5977392196655273\n",
      "cnt: 0 - valLoss: 0.5938800573348999 - trainLoss: 0.5977296233177185\n",
      "cnt: 0 - valLoss: 0.5938741564750671 - trainLoss: 0.5977196097373962\n",
      "cnt: 0 - valLoss: 0.5938650369644165 - trainLoss: 0.5977111458778381\n",
      "cnt: 0 - valLoss: 0.5938581824302673 - trainLoss: 0.5977023243904114\n",
      "cnt: 0 - valLoss: 0.5938517451286316 - trainLoss: 0.5976923108100891\n",
      "cnt: 0 - valLoss: 0.5938425064086914 - trainLoss: 0.5976833701133728\n",
      "cnt: 0 - valLoss: 0.5938360095024109 - trainLoss: 0.5976750254631042\n",
      "cnt: 0 - valLoss: 0.5938295722007751 - trainLoss: 0.5976648330688477\n",
      "cnt: 0 - valLoss: 0.5938200354576111 - trainLoss: 0.5976558327674866\n",
      "cnt: 0 - valLoss: 0.5938138961791992 - trainLoss: 0.5976473689079285\n",
      "cnt: 0 - valLoss: 0.5938073396682739 - trainLoss: 0.5976373553276062\n",
      "cnt: 0 - valLoss: 0.5937976241111755 - trainLoss: 0.5976281762123108\n",
      "cnt: 0 - valLoss: 0.5937918424606323 - trainLoss: 0.5976197123527527\n",
      "cnt: 0 - valLoss: 0.593785285949707 - trainLoss: 0.5976099371910095\n",
      "cnt: 0 - valLoss: 0.5937755703926086 - trainLoss: 0.5976005792617798\n",
      "cnt: 0 - valLoss: 0.593768835067749 - trainLoss: 0.5975922346115112\n",
      "cnt: 0 - valLoss: 0.5937631130218506 - trainLoss: 0.597582221031189\n",
      "cnt: 0 - valLoss: 0.593753457069397 - trainLoss: 0.5975730419158936\n",
      "cnt: 0 - valLoss: 0.593746542930603 - trainLoss: 0.5975646376609802\n",
      "cnt: 0 - valLoss: 0.5937401652336121 - trainLoss: 0.5975545644760132\n",
      "cnt: 0 - valLoss: 0.5937316417694092 - trainLoss: 0.5975450277328491\n",
      "cnt: 0 - valLoss: 0.5937247276306152 - trainLoss: 0.5975372195243835\n",
      "cnt: 0 - valLoss: 0.5937180519104004 - trainLoss: 0.5975270867347717\n",
      "cnt: 0 - valLoss: 0.5937090516090393 - trainLoss: 0.5975173711776733\n",
      "cnt: 0 - valLoss: 0.5937026739120483 - trainLoss: 0.5975096821784973\n",
      "cnt: 0 - valLoss: 0.5936959385871887 - trainLoss: 0.5974994897842407\n",
      "cnt: 0 - valLoss: 0.5936866402626038 - trainLoss: 0.5974897146224976\n",
      "cnt: 0 - valLoss: 0.5936806201934814 - trainLoss: 0.5974817872047424\n",
      "cnt: 0 - valLoss: 0.5936739444732666 - trainLoss: 0.5974718332290649\n",
      "cnt: 0 - valLoss: 0.5936643481254578 - trainLoss: 0.5974621772766113\n",
      "cnt: 0 - valLoss: 0.5936578512191772 - trainLoss: 0.5974540710449219\n",
      "cnt: 0 - valLoss: 0.5936519503593445 - trainLoss: 0.5974441170692444\n",
      "cnt: 0 - valLoss: 0.5936424136161804 - trainLoss: 0.5974346399307251\n",
      "cnt: 0 - valLoss: 0.5936355590820312 - trainLoss: 0.5974264740943909\n",
      "cnt: 0 - valLoss: 0.5936292409896851 - trainLoss: 0.5974162817001343\n",
      "cnt: 0 - valLoss: 0.5936206579208374 - trainLoss: 0.597406804561615\n",
      "cnt: 0 - valLoss: 0.5936137437820435 - trainLoss: 0.5973988175392151\n",
      "cnt: 0 - valLoss: 0.5936071276664734 - trainLoss: 0.5973886251449585\n",
      "cnt: 0 - valLoss: 0.5935981273651123 - trainLoss: 0.5973790884017944\n",
      "cnt: 0 - valLoss: 0.5935917496681213 - trainLoss: 0.5973712801933289\n",
      "cnt: 0 - valLoss: 0.5935851335525513 - trainLoss: 0.5973610281944275\n",
      "cnt: 0 - valLoss: 0.5935757160186768 - trainLoss: 0.5973514914512634\n",
      "cnt: 0 - valLoss: 0.5935697555541992 - trainLoss: 0.5973432660102844\n",
      "cnt: 0 - valLoss: 0.5935633182525635 - trainLoss: 0.5973332524299622\n",
      "cnt: 0 - valLoss: 0.5935536026954651 - trainLoss: 0.5973239541053772\n",
      "cnt: 0 - valLoss: 0.593546986579895 - trainLoss: 0.5973154902458191\n",
      "cnt: 0 - valLoss: 0.5935414433479309 - trainLoss: 0.5973054766654968\n",
      "cnt: 0 - valLoss: 0.593531608581543 - trainLoss: 0.5972964763641357\n",
      "cnt: 0 - valLoss: 0.593524694442749 - trainLoss: 0.5972877144813538\n",
      "cnt: 0 - valLoss: 0.5935187339782715 - trainLoss: 0.5972774624824524\n",
      "cnt: 0 - valLoss: 0.5935099124908447 - trainLoss: 0.5972686409950256\n",
      "cnt: 0 - valLoss: 0.5935030579566956 - trainLoss: 0.5972599983215332\n",
      "cnt: 0 - valLoss: 0.5934967398643494 - trainLoss: 0.5972497463226318\n",
      "cnt: 0 - valLoss: 0.5934873819351196 - trainLoss: 0.5972409844398499\n",
      "cnt: 0 - valLoss: 0.5934810638427734 - trainLoss: 0.5972322821617126\n",
      "cnt: 0 - valLoss: 0.5934749841690063 - trainLoss: 0.5972219705581665\n",
      "cnt: 0 - valLoss: 0.5934650301933289 - trainLoss: 0.5972134470939636\n",
      "cnt: 0 - valLoss: 0.5934591293334961 - trainLoss: 0.5972041487693787\n",
      "cnt: 0 - valLoss: 0.5934531688690186 - trainLoss: 0.5971940755844116\n",
      "cnt: 0 - valLoss: 0.5934430360794067 - trainLoss: 0.5971859693527222\n",
      "cnt: 0 - valLoss: 0.5934362411499023 - trainLoss: 0.5971762537956238\n",
      "cnt: 0 - valLoss: 0.5934313535690308 - trainLoss: 0.5971660614013672\n",
      "cnt: 0 - valLoss: 0.5934211015701294 - trainLoss: 0.5971584916114807\n",
      "cnt: 0 - valLoss: 0.5934141874313354 - trainLoss: 0.5971484184265137\n",
      "cnt: 0 - valLoss: 0.5934086441993713 - trainLoss: 0.5971381068229675\n",
      "cnt: 0 - valLoss: 0.5933994054794312 - trainLoss: 0.5971304774284363\n",
      "cnt: 0 - valLoss: 0.5933926105499268 - trainLoss: 0.5971205830574036\n",
      "cnt: 0 - valLoss: 0.5933825373649597 - trainLoss: 0.5971105694770813\n",
      "cnt: 0 - valLoss: 0.5933773517608643 - trainLoss: 0.5971029996871948\n",
      "cnt: 0 - valLoss: 0.5933681726455688 - trainLoss: 0.5970918536186218\n",
      "cnt: 0 - valLoss: 0.5933631658554077 - trainLoss: 0.5970849990844727\n",
      "cnt: 0 - valLoss: 0.5933522582054138 - trainLoss: 0.5970736742019653\n",
      "cnt: 0 - valLoss: 0.5933477282524109 - trainLoss: 0.5970666408538818\n",
      "cnt: 0 - valLoss: 0.5933374166488647 - trainLoss: 0.5970549583435059\n",
      "cnt: 0 - valLoss: 0.5933334827423096 - trainLoss: 0.597048819065094\n",
      "cnt: 0 - valLoss: 0.5933218002319336 - trainLoss: 0.5970367193222046\n",
      "cnt: 0 - valLoss: 0.5933191776275635 - trainLoss: 0.5970300436019897\n",
      "cnt: 0 - valLoss: 0.5933071374893188 - trainLoss: 0.5970184803009033\n",
      "cnt: 0 - valLoss: 0.5933037400245667 - trainLoss: 0.5970118045806885\n",
      "cnt: 0 - valLoss: 0.5932936668395996 - trainLoss: 0.5969997048377991\n",
      "cnt: 0 - valLoss: 0.5932912826538086 - trainLoss: 0.5969921946525574\n",
      "cnt: 0 - valLoss: 0.593278706073761 - trainLoss: 0.5969830751419067\n",
      "cnt: 0 - valLoss: 0.5932758450508118 - trainLoss: 0.596973717212677\n",
      "cnt: 0 - valLoss: 0.5932636857032776 - trainLoss: 0.5969642996788025\n",
      "cnt: 0 - valLoss: 0.59326171875 - trainLoss: 0.5969556570053101\n",
      "cnt: 0 - valLoss: 0.593248188495636 - trainLoss: 0.5969460606575012\n",
      "cnt: 0 - valLoss: 0.5932462811470032 - trainLoss: 0.5969369411468506\n",
      "cnt: 0 - valLoss: 0.5932360887527466 - trainLoss: 0.5969275236129761\n",
      "cnt: 0 - valLoss: 0.5932256579399109 - trainLoss: 0.5969178080558777\n",
      "cnt: 0 - valLoss: 0.5932233929634094 - trainLoss: 0.5969098806381226\n",
      "cnt: 0 - valLoss: 0.5932122468948364 - trainLoss: 0.5969001054763794\n",
      "cnt: 0 - valLoss: 0.593211829662323 - trainLoss: 0.5968902111053467\n",
      "cnt: 0 - valLoss: 0.5931973457336426 - trainLoss: 0.5968830585479736\n",
      "cnt: 0 - valLoss: 0.5931873321533203 - trainLoss: 0.5968718528747559\n",
      "cnt: 0 - valLoss: 0.5931859612464905 - trainLoss: 0.5968644022941589\n",
      "cnt: 0 - valLoss: 0.5931744575500488 - trainLoss: 0.5968546867370605\n",
      "cnt: 0 - valLoss: 0.5931733250617981 - trainLoss: 0.596844494342804\n",
      "cnt: 0 - valLoss: 0.5931622982025146 - trainLoss: 0.596837043762207\n",
      "cnt: 0 - valLoss: 0.5931519269943237 - trainLoss: 0.5968272089958191\n",
      "cnt: 0 - valLoss: 0.5931507349014282 - trainLoss: 0.5968173146247864\n",
      "cnt: 0 - valLoss: 0.593139111995697 - trainLoss: 0.5968098044395447\n",
      "cnt: 0 - valLoss: 0.5931285619735718 - trainLoss: 0.5967995524406433\n",
      "cnt: 0 - valLoss: 0.5931278467178345 - trainLoss: 0.5967903137207031\n",
      "cnt: 0 - valLoss: 0.5931162238121033 - trainLoss: 0.596782386302948\n",
      "cnt: 0 - valLoss: 0.5931055545806885 - trainLoss: 0.5967721343040466\n",
      "cnt: 0 - valLoss: 0.5931050181388855 - trainLoss: 0.5967629551887512\n",
      "cnt: 0 - valLoss: 0.5930935144424438 - trainLoss: 0.5967550277709961\n",
      "cnt: 0 - valLoss: 0.5930826663970947 - trainLoss: 0.5967447757720947\n",
      "cnt: 0 - valLoss: 0.5930821895599365 - trainLoss: 0.5967355966567993\n",
      "cnt: 0 - valLoss: 0.5930707454681396 - trainLoss: 0.5967276096343994\n",
      "cnt: 0 - valLoss: 0.593059778213501 - trainLoss: 0.5967174172401428\n",
      "cnt: 0 - valLoss: 0.5930585265159607 - trainLoss: 0.5967081189155579\n",
      "cnt: 0 - valLoss: 0.5930483341217041 - trainLoss: 0.5966999530792236\n",
      "cnt: 0 - valLoss: 0.5930373668670654 - trainLoss: 0.5966901183128357\n",
      "cnt: 0 - valLoss: 0.5930358171463013 - trainLoss: 0.5966808199882507\n",
      "cnt: 0 - valLoss: 0.5930248498916626 - trainLoss: 0.5966724753379822\n",
      "cnt: 0 - valLoss: 0.5930149555206299 - trainLoss: 0.5966625809669495\n",
      "cnt: 0 - valLoss: 0.5930135250091553 - trainLoss: 0.5966535210609436\n",
      "cnt: 0 - valLoss: 0.5930020809173584 - trainLoss: 0.596645176410675\n",
      "cnt: 0 - valLoss: 0.5929917693138123 - trainLoss: 0.5966349840164185\n",
      "cnt: 0 - valLoss: 0.5929909348487854 - trainLoss: 0.5966262221336365\n",
      "cnt: 0 - valLoss: 0.5929795503616333 - trainLoss: 0.5966178178787231\n",
      "cnt: 0 - valLoss: 0.5929688811302185 - trainLoss: 0.5966076254844666\n",
      "cnt: 0 - valLoss: 0.5929684638977051 - trainLoss: 0.5965986251831055\n",
      "cnt: 0 - valLoss: 0.592957079410553 - trainLoss: 0.5965904593467712\n",
      "cnt: 0 - valLoss: 0.5929461121559143 - trainLoss: 0.5965802073478699\n",
      "cnt: 0 - valLoss: 0.59294593334198 - trainLoss: 0.5965710282325745\n",
      "cnt: 0 - valLoss: 0.5929344892501831 - trainLoss: 0.5965630412101746\n",
      "cnt: 0 - valLoss: 0.5929235219955444 - trainLoss: 0.596552848815918\n",
      "cnt: 0 - valLoss: 0.5929222702980042 - trainLoss: 0.596543550491333\n",
      "cnt: 0 - valLoss: 0.5929123759269714 - trainLoss: 0.5965352654457092\n",
      "cnt: 0 - valLoss: 0.592901349067688 - trainLoss: 0.5965256094932556\n",
      "cnt: 0 - valLoss: 0.5929000377655029 - trainLoss: 0.5965160131454468\n",
      "cnt: 0 - valLoss: 0.5928888320922852 - trainLoss: 0.5965079069137573\n",
      "cnt: 0 - valLoss: 0.5928791165351868 - trainLoss: 0.5964978337287903\n",
      "cnt: 0 - valLoss: 0.592877984046936 - trainLoss: 0.5964884757995605\n",
      "cnt: 0 - valLoss: 0.5928665995597839 - trainLoss: 0.596480667591095\n",
      "cnt: 0 - valLoss: 0.5928557515144348 - trainLoss: 0.5964704751968384\n",
      "cnt: 0 - valLoss: 0.5928556323051453 - trainLoss: 0.5964608788490295\n",
      "cnt: 0 - valLoss: 0.5928443074226379 - trainLoss: 0.5964532494544983\n",
      "cnt: 0 - valLoss: 0.5928331017494202 - trainLoss: 0.5964430570602417\n",
      "cnt: 0 - valLoss: 0.5928323864936829 - trainLoss: 0.5964330434799194\n",
      "cnt: 0 - valLoss: 0.5928223133087158 - trainLoss: 0.5964255928993225\n",
      "cnt: 0 - valLoss: 0.5928110480308533 - trainLoss: 0.5964158177375793\n",
      "cnt: 0 - valLoss: 0.5928009152412415 - trainLoss: 0.596405565738678\n",
      "cnt: 0 - valLoss: 0.5927992463111877 - trainLoss: 0.5963979959487915\n",
      "cnt: 0 - valLoss: 0.5927891731262207 - trainLoss: 0.5963882207870483\n",
      "cnt: 0 - valLoss: 0.5927788615226746 - trainLoss: 0.5963783860206604\n",
      "cnt: 0 - valLoss: 0.5927770137786865 - trainLoss: 0.596370279788971\n",
      "cnt: 0 - valLoss: 0.5927661657333374 - trainLoss: 0.5963607430458069\n",
      "cnt: 0 - valLoss: 0.5927568674087524 - trainLoss: 0.5963507294654846\n",
      "cnt: 0 - valLoss: 0.5927552580833435 - trainLoss: 0.5963425636291504\n",
      "cnt: 0 - valLoss: 0.5927440524101257 - trainLoss: 0.5963334441184998\n",
      "cnt: 0 - valLoss: 0.5927338004112244 - trainLoss: 0.5963233113288879\n",
      "cnt: 0 - valLoss: 0.5927332043647766 - trainLoss: 0.5963147878646851\n",
      "cnt: 0 - valLoss: 0.5927219390869141 - trainLoss: 0.5963060855865479\n",
      "cnt: 0 - valLoss: 0.5927112698554993 - trainLoss: 0.5962958335876465\n",
      "cnt: 0 - valLoss: 0.5927112102508545 - trainLoss: 0.5962867140769958\n",
      "cnt: 0 - valLoss: 0.5927000045776367 - trainLoss: 0.596278727054596\n",
      "cnt: 0 - valLoss: 0.5926889777183533 - trainLoss: 0.5962685346603394\n",
      "cnt: 0 - valLoss: 0.5926880836486816 - trainLoss: 0.596258819103241\n",
      "cnt: 0 - valLoss: 0.5926783680915833 - trainLoss: 0.5962508320808411\n",
      "cnt: 0 - valLoss: 0.5926671624183655 - trainLoss: 0.5962412357330322\n",
      "cnt: 0 - valLoss: 0.5926570892333984 - trainLoss: 0.5962311029434204\n",
      "cnt: 0 - valLoss: 0.5926553010940552 - trainLoss: 0.5962233543395996\n",
      "cnt: 0 - valLoss: 0.5926456451416016 - trainLoss: 0.5962134003639221\n",
      "cnt: 0 - valLoss: 0.5926353335380554 - trainLoss: 0.5962038040161133\n",
      "cnt: 0 - valLoss: 0.5926337838172913 - trainLoss: 0.5961954593658447\n",
      "cnt: 0 - valLoss: 0.5926226377487183 - trainLoss: 0.5961861610412598\n",
      "cnt: 0 - valLoss: 0.5926129817962646 - trainLoss: 0.5961759686470032\n",
      "cnt: 0 - valLoss: 0.5926120281219482 - trainLoss: 0.5961676239967346\n",
      "cnt: 0 - valLoss: 0.5926008820533752 - trainLoss: 0.5961587429046631\n",
      "cnt: 0 - valLoss: 0.5925905704498291 - trainLoss: 0.5961486101150513\n",
      "cnt: 0 - valLoss: 0.59259033203125 - trainLoss: 0.5961393713951111\n",
      "cnt: 0 - valLoss: 0.5925791263580322 - trainLoss: 0.5961313843727112\n",
      "cnt: 0 - valLoss: 0.5925681591033936 - trainLoss: 0.5961211919784546\n",
      "cnt: 0 - valLoss: 0.5925594568252563 - trainLoss: 0.5961111783981323\n",
      "cnt: 0 - valLoss: 0.5925578474998474 - trainLoss: 0.5961036086082458\n",
      "cnt: 0 - valLoss: 0.5925467014312744 - trainLoss: 0.5960939526557922\n",
      "cnt: 0 - valLoss: 0.592536449432373 - trainLoss: 0.5960838198661804\n",
      "cnt: 0 - valLoss: 0.592536211013794 - trainLoss: 0.5960753560066223\n",
      "cnt: 0 - valLoss: 0.5925251245498657 - trainLoss: 0.5960665345191956\n",
      "cnt: 0 - valLoss: 0.5925144553184509 - trainLoss: 0.5960564017295837\n",
      "cnt: 0 - valLoss: 0.5925133228302002 - trainLoss: 0.5960471034049988\n",
      "cnt: 0 - valLoss: 0.5925031900405884 - trainLoss: 0.5960386991500854\n",
      "cnt: 0 - valLoss: 0.5924928188323975 - trainLoss: 0.5960291028022766\n",
      "cnt: 0 - valLoss: 0.5924829244613647 - trainLoss: 0.5960190296173096\n",
      "cnt: 0 - valLoss: 0.5924811363220215 - trainLoss: 0.5960112810134888\n",
      "cnt: 0 - valLoss: 0.5924709439277649 - trainLoss: 0.5960012674331665\n",
      "cnt: 0 - valLoss: 0.5924612879753113 - trainLoss: 0.5959916710853577\n",
      "cnt: 0 - valLoss: 0.5924599766731262 - trainLoss: 0.5959830284118652\n",
      "cnt: 0 - valLoss: 0.5924489498138428 - trainLoss: 0.5959740281105042\n",
      "cnt: 0 - valLoss: 0.5924389958381653 - trainLoss: 0.5959638953208923\n",
      "cnt: 0 - valLoss: 0.5924385786056519 - trainLoss: 0.595954954624176\n",
      "cnt: 0 - valLoss: 0.5924274325370789 - trainLoss: 0.5959466099739075\n",
      "cnt: 0 - valLoss: 0.5924167633056641 - trainLoss: 0.5959364771842957\n",
      "cnt: 0 - valLoss: 0.5924080014228821 - trainLoss: 0.5959265232086182\n",
      "cnt: 0 - valLoss: 0.5924064517021179 - trainLoss: 0.5959188342094421\n",
      "cnt: 0 - valLoss: 0.5923954844474792 - trainLoss: 0.5959092378616333\n",
      "cnt: 0 - valLoss: 0.5923851728439331 - trainLoss: 0.5958990454673767\n",
      "cnt: 0 - valLoss: 0.5923851728439331 - trainLoss: 0.5958903431892395\n",
      "cnt: 0 - valLoss: 0.5923741459846497 - trainLoss: 0.5958817601203918\n",
      "cnt: 0 - valLoss: 0.5923633575439453 - trainLoss: 0.5958716869354248\n",
      "cnt: 0 - valLoss: 0.5923625826835632 - trainLoss: 0.5958618521690369\n",
      "cnt: 0 - valLoss: 0.592353105545044 - trainLoss: 0.5958539247512817\n",
      "cnt: 0 - valLoss: 0.5923421382904053 - trainLoss: 0.5958443880081177\n",
      "cnt: 0 - valLoss: 0.5923319458961487 - trainLoss: 0.5958342552185059\n",
      "cnt: 0 - valLoss: 0.5923306941986084 - trainLoss: 0.595825731754303\n",
      "cnt: 0 - valLoss: 0.5923205018043518 - trainLoss: 0.5958164930343628\n",
      "cnt: 0 - valLoss: 0.5923105478286743 - trainLoss: 0.595806896686554\n",
      "cnt: 0 - valLoss: 0.5923097729682922 - trainLoss: 0.5957973003387451\n",
      "cnt: 0 - valLoss: 0.5922988057136536 - trainLoss: 0.5957891345024109\n",
      "cnt: 0 - valLoss: 0.5922885537147522 - trainLoss: 0.5957791209220886\n",
      "cnt: 0 - valLoss: 0.592279314994812 - trainLoss: 0.5957694053649902\n",
      "cnt: 0 - valLoss: 0.5922780632972717 - trainLoss: 0.5957610607147217\n",
      "cnt: 0 - valLoss: 0.5922671556472778 - trainLoss: 0.5957517623901367\n",
      "cnt: 0 - valLoss: 0.592257022857666 - trainLoss: 0.5957416296005249\n",
      "cnt: 0 - valLoss: 0.592257022857666 - trainLoss: 0.5957325100898743\n",
      "cnt: 0 - valLoss: 0.5922461748123169 - trainLoss: 0.59572434425354\n",
      "cnt: 0 - valLoss: 0.592235267162323 - trainLoss: 0.595714271068573\n",
      "cnt: 0 - valLoss: 0.5922260880470276 - trainLoss: 0.5957041382789612\n",
      "cnt: 0 - valLoss: 0.5922253131866455 - trainLoss: 0.5956963300704956\n",
      "cnt: 0 - valLoss: 0.5922144055366516 - trainLoss: 0.5956868529319763\n",
      "cnt: 0 - valLoss: 0.5922040939331055 - trainLoss: 0.5956767201423645\n",
      "cnt: 0 - valLoss: 0.5922043323516846 - trainLoss: 0.5956672430038452\n",
      "cnt: 1 - valLoss: 0.5921935439109802 - trainLoss: 0.5956594347953796\n",
      "cnt: 0 - valLoss: 0.5921826958656311 - trainLoss: 0.5956493020057678\n",
      "cnt: 0 - valLoss: 0.5921733975410461 - trainLoss: 0.5956392884254456\n",
      "cnt: 0 - valLoss: 0.5921730995178223 - trainLoss: 0.5956310629844666\n",
      "cnt: 0 - valLoss: 0.5921623706817627 - trainLoss: 0.595622181892395\n",
      "cnt: 0 - valLoss: 0.592151939868927 - trainLoss: 0.5956122279167175\n",
      "cnt: 0 - valLoss: 0.5921432971954346 - trainLoss: 0.5956024527549744\n",
      "cnt: 0 - valLoss: 0.5921424031257629 - trainLoss: 0.5955944061279297\n",
      "cnt: 0 - valLoss: 0.5921317934989929 - trainLoss: 0.5955852270126343\n",
      "cnt: 0 - valLoss: 0.5921211838722229 - trainLoss: 0.5955753326416016\n",
      "cnt: 0 - valLoss: 0.5921222567558289 - trainLoss: 0.5955653786659241\n",
      "cnt: 1 - valLoss: 0.5921115875244141 - trainLoss: 0.5955581665039062\n",
      "cnt: 0 - valLoss: 0.5921009182929993 - trainLoss: 0.5955482125282288\n",
      "cnt: 0 - valLoss: 0.592090904712677 - trainLoss: 0.5955382585525513\n",
      "cnt: 0 - valLoss: 0.59209144115448 - trainLoss: 0.5955290794372559\n",
      "cnt: 1 - valLoss: 0.5920807719230652 - trainLoss: 0.5955211520195007\n",
      "cnt: 0 - valLoss: 0.5920701622962952 - trainLoss: 0.5955111384391785\n",
      "cnt: 0 - valLoss: 0.5920606851577759 - trainLoss: 0.5955012440681458\n",
      "cnt: 0 - valLoss: 0.5920607447624207 - trainLoss: 0.5954926013946533\n",
      "cnt: 1 - valLoss: 0.5920500755310059 - trainLoss: 0.5954840779304504\n",
      "cnt: 0 - valLoss: 0.5920397043228149 - trainLoss: 0.595474123954773\n",
      "cnt: 0 - valLoss: 0.5920308828353882 - trainLoss: 0.595464289188385\n",
      "cnt: 0 - valLoss: 0.59203040599823 - trainLoss: 0.5954557657241821\n",
      "cnt: 0 - valLoss: 0.5920198559761047 - trainLoss: 0.5954470634460449\n",
      "cnt: 0 - valLoss: 0.5920092463493347 - trainLoss: 0.5954371690750122\n",
      "cnt: 0 - valLoss: 0.5920001268386841 - trainLoss: 0.5954272747039795\n",
      "cnt: 0 - valLoss: 0.591999888420105 - trainLoss: 0.5954192876815796\n",
      "cnt: 0 - valLoss: 0.5919893383979797 - trainLoss: 0.5954101085662842\n",
      "cnt: 0 - valLoss: 0.5919790267944336 - trainLoss: 0.5954001545906067\n",
      "cnt: 0 - valLoss: 0.5919704437255859 - trainLoss: 0.5953903198242188\n",
      "cnt: 0 - valLoss: 0.5919697880744934 - trainLoss: 0.5953822135925293\n",
      "cnt: 0 - valLoss: 0.5919592380523682 - trainLoss: 0.5953731536865234\n",
      "cnt: 0 - valLoss: 0.5919486880302429 - trainLoss: 0.595363199710846\n",
      "cnt: 0 - valLoss: 0.5919397473335266 - trainLoss: 0.5953532457351685\n",
      "cnt: 0 - valLoss: 0.5919394493103027 - trainLoss: 0.5953455567359924\n",
      "cnt: 0 - valLoss: 0.5919288992881775 - trainLoss: 0.5953360795974731\n",
      "cnt: 0 - valLoss: 0.5919185876846313 - trainLoss: 0.5953261256217957\n",
      "cnt: 0 - valLoss: 0.5919101238250732 - trainLoss: 0.5953163504600525\n",
      "cnt: 0 - valLoss: 0.5919094085693359 - trainLoss: 0.5953083038330078\n",
      "cnt: 0 - valLoss: 0.5918989181518555 - trainLoss: 0.5952991843223572\n",
      "cnt: 0 - valLoss: 0.5918884873390198 - trainLoss: 0.5952892303466797\n",
      "cnt: 0 - valLoss: 0.5918794870376587 - trainLoss: 0.5952792763710022\n",
      "cnt: 0 - valLoss: 0.5918793082237244 - trainLoss: 0.5952715277671814\n",
      "cnt: 0 - valLoss: 0.5918688178062439 - trainLoss: 0.5952621102333069\n",
      "cnt: 0 - valLoss: 0.591858446598053 - trainLoss: 0.5952521562576294\n",
      "cnt: 0 - valLoss: 0.5918501019477844 - trainLoss: 0.5952423214912415\n",
      "cnt: 0 - valLoss: 0.5918495655059814 - trainLoss: 0.5952342748641968\n",
      "cnt: 0 - valLoss: 0.5918391346931458 - trainLoss: 0.5952251553535461\n",
      "cnt: 0 - valLoss: 0.5918287038803101 - trainLoss: 0.5952152609825134\n",
      "cnt: 0 - valLoss: 0.5918195843696594 - trainLoss: 0.5952053070068359\n",
      "cnt: 0 - valLoss: 0.5918197631835938 - trainLoss: 0.5951972007751465\n",
      "cnt: 1 - valLoss: 0.5918092727661133 - trainLoss: 0.5951881408691406\n",
      "cnt: 0 - valLoss: 0.5917989611625671 - trainLoss: 0.5951781868934631\n",
      "cnt: 0 - valLoss: 0.5917904376983643 - trainLoss: 0.5951682925224304\n",
      "cnt: 0 - valLoss: 0.5917903780937195 - trainLoss: 0.5951597094535828\n",
      "cnt: 0 - valLoss: 0.591779887676239 - trainLoss: 0.5951511859893799\n",
      "cnt: 0 - valLoss: 0.5917696356773376 - trainLoss: 0.5951412916183472\n",
      "cnt: 0 - valLoss: 0.5917601585388184 - trainLoss: 0.5951313972473145\n",
      "cnt: 0 - valLoss: 0.5917608737945557 - trainLoss: 0.5951223969459534\n",
      "cnt: 1 - valLoss: 0.59175044298172 - trainLoss: 0.5951142311096191\n",
      "cnt: 0 - valLoss: 0.5917401909828186 - trainLoss: 0.5951043367385864\n",
      "cnt: 0 - valLoss: 0.5917307734489441 - trainLoss: 0.5950945019721985\n",
      "cnt: 0 - valLoss: 0.5917220115661621 - trainLoss: 0.595085084438324\n",
      "cnt: 0 - valLoss: 0.5917215347290039 - trainLoss: 0.5950767993927002\n",
      "cnt: 0 - valLoss: 0.5917113423347473 - trainLoss: 0.5950675010681152\n",
      "cnt: 0 - valLoss: 0.5917015671730042 - trainLoss: 0.5950576663017273\n",
      "cnt: 0 - valLoss: 0.5916928052902222 - trainLoss: 0.5950480699539185\n",
      "cnt: 0 - valLoss: 0.5916927456855774 - trainLoss: 0.5950391292572021\n",
      "cnt: 0 - valLoss: 0.591682493686676 - trainLoss: 0.5950307250022888\n",
      "cnt: 0 - valLoss: 0.5916723012924194 - trainLoss: 0.5950208306312561\n",
      "cnt: 0 - valLoss: 0.5916630625724792 - trainLoss: 0.5950109958648682\n",
      "cnt: 0 - valLoss: 0.5916543006896973 - trainLoss: 0.5950016379356384\n",
      "cnt: 0 - valLoss: 0.5916538238525391 - trainLoss: 0.5949934124946594\n",
      "cnt: 0 - valLoss: 0.5916436910629272 - trainLoss: 0.5949839949607849\n",
      "cnt: 0 - valLoss: 0.5916339755058289 - trainLoss: 0.5949741005897522\n",
      "cnt: 0 - valLoss: 0.5916251540184021 - trainLoss: 0.5949645638465881\n",
      "cnt: 0 - valLoss: 0.5916252136230469 - trainLoss: 0.5949555039405823\n",
      "cnt: 1 - valLoss: 0.5916150808334351 - trainLoss: 0.594947099685669\n",
      "cnt: 0 - valLoss: 0.5916049480438232 - trainLoss: 0.594937264919281\n",
      "cnt: 0 - valLoss: 0.5915963053703308 - trainLoss: 0.5949274301528931\n",
      "cnt: 0 - valLoss: 0.5915868282318115 - trainLoss: 0.5949180722236633\n",
      "cnt: 0 - valLoss: 0.5915865302085876 - trainLoss: 0.59490966796875\n",
      "cnt: 0 - valLoss: 0.5915763974189758 - trainLoss: 0.5949004292488098\n",
      "cnt: 0 - valLoss: 0.5915666818618774 - trainLoss: 0.5948905944824219\n",
      "cnt: 0 - valLoss: 0.5915578007698059 - trainLoss: 0.594880998134613\n",
      "cnt: 0 - valLoss: 0.5915579795837402 - trainLoss: 0.5948716402053833\n",
      "cnt: 1 - valLoss: 0.5915478467941284 - trainLoss: 0.5948635339736938\n",
      "cnt: 0 - valLoss: 0.5915377140045166 - trainLoss: 0.5948536992073059\n",
      "cnt: 0 - valLoss: 0.5915284752845764 - trainLoss: 0.594843864440918\n",
      "cnt: 0 - valLoss: 0.5915195345878601 - trainLoss: 0.5948345065116882\n",
      "cnt: 0 - valLoss: 0.5915194749832153 - trainLoss: 0.5948256254196167\n",
      "cnt: 0 - valLoss: 0.5915093421936035 - trainLoss: 0.5948168039321899\n",
      "cnt: 0 - valLoss: 0.5914996862411499 - trainLoss: 0.594806969165802\n",
      "cnt: 0 - valLoss: 0.5914908051490784 - trainLoss: 0.5947973728179932\n",
      "cnt: 0 - valLoss: 0.5914815068244934 - trainLoss: 0.5947878360748291\n",
      "cnt: 0 - valLoss: 0.5914811491966248 - trainLoss: 0.5947794914245605\n",
      "cnt: 0 - valLoss: 0.5914710164070129 - trainLoss: 0.594770073890686\n",
      "cnt: 0 - valLoss: 0.5914616584777832 - trainLoss: 0.5947601795196533\n",
      "cnt: 0 - valLoss: 0.591452419757843 - trainLoss: 0.5947507619857788\n",
      "cnt: 0 - valLoss: 0.5914527773857117 - trainLoss: 0.5947412252426147\n",
      "cnt: 1 - valLoss: 0.5914426445960999 - trainLoss: 0.5947331786155701\n",
      "cnt: 0 - valLoss: 0.5914327502250671 - trainLoss: 0.5947232842445374\n",
      "cnt: 0 - valLoss: 0.5914240479469299 - trainLoss: 0.594713568687439\n",
      "cnt: 0 - valLoss: 0.5914143919944763 - trainLoss: 0.5947041511535645\n",
      "cnt: 0 - valLoss: 0.5914143919944763 - trainLoss: 0.5946950316429138\n",
      "cnt: 0 - valLoss: 0.5914042592048645 - trainLoss: 0.5946863889694214\n",
      "cnt: 0 - valLoss: 0.5913947224617004 - trainLoss: 0.5946765542030334\n",
      "cnt: 0 - valLoss: 0.5913857221603394 - trainLoss: 0.5946670770645142\n",
      "cnt: 0 - valLoss: 0.5913763046264648 - trainLoss: 0.5946574211120605\n",
      "cnt: 0 - valLoss: 0.5913761258125305 - trainLoss: 0.5946488976478577\n",
      "cnt: 0 - valLoss: 0.5913660526275635 - trainLoss: 0.5946397185325623\n",
      "cnt: 0 - valLoss: 0.5913568139076233 - trainLoss: 0.5946299433708191\n",
      "cnt: 0 - valLoss: 0.5913474559783936 - trainLoss: 0.5946205854415894\n",
      "cnt: 0 - valLoss: 0.5913382768630981 - trainLoss: 0.5946107506752014\n",
      "cnt: 0 - valLoss: 0.5913378596305847 - trainLoss: 0.5946027040481567\n",
      "cnt: 0 - valLoss: 0.5913280844688416 - trainLoss: 0.5945930480957031\n",
      "cnt: 0 - valLoss: 0.5913193225860596 - trainLoss: 0.5945833325386047\n",
      "cnt: 0 - valLoss: 0.5913094282150269 - trainLoss: 0.5945738554000854\n",
      "cnt: 0 - valLoss: 0.5913096070289612 - trainLoss: 0.5945643782615662\n",
      "cnt: 1 - valLoss: 0.5912995338439941 - trainLoss: 0.5945561528205872\n",
      "cnt: 0 - valLoss: 0.5912900567054749 - trainLoss: 0.5945462584495544\n",
      "cnt: 0 - valLoss: 0.5912810564041138 - trainLoss: 0.5945367813110352\n",
      "cnt: 0 - valLoss: 0.5912714600563049 - trainLoss: 0.5945271253585815\n",
      "cnt: 0 - valLoss: 0.5912712812423706 - trainLoss: 0.5945181846618652\n",
      "cnt: 0 - valLoss: 0.5912612676620483 - trainLoss: 0.5945093631744385\n",
      "cnt: 0 - valLoss: 0.5912520885467529 - trainLoss: 0.5944995284080505\n",
      "cnt: 0 - valLoss: 0.5912428498268127 - trainLoss: 0.5944902300834656\n",
      "cnt: 0 - valLoss: 0.5912334322929382 - trainLoss: 0.5944804549217224\n",
      "cnt: 0 - valLoss: 0.5912331342697144 - trainLoss: 0.5944717526435852\n",
      "cnt: 0 - valLoss: 0.5912233591079712 - trainLoss: 0.5944626331329346\n",
      "cnt: 0 - valLoss: 0.5912147164344788 - trainLoss: 0.594452977180481\n",
      "cnt: 0 - valLoss: 0.5912047624588013 - trainLoss: 0.5944435000419617\n",
      "cnt: 0 - valLoss: 0.5911955237388611 - trainLoss: 0.5944336652755737\n",
      "cnt: 0 - valLoss: 0.5911951661109924 - trainLoss: 0.5944252610206604\n",
      "cnt: 0 - valLoss: 0.5911855697631836 - trainLoss: 0.5944159030914307\n",
      "cnt: 0 - valLoss: 0.5911767482757568 - trainLoss: 0.5944063067436218\n",
      "cnt: 0 - valLoss: 0.5911669135093689 - trainLoss: 0.594396710395813\n",
      "cnt: 0 - valLoss: 0.5911576151847839 - trainLoss: 0.5943869948387146\n",
      "cnt: 0 - valLoss: 0.5911573171615601 - trainLoss: 0.5943785905838013\n",
      "cnt: 0 - valLoss: 0.5911479592323303 - trainLoss: 0.594369113445282\n",
      "cnt: 0 - valLoss: 0.5911389589309692 - trainLoss: 0.5943596959114075\n",
      "cnt: 0 - valLoss: 0.5911290645599365 - trainLoss: 0.5943499803543091\n",
      "cnt: 0 - valLoss: 0.5911199450492859 - trainLoss: 0.5943402051925659\n",
      "cnt: 0 - valLoss: 0.5911195278167725 - trainLoss: 0.5943318605422974\n",
      "cnt: 0 - valLoss: 0.5911104083061218 - trainLoss: 0.5943224430084229\n",
      "cnt: 0 - valLoss: 0.5911012291908264 - trainLoss: 0.5943130850791931\n",
      "cnt: 0 - valLoss: 0.5910913944244385 - trainLoss: 0.5943032503128052\n",
      "cnt: 0 - valLoss: 0.5910822153091431 - trainLoss: 0.5942934155464172\n",
      "cnt: 0 - valLoss: 0.5910820364952087 - trainLoss: 0.5942849516868591\n",
      "cnt: 0 - valLoss: 0.5910735130310059 - trainLoss: 0.5942756533622742\n",
      "cnt: 0 - valLoss: 0.5910636186599731 - trainLoss: 0.5942662358283997\n",
      "cnt: 0 - valLoss: 0.59105384349823 - trainLoss: 0.5942564010620117\n",
      "cnt: 0 - valLoss: 0.5910446643829346 - trainLoss: 0.5942466259002686\n",
      "cnt: 0 - valLoss: 0.5910454988479614 - trainLoss: 0.5942380428314209\n",
      "cnt: 1 - valLoss: 0.5910356640815735 - trainLoss: 0.5942292213439941\n",
      "cnt: 0 - valLoss: 0.5910258889198303 - trainLoss: 0.5942193269729614\n",
      "cnt: 0 - valLoss: 0.5910161137580872 - trainLoss: 0.5942094922065735\n",
      "cnt: 0 - valLoss: 0.5910080075263977 - trainLoss: 0.5941997766494751\n",
      "cnt: 0 - valLoss: 0.5910081267356873 - trainLoss: 0.5941910147666931\n",
      "cnt: 1 - valLoss: 0.5909982919692993 - trainLoss: 0.5941823720932007\n",
      "cnt: 0 - valLoss: 0.5909884572029114 - trainLoss: 0.5941725373268127\n",
      "cnt: 0 - valLoss: 0.5909788608551025 - trainLoss: 0.5941627025604248\n",
      "cnt: 0 - valLoss: 0.590970516204834 - trainLoss: 0.5941530466079712\n",
      "cnt: 0 - valLoss: 0.5909707546234131 - trainLoss: 0.5941438674926758\n",
      "cnt: 1 - valLoss: 0.5909609198570251 - trainLoss: 0.5941355228424072\n",
      "cnt: 0 - valLoss: 0.590951144695282 - trainLoss: 0.5941256880760193\n",
      "cnt: 0 - valLoss: 0.5909416675567627 - trainLoss: 0.5941158533096313\n",
      "cnt: 0 - valLoss: 0.5909331440925598 - trainLoss: 0.5941062569618225\n",
      "cnt: 0 - valLoss: 0.590924084186554 - trainLoss: 0.5940967202186584\n",
      "cnt: 0 - valLoss: 0.5909238457679749 - trainLoss: 0.5940884947776794\n",
      "cnt: 0 - valLoss: 0.5909140706062317 - trainLoss: 0.5940788388252258\n",
      "cnt: 0 - valLoss: 0.5909046530723572 - trainLoss: 0.5940690636634827\n",
      "cnt: 0 - valLoss: 0.5908960700035095 - trainLoss: 0.5940594673156738\n",
      "cnt: 0 - valLoss: 0.5908867716789246 - trainLoss: 0.594049870967865\n",
      "cnt: 0 - valLoss: 0.5908868312835693 - trainLoss: 0.5940409898757935\n",
      "cnt: 1 - valLoss: 0.5908770561218262 - trainLoss: 0.5940319895744324\n",
      "cnt: 0 - valLoss: 0.5908677577972412 - trainLoss: 0.5940222144126892\n",
      "cnt: 0 - valLoss: 0.590859055519104 - trainLoss: 0.5940126180648804\n",
      "cnt: 0 - valLoss: 0.5908494591712952 - trainLoss: 0.5940030217170715\n",
      "cnt: 0 - valLoss: 0.5908498167991638 - trainLoss: 0.5939934253692627\n",
      "cnt: 1 - valLoss: 0.5908401608467102 - trainLoss: 0.5939850807189941\n",
      "cnt: 0 - valLoss: 0.59083092212677 - trainLoss: 0.5939752459526062\n",
      "cnt: 0 - valLoss: 0.590822160243988 - trainLoss: 0.5939658284187317\n",
      "cnt: 0 - valLoss: 0.5908125042915344 - trainLoss: 0.5939561128616333\n",
      "cnt: 0 - valLoss: 0.590803325176239 - trainLoss: 0.5939463376998901\n",
      "cnt: 0 - valLoss: 0.5908033847808838 - trainLoss: 0.5939375758171082\n",
      "cnt: 1 - valLoss: 0.5907941460609436 - trainLoss: 0.5939284563064575\n",
      "cnt: 0 - valLoss: 0.5907854437828064 - trainLoss: 0.5939189195632935\n",
      "cnt: 0 - valLoss: 0.5907758474349976 - trainLoss: 0.5939092636108398\n",
      "cnt: 0 - valLoss: 0.5907662510871887 - trainLoss: 0.5938994884490967\n",
      "cnt: 0 - valLoss: 0.5907666683197021 - trainLoss: 0.5938897728919983\n",
      "cnt: 1 - valLoss: 0.5907575488090515 - trainLoss: 0.5938814282417297\n",
      "cnt: 0 - valLoss: 0.5907487869262695 - trainLoss: 0.5938720107078552\n",
      "cnt: 0 - valLoss: 0.5907391309738159 - trainLoss: 0.5938622951507568\n",
      "cnt: 0 - valLoss: 0.5907295346260071 - trainLoss: 0.5938525795936584\n",
      "cnt: 0 - valLoss: 0.5907205939292908 - trainLoss: 0.5938426852226257\n",
      "cnt: 0 - valLoss: 0.5907216668128967 - trainLoss: 0.593833863735199\n",
      "cnt: 1 - valLoss: 0.5907120704650879 - trainLoss: 0.593825101852417\n",
      "cnt: 0 - valLoss: 0.5907023549079895 - trainLoss: 0.5938153266906738\n",
      "cnt: 0 - valLoss: 0.590692937374115 - trainLoss: 0.5938054919242859\n",
      "cnt: 0 - valLoss: 0.5906845927238464 - trainLoss: 0.5937957763671875\n",
      "cnt: 0 - valLoss: 0.5906755328178406 - trainLoss: 0.5937862992286682\n",
      "cnt: 0 - valLoss: 0.5906757116317749 - trainLoss: 0.5937775373458862\n",
      "cnt: 1 - valLoss: 0.5906660556793213 - trainLoss: 0.5937683582305908\n",
      "cnt: 0 - valLoss: 0.590656578540802 - trainLoss: 0.5937585830688477\n",
      "cnt: 0 - valLoss: 0.5906482934951782 - trainLoss: 0.5937488079071045\n",
      "cnt: 0 - valLoss: 0.5906388163566589 - trainLoss: 0.59373939037323\n",
      "cnt: 0 - valLoss: 0.5906298160552979 - trainLoss: 0.5937296152114868\n",
      "cnt: 0 - valLoss: 0.5906299352645874 - trainLoss: 0.5937210321426392\n",
      "cnt: 1 - valLoss: 0.5906203389167786 - trainLoss: 0.5937115550041199\n",
      "cnt: 0 - valLoss: 0.5906121730804443 - trainLoss: 0.5937017798423767\n",
      "cnt: 0 - valLoss: 0.5906025767326355 - trainLoss: 0.593692421913147\n",
      "cnt: 0 - valLoss: 0.5905930995941162 - trainLoss: 0.593682587146759\n",
      "cnt: 0 - valLoss: 0.5905842185020447 - trainLoss: 0.5936728119850159\n",
      "cnt: 0 - valLoss: 0.5905842185020447 - trainLoss: 0.5936643481254578\n",
      "cnt: 0 - valLoss: 0.5905753970146179 - trainLoss: 0.5936548113822937\n",
      "cnt: 0 - valLoss: 0.5905666351318359 - trainLoss: 0.5936453938484192\n",
      "cnt: 0 - valLoss: 0.5905570387840271 - trainLoss: 0.593635618686676\n",
      "cnt: 0 - valLoss: 0.5905476212501526 - trainLoss: 0.5936257839202881\n",
      "cnt: 0 - valLoss: 0.5905392169952393 - trainLoss: 0.5936160087585449\n",
      "cnt: 0 - valLoss: 0.5905399918556213 - trainLoss: 0.5936077833175659\n",
      "cnt: 1 - valLoss: 0.5905304551124573 - trainLoss: 0.5935983061790466\n",
      "cnt: 0 - valLoss: 0.5905209183692932 - trainLoss: 0.5935885310173035\n",
      "cnt: 0 - valLoss: 0.590511679649353 - trainLoss: 0.5935786962509155\n",
      "cnt: 0 - valLoss: 0.590503454208374 - trainLoss: 0.5935690402984619\n",
      "cnt: 0 - valLoss: 0.5904944539070129 - trainLoss: 0.5935595631599426\n",
      "cnt: 0 - valLoss: 0.5904947519302368 - trainLoss: 0.5935506224632263\n",
      "cnt: 1 - valLoss: 0.590485155582428 - trainLoss: 0.5935414433479309\n",
      "cnt: 0 - valLoss: 0.590475857257843 - trainLoss: 0.5935316681861877\n",
      "cnt: 0 - valLoss: 0.5904676914215088 - trainLoss: 0.5935219526290894\n",
      "cnt: 0 - valLoss: 0.5904582738876343 - trainLoss: 0.5935124754905701\n",
      "cnt: 0 - valLoss: 0.5904491543769836 - trainLoss: 0.5935027003288269\n",
      "cnt: 0 - valLoss: 0.5904495716094971 - trainLoss: 0.5934934616088867\n",
      "cnt: 1 - valLoss: 0.5904401540756226 - trainLoss: 0.59348464012146\n",
      "cnt: 0 - valLoss: 0.5904313325881958 - trainLoss: 0.593474805355072\n",
      "cnt: 0 - valLoss: 0.5904226303100586 - trainLoss: 0.5934653878211975\n",
      "cnt: 0 - valLoss: 0.5904132723808289 - trainLoss: 0.5934556126594543\n",
      "cnt: 0 - valLoss: 0.5904039740562439 - trainLoss: 0.5934458374977112\n",
      "cnt: 0 - valLoss: 0.5904045701026917 - trainLoss: 0.5934362411499023\n",
      "cnt: 1 - valLoss: 0.5903956294059753 - trainLoss: 0.5934277176856995\n",
      "cnt: 0 - valLoss: 0.5903870463371277 - trainLoss: 0.5934181809425354\n",
      "cnt: 0 - valLoss: 0.5903777480125427 - trainLoss: 0.5934085249900818\n",
      "cnt: 0 - valLoss: 0.590368390083313 - trainLoss: 0.5933987498283386\n",
      "cnt: 0 - valLoss: 0.5903592109680176 - trainLoss: 0.5933888554573059\n",
      "cnt: 0 - valLoss: 0.5903513431549072 - trainLoss: 0.5933793187141418\n",
      "cnt: 0 - valLoss: 0.5903518199920654 - trainLoss: 0.5933704972267151\n",
      "cnt: 1 - valLoss: 0.5903424024581909 - trainLoss: 0.5933613777160645\n",
      "cnt: 0 - valLoss: 0.5903330445289612 - trainLoss: 0.5933516025543213\n",
      "cnt: 0 - valLoss: 0.5903236865997314 - trainLoss: 0.5933418273925781\n",
      "cnt: 0 - valLoss: 0.5903156995773315 - trainLoss: 0.5933319926261902\n",
      "cnt: 0 - valLoss: 0.5903064012527466 - trainLoss: 0.5933225750923157\n",
      "cnt: 0 - valLoss: 0.5902977585792542 - trainLoss: 0.5933127999305725\n",
      "cnt: 0 - valLoss: 0.5902979373931885 - trainLoss: 0.5933043360710144\n",
      "cnt: 1 - valLoss: 0.5902885794639587 - trainLoss: 0.593294620513916\n",
      "cnt: 0 - valLoss: 0.5902796983718872 - trainLoss: 0.5932848453521729\n",
      "cnt: 0 - valLoss: 0.5902712941169739 - trainLoss: 0.5932754278182983\n",
      "cnt: 0 - valLoss: 0.5902619957923889 - trainLoss: 0.5932656526565552\n",
      "cnt: 0 - valLoss: 0.5902529358863831 - trainLoss: 0.593255877494812\n",
      "cnt: 0 - valLoss: 0.5902535915374756 - trainLoss: 0.5932464599609375\n",
      "cnt: 1 - valLoss: 0.5902444124221802 - trainLoss: 0.5932376384735107\n",
      "cnt: 0 - valLoss: 0.590236246585846 - trainLoss: 0.5932280421257019\n",
      "cnt: 0 - valLoss: 0.590226948261261 - trainLoss: 0.5932185053825378\n",
      "cnt: 0 - valLoss: 0.590217649936676 - trainLoss: 0.5932086706161499\n",
      "cnt: 0 - valLoss: 0.5902084112167358 - trainLoss: 0.5931988954544067\n",
      "cnt: 0 - valLoss: 0.5902000665664673 - trainLoss: 0.5931890606880188\n",
      "cnt: 0 - valLoss: 0.5902011394500732 - trainLoss: 0.5931802988052368\n",
      "cnt: 1 - valLoss: 0.5901918411254883 - trainLoss: 0.5931711792945862\n",
      "cnt: 0 - valLoss: 0.5901825428009033 - trainLoss: 0.5931613445281982\n",
      "cnt: 0 - valLoss: 0.5901735424995422 - trainLoss: 0.5931515693664551\n",
      "cnt: 0 - valLoss: 0.590165376663208 - trainLoss: 0.5931419134140015\n",
      "cnt: 0 - valLoss: 0.5901561975479126 - trainLoss: 0.5931323766708374\n",
      "cnt: 0 - valLoss: 0.5901471972465515 - trainLoss: 0.5931225419044495\n",
      "cnt: 0 - valLoss: 0.590147852897644 - trainLoss: 0.5931132435798645\n",
      "cnt: 1 - valLoss: 0.5901386141777039 - trainLoss: 0.5931043028831482\n",
      "cnt: 0 - valLoss: 0.5901299118995667 - trainLoss: 0.5930944681167603\n",
      "cnt: 0 - valLoss: 0.5901215076446533 - trainLoss: 0.5930850505828857\n",
      "cnt: 0 - valLoss: 0.5901122093200684 - trainLoss: 0.5930752754211426\n",
      "cnt: 0 - valLoss: 0.5901030898094177 - trainLoss: 0.5930654406547546\n",
      "cnt: 0 - valLoss: 0.5900943279266357 - trainLoss: 0.5930556654930115\n",
      "cnt: 0 - valLoss: 0.5900959372520447 - trainLoss: 0.5930463671684265\n",
      "cnt: 1 - valLoss: 0.5900866985321045 - trainLoss: 0.5930377244949341\n",
      "cnt: 0 - valLoss: 0.5900774598121643 - trainLoss: 0.5930279493331909\n",
      "cnt: 0 - valLoss: 0.5900682210922241 - trainLoss: 0.593018114566803\n",
      "cnt: 0 - valLoss: 0.5900595784187317 - trainLoss: 0.5930082201957703\n",
      "cnt: 0 - valLoss: 0.5900512933731079 - trainLoss: 0.5929988026618958\n",
      "cnt: 0 - valLoss: 0.5900421738624573 - trainLoss: 0.5929891467094421\n",
      "cnt: 0 - valLoss: 0.5900335311889648 - trainLoss: 0.5929793119430542\n",
      "cnt: 0 - valLoss: 0.5900339484214783 - trainLoss: 0.592970609664917\n",
      "cnt: 1 - valLoss: 0.5900249481201172 - trainLoss: 0.5929610133171082\n",
      "cnt: 0 - valLoss: 0.5900170207023621 - trainLoss: 0.5929512977600098\n",
      "cnt: 0 - valLoss: 0.5900078415870667 - trainLoss: 0.5929417610168457\n",
      "cnt: 0 - valLoss: 0.589998722076416 - trainLoss: 0.5929319858551025\n",
      "cnt: 0 - valLoss: 0.5899896025657654 - trainLoss: 0.5929221510887146\n",
      "cnt: 0 - valLoss: 0.5899811387062073 - trainLoss: 0.5929123759269714\n",
      "cnt: 0 - valLoss: 0.5899826288223267 - trainLoss: 0.592903196811676\n",
      "cnt: 1 - valLoss: 0.5899734497070312 - trainLoss: 0.5928943753242493\n",
      "cnt: 0 - valLoss: 0.5899642705917358 - trainLoss: 0.5928844809532166\n",
      "cnt: 0 - valLoss: 0.5899551510810852 - trainLoss: 0.5928747057914734\n",
      "cnt: 0 - valLoss: 0.5899466872215271 - trainLoss: 0.5928649306297302\n",
      "cnt: 0 - valLoss: 0.5899382829666138 - trainLoss: 0.5928554534912109\n",
      "cnt: 0 - valLoss: 0.5899292826652527 - trainLoss: 0.5928456783294678\n",
      "cnt: 0 - valLoss: 0.5899205207824707 - trainLoss: 0.5928358435630798\n",
      "cnt: 0 - valLoss: 0.589921236038208 - trainLoss: 0.592826783657074\n",
      "cnt: 1 - valLoss: 0.5899123549461365 - trainLoss: 0.592817485332489\n",
      "cnt: 0 - valLoss: 0.5899043679237366 - trainLoss: 0.5928078293800354\n",
      "cnt: 0 - valLoss: 0.5898952484130859 - trainLoss: 0.5927982926368713\n",
      "cnt: 0 - valLoss: 0.5898861885070801 - trainLoss: 0.5927883982658386\n",
      "cnt: 0 - valLoss: 0.589877188205719 - trainLoss: 0.5927786827087402\n",
      "cnt: 0 - valLoss: 0.5898685455322266 - trainLoss: 0.5927687883377075\n",
      "cnt: 0 - valLoss: 0.5898606777191162 - trainLoss: 0.5927592515945435\n",
      "cnt: 0 - valLoss: 0.5898615121841431 - trainLoss: 0.5927501320838928\n",
      "cnt: 1 - valLoss: 0.5898524522781372 - trainLoss: 0.5927410125732422\n",
      "cnt: 0 - valLoss: 0.5898433327674866 - trainLoss: 0.592731237411499\n",
      "cnt: 0 - valLoss: 0.5898343324661255 - trainLoss: 0.5927214026451111\n",
      "cnt: 0 - valLoss: 0.5898259282112122 - trainLoss: 0.5927115082740784\n",
      "cnt: 0 - valLoss: 0.5898177027702332 - trainLoss: 0.5927020311355591\n",
      "cnt: 0 - valLoss: 0.5898087024688721 - trainLoss: 0.5926921367645264\n",
      "cnt: 0 - valLoss: 0.5897999405860901 - trainLoss: 0.5926822423934937\n",
      "cnt: 0 - valLoss: 0.5898008942604065 - trainLoss: 0.5926728844642639\n",
      "cnt: 1 - valLoss: 0.5897920727729797 - trainLoss: 0.5926637649536133\n",
      "cnt: 0 - valLoss: 0.5897841453552246 - trainLoss: 0.5926540493965149\n",
      "cnt: 0 - valLoss: 0.5897751450538635 - trainLoss: 0.5926443934440613\n",
      "cnt: 0 - valLoss: 0.5897661447525024 - trainLoss: 0.5926344990730286\n",
      "cnt: 0 - valLoss: 0.5897571444511414 - trainLoss: 0.5926246643066406\n",
      "cnt: 0 - valLoss: 0.5897486209869385 - trainLoss: 0.5926147699356079\n",
      "cnt: 0 - valLoss: 0.5897406935691833 - trainLoss: 0.5926050543785095\n",
      "cnt: 0 - valLoss: 0.5897321105003357 - trainLoss: 0.5925954580307007\n",
      "cnt: 0 - valLoss: 0.5897329449653625 - trainLoss: 0.5925863981246948\n",
      "cnt: 1 - valLoss: 0.5897240042686462 - trainLoss: 0.5925768613815308\n",
      "cnt: 0 - valLoss: 0.5897150039672852 - trainLoss: 0.592566967010498\n",
      "cnt: 0 - valLoss: 0.5897064805030823 - trainLoss: 0.5925570726394653\n",
      "cnt: 0 - valLoss: 0.5896984338760376 - trainLoss: 0.5925474762916565\n",
      "cnt: 0 - valLoss: 0.5896895527839661 - trainLoss: 0.5925377011299133\n",
      "cnt: 0 - valLoss: 0.5896806120872498 - trainLoss: 0.5925278663635254\n",
      "cnt: 0 - valLoss: 0.589671790599823 - trainLoss: 0.5925179719924927\n",
      "cnt: 0 - valLoss: 0.5896729826927185 - trainLoss: 0.5925082564353943\n",
      "cnt: 1 - valLoss: 0.5896645784378052 - trainLoss: 0.592499315738678\n",
      "cnt: 0 - valLoss: 0.5896563529968262 - trainLoss: 0.5924898386001587\n",
      "cnt: 0 - valLoss: 0.5896474123001099 - trainLoss: 0.5924798846244812\n",
      "cnt: 0 - valLoss: 0.5896385312080383 - trainLoss: 0.5924700498580933\n",
      "cnt: 0 - valLoss: 0.5896297097206116 - trainLoss: 0.5924602150917053\n",
      "cnt: 0 - valLoss: 0.5896221399307251 - trainLoss: 0.5924503803253174\n",
      "cnt: 0 - valLoss: 0.5896132588386536 - trainLoss: 0.5924408435821533\n",
      "cnt: 0 - valLoss: 0.5896044373512268 - trainLoss: 0.5924309492111206\n",
      "cnt: 0 - valLoss: 0.5895960927009583 - trainLoss: 0.5924211144447327\n",
      "cnt: 0 - valLoss: 0.5895969271659851 - trainLoss: 0.5924121737480164\n",
      "cnt: 1 - valLoss: 0.5895881056785583 - trainLoss: 0.592402458190918\n",
      "cnt: 0 - valLoss: 0.5895804762840271 - trainLoss: 0.59239262342453\n",
      "cnt: 0 - valLoss: 0.5895715951919556 - trainLoss: 0.5923830270767212\n",
      "cnt: 0 - valLoss: 0.589562714099884 - trainLoss: 0.5923731327056885\n",
      "cnt: 0 - valLoss: 0.5895538926124573 - trainLoss: 0.5923632383346558\n",
      "cnt: 0 - valLoss: 0.5895451903343201 - trainLoss: 0.5923534035682678\n",
      "cnt: 0 - valLoss: 0.5895376205444336 - trainLoss: 0.5923435688018799\n",
      "cnt: 0 - valLoss: 0.5895289778709412 - trainLoss: 0.5923340320587158\n",
      "cnt: 0 - valLoss: 0.5895301103591919 - trainLoss: 0.592324435710907\n",
      "cnt: 1 - valLoss: 0.5895212888717651 - trainLoss: 0.5923153162002563\n",
      "cnt: 0 - valLoss: 0.5895124077796936 - trainLoss: 0.5923053622245789\n",
      "cnt: 0 - valLoss: 0.5895037651062012 - trainLoss: 0.5922954678535461\n",
      "cnt: 0 - valLoss: 0.5894960761070251 - trainLoss: 0.592285692691803\n",
      "cnt: 0 - valLoss: 0.5894872546195984 - trainLoss: 0.5922760963439941\n",
      "cnt: 0 - valLoss: 0.5894784927368164 - trainLoss: 0.5922662615776062\n",
      "cnt: 0 - valLoss: 0.5894697308540344 - trainLoss: 0.5922563672065735\n",
      "cnt: 0 - valLoss: 0.5894611477851868 - trainLoss: 0.5922464728355408\n",
      "cnt: 0 - valLoss: 0.5894536972045898 - trainLoss: 0.5922366976737976\n",
      "cnt: 0 - valLoss: 0.5894550085067749 - trainLoss: 0.5922273993492126\n",
      "cnt: 1 - valLoss: 0.5894461870193481 - trainLoss: 0.5922181606292725\n",
      "cnt: 0 - valLoss: 0.5894373059272766 - trainLoss: 0.5922082662582397\n",
      "cnt: 0 - valLoss: 0.5894285440444946 - trainLoss: 0.592198371887207\n",
      "cnt: 0 - valLoss: 0.589419960975647 - trainLoss: 0.5921884775161743\n",
      "cnt: 0 - valLoss: 0.5894123315811157 - trainLoss: 0.5921787023544312\n",
      "cnt: 0 - valLoss: 0.5894035696983337 - trainLoss: 0.5921691060066223\n",
      "cnt: 0 - valLoss: 0.5893948674201965 - trainLoss: 0.5921591520309448\n",
      "cnt: 0 - valLoss: 0.5893861651420593 - trainLoss: 0.5921493172645569\n",
      "cnt: 0 - valLoss: 0.589377760887146 - trainLoss: 0.5921393632888794\n",
      "cnt: 0 - valLoss: 0.5893799662590027 - trainLoss: 0.5921298265457153\n",
      "cnt: 1 - valLoss: 0.5893712043762207 - trainLoss: 0.592120885848999\n",
      "cnt: 0 - valLoss: 0.589362382888794 - trainLoss: 0.5921109318733215\n",
      "cnt: 0 - valLoss: 0.5893536806106567 - trainLoss: 0.5921010971069336\n",
      "cnt: 0 - valLoss: 0.5893450379371643 - trainLoss: 0.5920912027359009\n",
      "cnt: 0 - valLoss: 0.5893374681472778 - trainLoss: 0.5920813083648682\n",
      "cnt: 0 - valLoss: 0.5893288254737854 - trainLoss: 0.5920716524124146\n",
      "cnt: 0 - valLoss: 0.5893201231956482 - trainLoss: 0.5920618772506714\n",
      "cnt: 0 - valLoss: 0.5893114805221558 - trainLoss: 0.5920519828796387\n",
      "cnt: 0 - valLoss: 0.5893028378486633 - trainLoss: 0.5920420289039612\n",
      "cnt: 0 - valLoss: 0.5892949104309082 - trainLoss: 0.5920321941375732\n",
      "cnt: 0 - valLoss: 0.589296817779541 - trainLoss: 0.5920228958129883\n",
      "cnt: 1 - valLoss: 0.589288055896759 - trainLoss: 0.5920135378837585\n",
      "cnt: 0 - valLoss: 0.5892793536186218 - trainLoss: 0.5920036435127258\n",
      "cnt: 0 - valLoss: 0.5892705917358398 - trainLoss: 0.5919937491416931\n",
      "cnt: 0 - valLoss: 0.5892624258995056 - trainLoss: 0.5919838547706604\n",
      "cnt: 0 - valLoss: 0.5892545580863953 - trainLoss: 0.5919742584228516\n",
      "cnt: 0 - valLoss: 0.5892459154129028 - trainLoss: 0.5919643640518188\n",
      "cnt: 0 - valLoss: 0.5892372727394104 - trainLoss: 0.5919545292854309\n",
      "cnt: 0 - valLoss: 0.5892286896705627 - trainLoss: 0.5919446349143982\n",
      "cnt: 0 - valLoss: 0.589220404624939 - trainLoss: 0.5919346809387207\n",
      "cnt: 0 - valLoss: 0.5892128348350525 - trainLoss: 0.5919250249862671\n",
      "cnt: 0 - valLoss: 0.5892044901847839 - trainLoss: 0.5919152498245239\n",
      "cnt: 0 - valLoss: 0.5892058610916138 - trainLoss: 0.5919057726860046\n",
      "cnt: 1 - valLoss: 0.5891971588134766 - trainLoss: 0.5918962955474854\n",
      "cnt: 0 - valLoss: 0.5891884565353394 - trainLoss: 0.5918864011764526\n",
      "cnt: 0 - valLoss: 0.589180052280426 - trainLoss: 0.5918765068054199\n",
      "cnt: 0 - valLoss: 0.5891725420951843 - trainLoss: 0.5918666124343872\n",
      "cnt: 0 - valLoss: 0.5891638994216919 - trainLoss: 0.5918570756912231\n",
      "cnt: 0 - valLoss: 0.589155375957489 - trainLoss: 0.5918470621109009\n",
      "cnt: 0 - valLoss: 0.5891467928886414 - trainLoss: 0.5918371677398682\n",
      "cnt: 0 - valLoss: 0.5891382694244385 - trainLoss: 0.5918272733688354\n",
      "cnt: 0 - valLoss: 0.5891309380531311 - trainLoss: 0.5918174386024475\n",
      "cnt: 0 - valLoss: 0.589122474193573 - trainLoss: 0.5918078422546387\n",
      "cnt: 0 - valLoss: 0.5891140699386597 - trainLoss: 0.5917978882789612\n",
      "cnt: 0 - valLoss: 0.5891156196594238 - trainLoss: 0.5917881727218628\n",
      "cnt: 1 - valLoss: 0.5891069769859314 - trainLoss: 0.5917789340019226\n",
      "cnt: 0 - valLoss: 0.5890983939170837 - trainLoss: 0.5917689800262451\n",
      "cnt: 0 - valLoss: 0.5890901684761047 - trainLoss: 0.5917590260505676\n",
      "cnt: 0 - valLoss: 0.5890824794769287 - trainLoss: 0.591749370098114\n",
      "cnt: 0 - valLoss: 0.5890739560127258 - trainLoss: 0.5917395353317261\n",
      "cnt: 0 - valLoss: 0.5890653729438782 - trainLoss: 0.5917295813560486\n",
      "cnt: 0 - valLoss: 0.5890569090843201 - trainLoss: 0.5917196869850159\n",
      "cnt: 0 - valLoss: 0.5890486240386963 - trainLoss: 0.5917097926139832\n",
      "cnt: 0 - valLoss: 0.5890411734580994 - trainLoss: 0.5916999578475952\n",
      "cnt: 0 - valLoss: 0.5890327095985413 - trainLoss: 0.5916903018951416\n",
      "cnt: 0 - valLoss: 0.5890242457389832 - trainLoss: 0.5916803479194641\n",
      "cnt: 0 - valLoss: 0.5890158414840698 - trainLoss: 0.5916704535484314\n",
      "cnt: 0 - valLoss: 0.5890076756477356 - trainLoss: 0.5916606187820435\n",
      "cnt: 0 - valLoss: 0.5890100598335266 - trainLoss: 0.5916513204574585\n",
      "cnt: 1 - valLoss: 0.5890015363693237 - trainLoss: 0.5916417837142944\n",
      "cnt: 0 - valLoss: 0.5889930129051208 - trainLoss: 0.5916317701339722\n",
      "cnt: 0 - valLoss: 0.5889845490455627 - trainLoss: 0.5916218161582947\n",
      "cnt: 0 - valLoss: 0.5889760851860046 - trainLoss: 0.5916119813919067\n",
      "cnt: 0 - valLoss: 0.5889678597450256 - trainLoss: 0.5916020274162292\n",
      "cnt: 0 - valLoss: 0.5889603495597839 - trainLoss: 0.5915923118591309\n",
      "cnt: 0 - valLoss: 0.5889519453048706 - trainLoss: 0.5915825366973877\n",
      "cnt: 0 - valLoss: 0.5889434814453125 - trainLoss: 0.5915725827217102\n",
      "cnt: 0 - valLoss: 0.5889350771903992 - trainLoss: 0.5915626883506775\n",
      "cnt: 0 - valLoss: 0.5889267921447754 - trainLoss: 0.591552734375\n",
      "cnt: 0 - valLoss: 0.5889196991920471 - trainLoss: 0.5915428400039673\n",
      "cnt: 0 - valLoss: 0.5889114737510681 - trainLoss: 0.5915332436561584\n",
      "cnt: 0 - valLoss: 0.5889032483100891 - trainLoss: 0.5915233492851257\n",
      "cnt: 0 - valLoss: 0.5888950228691101 - trainLoss: 0.5915133953094482\n",
      "cnt: 0 - valLoss: 0.5888969302177429 - trainLoss: 0.5915036797523499\n",
      "cnt: 1 - valLoss: 0.5888884663581848 - trainLoss: 0.5914942622184753\n",
      "cnt: 0 - valLoss: 0.5888803005218506 - trainLoss: 0.5914842486381531\n",
      "cnt: 0 - valLoss: 0.5888727307319641 - trainLoss: 0.5914745926856995\n",
      "cnt: 0 - valLoss: 0.5888643860816956 - trainLoss: 0.5914647579193115\n",
      "cnt: 0 - valLoss: 0.5888558626174927 - trainLoss: 0.591454803943634\n",
      "cnt: 0 - valLoss: 0.5888474583625793 - trainLoss: 0.5914448499679565\n",
      "cnt: 0 - valLoss: 0.5888392329216003 - trainLoss: 0.5914349555969238\n",
      "cnt: 0 - valLoss: 0.5888321995735168 - trainLoss: 0.5914250016212463\n",
      "cnt: 0 - valLoss: 0.5888239741325378 - trainLoss: 0.5914154052734375\n",
      "cnt: 0 - valLoss: 0.5888156890869141 - trainLoss: 0.5914055109024048\n",
      "cnt: 0 - valLoss: 0.5888075232505798 - trainLoss: 0.5913956165313721\n",
      "cnt: 0 - valLoss: 0.5887992978096008 - trainLoss: 0.5913857221603394\n",
      "cnt: 0 - valLoss: 0.5887913107872009 - trainLoss: 0.5913757681846619\n",
      "cnt: 0 - valLoss: 0.5887841582298279 - trainLoss: 0.5913660526275635\n",
      "cnt: 0 - valLoss: 0.5887759923934937 - trainLoss: 0.5913562774658203\n",
      "cnt: 0 - valLoss: 0.5887678265571594 - trainLoss: 0.5913463830947876\n",
      "cnt: 0 - valLoss: 0.5887596607208252 - trainLoss: 0.5913364291191101\n",
      "cnt: 0 - valLoss: 0.5887617468833923 - trainLoss: 0.5913265347480774\n",
      "cnt: 1 - valLoss: 0.5887534022331238 - trainLoss: 0.5913171172142029\n",
      "cnt: 0 - valLoss: 0.5887454748153687 - trainLoss: 0.5913071632385254\n",
      "cnt: 0 - valLoss: 0.588738203048706 - trainLoss: 0.591297447681427\n",
      "cnt: 0 - valLoss: 0.5887299180030823 - trainLoss: 0.5912876129150391\n",
      "cnt: 0 - valLoss: 0.5887216925621033 - trainLoss: 0.5912776589393616\n",
      "cnt: 0 - valLoss: 0.588713526725769 - trainLoss: 0.5912677049636841\n",
      "cnt: 0 - valLoss: 0.5887054204940796 - trainLoss: 0.5912577509880066\n",
      "cnt: 0 - valLoss: 0.5886985063552856 - trainLoss: 0.5912479162216187\n",
      "cnt: 0 - valLoss: 0.5886903405189514 - trainLoss: 0.5912382006645203\n",
      "cnt: 0 - valLoss: 0.5886821746826172 - trainLoss: 0.5912283062934875\n",
      "cnt: 0 - valLoss: 0.5886740684509277 - trainLoss: 0.5912183523178101\n",
      "cnt: 0 - valLoss: 0.5886659026145935 - trainLoss: 0.5912084579467773\n",
      "cnt: 0 - valLoss: 0.5886580348014832 - trainLoss: 0.5911984443664551\n",
      "cnt: 0 - valLoss: 0.5886510014533997 - trainLoss: 0.5911886692047119\n",
      "cnt: 0 - valLoss: 0.5886428356170654 - trainLoss: 0.5911788940429688\n",
      "cnt: 0 - valLoss: 0.5886347889900208 - trainLoss: 0.5911689400672913\n",
      "cnt: 0 - valLoss: 0.5886268019676208 - trainLoss: 0.5911589860916138\n",
      "cnt: 0 - valLoss: 0.5886186957359314 - trainLoss: 0.591149091720581\n",
      "cnt: 0 - valLoss: 0.5886109471321106 - trainLoss: 0.5911391377449036\n",
      "cnt: 0 - valLoss: 0.5886038541793823 - trainLoss: 0.5911294221878052\n",
      "cnt: 0 - valLoss: 0.5885958671569824 - trainLoss: 0.5911195874214172\n",
      "cnt: 0 - valLoss: 0.5885878205299377 - trainLoss: 0.5911096334457397\n",
      "cnt: 0 - valLoss: 0.5885798335075378 - trainLoss: 0.591099739074707\n",
      "cnt: 0 - valLoss: 0.5885718464851379 - trainLoss: 0.5910897254943848\n",
      "cnt: 0 - valLoss: 0.5885643362998962 - trainLoss: 0.591079831123352\n",
      "cnt: 0 - valLoss: 0.5885670781135559 - trainLoss: 0.591070294380188\n",
      "cnt: 1 - valLoss: 0.5885590314865112 - trainLoss: 0.5910605192184448\n",
      "cnt: 0 - valLoss: 0.5885509252548218 - trainLoss: 0.5910505652427673\n",
      "cnt: 0 - valLoss: 0.5885429978370667 - trainLoss: 0.5910405516624451\n",
      "cnt: 0 - valLoss: 0.588534951210022 - trainLoss: 0.5910305976867676\n",
      "cnt: 0 - valLoss: 0.588527500629425 - trainLoss: 0.5910205841064453\n",
      "cnt: 0 - valLoss: 0.5885202884674072 - trainLoss: 0.5910109877586365\n",
      "cnt: 0 - valLoss: 0.5885123014450073 - trainLoss: 0.591001033782959\n",
      "cnt: 0 - valLoss: 0.5885043144226074 - trainLoss: 0.5909910202026367\n",
      "cnt: 0 - valLoss: 0.5884963274002075 - trainLoss: 0.5909810662269592\n",
      "cnt: 0 - valLoss: 0.5884885191917419 - trainLoss: 0.590971052646637\n",
      "cnt: 0 - valLoss: 0.5884817838668823 - trainLoss: 0.5909611582756042\n",
      "cnt: 0 - valLoss: 0.5884737968444824 - trainLoss: 0.5909514427185059\n",
      "cnt: 0 - valLoss: 0.5884659290313721 - trainLoss: 0.5909414887428284\n",
      "cnt: 0 - valLoss: 0.5884580016136169 - trainLoss: 0.5909315347671509\n",
      "cnt: 0 - valLoss: 0.5884500741958618 - trainLoss: 0.5909215211868286\n",
      "cnt: 0 - valLoss: 0.5884422659873962 - trainLoss: 0.5909115672111511\n",
      "cnt: 0 - valLoss: 0.5884355902671814 - trainLoss: 0.5909016132354736\n",
      "cnt: 0 - valLoss: 0.5884276628494263 - trainLoss: 0.5908918976783752\n",
      "cnt: 0 - valLoss: 0.5884197950363159 - trainLoss: 0.5908819437026978\n",
      "cnt: 0 - valLoss: 0.5884119272232056 - trainLoss: 0.5908719301223755\n",
      "cnt: 0 - valLoss: 0.5884040594100952 - trainLoss: 0.5908618569374084\n",
      "cnt: 0 - valLoss: 0.5883963108062744 - trainLoss: 0.5908519625663757\n",
      "cnt: 0 - valLoss: 0.5883896350860596 - trainLoss: 0.5908420085906982\n",
      "cnt: 0 - valLoss: 0.588381826877594 - trainLoss: 0.5908322930335999\n",
      "cnt: 0 - valLoss: 0.5883739590644836 - trainLoss: 0.5908222794532776\n",
      "cnt: 0 - valLoss: 0.5883661508560181 - trainLoss: 0.5908122658729553\n",
      "cnt: 0 - valLoss: 0.5883583426475525 - trainLoss: 0.5908023118972778\n",
      "cnt: 0 - valLoss: 0.5883505940437317 - trainLoss: 0.5907922983169556\n",
      "cnt: 0 - valLoss: 0.5883440375328064 - trainLoss: 0.5907823443412781\n",
      "cnt: 0 - valLoss: 0.5883361101150513 - trainLoss: 0.5907725691795349\n",
      "cnt: 0 - valLoss: 0.5883283615112305 - trainLoss: 0.5907625555992126\n",
      "cnt: 0 - valLoss: 0.5883206129074097 - trainLoss: 0.5907526016235352\n",
      "cnt: 0 - valLoss: 0.5883128046989441 - trainLoss: 0.5907425880432129\n",
      "cnt: 0 - valLoss: 0.5883051156997681 - trainLoss: 0.5907325148582458\n",
      "cnt: 0 - valLoss: 0.588298499584198 - trainLoss: 0.5907225608825684\n",
      "cnt: 0 - valLoss: 0.5882907509803772 - trainLoss: 0.5907129049301147\n",
      "cnt: 0 - valLoss: 0.5882830023765564 - trainLoss: 0.5907027721405029\n",
      "cnt: 0 - valLoss: 0.5882752537727356 - trainLoss: 0.5906928181648254\n",
      "cnt: 0 - valLoss: 0.5882675051689148 - trainLoss: 0.5906827449798584\n",
      "cnt: 0 - valLoss: 0.5882598161697388 - trainLoss: 0.5906727910041809\n",
      "cnt: 0 - valLoss: 0.5882524847984314 - trainLoss: 0.5906627178192139\n",
      "cnt: 0 - valLoss: 0.5882455110549927 - trainLoss: 0.5906529426574707\n",
      "cnt: 0 - valLoss: 0.5882378220558167 - trainLoss: 0.590643048286438\n",
      "cnt: 0 - valLoss: 0.5882300734519958 - trainLoss: 0.5906329154968262\n",
      "cnt: 0 - valLoss: 0.5882223844528198 - trainLoss: 0.5906229019165039\n",
      "cnt: 0 - valLoss: 0.588214635848999 - trainLoss: 0.5906128287315369\n",
      "cnt: 0 - valLoss: 0.5882071852684021 - trainLoss: 0.5906028151512146\n",
      "cnt: 0 - valLoss: 0.5882004499435425 - trainLoss: 0.5905929207801819\n",
      "cnt: 0 - valLoss: 0.5881927013397217 - trainLoss: 0.5905829668045044\n",
      "cnt: 0 - valLoss: 0.5881850719451904 - trainLoss: 0.5905728936195374\n",
      "cnt: 0 - valLoss: 0.5881772637367249 - trainLoss: 0.5905628800392151\n",
      "cnt: 0 - valLoss: 0.5881695747375488 - trainLoss: 0.590552806854248\n",
      "cnt: 0 - valLoss: 0.5881621837615967 - trainLoss: 0.5905426740646362\n",
      "cnt: 0 - valLoss: 0.5881554484367371 - trainLoss: 0.5905328392982483\n",
      "cnt: 0 - valLoss: 0.5881478190422058 - trainLoss: 0.5905228853225708\n",
      "cnt: 0 - valLoss: 0.5881403684616089 - trainLoss: 0.5905128121376038\n",
      "cnt: 0 - valLoss: 0.588132381439209 - trainLoss: 0.5905026793479919\n",
      "cnt: 0 - valLoss: 0.5881244540214539 - trainLoss: 0.5904922485351562\n",
      "cnt: 0 - valLoss: 0.5881224870681763 - trainLoss: 0.5904819369316101\n",
      "cnt: 0 - valLoss: 0.588114857673645 - trainLoss: 0.590472400188446\n",
      "cnt: 0 - valLoss: 0.5881083011627197 - trainLoss: 0.590462327003479\n",
      "cnt: 0 - valLoss: 0.5881004929542542 - trainLoss: 0.5904524922370911\n",
      "cnt: 0 - valLoss: 0.5880928635597229 - trainLoss: 0.590442419052124\n",
      "cnt: 0 - valLoss: 0.5880850553512573 - trainLoss: 0.590432345867157\n",
      "cnt: 0 - valLoss: 0.5880773663520813 - trainLoss: 0.5904222726821899\n",
      "cnt: 0 - valLoss: 0.5880696773529053 - trainLoss: 0.5904121398925781\n",
      "cnt: 0 - valLoss: 0.5880632996559143 - trainLoss: 0.5904020667076111\n",
      "cnt: 0 - valLoss: 0.5880551934242249 - trainLoss: 0.5903922319412231\n",
      "cnt: 0 - valLoss: 0.5880472660064697 - trainLoss: 0.5903817415237427\n",
      "cnt: 0 - valLoss: 0.588039219379425 - trainLoss: 0.5903712511062622\n",
      "cnt: 0 - valLoss: 0.5880312323570251 - trainLoss: 0.5903607606887817\n",
      "cnt: 0 - valLoss: 0.5880233645439148 - trainLoss: 0.590350329875946\n",
      "cnt: 0 - valLoss: 0.5880166292190552 - trainLoss: 0.5903398990631104\n",
      "cnt: 0 - valLoss: 0.5880124568939209 - trainLoss: 0.5903302431106567\n",
      "cnt: 0 - valLoss: 0.5880043506622314 - trainLoss: 0.5903208255767822\n",
      "cnt: 0 - valLoss: 0.5879963636398315 - trainLoss: 0.590310275554657\n",
      "cnt: 0 - valLoss: 0.5879883766174316 - trainLoss: 0.5902997851371765\n",
      "cnt: 0 - valLoss: 0.5879803895950317 - trainLoss: 0.590289294719696\n",
      "cnt: 0 - valLoss: 0.5879726409912109 - trainLoss: 0.5902788639068604\n",
      "cnt: 0 - valLoss: 0.5879659056663513 - trainLoss: 0.5902684926986694\n",
      "cnt: 0 - valLoss: 0.5879616737365723 - trainLoss: 0.5902588963508606\n",
      "cnt: 0 - valLoss: 0.5879535675048828 - trainLoss: 0.5902493596076965\n",
      "cnt: 0 - valLoss: 0.5879454612731934 - trainLoss: 0.5902388095855713\n",
      "cnt: 0 - valLoss: 0.5879374742507935 - trainLoss: 0.5902283787727356\n",
      "cnt: 0 - valLoss: 0.5879295468330383 - trainLoss: 0.5902178883552551\n",
      "cnt: 0 - valLoss: 0.5879217982292175 - trainLoss: 0.5902073979377747\n",
      "cnt: 0 - valLoss: 0.5879151225090027 - trainLoss: 0.5901970863342285\n",
      "cnt: 0 - valLoss: 0.5879109501838684 - trainLoss: 0.5901876091957092\n",
      "cnt: 0 - valLoss: 0.5879027843475342 - trainLoss: 0.5901779532432556\n",
      "cnt: 0 - valLoss: 0.5878946781158447 - trainLoss: 0.5901674032211304\n",
      "cnt: 0 - valLoss: 0.5878865122795105 - trainLoss: 0.5901569128036499\n",
      "cnt: 0 - valLoss: 0.5878784656524658 - trainLoss: 0.5901463627815247\n",
      "cnt: 0 - valLoss: 0.587870717048645 - trainLoss: 0.5901358127593994\n",
      "cnt: 0 - valLoss: 0.5878675580024719 - trainLoss: 0.5901256203651428\n",
      "cnt: 0 - valLoss: 0.5878593921661377 - trainLoss: 0.5901167988777161\n",
      "cnt: 0 - valLoss: 0.587851345539093 - trainLoss: 0.5901062488555908\n",
      "cnt: 0 - valLoss: 0.587843120098114 - trainLoss: 0.5900956988334656\n",
      "cnt: 0 - valLoss: 0.5878350734710693 - trainLoss: 0.5900852084159851\n",
      "cnt: 0 - valLoss: 0.587827205657959 - trainLoss: 0.5900746583938599\n",
      "cnt: 0 - valLoss: 0.5878203511238098 - trainLoss: 0.5900643467903137\n",
      "cnt: 0 - valLoss: 0.5878164172172546 - trainLoss: 0.5900543928146362\n",
      "cnt: 0 - valLoss: 0.5878082513809204 - trainLoss: 0.590045154094696\n",
      "cnt: 0 - valLoss: 0.5878000855445862 - trainLoss: 0.5900346040725708\n",
      "cnt: 0 - valLoss: 0.5877920389175415 - trainLoss: 0.5900241136550903\n",
      "cnt: 0 - valLoss: 0.587783932685852 - trainLoss: 0.5900135636329651\n",
      "cnt: 0 - valLoss: 0.5877760648727417 - trainLoss: 0.5900030732154846\n",
      "cnt: 0 - valLoss: 0.5877693891525269 - trainLoss: 0.5899926424026489\n",
      "cnt: 0 - valLoss: 0.5877652764320374 - trainLoss: 0.5899830460548401\n",
      "cnt: 0 - valLoss: 0.5877572298049927 - trainLoss: 0.589973509311676\n",
      "cnt: 0 - valLoss: 0.5877491235733032 - trainLoss: 0.5899630188941956\n",
      "cnt: 0 - valLoss: 0.5877410173416138 - trainLoss: 0.5899524688720703\n",
      "cnt: 0 - valLoss: 0.5877329707145691 - trainLoss: 0.5899419188499451\n",
      "cnt: 0 - valLoss: 0.5877251029014587 - trainLoss: 0.5899314284324646\n",
      "cnt: 0 - valLoss: 0.5877180099487305 - trainLoss: 0.5899209380149841\n",
      "cnt: 0 - valLoss: 0.5877143740653992 - trainLoss: 0.5899114608764648\n",
      "cnt: 0 - valLoss: 0.5877062082290649 - trainLoss: 0.5899018049240112\n",
      "cnt: 0 - valLoss: 0.5876981616020203 - trainLoss: 0.589891254901886\n",
      "cnt: 0 - valLoss: 0.5876900553703308 - trainLoss: 0.5898807048797607\n",
      "cnt: 0 - valLoss: 0.5876820683479309 - trainLoss: 0.5898701548576355\n",
      "cnt: 0 - valLoss: 0.5876744985580444 - trainLoss: 0.589859664440155\n",
      "cnt: 0 - valLoss: 0.5876674056053162 - trainLoss: 0.5898494720458984\n",
      "cnt: 0 - valLoss: 0.587663471698761 - trainLoss: 0.5898392796516418\n",
      "cnt: 0 - valLoss: 0.5876553654670715 - trainLoss: 0.5898300409317017\n",
      "cnt: 0 - valLoss: 0.5876473188400269 - trainLoss: 0.5898194909095764\n",
      "cnt: 0 - valLoss: 0.5876392126083374 - trainLoss: 0.589809000492096\n",
      "cnt: 0 - valLoss: 0.5876312851905823 - trainLoss: 0.5897983908653259\n",
      "cnt: 0 - valLoss: 0.5876235365867615 - trainLoss: 0.5897879600524902\n",
      "cnt: 0 - valLoss: 0.587616503238678 - trainLoss: 0.5897775292396545\n",
      "cnt: 0 - valLoss: 0.5876091122627258 - trainLoss: 0.5897672176361084\n",
      "cnt: 0 - valLoss: 0.5876048803329468 - trainLoss: 0.5897578597068787\n",
      "cnt: 0 - valLoss: 0.5875968337059021 - trainLoss: 0.5897477269172668\n",
      "cnt: 0 - valLoss: 0.5875887870788574 - trainLoss: 0.5897372364997864\n",
      "cnt: 0 - valLoss: 0.5875807404518127 - trainLoss: 0.5897266268730164\n",
      "cnt: 0 - valLoss: 0.5875727534294128 - trainLoss: 0.5897161364555359\n",
      "cnt: 0 - valLoss: 0.5875658392906189 - trainLoss: 0.5897056460380554\n",
      "cnt: 0 - valLoss: 0.5875582098960876 - trainLoss: 0.589695394039154\n",
      "cnt: 0 - valLoss: 0.5875542759895325 - trainLoss: 0.5896852016448975\n",
      "cnt: 0 - valLoss: 0.5875462293624878 - trainLoss: 0.5896759033203125\n",
      "cnt: 0 - valLoss: 0.5875382423400879 - trainLoss: 0.5896653532981873\n",
      "cnt: 0 - valLoss: 0.5875301361083984 - trainLoss: 0.589654803276062\n",
      "cnt: 0 - valLoss: 0.5875222086906433 - trainLoss: 0.5896442532539368\n",
      "cnt: 0 - valLoss: 0.5875144600868225 - trainLoss: 0.5896337628364563\n",
      "cnt: 0 - valLoss: 0.587507426738739 - trainLoss: 0.5896233916282654\n",
      "cnt: 0 - valLoss: 0.5874999165534973 - trainLoss: 0.5896130204200745\n",
      "cnt: 0 - valLoss: 0.5874958038330078 - trainLoss: 0.5896032452583313\n",
      "cnt: 0 - valLoss: 0.5874877572059631 - trainLoss: 0.5895934700965881\n",
      "cnt: 0 - valLoss: 0.5874797701835632 - trainLoss: 0.5895829200744629\n",
      "cnt: 0 - valLoss: 0.5874717235565186 - trainLoss: 0.5895723700523376\n",
      "cnt: 0 - valLoss: 0.5874637961387634 - trainLoss: 0.5895618200302124\n",
      "cnt: 0 - valLoss: 0.5874562859535217 - trainLoss: 0.5895513296127319\n",
      "cnt: 0 - valLoss: 0.5874491930007935 - trainLoss: 0.5895410776138306\n",
      "cnt: 0 - valLoss: 0.587441623210907 - trainLoss: 0.5895305871963501\n",
      "cnt: 0 - valLoss: 0.5874375700950623 - trainLoss: 0.5895206928253174\n",
      "cnt: 0 - valLoss: 0.5874295830726624 - trainLoss: 0.589510977268219\n",
      "cnt: 0 - valLoss: 0.5874216556549072 - trainLoss: 0.5895004868507385\n",
      "cnt: 0 - valLoss: 0.5874136686325073 - trainLoss: 0.5894898772239685\n",
      "cnt: 0 - valLoss: 0.587405800819397 - trainLoss: 0.589479386806488\n",
      "cnt: 0 - valLoss: 0.5873988270759583 - trainLoss: 0.5894688963890076\n",
      "cnt: 0 - valLoss: 0.5873909592628479 - trainLoss: 0.5894586443901062\n",
      "cnt: 0 - valLoss: 0.5873833298683167 - trainLoss: 0.589448094367981\n",
      "cnt: 0 - valLoss: 0.5873793959617615 - trainLoss: 0.5894379615783691\n",
      "cnt: 0 - valLoss: 0.5873714089393616 - trainLoss: 0.5894284844398499\n",
      "cnt: 0 - valLoss: 0.5873634219169617 - trainLoss: 0.5894179344177246\n",
      "cnt: 0 - valLoss: 0.5873555541038513 - trainLoss: 0.5894073843955994\n",
      "cnt: 0 - valLoss: 0.5873477458953857 - trainLoss: 0.5893968939781189\n",
      "cnt: 0 - valLoss: 0.5873407125473022 - trainLoss: 0.589386522769928\n",
      "cnt: 0 - valLoss: 0.5873329043388367 - trainLoss: 0.5893762111663818\n",
      "cnt: 0 - valLoss: 0.5873252153396606 - trainLoss: 0.5893656611442566\n",
      "cnt: 0 - valLoss: 0.5873178243637085 - trainLoss: 0.5893552303314209\n",
      "cnt: 0 - valLoss: 0.5873135924339294 - trainLoss: 0.5893455147743225\n",
      "cnt: 0 - valLoss: 0.5873056650161743 - trainLoss: 0.5893355011940002\n",
      "cnt: 0 - valLoss: 0.5872977375984192 - trainLoss: 0.5893250107765198\n",
      "cnt: 0 - valLoss: 0.5872899889945984 - trainLoss: 0.5893144607543945\n",
      "cnt: 0 - valLoss: 0.5872829556465149 - trainLoss: 0.5893040895462036\n",
      "cnt: 0 - valLoss: 0.5872750878334045 - trainLoss: 0.5892936587333679\n",
      "cnt: 0 - valLoss: 0.587267279624939 - trainLoss: 0.5892831683158875\n",
      "cnt: 0 - valLoss: 0.5872595906257629 - trainLoss: 0.589272677898407\n",
      "cnt: 0 - valLoss: 0.5872522592544556 - trainLoss: 0.5892622470855713\n",
      "cnt: 0 - valLoss: 0.5872480273246765 - trainLoss: 0.5892525911331177\n",
      "cnt: 0 - valLoss: 0.5872400999069214 - trainLoss: 0.5892424583435059\n",
      "cnt: 0 - valLoss: 0.5872324109077454 - trainLoss: 0.5892319679260254\n",
      "cnt: 0 - valLoss: 0.5872253775596619 - trainLoss: 0.5892215371131897\n",
      "cnt: 0 - valLoss: 0.5872175097465515 - trainLoss: 0.5892111659049988\n",
      "cnt: 0 - valLoss: 0.5872097015380859 - trainLoss: 0.5892006158828735\n",
      "cnt: 0 - valLoss: 0.5872018337249756 - trainLoss: 0.5891900658607483\n",
      "cnt: 0 - valLoss: 0.5871942043304443 - trainLoss: 0.5891796350479126\n",
      "cnt: 0 - valLoss: 0.5871866345405579 - trainLoss: 0.5891690850257874\n",
      "cnt: 0 - valLoss: 0.5871828198432922 - trainLoss: 0.5891589522361755\n",
      "cnt: 0 - valLoss: 0.5871750712394714 - trainLoss: 0.5891492366790771\n",
      "cnt: 0 - valLoss: 0.5871680378913879 - trainLoss: 0.5891388654708862\n",
      "cnt: 0 - valLoss: 0.5871602296829224 - trainLoss: 0.5891284346580505\n",
      "cnt: 0 - valLoss: 0.587152361869812 - trainLoss: 0.5891179442405701\n",
      "cnt: 0 - valLoss: 0.5871445536613464 - trainLoss: 0.5891074538230896\n",
      "cnt: 0 - valLoss: 0.5871367454528809 - trainLoss: 0.5890968441963196\n",
      "cnt: 0 - valLoss: 0.5871289968490601 - trainLoss: 0.5890864133834839\n",
      "cnt: 0 - valLoss: 0.5871214270591736 - trainLoss: 0.5890758633613586\n",
      "cnt: 0 - valLoss: 0.5871146321296692 - trainLoss: 0.5890655517578125\n",
      "cnt: 0 - valLoss: 0.5871082544326782 - trainLoss: 0.5890546441078186\n",
      "cnt: 0 - valLoss: 0.5871006846427917 - trainLoss: 0.5890439748764038\n",
      "cnt: 0 - valLoss: 0.5870931148529053 - trainLoss: 0.5890324711799622\n",
      "cnt: 0 - valLoss: 0.587085485458374 - trainLoss: 0.5890210270881653\n",
      "cnt: 0 - valLoss: 0.5870779752731323 - trainLoss: 0.5890095829963684\n",
      "cnt: 0 - valLoss: 0.5870704054832458 - trainLoss: 0.5889981389045715\n",
      "cnt: 0 - valLoss: 0.5870628952980042 - trainLoss: 0.5889866948127747\n",
      "cnt: 0 - valLoss: 0.5870552659034729 - trainLoss: 0.588975191116333\n",
      "cnt: 0 - valLoss: 0.5870476961135864 - trainLoss: 0.5889637470245361\n",
      "cnt: 0 - valLoss: 0.5870400071144104 - trainLoss: 0.588952362537384\n",
      "cnt: 0 - valLoss: 0.5870325565338135 - trainLoss: 0.5889409184455872\n",
      "cnt: 0 - valLoss: 0.5870256423950195 - trainLoss: 0.5889295935630798\n",
      "cnt: 0 - valLoss: 0.5870180130004883 - trainLoss: 0.588918149471283\n",
      "cnt: 0 - valLoss: 0.5870106816291809 - trainLoss: 0.5889067053794861\n",
      "cnt: 0 - valLoss: 0.5870043039321899 - trainLoss: 0.5888957977294922\n",
      "cnt: 0 - valLoss: 0.5869967937469482 - trainLoss: 0.5888846516609192\n",
      "cnt: 0 - valLoss: 0.5869891047477722 - trainLoss: 0.5888731479644775\n",
      "cnt: 0 - valLoss: 0.5869815349578857 - trainLoss: 0.5888616442680359\n",
      "cnt: 0 - valLoss: 0.5869739055633545 - trainLoss: 0.5888500809669495\n",
      "cnt: 0 - valLoss: 0.5869662761688232 - trainLoss: 0.5888386964797974\n",
      "cnt: 0 - valLoss: 0.586958646774292 - trainLoss: 0.5888271927833557\n",
      "cnt: 0 - valLoss: 0.5869510173797607 - trainLoss: 0.5888157486915588\n",
      "cnt: 0 - valLoss: 0.5869433879852295 - trainLoss: 0.5888042449951172\n",
      "cnt: 0 - valLoss: 0.5869358777999878 - trainLoss: 0.5887928009033203\n",
      "cnt: 0 - valLoss: 0.5869289636611938 - trainLoss: 0.5887814164161682\n",
      "cnt: 0 - valLoss: 0.5869212746620178 - trainLoss: 0.5887699723243713\n",
      "cnt: 0 - valLoss: 0.5869138836860657 - trainLoss: 0.5887584686279297\n",
      "cnt: 0 - valLoss: 0.5869077444076538 - trainLoss: 0.588747501373291\n",
      "cnt: 0 - valLoss: 0.5869001150131226 - trainLoss: 0.5887364149093628\n",
      "cnt: 0 - valLoss: 0.5868924856185913 - trainLoss: 0.5887248516082764\n",
      "cnt: 0 - valLoss: 0.5868849158287048 - trainLoss: 0.5887132883071899\n",
      "cnt: 0 - valLoss: 0.5868772268295288 - trainLoss: 0.5887017846107483\n",
      "cnt: 0 - valLoss: 0.5868695974349976 - trainLoss: 0.5886902213096619\n",
      "cnt: 0 - valLoss: 0.5868619680404663 - trainLoss: 0.5886787176132202\n",
      "cnt: 0 - valLoss: 0.5868543386459351 - trainLoss: 0.5886672735214233\n",
      "cnt: 0 - valLoss: 0.5868467092514038 - trainLoss: 0.5886557102203369\n",
      "cnt: 0 - valLoss: 0.5868390202522278 - trainLoss: 0.5886442065238953\n",
      "cnt: 0 - valLoss: 0.5868315100669861 - trainLoss: 0.5886327028274536\n",
      "cnt: 0 - valLoss: 0.5868248343467712 - trainLoss: 0.5886213183403015\n",
      "cnt: 0 - valLoss: 0.5868186354637146 - trainLoss: 0.5886104106903076\n",
      "cnt: 0 - valLoss: 0.5868110060691833 - trainLoss: 0.5885991454124451\n",
      "cnt: 0 - valLoss: 0.5868033170700073 - trainLoss: 0.5885875225067139\n",
      "cnt: 0 - valLoss: 0.5867956876754761 - trainLoss: 0.5885759592056274\n",
      "cnt: 0 - valLoss: 0.5867879986763 - trainLoss: 0.588564395904541\n",
      "cnt: 0 - valLoss: 0.586780309677124 - trainLoss: 0.5885528922080994\n",
      "cnt: 0 - valLoss: 0.5867726802825928 - trainLoss: 0.5885413289070129\n",
      "cnt: 0 - valLoss: 0.5867650508880615 - trainLoss: 0.5885298252105713\n",
      "cnt: 0 - valLoss: 0.5867574214935303 - trainLoss: 0.5885182619094849\n",
      "cnt: 0 - valLoss: 0.5867497324943542 - trainLoss: 0.5885066986083984\n",
      "cnt: 0 - valLoss: 0.5867421627044678 - trainLoss: 0.588495135307312\n",
      "cnt: 0 - valLoss: 0.5867366194725037 - trainLoss: 0.5884838700294495\n",
      "cnt: 0 - valLoss: 0.5867293477058411 - trainLoss: 0.5884732604026794\n",
      "cnt: 0 - valLoss: 0.5867217183113098 - trainLoss: 0.5884614586830139\n",
      "cnt: 0 - valLoss: 0.586713969707489 - trainLoss: 0.5884498357772827\n",
      "cnt: 0 - valLoss: 0.5867063403129578 - trainLoss: 0.5884382128715515\n",
      "cnt: 0 - valLoss: 0.5866986513137817 - trainLoss: 0.5884266495704651\n",
      "cnt: 0 - valLoss: 0.5866910219192505 - trainLoss: 0.5884151458740234\n",
      "cnt: 0 - valLoss: 0.5866832733154297 - trainLoss: 0.5884035229682922\n",
      "cnt: 0 - valLoss: 0.5866756439208984 - trainLoss: 0.588391900062561\n",
      "cnt: 0 - valLoss: 0.5866679549217224 - trainLoss: 0.5883802771568298\n",
      "cnt: 0 - valLoss: 0.5866606831550598 - trainLoss: 0.5883687138557434\n",
      "cnt: 0 - valLoss: 0.5866544842720032 - trainLoss: 0.5883579850196838\n",
      "cnt: 0 - valLoss: 0.5866467952728271 - trainLoss: 0.5883466005325317\n",
      "cnt: 0 - valLoss: 0.5866392254829407 - trainLoss: 0.5883349180221558\n",
      "cnt: 0 - valLoss: 0.5866321921348572 - trainLoss: 0.5883232951164246\n",
      "cnt: 0 - valLoss: 0.5866243839263916 - trainLoss: 0.5883117318153381\n",
      "cnt: 0 - valLoss: 0.5866166949272156 - trainLoss: 0.5882999300956726\n",
      "cnt: 0 - valLoss: 0.58660888671875 - trainLoss: 0.5882881879806519\n",
      "cnt: 0 - valLoss: 0.586601197719574 - trainLoss: 0.5882763862609863\n",
      "cnt: 0 - valLoss: 0.5865933895111084 - trainLoss: 0.5882646441459656\n",
      "cnt: 0 - valLoss: 0.5865859389305115 - trainLoss: 0.5882529020309448\n",
      "cnt: 0 - valLoss: 0.5865799188613892 - trainLoss: 0.5882418751716614\n",
      "cnt: 0 - valLoss: 0.5865721702575684 - trainLoss: 0.5882304310798645\n",
      "cnt: 0 - valLoss: 0.5865644216537476 - trainLoss: 0.588218629360199\n",
      "cnt: 0 - valLoss: 0.586556613445282 - trainLoss: 0.5882067680358887\n",
      "cnt: 0 - valLoss: 0.5865488052368164 - trainLoss: 0.5881949663162231\n",
      "cnt: 0 - valLoss: 0.5865410566329956 - trainLoss: 0.5881831645965576\n",
      "cnt: 0 - valLoss: 0.5865333080291748 - trainLoss: 0.5881714224815369\n",
      "cnt: 0 - valLoss: 0.5865254998207092 - trainLoss: 0.5881596207618713\n",
      "cnt: 0 - valLoss: 0.5865177512168884 - trainLoss: 0.5881478190422058\n",
      "cnt: 0 - valLoss: 0.5865103006362915 - trainLoss: 0.5881360769271851\n",
      "cnt: 0 - valLoss: 0.5865052938461304 - trainLoss: 0.5881248712539673\n",
      "cnt: 0 - valLoss: 0.58649742603302 - trainLoss: 0.5881137251853943\n",
      "cnt: 0 - valLoss: 0.5864896774291992 - trainLoss: 0.5881018042564392\n",
      "cnt: 0 - valLoss: 0.5864818692207336 - trainLoss: 0.5880898833274841\n",
      "cnt: 0 - valLoss: 0.5864740610122681 - trainLoss: 0.5880781412124634\n",
      "cnt: 0 - valLoss: 0.5864663124084473 - trainLoss: 0.5880663394927979\n",
      "cnt: 0 - valLoss: 0.5864585041999817 - trainLoss: 0.5880545377731323\n",
      "cnt: 0 - valLoss: 0.5864507555961609 - trainLoss: 0.588042676448822\n",
      "cnt: 0 - valLoss: 0.5864430665969849 - trainLoss: 0.5880308747291565\n",
      "cnt: 0 - valLoss: 0.5864374041557312 - trainLoss: 0.5880192518234253\n",
      "cnt: 0 - valLoss: 0.5864295363426208 - trainLoss: 0.588008463382721\n",
      "cnt: 0 - valLoss: 0.5864217281341553 - trainLoss: 0.5879965424537659\n",
      "cnt: 0 - valLoss: 0.5864138603210449 - trainLoss: 0.5879846215248108\n",
      "cnt: 0 - valLoss: 0.5864059925079346 - trainLoss: 0.5879727005958557\n",
      "cnt: 0 - valLoss: 0.586398184299469 - trainLoss: 0.5879608392715454\n",
      "cnt: 0 - valLoss: 0.5863903164863586 - trainLoss: 0.5879489779472351\n",
      "cnt: 0 - valLoss: 0.5863824486732483 - trainLoss: 0.5879371166229248\n",
      "cnt: 0 - valLoss: 0.5863746404647827 - trainLoss: 0.5879251956939697\n",
      "cnt: 0 - valLoss: 0.5863695740699768 - trainLoss: 0.587913453578949\n",
      "cnt: 0 - valLoss: 0.5863619446754456 - trainLoss: 0.5879030823707581\n",
      "cnt: 0 - valLoss: 0.5863540172576904 - trainLoss: 0.5878908634185791\n",
      "cnt: 0 - valLoss: 0.5863461494445801 - trainLoss: 0.5878788828849792\n",
      "cnt: 0 - valLoss: 0.586338222026825 - trainLoss: 0.5878669619560242\n",
      "cnt: 0 - valLoss: 0.5863302946090698 - trainLoss: 0.5878551006317139\n",
      "cnt: 0 - valLoss: 0.5863223075866699 - trainLoss: 0.587843120098114\n",
      "cnt: 0 - valLoss: 0.58631432056427 - trainLoss: 0.5878311991691589\n",
      "cnt: 0 - valLoss: 0.586306631565094 - trainLoss: 0.5878193378448486\n",
      "cnt: 0 - valLoss: 0.586300790309906 - trainLoss: 0.5878079533576965\n",
      "cnt: 0 - valLoss: 0.5862928032875061 - trainLoss: 0.5877967476844788\n",
      "cnt: 0 - valLoss: 0.5862848162651062 - trainLoss: 0.5877847075462341\n",
      "cnt: 0 - valLoss: 0.5862767696380615 - trainLoss: 0.5877727270126343\n",
      "cnt: 0 - valLoss: 0.5862688422203064 - trainLoss: 0.5877607464790344\n",
      "cnt: 0 - valLoss: 0.5862608551979065 - trainLoss: 0.5877487659454346\n",
      "cnt: 0 - valLoss: 0.5862528681755066 - trainLoss: 0.5877368450164795\n",
      "cnt: 0 - valLoss: 0.5862450003623962 - trainLoss: 0.5877248048782349\n",
      "cnt: 0 - valLoss: 0.5862399935722351 - trainLoss: 0.5877130627632141\n",
      "cnt: 0 - valLoss: 0.58623206615448 - trainLoss: 0.587702751159668\n",
      "cnt: 0 - valLoss: 0.5862240791320801 - trainLoss: 0.5876903533935547\n",
      "cnt: 0 - valLoss: 0.5862160325050354 - trainLoss: 0.5876783132553101\n",
      "cnt: 0 - valLoss: 0.5862079858779907 - trainLoss: 0.5876663327217102\n",
      "cnt: 0 - valLoss: 0.5861999988555908 - trainLoss: 0.5876542925834656\n",
      "cnt: 0 - valLoss: 0.5861920118331909 - trainLoss: 0.5876423120498657\n",
      "cnt: 0 - valLoss: 0.586184561252594 - trainLoss: 0.5876303315162659\n",
      "cnt: 0 - valLoss: 0.5861784815788269 - trainLoss: 0.587619423866272\n",
      "cnt: 0 - valLoss: 0.5861704349517822 - trainLoss: 0.5876078009605408\n",
      "cnt: 0 - valLoss: 0.5861624479293823 - trainLoss: 0.5875957012176514\n",
      "cnt: 0 - valLoss: 0.5861544013023376 - trainLoss: 0.587583601474762\n",
      "cnt: 0 - valLoss: 0.5861462950706482 - trainLoss: 0.5875715613365173\n",
      "cnt: 0 - valLoss: 0.5861383676528931 - trainLoss: 0.5875595808029175\n",
      "cnt: 0 - valLoss: 0.5861306190490723 - trainLoss: 0.5875474810600281\n",
      "cnt: 0 - valLoss: 0.5861250758171082 - trainLoss: 0.5875361561775208\n",
      "cnt: 0 - valLoss: 0.5861174464225769 - trainLoss: 0.5875251889228821\n",
      "cnt: 0 - valLoss: 0.5861093997955322 - trainLoss: 0.5875129103660583\n",
      "cnt: 0 - valLoss: 0.5861012935638428 - trainLoss: 0.5875008702278137\n",
      "cnt: 0 - valLoss: 0.5860932469367981 - trainLoss: 0.5874888300895691\n",
      "cnt: 0 - valLoss: 0.5860852003097534 - trainLoss: 0.5874767303466797\n",
      "cnt: 0 - valLoss: 0.5860775709152222 - trainLoss: 0.5874646306037903\n",
      "cnt: 0 - valLoss: 0.5860716700553894 - trainLoss: 0.5874534249305725\n",
      "cnt: 0 - valLoss: 0.5860635638237 - trainLoss: 0.5874420404434204\n",
      "cnt: 0 - valLoss: 0.5860555171966553 - trainLoss: 0.5874298810958862\n",
      "cnt: 0 - valLoss: 0.5860474109649658 - trainLoss: 0.5874177813529968\n",
      "cnt: 0 - valLoss: 0.5860393643379211 - trainLoss: 0.5874056220054626\n",
      "cnt: 0 - valLoss: 0.5860313177108765 - trainLoss: 0.5873935222625732\n",
      "cnt: 0 - valLoss: 0.5860236883163452 - trainLoss: 0.5873814225196838\n",
      "cnt: 0 - valLoss: 0.5860177278518677 - trainLoss: 0.5873703360557556\n",
      "cnt: 0 - valLoss: 0.5860096216201782 - trainLoss: 0.587358832359314\n",
      "cnt: 0 - valLoss: 0.5860015153884888 - trainLoss: 0.587346613407135\n",
      "cnt: 0 - valLoss: 0.5859934687614441 - trainLoss: 0.587334394454956\n",
      "cnt: 0 - valLoss: 0.5859860777854919 - trainLoss: 0.5873222947120667\n",
      "cnt: 0 - valLoss: 0.5859781503677368 - trainLoss: 0.587310254573822\n",
      "cnt: 0 - valLoss: 0.5859725475311279 - trainLoss: 0.5872983336448669\n",
      "cnt: 0 - valLoss: 0.5859644412994385 - trainLoss: 0.5872876644134521\n",
      "cnt: 0 - valLoss: 0.585956335067749 - trainLoss: 0.5872753262519836\n",
      "cnt: 0 - valLoss: 0.5859482288360596 - trainLoss: 0.5872631669044495\n",
      "cnt: 0 - valLoss: 0.5859401822090149 - trainLoss: 0.5872509479522705\n",
      "cnt: 0 - valLoss: 0.5859320759773254 - trainLoss: 0.5872387886047363\n",
      "cnt: 0 - valLoss: 0.5859244465827942 - trainLoss: 0.5872266292572021\n",
      "cnt: 0 - valLoss: 0.5859184861183167 - trainLoss: 0.5872157216072083\n",
      "cnt: 0 - valLoss: 0.5859103798866272 - trainLoss: 0.5872039794921875\n",
      "cnt: 0 - valLoss: 0.5859022736549377 - trainLoss: 0.5871917605400085\n",
      "cnt: 0 - valLoss: 0.5858941078186035 - trainLoss: 0.5871795415878296\n",
      "cnt: 0 - valLoss: 0.5858860015869141 - trainLoss: 0.5871673226356506\n",
      "cnt: 0 - valLoss: 0.5858781337738037 - trainLoss: 0.5871551036834717\n",
      "cnt: 0 - valLoss: 0.5858725309371948 - trainLoss: 0.5871434807777405\n",
      "cnt: 0 - valLoss: 0.5858650803565979 - trainLoss: 0.5871325135231018\n",
      "cnt: 0 - valLoss: 0.5858569145202637 - trainLoss: 0.5871202349662781\n",
      "cnt: 0 - valLoss: 0.5858487486839294 - trainLoss: 0.5871079564094543\n",
      "cnt: 0 - valLoss: 0.5858407616615295 - trainLoss: 0.5870957374572754\n",
      "cnt: 0 - valLoss: 0.585832953453064 - trainLoss: 0.5870835781097412\n",
      "cnt: 0 - valLoss: 0.5858275294303894 - trainLoss: 0.5870718359947205\n",
      "cnt: 0 - valLoss: 0.5858194231987 - trainLoss: 0.5870609283447266\n",
      "cnt: 0 - valLoss: 0.5858113765716553 - trainLoss: 0.5870485305786133\n",
      "cnt: 0 - valLoss: 0.5858033299446106 - trainLoss: 0.5870363116264343\n",
      "cnt: 0 - valLoss: 0.5857953429222107 - trainLoss: 0.5870240330696106\n",
      "cnt: 0 - valLoss: 0.5857874751091003 - trainLoss: 0.5870117545127869\n",
      "cnt: 0 - valLoss: 0.5857821106910706 - trainLoss: 0.5869998931884766\n",
      "cnt: 0 - valLoss: 0.5857739448547363 - trainLoss: 0.586989164352417\n",
      "cnt: 0 - valLoss: 0.5857658982276917 - trainLoss: 0.5869767069816589\n",
      "cnt: 0 - valLoss: 0.585757851600647 - trainLoss: 0.5869643688201904\n",
      "cnt: 0 - valLoss: 0.5857498049736023 - trainLoss: 0.5869521498680115\n",
      "cnt: 0 - valLoss: 0.5857419967651367 - trainLoss: 0.5869399309158325\n",
      "cnt: 0 - valLoss: 0.5857365727424622 - trainLoss: 0.586928129196167\n",
      "cnt: 0 - valLoss: 0.5857284069061279 - trainLoss: 0.5869172215461731\n",
      "cnt: 0 - valLoss: 0.5857203602790833 - trainLoss: 0.5869048237800598\n",
      "cnt: 0 - valLoss: 0.5857123136520386 - trainLoss: 0.5868925452232361\n",
      "cnt: 0 - valLoss: 0.5857042670249939 - trainLoss: 0.5868801474571228\n",
      "cnt: 0 - valLoss: 0.5856974720954895 - trainLoss: 0.5868679285049438\n",
      "cnt: 0 - valLoss: 0.5856917500495911 - trainLoss: 0.5868568420410156\n",
      "cnt: 0 - valLoss: 0.5856835842132568 - trainLoss: 0.5868452191352844\n",
      "cnt: 0 - valLoss: 0.5856754183769226 - trainLoss: 0.5868328213691711\n",
      "cnt: 0 - valLoss: 0.5856673717498779 - trainLoss: 0.5868204236030579\n",
      "cnt: 0 - valLoss: 0.585659384727478 - trainLoss: 0.5868080854415894\n",
      "cnt: 0 - valLoss: 0.5856541991233826 - trainLoss: 0.5867958664894104\n",
      "cnt: 0 - valLoss: 0.5856460332870483 - trainLoss: 0.5867855548858643\n",
      "cnt: 0 - valLoss: 0.5856378078460693 - trainLoss: 0.5867729783058167\n",
      "cnt: 0 - valLoss: 0.5856297612190247 - trainLoss: 0.5867605805397034\n",
      "cnt: 0 - valLoss: 0.5856216549873352 - trainLoss: 0.5867481827735901\n",
      "cnt: 0 - valLoss: 0.5856141448020935 - trainLoss: 0.586735725402832\n",
      "cnt: 0 - valLoss: 0.5856083035469055 - trainLoss: 0.5867248177528381\n",
      "cnt: 0 - valLoss: 0.5856001973152161 - trainLoss: 0.5867129564285278\n",
      "cnt: 0 - valLoss: 0.5855920314788818 - trainLoss: 0.5867004990577698\n",
      "cnt: 0 - valLoss: 0.5855838656425476 - trainLoss: 0.5866880416870117\n",
      "cnt: 0 - valLoss: 0.5855762362480164 - trainLoss: 0.5866756439208984\n",
      "cnt: 0 - valLoss: 0.585570752620697 - trainLoss: 0.5866641402244568\n",
      "cnt: 0 - valLoss: 0.5855631232261658 - trainLoss: 0.5866529941558838\n",
      "cnt: 0 - valLoss: 0.5855550169944763 - trainLoss: 0.5866404175758362\n",
      "cnt: 0 - valLoss: 0.5855468511581421 - trainLoss: 0.5866279006004333\n",
      "cnt: 0 - valLoss: 0.5855391621589661 - trainLoss: 0.5866153836250305\n",
      "cnt: 0 - valLoss: 0.5855335593223572 - trainLoss: 0.5866039991378784\n",
      "cnt: 0 - valLoss: 0.5855253338813782 - trainLoss: 0.5865927338600159\n",
      "cnt: 0 - valLoss: 0.5855171084403992 - trainLoss: 0.5865800976753235\n",
      "cnt: 0 - valLoss: 0.5855089426040649 - trainLoss: 0.5865676403045654\n",
      "cnt: 0 - valLoss: 0.5855011940002441 - trainLoss: 0.5865551233291626\n",
      "cnt: 0 - valLoss: 0.5854957103729248 - trainLoss: 0.5865435600280762\n",
      "cnt: 0 - valLoss: 0.5854874849319458 - trainLoss: 0.5865324139595032\n",
      "cnt: 0 - valLoss: 0.5854793190956116 - trainLoss: 0.5865197777748108\n",
      "cnt: 0 - valLoss: 0.5854710936546326 - trainLoss: 0.586507260799408\n",
      "cnt: 0 - valLoss: 0.5854634046554565 - trainLoss: 0.5864947438240051\n",
      "cnt: 0 - valLoss: 0.5854578614234924 - trainLoss: 0.5864832401275635\n",
      "cnt: 0 - valLoss: 0.5854496359825134 - trainLoss: 0.5864720344543457\n",
      "cnt: 0 - valLoss: 0.5854414105415344 - trainLoss: 0.5864593386650085\n",
      "cnt: 0 - valLoss: 0.5854331851005554 - trainLoss: 0.5864468216896057\n",
      "cnt: 0 - valLoss: 0.5854254961013794 - trainLoss: 0.5864343047142029\n",
      "cnt: 0 - valLoss: 0.5854201316833496 - trainLoss: 0.5864228010177612\n",
      "cnt: 0 - valLoss: 0.5854125022888184 - trainLoss: 0.5864116549491882\n",
      "cnt: 0 - valLoss: 0.5854042768478394 - trainLoss: 0.5863989591598511\n",
      "cnt: 0 - valLoss: 0.5853960514068604 - trainLoss: 0.5863863825798035\n",
      "cnt: 0 - valLoss: 0.5853885412216187 - trainLoss: 0.5863739252090454\n",
      "cnt: 0 - valLoss: 0.585382878780365 - trainLoss: 0.5863627791404724\n",
      "cnt: 0 - valLoss: 0.585374653339386 - trainLoss: 0.5863511562347412\n",
      "cnt: 0 - valLoss: 0.585366427898407 - trainLoss: 0.586338460445404\n",
      "cnt: 0 - valLoss: 0.5853582620620728 - trainLoss: 0.5863258838653564\n",
      "cnt: 0 - valLoss: 0.5853508114814758 - trainLoss: 0.5863133668899536\n",
      "cnt: 0 - valLoss: 0.5853450894355774 - trainLoss: 0.5863024592399597\n",
      "cnt: 0 - valLoss: 0.5853367447853088 - trainLoss: 0.5862905383110046\n",
      "cnt: 0 - valLoss: 0.5853285193443298 - trainLoss: 0.5862779021263123\n",
      "cnt: 0 - valLoss: 0.5853202939033508 - trainLoss: 0.5862653255462646\n",
      "cnt: 0 - valLoss: 0.5853129029273987 - trainLoss: 0.586252748966217\n",
      "cnt: 0 - valLoss: 0.5853071212768555 - trainLoss: 0.5862420797348022\n",
      "cnt: 0 - valLoss: 0.5852988362312317 - trainLoss: 0.5862299203872681\n",
      "cnt: 0 - valLoss: 0.5852906107902527 - trainLoss: 0.5862172245979309\n",
      "cnt: 0 - valLoss: 0.5852824449539185 - trainLoss: 0.5862046480178833\n",
      "cnt: 0 - valLoss: 0.5852774381637573 - trainLoss: 0.5861921906471252\n",
      "cnt: 0 - valLoss: 0.5852691531181335 - trainLoss: 0.5861818194389343\n",
      "cnt: 0 - valLoss: 0.5852608680725098 - trainLoss: 0.5861690640449524\n",
      "cnt: 0 - valLoss: 0.585252583026886 - trainLoss: 0.5861563682556152\n",
      "cnt: 0 - valLoss: 0.5852448344230652 - trainLoss: 0.5861437320709229\n",
      "cnt: 0 - valLoss: 0.5852397084236145 - trainLoss: 0.5861321687698364\n",
      "cnt: 0 - valLoss: 0.5852317810058594 - trainLoss: 0.586121141910553\n",
      "cnt: 0 - valLoss: 0.5852234959602356 - trainLoss: 0.5861082673072815\n",
      "cnt: 0 - valLoss: 0.5852152109146118 - trainLoss: 0.5860955715179443\n",
      "cnt: 0 - valLoss: 0.5852078795433044 - trainLoss: 0.5860828757286072\n",
      "cnt: 0 - valLoss: 0.5852020382881165 - trainLoss: 0.5860723257064819\n",
      "cnt: 0 - valLoss: 0.5851936936378479 - trainLoss: 0.5860600471496582\n",
      "cnt: 0 - valLoss: 0.5851854681968689 - trainLoss: 0.5860472321510315\n",
      "cnt: 0 - valLoss: 0.585177481174469 - trainLoss: 0.5860345959663391\n",
      "cnt: 0 - valLoss: 0.5851722359657288 - trainLoss: 0.5860226154327393\n",
      "cnt: 0 - valLoss: 0.585163950920105 - trainLoss: 0.5860117077827454\n",
      "cnt: 0 - valLoss: 0.5851555466651917 - trainLoss: 0.5859988927841187\n",
      "cnt: 0 - valLoss: 0.5851473212242126 - trainLoss: 0.5859861373901367\n",
      "cnt: 0 - valLoss: 0.5851398706436157 - trainLoss: 0.5859735012054443\n",
      "cnt: 0 - valLoss: 0.5851342082023621 - trainLoss: 0.5859627723693848\n",
      "cnt: 0 - valLoss: 0.5851258039474487 - trainLoss: 0.5859505534172058\n",
      "cnt: 0 - valLoss: 0.5851174592971802 - trainLoss: 0.5859377384185791\n",
      "cnt: 0 - valLoss: 0.5851094126701355 - trainLoss: 0.5859250426292419\n",
      "cnt: 0 - valLoss: 0.5851042866706848 - trainLoss: 0.5859130024909973\n",
      "cnt: 0 - valLoss: 0.5850958824157715 - trainLoss: 0.585902214050293\n",
      "cnt: 0 - valLoss: 0.5850875377655029 - trainLoss: 0.5858892798423767\n",
      "cnt: 0 - valLoss: 0.5850791931152344 - trainLoss: 0.5858765244483948\n",
      "cnt: 0 - valLoss: 0.5850719213485718 - trainLoss: 0.5858637690544128\n",
      "cnt: 0 - valLoss: 0.5850660800933838 - trainLoss: 0.5858532786369324\n",
      "cnt: 0 - valLoss: 0.585058331489563 - trainLoss: 0.5858408808708191\n",
      "cnt: 0 - valLoss: 0.5850499272346497 - trainLoss: 0.5858280062675476\n",
      "cnt: 0 - valLoss: 0.5850422978401184 - trainLoss: 0.5858151912689209\n",
      "cnt: 0 - valLoss: 0.5850368142127991 - trainLoss: 0.5858039259910583\n",
      "cnt: 0 - valLoss: 0.585028350353241 - trainLoss: 0.5857923030853271\n",
      "cnt: 0 - valLoss: 0.5850200653076172 - trainLoss: 0.5857793688774109\n",
      "cnt: 0 - valLoss: 0.5850119590759277 - trainLoss: 0.5857665538787842\n",
      "cnt: 0 - valLoss: 0.5850068926811218 - trainLoss: 0.5857544541358948\n",
      "cnt: 0 - valLoss: 0.584998369216919 - trainLoss: 0.5857436060905457\n",
      "cnt: 0 - valLoss: 0.5849900245666504 - trainLoss: 0.5857306122779846\n",
      "cnt: 0 - valLoss: 0.5849817395210266 - trainLoss: 0.5857177972793579\n",
      "cnt: 0 - valLoss: 0.5849769115447998 - trainLoss: 0.5857051610946655\n",
      "cnt: 0 - valLoss: 0.5849684476852417 - trainLoss: 0.5856947898864746\n",
      "cnt: 0 - valLoss: 0.5849599838256836 - trainLoss: 0.5856818556785583\n",
      "cnt: 0 - valLoss: 0.5849515795707703 - trainLoss: 0.5856689810752869\n",
      "cnt: 0 - valLoss: 0.5849443078041077 - trainLoss: 0.5856561064720154\n",
      "cnt: 0 - valLoss: 0.5849384069442749 - trainLoss: 0.5856456756591797\n",
      "cnt: 0 - valLoss: 0.584929883480072 - trainLoss: 0.5856330990791321\n",
      "cnt: 0 - valLoss: 0.5849213600158691 - trainLoss: 0.585620105266571\n",
      "cnt: 0 - valLoss: 0.5849137306213379 - trainLoss: 0.58560711145401\n",
      "cnt: 0 - valLoss: 0.5849080681800842 - trainLoss: 0.585595965385437\n",
      "cnt: 0 - valLoss: 0.5848995447158813 - trainLoss: 0.5855839848518372\n",
      "cnt: 0 - valLoss: 0.5848909616470337 - trainLoss: 0.5855709314346313\n",
      "cnt: 0 - valLoss: 0.5848830938339233 - trainLoss: 0.5855578780174255\n",
      "cnt: 0 - valLoss: 0.5848777294158936 - trainLoss: 0.5855461955070496\n",
      "cnt: 0 - valLoss: 0.5848691463470459 - trainLoss: 0.5855347514152527\n",
      "cnt: 0 - valLoss: 0.584860622882843 - trainLoss: 0.5855215787887573\n",
      "cnt: 0 - valLoss: 0.5848525762557983 - trainLoss: 0.5855085253715515\n",
      "cnt: 0 - valLoss: 0.5848475098609924 - trainLoss: 0.5854966640472412\n",
      "cnt: 0 - valLoss: 0.584839403629303 - trainLoss: 0.5854854583740234\n",
      "cnt: 0 - valLoss: 0.5848307013511658 - trainLoss: 0.5854721665382385\n",
      "cnt: 0 - valLoss: 0.5848228931427002 - trainLoss: 0.5854590535163879\n",
      "cnt: 0 - valLoss: 0.5848175287246704 - trainLoss: 0.5854474902153015\n",
      "cnt: 0 - valLoss: 0.584808886051178 - trainLoss: 0.5854358673095703\n",
      "cnt: 0 - valLoss: 0.5848003029823303 - trainLoss: 0.5854227542877197\n",
      "cnt: 0 - valLoss: 0.5847923159599304 - trainLoss: 0.5854096412658691\n",
      "cnt: 0 - valLoss: 0.5847870111465454 - trainLoss: 0.5853979587554932\n",
      "cnt: 0 - valLoss: 0.584778368473053 - trainLoss: 0.5853863954544067\n",
      "cnt: 0 - valLoss: 0.5847697854042053 - trainLoss: 0.5853732228279114\n",
      "cnt: 0 - valLoss: 0.5847618579864502 - trainLoss: 0.5853601098060608\n",
      "cnt: 0 - valLoss: 0.5847564935684204 - trainLoss: 0.5853485465049744\n",
      "cnt: 0 - valLoss: 0.584747850894928 - trainLoss: 0.5853368043899536\n",
      "cnt: 0 - valLoss: 0.5847392082214355 - trainLoss: 0.5853236317634583\n",
      "cnt: 0 - valLoss: 0.5847313404083252 - trainLoss: 0.5853105187416077\n",
      "cnt: 0 - valLoss: 0.5847258567810059 - trainLoss: 0.5852990746498108\n",
      "cnt: 0 - valLoss: 0.5847172141075134 - trainLoss: 0.5852872729301453\n",
      "cnt: 0 - valLoss: 0.584708571434021 - trainLoss: 0.5852739810943604\n",
      "cnt: 0 - valLoss: 0.5847009420394897 - trainLoss: 0.5852608680725098\n",
      "cnt: 0 - valLoss: 0.5846953392028809 - trainLoss: 0.5852497816085815\n",
      "cnt: 0 - valLoss: 0.5846866369247437 - trainLoss: 0.5852375626564026\n",
      "cnt: 0 - valLoss: 0.5846779346466064 - trainLoss: 0.5852242708206177\n",
      "cnt: 0 - valLoss: 0.5846704840660095 - trainLoss: 0.5852111577987671\n",
      "cnt: 0 - valLoss: 0.5846647620201111 - trainLoss: 0.5852003693580627\n",
      "cnt: 0 - valLoss: 0.5846560001373291 - trainLoss: 0.5851877927780151\n",
      "cnt: 0 - valLoss: 0.5846473574638367 - trainLoss: 0.5851746201515198\n",
      "cnt: 0 - valLoss: 0.5846399664878845 - trainLoss: 0.5851613879203796\n",
      "cnt: 0 - valLoss: 0.5846340656280518 - trainLoss: 0.585150957107544\n",
      "cnt: 0 - valLoss: 0.5846253037452698 - trainLoss: 0.5851380228996277\n",
      "cnt: 0 - valLoss: 0.5846167802810669 - trainLoss: 0.5851247310638428\n",
      "cnt: 0 - valLoss: 0.5846123099327087 - trainLoss: 0.5851117968559265\n",
      "cnt: 0 - valLoss: 0.5846039056777954 - trainLoss: 0.5851015448570251\n",
      "cnt: 0 - valLoss: 0.5845952033996582 - trainLoss: 0.5850879549980164\n",
      "cnt: 0 - valLoss: 0.5845872759819031 - trainLoss: 0.5850747227668762\n",
      "cnt: 0 - valLoss: 0.5845819115638733 - trainLoss: 0.5850630402565002\n",
      "cnt: 0 - valLoss: 0.5845731496810913 - trainLoss: 0.5850513577461243\n",
      "cnt: 0 - valLoss: 0.5845644474029541 - trainLoss: 0.5850380063056946\n",
      "cnt: 0 - valLoss: 0.5845568776130676 - trainLoss: 0.5850246548652649\n",
      "cnt: 0 - valLoss: 0.5845511555671692 - trainLoss: 0.5850139260292053\n",
      "cnt: 0 - valLoss: 0.5845430493354797 - trainLoss: 0.5850011110305786\n",
      "cnt: 0 - valLoss: 0.5845350027084351 - trainLoss: 0.5849882364273071\n",
      "cnt: 0 - valLoss: 0.5845261216163635 - trainLoss: 0.5849754810333252\n",
      "cnt: 0 - valLoss: 0.5845186710357666 - trainLoss: 0.5849621295928955\n",
      "cnt: 0 - valLoss: 0.5845129489898682 - trainLoss: 0.5849515199661255\n",
      "cnt: 0 - valLoss: 0.5845047831535339 - trainLoss: 0.5849380493164062\n",
      "cnt: 0 - valLoss: 0.5844966769218445 - trainLoss: 0.5849251747131348\n",
      "cnt: 0 - valLoss: 0.5844886302947998 - trainLoss: 0.5849124193191528\n",
      "cnt: 0 - valLoss: 0.5844805240631104 - trainLoss: 0.5848995447158813\n",
      "cnt: 0 - valLoss: 0.5844725966453552 - trainLoss: 0.5848867297172546\n",
      "cnt: 0 - valLoss: 0.5844689011573792 - trainLoss: 0.5848742127418518\n",
      "cnt: 0 - valLoss: 0.5844608545303345 - trainLoss: 0.5848625898361206\n",
      "cnt: 0 - valLoss: 0.5844528675079346 - trainLoss: 0.5848497748374939\n",
      "cnt: 0 - valLoss: 0.5844448804855347 - trainLoss: 0.5848369598388672\n",
      "cnt: 0 - valLoss: 0.58443683385849 - trainLoss: 0.5848241448402405\n",
      "cnt: 0 - valLoss: 0.5844288468360901 - trainLoss: 0.5848113298416138\n",
      "cnt: 0 - valLoss: 0.5844208598136902 - trainLoss: 0.5847985744476318\n",
      "cnt: 0 - valLoss: 0.5844130516052246 - trainLoss: 0.5847858190536499\n",
      "cnt: 0 - valLoss: 0.5844093561172485 - trainLoss: 0.5847733616828918\n",
      "cnt: 0 - valLoss: 0.5844012498855591 - trainLoss: 0.5847617983818054\n",
      "cnt: 0 - valLoss: 0.5843932032585144 - trainLoss: 0.5847489237785339\n",
      "cnt: 0 - valLoss: 0.5843851566314697 - trainLoss: 0.5847360491752625\n",
      "cnt: 0 - valLoss: 0.584377110004425 - trainLoss: 0.5847232341766357\n",
      "cnt: 0 - valLoss: 0.5843691229820251 - trainLoss: 0.5847103595733643\n",
      "cnt: 0 - valLoss: 0.5843611359596252 - trainLoss: 0.5846975445747375\n",
      "cnt: 0 - valLoss: 0.5843535661697388 - trainLoss: 0.5846847891807556\n",
      "cnt: 0 - valLoss: 0.5843495726585388 - trainLoss: 0.5846728682518005\n",
      "cnt: 0 - valLoss: 0.5843414664268494 - trainLoss: 0.5846607089042664\n",
      "cnt: 0 - valLoss: 0.5843333601951599 - trainLoss: 0.5846477746963501\n",
      "cnt: 0 - valLoss: 0.5843253135681152 - trainLoss: 0.5846349596977234\n",
      "cnt: 0 - valLoss: 0.5843172669410706 - trainLoss: 0.5846221446990967\n",
      "cnt: 0 - valLoss: 0.5843091607093811 - trainLoss: 0.5846092104911804\n",
      "cnt: 0 - valLoss: 0.5843012928962708 - trainLoss: 0.5845963358879089\n",
      "cnt: 0 - valLoss: 0.584297776222229 - trainLoss: 0.5845836997032166\n",
      "cnt: 0 - valLoss: 0.58428955078125 - trainLoss: 0.5845723152160645\n",
      "cnt: 0 - valLoss: 0.5842813849449158 - trainLoss: 0.584559440612793\n",
      "cnt: 0 - valLoss: 0.5842733383178711 - trainLoss: 0.5845464468002319\n",
      "cnt: 0 - valLoss: 0.5842652916908264 - trainLoss: 0.5845335721969604\n",
      "cnt: 0 - valLoss: 0.584257185459137 - trainLoss: 0.584520697593689\n",
      "cnt: 0 - valLoss: 0.5842491984367371 - trainLoss: 0.5845078229904175\n",
      "cnt: 0 - valLoss: 0.5842458009719849 - trainLoss: 0.5844950079917908\n",
      "cnt: 0 - valLoss: 0.5842381119728088 - trainLoss: 0.584483802318573\n",
      "cnt: 0 - valLoss: 0.5842299461364746 - trainLoss: 0.5844706892967224\n",
      "cnt: 0 - valLoss: 0.5842218995094299 - trainLoss: 0.5844577550888062\n",
      "cnt: 0 - valLoss: 0.5842137932777405 - trainLoss: 0.5844448208808899\n",
      "cnt: 0 - valLoss: 0.5842056274414062 - trainLoss: 0.5844318866729736\n",
      "cnt: 0 - valLoss: 0.5841978192329407 - trainLoss: 0.5844190120697021\n",
      "cnt: 0 - valLoss: 0.5841941833496094 - trainLoss: 0.5844066739082336\n",
      "cnt: 0 - valLoss: 0.5841859579086304 - trainLoss: 0.5843949913978577\n",
      "cnt: 0 - valLoss: 0.5841777920722961 - trainLoss: 0.5843819379806519\n",
      "cnt: 0 - valLoss: 0.5841696262359619 - trainLoss: 0.5843689441680908\n",
      "cnt: 0 - valLoss: 0.5841614603996277 - trainLoss: 0.5843560695648193\n",
      "cnt: 0 - valLoss: 0.5841533541679382 - trainLoss: 0.5843430757522583\n",
      "cnt: 0 - valLoss: 0.584145724773407 - trainLoss: 0.584330141544342\n",
      "cnt: 0 - valLoss: 0.5841419100761414 - trainLoss: 0.5843181610107422\n",
      "cnt: 0 - valLoss: 0.5841336250305176 - trainLoss: 0.584306001663208\n",
      "cnt: 0 - valLoss: 0.5841254591941833 - trainLoss: 0.5842929482460022\n",
      "cnt: 0 - valLoss: 0.5841172337532043 - trainLoss: 0.5842799544334412\n",
      "cnt: 0 - valLoss: 0.5841091275215149 - trainLoss: 0.5842670202255249\n",
      "cnt: 0 - valLoss: 0.5841009616851807 - trainLoss: 0.5842540264129639\n",
      "cnt: 0 - valLoss: 0.584097683429718 - trainLoss: 0.5842410922050476\n",
      "cnt: 0 - valLoss: 0.5840892791748047 - trainLoss: 0.5842299461364746\n",
      "cnt: 0 - valLoss: 0.5840811133384705 - trainLoss: 0.584216833114624\n",
      "cnt: 0 - valLoss: 0.5840728878974915 - trainLoss: 0.5842037796974182\n",
      "cnt: 0 - valLoss: 0.5840647220611572 - trainLoss: 0.5841907262802124\n",
      "cnt: 0 - valLoss: 0.584056556224823 - trainLoss: 0.5841777324676514\n",
      "cnt: 0 - valLoss: 0.5840489864349365 - trainLoss: 0.5841646790504456\n",
      "cnt: 0 - valLoss: 0.5840450525283813 - trainLoss: 0.5841531157493591\n",
      "cnt: 0 - valLoss: 0.5840367674827576 - trainLoss: 0.5841405987739563\n",
      "cnt: 0 - valLoss: 0.5840285420417786 - trainLoss: 0.5841274857521057\n",
      "cnt: 0 - valLoss: 0.5840203166007996 - trainLoss: 0.5841144323348999\n",
      "cnt: 0 - valLoss: 0.5840121507644653 - trainLoss: 0.5841013789176941\n",
      "cnt: 0 - valLoss: 0.5840043425559998 - trainLoss: 0.5840883255004883\n",
      "cnt: 0 - valLoss: 0.5840006470680237 - trainLoss: 0.5840762257575989\n",
      "cnt: 0 - valLoss: 0.5839922428131104 - trainLoss: 0.5840641856193542\n",
      "cnt: 0 - valLoss: 0.5839840173721313 - trainLoss: 0.5840510129928589\n",
      "cnt: 0 - valLoss: 0.5839757323265076 - trainLoss: 0.5840379595756531\n",
      "cnt: 0 - valLoss: 0.5839675068855286 - trainLoss: 0.5840248465538025\n",
      "cnt: 0 - valLoss: 0.583959698677063 - trainLoss: 0.5840117335319519\n",
      "cnt: 0 - valLoss: 0.5839560627937317 - trainLoss: 0.583999514579773\n",
      "cnt: 0 - valLoss: 0.5839475989341736 - trainLoss: 0.5839876532554626\n",
      "cnt: 0 - valLoss: 0.5839391946792603 - trainLoss: 0.5839743614196777\n",
      "cnt: 0 - valLoss: 0.5839307904243469 - trainLoss: 0.5839611291885376\n",
      "cnt: 0 - valLoss: 0.5839224457740784 - trainLoss: 0.5839478969573975\n",
      "cnt: 0 - valLoss: 0.583914577960968 - trainLoss: 0.5839347243309021\n",
      "cnt: 0 - valLoss: 0.5839108228683472 - trainLoss: 0.5839225053787231\n",
      "cnt: 0 - valLoss: 0.5839023590087891 - trainLoss: 0.5839104652404785\n",
      "cnt: 0 - valLoss: 0.583893895149231 - trainLoss: 0.5838971734046936\n",
      "cnt: 0 - valLoss: 0.5838854908943176 - trainLoss: 0.5838838815689087\n",
      "cnt: 0 - valLoss: 0.5838770866394043 - trainLoss: 0.583870530128479\n",
      "cnt: 0 - valLoss: 0.5838693976402283 - trainLoss: 0.5838572382926941\n",
      "cnt: 0 - valLoss: 0.5838654041290283 - trainLoss: 0.5838453769683838\n",
      "cnt: 0 - valLoss: 0.5838568806648254 - trainLoss: 0.5838328003883362\n",
      "cnt: 0 - valLoss: 0.5838484764099121 - trainLoss: 0.5838193297386169\n",
      "cnt: 0 - valLoss: 0.583840012550354 - trainLoss: 0.583806037902832\n",
      "cnt: 0 - valLoss: 0.5838315486907959 - trainLoss: 0.5837926864624023\n",
      "cnt: 0 - valLoss: 0.583828330039978 - trainLoss: 0.5837793350219727\n",
      "cnt: 0 - valLoss: 0.5838198065757751 - trainLoss: 0.5837682485580444\n",
      "cnt: 0 - valLoss: 0.5838112831115723 - trainLoss: 0.5837547779083252\n",
      "cnt: 0 - valLoss: 0.5838028192520142 - trainLoss: 0.5837413668632507\n",
      "cnt: 0 - valLoss: 0.583794355392456 - trainLoss: 0.583728015422821\n",
      "cnt: 0 - valLoss: 0.5837864279747009 - trainLoss: 0.5837146639823914\n",
      "cnt: 0 - valLoss: 0.5837826728820801 - trainLoss: 0.5837023854255676\n",
      "cnt: 0 - valLoss: 0.5837740898132324 - trainLoss: 0.5836901664733887\n",
      "cnt: 0 - valLoss: 0.5837655663490295 - trainLoss: 0.5836766958236694\n",
      "cnt: 0 - valLoss: 0.5837571024894714 - trainLoss: 0.583663284778595\n",
      "cnt: 0 - valLoss: 0.5837486982345581 - trainLoss: 0.5836498141288757\n",
      "cnt: 0 - valLoss: 0.5837454199790955 - trainLoss: 0.5836367011070251\n",
      "cnt: 0 - valLoss: 0.583736777305603 - trainLoss: 0.5836253762245178\n",
      "cnt: 0 - valLoss: 0.5837282538414001 - trainLoss: 0.5836118459701538\n",
      "cnt: 0 - valLoss: 0.5837196707725525 - trainLoss: 0.5835983753204346\n",
      "cnt: 0 - valLoss: 0.5837112665176392 - trainLoss: 0.5835849642753601\n",
      "cnt: 0 - valLoss: 0.5837035179138184 - trainLoss: 0.5835715532302856\n",
      "cnt: 0 - valLoss: 0.5836995840072632 - trainLoss: 0.5835599303245544\n",
      "cnt: 0 - valLoss: 0.583690881729126 - trainLoss: 0.583547055721283\n",
      "cnt: 0 - valLoss: 0.5836822986602783 - trainLoss: 0.5835334658622742\n",
      "cnt: 0 - valLoss: 0.5836737751960754 - trainLoss: 0.5835199952125549\n",
      "cnt: 0 - valLoss: 0.5836658477783203 - trainLoss: 0.5835065245628357\n",
      "cnt: 0 - valLoss: 0.5836620926856995 - trainLoss: 0.5834943652153015\n",
      "cnt: 0 - valLoss: 0.5836533904075623 - trainLoss: 0.583482027053833\n",
      "cnt: 0 - valLoss: 0.5836448073387146 - trainLoss: 0.5834683775901794\n",
      "cnt: 0 - valLoss: 0.5836362242698669 - trainLoss: 0.5834549069404602\n",
      "cnt: 0 - valLoss: 0.5836281776428223 - trainLoss: 0.5834413766860962\n",
      "cnt: 0 - valLoss: 0.5836246013641357 - trainLoss: 0.5834289789199829\n",
      "cnt: 0 - valLoss: 0.5836159586906433 - trainLoss: 0.5834168791770935\n",
      "cnt: 0 - valLoss: 0.5836071968078613 - trainLoss: 0.5834032893180847\n",
      "cnt: 0 - valLoss: 0.5835986733436584 - trainLoss: 0.5833896994590759\n",
      "cnt: 0 - valLoss: 0.583590567111969 - trainLoss: 0.5833761692047119\n",
      "cnt: 0 - valLoss: 0.5835869312286377 - trainLoss: 0.5833637714385986\n",
      "cnt: 0 - valLoss: 0.5835782289505005 - trainLoss: 0.5833516716957092\n",
      "cnt: 0 - valLoss: 0.5835695862770081 - trainLoss: 0.5833379626274109\n",
      "cnt: 0 - valLoss: 0.5835610032081604 - trainLoss: 0.5833243727684021\n",
      "cnt: 0 - valLoss: 0.5835529565811157 - trainLoss: 0.5833108425140381\n",
      "cnt: 0 - valLoss: 0.5835492610931396 - trainLoss: 0.5832986235618591\n",
      "cnt: 0 - valLoss: 0.5835405588150024 - trainLoss: 0.5832862854003906\n",
      "cnt: 0 - valLoss: 0.5835318565368652 - trainLoss: 0.5832725763320923\n",
      "cnt: 0 - valLoss: 0.583523154258728 - trainLoss: 0.5832589864730835\n",
      "cnt: 0 - valLoss: 0.5835153460502625 - trainLoss: 0.5832454562187195\n",
      "cnt: 0 - valLoss: 0.583511471748352 - trainLoss: 0.5832334756851196\n",
      "cnt: 0 - valLoss: 0.5835027098655701 - trainLoss: 0.5832207798957825\n",
      "cnt: 0 - valLoss: 0.5834940075874329 - trainLoss: 0.5832070708274841\n",
      "cnt: 0 - valLoss: 0.5834853649139404 - trainLoss: 0.5831934213638306\n",
      "cnt: 0 - valLoss: 0.5834777355194092 - trainLoss: 0.5831798911094666\n",
      "cnt: 0 - valLoss: 0.5834736824035645 - trainLoss: 0.5831685066223145\n",
      "cnt: 0 - valLoss: 0.5834648609161377 - trainLoss: 0.5831553339958191\n",
      "cnt: 0 - valLoss: 0.5834560990333557 - trainLoss: 0.5831415057182312\n",
      "cnt: 0 - valLoss: 0.5834475755691528 - trainLoss: 0.5831279158592224\n",
      "cnt: 0 - valLoss: 0.5834444761276245 - trainLoss: 0.5831146240234375\n",
      "cnt: 0 - valLoss: 0.583435595035553 - trainLoss: 0.583103358745575\n",
      "cnt: 0 - valLoss: 0.583426833152771 - trainLoss: 0.5830895304679871\n",
      "cnt: 0 - valLoss: 0.583418071269989 - trainLoss: 0.5830758810043335\n",
      "cnt: 0 - valLoss: 0.5834100842475891 - trainLoss: 0.5830622315406799\n",
      "cnt: 0 - valLoss: 0.5834066867828369 - trainLoss: 0.5830499529838562\n",
      "cnt: 0 - valLoss: 0.5833980441093445 - trainLoss: 0.5830374360084534\n",
      "cnt: 0 - valLoss: 0.5833895206451416 - trainLoss: 0.5830235481262207\n",
      "cnt: 0 - valLoss: 0.583380937576294 - trainLoss: 0.5830098390579224\n",
      "cnt: 0 - valLoss: 0.5833736658096313 - trainLoss: 0.5829960107803345\n",
      "cnt: 0 - valLoss: 0.5833697319030762 - trainLoss: 0.5829846858978271\n",
      "cnt: 0 - valLoss: 0.5833610892295837 - trainLoss: 0.5829711556434631\n",
      "cnt: 0 - valLoss: 0.5833525061607361 - trainLoss: 0.5829572677612305\n",
      "cnt: 0 - valLoss: 0.5833444595336914 - trainLoss: 0.5829433798789978\n",
      "cnt: 0 - valLoss: 0.5833413004875183 - trainLoss: 0.5829306840896606\n",
      "cnt: 0 - valLoss: 0.5833325982093811 - trainLoss: 0.5829185843467712\n",
      "cnt: 0 - valLoss: 0.5833239555358887 - trainLoss: 0.582904577255249\n",
      "cnt: 0 - valLoss: 0.583315372467041 - trainLoss: 0.5828908085823059\n",
      "cnt: 0 - valLoss: 0.5833080410957336 - trainLoss: 0.5828770399093628\n",
      "cnt: 0 - valLoss: 0.5833040475845337 - trainLoss: 0.5828659534454346\n",
      "cnt: 0 - valLoss: 0.5832953453063965 - trainLoss: 0.5828523635864258\n",
      "cnt: 0 - valLoss: 0.5832865238189697 - trainLoss: 0.5828385949134827\n",
      "cnt: 0 - valLoss: 0.5832784175872803 - trainLoss: 0.5828246474266052\n",
      "cnt: 0 - valLoss: 0.5832748413085938 - trainLoss: 0.5828121304512024\n",
      "cnt: 0 - valLoss: 0.5832657814025879 - trainLoss: 0.5827994346618652\n",
      "cnt: 0 - valLoss: 0.5832563042640686 - trainLoss: 0.5827851295471191\n",
      "cnt: 0 - valLoss: 0.5832472443580627 - trainLoss: 0.5827705264091492\n",
      "cnt: 0 - valLoss: 0.5832434892654419 - trainLoss: 0.5827566385269165\n",
      "cnt: 0 - valLoss: 0.5832338929176331 - trainLoss: 0.5827441811561584\n",
      "cnt: 0 - valLoss: 0.5832241773605347 - trainLoss: 0.5827291011810303\n",
      "cnt: 0 - valLoss: 0.5832145810127258 - trainLoss: 0.5827140212059021\n",
      "cnt: 0 - valLoss: 0.5832062363624573 - trainLoss: 0.5826988816261292\n",
      "cnt: 0 - valLoss: 0.5832009315490723 - trainLoss: 0.5826863646507263\n",
      "cnt: 0 - valLoss: 0.5831905603408813 - trainLoss: 0.582671046257019\n",
      "cnt: 0 - valLoss: 0.58318030834198 - trainLoss: 0.5826553702354431\n",
      "cnt: 0 - valLoss: 0.5831708312034607 - trainLoss: 0.5826396346092224\n",
      "cnt: 0 - valLoss: 0.5831656455993652 - trainLoss: 0.5826254487037659\n",
      "cnt: 0 - valLoss: 0.5831557512283325 - trainLoss: 0.5826104283332825\n",
      "cnt: 0 - valLoss: 0.5831459164619446 - trainLoss: 0.5825946927070618\n",
      "cnt: 0 - valLoss: 0.583136796951294 - trainLoss: 0.5825792551040649\n",
      "cnt: 0 - valLoss: 0.5831324458122253 - trainLoss: 0.5825659036636353\n",
      "cnt: 0 - valLoss: 0.5831224918365479 - trainLoss: 0.5825523138046265\n",
      "cnt: 0 - valLoss: 0.5831127166748047 - trainLoss: 0.5825373530387878\n",
      "cnt: 0 - valLoss: 0.5831034779548645 - trainLoss: 0.5825222730636597\n",
      "cnt: 0 - valLoss: 0.5830991268157959 - trainLoss: 0.5825085043907166\n",
      "cnt: 0 - valLoss: 0.5830892324447632 - trainLoss: 0.5824950337409973\n",
      "cnt: 0 - valLoss: 0.58307945728302 - trainLoss: 0.5824798941612244\n",
      "cnt: 0 - valLoss: 0.5830702185630798 - trainLoss: 0.5824651122093201\n",
      "cnt: 0 - valLoss: 0.5830661654472351 - trainLoss: 0.5824514627456665\n",
      "cnt: 0 - valLoss: 0.5830562710762024 - trainLoss: 0.5824385285377502\n",
      "cnt: 0 - valLoss: 0.5830464363098145 - trainLoss: 0.5824235081672668\n",
      "cnt: 0 - valLoss: 0.5830371975898743 - trainLoss: 0.582408607006073\n",
      "cnt: 0 - valLoss: 0.5830332636833191 - trainLoss: 0.5823951363563538\n",
      "cnt: 0 - valLoss: 0.583023190498352 - trainLoss: 0.5823820233345032\n",
      "cnt: 0 - valLoss: 0.5830132961273193 - trainLoss: 0.5823668837547302\n",
      "cnt: 0 - valLoss: 0.5830041170120239 - trainLoss: 0.5823518633842468\n",
      "cnt: 0 - valLoss: 0.5829999446868896 - trainLoss: 0.5823382139205933\n",
      "cnt: 0 - valLoss: 0.5829899907112122 - trainLoss: 0.5823249816894531\n",
      "cnt: 0 - valLoss: 0.5829800367355347 - trainLoss: 0.5823098421096802\n",
      "cnt: 0 - valLoss: 0.5829708576202393 - trainLoss: 0.582294762134552\n",
      "cnt: 0 - valLoss: 0.582966685295105 - trainLoss: 0.5822813510894775\n",
      "cnt: 0 - valLoss: 0.5829566121101379 - trainLoss: 0.5822678804397583\n",
      "cnt: 0 - valLoss: 0.5829464197158813 - trainLoss: 0.5822526812553406\n",
      "cnt: 0 - valLoss: 0.5829373002052307 - trainLoss: 0.582237720489502\n",
      "cnt: 0 - valLoss: 0.5829326510429382 - trainLoss: 0.5822252035140991\n",
      "cnt: 0 - valLoss: 0.5829222798347473 - trainLoss: 0.5822116732597351\n",
      "cnt: 0 - valLoss: 0.5829125046730042 - trainLoss: 0.5821965932846069\n",
      "cnt: 0 - valLoss: 0.5829041004180908 - trainLoss: 0.5821815133094788\n",
      "cnt: 0 - valLoss: 0.5828996300697327 - trainLoss: 0.5821693539619446\n",
      "cnt: 0 - valLoss: 0.5828896164894104 - trainLoss: 0.5821547508239746\n",
      "cnt: 0 - valLoss: 0.5828801989555359 - trainLoss: 0.5821393728256226\n",
      "cnt: 0 - valLoss: 0.5828766822814941 - trainLoss: 0.5821248888969421\n",
      "cnt: 0 - valLoss: 0.5828665494918823 - trainLoss: 0.5821120738983154\n",
      "cnt: 0 - valLoss: 0.5828565955162048 - trainLoss: 0.5820963978767395\n",
      "cnt: 0 - valLoss: 0.5828477740287781 - trainLoss: 0.5820809006690979\n",
      "cnt: 0 - valLoss: 0.5828434824943542 - trainLoss: 0.5820677280426025\n",
      "cnt: 0 - valLoss: 0.5828333497047424 - trainLoss: 0.5820533633232117\n",
      "cnt: 0 - valLoss: 0.5828235745429993 - trainLoss: 0.5820373892784119\n",
      "cnt: 0 - valLoss: 0.5828201770782471 - trainLoss: 0.5820222496986389\n",
      "cnt: 0 - valLoss: 0.5828099846839905 - trainLoss: 0.582009494304657\n",
      "cnt: 0 - valLoss: 0.5827998518943787 - trainLoss: 0.581993579864502\n",
      "cnt: 0 - valLoss: 0.5827909708023071 - trainLoss: 0.5819776654243469\n",
      "cnt: 0 - valLoss: 0.5827865600585938 - trainLoss: 0.581964373588562\n",
      "cnt: 0 - valLoss: 0.5827762484550476 - trainLoss: 0.5819492936134338\n",
      "cnt: 0 - valLoss: 0.5827666521072388 - trainLoss: 0.5819328427314758\n",
      "cnt: 0 - valLoss: 0.5827628970146179 - trainLoss: 0.5819177627563477\n",
      "cnt: 0 - valLoss: 0.5827527642250061 - trainLoss: 0.5819039344787598\n",
      "cnt: 0 - valLoss: 0.5827431678771973 - trainLoss: 0.5818873643875122\n",
      "cnt: 0 - valLoss: 0.5827347040176392 - trainLoss: 0.5818703174591064\n",
      "cnt: 0 - valLoss: 0.5827298760414124 - trainLoss: 0.5818573832511902\n",
      "cnt: 0 - valLoss: 0.582719624042511 - trainLoss: 0.5818411111831665\n",
      "cnt: 0 - valLoss: 0.5827105045318604 - trainLoss: 0.5818246603012085\n",
      "cnt: 0 - valLoss: 0.5827063322067261 - trainLoss: 0.5818106532096863\n",
      "cnt: 0 - valLoss: 0.5826959013938904 - trainLoss: 0.5817958116531372\n",
      "cnt: 0 - valLoss: 0.5826863050460815 - trainLoss: 0.5817793011665344\n",
      "cnt: 0 - valLoss: 0.5826825499534607 - trainLoss: 0.5817643404006958\n",
      "cnt: 0 - valLoss: 0.582672119140625 - trainLoss: 0.5817504525184631\n",
      "cnt: 0 - valLoss: 0.5826621055603027 - trainLoss: 0.5817338228225708\n",
      "cnt: 0 - valLoss: 0.5826587677001953 - trainLoss: 0.5817180275917053\n",
      "cnt: 0 - valLoss: 0.5826480984687805 - trainLoss: 0.5817046165466309\n",
      "cnt: 0 - valLoss: 0.5826378464698792 - trainLoss: 0.5816875696182251\n",
      "cnt: 0 - valLoss: 0.582634687423706 - trainLoss: 0.5816709995269775\n",
      "cnt: 0 - valLoss: 0.5826240181922913 - trainLoss: 0.581658124923706\n",
      "cnt: 0 - valLoss: 0.5826135873794556 - trainLoss: 0.5816412568092346\n",
      "cnt: 0 - valLoss: 0.582610547542572 - trainLoss: 0.581624448299408\n",
      "cnt: 0 - valLoss: 0.5825998783111572 - trainLoss: 0.5816119909286499\n",
      "cnt: 0 - valLoss: 0.5825893878936768 - trainLoss: 0.5815950632095337\n",
      "cnt: 0 - valLoss: 0.5825808048248291 - trainLoss: 0.581578254699707\n",
      "cnt: 0 - valLoss: 0.5825759172439575 - trainLoss: 0.5815655589103699\n",
      "cnt: 0 - valLoss: 0.5825652480125427 - trainLoss: 0.5815489888191223\n",
      "cnt: 0 - valLoss: 0.5825564861297607 - trainLoss: 0.5815321207046509\n",
      "cnt: 0 - valLoss: 0.5825517773628235 - trainLoss: 0.5815191268920898\n",
      "cnt: 0 - valLoss: 0.5825411081314087 - trainLoss: 0.5815029144287109\n",
      "cnt: 0 - valLoss: 0.5825322270393372 - trainLoss: 0.5814861059188843\n",
      "cnt: 0 - valLoss: 0.5825275778770447 - trainLoss: 0.5814728736877441\n",
      "cnt: 0 - valLoss: 0.5825168490409851 - trainLoss: 0.5814567804336548\n",
      "cnt: 0 - valLoss: 0.5825081467628479 - trainLoss: 0.5814397931098938\n",
      "cnt: 0 - valLoss: 0.5825034379959106 - trainLoss: 0.5814269185066223\n",
      "cnt: 0 - valLoss: 0.5824939608573914 - trainLoss: 0.5814101696014404\n",
      "cnt: 0 - valLoss: 0.5824844837188721 - trainLoss: 0.5813939571380615\n",
      "cnt: 0 - valLoss: 0.5824826955795288 - trainLoss: 0.5813776850700378\n",
      "cnt: 0 - valLoss: 0.5824730396270752 - trainLoss: 0.5813639163970947\n",
      "cnt: 0 - valLoss: 0.5824634432792664 - trainLoss: 0.5813475251197815\n",
      "cnt: 0 - valLoss: 0.5824538469314575 - trainLoss: 0.581331193447113\n",
      "cnt: 0 - valLoss: 0.5824443101882935 - trainLoss: 0.5813148617744446\n",
      "cnt: 0 - valLoss: 0.5824358463287354 - trainLoss: 0.5812984704971313\n",
      "cnt: 0 - valLoss: 0.5824329257011414 - trainLoss: 0.5812842845916748\n",
      "cnt: 0 - valLoss: 0.5824231505393982 - trainLoss: 0.5812678337097168\n",
      "cnt: 0 - valLoss: 0.5824135541915894 - trainLoss: 0.5812510848045349\n",
      "cnt: 0 - valLoss: 0.5824038982391357 - trainLoss: 0.5812345147132874\n",
      "cnt: 0 - valLoss: 0.5824022889137268 - trainLoss: 0.5812181830406189\n",
      "cnt: 0 - valLoss: 0.5823928713798523 - trainLoss: 0.5812045931816101\n",
      "cnt: 0 - valLoss: 0.582383394241333 - trainLoss: 0.5811882019042969\n",
      "cnt: 0 - valLoss: 0.582373857498169 - trainLoss: 0.5811718106269836\n",
      "cnt: 0 - valLoss: 0.5823648571968079 - trainLoss: 0.5811553001403809\n",
      "cnt: 0 - valLoss: 0.582362949848175 - trainLoss: 0.5811396837234497\n",
      "cnt: 0 - valLoss: 0.5823532342910767 - trainLoss: 0.5811250805854797\n",
      "cnt: 0 - valLoss: 0.5823434591293335 - trainLoss: 0.5811084508895874\n",
      "cnt: 0 - valLoss: 0.5823336839675903 - trainLoss: 0.5810918211936951\n",
      "cnt: 0 - valLoss: 0.5823249816894531 - trainLoss: 0.5810751914978027\n",
      "cnt: 0 - valLoss: 0.5823224782943726 - trainLoss: 0.5810607671737671\n",
      "cnt: 0 - valLoss: 0.5823125243186951 - trainLoss: 0.5810448527336121\n",
      "cnt: 0 - valLoss: 0.5823026895523071 - trainLoss: 0.5810280442237854\n",
      "cnt: 0 - valLoss: 0.5822933316230774 - trainLoss: 0.5810113549232483\n",
      "cnt: 0 - valLoss: 0.5822914242744446 - trainLoss: 0.5809956789016724\n",
      "cnt: 0 - valLoss: 0.5822815299034119 - trainLoss: 0.5809811353683472\n",
      "cnt: 0 - valLoss: 0.5822716355323792 - trainLoss: 0.5809644460678101\n",
      "cnt: 0 - valLoss: 0.5822617411613464 - trainLoss: 0.5809478759765625\n",
      "cnt: 0 - valLoss: 0.582253098487854 - trainLoss: 0.5809313058853149\n",
      "cnt: 0 - valLoss: 0.5822505354881287 - trainLoss: 0.5809175372123718\n",
      "cnt: 0 - valLoss: 0.5822405815124512 - trainLoss: 0.5809014439582825\n",
      "cnt: 0 - valLoss: 0.5822306275367737 - trainLoss: 0.5808849334716797\n",
      "cnt: 0 - valLoss: 0.5822213888168335 - trainLoss: 0.5808683037757874\n",
      "cnt: 0 - valLoss: 0.5822192430496216 - trainLoss: 0.580853283405304\n",
      "cnt: 0 - valLoss: 0.5822092294692993 - trainLoss: 0.5808380842208862\n",
      "cnt: 0 - valLoss: 0.5821991562843323 - trainLoss: 0.5808213949203491\n",
      "cnt: 0 - valLoss: 0.5821897983551025 - trainLoss: 0.5808048248291016\n",
      "cnt: 0 - valLoss: 0.5821877717971802 - trainLoss: 0.5807896852493286\n",
      "cnt: 0 - valLoss: 0.5821776390075684 - trainLoss: 0.5807749629020691\n",
      "cnt: 0 - valLoss: 0.5821674466133118 - trainLoss: 0.5807582139968872\n",
      "cnt: 0 - valLoss: 0.5821581482887268 - trainLoss: 0.5807415246963501\n",
      "cnt: 0 - valLoss: 0.5821559429168701 - trainLoss: 0.5807266235351562\n",
      "cnt: 0 - valLoss: 0.5821454524993896 - trainLoss: 0.5807116031646729\n",
      "cnt: 0 - valLoss: 0.5821350812911987 - trainLoss: 0.5806945562362671\n",
      "cnt: 0 - valLoss: 0.5821258425712585 - trainLoss: 0.5806774497032166\n",
      "cnt: 0 - valLoss: 0.582123339176178 - trainLoss: 0.5806630253791809\n",
      "cnt: 0 - valLoss: 0.5821128487586975 - trainLoss: 0.5806466937065125\n",
      "cnt: 0 - valLoss: 0.5821026563644409 - trainLoss: 0.5806292295455933\n",
      "cnt: 0 - valLoss: 0.582101047039032 - trainLoss: 0.5806121826171875\n",
      "cnt: 0 - valLoss: 0.5820903182029724 - trainLoss: 0.5805980563163757\n",
      "cnt: 0 - valLoss: 0.5820795893669128 - trainLoss: 0.5805805325508118\n",
      "cnt: 0 - valLoss: 0.582069993019104 - trainLoss: 0.5805628895759583\n",
      "cnt: 0 - valLoss: 0.5820673108100891 - trainLoss: 0.580547571182251\n",
      "cnt: 0 - valLoss: 0.5820563435554504 - trainLoss: 0.5805314779281616\n",
      "cnt: 0 - valLoss: 0.5820455551147461 - trainLoss: 0.5805134773254395\n",
      "cnt: 0 - valLoss: 0.5820435285568237 - trainLoss: 0.5804961919784546\n",
      "cnt: 0 - valLoss: 0.5820323824882507 - trainLoss: 0.5804818272590637\n",
      "cnt: 0 - valLoss: 0.5820212364196777 - trainLoss: 0.5804637670516968\n",
      "cnt: 0 - valLoss: 0.5820114016532898 - trainLoss: 0.5804455280303955\n",
      "cnt: 0 - valLoss: 0.5820083022117615 - trainLoss: 0.5804298520088196\n",
      "cnt: 0 - valLoss: 0.5819964408874512 - trainLoss: 0.5804119110107422\n",
      "cnt: 0 - valLoss: 0.5819852948188782 - trainLoss: 0.5803928971290588\n",
      "cnt: 0 - valLoss: 0.5819825530052185 - trainLoss: 0.5803753137588501\n",
      "cnt: 0 - valLoss: 0.5819702744483948 - trainLoss: 0.5803580284118652\n",
      "cnt: 0 - valLoss: 0.581958532333374 - trainLoss: 0.5803380012512207\n",
      "cnt: 0 - valLoss: 0.5819563269615173 - trainLoss: 0.5803186893463135\n",
      "cnt: 0 - valLoss: 0.5819430351257324 - trainLoss: 0.5803009271621704\n",
      "cnt: 0 - valLoss: 0.5819306969642639 - trainLoss: 0.5802776217460632\n",
      "cnt: 0 - valLoss: 0.5819276571273804 - trainLoss: 0.5802561044692993\n",
      "cnt: 0 - valLoss: 0.5819144248962402 - trainLoss: 0.5802362561225891\n",
      "cnt: 0 - valLoss: 0.5819027423858643 - trainLoss: 0.5802131295204163\n",
      "cnt: 0 - valLoss: 0.5818992257118225 - trainLoss: 0.5801928639411926\n",
      "cnt: 0 - valLoss: 0.5818861722946167 - trainLoss: 0.580172061920166\n",
      "cnt: 0 - valLoss: 0.5818747282028198 - trainLoss: 0.5801491737365723\n",
      "cnt: 0 - valLoss: 0.5818709135055542 - trainLoss: 0.5801297426223755\n",
      "cnt: 0 - valLoss: 0.5818576812744141 - trainLoss: 0.5801081657409668\n",
      "cnt: 0 - valLoss: 0.5818467140197754 - trainLoss: 0.5800851583480835\n",
      "cnt: 0 - valLoss: 0.581842303276062 - trainLoss: 0.5800667405128479\n",
      "cnt: 0 - valLoss: 0.5818295478820801 - trainLoss: 0.580044150352478\n",
      "cnt: 0 - valLoss: 0.5818268656730652 - trainLoss: 0.5800220370292664\n",
      "cnt: 0 - valLoss: 0.581813395023346 - trainLoss: 0.5800031423568726\n",
      "cnt: 0 - valLoss: 0.5818012952804565 - trainLoss: 0.5799799561500549\n",
      "cnt: 0 - valLoss: 0.58179771900177 - trainLoss: 0.5799596309661865\n",
      "cnt: 0 - valLoss: 0.5817843079566956 - trainLoss: 0.5799388289451599\n",
      "cnt: 0 - valLoss: 0.581773042678833 - trainLoss: 0.5799157023429871\n",
      "cnt: 0 - valLoss: 0.5817685723304749 - trainLoss: 0.5798973441123962\n",
      "cnt: 0 - valLoss: 0.5817558765411377 - trainLoss: 0.5798746347427368\n",
      "cnt: 0 - valLoss: 0.5817528367042542 - trainLoss: 0.5798536539077759\n",
      "cnt: 0 - valLoss: 0.5817380547523499 - trainLoss: 0.5798349976539612\n",
      "cnt: 0 - valLoss: 0.5817353129386902 - trainLoss: 0.5798121094703674\n",
      "cnt: 0 - valLoss: 0.5817201733589172 - trainLoss: 0.5797950625419617\n",
      "cnt: 0 - valLoss: 0.5817161202430725 - trainLoss: 0.5797713398933411\n",
      "cnt: 0 - valLoss: 0.5817015767097473 - trainLoss: 0.5797541737556458\n",
      "cnt: 0 - valLoss: 0.5816968083381653 - trainLoss: 0.5797320008277893\n",
      "cnt: 0 - valLoss: 0.5816829800605774 - trainLoss: 0.5797132849693298\n",
      "cnt: 0 - valLoss: 0.5816774964332581 - trainLoss: 0.5796926617622375\n",
      "cnt: 0 - valLoss: 0.5816643238067627 - trainLoss: 0.5796723365783691\n",
      "cnt: 0 - valLoss: 0.581658124923706 - trainLoss: 0.5796533226966858\n",
      "cnt: 0 - valLoss: 0.581645667552948 - trainLoss: 0.5796314477920532\n",
      "cnt: 0 - valLoss: 0.5816391110420227 - trainLoss: 0.5796140432357788\n",
      "cnt: 0 - valLoss: 0.5816347002983093 - trainLoss: 0.5795914530754089\n",
      "cnt: 0 - valLoss: 0.5816203355789185 - trainLoss: 0.5795736908912659\n",
      "cnt: 0 - valLoss: 0.5816149115562439 - trainLoss: 0.5795523524284363\n",
      "cnt: 0 - valLoss: 0.5816015005111694 - trainLoss: 0.5795325040817261\n",
      "cnt: 0 - valLoss: 0.5815950632095337 - trainLoss: 0.5795133709907532\n",
      "cnt: 0 - valLoss: 0.5815827250480652 - trainLoss: 0.5794912576675415\n",
      "cnt: 0 - valLoss: 0.5815761685371399 - trainLoss: 0.5794742703437805\n",
      "cnt: 0 - valLoss: 0.5815712809562683 - trainLoss: 0.5794519186019897\n",
      "cnt: 0 - valLoss: 0.5815572142601013 - trainLoss: 0.579433262348175\n",
      "cnt: 0 - valLoss: 0.5815511345863342 - trainLoss: 0.5794130563735962\n",
      "cnt: 0 - valLoss: 0.5815382599830627 - trainLoss: 0.5793917775154114\n",
      "cnt: 0 - valLoss: 0.5815317630767822 - trainLoss: 0.5793741941452026\n",
      "cnt: 0 - valLoss: 0.5815268754959106 - trainLoss: 0.5793519616127014\n",
      "cnt: 0 - valLoss: 0.5815126299858093 - trainLoss: 0.5793336033821106\n",
      "cnt: 0 - valLoss: 0.5815064311027527 - trainLoss: 0.5793132781982422\n",
      "cnt: 0 - valLoss: 0.5814935564994812 - trainLoss: 0.579291820526123\n",
      "cnt: 0 - valLoss: 0.5814870595932007 - trainLoss: 0.5792745351791382\n",
      "cnt: 0 - valLoss: 0.5814818739891052 - trainLoss: 0.5792524814605713\n",
      "cnt: 0 - valLoss: 0.5814678072929382 - trainLoss: 0.5792334079742432\n",
      "cnt: 0 - valLoss: 0.5814613699913025 - trainLoss: 0.5792139172554016\n",
      "cnt: 0 - valLoss: 0.5814568996429443 - trainLoss: 0.5791919827461243\n",
      "cnt: 0 - valLoss: 0.5814419984817505 - trainLoss: 0.5791747570037842\n",
      "cnt: 0 - valLoss: 0.5814359784126282 - trainLoss: 0.5791535377502441\n",
      "cnt: 0 - valLoss: 0.5814226865768433 - trainLoss: 0.5791327357292175\n",
      "cnt: 0 - valLoss: 0.5814162492752075 - trainLoss: 0.579115092754364\n",
      "cnt: 0 - valLoss: 0.5814107656478882 - trainLoss: 0.5790932774543762\n",
      "cnt: 0 - valLoss: 0.581396758556366 - trainLoss: 0.5790739059448242\n",
      "cnt: 0 - valLoss: 0.5813903212547302 - trainLoss: 0.5790548920631409\n",
      "cnt: 0 - valLoss: 0.5813853144645691 - trainLoss: 0.5790332555770874\n",
      "cnt: 0 - valLoss: 0.5813707709312439 - trainLoss: 0.5790148377418518\n",
      "cnt: 0 - valLoss: 0.5813642740249634 - trainLoss: 0.5789949297904968\n",
      "cnt: 0 - valLoss: 0.5813595056533813 - trainLoss: 0.5789733529090881\n",
      "cnt: 0 - valLoss: 0.5813445448875427 - trainLoss: 0.5789557099342346\n",
      "cnt: 0 - valLoss: 0.581338107585907 - trainLoss: 0.5789351463317871\n",
      "cnt: 0 - valLoss: 0.5813333988189697 - trainLoss: 0.5789137482643127\n",
      "cnt: 0 - valLoss: 0.5813182592391968 - trainLoss: 0.5788962841033936\n",
      "cnt: 0 - valLoss: 0.5813117623329163 - trainLoss: 0.5788756012916565\n",
      "cnt: 0 - valLoss: 0.5813069939613342 - trainLoss: 0.5788542032241821\n",
      "cnt: 0 - valLoss: 0.581291913986206 - trainLoss: 0.5788366198539734\n",
      "cnt: 0 - valLoss: 0.5812854766845703 - trainLoss: 0.5788161754608154\n",
      "cnt: 0 - valLoss: 0.5812804102897644 - trainLoss: 0.5787948369979858\n",
      "cnt: 0 - valLoss: 0.581265389919281 - trainLoss: 0.5787768959999084\n",
      "cnt: 0 - valLoss: 0.5812588930130005 - trainLoss: 0.5787568092346191\n",
      "cnt: 0 - valLoss: 0.5812535285949707 - trainLoss: 0.5787355899810791\n",
      "cnt: 0 - valLoss: 0.5812388062477112 - trainLoss: 0.5787168741226196\n",
      "cnt: 0 - valLoss: 0.5812322497367859 - trainLoss: 0.5786976218223572\n",
      "cnt: 0 - valLoss: 0.5812264084815979 - trainLoss: 0.5786764621734619\n",
      "cnt: 0 - valLoss: 0.5812119841575623 - trainLoss: 0.5786567330360413\n",
      "cnt: 0 - valLoss: 0.5812055468559265 - trainLoss: 0.5786385536193848\n",
      "cnt: 0 - valLoss: 0.581199049949646 - trainLoss: 0.5786174535751343\n",
      "cnt: 0 - valLoss: 0.581194281578064 - trainLoss: 0.5785964131355286\n",
      "cnt: 0 - valLoss: 0.5811784863471985 - trainLoss: 0.5785794258117676\n",
      "cnt: 0 - valLoss: 0.5811720490455627 - trainLoss: 0.5785586833953857\n",
      "cnt: 0 - valLoss: 0.5811663269996643 - trainLoss: 0.5785377025604248\n",
      "cnt: 0 - valLoss: 0.5811514258384705 - trainLoss: 0.5785186290740967\n",
      "cnt: 0 - valLoss: 0.5811449289321899 - trainLoss: 0.5784999132156372\n",
      "cnt: 0 - valLoss: 0.5811384320259094 - trainLoss: 0.5784790515899658\n",
      "cnt: 0 - valLoss: 0.5811334848403931 - trainLoss: 0.5784582495689392\n",
      "cnt: 0 - valLoss: 0.5811176300048828 - trainLoss: 0.5784409642219543\n",
      "cnt: 0 - valLoss: 0.5811111927032471 - trainLoss: 0.5784205198287964\n",
      "cnt: 0 - valLoss: 0.5811049938201904 - trainLoss: 0.5783997774124146\n",
      "cnt: 0 - valLoss: 0.58109050989151 - trainLoss: 0.5783799290657043\n",
      "cnt: 0 - valLoss: 0.5810840725898743 - trainLoss: 0.5783635973930359\n",
      "cnt: 0 - valLoss: 0.5810785889625549 - trainLoss: 0.578342854976654\n",
      "cnt: 0 - valLoss: 0.581063449382782 - trainLoss: 0.578324556350708\n",
      "cnt: 0 - valLoss: 0.5810568928718567 - trainLoss: 0.5783067345619202\n",
      "cnt: 0 - valLoss: 0.5810521841049194 - trainLoss: 0.5782859921455383\n",
      "cnt: 0 - valLoss: 0.5810371041297913 - trainLoss: 0.5782691836357117\n",
      "cnt: 0 - valLoss: 0.5810320377349854 - trainLoss: 0.5782498121261597\n",
      "cnt: 0 - valLoss: 0.5810190439224243 - trainLoss: 0.5782291889190674\n",
      "cnt: 0 - valLoss: 0.5810138583183289 - trainLoss: 0.5782136917114258\n",
      "cnt: 0 - valLoss: 0.581009566783905 - trainLoss: 0.5781928300857544\n",
      "cnt: 0 - valLoss: 0.5809957385063171 - trainLoss: 0.5781738758087158\n",
      "cnt: 0 - valLoss: 0.580990731716156 - trainLoss: 0.578156590461731\n",
      "cnt: 0 - valLoss: 0.5809876322746277 - trainLoss: 0.5781359672546387\n",
      "cnt: 0 - valLoss: 0.5809735655784607 - trainLoss: 0.5781192779541016\n",
      "cnt: 0 - valLoss: 0.5809690356254578 - trainLoss: 0.5781015157699585\n",
      "cnt: 0 - valLoss: 0.5809664130210876 - trainLoss: 0.5780816674232483\n",
      "cnt: 0 - valLoss: 0.580951988697052 - trainLoss: 0.5780659914016724\n",
      "cnt: 0 - valLoss: 0.5809474587440491 - trainLoss: 0.5780472159385681\n",
      "cnt: 0 - valLoss: 0.5809352993965149 - trainLoss: 0.5780274868011475\n",
      "cnt: 0 - valLoss: 0.5809313058853149 - trainLoss: 0.5780127048492432\n",
      "cnt: 0 - valLoss: 0.5809279680252075 - trainLoss: 0.577992856502533\n",
      "cnt: 0 - valLoss: 0.5809152722358704 - trainLoss: 0.5779743790626526\n",
      "cnt: 0 - valLoss: 0.5809114575386047 - trainLoss: 0.577958345413208\n",
      "cnt: 0 - valLoss: 0.5809088945388794 - trainLoss: 0.5779384970664978\n",
      "cnt: 0 - valLoss: 0.5808960795402527 - trainLoss: 0.5779212713241577\n",
      "cnt: 0 - valLoss: 0.5808926224708557 - trainLoss: 0.5779043436050415\n",
      "cnt: 0 - valLoss: 0.5808907151222229 - trainLoss: 0.5778847932815552\n",
      "cnt: 0 - valLoss: 0.580877423286438 - trainLoss: 0.5778687596321106\n",
      "cnt: 0 - valLoss: 0.580873966217041 - trainLoss: 0.577850878238678\n",
      "cnt: 0 - valLoss: 0.5808724164962769 - trainLoss: 0.5778313279151917\n",
      "cnt: 0 - valLoss: 0.5808587074279785 - trainLoss: 0.5778161287307739\n",
      "cnt: 0 - valLoss: 0.5808553099632263 - trainLoss: 0.5777974128723145\n",
      "cnt: 0 - valLoss: 0.5808435678482056 - trainLoss: 0.5777778625488281\n",
      "cnt: 0 - valLoss: 0.5808402299880981 - trainLoss: 0.5777634382247925\n",
      "cnt: 0 - valLoss: 0.5808377265930176 - trainLoss: 0.5777438282966614\n",
      "cnt: 0 - valLoss: 0.5808258056640625 - trainLoss: 0.5777254700660706\n",
      "cnt: 0 - valLoss: 0.5808228850364685 - trainLoss: 0.5777102708816528\n",
      "cnt: 0 - valLoss: 0.5808207392692566 - trainLoss: 0.5776909589767456\n",
      "cnt: 0 - valLoss: 0.5808084607124329 - trainLoss: 0.5776734352111816\n",
      "cnt: 0 - valLoss: 0.5808055996894836 - trainLoss: 0.5776574611663818\n",
      "cnt: 0 - valLoss: 0.5808036923408508 - trainLoss: 0.5776380896568298\n",
      "cnt: 0 - valLoss: 0.5807910561561584 - trainLoss: 0.5776214003562927\n",
      "cnt: 0 - valLoss: 0.5807880163192749 - trainLoss: 0.5776048898696899\n",
      "cnt: 0 - valLoss: 0.5807863473892212 - trainLoss: 0.5775858759880066\n",
      "cnt: 0 - valLoss: 0.5807734131813049 - trainLoss: 0.5775697827339172\n",
      "cnt: 0 - valLoss: 0.5807704329490662 - trainLoss: 0.5775527358055115\n",
      "cnt: 0 - valLoss: 0.5807689428329468 - trainLoss: 0.5775336623191833\n",
      "cnt: 0 - valLoss: 0.5807558298110962 - trainLoss: 0.5775180459022522\n",
      "cnt: 0 - valLoss: 0.5807527899742126 - trainLoss: 0.577500581741333\n",
      "cnt: 0 - valLoss: 0.5807515978813171 - trainLoss: 0.5774815082550049\n",
      "cnt: 0 - valLoss: 0.5807381272315979 - trainLoss: 0.5774662494659424\n",
      "cnt: 0 - valLoss: 0.5807352066040039 - trainLoss: 0.5774483680725098\n",
      "cnt: 0 - valLoss: 0.5807340741157532 - trainLoss: 0.5774293541908264\n",
      "cnt: 0 - valLoss: 0.5807204246520996 - trainLoss: 0.577414333820343\n",
      "cnt: 0 - valLoss: 0.5807175636291504 - trainLoss: 0.5773961544036865\n",
      "cnt: 0 - valLoss: 0.5807164907455444 - trainLoss: 0.5773770809173584\n",
      "cnt: 0 - valLoss: 0.5807027816772461 - trainLoss: 0.5773623585700989\n",
      "cnt: 0 - valLoss: 0.5806998610496521 - trainLoss: 0.5773439407348633\n",
      "cnt: 0 - valLoss: 0.5806989669799805 - trainLoss: 0.5773248672485352\n",
      "cnt: 0 - valLoss: 0.5806850790977478 - trainLoss: 0.57731032371521\n",
      "cnt: 0 - valLoss: 0.5806821584701538 - trainLoss: 0.57729172706604\n",
      "cnt: 0 - valLoss: 0.5806812047958374 - trainLoss: 0.5772726535797119\n",
      "cnt: 0 - valLoss: 0.58066725730896 - trainLoss: 0.5772581100463867\n",
      "cnt: 0 - valLoss: 0.5806643962860107 - trainLoss: 0.5772395730018616\n",
      "cnt: 0 - valLoss: 0.5806633830070496 - trainLoss: 0.5772204995155334\n",
      "cnt: 0 - valLoss: 0.5806494951248169 - trainLoss: 0.5772059559822083\n",
      "cnt: 0 - valLoss: 0.5806464552879333 - trainLoss: 0.5771875977516174\n",
      "cnt: 0 - valLoss: 0.5806452035903931 - trainLoss: 0.5771689414978027\n",
      "cnt: 0 - valLoss: 0.5806315541267395 - trainLoss: 0.5771543383598328\n",
      "cnt: 0 - valLoss: 0.5806289911270142 - trainLoss: 0.5771363973617554\n",
      "cnt: 0 - valLoss: 0.5806280374526978 - trainLoss: 0.5771176815032959\n",
      "cnt: 0 - valLoss: 0.5806146860122681 - trainLoss: 0.5771028399467468\n",
      "cnt: 0 - valLoss: 0.5806119441986084 - trainLoss: 0.5770853757858276\n",
      "cnt: 0 - valLoss: 0.5806108117103577 - trainLoss: 0.5770667791366577\n",
      "cnt: 0 - valLoss: 0.580597460269928 - trainLoss: 0.5770518183708191\n",
      "cnt: 0 - valLoss: 0.5805947780609131 - trainLoss: 0.5770346522331238\n",
      "cnt: 0 - valLoss: 0.580593466758728 - trainLoss: 0.5770160555839539\n",
      "cnt: 0 - valLoss: 0.5805802345275879 - trainLoss: 0.5770007967948914\n",
      "cnt: 0 - valLoss: 0.580577552318573 - trainLoss: 0.5769838690757751\n",
      "cnt: 0 - valLoss: 0.5805761218070984 - trainLoss: 0.57696533203125\n",
      "cnt: 0 - valLoss: 0.5805630683898926 - trainLoss: 0.5769496560096741\n",
      "cnt: 0 - valLoss: 0.5805603861808777 - trainLoss: 0.5769331455230713\n",
      "cnt: 0 - valLoss: 0.5805588364601135 - trainLoss: 0.5769145488739014\n",
      "cnt: 0 - valLoss: 0.5805458426475525 - trainLoss: 0.5768985748291016\n",
      "cnt: 0 - valLoss: 0.5805431604385376 - trainLoss: 0.5768823027610779\n",
      "cnt: 0 - valLoss: 0.5805414319038391 - trainLoss: 0.5768637657165527\n",
      "cnt: 0 - valLoss: 0.5805286169052124 - trainLoss: 0.5768474340438843\n",
      "cnt: 0 - valLoss: 0.5805259346961975 - trainLoss: 0.5768315196037292\n",
      "cnt: 0 - valLoss: 0.5805240273475647 - trainLoss: 0.5768129825592041\n",
      "cnt: 0 - valLoss: 0.5805113911628723 - trainLoss: 0.5767961740493774\n",
      "cnt: 0 - valLoss: 0.5805086493492126 - trainLoss: 0.5767806768417358\n",
      "cnt: 0 - valLoss: 0.580506443977356 - trainLoss: 0.576762318611145\n",
      "cnt: 0 - valLoss: 0.5804939866065979 - trainLoss: 0.5767452716827393\n",
      "cnt: 0 - valLoss: 0.5804912447929382 - trainLoss: 0.5767304301261902\n",
      "cnt: 0 - valLoss: 0.5804887413978577 - trainLoss: 0.5767120122909546\n",
      "cnt: 0 - valLoss: 0.5804765820503235 - trainLoss: 0.5766943693161011\n",
      "cnt: 0 - valLoss: 0.5804738402366638 - trainLoss: 0.576680064201355\n",
      "cnt: 0 - valLoss: 0.5804709792137146 - trainLoss: 0.5766617059707642\n",
      "cnt: 0 - valLoss: 0.5804588198661804 - trainLoss: 0.5766436457633972\n",
      "cnt: 0 - valLoss: 0.5804558992385864 - trainLoss: 0.5766304731369019\n",
      "cnt: 0 - valLoss: 0.5804529786109924 - trainLoss: 0.5766125321388245\n",
      "cnt: 0 - valLoss: 0.5804516077041626 - trainLoss: 0.5765947699546814\n",
      "cnt: 0 - valLoss: 0.5804377198219299 - trainLoss: 0.5765806436538696\n",
      "cnt: 0 - valLoss: 0.5804347395896912 - trainLoss: 0.576563835144043\n",
      "cnt: 0 - valLoss: 0.5804321765899658 - trainLoss: 0.5765460729598999\n",
      "cnt: 0 - valLoss: 0.5804181694984436 - trainLoss: 0.5765305757522583\n",
      "cnt: 0 - valLoss: 0.5804141759872437 - trainLoss: 0.576514720916748\n",
      "cnt: 0 - valLoss: 0.5804107189178467 - trainLoss: 0.5764966011047363\n",
      "cnt: 0 - valLoss: 0.5803971290588379 - trainLoss: 0.5764798521995544\n",
      "cnt: 0 - valLoss: 0.5803930163383484 - trainLoss: 0.5764652490615845\n",
      "cnt: 0 - valLoss: 0.5803890228271484 - trainLoss: 0.5764471888542175\n",
      "cnt: 0 - valLoss: 0.5803760290145874 - trainLoss: 0.576429545879364\n",
      "cnt: 0 - valLoss: 0.5803719162940979 - trainLoss: 0.576416015625\n",
      "cnt: 0 - valLoss: 0.5803678631782532 - trainLoss: 0.5763980150222778\n",
      "cnt: 0 - valLoss: 0.5803653001785278 - trainLoss: 0.5763801336288452\n",
      "cnt: 0 - valLoss: 0.5803505778312683 - trainLoss: 0.57636559009552\n",
      "cnt: 0 - valLoss: 0.5803465247154236 - trainLoss: 0.5763488411903381\n",
      "cnt: 0 - valLoss: 0.5803434252738953 - trainLoss: 0.5763309597969055\n",
      "cnt: 0 - valLoss: 0.5803294777870178 - trainLoss: 0.5763151049613953\n",
      "cnt: 0 - valLoss: 0.580325722694397 - trainLoss: 0.5763007998466492\n",
      "cnt: 0 - valLoss: 0.5803219079971313 - trainLoss: 0.5762839913368225\n",
      "cnt: 0 - valLoss: 0.580319881439209 - trainLoss: 0.5762673020362854\n",
      "cnt: 0 - valLoss: 0.5803049206733704 - trainLoss: 0.5762544870376587\n",
      "cnt: 0 - valLoss: 0.5803011655807495 - trainLoss: 0.576238214969635\n",
      "cnt: 0 - valLoss: 0.5802980661392212 - trainLoss: 0.5762214660644531\n",
      "cnt: 0 - valLoss: 0.5802841186523438 - trainLoss: 0.5762062668800354\n",
      "cnt: 0 - valLoss: 0.5802803635597229 - trainLoss: 0.576192319393158\n",
      "cnt: 0 - valLoss: 0.580276608467102 - trainLoss: 0.5761755704879761\n",
      "cnt: 0 - valLoss: 0.5802741646766663 - trainLoss: 0.576158881187439\n",
      "cnt: 0 - valLoss: 0.5802594423294067 - trainLoss: 0.5761455297470093\n",
      "cnt: 0 - valLoss: 0.5802555680274963 - trainLoss: 0.5761299133300781\n",
      "cnt: 0 - valLoss: 0.580251932144165 - trainLoss: 0.5761133432388306\n",
      "cnt: 0 - valLoss: 0.580238401889801 - trainLoss: 0.5760972499847412\n",
      "cnt: 0 - valLoss: 0.5802345871925354 - trainLoss: 0.5760843753814697\n",
      "cnt: 0 - valLoss: 0.5802310109138489 - trainLoss: 0.5760676860809326\n",
      "cnt: 0 - valLoss: 0.5802283883094788 - trainLoss: 0.5760504603385925\n",
      "cnt: 0 - valLoss: 0.5802160501480103 - trainLoss: 0.5760348439216614\n",
      "cnt: 0 - valLoss: 0.5802124738693237 - trainLoss: 0.5760194063186646\n",
      "cnt: 0 - valLoss: 0.580213189125061 - trainLoss: 0.5760018229484558\n",
      "cnt: 1 - valLoss: 0.5801992416381836 - trainLoss: 0.5759877562522888\n",
      "cnt: 0 - valLoss: 0.5801994204521179 - trainLoss: 0.5759696960449219\n",
      "cnt: 1 - valLoss: 0.5801859498023987 - trainLoss: 0.5759544968605042\n",
      "cnt: 0 - valLoss: 0.5801857113838196 - trainLoss: 0.5759375691413879\n",
      "cnt: 0 - valLoss: 0.5801727771759033 - trainLoss: 0.5759212374687195\n",
      "cnt: 0 - valLoss: 0.580172061920166 - trainLoss: 0.5759053826332092\n",
      "cnt: 0 - valLoss: 0.5801595449447632 - trainLoss: 0.5758879780769348\n",
      "cnt: 0 - valLoss: 0.5801583528518677 - trainLoss: 0.5758731365203857\n",
      "cnt: 0 - valLoss: 0.5801463723182678 - trainLoss: 0.5758547186851501\n",
      "cnt: 0 - valLoss: 0.5801447033882141 - trainLoss: 0.5758408904075623\n",
      "cnt: 0 - valLoss: 0.5801331996917725 - trainLoss: 0.5758213996887207\n",
      "cnt: 0 - valLoss: 0.580131471157074 - trainLoss: 0.5758086442947388\n",
      "cnt: 0 - valLoss: 0.5801324844360352 - trainLoss: 0.5757890343666077\n",
      "cnt: 1 - valLoss: 0.5801181793212891 - trainLoss: 0.5757754445075989\n",
      "cnt: 0 - valLoss: 0.5801185965538025 - trainLoss: 0.5757566094398499\n",
      "cnt: 1 - valLoss: 0.5801047682762146 - trainLoss: 0.575741708278656\n",
      "cnt: 0 - valLoss: 0.5801045298576355 - trainLoss: 0.5757244825363159\n",
      "cnt: 0 - valLoss: 0.5800914168357849 - trainLoss: 0.5757080912590027\n",
      "cnt: 0 - valLoss: 0.5800905823707581 - trainLoss: 0.5756922960281372\n",
      "cnt: 0 - valLoss: 0.5800780653953552 - trainLoss: 0.5756745338439941\n",
      "cnt: 0 - valLoss: 0.5800765752792358 - trainLoss: 0.5756600499153137\n",
      "cnt: 0 - valLoss: 0.5800647735595703 - trainLoss: 0.5756409764289856\n",
      "cnt: 0 - valLoss: 0.5800631046295166 - trainLoss: 0.5756278038024902\n",
      "cnt: 0 - valLoss: 0.5800639986991882 - trainLoss: 0.5756081938743591\n",
      "cnt: 1 - valLoss: 0.5800495743751526 - trainLoss: 0.5755946636199951\n",
      "cnt: 0 - valLoss: 0.5800499320030212 - trainLoss: 0.5755758285522461\n",
      "cnt: 1 - valLoss: 0.5800362229347229 - trainLoss: 0.5755605101585388\n",
      "cnt: 0 - valLoss: 0.5800358653068542 - trainLoss: 0.575543224811554\n",
      "cnt: 0 - valLoss: 0.5800228714942932 - trainLoss: 0.5755264163017273\n",
      "cnt: 0 - valLoss: 0.580021858215332 - trainLoss: 0.5755106806755066\n",
      "cnt: 0 - valLoss: 0.5800095200538635 - trainLoss: 0.575492262840271\n",
      "cnt: 0 - valLoss: 0.5800079703330994 - trainLoss: 0.5754780769348145\n",
      "cnt: 0 - valLoss: 0.5800092220306396 - trainLoss: 0.575458288192749\n",
      "cnt: 1 - valLoss: 0.5799943804740906 - trainLoss: 0.575445294380188\n",
      "cnt: 0 - valLoss: 0.579994797706604 - trainLoss: 0.5754258632659912\n",
      "cnt: 1 - valLoss: 0.5799808502197266 - trainLoss: 0.5754108428955078\n",
      "cnt: 0 - valLoss: 0.5799803733825684 - trainLoss: 0.5753932595252991\n",
      "cnt: 0 - valLoss: 0.5799673199653625 - trainLoss: 0.5753764510154724\n",
      "cnt: 0 - valLoss: 0.5799660682678223 - trainLoss: 0.5753607749938965\n",
      "cnt: 0 - valLoss: 0.5799537301063538 - trainLoss: 0.575342059135437\n",
      "cnt: 0 - valLoss: 0.5799522995948792 - trainLoss: 0.5753281712532043\n",
      "cnt: 0 - valLoss: 0.5799533128738403 - trainLoss: 0.5753084421157837\n",
      "cnt: 1 - valLoss: 0.5799385905265808 - trainLoss: 0.5752949118614197\n",
      "cnt: 0 - valLoss: 0.5799388885498047 - trainLoss: 0.5752760171890259\n",
      "cnt: 1 - valLoss: 0.5799251794815063 - trainLoss: 0.5752605199813843\n",
      "cnt: 0 - valLoss: 0.5799247026443481 - trainLoss: 0.5752435922622681\n",
      "cnt: 0 - valLoss: 0.5799119472503662 - trainLoss: 0.5752261877059937\n",
      "cnt: 0 - valLoss: 0.5799106955528259 - trainLoss: 0.575211226940155\n",
      "cnt: 0 - valLoss: 0.5798987150192261 - trainLoss: 0.5751925706863403\n",
      "cnt: 0 - valLoss: 0.5798971056938171 - trainLoss: 0.5751790404319763\n",
      "cnt: 0 - valLoss: 0.5798981189727783 - trainLoss: 0.5751595497131348\n",
      "cnt: 1 - valLoss: 0.5798835158348083 - trainLoss: 0.5751460790634155\n",
      "cnt: 0 - valLoss: 0.5798836946487427 - trainLoss: 0.5751275420188904\n",
      "cnt: 1 - valLoss: 0.5798700451850891 - trainLoss: 0.5751121044158936\n",
      "cnt: 0 - valLoss: 0.5798694491386414 - trainLoss: 0.5750953555107117\n",
      "cnt: 0 - valLoss: 0.5798566341400146 - trainLoss: 0.5750781893730164\n",
      "cnt: 0 - valLoss: 0.57985520362854 - trainLoss: 0.5750632286071777\n",
      "cnt: 0 - valLoss: 0.5798429250717163 - trainLoss: 0.5750446319580078\n",
      "cnt: 0 - valLoss: 0.579840898513794 - trainLoss: 0.5750319957733154\n",
      "cnt: 0 - valLoss: 0.5798412561416626 - trainLoss: 0.5750132203102112\n",
      "cnt: 1 - valLoss: 0.5798265933990479 - trainLoss: 0.5749996304512024\n",
      "cnt: 0 - valLoss: 0.5798258185386658 - trainLoss: 0.5749825239181519\n",
      "cnt: 0 - valLoss: 0.5798124074935913 - trainLoss: 0.5749664902687073\n",
      "cnt: 0 - valLoss: 0.5798105001449585 - trainLoss: 0.5749517679214478\n",
      "cnt: 0 - valLoss: 0.5797978043556213 - trainLoss: 0.5749333500862122\n",
      "cnt: 0 - valLoss: 0.5797953009605408 - trainLoss: 0.5749208331108093\n",
      "cnt: 0 - valLoss: 0.5797948241233826 - trainLoss: 0.5749019384384155\n",
      "cnt: 0 - valLoss: 0.5797800421714783 - trainLoss: 0.5748878121376038\n",
      "cnt: 0 - valLoss: 0.5797784328460693 - trainLoss: 0.5748709440231323\n",
      "cnt: 0 - valLoss: 0.5797649025917053 - trainLoss: 0.5748542547225952\n",
      "cnt: 0 - valLoss: 0.5797624588012695 - trainLoss: 0.5748399496078491\n",
      "cnt: 0 - valLoss: 0.57976233959198 - trainLoss: 0.5748212933540344\n",
      "cnt: 0 - valLoss: 0.5797466039657593 - trainLoss: 0.5748090744018555\n",
      "cnt: 0 - valLoss: 0.5797451734542847 - trainLoss: 0.5747914910316467\n",
      "cnt: 0 - valLoss: 0.5797309279441833 - trainLoss: 0.5747762322425842\n",
      "cnt: 0 - valLoss: 0.5797282457351685 - trainLoss: 0.5747616291046143\n",
      "cnt: 0 - valLoss: 0.5797283053398132 - trainLoss: 0.5747435092926025\n",
      "cnt: 1 - valLoss: 0.579712450504303 - trainLoss: 0.5747315287590027\n",
      "cnt: 0 - valLoss: 0.5797109603881836 - trainLoss: 0.5747137665748596\n",
      "cnt: 0 - valLoss: 0.5796965956687927 - trainLoss: 0.5746987462043762\n",
      "cnt: 0 - valLoss: 0.5796938538551331 - trainLoss: 0.574684202671051\n",
      "cnt: 0 - valLoss: 0.5796936750411987 - trainLoss: 0.5746662616729736\n",
      "cnt: 0 - valLoss: 0.5796778202056885 - trainLoss: 0.5746542811393738\n",
      "cnt: 0 - valLoss: 0.5796761512756348 - trainLoss: 0.5746367573738098\n",
      "cnt: 0 - valLoss: 0.5796618461608887 - trainLoss: 0.5746213793754578\n",
      "cnt: 0 - valLoss: 0.5796590447425842 - trainLoss: 0.5746071934700012\n",
      "cnt: 0 - valLoss: 0.579658567905426 - trainLoss: 0.5745893120765686\n",
      "cnt: 0 - valLoss: 0.5796428322792053 - trainLoss: 0.5745769739151001\n",
      "cnt: 0 - valLoss: 0.5796406865119934 - trainLoss: 0.5745599865913391\n",
      "cnt: 0 - valLoss: 0.5796265602111816 - trainLoss: 0.5745441913604736\n",
      "cnt: 0 - valLoss: 0.5796235203742981 - trainLoss: 0.5745306015014648\n",
      "cnt: 0 - valLoss: 0.5796225666999817 - trainLoss: 0.5745128393173218\n",
      "cnt: 0 - valLoss: 0.5796070694923401 - trainLoss: 0.5744998455047607\n",
      "cnt: 0 - valLoss: 0.5796045064926147 - trainLoss: 0.5744835138320923\n",
      "cnt: 0 - valLoss: 0.5795906186103821 - trainLoss: 0.5744670629501343\n",
      "cnt: 0 - valLoss: 0.5795876383781433 - trainLoss: 0.574454128742218\n",
      "cnt: 0 - valLoss: 0.5795865654945374 - trainLoss: 0.574436366558075\n",
      "cnt: 0 - valLoss: 0.5795711874961853 - trainLoss: 0.5744227170944214\n",
      "cnt: 0 - valLoss: 0.5795683860778809 - trainLoss: 0.5744069218635559\n",
      "cnt: 0 - valLoss: 0.5795547366142273 - trainLoss: 0.5743898153305054\n",
      "cnt: 0 - valLoss: 0.5795517563819885 - trainLoss: 0.5743775367736816\n",
      "cnt: 0 - valLoss: 0.5795503258705139 - trainLoss: 0.5743598341941833\n",
      "cnt: 0 - valLoss: 0.5795352458953857 - trainLoss: 0.5743453502655029\n",
      "cnt: 0 - valLoss: 0.5795323252677917 - trainLoss: 0.5743303894996643\n",
      "cnt: 0 - valLoss: 0.5795320868492126 - trainLoss: 0.5743125677108765\n",
      "cnt: 0 - valLoss: 0.5795157551765442 - trainLoss: 0.5743008255958557\n",
      "cnt: 0 - valLoss: 0.5795138478279114 - trainLoss: 0.5742831826210022\n",
      "cnt: 0 - valLoss: 0.5794992446899414 - trainLoss: 0.5742677450180054\n",
      "cnt: 0 - valLoss: 0.5794963836669922 - trainLoss: 0.5742538571357727\n",
      "cnt: 0 - valLoss: 0.5794954895973206 - trainLoss: 0.5742360353469849\n",
      "cnt: 0 - valLoss: 0.5794795751571655 - trainLoss: 0.5742231011390686\n",
      "cnt: 0 - valLoss: 0.5794771313667297 - trainLoss: 0.5742066502571106\n",
      "cnt: 0 - valLoss: 0.579463005065918 - trainLoss: 0.5741899609565735\n",
      "cnt: 0 - valLoss: 0.5794601440429688 - trainLoss: 0.5741772055625916\n",
      "cnt: 0 - valLoss: 0.5794587135314941 - trainLoss: 0.5741594433784485\n",
      "cnt: 0 - valLoss: 0.5794434547424316 - trainLoss: 0.5741451978683472\n",
      "cnt: 0 - valLoss: 0.5794404745101929 - trainLoss: 0.5741300582885742\n",
      "cnt: 0 - valLoss: 0.5794399976730347 - trainLoss: 0.5741122961044312\n",
      "cnt: 0 - valLoss: 0.5794236063957214 - trainLoss: 0.5741002559661865\n",
      "cnt: 0 - valLoss: 0.5794212818145752 - trainLoss: 0.5740829706192017\n",
      "cnt: 0 - valLoss: 0.5794069170951843 - trainLoss: 0.5740668773651123\n",
      "cnt: 0 - valLoss: 0.5794039368629456 - trainLoss: 0.5740535855293274\n",
      "cnt: 0 - valLoss: 0.579402506351471 - trainLoss: 0.5740358829498291\n",
      "cnt: 0 - valLoss: 0.5793869495391846 - trainLoss: 0.5740217566490173\n",
      "cnt: 0 - valLoss: 0.5793840289115906 - trainLoss: 0.5740064382553101\n",
      "cnt: 0 - valLoss: 0.579383373260498 - trainLoss: 0.5739887952804565\n",
      "cnt: 0 - valLoss: 0.5793669819831848 - trainLoss: 0.5739765167236328\n",
      "cnt: 0 - valLoss: 0.5793644189834595 - trainLoss: 0.5739595293998718\n",
      "cnt: 0 - valLoss: 0.5793499946594238 - trainLoss: 0.5739428997039795\n",
      "cnt: 0 - valLoss: 0.5793470740318298 - trainLoss: 0.573930025100708\n",
      "cnt: 0 - valLoss: 0.5793453454971313 - trainLoss: 0.5739123821258545\n",
      "cnt: 0 - valLoss: 0.5793300271034241 - trainLoss: 0.5738975405693054\n",
      "cnt: 0 - valLoss: 0.5793269872665405 - trainLoss: 0.5738829970359802\n",
      "cnt: 0 - valLoss: 0.5793259143829346 - trainLoss: 0.5738654136657715\n",
      "cnt: 0 - valLoss: 0.5793098211288452 - trainLoss: 0.5738520622253418\n",
      "cnt: 0 - valLoss: 0.5793068408966064 - trainLoss: 0.5738360285758972\n",
      "cnt: 0 - valLoss: 0.5793063044548035 - trainLoss: 0.5738184452056885\n",
      "cnt: 0 - valLoss: 0.5792893767356873 - trainLoss: 0.5738064050674438\n",
      "cnt: 0 - valLoss: 0.5792866349220276 - trainLoss: 0.5737892985343933\n",
      "cnt: 0 - valLoss: 0.5792731046676636 - trainLoss: 0.5737723112106323\n",
      "cnt: 0 - valLoss: 0.579269289970398 - trainLoss: 0.5737598538398743\n",
      "cnt: 0 - valLoss: 0.5792673230171204 - trainLoss: 0.5737422108650208\n",
      "cnt: 0 - valLoss: 0.5792525410652161 - trainLoss: 0.5737267136573792\n",
      "cnt: 0 - valLoss: 0.5792489051818848 - trainLoss: 0.573712944984436\n",
      "cnt: 0 - valLoss: 0.5792472958564758 - trainLoss: 0.5736953616142273\n",
      "cnt: 0 - valLoss: 0.579231858253479 - trainLoss: 0.5736806988716125\n",
      "cnt: 0 - valLoss: 0.5792285203933716 - trainLoss: 0.5736661553382874\n",
      "cnt: 0 - valLoss: 0.579227089881897 - trainLoss: 0.5736487507820129\n",
      "cnt: 0 - valLoss: 0.5792111754417419 - trainLoss: 0.5736345052719116\n",
      "cnt: 0 - valLoss: 0.5792078375816345 - trainLoss: 0.573619544506073\n",
      "cnt: 0 - valLoss: 0.5792064070701599 - trainLoss: 0.5736022591590881\n",
      "cnt: 0 - valLoss: 0.5791906118392944 - trainLoss: 0.5735880136489868\n",
      "cnt: 0 - valLoss: 0.5791870355606079 - trainLoss: 0.5735729932785034\n",
      "cnt: 0 - valLoss: 0.5791855454444885 - trainLoss: 0.5735557079315186\n",
      "cnt: 0 - valLoss: 0.5791700482368469 - trainLoss: 0.573541522026062\n",
      "cnt: 0 - valLoss: 0.5791661739349365 - trainLoss: 0.5735265016555786\n",
      "cnt: 0 - valLoss: 0.5791645646095276 - trainLoss: 0.573509156703949\n",
      "cnt: 0 - valLoss: 0.5791494250297546 - trainLoss: 0.5734947919845581\n",
      "cnt: 0 - valLoss: 0.5791452527046204 - trainLoss: 0.5734800100326538\n",
      "cnt: 0 - valLoss: 0.5791435241699219 - trainLoss: 0.573462724685669\n",
      "cnt: 0 - valLoss: 0.5791287422180176 - trainLoss: 0.5734480619430542\n",
      "cnt: 0 - valLoss: 0.5791242122650146 - trainLoss: 0.5734334588050842\n",
      "cnt: 0 - valLoss: 0.5791223645210266 - trainLoss: 0.5734162330627441\n",
      "cnt: 0 - valLoss: 0.5791081190109253 - trainLoss: 0.5734012126922607\n",
      "cnt: 0 - valLoss: 0.5791031718254089 - trainLoss: 0.5733870267868042\n",
      "cnt: 0 - valLoss: 0.5791011452674866 - trainLoss: 0.5733696818351746\n",
      "cnt: 0 - valLoss: 0.5790873765945435 - trainLoss: 0.5733542442321777\n",
      "cnt: 0 - valLoss: 0.5790824294090271 - trainLoss: 0.5733404755592346\n",
      "cnt: 0 - valLoss: 0.5790797472000122 - trainLoss: 0.5733232498168945\n",
      "cnt: 0 - valLoss: 0.5790666937828064 - trainLoss: 0.5733072757720947\n",
      "cnt: 0 - valLoss: 0.5790618062019348 - trainLoss: 0.5732938051223755\n",
      "cnt: 0 - valLoss: 0.5790585279464722 - trainLoss: 0.5732763409614563\n",
      "cnt: 0 - valLoss: 0.5790460109710693 - trainLoss: 0.573259711265564\n",
      "cnt: 0 - valLoss: 0.5790411233901978 - trainLoss: 0.5732467174530029\n",
      "cnt: 0 - valLoss: 0.5790371894836426 - trainLoss: 0.5732293128967285\n",
      "cnt: 0 - valLoss: 0.5790254473686218 - trainLoss: 0.5732120871543884\n",
      "cnt: 0 - valLoss: 0.5790205001831055 - trainLoss: 0.5731996893882751\n",
      "cnt: 0 - valLoss: 0.5790160298347473 - trainLoss: 0.573182225227356\n",
      "cnt: 0 - valLoss: 0.5790150761604309 - trainLoss: 0.5731648206710815\n",
      "cnt: 0 - valLoss: 0.5789996981620789 - trainLoss: 0.5731518864631653\n",
      "cnt: 0 - valLoss: 0.5789948105812073 - trainLoss: 0.5731350779533386\n",
      "cnt: 0 - valLoss: 0.5789931416511536 - trainLoss: 0.5731176137924194\n",
      "cnt: 0 - valLoss: 0.578978955745697 - trainLoss: 0.5731035470962524\n",
      "cnt: 0 - valLoss: 0.5789740085601807 - trainLoss: 0.5730878710746765\n",
      "cnt: 0 - valLoss: 0.5789710283279419 - trainLoss: 0.5730704665184021\n",
      "cnt: 0 - valLoss: 0.5789581537246704 - trainLoss: 0.57305508852005\n",
      "cnt: 0 - valLoss: 0.5789532661437988 - trainLoss: 0.5730406641960144\n",
      "cnt: 0 - valLoss: 0.5789490342140198 - trainLoss: 0.5730232000350952\n",
      "cnt: 0 - valLoss: 0.5789372324943542 - trainLoss: 0.5730066299438477\n",
      "cnt: 0 - valLoss: 0.5789323449134827 - trainLoss: 0.5729935169219971\n",
      "cnt: 0 - valLoss: 0.5789276361465454 - trainLoss: 0.5729760527610779\n",
      "cnt: 0 - valLoss: 0.5789264440536499 - trainLoss: 0.5729586482048035\n",
      "cnt: 0 - valLoss: 0.5789111852645874 - trainLoss: 0.5729454159736633\n",
      "cnt: 0 - valLoss: 0.5789063572883606 - trainLoss: 0.5729292631149292\n",
      "cnt: 0 - valLoss: 0.5789029598236084 - trainLoss: 0.5729120373725891\n",
      "cnt: 0 - valLoss: 0.5788900256156921 - trainLoss: 0.5728965401649475\n",
      "cnt: 0 - valLoss: 0.5788851380348206 - trainLoss: 0.5728825926780701\n",
      "cnt: 0 - valLoss: 0.5788808465003967 - trainLoss: 0.57286536693573\n",
      "cnt: 0 - valLoss: 0.5788789987564087 - trainLoss: 0.5728483200073242\n",
      "cnt: 0 - valLoss: 0.5788636207580566 - trainLoss: 0.5728350877761841\n",
      "cnt: 0 - valLoss: 0.5788593888282776 - trainLoss: 0.5728188753128052\n",
      "cnt: 0 - valLoss: 0.5788556337356567 - trainLoss: 0.5728017091751099\n",
      "cnt: 0 - valLoss: 0.578842282295227 - trainLoss: 0.5727858543395996\n",
      "cnt: 0 - valLoss: 0.578838050365448 - trainLoss: 0.572772204875946\n",
      "cnt: 0 - valLoss: 0.5788337588310242 - trainLoss: 0.5727550983428955\n",
      "cnt: 0 - valLoss: 0.5788307189941406 - trainLoss: 0.5727380514144897\n",
      "cnt: 0 - valLoss: 0.5788158178329468 - trainLoss: 0.5727241635322571\n",
      "cnt: 0 - valLoss: 0.5788115859031677 - trainLoss: 0.572708785533905\n",
      "cnt: 0 - valLoss: 0.5788072943687439 - trainLoss: 0.5726917386054993\n",
      "cnt: 0 - valLoss: 0.5788036584854126 - trainLoss: 0.5726747512817383\n",
      "cnt: 0 - valLoss: 0.5787887573242188 - trainLoss: 0.5726611018180847\n",
      "cnt: 0 - valLoss: 0.5787844657897949 - trainLoss: 0.5726458430290222\n",
      "cnt: 0 - valLoss: 0.5787801146507263 - trainLoss: 0.572628915309906\n",
      "cnt: 0 - valLoss: 0.5787761807441711 - trainLoss: 0.5726121664047241\n",
      "cnt: 0 - valLoss: 0.5787615180015564 - trainLoss: 0.5725979208946228\n",
      "cnt: 0 - valLoss: 0.5787572264671326 - trainLoss: 0.5725831985473633\n",
      "cnt: 0 - valLoss: 0.578752875328064 - trainLoss: 0.5725662708282471\n",
      "cnt: 0 - valLoss: 0.5787485837936401 - trainLoss: 0.5725496411323547\n",
      "cnt: 0 - valLoss: 0.5787342190742493 - trainLoss: 0.5725345611572266\n",
      "cnt: 0 - valLoss: 0.5787299275398254 - trainLoss: 0.5725205540657043\n",
      "cnt: 0 - valLoss: 0.5787255167961121 - trainLoss: 0.5725036263465881\n",
      "cnt: 0 - valLoss: 0.5787209272384644 - trainLoss: 0.572486937046051\n",
      "cnt: 0 - valLoss: 0.5787070393562317 - trainLoss: 0.5724711418151855\n",
      "cnt: 0 - valLoss: 0.5787029266357422 - trainLoss: 0.572458028793335\n",
      "cnt: 0 - valLoss: 0.5786988139152527 - trainLoss: 0.5724412798881531\n",
      "cnt: 0 - valLoss: 0.5786940455436707 - trainLoss: 0.5724247694015503\n",
      "cnt: 0 - valLoss: 0.5786904096603394 - trainLoss: 0.5724084377288818\n",
      "cnt: 0 - valLoss: 0.5786755084991455 - trainLoss: 0.572394609451294\n",
      "cnt: 0 - valLoss: 0.578671395778656 - trainLoss: 0.5723794102668762\n",
      "cnt: 0 - valLoss: 0.578666627407074 - trainLoss: 0.5723628997802734\n",
      "cnt: 0 - valLoss: 0.5786619782447815 - trainLoss: 0.5723465085029602\n",
      "cnt: 0 - valLoss: 0.5786585211753845 - trainLoss: 0.5723302364349365\n",
      "cnt: 0 - valLoss: 0.578643262386322 - trainLoss: 0.5723171234130859\n",
      "cnt: 0 - valLoss: 0.5786390900611877 - trainLoss: 0.5723010897636414\n",
      "cnt: 0 - valLoss: 0.5786342620849609 - trainLoss: 0.5722846388816833\n",
      "cnt: 0 - valLoss: 0.5786295533180237 - trainLoss: 0.5722683072090149\n",
      "cnt: 0 - valLoss: 0.5786154866218567 - trainLoss: 0.5722523331642151\n",
      "cnt: 0 - valLoss: 0.5786111950874329 - trainLoss: 0.5722391605377197\n",
      "cnt: 0 - valLoss: 0.578606367111206 - trainLoss: 0.5722226500511169\n",
      "cnt: 0 - valLoss: 0.5786014795303345 - trainLoss: 0.5722063183784485\n",
      "cnt: 0 - valLoss: 0.578596830368042 - trainLoss: 0.5721901059150696\n",
      "cnt: 0 - valLoss: 0.5785825252532959 - trainLoss: 0.5721747279167175\n",
      "cnt: 0 - valLoss: 0.5785781145095825 - trainLoss: 0.5721608996391296\n",
      "cnt: 0 - valLoss: 0.5785732865333557 - trainLoss: 0.5721443891525269\n",
      "cnt: 0 - valLoss: 0.5785683989524841 - trainLoss: 0.5721281170845032\n",
      "cnt: 0 - valLoss: 0.5785638093948364 - trainLoss: 0.5721119046211243\n",
      "cnt: 0 - valLoss: 0.5785494446754456 - trainLoss: 0.5720966458320618\n",
      "cnt: 0 - valLoss: 0.578544557094574 - trainLoss: 0.5720826983451843\n",
      "cnt: 0 - valLoss: 0.5785396695137024 - trainLoss: 0.5720663666725159\n",
      "cnt: 0 - valLoss: 0.5785347819328308 - trainLoss: 0.5720500946044922\n",
      "cnt: 0 - valLoss: 0.5785298943519592 - trainLoss: 0.5720338225364685\n",
      "cnt: 0 - valLoss: 0.5785157084465027 - trainLoss: 0.5720179080963135\n",
      "cnt: 0 - valLoss: 0.5785108208656311 - trainLoss: 0.5720045566558838\n",
      "cnt: 0 - valLoss: 0.5785059332847595 - trainLoss: 0.5719881653785706\n",
      "cnt: 0 - valLoss: 0.5785007476806641 - trainLoss: 0.5719720125198364\n",
      "cnt: 0 - valLoss: 0.5784955620765686 - trainLoss: 0.5719559788703918\n",
      "cnt: 0 - valLoss: 0.5784912705421448 - trainLoss: 0.571940004825592\n",
      "cnt: 0 - valLoss: 0.578476071357727 - trainLoss: 0.5719259977340698\n",
      "cnt: 0 - valLoss: 0.5784710049629211 - trainLoss: 0.5719110369682312\n",
      "cnt: 0 - valLoss: 0.5784658193588257 - trainLoss: 0.5718948841094971\n",
      "cnt: 0 - valLoss: 0.5784606337547302 - trainLoss: 0.5718788504600525\n",
      "cnt: 0 - valLoss: 0.5784555077552795 - trainLoss: 0.5718628764152527\n",
      "cnt: 0 - valLoss: 0.5784410834312439 - trainLoss: 0.5718470811843872\n",
      "cnt: 0 - valLoss: 0.5784359574317932 - trainLoss: 0.5718338489532471\n",
      "cnt: 0 - valLoss: 0.5784308910369873 - trainLoss: 0.5718177556991577\n",
      "cnt: 0 - valLoss: 0.5784257054328918 - trainLoss: 0.5718017220497131\n",
      "cnt: 0 - valLoss: 0.5784205794334412 - trainLoss: 0.5717857480049133\n",
      "cnt: 0 - valLoss: 0.5784157514572144 - trainLoss: 0.5717697739601135\n",
      "cnt: 0 - valLoss: 0.5784007906913757 - trainLoss: 0.5717548727989197\n",
      "cnt: 0 - valLoss: 0.5783957242965698 - trainLoss: 0.5717406868934631\n",
      "cnt: 0 - valLoss: 0.5783904790878296 - trainLoss: 0.5717245936393738\n",
      "cnt: 0 - valLoss: 0.5783852338790894 - trainLoss: 0.5717087984085083\n",
      "cnt: 0 - valLoss: 0.5783799290657043 - trainLoss: 0.5716930031776428\n",
      "cnt: 0 - valLoss: 0.5783750414848328 - trainLoss: 0.5716772079467773\n",
      "cnt: 0 - valLoss: 0.5783599019050598 - trainLoss: 0.5716624855995178\n",
      "cnt: 0 - valLoss: 0.5783547163009644 - trainLoss: 0.571648359298706\n",
      "cnt: 0 - valLoss: 0.5783494710922241 - trainLoss: 0.5716325044631958\n",
      "cnt: 0 - valLoss: 0.5783441662788391 - trainLoss: 0.5716166496276855\n",
      "cnt: 0 - valLoss: 0.5783388614654541 - trainLoss: 0.5716008543968201\n",
      "cnt: 0 - valLoss: 0.5783336758613586 - trainLoss: 0.5715850591659546\n",
      "cnt: 0 - valLoss: 0.57831871509552 - trainLoss: 0.5715697407722473\n",
      "cnt: 0 - valLoss: 0.5783135294914246 - trainLoss: 0.5715561509132385\n",
      "cnt: 0 - valLoss: 0.5783082842826843 - trainLoss: 0.5715402960777283\n",
      "cnt: 0 - valLoss: 0.5783029794692993 - trainLoss: 0.5715245008468628\n",
      "cnt: 0 - valLoss: 0.5782976150512695 - trainLoss: 0.5715087056159973\n",
      "cnt: 0 - valLoss: 0.5782922506332397 - trainLoss: 0.5714929699897766\n",
      "cnt: 0 - valLoss: 0.5782877206802368 - trainLoss: 0.5714772343635559\n",
      "cnt: 0 - valLoss: 0.5782719254493713 - trainLoss: 0.5714631080627441\n",
      "cnt: 0 - valLoss: 0.5782666206359863 - trainLoss: 0.5714482665061951\n",
      "cnt: 0 - valLoss: 0.5782613158226013 - trainLoss: 0.5714324116706848\n",
      "cnt: 0 - valLoss: 0.5782559514045715 - trainLoss: 0.5714165568351746\n",
      "cnt: 0 - valLoss: 0.5782502293586731 - trainLoss: 0.5714007616043091\n",
      "cnt: 0 - valLoss: 0.5782445073127747 - trainLoss: 0.5713850259780884\n",
      "cnt: 0 - valLoss: 0.5782396793365479 - trainLoss: 0.5713691711425781\n",
      "cnt: 0 - valLoss: 0.5782235860824585 - trainLoss: 0.5713555216789246\n",
      "cnt: 0 - valLoss: 0.5782179832458496 - trainLoss: 0.571340024471283\n",
      "cnt: 0 - valLoss: 0.5782123804092407 - trainLoss: 0.5713241100311279\n",
      "cnt: 0 - valLoss: 0.5782065987586975 - trainLoss: 0.5713082551956177\n",
      "cnt: 0 - valLoss: 0.5782008767127991 - trainLoss: 0.5712923407554626\n",
      "cnt: 0 - valLoss: 0.5781950950622559 - trainLoss: 0.5712765455245972\n",
      "cnt: 0 - valLoss: 0.5781902074813843 - trainLoss: 0.5712607502937317\n",
      "cnt: 0 - valLoss: 0.5781740546226501 - trainLoss: 0.5712469816207886\n",
      "cnt: 0 - valLoss: 0.5781683325767517 - trainLoss: 0.5712314248085022\n",
      "cnt: 0 - valLoss: 0.5781626105308533 - trainLoss: 0.5712155103683472\n",
      "cnt: 0 - valLoss: 0.5781569480895996 - trainLoss: 0.5711995959281921\n",
      "cnt: 0 - valLoss: 0.5781511664390564 - trainLoss: 0.5711837410926819\n",
      "cnt: 0 - valLoss: 0.5781453847885132 - trainLoss: 0.5711680054664612\n",
      "cnt: 0 - valLoss: 0.578140139579773 - trainLoss: 0.5711522102355957\n",
      "cnt: 0 - valLoss: 0.5781241655349731 - trainLoss: 0.5711376070976257\n",
      "cnt: 0 - valLoss: 0.5781185030937195 - trainLoss: 0.5711228251457214\n",
      "cnt: 0 - valLoss: 0.5781127214431763 - trainLoss: 0.5711069107055664\n",
      "cnt: 0 - valLoss: 0.5781069993972778 - trainLoss: 0.5710909962654114\n",
      "cnt: 0 - valLoss: 0.5781012177467346 - trainLoss: 0.5710752010345459\n",
      "cnt: 0 - valLoss: 0.5780954360961914 - trainLoss: 0.5710594058036804\n",
      "cnt: 0 - valLoss: 0.5780895352363586 - trainLoss: 0.5710435509681702\n",
      "cnt: 0 - valLoss: 0.578082799911499 - trainLoss: 0.5710278153419495\n",
      "cnt: 0 - valLoss: 0.5780743956565857 - trainLoss: 0.5710126757621765\n",
      "cnt: 0 - valLoss: 0.5780666470527649 - trainLoss: 0.5709962844848633\n",
      "cnt: 0 - valLoss: 0.5780592560768127 - trainLoss: 0.5709791779518127\n",
      "cnt: 0 - valLoss: 0.5780512094497681 - trainLoss: 0.5709646940231323\n",
      "cnt: 0 - valLoss: 0.5780448317527771 - trainLoss: 0.5709473490715027\n",
      "cnt: 0 - valLoss: 0.5780361294746399 - trainLoss: 0.5709320306777954\n",
      "cnt: 0 - valLoss: 0.5780273675918579 - trainLoss: 0.5709141492843628\n",
      "cnt: 0 - valLoss: 0.5780190229415894 - trainLoss: 0.570896327495575\n",
      "cnt: 0 - valLoss: 0.5780130624771118 - trainLoss: 0.5708790421485901\n",
      "cnt: 0 - valLoss: 0.5780043005943298 - trainLoss: 0.5708647966384888\n",
      "cnt: 0 - valLoss: 0.5779955387115479 - trainLoss: 0.5708468556404114\n",
      "cnt: 0 - valLoss: 0.5779868960380554 - trainLoss: 0.5708289742469788\n",
      "cnt: 0 - valLoss: 0.5779799818992615 - trainLoss: 0.5708111524581909\n",
      "cnt: 0 - valLoss: 0.5779725909233093 - trainLoss: 0.5707970857620239\n",
      "cnt: 0 - valLoss: 0.5779638290405273 - trainLoss: 0.57077956199646\n",
      "cnt: 0 - valLoss: 0.5779550671577454 - trainLoss: 0.5707616209983826\n",
      "cnt: 0 - valLoss: 0.5779477953910828 - trainLoss: 0.5707437992095947\n",
      "cnt: 0 - valLoss: 0.577940821647644 - trainLoss: 0.5707288980484009\n",
      "cnt: 0 - valLoss: 0.5779319405555725 - trainLoss: 0.570712149143219\n",
      "cnt: 0 - valLoss: 0.5779231786727905 - trainLoss: 0.5706941485404968\n",
      "cnt: 0 - valLoss: 0.5779156684875488 - trainLoss: 0.5706762075424194\n",
      "cnt: 0 - valLoss: 0.5779087543487549 - trainLoss: 0.5706609487533569\n",
      "cnt: 0 - valLoss: 0.5778998732566833 - trainLoss: 0.5706445574760437\n",
      "cnt: 0 - valLoss: 0.5778911113739014 - trainLoss: 0.5706265568733215\n",
      "cnt: 0 - valLoss: 0.5778835415840149 - trainLoss: 0.5706085562705994\n",
      "cnt: 0 - valLoss: 0.5778767466545105 - trainLoss: 0.5705931782722473\n",
      "cnt: 0 - valLoss: 0.5778678059577942 - trainLoss: 0.5705769062042236\n",
      "cnt: 0 - valLoss: 0.5778589844703674 - trainLoss: 0.5705588459968567\n",
      "cnt: 0 - valLoss: 0.5778514742851257 - trainLoss: 0.5705407857894897\n",
      "cnt: 0 - valLoss: 0.5778446197509766 - trainLoss: 0.570525586605072\n",
      "cnt: 0 - valLoss: 0.5778356790542603 - trainLoss: 0.570509135723114\n",
      "cnt: 0 - valLoss: 0.5778267979621887 - trainLoss: 0.5704909563064575\n",
      "cnt: 0 - valLoss: 0.5778194665908813 - trainLoss: 0.5704728960990906\n",
      "cnt: 0 - valLoss: 0.5778124332427979 - trainLoss: 0.5704581141471863\n",
      "cnt: 0 - valLoss: 0.5778035521507263 - trainLoss: 0.5704411864280701\n",
      "cnt: 0 - valLoss: 0.5777947902679443 - trainLoss: 0.5704230070114136\n",
      "cnt: 0 - valLoss: 0.5777878761291504 - trainLoss: 0.5704048871994019\n",
      "cnt: 0 - valLoss: 0.5777806043624878 - trainLoss: 0.5703908205032349\n",
      "cnt: 0 - valLoss: 0.5777717232704163 - trainLoss: 0.5703731775283813\n",
      "cnt: 0 - valLoss: 0.5777631402015686 - trainLoss: 0.5703549385070801\n",
      "cnt: 0 - valLoss: 0.5777575969696045 - trainLoss: 0.5703372955322266\n",
      "cnt: 0 - valLoss: 0.5777486562728882 - trainLoss: 0.5703232288360596\n",
      "cnt: 0 - valLoss: 0.5777397751808167 - trainLoss: 0.5703049302101135\n",
      "cnt: 0 - valLoss: 0.5777319073677063 - trainLoss: 0.5702866315841675\n",
      "cnt: 0 - valLoss: 0.5777256488800049 - trainLoss: 0.5702706575393677\n",
      "cnt: 0 - valLoss: 0.5777167081832886 - trainLoss: 0.5702548623085022\n",
      "cnt: 0 - valLoss: 0.5777077674865723 - trainLoss: 0.5702365040779114\n",
      "cnt: 0 - valLoss: 0.5777007341384888 - trainLoss: 0.5702182650566101\n",
      "cnt: 0 - valLoss: 0.5776937007904053 - trainLoss: 0.570203959941864\n",
      "cnt: 0 - valLoss: 0.5776847004890442 - trainLoss: 0.5701864957809448\n",
      "cnt: 0 - valLoss: 0.5776762962341309 - trainLoss: 0.5701680779457092\n",
      "cnt: 0 - valLoss: 0.577670693397522 - trainLoss: 0.5701508522033691\n",
      "cnt: 0 - valLoss: 0.5776615738868713 - trainLoss: 0.570136308670044\n",
      "cnt: 0 - valLoss: 0.5776526927947998 - trainLoss: 0.5701178312301636\n",
      "cnt: 0 - valLoss: 0.5776451826095581 - trainLoss: 0.570099413394928\n",
      "cnt: 0 - valLoss: 0.5776385068893433 - trainLoss: 0.5700845122337341\n",
      "cnt: 0 - valLoss: 0.5776295065879822 - trainLoss: 0.5700675845146179\n",
      "cnt: 0 - valLoss: 0.5776210427284241 - trainLoss: 0.5700491666793823\n",
      "cnt: 0 - valLoss: 0.5776154398918152 - trainLoss: 0.5700318813323975\n",
      "cnt: 0 - valLoss: 0.5776063799858093 - trainLoss: 0.5700175762176514\n",
      "cnt: 0 - valLoss: 0.5775973796844482 - trainLoss: 0.569999098777771\n",
      "cnt: 0 - valLoss: 0.5775902271270752 - trainLoss: 0.5699806809425354\n",
      "cnt: 0 - valLoss: 0.577583372592926 - trainLoss: 0.5699664354324341\n",
      "cnt: 0 - valLoss: 0.5775743722915649 - trainLoss: 0.5699490308761597\n",
      "cnt: 0 - valLoss: 0.5775663256645203 - trainLoss: 0.5699305534362793\n",
      "cnt: 0 - valLoss: 0.5775606036186218 - trainLoss: 0.5699141621589661\n",
      "cnt: 0 - valLoss: 0.5775514841079712 - trainLoss: 0.5698989033699036\n",
      "cnt: 0 - valLoss: 0.577542781829834 - trainLoss: 0.5698803067207336\n",
      "cnt: 0 - valLoss: 0.5775377750396729 - trainLoss: 0.5698623061180115\n",
      "cnt: 0 - valLoss: 0.5775286555290222 - trainLoss: 0.5698486566543579\n",
      "cnt: 0 - valLoss: 0.5775195956230164 - trainLoss: 0.569830060005188\n",
      "cnt: 0 - valLoss: 0.5775125622749329 - trainLoss: 0.5698114633560181\n",
      "cnt: 0 - valLoss: 0.577505886554718 - trainLoss: 0.5697972774505615\n",
      "cnt: 0 - valLoss: 0.5774967670440674 - trainLoss: 0.5697796940803528\n",
      "cnt: 0 - valLoss: 0.5774890780448914 - trainLoss: 0.5697611570358276\n",
      "cnt: 0 - valLoss: 0.5774829983711243 - trainLoss: 0.5697453618049622\n",
      "cnt: 0 - valLoss: 0.5774738192558289 - trainLoss: 0.5697293877601624\n",
      "cnt: 0 - valLoss: 0.5774655342102051 - trainLoss: 0.5697107315063477\n",
      "cnt: 0 - valLoss: 0.5774601101875305 - trainLoss: 0.5696937441825867\n",
      "cnt: 0 - valLoss: 0.5774508714675903 - trainLoss: 0.5696789622306824\n",
      "cnt: 0 - valLoss: 0.5774421095848083 - trainLoss: 0.5696601867675781\n",
      "cnt: 0 - valLoss: 0.5774372220039368 - trainLoss: 0.5696424245834351\n",
      "cnt: 0 - valLoss: 0.5774279832839966 - trainLoss: 0.5696287751197815\n",
      "cnt: 0 - valLoss: 0.5774192214012146 - trainLoss: 0.5696099400520325\n",
      "cnt: 0 - valLoss: 0.577414333820343 - trainLoss: 0.569592297077179\n",
      "cnt: 0 - valLoss: 0.5774050354957581 - trainLoss: 0.5695786476135254\n",
      "cnt: 0 - valLoss: 0.5773966908454895 - trainLoss: 0.5695596933364868\n",
      "cnt: 0 - valLoss: 0.5773914456367493 - trainLoss: 0.5695428252220154\n",
      "cnt: 0 - valLoss: 0.5773820877075195 - trainLoss: 0.5695284008979797\n",
      "cnt: 0 - valLoss: 0.5773742198944092 - trainLoss: 0.5695093870162964\n",
      "cnt: 0 - valLoss: 0.5773684978485107 - trainLoss: 0.5694934725761414\n",
      "cnt: 0 - valLoss: 0.577359139919281 - trainLoss: 0.5694781541824341\n",
      "cnt: 0 - valLoss: 0.5773516893386841 - trainLoss: 0.569459080696106\n",
      "cnt: 0 - valLoss: 0.5773455500602722 - trainLoss: 0.5694440603256226\n",
      "cnt: 0 - valLoss: 0.5773361921310425 - trainLoss: 0.5694277286529541\n",
      "cnt: 0 - valLoss: 0.5773292183876038 - trainLoss: 0.569408655166626\n",
      "cnt: 0 - valLoss: 0.5773226022720337 - trainLoss: 0.5693947672843933\n",
      "cnt: 0 - valLoss: 0.5773133635520935 - trainLoss: 0.5693771839141846\n",
      "cnt: 0 - valLoss: 0.5773090124130249 - trainLoss: 0.5693583488464355\n",
      "cnt: 0 - valLoss: 0.5772994756698608 - trainLoss: 0.5693455338478088\n",
      "cnt: 0 - valLoss: 0.5772912502288818 - trainLoss: 0.5693262815475464\n",
      "cnt: 0 - valLoss: 0.5772859454154968 - trainLoss: 0.569309651851654\n",
      "cnt: 0 - valLoss: 0.5772764682769775 - trainLoss: 0.5692946910858154\n",
      "cnt: 0 - valLoss: 0.5772691369056702 - trainLoss: 0.5692754983901978\n",
      "cnt: 0 - valLoss: 0.5772628784179688 - trainLoss: 0.5692608952522278\n",
      "cnt: 0 - valLoss: 0.5772534608840942 - trainLoss: 0.569243848323822\n",
      "cnt: 0 - valLoss: 0.5772491693496704 - trainLoss: 0.5692247152328491\n",
      "cnt: 0 - valLoss: 0.5772395730018616 - trainLoss: 0.5692120790481567\n",
      "cnt: 0 - valLoss: 0.5772314071655273 - trainLoss: 0.5691927075386047\n",
      "cnt: 0 - valLoss: 0.5772259831428528 - trainLoss: 0.569176197052002\n",
      "cnt: 0 - valLoss: 0.577216386795044 - trainLoss: 0.5691609978675842\n",
      "cnt: 0 - valLoss: 0.5772093534469604 - trainLoss: 0.569141685962677\n",
      "cnt: 0 - valLoss: 0.5772027969360352 - trainLoss: 0.5691276788711548\n",
      "cnt: 0 - valLoss: 0.5771934986114502 - trainLoss: 0.569110095500946\n",
      "cnt: 0 - valLoss: 0.5771891474723816 - trainLoss: 0.569091260433197\n",
      "cnt: 0 - valLoss: 0.577179491519928 - trainLoss: 0.5690783262252808\n",
      "cnt: 0 - valLoss: 0.5771715044975281 - trainLoss: 0.5690587759017944\n",
      "cnt: 0 - valLoss: 0.5771657824516296 - trainLoss: 0.569042980670929\n",
      "cnt: 0 - valLoss: 0.577156126499176 - trainLoss: 0.5690269470214844\n",
      "cnt: 0 - valLoss: 0.5771495699882507 - trainLoss: 0.5690074563026428\n",
      "cnt: 0 - valLoss: 0.5771425366401672 - trainLoss: 0.5689945220947266\n",
      "cnt: 0 - valLoss: 0.5771339535713196 - trainLoss: 0.5689757466316223\n",
      "cnt: 0 - valLoss: 0.5771288275718689 - trainLoss: 0.5689585208892822\n",
      "cnt: 0 - valLoss: 0.5771191120147705 - trainLoss: 0.5689436793327332\n",
      "cnt: 0 - valLoss: 0.5771120190620422 - trainLoss: 0.5689241886138916\n",
      "cnt: 0 - valLoss: 0.5771054029464722 - trainLoss: 0.5689102411270142\n",
      "cnt: 0 - valLoss: 0.5770965218544006 - trainLoss: 0.5688921809196472\n",
      "cnt: 0 - valLoss: 0.5770916938781738 - trainLoss: 0.5688743591308594\n",
      "cnt: 0 - valLoss: 0.5770819187164307 - trainLoss: 0.5688601732254028\n",
      "cnt: 0 - valLoss: 0.5770746469497681 - trainLoss: 0.5688404440879822\n",
      "cnt: 0 - valLoss: 0.5770682096481323 - trainLoss: 0.5688261985778809\n",
      "cnt: 0 - valLoss: 0.5770592093467712 - trainLoss: 0.5688084363937378\n",
      "cnt: 0 - valLoss: 0.5770543813705444 - trainLoss: 0.5687904357910156\n",
      "cnt: 0 - valLoss: 0.5770446062088013 - trainLoss: 0.5687763690948486\n",
      "cnt: 0 - valLoss: 0.5770373344421387 - trainLoss: 0.5687565803527832\n",
      "cnt: 0 - valLoss: 0.5770308971405029 - trainLoss: 0.5687423348426819\n",
      "cnt: 0 - valLoss: 0.5770218968391418 - trainLoss: 0.5687244534492493\n",
      "cnt: 0 - valLoss: 0.5770171284675598 - trainLoss: 0.5687066912651062\n",
      "cnt: 0 - valLoss: 0.5770071744918823 - trainLoss: 0.5686922669410706\n",
      "cnt: 0 - valLoss: 0.5770001411437988 - trainLoss: 0.5686723589897156\n",
      "cnt: 0 - valLoss: 0.576993465423584 - trainLoss: 0.568658709526062\n",
      "cnt: 0 - valLoss: 0.5769847631454468 - trainLoss: 0.5686402320861816\n",
      "cnt: 0 - valLoss: 0.5769796371459961 - trainLoss: 0.5686231255531311\n",
      "cnt: 0 - valLoss: 0.5769696831703186 - trainLoss: 0.5686080455780029\n",
      "cnt: 0 - valLoss: 0.5769630074501038 - trainLoss: 0.5685880780220032\n",
      "cnt: 0 - valLoss: 0.5769559741020203 - trainLoss: 0.5685752034187317\n",
      "cnt: 0 - valLoss: 0.5769475102424622 - trainLoss: 0.5685560703277588\n",
      "cnt: 0 - valLoss: 0.5769422054290771 - trainLoss: 0.5685393810272217\n",
      "cnt: 0 - valLoss: 0.5769322514533997 - trainLoss: 0.5685238242149353\n",
      "cnt: 0 - valLoss: 0.5769282579421997 - trainLoss: 0.5685039758682251\n",
      "cnt: 0 - valLoss: 0.5769181847572327 - trainLoss: 0.5684913992881775\n",
      "cnt: 0 - valLoss: 0.5769105553627014 - trainLoss: 0.5684713125228882\n",
      "cnt: 0 - valLoss: 0.5769043564796448 - trainLoss: 0.5684563517570496\n",
      "cnt: 0 - valLoss: 0.5768952965736389 - trainLoss: 0.5684389472007751\n",
      "cnt: 0 - valLoss: 0.5768904685974121 - trainLoss: 0.5684210062026978\n",
      "cnt: 0 - valLoss: 0.5768803358078003 - trainLoss: 0.5684065222740173\n",
      "cnt: 0 - valLoss: 0.5768736004829407 - trainLoss: 0.5683863162994385\n",
      "cnt: 0 - valLoss: 0.5768665075302124 - trainLoss: 0.568373441696167\n",
      "cnt: 0 - valLoss: 0.5768584609031677 - trainLoss: 0.5683540105819702\n",
      "cnt: 0 - valLoss: 0.5768526196479797 - trainLoss: 0.5683380365371704\n",
      "cnt: 0 - valLoss: 0.57684326171875 - trainLoss: 0.5683214664459229\n",
      "cnt: 0 - valLoss: 0.5768386721611023 - trainLoss: 0.5683028697967529\n",
      "cnt: 0 - valLoss: 0.5768285393714905 - trainLoss: 0.5682888627052307\n",
      "cnt: 0 - valLoss: 0.5768215656280518 - trainLoss: 0.5682685971260071\n",
      "cnt: 0 - valLoss: 0.576814591884613 - trainLoss: 0.5682554244995117\n",
      "cnt: 0 - valLoss: 0.5768064856529236 - trainLoss: 0.5682360529899597\n",
      "cnt: 0 - valLoss: 0.5768006443977356 - trainLoss: 0.5682201981544495\n",
      "cnt: 0 - valLoss: 0.5767913460731506 - trainLoss: 0.5682033896446228\n",
      "cnt: 0 - valLoss: 0.5767866969108582 - trainLoss: 0.5681851506233215\n",
      "cnt: 0 - valLoss: 0.576776385307312 - trainLoss: 0.5681707262992859\n",
      "cnt: 0 - valLoss: 0.5767697691917419 - trainLoss: 0.5681503415107727\n",
      "cnt: 0 - valLoss: 0.5767624378204346 - trainLoss: 0.5681378245353699\n",
      "cnt: 0 - valLoss: 0.5767546892166138 - trainLoss: 0.5681177377700806\n",
      "cnt: 0 - valLoss: 0.5767484307289124 - trainLoss: 0.5681027173995972\n",
      "cnt: 0 - valLoss: 0.5767396092414856 - trainLoss: 0.5680849552154541\n",
      "cnt: 0 - valLoss: 0.5767344236373901 - trainLoss: 0.568067729473114\n",
      "cnt: 0 - valLoss: 0.576724648475647 - trainLoss: 0.5680521726608276\n",
      "cnt: 0 - valLoss: 0.5767202973365784 - trainLoss: 0.5680328607559204\n",
      "cnt: 0 - valLoss: 0.5767098069190979 - trainLoss: 0.5680192708969116\n",
      "cnt: 0 - valLoss: 0.5767030715942383 - trainLoss: 0.5679985880851746\n",
      "cnt: 0 - valLoss: 0.5766958594322205 - trainLoss: 0.5679857730865479\n",
      "cnt: 0 - valLoss: 0.5766879916191101 - trainLoss: 0.5679657459259033\n",
      "cnt: 0 - valLoss: 0.5766817927360535 - trainLoss: 0.5679507851600647\n",
      "cnt: 0 - valLoss: 0.5766730308532715 - trainLoss: 0.5679329037666321\n",
      "cnt: 0 - valLoss: 0.5766676664352417 - trainLoss: 0.5679160356521606\n",
      "cnt: 0 - valLoss: 0.5766581296920776 - trainLoss: 0.5678998827934265\n",
      "cnt: 0 - valLoss: 0.5766535401344299 - trainLoss: 0.5678813457489014\n",
      "cnt: 0 - valLoss: 0.5766432285308838 - trainLoss: 0.567866861820221\n",
      "cnt: 0 - valLoss: 0.5766392946243286 - trainLoss: 0.5678467154502869\n",
      "cnt: 0 - valLoss: 0.5766286253929138 - trainLoss: 0.5678337216377258\n",
      "cnt: 0 - valLoss: 0.5766216516494751 - trainLoss: 0.5678128600120544\n",
      "cnt: 0 - valLoss: 0.576614499092102 - trainLoss: 0.5677998065948486\n",
      "cnt: 0 - valLoss: 0.5766065716743469 - trainLoss: 0.5677800178527832\n",
      "cnt: 0 - valLoss: 0.5766003727912903 - trainLoss: 0.5677648782730103\n",
      "cnt: 0 - valLoss: 0.5765916705131531 - trainLoss: 0.5677468776702881\n",
      "cnt: 0 - valLoss: 0.5765860676765442 - trainLoss: 0.5677302479743958\n",
      "cnt: 0 - valLoss: 0.576576828956604 - trainLoss: 0.5677136778831482\n",
      "cnt: 0 - valLoss: 0.5765718817710876 - trainLoss: 0.567695677280426\n",
      "cnt: 0 - valLoss: 0.5765619874000549 - trainLoss: 0.5676803588867188\n",
      "cnt: 0 - valLoss: 0.5765575170516968 - trainLoss: 0.5676612854003906\n",
      "cnt: 0 - valLoss: 0.5765471458435059 - trainLoss: 0.5676470398902893\n",
      "cnt: 0 - valLoss: 0.5765432119369507 - trainLoss: 0.567626953125\n",
      "cnt: 0 - valLoss: 0.5765323638916016 - trainLoss: 0.5676136016845703\n",
      "cnt: 0 - valLoss: 0.5765289068222046 - trainLoss: 0.5675926208496094\n",
      "cnt: 0 - valLoss: 0.5765178203582764 - trainLoss: 0.5675801038742065\n",
      "cnt: 0 - valLoss: 0.5765108466148376 - trainLoss: 0.5675588846206665\n",
      "cnt: 0 - valLoss: 0.5765035152435303 - trainLoss: 0.567546010017395\n",
      "cnt: 0 - valLoss: 0.5764960646629333 - trainLoss: 0.5675255060195923\n",
      "cnt: 0 - valLoss: 0.576489269733429 - trainLoss: 0.5675115585327148\n",
      "cnt: 0 - valLoss: 0.5764812231063843 - trainLoss: 0.5674921870231628\n",
      "cnt: 0 - valLoss: 0.5764749646186829 - trainLoss: 0.5674771666526794\n",
      "cnt: 0 - valLoss: 0.57646644115448 - trainLoss: 0.5674586296081543\n",
      "cnt: 0 - valLoss: 0.5764606595039368 - trainLoss: 0.5674427151679993\n",
      "cnt: 0 - valLoss: 0.5764516592025757 - trainLoss: 0.5674250721931458\n",
      "cnt: 0 - valLoss: 0.5764462947845459 - trainLoss: 0.5674083828926086\n",
      "cnt: 0 - valLoss: 0.5764369368553162 - trainLoss: 0.5673914551734924\n",
      "cnt: 0 - valLoss: 0.5764318704605103 - trainLoss: 0.5673742294311523\n",
      "cnt: 0 - valLoss: 0.5764222145080566 - trainLoss: 0.5673577189445496\n",
      "cnt: 0 - valLoss: 0.5764174461364746 - trainLoss: 0.5673400163650513\n",
      "cnt: 0 - valLoss: 0.5764075517654419 - trainLoss: 0.5673240423202515\n",
      "cnt: 0 - valLoss: 0.5764029622077942 - trainLoss: 0.5673059225082397\n",
      "cnt: 0 - valLoss: 0.5763928890228271 - trainLoss: 0.5672902464866638\n",
      "cnt: 0 - valLoss: 0.5763884782791138 - trainLoss: 0.5672718286514282\n",
      "cnt: 0 - valLoss: 0.5763782262802124 - trainLoss: 0.5672563910484314\n",
      "cnt: 0 - valLoss: 0.576373815536499 - trainLoss: 0.567237913608551\n",
      "cnt: 0 - valLoss: 0.5763636231422424 - trainLoss: 0.5672224164009094\n",
      "cnt: 0 - valLoss: 0.5763591527938843 - trainLoss: 0.5672040581703186\n",
      "cnt: 0 - valLoss: 0.5763489603996277 - trainLoss: 0.5671883225440979\n",
      "cnt: 0 - valLoss: 0.5763445496559143 - trainLoss: 0.5671701431274414\n",
      "cnt: 0 - valLoss: 0.5763344168663025 - trainLoss: 0.5671542286872864\n",
      "cnt: 0 - valLoss: 0.5763298869132996 - trainLoss: 0.567136287689209\n",
      "cnt: 0 - valLoss: 0.5763198137283325 - trainLoss: 0.5671200752258301\n",
      "cnt: 0 - valLoss: 0.5763150453567505 - trainLoss: 0.5671024322509766\n",
      "cnt: 0 - valLoss: 0.5763051509857178 - trainLoss: 0.5670859217643738\n",
      "cnt: 0 - valLoss: 0.576300323009491 - trainLoss: 0.5670686364173889\n",
      "cnt: 0 - valLoss: 0.5762906074523926 - trainLoss: 0.5670517086982727\n",
      "cnt: 0 - valLoss: 0.5762855410575867 - trainLoss: 0.567034900188446\n",
      "cnt: 0 - valLoss: 0.5762760043144226 - trainLoss: 0.5670173764228821\n",
      "cnt: 0 - valLoss: 0.5762726068496704 - trainLoss: 0.5670011639595032\n",
      "cnt: 0 - valLoss: 0.5762614011764526 - trainLoss: 0.5669850707054138\n",
      "cnt: 0 - valLoss: 0.5762575268745422 - trainLoss: 0.566965639591217\n",
      "cnt: 0 - valLoss: 0.5762467980384827 - trainLoss: 0.5669505596160889\n",
      "cnt: 0 - valLoss: 0.5762442946434021 - trainLoss: 0.5669321417808533\n",
      "cnt: 0 - valLoss: 0.576232373714447 - trainLoss: 0.5669180154800415\n",
      "cnt: 0 - valLoss: 0.5762291550636292 - trainLoss: 0.5668970346450806\n",
      "cnt: 0 - valLoss: 0.576217770576477 - trainLoss: 0.5668833255767822\n",
      "cnt: 0 - valLoss: 0.5762157440185547 - trainLoss: 0.5668637752532959\n",
      "cnt: 0 - valLoss: 0.5762034058570862 - trainLoss: 0.5668506026268005\n",
      "cnt: 0 - valLoss: 0.5762004852294922 - trainLoss: 0.5668286085128784\n",
      "cnt: 0 - valLoss: 0.5761887431144714 - trainLoss: 0.5668156743049622\n",
      "cnt: 0 - valLoss: 0.5761870741844177 - trainLoss: 0.5667955875396729\n",
      "cnt: 0 - valLoss: 0.5761743783950806 - trainLoss: 0.5667828917503357\n",
      "cnt: 0 - valLoss: 0.5761736035346985 - trainLoss: 0.5667604804039001\n",
      "cnt: 0 - valLoss: 0.5761598348617554 - trainLoss: 0.5667500495910645\n",
      "cnt: 0 - valLoss: 0.5761588215827942 - trainLoss: 0.5667268633842468\n",
      "cnt: 0 - valLoss: 0.5761452317237854 - trainLoss: 0.5667157769203186\n",
      "cnt: 0 - valLoss: 0.5761440992355347 - trainLoss: 0.5666932463645935\n",
      "cnt: 0 - valLoss: 0.5761311650276184 - trainLoss: 0.5666815638542175\n",
      "cnt: 0 - valLoss: 0.5761229991912842 - trainLoss: 0.5666590929031372\n",
      "cnt: 0 - valLoss: 0.5761197209358215 - trainLoss: 0.5666456818580627\n",
      "cnt: 0 - valLoss: 0.5761086940765381 - trainLoss: 0.5666279792785645\n",
      "cnt: 0 - valLoss: 0.5761061906814575 - trainLoss: 0.5666105151176453\n",
      "cnt: 0 - valLoss: 0.5760942697525024 - trainLoss: 0.5665948987007141\n",
      "cnt: 0 - valLoss: 0.5760926008224487 - trainLoss: 0.5665755867958069\n",
      "cnt: 0 - valLoss: 0.5760798454284668 - trainLoss: 0.5665618181228638\n",
      "cnt: 0 - valLoss: 0.5760788917541504 - trainLoss: 0.5665407180786133\n",
      "cnt: 0 - valLoss: 0.5760664939880371 - trainLoss: 0.5665281414985657\n",
      "cnt: 0 - valLoss: 0.5760577321052551 - trainLoss: 0.5665069222450256\n",
      "cnt: 0 - valLoss: 0.5760548710823059 - trainLoss: 0.5664923191070557\n",
      "cnt: 0 - valLoss: 0.5760437250137329 - trainLoss: 0.566474974155426\n",
      "cnt: 0 - valLoss: 0.5760357975959778 - trainLoss: 0.5664553046226501\n",
      "cnt: 0 - valLoss: 0.5760323405265808 - trainLoss: 0.5664424896240234\n",
      "cnt: 0 - valLoss: 0.5760231018066406 - trainLoss: 0.5664231181144714\n",
      "cnt: 0 - valLoss: 0.5760143399238586 - trainLoss: 0.566405177116394\n",
      "cnt: 0 - valLoss: 0.5760133266448975 - trainLoss: 0.5663876533508301\n",
      "cnt: 0 - valLoss: 0.5760007500648499 - trainLoss: 0.5663728713989258\n",
      "cnt: 0 - valLoss: 0.5759924054145813 - trainLoss: 0.5663533210754395\n",
      "cnt: 0 - valLoss: 0.5759909749031067 - trainLoss: 0.5663371086120605\n",
      "cnt: 0 - valLoss: 0.5759814977645874 - trainLoss: 0.566320538520813\n",
      "cnt: 0 - valLoss: 0.5759721994400024 - trainLoss: 0.566302478313446\n",
      "cnt: 0 - valLoss: 0.5759633779525757 - trainLoss: 0.5662845373153687\n",
      "cnt: 0 - valLoss: 0.5759644508361816 - trainLoss: 0.5662674903869629\n",
      "cnt: 1 - valLoss: 0.5759515762329102 - trainLoss: 0.5662533044815063\n",
      "cnt: 0 - valLoss: 0.5759422183036804 - trainLoss: 0.5662329792976379\n",
      "cnt: 0 - valLoss: 0.5759341716766357 - trainLoss: 0.5662150382995605\n",
      "cnt: 0 - valLoss: 0.5759344100952148 - trainLoss: 0.5661996006965637\n",
      "cnt: 1 - valLoss: 0.5759236216545105 - trainLoss: 0.5661829710006714\n",
      "cnt: 0 - valLoss: 0.5759140849113464 - trainLoss: 0.5661642551422119\n",
      "cnt: 0 - valLoss: 0.5759052634239197 - trainLoss: 0.5661461353302002\n",
      "cnt: 0 - valLoss: 0.5759062767028809 - trainLoss: 0.5661291480064392\n",
      "cnt: 1 - valLoss: 0.5758953094482422 - trainLoss: 0.5661143660545349\n",
      "cnt: 0 - valLoss: 0.5758857727050781 - trainLoss: 0.5660953521728516\n",
      "cnt: 0 - valLoss: 0.5758762955665588 - trainLoss: 0.5660771727561951\n",
      "cnt: 0 - valLoss: 0.5758686661720276 - trainLoss: 0.5660591125488281\n",
      "cnt: 0 - valLoss: 0.5758686065673828 - trainLoss: 0.5660446882247925\n",
      "cnt: 0 - valLoss: 0.5758576393127441 - trainLoss: 0.5660271048545837\n",
      "cnt: 0 - valLoss: 0.5758481025695801 - trainLoss: 0.5660082697868347\n",
      "cnt: 0 - valLoss: 0.575839638710022 - trainLoss: 0.565990149974823\n",
      "cnt: 0 - valLoss: 0.5758402347564697 - trainLoss: 0.565974235534668\n",
      "cnt: 1 - valLoss: 0.5758290886878967 - trainLoss: 0.5659582018852234\n",
      "cnt: 0 - valLoss: 0.5758194923400879 - trainLoss: 0.56593918800354\n",
      "cnt: 0 - valLoss: 0.5758106708526611 - trainLoss: 0.5659209489822388\n",
      "cnt: 0 - valLoss: 0.5758115649223328 - trainLoss: 0.5659044981002808\n",
      "cnt: 1 - valLoss: 0.5758004188537598 - trainLoss: 0.5658891797065735\n",
      "cnt: 0 - valLoss: 0.5757906436920166 - trainLoss: 0.565869927406311\n",
      "cnt: 0 - valLoss: 0.5757817625999451 - trainLoss: 0.5658516883850098\n",
      "cnt: 0 - valLoss: 0.5757826566696167 - trainLoss: 0.5658349990844727\n",
      "cnt: 1 - valLoss: 0.5757713913917542 - trainLoss: 0.5658199191093445\n",
      "cnt: 0 - valLoss: 0.575761616230011 - trainLoss: 0.565800666809082\n",
      "cnt: 0 - valLoss: 0.5757526755332947 - trainLoss: 0.5657823085784912\n",
      "cnt: 0 - valLoss: 0.5757536292076111 - trainLoss: 0.5657657384872437\n",
      "cnt: 1 - valLoss: 0.5757423043251038 - trainLoss: 0.5657504796981812\n",
      "cnt: 0 - valLoss: 0.5757324695587158 - trainLoss: 0.5657311677932739\n",
      "cnt: 0 - valLoss: 0.5757237672805786 - trainLoss: 0.5657128691673279\n",
      "cnt: 0 - valLoss: 0.5757245421409607 - trainLoss: 0.5656968355178833\n",
      "cnt: 1 - valLoss: 0.5757132172584534 - trainLoss: 0.5656810998916626\n",
      "cnt: 0 - valLoss: 0.5757033228874207 - trainLoss: 0.5656617283821106\n",
      "cnt: 0 - valLoss: 0.5756946802139282 - trainLoss: 0.5656433701515198\n",
      "cnt: 0 - valLoss: 0.575695276260376 - trainLoss: 0.5656275749206543\n",
      "cnt: 1 - valLoss: 0.5756837725639343 - trainLoss: 0.5656114220619202\n",
      "cnt: 0 - valLoss: 0.5756737589836121 - trainLoss: 0.5655920505523682\n",
      "cnt: 0 - valLoss: 0.5756656527519226 - trainLoss: 0.5655736923217773\n",
      "cnt: 0 - valLoss: 0.5756657123565674 - trainLoss: 0.5655588507652283\n",
      "cnt: 1 - valLoss: 0.5756542086601257 - trainLoss: 0.5655415654182434\n",
      "cnt: 0 - valLoss: 0.5756444334983826 - trainLoss: 0.5655223727226257\n",
      "cnt: 0 - valLoss: 0.5756460428237915 - trainLoss: 0.5655043125152588\n",
      "cnt: 1 - valLoss: 0.5756343603134155 - trainLoss: 0.5654906630516052\n",
      "cnt: 0 - valLoss: 0.5756242275238037 - trainLoss: 0.5654709339141846\n",
      "cnt: 0 - valLoss: 0.5756153464317322 - trainLoss: 0.5654523968696594\n",
      "cnt: 0 - valLoss: 0.5756160616874695 - trainLoss: 0.5654363036155701\n",
      "cnt: 1 - valLoss: 0.5756043791770935 - trainLoss: 0.5654205083847046\n",
      "cnt: 0 - valLoss: 0.5755942463874817 - trainLoss: 0.5654008984565735\n",
      "cnt: 0 - valLoss: 0.5755862593650818 - trainLoss: 0.5653824210166931\n",
      "cnt: 0 - valLoss: 0.575586199760437 - trainLoss: 0.5653682351112366\n",
      "cnt: 0 - valLoss: 0.5755744576454163 - trainLoss: 0.5653504133224487\n",
      "cnt: 0 - valLoss: 0.575564980506897 - trainLoss: 0.565330982208252\n",
      "cnt: 0 - valLoss: 0.5755662322044373 - trainLoss: 0.5653136968612671\n",
      "cnt: 1 - valLoss: 0.5755543112754822 - trainLoss: 0.5652991533279419\n",
      "cnt: 0 - valLoss: 0.5755440592765808 - trainLoss: 0.5652793645858765\n",
      "cnt: 0 - valLoss: 0.575535774230957 - trainLoss: 0.5652607083320618\n",
      "cnt: 0 - valLoss: 0.5755359530448914 - trainLoss: 0.5652461647987366\n",
      "cnt: 1 - valLoss: 0.575524091720581 - trainLoss: 0.5652287602424622\n",
      "cnt: 0 - valLoss: 0.5755144357681274 - trainLoss: 0.565209150314331\n",
      "cnt: 0 - valLoss: 0.5755156874656677 - trainLoss: 0.5651919841766357\n",
      "cnt: 1 - valLoss: 0.5755036473274231 - trainLoss: 0.565177321434021\n",
      "cnt: 0 - valLoss: 0.5754932761192322 - trainLoss: 0.565157413482666\n",
      "cnt: 0 - valLoss: 0.5754852294921875 - trainLoss: 0.5651386380195618\n",
      "cnt: 0 - valLoss: 0.5754849910736084 - trainLoss: 0.5651247501373291\n",
      "cnt: 0 - valLoss: 0.5754729509353638 - trainLoss: 0.5651064515113831\n",
      "cnt: 0 - valLoss: 0.5754639506340027 - trainLoss: 0.5650869011878967\n",
      "cnt: 0 - valLoss: 0.5754644870758057 - trainLoss: 0.565071165561676\n",
      "cnt: 1 - valLoss: 0.5754547715187073 - trainLoss: 0.5650547742843628\n",
      "cnt: 0 - valLoss: 0.5754441618919373 - trainLoss: 0.5650359392166138\n",
      "cnt: 0 - valLoss: 0.5754352807998657 - trainLoss: 0.5650169849395752\n",
      "cnt: 0 - valLoss: 0.5754356980323792 - trainLoss: 0.5650014281272888\n",
      "cnt: 1 - valLoss: 0.5754287242889404 - trainLoss: 0.5649836659431458\n",
      "cnt: 0 - valLoss: 0.5754216909408569 - trainLoss: 0.5649653077125549\n",
      "cnt: 0 - valLoss: 0.575414776802063 - trainLoss: 0.5649470090866089\n",
      "cnt: 0 - valLoss: 0.5754077434539795 - trainLoss: 0.5649287104606628\n",
      "cnt: 0 - valLoss: 0.5754005908966064 - trainLoss: 0.564910352230072\n",
      "cnt: 0 - valLoss: 0.5753936171531677 - trainLoss: 0.5648919939994812\n",
      "cnt: 0 - valLoss: 0.5753864645957947 - trainLoss: 0.5648737549781799\n",
      "cnt: 0 - valLoss: 0.5753793716430664 - trainLoss: 0.5648553967475891\n",
      "cnt: 0 - valLoss: 0.5753722190856934 - trainLoss: 0.5648370385169983\n",
      "cnt: 0 - valLoss: 0.5753651261329651 - trainLoss: 0.5648187398910522\n",
      "cnt: 0 - valLoss: 0.5753579139709473 - trainLoss: 0.5648004412651062\n",
      "cnt: 0 - valLoss: 0.575350821018219 - trainLoss: 0.5647820830345154\n",
      "cnt: 0 - valLoss: 0.5753436088562012 - trainLoss: 0.5647637248039246\n",
      "cnt: 0 - valLoss: 0.5753364562988281 - trainLoss: 0.5647453665733337\n",
      "cnt: 0 - valLoss: 0.5753291845321655 - trainLoss: 0.5647270679473877\n",
      "cnt: 0 - valLoss: 0.5753219723701477 - trainLoss: 0.5647087097167969\n",
      "cnt: 0 - valLoss: 0.5753148198127747 - trainLoss: 0.5646904110908508\n",
      "cnt: 0 - valLoss: 0.5753075480461121 - trainLoss: 0.56467205286026\n",
      "cnt: 0 - valLoss: 0.5753003358840942 - trainLoss: 0.5646536946296692\n",
      "cnt: 0 - valLoss: 0.5752930045127869 - trainLoss: 0.5646353363990784\n",
      "cnt: 0 - valLoss: 0.5752857327461243 - trainLoss: 0.5646169781684875\n",
      "cnt: 0 - valLoss: 0.5752784609794617 - trainLoss: 0.5645986199378967\n",
      "cnt: 0 - valLoss: 0.5752711892127991 - trainLoss: 0.5645802617073059\n",
      "cnt: 0 - valLoss: 0.5752639174461365 - trainLoss: 0.5645619034767151\n",
      "cnt: 0 - valLoss: 0.5752565860748291 - trainLoss: 0.5645435452461243\n",
      "cnt: 0 - valLoss: 0.5752492547035217 - trainLoss: 0.5645251274108887\n",
      "cnt: 0 - valLoss: 0.5752419233322144 - trainLoss: 0.5645067691802979\n",
      "cnt: 0 - valLoss: 0.575234591960907 - trainLoss: 0.564488410949707\n",
      "cnt: 0 - valLoss: 0.5752272605895996 - trainLoss: 0.5644699931144714\n",
      "cnt: 0 - valLoss: 0.5752199292182922 - trainLoss: 0.5644516944885254\n",
      "cnt: 0 - valLoss: 0.5752125382423401 - trainLoss: 0.5644332766532898\n",
      "cnt: 0 - valLoss: 0.5752051472663879 - trainLoss: 0.5644148588180542\n",
      "cnt: 0 - valLoss: 0.5751978158950806 - trainLoss: 0.5643964409828186\n",
      "cnt: 0 - valLoss: 0.5751904249191284 - trainLoss: 0.5643780827522278\n",
      "cnt: 0 - valLoss: 0.575183093547821 - trainLoss: 0.5643596649169922\n",
      "cnt: 0 - valLoss: 0.5751758813858032 - trainLoss: 0.5643413066864014\n",
      "cnt: 0 - valLoss: 0.5751684904098511 - trainLoss: 0.5643229484558105\n",
      "cnt: 0 - valLoss: 0.5751610994338989 - trainLoss: 0.564304530620575\n",
      "cnt: 0 - valLoss: 0.575153648853302 - trainLoss: 0.5642860531806946\n",
      "cnt: 0 - valLoss: 0.5751460790634155 - trainLoss: 0.5642676949501038\n",
      "cnt: 0 - valLoss: 0.5751401782035828 - trainLoss: 0.5642493963241577\n",
      "cnt: 0 - valLoss: 0.5751326084136963 - trainLoss: 0.5642311573028564\n",
      "cnt: 0 - valLoss: 0.5751250386238098 - trainLoss: 0.5642127394676208\n",
      "cnt: 0 - valLoss: 0.5751174688339233 - trainLoss: 0.5641944408416748\n",
      "cnt: 0 - valLoss: 0.575111448764801 - trainLoss: 0.5641761422157288\n",
      "cnt: 0 - valLoss: 0.5751038789749146 - trainLoss: 0.5641579031944275\n",
      "cnt: 0 - valLoss: 0.5750961899757385 - trainLoss: 0.5641395449638367\n",
      "cnt: 0 - valLoss: 0.575090229511261 - trainLoss: 0.5641211271286011\n",
      "cnt: 0 - valLoss: 0.5750824809074402 - trainLoss: 0.5641029477119446\n",
      "cnt: 0 - valLoss: 0.5750748515129089 - trainLoss: 0.564084529876709\n",
      "cnt: 0 - valLoss: 0.5750671625137329 - trainLoss: 0.5640661716461182\n",
      "cnt: 0 - valLoss: 0.5750611424446106 - trainLoss: 0.5640478730201721\n",
      "cnt: 0 - valLoss: 0.575053334236145 - trainLoss: 0.5640295743942261\n",
      "cnt: 0 - valLoss: 0.5750472545623779 - trainLoss: 0.5640112161636353\n",
      "cnt: 0 - valLoss: 0.5750394463539124 - trainLoss: 0.5639929175376892\n",
      "cnt: 0 - valLoss: 0.5750316977500916 - trainLoss: 0.5639745593070984\n",
      "cnt: 0 - valLoss: 0.5750255584716797 - trainLoss: 0.5639562010765076\n",
      "cnt: 0 - valLoss: 0.5750177502632141 - trainLoss: 0.5639379024505615\n",
      "cnt: 0 - valLoss: 0.5750116109848022 - trainLoss: 0.5639195442199707\n",
      "cnt: 0 - valLoss: 0.5750037431716919 - trainLoss: 0.5639012455940247\n",
      "cnt: 0 - valLoss: 0.5749959349632263 - trainLoss: 0.5638828277587891\n",
      "cnt: 0 - valLoss: 0.5749897360801697 - trainLoss: 0.563864529132843\n",
      "cnt: 0 - valLoss: 0.5749818682670593 - trainLoss: 0.5638461709022522\n",
      "cnt: 0 - valLoss: 0.5749756097793579 - trainLoss: 0.5638278126716614\n",
      "cnt: 0 - valLoss: 0.5749676823616028 - trainLoss: 0.5638094544410706\n",
      "cnt: 0 - valLoss: 0.5749613642692566 - trainLoss: 0.5637910962104797\n",
      "cnt: 0 - valLoss: 0.5749534964561462 - trainLoss: 0.5637727379798889\n",
      "cnt: 0 - valLoss: 0.5749471187591553 - trainLoss: 0.5637543201446533\n",
      "cnt: 0 - valLoss: 0.5749391913414001 - trainLoss: 0.5637360215187073\n",
      "cnt: 0 - valLoss: 0.574932873249054 - trainLoss: 0.5637176036834717\n",
      "cnt: 0 - valLoss: 0.5749248266220093 - trainLoss: 0.5636993050575256\n",
      "cnt: 0 - valLoss: 0.5749184489250183 - trainLoss: 0.56368088722229\n",
      "cnt: 0 - valLoss: 0.5749104619026184 - trainLoss: 0.5636624693870544\n",
      "cnt: 0 - valLoss: 0.5749040842056274 - trainLoss: 0.5636441707611084\n",
      "cnt: 0 - valLoss: 0.5748960375785828 - trainLoss: 0.5636257529258728\n",
      "cnt: 0 - valLoss: 0.574889600276947 - trainLoss: 0.563607394695282\n",
      "cnt: 0 - valLoss: 0.574883222579956 - trainLoss: 0.5635889768600464\n",
      "cnt: 0 - valLoss: 0.5748750567436218 - trainLoss: 0.5635706186294556\n",
      "cnt: 0 - valLoss: 0.5748686194419861 - trainLoss: 0.5635522603988647\n",
      "cnt: 0 - valLoss: 0.5748604536056519 - trainLoss: 0.5635338425636292\n",
      "cnt: 0 - valLoss: 0.5748540759086609 - trainLoss: 0.5635154843330383\n",
      "cnt: 0 - valLoss: 0.5748474597930908 - trainLoss: 0.5634970664978027\n",
      "cnt: 0 - valLoss: 0.5748392939567566 - trainLoss: 0.5634785890579224\n",
      "cnt: 0 - valLoss: 0.5748327970504761 - trainLoss: 0.5634602904319763\n",
      "cnt: 0 - valLoss: 0.5748262405395508 - trainLoss: 0.5634418725967407\n",
      "cnt: 0 - valLoss: 0.574817955493927 - trainLoss: 0.5634235143661499\n",
      "cnt: 0 - valLoss: 0.5748113989830017 - trainLoss: 0.5634050965309143\n",
      "cnt: 0 - valLoss: 0.5748048424720764 - trainLoss: 0.5633866190910339\n",
      "cnt: 0 - valLoss: 0.5747964978218079 - trainLoss: 0.5633682608604431\n",
      "cnt: 0 - valLoss: 0.5747899413108826 - trainLoss: 0.5633498430252075\n",
      "cnt: 0 - valLoss: 0.5747833251953125 - trainLoss: 0.5633314251899719\n",
      "cnt: 0 - valLoss: 0.5747767090797424 - trainLoss: 0.5633129477500916\n",
      "cnt: 0 - valLoss: 0.5747683048248291 - trainLoss: 0.5632945895195007\n",
      "cnt: 0 - valLoss: 0.5747616291046143 - trainLoss: 0.5632762312889099\n",
      "cnt: 0 - valLoss: 0.5747549533843994 - trainLoss: 0.5632577538490295\n",
      "cnt: 0 - valLoss: 0.5747482776641846 - trainLoss: 0.563239336013794\n",
      "cnt: 0 - valLoss: 0.5747414827346802 - trainLoss: 0.5632208585739136\n",
      "cnt: 0 - valLoss: 0.5747330188751221 - trainLoss: 0.563202440738678\n",
      "cnt: 0 - valLoss: 0.5747262239456177 - trainLoss: 0.5631840825080872\n",
      "cnt: 0 - valLoss: 0.5747194886207581 - trainLoss: 0.5631656646728516\n",
      "cnt: 0 - valLoss: 0.5747126340866089 - trainLoss: 0.5631471872329712\n",
      "cnt: 0 - valLoss: 0.5747057795524597 - trainLoss: 0.5631287693977356\n",
      "cnt: 0 - valLoss: 0.5746988654136658 - trainLoss: 0.5631103515625\n",
      "cnt: 0 - valLoss: 0.5746902823448181 - trainLoss: 0.5630919337272644\n",
      "cnt: 0 - valLoss: 0.574683427810669 - trainLoss: 0.5630735158920288\n",
      "cnt: 0 - valLoss: 0.574676513671875 - trainLoss: 0.5630550980567932\n",
      "cnt: 0 - valLoss: 0.5746696591377258 - trainLoss: 0.5630366206169128\n",
      "cnt: 0 - valLoss: 0.5746626853942871 - trainLoss: 0.563018262386322\n",
      "cnt: 0 - valLoss: 0.5746557712554932 - trainLoss: 0.5629997849464417\n",
      "cnt: 0 - valLoss: 0.5746487975120544 - trainLoss: 0.5629814267158508\n",
      "cnt: 0 - valLoss: 0.5746418237686157 - trainLoss: 0.5629629492759705\n",
      "cnt: 0 - valLoss: 0.5746347904205322 - trainLoss: 0.5629445314407349\n",
      "cnt: 0 - valLoss: 0.5746277570724487 - trainLoss: 0.5629261136054993\n",
      "cnt: 0 - valLoss: 0.5746207237243652 - trainLoss: 0.5629076361656189\n",
      "cnt: 0 - valLoss: 0.5746136903762817 - trainLoss: 0.5628892183303833\n",
      "cnt: 0 - valLoss: 0.5746066570281982 - trainLoss: 0.5628708004951477\n",
      "cnt: 0 - valLoss: 0.5745995044708252 - trainLoss: 0.5628523230552673\n",
      "cnt: 0 - valLoss: 0.5745924115180969 - trainLoss: 0.5628339648246765\n",
      "cnt: 0 - valLoss: 0.5745853185653687 - trainLoss: 0.5628154277801514\n",
      "cnt: 0 - valLoss: 0.5745781660079956 - trainLoss: 0.5627970099449158\n",
      "cnt: 0 - valLoss: 0.5745710134506226 - trainLoss: 0.5627785325050354\n",
      "cnt: 0 - valLoss: 0.5745638608932495 - trainLoss: 0.5627601742744446\n",
      "cnt: 0 - valLoss: 0.5745567083358765 - trainLoss: 0.5627416372299194\n",
      "cnt: 0 - valLoss: 0.5745495557785034 - trainLoss: 0.5627231597900391\n",
      "cnt: 0 - valLoss: 0.5745422840118408 - trainLoss: 0.5627047419548035\n",
      "cnt: 0 - valLoss: 0.5745351314544678 - trainLoss: 0.5626862645149231\n",
      "cnt: 0 - valLoss: 0.5745280385017395 - trainLoss: 0.5626678466796875\n",
      "cnt: 0 - valLoss: 0.5745366811752319 - trainLoss: 0.5626494884490967\n",
      "cnt: 1 - valLoss: 0.5745266675949097 - trainLoss: 0.5626336336135864\n",
      "cnt: 0 - valLoss: 0.574517011642456 - trainLoss: 0.5626145601272583\n",
      "cnt: 0 - valLoss: 0.574507474899292 - trainLoss: 0.5625956058502197\n",
      "cnt: 0 - valLoss: 0.5744979381561279 - trainLoss: 0.5625765919685364\n",
      "cnt: 0 - valLoss: 0.574488639831543 - trainLoss: 0.5625576972961426\n",
      "cnt: 0 - valLoss: 0.5744794011116028 - trainLoss: 0.5625388622283936\n",
      "cnt: 0 - valLoss: 0.5744721293449402 - trainLoss: 0.5625202655792236\n",
      "cnt: 0 - valLoss: 0.5744808316230774 - trainLoss: 0.5625019669532776\n",
      "cnt: 1 - valLoss: 0.5744706392288208 - trainLoss: 0.5624862313270569\n",
      "cnt: 0 - valLoss: 0.5744606256484985 - trainLoss: 0.5624670386314392\n",
      "cnt: 0 - valLoss: 0.5744509100914001 - trainLoss: 0.5624479651451111\n",
      "cnt: 0 - valLoss: 0.5744413137435913 - trainLoss: 0.562428891658783\n",
      "cnt: 0 - valLoss: 0.5744317770004272 - trainLoss: 0.5624099373817444\n",
      "cnt: 0 - valLoss: 0.5744242668151855 - trainLoss: 0.5623909831047058\n",
      "cnt: 0 - valLoss: 0.5744166970252991 - trainLoss: 0.5623725652694702\n",
      "cnt: 0 - valLoss: 0.5744094848632812 - trainLoss: 0.5623539686203003\n",
      "cnt: 0 - valLoss: 0.5744180679321289 - trainLoss: 0.5623360872268677\n",
      "cnt: 1 - valLoss: 0.5744076371192932 - trainLoss: 0.5623201727867126\n",
      "cnt: 0 - valLoss: 0.5743973851203918 - trainLoss: 0.5623008608818054\n",
      "cnt: 0 - valLoss: 0.5743873119354248 - trainLoss: 0.562281608581543\n",
      "cnt: 0 - valLoss: 0.5743775963783264 - trainLoss: 0.5622624754905701\n",
      "cnt: 0 - valLoss: 0.5743680000305176 - trainLoss: 0.5622434616088867\n",
      "cnt: 0 - valLoss: 0.5743768811225891 - trainLoss: 0.562224805355072\n",
      "cnt: 1 - valLoss: 0.5743662118911743 - trainLoss: 0.5622096061706543\n",
      "cnt: 0 - valLoss: 0.5743559002876282 - trainLoss: 0.5621902942657471\n",
      "cnt: 0 - valLoss: 0.5743456482887268 - trainLoss: 0.5621709227561951\n",
      "cnt: 0 - valLoss: 0.574335515499115 - trainLoss: 0.5621516704559326\n",
      "cnt: 0 - valLoss: 0.5743261575698853 - trainLoss: 0.5621324777603149\n",
      "cnt: 0 - valLoss: 0.5743347406387329 - trainLoss: 0.5621145963668823\n",
      "cnt: 1 - valLoss: 0.5743240118026733 - trainLoss: 0.5620987415313721\n",
      "cnt: 0 - valLoss: 0.5743135213851929 - trainLoss: 0.5620793104171753\n",
      "cnt: 0 - valLoss: 0.574303150177002 - trainLoss: 0.5620599389076233\n",
      "cnt: 0 - valLoss: 0.5742929577827454 - trainLoss: 0.5620406270027161\n",
      "cnt: 0 - valLoss: 0.5742840766906738 - trainLoss: 0.5620213747024536\n",
      "cnt: 0 - valLoss: 0.5742918848991394 - trainLoss: 0.5620052218437195\n",
      "cnt: 1 - valLoss: 0.5742810964584351 - trainLoss: 0.561987578868866\n",
      "cnt: 0 - valLoss: 0.5742704272270203 - trainLoss: 0.5619680881500244\n",
      "cnt: 0 - valLoss: 0.5742600560188293 - trainLoss: 0.5619486570358276\n",
      "cnt: 0 - valLoss: 0.5742501616477966 - trainLoss: 0.5619293451309204\n",
      "cnt: 0 - valLoss: 0.5742586255073547 - trainLoss: 0.5619111061096191\n",
      "cnt: 1 - valLoss: 0.5742476582527161 - trainLoss: 0.5618955492973328\n",
      "cnt: 0 - valLoss: 0.5742368698120117 - trainLoss: 0.5618759989738464\n",
      "cnt: 0 - valLoss: 0.5742263197898865 - trainLoss: 0.5618565082550049\n",
      "cnt: 0 - valLoss: 0.5742159485816956 - trainLoss: 0.5618370771408081\n",
      "cnt: 0 - valLoss: 0.5742246508598328 - trainLoss: 0.56181800365448\n",
      "cnt: 1 - valLoss: 0.5742135643959045 - trainLoss: 0.5618033409118652\n",
      "cnt: 0 - valLoss: 0.5742026567459106 - trainLoss: 0.5617836713790894\n",
      "cnt: 0 - valLoss: 0.5741919875144958 - trainLoss: 0.5617640614509583\n",
      "cnt: 0 - valLoss: 0.5741814970970154 - trainLoss: 0.5617446303367615\n",
      "cnt: 0 - valLoss: 0.5741903185844421 - trainLoss: 0.5617255568504333\n",
      "cnt: 1 - valLoss: 0.5741789937019348 - trainLoss: 0.5617108941078186\n",
      "cnt: 0 - valLoss: 0.5741679668426514 - trainLoss: 0.5616911053657532\n",
      "cnt: 0 - valLoss: 0.5741571187973022 - trainLoss: 0.5616715550422668\n",
      "cnt: 0 - valLoss: 0.5741468667984009 - trainLoss: 0.5616519451141357\n",
      "cnt: 0 - valLoss: 0.574155330657959 - trainLoss: 0.5616335272789001\n",
      "cnt: 1 - valLoss: 0.5741440057754517 - trainLoss: 0.5616182684898376\n",
      "cnt: 0 - valLoss: 0.5741328597068787 - trainLoss: 0.561598539352417\n",
      "cnt: 0 - valLoss: 0.57412189245224 - trainLoss: 0.5615788102149963\n",
      "cnt: 0 - valLoss: 0.5741121172904968 - trainLoss: 0.5615592002868652\n",
      "cnt: 0 - valLoss: 0.5741201043128967 - trainLoss: 0.5615421533584595\n",
      "cnt: 1 - valLoss: 0.5741086006164551 - trainLoss: 0.5615255236625671\n",
      "cnt: 0 - valLoss: 0.5740973949432373 - trainLoss: 0.5615056753158569\n",
      "cnt: 0 - valLoss: 0.5740863084793091 - trainLoss: 0.5614859461784363\n",
      "cnt: 0 - valLoss: 0.5740772485733032 - trainLoss: 0.5614662766456604\n",
      "cnt: 0 - valLoss: 0.5740844011306763 - trainLoss: 0.5614509582519531\n",
      "cnt: 1 - valLoss: 0.5740728974342346 - trainLoss: 0.5614326000213623\n",
      "cnt: 0 - valLoss: 0.5740616321563721 - trainLoss: 0.5614127516746521\n",
      "cnt: 0 - valLoss: 0.5740509629249573 - trainLoss: 0.5613929629325867\n",
      "cnt: 0 - valLoss: 0.5740595459938049 - trainLoss: 0.5613744258880615\n",
      "cnt: 1 - valLoss: 0.5740477442741394 - trainLoss: 0.5613592863082886\n",
      "cnt: 0 - valLoss: 0.574036180973053 - trainLoss: 0.5613393187522888\n",
      "cnt: 0 - valLoss: 0.5740248560905457 - trainLoss: 0.5613194704055786\n",
      "cnt: 0 - valLoss: 0.5740156173706055 - trainLoss: 0.5612995624542236\n",
      "cnt: 0 - valLoss: 0.5740227699279785 - trainLoss: 0.5612843632698059\n",
      "cnt: 1 - valLoss: 0.574010968208313 - trainLoss: 0.5612659454345703\n",
      "cnt: 0 - valLoss: 0.5739994049072266 - trainLoss: 0.5612459182739258\n",
      "cnt: 0 - valLoss: 0.5739890933036804 - trainLoss: 0.5612260103225708\n",
      "cnt: 0 - valLoss: 0.5739971399307251 - trainLoss: 0.5612086057662964\n",
      "cnt: 1 - valLoss: 0.5739850997924805 - trainLoss: 0.5611923933029175\n",
      "cnt: 0 - valLoss: 0.5739732980728149 - trainLoss: 0.5611721873283386\n",
      "cnt: 0 - valLoss: 0.5739623308181763 - trainLoss: 0.5611522197723389\n",
      "cnt: 0 - valLoss: 0.5739708542823792 - trainLoss: 0.5611333847045898\n",
      "cnt: 1 - valLoss: 0.5739586353302002 - trainLoss: 0.5611186027526855\n",
      "cnt: 0 - valLoss: 0.5739467740058899 - trainLoss: 0.5610983371734619\n",
      "cnt: 0 - valLoss: 0.573935329914093 - trainLoss: 0.5610781908035278\n",
      "cnt: 0 - valLoss: 0.5739419460296631 - trainLoss: 0.561058759689331\n",
      "cnt: 1 - valLoss: 0.5739297866821289 - trainLoss: 0.5610438585281372\n",
      "cnt: 0 - valLoss: 0.5739177465438843 - trainLoss: 0.5610237121582031\n",
      "cnt: 0 - valLoss: 0.5739073157310486 - trainLoss: 0.561003565788269\n",
      "cnt: 0 - valLoss: 0.5739151835441589 - trainLoss: 0.5609862804412842\n",
      "cnt: 1 - valLoss: 0.5739028453826904 - trainLoss: 0.5609699487686157\n",
      "cnt: 0 - valLoss: 0.573890745639801 - trainLoss: 0.5609496235847473\n",
      "cnt: 0 - valLoss: 0.5738801956176758 - trainLoss: 0.5609293580055237\n",
      "cnt: 0 - valLoss: 0.5738880038261414 - trainLoss: 0.5609121322631836\n",
      "cnt: 1 - valLoss: 0.5738755464553833 - trainLoss: 0.5608956813812256\n",
      "cnt: 0 - valLoss: 0.5738633275032043 - trainLoss: 0.5608753561973572\n",
      "cnt: 0 - valLoss: 0.5738528966903687 - trainLoss: 0.5608550310134888\n",
      "cnt: 0 - valLoss: 0.5738605856895447 - trainLoss: 0.5608382821083069\n",
      "cnt: 1 - valLoss: 0.5738479495048523 - trainLoss: 0.5608214139938354\n",
      "cnt: 0 - valLoss: 0.5738356709480286 - trainLoss: 0.5608009696006775\n",
      "cnt: 0 - valLoss: 0.573825478553772 - trainLoss: 0.5607806444168091\n",
      "cnt: 0 - valLoss: 0.5738327503204346 - trainLoss: 0.5607646703720093\n",
      "cnt: 1 - valLoss: 0.5738201141357422 - trainLoss: 0.560746967792511\n",
      "cnt: 0 - valLoss: 0.5738076567649841 - trainLoss: 0.5607264041900635\n",
      "cnt: 0 - valLoss: 0.5737979412078857 - trainLoss: 0.5607060790061951\n",
      "cnt: 0 - valLoss: 0.5738047957420349 - trainLoss: 0.5606912970542908\n",
      "cnt: 1 - valLoss: 0.5737919807434082 - trainLoss: 0.5606723427772522\n",
      "cnt: 0 - valLoss: 0.5737797617912292 - trainLoss: 0.5606518387794495\n",
      "cnt: 0 - valLoss: 0.5737864375114441 - trainLoss: 0.5606318712234497\n",
      "cnt: 1 - valLoss: 0.5737735033035278 - trainLoss: 0.5606173276901245\n",
      "cnt: 0 - valLoss: 0.5737608075141907 - trainLoss: 0.5605967044830322\n",
      "cnt: 0 - valLoss: 0.5737507939338684 - trainLoss: 0.5605761408805847\n",
      "cnt: 0 - valLoss: 0.5737576484680176 - trainLoss: 0.5605610609054565\n",
      "cnt: 1 - valLoss: 0.5737486481666565 - trainLoss: 0.5605423450469971\n",
      "cnt: 0 - valLoss: 0.573735773563385 - trainLoss: 0.5605229139328003\n",
      "cnt: 0 - valLoss: 0.573724627494812 - trainLoss: 0.560502290725708\n",
      "cnt: 0 - valLoss: 0.5737323760986328 - trainLoss: 0.5604850053787231\n",
      "cnt: 1 - valLoss: 0.5737231969833374 - trainLoss: 0.5604671835899353\n",
      "cnt: 0 - valLoss: 0.5737140774726868 - trainLoss: 0.5604478120803833\n",
      "cnt: 0 - valLoss: 0.5737050771713257 - trainLoss: 0.5604285597801208\n",
      "cnt: 0 - valLoss: 0.5736961960792542 - trainLoss: 0.5604093074798584\n",
      "cnt: 0 - valLoss: 0.5736873745918274 - trainLoss: 0.5603899955749512\n",
      "cnt: 0 - valLoss: 0.5736768245697021 - trainLoss: 0.560370683670044\n",
      "cnt: 0 - valLoss: 0.5736838579177856 - trainLoss: 0.5603550672531128\n",
      "cnt: 1 - valLoss: 0.5736744999885559 - trainLoss: 0.5603343844413757\n",
      "cnt: 0 - valLoss: 0.5736653208732605 - trainLoss: 0.5603150129318237\n",
      "cnt: 0 - valLoss: 0.5736562013626099 - trainLoss: 0.5602957010269165\n",
      "cnt: 0 - valLoss: 0.5736472606658936 - trainLoss: 0.5602763295173645\n",
      "cnt: 0 - valLoss: 0.5736384987831116 - trainLoss: 0.5602570176124573\n",
      "cnt: 0 - valLoss: 0.5736492872238159 - trainLoss: 0.5602383017539978\n",
      "cnt: 1 - valLoss: 0.5736395120620728 - trainLoss: 0.5602211356163025\n",
      "cnt: 2 - valLoss: 0.5736298561096191 - trainLoss: 0.5602017045021057\n",
      "cnt: 0 - valLoss: 0.5736202597618103 - trainLoss: 0.5601822733879089\n",
      "cnt: 0 - valLoss: 0.5736109018325806 - trainLoss: 0.5601627826690674\n",
      "cnt: 0 - valLoss: 0.5736016035079956 - trainLoss: 0.5601434111595154\n",
      "cnt: 0 - valLoss: 0.5735924243927002 - trainLoss: 0.5601240396499634\n",
      "cnt: 0 - valLoss: 0.5735841989517212 - trainLoss: 0.5601046681404114\n",
      "cnt: 0 - valLoss: 0.573594331741333 - trainLoss: 0.560087263584137\n",
      "cnt: 1 - valLoss: 0.5735843777656555 - trainLoss: 0.5600688457489014\n",
      "cnt: 2 - valLoss: 0.5735745429992676 - trainLoss: 0.560049295425415\n",
      "cnt: 0 - valLoss: 0.5735647678375244 - trainLoss: 0.5600298047065735\n",
      "cnt: 0 - valLoss: 0.5735552310943604 - trainLoss: 0.5600103139877319\n",
      "cnt: 0 - valLoss: 0.5735457539558411 - trainLoss: 0.5599908232688904\n",
      "cnt: 0 - valLoss: 0.5735374093055725 - trainLoss: 0.5599713921546936\n",
      "cnt: 0 - valLoss: 0.5735474824905396 - trainLoss: 0.5599539875984192\n",
      "cnt: 1 - valLoss: 0.5735373497009277 - trainLoss: 0.5599356889724731\n",
      "cnt: 0 - valLoss: 0.5735272765159607 - trainLoss: 0.559916079044342\n",
      "cnt: 0 - valLoss: 0.573517382144928 - trainLoss: 0.5598964691162109\n",
      "cnt: 0 - valLoss: 0.5735076069831848 - trainLoss: 0.5598769187927246\n",
      "cnt: 0 - valLoss: 0.5734981298446655 - trainLoss: 0.5598574280738831\n",
      "cnt: 0 - valLoss: 0.5735090374946594 - trainLoss: 0.5598383545875549\n",
      "cnt: 1 - valLoss: 0.573498547077179 - trainLoss: 0.5598218441009521\n",
      "cnt: 2 - valLoss: 0.5734882354736328 - trainLoss: 0.5598020553588867\n",
      "cnt: 0 - valLoss: 0.573478102684021 - trainLoss: 0.5597824454307556\n",
      "cnt: 0 - valLoss: 0.5734681487083435 - trainLoss: 0.5597627758979797\n",
      "cnt: 0 - valLoss: 0.5734583735466003 - trainLoss: 0.5597431659698486\n",
      "cnt: 0 - valLoss: 0.5734632015228271 - trainLoss: 0.5597237944602966\n",
      "cnt: 1 - valLoss: 0.5734552145004272 - trainLoss: 0.5597076416015625\n",
      "cnt: 0 - valLoss: 0.5734474062919617 - trainLoss: 0.5596878528594971\n",
      "cnt: 0 - valLoss: 0.5734378695487976 - trainLoss: 0.5596681833267212\n",
      "cnt: 0 - valLoss: 0.573427677154541 - trainLoss: 0.5596484541893005\n",
      "cnt: 0 - valLoss: 0.573418140411377 - trainLoss: 0.5596287846565247\n",
      "cnt: 0 - valLoss: 0.5734157562255859 - trainLoss: 0.5596101880073547\n",
      "cnt: 0 - valLoss: 0.5734078288078308 - trainLoss: 0.5595932602882385\n",
      "cnt: 0 - valLoss: 0.5733999013900757 - trainLoss: 0.5595734119415283\n",
      "cnt: 0 - valLoss: 0.5733920931816101 - trainLoss: 0.5595536231994629\n",
      "cnt: 0 - valLoss: 0.5733842849731445 - trainLoss: 0.5595338940620422\n",
      "cnt: 0 - valLoss: 0.5733776092529297 - trainLoss: 0.5595141649246216\n",
      "cnt: 0 - valLoss: 0.5733684301376343 - trainLoss: 0.5594971179962158\n",
      "cnt: 0 - valLoss: 0.5733605027198792 - trainLoss: 0.5594787001609802\n",
      "cnt: 0 - valLoss: 0.573352575302124 - trainLoss: 0.5594587922096252\n",
      "cnt: 0 - valLoss: 0.5733447670936584 - trainLoss: 0.5594388842582703\n",
      "cnt: 0 - valLoss: 0.5733374953269958 - trainLoss: 0.5594191551208496\n",
      "cnt: 0 - valLoss: 0.5733290314674377 - trainLoss: 0.5594004988670349\n",
      "cnt: 0 - valLoss: 0.5733211636543274 - trainLoss: 0.5593836903572083\n",
      "cnt: 0 - valLoss: 0.5733132362365723 - trainLoss: 0.5593637228012085\n",
      "cnt: 0 - valLoss: 0.5733053088188171 - trainLoss: 0.5593437552452087\n",
      "cnt: 0 - valLoss: 0.5732977986335754 - trainLoss: 0.5593238472938538\n",
      "cnt: 0 - valLoss: 0.5732897520065308 - trainLoss: 0.5593047142028809\n",
      "cnt: 0 - valLoss: 0.5732817649841309 - trainLoss: 0.559288501739502\n",
      "cnt: 0 - valLoss: 0.5732738971710205 - trainLoss: 0.5592684149742126\n",
      "cnt: 0 - valLoss: 0.5732659697532654 - trainLoss: 0.5592484474182129\n",
      "cnt: 0 - valLoss: 0.5732585787773132 - trainLoss: 0.5592284798622131\n",
      "cnt: 0 - valLoss: 0.5732505321502686 - trainLoss: 0.5592095255851746\n",
      "cnt: 0 - valLoss: 0.5732425451278687 - trainLoss: 0.5591931343078613\n",
      "cnt: 0 - valLoss: 0.5732346177101135 - trainLoss: 0.5591729283332825\n",
      "cnt: 0 - valLoss: 0.5732266902923584 - trainLoss: 0.5591529011726379\n",
      "cnt: 0 - valLoss: 0.5732196569442749 - trainLoss: 0.5591329336166382\n",
      "cnt: 0 - valLoss: 0.5732112526893616 - trainLoss: 0.5591148734092712\n",
      "cnt: 0 - valLoss: 0.5732032656669617 - trainLoss: 0.5590975880622864\n",
      "cnt: 0 - valLoss: 0.5731952786445618 - trainLoss: 0.5590773820877075\n",
      "cnt: 0 - valLoss: 0.5731874108314514 - trainLoss: 0.5590572953224182\n",
      "cnt: 0 - valLoss: 0.5731810927391052 - trainLoss: 0.5590371489524841\n",
      "cnt: 0 - valLoss: 0.5731720924377441 - trainLoss: 0.5590205788612366\n",
      "cnt: 0 - valLoss: 0.5731641054153442 - trainLoss: 0.5590018630027771\n",
      "cnt: 0 - valLoss: 0.5731561183929443 - trainLoss: 0.5589815974235535\n",
      "cnt: 0 - valLoss: 0.5731485486030579 - trainLoss: 0.5589615106582642\n",
      "cnt: 0 - valLoss: 0.5731409192085266 - trainLoss: 0.5589420795440674\n",
      "cnt: 0 - valLoss: 0.5731330513954163 - trainLoss: 0.5589258670806885\n",
      "cnt: 0 - valLoss: 0.5731251835823059 - trainLoss: 0.5589061379432678\n",
      "cnt: 0 - valLoss: 0.5731173157691956 - trainLoss: 0.5588864088058472\n",
      "cnt: 0 - valLoss: 0.57310950756073 - trainLoss: 0.5588666796684265\n",
      "cnt: 0 - valLoss: 0.5731015801429749 - trainLoss: 0.5588469505310059\n",
      "cnt: 0 - valLoss: 0.5730937719345093 - trainLoss: 0.5588271617889404\n",
      "cnt: 0 - valLoss: 0.5730859041213989 - trainLoss: 0.5588074922561646\n",
      "cnt: 0 - valLoss: 0.5730780363082886 - trainLoss: 0.5587877631187439\n",
      "cnt: 0 - valLoss: 0.5730701684951782 - trainLoss: 0.5587679743766785\n",
      "cnt: 0 - valLoss: 0.5730623602867126 - trainLoss: 0.5587482452392578\n",
      "cnt: 0 - valLoss: 0.5730544328689575 - trainLoss: 0.5587285161018372\n",
      "cnt: 0 - valLoss: 0.5730466246604919 - trainLoss: 0.5587088465690613\n",
      "cnt: 0 - valLoss: 0.5730388164520264 - trainLoss: 0.5586889982223511\n",
      "cnt: 0 - valLoss: 0.5730308890342712 - trainLoss: 0.5586693286895752\n",
      "cnt: 0 - valLoss: 0.5730230808258057 - trainLoss: 0.5586495399475098\n",
      "cnt: 0 - valLoss: 0.5730152130126953 - trainLoss: 0.5586298704147339\n",
      "cnt: 0 - valLoss: 0.573007345199585 - trainLoss: 0.5586100816726685\n",
      "cnt: 0 - valLoss: 0.5729994773864746 - trainLoss: 0.5585903525352478\n",
      "cnt: 0 - valLoss: 0.5729917287826538 - trainLoss: 0.5585705637931824\n",
      "cnt: 0 - valLoss: 0.5729838013648987 - trainLoss: 0.5585508346557617\n",
      "cnt: 0 - valLoss: 0.5729759931564331 - trainLoss: 0.5585310459136963\n",
      "cnt: 0 - valLoss: 0.5729681253433228 - trainLoss: 0.5585113167762756\n",
      "cnt: 0 - valLoss: 0.5729602575302124 - trainLoss: 0.5584915280342102\n",
      "cnt: 0 - valLoss: 0.5729524493217468 - trainLoss: 0.5584717392921448\n",
      "cnt: 0 - valLoss: 0.5729445815086365 - trainLoss: 0.5584520101547241\n",
      "cnt: 0 - valLoss: 0.5729367733001709 - trainLoss: 0.5584322810173035\n",
      "cnt: 0 - valLoss: 0.5729288458824158 - trainLoss: 0.5584124326705933\n",
      "cnt: 0 - valLoss: 0.5729210376739502 - trainLoss: 0.5583927035331726\n",
      "cnt: 0 - valLoss: 0.5729131698608398 - trainLoss: 0.5583729147911072\n",
      "cnt: 0 - valLoss: 0.5729053616523743 - trainLoss: 0.5583531260490417\n",
      "cnt: 0 - valLoss: 0.5728974938392639 - trainLoss: 0.5583333373069763\n",
      "cnt: 0 - valLoss: 0.5728896856307983 - trainLoss: 0.5583135485649109\n",
      "cnt: 0 - valLoss: 0.5728818774223328 - trainLoss: 0.5582937598228455\n",
      "cnt: 0 - valLoss: 0.5728740096092224 - trainLoss: 0.55827397108078\n",
      "cnt: 0 - valLoss: 0.5728661417961121 - trainLoss: 0.5582541823387146\n",
      "cnt: 0 - valLoss: 0.5728583335876465 - trainLoss: 0.5582343339920044\n",
      "cnt: 0 - valLoss: 0.5728505253791809 - trainLoss: 0.558214545249939\n",
      "cnt: 0 - valLoss: 0.5728427171707153 - trainLoss: 0.5581947565078735\n",
      "cnt: 0 - valLoss: 0.572834849357605 - trainLoss: 0.5581749677658081\n",
      "cnt: 0 - valLoss: 0.5728270411491394 - trainLoss: 0.5581551194190979\n",
      "cnt: 0 - valLoss: 0.572819173336029 - trainLoss: 0.5581353306770325\n",
      "cnt: 0 - valLoss: 0.5728113651275635 - trainLoss: 0.5581154823303223\n",
      "cnt: 0 - valLoss: 0.5728035569190979 - trainLoss: 0.5580956935882568\n",
      "cnt: 0 - valLoss: 0.5727957487106323 - trainLoss: 0.5580759048461914\n",
      "cnt: 0 - valLoss: 0.572787880897522 - trainLoss: 0.5580560564994812\n",
      "cnt: 0 - valLoss: 0.5727800726890564 - trainLoss: 0.558036208152771\n",
      "cnt: 0 - valLoss: 0.5727725028991699 - trainLoss: 0.5580164194107056\n",
      "cnt: 0 - valLoss: 0.5727666616439819 - trainLoss: 0.5579971075057983\n",
      "cnt: 0 - valLoss: 0.5727587342262268 - trainLoss: 0.5579779148101807\n",
      "cnt: 0 - valLoss: 0.5727508068084717 - trainLoss: 0.5579580664634705\n",
      "cnt: 0 - valLoss: 0.572742760181427 - trainLoss: 0.5579381585121155\n",
      "cnt: 0 - valLoss: 0.5727348923683167 - trainLoss: 0.5579182505607605\n",
      "cnt: 0 - valLoss: 0.5727269649505615 - trainLoss: 0.5578984022140503\n",
      "cnt: 0 - valLoss: 0.5727190375328064 - trainLoss: 0.5578785538673401\n",
      "cnt: 0 - valLoss: 0.5727111101150513 - trainLoss: 0.5578586459159851\n",
      "cnt: 0 - valLoss: 0.5727032423019409 - trainLoss: 0.5578387379646301\n",
      "cnt: 0 - valLoss: 0.5726953744888306 - trainLoss: 0.5578188896179199\n",
      "cnt: 0 - valLoss: 0.5726874470710754 - trainLoss: 0.5577989816665649\n",
      "cnt: 0 - valLoss: 0.5726795792579651 - trainLoss: 0.5577791333198547\n",
      "cnt: 0 - valLoss: 0.5726717114448547 - trainLoss: 0.5577592253684998\n",
      "cnt: 0 - valLoss: 0.5726637840270996 - trainLoss: 0.5577393174171448\n",
      "cnt: 0 - valLoss: 0.5726560354232788 - trainLoss: 0.5577194690704346\n",
      "cnt: 0 - valLoss: 0.5726507306098938 - trainLoss: 0.5576996207237244\n",
      "cnt: 0 - valLoss: 0.5726427435874939 - trainLoss: 0.5576814413070679\n",
      "cnt: 0 - valLoss: 0.5726346969604492 - trainLoss: 0.5576614737510681\n",
      "cnt: 0 - valLoss: 0.5726267099380493 - trainLoss: 0.5576414465904236\n",
      "cnt: 0 - valLoss: 0.5726187229156494 - trainLoss: 0.5576215386390686\n",
      "cnt: 0 - valLoss: 0.5726107358932495 - trainLoss: 0.5576015114784241\n",
      "cnt: 0 - valLoss: 0.5726028084754944 - trainLoss: 0.5575816035270691\n",
      "cnt: 0 - valLoss: 0.5725948810577393 - trainLoss: 0.5575616359710693\n",
      "cnt: 0 - valLoss: 0.5725868940353394 - trainLoss: 0.5575416088104248\n",
      "cnt: 0 - valLoss: 0.572579026222229 - trainLoss: 0.5575217008590698\n",
      "cnt: 0 - valLoss: 0.5725710988044739 - trainLoss: 0.5575017929077148\n",
      "cnt: 0 - valLoss: 0.5725635290145874 - trainLoss: 0.5574818253517151\n",
      "cnt: 0 - valLoss: 0.5725583434104919 - trainLoss: 0.5574625730514526\n",
      "cnt: 0 - valLoss: 0.5725502967834473 - trainLoss: 0.5574440360069275\n",
      "cnt: 0 - valLoss: 0.5725421905517578 - trainLoss: 0.5574239492416382\n",
      "cnt: 0 - valLoss: 0.5725340843200684 - trainLoss: 0.5574038624763489\n",
      "cnt: 0 - valLoss: 0.5725260972976685 - trainLoss: 0.5573838353157043\n",
      "cnt: 0 - valLoss: 0.5725181102752686 - trainLoss: 0.5573638081550598\n",
      "cnt: 0 - valLoss: 0.5725101232528687 - trainLoss: 0.5573437809944153\n",
      "cnt: 0 - valLoss: 0.5725021362304688 - trainLoss: 0.5573237538337708\n",
      "cnt: 0 - valLoss: 0.5724941492080688 - trainLoss: 0.5573037266731262\n",
      "cnt: 0 - valLoss: 0.5724867582321167 - trainLoss: 0.5572836995124817\n",
      "cnt: 0 - valLoss: 0.572481632232666 - trainLoss: 0.5572647452354431\n",
      "cnt: 0 - valLoss: 0.5724734663963318 - trainLoss: 0.5572461485862732\n",
      "cnt: 0 - valLoss: 0.5724653601646423 - trainLoss: 0.5572260022163391\n",
      "cnt: 0 - valLoss: 0.5724571943283081 - trainLoss: 0.5572057962417603\n",
      "cnt: 0 - valLoss: 0.5724491477012634 - trainLoss: 0.5571856498718262\n",
      "cnt: 0 - valLoss: 0.5724411010742188 - trainLoss: 0.5571655631065369\n",
      "cnt: 0 - valLoss: 0.5724330544471741 - trainLoss: 0.5571455359458923\n",
      "cnt: 0 - valLoss: 0.5724250674247742 - trainLoss: 0.5571253299713135\n",
      "cnt: 0 - valLoss: 0.5724179744720459 - trainLoss: 0.557105302810669\n",
      "cnt: 0 - valLoss: 0.5724127292633057 - trainLoss: 0.5570870637893677\n",
      "cnt: 0 - valLoss: 0.5724045038223267 - trainLoss: 0.55706787109375\n",
      "cnt: 0 - valLoss: 0.5723963379859924 - trainLoss: 0.5570476055145264\n",
      "cnt: 0 - valLoss: 0.5723881721496582 - trainLoss: 0.5570273995399475\n",
      "cnt: 0 - valLoss: 0.5723800659179688 - trainLoss: 0.5570071935653687\n",
      "cnt: 0 - valLoss: 0.5723719000816345 - trainLoss: 0.5569869875907898\n",
      "cnt: 0 - valLoss: 0.5723639726638794 - trainLoss: 0.5569668412208557\n",
      "cnt: 0 - valLoss: 0.5723569393157959 - trainLoss: 0.5569467544555664\n",
      "cnt: 0 - valLoss: 0.5723516941070557 - trainLoss: 0.5569286942481995\n",
      "cnt: 0 - valLoss: 0.5723434090614319 - trainLoss: 0.556909441947937\n",
      "cnt: 0 - valLoss: 0.5723351836204529 - trainLoss: 0.5568891167640686\n",
      "cnt: 0 - valLoss: 0.5723270177841187 - trainLoss: 0.5568687319755554\n",
      "cnt: 0 - valLoss: 0.5723188519477844 - trainLoss: 0.5568485260009766\n",
      "cnt: 0 - valLoss: 0.5723106861114502 - trainLoss: 0.5568283200263977\n",
      "cnt: 0 - valLoss: 0.572303295135498 - trainLoss: 0.5568080544471741\n",
      "cnt: 0 - valLoss: 0.5722978115081787 - trainLoss: 0.5567892789840698\n",
      "cnt: 0 - valLoss: 0.5722902417182922 - trainLoss: 0.556770920753479\n",
      "cnt: 0 - valLoss: 0.5722820162773132 - trainLoss: 0.556750476360321\n",
      "cnt: 0 - valLoss: 0.5722737908363342 - trainLoss: 0.5567300915718079\n",
      "cnt: 0 - valLoss: 0.5722655653953552 - trainLoss: 0.5567097663879395\n",
      "cnt: 0 - valLoss: 0.5722573399543762 - trainLoss: 0.556689441204071\n",
      "cnt: 0 - valLoss: 0.5722503662109375 - trainLoss: 0.5566691160202026\n",
      "cnt: 0 - valLoss: 0.5722357630729675 - trainLoss: 0.5566513538360596\n",
      "cnt: 0 - valLoss: 0.5722300410270691 - trainLoss: 0.5566320419311523\n",
      "cnt: 0 - valLoss: 0.5722243785858154 - trainLoss: 0.5566115379333496\n",
      "cnt: 0 - valLoss: 0.5722185373306274 - trainLoss: 0.5565911531448364\n",
      "cnt: 0 - valLoss: 0.5722121596336365 - trainLoss: 0.5565707683563232\n",
      "cnt: 0 - valLoss: 0.57220458984375 - trainLoss: 0.5565503835678101\n",
      "cnt: 0 - valLoss: 0.5721734762191772 - trainLoss: 0.5565313696861267\n",
      "cnt: 0 - valLoss: 0.5721538662910461 - trainLoss: 0.5565123558044434\n",
      "cnt: 0 - valLoss: 0.572134792804718 - trainLoss: 0.5564914345741272\n",
      "cnt: 0 - valLoss: 0.5721163153648376 - trainLoss: 0.5564705729484558\n",
      "cnt: 0 - valLoss: 0.5720981955528259 - trainLoss: 0.5564497113227844\n",
      "cnt: 0 - valLoss: 0.5720806121826172 - trainLoss: 0.5564289689064026\n",
      "cnt: 0 - valLoss: 0.5720635056495667 - trainLoss: 0.5564082264900208\n",
      "cnt: 0 - valLoss: 0.57204669713974 - trainLoss: 0.5563874840736389\n",
      "cnt: 0 - valLoss: 0.5720281600952148 - trainLoss: 0.5563666820526123\n",
      "cnt: 0 - valLoss: 0.5720100998878479 - trainLoss: 0.5563458204269409\n",
      "cnt: 0 - valLoss: 0.5719931721687317 - trainLoss: 0.5563250780105591\n",
      "cnt: 0 - valLoss: 0.5719766616821289 - trainLoss: 0.556304395198822\n",
      "cnt: 0 - valLoss: 0.5719605088233948 - trainLoss: 0.5562836527824402\n",
      "cnt: 0 - valLoss: 0.5719448328018188 - trainLoss: 0.5562629699707031\n",
      "cnt: 0 - valLoss: 0.5719369649887085 - trainLoss: 0.5562423467636108\n",
      "cnt: 0 - valLoss: 0.5719214677810669 - trainLoss: 0.5562218427658081\n",
      "cnt: 0 - valLoss: 0.5719137191772461 - trainLoss: 0.5562012195587158\n",
      "cnt: 0 - valLoss: 0.5718984603881836 - trainLoss: 0.5561806559562683\n",
      "cnt: 0 - valLoss: 0.5718833804130554 - trainLoss: 0.556160032749176\n",
      "cnt: 0 - valLoss: 0.5718761682510376 - trainLoss: 0.5561395287513733\n",
      "cnt: 0 - valLoss: 0.5718612670898438 - trainLoss: 0.5561188459396362\n",
      "cnt: 0 - valLoss: 0.5718541741371155 - trainLoss: 0.5560983419418335\n",
      "cnt: 0 - valLoss: 0.5718393921852112 - trainLoss: 0.5560777187347412\n",
      "cnt: 0 - valLoss: 0.5718545913696289 - trainLoss: 0.5560572147369385\n",
      "cnt: 1 - valLoss: 0.5718385577201843 - trainLoss: 0.5560376644134521\n",
      "cnt: 0 - valLoss: 0.5718230605125427 - trainLoss: 0.5560169816017151\n",
      "cnt: 0 - valLoss: 0.5718078017234802 - trainLoss: 0.555996298789978\n",
      "cnt: 0 - valLoss: 0.5718226432800293 - trainLoss: 0.5559756755828857\n",
      "cnt: 1 - valLoss: 0.5718062520027161 - trainLoss: 0.5559561848640442\n",
      "cnt: 0 - valLoss: 0.5717902183532715 - trainLoss: 0.5559354424476624\n",
      "cnt: 0 - valLoss: 0.5717745423316956 - trainLoss: 0.5559147596359253\n",
      "cnt: 0 - valLoss: 0.5717891454696655 - trainLoss: 0.5558943152427673\n",
      "cnt: 1 - valLoss: 0.5717722773551941 - trainLoss: 0.5558746457099915\n",
      "cnt: 0 - valLoss: 0.5717558264732361 - trainLoss: 0.5558538436889648\n",
      "cnt: 0 - valLoss: 0.5717398524284363 - trainLoss: 0.555833101272583\n",
      "cnt: 0 - valLoss: 0.5717542767524719 - trainLoss: 0.555813193321228\n",
      "cnt: 1 - valLoss: 0.5717371106147766 - trainLoss: 0.5557929873466492\n",
      "cnt: 0 - valLoss: 0.5717204213142395 - trainLoss: 0.5557722449302673\n",
      "cnt: 0 - valLoss: 0.5717343688011169 - trainLoss: 0.5557515621185303\n",
      "cnt: 1 - valLoss: 0.5717166066169739 - trainLoss: 0.5557320713996887\n",
      "cnt: 0 - valLoss: 0.571699321269989 - trainLoss: 0.5557112097740173\n",
      "cnt: 0 - valLoss: 0.5716825127601624 - trainLoss: 0.5556904077529907\n",
      "cnt: 0 - valLoss: 0.571696400642395 - trainLoss: 0.5556707382202148\n",
      "cnt: 1 - valLoss: 0.5716785788536072 - trainLoss: 0.5556502938270569\n",
      "cnt: 0 - valLoss: 0.5716612339019775 - trainLoss: 0.5556293725967407\n",
      "cnt: 0 - valLoss: 0.5716747045516968 - trainLoss: 0.5556092858314514\n",
      "cnt: 1 - valLoss: 0.5716587901115417 - trainLoss: 0.5555892586708069\n",
      "cnt: 0 - valLoss: 0.5716408491134644 - trainLoss: 0.5555684566497803\n",
      "cnt: 0 - valLoss: 0.5716549158096313 - trainLoss: 0.555547833442688\n",
      "cnt: 1 - valLoss: 0.5715711712837219 - trainLoss: 0.5555301904678345\n",
      "cnt: 0 - valLoss: 0.5715869665145874 - trainLoss: 0.5555146932601929\n",
      "cnt: 1 - valLoss: 0.5716013312339783 - trainLoss: 0.5554913878440857\n",
      "cnt: 2 - valLoss: 0.5716166496276855 - trainLoss: 0.5554684996604919\n",
      "cnt: 3 - valLoss: 0.5715324282646179 - trainLoss: 0.5554521679878235\n",
      "cnt: 0 - valLoss: 0.5715481638908386 - trainLoss: 0.5554353594779968\n",
      "cnt: 1 - valLoss: 0.5715625882148743 - trainLoss: 0.5554119348526001\n",
      "cnt: 2 - valLoss: 0.5715793371200562 - trainLoss: 0.5553891062736511\n",
      "cnt: 3 - valLoss: 0.5714936256408691 - trainLoss: 0.5553744435310364\n",
      "cnt: 0 - valLoss: 0.5715094208717346 - trainLoss: 0.5553559064865112\n",
      "cnt: 1 - valLoss: 0.5715247392654419 - trainLoss: 0.555332601070404\n",
      "cnt: 2 - valLoss: 0.5714711546897888 - trainLoss: 0.555311381816864\n",
      "cnt: 0 - valLoss: 0.5714866518974304 - trainLoss: 0.5552958846092224\n",
      "cnt: 1 - valLoss: 0.5715035796165466 - trainLoss: 0.5552725791931152\n",
      "cnt: 2 - valLoss: 0.5714476704597473 - trainLoss: 0.555255651473999\n",
      "cnt: 0 - valLoss: 0.5714632868766785 - trainLoss: 0.5552358627319336\n",
      "cnt: 1 - valLoss: 0.5714110136032104 - trainLoss: 0.5552135705947876\n",
      "cnt: 0 - valLoss: 0.5714272260665894 - trainLoss: 0.5551990866661072\n",
      "cnt: 1 - valLoss: 0.5714447498321533 - trainLoss: 0.5551755428314209\n",
      "cnt: 2 - valLoss: 0.5713896155357361 - trainLoss: 0.555158257484436\n",
      "cnt: 0 - valLoss: 0.5714059472084045 - trainLoss: 0.5551388263702393\n",
      "cnt: 1 - valLoss: 0.5713542103767395 - trainLoss: 0.5551164746284485\n",
      "cnt: 0 - valLoss: 0.5713708996772766 - trainLoss: 0.5551018118858337\n",
      "cnt: 1 - valLoss: 0.5713892579078674 - trainLoss: 0.5550782084465027\n",
      "cnt: 2 - valLoss: 0.5713343620300293 - trainLoss: 0.5550615191459656\n",
      "cnt: 0 - valLoss: 0.5713516473770142 - trainLoss: 0.5550413131713867\n",
      "cnt: 1 - valLoss: 0.5712980031967163 - trainLoss: 0.5550199151039124\n",
      "cnt: 0 - valLoss: 0.5713896751403809 - trainLoss: 0.555005669593811\n",
      "cnt: 1 - valLoss: 0.5713145136833191 - trainLoss: 0.5549951195716858\n",
      "cnt: 2 - valLoss: 0.5712563395500183 - trainLoss: 0.5549623966217041\n",
      "cnt: 0 - valLoss: 0.5713537931442261 - trainLoss: 0.5549535751342773\n",
      "cnt: 1 - valLoss: 0.5712791681289673 - trainLoss: 0.5549376606941223\n",
      "cnt: 2 - valLoss: 0.5712167024612427 - trainLoss: 0.5549051761627197\n",
      "cnt: 0 - valLoss: 0.5713195204734802 - trainLoss: 0.5549008846282959\n",
      "cnt: 1 - valLoss: 0.5712421536445618 - trainLoss: 0.554880678653717\n",
      "cnt: 2 - valLoss: 0.5713375806808472 - trainLoss: 0.5548481345176697\n",
      "cnt: 3 - valLoss: 0.5712607502937317 - trainLoss: 0.554853081703186\n",
      "cnt: 4 - valLoss: 0.571186900138855 - trainLoss: 0.5548179149627686\n",
      "cnt: 0 - valLoss: 0.5712909698486328 - trainLoss: 0.5548006892204285\n",
      "cnt: 1 - valLoss: 0.5712155699729919 - trainLoss: 0.5547929406166077\n",
      "cnt: 2 - valLoss: 0.5711361169815063 - trainLoss: 0.554758608341217\n",
      "cnt: 0 - valLoss: 0.5712231397628784 - trainLoss: 0.5547515153884888\n",
      "cnt: 1 - valLoss: 0.571134090423584 - trainLoss: 0.5547271966934204\n",
      "cnt: 0 - valLoss: 0.5712442398071289 - trainLoss: 0.5547081232070923\n",
      "cnt: 1 - valLoss: 0.5711669325828552 - trainLoss: 0.5547003746032715\n",
      "cnt: 2 - valLoss: 0.5711774230003357 - trainLoss: 0.5546675324440002\n",
      "cnt: 3 - valLoss: 0.5710829496383667 - trainLoss: 0.5546519160270691\n",
      "cnt: 0 - valLoss: 0.5711750388145447 - trainLoss: 0.5546371936798096\n",
      "cnt: 1 - valLoss: 0.571082353591919 - trainLoss: 0.5546188354492188\n",
      "cnt: 0 - valLoss: 0.5710992217063904 - trainLoss: 0.5545989274978638\n",
      "cnt: 1 - valLoss: 0.5711145401000977 - trainLoss: 0.5545802116394043\n",
      "cnt: 2 - valLoss: 0.5711162686347961 - trainLoss: 0.5545617341995239\n",
      "cnt: 3 - valLoss: 0.5711174011230469 - trainLoss: 0.5545443892478943\n",
      "cnt: 4 - valLoss: 0.5711167454719543 - trainLoss: 0.5545269250869751\n",
      "cnt: 5 - valLoss: 0.5710183382034302 - trainLoss: 0.5545105338096619\n",
      "cnt: 0 - valLoss: 0.5710029006004333 - trainLoss: 0.5544939637184143\n",
      "cnt: 0 - valLoss: 0.5709878206253052 - trainLoss: 0.5544762015342712\n",
      "cnt: 0 - valLoss: 0.57097327709198 - trainLoss: 0.554458498954773\n",
      "cnt: 0 - valLoss: 0.5709590315818787 - trainLoss: 0.5544408559799194\n",
      "cnt: 0 - valLoss: 0.570945143699646 - trainLoss: 0.5544231534004211\n",
      "cnt: 0 - valLoss: 0.570931613445282 - trainLoss: 0.5544055104255676\n",
      "cnt: 0 - valLoss: 0.5709183812141418 - trainLoss: 0.5543878674507141\n",
      "cnt: 0 - valLoss: 0.5709054470062256 - trainLoss: 0.5543702244758606\n",
      "cnt: 0 - valLoss: 0.5708927512168884 - trainLoss: 0.5543525218963623\n",
      "cnt: 0 - valLoss: 0.5708804130554199 - trainLoss: 0.5543348789215088\n",
      "cnt: 0 - valLoss: 0.5708681344985962 - trainLoss: 0.5543171763420105\n",
      "cnt: 0 - valLoss: 0.5708562731742859 - trainLoss: 0.554299533367157\n",
      "cnt: 0 - valLoss: 0.5708444714546204 - trainLoss: 0.5542818903923035\n",
      "cnt: 0 - valLoss: 0.5708329081535339 - trainLoss: 0.5542643070220947\n",
      "cnt: 0 - valLoss: 0.5708215236663818 - trainLoss: 0.5542466640472412\n",
      "cnt: 0 - valLoss: 0.5708103179931641 - trainLoss: 0.5542289614677429\n",
      "cnt: 0 - valLoss: 0.5707992315292358 - trainLoss: 0.5542113184928894\n",
      "cnt: 0 - valLoss: 0.5707883238792419 - trainLoss: 0.5541936755180359\n",
      "cnt: 0 - valLoss: 0.5707775354385376 - trainLoss: 0.5541760325431824\n",
      "cnt: 0 - valLoss: 0.5707669258117676 - trainLoss: 0.5541583895683289\n",
      "cnt: 0 - valLoss: 0.5707563757896423 - trainLoss: 0.5541407465934753\n",
      "cnt: 0 - valLoss: 0.5707460045814514 - trainLoss: 0.5541231036186218\n",
      "cnt: 0 - valLoss: 0.5707356333732605 - trainLoss: 0.5541054606437683\n",
      "cnt: 0 - valLoss: 0.5707254409790039 - trainLoss: 0.5540878176689148\n",
      "cnt: 0 - valLoss: 0.5707152485847473 - trainLoss: 0.5540701746940613\n",
      "cnt: 0 - valLoss: 0.5707051753997803 - trainLoss: 0.554052472114563\n",
      "cnt: 0 - valLoss: 0.5706952810287476 - trainLoss: 0.5540348887443542\n",
      "cnt: 0 - valLoss: 0.5706853866577148 - trainLoss: 0.5540172457695007\n",
      "cnt: 0 - valLoss: 0.5706754922866821 - trainLoss: 0.5539996027946472\n",
      "cnt: 0 - valLoss: 0.5706657767295837 - trainLoss: 0.5539819002151489\n",
      "cnt: 0 - valLoss: 0.5706560611724854 - trainLoss: 0.5539642572402954\n",
      "cnt: 0 - valLoss: 0.5706464648246765 - trainLoss: 0.5539466142654419\n",
      "cnt: 0 - valLoss: 0.5706368088722229 - trainLoss: 0.5539289712905884\n",
      "cnt: 0 - valLoss: 0.5706272721290588 - trainLoss: 0.5539112687110901\n",
      "cnt: 0 - valLoss: 0.5706177353858948 - trainLoss: 0.5538936257362366\n",
      "cnt: 0 - valLoss: 0.5706083178520203 - trainLoss: 0.5538759827613831\n",
      "cnt: 0 - valLoss: 0.5705989003181458 - trainLoss: 0.5538583397865295\n",
      "cnt: 0 - valLoss: 0.570589542388916 - trainLoss: 0.553840696811676\n",
      "cnt: 0 - valLoss: 0.570580244064331 - trainLoss: 0.5538230538368225\n",
      "cnt: 0 - valLoss: 0.5705708861351013 - trainLoss: 0.5538052916526794\n",
      "cnt: 0 - valLoss: 0.5705616474151611 - trainLoss: 0.5537876486778259\n",
      "cnt: 0 - valLoss: 0.570552408695221 - trainLoss: 0.5537700653076172\n",
      "cnt: 0 - valLoss: 0.5705310702323914 - trainLoss: 0.5537523627281189\n",
      "cnt: 0 - valLoss: 0.5705227255821228 - trainLoss: 0.5537348389625549\n",
      "cnt: 0 - valLoss: 0.5705143213272095 - trainLoss: 0.5537170767784119\n",
      "cnt: 0 - valLoss: 0.5705059170722961 - trainLoss: 0.5536994934082031\n",
      "cnt: 0 - valLoss: 0.5704853534698486 - trainLoss: 0.5536818504333496\n",
      "cnt: 0 - valLoss: 0.5704776644706726 - trainLoss: 0.5536642074584961\n",
      "cnt: 0 - valLoss: 0.5704699158668518 - trainLoss: 0.5536465644836426\n",
      "cnt: 0 - valLoss: 0.5704500079154968 - trainLoss: 0.5536289811134338\n",
      "cnt: 0 - valLoss: 0.5704429745674133 - trainLoss: 0.5536112785339355\n",
      "cnt: 0 - valLoss: 0.5704237818717957 - trainLoss: 0.553593635559082\n",
      "cnt: 0 - valLoss: 0.5704175233840942 - trainLoss: 0.5535761117935181\n",
      "cnt: 0 - valLoss: 0.5704111456871033 - trainLoss: 0.5535584092140198\n",
      "cnt: 0 - valLoss: 0.5703924894332886 - trainLoss: 0.553540825843811\n",
      "cnt: 0 - valLoss: 0.5703865885734558 - trainLoss: 0.5535231828689575\n",
      "cnt: 0 - valLoss: 0.5703684687614441 - trainLoss: 0.5535056591033936\n",
      "cnt: 0 - valLoss: 0.5703630447387695 - trainLoss: 0.5534879565238953\n",
      "cnt: 0 - valLoss: 0.5703453421592712 - trainLoss: 0.5534703731536865\n",
      "cnt: 0 - valLoss: 0.5703417062759399 - trainLoss: 0.553452730178833\n",
      "cnt: 0 - valLoss: 0.5703242421150208 - trainLoss: 0.5534352660179138\n",
      "cnt: 0 - valLoss: 0.5703073740005493 - trainLoss: 0.5534175634384155\n",
      "cnt: 0 - valLoss: 0.5703045129776001 - trainLoss: 0.5534000396728516\n",
      "cnt: 0 - valLoss: 0.570287823677063 - trainLoss: 0.5533824563026428\n",
      "cnt: 0 - valLoss: 0.5702731013298035 - trainLoss: 0.5533648729324341\n",
      "cnt: 0 - valLoss: 0.5702587366104126 - trainLoss: 0.5533472299575806\n",
      "cnt: 0 - valLoss: 0.5702568888664246 - trainLoss: 0.5533296465873718\n",
      "cnt: 0 - valLoss: 0.5702410340309143 - trainLoss: 0.5533121228218079\n",
      "cnt: 0 - valLoss: 0.5702270865440369 - trainLoss: 0.5532944798469543\n",
      "cnt: 0 - valLoss: 0.5702120065689087 - trainLoss: 0.5532769560813904\n",
      "cnt: 0 - valLoss: 0.5702109336853027 - trainLoss: 0.5532593727111816\n",
      "cnt: 0 - valLoss: 0.570195734500885 - trainLoss: 0.5532418489456177\n",
      "cnt: 0 - valLoss: 0.5701824426651001 - trainLoss: 0.5532242655754089\n",
      "cnt: 0 - valLoss: 0.5701695084571838 - trainLoss: 0.5532066226005554\n",
      "cnt: 0 - valLoss: 0.5701553821563721 - trainLoss: 0.5531890988349915\n",
      "cnt: 0 - valLoss: 0.5701430439949036 - trainLoss: 0.5531715750694275\n",
      "cnt: 0 - valLoss: 0.5701310038566589 - trainLoss: 0.553153932094574\n",
      "cnt: 0 - valLoss: 0.570117712020874 - trainLoss: 0.5531364679336548\n",
      "cnt: 0 - valLoss: 0.5701062083244324 - trainLoss: 0.553118884563446\n",
      "cnt: 0 - valLoss: 0.570094883441925 - trainLoss: 0.5531013011932373\n",
      "cnt: 0 - valLoss: 0.5700822472572327 - trainLoss: 0.5530837774276733\n",
      "cnt: 0 - valLoss: 0.5700713396072388 - trainLoss: 0.5530661940574646\n",
      "cnt: 0 - valLoss: 0.5700591802597046 - trainLoss: 0.5530486702919006\n",
      "cnt: 0 - valLoss: 0.5700486302375793 - trainLoss: 0.5530311465263367\n",
      "cnt: 0 - valLoss: 0.5700382590293884 - trainLoss: 0.5530136227607727\n",
      "cnt: 0 - valLoss: 0.5700264573097229 - trainLoss: 0.552996039390564\n",
      "cnt: 0 - valLoss: 0.5700163841247559 - trainLoss: 0.5529784560203552\n",
      "cnt: 0 - valLoss: 0.570004940032959 - trainLoss: 0.552960991859436\n",
      "cnt: 0 - valLoss: 0.5699951648712158 - trainLoss: 0.5529434084892273\n",
      "cnt: 0 - valLoss: 0.5699838995933533 - trainLoss: 0.5529258847236633\n",
      "cnt: 0 - valLoss: 0.5699742436408997 - trainLoss: 0.5529083013534546\n",
      "cnt: 0 - valLoss: 0.5699631571769714 - trainLoss: 0.5528908371925354\n",
      "cnt: 0 - valLoss: 0.5699522495269775 - trainLoss: 0.5528731942176819\n",
      "cnt: 0 - valLoss: 0.5699429512023926 - trainLoss: 0.5528556704521179\n",
      "cnt: 0 - valLoss: 0.569932222366333 - trainLoss: 0.552838146686554\n",
      "cnt: 0 - valLoss: 0.5699231028556824 - trainLoss: 0.5528205633163452\n",
      "cnt: 0 - valLoss: 0.569912314414978 - trainLoss: 0.552803099155426\n",
      "cnt: 0 - valLoss: 0.5699031352996826 - trainLoss: 0.5527855753898621\n",
      "cnt: 0 - valLoss: 0.5698925256729126 - trainLoss: 0.5527679920196533\n",
      "cnt: 0 - valLoss: 0.5698835253715515 - trainLoss: 0.5527504682540894\n",
      "cnt: 0 - valLoss: 0.5698729753494263 - trainLoss: 0.5527329444885254\n",
      "cnt: 0 - valLoss: 0.5698626041412354 - trainLoss: 0.5527153611183167\n",
      "cnt: 0 - valLoss: 0.5698539018630981 - trainLoss: 0.5526978969573975\n",
      "cnt: 0 - valLoss: 0.569843590259552 - trainLoss: 0.5526803731918335\n",
      "cnt: 0 - valLoss: 0.5698349475860596 - trainLoss: 0.5526627898216248\n",
      "cnt: 0 - valLoss: 0.5698248744010925 - trainLoss: 0.5526452660560608\n",
      "cnt: 0 - valLoss: 0.5698163509368896 - trainLoss: 0.5526277422904968\n",
      "cnt: 0 - valLoss: 0.5698062777519226 - trainLoss: 0.5526102185249329\n",
      "cnt: 0 - valLoss: 0.5697962641716003 - trainLoss: 0.5525926351547241\n",
      "cnt: 0 - valLoss: 0.5697879195213318 - trainLoss: 0.5525751113891602\n",
      "cnt: 0 - valLoss: 0.5697780251502991 - trainLoss: 0.5525575876235962\n",
      "cnt: 0 - valLoss: 0.5697681307792664 - trainLoss: 0.5525400042533875\n",
      "cnt: 0 - valLoss: 0.5697599053382874 - trainLoss: 0.5525224804878235\n",
      "cnt: 0 - valLoss: 0.5697502493858337 - trainLoss: 0.5525049567222595\n",
      "cnt: 0 - valLoss: 0.5697420835494995 - trainLoss: 0.5524874329566956\n",
      "cnt: 0 - valLoss: 0.5697324275970459 - trainLoss: 0.5524698495864868\n",
      "cnt: 0 - valLoss: 0.5697228312492371 - trainLoss: 0.5524523258209229\n",
      "cnt: 0 - valLoss: 0.5697147250175476 - trainLoss: 0.5524347424507141\n",
      "cnt: 0 - valLoss: 0.5697051286697388 - trainLoss: 0.5524172186851501\n",
      "cnt: 0 - valLoss: 0.5696955919265747 - trainLoss: 0.5523996949195862\n",
      "cnt: 0 - valLoss: 0.5696876049041748 - trainLoss: 0.5523821115493774\n",
      "cnt: 0 - valLoss: 0.5696781277656555 - trainLoss: 0.5523645877838135\n",
      "cnt: 0 - valLoss: 0.5696701407432556 - trainLoss: 0.5523470640182495\n",
      "cnt: 0 - valLoss: 0.5696606636047363 - trainLoss: 0.5523294806480408\n",
      "cnt: 0 - valLoss: 0.5696512460708618 - trainLoss: 0.552311897277832\n",
      "cnt: 0 - valLoss: 0.5696433186531067 - trainLoss: 0.5522944331169128\n",
      "cnt: 0 - valLoss: 0.5696338415145874 - trainLoss: 0.5522768497467041\n",
      "cnt: 0 - valLoss: 0.5696244239807129 - trainLoss: 0.5522593259811401\n",
      "cnt: 0 - valLoss: 0.5696166157722473 - trainLoss: 0.5522417426109314\n",
      "cnt: 0 - valLoss: 0.5696072578430176 - trainLoss: 0.5522241592407227\n",
      "cnt: 0 - valLoss: 0.5695993900299072 - trainLoss: 0.5522065758705139\n",
      "cnt: 0 - valLoss: 0.5695899724960327 - trainLoss: 0.5521889925003052\n",
      "cnt: 0 - valLoss: 0.569580614566803 - trainLoss: 0.5521714687347412\n",
      "cnt: 0 - valLoss: 0.5695728659629822 - trainLoss: 0.5521538853645325\n",
      "cnt: 0 - valLoss: 0.5695634484291077 - trainLoss: 0.5521364212036133\n",
      "cnt: 0 - valLoss: 0.5695542097091675 - trainLoss: 0.5521187782287598\n",
      "cnt: 0 - valLoss: 0.5695464015007019 - trainLoss: 0.5521012544631958\n",
      "cnt: 0 - valLoss: 0.5695371031761169 - trainLoss: 0.5520837306976318\n",
      "cnt: 0 - valLoss: 0.569527804851532 - trainLoss: 0.5520661473274231\n",
      "cnt: 0 - valLoss: 0.5695200562477112 - trainLoss: 0.5520486235618591\n",
      "cnt: 0 - valLoss: 0.569510817527771 - trainLoss: 0.5520310401916504\n",
      "cnt: 0 - valLoss: 0.569503128528595 - trainLoss: 0.5520134568214417\n",
      "cnt: 0 - valLoss: 0.5694937705993652 - trainLoss: 0.5519958734512329\n",
      "cnt: 0 - valLoss: 0.5694844722747803 - trainLoss: 0.551978349685669\n",
      "cnt: 0 - valLoss: 0.569476842880249 - trainLoss: 0.5519607663154602\n",
      "cnt: 0 - valLoss: 0.5694675445556641 - trainLoss: 0.5519432425498962\n",
      "cnt: 0 - valLoss: 0.5694597959518433 - trainLoss: 0.5519255995750427\n",
      "cnt: 0 - valLoss: 0.5694505572319031 - trainLoss: 0.5519080758094788\n",
      "cnt: 0 - valLoss: 0.5694413185119629 - trainLoss: 0.55189049243927\n",
      "cnt: 0 - valLoss: 0.5694336891174316 - trainLoss: 0.551872968673706\n",
      "cnt: 0 - valLoss: 0.5694244503974915 - trainLoss: 0.5518553256988525\n",
      "cnt: 0 - valLoss: 0.5694152116775513 - trainLoss: 0.5518377423286438\n",
      "cnt: 0 - valLoss: 0.5694077014923096 - trainLoss: 0.5518202185630798\n",
      "cnt: 0 - valLoss: 0.5693985223770142 - trainLoss: 0.5518026351928711\n",
      "cnt: 0 - valLoss: 0.5693908929824829 - trainLoss: 0.5517850518226624\n",
      "cnt: 0 - valLoss: 0.5693816542625427 - trainLoss: 0.5517674684524536\n",
      "cnt: 0 - valLoss: 0.5693724751472473 - trainLoss: 0.5517499446868896\n",
      "cnt: 0 - valLoss: 0.5693649053573608 - trainLoss: 0.5517323017120361\n",
      "cnt: 0 - valLoss: 0.5693557262420654 - trainLoss: 0.5517147779464722\n",
      "cnt: 0 - valLoss: 0.5693480968475342 - trainLoss: 0.5516971945762634\n",
      "cnt: 0 - valLoss: 0.569338858127594 - trainLoss: 0.5516796112060547\n",
      "cnt: 0 - valLoss: 0.5693297386169434 - trainLoss: 0.551662027835846\n",
      "cnt: 0 - valLoss: 0.5693221092224121 - trainLoss: 0.5516444444656372\n",
      "cnt: 0 - valLoss: 0.5693129897117615 - trainLoss: 0.5516269207000732\n",
      "cnt: 0 - valLoss: 0.5693053603172302 - trainLoss: 0.5516092777252197\n",
      "cnt: 0 - valLoss: 0.5692961812019348 - trainLoss: 0.5515917539596558\n",
      "cnt: 0 - valLoss: 0.5692870020866394 - trainLoss: 0.551574170589447\n",
      "cnt: 0 - valLoss: 0.5692793726921082 - trainLoss: 0.5515565872192383\n",
      "cnt: 0 - valLoss: 0.5692701935768127 - trainLoss: 0.5515390038490295\n",
      "cnt: 0 - valLoss: 0.5692625641822815 - trainLoss: 0.551521360874176\n",
      "cnt: 0 - valLoss: 0.5692533850669861 - trainLoss: 0.5515038371086121\n",
      "cnt: 0 - valLoss: 0.5692442059516907 - trainLoss: 0.5514862537384033\n",
      "cnt: 0 - valLoss: 0.569236695766449 - trainLoss: 0.5514686703681946\n",
      "cnt: 0 - valLoss: 0.5692275166511536 - trainLoss: 0.5514510869979858\n",
      "cnt: 0 - valLoss: 0.5692198872566223 - trainLoss: 0.5514335036277771\n",
      "cnt: 0 - valLoss: 0.5692105889320374 - trainLoss: 0.5514159202575684\n",
      "cnt: 0 - valLoss: 0.5692015290260315 - trainLoss: 0.5513983368873596\n",
      "cnt: 0 - valLoss: 0.569193959236145 - trainLoss: 0.5513807535171509\n",
      "cnt: 0 - valLoss: 0.5691847801208496 - trainLoss: 0.5513631105422974\n",
      "cnt: 0 - valLoss: 0.5691772699356079 - trainLoss: 0.5513455271720886\n",
      "cnt: 0 - valLoss: 0.5691680312156677 - trainLoss: 0.5513279438018799\n",
      "cnt: 0 - valLoss: 0.5691604614257812 - trainLoss: 0.5513104200363159\n",
      "cnt: 0 - valLoss: 0.5691512823104858 - trainLoss: 0.5512928366661072\n",
      "cnt: 0 - valLoss: 0.5691414475440979 - trainLoss: 0.5512752532958984\n",
      "cnt: 0 - valLoss: 0.5691334009170532 - trainLoss: 0.5512576103210449\n",
      "cnt: 0 - valLoss: 0.5691251754760742 - trainLoss: 0.5512401461601257\n",
      "cnt: 0 - valLoss: 0.5691154599189758 - trainLoss: 0.551222562789917\n",
      "cnt: 0 - valLoss: 0.5691074132919312 - trainLoss: 0.5512049794197083\n",
      "cnt: 0 - valLoss: 0.569098174571991 - trainLoss: 0.5511874556541443\n",
      "cnt: 0 - valLoss: 0.5690909624099731 - trainLoss: 0.5511698722839355\n",
      "cnt: 0 - valLoss: 0.5690818428993225 - trainLoss: 0.5511524081230164\n",
      "cnt: 0 - valLoss: 0.5690746903419495 - trainLoss: 0.5511348247528076\n",
      "cnt: 0 - valLoss: 0.5690656304359436 - trainLoss: 0.5511173009872437\n",
      "cnt: 0 - valLoss: 0.5690585374832153 - trainLoss: 0.5510997176170349\n",
      "cnt: 0 - valLoss: 0.5690494775772095 - trainLoss: 0.5510821342468262\n",
      "cnt: 0 - valLoss: 0.5690425038337708 - trainLoss: 0.5510646104812622\n",
      "cnt: 0 - valLoss: 0.5690335631370544 - trainLoss: 0.5510470271110535\n",
      "cnt: 0 - valLoss: 0.5690265893936157 - trainLoss: 0.5510294437408447\n",
      "cnt: 0 - valLoss: 0.5690194964408875 - trainLoss: 0.5510119795799255\n",
      "cnt: 0 - valLoss: 0.5690104961395264 - trainLoss: 0.5509943962097168\n",
      "cnt: 0 - valLoss: 0.5690035223960876 - trainLoss: 0.5509768724441528\n",
      "cnt: 0 - valLoss: 0.5689945816993713 - trainLoss: 0.5509592890739441\n",
      "cnt: 0 - valLoss: 0.5689876079559326 - trainLoss: 0.5509416460990906\n",
      "cnt: 0 - valLoss: 0.5689787864685059 - trainLoss: 0.5509241223335266\n",
      "cnt: 0 - valLoss: 0.5689718723297119 - trainLoss: 0.5509065985679626\n",
      "cnt: 0 - valLoss: 0.5689681768417358 - trainLoss: 0.5508889555931091\n",
      "cnt: 0 - valLoss: 0.5689641833305359 - trainLoss: 0.5508713722229004\n",
      "cnt: 0 - valLoss: 0.5689599514007568 - trainLoss: 0.5508537292480469\n",
      "cnt: 0 - valLoss: 0.5689555406570435 - trainLoss: 0.5508361458778381\n",
      "cnt: 0 - valLoss: 0.5689507722854614 - trainLoss: 0.5508185029029846\n",
      "cnt: 0 - valLoss: 0.5689458250999451 - trainLoss: 0.5508009195327759\n",
      "cnt: 0 - valLoss: 0.5689407587051392 - trainLoss: 0.5507832765579224\n",
      "cnt: 0 - valLoss: 0.5689354538917542 - trainLoss: 0.5507656931877136\n",
      "cnt: 0 - valLoss: 0.56892991065979 - trainLoss: 0.5507480502128601\n",
      "cnt: 0 - valLoss: 0.5689243078231812 - trainLoss: 0.5507304668426514\n",
      "cnt: 0 - valLoss: 0.5689185261726379 - trainLoss: 0.5507128238677979\n",
      "cnt: 0 - valLoss: 0.5689126253128052 - trainLoss: 0.5506952404975891\n",
      "cnt: 0 - valLoss: 0.5689066052436829 - trainLoss: 0.5506775975227356\n",
      "cnt: 0 - valLoss: 0.568900465965271 - trainLoss: 0.5506600141525269\n",
      "cnt: 0 - valLoss: 0.5688959956169128 - trainLoss: 0.5506424307823181\n",
      "cnt: 0 - valLoss: 0.5688842535018921 - trainLoss: 0.5506248474121094\n",
      "cnt: 0 - valLoss: 0.5688729286193848 - trainLoss: 0.5506073236465454\n",
      "cnt: 0 - valLoss: 0.5688619017601013 - trainLoss: 0.5505897998809814\n",
      "cnt: 0 - valLoss: 0.5688509941101074 - trainLoss: 0.5505722761154175\n",
      "cnt: 0 - valLoss: 0.5688403844833374 - trainLoss: 0.550554633140564\n",
      "cnt: 0 - valLoss: 0.5688300728797913 - trainLoss: 0.5505371689796448\n",
      "cnt: 0 - valLoss: 0.5688217878341675 - trainLoss: 0.5505196452140808\n",
      "cnt: 0 - valLoss: 0.5688117146492004 - trainLoss: 0.5505021214485168\n",
      "cnt: 0 - valLoss: 0.5688017010688782 - trainLoss: 0.5504845976829529\n",
      "cnt: 0 - valLoss: 0.5687920451164246 - trainLoss: 0.5504670143127441\n",
      "cnt: 0 - valLoss: 0.5687824487686157 - trainLoss: 0.550449550151825\n",
      "cnt: 0 - valLoss: 0.5687729716300964 - trainLoss: 0.550432026386261\n",
      "cnt: 0 - valLoss: 0.5687636733055115 - trainLoss: 0.550414502620697\n",
      "cnt: 0 - valLoss: 0.5687544941902161 - trainLoss: 0.5503969788551331\n",
      "cnt: 0 - valLoss: 0.5687453746795654 - trainLoss: 0.5503794550895691\n",
      "cnt: 0 - valLoss: 0.5687364935874939 - trainLoss: 0.5503619909286499\n",
      "cnt: 0 - valLoss: 0.5687276124954224 - trainLoss: 0.5503444075584412\n",
      "cnt: 0 - valLoss: 0.5687189102172852 - trainLoss: 0.5503268837928772\n",
      "cnt: 0 - valLoss: 0.5687102675437927 - trainLoss: 0.550309419631958\n",
      "cnt: 0 - valLoss: 0.5687016844749451 - trainLoss: 0.5502918362617493\n",
      "cnt: 0 - valLoss: 0.5686919689178467 - trainLoss: 0.5502743721008301\n",
      "cnt: 0 - valLoss: 0.5686824917793274 - trainLoss: 0.5502568483352661\n",
      "cnt: 0 - valLoss: 0.5686731338500977 - trainLoss: 0.5502393245697021\n",
      "cnt: 0 - valLoss: 0.5686639547348022 - trainLoss: 0.5502218008041382\n",
      "cnt: 0 - valLoss: 0.5686548352241516 - trainLoss: 0.550204336643219\n",
      "cnt: 0 - valLoss: 0.5686459541320801 - trainLoss: 0.5501867532730103\n",
      "cnt: 0 - valLoss: 0.5686371326446533 - trainLoss: 0.5501692891120911\n",
      "cnt: 0 - valLoss: 0.5686283707618713 - trainLoss: 0.5501517653465271\n",
      "cnt: 0 - valLoss: 0.5686197876930237 - trainLoss: 0.5501342415809631\n",
      "cnt: 0 - valLoss: 0.568611204624176 - trainLoss: 0.550116777420044\n",
      "cnt: 0 - valLoss: 0.5686028599739075 - trainLoss: 0.55009925365448\n",
      "cnt: 0 - valLoss: 0.5685944557189941 - trainLoss: 0.5500817894935608\n",
      "cnt: 0 - valLoss: 0.5685859322547913 - trainLoss: 0.5500642657279968\n",
      "cnt: 0 - valLoss: 0.5685776472091675 - trainLoss: 0.5500468015670776\n",
      "cnt: 0 - valLoss: 0.5685693025588989 - trainLoss: 0.5500292778015137\n",
      "cnt: 0 - valLoss: 0.5685610771179199 - trainLoss: 0.5500117540359497\n",
      "cnt: 0 - valLoss: 0.5685529112815857 - trainLoss: 0.5499942898750305\n",
      "cnt: 0 - valLoss: 0.5685449242591858 - trainLoss: 0.5499767661094666\n",
      "cnt: 0 - valLoss: 0.5685368776321411 - trainLoss: 0.5499593019485474\n",
      "cnt: 0 - valLoss: 0.5685288906097412 - trainLoss: 0.5499417781829834\n",
      "cnt: 0 - valLoss: 0.5685209631919861 - trainLoss: 0.5499242544174194\n",
      "cnt: 0 - valLoss: 0.5685130953788757 - trainLoss: 0.5499067902565002\n",
      "cnt: 0 - valLoss: 0.5685052871704102 - trainLoss: 0.5498892664909363\n",
      "cnt: 0 - valLoss: 0.5684974789619446 - trainLoss: 0.5498717427253723\n",
      "cnt: 0 - valLoss: 0.5684897899627686 - trainLoss: 0.5498542785644531\n",
      "cnt: 0 - valLoss: 0.568481981754303 - trainLoss: 0.5498368144035339\n",
      "cnt: 0 - valLoss: 0.5684743523597717 - trainLoss: 0.5498192310333252\n",
      "cnt: 0 - valLoss: 0.5684666633605957 - trainLoss: 0.549801766872406\n",
      "cnt: 0 - valLoss: 0.5684590935707092 - trainLoss: 0.5497843027114868\n",
      "cnt: 0 - valLoss: 0.5684497952461243 - trainLoss: 0.5497667789459229\n",
      "cnt: 0 - valLoss: 0.5684407949447632 - trainLoss: 0.5497493147850037\n",
      "cnt: 0 - valLoss: 0.568431556224823 - trainLoss: 0.5497317314147949\n",
      "cnt: 0 - valLoss: 0.5684224963188171 - trainLoss: 0.5497142672538757\n",
      "cnt: 0 - valLoss: 0.5684136152267456 - trainLoss: 0.5496968030929565\n",
      "cnt: 0 - valLoss: 0.5684047937393188 - trainLoss: 0.5496793389320374\n",
      "cnt: 0 - valLoss: 0.5683961510658264 - trainLoss: 0.5496618151664734\n",
      "cnt: 0 - valLoss: 0.5683875679969788 - trainLoss: 0.5496443510055542\n",
      "cnt: 0 - valLoss: 0.5683791637420654 - trainLoss: 0.549626886844635\n",
      "cnt: 0 - valLoss: 0.5683708190917969 - trainLoss: 0.549609363079071\n",
      "cnt: 0 - valLoss: 0.5683625340461731 - trainLoss: 0.5495918989181519\n",
      "cnt: 0 - valLoss: 0.5683544278144836 - trainLoss: 0.5495744347572327\n",
      "cnt: 0 - valLoss: 0.5683462023735046 - trainLoss: 0.5495569705963135\n",
      "cnt: 0 - valLoss: 0.5683382153511047 - trainLoss: 0.5495395064353943\n",
      "cnt: 0 - valLoss: 0.5683302283287048 - trainLoss: 0.5495220422744751\n",
      "cnt: 0 - valLoss: 0.5683222413063049 - trainLoss: 0.5495044589042664\n",
      "cnt: 0 - valLoss: 0.5683143734931946 - trainLoss: 0.5494869947433472\n",
      "cnt: 0 - valLoss: 0.5683066844940186 - trainLoss: 0.549469530582428\n",
      "cnt: 0 - valLoss: 0.5682988166809082 - trainLoss: 0.549452006816864\n",
      "cnt: 0 - valLoss: 0.5682911276817322 - trainLoss: 0.5494345426559448\n",
      "cnt: 0 - valLoss: 0.5682834386825562 - trainLoss: 0.5494170784950256\n",
      "cnt: 0 - valLoss: 0.5682758092880249 - trainLoss: 0.5493996143341064\n",
      "cnt: 0 - valLoss: 0.5682681798934937 - trainLoss: 0.5493821501731873\n",
      "cnt: 0 - valLoss: 0.568260669708252 - trainLoss: 0.5493646860122681\n",
      "cnt: 0 - valLoss: 0.5682530999183655 - trainLoss: 0.5493471622467041\n",
      "cnt: 0 - valLoss: 0.5682455897331238 - trainLoss: 0.5493296980857849\n",
      "cnt: 0 - valLoss: 0.5682381391525269 - trainLoss: 0.5493122339248657\n",
      "cnt: 0 - valLoss: 0.5682306885719299 - trainLoss: 0.5492947697639465\n",
      "cnt: 0 - valLoss: 0.5682231783866882 - trainLoss: 0.5492771863937378\n",
      "cnt: 0 - valLoss: 0.5682157874107361 - trainLoss: 0.5492597222328186\n",
      "cnt: 0 - valLoss: 0.5682083964347839 - trainLoss: 0.5492422580718994\n",
      "cnt: 0 - valLoss: 0.5682010650634766 - trainLoss: 0.5492247939109802\n",
      "cnt: 0 - valLoss: 0.5681937336921692 - trainLoss: 0.5492072701454163\n",
      "cnt: 0 - valLoss: 0.5681862831115723 - trainLoss: 0.5491898059844971\n",
      "cnt: 0 - valLoss: 0.5681790709495544 - trainLoss: 0.5491722822189331\n",
      "cnt: 0 - valLoss: 0.5681717395782471 - trainLoss: 0.5491548180580139\n",
      "cnt: 0 - valLoss: 0.5681644678115845 - trainLoss: 0.5491373538970947\n",
      "cnt: 0 - valLoss: 0.5681571960449219 - trainLoss: 0.5491198897361755\n",
      "cnt: 0 - valLoss: 0.5681499242782593 - trainLoss: 0.5491024255752563\n",
      "cnt: 0 - valLoss: 0.5681426525115967 - trainLoss: 0.5490849018096924\n",
      "cnt: 0 - valLoss: 0.5681354403495789 - trainLoss: 0.5490673780441284\n",
      "cnt: 0 - valLoss: 0.5681281685829163 - trainLoss: 0.5490499138832092\n",
      "cnt: 0 - valLoss: 0.5681209564208984 - trainLoss: 0.5490323901176453\n",
      "cnt: 0 - valLoss: 0.5681138038635254 - trainLoss: 0.5490149259567261\n",
      "cnt: 0 - valLoss: 0.5681065917015076 - trainLoss: 0.5489974617958069\n",
      "cnt: 0 - valLoss: 0.5680995583534241 - trainLoss: 0.5489799976348877\n",
      "cnt: 0 - valLoss: 0.5680925846099854 - trainLoss: 0.548962414264679\n",
      "cnt: 0 - valLoss: 0.5680855512619019 - trainLoss: 0.5489449501037598\n",
      "cnt: 0 - valLoss: 0.5680785179138184 - trainLoss: 0.5489275455474854\n",
      "cnt: 0 - valLoss: 0.5680715441703796 - trainLoss: 0.5489100217819214\n",
      "cnt: 0 - valLoss: 0.5680645108222961 - trainLoss: 0.5488924980163574\n",
      "cnt: 0 - valLoss: 0.5680575370788574 - trainLoss: 0.5488750338554382\n",
      "cnt: 0 - valLoss: 0.5680505037307739 - trainLoss: 0.5488575100898743\n",
      "cnt: 0 - valLoss: 0.5680435299873352 - trainLoss: 0.5488400459289551\n",
      "cnt: 0 - valLoss: 0.5680364370346069 - trainLoss: 0.5488225817680359\n",
      "cnt: 0 - valLoss: 0.5680294036865234 - trainLoss: 0.5488051176071167\n",
      "cnt: 0 - valLoss: 0.5680223703384399 - trainLoss: 0.5487876534461975\n",
      "cnt: 0 - valLoss: 0.5680153369903564 - trainLoss: 0.5487700700759888\n",
      "cnt: 0 - valLoss: 0.5680083632469177 - trainLoss: 0.5487526059150696\n",
      "cnt: 0 - valLoss: 0.5680013298988342 - trainLoss: 0.5487350821495056\n",
      "cnt: 0 - valLoss: 0.5679942965507507 - trainLoss: 0.5487176179885864\n",
      "cnt: 0 - valLoss: 0.5679872632026672 - trainLoss: 0.5487001538276672\n",
      "cnt: 0 - valLoss: 0.5679802298545837 - trainLoss: 0.548682689666748\n",
      "cnt: 0 - valLoss: 0.567973256111145 - trainLoss: 0.5486651659011841\n",
      "cnt: 0 - valLoss: 0.5679662227630615 - trainLoss: 0.5486477017402649\n",
      "cnt: 0 - valLoss: 0.567959189414978 - trainLoss: 0.5486302375793457\n",
      "cnt: 0 - valLoss: 0.5679522156715393 - trainLoss: 0.5486127138137817\n",
      "cnt: 0 - valLoss: 0.5679451823234558 - trainLoss: 0.5485951900482178\n",
      "cnt: 0 - valLoss: 0.5679381489753723 - trainLoss: 0.5485776662826538\n",
      "cnt: 0 - valLoss: 0.5679311156272888 - trainLoss: 0.5485602021217346\n",
      "cnt: 0 - valLoss: 0.5679241418838501 - trainLoss: 0.5485427379608154\n",
      "cnt: 0 - valLoss: 0.5679171085357666 - trainLoss: 0.5485252737998962\n",
      "cnt: 0 - valLoss: 0.5679100751876831 - trainLoss: 0.5485076904296875\n",
      "cnt: 0 - valLoss: 0.5679030418395996 - trainLoss: 0.5484902262687683\n",
      "cnt: 0 - valLoss: 0.5678960680961609 - trainLoss: 0.5484727621078491\n",
      "cnt: 0 - valLoss: 0.5678890943527222 - trainLoss: 0.5484552383422852\n",
      "cnt: 0 - valLoss: 0.5678821206092834 - trainLoss: 0.548437774181366\n",
      "cnt: 0 - valLoss: 0.5678751468658447 - trainLoss: 0.5484203100204468\n",
      "cnt: 0 - valLoss: 0.5678688287734985 - trainLoss: 0.5484027862548828\n",
      "cnt: 0 - valLoss: 0.5678617358207703 - trainLoss: 0.5483853816986084\n",
      "cnt: 0 - valLoss: 0.5678554177284241 - trainLoss: 0.5483678579330444\n",
      "cnt: 0 - valLoss: 0.5678490400314331 - trainLoss: 0.5483503937721252\n",
      "cnt: 0 - valLoss: 0.5678426027297974 - trainLoss: 0.548332929611206\n",
      "cnt: 0 - valLoss: 0.5678361058235168 - trainLoss: 0.5483154058456421\n",
      "cnt: 0 - valLoss: 0.5678296089172363 - trainLoss: 0.5482980012893677\n",
      "cnt: 0 - valLoss: 0.5678223967552185 - trainLoss: 0.5482804179191589\n",
      "cnt: 0 - valLoss: 0.5678159594535828 - trainLoss: 0.5482629537582397\n",
      "cnt: 0 - valLoss: 0.5678093433380127 - trainLoss: 0.5482454895973206\n",
      "cnt: 0 - valLoss: 0.5678028464317322 - trainLoss: 0.5482280254364014\n",
      "cnt: 0 - valLoss: 0.5677962303161621 - trainLoss: 0.5482105612754822\n",
      "cnt: 0 - valLoss: 0.5677896738052368 - trainLoss: 0.5481930375099182\n",
      "cnt: 0 - valLoss: 0.5677830576896667 - trainLoss: 0.548175573348999\n",
      "cnt: 0 - valLoss: 0.5677757263183594 - trainLoss: 0.5481581091880798\n",
      "cnt: 0 - valLoss: 0.5677691698074341 - trainLoss: 0.5481405854225159\n",
      "cnt: 0 - valLoss: 0.567762553691864 - trainLoss: 0.5481231212615967\n",
      "cnt: 0 - valLoss: 0.567755937576294 - trainLoss: 0.5481057167053223\n",
      "cnt: 0 - valLoss: 0.5677478909492493 - trainLoss: 0.5480882525444031\n",
      "cnt: 0 - valLoss: 0.5677419304847717 - trainLoss: 0.5480707883834839\n",
      "cnt: 0 - valLoss: 0.5677339434623718 - trainLoss: 0.5480533242225647\n",
      "cnt: 0 - valLoss: 0.5677259564399719 - trainLoss: 0.5480359792709351\n",
      "cnt: 0 - valLoss: 0.5677194595336914 - trainLoss: 0.5480185747146606\n",
      "cnt: 0 - valLoss: 0.567711591720581 - trainLoss: 0.5480011105537415\n",
      "cnt: 0 - valLoss: 0.5677037835121155 - trainLoss: 0.547983705997467\n",
      "cnt: 0 - valLoss: 0.5676981210708618 - trainLoss: 0.5479663014411926\n",
      "cnt: 0 - valLoss: 0.5676903128623962 - trainLoss: 0.5479488968849182\n",
      "cnt: 0 - valLoss: 0.5676845908164978 - trainLoss: 0.547931432723999\n",
      "cnt: 0 - valLoss: 0.5676767826080322 - trainLoss: 0.5479140281677246\n",
      "cnt: 0 - valLoss: 0.5676690936088562 - trainLoss: 0.5478966236114502\n",
      "cnt: 0 - valLoss: 0.5676634907722473 - trainLoss: 0.5478792786598206\n",
      "cnt: 0 - valLoss: 0.5676557421684265 - trainLoss: 0.5478618144989014\n",
      "cnt: 0 - valLoss: 0.5676481127738953 - trainLoss: 0.547844409942627\n",
      "cnt: 0 - valLoss: 0.5676418542861938 - trainLoss: 0.5478270053863525\n",
      "cnt: 0 - valLoss: 0.5676342248916626 - trainLoss: 0.5478096008300781\n",
      "cnt: 0 - valLoss: 0.5676286816596985 - trainLoss: 0.5477921366691589\n",
      "cnt: 0 - valLoss: 0.5676210522651672 - trainLoss: 0.5477747917175293\n",
      "cnt: 0 - valLoss: 0.5676135420799255 - trainLoss: 0.5477573275566101\n",
      "cnt: 0 - valLoss: 0.5676080584526062 - trainLoss: 0.5477399230003357\n",
      "cnt: 0 - valLoss: 0.5676004886627197 - trainLoss: 0.5477225184440613\n",
      "cnt: 0 - valLoss: 0.5675929188728333 - trainLoss: 0.5477051138877869\n",
      "cnt: 0 - valLoss: 0.5675874352455139 - trainLoss: 0.5476876497268677\n",
      "cnt: 0 - valLoss: 0.5675798654556274 - trainLoss: 0.5476702451705933\n",
      "cnt: 0 - valLoss: 0.5675737857818604 - trainLoss: 0.5476529002189636\n",
      "cnt: 0 - valLoss: 0.5675662159919739 - trainLoss: 0.5476354360580444\n",
      "cnt: 0 - valLoss: 0.5675587058067322 - trainLoss: 0.54761803150177\n",
      "cnt: 0 - valLoss: 0.5675534009933472 - trainLoss: 0.5476006865501404\n",
      "cnt: 0 - valLoss: 0.5675458312034607 - trainLoss: 0.547583281993866\n",
      "cnt: 0 - valLoss: 0.567538321018219 - trainLoss: 0.5475658178329468\n",
      "cnt: 0 - valLoss: 0.5675328373908997 - trainLoss: 0.5475484132766724\n",
      "cnt: 0 - valLoss: 0.5675250887870789 - trainLoss: 0.5475309491157532\n",
      "cnt: 0 - valLoss: 0.5675188302993774 - trainLoss: 0.5475136041641235\n",
      "cnt: 0 - valLoss: 0.5675112009048462 - trainLoss: 0.5474961996078491\n",
      "cnt: 0 - valLoss: 0.5675036907196045 - trainLoss: 0.5474787354469299\n",
      "cnt: 0 - valLoss: 0.5674982070922852 - trainLoss: 0.5474613904953003\n",
      "cnt: 0 - valLoss: 0.5674906373023987 - trainLoss: 0.5474439859390259\n",
      "cnt: 0 - valLoss: 0.5674851536750793 - trainLoss: 0.5474265813827515\n",
      "cnt: 0 - valLoss: 0.5674272775650024 - trainLoss: 0.5474095940589905\n",
      "cnt: 0 - valLoss: 0.5674259066581726 - trainLoss: 0.5473925471305847\n",
      "cnt: 0 - valLoss: 0.5674239993095398 - trainLoss: 0.5473750829696655\n",
      "cnt: 0 - valLoss: 0.5674217343330383 - trainLoss: 0.5473576784133911\n",
      "cnt: 0 - valLoss: 0.5673689246177673 - trainLoss: 0.547340452671051\n",
      "cnt: 0 - valLoss: 0.5673699378967285 - trainLoss: 0.5473237037658691\n",
      "cnt: 1 - valLoss: 0.5673704147338867 - trainLoss: 0.54730623960495\n",
      "cnt: 2 - valLoss: 0.5673702359199524 - trainLoss: 0.5472888350486755\n",
      "cnt: 3 - valLoss: 0.567319393157959 - trainLoss: 0.5472722053527832\n",
      "cnt: 0 - valLoss: 0.5672787427902222 - trainLoss: 0.547253429889679\n",
      "cnt: 0 - valLoss: 0.5672409534454346 - trainLoss: 0.547234296798706\n",
      "cnt: 0 - valLoss: 0.5672057867050171 - trainLoss: 0.547215461730957\n",
      "cnt: 0 - valLoss: 0.5671730041503906 - trainLoss: 0.5471968054771423\n",
      "cnt: 0 - valLoss: 0.5671424269676208 - trainLoss: 0.5471782684326172\n",
      "cnt: 0 - valLoss: 0.5671137571334839 - trainLoss: 0.5471599102020264\n",
      "cnt: 0 - valLoss: 0.5670886039733887 - trainLoss: 0.5471416711807251\n",
      "cnt: 0 - valLoss: 0.5670649409294128 - trainLoss: 0.5471234917640686\n",
      "cnt: 0 - valLoss: 0.5670427083969116 - trainLoss: 0.5471055507659912\n",
      "cnt: 0 - valLoss: 0.5670217275619507 - trainLoss: 0.5470874905586243\n",
      "cnt: 0 - valLoss: 0.5670019388198853 - trainLoss: 0.5470695495605469\n",
      "cnt: 0 - valLoss: 0.5669832229614258 - trainLoss: 0.5470516681671143\n",
      "cnt: 0 - valLoss: 0.5669654607772827 - trainLoss: 0.5470337867736816\n",
      "cnt: 0 - valLoss: 0.5669485926628113 - trainLoss: 0.5470159649848938\n",
      "cnt: 0 - valLoss: 0.5669325590133667 - trainLoss: 0.546998143196106\n",
      "cnt: 0 - valLoss: 0.5669172406196594 - trainLoss: 0.5469803214073181\n",
      "cnt: 0 - valLoss: 0.5669026970863342 - trainLoss: 0.546962559223175\n",
      "cnt: 0 - valLoss: 0.566888689994812 - trainLoss: 0.546944797039032\n",
      "cnt: 0 - valLoss: 0.5668753385543823 - trainLoss: 0.5469270944595337\n",
      "cnt: 0 - valLoss: 0.5668624639511108 - trainLoss: 0.5469093322753906\n",
      "cnt: 0 - valLoss: 0.5668439865112305 - trainLoss: 0.5468916296958923\n",
      "cnt: 0 - valLoss: 0.566826581954956 - trainLoss: 0.5468738675117493\n",
      "cnt: 0 - valLoss: 0.566809892654419 - trainLoss: 0.5468560457229614\n",
      "cnt: 0 - valLoss: 0.5667941570281982 - trainLoss: 0.5468382835388184\n",
      "cnt: 0 - valLoss: 0.5667791366577148 - trainLoss: 0.5468205213546753\n",
      "cnt: 0 - valLoss: 0.566764771938324 - trainLoss: 0.546802818775177\n",
      "cnt: 0 - valLoss: 0.5667510032653809 - trainLoss: 0.5467851161956787\n",
      "cnt: 0 - valLoss: 0.5667378902435303 - trainLoss: 0.5467674136161804\n",
      "cnt: 0 - valLoss: 0.5667251348495483 - trainLoss: 0.5467497706413269\n",
      "cnt: 0 - valLoss: 0.5667130351066589 - trainLoss: 0.5467320680618286\n",
      "cnt: 0 - valLoss: 0.5667012929916382 - trainLoss: 0.5467143654823303\n",
      "cnt: 0 - valLoss: 0.5666899681091309 - trainLoss: 0.546696662902832\n",
      "cnt: 0 - valLoss: 0.5666790008544922 - trainLoss: 0.5466790795326233\n",
      "cnt: 0 - valLoss: 0.5666683912277222 - trainLoss: 0.5466614365577698\n",
      "cnt: 0 - valLoss: 0.5666580200195312 - trainLoss: 0.5466437935829163\n",
      "cnt: 0 - valLoss: 0.5666834115982056 - trainLoss: 0.5466263890266418\n",
      "cnt: 1 - valLoss: 0.5666706562042236 - trainLoss: 0.5466088056564331\n",
      "cnt: 2 - valLoss: 0.5666584372520447 - trainLoss: 0.5465911626815796\n",
      "cnt: 3 - valLoss: 0.5666466355323792 - trainLoss: 0.5465735197067261\n",
      "cnt: 0 - valLoss: 0.5666351914405823 - trainLoss: 0.5465558171272278\n",
      "cnt: 0 - valLoss: 0.5666598081588745 - trainLoss: 0.5465384721755981\n",
      "cnt: 1 - valLoss: 0.5666459202766418 - trainLoss: 0.5465209484100342\n",
      "cnt: 2 - valLoss: 0.5666325688362122 - trainLoss: 0.5465033054351807\n",
      "cnt: 0 - valLoss: 0.5666198134422302 - trainLoss: 0.5464856028556824\n",
      "cnt: 0 - valLoss: 0.5666431188583374 - trainLoss: 0.5464681386947632\n",
      "cnt: 1 - valLoss: 0.5666282176971436 - trainLoss: 0.5464507341384888\n",
      "cnt: 2 - valLoss: 0.5666138529777527 - trainLoss: 0.5464330911636353\n",
      "cnt: 0 - valLoss: 0.5666002631187439 - trainLoss: 0.546415388584137\n",
      "cnt: 0 - valLoss: 0.5665739178657532 - trainLoss: 0.5463981032371521\n",
      "cnt: 0 - valLoss: 0.5665982961654663 - trainLoss: 0.5463820099830627\n",
      "cnt: 1 - valLoss: 0.5665353536605835 - trainLoss: 0.5463653802871704\n",
      "cnt: 0 - valLoss: 0.5665618181228638 - trainLoss: 0.5463488101959229\n",
      "cnt: 1 - valLoss: 0.5665364861488342 - trainLoss: 0.546331524848938\n",
      "cnt: 2 - valLoss: 0.5665128231048584 - trainLoss: 0.5463147759437561\n",
      "cnt: 0 - valLoss: 0.5664905905723572 - trainLoss: 0.5462981462478638\n",
      "cnt: 0 - valLoss: 0.5665186047554016 - trainLoss: 0.5462818741798401\n",
      "cnt: 1 - valLoss: 0.5664947032928467 - trainLoss: 0.5462654232978821\n",
      "cnt: 2 - valLoss: 0.5664723515510559 - trainLoss: 0.5462487936019897\n",
      "cnt: 0 - valLoss: 0.5664513111114502 - trainLoss: 0.5462322235107422\n",
      "cnt: 0 - valLoss: 0.5664316415786743 - trainLoss: 0.5462156534194946\n",
      "cnt: 0 - valLoss: 0.5664129257202148 - trainLoss: 0.5461991429328918\n",
      "cnt: 0 - valLoss: 0.5663953423500061 - trainLoss: 0.5461826324462891\n",
      "cnt: 0 - valLoss: 0.5663787126541138 - trainLoss: 0.546166181564331\n",
      "cnt: 0 - valLoss: 0.566411554813385 - trainLoss: 0.5461499691009521\n",
      "cnt: 1 - valLoss: 0.5663923621177673 - trainLoss: 0.5461335778236389\n",
      "cnt: 2 - valLoss: 0.5663743019104004 - trainLoss: 0.5461170673370361\n",
      "cnt: 0 - valLoss: 0.5663571357727051 - trainLoss: 0.5461006760597229\n",
      "cnt: 0 - valLoss: 0.5663409233093262 - trainLoss: 0.5460842251777649\n",
      "cnt: 0 - valLoss: 0.5663255453109741 - trainLoss: 0.5460678339004517\n",
      "cnt: 0 - valLoss: 0.5663108825683594 - trainLoss: 0.5460515022277832\n",
      "cnt: 0 - valLoss: 0.5662968754768372 - trainLoss: 0.54603511095047\n",
      "cnt: 0 - valLoss: 0.5662835836410522 - trainLoss: 0.5460187792778015\n",
      "cnt: 0 - valLoss: 0.5662708878517151 - trainLoss: 0.5460025072097778\n",
      "cnt: 0 - valLoss: 0.5662584900856018 - trainLoss: 0.5459861159324646\n",
      "cnt: 0 - valLoss: 0.5662466287612915 - trainLoss: 0.5459698438644409\n",
      "cnt: 0 - valLoss: 0.5662352442741394 - trainLoss: 0.5459535717964172\n",
      "cnt: 0 - valLoss: 0.5662241578102112 - trainLoss: 0.5459372401237488\n",
      "cnt: 0 - valLoss: 0.5662135481834412 - trainLoss: 0.5459209680557251\n",
      "cnt: 0 - valLoss: 0.566203236579895 - trainLoss: 0.5459046959877014\n",
      "cnt: 0 - valLoss: 0.5661933422088623 - trainLoss: 0.5458884239196777\n",
      "cnt: 0 - valLoss: 0.5661836266517639 - trainLoss: 0.5458720922470093\n",
      "cnt: 0 - valLoss: 0.5661742687225342 - trainLoss: 0.5458558201789856\n",
      "cnt: 0 - valLoss: 0.5661650896072388 - trainLoss: 0.5458395481109619\n",
      "cnt: 0 - valLoss: 0.5661561489105225 - trainLoss: 0.5458232760429382\n",
      "cnt: 0 - valLoss: 0.5661473870277405 - trainLoss: 0.5458070039749146\n",
      "cnt: 0 - valLoss: 0.5661389231681824 - trainLoss: 0.5457907915115356\n",
      "cnt: 0 - valLoss: 0.5661305785179138 - trainLoss: 0.545774519443512\n",
      "cnt: 0 - valLoss: 0.5661224126815796 - trainLoss: 0.5457583069801331\n",
      "cnt: 0 - valLoss: 0.5661143660545349 - trainLoss: 0.5457420349121094\n",
      "cnt: 0 - valLoss: 0.566106379032135 - trainLoss: 0.5457257628440857\n",
      "cnt: 0 - valLoss: 0.5660986304283142 - trainLoss: 0.545709490776062\n",
      "cnt: 0 - valLoss: 0.5660909414291382 - trainLoss: 0.5456932187080383\n",
      "cnt: 0 - valLoss: 0.5660833716392517 - trainLoss: 0.5456770062446594\n",
      "cnt: 0 - valLoss: 0.5660758018493652 - trainLoss: 0.5456607341766357\n",
      "cnt: 0 - valLoss: 0.5660684108734131 - trainLoss: 0.5456445217132568\n",
      "cnt: 0 - valLoss: 0.5660610198974609 - trainLoss: 0.5456282496452332\n",
      "cnt: 0 - valLoss: 0.5660538673400879 - trainLoss: 0.5456120371818542\n",
      "cnt: 0 - valLoss: 0.5660466551780701 - trainLoss: 0.5455957651138306\n",
      "cnt: 0 - valLoss: 0.5660394430160522 - trainLoss: 0.5455795526504517\n",
      "cnt: 0 - valLoss: 0.5660324096679688 - trainLoss: 0.545563280582428\n",
      "cnt: 0 - valLoss: 0.5660253167152405 - trainLoss: 0.5455470085144043\n",
      "cnt: 0 - valLoss: 0.566018283367157 - trainLoss: 0.5455307960510254\n",
      "cnt: 0 - valLoss: 0.5660114288330078 - trainLoss: 0.5455145835876465\n",
      "cnt: 0 - valLoss: 0.5660044550895691 - trainLoss: 0.5454983115196228\n",
      "cnt: 0 - valLoss: 0.5659976005554199 - trainLoss: 0.5454820990562439\n",
      "cnt: 0 - valLoss: 0.5659907460212708 - trainLoss: 0.545465886592865\n",
      "cnt: 0 - valLoss: 0.5659839510917664 - trainLoss: 0.5454496145248413\n",
      "cnt: 0 - valLoss: 0.5659772157669067 - trainLoss: 0.5454334020614624\n",
      "cnt: 0 - valLoss: 0.5659704208374023 - trainLoss: 0.5454171895980835\n",
      "cnt: 0 - valLoss: 0.5659636855125427 - trainLoss: 0.5454009175300598\n",
      "cnt: 0 - valLoss: 0.5659570097923279 - trainLoss: 0.5453847050666809\n",
      "cnt: 0 - valLoss: 0.5659503936767578 - trainLoss: 0.545368492603302\n",
      "cnt: 0 - valLoss: 0.5659436583518982 - trainLoss: 0.5453522801399231\n",
      "cnt: 0 - valLoss: 0.5659370422363281 - trainLoss: 0.5453360080718994\n",
      "cnt: 0 - valLoss: 0.5659304857254028 - trainLoss: 0.5453197360038757\n",
      "cnt: 0 - valLoss: 0.565923810005188 - trainLoss: 0.5453035831451416\n",
      "cnt: 0 - valLoss: 0.5659172534942627 - trainLoss: 0.5452873110771179\n",
      "cnt: 0 - valLoss: 0.5659106373786926 - trainLoss: 0.5452711582183838\n",
      "cnt: 0 - valLoss: 0.5659040808677673 - trainLoss: 0.5452548861503601\n",
      "cnt: 0 - valLoss: 0.5658975839614868 - trainLoss: 0.5452386736869812\n",
      "cnt: 0 - valLoss: 0.5658910274505615 - trainLoss: 0.5452224016189575\n",
      "cnt: 0 - valLoss: 0.565884530544281 - trainLoss: 0.5452061891555786\n",
      "cnt: 0 - valLoss: 0.5658780336380005 - trainLoss: 0.5451899766921997\n",
      "cnt: 0 - valLoss: 0.5658714771270752 - trainLoss: 0.5451737642288208\n",
      "cnt: 0 - valLoss: 0.5658650994300842 - trainLoss: 0.5451576113700867\n",
      "cnt: 0 - valLoss: 0.5658585429191589 - trainLoss: 0.545141339302063\n",
      "cnt: 0 - valLoss: 0.5658521056175232 - trainLoss: 0.5451251268386841\n",
      "cnt: 0 - valLoss: 0.5658456683158875 - trainLoss: 0.5451089143753052\n",
      "cnt: 0 - valLoss: 0.5658392310142517 - trainLoss: 0.545092761516571\n",
      "cnt: 0 - valLoss: 0.565832793712616 - trainLoss: 0.5450764894485474\n",
      "cnt: 0 - valLoss: 0.5658263564109802 - trainLoss: 0.5450602769851685\n",
      "cnt: 0 - valLoss: 0.5658199191093445 - trainLoss: 0.5450441241264343\n",
      "cnt: 0 - valLoss: 0.5658134818077087 - trainLoss: 0.5450279116630554\n",
      "cnt: 0 - valLoss: 0.565807044506073 - trainLoss: 0.5450116991996765\n",
      "cnt: 0 - valLoss: 0.5658007264137268 - trainLoss: 0.5449954867362976\n",
      "cnt: 0 - valLoss: 0.5657923221588135 - trainLoss: 0.5449793338775635\n",
      "cnt: 0 - valLoss: 0.5657860636711121 - trainLoss: 0.5449630618095398\n",
      "cnt: 0 - valLoss: 0.5657798647880554 - trainLoss: 0.5449468493461609\n",
      "cnt: 0 - valLoss: 0.5657736659049988 - trainLoss: 0.5449306964874268\n",
      "cnt: 0 - valLoss: 0.5657673478126526 - trainLoss: 0.5449144840240479\n",
      "cnt: 0 - valLoss: 0.5657591819763184 - trainLoss: 0.544898271560669\n",
      "cnt: 0 - valLoss: 0.5657531023025513 - trainLoss: 0.5448821187019348\n",
      "cnt: 0 - valLoss: 0.5657470226287842 - trainLoss: 0.5448659062385559\n",
      "cnt: 0 - valLoss: 0.5657409429550171 - trainLoss: 0.544849693775177\n",
      "cnt: 0 - valLoss: 0.5657328367233276 - trainLoss: 0.5448335409164429\n",
      "cnt: 0 - valLoss: 0.5657268762588501 - trainLoss: 0.544817328453064\n",
      "cnt: 0 - valLoss: 0.5657209157943726 - trainLoss: 0.5448011755943298\n",
      "cnt: 0 - valLoss: 0.5657143592834473 - trainLoss: 0.5447849631309509\n",
      "cnt: 0 - valLoss: 0.5657058954238892 - trainLoss: 0.544768750667572\n",
      "cnt: 0 - valLoss: 0.565699577331543 - trainLoss: 0.5447526574134827\n",
      "cnt: 0 - valLoss: 0.5656932592391968 - trainLoss: 0.5447364449501038\n",
      "cnt: 0 - valLoss: 0.5656869411468506 - trainLoss: 0.5447202920913696\n",
      "cnt: 0 - valLoss: 0.565680742263794 - trainLoss: 0.5447040796279907\n",
      "cnt: 0 - valLoss: 0.5656724572181702 - trainLoss: 0.5446879267692566\n",
      "cnt: 0 - valLoss: 0.5656663179397583 - trainLoss: 0.5446717739105225\n",
      "cnt: 0 - valLoss: 0.5656601786613464 - trainLoss: 0.5446556210517883\n",
      "cnt: 0 - valLoss: 0.5656540393829346 - trainLoss: 0.5446394681930542\n",
      "cnt: 0 - valLoss: 0.5656479597091675 - trainLoss: 0.5446233153343201\n",
      "cnt: 0 - valLoss: 0.565639853477478 - trainLoss: 0.5446071028709412\n",
      "cnt: 0 - valLoss: 0.5656338930130005 - trainLoss: 0.544590950012207\n",
      "cnt: 0 - valLoss: 0.565627932548523 - trainLoss: 0.5445747971534729\n",
      "cnt: 0 - valLoss: 0.5656219124794006 - trainLoss: 0.5445586442947388\n",
      "cnt: 0 - valLoss: 0.5656158924102783 - trainLoss: 0.5445424914360046\n",
      "cnt: 0 - valLoss: 0.5656079053878784 - trainLoss: 0.5445263385772705\n",
      "cnt: 0 - valLoss: 0.5656020641326904 - trainLoss: 0.5445101857185364\n",
      "cnt: 0 - valLoss: 0.5655961632728577 - trainLoss: 0.5444940328598022\n",
      "cnt: 0 - valLoss: 0.5655902624130249 - trainLoss: 0.5444778800010681\n",
      "cnt: 0 - valLoss: 0.5655843615531921 - trainLoss: 0.544461727142334\n",
      "cnt: 0 - valLoss: 0.5655829310417175 - trainLoss: 0.5444455742835999\n",
      "cnt: 0 - valLoss: 0.5655811429023743 - trainLoss: 0.5444293022155762\n",
      "cnt: 0 - valLoss: 0.5655788779258728 - trainLoss: 0.544413149356842\n",
      "cnt: 0 - valLoss: 0.5655784010887146 - trainLoss: 0.5443969368934631\n",
      "cnt: 0 - valLoss: 0.565575361251831 - trainLoss: 0.5443806648254395\n",
      "cnt: 0 - valLoss: 0.5655721426010132 - trainLoss: 0.5443645119667053\n",
      "cnt: 0 - valLoss: 0.5655685067176819 - trainLoss: 0.5443482995033264\n",
      "cnt: 0 - valLoss: 0.565564751625061 - trainLoss: 0.5443320870399475\n",
      "cnt: 0 - valLoss: 0.5655607581138611 - trainLoss: 0.5443158745765686\n",
      "cnt: 0 - valLoss: 0.5655586123466492 - trainLoss: 0.5442997217178345\n",
      "cnt: 0 - valLoss: 0.5655540823936462 - trainLoss: 0.5442835092544556\n",
      "cnt: 0 - valLoss: 0.565549373626709 - trainLoss: 0.5442674160003662\n",
      "cnt: 0 - valLoss: 0.5655446648597717 - trainLoss: 0.5442511439323425\n",
      "cnt: 0 - valLoss: 0.5655417442321777 - trainLoss: 0.5442349910736084\n",
      "cnt: 0 - valLoss: 0.5655365586280823 - trainLoss: 0.5442188382148743\n",
      "cnt: 0 - valLoss: 0.5655312538146973 - trainLoss: 0.5442026257514954\n",
      "cnt: 0 - valLoss: 0.5655279159545898 - trainLoss: 0.5441864728927612\n",
      "cnt: 0 - valLoss: 0.5655223727226257 - trainLoss: 0.5441703200340271\n",
      "cnt: 0 - valLoss: 0.5655187368392944 - trainLoss: 0.5441541075706482\n",
      "cnt: 0 - valLoss: 0.5655128359794617 - trainLoss: 0.5441378951072693\n",
      "cnt: 0 - valLoss: 0.5655069947242737 - trainLoss: 0.5441217422485352\n",
      "cnt: 0 - valLoss: 0.5655031204223633 - trainLoss: 0.544105589389801\n",
      "cnt: 0 - valLoss: 0.5654970407485962 - trainLoss: 0.5440894365310669\n",
      "cnt: 0 - valLoss: 0.5654929280281067 - trainLoss: 0.5440732836723328\n",
      "cnt: 0 - valLoss: 0.5654850006103516 - trainLoss: 0.5440571308135986\n",
      "cnt: 0 - valLoss: 0.5654793381690979 - trainLoss: 0.5440410375595093\n",
      "cnt: 0 - valLoss: 0.5654715299606323 - trainLoss: 0.5440248250961304\n",
      "cnt: 0 - valLoss: 0.5654658675193787 - trainLoss: 0.5440086722373962\n",
      "cnt: 0 - valLoss: 0.5654582381248474 - trainLoss: 0.5439925789833069\n",
      "cnt: 0 - valLoss: 0.5654527544975281 - trainLoss: 0.5439764261245728\n",
      "cnt: 0 - valLoss: 0.5654471516609192 - trainLoss: 0.5439602732658386\n",
      "cnt: 0 - valLoss: 0.5654395818710327 - trainLoss: 0.5439441204071045\n",
      "cnt: 0 - valLoss: 0.5654340982437134 - trainLoss: 0.5439280271530151\n",
      "cnt: 0 - valLoss: 0.565428614616394 - trainLoss: 0.5439118146896362\n",
      "cnt: 0 - valLoss: 0.5654117465019226 - trainLoss: 0.5438958406448364\n",
      "cnt: 0 - valLoss: 0.5653979182243347 - trainLoss: 0.5438798069953918\n",
      "cnt: 0 - valLoss: 0.5653848052024841 - trainLoss: 0.5438638925552368\n",
      "cnt: 0 - valLoss: 0.5653724074363708 - trainLoss: 0.5438480377197266\n",
      "cnt: 0 - valLoss: 0.5653604865074158 - trainLoss: 0.5438321232795715\n",
      "cnt: 0 - valLoss: 0.5653507113456726 - trainLoss: 0.543816328048706\n",
      "cnt: 0 - valLoss: 0.5653414130210876 - trainLoss: 0.543800413608551\n",
      "cnt: 0 - valLoss: 0.5653323531150818 - trainLoss: 0.5437845587730408\n",
      "cnt: 0 - valLoss: 0.5653235912322998 - trainLoss: 0.5437687039375305\n",
      "cnt: 0 - valLoss: 0.5653133392333984 - trainLoss: 0.5437528491020203\n",
      "cnt: 0 - valLoss: 0.565305233001709 - trainLoss: 0.5437370538711548\n",
      "cnt: 0 - valLoss: 0.5652955770492554 - trainLoss: 0.5437211990356445\n",
      "cnt: 0 - valLoss: 0.5652862787246704 - trainLoss: 0.5437053442001343\n",
      "cnt: 0 - valLoss: 0.5652772784233093 - trainLoss: 0.5436895489692688\n",
      "cnt: 0 - valLoss: 0.5652685761451721 - trainLoss: 0.5436736941337585\n",
      "cnt: 0 - valLoss: 0.5652508735656738 - trainLoss: 0.5436578989028931\n",
      "cnt: 0 - valLoss: 0.56523597240448 - trainLoss: 0.5436422824859619\n",
      "cnt: 0 - valLoss: 0.5652217864990234 - trainLoss: 0.543626606464386\n",
      "cnt: 0 - valLoss: 0.565208375453949 - trainLoss: 0.5436110496520996\n",
      "cnt: 0 - valLoss: 0.5651956796646118 - trainLoss: 0.5435954332351685\n",
      "cnt: 0 - valLoss: 0.5651835799217224 - trainLoss: 0.5435798764228821\n",
      "cnt: 0 - valLoss: 0.5651718974113464 - trainLoss: 0.5435643196105957\n",
      "cnt: 0 - valLoss: 0.5651606917381287 - trainLoss: 0.5435487031936646\n",
      "cnt: 0 - valLoss: 0.5651500821113586 - trainLoss: 0.5435331463813782\n",
      "cnt: 0 - valLoss: 0.5651397705078125 - trainLoss: 0.5435176491737366\n",
      "cnt: 0 - valLoss: 0.5651299357414246 - trainLoss: 0.5435020923614502\n",
      "cnt: 0 - valLoss: 0.5651203989982605 - trainLoss: 0.5434865355491638\n",
      "cnt: 0 - valLoss: 0.5651112198829651 - trainLoss: 0.543471097946167\n",
      "cnt: 0 - valLoss: 0.5651023387908936 - trainLoss: 0.5434556007385254\n",
      "cnt: 0 - valLoss: 0.5650937557220459 - trainLoss: 0.543440043926239\n",
      "cnt: 0 - valLoss: 0.5650853514671326 - trainLoss: 0.5434244871139526\n",
      "cnt: 0 - valLoss: 0.5650771260261536 - trainLoss: 0.543408989906311\n",
      "cnt: 0 - valLoss: 0.5650691390037537 - trainLoss: 0.5433934926986694\n",
      "cnt: 0 - valLoss: 0.5650613307952881 - trainLoss: 0.5433779954910278\n",
      "cnt: 0 - valLoss: 0.5650537610054016 - trainLoss: 0.5433624982833862\n",
      "cnt: 0 - valLoss: 0.5650443434715271 - trainLoss: 0.5433470010757446\n",
      "cnt: 0 - valLoss: 0.565037190914154 - trainLoss: 0.5433314442634583\n",
      "cnt: 0 - valLoss: 0.5650302171707153 - trainLoss: 0.5433160066604614\n",
      "cnt: 0 - valLoss: 0.5650232434272766 - trainLoss: 0.543300449848175\n",
      "cnt: 0 - valLoss: 0.5650144219398499 - trainLoss: 0.5432850122451782\n",
      "cnt: 0 - valLoss: 0.5650079250335693 - trainLoss: 0.5432695150375366\n",
      "cnt: 0 - valLoss: 0.5650014281272888 - trainLoss: 0.543254017829895\n",
      "cnt: 0 - valLoss: 0.5649930238723755 - trainLoss: 0.5432385206222534\n",
      "cnt: 0 - valLoss: 0.5649868249893188 - trainLoss: 0.5432230830192566\n",
      "cnt: 0 - valLoss: 0.564980685710907 - trainLoss: 0.543207585811615\n",
      "cnt: 0 - valLoss: 0.5649725198745728 - trainLoss: 0.5431920886039734\n",
      "cnt: 0 - valLoss: 0.5649664998054504 - trainLoss: 0.5431766510009766\n",
      "cnt: 0 - valLoss: 0.5649590492248535 - trainLoss: 0.543161153793335\n",
      "cnt: 0 - valLoss: 0.5649495720863342 - trainLoss: 0.5431457161903381\n",
      "cnt: 0 - valLoss: 0.5649425387382507 - trainLoss: 0.5431302785873413\n",
      "cnt: 0 - valLoss: 0.564935564994812 - trainLoss: 0.5431147217750549\n",
      "cnt: 0 - valLoss: 0.5649287104606628 - trainLoss: 0.5430994033813477\n",
      "cnt: 0 - valLoss: 0.5649219751358032 - trainLoss: 0.543083906173706\n",
      "cnt: 0 - valLoss: 0.564913272857666 - trainLoss: 0.5430684089660645\n",
      "cnt: 0 - valLoss: 0.5649068355560303 - trainLoss: 0.5430529713630676\n",
      "cnt: 0 - valLoss: 0.5649004578590393 - trainLoss: 0.5430375337600708\n",
      "cnt: 0 - valLoss: 0.5648941993713379 - trainLoss: 0.543022096157074\n",
      "cnt: 0 - valLoss: 0.5648859143257141 - trainLoss: 0.5430066585540771\n",
      "cnt: 0 - valLoss: 0.564879834651947 - trainLoss: 0.5429912805557251\n",
      "cnt: 0 - valLoss: 0.5648739337921143 - trainLoss: 0.5429757833480835\n",
      "cnt: 0 - valLoss: 0.5648679137229919 - trainLoss: 0.5429603457450867\n",
      "cnt: 0 - valLoss: 0.5648598670959473 - trainLoss: 0.5429449081420898\n",
      "cnt: 0 - valLoss: 0.5648542046546936 - trainLoss: 0.542929470539093\n",
      "cnt: 0 - valLoss: 0.5648484230041504 - trainLoss: 0.5429140329360962\n",
      "cnt: 0 - valLoss: 0.5648406147956848 - trainLoss: 0.5428985357284546\n",
      "cnt: 0 - valLoss: 0.5648350715637207 - trainLoss: 0.5428831577301025\n",
      "cnt: 0 - valLoss: 0.5648294687271118 - trainLoss: 0.5428677201271057\n",
      "cnt: 0 - valLoss: 0.5648238658905029 - trainLoss: 0.5428523421287537\n",
      "cnt: 0 - valLoss: 0.5648161768913269 - trainLoss: 0.5428369045257568\n",
      "cnt: 0 - valLoss: 0.5648108124732971 - trainLoss: 0.54282146692276\n",
      "cnt: 0 - valLoss: 0.564805269241333 - trainLoss: 0.5428060293197632\n",
      "cnt: 0 - valLoss: 0.5647978186607361 - trainLoss: 0.5427905917167664\n",
      "cnt: 0 - valLoss: 0.5647925138473511 - trainLoss: 0.5427752137184143\n",
      "cnt: 0 - valLoss: 0.5647871494293213 - trainLoss: 0.5427597761154175\n",
      "cnt: 0 - valLoss: 0.5647817254066467 - trainLoss: 0.5427443981170654\n",
      "cnt: 0 - valLoss: 0.5647742748260498 - trainLoss: 0.5427289605140686\n",
      "cnt: 0 - valLoss: 0.5647690296173096 - trainLoss: 0.5427135825157166\n",
      "cnt: 0 - valLoss: 0.5647637248039246 - trainLoss: 0.5426981449127197\n",
      "cnt: 0 - valLoss: 0.5647562742233276 - trainLoss: 0.5426827669143677\n",
      "cnt: 0 - valLoss: 0.5647560954093933 - trainLoss: 0.5426673889160156\n",
      "cnt: 0 - valLoss: 0.5647533535957336 - trainLoss: 0.5426520109176636\n",
      "cnt: 0 - valLoss: 0.5647523999214172 - trainLoss: 0.5426366925239563\n",
      "cnt: 0 - valLoss: 0.5647488236427307 - trainLoss: 0.542621374130249\n",
      "cnt: 0 - valLoss: 0.5647472739219666 - trainLoss: 0.5426060557365417\n",
      "cnt: 0 - valLoss: 0.5647432208061218 - trainLoss: 0.5425907373428345\n",
      "cnt: 0 - valLoss: 0.5647410750389099 - trainLoss: 0.5425754189491272\n",
      "cnt: 0 - valLoss: 0.564736545085907 - trainLoss: 0.5425601005554199\n",
      "cnt: 0 - valLoss: 0.5647339820861816 - trainLoss: 0.5425447821617126\n",
      "cnt: 0 - valLoss: 0.5647311210632324 - trainLoss: 0.5425295233726501\n",
      "cnt: 0 - valLoss: 0.5647260546684265 - trainLoss: 0.5425142049789429\n",
      "cnt: 0 - valLoss: 0.5647228360176086 - trainLoss: 0.5424988865852356\n",
      "cnt: 0 - valLoss: 0.5647174119949341 - trainLoss: 0.5424835681915283\n",
      "cnt: 0 - valLoss: 0.5647140741348267 - trainLoss: 0.542468249797821\n",
      "cnt: 0 - valLoss: 0.5647104382514954 - trainLoss: 0.5424529910087585\n",
      "cnt: 0 - valLoss: 0.5647046566009521 - trainLoss: 0.542437732219696\n",
      "cnt: 0 - valLoss: 0.5647009015083313 - trainLoss: 0.5424224138259888\n",
      "cnt: 0 - valLoss: 0.5646969079971313 - trainLoss: 0.5424071550369263\n",
      "cnt: 0 - valLoss: 0.5646908283233643 - trainLoss: 0.5423917770385742\n",
      "cnt: 0 - valLoss: 0.5646868348121643 - trainLoss: 0.5423765182495117\n",
      "cnt: 0 - valLoss: 0.5646826028823853 - trainLoss: 0.5423612594604492\n",
      "cnt: 0 - valLoss: 0.5646783113479614 - trainLoss: 0.5423460006713867\n",
      "cnt: 0 - valLoss: 0.5646718144416809 - trainLoss: 0.5423306822776794\n",
      "cnt: 0 - valLoss: 0.5646674633026123 - trainLoss: 0.5423154234886169\n",
      "cnt: 0 - valLoss: 0.5646631121635437 - trainLoss: 0.5423001646995544\n",
      "cnt: 0 - valLoss: 0.5646585822105408 - trainLoss: 0.5422848463058472\n",
      "cnt: 0 - valLoss: 0.5646517872810364 - trainLoss: 0.5422696471214294\n",
      "cnt: 0 - valLoss: 0.5646473169326782 - trainLoss: 0.5422543287277222\n",
      "cnt: 0 - valLoss: 0.5646426677703857 - trainLoss: 0.5422390699386597\n",
      "cnt: 0 - valLoss: 0.5646379590034485 - trainLoss: 0.5422238111495972\n",
      "cnt: 0 - valLoss: 0.5646311044692993 - trainLoss: 0.5422085523605347\n",
      "cnt: 0 - valLoss: 0.5646263957023621 - trainLoss: 0.5421932935714722\n",
      "cnt: 0 - valLoss: 0.5646216869354248 - trainLoss: 0.5421779751777649\n",
      "cnt: 0 - valLoss: 0.5646169185638428 - trainLoss: 0.5421627759933472\n",
      "cnt: 0 - valLoss: 0.5646120309829712 - trainLoss: 0.5421474575996399\n",
      "cnt: 0 - valLoss: 0.5646049380302429 - trainLoss: 0.5421322584152222\n",
      "cnt: 0 - valLoss: 0.5646002292633057 - trainLoss: 0.5421169400215149\n",
      "cnt: 0 - valLoss: 0.5645953416824341 - trainLoss: 0.5421017408370972\n",
      "cnt: 0 - valLoss: 0.5645903944969177 - trainLoss: 0.5420864224433899\n",
      "cnt: 0 - valLoss: 0.5645854473114014 - trainLoss: 0.5420712232589722\n",
      "cnt: 0 - valLoss: 0.5645783543586731 - trainLoss: 0.5420559644699097\n",
      "cnt: 0 - valLoss: 0.5645734667778015 - trainLoss: 0.5420407056808472\n",
      "cnt: 0 - valLoss: 0.5645685791969299 - trainLoss: 0.5420254468917847\n",
      "cnt: 0 - valLoss: 0.564563512802124 - trainLoss: 0.5420102477073669\n",
      "cnt: 0 - valLoss: 0.5645585060119629 - trainLoss: 0.5419949889183044\n",
      "cnt: 0 - valLoss: 0.564553439617157 - trainLoss: 0.5419797301292419\n",
      "cnt: 0 - valLoss: 0.5645462870597839 - trainLoss: 0.5419645309448242\n",
      "cnt: 0 - valLoss: 0.5645413398742676 - trainLoss: 0.5419492721557617\n",
      "cnt: 0 - valLoss: 0.5645363330841064 - trainLoss: 0.541934072971344\n",
      "cnt: 0 - valLoss: 0.5645313262939453 - trainLoss: 0.5419187545776367\n",
      "cnt: 0 - valLoss: 0.5645262598991394 - trainLoss: 0.5419036149978638\n",
      "cnt: 0 - valLoss: 0.5645190477371216 - trainLoss: 0.5418883562088013\n",
      "cnt: 0 - valLoss: 0.56451416015625 - trainLoss: 0.5418731570243835\n",
      "cnt: 0 - valLoss: 0.5645092129707336 - trainLoss: 0.541857898235321\n",
      "cnt: 0 - valLoss: 0.5645041465759277 - trainLoss: 0.5418426990509033\n",
      "cnt: 0 - valLoss: 0.5644991397857666 - trainLoss: 0.5418274998664856\n",
      "cnt: 0 - valLoss: 0.5644940137863159 - trainLoss: 0.5418122410774231\n",
      "cnt: 0 - valLoss: 0.5644867420196533 - trainLoss: 0.5417970418930054\n",
      "cnt: 0 - valLoss: 0.5644818544387817 - trainLoss: 0.5417818427085876\n",
      "cnt: 0 - valLoss: 0.5644768476486206 - trainLoss: 0.5417665839195251\n",
      "cnt: 0 - valLoss: 0.5644718408584595 - trainLoss: 0.5417513847351074\n",
      "cnt: 0 - valLoss: 0.5644667148590088 - trainLoss: 0.5417361855506897\n",
      "cnt: 0 - valLoss: 0.5644615888595581 - trainLoss: 0.5417209267616272\n",
      "cnt: 0 - valLoss: 0.5644564628601074 - trainLoss: 0.5417057871818542\n",
      "cnt: 0 - valLoss: 0.5644491910934448 - trainLoss: 0.5416905879974365\n",
      "cnt: 0 - valLoss: 0.5644441843032837 - trainLoss: 0.5416753888130188\n",
      "cnt: 0 - valLoss: 0.5644391775131226 - trainLoss: 0.5416601896286011\n",
      "cnt: 0 - valLoss: 0.5644341707229614 - trainLoss: 0.5416449308395386\n",
      "cnt: 0 - valLoss: 0.5644290447235107 - trainLoss: 0.5416297316551208\n",
      "cnt: 0 - valLoss: 0.5644239187240601 - trainLoss: 0.5416145324707031\n",
      "cnt: 0 - valLoss: 0.5644188523292542 - trainLoss: 0.5415993332862854\n",
      "cnt: 0 - valLoss: 0.5644115209579468 - trainLoss: 0.5415841937065125\n",
      "cnt: 0 - valLoss: 0.5644065737724304 - trainLoss: 0.5415689945220947\n",
      "cnt: 0 - valLoss: 0.5644016265869141 - trainLoss: 0.541553795337677\n",
      "cnt: 0 - valLoss: 0.5643965601921082 - trainLoss: 0.5415385961532593\n",
      "cnt: 0 - valLoss: 0.5643914341926575 - trainLoss: 0.5415233969688416\n",
      "cnt: 0 - valLoss: 0.5643863677978516 - trainLoss: 0.5415082573890686\n",
      "cnt: 0 - valLoss: 0.5643813014030457 - trainLoss: 0.5414930582046509\n",
      "cnt: 0 - valLoss: 0.5643740296363831 - trainLoss: 0.5414778590202332\n",
      "cnt: 0 - valLoss: 0.5643690228462219 - trainLoss: 0.5414627194404602\n",
      "cnt: 0 - valLoss: 0.5643640756607056 - trainLoss: 0.5414475202560425\n",
      "cnt: 0 - valLoss: 0.5643590688705444 - trainLoss: 0.5414324402809143\n",
      "cnt: 0 - valLoss: 0.5643540024757385 - trainLoss: 0.5414171814918518\n",
      "cnt: 0 - valLoss: 0.5643488764762878 - trainLoss: 0.5414020419120789\n",
      "cnt: 0 - valLoss: 0.5643438100814819 - trainLoss: 0.5413869023323059\n",
      "cnt: 0 - valLoss: 0.5643386840820312 - trainLoss: 0.5413717031478882\n",
      "cnt: 0 - valLoss: 0.5643335580825806 - trainLoss: 0.5413565635681152\n",
      "cnt: 0 - valLoss: 0.5643262267112732 - trainLoss: 0.5413414239883423\n",
      "cnt: 0 - valLoss: 0.5643212199211121 - trainLoss: 0.5413262248039246\n",
      "cnt: 0 - valLoss: 0.5643162727355957 - trainLoss: 0.5413111448287964\n",
      "cnt: 0 - valLoss: 0.5643113255500793 - trainLoss: 0.5412958860397339\n",
      "cnt: 0 - valLoss: 0.5643062591552734 - trainLoss: 0.5412807464599609\n",
      "cnt: 0 - valLoss: 0.5643011331558228 - trainLoss: 0.541265606880188\n",
      "cnt: 0 - valLoss: 0.5642960667610168 - trainLoss: 0.541250467300415\n",
      "cnt: 0 - valLoss: 0.5642909407615662 - trainLoss: 0.5412353277206421\n",
      "cnt: 0 - valLoss: 0.5642850995063782 - trainLoss: 0.5412201881408691\n",
      "cnt: 0 - valLoss: 0.5642778873443604 - trainLoss: 0.5412050485610962\n",
      "cnt: 0 - valLoss: 0.5642729997634888 - trainLoss: 0.5411899089813232\n",
      "cnt: 0 - valLoss: 0.5642673373222351 - trainLoss: 0.5411747694015503\n",
      "cnt: 0 - valLoss: 0.5642625093460083 - trainLoss: 0.5411596298217773\n",
      "cnt: 0 - valLoss: 0.5642575621604919 - trainLoss: 0.5411445498466492\n",
      "cnt: 0 - valLoss: 0.5642518997192383 - trainLoss: 0.5411294102668762\n",
      "cnt: 0 - valLoss: 0.56424480676651 - trainLoss: 0.541114330291748\n",
      "cnt: 0 - valLoss: 0.5642400979995728 - trainLoss: 0.5410991907119751\n",
      "cnt: 0 - valLoss: 0.5642346143722534 - trainLoss: 0.5410839915275574\n",
      "cnt: 0 - valLoss: 0.5642297863960266 - trainLoss: 0.541068971157074\n",
      "cnt: 0 - valLoss: 0.5642250776290894 - trainLoss: 0.541053831577301\n",
      "cnt: 0 - valLoss: 0.5642194151878357 - trainLoss: 0.5410386323928833\n",
      "cnt: 0 - valLoss: 0.5642125606536865 - trainLoss: 0.5410234928131104\n",
      "cnt: 0 - valLoss: 0.5642075538635254 - trainLoss: 0.5410084128379822\n",
      "cnt: 0 - valLoss: 0.5642015933990479 - trainLoss: 0.5409933924674988\n",
      "cnt: 0 - valLoss: 0.5641965270042419 - trainLoss: 0.540978193283081\n",
      "cnt: 0 - valLoss: 0.5641915798187256 - trainLoss: 0.5409631729125977\n",
      "cnt: 0 - valLoss: 0.5641857385635376 - trainLoss: 0.5409480333328247\n",
      "cnt: 0 - valLoss: 0.5641806721687317 - trainLoss: 0.5409329533576965\n",
      "cnt: 0 - valLoss: 0.5641756653785706 - trainLoss: 0.5409178733825684\n",
      "cnt: 0 - valLoss: 0.5641706585884094 - trainLoss: 0.5409027338027954\n",
      "cnt: 0 - valLoss: 0.5641648173332214 - trainLoss: 0.540887713432312\n",
      "cnt: 0 - valLoss: 0.564157247543335 - trainLoss: 0.5408725738525391\n",
      "cnt: 0 - valLoss: 0.5641512274742126 - trainLoss: 0.5408574938774109\n",
      "cnt: 0 - valLoss: 0.5641461610794067 - trainLoss: 0.5408424139022827\n",
      "cnt: 0 - valLoss: 0.564141035079956 - trainLoss: 0.5408273339271545\n",
      "cnt: 0 - valLoss: 0.5641351342201233 - trainLoss: 0.5408122539520264\n",
      "cnt: 0 - valLoss: 0.5641301274299622 - trainLoss: 0.5407971739768982\n",
      "cnt: 0 - valLoss: 0.5641250610351562 - trainLoss: 0.54078209400177\n",
      "cnt: 0 - valLoss: 0.5641199350357056 - trainLoss: 0.5407670140266418\n",
      "cnt: 0 - valLoss: 0.5641141533851624 - trainLoss: 0.5407519936561584\n",
      "cnt: 0 - valLoss: 0.5641090869903564 - trainLoss: 0.5407368540763855\n",
      "cnt: 0 - valLoss: 0.5641041398048401 - trainLoss: 0.5407218337059021\n",
      "cnt: 0 - valLoss: 0.5640982985496521 - trainLoss: 0.5407067537307739\n",
      "cnt: 0 - valLoss: 0.5640933513641357 - trainLoss: 0.5406916737556458\n",
      "cnt: 0 - valLoss: 0.5640883445739746 - trainLoss: 0.5406766533851624\n",
      "cnt: 0 - valLoss: 0.5640826225280762 - trainLoss: 0.540661633014679\n",
      "cnt: 0 - valLoss: 0.5640755295753479 - trainLoss: 0.5406465530395508\n",
      "cnt: 0 - valLoss: 0.5640700459480286 - trainLoss: 0.5406314134597778\n",
      "cnt: 0 - valLoss: 0.5640653967857361 - trainLoss: 0.5406163930892944\n",
      "cnt: 0 - valLoss: 0.5640606880187988 - trainLoss: 0.540601372718811\n",
      "cnt: 0 - valLoss: 0.5640551447868347 - trainLoss: 0.5405863523483276\n",
      "cnt: 0 - valLoss: 0.5640504360198975 - trainLoss: 0.5405712723731995\n",
      "cnt: 0 - valLoss: 0.5640456676483154 - trainLoss: 0.5405562520027161\n",
      "cnt: 0 - valLoss: 0.5640401840209961 - trainLoss: 0.5405411720275879\n",
      "cnt: 0 - valLoss: 0.5640354752540588 - trainLoss: 0.5405261516571045\n",
      "cnt: 0 - valLoss: 0.5640307664871216 - trainLoss: 0.5405111312866211\n",
      "cnt: 0 - valLoss: 0.5640251040458679 - trainLoss: 0.5404961109161377\n",
      "cnt: 0 - valLoss: 0.5640183091163635 - trainLoss: 0.5404810309410095\n",
      "cnt: 0 - valLoss: 0.5640129446983337 - trainLoss: 0.5404660105705261\n",
      "cnt: 0 - valLoss: 0.5640084743499756 - trainLoss: 0.5404510498046875\n",
      "cnt: 0 - valLoss: 0.5640038251876831 - trainLoss: 0.5404359698295593\n",
      "cnt: 0 - valLoss: 0.5639985203742981 - trainLoss: 0.5404209494590759\n",
      "cnt: 0 - valLoss: 0.5639939308166504 - trainLoss: 0.5404059290885925\n",
      "cnt: 0 - valLoss: 0.5639893412590027 - trainLoss: 0.5403909087181091\n",
      "cnt: 0 - valLoss: 0.5639832615852356 - trainLoss: 0.5403758883476257\n",
      "cnt: 0 - valLoss: 0.5639780163764954 - trainLoss: 0.5403608679771423\n",
      "cnt: 0 - valLoss: 0.5639728307723999 - trainLoss: 0.5403458476066589\n",
      "cnt: 0 - valLoss: 0.5639669299125671 - trainLoss: 0.5403308868408203\n",
      "cnt: 0 - valLoss: 0.5639618635177612 - trainLoss: 0.5403159260749817\n",
      "cnt: 0 - valLoss: 0.5639560222625732 - trainLoss: 0.5403008460998535\n",
      "cnt: 0 - valLoss: 0.5639510750770569 - trainLoss: 0.5402858853340149\n",
      "cnt: 0 - valLoss: 0.5639460682868958 - trainLoss: 0.5402708649635315\n",
      "cnt: 0 - valLoss: 0.5639403462409973 - trainLoss: 0.5402559041976929\n",
      "cnt: 0 - valLoss: 0.5639354586601257 - trainLoss: 0.5402408838272095\n",
      "cnt: 0 - valLoss: 0.5639305114746094 - trainLoss: 0.5402259230613708\n",
      "cnt: 0 - valLoss: 0.5639247894287109 - trainLoss: 0.5402109026908875\n",
      "cnt: 0 - valLoss: 0.5639172792434692 - trainLoss: 0.5401959419250488\n",
      "cnt: 0 - valLoss: 0.5639098882675171 - trainLoss: 0.5401808619499207\n",
      "cnt: 0 - valLoss: 0.5639020800590515 - trainLoss: 0.540165901184082\n",
      "cnt: 0 - valLoss: 0.5638952255249023 - trainLoss: 0.5401508808135986\n",
      "cnt: 0 - valLoss: 0.5638886094093323 - trainLoss: 0.5401358604431152\n",
      "cnt: 0 - valLoss: 0.5638813376426697 - trainLoss: 0.5401208400726318\n",
      "cnt: 0 - valLoss: 0.5638751983642578 - trainLoss: 0.5401058197021484\n",
      "cnt: 0 - valLoss: 0.5638689994812012 - trainLoss: 0.5400907397270203\n",
      "cnt: 0 - valLoss: 0.5638622641563416 - trainLoss: 0.5400757789611816\n",
      "cnt: 0 - valLoss: 0.5638564825057983 - trainLoss: 0.540060818195343\n",
      "cnt: 0 - valLoss: 0.5638507604598999 - trainLoss: 0.5400457978248596\n",
      "cnt: 0 - valLoss: 0.5638443827629089 - trainLoss: 0.5400307774543762\n",
      "cnt: 0 - valLoss: 0.5638388395309448 - trainLoss: 0.5400157570838928\n",
      "cnt: 0 - valLoss: 0.5638334155082703 - trainLoss: 0.540000855922699\n",
      "cnt: 0 - valLoss: 0.5638272762298584 - trainLoss: 0.5399858355522156\n",
      "cnt: 0 - valLoss: 0.5638220310211182 - trainLoss: 0.5399708151817322\n",
      "cnt: 0 - valLoss: 0.5638167858123779 - trainLoss: 0.5399558544158936\n",
      "cnt: 0 - valLoss: 0.5638108849525452 - trainLoss: 0.5399408936500549\n",
      "cnt: 0 - valLoss: 0.5638057589530945 - trainLoss: 0.5399258732795715\n",
      "cnt: 0 - valLoss: 0.5638006925582886 - trainLoss: 0.5399109125137329\n",
      "cnt: 0 - valLoss: 0.5637948513031006 - trainLoss: 0.5398959517478943\n",
      "cnt: 0 - valLoss: 0.5637898445129395 - trainLoss: 0.5398809313774109\n",
      "cnt: 0 - valLoss: 0.563784122467041 - trainLoss: 0.5398659706115723\n",
      "cnt: 0 - valLoss: 0.5637792348861694 - trainLoss: 0.5398510098457336\n",
      "cnt: 0 - valLoss: 0.5637744665145874 - trainLoss: 0.539836049079895\n",
      "cnt: 0 - valLoss: 0.5637688040733337 - trainLoss: 0.5398210883140564\n",
      "cnt: 0 - valLoss: 0.5637639760971069 - trainLoss: 0.5398061275482178\n",
      "cnt: 0 - valLoss: 0.563758373260498 - trainLoss: 0.5397912263870239\n",
      "cnt: 0 - valLoss: 0.5637537240982056 - trainLoss: 0.5397761464118958\n",
      "cnt: 0 - valLoss: 0.5637489557266235 - trainLoss: 0.5397612452507019\n",
      "cnt: 0 - valLoss: 0.5637433528900146 - trainLoss: 0.5397462844848633\n",
      "cnt: 0 - valLoss: 0.5637387037277222 - trainLoss: 0.5397313833236694\n",
      "cnt: 0 - valLoss: 0.5637332201004028 - trainLoss: 0.5397164225578308\n",
      "cnt: 0 - valLoss: 0.5637285709381104 - trainLoss: 0.539701521396637\n",
      "cnt: 0 - valLoss: 0.5637238621711731 - trainLoss: 0.5396865606307983\n",
      "cnt: 0 - valLoss: 0.5637184977531433 - trainLoss: 0.5396715998649597\n",
      "cnt: 0 - valLoss: 0.5637138485908508 - trainLoss: 0.5396566987037659\n",
      "cnt: 0 - valLoss: 0.5637084245681763 - trainLoss: 0.539641797542572\n",
      "cnt: 0 - valLoss: 0.5637038350105286 - trainLoss: 0.5396267771720886\n",
      "cnt: 0 - valLoss: 0.5636992454528809 - trainLoss: 0.53961181640625\n",
      "cnt: 0 - valLoss: 0.5636937618255615 - trainLoss: 0.5395969748497009\n",
      "cnt: 0 - valLoss: 0.5636892318725586 - trainLoss: 0.5395820140838623\n",
      "cnt: 0 - valLoss: 0.5636749267578125 - trainLoss: 0.539567232131958\n",
      "cnt: 0 - valLoss: 0.5636624097824097 - trainLoss: 0.5395525097846985\n",
      "cnt: 0 - valLoss: 0.5636506080627441 - trainLoss: 0.5395378470420837\n",
      "cnt: 0 - valLoss: 0.5636395812034607 - trainLoss: 0.5395231246948242\n",
      "cnt: 0 - valLoss: 0.5636292099952698 - trainLoss: 0.5395085215568542\n",
      "cnt: 0 - valLoss: 0.5636193752288818 - trainLoss: 0.5394939184188843\n",
      "cnt: 0 - valLoss: 0.5636101365089417 - trainLoss: 0.5394792556762695\n",
      "cnt: 0 - valLoss: 0.5636005401611328 - trainLoss: 0.5394645929336548\n",
      "cnt: 0 - valLoss: 0.563592255115509 - trainLoss: 0.5394500494003296\n",
      "cnt: 0 - valLoss: 0.5635842680931091 - trainLoss: 0.5394354462623596\n",
      "cnt: 0 - valLoss: 0.5635766983032227 - trainLoss: 0.5394208431243896\n",
      "cnt: 0 - valLoss: 0.5635685324668884 - trainLoss: 0.5394061803817749\n",
      "cnt: 0 - valLoss: 0.5635615587234497 - trainLoss: 0.5393916368484497\n",
      "cnt: 0 - valLoss: 0.5635548233985901 - trainLoss: 0.5393770337104797\n",
      "cnt: 0 - valLoss: 0.5635475516319275 - trainLoss: 0.5393624305725098\n",
      "cnt: 0 - valLoss: 0.5635411739349365 - trainLoss: 0.5393478870391846\n",
      "cnt: 0 - valLoss: 0.5635350942611694 - trainLoss: 0.5393332839012146\n",
      "cnt: 0 - valLoss: 0.5635283589363098 - trainLoss: 0.5393187403678894\n",
      "cnt: 0 - valLoss: 0.5635225772857666 - trainLoss: 0.5393041372299194\n",
      "cnt: 0 - valLoss: 0.5635161399841309 - trainLoss: 0.5392895936965942\n",
      "cnt: 0 - valLoss: 0.5635106563568115 - trainLoss: 0.5392749905586243\n",
      "cnt: 0 - valLoss: 0.5635052919387817 - trainLoss: 0.5392604470252991\n",
      "cnt: 0 - valLoss: 0.5634992122650146 - trainLoss: 0.5392459034919739\n",
      "cnt: 0 - valLoss: 0.5634428858757019 - trainLoss: 0.5392313003540039\n",
      "cnt: 0 - valLoss: 0.5634430646896362 - trainLoss: 0.5392172336578369\n",
      "cnt: 1 - valLoss: 0.5634427070617676 - trainLoss: 0.5392026305198669\n",
      "cnt: 0 - valLoss: 0.5634418725967407 - trainLoss: 0.5391880869865417\n",
      "cnt: 0 - valLoss: 0.56344074010849 - trainLoss: 0.5391735434532166\n",
      "cnt: 0 - valLoss: 0.5634384155273438 - trainLoss: 0.5391589999198914\n",
      "cnt: 0 - valLoss: 0.563385546207428 - trainLoss: 0.5391443967819214\n",
      "cnt: 0 - valLoss: 0.5633887052536011 - trainLoss: 0.5391305685043335\n",
      "cnt: 1 - valLoss: 0.5633910894393921 - trainLoss: 0.5391159057617188\n",
      "cnt: 2 - valLoss: 0.563392698764801 - trainLoss: 0.5391013026237488\n",
      "cnt: 3 - valLoss: 0.5633933544158936 - trainLoss: 0.5390866994857788\n",
      "cnt: 4 - valLoss: 0.5633423924446106 - trainLoss: 0.5390725135803223\n",
      "cnt: 0 - valLoss: 0.5633476972579956 - trainLoss: 0.5390583872795105\n",
      "cnt: 1 - valLoss: 0.5633518099784851 - trainLoss: 0.5390437841415405\n",
      "cnt: 2 - valLoss: 0.5633549690246582 - trainLoss: 0.5390291213989258\n",
      "cnt: 3 - valLoss: 0.5633059144020081 - trainLoss: 0.539014995098114\n",
      "cnt: 0 - valLoss: 0.5633130073547363 - trainLoss: 0.5390008687973022\n",
      "cnt: 1 - valLoss: 0.5633190274238586 - trainLoss: 0.5389862060546875\n",
      "cnt: 2 - valLoss: 0.5632718801498413 - trainLoss: 0.5389716029167175\n",
      "cnt: 0 - valLoss: 0.5632810592651367 - trainLoss: 0.538957953453064\n",
      "cnt: 1 - valLoss: 0.5632889270782471 - trainLoss: 0.5389432311058044\n",
      "cnt: 2 - valLoss: 0.5632443428039551 - trainLoss: 0.5389286279678345\n",
      "cnt: 0 - valLoss: 0.5632549524307251 - trainLoss: 0.5389149785041809\n",
      "cnt: 1 - valLoss: 0.5632644295692444 - trainLoss: 0.5389002561569214\n",
      "cnt: 2 - valLoss: 0.5632198452949524 - trainLoss: 0.5388860106468201\n",
      "cnt: 0 - valLoss: 0.5632314682006836 - trainLoss: 0.5388721227645874\n",
      "cnt: 1 - valLoss: 0.5632426738739014 - trainLoss: 0.5388572812080383\n",
      "cnt: 2 - valLoss: 0.5631983280181885 - trainLoss: 0.5388433933258057\n",
      "cnt: 0 - valLoss: 0.5632074475288391 - trainLoss: 0.5388290882110596\n",
      "cnt: 1 - valLoss: 0.5632167458534241 - trainLoss: 0.5388143062591553\n",
      "cnt: 2 - valLoss: 0.5631700754165649 - trainLoss: 0.5388006567955017\n",
      "cnt: 0 - valLoss: 0.5631811022758484 - trainLoss: 0.5387860536575317\n",
      "cnt: 1 - valLoss: 0.5631390810012817 - trainLoss: 0.538771390914917\n",
      "cnt: 0 - valLoss: 0.5631520748138428 - trainLoss: 0.5387577414512634\n",
      "cnt: 1 - valLoss: 0.563164472579956 - trainLoss: 0.5387428402900696\n",
      "cnt: 2 - valLoss: 0.5631210803985596 - trainLoss: 0.5387290120124817\n",
      "cnt: 0 - valLoss: 0.563135027885437 - trainLoss: 0.538714587688446\n",
      "cnt: 1 - valLoss: 0.563095211982727 - trainLoss: 0.5387000441551208\n",
      "cnt: 0 - valLoss: 0.5631105303764343 - trainLoss: 0.538686215877533\n",
      "cnt: 1 - valLoss: 0.5630727410316467 - trainLoss: 0.5386713743209839\n",
      "cnt: 0 - valLoss: 0.5630892515182495 - trainLoss: 0.5386578440666199\n",
      "cnt: 1 - valLoss: 0.5630519986152649 - trainLoss: 0.5386428833007812\n",
      "cnt: 0 - valLoss: 0.5630698204040527 - trainLoss: 0.5386295318603516\n",
      "cnt: 1 - valLoss: 0.5630344152450562 - trainLoss: 0.5386143922805786\n",
      "cnt: 0 - valLoss: 0.5630533695220947 - trainLoss: 0.5386010408401489\n",
      "cnt: 1 - valLoss: 0.563017725944519 - trainLoss: 0.5385861396789551\n",
      "cnt: 0 - valLoss: 0.5630376935005188 - trainLoss: 0.5385726094245911\n",
      "cnt: 1 - valLoss: 0.5630033016204834 - trainLoss: 0.5385578870773315\n",
      "cnt: 0 - valLoss: 0.5630242824554443 - trainLoss: 0.5385441184043884\n",
      "cnt: 1 - valLoss: 0.5629891157150269 - trainLoss: 0.5385296940803528\n",
      "cnt: 0 - valLoss: 0.562964141368866 - trainLoss: 0.5385154485702515\n",
      "cnt: 0 - valLoss: 0.5629413723945618 - trainLoss: 0.5385003685951233\n",
      "cnt: 0 - valLoss: 0.5629206299781799 - trainLoss: 0.5384853482246399\n",
      "cnt: 0 - valLoss: 0.5629016160964966 - trainLoss: 0.538470447063446\n",
      "cnt: 0 - valLoss: 0.5628840923309326 - trainLoss: 0.5384556651115417\n",
      "cnt: 0 - valLoss: 0.5628679394721985 - trainLoss: 0.5384408235549927\n",
      "cnt: 0 - valLoss: 0.5628530383110046 - trainLoss: 0.5384261012077332\n",
      "cnt: 0 - valLoss: 0.5628392100334167 - trainLoss: 0.5384113192558289\n",
      "cnt: 0 - valLoss: 0.56276935338974 - trainLoss: 0.5383962988853455\n",
      "cnt: 0 - valLoss: 0.5627070069313049 - trainLoss: 0.5383763313293457\n",
      "cnt: 0 - valLoss: 0.5626513361930847 - trainLoss: 0.5383573174476624\n",
      "cnt: 0 - valLoss: 0.5626013278961182 - trainLoss: 0.5383392572402954\n",
      "cnt: 0 - valLoss: 0.5625563859939575 - trainLoss: 0.538321852684021\n",
      "cnt: 0 - valLoss: 0.5625178217887878 - trainLoss: 0.5383051037788391\n",
      "cnt: 0 - valLoss: 0.5624830722808838 - trainLoss: 0.5382888317108154\n",
      "cnt: 0 - valLoss: 0.562451183795929 - trainLoss: 0.5382727384567261\n",
      "cnt: 0 - valLoss: 0.5624211430549622 - trainLoss: 0.5382568836212158\n",
      "cnt: 0 - valLoss: 0.5623937845230103 - trainLoss: 0.5382412075996399\n",
      "cnt: 0 - valLoss: 0.5623689889907837 - trainLoss: 0.5382256507873535\n",
      "cnt: 0 - valLoss: 0.5623463988304138 - trainLoss: 0.5382102131843567\n",
      "cnt: 0 - valLoss: 0.562325656414032 - trainLoss: 0.538195013999939\n",
      "cnt: 0 - valLoss: 0.5623067617416382 - trainLoss: 0.5381797552108765\n",
      "cnt: 0 - valLoss: 0.5622892379760742 - trainLoss: 0.5381646156311035\n",
      "cnt: 0 - valLoss: 0.5622732043266296 - trainLoss: 0.5381495356559753\n",
      "cnt: 0 - valLoss: 0.5622584223747253 - trainLoss: 0.5381344556808472\n",
      "cnt: 0 - valLoss: 0.5622444748878479 - trainLoss: 0.5381194353103638\n",
      "cnt: 0 - valLoss: 0.5622314214706421 - trainLoss: 0.5381044745445251\n",
      "cnt: 0 - valLoss: 0.5622209906578064 - trainLoss: 0.5380895137786865\n",
      "cnt: 0 - valLoss: 0.5622110962867737 - trainLoss: 0.5380746126174927\n",
      "cnt: 0 - valLoss: 0.5622016191482544 - trainLoss: 0.5380597114562988\n",
      "cnt: 0 - valLoss: 0.5621917247772217 - trainLoss: 0.5380447506904602\n",
      "cnt: 0 - valLoss: 0.5621823072433472 - trainLoss: 0.5380298495292664\n",
      "cnt: 0 - valLoss: 0.5621733069419861 - trainLoss: 0.5380148887634277\n",
      "cnt: 0 - valLoss: 0.562164843082428 - trainLoss: 0.5379999876022339\n",
      "cnt: 0 - valLoss: 0.5621566772460938 - trainLoss: 0.53798508644104\n",
      "cnt: 0 - valLoss: 0.5621512532234192 - trainLoss: 0.5379701852798462\n",
      "cnt: 0 - valLoss: 0.5621442198753357 - trainLoss: 0.5379554033279419\n",
      "cnt: 0 - valLoss: 0.5621375441551208 - trainLoss: 0.5379405617713928\n",
      "cnt: 0 - valLoss: 0.5621308088302612 - trainLoss: 0.5379257798194885\n",
      "cnt: 0 - valLoss: 0.5621244311332703 - trainLoss: 0.5379109382629395\n",
      "cnt: 0 - valLoss: 0.5621181726455688 - trainLoss: 0.5378962159156799\n",
      "cnt: 0 - valLoss: 0.5621119141578674 - trainLoss: 0.5378814339637756\n",
      "cnt: 0 - valLoss: 0.5621048212051392 - trainLoss: 0.5378665924072266\n",
      "cnt: 0 - valLoss: 0.5620978474617004 - trainLoss: 0.5378517508506775\n",
      "cnt: 0 - valLoss: 0.5620911121368408 - trainLoss: 0.5378369092941284\n",
      "cnt: 0 - valLoss: 0.5620845556259155 - trainLoss: 0.5378221869468689\n",
      "cnt: 0 - valLoss: 0.5620781779289246 - trainLoss: 0.5378074049949646\n",
      "cnt: 0 - valLoss: 0.5620711445808411 - trainLoss: 0.5377925634384155\n",
      "cnt: 0 - valLoss: 0.5620643496513367 - trainLoss: 0.5377777814865112\n",
      "cnt: 0 - valLoss: 0.5620577335357666 - trainLoss: 0.5377630591392517\n",
      "cnt: 0 - valLoss: 0.5620512366294861 - trainLoss: 0.5377482771873474\n",
      "cnt: 0 - valLoss: 0.5620455741882324 - trainLoss: 0.5377334952354431\n",
      "cnt: 0 - valLoss: 0.5620399117469788 - trainLoss: 0.5377187728881836\n",
      "cnt: 0 - valLoss: 0.5620335936546326 - trainLoss: 0.5377041101455688\n",
      "cnt: 0 - valLoss: 0.5620273947715759 - trainLoss: 0.5376892685890198\n",
      "cnt: 0 - valLoss: 0.5620214343070984 - trainLoss: 0.5376745462417603\n",
      "cnt: 0 - valLoss: 0.5620154738426208 - trainLoss: 0.5376598238945007\n",
      "cnt: 0 - valLoss: 0.5620095729827881 - trainLoss: 0.5376451015472412\n",
      "cnt: 0 - valLoss: 0.5620037317276001 - trainLoss: 0.5376303791999817\n",
      "cnt: 0 - valLoss: 0.5619980692863464 - trainLoss: 0.5376155972480774\n",
      "cnt: 0 - valLoss: 0.5619924068450928 - trainLoss: 0.5376009345054626\n",
      "cnt: 0 - valLoss: 0.5619868040084839 - trainLoss: 0.5375862121582031\n",
      "cnt: 0 - valLoss: 0.5619803071022034 - trainLoss: 0.5375714302062988\n",
      "cnt: 0 - valLoss: 0.5619739294052124 - trainLoss: 0.5375567078590393\n",
      "cnt: 0 - valLoss: 0.561967670917511 - trainLoss: 0.5375419855117798\n",
      "cnt: 0 - valLoss: 0.5619615316390991 - trainLoss: 0.5375272035598755\n",
      "cnt: 0 - valLoss: 0.5619555115699768 - trainLoss: 0.537512481212616\n",
      "cnt: 0 - valLoss: 0.5619495511054993 - trainLoss: 0.5374977588653564\n",
      "cnt: 0 - valLoss: 0.561943769454956 - trainLoss: 0.5374830365180969\n",
      "cnt: 0 - valLoss: 0.5619379281997681 - trainLoss: 0.5374683141708374\n",
      "cnt: 0 - valLoss: 0.5619321465492249 - trainLoss: 0.5374535918235779\n",
      "cnt: 0 - valLoss: 0.5619264841079712 - trainLoss: 0.5374388694763184\n",
      "cnt: 0 - valLoss: 0.5619209408760071 - trainLoss: 0.5374241471290588\n",
      "cnt: 0 - valLoss: 0.5619153380393982 - trainLoss: 0.5374094247817993\n",
      "cnt: 0 - valLoss: 0.5619099736213684 - trainLoss: 0.5373947620391846\n",
      "cnt: 0 - valLoss: 0.5619044303894043 - trainLoss: 0.5373799800872803\n",
      "cnt: 0 - valLoss: 0.5618990659713745 - trainLoss: 0.5373653173446655\n",
      "cnt: 0 - valLoss: 0.5618937015533447 - trainLoss: 0.537350594997406\n",
      "cnt: 0 - valLoss: 0.5618883371353149 - trainLoss: 0.5373359322547913\n",
      "cnt: 0 - valLoss: 0.5618830323219299 - trainLoss: 0.5373212695121765\n",
      "cnt: 0 - valLoss: 0.5618777275085449 - trainLoss: 0.537306547164917\n",
      "cnt: 0 - valLoss: 0.5618724822998047 - trainLoss: 0.5372918248176575\n",
      "cnt: 0 - valLoss: 0.5618671774864197 - trainLoss: 0.5372772216796875\n",
      "cnt: 0 - valLoss: 0.5618619322776794 - trainLoss: 0.537262499332428\n",
      "cnt: 0 - valLoss: 0.5618561506271362 - trainLoss: 0.5372478365898132\n",
      "cnt: 0 - valLoss: 0.561850368976593 - trainLoss: 0.5372331738471985\n",
      "cnt: 0 - valLoss: 0.5618447065353394 - trainLoss: 0.5372185111045837\n",
      "cnt: 0 - valLoss: 0.5618383288383484 - trainLoss: 0.5372039079666138\n",
      "cnt: 0 - valLoss: 0.5618329048156738 - trainLoss: 0.537189245223999\n",
      "cnt: 0 - valLoss: 0.5618267059326172 - trainLoss: 0.5371745824813843\n",
      "cnt: 0 - valLoss: 0.5618215203285217 - trainLoss: 0.5371599793434143\n",
      "cnt: 0 - valLoss: 0.5618153810501099 - trainLoss: 0.5371452569961548\n",
      "cnt: 0 - valLoss: 0.5618102550506592 - trainLoss: 0.5371306538581848\n",
      "cnt: 0 - valLoss: 0.5618043541908264 - trainLoss: 0.5371159911155701\n",
      "cnt: 0 - valLoss: 0.5617993474006653 - trainLoss: 0.5371013879776001\n",
      "cnt: 0 - valLoss: 0.5617935657501221 - trainLoss: 0.5370867848396301\n",
      "cnt: 0 - valLoss: 0.5617886185646057 - trainLoss: 0.5370721220970154\n",
      "cnt: 0 - valLoss: 0.5617828965187073 - trainLoss: 0.5370575189590454\n",
      "cnt: 0 - valLoss: 0.5617777109146118 - trainLoss: 0.5370429158210754\n",
      "cnt: 0 - valLoss: 0.5617717504501343 - trainLoss: 0.5370283126831055\n",
      "cnt: 0 - valLoss: 0.5617667436599731 - trainLoss: 0.5370137095451355\n",
      "cnt: 0 - valLoss: 0.5617609620094299 - trainLoss: 0.5369991064071655\n",
      "cnt: 0 - valLoss: 0.561755895614624 - trainLoss: 0.5369845032691956\n",
      "cnt: 0 - valLoss: 0.5617501735687256 - trainLoss: 0.5369699001312256\n",
      "cnt: 0 - valLoss: 0.561745285987854 - trainLoss: 0.5369553565979004\n",
      "cnt: 0 - valLoss: 0.5617396235466003 - trainLoss: 0.5369407534599304\n",
      "cnt: 0 - valLoss: 0.5617347955703735 - trainLoss: 0.5369261503219604\n",
      "cnt: 0 - valLoss: 0.5617292523384094 - trainLoss: 0.5369115471839905\n",
      "cnt: 0 - valLoss: 0.5617244839668274 - trainLoss: 0.5368970036506653\n",
      "cnt: 0 - valLoss: 0.5617190003395081 - trainLoss: 0.5368824601173401\n",
      "cnt: 0 - valLoss: 0.561714231967926 - trainLoss: 0.5368679165840149\n",
      "cnt: 0 - valLoss: 0.5617087483406067 - trainLoss: 0.5368533134460449\n",
      "cnt: 0 - valLoss: 0.561703085899353 - trainLoss: 0.5368387699127197\n",
      "cnt: 0 - valLoss: 0.5616968274116516 - trainLoss: 0.5368241667747498\n",
      "cnt: 0 - valLoss: 0.5616914629936218 - trainLoss: 0.5368095636367798\n",
      "cnt: 0 - valLoss: 0.5616853833198547 - trainLoss: 0.5367950201034546\n",
      "cnt: 0 - valLoss: 0.5616801977157593 - trainLoss: 0.5367804169654846\n",
      "cnt: 0 - valLoss: 0.5616742968559265 - trainLoss: 0.5367658734321594\n",
      "cnt: 0 - valLoss: 0.5616692304611206 - trainLoss: 0.5367512702941895\n",
      "cnt: 0 - valLoss: 0.5616634488105774 - trainLoss: 0.5367367267608643\n",
      "cnt: 0 - valLoss: 0.5616585612297058 - trainLoss: 0.5367221236228943\n",
      "cnt: 0 - valLoss: 0.5616528987884521 - trainLoss: 0.5367076396942139\n",
      "cnt: 0 - valLoss: 0.5616480708122253 - trainLoss: 0.5366930961608887\n",
      "cnt: 0 - valLoss: 0.5616424083709717 - trainLoss: 0.5366785526275635\n",
      "cnt: 0 - valLoss: 0.5616369247436523 - trainLoss: 0.5366638898849487\n",
      "cnt: 0 - valLoss: 0.5616322755813599 - trainLoss: 0.5366494059562683\n",
      "cnt: 0 - valLoss: 0.5616267919540405 - trainLoss: 0.5366348624229431\n",
      "cnt: 0 - valLoss: 0.5616222023963928 - trainLoss: 0.5366203188896179\n",
      "cnt: 0 - valLoss: 0.5616167783737183 - trainLoss: 0.5366058349609375\n",
      "cnt: 0 - valLoss: 0.5616121888160706 - trainLoss: 0.5365912914276123\n",
      "cnt: 0 - valLoss: 0.561606228351593 - trainLoss: 0.5365767478942871\n",
      "cnt: 0 - valLoss: 0.5616011023521423 - trainLoss: 0.5365622043609619\n",
      "cnt: 0 - valLoss: 0.5615952610969543 - trainLoss: 0.5365476608276367\n",
      "cnt: 0 - valLoss: 0.5615903735160828 - trainLoss: 0.5365331172943115\n",
      "cnt: 0 - valLoss: 0.5615846514701843 - trainLoss: 0.5365186929702759\n",
      "cnt: 0 - valLoss: 0.5615790486335754 - trainLoss: 0.5365040898323059\n",
      "cnt: 0 - valLoss: 0.5615742802619934 - trainLoss: 0.5364896059036255\n",
      "cnt: 0 - valLoss: 0.5615687966346741 - trainLoss: 0.5364751219749451\n",
      "cnt: 0 - valLoss: 0.5615640878677368 - trainLoss: 0.5364605784416199\n",
      "cnt: 0 - valLoss: 0.5615586042404175 - trainLoss: 0.5364460349082947\n",
      "cnt: 0 - valLoss: 0.561553955078125 - trainLoss: 0.5364315509796143\n",
      "cnt: 0 - valLoss: 0.5615487098693848 - trainLoss: 0.5364170074462891\n",
      "cnt: 0 - valLoss: 0.5614943504333496 - trainLoss: 0.5364026427268982\n",
      "cnt: 0 - valLoss: 0.5614956021308899 - trainLoss: 0.5363885760307312\n",
      "cnt: 1 - valLoss: 0.5614960193634033 - trainLoss: 0.536374032497406\n",
      "cnt: 2 - valLoss: 0.5614959001541138 - trainLoss: 0.5363595485687256\n",
      "cnt: 3 - valLoss: 0.5614944696426392 - trainLoss: 0.5363450050354004\n",
      "cnt: 4 - valLoss: 0.5614930987358093 - trainLoss: 0.53633052110672\n",
      "cnt: 0 - valLoss: 0.5614408850669861 - trainLoss: 0.5363163352012634\n",
      "cnt: 0 - valLoss: 0.5614449381828308 - trainLoss: 0.5363022685050964\n",
      "cnt: 1 - valLoss: 0.5614479184150696 - trainLoss: 0.5362877249717712\n",
      "cnt: 2 - valLoss: 0.5614503026008606 - trainLoss: 0.536273181438446\n",
      "cnt: 3 - valLoss: 0.5614011883735657 - trainLoss: 0.5362587571144104\n",
      "cnt: 0 - valLoss: 0.5614075660705566 - trainLoss: 0.5362450480461121\n",
      "cnt: 1 - valLoss: 0.5614127516746521 - trainLoss: 0.5362304449081421\n",
      "cnt: 2 - valLoss: 0.5614174604415894 - trainLoss: 0.5362158417701721\n",
      "cnt: 3 - valLoss: 0.5613693594932556 - trainLoss: 0.5362018346786499\n",
      "cnt: 0 - valLoss: 0.5613772869110107 - trainLoss: 0.5361878275871277\n",
      "cnt: 1 - valLoss: 0.5613839030265808 - trainLoss: 0.5361731052398682\n",
      "cnt: 2 - valLoss: 0.5613384246826172 - trainLoss: 0.5361588001251221\n",
      "cnt: 0 - valLoss: 0.5613483190536499 - trainLoss: 0.5361451506614685\n",
      "cnt: 1 - valLoss: 0.5613569021224976 - trainLoss: 0.536130428314209\n",
      "cnt: 2 - valLoss: 0.5613126754760742 - trainLoss: 0.5361161828041077\n",
      "cnt: 0 - valLoss: 0.5613237023353577 - trainLoss: 0.5361024141311646\n",
      "cnt: 1 - valLoss: 0.5613332390785217 - trainLoss: 0.536087691783905\n",
      "cnt: 2 - valLoss: 0.5612897276878357 - trainLoss: 0.5360738039016724\n",
      "cnt: 0 - valLoss: 0.5613017678260803 - trainLoss: 0.5360596776008606\n",
      "cnt: 1 - valLoss: 0.5612626671791077 - trainLoss: 0.5360449552536011\n",
      "cnt: 0 - valLoss: 0.5612767934799194 - trainLoss: 0.5360317230224609\n",
      "cnt: 1 - valLoss: 0.5612893104553223 - trainLoss: 0.5360168218612671\n",
      "cnt: 2 - valLoss: 0.5612479448318481 - trainLoss: 0.5360031127929688\n",
      "cnt: 0 - valLoss: 0.561262845993042 - trainLoss: 0.5359888672828674\n",
      "cnt: 1 - valLoss: 0.5612248778343201 - trainLoss: 0.5359746217727661\n",
      "cnt: 0 - valLoss: 0.5612412095069885 - trainLoss: 0.535960853099823\n",
      "cnt: 1 - valLoss: 0.5612032413482666 - trainLoss: 0.5359463095664978\n",
      "cnt: 0 - valLoss: 0.5609645247459412 - trainLoss: 0.5359321236610413\n",
      "cnt: 0 - valLoss: 0.5607597231864929 - trainLoss: 0.5358776450157166\n",
      "cnt: 0 - valLoss: 0.5605840086936951 - trainLoss: 0.5358328819274902\n",
      "cnt: 0 - valLoss: 0.5604201555252075 - trainLoss: 0.5357938408851624\n",
      "cnt: 0 - valLoss: 0.5602787137031555 - trainLoss: 0.5357584953308105\n",
      "cnt: 0 - valLoss: 0.5600795149803162 - trainLoss: 0.535721480846405\n",
      "cnt: 0 - valLoss: 0.559907078742981 - trainLoss: 0.5356733202934265\n",
      "cnt: 0 - valLoss: 0.5597583055496216 - trainLoss: 0.5356326699256897\n",
      "cnt: 0 - valLoss: 0.5596305131912231 - trainLoss: 0.5355978608131409\n",
      "cnt: 0 - valLoss: 0.5595194697380066 - trainLoss: 0.5355677604675293\n",
      "cnt: 0 - valLoss: 0.559467077255249 - trainLoss: 0.5355424284934998\n",
      "cnt: 0 - valLoss: 0.5594215393066406 - trainLoss: 0.5355243682861328\n",
      "cnt: 0 - valLoss: 0.5593834519386292 - trainLoss: 0.5355069041252136\n",
      "cnt: 0 - valLoss: 0.5593494176864624 - trainLoss: 0.5354899764060974\n",
      "cnt: 0 - valLoss: 0.5593190789222717 - trainLoss: 0.5354732275009155\n",
      "cnt: 0 - valLoss: 0.5592913627624512 - trainLoss: 0.5354567766189575\n",
      "cnt: 0 - valLoss: 0.559265673160553 - trainLoss: 0.5354403853416443\n",
      "cnt: 0 - valLoss: 0.5592424273490906 - trainLoss: 0.535423994064331\n",
      "cnt: 0 - valLoss: 0.5592213869094849 - trainLoss: 0.5354077816009521\n",
      "cnt: 0 - valLoss: 0.5592021346092224 - trainLoss: 0.5353916883468628\n",
      "cnt: 0 - valLoss: 0.5591845512390137 - trainLoss: 0.5353755950927734\n",
      "cnt: 0 - valLoss: 0.5591683387756348 - trainLoss: 0.5353595018386841\n",
      "cnt: 0 - valLoss: 0.5591533780097961 - trainLoss: 0.5353434085845947\n",
      "cnt: 0 - valLoss: 0.5591393709182739 - trainLoss: 0.5353274345397949\n",
      "cnt: 0 - valLoss: 0.5591269135475159 - trainLoss: 0.5353114604949951\n",
      "cnt: 0 - valLoss: 0.5591158866882324 - trainLoss: 0.5352955460548401\n",
      "cnt: 0 - valLoss: 0.5591055154800415 - trainLoss: 0.5352796912193298\n",
      "cnt: 0 - valLoss: 0.5590946674346924 - trainLoss: 0.5352638363838196\n",
      "cnt: 0 - valLoss: 0.5590839385986328 - trainLoss: 0.5352479219436646\n",
      "cnt: 0 - valLoss: 0.5590736865997314 - trainLoss: 0.5352320671081543\n",
      "cnt: 0 - valLoss: 0.5590639114379883 - trainLoss: 0.5352162718772888\n",
      "cnt: 0 - valLoss: 0.5590544939041138 - trainLoss: 0.5352004170417786\n",
      "cnt: 0 - valLoss: 0.5590451955795288 - trainLoss: 0.5351845622062683\n",
      "cnt: 0 - valLoss: 0.5590360164642334 - trainLoss: 0.5351687669754028\n",
      "cnt: 0 - valLoss: 0.5590271949768066 - trainLoss: 0.5351529121398926\n",
      "cnt: 0 - valLoss: 0.5590185523033142 - trainLoss: 0.5351370573043823\n",
      "cnt: 0 - valLoss: 0.5590102076530457 - trainLoss: 0.5351212620735168\n",
      "cnt: 0 - valLoss: 0.5590015649795532 - trainLoss: 0.5351054072380066\n",
      "cnt: 0 - valLoss: 0.5589931011199951 - trainLoss: 0.5350895524024963\n",
      "cnt: 0 - valLoss: 0.5589849352836609 - trainLoss: 0.5350738167762756\n",
      "cnt: 0 - valLoss: 0.5589768886566162 - trainLoss: 0.5350580215454102\n",
      "cnt: 0 - valLoss: 0.5589686036109924 - trainLoss: 0.5350421667098999\n",
      "cnt: 0 - valLoss: 0.5589605569839478 - trainLoss: 0.5350263714790344\n",
      "cnt: 0 - valLoss: 0.5589526891708374 - trainLoss: 0.535010576248169\n",
      "cnt: 0 - valLoss: 0.5589449405670166 - trainLoss: 0.5349947810173035\n",
      "cnt: 0 - valLoss: 0.5589373111724854 - trainLoss: 0.534978985786438\n",
      "cnt: 0 - valLoss: 0.5589297413825989 - trainLoss: 0.5349631309509277\n",
      "cnt: 0 - valLoss: 0.5589222311973572 - trainLoss: 0.534947395324707\n",
      "cnt: 0 - valLoss: 0.5589148998260498 - trainLoss: 0.5349316000938416\n",
      "cnt: 0 - valLoss: 0.558907687664032 - trainLoss: 0.5349158048629761\n",
      "cnt: 0 - valLoss: 0.5589003562927246 - trainLoss: 0.5349000096321106\n",
      "cnt: 0 - valLoss: 0.5588932633399963 - trainLoss: 0.5348842740058899\n",
      "cnt: 0 - valLoss: 0.5588861107826233 - trainLoss: 0.5348684787750244\n",
      "cnt: 0 - valLoss: 0.558879017829895 - trainLoss: 0.5348528027534485\n",
      "cnt: 0 - valLoss: 0.5588719844818115 - trainLoss: 0.534837007522583\n",
      "cnt: 0 - valLoss: 0.558864951133728 - trainLoss: 0.5348212718963623\n",
      "cnt: 0 - valLoss: 0.5588579773902893 - trainLoss: 0.5348055362701416\n",
      "cnt: 0 - valLoss: 0.5588510632514954 - trainLoss: 0.5347898006439209\n",
      "cnt: 0 - valLoss: 0.5588442087173462 - trainLoss: 0.534774124622345\n",
      "cnt: 0 - valLoss: 0.5588372945785522 - trainLoss: 0.5347583889961243\n",
      "cnt: 0 - valLoss: 0.5588304400444031 - trainLoss: 0.5347425937652588\n",
      "cnt: 0 - valLoss: 0.5588236451148987 - trainLoss: 0.5347269177436829\n",
      "cnt: 0 - valLoss: 0.5588163137435913 - trainLoss: 0.5347112417221069\n",
      "cnt: 0 - valLoss: 0.5588091015815735 - trainLoss: 0.5346955060958862\n",
      "cnt: 0 - valLoss: 0.5588019490242004 - trainLoss: 0.5346798300743103\n",
      "cnt: 0 - valLoss: 0.5587949156761169 - trainLoss: 0.5346640944480896\n",
      "cnt: 0 - valLoss: 0.5587879419326782 - trainLoss: 0.5346483588218689\n",
      "cnt: 0 - valLoss: 0.5587809681892395 - trainLoss: 0.534632682800293\n",
      "cnt: 0 - valLoss: 0.5587740540504456 - trainLoss: 0.5346169471740723\n",
      "cnt: 0 - valLoss: 0.5587671399116516 - trainLoss: 0.5346013307571411\n",
      "cnt: 0 - valLoss: 0.5587602853775024 - trainLoss: 0.5345855951309204\n",
      "cnt: 0 - valLoss: 0.558753252029419 - trainLoss: 0.5345699787139893\n",
      "cnt: 0 - valLoss: 0.558746337890625 - trainLoss: 0.5345543026924133\n",
      "cnt: 0 - valLoss: 0.558739423751831 - trainLoss: 0.5345386266708374\n",
      "cnt: 0 - valLoss: 0.5587325096130371 - trainLoss: 0.5345230102539062\n",
      "cnt: 0 - valLoss: 0.5587257146835327 - trainLoss: 0.5345073342323303\n",
      "cnt: 0 - valLoss: 0.5587189197540283 - trainLoss: 0.5344917178153992\n",
      "cnt: 0 - valLoss: 0.5587121248245239 - trainLoss: 0.5344760417938232\n",
      "cnt: 0 - valLoss: 0.5587054491043091 - trainLoss: 0.5344603657722473\n",
      "cnt: 0 - valLoss: 0.5586987137794495 - trainLoss: 0.5344448089599609\n",
      "cnt: 0 - valLoss: 0.5586917996406555 - trainLoss: 0.534429132938385\n",
      "cnt: 0 - valLoss: 0.5586849451065063 - trainLoss: 0.5344135165214539\n",
      "cnt: 0 - valLoss: 0.558678150177002 - trainLoss: 0.5343979597091675\n",
      "cnt: 0 - valLoss: 0.5586713552474976 - trainLoss: 0.5343822836875916\n",
      "cnt: 0 - valLoss: 0.5586646199226379 - trainLoss: 0.5343666672706604\n",
      "cnt: 0 - valLoss: 0.5586578845977783 - trainLoss: 0.534351110458374\n",
      "cnt: 0 - valLoss: 0.5586512088775635 - trainLoss: 0.5343355536460876\n",
      "cnt: 0 - valLoss: 0.5586444735527039 - trainLoss: 0.5343199372291565\n",
      "cnt: 0 - valLoss: 0.5586379170417786 - trainLoss: 0.5343043804168701\n",
      "cnt: 0 - valLoss: 0.5586313605308533 - trainLoss: 0.534288763999939\n",
      "cnt: 0 - valLoss: 0.5586246252059937 - trainLoss: 0.5342732667922974\n",
      "cnt: 0 - valLoss: 0.5585429668426514 - trainLoss: 0.5342575907707214\n",
      "cnt: 0 - valLoss: 0.5585170984268188 - trainLoss: 0.5342374444007874\n",
      "cnt: 0 - valLoss: 0.5584940314292908 - trainLoss: 0.5342212319374084\n",
      "cnt: 0 - valLoss: 0.558474600315094 - trainLoss: 0.5342051386833191\n",
      "cnt: 0 - valLoss: 0.5584571361541748 - trainLoss: 0.5341892242431641\n",
      "cnt: 0 - valLoss: 0.5584419369697571 - trainLoss: 0.534173309803009\n",
      "cnt: 0 - valLoss: 0.5584279894828796 - trainLoss: 0.5341575145721436\n",
      "cnt: 0 - valLoss: 0.5584150552749634 - trainLoss: 0.5341416597366333\n",
      "cnt: 0 - valLoss: 0.558402955532074 - trainLoss: 0.5341259241104126\n",
      "cnt: 0 - valLoss: 0.5583915710449219 - trainLoss: 0.5341101288795471\n",
      "cnt: 0 - valLoss: 0.5583809614181519 - trainLoss: 0.5340943336486816\n",
      "cnt: 0 - valLoss: 0.5583708882331848 - trainLoss: 0.5340786576271057\n",
      "cnt: 0 - valLoss: 0.5583614110946655 - trainLoss: 0.5340628623962402\n",
      "cnt: 0 - valLoss: 0.5583523511886597 - trainLoss: 0.5340472459793091\n",
      "cnt: 0 - valLoss: 0.5583456158638 - trainLoss: 0.5340314507484436\n",
      "cnt: 0 - valLoss: 0.5583389401435852 - trainLoss: 0.5340158939361572\n",
      "cnt: 0 - valLoss: 0.5583322644233704 - trainLoss: 0.5340002775192261\n",
      "cnt: 0 - valLoss: 0.5583257079124451 - trainLoss: 0.5339846014976501\n",
      "cnt: 0 - valLoss: 0.5583191514015198 - trainLoss: 0.5339690446853638\n",
      "cnt: 0 - valLoss: 0.5583139657974243 - trainLoss: 0.5339534282684326\n",
      "cnt: 0 - valLoss: 0.5583085417747498 - trainLoss: 0.5339378714561462\n",
      "cnt: 0 - valLoss: 0.5583029985427856 - trainLoss: 0.5339222550392151\n",
      "cnt: 0 - valLoss: 0.558297336101532 - trainLoss: 0.5339066982269287\n",
      "cnt: 0 - valLoss: 0.5582916140556335 - trainLoss: 0.5338911414146423\n",
      "cnt: 0 - valLoss: 0.5582858920097351 - trainLoss: 0.5338756442070007\n",
      "cnt: 0 - valLoss: 0.5582799911499023 - trainLoss: 0.5338600277900696\n",
      "cnt: 0 - valLoss: 0.5582740902900696 - trainLoss: 0.533844530582428\n",
      "cnt: 0 - valLoss: 0.558268129825592 - trainLoss: 0.5338290333747864\n",
      "cnt: 0 - valLoss: 0.5582618117332458 - trainLoss: 0.5338134765625\n",
      "cnt: 0 - valLoss: 0.5582555532455444 - trainLoss: 0.5337979197502136\n",
      "cnt: 0 - valLoss: 0.5582492351531982 - trainLoss: 0.533782422542572\n",
      "cnt: 0 - valLoss: 0.5582429766654968 - trainLoss: 0.5337669253349304\n",
      "cnt: 0 - valLoss: 0.5582358241081238 - trainLoss: 0.533751368522644\n",
      "cnt: 0 - valLoss: 0.5582297444343567 - trainLoss: 0.5337358117103577\n",
      "cnt: 0 - valLoss: 0.5582226514816284 - trainLoss: 0.5337204337120056\n",
      "cnt: 0 - valLoss: 0.5582157969474792 - trainLoss: 0.533704936504364\n",
      "cnt: 0 - valLoss: 0.5582089424133301 - trainLoss: 0.5336894392967224\n",
      "cnt: 0 - valLoss: 0.5582021474838257 - trainLoss: 0.5336739420890808\n",
      "cnt: 0 - valLoss: 0.5581963062286377 - trainLoss: 0.533658504486084\n",
      "cnt: 0 - valLoss: 0.5581897497177124 - trainLoss: 0.5336430668830872\n",
      "cnt: 0 - valLoss: 0.5581830739974976 - trainLoss: 0.5336276292800903\n",
      "cnt: 0 - valLoss: 0.5581765174865723 - trainLoss: 0.5336121320724487\n",
      "cnt: 0 - valLoss: 0.558169960975647 - trainLoss: 0.5335967540740967\n",
      "cnt: 0 - valLoss: 0.5581643581390381 - trainLoss: 0.5335812568664551\n",
      "cnt: 0 - valLoss: 0.5581578612327576 - trainLoss: 0.5335658192634583\n",
      "cnt: 0 - valLoss: 0.558151364326477 - trainLoss: 0.5335504412651062\n",
      "cnt: 0 - valLoss: 0.5581449270248413 - trainLoss: 0.5335350036621094\n",
      "cnt: 0 - valLoss: 0.5581384897232056 - trainLoss: 0.5335196256637573\n",
      "cnt: 0 - valLoss: 0.5581321716308594 - trainLoss: 0.5335041880607605\n",
      "cnt: 0 - valLoss: 0.5581267476081848 - trainLoss: 0.5334888100624084\n",
      "cnt: 0 - valLoss: 0.5581200122833252 - trainLoss: 0.5334734320640564\n",
      "cnt: 0 - valLoss: 0.5581135153770447 - trainLoss: 0.5334579944610596\n",
      "cnt: 0 - valLoss: 0.5581068396568298 - trainLoss: 0.5334426164627075\n",
      "cnt: 0 - valLoss: 0.5581000447273254 - trainLoss: 0.5334272384643555\n",
      "cnt: 0 - valLoss: 0.5580934882164001 - trainLoss: 0.5334118604660034\n",
      "cnt: 0 - valLoss: 0.5580878257751465 - trainLoss: 0.5333964824676514\n",
      "cnt: 0 - valLoss: 0.5580812096595764 - trainLoss: 0.5333811044692993\n",
      "cnt: 0 - valLoss: 0.5580747127532959 - trainLoss: 0.533365786075592\n",
      "cnt: 0 - valLoss: 0.5580682754516602 - trainLoss: 0.53335040807724\n",
      "cnt: 0 - valLoss: 0.5580618977546692 - trainLoss: 0.5333350896835327\n",
      "cnt: 0 - valLoss: 0.5580555200576782 - trainLoss: 0.5333197116851807\n",
      "cnt: 0 - valLoss: 0.5580489039421082 - trainLoss: 0.5333043932914734\n",
      "cnt: 0 - valLoss: 0.5580432415008545 - trainLoss: 0.5332890748977661\n",
      "cnt: 0 - valLoss: 0.5580366253852844 - trainLoss: 0.5332736968994141\n",
      "cnt: 0 - valLoss: 0.5580300092697144 - trainLoss: 0.5332584381103516\n",
      "cnt: 0 - valLoss: 0.5580236315727234 - trainLoss: 0.5332430601119995\n",
      "cnt: 0 - valLoss: 0.558017373085022 - trainLoss: 0.5332277417182922\n",
      "cnt: 0 - valLoss: 0.5580109357833862 - trainLoss: 0.5332124829292297\n",
      "cnt: 0 - valLoss: 0.5580046772956848 - trainLoss: 0.5331971049308777\n",
      "cnt: 0 - valLoss: 0.5579994320869446 - trainLoss: 0.5331817865371704\n",
      "cnt: 0 - valLoss: 0.5579931139945984 - trainLoss: 0.5331665277481079\n",
      "cnt: 0 - valLoss: 0.557986855506897 - trainLoss: 0.5331512689590454\n",
      "cnt: 0 - valLoss: 0.5579806566238403 - trainLoss: 0.5331360101699829\n",
      "cnt: 0 - valLoss: 0.5579744577407837 - trainLoss: 0.5331206917762756\n",
      "cnt: 0 - valLoss: 0.5579683184623718 - trainLoss: 0.5331054329872131\n",
      "cnt: 0 - valLoss: 0.55796217918396 - trainLoss: 0.5330901145935059\n",
      "cnt: 0 - valLoss: 0.5579561591148376 - trainLoss: 0.5330749154090881\n",
      "cnt: 0 - valLoss: 0.5579500794410706 - trainLoss: 0.5330595970153809\n",
      "cnt: 0 - valLoss: 0.5579442977905273 - trainLoss: 0.5330443382263184\n",
      "cnt: 0 - valLoss: 0.5579376816749573 - trainLoss: 0.5330290794372559\n",
      "cnt: 0 - valLoss: 0.557931125164032 - trainLoss: 0.5330137610435486\n",
      "cnt: 0 - valLoss: 0.5579246282577515 - trainLoss: 0.5329985618591309\n",
      "cnt: 0 - valLoss: 0.5579182505607605 - trainLoss: 0.5329833030700684\n",
      "cnt: 0 - valLoss: 0.5579119324684143 - trainLoss: 0.5329681038856506\n",
      "cnt: 0 - valLoss: 0.5579057335853577 - trainLoss: 0.5329528450965881\n",
      "cnt: 0 - valLoss: 0.557899534702301 - trainLoss: 0.5329376459121704\n",
      "cnt: 0 - valLoss: 0.5578927397727966 - trainLoss: 0.5329224467277527\n",
      "cnt: 0 - valLoss: 0.5578870177268982 - trainLoss: 0.5329071283340454\n",
      "cnt: 0 - valLoss: 0.5578803420066833 - trainLoss: 0.5328918695449829\n",
      "cnt: 0 - valLoss: 0.5578739047050476 - trainLoss: 0.5328766703605652\n",
      "cnt: 0 - valLoss: 0.5578674077987671 - trainLoss: 0.5328614711761475\n",
      "cnt: 0 - valLoss: 0.5578610897064209 - trainLoss: 0.5328462719917297\n",
      "cnt: 0 - valLoss: 0.5578548908233643 - trainLoss: 0.532831072807312\n",
      "cnt: 0 - valLoss: 0.5578486323356628 - trainLoss: 0.5328158736228943\n",
      "cnt: 0 - valLoss: 0.5578425526618958 - trainLoss: 0.5328006148338318\n",
      "cnt: 0 - valLoss: 0.5578354597091675 - trainLoss: 0.5327854752540588\n",
      "cnt: 0 - valLoss: 0.5578280091285706 - trainLoss: 0.5327703356742859\n",
      "cnt: 0 - valLoss: 0.5578216910362244 - trainLoss: 0.5327551364898682\n",
      "cnt: 0 - valLoss: 0.5578145980834961 - trainLoss: 0.5327399373054504\n",
      "cnt: 0 - valLoss: 0.5578077435493469 - trainLoss: 0.5327247977256775\n",
      "cnt: 0 - valLoss: 0.5578010082244873 - trainLoss: 0.5327095985412598\n",
      "cnt: 0 - valLoss: 0.5577943921089172 - trainLoss: 0.5326944589614868\n",
      "cnt: 0 - valLoss: 0.5577879548072815 - trainLoss: 0.5326792597770691\n",
      "cnt: 0 - valLoss: 0.5577815175056458 - trainLoss: 0.5326641201972961\n",
      "cnt: 0 - valLoss: 0.5577753186225891 - trainLoss: 0.5326489806175232\n",
      "cnt: 0 - valLoss: 0.557769775390625 - trainLoss: 0.5326338410377502\n",
      "cnt: 0 - valLoss: 0.5577642917633057 - trainLoss: 0.5326187610626221\n",
      "cnt: 0 - valLoss: 0.5577588081359863 - trainLoss: 0.5326036810874939\n",
      "cnt: 0 - valLoss: 0.5577529668807983 - trainLoss: 0.5325886011123657\n",
      "cnt: 0 - valLoss: 0.5577472448348999 - trainLoss: 0.5325734615325928\n",
      "cnt: 0 - valLoss: 0.5577414035797119 - trainLoss: 0.5325583815574646\n",
      "cnt: 0 - valLoss: 0.557735800743103 - trainLoss: 0.5325433015823364\n",
      "cnt: 0 - valLoss: 0.5577300190925598 - trainLoss: 0.5325281620025635\n",
      "cnt: 0 - valLoss: 0.5577243566513062 - trainLoss: 0.5325131416320801\n",
      "cnt: 0 - valLoss: 0.5577186346054077 - trainLoss: 0.5324980616569519\n",
      "cnt: 0 - valLoss: 0.5577130317687988 - trainLoss: 0.532482922077179\n",
      "cnt: 0 - valLoss: 0.5577073097229004 - trainLoss: 0.5324679613113403\n",
      "cnt: 0 - valLoss: 0.5577017068862915 - trainLoss: 0.5324528813362122\n",
      "cnt: 0 - valLoss: 0.5576960444450378 - trainLoss: 0.5324378609657288\n",
      "cnt: 0 - valLoss: 0.557690441608429 - trainLoss: 0.5324227809906006\n",
      "cnt: 0 - valLoss: 0.5576848387718201 - trainLoss: 0.5324077606201172\n",
      "cnt: 0 - valLoss: 0.5576792359352112 - trainLoss: 0.5323927402496338\n",
      "cnt: 0 - valLoss: 0.557673454284668 - trainLoss: 0.5323777198791504\n",
      "cnt: 0 - valLoss: 0.55766761302948 - trainLoss: 0.532362699508667\n",
      "cnt: 0 - valLoss: 0.557661771774292 - trainLoss: 0.5323476791381836\n",
      "cnt: 0 - valLoss: 0.5576598048210144 - trainLoss: 0.5323326587677002\n",
      "cnt: 0 - valLoss: 0.5576573610305786 - trainLoss: 0.5323178172111511\n",
      "cnt: 0 - valLoss: 0.5576543807983398 - trainLoss: 0.5323029160499573\n",
      "cnt: 0 - valLoss: 0.5576511025428772 - trainLoss: 0.5322879552841187\n",
      "cnt: 0 - valLoss: 0.5576475262641907 - trainLoss: 0.5322731137275696\n",
      "cnt: 0 - valLoss: 0.5576436519622803 - trainLoss: 0.5322582125663757\n",
      "cnt: 0 - valLoss: 0.5576391220092773 - trainLoss: 0.5322433114051819\n",
      "cnt: 0 - valLoss: 0.5576332211494446 - trainLoss: 0.5322285294532776\n",
      "cnt: 0 - valLoss: 0.5576273798942566 - trainLoss: 0.5322136282920837\n",
      "cnt: 0 - valLoss: 0.5576217174530029 - trainLoss: 0.5321987271308899\n",
      "cnt: 0 - valLoss: 0.5576160550117493 - trainLoss: 0.5321839451789856\n",
      "cnt: 0 - valLoss: 0.5576103925704956 - trainLoss: 0.5321690440177917\n",
      "cnt: 0 - valLoss: 0.5576048493385315 - trainLoss: 0.5321541428565979\n",
      "cnt: 0 - valLoss: 0.5576012134552002 - trainLoss: 0.5321393609046936\n",
      "cnt: 0 - valLoss: 0.5575973987579346 - trainLoss: 0.5321245193481445\n",
      "cnt: 0 - valLoss: 0.5575933456420898 - trainLoss: 0.5321097373962402\n",
      "cnt: 0 - valLoss: 0.557589054107666 - trainLoss: 0.5320949554443359\n",
      "cnt: 0 - valLoss: 0.5575847029685974 - trainLoss: 0.5320801138877869\n",
      "cnt: 0 - valLoss: 0.5575801730155945 - trainLoss: 0.5320653915405273\n",
      "cnt: 0 - valLoss: 0.5575755834579468 - trainLoss: 0.5320505499839783\n",
      "cnt: 0 - valLoss: 0.5575708746910095 - trainLoss: 0.5320358276367188\n",
      "cnt: 0 - valLoss: 0.5575661063194275 - trainLoss: 0.5320211052894592\n",
      "cnt: 0 - valLoss: 0.5575612783432007 - trainLoss: 0.5320063233375549\n",
      "cnt: 0 - valLoss: 0.5575563907623291 - trainLoss: 0.5319915413856506\n",
      "cnt: 0 - valLoss: 0.5575515031814575 - trainLoss: 0.5319768190383911\n",
      "cnt: 0 - valLoss: 0.5575464963912964 - trainLoss: 0.5319620966911316\n",
      "cnt: 0 - valLoss: 0.5575414896011353 - trainLoss: 0.5319473743438721\n",
      "cnt: 0 - valLoss: 0.5575230121612549 - trainLoss: 0.5319326519966125\n",
      "cnt: 0 - valLoss: 0.5575065016746521 - trainLoss: 0.5319179892539978\n",
      "cnt: 0 - valLoss: 0.5575041770935059 - trainLoss: 0.5319035053253174\n",
      "cnt: 0 - valLoss: 0.5574884414672852 - trainLoss: 0.5318889021873474\n",
      "cnt: 0 - valLoss: 0.5574743747711182 - trainLoss: 0.531874418258667\n",
      "cnt: 0 - valLoss: 0.5574616193771362 - trainLoss: 0.5318598747253418\n",
      "cnt: 0 - valLoss: 0.557449996471405 - trainLoss: 0.5318453907966614\n",
      "cnt: 0 - valLoss: 0.5574393272399902 - trainLoss: 0.531830906867981\n",
      "cnt: 0 - valLoss: 0.5574294328689575 - trainLoss: 0.5318165421485901\n",
      "cnt: 0 - valLoss: 0.5574203133583069 - trainLoss: 0.5318020582199097\n",
      "cnt: 0 - valLoss: 0.557411789894104 - trainLoss: 0.531787633895874\n",
      "cnt: 0 - valLoss: 0.5574037432670593 - trainLoss: 0.5317732691764832\n",
      "cnt: 0 - valLoss: 0.5573961138725281 - trainLoss: 0.5317588448524475\n",
      "cnt: 0 - valLoss: 0.5573888421058655 - trainLoss: 0.5317444801330566\n",
      "cnt: 0 - valLoss: 0.5573818683624268 - trainLoss: 0.531730055809021\n",
      "cnt: 0 - valLoss: 0.5573751926422119 - trainLoss: 0.5317156910896301\n",
      "cnt: 0 - valLoss: 0.557368814945221 - trainLoss: 0.5317013263702393\n",
      "cnt: 0 - valLoss: 0.5573624968528748 - trainLoss: 0.5316869616508484\n",
      "cnt: 0 - valLoss: 0.5573564171791077 - trainLoss: 0.5316725969314575\n",
      "cnt: 0 - valLoss: 0.5573505163192749 - trainLoss: 0.5316582322120667\n",
      "cnt: 0 - valLoss: 0.5573446750640869 - trainLoss: 0.5316439270973206\n",
      "cnt: 0 - valLoss: 0.5573389530181885 - trainLoss: 0.5316295623779297\n",
      "cnt: 0 - valLoss: 0.5573320984840393 - trainLoss: 0.5316151976585388\n",
      "cnt: 0 - valLoss: 0.557325541973114 - trainLoss: 0.5316008925437927\n",
      "cnt: 0 - valLoss: 0.5573191046714783 - trainLoss: 0.5315865278244019\n",
      "cnt: 0 - valLoss: 0.5573128461837769 - trainLoss: 0.531572163105011\n",
      "cnt: 0 - valLoss: 0.5573068857192993 - trainLoss: 0.5315578579902649\n",
      "cnt: 0 - valLoss: 0.5573010444641113 - trainLoss: 0.5315434336662292\n",
      "cnt: 0 - valLoss: 0.5572952032089233 - trainLoss: 0.5315291285514832\n",
      "cnt: 0 - valLoss: 0.5572895407676697 - trainLoss: 0.5315147638320923\n",
      "cnt: 0 - valLoss: 0.5572839975357056 - trainLoss: 0.5315004587173462\n",
      "cnt: 0 - valLoss: 0.5572785139083862 - trainLoss: 0.5314861536026001\n",
      "cnt: 0 - valLoss: 0.5572730302810669 - trainLoss: 0.5314719080924988\n",
      "cnt: 0 - valLoss: 0.5572676658630371 - trainLoss: 0.5314575433731079\n",
      "cnt: 0 - valLoss: 0.5572623610496521 - trainLoss: 0.5314432978630066\n",
      "cnt: 0 - valLoss: 0.5572571158409119 - trainLoss: 0.5314289927482605\n",
      "cnt: 0 - valLoss: 0.557231068611145 - trainLoss: 0.5314149856567383\n",
      "cnt: 0 - valLoss: 0.5572082996368408 - trainLoss: 0.5314008593559265\n",
      "cnt: 0 - valLoss: 0.5572084784507751 - trainLoss: 0.5313868522644043\n",
      "cnt: 1 - valLoss: 0.5571871995925903 - trainLoss: 0.5313730239868164\n",
      "cnt: 0 - valLoss: 0.5571684241294861 - trainLoss: 0.531359076499939\n",
      "cnt: 0 - valLoss: 0.5571413636207581 - trainLoss: 0.5313452482223511\n",
      "cnt: 0 - valLoss: 0.556776225566864 - trainLoss: 0.5313298106193542\n",
      "cnt: 0 - valLoss: 0.5565171241760254 - trainLoss: 0.5312880277633667\n",
      "cnt: 0 - valLoss: 0.5563274621963501 - trainLoss: 0.5312574505805969\n",
      "cnt: 0 - valLoss: 0.5561676621437073 - trainLoss: 0.531233549118042\n",
      "cnt: 0 - valLoss: 0.5560324788093567 - trainLoss: 0.5312120914459229\n",
      "cnt: 0 - valLoss: 0.5559180974960327 - trainLoss: 0.5311923027038574\n",
      "cnt: 0 - valLoss: 0.5558207631111145 - trainLoss: 0.531173825263977\n",
      "cnt: 0 - valLoss: 0.5557376146316528 - trainLoss: 0.5311561226844788\n",
      "cnt: 0 - valLoss: 0.555666446685791 - trainLoss: 0.531139075756073\n",
      "cnt: 0 - valLoss: 0.5554741024971008 - trainLoss: 0.5311223268508911\n",
      "cnt: 0 - valLoss: 0.5548881888389587 - trainLoss: 0.5310802459716797\n",
      "cnt: 0 - valLoss: 0.5542416572570801 - trainLoss: 0.5309795141220093\n",
      "cnt: 0 - valLoss: 0.5534797310829163 - trainLoss: 0.530846118927002\n",
      "cnt: 0 - valLoss: 0.5530809760093689 - trainLoss: 0.530665397644043\n",
      "cnt: 0 - valLoss: 0.5530654788017273 - trainLoss: 0.5306054949760437\n",
      "cnt: 0 - valLoss: 0.5530501008033752 - trainLoss: 0.5305851101875305\n",
      "cnt: 0 - valLoss: 0.5530350804328918 - trainLoss: 0.5305646657943726\n",
      "cnt: 0 - valLoss: 0.5530210137367249 - trainLoss: 0.5305442810058594\n",
      "cnt: 0 - valLoss: 0.5530069470405579 - trainLoss: 0.5305238962173462\n",
      "cnt: 0 - valLoss: 0.55299311876297 - trainLoss: 0.5305034518241882\n",
      "cnt: 0 - valLoss: 0.5529794096946716 - trainLoss: 0.530483067035675\n",
      "cnt: 0 - valLoss: 0.5529657006263733 - trainLoss: 0.5304626822471619\n",
      "cnt: 0 - valLoss: 0.5529525279998779 - trainLoss: 0.5304423570632935\n",
      "cnt: 0 - valLoss: 0.5526500940322876 - trainLoss: 0.5304230451583862\n",
      "cnt: 0 - valLoss: 0.5526452660560608 - trainLoss: 0.5303908586502075\n",
      "cnt: 0 - valLoss: 0.5525401830673218 - trainLoss: 0.5303693413734436\n",
      "cnt: 0 - valLoss: 0.5525491237640381 - trainLoss: 0.5303475856781006\n",
      "cnt: 1 - valLoss: 0.5525534749031067 - trainLoss: 0.5303251147270203\n",
      "cnt: 2 - valLoss: 0.5525554418563843 - trainLoss: 0.5303028225898743\n",
      "cnt: 3 - valLoss: 0.5524552464485168 - trainLoss: 0.5302814245223999\n",
      "cnt: 0 - valLoss: 0.5524695515632629 - trainLoss: 0.5302597284317017\n",
      "cnt: 1 - valLoss: 0.5524790287017822 - trainLoss: 0.5302371978759766\n",
      "cnt: 2 - valLoss: 0.5523859262466431 - trainLoss: 0.5302153825759888\n",
      "cnt: 0 - valLoss: 0.5524052977561951 - trainLoss: 0.5301942825317383\n",
      "cnt: 1 - valLoss: 0.5524196624755859 - trainLoss: 0.5301715731620789\n",
      "cnt: 2 - valLoss: 0.5523293018341064 - trainLoss: 0.5301502346992493\n",
      "cnt: 0 - valLoss: 0.5523515939712524 - trainLoss: 0.5301286578178406\n",
      "cnt: 1 - valLoss: 0.5522702932357788 - trainLoss: 0.5301061868667603\n",
      "cnt: 0 - valLoss: 0.552298367023468 - trainLoss: 0.5300856828689575\n",
      "cnt: 1 - valLoss: 0.5523207783699036 - trainLoss: 0.5300628542900085\n",
      "cnt: 2 - valLoss: 0.5522350668907166 - trainLoss: 0.5300422310829163\n",
      "cnt: 0 - valLoss: 0.5522634983062744 - trainLoss: 0.530019998550415\n",
      "cnt: 1 - valLoss: 0.5521841645240784 - trainLoss: 0.5299986600875854\n",
      "cnt: 0 - valLoss: 0.5522171258926392 - trainLoss: 0.529977023601532\n",
      "cnt: 1 - valLoss: 0.5521414279937744 - trainLoss: 0.5299555659294128\n",
      "cnt: 0 - valLoss: 0.5521776080131531 - trainLoss: 0.5299340486526489\n",
      "cnt: 1 - valLoss: 0.5521039366722107 - trainLoss: 0.5299127101898193\n",
      "cnt: 0 - valLoss: 0.5521445870399475 - trainLoss: 0.5298908948898315\n",
      "cnt: 1 - valLoss: 0.5520731806755066 - trainLoss: 0.5298701524734497\n",
      "cnt: 0 - valLoss: 0.5521153807640076 - trainLoss: 0.5298478603363037\n",
      "cnt: 1 - valLoss: 0.5520444512367249 - trainLoss: 0.5298278331756592\n",
      "cnt: 0 - valLoss: 0.5519863367080688 - trainLoss: 0.529805600643158\n",
      "cnt: 0 - valLoss: 0.5520367622375488 - trainLoss: 0.5297847986221313\n",
      "cnt: 1 - valLoss: 0.5519735217094421 - trainLoss: 0.5297638177871704\n",
      "cnt: 0 - valLoss: 0.551921546459198 - trainLoss: 0.5297420024871826\n",
      "cnt: 0 - valLoss: 0.5519779324531555 - trainLoss: 0.5297213792800903\n",
      "cnt: 1 - valLoss: 0.5519186854362488 - trainLoss: 0.5297005772590637\n",
      "cnt: 0 - valLoss: 0.5518699288368225 - trainLoss: 0.5296789407730103\n",
      "cnt: 0 - valLoss: 0.5519295334815979 - trainLoss: 0.5296577215194702\n",
      "cnt: 1 - valLoss: 0.5518730878829956 - trainLoss: 0.5296376943588257\n",
      "cnt: 2 - valLoss: 0.5518254041671753 - trainLoss: 0.5296162366867065\n",
      "cnt: 0 - valLoss: 0.5517847537994385 - trainLoss: 0.5295950174331665\n",
      "cnt: 0 - valLoss: 0.5517501831054688 - trainLoss: 0.5295740365982056\n",
      "cnt: 0 - valLoss: 0.5518198609352112 - trainLoss: 0.5295537114143372\n",
      "cnt: 1 - valLoss: 0.5517722368240356 - trainLoss: 0.5295330882072449\n",
      "cnt: 2 - valLoss: 0.5517316460609436 - trainLoss: 0.5295119285583496\n",
      "cnt: 0 - valLoss: 0.5516965985298157 - trainLoss: 0.5294910073280334\n",
      "cnt: 0 - valLoss: 0.5516659617424011 - trainLoss: 0.5294702053070068\n",
      "cnt: 0 - valLoss: 0.5516388416290283 - trainLoss: 0.5294495224952698\n",
      "cnt: 0 - valLoss: 0.5516145825386047 - trainLoss: 0.5294287800788879\n",
      "cnt: 0 - valLoss: 0.5515925288200378 - trainLoss: 0.5294082164764404\n",
      "cnt: 0 - valLoss: 0.551572322845459 - trainLoss: 0.5293875932693481\n",
      "cnt: 0 - valLoss: 0.5515535473823547 - trainLoss: 0.5293670892715454\n",
      "cnt: 0 - valLoss: 0.551535964012146 - trainLoss: 0.5293464660644531\n",
      "cnt: 0 - valLoss: 0.5515192151069641 - trainLoss: 0.5293259620666504\n",
      "cnt: 0 - valLoss: 0.5515033006668091 - trainLoss: 0.5293055176734924\n",
      "cnt: 0 - valLoss: 0.5514880418777466 - trainLoss: 0.5292850732803345\n",
      "cnt: 0 - valLoss: 0.5514732003211975 - trainLoss: 0.5292646288871765\n",
      "cnt: 0 - valLoss: 0.5514587759971619 - trainLoss: 0.5292441248893738\n",
      "cnt: 0 - valLoss: 0.5514446496963501 - trainLoss: 0.5292237401008606\n",
      "cnt: 0 - valLoss: 0.5514307618141174 - trainLoss: 0.5292032957077026\n",
      "cnt: 0 - valLoss: 0.5514171719551086 - trainLoss: 0.5291829109191895\n",
      "cnt: 0 - valLoss: 0.5514037013053894 - trainLoss: 0.529162585735321\n",
      "cnt: 0 - valLoss: 0.551391065120697 - trainLoss: 0.5291422605514526\n",
      "cnt: 0 - valLoss: 0.5513784289360046 - trainLoss: 0.5291218161582947\n",
      "cnt: 0 - valLoss: 0.5513656735420227 - trainLoss: 0.529101550579071\n",
      "cnt: 0 - valLoss: 0.5513530969619751 - trainLoss: 0.5290812253952026\n",
      "cnt: 0 - valLoss: 0.5513403415679932 - trainLoss: 0.529060959815979\n",
      "cnt: 0 - valLoss: 0.5513277053833008 - trainLoss: 0.5290406346321106\n",
      "cnt: 0 - valLoss: 0.5513150691986084 - trainLoss: 0.529020369052887\n",
      "cnt: 0 - valLoss: 0.551302433013916 - trainLoss: 0.5290001034736633\n",
      "cnt: 0 - valLoss: 0.5512897372245789 - trainLoss: 0.5289799571037292\n",
      "cnt: 0 - valLoss: 0.5512771606445312 - trainLoss: 0.5289597511291504\n",
      "cnt: 0 - valLoss: 0.5512645244598389 - trainLoss: 0.5289395451545715\n",
      "cnt: 0 - valLoss: 0.5512518882751465 - trainLoss: 0.5289193391799927\n",
      "cnt: 0 - valLoss: 0.5512392520904541 - trainLoss: 0.5288991332054138\n",
      "cnt: 0 - valLoss: 0.5512267351150513 - trainLoss: 0.5288790464401245\n",
      "cnt: 0 - valLoss: 0.5512141585350037 - trainLoss: 0.5288589000701904\n",
      "cnt: 0 - valLoss: 0.5512016415596008 - trainLoss: 0.5288387537002563\n",
      "cnt: 0 - valLoss: 0.5511890649795532 - trainLoss: 0.528818666934967\n",
      "cnt: 0 - valLoss: 0.5511763095855713 - trainLoss: 0.5287985801696777\n",
      "cnt: 0 - valLoss: 0.5511637330055237 - trainLoss: 0.5287784934043884\n",
      "cnt: 0 - valLoss: 0.5508791208267212 - trainLoss: 0.5287588238716125\n",
      "cnt: 0 - valLoss: 0.5510188341140747 - trainLoss: 0.5287444591522217\n",
      "cnt: 1 - valLoss: 0.5510305166244507 - trainLoss: 0.5287197232246399\n",
      "cnt: 2 - valLoss: 0.5510373711585999 - trainLoss: 0.528699517250061\n",
      "cnt: 3 - valLoss: 0.5510401129722595 - trainLoss: 0.5286793112754822\n",
      "cnt: 4 - valLoss: 0.5510402321815491 - trainLoss: 0.5286592841148376\n",
      "cnt: 5 - valLoss: 0.5507652759552002 - trainLoss: 0.5286399722099304\n",
      "cnt: 0 - valLoss: 0.5509151220321655 - trainLoss: 0.5286259055137634\n",
      "cnt: 1 - valLoss: 0.550934374332428 - trainLoss: 0.5286011099815369\n",
      "cnt: 2 - valLoss: 0.5509459376335144 - trainLoss: 0.5285807847976685\n",
      "cnt: 3 - valLoss: 0.5509539842605591 - trainLoss: 0.5285606980323792\n",
      "cnt: 4 - valLoss: 0.5506830811500549 - trainLoss: 0.5285436511039734\n",
      "cnt: 0 - valLoss: 0.5507387518882751 - trainLoss: 0.5285273194313049\n",
      "cnt: 1 - valLoss: 0.5507810711860657 - trainLoss: 0.5285053849220276\n",
      "cnt: 2 - valLoss: 0.5508109927177429 - trainLoss: 0.5284842848777771\n",
      "cnt: 3 - valLoss: 0.5508328676223755 - trainLoss: 0.5284637212753296\n",
      "cnt: 4 - valLoss: 0.5505757331848145 - trainLoss: 0.5284443497657776\n",
      "cnt: 0 - valLoss: 0.5506405234336853 - trainLoss: 0.5284311771392822\n",
      "cnt: 1 - valLoss: 0.5506899356842041 - trainLoss: 0.5284087657928467\n",
      "cnt: 2 - valLoss: 0.5507258772850037 - trainLoss: 0.5283873677253723\n",
      "cnt: 3 - valLoss: 0.5507540702819824 - trainLoss: 0.5283666849136353\n",
      "cnt: 4 - valLoss: 0.550499677658081 - trainLoss: 0.5283501744270325\n",
      "cnt: 0 - valLoss: 0.5505672693252563 - trainLoss: 0.528334379196167\n",
      "cnt: 1 - valLoss: 0.5506178736686707 - trainLoss: 0.5283118486404419\n",
      "cnt: 2 - valLoss: 0.5506570339202881 - trainLoss: 0.5282904505729675\n",
      "cnt: 3 - valLoss: 0.5504140257835388 - trainLoss: 0.5282716155052185\n",
      "cnt: 0 - valLoss: 0.5504878759384155 - trainLoss: 0.5282584428787231\n",
      "cnt: 1 - valLoss: 0.5505436062812805 - trainLoss: 0.5282354354858398\n",
      "cnt: 2 - valLoss: 0.5505887866020203 - trainLoss: 0.5282137989997864\n",
      "cnt: 3 - valLoss: 0.5503591299057007 - trainLoss: 0.5281963348388672\n",
      "cnt: 0 - valLoss: 0.5504266023635864 - trainLoss: 0.5281819105148315\n",
      "cnt: 1 - valLoss: 0.5504850149154663 - trainLoss: 0.5281586647033691\n",
      "cnt: 2 - valLoss: 0.5505328178405762 - trainLoss: 0.5281369686126709\n",
      "cnt: 3 - valLoss: 0.5502866506576538 - trainLoss: 0.5281228423118591\n",
      "cnt: 0 - valLoss: 0.5503509044647217 - trainLoss: 0.5281071066856384\n",
      "cnt: 1 - valLoss: 0.5504117608070374 - trainLoss: 0.5280833840370178\n",
      "cnt: 2 - valLoss: 0.550431489944458 - trainLoss: 0.5280613899230957\n",
      "cnt: 3 - valLoss: 0.5502133965492249 - trainLoss: 0.5280462503433228\n",
      "cnt: 0 - valLoss: 0.5502837300300598 - trainLoss: 0.5280332565307617\n",
      "cnt: 1 - valLoss: 0.5503377914428711 - trainLoss: 0.528008759021759\n",
      "cnt: 2 - valLoss: 0.5503636598587036 - trainLoss: 0.5279875993728638\n",
      "cnt: 3 - valLoss: 0.550159215927124 - trainLoss: 0.5279737710952759\n",
      "cnt: 0 - valLoss: 0.5502309203147888 - trainLoss: 0.5279584527015686\n",
      "cnt: 1 - valLoss: 0.5502626895904541 - trainLoss: 0.5279347896575928\n",
      "cnt: 2 - valLoss: 0.5502884387969971 - trainLoss: 0.5279150009155273\n",
      "cnt: 3 - valLoss: 0.5500971078872681 - trainLoss: 0.5279004573822021\n",
      "cnt: 0 - valLoss: 0.5501489639282227 - trainLoss: 0.5278850197792053\n",
      "cnt: 1 - valLoss: 0.5501887202262878 - trainLoss: 0.5278634428977966\n",
      "cnt: 2 - valLoss: 0.5502208471298218 - trainLoss: 0.5278430581092834\n",
      "cnt: 3 - valLoss: 0.5500336289405823 - trainLoss: 0.5278277397155762\n",
      "cnt: 0 - valLoss: 0.5500897765159607 - trainLoss: 0.5278135538101196\n",
      "cnt: 1 - valLoss: 0.5501332879066467 - trainLoss: 0.5277915596961975\n",
      "cnt: 2 - valLoss: 0.5501692891120911 - trainLoss: 0.5277709364891052\n",
      "cnt: 3 - valLoss: 0.54998379945755 - trainLoss: 0.5277575850486755\n",
      "cnt: 0 - valLoss: 0.5500418543815613 - trainLoss: 0.5277414917945862\n",
      "cnt: 1 - valLoss: 0.5500872135162354 - trainLoss: 0.5277193784713745\n",
      "cnt: 2 - valLoss: 0.5499176979064941 - trainLoss: 0.5276995301246643\n",
      "cnt: 0 - valLoss: 0.549982488155365 - trainLoss: 0.5276898145675659\n",
      "cnt: 1 - valLoss: 0.5500329732894897 - trainLoss: 0.5276668071746826\n",
      "cnt: 2 - valLoss: 0.5500763654708862 - trainLoss: 0.5276455879211426\n",
      "cnt: 3 - valLoss: 0.5498948693275452 - trainLoss: 0.5276350378990173\n",
      "cnt: 0 - valLoss: 0.5499563813209534 - trainLoss: 0.5276163816452026\n",
      "cnt: 1 - valLoss: 0.5500071048736572 - trainLoss: 0.5275939106941223\n",
      "cnt: 2 - valLoss: 0.549835741519928 - trainLoss: 0.5275790095329285\n",
      "cnt: 0 - valLoss: 0.5499029159545898 - trainLoss: 0.527564525604248\n",
      "cnt: 1 - valLoss: 0.5499580502510071 - trainLoss: 0.5275413990020752\n",
      "cnt: 2 - valLoss: 0.549791157245636 - trainLoss: 0.527525782585144\n",
      "cnt: 0 - valLoss: 0.5498611330986023 - trainLoss: 0.5275119543075562\n",
      "cnt: 1 - valLoss: 0.5499191880226135 - trainLoss: 0.5274884104728699\n",
      "cnt: 2 - valLoss: 0.5497537851333618 - trainLoss: 0.5274739265441895\n",
      "cnt: 0 - valLoss: 0.5498253703117371 - trainLoss: 0.527458906173706\n",
      "cnt: 1 - valLoss: 0.5498833656311035 - trainLoss: 0.5274352431297302\n",
      "cnt: 2 - valLoss: 0.5497186779975891 - trainLoss: 0.5274226665496826\n",
      "cnt: 0 - valLoss: 0.5497913360595703 - trainLoss: 0.5274059176445007\n",
      "cnt: 1 - valLoss: 0.5498512387275696 - trainLoss: 0.5273821949958801\n",
      "cnt: 2 - valLoss: 0.5496861338615417 - trainLoss: 0.5273719429969788\n",
      "cnt: 0 - valLoss: 0.5497599244117737 - trainLoss: 0.5273528099060059\n",
      "cnt: 1 - valLoss: 0.5496147871017456 - trainLoss: 0.5273301601409912\n",
      "cnt: 0 - valLoss: 0.5495712757110596 - trainLoss: 0.5273174047470093\n",
      "cnt: 0 - valLoss: 0.5495360493659973 - trainLoss: 0.5272988677024841\n",
      "cnt: 0 - valLoss: 0.5495754480361938 - trainLoss: 0.527281641960144\n",
      "cnt: 1 - valLoss: 0.5495337247848511 - trainLoss: 0.5272641777992249\n",
      "cnt: 0 - valLoss: 0.5494987368583679 - trainLoss: 0.527245819568634\n",
      "cnt: 0 - valLoss: 0.5495392680168152 - trainLoss: 0.5272279381752014\n",
      "cnt: 1 - valLoss: 0.5494985580444336 - trainLoss: 0.5272112488746643\n",
      "cnt: 0 - valLoss: 0.5494644641876221 - trainLoss: 0.5271929502487183\n",
      "cnt: 0 - valLoss: 0.5494356155395508 - trainLoss: 0.5271750092506409\n",
      "cnt: 0 - valLoss: 0.5494801998138428 - trainLoss: 0.5271574854850769\n",
      "cnt: 1 - valLoss: 0.5494433641433716 - trainLoss: 0.5271406173706055\n",
      "cnt: 2 - valLoss: 0.5494120717048645 - trainLoss: 0.5271225571632385\n",
      "cnt: 0 - valLoss: 0.5493853092193604 - trainLoss: 0.5271047949790955\n",
      "cnt: 0 - valLoss: 0.5493620038032532 - trainLoss: 0.5270872116088867\n",
      "cnt: 0 - valLoss: 0.5493419170379639 - trainLoss: 0.527069628238678\n",
      "cnt: 0 - valLoss: 0.5493929386138916 - trainLoss: 0.5270525217056274\n",
      "cnt: 1 - valLoss: 0.5493614673614502 - trainLoss: 0.5270354151725769\n",
      "cnt: 2 - valLoss: 0.5493345856666565 - trainLoss: 0.5270177125930786\n",
      "cnt: 0 - valLoss: 0.5493112802505493 - trainLoss: 0.5270000696182251\n",
      "cnt: 0 - valLoss: 0.5492907166481018 - trainLoss: 0.5269826054573059\n",
      "cnt: 0 - valLoss: 0.549272358417511 - trainLoss: 0.5269652009010315\n",
      "cnt: 0 - valLoss: 0.5492556691169739 - trainLoss: 0.5269479155540466\n",
      "cnt: 0 - valLoss: 0.5492404699325562 - trainLoss: 0.526930570602417\n",
      "cnt: 0 - valLoss: 0.5492262244224548 - trainLoss: 0.5269133448600769\n",
      "cnt: 0 - valLoss: 0.5492128729820251 - trainLoss: 0.526896059513092\n",
      "cnt: 0 - valLoss: 0.5492002964019775 - trainLoss: 0.5268787741661072\n",
      "cnt: 0 - valLoss: 0.549187958240509 - trainLoss: 0.5268615484237671\n",
      "cnt: 0 - valLoss: 0.5491760969161987 - trainLoss: 0.526844322681427\n",
      "cnt: 0 - valLoss: 0.5491646528244019 - trainLoss: 0.5268271565437317\n",
      "cnt: 0 - valLoss: 0.5491534471511841 - trainLoss: 0.5268099904060364\n",
      "cnt: 0 - valLoss: 0.549142599105835 - trainLoss: 0.5267928242683411\n",
      "cnt: 0 - valLoss: 0.5491318702697754 - trainLoss: 0.5267756581306458\n",
      "cnt: 0 - valLoss: 0.549119234085083 - trainLoss: 0.5267585515975952\n",
      "cnt: 0 - valLoss: 0.5491072535514832 - trainLoss: 0.5267414450645447\n",
      "cnt: 0 - valLoss: 0.5490956902503967 - trainLoss: 0.5267243385314941\n",
      "cnt: 0 - valLoss: 0.5490844249725342 - trainLoss: 0.5267072319984436\n",
      "cnt: 0 - valLoss: 0.5490733981132507 - trainLoss: 0.5266901850700378\n",
      "cnt: 0 - valLoss: 0.5490627288818359 - trainLoss: 0.5266731381416321\n",
      "cnt: 0 - valLoss: 0.5490521192550659 - trainLoss: 0.5266560912132263\n",
      "cnt: 0 - valLoss: 0.549041748046875 - trainLoss: 0.5266390442848206\n",
      "cnt: 0 - valLoss: 0.5490314364433289 - trainLoss: 0.5266220569610596\n",
      "cnt: 0 - valLoss: 0.5490212440490723 - trainLoss: 0.5266050100326538\n",
      "cnt: 0 - valLoss: 0.54901123046875 - trainLoss: 0.5265880227088928\n",
      "cnt: 0 - valLoss: 0.5490012168884277 - trainLoss: 0.5265710949897766\n",
      "cnt: 0 - valLoss: 0.5489911437034607 - trainLoss: 0.5265541672706604\n",
      "cnt: 0 - valLoss: 0.548981249332428 - trainLoss: 0.5265371799468994\n",
      "cnt: 0 - valLoss: 0.5489575862884521 - trainLoss: 0.526520311832428\n",
      "cnt: 0 - valLoss: 0.5489370226860046 - trainLoss: 0.5265035033226013\n",
      "cnt: 0 - valLoss: 0.5489189624786377 - trainLoss: 0.5264867544174194\n",
      "cnt: 0 - valLoss: 0.548902690410614 - trainLoss: 0.5264701843261719\n",
      "cnt: 0 - valLoss: 0.5488877296447754 - trainLoss: 0.5264535546302795\n",
      "cnt: 0 - valLoss: 0.5488738417625427 - trainLoss: 0.526436984539032\n",
      "cnt: 0 - valLoss: 0.5488608479499817 - trainLoss: 0.5264204144477844\n",
      "cnt: 0 - valLoss: 0.548848569393158 - trainLoss: 0.5264039039611816\n",
      "cnt: 0 - valLoss: 0.548836886882782 - trainLoss: 0.5263874530792236\n",
      "cnt: 0 - valLoss: 0.5488255023956299 - trainLoss: 0.5263708829879761\n",
      "cnt: 0 - valLoss: 0.5488145351409912 - trainLoss: 0.5263544321060181\n",
      "cnt: 0 - valLoss: 0.5488033294677734 - trainLoss: 0.5263379216194153\n",
      "cnt: 0 - valLoss: 0.5487924814224243 - trainLoss: 0.5263214111328125\n",
      "cnt: 0 - valLoss: 0.5487819314002991 - trainLoss: 0.5263049602508545\n",
      "cnt: 0 - valLoss: 0.5487715005874634 - trainLoss: 0.5262885093688965\n",
      "cnt: 0 - valLoss: 0.548761248588562 - trainLoss: 0.5262720584869385\n",
      "cnt: 0 - valLoss: 0.5487511157989502 - trainLoss: 0.5262556672096252\n",
      "cnt: 0 - valLoss: 0.5487411618232727 - trainLoss: 0.5262392163276672\n",
      "cnt: 0 - valLoss: 0.54873126745224 - trainLoss: 0.5262227654457092\n",
      "cnt: 0 - valLoss: 0.5487213730812073 - trainLoss: 0.5262063145637512\n",
      "cnt: 0 - valLoss: 0.5487115979194641 - trainLoss: 0.5261899828910828\n",
      "cnt: 0 - valLoss: 0.5487019419670105 - trainLoss: 0.5261735916137695\n",
      "cnt: 0 - valLoss: 0.5486932992935181 - trainLoss: 0.5261572599411011\n",
      "cnt: 0 - valLoss: 0.5486845970153809 - trainLoss: 0.5261409282684326\n",
      "cnt: 0 - valLoss: 0.5484980940818787 - trainLoss: 0.5261249542236328\n",
      "cnt: 0 - valLoss: 0.5485875606536865 - trainLoss: 0.5261158347129822\n",
      "cnt: 1 - valLoss: 0.5485947132110596 - trainLoss: 0.5260933637619019\n",
      "cnt: 2 - valLoss: 0.5485985279083252 - trainLoss: 0.5260767936706543\n",
      "cnt: 3 - valLoss: 0.5485994815826416 - trainLoss: 0.5260602831840515\n",
      "cnt: 4 - valLoss: 0.5485984086990356 - trainLoss: 0.5260439515113831\n",
      "cnt: 5 - valLoss: 0.5484186410903931 - trainLoss: 0.5260277390480042\n",
      "cnt: 0 - valLoss: 0.5486540198326111 - trainLoss: 0.5260226130485535\n",
      "cnt: 1 - valLoss: 0.5484475493431091 - trainLoss: 0.5260113477706909\n",
      "cnt: 2 - valLoss: 0.5484691262245178 - trainLoss: 0.5259835720062256\n",
      "cnt: 3 - valLoss: 0.5484842658042908 - trainLoss: 0.5259664058685303\n",
      "cnt: 4 - valLoss: 0.5484945178031921 - trainLoss: 0.5259495377540588\n",
      "cnt: 5 - valLoss: 0.5485005974769592 - trainLoss: 0.5259329080581665\n",
      "cnt: 6 - valLoss: 0.5485050082206726 - trainLoss: 0.5259164571762085\n",
      "cnt: 7 - valLoss: 0.5483258962631226 - trainLoss: 0.5259034037590027\n",
      "cnt: 0 - valLoss: 0.5485695004463196 - trainLoss: 0.5259007215499878\n",
      "cnt: 1 - valLoss: 0.5483627915382385 - trainLoss: 0.5258886218070984\n",
      "cnt: 2 - valLoss: 0.5485280752182007 - trainLoss: 0.5258581638336182\n",
      "cnt: 3 - valLoss: 0.5483271479606628 - trainLoss: 0.5258550047874451\n",
      "cnt: 4 - valLoss: 0.5484951138496399 - trainLoss: 0.5258316397666931\n",
      "cnt: 5 - valLoss: 0.5482982397079468 - trainLoss: 0.5258229970932007\n",
      "cnt: 0 - valLoss: 0.548468291759491 - trainLoss: 0.5258039236068726\n",
      "cnt: 1 - valLoss: 0.5482736825942993 - trainLoss: 0.5257921814918518\n",
      "cnt: 0 - valLoss: 0.5484452247619629 - trainLoss: 0.5257754325866699\n",
      "cnt: 1 - valLoss: 0.5482515692710876 - trainLoss: 0.5257620811462402\n",
      "cnt: 0 - valLoss: 0.5484245419502258 - trainLoss: 0.5257464051246643\n",
      "cnt: 1 - valLoss: 0.5482321977615356 - trainLoss: 0.5257323384284973\n",
      "cnt: 0 - valLoss: 0.548406183719635 - trainLoss: 0.5257169604301453\n",
      "cnt: 1 - valLoss: 0.5482134222984314 - trainLoss: 0.5257031917572021\n",
      "cnt: 0 - valLoss: 0.5483878254890442 - trainLoss: 0.5256873965263367\n",
      "cnt: 1 - valLoss: 0.5481997132301331 - trainLoss: 0.5256741046905518\n",
      "cnt: 0 - valLoss: 0.5483728647232056 - trainLoss: 0.5256568789482117\n",
      "cnt: 1 - valLoss: 0.5481845736503601 - trainLoss: 0.5256457328796387\n",
      "cnt: 0 - valLoss: 0.5483566522598267 - trainLoss: 0.5256265997886658\n",
      "cnt: 1 - valLoss: 0.5481684803962708 - trainLoss: 0.5256171822547913\n",
      "cnt: 0 - valLoss: 0.5483394861221313 - trainLoss: 0.525596559047699\n",
      "cnt: 1 - valLoss: 0.5481518507003784 - trainLoss: 0.5255884528160095\n",
      "cnt: 0 - valLoss: 0.5483220815658569 - trainLoss: 0.5255667567253113\n",
      "cnt: 1 - valLoss: 0.5481346845626831 - trainLoss: 0.525559663772583\n",
      "cnt: 0 - valLoss: 0.5483043789863586 - trainLoss: 0.5255370140075684\n",
      "cnt: 1 - valLoss: 0.5481175780296326 - trainLoss: 0.5255309343338013\n",
      "cnt: 0 - valLoss: 0.5482854843139648 - trainLoss: 0.5255074501037598\n",
      "cnt: 1 - valLoss: 0.5480997562408447 - trainLoss: 0.5255019068717957\n",
      "cnt: 0 - valLoss: 0.5482670068740845 - trainLoss: 0.5254780054092407\n",
      "cnt: 1 - valLoss: 0.5480820536613464 - trainLoss: 0.5254731178283691\n",
      "cnt: 0 - valLoss: 0.5482498407363892 - trainLoss: 0.5254485607147217\n",
      "cnt: 1 - valLoss: 0.5480648279190063 - trainLoss: 0.5254445672035217\n",
      "cnt: 0 - valLoss: 0.5480523705482483 - trainLoss: 0.5254198312759399\n",
      "cnt: 0 - valLoss: 0.5480406880378723 - trainLoss: 0.5254063606262207\n",
      "cnt: 0 - valLoss: 0.5480297803878784 - trainLoss: 0.5253929495811462\n",
      "cnt: 0 - valLoss: 0.5480194091796875 - trainLoss: 0.5253795981407166\n",
      "cnt: 0 - valLoss: 0.5480093955993652 - trainLoss: 0.5253662467002869\n",
      "cnt: 0 - valLoss: 0.5479996800422668 - trainLoss: 0.5253528952598572\n",
      "cnt: 0 - valLoss: 0.5479902625083923 - trainLoss: 0.5253395438194275\n",
      "cnt: 0 - valLoss: 0.5479810833930969 - trainLoss: 0.5253261923789978\n",
      "cnt: 0 - valLoss: 0.5479719638824463 - trainLoss: 0.5253128409385681\n",
      "cnt: 0 - valLoss: 0.54796302318573 - trainLoss: 0.5252995491027832\n",
      "cnt: 0 - valLoss: 0.5479542016983032 - trainLoss: 0.5252862572669983\n",
      "cnt: 0 - valLoss: 0.547945499420166 - trainLoss: 0.5252729058265686\n",
      "cnt: 0 - valLoss: 0.5479368567466736 - trainLoss: 0.5252596139907837\n",
      "cnt: 0 - valLoss: 0.5479280948638916 - trainLoss: 0.5252463221549988\n",
      "cnt: 0 - valLoss: 0.5479196310043335 - trainLoss: 0.5252330303192139\n",
      "cnt: 0 - valLoss: 0.5479109883308411 - trainLoss: 0.525219738483429\n",
      "cnt: 0 - valLoss: 0.5479024648666382 - trainLoss: 0.5252065062522888\n",
      "cnt: 0 - valLoss: 0.5478938817977905 - trainLoss: 0.5251932144165039\n",
      "cnt: 0 - valLoss: 0.5478854179382324 - trainLoss: 0.5251799821853638\n",
      "cnt: 0 - valLoss: 0.5478769540786743 - trainLoss: 0.5251666903495789\n",
      "cnt: 0 - valLoss: 0.5478684306144714 - trainLoss: 0.525153398513794\n",
      "cnt: 0 - valLoss: 0.5478601455688477 - trainLoss: 0.5251402258872986\n",
      "cnt: 0 - valLoss: 0.5478516221046448 - trainLoss: 0.5251269936561584\n",
      "cnt: 0 - valLoss: 0.547843337059021 - trainLoss: 0.5251137614250183\n",
      "cnt: 0 - valLoss: 0.5478348731994629 - trainLoss: 0.5251005291938782\n",
      "cnt: 0 - valLoss: 0.5478270649909973 - trainLoss: 0.5250873565673828\n",
      "cnt: 0 - valLoss: 0.5478190183639526 - trainLoss: 0.5250741243362427\n",
      "cnt: 0 - valLoss: 0.5478107929229736 - trainLoss: 0.5250609517097473\n",
      "cnt: 0 - valLoss: 0.5478025078773499 - trainLoss: 0.5250478386878967\n",
      "cnt: 0 - valLoss: 0.5477942824363708 - trainLoss: 0.5250346660614014\n",
      "cnt: 0 - valLoss: 0.5477859973907471 - trainLoss: 0.525021493434906\n",
      "cnt: 0 - valLoss: 0.5477777719497681 - trainLoss: 0.5250083208084106\n",
      "cnt: 0 - valLoss: 0.5477695465087891 - trainLoss: 0.5249952077865601\n",
      "cnt: 0 - valLoss: 0.5477612018585205 - trainLoss: 0.5249820947647095\n",
      "cnt: 0 - valLoss: 0.5477529764175415 - trainLoss: 0.5249689221382141\n",
      "cnt: 0 - valLoss: 0.5477447509765625 - trainLoss: 0.5249558687210083\n",
      "cnt: 0 - valLoss: 0.5477365255355835 - trainLoss: 0.5249427556991577\n",
      "cnt: 0 - valLoss: 0.5477286577224731 - trainLoss: 0.5249295830726624\n",
      "cnt: 0 - valLoss: 0.5477204322814941 - trainLoss: 0.5249164700508118\n",
      "cnt: 0 - valLoss: 0.5477116703987122 - trainLoss: 0.5249033570289612\n",
      "cnt: 0 - valLoss: 0.5477035045623779 - trainLoss: 0.5248902440071106\n",
      "cnt: 0 - valLoss: 0.5476953983306885 - trainLoss: 0.5248772501945496\n",
      "cnt: 0 - valLoss: 0.547687292098999 - trainLoss: 0.5248641967773438\n",
      "cnt: 0 - valLoss: 0.5476791262626648 - trainLoss: 0.5248510837554932\n",
      "cnt: 0 - valLoss: 0.5476709008216858 - trainLoss: 0.5248380303382874\n",
      "cnt: 0 - valLoss: 0.5476627945899963 - trainLoss: 0.5248250365257263\n",
      "cnt: 0 - valLoss: 0.5476546287536621 - trainLoss: 0.5248119831085205\n",
      "cnt: 0 - valLoss: 0.5476464629173279 - trainLoss: 0.5247989296913147\n",
      "cnt: 0 - valLoss: 0.5476383566856384 - trainLoss: 0.5247859954833984\n",
      "cnt: 0 - valLoss: 0.547630250453949 - trainLoss: 0.5247728824615479\n",
      "cnt: 0 - valLoss: 0.5476220846176147 - trainLoss: 0.5247599482536316\n",
      "cnt: 0 - valLoss: 0.5476139783859253 - trainLoss: 0.5247468948364258\n",
      "cnt: 0 - valLoss: 0.5476058721542358 - trainLoss: 0.5247339010238647\n",
      "cnt: 0 - valLoss: 0.5475977659225464 - trainLoss: 0.5247209668159485\n",
      "cnt: 0 - valLoss: 0.5475896596908569 - trainLoss: 0.5247079730033875\n",
      "cnt: 0 - valLoss: 0.5475816130638123 - trainLoss: 0.5246950387954712\n",
      "cnt: 0 - valLoss: 0.547573447227478 - trainLoss: 0.5246821045875549\n",
      "cnt: 0 - valLoss: 0.5475654006004333 - trainLoss: 0.5246690511703491\n",
      "cnt: 0 - valLoss: 0.5475573539733887 - trainLoss: 0.5246561765670776\n",
      "cnt: 0 - valLoss: 0.5475492477416992 - trainLoss: 0.5246431827545166\n",
      "cnt: 0 - valLoss: 0.5475412011146545 - trainLoss: 0.5246302485466003\n",
      "cnt: 0 - valLoss: 0.5475332140922546 - trainLoss: 0.5246173143386841\n",
      "cnt: 0 - valLoss: 0.5475251078605652 - trainLoss: 0.5246043801307678\n",
      "cnt: 0 - valLoss: 0.5475171804428101 - trainLoss: 0.5245914459228516\n",
      "cnt: 0 - valLoss: 0.5475091934204102 - trainLoss: 0.5245785713195801\n",
      "cnt: 0 - valLoss: 0.5475011467933655 - trainLoss: 0.5245656371116638\n",
      "cnt: 0 - valLoss: 0.5474931597709656 - trainLoss: 0.5245528221130371\n",
      "cnt: 0 - valLoss: 0.5474851727485657 - trainLoss: 0.5245398879051208\n",
      "cnt: 0 - valLoss: 0.5474772453308105 - trainLoss: 0.5245270133018494\n",
      "cnt: 0 - valLoss: 0.5474692583084106 - trainLoss: 0.5245141386985779\n",
      "cnt: 0 - valLoss: 0.5474613308906555 - trainLoss: 0.5245012640953064\n",
      "cnt: 0 - valLoss: 0.5474534034729004 - trainLoss: 0.5244884490966797\n",
      "cnt: 0 - valLoss: 0.5474454760551453 - trainLoss: 0.5244755744934082\n",
      "cnt: 0 - valLoss: 0.5474374890327454 - trainLoss: 0.5244626998901367\n",
      "cnt: 0 - valLoss: 0.5474295616149902 - trainLoss: 0.52444988489151\n",
      "cnt: 0 - valLoss: 0.5474218130111694 - trainLoss: 0.5244370698928833\n",
      "cnt: 0 - valLoss: 0.5474137663841248 - trainLoss: 0.5244242548942566\n",
      "cnt: 0 - valLoss: 0.5474042296409607 - trainLoss: 0.5244114398956299\n",
      "cnt: 0 - valLoss: 0.5473949909210205 - trainLoss: 0.524398684501648\n",
      "cnt: 0 - valLoss: 0.5473861694335938 - trainLoss: 0.524385929107666\n",
      "cnt: 0 - valLoss: 0.5473761558532715 - trainLoss: 0.5243732333183289\n",
      "cnt: 0 - valLoss: 0.5473954081535339 - trainLoss: 0.5243602991104126\n",
      "cnt: 1 - valLoss: 0.5473905205726624 - trainLoss: 0.5243459939956665\n",
      "cnt: 2 - valLoss: 0.5473845601081848 - trainLoss: 0.5243324637413025\n",
      "cnt: 3 - valLoss: 0.5473777055740356 - trainLoss: 0.5243191123008728\n",
      "cnt: 4 - valLoss: 0.5473701357841492 - trainLoss: 0.5243055820465088\n",
      "cnt: 0 - valLoss: 0.5473621487617493 - trainLoss: 0.5242922306060791\n",
      "cnt: 0 - valLoss: 0.5473535656929016 - trainLoss: 0.5242788791656494\n",
      "cnt: 0 - valLoss: 0.5473447442054749 - trainLoss: 0.524265468120575\n",
      "cnt: 0 - valLoss: 0.5473357439041138 - trainLoss: 0.5242521166801453\n",
      "cnt: 0 - valLoss: 0.5473265051841736 - trainLoss: 0.5242387652397156\n",
      "cnt: 0 - valLoss: 0.5473170876502991 - trainLoss: 0.5242254137992859\n",
      "cnt: 0 - valLoss: 0.5473076105117798 - trainLoss: 0.5242120623588562\n",
      "cnt: 0 - valLoss: 0.5472979545593262 - trainLoss: 0.5241987705230713\n",
      "cnt: 0 - valLoss: 0.5472882986068726 - trainLoss: 0.5241854190826416\n",
      "cnt: 0 - valLoss: 0.5472785830497742 - trainLoss: 0.5241720676422119\n",
      "cnt: 0 - valLoss: 0.5472691655158997 - trainLoss: 0.524158775806427\n",
      "cnt: 0 - valLoss: 0.5472595691680908 - trainLoss: 0.5241454243659973\n",
      "cnt: 0 - valLoss: 0.547249972820282 - trainLoss: 0.5241321921348572\n",
      "cnt: 0 - valLoss: 0.5472403764724731 - trainLoss: 0.5241189002990723\n",
      "cnt: 0 - valLoss: 0.5472306609153748 - trainLoss: 0.5241056084632874\n",
      "cnt: 0 - valLoss: 0.5472209453582764 - trainLoss: 0.5240923166275024\n",
      "cnt: 0 - valLoss: 0.547211229801178 - trainLoss: 0.5240790247917175\n",
      "cnt: 0 - valLoss: 0.5471839308738708 - trainLoss: 0.5240658521652222\n",
      "cnt: 0 - valLoss: 0.5471780300140381 - trainLoss: 0.5240527391433716\n",
      "cnt: 0 - valLoss: 0.5471711754798889 - trainLoss: 0.5240394473075867\n",
      "cnt: 0 - valLoss: 0.5471463203430176 - trainLoss: 0.5240262150764465\n",
      "cnt: 0 - valLoss: 0.5471421480178833 - trainLoss: 0.5240131616592407\n",
      "cnt: 0 - valLoss: 0.5471367239952087 - trainLoss: 0.5239999294281006\n",
      "cnt: 0 - valLoss: 0.5471130013465881 - trainLoss: 0.52398681640625\n",
      "cnt: 0 - valLoss: 0.5471097826957703 - trainLoss: 0.5239736437797546\n",
      "cnt: 0 - valLoss: 0.5470877289772034 - trainLoss: 0.5239604711532593\n",
      "cnt: 0 - valLoss: 0.5470856428146362 - trainLoss: 0.5239474177360535\n",
      "cnt: 0 - valLoss: 0.5470647215843201 - trainLoss: 0.5239343047142029\n",
      "cnt: 0 - valLoss: 0.5470633506774902 - trainLoss: 0.5239211916923523\n",
      "cnt: 0 - valLoss: 0.547042965888977 - trainLoss: 0.5239081978797913\n",
      "cnt: 0 - valLoss: 0.5470250248908997 - trainLoss: 0.5238950252532959\n",
      "cnt: 0 - valLoss: 0.5470259785652161 - trainLoss: 0.5238820910453796\n",
      "cnt: 1 - valLoss: 0.5470074415206909 - trainLoss: 0.5238690376281738\n",
      "cnt: 0 - valLoss: 0.5469909310340881 - trainLoss: 0.5238560438156128\n",
      "cnt: 0 - valLoss: 0.5469930171966553 - trainLoss: 0.523842990398407\n",
      "cnt: 1 - valLoss: 0.546975314617157 - trainLoss: 0.5238300561904907\n",
      "cnt: 0 - valLoss: 0.5469594597816467 - trainLoss: 0.5238170623779297\n",
      "cnt: 0 - valLoss: 0.5468536615371704 - trainLoss: 0.5238029360771179\n",
      "cnt: 0 - valLoss: 0.5467889308929443 - trainLoss: 0.5237762331962585\n",
      "cnt: 0 - valLoss: 0.5467538833618164 - trainLoss: 0.5237581133842468\n",
      "cnt: 0 - valLoss: 0.5467249155044556 - trainLoss: 0.5237430334091187\n",
      "cnt: 0 - valLoss: 0.5467004179954529 - trainLoss: 0.5237283706665039\n",
      "cnt: 0 - valLoss: 0.5466796159744263 - trainLoss: 0.5237140655517578\n",
      "cnt: 0 - valLoss: 0.5466613173484802 - trainLoss: 0.523699939250946\n",
      "cnt: 0 - valLoss: 0.5466449856758118 - trainLoss: 0.5236859321594238\n",
      "cnt: 0 - valLoss: 0.5466300249099731 - trainLoss: 0.5236720442771912\n",
      "cnt: 0 - valLoss: 0.5466161966323853 - trainLoss: 0.5236580967903137\n",
      "cnt: 0 - valLoss: 0.5466032028198242 - trainLoss: 0.523644208908081\n",
      "cnt: 0 - valLoss: 0.5465908646583557 - trainLoss: 0.5236303806304932\n",
      "cnt: 0 - valLoss: 0.5465790629386902 - trainLoss: 0.5236164927482605\n",
      "cnt: 0 - valLoss: 0.5465675592422485 - trainLoss: 0.5236027240753174\n",
      "cnt: 0 - valLoss: 0.546556293964386 - trainLoss: 0.5235888957977295\n",
      "cnt: 0 - valLoss: 0.5465452075004578 - trainLoss: 0.5235750675201416\n",
      "cnt: 0 - valLoss: 0.5465344190597534 - trainLoss: 0.5235612988471985\n",
      "cnt: 0 - valLoss: 0.5465236902236938 - trainLoss: 0.5235474705696106\n",
      "cnt: 0 - valLoss: 0.5465131402015686 - trainLoss: 0.5235337018966675\n",
      "cnt: 0 - valLoss: 0.5465027093887329 - trainLoss: 0.5235199332237244\n",
      "cnt: 0 - valLoss: 0.5464922189712524 - trainLoss: 0.5235061645507812\n",
      "cnt: 0 - valLoss: 0.5464818477630615 - trainLoss: 0.5234923362731934\n",
      "cnt: 0 - valLoss: 0.546471357345581 - trainLoss: 0.5234785676002502\n",
      "cnt: 0 - valLoss: 0.5464608073234558 - trainLoss: 0.5234649181365967\n",
      "cnt: 0 - valLoss: 0.5464504361152649 - trainLoss: 0.5234510898590088\n",
      "cnt: 0 - valLoss: 0.546440064907074 - trainLoss: 0.5234374403953552\n",
      "cnt: 0 - valLoss: 0.5464298129081726 - trainLoss: 0.5234237313270569\n",
      "cnt: 0 - valLoss: 0.5464195609092712 - trainLoss: 0.5234099626541138\n",
      "cnt: 0 - valLoss: 0.5464093685150146 - trainLoss: 0.5233962535858154\n",
      "cnt: 0 - valLoss: 0.5463992357254028 - trainLoss: 0.5233825445175171\n",
      "cnt: 0 - valLoss: 0.5463888645172119 - trainLoss: 0.5233689546585083\n",
      "cnt: 0 - valLoss: 0.5463785529136658 - trainLoss: 0.52335524559021\n",
      "cnt: 0 - valLoss: 0.5463683605194092 - trainLoss: 0.5233415961265564\n",
      "cnt: 0 - valLoss: 0.5463581681251526 - trainLoss: 0.5233278870582581\n",
      "cnt: 0 - valLoss: 0.5463480353355408 - trainLoss: 0.5233142375946045\n",
      "cnt: 0 - valLoss: 0.5463378429412842 - trainLoss: 0.5233006477355957\n",
      "cnt: 0 - valLoss: 0.5463278293609619 - trainLoss: 0.5232869386672974\n",
      "cnt: 0 - valLoss: 0.5463177561759949 - trainLoss: 0.5232733488082886\n",
      "cnt: 0 - valLoss: 0.5463076829910278 - trainLoss: 0.523259699344635\n",
      "cnt: 0 - valLoss: 0.5462976694107056 - trainLoss: 0.5232461094856262\n",
      "cnt: 0 - valLoss: 0.5462875962257385 - trainLoss: 0.5232324600219727\n",
      "cnt: 0 - valLoss: 0.5462775826454163 - trainLoss: 0.5232189297676086\n",
      "cnt: 0 - valLoss: 0.5462676286697388 - trainLoss: 0.5232053399085999\n",
      "cnt: 0 - valLoss: 0.5462575554847717 - trainLoss: 0.5231917500495911\n",
      "cnt: 0 - valLoss: 0.5462476015090942 - trainLoss: 0.5231781601905823\n",
      "cnt: 0 - valLoss: 0.5461722612380981 - trainLoss: 0.5231643915176392\n",
      "cnt: 0 - valLoss: 0.5461148619651794 - trainLoss: 0.5231406092643738\n",
      "cnt: 0 - valLoss: 0.5460700392723083 - trainLoss: 0.5231211185455322\n",
      "cnt: 0 - valLoss: 0.5460338592529297 - trainLoss: 0.5231038331985474\n",
      "cnt: 0 - valLoss: 0.546005368232727 - trainLoss: 0.5230879187583923\n",
      "cnt: 0 - valLoss: 0.5460888743400574 - trainLoss: 0.5230766534805298\n",
      "cnt: 1 - valLoss: 0.5460397005081177 - trainLoss: 0.5230656862258911\n",
      "cnt: 2 - valLoss: 0.5460001230239868 - trainLoss: 0.5230477452278137\n",
      "cnt: 0 - valLoss: 0.5459675788879395 - trainLoss: 0.5230314135551453\n",
      "cnt: 0 - valLoss: 0.5459421873092651 - trainLoss: 0.5230159759521484\n",
      "cnt: 0 - valLoss: 0.5459608435630798 - trainLoss: 0.5230006575584412\n",
      "cnt: 1 - valLoss: 0.5459301471710205 - trainLoss: 0.5229875445365906\n",
      "cnt: 0 - valLoss: 0.5458547472953796 - trainLoss: 0.52297043800354\n",
      "cnt: 0 - valLoss: 0.5458813309669495 - trainLoss: 0.5229620933532715\n",
      "cnt: 1 - valLoss: 0.5459034442901611 - trainLoss: 0.5229384303092957\n",
      "cnt: 2 - valLoss: 0.5458236336708069 - trainLoss: 0.522926390171051\n",
      "cnt: 0 - valLoss: 0.5458493828773499 - trainLoss: 0.522912323474884\n",
      "cnt: 1 - valLoss: 0.5458722710609436 - trainLoss: 0.5228891372680664\n",
      "cnt: 2 - valLoss: 0.5457899570465088 - trainLoss: 0.5228811502456665\n",
      "cnt: 0 - valLoss: 0.5458166003227234 - trainLoss: 0.5228632688522339\n",
      "cnt: 1 - valLoss: 0.5457476377487183 - trainLoss: 0.5228424072265625\n",
      "cnt: 0 - valLoss: 0.5457773208618164 - trainLoss: 0.5228352546691895\n",
      "cnt: 1 - valLoss: 0.5458036661148071 - trainLoss: 0.5228103995323181\n",
      "cnt: 2 - valLoss: 0.5457245111465454 - trainLoss: 0.52280193567276\n",
      "cnt: 0 - valLoss: 0.5457540154457092 - trainLoss: 0.5227839946746826\n",
      "cnt: 1 - valLoss: 0.5456857681274414 - trainLoss: 0.5227649807929993\n",
      "cnt: 0 - valLoss: 0.5457169413566589 - trainLoss: 0.5227555632591248\n",
      "cnt: 1 - valLoss: 0.5456540584564209 - trainLoss: 0.5227314829826355\n",
      "cnt: 0 - valLoss: 0.5456865429878235 - trainLoss: 0.5227257609367371\n",
      "cnt: 1 - valLoss: 0.5456264615058899 - trainLoss: 0.5226995944976807\n",
      "cnt: 0 - valLoss: 0.5456600189208984 - trainLoss: 0.5226951241493225\n",
      "cnt: 1 - valLoss: 0.545600950717926 - trainLoss: 0.5226686596870422\n",
      "cnt: 0 - valLoss: 0.5456355214118958 - trainLoss: 0.522663950920105\n",
      "cnt: 1 - valLoss: 0.5455765724182129 - trainLoss: 0.522638201713562\n",
      "cnt: 0 - valLoss: 0.5456122756004333 - trainLoss: 0.5226325988769531\n",
      "cnt: 1 - valLoss: 0.5455529689788818 - trainLoss: 0.5226081013679504\n",
      "cnt: 0 - valLoss: 0.5455896854400635 - trainLoss: 0.5226010084152222\n",
      "cnt: 1 - valLoss: 0.545529842376709 - trainLoss: 0.5225780606269836\n",
      "cnt: 0 - valLoss: 0.5455673933029175 - trainLoss: 0.5225694179534912\n",
      "cnt: 1 - valLoss: 0.5455068349838257 - trainLoss: 0.5225481986999512\n",
      "cnt: 0 - valLoss: 0.5455453991889954 - trainLoss: 0.5225377082824707\n",
      "cnt: 1 - valLoss: 0.5454840660095215 - trainLoss: 0.5225184559822083\n",
      "cnt: 0 - valLoss: 0.5455231666564941 - trainLoss: 0.5225061774253845\n",
      "cnt: 1 - valLoss: 0.5454610586166382 - trainLoss: 0.5224887132644653\n",
      "cnt: 0 - valLoss: 0.545501172542572 - trainLoss: 0.5224747061729431\n",
      "cnt: 1 - valLoss: 0.5454383492469788 - trainLoss: 0.5224590301513672\n",
      "cnt: 0 - valLoss: 0.545479416847229 - trainLoss: 0.5224432349205017\n",
      "cnt: 1 - valLoss: 0.5454156994819641 - trainLoss: 0.5224294066429138\n",
      "cnt: 0 - valLoss: 0.5454578399658203 - trainLoss: 0.5224118232727051\n",
      "cnt: 1 - valLoss: 0.5453932881355286 - trainLoss: 0.5223998427391052\n",
      "cnt: 0 - valLoss: 0.5454363226890564 - trainLoss: 0.5223804116249084\n",
      "cnt: 1 - valLoss: 0.5453708171844482 - trainLoss: 0.5223702788352966\n",
      "cnt: 0 - valLoss: 0.5454148650169373 - trainLoss: 0.522348940372467\n",
      "cnt: 1 - valLoss: 0.5453484058380127 - trainLoss: 0.5223408341407776\n",
      "cnt: 0 - valLoss: 0.5453934669494629 - trainLoss: 0.52231764793396\n",
      "cnt: 1 - valLoss: 0.5453261137008667 - trainLoss: 0.5223114490509033\n",
      "cnt: 0 - valLoss: 0.5453720688819885 - trainLoss: 0.5222864151000977\n",
      "cnt: 1 - valLoss: 0.5453047156333923 - trainLoss: 0.5222821235656738\n",
      "cnt: 0 - valLoss: 0.5452606081962585 - trainLoss: 0.5222569704055786\n",
      "cnt: 0 - valLoss: 0.545308530330658 - trainLoss: 0.5222502946853638\n",
      "cnt: 1 - valLoss: 0.5452527403831482 - trainLoss: 0.5222322344779968\n",
      "cnt: 0 - valLoss: 0.5453014969825745 - trainLoss: 0.5222142934799194\n",
      "cnt: 1 - valLoss: 0.5452393889427185 - trainLoss: 0.5222057104110718\n",
      "cnt: 0 - valLoss: 0.545198380947113 - trainLoss: 0.5221822261810303\n",
      "cnt: 0 - valLoss: 0.5452486872673035 - trainLoss: 0.5221755504608154\n",
      "cnt: 1 - valLoss: 0.5451937317848206 - trainLoss: 0.5221583247184753\n",
      "cnt: 0 - valLoss: 0.545245885848999 - trainLoss: 0.5221383571624756\n",
      "cnt: 1 - valLoss: 0.5451839566230774 - trainLoss: 0.5221324563026428\n",
      "cnt: 0 - valLoss: 0.5451419353485107 - trainLoss: 0.5221093893051147\n",
      "cnt: 0 - valLoss: 0.5451957583427429 - trainLoss: 0.5220990777015686\n",
      "cnt: 1 - valLoss: 0.5451391935348511 - trainLoss: 0.5220856666564941\n",
      "cnt: 0 - valLoss: 0.5451017022132874 - trainLoss: 0.5220642685890198\n",
      "cnt: 0 - valLoss: 0.5451579689979553 - trainLoss: 0.5220561027526855\n",
      "cnt: 1 - valLoss: 0.5451024770736694 - trainLoss: 0.5220412015914917\n",
      "cnt: 2 - valLoss: 0.545066237449646 - trainLoss: 0.5220205187797546\n",
      "cnt: 0 - valLoss: 0.5451253652572632 - trainLoss: 0.5220115184783936\n",
      "cnt: 1 - valLoss: 0.5450695753097534 - trainLoss: 0.5219976305961609\n",
      "cnt: 2 - valLoss: 0.5450324416160583 - trainLoss: 0.5219771862030029\n",
      "cnt: 0 - valLoss: 0.5450946688652039 - trainLoss: 0.5219663977622986\n",
      "cnt: 1 - valLoss: 0.5450395345687866 - trainLoss: 0.5219545364379883\n",
      "cnt: 2 - valLoss: 0.5449995994567871 - trainLoss: 0.5219342112541199\n",
      "cnt: 0 - valLoss: 0.5450650453567505 - trainLoss: 0.5219210386276245\n",
      "cnt: 1 - valLoss: 0.5450101494789124 - trainLoss: 0.5219116806983948\n",
      "cnt: 2 - valLoss: 0.5449690222740173 - trainLoss: 0.5218914151191711\n",
      "cnt: 0 - valLoss: 0.5450359582901001 - trainLoss: 0.5218756794929504\n",
      "cnt: 1 - valLoss: 0.5449811220169067 - trainLoss: 0.521868884563446\n",
      "cnt: 2 - valLoss: 0.5449392795562744 - trainLoss: 0.5218486785888672\n",
      "cnt: 0 - valLoss: 0.5449098348617554 - trainLoss: 0.521831214427948\n",
      "cnt: 0 - valLoss: 0.5450240969657898 - trainLoss: 0.5218234062194824\n",
      "cnt: 1 - valLoss: 0.5449616312980652 - trainLoss: 0.5218169689178467\n",
      "cnt: 2 - valLoss: 0.5449145436286926 - trainLoss: 0.5217947959899902\n",
      "cnt: 3 - valLoss: 0.5448780059814453 - trainLoss: 0.5217763781547546\n",
      "cnt: 0 - valLoss: 0.5449486970901489 - trainLoss: 0.5217600464820862\n",
      "cnt: 1 - valLoss: 0.544897198677063 - trainLoss: 0.5217545628547668\n",
      "cnt: 2 - valLoss: 0.5448575615882874 - trainLoss: 0.5217351317405701\n",
      "cnt: 0 - valLoss: 0.5448271632194519 - trainLoss: 0.5217182636260986\n",
      "cnt: 0 - valLoss: 0.5449447631835938 - trainLoss: 0.5217078924179077\n",
      "cnt: 1 - valLoss: 0.5448833703994751 - trainLoss: 0.5217040181159973\n",
      "cnt: 2 - valLoss: 0.5448370575904846 - trainLoss: 0.5216822028160095\n",
      "cnt: 3 - valLoss: 0.5448011159896851 - trainLoss: 0.5216639637947083\n",
      "cnt: 0 - valLoss: 0.5448136329650879 - trainLoss: 0.5216498374938965\n",
      "cnt: 1 - valLoss: 0.5447787046432495 - trainLoss: 0.5216361284255981\n",
      "cnt: 0 - valLoss: 0.5447921752929688 - trainLoss: 0.5216232538223267\n",
      "cnt: 1 - valLoss: 0.5447582006454468 - trainLoss: 0.5216084718704224\n",
      "cnt: 0 - valLoss: 0.544772207736969 - trainLoss: 0.5215966105461121\n",
      "cnt: 1 - valLoss: 0.5447805523872375 - trainLoss: 0.5215814709663391\n",
      "cnt: 2 - valLoss: 0.5447421073913574 - trainLoss: 0.5215693116188049\n",
      "cnt: 0 - valLoss: 0.5447534322738647 - trainLoss: 0.521555483341217\n",
      "cnt: 1 - valLoss: 0.5447174310684204 - trainLoss: 0.521541178226471\n",
      "cnt: 0 - valLoss: 0.5447304248809814 - trainLoss: 0.5215292572975159\n",
      "cnt: 1 - valLoss: 0.5447378754615784 - trainLoss: 0.521514356136322\n",
      "cnt: 2 - valLoss: 0.5446986556053162 - trainLoss: 0.5215017795562744\n",
      "cnt: 0 - valLoss: 0.5447095632553101 - trainLoss: 0.5214885473251343\n",
      "cnt: 1 - valLoss: 0.5447155237197876 - trainLoss: 0.5214739441871643\n",
      "cnt: 2 - valLoss: 0.5446747541427612 - trainLoss: 0.5214617252349854\n",
      "cnt: 0 - valLoss: 0.5446848273277283 - trainLoss: 0.5214483737945557\n",
      "cnt: 1 - valLoss: 0.5446902513504028 - trainLoss: 0.52143394947052\n",
      "cnt: 2 - valLoss: 0.5446491241455078 - trainLoss: 0.5214213728904724\n",
      "cnt: 0 - valLoss: 0.5446589589118958 - trainLoss: 0.5214084386825562\n",
      "cnt: 1 - valLoss: 0.5446638464927673 - trainLoss: 0.5213940739631653\n",
      "cnt: 2 - valLoss: 0.5446216464042664 - trainLoss: 0.5213810801506042\n",
      "cnt: 0 - valLoss: 0.5446316003799438 - trainLoss: 0.5213687419891357\n",
      "cnt: 1 - valLoss: 0.5446358323097229 - trainLoss: 0.5213543176651001\n",
      "cnt: 2 - valLoss: 0.544636607170105 - trainLoss: 0.5213407278060913\n",
      "cnt: 3 - valLoss: 0.5445919632911682 - trainLoss: 0.5213286280632019\n",
      "cnt: 0 - valLoss: 0.5445996522903442 - trainLoss: 0.5213155746459961\n",
      "cnt: 1 - valLoss: 0.5446028709411621 - trainLoss: 0.5213016271591187\n",
      "cnt: 2 - valLoss: 0.544603168964386 - trainLoss: 0.5212880969047546\n",
      "cnt: 3 - valLoss: 0.5445578694343567 - trainLoss: 0.5212755799293518\n",
      "cnt: 0 - valLoss: 0.5445647239685059 - trainLoss: 0.5212629437446594\n",
      "cnt: 1 - valLoss: 0.5445679426193237 - trainLoss: 0.5212491154670715\n",
      "cnt: 2 - valLoss: 0.5445681214332581 - trainLoss: 0.5212355852127075\n",
      "cnt: 3 - valLoss: 0.5445227026939392 - trainLoss: 0.5212223529815674\n",
      "cnt: 0 - valLoss: 0.5445296764373779 - trainLoss: 0.5212105512619019\n",
      "cnt: 1 - valLoss: 0.5445325970649719 - trainLoss: 0.521196722984314\n",
      "cnt: 2 - valLoss: 0.5445325970649719 - trainLoss: 0.5211833119392395\n",
      "cnt: 3 - valLoss: 0.5445303320884705 - trainLoss: 0.5211700201034546\n",
      "cnt: 4 - valLoss: 0.5445264577865601 - trainLoss: 0.521156907081604\n",
      "cnt: 5 - valLoss: 0.5444653034210205 - trainLoss: 0.5211446285247803\n",
      "cnt: 0 - valLoss: 0.5444735288619995 - trainLoss: 0.5211327075958252\n",
      "cnt: 1 - valLoss: 0.5444774627685547 - trainLoss: 0.5211188197135925\n",
      "cnt: 2 - valLoss: 0.5444782376289368 - trainLoss: 0.5211052894592285\n",
      "cnt: 3 - valLoss: 0.5444638729095459 - trainLoss: 0.5210921764373779\n",
      "cnt: 0 - valLoss: 0.5444636940956116 - trainLoss: 0.5210793614387512\n",
      "cnt: 0 - valLoss: 0.5444486141204834 - trainLoss: 0.5210666060447693\n",
      "cnt: 0 - valLoss: 0.5444352626800537 - trainLoss: 0.5210537314414978\n",
      "cnt: 0 - valLoss: 0.5444231629371643 - trainLoss: 0.5210409760475159\n",
      "cnt: 0 - valLoss: 0.5444120168685913 - trainLoss: 0.5210282206535339\n",
      "cnt: 0 - valLoss: 0.5444015264511108 - trainLoss: 0.521015465259552\n",
      "cnt: 0 - valLoss: 0.5443916320800781 - trainLoss: 0.5210027694702148\n",
      "cnt: 0 - valLoss: 0.5443820953369141 - trainLoss: 0.5209900736808777\n",
      "cnt: 0 - valLoss: 0.5443727374076843 - trainLoss: 0.5209773778915405\n",
      "cnt: 0 - valLoss: 0.5443636178970337 - trainLoss: 0.5209646821022034\n",
      "cnt: 0 - valLoss: 0.5443547368049622 - trainLoss: 0.5209519863128662\n",
      "cnt: 0 - valLoss: 0.5443459153175354 - trainLoss: 0.5209393501281738\n",
      "cnt: 0 - valLoss: 0.5443371534347534 - trainLoss: 0.5209267139434814\n",
      "cnt: 0 - valLoss: 0.544328510761261 - trainLoss: 0.5209140777587891\n",
      "cnt: 0 - valLoss: 0.5443199276924133 - trainLoss: 0.5209014415740967\n",
      "cnt: 0 - valLoss: 0.5443112850189209 - trainLoss: 0.5208888053894043\n",
      "cnt: 0 - valLoss: 0.544302761554718 - trainLoss: 0.5208761692047119\n",
      "cnt: 0 - valLoss: 0.5442942380905151 - trainLoss: 0.5208635926246643\n",
      "cnt: 0 - valLoss: 0.544285774230957 - trainLoss: 0.5208509564399719\n",
      "cnt: 0 - valLoss: 0.5442772507667542 - trainLoss: 0.5208383202552795\n",
      "cnt: 0 - valLoss: 0.5442687273025513 - trainLoss: 0.5208257436752319\n",
      "cnt: 0 - valLoss: 0.5442603230476379 - trainLoss: 0.5208131670951843\n",
      "cnt: 0 - valLoss: 0.5442517995834351 - trainLoss: 0.5208005905151367\n",
      "cnt: 0 - valLoss: 0.5442434549331665 - trainLoss: 0.5207880139350891\n",
      "cnt: 0 - valLoss: 0.5442349910736084 - trainLoss: 0.5207754969596863\n",
      "cnt: 0 - valLoss: 0.5442265868186951 - trainLoss: 0.5207629203796387\n",
      "cnt: 0 - valLoss: 0.5442181825637817 - trainLoss: 0.5207503437995911\n",
      "cnt: 0 - valLoss: 0.5442097783088684 - trainLoss: 0.5207378268241882\n",
      "cnt: 0 - valLoss: 0.5442013740539551 - trainLoss: 0.5207253098487854\n",
      "cnt: 0 - valLoss: 0.5441930294036865 - trainLoss: 0.5207127928733826\n",
      "cnt: 0 - valLoss: 0.5441846251487732 - trainLoss: 0.5207002758979797\n",
      "cnt: 0 - valLoss: 0.5441763401031494 - trainLoss: 0.5206877589225769\n",
      "cnt: 0 - valLoss: 0.5441679358482361 - trainLoss: 0.5206752419471741\n",
      "cnt: 0 - valLoss: 0.5441596508026123 - trainLoss: 0.5206627249717712\n",
      "cnt: 0 - valLoss: 0.5441510081291199 - trainLoss: 0.520650327205658\n",
      "cnt: 0 - valLoss: 0.544142484664917 - trainLoss: 0.5206378698348999\n",
      "cnt: 0 - valLoss: 0.5441339612007141 - trainLoss: 0.5206253528594971\n",
      "cnt: 0 - valLoss: 0.5441255569458008 - trainLoss: 0.5206129550933838\n",
      "cnt: 0 - valLoss: 0.5441171526908875 - trainLoss: 0.5206004977226257\n",
      "cnt: 0 - valLoss: 0.5440964698791504 - trainLoss: 0.5205880999565125\n",
      "cnt: 0 - valLoss: 0.5440914034843445 - trainLoss: 0.5205758213996887\n",
      "cnt: 0 - valLoss: 0.5440855026245117 - trainLoss: 0.5205633640289307\n",
      "cnt: 0 - valLoss: 0.5440666079521179 - trainLoss: 0.5205509662628174\n",
      "cnt: 0 - valLoss: 0.5440629720687866 - trainLoss: 0.5205387473106384\n",
      "cnt: 0 - valLoss: 0.5440581440925598 - trainLoss: 0.5205262899398804\n",
      "cnt: 0 - valLoss: 0.5440399646759033 - trainLoss: 0.5205141305923462\n",
      "cnt: 0 - valLoss: 0.5440369248390198 - trainLoss: 0.5205017924308777\n",
      "cnt: 0 - valLoss: 0.5440201163291931 - trainLoss: 0.520489513874054\n",
      "cnt: 0 - valLoss: 0.5440179109573364 - trainLoss: 0.5204772353172302\n",
      "cnt: 0 - valLoss: 0.5440018177032471 - trainLoss: 0.5204648971557617\n",
      "cnt: 0 - valLoss: 0.5440002679824829 - trainLoss: 0.520452618598938\n",
      "cnt: 0 - valLoss: 0.5439844727516174 - trainLoss: 0.5204405784606934\n",
      "cnt: 0 - valLoss: 0.5439708828926086 - trainLoss: 0.5204281806945801\n",
      "cnt: 0 - valLoss: 0.5440157055854797 - trainLoss: 0.5204161405563354\n",
      "cnt: 1 - valLoss: 0.5439890027046204 - trainLoss: 0.520406186580658\n",
      "cnt: 2 - valLoss: 0.5439674854278564 - trainLoss: 0.5203929543495178\n",
      "cnt: 0 - valLoss: 0.5439498424530029 - trainLoss: 0.5203802585601807\n",
      "cnt: 0 - valLoss: 0.54393470287323 - trainLoss: 0.5203679203987122\n",
      "cnt: 0 - valLoss: 0.5439658761024475 - trainLoss: 0.5203559994697571\n",
      "cnt: 1 - valLoss: 0.5439419746398926 - trainLoss: 0.5203452706336975\n",
      "cnt: 2 - valLoss: 0.5439224243164062 - trainLoss: 0.520332396030426\n",
      "cnt: 0 - valLoss: 0.543906033039093 - trainLoss: 0.5203198790550232\n",
      "cnt: 0 - valLoss: 0.543936550617218 - trainLoss: 0.5203080773353577\n",
      "cnt: 1 - valLoss: 0.5439118146896362 - trainLoss: 0.5202972888946533\n",
      "cnt: 2 - valLoss: 0.5438918471336365 - trainLoss: 0.5202844142913818\n",
      "cnt: 0 - valLoss: 0.5438750982284546 - trainLoss: 0.520271897315979\n",
      "cnt: 0 - valLoss: 0.5439054369926453 - trainLoss: 0.5202606916427612\n",
      "cnt: 1 - valLoss: 0.5438805222511292 - trainLoss: 0.5202493667602539\n",
      "cnt: 2 - valLoss: 0.5438604354858398 - trainLoss: 0.5202364325523376\n",
      "cnt: 0 - valLoss: 0.543889045715332 - trainLoss: 0.5202240347862244\n",
      "cnt: 1 - valLoss: 0.5438618063926697 - trainLoss: 0.5202139616012573\n",
      "cnt: 2 - valLoss: 0.543840229511261 - trainLoss: 0.5202008485794067\n",
      "cnt: 0 - valLoss: 0.5438714623451233 - trainLoss: 0.5201882719993591\n",
      "cnt: 1 - valLoss: 0.5438400506973267 - trainLoss: 0.5201783180236816\n",
      "cnt: 0 - valLoss: 0.5438181161880493 - trainLoss: 0.520165205001831\n",
      "cnt: 0 - valLoss: 0.5438516139984131 - trainLoss: 0.5201529264450073\n",
      "cnt: 1 - valLoss: 0.5438175201416016 - trainLoss: 0.5201426148414612\n",
      "cnt: 0 - valLoss: 0.5437950491905212 - trainLoss: 0.5201295018196106\n",
      "cnt: 0 - valLoss: 0.5438310503959656 - trainLoss: 0.5201176404953003\n",
      "cnt: 1 - valLoss: 0.5437968373298645 - trainLoss: 0.5201069712638855\n",
      "cnt: 2 - valLoss: 0.5437718033790588 - trainLoss: 0.5200938582420349\n",
      "cnt: 0 - valLoss: 0.5438101887702942 - trainLoss: 0.5200825333595276\n",
      "cnt: 1 - valLoss: 0.5437759160995483 - trainLoss: 0.5200713872909546\n",
      "cnt: 2 - valLoss: 0.5437491536140442 - trainLoss: 0.520058274269104\n",
      "cnt: 0 - valLoss: 0.5437893867492676 - trainLoss: 0.5200473666191101\n",
      "cnt: 1 - valLoss: 0.5437549948692322 - trainLoss: 0.5200358629226685\n",
      "cnt: 2 - valLoss: 0.543728232383728 - trainLoss: 0.5200228095054626\n",
      "cnt: 0 - valLoss: 0.5437685251235962 - trainLoss: 0.5200124382972717\n",
      "cnt: 1 - valLoss: 0.543734073638916 - trainLoss: 0.5200004577636719\n",
      "cnt: 2 - valLoss: 0.5437692999839783 - trainLoss: 0.5199875235557556\n",
      "cnt: 3 - valLoss: 0.5437307953834534 - trainLoss: 0.5199775695800781\n",
      "cnt: 4 - valLoss: 0.5437011122703552 - trainLoss: 0.5199641585350037\n",
      "cnt: 0 - valLoss: 0.5437395572662354 - trainLoss: 0.5199536085128784\n",
      "cnt: 1 - valLoss: 0.543703556060791 - trainLoss: 0.5199417471885681\n",
      "cnt: 2 - valLoss: 0.5437377691268921 - trainLoss: 0.519929051399231\n",
      "cnt: 3 - valLoss: 0.5436984300613403 - trainLoss: 0.5199189186096191\n",
      "cnt: 0 - valLoss: 0.5436680912971497 - trainLoss: 0.5199054479598999\n",
      "cnt: 0 - valLoss: 0.5437062382698059 - trainLoss: 0.5198954939842224\n",
      "cnt: 1 - valLoss: 0.5436698794364929 - trainLoss: 0.5198830366134644\n",
      "cnt: 2 - valLoss: 0.543704092502594 - trainLoss: 0.5198711156845093\n",
      "cnt: 3 - valLoss: 0.5436568260192871 - trainLoss: 0.5198603272438049\n",
      "cnt: 0 - valLoss: 0.5436909794807434 - trainLoss: 0.5198481678962708\n",
      "cnt: 1 - valLoss: 0.543643593788147 - trainLoss: 0.5198373198509216\n",
      "cnt: 0 - valLoss: 0.543677568435669 - trainLoss: 0.5198251008987427\n",
      "cnt: 1 - valLoss: 0.5436301231384277 - trainLoss: 0.5198143720626831\n",
      "cnt: 0 - valLoss: 0.5436563491821289 - trainLoss: 0.5198023915290833\n",
      "cnt: 1 - valLoss: 0.5436110496520996 - trainLoss: 0.5197910666465759\n",
      "cnt: 0 - valLoss: 0.5436388254165649 - trainLoss: 0.5197802186012268\n",
      "cnt: 1 - valLoss: 0.5436040759086609 - trainLoss: 0.5197679400444031\n",
      "cnt: 0 - valLoss: 0.5436300039291382 - trainLoss: 0.5197572708129883\n",
      "cnt: 1 - valLoss: 0.5435938239097595 - trainLoss: 0.5197451710700989\n",
      "cnt: 0 - valLoss: 0.5436189770698547 - trainLoss: 0.5197345614433289\n",
      "cnt: 1 - valLoss: 0.5435819029808044 - trainLoss: 0.5197222232818604\n",
      "cnt: 0 - valLoss: 0.5436162948608398 - trainLoss: 0.5197117328643799\n",
      "cnt: 1 - valLoss: 0.5435761213302612 - trainLoss: 0.51969975233078\n",
      "cnt: 0 - valLoss: 0.5436083674430847 - trainLoss: 0.5196884274482727\n",
      "cnt: 1 - valLoss: 0.5435665249824524 - trainLoss: 0.5196771025657654\n",
      "cnt: 0 - valLoss: 0.5435976982116699 - trainLoss: 0.5196653604507446\n",
      "cnt: 1 - valLoss: 0.5435534119606018 - trainLoss: 0.5196542739868164\n",
      "cnt: 0 - valLoss: 0.543584406375885 - trainLoss: 0.5196425914764404\n",
      "cnt: 1 - valLoss: 0.5435386300086975 - trainLoss: 0.5196313261985779\n",
      "cnt: 0 - valLoss: 0.5435700416564941 - trainLoss: 0.5196200013160706\n",
      "cnt: 1 - valLoss: 0.5435243844985962 - trainLoss: 0.5196084380149841\n",
      "cnt: 0 - valLoss: 0.5435529351234436 - trainLoss: 0.5195974111557007\n",
      "cnt: 1 - valLoss: 0.5435083508491516 - trainLoss: 0.519585371017456\n",
      "cnt: 0 - valLoss: 0.5435374975204468 - trainLoss: 0.5195751190185547\n",
      "cnt: 1 - valLoss: 0.5434934496879578 - trainLoss: 0.5195624828338623\n",
      "cnt: 0 - valLoss: 0.5435230135917664 - trainLoss: 0.5195527076721191\n",
      "cnt: 1 - valLoss: 0.5434791445732117 - trainLoss: 0.5195396542549133\n",
      "cnt: 0 - valLoss: 0.5435090065002441 - trainLoss: 0.5195304155349731\n",
      "cnt: 1 - valLoss: 0.5435290336608887 - trainLoss: 0.5195171236991882\n",
      "cnt: 2 - valLoss: 0.5434775948524475 - trainLoss: 0.5195075273513794\n",
      "cnt: 0 - valLoss: 0.5435025691986084 - trainLoss: 0.5194957256317139\n",
      "cnt: 1 - valLoss: 0.5434547662734985 - trainLoss: 0.5194839239120483\n",
      "cnt: 0 - valLoss: 0.54348224401474 - trainLoss: 0.5194739103317261\n",
      "cnt: 1 - valLoss: 0.543500542640686 - trainLoss: 0.5194610357284546\n",
      "cnt: 2 - valLoss: 0.5434474945068359 - trainLoss: 0.519451379776001\n",
      "cnt: 0 - valLoss: 0.5434715747833252 - trainLoss: 0.5194397568702698\n",
      "cnt: 1 - valLoss: 0.5434229969978333 - trainLoss: 0.5194275379180908\n",
      "cnt: 0 - valLoss: 0.5434497594833374 - trainLoss: 0.5194180607795715\n",
      "cnt: 1 - valLoss: 0.5434677600860596 - trainLoss: 0.5194053053855896\n",
      "cnt: 2 - valLoss: 0.5434145331382751 - trainLoss: 0.5193949937820435\n",
      "cnt: 0 - valLoss: 0.5434382557868958 - trainLoss: 0.5193840265274048\n",
      "cnt: 1 - valLoss: 0.5434540510177612 - trainLoss: 0.519371509552002\n",
      "cnt: 2 - valLoss: 0.5433989763259888 - trainLoss: 0.5193617343902588\n",
      "cnt: 0 - valLoss: 0.5434216260910034 - trainLoss: 0.519350528717041\n",
      "cnt: 1 - valLoss: 0.5434365272521973 - trainLoss: 0.5193381905555725\n",
      "cnt: 2 - valLoss: 0.5433806777000427 - trainLoss: 0.5193281769752502\n",
      "cnt: 0 - valLoss: 0.543402910232544 - trainLoss: 0.5193171501159668\n",
      "cnt: 1 - valLoss: 0.5434172749519348 - trainLoss: 0.5193049311637878\n",
      "cnt: 2 - valLoss: 0.5433611869812012 - trainLoss: 0.5192946195602417\n",
      "cnt: 0 - valLoss: 0.5433834195137024 - trainLoss: 0.5192840099334717\n",
      "cnt: 1 - valLoss: 0.5433977246284485 - trainLoss: 0.519271731376648\n",
      "cnt: 2 - valLoss: 0.5433412790298462 - trainLoss: 0.5192610025405884\n",
      "cnt: 0 - valLoss: 0.5433635711669922 - trainLoss: 0.5192508697509766\n",
      "cnt: 1 - valLoss: 0.5433779358863831 - trainLoss: 0.5192385911941528\n",
      "cnt: 2 - valLoss: 0.5433216094970703 - trainLoss: 0.5192274451255798\n",
      "cnt: 0 - valLoss: 0.5433439016342163 - trainLoss: 0.5192177891731262\n",
      "cnt: 1 - valLoss: 0.5433583855628967 - trainLoss: 0.5192055106163025\n",
      "cnt: 2 - valLoss: 0.5433019399642944 - trainLoss: 0.5191939473152161\n",
      "cnt: 0 - valLoss: 0.5433242321014404 - trainLoss: 0.5191847085952759\n",
      "cnt: 1 - valLoss: 0.5433388352394104 - trainLoss: 0.5191724896430969\n",
      "cnt: 2 - valLoss: 0.5433474779129028 - trainLoss: 0.5191608667373657\n",
      "cnt: 3 - valLoss: 0.5432866215705872 - trainLoss: 0.5191507935523987\n",
      "cnt: 0 - valLoss: 0.5433060526847839 - trainLoss: 0.5191403031349182\n",
      "cnt: 1 - valLoss: 0.543318510055542 - trainLoss: 0.5191283226013184\n",
      "cnt: 2 - valLoss: 0.5433257222175598 - trainLoss: 0.5191169381141663\n",
      "cnt: 3 - valLoss: 0.5432634949684143 - trainLoss: 0.5191066861152649\n",
      "cnt: 0 - valLoss: 0.5432822704315186 - trainLoss: 0.519096314907074\n",
      "cnt: 1 - valLoss: 0.5432942509651184 - trainLoss: 0.5190844535827637\n",
      "cnt: 2 - valLoss: 0.543300986289978 - trainLoss: 0.5190730690956116\n",
      "cnt: 3 - valLoss: 0.5432384610176086 - trainLoss: 0.5190624594688416\n",
      "cnt: 0 - valLoss: 0.5432571172714233 - trainLoss: 0.5190525650978088\n",
      "cnt: 1 - valLoss: 0.5432689189910889 - trainLoss: 0.5190407633781433\n",
      "cnt: 2 - valLoss: 0.5432756543159485 - trainLoss: 0.5190293788909912\n",
      "cnt: 3 - valLoss: 0.5432129502296448 - trainLoss: 0.5190182328224182\n",
      "cnt: 0 - valLoss: 0.5432316064834595 - trainLoss: 0.5190088748931885\n",
      "cnt: 1 - valLoss: 0.5432434678077698 - trainLoss: 0.5189971327781677\n",
      "cnt: 2 - valLoss: 0.5432501435279846 - trainLoss: 0.5189858078956604\n",
      "cnt: 3 - valLoss: 0.5432532429695129 - trainLoss: 0.5189746618270874\n",
      "cnt: 4 - valLoss: 0.54318767786026 - trainLoss: 0.5189639925956726\n",
      "cnt: 0 - valLoss: 0.5432045459747314 - trainLoss: 0.5189542174339294\n",
      "cnt: 1 - valLoss: 0.5432150959968567 - trainLoss: 0.5189425945281982\n",
      "cnt: 2 - valLoss: 0.5432209968566895 - trainLoss: 0.5189313292503357\n",
      "cnt: 3 - valLoss: 0.5432233214378357 - trainLoss: 0.5189203023910522\n",
      "cnt: 4 - valLoss: 0.5432233810424805 - trainLoss: 0.5189093351364136\n",
      "cnt: 5 - valLoss: 0.543155312538147 - trainLoss: 0.5188989639282227\n",
      "cnt: 0 - valLoss: 0.5431708097457886 - trainLoss: 0.5188889503479004\n",
      "cnt: 1 - valLoss: 0.5431802272796631 - trainLoss: 0.5188774466514587\n",
      "cnt: 2 - valLoss: 0.543185293674469 - trainLoss: 0.5188663005828857\n",
      "cnt: 3 - valLoss: 0.5431872010231018 - trainLoss: 0.5188553333282471\n",
      "cnt: 4 - valLoss: 0.5431867241859436 - trainLoss: 0.5188444256782532\n",
      "cnt: 5 - valLoss: 0.5431846380233765 - trainLoss: 0.518833577632904\n",
      "cnt: 6 - valLoss: 0.5431815385818481 - trainLoss: 0.5188227891921997\n",
      "cnt: 7 - valLoss: 0.5431107878684998 - trainLoss: 0.5188122391700745\n",
      "cnt: 0 - valLoss: 0.543124794960022 - trainLoss: 0.5188023447990417\n",
      "cnt: 1 - valLoss: 0.543133020401001 - trainLoss: 0.5187910199165344\n",
      "cnt: 2 - valLoss: 0.5431373715400696 - trainLoss: 0.5187799334526062\n",
      "cnt: 3 - valLoss: 0.5431386828422546 - trainLoss: 0.5187690258026123\n",
      "cnt: 4 - valLoss: 0.5431379079818726 - trainLoss: 0.518758237361908\n",
      "cnt: 5 - valLoss: 0.5431355834007263 - trainLoss: 0.5187474489212036\n",
      "cnt: 6 - valLoss: 0.543131947517395 - trainLoss: 0.5187366604804993\n",
      "cnt: 7 - valLoss: 0.5431275963783264 - trainLoss: 0.5187259912490845\n",
      "cnt: 8 - valLoss: 0.5431226491928101 - trainLoss: 0.5187152624130249\n",
      "cnt: 9 - valLoss: 0.5431174039840698 - trainLoss: 0.5187044739723206\n",
      "cnt: 10 - valLoss: 0.5431118011474609 - trainLoss: 0.5186938047409058\n",
      "cnt: 11 - valLoss: 0.5431060791015625 - trainLoss: 0.518683135509491\n",
      "cnt: 0 - valLoss: 0.5431002378463745 - trainLoss: 0.5186724066734314\n",
      "cnt: 0 - valLoss: 0.5430943965911865 - trainLoss: 0.5186617970466614\n",
      "cnt: 0 - valLoss: 0.5430883765220642 - trainLoss: 0.5186510682106018\n",
      "cnt: 0 - valLoss: 0.5430824160575867 - trainLoss: 0.518640398979187\n",
      "cnt: 0 - valLoss: 0.5430763363838196 - trainLoss: 0.5186296701431274\n",
      "cnt: 0 - valLoss: 0.5430702567100525 - trainLoss: 0.5186190605163574\n",
      "cnt: 0 - valLoss: 0.5430642366409302 - trainLoss: 0.5186083316802979\n",
      "cnt: 0 - valLoss: 0.5430582165718079 - trainLoss: 0.5185977220535278\n",
      "cnt: 0 - valLoss: 0.5430504679679871 - trainLoss: 0.5185871124267578\n",
      "cnt: 0 - valLoss: 0.5430431962013245 - trainLoss: 0.518576443195343\n",
      "cnt: 0 - valLoss: 0.5430362820625305 - trainLoss: 0.518565833568573\n",
      "cnt: 0 - valLoss: 0.5430297255516052 - trainLoss: 0.5185552835464478\n",
      "cnt: 0 - valLoss: 0.5430232882499695 - trainLoss: 0.5185446739196777\n",
      "cnt: 0 - valLoss: 0.5430169105529785 - trainLoss: 0.5185340642929077\n",
      "cnt: 0 - valLoss: 0.5430107116699219 - trainLoss: 0.5185235142707825\n",
      "cnt: 0 - valLoss: 0.5430045127868652 - trainLoss: 0.5185129046440125\n",
      "cnt: 0 - valLoss: 0.5429984927177429 - trainLoss: 0.5185022950172424\n",
      "cnt: 0 - valLoss: 0.542992353439331 - trainLoss: 0.5184917449951172\n",
      "cnt: 0 - valLoss: 0.5429862141609192 - trainLoss: 0.5184811949729919\n",
      "cnt: 0 - valLoss: 0.5429802536964417 - trainLoss: 0.5184706449508667\n",
      "cnt: 0 - valLoss: 0.5429741740226746 - trainLoss: 0.5184600949287415\n",
      "cnt: 0 - valLoss: 0.542968213558197 - trainLoss: 0.5184495449066162\n",
      "cnt: 0 - valLoss: 0.5429621338844299 - trainLoss: 0.5184389352798462\n",
      "cnt: 0 - valLoss: 0.5429562330245972 - trainLoss: 0.5184284448623657\n",
      "cnt: 0 - valLoss: 0.5429502129554749 - trainLoss: 0.5184178948402405\n",
      "cnt: 0 - valLoss: 0.5429442524909973 - trainLoss: 0.5184073448181152\n",
      "cnt: 0 - valLoss: 0.5429382920265198 - trainLoss: 0.5183968544006348\n",
      "cnt: 0 - valLoss: 0.542932391166687 - trainLoss: 0.5183863043785095\n",
      "cnt: 0 - valLoss: 0.5429264307022095 - trainLoss: 0.5183758735656738\n",
      "cnt: 0 - valLoss: 0.5429204106330872 - trainLoss: 0.5183652639389038\n",
      "cnt: 0 - valLoss: 0.5429145693778992 - trainLoss: 0.5183548331260681\n",
      "cnt: 0 - valLoss: 0.5429085493087769 - trainLoss: 0.5183443427085876\n",
      "cnt: 0 - valLoss: 0.5429026484489441 - trainLoss: 0.5183338522911072\n",
      "cnt: 0 - valLoss: 0.5428968667984009 - trainLoss: 0.5183233618736267\n",
      "cnt: 0 - valLoss: 0.542822539806366 - trainLoss: 0.5183130502700806\n",
      "cnt: 0 - valLoss: 0.5430358052253723 - trainLoss: 0.5183097124099731\n",
      "cnt: 1 - valLoss: 0.542956531047821 - trainLoss: 0.5183073878288269\n",
      "cnt: 2 - valLoss: 0.5428987741470337 - trainLoss: 0.5182914137840271\n",
      "cnt: 3 - valLoss: 0.5428142547607422 - trainLoss: 0.5182761549949646\n",
      "cnt: 0 - valLoss: 0.5430266261100769 - trainLoss: 0.5182656049728394\n",
      "cnt: 1 - valLoss: 0.5429437756538391 - trainLoss: 0.5182678699493408\n",
      "cnt: 2 - valLoss: 0.5428835153579712 - trainLoss: 0.5182514190673828\n",
      "cnt: 3 - valLoss: 0.5427953600883484 - trainLoss: 0.5182375311851501\n",
      "cnt: 0 - valLoss: 0.5430088043212891 - trainLoss: 0.5182250142097473\n",
      "cnt: 1 - valLoss: 0.5429248809814453 - trainLoss: 0.5182275772094727\n",
      "cnt: 2 - valLoss: 0.5428640246391296 - trainLoss: 0.5182110071182251\n",
      "cnt: 3 - valLoss: 0.5427737832069397 - trainLoss: 0.518198549747467\n",
      "cnt: 0 - valLoss: 0.5429673194885254 - trainLoss: 0.5181859135627747\n",
      "cnt: 1 - valLoss: 0.5428893566131592 - trainLoss: 0.5181854963302612\n",
      "cnt: 2 - valLoss: 0.5429102182388306 - trainLoss: 0.518170177936554\n",
      "cnt: 3 - valLoss: 0.5428458452224731 - trainLoss: 0.5181613564491272\n",
      "cnt: 4 - valLoss: 0.5427524447441101 - trainLoss: 0.5181507468223572\n",
      "cnt: 0 - valLoss: 0.5428863763809204 - trainLoss: 0.5181370377540588\n",
      "cnt: 1 - valLoss: 0.5428243279457092 - trainLoss: 0.5181311368942261\n",
      "cnt: 2 - valLoss: 0.5427326560020447 - trainLoss: 0.5181213021278381\n",
      "cnt: 0 - valLoss: 0.5428678393363953 - trainLoss: 0.5181099772453308\n",
      "cnt: 1 - valLoss: 0.5428065657615662 - trainLoss: 0.5181013345718384\n",
      "cnt: 2 - valLoss: 0.5427154302597046 - trainLoss: 0.5180923342704773\n",
      "cnt: 0 - valLoss: 0.5428512096405029 - trainLoss: 0.5180824995040894\n",
      "cnt: 1 - valLoss: 0.5428681373596191 - trainLoss: 0.5180716514587402\n",
      "cnt: 2 - valLoss: 0.5428007245063782 - trainLoss: 0.5180631279945374\n",
      "cnt: 3 - valLoss: 0.5428293347358704 - trainLoss: 0.5180529952049255\n",
      "cnt: 4 - valLoss: 0.5428486466407776 - trainLoss: 0.518041729927063\n",
      "cnt: 5 - valLoss: 0.542782723903656 - trainLoss: 0.5180323123931885\n",
      "cnt: 6 - valLoss: 0.5428121089935303 - trainLoss: 0.5180230140686035\n",
      "cnt: 7 - valLoss: 0.5428319573402405 - trainLoss: 0.518011748790741\n",
      "cnt: 8 - valLoss: 0.5427663326263428 - trainLoss: 0.518001914024353\n",
      "cnt: 9 - valLoss: 0.5427961349487305 - trainLoss: 0.5179929733276367\n",
      "cnt: 10 - valLoss: 0.5428162813186646 - trainLoss: 0.5179816484451294\n",
      "cnt: 11 - valLoss: 0.5427504777908325 - trainLoss: 0.5179715156555176\n",
      "cnt: 12 - valLoss: 0.5427803993225098 - trainLoss: 0.5179629921913147\n",
      "cnt: 13 - valLoss: 0.5428003072738647 - trainLoss: 0.5179517269134521\n",
      "cnt: 14 - valLoss: 0.5427346229553223 - trainLoss: 0.5179412364959717\n",
      "cnt: 15 - valLoss: 0.5427645444869995 - trainLoss: 0.5179330706596375\n",
      "cnt: 16 - valLoss: 0.5427846312522888 - trainLoss: 0.5179218649864197\n",
      "cnt: 17 - valLoss: 0.5427976250648499 - trainLoss: 0.5179112553596497\n",
      "cnt: 18 - valLoss: 0.5427268147468567 - trainLoss: 0.5179023146629333\n",
      "cnt: 19 - valLoss: 0.5427532196044922 - trainLoss: 0.5178927183151245\n",
      "cnt: 20 - valLoss: 0.5427709221839905 - trainLoss: 0.5178817510604858\n",
      "cnt: 21 - valLoss: 0.5427822470664978 - trainLoss: 0.5178713798522949\n",
      "cnt: 22 - valLoss: 0.5427099466323853 - trainLoss: 0.5178624987602234\n",
      "cnt: 0 - valLoss: 0.5427356362342834 - trainLoss: 0.5178528428077698\n",
      "cnt: 1 - valLoss: 0.5427526831626892 - trainLoss: 0.5178420543670654\n",
      "cnt: 2 - valLoss: 0.5427635312080383 - trainLoss: 0.5178316235542297\n",
      "cnt: 3 - valLoss: 0.5426909923553467 - trainLoss: 0.5178225040435791\n",
      "cnt: 0 - valLoss: 0.5427164435386658 - trainLoss: 0.5178132057189941\n",
      "cnt: 1 - valLoss: 0.5427332520484924 - trainLoss: 0.517802357673645\n",
      "cnt: 2 - valLoss: 0.5427440404891968 - trainLoss: 0.5177919864654541\n",
      "cnt: 3 - valLoss: 0.5426713824272156 - trainLoss: 0.5177825689315796\n",
      "cnt: 0 - valLoss: 0.5426967740058899 - trainLoss: 0.5177735686302185\n",
      "cnt: 1 - valLoss: 0.5427135825157166 - trainLoss: 0.5177627205848694\n",
      "cnt: 2 - valLoss: 0.5427244305610657 - trainLoss: 0.5177523493766785\n",
      "cnt: 3 - valLoss: 0.5426515340805054 - trainLoss: 0.5177425742149353\n",
      "cnt: 0 - valLoss: 0.5426769852638245 - trainLoss: 0.5177339911460876\n",
      "cnt: 1 - valLoss: 0.5426940321922302 - trainLoss: 0.5177232027053833\n",
      "cnt: 2 - valLoss: 0.5427049398422241 - trainLoss: 0.5177128911018372\n",
      "cnt: 3 - valLoss: 0.5427113771438599 - trainLoss: 0.5177027583122253\n",
      "cnt: 4 - valLoss: 0.5426346063613892 - trainLoss: 0.5176936984062195\n",
      "cnt: 0 - valLoss: 0.5426580309867859 - trainLoss: 0.5176844596862793\n",
      "cnt: 1 - valLoss: 0.5426735281944275 - trainLoss: 0.5176738500595093\n",
      "cnt: 2 - valLoss: 0.5426830649375916 - trainLoss: 0.5176635384559631\n",
      "cnt: 3 - valLoss: 0.5426884889602661 - trainLoss: 0.5176535248756409\n",
      "cnt: 4 - valLoss: 0.542611300945282 - trainLoss: 0.5176441073417664\n",
      "cnt: 0 - valLoss: 0.5426340699195862 - trainLoss: 0.5176352858543396\n",
      "cnt: 1 - valLoss: 0.5426493287086487 - trainLoss: 0.5176246762275696\n",
      "cnt: 2 - valLoss: 0.5426589250564575 - trainLoss: 0.517614483833313\n",
      "cnt: 3 - valLoss: 0.5426644086837769 - trainLoss: 0.517604410648346\n",
      "cnt: 4 - valLoss: 0.5425870418548584 - trainLoss: 0.5175946354866028\n",
      "cnt: 0 - valLoss: 0.5426099896430969 - trainLoss: 0.5175862312316895\n",
      "cnt: 1 - valLoss: 0.5426252484321594 - trainLoss: 0.5175756216049194\n",
      "cnt: 2 - valLoss: 0.5426348447799683 - trainLoss: 0.5175654292106628\n",
      "cnt: 3 - valLoss: 0.5426405072212219 - trainLoss: 0.5175555348396301\n",
      "cnt: 4 - valLoss: 0.5426412224769592 - trainLoss: 0.5175457000732422\n",
      "cnt: 5 - valLoss: 0.5426404476165771 - trainLoss: 0.517535924911499\n",
      "cnt: 6 - valLoss: 0.5425578951835632 - trainLoss: 0.5175264477729797\n",
      "cnt: 0 - valLoss: 0.542579710483551 - trainLoss: 0.5175177454948425\n",
      "cnt: 1 - valLoss: 0.5425944924354553 - trainLoss: 0.5175072550773621\n",
      "cnt: 2 - valLoss: 0.5426015853881836 - trainLoss: 0.5174972414970398\n",
      "cnt: 3 - valLoss: 0.542605459690094 - trainLoss: 0.5174873471260071\n",
      "cnt: 4 - valLoss: 0.5426070094108582 - trainLoss: 0.5174775123596191\n",
      "cnt: 5 - valLoss: 0.5426067113876343 - trainLoss: 0.5174677968025208\n",
      "cnt: 6 - valLoss: 0.5426052212715149 - trainLoss: 0.5174580812454224\n",
      "cnt: 7 - valLoss: 0.5426028966903687 - trainLoss: 0.5174484252929688\n",
      "cnt: 8 - valLoss: 0.5425998568534851 - trainLoss: 0.5174387693405151\n",
      "cnt: 9 - valLoss: 0.5425963997840881 - trainLoss: 0.5174291133880615\n",
      "cnt: 10 - valLoss: 0.5425926446914673 - trainLoss: 0.5174193978309631\n",
      "cnt: 11 - valLoss: 0.5425886511802673 - trainLoss: 0.5174096822738647\n",
      "cnt: 12 - valLoss: 0.5425844788551331 - trainLoss: 0.5174000263214111\n",
      "cnt: 13 - valLoss: 0.542579710483551 - trainLoss: 0.5173903703689575\n",
      "cnt: 14 - valLoss: 0.5425750017166138 - trainLoss: 0.5173807740211487\n",
      "cnt: 15 - valLoss: 0.5425703525543213 - trainLoss: 0.5173711180686951\n",
      "cnt: 16 - valLoss: 0.5425657629966736 - trainLoss: 0.5173615217208862\n",
      "cnt: 17 - valLoss: 0.5425611138343811 - trainLoss: 0.5173519253730774\n",
      "cnt: 18 - valLoss: 0.5425565838813782 - trainLoss: 0.5173422694206238\n",
      "cnt: 0 - valLoss: 0.5425519943237305 - trainLoss: 0.5173326730728149\n",
      "cnt: 0 - valLoss: 0.5425474643707275 - trainLoss: 0.5173231363296509\n",
      "cnt: 0 - valLoss: 0.5425429344177246 - trainLoss: 0.5173134803771973\n",
      "cnt: 0 - valLoss: 0.5425384044647217 - trainLoss: 0.5173039436340332\n",
      "cnt: 0 - valLoss: 0.5425338745117188 - trainLoss: 0.5172942876815796\n",
      "cnt: 0 - valLoss: 0.5425293445587158 - trainLoss: 0.5172846913337708\n",
      "cnt: 0 - valLoss: 0.5425248742103577 - trainLoss: 0.5172751545906067\n",
      "cnt: 0 - valLoss: 0.5425203442573547 - trainLoss: 0.5172655582427979\n",
      "cnt: 0 - valLoss: 0.5425158739089966 - trainLoss: 0.5172560214996338\n",
      "cnt: 0 - valLoss: 0.5425113439559937 - trainLoss: 0.5172464847564697\n",
      "cnt: 0 - valLoss: 0.5425068736076355 - trainLoss: 0.5172368884086609\n",
      "cnt: 0 - valLoss: 0.5425024032592773 - trainLoss: 0.5172273516654968\n",
      "cnt: 0 - valLoss: 0.5424979329109192 - trainLoss: 0.5172178149223328\n",
      "cnt: 0 - valLoss: 0.5424935221672058 - trainLoss: 0.5172082185745239\n",
      "cnt: 0 - valLoss: 0.5424890518188477 - trainLoss: 0.5171986818313599\n",
      "cnt: 0 - valLoss: 0.5424845814704895 - trainLoss: 0.5171892046928406\n",
      "cnt: 0 - valLoss: 0.5424801111221313 - trainLoss: 0.5171796679496765\n",
      "cnt: 0 - valLoss: 0.542475700378418 - trainLoss: 0.5171700716018677\n",
      "cnt: 0 - valLoss: 0.5424712300300598 - trainLoss: 0.5171605944633484\n",
      "cnt: 0 - valLoss: 0.5424667596817017 - trainLoss: 0.5171510577201843\n",
      "cnt: 0 - valLoss: 0.5424622893333435 - trainLoss: 0.5171415209770203\n",
      "cnt: 0 - valLoss: 0.5424578189849854 - trainLoss: 0.5171319842338562\n",
      "cnt: 0 - valLoss: 0.5424532890319824 - trainLoss: 0.5171224474906921\n",
      "cnt: 0 - valLoss: 0.5424488186836243 - trainLoss: 0.5171129703521729\n",
      "cnt: 0 - valLoss: 0.5424444079399109 - trainLoss: 0.5171034336090088\n",
      "cnt: 0 - valLoss: 0.5424399375915527 - trainLoss: 0.5170939564704895\n",
      "cnt: 0 - valLoss: 0.5424354076385498 - trainLoss: 0.5170844793319702\n",
      "cnt: 0 - valLoss: 0.542431116104126 - trainLoss: 0.5170750021934509\n",
      "cnt: 0 - valLoss: 0.5424266457557678 - trainLoss: 0.5170655250549316\n",
      "cnt: 0 - valLoss: 0.5424221754074097 - trainLoss: 0.5170560479164124\n",
      "cnt: 0 - valLoss: 0.5424178242683411 - trainLoss: 0.5170465111732483\n",
      "cnt: 0 - valLoss: 0.5424134135246277 - trainLoss: 0.517037034034729\n",
      "cnt: 0 - valLoss: 0.5424090027809143 - trainLoss: 0.5170276165008545\n",
      "cnt: 0 - valLoss: 0.5424045920372009 - trainLoss: 0.5170180797576904\n",
      "cnt: 0 - valLoss: 0.5424002408981323 - trainLoss: 0.5170086026191711\n",
      "cnt: 0 - valLoss: 0.542395830154419 - trainLoss: 0.5169991850852966\n",
      "cnt: 0 - valLoss: 0.5423914194107056 - trainLoss: 0.5169897675514221\n",
      "cnt: 0 - valLoss: 0.5423871278762817 - trainLoss: 0.5169802904129028\n",
      "cnt: 0 - valLoss: 0.5423827171325684 - trainLoss: 0.5169708728790283\n",
      "cnt: 0 - valLoss: 0.5423783659934998 - trainLoss: 0.5169614553451538\n",
      "cnt: 0 - valLoss: 0.5423738956451416 - trainLoss: 0.5169519186019897\n",
      "cnt: 0 - valLoss: 0.5423696041107178 - trainLoss: 0.5169425010681152\n",
      "cnt: 0 - valLoss: 0.5423652529716492 - trainLoss: 0.5169330835342407\n",
      "cnt: 0 - valLoss: 0.5423609018325806 - trainLoss: 0.5169236660003662\n",
      "cnt: 0 - valLoss: 0.542356550693512 - trainLoss: 0.5169142484664917\n",
      "cnt: 0 - valLoss: 0.5423521995544434 - trainLoss: 0.5169048309326172\n",
      "cnt: 0 - valLoss: 0.5423478484153748 - trainLoss: 0.5168954133987427\n",
      "cnt: 0 - valLoss: 0.5423435568809509 - trainLoss: 0.5168859958648682\n",
      "cnt: 0 - valLoss: 0.5423392653465271 - trainLoss: 0.5168765783309937\n",
      "cnt: 0 - valLoss: 0.5423349142074585 - trainLoss: 0.5168671607971191\n",
      "cnt: 0 - valLoss: 0.5423305630683899 - trainLoss: 0.5168578624725342\n",
      "cnt: 0 - valLoss: 0.5423262715339661 - trainLoss: 0.5168484449386597\n",
      "cnt: 0 - valLoss: 0.5423218607902527 - trainLoss: 0.5168390870094299\n",
      "cnt: 0 - valLoss: 0.5423175692558289 - trainLoss: 0.5168296694755554\n",
      "cnt: 0 - valLoss: 0.5423133373260498 - trainLoss: 0.5168203115463257\n",
      "cnt: 0 - valLoss: 0.5423089265823364 - trainLoss: 0.5168108940124512\n",
      "cnt: 0 - valLoss: 0.5423046350479126 - trainLoss: 0.5168015360832214\n",
      "cnt: 0 - valLoss: 0.5423004031181335 - trainLoss: 0.5167921781539917\n",
      "cnt: 0 - valLoss: 0.5422960519790649 - trainLoss: 0.516782820224762\n",
      "cnt: 0 - valLoss: 0.5422918200492859 - trainLoss: 0.516773521900177\n",
      "cnt: 0 - valLoss: 0.5422874689102173 - trainLoss: 0.5167641043663025\n",
      "cnt: 0 - valLoss: 0.5422831773757935 - trainLoss: 0.5167548060417175\n",
      "cnt: 0 - valLoss: 0.5422788858413696 - trainLoss: 0.5167454481124878\n",
      "cnt: 0 - valLoss: 0.5422745943069458 - trainLoss: 0.5167360305786133\n",
      "cnt: 0 - valLoss: 0.5422703623771667 - trainLoss: 0.5167267918586731\n",
      "cnt: 0 - valLoss: 0.5422660708427429 - trainLoss: 0.5167174935340881\n",
      "cnt: 0 - valLoss: 0.5422617197036743 - trainLoss: 0.5167080760002136\n",
      "cnt: 0 - valLoss: 0.5422574877738953 - trainLoss: 0.5166987180709839\n",
      "cnt: 0 - valLoss: 0.5424867868423462 - trainLoss: 0.5166895985603333\n",
      "cnt: 1 - valLoss: 0.5423330664634705 - trainLoss: 0.516701340675354\n",
      "cnt: 2 - valLoss: 0.5422189831733704 - trainLoss: 0.5166727900505066\n",
      "cnt: 0 - valLoss: 0.5424553155899048 - trainLoss: 0.5166689157485962\n",
      "cnt: 1 - valLoss: 0.5423077344894409 - trainLoss: 0.5166706442832947\n",
      "cnt: 2 - valLoss: 0.5422826409339905 - trainLoss: 0.5166439414024353\n",
      "cnt: 3 - valLoss: 0.5422638654708862 - trainLoss: 0.5166342854499817\n",
      "cnt: 4 - valLoss: 0.5422493815422058 - trainLoss: 0.5166248083114624\n",
      "cnt: 5 - valLoss: 0.5422376394271851 - trainLoss: 0.5166154503822327\n",
      "cnt: 6 - valLoss: 0.5422282218933105 - trainLoss: 0.5166060924530029\n",
      "cnt: 7 - valLoss: 0.5422203540802002 - trainLoss: 0.516596794128418\n",
      "cnt: 8 - valLoss: 0.5424524545669556 - trainLoss: 0.516588568687439\n",
      "cnt: 9 - valLoss: 0.5422902703285217 - trainLoss: 0.5165994167327881\n",
      "cnt: 10 - valLoss: 0.5421746373176575 - trainLoss: 0.5165704488754272\n",
      "cnt: 0 - valLoss: 0.5424153804779053 - trainLoss: 0.5165697932243347\n",
      "cnt: 1 - valLoss: 0.5422614216804504 - trainLoss: 0.5165682435035706\n",
      "cnt: 2 - valLoss: 0.5422345995903015 - trainLoss: 0.5165424346923828\n",
      "cnt: 3 - valLoss: 0.5422161221504211 - trainLoss: 0.5165327787399292\n",
      "cnt: 4 - valLoss: 0.5422002673149109 - trainLoss: 0.5165233612060547\n",
      "cnt: 5 - valLoss: 0.5424327850341797 - trainLoss: 0.5165141820907593\n",
      "cnt: 6 - valLoss: 0.5422637462615967 - trainLoss: 0.5165265798568726\n",
      "cnt: 7 - valLoss: 0.5421443581581116 - trainLoss: 0.5164973139762878\n",
      "cnt: 0 - valLoss: 0.5423874855041504 - trainLoss: 0.5164979100227356\n",
      "cnt: 1 - valLoss: 0.5422297120094299 - trainLoss: 0.5164943933486938\n",
      "cnt: 2 - valLoss: 0.5422024726867676 - trainLoss: 0.5164692401885986\n",
      "cnt: 3 - valLoss: 0.5421821475028992 - trainLoss: 0.516459584236145\n",
      "cnt: 4 - valLoss: 0.5421666502952576 - trainLoss: 0.5164502263069153\n",
      "cnt: 5 - valLoss: 0.5424027442932129 - trainLoss: 0.5164433717727661\n",
      "cnt: 6 - valLoss: 0.5422314405441284 - trainLoss: 0.5164526104927063\n",
      "cnt: 7 - valLoss: 0.5422061681747437 - trainLoss: 0.5164241194725037\n",
      "cnt: 8 - valLoss: 0.5421784520149231 - trainLoss: 0.5164144039154053\n",
      "cnt: 9 - valLoss: 0.5421579480171204 - trainLoss: 0.5164048671722412\n",
      "cnt: 10 - valLoss: 0.5423934459686279 - trainLoss: 0.5163964629173279\n",
      "cnt: 11 - valLoss: 0.5422189831733704 - trainLoss: 0.5164077281951904\n",
      "cnt: 12 - valLoss: 0.5421913266181946 - trainLoss: 0.5163784623146057\n",
      "cnt: 13 - valLoss: 0.5421709418296814 - trainLoss: 0.5163688063621521\n",
      "cnt: 14 - valLoss: 0.5421555042266846 - trainLoss: 0.5163593888282776\n",
      "cnt: 15 - valLoss: 0.5421436429023743 - trainLoss: 0.5163501501083374\n",
      "cnt: 0 - valLoss: 0.5423794984817505 - trainLoss: 0.5163410902023315\n",
      "cnt: 1 - valLoss: 0.5422021150588989 - trainLoss: 0.5163535475730896\n",
      "cnt: 2 - valLoss: 0.5421727895736694 - trainLoss: 0.5163236260414124\n",
      "cnt: 3 - valLoss: 0.5421512722969055 - trainLoss: 0.5163139700889587\n",
      "cnt: 4 - valLoss: 0.5421350598335266 - trainLoss: 0.516304612159729\n",
      "cnt: 0 - valLoss: 0.542124330997467 - trainLoss: 0.5162952542304993\n",
      "cnt: 0 - valLoss: 0.5423688292503357 - trainLoss: 0.5162872076034546\n",
      "cnt: 1 - valLoss: 0.5421874523162842 - trainLoss: 0.5162999033927917\n",
      "cnt: 2 - valLoss: 0.542155921459198 - trainLoss: 0.5162689685821533\n",
      "cnt: 3 - valLoss: 0.5421327948570251 - trainLoss: 0.5162593126296997\n",
      "cnt: 4 - valLoss: 0.5421155095100403 - trainLoss: 0.5162498950958252\n",
      "cnt: 0 - valLoss: 0.5423597097396851 - trainLoss: 0.5162410140037537\n",
      "cnt: 1 - valLoss: 0.5421752333641052 - trainLoss: 0.5162553191184998\n",
      "cnt: 2 - valLoss: 0.5421417951583862 - trainLoss: 0.5162236094474792\n",
      "cnt: 3 - valLoss: 0.5421174168586731 - trainLoss: 0.5162138938903809\n",
      "cnt: 4 - valLoss: 0.5420992374420166 - trainLoss: 0.5162044763565063\n",
      "cnt: 0 - valLoss: 0.5423433780670166 - trainLoss: 0.5161964297294617\n",
      "cnt: 1 - valLoss: 0.5421580076217651 - trainLoss: 0.5162098407745361\n",
      "cnt: 2 - valLoss: 0.5421243906021118 - trainLoss: 0.5161782503128052\n",
      "cnt: 3 - valLoss: 0.5420997142791748 - trainLoss: 0.5161685347557068\n",
      "cnt: 4 - valLoss: 0.542081892490387 - trainLoss: 0.516159176826477\n",
      "cnt: 0 - valLoss: 0.5423265695571899 - trainLoss: 0.5161523222923279\n",
      "cnt: 1 - valLoss: 0.5421407222747803 - trainLoss: 0.5161643624305725\n",
      "cnt: 2 - valLoss: 0.5421071648597717 - trainLoss: 0.5161330699920654\n",
      "cnt: 3 - valLoss: 0.5420827269554138 - trainLoss: 0.516123354434967\n",
      "cnt: 4 - valLoss: 0.5423215627670288 - trainLoss: 0.5161141157150269\n",
      "cnt: 5 - valLoss: 0.5421324968338013 - trainLoss: 0.5161293745040894\n",
      "cnt: 6 - valLoss: 0.5420970916748047 - trainLoss: 0.5160971283912659\n",
      "cnt: 7 - valLoss: 0.5420714020729065 - trainLoss: 0.5160874128341675\n",
      "cnt: 0 - valLoss: 0.5423114895820618 - trainLoss: 0.5160785913467407\n",
      "cnt: 1 - valLoss: 0.5421214699745178 - trainLoss: 0.5160936713218689\n",
      "cnt: 2 - valLoss: 0.5420850515365601 - trainLoss: 0.5160613059997559\n",
      "cnt: 3 - valLoss: 0.5420587062835693 - trainLoss: 0.5160514712333679\n",
      "cnt: 0 - valLoss: 0.5422987937927246 - trainLoss: 0.5160433650016785\n",
      "cnt: 1 - valLoss: 0.5421080589294434 - trainLoss: 0.5160577297210693\n",
      "cnt: 2 - valLoss: 0.5420714616775513 - trainLoss: 0.5160254240036011\n",
      "cnt: 3 - valLoss: 0.5420448780059814 - trainLoss: 0.5160157084465027\n",
      "cnt: 0 - valLoss: 0.5422852635383606 - trainLoss: 0.5160084366798401\n",
      "cnt: 1 - valLoss: 0.5420941710472107 - trainLoss: 0.5160216689109802\n",
      "cnt: 2 - valLoss: 0.5420575141906738 - trainLoss: 0.5159896016120911\n",
      "cnt: 3 - valLoss: 0.5420308709144592 - trainLoss: 0.5159798860549927\n",
      "cnt: 0 - valLoss: 0.542269766330719 - trainLoss: 0.515973687171936\n",
      "cnt: 1 - valLoss: 0.5420789122581482 - trainLoss: 0.5159854888916016\n",
      "cnt: 2 - valLoss: 0.54204261302948 - trainLoss: 0.5159538984298706\n",
      "cnt: 3 - valLoss: 0.5420162081718445 - trainLoss: 0.5159440636634827\n",
      "cnt: 0 - valLoss: 0.54225754737854 - trainLoss: 0.5159390568733215\n",
      "cnt: 1 - valLoss: 0.5420659184455872 - trainLoss: 0.5159497261047363\n",
      "cnt: 2 - valLoss: 0.5420292615890503 - trainLoss: 0.5159181952476501\n",
      "cnt: 3 - valLoss: 0.5420027375221252 - trainLoss: 0.5159084796905518\n",
      "cnt: 0 - valLoss: 0.5422441959381104 - trainLoss: 0.5159043073654175\n",
      "cnt: 1 - valLoss: 0.5420522093772888 - trainLoss: 0.5159139037132263\n",
      "cnt: 2 - valLoss: 0.5420153737068176 - trainLoss: 0.5158825516700745\n",
      "cnt: 3 - valLoss: 0.5422478914260864 - trainLoss: 0.5158737897872925\n",
      "cnt: 4 - valLoss: 0.5420511364936829 - trainLoss: 0.5158891677856445\n",
      "cnt: 5 - valLoss: 0.5420107841491699 - trainLoss: 0.515856146812439\n",
      "cnt: 6 - valLoss: 0.5422419905662537 - trainLoss: 0.5158466100692749\n",
      "cnt: 7 - valLoss: 0.5420436263084412 - trainLoss: 0.5158630013465881\n",
      "cnt: 8 - valLoss: 0.5420023798942566 - trainLoss: 0.5158295035362244\n",
      "cnt: 0 - valLoss: 0.5422351360321045 - trainLoss: 0.515820324420929\n",
      "cnt: 1 - valLoss: 0.5420348644256592 - trainLoss: 0.5158365964889526\n",
      "cnt: 2 - valLoss: 0.5419931411743164 - trainLoss: 0.5158028602600098\n",
      "cnt: 0 - valLoss: 0.5422239899635315 - trainLoss: 0.5157941579818726\n",
      "cnt: 1 - valLoss: 0.5420238375663757 - trainLoss: 0.5158095955848694\n",
      "cnt: 2 - valLoss: 0.5419823527336121 - trainLoss: 0.5157762765884399\n",
      "cnt: 0 - valLoss: 0.5422136187553406 - trainLoss: 0.51576828956604\n",
      "cnt: 1 - valLoss: 0.5420134663581848 - trainLoss: 0.5157827734947205\n",
      "cnt: 2 - valLoss: 0.5419719815254211 - trainLoss: 0.5157496333122253\n",
      "cnt: 0 - valLoss: 0.5422054529190063 - trainLoss: 0.5157424807548523\n",
      "cnt: 1 - valLoss: 0.5420044660568237 - trainLoss: 0.5157563090324402\n",
      "cnt: 2 - valLoss: 0.5419625639915466 - trainLoss: 0.5157229900360107\n",
      "cnt: 0 - valLoss: 0.5421943068504333 - trainLoss: 0.5157164335250854\n",
      "cnt: 1 - valLoss: 0.5419934988021851 - trainLoss: 0.5157293677330017\n",
      "cnt: 2 - valLoss: 0.5419518947601318 - trainLoss: 0.5156964659690857\n",
      "cnt: 0 - valLoss: 0.54218590259552 - trainLoss: 0.5156907439231873\n",
      "cnt: 1 - valLoss: 0.5419843196868896 - trainLoss: 0.5157029628753662\n",
      "cnt: 2 - valLoss: 0.5419424772262573 - trainLoss: 0.5156700015068054\n",
      "cnt: 0 - valLoss: 0.5421746373176575 - trainLoss: 0.51566481590271\n",
      "cnt: 1 - valLoss: 0.541973352432251 - trainLoss: 0.5156760811805725\n",
      "cnt: 2 - valLoss: 0.541931688785553 - trainLoss: 0.5156435370445251\n",
      "cnt: 0 - valLoss: 0.5421659350395203 - trainLoss: 0.5156391859054565\n",
      "cnt: 1 - valLoss: 0.5419639945030212 - trainLoss: 0.515649676322937\n",
      "cnt: 2 - valLoss: 0.5419221520423889 - trainLoss: 0.5156171321868896\n",
      "cnt: 0 - valLoss: 0.5421546101570129 - trainLoss: 0.5156133770942688\n",
      "cnt: 1 - valLoss: 0.5419507026672363 - trainLoss: 0.5156229138374329\n",
      "cnt: 2 - valLoss: 0.5419095158576965 - trainLoss: 0.5155906081199646\n",
      "cnt: 0 - valLoss: 0.542142927646637 - trainLoss: 0.515588104724884\n",
      "cnt: 1 - valLoss: 0.541939377784729 - trainLoss: 0.5155960917472839\n",
      "cnt: 2 - valLoss: 0.5421624183654785 - trainLoss: 0.5155643224716187\n",
      "cnt: 3 - valLoss: 0.5419493913650513 - trainLoss: 0.5155824422836304\n",
      "cnt: 4 - valLoss: 0.5419033765792847 - trainLoss: 0.5155472755432129\n",
      "cnt: 0 - valLoss: 0.5421363711357117 - trainLoss: 0.5155434608459473\n",
      "cnt: 1 - valLoss: 0.5419283509254456 - trainLoss: 0.5155534148216248\n",
      "cnt: 2 - valLoss: 0.5421505570411682 - trainLoss: 0.5155206322669983\n",
      "cnt: 3 - valLoss: 0.5419355034828186 - trainLoss: 0.5155391097068787\n",
      "cnt: 4 - valLoss: 0.5418887138366699 - trainLoss: 0.515503466129303\n",
      "cnt: 0 - valLoss: 0.5421199798583984 - trainLoss: 0.5155004262924194\n",
      "cnt: 1 - valLoss: 0.5419117212295532 - trainLoss: 0.5155094265937805\n",
      "cnt: 2 - valLoss: 0.5421346426010132 - trainLoss: 0.5154780149459839\n",
      "cnt: 3 - valLoss: 0.5419192314147949 - trainLoss: 0.5154951214790344\n",
      "cnt: 4 - valLoss: 0.5418723225593567 - trainLoss: 0.5154597759246826\n",
      "cnt: 0 - valLoss: 0.5421062707901001 - trainLoss: 0.5154579281806946\n",
      "cnt: 1 - valLoss: 0.5418968796730042 - trainLoss: 0.5154658555984497\n",
      "cnt: 2 - valLoss: 0.542120099067688 - trainLoss: 0.5154352784156799\n",
      "cnt: 3 - valLoss: 0.5419037342071533 - trainLoss: 0.5154515504837036\n",
      "cnt: 4 - valLoss: 0.5418567657470703 - trainLoss: 0.515416145324707\n",
      "cnt: 0 - valLoss: 0.5420892238616943 - trainLoss: 0.515415370464325\n",
      "cnt: 1 - valLoss: 0.5418793559074402 - trainLoss: 0.5154219269752502\n",
      "cnt: 2 - valLoss: 0.5421037077903748 - trainLoss: 0.515393078327179\n",
      "cnt: 3 - valLoss: 0.5418866872787476 - trainLoss: 0.5154076814651489\n",
      "cnt: 4 - valLoss: 0.5421072244644165 - trainLoss: 0.5153737664222717\n",
      "cnt: 5 - valLoss: 0.5418868064880371 - trainLoss: 0.5153918266296387\n",
      "cnt: 6 - valLoss: 0.5421057343482971 - trainLoss: 0.5153557062149048\n",
      "cnt: 7 - valLoss: 0.5418834686279297 - trainLoss: 0.5153751373291016\n",
      "cnt: 8 - valLoss: 0.5418338775634766 - trainLoss: 0.5153383612632751\n",
      "cnt: 0 - valLoss: 0.5420677065849304 - trainLoss: 0.5153380632400513\n",
      "cnt: 1 - valLoss: 0.5418549180030823 - trainLoss: 0.5153444409370422\n",
      "cnt: 2 - valLoss: 0.5420791506767273 - trainLoss: 0.5153162479400635\n",
      "cnt: 3 - valLoss: 0.5418602824211121 - trainLoss: 0.5153298377990723\n",
      "cnt: 4 - valLoss: 0.5420789122581482 - trainLoss: 0.5152974128723145\n",
      "cnt: 5 - valLoss: 0.541857898235321 - trainLoss: 0.5153133869171143\n",
      "cnt: 6 - valLoss: 0.5420753955841064 - trainLoss: 0.5152798295021057\n",
      "cnt: 7 - valLoss: 0.5418535470962524 - trainLoss: 0.5152965188026428\n",
      "cnt: 8 - valLoss: 0.5420708060264587 - trainLoss: 0.5152628421783447\n",
      "cnt: 9 - valLoss: 0.5418482422828674 - trainLoss: 0.5152794122695923\n",
      "cnt: 10 - valLoss: 0.5420653820037842 - trainLoss: 0.5152458548545837\n",
      "cnt: 11 - valLoss: 0.5418425798416138 - trainLoss: 0.515262246131897\n",
      "cnt: 12 - valLoss: 0.5420596599578857 - trainLoss: 0.5152289867401123\n",
      "cnt: 13 - valLoss: 0.5418366193771362 - trainLoss: 0.5152450799942017\n",
      "cnt: 14 - valLoss: 0.5420543551445007 - trainLoss: 0.5152122378349304\n",
      "cnt: 15 - valLoss: 0.5417757630348206 - trainLoss: 0.5152279734611511\n",
      "cnt: 0 - valLoss: 0.5420117974281311 - trainLoss: 0.5152053236961365\n",
      "cnt: 1 - valLoss: 0.5418004393577576 - trainLoss: 0.5152051448822021\n",
      "cnt: 2 - valLoss: 0.5420258045196533 - trainLoss: 0.5151830911636353\n",
      "cnt: 3 - valLoss: 0.5417863726615906 - trainLoss: 0.5151910781860352\n",
      "cnt: 4 - valLoss: 0.5420146584510803 - trainLoss: 0.515167772769928\n",
      "cnt: 5 - valLoss: 0.5417764782905579 - trainLoss: 0.5151732563972473\n",
      "cnt: 6 - valLoss: 0.5420066118240356 - trainLoss: 0.515151858329773\n",
      "cnt: 7 - valLoss: 0.5417687296867371 - trainLoss: 0.5151559114456177\n",
      "cnt: 0 - valLoss: 0.5419999957084656 - trainLoss: 0.5151355862617493\n",
      "cnt: 1 - valLoss: 0.5417618751525879 - trainLoss: 0.5151388645172119\n",
      "cnt: 0 - valLoss: 0.5419941544532776 - trainLoss: 0.515119194984436\n",
      "cnt: 1 - valLoss: 0.5417212843894958 - trainLoss: 0.5151219367980957\n",
      "cnt: 0 - valLoss: 0.541965126991272 - trainLoss: 0.5151091814041138\n",
      "cnt: 1 - valLoss: 0.5417340993881226 - trainLoss: 0.5151013731956482\n",
      "cnt: 2 - valLoss: 0.5419719815254211 - trainLoss: 0.5150890350341797\n",
      "cnt: 3 - valLoss: 0.5417355895042419 - trainLoss: 0.5150864720344543\n",
      "cnt: 4 - valLoss: 0.5419731736183167 - trainLoss: 0.5150710940361023\n",
      "cnt: 5 - valLoss: 0.5416992902755737 - trainLoss: 0.5150707960128784\n",
      "cnt: 0 - valLoss: 0.5419464707374573 - trainLoss: 0.5150603652000427\n",
      "cnt: 1 - valLoss: 0.5417141318321228 - trainLoss: 0.5150505900382996\n",
      "cnt: 2 - valLoss: 0.5419559478759766 - trainLoss: 0.5150399208068848\n",
      "cnt: 3 - valLoss: 0.5416828989982605 - trainLoss: 0.5150363445281982\n",
      "cnt: 0 - valLoss: 0.5419307351112366 - trainLoss: 0.5150282382965088\n",
      "cnt: 1 - valLoss: 0.5416987538337708 - trainLoss: 0.5150163769721985\n",
      "cnt: 2 - valLoss: 0.5419434905052185 - trainLoss: 0.5150076150894165\n",
      "cnt: 3 - valLoss: 0.5416690111160278 - trainLoss: 0.5150026679039001\n",
      "cnt: 0 - valLoss: 0.5419194102287292 - trainLoss: 0.5149957537651062\n",
      "cnt: 1 - valLoss: 0.5416520237922668 - trainLoss: 0.5149829387664795\n",
      "cnt: 0 - valLoss: 0.5419078469276428 - trainLoss: 0.5149815678596497\n",
      "cnt: 1 - valLoss: 0.5416420698165894 - trainLoss: 0.5149653553962708\n",
      "cnt: 0 - valLoss: 0.5418975353240967 - trainLoss: 0.5149659514427185\n",
      "cnt: 1 - valLoss: 0.541632890701294 - trainLoss: 0.5149479508399963\n",
      "cnt: 0 - valLoss: 0.5418902039527893 - trainLoss: 0.5149502158164978\n",
      "cnt: 1 - valLoss: 0.5416252613067627 - trainLoss: 0.5149310827255249\n",
      "cnt: 0 - valLoss: 0.5418834090232849 - trainLoss: 0.5149341821670532\n",
      "cnt: 1 - valLoss: 0.5416181087493896 - trainLoss: 0.514914333820343\n",
      "cnt: 0 - valLoss: 0.5418776869773865 - trainLoss: 0.5149180293083191\n",
      "cnt: 1 - valLoss: 0.5416114926338196 - trainLoss: 0.5148977041244507\n",
      "cnt: 0 - valLoss: 0.5418744683265686 - trainLoss: 0.5149019360542297\n",
      "cnt: 1 - valLoss: 0.5416063666343689 - trainLoss: 0.5148816108703613\n",
      "cnt: 0 - valLoss: 0.5418681502342224 - trainLoss: 0.5148854851722717\n",
      "cnt: 1 - valLoss: 0.5415993928909302 - trainLoss: 0.5148649215698242\n",
      "cnt: 0 - valLoss: 0.5418625473976135 - trainLoss: 0.5148693919181824\n",
      "cnt: 1 - valLoss: 0.5415927767753601 - trainLoss: 0.5148484110832214\n",
      "cnt: 0 - valLoss: 0.5418572425842285 - trainLoss: 0.5148532390594482\n",
      "cnt: 1 - valLoss: 0.5415864586830139 - trainLoss: 0.5148319005966187\n",
      "cnt: 0 - valLoss: 0.5418513417243958 - trainLoss: 0.5148370862007141\n",
      "cnt: 1 - valLoss: 0.5415797829627991 - trainLoss: 0.5148153901100159\n",
      "cnt: 0 - valLoss: 0.5418459177017212 - trainLoss: 0.5148209929466248\n",
      "cnt: 1 - valLoss: 0.5415734052658081 - trainLoss: 0.5147989392280579\n",
      "cnt: 0 - valLoss: 0.5418407917022705 - trainLoss: 0.5148048996925354\n",
      "cnt: 1 - valLoss: 0.5415670871734619 - trainLoss: 0.5147825479507446\n",
      "cnt: 0 - valLoss: 0.5418358445167542 - trainLoss: 0.5147887468338013\n",
      "cnt: 1 - valLoss: 0.5415608882904053 - trainLoss: 0.5147661566734314\n",
      "cnt: 0 - valLoss: 0.5418306589126587 - trainLoss: 0.5147725939750671\n",
      "cnt: 1 - valLoss: 0.5417142510414124 - trainLoss: 0.5147497653961182\n",
      "cnt: 2 - valLoss: 0.5418965816497803 - trainLoss: 0.5147309303283691\n",
      "cnt: 3 - valLoss: 0.5417709350585938 - trainLoss: 0.5147396326065063\n",
      "cnt: 4 - valLoss: 0.5415163636207581 - trainLoss: 0.5147163271903992\n",
      "cnt: 0 - valLoss: 0.5417979955673218 - trainLoss: 0.514738142490387\n",
      "cnt: 1 - valLoss: 0.5415281653404236 - trainLoss: 0.5147051215171814\n",
      "cnt: 2 - valLoss: 0.5418024659156799 - trainLoss: 0.5147182941436768\n",
      "cnt: 3 - valLoss: 0.5416901707649231 - trainLoss: 0.5146898031234741\n",
      "cnt: 4 - valLoss: 0.5418723225593567 - trainLoss: 0.514675498008728\n",
      "cnt: 5 - valLoss: 0.5417516231536865 - trainLoss: 0.5146799683570862\n",
      "cnt: 6 - valLoss: 0.5414938926696777 - trainLoss: 0.5146583914756775\n",
      "cnt: 0 - valLoss: 0.5417763590812683 - trainLoss: 0.5146818161010742\n",
      "cnt: 1 - valLoss: 0.5416680574417114 - trainLoss: 0.5146464109420776\n",
      "cnt: 2 - valLoss: 0.5418524146080017 - trainLoss: 0.5146365165710449\n",
      "cnt: 3 - valLoss: 0.5417354702949524 - trainLoss: 0.5146369934082031\n",
      "cnt: 4 - valLoss: 0.541476845741272 - trainLoss: 0.5146164894104004\n",
      "cnt: 0 - valLoss: 0.5417611002922058 - trainLoss: 0.5146419405937195\n",
      "cnt: 1 - valLoss: 0.5416551232337952 - trainLoss: 0.5146039128303528\n",
      "cnt: 2 - valLoss: 0.5418375134468079 - trainLoss: 0.5145965218544006\n",
      "cnt: 3 - valLoss: 0.5417240858078003 - trainLoss: 0.5145946741104126\n",
      "cnt: 4 - valLoss: 0.5414628386497498 - trainLoss: 0.5145754814147949\n",
      "cnt: 0 - valLoss: 0.5417256951332092 - trainLoss: 0.5146017074584961\n",
      "cnt: 1 - valLoss: 0.5416251420974731 - trainLoss: 0.5145601034164429\n",
      "cnt: 2 - valLoss: 0.5418127775192261 - trainLoss: 0.5145589709281921\n",
      "cnt: 3 - valLoss: 0.5417038798332214 - trainLoss: 0.5145515203475952\n",
      "cnt: 4 - valLoss: 0.5418190360069275 - trainLoss: 0.5145341157913208\n",
      "cnt: 5 - valLoss: 0.5417073965072632 - trainLoss: 0.5145360231399536\n",
      "cnt: 6 - valLoss: 0.5416097640991211 - trainLoss: 0.5145174860954285\n",
      "cnt: 7 - valLoss: 0.5417963266372681 - trainLoss: 0.514519453048706\n",
      "cnt: 8 - valLoss: 0.5416892170906067 - trainLoss: 0.5145090818405151\n",
      "cnt: 9 - valLoss: 0.5419459939002991 - trainLoss: 0.5144937634468079\n",
      "cnt: 10 - valLoss: 0.5416794419288635 - trainLoss: 0.5145130753517151\n",
      "cnt: 11 - valLoss: 0.5419377684593201 - trainLoss: 0.514478325843811\n",
      "cnt: 12 - valLoss: 0.5416721105575562 - trainLoss: 0.5144955515861511\n",
      "cnt: 13 - valLoss: 0.5419310927391052 - trainLoss: 0.514462411403656\n",
      "cnt: 14 - valLoss: 0.5416658520698547 - trainLoss: 0.5144784450531006\n",
      "cnt: 15 - valLoss: 0.5419251322746277 - trainLoss: 0.5144461393356323\n",
      "cnt: 16 - valLoss: 0.5416600108146667 - trainLoss: 0.5144614577293396\n",
      "cnt: 17 - valLoss: 0.5419196486473083 - trainLoss: 0.5144298076629639\n",
      "cnt: 18 - valLoss: 0.5416544079780579 - trainLoss: 0.5144447088241577\n",
      "cnt: 19 - valLoss: 0.5419142842292786 - trainLoss: 0.5144134759902954\n",
      "cnt: 20 - valLoss: 0.5416489243507385 - trainLoss: 0.514427900314331\n",
      "cnt: 21 - valLoss: 0.5419090390205383 - trainLoss: 0.5143970847129822\n",
      "cnt: 22 - valLoss: 0.541643500328064 - trainLoss: 0.514411211013794\n",
      "cnt: 23 - valLoss: 0.541901707649231 - trainLoss: 0.514380693435669\n",
      "cnt: 24 - valLoss: 0.541724443435669 - trainLoss: 0.514394223690033\n",
      "cnt: 25 - valLoss: 0.541623592376709 - trainLoss: 0.5143628716468811\n",
      "cnt: 26 - valLoss: 0.5418854355812073 - trainLoss: 0.5143592357635498\n",
      "cnt: 27 - valLoss: 0.5417108535766602 - trainLoss: 0.5143678784370422\n",
      "cnt: 28 - valLoss: 0.5416116118431091 - trainLoss: 0.5143370628356934\n",
      "cnt: 29 - valLoss: 0.5418748259544373 - trainLoss: 0.5143357515335083\n",
      "cnt: 30 - valLoss: 0.5417010188102722 - trainLoss: 0.5143424868583679\n",
      "cnt: 31 - valLoss: 0.5416023135185242 - trainLoss: 0.5143116116523743\n",
      "cnt: 32 - valLoss: 0.5418661832809448 - trainLoss: 0.5143116116523743\n",
      "cnt: 33 - valLoss: 0.5416924357414246 - trainLoss: 0.514317512512207\n",
      "cnt: 34 - valLoss: 0.5415939092636108 - trainLoss: 0.5142863988876343\n",
      "cnt: 35 - valLoss: 0.5418581366539001 - trainLoss: 0.5142872929573059\n",
      "cnt: 36 - valLoss: 0.5416843295097351 - trainLoss: 0.5142925381660461\n",
      "cnt: 37 - valLoss: 0.5415677428245544 - trainLoss: 0.5142613649368286\n",
      "cnt: 38 - valLoss: 0.5418373942375183 - trainLoss: 0.5142678022384644\n",
      "cnt: 39 - valLoss: 0.5416677594184875 - trainLoss: 0.5142658352851868\n",
      "cnt: 40 - valLoss: 0.5415538549423218 - trainLoss: 0.5142356753349304\n",
      "cnt: 41 - valLoss: 0.5418254733085632 - trainLoss: 0.5142450928688049\n",
      "cnt: 42 - valLoss: 0.5416576266288757 - trainLoss: 0.514240562915802\n",
      "cnt: 43 - valLoss: 0.5415443778038025 - trainLoss: 0.5142106413841248\n",
      "cnt: 44 - valLoss: 0.5418166518211365 - trainLoss: 0.5142213106155396\n",
      "cnt: 45 - valLoss: 0.5416489243507385 - trainLoss: 0.5142156481742859\n",
      "cnt: 46 - valLoss: 0.5415357947349548 - trainLoss: 0.514185905456543\n",
      "cnt: 47 - valLoss: 0.5418084859848022 - trainLoss: 0.5141972303390503\n",
      "cnt: 48 - valLoss: 0.5416406989097595 - trainLoss: 0.5141909718513489\n",
      "cnt: 49 - valLoss: 0.5415276288986206 - trainLoss: 0.5141611695289612\n",
      "cnt: 50 - valLoss: 0.5418004989624023 - trainLoss: 0.5141730904579163\n",
      "cnt: 51 - valLoss: 0.5416326522827148 - trainLoss: 0.5141663551330566\n",
      "cnt: 52 - valLoss: 0.5415195226669312 - trainLoss: 0.5141366124153137\n",
      "cnt: 53 - valLoss: 0.5417928099632263 - trainLoss: 0.514149010181427\n",
      "cnt: 54 - valLoss: 0.5416246652603149 - trainLoss: 0.5141417980194092\n",
      "cnt: 55 - valLoss: 0.5415114760398865 - trainLoss: 0.5141119360923767\n",
      "cnt: 56 - valLoss: 0.5417850017547607 - trainLoss: 0.514124870300293\n",
      "cnt: 57 - valLoss: 0.5416167974472046 - trainLoss: 0.5141171813011169\n",
      "cnt: 58 - valLoss: 0.5415035486221313 - trainLoss: 0.514087438583374\n",
      "cnt: 59 - valLoss: 0.5417773723602295 - trainLoss: 0.5141008496284485\n",
      "cnt: 60 - valLoss: 0.541608989238739 - trainLoss: 0.514092743396759\n",
      "cnt: 61 - valLoss: 0.5414955615997314 - trainLoss: 0.5140628814697266\n",
      "cnt: 62 - valLoss: 0.5417696237564087 - trainLoss: 0.5140767693519592\n",
      "cnt: 63 - valLoss: 0.5416011214256287 - trainLoss: 0.5140682458877563\n",
      "cnt: 64 - valLoss: 0.5414876341819763 - trainLoss: 0.5140384435653687\n",
      "cnt: 65 - valLoss: 0.5417425036430359 - trainLoss: 0.5140529870986938\n",
      "cnt: 66 - valLoss: 0.5415518879890442 - trainLoss: 0.514041543006897\n",
      "cnt: 67 - valLoss: 0.541786253452301 - trainLoss: 0.5140207409858704\n",
      "cnt: 68 - valLoss: 0.5415782332420349 - trainLoss: 0.514035165309906\n",
      "cnt: 69 - valLoss: 0.5417079925537109 - trainLoss: 0.5139991641044617\n",
      "cnt: 70 - valLoss: 0.5415254831314087 - trainLoss: 0.5140072703361511\n",
      "cnt: 71 - valLoss: 0.5417648553848267 - trainLoss: 0.513993501663208\n",
      "cnt: 72 - valLoss: 0.5415608882904053 - trainLoss: 0.5140030384063721\n",
      "cnt: 73 - valLoss: 0.5416929125785828 - trainLoss: 0.5139690637588501\n",
      "cnt: 74 - valLoss: 0.5415117144584656 - trainLoss: 0.5139763951301575\n",
      "cnt: 75 - valLoss: 0.541752278804779 - trainLoss: 0.5139634609222412\n",
      "cnt: 76 - valLoss: 0.5415489077568054 - trainLoss: 0.5139725208282471\n",
      "cnt: 77 - valLoss: 0.5417768359184265 - trainLoss: 0.5139385461807251\n",
      "cnt: 78 - valLoss: 0.541561484336853 - trainLoss: 0.513962984085083\n",
      "cnt: 79 - valLoss: 0.5416590571403503 - trainLoss: 0.5139212608337402\n",
      "cnt: 80 - valLoss: 0.5414837598800659 - trainLoss: 0.5139285922050476\n",
      "cnt: 81 - valLoss: 0.541728138923645 - trainLoss: 0.5139206051826477\n",
      "cnt: 82 - valLoss: 0.5415263772010803 - trainLoss: 0.5139262676239014\n",
      "cnt: 83 - valLoss: 0.5417263507843018 - trainLoss: 0.5138951539993286\n",
      "cnt: 84 - valLoss: 0.5415232181549072 - trainLoss: 0.5139122605323792\n",
      "cnt: 85 - valLoss: 0.5417225956916809 - trainLoss: 0.5138801336288452\n",
      "cnt: 86 - valLoss: 0.5415187478065491 - trainLoss: 0.5138978958129883\n",
      "cnt: 87 - valLoss: 0.5417179465293884 - trainLoss: 0.5138654112815857\n",
      "cnt: 88 - valLoss: 0.5415138602256775 - trainLoss: 0.5138834118843079\n",
      "cnt: 89 - valLoss: 0.5417129993438721 - trainLoss: 0.5138508677482605\n",
      "cnt: 90 - valLoss: 0.5415085554122925 - trainLoss: 0.5138688087463379\n",
      "cnt: 91 - valLoss: 0.5417078137397766 - trainLoss: 0.5138363242149353\n",
      "cnt: 92 - valLoss: 0.5415032505989075 - trainLoss: 0.5138542056083679\n",
      "cnt: 93 - valLoss: 0.5417026281356812 - trainLoss: 0.5138217806816101\n",
      "cnt: 94 - valLoss: 0.5414978861808777 - trainLoss: 0.5138396620750427\n",
      "cnt: 95 - valLoss: 0.5416973233222961 - trainLoss: 0.5138072967529297\n",
      "cnt: 96 - valLoss: 0.5414925217628479 - trainLoss: 0.5138250589370728\n",
      "cnt: 97 - valLoss: 0.5416909456253052 - trainLoss: 0.5137929320335388\n",
      "cnt: 98 - valLoss: 0.5414864420890808 - trainLoss: 0.5138102769851685\n",
      "cnt: 99 - valLoss: 0.541685163974762 - trainLoss: 0.5137786269187927\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x17561b760>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def trainAI(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    trainLoss=0\n",
    "    for X, y in dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        trainLoss +=loss.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    return trainLoss\n",
    "\n",
    "def valAI(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    valLoss =0\n",
    "    with torch.no_grad():\n",
    "        for X ,y  in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y)\n",
    "            # for i in loss:\n",
    "            valLoss+= loss.item()\n",
    "    return valLoss\n",
    "\n",
    "trainLoss=[]\n",
    "valLoss=[]\n",
    "bestModel = model\n",
    "bestLoss = float('inf')\n",
    "cnt=0\n",
    "\n",
    "while(cnt<100):\n",
    "    trainLoss.append(trainAI(trainLoader, model, loss_fn, optimizer))\n",
    "    valLoss.append(valAI(valLoader, model, loss_fn))\n",
    "\n",
    "    print(f'cnt: {cnt} - valLoss: {valLoss[-1]} - trainLoss: {trainLoss[-1]}')\n",
    "    if bestLoss<valLoss[-1]:\n",
    "        cnt+=1\n",
    "    else:\n",
    "        cnt = 0\n",
    "        bestLoss = valLoss[-1]\n",
    "        bestModel = model\n",
    "\n",
    "plt.plot(trainLoss,label='trainLoss')\n",
    "plt.plot(valLoss,label='valLoss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>passengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>1305</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>1306</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>1307</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>1308</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>1309</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     passengerId  Survived\n",
       "0            892         0\n",
       "1            893         0\n",
       "2            894         0\n",
       "3            895         0\n",
       "4            896         1\n",
       "..           ...       ...\n",
       "413         1305         0\n",
       "414         1306         1\n",
       "415         1307         0\n",
       "416         1308         0\n",
       "417         1309         0\n",
       "\n",
       "[418 rows x 2 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def testAI(dataloader, model):\n",
    "    model.eval()\n",
    "    result = []\n",
    "    y=892\n",
    "    with torch.no_grad():\n",
    "        for X ,_  in dataloader:\n",
    "            X  = X.to(device)\n",
    "            pred = model(X)\n",
    "            for i in pred:\n",
    "                result.append([y,torch.argmax(i).item()])\n",
    "                y+=1\n",
    "    return result\n",
    "\n",
    "result = testAI(testLoader, bestModel)\n",
    "result = pd.DataFrame(result)\n",
    "result = result.astype(int)\n",
    "result.columns=['passengerId','Survived']\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('result.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
